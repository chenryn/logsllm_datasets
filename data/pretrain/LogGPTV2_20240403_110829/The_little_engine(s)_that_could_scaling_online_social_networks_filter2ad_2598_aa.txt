title:The little engine(s) that could: scaling online social networks
author:Josep M. Pujol and
Vijay Erramilli and
Georgos Siganos and
Xiaoyuan Yang and
Nikolaos Laoutaris and
Parminder Chhabra and
Pablo Rodriguez
The Little Engine(s) That Could:
Scaling Online Social Networks
Josep M. Pujol, Vijay Erramilli, Georgos Siganos, Xiaoyuan Yang
Nikos Laoutaris, Parminder Chhabra, Pablo Rodriguez
{ jmps, vijay, georgos, yxiao, nikos, pchhabra, pablorr }@tid.es
Telefonica Research
ABSTRACT
The diﬃculty of scaling Online Social Networks (OSNs) has
introduced new system design challenges that has often caused
costly re-architecting for services like Twitter and Facebook.
The complexity of interconnection of users in social net-
works has introduced new scalability challenges. Conven-
tional vertical scaling by resorting to full replication can
be a costly proposition. Horizontal scaling by partitioning
and distributing data among multiples servers – e.g. using
DHTs – can lead to costly inter-server communication.
We design, implement, and evaluate SPAR, a social parti-
tioning and replication middle-ware that transparently lever-
ages the social graph structure to achieve data locality while
minimizing replication. SPAR guarantees that for all users
in an OSN, their direct neighbor’s data is co-located in the
same server. The gains from this approach are multi-fold:
application developers can assume local semantics, i.e., de-
velop as they would for a single server; scalability is achieved
by adding commodity servers with low memory and network
I/O requirements; and redundancy is achieved at a fraction
of the cost.
We detail our system design and an evaluation based on
datasets from Twitter, Orkut, and Facebook, with a working
implementation. We show that SPAR incurs minimum over-
head, and can help a well-known open-source Twitter clone
reach Twitter’s scale without changing a line of its applica-
tion logic and achieves higher throughput than Cassandra,
Facebook’s DHT based key-value store database.
Categories and Subject Descriptors
D.3.4 Information Systems [Information storage and re-
trieval]: Systems and Software, Distributed systems; E.1
Data [Data Structures]: Graphs and networks
General Terms
Algorithms, Experiments, Performance
Keywords
Social Networks, Scalability, Partition, Replication
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’10, August 30–September 3, 2010, New Delhi, India.
Copyright 2010 ACM 978-1-4503-0201-2/10/08 ...$10.00.
1.
INTRODUCTION
Recently, there has been a recent unprecedented increase
in the use of Online Social Networks (OSNs) and applica-
tions with a social component. The most popular OSNs at-
tract hundreds of millions of users – e.g. Facebook [16] – and
deliver status updates at very high rates – e.g. Twitter [7].
OSNs diﬀer from traditional web applications signiﬁcantly
in diﬀerent ways: they handle highly personalized content;
produce non-traditional workloads [11, 32]; and most im-
portantly, they deal with highly interconnected data due to
the presence of a strong community structure among their
end users [23, 26, 27, 12]. All these factors create new chal-
lenges for the maintenance, management and scaling of OSN
systems.
Scaling real systems is hard as it is, but the problem is
particularly acute for OSNs, due to their interconnected na-
ture and their astounding growth rate. Twitter, for instance,
grew by 1382% between Feb and Mar 2009 [24] and was thus
forced to redesign and re-implement its architecture several
times in order to keep up with the demand. Other OSNs
that failed to do so have virtually disappeared [2].
A natural and traditional solution to cope with higher
demand is to upgrade existing hardware. Such vertical scal-
ing, however, is expensive because of the cost of high perfor-
mance servers. In some cases, vertical scaling can even be
technically infeasible, e.g. Facebook requires multiple hun-
dreds of Terabytes of memory across thousands of machines
[3]. A more cost eﬀective approach is to rely on horizontal
scaling by engaging a higher number of cheap commodity
servers and partitioning the load among them. The advent
of cloud computing systems like Amazon EC2 and Google
AppEngine has streamlined horizontal scaling by removing
the need to own hardware and instead providing the ability
to lease virtual machines (VMs) dynamically from the cloud.
Horizontal scaling has eased most of the scaling problems
faced by traditional web applications. Since the application
front-end and logic is stateless, it can be instantiated on
new servers on demand in order to meet the current load.
The data back-end layer however, is more problematic since
it maintains state. If data can be partitioned into disjoint
components, horizontal scaling still holds. However, this last
condition does not hold for OSNs.
In the case of OSNs, the existence of social communi-
ties [23, 26, 27], hinders the partitioning of the data back-
end into clean, disjoint components[18, 17]. The problem
in OSNs is that most of the operations are based on the
data of a user and her neighbors. Since users belong to
more than one community, there is no disjoint partition (i.e.
375server) where users and all her neighbors can be co-located.
This hairball that is the community structure causes a lot
of inter-server traﬃc for resolving queries. The problem be-
comes particularly acute under random partitioning, which
is the de-facto standard in OSNs [16, 30]. On the other
hand, replicating user’s data in multiple or all the servers
eliminates the inter-server traﬃc for reads but increases the
replication overhead. This has a negative impact on query
execution times and network traﬃc for propagating updates
and maintaining consistency across replicas. Scalability for
OSNs is indeed a diﬃcult beast to tame.
2. OUR CONTRIBUTION – SPAR
The main contribution of this work is the design, imple-
mentation and extensive evaluation of SPAR: a Social Parti-
tioning And Replication middle-ware for social applications.
2.1 What does SPAR do?
Solves the Designer’s Dilemma for early stage OSNs.
Designers and developers of an early-stage OSN are con-
fronted with the Designer’s Dilemma : “Should they commit
scarce developer resources towards adding features or should
they ﬁrst ensure that they have a highly scalable system in
place that can handle high traﬃc volume?”. Choosing the
ﬁrst option can lead to “death-by-success” – users join at-
tracted to appealing features, but if the infrastructure can-
not support an adequate QoS, it results in frustrated users
leaving the service en-masse – e.g.
this was the story be-
hind the demise of Friendster [2]. On the other hand, start-
ing with a highly scalable system, similar to the ones that
power established OSNs like Facebook and Twitter, requires
devoting scarce resources to complex distributed program-
ming and management issues. This comes at the expense
of building the core of the application that attracts users in
the ﬁrst place.
SPAR avoids this dilemma by enabling transparent OSN
scalability. SPAR constricts all relevant data for a user on a
server. The enforcement of local semantics at the data level
allows queries to be resolved locally on that server, creating
the illusion that the system is running on one centralized
server. This simpliﬁes programming for developers and en-
ables them to focus on the core features of the service.
Avoids performance bottlenecks in established OSNs.
SPAR avoids the potential performance problems of hav-
ing to query multiple servers across a network by enforc-
ing local semantics on the data. For instance, the de-facto
standard random partitioning used by Twitter or Facebook,
splits data across hundreds of data back-end servers. These
servers are then queried with multi-get requests to fetch the
neighbors’ data (e.g., all the friends’ tweets). This can result
in unpredictable response times, determined by the highest-
latency server. The problem can be particularly acute under
heavy data center loads, where network congestion can cause
severe network delays.
2
10
4
3
9
1
1
2
2
4
10
10
8
3
9
4
8
read trafﬁc = 0
write trafﬁc = 10
memory = 10 
5
7
6
(a)
read trafﬁc = 10
write trafﬁc = 0
memory = 0 
(b)
read trafﬁc = 0
write trafﬁc = 8
memory = 8 
1
2
10
4
8
1
1
2
2
4
3
4
10
8
10
9
5
7
2
10
4
4
3
2
9
10
5
7
4
8
5
3
7
9
6
3
9
5
7
1
2
10
3
9
5
7
6
6
2
10
4
8
3
9
5
7
6
(c)
read trafﬁc = 0
write trafﬁc = 2
memory = 2 
3
(d)
4
8
5
7
6
Figure 1: Sketch of social network to be partitioned in two
servers using (a) Full Replication, (b) Partition using DHT
(random partitioning), (c) Random Partitioning (DHT) with
replication of the neighbors, (d) SPAR, socially aware parti-
tion and replication of the neighbors.
requests [1]. Using additional servers decreases the band-
width per server. It does not, however, decrease the number
of requests per server, which means that CPU usage stays
roughly the same. SPAR reduces the impact of such multi-
get operations by ensuring that all required data is kept
local to the server, avoiding potential network and server
bottlenecks and thus increasing throughput.
Minimizes the effect of provider lock-ins.
The Designer’s Dilemma has prompted several providers
of cloud services to develop and oﬀer scalable “Key-Value”
stores (so-called NoSQL) that run on top of DHTs. They
oﬀer transparent scalability at the expense of sacriﬁcing the
full power of established RDBMS, losing an expressive query
language like SQL, powerful query optimizers and the encap-
sulation and abstraction of data related operations, etc. Fur-
ther, systems such as Amazon’s SimpleDB and Google’s
BigTable require using APIs that are tied to a particular
cloud provider and thus suﬀer from poor portability that
can lead to architectural lock-ins [9].
In addition to potential network problems, individual servers
Cross platform Key-Value stores like Cassandra or CouchDB
could also suﬀer performance problems (e.g., Network I/O,
Disk I/O or CPU bottlenecks) and drive down the perfor-
mance of the system. For instance, servers could become
CPU bounded as they need to handle a larger number of
query requests from other servers. When a server CPU
is bound, adding more servers does not help serve more
do not cause lock-in concerns, but suﬀer from the aforemen-
tioned shortcomings in addition to performance problems as
we will argue shortly.
SPAR is implemented as a middle-ware that is platform
agnostic and allows developers to select its preferred data-
store, either be a Key-Value store or a relational database.
3762.2 How does SPAR do it?
3. PROBLEM STATEMENT
Through joint partitioning and replication. On the par-
tition side, SPAR ensures that the underlying community
structure is preserved as much as possible. On the replica-
tion side, SPAR ensures that data of all one-hop neighbors
of a user hosted on a particular server is co-located on that
same server, thereby guaranteeing local semantics of data.
Note that most of the relevant data for a user in an OSN is
one-hop away (friends, followers, etc.).
Fig. 1 is a toy-example highlighting the operation and ben-
eﬁts of SPAR. At the top of Fig. 1, we depict a social graph
with 10 users (nodes) and 15 edges (bidirectional friendship
relationships). The social graph contains an evident strong
community structure: two communities that are connected
through the “bridge” nodes 3 and 4. Then, we depict the
physical placement of users on 2 servers under four diﬀer-
ent solutions. We summarize the memory (in terms of user
proﬁles/data) and network cost assuming unit-sized proﬁles
and a read rate of 1 for all proﬁles.
Random partition (b) – the de-facto standard of Key-
Value stores – minimizes the replication overhead (0 units),
and thus has the lowest memory (either RAM or Disk) foot-
print on the servers. On the downside, random partition
imposes the highest aggregate network traﬃc due to reads
(10 units), and thus increases the network I/O cost of the
servers and the networking equipment that interconnects
them. Replicating the neighbors as shown in (c) will elim-
inate the read traﬃc across servers, but will increase the
memory in return. Another widely used approach is Full
Replication (a).
In this case, network read traﬃc falls to
0, but the memory requirements are high (10 units). Full
replication also results in high write traﬃc to maintain con-
sistency. Our proposed solution, SPAR (d), performs the
best overall.
Summary of results.
The above toy example is a preview of the performance
results of Sec. 5 based on workloads from Twitter, Orkut,
and Facebook. We summarize the results here:
• SPAR provides local semantics with the least overhead
(reduction of 200% over random in some cases), while
reducing inter-server read traﬃc to zero.
• SPAR handles node and edge dynamics observed in
OSNs with minimal overhead in the system. In addi-
tion, SPAR provides mechanisms for addition/removal
of servers and handle failures gracefully.
• In our implementation, SPAR serves 300% more req/s
than Cassandra while reducing network traﬃc by a
factor of 8. We also show substantial gains when we
implement SPAR with MySQL.
2.3 What does SPAR not do?
SPAR is not designed for the distribution of content such
as pictures and videos in an OSN. This problem is well stud-
ied in the area of Content Distribution Networks (CDN).
SPAR is an On-line Transaction Processing system (OLTP),
it is not a solution for storage or for batch data analysis such
as Hadoop (MapReduce). SPAR is not intended to charac-
terize or aid in computing properties of the OSN graph,
although we leave this for future work.
We ﬁrst describe the requirements that SPAR has to ad-
dress. Next, we formulate the problem solved by SPAR.
Finally, we discuss why existing social partition based solu-
tions are inadequate to meet our set of requirements.
3.1 Requirements
The set of requirements that SPAR must fulﬁll are:
Maintain local semantics: Relevant data for a user in
OSNs is her own and that of her direct neighbors (e.g. fol-
lowees’ tweets, friends’ status updates, etc.). To achieve lo-
cal semantics we need to ensure that for every master replica
of a user, either a master or a slave replica of all her direct
neighbors is co-located in the same server. We use the term
replica to refer to a copy of the user’s data. We diﬀerenti-
ate between the master replica, serving all application level
read/write operations; and the slave replica, required for