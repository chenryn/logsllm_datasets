calls in parallel, allowing us to model many users from a single node.
Attacks were initiated from the W-UAs at caller W-UA-1 and callee
W-UA-2. We staged attacks by repurposing SIPp as an attack tool, supplying
it with scenario ﬁles that specify malicious caller/callee behaviors such as ﬂood-
ing requests or excessive duplication of responses. For example, a BYE ﬂood
attack equates to W-UA-1 initiating a number of spurious BYE transactions,
each with a new call id to represent a new transaction. Because SIP does not
associate a BYE with a prior INVITE, the BYE is accepted and transaction
memory is wastefully reserved while the attack is in process. W-UA-2 colludes
with W-UA-1 by purposefully not responding to the request, which adds to the
time transaction memory is held at the server. Like the benign workload, we can
tune the amplitude of SIPp to control the number of simultaneous attack calls.
The Cogo model for OpenSIPS was built from ﬁve benign observation col-
lecting runs, totaling 8 h of benign measurements. During this observation run
OpenSIPS was subjected to a benign load between the SIPp clients (UA-1, UA-2)
and the SIP server. The clients initiated calls to the server. Call setup and call
disconnect were speciﬁed using XML ﬁles input to SIPp and followed standard
SIP call setup conventions for invite, ringing, bye, and appropriate response and
status messages. Call hold used the SIPp log-normal distribution. The SIPp
maximum calls per second rate was set to 10 and call limit to 200. This com-
bination of SIPp settings produced a steady call rate of ∼7 calls every second.
Several additional benign observation runs were made during which OpenSIPS
was started and then terminated to ensure the observations captured startup
Practical and Accurate Runtime Application Protection
467
Table 3. Summary of OpenSIPS results. The number of benign call requests was 6,342.
Item
Training time (sec.)
Model size (MB)
43,493
Radmin
Improvement
(cid:2) 169×
(cid:3) 0.75×
41
9, 0.1419% 4, 0.0631% (cid:2) 2.25×
Cogo
258
55
FPs, FPR
Bye ﬂood detection delay (sec.) ∞
Invite ﬂood detection delay (sec.) ∞
6
4
(cid:2) ∞
(cid:2) ∞
and shutdown which from experience has shown considerable variability. The
total size of the observation data was 515 MB. Processing these observations
resulted in a model of 55 MB. Several test runs were made using the model and
with it Cogo exhibited virtually zero false positives under a load with the same
characteristics as that used for the observation runs.
Detection Results. Table 3 summarizes the results. Cogo reduced training
time from about 12 h to only 4 min (greater than 169× reduction). The model
size increased by a factor of only 0.75×, from 41 MB to 55 MB. In terms of
accuracy, Cogo only had 4 FPs throughout the experiment. The four FPs all
occurred at startup time of OpenSIPS. Radmin triggered 9 FPs also at startup
time. The impact of BYE and INVITE ﬂoods on OpenSIPS, and the detection
behavior of Cogo is shown in Fig. 7. The attacks were not detectable by Radmin
since OpenSIPS uses a ﬁxed size memory pool, therefore preventing memory
exhaustion by the attack calls. In other words, without monitoring network I/O,
it is impossible to early detect BYE and INVITE ﬂoods. Cogo, on the other
hand, detected the attacks almost immediately, within less than six seconds after
the attacks onset. Note that we did not implement any remediation policy for
OpenSIPS; proper remediation requires a protocol-speciﬁc solution that times
out or hangs up the attack calls.
5.3 Performance Overhead
Cogo eﬀectively had a negligible overhead. We measured the throughput of
Apache and OpenSIPS on the benign workloads with and without Cogo. Apache
maintained a steady rate of 130 requests per second. We benchmarked Apache
with HTTPerf and experienced a very marginal 0.2± 0.3 ms response time
increase per request. The average response time increased from 10.3 ms to
10.5 ms. For OpenSIPS, it maintained a steady call rate of 200 calls per sec-
ond. We experimented with call rates from 300 to 1000 calls per second and did
not observe any degradation in throughput.
468
M. Elsabagh et al.
Fig. 7. Cogo detection of bye and invite ﬂoods against OpenSIPS.
6 Related Work
Modern operating systems have a number of threshold-based facilities to limit
resource consumption (ulimit, AppArmor [3]). However, these limits are static
upper bounds and disregard diﬀerent consumption of diﬀerent program segments
for diﬀerent inputs. This enables attackers to maximize the DoS time by crafting
inputs that trigger prolonged resource consumption or starvation, such as Slow-
rate attacks [21,23,30]. Several static and dynamic instrumentation tools exist
for proﬁling, such as Valgrind [32] and Intel Pin [31]. However, the instrumenta-
tion overhead is often too high to enable their continuous usage, especially when
detection of exhaustion is the goal [34,35]. Apostolico [18] presented a theoretic
study for linear prediction using a Generalized Suﬃx Trees (GST). However, to
the best of our knowledge, there is no implementation or a quantitative study
of [18]. Our approach, builds a simpler model using a PFA construction that
provided tight detection time and space guarantees instead of a GST.
In [14,30] there is a survey of diﬀerent approaches for anomalous traﬃc detec-
tion which is not connected directly or indirectly to resource consumption at the
application layer. Antunes et al. [17] proposed a system for testing servers for
exhaustion vulnerabilities using fuzzed test cases from user-supplied specs of the
server protocol. Groza et al. [27] formalized DoS attacks using a protocol-speciﬁc
cost rules. Aiello et al. [16] formalized DoS resilience rules that protocols should
meet but they are not feasible requirements in practice [37]. Chang et al. [20]
proposed a static analysis system for identifying source code sites that may result
in uncontrolled CPU time and stack consumption. The system employed taint
analysis and control-dependency analysis to identify source code sites that can be
inﬂuenced by untrusted input. Several similar approaches that required manual
code annotation were also developed [28,38]. Closely related, Burnim et al. [19]
used symbolic execution to generate inputs that exhibit worst case complexity.
Cogo substantially diﬀers from those systems in that it does not require
access to the source code or any side information and it covers network resources
Practical and Accurate Runtime Application Protection
469
used by an application, not only CPU and memory. Elsabagh et al. [25] proposed
Radmin, a system for early detection of application layer DoS attacks. This is the
system we used as a starting point for Cogo. The system showed good accuracy
and low overhead. However, it did not monitor network I/O, had a prohibitive
quadratic training time, and could not monitor containerized processes or attach
to a running process.
7 Conclusions
This paper presented Cogo, a practical and accurate system for early detection
of DoS attacks at the application layer. Unlike prior solutions, Cogo builds a
PFA model from the temporal and spatial resource usage information in linear
time. Cogo monitors network state, supports containerized processes monitoring
and attaching to running processes. Cogo detected real-world attacks on Apache
and OpenSIPS, both are large-scale servers. It achieved high accuracy, early
detection, and incurred negligible overhead. Cogo required less than 12 min of
training time, incurred less than 0.0194% false positives rate, detected a wide
range of attacks less than seven seconds into the attacks, and had a negligible
response time overhead of only 0.2± 0.3 ms. Cogo is both scalable and accurate,
suitable for large-scale deployment.
Acknowledgments. We thank the anonymous reviewers for their insightful com-
ments and suggestions. This material is based upon work supported in part by the
National Science Foundation (NSF) SaTC award 1421747, the National Institute of
Standards and Technology (NIST) award 60NANB16D285, and the Defense Advanced
Research Projects Agency (DARPA) contract no. HR0011-16-C-0061 in conjunction
with Vencore Labs. Opinions, ﬁndings, conclusions, and recommendations expressed in
this material are those of the authors and do not necessarily reﬂect the views of the
NSF, NIST, DARPA or the US Government.
References
1. myths of ddos attacks. http://blog.radware.com/security/2012/02/4-massive-
myths-of-ddos/
2. The apache http server project. https://httpd.apache.org/
3. Apparmor. http://wiki.apparmor.net/index.php/Main Page
4. Application layer DoS attack simulator. http://github.com/shekyan/slowhttptest
5. Are you ready for slow reading? https://blog.qualys.com/securitylabs/2012/01/
05/slow-read
6. Availability overrides security. http://hrfuture.net/performance-and-productivity/
availability-over-rides-cloud-security-concerns.php?Itemid=169
7. Httperf - http performance measurement tool. http://linux.die.net/man/1/httperf
8. Mobile users favor productivity over security. http://www.infoworld.com/article/
2686762/security/mobile-users-favor-productivity-over-security-as-they-should.
html
9. OpenSIPS: the new breed of communication engine. https://www.opensips.org/
470
M. Elsabagh et al.
10. Sipp: traﬃc generator proxy for the sip protocol. http://sipp.sourceforge.net/
11. Slow-Rate Attack. https://security.radware.com/ddos-knowledge-center/ddospedia/
slow-rate-attack/
12. Slowloris
- apache server vulnerabilities. https://security.radware.com/ddos-
knowledge-center/ddospedia/slowloris/
13. When the lights went out: Ukraine cybersecurity threat brieﬁng. https://www.
boozallen.com/insights/2016/09/ukraine-cybersecurity-threat-brieﬁng/
14. Denial of service attacks: A comprehensive guide to trends, techniques, and tech-
nologies. ADC Monthly Web Attacks Analysis 12 (2012)
15. Ahrenholz, J.: Comparison of core network emulation platforms. In: Military Com-
munications Conference (2010)
16. Aiello, W., Bellovin, S.M., Blaze, M., Ioannidis, J., Reingold, O., Canetti, R.,
Keromytis, A.D.: Eﬃcient, DoS-resistant, secure key exchange for internet proto-
cols. In: 9th ACM Conference on Computer and Communications Security (2002)
17. Antunes, J., Neves, N.F., Ver´ıssimo, P.J.: Detection and prediction of resource-
exhaustion vulnerabilities. In: International Symposium on Software Reliability
Engineering (2008)
18. Apostolico, A., Bejerano, G.: Optimal amnesic probabilistic automata. J. Comput.
Biol. 7(3–4), 381–393 (2000)
19. Burnim, J., Juvekar, S., Sen, K.: Wise: automated test generation for worst-case
complexity. In: 31st International Conference on Software Engineering (2009)
20. Chang, R.M., et al.: Inputs of coma: static detection of denial-of-service vulnera-
bilities. In: 22nd Computer Security Foundations Symposium (2009)
21. Chee, W.O., Brennan, T.: Layer-7 ddos. (2010). https://www.owasp.org/images/
4/43/Layer 7 DDOS.pdf
22. Choi, H.K., Limb, J.O.: A behavioral model of web traﬃc. In: 7th International
Conference on Network Protocols (1999)
23. Crosby, S., Wallach, D.: Algorithmic dos. In: van Tilborg, H.C.A., Jajodia, S. (eds.)
Encyclopedia of Cryptography and Security, pp. 32–33. Springer, USA (2011)
24. Desnoyers, M.: Using the linux kernel tracepoints. https://www.kernel.org/doc/
Documentation/trace/tracepoints.txt
25. Elsabagh, M., Barbar´a, D., Fleck, D., Stavrou, A.: Radmin: early detection of
application-level resource exhaustion and starvation attacks. In: 18th International
Conference on Research in Attacks, Intrusions and Defenses (2015)
26. Gray, R.M., Neuhoﬀ, D.L.: Quantization. IEEE Trans. Inform. Theory 44(6), 2325–
2383 (1998)
27. Groza, B., Minea, M.: Formal modelling and automatic detection of resource
exhaustion attacks. In: Symposium on Information, Computer and Communica-
tions Security (2011)
28. Gulavani, B.S., Gulwani, S.: A numerical abstract domain based on expression
abstraction and max operator with application in timing analysis. In: Computer
Aided Veriﬁcation, pp. 370–384 (2008)
29. Hilt, V., Eric, N., Charles, S., Ahmed, A.: Design considerations for session initia-
tion protocol (SIP) overload control (2011). https://tools.ietf.org/html/rfc6357
30. Kostadinov, D.: Layer-7 ddos attacks: detection and mitig. InfoSec Institute (2013)
31. Luk, C.K., Cohn, R., Muth, R., Patil, H., Klauser, A., Lowney, G., Wallace, S.,
Reddi, V.J., Hazelwood, K.: Pin: building customized program analysis tools with
dynamic instrumentation. In: Proceedings of the 2005 ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2005, pp. 190–200.
ACM, New York (2005). http://doi.acm.org/10.1145/1065010.1065034
Practical and Accurate Runtime Application Protection
471
32. Nethercote, N., Seward, J.: Valgrind: a framework for heavyweight dynamic binary
instrumentation. In: ACM Sigplan Notices, vol. 42, pp. 89–100. ACM (2007)
33. Rosenberg, J., et al.: SIP: Session initiation protocol (2002). https://www.ietf.org/
rfc/rfc3261.txt
34. Ruiz-Alvarez, A., Hazelwood, K.: Evaluating the impact of dynamic binary trans-
lation systems on hardware cache performance. In: International Symposium on
Workload Characterization (2008)
35. Uh, G.R., Cohn, R., Yadavalli, B., Peri, R., Ayyagari, R.: Analyzing dynamic
binary instrumentation overhead. In: Workshop on Binary Instrumentation and
Application (2007)
36. Ukkonen, E.: Online construction of suﬃx trees. Algorithmica 14(3), 249–260
(1995)
37. Zargar, S.T., Joshi, J., Tipper, D.: A survey of defense mechanisms against dis-
tributed denial of service (ddos) ﬂooding attacks. IEEE Commun. Surv. Tutorials
15(4), 2046–2069 (2013)
38. Zheng, L., Myers, A.C.: End-to-end availability policies and noninterference. In:
18th IEEE Workshop Computer Security Foundations (2005)