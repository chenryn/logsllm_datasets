word-level n-grams [13], [14] to more evolved structural
features obtained from abstract syntax trees [16], [34]. In
particular, Burrows et al. present an approach based on n-grams
that reaches an accuracy of 77% in differentiating 10 different
programmers [14].
Similarly, Kothari et al. combine n-grams with lexical
markers such as the line length, to build programmer proﬁles
that allow them to identify 12 authors with an accuracy of
76% [26]. Lange et al. further show that metrics based on
layout and lexical features along with a genetic algorithm
reach an accuracy of 75% in de-anonymizing 20 authors [28].
Finally, Caliskan-Islam et al. incorporate abstract syntax tree
based structural features to represent programmers’ coding
style [16]. They reach 94% accuracy in identifying 1,600
programmers of the GCJ data set.
Executable binary stylometry. In contrast,
identifying
programmers from compiled code is considerably more difﬁ-
cult and has received little attention to date. Code compilation
results in a loss of information and obstructs stylistic features.
We are aware of only two prior works, both of which perform
their evaluation and experiments on controlled corpora that
are not noisy, such as the GCJ dataset and student homework
assignments [39], [9]. Our work signiﬁcantly overperforms
previous work by using different methods and in addition we
investigate noisy real-world datasets, an open-world setting,
effects of optimizations, and obfuscations.
[9] present an onion approach for binary code authorship
attribution. [39] identify authors of program binaries. Both
Alrabaee et al. and Rosenblum et al. utilize the GCJ corpus.
1Note that popular plagiarism-detection tools such as Moss are not based
on stylometry; rather they detect code that may have been copied, possibly
with modiﬁcations. This is an orthogonal problem [8].
3
Rosenblum et al. present two main machine learning tasks
based on programmer de-anonymization. One is based on
supervised classiﬁcation with a support vector machine to iden-
tify the authors of compiled code [18]. The second machine
learning approach they use is based on clustering to group
together programs written by the same programmers. They
incorporate a distance based similarity metric to differentiate
between features related to programmer style to increase clus-
tering accuracy. They use the Paradyn project’s Parse API for
parsing executable binaries to get the instruction sequences and
control ﬂow graphs whereas we use four different resources to
parse executable binaries to generate a richer representation.
Their dataset consists of submissions from GCJ and homework
assignments with skeleton code.
Malware attribution. While the analysis of malware is
a well developed ﬁeld, authorship attribution of malware has
received much less attention. Stylometry may have a role
in this application, and this is a ripe area for future work
that requires automated packer and encryption detection along
with binary segment and metadata analysis. The difﬁculty in
obtaining ground truth labels for malware samples has led
much work in this area to focus on clustering malware in
some fashion, and the wide range of obfuscation techniques in
common use have led many researchers to focus on dynamic
analysis rather than the static features we consider. The work
of [29] examines several static features intended to provide
credible links between executable malware binary produced by
the same authors, however many of these features are speciﬁc
to malware, such as command and control infrastructure and
data exﬁltration methods, and the authors note that many must
be extracted by hand. In dynamic analysis, the work of [35]
examines information obtained via both static and dynamic
analysis of malware samples to organize code samples into
lineages that indicate the order in which samples are derived
from each other. [11] convert detailed execution traces from
dynamic analysis into more general behavioral proﬁles, which
are then used to cluster malware into groups with related
functionality and activity. Supervised methods are used by [38]
to match new instances of malware with previously observed
families, again on the basis of dynamic analysis.
IV. APPROACH
Our ultimate goal is to automatically recognize program-
mers of compiled code. We approach this problem using
supervised machine learning, that is, we generate a classiﬁer
from training data of sample executable binaries with known
authors. The advantage of such learning-based methods over
techniques based on manually speciﬁed rules is that the ap-
proach is easily retargetable to any set of programmers for
which sample executable binaries exist. A drawback is that the
method is inoperable if samples are not available or too short
to represent authorial style. We study the amount of sample
data necessary for successful classiﬁcation in Section V.
Data representation is critical to the success of machine
learning. Accordingly, we design a feature set for executable
binary authorship attribution with the goal of faithfully repre-
senting properties of executable binaries relevant for program-
mer style. We obtain this feature set by augmenting lower-level
features extractable from disassemblers with additional string
and symbol information, and, most importantly, incorporating
higher-level syntactical features obtained from decompilers.
In summary, such an approach results in a method consist-
ing of the following four steps (see Figure 1) and the code is
available at https://github.com/calaylin/bda.
•
•
•
•
Disassembly. We begin by disassembling the program
to obtain features based on machine code instructions,
referenced strings, symbol information, and control
ﬂow graphs (Section IV-A).
Decompilation. We proceed to translate the program
into C-like pseudo code via decompilation. By sub-
sequently passing the code to a fuzzy parser for
C, we thus obtain abstract syntax trees from which
syntactical features and n-grams can be extracted
(Section IV-B).
Dimensionality reduction. With features from disas-
semblers and decompilers at hand, we select those
among them that are particularly useful for classi-
ﬁcation by employing a standard feature selection
technique based on information gain and correlation
based feature selection (Section IV-C).
Classiﬁcation. Finally, a random-forest classiﬁer is
trained on the corresponding feature vectors to yield
a program that can be used for automatic executable
binary authorship attribution (Section IV-D).
In the following sections, we describe these steps in greater
detail and provide background information on static code
analysis and machine learning where necessary.
A. Feature extraction via disassembly
As a ﬁrst step, we disassemble the executable binary to
extract low-level features that have been shown to be suitable
for authorship attribution in previous work. In particular,
we follow the basic example set by Rosenblum et al. and
extract raw instruction traces from the executable binary [39].
In addition to this, disassemblers commonly make symbol
information available, as well as strings referenced in the code,
both of which greatly simplify manual reverse engineering. We
augment the feature set accordingly. Finally, we can obtain
control ﬂow graphs of functions from disassemblers, providing
features based on program basic blocks. The required informa-
tion necessary to construct our feature set is obtained from the
following two disassemblers.
We use two disassemblers to generate two sets of instruc-
tions for each binary. We disassemble the binary with the
Netwide Disassembler (ndisasm) which is a widely available
x86 disassembler. We then use the open source radare2 disas-
sembler to get more detailed and higher level instructions than
ndisasm’s disassembly.
•
The netwide disassembler. We begin by exploring
whether simple instruction decoding alone can already
provide useful features for de-anonymization. To this
end, we process each executable binary using the
netwide disassembler (ndisasm), a rudimentary disas-
sembler that is capable of decoding instructions but is
unaware of the executable’s ﬁle format [44]. Due to
4
Fig. 1: Overview of our method. Instructions, symbols, and strings are extracted using disassemblers (1), abstract syntax tree
and control-ﬂow features are obtained from decompilers (2). Dimensionality reduction ﬁrst by information gain criteria and then
by correlation analysis is performed to obtain features that represent programmer style (3). Finally, a random forest classiﬁer is
trained to de-anonymize programmers (4).
•
this limitation, it resorts to simply decoding the exe-
cutable binary from start to end, skipping bytes when
invalid instructions are encountered. A problem with
this approach is that no distinction is made between
bytes that represent data versus bytes that represent
code. Nonetheless, we explore this simplistic approach
as these inaccuracies may not degrade a classiﬁer,
given the statistical nature of machine learning.
The radare2 disassembler. We proceed to apply
radare2 [33], a state-of-the-art open-source disas-
sembler based on the capstone disassembly frame-
work [37]. In contrast to ndisasm, radare2 under-
stands the executable binary format, allowing it to
process relocation and symbol
information in par-
ticular. This allows us to extract symbols from the
dynamic (.dynsym) as well as the static symbol table
(.symtab) where present, and any strings referenced
in the code. Our approach thus gains knowledge
over functions of dynamic libraries used in the code.
Finally, radare2 attempts to identify functions in code
and generates corresponding control ﬂow graphs.
Firstly, we strip the hexadecimal numbers from assembly
instructions and replace them with the uni-gram number, to
avoid overﬁtting that might be caused by unique hexadecimal
numbers. Then, information provided by the two disassemblers
is combined to obtain our disassembly feature set as follows:
we tokenize the instruction traces of both disassemblers and
extract token uni-grams, bi-grams, and tri-grams within a sin-
gle line of assembly, and 6-grams, which span two consecutive
lines of assembly. We cannot know exactly what each 6-
gram corresponds to in assembly code but for most assembly
instructions, a meaningful construct is longer than a line of
assembly code. In addition, we extract single basic blocks of
radare2’s control ﬂow graphs, as well as pairs of basic blocks
connected by control ﬂow.
B. Feature extraction via decompilation
Decompilers are the second source of information that we
consider for feature extraction in this work. In contrast to
disassemblers, decompilers do not only uncover the program’s
machine code instructions, but additionally reconstruct higher
level constructs in an attempt to translate an executable binary
into equivalent source code. In particular, decompilers can
reconstruct control structures such as different types of loops
and branching constructs. We make use of these syntactical
features of code as they have been shown to be valuable in
the context of source code authorship attribution [16]. For
decompilation, we employ the Hex-Rays decompiler [1].
Hex-Rays is a commercial state-of-the-art decompiler. It
converts executable programs into a human readable C-like
pseudo code to be read by human analysts. It is noteworthy
that this code is typically signiﬁcantly longer than the original
source code. For example, decompiling an executable binary
generated from 70 lines of source code with Hex-Rays pro-
duces on average 900 lines of decompiled code. We extract
two types of features from this pseudo code: lexical features,
and syntactical features. Lexical features are simply the word
unigrams, which capture the integer types used in a program,
names of library functions, and names of internal functions
when symbol information is available. Syntactical features are
obtained by passing the C-pseudo code to joern, a fuzzy parser
for C that is capable of producing fuzzy abstract syntax trees
(ASTs) from Hex-Rays pseudo code output [47]. We derive
syntactic features from the abstract syntax tree, which represent
the grammatical structure of the program. Such features are
(illustrated in Figure 2) AST node unigrams, labeled AST
edges, AST node term frequency inverse document frequency,
and AST node average depth. Previous work on source code
authorship attribution [16], [46] shows that these features are
highly effective in representing programming style.
5
We applied the ﬁrst dimensionality reduction step using
WEKA’s information gain attribute selection criterion [21],
which evaluates the difference between the entropy of the dis-
tribution of classes and the Shannon entropy of the conditional
distribution of classes given a particular feature [36].
The second dimensionality reduction step was based on
correlation based feature selection, which generates a feature-
class and feature-feature correlation matrix. The selection
method then evaluates the worth of a subset of attributes by
considering the individual predictive ability of each feature
along with the degree of redundancy between them [22]. Fea-
ture selection is performed iteratively with greedy hillclimbing
and backtracking ability by adding attributes that have the
highest correlation with the class to the list of selected features.
D. Classiﬁcation
We used random forests as our classiﬁer which are en-
semble learners built from collections of decision trees, where
each tree is trained on a subsample of the data obtained
by random sampling with replacement. Random forests by
nature are multi-class classiﬁers that avoid overﬁtting. To
reduce correlation between trees, features are also subsampled;
commonly (logM )+1 features are selected at random (without
replacement) out of M, and the best split on these (logM ) + 1
features is used to split the tree nodes.
The number of selected features represents one of the few
tuning parameters in random forests: increasing it increases
the correlation between trees in the forest which can harm
the accuracy of the overall ensemble, however increasing the
number of features that can be chosen between at each split
also increases the classiﬁcation accuracy of each individual
tree making them stronger classiﬁers with low error rates. The
optimal range of number of features can be found using the
out of bag error estimate, or the error estimate derived from
those samples not selected for training on a given tree.
During classiﬁcation, each test example is classiﬁed via
each of the trained decision trees by following the binary deci-
sions made at each node until a leaf is reached, and the results
are aggregated. The most populous class is selected as the
output of the forest for simple classiﬁcation, or classiﬁcations
can be ranked according to the number of trees that ‘voted’
for the label in question when performing relaxed attribution
for top-n classiﬁcation.
We employed random forests with 500 trees, which em-
pirically provided the best
tradeoff between accuracy and
processing time. Examination of out of bag error values across
multiple ﬁts suggested that (logM )+1 random features (where
M denotes the total number of features) at each split of the
decision trees was in fact optimal in all of the experiments
listed in Section V, and was used throughout. Node splits were
selected based on the information gain criteria, and all trees
were grown to the largest extent possible, without pruning.
The data was analyzed via k-fold cross-validation, where
the data was split
into training and test sets stratiﬁed by
author (ensuring that the number of code samples per author
in the training and test sets was identical across authors).
The parameter k varies according to datasets and is equal
to the number of instances present from each author. The
Fig. 2: Feature extraction via decompilation and fuzzy parsing: C-like
pseudo code produced by Hex-Rays is transformed into an abstract
syntax tree and control-ﬂow graph to obtain syntactic and control-ﬂow
features.
C. Dimensionality reduction
Feature extraction produces a large amount of features,
resulting in sparse feature vectors with thousands of elements.
However, not all features are equally informative to express
a programmer’s style. This makes it desirable to perform
feature selection to obtain a compact representation of the
data to reduce the computational burden during classiﬁcation
as well as the chances of overﬁtting. Moreover, sparse vectors
may result in a large number of zero-valued attributes being
selected during random forest’s random subsampling of the
attributes to select a best split. Reducing the dimensions of the
feature set is important for avoiding overﬁtting. One example
to overﬁtting would be a rare assembly instruction uniquely
identifying an author. For these reasons, we use information
gain criteria followed by correlation based feature selection
to identify the most informative attributes that represent each
author as a class. This reduces vector size and sparsity while
increasing accuracy and model training speed. For example, we
get 705,000 features from the 900 executable binary samples of
100 programmers. If we use all of these features in classiﬁca-
tion, the resulting de-anonymization accuracy is slightly above
30% because the random forest might be randomly selecting
features with values of zero in the sparse feature vectors.
Once information gain criteria is applied, we get less than
2,000 features and the correct classiﬁcation accuracy of 100
programmers increases from to 90%. Then, we identify locally
predictive features that are highly correlated with classes and
have low intercorrelation. After this second dimensionality re-
duction method, we are left with 53 predictive features and no
sparsity remains in the feature vectors. Extracting 53 features
or training a machine learning model where each instance has
53 attributes is computationally efﬁcient. Given such proper
representation of instances, the correct classiﬁcation accuracy
of 100 programmers reaches 96%.
6
int=v0f0<v0MAXfuncdeclifstmtpredcall...ifint=stmtfuncdeclpredfuncdecliffuncintdeclint=v0f0<v0C0funcdeclifstmtpredcall...Abstract syntax tree (AST)Syntactic featuresAST unigrams:ifint=stmtfuncdeclpred...AST bigrams:funcdecliffuncintdecl...AST depth: 5entryblk1blk2blk4blk3exitblk1blk2blk3blk4blk1blk2blk1blk3Control-ﬂow graph (CFG)Control-ﬂow featuresCFG unigrams:CFG bigrams:...entryblk1blk2blk4blk3exitblk1blk2blk3blk4...blk1blk2blk1blk3cross-validation procedure was repeated 10 times, each with a
different random seed, and average results across all iterations
are reported, ensuring that results are not biased by improbably
easy or difﬁcult to classify subsets.
We report our classiﬁcation results in terms of kappa
statistics, which is roughly equivalent to accuracy but subtracts
the random chance of correct classiﬁcation from the ﬁnal
accuracy. As programmer de-anonymization is a multi-class
classiﬁcation problem, an evaluation based on accuracy, or the
true positive rate, represents the correct classiﬁcation rate in