title:OSIRIS: Efficient and Consistent Recovery of Compartmentalized Operating
Systems
author:Koustubha Bhat and
Dirk Vogt and
Erik van der Kouwe and
Ben Gras and
Lionel Sambuc and
Andrew S. Tanenbaum and
Herbert Bos and
Cristiano Giuffrida
OSIRIS: Efﬁcient and Consistent Recovery of
Compartmentalized Operating Systems
Koustubha Bhat∗ Dirk Vogt∗ Erik van der Kouwe‡ Ben Gras† Lionel Sambuc†
Andrew S. Tanenbaum‡ Herbert Bos‡ Cristiano Giuffrida‡
Department of Computer Science
∗{k.bhat, d.vogt}@vu.nl, †{ben, lionel}@minix3.org, ‡{vdkouwe, ast, herbertb, giuffrida}@cs.vu.nl
Vrije Universiteit Amsterdam, The Netherlands
Abstract—Much research has gone into making operating
systems more amenable to recovery and more resilient to crashes.
Traditional solutions rely on partitioning the operating system
(OS) to contain the effects of crashes within compartments and
facilitate modular recovery. However, state dependencies among
the compartments hinder recovery that is globally consistent.
Such recovery typically requires expensive runtime dependency
tracking which results in high performance overhead, high
complexity and a large Reliable Computing Base (RCB).
We propose a lightweight strategy that limits recovery to
cases where we can statically and conservatively prove that
compartment recovery leads to a globally consistent state—
trading recoverable surface for a simpler and smaller RCB with
lower performance overhead and maintenance cost. We present
OSIRIS, a research OS design prototype that demonstrates
efﬁcient and consistent crash recovery. Our evaluation shows that
OSIRIS effectively recovers from important classes of real-world
software bugs with a modest RCB and low overheads.
I.
INTRODUCTION
All modern operating systems contain bugs [1], [2]. Studies
show that all software averages between 1 and 16 bugs per
1,000 lines of code [3], [4] even in tested and deployed soft-
ware, and dormant software faults persist even in mature code
bases [5]. With code bases that often easily exceed a million
lines of code, operating systems (OSes) contain thousands of
bugs at any time. Some of these bugs are in code that is beyond
the control of the operating system developers. For instance, a
considerable portion of the privileged code consists of code
in kernel extensions such as third-party drivers. A decade
ago, Chou et al. [6], [7] identiﬁed buggy kernel extensions
as an important cause of operating system crashes for both
Windows and Linux. Assuming that drivers are mostly stateless
and faults are mostly transient, we can handle such faults by
isolating the driver code and restarting it in case of a crash [8].
However, a more recent study on the Linux kernel [9]
shows that faults in stateful core OS subsystems have started to
outrank the buggy drivers in importance, even though the latter
are still large in number. Furthermore, these bugs are typically
not transient [10], [11]. In other words, relying on simple re-
execution is no longer a viable solution to recover from such
faults. Since faults in a core OS subsystem greatly reduce over-
all system dependability and existing techniques cannot handle
them, we conclude that software faults in the operating system
still rank among the greatest challenges to system dependabil-
ity. In this paper, we seek a method to mitigate their effects.
1
Reliability and stateful interaction: There are many ap-
proaches that improve fault tolerance in operating systems.
Typically, they protect either applications [12] or speciﬁc OS
subsystems from the effects of software faults in the operating
system [13], [14], [15], [16], but there also have been efforts
to provide whole-OS fault tolerance [17], [18], [19], [8].
Extending fault mitigation techniques to an entire OS is a
complex problem. Most of the solutions compartmentalize the
OS, primarily to prevent the effects of faults in one component
from spreading onto other components. Moreover, compart-
mentalization allows components to be recovered individually.
However, stateful runtime interactions among components
make per-component recovery nontrivial. In a scheme that en-
forces strict fault isolation between OS components, recovering
from crashes resembles a distributed systems recovery prob-
lem. In this context, the solutions are generally of three kinds:
1)
2)
3)
Global replication – which is least suitable for a
general-purpose operating system that aims for ju-
dicious and efﬁcient usage of system resources;
Dependency tracking – which does not scale for the
high-frequency inter-component interactions found in
an operating system setting;
Global checkpointing – which not only hinders nor-
mal execution performance, but also degrades it expo-
nentially when we scale to a larger number of active
system components.
Global
checkpointing in the
checkpointing
solutions. Local,
context of operating
systems [20] offers strong global consistency guarantees,
but suffers from the need to synchronize all components,
introducing bottlenecks which greatly affect overall system
performance. Since OS components interact all
the time,
checkpoints should be taken at high frequencies—typically
orders of magnitude beyond what is possible with today’s
global
per-component
checkpointing allows for more concurrency in the system but
dependent components still have to coordinate to ensure that
their locally checkpointed state remains consistent with that
of their counterparts. In case of uncoordinated checkpoints,
there is the risk of a domino effect [21], where crash recovery
leads to unbounded rollback of inter-dependent components.
In general, expensive runtime dependency tracking is the price
that local checkpointing schemes pay towards guaranteeing
globally consistent recovery.
Safe recovery windows:
In this paper, we introduce a
new design called OSIRIS (Operating System with Integrated
Recovery preventing Inconsistent State), which seeks to strike
a new balance between performance and globally consistent
recoverability of an operating system. Building on efﬁcient in-
memory checkpointing, OSIRIS recovers only in cases where
we can conservatively infer that performing local recovery
will not lead to global state inconsistencies. This eliminates
the need for dependency tracking and synchronization, also
greatly simplifying the recovery mechanism. Such solution
offers better performance than any other existing scheme, at
the cost of not being able to recover in every case.
Our solution is the ﬁrst
to achieve globally consistent
recovery of stateful core OS components with very low perfor-
mance overhead. With a runtime performance penalty of just
5.4% (on a microkernel-based OS baseline), OSIRIS brings
OS recoverability (limited, but still powerful) within the reach
of production systems. Moreover, unlike many existing OS
recovery approaches, we do not limit ourselves to drivers and
explicitly target the core system services. Given their heavily
stateful nature, it is much harder to recover from faults in
such components than from faults in drivers. For instance,
a system call like exec involves the ﬁle system, memory
manager, cache manager, process manager, etc. Crash recovery
in any of these components must keep its state consistent with
other components’ state and resume execution in a globally
consistent way.
Our approach uses knowledge about the nature of inter-
component interactions in the system to perform recovery only
within dependency-safe recovery windows—intervals during
which state changes within a component have not affected
other components. We use a lightweight in-memory check-
pointing system [22] to allow efﬁcient high-frequency creation
of per-component checkpoints. We further optimize this ap-
proach by disabling our runtime (memory logging) instrumen-
tation whenever recovery to a consistent state is known to be
impossible. Our design results in lower runtime overhead and
provides a ﬁne-grained trade-off between recoverable surface
and performance. Finally, instead of replaying the execution
after recovering from a crash, we send an error to the compo-
nent that sent the request that triggered the crash. This allows
us to deal with persistent faults in addition to transient faults.
Contributions: First, we describe an operating system
recovery method that determines whether rolling back only
component-local state can restore the OS to a globally consis-
tent state.
Second, we show how OSIRIS occupies a meaningful new
point in the design space of dependable operating systems
by introducing a new trade-off that solves the performance,
maintenance, and complexity drawbacks of existing solutions
at the cost of reduced recovery surface. With a performance
overhead of just 5.4%, we show that we can achieve an average
recovery surface of 68.4% in critical OS components with the
total size of Reliable Computing Base (RCB) constituting only
12.5% of the code base.
Third, we explain how our approach deals with persistent
software faults in core system services whereas most prior
efforts are limited to transient faults.
Fourth, we minimize checkpointing instrumentation over-
head by disabling memory logging when we cannot recover,
providing a ﬁne-grained trade-off between performance and
recovery surface.
Fifth, we show that our prototype implementation performs
meaningful recovery from real-world faults by means of large-
scale fault injection experiments. We compare our results with
a baseline recovery strategy and demonstrate a signiﬁcant
improvement. We also show that the performance and memory
overheads are within practical limits.
Roadmap: The remainder of this paper is laid out as
follows. We provide a background on recovery techniques and
the challenges of stateful recovery in Section II. We provide
an overview of OSIRIS in Section III and detail its design
elements in Section IV. We provide implementation details in
Section V. We evaluate our prototype in Section VI and discuss
the limitations and future work in Section VII. We survey
related work in Section VIII and conclude in Section IX.
II. BACKGROUND AND PROBLEM STATEMENT
Software recoverability has been studied in various
contexts. They range from preventing loss of unsaved
application context due
to operating system crashes,
insulating operating systems from faulty drivers and hardware
peripherals, and improving availability of server applications
to dealing with fatal faults in high-performance computing and
large-scale distributed systems. Recovery solutions typically
consist of the following elements: fault isolation, a way to
statefully restart components, and dependency monitoring. We
will discuss each in turn. Afterwards, we pose our problem
statement and the fault model that we assume in this work.
A. Fault isolation
Modular software design, along with enforcing component
boundaries allows a system to be compartmentalized, reducing
the scope of recovery to just the affected compartments. Many
fault-isolation techniques exist. Software-only techniques use
static analysis [15], [23], or dynamic object tracking [24],
while hardware-assisted methods may build on hypervisor-
supported service domains [17] and virtual-memory-based
isolation [18], [25]. In addition, there are solutions that su-
perimpose higher-level semantics over static program analysis
to isolate fault regions [19], [26], [27]. Regardless of the tech-
niques applied, compartmentalization of software into smaller
fault domains represents a ﬁrst step towards achieving practical
software recoverability.
B. Stateful restart
A crashed component needs revival
to resume normal
system execution. In the simplest of cases, components are
mostly stateless and can be revived by simply restarting
them—much like the device drivers in MINIX 3 [25].
However, in case of stateful components and inter-component
interactions, revival must ensure that the affected components
are restored to a sane state. Kadav et al. [15] repurpose
power management code in peripheral devices to take device
checkpoints in addition to saving device driver state. Similarly,
researchers have proposed runtime object tracking [14] and
request-oriented undo logging [22] to protect drivers and
even the Linux kernel [19]. In a distributed system setting,
Cruz [28] explored a coordinated checkpoint-restart scheme
using process and network state migration techniques.
2
C. Dependency tracking
E. Fault model
While fault isolation and stateful restart are mostly solved
problems by themselves, monitoring state dependencies is
not. We ﬁrst describe why it is necessary to monitor state
dependencies and then explain why current solutions are not
adequate for use in production environments.
If a request sent from component A to component B
triggers state changes in component B, it creates a dependency
between the states of the two components A and B. If one
is rolled back to the last checkpoint while the other retains
its state,
this may lead to two kinds of inconsistencies:
1) If component A crashes before component B completes,
state changes in component B may get orphaned because
interested in the results. Moreover, any
A is no longer
inter-component
interactions performed by B to fulﬁll
the
request also get orphaned recursively. 2) If component B
crashes while handling the request, component A may get no
reply or an incorrect reply and component A would not know
in which state component B ends up.
We must deal with both the cases in order to avoid inconsis-
tent crash recovery. Existing solutions track state dependencies
to solve these problems, but doing so complicates recovery
and results in increased software complexity and performance
overhead. For example,
the hierarchical recovery domains
in Akeso [19] incur slowdowns between 1.08x to 5.6x even
in a virtualized environment. The device driver tracking in
Nooks [24], which allows the system to recover from crashes
only in kernel extensions, incurs up to 60% runtime overhead
depending on the workload. Shadow drivers [14], which build
on top of Nooks, report an increase in CPU utilization by up
to 30% for a benchmark that performs a signiﬁcant number
of driver-kernel interactions. In addition, dependency tracking
complicates the recovery mechanism, possibly even to the
extent of making it one of the more complex elements in
the system. In contrast, the approach taken by ASSURE [26]
and REASSURE [27], which stops all the execution threads
in a checkpointing interval, is much simpler, but may not be
applicable in a broader context and is prone to deadlocks. We
conclude that runtime dependency tracking adds considerable
performance overhead and complexity to a system, making
operating system recovery impractical in production systems.
D. Problem statement and goals
As discussed, dependency tracking and recursive recovery
greatly increase complexity and cause substantial performance
overhead, making them unsuitable for production systems. In
this paper, we aim to show that, by adjusting expectations, it
is possible to achieve a practical recovery solution with much
less complexity and much better performance.
Our goals for consistent recovery in OSIRIS are twofold:
(1) ensure global state consistent recovery, (2) keep the
crash recovery infrastructure simple to minimize the risk of
introducing faults in the RCB. In the remainder of this paper,
we present our recovery solution, which takes into account the
possibility of system state being spread across several fault
domains and yet avoids performing complex and expensive
runtime dependency tracking.
In this section, we describe our fault model and discuss its
consequences.
Many previous efforts assume that faults are transient [25],
[29], [30], which means that simply restarting a component
and restoring its original state is sufﬁcient. Given the transient
nature of the fault, attempting the same operation again is
likely to succeed. While it is true that hardware faults are
often transient (e.g., bit ﬂips in memory) and software faults
may also be transient (e.g., a race condition that only occurs
for particular scheduling decisions), many common software
faults are persistent, where the fault can be a function of
the inputs and the existing state. In the case of persistent
faults, recovering a component and replaying the same inputs
will inevitably trigger the same fault again. Our fault model
considers both transient and persistent software faults.
Fail-stop faults cause the affected component
to crash
immediately, unlike fail-silent errors which corrupt its state
without an immediate crash. Fail-stop faults are common.
Typical examples of fail-stop errors are NULL-pointer deref-
erences and divisions by zero. Moreover, defensive coding
practices such as the use of assertions to verify invariants
transform many potential fail-silent faults into fail-stop faults.
In addition, compilers are increasingly equipped with options
that convert traditionally fail-silent faults into fail-stop faults.
Typical examples are bounds checking and the wide range of
sanitization and stack protector options in popular compilers
like gcc and clang. Finally, hung-component faults can be
detected, and thus transformed into fail-stop faults, by sending
regular heartbeat messages and killing them if they do not