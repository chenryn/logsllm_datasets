Yes
Yes
Score
Score
Score
Score
Decision
window is classiﬁed as malicious, the entire sequence is considered malicious. Thus, even cases such
as malicious payloads injected into goodware (e.g., using Metasploit), where only a small subset of
the sequence is malicious, would be detected.
We use one-hot encoding for each API call type in order to cope with the limitations of scikit-
learn’s implementation of decision trees and random forests, as mentioned in [10]. The output
of each classiﬁer is either the predicted class (whether the API call trace is malicious or benign)
or the conﬁdence score of the prediction (a value between 0 for a benign process to 1.0 for a
malicious process). Appendix B contains a description of the classiﬁers used in our study and their
hyperparameters.
In order to attack such a malware classiﬁer, we want to add API calls without changing the
malware functionality. Removing API calls without modifying the malware functionality requires
complex analysis of the malware code and thus cannot be scaled. Therefore, we use a no-op
mimicry attack [44], that is, we add API calls with no eﬀect or an irrelevant eﬀect on the malware
functionality. We use this method regardless of the perturbation used to generate the API call type,
either random or benign perturbations (shown below). Almost every API call can become a no-op
if provided with the right arguments, e.g., opening a non-existent ﬁle.
However, analyzing arguments would make our attack easier to detect, e.g., by considering only
successful API calls and ignoring failed API calls or by looking for irregularities in the arguments of
the API calls (e.g., invalid ﬁle handles, etc.). In order to address this issue, we use valid (non-null)
arguments with a no-op eﬀect, such as writing into a temporary ﬁle handle (instead of an invalid
ﬁle handle) or reading zero bytes from a valid ﬁle. This makes detecting the no-op API calls much
harder, since the API call runs correctly, with a return value indicative of success. It is extremely
challenging for the malware classiﬁer to diﬀerentiate between malware that is trying to read a non-
existent registry key as an added adversarial no-op and a benign application functionality, e.g.,
trying to ﬁnd a registry key containing information from previous runs and creating it if it doesn’t
Accepted as a conference paper at ACSAC 2020
Figure 1: The Proposed Attack’s Flow
exist (for instance, during the ﬁrst run of the application). This makes our problem-space attack
robust to preprocessing [38].
To conclude, we believe that any detector will suﬀer from a very high false positive rate and thus
will not be a practical solution for detecting our attacks. We don’t add API types which cannot be
no-opped and thus always aﬀect the malware functionality. In this study, we focus on the 314 API
call types monitored by Cuckoo Sandbox. Of those, only two API call types are not added because
they can’t be no-opped: ExitWindowsEx() and NtTerminateProcess(). Every other API call have
a no-op variant, generated by the abovementioned method.
3.2 The Proposed Black-Box Attack
Our proposed attack’s ﬂow is detailed in Figure 1. As mentioned before, our proposed attacks can
be characterized by the knowledge the attacker has, the method for selecting which new API call to
add (termed perturbation type), and the method to select the number of added API calls (termed
iteration method). As can be seen in Figure 1, these characteristics aﬀect the attack’s ﬂow. These
characteristics are described in the subsections below.
Iteration Method
3.2.1
In this subsection, we describe the method to select the number of added API calls by the attack.
In addition to the regular linear iteration method, we propose an eﬃcient logarithmic backtracking
transformation as a method for determining the number of API calls to add. This method starts
with a large ratio of perturbation (that is, a larger number of API calls are added to the original
sequence to fool the classiﬁer) which rapidly decreases as long as the sequence remains misclassiﬁed.
In order to handle the entire API call sequence, we use Algorithm 1. In this algorithm, the
attacker splits the malicious API call sequence xm into windows of n API calls (wm
), similar to
the division made by the malware classiﬁer, and modiﬁes each window in turn using Algorithm
will become the beginning of
3 (described below; line 8). The API calls “pushed out” from wm
j
j
Accepted as a conference paper at ACSAC 2020
).
j
n
j+1
, so no API is ignored. The adversarial window size n might be diﬀerent from the malware
wm
classiﬁer’s window size k, which is not known to the attacker. As shown in Section 4.2, this has a
negligible eﬀect on the attack performance. In the case of a benign perturbation, the benign API
call sequence xb is similarly split into windows of n API calls (wb
The adversarial API call sequence length l might be larger than n, the length of the sliding
window API call sequence that is used by the adversary. Therefore, the attack is performed sequen-
(cid:7) windows of n API calls (e.g., for l = 1000 and n=100 the malware classiﬁer would run
tially on(cid:6) l
on ten windows, with API call indices: 1..100, 101..200, ..., 900..1000). Note that the knowledge of
k (the window size of the malware classiﬁer) is not required, as shown in Section 4.2.
D is the vocabulary of available features. In this case, these features are all of the API call types
recorded by the malware classiﬁer, e.g., CreateFileW (). Note that D is not necessarily known to the
(cid:48)
(cid:48) , which might be a subset or superset of D. This knowledge of D
attacker. The attacker knows D
is a commonly accepted assumption about the attacker’s knowledge [29]. In fact, it is enough for the
attacker to know the feature type used by the target classiﬁer (API call types in this study), which
is public information that is usually published by classiﬁer implementers. With that knowledge,
(cid:48) , which is a superset
the attacker can cover all API call types (several thousands) to generate D
of D.
In our research, we observed that API call types in D(cid:48) − D are not monitored by the classiﬁer,
and thus adding them does not assist in creating adversarial examples; those API calls just add
API call overhead to the modiﬁed sequence and serve as wasted queries. API call types in D − D(cid:48),
unknown to the attacker, are not generated by the attack and therefore decreasing the adversarial
feature space and thus decreasing the possibilities for generating modiﬁed sequences that can evade
(cid:48) is a superset of D, the attack has higher overhead but remains as eﬀective.
detection. Thus, when D
An attacker might also reverse engineer the features from the malware classiﬁer program, as has
already done in real-world adversarial example generation for malware classiﬁer scenarios [3].
In order to decrease the number of malware classiﬁer queries (the number of calls to f (.)), we
can use logarithmic backtracking. This iteration method is similar to binary search. In this case,
we only query the malware classiﬁer in Algorithm 1 after modifying Mw API calls in Algorithm 2
(instead of querying the model after modifying a single API call in a linear iteration), which should
be a suﬃciently large perturbation to evade the malware classiﬁer. Then, we start reducing the
number of modiﬁed API calls by half before querying the malware classiﬁer (lines 13-14) until it
detects the sample again. Finally, we keep restoring half of the API calls we previously removed
before querying (line 22), until we achieve a perturbation that fools the malware classiﬁer with a
minimal number of additional API calls and malware classiﬁer queries.
In Algorithm 2, the attacker chooses the API calls to add and remove randomly. Note that we
do not remove the malware’s original API calls (only the no-op API calls that were previously added
by the adversary), in order to prevent harm to its functionality. Since we add or remove half of the
API calls each time, we perform O(log n) queries per adversarial sliding window if IterationM ethod
is logarithmic backtracking, instead of the O(n) queries that are performed if IterationM ethod is
linear iteration (where n is the size of the adversarial sliding window), making this attack query-
eﬃcient, as can be seen in Tables 3 and 4. While the proposed attack is designed for API call-based
classiﬁers, it can be used for any adversarial sequence generation.
The benign sequence xb is generated by a specially crafted GAN, which is described below.
Accepted as a conference paper at ACSAC 2020
ALGORITHM 1: Full Sequence Attack
1 Input: f - black-box model,
2 xm - malicious sequence to perturb, xb - benign sequence to mimic,
3 n - size of adversarial sliding window, D
4 Mw - maximum API modiﬁcations per window, P erturbT ype - benign or random perturbation,
5 IterationM ethod - logarithmic backtracking or linear iteration, AttackerKnowledge - decision or
- adversarial vocabulary,
(cid:48)
score-based.
6
7 for each sliding windows (wm
8
j , addedAP Is) =
j , wb
Algorithm2(f, wm
j , n, D
(wm
(cid:48)
j , wb
j ) of n API calls in (xm, xb), respectively:
, Mw, P erturbT ype, IterationM ethod, AttackerKnowledge)}
if IterationM ethod is logarithmic backtracking:
remainingAP Is = addedAP Is
while (f (wm
j ) = benign):
9
10
11
12
13
14
15
16
17
18
19
20
# Remove added API calls until evasion is lost:
Randomly split addedAP Is into two equally sized groups: remainingAP Is, deletedAP Is
remove deletedAP Is from wm
j
if f (wm
j + deletedAP Is − remainingAP Is # Remove remainingAP Is instead of
j ) = malicious :
j = wm
deletedAP Is from wm
j
wm
Switch between remainingAP Is and deletedAP Is
recoveredAP Is = deletedAP Is
while (f (wm
j ) = malicious):
# While there are still added API calls that were removed, add them back until evasion is
restored:
21
22
23 return (perturbed) xm
recoveredAP Is =Randomly pick half of the API calls remaining in deletedAP Is
Add recoveredAP Is to wm
j
Accepted as a conference paper at ACSAC 2020
3.2.2 Perturbation Type
In this subsection, we describe the methods to select what API calls to add by our attack. As an
alternative to choosing the API calls randomly from all available API calls (random perturbation),
we propose the benign perturbation method. In this method, instead of adding random API calls,
we add API calls selected from sequences generated by a generative adversarial network (GAN) that
has been trained to mimic real benign sequences. This concept is inspired by the modus operandi of
biological viruses (malware) which are sometimes composed of human (“benign”) proteins in order
to evade the immune system (malware classiﬁer) of the host.
When the attackers add an API call to our adversarial sequence, they want to have the maximum
impact on the classiﬁer’s score. Thus, Algorithm 1 can take xb, a benign API sequence, as input.
The idea is that adding a “benign” API call would make the trace “more benign” than adding a
random API call. This is due to the fact that no API call is malicious or benign per se. The context
and ﬂow of API calls determine the functionality (and therefore the code’s “maliciousness”). Using
benign perturbation creates a “benign API call context” and thus improves the attack eﬀectiveness
and also makes it more query-eﬃcient, because fewer queries are needed to fool the classiﬁer, as
can be seen in Table 4, which uses benign perturbations, as opposed to Table 3, which uses linear
iteration.
One way to generate xb is by taking the API call sequence of an actual benign sample from
our dataset. The downside of this approach is that those hard-coded API calls can be detected
explicitly as an evasion attack.
A better approach is to generate a diﬀerent benign sequence each time, using a generative model.
One way to do this is to use a generative adversarial network (GAN), with a stochastic input seed
and an output of an API call sequence that is indistinguishable (to the discriminator classiﬁer)
from actual benign sequences from the dataset. This approach is rarely used for API call sequence
generation, but it has been used for text generation. Note that this approach does not require
queries to the target classiﬁer (unlike, e.g., building a substitute model, as done in [42]).
In comparison to other approaches (e.g., VAE), using a GAN tends to generate better output.
Most other methods require that the generative model has some particular functional form (like a
Gaussian output layer). Moreover, all of the other approaches require that the generative model
puts non-zero mass everywhere.
However, a challenge with the GAN approach is that the discrete outputs from the generative
model make it diﬃcult to pass the gradient update from the discriminative model to the generative
model. Another challenge is that the discriminative model can only classify a complete input
sequence. We used SeqGAN [46] implementation. Following a pretraining procedure that follows
the MLE (maximum likelihood estimation) metric, the generator G is modeled as a stochastic policy
in reinforcement learning (RL). The agent’s state is the API call subsequence of the ﬁrst t API call
types in the sequence to be generated by the GAN, st = [x0, x1, ..xt−1]. The agent’s action is the
next API call type in the sequence to be generated by the SeqGAN model xt ∼ (x|st). The reward
is the feedback given to G by D when evaluating the generated sequence, bypassing the gradient
update challenge by directly performing a gradient policy update.
In a stochastic parameterized policy, the actions are drawn from a distribution that parame-
terizes the policy. An action may be sampled from, e.g., a normal distribution whose mean and
variance will be predicted by the policy. The objective of G is to generate a sequence from the
start state s0 in such a way that maximizes the expected end reward. This action-value function
is estimated by the discriminator. However, D only provides a reward at the end of a ﬁnished
sequence. Yet, it is important that at every time-step, the ﬁtness of both previous tokens as well
Accepted as a conference paper at ACSAC 2020
as future outcome are considered. For this, the policy gradient used in SeqGANs employs a Monte
Carlo search with a roll-out policy (P) to sample the unknown remaining tokens and approximates
the state-action value in an intermediate step.
We trained our GAN using a benign hold-out set of 3,000 sequences that were taken from
the same distribution as the test set, available to the attacker. This hold-out set was not used
subsequently as part of the test set, to avoid data leak to the attacker, artiﬁcially increasing the
attack eﬀectiveness.
We also tried other GAN architectures (e.g., GSGAN), but SeqGAN outperformed all of them
(Appendix C). SeqGAN outperformed the random perturbation as well, as shown in Section 4.2.
3.2.3 Attacker Knowledge
This characteristic entails a trade oﬀ: the more information the attackers have about the classiﬁer,
the more query-eﬃcient their attack would be.
In this paper, the attacker may have access to
the conﬁdence score of the malware classiﬁer (score-based attack), or only to the predicted label
(decision-based attack). For the ﬁrst case, we propose a score-based attack that uses a gradient-free
optimization algorithm that until now has never been used for adversarial learning, outperforming
state-of-the-art attacks for low number of queries to the attacked classiﬁer. This attack is designed
for discrete values in sequences of variable length. Thus, it ﬁts API call sequences, as opposed to
image pixels, which were the focus of most previous research.