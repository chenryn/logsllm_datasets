Since both models have similar decision boundaries, adversarial
examples crafted with the substitute model are likely transferable
to the black-box target model. Adversarial perturbations can also be
crafted using dierential evolution [55] or greedy local search [44].
When facing the trapdoor-enabled detection [51], commonly
used and state-of-the-art adversarial attacks like FGSM, PGD, C&W,
Elastic Net, BPDA, and SPSA are all ineective [51].
2.2 Defenses against Adversarial Attacks
Along with the development of dierent adversarial attacks, many
defense methods have also been developed. Early attempts to secure
neural networks against adversarial attacks include removing adver-
sarial perturbations with denoising autoencoders [22], increasing
local stability [49] or robustness of DNNs via robustness metrics [2]
or adversarial training [28, 41, 57, 64, 65], and defensive distilla-
tion [47] that uses the distillation technique [27] to retrain the
same network with category probabilities predicted by the original
network. These defenses fail or are signicantly weakened when
facing stronger adversarial attacks or high-condence adversarial
examples [7â€“9, 26].
Since most adversarial attacks, such as FGSM [20], PGD [33, 34],
C&W [10], and Elastic Net [11], rely on computing gradients in craft-
ing adversarial examples, many defenses [5, 15, 24, 40, 48, 53, 61]
apply gradient masking (i.e., reducing useful gradients) to dis-
rupt computation of gradients to thwart adversarial attacks. These
gradient-masking defenses are proved vulnerable to black-box at-
tacks [45, 57], gradient-approximation attacks like BPDA [1], and
adversarial attacks without using gradients like SPSA [58].
Defenses focusing on detecting adversarial examples at inference
time are also proposed. Many utilize statistical tests to dierentiate
adversarial examples from benign examples [19, 21, 38, 63]. These
defenses fail to detect more powerful adversarial attacks such as
C&W [10]. Magnet [42] uses one or more detection networks and
a reformer network. The detection networks learn to distinguish
between normal and adversarial examples, while the reformer net-
work is used to remove any remaining minor adversarial nature in
examples classied as benign by the detectors. However, Magnet is
found vulnerable to an attack that adversaries train their own copy
of the defense and generate transferable adversarial examples [9].
Latent Intrinsic Dimensionality (LID) [40] exploits the dierence of
a modelâ€™s internal dimensionality characteristics between normal
and adversarial examples to detect adversarial examples. LID is
found [1] to be vulnerable to high condence adversarial examples
of C&W. Neural-network Invariant Checking (NIC) [39] extends LID
by using one-class Support Vector Machines (SVM) to model the
benign distribution of latent activation within and across layers to
detect adversarial examples without using any prior knowledge of
adversarial attacks.
A recently proposed detection method, Trapdoor-enabled Detec-
tion (TeD) [51], deliberately injects one or more trapdoors into DNN
models through embedding defensive backdoors to trap and detect
adversarial examples. When an adversarial attack searches for mini-
mum perturbations to a target category protected by TeD in crafting
adversarial examples, it is likely trapped into a shortcut created by
the backdoored model, resulting in crafted adversarial examples
likely detected with signatures of backdoor samples extracted from
their latent representations. More details are provided in Section 3.
TeD can detect, with high accuracy, adversarial examples gener-
ated by state-of-the-art attacks such as PGD, C&W, Elastic Net, and
BPDA, with negligible impact on normal classication [51].
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3161While the above defenses are all best-eort approaches, an emerg-
ing approach called randomized smoothing is proposed [13, 36, 37]
to transform the original classier into a smoothed classier, which
is used to return the category with the highest probability by query-
ing isotropic Gaussian ğ‘ (ğ‘¥, ğœ2ğ¼) around an input ğ‘¥. Randomized
smoothing provides certiable robustness against adversarial exam-
ples within an ğ¿2 ball around any input ğ‘¥, a desirable property that
best-eort approaches lack. However, current randomized smooth-
ing methods may not be practical for many applications due to
their reduced accuracy [43] and signicant inference overhead [13].
Best-eort defenses are still needed before randomized smoothing
becomes more practical.
2.3 Related Work
2.3.1 Activation Aack and Its Variants. Our FIA is related to Ac-
tivation Attack (AA) [31], a transfer-based black-box targeted ad-
versarial attack. It uses a white-box model to generate adversarial
examples to attack a black-box model. To strengthen transferability
of adversarial examples, AA crafts adversarial examples by mini-
mizing the Euclidean distance to a target example at some latent
layer in the feature space. The target example is the one with the
largest Euclidean distance to the feature vector of the source exam-
ple of the current adversarial example among a small set of benign
examples randomly sampled from the target category.
The same team has developed several variants [29, 30] with
the same goal: strengthening transferability of adversarial exam-
ples. Feature Distribution Attack (FDA) [29] captures layer-wise
and category-wise feature distributions of the white-box model
with binary neural networks and generates adversarial examples
by maximizing the target category probability at a latent layer, op-
tionally minimizing the source category probability or maximizing
the distance of the perturbed features from the original features at
the same layer simultaneously. FDA is extended in [30] to multiple
layers by optimizing these layers simultaneously and also by adding
the cross-entropy loss function to optimize.
Our FIA diers from AA and its variants by pursuing a dierent
goal. We aim to generate adversarial examples indistinguishable
from benign examples of the target category while AA and its vari-
ants aim to strengthen transferability of adversarial examples from
the white-box model to the black-box model. The dierences in the
goal to pursue result in the following major dierences between our
FIA and AA and its variants. First, the target to drive adversarial
examples to is dierent. The target in AA is the example with the
furthest distance to the source example in the feature space among a
small set of benign examples randomly sampled from the target cat-
egory, while the target in FIA is the expectation of feature vectors of
benign examples in the target category to maximize the chance that
a generated adversarial example is indistinguishable from benign
examples in the target category. Second, our generated adversarial
examples are guaranteed, according to our indistinguishability met-
ric, to be within the distribution of and thus indistinguishable from
benign examples in the target category (otherwise the adversarial
example generation fails), while adversarial examples generated
by AA and its variants may not be indistinguishable from benign
examples of the target category since the focus is on strengthening
adversarial transferability. Third, FIA has an additional term to
optimize that AA and its variants lack: maximizing the distances to
positive examples in the feature space.
2.3.2 Existing Aacks on Trapdoor-enabled Detection. Carlini [6]
introduces two advanced attacks, Oracle Signature Attack (OSA) and
Trapdoor Vault Attack (TVA), that can partially break the strength-
ened trapdoor-enabled detection. Oracle Signature Attack [6, 51] is
a white-box attack for both the model and the trapdoored defense:
adversaries have the knowledge of precise values of the trapdoor
signature(s). It optimizes for both maximum cosine distance to the
known trapdoor signature and minimal cross-entropy to the target
category. It can evade the trapdoor-enabled detection with a success
rate nearly 90% on MNIST when randomized neuron sampling is
not used, but the attack success rate reduces to below 40% after 5%
of randomly sampled neurons and multiple trapdoors are used [51].
Trapdoor Vault Attack [6, 51] is a weaker attack than Oracle
Signature Attack. In this attack, adversaries know the basic setting
of the trapdoor-enabled detection, such as the number of trapdoors
used in the trapdoored defense, but have no knowledge of any
trapdoor signature. The attack estimates the trapdoor signature(s)
from adversarial examples generated with a conventional method
such as PGD. For the case that ğ‘ trapdoors are used in the trap-
doored defense, it uses a clustering approach to approximate neuron
signatures for each of the ğ‘ trapdoors. The attack then uses the
estimated trapdoor signature(s) with the same approach as Oracle
Signature Attack to generate adversarial examples. Its success rate
is lower than that of Oracle Signature Attack. When 5% of randomly
sampled neurons and multiple trapdoors are used, its success rate
is around 20% on MNIST [51].
Our FIA diers from both of Carliniâ€™s attacks in several ways.
First, FIA is a black-box attack on the trapdoored defense (but white-
box on the model): adversaries have no knowledge of the charac-
teristics of the the trapdoored defense (e.g., how many trapdoors
or which layerâ€™s signatures are used). FIA does not need in general
to know or estimate trapdoor signatures used in the trapdoored
defense. Second, the optimization goal is dierent. FIA minimizes
the distance to the expectation of benign examples in the target
category at some latent layer and ensures that the feature vector of
a generated adversary example at that latent layer is indistinguish-
able from those of benign samples in the target category, while
Carliniâ€™s attacks minimize the conventional adversarial loss (i.e.,
the cross entropy to the target category). Third, Carliniâ€™s attacks
maximize the distance to the known or estimated trapdoor signa-
tures (and thus adversaries need to know the detection layer and
the number of trapdoor signatures used in the trapdoored defense),
while FIA maximizes distances to positive adversarial examples it
queries the trapdoored defense at the preparation phase. According
to our ablation study presented in Section 6.7, maximizing distances
to positive adversarial examples plays a minor role in FIA. In many
cases, adversarial examples generated with the basic FIA scheme are
all negative (i.e. undetected) for TeD in the preparation phase, and
thus FIA actually uses the basic scheme (which does not have the
maximizing term) to generate adversarial examples (see Section 5.2
for details).
2.4 Notation
The notation used in this paper is summarized in Table 1.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3162Table 1: Notation.
Notation Denition
Fğœƒ
X, Y
Fğ¿(ğ‘¥)
ğ¶ğ‘¡
Dğ‘¡
ğœ–
ğ›¿
D(Â·)
E(Â·)
cos(ğ‘¥, ğ‘¦)
DNN model with parameters ğœƒ.
Input space and output space of model Fğœƒ .
Feature representation of ğ‘¥ âˆˆ X at layer ğ¿ of Fğœƒ .
Target category ğ‘¡
Feature representation distribution of category ğ‘¡
Adversarial perturbation
Pixel-wise bound on adversarial perturbations
Distance function
Expectation function
Cosine similarity between ğ‘¥ and ğ‘¦.
3 TRAPDOOR-ENABLED DETECTION
3.1 Backdoor Attacks
Trapdoor-enabled Detection (TeD) [51] exploits DNNâ€™s vulnerabil-
ity to backdoor attacks to train defensive backdoored DNN models
to detect adversarial examples. Backdoor attacks [23] are another
well-known vulnerability of DNNs. While adversarial attacks are
passive attacks that donâ€™t change the target model, backdoor attacks
are active attacks that require modication of the target model to
inject one or multiple backdoors into the target model. In a back-
door attack, the adversary selects a target category and a special
pattern, called backdoor trigger, and injects the backdoor into a
DNN model through poisoning training data. A backdoored model
behaves normally and has similar accuracy as a clean model when
the backdoor trigger is not applied. When the backdoor trigger
is applied on an arbitrary normal example of any category, the
backdoored model will misclassify it into the target category.
3.2 Trapdoor-enabled Detection (TeD)
TeD deliberately injects "trapdoors" into a DNN model through
embedding one or multiple defensive backdoors to trap and detect
adversarial examples. Since targeted adversarial attacks essentially
search for a minimum perturbation to the target category with an
optimization algorithm in crafting adversarial examples, generated
adversarial examples will be most likely trapped into a shortcut
created by the backdoor and thus can be detected with signatures
of backdoor samples extracted from their latent representations.
TeD selects a latent layer, called detection layer in this paper, to
detect adversarial examples. A detection layer should be a layer late
in the forward pipeline of a DNN model, usually the penultimate
layer (i.e., the last layer before the output softmax layer). After
training trapdoored model Fğœƒ with a trapdoor trigger Î” for target
category ğ¶ğ‘¡, TeD calculates trapdoor signature ğ‘†Î” as follows:
ğ‘†Î” = Eğ‘¥âˆ‰ğ¶ğ‘¡ Fğ¿(ğ‘¥ + Î”),
(1)
where E(Â·) is the expectation function, Fğ¿(ğ‘¥) is the feature repre-
sentation of input ğ‘¥ âˆˆ X at detection layer ğ¿ of model Fğœƒ . To build
this signature in practice, the model owner computes and records
neuron activation vectors of many inputs containing trigger Î”.
To determine if input ğ‘¥ âˆˆ X is an adversarial example or not, TeD
calculates the cosine similarity between the feature representation
of ğ‘¥ at detection layer ğ¿, Fğ¿(ğ‘¥), and trapdoor signature ğ‘†Î”. If the
similarity exceeds a preset threshold ğœ™ğ‘¡, input ğ‘¥ is determined as
adversarial (i.e., positive). Otherwise it is determined as normal (i.e.,
negative). Threshold ğœ™ğ‘¡ is chosen as the ğ‘˜-th percentile value of the
statistical distribution of cosine similarity between benign samples
and ğ‘†Î”, with 1 âˆ’ ğ‘˜
100 as the desired false positive rate.
To thwart the two attacks proposed by Carlini [6] to circumvent
TeD, TeD can be enhanced by injecting multiple trapdoors to protect
a single category ğ¶ğ‘¡. Each trapdoor is associated with a trapdoor
signature at the detection layer. In this case, TeD calculates the
cosine similarity of input ğ‘¥ with each of the trapdoor signatures. If
any one exceeds the preset threshold, it determines that the input is
adversarial. In addition, TeD can detect adversarial examples using
the activation of a subset of neurons, for example, 5% or 10% of
randomly sampled neurons at detection layer ğ¿.
On a subset of randomly sampled neurons, it is possible that the
distribution of benign samples in target category ğ¶ğ‘¡ and trapdoored
samples are not well separated, resulting in a high false positive
rate if the subset is used in detection. The authors of TeD havenâ€™t
provided details on selecting or using subsets of neurons in their pa-
per [51] or their released code [50]. In our experimental evaluation
(Section 6), we use the following way to select a subset of neurons
at the detection layer: given a number of neurons such as 5% to
select, we randomly sample neurons to form a subset of neurons.
Then we use the expectation of trapdoored samples on the subset,
say ğ‘†1
Î”, as the detection signature, calculate the cosine similarity
distributions of both trapdoored samples and benign samples in
target category ğ¶ğ‘¡ with ğ‘†1
Î”, and determine ğ‘˜Î”-th percentile value
ğ‘£ğ‘˜Î” of the trapdoored sample distribution and ğ‘˜ğ¶ğ‘¡ -th percentile
value ğ‘£ğ‘˜ğ¶ğ‘¡ of the benign sample distribution. In our evaluation,
we set ğ‘˜Î” = 25 and ğ‘˜ğ¶ğ‘¡ = 75. If ğ‘£ğ‘˜Î” âˆ’ ğ‘£ğ‘˜ğ¶ğ‘¡ is larger than a preset
threshold, the subset can be used; otherwise it is not used. We select
ğ‘ useful subsets in this way. The selected subsets are used in the
same way as in the case of multiple trapdoors: if any of the subsets
signals positive, the input sample is determined to be adversarial,
otherwise it is determined to be benign.
3.3 Projection-based TeD (P-TeD)