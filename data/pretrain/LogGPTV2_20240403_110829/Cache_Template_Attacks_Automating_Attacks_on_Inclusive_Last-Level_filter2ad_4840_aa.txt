title:Cache Template Attacks: Automating Attacks on Inclusive Last-Level
Caches
author:Daniel Gruss and
Raphael Spreitzer and
Stefan Mangard
Cache Template Attacks: Automating Attacks  
on Inclusive Last-Level Caches
Daniel Gruss, Raphael Spreitzer, and Stefan Mangard, Graz University of Technology
https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/gruss
This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-939133-11-3Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXCache Template Attacks:
Automating Attacks on Inclusive Last-Level Caches
Daniel Gruss, Raphael Spreitzer, and Stefan Mangard
Graz University of Technology, Austria
Abstract
Recent work on cache attacks has shown that CPU
caches represent a powerful source of information leak-
age. However, existing attacks require manual identiﬁ-
cation of vulnerabilities, i.e., data accesses or instruction
execution depending on secret information. In this pa-
per, we present Cache Template Attacks. This generic
attack technique allows us to proﬁle and exploit cache-
based information leakage of any program automatically,
without prior knowledge of speciﬁc software versions or
even speciﬁc system information. Cache Template At-
tacks can be executed online on a remote system without
any prior ofﬂine computations or measurements.
Cache Template Attacks consist of two phases. In the
proﬁling phase, we determine dependencies between the
processing of secret information, e.g., speciﬁc key inputs
or private keys of cryptographic primitives, and speciﬁc
cache accesses. In the exploitation phase, we derive the
secret values based on observed cache accesses. We il-
lustrate the power of the presented approach in several
attacks, but also in a useful application for developers.
Among the presented attacks is the application of Cache
Template Attacks to infer keystrokes and—even more
severe—the identiﬁcation of speciﬁc keys on Linux and
Windows user interfaces. More speciﬁcally, for lower-
case only passwords, we can reduce the entropy per char-
acter from log2(26) =4.7 to 1 .4 bits on Linux systems.
Furthermore, we perform an automated attack on the T-
table-based AES implementation of OpenSSL that is as
efﬁcient as state-of-the-art manual cache attacks.
1
Introduction
Cache-based side-channel attacks have gained increas-
ing attention among the scientiﬁc community. First, in
terms of ever improving attacks against cryptographic
implementations, both symmetric [4, 6, 16, 39, 41, 53] as
well as asymmetric cryptography [3, 7, 9, 54], and sec-
ond, in terms of developing countermeasures to prevent
these types of attacks [31, 34]. Recently, Yarom and
Falkner [55] proposed the Flush+Reload attack, which
has been successfully applied against cryptographic im-
plementations [3, 17, 22]. Besides the possibility of
attacking cryptographic implementations, Yarom and
Falkner pointed out that their attack might also be used
to attack other software as well, for instance, to collect
keystroke timing information. However, no clear indica-
tion is given on how to exploit such vulnerabilities with
their attack. A similar attack has already been suggested
in 2009 by Ristenpart et al. [44], who reported being
able to gather keystroke timing information by observ-
ing cache activities on an otherwise idle machine.
The limiting factor of all existing attacks is that sophis-
ticated knowledge about the attacked algorithm or soft-
ware is necessary, i.e., access to the source code or even
modiﬁcation of the source code [7] is required in order
to identify vulnerable memory accesses or the execution
of speciﬁc code fragments manually.
In this paper, we make use of the Flush+Reload at-
tack [55] and present the concept of Cache Template At-
tacks,1 a generic approach to exploit cache-based vul-
nerabilities in any program running on architectures with
shared inclusive last-level caches. Our attack exploits
four fundamental concepts of modern cache architectures
and operating systems.
1. Last-level caches are shared among all CPUs.
2. Last-level caches are inclusive, i.e., all data which
is cached within the L1 and L2 cache must also be
cached in the L3 cache. Thus, any modiﬁcation of
the L3 cache on one core immediately inﬂuences
the cache behavior of all other cores.
3. Cache lines are shared among different processes.
4. The operating system allows programs to map any
other program binary or library, i.e., code and static
data, into their own address space.
1The basic framework can be found at https://github.com/
IAIK/cache_template_attacks.
USENIX Association  
24th USENIX Security Symposium  897
Based on these observations, we demonstrate how to per-
form Cache Template Attacks on any program automat-
ically in order to determine memory addresses which
are accessed depending on secret information or speciﬁc
events. Thus, we are not only able to attack crypto-
graphic implementations, but also any other event, e.g.,
keyboard input, which might be of interest to an attacker.
We demonstrate how to use Cache Template Attacks
to derive keystroke information with a deviation of less
than 1 microsecond from the actual keystroke and an
accuracy of almost 100%. With our approach, we are
not only able to infer keystroke timing information, but
even to infer speciﬁc keys pressed on the keyboard, both
for GTK-based Linux user interfaces and Windows user
interfaces. Furthermore, all attacks to date require so-
phisticated knowledge of the attacked software and the
executable itself. In contrast, our technique can be ap-
plied to any executable in a generic way.
In order to
demonstrate this, we automatically attack the T-table-
based AES [10, 35] implementation of OpenSSL [37].
Besides demonstrating the power of Cache Template
Attacks to exploit cache-based vulnerabilities, we also
discuss how this generic concept supports developers in
detecting cache-based information leaks within their own
software, including third party libraries. Based on the in-
sights we gained during the development of the presented
concept, we also present possible countermeasures to
mitigate speciﬁc types of cache attacks.
Outline. The remaining paper is organized as follows.
In Section 2, we provide background information on
CPU caches, shared memory, and cache attacks in gen-
eral. We describe Cache Template Attacks in Section 3.
We illustrate the basic idea on an artiﬁcial example pro-
gram in Section 4 and demonstrate Cache Template At-
tacks against real-world applications in Section 5.
In
Section 6, we discuss countermeasures against cache at-
tacks in general. Finally, we conclude in Section 7.
2 Background and Related Work
In this section, we give a basic introduction to the con-
cept of CPU caches and shared memory. Furthermore,
we provide a basic introduction to cache attacks.
2.1 CPU Caches
The basic idea of CPU caches is to hide memory ac-
cesses to the slow physical memory by buffering fre-
quently used data in a small and fast memory. Today,
most architectures employ set-associative caches, mean-
ing that the cache is divided into multiple cache sets and
each cache set consists of several cache lines (also called
ways). An index is used to map speciﬁc memory loca-
tions to the sets of the cache memory.
We distinguish between virtually indexed and physi-
cally indexed caches, which derive the index from the
virtual or physical address, respectively. In general, vir-
tually indexed caches are considered to be faster than
physically indexed caches. However, the drawback of
virtually indexed caches is that different virtual addresses
mapping to the same physical address are cached in dif-
ferent cache lines. In order to uniquely identify a spe-
ciﬁc cache line within a cache set, so-called tags are
used. Again, caches can be virtually tagged or physically
tagged. A virtual tag has the same drawback as a virtual
index. Physical tags, however, are less expensive than
physical indices as they can be computed simultaneously
with the virtual index.
In addition, there is a distinction between inclusive and
exclusive caches. On Intel systems, the L3 cache is an
inclusive cache, meaning that all data within the L1 and
L2 caches are also present within the L3 cache. Further-
more, the L3 cache is shared among all cores. Due to
the shared L3 cache, executing code or accessing data on
one core has immediate consequences for all other cores.
This is the basis for the Flush+Reload [55] attack as de-
scribed in Section 2.3.
Our test systems (Intel Core i5-2/3 CPUs) have
two 32 KB L1 caches—one for data and one for
instructions—per core, a uniﬁed L2 cache of 256 KB,
and a uniﬁed L3 cache of 3 MB (12 ways) shared among
all cores. The cache-line size is 64 bytes for all caches.
2.2 Shared Memory
Operating systems use shared memory to reduce memory
utilization. For instance, libraries used by several pro-
grams are shared among all processes using them. The
operating system loads the libraries into physical mem-
ory only once and maps the same physical pages into the
address space of each process.
The operating system employs shared memory in sev-
eral more cases. First, when forking a process, the mem-
ory is shared between the two processes. Only when
the data is modiﬁed, the corresponding memory regions
are copied. Second, a similar mechanism is used when
starting another instance of an already running program.
Third, it is also possible for user programs to request
shared memory using system calls like mmap.
The operating system tries to unify these three cate-
gories. On Linux, mapping a program ﬁle or a shared
library ﬁle as a read-only memory with mmap results
in sharing memory with all these programs, respec-
tively programs using the same shared library or pro-
gram binary. This is also possible on Windows using the
LoadLibrary function. Thus, even if a program is stat-
898  24th USENIX Security Symposium 
USENIX Association
2
ically linked, its memory is shared with other programs
which execute or map the same binary.
Another form of shared memory is content-based page
deduplication. The hypervisor or operating system scans
the physical memory for pages with identical content.
All mappings to identical pages are redirected to one
of the pages while the other pages are marked as free.
Thus, memory is shared between completely unrelated
processes and even between processes running in differ-
ent virtual machines. When the data is modiﬁed by one
process, memory is duplicated again. These examples
demonstrate that code as well as static data can be shared
among processes, even without their knowledge. Never-
theless, page deduplication can enhance system perfor-
mance and besides the application in cloud systems, it is
also relevant in smaller systems like smartphones.
User programs can retrieve information on their virtual
and physical memory using operating-system services
like /proc//maps on Linux or tools like vmmap
on Windows. The list of mappings typically includes all
loaded shared-object ﬁles and the program binary.
2.3 Cache Attacks
Cache attacks are a speciﬁc type of side-channel attacks
that exploit the effects of the cache memory on the execu-
tion time of algorithms. The ﬁrst theoretical attacks were
mentioned by Kocher [28] and Kelsey et al. [26]. Later
on, practical attacks for DES were proposed by Page [41]
as well as Tsunoo et al. [50].
In 2004, Bernstein [4]
proposed the ﬁrst time-driven cache attack against AES.
This attack has been investigated quite extensively [36].
A more ﬁne-grained attack has been proposed by Per-
cival [42], who suggested to measure the time to access
all ways of a cache set. As the access time correlates with
the number of occupied cache ways, an attacker can de-
termine the cache ways occupied by other processes. At
the same time, Osvik et al. [39] proposed two fundamen-
tal techniques that allow an attacker to determine which
speciﬁc cache sets have been accessed by a victim pro-
gram. The ﬁrst technique is Evict+Time, which consists
of three steps. First, the victim program is executed and
its execution time is measured. Afterwards, an attacker
evicts one speciﬁc cache set and ﬁnally measures the ex-
ecution time of the victim again. If the execution time
increased, the cache set was probably accessed during
the execution.
The second technique is Prime+Probe, which is sim-
ilar to Percival’s attack. During the Prime step, the at-
tacker occupies speciﬁc cache sets. After the victim pro-
gram has been scheduled, the Probe step is used to deter-
mine which cache sets are still occupied.
Later on, Gullasch et al. [16] proposed a signiﬁcantly
more powerful attack that exploits the fact that shared
memory is loaded into the same cache sets for differ-
ent processes. While Gullasch et al. attacked the L1
cache, Yarom and Falkner [55] presented an improve-
ment called Flush+Reload that targets the L3 cache.
Flush+Reload relies on the availability of shared mem-
ory and especially shared libraries between the attacker
and the victim program. An attacker constantly ﬂushes
a cache line using the clflush instruction on an ad-
dress within the shared memory. After the victim has
been scheduled, the attacker measures the time it takes
to reaccess the same address again. The measured time
reveals whether the data has been loaded into the cache
by reaccessing it or whether the victim program loaded
the data into the cache before reaccessing. This allows
the attacker to determine the memory accesses of the vic-
tim process. As the L3 cache is shared among all cores,
it is not necessary to constantly interrupt the victim pro-
cess. Instead, both processes run on different cores while
still working on the same L3 cache. Furthermore, the
L3 cache is a uniﬁed inclusive cache and, thus, even al-
lows to determine when a certain instruction is executed.
Because of the size of the L3 cache, there are signiﬁ-
cantly fewer false negative cache-hit detections caused
by evictions. Even though false positive cache-hit detec-
tions (as in Prime+Probe) are not possible because of the
shared-memory-based approach, false positive cache hits
can still occur if data is loaded into the cache acciden-
tally (e.g., by the prefetcher). Nevertheless, applications
of Flush+Reload have been shown to be quite reliable
and powerful, for example, to detect speciﬁc versions of
cryptographic libraries [23], to revive supposedly ﬁxed
attacks (e.g., Lucky 13) [24] as well as to improve at-
tacks against T-table-based AES implementations [17].
As shared memory is not always available between
different virtual machines in the cloud, more recent cache
attacks use the Prime+Probe technique to perform cache
attacks across virtual machine borders. For example, Ira-
zoqui et al. [20] demonstrated a cross-VM attack on a
T-Table-based AES implementation and Liu et al. [32]
demonstrated a cross-VM attack on GnuPG. Both attacks