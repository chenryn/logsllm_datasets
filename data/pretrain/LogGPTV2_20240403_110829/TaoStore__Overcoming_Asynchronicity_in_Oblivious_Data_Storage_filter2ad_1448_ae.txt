another shared data structure that needs to be controlled. Since
it is a block level data storage, we apply read-write locks at
the block level. The control unit also uses block level read-
write locks to maintain concurrent operations on the request
map.
Our server implementation performs I/O operations directly
on the disk. TaoStore is an I/O intensive infrastructure, and
for higher performance it is important to minimize the I/O
overhead. Our implementation performs I/O operations at the
path level, i.e., reading or writing the buckets along the path
at once, rather than at the bucket level, which would require
separate I/O operations for each bucket. Performing I/O at
the bucket level requires more I/O scheduling and context-
switch overheads; therefore TaoStore avoids it. The server
responses are returned with callbacks which have signiﬁcant
performance advantages over thread pooling and scheduling.
TaoStore can cache the top levels of the tree and serve
directly from memory to eliminate a signiﬁcant amount of
I/O overhead in the untrusted cloud storage. In our implemen-
tation, caching is done using a dictionary data structure.
In real world deployment scenario, the trusted proxy and
the server communicate and exchange data over asynchronous
TCP sockets.
B. Experimental Setup
The ﬁrst set of experiments are conducted to analyze how
TaoStore performs as an oblivious cloud storage in the real
world. The trusted proxy runs on a machine on a university
network with i5-2320 3 GHZ CPU, Samsung 850 PRO SSD,
and 16 GB memory. The cloud storage server is deployed
to an i2.4xlarge Amazon EC2 instance. The average round-
trip latency from the trusted proxy to the storage server is
12 ms. The average downstream and upstream bandwidths are
approximately 11 MBytes/s10.
The second set of experiments are conducted to compare
TaoStore with ObliviStore. To be comparable with ObliviStore,
we use a conﬁguration which is similar to the ObliviStore
paper. The network communication between the trusted proxy
and the storage server is simulated with a 50 ms latency.
Although there are multiple clients and they query the trusted
proxy concurrently, the network latency between the clients
and the trusted proxy is assumed to be 0 ms. The trusted
proxy and the storage server run on the same machine -it is
9We stress that our algorithm presentation above does lock the whole tree –
this makes the proof slightly simpler, but the proof extends also to this higher
level of granularity.
10Measured using iPerf tool[2].
Fig. 6: Trusted Proxy Implementation
Fig. 7: Subtree Structure
based environment, which is similar to the setting in Oblivi-
Store paper.
A. Implementation
We implemented a prototype of TaoStore in C#. We start
by brieﬂy highlighting some technical aspects of our imple-
mentation.
The trusted proxy (see Figure 6) runs an implementation of
TaORAM as described in Section IV, which internally runs
many threads, where each is a processing unit responsible
for handling a client request and then returning a response
to the client. The request map is implemented as a dynamic
dictionary in the format of (bid, queue) pairs where block id,
bid, is a key in the map and each value is a queue object that
keeps track of the threads waiting for block bid. Additionally,
the control unit communicates with the threads and the ﬂush
controller to maintain the state of the system. The position map
is an array based data structure. The proxy also has a local
cache with 2 components: a subtree and a stash. The subtree is
implemented as a dynamic data structure that takes advantage
of a dictionary and a tree structure as shown in Figure 7.
For faster lookup, the dictionary component maintains the
information for mapping the blocks to buckets. If a block is
stored in the subtree, the dictionary points to the bucket in
which the block is stored. The nodes themselves also use a
dictionary structure to store blocks. Maintaining this two-level
structure enables an O(1) lookup for stored blocks. The other
caching component, the stash, has a dictionary format of (bid,
block). To provide data conﬁdentiality, the data is encrypted
at the bucket level using a semantically secure randomized
encryption scheme, AES-128 [30] in CBC-mode, before it is
outsourced to the cloud storage.
The components of the local cache are implemented in
memory. When paths are fetched from the untrusted cloud
storage, concurrent fetches are likely to have overlapping
buckets, especially at the top levels of the tree. To avoid
locking the complete subtree (which would be very costly),
209209
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
the machine that is used as a trusted proxy in the initial set
of experiments.
In both set of experiments, each bucket is conﬁgured to have
four blocks of size 4 KB each. The default dataset sizes are
1 GB, i.e. 244,140 blocks and 13 GB, i.e. 3,173,828 blocks
for real world and simulation based experiments, respectively.
Additionally, the write-back threshold is set to k = 40 paths.
In our experiments, the clients issue concurrent read and
write requests. Three parameters may affect the performance
of the system: 1) the number of clients, 2) the scheduling
of client requests, and 3) the network bandwidth. For 2), we
consider an adaptive scheduling of requests, where each client
sends the next request immediately after receiving the answer
for the previous one. The requested blocks are selected from
a uniformly distributed workload and each set of experiments
uses the same workload11.
The main metrics to evaluate the performance are response
time and throughput. Response time spans the time period
from initiating a client request until the time that this client
receives a response. This metric shows how fast the system can
handle client requests. Throughput is deﬁned as the number of
(concurrent) requests that the system answers per unit time.
The goal is to achieve a low average response time while
ensuring high throughput. To report reliable results, each set
of experiments is run multiple times and the averages of the
gathered results are presented with a 95% conﬁdence interval.
Some intervals are not clearly seen in Figure 8 due to their
small sizes compared to the scale.
We also note that in order to calculate the experimental
results in the steady state, the system is warmed up before
taking any measurements. Warming up is achieved by the ﬁrst
10% of the workload.
C. Experimental Results
1) Cloud-based TaoStore Evaluation: In this section, we
vary different system parameters and study their effects on
the performance of TaoStore by deploying it to a real world
environment using AWS.
a) Effect of Concurrency: Figure 8(a) shows the effect of
concurrency on TaoStore’s average response time and through-
put while varying the number of concurrent clients from 1 to
15. The left and right vertical axes represent throughput and
response time, respectively.
With a single client, the response time is 55.68 ms, which
leads to a throughput of 17.95 op/s. As the number of
concurrent clients increases, the throughput also increases as
long as the system can support more simultaneous operations.
The system reaches its limit and stabilizes at a throughput
of approximately 40 ops/s when the number of concurrent
clients is 10. When the number of concurrent clients goes
above 10, the clients generate more requests than the system
can handle concurrently. In such a case, the clients experience
increasingly worse performance in terms of response time
11Please note that the distribution of requested blocks does not affect the
performance of TaoStore unlike ObliviStore.
although the performance of the system does not degrade in
terms of throughput. Consider the case when the number of
clients is 15. Although the system achieves approximately
the same throughput at around 40 ops/s, the response time
increases by 45% compared to the case with 10 concurrent
clients. We observe that the network bandwidth is the main
bottleneck in our experiments and it is the main reason for
the observed behavior. Each path request results in transfer-
ring approximately 260-270 KBytes of data from the storage
server to the proxy. Since the system handles 40 ops/s, the
bandwidth utilization of the system is approximately 10.4-10.8
MBytes/s. Recall that the downstream network bandwidth is
11 MBytes/s, the system utilizes almost all the bandwidth and
achieves its best throughput performance at around 40 ops/s.
To understand the system behavior with higher network
bandwidth, we perform an additional set of experiments by
running a proxy on another Amazon EC2 instance in the same
datacenter where the storage server is located. The proxy runs
on an m3.xlarge EC2 machine and we measure the bandwidth
between the server and the proxy to be 125.25 MBytes/s. In
this setting, the system achieves a throughput of 97.63 ops/s
with an average response time of 102 ms when the number of
clients is 10. The system performance increases dramatically
with the increase in network resources, 149% increase in the
throughput and 60% decrease in the response time.
As a result of our experiments we observe that higher
bandwidth can facilitate outstanding improvements in the
system performance. Therefore, the bandwidth is one of the
important
issues for oblivious cloud storage systems in a
realistic deployment setting as well as supporting concurrency
and asynchronicity.
Please note that
the default setting for the number of
concurrent clients is 10 in the rest of our experiments unless
otherwise stated.
b) Caching at the Cloud Storage: Caching the top levels
of the tree at the untrusted cloud storage eliminates a signif-
icant amount of the I/O overhead. Figure 8(b), 8(c) and 8(d)
present the effects of applying caching in terms of response
time, throughput, and path fetch time versus caching ratio. The
caching ratio represents the amount of data cached in the cloud
memory compared to complete dataset size. When there is no
caching, the requested buckets in the path are fetched in 6.12
ms from the disk. When the caching is applied, the cached
buckets are retrieved from the memory and the remaining
buckets are fetched directly from the disk. Caching 1.6% of
the dataset, approximately 16 MBytes, decreases path retrieval
time from 6.12 ms to 2.68 ms. As the caching ratio increases,
the time to fetch path decreases. When this ratio is 6.3%,
the path is fetched in 1.7 ms. However, 3-4 ms performance
improvement in data retrieval is not reﬂected in the overall
system performance in terms of response time and throughput
because of the network bandwidth limitations. As Figure 8(c)
and 8(d) show, the system provides similar throughput and
response time over varying caching ratios.
c) Impact of the Write-back Threshold: Recall that the
write-back threshold k determines the number of paths that are
210210
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
50
40
30
20
10
0
)
s
/
s
p
o
(
t
u
p
h
g
u
o
r
h
T
Throughput
Response Time
1
5
10
15
Number Of Concurrent Clients
400
300
200
100
0
)
s
m
i
(
e
m
T
e
s
n
o
p
s
e
R
)
s
m
i
(
e
m
T
h
c
t
e
F
h
t
a
P
7
6
5
4
3
2
1
0
0.000
0.016
0.031
0.063
Storage Cache/Dataset Size Ratio 
)
s
m
i
(
e
m
T
e
s
n
o
p
s
e
R
300
250
200
150
100
50
0
0.000
0.016
0.031
0.063
Storage Cache/Dataset Size Ratio 
(a) Effect of Number of Concurrent Clients
(b) Effect of caching at
the untrusted
server on average path fetch time from
disk
(c) Effect of caching at the untrusted server
on response time