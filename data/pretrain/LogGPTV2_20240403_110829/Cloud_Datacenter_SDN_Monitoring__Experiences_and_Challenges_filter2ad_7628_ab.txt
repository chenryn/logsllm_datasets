IMC ’18, October 31-November 2, 2018, Boston, MA, USA
that they cannot interfere with monitoring infrastructure. To (phys-
ical) switches, probes are indistinguishable from VM-generated
traffic. VNET/VM churn is accounted for by post-processing col-
lected ping data with separate systems that track VM and VM-NIC
liveness. We are also investigating VSwitch mechanisms that can
disambiguate between failed and administratively disabled VMs.
Certain advantages apply over physical Pingmesh. Since VNETs
contain less nodes than the physical network, we maintain full-
mesh CA⇒CA VNET ping statistics. While physical Pingmesh
measures latency from userspace, VNET Pingmesh measures la-
tency from the kernel. Thus, VNET Pingmesh probes that hit the
mapping lookup cache are not subject to context switching and
scheduler variation, and can more accurately measure latency than
physical Pingmesh.
4 FAULT DETECTION
VNET Pingmesh indicates good overall performance, under nor-
mal circumstances; ≥ 94% of Azure VNETs meet or exceed latency
requirements ≥99.999% of the time when considering per-VM 5-
minute averages, and Azure achieves just under five-9’s connectiv-
ity across every VNET in a typical hour. Despite favorable high-
level metrics, large network scale makes faults inevitable, and so we
must be certain of monitoring accuracy. Thus, we examine VNET
Pingmesh’s fault detection effectiveness, focusing specifically on
precision and recall.
4.1 Fundamental blind spots impact recall
Like physical-layer Pingmesh, VNET Pingmesh has had a good track
record detecting customer-impacting network anomalies within
its purview. For example, in March 2017, a datacenter incident
prompted various clients to raise support requests complaining
about high VM-to-storage latency. While storage was investigated
initially, a correlated VNET Pingmesh latency spike suggested a
network cause, despite a lack of physical layer and VM-level mon-
itoring alarms. Later correlation with physical Pingmesh along
with address resolution failures during the affected time-period
confirmed a network fault, where the impact magnitude flew under
physical-layer monitoring detection thresholds. Investigations re-
vealed a congestion-causing linecard misconfiguration root cause.
Another incident involved customer VM connectivity issues, con-
firmed through a transient but significant VNET Pingmesh connec-
tivity drop. Correlation with other monitoring systems identified a
ToR reboot root cause.
Despite these successes, we cannot fully quantify recall since
VNET Pingmesh fundamentally includes coverage blind spots. First,
our system cannot fully monitor hybrid client networks contain-
ing VMs within Azure as well as client servers outside of Azure,
which necessarily cannot run VNET Pingmesh infrastructure. A
side effect of the inability to monitor the entire network is that
VNET Pingmesh cannot monitor routing tunnels (e.g. middleboxes).
Thus, rather than ascertaining the specific fate of customer traffic
by mimicking their routing behaviour exactly, we instead are lim-
ited to measuring the health of the virtualization infrastructure in
a point-to-point manner only. Second, while active probes can pro-
vide indications of queueing or widely-impacting network losses,
they can miss ‘gray faults’ that only impact unpredictable subsets of
traffic [3, 9, 12, 18]. For such cases, supplementing VNET Pingmesh
with passive monitoring techniques [6, 12] may prove useful.
4.2 Cross-layer aliasing impacts precision
In its early days, VNET Pingmesh frustrated operators due to wasted
effort diagnosing alerts with no customer impact, potentially mask-
ing actual performance issues. This reduction in monitoring preci-
sion stemmed from difficulty interpreting collected data. We discuss
these cases and how we changed our usage of VNET pingmesh to
successfully account for them.
Effectively interpreting Pingmesh statistics can convey signifi-
cant insight into network health. Physical latency can be explained
by locality (inter-pod ≥ intra-pod), queuing delay (O(µ-seconds))
and packet loss (O(milliseconds)). Deviations from baseline per-
formance may reveal faults [2, 9, 12]; p99 latency ≥ TCP RTO
could indicate intermittent silent packet loss, while p50 latency of
O(milliseconds) could indicate persistent switch queues [9].
VNET Pingmesh also tracks latency and loss. However, we dis-
covered that different layers of the Azure stack impacted our mea-
surements in ways that were hard to tease apart. We call these inter-
actions cross-layer aliasing, where different actors (either in Azure,
or the tenants themselves) perform (possibly non-network) actions
that impact networking metrics and complicate their interpretation.
Until aliasing is accounted for, we can receive confusing outcomes
that both complicate determining if a problematic measurement
indicates an actual customer issue (and not a false positive) and
risk masking actual issues. As a case study, we consider latency.
Figure 1 depicts VNET latency in a cluster over 1 hour, projected
to physical ToR. Cells depict source (x-axis) to destination (y-axis)
ToR latency, measured as the average for all VMs in all VNETs in
all servers in the source. We see that:
(1) Latency ranges from O(100s-1000) microseconds, higher than
expected for datacenters. Intra-rack latency is ≥ 2.5× inter-
rack latency on average.
(2) Some racks (darkly-colored intersecting horizontal/vertical
stripes) show ≥ 1.5× median rack latency. Conversely, others
exhibit ping latency averages an order-of-magnitude smaller
than other servers.
(pings generated by the server) ≥ 2× inbound.
(3) Some servers (omitted for space) exhibit outbound latency
While higher VNET latency can correlate with physical-layer
faults, queueing or locality, diagnosis revealed that VNET architec-
ture and large (≥ 30 VMs) tenant traffic patterns were responsible—
specifically, kernel CA ⇒ PA mapping cache misses in large VNETs.
Since VNET Pingmesh iterates through CAs consecutively, large
VNETs with sparse traffic matrices may lead to cache misses for a
given ping, unless the VM was actively communicating with the
pinged VM (upon which measured latency would drop to the ex-
pected O(10s of microseconds) latency). High average latencies can
thus be explained by large VNETs—as a large fraction of measured
latencies are due to large VNETs, their performance dominates.
High intra-rack latency can also be explained by large VNETs; since
VM placement within a cluster is random, pinging a VM in the
same rack is more likely for large VNETs. Figure 2 reveals that large
VNET latency is higher, cluster-dependent, and noisier. Latencies
range from 100s of microseconds to a millisecond depending on
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
Roy et al.
cluster, with a deviation on par with the total latency for small
VNETs. Locality and latency trends vanish—instead, jitter from
mapping lookup context switches dominates.
Cold (dark green) stripes in the latency heatmap correspond
with either servers handling VMs within small VNETs exclusively
(if the VNET size is small enough, mappings will always be within
the cache due to VNET Pingmesh) or VMs in large VNETs that
constantly communicate with other VMs (keeping the mapping
cache hot). In other words, the tenant traffic pattern influenced the
measured latency as well, by driving it down for certain VM pairs
while mapping latency drove it up for other pairs.
In the initial VNET Pingmesh implementation, these two compet-
ing aliasing effects—neither signifying physical or VNET dataplane
latency—caused VNET Pingmesh to report high server-average
latencies that were heavily influenced by mapping latencies for
non-communicating VM pairs. While mapping latency is impor-
tant, especially for large-scale sparse/intermittent traffic [11], it
must be tracked separately from fast-path dataplane latency. By
overloading our latency signal with both modes of operation, we
risk both masking actual poor network performance (O(100s of
microseconds) congestion-based latency masked by millisecond-
plus mapping latencies) and yield false alerts on high latency that
do not correspond with tenant impact (since they are triggered by
non-communicating VM pairs).
Once the issue was identified, fixing monitoring was trivial—we
send multiple consecutive pings. The initial ping is saved as a (new)
mapping latency metric, while the average of the remainder is saved
as the (intended) dataplane latency metric. After accounting for
aliasing, VNET Pingmesh behaviour comports with physical, as
seen in the generally ≤ 100 microsecond and locality-correlated
small-VNET dataplane latency measurement in Figure 2.
Some takeaway lessons apply. First, cross-layer aliasing can re-
duce both monitoring precision (e.g. false-positive latency alarms)
and recall (e.g. congestion masked by mapping latency). Interpreta-
tion suffers when aliasing causes measurement cross-contamination
from non-network causes (we later see how server-based SDN vir-
tualization can cause such cross-contamination). Second, while
accounting for aliasing is easy post-diagnosis, diagnosis itself can
be complicated by cross-layer aliasing, as discussed next.
5 FAULT MANAGEMENT
Detected faults require management: triage (is the network or the
virtual overlay faulty?), diagnosis (e.g. is the network experiencing
packet loss?), root-causing (e.g. why is there packet loss?) and
mitigation (e.g. can I take another path?). Here, we show how VNET
Pingmesh can aid triage when used with cross-layer monitoring;
however, its effectiveness in aiding both diagnosis and root-causing
is impacted by cross-layer aliasing.
5.1 Cross-layer monitoring aids triage
At a high level, we seek to determine whether we can use VNET
Pingmesh and other (different layer) monitors in conjunction to
appropriately triage faults: Is a given fault due to the underlying
physical network, or is it a VNET specific ailment? We focus on the
Figure 1: Rack-level latency heatmap (microseconds)
Category
Table 1: Probability that physical layer Pingmesh ≥ 1.5×
cluster average if VNET latency ≥ 2× average for a VM.
% incidents % physical
anomalous
100
40.90
23.50
59.00
55.00
22.60
79.00
18.40
2.20
92.60
Total
Large VNETs only
Small VNETs only
Large and small VNETs
Server->ToR packet loss
minority of servers with higher than usual VNET latency, and the re-
lationship between concurrent VNET and physical-layer Pingmesh
measurements for these servers.
Over a randomly picked hour with no active network alerts,
we examined servers with ≥ 1 VM with average outbound ping
latencies of ≥ 2× their VNET average. Table 1 categorizes servers
depending on if their poor-latency VMs belong to large VNETs only,
small VNETs only, or a mix of large and small VNETs; specifically, it
depicts the probability that physical Pingmesh also reports latency
anomalies in each case. We call physical latency anomalous if the
average server ping latency is ≥ 1.5x the cluster average.
Large and small VNETs are concurrently impacted ≈20% of the
time; physical Pingmesh is also impacted in ≈80% of these cases.
Here, it seems reasonable to suspect a physical networking issue
impacting all monitoring layers at or above it. Conversely, ≈60%
of high latency indications impact large VNETs only; physical
Pingmesh is only impacted 23.5% of the time here. These statis-
tics are from the original VNET Pingmesh mechanism which tracks
mapping latency for large VNETs; thus, one may surmise that the
≈77% of servers with VNET-only latency increases may be subject
to CPU or IO contention slowing down map operations.
However, confounding factors remain, due to the generally
disjoint network coverage provided by VNET and physical-layer
020406080100120Source/24020406080100120Destination/24102103104Cloud Datacenter SDN Monitoring
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
Figure 2: VNET latency vs. locality