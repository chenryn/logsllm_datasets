transferability of poisoning availability attacks.
Backdoor Attacks. While red-herring attacks [48] can be consid-
ered as precursors to backdoor attacks, Gu et al. [23] is generally
regarded as the first backdoor attack against modern neural net-
works. It identified a security issue with ML-as-a-service models,
and involved generating poisoned data with a backdoor pattern to
influence the model to classify incorrectly new backdoored test-
ing points. Successive work introduce clean-label backdoor attacks
which assume that the adversary does not control the labeling func-
tion [71]. Other applications of machine learning, such as Federated
Learning models, have been shown to be vulnerable to backdoor
attacks [1]. To defend against backdoor attacks, [70] use SVD de-
composition on the latent space learned by the network to develop
an outlier score. [42] combines pruning and fine-tuning the network.
[74] identify poisoning by measuring the minimum perturbation
necessary to transform inputs into a target class.
Targeted Attacks. Shafahi et al. [60] introduce a clean-label, opti-
mization-based targeted poisoning attack. Suciu et al. [67] study
the transferability of targeted attacks. Schuster et al. [59] show
targeted poisoning attacks on word embedding models used for
NLP tasks. Koh et al. [30] introduce an influence-based targeted
attack and show an example on targeted attacks affecting multiple
points at once. Koh et al. [32] evaluate the effectiveness of influence
functions for predicting how models change when multiple points
are removed from a dataset. Our work uses larger datasets and mod-
els, and constructs attacks, which add poisoned points to influence
predictions on the target subpopulation. Witchesâ€™ Brew [21] uses a
gradient matching loss, along with various optimization techniques,
to perform targeted attacks which require little information of the
learnerâ€™s setup. We show in Section 6 that our identification of
subpopulations can be used to make targeted attacks more efficient,
comparing to both [30] and [21].
Related Attacks - Table 1. We outline in Table 1 a comparison
with other attacks in prior work that are closer to the attacks we
present. We include targeted attacks, such as [1, 21], which are
capable of attacking multiple points. These types of attacks do not
generalize to new points, howeverâ€” they are only capable of harm-
ing the specific points the attack was generated for. The reflection
backdoor [43] adds natural image modifications induced by light
reflection. The composite backdoor [41] adds natural modifications,
such as certain faces, into a sample to induce misclassifications.
These types of backdoors, while still using natural images, require
test-time modifications to induce misclassification. Our attacks are
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3106the only ones which do not require test-time misclassifications,
generalize to unseen points, function in a variety of data modali-
ties, and do not require knowledge of the exact training set. Other
related attacks exist for specific settings. Kulynych et al. [36] pro-
pose â€œprotective optimization technologiesâ€, where users may craft
data to improve modelsâ€™ performance on specific groups; our work
focuses on malicious data, where there are additional concerns,
such as minimizing collateral damage, understanding defenses, and
designing the target subpopulation. In concurrent work, Nolans
et al. [64] and Chang et al. [10] demonstrate attacks on models
specifically designed to satisfy algorithmic fairness restrictions, a
consequence of our attacks we discuss in Section 5.5.
3 SUBPOPULATION ATTACKS
In this section, we introduce the threat model, main definitions,
and the principal features that characterize our newly proposed
subpopulation poisoning attack.
3.1 Threat Model
Similar to most forms of poisoning attack, the goal of the adver-
sary is to introduce a small quantity of contaminants in the data
used to train a machine learning classification model in order to
introduce a desired characteristic in the learned model parameters.
We consider a realistic adversary who does not have access to the
internal parameters of the victim model, and, similarly to availabil-
ity poisoning attacks, cannot modify any data point submitted to
the victim model at testing time. Moreover, the adversary is unable
to gain knowledge of the exact data points employed for training,
and can modify the training set only by adding new points. This
reflects the scenario in which the attacker can only disseminate the
poisoned points, which will then be gathered, together with a large
quantity of benign points, by the developers of the victim model to
create the training set. However, we allow the adversary to have
the computational power required to train a model comparable to
the victim one, and to have access to a separate auxiliary dataset
ğ·ğ‘ğ‘¢ğ‘¥, distinct from the training data ğ·, sampled from the same dis-
tribution. We also allow the adversary knowledge of the learnerâ€™s
loss function (we consider only the widely used cross entropy loss)
and architecture (an assumption we remove in Section B in the
Appendix). The adversary has no knowledge of the victim model
parameters, or the actual training data.
Many poisoning attacks in the literature employ a white-box
attack model, with full knowledge of the training set, model archi-
tecture and parameters [4, 29, 30, 76], and many transfer the attack
from another model [14, 21]. Here, we make the assumption that
the adversary is able to access an auxiliary dataset ğ·ğ‘ğ‘¢ğ‘¥, which
we believe is reasonable given the availability of public datasets,
and has been a common assumption for black-box attacks in prior
work [29, 50]. While this assumption could potentially be removed
with a good generative model, we leave the exploration of this
hypothesis to future work. Finally, we consider stealthiness and
practicality to be highly important for the attacker, and therefore,
we assume the adversary will be able to poison only a small number
of points in the training set.
3.2 Definition
We propose a new type of subpopulation attack, that is an interpo-
lation in the space of poisoning attacks between a targeted attack
(misclassifying a small fixed set of points) and an availability attack
(misclassifying as many points as possible). To define our attack,
we first provide a general, intuitive definition of a subpopulation:
Definition 3.1. A subpopulation of a data distribution is a restric-
tion of its input domain to a set of points which are close to each
other based on some distance function.
This definition can capture multiple types of subpopulations.
Individual fairness definitions from the algorithmic fairness liter-
ature correspond to subpopulations defined with â„“2 distance, but
may be unnatural for image data. As we will see, subpopulations
defined by â„“2 distance in a trained modelâ€™s representation space bet-
ter capture image similarity. In algorithmic fairness, group fairness
uses subpopulations defined by a distance function which is 0 for
members of the same subpopulation and 1 for members of different
subpopulations.
For a targeted subpopulation, the adversaryâ€™s goal is twofoldâ€”
impact the predictions on inputs coming from the subpopulation in
the data, but do not impact the performance of the model on points
outside this subpopulation. Crucially, this subpopulation consists of
natural data, and does not require modifying points to observe the
attack, as is the case for backdoor attacks. We allow the adversary to
pick a subpopulation by selecting a filter function, which partitions
the population into the subpopulation to impact and the remainder
of the data, whose performance should not change. Formally:
Definition 3.2. Subpopulation Poisoning Attacks. Fix some learn-
ing algorithm ğ´ and training dataset ğ· (not necessarily known
to the adversary). A subpopulation attack consists of a dataset of
contaminants ğ·ğ‘ and a filter function F : X â†’ {0, 1} for defining
subpopulations. ğ·ğ‘ is the poisoning set constructed to minimize
the collateral damage and maximize the target damage on the sub-
population of interest when appended to the training set:
Collat(F , ğ·ğ‘) = E(ğ‘¥,ğ‘¦)âˆ¼D[1(cid:0)ğ´(ğ· âˆª ğ·ğ‘)(ğ‘¥) â‰  ğ‘¦(cid:1) âˆ’
Target(F , ğ·ğ‘) = E(ğ‘¥,ğ‘¦)âˆ¼D[1(cid:0)ğ´(ğ· âˆª ğ·ğ‘)(ğ‘¥) â‰  ğ‘¦(cid:1) âˆ’
(1)
(2)
1 (ğ´(ğ·)(ğ‘¥) â‰  ğ‘¦) | F (ğ‘¥) = 0]
1 (ğ´(ğ·)(ğ‘¥) â‰  ğ‘¦) | F (ğ‘¥) = 1]
We will evaluate subpopulation attacks by reporting the collat-
eral damage (1) and the target damage (2) on the unseen test set
ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡. Crucially, we use ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ to ensure that the attack general-
izes to new points in the subpopulation, contrasting with prior
attacks which only target a pre-specified set of points. A successful
attack will have small collateral damage and large target damage,
using few poisoning points. Note that, under our definition, target
damage simply seeks to maximize the classification error on the
subpopulation, making it a class-untargeted attack [8]; the defini-
tion can be easily modified to capture a class-targeted attack, where
samples from the subpopulation satisfying the filter function should
be classified into a specific target class.
Subpopulation attacks are an interpolation between targeted
poisoning attacks and availability poisoning attacks. The extreme
case in which the filter selects a single point (or small set of points)
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3107corresponds to targeted poisoning attacks [60]. When the filter
function is defined to select the entire data domain, the attack
is an availability attack [29]. However, the most interesting sub-
population attacks, as we will demonstrate, use a relatively small
number of poisoning points to attack a subpopulation of interest,
while minimizing the collateral on other data. These subpopula-
tion attacks are stealthy and hard to detect, in comparison with
availability attacks, and have a potentially larger impact than a
targeted attack. The choice of filter function is as important to the
adversary as the selection of contaminants. There may be some
choices of filter function which result in subpopulations harder to
attack. For instance, a subpopulation closer to the decision bound-
ary, on which the model has low confidence, may be attacked easier
than a high-confidence subpopulation. In the next section, we will
discuss our framework for generating these subpopulation attacks -
decomposed into subpopulation selection and attack generation.
4 METHODOLOGY
There are two main components that contribute to a full-fledged
subpopulation attack, as shown in Figure 1: choosing a suitable
filter function to select subpopulations, and selecting an algorithm
to generate the poisoning data. In this section we introduce our
methodology to obtain a complete subpopulation attack and de-
scribe the components in our framework.
4.1 Subpopulation Selection
The adversary first needs to identify their target subpopulation.
This is the component that is typically avoided by existing attacks,
which instead focus on optimizing the attack data in order to best
achieve an arbitrarily chosen objective. However, the selection strat-
egy is important: in order to keep collateral small, one must select a
subpopulation which can be separated from the rest of the distribu-
tion. Otherwise, regions of the distribution which are misclassified
will include samples not satisfying the filter function. We propose
two subpopulation selection approaches, called FeatureMatch
and ClusterMatch.
FeatureMatch. This filter function aims at matching some
4.1.1
set of specific features of the data, that the adversary may be inter-
ested in targeting a priori. To use this filter, in addition to having
access to realistic data points ğ‘¥ğ‘– and labels ğ‘¦ğ‘–, the adversary must
have access to a set of annotations ğ‘ğ‘– on these points. These annota-
tions represent structure that is finer grained or separate from the
labels, such as race or age values for gender classification or color of
automobiles in CIFAR-10. The annotations can be created manually
by the adversary, or by selecting a subset of features in a tabular
dataset (as we will see with the UCI Adult dataset). FeatureMatch
(Algorithm 1) simply matches on the exact value of these annota-
tions to identify the subpopulations. This is related to the standard
definition for notions of group fairness in the algorithmic fairness
literature [6, 19, 25].
Some attacks from the literature have run attacks like
FeatureMatch. For example, Bagdasaryan et al. [1] attack CIFAR-
10 models to target manually selected green cars. However, the
attack considered in that work was not designed to generalize -
they target a specific set of images, but donâ€™t evaluate on a hold-
out set of green cars to see how generalizable the attack was. By
Algorithm 1 FeatureMatch Algorithm - leverage data annota-
tions
Input: ğ‘‹ - features, ğ´ğ‘›ğ‘› - manual subpopulation annotations,
ğ´ğ‘›ğ‘›ğ‘¡ğ‘ğ‘Ÿğ‘” - target subpopulation annotation
return F = ğœ† ğ‘¥, ğ‘¦, ğ‘ğ‘›ğ‘› : 1(ğ‘ğ‘›ğ‘› = ğ´ğ‘›ğ‘›ğ‘¡ğ‘ğ‘Ÿğ‘”)
Algorithm 2 ClusterMatch Algorithm - automatically identify
subpopulations
Input: ğ‘‹ âˆˆ ğ·ğ‘ğ‘¢ğ‘¥ - feature values; ğ‘˜cluster - number of clusters;
PreProcess - preprocessing function
centers = Cluster(PreProcess(ğ‘‹), ğ‘˜cluster)
target = PickCluster(centers)
return F = ğœ† ğ‘¥ : 1(ClosestCenter(PreProcess(ğ‘¥),
centers) == target)
contrast, we measure FeatureMatch on how well it manages to
attack test points from the subpopulation, ensuring that the attack
is targeting the subpopulation as a whole and not those specific
examples. Another example is the work of Schuster et al. [59], who
propose attacks which can, as an application, attack machine trans-
lation, compromising all translations containing a specific word.
This is a FeatureMatch attack, and they design a poisoning attack
strategy specifically for this task. We will demonstrate that the
generic attacks we develop here function on text classifiers trained
using BERT as well.
4.1.2 ClusterMatch. Our next filter function, ClusterMatch
(Algorithm 2), replaces the need for annotation with clustering to
identify subpopulations of interest. By identifying natural clusters
in the data, one can compromise the model for one cluster but
not elsewhere. In ClusterMatch the attacker uses the auxiliary
dataset ğ·ğ‘ğ‘¢ğ‘¥ for clustering and identifying the most vulnerable
subpopulations.
There are various design decisions that need to be taken care
of before we can use ClusterMatch. We must specify a prepro-
cessing function PreProcess applied to the auxiliary data, and a
clustering algorithm. For preprocessing phase, we first use the rep-
resentation layer of a neural network trained on ğ·ğ‘ğ‘¢ğ‘¥ (we test
which layer is most effective in Section 5), and then apply a PCA
projection. For clustering, we use KMeans, but any procedure for
generating meaningful clusters on a given dataset should work.
Interestingly, ClusterMatch can also be used in cases in which
the adversary has a targeted subpopulation in mind. For example,
consider an adversary who wishes to disrupt street sign detection in
a self-driving car through a subpopulation attackâ€”ClusterMatch
would help identify vulnerable street signs which will be easiest
to target, increasing the impact and stealth of their attack. In gen-
eral, an adversary can generate a clustering and identify a cluster
that is both aligned with their goals and will be easy to attack.
We show in the Appendix C that subpopulations generated with
ClusterMatch can be semantically meaningful.
4.2 Poisoning Attack Generation
4.2.1 Label Flipping. For our poisoning attack generation, we be-
gin by adapting a common baseline algorithm, label flipping, to
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3108Figure 1: Overview of our subpopulation attack framework. The attacker has access to an auxiliary dataset, from which it can
determine vulnerable subpopulation by using either FeatureMatch or ClusterMatch. Poisoning attack generation can be
done by label flipping (where a point drawn from a subpopulation with majority class ğ‘ is added with label ğ‘¡ â‰  ğ‘), or with
attack optimization (starting from label flipping, use either influence or gradient optimization for the final attack point).
our setting. Label flipping has been used in poisoning availability
attacks [76] to create poisoning points that have similar feature
values with legitimate data, but use a different label.
If the subpopulation size is ğ‘š, and the adversary uses a poison-
ing rate ğ›¼ relative to the subpopulation, they add ğ›¼ğ‘š poisoned
points, which should be small relative to the entire dataset size. In
label flipping attacks, these points are generated by sampling ğ›¼ğ‘š
points satisfying the filter function from ğ·ğ‘ğ‘¢ğ‘¥ and adding these
to the training set with a label ğ‘¡ different from the original one
ğ‘. We choose a single label for the whole subpopulation, which
maximizes the loss on the poison point. Label flipping ensures high
target damage, while the filter function itself is what guarantees
low collateralâ€”if it is a good enough separation, then the learn-
ing algorithm will be able to independently learn the poisoned
subpopulation, without impacting the rest of the distribution.
While simple, this attack is very general and applicable to various
data modalities, including images, text, and tabular data, as our
experiments will show. To demonstrate the modularity of our attack
framework, we show that leveraging optimization techniques such
as influence functions, following the results of [30], and gradient
optimization, can improve the effectiveness of our attacks.
4.2.2 Attack Optimization. In order to optimize points generated
by label flipping, we follow Koh and Liang [30]. They propose
influence functions to understand the impact of training samples on
trained models, and demonstrates as an application an optimization
procedure for generating poisoning data. To increase the loss on
a test point ğ‘¥ğ‘¡ğ‘’ğ‘ ğ‘¡ , ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡, by modifying a training point ğ‘¥, ğ‘¦ by a ğ›¿
perturbation ğ‘¥ + ğ›¿, ğ‘¦, [30] use gradient descent on the influence I,
with the following update, to optimize poisoning data:
âˆ‡ğ‘¥I(ğ‘¥ğ‘¡ğ‘’ğ‘ ğ‘¡ , ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡) = âˆ’âˆ‡ğœƒ ğ¿(ğ‘¥ğ‘¡ğ‘’ğ‘ ğ‘¡ , ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡ , ğœƒ)ğ‘‡ ğ»âˆ’1
ğœƒ âˆ‡ğ‘¥âˆ‡ğœƒ ğ¿(ğ‘¥, ğ‘¦, ğœƒ)
(3)
This is derived by approximating the modelâ€™s loss function with a
quadratic function around the unpoisoned parameters ğœƒ. The curva-
ture of the quadratic function is described by the Hessian around ğœƒ,
ğ»ğœƒ . In order to use the Hessian for even small models and datasets,
[30] use various optimizations and approximations; using Hessians
is computationally expensive, and dominates the attackâ€™s running
time. Indistinguishable attacks are a goal of [30]; subpopulation
attacks do not have this limitation: we only require that inputs
remain valid (for images, pixels remain in [0, 255] bounds).