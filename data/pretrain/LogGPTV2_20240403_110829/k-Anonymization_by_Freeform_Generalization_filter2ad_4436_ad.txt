a Tm ∈ {T1, . . . , T(cid:2)−1}; thus, the row of Sy should have an
empty cell in R3 whose column does not contain an “(cid:9)”.
Let w1, w2, w3, and w4 denote the number of “(cid:9)” entries in
region R1, R2, R3, and R4, respectively. We devise sufﬁcient
conditions for the above requirements as follows: Requirement (2)
implies that there should exist a column Tm ∈ {T1, . . . , T(cid:2)−1}
that does not contain an “(cid:9)” entry. Thus, the total number of “(cid:9)”
entries in these columns should be less than the number of columns
themselves. This number of “(cid:9)” entries is w1 + w3, while the
number of columns is (cid:4) − 1. Thus, it should hold that:
w1 + w3  0, Inequality (12) is always
true. Then, by Equations (14)-(16):
2. There should exist a Tm, neither already used in the cur-
rent iteration and nor matched to Sy in previous iterations,
which Sy can obtain as a substitute for Tj. Such a Tm can-
not be in {T(cid:2), . . . , Tn}, given that all T matches therein
have been used already in the current iteration, hence their
columns contain “(cid:9)” entries, as discussed; it should then be
Then, to satisfy Inequality (13), we have to ensure that:
n − (cid:4) − i + x + 2 ≥ (cid:4) − 1
which is equivalent to:
x ≥ 2 · (cid:4) + i − n − 3
w4 = n − (cid:4) − i + x + 2 + w1
Since w1 ≥ 0, it follows that:
w4 ≥ n − (cid:4) − i + x + 2
(18)
(19)
(20)
(21)
Thus, we can ﬁnd the desired Sy by examining no more than
2 · (cid:4) + i − n − 3 previously matched vertices.
We can now prove the following Theorem:
THEOREM 5.2. Our Greedy algorithm always resolves the dead-
end for k  2 · (cid:4) + i − n − 3, or
(cid:4)  wk,j and Si is
the next vertex to be processed. Then, assuming ei,j is the lightest
edge incident to Si, it will be picked up; thus, ek,j will be ren-
dered unavailable, even though it was a better choice of edge from
a global (though still greedy) perspective.
Motivated by this observation, we propose an enhanced greedy
algorithm, which we call SortGreedy. The external shell of the al-
gorithm remains the same, i.e., it operates over k iterations, with
each iteration striving to select an assignment that brings about a
small increase of the total GCP , and edge weights properly rede-
ﬁned among iterations. What differs is the internal edge selection
process within each iteration. We outline this process below.
Algorithm 2: SortGreedy Algorithm Iteration
Data: A weighted bipartite graph G = (S, T, E)
Result: An assignment A with weight close to minimum
1 Sort edges E by ascending weight;
2 while E (cid:2)= (cid:3) do
Select ei,j with minimum weight;
3
if Si ∈ S and Tj ∈ T then
4
S = S − Si, T = T − Tj;
5
Remove ei,j from E;
6
Add ei,j to A;
7
8 if ∃ unmatched vertices then
9
10
11
12 return A;
foreach unmatched vertex Si ∈ S do
ﬁnd Sy matched to Tj such that ei,j and ey,m are available;
substitute ey,m for ey,j and add ei,j to A;
We ﬁrst sort all edges in E by ascending weight at O(n2 log n)
cost. Then, instead of scanning a vertex list S, we scan the sorted
list of edges instead and try to pick up good edges directly there-
from. For each encountered edge, ei,j, we check whether its adja-
cent vertices, Si and Tj, are both available for matching. If that is
the case, we select ei,j as a match and remove it from E, while also
removing Si from S and Tj from T , as they are no longer available
for matching. Otherwise, we proceed to the next edge in the sorted
list, until all edges are examined.
As with our basic Greedy algorithm, the above process may not
terminate successfully, i.e., it may not have built a perfect matching
of n edges after one pass through the edge list; some vertices may
remain unmatched even after all edges have been processed. If this
is the case, we call a backtracking procedure similar to the one out-
lined in Section 5. We scan the vertex list S, in lexicographic order,
so as to detect unmatched vertices; for each such vertex Si we look
for an eligible substitution candidate among its neighbors in the lex-
icographic order; now we look not only at its predecessors, but at
both predecessors and successors, as already-matched vertices can
be found anywhere in the lexicographically ordered list. However,
the essence of the backtracking process remains the same, hence
Lemma 5.1 and Theorem 5.2 still hold. Algorithm 2 presents the
basic iteration of this SortGreedy algorithm. As the complexity of
an iteration is dominated by the sorting step, the overall complexity
of SortGreedy is O(kn2 log n).
7. THE HUNGARIAN-BASED ALGORITHM
Both our greedy algorithms work over k iterations, and at each
iteration they attempt to ﬁnd a perfect matching (assignment) that
achieves small sum of edge weights (i.e., GCP increase). They
follow a heuristic greedy logic in solving k local problems instead
of the global problem of ﬁnding a k-regular generalization graph
that minimizes GCP in one go. We have maintained the heuristic
logic of k iterations and enhanced the internal greedy algorithm for
assignment extraction. Still, the weight minimization problem ad-
dressed by this internal process is polynomially solvable - it is the
Assignment Problem that ﬁnds a Maximum (or Minimum) Weight
Perfect Matching (MWPM) in a bipartite graph. We can then apply
the O(n3) Hungarian algorithm that ﬁnds an optimal solution for
this problem as the internal process, while maintaining the shell of
k iterations. We call the resulting O(kn3) algorithm Hungarian
for brevity. This algorithm remains a heuristic, as it iteratively per-
forms local optimizations. For the sake of completeness, we offer a
brief description of the plain Hungarian algorithm, which we apply
for minimizing the GCP increase at each iteration.
The Hungarian algorithm was developed by Kuhn [18], Edmonds
and Karp [13], and Tomizawa [26]. Consider the graph in Fig-
ure 5a, where edge ei,j = (Xi, Yj) carries a weight wi,j. Without
loss of generality, we aim to ﬁnd a perfect matching of maximum
weight (MWPM). Such a matching is formed by red edges in Fig-
ure 5a, with total weight 16. Given a bipartite graph G with edges
E and a matching M, an augmenting path is a path starting out
from and terminating at nodes not in M, with edges alternating be-
tween E−M and M. The path X1 → Y2 → X2 → Y3 in Figure 5c
is an augmenting path with respect to the matching M formed by
green edges. Such a path can be used to increase the size of a
matching M by exchanging the roles of edges in M and not in M;
e.g. M = {e3,1, e2,2}, will be extended to {e3,1, e1,2, e2,3} after
exchanging edges within the augmenting path.
A labeling L labels each vertex v in G with a weight L(v);
such weights are shown in rectangles in Figure 5a. L is feasible if
L(Xi) +L (Yj) ≥ wi,j for any edge ei,j. An equality graph EL
for L is formed by the set of edges {ei,j : L(Xi)+L(Yj) = wi,j}.
By the Kuhn-Munkres theorem, if L is feasible and M is a perfect
matching in EL then M is a MWPM for G. We can see that this
theorem holds as follows: Since, by deﬁnition, no matching can
have weight more than the total weight of its vertex labels, a match-
ing ML that establishes the equality achieves the maximum weight.
The Hungarian algorithm iteratively ﬁnds a maximum matching
(i.e. a matching with the most edges) within EL, and extends EL
by adjusting L, until a perfect matching is found within EL.
1Y
1Y
0
0
2Y
2Y
0
0
3Y
3Y
0
0
1Y
0
2Y
0
3Y
0
1Y
0
2Y
2
3Y
0
1
1
6
6
8
8
6
1
4
6
8
4
6
8
1X
6
8
1X
6
3X
4
2X
6
2X
(c)
(a)
Figure 5: Hungarian Algorithm Illustration
2X
(b)
1X
3X
4
4
8
6
4
3X
4
We clarify that, before applying the algorithm to our problem at
each iteration, we convert each edge weight wi,j to −wi,j, so as
to minimize GCP increase. The algorithm starts with an initial
feasible labeling L, obtained by assigning the largest edge weight
at each node on one side, and 0 on the other side. Figure 5a shows
such an initial labeling, which creates the equality graph EL shown
by dotted blue and solid green edges in Figure 5b. We pick a match-
ing ML inside EL, formed by solid green edges, and extend it using
augmenting paths found by breadth-ﬁrst search until no more such
path can be found. By Berge’s Theorem, a matching ML is maxi-
mum iff it has no augmenting path. When ML becomes maximum
in EL, we extend EL by reducing the weight of some Xi’s by a
certain amount σ, so that an edge that was not in EL now becomes
a part of EL. In order to ensure the original edges in EL are re-
tained after the modiﬁcation, σ is compensated to the neighbors of
Xi. For example, the equality graph in Figure 5b is extended to
the one in Figure 5c by reducing the weights of X1 and X2 by 2
and increasing the weight of Y2 by 2, thereby bringing edge e2,3
into EL. Then we extend ML in the new EL. Eventually, if the
graph contains a perfect matching, it will be found within EL and
returned as the MWPM.
8. EXPERIMENTAL EVALUATION
In this section we conduct a thorough experimental study of the
algorithms we have introduced in comparison to previous work. To
the best of our knowledge, this is the ﬁrst work to provide experi-
mental results comparing a practical k-anonymization algorithm to
an optimal solution. Our study features the following algorithms:
• NH The ring-generalization method proposed in [28]. We
run this method exactly as it is proposed, with ring general-
ization applied on the partitions of size between k and 2k−1.
We attempted to apply ring generalization on larger parti-
tions, yet we determined that the best results are obtained
when the method runs in its proposed form. This ﬁnding
conﬁrms that the utility gains achieved by NH are primarily
due to the employed partitioning method, rather than due to
the ring generalization itself. Runtime measures for NH in-
clude the time for partitioning, building rings, and the ﬁnal
randomization step that extracts assignments.
• k-c The agglomerative algorithm implementing the k-con-
cealment method in [16, 25]. A randomization step for this
algorithm is proposed in [25], aiming to provide security
against reverse-engineering the algorithm. Yet this step in-
troduces extra information loss.
In order to allow for the
best-case scenario for k−c, in terms of both information loss
and runtime, we do not include this step in our experiments.
• minCostFlow The CPLEX solver for the minimum-cost net-
work ﬂow problem, in which the minimized objective func-
tion is not GCP , but the sum of edge weights in the original
complete graph. We include this so as to check whether an
off-the-shelf algorithm can perform well on our problem.
• MIP The CPLEX MIP solver running our formulation for an
optimal solution in Section 4.
• Greedy Our Greedy algorithm of Section 5, with a random-
ization step for assignment extraction as in [28, 29].
• SortGreedy Our enhanced greedy algorithm of Section 6,
with randomized assignment extraction included.
• Hungarian Our Hungarian-based algorithm of Section 7, with
assignment extraction by randomization.
Attribute
Age
Gender
Education Level
Marital Status
Race
Work Class
Country
Occupation
Cardinality
79
2
17
6
9
10
83
51
Type
numerical
categorical
numerical
categorical
categorical
categorical
categorical
categorical
Table 2: The CENSUS dataset
MIP and minCostFlow employ the IBM CPLEX Studio 12.4,
invoked by a C++ interface. NH is implemented in C++, while
Greedy, SortGreedy, Hungarian, and k−c, are in Java. The meth-
ods we compare against have no efﬁciency disadvantage arising
from their implementation environment. All experiments ran on
a 24-core Intel Xeon CPU machine @2.67GHz with 48GB RAM
running Ubuntu 12.04.1 LTS. We use the CENSUS dataset [1],
which contains 500K tuples on 8 attributes as shown in Table 2.
8.1 Evaluation under Partitioning
We commence our experimental study with the following ob-
servation: Our methods are conﬁgured to run on the full data set;
nevertheless, the main competing technique, NH, does not do so.
We tried to run NH on the full data set, yet the GCP results it
achieved were worse than those achieved in its default partitioning-
based version. This ﬁnding indicates that the ring generalization
employed by NH may be a liability on large data sets.
It is ar-
guably a good idea to apply our methods on a per-partition basis
too, for the sake of efﬁciency. This is what we do in this exper-
iment. We ﬁrst sort the input data set following a lexicographic
order as described in Section 5. Then we divide the data into par-
titions of equal size P by simply selecting segments of P tuples
along the lexicographic order. Furthermore, we use a different par-
tition size for each algorithm, so as to ensure that SortGreedy and
Hungarian run at time close to that of Greedy on a single partition.
We envisage these algorithms running at a data center offering par-
allel processing capabilities, with each partition utilizing a different
machine. Thus, the runtime of an algorithm is measured as the time
for partitioning plus the time for processing a single partition, in-
cluding the extraction of k random assignments. This conﬁguration
allows us to compare the performance of Hungarian, SortGreedy,
and Greedy when they are given an equal time budget per partition.
In the case of NH, there is no point of selecting the largest parti-