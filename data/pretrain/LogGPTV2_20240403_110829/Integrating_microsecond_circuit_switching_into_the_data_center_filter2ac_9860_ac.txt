an OCS capable of switching at O(10) µs, and (2) modifying ToRs
to support ﬂow control on a per-destination basis at microsecond
timescales. Unfortunately, as we discuss in Section 8, the only
practical commercially available OCSes that can switch in sub-
100 µs timescales have small port counts (e.g., 4 or 8 ports). To
evaluate at the scale of a ToR (e.g., 24–48 hosts), we instead build
our own prototype OCS supporting 24 ports based on commercial
wavelength-selective switches, described in Section 5.1. Instead of
building our own ToRs with our ﬂow control requirements, we in-
stead emulate them using commodity Linux servers, as described
in Section 5.2.
Figure 4: Virtual output queue (VOQ) buffer occupancies for a
ToR from cold start.
which is representative of the Mordia OCS, then the effective link
rate R is 9 Gbps.
The duty cycle can be increased by reducing the setup time or
increasing the duration that a circuit is open. Reducing setup time
tsetup depends on switch technology. The duration tstable that a
circuit is open is controllable. However, having circuits that are
open for a long period of time affects the amount of buffering that
is required at the host, as we discuss below. Since the Mordia
OCS is rate agnostic, it is also possible to increase overall delivered
bandwidth by using faster optical transceivers to increase the link
rate while simultaneously reducing the duty cycle for the circuit-
switched portion of the network.
4.2 Buffer requirements
Buffering is required at the source ToR in a circuit-switched net-
work because there is not always a circuit established between a
particular source and destination. In this section, we analyze these
buffering requirements.
Assume that each ToR connected to the Mordia switch maintains
a set of N virtual output queues (VOQs) [20], one for every possi-
ble circuit destination. Strict VOQ is not required, but the ToR must
maintain at least one set of queues for each possible destination.
When a circuit is established, all trafﬁc destined for that particular
destination is drained from the respective queue. Figure 4 shows
the buffer occupancies of these VOQs of a ToR from a cold start,
in units of the slot time T (without loss of generality, we assume
uniform slot times here). In less than one complete scheduling pe-
riod a ToR has ﬁlled its VOQs to the steady-state level. A particular
queue ﬁlls at a rate dictated by the trafﬁc matrix until a circuit is es-
tablished to the appropriate destination. The queue is drained over
the duration that the circuit is open.
For an all-to-all workload with N ToRs, an effective link rate of
R bits per second, and a slot duration of T seconds, the buffering
required by each host is:
B = R(N − 1)T
(bits)
(3)
Examining (3) shows why millisecond switching times are sim-
ply too large to support trafﬁc matrix scheduling. For example if R
= 9 Gbps, N = 64, and T = 100 ms, then B is 7.1 GB of buffering
per OCS port, which is not currently practical. Given that R and
N are both likely to increase in future data centers, the only way to
make trafﬁc matrix scheduling practical is to decrease the slot time
T by using, e.g., microsecond switching. Setting T = 100 µs yields
B = 7.1 MB of buffering per port, which is currently practical. Re-
ducing buffering within the ToR leads to more practical and lower
cost designs.
4.3 Longest time-slot ﬁrst scheduling
While BvN always produces a schedule that eventually serves
all input-output pairs according to their demand, sometimes it may
VOQTotalTime slot77704444444445555555666661112345678111111112222222233567412312312312312370456223333300000000017056704563706170210213421354235643312704561000000007131822252728282828282828282828451build a single 24 × 24-port switch. We now brieﬂy summarize the
operation of the data path.
5.1.2 Data plane
The Mordia OCS prototype is physically constructed as a unidi-
rectional ring of N = 24 individual wavelengths carried in a single
optical ﬁber. Each wavelength is an individual channel connecting
an input port to an output port, and each input port is assigned its
own speciﬁc wavelength that is not used by any other input port.
An output port can tune to receive any of the wavelengths in the
ring, and deliver packets from any of the input ports. Consequently,
this architecture supports circuit unicast, circuit multicast, circuit
broadcast, and also circuit loopback, in which trafﬁc from each port
transits the entire ring before returning back to the source. We note
that although the data plane is physically a ring, any host can send
to any other host, and the input-to-output mapping can be conﬁg-
ured arbitrarily (an example of which is shown in Figure 5).
Wavelengths are dropped and added from/to the ring at six sta-
tions. A station is an interconnection point for ToRs to receive and
transmit packets from/to the Mordia prototype. To receive packets,
the input containing all N wavelengths enters the WSS to be wave-
length multiplexed. The WSS selects four of these wavelengths,
and routes one of each to the four WSS output ports, and onto the
four ToRs at that station. To transmit packets, each station adds
four wavelengths to the ring, identical to the four wavelengths the
station initially drops. To enable this scheme, each station contains
a commercial 1 × 4-port WSS.
5.1.3 ToRs
Each ToR connects to the OCS via one or more optical uplinks,
and internally maintains N − 1 queues of outgoing packets, one
for each of the N − 1 OCS output ports. The ToR participates in a
control plane, which informs each ToR of the short-term schedule
of impending circuit conﬁgurations. In this way, the ToRs know
which circuits will be established in the near future, and can use
that foreknowledge to make efﬁcient use of circuits once they are
established.
Initially, the ToR does not send any packets into the network, and
simply waits to become synchronized with the Mordia OCS. This
synchronization is necessary since the OCS cannot buffer any pack-
ets, and so the ToR must drain packets from the appropriate queue
in sync with the OCS’s circuit establishment. Synchronization con-
sists of two steps: (1) receiving a schedule from the scheduler via an
out-of-band channel (e.g., an Ethernet-based management port on
the ToR), and (2) determining the current state of the OCS. Step 2
can be accomplished by having the ToR monitor the link up and
down events and matching their timings with the schedule received
in Step 1. Given the duration of circuit reconﬁguration is always
11.5 µs, the scheduler can artiﬁcially extend one reconﬁguration
delay periodically to serve as a synchronization point. The delay
must exceed the error of its measurement and any variation in re-
conﬁguration times to be detectable (i.e., must be greater than 1 µs
in our case). Adding this extra delay incurs negligible overhead
since it is done infrequently (e.g., every second).
We use the terminology day to refer to a period when a circuit is
established and packets can transit a circuit, and we say that night
is when the switch is being reconﬁgured, and no light (and hence
no packets) are transiting the circuit. The length of a single sched-
ule is called a week, and the day and week lengths can vary from
day-to-day and from week-to-week. When the OCS is undergoing
reconﬁguration, each ToR port detects a link down event, and night
begins. Once the reconﬁguration is complete, the link comes back
up and the next “day” begins.
Figure 5: The Mordia OCS prototype, which consists of a ring
conveying N wavelengths through six stations. Each source
ToR transmits on its own wavelength, and each station for-
wards a subset of four wavelengths to the ToRs attached to it.
This prototype supports an arbitrary reconﬁgurable mapping
of source to destination ports with a switch time of 11.5 µs.
5.1 Mordia prototype
The Mordia prototype is a 24-port OCS that supports arbitrary
reconﬁguration of the input-to-output port mappings. We ﬁrst de-
scribe the underlying technology we leveraged in building the OCS,
then describe its design.
5.1.1 Technology
Unlike previous data center OCS designs [8, 25], we chose not
to use 3D-MEMS based switching due to its high reconﬁguration
time. The maximum achievable speed of a 3D-MEMS space switch
depends on the number of ports, since more ports require precise
analog control of the 2-axis orientation of relatively large mirrors.
Since the mirror response time depends on the size and angular
range, there is in general a design tradeoff between the switch port
count, insertion loss, and switching speed. As a result, commer-
cial 3D-MEMS switches support reconﬁguration times in the 10s
of milliseconds range [10].
Another type of optical circuit switch is a wavelength-selective
switch (WSS). It takes as input a ﬁber with N wavelengths in it, and
it can be conﬁgured to carry any subset of those N wavelengths to
M output ports. Typically a WSS switch has an extra “bypass” port
that carries the remaining N − M frequencies. We call this type
of WSS switch a 1 × M switch, and in our prototype, M = 4.
Our switch does not have a bypass port, and so we implement the
bypass functionality external to the WSS using additional optical
components.
The internal switching elements used in a wavelength-selective
switch can be built using liquid crystal technology or MEMS [9].
Most MEMS WSSes use analog tilt to address multiple outputs, but
at least one commercial WSS has been built using binary MEMS-
based switches [19]. Binary MEMS switching technology uses
only two positions for each mirror moving between two mechan-
ically stopped angles, and also uses much smaller mirrors with re-
spect to a 3D-MEMS space switch. A similar binary MEMS switch
is used for commercial projection televisions. The binary switching
of small mirror elements results in an achievable switching speed
that is several orders of magnitude faster than a commercial 3D-
MEMS switch.
In general, there is a tradeoff between 3D-MEMS, which offers
high port count at relatively slow reconﬁguration time, and 2D-
MEMS, which offers microsecond switching time at small port
counts (e.g., 1 × 4 or 1 × 8). The key idea in the Mordia OCS
prototype is to harness six 1 × 4 switches with bypass ports to
λ{1-24}WSST1T2T3T4λ8λ6λ1λ4λ{5-24}Station 1WSST5T6T7T8λ4λ6λ8Station 2Station 3TiStation 4TiStation 5TiStation 6Tiλ{1-4,9-24}452During normal-time operation, any data received by the ToR
from its connected hosts is simply buffered internally into the ap-
propriate queue based on the destination. The mapping of the packet
destination and the queue number is topology-speciﬁc, and is con-
ﬁgured out-of-band via the control plane at initialization time and
whenever the topology changes. When the ToR detects that day i
has started, it begins draining packets from queue i into the OCS.
When it detects night time (link down), it re-buffers the packet
it was transmitting (since that packet likely was truncated mid-
transmission), and stops sending any packets into the network.
5.1.4 Data plane example
Figure 5 shows an overview of the Mordia prototype’s data path.
In this example, there are three circuits established: one from T6
to T4, one from T8 to T1, and one from T4 to T5. Consider the
circuit from T4 to T5. T4 has a transceiver with its own frequency,
shown in the Figure as λ4. This signal is introduced into the ring by
an optical mux, shown as a black diamond, and transits to the next
station, along with the other N −1 frequencies. The WSS switch in
Station 2 is conﬁgured to forward λ4 to its ﬁrst output port, which
corresponds to T5. In this way, the signal from T4 terminates at T5.
The N − 4 signals that the WSS is not conﬁgured to map to local
ToRs bypass the WSS, which is shown as λ{1−4,9−24}. These are
re-integrated with the signals from ToRs T5 through T8 originating
in Station 2, and sent back into the ring. A lower-bound on the
end-to-end reconﬁguration time of such a network is gated on the
switching speed of the individual WSS switches, which we evaluate
in Section 6.1.
5.1.5 Implementation details
The implementation of the hardware for the Mordia prototype
consists of four rack-mounted sliding trays. Three of these trays
contain the components for the six stations with each tray housing
the components for two stations. The fourth tray contains power
supplies and an FPGA control board that implements the scheduler.
This board is based on a Xilinx Spartan-6 XC6SLX45 FPGA de-
vice. Each tray contains two wavelength-selective switches, which
are 1×4 Nistica Full Fledge 100 switches. Although these switches
can be programmed arbitrarily, the signaling path to do so has not
yet been optimized for low latency. Thus we asked the vendor to
modify the WSS switches to enable low-latency operation by sup-
porting a single signaling pin to step the switch forward through a
programmable schedule. As a result, although our prototype only
supports weighted round-robin schedules, those schedules can be
reprogrammed on a week-to-week basis. This limitation is not fun-
damental, but rather one of engineering expediency.
5.2 Emulating ToRs with commodity servers
To construct our prototype, we use commodity servers to emu-
late each of the ToRs. Although the Mordia OCS supports 24 ports,
our transceiver vendor was not able to meet speciﬁcations on one
of those transceivers, leaving us with 23 usable ports in total. Each
of our 23 servers is an HP DL 380G6 with two Intel E5520 4-core
CPUs, 24 GB of memory, and a dual-port Myricom 10G-PCIE-8B
10 Gbps NIC. One port on each server contains a DWDM 10 Gbps
transceiver, taken from the following ITU-T DWDM laser chan-
nels: 15–18, 23–26, 31–34, 39–42, 47–50, and 55–58. Each server
runs Linux 2.6.32.
5.2.1 Explicit synchronization and control
Each of the emulated ToRs must transmit packets from the ap-
propriate queue in sync with the OCS with microsecond precision.
The source code to our NIC ﬁrmware is not publicly available,
Figure 6: A software implementation of multi-queue support
in Linux using commodity Ethernet NICs. Sync frames co-
ordinate state between each emulated ToR (server) and the
scheduler, so that each Qdisc knows when to transmit Ether-
net frames.
and so we cannot detect link up and down events in real time and
cannot implement the synchronization approach presented in Sec-
tion 5.1.3. Instead, we have modiﬁed our prototype to include a
separate synchronization channel between the scheduler and the
servers that the scheduler uses to notify the servers when the switch
is being reconﬁgured. Ethernet NICs do not typically provide much
direct control over the scheduling of packet transmissions. Thus we
have implemented a Linux kernel module to carry out these tasks.
We now describe how we modiﬁed the Linux networking stack on