```
如果我们没有给文件设置执行权限，我们可以使用以下命令执行该脚本：
```
sh ./my_file.sh
```
### 传递参数
你很快就会意识到，这样的脚本很方便，但立即变得无用。每次执行 Python/Spark 应用程序时，都会生成一个新的 ID。因此，对于每次运行，我们都必须编辑文件并添加新的应用程序 ID。这无疑降低了脚本的可用性。为了提高其可用性，我们应该将应用程序 ID 作为参数传递：
```
#!/bin/bash
yarn –log -applicationId ${1} | less
```
我们需要这样执行脚本：
```
./show_log.sh 123
```
脚本将执行 `yarn` 命令，获取应用程序的日志并允许我们查看它。
如果我们想将输出重定向到一个文件中怎么办？没问题。我们可以将输出重定向到一个文件而不是发送给 `less`：
```
#!/bin/bash
ls –l ${1} > ${2}
view ${2}
```
要运行脚本，我们需要提供两个参数，命令变为：
```
./my_file.sh /tmp /tmp/listing.txt
```
当执行时，`$1` 将绑定到 `/tmp`，`$2` 将绑定到 `/tmp/listing.txt`。对于 Shell，参数从一到九命名。这并不意味着我们不能将超过九个参数传递给脚本。我们可以，但这是另一篇文章的主题。你会注意到，我将参数命名为 `${1}` 和 `${2}`，而不是 `$1` 和 `$2`。将参数名称封闭在花括号中是一个好习惯，因为它使我们能够无歧义地将参数作为较长变量的一部分组合起来。例如，我们可以要求用户将文件名作为参数，并使用它来形成一个更大的文件名。例如，我们可以将 `$1` 作为参数，创建一个新的文件名为 `${1}_student_names.txt`。
### 使脚本更健壮
如果用户忘记提供参数怎么办？Shell 允许我们检查这种情况。我们将脚本修改如下：
```
#!/bin/bash
if [ -z "${2}" ]; then
  echo "file name not provided"
  exit 1
fi
if [ -z "${1}" ]; then
  echo "directory name not provided"
  exit 1
fi
DIR_NAME=${1}
FILE_NAME=${2}
ls -l ${DIR_NAME} > /tmp/${FILE_NAME}
view /tmp/${FILE_NAME}
```
在这个程序中，我们检查是否传递了正确的参数。如果未传递参数，我们将退出脚本。你会注意到，我以相反的顺序检查参数。如果我们在检查第一个参数存在之前检查第二个参数的存在，如果只传递了一个参数，脚本将进行到下一步。虽然可以按递增顺序检查参数的存在，但我最近意识到，从九到一检查会更好，因为我们可以提供适当的错误消息。你还会注意到，参数已分配给变量。参数一到九是位置参数。将位置参数分配给具名参数可以在出现问题时更容易调试脚本。
### 自动化备份
我自动化的另一个任务是备份。在开发的初期阶段，我们没有使用版本控制系统。但我们需要有一个机制来定期备份。因此，最好的方法是编写一个 Shell 脚本，在执行时将所有代码文件复制到一个单独的目录中，将它们压缩，并使用日期和时间作为后缀来上传到 HDFS。我知道，这种方法不如使用版本控制系统那样清晰，因为我们存储了完整的文件，查找差异仍然需要使用像 `diff` 这样的程序；但它总比没有好。尽管我们最终没有删除代码文件，但团队确实删除了存储助手脚本的 `bin` 目录！！！而且对于这个目录，我没有备份。我别无选择，只能重新创建所有的脚本。
一旦建立了源代码控制系统，我很容易将备份脚本扩展到除了之前上传到 HDFS 的方法之外，还可以将文件上传到版本控制系统。
### 总结
如今，像 Python、Spark、Scala 和 Java 这样的编程语言很受欢迎，因为它们用于开发与人工智能和机器学习相关的应用程序。尽管与 Shell 相比，这些语言更强大，但“不起眼”的 Shell 提供了一个即用即得的平台，让我们能够创建辅助脚本来简化我们的日常任务。Shell 是相当强大的，尤其是因为我们可以结合操作系统上安装的所有应用程序的功能。正如我在我的项目中发现的那样，即使经过了几十年，Shell 脚本仍然非常强大。我希望我已经说服你尝试一下了。
### 最后一个例子
Shell 脚本确实非常方便。考虑以下命令：
```
spark3-submit --queue pyspark --conf "spark.yarn.principal=PI:EMAIL" --conf "spark.yarn.keytab=/keytabs/abcd.keytab" --jars /opt/custom_jars/abcd_1.jar --deploy-mode cluster --master yarn $*
```
我们要求在执行 Python/Spark 应用程序时使用此命令。现在想象一下，这个命令必须每天被一个由 40 个人组成的团队多次使用。大多数人会在记事本中复制这个命令，每次需要使用时，会将其从记事本中复制并粘贴到终端中。如果复制粘贴过程中出现错误怎么办？如果有人错误使用了参数怎么办？我们如何调试使用的是哪个命令？查看历史记录并没有太多帮助。
为了让团队能够简单地执行 Python/Spark 应用程序，我们可以创建一个 Bash Shell 脚本，如下所示：
```
#!/bin/bash
SERVICE_PRINCIPAL=PI:EMAIL
KEYTAB_PATH=/keytabs/abcd.keytab
MY_JARS=/opt/custom_jars/abcd_1.jar
MAX_RETRIES=128
QUEUE=pyspark
MASTER=yarn
MODE=cluster
spark3-submit --queue ${QUEUE} --conf "spark.yarn.principal=${SERVICE_PRINCIPAL}" --conf "spark.yarn.keytab=${KEYTAB_PATH}" --jars ${MY_JARS} --deploy-mode ${MODE} --master ${MASTER} $*
```
这展示了一个 Shell 脚本的强大之处，让我们的生活变得简单。根据你的需求，你可以尝试更多的命令和脚本，并进一步探索。
*（题图：MJ/f32880e8-0cdc-4897-8a1c-242c131111bf）*
---
via: 
作者：[Bipin Patwardhan](https://www.opensourceforu.com/author/bipin-patwardhan/) 选题：[lkxed](https://github.com/lkxed) 译者：ChatGPT 校对：[wxy](https://github.com/wxy)
本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创编译，[Linux中国](https://linux.cn/) 荣誉推出