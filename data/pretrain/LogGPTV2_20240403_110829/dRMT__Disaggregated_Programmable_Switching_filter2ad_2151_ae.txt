is T clock cycles, each processor must receive and process up to m =
⌈T /P⌉ packets in parallel. Hence, the processor uses an m-threaded
design to store packets awaiting service, each thread corresponding
to a different packet. This is similar to hardware multithreading in
general purpose CPU cores and GPUs [24], with the same purpose:
to achieve high utilization of the processor in the face of high latency
operations (e.g., L1 cache misses for general purpose CPUs, table
matches for dRMT).
Figure 9 shows the processor’s architecture. We list its compo-
nents below.
(1) Packet header vectors to store packet headers and metadata,
i.e., data about the packet that was derived from the packet
and/or table lookup contents, but is not part of the packet
itself.
(2) Match table key generation logic to generate match keys for
table lookups.
(3) Action Arithmetic Logic Units (ALUs) that allow multiple
packet fields to be modified in parallel every clock cycle.
(4) A Very Large Instruction Word (VLIW) [16] instruction mem-
ory that specifies the configuration (opcode and operands) for
each action ALU on every clock cycle.
(5) An action input crossbar to select inputs for each ALU.
(6) An action output crossbar to route ALU outputs to the packet
header vectors after modification.
(7) A scratch pad to temporarily store associated data returned as
a result of a table match (which we term action data segment),
if it is only going to be used in a later clock cycle.
(8) A thread scheduling configuration table to help select the
thread for match and action operations in every clock cycle.
Selecting multiple threads provides opportunities for inter-
packet concurrency, as discussed earlier.
Match key generation. Based on RMT, we use a 4-kbit packet
header vector, structured as 64 8-bit fields, 96 16-bit fields, and 64
32-bit fields, for a total of 224 fields. From this vector, we extract
101520253035Processors100101102103104105Time [Seconds]IngressEgressCombinedPacket	Header	Vector(4Kb)Packet	Header	Vector(4Kb)Packet	Header	Vector(4Kb)Packet	Header	Vector(4	kbit)VLIW	InstructionMemoryConfigTableCrossbar	toMemoriesALUThread	SelectScratchPad640b	KeyAction	Input	Selector(crossbar)ALUAction	Output	Selector(crossbar)Op	codeCtrl…	…8x96bAction	resultsMatch	Key	Generation	(crossbar)Inst.	pointerdRMT: Disaggregated Programmable Switching
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
fields to send up to 640 bits of match search key to the crossbar. This
is structured as eight 80-bit key segments, e.g., it can consist of one
320-bit key spanning 4 segments, and two 160-bit keys spanning
2 segments each. We do not find the need to form separate 640-bit
keys for the hash tables and TCAMs and find that we can generally
fit both the hash table and TCAM keys generated from a processor
into 640 bits.
In RMT, the match keys are generated by a 4-kbit-to-640-bit
crossbar that takes in the packet header vector as input and outputs
up to eight search keys. Each match-action stage needs only one
configuration setting for the crossbar, stored in that stage in flip-
flops. Different stages have independent configurations so each can
generate different search keys depending on what is being looked up
in each stage.
On the contrary, dRMT is a run-to-completion architecture, so
each processor must be able to generate different search keys in
each of the P clock cycles of its repeating schedule. To reduce chip
area, instead of storing an identical set of P search-key crossbar
configuration settings in each processor, we store one copy of the
complete configuration in a P-entry table in a central chip location,
and then distribute the configuration to each processor just in time
for key generation, via a two-level distribution tree.
Recall that dRMT can schedule match/action operations from
multiple threads in the same clock cycle (IPC > 1; see §3). The only
restriction is that the combined search keys from all the threads fit
into the 640-bit search key.
ALUs. The goal of each arithmetic and logic unit (ALU) is to per-
form one operation per clock cycle on one or two packet header
fields, such as arithmetic on two 32-bit numbers, left or right shifts,
comparisons, or logic operations. Both in RMT and dRMT, ALUs
themselves are relatively cheap to implement, but can require ex-
pensive logic around them to get operands and outputs into and
out of the ALUs. For instance, both in RMT and dRMT, the cross-
bar logic required to extract ALU inputs/operands from the 4-kbit
packet header vector and the 768-bit action data segments (i.e., the
match results from up to eight tables, each of which is 96 bits) is
considerable.
In addition, while in RMT, each stage only needs instruction
memory for the program fragment that is resident on that stage,
in dRMT, each processor needs to store the entire program. This
requires us to reduce the number of ALUs in the dRMT design,
because a larger number of ALUs increases the width of the VLIW
instruction. RMT uses 224 ALUs within its VLIW instruction, one
for each of the 224 fields in the 4-kbit packet header vector. Based
on our analysis of switch.p4 [13], we find that 32 parallel ALUs
are sufficient. Figure 10 illustrates this analysis of switch.p4. We
note that over 90% of the tables require eight or fewer primitive
actions [12] to be performed for their largest compound action, and
over 90% of the primitive actions used can be performed with a
single ALU. Analysis of a proprietary P4 program gives similar
results.
The RMT designers justify their choice of an ALU for every
field by noting: “Allowing each logical stage to rewrite every field
may seem like overkill, but it is useful when shifting headers... A
logical MPLS stage may pop an MPLS header, shifting subsequent
MPLS headers forward” [16]. For the particular use case mentioned,
Figure 10: Primitive actions required for switch.p4 tables.
an array of 6 MPLS headers in P4 requires only 6 32-bit ALUs
to implement a push or pop operation. Larger header arrays are
uncommon in networking protocols. If such cases are encountered in
a P4 program that cannot be handled with a single VLIW instruction
for 32 ALUs, dRMT with 32 ALUs can perform it in multiple cycles.
dRMT’s ALUs use 32-bit inputs and outputs and are functionally
identical to the ALUs on 32-bit fields in the RMT chip. With 32 such
ALUs, we can modify up to 1 kbit in the packet vector in one clock
cycle. Because we only have 32 ALUs, we implement a 32 × 224
crossbar to write back the ALU outputs to 32 out of 224 locations
in the packet header vector. This output crossbar does not exist in
RMT because each of the 224 fields has one ALU hard-wired to it.
Recall that up to eight matches may execute every clock cycle,
so we can have up to eight distinct actions executing every clock
cycle. In aggregate, these eight actions can use up to 32 ALUs.
The compiler ensures that the eight actions are implemented using
disjoint sets of ALUs in order to avoid resource conflict. Thus,
each of the 32 ALUs is assigned to one of the eight actions every
clock cycle; this action configures that ALU with an operand and an
opcode during that cycle.
VLIW instruction memory. The VLIW instruction memory is used
to configure each of the 32 ALUs within a dRMT processor by
supplying it with opcodes and operands every clock cycle. We im-
plement the instruction memory as 32 per-ALU SRAM slices (Fig-
ure 11). Each slice can hold up to 1K entries; each entry corresponds
to the configuration of that ALU for one of 1K different actions. We
chose the number 1K to provide the same number of actions allowed
by RMT (32 stages * 32 user-defined actions per stage). Each entry
within the slice stores the opcode for the ALU along with the address
of the operand. The operand can be one of the 224 packet fields, one
of the eight 96-bit action data segments (e.g., the value of the next
hop for an action that sets the next hop) returned as a result of the
table lookup, a constant, or data from the scratchpad.
Which of the 1K entries within a slice configures an ALU on every
cycle? Each table lookup, in addition to returning a data segment,
also returns a 10-bit instruction pointer that represents one of the 1K
different actions. These instruction pointers returned from a table
lookup address the per-ALU slices during that cycle. To determine
which of the 32 per-ALU slices a particular instruction pointer should
address on a particular clock cycle, we use a 32 × 96b configuration
table. This table is indexed by a 5-bit program counter and hence has
32 entries. Each entry in the table consists of 32 3-bit select fields,
one for each ALU. The entry indicates which of the eight actions
(and hence which of the 8 10-bit instruction pointers) executing in
that cycle are assigned to each of the 32 ALUs for that cycle.
0%10%20%30%012345678101418212529Fraction of tablesNumber of primitive actions to executeSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Chole et al.
Figure 12: Unit, segment, and full crossbars. This figure repre-
sents one specific configuration of each of the three crossbars,
not the entire wiring diagram.
and actions to reduce the temporary storage required. This can be
achieved by, for example, restricting the allowed number of inter-
leaved match-then-action operations (e.g., by changing Equation (2)
to an equality instead of an inequality for some dependency con-
straints, thus leaving less slack), possibly at the expense of a higher
program latency, and even lower throughput if a feasible schedule
with these new restrictions cannot be found anymore. We leave this
refinement to our ILP formulation (§3.3) to future work.
5.2 Memory cluster
A dRMT memory cluster is organized like the memory within a
single RMT stage. Each memory cluster has a set of memory blocks
that can be grouped together to form wider or deeper logical tables.
The parameters for memory blocks are identical to RMT [16]. The
number of processors is typically equal to the number of memory
clusters, but the architecture allows these numbers to differ (§2.1).
5.3 Crossbar
The crossbar connects processors to memory clusters. Every clock
cycle, a processor can produce up to ¯M = 8 key segments of b =
80 bits, and expect eight 10-bit instruction pointers plus eight 96-
bit action data segments from the memory clusters. The crossbar
configuration is statically programmed during compilation, based on
the result of the scheduling algorithm.
Crossbar types. When designing the dRMT architecture, we con-
sidered various crossbar types and their tradeoffs (Figure 12).
(1) Unit Crossbar: One-to-one connectivity between a processor
and a memory cluster.
(2) Segment Crossbar: One-to-one connectivity between a key
(or action data) segment at the kth index on a processor and a
segment at the kth index on a memory cluster. This is equiva-
lent to k parallel unit crossbars, one for each segment.
(3) Full Crossbar: One-to-one connectivity between any segment
on a processor and any segment on a memory cluster. This
gives us complete flexibility in terms of table allocation on
memory clusters.
In addition, for each of the above-mentioned types, we considered
one-to-many multicast crossbars. A one-to-many crossbar enables
the processor to access multiple memory clusters at once without
incurring extra latency. This allows large tables to be spread across
multiple clusters, and still be accessed in a single match operation.
While the segment crossbar seems to be more constrained than
the full crossbar, we can prove the following powerful result:
Figure 11: VLIW instruction memory.
The VLIW instruction memory cannot be replaced by a central-
ized distribution tree like the match configuration. This is because
the VLIW ALUs’ opcodes need to be determined at run-time and
potentially could be different for every packet, depending on the
instruction pointer returned by the match-action table lookup for
that packet. On the other hand, the match crossbar configuration can
be determined at compile time and simply needs to be sent to each
processor just before the processor needs it.
Scratch pad. If the program only consists of a fixed pattern of
consecutive match-immediately-followed-by-action elements, the
action data segments are consumed as they are returned. However,
in dRMT there are additional challenges:
• The program scheduler may decide to delay an action using
no-ops to assign the clock cycle to another packet;
• The scheduler may decide to interleave matches and actions,
e.g., M1M2 · · · A1A2, where Mi corresponds to matches and
Ai to actions;
• The scheduler may even merge actions following multi-
ple match operations to reduce latency, e.g., M1M2 · · · A1+2,
where A1+2 corresponds to a merged action that runs both
actions A1 and A2 in parallel.
While this flexibility improves the scheduling outcome, it requires
temporary storage for the results of match operations that are re-
turned from the memory clusters back to the processors, since these
results are not always immediately consumed.3 We implement a
scratch pad for this purpose. The width of the scratch pad is 96 bits
(the width of the action data segment) and it can store up to 64 en-
tries, which we found to be adequate for programs we evaluated. The
scratch pad has eight write ports and eight read ports. This allows
the processor to write data from eight action results in parallel and
later read up to eight action data segments in parallel.
A deeper scratch pad leaves more flexibility for the scheduler
to postpone actions, but adds to the hardware cost. If the compiler
is given a program where this number of scratch-pad entries is
insufficient, the scheduler can rearrange the sequence of matches
3The results of actions are always written back into the packet header vector, and
therefore do not need temporary storage.
1K slice(for ALU 0)1K slice (for ALU 31)Action ProgramCounterBits[95:93]Instruction Ptr0 (10b)… …8:1 muxInstruction Ptr7 (10b)Instruction Ptr0 (10b)… …8:1 muxInstruction Ptr7 (10b)…Bits[2:0]32 X 96bALU Select TableProc.  1Mem. 1Proc.  2Mem. 2Proc.  1Mem. 1Proc.  2Mem. 2Proc.  1Mem. 1Proc.  2Mem. 2Unit CrossbarSegment CrossbarFull CrossbardRMT: Disaggregated Programmable Switching
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Unit
Segment
Full
Crossbar Crossbar Crossbar
One-to-One
One-to-Many
0.544
0.561
0.561
0.576
4.448
4.464
Table 4: 32x32 crossbar synthesis gate-only area (mm2).
dRMT
RMT (IPC = 1)
dRMT
(IPC = 2)
Component
Key Generation
Match key config. register
Match key xbar
Packet Storage
Packet header vectors
Scratch pad
Actions
Action input selector
ALUs
Action output selector
VLIW instruction table
Total
0.007
0.098
0.110
N/A
0.004
0.049