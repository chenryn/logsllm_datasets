Silva, B., Cardoso, J., “Semantic Data Extraction for B2B Integration”, International Workshop on Dynamic Distributed Systems (IWDDS), In
conjunction with the ICDCS 2006, The 26th International Conference on Distributed Computing Systems July 4-7, 2006 - Lisboa, Portugal. IEEE
Computer Society. ISBN: 0-7695-2541-5, ISSN: 1545-0678.
Semantic Data Extraction for B2B Integration
Bruno Silva, Jorge Cardoso
Department of Mathematics and Engineering
University of Madeira
9050-390 Funchal, Portugal
PI:EMAIL, PI:EMAIL
Abstract autonomous, and distributed data sources: syntactic
heterogeneity: the technology supporting the data
Business-to-business (B2B) data exchange and sources differs (e.g. databases, Web pages, XML
integration is a common daily operation in today’s streams, etc); schematic heterogeneity: data sources
organizations. These operations are crucial since they schema have different structures; and semantic
affect organizations’ capability to compete in today’s heterogeneity: data sources use different meanings,
marketplace. Data exchange and integration has been nomenclatures, vocabulary or units for concept.
proven to be a challenge due to the heterogeneity of Our approach uses a Syntactic-to-Semantic (S2S)
the information systems involved."This paper middleware approach to resolve data source
described a Syntactic-to-Semantic (S2S) middleware heterogeneity problems and offers the advantages of
which, when based on a single query, integrates data using a common shared structured format represented
residing in different data sources possibly with with an ontology. Thus, by the interpretation of a query
different formats, structures, schema, and semantics. (single point of entry), S2S generates ontology
The middleware uses an ontology-based multi-source instances that permits having the retrieved data in a
data extractor/wrapper approach to transform conceptual representation. This way, besides offering a
syntactic data into semantic knowledge. solution for B2B integration issues (through
standardised business models), it enables semantic
1. Introduction knowledge processing.
2. S2S Middleware Architecture
As organizations grow and change, their needs to
manage and access information increases
exponentially. In many situations, “data supporting Approaches to the problems of semantic
architectures have shifted from a centralized to a heterogeneity should equip heterogeneous,
distributed approach due to the advantages in the cost autonomous, and distributed software systems with the
and flexibility“. While these trends have resulted in ability to share and exchange information in a
many advantages for organizations, they have also semantically consistent way [1]. A suitable solution to
introduced a large gap in the ability to integrate data the problem of semantic heterogeneity is to rely on the
between applications and organizations. technological foundations of the semantic Web; or
A middleware for data integration should allow more precisely, to semantically define the meaning of
users to focus on ‘what’ information is needed and the terminology of each distributed system data using
leave the details on ‘how’ to obtain and integrate the concepts present in a shared ontology to make clear
information hidden from users. Thus, in general, data the relationships and differences between concepts.
integration systems must provide mechanisms to The S2S approach introduces the ability to extract
communicate with an autonomous data source, handle data from various data source types (unstructured,
queries across heterogeneous data sources, and semi-structured, and structured) and wrap the result in
combine the results in an interoperable format. OWL (Web Ontology Language) format [2], providing
Therefore, the key problem is to bridge syntactic, a homogenous access to a heterogeneous set of
schematic and semantic gaps between data sources, information sources.
thereby solving data source heterogeneity. The decision to adopt OWL as the ontology
At least three types of data heterogeneity may occur language is based on the fact that this is the World
when integrating information from heterogeneous,
Wide Web Consortium (W3C) recommendation for structured (e.g. XML) and unstructured (e.g. Web
building ontologies. pages and plain text files). The supported data source
Figure 1 presents a high level illustration of the S2S types can easily be increased to support other formats.
architecture. Two key areas can be identified. The first
concerns the extractor (Extractor Manager) used to 2.2 Ontology schema
connect to the different data sources registered in the
system and to extract data form them. The extracted To conceptualize a domain in a machine readable
data fragments are then compiled in order to generate format an ontology is necessary. In B2B applications,
ontology instances. The second key area is the mapping ontologies play an important role in order to promote
result between an ontology schema and the data and facilitate interoperability among systems, enable
sources (Mapping Module). This information is intelligent processing, and to share and reuse
produced when the ontology attributes and classes are knowledge. From a data integration point of view,
intersected with the data sources forming an extraction ontologies provide a shared common understanding of
schema used by the extractor to retrieve data from the a domain.
sources.
Figure 1 – Syntactic to Semantic architecture
Other areas also play an important role in the S2S middleware represents ontologies using the
architecture. This is the case of the Query Handler, Web Ontology Language (OWL), a semantic markup
which handles the queries to the data sources, the language for publishing and sharing ontologies on the
Instance Generator, which is responsible for providing World Wide Web. Other alternative formal languages
information about any error that has occurred during can also be used to express ontologies, for instance
the extraction process or in the query, and finally the CycL [3], KIF [4] and RDF [5].
Ontology Schema that plays a major role in data Since the ontology schema defines the structure and
mapping. the semantics of data (Figure 2) it is understandable
that there is a need for the schema in the extraction
2.1 Data sources process. The ontology is used to create mappings
between data sources and the schema. Another
The data sources define the scope of the integration important role of the ontology schema is to define the
system, thus data source diversity provides a wider query specification process.
integration range and data visibility. S2S middleware
can connect to B2B traditional data source formats,
such as structured (e.g. relational databases), semi-
rules. The last step maps the attribute with the
extraction rule.
Figure 2 –Ontology schema example
2.3 Mapping Module
To enable the extraction from distributed and
heterogeneous sources it is necessary to formally
denote the notion of mapping between remote data and
the local ontology. The mapping is the result of
information crossing between the ontology schema and
the data sources in order to provide information about
ontology’s attributes in the extraction process.
Depending on data source characteristics, two data
extraction scenarios may emerge. This is because data
sources might have ‘one’ data record (for instance a
Web page describing a watch) or might have ‘n’ data
records (for instance a database of watches). The data
Figure 3 – Attribute Registration
source scenario defines how the mapping is made and
how data is extracted (in order to support the existence
Step 1 – Attribute Naming
of an infinite number of records).
The mapping information is supported by attributes;
According to our approach, the mapping procedures
therefore the ontology (Figure 2) must have a
are carried out manually. This task is time consuming
corresponding extraction rule for all of its attributes.
but offers the highest degree of data extraction
The mapping is based on ontology attributes rather than
accuracy and domain consistency. This fact is very
classes. The mapping system first selects a unique
important when integrating data since the integrity and
identifier for each attribute as shown in Figure 4.
correlation between the sources and the ontology must
be very accurate so that the “meaning” of the data is
not lost. Although time consuming, the mapping should
not need substantial maintenance after being created.
Data sources do not normally change their structures
(except perhaps Web pages), so few mapping updates
Figure 4 – Attribute naming
should be necessary.
Besides having a unique ID to each attribute (since
2.3.1 Attribute registration. In order to register an
an attribute name may occur in more than one class), it
attribute we need information about the ontology
is possible to have a path to the attributes (through the
schema and how to extract the information from a
ontology classes) keeping a notion of the ontology
specific data source. The objective is to have a
hierarchy. We will see that this information is very
mapping specification that relates information about
important to instantiate the ontology with the extracted
attributes, data sources and extraction rules.
data.
Figure 3 illustrates the attribute registration process.
In the example the data source is a Web page, so the
Step 2 – Extraction Rules
extraction rules were set using a Web extraction
Each attribute is associated to an extraction rule.
language. The attribute registration process requires a
Extraction rules are basically a segment of code that
set of steps to be completed in order to achieve a
allows taking out the necessary data from the data
correct mapping. The first step is to name the
source and filling a given attribute. These rules are
attributes. The second step is to define the extraction
written according to the data source type. For XML
data sources, XPath and XQuery can be used. For sources repository) and how to extract data from them
databases, the clear option is to use SQL. In the (in the attribute repository). Therefore all mapping
attribute registration example from Figure 3, the data requisites are fulfilled, now data extraction may take
source was a Web page so the extraction rules were place
defined in a Web extraction language (WebL [6]).
Other languages or wrappers (e.g. W4F [7], Caméléon 2.3.2 Register data sources. Data sources often need
[8]) can be used. specific connection information. Data source
In the example, in Figure 3, the data source is a Web connection information must be specified to every data
page structured with HTML tags with the following source used in S2S middleware. The information varies
information: by data source type. For example, Web pages require
… URLs, files require paths, and databases require
location, login, password, and driver type. Registering
Seiko Men's Automatic Dive Watch
 data sources separately from the extraction rules is
…
useful to create a centralized connection information
store, allowing reuse and preventing information
The following segment of code illustrates an
redundancy.
extraction rule written in WebL for extracting of the
watch brand from the HTML data source. The code
2.4 Extractor Manager
connects to the Web site using its URL, retrieves the
page and using a set of regular expressions, extracts the
This component handles data sources for retrieving
watch brand.
the raw data to accomplish query requirements. The
extraction method varies by data source so the
var P = GetURL("www.amazon.com/watches...");
var pText = Text(P); extractor must support several extraction methods. The
var regexpr = "" + `[0-9a-zA-Z']+`; extractor and mapping architecture were designed in
var St = Str_Search(pText, regexpr);
order to be easily extended to support other extraction