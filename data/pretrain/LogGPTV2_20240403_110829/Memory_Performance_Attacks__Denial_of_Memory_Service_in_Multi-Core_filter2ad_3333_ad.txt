reset. The more difﬁcult part of maintaining an accurate
account of eLi(β) can be done as follows: At all times,
maintain for each active thread i and for each bank the
row that would currently be in the row-buffer if i had
been the only thread using the DRAM memory system.
This can be done by simulating an FR-FCFS priority
scheme for each thread and bank that ignores all requests
issued by threads other than i. The e`k
i,b latency of each
request Rk
i,b then corresponds to the latency this request
would have caused if DRAM memory was not shared.
Whenever a request is served, the memory controller can
add this “ideal latency” to the corresponding eLi,b(β) of
that thread and–if necessary–update the simulated state
of the row-buffer accordingly. For instance, assume that
a request Rk
i,b is served, but results in a row conﬂict. As-
sume further that the same request would have been a
row hit, if thread i had run by itself, i.e., Rk−1
accesses
the same row as Rk
i,b. In this case, eLi,b(β) is increased
by row-hit latency Thit, whereas Li,b(β) is increased by
the bank-conﬂict latency Tconf. By thus “simulating”
its own execution for each thread, the memory controller
obtains accurate information for all eLi,b(β).
The obvious problem with the above implementation
is that it is expensive in terms of hardware overhead.
It requires maintaining at least one counter for each
core×bank pair. Similarly severe, it requires one di-
vider per core in order to compute the value χi(β) =
i,b
USENIX Association
16th USENIX Security Symposium
267
Li(β)/eLi(β) for the thread that is currently running on
that core in every memory cycle. Fortunately, much
less expensive hardware implementations are possible
because the memory controller does not need to know
the exact values of Li,b and eLi,b at any given moment.
Instead, using reasonably accurate approximate values
sufﬁces to maintain an excellent level of fairness and se-
curity.
Reduce counters by sampling: Using sampling tech-
niques, the number of counters that need to be main-
tained can be reduced from O(#Banks × #Cores) to
O(#Cores) with only little loss in accuracy. Brieﬂy, the
idea is the following. For each core and its active thread,
we keep two counters Si and Hi denoting the number of
samples and sampled hits, respectively. Instead of keep-
ing track of the exact row that would be open in the row-
buffer if a thread i was running alone, we randomly sam-
ple a subset of requests Rk
i,b issued by thread i and check
whether the next request by i to the same bank, Rk+1
i,b , is
for the same row. If so, the memory controller increases
both Si and Hi, otherwise, only Si is increased. Requests
Rq
i,b and
Rk+1
are ignored. Finally, if none of the Q requests of
thread i following Rk
i,b go to bank b, the sample is dis-
carded, neither Si nor Hi is increased, and a new sam-
ple request is taken. With this technique, the probability
Hi/Si that a request results in a row hit gives the memory
controller a reasonably accurate picture of each thread’s
row-buffer locality. An approximation of eLi can thus be
maintained by adding the expected amortized latency to
it whenever a request is served, i.e.,
eLnew
i,b0 to different banks b0 6= b served between Rk
i + (Hi/Si · Thit + (1 − Hi/Si) · Tconf ) .
:= eLold
i,b
i
The
ideal
scheme
Reuse dividers:
employs
O(#Cores) hardware dividers, which signiﬁcantly
increases the memory controller’s energy consumption.
Instead, a single divider can be used for all cores by
assigning individual
in a round robin
fashion. That is, while the slowdowns Li(β) and eLi(β)
can be updated in every memory cycle, their quotient
χi(β) is recomputed in intervals.
threads to it
6 Evaluation
6.1 Experimental Methodology
We evaluate our solution using a detailed processor
and memory system simulator based on the Pin dy-
namic binary instrumentation tool [20]. Our in-house
instruction-level performance simulator can simulate ap-
plications compiled for the x86 instruction set architec-
ture. We simulate the memory system in detail using
a model loosely based on DRAMsim [36]. Both our
processor model and the memory model mimic the de-
sign of a modern high-performance dual-core proces-
sor loosely based on the Intel Pentium M [11]. The
size/bandwidth/latency/capacity of different processor
structures along with the number of cores and other
structures are parameters to the simulator. The simulator
faithfully models the bandwidth, latency, and capacity
of each buffer, bus, and structure in the memory subsys-
tem (including the caches, memory controller, DRAM
buses, and DRAM banks). The relevant parameters of
the modeled baseline processor are shown in Table 1.
Unless otherwise stated, all evaluations in this section are
performed on a simulated dual-core system using these
parameters. For our measurements with the FairMem
system presented in Section 5, the parameters are set to
α = 1.025 and β = 105.
We simulate each application for 100 million x86 in-
structions. The portions of applications that are sim-
ulated are determined using the SimPoint tool [32],
which selects simulation points in the application that
are representative of the application’s behavior as a
whole. Our applications include stream and rdarray (de-
scribed in Section 3), several large benchmarks from the
SPEC CPU2000 benchmark suite [34], and one memory-
intensive benchmark from the Olden suite [31]. These
applications are described in Table 2.
6.2 Evaluation Results
6.2.1 Dual-core Systems
Two microbenchmark applications - stream and
rdarray: Figure 7 shows the normalized execution
time of stream and rdarray applications when run alone
or together using either the baseline FR-FCFS or our
FairMem memory scheduling algorithms. Execution
time of each application is normalized to the execution
time they experience when they are run alone using the
FR-FCFS scheduling algorithm (This is true for all nor-
malized results in this paper). When stream and rdarray
are run together on the baseline system, stream—which
acts as an MPH—experiences a slowdown of only 1.22X
whereas rdarray slows down by 2.45X. In contrast, a
memory controller that uses our FairMem algorithm pre-
vents stream from behaving like an MPH against rdarray
– both applications experience similar slowdowns when
run together. FairMem does not signiﬁcantly affect per-
formance when the applications are run alone or when
run with identical copies of themselves (i.e. when mem-
ory performance is not unfairly impacted). These exper-
iments show that our simulated system closely matches
the behavior we observe in an existing dual-core system
(Figure 4), and that FairMem successfully provides fair-
ness among threads. Next, we show that with real appli-
cations, the effect of an MPH can be drastic.
Effect on real applications: Figure 8 shows the normal-
ized execution time of 8 different pairs of applications
when run alone or together using either the baseline FR-
268
16th USENIX Security Symposium
USENIX Association
Processor pipeline
Fetch/Execute width per core
L1 Caches
L2 Caches
Memory controller
DRAM parameters
DRAM latency (round-trip L2 miss latency)
4 GHz processor, 128-entry instruction window, 12-stage pipeline
3 instructions can be fetched/executed every cycle; only 1 can be a memory operation
32 K-byte per-core, 4-way set associative, 32-byte block size, 2-cycle latency
512 K-byte per core, 8-way set associative, 32-byte block size, 12-cycle latency
128 request buffer entries, FR-FCFS baseline scheduling policy, runs at 2 GHz
8 banks, 2K-byte row-buffer
row-buffer hit: 50ns (200 cycles), closed: 75ns (300 cycles), conﬂict: 100ns (400 cycles)
Table 1: Baseline processor conﬁguration
Brief description
Suite
Microbenchmark Streaming on 32-byte-element arrays
Microbenchmark Random access on arrays
Benchmark
stream
rdarray
small-stream Microbenchmark Streaming on 4-byte-element arrays
art
Object recognition in thermal image
crafty
health
mcf
vpr
SPEC 2000 FP
SPEC 2000 INT Chess game
Olden
SPEC 2000 INT Single-depot vehicle scheduling
SPEC 2000 INT FPGA circuit placement and routing
Columbian health care system simulator
Base performance L2-misses per 1K inst.
46.30 cycles/inst.
56.29 cycles/inst.
13.86 cycles/inst.
7.85 cycles/inst.
0.64 cycles/inst.
7.24 cycles/inst.
4.73 cycles/inst.
1.71 cycles/inst.
629.65
629.18
71.43
70.82
0.35
83.45
45.95
5.08
row-buffer hit rate
96%
3%
97%
88%
15%
27%
51%
14%
Table 2: Evaluated applications and their performance characteristics on the baseline processor
baseline (FR-FCFS)
FairMem
STREAM
2.5
2.0
1.5
1.0
0.5
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
baseline (FR-FCFS)
FairMem
RDARRAY
2.5
2.0
1.5
1.0
0.5
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
stream alone
0.0
Figure 7: Slowdown of (a) stream and (b) rdarray benchmarks using FR-FCFS and our FairMem algorithm
with another rdarray
with another stream
rdarray alone
with rdarray
0.0
with stream
FCFS or FairMem. The results show that 1) an MPH can
severely damage the performance of another application,
and 2) our FairMem algorithm is effective at preventing
it. For example, when stream and health are run together
in the baseline system, stream acts as an MPH slowing
down health by 8.6X while itself being slowed down by
only 1.05X. This is because it has 7 times higher L2 miss
rate and much higher row-buffer locality (96% vs. 27%)
— therefore, it exploits unfairness in both row-buffer-
hit ﬁrst and oldest-ﬁrst scheduling policies by ﬂooding
the memory system with its requests. When the two
applications are run on our FairMem system, health’s
slowdown is reduced from 8.63X to 2.28X. The ﬁgure
also shows that even regular applications with high row-
buffer locality can act as MPHs. For instance when art
and vpr are run together in the baseline system, art acts as
an MPH slowing down vpr by 2.35X while itself being
slowed down by only 1.05X. When the two are run on
our FairMem system, each slows down by only 1.35X;
thus, art is no longer a performance hog.
Effect on Throughput and Unfairness: Table 3 shows
the overall throughput (in terms of executed instructions
per 1000 cycles) and DRAM unfairness (relative dif-
ference between the maximum and minimum memory-
related slowdowns, deﬁned as Ψ in Section 4) when dif-
ferent application combinations are executed together. In
all cases, FairMem reduces the unfairness to below 1.20
(Remember that 1.00 is the best possible Ψ value). In-
terestingly, in most cases, FairMem also improves over-
all throughput signiﬁcantly. This is especially true when
a very memory-intensive application (e.g.stream) is run
with a much less memory-intensive application (e.g.vpr).
Providing fairness leads to higher overall system
throughput because it enables better utilization of the
cores (i.e. better utilization of the multi-core system).
The baseline FR-FCFS algorithm signiﬁcantly hinders
the progress of a less memory-intensive application,
whereas FairMem allows this application to stall less
due to the memory system, thereby enabling it to make
fast progress through its instruction stream. Hence,
rather than wasting execution cycles due to unfairly-
induced memory stalls, some cores are better utilized
with FairMem.11 On the other hand, FairMem re-
duces the overall throughput by 9% when two extremely
memory-intensive applications,stream and rdarray, are
run concurrently. In this case, enforcing fairness reduces
stream’s data throughput without signiﬁcantly increas-
ing rdarray’s throughput because rdarray encounters L2
cache misses as frequently as stream (see Table 2).
11Note that the data throughput obtained from the DRAM itself may
be, and usually is reduced using FairMem. However, overall through-
put in terms of instructions executed per cycle usually increases.
USENIX Association
16th USENIX Security Symposium
269
)
e
n
o
l
a
i
g
n
n
n
u
r
:
e
s
a
b
(
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
)
e
n
o
l
a
i
g
n
n
n
u
r
:
e
s
a
b
(
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
3.0
2.5
2.0
1.5
1.0
0.5
0.0
9.0
8.5
8.0
7.5
7.0
6.5
6.0
5.5
5.0
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
art
vpr
baseline (FR-FCFS)
FairMem
stream
vpr
baseline (FR-FCFS)
FairMem
)
e
n
o
l
a
i