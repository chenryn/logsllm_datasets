7.27
6.40 %
500Hz-
7000Hz
20/20
8.45
6.50 %
500Hz-
5000Hz
17/20
7.90
6.33 %
500Hz-
3000Hz
12/20
7.39
7.09 %
amples based on audio ﬁles containing music and bird sounds.
The results are presented in Table 3.
We can repeat our observations from the previous experi-
ment. When we utilize a more aggressive ﬁlter, we observe
that the perturbation energy of adversarial examples increases
with respect to the baselines by up to 24.08 dB (birds) and
21.32 dB (music). Equally, the attack’s general success de-
creases to 5/50 (birds) and 3/50 (music) successful adversarial
examples.
Note that the SNRseg for music samples are in general
higher than that of speech and bird ﬁles as these samples have
a more dynamic range of signal energy. Hence, potentially
added adversarial perturbations have a smaller impact on the
calculation of the SNRseg. The absolute amount of added
perturbations, however, is similar to that of other content.
Thus, when listening to the created adversarial examples2 the
samples are similarly distorted. This is further conﬁrmed in
Section 4.4 with our listening test.
4.3.2 Target Phone Rate
The success of the attack depends on the ratio between the
length of the audio ﬁle and the length of the target text, which
we refer to as the target phone rate. This rate describes how
2 rub-syssec.github.io/dompteur
Figure 6: Word Error Rate (WER) and Segmental Signal-
to-Noise (SNRseg) ratio for different phone rates. We re-
port the mean and std. deviation for adversarial examples
computed for targets with varying length.
many phones an attacker can hide within one second of audio
content.
In our experiments, we used the default ratios recom-
mended by Schönherr et al. However, a better rate might
exist for our setting. Therefore, to evaluate the effect of the
target phone rate, we sample target texts of varying lengths
from the WSJ corpus and compute adversarial examples for
different target phone rates. We pick phone rates ranging
from 1 to 20 and run 20 attacks for each of them for at most
1000 iterations, resulting in 400 attacks.
The results in Figure 6 show that, in general, with increas-
ing phone rates, the SNRseg decreases and stagnates for target
phone rate beyond 12. This is expected as the attacker tries
to hide more phones and, consequently, needs to change the
signal more drastically. Thus, we conclude that the default
settings are adequate for our setting.
4.3.3 Band-Pass Cut-off Frequencies
So far, we only considered a relatively wide band-pass ﬁlter
(200-7000 Hz). We also want to investigate other cut-off
frequencies. Thus, we disable the psychoacoustic ﬁltering and
compute adversarial examples for different models examined
in Section 4.2. We run the attack for each band-pass model
with 20 speech samples for at most 1000 iterations.
The results are reported in Table 4. We observe that the
energy amount of adversarial perturbation remains relatively
constant for different ﬁlters, which is expected since the at-
tacker has complete knowledge of the system. As we narrow
the frequency band, the attacker adopts and puts more pertur-
bation within these bands.
Apart from the SNRseg, we also observe a decrease in
the attack success, especially for small high cut-off frequen-
cies, with only 11/20 (300-3000 Hz) and 12/20 (500-3000 Hz)
successful adversarial examples.
2318    30th USENIX Security Symposium
USENIX Association
5101520PhoneRate(phones/s)0.020.040.060.080.0100.0WER(%)WERΦ=0SNRsegΦ=0−50510SNRseg(dB)Table 5: Regression results for perceived sound quality
predicted by different audio stimuli. The dependent vari-
able is the quality score assigned to each audio stimulus.
We trained three different models, one for each data set
(speech/music/bird). Each model consists of two steps, with
the ﬁrst step entering the audio stimulus as a predictor and the
second step entering type of device as a covariate. All mod-
els include the control variables gender, age, and language.
All regressions use ordinary least squares. Cluster adjusted
standard errors are indicated in parentheses. The R2 values
indicate the percentage of the variance of the perceived sound
quality explained by the respective audio stimuli.
Speech
Music
Bird
Step 1
Step 2
Step 1
Step 2
Step 1
Step 2
Audio
stimulus
Device
Controls
Obs.
R2
-.905**
(.131)
-.905**
(.131)
.030**
(.473)
Included
4259
-.871**
(.166)
-.871**
(.166)
.008
(.597)
-.830**
(.171)
-.830**
(.171)
.045**
(.615)
Included
4259
Included
4259
.820
.821
.760
.761
.690
.692
P-value  .80) for the main effect of univariate analyses of
variance (UNIANOVA) among six experimental conditions
and a signiﬁcance level of α = .05.
We used Amazon MTurk to recruit 355 participants (µage =
41.61 years, σage = 10.96; 56.60% female). Participants were
only allowed to use a computer and no mobile device. How-
ever, they were free to use headphones or speakers as long
as they indicated what type of listening device was used. To
ﬁlter individuals who did not meet the technical requirements
needed, or did not understand or follow the instructions, we
used a control question to exclude all participants who failed
to distinguish the anchor from the reference correctly.
In the main part of
the experiment, participants
were presented with six different audio sets (2 of each:
speech/bird/music), each of which contained six audio stimuli
varying in sound quality. After listening to each sound, they
were asked to rank the individual stimulus by its perceived
USENIX Association
30th USENIX Security Symposium    2319
sound quality. After completing of the tasks, participants
answered demographic questions, were debriefed (MTurk
default), and compensated with 3.00 USD. The participant re-
quired on average approximately 20 minutes to ﬁnish the test.
In a ﬁrst step, we ﬁrst use an UNIANOVA to examine
whether there is a signiﬁcant difference between the six au-
dio stimuli and the perceived sound quality. Our analysis
reveals a signiﬁcant main effect of the audio stimulus on the
perceived sound quality, F(5,12780) = 8335.610, p  1% for our p-value and
an effect size of η2 > .5, our result shows a high experimental
signiﬁcance [58]. Thus, we can conclude that DOMPTEUR
indeed forces adversarial perturbations into the perceptible
acoustic range of human listeners.
To examine whether the effect remains stable across differ-
ent audio samples and listening devices, we further conducted
multiple regression analyses. We entered the audio stimuli as
our main predictors (ﬁrst step) and the type of device (second
step) as covariates for each analysis. Our results remain sta-
ble across all audio types. The highest predictive power was
found in the speech sets, where 82.1% of the variance is ex-
plained by our regression model, followed by music (76.1%)
and bird sets (69.2%) (see Table 5 for details). Moreover,
we found a small yet signiﬁcant positive coefﬁcient for the
type of device used across all audio types. This ﬁnding sug-
gests that headphone users generally indicate higher quality
rankings, potentially due to better sound perceptions. The
results with listening device speaker are presented in Figure
7. Importantly, all results remain stable across the control
variables of age, gender, and ﬁrst language.
In conclusion, the results strongly support our hypothesis
that DOMPTEUR forces the attacker into the audible range,
making the attack clearly noticeable for human listeners.
5 Related Work
In this section, we summarize research related to our work,
surveying recent attacks and countermeasures.
Audio Adversarial Examples Carlini and Wagner [59] in-
troduced targeted audio adversarial examples for ASR sys-
tems. For the attack, they assume a white-box attacker and
use an optimization-based method to construct general adver-
sarial examples for arbitrary target phrases against the ASR
system DEEPSPEECH [32].
Similarly, Schönherr et al. [17] and Yuan et al. [16] have
proposed an attack against the KALDI [35] toolkit. Both
assume a white-box attacker and also use optimization-based
methods to ﬁnd adversarial examples. Furthermore, the attack
from Schönherr et al. [17] can optionally compute adversarial
examples that are especially unobtrusive for human listeners.
Alzantot et al. [60] proposed a black-box attack, which
does not require knowledge about the model. For this, the au-
thors have used a genetic algorithm to create their adversarial
(a) Speech
(b) Music
(c) Birds
Figure 7: Ratings of participants with listening device
speaker. In the user study, we tested six audio samples, di-
vided into two samples each of spoken content, music and
bird twittering.
examples for a keyword spotting system. Khare et al. [61]
proposed a black-box attack based on evolutionary optimiza-
tion, and also Taori et al. [62] presented a similar approach in
their paper.
Recently, Chen et al. [63] and Schönherr et al. [18] pub-
lished works where they can calculate over-the-air attacks,
where adversarial examples are optimized such that these re-
main viable if played via a loudspeaker by considering room
characteristics.
Aghakhani et al. [64] presented another line of attack,
namely a poisoning attack against ASR systems.
In con-
trast to adversarial examples, these are attacks against the
training set of a machine learning system, with the target to
manipulate the training data s.t a model that is trained with
the poisoned data set misclassiﬁes speciﬁc inputs.
Abdullah et al. [19] provides a detailed overview of existing
attacks in their systemization of knowledge on attacks against
speech systems.
2320    30th USENIX Security Symposium
USENIX Association
ReferenceBaselineΦ=0Φ=6Φ=12Anchor0255075100MUSHRA-PointsSpeechSet1ReferenceBaselineΦ=0Φ=6Φ=12AnchorSpeechSet2ReferenceBaselineΦ=0Φ=6Φ=12Anchor0255075100MUSHRA-PointsMusicSet1ReferenceBaselineΦ=0Φ=6Φ=12AnchorMusicSet2ReferenceBaselineΦ=0Φ=6Φ=12Anchor0255075100MUSHRA-PointsBirdsSet1ReferenceBaselineΦ=0Φ=6Φ=12AnchorBirdsSet2Countermeasures There is a long line of research about
countermeasures against adversarial examples in general and
especially in the image domain (e. g., [23–25]), but most of
the proposed defenses were shown to be broken once an at-
tacker is aware of the employed mechanism. In fact, due to
the difﬁculty to create robust adversarial example defenses,
Carlini et al. proposed guidelines for the evaluation of ad-
versarial robustness. They list all important properties of a
successful countermeasure against adversarial examples [30].
Compared to the image domain, defenses against audio adver-
sarial examples remained relatively unnoticed so far. For the
audio domain, only a few works have investigated possible
countermeasures. Moreover, these tend to focus on speciﬁc
attacks and not adaptive attackers.
Ma et al. [65] describe how the correlation of audio and
video streams can be used to detect adversarial examples
for an audiovisual speech recognition task. However, all of
these simple approaches—while reasonable in principle—are
speciﬁcally trained for a deﬁned set of attacks, and hence an
attacker can easily leverage that knowledge as demonstrated
repeatedly in the image domain [25].
Zeng et al. [66] proposed an approach inspired by multi-
version programming. Therefore, the authors combine the
output of multiple ASR systems and calculate a similarity
score between the transcriptions. If these differ too much,
the input is assumed to be an adversarial example. The secu-
rity of this approach relies on the property that current audio