 10
 100
Max-Trough for Core Link
Figure 15: Difference between the peak and trough utilization.
7.
IMPLICATIONS FOR DATA CENTER
DESIGN
7.1 Role of Bisection Bandwidth
Several proposals [1, 22, 11, 2] for new data center network ar-
chitectures attempt to maximize the network bisection bandwidth.
These approaches, while well suited for data centers, which run
applications that stress the network’s fabric with all-to-all trafﬁc,
would be unwarranted in data centers where the bisection band-
width is not taxed by the applications. In this section, we re-evaluate
the SNMP and topology data captured from the 10 data centers and
examine whether the prevalent trafﬁc patterns are likely to stress the
existing bisection bandwidth. We also examine how much of the
existing bisection bandwidth is needed at any given time to support
the prevalent trafﬁc patterns.
Before explaining how we address these questions, we provide
a few deﬁnitions. We deﬁne the bisection links for a tiered data
center to be the set of links at the top-most tier of the data center’s
tree architecture; in other words, the core links make up the bisec-
tion links. The bisection capacity is the aggregate capacity of these
links. The full bisection capacity is the capacity that would be re-
quired to support servers communicating at full link speeds with
arbitrary trafﬁc matrices and no oversubscription. The full bisec-
Current
Full
d
e
z
i
l
i
t
U
n
o
i
t
c
e
s
B
i
f
o
t
n
e
c
e
r
P
0
3
0
2
0
1
0
1
D
L
C
2
D
L
C
3
D
L
C
4
D
L
C
5
D
L
C
1
U
D
E
2
U
D
E
3
U
D
E
1
V
R
P
2
V
R
P
Data Center
Figure 16: The ﬁrst bar is the ratio of aggregate server trafﬁc
over Bisection BW and the second bar is the ratio of aggregate
server trafﬁc over full bisection capacity. The y-axis displays
utilization as a percentage.
tion capacity can be computed as simply the aggregate capacity of
the server NICs.
Returning to the questions posed earlier in this section, we use
SNMP data to compute the following: (1) the ratio of the current
aggregate server-generated trafﬁc to the current bisection capacity
and (2) the ratio of the current trafﬁc to the full bisection capacity.
In doing so, we make the assumption that the bisection links can
be treated as a single pool of capacity from which all offered trafﬁc
can draw. While this may not be true in all current networks, it
allows us to determine whether more capacity is needed or rather
better use of existing capacity is needed (for example, by improving
routing, topology, or the migration of application servers inside the
data center).
In Figure 16, we present these two ratios for each of the data
centers studied. Recall (from Table 2) that all data centers are over-
subscribed, meaning that if all servers sent data as fast as they can
and all trafﬁc left the racks, then the bisection links would be fully
congested (we would expect to ﬁnd utilization ratios over 100%).
However, we ﬁnd in Figure 16 that the prevalent trafﬁc patterns are
such that, even in the worst case where all server-generated trafﬁc
is assumed to leave the rack hosting the server, the aggregate output
from servers is smaller than the network’s current bisection capac-
ity. This means even if the applications were moved around and
the trafﬁc matrix changed, the current bisection would still be more
than sufﬁcient and no more than 25% of it would be utilized across
all data centers, including the MapReduce data centers. Finally, we
note that the aggregate output from servers is a negligible fraction
of the ideal bisection capacity in all cases. This implies that should
these data centers be equipped with a network that provides full bi-
section bandwidth, at least 95% of this capacity would go unused
and be wasted by today’s trafﬁc patterns.
Thus, the prevalent trafﬁc patterns in the data centers can be sup-
ported by the existing bisection capacity, even if applications were
placed in such a way that there was more inter-rack trafﬁc than
exists today. This analysis assumes that the aggregate capacity of
the bisection links forms a shared resource pool from which all
offered trafﬁc can draw. If the topology prevents some offered traf-
ﬁc from reaching some links, then some links can experience high
utilization while others see low utilization. Even in this situation,
however, the issue is one of changing the topology and selecting
a routing algorithm that allows offered trafﬁc to draw effectively
278from the existing capacity, rather than a question of adding more
capacity. Centralized routing, discussed next, could help in con-
structing the requisite network paths.
7.2 Centralized Controllers in Data Centers
The architectures for several proposals [1, 22, 12, 2, 14, 21, 4,
18, 29] rely in some form or another on a centralized controller
for conﬁguring routes or for disseminating routing information to
endhosts. A centralized controller is only practical if it is able to
scale up to meet the demands of the trafﬁc characteristics within the
data centers. In this section, we examine this issue in the context of
the ﬂow properties that we analyzed in Section 5.
In particular, we focus on the proposals (Hedera [2], MicroTE [4]
and ElasticTree [14]) that rely on OpenFlow and NOX [15, 23]. In
an OpenFlow architecture, the ﬁrst packet of a ﬂow, when encoun-
tered at a switch, can be forwarded to a central controller that deter-
mines the route that the packet should follow in order to meet some
network-wide objective. Alternatively, to eliminate the setup delay,
the central controller can precompute a set of network paths that
meet network-wide objectives and install them into the network at
startup time.
Our empirical observations in Section 5, have important implica-
tions for such centralized approaches. First, the fact that the number
of active ﬂows is small (see Figure 4(a)) implies that switches en-
abled with OpenFlow can make do with a small ﬂow table, which
is a constrained resource on switches today.
Second, ﬂow inter-arrival times have important implications for
the scalability of the controller. As we observed in Section 5, a sig-
niﬁcant number of new ﬂows (2–20%) can arrive at a given switch
within 10µs of each other. The switch must forward the ﬁrst pack-
ets of these ﬂows to the controller for processing. Even if the data
center has as few as a 100 edge switches, in the worst case, a con-
troller can see 10 new ﬂows per µs or 10 million ﬂows per sec-
ond. Depending on the complexity of the objective implemented at
the controller, computing a route for each of these ﬂows could be
expensive. For example, prior work [5] showed a commodity ma-
chine computing a simple shortest path for only 50K ﬂow arrivals
per second. Thus, to scale the throughput of a centralized con-
trol framework while supporting complex routing objectives, we
must employ parallelism (i.e., use multiple CPUs per controller and
multiple controllers) and/or use faster but less optimal heuristics to
compute routes. Prior work [28] has shown, through parallelism,
the ability of a central controller to scale to 20 million ﬂows per
second.
Finally, the ﬂow duration and size also have implications for the
centralized controller. The lengths of ﬂows determine the relative
impact of the latency imposed by a controller on a new ﬂow. Recall
that we found that most ﬂows last less than 100ms. Prior work [5]
showed than it takes reactive controllers, which make decisions at
ﬂow start up time, approximately 10ms to install ﬂow entries for
new ﬂows. Given our results, this imposes a 10% delay overhead
on most ﬂows. Additional processing delay may be acceptable
for some trafﬁc, but might be unacceptable for other kinds. For
the class of workloads that ﬁnd such a delay unacceptable, Open-
Flow provides a proactive mechanism that allows the controllers,
at switch start up time, to install ﬂow entries in the switches. This
proactive mechanism eliminates the 10ms delay but limits the con-
troller to proactive algorithms.
In summary, it appears the number and inter-arrival time of data
center ﬂows can be handled by a sufﬁciently parallelized imple-
mentation of the centralized controller. However, the overhead of
reactively computing ﬂow placements is a reasonable fraction of
the length of the typical ﬂow.
8. SUMMARY
In this paper, we conducted an empirical study of the network
trafﬁc of 10 data centers spanning three very different categories,
namely university campus, private enterprise data centers, and cloud
data centers running Web services, customer-facing applications,
and intensive Map-Reduce jobs. To the best of our knowledge, this
is the broadest-ever large-scale measurement study of data centers.
We started our study by examining the applications run within
the various data centers. We found that a variety of applications
are deployed and that they are placed non-uniformly across racks.
Next, we studied the transmission properties of the applications in
terms of the ﬂow and packet arrival processes at the edge switches.
We discovered that the arrival process at the edge switches is
ON/OFF in nature where the ON/OFF durations can be character-
ized by heavy-tailed distributions. In analyzing the ﬂows that con-
stitute these arrival process, we observed that ﬂows within the data
centers studied are generally small in size and several of these ﬂows
last only a few milliseconds.
We studied the implications of the deployed data center applica-
tions and their transmission properties on the data center network
and its links. We found that most of the server generated trafﬁc in
the cloud data centers stays within a rack, while the opposite is true
for campus data centers. We found that at the edge and aggrega-
tion layers, link utilizations are fairly low and show little variation.
In contrast, link utilizations at the core are high with signiﬁcant
variations over the course of a day. In some data centers, a small
but signiﬁcant fraction of core links appear to be persistently con-
gested, but there is enough spare capacity in the core to alleviate
congestion. We observed losses on the links that are lightly uti-
lized on average and argued that these losses can be attributed to
the bursty nature of the underlying applications run within the data
centers.
On the whole, our empirical observations can help inform data
center trafﬁc engineering and QoS approaches, as well as recent
techniques for managing other resources, such as data center net-
work energy consumption. To further highlight the implications of
our study, we re-examined recent data center proposals and archi-
tectures in light of our results. In particular, we determined that full
bisection bandwidth is not essential for supporting current applica-
tions. We also highlighted practical issues in successfully employ-
ing centralized routing mechanisms in data centers.
Our empirical study is by no means all-encompassing. We rec-
ognize that there may be other data centers in the wild that may or
may not share all the properties that we have observed. Our work
points out that it is worth closely examining the different design and
usage patterns, as there are important differences and commonali-
ties.
9. ACKNOWLEDGMENTS
We would like to thank the operators at the various universities,
online services providers, and private enterprises for both the time
and data that they provided us. We would also like to thank the
anonymous reviewers for their insightful feedback.
This work is supported in part by an NSF FIND grant (CNS-
0626889), an NSF CAREER Award (CNS-0746531), an NSF NetSE
grant (CNS-0905134), and by grants from the University of
Wisconsin-Madison Graduate School. Theophilus Benson is sup-
ported by an IBM PhD Fellowship.
10. REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable,
commodity data center network architecture. In SIGCOMM,
pages 63–74, 2008.
279[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, W. College,
[15] NOX: An OpenFlow Controller.
N. Huang, and A. Vahdat. Hedera: Dynamic ﬂow scheduling
for data center networks. In Proceedings of NSDI 2010, San
Jose, CA, USA, April 2010.
[3] T. Benson, A. Anand, A. Akella, and M. Zhang.
Understanding Data Center Trafﬁc Characteristics. In
Proceedings of Sigcomm Workshop: Research on Enterprise
Networks, 2009.
[4] T. Benson, A. Anand, A. Akella, and M. Zhang. The case for
ﬁne-grained trafﬁc engineering in data centers. In
Proceedings of INM/WREN ’10, San Jose, CA, USA, April
2010.
http://noxrepo.org/wp/.
[16] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu. Dcell: a
scalable and fault-tolerant network structure for data centers.
In SIGCOMM ’08: Proceedings of the ACM SIGCOMM
2008 conference on Data communication, pages 75–86, New
York, NY, USA, 2008. ACM.
[17] W. John and S. Tafvelin. Analysis of Internet backbone
trafﬁc and header anomalies observed. In IMC ’07:
Proceedings of the 7th ACM SIGCOMM conference on
Internet measurement, pages 111–116, New York, NY, USA,
2007. ACM.
[5] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown,
and S. Shenker. Ethane: taking control of the enterprise. In
SIGCOMM, 2007.
[18] S. Kandula, J. Padhye, and P. Bahl. Flyways to de-congest
data center networks. In Proc. ACM Hotnets-VIII, New York
City, NY. USA., Oct. 2009.
[6] J. Dean and S. Ghemawat. MapReduce: simpliﬁed data
processing on large clusters. volume 51, pages 107–113,
New York, NY, USA, 2008. ACM.
[7] A. B. Downey. Evidence for long-tailed distributions in the
internet. In In Proceedings of ACM SIGCOMM Internet
Measurment Workshop, pages 229–241. ACM Press, 2001.
[8] M. Fomenkov, K. Keys, D. Moore, and K. Claffy.
Longitudinal study of Internet trafﬁc in 1998-2003. In
WISICT ’04: Proceedings of the Winter International
Symposium on Information and Communication
Technologies, pages 1–6. Trinity College Dublin, 2004.
[9] H. J. Fowler, W. E. Leland, and B. Bellcore. Local area
network trafﬁc characteristics, with implications for
broadband network congestion management. IEEE Journal
on Selected Areas in Communications, 9:1139–1149, 1991.
[10] C. Fraleigh, S. Moon, B. Lyles, C. Cotton, M. Khan,
D. Moll, R. Rockell, T. Seely, and C. Diot. Packet-level
trafﬁc measurements from the Sprint IP backbone. IEEE
Network, 17:6–16, 2003.
[19] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and
R. Chaiken. The Nature of Data Center Trafﬁc:
Measurements and Analysis. In IMC, 2009.
[20] W. E. Leland, M. S. Taqqu, W. Willinger, and D. V. Wilson.
On the self-similar nature of ethernet trafﬁc. In SIGCOMM
’93: Conference proceedings on Communications
architectures, protocols and applications, pages 183–193,
New York, NY, USA, 1993. ACM.
[21] J. Mudigonda, P. Yalagandula, M. Al-Fares, and J. C. Mogul.
Spain: Cots data-center ethernet for multipathing over
arbitrary topologies. In Proceedings of NSDI 2010, San Jose,
CA, USA, April 2010.
[22] R. Niranjan Mysore, A. Pamboris, N. Farrington, N. Huang,
P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat.
Portland: a scalable fault-tolerant layer 2 data center network
fabric. In SIGCOMM, 2009.
[23] The OpenFlow Switch Consortium.
http://www.openflowswitch.org/.
[24] V. Paxson. Empirically-Derived Analytic Models of
[11] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim,
Wide-Area TCP Connections. 2(4):316–336, Aug. 1994.
P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: a
scalable and ﬂexible data center network. In SIGCOMM,
2009.
[12] A. Greenberg, P. Lahiri, D. A. Maltz, P. Patel, and
S. Sengupta. Towards a next generation data center
architecture: scalability and commoditization. In PRESTO
’08: Proceedings of the ACM workshop on Programmable
routers for extensible services of tomorrow, pages 57–62,
New York, NY, USA, 2008. ACM.
[13] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian,
Y. Zhang, and S. Lu. BCube: A High Performance,
Server-centric Network Architecture for Modular Data
Centers. In Proceedings of the ACM SIGCOMM 2009
Conference on Data Communication, Barcelona, Spain,
August 17 - 21 2009.
[14] B. Heller, S. Seetharaman, P. Mahadevan, Y. Yiakoumis,
P. Sharma, S. Banerjee, and N. McKeown. Elastictree:
Saving energy in data center networks. April 2010.
[25] V. Paxson. Measurements and analysis of end-to-end internet
dynamics. Technical report, 1997.
[26] V. Paxson. Bro: a system for detecting network intruders in
real-time. In SSYM’98: Proceedings of the 7th conference on
USENIX Security Symposium, pages 3–3, Berkeley, CA,
USA, 1998. USENIX Association.
[27] V. Paxson and S. Floyd. Wide area trafﬁc: the failure of
poisson modeling. IEEE/ACM Trans. Netw., 3(3):226–244,
1995.
[28] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker.
Applying nox to the datacenter. In Proc. of workshop on Hot
Topics in Networks (HotNets-VIII), 2009.
[29] G. Wang, D. G. Andersen, M. Kaminsky, M. Kozuch,
T. S. E. Ng, K. Papagiannaki, M. Glick, and L. Mummert.
Your data center is a router: The case for reconﬁgurable
optical circuit switched paths. In Proc. ACM Hotnets-VIII,
New York City, NY. USA., Oct. 2009.
280