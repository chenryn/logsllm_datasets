a member or not. Yeom et al. [48] and Song et al. [44] ﬁnd
that the metric of prediction conﬁdence of correct label F(x)y
can be compared with a certain threshold value to achieve
similar attack performance as NN-based attacks. Shokri et
al. [41] show a large divergence between prediction entropy
distributions over training data and test data, although this
metric was not explicitly used for attacks.
Despite the existence of such non-NN based attacks, many
research papers [20,31,32] still only train NN attack classiﬁers
to evaluate target models’ privacy risks. We ﬁnd that this
can lead to severe underestimation of privacy risks by re-
evaluating the same target models with non-NN based attacks.
Furthermore, we improve existing non-NN based attacks by
setting different threshold values for different class labels,
building upon the motivation of separated attack classiﬁers
for each class label by Shokri et al. [41]. We also propose
a new inference attack method by considering ground truth
label when evaluating prediction uncertainty.
2.2.2 White-box membership inference attacks
Nasr et al. [32] analyze membership inference attacks in the
white-box setting, where the adversary has the full access
to the target machine learning model and knows the model
architecture and model parameters. They ﬁnd that simply com-
bining target model’s ﬁnal predictions and its intermediate
computations to learn the attack classiﬁer results in attack
accuracy no better than that of the corresponding black-box
attacks. Instead, by using the gradient of prediction loss with
regard to model parameters ∂(cid:96)(Fθ,z)
as additional features, the
white-box membership inference attacks obtain higher attack
accuracy than the black-box attacks. We show that the gap
between white-box attack accuracy and black-box attack ac-
curacy is much smaller than previous estimates in this paper.
∂θ
2.3 Defenses against membership inference at-
tacks
To mitigate the risks of membership inference attacks, sev-
eral defense ideas have been proposed. L2 norm regulariza-
tion [23] and dropout [45] are standard techniques for reduc-
ing overﬁtting in machine learning. They are also shown to
decrease privacy risks to some degree [38, 41]. However, tar-
get models can still be quite vulnerable after applying these
techniques. Differential privacy [9, 10] can also be applied
to ML models for provable risk mitigation [1, 29, 33, 40],
however, it induces signiﬁcant accuracy drop for desired val-
ues of the privacy parameter [19]. Two dedicated defenses,
adversarial regularization [32] and MemGuard [20], were re-
cently proposed against membership inference attacks. Both
defenses are reported to have the ability of decreasing the
attack accuracy to around 50%, which is the performance of
random guessing. We explain their details below.
2.3.1 Adversarial regularization [31]
Nasr et al. [31] propose to include the membership inference
adversary with the NN-based attack into the training process
itself to mitigate privacy risks. At each training step, the attack
classiﬁer is ﬁrst updated to distinguish between training data
(members) and validation data (non-members), and then the
target classiﬁer is updated to simultaneously minimize the
prediction loss and mislead the attack classiﬁer.
More speciﬁcally, to train the classiﬁer F with parameters
θ in a manner that is resilient against membership inference
attacks, Nasr et al. [31] use another classiﬁer I with parame-
ters ϑ to perform membership inference attacks. The attack
classiﬁer I takes the target model’s prediction F(x) and the
input label y as input features and generate one single output
I(F(x),y), which is in the range [0, 1]. It infers the input sam-
ple as a member if the output is larger than 0.5, a non-member
otherwise. At each training step, they ﬁrst update the attack
classiﬁer I by maximizing the membership inference gain
over the training set Dtr and the validation set Dval.
∑z∈Dtr log(I(F(x),y))
+
argmax
ϑ
2|Dtr|
∑z∈Dval log(1− I(F(x),y))
2|Dval|
(2)
They further train the target classiﬁer by minimizing both
model prediction loss and membership inference gain over
the training set Dtr.
argmin
θ
1
|Dtr| ∑
z∈Dtr
(cid:96)(F(x),y) + λlog(I(F(x),y)),
(3)
where λ is a penalty parameter for the privacy risk. In this way,
the target model F is trained with an additional regularization
term to defend against membership inference attacks.
2.3.2 MemGuard [20]
Jia et al. [20] propose MemGuard as a defense method against
membership inference attacks, which, different from Nasr et
al. [31], does not need to modify the training process. Instead,
given a pre-trained target model F, they obfuscate its predic-
tions with well-designed noises to confuse the membership
inference classiﬁer I, without changing classiﬁcation results.
The attack classiﬁer I is trained following the shadow-
training technique [41], which takes the model prediction
F(x) with the sample label y, and outputs a score I(F(x),y)
in the range [0 ,1] for membership inference: if the output is
larger than 0.5, the data sample is inferred as a member, and
vice versa. The key question of how to add noise n to F(x)
2618    30th USENIX Security Symposium
USENIX Association
can be formulated as the following optimization problem:
d(F(x) + n,F(x)),
min
n
subject to:argmax
i
(F(x)i + ni) = argmax
i
F(x)i,
I(F(x) + n) = 0.5,
F(x)i + ni ≥ 0,∀i
∑
ni = 0,
i
(4)
where the objective is to minimize the distance d between
original predictions and noisy predictions. The ﬁrst constraint
ensures the classiﬁcation result does not change after adding
noise, the second constraint ensures the attack classiﬁer can-
not determine whether the sample is a member or a non-
member with the noisy predictions, and last two constraints
ensure the noisy predictions are valid.
When evaluating the defense performance, both Nasr et
al. [31] and Jia et al. [20] train NN classiﬁers for inference
attacks. As shown in the following section, we ﬁnd that their
evaluations underestimate privacy risks. With our benchmark
attacks, the adversary achieves signiﬁcantly higher attack
accuracy on defended models than previous estimates. We
further ﬁnd that the performance of adversarial regulariza-
tion [31] is no better than early stopping, and the evaluation of
MemGuard [20] lacks consideration of strategic adversaries.
3 Systematically Evaluating Membership In-
ference Privacy Risks
In this section, we ﬁrst present a suite of non-NN based attacks
to benchmark privacy risks, which only need to observe target
model’s output predictions (i.e., black-box setting). Next, we
provide two recommendations, comparison with early stop-
ping and considering adaptive attacks, to rigorously measure
the effectiveness of defense approaches. Finally, we present
experiment results by re-evaluating target models in prior
work [20, 31, 32] with our proposed benchmark attacks.
3.1 Benchmarks of membership inference at-
tacks
We propose to use a suite of non-NN based attack methods to
benchmark membership inference privacy risks of machine
learning models. We call these attack methods “metric-based
attacks” as they ﬁrst measure the performance metrics of tar-
get model’s predictions, such as correctness, conﬁdence, and
entropy, and then compare those metrics with certain thresh-
old values to infer whether the input sample is a member or
a non-member [24, 44]. We improve existing metric-based
attacks by setting different threshold values for different class
labels of target models. Then we propose another new metric-
based attack by considering ground truth label when evaluat-
ing prediction uncertainty. We denote the inference strategy as
I , which codes members as 1, and non-members as 0. Overall,
we propose that existing NN based attacks should be supple-
mented with our metric-based attacks for systematically and
rigorously evaluating privacy risks of ML models.
3.1.1 Existing attacks
Inference attack based on prediction correctness Leino et
al. [24] observe that the membership inference attacks based
on whether the input is classiﬁed correctly or not achieve
comparable success as NN-based attack on target models
with large generalization errors. The intuition is that the
target model is trained to predict correctly on training data
(members), which may not generalize well on test data (non-
members). Thus, we can rely on the prediction correctness
metric for membership inference. The adversary infers an
input sample as a member if it is correctly predicted, a non-
member otherwise.
Icorr(F, (x,y)) = 1{argmax
where 1{·} is the indicator function.
i
F(x)i = y},
(5)
3.1.2
Improving existing attacks with class-dependent
thresholds
Inference attack based on prediction conﬁdence Yeom et
al. [48] and Song et al. [44] show that the attack strategy
of using a threshold on the prediction conﬁdence results in
similar attack accuracy as NN-based attacks. The intuition is
that the target model is trained by minimizing prediction loss
over training data, which means the prediction conﬁdence of
a training sample F(x)y should be close to 1. On the other
hand, the model is usually less conﬁdent in predictions on
a test sample. Thus, we can rely on the metric of prediction
conﬁdence for membership inference. The adversary infers
an input example as a member if its prediction conﬁdence is
larger than a preset threshold, a non-member otherwise.
Iconf(F, (x,y)) = 1{F(x)y ≥ τy}.
(6)
Yeom et al. [48] and Song et al. [44] choose to use a single
threshold for all class labels. We improve this method by
setting different threshold values τy for different class labels
y. The reason is that the dataset may be unbalanced so that
the target model indeed has different conﬁdence levels for
different class labels. Our experiments show that this class-
dependent thresholding technique leads to better attack perfor-
mance. The class-dependent threshold values τy are learned
with the shadow-training technique [41]: the adversary (1)
ﬁrst trains a shadow model to simulate the behavior of the
target model; (2) then obtains the shadow model’s prediction
conﬁdence values on both shadow training and shadow test
data; (3) ﬁnally leverages knowledge of membership labels
(member vs non-member) of the shadow data to select the
USENIX Association
30th USENIX Security Symposium    2619
threshold value τy which achieves the highest accuracy in
distinguishing between shadow training data and shadow test
data with the class label y based on Equation (6).
Inference attack based on prediction entropy Although
there is no prior work using prediction entropy for member-
ship inference attacks, Shokri et al. [41] indeed present the
difference of prediction entropy distributions between train-
ing and test data to explain why privacy risks exist. Salem
et al. [38] also mention the possibility of using prediction
entropy for attacks. The intuition is that the target model is
trained by minimizing the prediction loss over training data,
which means the prediction output of a training sample should
be close to a one-hot encoded vector and its prediction entropy
should be close to 0. On the other hand, the target model usu-
ally has a larger prediction entropy on an unseen test sample.
Therefore, we can rely on the metric of prediction entropy
for membership inference. The adversary classiﬁes an input
example as a member if its prediction entropy is smaller than
a preset threshold, a non-member otherwise.
Ientr(F, (x,y)) = 1{−∑
i
F(x)i log(F(x)i) ≤ ˆτy}.
(7)
Similar to the conﬁdence-based attack, we propose to use the
threshold values ˆτy that are dependent on the class labels and
are set with the shadow-training technique [41].
3.1.3 Our new inference attack based on modiﬁed pre-
diction entropy
The attack based on prediction entropy has one serious issue:
it does not contain any information about the ground truth
label. In fact, both a correct classiﬁcation with probability of
1 and a totally wrong classiﬁcation with probability of 1 lead
to zero prediction entropy values.
To resolve this issue, we design a new metric with following
two properties to measure the model prediction uncertainty
given the ground truth label: it should be (1) monotonically
decreasing with the prediction probability of the correct label
F(x)y, and (2) monotonically increasing with the prediction
probability of any incorrect label F(x)i,∀i (cid:54)= y. Let x ∈ [0,1]
denote the prediction probability for a certain label, the func-
tion used in conventional entropy computations −xlogx is
not a monotonic function. As a contrast, −(1− x)logx is a
monotonically decreasing function, and −xlog(1 − x) is a
monotonically increasing function. Therefore, we propose the
modiﬁed prediction entropy metric, computed as follows.
Mentr(F(x),y) =− (1− F(x)y)log(F(x)y)
F(x)i log(1− F(x)i).
−∑
i(cid:54)=y
(8)
In this way, a correct classiﬁcation with probability of 1 leads
to modiﬁed entropy of 0, while a wrong classiﬁcation with
probability of 1 leads to modiﬁed entropy of inﬁnity.
Now, with the new metric of modiﬁed prediction entropy,
the adversary classiﬁes an input example as a member if its
modiﬁed prediction entropy is smaller than a preset threshold,
a non-member otherwise.
IMentr(F, (x,y)) = 1{Mentr(F(x),y) ≤ ˇτy}.
(9)
Similar to previous scenarios, we set different threshold values
ˇτy for different class labels, which are learned with the shadow
training technique [41]. Experiments show that the inference
attack based on our modiﬁed prediction entropy is strictly
superior to the inference attack based on prediction entropy.
3.2 Rigorously evaluating membership infer-
ence defenses
To evaluate the effectiveness of defenses against membership
inference attacks, we make the following two recommenda-
tions, besides using our metric-based benchmark attacks.
3.2.1 Comparison with early stopping
During the training process, the target model’s parameters
are updated following gradient descent methods, so the train-
ing error and test error usually get reduced gradually with
an increasing number of training epochs. However, as the
number of training epochs increases, the target model also
becomes more vulnerable to membership inference attacks,
due to increased memorization. We thus propose early stop-
ping [6, 34, 47] as a benchmark defense method, in which
fewer training epochs are used in order to tradeoff a slight
reduction in model accuracy with lower privacy risk.
Figure 2: Test accuracy at different training epochs for Pur-
chase100 classiﬁers without defense and with adversarial
regularization defense [31]. We should compare the ﬁnal de-
fended model to the model with early stopping.
We recommend that whenever a defense method is pro-
posed in the literature that reduces the threat of membership
inference attacks at the cost of degradation in model accu-
racy, the performance of the defense method should be bench-
marked against our early stopping approach. This is indeed the
2620    30th USENIX Security Symposium
USENIX Association
case for the defense method of adversarial regularization (Ad-