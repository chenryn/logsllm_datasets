functions [8]. VFW Controller uses Floodlight REST APIs
to communicate ﬂow updates to Floodlight module, which
programs SDN switches through an OpenFlow interface. The
same channel is also used by SDN switches to send network
trafﬁc statistics back to VFW Controller.
To improve the performance of VFW Controller, our
implementation uses both online and prior processing. All the
analyses, including dependency analysis, ﬂow update analy-
sis, update cost calculation, and buffer cost calculation are
carried out prior. VFW Controller maintains a copy of
ﬁrewall rules for each virtual ﬁrewall and the ﬂow rules in
its database for prior analyses. Results from the analyses are
stored and retrieved whenever scaling is to be performed. Over-
load/underload detection, optimal scaling calculation, virtual
ﬁrewall creation/deletion, migrations of ﬁrewall rules and ﬂow
states, and ﬂow updates are carried out online.
6XL is a toolstack that provides the capability to provision guest VM in
Xen.
B. Evaluation
VFW Controller achieves safe, efﬁcient and optimal
virtual ﬁrewall scaling. We evaluate VFW Controller with
the following goals:
•
•
•
•
•
Demonstration of the relationship between virtual ﬁre-
wall performance and the rule size (Figure 7). This
justiﬁes VFW Controller’s choice of rule split
over rule copy.
Study of the rule dependency relations in ﬁrewall
policies (Table I). This justiﬁes the necessity of de-
pendency analysis in VFW Controller.
Demonstration of VFW Controller’s capability to
quickly scale (Figure 8).
Quantifying the impact of ﬁrewall rule migration on
virtual ﬁrewall throughput (Figure 9).
Evaluating the performance of VFW Controller’s
optimal scaling calculation (Figure 10).
Setup and Methodology: Our experiments were conducted
using CloudLab [3], an open cloud platform providing various
resources (server, storage, and network) for experimenters to
run cloud software stacks such as OpenStack and CloudStack.
In our experiments, we deployed a client machine that gen-
erated trafﬁc, a server machine that received trafﬁc, and a
ﬁrewall machine created by VFW Controller to process the
trafﬁc between the client and the server. The client generated
synthetic workloads using scapy 7, a powerful
interactive
packet manipulation program.
1) Performance of Virtual Firewalls: In this experiment,
we examined the relationship between the processing capacity,
c, of a virtual ﬁrewall and its ﬁrewall rule size S. We
used three trafﬁc datasets captured from real-world networks:
1) the CAIDA UCSD anonymized Internet trace [16] is a
representative of Internet trafﬁc; 2) the LBNL/ICSI enterprise
trace [5] is a typical
trafﬁc collected from an enterprise
network; and 3) the Campus network trace that was collected
from our campus network gateway. Against each of the dataset,
we conducted experiments to study how the ﬁrewall rule
size affects the performance of a virtual ﬁrewall. In each
experiment, we let S increase from 1 to 3000, which we
considered as a maximum number of rules in a single VFW.
Then we measured the processing capability for each S value
and repeated each measurement 100 times to calculate the
average processing capacity, as shown in Figure 7(a). The
average processing capacity linearly decreases as S increases
over all datasets. The LBNL enterprise trafﬁc contains packets
with greater length of payload, thus bares a more signiﬁcant
impact on performance. We applied polynomial curve ﬁtting
linear regress on the CAIDA trace, which was captured by
April 2016 and represents the most up-to-date characteristic
of today’s Internet trafﬁc, and obtained the function c(S) as
c(S) = −0.0043S + 6.2785
(2)
This function ﬁts the corresponding CAIDA curve in Fig-
ure 7(a) with R2 = 0.9864. R2 is a measure of goodness of
ﬁt with a value of 1 denoting a perfect ﬁt. c(S) will be used
by the evaluation of optimal scaling calculation (§ VIII-B5).
7http://www.secdev.org/projects/scapy/.
11
TABLE I: Rule dependencies in real-world ﬁrewall policies.
Policy
Rule (#)
Group (#)
A
B
C
D
E
F
G
H
12
18
25
52
83
132
354
926
2
3
3
7
9
10
10
13
Largest
Member (#)
Group
3
5
6
7
7
9
12
18
(a) Processing capacity.
(b) Processing time per packet.
Fig. 7: Relationship between virtual ﬁrewall performance and
rule size.
In the experiment, we also recorded the average time a
virtual ﬁrewall instance spent to process a packet for each of
the real-world dataset, as shown in Figure 7(b). As S increases,
the average processing time shows an linear increase, which
eventually incurs the latency of the passed packets. These re-
sults of this experiment can be used for buffer cost calculation
(§ VI-A).
2) Rule Dependencies in Firewall Policies: In this experi-
ment, we studied 8 real-world ﬁrewall policies from different
resources. Most of them are from campus networks and some
are from major ISPs. We partitioned each policy to disjoint
groups using the partition algorithm given in Section IV. The
experimental results are listed in Table I. The ﬁrst two columns
of the table show the policies we used and their rule numbers.
The third column gives the number of groups identiﬁed in each
policy, and the fourth column shows the number of the ﬁrewall
rules in the largest group of each policy. This experiment
demonstrates that rule dependencies are common in real-world
ﬁrewall policies. Therefore, the dependency analysis in VFW
Controller is necessary. We also noticed from our study
that the largest group we encountered contains only 18 ﬁrewall
rules (from Policy H in Table I).
3) Elasticity of VFW Controller: In this experiment, we
demonstrated VFW Controller’s ability to elastically scale
out an overloaded virtual ﬁrewall. We designed three scenarios.
In scenario (1), a single virtual ﬁrewall was created and
conﬁgured with 400 ﬁrewall rules to process the trafﬁc between
the client and the server. In scenario (2), two single virtual
12
ﬁrewalls were created to work in parallel process the same
trafﬁc. The 400 ﬁrewall rules in scenario (1) were split and
installed in those two virtual ﬁrewalls. In both scenarios, the
virtual ﬁrewalls worked in standalone mode, which meant they
were not connected to VFW Controller and no scaling
would be performed. In scenario (3), a single virtual ﬁrewall
with the same conﬁguration as that in scenario (1) was created
to work in standalone mode at ﬁrst, then switched to connected
mode, which meant the virtual ﬁrewall was connected to VFW
Controller and was scaled out. We compared the runtime
throughput of the virtual ﬁrewalls in the three scenarios as
shown in Figure 8.
Figure 8(a) shows the runtime throughput of the three
scenarios when the client generated 4 UDP ﬂows with an
aggregated trafﬁc rate of about 2.8 Gbps. In scenario (1), the
single virtual ﬁrewall achieved a maximum throughput (i.e.
processing capacity) of about 1.5 Gbps. Signiﬁcant packet
loss was experienced. In scenario (2), the two virtual ﬁrewall
were able to handle the incoming trafﬁc, producing a runtime
throughput of about 2.8 Gbps. No packet loss occurred. In
scenario (3), we intentionally let a single virtual ﬁrewall to
work from time t = 0s to t = 5s. Packet loss occurred
during this time period. Then we connected this virtual ﬁrewall
to VFW Controller and it was scaled into two virtual
ﬁrewall instances. The scaling-out took a short period of time
(¡ 1 second), after which we observed the aggregated runtime
throughput increased to around 2.8 Gbps.
We also evaluated VFW Controller against TCP trafﬁc.
Figure 8(b) shows the runtime throughput of the three scenar-
ios when the client established 2 TCP connections with the
server. We observed a boost of runtime throughput between
t = 5s and t = 6s.
In summary,
the above results demonstrated that VFW
Controller can quickly scaled out an overloaded virtual
ﬁrewall and solved the overload condition.
4) Impact of Migration: During the migration, the in-ﬂight
trafﬁc are buffered until the migration completes, therefore the
runtime throughput of a virtual ﬁrewall is expected to degrade
during the migration. According to equation (1) in §VI, if more
trafﬁc ﬂows are associated with the migrated ﬁrewall rules,
more in-ﬂight trafﬁc will be buffered. Therefore larger the
degradation will be and longer the degradation will last. In this
experiment, we quantiﬁed both the duration and magnitude of
throughput degradation. We set up two scenarios to test UDP
and TCP ﬂows, respectively.
In scenario (1), the client kept sending UDP trafﬁc destined
to the server. The UDP packets were routed through a virtual
ﬁrewall created by VFW Controller. In Figure 9(a), the
06001200180024003000Number of firewall rules020040060080010001200Processing Capacity (Mbps)(a) Processing capacityCampusLBNLCAIDA06001200180024003000Number of firewall rules0.000.020.040.060.080.10Processing Time (microsecond)(b) Processing timeCampusLBNLCAIDA(a) Split with UDP ﬂow overload.
(b) Split with TCP ﬂow overload.
Fig. 8: VFW Controller for VFW elasticity.
(a) Impact on UDP throughput.
(b) Impact on TCP throughput.
Fig. 9: Performance overhead of migration.
Fig. 10: Performance of provision plan calculation.
solid black line, as a base line, shows the throughput of the
virtual ﬁrewall when no migration occurred. The dotted lines in
Figure 9(a) show the runtime throughput of the virtual ﬁrewall
when migrating 200, 600, 1200 ﬁrewall rules, respectively.
The migrations were scheduled at t = 6s. We fabricated
ﬁrewall rules so that each ﬁrewall rule has one matching
ﬂow. The duration and magnitude of throughput degradation
increased as the number of migrated ﬁrewall rules (or the
number of UDP ﬂows) increased. However, the degradation
lasted for a very short period of time (≈0.75s with 1200
ﬂows) and the throughput bounced back very quickly. In
scenario (2), the client sent TCP trafﬁc. Figure 9(b) shows
that the degradation of throughput of the virtual ﬁrewall when
processing TCP trafﬁc. As the number of ﬁrewall rules (or
TCP ﬂows) increases, the degradation lasted longer and was
bigger. The degradation in the case of TCP trafﬁc lasted
slightly longer than that of UDP, because TCP depends on
congestion avoidance mechanisms to control its trafﬁc rate,
which means TCP ﬂows take more time to recover from a
throughput degradation than UDP ﬂows. For both UDP and
TCP ﬂows, the throughput began to bounce back in less than
0.1 second after it reaching the lowest point (see Figures 9(a)
and (b)).
To sum up,
the duration and magnitude of throughput
degradation increase as the number of migrated ﬁrewall rules
(or the number of ﬂows) increase. And the degradation shows
an bigger impact on TCP ﬂows than UDP ﬂows.
5) Performance of Optimal Scaling Calculation: We intro-
duced a three-step heuristic approach in VFW Controller
to calculate an optimal solution for scaling out an overloaded
virtual ﬁrewall. The performance of this approach depends
almost fully on the ILP formulation and solving used in the
second step. Therefore, we mainly tested the performance of
our ILP formulation and solving. The ILP performance is
inﬂuenced by (i) m, the number of ﬁrewall rule groups on
the overloaded virtual ﬁrewall; and (ii) n, the number of new
virtual ﬁrewall instances to be created. In our experiment, we
measured the time to ﬁnd the optimal solution for varied m and
n values. We let n range from 2 to 6, and m range from 100
up to 1000. Figure 10(a) shows our experiment results. The
time to ﬁnd an optimal solution increases as m or n increases,
however, the time is very short. Even for m = 1000, our
approach needs less than 0.11 second.
13
0246810Data Transfer Elapsed Time (seconds)0.00.51.01.52.02.53.0Runtime Throughput (Gbps)(a) Split with UDP flows overloadSingle VFWWith VFW SplitTwo Paralell VFWs0246810Data Transfer Elapsed Time (seconds)0.00.51.01.52.02.53.0Runtime Throughput (Gbps)(b) Split with TCP flows overloadWith VFW SplitTwo Paralell VFWsSingle VFW12345678910Data Transfer Elapsed Time (seconds)0.00.51.01.52.0UDP Throughput (Gbps)(a) Impact on UDP Throughput200 flows600 flows1200 flowsWithout migration12345678910Data Transfer Elapsed Time (seconds)0.00.51.01.52.0TCP Throughput (Gbps)(b) Impact on TCP Throughput200 flows600 flows1200 flowsWithout migration01002003004005006007008009001000Number of Groups0.000.020.040.060.080.100.12Time (second)(a)234560102030405060708090100Number of Under-loaded VFWs0.000.020.040.060.080.10Time (second)(b)Figure 10(b) depicts the time that our approach consumed
to ﬁnd an optimal solution for scaling in when the number
of underloaded virtual ﬁrewall
instances is changed. Our
approach is very efﬁcient. Even when 100 virtual ﬁrewall
instances are detected to be underloaded, our approach takes
less than 0.08 second to ﬁnd an optimal scaling-in solution.
IX. RELATED WORK
Several recent research efforts, such as Pico Replica-
tion [43], Split/Merge [44] and OpenNF [28], have been
devoted to designing control systems to address scaling issues
of virtualized network functions. Pico Replication provides
APIs that NFs can use to create, access, and modify internal
states. Split/Merge achieves load-balanced elasticity of virtual
middleboxes, via splitting internal states of NFs among virtual
middlebox replicas and re-routing ﬂows. However, both Pico
Replication and Split/Merge cannot achieve loss-free during