track my {agent}s phone
app,
without them knowing, track location of my {agent}
app, read sms from another phone app, app for tracking
my kids, app for seeing my kids phone, keylogger for
android, easy spy app for android, hidden spy app for
android, app to see photos in my phone remotely, see
all whatsapp messages app, see all facebook messages
app, spyware for android devices, record calls app
run,
Blacklist
game, sport, mile, gta, xbox, royale, golf, ﬁt,
food, ﬂight,
tracks$, car, cheating tom,
cheat.*code, refund, cheatsheet, chart, cheat.*sheet,
cheat.*engine, gas budd?y, calorie, money, ex-
pense, spending, tax, budget, period, diet, preg-
nancy, fertility, weight, gym, water, work ?out,
track and ﬁeld, exercise, cheats, baby.*photos,
time, hour, minute, day, month, year, sale,
tv,
ski, sleep, walking, block, anti.*tracking,
rent,
nutrition, corporate, insta(gram)?, facebook, twit-
ter, tinder, spyfall, forms?, exam, dhl, fedex, ups,
read.*loud, quotes, ps4, ps3
Fig. 9: List of seed search terms (separated by “,”) for conducting query snowballing with Google Play and Google search.
Here {agent} is replaced with each of {boyfriend, girlfriend, wife, husband, spouse, partner}. On the right is the blacklist of
words or regular expressions used to ﬁlter queries that have them.
App Store does not provide all the information we get from
Play Store. Notably, in the App Store, permissions requested
by an app are not available. Therefore we had to modify and
retrain our machine learning algorithm separately for Apple.
For training and cross-validation we used the 500 hand labeled
apps mentioned above. The feature set was constructed using
the app description, the app title, and the genres of that app
as listed in the App Store. A bag-of-word model with pruning
was constructed in the same way described in Section III-C.
From the BoW model we pick the 1,100 most discriminatory
features (1,000 from description and 100 from the title and
the genres) based on χ2-statistic [61].
In 10-fold cross validation using logistic regression (LR)
model (with L2 penalty, and inverse of regularization strength,
C, set
to 0.0385), we found the classiﬁer can accurately
classify 92% of apps, with a false positive rate of 7% and
false negative rate of 8%. If we set the cutoff to 0.3 (as we
did in case of Android), the false negative rate goes below 1%
with 10% false positive.
C. Pruning with MTurk.
In Page 6 we show how we can tune machine learning to
remove apps that are obviously irrelevant to IPS, leaving us
with nearly 34% of all apps that ML classiﬁer ﬂags as “dual-
use.” However, among the apps ﬂagged by the classiﬁer, nearly
20% are falsely tagged (based on our hand-labeled training
data). Therefore, we decide to use a second level pruning
of false positive apps from the Google Play store leveraging
Amazon’s Mechanical Turk (MTurk) to rapidly employ a large
pool of human workers to label apps as dual-use or not.
While our initial experiment does not produce results better
than the ML classiﬁer, it deﬁnitely testiﬁes the possibility
and opens up an interesting question of how to utilize a
crowdsourcing framework to perform a non-trivial task such
as identifying spyware apps based on their descriptions.
Pilot study to ensure feasibility. Though MTurk provides
an efﬁcient method for simple classiﬁcation tasks, such as
image tagging, our task is more nuanced, and could require
domain knowledge from the workers to perform correctly.
For example, the deﬁnition of a dual-use app is not always
immediately apparent, and often relies on “what-if” judgments
Ground Truth
dual-use
benign
MTurk
dual-use
benign
Total
30
3
33
1
65
66
Total
31
68
99
Fig. 10: Confusion matrix of MTurk labels (majority among
5 workers) and ground truth (researchers’ labels) of 99 apps
(randomly sampled from TR) from the pilot study.
about potential app usage rather than any observable phenom-
ena. In order to verify MTurk’s viability for completing our
classiﬁcation task, we conducted a pilot study with a small set
of workers.
As part of our required qualiﬁcation test, we gave workers
a short (i.e., a couple paragraphs) description of dual-use apps
and examples of both benign and dual-use apps, including
some “borderline” cases. We then asked workers to classify
ten sample apps we hand-labeled beforehand as either benign
or dual-use. We found that most workers (84.6%) were able
to accurately classify all ten apps by their second attempt at
the qualiﬁcation test.
Once a worker passes the qualiﬁcation test, the worker is
allowed to accept actual classiﬁcation tasks (HITs). Each task
contains 3 apps (with a $0.06 reward for labeling each app) and
must be completed by ﬁve different workers. For each app, we
take the majority vote of the classiﬁcation submitted from all
ﬁve workers. To evaluate the promise of crowdsourced labels,
we ﬁrst performed a pilot study by submitting 99 randomly
sampled apps from the TR set (data that we hand-labeled and
used for training the ML classiﬁer). We use Cohen’s Kappa (κ)
statistic [19] to compare the agreement between crowdsourced
labels and the researcher-assigned labels.
In the pilot study, we found a promising agreement rate
between the crowdsourced labels and the researchers’ labels
(κ = 0.96; the maximum possible value of κ is 1). This
amounts to 95% of the labels matching across the 99 apps, and
only a 1% false negative rate (taking the researchers’ labels as
the ground truth). In Figure 10 we note the confusion matrix
of this experiment. The results suggest the viability of using
crowdsourcing to identify dual-use apps.
Study with all the hand-labeled apps. Following the pilot
456
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:39:30 UTC from IEEE Xplore.  Restrictions apply. 
study, we submitted all of the remaining apps from our hand-
labeled set of 1,200 apps (TR + TS1 + TS2) to MTurk. To
expedite the data collection, we included 7 apps in each HIT
for a total payment of 0.42 ($0.06 × 7) per assignment. All
of the apps were labeled by ﬁve different workers within 48
hours. However, the ﬁnal agreement rate was worse than the
pilot study at κ = 0.64: only 85% of crowdsourced labels
matched the researcher labels, with a 12% false negative rate.
We found that a small number of the workers mislabeled
a relatively large number of apps. After removing all labels
from workers with agreement rate κ ≤ 0.5, we re-submitted
the apps requiring more labels (using the same HIT format: 7
apps per HIT). We also modiﬁed our initial qualiﬁcation test
by giving more exemplary instruction of major classes of dual-
use apps. After obtaining the new labeling the agreement of the
MTurk majority with the ground truth improved to κ = 0.76.
In Figure 11 we show the performance of the MTurk majority
labeling.
Evaluation. To conjunct MTurk into our rest of the pipeline,
we decided to use 0.3 as our classiﬁcation threshold. With
this threshold, we do not submit any negatively-labeled apps
by our machine classiﬁer to MTurk, and also the apps on
which machine classiﬁer’s conﬁdence is high (≥ 0.7). We only
submit the positive-apps for which the classiﬁer’s conﬁdence
is low (≤ 0.7). For the rest of the apps, we will take the ML
classiﬁer’s labeling as the ﬁnal label.
The ﬁnal performance of this pipeline is recorded in the
Figure 11 (last row). Interestingly, the pipeline has consis-
tently lower false negatives across all datasets than logistic
regression with cutoff 0.5, while having similar false positive
rate. Also, we found for test data TS2, the accuracy is > 97%,
better than the best machine learning can achieve.
While the initial results are not very promising, we can
improve on this. For example, given that the labeling dual-use
apps require some domain knowledge, we can design a more
nuanced worker-training process. Also, we can task workers to
identify capabilities and purpose of the apps, instead of making
a judgment call about whether or not the app is IPS relevant.
For example, the worker ﬁnds out from the description whether
the app can sync SMS, or can be used for parental control,
etc. This information can be used to further classify those apps
more accurately into IPS and benign categories. We leave a
detailed analysis of this approach as future work.
D. Analysis of Google Ads on IPS search terms
We searched Google for ten days in October 2017 with a
subset of 1,400 queries from the 10,000 terms we found in
Section III-A. We searched from a Chrome browser on an
OSX machine and recorded the contents of the ﬁrst page of
the search results. We did not set up any user proﬁle, and
performed each search from a new browser session (though
persistent cookies were not purged). We extracted a total of
7,776 ad impressions associated with 214 domains. Among
our search terms, 340 showed at least one ad during the
measurement period. The term “how to catch a cheating
Training
Test
(1st wk)
Test
(4th wk)
dual-use
benign
Accu.
FNR
FPR
Accu.
FNR
FPR
Accu.
FNR
FPR
Accu.
FNR
FPR
Accu.
FNR
FPR
280
720
96%
4%
4%
93%
2%
9%
86%
< 1%
19%
91%
20%
4%
96%
4%
5%
Logistic
Regression
(cutoff: 0.5)
Logistic
Regression
(cutoff: 0.4)
Logistic
Regression
(cutoff: 0.3)
MTurk
(majority
among 5)
Whole
Pipeline
28
72
91%
4%
11%
88%
4%
15%
82%
0%
25%
89%
11%
11%
91%
0%
12%
22
78
95%
10%
6%
88%
10%
12%
81%
0%
24%
96%
19%
0%
97%
5%
3%
Fig. 11: Training and testing accuracy of our pipeline. First
two rows show the statistic of our hand-labeled data (ground-
truth). For the pipeline, we consider an app dual-use if LR
classiﬁer’s conﬁdence is more than 0.7 or if the conﬁdence is
within [0.3, 0.7] and majority of MTurk worker labeled it as
‘dual-use’.
spouse with his cell phone,” served the most ad impressions
associated to 18 different domains. The most common domain
(truthﬁnder.com) appeared in 897 ad impressions across 112
different search terms.
We repeated the scraping process in November for three
days, following the publication of an article by The Daily
Beast accusing Google of showing ads about
illegal spy-
ware [21]. We observed a total of 2,866 ad impressions
linking to 186 domains, resulting from 432 search terms. Some
searches yielded as many as 7 ad impressions on the ﬁrst page
of search results. The most advertised domain remained the
same. We ran one further scrape in March for one day and
collected 1,843 ad impressions linked to 137 domains and 372
search terms.
We analyzed all 96 domains that appeared in at least 10
ad impressions across all measurement periods. These 96
domains are associated with 11,831 ad impressions (95%). Of
these domains, 20 belong to services offering public record or
reverse phone number lookups. Those represent half (6,217) of
the ad impressions. Another 22 domains are of tracking apps
and software and account for 3,128 ad impressions. Eighteen
domains (linked to 1,162 ads) belong to miscellaneous but
relevant sites, including: manufacturers of physical tracking
beacons, private eye services, blogs and forums of the kind
discussed below, and social networking sites which facilitate
inﬁdelity. The remaining 34 domains linked to 1,324 ads are
not at all relevant to IPS.
We analyzed the 598 search terms that returned ads across
all measurement periods. We determined whether each term
explicitly indicated that the searcher intended to engage in IPS.
Terms that indicated the intent to track a cell phone but did not
indicate that it was another person’s phone (such as “best free
457
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:39:30 UTC from IEEE Xplore.  Restrictions apply. 
gps phone tracking app”) or that indicated the intent to track a
child’s phone (such as “free family tracker app”) were labeled
“relevant” but not explicit. Terms that discussed a spouse but
did not mention tracking (such as “cheating spouse forum”)
were also marked relevant but not explicit. Of the 598 search
terms, 135 were explicit, 324 were relevant to IPS but not
explicit, and 139 were irrelevant (e.g., “Spyro the Dragon”). Of
12,484 observed ad impressions, 58% were on explicit terms,
39% on relevant terms and 3% on irrelevant terms.
We further examined the 3,128 ad impressions shown for
the 22 domains that sold IPS-usable software. Of these, 1,203
(38%) were shown on IPS-explicit terms, 1,920 were shown
on IPS-relevant, but not explicit, terms, and four were shown
on irrelevant terms. The rate of ads on IPS-explicit terms for
speciﬁc apps ranged from 0%, in the case of TeenSafe, (the one
app that admonished us when speaking to customer service,
590 total ad impressions) to 91% in the case of RemoteCellSpy
(418 total ads). Though further study is required to ﬁnd out
which words in our search term is triggering the ad,
the
discrepancy in the number of IPS-explicit terms showing ads
for one company but not for another seems to indicate that
some companies are actively trying to advertise for IPS use.
458
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:39:30 UTC from IEEE Xplore.  Restrictions apply.