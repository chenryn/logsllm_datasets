title:Optimized Invariant Representation of Network Traffic for Detecting
Unseen Malware Variants
author:Karel Bartos and
Michal Sofka and
Vojtech Franc
Optimized Invariant Representation of Network 
Traffic for Detecting Unseen Malware Variants
Karel Bartos and Michal Sofka, Cisco Systems, Inc.; Vojtech Franc,  
Czech Technical University in Prague
 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/bartos
This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX Optimized Invariant Representation of Network Trafﬁc for Detecting
Unseen Malware Variants
Karel Bartos
Cisco Systems, Inc.
Czech Technical University in Prague,
Faculty of Electrical Engineering
Michal Sofka
Cisco Systems, Inc.
Czech Technical University in Prague,
Faculty of Electrical Engineering
Vojtech Franc
Czech Technical University in Prague,
Faculty of Electrical Engineering
Abstract
New and unseen polymorphic malware, zero-day attacks,
or other types of advanced persistent threats are usually
not detected by signature-based security devices, ﬁre-
walls, or anti-viruses. This represents a challenge to
the network security industry as the amount and vari-
ability of incidents has been increasing. Consequently,
this complicates the design of learning-based detection
systems relying on features extracted from network data.
The problem is caused by different joint distribution of
observation (features) and labels in the training and test-
ing data sets. This paper proposes a classiﬁcation sys-
tem designed to detect both known as well as previously-
unseen security threats. The classiﬁers use statistical
feature representation computed from the network traf-
ﬁc and learn to recognize malicious behavior. The rep-
resentation is designed and optimized to be invariant to
the most common changes of malware behaviors. This
is achieved in part by a feature histogram constructed
for each group of HTTP ﬂows (proxy log records) of a
user visiting a particular hostname and in part by a fea-
ture self-similarity matrix computed for each group. The
parameters of the representation (histogram bins) are op-
timized and learned based on the training samples along
with the classiﬁers. The proposed classiﬁcation system
was deployed on large corporate networks, where it de-
tected 2,090 new and unseen variants of malware sam-
ples with 90% precision (9 of 10 alerts were malicious),
which is a considerable improvement when compared to
the current ﬂow-based approaches or existing signature-
based web security devices.
1
Introduction
Current network security devices classify large amounts
of the malicious network trafﬁc and report the results
in many individually-identiﬁed incidents, some of which
are false alerts. On the other hand, a lot of malicious traf-
ﬁc remains undetected due to the increasing variability
of malware attacks. As a result, security analysts might
miss severe complex attacks because the incidents are not
correctly prioritized or reported.
The network trafﬁc can be classiﬁed at different lev-
els of detail. Approaches based on packet inspection
and signature matching [15] rely on a database of known
malware samples. These techniques are able to achieve
results with high precision (low number of false alerts),
but their detection ability is limited only to the known
samples and patterns included in the database (limited
recall). Moreover, due to the continuous improvements
of network bandwidth, analyzing individual packets is
becoming intractable on high-speed network links.
It
is more efﬁcient to classify network trafﬁc based on
ﬂows representing groups of packets (e.g. NetFlow [1]
or proxy logs [26]). While this approach has typically
lower precision, it uses statistical modeling and behav-
ioral analysis [8] to ﬁnd new and previously unseen ma-
licious threats (higher recall).
Statistical features calculated from ﬂows can be used
for unsupervised anomaly detection, or in supervised
classiﬁcation to train data-driven classiﬁers of malicious
trafﬁc. While the former approach is typically used to
detect new threats, it suffers from lower precision which
limits its practical usefulness due to large amount of false
alerts. Data-driven classiﬁers trained on known mali-
cious samples achieve better efﬁcacy results, but the re-
sults are directly dependent on the samples used in the
training. Once a malware changes the behavior, the sys-
tem needs to be retrained. With continuously rising num-
ber of malware variants, this becomes a major bottleneck
in modern malware detection systems. Therefore, the ro-
bustness and invariance of features extracted from raw
data plays the key role when classifying new malware.
The problem of changing malware behavior can be
formalized by recognizing that a joint distribution of the
malware samples (or features) differs for already known
training (source) and yet unseen testing (target) data.
USENIX Association  
25th USENIX Security Symposium  807
This can happen as a result of target evolving after the
initial classiﬁer or detector has been trained. In super-
vised learning, this problem is solved by domain adapta-
tion. Under the assumption that the source and target
distributions do not change arbitrarily, the goal of the
domain adaptation is to leverage the knowledge in the
source domain and transfer it to the target domain. In
this work, we focus on the case where the conditional
distribution of the observation given labels is different,
also called a conditional shift.
The domain adaptation (or knowledge transfer) can
be achieved by adapting the detector using importance
weighting such that training instances from the source
distribution match the target distribution [37]. Another
approach is to transform the training instances to the do-
main of the testing data or to create a new data represen-
tation with the same joint distribution of observation and
labels [4]. The challenging part is to design a meaning-
ful transformation that transfers the knowledge from the
source domain and improves the robustness of the detec-
tor on the target domain.
In this paper, we present a new optimized invari-
ant representation of network trafﬁc data that enables
domain adaptation under conditional shift. The rep-
resentation is computed for bags of samples, each of
which consists of features computed from network traf-
ﬁc logs. The bags are constructed for each user and con-
tain all network communication with a particular host-
name/domain. The representation is designed to be in-
variant under shifting and scaling of the feature values
and under permutation and size changes of the bags. This
is achieved by combining bag histograms with an invari-
ant self similarity matrix for each bag. All parameters of
the representation are learned automatically for the train-
ing data using the proposed optimization approach.
The proposed invariant representation is applied to de-
tect malicious HTTP trafﬁc. We will show that the clas-
siﬁer trained on malware samples from one category can
successfully detect new samples from a different cate-
gory. This way, the knowledge of the malware behavior
is correctly transferred to the new domain. Compared
to the baseline ﬂow-based representation or widely-used
security device, the proposed approach shows consider-
able improvements and correctly classiﬁes new types of
network threats that were not part of the training data.
This paper has the following major contributions:
• Classifying new malware categories – we propose
a supervised method that is able to detect new types
of malware categories from a limited amount of
training samples. Unlike classifying each category
separately, which limits the robustness, we propose
an invariant training from malware samples of mul-
tiple categories.
• Bag representation of samples – Instead of classi-
fying ﬂows individually, we propose to group ﬂows
into bags, where each bag contains ﬂows that are re-
lated to each other (e.g. having the same user and
target domain). Even though the concept of group-
ing ﬂows together has been already introduced in
the previously published work (e.g. in [32]), these
approaches rely on a sequence of ﬂow-based fea-
tures rather than on more complex representation.
• Features describing the dynamics of the samples
– To enforce the invariant properties of the represen-
tation, we propose to use a novel approach, where
the features are derived from the self-similarity of
ﬂows within a bag. These features describe the dy-
namics of each bag and have many invariant proper-
ties that are useful when ﬁnding new malware vari-
ants and categories.
• Learning the representation from the training
data – To optimize the parameters of the representa-
tion, we propose a novel method that combines the
process of learning the representation with the pro-
cess of learning the classiﬁer. The resulting repre-
sentation ensures easier separation of malicious and
legitimate communication and at the same time con-
trols the complexity of the classiﬁer.
• Large scale evaluation – We evaluated the pro-
posed representation on real network trafﬁc of mul-
tiple companies. Unlike most of the previously pub-
lished work, we performed the evaluation on highly
imbalanced datasets as they appear in practice (con-
sidering the number of malicious samples), with
most of the trafﬁc being legitimate, to show the po-
tential of the approach in practice. This makes the
classiﬁcation problem much harder. We provided a
comparison with state-of-the-art approaches and a
widely-used signature-based web security device to
show the advantages of the proposed approach.
2 Related Work
Network perimeter can be secured by a large variety
of network security devices and mechanisms, such as
host-based or network-based Intrusion Detection Sys-
tems (IDS) [36]. We brieﬂy review both systems, focus-
ing our discussion on network-based IDS, which are the
most relevant to the presented work.
Host-based IDS systems analyze malicious code and
processes and system calls related to OS information.
Traditional and widely-used anti-virus software or spy-
ware scanners can be easily evaded by simple transfor-
mations of malware code. To address this weakness,
methods of static analysis [30], [38] were proposed.
808  25th USENIX Security Symposium 
USENIX Association
2
Static analysis, relying on semantic signatures, concen-
trates on pure investigation of code snippets without ac-
tually executing them. These methods are more resilient
to changes in malware codes, however they can be easily
evaded by obfuscation techniques. Methods of dynamic
analysis [29], [34], [42] were proposed to deal with the
weaknesses of static analysis, focusing on obtaining re-
liable information on execution of malicious programs.
The downside of the dynamic analysis is the necessity
to run the codes in a restricted environment which may
inﬂuence malware behavior or difﬁculty of the analysis
and tracing the problem back to the exact code location.
Recently, a combination of static and dynamic analysis
was used to analyze malicious browser extensions [20].
Network-based IDS systems are typically deployed on
the key points of the network infrastructure and moni-
tor incoming and outgoing network trafﬁc by using static
signature matching [15] or dynamic anomaly detection
methods [8]. Signature-based IDS systems evaluate each
network connection according to the predeﬁned malware
signatures regardless of the context. They are capable of
detecting well-known attacks, but with limited amount of
detected novel intrusions. On the other hand, anomaly-
based IDS systems are designed to detect wide range of
network anomalies including yet undiscovered attacks,
but at the expense of higher false alarm rates [8].
Network-based approaches are designed to detect ma-
licious communication by processing network packets
or logs. An overview of the existing state-of-the-art
approaches is shown in Table 1. The focus has been
on the trafﬁc classiﬁcation from packet traces [5], [28],
[39], [41], as this source provides detailed information
about the underlying network communication. Due to
the still increasing demands for larger bandwidth, an-
alyzing individual packets is becoming intractable on
high-speed network links. Moreover, some environments
with highly conﬁdential data transfers such as banks or
government organizations do not allow deployment of
packet inspection devices due to the legal or privacy rea-
sons. The alternative approach is the classiﬁcation based
on network trafﬁc logs, e.g. NetFlow [1], DNS records,
or proxy logs. The logs are extracted at the transport
layer and contain information only from packet headers.
Methods introduced in [12] and [23] apply features
extracted from NetFlow data to classify network traf-
ﬁc into general classes, such as P2P, IMAP, FTP, POP3,
DNS, IRC, etc. A comparison and evaluation of these ap-
proaches can be found in a comprehensive survey [24].
A combination of host-based statistics with SNORT rules
to detect botnets was introduced in [16]. The authors
showed that it is possible to detect malicious trafﬁc using
statistical features computed from NetFlow data, which
motivated further research in this ﬁeld. An alternative
approach for classiﬁcation of botnets from NetFlow fea-
tures was proposed in [6]. The authors of [33] have used
normalized NetFlow features to cluster ﬂow-based sam-
ples of network trafﬁc into four predeﬁned categories.
As opposed to our approach, the normalization was per-
formed to be able to compare individual features with
each other. In our approach, we extended this idea and
use normalization to be able to compare various malware
categories. While all these approaches represent rele-
vant state-of-the-art, network threats evolve so rapidly
that these methods are becoming less effective due to the
choice of features and the way they are used.
One of the largest changes in the network security
landscape is the fact that HTTP(S) trafﬁc is being used
not only for web browsing, but also for other types of
services and applications (TOR, multimedia streaming,
remote desktop) including lots of malicious attacks. Ac-
cording to recent analysis [18], majority of malware sam-
ples communicate via HTTP. This change has drawn
more attention to classifying malware from web traf-
ﬁc.
In [25], the authors proposed an anomaly detec-
tion system composed of several techniques to detect at-
tacks against web servers. They divide URIs into groups,
where each group contains URIs with the same resource
path. URIs without a query string or with return code
outside of interval [200, 300] are considered as irrele-
vant. The system showed the ability to detect unseen
malware samples and the recall will be compared with
our proposed approach in Section 8.
In [40], the au-
thors introduced a method for predicting compromised
websites using features extracted from page content and
Alexa Web Information Service.
Having sufﬁcient amount of labeled malware samples
at disposal, numerous approaches proposed supervised
learning methods to achieve better efﬁcacy. Clasifying
DGA malware from DNS records based on connections
to non-existent domains (NXDomains) was proposed in
[2]. Even though several other data sources were used
to detect malware (such as malware executions [3] or
JavaScript analysis [22]), the most relevant work to our
approach uses proxy logs [9], [17], [27], [44], [32].
In all these methods, proxy log features are extracted
from real legitimate and malicious samples to train a
data-driven classiﬁer, which is used to ﬁnd new mali-
cious samples from the testing set. There are ﬁve core
differences between these approaches and our approach:
(1) we do not classify individual ﬂows (in our case proxy
log records), but sets of related ﬂows called bags, (2)
we propose a novel representation based on features de-
scribing the dynamics of each bag, (3) the features are
computed from the bags and are invariant against various
changes an attacker could implement to evade detection,
(4) parameters of the proposed representation are learned
automatically from the input data to maximize the detec-
tion performance, (5) the proposed classiﬁcation system
USENIX Association  
25th USENIX Security Symposium  809
3
Approach
Type Method
Features
Wang [41]
Kruegel [25]
Gu [16]
Bilge [6]
Antonakakis [2]
Bailey [3]
Kapravelos [22]
Choi [9]
Zhao [44]
Huang [17]
Ma [27]
Invernizzi [18]
Soska [40]
Nelms [32]
Our approach
U
U
U
S
S
S
S
S
S
S
S
U
S
S
S
anomaly detection
anomaly detection
clustering
random forest
multiple
hierarch. clustering
similarity of trees
SVM + RAkEL
active learning
SVM
multiple
graph clustering
random forests
heuristics
learned repr.+SVM learned bag dynamics
packet payload
URL query parameters
host statistics+SNORT
ﬂow size, time
NXDomains
state changes
abstract syntax tree
URL lexical, host, dns
URL lexical + host
URL lexical
URL lexical + host
proxy log ﬁelds
content of web pages
web paths