title:All Your iFRAMEs Point to Us
author:Niels Provos and
Panayiotis Mavrommatis and
Moheeb Abu Rajab and
Fabian Monrose
All Your iFRAMEs Point to Us
Niels Provos
Panayiotis Mavrommatis
Google Inc.
Moheeb Abu Rajab
Fabian Monrose
Johns Hopkins University
{niels, panayiotis}@google.com
{moheeb, fabian}@cs.jhu.edu
Abstract
As the web continues to play an ever increasing role
in information exchange, so too is it becoming the pre-
vailing platform for infecting vulnerable hosts. In this
paper, we provide a detailed study of the pervasiveness
of so-called drive-by downloads on the Internet. Drive-
by downloads are caused by URLs that attempt to exploit
their visitors and cause malware to be installed and run
automatically. Over a period of 10 months we processed
billions of URLs, and our results shows that a non-trivial
amount, of over 3 million malicious URLs, initiate drive-
by downloads. An even more troubling ﬁnding is that
approximately 1.3% of the incoming search queries to
Google’s search engine returned at least one URL labeled
as malicious in the results page. We also explore sev-
eral aspects of the drive-by downloads problem. Speciﬁ-
cally, we study the relationship between the user brows-
ing habits and exposure to malware, the techniques used
to lure the user into the malware distribution networks,
and the different properties of these networks.
1 Introduction
It should come as no surprise that our increasing reliance
on the Internet for many facets of our daily lives (e.g.,
commerce, communication, entertainment, etc.) makes
the Internet an attractive target for a host of illicit ac-
tivities. Indeed, over the past several years, Internet ser-
vices have witnessed major disruptions from attacks, and
the network itself is continually plagued with malfea-
sance [1]. While the monetary gains from the myriad
of illicit behaviors being perpetrated today (e.g., phish-
ing, spam) is just barely being understood [11], it is clear
that there is a general shift in tactics—wide-scale attacks
aimed at overwhelming computing resources are becom-
ing less prevalent, and instead, traditional scanning at-
tacks are being replaced by other mechanisms. Chief
among these is the exploitation of the web, and the ser-
vices built upon it, to distribute malware.
This change in the playing ﬁeld is particularly alarm-
ing, because unlike traditional scanning attacks that use
push-based infection to increase their population, web-
based malware infection follows a pull-based model. For
the most part, the techniques in use today for deliver-
ing web-malware can be divided into two main cate-
gories. In the ﬁrst case, attackers use various social en-
gineering techniques to entice the visitors of a website
to download and run malware. The second, more de-
vious case, involves the underhanded tactic of targeting
various browser vulnerabilities to automatically down-
load and run—i.e., unknowingly to the visitor—the bi-
nary upon visiting a website. When popular websites
are exploited, the potential victim base from these so-
called drive-by downloads can be far greater than other
forms of exploitation because traditional defenses (e.g.,
ﬁrewalls, dynamic addressing, proxies) pose no barrier
to infection. While social engineering may, in general,
be an important malware spreading vector, in this work
we restrict our focus and analysis to malware delivered
via drive-by downloads.
Recently, Provos et al. [0] provided insights on this
new phenomenon, and presented a cursory overview of
web-based malware. Speciﬁcally, they described a num-
ber of server- and client-side exploitation techniques that
are used to spread malware, and elucidated the mecha-
nisms by which a successful exploitation chain can start
and continue to the automatic installation of malware. In
this paper, we present a detailed analysis of the malware
serving infrastructure on the web using a large corpus of
malicious URLs collected over a period of ten months.
Using this data, we estimate the global prevalence of
drive-by downloads, and identify several trends for dif-
USENIX Association  
17th USENIX Security Symposium 
1
ferent aspects of the web malware problem. Our results
reveal an alarming contribution of Chinese-based web
sites to the web malware problem: overall, 7% of the
malware distribution servers and 
% of the web sites
that link to them are located in China. These results raise
serious question about the security practices employed
by web site administrators.
Additionally, we study several properties of the mal-
ware serving infrastructure, and show that (for the most
part) the malware serving networks are composed of
tree-like structures with strong fan-in edges leading to
the main malware distribution sites. These distribution
sites normally deliver the malware to the victim after a
number of indirection steps traversing a path on the dis-
tribution network tree. More interestingly, we show that
several malware distribution networks have linkages that
can be attributed to various relationships.
In general, the edges of these malware distribution
networks represent the hop-points used to lure users to
the malware distribution site. By investigating these
edges, we reveal a number of causal relationships that
eventually lead to browser exploitation. More troubling,
we show that drive-by downloads are being induced by
mechanisms beyond the conventional techniques of con-
trolling the content of compromised websites.
In par-
ticular, our results reveal that Ad serving networks are
increasingly being used as hops in the malware serving
chain. We attribute this increase to syndication, a com-
mon practice which allows advertisers to rent out part of
their advertising space to other parties. These ﬁndings
are problematic as they show that even protected web-
servers can be used as vehicles for transferring malware.
Additionally, we also show that contrary to common wis-
dom, the practice of following “safe browsing” habits
(i.e., avoiding gray content) by itself is not an effective
safeguard against exploitation.
The remainder of this paper is organized as follows.
In Section , we provide background information on how
vulnerable computer systems can be compromised solely
by visiting a malicious web page. Section  gives an
overview of our data collection infrastructure and in Sec-
tion  we discuss the prevalence of malicious web sites
on the Internet.
In Section , we explore the mecha-
nisms used to inject malicious content into web pages.
We analyze several aspects of the web malware distribu-
tion networks in Section . In Section 7 we provide an
overview of the impact of the installed malware on the
infected system. Section  discusses implications of our
results and Section  presents related work. Finally, we
conclude in Section 10.
2 Background
Unfortunately, there are a number of existing exploita-
tion strategies for installing malware on a user’s com-
puter. One common technique for doing so is by re-
motely exploiting vulnerable network services. How-
ever, lately, this attack strategy has become less suc-
cessful (and presumably, less proﬁtable). Arguably, the
proliferation of technologies such as Network Address
Translators (NATs) and ﬁrewalls make it difﬁcult to re-
motely connect and exploit services running on users’
computers. This, in turn, has lead attackers to seek other
avenues of exploitation. An equally potent alternative is
to simply lure web users to connect to (compromised)
malicious servers that subsequently deliver exploits tar-
geting vulnerabilities of web browsers or their plugins.
Adversaries use a number of techniques to inject con-
tent under their control into benign websites. In many
cases, adversaries exploit web servers via vulnerable
scripting applications. Typically, these vulnerabilities
(e.g., in phpBB or InvisionBoard) allow an adversary
to gain direct access to the underlying operating sys-
tem. That access can often be escalated to super-user
privileges which in turn can be used to compromise any
web server running on the compromised host. In general,
upon successful exploitation of a web server the adver-
sary injects new content to the compromised website. In
most cases, the injected content is a link that redirects
the visitors of these websites to a URL that hosts a script
crafted to exploit the browser. To avoid visual detection
by website owners, adversaries normally use invisible
HTML components (e.g., zero pixel IFRAMEs) to hide
the injected content.
Another common content injection technique is to use
websites that allow users to contribute their own con-
tent, for example, via postings to forums or blogs. De-
pending on the site’s conﬁguration, user contributed con-
tent may be restricted to text but often can also contain
HTML such as links to images or other external content.
This is particularly dangerous, as without proper ﬁlter-
ing in place, the adversary can simply inject the exploit
URL without the need to compromise the web server.
Figure 1 illustrates the main phases in a typical in-
teraction that takes place when a user visits a web-
site with injected malicious content. Upon visiting this
website, the browser downloads the initial exploit script
(e.g., via an IFRAME). The exploit script (in most cases,
javascript) targets a vulnerability in the browser or
one of its plugins.
Interested readers are referred to
Provos et al. [0] for a number of vulnerabilities that
are commonly used to gain control of the infected sys-
tem. Successful exploitation of one of these vulnera-
 
17th USENIX Security Symposium 
USENIX Association
Pre-processing Phase. As Figure 2 illustrates, the data
processing starts from a large web repository maintained
by Google. Our goal is to inspect URLs from this repos-
itory and identify the ones that trigger drive-by down-
loads. However, exhaustive inspection of each URL in
the repository is prohibitively expensive due to the large
number of URLs in the repository (on the order of bil-
lions). Therefore, we ﬁrst use light-weight techniques to
extract URLs that are likely malicious then subject them
to a more detailed analysis and veriﬁcation phase.
Figure 1: A typical Interaction with of drive-by down-
load victim with a landing URL .
bilities results in the automatic execution of the exploit
code, thereby triggering a drive-by download. Drive-by
downloads start when the exploit instructs the browser to
connect to a malware distribution site to retrieve malware
executable(s). The downloaded executable is then auto-
matically installed and started on the infected system1.
Finally, attackers use a number of techniques to evade
detection and complicate forensic analysis. For example,
the use of randomly seeded obfuscated javascript in
their exploit code is not uncommon. Moreover, to com-
plicate network based detection attackers use a number
or redirection steps before the browser eventually con-
tacts the malware distribution site.
3 Infrastructure and Methodology
Our primary objective is to identify malicious web sites
(i.e., URLs that trigger drive-by downloads) and help
improve the safety of the Internet. Before proceeding
further with the details of our data collection methodol-
ogy, we ﬁrst deﬁne some terms we use throughout this
paper. We use the terms landing pages and malicious
URLs interchangeably to denote the URLs that initiate
drive-by downloads when users visit them. In our subse-
quent analysis, we group these URLs according to their
top level domain names and we refer to the resulting set
as the landing sites. In many cases, the malicious pay-
load is not hosted on the landing site, but instead loaded
via an IFRAME or a SCRIPT from a remote site. We
call the remote site that hosts malicious payloads a dis-
tribution site.
In what follows, we detail the different
components of our data collection infrastructure.
Figure 2: URL selection and veriﬁcation workﬂow.
We employ the mapreduce [9] framework to process
billions of web pages in parallel. For each web page, we
extract several features, some of which take advantage of
the fact that many landing URLs are hijacked to include
malicious payload(s) or to point to malicious payload(s)
from a distribution site. For example, we use “out of
place” IFRAMEs, obfuscated JavaScript, or IFRAMEs to
known distribution sites as features. Using a specialized
machine-learning framework [7], we translate these fea-
tures into a likelihood score. We employ ﬁve-fold cross-
validation to measure the quality of the machine-learning
framework. The cross-validation operates by splitting
the data set into 5 randomly chosen partitions and then
training on four partitions while using the remaining par-
tition for validation. This process is repeated ﬁve times.
For each trained model, we create an ROC curve and use
the average ROC curve to estimate the overall accuracy.
Using this ROC curve, we estimate the false positive and
detection rate for different thresholds. Our infrastructure
pre-processes roughly one billion pages daily. In order to
fully utilize the capacity of the subsequent detailed ver-
iﬁcation phase, we choose a threshold score that results
in an outcome false positive rate of about 10−3 with a
corresponding detection rate of approximately 0.9. This
amounts to about one million URLs that we subject to
the computationally more expensive veriﬁcation phase.
USENIX Association  
17th USENIX Security Symposium 

In addition to analyzing web pages in the crawled web
repository, we also regularly select several hundred thou-
sands URLs for in-depth veriﬁcation. These URLs are
randomly sampled from popular URLs as well as from
the global index. We also process URLs reported by
users.
new URLs are ﬂagged as malicious. The veriﬁcation sys-
tem records all the network interactions as well as the
state changes. In what follows, we describe how we pro-
cess the network traces associated with the detected ma-
licious URLs to shed light on the malware distribution
infrastructure.
Veriﬁcation Phase. This phase aims to verify whether
a candidate URL from the pre-processing phase is ma-
licious (i.e., initiates a drive-by download). To do that,
we developed a large scale web-honeynet that simultane-
ously runs a large number of Microsoft Windows images
in virtual machines. Our system design draws on the ex-
perience from earlier work [
tures that are speciﬁc to our goals. In what follows we
discuss the details of the URL veriﬁcation process.
], and includes unique fea-
Each honeypot instance runs an unpatched version of
Internet Explorer. To inspect a candidate URL , the sys-
tem ﬁrst loads a clean Windows image then automati-
cally starts the browser and instructs it to visit the candi-
date URL . We detect malicious URLs using a combina-
tion of execution based heuristics and results from anti-
virus engines. Speciﬁcally, for each visited URL we run
the virtual machine for approximately two minutes and
monitor the system behavior for abnormal state changes
including ﬁle system changes, newly created processes
and changes to the system’s registry. Additionally, we
subject the HTTP responses to virus scans using multi-
ple anti-virus engines. To detect malicious URLs , we de-
velop scoring heuristics used to determines the likelihood
that a URL is malicious. We determine a URL score based
on a combined measure of the different state changes
resulting from visiting the URL . Our heuristics score
URLs based on the number of created processes, the
number of observed registry changes and the number of
ﬁle system changes resulting from visiting the URL .
To limit false positives, we choose a conservative de-
cision criteria that uses an empirically derived thresh-
old to mark a URL as malicious. This threshold is set
such that it will be met if we detect changes in the sys-
tem state, including the ﬁle system as well as creation
of new processes. A visited URL is marked as malicious
if it meets the threshold and one of the incoming HTTP
responses is marked as malicious by at least one anti-
virus scanner. Our extensive evaluation shows that this
criteria introduces negligible false positives. Finally, a
URL that meets the threshold requirement but has no in-
coming payload ﬂagged by any of the anti-virus engines,
is marked as suspicious.
On average, the detailed veriﬁcation stage processes
about one million URLs daily, of which roughly 25, 000
Constructing the Malware Distribution Networks.
To understand the properties of the web malware serving
infrastructure on the Internet, we analyze the recorded
network traces associated with the detected malicious
URLs to construct the malware distribution networks.
We deﬁne a distribution network as the set of malware
delivery trees from all the landing sites that lead to a par-
ticular malware distribution site. A malware delivery tree
consists of the landing site, as the leaf node, and all nodes
(i.e., web sites) that the browser visits until it contacts the
malware distribution site (the root of the tree). To con-
struct the delivery trees we extract the edges connecting
these nodes by inspecting the Referer header from the
recorded successive HTTP requests the browser makes
after visiting the landing page. However, in many cases
the Referer headers are not sufﬁcient to extract the
full chain. For example, when the browser redirection
results from an external script the Referrer, in this
case, points to the base page and not the external script
ﬁle. Additionally, in many cases the Referer header is
not set (e.g., because the requests are made from within
a browser plugin or newly-downloaded malware).
To connect the missing causality links, we interpret the
HTML and JavaScript content of the pages fetched by the
browser and extract all the URLs from the fetched pages.
Then, to identify causal edges we look for any URLs that
match any of the HTTP fetches that were subsequently
visited by the browser.
In some cases, URLs contain
randomly generated strings, so some requests cannot be
matched exactly.
In these cases, we apply heuristics
based on edit distance to identify the most probable par-
ent of the URL . Finally, for each malware distribution
site, we construct its associated distribution network by
combining the different malware delivery trees from all
landing pages that lead to that site.
Our infrastructure has been live for more than one
year, continuously monitoring the web and detecting ma-
licious URLs. In what follows, we report our ﬁndings
based on analyzing data collected during that time pe-
riod. Again, recall that we focus here on the perva-
siveness of malicious activity (perpetrated by drive-by
downloads) that is induced simply by visiting a landing
page, thereafter requiring no additional interaction on the
 
17th USENIX Security Symposium 
USENIX Association
client’s part (e.g., clicking on embedded links). Finally,
we note that due to the large scale of our data collection
and some infrastructural constraints, a number longitu-
dinal aspects of the web malware problem (e.g., the life-