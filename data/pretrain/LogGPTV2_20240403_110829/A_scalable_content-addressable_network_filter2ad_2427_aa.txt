title:A scalable content-addressable network
author:Sylvia Ratnasamy and
Paul Francis and
Mark Handley and
Richard M. Karp and
Scott Shenker
A Scalable Content-Addressable Network
Sylvia Ratnasamy;
Paul Francis
Mark Handley
Richard Karp;
Scott Shenker
Dept. of Electrical Eng. & Comp. Sci.
University of California, Berkeley
Berkeley, CA, USA
ACIRI
AT&T Center for Internet Research at ICSI
Berkeley, CA, USA
ABSTRACT
Hash tables – which map “keys” onto “values” – are an essential building
block in modern software systems. We believe a similar functionality would
be equally valuable to large distributed systems. In this paper, we intro-
duce the concept of a Content-Addressable Network (CAN) as a distributed
infrastructure that provides hash table-like functionality on Internet-like
scales. The CAN is scalable, fault-tolerant and completely self-organizing,
and we demonstrate its scalability, robustness and low-latency properties
through simulation.
1.
INTRODUCTION
A hash table is a data structure that efﬁciently maps “keys” onto
“values” and serves as a core building block in the implementa-
tion of software systems. We conjecture that many large-scale dis-
tributed systems could likewise beneﬁt from hash table functional-
ity. We use the term Content-Addressable Network (CAN) to de-
scribe such a distributed, Internet-scale, hash table.
Perhaps the best example of current Internet systems that could
potentially be improved by a CAN are the recently introduced peer-
to-peer ﬁle sharing systems such as Napster [14] and Gnutella [6].
In these systems, ﬁles are stored at the end user machines (peers)
rather than at a central server and, as opposed to the traditional
client-server model, ﬁles are transferred directly between peers.
These peer-to-peer systems have become quite popular. Napster
was introduced in mid-1999 and, as of December 2000, the soft-
ware has been down-loaded by 50 million users, making it the
fastest growing application on the Web. New ﬁle sharing systems
such as Scour, FreeNet, Ohaha, Jungle Monkey, and MojoNation
have all been introduced within the last year.
While there remains some (quite justiﬁed) skepticism about the
business potential of these ﬁle sharing systems, we believe their
rapid and wide-spread deployment suggests that there are impor-
tant advantages to peer-to-peer systems. Peer-to-peer designs har-
ness huge amounts of resources - the content advertised through
Napster has been observed to exceed 7 TB of storage on a single
day,1 without requiring centralized planning or huge investments in
Private communication with Yin Zhang and Vern Paxson
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’01, August 27-31, 2001, San Diego, California, USA..
Copyright 2001 ACM 1-58113-411-8/01/0008 ...$5.00.
hardware, bandwidth, or rack space. As such, peer-to-peer ﬁle shar-
ing may lead to new content distribution models for applications
such as software distribution, ﬁle sharing, and static web content
delivery.
Unfortunately, most of the current peer-to-peer designs are not
scalable. For example, in Napster a central server stores the in-
dex of all the ﬁles available within the Napster user community.
To retrieve a ﬁle, a user queries this central server using the de-
sired ﬁle’s well known name and obtains the IP address of a user
machine storing the requested ﬁle. The ﬁle is then down-loaded di-
rectly from this user machine. Thus, although Napster uses a peer-
to-peer communication model for the actual ﬁle transfer, the pro-
cess of locating a ﬁle is still very much centralized. This makes it
both expensive (to scale the central directory) and vulnerable (since
there is a single point of failure). Gnutella goes a step further and
de-centralizes the ﬁle location process as well. Users in a Gnutella
network self-organize into an application-level mesh on which re-
quests for a ﬁle are ﬂooded with a certain scope. Flooding on every
request is clearly not scalable [7] and, because the ﬂooding has to
be curtailed at some point, may fail to ﬁnd content that is actu-
ally in the system. We started our investigation with the question:
could one make a scalable peer-to-peer ﬁle distribution system? We
soon recognized that central to any peer-to-peer system is the in-
dexing scheme used to map ﬁle names (whether well known or dis-
covered through some external mechanism) to their location in the
system. That is, the peer-to-peer ﬁle transfer process is inherently
scalable, but the hard part is ﬁnding the peer from whom to retrieve
the ﬁle. Thus, a scalable peer-to-peer system requires, at the very
least, a scalable indexing mechanism. We call such indexing sys-
tems Content-Addressable Networks and, in this paper, propose a
particular CAN design.
However, the applicability of CANs is not limited to peer-to-
peer systems. CANs could also be used in large scale storage
management systems such as OceanStore [10], Farsite [1], and
Publius [13]. These systems all require efﬁcient insertion and re-
trieval of content in a large distributed storage infrastructure; a scal-
able indexing mechanism is an essential component of such an in-
frastructure. In fact, as we discuss in Section 5, the OceanStore
system already includes a CAN in its core design (although the
OceanStore CAN, based on Plaxton’s algorithm[15], is somewhat
different from what we propose here).
Another potential application for CANs is in the construction of
wide-area name resolution services that (unlike the DNS) decou-
ple the naming scheme from the name resolution process thereby
enabling arbitrary, location-independent naming schemes.
Our interest in CANs is based on the belief that a hash table-
like abstraction would give Internet system developers a powerful
design tool that could enable new applications and communication
models. However, in this paper our focus is not on the use of CANs
but on their design. In [17], we describe, in some detail, one possi-
ble application, which we call a “grass-roots” content distribution
system, that leverages our CAN work.
As we have said, CANs resemble a hash table; the basic oper-
ations performed on a CAN are the insertion, lookup and deletion
of (key,value) pairs. In our design, the CAN is composed of many
individual nodes. Each CAN node stores a chunk (called a zone) of
the entire hash table. In addition, a node holds information about
a small number of “adjacent” zones in the table. Requests (insert,
lookup, or delete) for a particular key are routed by intermediate
CAN nodes towards the CAN node whose zone contains that key.
Our CAN design is completely distributed (it requires no form of
centralized control, coordination or conﬁguration), scalable (nodes
maintain only a small amount of control state that is independent
of the number of nodes in the system), and fault-tolerant (nodes
can route around failures). Unlike systems such as the DNS or IP
routing, our design does not impose any form of rigid hierarchical
naming structure to achieve scalability. Finally, our design can be
implemented entirely at the application level.
In what follows, we describe our basic design for a CAN in Sec-
tion 2, describe and evaluate this design in more detail in Section 3
and discuss our results in Section 4. We discuss related work in
Section 5 and directions for future work in Section 6.
2. DESIGN
First we describe our Content Addressable Network in its most
basic form; in Section 3 we present additional design features that
greatly improve performance and robustness.
Our design centers around a virtual d-dimensional Cartesian co-
ordinate space on a d-torus.2 This coordinate space is completely
logical and bears no relation to any physical coordinate system. At
any point in time, the entire coordinate space is dynamically par-
titioned among all the nodes in the system such that every node
“owns” its individual, distinct zone within the overall space. For
example, Figure 1 shows a 2-dimensional [ ; ] (cid:2) [ ; ] coordinate
space partitioned between 5 CAN nodes.
This virtual coordinate space is used to store (key,value) pairs
as follows:
to store a pair (K,V), key K is deterministically
mapped onto a point P in the coordinate space using a uniform
hash function. The corresponding (key,value) pair is then stored
at the node that owns the zone within which the point P lies. To
retrieve an entry corresponding to key K, any node can apply the
same deterministic hash function to map K onto point P and then
retrieve the corresponding value from the point P . If the point P
is not owned by the requesting node or its immediate neighbors,
the request must be routed through the CAN infrastructure until it
reaches the node in whose zone P lies. Efﬁcient routing is therefore
a critical aspect of a CAN.
Nodes in the CAN self-organize into an overlay network that rep-
resents this virtual coordinate space. A node learns and maintains
the IP addresses of those nodes that hold coordinate zones adjoin-
ing its own zone. This set of immediate neighbors in the coordinate
space serves as a coordinate routing table that enables routing be-
tween arbitrary points in this space.
We will describe the three most basic pieces of our design: CAN
routing, construction of the CAN coordinate overlay, and mainte-
nance of the CAN overlay.
2.1 Routing in a CAN
For simplicity, the illustrations in this paper do not show a torus,
so the reader must remember that the coordinate space wraps.
Intuitively, routing in a Content Addressable Network works by
following the straight line path through the Cartesian space from
source to destination coordinates.
A CAN node maintains a coordinate routing table that holds the
IP address and virtual coordinate zone of each of its immediate
neighbors in the coordinate space. In a d-dimensional coordinate
space, two nodes are neighbors if their coordinate spans overlap
along d(cid:0) dimensions and abut along one dimension. For example,
in Figure 2, node 5 is a neighbor of node 1 because its coordinate
zone overlaps with 1’s along the Y axis and abuts along the X-axis.
On the other hand, node 6 is not a neighbor of 1 because their co-
ordinate zones abut along both the X and Y axes. This purely local
neighbor state is sufﬁcient to route between two arbitrary points in
the space: A CAN message includes the destination coordinates.
Using its neighbor coordinate set, a node routes a message towards
its destination by simple greedy forwarding to the neighbor with
coordinates closest to the destination coordinates. Figure 2 shows
a sample routing path.
For a d dimensional space partitioned into n equal zones, the av-
erage routing path length is (d=)(n=d) hops and individual nodes
maintain d neighbors3. These scaling results mean that for a d-
dimensional space, we can grow the number of nodes (and hence
zones) without increasing per node state while the average path
length grows as O(n=d).
Note that many different paths exist between two points in the
space and so, even if one or more of a node’s neighbors were to
crash, a node can automatically route along the next best available
path.
If however, a node loses all its neighbors in a certain direction,
and the repair mechanisms described in Section 2.3 have not yet
rebuilt the void in the coordinate space, then greedy forwarding
may temporarily fail. In this case, a node may use an expanding
ring search (using stateless, controlled ﬂooding over the unicast
CAN overlay mesh) to locate a node that is closer to the destination
than itself. The message is then forwarded to this closer node, from
which greedy forwarding is resumed.
2.2 CAN construction
As described above, the entire CAN space is divided amongst
the nodes currently in the system. To allow the CAN to grow in-
crementally, a new node that joins the system must be allocated its
own portion of the coordinate space. This is done by an existing
node splitting its allocated zone in half, retaining half and handing
the other half to the new node.
The process takes three steps:
1. First the new node must ﬁnd a node already in the CAN.
2. Next, using the CAN routing mechanisms, it must ﬁnd a node
whose zone will be split.
3. Finally, the neighbors of the split zone must be notiﬁed so
that routing can include the new node.
Bootstrap
A new CAN node ﬁrst discovers the IP address of any node cur-
rently in the system. The functioning of a CAN does not depend
Recently proposed routing algorithms for location services [15,
20] route in O(log n) hops with each node maintaining O(log n)
neighbors. Notice that were we to select the number of dimensions
d=(log n)=, we could achieve the same scaling properties.We
choose to hold d ﬁxed independent of n, since we envision apply-
ing CANs to very large systems with frequent topology changes.
In such systems, it is important to keep the number of neighbors
independent of the system size
1.0
C
(0-0.5,0.5-1.0)
(0.5-0.75,0.5-1.0)
D
E
(0.75-1.0,0.5-1.0)
A
(0-0.5,0-0.5)
B
(0.5-1.0,0.0-0.5)
1.0
0.0
0.0
node B’s virtual coordinate zone
Figure 1: Example 2-d space with 5 nodes
2
1
4
5
6
3
(x,y)
sample routing 
path from node 1  
to point (x,y)
6
3
2
1
7
5
4
1’s coordinate neighbor set = {2,3,4,5} 
7’s coordinate neighbor set = { } 
1’s coordinate neighbor set = {2,3,4,7} 
7’s coordinate neighbor set = {1,2,4,5} 
Figure 2: Example 2-d space before node
7 joins
Figure 3: Example 2-d space after node
7 joins
on the details of how this is done, but we use the same bootstrap
mechanism as YOID [4].
As in [4] we assume that a CAN has an associated DNS domain
name, and that this resolves to the IP address of one or more CAN
bootstrap nodes. A bootstrap node maintains a partial list of CAN
nodes it believes are currently in the system. Simple techniques to
keep this list reasonably current are described in [4].
To join a CAN, a new node looks up the CAN domain name in
DNS to retrieve a bootstrap node’s IP address. The bootstrap node
then supplies the IP addresses of several randomly chosen nodes
currently in the system.
Finding a Zone
The new node then randomly chooses a point P in the space and
sends a JOIN request destined for point P . This message is sent
into the CAN via any existing CAN node. Each CAN node then
uses the CAN routing mechanism to forward the message, until it
reaches the node in whose zone P lies.
This current occupant node then splits its zone in half and assigns
one half to the new node. The split is done by assuming a certain
ordering of the dimensions in deciding along which dimension a
zone is to be split, so that zones can be re-merged when nodes leave.
For a 2-d space a zone would ﬁrst be split along the X dimension,
then the Y and so on. The (key, value) pairs from the half zone to
be handed over are also transfered to the new node.
Joining the Routing
Having obtained its zone, the new node learns the IP addresses of
its coordinate neighbor set from the previous occupant. This set is
a subset of the previous occupant’s neighbors, plus that occupant
itself. Similarly, the previous occupant updates its neighbor set to
eliminate those nodes that are no longer neighbors. Finally, both
the new and old nodes’ neighbors must be informed of this realloca-
tion of space. Every node in the system sends an immediate update
message, followed by periodic refreshes, with its currently assigned
zone to all its neighbors. These soft-state style updates ensure that
all of their neighbors will quickly learn about the change and will
update their own neighbor sets accordingly. Figures 2 and 3 show
an example of a new node (node 7) joining a 2-dimensional CAN.
The addition of a new node affects only a small number of ex-
isting nodes in a very small locality of the coordinate space. The
number of neighbors a node maintains depends only on the dimen-
sionality of the coordinate space and is independent of the total
number of nodes in the system. Thus, node insertion affects only
O(number of dimensions) existing nodes, which is important for
CANs with huge numbers of nodes.
2.3 Node departure, recovery and CAN main-
tenance
When nodes leave a CAN, we need to ensure that the zones they
occupied are taken over by the remaining nodes. The normal pro-
cedure for doing this is for a node to explicitly hand over its zone
and the associated (key,value) database to one of its neighbors. If
the zone of one of the neighbors can be merged with the departing
node’s zone to produce a valid single zone, then this is done. If
not, then the zone is handed to the neighbor whose current zone is
smallest, and that node will then temporarily handle both zones.
The CAN also needs to be robust to node or network failures,
where one or more nodes simply become unreachable. This is han-
dled through an immediate takeover algorithm that ensures one of
the failed node’s neighbors takes over the zone. However in this
case the (key,value) pairs held by the departing node are lost until
the state is refreshed by the holders of the data4.
Under normal conditions a node sends periodic update messages
to each of its neighbors giving its zone coordinates and a list of its
neighbors and their zone coordinates. The prolonged absence of an
update message from a neighbor signals its failure.
Once a node has decided that its neighbor has died it initiates
the takeover mechanism and starts a takeover timer running. Each
neighbor of the failed node will do this independently, with the
timer initialized in proportion to the volume of the node’s own
zone. When the timer expires, a node sends a TAKEOVER message
conveying its own zone volume to all of the failed node’s neighbors.
On receipt of a TAKEOVER message, a node cancels its own
timer if the zone volume in the message is smaller that its own zone
volume, or it replies with its own TAKEOVER message. In this way,
a neighboring node is efﬁciently chosen that is still alive and has a
small zone volume5.
Under certain failure scenarios involving the simultaneous fail-
ure of multiple adjacent nodes, it is possible that a node detects
To prevent stale entries as well as to refresh lost entries, nodes
that insert (key,value) pairs into the CAN periodically refresh these
entries
Additional metrics such as load or the quality of connectivity can
also be taken into account, but in the interests of simplicity we
won’t discuss these further here.
a failure, but less than half of the failed node’s neighbors are still
reachable. If the node takes over another zone under these circum-
stances, it is possible for the CAN state to become inconsistent. In
such cases, prior to triggering the repair mechanism, the node per-
forms an expanding ring search for any nodes residing beyond the
failure region and hence it eventually rebuilds sufﬁcient neighbor
state to initiate a takeover safely.
Finally, both the normal leaving procedure and the immediate
takeover algorithm can result in a node holding more than one
zone. To prevent repeated further fragmentation of the space, a
background zone-reassignment algorithm, which we describe in
Appendix A, runs to ensure that the CAN tends back towards one
zone per node.
3. DESIGN IMPROVEMENTS
Our basic CAN algorithm as described in the previous section
provides a balance between low per-node state (O(d) for a
d-dimensional space) and short path lengths with O(dn=d) hops
for d dimensions and n nodes. This bound applies to the number
of hops in the CAN path. These are application level hops, not IP-
level hops, and the latency of each hop might be substantial; recall
that nodes that are adjacent in the CAN might be many miles and
many IP hops away from each other. The average total latency of
a lookup is the average number of CAN hops times the average la-
tency of each CAN hop. We would like to achieve a lookup latency
that is comparable within a small factor to the underlying IP path
latencies between the requester and the CAN node holding the key.
In this section, we describe a number of design techniques whose
primary goal is to reduce the latency of CAN routing. Not unin-
tentionally, many of these techniques offer the additional advan-
tage of improved CAN robustness both in terms of routing and data
availability. In a nutshell, our strategy in attempting to reduce path
latency is to reduce either the path length or the per-CAN-hop la-
tency. A ﬁnal improvement we make to our basic design is to add
simple load balancing mechanisms (described in Sections 3.7 and
3.8).
First, we describe and evaluate each design feature individually
and then, in Section 4, discuss how together they affect the overall
performance. These added features yield signiﬁcant improvements
but come at the cost of increased per-node state (although per-node
state still remains independent of the number of nodes in the sys-
tem) and somewhat increased complexity. The extent to which the
following techniques are applied (if at all) involves a trade-off be-
tween improved routing performance and system robustness on the
one hand and increased per-node state and system complexity on
the other. Until we have greater deployment experience, and know
the application requirements better, we are not prepared to decide
on these tradeoffs.
We simulated our CAN design on Transit-Stub (TS) topologies
using the GT-ITM topology generator [22]. TS topologies model
networks using a 2-level hierarchy of routing domains with transit
domains that interconnect lower level stub domains.
3.1 Multi-dimensioned coordinate spaces
The ﬁrst observation is that our design does not restrict the di-
mensionality of the coordinate space. Increasing the dimensions
of the CAN coordinate space reduces the routing path length, and
hence the path latency, for a small increase in the size of the coor-
dinate routing table.
Figure 4 measures this effect of increasing dimensions on rout-
ing path length. We plot the path length for increasing numbers of
CAN nodes for coordinate spaces with different dimensions. For a
system with n nodes and d dimensions, we see that the path length
scales as O(d(n=d)) in keeping with the analytical results for per-
fectly partitioned coordinate spaces.
Because increasing the number of dimensions implies that a node
has more neighbors, the routing fault tolerance also improves as a
node now has more potential next hop nodes along which messages
can be routed in the event that one or more neighboring nodes crash.
3.2 Realities: multiple coordinate spaces
The second observation is that we can maintain multiple, inde-
pendent coordinate spaces with each node in the system being as-
signed a different zone in each coordinate space. We call each such
coordinate space a “reality”. Hence, for a CAN with r realities, a
single node is assigned r coordinate zones, one on every reality and
holds r independent neighbor sets.
The contents of the hash table are replicated on every reality.
This replication improves data availability. For example, say a
pointer to a particular ﬁle is to be stored at the coordinate loca-
tion (x,y,z). With four independent realities, this pointer would
be stored at four different nodes corresponding to the coordinates
(x,y,z) on each reality and hence it is unavailable only when all
four nodes are unavailable. Multiple realities also improve rout-
ing fault tolerance, because in the case of a routing breakdown on
one reality, messages can continue to be routed using the remaining
realities.
Further, because the contents of the hash table are replicated on
every reality, routing to location (x,y,z) translates to reaching (x,y,z)
on any reality. A given node owns one zone per reality each of
which is at a distinct, and possibly distant, location in the coordi-
nate space. Thus, an individual node has the ability to reach distant
portions of the coordinate space in a single hop, thereby greatly re-
ducing the average path length. To forward a message, a node now
checks all its neighbors on each reality and forwards the message to
that neighbor with coordinates closest to the destination. Figure 5
plots the path length for increasing numbers of nodes for different
numbers of realities. From the graph, we see that realities greatly
reduce path length. Thus, using multiple realities reduces the path
length and hence the overall CAN path latency.
Multiple dimensions versus multiple realities