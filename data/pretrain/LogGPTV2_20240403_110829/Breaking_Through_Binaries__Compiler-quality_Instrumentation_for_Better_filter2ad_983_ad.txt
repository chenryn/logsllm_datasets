clean_text
jasper
readelf
sfconvert
tcpdump
unrtf
Mean Rel. Increase
Mean MWU Score
ZAFL vs. AFL-Dyninst
rel.
rel.
queue
crash
1.06
0.80
4.99
1.41
1.25
1.68
2.30
2.60
3.52
1.00
4.04
1.30
2.68
0.90
1.50
36.1
+26% +48% +260% +131% +159% +337%
0.018
ZAFL vs. AFL-QEMU
rel.
rel.
queue
crash
2.79
8.00
4.51
1.95
6.25
1.93
1.67
1.39
5.60
1.00
3.30
1.18
4.99
3.17
1.62
35.5
rel.
total
1.25
1.79
1.48
2.70
1.03
0.96
1.44
1.78
rel.
total
2.59
3.05
3.23
2.05
3.44
0.90
4.95
2.51
0.001
0.001
0.002
0.001
0.001
Table 5: ZAFL’s real-world software mean triaged crashes and total/queued
test cases rel. to AFL-Dyninst and AFL-QEMU. We report geometric means
for all metrics and MWU test p-values (p ≤ 0.05 indicates signiﬁcance).
over to real-world programs. We therefore expand our crash-
ﬁnding evaluation to eight diverse, real-world benchmarks and
extend all trials to 24 hours as per the standard set by Klees et
al. [52]. We further show that ZAFL achieves compiler-quality
performance in a coverage-tracing overhead comparison of
all three instrumenters.
7.3.1 Benchmarks
To capture the diversity of real-world software we select
eight binaries of varying type, size, and libraries which previ-
ously appear in the fuzzing literature: bsdtar, cert-basic,
clean_text, jasper, readelf, sfconvert, tcpdump, and
unrtf. We intentionally select older versions known to con-
tain AFL-ﬁndable bugs to facilitate a self-evident bug-ﬁnding
comparison. Statistics for each (e.g., package, size, number
of basic blocks) are listed in Table 8.
7.3.2 Experimental Setup and Infrastructure
In both crash-ﬁnding and overhead experiments we conﬁgure
instrumenters and binaries as described in § 7.1 and § 7.2.1,
and utilize either AFL- or developer-provided seed inputs
in fuzzing evaluations. For crash-ﬁnding, we fuzz all instru-
mented binaries with AFL on a cluster for 8×24-hour trials
each and to evaluate overhead, we perform 5×24-hour trials
on our LAVA-M experiment infrastructure (§ 7.2.2).
7.3.3 Real-world Crash-ﬁnding
We apply all ZAFL-implemented transformations (Table 3) to
all eight binaries, but omit context sensitivity for clean_text
as it otherwise consumes 100% of its coverage map. Triage is
performed as in § 7.2.3 but is based on stack hashing as seen in
the literature [52, 57, 66].2 Table 5 shows ZAFL-instrumented
fuzzing crash-ﬁnding as well as total and queued test cases
relative to AFL-Dyninst and AFL-QEMU. We further report
the geometric mean Mann-Whitney U signiﬁcance test p-
values across all metrics.
ZAFL versus AFL-Dyninst: Our results show ZAFL aver-
ages 26% more real-world crashes and 48% more test cases
than AFL-Dyninst in 24 hours. Though ZAFL ﬁnds 10–20%
fewer on bsdtar and tcpdump, the raw differences amount
to only 1–2 crashes, suggesting that it and AFL-Dyninst con-
verge on these two benchmarks (as shown in Figure 3d). Like-
wise for readelf our triage reveals two unique crashes across
all trials, both found by all three instrumenters. For all others
ZAFL holds a lead (as shown in Figure 3), averaging 61%
more crashes. Given the Mann-Whitney U p-values (0.001–
0.018) below the 0.05 signiﬁcance level, we conclude that
ZAFL’s compiler-quality transformations bear a statistically
signiﬁcant advantage over AFL-Dyninst.
ZAFL versus AFL-QEMU: While ZAFL surpasses AFL-
QEMU’s LAVA-M crash-ﬁnding by 42%, ZAFL’s real-world
crash-ﬁnding is an even higher 131%. Apart from the two
readelf bugs found by all three instrumenters, ZAFL’s
fuzzing-enhancing program transformations and 159% higher
execution rate allow it to hone-in on more crash-triggering
paths on average. As with AFL-Dyninst, comparing to
AFL-QEMU produces Mann-Whitney U p-values (0.001–
0.002) which prove ZAFL’s increased effectiveness is statis-
tically signiﬁcant. Furthermore the disparity between AFL-
QEMU’s LAVA-M and real-world crash-ﬁnding suggests that
increasingly-complex binaries heighten the need for more
powerful binary rewriters.
7.3.4 Real-world Coverage-tracing Overhead
For our coverage-tracing overhead evaluation we follow es-
tablished practice [62]: we collect 5×24-hour test case dumps
per benchmark; instrument a forkserver-only “baseline” (i.e.,
no coverage-tracing) version of each benchmark; log every
instrumented binary’s coverage-tracing time for each test case
per dump; apply 30% trimmed-mean de-noising on the exe-
cution times per instrumenter-benchmark pair; and scale the
2In stack hashing we consider both function names and lines; and con-
dense recursive calls as they would otherwise over-approximate bug counts.
1692    30th USENIX Security Symposium
USENIX Association
0.00.20.40.60.8Prop. Test Cases / 24-hours0.00.20.40.60.81.0Rel. Avg. CrashesAFL-Dyn.AFL-QEMUZAFL0.00.20.40.60.8Prop. Test Cases / 24-hours0.00.20.40.60.81.0Rel. Avg. CrashesAFL-Dyn.AFL-QEMUZAFL0.00.20.40.60.8Prop. Test Cases / 24-hours0.00.20.40.60.81.0Rel. Avg. CrashesAFL-Dyn.AFL-QEMUZAFL0.00.20.40.60.8Prop. Test Cases / 24-hours0.00.20.40.60.81.0Rel. Avg. CrashesAFL-Dyn.AFL-QEMUZAFLsome binary characteristics are better-suited for dynamic vs.
static rewriting, existing instrumenters do not match ZAFL’s
performance across all benchmarks. Our Mann-Whitney U
tests reveal that both ZAFL-NONE and ZAFL-ALL obtain p-
values of 0.012, suggesting that ZAFL achieves statistically
better performance over AFL-QEMU.
Comparing ZAFL to Compiler Instrumentation: On av-
erage, compared to a forkserver-only binary, ZAFL incurs
a baseline overhead of 5% just for adding rewriting sup-
port to the binary; tracing all code coverage increases over-
head to 32%; optimizing coverage tracing using graph anal-
ysis reduces overhead to 20%; and applying all fuzzing-
enhancing program transformations brings overhead back
up to 27%. These overheads are similar to the 24% overhead
of AFL’s compiler-based instrumentation, and slightly better
than AFL’s assembler-based trampolining overhead of 34%.
Comparing ZAFL-NONE and ZAFL-ALL to compiler instru-
mentation yields mean Mann-Whitney U p-values ranging
0.12–0.18 which, being larger than 0.05, suggests that ZAFL
is indistinguishable from compiler-level performance.
7.4 Fuzzing Closed-source Binaries
To evaluate whether ZAFL’s improvements extend to true
binary-only use cases, we expand our evaluation with ﬁve di-
verse, closed-source binary benchmarks. Our results show that
ZAFL’s compiler-quality instrumentation and speed help re-
veal more unique crashes than AFL-Dyninst and AFL-QEMU
across all benchmarks. We further conduct several case studies
showing that ZAFL achieves far shorter time-to-bug-discovery
compared to AFL-Dyninst and AFL-QEMU.
7.4.1 Benchmarks
We drill-down the set of all closed-source binaries we
tested with ZAFL (Table 9) into ﬁve AFL-compatible (i.e.,
command-line interfacing) benchmarks: idat64 from IDA
Pro, nconvert from XNView’s NConvert, nvdisasm from
NVIDIA’s CUDA Utilities, pngout from Ken Silverman’s
PNGOUT, and unrar from RarLab’s RAR. Table 9 lists the
key features of each benchmark.
7.4.2 Closed-source Crash-ﬁnding
We repeat the evaluation from § 7.3.3, running ﬁve 24-hour ex-
periments per conﬁguration. Our results (mean unique triaged
crashes, total and queued test cases, and MWU p-scores)
among all benchmarks are shown in Table 6; and plots of
unique triaged crashes over time are shown in Figure 5.
ZAFL versus AFL-Dyninst: Despite AFL-Dyninst being
faster on idat64, nconvert, nvdisasm, and unrar, ZAFL
averages a statistically-signiﬁcant (mean MWU p-value of
0.036) 55% higher crash-ﬁnding. We believe AFL-Dyninst’s
speed, small queues, and lack of crashes in unrar are due
to it missing signiﬁcant parts of these binaries, as our own
Figure 4: Compiler, assembler, AFL-Dyninst, AFL-QEMU, and ZAFL
fuzzing instrumentation performance relative to baseline (higher is better).
resulting overheads relative to baseline.
We compare ZAFL to AFL-Dyninst, AFL-QEMU, and to
the compiler- and assembler-based instrumentation available
in AFL [93]. We assess all aspects of ZAFL’s performance: (1)
its baseline forkserver-only rewritten binary overhead (ZAFL-
FSRVR); and instrumentation overheads (2) with no trans-
formations (ZAFL-NONE), (3) only performance-enhancing
transformations (ZAFL-PERF), and (4) all (Table 3) trans-
formations (ZAFL-ALL). We additionally compute geomet-
ric mean Mann-Whitney U p-values of both ZAFL-NONE’s
and ZAFL-ALL’s execution times compared to those of com-
piler and assembler instrumentation, AFL-Dyninst, and AFL-
QEMU among all benchmarks.
Figure 4 displays the instrumenters’ relative overheads. On
average, ZAFL-FSRVR, ZAFL-NONE, ZAFL-PERF, and ZAFL-
ALL obtain overheads of 5%, 32%, 17%, and 27%, while com-
piler and assembler instrumentation average 24% and 34%,
and AFL-Dyninst and AFL-QEMU average 88% and 256%,
respectively. Thus, even ZAFL with all fuzzing-enhancing
transformations approaches compiler performance.
ZAFL versus AFL-Dyninst: We observe ZAFL performs
slightly worse on sfconvert as it has the fewest basic blocks
by far we believe our rewriting overhead is more pronounced
on such tiny binaries. Other results suggest that this case
is pathological. Even ZAFL’s most heavyweight conﬁgura-
tion (ZAFL-ALL) incurs 61% less average overhead than
AFL-Dyninst, even though this comparison includes ZAFL’s
performance-enhancing transformations. If omitted, this still
leaves ZAFL ahead of AFL-Dyninst—which, too, beneﬁts
from performance-enhancing single successor-based prun-
ing. Comparing the execution times of ZAFL-NONE and
ZAFL-ALL to AFL-Dyninst’s yields mean Mann-Whitney
U p-values of 0.020–0.023. As these are below 0.05, sug-
gesting that ZAFL, both with- and without-transformations,
achieves statistically better performance over AFL-Dyninst.
ZAFL versus AFL-QEMU: Though AFL-QEMU’s block
caching reduces its overhead from previous reports [62], ZAFL
outperforms it with nearly 229% less overhead. Interestingly,
AFL-QEMU beats AFL-Dyninst on jasper, consistent with
the relative throughput gains in Table 5. Thus, while it appears
USENIX Association
30th USENIX Security Symposium    1693
bsdtarcert-basicclean_textjasperreadelfsfconverttcpdumpunrtfAVG.Benchmark0.00.10.20.30.40.50.60.70.80.91.0Throughput Rel. to BaselineCompilerAssemblerAFL-DyninstAFL-QEMUZAFL-fsrvrZAFL-noneZAFL-perfZAFL-all(a) idat64
Figure 5: Closed-source binary fuzzing unique triaged crashes averaged over 5×24-hour trials.
(b) nconvert
(c) pngout
(d) unrar
Binary
idat64
nconvert
nvdisasm
pngout
unrar
Mean Rel. Increase
Mean MWU Score

ZAFL vs. AFL-Dyninst
rel.
rel.
queue
crash
2.332
1.000
48.140
3.538
1.111
1.484
1.380
1.476
6.112
ZAFL vs. AFL-QEMU
rel.
rel.
queue
crash
1.192
1.303
1.252
1.023
1.249
+55% +16% +326% +38% +52% +20%
0.036
0.045
rel.
total
1.657
1.910
0.578
3.419
1.284
rel.
total
0.789
0.708
0.757
5.842
0.838
0.021
0.041

1.095
1.111
1.476
2.000
0.082
0.009
Table 6: ZAFL’s closed-source binary mean triaged crashes and total/queued
test cases relative to AFL-Dyninst and AFL-QEMU. We report geometric
means for all metrics and MWU test p-values (p ≤ 0.05 indicates signiﬁ-
cance).  = ZAFL ﬁnds crashes while competitor ﬁnds zero.
testing with its graph-pruning off shows it leaves over 50% of
basic blocks uninstrumented for all but pngout. We conclude
that ZAFL’s support for complex, stripped binaries brings a
decisive advantage over existing tools like AFL-Dyninst.
ZAFL versus AFL-QEMU: ZAFL’s speed and transfor-
mations enable it to average 38% more triaged crashes and
52% more test cases than AFL-QEMU. While ZAFL offers
a statistically signiﬁcant improvement in throughput for four
benchmarks (mean MWU p-value of 0.021), we posit that its
slower speed on nvdisasm is due to AFL prioritizing slower
paths: AFL’s logs show ZAFL’s initial speed is over 2×AFL-
QEMU’s (2500 execs/s vs. 1200), but it ﬂuctuates around 5
execs/s for much of the campaign afterwards. Though the
crash-ﬁnding gap between ZAFL and AFL-QEMU is not over-
whelming, ZAFL successfully uncovers a heap overread crash
in idat64—while AFL-QEMU ﬁnds nothing.
7.4.3 Bug-ﬁnding Case Study
Following additional manual triage with binary-level memory
error checkers (e.g., QASan [30] and Dr. Memory [16]), we
compare the time-to-discovery (TTD) for ﬁve closed-source
binary bugs found by ZAFL, AFL-Dyninst, or AFL-QEMU:
a heap overﬂow in nconvert, a stack overﬂow in unrar, a
heap use-after-free and heap overﬂow in pngout, and a heap
overread in idat64’s libida64.so.
Table 7 reports the geometric mean TTD among all ﬁve
bugs for all three instrumenters. We observe that, on aver-
age, ZAFL ﬁnds these bugs 660% faster than AFL-Dyninst,
and 113% faster than AFL-QEMU. Thus, ZAFL’s balance
of compiler-quality transformation and performance lends a
valuable asset to bug-ﬁnding in closed-source code.
Location
nconvert
Error Type
heap overﬂow
stack overﬂow
heap overﬂow
use-after-free
heap overread
ZAFL Mean Rel. Decrease
unrar
pngout
pngout
libida64.so
AFL-Dyninst
AFL-QEMU


12.6 hrs
9.35 hrs
23.7 hrs
-660%
18.3 hrs
12.3 hrs
6.26 hrs
4.67 hrs

-113%
ZAFL