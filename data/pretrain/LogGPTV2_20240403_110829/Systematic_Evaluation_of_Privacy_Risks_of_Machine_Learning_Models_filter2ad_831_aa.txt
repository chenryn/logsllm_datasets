title:Systematic Evaluation of Privacy Risks of Machine Learning Models
author:Liwei Song and
Prateek Mittal
Systematic Evaluation of Privacy Risks of 
Machine Learning Models
Liwei Song and Prateek Mittal, Princeton University
https://www.usenix.org/conference/usenixsecurity21/presentation/song
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Systematic Evaluation of Privacy Risks of Machine Learning Models
Liwei Song
PI:EMAIL
Princeton University
Prateek Mittal
PI:EMAIL
Princeton University
Abstract
Machine learning models are prone to memorizing sensitive
data, making them vulnerable to membership inference at-
tacks in which an adversary aims to guess if an input sample
was used to train the model. In this paper, we show that prior
work on membership inference attacks may severely underes-
timate the privacy risks by relying solely on training custom
neural network classiﬁers to perform attacks and focusing
only on the aggregate results over data samples, such as the at-
tack accuracy. To overcome these limitations, we ﬁrst propose
to benchmark membership inference privacy risks by improv-
ing existing non-neural network based inference attacks and
proposing a new inference attack method based on a modiﬁca-
tion of prediction entropy. We propose to supplement existing
neural network based attacks with our proposed benchmark
attacks to effectively measure the privacy risks. We also pro-
pose benchmarks for defense mechanisms by accounting for
adaptive adversaries with knowledge of the defense and also
accounting for the trade-off between model accuracy and pri-
vacy risks. Using our benchmark attacks, we demonstrate that
existing defense approaches against membership inference
attacks are not as effective as previously reported.
Next, we introduce a new approach for ﬁne-grained privacy
analysis by formulating and deriving a new metric called the
privacy risk score. Our privacy risk score metric measures an
individual sample’s likelihood of being a training member,
which allows an adversary to identify samples with high pri-
vacy risks and perform membership inference attacks with
high conﬁdence. We propose to combine both existing aggre-
gate privacy analysis and our proposed ﬁne-grained privacy
analysis for systematically measuring privacy risks. We exper-
imentally validate the effectiveness of the privacy risk score
metric and demonstrate that the distribution of privacy risk
scores across individual samples is heterogeneous. Finally, we
perform an in-depth investigation to understand why certain
samples have high privacy risk scores, including correlations
with model properties such as model sensitivity, generaliza-
tion error, and feature embeddings. Our work emphasizes
the importance of a systematic and rigorous evaluation of
privacy risks of machine learning models. We publicly re-
lease our code at https://github.com/inspire-group/
membership-inference-evaluation and our evaluation
mechanisms have also been integrated in Google’s Tensor-
Flow Privacy library.
1
Introduction
A recent thread of research has shown that machine learning
(ML) models memorize sensitive information of training data,
indicating serious privacy risks [4,11,12,17,37,41,43]. In this
paper, we focus on the membership inference attack, where
the adversary aims to guess whether an input sample was used
to train the target machine learning model or not [41, 48]. It
poses a severe privacy risk as the membership can reveal an
individual’s sensitive information [3,35]. For example, partici-
pation in a hospital’s health analytic training set means that an
individual was once a patient in that hospital. As membership
inference attacks expose the privacy risks of an individual
user participating in the training data, they serve as a valuable
tool to quantify the privacy provided by differential privacy
implementations [19] and to help to guide the selection of
privacy parameters in the broader class of statistical privacy
frameworks [25]. Shokri et al. [41] conducted membership
inference attacks against machine learning classiﬁers in the
black-box manner, where the adversary only observes predic-
tion outputs of the target model. They formalize the attack as
a classiﬁcation problem and train dedicated neural network
(NN) classiﬁers to distinguish between training members and
non-members. The research community has since extended
the idea of membership inference attacks to generative mod-
els [7, 13, 16, 46], to differentially private models [19, 36], to
decentralized settings where the models are trained across
multiple users without sharing their data [30,32], and to white-
box settings where the adversary also has the access to the
target model’s architecture and weights [32].
To mitigate such privacy risks, several defenses against
membership inference attacks have been proposed. Nasr et
al. [31] propose to include membership inference attacks
USENIX Association
30th USENIX Security Symposium    2615
during the training process: they train the target model to
simultaneously achieve correct predictions and low member-
ship inference attack accuracy by adding the inference attack
as an adversarial regularization term. Jia et al. [20] propose
a defense method called MemGuard which does not require
retraining the model: the model prediction outputs are obfus-
cated with noisy perturbations such that the adversary cannot
distinguish between members and non-members based on
the perturbed outputs. Both papers show that their defenses
greatly mitigate membership inference privacy risks, resulting
in attack performance that is close to random guessing.
In this paper, we critically examine how previous work [20,
31,32,38,41] has evaluated the membership inference privacy
risks of machine learning models, and demonstrate two key
limitations that lead to a severe underestimation of privacy
risks. First, many prior papers, particularly those proposing
defense methods [20, 31], solely rely on training custom NN
classiﬁers to perform membership inference attacks. These
NN attack classiﬁers may underestimate privacy risks due to
inappropriate settings of hyperparameters such as number of
hidden layers and learning rate. Second, existing evaluations
only focus on aggregate notions of privacy risks faced by all
data samples, lacking a ﬁne-grained understanding of privacy
risks faced by individual samples.
Table 1: Benchmarking the effectiveness of existing defenses
[20, 31] against membership inference attacks. Both Nasr et
al. [31] and Jia et al. [20] report that for their defended models,
custom NN classiﬁers achieve attack accuracy close to 50%,
which is the accuracy of random guessing. By using a suite
of non-NN based attacks as our benchmark, we ﬁnd that the
attack accuracy is signiﬁcantly larger than previous estimates,
ranging from an increase of 7.6% to 23.9%.
defense methods
dataset
reported
attack acc
our benchmark
attack acc
adversarial
regularization [31]
MemGuard [20]
Purchase100
Texas100
Location30
Texas100
51.6%
51.0%
50.1%
50.3%
59.5%
58.6%
69.1%
74.2%
To overcome the limitation of reliance on NN-based at-
tacks, we propose to use a suite of alternative existing and
novel non-NN based attack methods to benchmark the mem-
bership inference privacy risks. These benchmark attack meth-
ods make inference decisions based on computing custom
metrics on the predictions of the target model. Compared
to NN-based attacks, our proposed benchmark attacks are
easy to implement without hyperparameter tuning. We only
need to set the threshold values using the shadow-training
technique [41]. We also show that rigorously benchmarking
defense mechanisms requires a careful consideration of strate-
gic adversaries aware of the defense mechanism, as well as
alternative baselines that trade-off accuracy of the target ma-
chine learning model with privacy risks. With our proposed
benchmark attacks, we indeed ﬁnd that that existing member-
ship inference defense methods [20, 31] are not as effective
as previously reported. As shown in Table 1, the adversary
can still perform membership inference attacks on models de-
fended by adversarial regularization [31] and MemGuard [20]
with an accuracy ranging from 58.6% to 74.2%, instead of
the reported accuracy around 50%, which is the accuracy of
random guessing. Therefore, we argue that these non-NN
based attacks should supplement existing NN based attacks
to effectively measure the privacy risks.
Figure 1: Cumulative distribution of privacy risk scores for
undefended models trained on Purchase100, Location30, and
CIFAR100 datasets.
To overcome the limitation of a lack of understanding of
ﬁne-grained privacy risks in existing works, we propose a
new metric called the privacy risk score, that represents an in-
dividual sample’s probability of being a member in the target
model’s training set. Figure 1 shows the cumulative distri-
butions of privacy risk scores on target undefended models
trained on Purchase100, Location30, and CIFAR100 datasets
respectively. We can see that the privacy risk faced by indi-
vidual training samples is heterogeneous. By utilizing the
privacy risk score, an adversary can perform membership
inference attacks with high conﬁdence: an input sample is
inferred as a member if and only if its privacy risk score is
higher than a certain threshold value. Overall, we recommend
that our per-sample privacy risk analysis should be used in
conjunction with existing aggregate privacy analysis for an
in-depth understanding of privacy risks of machine learning
models. Conventional aggregate analysis provides an average
perspective of privacy risks incurred by all samples, while pri-
vacy risk score provides a perspective on privacy risk from the
viewpoint of an individual sample. The former provides an ag-
gregate estimation of privacy risks, while the latter allows us
to understand the heterogeneous distribution of privacy risks
faced by individual samples and identify samples with high
privacy risks. We summarize our contributions as follows:
2616    30th USENIX Security Symposium
USENIX Association
1. We propose a suite of non-NN based attacks to bench-
mark target models’ privacy risks by improving existing
attacks with class-speciﬁc threshold settings and design-
ing a new inference attack based on a modiﬁed predic-
tion entropy estimation in a manner that incorporates
the ground truth class label. Furthermore, to rigorously
evaluate the performance of membership inference de-
fenses, we make recommendations for comparison with
early stopping baseline and considering adaptive attack-
ers with knowledge of defense mechanisms.
2. With our benchmark attacks, we ﬁnd that two state-of-
the-art defense approaches [20, 31] are not as effective
as previously reported. Furthermore, we observe that the
defense performance of adversarial regularization [31] is
no better than early stopping, and the evaluation of Mem-
Guard [20] lacks a consideration of adaptive adversaries.
We also ﬁnd that the existing white-box attacks [32] have
limited advantages over our benchmark attacks, which
only need black-box access to the target model. We also
show that our attacks with class-speciﬁc threshold set-
tings strictly outperform attacks with class-independent
thresholds, and our new inference attack based on modi-
ﬁed prediction entropy strictly outperforms conventional
prediction entropy based attack.
3. We propose to analyze privacy risks of machine learning
models in a ﬁne-grained manner by focusing on individ-
ual samples. We deﬁne a new metric called the privacy
risk score, that estimates an individual sample’s proba-
bility of being in the target model’s training set.
4. We experimentally demonstrate the effectiveness of our
new metric in being able to capture the likelihood of
an individual sample being a training member. We also
show how an adversary can exploit our metric to launch
membership inference attacks on individual samples
with high conﬁdence. Finally we perform an in-depth
investigation of our privacy risk score metric, and its
correlations with model sensitivity, generalization error,
and feature embeddings.
Our code is publicly available at https://github.com/
inspire-group/membership-inference-evaluation
for the purpose of reproducible research. Furthermore, our
evaluation mechanisms have also been integrated in Google’s
TensorFlow Privacy library.
2 Background and Related Work
In this section, we ﬁrst brieﬂy introduce machine learning
basics and notations. Next, we present existing membership
inference attacks, including black-box attacks and white-box
attacks. Finally, we discuss two state-of-the-art defense meth-
ods: adversarial regularization [31] and MemGuard [20].
2.1 Machine learning basics and notations
Let Fθ : Rd → Rk be a machine learning model with d input
features and k output classes, parameterized by weights θ.
For an example z = (x,y) with the input feature x and the
ground truth label y, the model outputs a prediction vector
over all class labels Fθ(x) with ∑k−1
i=0 Fθ(x)i = 1, and the ﬁnal
classiﬁcation result will be the label with the largest prediction
probability ˆy = argmaxi Fθ(x)i.
Given a training set Dtr, the model weights are optimized
by minimizing the prediction loss over all training examples.
min
θ
1
|Dtr| ∑
z∈Dtr
(cid:96)(Fθ,z),
(1)
where |Dtr| denotes the size of training set, and (cid:96) computes
the prediction loss, such as cross-entropy loss. In this paper,
we skip the model parameter θ for simplicity and use F to
denote the machine learning model.
2.2 Membership inference attacks
For a target machine learning model, membership inference
attacks aim to determine whether a given data point was used
to train the model or not [26, 38, 41, 48]. The attack poses a
serious privacy risk to the individuals whose data is used for
model training, for example in the setting of health analytics.
2.2.1 Black-box membership inference attacks
Shokri et al. [41] investigated the membership inference at-
tacks against machine learning models in the black-box set-
ting. For an input sample z = (x,y) to the target model F,
the adversary only observes the prediction output F(x) and
infers if z belongs to the model’s training set Dtr. To distin-
guish between target model’s predictions on members and
non-members, the adversary learns an attack model using the
shadow training technique: (1) the adversary ﬁrst trains mul-
tiple shadow models to simulate the behavior of the target
model; (2) based on shadow models’ outputs on their own
training and test examples, the adversary obtains a labeled
(member vs non-member) dataset, and (3) ﬁnally trains multi-
ple neural network (NN) classiﬁers, one for each class label,
to perform inference attacks against the target model.
Salem et al. [38] show that even with only a single shadow
model, membership inference attacks are still quite successful.
Furthermore, in the case where the adversary knows a subset
of target model’s training set and test set, the attack classiﬁer
can be directly trained with target model’s predictions on
those known samples, and then tested on unknown training
and test sample [31, 32]. Nasr et al. [31] redesign the attack
by using one-hot encoded class labels as part of input features
and training a single NN attack classiﬁer for all class labels.
Besides membership inference attacks that rely on training
NN classiﬁers, there are non-NN based attack methods that
USENIX Association
30th USENIX Security Symposium    2617
make inference decisions based on computing custom metrics
on the predictions of the target model. Leino et al. [24] suggest
using the metric of prediction correctness as a sign of being