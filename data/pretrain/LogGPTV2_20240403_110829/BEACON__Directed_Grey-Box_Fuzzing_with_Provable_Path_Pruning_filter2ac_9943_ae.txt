3.01h
1.38h
7.69M 95.9%
2,229s
2.00h
7.74h
0.84h
2,258s
3.76M 94.9%
1.47h
5.88h
1.66h
5,381s
0.33M 95.1%
0.17h
2.07h
18.01h 16.46M 83.6%
5,238s 16.55h
T.O.
19.96h 18.47M 85.1%
7,049s 18.00h
T.O.
33.14h 40.15M 95.2%
5,672s 31.56h
T.O.
9.16h
8.42h
1.97M 79.3%
T.O. 2,654s
2,909s 39.03h
39.84h 95.85M 91.2%
T.O.
84.31h 261.2M 96.1%
3,067s 83.46h
T.O.
41.29h 166.3M 96.0%
2,825s 40.51h
T.O.
62.33h 41.13M 86.1%
3,420s 61.38h
T.O.
10.59h
3,326s 22.24h
4.32M 94.3%
T.O.
2,950s 45.69h
46.51h 121.55M 92.6%
T.O.
20.09h 20.65h 31.75M 85.7%
T.O. 2,033s
20.47h 20.21M 90.5%
T.O.
4.74M 92.3%
5.20h
T.O.
4.55h
4.31M 96.9%
T.O.
11.46h 20.12M 91.2%
49.78h
4.01h 19.78M 90.4%
7.34h
3.23h 65.81M 91.3%
T.O.
2.98M 76.5%
T.O. 4,904s 102.90h 104.26h
5,899s 90.25h
3.76M 69.4%
91.89h
T.O.
96.50h
4,153s 95.35h
T.O.
3.13M 67.2%
64.37h 12.94M 75.2%
T.O.
6,593s 62.54h
74.80h 14.60M 86.3%
T.O. 2,950s 73.98h
83.21h 14.21M 81.1%
T.O.
2,686s 82.46h
T.O.
2,995s 92.65h
93.48h 13.76M 70.1%
6.00M 53.4%
T.O. 14,335s 89.57h
93.55h
7.65M 69.8%
T.O. 14,893s 98.20h 102.34h
3.44M 56.7%
52.03h
T.O. 14,623s 47.97h
57.47h
T.O. 16,600s 52.86h
6.89M 51.3%
99.77h 13.49M 52.6%
T.O. 14,239s 95.81h
11.50x
82.9%
2,484s 19.78h
4.46h
2,671s
3.83h
2,608s
93s 11.46h
3.98h
106s
85s
3.22h
ming4.8
ming4.7
2016-9827
1
2016-9829
2
2016-9831
3
2017-7578
4
2017-9988
5
2017-11728
6
2017-11729
7
2018-7868
8
2018-8807
9
2018-8962
10
2018-11095
11
2018-11225
12
2018-11226
13
2018-20427
14
2019-9114
15
2019-12982
16
2020-6628
17
2017-8846
18
2018-11496
19
2016-4491
20
2016-6131
21
2017-5969
22
2017-9047
23
2017-9048
24
2017-9049
25
2017-8392
26
2017-8396
27
2017-8397
28
2017-8398
29
2017-14940
30
2017-16828
31
2018-17360
32
2017-7303
33
2017-8393
34
2017-8394
35
2017-8395
36
2018-14498
37
38
2020-13790
39 pngimage 2018-13785
2019-10872
40
2019-10873
41
42
2019-14494
43 pdftops 2019-10871
2018-19058
44
2018-19059
45
46
2018-19060
2018-11102
47
2018-11224
48
2018-18829
49
2019-14441
50
51
2019-14443
Avg.
T.O.: time outs (>120 hours)
pdfdetach
pdftoppm
objdump
objcopy
avconv
lrzip
cxxﬁlt
xmllint
cjpeg
A. Compared to the State of the Art
To compare BEACON to the (directed) fuzzers in Table I,
we ran them to reproduce the CVE-identiﬁed vulnerabilities
listed in Table III.
1) Compared to AFLGo: Table III lists the comparison re-
sults, where we show the time cost of AFLGo and BEACON for
reproducing the vulnerabilities, as well as the time cost of the
static analysis employed by BEACON. We observe that AFLGo
fails to reproduce 34 out of the 51 vulnerabilities in 120 hours,
while BEACON can succeed in reproducing all of them, 23
within 5 hours, 33 in 24 hours (including the time of static
analysis). Overall, BEACON achieved 1.2x to 68.5x speedup,
11.50x speedup on average, compared to AFLGo. Table III
45
Table IV: Comparing to AFLGo and Hawkeye of average
results (s) from 10 repeated experiments. Since Hawkeye is
not open source, we use the data reported in its paper. The
p-value is the comparison between AFLGo and BEACON.
CVE
2016-4487/8
2016-4489
2016-4490
2016-4491
2016-4492
2016-6131
AFLGo
412 (x2.7)
567 (x3.1)
306 (x3.7)
27,880 (x5.6)
540 (x1.7)
21,180 (x7.3)
Hawkeye
177 (x1.1)
206 (x1.1)
103 (x1.2)
18,733 (x3.6)
477 (x1.5)
17,314 (x5.7)
Beacon
151
180
82
4985
325
3013
p-value
0.00906
0.00804
0.01596
0.00018
0.00804
0.00018
Figure 8: Coverage comparison between AFLGo and BEA-
CON. The x-axis is the CVEs listed in Table III. The y-axis is
the relative coverage compared with AFLGo.
also lists the number of program executions for reproducing a
vulnerability, as well as the ratio of the executions that can be
stopped early by BEACON. We can observe that in most cases,
more than 80% of the executions can be stopped early during
fuzz testing, which allows BEACON to save much time.
We observe that the time spent for the precondition analysis
in BEACON is at most 5 hours, and, in many cases, it can
complete in only a few minutes. As discussed above, even
with the static analysis time, BEACON is still much faster than
AFLGo. In practice, we can further speed up the static analysis
by leveraging other techniques, e.g.,
incremental analysis,
proposed by the static analysis community. However, as this is
out of the scope of this paper, we leave it as our future work.
In addition to the experiments of vulnerability reproduction,
we also ran BEACON and AFLGo to test the patches of the
CVE-identiﬁed vulnerabilities. Surprisingly, BEACON detected
3, 9, and 2 incomplete patches in Binutils, Ming, and Lrzip,
respectively, and 8 additional bugs, whereas AFLGo only
detected 6 incomplete patches. We have reported the detected
issues to the developers, and all of them have been conﬁrmed.
All links to the bug reports are available through this link.
Moreover, we evaluated the coverage achieved when the
vulnerabilities are reproduced. The results are shown in Fig-
ure 8. Interestingly, we ﬁnd the coverage achieved by BEACON
and AFLGo ﬂuctuates. On the one hand, BEACON needs less
coverage (91.2% on average) for those vulnerabilities repro-
ducible for AFLGo. On the other hand, BEACON could achieve
higher coverage in some scenarios, especially when AFLGo
cannot reproduce the vulnerabilities. We ﬁnd the reason is that
AFLGo spends too much time executing infeasible paths, and
thus the execution ﬁltration rates are high (ą 80%) as shown
in Table III.
2) Compared to Hawkeye: We also tried to compare with
Hawkeye, the other recent directed grey-box fuzzer, that was
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:18 UTC from IEEE Xplore.  Restrictions apply. 
Figure 10: Comparison of BEACON and BEACON*. The x-
axes are the CVE-identiﬁed vulnerabilities listed in Table III.
The y-axes are the reproduction time and the ratio of paths that
are stopped early, respectively. We used 120h as the maximum
timeout budget.
B. Impacts of Path Slicing & Precondition Checking
Recall that, in addition to the precondition-based path prun-
ing, BEACON also leverages conventional reachability analysis
on the control ﬂow graph to slice away paths that simply
cannot reach the target code. To evaluate how path slicing
and precondition checking contribute to the time reduction
in BEACON, we also set up a naive variant of BEACON,
BEACON˚, which disables the precondition analysis. We then
reran the experiments discussed before using BEACON and
BEACON˚. The experimental results are shown in Figure 10,
where we can observe that BEACON is much faster (1.1x to
18.4x) than BEACON˚ for reproducing the vulnerabilities, as it