For Bluetooth and USB AT interfaces exposed by modern smart-
phones, we define the following two different threat models.
3.1.1 Threat model for Bluetooth. For the threat model over Blue-
tooth, we assume a malicious/compromised Bluetooth peripheral
device (e.g. headphones, speaker) is paired to an Android device.
We assume the malicious Bluetooth device is connected through
its default profile. For instance, the victim smartphone which is
connected to the malicious headphone has only given audio permis-
sions to the headphone. Also, it can be the case when the adversary
sets up a fake peripheral device through the man-in-the-middle
(MitM) attacks exploiting known vulnerabilities of Bluetooth pair-
ing and bonding [7, 39, 51] procedures. We, however, do not assume
the presence of malicious apps on the device.
3.1.2 Threat model for USB. For USB, we assume a malicious USB
host, such as a PC or USB charging station controlled by an ad-
versary, that tries to attack the connected Android phone via USB.
We assume the attacker can get access to the exposed AT interface
even if the device is inactive. We also neither require a malicious
app to be installed on the device nor the device’s USB debugging to
be turned on.
3.2 Problem Statement
Let I be the set of finite strings over printable ASCII characters,
R = {ok, error} be the set of parsing statuses, and A be a set of
actions (e.g., phone-call ∈ A). The AT interface of a smartphone
can be viewed as a function P from I to R × 2(A∪{nop,⊥}), that is,
P : I → R × 2(A∪{nop,⊥}) in which “nop” refers to no operation
whereas ⊥ captures undefined behavior including a crash. nop
is used to capture the behavior of P ignoring an AT command,
possibly, due to blacklisting or parsing errors.
Given the smartphone AT interface under test PTest and a refer-
ence AT interface induced by the standard PRef, we aim to identify
concrete vulnerable AT command instances s ∈ I such that PTest
and PRef do not agree on their response for s, that is, PRef(s) (cid:44)
PTest(s). Given pairs ⟨r1, a1⟩, ⟨r2, a2⟩ ∈ R × 2(A∪{nop,⊥}), we write
⟨r1, a1⟩ = ⟨r2, a2⟩ if and only if r1 = r2 and a1 = a2. Note that, a1
and a2 are both sets of actions as one command can mistakenly
trigger multiple actions.
Note that, there can be a reason PRef and PTest can legitimately
disagree on a specific input AT command s ∈ I as s can be black-
listed by PTest. Due to CVE-2016-4030 [2], CVE-2016-4031 [3], and
CVE-2016-4032 [4], Samsung has locked down the exposed AT
interface over USB with a command whitelist for some phones.
In this case, we do not consider s to be a vulnerable input in-
stance. Precisely, when s is a blacklisted command, we observed
that PTest often returns ⟨ok, nop⟩. Finally, we instantiate the oracle
PRef through manual inspection of the standard.
3.3 Running Example
To explain ATFuzzer’s approach, we now provide a partial, example
context-free grammar (CFG) of a small set of AT commands (see
Figure 3 for the grammar and Figure 2 for the partial Abstract Syn-
tax Tree (AST) of the grammar) which we adopted from the original
3GPP suggested grammar [11, 52]. In our presentation, we use the
bold-faced font to represent non-terminals and regular-faced font
to identify terminals. We use “·” to represent explicit concatenation,
especially, to make the separation of terminals and non-terminals in
a production rule clear. We use [. . . ] to define regular expressions
in grammar production rules and [. . . ]∗ to represent the Kleene star
operation on a regular expression denoted by [. . . ]. In our example,
Dnum can take any alphanumeric string up to length n as an argu-
ment. Our production rules are of the form: s → α · B1{ϕ} where
s is a non-terminal, α denotes a (possibly, empty) sequence of ter-
minals, B1 represents a possibly empty sequence of non-terminals
and terminals, and ϕ represents a condition that imposes additional
well-formedness restrictions on the production.
In the above example, we show the correct AT command format
for making a phone call. Examples of valid inputs generated from
this grammar can be— ATD ∗ 752# + 436644101453;
3.4 Overview of ATFuzzer
In this section, we first touch on the technical challenges that
ATFuzzer faces and how we address them. We conclude by pro-
viding the high-level operational view of ATFuzzer.
3.4.1 Challenges. For effectively navigating the input search space
and finding vulnerable AT commands, ATFuzzer has to address the
following four technical challenges.
C1: (Problematic input representation). The first challenge is to ef-
ficiently encode the pattern of problematic inputs. It is crucial as
the problematic AT commands that have similar formats/structures
but are not identical may trigger the same behavior. For instance,
both ATD123 and ATD1111111111 test inputs are problematic (nei-
ther of them is a compliant AT command due to missing a trailing
semicolon) and have a similar structure (i.e., ATD followed by a
phone number), but are not the same concrete test inputs. While
processing these problematic AT commands, one of our test devices,
however, stops cellular connectivity. Mutation in the concrete input
level will require the fuzzer to try a lot of inputs of the same vulner-
able structure before shying away from that abstract input space.
This may limit the fuzzer from testing diverse classes of inputs.
C2: (Syntactic correctness). As shown in Figure 3, most of the AT
commands have a specific number and type of arguments. For in-
stance, +CFUN= has two arguments: CFUNarg1 and CFUNarg2.
The second challenge is to effectively test this structural behavior
and argument types, thoroughly by generating diverse inputs that
do not comply with the command structure or the argument types.
C3: (Semantic correctness). Each argument of an AT command may
have associated conditions. For instance, Lenдth(Dnum) ≤ n in
the fifth production rule of Figure 3. Also, arguments may correlate
with each other, such as, one argument defines a type on which an-
other argument depends. For instance, +CTFR= refers to a service
that causes an incoming alert call to be forwarded to a specified
number. It takes four arguments— the first two are number and
type, respectively. Interestingly, the second argument defines the
532Figure 2: Partial Abstract Syntax Tree(AST) of the reference grammar (Grey-box denotes non-terminal symbols and white box indicates ter-
minal symbols)
command → AT · cmd
cmd → dgrammar| cfungrammar
cmd → ϵ | cmd_AT
cmd_AT → cmd ; cmd
dgrammar → D · Dnum · Darg ;
cfungrammar → +CFUN? | +CFUN =CFUNarg1, CFUNarg2
cmd → +CTFR = number, type, subaddr, satype
Dnum → [a − zA − Z 0 − 9 + ∗#]∗ {Lenдth(Dnum) ≤ n}
Darg → I | G | ϵ
CFUNarg1 → [0 − 9]∗ {CFUNarg1 ∈ Z and 0 ≤ CFUNarg1 ≤ 127}
CFUNarg2 → [0 − 1] {CFUNarg2 ∈ Z and 0 ≤ CFUNarg2 ≤ 1}
number1 → number1 | number2
number2 → [a − zA − Z 0 − 9 + ∗#]∗ {if type = 145}
number → [a − zA − Z 0 − 9 ∗ #]∗ {if type = 129}
type → 145|129
subaddr → [a − zA − Z 0 − 9 + ∗#]∗
satype → [0 − 9]∗{if satype = ϵ, satype = 128}
.
.
.
Figure 3: Partial reference context-free grammar for AT commands.
format of the number given as the first argument. If the dialing
string includes access code character “+”, then the type should be
145, otherwise, it should be 129. These correlations are prevalent
in many AT commands. Hence, the third challenge is to systemati-
cally test conditions associated with the arguments of commands
to cover both syntactical and semantic bugs.
C4: (Feedback of a test input). The AT interface can be viewed as
a black-box providing only limited output of the form: OK (i.e.,
correctly parsed) or ERROR (i.e., parsing error). The final challenge
is to devise a mechanism that can provide information about the
code-coverage of the AT interface for the injected test AT command
and thus effectively guiding us through the fuzzing process.
Insights on addressing challenges. For addressing C1, we use
3.4.2
the grammar itself as the seed of our evolutionary fuzzing frame-
work rather than using a particular instance (i.e., a concrete test
input) of the grammar. This is highly effective as the mutation of a
production rule can influence the fuzzer to test a diverse set of in-
puts. Also, when a problematic grammar is identified, it can serve as
abstract evidence of the underlying flaw in the AT interface. Finally,
as grammar can be viewed as a symbolic representation of concrete
input AT commands, mutating a grammar can enable the fuzzer
to cover large diverse classes of AT commands. The insight here
is that testing diverse input classes are likely to uncover diverse
types of issues.
To address challenges C2 and C3, at each iteration, ATFuzzer
chooses parents with the highest fitness scores and switches parts
of the grammar production rules among each other. This causes
changes to not only the structural and type information in the child
grammars but also forms two very different grammars that try to
break the correlation of the arguments. For instance, suppose that
the ATFuzzer has selected following two production rules from two
different parent grammars: +CFUN = CFUNarg1, CFUNarg2
and +CTFR = number, type, subaddr, satype. After applying
our proposed grammar crossover mechanisms, the resultant child
grammar production rules are: +CFUN = CFUNarg1, type, subad-
dr, satype and +CTFR = number CFUNarg2. The production
rule +CFUN takes only two arguments whereas our new child
grammar creates a production rule that has four arguments. The
same reasoning also applies for +CTFR. Thus, the new grammars
with modified production rules would test this structural behav-
ior precisely. Furthermore, +CTFR’s first argument number is
correlated with its second argument type. In the modified child
grammars, type, however, has been replaced with CFUNarg2. Re-
call from our grammar definition, type takes argument from the
set {145, 129} whereas +CFUNarg2 takes argument from the set
{0, 1}. Therefore, this single operation completes two tasks at once—
it not only tests the correlation among two arguments of +CTFR
but also tests conditions of both +CFUN and +CTFR. Crossing
over grammar production rules creates a drastic modification in the
input format and it aims to explore the diverse portions of the input
space to create highly unusual inputs. To test both the structural
aspects, we use three very different mutation strategies which cre-
ate little change to the grammar (compared to crossover) but prove
highly effective for checking the robustness of the AT interface.
For addressing C4, we use the precise timing information of in-
jecting an AT command and receiving its output. We keep an upper
bound on this time, i.e., a timer (T ). If the output is not received
within T , we suspect the AT interface has become unresponsive
possibly due to the blacklisting mechanism enforced by several
vendors. We use this timing information as a loose-indicator for
the code-coverage information. Our intuition is to explore as much
of the AT interface as possible. A high execution time loosely in-
dicates that the test command traverses more basic-blocks than
the other inputs with lower execution time. We try to leverage
this simple positive correlation to design a feedback edge (i.e., a
fitness function) of the closed-loop. The timing information, how-
ever, cannot help to infer how many new basic-blocks a test input
could explore. Since our focus is mainly on baseband related AT
commands, an error in the AT interface has a higher probability
commandATcmddgrammarcmd_AT+CFUN?cfungrammarDnumDDargCFUNarg1+CFUN	=CFUNarg2;cmdcmd[a-zA-Z0-9+*#]*GI[0-9]*[0-1]............... ...533Figure 4: Overview of ATFuzzer framework
of causing disruptions in the baseband which also trickles down
to cellular connectivity. We leverage this key insight and consider
both the cellular Internet connectivity information from the target
device and the device’s debug information (Logcat) as an indication
of the baseband health after running an AT command. Using this
information, we devise our fitness function for guiding ATFuzzer.
3.4.3 High-level description of ATFuzzer. ATFuzzer comprises of
two modules, namely, evolution module and evaluation module, in-
teracting in a closed-loop (see Figure 4). The evolution module is
bootstrapped with a seed AT command grammar which is mutated
to generate Psize (refers to population size; a parameter to ATFuzzer)
different versions of that grammar. Concretely, new grammars are
generated from parent grammar(s) by ATFuzzer through the follow-
ing high-level operations: (1) Population initialization; (2) Parent
selection; (3) Grammar crossover; (4) Grammar mutation. Particu-
larly relevant is the operation of parent selection in which ATFuzzer
uses a fitness function to select higher-ranked (parent) grammars
for which to apply the crossover/mutation operations (i.e., steps
3 and 4) to generate new grammars. Choosing the higher-ranked
grammars to apply mutation is particularly relevant for generating
effective grammars in the future.
Evaluating fitness function requires the evaluation module. For
a given grammar д, evaluation module samples several д-compliant
commands to test. It uses the AT command injector (as shown in
Figures 1 and 4) to send these test commands to the device-under-
test. The fitness function uses the individual scores of the concrete
д-compliant instances to assign the overall score to д.
4 DETAILED DESIGN OF ATFUZZER
In this section, we discuss our proposed crossover and mutation
techniques for the evolution module followed by the fitness function
design used by the evaluation module.
4.1 Evolution Module
Given the AT grammar (shown in Figure 3), ATFuzzer’s evolution
module randomly selects at most n cmds to generate the initial
seed AT grammar denoted as GAT. The evolution module yields the
grammars Gbest with the highest scores until a certain stopping
condition is met, such as, total time of testing or the number of
iterations. Algorithm 1 describes the high-level steps of ATFuzzer’s
evolution module.
Initialization. The evolution module starts with initializing
4.1.1
the population P (Line 1 in Algorithm 1) by applying both our
proposed crossover and mutation strategies with three parame-
ters: the population size Psize; the probability Ppop of applying
crossover and mutation on the grammar; the tournament size Tsize.
The key-insight of using Ppop is that it correlates with the number
of syntactic and semantic bugs explored. A higher value of Ppop is,
the diverse the initial population is and vice versa. The diverse the
initial population, the higher the number of test inputs that check
syntactic correctness is and vice versa. Therefore, to explore both
syntactic and semantic bugs, we vary the values of Ppop; aiming
to strike a balance between grammar diversity. To assess the fit-
ness of the initial population P, the evolution module invokes the
evaluation module (Line 3-8) with the generated grammars.
Algorithm 1: ATFuzzer
Data: Psize, Ppop, GAT, Tsize
Result: Gbest: Best Grammar
1 P ← InitializePopulation(Psize, Ppop, GAT);
2 while stopping condition is not met do
3
4
5
6
7
8
9
10
for each grammar Gi ∈ P do
Generate random input I
AssesFitness(Gi, I)
if Fitness(Gi) > Fitness(Gbest) then
Gbest = Gi;
end
end
Q = {}
for Psize
2
times do
Pa ← ParentSelection(P, Tsize)
Pb ← ParentSelection(P, Tsize)
Ca, Cb ← GrammarBasedCrossover(Pa, Pb)
Q = Q ∪ {Mutate(Ca), Mutate(Cb)}
11
12
13
14
15
16
17
18 end
end
P ← Q
4.1.2 Parent selection for the next round. We use the tournament
selection technique to get a diverse population at every round. We
534perform “tournaments” among P grammars (Line 12-13 in Algo-
rithm 1). The winner of each tournament (the one with the highest
fitness score) is then selected for crossover and mutation. In what
follows, we discuss in detail our tournament selection technique
addressing the functional and structural bloating problems of evo-
lutionary fuzzers [54].
Restraining functional bloating. We leverage another insight
in selecting grammars at each round of the tournament selection
procedure to reduce functional bloating [54]— the continuous gen-
eration of grammars containing similar mutated production rules—
which adversely affects diverse input generation in evolutionary
fuzzing. At each round, we randomly select grammars from our
population. This is due to the fact that while running an evolu-
tionary fuzzing, the range of fitness values becomes narrow and
reduces the search space it focuses on. For example, at any round,
if the fuzzer finds a grammar that has a mutated production rule
related to +CFUN causing an error state in the AT interface, then
all the grammars containing this mutated rule will obtain high
fitness values. If we then only select parents based on the highest
fitness, we would inevitably fall into functional bloating and would
narrow down our focused search space with grammars that are
somehow associated with this mutated version of +CFUN only.
To constraint this behavior, we perform the tournament selection
procedure in which we randomly choose Tsize (where T denotes
the set of selected grammars for the tournament and Tsize ≤ Psize)
number of grammars from the population P. The key insight of
choosing randomly is to give chances to the lower fitness grammars
in the next round to ensure a diverse pool of candidates with both
higher and lower fitness scores.
Restraining structural bloating. After running ATFuzzer for a
while, i.e., after a certain number of generations, the average length