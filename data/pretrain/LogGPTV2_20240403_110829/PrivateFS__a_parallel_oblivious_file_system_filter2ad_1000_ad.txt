ertheless, this simpler BF construction process is employed
instead of the k
5.5 Security against malicious adversaries
√
n ln n variant.
Ensuring security against a malicious adversary ﬁrst re-
quires an underlying ORAM providing these guarantees.
Fortunately, the BF-based ORAM of [19] provides such a
mechanism. Second, maintaining security in a parallel set-
ting requires clients to test that they all see a consistent
view. This is achieved using a hash tree over the set of all
previous queries. Whenever a client performs the top level
shuﬄe, it is responsible for updating this hash tree and at-
taching the root value with a MAC to the new query log.
This entails hashing the current query log, appending it as a
new leaf node to the hash tree, and recomputing hash values
along the path from this new node to the top.
Second, whenever a client performs a query, it performs
it veriﬁes that the last query it
one additional operation:
performed is included in the hash tree, whose root corre-
sponds to the value attached to this query log. This is done
by verifying all nodes in the hash tree adjacent to the path
from the root to its last request.
These operations will not detect a forking / split universe
attack, unless out-of-band communication is available, or
clients perform periodic accesses to ensure they are still op-
erating in the same view as other clients. However, this
solution has the property that once the server forks the uni-
verse into multiple views, it cannot rejoin the views without
being detected by the clients (by hash value disagreement).
The PD-ORAM implementation analyzed in the following
sections, however, assumes an honest but curious adversary.
6. EXPERIMENTS AND ANALYSIS
Amortized Measurements. A signiﬁcant challenge in
measuring the performance of any amortized system is en-
suring the trial captures the average performance, not just
peak performance. This is complicated by the requirement
of running trials for periods that are too short to encompass
the full period over which the amortization is performed. For
example, at even several queries per second, the reconstruc-
tion of the lowest level of a terabyte database is amortized
over a period on the order of weeks or longer.
De-amortizing the level construction provides the oppor-
tunity of improving measurement accuracy. The challenge
remains, however, of ensuring the de-amortized background
shuﬄe proceeds proportionally to the query rate: the de-
amortization is perfect when the new level construction is
completed at the instant it is needed by a query. Inaccu-
racies in this rate synchronization will aﬀect the measured
results, since measured query throughput of a short period
might be higher or lower than the sustainable rate.
To avoid this eﬀect, PD-ORAM maintains progress me-
ters for level construction, allowing queries to proceed when
every level is proportionally constructed. The level construc-
tions processes are also suspended when a level gets too far
ahead of the current query. This keeps querying and level
construction smooth, minimizing worst case latency.
Proper de-amortization: Theory vs. Reality. Per-
forming proper de-amortization proved a non-trivial systems
challenge. Research solutions, such as [4], and PD-ORAM
(Section 5.3) express de-amortization in terms such as “per-
form the proportional amount of work required”, or “per-
form the next O(f (x)) accesses.” While these terms suﬃce
for proving existence of a de-amortized construction, pro-
gramming models do not typically provide this type of ab-
stract control. PD-ORAM achieves this control by metering
progress over the construction of individual levels. Since the
level construction involves diﬀerent types of computation
across the client and server, accurate progress metering re-
quired splitting level construction into tasks whose progress
can be reported over time. Moreover, this metering uses ex-
perimentally determined values to identify what portion of
the level construction corresponds to which subtasks.
As an aside, suspending the level construction when it out-
paces the queries proved critical on the larger database sizes.
The sheer number of requests being sent from the ORAM
Instance to the ORAM Server for construction tended to
starve the requests of actual queries (much fewer in num-
ber), causing the query rate to drop quickly as more levels
were introduced. This behavior was corrected by forcing
level construction to remain proportional to query progress:
this keeps the individual query rates much closer to average.
Since it is impractical to repeatedly running trials over the
entire (up to 1TB) measured epoch, the database for these
trials is ﬁrst constructed non-obliviously on the server via a
specially designed module. The items are inserted randomly
so that the ﬁnal result mirrors an oblivious construction (as
would occur from a sequence of write queries).
6.1 Setup
PD-ORAM is written in Java. Clients run on quad-core
3.16GHz Xeon X5460 machines. The server runs on a single
Quad-Core Intel i7-2600K Sandy Bridge 3.4GHz CPU, with
16GB DDR3 1600 SD-RAM and 7x2TB HITACHI Deskstar
7200RPM SATA 3.0Gb/s disks (RAID0 / LVM).
All the machines share a gigabit switch. Network latency
is shaped by forcing server threads to sleep for the desired
round trip duration upon receiving a request. This allows
simulation of link latency without capping link bandwidth.
The implementation uses a BF with 8 hash functions, and
2400 bits of space per item which allows an eﬃcient con-
−64 per lookup.
struction within the false positive rate of 2
The resulting BF constitutes roughly 25% of the total size
of the database records.
Optimization. Rather than optimizing the BF size re-
quired to obtain this error rate by using a larger number
of hashes, as suggested in [13], PD-ORAM uses larger BFs
985)
c
e
s
/
s
e
i
r
e
u
q
(
s
t
n
e
i
l
c
l
l
a
,
t
u
p
h
g
u
o
r
h
t
y
r
e
u
Q
Overall query throughput vs. database size
and num clients, 50ms network latency
10 clients
5 clients
2 clients
1 client
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
 1e+08  1e+09  1e+10  1e+11  1e+12  1e+13
Database size (bytes).
)
c
e
s
/
s
e
i
r
e
u
q
(
s
t
n
e
i
l
c
l
l
a
,
t
u
p
h
g
u
o
r
h
t
y
r
e
u
Q
Overall query throughput vs. net latency
and num clients, 1.3e+10-byte database
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
10 clients
5 clients
2 clients
1 client
 0  10  20  30  40  50  60  70  80
Round trip latency (ms)
s
m
,
y
c
n
e
t
a
l
y
r
e
u
Q
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0
Query latency vs. time on
1.3e+10-byte database, 50 ms latency
instantaneous
running average
 0
 200  400  600  800  1000
Query number
Figure 6: Query throughput vs.
data size for varying number of
clients x-axis is log scale. Each point
is sampled over 3000 queries.
Figure 7: Query throughput vs.
network latency for varying number
of clients. Each point is sampled
over approx. 3000 queries.
Figure 8:
Individual query la-
tencies + running averages of a
single client. With perfect de-
amortization, all queries would re-
quire the same amount of time.
with fewer hashes, to minimize item lookup disk seeks while
obtaining the same error rate.
6.2 Experiments
One main goal of the experiments is to understand the
interaction between network performance parameters and
the parallel nature of PD-ORAM.
Size + clients vs. query throughput. Figure 6 plots
the eﬀect of database size and client parallelization on overall
query throughput. Fresh databases were used for all trials to
prevent dependency of the measurements on the order of the
trials, except for the 1TB trials, where this proved imprac-
tical. Even though individual query latency increases with
higher resource contention, the beneﬁt of parallelization are
obvious: signiﬁcantly higher overall throughputs.
Clients + network latency vs. performance. Fig-
ure 7 plots the eﬀect of parallel clients and network latency
on overall query throughput for a ﬁxed database size. The
premise of this measurement is that parallelization becomes
more important as network latency increases.
De-amortization optimality. Figure 8 plots the ob-
served latency of individual queries vs. time on a growing
database. With perfect de-amortization, all queries would
require the same amount of time. Most queries take around
1200ms; a ﬁxed lower limit is imposed by the network la-
tency. The bands at 2600ms and 3100ms reﬂect the con-
struction of the top level, which is not de-amortized.
Progress metering. To validate the accuracy of the
progress metering, Figure 9 shows the reported construction
progress of a single level as sampled every 5 seconds. Strict
de-amortization and querying is disabled to avoid cool-down
periods when construction has progressed farther than is
needed, and to ensure measurement of its progress only.
6.3 Impact of disk latency
The experiments were repeated (for database sizes up to
300GB) in a diﬀerent setup, in which the server was run on
dual 3.16Ghz Xeon X5460 quad-core CPUs and six 0.4 TB
15K RPM SCSI (hardware RAID0) disks.
This conﬁguration surprisingly outperformed the setup
above by a factor of 2x in most trials. The primary ad-
vantage is the superior seek time on the server disks, so the
markedly diﬀerent results suggest that server disk seek costs
play an important role in overall performance. This was
somewhat surprising, since the level construction mecha-
nisms were designed speciﬁcally to minimize disk seeks (with
the hash table insertion being the only random-access op-
eration during level construction, requiring an average of 2
random writes per insert). The rest of the level construction
simply requires reading from one or two sequential buﬀers,
and writing out sequentially to one or two.
The culprit is most likely the de-amortization process,
which constructs diﬀerent levels in parallel, and in eﬀect
randomizes disk access patterns. While individual level con-
struction is mostly limited by sequential disk throughput,
running many of these processes in parallel across the same
ﬁle system results in disk seeks even in the sequential access
regions, resulting in a much lower overall disk throughput.
SSDs.
Several software and hardware solutions present
themselves. Better data placement would split data in a
more eﬃcient manner across the available disks (instead of
using a RAID conﬁguration), to allow the sequential nature
of each level construction process to transfer to sequential
disk access. Obtaining optimal throughput in this manner
would require a relatively large number of disks. Further,
the use of more expensive (but quickly dropping in cost)
low-latency solid state disks (SSDs) would be a simple hard-
ware solution to eliminate this performance bottleneck. It
remains to be seen however, whether the sustained random
write performance degradation plaguing current SSDs does
not constitute a bigger bottleneck in itself, as preliminary
throughput experiments on several recent 128GB Samsung
SSDs with 2011 ﬁrmware updates seem to suggest.
7. AN OBLIVIOUS FILE SYSTEM
ORAM lends itself naturally to the creation of a block de-
vice. Due to existing results’ impractical performance over-
head this has not been previously possible. A Linux-based
deployment of PD-ORAM is used here to design and build
privatefs, a fully-functional oblivious network ﬁle system in
which ﬁles can be accessed on a remote server with compu-
tational access privacy and data conﬁdentiality.
An initial implementation was built on top of the Linux
Network Block Device (NBD) driver, which is the simplest
and most natural approach, since PD-ORAM already pro-
vides a block interface. However, NBD supports only serial,