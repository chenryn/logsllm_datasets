Certainly! Here is a more polished and coherent version of the provided text:

---

### 5.5 Security against Malicious Adversaries

Ensuring security against a malicious adversary first requires an underlying Oblivious RAM (ORAM) that provides these guarantees. Fortunately, the Bloom Filter (BF)-based ORAM from [19] offers such a mechanism. In a parallel setting, clients must verify that they all see a consistent view. This is achieved using a hash tree over the set of all previous queries.

Whenever a client performs the top-level shuffle, it is responsible for updating this hash tree and attaching the root value with a Message Authentication Code (MAC) to the new query log. This involves hashing the current query log, appending it as a new leaf node to the hash tree, and recomputing hash values along the path from this new node to the root.

Additionally, whenever a client performs a query, it verifies that its last query is included in the hash tree, whose root corresponds to the value attached to the query log. This verification is done by checking all nodes in the hash tree adjacent to the path from the root to the last request.

These operations do not detect a forking or split universe attack unless out-of-band communication is available or clients perform periodic accesses to ensure they are still operating in the same view as other clients. However, once the server forks the universe into multiple views, it cannot rejoin the views without being detected by the clients (due to hash value disagreement).

The PD-ORAM implementation analyzed in the following sections assumes an honest but curious adversary.

### 6. Experiments and Analysis

#### Amortized Measurements

A significant challenge in measuring the performance of any amortized system is ensuring that the trial captures the average performance, not just peak performance. This is complicated by the requirement of running trials for periods that are too short to encompass the full period over which the amortization is performed. For example, even at several queries per second, the reconstruction of the lowest level of a terabyte database is amortized over a period on the order of weeks or longer.

De-amortizing the level construction provides the opportunity to improve measurement accuracy. The challenge remains, however, of ensuring that the de-amortized background shuffle proceeds proportionally to the query rate: the de-amortization is perfect when the new level construction is completed at the instant it is needed by a query. Inaccuracies in this rate synchronization will affect the measured results, as the measured query throughput of a short period might be higher or lower than the sustainable rate.

To avoid this effect, PD-ORAM maintains progress meters for level construction, allowing queries to proceed when every level is proportionally constructed. The level construction processes are also suspended when a level gets too far ahead of the current query. This keeps querying and level construction smooth, minimizing worst-case latency.

#### Proper De-Amortization: Theory vs. Reality

Performing proper de-amortization proved to be a non-trivial systems challenge. Research solutions, such as [4], and PD-ORAM (Section 5.3) express de-amortization in terms such as "perform the proportional amount of work required" or "perform the next O(f(x)) accesses." While these terms suffice for proving the existence of a de-amortized construction, programming models do not typically provide this type of abstract control. PD-ORAM achieves this control by metering progress over the construction of individual levels. Since level construction involves different types of computation across the client and server, accurate progress metering required splitting level construction into tasks whose progress can be reported over time. Moreover, this metering uses experimentally determined values to identify what portion of the level construction corresponds to which subtasks.

Suspending the level construction when it outpaces the queries proved critical for larger database sizes. The sheer number of requests being sent from the ORAM Instance to the ORAM Server for construction tended to starve the requests of actual queries (much fewer in number), causing the query rate to drop quickly as more levels were introduced. This behavior was corrected by forcing level construction to remain proportional to query progress, keeping the individual query rates much closer to the average.

Since it is impractical to repeatedly run trials over the entire (up to 1TB) measured epoch, the database for these trials is first constructed non-obliviously on the server via a specially designed module. The items are inserted randomly so that the final result mirrors an oblivious construction (as would occur from a sequence of write queries).

### 6.1 Setup

PD-ORAM is written in Java. Clients run on quad-core 3.16GHz Xeon X5460 machines. The server runs on a single Quad-Core Intel i7-2600K Sandy Bridge 3.4GHz CPU, with 16GB DDR3 1600 SD-RAM and 7x2TB HITACHI Deskstar 7200RPM SATA 3.0Gb/s disks (RAID0 / LVM). All machines share a gigabit switch. Network latency is shaped by forcing server threads to sleep for the desired round trip duration upon receiving a request, simulating link latency without capping link bandwidth.

The implementation uses a Bloom Filter with 8 hash functions and 2400 bits of space per item, allowing efficient construction within a false positive rate of \(2^{-64}\) per lookup. The resulting Bloom Filter constitutes roughly 25% of the total size of the database records.

Optimization. Rather than optimizing the Bloom Filter size required to obtain this error rate by using a larger number of hashes, as suggested in [13], PD-ORAM uses larger Bloom Filters with fewer hashes to minimize item lookup disk seeks while obtaining the same error rate.

### 6.2 Experiments

One main goal of the experiments is to understand the interaction between network performance parameters and the parallel nature of PD-ORAM.

#### Size + Clients vs. Query Throughput

Figure 6 plots the effect of database size and client parallelization on overall query throughput. Fresh databases were used for all trials to prevent dependency on the order of the trials, except for the 1TB trials, where this proved impractical. Even though individual query latency increases with higher resource contention, the benefits of parallelization are clear: significantly higher overall throughputs.

#### Clients + Network Latency vs. Performance

Figure 7 plots the effect of parallel clients and network latency on overall query throughput for a fixed database size. The premise of this measurement is that parallelization becomes more important as network latency increases.

#### De-Amortization Optimality

Figure 8 plots the observed latency of individual queries versus time on a growing database. With perfect de-amortization, all queries would require the same amount of time. Most queries take around 1200ms; a fixed lower limit is imposed by the network latency. The bands at 2600ms and 3100ms reflect the construction of the top level, which is not de-amortized.

#### Progress Metering

To validate the accuracy of the progress metering, Figure 9 shows the reported construction progress of a single level as sampled every 5 seconds. Strict de-amortization and querying are disabled to avoid cool-down periods when construction has progressed farther than needed, and to ensure measurement of its progress only.

### 6.3 Impact of Disk Latency

The experiments were repeated (for database sizes up to 300GB) in a different setup, where the server was run on dual 3.16Ghz Xeon X5460 quad-core CPUs and six 0.4 TB 15K RPM SCSI (hardware RAID0) disks. This configuration outperformed the previous setup by a factor of 2x in most trials. The primary advantage is the superior seek time on the server disks, suggesting that server disk seek costs play an important role in overall performance.

This was somewhat surprising, as the level construction mechanisms were designed specifically to minimize disk seeks (with the hash table insertion being the only random-access operation during level construction, requiring an average of 2 random writes per insert). The rest of the level construction simply requires reading from one or two sequential buffers and writing out sequentially to one or two.

The culprit is likely the de-amortization process, which constructs different levels in parallel, effectively randomizing disk access patterns. While individual level construction is mostly limited by sequential disk throughput, running many of these processes in parallel across the same file system results in disk seeks even in the sequential access regions, leading to a much lower overall disk throughput.

#### Solutions

Several software and hardware solutions present themselves. Better data placement would split data more efficiently across the available disks (instead of using a RAID configuration) to allow the sequential nature of each level construction process to transfer to sequential disk access. Obtaining optimal throughput in this manner would require a relatively large number of disks. Additionally, the use of low-latency solid-state drives (SSDs) would be a simple hardware solution to eliminate this performance bottleneck. However, it remains to be seen whether the sustained random write performance degradation plaguing current SSDs does not constitute a bigger bottleneck in itself, as preliminary throughput experiments on several recent 128GB Samsung SSDs with 2011 firmware updates seem to suggest.

### 7. An Oblivious File System

ORAM lends itself naturally to the creation of a block device. Due to existing results' impractical performance overhead, this has not been previously possible. A Linux-based deployment of PD-ORAM is used here to design and build privatefs, a fully-functional oblivious network file system in which files can be accessed on a remote server with computational access privacy and data confidentiality.

An initial implementation was built on top of the Linux Network Block Device (NBD) driver, which is the simplest and most natural approach since PD-ORAM already provides a block interface. However, NBD supports only serial, non-parallel access, which limits its scalability.

---

I hope this optimized version meets your needs!