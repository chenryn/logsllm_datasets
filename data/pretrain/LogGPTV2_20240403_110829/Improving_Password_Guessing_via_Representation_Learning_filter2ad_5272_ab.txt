has a signiﬁcant impact on the dynamics of the training, al-
lowing us to perform 30 times more training iterations without
training collapse [21]. We keep the general GAN framework
mostly unchanged because of the excellent performance of the
gradient-penalty-WGAN [33].
With our improvements in the training process, we can
exploit a deeper architecture for both the generator and the
critic. We substitute the plain residual blocks with deeper
residual bottleneck blocks [34], leaving their number intact.
We ﬁnd the use of batch normalization in the generator to be
essential for increasing the number of layers of the networks
successfully.
The new architecture and the revised training process allow
us to learn a better approximation of the target password distri-
bution, and consequently, outperform the original PassGAN. A
comparison between the original and our improved approach
is reported in TABLE I. In this experiment, both models are
trained on 80% of RockYou leak and compared in a trawling
attack4 on the remaining 20% of the set. As the 20% test-
set does not contain passwords present in the train-set, the
performance of a model in this test demonstrates its ability to
generate new valid passwords, excluding over-ﬁtting artifacts.
In this work, we use the improved settings described in the
present section. We train three different generators, using a
80-20% split of RockYou leak, considering passwords with a
maximum length of 10, 16, and 22, respectively.
3Each string is represented as a binary matrix obtained by the concatenation
of the one-hot encoded characters.
4Under the same conﬁguration proposed in [35].
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:44 UTC from IEEE Xplore.  Restrictions apply. 
1384
THE MATCHED PASSWORDS BY PASSGAN AND OUR IMPROVED MODEL
OVER THE ROCKYOU TEST-SET
TABLE I
Number
guesses
1 · 108
1 · 109
1 · 1010
2 · 1010
3 · 1010
4 · 1010
5 · 1010
PassGAN
(%)
Our GAN
(%)
6.72
15.09
26.03
29.54
31.60
33.05
34.19
9.51
23.33
40.48
45.55
48.40
50.34
51.80
2) Autoencoder for password guessing: To highlight the
generality of the proposed approaches, we introduce a second
and novel deep generative model for password guessing. It is
based on Wasserstein Autoencoder (WAE) [51] with moment
matching regularization applied to the latent space (called
WAE-MMD [51]). To allow for sampling from the latent
space, WAE regularizes the latent space to make it coherent
with a chosen prior latent distribution.
A WAE learns a latent representation that shares several
properties with the one coming from the GAN-based tech-
nique. Nevertheless, these models naturally provide a very
accurate inverse mapping, i.e., Enc, which makes the model
superior to the default GAN-based one in certain scenarios.
To add further regulation to the WAE, we train the model
as a Context AE (CAE) [47]. During every iteration of the
training process of a CAE, the encoder receives a noisy version
˜xi of the input password xi. The noisy input is obtained
by removing each of the characters in the password x with
a certain probability p = |xi| where |xi| is the number of
characters in the password, and  is a hyper-parameter ﬁxed
to 5 in our setup. Our model receives the mangled input ˜xi,
and then it is trained to reproduce the complete password
as the output (x = Dec(Enc(˜x))); that is, the model must
estimate the missing characters from the context given by
the available ones. Furthermore, the CAE training procedure
allows us to contextualize the wildcard character that we will
use in Section III-B. We refer to our ﬁnal model as the Context
Wasserstein Autoencoder, or CWAE.
We set up the CWAE with a deeper version of the archi-
tecture used for the GAN generator. We use the same prior
latent distribution of our GAN generator, i.e., N (0, I) with a
dimension of 128. The training process is performed over the
same train-sets of the GAN.
III. CONDITIONAL PASSWORD GUESSING (CPG) AND
PASSWORDS STRONG LOCALITY
In this section, we present the ﬁrst contribution of our paper,
i.e., the password locality concept, and its possible applications
for password guessing. In Section III-A, we describe the most
natural form of locality that we call password strong locality.
In Section III-B, we demonstrate the practical application
of password locality by introducing a technique that we
call “password template inversion” for conditional and partial
knowledge passwords generation. Finally, we demonstrate the
Fig. 2.
2D representation of small portions around three latent points
corresponding to three sample passwords “jimmy91”, “abc123abc”, and
“123456” in the latent space learned from the RockYou train-set. Note: for
the sake of better illustration, the image has been cropped.
advantages that our technique offers over existing probabilistic
and non-probabilistic password models.
A. Password strong locality and localized sampling
As we brieﬂy introduced in Section II-A, the latent rep-
resentation learned by the generator enforces geometric con-
nections among latent points that share semantic relations in
the data space. As a result, the latent representation maintains
“similar” instances closer.
In general, the concept of similarity harnessed in the latent
space of a deep generative model solely depends on the mod-
eled data domain (e.g., images, text) and its distribution. How-
ever, external properties can be incentivized by the designer
via injection of inductive bias during the training. An example
is reported in Appendix A. In the case of our passwords latent
representations, the concept of similarity mainly relies on a
few key factors such as the structure of the password, the
occurrence of common substrings, and the class of characters.
Fig. 2 (obtained by t-SNE [39]) depicts this observation
by showing a 2D representation of small portions around
three latent points (corresponding to three sample passwords
“jimmy91”, “abc123abc”, and “123456”) in the latent
space. Looking at the area with password “jimmy91” as the
center, we can observe how the surrounding passwords share
the same general structure (5L2D, i.e., 5 letters followed by 2
digits) and tend to maintain the substring “jimmy” with minor
variations. Likewise, the area with the string “abc123abc”
exhibits a similar phenomenon, where such a string is not
present
in the selected train-set and does not represent a
common password template.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:44 UTC from IEEE Xplore.  Restrictions apply. 
1385
We loosely name password strong locality the representa-
tion’s inherent property of grouping together passwords that
share very ﬁne-grained characteristics. The password strong
locality property asserts that latent representation of pass-
words sharing some speciﬁc characteristics, such as identical
substrings and structure, are organized in close proximity to
each other. In Section IV-A, we will show that strong locality
also implies a weaker but general form of semantic bounding,
which we refer to as weak locality.
The strong locality becomes particularly compelling when
selecting where to focus on the sampling operation during the
password generation process. Indeed, since different classes
of passwords are organized and bounded into different
zones of the underlying space, it is possible to generate
speciﬁc classes of passwords by sampling from speciﬁc
areas of it. We leverage this technique to induce arbitrary
biases in the generation process.5 However, we must ﬁrst
deﬁne a meaningful and practical way to express such biases,
that
to localize the zones of the latent space we are
interested in.
is,
One naive solution resorts to a prototype password x to
guide the localization process. In particular, we can generate
passwords strictly related to the chosen prototype password
x, by fetching latent points around the latent representation
z of x (i.e., x = G(z)). Thanks to the strong locality, the
obtained latent points should be valid latent representations
of passwords with an arbitrary strong relation with x. In this
context, we refer to the chosen x (or its corresponding latent
representation z) with the term pivot. The three dark red boxes
in Fig. 2 are the pivot points in the latent space for their
corresponding passwords.
To infer the latent representation z from x, we use the
encoder network described in Section II-B2, as that z = E(x).
We highlight
that, being this process general and model-
independent, other deep generative models such as [25], [36],
[40] can be used as well.
Once we obtain the intended pivot z, we can easily generate
coherent passwords by restricting the generator’s sampling in a
conﬁned area of the latent space around z (loosely represented
by the small dashed circles in Fig. 2). To that purpose, we
consider a new latent distribution for the generator. The new
distribution has the latent representation of the pivot password
as its expected value and an arbitrarily small scale. To remain
coherent with prior latent distribution and partially avoiding
distribution mismatch for the sampled points [57], we chose
a Gaussian distribution: N (z, σI).
According to the concept of password locality, the strength
of the semantic relation between a sampled latent point and
its pivot should be proportional to the spatial distance between
the chosen value of σ (i.e., standard
them. Consequently,
deviation) offers us a direct way to control
the level of
semantic bounding existing in the generated passwords. This
intuition is better explained by TABLE II, where passwords
5From the model’s point of view, this is equivalent to changing the latent
distribution, and in particular, reallocating its expected value to a different
zone of the latent space.
THE FIRST-TEN PASSWORDS OBTAINED WITH DIFFERENT VALUES OF σ
STARTING FROM THE PIVOT STRING “jimmy91”
TABLE II
σ=0.05
jimmy91
jimmy11
jimmy21
jimmy88
jimmy81
jimmy98
mimmy98
jimmy28
simmy91
mimmy91
σ=0.08
jimmy99
micmy91
jimsy91
mimmyo1
jbmmy88
simmy98
dijmy91
jimmy98
timsy91
jnmm988
σ=0.10
σ=0.15
mnmm988
tbmmy98
jismyo15
jizmyon
j144988
jbmm998
timsy91
jrm4985
jhmmy88
jhmm988
jimmy91992
jrm6998
sirsy91
jrz4988
Rimky28
missy11
jimmy119
sikjy91
licky916
gimjyon
obtained with different values of σ for the same pivot password
are reported. Lower values of σ produce highly aligned
passwords, whereas larger values of σ allow us to explore
areas far from the pivot and produce a different
type of
“similar” passwords. As shown in TABLE II, all the pass-
words generated with σ = 0.05 retained not only the
structure of the pivot (i.e., 5L2D), but also observed minor
variations coherent with the underlying password distribution.
Of note, passwords generated with σ = 0.15 tend to escape the
password template imposed by the pivot and reaching related-
but-dissimilar password structures (e.g., “jimmy91992” and
“j144988”).
B. Localized passwords generation with password template
inversion
As brieﬂy discussed in Section III-A, the password locality
property offers a natural way to generate a very speciﬁc/con-
ﬁned class of passwords for a chosen pivot, a task accom-
plished by exploiting an encoder network E. This encoder is
trained to approximate the inverse function G−1, and it is the
only tool we have to explore the latent space meaningfully. The
default behavior of the encoder is to take as an input a string s
and precisely localize the corresponding latent representation
in the latent space. As shown in TABLE II, sampling from a
distribution centered on the obtained latent point, allows us to
generate a set of related passwords. However, this approach
alone is not sufﬁcient within the password guessing scenario.
is possible to “trick”
the encoder network into further localizing general classes
of passwords. We can arbitrarily deﬁne these classes via a
minimal template, which expresses the deﬁnition of the target
password class.
In this section, we show that
it
The encoder network can be forced to work around a spe-
ciﬁc password deﬁnition by introducing a wildcard character
into its alphabet. The wildcard character - represented by the
symbol ‘∗’ in the present paper - can be used as a placeholder
to indicate an unspeciﬁed character. For instance, the template
“jimmy∗∗” expresses a class of passwords starting with
the string “jimmy” followed by two undeﬁned characters.
When the encoder inverts this string, the obtained latent point
represents the center of the cluster of passwords in the latent
space with a total length of 8 characters and a preﬁx “jimmy”.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:44 UTC from IEEE Xplore.  Restrictions apply. 
1386
Therefore, sampling around this latent point allows us to
generate good instantiations (according to p(x)) of the input
template. Column A of TABLE III shows an example for the
template “jimmy∗∗”. In practice, we implement this behavior
by mapping a wildcard character to an empty one-hot encoded
vector when the matrix corresponding to the input string is
given to the encoder. The wildcard characters can be placed
in any position to deﬁne an arbitrarily complex password
template; some examples are reported in TABLE III.
Relying on this technique, the template inversion guides us
towards the most plausible zone of the latent space. When
we sample from that zone, the wildcards are replaced with
high-probability characters according to the distribution p(x),
i.e., the probability distribution modeled by the generator. This
phenomenon can be observed in the generated samples (Col-
umn A of TABLE III): wildcards in most of the generated pass-
words have been replaced with digits to conceivably reproduce
the frequent password pattern ‘lower case string+digits’ [52].
On the contrary, passwords from the template “∗∗∗∗∗91” are
reported in Column E of TABLE III. In this example, we ask
the generator to ﬁnd 7-character long passwords where the
last two characters are digits. Here, the generated passwords
tend to lie towards two most likely password classes for this
case, i.e., ‘lower case string+digits’ complementary to the
previous case and ‘all digits.’ As the localized zone of the
latent space is a function of all the observed characters, the
same template with more observable digits (e.g., Column F
of TABLE III) ends up generating all digits passwords with
higher probability.
C. Conditional Password Guessing (CPG)
One of the most signiﬁcant limitations of available proba-
bilistic guessers is their intrinsic rigidity. The inductive bias
imposed on such models allows them to be extremely suitable
for general trawling attacks, yet it causes them to fail at
adapting to different guessing scenarios. For instance, they fail
to handle a natural as well as a general form of conditional
password generation, such as the template-based one that we
proposed in Section III-B. Despite the limitations of existing
approaches, generating guesses under arbitrary biases is a
useful and helpful procedure. This applies to both security
practitioners and common users. Some examples are below:
• An attacker can be interested in generating an arbitrary
number of guesses having a particular structure or com-
mon substring. For instance, an attacker might want to
generate passwords containing the name of the attacked
web application as substring.6
• A conditional password generation capable of working
with partial knowledge can be used by an attacker to
improve the impact of side-channel attacks targeting user
input [16], [41], [54], [17]. These attacks often recover
only an incomplete password (e.g., some characters) due
to their accuracy. An attacker can leverage conditional
password generation mechanisms to input missing char-
acters and recover the target password.
• Similarly, a legitimate user can be interested in recovering
her/his forgotten password while remembering a partial
template, for example, “***Jimmy**1**8#”.
In this direction, conditional password generation is particu-
larly difﬁcult for autoregressive password guessers, such as the
RNN-based ones (e.g., FLA [42]). Indeed, these approaches, in
the general case, are unable to assign a probability to missing
characters of a template efﬁciently; the forward-directionality,
intrinsic in their generation process, eliminates the possibility
of an efﬁcient appreciation of wildcards occurring before a
given substring (e.g., the case in Columns C and E of TA-
BLE III). In these cases, the probability of an exponential
number of passwords could be computed before using the
characters in the template to prune the visit tree. This is
the case of the template reported in Column E, where the
required computational cost for these approaches is not far
from computing all the passwords into the chosen probability
threshold and ﬁlter the ones coherent with the template. More
generally, these approaches cannot be efﬁciently applied when
a large number of wildcards is considered. Sampling from the
posterior distribution over the missing variables (i.e., wild-
cards), indeed, is intractable for not minimal alphabets; for
instance, for an alphabet of size |Σ| , it requires O(|Σ|) runs
6It has been widely observed that many users tend to incorporate such
names in their passwords.
AN EXAMPLE OF EXPLOITING STRONG LOCALITY PROPERTY OVER A GENERATOR TRAINED ON ROCKYOU TRAIN-SET FOR SOME PASSWORD
TEMPLATES. PASSWORDS ARE GENERATED BY SAMPLING 10000 STRINGS WITH α = 0.8 AND REPORTED IN DECREASING FREQUENCY ORDER.
TABLE III
A
jimmy∗∗
jimmy11
jimmy13
jimmy01
jimmy12
jimmy10
jimmy20
jimmy21
jimmy16
jimmy19
jimmyes
B
jimmy∗∗∗∗
jimmybean