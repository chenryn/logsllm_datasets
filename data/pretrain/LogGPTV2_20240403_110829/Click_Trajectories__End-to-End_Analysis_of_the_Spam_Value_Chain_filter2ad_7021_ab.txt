### Storefront and Payment Process for "Pharmacy Express"

"Pharmacy Express" is a brand associated with the Mailien pharmaceutical affiliate program, which is based in Russia. When a user selects an item to purchase and clicks on "Checkout," the storefront redirects them to a payment portal hosted at payquickonline.com. This portal, which serves content from an IP address in Turkey, collects the user's shipping, email contact, and payment information, and provides an order confirmation number. A subsequent email confirms the order, provides an EMS tracking number, and includes a contact email for customer inquiries. The issuing bank then transfers the funds to the acquiring bank, in this case, the Azerigazbank Joint-Stock Investment Bank in Baku, Azerbaijan (BIN 404610). Ten days later, the product arrives, blister-packaged, in a cushioned white envelope. Postal markings indicate that the supplier, named PPW, is based in Chennai, India.

### Cybercrime Economics

While numerous studies have examined the various components of spam (e.g., botnets, fast flux), recent literature has focused on using economic tools to better understand cybercrime, including spam. The goal is to enable more effective reasoning about interventions. Here, we highlight key aspects of this work that have influenced our study.

Some early research aimed to understand the scope of underground markets by evaluating the value of stolen financial credentials, as seen in IRC chatrooms, forums, malware drop-zones, or through direct interception of communications with botnet C&C servers. Herley and FlorÃªncio critiqued this approach, arguing that it does not distinguish between claimed and actual losses and suggests that such environments may reflect "lemon markets" where few participants achieve significant profits, particularly spammers. Although this hypothesis remains untested, its outcome is orthogonal to our focus on understanding the structure of the value chain itself.

Our previous work on spam conversion used empirical methods to infer parts of the return-on-investment picture in the spam business model. In contrast, this study aims to be more comprehensive in both breadth (covering what we believe to be most large spam campaigns) and depth (covering the full value chain), but with less precision regarding specific costs.

Another line of research has examined interventions from an economic perspective, considering the efficacy of site and domain takedowns in creating economic impediments for cybercrime enterprises, particularly phishing. Molnar et al. further developed this approach by comparing it to research on the illicit drug ecosystem. Our work builds on these efforts but focuses specifically on the spam problem.

### Data Collection Methodology

#### Datasets and Methodology

In this section, we describe our datasets and the methodology used to collect, process, and validate them. Figure 2 summarizes our data sources and methods. We start with a variety of full-message spam feeds, URL feeds, and our own botnet-harvested spam. Feed parsers extract embedded URLs from the raw feed data for further processing. A DNS crawler enumerates the resource record sets of the URL's domain, while a farm of Web crawlers visits the URLs, records HTTP-level interactions, and captures landing pages. A clustering tool groups pages by content similarity, and a content tagger labels the clusters according to the category of goods sold and the associated affiliate programs. We then make targeted purchases from each affiliate program and store the feed data and derived metadata in a database.

#### Feeds of Spam-Advertised URLs

We collected feed data from August 1, 2010, through October 31, 2010, obtaining seven distinct URL feeds from third-party partners, including multiple commercial anti-spam providers, and harvested URLs from our own botfarm environment. During this period, we received nearly 1 billion URLs. Table I summarizes our feed sources, the type of each feed, the number of URLs received, and the number of distinct registered domains in those URLs. Note that "bot" feeds tend to be focused spam sources, while other feeds are spam sinks comprising a blend of spam from various sources. Individual feeds, especially those from botnets, can be heavily skewed. For example, the Grum bot provided over 11 million URLs, but only 348 distinct registered domains. Conversely, the Rustock bot generated 13 million distinct domains, many of which were "garbage" domains used in a blacklist-poisoning campaign.

#### Crawler Data

The URL feed data drives active crawling measurements that collect information about both the DNS infrastructure and the Web hosting infrastructure. We use distinct crawlers for each set of measurements.

**DNS Crawler:**
We developed a DNS crawler to identify the name server infrastructure supporting spam-advertised domains and the address records they specify. This process is complicated by fast flux techniques used to minimize central points of weakness. The crawler extracts the fully qualified domain name and the registered domain suffix, ignoring IPv4 addresses, invalidly formatted domain names, and duplicate domains queried within the last day. It performs recursive queries, identifies successfully resolved domains, and filters out unregistered domains and those with unreachable name servers. The crawler also detects wildcard domains and exhaustively enumerates all relevant DNS records until convergence.

**Web Crawler:**
The Web crawler simulates a user clicking on the URLs from the spam feeds. It captures application-level redirects, DNS names, HTTP headers, and the final page displayed, represented by its DOM tree and a screenshot. Crawling spam URLs presents practical challenges in terms of scale, robustness, and adversarial conditions. For this study, we crawled nearly 15 million URLs, successfully visiting and downloading correct Web content for over 6 million. To manage this load, we replicated the crawler across a cluster of machines, each with a controller managing over 100 instances of Firefox 3.6.10 running in parallel. The controller retrieves batches of URLs from the database and assigns them to browser instances in a round-robin fashion across diverse IP address ranges. Table II summarizes our crawling efforts, showing that we covered over 98% of the URLs received.

#### Content Clustering and Tagging

The crawlers provide low-level information about URLs and domains. In the next stage, we process this output to associate it with higher-level spam business activities. We focus on businesses selling three categories of spam-advertised products: pharmaceuticals, replicas, and software. These categories are among the most popular goods advertised in spam.

To classify each website, we use content clustering to group sites with similar content, category tagging to label clustered sites with the category of goods they sell, and program tagging to label clusters with their specific affiliate program and/or storefront brand. We use a combination of automated and manual analysis techniques to make clustering and tagging feasible for large datasets while ensuring manageable validation.

Table III summarizes the results of this process, listing the number of received URLs with registered domains used by the affiliate programs, the number of registered domains in those URLs, the number of clusters formed based on the contents of storefront web pages, and the number of affiliate programs identified from the clusters. As expected, pharmaceutical affiliate programs dominate the dataset, followed by replicas and then software. We identified a total of 45 affiliate programs for the three categories combined, advertised via 69,002 distinct registered domains (contained within 38% of all URLs received in our feeds).

**Content Clustering:**
The first step uses a clustering tool to group together web pages with very similar content. The tool generates a fingerprint for each page using a q-gram similarity approach and compares it with existing cluster fingerprints. If the page fingerprint exceeds a similarity threshold (Jaccard index of 0.75), it is placed in the most similar cluster; otherwise, a new cluster is created.

**Category Tagging:**
Clusters group URLs and domains with the same page content. Category tagging separates these clusters into those selling goods of interest and those that do not. We use generic keywords found in the page content to identify and label interesting clusters, being conservative to avoid false negatives.

This detailed methodology allows us to comprehensively analyze the spam ecosystem and understand the value chain involved in spam-advertised products.