the storefront for “Pharmacy Express,” a brand associated
with the Mailien pharmaceutical afﬁliate program based in
Russia ().
After selecting an item to purchase and clicking on
“Checkout”, the storefront redirects the user to a payment
portal served from payquickonline.com (this time serving
content via an IP address in Turkey), which accepts the
user’s shipping, email contact, and payment information, and
provides an order conﬁrmation number. Subsequent email
conﬁrms the order, provides an EMS tracking number, and
includes a contact email for customer questions. The bank
that issued the user’s credit card transfers money to the
acquiring bank, in this case the Azerigazbank Joint-Stock
Investment Bank in Baku, Azerbaijan (BIN 404610, ).
Ten days later the product arrives, blister-packaged, in a
cushioned white envelope with postal markings indicating
a supplier named PPW based in Chennai, India as its
originator ().
C. Cybercrime economics
Alongside the myriad studies of the various components
employed in spam (e.g., botnets, fast ﬂux, etc.), a literature
has recently emerged that focuses on using economic tools
for understanding cybercrime (including spam) in a more
systematic fashion, with an aim towards enabling better
reasoning about effective interventions. Here we highlight
elements of this work that have inﬂuenced our study.
Some of the earliest such work has aimed to understand
the scope of underground markets based on the value of
found goods (typically stolen ﬁnancial credentials), either as
seen on IRC chatrooms [10], forums [59], malware “drop-
zones” [16], or directly by intercepting communications to
botnet C&C servers [50]. Herley and Florˆencio critique this
line of work as not distinguishing between claimed and
true losses, and speculate that such environments inherently
reﬂect “lemon markets” in which few participants are likely
to acquire signiﬁcant proﬁts (particularly spammers) [15].
While this hypothesis remains untested,
its outcome is
orthogonal to our focus of understanding the structure of
the value chain itself.
Our own previous work on spam conversion also used
empirical means to infer parts of the return-on-investment
picture in the spam business model [21]. By contrast,
this study aims to be considerably more comprehensive in
breadth (covering what we believe reﬂect most large spam
campaigns) and depth (covering the fullness of the value
chain), but offering less precision regarding speciﬁc costs.
Finally, another line of work has examined interventions
from an economic basis, considering the efﬁcacy of site
and domain takedown in creating an economic impediment
for cybercrime enterprises (notably phishing) [6], [35], [36].
Molnar et al. further develop this approach via comparisons
with research on the illicit drug ecosystem [34]. Our work
builds on this, but focuses deeply on the spam problem in
particular.
III. DATA COLLECTION METHODOLOGY
In this section we describe our datasets and the method-
ology by which we collected, processed, and validated
them. Figure 2 concisely summarizes our data sources and
methods. We start with a variety of full-message spam feeds,
URL feeds, and our own botnet-harvested spam (). Feed
parsers extract embedded URLs from the raw feed data for
further processing (). A DNS crawler enumerates various
resource record sets of the URL’s domain, while a farm
of Web crawlers visits the URLs and records HTTP-level
interactions and landing pages (). A clustering tool clusters
pages by content similarity (). A content tagger labels the
content clusters according to the category of goods sold, and
the associated afﬁliate programs (). We then make targeted
purchases from each afﬁliate program (), and store the
feed data and distilled and derived metadata in a database
434
Feed
Name
Feed A
Feed B
Feed C
Feed D
Feed X
Feed Y
Feed Z
Cutwail
Grum
MegaD
Rustock
Other bots
Total
Feed
Description
MX honeypot
Seeded honey accounts
MX honeypot
Seeded honey accounts
MX honeypot
Human identiﬁed
MX honeypot
Bot
Bot
Bot
Bot
Bot
Received
URLs
32,548,304
73,614,895
451,603,575
30,991,248
198,871,030
10,733,231
12,517,244
3,267,575
11,920,449
1,221,253
141,621,731
7,768
968,918,303
Distinct
Domains
100,631
35,506
1,315,292
79,040
2,127,164
1,051,211
67,856
65
348
4
13,612,815
4
17,813,952
Table I: Feeds of spam-advertised URLs used in this study. We
collected feed data from August 1, 2010 through October 31, 2010.
obtained seven distinct URL feeds from third-party partners
(including multiple commercial anti-spam providers), and
harvested URLs from our own botfarm environment.
For this study, we used the data from these feeds from
August 1, 2010 through October 31, 2010, which together
comprised nearly 1 billion URLs. Table I summarizes our
feed sources along with the “type” of each feed, the number
of URLs received in the feed during this time period, and
the number of distinct registered domains in those URLs.
Note that the “bot” feeds tend to be focused spam sources,
while the other feeds are spam sinks comprised of a blend
of spam from a variety of sources. Further, individual feeds,
particularly those gathered directly from botnets, can be
heavily skewed in their makeup. For example, we received
over 11M URLs from the Grum bot, but these only contained
348 distinct registered domains. Conversely, the 13M distinct
domains produced by the Rustock bot are artifacts of a
“blacklist-poisoning” campaign undertaken by the bot op-
erators that comprised millions of “garbage” domains [54].
Thus, one must be mindful of these issues when analyzing
such feed data in aggregate.
From these feeds we extract and normalize embedded
URLs and insert them into a large multi-terabyte Postgres
database. The resulting “feed tables” drive virtually all
subsequent data gathering.
B. Crawler data
The URL feed data subsequently drives active crawling
measurements that collect information about both the DNS
infrastructure used to name the site being advertised and the
Web hosting infrastructure that serves site content to visitors.
We use distinct crawlers for each set of measurements.
DNS Crawler: We developed a DNS crawler to iden-
tify the name server infrastructure used to support spam-
advertised domains, and the address records they specify for
hosting those names. Under normal use of DNS this process
would be straightforward, but in practice it is signiﬁcantly
435
Figure 2: Our data collection and processing workﬂow.
for subsequent analysis in Section IV. (Steps  and  are
partially manual operations, the others are fully automated.)
The rest of this section describes these steps in detail.
A. Collecting Spam-Advertised URLs
Our study is driven by a broad range of data sources of
varying types, some of which are provided by third parties,
while others we collect ourselves. Since the goal of this
study is to decompose the spam ecosystem, it is natural
that our seed data arises from spam email
itself. More
speciﬁcally, we focus on the URLs embedded within such
email, since these are the vectors used to drive recipient
trafﬁc to particular Web sites. To support this goal, we
complicated by fast ﬂux techniques employed to minimize
central points of weakness. Similar to the work of [18], we
query servers repeatedly to enumerate the set of domains
collectively used for click support (Section II-A).
From each URL, we extract both the fully qualiﬁed
domain name and the registered domain sufﬁx (for example,
if we see a domain foo.bar.co.uk we will extract both
foo.bar.co.uk as well as bar.co.uk). We ignore URLs
with IPv4 addresses (just 0.36% of URLs) or invalidly
formatted domain names, as well as duplicate domains
already queried within the last day.
The crawler then performs recursive queries on these
domains. It identiﬁes the domains that resolve successfully
and their authoritative domains, and ﬁlters out unregistered
domains and domains with unreachable name servers. To
prevent fruitless domain enumeration, it also detects wild-
card domains (abc.example.com, def.example.com, etc.)
where all child domains resolve to the same IP address. In
each case, the crawler exhaustively enumerates all A, NS,
SOA, CNAME, MX, and TXT records linked to a particular
domain.
The crawler periodically queries new records until
it
converges on a set of distinct results. It heuristically de-
termines convergence using standard maximum likelihood
methods to estimate when the probability of observing a
new unique record has become small. For added assurance,
after convergence the crawler continues to query domains
daily looking for new records (ultimately timing out after a
week if it discovers none).
Web Crawler: The Web crawler replicates the experience
of a user clicking on the URLs derived from the spam
feeds. It captures any application-level redirects (HTML,
JavaScript, Flash), the DNS names and HTTP headers of any
intermediate servers and the ﬁnal server, and the page that is
ultimately displayed—represented both by its DOM tree and
as a screenshot from a browser. Although straightforward in
theory, crawling spam URLs presents a number of practical
challenges in terms of scale, robustness, and adversarial
conditions.
For this study we crawled nearly 15 million URLs, of
which we successfully visited and downloaded correct Web
content for over 6 million (unreachable domains, blacklist-
ing, etc., prevent successful crawling of many pages).3 To
manage this load, we replicate the crawler across a cluster
of machines. Each crawler replica consists of a controller
managing over 100 instances of Firefox 3.6.10 running in
parallel. The controller connects to a custom Firefox exten-
sion to manage each browser instance, which incorporates
the Screengrab! extension [38] to capture screen shots (used
for manual investigations). The controller retrieves batches
of URLs from the database, and assigns URLs to Firefox
3By comparison, the spam hosting studies of Anderson et al. and Konte
et al. analyzed 150,000 messages per day and 115,000 messages per month
respectively [1], [22].
436
Stage
Received URLs
Distinct URLs
Distinct domains
Distinct domains crawled
URLs covered
Count
968,918,303
93,185,779
17,813,952
3,495,627
950,716,776
(9.6%)
(98.1%)
Table II: Summary results of URL crawling. We crawl the regis-
tered domains used by over 98% of the URLs received.
instances in a round-robin fashion across a diverse set of IP
address ranges.4
Table II summarizes our crawling efforts. Since there is
substantial redundancy in the feeds (e.g., fewer than 10%
of the URLs are even unique), crawling every URL is
unnecessary and resource inefﬁcient. Instead, we focus on
crawling URLs that cover the set of registered domains used
by all URLs in the feed. Except in rare instances, all URLs
to a registered domain are for the same afﬁliate program.
Thus, the crawler prioritizes URLs with previously unseen
registered domains, ignores any URLs crawled previously,
and rate limits crawling URLs containing the same regis-
tered domain—both to deal with feed skew as well as to
prevent the crawler from being blacklisted. For timeliness,
the crawler visits URLs within 30 minutes of appearing in
the feeds.
We achieve nearly complete coverage: Over 98% of the
URLs received in the raw feeds use registered domains that
we crawl. Note that we obtain this coverage even though
we crawled URLs that account for only 20% of the nearly
18 million distinct registered domains in the feeds. This
outcome reﬂects the inherent skew in the feed makeup. The
vast majority of the remaining 80% of domains we did
not crawl, and the corresponding 2% URLs that use those
domains, are from the domain-poisoning spam sent by the
Rustock bot and do not reﬂect real sites (Section III-A).
C. Content Clustering and Tagging
The crawlers provide low-level information about URLs
and domains. In the next stage of our methodology, we
process the crawler output to associate this information with
higher-level spam business activities.
Note that in this study we exclusively focus on businesses
selling three categories of spam-advertised products: phar-
maceuticals, replicas, and software. We chose these cate-
gories because they are reportedly among the most popular
goods advertised in spam [31]—an observation borne out in
our data as well.5
4Among the complexities, scammers are aware that security companies
crawl them and blacklist IP addresses they suspect are crawlers. We mitigate
this effect by tunneling requests through proxies running in multiple
disparate IP address ranges.
5We did not consider two other popular categories (pornography and
gambling) for institutional and procedural reasons.
Stage
URLs
Domains
Web clusters
Programs
Pharmacy
346,993,046
54,220
968
30
Software
3,071,828
7,252
51
5
Replicas
15,330,404
7,530
20
10
Total
365,395,278
69,002
1,039
45
Table III: Breakdown of clustering and tagging results.
To classify each Web site, we use content clustering to
match sites with lexically similar content structure, category
tagging to label clustered sites with the category of goods
they sell, and program tagging to label clusters with their
speciﬁc afﬁliate program and/or storefront brand. We use a
combination of automated and manual analysis techniques to
make clustering and tagging feasible for our large datasets,
while still being able to manageably validate our results.
Table III summarizes the results of this process. It lists
the number of received URLs with registered domains used
by the afﬁliate programs we study, the number of registered
domains in those URLs, the number of clusters formed based
on the contents of storefront Web pages, and the number
of afﬁliate programs that we identify from the clusters. As
expected, pharmaceutical afﬁliate programs dominate the
data set, followed by replicas and then software. We identify
a total of 45 afﬁliate programs for the three categories
combined, that are advertised via 69,002 distinct registered
domains (contained within 38% of all URLs received in our
feeds). We next describe the clustering and tagging process
in more detail.
Content clustering: The ﬁrst step in our process uses a
clustering tool to group together Web pages that have very
similar content. The tool uses the HTML text of the crawled
Web pages as the basis for clustering. For each crawled
Web page, it uses a q-gram similarity approach to generate
a ﬁngerprint consisting of a set of multiple independent
hash values over all 4-byte tokens of the HTML text. After
the crawler visits a page, the clustering tool computes the
ﬁngerprint of the page and compares it with the ﬁngerprints
representing existing clusters. If the page ﬁngerprint exceeds
a similarity threshold with a cluster ﬁngerprint (equivalent
to a Jaccard index of 0.75), it places the page in the cluster
with the greatest similarity. Otherwise, it instantiates a new
cluster with the page as its representative.
Category tagging: The clusters group together URLs and
domains that map to the same page content. The next step of
category tagging broadly separates these clusters into those
selling goods that we are interested in, and those clusters
that do not (e.g., domain parking, gambling, etc). We are
intentionally conservative in this step, potentially including
clusters that turn out to be false positives to ensure that
we include all clusters that fall into one of our categories
(thereby avoiding false negatives).
We identify interesting clusters using generic keywords
those clusters
found in the page content, and we label
437