的是，封装机制通常也会带来成本问题：包尺寸的增加。包封装需要一定程度的成本
包封装是一个强有力的手段，可以为我们的网络设计和改进提供足够的灵活性。不幸
接即可，它们甚至可以存在不同的大陆上。
到的那样。网络负载均衡器和后端不再需要共存在同一个广播域中，只要中间有路由连
接收到网络包，将IP和GRE层拆除，直接处理内部的IP包，就像直接从网络接口接收
[Han94]）封装到另外一个IP包中，使用后端服务器的地址作为目标地址。后端服务器
模式。网络服务在均衡器将待转发的网络包采用通用路由封装协议（GRE，参见文献
我们现在的VIP负载均衡解决方案（参见文献[Eis16]）使用的是包封装（encapsulation）
放弃了这种方案。
需要在同一个广播域中。正如你想的那样，Google在一段时间后，由于规模原因，已经
通。如果服务器数量不多，网络能够支撑这样的连接那就不是问题，但是所有的机器都
所有的机器（也就是所有的负载均衡器和所有的后端服务器）必须可以在数据链路层相
不需要保持状态。但是使用2层信息进行内部负载均衡会导致在大规模部署下出现问题：
节约大量资源，因为仅仅只有一小部分用户流量需要穿过负载均衡器。更好的是，DSR
应（DSR）。如果用户请求很小，而回复很大（恰如大多数HTTP请求那样），DSR可以
源和目标地址信息。后端服务器可以直接发送回应给用户一
第19章前端服务器的负载均衡
一但是不管是在前端负载均衡方面，还是在数
一这被称之为直接服务器响
---
## Page 239
的各种不同算法。某些算法可能更适合（或者更不适合）某一个特定的案例，但是这些
不完全一样。这里采用的是一个通用化的场景，这样可以更容易地讨论对不同服务适用
服务每个都在使用本章讨论的算法的不同组合。我们这里举的例子，和任意一个服务都
我们这里要提到的是，Google数据中心内部运行了大量的、非常不同的用户服务。这些
处理该请求。客户端与后端的通信采用的是建立在TCP和UDP之上的应用层协议。
和这些后端任务建立连接。客户端任务在处理每个请求时，必须决定哪个后端任务应该
决于数据中心大小）。通常情况下，服务一般由100~1,000个进程组成。我们将这些进
将会导致50%以上的服务容量不可用），最大的服务可能由超过10,000个进程组成（取
通常有至少三个这样的进程在运行（如果进程少于三个，通常意味着一台物理机器失败
服务器，以进程的方式运行在很多不同的物理服务器上构成的。一个最小的这样的服务
们假设数据中心中有对应该请求的用户服务，这些服务是由一些同质的、可互换的软件
或者只是刚刚超过（这时数据中心级别的负载均衡还没有迁走这部分流量）。同时，我
自另外一个数据中心，或者两者皆有。该请求的速率还没有超过数据中心的处理能力，
假设目前已经有一些请求抵达了数据中心一
之内。
备（例如交换机、数据包路由等），以及数据中心级别的负载均衡都不在本章讨论范围
算法。本章覆盖了将请求路由给某个具体服务器的应用级别策略。底层的网络链接和设
程称之为后端任务（或者后端，backend）。其他任务，我们称之为客户端任务（client）
本章关注于数据中心内部的负载均衡系统，我们将讨论在数据中心内部采用的负载均衡
数据中心内部的负载均衡系统
一这些请求可能来自数据中心内部，或者来
作者：AlejandroForeroCuervo
编辑：Sarah Chavis
第20章
197
232
231
---
## Page 240
233
CPU都处于闲置状态。
图20-2的左图所示的情况是大量的容量都被浪费了：除了最忙的任务之外，其他任务的
图20-1：任务负载分布的两种场景
免再给该数据中心发送额外请求，因为这样做可能会导致某些任务过载。
20-1列举的同一时间周期内的两个不同场景。在此时，跨数据中心负载均衡算法必须避
在最忙的任务达到容量限制之前，我们可以继续将用户流量发往这个数据中心。正如图
点上，最忙和最不忙的任务永远消耗同样数量的CPU。
在理想情况下，某个服务的负载会完全均匀地分发给所有的后端任务。在任何一个时间
理想情况
串的后端依赖请求，也可能在多个节点处拓展为多个并发请求。
赖的其他服务。有的时候，整个依赖处理栈会非常深，单个HTTP请求可能会触发一长
这些后端服务器都需要采用同样的负载均衡算法联系其他基础设施服务，或者它们所依
后端服务器。
套系统使用一个基于URL模式匹配的配置文件将某个特定请求转发给其他团队控制的
算法，以及第19章中介绍的算法，将请求转发给某个特定的服务进程以处理该请求。这
求都会由Google 前端服务器（GFE）处理一
这些算法在Google整个技术栈的多个部分上都有应用。例如，大部分的外部HTTP请
算法都是数个Google工程师在多年内积累而成的。
198
第20章数据中心内部的负载均衡系统
Capacity Limit
(pertask)
。为了处理这些请求（这里的回应会先回复给GFE，再转发给客户端浏览器），
CPU
1
BAD
Time
Per-task Load Distribution
一Google的反向代理系统。GFE使用这些
GOOD
Time
---
## Page 241
可能在该限制到达之前就过载了。反之亦然，在某些情况下，客户端可能在后端还有充
不幸的是，这种简单的方法只能保护后端任务不受非常极端的过载情况影响，后端很有
户端会自动避开这个后端，从而将任务分配给其他的后端。
也是一种非常简单的负载均衡机制：如果某个后端任务过载了，请求处理开始变慢，客
内完成，使得这个限制在正常情况下几乎不会触发。这种（非常简单的）流速控制机制
数后端任务来说，100是个合理的限制。在大多数情况下，请求通常能够在很短的时间
到一定数量时，该客户端将该后端服务器标记为异常状态，不再给它发送请求。对大多
假设我们的客户端任务会跟踪记录发往每个后端的请求状态。当某个后端的活跃请求达
异常任务的简单应对办法：流速控制
后端任务池中处于不健康状态的任务。
在我们决定哪个后端任务应该接受客户端请求之前，首先要先识别一
识别异常任务：流速控制和跛脚鸭任务
为你的服务一共预留了1000个CPU，但是却无法让实际使用超过700个CPU。
上述这个例子描述了糟糕的负载均衡策略会导致资源可用率下降：你可能在数据中心中
费的意思是指“保留的”但是“未使用”的。
任务O的CPU差值。也就是说，我们总共会浪费CPU[O]－CPU[i的总和。在这里，浪
0是负载最高的任务。那么，在分布情况下，我们这里消耗的CPU资源就是其他任务与
我们更正式地定义CPUT司为某个任务i在某个时刻所消耗的CPU资源数量，假设任务
图20-2:CPU的使用和浪费的直方图
Time
BAD
CPU Usage by Task at a Given Time
CPU used
识别异常任务：流速控制和跛脚鸭任务
09.0
cPU wasted
GOOD
Time
并且避开一
199
一在
234
---
## Page 242
235
200
用户透明。这个停止过程通常按照以下步骤进行：
后端任务可以让处理代码推送、设备维护活动，和机器故障问题导致的任务重启变得对
止过程中的任务不会给正在处理的请求返回一个错误值。能够无影响地停止一个活跃的
允许任务处于这种半正常的跛脚鸭状态的好处就是让无缝停止任务变得更容易，处于停
快地传递给所有的客户端一通常在一到两个RTT周期内一
立TCP连接的客户端）也会定期发送UDP健康检查包。这就使跛脚鸭状态可以相对较
些没有建立连接的客户端呢？在Google的RPC框架实现中，不活跃的客户端（没有建
当某个请求进入跋脚鸭状态时，它会将这个状态广播给所有已经连接的客户端。但是那
玻脚鸭状态
拒绝连接
健康
从一个客户端的视角来看，某个后端任务可能处于下列任一种状态中：
一个可靠的识别异常任务的方法：跛脚鸭状态
务是真的处于不健康状态，还是仅仅回复有点慢。
节这个限制来暂时避免这种情况发生，但是这并不能解决底层的根源问题：识别一个任
记为不可用，所有的客户端请求都被阻挡了，直到这些请求超时并且失败。可以通过调
不会很快回复。我们曾经见到过这样的案例，这个默认限制导致了全部后端任务都被标
足资源的情况下就触发了这个限制。例如，有些后端服务器可能有特殊的长连接请求，
4.随着请求回复被发送回客户端，该后端任务的活跃请求逐渐降低为0。
后端任务正在监听端口，并且可以服务请求，但是已经明确要求客户端停止发送请求，
3.任何在后端进入跛脚鸭状态时正在进行的请求（或者在进入状态之后，但是其他
2.后端任务进入跋脚鸭状态，同时请求它的所有客户端发送请求给其他后端任务，
1.任务编排系统发送一个SIGTERM信号给该任务。
正处于一种异常状态（虽然很少有后端任务在非停止状态下停止监听端口）。
后端任务处于无响应状态。这可能是因为任务正在启动或者停止，或者是因为后端
后端任务初始化成功，正在处理请求。
客户端收到通知之前）仍会继续进行。
这通过SIGTERM信号处理程序中调用RPC实现中的API完成。
第20章数据中心内部的负载均衡系统
一无论它们处于什么状态下
---
## Page 243
客户端连接过多后端任务，或者一个后端任务接收过多客户端连接。
然这个消耗理论上很小，但是一旦数量多起来就可能变得很可观。子集化可以避免一个
每个连接都需要双方消耗一定数量的内存和CPU（由于定期健康检查导致）来维护。
实现可以自动将该连接转为“不活跃”状态，转为UDP模式连接，而非TCP模式。
资源成本和造成延迟问题。在极端情况下，如果某个连接闲置时间非常长，我们的RPC
到客户端终止。另外一个方案是针对每个请求建立和销毁后端连接，这样会带来极大的
通常在客户端启动的时候就建立完成，并且保持活跃状态，不停地有请求通过它们，直
我们的RPC系统中的每个客户端都会针对后端程序维持一个长连接发送请求。这些连接
需要连接的后端任务数量。
在健康管理之外，负载均衡另外要考虑的一个因素就是子集划分：限制某个客户端任务
利用划分子集限制连接池大小
加了一些不必要的延迟。
服务请求）就建立连接。如果后端程序等到服务可以接受请求的时候才建立链接，就增
集大小主要由你的服务决定。例如，以下情况可能需要一个相对大的子集数量：
法决定。我们通常使用20~100个后端数量作为子集大小，但是每个系统的“正确”子
选择合适的子集
这个策略使客户端可以在后端程序进行耗时较长的初始化过程中（这时后端程序还不能
子集的选择过程由确定每个客户端任务需要连接的后端数量一
5.在配置的时间过后，该后端程序要么自己干净地退出，要么任务编排系统主动杀
·某个客户端任务经常出现负载不平衡的情况（也就是说，某个客户端会比其他的
·客户端数量相比后端数量少很多。在这种情况下，你希望每个客户端都有足够的
后端任务可供连接，以免某些后端接收不到请求。
150s是一个不错的选择。
间完成。
掉它。该时间应该被设置为一个足够大的值，以便一般的请求可以有足够的时
的并发请求（例如，读取某个用户的全部关注者的全部信息）。因为这样的突发
常见。这些客户端任务接收其他客户端任务发来的请求，将其拓展为一个非常大
客户端发送更多的请求）。这个场景在某个客户端偶尔发送突发性请求的时候很
。每个服务的该数值都不同，一般来说取决于客户端的复杂程度，10s到
。一旦后端程序可以提供服务了，它就会主动通知所有客户端。
利用划分子集限制连接池大小