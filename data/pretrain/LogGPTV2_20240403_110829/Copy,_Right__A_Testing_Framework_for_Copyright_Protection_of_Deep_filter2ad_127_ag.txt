distinguish the two types of models. One reason is that CW at-
tack optimizes adversarial examples for minimal perturbations,
which is more sensitive (less robust) to model modiﬁcations.
FGSM can be regarded as a one-step PGD, which usually has
a larger average perturbation than PGD. When the perturbation
increases, the RobD value of negative suspect models would
decrease since adversarial examples with larger perturbations
tend to have better transferability [43]. It is similar to PGD3
when the perturbation bound increases. In general, the absolute
RobD gap between the positive and negative suspects tested
with PGD-generated test cases is larger than that of FGSM
and CW. Moreover, PGD is relatively cheaper to calculate
than CW, i.e., the time cost of PGD is 100× lower than CW.
Overall, PGD is more suitable for ﬁngerprinting the decision
boundary with untargeted adversarial examples, as shown in
Fig. 2. We will explore more effective metrics and test case
generation methods with diverse granularity in future work.
Remark 5: Different generation strategies and pa-
rameters can impact DEEPJUDGE differently. Overall,
PGD is a better choice for characterizing the model’s
decision boundary.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
838
TABLE X: Using different methods to generate adversarial examples for the RobD metric evaluation on CIFAR-10 dataset.
PGD/3 : with 1
3× perturbation bound, PGD3 : with 3× bound, PGD10s: with 10× steps.
Model Type
Positive
Suspect
Models
Negative
Suspect
Models
FT-LL
FT-AL
RT-AL
P-20%
P-60%
Neg-1
Neg-2
τλ
FGSM
0.024±0.004
0.261±0.025
0.267±0.025
0.252±0.030
0.293±0.027
0.662±0.058
0.672±0.019
0.583
CW
0.0±0.0
0.905±0.028
0.917±0.024
0.882±0.038
0.940±0.013
0.999±0.002
0.998±0.003
0.897
PGD
0.0±0.0
0.192±0.028
0.237±0.055
0.155±0.032
0.318±0.036
0.920±0.021
0.926±0.030
0.816
PGD/3
0.034±0.002
0.733±0.012
0.748±0.046
0.702±0.023
0.792±0.023
0.989±0.007
0.986±0.004
0.886
PGD3
0.0±0.0
0.046±0.010
0.073±0.022
0.045±0.020
0.123±0.022
0.573±0.093
0.576±0.030
0.489
PGD10s
0±0.0
0.350±0.027
0.400±0.046
0.299±0.049
0.502±0.031
0.958±0.013
0.948±0.012
0.851
Fig. 11: A trigger input example used in backdooring.
Fig. 10: Normalized distance evaluations based on different
layers of the CIFAR-10 network: ‘-Shallow’ means the results
on the shallow layer, and ‘-Deep’ means the deep layer.
Layer Selection. Layer selection is important when applying
DEEPJUDGE in the white-box setting with NOD and NAD.
Here, we evaluate how the choice of layers affects the perfor-
mance of DEEPJUDGE. For comparison, we choose a shallow
layer and a deep layer of the victim model, and re-generate
the test cases respectively for each layer. Fig. 10 shows the
results of NOD and NAD metrics. In general, the NOD/NAD
difference between the positive and negative suspect models
becomes much larger at the shallow layer. The reason is that
the shallow layers of a network usually learn the low-level
features [44], and they tend to stay the same or at
least
similar during model ﬁnetuning. Particularly, the performance
on RT-AL degrades the most when the deep layer is selected,
since the parameters of the last layer are re-initialized. Thus,
choosing the shallow layers to compute the NOD and NAD
metrics could help the robustness of DEEPJUDGE. Moreover,
the time cost of generating and testing with the shallow layer
is 10× less than the deep layer, since most of the back-
propagation computations are eliminated.
Remark 6: The shallow layers are a better choice for
testing metrics NOD and NAD.
E. Defense Baselines
1) Backdoor-based watermarking (Black-box):
[47] em-
beds backdoors into the model. In our experiments, we select
500 samples from the training dataset, of which the ground
truth labels are “automobile”. Then we patch an “apple” logo
at the bottom right corner of each sample and change their
labels to “cat” (see Fig. 11). These trigger examples (i.e.,
trigger set) are mixed into the clean training dataset to train
2) Signature-based watermarking (White-box):
a watermarked model from scratch. The initial TSA of the
watermarked model is 100.0% (on a separate trigger set).
[40] em-
beds a T -bit vector (i.e., the watermark) b ∈ {0, 1}T into one
of the convolutional layers, by adding an additional parameter
regularizer into the loss function: E(w) = E0(w) +λE R(w),
where E0(w) is the original task loss function, ER(w) is the
regularizer that imposes a certain restriction on the model
parameters w, and λ is a hyper-parameter. In our experiments,
λ is set to 0.01, and we embed a 128-bit watermark (generated
by the random strategy) into the second convolutional block
(Conv-2 group) as recommended in [40]. The initial BER of
the watermarked model is 3.13%.
3) Fingerprinting (Black-box): IPGuard [2] proposes a type
of adversarial attack that targets on generating adversarial
examples x(cid:2) around the classiﬁcation boundaries of the victim
model, and the matching rate (MR) of these key samples is
calculated for the veriﬁcation similar to [47]. We generate a set
of 1,000 adversarial examples following [2] and the initial MR
of the victim model on the generated key samples is 100.0%.
F. Model Extraction Attacks
Jacobian-Based Augmentation. The seeds used for augmen-
tation are all sampled from the testing dataset. We sample
150 seeds for extracting the MNIST victim model, 500 seeds
for SpeechCommands, 1,000 seeds for CIFAR-10, and use all
other default settings [33].
Knockoff Nets. We use the Fashion-MNIST dataset for ex-
tracting the MNIST victim model, an independent speech
dataset for SpeechCommands, and CIFAR-100 for CIFAR-10.
We use other default hyper-parameter settings of [32].
ES Attack. We use the OPT-SYN algorithm [45] to heuristi-
cally synthesize the surrogate data. We set the stealing epoch to
50 for MNIST and 400 for CIFAR-10. We failed to extract the
SpeechCommands Victim model since the validation accuracy
could not exceed 20%. All other hyper-parameters are the
same as in [45].
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
839
∗Functionality-equivalent Extraction. Besides the above
three extraction attacks, we are also aware of the functionality-
equivalent extraction attacks [19], [3] that attempt to obtain
a precise functional approximation of the victim model. For
instance, [3] proposed a differential attack that could steal the
parameters of the victim model up to ﬂoating-point precision
without
the knowledge of training data. We remark that
defending this type of attack is a trivial task for DEEPJUDGE
as there will be no difference between the extracted model and
the victim model in an ideal approximation.
Note that Black-box model extraction is still underexplored,
and more extraction attacks may appear in the future. This
poses a continuous challenge for deep learning copyright
protection. We hope that DEEPJUDGE could evolve with the
adversaries by incorporating more advanced testing metrics
and test case generation methods, and provide a possibility to
ﬁght against this continuing model stealing threat.
G. Adaptive Attacks for Watermarking & Fingerprinting
In addition to Section VI, here we conduct an extra eval-
uation of existing watermarking [40], [47] and ﬁngerprinting
[2] methods under similar adaptive attack settings.
Adaptive attacks. Adv-Train and VTL are the two adaptive
attacks in Table VII, while the Adapt-X attack is speciﬁcally
designed for each method as follows:
• Adapt-X for [40]. Since the embedded watermark (signa-
ture) is known, the adversary copies (steals) the victim
model
then ﬁne-tunes it on a small subset of clean
examples while maximizing the embedding loss ER(w)
on the signature.
• Adapt-X for [47]. Since the embedded watermark (back-
door) is known,
the adversary can follow a similar
approach as above to steal the victim model and remove
the backdoor watermark with a few backdoor-patched but
correctly-labeled examples.
• Adapt-X for [2]. Similar to our Adapt-B for DEEPJUDGE,
the adversary copies the victim model then ﬁne-tunes it
on a small subset of clean and correctly-labeled ﬁnger-
print examples to circumvent ﬁngerprinting.
As the results in Table XI show, all three methods are
completely broken by the adaptive attacks. DEEPJUDGE is the
only method that can survive these attacks and was partially
compromised but not fully broken (the ﬁnal judgments are still
correct, as shown in the ‘Copy?’ column of Table VII). This
implies that a single metric of watermarking or ﬁngerprinting
is not sufﬁcient enough to combat adaptive attacks. By con-
trast, a testing framework with comprehensive testing metrics
and test case generation methods may have the required
ﬂexibility to address this challenge. For example, Adv-Train
may break the black-box testing of DEEPJUDGE but cannot
break the white-box testing (see Section VI-B). Moreover,
DEEPJUDGE can quickly recover its performance by switching
to a new set of seeds (see Fig. 9).
H. How Different Levels of Finetuning, Pruning and Transfer
Learning Affect DEEPJUDGE?
There is a spectrum of building a new model with access to
a victim model, from different ways of ﬁnetuning to transfer
learning. Different levels of modiﬁcations to the victim model
would accordingly inﬂuence the testing of DEEPJUDGE in
different ways. Intuitively, a larger modiﬁcation would lead
to more dissimilarity between the victim and suspect models
and a larger metric distance.
Here, we test different proportions of training samples
and learning rates used for ﬁnetuning, proportions of pruned
weights (pruning ratios) for pruning, and proportions of sam-
ples used for transfer learning (w.r.t. the setting described in
Section VI-B2). Fig. 16 shows the metrics’ values at different
levels of ﬁnetuning, pruning and transfer learning. At a high
level, black-box metrics (i.e., RobD and JSD) have higher nor-
malized distances than white-box metrics (i.e., NOD and NAD)
on average. This implies that the model’s decision boundary
is more sensitive to almost all levels of modiﬁcations. For
ﬁnetuning,
the two black-box metrics (yellow and orange
bars) increase signiﬁcantly with the amount of ﬁnetuning
samples or ampliﬁed learning rate, whereas the two white-
box metrics are relatively stable. Note that ‘4x’ (4 times the
default learning rate) causes a signiﬁcant drop (∼ 20%) in the
model accuracy. For pruning, all metrics including the white-
box metrics increase with the amount of pruned weights at a
much higher rate than ﬁnetuning with different sample sizes.
This indicates that pruning has more impact on the model
than ﬁnetuning and will greatly distort the model’s internal
activations (measured by the two white-box metrics NOD and
NAD). Transfer learning has much higher metric values (only
white-box metrics are applicable here) than ﬁnetuning. This
is because the victim model’s functionality has been greatly
altered by transferring to a new data distribution. However, it
seems that the modiﬁcation caused by transfer learning does
not accumulate with more samples, resulting in similar metric
values even with 40% more samples.
In this work, we follow the principle that any derivations
from the victim model other than independent training should
be treated as having a certain level of copying. However, it
can be hard to judge what degree of similarity (or level of
modiﬁcation) should be considered as “real copying” in real-
world scenarios. In DEEPJUDGE, we introduced a good range
of testing metrics, hoping to provide more comprehensive
evidence for making the ﬁnal judgement. Moreover, the ﬁnal
judgement mechanism of DEEPJUDGE (Section IV-D) can be
ﬂexibly adjusted to suit different application needs.
I. Additional Figures
Fig. 12: Existing watermarking methods [47] and [40] failed
to verify the ownership of the extracted models by several
extraction attacks.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
840
TABLE XI: Performance of existing watermarking and ﬁngerprinting baselines on CIFAR-10 dataset against adaptive attacks:
1) Adapt-X, adaptive attack designed speciﬁcally against the defense method; 2) Adv-Train, blind adversarial training; and 3)
VTL, vanilla transfer learning. The broken metrics (close to the negatives) are highlighted in red. Adapt-X breaks all three
metrics, while Adv-Train breaks the adversarial-examples-based ﬁngerprinting.
Black-box Watermarking [47]
ACC
82.9%
TSA
White-box Watermarking [40]
ACC
83.8%
Black-box Fingerprinting [2]
ACC
84.8%
MR
Model Type
Victim Model
Positive
suspect
models
Negative
models
Adapt-X
Adv-Train
VTL
Neg-1
Neg-2
81.8±0.8%
73.8±1.6%
92.2±1.3%
84.2±0.6%
84.9±0.5%
100.0%
0.01±0.01%
5.5±3.2%
0.05±0.04%
0.03±0.03%
×
71.2±2.6%
73.5±1.7%
91.7±1.6%
84.2±0.6%
84.9±0.5%
BER
3.13%
46.3±3.3%
4.0±0.8%
6.1±1.2%
50.3±4.1%
50.6±4.3%
81.9±0.2%
74.5±2.3%
93.3±1.7%
84.2±0.6%
84.9±0.5%
100.0%
1.2±0.8%
4.2±1.5%
2.2±1.4%
3.1±1.2%
×
Fig. 13: Similarity evaluation between the victim and suspect models on MNIST (left 3 columns) and ImageNet (right 3
columns). We use the orange line for positive suspect models and the blue line for negatives.
Fig. 14: The RobD/JSD scores between the CIFAR-10 (ﬁrst
row) and SpeechCommands (second row) victim models and
their extracted copies by model extraction attacks.
Fig. 16: DEEPJUDGE metrics computed at different levels of
model modiﬁcations on CIFAR-10. ‘2x’ means 2 times the
default learning rate.
Fig. 15: Selected test seeds with the highest or lowest certainty
scores. The ﬁrst row belongs to ‘automobile’ and the second
row belongs to ‘horse’.
Fig. 17: Example test cases generated in black-box and white-
box testings. Note that the white-box test cases are not regular
images and are unlabeled.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
841