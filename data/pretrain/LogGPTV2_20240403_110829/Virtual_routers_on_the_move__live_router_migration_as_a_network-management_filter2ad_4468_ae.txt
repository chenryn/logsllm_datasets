the packet rate gets, the less additional delay the migration
processes add to the packet forwarding. This explains why
when the packet rate is 25k packets/s, the delay increase
caused by migration becomes negligible. This also explains
why migration does not cause any packet drops in the ex-
periments. Finally, our experiments indicate that the link
migration does not aﬀect forwarding delay.
6.3.3 Reserved migration bandwidth is important
)
%
(
e
s
a
e
r
c
n
i
y
a
e
D
l
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0
 500
 600
 700
 800
 900
Data traffic rate (Mbps)
Figure 9: Delay increase of the data traﬃc, due to
bandwidth contention with migration traﬃc
In 6.3.1 and 6.3.2, migration traﬃc is given its own link
(i.e., has separate bandwidth). Here we study the impor-
tance of this requirement and the performance implications
for data traﬃc if it is not met.
We use the dumbbell testbed in this experiment, where
migration traﬃc and data traﬃc share the same bottleneck
link. We load the ospfd of a virtual router with 250k routes.
We start the data traﬃc rate from 500 Mbps, and gradually
increase it to 900 Mbps. Because OpenVZ uses TCP (scp)
for memory copy, the migration traﬃc only receives the left-
over bandwidth of the UDP data traﬃc. As the available
bandwidth decreases to below 300 Mbps, the migration time
increases, which translates into a longer control-plane down-
time for the virtual router.
Figure 9 compares the delay increase of the data traﬃc
at diﬀerent rates. Both the average delay and the delay
jitter increase dramatically as the bandwidth contention be-
comes severe. Table 3 compares the packet loss rates of the
data traﬃc at diﬀerent rates, with and without migration
240traﬃc. Not surprisingly, bandwidth contention (i.e., data
traﬃc rate ≥ 700 Mbps) causes data packet loss. The above
results indicate that in order to minimize the control-plane
downtime of the virtual router, and to eliminate the per-
formance impact to data traﬃc, operators should provide
separate bandwidth for the migration traﬃc.
6.4 Control Plane Impact
In this subsection, we investigate the control plane dynam-
ics introduced by router migration, especially how migration
aﬀects the protocol adjacencies. We assume a backbone net-
work running MPLS, in which its edge routers run OSPF
and BGP, while its core routers run only OSPF. Our results
show that, with default timers, protocol adjacencies of both
OSPF and BGP are kept intact, and at most one OSPF LSA
retransmission is needed in the worst case.
6.4.1 Core Router Migration
We conﬁgure virtual routers VR1, VR6, VR8 and VR10
on the Abilene testbed (Figure 7) as edge routers, and the
remaining virtual routers as core routers. By migrating VR5
from physical node Chicago-1 to Chicago-2, we observe the
impact of migrating a core router on OSPF dynamics.
No events during migration: We ﬁrst look at the case in
which there are no network events during the migration. Our
experiment results show that the control-plane downtime of
VR5 is between 0.924 and 1.008 seconds, with an average of
0.972 seconds over 10 runs.
We start with the default OSPF timers of Cisco routers:
hello-interval of 10 seconds and dead-interval of 40 seconds.
We then reduce the hello-interval to 5, 2, and 1 second in
subsequent runs, while keeping the dead-interval equal to
four times the hello-interval. We ﬁnd that the OSPF adja-
cencies between the migrating VR5 and its neighbors (VR4
and VR6) stay up in all cases. Even in the most restrictive
1-second hello-interval case, at most one OSPF hello mes-
sage is lost and VR5 comes back up on Chicago-2 before its
neighbors’ dead timers expire.
Events happen during migration: We then investigate
the case in which there are events during the migration and
the migrating router VR5 misses the LSAs triggered by the
events. We trigger new LSAs by ﬂapping the link between
VR2 and VR3. We observe that VR5 misses an LSA when
the LSA is generated during VR5’s 1-second downtime. In
such a case, VR5 gets a retransmission of the missing LSA 5
seconds later, which is the default LSA retransmit-interval.
We then reduce the LSA retransmit-interval from 5 sec-
onds to 1 second, in order to reduce the time that VR5 may
have a stale view of the network. This change brings down
the maximum interval between the occurrence of a link ﬂap
and VR5’s reception of the resulting LSA to 2 seconds (i.e.,
the 1 second control plane downtime plus the 1 second LSA
retransmit-interval ).
6.4.2 Edge Router Migration
Here we conﬁgure VR5 as the ﬁfth edge router in the
network that runs BGP in addition to OSPF. VR5 receives
a full Internet BGP routing table with 255k routes (obtained
from RouteViewson Dec 12, 2007) from an eBGP peer that
is not included in Figure 7, and it forms an iBGP full mesh
with the other four edge routers.
With the addition of a full BGP table, the memory dump
ﬁle size grows from 3.2 MB to 76.0 MB. As a result, it takes
longer to suspend/dump the virtual router, copy over its
dump ﬁle, and resume it. The average downtime of the con-
trol plane during migration increases to between 3.484 and
3.594 seconds, with an average of 3.560 seconds over 10 runs.
We observe that all of VR5’s BGP sessions stay intact dur-
ing its migration. The minimal integer hello-interval VR5
can support without breaking its OSPF adjacencies during
migration is 2 seconds (with dead-interval set to 8 seconds).
In practice, ISPs are unlikely to set the timers much lower
than the default values, in order to shield themselves from
faulty links or equipment.
7. MIGRATION SCHEDULING
This paper primarily discusses the question of migration
mechanisms (“how to migrate”) for VROOM. Another im-
portant question is the migration scheduling (“where to mi-
grate”). Here we brieﬂy discuss the constraints that need
to be considered when scheduling migration and several op-
timization problems that are part of our ongoing work on
VROOM migration scheduling.
When deciding where to migrate a virtual router, sev-
eral physical constraints need to be taken into considera-
tion. First of all, an “eligible” destination physical router
for migration must use a software platform compatible with
the original physical router, and have similar (or greater)
capabilities (such as the number of access control lists sup-
ported). In addition, the destination physical router must
have suﬃcient resources available, including processing power
(whether the physical router is already hosting the max-
imum number of virtual routers it can support) and link
capacity (whether the links connected to the physical router
have enough unused bandwidth to handle the migrating vir-
tual router’s traﬃc load). Furthermore, the redundancy re-
quirement of the virtual router also needs to be considered—
today a router is usually connected to two diﬀerent routers
(one as primary and the other as backup) for redundancy.
If the primary and backup are migrated to the same node,
physical redundancy will be lost.
Fortunately, ISPs typically leave enough “head room” in
link capacities to absorb increased traﬃc volume. Addition-
ally, most ISPs use routers from one or two vendors, with
a small number of models, which leaves a large number of
eligible physical routers to be chosen for the migration.
Given a physical router that requires maintenance, the
question of where to migrate the virtual routers it currently
hosts can be formulated as an optimization problem, subject
to all the above constraints. Depending on the preference
of the operator, diﬀerent objectives can be used to pick the
best destination router, such as minimizing the overall CPU
load of the physical router, minimizing the maximum load
of physical links in the network, minimizing the stretch (i.e.,
latency increase) of virtual links introduced by the migra-
tion, or maximizing the reliability of the network (e.g., the
ability to survive the failure of any physical node or link).
However, ﬁnding optimal solutions to these problems may
be computationally intractable. Fortunately, simple local-
search algorithms should perform reasonably well, since the
number of physical routers to consider is limited (e.g., to
hundreds or small thousands, even for large ISPs) and ﬁnd-
ing a “good” solution (rather than an optimal one) is accept-
able in practice.
Besides migration scheduling for planned maintenance, we
are also working on the scheduling problems of power sav-
241ings and traﬃc engineering. In the case of power savings,
we take the power prices in diﬀerent geographic locations
into account and try to minimize power consumption with
a certain migration granularity (e.g., once every hour, ac-
cording to the hourly traﬃc matrices). In the case of traﬃc
engineering, we migrate virtual routers to shift load away
from congested physical links.
8. CONCLUSIONS
VROOM is a new network-management primitive that
supports live migration of virtual routers from one physical
router to another. To minimize disruptions, VROOM allows
the migrated control plane to clone the data-plane state at
the new location while continuing to update the state at the
old location. VROOM temporarily forwards packets using
both data planes to support asynchronous migration of the
links. These designs are readily applicable to commercial
router platforms. Experiments with our prototype system
demonstrate that VROOM does not disrupt the data plane
and only brieﬂy freezes the control plane.
In the unlikely
scenario that a control-plane event occurs during the freeze,
the eﬀects are largely hidden by existing mechanisms for
retransmitting routing-protocol messages.
Our research on VROOM raises several broader questions
about the design of future routers and the relationship with
the underlying transport network. Recent innovations in
transport networks support rapid set-up and tear-down of
links, enabling the network topology to change underneath
the IP routers. Dynamic topologies coupled with VROOM’s
migration of the control plane and cloning of the data plane
make the router an increasingly ephemeral concept, not tied
to a particular location or piece of hardware. Future work
on router hypervisors could take this idea one step further.
Just as today’s commercial routers have a clear separation
between the control and data planes, future routers could
decouple the control-plane software from the control-plane
state (e.g., routing information bases). Such a “control-plane
hypervisor” would make it easier to upgrade router software
and for virtual routers to migrate between physical routers
that run diﬀerent code bases.
9. REFERENCES
[1] The Internet2 Network. http://www.internet2.edu/.
[2] T. Aﬀerton, R. Doverspike, C. Kalmanek, and K. K.
Ramakrishnan. Packet-aware transport for metro networks.
IEEE Communication Magazine, March 2004.
[3] M. Agrawal, S. Bailey, A. Greenberg, J. Pastor, P. Sebos,
S. Seshan, J. van der Merwe, and J. Yates. RouterFarm:
Towards a dynamic, manageable network edge. In Proc.
ACM SIGCOMM Workshop on Internet Network
Management (INM), September 2006.
[4] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris,
A. Ho, R. Neugebar, I. Pratt, and A. Warﬁeld. Xen and the
Art of Virtualization. In Proc. SOSP, October 2003.
[5] O. Bonaventure, C. Filsﬁls, and P. Francois. Achieving
sub-50 milliseconds recovery upon BGP peering link
failures. IEEE/ACM Trans. Networking, October 2007.
[6] S. Bryant and P. Pate. Pseudo wire emulation edge-to-edge
(PWE3) architecture. RFC 3985, March 2005.
[7] J. Chabarek, J. Sommers, P. Barford, C. Estan, D. Tsiang,
and S. Wright. Power awareness in network design and
routing. In Proc. IEEE INFOCOM, 2008.
[8] E. Chen, R. Fernando, J. Scudder, and Y. Rekhter.
Graceful Restart Mechanism for BGP. RFC 4724, January
2007.
[9] Ciena CoreDirector Switch. http://www.ciena.com.
[10] MPLS VPN Carrier Supporting Carrier.
http://www.cisco.com/en/US/docs/ios/12_0st/12_0st14/
feature/guide/csc.html.
[11] Cisco Logical Routers.
http://www.cisco.com/en/US/docs/ios_xr_sw/iosxr_r3.
2/interfaces/command/reference/hr32lr.html.
[12] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul,
C. Limpach, I. Pratt, and A. Warﬁeld. Live Migration of
Virtual Machines. In Proc. NSDI, May 2005.
[13] B. Cully, G. Lefebvre, D. Meyer, M. Feeley, N. Hutchinson,
and A. Warﬁeld. Remus: High availability via asynchronous
virtual machine replication. In Proc. NSDI, April 2008.
[14] D-ITG. http://www.grid.unina.it/software/ITG/.
[15] Emulab. http://www.emulab.net.
[16] N. Feamster, L. Gao, and J. Rexford. How to lease the
Internet in your spare time. ACM SIGCOMM Computer
Communications Review, Jan 2007.
[17] P. Francois, M. Shand, and O. Bonaventure.
Disruption-free topology reconﬁguration in OSPF networks.
In Proc. IEEE INFOCOM, May 2007.
[18] M. Gupta and S. Singh. Greening of the Internet. In Proc.
ACM SIGCOMM, August 2003.
[19] G. Iannaccone, C.-N. Chuah, S. Bhattacharyya, and
C. Diot. Feasibility of IP restoration in a tier-1 backbone.
IEEE Network Magazine, Mar 2004.
[20] Juniper Logical Routers.
http://www.juniper.net/techpubs/software/junos/
junos85/feature-guide-85/id-11139212.html.
[21] Z. Kerravala. Conﬁguration Management Delivers Business
Resiliency. The Yankee Group, November 2002.
[22] M. McNett, D. Gupta, A. Vahdat, and G. M. Voelker.
Usher: An extensible framework for managing clusters of
virtual machines. In Proc. USENIX LISA Conference,
November 2007.
[23] NetFPGA. http://yuba.stanford.edu/NetFPGA/.
[24] OpenVZ. http://openvz.org.
[25] Average retail price of electricity. http://www.eia.doe.
gov/cneaf/electricity/epm/table5_6_a.html.
[26] Quagga Routing Suite. http://www.quagga.net.
[27] A. Rostami and E. Sargent. An optical integrated system
for implementation of NxM optical cross-connect, beam
splitter, mux/demux and combiner. IJCSNS International
Journal of Computer Science and Network Security, July
2006.
[28] K. Roth, F. Goldstein, and J. Kleinman. Energy
Consumption by Oﬃce and Telecommunications
Equipment in commercial buildings Volume I: Energy
Consumption Baseline. National Technical Information
Service (NTIS), U.S. Department of Commerce, Springﬁeld,
VA 22161, NTIS Number: PB2002-101438, 2002.
[29] A. Shaikh, R. Dube, and A. Varma. Avoiding instability
during graceful shutdown of multiple OSPF routers.
IEEE/ACM Trans. Networking, 14(3):532–542, June 2006.
[30] R. Teixeira, A. Shaikh, T. Griﬃn, and J. Rexford.
Dynamics of hot-potato routing in IP networks. In Proc.
ACM SIGMETRICS, June 2004.
[31] J. van der Merwe and I. Leslie. Switchlets and dynamic
virtual ATM networks. In Proc. IFIP/IEEE International
Symposium on Integrated Network Management, May 1997.
[32] VINI. http://www.vini-veritas.net/.
[33] Y. Wang, J. van der Merwe, and J. Rexford. VROOM:
Virtual ROuters On the Move. In Proc. ACM SIGCOMM
Workshop on Hot Topics in Networking, Nov 2007.
[34] J. Wei, K. Ramakrishnan, R. Doverspike, and J. Pastor.
Convergence through packet-aware transport. Journal of
Optical Networking, 5(4), April 2006.
[35] T. Wood, P. Shenoy, A. Venkataramani, and M. Yousif.
Black-box and Gray-box Strategies for Virtual Machine
Migration. In Proc. NSDI, April 2007.
242