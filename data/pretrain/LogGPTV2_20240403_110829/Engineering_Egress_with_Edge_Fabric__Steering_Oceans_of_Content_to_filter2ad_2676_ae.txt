prefixes’ production traffic (minus the small amount detoured for
AltPath) to these routes. We left these overrides in place for 24
hours.
Figure 13 shows the achieved performance for prefixes redirected
because of better median latency, comparing the performance of
AltPath’s designated path (now carrying the majority of traffic)
versus the path preferred by our default BGP policy (now carrying
a small amount of traffic for AltPath measurements). For these
prefixes, 45% of prefixes achieved a median latency that was better
by at least 20ms (even with most traffic shifted to the path, which
potentially degrades performance), and 28% of prefixes improved
by at least 100ms. On the other hand, some overrides did not work
as expected. For example, 17% of prefixes experienced a median
latency at least 20ms worse than the default path, and 1% were
worse by at least 100ms. If the system dynamically adjusted routing
(as opposed to our one-time static overrides), these results signal
the potential for oscillations. The vast majority of these cases were
prefixes moved from peer routes to transit provider routes.
When overrides result in worse performance, we speculate that
this difference is likely a combination of two factors: (1) a path’s
performance is a function of the load placed onto it, and (2) a path’s
 0.001 0.01 0.1 1 0 0.2 0.4 0.6 0.8 1CCDF of interfacesFraction of time interface is overloaded 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 10 100 1000 10000CDF of samplesDuration (minutes)Detour durationGap between detours 0 0.05 0.1 0.15 0.2 0.25 0.3WedThuFriSatSunMonTueWedFraction of traffic detouredDay of weekTraffic at PoP with most detoursGlobal traffic 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1-400-300-200-100 0 100 200 300CDF of override samplesDifference in median latency w.r.t BGP primary path (ms)Engineering Egress with Edge Fabric
ACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
performance can change over time. As future work, we intend
to investigate these issues by conducting more granular shifts of
traffic, so that we can understand how the load that we place onto
particular paths impacts performance. In addition, this challenge
points to the need for a robust and reactive control system that
decides whether to shift traffic based not only on measurements for
individual prefixes, but also on whether other traffic is being shifted
to or from shared paths, historical information, and other factors
that can impact the shift. Towards this goal, we plan to further
increase the measurement sampling rate of AltPath, such that a
controller could have sufficient samples to evaluate performance
over a short timescale.
AltPath also identified 700 prefixes for which BGP’s 3rd pref-
erence out-performed its 2nd preference, and so we configured
Edge Fabric to use the 3rd preference when it needed to detour the
prefixes to prevent congestion. However, during our experiment,
fewer than 5% of these prefixes were detoured by Edge Fabric. We
compared the performance of the selected detour path to AltPath
measurements of BGP’s 2nd preference (Edge Fabric’s default de-
tour) during these detours and found that all but 2 of the prefixes
detoured achieved better performance. This result indicates that
AltPath can help limit the performance hit experienced by prefixes
that Edge Fabric detours due to congestion.
Does AltPath accurately capture end-to-end performance? We
conducted a controlled experiment using the PEERING testbed [24].
The testbed peers directly with Facebook across the public AMS-
IX fabric, and PEERING announcements also reach Facebook via
PEERING’s transit providers. We operated clients on IP addresses in
a PEERING prefix that we announced both directly to Facebook and
via transit. The clients continuously used HTTP to fetch 515KB and
30KB files from Facebook at a frequency that keeps the client’s CPU
and bandwidth utilization below 20%. Over time, we used Linux’s
traffic control framework to induce 40ms increased latency on
traffic arriving directly from Facebook, via transit, or both. We used
AltPath measurements to estimate the difference in performance
(i.e., the difference in induced latency) on the direct and transit
paths over 5-minute intervals. AltPath identified the difference
in induced latency to within 2.2ms of the induced difference in all
5-minute intervals during an 18 hour experiment (except in the
5-minute intervals during which induced latency was in flux), with
an average error of 0.6ms. This level of accuracy is sufficient to
allow Edge Fabric to compare performance across paths when
making detour decisions.
8 OPERATIONAL EXPERIENCE
Edge Fabric has evolved over years in response to growth at our
PoPs and from realizations derived from operational experience.
Our current design of Edge Fabric focuses on providing the flexi-
bility that we require to handle different egress routing scenarios,
but prefers well understood techniques and protocols over more
complex approaches whenever possible.
8.1 Evolution of Edge Control
As the size and number of our PoPs have continued to grow, we
strive for a simple, scalable design. These desired traits have re-
quired the continuous evaluation and improvement of different
pieces of our design and the careful consideration of how a design
decision will impact us in the long-term.
8.1.1 From stateful to stateless control. Our current implementa-
tion of Edge Fabric is stateless, meaning that it makes its alloca-
tion and override decisions from scratch in each 30 second cycle,
without being aware of its previous detours. This approach has a
number of advantages stemming from the simplicity of the design.
For instance, because the stateless allocator begins each cycle by
gathering all information it needs and projecting what utilization
will be if the controller does not intervene (§5.1.2), it is straightfor-
ward to test, restart, or failover the controller. The controller only
needs to calculate what traffic should be moved given its inputs and
projection, and can be tested by simply providing input scenarios
and checking its decision (§5.4).
In comparison, our previous stateful implementation required
recording the allocator’s state after each round both locally and
remotely. If the controller was restarted due to an upgrade or a
failure, it had to recover its previous decisions from a remote log,
increasing complexity. In addition, the stateful controller’s decision
process was more complicated, as the controller not only had to
decide which prefixes to shift when interfaces were overloaded but
also which existing overrides to remove given current interface
load. Because the stateful controller would not consider removing
overrides until interface utilization dropped below a threshold, it
could not backtrack while interface utilization was still increasing,
and its options for further detouring were restricted by the impact
of its previous actions. In some cases, the controller would shift a
prefix to a detour interface, only to have the detour interface be-
come overloaded in a subsequent cycle (due to the natural growth
of traffic), requiring that the prefix be shifted yet again. Maintain-
ing proper accounting of these states and decisions complicated
the implementation and testing, since the logic and tests had to
reason about and inject cascading decisions and states, ultimately
providing the motivation for the stateless redesign.
8.1.2 From host-based to edge-based routing. Our current imple-
mentation of Edge Fabric uses BGP to enact overrides (§5.3) and
only requires hosts to signal which flows require special treatment,
such as those used for alternate path measurements (§§ 6.1 and 6.2).
In comparison, previous implementations of Edge Fabric relied
on host-based routing to enact overrides. In this model, the con-
troller installed rules on every server in the PoP. These rules applied
markings to traffic destined towards different prefix. Corresponding
rules at PRs matched on these markings to determine which egress
interface a packet should traverse, bypassing standard IP routing.
During the host-based routing era, Edge Fabric evolved through
three different marking mechanisms in production: MPLS, DSCP,
and GRE. (Our MPLS based implementation went a step further
than what Edge Fabric does today by routing all egress traffic
based on decisions made at the hosts, effectively shifting all IP
routing decisions away from our PRs.) MPLS and DSCP were com-
patible with our early PoP architectures, in which we strived for
balanced peer and transit connectivity across PRs, and any traffic
that required detouring was sent to transit. Since traffic was subject
to ECMP, all PRs had identical rules for the same peers (e.g., DSCP
value 12 would detour traffic to transit X on all PRs). However, as
ACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
B. Schlinker et al.
our PoP architectures grew, we increasingly had imbalanced tran-
sit and peering capacity across PRs and wanted control of which
PR traffic egressed at, and so we switched to using GRE tunnels
between servers and PRs.
From our experience with these mechanisms, we have found that
it is non-trivial to obtain both host software and vendor software
that provide fast and robust support for these tunneling protocols.
Shifting the responsibility of routing traffic via a specific egress
interface to end-hosts makes debugging and auditing the network’s
behavior more difficult, as configuration must be inspected at mul-
tiple layers. Further, when interfaces fail or routes are withdrawn,
end-hosts must react quickly to avoid blackholing traffic, making
synchronization among end-hosts, PRs, and controllers critical.
In comparison, Edge Fabric does not require hosts to be aware
of network state and reduces synchronization complexities by in-
jecting overrides to PRs at the edge of the network. In addition,
this approach empowers PRs to invalidate controller overrides to
prevent blackholing of traffic, since an interface failure will cause
the PR to begin routing traffic to the next best route.
We believe our current edge-based approach provides us with
many of the advantages of host-based routing with minimal com-
plexity. While host-based routing gives hosts more control of how
packets are routed, the additional flexibility is not currently worth
the added complexity for the following reasons. First, our measure-
ments demonstrate that the majority of traffic can use paths selected
by BGP’s standard process, with Edge Fabric overriding BGP only
for a small portion of traffic in order to avoid causing congestion
(§7.2) or to improve performance (§7.3). Second, our approach to
overriding destination-based routing allows servers to tag select
flows (§6.1) for special treatment, and it allows controllers to de-
cide whether the routes for these flows should be overridden (§6.3).
Although this decoupling limits the degree of control at servers,
we believe that it results in a system that is simpler to design and
troubleshoot, and that it provides sufficient flexibility for our in-
tended use cases (§6.3). Third, because Facebook chooses to have
traffic ingress, egress, and be served at the same PoP, the choices for
routes are limited to decisions that can be signaled to and enacted
at the PRs. If another PoP starts to provide better performance for
a user network, the global traffic controller will redirect the traffic
to that PoP.
8.1.3 From global to per-PoP egress options. Previously, Face-
book propagated routes from external peers between PoPs in an
iBGP mesh, such that a user’s traffic could ingress via one PoP and
egress via another. The ability to route traffic across the WAN to
egress at a distant PoP can improve performance in some cases, but
we had to design mechanisms to keep it from causing oscillations.
For instance, some of the traffic on an overloaded egress interface
may have ingressed at a remote PoP. If Edge Fabric overrode the
route at the egress PoP to avoid congestion, the override update
propagated to the ingress PoP. If we had allowed the override to
cause the ingress PoP’s BGP process to stop preferring the egress
PoP, the projected demand for the overloaded egress interface could
have dropped, which could cause Edge Fabric to remove the over-
ride, which would cause the ingress PoP to again prefer the original
egress PoP, an oscillation. To prevent the override from causing the
ingress PoP to change its preferred egress PoP, the controller set
the BGP attributes of the override route to be equal to the original
route. However, manipulating BGP attributes at the controller ob-
fuscated route information, making it difficult to understand and
debug egress routing. We disabled route redistribution between
PoPs once we improved the accuracy and granularity of the global
load balancer’s mapping, and now traffic egresses at the same PoP
as it ingresses. Since the global load balancer controls where traffic
ingresses, it can spread traffic for a client network across PoPs that
it considers to be equivalent to make use of egress capacity at mul-
tiple PoPs. This allows Facebook to avoid using backbone capacity
to route traffic between PoPs and simplifies Edge Fabric’s design.
8.1.4 From balanced to imbalanced capacity. As our PoPs have
grown, we have had to extend our PoP design to handle corner-
cases. As we increased the size and scale of our PoPs, we began
to have more peers with imbalanced capacity (varying capacity to
the same peer across PRs), partly due to the incremental growth
of peering connectivity, but also because of inevitable failures of
peering links and long recovery times. These imbalances created a
problem because our ASWs use ECMP to distribute traffic evenly
among PRs with the best paths. Instead of extending Edge Fabric
to handle these imbalances, we could have chosen to use WCMP
(Weighted Equal-Cost Multi-Path routing) at our ASWs and PR.
However, we chose to instead extend Edge Fabric to handle
these capacity imbalances for a number of reasons. First, while
a number of routing and switching chipsets support WCMP, it
has far less adoption and support from our vendors than ECMP,
making it riskier to adopt. Even with vanilla-ECMP, we have ob-
served unequal traffic distributions (§5.4) and other erroneous or
unexpected behavior. Second, since the WCMP implementations
used by vendors are proprietary, we cannot predict how WCMP
will behave, making projecting utilization and engineering traffic
more difficult, and potentially increasing the complexity of Edge
Fabric. Finally, WCMP implementations operate on the router’s
entire routing table, can take minutes to converge after an event
(such as a link failure creating imbalanced capacity), and may not
be efficient enough to balance traffic properly [33]. In comparison,
Edge Fabric can identify the subset of prefixes with traffic and
inject routes to mitigate the failure within seconds.
8.2 The Challenge of IXPs
Internet exchange points (IXPs) have been a focus in academia
[2, 12], but they present challenges to a provider of Facebook’s
scale. In contrast to a dedicated private interconnect, a provider
cannot know how much capacity is available at a peer’s port, since
other networks at the IXP may be sending to it as well. This lim-
ited visibility makes it harder to simultaneously avoid congestion
and maximize interface utilization. Edge Fabric supports setting
limits on the rate of traffic sent to a peer via a public exchange to
avoid congesting a peer’s public exchange connection (§3). Some
exchanges report the total capacity of each peer’s connection to the
exchange fabric, but this information alone cannot be used to set a
limit since we need to account for traffic that the peer will receive
from other peers on the exchange. As a result, we set capacity con-
straints by contacting large public exchange peers and asking them
for estimated limits on the maximum rate of traffic that we can
send to them, as the peers have more insight into their interface