sult page for each site that we study. We make these scripts
and parsers available to the research community.
The second challenge we face is noise, or inconsistencies
in search results that are not due to personalization. Noise
can be caused by a variety of factors:
• Distributed
Large-scale
• Updates to the e-commerce site: E-commerce
services are known to update their inventory often,
as products sell out, become available, or prices are
changed. This means that the results for a query may
change even over short timescales.
infrastructure:
e-
commerce sites are often spread across geographically
diverse datacenters. Our tests show that diﬀerent
datacenters may return diﬀerent results for the same
search, due to inconsistencies across data centers.
• Unknown: As an e-commerce site is eﬀectively a
black-box (i.e., we do not know their internal archi-
tecture or algorithms), there may be other, unknown
sources of noise that we are unaware of.
To control for all sources of noise, we include a control in
each experiment that is conﬁgured identically to one other
treatment (i.e., we run one of the experimental treatments
twice). Doing so allows us to measure the noise as the level of
inconsistency between the control account and its twin; since
these two treatments are conﬁgured identically, any incon-
sistencies between them must be due to noise, not personal-
ization. Then, we can measure the level of inconsistency be-
tween the diﬀerent experimental treatments; if this is higher
than the baseline noise, the increased inconsistencies are due
to personalization. As a result, we cannot declare any par-
ticular inconsistency to be due to personalization (or noise),
but we can report the overall rate.
To see why this works, suppose we want to determine if
Firefox users receive diﬀerent prices than Safari users on a
given site. The na¨ıve experiment would be to send a pair of
identical, simultaneous searches—one with a Firefox User-
Agent and one with a Safari User-Agent—and then look for
inconsistencies. However, the site may be performing A/B
testing, and the diﬀerences may be due to requests given
diﬀerent A/B treatments.
Instead, we run an additional
control (e.g., a third request with a Firefox User-Agent),
and can measure the frequency of diﬀerences due to noise.
Of course, running a single query is insuﬃcient to accu-
rately measure noise and personalization. Instead, we run a
large set of searches on each site over multiple days and re-
port the aggregate level of noise and personalization across
all results.
3.2 Implementation
Except for when we use real users’ accounts (our Ama-
zon Mechanical Turk users in § 4), we collect data from
the various e-commerce sites using custom scripts for Phan-
tomJS [35]. We chose PhantomJS because it is a full imple-
mentation of the WebKit browser, meaning that it executes
JavaScript, manages cookies, etc. Thus, using PhantomJS
is signiﬁcantly more realistic than using custom code that
does not execute JavaScript, and it is more scalable than
automating a full web browser (e.g., Selenium [41]). Phan-
307Retailer
Best Buy
CDW
HomeDepot
JCPenney
Macy’s
Newegg
Oﬃce Depot
Sears
Staples
Walmart
Category
Electronics
Computers
Home-improvement
Clothes, housewares
Clothes, housewares
Computers
Site
http://bestbuy.com
http://cdw.com
http://homedepot.com
http://jcp.com
http://macys.com
http://newegg.com
http://officedepot.com Oﬃce supplies
http://sears.com
http://sears.com
http://walmart.com
Clothes, housewares
Oﬃce supplies
General retailer
Retailer
Cheaptickets
Expedia
Hotels.com
Orbitz
Priceline
Travelocity
Site
http://cheaptickets.com
http://expedia.com
http://hotels.com
http://orbitz.com
http://priceline.com
http://travelocity.com
Hotels Cars
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)(cid:8)
Table 2: The travel retailers we measured in this study.
Table 1: The general retailers we measured in this study.
tomJS has been proven to allow large-scale measurements
in previous work [17].
IP Address Geolocation.
In our experiments, all
PhantomJS instances issued queries from a /24 block of IP
addresses located in Boston. Controlling the source IP ad-
dress is critical since e-commerce sites may personalize re-
sults based on a user’s geolocation. We do not examine
geolocation-based personalization in this paper; instead, we
refer interested readers to work by Mikians et al. that thor-
oughly examines this topic [29, 30].
Browser Fingerprinting.
All of our PhantomJS
instances are conﬁgured identically, and present identi-
cal HTTP request headers and DOM properties (e.g.,
screen.width) to web servers. The only exceptions to this
rule are in cases where we explicitly test for the eﬀect of dif-
ferent User-Agent strings, or when an e-commerce site stores
state on the client (e.g., with a cookie). Although we do not
test for e-commerce personalization based on browser ﬁnger-
prints [34], any website attempting to do so would observe
identical ﬁngerprints from all of our PhantomJS instances
(barring the two previous exceptions). Thus, personalization
based on currently known browser ﬁngerprinting techniques
is unlikely to impact our results.
3.3 E-commerce Sites
We focus on two classes of e-commerce web sites: general
e-commerce retailers (e.g., Best Buy) and travel retailers
(e.g., Expedia). We choose to include travel retailers be-
cause there is anecdotal evidence of price steering among
such sites [28]. Of course, our methodology can be applied
to other categories of e-commerce sites as well.
General Retailers. We select 10 of the largest e-commerce
retailers, according to the Top500 e-commerce database [43],
for our study, shown in Table 1. We exclude Amazon, as
Amazon hosts a large number of diﬀerent merchants, making
it diﬃcult to measure Amazon itself. We also exclude sites
like apple.com that only sell their own brand.
Travel Retailers. We select six of the most popular web-
based travel retailers [44] to study, shown in Table 2. For
these retailers, we focus on searches for hotels and rental
cars. We do not include airline tickets, as airline ticket pric-
ing is done transparently through a set of Global Distribu-
tion Systems (GDSes) [6]. Furthermore, a recent study by
Vissers et al. looked for, but was unable to ﬁnd, evidence of
price discrimination on the websites of 25 major airlines [45].
applied helps to avoid diﬀerences in pricing that are due to
the location of the business and/or the customer.
3.4 Searches
We select 20 searches to send to each target e-commerce
site; it is the results of these searches that we use to look for
personalization. We select the searches to cover a variety of
product types, and tailor the searches to the type of products
each retailer sells. For example, for JCPenney, our searches
include “pillows”, “sunglasses”, and “chairs”; for Newegg, our
searches include “ﬂash drives”, “LCD TVs”, and “phones”.
For travel web sites, we select 20 searches (location and
date range) that we send to each site when searching for
hotels or rental cars. We select 10 diﬀerent cities across
the globe (Miami, Honolulu, Las Vegas, London, Paris, Flo-
rence, Bangkok, Cairo, Cancun, and Montreal), and choose
date ranges that are both short (4-day stays/rentals) and
long (11-day stays/rentals).
4. REAL-WORLD PERSONALIZATION
We begin by addressing our ﬁrst question: how widespread
are price discrimination and steering on today’s e-commerce
web sites? To do so, we have a large set of real-world users
run our experimental searches and examine the results that
they receive.
4.1 Data Collection
To obtain a diverse set of users, we recruited from Ama-
zon’s Mechanical Turk (AMT) [3]. We posted three Human
Intelligence Tasks (HITs) to AMT, with each HIT focus-
ing on e-commerce, hotels, or rental cars. In the HIT, we
explained our study and oﬀered each user $1.00 to partici-
pate.2 Users were required to live in the United States, and
could only complete the HIT once.
Users who accepted the HIT were instructed to conﬁgure
their web browser to use a Proxy Auto-Conﬁg (PAC) ﬁle pro-
vided by us. The PAC ﬁle routes all traﬃc to the sites under
study to an HTTP proxy controlled by us. Then, users were
directed to visit a web page containing JavaScript that per-
formed our set of searches in an iframe. After each search,
the Javascript grabs the HTML in the iframe and uploads it
to our server, allowing us to view the results of the search.
By having the user run the searches within their browser,
any cookies that the user’s browser had previously been as-
signed would automatically be forwarded in our searches.
This allows us to examine the results that the user would
have received. We waited 15 seconds between each search,
and the overall experiment took ≈45 minutes to complete
(between ﬁve and 10 sites, each with 20 searches).
In all cases, the prices of products returned in search re-
sults are in US dollars, are pre-tax, and do not include ship-
ping fees. Examining prices before taxes and shipping are
2This study was conducted under Northeastern University
IRB protocol #13-04-12; all personally identiﬁable informa-
tion was removed from our collected data.
308Figure 1: Previous usage (i.e., having an account and making a purchase) of diﬀerent e-commerce sites by our AMT users.
The HTTP proxy serves two important functions. First,
it allows us to quantify the baseline amount of noise in
search results. Whenever the proxy observes a search re-
quest, it ﬁres oﬀ two identical searches using PhantomJS
(with no cookies) and saves the resulting pages. The results
from PhantomJS serve as a comparison and a control result.
As outlined in § 3.1, we compare the results served to the
comparison and control to determine the underlying level
of noise in the search results. We also compare the results
served to the comparison and the real user; any diﬀerences
between the real user and the comparison above the level
observed between the comparison and the control can be
attributed to personalization.
Second, the proxy reduces the amount of noise by sending
the experimental, comparison, and control searches to the
web site at the same time and from the same IP address. As
stated in § 3.2, sending all queries from the same IP address
controls for personalization due to geolocation, which we
are speciﬁcally not studying in this paper. Furthermore,
we hard-coded a DNS mapping for each of the sites on the
proxy to avoid discrepancies that might come from round-
robin DNS sending requests to diﬀerent data centers.
In total, we recruited 100 AMT users in each of our retail,
hotel, and car experiments. In each of the experiments, the
participants ﬁrst answered a brief survey about whether they
had an account and/or had purchased something from each
site. We present the results of this survey in Figure 1. We
observe that many of our users have accounts and a purchase
history on a large number of the sites we study.3
4.2 Price Steering
We begin by looking for price steering, or personalizing
search results to place more- or less-expensive products at
the top of the list. We do not examine rental car results
for price steering because travel sites tend to present these
results in a deterministically ordered grid of car types (e.g.,
economy, SUV) and car companies (with the least expensive
car in the upper left). This arrangement prevents travel sites
from personalizing the order of rental cars.
To measure price steering, we use three metrics:
Jaccard Index. To measure the overlap between two dif-
ferent sets of results, we use Jaccard index, which is the size
of the intersection over the size of the union. A Jaccard In-
dex of 0 represents no overlap between the results, while 1
indicates they contain the same results (although not neces-
sarily in the same order).
3Note that the fraction of users having made purchases can
be higher than the fraction with an account, as many sites
allow purchases as a “guest”.
Kendall’s τ . To measure reordering between two lists of
result, we use Kendall’s τ rank correlation coeﬃcient. This
metric is commonly used in the Information Retrieval (IR)
literature to compare ranked lists [23]. The metric ranges
between -1 and 1, with 1 representing the same order, 0
signifying no correlation, and -1 being inverse ordering.
then calculated as DCG(R) = g(r1) +(cid:80)k
nDGG. To measure how strongly the ordering of search
results is correlated with product prices, we use Normal-
ized Discounted Cumulative Gain (nDCG). nDCG is a met-
ric from the IR literature [20] for calculating how “close”
a given list of search results is to an ideal ordering of re-
sults. Each possible search result r is assigned a “gain” score
g(r). The DCG of a page of results R = [r1, r2, . . . , rk] is
i=2(g(ri)/ log2(i)).
Normalized DCG is simply DCG(R)/DCG(R(cid:48)), where R(cid:48) is
the list of search results with the highest gain scores, sorted
from greatest to least. Thus, nDCG of 1 means the ob-
served search results are the same as the ideal results, while
0 means no useful results were returned.
In our context, we use product prices as gain scores, and
construct R(cid:48) by aggregating all of the observed products
for a given query. For example, to create R(cid:48) for the query
“ladders” on Home Depot, we construct the union of the re-
sults for the AMT user, the comparison, and the control,
then sort the union from most- to least-expensive.
Intu-
itively, R(cid:48) is the most expensive possible ordering of prod-