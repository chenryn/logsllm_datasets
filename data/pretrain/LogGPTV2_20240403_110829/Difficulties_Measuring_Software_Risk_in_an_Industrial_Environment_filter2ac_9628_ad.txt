into subdomains such that one or more element of each sub- 
domain is selected  as a test case.  More formally, a test se- 
lection method A4  is subdomain-based  if for each program 
P  and specification S ,  there  is a nonempty multiset  (i.e.  a 
collection of objects that may contain duplicates) of subdo- 
mains S D M  ( P ,  S ) ,  such that M  requires that one or more 
test case be selected  from each  subdomain in SD,w ( P ,  S ) .  
In  practice,  most  subdomain-based strategies  used  require 
only one test case to be selected from each subdomain. We 
will therefore assume that that is the requirement. 
There has been a great deal of research that compares test 
case selection methods both analytically and empirically in- 
cluding [3, 4, 5, 9, 10, 17, 191. In many cases, the research 
restricted  attention to a particular type of  subdomain test- 
ing strategy known as partition testing in which the subdo- 
mains are disjoint.  Realistically,  few if any test case selec- 
tion methods used  in practice  divide the domain into these 
disjoint subsets, but this restriction does facilitate analysis. 
Examples  of  well-known  subdomain  testing  strategies 
include statement testing in which the goal  is to select a set 
of  test  cases  that  cause every  statement of  the program to 
be executed at least once.  Similarly, branch testing requires 
that test cases be selected that guarantee that every possible 
exit of  every  decision  statement has been  executed  at least 
once. 
An  underlying assumption  of  all  subdomain-based test 
selection  strategies  is that the elements of each  subdomain 
form a sort of equivalence  class in the sense that selecting 
any  element  of  the  subdomain will  produce  the  same  re- 
sult from the point of view of correctness. Realistically,  we 
know that this is not true unless the subdomains are all what 
Weyuker and  Ostrand called  revealing  [ 181 or Hamlet  and 
Taylor called  homogeneous  [9].  Essentially this says that 
either every  element of the subdomain fails, or none does 
and therefore it doesn’t matter  which element of the subdo- 
main  is selected  since they all behave the same way.  But in 
practice,  programs don’t satisfy  this  property for the  stan- 
dard  subdomain-based test  case  selection algorithms,  and 
therefore the actual test cases selected to satisfy one of these 
testing methods can give highly differing results depending 
on the particular test case selected from each subdomain. 
Nonetheless,  if  a  subdomain-based  testing  strategy  is 
used,  we  want  to be  able to compute  the risk of  the soft- 
ware under test. We therefore  assume that P  was tested us- 
ing a subdomain-based strategy that induced k  subdomains 
{SI, ...) sk}. We assume,  too, that test case ti was selected 
as representative  of  subdomain  si for some of  the subdo- 
mains and that T  subdomains were not tested: { s j , ,  ..., s j , }  
Let ti be a canonical representative  of subdomain si, and 
let C(P, S, Ad, t i )  be the consequence of failure associated 
with elements of subdomain si. Note that we are assuming 
here that there is either a uniform consequence of failure for 
all elements of a given subdomain, or that some meaningful 
value has been selected as the cost such as a maximal  value. 
We must  again  consider how  to define Pr(t) for a given 
test  case  for  subdomain-based  testing.  At  first  glance  it 
might seem like we need  only assign 1/k  to each  test case. 
The problem  is that  since a  given  test  case  will  generally 
cause many  different subdomains to be exercised, and that 
a given subdomain will be exercised  by  many  different  se- 
lected  test  cases,  we have  to be careful  when  defining  this 
distribution. 
When testing is complete, if test cases have been selected 
that  exercise  j  <  b  of  the subdomains,  then  we  will  use 
( k  - j ) / k  as  the probability that  has not been  tested.  For 
the reasons mentioned above, this does not imply that j test 
cases have actually been  selected and run, or that k  - j test 
cases  are required to exercise these remaining subdomains, 
but  again  we  are  looking at  an  upper bound  on additional 
required  test cases,  and therefore risk.  If  there is a known 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:16 UTC from IEEE Xplore.  Restrictions apply. 
uniform cost associated with each untested subdomain, then 
that should be used  for the consequence of the untested el- 
ements.  If there  is either no known cost associated  with an 
untested subdomain, or the cost is not uniform for elements 
in the subdomain, then the technique of using  some sort of 
maximal cost should be used. 
Returning to our running example, we now imagine that 
instead  of  being  individual  inputs,  each  element  ij  is  a 
subdomain.  There  are  a total  of  1000 subdomains.  As- 
sume that  a set  of  TI  test  cases  are  selected  and run, cov- 
ering  all  subdomains except  i l   and  i 4 .   Then  the  risk  is 
2/1000  x  (5000 .f  100) =  10.20, assuming the  costs  as- 
sociated with subdomains il and i 4  are known. If test cases 
had  been  selected  that  covered  all  subdomains except  for 
il, then the risk would be 1/1000 x  5000 = 5. If  test cases 
had been selected that covered all subdomains except for i 4 ,  
then the risk  would be 1/1000  x  100 = . I .   The risk would 
be the same if all  1000 subdomains had been  selected and 
only the test case associated  with i 4  had failed. If only sub- 
domain i l 0 0 0  fails, then the risk would be 106 
4.4  Comparison 
The results of these examples are summarized in Table 2. 
As can be seen, depending on the test case selection method 
used, the failure of a given  test case can  lead to a very dif- 
ferent picture of the software’s  reliability.  We see that us- 
ing probabilistic testing with a uniform distribution can give 
a very  distorted picture of the risk associated  with  using  a 
software system.  This distortion could  represent  either a 
significant over- or under-assessment  of risk.  Notice,  also, 
that if no cost of failure can be associated  with a particular 
input, and if a maximal value is used  for all untested inputs, 
then  the risk  can  look very  high.  This is  the  case  for the 
DST2 algorithm if  at  least  one  input that  is not  tested  (in 
this case i l o o o )  has a very high failure cost. 
Our recommendation  is, therefore,  that the risk compu- 
tation be done using either probabilistic testing with the op- 
erational profile, or the DST algorithm, weighted by  cost of 
failure as described  in [ 161.  The disadvantage of using the 
operational profile-based probabilistic testing method is that 
there is no guarantee that the test cases  with highest proba- 
bility of occurrence and highest consequence of failure will 
be selected.  In contrast to this, the non-adjusted  DST algo- 
rithm does assure that the most likely inputs to occur in the 
field  will  always  be  selected  for testing.  However,  inputs 
with low probability of occurrence but high consequence of 
failure will  never be  selected.  This was  the case  for input 
i l o o o  in our example.  The cost-adjusted test case  selection 
version of DST, however, addresses both of these concerns. 
It  does, however,  require that  costs of  failure be known  a 
priori in order to make the adjustments to the probabilities. 
5  Conclusions 
We defined a new notion of risk that depends on both the 
behavior of the software (whether or not it fails) and the de- 
gree to which it has been tested relative to a chosen test case 
selection  method.  As  the  amount of  testing increases,  the 
risk will typically go down since we will  normally  see rel- 
atively few required  test cases  failing, and  unexercised  test 
cases are treated  as if  they have failed. In addition, as fail- 
ures are encountered and faults are removed  from the soft- 
ware, the measured  risk will  also typically go down.  For 
each  test  case  that  fails, the consequence of  that  failure is 
assessed  and contributes to the computed value of the risk. 
Thus, high  impact failures increase  the risk more substan- 
tially than trivial failures. 
We have very pragmatic reasons for formally defining an 
easily computable notion of risk: we expect to regularly use 
this definition as a means of assessing the expected  loss as- 
sociated with a software system prior to release, once it has 
been  thoroughly tested.  By  making this definition  formal 
and readily  computable, management  should have  a clear 
indication of whether or not it is safe to release the software 
in its current condition, or whether  the expected  loss is too 
high to tolerate. 
Our definition  also allows us to distinguish to what  de- 
gree the software is deemed to be at risk because of poor be- 
havior, and the contribution to the risk computation from in- 
sufficient testing. This allows management to decide where 
additional effort needs to be placed.  We have recently  iden- 
tified a large production  project  for which  operational  pro- 
file data  is being collected.  They  have  already  made  pro- 
jections of expected  costs of  various  types of  failures.  We 
are expecting to apply our definition  of risk to this project 
to help it plan for release. 
The use  of our definition  requires that  a systematic test 
case selection  method  be adopted.  This, alone, should im- 
prove testing, and consequently software dependability, es- 
pecially  when  it is coupled with  a requirement  that  once a 
test case selection  method  has been  agreed  upon, anything 
less than complete compliance with that method will lead to 
an increased value for the risk. Of course, if a poor or unde- 
manding testing method has been chosen, then it is possible 
that  the computed risk  for the  software will  appear much 
lower than  it actually  is.  This underscores  the necessity  of 
doing comprehensive  testing. 
It  is essential  that  we make the assessment  of  software 
risk less dependent on subjective factors, such as a tester’s 
or system engineer’s perception  of risk based  largely on in- 
tuition or past experience with systems perceived  to be simi- 
lar, and more dependent on actual quantitative evidence col- 
lected  about  the current  system  under  consideration.  Our 
approach  to risk assessment  should move us in  that direc- 
tion. 
23 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:16 UTC from IEEE Xplore.  Restrictions apply. 
Table 2. Comparison of Testing Strategies - 100 Test Cases Selected 
References 
[ 11  A. Avritzer and E. J. Weyuker. The Automatic genera- 
tion of load test suites and the assessment of the result- 
ing  software,  IEEE  Trans. on Soffivare Engineering, 
Sept  1995, pp.705-7 16. 
[ 121  J. D. Musa.  Operational profiles in software reliability 
engineering. IEEESoftware, Vol  10, No 2, March  1993, 
pp. 14-32. 
[131  S.C. Ntafos ne  ofsoftwarefai~ures.  proceedings 
IASTED Software Engineering  Conference, Nov.  1997, 
pp 53-57. 
[21  B. Boehm. Software risk management.  In f'roceeclings 
ESEC, Warwick, U.K.,  pages  1-19,  Sept. 1989. 
[ 141  S.A. Sherer. Software Failure Risk Plenum Press, New 
York, 1992. 
[15]  M.Z.  Tsoukalas,  J.W.  Duran,  and  S.C. Ntafos.  On 
some  reliability  estimation  problems  in  random  and 
partition  testing.  IEEE  Transactions on Software En- 
gineering, Vol  19, No 7, July  1993, pp.687-697. 
[ 161  E. J. Weyuker. Using failure cost information for test- 
ing and  reliability  assessment,  ACM  Transactions on 
Sojiware  Engineering  and  Methodology, 5(2):87-98, 
April  1996. 
[ 171  E. J. Weyuker and B. Jeng. Analyzing partition testing 
strategies. IEEE Transactions on Software Engineering, 
17(7):703-711,  July  1991. 
[ 181  E.J.  Weyuker and T.J. Ostrand.  Theories  of program 
testing  and  the  application  of  revealing  subdomains. 
IEEE  Transactions on Software Engineering,  Vol  SE- 
6, No 3, May  1980, pp.236-245. 
[ 191  E.  J.  Weyuker,  S. N. Weiss,  and  D.  Hamlet.  Com- 
parison  of  program  testing strategies.  In  Proceedings 
Fourth Symposium  on Software  Testing, Analysis,  and 
Veri'cation,  pages  1-1  0. ACM Press, Oct. 1991. 
[3]  J. W. Duran and S. C. Ntafos. An evaluation of random 
testing.  IEEE  Transactions on Software  Engineering, 
SE-10(7):438444, July  1984. 
[4]  P.  G. Frankl  and  S.N. Weiss.  An  Experimental  Com- 
parison of the Effectiveness of Branch Testing and Data 
Flow Testing IEEE Transactions on Software Engineer- 
ing, 19(8):774-787, A u ~  1993. 
[SI  P.  G. Frankl and E. J. Weyuker. Provable improvements 
on branch testing.  IEEE Transactions on Software En- 
gineering, 19( 10):962-975, Oct.  1993. 
[6]  P.G. Frankl  and E.J. Weyuker.  Testing software to de- 
tect and reduce risk. J. Systems and Software, to appear. 
[7]  W.J.  Gutjahr.  Optimal  test  distributions for  software 
failure cost estimation. IEEE Transactions on Software 
Engineering, 21(3):219-228,  Mar.  1995. 
[8]  E.M. Hall.  Managing Software Systems Risk.  Addison 
Wesley Longman, New York,  1998. 
[9]  D. Hamlet and R. Taylor.  Partition testing does not  in- 
spire confidence. IEEE Transactions on Software Engi- 
neering, 16(12):1402- 141 1, Dec.  1990. 
[IO]  M.  Hutchins,  H.  Foster,  T.  Goradia,  and  T.  Os- 
trand.  Experiments  on  the  effectiveness  of  dataflow- 
and controlflow-based test adequacy criteria.  Proceed- 
ings of  the  16th International Conference on Software 
Engineering, May  1994, pp. 19 1-200. 
[ 1 I]  N.G. Leveson.  Safeware. System Safety and  Coniput- 
ers.  Addison Wesley Longman, New York,  1995. 
24 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:16 UTC from IEEE Xplore.  Restrictions apply.