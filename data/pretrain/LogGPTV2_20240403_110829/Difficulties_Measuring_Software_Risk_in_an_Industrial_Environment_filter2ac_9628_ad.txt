### Subdomain-Based Test Case Selection

A subdomain-based test selection method, denoted as \( A4 \), is defined such that for each program \( P \) and its specification \( S \), there exists a nonempty multiset of subdomains, \( SDM(P, S) \). This multiset requires the selection of one or more test cases from each subdomain. In practice, most subdomain-based strategies typically select only one test case per subdomain. For simplicity, we will assume this requirement.

### Research on Test Case Selection Methods

Extensive research has been conducted to compare various test case selection methods, both analytically and empirically. Key studies include [3, 4, 5, 9, 10, 17, 19]. Many of these studies have focused on a specific type of subdomain testing strategy known as partition testing, where the subdomains are disjoint. Although practical test case selection methods rarely divide the domain into disjoint subsets, this restriction simplifies the analysis.

### Examples of Subdomain Testing Strategies

- **Statement Testing**: The goal is to select a set of test cases that ensure every statement in the program is executed at least once.
- **Branch Testing**: This requires selecting test cases that guarantee every possible exit of every decision statement is executed at least once.

### Underlying Assumption of Subdomain-Based Testing

The fundamental assumption in subdomain-based testing is that the elements within each subdomain form an equivalence class. This means that selecting any element from the subdomain should produce the same result from the perspective of correctness. However, this assumption holds true only if the subdomains are "revealing" (as defined by Weyuker and Ostrand [18]) or "homogeneous" (as defined by Hamlet and Taylor [9]). Essentially, all elements in a revealing or homogeneous subdomain either fail or succeed together, making the choice of any element from the subdomain irrelevant. In reality, programs do not often satisfy this property, leading to significant variations in results depending on the specific test case selected from each subdomain.

### Risk Computation in Subdomain-Based Testing

If a subdomain-based testing strategy is used, it is essential to compute the risk associated with the software under test. Suppose \( P \) is tested using a subdomain-based strategy that induces \( k \) subdomains \(\{S_1, S_2, \ldots, S_k\}\). Let \( t_i \) be the test case selected to represent subdomain \( S_i \), and let \( T \) be the set of subdomains that were not tested: \(\{S_{j_1}, S_{j_2}, \ldots, S_{j_T}\}\).

Let \( C(P, S, A4, t_i) \) denote the consequence of failure associated with elements of subdomain \( S_i \). We assume a uniform consequence of failure for all elements in a given subdomain or a meaningful value, such as a maximal value, is selected.

To define \( Pr(t) \) for a given test case, it might initially seem sufficient to assign \( \frac{1}{k} \) to each test case. However, since a given test case can exercise multiple subdomains and a given subdomain can be exercised by multiple test cases, the distribution must be carefully defined.

When testing is complete, if \( j < k \) subdomains have been exercised, the probability that a subdomain has not been tested is \( \frac{k - j}{k} \). This does not imply that exactly \( j \) test cases have been run or that \( k - j \) additional test cases are required. Instead, it provides an upper bound on the additional required test cases and thus the risk.

If there is a known uniform cost associated with each untested subdomain, this cost should be used. If no known cost or a non-uniform cost is associated with an untested subdomain, a maximal cost should be used.

### Example

Consider a scenario with 1000 subdomains. Assume a set of \( T_1 \) test cases are selected and run, covering all subdomains except \( S_1 \) and \( S_4 \). The risk is then calculated as \( \frac{2}{1000} \times (5000 + 100) = 10.20 \), assuming the costs associated with subdomains \( S_1 \) and \( S_4 \) are known. If test cases covered all subdomains except \( S_1 \), the risk would be \( \frac{1}{1000} \times 5000 = 5 \). If test cases covered all subdomains except \( S_4 \), the risk would be \( \frac{1}{1000} \times 100 = 0.1 \). The risk would be the same if all 1000 subdomains were tested and only the test case associated with \( S_4 \) failed. If only subdomain \( S_{1000} \) fails, the risk would be \( 10^6 \).

### Comparison of Testing Strategies

The results of these examples are summarized in Table 2. Depending on the test case selection method used, the failure of a given test case can lead to different assessments of the software's reliability. Probabilistic testing with a uniform distribution can give a distorted picture of the risk. This distortion could over- or under-estimate the risk. If no failure cost is associated with a particular input and a maximal value is used for all untested inputs, the risk can appear very high, as seen in the DST2 algorithm if at least one untested input has a very high failure cost.

### Recommendations

We recommend computing the risk using either probabilistic testing with the operational profile or the DST algorithm, weighted by the cost of failure as described in [16]. The operational profile-based probabilistic testing method does not guarantee the selection of test cases with the highest probability and highest consequence of failure. In contrast, the non-adjusted DST algorithm ensures the selection of the most likely inputs but may miss inputs with low probability and high consequence of failure. The cost-adjusted version of DST addresses both concerns but requires a priori knowledge of failure costs.

### Conclusions

We have defined a new notion of risk that depends on the software's behavior and the extent of testing relative to the chosen test case selection method. As testing increases, the risk typically decreases, as fewer required test cases fail, and unexercised test cases are treated as failures. High-impact failures increase the risk more substantially than trivial failures.

This definition of risk is pragmatic and easily computable, allowing management to assess the expected loss associated with a software system before release. It also helps distinguish between the risk due to poor behavior and insufficient testing, enabling better decision-making.

### References

[1] A. Avritzer and E. J. Weyuker. The Automatic generation of load test suites and the assessment of the resulting software, IEEE Trans. on Software Engineering, Sept 1995, pp. 705-716.

[2] B. Boehm. Software risk management. In Proceedings ESEC, Warwick, U.K., pages 1-19, Sept. 1989.

[3] J. W. Duran and S. C. Ntafos. An evaluation of random testing. IEEE Transactions on Software Engineering, SE-10(7):438-444, July 1984.

[4] P. G. Frankl and S.N. Weiss. An Experimental Comparison of the Effectiveness of Branch Testing and Data Flow Testing. IEEE Transactions on Software Engineering, 19(8):774-787, Aug. 1993.

[5] P. G. Frankl and E. J. Weyuker. Provable improvements on branch testing. IEEE Transactions on Software Engineering, 19(10):962-975, Oct. 1993.

[6] P.G. Frankl and E.J. Weyuker. Testing software to detect and reduce risk. J. Systems and Software, to appear.

[7] W.J. Gutjahr. Optimal test distributions for software failure cost estimation. IEEE Transactions on Software Engineering, 21(3):219-228, Mar. 1995.

[8] E.M. Hall. Managing Software Systems Risk. Addison Wesley Longman, New York, 1998.

[9] D. Hamlet and R. Taylor. Partition testing does not inspire confidence. IEEE Transactions on Software Engineering, 16(12):1402-1411, Dec. 1990.

[10] M. Hutchins, H. Foster, T. Goradia, and T. Ostrand. Experiments on the effectiveness of dataflow- and controlflow-based test adequacy criteria. Proceedings of the 16th International Conference on Software Engineering, May 1994, pp. 191-200.

[11] N.G. Leveson. Safeware. System Safety and Computers. Addison Wesley Longman, New York, 1995.

[12] J.D. Musa. Operational profiles in software reliability engineering. IEEE Software, Vol 10, No 2, March 1993, pp. 14-32.

[13] S.C. Ntafos. Analysis of software failures. Proceedings IASTED Software Engineering Conference, Nov. 1997, pp 53-57.

[14] S.A. Sherer. Software Failure Risk. Plenum Press, New York, 1992.

[15] M.Z. Tsoukalas, J.W. Duran, and S.C. Ntafos. On some reliability estimation problems in random and partition testing. IEEE Transactions on Software Engineering, Vol 19, No 7, July 1993, pp. 687-697.

[16] E. J. Weyuker. Using failure cost information for testing and reliability assessment. ACM Transactions on Software Engineering and Methodology, 5(2):87-98, April 1996.

[17] E. J. Weyuker and B. Jeng. Analyzing partition testing strategies. IEEE Transactions on Software Engineering, 17(7):703-711, July 1991.

[18] E.J. Weyuker and T.J. Ostrand. Theories of program testing and the application of revealing subdomains. IEEE Transactions on Software Engineering, Vol SE-6, No 3, May 1980, pp. 236-245.

[19] E. J. Weyuker, S. N. Weiss, and D. Hamlet. Comparison of program testing strategies. In Proceedings Fourth Symposium on Software Testing, Analysis, and Verification, pages 1-10. ACM Press, Oct. 1991.