9
(cid:16)( ¯A
(cid:17)
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
¯A1)R1 + R2
=A′R1+R2
−1
2
,
Session D1:  Functional Encryption and ObfuscationCCS’17, October 30-November 3, 2017, Dallas, TX, USA791which is pseudorandom under LWE. Using this argument requires
that R be chosen from the LWE error distribution. Our implemen-
tation therefore relies on the assumption that LWE is hard even
when the error distribution uses Gaussian parameter r = 4.
Moreover, this LWE-based assumption also requires that ¯m is
large enough so we can use ¯m − n as the “security parameter” for
LWE. (Recall that ¯m is the number of columns in ¯A.) In our setting
of parameters, we in particular must set ¯m large enough so given ¯A
and A′ = ¯AR it will be hard to find R. Finding each column of R is an
instance of the small-integer-solution (SIS) problem in dimensions
n-by- ¯m over Zq, so we must set ¯m large enough to get the level of
security that we want. Setting the dimension to get SIS-security is
not entirely straightforward and typically involves consulting some
tables [1], but in the range that we consider (with λ ∈ [80, 256]) it
behaves very roughly as6
√
¯m ≥ (
(9)
With n ≈ 100 (for NFAs with 100 states) and log2 q ≈ 535, this
means that we must ensure ¯m ≥ (√
The large dimension m. We use the formula from [12, Appen-
dix C] to relate the lattice dimension m in the LWE instances to the
desired security level.7 Namely to get security level λ we need the
dimension m to be at least m ≥ log2(q/fresh-noise-magnitude) ·
(λ + 110)/7.2. The fresh noise magnitude does not have a crucial
impact on our parameters, so we can choose it rather large (e.g.,
Gaussian of parameter ≈ 27). Thus we get the constraint
λ + 2) ·(cid:112)n log2 q.
λ + 2) · √100 · 535 ≈ 2545.
m ≥ (log2 q − 7)(λ + 110)/7.2 .
(10)
With log2 q ≈ 535 and λ = 80, this yields m ≥ (535 − 7) · (80 +
110)/7.2 ≈ 213.8.
5.3 Putting it together
Given the desired program length L, the dimension of plaintext
matrices n (which depends on the number of states in the branch-
ing program), the parameter e (which depends on the hardware
platform), and the security parameter λ, our implementation tries to
find the smallest number k of co-prime factors that satisfies all the
, then
set m using Eqn. (10), compute w = nek and ¯m = m − w, and verify
that this value of ¯m satisfies Eqn. (9). Next we compute σx using
Eqn. (7), and finally check if the value of q satisfies the functionality
constraint from Eqn. (8). Some example parameter values that we
used in our tests can be found in Table 1.
constraints above. Trying k = 1, 2, 3, . . ., we set q =
i <k pe
i
L :
σx :
k :
log2 q :
¯m :
m :
5
218.0
6
133
1462
3352
8
218.3
10
219
2471
5621
10
218.4
12
261
2950
6730
12
218.6
15
322
3614
8339
14
218.7
18
382
4253
9923
17
218.8
22
458
4998
11928
20
218.9
26
542
5955
14145
Table 1: Parameters in our tests, security λ = 80, plaintext
dimension n = 105
6This yields root Hermite factors of δ ≈ 1.006 for λ = 80, δ ≈ 1.0044 for λ = 128,
and δ ≈ 1.0023 for λ = 256.
7That formula is somewhat out of vogue these days, and should really be replaced by
more refined analyses such as [3], but it still gives reasonable values.
6 EFFICIENT MATRIX ARITHMETIC
The majority of the obfuscation time is spent doing matrix arith-
metic, either modulo small integers or over single-precision integers
(or floating point numbers). We first discuss the modular matrix
arithmetic.
6.1 Matrix multiplication in Zt
As discussed in Section 4.3, we use a CRT representation of Zq,
where q is the product of small co-prime factors. The parameters
are typically chosen so that each factor has a bit-length at most 23
(or at most 60), the reason for this will be explained shortly.
So assume that we are working modulo a small number t of bit-
length at most 23 bits. The two main operations of concern are large
matrix multiplication and inversion over Zt , where the dimensions
of the matrices are measured in the thousands. For matrix inversion,
we assume that t is a prime power. Consider computing the product
C = AB, where A and B are large matrices over Zt .
Cache friendly memory access. To obtain cache friendly code, all
the matrices are organized into panels, which are matrices with
many rows but only 32 columns. We compute the ith panel of C
by computing ABi, where Bi is ith panel of B. If multiple cores are
available, we use them to parallelize the computation, as the panels
of C can be computed independently.
Next consider the computation of AP, where P is a single panel.
j Aj Pj, where each Aj is a panel of A and
each Pj is a 32 × 32 square sub-matrix of P. We thus reduced the
problem to that of computing
We can write AP =
Q ← Q + RS,
(11)
where Q and R are panels, and S is a 32 × 32 square matrix. The
matrix S is small and fits into the first-level cache on most machines
— that is why we chose a panel size of 32. While the panels Q and
R typically do not fit into the first-level cache, the data in each
panel is laid out in contiguous memory in row-major order. In the
implementation of Eqn. (11), we process the panels a few rows at
a time, so the data in each panel gets processed sequentially, and
we rely on hardware prefetch (which is common on modern high-
performance systems) to speed up the memory access. Indeed, the
computation of Eqn. (11) can be reduced to operations of the form
(12)
where u consists of a few rows of Q and v consists of a few rows
of R. While we may need to fetch u and v from a slower cache,
the hardware prefetcher should help a bit, and, more significantly,
these slower fetches are paired with a CPU-intensive computation
(involving S, which is in the first-level cache), so the resulting code
is fairly cache friendly.
u ← u + vS,
Fast modular arithmetic. The basic arithmetic operation in any
matrix multiplication algorithm is the computation of the form
x ← x + yz, where x and y are scalars. In our case, the scalars lie in
Zt . Unfortunately, modern CPUs do not provide very good direct
hardware support for arithmetic in Zt . However, by restricting
t to 23 bits, we can use the underlying floating point hardware
that is commonly available and typically very fast. Indeed, if we
have 23-bit numbers w and xi and yi, for i = 1, . . . , k, then we can
i xiyi exactly in floating point, provided k is not
compute w +
10
Session D1:  Functional Encryption and ObfuscationCCS’17, October 30-November 3, 2017, Dallas, TX, USA792too big: since standard (double precision) floating point can exactly
represent 53-bit integers, we can take k up to 253−23·2 = 27. If k is
larger than this, we can still use the fast floating point hardware,
interspersed with occasional “clean up” operations which convert
the accumulated floating point sum to an integer, reduce it mod t,
and then convert it back to floating point.
Using AVX instructions. By using this floating point implemen-
tation, we can also exploit the fact that modern x86 CPUs come
equipped with very fast SIMD instructions for quickly performing
several floating point operations concurrently. Our code is geared to
Intel’s AVX (and AVX2) instruction set, which allows us to process
floating points operations 4 at a time (the next-generation AVX512
instruction set will allow us to process 8 at a time).
can be organized as u ← u +
The core of our matrix arithmetic code is a routine that computes
Eqn. (12) in floating point using Intel’s AVX (and AVX2) instructions.
Suppose u and v are single rows. If the ith entry of v is vi and the ith
row of S is Si (for i = 1, . . . , 32), then the computation of Eqn. (12)
To carry this out, we load the vector u from memory into eight
AVX registers, into which we will accumulate the result, and then
store it back to memory. To accumulate the result, for i = 1, . . . , 32,
we do the following:
i vi Si .
• Use the AVX “broadcast” instruction to initialize an AVX
register r with 4 copies of vi.
• Load the values of Si four at a time into an AVX register,
multiply by the register r, and add the result into the corre-
sponding accumulator register. (For AVX2, we use a fused
multiply-add instruction, which saves the use of an instruc-
tion and a temporary register.)
That is the simplest implementation, but not necessarily the
fastest. We experimented with a number of different implementa-
tions. In our actual implementation, we process 2 or 3 rows of Q
and R at a time in Eqn. (11), so that u and v in Eqn. (12) consist
of either 2 or 3 rows. (The choice depends on whether we have a
fused multiply-add instruction, as without it, we run out of AVX
registers more quickly.) With this strategy, values loaded from S
into AVX registers can participate in several arithmetic operations
instead of just one — while loads from S are fast (it is in first-level
cache), they are still not as fast as direct register access.
6.2 Matrix inversion in Zt
Let A be a high-dimension square matrix over Zt . We perform
an “in place” Gaussian elimination, performing elementary row
operations on A until we get the identity matrix, but we store the
entries of the inverse in A itself. Our algorithm works when t is
a prime or a prime power.8 This is easy to do: when selecting a
pivot, instead of choosing any non-zero pivot, we always choose
an invertible pivot modulo t.
Just as for multiplication, we organize A into panels. In carrying
out Gaussian elimination, we start with the first panel, and we carry
out the algorithm just on this panel, ignoring all the rest. After this,
we perform a series of panel/square operations, as in Eqn. (11)
to perform the same elementary row operations on the remaining
panels of A that were performed on the first panel (if any rows in the
8For a composite t, it can fail even if A is invertible modulo t.
11
first panel were swapped, we first perform those same swaps of the
remaining panels before performing the panel/square operation). If
multiple cores are available, these panel/square operations can be
done in parallel. After we finish with the first panel, we move on
to the second panel, and so on, until we are done with the whole
matrix. We use the same floating point strategy for arithmetic in
Zt as we did above, exploiting AVX instructions, if available.
6.3 Integration into NTL
Our new matrix arithemtic has been integrated into NTL (see http:
//www.shoup.net/ntl/), which has an interface that supports matrix
arithmetic modulo small numbers p that “fit” into a machine word.
On 64-bit machines, the bit length of the modulus p may go up to
60 (or 62, with a special compilation flag).
For p up to 23 bits, the strategy outlined above is used. For larger
p, the code reverts to using scalar integer instructions (rather than
the AVX floating-point instructions), but still uses the same “cache
friendly” panel/square memory organization, and utilizing multiple
cores, if available.
Besides matrix multiplication and inverse, the same strategies
are used for general Gaussian elimination, and image and kernel
calculations.
6.4 Multi-dimensional Gaussian sampling
In Section 4.4, we sketched our basic strategy for sampling from
Gaussian distributions. As discussed in that section, to sample from
a multi-dimensional Gaussian distribution, we need to compute the
conditional mean and covariance as in Eqn. (3). It turns out that
this computation is very similar in structure to Gaussian elimina-
tion (over floating point numbers). As such, we were able to easily
re-purpose our AVX-enabled floating-point code for Gaussian elim-
ination, discussed above, to significantly improve the performance
of this computation. This resulted in a roughly 10× speedup over
a straightforward implementation (of the same algorithm) of the
mean and covariance computation.
6.5 Experimental results for matrix arithmetic
Our testing was done on a machine with Intel Xeon CPU, E5-2698
v3 @2.30GHz (which is a Haswell processor), featuring 32 cores
and 250GB of main memory. The compiler was GCC version 4.8.5,
and we used NTL version 10.3.0 and GMP version 6.0 (see http:
//gmplib.org).
We compared NTL’s new matrix multiplication and inverse code
to the current versions of FFLAS, (see http://linbox-team.github.io/
fflas-ffpack/) and FLINT (see http://www.flintlib.org/).
• FFLAS refers to the current version of FFLAS (version 2.2.2,
available at http://linbox-team.github.io/fflas-ffpack/). FFLAS
stands for “Finite Field Linear Algebra Subprograms”, and
provides an interface analogous to the well-known BLAS
interface for linear algebra over floating point numbers.
Roughly speaking, FFLAS works by reducing all linear alge-
braic operations over Zt to matrix multiplication over float-
ing point numbers, and then uses a BLAS implementation
for the latter. In our tests, we use the BLAS implementation
OpenBLAS (see http://www.openblas.net/), which is recom-
mended by the FFLAS authors. OpenBLAS itself is highly
Session D1:  Functional Encryption and ObfuscationCCS’17, October 30-November 3, 2017, Dallas, TX, USA793optimized for many different architectures. We configured
and built OpenBLAS so that it was optimized for our Haswell
architecture.
For small t, this floating point strategy has some similarities
to NTL’s strategy, although NTL implements everything
directly (in particular, NTL does not use BLAS and is not
so well optimized for anything other than AVX-enabled x86
CPUs).
For larger t, FFLAS uses a Chinese remaindering strategy,
while NTL does not (for larger, but still word-sized t, NTL
uses scalar integer arithmetic, as discussed above).
• FLINT refers to the current version of FLINT (version 2.5.2,
available at http://www.flintlib.org/). FLINT stands for “Fast
Library for Number Theory”. FLINT only uses scalar integer
arithmetic — it does not use any floating point. For matrix
multiplication, it uses Strassen’s recursive algorithm. This
gives slightly better asymptotic complexity, and more im-
portantly it yields much more cache-friendly code. Matrix
inversion is implemented by a reduction to matrix multipli-
cation.
We were able to compare both single-threaded and multi-threaded
performance of NTL’s and FFLAS’s multiplication routines, as de-
scribed in Table 2. FLINT’s multiplication does not exploit multiple
threads. Neither FFLAS’s nor FLINT’s inversion routine exploit mul-
tiple threads. We can see that NTL’s multiplication is a bit slower
than FFLAS’s, while NTL’s inversion is a bit faster. Both NTL and
FFLAS are several times faster than FLINT.
We also mention here that for matrix inversion, both FFLAS
and FLINT require that the modulus t be a prime number, but the
modulus in our application is a prime power rather han a prime.
NTL’s inverse routine directly supports prime-power moduli, with
no extra computational cost. In contrast, using FFLAS or FLINT
directly would require some type of Hensel lifting to go from prime
to prime-power, which would significantly increase the cost of the
inverse operation.
Table 3 gives some timing data for matrix multiplication and
inversion over Zt for different sized moduli t. Data for 20-bit and
60-bit t is presented. We compared NTL and FLINT, along with an