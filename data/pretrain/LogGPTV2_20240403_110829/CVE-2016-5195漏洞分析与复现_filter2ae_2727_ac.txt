            if (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
                return -EFAULT;
            BUG();
        }
        if (tsk) {
            if (ret & VM_FAULT_MAJOR)
                tsk->maj_flt++;
            else
                tsk->min_flt++;
        }
        if (ret & VM_FAULT_RETRY) {
            if (nonblocking)
                *nonblocking = 0;
            return -EBUSY;
        }
            /*
         * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when
         * necessary, even if maybe_mkwrite decided not to set pte_write. We
         * can thus safely do subsequent page lookups as if they were reads.
         * But only do so when looping for pte_write is futile: in some cases
         * userspace may also be wanting to write to the gotten user page,
         * which a read fault here might prevent (a readonly page might get
         * reCOWed by userspace write).
         */
        if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
            *flags &= ~FOLL_WRITE;
                    /*一旦设置VM_FAULT_WRITE位，表示COW的部分已经跳出来了(可能已经COW完成了)，因
                    此可以按照只读的情况处理后续(也就是说我们就算写入也可以按照读的情况处理，因为有COW相当于有一个新的副本，读写都在这个新副本进行)，所以就去除了我们的`FOOL_WRITE`位，这让我们可以成功绕过`follow_page_pte`函数的限制，得到一个只读的page
                    */
        return 0;
    }
    //handle_mm_fault做些检查，找到合适的vma，内部调用__handle_mm_fault函数
    int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
            unsigned int flags)
    {
        int ret;
        __set_current_state(TASK_RUNNING);
        count_vm_event(PGFAULT);
        mem_cgroup_count_vm_event(vma->vm_mm, PGFAULT);
        /* do counter updates before entering really critical section. */
        check_sync_rss_stat(current);
        /*
         * Enable the memcg OOM handling for faults triggered in user
         * space.  Kernel faults are handled more gracefully.
         */
        if (flags & FAULT_FLAG_USER)
            mem_cgroup_oom_enable();
        if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
                            flags & FAULT_FLAG_INSTRUCTION,
                            flags & FAULT_FLAG_REMOTE))
            return VM_FAULT_SIGSEGV;
        if (unlikely(is_vm_hugetlb_page(vma)))
            ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
        else
            ret = __handle_mm_fault(vma, address, flags);//这里
            if (flags & FAULT_FLAG_USER) {
            mem_cgroup_oom_disable();
                    /*
                     * The task may have entered a memcg OOM situation but
                     * if the allocation error was handled gracefully (no
                     * VM_FAULT_OOM), there is no need to kill anything.
                     * Just clean up the OOM state peacefully.
                     */
                    if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
                            mem_cgroup_oom_synchronize(false);
        }
        return ret;
    }
    //这个函数的核心是handle_pte_fault，
    static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
            unsigned int flags)
    {
        struct fault_env fe = {
            .vma = vma,
            .address = address,
            .flags = flags,
        };
        struct mm_struct *mm = vma->vm_mm;
        pgd_t *pgd;
        pud_t *pud;
        pgd = pgd_offset(mm, address);
        pud = pud_alloc(mm, pgd, address);
        if (!pud)
            return VM_FAULT_OOM;
        fe.pmd = pmd_alloc(mm, pud, address);
        if (!fe.pmd)
            return VM_FAULT_OOM;
        if (pmd_none(*fe.pmd) && transparent_hugepage_enabled(vma)) {
            int ret = create_huge_pmd(&fe);
            if (!(ret & VM_FAULT_FALLBACK))
                return ret;
        } else {
                    pmd_t orig_pmd = *fe.pmd;
            int ret;
            barrier();
            if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
                if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
                    return do_huge_pmd_numa_page(&fe, orig_pmd);
                if ((fe.flags & FAULT_FLAG_WRITE) &&
                        !pmd_write(orig_pmd)) {
                    ret = wp_huge_pmd(&fe, orig_pmd);
                    if (!(ret & VM_FAULT_FALLBACK))
                        return ret;
                } else {
                    huge_pmd_set_accessed(&fe, orig_pmd);
                    return 0;
                }
            }
        }
        return handle_pte_fault(&fe);
    }
    //这个函数是缺页处理的核心函数，负责各种缺页情况的处理，在《深入了解Linux内核》的第九章有详细讲解
    static int handle_pte_fault(struct fault_env *fe)
    {
        pte_t entry;
        if (unlikely(pmd_none(*fe->pmd))) {
            /*
             * Leave __pte_alloc() until later: because vm_ops->fault may
             * want to allocate huge page, and if we expose page table
             * for an instant, it will be difficult to retract from
             * concurrent faults and from rmap lookups.
             */
            fe->pte = NULL;//如果没有pmd表，就没有pte项
        } else {
            /* See comment in pte_alloc_one_map() */
            if (pmd_trans_unstable(fe->pmd) || pmd_devmap(*fe->pmd))
                return 0;
            /*
             * A regular pmd is established and it can't morph into a huge
             * pmd from under us anymore at this point because we hold the
             * mmap_sem read mode and khugepaged takes it in write mode.
             * So now it's safe to run pte_offset_map().
             */
            fe->pte = pte_offset_map(fe->pmd, fe->address);
            entry = *fe->pte;
            /*
             * some architectures can have larger ptes than wordsize,
             * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and
             * CONFIG_32BIT=y, so READ_ONCE or ACCESS_ONCE cannot guarantee
             * atomic accesses.  The code below just needs a consistent
             * view for the ifs and we later double check anyway with the
             * ptl lock held. So here a barrier will do.
             */
            barrier();
            if (pte_none(entry)) {
                pte_unmap(fe->pte);
                fe->pte = NULL;
            }
        }
        if (!fe->pte) {
            if (vma_is_anonymous(fe->vma))
                return do_anonymous_page(fe);//线性区没有映射磁盘文件，也就说这是个匿名映射
            else
                return do_fault(fe);//还没有这个页表项，第一次请求调页会走到这里
        }
            if (!pte_present(entry))
            return do_swap_page(fe, entry);//进程已经访问过这个页，但是其内容被临时保存在磁盘上。内核能够识别这///种情况，这是因为相应表项没被填充为0，但是Present和Dirty标志被清0.
        if (pte_protnone(entry) && vma_is_accessible(fe->vma))
            return do_numa_page(fe, entry);
        fe->ptl = pte_lockptr(fe->vma->vm_mm, fe->pmd);
        spin_lock(fe->ptl);
        if (unlikely(!pte_same(*fe->pte, entry)))
            goto unlock;
        if (fe->flags & FAULT_FLAG_WRITE) {
            if (!pte_write(entry))
                return do_wp_page(fe, entry);//如果要写这个页且页本身不可写，则写时复制被激活
            entry = pte_mkdirty(entry);
        }
        entry = pte_mkyoung(entry);
        if (ptep_set_access_flags(fe->vma, fe->address, fe->pte, entry,
                    fe->flags & FAULT_FLAG_WRITE)) {
            update_mmu_cache(fe->vma, fe->address, fe->pte);
        } else {
            /*
             * This is needed only for protection faults but the arch code
             * is not yet telling us if this is a protection fault or not.
             * This still avoids useless tlb flushes for .text page faults
             * with threads.
             */
            if (fe->flags & FAULT_FLAG_WRITE)
                flush_tlb_fix_spurious_fault(fe->vma, fe->address);
        }
    unlock:
        pte_unmap_unlock(fe->pte, fe->ptl);
        return 0;
    }
    //
    /*
     * We enter with non-exclusive mmap_sem (to exclude vma changes,
     * but allow concurrent faults).
     * The mmap_sem may have been released depending on flags and our
     * return value.  See filemap_fault() and __lock_page_or_retry().
     */
    static int do_fault(struct fault_env *fe)
    {
        struct vm_area_struct *vma = fe->vma;
        pgoff_t pgoff = linear_page_index(vma, fe->address);
        /* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
        if (!vma->vm_ops->fault)
            return VM_FAULT_SIGBUS;
        if (!(fe->flags & FAULT_FLAG_WRITE))
            return do_read_fault(fe, pgoff);//如果不需要获取的页面具备可写属性则调用do_read_fault
        if (!(vma->vm_flags & VM_SHARED))
            return do_cow_fault(fe, pgoff);/*需要获取的页面具有可写属性，且我们走的VM_PRIVATE因此会调用do_cow_fault来进行COW操作*/
        return do_shared_fault(fe, pgoff);//共享页面
    }
    //
    static int do_cow_fault(struct fault_env *fe, pgoff_t pgoff)
    {
        struct vm_area_struct *vma = fe->vma;
        struct page *fault_page, *new_page;
        void *fault_entry;
        struct mem_cgroup *memcg;
        int ret;
        if (unlikely(anon_vma_prepare(vma)))
            return VM_FAULT_OOM;//out of memory
        new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, fe->address);//分配一个新的页
        if (!new_page)
            return VM_FAULT_OOM;
        if (mem_cgroup_try_charge(new_page, vma->vm_mm, GFP_KERNEL,
                    &memcg, false)) {
            put_page(new_page);
            return VM_FAULT_OOM;
        }
        ret = __do_fault(fe, pgoff, new_page, &fault_page, &fault_entry);/*将文件内容读取到newpage中*/
        if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
            goto uncharge_out;
        if (!(ret & VM_FAULT_DAX_LOCKED))
            copy_user_highpage(new_page, fault_page, fe->address, vma);
        __SetPageUptodate(new_page);
        ret |= alloc_set_pte(fe, memcg, new_page);//将这个新的page同虚拟内存的映射关系拷贝到页表中去
        if (fe->pte)
            pte_unmap_unlock(fe->pte, fe->ptl);
                    if (!(ret & VM_FAULT_DAX_LOCKED)) {
            unlock_page(fault_page);
            put_page(fault_page);
        } else {
            dax_unlock_mapping_entry(vma->vm_file->f_mapping, pgoff);
        }
        if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
            goto uncharge_out;
        return ret;
    uncharge_out:
        mem_cgroup_cancel_charge(new_page, memcg, false);
        put_page(new_page);
        return ret;
    }
    //
    /**
     * alloc_set_pte - setup new PTE entry for given page and add reverse page
     * mapping. If needed, the fucntion allocates page table or use pre-allocated.
     *
     * @fe: fault environment
     * @memcg: memcg to charge page (only for private mappings)
     * @page: page to map
     *
     * Caller must take care of unlocking fe->ptl, if fe->pte is non-NULL on return.
     *
     * Target users are page handler itself and implementations of
     * vm_ops->map_pages.
     */
    int alloc_set_pte(struct fault_env *fe, struct mem_cgroup *memcg,
            struct page *page)
    {
        struct vm_area_struct *vma = fe->vma;
        bool write = fe->flags & FAULT_FLAG_WRITE;//要求目标具有可写权限
        pte_t entry;
        int ret;
        if (pmd_none(*fe->pmd) && PageTransCompound(page) &&
                IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE)) {
            /* THP on COW? */
            VM_BUG_ON_PAGE(memcg, page);
            ret = do_set_pmd(fe, page);
            if (ret != VM_FAULT_FALLBACK)
                return ret;
        }
            if (!fe->pte) {
            ret = pte_alloc_one_map(fe);
            if (ret)
                return ret;
        }
        /* Re-check under ptl */
        if (unlikely(!pte_none(*fe->pte)))
            return VM_FAULT_NOPAGE;
        flush_icache_page(vma, page);
        entry = mk_pte(page, vma->vm_page_prot);
        if (write)
            entry = maybe_mkwrite(pte_mkdirty(entry), vma);
        /* copy-on-write page */
        if (write && !(vma->vm_flags & VM_SHARED)) {
            inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
            page_add_new_anon_rmap(page, vma, fe->address, false);//建立映射
            mem_cgroup_commit_charge(page, memcg, false, false);