pruning, which detects irrelevant predicates in all the failing
path conditions. For each failing path condition ρf =
[φ1, φ2, . . . , φj, . . . , φ|ρf|], the algorithm checks the predi-
cates one by one in a backward manner, starting from the last
predicate φ|ρf| (Lines 3-6). To determine whether a predicate
φj is not in a c-depend relation, we ﬁrst assume that φj is in
683
a c-depend relation, and then try to ﬁnd a contradiction. Our
technique searches for a passing path condition ρp such that
ρp shares the same preﬁx as ρf up to φj, but ρp deviates
at φj and still reaches the assertion-containing location e.
That is, ρp := [φ1, φ2, . . . ,¬φj, . . . , φe, . . . , φ|ρp|] where φe
is the predicate derived from e. If there exists such a path, φj
is not in a c-depend relation. Next our technique determines
whether the predicate φj is in a d-impact relation. First, we
assume that the predicate φj is not in a d-impact relation.
(cid:2) such
Our technique searches for a failing path condition ρf
(cid:2) de-
(cid:2) shares the same preﬁx as ρf up to φj, but ρf
that ρf
viates at φj and eventually reaches the assertion-containing
location e to cause assertion violation, and the symbolic
(cid:2) is not the same
expression at the last-branch predicate in ρf
(cid:2)|] and
as in ρf . That is, ρf
(cid:2)| (cid:17)≡ φ|ρf|. If there exists such a path, φj is in a d-impact
φ|ρf
relation. If a predicate is neither in a c-depend relation nor a
d-impact relation, our technique removes the predicate from
its path condition.
(cid:2) = [φ1, φ2, . . . ,¬φj, . . . , φ|ρf
B. Collection-Element Generalization
The presence of input-dependent
loops and collection
structures in the execution causes path conditions to contain
many overly speciﬁc predicates over the indices of collection
structures (e.g., loop variables). Since these predicates are
in either c-depend or d-impact relations, our technique of
dynamic predicate pruning cannot prune these predicates,
and thus the generated precondition can contain a lot of
these overly speciﬁc predicates. Based on our empirical
observations, the overly speciﬁc predicates over the indices
of collection structures often follow certain patterns, pre-
senting opportunities to summarize these predicates using
a quantiﬁed constraint over the indices of collection struc-
tures. Based on this insight, we describe how to produce
preconditions with a quantiﬁed condition for the failing path
conditions.
For each individual predicate pred from the precondition
candidate generated by the technique of dynamic predicate
pruning, the technique of collection-element generalization
chooses which predicates can be generalized with a quanti-
ﬁed constraint over array index. The quantiﬁed constraint is
of the form ∀x.(A(x) → B(x)) or ∃x.(A(x)∧ B(x)) where
A and B are sets of predicates. Predicates in A constrain the
domain of the constraint (typically the integer domain used
to iterate over collection structures), and predicates in B
are those expressing the violation of a property for causing
failures.
Our technique breaks down the problem of inferring
a quantiﬁed constraint into two general steps. First, our
technique selects predicates that express the violation of a
property (belonging to set B). Then, our technique selects a
quantiﬁer based on whether all eligible collection elements
witness the violation of the property expressed by the
predicates in set B. Finally, our technique selects predicates
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:38 UTC from IEEE Xplore.  Restrictions apply. 
that restrict the domain of the constraint denoting the eligible
collection elements. Next we describe in detail the technique
of collection-element generalization.
Identifying the violation of a property. Similar to the
technique of dynamic predicate pruning,
the last-branch
predicate is the pivot point for our generalization. As
mentioned earlier, typically an assertion-violation failure is
control-dependent on the branch condition represented as the
last-branch predicate. Thus, in the cases where an assertion-
violation failure is dependent on collection structures, the
last-branch predicate likely expresses the violation of the
property.
For example, the path condition of the failing test tf3
(Table II) includes s[2] == null as the last-branch predicate.
This predicate indicates that the program execution accesses
s[2] and results in an exception. Thus, s[2] == null
expresses the violation of the property: collection element
should not be null. Consider another example program where
each element of a collection arr (starting from the ﬁrst
element) is used as a denominator in division. When the
ﬁrst three elements are not 0 and the fourth element is 0,
the last-branch predicate would be arr[3] == 0, expressing
the violation of the property: no collection element should
be zero.
Inferring the range of collection index. Based on our
empirical observations, our technique currently focuses on
two common types of generalization templates, Existential
Template and Universal Template,
to infer the range of
collection index. But new types of templates can be easily
added as long as they operate over the predicates from
failing path conditions. For a path condition ρ, our technique
ﬁrst generalizes the identiﬁed violation of the property as a
predicate φ, and then try to instantiate the generalization
templates.
• Existential Template. For a path condition ρ over
a visited collection a in the program under test, if
only the last visited collection element a[i] satisﬁes a
predicate φ, represented as φ(a[i]), while all the previ-
ously visited collection elements a[j] do not satisfy φ,
represented as ¬φ(a[j]), then the following generalized
property-violation condition can be inferred:
ϕρ = ∃i, (0 ≤ i < a.length) ∧ φ(a[i])
• Universal Template. For a visited collection a in the
program under test, if all visited elements of a satisfy a
predicate φ, represented as φ(a[i]), the following gen-
eralized property-violation condition can be inferred:
ϕρ = ∀i, (0 ≤ i < a.length) → φ(a[i])
property-violation condition is that there exists an collection
element that satisﬁes the predicate. Dually, in the Universal
Template, an exception from an assertion-containing location
within a loop is triggered because all elements satisfy the
predicate, although the given path may not visit all elements
in the collection.
Example. Consider the method under test example in
Figure 1. In Lines 15-17, a loop iterates over the array
elements to compute the sum of the elements’ lengths.
One failing path condition caused by the implicit assertion
violation on Line 16 can contain predicates s (cid:17)= null ∧ 0 <
s.length ∧ s[0] (cid:17)= null ∧ 1 < s.length ∧ s[1] (cid:17)= null ∧
2 < s.length ∧ s[2] == null. Note that s[2] == null
causes the NullReference exception. These predicates can
be summarized using the Existential Template with pred-
icate s[i] == null (i.e., the violation of the property) as
∃i, (i < s.length ∧ s[i] == null).
Our technique can be easily extended with more templates
on the predicates over a collection index. Consider a program
that iterates over the even-numbered elements of an array,
a, to check that φ(a[i]) holds for every even i. We can sum-
marize these predicates by adding the following template:
ϕρ = ∀i, (0 ≤ i < a.length ∧ i%2 == 0) → φ(a[i])
To instantiate this template for a path condition ρ, our
technique can check that for every even index i, φ(a[i])
must hold.
V. EVALUATION
We implement our PREINFER approach as a prototype
on top of Pex [2], [3], an industrial test generation tool.
To assess PREINFER’s effectiveness, we compare PREINFER
with two related state-of-the-art approaches for precondition
inference (DySy [8] and FixIt [9]). In particular, we conduct
an evaluation of PREINFER and the related approaches
on two benchmark suites and two real-world open-source
projects. This comparison helps characterize the strengths
of PREINFER compared with the previous related work. We
intend to answer the following two research questions:
• RQ1: How effective is PREINFER in inferring pre-
conditions compared to the related state-of-the-art ap-
proaches (DySy [8] and FixIt [9])?
• RQ2: How complex are the preconditions inferred by
PREINFER compared to those inferred by DySy and
FixIt?
A. Evaluation Subjects
In the Existential Template, an exception (i.e., an assertion
violation) from an assertion-containing location within a
loop is triggered by the value of a collection element.
The collection element is always the last-visited one since
the program aborts with an exception. Thus, the inferred
We select four evaluation subjects (written in C#) from
GitHub [10]. Table III shows the characteristics of each
evaluation subject. Additionally,
these four subjects are
classiﬁed into three categories: open-source projects, well-
studied benchmarks, and manually-constructed benchmarks.
684
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:38 UTC from IEEE Xplore.  Restrictions apply. 
CHARACTERISTICS OF EVALUATION SUBJECTS
Table III
Subject
Algorithmia
CodeContracts
DSA
SVComp
#Classes
91
4
48
4
#Methods
525
150
457
11
#Lines
18249
1965
9468
1219
#Files
95
7
61
14
AVERAGE BLOCK COVERAGE ACHIEVED BY PEX FOR ALL THE
Table IV
METHODS IN EACH EVALUATION SUBJECT
Subject
Average Block Coverage
65.41%
Algorithmia
99.20%
CodeContracts
100.00%
DSA
SVComp
95.61%
1) Open-Source Projects: We select
two open-source
projects, Data Structures and Algorithms (DSA) [11] and
Algorithmia [12], used in previous studies. DSA is used in
previous empirical studies of speciﬁcations [13], while Al-
gorithmia is used in previous empirical studies of structural
test generation [14].
2) Well-Studied Benchmarks: We select array-examples,
loop-acceleration, and array-industry-pattern benchmark
suites from SV-COMP [15]. We choose SV-COMP because
its benchmarks are well studied, often used in evaluating
software veriﬁcation tools [16], [17], and contain non-
trivial quantiﬁed preconditions. From the selected bench-
mark suites, we further exclude programs that do not have
preconditions or that we are not able to translate to C#.
3) Manually-Constructed Benchmarks: We extract re-
gression tests for the static analyzer cccheck [18] to con-
struct a benchmark suite. The static analyzer infers precon-
ditions for .NET programs. For the suite, we include only
tests that stress-test the precondition inference algorithms in
cccheck, without including other tests that stress-test other
components (e.g., abstract domains).
B. Evaluation Setup
For each evaluation subject, we use Pex to generate tests
for each public method in the subject. If there exists a
generated test that triggers an uncaught runtime exception
at an assertion-containing location, we denote the test as
failing and the method as an exception-throwing method.
Note that multiple assertion-containing locations can be
triggered in a method. The ﬁnal set of methods used in our
evaluation contains all the exception-throwing methods in
each evaluation subject. Furthermore, the total number of
exception-throwing locations used in our evaluation is the
number of triggered assertion-containing locations across all
methods in the ﬁnal set. Column #ACL in Table V shows
the total number of exception-throwing locations evaluated
per subject. In total, our evaluation subjects include 188
exception-throwing locations, among which 33 are from
collection-element cases, as shown in Column #ACL in
Table VI.
To ensure fair comparison between different
inferred
preconditions for an assertion-containing location, we use
the same set of Pex-generated tests for different approaches
under comparison to check whether an inferred precondition
blocks all the failing tests in the set and allows all the passing
tests in the set. In particular, we use Pex to produce a set
of tests T for the method under test m. Given an assertion-
containing location e in m, we partition T into Tpass and
Tf ail: (1) we assign a test t to Tpass if t’s execution does
not reach the assertion-containing location e, or reaches e
but does not violate the assertion in e, and (2) we assign t
to Tf ail if t’s execution does reach the assertion-containing
location e and violate the assertion in e. Table IV shows the
average block coverage achieved by the tests generated by
Pex for all the methods in each evaluation subject.
To assess the quality of an inferred precondition for each
assertion-containing location, we use two metrics.
Correctness. We manually inspect an inferred precon-
dition against a ground-truth precondition. In other words,
we check whether the inferred precondition is semantically
equivalent
to the ground-truth precondition. To obtain a
ground-truth precondition for each assertion-containing lo-
cation, we employ the following steps. Initially, an author of
this paper inspects the source code of the method containing
the assertion-containing location and derives a precondition,
pred. For difﬁcult cases, where the author is unsure of
the correctness of pred, another author is engaged. If both
authors cannot reach a consensus, then we test the strength
of pred and ¬pred using Pex. If pred is ‘likely’ perfect,
then inserting pred at the entry point of the method should
invalidate all failing runs, while inserting ¬pred should
validate only failing runs. We can only be certain pred is
‘likely’ perfect because Pex may not explore all execution
paths.
Complexity. Unlike our methodology for manually judg-
ing correctness of an inferred precondition, we rely on
tool automation that parses a string representation of the
inferred precondition to compute complexity. In particular,
we measure the complexity of an inferred precondition ψ
of a method by calculating a relative complexity, which
represents the percentage difference between its complex-
ity (|ψ|) and the complexity of the method’s ground-truth
∗|) generated manually. In other words, the
precondition (|ψ
relative complexity of ψ is
||ψ| − |ψ
∗|
|ψ
∗|
|
The lower the relative complexity is, the more succinct the
precondition is, i.e., a perfect inferred precondition has the
relative complexity equal to zero.
685
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:38 UTC from IEEE Xplore.  Restrictions apply. 
COMPARISON OF PRECONDITIONS GENERATED BY THE THREE APPROACHES ON ALL THE SUBJECTS
Table V
Namespace
Algorithmia.Sorting
Algorithmia.GeneralDataStr
DSA.Algorithm
CodeContracts.ExamplesPuri
CodeContracts.PreInference
CodeContracts.ArrayPurityI
SVComp.SVCompCSharp
Total
Exception Location #ACL
3
Before loop
Inside loop
0
0
After loop
3
Total
13
Before loop
5
Inside loop
After loop
0
18
Total
11
Before loop
17
Inside loop
5
After loop
Total
33
14
Before loop
18
Inside loop
0
After loop
32
Total
Before loop
58
21
Inside loop
7
After loop
86
Total