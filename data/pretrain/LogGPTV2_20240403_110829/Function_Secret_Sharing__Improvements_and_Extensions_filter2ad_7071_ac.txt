128-bit output
Eval• - #AES Gen• - #AES
operations
operations
n
16
25
40
80
160
2n
32
50
80
160
320
Table 1: Performance of the optimized DPF construction as in Figure 1.
In the case of single bit outputs we use the
additional early termination optimization, which is presented in Section 3.2.1. The precise analytic expression for the key
size is somewhat cumbersome, but is reﬂected in the exact key size for each concrete input domain. We implement the PRG
operation expanding s ∈ {0, 1}127 to 256 bits by computing AESs||0(0)||AESs||0(1).
Optimized Distributed Point Function (Gen•, Eval•)
Let G : {0, 1}λ → {0, 1}2(λ+1) a pseudorandom generator.
Let ConvertG : {0, 1}λ → G be a map converting a random
λ-bit string to a pseudorandom group element of G.
Gen•(1λ, α, β, G):
1: Let α = α1, . . . , αn ∈ {0, 1}n be the bit decomposition.
2: Sample random s(0)
3: Sample random t(0)
4: for i = 1 to n do
0 ||tR
5:
1 ← {0, 1}λ
0 ← {0, 1}λ and s(0)
1 ← t(0)
0 ← {0, 1} and take t(0)
1 ||tL
0 ← G(s(i−1)
0 ⊕ 1
1 ||tR
(cid:12)(cid:12)(cid:12)(cid:12) sR
(cid:12)(cid:12)(cid:12)(cid:12) sR
) and sL
1 ←
1
0
0 ⊕ tR
1 ⊕ αi
1 )(cid:3),
0 ) + Convert(s(n)
1
b
b
1
CW
1 ⊕ αi ⊕ 1 and tR
1(cid:2)β − Convert(s(n)
CW ← tR
CW||tR
· sCW for b = 0, 1
· tKeep
CW for b = 0, 1
0 ||tL
sL
0
G(s(i−1)
).
if αi = 0 then Keep ← L, Lose ← R
else Keep ← R, Lose ← L
end if
0 ⊕ sLose
sCW ← sLose
0 ⊕ tL
CW ← tL
tL
CW (i) ← sCW||tL
b ⊕ t(i−1)
b ← sKeep
s(i)
b ← tKeep
b ⊕ t(i−1)
t(i)
6:
7:
8:
9:
10:
11:
12:
13:
14: end for
15: CW (n+1) ← (−1)tn
with addition in G
||t(0)
16: Let kb = s(0)
b
17: return (k0, k1)
Eval•(b, kb, x):
1: Parse kb = s(0)||t(0)||CW (1)||···||CW (n+1)
2: for i = 1 to n do
3:
4:
5:
6:
7:
8:
9: end for
τ (i) ← G(s(i−1)) ⊕ (t(i−1) ·(cid:2)sCW||tL
Parse τ (i) = sL||tL (cid:12)(cid:12)(cid:12)(cid:12) sR||tR ∈ {0, 1}2(λ+1)
if xi = 0 then s(i) ← sL, t(i) ← tL
else s(i) ← sR and t(i) ← tR
end if
Parse CW (i) = sCW||tL
||CW (1)||···||CW (n+1)
CW||tR
CW
b
(cid:3))
CW||sCW||tR
CW
10: return (−1)b(cid:2)Convert(s(n)) + t(n) · CW (n+1)(cid:3) ∈ G
Figure 1: Pseudocode for optimized DPF construction for
fα,β : {0, 1}n → G, where || denotes string concatenation.
Subscripts 0 and 1 refer to party id. All s values are λ-bit
strings and t values are a single bit.
G in every other location. The two improvements described
above lead to the following theorem statement.
Theorem 3.3
(Full domain evaluation). Let λ be a
security parameter, G be an abelian group and (cid:96) = log |G|.
Let z = max{λ, (cid:96)} and let G be a PRG G : {0, 1}λ →
{0, 1}2z+1. There exists a full domain evaluation protocol for
(cid:96)(cid:99))(λ + 2)
fα,β : {0, 1}n → G with key size at most (n−(cid:98)log z
z(cid:101) PRG operations.
and at most (cid:100)2n (cid:96)
3.3 FSS for Decision Trees
We now describe how the tensoring approach can be uti-
lized to provide FSS for the broader class of decision trees.
A decision tree is deﬁned by a tree topology, variable labels
on each node v (which can take values in some node-speciﬁc
set Sv), value labels on each edge (in our case, correspond-
ing to some element in Sv), and output labels on each leaf
node.
In our construction, the key size is roughly λ·|V |, where V
is the set of nodes, and evaluation on a given input requires
|V | executions of a pseudorandom generator, and a compa-
rable number of additions. The FSS is guaranteed to hide
the secret edge value labels and leaf output labels (which
we refer to as “Decisions”), but (in order to achieve this ef-
ﬁciency) reveals the base tree topology and node variable
assignments (which we refer to as “Tree”).
As a simple illustrative example, consider a decision tree
representation of the OR function on n bits xi. The tree
topology includes a length-n chain of nodes (each labeled
by a unique input xi), with edges all labeled by 0, ending
in a terminal output node (labeled by 0). In addition, from
each internal node there is a second edge, labeled by 1, ter-
minating in a leaf labeled by 1. In this example, the leaked
information “Tree” consists of the structure of the tree and
the n node labels xi; the hidden information “Decisions” con-
sists of the choice of condition labels 0,1 on each edge, as
well as the 0,1 leaf output labels. In particular, the result-
ing FSS key cannot be distinguished from the analogous FSS
key for the AND function, which has an identical structure
but with the 0 and 1 roles reversed.
Note that FSS for decision trees could be attained di-
rectly from a linear combination of separate DPFs: for each
leaf node, simply include an additional corresponding DPF.
However, such an approach is wasteful; in particular, our
DPF constructions have the property that a DPF key con-
tains within it explicitly DPF keys for each of its preﬁxes.
Because of this, we can optimize the required key size and
1296computation, to leverage the “shared backbone” of paths
within the decision tree. Our construction instead directly
applies the tensoring approach to “append” each node onto
the backbone structure, one by one.
More speciﬁcally, our construction is recursive. The key
generation procedure Gen starts from the leaves.
In each
step, we obtain an FSS key for a given tree structure Γ
by sampling a key for a smaller tree Γ(cid:48) with all siblings
of one leaf node in Γ removed (say, children ua of a node
v). The output value we select for this newly formed leaf
v in the restricted decision tree program will be the same
structure s||1 ∈ {0, 1}λ+1 as in the tensor operation from
the previous section. And, in an analogous fashion, we will
construct a “correction word” associated with this node v,
which contains the target values for its children, but masked
by the PRG-output of the parties’ respective shares of s (i.e.,
G(sb)). The key thus consists of a correction word for every
node, which is an element of {0, 1}λ+1 for each internal node,
and is an element of the output group G for each leaf node.
Security of the scheme follows the same argument as in the
tensor.
The evaluation procedure Eval begins from the root node,
and calls a recursive function EvalNode. When executed on
a leaf, EvalNode outputs its correction word directly. When
executed on an internal node, EvalNode outputs the sum of
EvalNode on each of its children.
We refer the reader to the full version for formal deﬁni-
tions, constructions, and proof.
Constant-dimension intervals. A sample application of
our FSS construction for decision trees is for constant d-
dimensional interval queries: that is, functions f (x1, . . . , xd)
which evaluate to a selected nonzero value precisely when
ai ≤ xi ≤ bi for some secret interval ranges (ai, bi)i∈[d].
(See, e.g., [28] for supporting a similar functionality in the
context of searching on encrypted data.) For n-bit inputs
of length (cid:96) we achieve FSS for d-dimensional intervals with
key size and computation time O(nd). For small values of d,
such as d = 2 for supporting a conjunction of intervals, this
yields solutions with reasonably good concrete eﬃciency.
Corollary 3.4. For d ∈ N there exists FSS for the class
of d-dimensional intervals (ai, bi)i∈[d] with key size O(λ·nd).
The construction can be achieved as follows. First, we re-
duce from general d-dimensional intervals to the problem of
2d “special” intervals, whose left-boundary ai is equal to 0.
This can be done by means of a linear combination of spe-
cial intervals via inclusion-exclusion (and recalling that FSS
schemes combine linearly [7]).
To illustrate the construction of FSS for these special d-
dimensional intervals, consider the case of d = 1 and 2.
Observe that a 1-dimensional special interval for n-bit in-
puts can be expressed directly as a decision list; that is, a
decision tree with one long length-n path u1, . . . , un with
edges (ui, ui+1), and single terminal edges with appropriate
0/1 output labels departing from each node along the path.
(Namely, a generalization of the OR function construction
discussed earlier).
To extend to 2 dimensions, the 0/1 terminal edges from
nodes ui are each replaced by a length-n decision list (as
above), this time labeled by the corresponding bits of the
second input y. Departing from the primary path corre-
sponds to either falling outside the x-dimension interval (in
which case the ﬁnal leaf will be labeled 0) or within it, in
which case the leaf will be labeled based on the 1-dimensional
y interval. A similar approach can be taken to extend to
general d dimensions, for constant d.
We remark that revealing the topology and node labels
of the utilized d-dimension decision tree (as is the case in
our FSS for decision trees construction) does not adversely
aﬀect security, since this structure is identical across any
choice of secret interval boundaries. Rather, the only thing
that diﬀers in the construction is the choice of edge and leaf
node labels, which is precisely what is hidden by our FSS
construction.
3.4 A Product Operator for FSS
In this section we present a simple technique for increas-
ing the expressive power of FSS by increasing the number
of parties. We consider here function families F such that
for each f ∈ F the output range Rf is equipped with a
ring structure. For two functions f1, f2 with the same input
domain and output ring, we naturally deﬁne the product
f = f1 · f2 by f (x) = f1(x)f2(x).
Definition 3.5
(Product of function families). Let
F1,F2 be function families. Deﬁne
F1·F2 = {f1 · f2 : f1 ∈ F1, f2 ∈ F2, Df1 = Df2 , Rf1 = Rf2}
The product operator can be used for expressing function
classes that capture conjunctions. For instance, if the input
x is partitioned into (x1, x2) and F1,F2 are the classes of in-