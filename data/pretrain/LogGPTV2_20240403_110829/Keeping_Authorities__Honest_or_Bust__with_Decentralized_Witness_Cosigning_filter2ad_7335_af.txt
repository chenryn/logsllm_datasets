beyond around 256 witnesses.
The JVSS approach proves to be the least scalable variant,
becoming impractical beyond about 32 witnesses. This poor
scalability results from the fact that JVSS requires each of
the N witnesses to serve in a “dealer” role, each producing
an N-share secret polynomial whose shares are encrypted and
sent to the other N nodes. Every node must then combine
the N public polynomials and the N encrypted shares it
receives to form shares of a joint master polynomial. In
threshold Schnorr signing using JVSS, this O(N 2) dealing
cost is incurred both during initial key-pair setup and during
each signing round, because it is required to produce a fresh
shared Schnorr commit ˆV0 each round whose private value
is not known to any individual or sub-threshold group of
participants. Using a pairing-based signature scheme such as
BLS [19] in place of Schnorr could eliminate the need to deal
a fresh commit per signing round and thus reduce the per-
round cost of JVSS signing, but the O(N 2) joint dealing cost
would still be required at key generation time.
C. Computation Costs
The next experiment focuses on the protocol’s per-node
computation costs for signing and signature veriﬁcation.
The CoSi leader periodically initiates new collective signing
rounds, and we measure the total CPU time per round imposed
on the most heavily-loaded participant. Since all CoSi partici-
pants check the (partial) signatures submitted by their children
in the process of producing the full aggregate signature, this
computation cost includes the cost of signature checking.
Figure 4 shows how measured System and User time on
the most heavily-loaded signing node (typically the root)
varies depending on the number of cosigning witnesses. The
ﬁgure also shows the computation costs of comparable Naive
and NTree cosigning approaches using individual signatures,
as well as using joint veriﬁable secret sharing (JVSS). As
expected, the computational cost of the CoSi protocol stays
relatively ﬂat regardless of scale, whereas the computation
538538
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:54 UTC from IEEE Xplore.  Restrictions apply. 
104
)
s
e
t
y
B
k
(
c
i
f
f
a
r
t
k
r
o
w
t
e
n
l
a
t
o
T
103
102
101
100
2
JVSS
Naive
NTree
CoSi
8
32
128
512
2048
8192
32768
Number of witnesses
Fig. 5. Network trafﬁc (bandwidth consumption) at the root node versus
number of participating witnesses.
s
d
n
o
c
e
s
n
i
y
c
n
e
t
a
l
d
n
u
o
r
i
g
n
n
g
S
i
6
5
4
3
2
2048 Witnesses
4096 Witnesses
8192 Witnesses
depth=5
depth=5
depth=4
depth=4
depth=5
depth=3
2
4
depth=4
6
8
10
12
14
16
18
Branching factor
depth=3
Fig. 6. Collective signing latency versus branching factor.
costs of the competing schemes begin to explode with groups
beyond a few tens of witnesses.
The measured computation time is often greater than the
wall-clock signing latency because computation is done in
parallel and the graph represents the sum of the CPU time
spent by all threads running on a given witness server.
D. Network Trafﬁc
The next experiment measures the total network trafﬁc
produced by CoSi in comparison with the Naive, NTree, and
JVSS baselines. Figure 5 shows these results. Due to CoSi’s
aggregation mechanism, network trafﬁc at the root node rises
much more slowly than in the the baseline schemes, which
all lack the beneﬁt of aggregation, as the number of witnesses
grows. JVSS puts a particularly high burden on the network
due to its O(N 2) communication complexity.
539539
E. Effects of Spanning Tree Conﬁguration
Our next experiment explores the tradeoffs in organizing
the spanning tree with which CoSi aggregates signatures:
in particular the tradeoffs between wide, shallow trees and
narrower, deeper trees. This experiment is parameterized by
the tree’s branching factor, or maximum number of children
per interior node, where 2 represents a binary tree.
Figure 6 shows the relationship between per-round signing
latency and branching factor in spanning trees containing
2,048, 4,096, and 8,192 witnesses total, respectively. Low
branching factors increase tree depth, increasing root to leaf
round-trip latency by about 200 milliseconds per unit of depth
added. On the other hand, low branching factors also decrease
both the CPU time spent per node and the communication
costs each node incurs coordinating with its children.
Empirically, we ﬁnd that the higher the branching factor
the lower the signing latency. For example, in the case of
2,048 witnesses and a branching factor of 16, we get a tree
depth of 3 and a collective signing latency of below 2 seconds.
For trees of depth 3 or less we ﬁnd that computation time
dominates, while for depths 5 or more network latencies begin
to dominate. The current CoSi prototype makes no attempt to
optimize its computations, however; further optimization of
the computations might make small depths more attractive.
F. Effects of Testbed Oversubscription
Since we did not have thousands of dedicated physical hosts
on which to evaluate CoSi, we had to “oversubscribe” the
testbed by running multiple CoSi witness processes on each
physical testbed machine. The spanning trees are laid out such
that no two adjacent nodes in the tree run on the same physical
host, ensuring that the 200ms round-trip delays imposed by
DeterLab apply to all pairs of communicating witnesses in
the tree. However, oversubscription can introduce experimen-
tation artifacts resulting from compute load on each physical
machine and different CoSi witness processes’ contention for
other system resources; we would like to measure the potential
severity of these effects.
Figure 7 shows the signing round latencies we measured for
experiments using a given number of witnesses on the x-axis,
but with these witness processes spread across 8, 16, or 32
physical machines to compare different levels of oversubscrip-
tion. Unsurprisingly, the latencies become noticeably worse at
higher levels of oversubscription (fewer physical machines),
and this effect naturally increases as total number of witnesses
and hence total
load per machine increases. Nevertheless,
even with these oversubscription effects the measured latencies
remain “in the same ballpark” for groups up to 4,096 witnesses
(512× oversubscription on 8 machines). The performance
decrease observable in Figure 3 for more than 8,192 CoSi-
witnesses can be also attributed to oversubscription and thus
to the increased computational load the 32 physical machines
have to handle. Thus, since experimental oversubscription
works against CoSi’s performance and scalability, we can treat
these experimental results as conservative bounds on signing
time per round; a deployed witness cothority using dedicated
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:54 UTC from IEEE Xplore.  Restrictions apply. 
Witnesses split over 8 physical machines
Witnesses split over 16 physical machines
Witnesses split over 32 physical machines
8.00
4.00
2.00
s
d
n
o
c
e
s
n
i
y
c
n
e
t
a
l
d
n
u
o
r
i
g
n
n
g
S
i
16
32
64
128
256
512 1024 2048 4096 8192 16384 32768
Total number of witnesses
Fig. 7. Collective signing latency versus testbed oversubscription ratio
(witness processes per physical machine) for a tree depth of 3.
(or at least less-overloaded) witness servers may well perform
signiﬁcantly better than in our experiments.
G. Timestamping Application Scalability
As discussed in Section V-A, our timestamping applica-
tion uses CoSi periodically to sign timestamp records that
can aggregate many clients’ timestamp requests each round.
In addition, further leveraging CoSi’s scalable structure, the
timestamp service allows not only the leader but also the
witness servers to handle timestamp requests from clients, each
server forming a local Merkle tree of timestamps per round
and then aggregating these local trees into one global tree
during the Commit phase of the CoSi protocol.
To evaluate the scalability of this timestamping service, as
opposed to the “bare” performance of CoSi signing, we ran an
experiment in which for each CoSi server a separate process on
the same physical machine acted as a client sending timestamp
requests at a constant rate. We tested the system under a variety
of client load rates, from one request every 5 seconds to one
request every 13ms – the last case amounting to 80 requests
per second on each timestamp server. Client loads within
this range did not signiﬁcantly affect the collective signing
latencies we observed, however, so we omit these graphs.
At large-scale experiments with 4,096 timestamp/witness
servers spread across 16 physical
testbed machines (256
servers per machine), each physical machine effectively han-
dled an aggregate client
load of about 20,000 timestamp
requests per second, or 320,000 timestamp requests per second
across the 4096-server collective. Further, the current CoSi
implementation and timestamp server code is largely unopti-
mized and completely unparallelized within each server: with
more powerful, unshared machines, we expect that each server
could readily handle much larger timestamping service loads.
H. Difﬁculty of Retroﬁtting Existing Authorities
Finally, to provide an informal sense for the software im-
plementation costs of retroﬁtting existing authority systems to
support witness cosigning, we relate our experience adapting
the CT log server. In this case, the log server is written in a
different language (C++), and we did not attempt to combine
the log server and CoSi implementation in a single program.
Instead, when our modiﬁed CT log server is conﬁgured to
attach collective signatures to its Signed Tree Heads (STHs),
the log server ﬁrst prepares the STH internally, then uses
inter-process communication to request that a separate process
implementing the CoSi leader initiate a signing round. The CT
log server’s STH signing process then waits for the CoSi round
to complete, and incorporates the CoSi-generated collective
signature into an extension ﬁeld in the STH. The veriﬁcation
is done in a separate program that requests the STH from
the log server and veriﬁes the signature against the aggregate
public key of the CoSi-tree.
With this two-process approach to integrating CoSi, the
necessary changes to the CT log server amounted to only
about 315 lines as counted by CLOC [41], or 385 “raw”
source code lines. Further, this integration took less than one
person-week of effort. While a production deployment would
of course involve signiﬁcantly more effort than merely writing
the code, nevertheless our experience suggests that it may be