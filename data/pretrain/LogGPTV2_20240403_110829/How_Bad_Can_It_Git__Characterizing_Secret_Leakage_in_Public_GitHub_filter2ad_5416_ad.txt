at which secrets and ﬁles were removed dramatically outpaces
the rate at which repos were removed; this indicates that users
were not deleting their repos, but were simply creating new
commits that removed the ﬁle or secret. Unfortunately, due
to the nature of the Git software, the secrets are likely still
accessible [50] (see Section VIII for more on this issue).
These conclusions suggest that many of the secrets discov-
ered were committed in error and that they were sensitive. 19%
of the secrets were removed at some point in roughly 2 weeks,
and most of those were done in the ﬁrst 24 hours. This also
means 81% of the secrets we discover were not removed. It is
likely that the developers for this 81% either do not know the
secrets are being committed or are underestimating the risk of
compromise. In absolute terms, 19% of our results amounts of
tens of thousands of secrets and serves as a lower bound on
the number of discovered secrets that were sensitive, adding
conﬁdence to our overall result.
Further, we examined whether users that removed their
secrets while keeping their repos performed any process to
rewrite history to remove the commit, as suggested by GitHub
[50]. For every such instance, we queried the GitHub Commits
API for information on the commit we discovered; if the commit
had been rewritten, it would no longer be accessible. We found
that none of the monitored repos had their history rewritten,
meaning the secrets were trivially accessible via Git history.
F. RSA Key Leakage
Table II shows a large portion of secrets in our dataset
were RSA keys, which is expected as they are used for a
large number of different applications. We performed various
experiments to investigate how many of these RSA keys pose
a signiﬁcant risk if discovered.
Number of Valid Keys RSA keys contain a deﬁned and
parseable structure. Thus, we can determine how many of these
8
  0% 20% 40% 60% 80%100%Percent with parallel target leakageTwitter Access TokenTwilio API KeyGoogle OAuth IDAmazon MWS Auth TokenAmazon AWS Access Key IDPercent of Distinct Secrets That Also Leak a Parallel SecretSearchBigQuery(a) Many secrets are removed in the ﬁrst few hours after being
committed, but the majority remain
(b) Secrets that still exist on GitHub for a day after commit tend to stay
on GitHub indeﬁnitely
Fig. 3: Short and Long Term Monitoring of Secrets
keys were valid using the Paramiko library [47]. Out of the
25,437 secrets discovered via the Search API, we found that
25,370 keys, or 99.74%, were valid. From the BigQuery dataset,
of the 15,262 keys, 98.31% or 15,004 keys were valid.
Number of Encrypted Keys The Public Key Cryptography
Standard (PKCS) allows for private keys to be encrypted [39].
While leaking a key is never a good idea, an attacker will have
a much more difﬁcult time compromising a leaked key if the
key is encrypted. Again, we used the Paramiko [47] library,
which can determine when a key is encrypted, on our keys
to count how many were encrypted. From this experiment, we
found that none of the leaked keys in the Search and BigQuery
datasets were encrypted, meaning an attacker could easily use
every single one.
OpenVPN Conﬁg Analysis Another application of RSA
keys is usage in OpenVPN conﬁguration ﬁles, in which keys
may be embedded for client authentication to the VPN server.
As an additional layer of protection, OpenVPN recommends
clients specify the auth-user-pass option in the conﬁg
ﬁle. This option requires a user to also enter a valid password
to be connected to the VPN, which makes use of a stolen
key more difﬁcult. In order to determine whether an attacker
could gain unauthorized access to VPN servers, we analyzed
how many OpenVPN conﬁgs containing RSA keys existed in
our dataset by looking for ﬁles with the .ovpn extension and
investigated whether they could be used without further effort.
In the Search dataset, we identiﬁed 1,890 total OpenVPN
conﬁg ﬁles in our dataset. Critically, 13.18% of these did not
use the auth-user-pass option, meaning the user could
easily be compromised by an attacker. In the BigQuery dataset,
we identiﬁed 5,390 total OpenVPN conﬁg ﬁles, of which
1.08% were vulnerable. There is a discrepancy between the
two datasets, likely because licensed repos are more mature
and contain more example ﬁles, but both datasets still revealed
a substantial number in absolute terms.
VI. METHODOLOGY EVALUATION
In this section, we evaluate the key aspects of our method-
ology. First in Section VI-A, we consider our data collection
methods. Second in Section VI-B, we evaluate the efﬁcacy of
our secret validation ﬁlters.
A. Search API Evaluation
The Search API collection gave us near real-time insight
into ﬁles committed to GitHub. We performed two experiments
to validate this method. First, we found how long it took for
a known random string to appear in search results after being
committed. Second, we measured the percent of ﬁles we could
see using a single API key with the Search API out of all ﬁles
committed within a time period.
Time to Discovery If the time for GitHub to index and
return search results is long, users have a large period of time
to remove their secret before we could detect it. To determine
this time to discovery, we set up an experiment in which a
known string was pushed to a known repository. Immediately
after pushing, we started a timer and began querying the API
continuously until the string appeared, at which point the timer
was stopped. This time measures the time to discovery. We
ran this experiment once a minute over a 24 hour period to
compensate for variations with time of day.
We found that the median time to discovery was 20 seconds,
with times ranging from half a second to over 4 minutes,
and no discernible impact from time-of-day. Importantly, this
experiment demonstrates that our Search API approach is able
to discover secrets almost immediately, achieving near real-time
discovery of secrets. If a user does realize their secrets were
mistakenly pushed, this leaves little time for them to correct
the problem before their secrets would be discovered.
Coverage Since we only used a single GitHub key for
Search API scanning, we were restricted to searching at 30
minute intervals to avoid rate limiting issues. Running at this
rate created the possibility of missing results that were pushed
and removed between the scanning times. To evaluate how
much coverage we could achieve at our interval, we set up
the following experiment. For each query (running individually
to get better scanning granularity), we did an initial pull of
results on GitHub (the start set). For the next 30 minutes, we
constantly pulled all results for the query from GitHub non-
stop. This constant pull represented the theoretical maximum
of results that we could potentially have pulled with no rate
limiting (the max set). Finally, we pulled a ﬁnal set of results
after the interval (the end set). Then, our total coverage is
9
0123456789101112131415161718192021222324Number of hours after detection80.0%82.5%85.0%87.5%90.0%92.5%95.0%97.5%100.0%Percent of total still presentShort-Term Hourly Monitoring ofRepositories, Files, and Secrets After DetectionRepositoryFileSecrets012345678910111213141516Number of days after detection80.0%82.5%85.0%87.5%90.0%92.5%95.0%97.5%100.0%Percent of total still presentLong-Term Daily Monitoring ofRepositories, Files, and Secrets After DetectionRepositoryFileSecretsgiven by:
coverage = 1 − |max set − (start set ∪ end set)|
(2)
|max set|
This experiment was repeated over a period of 3 days covering
both a weekend day and weekdays.
We found that the overall coverage across all our queries
was 98.92%. There was minimal ﬂuctuation on the weekday
as compared to the weekend, with each achieving 98.85% and
99.02% coverage, respectively. This result shows that a single
user operating legitimately within the API rate limits imposed
by GitHub is able to achieve near perfect coverage of all ﬁles
being committed on GitHub for our sensitive search queries.
Of course, a motivated attacker could obtain multiple API keys
and achieve full coverage.
B. Regular Expression and Valid Secret Filter Evaluation
In this section, we discuss three experiments we performed
to validate our regular expressions and ﬁlters. We previously
deﬁned a “valid” secret as a string that is a true instance of
the distinct secret for which it matches. The strings matching
our distinct secret regular expressions may not necessarily be
“valid” secrets, and therefore we designed and implemented a
series of ﬁlters on these strings to validate them using various
techniques. After collecting a large number of candidate secrets,
we were able to validate the performance of this ﬁlter.
Most Common Patterns and Words Recall that two of
the key stages for the validity ﬁlters were eliminating strings
containing patterns or English (and common GitHub) words.
To gain insight into which patterns were causing secrets to
be invalidated, we counted the individual patterns and words
detected by the ﬁlters on our dataset. The only word which
appeared more than 3 times in both datasets was “example”,
which appeared 9 times in Search and 20 times in BigQuery.
Patterns appeared more frequently. The most common
pattern was “XXXX”, which appeared 27 times in Search
and 46 times in BigQuery. There were over 90 patterns that
appeared more than 2 times. To count the relative occurrence,
each pattern was grouped into one of three categories: ascending
sequence, descending sequence, or repeated sequence. Ascend-
ing sequences were most common and comprised 52.91% of
all patterns, while repeated sequences made up 31.24% and
descending sequences made up 15.86% of the patterns.
The results of this experiment suggest that most of the
keys being invalidated by the pattern ﬁlter are example keys
or patterned gibberish sequences. Keys containing the word
“example” are most likely example keys. Patterns of letters that
perfectly ﬁt one of the secret regular expressions are unlikely
to happen by chance, which suggests that users are typing
sequences on their keyboard to act as an example key.
Filter Validation Determining whether a given string is
a valid secret is a difﬁcult task to do with perfect accuracy
as most secrets do not contain a veriﬁable structure. This
challenge also makes automatic validation of our ﬁlters itself
difﬁcult. RSA private keys, however, do have a parseable and
veriﬁable structure. These keys were also one of the largest
sets of secrets we had. We validated our ﬁlters by checking
that the RSA private keys we collected were valid. We used
10
the Paramiko library [47] to try and parse each RSA key. This
library throws a speciﬁc exception if a key is not parseable,
allowing us to distinguish between errors due to parsing and
other issues (e.g. encryption errors). All keys that threw non-
parsing errors were excluded as we would not be able to verify
these keys. After running the library, which provided ground
truth, we ran our ﬁlters and analyzed its predicted performance.
In the Search dataset, we considered a total of 25,437
RSA keys. We ﬁnd that our ﬁlters correctly conﬁrmed 24,935
(98.03%) of them to be valid (24,918) or invalid (17). Further-
more, our ﬁlter only failed to invalidate 50 keys that were not
parseable by the library. In the BigQuery dataset, we considered
a total of 15,252 RSA keys, we ﬁnd that our ﬁlter conﬁrmed
97.97% of them to be valid or invalid, and 60 keys were not
parseable by the library.
The results of this experiment provide mixed insights. On
one hand, it shows that the ﬁlter was able to validate a large
number of secrets with minimal inaccuracy. On the other hand,
the experiment also reveals that the number of valid keys
dramatically outweighs the number of invalid keys. In total,
only 67 of the 25,437 were invalid. We believe that the targeted
secrets have such distinct structures that truly invalid keys are
rare and a minimal concern for an attacker, who would be able
to simply test the keys themselves against the target service.
Regular Expression Accuracy In this section, we measure
the performance of the distinct secret regular expressions. For
each regular expression, we calculate precision as number of
valid matches (validated by all ﬁlters) divided by the total
number of matches of the regular expression. Unfortunately,
because ground truth on all secret leakages on GitHub is simply
not available, we are unable to compute recall.
The main concern was that a loose regular expression would
match a large number of invalid strings and create many false
positives. However, this was not the case with our distinct
secret regular expressions. Our regular expressions achieved
99.29% precision in the Search dataset and 98.93% precision in
the BigQuery dataset. The precision of each regular expression
is shown in Figure 4. Only 4 of the distinct secret regular
expressions gave more than a 2% rejection rate in both datasets,
and these all had relatively small sample sizes. This shows that
our regular expressions themselves are sufﬁcient to determine
valid keys for most of the secrets.
VII. POTENTIAL ROOT CAUSE ANALYSIS
It is now clear that secret leakage on GitHub puts a large
number of keys at risk. In this section, we demonstrate various
experiments that try to address the question of why secret
leakage is so extensive. Without asking developers directly,
we are unable to determine the root cause for each individual
leakage, but we are able to leverage our dataset to analyze
a number of potential contributing factors. Ultimately, we
ﬁnd that most secret leakage is likely caused by committed
cryptographic key ﬁles and API keys embedded in code.
A. Repository and Contributor Statistics
In order to evaluate whether the metadata of repos contain-
ing secrets and information about their owners could reveal
interesting insights, we sourced such data from the GitHub API.
Fig. 4: Most of the regular expressions had high precision
with minimal rejection by the ﬁlter
For each repo, we collected the number of forks, watchers, and
contributors. We chose these features as they indicate the level
of activity, and it may be the case that more active repos are
less (or more) likely to leak secrets. For each repo contributor,
we queried the user’s number of public repos, contributions in
the repo, and contributions in the past year across GitHub.
We chose these features because they act as a proxy for
developer experience with GitHub, and we question whether
less experienced developers are more likely to leak secrets or if
it occurs irrespective of experience. For a comparison, we also
collected a control group by randomly sampling approximately
100,000 GitHub repos and collecting the same information.
With both sets of metadata, we conducted a Mann-Whitney
U-test to determine if there were meaningful differences on the
variables. Our control dataset featured 99,878 repos and 360,074
developers, while our leakage dataset featured 95,456 repos
and 180,101 developers. Because we examine large datasets,
slight differences in the datasets have a strong probability of
passing statistical signiﬁcance tests. We chose a signiﬁcance
threshold of α = 0.01, and applied a conservative Bonferroni
correction to α = 0.01
10 = 0.001 to account for all of our
planned hypothesis tests.
We found no meaningful differences on the variables in
the repo metadata that leak secrets compared to a randomly
selected control dataset. All tests were statistically signiﬁcant
(p  0.64). This implies that secrets rarely propagate
Fig. 5: Most detected secrets were found in cryptographic or
source code ﬁles
by forks, and that most leaks are in an original repo.
B. Top File Types
Since certain usage patterns may be more prone to secret
leakage, a method of approximating this is to evaluate the
most common types of ﬁles containing secrets. As there are
many different ﬁle extensions, we grouped each extension
into one of several deﬁned categories. The “crypto” category
contained ﬁles commonly used for storing keys and certiﬁcates
(e.g., .key), “code” contained source code ﬁles (e.g., .py),
“data” contained ﬁles used to serialize data (e.g., .csv), and
“conﬁg” contained ﬁles used primarily for conﬁguration (e.g.,
.conf). All other ﬁles with extensions were grouped into
“other”, while ﬁles without extensions were grouped into “none”.
For a detailed breakdown of the extensions included in each
group, see Appendix Section A.
Figure 5 shows the relative share of ﬁles for each category
in the combined dataset. Unsurprisingly, “crypto” ﬁles make
up the largest percent of the dataset because private keys are
the largest group of compromised secrets. “Code” groups also
make up a very large percentage; this indicates that many
secrets, mainly API secrets, are being embedded directly within
source code. While we cannot say from this analysis alone why
cryptographic keys are leaked, it is clear that the poor practice
of embedding secrets in code is a major root cause.
C. Personal RSA Key Leakage
Many developers store personal ﬁles in GitHub repositories,
so another research question is whether overall leakage could
largely be attributed to this. Common examples of this are
dotfiles repos, which people use to backup conﬁguration
ﬁles and folders [19]. One common “dotfolder” is .ssh, a
directory which often contains SSH keys, commonly in a
ﬁle named id_rsa. To approximate the prevalence of secret
leakage through this source, we gathered metrics on how many
RSA keys appeared within a repo containing dotfiles in
the name, within a .ssh folder, or within an id_rsa ﬁle.
In the Search dataset, we found 1676 (6.61%) of all RSA
keys appeared within an id_rsa ﬁle, 653 (2.57%) within
11
  0% 20% 40% 60% 80%100%Percent of total regular expression matchesBraintree Access TokenTwilio API KeySquare OAuth SecretMailChimp API KeyStripe Standard API KeyTwitter Access TokenMailGun API KeyAmazon AWS Access Key IDRSA Private KeyEC Private KeyGeneral Private KeyFacebook Access TokenGoogle OAuth IDGoogle API KeyPGP Private KeyPicatic API KeySquare Access TokenPercent of Regular Expression Matches Filtered by Secret FilterNot FilteredFilteredCryptoCodeDataNoneOtherConfigFile extension category  0% 10% 20% 30% 40% 50%Percent of total filesPercent of Total Combined Seen Files for each Category of Filea .ssh folder, and 353 (1.39%) within a dotfiles repo.
In the BigQuery dataset, we found 651 (4.26%) of all RSA
keys appeared within a id_rsa ﬁle, 110 (.007%) within a
.ssh folder, and 39 (.002%) within a dotfiles repo. The
lower prevalence of this leakage in BigQuery is likely due to a
developer being unlikely to license their personal ﬁles. These
keys are also more likely to be personal keys, especially SSH
keys, which could allow an attacker to compromise a user. In
total, the low representation of dotfiles indicates this is not
the main cause of secret leakage, but it is a non-trivial factor.
D. TrufﬂeHog Analysis
Secret management is a difﬁcult task and tools exist to
assist developers. One such tool is TrufﬂeHog [59], which
is used in a local Git repository to check every commit for
secrets. TrufﬂeHog detects secrets by ﬁnding strings with an
entropy above a pre-deﬁned threshold or by matching one of a