技术本身并无好坏之分 —— 关键在于它被如何使用，以及它如何影响人们。这对枪械这样的武器是成立的，而搜寻引擎这样的软体系统与之类似。我认为，软体工程师仅仅专注于技术而忽视其后果是不够的：道德责任也是我们的责任。对道德推理很困难，但它太重要了，我们无法忽视。
### 预测性分析
举个例子，预测性分析是 “大资料” 炒作的主要内容之一。使用资料分析预测天气或疾病传播是一码事【81】；而预测一个罪犯是否可能再犯，一个贷款申请人是否有可能违约，或者一个保险客户是否可能进行昂贵的索赔，则是另外一码事。后者会直接影响到个人的生活。
当然，支付网路希望防止欺诈交易，银行希望避免不良贷款，航空公司希望避免劫机，公司希望避免雇佣效率低下或不值得信任的人。从它们的角度来看，失去商机的成本很低，而不良贷款或问题员工的成本则要高得多，因而组织希望保持谨慎也是自然而然的事情。所以如果存疑，它们通常会 Say No。
然而，随著演算法决策变得越来越普遍，被某种演算法（准确地或错误地）标记为有风险的某人可能会遭受大量这种 “No” 的决定。系统性地被排除在工作，航旅，保险，租赁，金融服务，以及其他社会关键领域之外。这是一种对个体自由的极大约束，因此被称为 “演算法监狱”【82】。在尊重人权的国家，刑事司法系统会做无罪推定（预设清白，直到被证明有罪）。另一方面，自动化系统可以系统地，任意地将一个人排除在社会参与之外，不需要任何有罪的证明，而且几乎没有申诉的机会。
#### 偏见与歧视
演算法做出的决定不一定比人类更好或更差。每个人都可能有偏见，即使他们主动抗拒这一点；而歧视性做法也可能已经在文化上被制度化了。人们希望根据资料做出决定，而不是透过人的主观评价与直觉，希望这样能更加公平，并给予传统体制中经常被忽视的人更好的机会【83】。
当我们开发预测性分析系统时，不是仅仅用软体透过一系列 IF ELSE 规则将人类的决策过程自动化，那些规则本身甚至都是从资料中推断出来的。但这些系统学到的模式是个黑盒：即使资料中存在一些相关性，我们可能也压根不知道为什么。如果演算法的输入中存在系统性的偏见，则系统很有可能会在输出中学习并放大这种偏见【84】。
在许多国家，反歧视法律禁止按种族、年龄、性别、性取向、残疾或信仰等受保护的特征区分对待不同的人。其他的个人特征可能是允许用于分析的，但是如果这些特征与受保护的特征存在关联，又会发生什么？例如在种族隔离地区中，一个人的邮政编码，甚至是他们的 IP 地址，都是很强的种族指示物。这样的话，相信一种演算法可以以某种方式将有偏见的资料作为输入，并产生公平和公正的输出【85】似乎是很荒谬的。然而这种观点似乎常常潜伏在资料驱动型决策的支持者中，这种态度被讽刺为 “在处理偏差上，机器学习与洗钱类似”（machine learning is like money laundering for bias）【86】。
预测性分析系统只是基于过去进行推断；如果过去是歧视性的，它们就会将这种歧视归纳为规律。如果我们希望未来比过去更好，那么就需要道德想象力，而这是只有人类才能提供的东西【87】。资料与模型应该是我们的工具，而不是我们的主人。
#### 责任与问责
自动决策引发了关于责任与问责的问题【87】。如果一个人犯了错误，他可以被追责，受决定影响的人可以申诉。演算法也会犯错误，但是如果它们出错，谁来负责【88】？当一辆自动驾驶汽车引发事故时，谁来负责？如果自动信用评分算法系统性地歧视特定种族或宗教的人，这些人是否有任何追索权？如果机器学习系统的决定要受到司法审查，你能向法官解释演算法是如何做出决定的吗？
收集关于人的资料并进行决策，信用评级机构是一个很经典的例子。不良的信用评分会使生活变得更艰难，但至少信用分通常是基于个人 **实际的** 借款历史记录，而记录中的任何错误都能被纠正（尽管机构通常会设定门槛）。然而，基于机器学习的评分演算法通常会使用更宽泛的输入，并且更不透明；因而很难理解特定决策是怎样作出的，以及是否有人被不公正地，歧视性地对待【89】。
信用分总结了 “你过去的表现如何？”，而预测性分析通常是基于 “谁与你类似，以及与你类似的人过去表现的如何？”。与他人的行为画上等号意味著刻板印象，例如，根据他们居住的地方（与种族和阶级关系密切的特征）。那么那些放错位置的人怎么办？而且，如果是因为错误资料导致的错误决定，追索几乎是不可能的【87】。
很多资料本质上是统计性的，这意味著即使机率分布在总体上是正确的，对于个例也可能是错误的。例如，如果贵国的平均寿命是 80 岁，这并不意味著你在 80 岁生日时就会死掉。很难从平均值与机率分布中对某个特定个体的寿命作出什么判断，同样，预测系统的输出是机率性的，对于个例可能是错误的。
盲目相信资料决策至高无上，这不仅仅是一种妄想，而是有切实危险的。随著资料驱动的决策变得越来越普遍，我们需要弄清楚，如何使演算法更负责任且更加透明，如何避免加强现有的偏见，以及如何在它们不可避免地出错时加以修复。
我们还需要想清楚，如何避免资料被用于害人，如何认识资料的积极潜力。例如，分析可以揭示人们生活的财务特点与社会特点。一方面，这种权力可以用来将援助与支援集中在帮助那些最需要援助的人身上。另一方面，它有时会被掠夺性企业用于识别弱势群体，并向其兜售高风险产品，比如高利贷和没有价值的大学文凭【87,90】。
#### 反馈回圈
即使是那些对人直接影响比较小的预测性应用，比如推荐系统，也有一些必须正视的难题。当服务变得善于预测使用者想要看到什么内容时，它最终可能只会向人们展示他们已经同意的观点，将人们带入滋生刻板印象，误导资讯，与极端思想的 **回音室**。我们已经看到过社交媒体回音室对竞选的影响了【91】。
当预测性分析影响人们的生活时，自我强化的反馈回圈会导致非常有害的问题。例如，考虑雇主使用信用分来评估候选人的例子。你可能是一个信用分不错的好员工，但因不可抗力的意外而陷入财务困境。由于不能按期付账单，你的信用分会受到影响，进而导致找到工作更为困难。失业使你陷入贫困，这进一步恶化了你的分数，使你更难找到工作【87】。在资料与数学严谨性的伪装背后，隐藏的是由恶毒假设导致的恶性回圈。
我们无法预测这种反馈回圈何时发生。然而透过对整个系统（不仅仅是计算机化的部分，而且还有与之互动的人）进行整体思考，许多后果是可以够预测的 —— 一种称为 **系统思维（systems thinking）** 的方法【92】。我们可以尝试理解资料分析系统如何响应不同的行为，结构或特性。该系统是否加强和增大了人们之间现有的差异（例如，损不足以奉有余，富者愈富，贫者愈贫），还是试图与不公作斗争？而且即使有著最好的动机，我们也必须当心意想不到的后果。
### 隐私和追踪
除了预测性分析 —— 使用资料来做出关于人的自动决策 —— 资料收集本身也存在道德问题。收集资料的组织，与被收集资料的人之间，到底属于什么关系？
当系统只储存使用者明确输入的资料时，是因为使用者希望系统以特定方式储存和处理这些资料，**系统是在为使用者提供服务**：使用者就是客户。但是，当用户的活动被跟踪并记录，作为他们正在做的其他事情的副作用时，这种关系就没有那么清晰了。该服务不再仅仅完成使用者想要它要做的事情，而是服务于它自己的利益，而这可能与使用者的利益相冲突。
追踪使用者行为资料对于许多面向用户的线上服务而言，变得越来越重要：追踪使用者点选了哪些搜寻结果有助于改善搜寻结果的排名；推荐 “喜欢 X 的人也喜欢 Y”，可以帮助使用者发现实用有趣的东西；A/B 测试和使用者流量分析有助于改善使用者介面。这些功能需要一定量的使用者行为跟踪，而使用者也可以从中受益。
但不同公司有著不同的商业模式，追踪并未止步于此。如果服务是透过广告盈利的，那么广告主才是真正的客户，而使用者的利益则屈居其次。跟踪的资料会变得更详细，分析变得更深入，资料会保留很长时间，以便为每个人建立详细画像，用于营销。
现在，公司与被收集资料的使用者之间的关系，看上去就不太一样了。公司会免费服务使用者，并引诱使用者尽可能多地使用服务。对使用者的追踪，主要不是服务于该使用者个体，而是服务于掏钱资助该服务的广告商。我认为这种关系可以用一个更具罪犯内涵的词来恰当地描述：**监视（surveilance）**。
#### 监视