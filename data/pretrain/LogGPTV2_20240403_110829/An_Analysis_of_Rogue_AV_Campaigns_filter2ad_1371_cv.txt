M
1
F
0
1
.
8
0
.
6
0
.
.
4
0
.
2
0
e
r
u
s
a
e
M
1
F
8
0
.
.
4
0
0
0
.
d =
1
2
4
6
8
10
f=0.5
f=0.2
f=0.1
f=0.05
20
50
100
200
500
1000
2000
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Community Size (n)
Threshold Multiplier k (V=mean+k*sd)
Fig. 13. F1 measure with varying com-
munity size and constant fraction f = d/n
infected, using TextEdit and prompttext
Fig. 14. F1 measure with n = 100 and
varying threshold multiplier using traces
from Mail and the mailspam exploit
Adium, then takes a snapshot of the current display (like prompttext but with-
out user interaction); and showpages infects Camino, then loads a series of web
pages (based on HTML proxies like Sobig’s trojan, DoS payloads like Code Red
and Yaha, and self-updating payloads like W32/sonic and W32/hybris [6]).
Except where noted, we gathered data using an Apple MacPro with two 2.66
GHz Dual-Core Intel Xeons and 6 GB of memory running Mac OS X 10.5.4,
and the results we present are representative. Using the resulting model, we
compute the distribution X of healthy client anomaly scores for each program
(Figure 10). The results of Section 5.2 show that behavioral variance comes from
client behavior over time, rather than client heterogeneity; the smartest way to
gather a good data set was, therefore, to monitor a single client for a long time.
Section 6.4 provides experiments supporting the merit of that decision.
We use the phrase “normal usage” to mean that no artiﬁcial workloads were
generated nor were certain activities prescribed. As is evident from the rate of
new sequences seen during training, plotted in Figure 11, we made no eﬀort
to train until convergence, nor to exercise the rarer features of these programs.
We also do not separate sequences by thread, instead ordering them strictly
by time of invocation. The resulting models are therefore small, imprecise, and
incomplete, as we might expect to achieve in practice; the Syzygy performance
numbers we present would only improve with better models.
6.2 Detection Performance
We ﬁrst consider Syzygy’s ability to detect epidemics for various sizes of com-
munity and infected population. Consider the experiments plotted in Figure 12
376
A.J. Oliner, A.V. Kulkarni, and A. Aiken
wherein a ﬁxed-size community is being infected. Syzygy’s performance improves
with infection size, peaking, in this experiment, at around 10 exploited clients
(10% of the community). Figure 13 shows, however, that with a suﬃciently
large community we require a vanishingly small fraction of the population to be
sacriﬁced before we detect the exploit. Although the community and infected
population are growing at the same rate, Syzygy’s ability to detect the infection
outpaces that growth.
6.3 Parameter Sensitivity
We next evaluate Syzygy’s sensitivity to the threshold V . Figure 14 shows per-
formance for various choices of V . Once the community and infected population
are suﬃciently large, we see the performance curve reach a maximum at a point
between V = μX and μY . Increasing the multiplier tends to increase precision,
decrease recall, and decrease the false positive rate (which falls oﬀ like the tail of
the normal distribution). To further visualize this, see Figure 15. As the number
of clients grows, the normal and infected distributions become more clearly sep-
arated. This increasing noise margin suggests that the exact placement of the
threshold does not strongly aﬀect Syzygy’s performance. Indeed, in the limit, all
choices of threshold μX < V < μY yield perfect detection.
y
t
i
s
n
e
D
2
.
1
8
.
0
4
.
0
0
.
0
TextEdit
Prompttext
y
t
i
s
n
e
D
3
2
1
0
TextEdit
Prompttext
0
1
2
3
4
5
6
1
Anomaly Score (n=1)
2
3
4
Average Anomaly Score (n=20)
5
Fig. 15. The left plot shows anomaly signal density estimates for TextEdit and the
prompttext exploit. There is no ideal position on the x-axis to set a threshold. On the
right, we see that averaging scores across a number of clients yields a clearer separation.
6.4 Client Variation
We expect clients to diﬀer in machine speciﬁcations and conﬁgurations, and for
these to change over time. To test this situation, we ran the same applications
as on our primary test machine (System A) on a second system (System B)
with diﬀerent speciﬁcations: an Apple PowerBook G4 with a single 1.33 GHz
PowerPC processor and 1.25 GB of memory running Mac OS X 10.5.4. The
data is summarized in Table 3. In Figure 16, we compare the anomaly scores for
these Adium traces against those from the training system and the screenshot
exploit. Although System B’s average score is higher by Δ (its model is from
another system), the programs behave similarly enough on both systems that
unusual but healthy clients are not easily confused with exploits.
Community Epidemic Detection Using Time-Correlated Anomalies
377
y
t
i
s
n
e
D
2
.
1
8
.
0
4
.
0
0
.
0
Adium (System A)
Adium (System B)
Screenshot
y
t
i
s
n
e
D
2
.
1
8
.
0
4
.
0
0
.
0
2
4
6
8
10
2
3
Anomaly Score (n=1)
4
6
Average Anomaly Score (n=20)
5
Adium (System A)
Adium (System B)
Screenshot
7
8
Fig. 16. Similar to Figure 15, except using the Adium program and giving data for
both our primary system (System A) and the laptop (System B). All curves are based
on the Adium model built using only System A.
Table 3. Data from OS X apps on a diﬀerent client. The Unique column indicates the
number of unique length-six sequences, and Ti is the maximum time from the begin-
ning of one system call to the start of the next. The Δ column shows the empirically
estimated average diﬀerence between anomaly scores on Systems A and B.
Program
Version Time (sec) Rate (calls/sec) Unique Ti (sec)
Adium
Camino
Mail
TextEdit
1.2.7
1.6.1Int-v2
3.3 (926.1/926)
1.5 (244)
2093
3901
1126
2506
54.8839
868.294
16.2869
92.8204
≈ Δ
6749 47.457 0.31589
21,619 1.84077 0.60442
7963 421.645 0.53272
2925 528.164 1.17758
As the community grows, however, System B begins looking like an exploit.
The healthy community score distribution variance, σH, shrinks, so V moves
closer to μX, slowly passing below System B’s average anomaly score. This
contrived problem is easily remedied by using a model constructed from System
B’s behavior rather than System A’s, or by normalizing the anomaly scores from
System B as prescribed in Section 3.2. In practice, such a situation may arise
when a client upgrades the application but does not retrain the model; if a client’s
anomaly signal remains high for long periods of time, this may indicate that the
model is no longer valid—only when many clients make such changes would we
expect spurious epidemic reports. Section 5 contains additional results related
to client variation that suggest heterogeneity is not a problem in practice.
6.5 Mimicry and Tainting
An exploit can avoid detection if its behavior is suﬃciently similar to the ap-
plication’s, from the perspective of a given model [38]. There are two ways an
exploit might mimic application behavior: (i) by ensuring that the distribution
of anomaly scores is suﬃciently similar or (ii) by limiting the rate at which it
exhibits bad behavior. Perfect mimicry, in which exploit behavior is indistin-
guishable from application behavior, can never be detected, by deﬁnition, using
any behavior-based epidemic detector; however, we can show Syzygy is robust
against a very high degree of mimicry and against rate-limiting an attack.
Scenario (i), mimicking the distribution, is quantiﬁed in Syzygy by the pa-
rameter δ. Recall that a lower value for δ means the two distributions are more
378
A.J. Oliner, A.V. Kulkarni, and A. Aiken
n =
20
50
100
200
500
1000
2000
e
r
u
s
a
e
M
1
F
0
1
.
8
0
.
6
0
.
4
0
.
2
0
.
e
r
u
s
a
e
M
1
F
0
1
.
8
0
.