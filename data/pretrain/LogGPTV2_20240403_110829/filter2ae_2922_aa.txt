**作者：w8ay  
原文链接：**
> crawlergo是一个使用`chrome
> headless`模式进行URL收集的浏览器爬虫。它对整个网页的关键位置与DOM渲染阶段进行HOOK，自动进行表单填充并提交，配合智能的JS事件触发，尽可能的收集网站暴露出的入口。内置URL去重模块，过滤掉了大量伪静态URL，对于大型网站仍保持较快的解析与抓取速度，最后得到高质量的请求结果集合。
>
> crawlergo 目前支持以下特性：
>
> * 原生浏览器环境，协程池调度任务
>
> * 表单智能填充、自动化提交
>
> * 完整DOM事件收集，自动化触发
>
> * 智能URL去重，去掉大部分的重复请求
>
> * 全面分析收集，包括javascript文件内容、页面注释、robots.txt文件和常见路径Fuzz
>
> * 支持Host绑定，自动添加Referer
>
> * 支持请求代理，支持爬虫结果主动推送
Github: 
作者开源了源码，我是很兴奋的，以前也有写一个的想法，但是开源的动态爬虫不多，看了其中几个。
## 调研
  * 递归dom搜索引擎
  * 发现ajax/fetch/jsonp/websocket请求
  * 支持cookie，代理，ua，http auth
  * 基于文本相似度的页面重复数据删除引擎
  * 根据文本长度 
  * 一个操作chrome headless的go库
  * 它比官方提供的chrome操作库更容易使用
  * 有效解决了chrome残留僵尸进程的问题
  * 通过一些通用接口获取url信息
  * Web静态爬虫，也提供了一些方法获取更多URL
1.虽然没有开源，但是它里面使用yaml进行的配置选项很多，通过配置选项可以大致知道它的一些特性。  
2.可以手动登陆  
3.启用图片  
4.显示对爬取url的一些限制
1.不允许的文件后缀  
2.不允许的url关键字  
3.不允许的域名  
4.不允许的url
设置下个页面最大点击和事件触发
## Crawlergo
之前也想过写一个动态爬虫来对接扫描器，但是动态爬虫有很多细节都需要打磨，一直没时间做，现在有现成的源码参考能省下不少事。
主要看几个点
  * 对浏览器 JavaScript环境的hoook
  * dom的触发，表单填充
  * url如何去重 
  * url的收集
目录结构
    ├─cmd
    │  └─crawlergo # 程序主入口
    ├─examples
    ├─imgs
    └─pkg
        ├─config  # 一些配置相关
        ├─engine  # chrome相关程序
        ├─filter  # 去重相关
        ├─js      # 一些注入的js
        ├─logger  # 日志
        ├─model   # url和请求相关的库
        └─tools   # 一些通用类库
            └─requests
根据源码的调用堆栈做了一个程序启动流程图
## 配置文件
`pkg/config/config.go`定义了一些默认的配置文件
    const (
        DefaultUA               = "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.0 Safari/537.36"
        MaxTabsCount            = 10
        TabRunTimeout           = 20 * time.Second
        DefaultInputText        = "Crawlergo" // 默认输入的文字
        FormInputKeyword        = "Crawlergo" // form输入的文字，但是代码中没有引用这个变量的
        SuspectURLRegex         = `(?:"|')(((?:[a-zA-Z]{1,10}://|//)[^"'/]{1,}\.[a-zA-Z]{2,}[^"']{0,})|((?:/|\.\./|\./)[^"'>= 32 {
                pathParts[index] = TooLongMark
            } else if onlyNumberRegex.MatchString(numSymbolRegex.ReplaceAllString(part, "")) {
                pathParts[index] = NumberMark
            } else if strings.HasSuffix(part, ".html") || strings.HasSuffix(part, ".htm") || strings.HasSuffix(part, ".shtml") {
                part = htmlReplaceRegex.ReplaceAllString(part, "")
                // 大写、小写、数字混合
                if numberRegex.MatchString(part) && alphaUpperRegex.MatchString(part) && alphaLowerRegex.MatchString(part) {
                    pathParts[index] = MixAlphaNumMark
                    // 纯数字
                } else if b := numSymbolRegex.ReplaceAllString(part, ""); onlyNumberRegex.MatchString(b) {
                    pathParts[index] = NumberMark
                }
                // 含有特殊符号
            } else if s.hasSpecialSymbol(part) {
                pathParts[index] = MixSymbolMark
            } else if chineseRegex.MatchString(part) {
                pathParts[index] = ChineseMark
            } else if unicodeRegex.MatchString(part) {
                pathParts[index] = UnicodeMark
            } else if onlyAlphaUpperRegex.MatchString(part) {
                pathParts[index] = UpperMark
                // 均为数字和一些符号组成
            } else if b := numSymbolRegex.ReplaceAllString(part, ""); onlyNumberRegex.MatchString(b) {
                pathParts[index] = NumberMark
                // 数字出现的次数超过3，视为伪静态path
            } else if b := OneNumberRegex.ReplaceAllString(part, "0"); strings.Count(b, "0") > 3 {
                pathParts[index] = MixNumMark
            }
        }
        newPath := strings.Join(pathParts, "/")
        return newPath
    }
### 基于网页结构去重
作者原帖中的基于网页结构去重写的非常精彩 
参考的论文下载: 
作者的将网页特征向量抽离出来，索引存储，用来判断大量网页的相似度，非常惊艳，未来用到资产收集系统或者网络空间引擎上也都是非常不错的选择。
## URL收集
### robots
在爬虫之前，会先请求robots.txt，解析出所有链接，加入到待爬取页面。
源码中使用了一个正则来匹配
    var urlFindRegex = regexp.MustCompile(`(?:Disallow|Allow):.*?(/.+)`)
看了下robots规范:，应该还可以再优化一下，来处理一些表达式。
### DIR FUZZ
在爬虫之前，如果没有指定dir字典的话，默认会使用内置的字典
    ['11', '123', '2017', '2018', 'message', 'mis', 'model', 'abstract', 'account', 'act', 'action', 'activity', 'ad', 'address', 'ajax', 'alarm', 'api', 'app', 'ar', 'attachment', 'auth', 'authority', 'award', 'back', 'backup', 'bak', 'base', 'bbs', 'bbs1', 'cms', 'bd', 'gallery', 'game', 'gift', 'gold', 'bg', 'bin', 'blacklist', 'blog', 'bootstrap', 'brand', 'build', 'cache', 'caches', 'caching', 'cacti', 'cake', 'captcha', 'category', 'cdn', 'ch', 'check', 'city', 'class', 'classes', 'classic', 'client', 'cluster', 'collection', 'comment', 'commit', 'common', 'commons', 'components', 'conf', 'config', 'mysite', 'confs', 'console', 'consumer', 'content', 'control', 'controllers', 'core', 'crontab', 'crud', 'css', 'daily', 'dashboard', 'data', 'database', 'db', 'default', 'demo', 'dev', 'doc', 'download', 'duty', 'es', 'eva', 'examples', 'excel', 'export', 'ext', 'fe', 'feature', 'file', 'files', 'finance', 'flashchart', 'follow', 'forum', 'frame', 'framework', 'ft', 'group', 'gss', 'hello', 'helper', 'helpers', 'history', 'home', 'hr', 'htdocs', 'html', 'hunter', 'image', 'img11', 'import', 'improve', 'inc', 'include', 'includes', 'index', 'info', 'install', 'interface', 'item', 'jobconsume', 'jobs', 'json', 'kindeditor', 'l', 'languages', 'lib', 'libraries', 'libs', 'link', 'lite', 'local', 'log', 'login', 'logs', 'mail', 'main', 'maintenance', 'manage', 'manager', 'manufacturer', 'menus', 'models', 'modules', 'monitor', 'movie', 'mysql', 'n', 'nav', 'network', 'news', 'notice', 'nw', 'oauth', 'other', 'page', 'pages', 'passport', 'pay', 'pcheck', 'people', 'person', 'php', 'phprpc', 'phptest', 'picture', 'pl', 'platform', 'pm', 'portal', 'post', 'product', 'project', 'protected', 'proxy', 'ps', 'public', 'qq', 'question', 'quote', 'redirect', 'redisclient', 'report', 'resource', 'resources', 's', 'save', 'schedule', 'schema', 'script', 'scripts', 'search', 'security', 'server', 'service', 'shell', 'show', 'simple', 'site', 'sites', 'skin', 'sms', 'soap', 'sola', 'sort', 'spider', 'sql', 'stat', 'static', 'statistics', 'stats', 'submit', 'subways', 'survey', 'sv', 'syslog', 'system', 'tag', 'task', 'tasks', 'tcpdf', 'template', 'templates', 'test', 'tests', 'ticket', 'tmp', 'token', 'tool', 'tools', 'top', 'tpl', 'txt', 'upload', 'uploadify', 'uploads', 'url', 'user', 'util', 'v1', 'v2', 'vendor', 'view', 'views', 'web', 'weixin', 'widgets', 'wm', 'wordpress', 'workspace', 'ws', 'www', 'www2', 'wwwroot', 'zone', 'admin', 'admin_bak', 'mobile', 'm', 'js']
根据状态码判断
### 爬虫时的url收集
  * 解析流量中的url
  * 获取xmr类型的请求
  * 正则解析js、html、json中url 
  * 收集当前页面上的url信息
    "src", "href", "data-url", "data-href"
    object[data]
    注释中的url
## 对浏览器环境的hook
在chrome的tab初始化时，会执行一段js代码，hook部分函数和事件来控制js的运行环境。
同时定义了一个js全局函数`addLink`、`Test`，通过这个函数可以与go进行交互。