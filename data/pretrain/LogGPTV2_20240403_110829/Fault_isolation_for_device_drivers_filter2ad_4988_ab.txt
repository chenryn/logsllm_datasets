ity. Preliminary measurements comparing MINIX 3 against
Linux and FreeBSD show an overhead of roughly 10–25%,
but the performance can no doubt be improved through
careful analysis and removal of bottlenecks. Independent
studies have already addressed this issue and shown that
the overhead incurred by modular designs can be limited
to 5–10% [11, 14, 20, 22]. However, instead of focusing on
performance, the issue we have tried to address is isolating
untrusted drivers that threaten OS dependability.
This section ﬁrst classiﬁes the privileged operations
drivers need and then presents per class the isolation tech-
niques MINIX 3 employs to enforce least authority.
4.1 Classiﬁcation of Driver Privileges
The starting point for our discussion is the classiﬁcation
of potentially dangerous driver operations shown in Fig. 3.
At the lowest level, CPU usage should be controlled in order
to prevent bypassing higher-level protection mechanisms.
For example, consider kernel-mode CPU instructions that
can be used to reset page tables or excessive use of CPU
time by a driver that winds up in an inﬁnite loop.
Unauthorized memory access is an important threat with
drivers that commonly exchange data with other parts of the
system and may engage in direct memory access (DMA).
Indeed, ﬁeld research has shown that memory corruption
is one of the most important causes (27%) of system out-
ages [35]. In 15% of the crashes the corruption is so severe
that the underlying cause cannot be deduced [28].
It is important to restrict access to I/O ports and regis-
ters and device memory in order to prevent unauthorized
access and resource conﬂicts. Programming device hard-
ware is complex due to its low-level interactions and lack
of documentation [30]. Especially the asynchronous nature
of interrupt handling can be hard to get correct, as evidenced
by the error IRQL NOT LESS OR EQUAL that was found to
cause 26% of all Windows XP crashes [10].
Interprocess communication (IPC) allows servers and
drivers running in separate protection domains to cooperate,
but dealing with unreliable and potentially hostile senders
and receivers is a challenge [18]. A related power built on
top of the IPC infrastructure, which routes requests through
the system, is requesting (privileged) OS services.
Privileges
(Class I) CPU Usage
+ Privileged instructions → User-mode processes
+ CPU time
Isolation Techniques
See Sec. 4.2.1
→ Feedback-queue scheduler
See Sec. 4.2.2
→ Address-space separation
→ Run-time memory granting
→ IOMMU protection
See Sec. 4.2.3
→ Per-driver I/O policy
→ User-level IRQ handling
See Sec. 4.2.4
→ Per-driver IPC policy
→ Per-driver call policy
(Class II) Memory access
+ Memory references
+ Copying and sharing
+ Direct memory access
(Class III) Device I/O
+ Device access
+ Interrupt handling
(Class IV) System services
+ Low-level IPC
+ OS services
Figure 3: Classiﬁcation of privileged operations needed by low-
level drivers and summary of MINIX 3’s defense mechanisms.
4.2 Per-Class Isolation Techniques
We now describe how MINIX 3 isolates drivers. In short,
each driver is run in an unprivileged UNIX process, but
based on the driver’s needs, we can selectively grant ﬁne-
grained access to each of the privileged resources in Fig. 3.
We believe that UNIX processes are attractive, since they
are lightweight, well-understood, and have proven to be an
effective model for encapsulating untrusted code.
4.2.1 Class-I Restrictions—CPU Usage
Privileged Instructions All drivers are runs in an ordi-
nary UNIX process with user-mode CPU privileges, just
like normal application programs. This prevents drivers
from executing privileged CPU instructions such as chang-
ing memory maps, performing I/O, or halting the CPU.
Only a tiny microkernel runs with kernel-mode CPU priv-
ileges and a small set of kernel calls is exported to allow
access to privileged services in a controlled manner.
CPU Time With drivers running as UNIX processes, nor-
mal process scheduling techniques can be used to prevent
CPU hogging. In particular, we use a multilevel-feedback-
queue scheduler (MLFQ). Processes with the same priority
reside in the same queue and are scheduled round-robin.
Starvation of low-priority processes is prevented by degrad-
ing a process’ priority after it consumes a full quantum.
Since CPU-bound processes are penalized more often, in-
teractive applications have good response times. Periodi-
cally, all priorities are increased if not at their initial value.
Two additional protection mechanisms exist. First, the
driver manager can be conﬁgured to periodically check the
driver’s state and start a fresh copy if it does not respond to
heartbeat requests, for example, if it winds up in an inﬁnite
loop [17]. Second, a resource reservation framework is pro-
vided in order to provide more stringent temporal protection
for processes with real-time requirements [23].
4.2.2 Class-II Restrictions—Memory Access
Memory References We use MMU-hardware protection
to enforce strict address-space separation. Each driver has
a private, virtual address space with a ﬁxed size depending
on the driver’s requirements. The MMU translates CPU-
visible addresses to physical addresses using the MMU ta-
bles controlled by the kernel. Unauthorized memory ref-
erences outside of the driver’s address space result in an
MMU exception and cause the driver to be killed.
Drivers that want to exchange data could potentially use
page sharing, but, although efﬁcient, with page sizes start-
ing at 4 KB the protection is too coarse-grained to share
safely small data structures. Therefore, we developed the
ﬁne-grained authorization mechanism discussed next.
Copying and Sharing We allow safe data exchange by
means of ﬁne-grained, delegatable memory grants. Each
grant deﬁnes a memory area with byte granularity and gives
a speciﬁc other process permission to read and/or write the
speciﬁed data. A process that wants to grant another pro-
cess access to its address space must create a grant table
to store the memory grants. On ﬁrst use, the kernel must
be informed about the location and size of the grant table.
After creating a memory grant it can be made available to
another process by sending an IPC message that contains
an index into the table, known as a grant ID. The grant
then is uniquely identiﬁed by the grantor’s process ID plus
grant ID. The receiver, say, B of a grant from A can re-
ﬁne and transfer its access rights to a third process C by
means of an indirect grant. This results in a hierarchical
structure as shown in Fig. 4. This resembles recursive ad-
dress spaces [22], but memory grants are different in their
purpose, granularity, and usage—since grants protect data
structures rather than build process address spaces.
A
B
Direct
Grant
ID = 1
Indirect
Grants
IDs = 1,4
l
e
b
a
T
t
n
a
r
G
s
A
’
5
4
3
2
1
0
...
...
...
...
B:R+W
...
l
e
b
a
T
t
n
a
r
G
s
B
’
5
4
3
2
1
0
...
C:R
...
...
D:R+W
...
Address Space of Process A
A:0x400
A:0x500
A:0x600
A allows B to Read+Write
512 B
A:0x440
A:0x500
C can Read
192 B
A:0x4c0
A:0x5c0
D can Read+Write
256 B
Figure 4: Hierarchical structure of memory grants. Process A
directly grants B access to a part of its memory; C can access
subparts of A’s memory through indirect grants created by B.
The SAFECOPY kernel call is provided to copy between
a driver’s local address space and a memory area granted by
another process. Upon receiving the request message, the
kernel extracts the grant ID and process ID, looks up the
corresponding memory grant, and veriﬁes that the caller is
indeed listed as the grantee. Indirect grants are processed
using a recursive lookup of the original, direct grant. The
overhead of these steps is small, since the kernel can di-
rectly access all physical memory to read from the grant
tables; no context switching is needed to follow the chain.
The request is checked against the minimal access rights
found in the path to the direct grant. If access is granted,
the kernel calculates the physical source and destination ad-
dresses and copies the requested amount of data. This de-
sign allows granting a speciﬁc driver access to a precisely
deﬁned memory region with perfect safety. If needed, cer-
tain non-copying page-level performance optimizations are
possible for large pieces of memory.
Direct Memory Access DMA from I/O devices can be
restricted in various ways. One way to prevent invalid
DMA is to restrict a driver’s I/O capabilities to deny ac-
cess to the motherboard’s DMA controller used by ISA de-
vices and have a trusted DMA driver mediate all access
attempts. However, this approach is impractical for PCI
devices using bus-mastering DMA, since it requires each
PCI device to be checked for DMA capabilities. Therefore,
we relied on modern hardware where the peripheral bus is
equipped with an IOMMU that controls all DMA attempts.
Rejected DMA writes are simply not executed, whereas re-
jected DMA reads ﬁll the device buffer with ones.
A driver that wants to use DMA needs to send a SET-
IOMMU request the trusted IOMMU driver in order to pro-
gram the IOMMU. Only DMA into the driver’s own address
space is allowed. Before setting up the IOMMU tables the
IOMMU driver veriﬁes this requirement by checking the
driver’s memory map through the UMAP kernel call. It also
ensures that the memory is pinned. When the DMA transfer
completes, the driver can copy the data from its own address
space into the address space of its client using the memory-
grant scheme discussed above. An extension outside the
scope of this paper is to use memory grants to program the
IOMMU. This improves ﬂexibility and performance, since
a driver could safely perform DMA directly into a buffer in
another process’ address space.
4.2.3 Class-III Restrictions—Device I/O
Device Access Since each driver typically has different
requirements, we associated each driver with an isolation
policy that grants ﬁne-grained access to the exact resources
needed. Policies are stored in simple text ﬁles deﬁned by
the administrator. Upon loading a driver the driver man-
ager reads the policy from disk and informs the kernel and
trusted OS servers, so that the restrictions can be enforced at
run-time. As an example, Fig. 5 shows the complete isola-
tion policy of the Realtek RTL8139 Ethernet driver. Below
we focus on device I/O (pci device), whereas access to sys-
tem services (ipc and kernel) is discussed in Sec. 4.2.4.
1
2
3
4
5
6
7
8
9
10
11
12
driver rtl8139
{
pci device
ipc
kernel
};
# ISOLATION POLICY
10ec/8139
;
KERNEL PM DS RS
INET PCI IOMMU TTY
;
DEVIO IRQCTL UMAP MAPDMA
SETGRANT SAFECOPY
TIMES SETALARM GETINFO
;
Figure 5: Per-driver policy deﬁnition is done using simple text
ﬁles. This is the complete isolation policy for the RTL8139 driver.
The speciﬁcation of I/O resources is different for PCI
and ISA devices. For PCI devices, the keys pci device
and pci class grant access to one speciﬁc PCI device or a
class of PCI devices, respectively. Upon loading a driver
the driver manager reports these keys to the trusted PCI-bus
driver, which dynamically determines the permissible I/O
resources by querying the PCI device’s conﬁguration space
initialized by the BIOS. For ISA devices, the keys io and
irq statically conﬁgure the I/O resources by explicitly list-
ing the permissible I/O ports and IRQ lines in the policy. In
both cases, the kernel is informed about the I/O resources
using the PRIVCTL kernel call and stores the privileges in
the process table before the driver gets to run.
If a driver requests I/O, the kernel ﬁrst veriﬁes that the
operation is permitted. For devices with memory-mapped
I/O, the driver can request to map device-speciﬁc memory
persistently into a its address space using the MEMMAP ker-
nel call. Before setting up the mapping, however, the kernel
performs a single check against the I/O resources reported
through PRIVCTL. For devices with programmed I/O, ﬁne-
grained access control to device ports and registers is im-
plemented in the DEVIO kernel call and the vectored variant
VDEVIO. If the call is permitted, the kernel performs the ac-
tual I/O instruction(s) and returns the result(s) in the reply
message. While this introduces some kernel-call overhead,
the I/O permission bitmap on x86 CPUs was not considered
a viable alternative, because the 8-KB per-driver bitmaps
would impose a much higher demand on memory and make
context switching more expensive.
In addition, I/O per-
mission bitmaps do not exist on other architectures, which
would complicate porting.
Interrupt Handling Although the lowest-level interrupt
handling must be done by the kernel, all device-speciﬁc
processing is done local to each driver in user space. This
is important because programming the hardware and inter-
rupt handling in particular are difﬁcult and relatively error-
prone [10]. Unfortunately, PCI devices with shared IRQ
lines can still introduce inter-driver dependencies that vio-
late least authority, as described in Sec. 3.
A user-space driver can register for interrupt notiﬁca-
tions for a speciﬁc IRQ line through the IRQCTL kernel
call. Before setting up the association, however, the kernel