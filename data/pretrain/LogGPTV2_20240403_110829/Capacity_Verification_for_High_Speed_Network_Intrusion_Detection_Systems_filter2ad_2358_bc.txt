Protection 
Wrapper
Wrapper
Wrapper
Filter / Redirect 
Filter / Redirect 
Filter / Redirect 
Wrapper
Wrapper
Wrapper
IIS
IIS
IIS
Integrity
Integrity
Sensors
Sensors
Network
Network
Sensors
Sensors
Enclave 
LAN & 
WAN
Firewall
Gateway
IP Switch
Host
Host
FGS Controller
Backup 
Backup 
Protection 
Protection 
Protection 
Wrapper
Wrapper
Wrapper
Filter / Redirect 
Filter / Redirect 
Filter / Redirect 
Wrapper
Wrapper
Wrapper
Apache
Apache
Apache
Integrity
Integrity
Sensors
Sensors
Network
Network
Sensors
Sensors
Host
Host
Application
Application
Monitor
Monitor
Application
Application
Monitor
Monitor
Monitor
Monitor
Monitor
Monitor
Policy
Server
Policy
Editor
MAC
Operator
Display
Event
Log
Out-of-Band Controller
Offline Backup
(Spare/Fishbowl)
Content
Filter
Circular
Buffer
Generalizer
Forensics
Analyzer
Out-of-Band Communication Mediator
Other Controllers
Duplicate
Duplicate
Duplicate
Duplicate
Primary
Primary
Backup
Backup
Sandbox
Sandbox
Fig. 4.  Learning Components
that  an 
intrusion  has  occurred.  Examples 
There  are  also  occasions  when  the  MAC  is  able  to  determine  relatively
unequivocally 
include  detecting
unauthorized  file  writes  or  program  executions.  In  many  cases,  the  MAC  itself  can
determine  with  high  reliability  which  connection  request  is  the  likely  attack.  For
example,  if  a  particular  request  attempts  to  write  an  unauthorized  file  or  start  an
unauthorized process, it is most  likely  an  attack. In  this  case,  the  suspect  request  is
forwarded to the Forensics Module as a prime candidate.
170
J.E. Just et al.
The  Forensics  Module  looks  in  the  circular  buffer  of  past  requests  to  identify
suspicious  requests.  Illustrative  rules  for  identifying  suspicious  web  requests  are
shown in Table 2. Rule one is the result of the fact that many buffer overflow attacks
use a repeated sequence of characters to move past the fixed length buffer. No valid
HTTP  transactions  use  methods  other  than  GET  or  POST  in  our  environment,  thus
rule  two.  This  would  obviously  need  to  change  when  other  methods  are  common.
Attempts  to  access  file  types  other  than  the  standard  set  served  are  classified  as
suspicious  by  rule  four.  Rule  five  classifies  as  suspicious  those  requests  that  use
unusually  long  commands  that  are  typically  found  in  remote  command  execution
attacks against server-side scripts. Unusual characters found in the request string are
also  a  good  indication of  a  suspect  transaction,  and  are  included  in  rule  six.  The  %
character  is  used  for  various  encoding  methods,  such  as  hex  encoding,  and  is  very
common in several classes of attacks. The + character is interpreted as a space. Many
directory  traversal  attacks  against  Microsoft  IIS  servers  include  them.  The  “..”
characters  are  also  a  sign  of  these  types  of  attacks.  The   50
2) HTTP method not GET or POST
3) Protocol header other than HTTP/1.0 or HTTP/1.1
4) File extension other than htm, html, txt, gif, jpg, jpeg, png, or css
5) Command length > 20
6) Request string contains any of  %, ?, +, .., //, <,  &gt;, &lt;, ;
The Forensics Module then determines which suspicious request (or requests) was
responsible for the observed symptoms of the attack by testing each in the Sandbox. If
there  is  no  repeatable  error,  the  Forensics  Module  returns.  If  there  is  a  repeatable
error, it has determined what request should be blocked in the future. The forensics
module then passes the known bad request to be blocked to the MAC, which calls the
UpdateBadReq method of ContentFilterBridge (which implements the Content Filter)
with the bad request as the parameter. UpdateBadReq adds the bad request to a static
bad request list in memory and writes it to the bad request file. Currently, requests are
truncated to the first two components of an HTTP request, namely, the method and
URI. 
Every time a request is received on the primary, it is forwarded to the MAC. The
MAC calls the AllowRequest method of ContentFilterBridge with the new request as
the  parameter.  The  method  checks  the  bad  request  list  for  an  exact  match  with  the
new  request.  If  there  is  a  match,  it  returns  false  to  the  MAC,  meaning  block  the
request; otherwise, it returns true.
Thus  far  the  learning  is  straight-forward  and  quite  general.  Unfortunately,  the
attack pattern that is being blocked is quite specific. If simple attack variants can be
produced easily (e.g., by changing the padding characters in a buffer overflow attack
Learning Unknown Attacks – A Start
171
or changing the order of commands in a cgi-script attack), then this specific learning
approach is easily circumvented by an attacker. What is needed  is  a  way  to  rapidly
generalize the observed attack pattern so as to block all simple variants of an attack
that are based on the same vulnerability initially exploited. This is a challenging area
and is the subject of a continuing research effort.
As a proof-of-concept, we implemented generalization for a common but prevalent
class of attacks: web server buffer overflows. Our initial approach was to enhance the
AllowRequest  method  so  that  if  an  exact  match  is  not  found,  it  then  analyzes  the
components  of  the  requests  (both  new  and  bad)  to  determine  if  the  new  request  is
"similar"  to  a  known  bad  request.  If  it  is  similar,  AllowRequest  returns  false;
otherwise, it returns true. In this way, learning is generalized from specific requests
that have been identified as bad.
In  principal,  similarity  is  rule  based  and  consists  of  two  steps:  classification  and
generalization. Classification categorizes bad requests into meaningful types such as
buffer  overflow  or  remote  command  execution  and,  as  required,  further  into  sub-
types.  Generalization  develops  a  set  of  rules  for  determining  similarity  between  an
observed  bad  request  and  a  new  request  based  on  the  classification  results.  These
rules  can  be  implemented  either  as  an  active  checking  process  or  as  comparison
templates for use by another program.
For the proof-of-concept on web server buffer overflow attacks via http requests,
we  implemented  one  rule  that  acts  as  both  a  classifier  and  a  generalizer.  It  is  the
following:
-  If (1) the query length of the bad request is greater than (256+X) [this part of the
rule classifies  the  request  as  a  buffer  overflow  type1]  and  (2)  the  methods  of  the
new request and the bad request are the same and (3) the file extensions of the new
and bad requests are the same and (4) the query length of the new request is greater
than or equal to the query length of the bad request, then return false (i.e., block the
request).
Even with X=0 in this rule, many variants of Code Red I and II are blocked. The
initial or padding characters in the query are irrelevant to how Code Red works; the
length is critical; so whether "XXX..." or "NNN..." or "XNXN..." are in the query of
the  attack,  the  attack  is  blocked.  In  addition,  the  name  of  the  file  (minus  the
extension) is also irrelevant to how Code Red works, because it is the file extension
that identifies the resource (Index Server) that is vulnerable to a buffer overflow, and
it is the query that causes the buffer overflow, not the entire URI. (The URI contains
the path identifying the resource and, optionally, the query.)
The  reason  for  the  first  condition  in  the  rule  is  to  differentiate  in  a  trivial  way
between bad requests that are buffer overflow attacks and bad requests that are some
other type of attack, like remote command execution. Unfortunately, it introduces the
possibility of false negatives, that is, a bad request that was a buffer overflow attack,
but with the overflow occurring after less than 256 characters, would be ignored as an
example to be generalized.
This  rule  has  been  constructed  from  extensive  analysis  of  buffer  overflows  in
general, buffer overflows in IIS and Apache web servers, and Code Red, in particular.
Note that it only generalizes "learned" behavior. That is, if the HACQIT cluster has
never been attacked by Code Red, it will not stop the first Code Red  attack.  It  will
1 X starts out equal to zero. Its role will be discussed later.
172
J.E. Just et al.
also  not  stop  the  first  case  of  a  variant  of  Code  Red  that  uses  the  .IDQ  extension2.
This variant would first have to be "experienced", learned as a bad request, and then
generalized by the above rule. Most importantly, the  rule does  not  prevent  use  of  a
resource  like  Index  Server;  it  prevents  a  wider  variety  of  attacks  that  exploit  an
identified vulnerability in it from reducing availability of the web server.
Although  this  rule  appears  Microsoft-oriented,  as  the  concept  of  file  extensions
does not exist under Unix, it would work against attacks exploiting vulnerabilities in
other software, such as php and perl, because these resources also use file extensions.
It might be possible to generalize this to file types under Unix. The key distinction to
be made is, does the path in the URI identify a document to be returned to the client
or does it identify an executing resource such as a search engine, a DBMS, etc.?
Finding the minimum length of padding characters for a buffer overflow attack is
not  difficult.  We  have  implemented  an  enhanced  version  of  the  forensics  and
generalization  modules  that  iteratively  tests  attack  variants  in  the  Sandbox  with
different  padding  character  lengths.  Specifically  it  successively  tests  padding
character  lengths  between  256  and  (Y-256)  where  Y  is  the  length  of  the  observed
buffer overflow padding size. From this testing, it determines the value of X (which
appears  in  the  first  condition  of  the  generalization  rule  above)  and  passes  it  to  the
ContentFilterBridge  for  inclusion  in  the  revised  generalization  rule.  The  observed
padding size is currently determined by the number of characters before the first non-
printing character (i.e., not ACSII character coder 32 through 126) in the query. While
this  is  only  an  approximation  that  depends  on  certain  assumptions  being  true,  it
proved  to  be  a  very  useful  approach  for  the  proof-of-concept  implementation.  Our
investigation with Code Red II shows the padding in the query that causes the buffer
overflow is no more than one byte over the minimum required; that is, if you remove
two characters from the query, a buffer overflow will not occur, and IIS will respond
to the request correctly and continue to function according to specification.
It is worth comparing this automatic generalization with Snort's hand-coded rules
for  preventing  Code  Red  attacks.  Snort  is  widely  used,  open  source,  lightweight
Intrusion Detection System. Immediately after the flurry of initial Code Red attacks,
Snort aficionados began crafting rules to block these attacks. It took at least two days
before rules  were  posted on  the  Snort  site.  These  were  not  generalized  and  did  not
work against trivial variants. Some three months later, the rules block on ".ida" and
".idq"  in  the  URI  and  "payload  size"  greater  than  239  [13].  The  use  of  the  file
extensions  shows  some  generalization  but  the  use  of  239  as  a  limit  on  legitimate
requests intended for Index Server in fact cause false positives because  the  payload
can be much greater than 239 (at least 373) without causing the web server to fail. 
Other improvements to generalization would use analysis based on HTTP headers
and  body  content.  These  and  other  improvements  are  the  central  focus  of  the  next
phase of research.
One  additional  aspect  of  the design  of  the  ContentFilterBridge  software  is  worth
discussing. It first calls AllowRequest with the bad request received from the MAC. If
AllowRequest returns true, that means the bad request is not on the bad request list, so
2  Index  Server  uses  file  types  indicated  by  the  extensions,  “.IDA”  and  “.IDQ”.  These  two
extensions are used by IIS to identify the Index Server resource, which is then passed either
the whole URI or the query component of the URI. The “path” component of the URI does
not affect the behavior of the Index Server, except for the file extension identifying it as the
resource target. Any file name other than “default” in “default.ida” works as well.
Learning Unknown Attacks – A Start
173
it is added. If AllowRequest returns false, this means it is on the bad request list, so it
is not added to the list. This prevents duplication.
With the addition of generalization, not only will duplicates be prevented, but also
trivial variants will not extend the bad request list to a performance-crippling length.
As there are over 21792 (or more than the number of atoms in the universe) variants of
Code Red, this is an important and effective aspect of the design.
6   Next Steps
6.1   Software Improvements
In its initial implementation, the Forensics module truncates bad requests to the first
two  components  of  the  HTTP  request,  namely,  the  method  and  URI.  This  makes
sense in the case of the buffer overflows on web servers but it needs to be enhanced
so there is a more robust way to identify the initiating event of an attack. In addition,
there is much work to do to enhance the Forensics module’s process for finding initial
attack sequences efficiently, especially for multi-request attacks. 
Similarly,  the  initial  generalization  rule  base  will  be  moved  into  a  separate
Generalization  module  that  reflects  the  architecture.  This  module  will  attempt  to
generalize  all  requests  or  patterns  returned  by  the  forensics  module  to  the  content
filter and insert specific new rules into the content filter. More broadly, we want the
Generalizer  to  be  able  to  task  the  Forensics  Module  to  run  Sandbox  tests  on  any
proposed set of filter rules and generalization parameters to what works, e.g., which
contain  the  essential  initiating  event.  In  this  way,  we  can  refine  the  generalization
while providing continued protection at the Content Filter level.
There is a great deal of work to be done in developing rules for generalizing attack
patterns  so  that  simple  attack  variants  won’t  work.  We  would  like  to  do  this  by
focusing  on  meaningful  attack  classes.  The  literature  contains  many  works  on
classifying  various  aspects  of  computer  security  including  fault  tolerance,  replay
attacks in cryptographic protocols, inference detection approaches, COTS equipment
security risks, and computer attacks. Essentially all of these authors have emphasized
that the utility of a taxonomy depends upon how well it accomplishes its purpose and
that there is no such thing as a universal taxonomy. 
Another  module  that  we  will  likely  need  is  one  that  allows  us  to  simulate
vulnerabilities in applications and generate resulting sensor reading. It is difficult to