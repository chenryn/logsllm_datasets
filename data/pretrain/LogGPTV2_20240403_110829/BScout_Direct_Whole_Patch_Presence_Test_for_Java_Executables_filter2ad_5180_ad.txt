17
18
17
16
16
17
0
2
17
15
10
9
9
13
8
10
by BSCOUT (s)
8.39 / 0.342
4.70 / 0.19
1.08 / 0.04
6.83 / 0.22
9.01 / 0.18
3.34 / 0.10
1.53 / 0.17
7.17 / 0.24
6.33 / 0.23
7.72 / 0.29
4.53 / 0.13
6.91 / 0.21
9.37 / 0.16
5.44 / 0.15
3.78 / 0.13
during customization.
2 8.39 means the total test time, while 0.34 means the average test time.
3:
Table
Effectiveness Results
Dataset_ROM_GT and Dataset_Apps.
of BSCOUT
on
Tool
Android ROMs
Java Apps
Overall
TP TN FN FP Acc. TP TN FN FP Acc. Acc. FPR
BSCOUT
0 100% 291 410 0
BSCOUT(cid:52) 266 177 31 0 93.5% 286 410 5
297 177 0
0 100.0% 100% 0.0%
0 99.3% 96.9% 0.0%
we collect some real-world Java (covering Android/desk-
top/server platforms) apps for evaluation.
Ground Truth. To ease the ground truth construction
of patch status on Android apps, we write a crawler to
download 4,561 open-source apps from F-Droid (which is
a repository for open-source Android apps)
[8]. Through
parsing the Gradle build ﬁles, we recognize all the libraries
that are used by each app and then collect all the reported
vulnerabilities for these libraries by querying NVD [11]. From
these vulnerabilities, we randomly select 15 CVEs which
affect 11 libraries. We further ﬁnd that these libraries are
incorporated in 261 apps. Among the 261 apps, we observe
that 123 apps can also be found in Google Play and 81 apps
have ProGuard enabled. Similarly, we collect 12 server apps 1
and 16 desktop apps 2 for experiments, and ﬁnd that they
incorporate 12 Java libraries affected by 29 CVEs. We mark
these 289 (=261+12+16) apps as Dataset_Apps. For each
CVE, we manually label the patch status on these apps. In
all, we construct 364 and 337 App-CVE pairs for Android
apps and desktop/server apps as ground truth respectively.
1WebSpere(70011), WebLogic(12.2.1.3.0), Atlassian Conﬂuence(10
versions)
2JEB Android Decompiler(3.0, 3.1), JEB Intel Decompiler(3.1), JEB
ARM Decompiler(3.1), JEB MIPS Decompiler(3.1), JEB WebAssembly
Decompiler(3.1), JEB Ethereum Decompiler(3.1), IntelliJ IDEA(10 versions)
The whole library dataset and CVE dataset are presented in
Table 10 of Appendix A.
Tools Setup. Due to name obfuscation, BSCOUT can not
directly locate patch-changed Java methods in 9 Android
apps. For these cases, we leverage existing code similarity
techniques [19] to recognize patch-changed methods for
further patch presence test. Note that code similarity analysis
may perform well in searching similar functions from a large
space, but meets constraints in patch presence test due to low
precision (as evaluated in §4.3). Besides, we use the same
parameter setting as § 4.1.1 here.
Results. The detailed results are presented in Table 3. Over-
all, both BSCOUT and BSCOUT(cid:52) are remarkably effective by
achieving an accuracy of 100% and 96.9% respectively with
no false positives. By checking the 5 false negatives incurred
by BSCOUT(cid:52), we ﬁnd they are also caused by wrong line-to-
line mappings which we plan to improve in the future.
4.2 Results of Version Pinning
Version pinning tools can pinpoint the most similar executable
to a given target from a set of reference executables. Though
version pinning tools do not directly test patch presence, two
state-of-the-art tools (OSSPolice [21] and LibScout [17]) eval-
uate their performance in version pinning by distinguishing
patched/unpatched versions of Java executables. Therefore,
we conduct experiments to measure their effectiveness in
the patch presence test. Speciﬁcally, we fetch the source
code of OSSPolice [13] with commit hash af09514, and the
source code of LibScout [10] with commit hash 4c14ca3.
Furthermore, we also update some library dependencies for
them to ﬁx issues in parsing DEX ﬁles.
Experiments Setup. Since both tools require a large set of
1156    29th USENIX Security Symposium
USENIX Association
Table 4: Results of LibScout and OSSPolice on
Dataset_ROM_GT (containing 474 ROM-CVE pairs).
Tool
Cannot Give Results
Count
Ratio
Can Give Results
TP TN FP FN Acc.
FPR
LibScout
OSSPolice
455
5
96.0%
1.1%
12
1
69 168
0
6
6
68.4% 0%
226 50.5% 3.5%
reference images to pinpoint, we only apply OSSPolice and
LibScout on Dataset_ROM_GT to ease experiment prepara-
tion. Speciﬁcally, we leverage the Dataset_ROM_Reference
(consisting of 215 unique ROMs) in §4.1.1 as the reference
set. Meanwhile, we also manually label the patch status of
each CVE for all executables in the reference set. For each
test target, we run OSSPolice and LibScout to recognize the
most similar executable(s) from the reference set and use the
patch presence status of the recognized executable(s) as the
result of patch presence test.
Results. Table 4 presents the results of LibScout and
OSSPolice in testing patch presence on Dataset_ROM_GT
(containing 474 ROM-CVE pairs). We ﬁnd that LibScout can
not give results for 96.0% of cases and OSSPolice can not give
results for 5 cases. There are two scenarios for them to give no
result: 1) no image in the reference set is found to be similar to
the given target, due to the heavy code customization placed
on the test target; 2) at least two images are found to be
quite similar to the given target with the same similarity, but
they have different patch presence status. The cause of this
scenario is that the code features considered by the two tools
are too coarse-grained to differentiate patch changes. In the
cases that OSSPolice and LibScout could give results, their
accuracy is still signiﬁcantly lower than that of BSCOUT.
This is mainly due to that the image-level code similarity
is too coarse-grained to reliably reﬂect the patch presence
status. Overall speaking, although version pinning tools can
distinguish different versions, they are too coarse-grained to
test patch presence.
4.3 Results of Function-level Similarity Test
Function-level similarity testing is frequently used to locate
vulnerable function clones [23, 40, 43]. Intuitively, this line
of techniques can also be applied to patch presence test by
measuring whether the test target is more similar to the pre-
patch reference function or the post-patch one. Hence, we
also perform some experiments to report the effectiveness
of leveraging function-level similarity to test patch presence.
As presented in §4.2, our experiments are also conducted on
Dataset_ROM_GT (containing 474 ROM-CVE pairs). Since
centroid [19] is widely used on Android platform [20, 21] to
calculate Java method similarity, we leverage this algorithm
to measure function-level similarity in this experiment.
150 CVEs
Setup. From the
Experiments
in
Figure 5: The ratio of cases that can give (correct) patch
presence results with function-level similarity testing, by
varying similarity threshold.
Dataset_ROM_GT, we collect 471 patch-related functions.
For each function, we build both pre-patch and post-patch
versions from AOSP as references. In our experiment setting,
the patch status of a testing target is determined by the
reference which it is more similar to (i.e. if a testing target is
more similar to the pre-patched one than the post-patched
one, it is unpatched; otherwise it is patched). To ﬁgure the
similarity degree, we deﬁne a threshold. If the distance
between two similarity scores does not exceed the threshold,
we think that they have the same similarity degree, and
function-level similarity testing can not give a patch presence
result in this scenario. By contrast, if the distance between
two similarity scores exceeds the threshold, function-level
similarity testing can give a patch presence result (i.e. the
patch status of the more similar version).
Results. Since the performance of function-level similarity
testing is sensitive to the value of the selected similarity
threshold, we vary the similarity threshold to collect testing
results. More speciﬁcally, under different thresholds, we count
the ROM-CVE pairs that function-level similarity testing can
give results, and for these results, we count how many of
them are correct (can give correct results). Figure 5 shows
the results with varied similarity threshold. From this ﬁgure,
we ﬁnd that function-level similarity testing can at most give
results for 82% of ROM-CVE pairs. For the left ROM-CVE
pairs, we ﬁnd that both pre-patch and post-patch reference
functions have the same similarity score with the testing target.
It shows that function-level similarity testing is too coarse-
grained to reﬂect ﬁne-grained patch changes. By increasing
the similarity threshold, the ratio of can give results drops
dramatically, because the similarity scores between testing
targets and pre-patched/post-patched reference ones become
more indistinguishable. Meanwhile, it is interesting to ﬁnd
that the ratio of can give correct results does not increase
signiﬁcantly with the increased similarity threshold. This
shows that the similarity threshold does not signiﬁcantly
affect accuracy. The above results indicate that function-level
similarity testing is not suitable for patch presence testing.
USENIX Association
29th USENIX Security Symposium    1157
0.000.050.150.200.10 Threshold20406080Ratio(%)Can Give ResultCan Give Correct Result5 Empirical Study
To understand the patch application practice in the real
world, we apply BSCOUT to perform a large-scale study.
Considering the severe fragmentation issues of the Android
platform [1] and its wide popularity, our study is conducted
on 150 collected Android framework CVEs with 2,506
ROMs collected from 7 vendors (Google, Samsung, Meizu,
Xiaomi, Oppo, Vivo, and Huawei). We mark this dataset as
Dataset_ROM_Large and present it in Table 5. For each ROM,
we also collect several attributes (vendor, model, Android
version, ROM build time, security patch level3) from the
build.prop ﬁle in the ROM image. To guarantee the validity
of the study, BSCOUT is conﬁgured to leverage the line
information when it is available in the testing targets. Since
the presence of line information for different Java classes
in a single ROM is not the same, we check the presence of
this information in all CVE-related classes for all ROMs in
Dataset_ROM_Large and ﬁnd the ratio is 99.4%.
Our study mainly focus on three aspects of patch applica-
tion practice: patch application status, the lag of applying
security patches, and the management of security patches.
RQ1: Does the severity of a vulnerability affect its patch
application status? It is common sense that highly severe
vulnerabilities should receive more attention from vendors
and are more likely to be patched by vendors to prevent
potential threats. To verify whether vendors follow this
practice, we correlate the unpatched ratio of each CVE to its
CVSS 5 score [4], which is shown in Figure 6. We surprisingly
ﬁnd that the severest CVE does not have the lowest unpatched
ratio. Furthermore, we perform a t-test [16] at a signiﬁcance
level of 0.05 to study the relationship between the unpatched
ratio and the vulnerability severity. It is very interesting to
ﬁnd that there is no signiﬁcant difference in the distribution
of unpatched ratio among different CVEs under each CVSS
score (except the CVSS score of 10 which has only 1 CVE)
from that of the whole CVE dataset. We also verify the
results among every individual vendor and conﬁrm these
observations also exist. This implies that developers may not
fully aware of vulnerability severity when applying security
patches, or perhaps vulnerability severity has not yet been
a good indicator for developers to assess the necessity of
applying security patches.
Table 5: A large-scale Dataset of ROMs Collected from
Smartphone Vendors (Dataset_ROM_Large).
Vendor
Phone Models
Build Time
Count
Versions
Google
Samsung
Meizu
Xiaomi
Oppo
Vivo
Huawei
14
24
44
45
31
46
31
569
468
481
464
281
152
91
4.4.4-8.1.0
5.0.0-8.1.0
5.0.1-8.1.0
4.4.4-8.1.0
4.4.4-8.1.0
5.0.2-8.1.0
6.0.0-7.0.0
2014.06-2019.05
2016.10-2018.09
2015.06-2019.07
2016.02-2019.08
2014.11-2019.08
2015.11-2019.05
2016.01-2017.10
5.1 Patch Application Status
Ideally, when the patch for a vulnerability has been released,
all ROMs built after that date should apply this patch. To
measure this practice, we ﬁrst recognize all the affected
ROMs (marked as Sall) that are built after the patch release
date 4 for each CVE. Thereafter, we use BSCOUT to detect
ROMs (marked as Sunpatched) from Sall that have not patched
the corresponding CVE. To quantify the ratio of patch
application status, we deﬁne the unpatched ratio for each
CVE as unpatched_ratio =
. We ﬁnd that only 9
CVEs are patched by all affected ROMs built after the patch
release date, and 22 CVEs have an unpatched ratio higher
than 50%, which means more than half of affected ROMs
built after the patch release date are still vulnerable.
|Sunpatched|
|Sall|
3Google assigns a security patch level for each public vulnerability which