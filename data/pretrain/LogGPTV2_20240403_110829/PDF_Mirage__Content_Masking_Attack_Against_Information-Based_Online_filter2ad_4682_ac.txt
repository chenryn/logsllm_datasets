ing are evaluations of the content masking attack match-
ing one paper to one reviewer, multiple papers to one
reviewer, and one paper to multiple reviewers.
Matching one paper to one reviewer: The automatic
reviewer assignment process compares a subject paper
with every paper from the collection of reviewers’ papers
to gather a list of similarity scores. The reviewer with the
highest similarity score is assigned the paper to judge (if
available). We therefore aim to change a testing paper
topic to a training paper topic, and to examine how well
this works with all papers. For each such pair of papers,
then, we replace the frequently appearing words A in the
testing paper with those frequently appearing words B
in the training paper via Algorithm 1. We test the topic
matching of each of the 100 testing papers against our
training data to see what is required to induce a match.
For each pair of training and testing papers, we re-
place important words in the testing paper one by one, to
see how many replacements are needed to make that pair
the most similar. Figure 2 illustrates this iterative pro-
cess for one example training/testing paper pair, showing
resultant similarity scores. The box plots show where
the greatest concentration of the 2094 similarity scores
dwell, while red pluses show outliers. The blue stars
which emerge to the top correspond to the similarity
scores between the testing paper and the target training
paper. Figure 2 shows a clear separation of that similarity
score from the rest after replacing 9 words, meaning that
for this pair, content masking all appearances of those 9
unique words in the testing paper will result in its assign-
ment to the reviewer who wrote that training paper.
Performing this process for all 100 testing papers, we
compile the results into Figure 3, which displays the cu-
mulative distribution function (CDF) for the number of
words requiring replacement. Evidently, all 100 papers
may be matched with the target with 12 words or fewer
masked. The sharp jump appearing from 4-9 words indi-
cates that most papers can be successfully targeted to a
speciﬁc reviewer masking between 4 and 9 words. The
font requirements for replacing these words is then rep-
resented in Figure 4. A majority of papers require 3 or
fewer masking fonts, while almost all of them need only
as many as 5. This is a comparatively small number and
should go unnoticed among the collection of fonts nat-
ural to academic papers. For example, this paper has
some 19 embedded fonts, between bold/italic variants,
fonts used in ﬁgures, and one picture font used in Table
1.
Matching multiple papers to a single reviewer:
Should an author wish to have multiple submitted papers
all assigned to a target reviewer, the author may simply
repeat the content masking process on each paper. While
in the previous case we ﬁnd that an average of 3 or 4
fonts is necessary to make a single test paper sufﬁciently
similar to the target training paper, that needs not directly
translate to 3 or 4 fonts per paper with multiple papers.
Some fonts may be reused among papers, resulting in
838    26th USENIX Security Symposium
USENIX Association
Figure 2: Similarity scores relative
to amount of words masked. Blue
stars show the desired matching.
Figure 3: Word masking require-
ments for all 100 testing papers.
Figure 4: Masking font require-
ments for all 100 testing papers.
5 Content Masking Attack Against Plagia-
rism Detection
While a method similar to the topic matching subver-
sion technique just outlined may be used to hide plagia-
rism, fewer requirements constrain the plagiarist than the
lazy author targeting a speciﬁc reviewer in a conference.
Speciﬁcally, an attacker needs only make the underlying
text different than the rendered, plagiarized text. The un-
derlying text does not need to be actual words, and so
only one font is needed, ensuring the naive defense of
limiting fonts is defeated. This scrambling font is just a
random scrambling of the characters. Each original letter
is replaced with the letter which displays as the original.
Resulting is a human-legible PDF document which ap-
pears as gibberish to Turnitin and necessarily has a sim-
ilarity score of 0%. Details and options for this method
are below, followed by an evaluation of each option.
5.1 Targeting a Speciﬁc Plagiarism Score
Because Turnitin is a similarity checker, not a plagiarism
detector, it relies on the human factor to actually detect
plagiarism. Turnitin informs the individual with grad-
ing duties of any pieces of similar prose, which naturally
arise due to the plethora of written work in existence and
the human tendency toward common patterns and ﬁgures
of speech.
It is unlikely then, and would stand out to
the grader, that a submission would have 0% similarity
with anything ever written. We offer and evaluate two
methods an attacker can use to target a speciﬁc (low but
non-zero) similarity score and more likely go unnoticed.
By Letter: Here, the attacker begins with a scram-
bling font and removes characters from being scrambled
successively until a target percentage of the text is not
being replaced. Intuitively, this small target percentage
would then appear plagiarized, yielding a credible simi-
larity score. This may be done in a calculated fashion us-
ing the known frequency of usage of letters in the English
(or other) language. The letters may be listed by their
Figure 5: Masking font requirements for matching from
1 to all 100 testing papers to a single reviewer.
Figure 6: Similarity scores relative to amount of words
masked, between a paper and three reviewers. Blue
stars, black circles, and green triangles show the desired
matchings.
fewer overall fonts used. Figure 5 conﬁrms this, show-
ing a trend more logarithmic than linear.
Matching a paper to multiple reviewers: Finally, we
evaluate the iterative reﬁnement method to split masked
words among three reviewers’ papers as discussed in
Section 4.3. Figure 6 shows that the similarity scores
for the three target reviewers (blue star, black circle,
and green triangle) consistently increase; after some 70
words masked, the subject paper is more similar to the
three target papers than any others.
USENIX Association
26th USENIX Security Symposium    839
13579111315Number of words masked00.20.40.60.8Similarity051015Number of words masked00.20.40.60.81CDF246810Number of masking fonts00.20.40.60.81CDF050100Number of masking fonts020406080100Number of papers(cid:49)(cid:49)(cid:48)(cid:50)(cid:48)(cid:51)(cid:48)(cid:52)(cid:48)(cid:53)(cid:48)(cid:54)(cid:48)(cid:55)(cid:48)(cid:56)(cid:48)(cid:57)(cid:48)(cid:78)(cid:117)(cid:109)(cid:98)(cid:101)(cid:114)(cid:32)(cid:111)(cid:102)(cid:32)(cid:119)(cid:111)(cid:114)(cid:100)(cid:115)(cid:32)(cid:109)(cid:97)(cid:115)(cid:107)(cid:101)(cid:100)(cid:48)(cid:48)(cid:46)(cid:50)(cid:48)(cid:46)(cid:52)(cid:48)(cid:46)(cid:54)(cid:48)(cid:46)(cid:56)(cid:83)(cid:105)(cid:109)(cid:105)(cid:108)(cid:97)(cid:114)(cid:105)(cid:116)(cid:121)curve that is too steep to be manageable for selecting a
small range of similarity scores. In contrast, the other
two methods are very suitable for comfortably picking a
speciﬁc range. Any probability between 17% and 20%
will net a similarity score in our desired 5-15% range in
the case of randomly chosen masking. When words are
replaced in order of their frequency of appearance, the 5-
15% range may be achieved by replacing anywhere be-
tween 20 and 40% of the words, offering a very wide
range of safety for the plagiarist.
Figure 7: Effects of the percentage of text changed upon
plagiarism similarity scores for 10 sample documents.
6 Document Indexing Subversion
frequency in ascending or descending order (we evaluate
both) and then excluded from scrambling in that order
until the target percentage of unaltered text is reached.
By Word: This method is similar to the previous, but
instead of leaving some characters unscrambled in the
custom font, the attacker leaves some words unaltered by
not applying the custom scrambling font to them. Here,
words within the document may be listed in frequency
of appearance, ascending or descending, and excluded
from the scrambling font in that order (we again evaluate
both). We also consider changing words at random with a
probability targeting some similarity score. This method
may be more effective for an attacker in the long run, if
Turnitin implements a requirement that some percentage
of words be found in a dictionary, English or otherwise.
In that case, this attack may be augmented by the previ-
ously described method of replacing real words for other
real words rendered as the originals.
5.2 Experiment
We use 10 already published papers retrieved from the
Internet and mask the content in varying degrees to see
the effects on Turnitin’s returned similarity scores. We
vary the amount a scrambling font is applied to the text
according to the previously described methods and up-
load the resultant papers to Turnitin. Again, we target a
speciﬁc range of similarity scores, between 5% and 15%,
such that a human grader is unlikely to suspect foul play.
Figure 7 plots the three methods. “Frequency descend-
ing” refers to the method of masking words in the order
of their frequency of appearance in the document, while
“Letter usage descending” refers to masking letters by
their frequency of usage. Ascending order proved un-
wieldy in both cases and not worth displaying. Finally,
“Random replacement” refers to the method of iterating
over all words and masking them with a probability of
1-100% in increments of 1%. These are all plotted in
terms of the percentage of text changed. Masking let-
ters by their frequency of usage results in a similarity
The ﬁnal direction of this attack is against search en-
gines, whether for the entire web or for small document
repositories or websites. Websites can implement a sim-
ple search returning pages housing the query text, or they
can use custom search engines offered by Google [25] or
Yahoo! [26]. Microsoft Bing also offers its API [27].
As small sites are unlikely to have a more sophisticated
search mechanism than the leading search engines, we
target and demonstrate our attack against these.
6.1 Method
We here consider modifying the entire content of a PDF
to render as something else. Both the underlying text
extracted by PDFMiner (or otherwise) and the rendered
text should make sense in this case, so that an individual
searching for certain terms will be caused to ﬁnd a PDF
holding those words but displaying something entirely
different. This results in a more extreme version of the
one-to-many character mapping challenge from the at-
tack against topic matching. Instead of masking a small
ﬁnite number of words, we now examine masking the en-
tire content. However, this is facilitated by the realization
that these masks are not necessarily delineated by spaces
as before; the attacker can treat the entire document as
a single word to be masked. It consequently encounters
the word length disparity challenge, to treat the variation
in length between real and rendered text, but only once.
Nevertheless, the strategy of adding new fonts, ad hoc,
to cover each new mapping quickly balloons out of con-
trol, in terms of the attacker needing to keep track of what
mappings appear in what font. The number of fonts will
increase with the number of characters to be masked, to
an upper limit of every character needing a map to every
other. Considering (for English) upper and lower case
letters, numbers, and common punctuation (22 symbols,
dependent upon count), all 26 + 26 + 10 + 22 = 84 char-
acters must each map to the other 83 different characters,
as well as themselves for those cases which a character
and its mask are the same. This requires 84 fonts and
represents 842 = 7056 mappings. Code can certainly be
840    26th USENIX Security Symposium
USENIX Association
00.10.20.30.4Percentage00.20.40.60.81SimilarityFrequency descendingRandom replacmentLetter usage descendingSearch Engine
Indexed Papers Attack Successful
Evades Spam Detection Not Later Removed
Google
Bing
Yahoo!
DuckDuckGo










Flagged / Cleared





Table 1: Results of content masking attack on search engines.
written to automatically construct all these mappings, but
to make this more efﬁcient, we offer an alternative - 84
fonts, in each of which all characters map to one masking
character. For example, in font “MaskAsA” character a
maps to a, b to a, 4 to a, ! to a, etc. To mask a document
as another, the attacker may simply apply fonts, charac-
ter by character, that correspond to the desired mask. At
the end of the documents, the three end behavior options
presented as part of Algorithm 1 and illustrated in Figure
1 function here as well, to handle the length variation.
6.2 Experiment
To demonstrate the efﬁcacy of this attack, we obtained
a handful of well-known academic papers, masked their
content, and then placed them on one author’s university
website to be indexed by several leading search engines.
For this simple proof of attack, we only used one mask-
ing font which scrambled the letters for rendering. The
resulting papers have legible text that renders to gibber-
ish, meaning that if they can be located by searching for
that legible text, the search engine is fooled.
We submitted the site housing these papers to Google,
Bing, and Yahoo! and searched for them some days
later. Search engine DuckDuckGo does not accept web-
site submissions but we searched there as well. Table 1
lists the results of our content masking attack on these
search engines. “Indexed Papers” indicates the search
engine listed the papers in its index. “Attack Successful”
means they are indexed using the underlying text, not
the rendered gibberish. After a successful attack, the pa-
pers may later be put behind a spam warning or removed
from the index, as shown in the last two columns. We
found similar results for each of the 5 papers tested: that
Bing, Yahoo!, and DuckDuckGo all indexed the papers
according to the masked legible text, and none removed
them later (at time of writing). Yahoo! did mark them as
spam after two days but confusingly some days after that
removed the spam warning.
Figure 8 illustrates this for one of tested paper. The
masked paper is shown in Figure 8a and contains no ren-
dered English words beyond what is shown. Figures 8b,
8c, and 8d show the search results for the legible underly-
ing text, and Figure 8e shows the spam warning appear-
ing days later but later disappearing. Each query was
(a) Gibberish paper
(b) Bing result for the gibberish paper
(c) DuckDuckGo result for the gibberish paper
(d) Yahoo! result for the gibberish paper
(e) Temporary Yahoo! spam warning
Figure 8: Results of the content masking attack against
popular search engines. The attack was not successful
against Google.
USENIX Association
26th USENIX Security Symposium    841
appended with “site:XXX.edu” to isolate the university
website where they are hosted for this proof of concept.