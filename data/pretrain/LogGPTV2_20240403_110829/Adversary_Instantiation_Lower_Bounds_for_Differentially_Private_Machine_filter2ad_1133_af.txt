[9] Christopher A Choquette Choo, Florian Tramer, Nicholas
Carlini, and Nicolas Papernot. Label-only membership
arXiv preprint arXiv:2007.14321,
inference attacks.
2020.
[10] Charles J Clopper and Egon S Pearson. The use of
conﬁdence or ﬁducial limits illustrated in the case of the
binomial. Biometrika, 26(4):404–413, 1934.
[11] Aloni Cohen and Kobbi Nissim. Towards formalizing the
gdpr’s notion of singling out. Proceedings of the National
Academy of Sciences, 117(15):8344–8352, 2020.
[12] Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng
Zhang, and Daniel Kifer. Detecting violations of differ-
ential privacy. In Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security,
pages 475–489, 2018.
[13] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian
differential privacy. arXiv preprint arXiv:1905.02383,
2019.
[14] C Dwork, F McSherry, K Nissim, and A Smith. Cali-
In
brating noise to sensitivity in private data analysis.
TCC, volume 3876, 2006.
[15] Cynthia Dwork. Differential privacy: A survey of results.
In Intl. Conf. on Theory and Applications of Models of
Computation, 2008.
[16] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSh-
erry, Ilya Mironov, and Moni Naor. Our data, ourselves:
In Annual
Privacy via distributed noise generation.
International Conference on the Theory and Applications
of Cryptographic Techniques, pages 486–503. Springer,
2006.
[17] Cynthia Dwork, Aaron Roth, et al. The algorithmic
foundations of differential privacy. Foundations and
Trends in Theoretical Computer Science, 9(3-4):211–407,
2014.
[18] Cynthia Dwork and Guy N Rothblum. Concentrated
differential privacy. arXiv preprint arXiv:1603.01887,
2016.
[19] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin
Ko, Susan M Swetter, Helen M Blau, and Sebastian
Thrun. Dermatologist-level classiﬁcation of skin cancer
with deep neural networks. nature, 542(7639):115–118,
2017.
[20] Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private
stochastic convex optimization: optimal rates in linear
time. In Proceedings of the 52nd Annual ACM SIGACT
Symposium on Theory of Computing, pages 439–449,
2020.
[21] Vitaly Feldman,
Ilya Mironov, Kunal Talwar, and
Abhradeep Thakurta. Privacy ampliﬁcation by iteration.
In 2018 IEEE 59th Annual Symposium on Foundations of
Computer Science (FOCS), pages 521–532. IEEE, 2018.
[22] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model inversion attacks that exploit conﬁdence infor-
In Proceedings of
mation and basic countermeasures.
the 22nd ACM SIGSAC Conference on Computer and
Communications Security, pages 1322–1333, 2015.
[23] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep learning. MIT press, 2016.
[24] Jamie Hayes, Luca Melis, George Danezis, and Emiliano
De Cristofaro. Logan: Membership inference attacks
Proceedings on Privacy
against generative models.
Enhancing Technologies, 2019(1):133–152, 2019.
[25] Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar,
Abhradeep Thakurta, and Lun Wang. Towards practical
differentially private convex optimization. In 2019 IEEE
Symposium on Security and Privacy (SP), pages 299–
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
879
316. IEEE, 2019.
[26] Matthew Jagielski, Jonathan Ullman, and Alina Oprea.
Auditing differentially private machine learning: How
private is private sgd? arXiv preprint arXiv:2006.07709,
2020.
[27] Bargav Jayaraman and David Evans. Evaluating dif-
In
ferentially private machine learning in practice.
28th {USENIX} Security Symposium ({USENIX} Secu-
rity 19), pages 1895–1912, 2019.
[28] Kaggle.
https://www.kaggle.com/c/
acquire-valued-shoppers-challenge/data, 2020.
[29] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.
The composition theorem for differential privacy.
In
International conference on machine learning, pages
1376–1385, 2015.
[30] Jakub Koneˇcn`y, H Brendan McMahan, Daniel Ramage,
and Peter Richt´arik. Federated optimization: Distributed
arXiv
machine learning for on-device intelligence.
preprint arXiv:1610.02527, 2016.
[31] Antti Koskela, Joonas J¨alk¨o, Lukas Prediger, and Antti
Tight approximate differential privacy for
arXiv preprint
Honkela.
discrete-valued mechanisms using fft.
arXiv:2006.07134, 2020.
[32] Alex Krizhevsky, Geoffrey Hinton, et al.
Learning
multiple layers of features from tiny images. 2009.
[33] Yann LeCun, Bernhard E Boser, John S Denker, Donnie
Henderson, Richard E Howard, Wayne E Hubbard, and
Lawrence D Jackel. Handwritten digit recognition with
In Advances in neural
a back-propagation network.
information processing systems, pages 396–404, 1990.
[34] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of
the IEEE, 86(11):2278–
2324, 1998.
[35] Ninghui Li, Tiancheng Li, and Suresh Venkatasubrama-
nian.
t-closeness: Privacy beyond k-anonymity and l-
diversity. In 2007 IEEE 23rd International Conference
on Data Engineering, pages 106–115. IEEE, 2007.
[36] Ashwin Machanavajjhala, Daniel Kifer,
Johannes
Gehrke, and Muthuramakrishnan Venkitasubramaniam.
ACM
l-diversity: Privacy beyond k-anonymity.
Transactions on Knowledge Discovery
from Data
(TKDD), 1(1):3–es, 2007.
[37] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efﬁcient learning of deep networks from decentralized
data. In Artiﬁcial Intelligence and Statistics, pages 1273–
1282. PMLR, 2017.
[38] H Brendan McMahan, Daniel Ramage, Kunal Talwar,
and Li Zhang. Learning differentially private recurrent
arXiv preprint arXiv:1710.06963,
language models.
2017.
[39] Luca Melis, Congzheng Song, Emiliano De Cristofaro,
and Vitaly Shmatikov. Exploiting unintended feature
In 2019 IEEE Sym-
leakage in collaborative learning.
posium on Security and Privacy (SP), pages 691–706.
IEEE, 2019.
[40] Ilya Mironov. R´enyi differential privacy. In 2017 IEEE
30th Computer Security Foundations Symposium (CSF),
pages 263–275. IEEE, 2017.
[41] Ilya Mironov, Kunal Talwar, and Li Zhang. R\’enyi
differential privacy of the sampled gaussian mechanism.
arXiv preprint arXiv:1908.10530, 2019.
[42] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and
Pascal Frossard. Deepfool: a simple and accurate method
to fool deep neural networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pages 2574–2582, 2016.
[43] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units
improve restricted boltzmann machines. In ICML, 2010.
[44] Arvind Narayanan and Vitaly Shmatikov. How to break
anonymity of the netﬂix prize dataset. arXiv preprint
cs/0610105, 2006.
[45] Milad Nasr, Reza Shokri, and Amir Houmansadr. Com-
prehensive privacy analysis of deep learning: Passive and
active white-box inference attacks against centralized and
federated learning. In 2019 IEEE Symposium on Security
and Privacy (SP), pages 739–753. IEEE, 2019.
[46] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth
´Ulfar Erlingsson.
arXiv preprint
Raghunathan, Kunal Talwar, and
Scalable private learning with pate.
arXiv:1802.08908, 2018.
[47] Venkatadheeraj Pichapati, Ananda Theertha Suresh, Fe-
lix X Yu, Sashank J Reddi, and Sanjiv Kumar. Ada-
clip: Adaptive clipping for private sgd. arXiv preprint
arXiv:1908.07643, 2019.
[48] Reza Shokri and Vitaly Shmatikov. Privacy-preserving
deep learning. In Proceedings of the 22nd ACM SIGSAC
conference on computer and communications security,
pages 1310–1321, 2015.
[49] Reza Shokri, Marco Stronati, Congzheng Song, and
Vitaly Shmatikov. Membership inference attacks against
machine learning models. In 2017 IEEE Symposium on
Security and Privacy (SP), pages 3–18. IEEE, 2017.
[50] Congzheng Song, Thomas Ristenpart,
and Vitaly
Shmatikov. Machine learning models that remember
In Proceedings of the 2017 ACM SIGSAC
too much.
Conference on Computer and Communications Security,
pages 587–601, 2017.
[51] Shuang Song, Kamalika Chaudhuri, and Anand D Sar-
wate. Stochastic gradient descent with differentially pri-
vate updates. In 2013 IEEE Global Conference on Signal
and Information Processing, pages 245–248. IEEE, 2013.
[52] Latanya Sweeney. k-anonymity: A model for protecting
privacy. International Journal of Uncertainty, Fuzziness
and Knowledge-Based Systems, 10(05):557–570, 2002.
[53] ADP Team et al. Learning with privacy at scale. Apple
Machine Learning Journal, 1(8), 2017.
[54] Yu-Xiang Wang, Borja Balle, and Shiva Prasad Ka-
siviswanathan. Subsampled r´enyi differential privacy and
In The 22nd Interna-
analytical moments accountant.
tional Conference on Artiﬁcial Intelligence and Statistics,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
880
pages 1226–1235. PMLR, 2019.
[55] Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri,
Somesh Jha, and Jeffrey Naughton. Bolt-on differential
privacy for scalable stochastic gradient descent-based
analytics. In Proceedings of the 2017 ACM International
Conference on Management of Data, pages 1307–1322,
2017.
[56] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin
Recht, and Oriol Vinyals. Understanding deep learn-
ing requires rethinking generalization. arXiv preprint
arXiv:1611.03530, 2016.
[57] Yuqing Zhu and Yu-Xiang Wang. Poission subsampled
r´enyi differential privacy. In International Conference on
Machine Learning, pages 7634–7642, 2019.
APPENDIX A: EXPERIMENTAL SETUP
We have implemented our algorithms in Objax, a machine
learning library that allows for efﬁciently training neural net-
works. Because DP-SGD requires per-example gradients, in-
stead of standard SGD which just requires minibatch-averaged
gradients, training models is computationally expensive. We
leverage the JAX per-example parallel processing to speed up
the training process. (URL to open source repository blinded
for review.)
A. Datasets
We run experiments on three datasets:
MNIST We use the MNIST image dataset which consists
of 70,000 instances of 28 × 28 handwritten grayscale digit
images and the task is to recognize the digits. MNIST splits
the training data to 60,000 images for training and 10,000 for
the testing phase.
CIFAR We use CIFAR10, which is a standard benchmark
dataset consisting of 32 × 32 RGB images. The learning task
is to classify the images into 10 classes of different objects.
This dataset is partitioned into 50,000 for training and 10,000
for testing.
Purchase The Purchase dataset is the shopping records of
several thousand online customers, extracted during Kaggle’s
“acquire valued shopper” challenge [28]. We used a processed
version of this dataset (courtesy of the authors of [49]). Each
record in the dataset is the shopping history of a single user.
The dataset contains 600 different products, and each user has
a binary record which indicates whether she has bought each
of the products. Records are clustered into 10 classes based
on the similarity of the purchase. We use 18,000 records for
training and 1,000 for testing.
Hyper-parameters
For both CIFAR10 and MNIST, we
used cross-entropy as our loss function. We used four lay-
ers convolutional neural network [23] for both MNIST and
CIFAR10 datasets. For Purchase dataset, we used a three
layers fully connected neural network. To train the models,
we use differenatially private SGD [1] with 0.15 learning
rate for MNIST and CIFAR and 1.1 for Purchase datasets,
clipping factor of 1.0 for MNIST and CIFAR and 0.05 for
Purchase dataset. We repeat MNIST, CIFAR and Purchase
y
c
a
v
i
r
p
d
e
r
u
s
a
e
m
3
2
1
0
MNIST
CIFAR
Purchase
Malicious
Theoretical
API
Blackbox
W hitebox
Adaptive
Gradient
Dataset
Practical
Theoretical
Fig. 9: Summary of our results, plotting emperically mea-
sured ε when training a model with ε = 2 differential privacy.
The dashed red line corresponds to the certiﬁable upper bound.
Each bar correspond to the privacy offered by increasingly
powerful adversaries. In the most realistic setting, training with
privacy offers much more empirically measured privacy. When
we provide full attack capabilities, our lower bound shows that
the DP-SGD upper bound is tight.
experiments 1000 each and the malicious dataset 1,000,000
times. We evaluated each experiment using noise multipliers
(DPSGD parameter) of 0.5, 0.6, 0.9, 1.0 and picked the one
which gives us the best practical epsilon. For MNIST, we
get accuracy of 95%,96.0%,97%,98.5%, on CIFAR we get
53%, 55%, 56%, 59%, and on Purchase 37%, 82%, 88%, 90%
for epsilons of 1, 2, 4, 10 respectively.
APPENDIX B: SUMMARY OF RESULTS
Figure 9 shows an overview of the results in the paper. We
target a speciﬁc theoretical epsilon for DPSGD for different
datasets. Since all of the training datasets has between 104
and 105 instances, we choose δ = 10−5 which means we can
leak one instance from the training dataset. This setting of
δ is typical for DP-SGD [1]. The dataset attack can achieve
a practical epsilon very close to the theoretical bounds. This
result suggest that if an adversary has access to all of the
assumptions in DPSGD then the theoretical analysis is tight
and the researchers should focus more on relaxing some of
the assumptions in the analysis. The gradient attack results are
also close to the theoretical bounds which means the adversary
does not need to modify the dataset. Going from having access
to gradient to only input, we can see a noticeable drop in the
empirical privacy parameters. When the adversary has access
to intermediate step models (e.g., in federated learning) it can
still achieve a close privacy parameters close to theoretical
values. However, when the adversary does not maliciously
modify the training dataset, gradients or input the practical
epsilon is much lower than the theoretical ones.
Overall, the results suggest the tightness of DP-SGD in the
worst case, but loseness of DP-SGD in the average case.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
881
RDP [41]
GDP [13]
10
5
ε
1
1.5
σ
2
Fig. 10: Comparison between R´enyi [41] and Gaussian Dif-
ferential Privacy [13]
VII. DIFFERENT ANALYSES OF DP-SGD
Abadi et al. [1] ﬁrst introduced DP-SGD and the momentum
accountant to analyze the privacy bounds. Since then, many
works [13, 2, 54, 41, 31, 3] improved the analysis of the DP-
SGD to get better theoretical privacy bounds. As mentioned
before, we used the R´enyi Differential privacy (RDP) [41]
to compute the theoretical privacy bounds. Recently Dong
et. al. [13] introduced Gaussian differential privacy (GDP)
to show a tight approach to compute the privacy bounds.
However, both approaches result in similar privacy bounds.
Using the hyperparameters for the MNIST dataset (sampling
rate 256
60000 and 60 epochs), we compute the ﬁnal privacy costs
using both R´enyi and Gaussian differential privacy which
is shown in the Figure 10. As we can see both results in
comparable privacy bounds and are within our measurement
error bounds.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
882