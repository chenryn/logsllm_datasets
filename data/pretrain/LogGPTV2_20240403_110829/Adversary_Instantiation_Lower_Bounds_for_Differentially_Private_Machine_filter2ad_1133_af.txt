以下是优化后的文本，使其更加清晰、连贯和专业：

---

### 参考文献

1. Christopher A. Choquette Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. 仅基于标签的成员推断攻击。arXiv 预印本 arXiv:2007.14321, 2020.
2. Charles J. Clopper 和 Egon S. Pearson. 二项式情况下置信限或可信限的应用。《Biometrika》, 26(4): 404–413, 1934.
3. Aloni Cohen 和 Kobbi Nissim. 朝着正式化 GDPR 的单挑概念迈进。《美国国家科学院院刊》，117(15): 8344–8352, 2020.
4. Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, 和 Daniel Kifer. 检测差分隐私违规。在《2018 ACM SIGSAC 计算机与通信安全会议论文集》，页码 475–489, 2018.
5. Jinshuo Dong, Aaron Roth, 和 Weijie J. Su. 高斯差分隐私。arXiv 预印本 arXiv:1905.02383, 2019.
6. C. Dwork, F. McSherry, K. Nissim, 和 A. Smith. 在私有数据分析中校准噪声以适应敏感性。TCC, 卷 3876, 2006.
7. Cynthia Dwork. 差分隐私：结果综述。在国际计算模型理论与应用会议，2008.
8. Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, 和 Moni Naor. 我们的数据，我们自己：通过分布式噪声生成实现隐私。年度国际密码技术理论与应用会议，页码 486–503. Springer, 2006.
9. Cynthia Dwork, Aaron Roth, 等. 差分隐私的算法基础。理论计算机科学的基础与趋势, 9(3-4): 211–407, 2014.
10. Cynthia Dwork 和 Guy N. Rothblum. 集中的差分隐私。arXiv 预印本 arXiv:1603.01887, 2016.
11. Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, 和 Sebastian Thrun. 使用深度神经网络达到皮肤科医生级别的皮肤癌分类。《自然》，542(7639): 115–118, 2017.
12. Vitaly Feldman, Tomer Koren, 和 Kunal Talwar. 私有随机凸优化：线性时间内的最优速率。在《第 52 届 ACM SIGACT 计算理论研讨会论文集》，页码 439–449, 2020.
13. Vitaly Feldman, Ilya Mironov, Kunal Talwar, 和 Abhradeep Thakurta. 迭代的隐私放大。在 2018 IEEE 第 59 届计算机科学基础年度研讨会 (FOCS)，页码 521–532. IEEE, 2018.
14. Matt Fredrikson, Somesh Jha, 和 Thomas Ristenpart. 利用置信信息和基本对策进行模型反转攻击。在《第 22 届 ACM SIGSAC 计算机与通信安全会议论文集》，页码 1322–1333, 2015.
15. Ian Goodfellow, Yoshua Bengio, 和 Aaron Courville. 深度学习。MIT 出版社, 2016.
16. Jamie Hayes, Luca Melis, George Danezis, 和 Emiliano De Cristofaro. Logan：针对生成模型的成员推断攻击。《隐私增强技术会议论文集》，2019(1): 133–152, 2019.
17. Roger Iyengar, Joseph P. Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, 和 Lun Wang. 走向实用的差分隐私凸优化。在 2019 IEEE 安全与隐私研讨会 (SP)，页码 299–316. IEEE, 2019.
18. Matthew Jagielski, Jonathan Ullman, 和 Alina Oprea. 审计差分隐私机器学习：私有 SGD 有多私密？arXiv 预印本 arXiv:2006.07709, 2020.
19. Bargav Jayaraman 和 David Evans. 实践中的差分隐私机器学习评估。在第 28 届 USENIX 安全研讨会 (USENIX Security 19)，页码 1895–1912, 2019.
20. Kaggle. https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data, 2020.
21. Peter Kairouz, Sewoong Oh, 和 Pramod Viswanath. 差分隐私的组合定理。在国际机器学习会议，页码 1376–1385, 2015.
22. Jakub Konečný, H. Brendan McMahan, Daniel Ramage, 和 Peter Richtárik. 联邦优化：设备智能的分布式机器学习。arXiv 预印本 arXiv:1610.02527, 2016.
23. Antti Koskela, Joonas Jälkö, Lukas Prediger, 和 Antti Honkela. 使用 FFT 的离散值机制的紧致近似差分隐私。arXiv 预印本 arXiv:2006.07134, 2020.
24. Alex Krizhevsky, Geoffrey Hinton, 等. 从微小图像中学习多层特征。2009.
25. Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E. Hubbard, 和 Lawrence D. Jackel. 使用反向传播网络的手写数字识别。在神经信息处理系统进展，页码 396–404, 1990.
26. Yann LeCun, Léon Bottou, Yoshua Bengio, 和 Patrick Haffner. 基于梯度的学习应用于文档识别。《IEEE 会刊》，86(11): 2278–2324, 1998.
27. Ninghui Li, Tiancheng Li, 和 Suresh Venkatasubramanian. t-紧密性：超越 k-匿名性和 l-多样性。在 2007 IEEE 第 23 届数据工程国际会议，页码 106–115. IEEE, 2007.
28. Ashwin Machanavajjhala, Daniel Kifer, Johannes Gehrke, 和 Muthuramakrishnan Venkitasubramaniam. l-多样性：超越 k-匿名性。《ACM 数据发现知识期刊》(TKDD), 1(1): 3–es, 2007.
29. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, 和 Blaise Aguera y Arcas. 分散数据的高效深度网络学习。在人工智能与统计学会议，页码 1273–1282. PMLR, 2017.
30. H. Brendan McMahan, Daniel Ramage, Kunal Talwar, 和 Li Zhang. 学习差分隐私递归语言模型。arXiv 预印本 arXiv:1710.06963, 2017.
31. Luca Melis, Congzheng Song, Emiliano De Cristofaro, 和 Vitaly Shmatikov. 利用协作学习中的意外特征泄漏。在 2019 IEEE 安全与隐私研讨会 (SP)，页码 691–706. IEEE, 2019.
32. Ilya Mironov. Rényi 差分隐私。在 2017 IEEE 第 30 届计算机安全基础研讨会 (CSF)，页码 263–275. IEEE, 2017.
33. Ilya Mironov, Kunal Talwar, 和 Li Zhang. 采样高斯机制的 Rényi 差分隐私。arXiv 预印本 arXiv:1908.10530, 2019.
34. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, 和 Pascal Frossard. DeepFool：一种简单而准确的方法来欺骗深度神经网络。在 IEEE 计算机视觉与模式识别会议论文集，页码 2574–2582, 2016.
35. Vinod Nair 和 Geoffrey E. Hinton. 整流线性单元改进受限玻尔兹曼机。在 ICML, 2010.
36. Arvind Narayanan 和 Vitaly Shmatikov. 如何打破 Netflix 大奖数据集的匿名性。arXiv 预印本 cs/0610105, 2006.
37. Milad Nasr, Reza Shokri, 和 Amir Houmansadr. 深度学习的全面隐私分析：针对集中式和联合学习的被动和主动白盒推理攻击。在 2019 IEEE 安全与隐私研讨会 (SP)，页码 739–753. IEEE, 2019.
38. Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, 和 Úlfar Erlingsson. 使用 PATE 进行可扩展的私有学习。arXiv 预印本 arXiv:1802.08908, 2018.
39. Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X. Yu, Sashank J. Reddi, 和 Sanjiv Kumar. AdaClip：私有 SGD 的自适应剪裁。arXiv 预印本 arXiv:1908.07643, 2019.
40. Reza Shokri 和 Vitaly Shmatikov. 保护隐私的深度学习。在第 22 届 ACM SIGSAC 计算机与通信安全会议论文集，页码 1310–1321, 2015.
41. Reza Shokri, Marco Stronati, Congzheng Song, 和 Vitaly Shmatikov. 对机器学习模型的成员推断攻击。在 2017 IEEE 安全与隐私研讨会 (SP)，页码 3–18. IEEE, 2017.
42. Congzheng Song, Thomas Ristenpart, 和 Vitaly Shmatikov. 记忆太多的机器学习模型。在 2017 ACM SIGSAC 计算机与通信安全会议论文集，页码 587–601, 2017.
43. Shuang Song, Kamalika Chaudhuri, 和 Anand D. Sarwate. 具有差分隐私更新的随机梯度下降。在 2013 IEEE 全球信号与信息处理会议，页码 245–248. IEEE, 2013.
44. Latanya Sweeney. k-匿名性：一种保护隐私的模型。《国际不确定性、模糊性和知识基础系统期刊》，10(05): 557–570, 2002.
45. ADP 团队等. 大规模隐私保护学习。苹果机器学习期刊，1(8), 2017.
46. Yu-Xiang Wang, Borja Balle, 和 Shiva Prasad Kasiviswanathan. 采样的 Rényi 差分隐私和分析矩会计。在第 22 届国际人工智能与统计学会议，页码 1226–1235. PMLR, 2019.
47. Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, 和 Jeffrey Naughton. 用于可扩展随机梯度下降分析的插件式差分隐私。在 2017 ACM 数据管理国际会议论文集，页码 1307–1322, 2017.
48. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, 和 Oriol Vinyals. 理解深度学习需要重新思考泛化。arXiv 预印本 arXiv:1611.03530, 2016.
49. Yuqing Zhu 和 Yu-Xiang Wang. 泊松子采样的 Rényi 差分隐私。在国际机器学习会议，页码 7634–7642, 2019.

---

### 附录 A：实验设置

我们在 Objax 中实现了我们的算法，这是一个允许高效训练神经网络的机器学习库。由于 DP-SGD 需要每个样本的梯度，而不是标准 SGD 所需的小批量平均梯度，因此训练模型是计算密集型的。我们利用 JAX 的每个样本并行处理来加速训练过程。（开源存储库 URL 为评审目的被屏蔽。）

#### A. 数据集

我们在三个数据集上进行了实验：

- **MNIST**：我们使用了 MNIST 图像数据集，该数据集包含 70,000 张 28 × 28 的手写灰度数字图像，任务是识别这些数字。MNIST 将训练数据分为 60,000 张图像用于训练，10,000 张用于测试。
- **CIFAR**：我们使用 CIFAR10，这是一个标准基准数据集，包含 32 × 32 的 RGB 图像。学习任务是将图像分类为 10 类不同的对象。该数据集被划分为 50,000 张用于训练，10,000 张用于测试。
- **Purchase**：Purchase 数据集是从 Kaggle 的“获得有价值的购物者”挑战赛中提取的数千名在线客户的购物记录 [28]。我们使用了经过处理的数据集（感谢 [49] 的作者）。数据集中的每条记录都是一个用户的购物历史。数据集包含 600 种不同的产品，每个用户都有一个二进制记录，表示是否购买了每种产品。记录根据购买的相似性被聚类成 10 类。我们使用 18,000 条记录进行训练，1,000 条记录进行测试。

#### 超参数

对于 CIFAR10 和 MNIST，我们使用交叉熵作为损失函数。对于 MNIST 和 CIFAR10 数据集，我们使用了四层卷积神经网络 [23]。对于 Purchase 数据集，我们使用了三层全连接神经网络。为了训练模型，我们使用了差分隐私 SGD [1]，其中 MNIST 和 CIFAR 的学习率为 0.15，Purchase 数据集的学习率为 1.1；MNIST 和 CIFAR 的裁剪因子为 1.0，Purchase 数据集的裁剪因子为 0.05。我们对 MNIST、CIFAR 和 Purchase 数据集分别重复了 1,000 次实验，并对恶意数据集进行了 1,000,000 次实验。我们使用 DPSGD 参数的不同噪声乘数（0.5, 0.6, 0.9, 1.0）进行了评估，并选择了提供最佳实际 epsilon 的那一个。对于 MNIST，我们分别获得了 95%, 96.0%, 97%, 98.5% 的准确率；对于 CIFAR，我们分别获得了 53%, 55%, 56%, 59% 的准确率；对于 Purchase，我们分别获得了 37%, 82%, 88%, 90% 的准确率，对应的 epsilon 分别为 1, 2, 4, 10。

---

### 附录 B：结果总结

图 9 显示了本文结果的概述。我们针对不同数据集的目标特定理论 epsilon 进行了 DPSGD。由于所有训练数据集的实例数量都在 10^4 到 10^5 之间，我们选择 δ = 10^-5，这意味着我们可以泄露一个训练数据集的实例。这种 δ 的设置对于 DPSGD 是典型的 [1]。数据集攻击可以实现非常接近理论界限的实际 epsilon。这个结果表明，如果对手拥有 DPSGD 的所有假设，则理论分析是紧致的，研究人员应该更多地关注放松一些假设。梯度攻击的结果也非常接近理论界限，这意味着对手不需要修改数据集。从访问梯度到只访问输入，我们可以看到经验隐私参数的显著下降。当对手可以访问中间步骤模型（例如，在联邦学习中），仍然可以实现接近理论值的隐私参数。然而，当对手不恶意修改训练数据集、梯度或输入时，实际 epsilon 比理论值低得多。

总体而言，结果表明在最坏情况下 DPSGD 的紧致性，但在平均情况下 DPSGD 的宽松性。

---

### 不同的 DPSGD 分析

Abadi 等人 [1] 首次引入了 DPSGD 和动量会计来分析隐私界限。此后，许多工作 [13, 2, 54, 41, 31, 3] 改进了 DPSGD 的分析，以获得更好的理论隐私界限。如前所述，我们使用了 Rényi 差分隐私 (RDP) [41] 来计算理论隐私界限。最近，Dong 等人 [13] 引入了高斯差分隐私 (GDP) 来展示一种计算隐私界限的紧致方法。然而，这两种方法都得到了类似的隐私界限。使用 MNIST 数据集的超参数（采样率为 256/60000，60 个 epoch），我们使用 Rényi 和高斯差分隐私计算了最终的隐私成本，如图 10 所示。正如我们所见，两种方法得到的隐私界限相当，并且在我们的测量误差范围内。

---