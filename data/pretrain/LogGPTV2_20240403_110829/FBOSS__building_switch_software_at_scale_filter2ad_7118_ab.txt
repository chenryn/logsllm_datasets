debugged. Lastly, since the code is open source, we make
our changes available back to the world and benefit from
discussions and bug fixes produced by external contributors.
Our experiences with general software services showed
that this principle is largely successful in terms of scalability,
code reuse, and deployment. Therefore, we designed FBOSS
based on the same principle. However, since data center net-
works have different operational requirements than a general
software service, there are a few caveats to naively adopting
this principle that are mentioned in Section 8.
2.2 Deploy-Early-and-Iterate
Our initial production deployments were intentionally lack-
ing in features. Bucking conventional network engineering
wisdom, we went into production without implementing a
long list of “must have” features, including control plane
policing, ARP/NDP expiration, IP fragmentation/reassembly,
or Spanning Tree Protocol (STP). Instead of implementing
these features, we prioritized on building the infrastructure
and tooling to efficiently and frequently update the switch
software, e.g., the warm boot feature (Section 7.1).
Keeping with our motivation to evolve the network quickly
and reduce complexity, we hypothesized that we could dy-
namically derive the actual minimal network requirements by
iteratively deploying switch software into production, observ-
ing what breaks, and quickly rolling out the fixes. By starting
small and relying on application-level fault tolerance, a small
initial team of developers were able to go from nothing to
code running in production in an order of magnitude fewer
person-years than in typical switch software development.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
S. Choi et al.
Figure 3: Average CPU utilization of FBOSS on across
various type of switches in one of Facebook’s data cen-
ters.
be quickly accessed by the ASIC; a parse pipeline, consisting
of a parser and a deparser, which locates, extracts, saves
the interesting data from the packet, and rebuilds the packet
before egressing it [19]; and match-action units, which specify
how the ASIC should process the packets based on the data
inside the packet, configured packet processing logic and the
data inside the ASIC memory.
PHY. The PHY is responsible for connecting the link-layer
device, such as the ASIC, to the physical medium, such as an
optical fiber, and translating analog signals from the link to
digital Ethernet frames. In certain switch designs, PHY can be
built within the ASIC. At high-speeds, electrical signal inter-
ference is so significant that it causes packet corruption inside
a switch. Therefore, complex noise reduction techniques, such
as PHY tuning [43], are needed. PHY tuning controls various
parameters such as preemphasis, variable power settings, or
the type of Forward Error Correction algorithm to use.
Port Subsystem. The port subsystem is responsible for
reading port configurations, detecting the type of ports in-
stalled, initializing the ports, and providing interfaces for the
ports to interact with the PHY. Data center switches house
multiple Quad Small Form-factor Pluggable (QSFP) ports.
A QSFP port is a compact, hot-pluggable transceiver used
to interface switch hardware to a cable, enabling data rates
up to 100Gb/s. The type and the number of QSFP ports are
determined by the switch specifications and the ASIC.
FBOSS interacts with the port subsystem by assigning
dynamic lane mapping and adapting to port change events.
Dynamic lane mapping refers to mapping multiple lanes in
each of the QSFPs to appropriate port virtual IDs. This allows
changing of port configurations without having to restart the
switch. FBOSS monitors the health of the ports and once any
abnormality is detected, FBOSS performs remediation steps,
such as reviving the port or rerouting the traffic to a live port.
CPU Board. There exists a CPU board within a switch that
runs a microserver [39]. A CPU board closely resembles a
commodity server, containing a commodity x86 CPU, RAM
Figure 2: A typical data center switch architecture.
Perhaps more importantly, using this principle, we were
able to derive and build the simplest possible network for our
environment and have a positive impact on the production
network sooner. For example, when we discovered that lack
of control plane policing was causing BGP session time-outs,
we quickly implemented and deployed it to fix the problem.
By having positive impact to the production network early,
we were able to make a convincing case for additional engi-
neers and with more help. To date, we still do not implement
IP fragmentation/reassembly, STP, or a long list of widely
believed “must have” features.
3 HARDWARE PLATFORM
To provide FBOSS’s design context, we first review what
typical switch hardware contains. Some examples are a switch
application-specific integrated circuit (ASIC), a port subsys-
tem, a Physical Layer subsystem (PHY), a CPU board, com-
plex programmable logic devices, and event handlers. The
internals of a typical data center switch are shown in Fig-
ure 2 [24].
3.1 Components
Switch ASIC. Switch ASIC is the important hardware
component on a switch. It is a specialized integrated circuit
for fast packet processing, capable of switching packets up
to 12.8 terabits per second [49]. Switches can augment the
switch ASIC with other processing units, such as FPGAs [53]
or x86 CPUs, at a far lower performance [52]. A switch ASIC
has multiple components: memory, typically either CAM,
TCAM or SRAM [19], that stores information that needs to
Power	SupplyFanx86	CPUASICSSDQSFP	PortsCPLDBMCTemperatureSensor0204060801000255075100125150CPU Utilizaation (%)Days→Average Switch CPU UtilizationWedge40Wedge100Wedge100sFBOSS: Building Switch Software at Scale
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
and a storage medium. In addition to these standard parts,
a CPU board has a PCI-E interconnect to the switch ASIC
that enables quick driver calls to the ASIC. The presence
of a x86 CPU enables installation of commodity Linux to
provide general OS functionalities. CPUs within switches
are conventionally underpowered compared to a server-grade
CPUs. However, FBOSS is designed under the assumption
that the CPUs in the switches are as powerful as server-grade
CPUs, so that the switch can run as much required server
services as possible. Fortunately, we designed and built our
data center switches in-house, giving us flexibility to choose
our own CPUs that fits within our design constraints. For
example, our Wedge 100 switch houses an Quad Core Intel
E3800 CPU. We over-provision the CPU, so that the switch
CPU runs under 40% utilization to account for any bursty
events from shutting down the switch. Such design choice can
be seen in various types of switches that we deploy, as seen
in Figure 3. The size allocated for the CPU board limited us
from including an even powerful CPU [24].
Miscellaneous Board Managers. A switch offloads mis-
cellaneous functions from the CPU and the ASIC to various
components to improve overall system performance. Two
examples of such components are Complex Programmable
Logic Device (CPLD) and the Baseboard Management Con-
troller (BMC). The CPLDs are responsible for status monitor-
ing, LED control, fan control and managing front panel ports.
The BMC is a specialized system-on-chip that has its own
CPU, memory, storage, and interfaces to connect to sensors
and CPLDs. BMC manages power supplies and fans. It also
provides system management functions such as remote power
control, serial over LAN, out-of-band monitoring and error
logging, and a pre-OS environment for users to install an
OS onto the microsever. The BMC is controlled by custom
software such as OpenBMC [25].
The miscellaneous board managers introduce additional
complexities for FBOSS. For example, FBOSS retrieves
QSFP control signals from the CPLDs, a process that requires
complex interactions with the CPLD drivers.
3.2 Event Handlers
Event handlers enable the switch to notify any external en-
tities of its internal state changes. The mechanics of a switch
event handler are very similar to any other hardware-based
event handlers, thus the handlers can be handled in both syn-
chronous or asynchronous fashion. We discuss two switch
specific event handlers: the link event handler, and the slow
path packet handler.
Link Event Handler. The link event handler notifies the
ASIC and FBOSS of any events that occur in the QSFP ports
or the port subsystem. Such events include link on and off
Figure 4: Growth of FBOSS open source project.
events and change in link configurations. The link status han-
dler is usually implemented with a busy polling method where
the switch software has an active thread that constantly moni-
tors the PHY for link status and then calls the user-supplied
callbacks when changes are detected. FBOSS provides a call-
back to the link event handler, and syncs its local view of the
link states when the callback is activated.
Slow Path Packet Handler. Most switches allow packets
to egress to a designated CPU port, the slow path. Similar
to the link status handler, the slow packet handler constantly
polls the CPU port. Once a packet is received at a CPU port,
the slow path packet handler notifies the switch software of
the captured packet and activates the supplied callback. The
callback is supplied with various information, which may
include the actual packet that is captured. This allows the
slow path packet handler to greatly extend a switch’s feature
set, as it enables custom packet processing without having
to change the data plane’s functionality. For example, one
can sample a subset of the packets for in-band monitoring or
modify the packets to include custom information. However,
as indicated by its name, the slow path packet handler is too
slow to perform custom packet processing at line rate. Thus it
is only suitable for use cases that involve using only a small
sample of the packets that the switch receives.
4 FBOSS
To manage the switches as described in Section 3, we devel-
oped FBOSS, vendor-agnostic switch software that can run on
a standard Linux distribution. FBOSS is currently deployed
to both ToR and aggregation switches in our production data
centers. FBOSS’s code base is publicly available as an open
source project and it is supported by a growing community.
As of January 2018, a total of 91 authors have contributed to
the project and the codebase now spans 609 files and 115,898
lines of code. To give a scope of how much lines of code a fea-
ture may take to implement, implementing link aggregation
in FBOSS required 5,932 lines of newly added code. Note
that this can be highly variable depending on the feature of
interest, and some features may not be easily divisible from
one another. Figure 4 shows the growth of the open source
024681012Mar-15Sep-15Mar-16Sep-16Mar-17Sep-17Lines of Code (x10000)Growth of FBOSS CodebaseSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
S. Choi et al.
4.1 Architecture
FBOSS consists of multiple interconnected components
that we categorize as follows: Switch Software Development
Kit (SDK), HwSwitch, Hardware abstraction layer, SwSwitch,
State observers, local config generator, a Thrift [2] manage-
ment interface and QSFP service. FBOSS agent is the main
process that runs most of FBOSS’s functionalities. The switch
SDK is bundled and compiled with the FBOSS agent, but is
provided externally by the switch ASIC vendor. All of the
other components besides the QSFP service, which runs as
its own independent process, reside inside the FBOSS agent.
We discuss each component in detail, except the local config
generator, which we will discuss in Section 6.
Switch SDK. A switch SDK is ASIC vendor-provided soft-
ware that exposes APIs for interacting with low-level ASIC
functions. These APIs include ASIC initialization, installing
forwarding table rules, and listening to event handlers.
HwSwitch. The HwSwitch represents an abstraction of
the switch hardware. The interfaces of HwSwitch provide
generic abstractions for configuring switch ports, sending and
receiving packets to these ports, and registering callbacks for
state changes on the ports and packet input/output events that
occur on these ports. Aside from the generic abstractions,
ASIC specific implementations are pushed to the hardware
abstraction layer, allowing switch-agnostic interaction with
the switch hardware. While not a perfect abstraction, FBOSS
has been ported to two ASIC families and more ports are in
progress. An example of a HwSwitch implementation can be
found here [14].
Hardware Abstraction Layer. FBOSS allows users to
easily add implementation that supports a specific ASIC by
extending the HwSwitch interface. This also allows easy sup-
port for multiple ASICs without making changes to the main
FBOSS code base. The custom implementation must sup-
port the minimal set of functionalities that are specified in
HwSwitch interface. However, given that HwSwitch only
specifies a small number of features, FBOSS allows custom
implementation to include additional features. For example,
open-source version of FBOSS implements custom features
such as specifying link aggregation, adding ASIC status mon-
itor, and configuring ECMP.
SwSwitch. The SwSwitch provides
the hardware-
independent logic for switching and routing packets, and in-
terfaces with the HwSwitch to transfer the commands down to
the switch ASIC. Some example of the features that SwSwitch
provides are, interfaces for L2 and L3 tables, ACL entries,
and state management.
State Observers. SwSwitch make it possible to implement
low-level control protocols such as ARP, NDP, LACP, and
Figure 5: Switch software and hardware components.
Figure 6: Architecture overview of FBOSS.
project since its inception. The big jump in the size of the
codebase that occurred in September of 2017 is a result of
adding a large number of hardcoded parameters for FBOSS
to support a particular vendor NIC.
FBOSS is responsible for managing the switch ASIC and
providing a higher level remote API that translates down to
specific ASIC SDK methods. The external processes include
management, control, routing, configuration, and monitoring
processes. Figure 5 illustrates FBOSS, other software pro-
cesses and hardware components in a switch. Note that in our
production deployment, FBOSS share the same Linux envi-
ronment (e.g., OS version, packaging system) as our server
fleet, so that we can utilize the same system tools and libraries
on both servers and switches.
BMCx86 MicroserverSwitch ASICSoftwareSwitch HardwareFront Panel Ports and ModulesConsole and OOBManagement PortsLinux KernelStandardSystem Tools & LibrariesMonitorConfiguration ManagerRouting DaemonFBOSS(Agent, QSFP Service, CLI)OpenBMCSwitch SDKPowerSupplyFansLink Status HandlerSlow PathPacket HandlerSwitch ASICAPIsSwitch Feature Implementation(L2, L3, ACL, LAG)Event HandlerCallbackHardware AbstractionGeneric Switch Hardware AbstractionHwSwitchHardware Independent Routing LogicSwitch State ManagerThrift HandlerSwSwitchSDKFBOSS AgentThrift Management InterfaceState ObserversQSFP ServiceLocal ConfigGeneratorFBOSS: Building Switch Software at Scale
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
1: string mac,
2: i32 port,
3: i32 vlanID,
1 struct L2EntryThrift {
2
3
4
5 }
6 list getL2Table()
7