the critical path, since fault diagnosis occurs only after error detection.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:52 UTC from IEEE Xplore.  Restrictions apply. 
2.3  End-To-End Invariants to Check
While we could dynamically verify a number of system-
wide invariants, we present here two speciﬁc checkers that
are motivated by the previous two subsections. Both invari-
ants are independent of the implementation and we could
thus also check them to dynamically verify SMPs which sat-
isfy the same properties with simpler implementations.
• In Section 5, we present a technique for dynamically
verifying that every cache coherence upgrade has a cor-
responding coherence downgrade at another node (or
nodes) in the system.
• In Section 6, we present a technique for dynamically
verifying that every node in the system observes the
same total order of cache coherence requests.
3  Hardware Backward Error Recovery
with SafetyNet
While checker failure could trigger a system crash, higher
availability can be achieved by invoking a backward error
recovery (BER) scheme. We choose the recently-developed
SafetyNet BER mechanism [16], an all-hardware scheme
that can hide the long error detection latencies incurred by
end-to-end invariant checking. SafetyNet periodically check-
points the system state, to allow the system to recover its
state to a consistent previous checkpoint. If the checkers
detect an error, SafetyNet recovers the state to the recovery
point, the old checkpoint most recently validated error-free.
Checkpoints between the recovery point and the active sys-
tem state are pending validation. A system-wide checkpoint
includes the state of the processor registers, memory values,
and coherence permissions. SafetyNet handles system I/O
with the standard solution of buffering inputs and delaying
outputs until validation [9]. We provide an overview of its
operation, but we refer interested readers to Sorin et al. [16]
for implementation speciﬁcs and more detailed evaluation.
Checkpointing Via Logging. Logically, SafetyNet check-
points contain a complete copy of the system’s architectural
state. SafetyNet explicitly checkpoints registers and incre-
mentally checkpoints memory state by logging previous val-
ues and coherence permissions. Conceptually, processors
and memory controllers log every change to the mem-
ory/coherence state (i.e., save the old copy of the block)
whenever they might have to undo an action (i.e., a store or a
transfer of ownership). To reduce storage and bandwidth
requirements, SafetyNet only logs the block on the ﬁrst such
action per checkpoint interval. By using coarse checkpoint
intervals, SafetyNet signiﬁcantly reduces logging overhead.
Creating Consistent Checkpoints. All of the components
(cache and memory controllers) coordinate their local check-
points, so that the collection of local checkpoints represents
a consistent global recovery point. Coordinated system-wide
Checkpoint 1 Checkpoint 2
Checkpoint 3
Checkpoint 4
Validated
Execution
Checkpoints Pending Validation
Active
Execution
time
FIGURE 3. Pipelined checkpoint validation
that
time, at
checkpoints
checkpointing avoids both cascading rollbacks [8] and an
output commit problem [9] for inter-node communication.
SafetyNet coordinates checkpoints across the system in logi-
cal time to avoid a potentially costly exchange of synchroni-
zation messages. To ensure
reﬂect
consistent system states, the logical time base must ensure
that all components can independently determine the check-
point interval in which any coherence transaction occurs (not
just its request). One basis of logical time in an SMP is for
each component to create a checkpoint after every Tc broad-
cast coherence requests (i.e., logical cycles). All nodes can
agree when a transaction occurred, since a transaction
appears in retrospect (i.e., after it has completed) to have
the time of the
occurred atomically in logical
th transaction
request. Thus all nodes can agree that the Tc
happened before the checkpoint and the Tc+1th transaction
happened after it. A lost coherence request is not a problem
because, even though this error can affect a node’s current
logical time, it does not affect any node’s view of the recov-
ery point (i.e., a checkpoint from before the error occurred).
Thus, when the checkers detect the error (the lost request),
all nodes will still recover to the same recovery point.
Validating Checkpoints off the Critical Path. Checkpoint
validation is the process of determining that a checkpoint is
error-free (e.g., using the checkers developed later) and can
be made the new recovery point. The key to enabling long-
latency error detection is that SafetyNet can pipeline valida-
tion and perform it in the background. Figure 3 illustrates an
example in which the active execution can proceed unim-
peded while the system works to validate Checkpoints 2-4.
Recovering to a Consistent Global State. If
the checkers
detect an error, SafetyNet restores the globally consistent
recovery point. Recovery requires that the processors restore
their register checkpoints and that the caches and memories
unroll their local logs to recover the system to the consistent
global state at the pre-error recovery point. The system dis-
cards all state associated with transactions in progress at the
time of recovery, since this state is (by deﬁnition) unvali-
dated state that occurs logically after the recovery point.
After recovery, the system reconﬁgures, if necessary, and
resumes execution.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:52 UTC from IEEE Xplore.  Restrictions apply. 
4 End-To-End Checking with Signature Analysis
In this section, we describe in general how to use distrib-
uted signature analysis to detect violations of end-to-end
system invariants due to errors. Signature analysis takes a
large amount of input data—in this case, system states and
events—and produces a small output, called a signature, that
almost-uniquely characterizes the large amount of input
data. In our schemes, in which events are incoming coher-
ence requests, compression is necessary, since coherence
requests are eight bytes each and can occur every network
cycle. The idea of signature analysis has existed for a long
time, and it is widely used in built-in self-test (BIST) [1].
However, to the best of our knowledge, this research is the
ﬁrst to use distributed signature analysis to dynamically ver-
ify end-to-end invariants of shared memory multiprocessors.
All of the N components in a distributed signature analy-
sis scheme maintain a local signature, S(i,k), where i is the
identity of the component and k is the number of events (of
interest) observed thus far at component i. The local signa-
ture is updated for every event, and we denote the kth event at
component i by E(i,k). When unambiguous, we will denote
an event simply as E, for clarity of notation. Components
update their signatures according to an update function U
that takes two parameters: S(i,k-1) and E(i,k). Thus, S(i,k) =
U[S(i,k-1), E(i,k)]. We assume that components process
events in order of occurrence. To check for errors, a central-
ized checker periodically performs a global reduction of the
local signatures.2 The checking function, C, periodically
(e.g., after every Tc events) takes all of the local signatures
from the N components as its variables and produces a bool-
ean result of the form C[S(0,nTc), S(1,nTc), ... , S(N-1,nTc)]
= {true, false}, where true denotes that the checker detected
an error. We illustrate the general situation in Figure 4.
Designing a distributed signature analysis scheme entails
choosing the two functions, U and C. We want to choose the
function U so that the same signature has an arbitrarily low
probability of characterizing two different histories (i.e.,
sequences of events), a phenomenon known as aliasing. We
say that aliasing occurs if:
(
)
)
∧
∃
)
)
=
S j k,(
m k≤(
S i k,(
With perfect anti-aliasing in U, C will detect all violations
of the invariant. However, it will not detect a violation if one
of three aliasing situations arises.
) E j m,(
≠
E i m,(
(EQ 1)
)
• Finite Resource Aliasing: Engineering restrictions limit
signatures to a ﬁnite number of bits, say b, and b bits can
only represent 2b sequences of events. Many typical sig-
2. The system performs the check periodically in logical time, in order to
obtain a consistent system view. As explained later, the system performs the
check after every Tc coherence requests (i.e., every Tc logical cycles).
Event E(i,a+2)
Event E(j,b+2)
Event E(i,a+1)
Event E(j,b+1)
Component i
Signature S(i,a)
S(i,nTc)
Component j
Signature S(j,b)
S(j,nTc)
Service Controller
Checks C[S(i,nTc),S(j,nTc)]
FIGURE 4. Generalized distributed signature analysis
example. Components maintain signatures that they
update for each event. Periodically, all components send
their signatures to the service controller which then
performs a reduction check on them to detect potential
violations of invariants.
nature analysis functions convolve the input stream with
a pseudo-random number generator so as to reduce
aliasing to an arbitrarily small probability. In hardware,
designers often implement pseudo-random number gen-
eration with a linear feedback shift register (LFSR) [10].
We do not address this issue further in this paper, since it
is simply an engineering trade-off, and we can make this
type of aliasing arbitrarily small at the cost of additional
hardware.
• Signature Analysis Fault Aliasing: Aliasing could occur
because of a fault in the signature analysis hardware
itself. We can protect this hardware from faults with
redundancy (e.g., TMR), since it is a small fraction of
total hardware, or we can consider its faults an unlikely
double-fault scenario (since a signature hardware fault
may mask a coherence error but not create one).
• Inherent Aliasing: Aliasing could occur because the
chosen update function, U, inherently suffers from alias-
ing. For example, if an update function adds the address
of the kth incoming message to S(i,k-1) and the address
is zero, then the occurrence of this event (the incoming
message) is indistinguishable from the case in which it
did not occur. As such, aliasing could occur even with
inﬁnite hardware resources for implementing signature
analysis. We address this form of aliasing in our exam-
ples of signature analysis, since it is a fundamental prop-
erty of the schemes and not an implementation artifact.
While aliasing can cause false negatives (i.e., we can fail
to detect an error), it cannot cause false positives (i.e., we
will not “detect” an error that did not occur).
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:52 UTC from IEEE Xplore.  Restrictions apply. 
TABLE 1. Coherence-level update functiona
e
h
c
a
c
r
e
l
l
o
r
t
n
o
c
Event E
Own
ReqForShared
Own
ReqForExclusive
Own
WritebackExclusive
Other
ReqForShared
Other
ReqForExclusive
Other
WritebackExclusive
ReqForShared
State
I->S
S(i,k) =
UCL[S(i,k-1), E(i,k)]
S(i,k-1) + Addr(E)
I,S->M S(i,k-1) + P*Addr(E)
O,M->I S(i,k-1) - Addr(E)
I,S
S(i,k-1)
S(i,k-1) - Addr(E)
O,M
I,S,O,M S(i,k-1) - Addr(E)
I,S,O,M S(i,k-1)
S(i,k-1)
S(i,k-1) - Addr(E)
I,S
O,M
I,S,O,M S(i,k-1) - Addr(E)
S(i,k-1) + Addr(E)
S(i,k-1)
5  Checking Coherence-Level Invariants
In this section, we develop a scheme for testing an end-to-
end invariant of the cache coherence protocol. The coher-
ence invariant we choose to test is that every upgrade of
coherence permissions at a controller (cache or memory) has
an appropriate downgrade at one or more other controllers.
As explained in Section 4, we test the invariant by feeding
signatures, S(i,nTc), computed at each component i, into a
global check function, C[S(0,nTc), S(1,nTc), ..., S(N-1,nTc)],
where N is the number of cache and memory controllers (and
thus N=2P). At a high level and assuming for now that every
upgrade has only one corresponding downgrade,
if an
upgrade corresponds to an addition of a constant and a
downgrade corresponds to a subtraction of the same con-
stant, the global reduction should sum to zero at the end of
every checkpoint interval.
The check function, CCL, veriﬁes that the global sum of
all local signatures equals zero:
[
,(
C CL S 0 nT c
,
,(
) S 1 nT c
,
true, if