square distributed [11]. A probability density boundary 
( )p z  can be mapped to a corresponding 
determined by 
boundary  in  its  Mahalanobis  distance.  Therefore,  we 
can  use  the  following  inequality  to  determine  the 
correlation boundary in Mahalanobis distance:   
−
m z
z
( )
Λ
P P
µ
)'(
µ
)
µ
)
((
−
=
−
=
=
−
')
−
1
−
1
z
z
z
−
1
(
(
(
(
(
2
2
(3)                                                 
=
...
1,
=
(2)
(1)
l
= Λ =
−
1
r
t
'
≤
2
χ α
)
(
−
µ
Σ
)'
t
t
2
2
1
2
λ λ
1
2
+
r
t
λ and 
1
λ are the eigenvalues of the covariance 
where 
2
matrix  ∑ , α is  the  probability  coverage  in  Gaussian 
distribution and 
λ
1
0
Λ =
t
1
t
µ
)
P
=
−
=
')
. 
r
t
−
1
z
(
(
;
0
λ
2
For example, if 90% of data points have to be included 
.  For  a 
inside  the  boundary,  then  we  have
selected  α of a Gaussian distribution, we can calculate 
0.9α=
2
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
P m z χ α α
(
.                      (1)                                                 
)χ α  so  that
2(
( )
≤
=
2
 (2)             
2
(
))
,  i.e.,  the 
a 
probability  coverage  α determines  an  ellipse  size 
)χ α  in Mahalanobis distance with which the ellipse 
2(
could  include  100*α percentage  of  data  points.  Note 
that  Inequality  (3)  represents  an  ellipse  in  two-
dimension space of monitoring data. Each cluster in the 
mixture  corresponds  to  an  ellipse  so  that  we  totally 
have  G  ellipses 
to  characterize  the  probabilistic 
relationship between monitoring data.                  
If  the parameters  of  Gaussian  distributions, θ,  are 
known  and  α is  given,  we  can  use  Inequality  (3)  to 
determine  the  boundary  in  Mahalanobis  distance. 
Given  the real  data  observations, the  question here  is 
how  to  learn the  parameters θ with  which the  GMM 
could  best  approximate the real  data distribution.  The 
usual choice for maximizing the posteriori estimates of 
mixture  parameters  is  the  well  known  EM  algorithm. 
Due  to  system  dynamics  such  as  varying  workloads 
and uncertainties such as caching, we use an online and 
recursive EM algorithm to update models dynamically 
in  operational  environments.  A  new  data  point  is 
regarded  as  an  outlier  if  it  locates  outside  of  the 
selected  boundary.  This  section  proposes  a  fault 
detection  algorithm  which  includes  the  recursive  EM 
algorithm  as  well  as  a  method  from  outlier  detection. 
The  recursive  EM  algorithm 
is  an  online 
discounting  variant  of  EM  algorithm.  In  real  time 
applications,  a  recursive  EM  algorithm  is  better  than 
the  classic  EM  in  the  sense  that  the  parameters  are 
updated  online  with  a  forgetting  factor  degrading  the 
influence of out-of-date samples. Thus a recursive EM 
algorithm is capable of adapting to system dynamics in 
real time. 
[4] 
(
,
,
l
θ
)
(1)
−
1
−
1
(2)
=
=
Z
l
{
) |
E-Step: 
Q
θ
M-Step: $
k
the 
)G
)
θθ
E
(log (
( ,
$
k
l
=
θθ−
Q
arg max(
( ,
$ 1
k
set  of  data 
The  EM  algorithm  is  an  iterative  procedure  that 
searches  for  the  parameters  that  maximize  a  log-
likelihood  function  as  shown  in  Equation  (4).  It 
consists of the following two basic steps:  
θ θ
p Z l
. 
)
|
,
$
k
.               (4) 
))
samples,  and 
where  Z is 
 is  a  binary  vector  of  G labels 
l
l
,...,
indicating which cluster a data sample belongs to, for 
=  it  means  that  this 
example  if 
sample  belongs  to  cluster  1.  For  GMMs,  the  EM 
algorithm is based on the observations  Z as incomplete 
data and the missing part is the information of labels. 
p Z l θ ,  from 
The  complete  log  likelihood  is log (
which we could estimate θ if the complete data { , }Z l
is given. The first step, referred to as E-Step, computes 
the  conditional  log-likelihood  given  the  observation 
Z and the parameter estimate  $ 1kθ− . The second step is 
=
0
)
l
l
,
G
|
(
)
to  search  the  parameters  that  maximize  the  log-
likelihood.  
j
j
|
G
,
θ
)
)
(
=
1
−
1
−
1
=
=
=
Z
E
) |
Q θθ−
$ 1
k
( ,
α
)
ˆ
θ
k
 V(
α
c
j
p Z l
,
(log (
J J Q
θ θ
$
k
arg max ;
Rather  than  maximizing 
 in  the  standard 
EM  algorithm,  we  maximize  the  following  criteria  in 
the recursive EM algorithm [4]. 
θθ
( ,
$
k
−
)
+ ∏           (5)  
log
l
)V α is  introduced  as  a  penalty  function  to 
where 
remove  unnecessary  clusters  in  the  mixtures.  Here 
V
the  negative 
α α α
}G
. It is straightforward to see that the penalty 
∏ is 
logarithm of a Dirichlet prior [12] and 
=∑
G
α
j
=
j
1
fewer  clusters. 
function  always  decreases  with 
 where  D  represents 
jc
the  number  of 
parameters per cluster. For the Maximum A Posteriori 
(MAP) solution, using Lagrange optimization method, 
we can have 
1{ ,...,
D= −
∑
α
(
)
∝ −
α
c
j
α
j
= −
log
log
/ 2
=
1
, 
c
=
1
=
1
G
G
j
j
j
j
(
E
l
(log (
p Z l
,
Zθ θ
) |
$ 1
−
k
,
|
)
∂
∂
α
j
+
log
             (6) 
G
α λ α
j
+
∑
(
c
j
j
−
1))
=
0
G
∏
=
1
=
1
j
j
where  λ  is 
the  Lagrange  multiplier.  Based  on 
Equation  (6),  the  following  recursive  equations  are 
obtained: 
t
)
; 
−
t
j
−
α ρ
1
c
T
−
Gc
T
          (7) 
t
t
t
j
t
j
)
+
1
+
1
+
1
+
=
=
α
t
j
o z
t
(
j
θ
p z
|
(
$
j
j
t
θ
+
p z
t
)
|
(
1
$
+
o z
t
t
)
(
1
α α ρ
j
−
1
−
o z
c
(