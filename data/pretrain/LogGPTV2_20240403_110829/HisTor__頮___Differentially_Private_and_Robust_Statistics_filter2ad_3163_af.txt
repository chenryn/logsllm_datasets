In comparison, HisTor also applies differential privacy tech-
niques to privately collect data, but in addition provides strong
integrity guarantees about the inﬂuence of malicious DCs.
PrivCount [18] extends the PrivEx secret-sharing variant
to make it suitable for a small-scale research deployment. It
allows multi-phase iterative measurements and expands the
privacy notion of PrivEx to simultaneously handle multiple
and diverse Tor statistics with an optimal allocation of the
 privacy budget. However, even in PrivCount, there are no
protections for the integrity of the statistics (see §III).
We use a slight modiﬁcation of the (, δ)-differential pri-
vacy scheme of Chen et al. [6]. They use a single mix, which
they call a proxy. This allows a malicious proxy to undetectably
alter the data and cause the analyst to reach an inaccurate
aggregate result. We detect such manipulation in HisTor by
adding redundancy across three mixes. As we prove in §VII,
HisTor detects such tampering if at least one of the mixes is
honest, and can attribute the misbehavior if two of the three
mixes are honest. Additionally, the scheme of Chen et al.
is vulnerable to compulsion attacks. HisTor uses oblivious
counters to mitigate such risks.
HisTor is partially inspired by SplitX, which executes
differentially private queries over distributed data [7]. Like
HisTor, SplitX uses the (, δ)-differential privacy scheme
of Chen et al. and uses xor-based encryption to distribute
secret shares to separate mixes. In SplitX, both the mixes
and the aggregator are assumed to be honest-but-curious. In
HisTor, we are able to tolerate a malicious mix by redundantly
encoding information in secret shares.
McSherry and Mahajan [25] apply differential privacy
techniques to the PINQ programming interface [26] in order
Initialization Time1.1701.1751.1801.1851.1901.1951.2001.2051.2101.215Time (Seconds)Incrementing Time0.00.51.01.52.02.53.03.54.0Time (s)Mapping Time700705710715720725730735740745750Time (ms)Random Vector Gen Time5.65.86.06.26.46.66.87.07.2Time (ms)Decryption Time50.050.551.051.552.052.553.053.554.054.5Time (Seconds)Noise Gen Time640650660670680690700710720730740Time (ms)Shuffle Time1.801.851.901.952.002.052.10Time (Seconds)Aggregation Time13.914.214.514.815.115.415.716.016.316.6Time (ms)Unmask Time570580590600610620630640650660670Time (ms)Verification Time2.152.202.252.302.352.402.452.50Time (Seconds)to support privacy-preserving network trace analysis. They
show that performing trace analysis in a differentially private
setting is technically feasible, but do not offer a distributed
solution. Combining their network trace analysis techniques
with HisTor could potentially allow network operators to
identify patterns of misbehavior in Tor. We leave synthesizing
these techniques as an exciting avenue of future research.
XI. DISCUSSION AND LIMITATIONS
In this section, we discuss practical aspects of deploying
HisTor, as well as some of the system’s limitations.
the threat
Detectability, attribution and suitability of
model.
A malicious mix may attempt to manipulate the
results of a query by modifying the inputs it receives from
the DCs. As we discuss in §VI and §VII, an analyst can
detect that misbehavior occurred if at least one of the mixes is
honest. Since data is replicated across all three mixes, we can
additionally attribute the misbehavior to a speciﬁc malicious
mix if exactly two of the three mixes are honest—the malicious
mix will be revealed through its non-conforming output.
The difﬁculty with performing attribution is that the cases
of one malicious mix vs. two malicious mixes can be in-
distinguishable if, in the latter case, the two mixes perform
identical manipulations. Unless it is readily apparent through
some other mechanism which mix(es) has been compromised,
a reasonable solution once misbehavior is detected is to re-
evaluate the security of all three mixes.
More generally, mixes should be carefully selected, since
collusion between two or more dishonest mixes compromises
data privacy. This is, to some degree, similar to Tor’s existing
if a majority of the Tor
quasi-centralized notion of trust:
directory authorities are compromised,
then Tor offers no
anonymity protections since the directories could advertise
only the existence of malicious relays.
Like directory authorities, mixes must therefore be chosen
carefully. We envision that the maintainers of the Tor Project
could selectively grant permission to operate HisTor mixes
to parties. Or, to keep the existing level of trust, the directory
authorities could additionally operate mixes.
From the perspective of integrity, it may at ﬁrst blush
seem that HisTor and PrivEx offer similar guarantees—that
is, certain nodes must behave honestly to ensure integrity
of the query results. We argue that the integrity guarantees
offered by HisTor are signiﬁcantly stronger, since in PrivEx,
a single relay can signiﬁcantly perturb the results of a query.
Successfully compromising PrivEx statistics gathering is thus a
fairly simple operation since there are no barriers to operating
a relay. In contrast, HisTor removes the necessity to trust the
data collectors, and instead relies on a much smaller set of
nodes (i.e., mixes). Additionally, so long as a single mix is
honest, query tampering can be trivially detected.
Selection of . A consistent problem in schemes that apply
differential privacy is selecting an appropriate value of . In
our experimentation, we apply the same conservative value as
existing work [25] and set  = 1.
Since  is a relative and not an absolute measure of
privacy, an interesting area of future work is to derive per-
query values of  that are guaranteed to protect individuals with
14
some ﬁxed probability. Lee and Clifton [21] provide one such
construction for -differential privacy. Incorporating this se-
lection process into HisTor (which provides (, δ)-differential
privacy) is an exciting potential future research direction.
In the current
implementation of HisTor,
the analyst
communicates its choice of  to the mixes. Since the number of
noise records is proportional to −2, large values of  offer little
security while too small values of  have little beneﬁt to privacy
while incurring potentially enormous communication costs. To
provide a simple sanity-check, a real-world deployment of
HisTor could establish system-wide parameters max and min
that bound the analyst’s choice.
Privacy budget.
(, δ)-differential privacy schemes impose
a privacy budget whose balance is decremented as a function
of δ and  for each issued query. This budget is deﬁned in
terms of a static database D over which queries are issued.
In HisTor, counters are zeroed after each epoch, effectively
resulting in a new database. This thus signiﬁcantly reduces the
risk of exceeding the privacy budget.
Unfortunately, for certain query types, there may be depen-
dencies in the statistic of interest between epochs that violates
this assumption of independence. This further motivates a
careful selection of  to minimize this privacy risk.
XII. CONCLUSION
This paper presents HisTor, a distributed statistics collec-
tion system for Tor. Unlike existing work, HisTor provides
strong integrity guarantees for collecting data on Tor. In
particular, we demonstrate that the inﬂuence of a colluding
group of malicious data collectors is tightly bounded by the
fraction of nodes that
in the network. More
practically speaking, HisTor ensures that a small colluding
group of malicious data collectors has negligible impact on
the results of statistics queries.
they control
In addition to ensuring integrity, HisTor provides strong
privacy guarantees as long as malicious mixes do not collude
with a dishonest analyst. HisTor also achieves resistance to
compulsion attacks through use of novel oblivious counters.
We demonstrate using real-world data sets and realistic
query workloads that HisTor enables highly accurate statistics
aggregation, with small bandwidth and computational over-
heads. Our performance experiments and microbenchmarks
indicate that dozens of simultaneous HisTor queries could
be supported on a single CPU core. To encourage its use by
privacy researchers, we are planning an open-source release of
HisTor in the near future.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their insightful
comments. We also thank Aaron Johnson, Henry Tan, and
Sridhar Venkatesan for the valuable discussions. This paper
is partially funded from National Science Foundation (NSF)
grants CNS-1149832 and CNS-1527401. The ﬁndings and
opinions expressed in this paper are those of the authors and
do not necessarily reﬂect the views of the NSF.
REFERENCES
[1] Akamai’s State of
the Internet Q2 2015 Report,
https://www.akamai.com/
at
Available
2015.
us/en/multimedia/documents/state-of-the-internet/
2015-q2-cloud-security-report.pdf.
[2] K. Bauer, M. Sherr, D. McCoy, and D. Grunwald. Ex-
perimenTor: A Testbed for Safe and Realistic Tor Ex-
perimentation. In USENIX Workshop on Cyber Security
Experimentation and Test (CSET), August 2011.
[3] A. Bhattachayya. On a Measure of Divergence between
two Statistical Population Deﬁned by their Population
Distributions. Bulletin Calcutta Mathematical Society,
35:99–109, 1943.
[4] A. Biryukov, I. Pustogarov, F. Thill, and R.-P. Weinmann.
Content and Popularity Analysis of Tor Hidden Services.
In International Conference on Distributed Computing
Systems Workshops (ICDCSW), 2014.
[5] T.-H. H. Chan, E. Shi, and D. Song. Private and Continual
Release of Statistics. ACM Transactions on Information
and System Security (TISSEC), 14(3), 2011.
[6] R. Chen, A. Reznichenko, P. Francis, and J. Gehrke.
Towards Statistical Queries over Distributed Private User
In USENIX Symposium on Networked Systems
Data.
Design and Implementation (NSDI), 2012.
[7] R. Chen, I. E. Akkus, and P. Francis. SplitX: High-
In Conference on Ap-
performance Private Analytics.
plications, Technologies, Architectures, and Protocols for
Computer Communications (SIGCOMM), 2013.
[8] R. Dingledine and S. Murdoch. Performance Improve-
ments on Tor, or, Why Tor is Slow and What We’re Going
to Do About It. https://svn.torproject.org/svn/projects/
roadmaps/2009-03-11-performance.pdf, March 2009.
[9] R. Dingledine, N. Mathewson, and P. Syverson. Tor: The
In USENIX Security
Second-Generation Onion Router.
Symposium (USENIX), August 2004.
[10] C. Dwork. Differential Privacy. Automata, Languages
and Programming, pages 1–12, 2006.
[11] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and
M. Naor. Our Data, Ourselves: Privacy via Distributed
In Advances in Cryptology (Euro-
Noise Generation.
crypt), 2006.
[12] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum.
Differential Privacy under Continual Observation.
In
ACM Symposium on Theory of Computing (STOC), 2010.
[13] C. Dwork, A. Roth, et al. The algorithmic foundations
of differential privacy. Foundations and Trends in Theo-
retical Computer Science, 9(3-4):211–407, 2014.
[14] T. Elahi, K. Bauer, M. AlSabah, R. Dingledine, and
I. Goldberg. Changing of the Guards: A Framework for
Understanding and Improving Entry Guard Selection in
In ACM Workshop on Privacy in the Electronic
Tor.
Society (WPES), 2012.
[15] T. Elahi, G. Danezis, and I. Goldberg. PrivEx: Private
Collection of Trafﬁc Statistics for Anonymous Commu-
nication Networks. In ACM Conference on Computer and
Communications Security (CCS), November 2014.
[16] S. Goldwasser and S. Micali. Probabilistic Encryption.
Journal of Computer and System Sciences, 28(2):270–
299, 1984.
[17] R. Jansen and N. Hopper. Shadow: Running Tor in
In
a Box for Accurate and Efﬁcient Experimentation.
15
Network and Distributed System Security Symposium
(NDSS), 2012.
[18] R. Jansen and A. Johnson. Safely Measuring Tor. In ACM
Conference on Computer and Communications Security
(CCS), 2016.
[19] A. Johnson, C. Wacek, R. Jansen, M. Sherr, and P. Syver-
son. Users Get Routed: Trafﬁc Correlation on Tor By
Realistic Adversaries. In ACM Conference on Computer
and Communications Security (CCS), November 2013.
[20] D. Kedogan, D. Agrawal, and S. Penz.
Limits of
Anonymity in Open Environments. In Information Hiding
Workshop (IH), 2002.
[21] J. Lee and C. Clifton. How Much is Enough? Choosing
In International Conference
ε for Differential Privacy.
on Information Security, 2011.
[22] K. Loesing, S. J. Murdoch, and R. Dingledine. A
Case Study on Measuring Statistical Data in the Tor
In Financial Cryptography and
Anonymity Network.
Data Security (FC). 2010.
[23] N. Mathewson. Some Thoughts on Hidden Services (Tor
Blog Post), 2014. Available at https://blog.torproject.org/
blog/some-thoughts-hidden-services.
[24] D. McCoy, K. Bauer, D. Grunwald, T. Kohno, and
D. Sicker. Shining Light in Dark Places: Understanding
In Privacy Enhancing Technologies
the Tor Network.
Symposium (PETS), 2008.
[25] F. McSherry and R. Mahajan. Differentially-private
Network Trace Analysis. ACM SIGCOMM Computer
Communication Review, 41(4):123–134, 2011.
[26] F. D. McSherry. Privacy Integrated Queries: An Exten-
sible Platform for Privacy-preserving Data Analysis. In
ACM SIGMOD International Conference on Management
of Data (SIGMOD), 2009.
[27] M. Perry. The Trouble with CloudFlare (Tor Blog Post),
March 2016. Available at https://blog.torproject.org/blog/
trouble-cloudﬂare.
[28] M. Prince. The Trouble with Tor (CloudFlare Blog Post),
March 2016. Available at https://blog.cloudﬂare.com/
the-trouble-with-tor/.
https://developers.google.com/
[29] Protocol Buffers.
protocol-buffers/.
[32] Tor Research Safety Board. Available at https://research.
torproject.org/safetyboard.html.
[33] C. Wacek, H. Tan, K. Bauer, and M. Sherr. An Empirical
In Network and
Evaluation of Relay Selection in Tor.
Distributed System Security Symposium (NDSS), Febru-
ary 2013.
[34] P. Winter, R. K¨ower, M. Mulazzani, M. Huber, S. Schrit-
twieser, S. Lindskog, and E. Weippl. Spoiled Onions: Ex-
posing Malicious Tor Exit Relays. In Privacy Enhancing
Technologies Symposium (PETS), 2014.
[30] C. Soghoian. Enforced Community Standards For Re-
search on Users of the Tor Anonymity Network. In Work-
shop on Ethics in Computer Security Research (WECSR),
2011.
[31] Tor Project, Inc. Tor Metrics Portal.
https://metrics.
torproject.org/.