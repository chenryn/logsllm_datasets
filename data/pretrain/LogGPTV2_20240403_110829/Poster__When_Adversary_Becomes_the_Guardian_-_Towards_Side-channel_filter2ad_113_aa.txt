title:Poster: When Adversary Becomes the Guardian - Towards Side-channel
Security With Adversarial Attacks
author:Stjepan Picek and
Dirmanto Jap and
Shivam Bhasin
Poster: When Adversary Becomes the Guardian –
Towards Side-channel Security With Adversarial Attacks
Delft University of Technology, Delft,
Nanyang Technological University
Nanyang Technological University
Stjepan Picek
The Netherlands
PI:EMAIL
Dirmanto Jap
Singapore
PI:EMAIL
Shivam Bhasin
Singapore
PI:EMAIL
ABSTRACT
Machine learning algorithms fall prey to adversarial examples. As
profiling side-channel attacks are seeing rapid adoption of machine
learning-based approaches that can even defeat commonly used
side-channel countermeasures, we investigate the potential of ad-
versarial example as a defense mechanism. We show that adversarial
examples have the potential to serve as a countermeasure against
machine learning-based side-channel attacks. Further, we exploit
the transferability property to show that a common adversarial
example can act as a countermeasure against a range of machine
learning-based side-channel classifiers.
CCS CONCEPTS
• Security and privacy → Side-channel analysis and counter-
measures; • Computing methodologies → Adversarial learn-
ing;
KEYWORDS
Side-channel Analysis; Profiled Attacks; Adversarial Examples; Ma-
chine Learning
ACM Reference Format:
Stjepan Picek, Dirmanto Jap, and Shivam Bhasin. 2019. Poster: When Adver-
sary Becomes the Guardian – Towards Side-channel Security With Adver-
sarial Attacks. In 2019 ACM SIGSAC Conference on Computer and Communi-
cations Security (CCS ’19), November 11–15, 2019, London, United Kingdom.
ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363284
1 INTRODUCTION
The field of deep learning has advanced rapidly in the past decade,
which is in line with the increase in computer processing power
and the proliferation of data. That progress is not limited to a
few domains but is spreading ever faster through distinct areas.
One domain where deep learning is playing a significant role is
security with a plethora of applications oriented towards making
systems more secure. Unfortunately, deep learning techniques (and,
in general, machine learning) have been demonstrated to be vulner-
able to well-crafted input samples, called adversarial examples [7].
Szegedyet et al. generated small perturbations on the images for
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11.
https://doi.org/10.1145/3319535.3363284
the image classification problem and tricked state-of-the-art deep
neural networks with high probability.
Still, as machine learning was not designed to be resilient against
adversarial attacks, it is not so surprising that such weaknesses
exist. There is also a different perspective on this problem. While
adversarial attacks are a serious threat to machine learning, when
machine learning assumes the role of the attack mechanism, then
the adversarial attack could provide a defense. Such research di-
rection is not a new one. For instance, Inci et al. used adversarial
learning as a defensive tool to obfuscate and mask private informa-
tion [3]. They considered side-channel leakage of various processes
in the form of Hardware Performance Counters (HPC) and con-
structed perturbations to cloak the leakage.
Considering side-channel analysis (SCA) domain, in recent years
machine learning techniques play an important role as they can
break many implementations, including those protected with coun-
termeasures [4]. Consequently, one can consider machine learning
(and more precisely, deep learning) to be the most powerful attack
mechanism in SCA. Then, a natural question is how can we protect
against such attacks and we investigate whether adversarial attacks
could work as a countermeasure against SCA. To that end, we con-
sider common measurements one would obtain in SCA, which are
leakages coming from power or electromagnetic emanation of a
device under attack. To properly assess the potential of adversar-
ial attack-based countermeasure, we investigate the behavior of
several commonly used machine learning techniques. To the best
of our knowledge, this is the first time that adversarial attacks are
considered as a countermeasure for such side-channel attacks.
2 BACKGROUND
Side-channel analysis (SCA) uses implementation related leakages
to mount an attack [5]. In the context of cryptography, attacks
target physical leakage from the insecure implementation of other-
wise theoretically secure cryptographic algorithms. These physical
leakages come from a variety of sources like power consumption,
timing, electromagnetic radiation, etc.
Profiled side-channel attacks are the strongest type of side-
channel attacks as they assume an adversary with access to a clone
device. There, the adversary can control all the inputs to the clone
device and observe the corresponding leakage. More precisely, the
adversary observes various measurements corresponding to ran-
dom plaintext and a key to allow detailed characterization. Then,
the adversary collects only a few additional measurements from
the attack device where the secret key is not known.
By comparing the attack measurements with the characterized
model, the secret key is revealed. Naturally, as real measurements
are noisy and are usually protected with countermeasures, this
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2673makes the attack more difficult. Nevertheless, recent results with
deep learning show it is powerful enough to approach theoretically
optimal attacks (a single measurement from the attacked device) [4].
3 SETTING AND RESULTS
3.1 SCA Threat Model
The threat model is a typical profiled side-channel scenario where
the attacker has access to a clone device running target crypto-
graphic algorithm. The clone device can be queried with a known
key and the plaintext. For each key and plaintext, the corresponding
measurement is stored. We do not limit the number of measure-
ments the attacker can obtain with the clone device. Next, the
attacker queries the attack device with a known plaintext to obtain
the unknown key. The corresponding side-channel measurements
are compared to the characterized model to recover the key.
Usually, the device is protected with some countermeasure. In
general, the countermeasures can be divided into hiding and mask-
ing type. In hiding, one tries to hide the relationship between the
key and the leakage by making it either constant or random. In
masking, one uses additional values (called masks) so the sensitive
variable is never processed on its own but always in combination
with random masks.
3.2 Adversarial-based Countermeasure
We consider a setting where a device is additionally protected with a
countermeasure in the form of adversarial examples and we denote
it as the adversarial-based countermeasure. Consequently, adver-
sarial attack (more precisely, evasion attack) is used to protect a
cryptographic implementation against profiling SCAs, especially
those based on machine learning. Evasion attack uses small per-
turbations of testing data points resulting in the wrong prediction
at the testing time on those points. In this work, we consider the
black-box setting where the adversarial-based countermeasure is
designed for a surrogate model instead of a real model, as we cannot
know which attack techniques will be used in SCA. Still, we assume
that the countermeasure could be effective due to the transferabil-
ity property [2], which is fulfilled when an attack developed for a
certain machine learning model (called a surrogate model) is also
effective against the target model.
To construct the countermeasure, we use the gradient-based
optimization technique, see, e.g., [1] with the maximum confi-
dence evasion [2]. In the gradient-based attack, the attacker uses a
gradient-ascend algorithm to manipulate the attack sample along
the gradient of the objective function, so to have it misclassified,
i.e., to evade detection by a learning algorithm. Here, we consider
a non-targeted attack where we do not aim to misclassify into any
particular (wrong) class. Finally, we use the maximum confidence
approach that produces adversarial examples misclassified with the
maximum confidence by the classifier, within the given space of
feasible modifications. Such an approach helps to improve the trans-
ferability or even to beat the countermeasure (against adversarial
attacks) based on gradient masking [2].
3.3 Dataset under Evaluation
DPAcontest v4 provides 100 000 measurements of a masked AES
software implementation [8]. As the mask is known, this can be
easily translated into an unprotected scenario. It is a software im-
plementation with the most leaking operation being the processing
of the S-box operation. The leakage model we use equals
∗
Y (k
) = Sbox[P1 ⊕ k