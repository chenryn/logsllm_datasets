loss measurements, we can also provide ﬁne-grain measurements
of the average and standard deviation of latency.
2.3 Coordinated streaming
We measure the goodness of a measurement scheme by its ac-
curacy for each metric (in terms of relative error), its storage over-
head, bandwidth requirements, and its computational overhead. A
naïve solution to the measurement problem is for the sender to store
a hash and timestamp of each sent packet and for the receiver to do
the same for each received packet. At the end of the interval, the
sender sends the hashes and timestamps for all N packets to the
receiver, who then matches the send and receive timestamps of suc-
cessfully received packets using the packet hashes, and computes
the average. Indeed, Papagiannaki et al. used a similar approach in
their study of router delays [28]. Unfortunately, the naïve solution
is very expensive in terms of our performance measures as it takes
O(N ) state at the sender and O(N ) bandwidth to communicate the
timestamps. N can be large. For example, if measurement interval
is one second, and the segment operates at 40 Gbps, then N can be
as large as 125 million 40-byte packets. We aim for a scheme that
is well within the capabilities of today’s ASICs.
The quest for efﬁcient solutions suggests considering streaming
algorithms. Several streaming algorithms are already popular in
the networking community for various applications such as ﬁnding
heavy hitters [11], counting ﬂows [12], estimating entropy [20], and
computing ﬂow-size distributions [19, 22]. The standard setting
for streaming problems considers a single computational entity that
receives a stream of data: The goal is to compute a function f of a
single set of N values using a synopsis data structure that is much
smaller than N.
Latency measurement, by contrast, is what we term a coordi-
nated streaming problem with loss. In the general setting, we have
two computational entities A and B. There are two streams of
data values ax and bx; ax is the time packet x left A, and bx is
the time it is received at B. Some packets are lost, so bx may be
undeﬁned. The goal here is to compute some function f of the
P
set of (ax, bx) pairs. For measuring average latency, the function is
x(bx−ax) over the cardinality of the set of packets for which ax
P
is deﬁned (i.e., packets that are received and not lost). For measur-
x(bx − ax)2 over the received pack-
ing variance, the function is
ets. For measuring, say, the maximum delay, the function would
be max(bx − ax). In all cases, the function requires a pairwise
matching between a received data item and the corresponding sent
item—a requirement absent in the standard streaming setting.
The coordinated streaming setting is strictly harder than the stan-
dard setting. To see this, observe that computing the maximum data
item in the stream is trivial in a standard streaming using O(1)
space and O(1) processing. However computing the maximum
delay requires Ω(N ) space, even without the assumption of loss.
(The proof is a straightforward reduction from Set Disjointness as
in Alon, Matias and Szegedy [3].) Despite this negative result for
the maximum delay, we will show that approximating both average
and standard deviation of delay can be done efﬁciently. In the next
section, we describe the Lossy Difference Aggregator, a mecha-
nism that estimates these statistics.
3. LDA
A Lossy Difference Aggregator (LDA) is a measurement data
structure that supports efﬁciently measuring the average delay and
standard deviation of delay. Both sender and receiver maintain an
LDA; at the end of a measurement period—in our experiments we
consider 1 second—the sender sends its LDA to the receiver and
the receiver computes the desired statistics. The only additional re-
quirements are tight time synchronization between sender and re-
ceiver (which is required by all one-way delay measurement mech-
anisms) and consistent packet ordering at the sender and receiver.
3.1 The data structure
To better explain the LDA, we begin with the simplest average
delay measurement primitive—a pair of counters—and then de-
velop the full LDA as shown in Figure 3.
3.1.1 No loss
To start, consider the problem of (passively) measuring the av-
erage latency between a sender A and a receiver B. A natural
approach is a pair of timestamp accumulators, adding up packet
timestamps on the sender and receiver sides, and a packet counter.
The average delay is then just the difference in timestamp accu-
mulators between sender and receiver, divided by the number of
packets: (TB − TA)/N. Of course, if packets are lost, this ap-
proach fails: The sender’s timestamp accumulator TA will include
the timestamps of the lost packets while the receiver’s will not.
3.1.2 Low loss
Consider the case of exactly one loss. If we randomly split the
trafﬁc into m separate “streams” and compute the average latency
for each such “stream” separately, then a single loss will only make
one of our measurements unusable; we can still estimate the overall
average latency using the remaining measurements.
Practically speaking, we maintain an array of several times-
tamp accumulators and packet counters (collectively called a bank).
Each packet is hashed to one of the m accumulator-counter pairs,
and the corresponding timestamp accumulator and packet counter
are updated as before. By using the same hash function on the
sender and receiver, we can determine exactly how many packets
hashed to each accumulator-counter pair as well as how many of
them were lost. Note that the sum of the receiver’s packet counters
gives us the number of packets received and the sum of the sender’s
packet counters, the number of packets sent; the difference gives
the number of lost packets.
If a packet is lost, the sender’s packet counter at the index of the
lost packet will be one more than the corresponding packet counter
on the receiver. We call such an index unusable and do not use it
in calculating our average delay estimate. The remaining usable
indices give us the average delay for a subset of the packets. With
a single loss, m accumulator-counter pairs are roughly equivalent
to sampling roughly every m − 1 in m packets, providing a very
accurate estimate of the overall average latency. The number of
packets that hashed to a usable index is the effective sample size
of the latency estimate. In other words, it is as if we had sampled
that many packets to arrive at the estimate. In general, for a small
number of losses L, the expected effective sample size is at least a
(1 − L/m) fraction of the received packets.
Example. Figure 2 shows an example conﬁguration with m = 4
and exactly one lost packet that hashed to the second accumulator-
counter pair. The sum of packet delays from the other three usable
accumulator pairs is (180 − 120) + (37 − 15) + (14 − 6) = 90;
the effective sample size is 5 + 2 + 1 = 8. The estimated delay is
thus 90/8 = 11.25.
3.1.3 Known loss rate
For larger loss rates, we need to sample the incoming packets
to reduce the number of potentially unusable rows. Sampling can
258timestamp acc. (cid:1)
packet counter (cid:1)
120
5
234
10
15
2
6
1
180
5
348
9
37
2
14
1
60
    5 ✓
114
10≠9!
don’t match(cid:1)
Unusable:
packet counts
22
     2 ✓
8
     1 ✓
Sender
Receiver
Difference
Figure 2: Computing LDA average delay with one bank of four
timestamp accumulator-counter pairs. Three pairs are usable
(with 5, 2, and 1 packets), while the second is not due to a packet
loss. Thus, the average delay is (60 + 22 + 8)/(5 + 2 + 1).
Packet
Hash
Probabilities
tuned toward
different loss rates
Sampling
stage
p
1
p
2
p
3
Only one bank
updated per 
packet
s
w
o
r
m
Timestamp
accumulator
Packet 
counter
n banks
Figure 3: The Lossy Difference Aggregator (LDA) with n banks
of m rows each.
easily be done in a coordinated fashion at receiver and sender by
(once again) hashing the packet contents to compute a sampling
probability. Thus we ensure that a packet is sampled at the receiver
only if it is sampled at the sender. At sample rate p, we expect
the number of lost packets that are recorded by the LDA to be pL,
so that the expected number of usable rows is at least m − pL. Of
course, packet sampling also reduces the overall number of packets
counted by the LDA, reducing the accuracy of the latency estimate.
In Section 3.3 we will address this issue formally; intuitively, how-
ever, we can see that for p on the order of m/L, we can expect at
least a constant fraction of the accumulator-counter pairs to suffer
no loss and therefore be usable in the latency estimator.
3.1.4 Arbitrary loss rate
So far we have seen that a single bank of timestamp accumulators
and packet counters can be used to measure the average latency
when the loss rate is known a priori. In practice, of course, this is
not the case. To handle a range of loss rates, we can use multiple
LDA banks, each tuned to a different loss rate (Figure 3). (In our
experiments, we found that two banks are a reasonable choice.)
At ﬁrst glance, maintaining multiple banks seems to require
maintaining each bank independently and then choosing the best
bank at the end of the measurement period for computing the es-
timate. However, we can structure a multi-bank LDA so that only
one bank needs to be updated per sampled packet.
The trick is to have disjoint sample sets, so that each packet is
sampled by a single bank, if at all. This way, only a single bank
needs to be updated and later, during post-processing, no packet is
double-counted. Furthermore, as a practical matter, a single row
hash function can be shared by all banks. Each packet is hashed
to a row uniformly and to a bank non-uniformly according to bank
sampling probabilities p1, p2, . . . , pn. For non-uniform sampling
probabilities that are powers of 1/2, this can be implemented by
hashing each packet to an integer uniformly and using the number
of leading zeros to determine the one bank that needs to be up-
dated. We can compute the average delay by combining all useable
elements across all banks. The full m × n LDA is shown in Fig-
ure 3.
Example. Consider two banks having sampling probabilities
p1 = 1/23 and p2 = 1/27. Each packet is hashed to an integer.
If the ﬁrst seven bits are zero, then bank 2 is updated. Otherwise,
if the ﬁrst three bits are zero, then bank 1 is updated. Otherwise, if
the ﬁrst three bits are not all zero, the packet is not sampled.
3.2 Update procedure
Formally, the update procedure is as follows. Let x denote a
packet, h(x) the row hash function, and g(x) the bank sampling
hash function. The row hash function h(x) maps x to a row in-
dex distributed uniformly between 1 and m. The sampling hash
function g(x) maps x to bank j, where g(x) = j with probability
pj. In our analysis we assume that h and g are 4-universal (which
is amenable to efﬁcient implementation), although in practice this
may not be necessary. We use the special value g(x) = 0 to de-
note that the packet is not sampled. Upon processing a packet x
at time τ , timestamp τ is added to the timestamp accumulator at
position (h(x), g(x)), and the corresponding packet counter is in-
cremented. If g(x) = 0, the the packet is simply ignored. Using
T to denote the m × n array of timestamp accumulators and S to
denote corresponding array packet counters, the procedure is:
i ← h(x)
j ← g(x)
if j ≥ 0 then
T [i, j] ← T [i, j] + τ
S[i, j] ← S[i, j] + 1
end if
1.
2.
3.
4.
5.
6.
3.3 Average latency estimator
From the discussion above, estimating the average latency is
straightforward: For each accumulator-counter pair, we check if
the packet counters on the sender and receiver agree. If they do, we
subtract the sender’s timestamp accumulator from the receiver’s. If
they don’t, this accumulator-counter pair is considered unusable.
The average delay is then estimated by the sum of these differences
divided by the number of packets counted.
Formally, let TA[·, ·] and TB[·, ·] denote the m × n timestamp
accumulator arrays of the sender and receiver, respectively, and
SA[·, ·] and SB[·, ·] the corresponding packet counters. Call a posi-
tion (i, j) usable if SA[i, j] = SB[i, j]. Let uij be an indicator for
this event, that is, uij = 1 if (i, j) is usable and uij = 0 otherwise.
Deﬁne
mX
mX
TA =
uij TA[i, j]
and TB =
uij TB[i, j];
i=1
i=1
TA and TB are the sum of the of the useable timestamp accu-
mulators on the sender and receiver, respectively. By deﬁnition
259mX
i=1
uij SB[i, j].
„X
x
uij SA[i, j] = uij SB[i, j], so let
mX
S =
uij SA[i, j] =
i=1
The estimate, then, is
`
TB − TA
´
.
D =
1
S
(4)
«2
X
x
sxax
«2
sx(bx − ax)
sxsx(cid:2) (bx − ax)(bx(cid:2) − ax(cid:2) )
X
x(cid:2)=x(cid:2)
sxbx −
„X
X
X
x,x(cid:2)
x
=
=
=
s
x
2
x(bx − ax)2 +
sxsx(cid:2) (bx − ax)(bx(cid:2) − ax(cid:2) )
The expectation of the cross terms E[sxsx(cid:2) ] is zero, giving us an
unbiased estimator for the square of the delays squared.
So far this implies that we keep a separate signed timestamp ac-
cumulator. Also, to deal with loss we would have to keep an array
of such counters, doubling the number of timestamp accumulators.
Fortunately, we can mine the existing LDA. Observe that the sign
hash sx above can be computed using the low-order bit of the hash
function we use to compute a row index in the full LDA. To achieve
the same effect without adding additional memory, we use this low-
order bit of the row hash value h(x) as the sign bit, “collapsing”
adjacent rows. (Thus the estimator uses 1
2 m× n timestamp accumulator and packet
Deﬁne the collapsed 1
2 m rows.)
counter arrays as:
˜TA(i, j) = TA[2i, j] − TA[2i − 1, j]
˜TB(i, j) = TB[2i, j] − TB[2i − 1, j]
˜SA(i, j) = SA[2i, j] + SA[2i − 1, j]
˜SB(i, j) = SB[2i, j] + SB[2i − 1, j]
P
Let ˜uij be an indicator for a position being usable; that is, ˜uij = 1
if ˜SA(i, j) = ˜SB(i, j), and ˜uij = 0 otherwise. As in the aver-
age latency estimator, let ˜S =
˜uij SA(i, j). Our latency second
frequency moment estimator is
`
˜TB(i, ˜ui) − ˜TA(i, ˜ui)
m/2X
´2
F =
˜uij