the red curve in the top plot, at the cost of an increased miss rate
(bottom plot).
This effect occurs on all ARM platforms tested, and explains
some of the residual bandwidth. The result of ﬂushing the TLB on
each context switch is given in Table 2, where the residual band-
width has dropped from 27 to 25 bits per second. There is clearly
still some interference effect, as the bandwidth is still higher than
the conﬁdence threshold of 13 b/s. We have not yet managed to
identify this further source of contention, although we have ruled
out contention in the branch predictors, which are reset by the full
L1 cache ﬂush between partitions. While it is disappointing that we
have not yet managed to completely close the channel, the empiri-
cal approach ensures that we do not have a false sense of security.
On close inspection of the E6550 matrix of Figure 9, we ﬁnd an-
other small but detectable artefact at half the L2 cache size (32,768
5763
0
1
/
d
e
h
c
u
o
t
s
e
n
i
L
y
l
a
m
o
n
A
y
l
a
m
o
n
A
30.8
30.4
6
0
-6
12
6
0
-6
1/2 cache
1/4 cache
0
10
20
30
40
50
60
Lines evicted /103
Figure 9: E6550 cache channel, cache colouring. N = 4836,
B = 76 b/s, CImax
0 = 42 b/s.
lines). The effect is clearer in the column averages, plotted in red.
This cannot be TLB contention, as the TLB on this chip is not
tagged, and is thus automatically ﬂushed on every context switch.
Rather, it appears to be due to the sender triggering instruction-
cache misses in the kernel. Note that, with the effective cache size
halved by colouring, the observed effect coincides with the point at
which the sender completely dirties the L2 cache. As the caches on
this chip are inclusive, this also evicts kernel code for the sender’s
domain.
The correlation is conﬁrmed by the blue curve, which plots the
anomaly if the sender is given only 1/4 of the cache. As expected,
the artefact moves to 16,384 lines. We eliminate this sharp artefact
with an improved implementation: colouring the kernel stack, and
ﬂushing the L1 caches with coloured data arrays (see Section 5.6
for details on x86 L1-cache ﬂushing).
The results are given in row 7 of Table 2: the observed bandwidth
shifts from 76 to 120 bits per second. Taking into account that the
new implementation has brought the timeslice length into line with
other platforms (halving it from 2ms to 1ms), this represents an im-
provement of 30% (with a 1ms timeslice, the old implementation’s
bandwidth would double to 150 b/s). There is, however, clearly still
a residual channel, as CImax
is only 62.
Overall, we see that while cache colouring remains broadly ef-
fective (if it can be implemented, and all residual channels are care-
fully eliminated), it is getting harder to implement on newer hard-
ware. This is exactly the same trend that we see for IBS: Undoc-
umented behaviour on complex CPUs is essential to both counter-
measures, yet is becoming steadily harder to reverse-engineer, and
sometimes renders implementation seemingly impossible.
5.5 Unexpected Channels
0
While analysing the results of the previous section, we discov-
ered two interesting and unexpected results: First, a way to greatly
increase the signal-to-noise ratio in a contention channel (thus
boosting its usable capacity), and that the cycle counter provides
an entirely new channel due to branch prediction.
Ampliﬁcation by Threshold Effects
)
6
0
1
×
(
s
k
c
i
t
T
P
H
1.8
1.6
1.4
1.2
1
10−1
10−2
10−3
0
1024
2048
3072
4096
Lines evicted
Figure 10: AM3358 cache channel, cache colouring, showing
cycle counter variation. N = 100, B = 2900 b/s.
We instrument the while loop of the measure thread in Fig-
ure 5 to show the number of cycles spent executing the busy loop
and the two background threads, on the AM3358. Given that the
residual channel on this platform is small (5.0 b/s), we expect to see
no strong correlation with the sender’s eviction rate. Yet Figure 10
shows a strong effect, with a measured bandwidth of 2,900 b/s.
The cause is suggested by the fact that the variation is of almost
exactly 532,000 cycles, or one 1 ms preemption period at 532 MHz.
We see a slight variation in where the preemption point falls, lead-
ing to the inner loop either terminating immediately (if the thread
is preempted, and the shared counter C updated, after it is read),
or running for a full timeslice if the preemption falls elsewhere. A
variation of a few cycles (maybe just one) is magniﬁed enormously,
under the sender’s control.
The effect is to eliminate the noise in the receiver’s measurement,
allowing the full capacity to be realised. It is important to note that
the number of discrete input and output symbols (the amount of
underlying variation that the sender controls) still places an upper
bound on channel capacity. This example reinforces the point we
made in Section 2 that adding noise is less effective than limiting
the underlying signal—Figure 10 shows that the noise can be elim-
inated by unexpected means, but the exploitable variation remains.
The Cycle Counter Channel
Some (barely visible) artefacts in Figure 10 point to a previously
unreported channel. Zooming into the ﬁrst few iterations (Fig-
ure 11) shows a weak effect at 7 evictions, where the previously
constant value splits in three, and a stronger one at 10, where it set-
tles on the lowest of the three. Each of these drops is almost exactly
53,200 lines, or 10% of a timeslice.
6
0
1
/
s
k
c
i
t
T
P
H
1.3
1.2
1.1
Branch mispredicts
.
r
e
t
i
/
.
d
e
r
p
s
i
M
1
0
0
10
20
30
40
50
Lines evicted
Figure 11: AM3358 cache channel, cache colouring. Cycle
counter effect of mispredicts. N = 997, B = 1100 b/s.
577The number of branch mispredicts per iteration (red line) pro-
vides an explanation. Assuming that the sender and receiver touch
lines at roughly the same rate (we only recorded the receiver’s rate),
we see roughly one branch misprediction per loop iteration, once
the loop is longer than 10 iterations, and 7 in rare cases.
The precise cause of these mispredicts is unclear (the loop should
be correctly predicted for 9/10 iterations), but the correlation with
the cycle counter is clear, with a small drop beginning at 7 itera-
tions (exaggerated by the log scale for probability), and ﬁnishing at
10. It appears that a branch mispredict leads to the counter miss-
ing a single cycle. Note that the wall-clock time between samples
is unaffected, only the cycle counter’s value varies, giving an ex-
ploitable bandwidth of 1,100 bits per second.
We conclude that the reported value of the cycle counter is im-
precise on ARM, and the imprecision is correlated with branch mis-
predicts. The cycle counter is globally visible, unless virtualised,
thus giving rise to a timing channel not previously reported. This
channel is distinct from traditional ones involving branch predic-
tion, where contention in the branch target buffer (BTB) leads to
variations in runtime [Aciiçmez et al., 2006]. The obvious defence
is virtualising the cycle counter, which is possible on both x86 and
ARM.
5.6 Cost of countermeasures
IBS can be implemented without run-time overhead, it simply
replaces the timer with the PMU as an interrupt source. (It reduces
fairness somewhat, as memory hogs now get longer time slices.)
Cache colouring has two costs: ﬂushing the L1 caches and TLB
on a partition switch, and reducing the effective cache size. The
latter cost depends on the size of the working set of the application
code: it is worst if the working set just ﬁts into the cache, and negli-
gible if the working set is less than half the cache size. The isolation
provided by colouring can also occasionally increase performance
[Tam et al., 2007].
The direct cost of an L1 ﬂush is low on ARM (single instruc-
tion) and expensive on x86 (due to lack of support for a selective
L1 cache ﬂush, requiring the kernel to replace any useful data by
traversing large arrays and jump tables). The indirect cost (of a
cold cache) can be expected to be low: The ﬂush is only needed at
a partition switch, which only occurs at the end of a time slice (of
1 ms or ≈ 1M cycles). The DM3730’s 512-line cache, with a miss
latency of 12 cycles4 takes roughly 6,000 cycles to reﬁll, or 0.6% of
a timeslice, which constitutes an upper bound on the indirect cost.
In most cases this cost will be much smaller, as the L1 cache is
normally cold after a partition switch even without ﬂushing (given
that other process have been executing for at least 1 M cycles).
The TLB ﬂush also has low direct cost, and it is likely to be
cold after a partition switch, resulting in low indirect cost. The full
cost of a TLB reﬁll on the DM3730 is ≈ 64 entries × 50 cycles =
3200 cycles.
6. REMOTE TIMING SIDE-CHANNELS
Both countermeasures evaluated so far address local channels
due to shared hardware. In this case we have some control over
the attacker, either over its resources (as in cache colouring), or
its access to time (as in IBS). Remote channels require a different
approach, as the attacker is effectively unrestrained. In particular, a
remote attacker must be assumed to have an accurate clock.
4http://www.7-cpu.com/cpu/Cortex-A8.html
6.1 OpenSSL vulnerability
We ﬁrst demonstrate the ease with which remote side-channel at-
tacks can be carried out at essentially unlimited distance, and then
present the scheduled delivery countermeasure, which uses a mon-
itor to hide response-time variations.
As a realistic vulnerability, we replicate the strongest form of
the Lucky 13 attack of AlFardan and Paterson [2013]—the dis-
tinguishing attack against Datagram TLS (DTLS) [Modadugu and
Rescorla, 2004], with sequence number checking disabled. By suc-
cessfully addressing this we also address its weaker forms, in par-
ticular plaintext recovery, which use the same mechanism.
The attack uses the fact that TLS ﬁrst calculates the MAC (mes-
sage authentication code, or digest), and then encrypts it. This al-
lows intercepted packets to be submitted to a server, which will
then decrypt and begin to process them before their authenticity
is established. We exploit the non-constant execution time of the
MAC check itself by manipulating the padding in the packet. Ulti-
mately, we construct two packets: M0 and M1, that take a different
length of time to process, before being rejected (the MAC of the
manipulated packet will fail), where the time depends on the (en-
crypted) contents. We distinguish two encrypted packets by inter-
cepting them, and forwarding them to the server.
y
t
i
l
i
b
a
b
o
r
P
0.2
0.1
0
M0 VL
M1 VL
M0 SD
M1 SD
M0 CT
M1 CT
725
750
775
800
825
850
Response time (µs)
Figure 12: Response times for M0 and M1. Shows peaks for
OpenSSL 1.0.1c (VL), scheduled delay (SD) and 1.0.1e (CT),
demonstrating reduced latency. 106 samples, binned at 1µs.
Version
1.0.1c
1.0.1e
1.0.1c-sd
Hops D (km)
0
0
4
12,000
0
0
1
3
4
13
1
1
Vmax ML (b) RTT±σ (ms)
0.73 ± 0.01
1.00
1.2 ± 0.2
0.60
1.30 ± 0.06
0.77
180 ± 30
0.63
0.80 ± 0.005
0.62
0.75 ± 0.005
0.57
0.99
0.11
0.57
0.21
0.07
0.03
Table 3: Vulnerability against network distance (Hops) and
physical distance (D), for DTLS distinguishing attack.
Figure 12 shows the measured response times for the two pack-
ets, as measured from an adjacent machine (no switch). In the ter-
minology of Section 4, the packets M0 and M1 are the two (se-
cret) channel inputs, and the response time is its output. Each pair
of peaks for M0 and M1 (labelled VL, SD and CT respectively)
in Figure 12 forms a channel matrix with just two columns (M0
and M1), built by taking 106 observations for each input.
The leftmost (VL) peaks are the response times for the vulnera-
ble implementation of OpenSSL 1.0.1c, on the AM3358. Times are
measured, as in the original attack, by sending a modiﬁed packet
immediately followed by a valid packet (also captured from the
wire), and taking the response time of the second. This avoids the
problem that DTLS does not acknowledge invalid packets. The
victim executes an echo server, over TLS. As line 1 of Table 3
578shows, these peaks are trivially distinguishable, allowing the at-
tacker to correctly guess which packet was sent with near certainty
(Vmax = 100%). This is a leak of 0.99 b of min-entropy.
The rightmost (CT) peaks give the round-trip for the constant-
time implementation of OpenSSL 1.0.1e, which substantially re-
duces the vulnerability—the curves are almost identical. However,
they still differ measurably, as row 5 of Table 3 shows—the two
can still be distinguished with Vmax = 62% probability, while to
be completely secure, we should only be able to guess with 50%
probability. This emphasises the difﬁculty of producing portable
cross-platform constant-time code, and indicates that the produc-
tion version of OpenSSL (as of writing) is still vulnerable.
y
t