frequency-detennining stage as indicated in earlier work [24].
A delay of 15°A> greater than the nominal value (conservative
guardband) was chosen to be the threshold beyond which is(cid:173)
sue queue entries require an extra clock cycle to complete op(cid:173)
erations. The maximum time required for an operation even in
the worst case is within two cycles. Our results indicate the ap(cid:173)
proximately 40% of the entries require the two cycles to com(cid:173)
plete operation. In our work, this translates to 10 ofthe 24 issue
queue entries.
5. The effects of PV on issue queue operation
The different pipeline activities that occur every cycle with
respect to the issue queue are as follows:
• Dispatch Writes: On a dispatch, new instructions are
written into their entries. There they wait until they are se(cid:173)
lected for execution. Dispatching instructions into the is(cid:173)
sue queue entails CAM writes and SRAM writes.
• Forwarding
they broadcast
ComparisonlWrite: When
instruc(cid:173)
their results
tions complete execution,
and tags to the issue queue. The dependent
instruc(cid:173)
tions use the tag CAM-match logic to identify if their
source operands are available in the forwarding paths.
Whenever these forwarding comparisons become suc(cid:173)
cessful, the operands get written into their entries result(cid:173)
ing in a SRAM write.
• Issue Reads: When an instruction gets selected for execu(cid:173)
tion, the opcode and operands of the instruction get read
(SRAM read operation) from the entry and the instruction
is sent for execution.
Each of these activities is important and there is significant
interplay between them as well. For example, a slow dispatch
write could cause forwarding operations to slow down since the
tags do not get written into the entries in time. Similarly if the
issue read out of an entry is slow, the destination tags will be
read slowly, thus affecting the forwarding paths and any later
dependent instruction in the issue queue. To understand the per(cid:173)
fonnance impact, we slowed down each of the components and
studied the subsequent effect on perfonnance.
1-4244-2398-9/08/$20.00 ©2008 IEEE
13
DSN 2008: Soundararajan et at.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
0.25
0.2
~ 0.2
en
c
CI,)o 0.15 +------------------------F--------.--r-- .-.---- --
~
:E 0.1 + - - - - - - - - - - - - - ------1--.-.----- ..
ca.coG. 0.05 -+------------------------------F------------------------------------- ---\-------------
--- ---- .... -
...
~
'iii 0.15
c
CI,)o
~ 0.1 +--------- --- -- --- -1--- .... ------------------4------.-----
:cca.cE 0.05 +-----------------~--------
0..
0.2l- ~ ~ 10--------------
I
One cycle
operation
I
I
~
.~ 0.15 i--
o
~ 0.1 .,----------------------____+_ --------------I-~------------.---
CI,)
:cca.cE 0.05 +-------------------F----- ------- --
0..
-J--------\-----
o +--,........-,--.......-;:::...~~-----,-------.----- - -·-----T ---~----.---::.---
o +- -~-------- ---,---,----,---,
o +---""";==----.-----,---,-- --T----'--,-----,--l~--,---·---T--___,---r----T----r-="'- 1 I
0.20
0.39
0.58
0.76
0.95
1.14
Normalized Delay
1.33
1.52
0.08
0.37
0.65
1.22
0.94
1.50
Normalized Delay
1.79
0.54
0.66
0.79
0.91
1.03
1.16
Normalized Delay
1.28
1.41
(A)
SRAMwrite
(B)
queue VIJ\;J.O,uu'J.J.
(C)
Figure 2. Delay variation in issue queue. (C) shows that 60% of entries operate in 1 cyle while 400/0 of entries require 2 cycles.
Figure 3 shows the results of this experiment. We show each
of the activities in descending order ofperformance impact and
compare it against the issue queue performance when there is
no proc~ss variation - NonPV. In certain cases, slow dispatch
brings down performance significantly because more instruc(cid:173)
tions are needed to exploit the ILP (mesa, bzip2) while in others
the issue queue holds enough instructions to meet IPC require(cid:173)
ments and hence slowing down forwarding and issue brings
down performance (gzip, sixtrack). Figure 3 shows that, across
all benchmarks, each of these activities playa vital role in issue
queue operation. Hence our variation-tolerant solutions have
addressed them together rather than concentrating on one spe(cid:173)
cific activity. Note that slowing down all the activities together,
Slow Dw/lr/Fw, has the biggest performance impact, as much
as 20%. Our solutions in section 6.2 is motivated based on this
observation.
• Slow Dw/lr/Fw (1.18, 1.61) ~
.Slow Dispatch Writes (1.27,1.74)
I DSlow Forwarding (1.27, 1.76)
D Slow Issue Reads (1.28, 1.77)
• NonPV (1.43, 2.01)
J
i
-------------_.~-------------_._----~
3 ----~--~---------
1.5
Figure 3. Performance Impact of slowing down different activi(cid:173)
ties that occur in the Issue Queue. Legend shows the average IPC
across all benchmarks and the high IPC ones.
Variation Analysis in Collapsible Issue Queues: Besides the
above-mentioned activities, collapsible issue queues also have
additional movement of instructions between neighboring en-
tries. Variations affecting the data movement logic has a sig(cid:173)
nificant impact on performance. This is because there is lin(cid:173)
ear dependency in instruction movement between entries. Vari(cid:173)
ations causing slow movement between any 2 entries would
affect all entries after them (slowing them as well), bringing
down performance. Since variations is a non-deterministic phe(cid:173)
nomenon, it is impossible to design the instruction movement
logic to minimize impact ofvariations. Further, [13] shows that
the power consumption of a collapsible issue queue is consid(cid:173)
erably higher than a non-collapsible queue; thus, the latter is
more preferable in modem power-aware designs. Hence, we
limit our investigation to the impact ofprocess variation in non(cid:173)
collapsible queues.
The select logic also plays an important role in issue queue
operation. In non-collapsing issue queue, the select logic is a
linear chain of multiplexers [13]. Hence it is less affected by
variations compared to the issue queue itself [10]. Also a slow
select logic operation affects only the issue reads. Hence it can
be modeled as issue read variations. But given the importance
of select logic, it warrants further investigation and it is very
much part of our future research.
6. Battling PV artifacts within the issue queue
Our solutions to handle issue-queue variations progres(cid:173)
sively move from tackling variations at per-entry level
to
sub-component analysis. Initially we look at steering schemes
that dispatch instructions to entries based on operating-speeds.
Optimizing on these schemes, our intra-entry variation anal(cid:173)
ysis looks at switching operands and ports to further miti(cid:173)
gate the impact of variations. Towards the end of the sec(cid:173)
tion, we also look at how existing solutions, proposed to
handle other phenomena, can be tuned for handling varia(cid:173)
tions.
6.1. VariSteer: PV-aware instruction steering
Naive PV-unaware issue queue allocation reduces over(cid:173)
all performance by about 20%. Since variation is non(cid:173)
deterministic, entries could either be fast or slow. In this
section, we look at steering schemes that optimize entry allo(cid:173)
cation based on operating speeds of the entries to minimize
stalls reducing the performance degradation to 5%. The al-
1-4244-2398-9/08/$20.00 ©2008 IEEE
14
DSN 2008: Soundararajan et al.
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
loc logic maintains the information regarding the slow entries
and steers instructions accordingly.
is required not in the current cycle but the next one. Figure 13
shows this in greater detail. Given that SOT is a small struc(cid:173)
ture (small access time as well) and its access is not on a crit(cid:173)
ical path, there are no timing issues due to variations affecting
it.
I
Alloc
Slow Entry Bit
Source Tags
Stall Optimization
Table
Figure 4. Microarchitectural changes required to implement Op(cid:173)
tiSteer.
6.1.1. SpeedSteer: A Speed-aware
steering mecha(cid:173)
nism Dependent instructions within the issue queue can lie
in faster or slower entries. If the instructions lie in slower en(cid:173)
tries, forwarded results need to be held for an additional cy(cid:173)
cle in order for slower entries to pick up the data. Since the
pipeline does not know if the dependent instructions are ly(cid:173)
ing in faster or slower entries, forwarding can be done every
other cycle only bringing down performance greatly.
Our proposed SpeedSteer scheme reduces this by maintain(cid:173)
ing any dependency chain within faster or slower entries. By
doing this, the instructions implicitly know that their depen(cid:173)
dent instructions also move to similar entry. Hence only in(cid:173)
structions in slow entries stall the forwarding paths. When the
source operands lie in both type of entries, instructions can be
dispatched to faster entries only. In such cases, the forwarding
paths might be stalled unwantedly by the instruction in slow en(cid:173)
try but this was minimal since the instruction could have other
dependents in slow entries.
To identify whether the instruction issued from a fast or slow
entry, the instruction carries a bit after issuing. This bit can be
added to the entries and set on dispatch - negligible change to
variation effects due to adding the bit (size of an entry is 160
bits). Alternatively, the select logic can set this bit while issu(cid:173)
ing instructions, based on the entries from which they were is(cid:173)
sued.
6.1.2. OptiSteer: An optimized table-based steering mech(cid:173)
anism Simple priority allocation of instructions to fast entries
(slow entries used only when fast entries are not available) is
not possible with SpeedSteer. This is because there is no mech(cid:173)
anism to stall forwarding of instructions that issued from fast
entries for an additional cycle when their dependents lie in slow
entries. To enable this, OptiSteer uses a table called the Stall
Optimization Table (SOT) whose entries are set, when slow de(cid:173)
pendents are in the issue queue, to stall forwarding.
The SOT has 128 I-bit entries, corresponding to the desti(cid:173)
nation tags (ROB id) of each instruction. An entry is set if in(cid:173)
struction with corresponding source operand tag is in a slow is(cid:173)
sue queue entry. When an instruction completes execution, it
accesses the table to check if forwarding needs to be stalled
for next cycle. Figure 4 shows how the SOT stalls the forward(cid:173)
ing path based on the source operand tags of instructions. Note
that the SOT access is not on the critical path since the value
Figure 5 shows the performance benefits in employing the
two instruction steering schemes. The conventional scheme
shown as PV-unAware is oblivious to variation-affected entries
and suffers an overall performance loss of 20.5%. For high
IPC benchmarks, this value goes upto 24%. The SpeedSteer
scheme in tum exhibits 8.8% (9.2% for high IPC benchmarks)
overall performance loss, while the OptiSteer scheme imposes
only 5.1 % (5.5% for high IPC benchmarks) performance over(cid:173)
head. In certain cases like bzip2, sixtrack and mesa, the PV(cid:173)
unAware scheme leads to 34% performance degradation while
the steering schemes have only about 10% performance loss in
those cases. The steering schemes do a better job of utilizing
the faster entries compared to PV-unAware scheme. Both the
steering schemes place about 80% of instructions in fast en(cid:173)
tries compared to PV-unAware scheme that places only 57% of
instructions in the fast entries.
• NonPV (1.43, 2.01) 0
_.__.._ -.-_.__ -.._.__ _--~_._ .._.._.__....•......__ .
.
_.•....__ _--.-.-_..-.-
-
__ _-_ _-.-_.__._.--._._._.._ ~~ - _.._-_
I
~ .PV-unAware (1.14,1.54)--
.-~---------------~----------------..t1/-------1 0 SpeedSteer (1.31, 1.83)
2.5 -------.JII------·-----------.m--~~----.m~--------------------
o OptiSteer (1.36, 1.90)
1.5
Figure 5. Performance obtained by employing the instruction
steering schemes. Legend shows the average IPC across all and high
IPC benchmarks clearly showing the requirement for the steering
schemes to reduce performance loss due to variations.
6.2.
Intra-Entry variations
Each entry of the issue queue consists of multiple sub(cid:173)
components that differ in their functionality (a tag CAM sub(cid:173)
component and operand SRAM sub-component). Naturally,
random variations could cause variability in their behavior.
Based on this fact and the observation that all sub-components
of an entry need not be useful for every instruction, we provide
techniques to avoid variation-affected sub-components when(cid:173)
ever possible. Random variability also means that performance
of sub-components do not get adversely affected with respect
to all the ports used to read or write into the entries. Since
the maximum bandwidth is not always required, ports can be
switched whenever possible.
1-4244-2398-9/08/$20.00 ©2008 I~EE
15
DSN 2008: Soundararajan et al.
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
Exploiting varying port-speeds would allow entries to have
fast dispatch writes while their issue reads could be slow. By
identifying this, unwanted stalls could be reduced. The conven(cid:173)
tional issue queue design allows these operations to be of vary(cid:173)
ing speeds and yet interface with each other. Figure 1(B) show(cid:173)
ing the time-line for conventional issue queue operation which
shows that each ofthe activities when complete sets a flag upon
which the next activity begins. CAM cells ofnew entries are ac(cid:173)