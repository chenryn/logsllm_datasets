(cid:17)
𝒉∼𝐹(cid:16)P′
𝜆
(cid:169)(cid:173)(cid:173)(cid:171) 𝐹 (PC) (𝒉)
(cid:170)(cid:174)(cid:174)(cid:172)
𝐹(cid:16)P′
(cid:17) (𝒉)
(cid:32)M(D(𝑛)
(cid:32)M(D(𝑛)
(cid:33)𝜆
 .
(cid:33)𝜆
(cid:33)𝜆
(cid:32)M(D(𝑛)
(cid:1). This completes the proof of
𝑚+1)(𝒉)
M(D′(𝑛)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
𝒉∼M(D′(𝑛)
𝑚+1)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
M(D′(𝑛)
M(D′(𝑛)
𝑚
(c)
=
𝑚=0
𝑞𝑚 (1 − 𝑞)𝑛−𝑚−1 E
E
= E𝑚∼Bin(𝑛−1,𝑞)
subsets of [𝑛 − 1] is equal to(cid:0)𝑛−1
𝒉∼M(D′(𝑛)
𝑚+1)
𝑚
Theorem 3.6.
The inequality (a) is the same as (24), just writing it differently. In
(b) we used (26) and in (c) we used the fact that number of 𝑚-sized
6 PROOF OF RDP FOR THE SPECIAL FORM
Fix an arbitrary 𝑚 ∈ N and consider any pair of neighboring
datasets (D𝑚, D′
𝑚) ∈ D𝑚same. Let D𝑚 = (𝑑, . . . , 𝑑) ∈ X𝑚 and
1, . . . , 𝑝′
D′
𝑚 = (𝑑, . . . , 𝑑, 𝑑′) ∈ X𝑚. Let 𝒑 = (𝑝1, . . . , 𝑝𝐵) and 𝒑′ = (𝑝′
𝐵)
be the probability distributions of the discrete 𝜖0-LDP mechanism
R : X → Y = [𝐵] when its inputs are 𝑑 and 𝑑′, respectively, where
𝑝 𝑗 = Pr[R(𝑑) = 𝑗] and 𝑝′
𝑗 = Pr[R(𝑑′) = 𝑗] for all 𝑗 ∈ [𝐵]. Since R
is 𝜖0-LDP, we have
𝑒−𝜖0 ≤ 𝑝 𝑗
𝑝′
𝑗
≤ 𝑒𝜖0,
∀𝑗 ∈ [𝐵].
(27)
Since M is a shuffle mechanism, it induces a distribution on A𝑚
𝐵 for
𝐵 ,M(D𝑚)(𝒉) andM(D′
𝑚)(𝒉)
any input dataset. So, for any 𝒉 ∈ A𝑚
are equal to the probabilities of seeing 𝒉 when the inputs to M
are D𝑚 and D′
𝑚, respectively. Thus, for a given histogram 𝒉 =
(ℎ1, . . . , ℎ𝐵) ∈ A𝑚
𝐵 with 𝑚 elements and 𝐵 bins, we have
M(D𝑚) (𝒉) = MN (𝑚, 𝒑, 𝒉) =
𝑝ℎ 𝑗
𝑗
,
(28)
(cid:18)𝑚
(cid:19) 𝐵
𝒉
𝑗=1
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2331𝒉
𝑚!
where MN (𝑚, 𝒑, 𝒉) denotes the Multinomial distribution with(cid:0)𝑚
(cid:1) =
𝐵
𝑗=1
(29)
𝑝′
𝑗 MN
M(D′
𝐵∑︁
𝑚)(𝒉) =
. Thus, M(D′
𝑚)(𝒉) for any 𝒉 ∈ A𝑚
ℎ1!···ℎ𝐵!. Note that (28) can be obtained as a special case of the gen-
eral distribution in (17) by putting 𝒑 𝑗 = 𝒑 for each client 𝑗.
For M(D′
𝑚), note that the last client (independent of the other
clients) maps its input data point 𝑑′ to the 𝑗’th bin with probability
𝑝′
𝑗, and the remaining (𝑚−1) clients’ mappings induce a distribution
on A𝑚−1
𝐵 can be written as
itly assume that if ℎ 𝑗 = 0 for some 𝑗 ∈ [𝐵], then MN
=
0 as one of the elements is negative. Note that similar to (28), (29)
can also be obtained from (17) as a special case. Using the polyno-
M(D′
M(D𝑚)(𝒉) − 1 in
(cid:17) ,
(cid:16)𝑚 − 1, 𝒑,(cid:101)𝒉𝑗
where(cid:101)𝒉𝑗 =(cid:0)ℎ1, . . . , ℎ 𝑗−1, ℎ 𝑗 − 1, ℎ 𝑗+1, . . . , ℎ𝐵(cid:1) ∈ A𝑚−1
(cid:16)𝑚 − 1, 𝒑,(cid:101)𝒉𝑗
𝑖=0(cid:0)𝑛
mial expansion (1 + 𝑥)𝑛 =𝑛
(cid:34)(cid:18)M(D′
(cid:19)𝜆(cid:35)
(cid:18)𝜆
(cid:19)
𝜆∑︁
(cid:1)𝑥𝑖 (with 𝑥 =
(cid:34)(cid:18)M(D′
M(D𝑚)(𝒉) − 1
𝑚(cid:16) M(D′
M(D𝑚)(𝒉) − 1(cid:17). Substituting this in (30) gives:
𝐵 → R be a random variable associated with the dis-
Let 𝑋 : A𝑚
tribution M(D𝑚) on A𝑚
𝐵 , define 𝑋(𝒉) :=
𝑚)(𝒉)
𝐵 , and for any 𝒉 ∈ A𝑚
𝑚)(𝒉)
M(D𝑚)(𝒉)
the following), we have:
E𝒉∼M(D𝑚)
E𝒉∼M(D𝑚)
. We implic-
(cid:19)𝑖(cid:35)
𝑚)(𝒉)
𝑚)(𝒉)
(30)
(cid:17)
𝑖=0
=
𝐵
𝑖
.
𝑖
(cid:34)(cid:18)M(D′
𝑚)(𝒉)
M(D𝑚)(𝒉)
(cid:19)𝜆(cid:35)
= 1 + 𝜆∑︁
𝑖
𝑖=1
(cid:18)𝜆
(cid:19) E𝒉∼M(D𝑚)(cid:2)(𝑋(𝒉))𝑖(cid:3)
.
𝑚𝑖
(31)
The RHS of (31) is in terms of the moments of 𝑋, which we bound
in the following lemma. Before that, first we simplify the expression
for 𝑋(𝒉) by computing the ratio M(D′
𝑚)(𝒉)
𝑝′
𝑚)(𝒉)
M(D′
M(D𝑚)(𝒉) =
Thus, we get 𝑋(𝒉) = 𝑚(cid:16) M(D′
𝑗=1
𝑗
𝐵∑︁
M(D𝑚)(𝒉) for any 𝒉 ∈ A𝑚
𝐵 :
(cid:19)
𝐵∑︁
MN (𝑚 − 1, 𝒑,(cid:101)𝒉𝑗)
(cid:18)𝐵
M(D𝑚)(𝒉) − 1(cid:17)
MN (𝑚, 𝒑, 𝒉)
𝑚)(𝒉)
𝑝′
𝑗
𝑝 𝑗
− 𝑚.
ℎ 𝑗
𝑚
𝑝′
𝑗
𝑝 𝑗
𝑗=1
𝑗=1
ℎ 𝑗
=
=
.
E𝒉∼M(D𝑚)
(32)
Remark 8. As mentioned in Remark 5, we could tighten our upper
bounds for specific mechanisms. As shown in (31) above, the Rényi
divergence of a mechanism between two neighboring datasets can
be written in terms of the moments of a r.v. 𝑋, which is defined as the
ratio of distributions of the mechanism on these two neighboring
datasets. However, since our goal is to bound RDP for all 𝜖0-LDP
mechanisms, we prove the worse-case bound on the moments of 𝑋
that holds for all mechanisms; see (34) in Lemma 6.1 for bound on
the 𝑖 ≥ 3’rd moments of 𝑋 and (38) in Lemma 6.2 for bound on the
variance of 𝑋.
Lemma 6.1. The random variable 𝑋 has the following properties:
(1) 𝑋 has zero mean, i.e., E𝒉∼M(D𝑚) [𝑋(𝒉)] = 0.
(33)
A proof of Lemma 6.1 is presented in Appendix C.1. Substituting
,
(34)
0 𝑥𝑧−1𝑒−𝑥𝑑𝑥 is the
𝑝′2
𝑗
𝑝 𝑗
− 1(cid:170)(cid:174)(cid:172)
(35)
𝑗=1
(2) The variance of 𝑋 is equal to
(3) For 𝑖 ≥ 3, the 𝑖’th moment of 𝑋 is bounded by
𝑝′2
𝑗
𝑝 𝑗
E𝒉∼M(D𝑚)(cid:2)𝑋(𝒉)2(cid:3) = 𝑚(cid:169)(cid:173)(cid:171) 𝐵∑︁
− 1(cid:170)(cid:174)(cid:172) .
E𝒉∼M(D𝑚)(cid:2)(𝑋(𝒉))𝑖(cid:3) ≤ 𝑖Γ (𝑖/2)(cid:16)2𝑚𝜈2(cid:17)𝑖/2
and Γ (𝑧) = ∫ ∞
(cid:19)𝜆(cid:35)
(cid:19) 1
(cid:18)𝜆
(cid:169)(cid:173)(cid:171) 𝐵∑︁
(cid:19)𝑖/2
(cid:18) (𝑒𝜖0 − 𝑒−𝜖0)2
M (D𝑚) (𝒉)
where 𝜈2 = (𝑒𝜖0−𝑒−𝜖0)2
Gamma function.
≤ 1 +
𝑗=1
𝑚
2
4
E𝒉∼M(D𝑚)
the bounds from Lemma 6.1 into (31), we get
(cid:34)(cid:18)M(cid:0)D′
𝑚(cid:1) (𝒉)
(cid:18)𝜆
(cid:19)
+ 𝜆∑︁
(cid:19)
𝑗=1
2𝑚
(cid:18)𝐵
𝑖Γ (𝑖/2)
𝑖
𝑖=3
1, . . . , 𝑝′
Note that 𝑝1, . . . , 𝑝𝑚, 𝑝′
𝑚 are defined for the fixed pair of
datasets (D𝑚, D′
𝑚) ∈ D𝑚same that we started with. So, the term con-
𝑝′2
in the RHS of (35) depends on (D𝑚, D′
− 1
𝑚),
taining
𝑗
𝑝 𝑗
and that is the only term in (35) that depends on (D𝑚, D′
𝑚). Since
Theorem 3.7 requires us to bound (35) for any pair of neighboring