Important file.jpeg
Foo
Bar
Subdir1
A.txt
B.txt
Important file.doc
Important file.jpeg
Foo
Dir
File
Bar
Subdir1
A.txt
B.txt
Important file.doc
Important file.jpeg
Bar
Subdir1
A.txt
B.txt
Important file.doc
Important file.jpeg
A.txt
B.txt
FIGURE 11-90 Comparison between the zap and salvage operations.
The ReFS file system, after salvage completes, tries to rebuild missing information using various 
best-effort techniques; for example, it can recover missing file IDs by reading the information from 
other buckets (thanks to the collating rule that separates files’ IDs and tables). Furthermore, ReFS also 
augments the Minstore object table with a little bit of extra information to expedite repair. Although 
ReFS has these best-effort heuristics, it’s important to understand that ReFS primarily relies on the re-
dundancy provided by metadata and the storage stack in order to repair corruption without data loss.
In the very rare cases in which critical metadata is corrupted, ReFS can mount the volume in read-
only mode, but not for any corrupted tables. For example, in case that the container table and all of its 
duplicates would all be corrupted, the volume wouldn’t be mountable in read-only mode. By skipping 
over these tables, the file system can simply ignore the usage of such global tables (like the allocator, 
for example), while still maintaining a chance for the user to recover her data.
Finally, ReFS also supports file integrity streams, where a checksum is used to guarantee the integrity 
of a file’s data (and not only of the file system’s metadata). For integrity streams, ReFS stores the checksum 
of each run that composes the file’s extent table (the checksum is stored in the data section of an extent 
table’s row). The checksum allows ReFS to validate the integrity of the data before accessing it. Before 
returning any data that has integrity streams enabled, ReFS will first calculate its checksum and compares 
it to the checksum contained in the file metadata. If the checksums don’t match, then the data is corrupt.
The ReFS file system exposes the FSCTL_SCRUB_DATA control code, which is used by the scrubber
(also known as the data integrity scanner). The data integrity scanner is implemented in the Discan.dll 
library and is exposed as a task scheduler task, which executes at system startup and every week. When 
the scrubber sends the FSCTL to the ReFS driver, the latter starts an integrity check of the entire volume: 
the ReFS driver checks the boot section, each global B+ tree, and file system’s metadata. 
CHAPTER 11
Caching and file systems
761
Note The online Salvage operation, described in this section, is different from its offline 
counterpart. The refsutil.exe tool, which is included in Windows, supports this operation. 
The tool is used when the volume is so corrupted that it is not even mountable in read-only 
mode (a rare condition). The offline Salvage operation navigates through all the volume 
clusters, looking for what appears to be metadata pages, and uses best-effort techniques 
to assemble them back together.
Leak detection
A cluster leak describes the situation in which a cluster is marked as allocated, but there are no refer-
ences to it. In ReFS, cluster leaks can happen for different reasons. When a corruption is detected on 
a directory, online salvage is able to isolate the corruption and rebuild the tree, eventually losing only 
some files that were located in the root directory itself. A system crash before the tree update algo-
rithm has written a Minstore transaction to disk can lead to a file name getting lost. In this case, the 
file’s data is correctly written to disk, but ReFS has no metadata that point to it. The B+ tree table repre-
senting the file itself can still exist somewhere in the disk, but its embedded table is no longer linked in 
any directory B+ tree.
The built-in refsutil.exe tool available in Windows supports the Leak Detection operation, which can 
scan the entire volume and, using Minstore, navigate through the entire volume namespace. It then 
builds a list of every B+ tree found in the namespace (every tree is identified by a well-known data 
structure that contains an identification header), and, by querying the Minstore allocators, compares 
the list of each identified tree with the list of trees that have been marked valid by the allocator. If it 
finds a discrepancy, the leak detection tool notifies the ReFS file system driver, which will mark the clus-
ters allocated for the found leaked tree as freed.
Another kind of leak that can happen on the volume affects the block reference counter table, such 
as when a cluster’s range located in one of its rows has a higher reference counter number than the 
actual files that reference it. The lower-case tool is able to count the correct number of references and 
fix the problem.
To correctly identify and fix leaks, the leak detection tool must operate on an offline volume, but, 
using a similar technique to NTFS’ online scan, it can operate on a read-only snapshot of the target 
volume, which is provided by the Volume Shadow Copy service. 
762
CHAPTER 11
Caching and file systems
EXPERIMENT: Use Refsutil to find and fix leaks on a ReFS volume
In this experiment, you use the built-in refsutil.exe tool on a ReFS volume to find and fix cluster 
leaks that could happen on a ReFS volume. By default, the tool doesn’t require a volume to be 
unmounted because it operates on a read-only volume snapshot. To let the tool fix the found 
leaks, you can override the setting by using the /x command-line argument. Open an adminis-
trative command prompt and type the following command. (In the example, a 1 TB ReFS volume 
was mounted as the E: drive. The /v switch enables the tool’s verbose output.)
C:\>refsutil leak /v e: 
Creating volume snapshot on drive \\?\Volume{92aa4440-51de-4566-8c00-bc73e0671b92}... 
Creating the scratch file... 
Beginning volume scan... This may take a while... 
Begin leak verification pass 1 (Cluster leaks)... 
End leak verification pass 1. Found 0 leaked clusters on the volume. 
Begin leak verification pass 2 (Reference count leaks)... 
End leak verification pass 2. Found 0 leaked references on the volume. 
Begin leak verification pass 3 (Compacted cluster leaks)... 
End leak verification pass 3. 
Begin leak verification pass 4 (Remaining cluster leaks)... 
End leak verification pass 4. Fixed 0 leaks during this pass. 
Finished.
Found leaked clusters: 0 
Found reference leaks: 0 
Total cluster fixed  : 0
Shingled magnetic recording (SMR) volumes 
At the time of this writing, one of the biggest problems that classical rotating hard disks are facing is 
in regard to the physical limitations inherent to the recording process. To increase disk size, the drive 
platter area density must always increase, while, to be able to read and write tiny units of information, 
the physical size of the heads of the spinning drives continue to get increasingly smaller. In turn, this 
causes the energy barrier for bit flips to decrease, which means that ambient thermal energy is more 
likely to accidentally flip flip bits, reducing data integrity. Solid state drives (SSD) have spread to a lot of 
consumer systems, large storage servers require more space and at a lower cost, which rotational drives 
still provide. Multiple solutions have been designed to overcome the rotating hard-disk problem. The 
most effective is called shingled magnetic recording (SMR), which is shown in Figure 11-91. Unlike PMR 
(perpendicular magnetic recording), which uses a parallel track layout, the head used for reading the 
data in SMR disks is smaller than the one used for writing. The larger writer means it can more effec-
tively magnetize (write) the media without having to compromise readability or stability.
EXPERIMENT: Use Refsutil to find and fix leaks on a ReFS volume
In this experiment, you use the built-in refsutil.exe tool on a ReFS volume to find and fix cluster 
leaks that could happen on a ReFS volume. By default, the tool doesn’t require a volume to be 
unmounted because it operates on a read-only volume snapshot. To let the tool fix the found 
leaks, you can override the setting by using the /x command-line argument. Open an adminis-
trative command prompt and type the following command. (In the example, a 1 TB ReFS volume 
was mounted as the E: drive. The /v switch enables the tool’s verbose output.)
C:\>refsutil leak /v e:
Creating volume snapshot on drive \\?\Volume{92aa4440-51de-4566-8c00-bc73e0671b92}...
Creating the scratch file...
Beginning volume scan... This may take a while...
Begin leak verification pass 1 (Cluster leaks)...
End leak verification pass 1. Found 0 leaked clusters on the volume.
Begin leak verification pass 2 (Reference count leaks)...
End leak verification pass 2. Found 0 leaked references on the volume.
Begin leak verification pass 3 (Compacted cluster leaks)...
End leak verification pass 3.
Begin leak verification pass 4 (Remaining cluster leaks)...
End leak verification pass 4. Fixed 0 leaks during this pass.
Finished.
Found leaked clusters: 0
Found reference leaks: 0
Total cluster fixed  : 0
CHAPTER 11
Caching and file systems
763
Reader
Writer
Track N
Track N + 1
Track N + 2
Track N + 3
FIGURE 11-91 In SMR disks, the writer track is larger than the reader track.
The new configuration leads to some logical problems. It is almost impossible to write to a disk track 
without partially replacing the data on the consecutive track. To solve this problem, SMR disks split the 
drive into zones, which are technically called bands. There are two main kinds of zones: 
I 
Conventional (or fast) zones work like traditional PMR disks, in which random writes are allowed.
I 
Write pointer zones are bands that have their own “write pointer” and require strictly sequen-
tial writes. (This is not exactly true, as host-aware SMR disks also support a concept of write
preferred zones, in which random writes are still supported. This kind of zone isn’t used by
ReFS though.)
Each band in an SMR disk is usually 256 MB and works as a basic unit of I/O. This means that the sys-
tem can write in one band without interfering with the next band. There are three types of SMR disks:
I 
Drive-managed: The drive appears to the host identical to a nonshingled drive. The host
does not need to follow any special protocol, as all handling of data and the existence of the
disk zones and sequential write constraints is managed by the device’s firmware. This type of
SMR disk is great for compatibility but has some limitations–the disk cache used to transform
random writes in sequential ones is limited, band cleaning is complex, and sequential write
detection is not trivial. These limitations hamper performance.
I 
Host-managed: The device requires strict adherence to special I/O rules by the host. The host
is required to write sequentially as to not destroy existing data. The drive refuses to execute
commands that violate this assumption. Host-managed drives support only sequential write
zones and conventional zones, where the latter could be any media including non-SMR, drive-
managed SMR, and flash.
I 
Host-aware: A combination of drive-managed and host-managed, the drive can manage the
shingled nature of the storage and will execute any command the host gives it, regardless of
whether it’s sequential. However, the host is aware that the drive is shingled and is able to query
the drive for getting SMR zone information. This allows the host to optimize writes for the
shingled nature while also allowing the drive to be flexible and backward-compatible. Host-
aware drives support the concept of sequential write preferred zones.
At the time of this writing, ReFS is the only file system that can support host-managed SMR disks 
natively. The strategy used by ReFS for supporting these kinds of drives, which can achieve very large 
capacities (20 terabytes or more), is the same as the one used for tiered volumes, usually generated by 
Storage Spaces (see the final section for more information about Storage Spaces).
764
CHAPTER 11
Caching and file systems
ReFS support for tiered volumes and SMR
Tiered volumes are similar to host-aware SMR disks. They’re composed of a fast, random access area 
(usually provided by a SSD) and a slower sequential write area. This isn’t a requirement, though; tiered 
disks can be composed by different random-access disks, even of the same speed. ReFS is able to 
properly manage tiered volumes (and SMR disks) by providing a new logical indirect layer between files 
and directory namespace on the top of the volume namespace. This new layer divides the volume into 
logical containers, which do not overlap (so a given cluster is present in only one container at time). A 
container represents an area in the volume and all containers on a volume are always of the same size, 
which is defined based on the type of the underlying disk: 64 MB for standard tiered disks and 256 MB 
for SMR disks. Containers are called ReFS bands because if they’re used with SMR disks, the containers’ 
size becomes exactly the same as the SMR bands’ size, and each container maps one-to-one to each 
SMR band.
The indirection layer is configured and provided by the global container table, as shown in Figure 11-92. 
The rows of this table are composed by keys that store the ID and the type of the container. Based on 
the type of container (which could also be a compacted or compressed container), the row’s data is 
different. For noncompacted containers (details about ReFS compaction are available in the next sec-
tion), the row’s data is a data structure that contains the mapping of the cluster range addressed by the 
container. This provides to ReFS a virtual LCN-to-real LCN namespace mapping.
File’s extent table
B+ tree
Bands divided
into clusters
{ID: 194 Type: Base }
{ID: 195 Type: Base }
{ID: 196 Type: Base }
{ID: 197 Type: Base }
RLCN 0x12E400
RLCN 0x12E800
RLCN 0x12F000
RLCN 0x12EC00
Virtual LCN namespace
Real LCN namespace
KEY
VALUE
Container table
FIGURE 11-92 The container table provides a virtual LCN-to-real LCN indirection layer.
The container table is important: all the data managed by ReFS and Minstore needs to pass through 
the container table (with only small exceptions), so ReFS maintains multiple copies of this vital table. 
To perform an I/O on a block, ReFS must first look up the location of the extent’s container to find the 
CHAPTER 11
Caching and file systems
765
real location of the data. This is achieved through the extent table, which contains target virtual LCN 
of the cluster range in the data section of its rows. The container ID is derived from the LCN, through a 
mathematical relationship. The new level of indirection allows ReFS to move the location of containers 
without consulting or modifying the file extent tables. 
ReFS consumes tiers produced by Storage Spaces, hardware tiered volumes, and SMR disks. ReFS 
redirects small random I/Os to a portion of the faster tiers and destages those writes in batches to the 
slower tiers using sequential writes (destages happen at container granularity). Indeed, in ReFS, the 
term fast tier (or ash tier) refers to the random-access zone, which might be provided by the conven-
tional bands of an SMR disk, or by the totality of an SSD or NVMe device. The term slow tier (or HDD 
tier) refers instead to the sequential write bands or to a rotating disk. ReFS uses different behaviors 
based on the class of the underlying medium. Non-SMR disks have no sequential requirements, so 
clusters can be allocated from anywhere on the volume; SMR disks, as discussed previously, need to 
have strictly sequential requirements, so ReFS never writes random data on the slow tier.
By default, all of the metadata that ReFS uses needs to stay in the fast tier; ReFS tries to use the 
fast tier even when processing general write requests. In non-SMR disks, as flash containers fill, ReFS 
moves containers from flash to HDD (this means that in a continuous write workload, ReFS is continu-
ally moving containers from flash into HDD). ReFS is also able to do the opposite when needed—select 
containers from the HDD and move them into flash to fill with subsequent writes. This feature is called 
container rotation and is implemented in two stages. After the storage driver has copied the actual 
data, ReFS modifies the container LCN mapping shown earlier. No modification in any file’s extent 
table is needed. 
Container rotation is implemented only for non-SMR disks. This is important, because in SMR 
disks, the ReFS file system driver never automatically moves data between tiers. Applications that are 
SMR disk–aware and want to write data in the SMR capacity tier can use the FSCTL_SET_REFS_FILE_
STRICTLY_SEQUENTIAL control code. If an application sends the control code on a file handle, the ReFS 
driver writes all of the new data in the capacity tier of the volume. 
EXPERIMENT: Witnessing SMR disk tiers 
You can use the FsUtil tool, which is provided by Windows, to query the information of an SMR 
disk, like the size of each tier, the usable and free space, and so on. To do so, just run the tool in 
an administrative command prompt. You can launch the command prompt as administrator by 
searching for cmd in the Cortana Search box and by selecting Run As Administrator after right-
clicking the Command Prompt label. Input the following parameters:
fsutil volume smrInfo 
replacing the VolumeDrive part with the drive letter of your SMR disk.
EXPERIMENT: Witnessing SMR disk tiers 
You can use the FsUtil tool, which is provided by Windows, to query the information of an SMR 
disk, like the size of each tier, the usable and free space, and so on. To do so, just run the tool in 
an administrative command prompt. You can launch the command prompt as administrator by 
searching for cmd in the Cortana Search box and by selecting Run As Administrator after right-
clicking the Command Prompt label. Input the following parameters:
fsutil volume smrInfo 
replacing the VolumeDrive part with the drive letter of your SMR disk.
766
CHAPTER 11
Caching and file systems
Furthermore, you can start a garbage collection (see the next paragraph for details about this 
feature) through the following command:
fsutil volume smrGc  Action=startfullspeed
The garbage collection can even be stopped or paused through the relative Action param-
eter. You can start a more precise garbage collection by specifying the IoGranularity parameter, 
which specifies the granularity of the garbage collection I/O, and using the start action instead 
of startfullspeed.
Container compaction
Container rotation has performance problems, especially when storing small files that don’t usually 
fit into an entire band. Furthermore, in SMR disks, container rotation is never executed, as we ex-
plained earlier. Recall that each SMR band has an associated write pointer (hardware implemented), 
which identifies the location for sequential writing. If the system were to write before or after the write 
pointer in a non-sequential way, it would corrupt data located in other clusters (the SMR firmware must 
therefore refuse such a write).
ReFS supports two types of containers: base containers, which map a virtual cluster’s range directly 
to physical space, and compacted containers, which map a virtual container to many different base 
containers. To correctly map the correspondence between the space mapped by a compacted contain-
er and the base containers that compose it, ReFS implements an allocation bitmap, which is stored in 
the rows of the global container index table (another table, in which every row describes a single com-