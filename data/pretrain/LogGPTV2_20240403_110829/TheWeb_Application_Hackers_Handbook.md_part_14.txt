### Chapter 4: Mapping the Application

**Page 75**

In many cases, a traditional web spider's URL-based view of an application is useful (see Chapter 3 for more details). For instance, in the EIS application, the `/shop` and `/pub` paths use REST-style URLs. Spidering these areas can easily provide unique links to the items within these paths.

**Figure 4-1: Mapping part of an application using Burp Spider**

While fully automated content enumeration can be effective, it has significant limitations:

- **Unusual Navigation Mechanisms:** Tools often fail to handle complex JavaScript-based navigation, potentially missing entire sections of an application.
- **Client-Side Objects:** Links embedded in compiled client-side objects like Flash or Java applets may not be detected by spiders.
- **Multistage Functionality:** Fine-grained input validation checks in multistage processes can prevent automated tools from progressing. For example, a user registration form with fields for name, email, phone number, and zip code might reject the test strings submitted by a spider, preventing it from proceeding.
- **URL-Based Identification:** Automated spiders use URLs as unique identifiers. If the same URL returns different content based on form submissions, the spider may miss significant parts of the application. Some tools, like Burp Spider, can be configured to handle this, but there are still limitations.
- **Volatile Data in URLs:** Applications that include volatile data in URLs (e.g., timers or random seeds) can cause the spider to run indefinitely, as each page appears to have new URLs.
- **Authentication Handling:** An effective spider must handle authentication to access protected functionality. However, even with manual configuration, the spider's operation can break the authenticated session, for example, by following a logout link or submitting invalid input.

**Page 76**

**WARNING:** Running a simple web spider can be extremely dangerous in some applications. For instance, if an application includes administrative functions like deleting users or shutting down a database, an application-aware spider could cause significant damage. The authors have encountered a CMS function that was not protected by access control, which, if discovered by a spider, could deface the main website in real time.

### User-Directed Spidering

This technique is more sophisticated and controlled, usually preferable to fully automated spidering. The user navigates the application using a standard browser while passing the traffic through a tool that combines an intercepting proxy and a spider. This tool builds a map of the application, incorporating all visited URLs and parsing responses to update the site map.

**Benefits of User-Directed Spidering:**

- **Complex Navigation:** Users can follow unusual or complex navigation mechanisms using a browser, ensuring that all accessed functions and content are processed by the proxy/spider tool.
- **Data Validation:** The user controls data submission, ensuring that validation requirements are met.
- **Session Management:** The user can log in and maintain an active session, relogging in if necessary.
- **Discretionary Requests:** The user can decide which functions to request or carry out, avoiding dangerous actions.

**Example: Extreme Internet Shopping Site**

In the EIS site, the spider could not index content within `/home` because it required authentication. With user-directed spidering, the user logs in, and the proxy/spider tool identifies the additional content available.

**Figure 4-2: Burpâ€™s site map after user-guided spidering**

This reveals additional resources within the home menu system, such as a private profile accessed via a JavaScript function.

**Tip:** Browser extensions like IEWatch can perform HTTP and HTML analysis, providing a second record of mapping data. See Chapter 20 for more information.

### Hack Steps

1. Configure your browser to use Burp or WebScarab as a local proxy.
2. Browse the entire application, visiting every link, submitting every form, and completing all multi-step functions. Try different browser configurations (JavaScript enabled/disabled, cookies enabled/disabled).
3. Review the site map generated by the proxy/spider tool, identifying any unvisited content. Use the browser to manually access and parse the response for further content.
4. Optionally, actively spider the site from the enumerated content, excluding dangerous URLs.

### Discovering Hidden Content

Applications often contain hidden content, such as testing or debugging functionality, or content visible only to certain user categories. Examples include backup files, new or old versions of files, configuration files, and log files.

### Brute-Force Techniques

Automated techniques can be used to guess the names or identifiers of hidden functionality. For example, after identifying the following content:

- `http://eis/auth/Login`
- `http://eis/auth/ForgotPassword`
- `http://eis/home/`
- `http://eis/pub/media/100/view`
- `http://eis/images/eis.gif`
- `http://eis/include/eis.css`

You can use Burp Intruder to iterate through common directory names and capture server responses. This can help identify valid directories and pages.

**Figure 4-4: Configuring Burp Intruder to probe for common directories**

**Figure 4-5: Results of a directory brute-force attack**

**Figure 4-6: Results of a file brute-force attack**

**Note:** Do not assume that a 200 OK response means the resource exists. Many applications handle requests for nonexistent resources in custom ways, often returning a 200 response code. A 302 Found response might indicate a redirect to a login page or another location, and a 400 Bad Request might indicate a custom naming scheme or invalid syntax in the wordlist.