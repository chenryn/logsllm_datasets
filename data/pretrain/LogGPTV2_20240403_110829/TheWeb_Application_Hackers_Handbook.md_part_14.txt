Stuttard c04.indd V3 - 07/22/2011 Page 75
Chapter 4 n Mapping the Application 75
(see Chapter 3 for more details). The traditional web spider’s URL-based view
of the application is useful in these situations. In the EIS application, the
/shop and /pub paths employ REST-style URLs, and spidering these areas eas-
ily provides unique links to the items available within these paths.
Figure 4-1: Mapping part of an application using Burp Spider
Although it can often be effective, this kind of fully automated approach to
content enumeration has some signifi cant limitations:
n Unusual navigation mechanisms (such as menus dynamically created
and handled using complicated JavaScript code) often are not handled
properly by these tools, so they may miss whole areas of an application.
n Links buried within compiled client-side objects such as Flash or Java
applets may not be picked up by a spider.
n Multistage functionality often implements fi ne-grained input validation
checks, which do not accept the values that may be submitted by an auto-
mated tool. For example, a user registration form may contain fi elds for
name, e-mail address, telephone number, and zip code. An automated
cc0044..iinndddd 7755 88//1199//22001111 1122::0044::4411 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 76
76 Chapter 4 n Mapping the Application
application spider typically submits a single test string in each editable
form fi eld, and the application returns an error message saying that one
or more of the items submitted were invalid. Because the spider is not
intelligent enough to understand and act on this message, it does not
proceed past the registration form and therefore does not discover any
more content or functions accessible beyond it.
n Automated spiders typically use URLs as identifi ers of unique content.
To avoid continuing spidering indefi nitely, they recognize when linked
content has already been requested and do not request it again. However,
many applications use forms-based navigation in which the same URL
may return very different content and functions. For example, a bank-
ing application may implement every user action via a POST request to
/account.jsp and use parameters to communicate the action being per-
formed. If a spider refuses to make multiple requests to this URL, it will
miss most of the application’s content. Some application spiders attempt
to handle this situation. For example, Burp Spider can be confi gured to
individuate form submissions based on parameter names and values.
However, there may still be situations where a fully automated approach
is not completely effective. We discuss approaches to mapping this kind
of functionality later in this chapter.
n Conversely to the previous point, some applications place volatile data
within URLs that is not actually used to identify resources or functions (for
example, parameters containing timers or random number seeds). Each
page of the application may contain what appears to be a new set of URLs
that the spider must request, causing it to continue running indefi nitely.
n Where an application uses authentication, an effective application spider
must be able to handle this to access the functionality that the authen-
tication protects. The spiders mentioned previously can achieve this by
manually confi guring the spider either with a token for an authenticated
session or with credentials to submit to the login function. However, even
when this is done, it is common to fi nd that the spider’s operation breaks
the authenticated session for various reasons:
n By following all URLs, at some point the spider will request the logout
function, causing its session to break.
n If the spider submits invalid input to a sensitive function, the applica-
tion may defensively terminate the session.
n If the application uses per-page tokens, the spider almost certainly will
fail to handle these properly by requesting pages out of their expected
sequence, probably causing the entire session to be terminated.
cc0044..iinndddd 7766 88//1199//22001111 1122::0044::4411 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 77
Chapter 4 n Mapping the Application 77
WARNING In some applications, running even a simple web spider that
parses and requests links can be extremely dangerous. For example, an applica-
tion may contain administrative functionality that deletes users, shuts down a
database, restarts the server, and the like. If an application-aware spider is used,
great damage can be done if the spider discovers and uses sensitive functional-
ity. The authors have encountered an application that included some Content
Management System (CMS) functionality for editing the content of the main
application. This functionality could be discovered via the site map and was not
protected by any access control. If an automated spider were run against this
site, it would fi nd the edit function and begin sending arbitrary data, resulting in
the main website’s being defaced in real time while the spider was running.
User-Directed Spidering
This is a more sophisticated and controlled technique that is usually prefer-
able to automated spidering. Here, the user walks through the application in
the normal way using a standard browser, attempting to navigate through all
the application’s functionality. As he does so, the resulting traffi c is passed
through a tool combining an intercepting proxy and spider, which monitors
all requests and responses. The tool builds a map of the application, incorpo-
rating all the URLs visited by the browser. It also parses all the application’s
responses in the same way as a normal application-aware spider and updates
the site map with the content and functionality it discovers. The spiders
within Burp Suite and WebScarab can be used in this way (see Chapter 20
for more information).
Compared with the basic spidering approach, this technique offers numer-
ous benefi ts:
n Where the application uses unusual or complex mechanisms for navigation,
the user can follow these using a browser in the normal way. Any functions
and content accessed by the user are processed by the proxy/spider tool.
n The user controls all data submitted to the application and can ensure
that data validation requirements are met.
n The user can log in to the application in the usual way and ensure that the
authenticated session remains active throughout the mapping process. If
any action performed results in session termination, the user can log in
again and continue browsing.
n Any dangerous functionality, such as deleteUser.jsp, is fully enumer-
ated and incorporated into the proxy’s site map, because links to it will be
parsed out of the application’s responses. But the user can use discretion
in deciding which functions to actually request or carry out.
cc0044..iinndddd 7777 88//1199//22001111 1122::0044::4411 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 78
78 Chapter 4 n Mapping the Application
In the Extreme Internet Shopping site, previously it was impossible for the
spider to index any content within /home, because this content is authenticated.
Requests to /home result in this response:
HTTP/1.1 302 Moved Temporarily
Date: Mon, 24 Jan 2011 16:13:12 GMT
Server: Apache
Location: /auth/Login?ReturnURL=/home/
With user-directed spidering, the user can simply log in to the application
using her browser, and the proxy/spider tool picks up the resulting session and
identifi es all the additional content now available to the user. Figure 4-2 shows
the EIS site map when the user has successfully authenticated to the protected
areas of the application.
Figure 4-2: Burp’s site map after user-guided spidering has been performed
This reveals some additional resources within the home menu system. The
fi gure shows a reference to a private profi le that is accessed through a JavaScript
function launched with the onClick event handler:
private profile
cc0044..iinndddd 7788 88//1199//22001111 1122::0044::4411 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 79
Chapter 4 n Mapping the Application 79
A conventional web spider that simply follows links within HTML is likely to
miss this type of link. Even the most advanced automated application crawlers
lag way behind the numerous navigational mechanisms employed by today’s
applications and browser extensions. With user-directed spidering, however,
the user simply needs to follow the visible on-screen link using her browser,
and the proxy/spider tool adds the resulting content to the site map.
Conversely, note that the spider has successfully identifi ed the link to /core/
sitestats contained in an HTML comment, even though this link is not shown
on-screen to the user.
TIP In addition to the proxy/spider tools just described, another range
of tools that are often useful during application mapping are the various
browser extensions that can perform HTTP and HTML analysis from within the
browser interface. For example, the IEWatch tool shown in Figure 4-3, which
runs within Microsoft Internet Explorer, monitors all details of requests and
responses, including headers, request parameters, and cookies. It analyzes
every application page to display links, scripts, forms, and thick-client compo-
nents. Of course, all this information can be viewed in your intercepting proxy,
but having a second record of useful mapping data can only help you better
understand the application and enumerate all its functionality. See Chapter 20
for more information about tools of this kind.
Figure 4-3: IEWatch performing HTTP and HTML analysis from within the browser
cc0044..iinndddd 7799 88//1199//22001111 1122::0044::4422 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 80
80 Chapter 4 n Mapping the Application
HACK STEPS
1. Configure your browser to use either Burp or WebScarab as a local proxy
(see Chapter 20 for specific details about how to do this if you’re unsure).
2. Browse the entire application normally, attempting to visit every link/URL
you discover, submitting every form, and proceeding through all multi-
step functions to completion. Try browsing with JavaScript enabled and
disabled, and with cookies enabled and disabled. Many applications can
handle various browser configurations, and you may reach different con-
tent and code paths within the application.
3. Review the site map generated by the proxy/spider tool, and identify
any application content or functions that you did not browse manually.
Establish how the spider enumerated each item. For example, in Burp
Spider, check the Linked From details. Using your browser, access the item
manually so that the response from the server is parsed by the proxy/spi-
der tool to identify any further content. Continue this step recursively until
no further content or functionality is identified.
4. Optionally, tell the tool to actively spider the site using all of the already
enumerated content as a starting point. To do this, first identify any URLs
that are dangerous or likely to break the application session, and config-
ure the spider to exclude these from its scope. Run the spider and review
the results for any additional content it discovers.
The site map generated by the proxy/spider tool contains a wealth of infor-
mation about the target application, which will be useful later in identifying
the various attack surfaces exposed by the application.
Discovering Hidden Content
It is common for applications to contain content and functionality that is not
directly linked to or reachable from the main visible content. A common example
is functionality that has been implemented for testing or debugging purposes
and has never been removed.
Another example arises when the application presents different functionality
to different categories of users (for example, anonymous users, authenticated
regular users, and administrators). Users at one privilege level who perform
exhaustive spidering of the application may miss functionality that is visible to
users at other levels. An attacker who discovers the functionality may be able
to exploit it to elevate her privileges within the application.
There are countless other cases in which interesting content and functionality
may exist that the mapping techniques previously described would not identify:
n Backup copies of live fi les. In the case of dynamic pages, their fi le extension
may have changed to one that is not mapped as executable, enabling you
cc0044..iinndddd 8800 88//1199//22001111 1122::0044::4422 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 81
Chapter 4 n Mapping the Application 81
to review the page source for vulnerabilities that can then be exploited
on the live page.
n Backup archives that contain a full snapshot of fi les within (or indeed
outside) the web root, possibly enabling you to easily identify all content
and functionality within the application.
n New functionality that has been deployed to the server for testing but not
yet linked from the main application.
n Default application functionality in an off-the-shelf application that has
been superfi cially hidden from the user but is still present on the server.
n Old versions of fi les that have not been removed from the server. In the
case of dynamic pages, these may contain vulnerabilities that have been
fi xed in the current version but that can still be exploited in the old version.
n Confi guration and include fi les containing sensitive data such as database
credentials.
n Source fi les from which the live application’s functionality has been
compiled.
n Comments in source code that in extreme cases may contain information
such as usernames and passwords but that more likely provide information
about the state of the application. Key phrases such as “test this function”
or something similar are strong indicators of where to start hunting for
vulnerabilities.
n Log fi les that may contain sensitive information such as valid usernames,
session tokens, URLs visited, and actions performed.
Effective discovery of hidden content requires a combination of automated and
manual techniques and often relies on a degree of luck.
Brute-Force Techniques
Chapter 14 describes how automated techniques can be leveraged to speed up
just about any attack against an application. In the present context of informa-
tion gathering, automation can be used to make huge numbers of requests to the
web server, attempting to guess the names or identifi ers of hidden functionality.
For example, suppose that your user-directed spidering has identifi ed the
following application content:
http://eis/auth/Login
http://eis/auth/ForgotPassword
http://eis/home/
http://eis/pub/media/100/view
http://eis/images/eis.gif
http://eis/include/eis.css
cc0044..iinndddd 8811 88//1199//22001111 1122::0044::4422 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 82
82 Chapter 4 n Mapping the Application
The fi rst step in an automated effort to identify hidden content might involve
the following requests, to locate additional directories:
http://eis/About/
http://eis/abstract/
http://eis/academics/
http://eis/accessibility/
http://eis/accounts/
http://eis/action/
...
Burp Intruder can be used to iterate through a list of common directory
names and capture details of the server’s responses, which can be reviewed to
identify valid directories. Figure 4-4 shows Burp Intruder being confi gured to
probe for common directories residing at the web root.
Figure 4-4: Burp Intruder being configured to probe for common directories
When the attack has been executed, clicking column headers such as “status”
and “length” sorts the results accordingly, enabling you to quickly identify a
list of potential further resources, as shown in Figure 4-5.
Having brute-forced for directories and subdirectories, you may then want
to fi nd additional pages in the application. Of particular interest is the /auth
directory containing the Login resource identifi ed during the spidering pro-
cess, which is likely to be a good starting point for an unauthenticated attacker.
Again, you can request a series of fi les within this directory:
cc0044..iinndddd 8822 88//1199//22001111 1122::0044::4422 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 83
Chapter 4 n Mapping the Application 83
http://eis/auth/About/
http://eis/auth/Aboutus/
http://eis/auth/AddUser/
http://eis/auth/Admin/
http://eis/auth/Administration/
http://eis/auth/Admins/
...
Figure 4-5: Burp Intruder showing the results of a directory brute-force attack
Figure 4-6 shows the results of this attack, which has identifi ed several resources
within the /auth directory:
Login
Logout
Register
Profile
Note that the request for Profile returns the HTTP status code 302. This
indicates that accessing this link without authentication redirects the user to
the login page. Of further interest is that although the Login page was discov-
ered during spidering, the Register page was not. It could be that this extra
functionality is operational, and an attacker could register a user account on
the site.
cc0044..iinndddd 8833 88//1199//22001111 1122::0044::4422 PPMM
Stuttard c04.indd V3 - 07/22/2011 Page 84
84 Chapter 4 n Mapping the Application
Figure 4-6: Burp Intruder showing the results of a file brute-force attack
NOTE Do not assume that the application will respond with 200 OK if a
requested resource exists and 404 Not Found if it does not. Many applica-
tions handle requests for nonexistent resources in a customized way, often
returning a bespoke error message and a 200 response code. Furthermore,
some requests for existent resources may receive a non-200 response. The fol-
lowing is a rough guide to the likely meaning of the response codes that you
may encounter during a brute-force exercise looking for hidden content:
n 302 Found — If the redirect is to a login page, the resource may be
accessible only by authenticated users. If the redirect is to an error mes-
sage, this may indicate a different reason. If it is to another location, the
redirect may be part of the application’s intended logic, and this should
be investigated further.
n 400 Bad Request — The application may use a custom naming scheme
for directories and fi les within URLs, which a particular request has not
complied with. More likely, however, is that the wordlist you are using
contains some whitespace characters or other invalid syntax.