4) Temporal completeness: Despite the rapid changes in
inferred attributes that we observe above, Facebook does not
provide any historical information about the attributes it had
inferred about a user. Thus Facebook’s data explanations do
not exhibit temporal completeness.
5) Correctness: Testing correctness precisely is challeng-
ing, as the provided data explanations are vague and do not
reveal the exact page the user liked, or the ad the user clicked.
9In the Self-Serve Ads Terms Facebook says “In instances where we believe
doing so will enhance the effectiveness of your advertising campaign, we
may broaden the targeting criteria you specify.” Thus, to be sure that the user
received the ad because Facebook thinks he in interested in the attribute, it is
not enough for the user to receive the ad of our ad campaign, but the attribute
also needs to appear in the explanation.
In order to brieﬂy test correctness, we created a fake
Facebook account, and liked 7 Facebook pages related to
U.S. Politics and 15 pages related to TV Shows. We run the
experiment in a controlled environment, in a browser with no
history, and we did not perform any other actions on Facebook
besides liking the mentioned pages. From these 22 likes,
Facebook inferred 27 interests; all of these interests had data
explanations like “You have this preference because you liked a
page related to [interest].” Thus, we did not ﬁnd any indication
that explanations were incorrect. While a more comprehensive
set of experiments is required for more complete results, we
leave such an exploration to future work.
E. Summary
While the Ad Preferences Page does bring some trans-
parency to the different attributes users can be targeted with,
the provided explanations are incomplete and often vague.
Facebook does not provide information about data-broker-
provided attributes in its data explanations or in its ad ex-
planations. This means that currently users have no way of
knowing what data broker attributes advertisers can use to
target them. This is despite the fact that close to half of the
targeting attributes come from data brokers and they have an
audience reach similar to Facebook’s own targeting attributes.
V. RELATED WORK
A. Bringing transparency to targeted advertising
While there have been many studies on online advertising,
ad auctions, tracking, and ad blocking in general, we focus
next only on the studies that are the closest to our proposal;
we refer the reader to [44] for a more general overview of the
work in the space. We split the related works according to the
kind of transparency they aim to provide.
Ad-level transparency: Two studies [17], [34] proposed tech-
niques to detect whether an ad is contextual, re-targeted or
behavioral. A few other studies took the next step and proposed
methods to detect why the ads are being targeted, that is, what
particular user action triggered the targeting of a particular
ad [30], [31], [20], [36]. At a high level, these approaches
monitor the actions of users (e.g., the emails users receive
and send, the videos users see on YouTube) and they propose
methods to estimate the likelihood that a given ad was shown
due to a given input by performing controlled experiments. In
contrast, we investigate how explanations provided by Face-
book reveal information about why an ad has been targeted.
User-level transparency: Closest to our work are three tools:
Floodwatch [6] and EyeWnder [4] collect
the ads people
receive while browsing the internet and provide aggregate
statistics about them; and MyAdChoices [36] detects whether
an ad is interest-based, generic, or retargeted, and allows users
to selectively block certain types of ads. None of the tools
focus on social media advertising and they do not analyze
ad explanations. Two other studies analyzed the Google Ad
Settings [7] (which is the equivalent of the Facebook Ad
Preferences Page). Datta et al. [20] checked whether users
receive different ads if they change their categories in the
Google Ad Settings in order to detect discrimination. Willis
et al. [43] investigated whether the Google Ad Setting pages
13
reveal all the categories Google inferred about a user and
found that some behavioral ads were not explained by the
revealed inferred categories. In contrast, we provide deﬁnite
proof that Facebook makes available more targeting attributes
to advertisers than it reveals to users.
Platform-level transparency: A few measurement studies bring
insights into various aspects of the ad ecosystem. Barford
et al. [16] focus mainly on presenting aggregated statistics
by crawling ads at
large scale. Using experiments based
on artiﬁcial personas, they also study the relation between
personas and advertiser categories and test whether an ad is
behavioral. This study, however, does not focus on social media
ad targeting but rather on the traditional ad ecosystem that
targets users when they browse the Internet.
B. Analyzing Facebook’s advertiser interface
A number of studies have investigated Facebook’s adver-
tiser interface and its pitfalls. For instance, ProPublica, an
investigative journalism organization, showed that advertisers
can create ads related to housing, while excluding users based
on race, an act which is illegal [13]. More recently, ProP-
ublica, as part of their “Breaking the Black Box” series [14],
investigated whether Facebook informs users sufﬁciently about
the use of data brokers in advertising [15] and found that
while advertisers can target users with attributes provided by
data brokers, they do not mention it in the Ad Preferences
Page. Our work conﬁrms this ﬁnding but also goes beyond in
investigating other types of transparency.
Finally, Korolova et al. [28] proposed an attack that exploits
Facebook’s advertiser interface to infer private attributes of
Facebook users. Later work by Venkatadri et al. [41] demon-
strated that more advanced attacks are possible through the
custom audience advertiser interface. However, the focus of
these studies is not transparency, but on pinpointing vulnera-
bilities in the advertising interface.
C. Interpretability of decision making systems
Transparency and interpretability have been the focus of
many recent studies in the context of automated decision
making systems, with many previous works acknowledging
the importance of having more interpretable models [22], [42],
[33]. A ﬁrst line of work focuses on providing explanations
to existing algorithms/decision making systems, by studying
for example, what are the inputs that have the biggest impact
on the outputs [21], or by uncovering how the model behaves
locally around speciﬁc predictions [38]. A second line of work
aims at building algorithms that are interpretable by design
by integrating interpretability constraints in their optimiza-
tion functions [27], [32]. The main use-case for interpretable
models is for the domain experts to understand whether the
algorithm is behaving appropriately or not. In our work, we
study explanations that are provided to users with the goal of
making sure that users get satisfactory and useful explanations.
Our work offers a different perspective on how to build good
explanations and, to our knowledge, is the ﬁrst empirical study
of real-world explanations in social media advertising.
While many studies emphasize that explanations and trans-
parency mechanisms bring trust
to a platform [33], [38],
Weller [42] warns that platforms can manipulate users to trust
their system, with explanations that are not useful to them. The
“Copy Machine” study [29] shows that useless explanations
that did not provide any actual information were almost equally
successful in gaining trust as meaningful explanations. Our
study shows the different ways in which explanations offered
by Facebook fail to provide adequate information to end users
or worse, provide them with misleading information.
VI. CONCLUSION
In this paper, we investigated transparency mechanisms
for social media advertising by analyzing Facebook’s ad ex-
planations and data explanations. We devised a set of key
properties that such explanations could satisfy, such as correct-
ness, completeness and speciﬁcity; we then performed a series
of controlled ad campaigns to analyze whether Facebook’s
explanations satisfy such properties.
Our experiments demonstrated that Facebook’s ad expla-
nations are often incomplete and sometimes misleading, and
that Facebook’s data explanations are incomplete and often
vague. These ﬁndings have important implications for users,
as they may lead them to incorrectly conclude how they
were targeted with ads. Moreover, these ﬁndings also suggest
that malicious advertisers may be able to obfuscate their true
targeting attributes by hiding rare (and potentially sensitive)
attributes by also selecting very common ones. To make
matters worse, Twitter recently introduced explanations that
are similar to Facebook’s explanations. This underscores the
urgent need to provide properly designed explanations as social
media advertising services mature. We hope that our study will
provide a basis to guide such a design.
To complement our work,
it would be interesting to
to different possible
perform a study on how users react
explanations that can be provided. This would explore another
dimension that could further inform the explanations’ design
choices. Yet we believe that it is important ﬁrst to understand
explanations at a technical level in order to understand their
vulnerabilities. Hence, we leave such a study for future work.
Facebook’s explanations only provide a partial view of
its advertising mechanisms. To move towards greater trans-
parency we built a tool, AdAnalyst, that works on top of
Facebook and provides explanations with some of the missing
properties. AdAnalyst keeps track of historical data about ads
and explanations to provide users with a temporal view; and
it provides a wider perspective by aggregating data across
users. The tool can be downloaded and installed from http:
//adanalyst.mpi-sws.org/. We hope that AdAnalyst will help
increase the transparency of Facebook advertising and that it
will allow users to detect malicious and deceptive advertising.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their helpful com-
ments. This research was supported in part by NSF through
grants CNS-1563320 and CNS-1616234, by ANR through
grants ANR-17-CE23-0014 and ANR-16-TERC-0012-01, by
Institut Mines Telecom through the “Future & Ruptures” pro-
gram and by a Data Transparency Lab grant. We acknowledge
funding from the Alexander von Humboldt Foundation as
well.
14
REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
reach,”
potential
https://www.facebook.com/business/help/
“About
1665333080167380, accessed: 2017-11-30.
“About the delivery system: Ad auctions,” https://www.facebook.com/
business/help/430291176997542, accessed: 2017-11-30.
“Datalogix segments,” http://bit.ly/2qzt5oI, accessed: 2017-11-30.
“Eyewnder,” http://www.eyewnder.com/, accessed: 2017-11-30.
“Facebook ad preferences,” https://www.facebook.com/ads/preferences/,
accessed: 2017-11-30.
“Floodwatch,” https://beta.ﬂoodwatch.me/, accessed: 2017-08-11.
“Google ad settings,” https://myaccount.google.com/, accessed: 2017-
11-30.
“US voter list information ,” http://voterlist.electproject.org/, accessed:
2017-11-30.
“EU General data protection regulation,” Apr. 2016, accessible from
https://www.eugdpr.org/.
“LOI no 2016-1321 du 7 octobre 2016 pour une R´epublique
num´erique,” Journal Ofﬁciel de la R´epublique Franc¸aise no 0235 du
8 octobre 2016, Oct. 2016, accessible at https://www.legifrance.gouv.
fr/eli/loi/2016/10/7/ECFI1524250L/jo/texte.
[11] Acxiom, “Consumer data products catalog,” http://bit.ly/2rjzWFT, ac-
cessed: 2017-11-30.
[12] ——, “Privacy faq,” http://bit.ly/2qupYAo, accessed: 2017-11-30.
[13]
J. Angwin and T. Parris Jr., “Facebook lets advertisers exclude users by
race,” http://bit.ly/2eXf7ap, October 28, 2016, accessed: 2017-11-30.
J. Angwin, T. Parris Jr., and S. Mattu, “Breaking the black box: What
Facebook knows about you,” http://bit.ly/2driPIj, September 28, 2016,
accessed: 2017-11-30.
[14]
[15] ——, “Facebook doesn’t tell users everything it really knows about
them,” http://bit.ly/2ieiNsq, December 27, 2016, accessed: 2017-11-30.
[16] P. Barford, I. Canadi, D. Krushevskaja, Q. Ma, and S. Muthukrishnan,
“Adscape: Harvesting and analyzing online display ads,” in WWW,
2014.
J. M. Carrascosa, J. Mikians, R. Cuevas, V. Erramilli, and N. Laoutaris,
“I always feel like somebody’s watching me: measuring online be-
havioural advertising,” in ACM CoNEXT, 2015.
J. Constine, “Facebook lets businesses plug in CRM email addresses
to target customers with hyper-relevant ads,” http://tcrn.ch/2q0JdxP,
September 20, 2012, accessed: 2017-11-30.
[17]
[18]
[19] B. Darwell, “Facebook platform supports more than 42 million pages
and 9 million apps,” http://bit.ly/28YXb1H, April 27, 2012, accessed:
2017-11-30.
[20] A. Datta, M. C. Tschantz, and A. Datta, “Automated experiments on
ad privacy settings,” in PETS, 2015.
[21] A. Datta, S. Sen, and Y. Zick, “Algorithmic transparency via quantitative
input inﬂuence: Theory and experiments with learning systems,” in
IEEE S&P, 2016.
[22] F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable
machine learning,” arXiv preprint 1702.08608, 2017.
[23] Experian, “Product and service privacy policies,” http://www.experian.
com/privacy/prod serv policy.html, accessed: 2017-11-30.
[24] Facebook, “How does the conversion pixel track conversions?” http:
//bit.ly/2peqORu, accessed: 2017-11-30.
[25] O. Goga, P. Loiseau, R. Sommer, R. Teixeira, and K. P. Gummadi, “On
the reliability of proﬁle matching across large online social networks,”
in ACM KDD, 2015.
[26] B. Goodman and S. Flaxman, “European Union regulations on algo-
rithmic decision-making and a ”right to explanation”,” in WHI, 2016.
[27] B. Kim, J. A. Shah, and F. Doshi-Velez, “Mind the gap: A generative
approach to interpretable feature selection and extraction,” in NIPS,
2015.
[28] A. Korolova, “Privacy violations using microtargeted ads: A case study,”
in IEEE ICDMW, 2010.
[29] E. J. Langer, A. Blank, and B. Chanowitz, “The mindlessness of
ostensibly thoughtful action: The role of ’placebic’ information in
interpersonal interaction.” Journal of personality and social psychology,
1978.
[30] M. L´ecuyer, G. Ducoffe, F. Lan, A. Papancea, T. Petsios, R. Spahn,
A. Chaintreau, and R. Geambasu, “Xray: Enhancing the web’s trans-
parency with differential correlation.” in USENIX Security, 2014.
[31] M. Lecuyer, R. Spahn, Y. Spiliopolous, A. Chaintreau, R. Geambasu,
and D. Hsu, “Sunlight: Fine-grained targeting detection at scale with
statistical conﬁdence,” in ACM CCS, 2015.
[32] B. Letham, C. Rudin, T. H. McCormick, D. Madigan et al., “Inter-
pretable classiﬁers using rules and bayesian analysis: Building a better
stroke prediction model,” The Annals of Applied Statistics, 2015.
[33] Z. C. Lipton, “The mythos of model interpretability,” in WHI, 2016.
[34] B. Liu, A. Sheth, U. Weinsberg, J. Chandrashekar, and R. Govindan,
“AdReveal: Improving transparency into online targeted advertising,”
in ACM HotNets, 2013.
J. R. Mayer and J. C. Mitchell, “Third-party web tracking: Policy and
technology,” in IEEE S&P, 2012.
J. Parra-Arnau, J. P. Achara, and C. Castelluccia, “MyAdChoices:
Bringing Transparency and Control to Online Advertising,” ACM Trans.
Web, 2017.
[35]
[36]
[37] A. C. Plane, E. M. Redmiles, M. L. Mazurek, and M. C. Tschantz,
“Exploring user perceptions of discrimination in online targeted adver-
tising,” in USENIX Security, 2017.
[38] M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should I trust you?:
Explaining the predictions of any classiﬁer,” in ACM KDD, 2016.
[39] N. Stokes, “Should you use Facebook or Google to log in to other
sites?” http://bit.ly/1kxEP3X, May 6, 2017, accessed: 2017-11-30.
[40] D. Tynan, “Acxiom exposed: A peek inside one of the world’s largest
data brokers,” http://bit.ly/2qvQQjy, May 15, 2013, accessed: 2017-11-
30.
[41] G. Venkatadri, Y. Liu, A. Andreou, O. Goga, P. Loiseau, A. Mislove, and
K. P. Gummadi, “Auditing Data Brokers’ Advertising Interfaces: Privacy
Risks with Facebook’s PII-based Targeting,” in IEEE S&P, 2018.
[42] A. Weller, “Challenges for transparency,” in WHI, 2017.
[43] C. E. Wills and C. Tatar, “Understanding what they do with what they
know,” in ACM WPES, 2012.
[44] S. Yuan, A. Z. Abidin, M. Sloan, and J. Wang, “Internet advertising:
An interplay among advertisers, online publishers, ad exchanges and
web users,” arXiv preprint arXiv:1206.1754, 2012.
15