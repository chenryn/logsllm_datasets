This is further explored in Figure 8, which presents a box plot
of daily downtime for Mastodon, where we separate instances
based on their number of toots. Although small instances (<10K
Figure 6: Distribution of federated subscription links be-
tween countries. The left axis lists the top-5 countries, and
the lower access indicates the fraction of the federated links
to instances in other countries.
instances in the same country. The top 5 countries attract 93.66% of
all federated subscription links. We posit that this dependency on a
small number of countries may undermine the initial motivation
for the DW, as large volumes of data are situated within just a small
number of jurisdictions; e.g., 89.1% of all toots reside on instances
in Japan, the US, and France.
ASes. Next, we inspect the distribution of instances across ASes;
this is important as an over-dependence on a single AS, may raise
questions related to data pooling or even system resilience. When
looking at the ASes that host Mastodon servers (bottom of Figure 5),
we observe instances spread across 351 ASes. On average, each
AS therefore hosts 10 instances. This suggests a high degree of
distribution, without an overt dependence on a single AS.
That said, due to the varying popularity of these instances, the
top three ASes account for almost two thirds (62%) of all global users,
with the largest one (Amazon) hosting more than 30% of all users—
even though it only is used by 6% of instances. Cloudflare also
plays a prominent role, with 31.7% of toots across 5.4% of instances.
The reasons for this co-location are obvious: Administrators are
naturally attracted to well known and low cost providers. Whereas,
a centrally orchestrated deployment might replicate across multiple
redundant ASes (as often seen with CDNs), this is more difficult in
the DW context because each instance is independently managed
(without coordination). Although these infrastructures are robust,
the failure (or change in policy) of a small set of ASes could therefore
impact a significant fraction of users. Again, this highlights another
222
Challenges in the Decentralised Web
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
ASN
AS9370
AS20473
AS8075
AS12322
AS2516
AS9371
Instances Failures
1
4
7
15
4
14
97
22
12
9
9
8
IPs Users Toots Org.
95
21
12
9
8
8
RankPeers
10
2.0K
33.4K 3.89M Sakura
5.7K
143
150
936K Choopa
1.7K 35.4K Microsoft 2.1K 257
63
123
559
123
3
165
4.4K Free SAS 3.2K
102K KDDI
70
2.4K
4.7K Sakura
Figure 8: Distribution of per-day downtime (measured every
five minutes) of Mastodon instances (binned by number of
toots), and Twitter (Feb–Dec 2007).
(a)
(b)
Figure 9: (a) Footprint of certificate authorities among the
instances. (b) Unavailability of instances (on a per-day ba-
sis).
toots) clearly have the most downtime (median 12%), those with
over 1M toots actually have worse availability than instances with
between 100K and 1M (2.1% vs. 0.34% median downtime). In fact,
the correlation between the number of toots on an instance and its
downtime is -0.04, i.e., instance popularity is not a good predictor of
availability. The figure also includes Twitter’s downtime in 2007 for
comparison (see Section 3). Although we see a number of outliers,
even Twitter, which was famous for its poor availability (the “Fail
Whale” [28]), had better availability compared to Mastodon: its
average downtime was just 1.25% vs. 10.95% for Mastodon instances.
Certificate Dependencies. Another possible reason for failures
is third party dependencies, e.g., TLS certificate problems (Mastodon
uses HTTPS by default). To test if this may have caused issues, we
take the certificate registration data from crt.sh [11], and check
which certificate authorities (CAs) are used by instances, presented
in Figure 9(a). Let’s Encrypt has been chosen as CA for more than
85% of the instances, likely because this service offers good au-
tomation and is free of cost [31]. This, again, confirms a central
dependency in the DW. We also observe that certificate expiry is
a noticeable issue (perhaps due to non-committed administrators).
Figure 9(b) presents the number of instances that have outages
caused by the expiry of their certificates. In the worst case we find
Table 1: AS failures per number of hosted instances. Rank
refers to CAIDA AS Rank, and Peers is the number of net-
works the AS peers [8].
Figure 10: CDF of continuous outage (in days) of instances
not accessible for at least one day (Y1-axis) and number of
toots and users affected due to the outage (Y2-axis).
105 instances to be down on one day (23 July 2018), removing nearly
200K toots from the system. Closer inspection reveals that this was
caused by the Let’s Encrypt CA short expiry policy (90 days), which
simultaneously expired certificates for all 105 instances. In total,
these certificate expirations were responsible for 6.3% of the outages
observed in our dataset.
AS Dependencies. Another potential explanation for some in-
stance unavailability is that AS-wide network outages might occur.
Due to the co-location of instances within the same AS, this could
obviously have a widespread impact. To test this, we correlate the
above instance unavailability to identify cases where all instances
in a given AS simultaneously fail — this may indicate an AS out-
age. Table 1 presents a summary of the most frequent failures (we
consider it to be an AS failure if all instances hosted in the same
AS became unavailable simultaneously). We only include ASes that
host at least 8 instances (to avoid mistaking a small number of
failures as an entire AS failure). We observe a small but notable
set of outages. In total, 6 ASes suffer an outage. The largest is by
AS9370 (Sakura, a Japanese hosting company), which lost 97 in-
stances simultaneously, rendering 3.89M toots unavailable. The AS
with most outages (15) is AS12322 (Free SAS), which removed 9
instances. These outages are responsible for less than 1% of the
failures observed, however, their impact is still significant. In total,
these AS outages resulted in the (temporary) removal of 4.98M
toots from the system, as well as 41.5K user accounts. Although this
centralisation can result in such vulnerabilities, the decentralised
management of Mastodon makes it difficult for administrators to
coordinate placement to avoid these “hot spots”.
Outage durations. Finally, for each outage, we briefly compute
its duration and plot the CDF in Figure 10 (blue line, Y1-axis). While
223
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
A. Raman et al.
Figure 11: CDF of the out-degree distribution of the so-
cial follower graph, federation graph, and Twitter follower
graph.
almost all instances (98%) go down at least once, a quarter of them
are unavailable for at least one day before coming back online,
ranging from 1 day (21%) to over a month (7%). Figure 10 also reports
the number of toots and users affected by the outages: 14% of users
cannot access their instances for a whole day at least once. Naturally,
these measures are closely correlated to toot unavailability (i.e.,
toots become unavailable when their host instance goes offline).
In the worst case, we find one day (April 15, 2017) where 6% of all
(global) toots were unavailable for the whole day. These findings
suggest a need for more reslient approaches to DW management.
5 EXPLORING FEDERATION
The previous section has explored the central role of independent
instances within the Mastodon ecosystem. The other major innova-
tion introduced by the DW is federation. Here we inspect federation
through two lenses: (i) the federated subscription graph that inter-
connects instances (Section 5.1); and (ii) the distributed placement
and sharing of content (toots) via this graph (Section 5.2). This
section studies the resilience properties of DW federation in light
of the frequent failures observed earlier.
5.1 Breaking the User Federation
Federation allows users to create global follower links with users
on other instances. This means that instance outages (Section 4.4)
can create a transitive ripple effect, e.g., if three users on different
instances follow each other, U1 → U2 → U3, then the failure of the
instance hosting U2 would also disconnect U1 and U3 (assuming
that no other paths exist). To highlight the risk, Figure 11 presents
the degree distribution of these graphs, alongside a snapshot of
the Twitter follower graph (see Section 3). We observe traditional
power law distributions across all three graphs. Although natural,
this creates clear points of centralisation, as outages within highly
connected nodes will have a disproportionate impact on the overall
graph structure [3].
To add context to these highly connected instances, Table 2
summarises the graph properties of the top 10 instances (ranked
by the number of toots generated on their timeline). As well as
having very high degree within the graphs, we also note that these
popular instances are operated by a mix of organisations, including
companies (e.g., Pixiv and Dwango), individuals, and crowd-funding.
224
Figure 12: Impact of removing user accounts from G(cid:0)V , E(cid:1).
Each iteration (X axis) represents the removal of the remain-
ing 1% of the highest degree nodes.
Ideally, important instances should have stable and predictable
funding. Curiously, we find less conventional business models, e.g.,
vocalodon.net, an instance dedicated to music that funds itself by
creating compilation albums from user contributions.
Impact of Removing Users. The above findings motivate us to
explore the impact of removing nodes from these graphs. Although
we are primarily interested in infrastructure outages, we start by
evaluating the impact of removing individual users from the social
graph, G(cid:0)V , E(cid:1). This would happen by users deleting their accounts.
Such a failure is not unique to the DW, and many past social net-
works have failed simply by users abandoning them [46]. Here, we
repeat past methodologies to test the resilience of the social graph
by removing the top users and computing two metrics: (i) the size
of the Largest Connected Component (LCC), which represents the
maximum potential number of users that toots can be propagated
to (via shares); and (ii) the number of disconnected components,
which relates to the number of isolated communities retaining in-
ternal connectivity for propagating toots. These metrics have been
used to characterise the attack and error tolerance of social and
other graphs [3, 23, 51].
We proceed in rounds, removing the top 1% of remaining nodes in
each iteration, and computing the size of the LCC in the remaining
graph, as well as the number of new components created by the
removal of crucial connecting nodes. Figure 12 presents the results
as a sensitivity graph. The results confirm that the user follower
graph is extremely sensitive to removing the highly connected
users. Although Mastodon appears to be a strong social graph, with
99.95% of users in the LCC, removing just the top 1% of accounts
decreases the LCC to 26.38% of all users.
As a comparison, we use the Twitter social graph from 2011
when Twitter was a similar age as Mastodon is now (and beset
with frequent “fail whale” appearances [28]). Without any node
removals, Twitter’s LCC contained 95% of users [10]; removing the
top 10% still leaves 80% of users within the LCC. This confirms that
Mastodon’s social graph, by comparison, is far more sensitive to
user removals. Although we expect that the top users on any plat-
form will be more engaged, and therefore less likely to abandon the
platform, the availability of top users to every other user cannot be
guaranteed since there is no central provider and instance outages
Challenges in the Decentralised Web
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Domain
mstdn.jp
friends.nico
pawoo.net
mimumedon.com
imastodon.net
mastodon.social
mastodon.cloud
mstdn-workers.com
vocalodon.net
mstdn.osaka
Toots from #Home
Users
Home Users
23.2K
9.87M
8998
6.54M
30.3K
4.72M
3.29M
1671