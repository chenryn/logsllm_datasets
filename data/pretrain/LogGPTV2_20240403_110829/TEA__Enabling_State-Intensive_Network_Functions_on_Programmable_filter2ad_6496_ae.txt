10.61
10.80
Lat.
(µs)
1.93
1.91
1.91
1.92
Server-based
Tput.
Lat.
(Mpps)
(µs)
5.62
8.49
8.37
5.59
8.37
5.64
5.99
8.25
NAT
Stateful firewall
Load balancer
VPN gateway
Table 3: Throughput and latency of NFs implemented using
TEA with a single server and corresponding software imple-
mentations running on a single server (4 CPU cores). Note
that TEA does not involve the CPU on the server.
(a) NAT processing latency distri-
bution.
(b) NAT throughput for real data
center traces.
Figure 11: Performance of NAT using TEA.
requests to server-1 as soon as it detects the event (at around 71
sec.). We observe that TEA can react to the changes in the link and
server availability quickly despite a slight throughput drop at the
time of failure.
6.2 NF Performance
Comparison with server-based NFs. We note that many factors
including hardware configurations (e.g., number of CPU cores)
and software optimizations can affect the performance of software-
based NFs. Our goal here is to show the cost benefit of TEA by
comparing the performance with the same hardware configuration
(i.e., a server connected to a switch). For the evaluation, we imple-
ment NFs described in Table 2 using Click-DPDK [15] which is one
of popular ways to implement high-performance NFs. We run them
on the server described above.
For a fair comparison, we focus on a per-packet processing la-
tency and throughput for 64 B packets with a single server for TEA
and server-based NFs. We inject packets using 4 traffic generator
nodes (max. traffic rate is ≈138 Mpps). Table 3 summarizes the
results with median values for each experiment. Within each imple-
mentation option, there is no significant differences between NFs.
Between TEA and server-based NFs, TEA shows up to 1.3× and
9.6× higher throughput, without and with the cache, respectively.
For latency, TEA is up to 2.6× faster without cache and 3.1× faster
with cache. TEA does not involve the server’s CPU at all during
the experiments while server-based NFs fully utilize 4 CPU cores.
Note that with more CPU cores, the server-based implementations
could achieve higher throughput, ideally, close to the NIC’s raw
performance (≈34 Mpps). Even compared to that case, TEA with
cache can still achieve ≈2.3× higher throughput with much lower
hardware cost since it does not involve the CPU.
Resource
Match Crossbar
SRAM
TCAM
VLIW Instruction
Hash Bits
Additional usage
12.6%
8.5%
0.4%
4.2%
6.3%
Table 4: Additional switch ASIC resources used by TEA.
Comparison with switch-based NFs. To understand the over-
head that TEA incurs, we compare the performance of a specific
NF, NAT, running on a programmable switch, when using TEA and
when using local SRAM tables (referred as baseline). The results
for other NFs are similar.
To measure latency, we replay both synthetic and real data center
packet traces [20] consisting of 64 B packets. Note that since the
real traces consist of varying sizes of packets, we make the payload
size of each packet be 64 B with the original headers (i.e., the flow
information is maintained). To measure the per-packet latency, we
record two timestamps when packets come into the switch and
leave the switch after the NAT processes the packet. Figure 11a
shows the CDF of the latency distribution. The baseline and uniform
represent the best and worst possible performance, respectively.
We see that the more skewed the flow size distribution is, the lower
the median latency is. Interestingly, we observe that the real traces
show a skewness even higher than α=0.99. In the traces, top 95
popular flows take more than 50% of total flows), so the cache can
serve more packets, lowering the median latency. Regardless of the
skewness, we see that the variance is small (no long tail), resulting
in the predictable latency.
To measure throughput, we replay real data center packet traces
at the rate which is higher than the original rate at which it was
captured. Since the packet sizes vary, we measure the throughput
in Gbps rather than Mpps. A single packet generator node can
replay the trace at 14.48 Gbps, thus the maximum transmission
rate we could achieve is around 57.92 Gbps with our four packet
generator nodes. Figure 11b shows the throughput of NAT with
varied transmission rates. We see that NAT with TEA can serve the
traffic at the incoming rate for all cases.
6.3 TEA ASIC Resource Usage
We evaluate how much ASIC resource is consumed only by TEA
based on the P4 compiler’s output. Note that as mentioned in §4.2.1,
the number of colliding entries in TEA-Table that are stored in the
SRAM is 0.1% of the total number of entries. Thus, the SRAM space
usage depends on the total number of inserted entries, and in this
evaluation, we insert 10 million entries. Table 4 shows the resource
consumption. We see that there are plenty of resources remaining
to implement other functionality on the ASIC along with TEA. It
consumes some amount of SRAM, TCAM, VLIW instruction, and
hash bits, all less than 9%. Match crossbar is the most consumed re-
source. We observe that count-min sketch, cache, stash, and lookup
response handler consume most of the match crossbar. Memory
address resolver and access requestor modules consume SRAM and
hash bits to store metadata for RDMA connections and resolve
bucket and server IDs.
024Latency(µs)0.00.51.0CDFBaselineUniformα=.90α=.95α=.99Real48163248Transmissionrate(Gbps)02040Throughput(Gbps)Baselinew/TEASIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Daehyeok Kim et al.
7 Discussion
Deployment locations. As a starting point, we focus on designing
TEA for ToR switches in NFV clusters. However, TEA can be de-
ployed in other locations. In data center racks, one can enable TEA
at ToR switches with compute servers. For that, we need to make
sure that there is unused DRAM space in servers and link band-
width. Moreover, our design can be extended to non-ToR switches
(e.g., aggregation-layer switches) in data centers, which do not have
directly connected servers under it. Since it requires multi-hop rout-
ing for lookup requests, we need to have a careful design that deals
with longer and (possibly) unpredictable lookup latencies and un-
reliability. For example, with RoCEv2 protocol [38], which runs
on top of IP/UDP and supports multi-hop routing, external DRAM
access requests from upper-level switches can be routed to servers.
Match types. In this paper, we mainly focus on exact-matching se-
mantics. Other NFs may require other lookup types such as longest-
prefix matching (LPM). Previous work emulates LPM using exact-
matching [65] or converts an LPM table into a large exact-match
table [45]. We can leverage such ideas to support other lookup types
in TEA.
Use cases. Although the current design of TEA-Table provides a
key-value based table abstraction, we can extend it to support other
use cases. For example, by adopting the FIFO queue abstraction,
TEA allows utilizing external DRAM as a large packet buffer which
can be useful for handling packet drops due to congestion.
Other programmable switch ASICs. While we use Tofino-based
programmable switches for our implementation, we believe our de-
sign can be implemented on other switch ASICs since hardware ca-
pabilities leveraged in TEA (i.e., packet manipulation, meter, packet
generation engine, etc.) are general features supported by most
switch ASICs available today.
TEA using on-board off-chip DRAM. As mentioned earlier, some
switch ASICs support on-board off-chip DRAM for specific pur-
poses such as packet buffers and select lookup tables [8]. As the traf-
fic demand increases, programmable switch ASIC vendors may also
consider to adopt such on-board DRAM. However, to use DRAM in
a flexible manner, they need to address the same practical challenges
as the ones described in this paper, including asynchronous and
low-latency DRAM access without stalling the packet processing
pipeline. Thus, we believe that our techniques designed for TEA can
be extended for such a future programmable switch architecture.
8 Related Work
Hardware-accelerated NFs. NF tasks have been accelerated using
programmable switch ASICs, FPGAs, or Smart NICs to outperform
CPU-only designs. Examples include offloading load balancers [53]
and network monitoring [5, 35, 56] to switches and IPSec gateway,
load balancer, and other NFs to FPGA-based smart NICs [31, 48].
TEA makes it possible to accelerate a wider range of NFs on pro-
grammable switches and support more operating scenarios by ad-
dressing the memory constraint issue.
Using external memory from switches. Prior work has sug-
gested system architectures that allow switches to utilize external
memory on servers [19, 44]. Such architectures run packet process-
ing logic on both a hardware switch and a software switch on the
servers and use servers’ memory (i.e., accessing lookup tables on
servers’ memory) by forwarding a subset of packets (i.e., offloading
traffic in certain conditions) to the software switch. This involves
CPUs, increasing both average and tail packet processing latencies.
In contrast, TEA purely uses DRAM on servers without involving
CPUs via RDMA while addressing practical challenges in using
multiple servers.
NFV state management. Previous work on state management for
stateful NFs in NFV utilizes the local or remote storage to manage
NF state [33, 41, 61, 66]. For example, statelessNF [41] allows NFs to
leverage a centralized storage to store and load states for NFs. Their
focus is better scaling and failure handling in the NFV context. In
contrast, TEA leverages external DRAM to enable state-heavy NFs
on programmable switches.
Other applications on programmable switches. Recent work
has shown that it can be useful to offload other applications or prim-
itives to programmable switches to enhance their performance. For
example, offloading the sequencer [49], key-value cache [40, 50],
and coordination service [39] improves the performance of dis-
tributed systems, in terms of throughput, scalability, and load bal-
ancing. Such systems also suffer due to switch memory constraints.
TEA-like techniques could help such applications as well.
Accessing remote memory via RDMA. RDMA has been used
in applications such as key-value stores [27, 42, 54], distributed
shared-memory [27], transactional systems [23, 28, 46], and dis-
tributed NVM systems [52, 63]. Our work demonstrates a novel
use of RDMA, which allows a programmable switch to leverage
external DRAM on such servers.
9 Conclusions
While emerging programmable switch ASIC designs make it possi-
ble for moving NFs from commodity servers to switches, the limited
memory on these ASICs has been a significant impediment in their
use for many NFs. To address this issue, we envision a new system
architecture, called TEA (Table Extension Architecture), for top-
of-rack switch ASICs in NFV clusters. TEA provides a performant
virtual table abstraction for NFs on programmable switches so that
they can make use of DRAM on servers connected to the switch in a
cost-efficient and scalable manner. Our evaluation with microbench-
marks and NF implementations shows that TEA can provide NFs
with low and predictable latency and scalable throughput for table
lookups without servers’ CPU involvement.
Ethics: This work does not raise any ethical issues.
Acknowledgments
We would like to thank the anonymous SIGCOMM reviewers and
our shepherd for their insightful comments and constructive feed-
back. This work was supported in part by the CONIX Research
Center, one of six centers in JUMP, a Semiconductor Research Cor-
poration (SRC) program sponsored by DARPA, and by NSF award
1700521. Daehyeok Kim was also supported by the Microsoft Re-
search PhD Fellowship.
TEA: Enabling State-Intensive Network Functions on Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
References
[1] 2010. Data Set for IMC 2010 Data Center Measurement. http://pages.cs.wisc.edu/
[2] 2011. 802.1Qbb – Priority-based Flow Control. https://1.ieee802.org/dcb/802-
[3] 2011. pktgen-dpdk: Traffic generator powered by DPDK. https://git.dpdk.org/
~tbenson/IMC10_Data.html.
1qbb/.
apps/pktgen-dpdk/.
[4] 2015.
Intel Xeon Processor E5-2640 v3. https://ark.intel.com/content/www/
us/en/ark/products/83359/intel-xeon-processor-e5-2640-v3-20m-cache-2-60-
ghz.html.
[5] 2018. Advanced Network Telemetry. https://www.barefootnetworks.com/use-
[6] 2018. Barefoot P4 Studio. https://www.barefootnetworks.com/products/brief-p4-
[7] 2018. Barefoot Tofino. https://www.barefootnetworks.com/products/brief-
cases/ad-telemetry/.
studio/.
tofino/.
[8] 2018.
BCM88690–10 Tb/s StrataDNX Jericho2 Ethernet Switch Se-
ries. https://www.broadcom.com/products/ethernet-connectivity/switching/
stratadnx/bcm88690.
[9] 2018. Cavium Xpliant Ethernet Switches. https://www.cavium.com/xpliant-
ethernet-switch-product-family.html.
[10] 2018. Perftest package. https://github.com/linux-rdma/perftest.
[11] 2019. Azure VPN Gateway.
https://docs.microsoft.com/en-us/azure/vpn-
gateway/vpn-gateway-about-vpngateways.
[12] 2019. Cisco Visual Networking Index. https://www.cisco.com/c/en/us/solutions/
collateral/service-provider/visual-networking-index-vni/white-paper-c11-
738429.html.
Compare Kemp LoadMaster, F5 Big-IP & Citrix Netscaler.
[13] 2019.