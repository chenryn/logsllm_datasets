be serialized with Cx and Cy. Cxy can be executed in parallel
with Cz however.
To beneﬁt from command inter-dependencies to parallelize
execution, some proposals add a deterministic scheduler (also
known as parallelizer) to the replicas [3]. The scheduler
delivers all the commands ordered through the agreement layer,
examines command dependencies, and distributes them among
a pool of worker threads for execution (see Figure 1 (d)). To
distribute the commands among threads, besides considering
dependencies, the scheduler can also balance the load among
threads. Threads that are less occupied can be given more
commands to execute if their execution does not conﬂict with
the commands that are being executed by other threads.
Although thanks to the scheduler the execution is par-
allelized,
the scheduler delivers and dispatches commands
sequentially, which restrains the overall performance from
scaling. For this reason, we identify these techniques as
Sequential Delivery-Parallel Execution (SDPE). Adapting a
sequential policy for delivery has its roots in the requirements
of SMR where replicas deliver one and only one stream of
ordered commands. Synchronization between the scheduler
and the worker threads for dispatching commands is yet
another performance overhead of this model.
E. Execute-Verify (EV)
One of the shortcomings of the SDPE model
is the
agreement layer, where only one stream of ordered requests
is generated. Eve addresses this issue by ﬁrst executing the
requests on replicas and then verifying the correctness of the
states through a veriﬁcation stage, hence named as Execute-
Verify (EV) (see Figure 1 (e)). Eve distinguishes one of the
replicas as the primary to which clients send their requests.
The primary replica organizes the requests into batches and
assigns to each batch a unique sequence number. The primary
then transmits the batched requests to the other replicas.
All the replicas, including the primary, are equipped with a
deterministic mixer. Using the application semantics, the mixer
converts a batch of requests to a set of parallel batches such
that all the requests in a parallel batch can be executed in
parallel. Once the execution of a parallel batch terminates,
replicas calculate a token based on their current state and
send their token to the veriﬁcation stage. The veriﬁcation
stage investigates the equality of the tokens. If the tokens are
equal, replicas commit the requests and respond to the clients.
Otherwise, replicas must roll back the execution and re-execute
the requests in the order determined by the primary as it was
batching the requests. The veriﬁcation stage also adds to Eve
the advantage of detecting concurrency bugs.
Similar to the scheduler in the SDPE model, the mixer in
Eve may restrict the execution performance since the content
of all the requests must be scrutinized by the mixer before
they can be executed. Moreover, the primary replica might
be overwhelmed by the amount of requests it receives. The
veriﬁcation stage is another synchronization point that besides
the mixer and the primary replica can threaten the scalability
of this approach.
F. Parallel Delivery-Parallel Execution (PEPD)
Motivated by the shortcomings of the previous models, P-
SMR proposes to parallelize command delivery in addition to
command execution [5]; hereafter we refer to this model as
Parallel Delivery-Parallel Execution (PDPE). P-SMR has no
scheduler and several threads on replicas concurrently deliver
and execute multiple disjoint streams of ordered commands.
To preserve correctness, commands in each stream must be
independent from the commands in any other stream. To ensure
independency among the concurrently delivered streams, un-
Agreement and VeriﬁcationAgreement and VeriﬁcationProxyProxyProxyProxyAgreementAgreementAgreementAgreementAgreementAgreementAgreementAgreementProxyAgreementSchedulerReplicaApplicationAgreementReplicaService ExecutionReplicaProxy(b) Sequential SMR(d) SDPE(f) PDPEAgreementService ExecutionService ExecutionServer(a) non-replicatedApplicationProxyApplicationProxyApplicationProxyProxyProxyProxyProxyService ExecutionReplica(c) Pipelined SMRAgreementService ExecutionApplicationProxyProxyApplicationAgreement and VeriﬁcationProxy(e) EVMixerReplicaService ExecutionProxyRequestResponseClientClientClientClientClientClientSingle coordination point
Scalability
Order on commands
Load balancing
Application semantics
Dependency tracking
Execution strategy
Rollback
Yes
None
Total
None
No
No
No
Conservative
Conservative
Sequential SMR Pipelined SMR
SDPE
Yes
Limited
Total
Yes
Yes
EV
Yes
Limited
Total
Yes
Yes
Server-side
Server-side
Conservative Optimistic
No
Yes
P-SMR
PDPE
opt-PSMR
No
Unlimited
Partial
Approximative
Yes
Client-side
Conservative
No
Optimistic
Possibly
Yes
Limited
Total
None
No
No
No
TABLE I.
A COMPARISON AMONG APPROACHES TO PARALLELIZING STATE-MACHINE REPLICATION.
like previous approaches in which command dependencies are
determined at the replicas, in P-SMR command dependencies
are determined by the clients, before commands are ordered.
Commands in P-SMR are ordered by an atomic multicast li-
brary and clients multicast independent commands to different
multicast groups. P-SMR implements a fully parallel model
in which independent commands are ordered, delivered, and
executed in parallel. Dependent commands are ordered through
dedicated multicast groups and executed sequentially, as we
explain next (see Figure 1 (f)).
In P-SMR clients submit commands to the client proxies,
which determine the destination groups of commands based
on command dependencies. To guarantee concurrent execution
of independent commands, client proxies assign independent
commands to different multicast groups and to guarantee
sequential execution of dependent commands, client proxies
assign at least one common group to every two dependent
commands. The amount of concurrency in a service depends
on the interdependencies among the service’s commands. P-
SMR organizes server threads into K multicast groups such
that the i-th thread of each replica, ti, belongs to group gi. A
thread ti executes the commands broadcast to gi concurrent
to thread tj who executes the commands broadcast to gj. The
two threads, however, must synchronize their execution if a
command is multicast to both gi and gj. To synchronize, one
of the threads, chosen deterministically among the two threads,
ti, waits for a notiﬁcation from the other thread, tj. tj notiﬁes
ti and waits for a notiﬁcation from ti to resume its execution.
Once ti executes the common command it notiﬁes thread tj.
G. Summary
Table I shows the main differences among the techniques
we have discussed in this section. The table also contains opt-
PSMR, the approach we introduce in Section IV.
Both SDPE and EV have centralized entities that can limit
scalability: the scheduler and the agreement layer in SPDE;
the mixer, the primary replica, and the veriﬁcation layer in EV.
PDPE does not include central roles in its design. Moreover,
differently from other approaches, PDPE orders requests using
an atomic multicast, as opposed to an atomic broadcast.
The parallelizer in SDPE and the mixer in EV also perform
load balancing on the server side. Although in a limited
way, clients in PDPE can try to distribute the load evenly
among server threads (e.g., by multicasting read commands
to different groups).
SPDE, EV, and PDPE rely on tracking command depen-
dencies to parallelize execution on replicas. In SDPE and
EV, command dependencies are checked on the server side.
In PDPE, however, it is the clients that track dependencies
and submit commands to the appropriate multicast groups.
Determining command dependencies in P-SMR is conservative
and can lead to false positives. In opt-PSMR, commands are
handled optimistically by the client proxies and on the server
side appropriate actions are taken to avoid inconsistencies.
Due to their optimistic nature, EV and opt-PSMR may be
subject to rollbacks. In opt-PSMR, however, depending on the
application, execution rollbacks might not be necessary as we
show in the next section.
IV. OPTIMISTIC P-SMR (OPT-PSMR)
In this section, we motivate the need for opt-SMR, describe
the novel technique in detail, and argue about its correctness.
A. Motivation for opt-PSMR
Consider a B+-tree service that stores key-value entries
where keys are integers and values are strings, and the
following operations are supported: read(in: int k,
out: char[] v),
update(in: int k, char[]
v),
insert(in: int k,
char[] v), where k is a key, v is a value, and in and
out specify the input and output parameters of a command
respectively.
delete(in: int k),
P-SMR assumes a conﬁgurable multiprogramming level
(i.e., the number of threads at each replica). Assume there are
K threads per replica and the same number of groups so that
the i-th thread at each replica is part of the i-th group. Clients
(actually client proxies) map commands to groups using the
following strategy. Commands to read and update key k are
mapped onto a single group g (e.g., using range partitioning,
g = ((cid:98) kK/M (cid:99)) + 1, where M is the value of the largest key
in the key space).
As a consequence, commands assigned to different groups
can execute concurrently on replicas, even if they all are
update operations. Commands on the same key are multicast
to the same group g and executed sequentially by the thread
associated with g. Unlike read and update operations,
insert and delete may cause structural changes in the
tree (i.e., splits and merges). Structural changes can spread to
many nodes of the tree and interfere with the execution of
other operations. Since it is impossible for a client to predict
these changes, to preserve correctness, clients conservatively
assume that insert and delete are dependent on all the
other operations and multicasts them to all the groups. All
threads in a replica deliver insert and delete commands.
Upon delivering such a command, threads coordinate so that
a single thread executes the command.
Fig. 2.
Throughput of P-SMR and SMR with a workload composed of
dependent and independent commands; for details of the experiment see
Section V.
The sequential mode of a multithreaded replica in P-SMR
is more expensive than the (sequential) execution mode of
a single-threaded replica in traditional SMR. To switch to
sequential mode, all the threads in P-SMR communicate and
pause their execution so that only one of the threads executes
dependent commands. In SMR, the single-threaded replica
simply delivers and executes a stream of commands, with-
out need of any synchronization operations. Synchronization
among threads has a negative impact on P-SMR’s performance.
Sequential execution of dependent commands is a must for
preserving consistency and thus, the performance overhead
incurred by the replica’s sequential execution mode is inherent
to P-SMR.
Figure 2 compares the performance of P-SMR and SMR
for the B+-tree example. As the percentage of insert
and delete operations in the workload increases (x-axis),
the throughput of P-SMR reduces and gradually falls below
the performance of SMR (for details of the experiment see
Section V). With 100% of dependent commands, threads in
P-SMR must coordinate for every delivered operation. The
difference in performance between SMR and P-SMR with
100% of dependent operations can be understood as the cost
of synchronizing threads. The break-even point of SMR and
P-SMR is when slightly fewer than half of commands are
dependent. With few dependent commands in the workload, P-
SMR largely outperforms SMR, since the execution is mostly
concurrent.
In light of the tradeoff shown in Figure 2, service designers
should strive to reduce command interdependencies (e.g.,
avoiding false sharing in the service data structures). Although
interdependencies cannot be always avoided, what really limits
P-SMR’s performance is that clients cannot accurately tell
when interdependencies happen since they do not have the
service’s state. Therefore, clients conservatively identify some
commands as dependent to prevent potential inconsistencies
that can arise due to their concurrent execution at the replicas.
In the B+-tree example, insert and delete operations are
among such commands. Clients multicast these commands to
all the groups and therefore all the threads on the replicas
deliver them and collaboratively enter the sequential mode.
Not all the insert and delete operations, however, result
in changes in the tree’s structure. This subset of commands
that are categorized as dependent but could be executed in