Leibler (KL) divergence between p(Œ∏|Dtr ain) and q(œâ). In the test
phase, Eq.(1) is degraded by averaging T(T > 0) models {Œ∏ j}T
j=1
(each of which is sampled from q(œâ)), namely:
Wi = Mi ¬∑ Vi with Vi ‚àº Bernoulli(ri),
p(y = 1|z) =
1
T
p(y = 1|z, Œ∏i).
(3)
Eq.3 says p(y = 1|z) is obtained just by keeping the dropout
switched and averaging the results of T times predictions.
Variational Bayesian Inference (VBI) [39]: It is also an approxi-
mate Bayesian method. Distinguishing from MC dropout, the pa-
rameters Œ∏ of neural network are directly sampled from a known
form distribution q(œâ) with variables œâ. That is
T
j=1
W ‚àº q(œâ) with W ‚àà Œ∏
The training and test manner is the same as MC dropout.
Deep Ensemble [25]: It learns T independent neural networks,
which are diversified by randomly initialized parameters. The intu-
ition behind this idea is that dropout itself is an ensemble [40].
Weighted Deep Ensemble (wEnsemble): It has the same setting
as Deep Ensemble, except for the weighted voting
T
p(y|z) =
with wi ‚â• 0 andT
i =1
i =1 wi = 1.
wip(y = 1|z, Œ∏i)
(4)
(5)
Figure 1: Our methodology for calibrating deep malware
detectors to answer the aforementioned RQ1-RQ4, where
dashed boxes indicate that the modules may not be neces-
sary for some calibration methods. In the training phase, we
learn the calibrated malware detectors. In the test phase, pre-
dicted probabilities (with weights if applicable) are obtained.
deep learning models in the training and testing phases according
to the incorporated calibration method. In this module, we load
basic block models sequentially in the training and test phases for
relieving the memory complexity. The model post-processing module
copes with the requirements of post-hoc calibration methods. The
input to this module is the detector‚Äôs output (i.e., predicted prob-
ability that a file belongs to which class) and a validation dataset
(which is a fraction of data sampled from the same distribution as
the training set). This means that this module does not affect the
parameters Œ∏ of neural networks.
3.4 Selecting Metrics
In order to quantify the predictive uncertainty of a calibrated de-
tector, we need to use a test dataset, denoted by Dtest , and some
metrics. In order to make the methodology widely applicable, we
consider two scenarios: the ground-truth labels of testing dataset
are available vs. are not available. It is important to make this dis-
tinction is important because ground-truth labels, when available,
can be used to validate the trustworthiness of detectors‚Äô predictive
uncertainties. However, ground-truth labels are often hard to costly
to obtain, which motivates us to accommodate this more realis-
tic scenario. This is relevant because even if the detector trainer
may have access to ground-truth labels when learning detectors,
these ground-truth labels may not be available to those that aim to
quantify the detectors‚Äô uncertainties in making predictions.
Selecting Metrics When Ground-Truth Labels Are Given. There
3.4.1
are standard metrics that can be applied for this purpose. However,
these metrics treat each example as equally important and are in-
sensitive to data imbalance, which is often encountered in malware
detection. This prompts us to propose variants of standard metrics
to deal with the issue of data imbalance. Specifically, we use the
following three predictive uncertainty metrics, which are applicable
when ground-truth labels are known.
The first standard metric is known as Negative Log-Likelihood
(NLL), which are commonly used as a loss function for training
a model and intuitively measure the goodness of a model fitting
the dataset [9]. A smaller NLL value means a better calibration.
3.3 Calibrating Detectors
Figure 1 highlights our methodology in calibrating deep malware
detectors for answering the research questions mentioned above
(i.e., RQ1-RQ4). Each calibration method is calibrated into each of
the select detectors. The methodology has a training phase and a
testing phase. Each phase has five modules: preprocessing, layers
customization, deep neural network, ensemble wrapper, and model
post-processing, which are described below.
The preprocessing module transforms program files into the data
formats that can be processed by deep learning models, including
feature extraction and low-level feature representation. The layers
customization module modifies the layers of standard deep learn-
ing models to incorporate appropriate calibration methods, such
as placing a dropout layer before the input of the fully connected
layer, the convolutional layer, or the LSTM layer; sampling param-
eters from learnable distributions. The deep neural network module
constructs the deep learning models with the customized layers
mentioned above to process the preprocessed data for training or
testing purposes. The ensemble wrapper module uses an ensemble of
Malicious/Benign files  ùëßùëñ,ùë¶ùëñ  ùëñ=1ùëõ LayersCustomizationDeep Neural Network Ensemble Wrapper‚àë Probabilities(w/ weights)?NewfilePredictingùëß ùë§1 ùë§ùëá  PreprocessingModel Post-processing               TrainingTesting599Can We Leverage Predictive Uncertainty to Detect Dataset Shift and Adversarial Examples in Android Malware Detection?ACSAC ‚Äô21, December 6‚Äì10, 2021, Virtual Event, USA
Formally, this metric is defined as:
E(z,y)‚ààDt est ‚àí (y log p(y = 1|z, Œ∏) + (1 ‚àí y) log(1 ‚àí p(y = 1|z, Œ∏))) .
In order to deal with imbalanced data, we propose using the follow-
ing variant, dubbed Balanced NLL (bNLL), which is formally defined
test) is the expectation of
test), where NLL(Di
as:
of class i, i ‚àà {0, 1}.
negative log-likelihood on the test set Di
bNLL treats each class equally important, while NLL treats each
sample equally.
|Y||Y|
i =0 NLL(Di
test
1
i =0 Brier(Di
|Y||Y|
The second standard metric is known as Brier Score Error (BSE),
which measures the accuracy of probabilistic predictions [7, 39].
A smaller BSE value means a better calibration. This metric is
formally defined as: E(z,y)‚ààDt est (y ‚àí p(y = 1|z, Œ∏))2. In order to
deal with imbalanced data, we propose using the following vari-
ant, dubbed balanced BSE (bBSE), which is formally defined as:
test) is the expectation of
1
BSE on the test set Di
The third standard metric is known as Expected Calibration Error
(ECE), which also measures accuracy of predicted probabilities yet
in a fine-grained manner [31]. A smaller ECE value means a better
calibration. Formally, this metric is defined as follows. Given S
buckets corresponding to quantiles of the probabilities {œÅi}S
i =1, the
ECE is defined as
test), where Brier(Di
of class i, i ‚àà {0, 1}.
test
ECE =
| Pr(y = 1|Bs) ‚àí conf(Bs)|,
Bs
N
(6)
where with a little abuse of notation, Bs = {(z, y) ‚àà Dtest
:
p(y = 1|z, Œ∏) ‚àà (œÅs , œÅs +1]} is the number of examples in bucket
s, Pr(y = 1|Bs) = |Bs|‚àí1(z,y)‚ààBs[y = 1] is the fraction of ma-
licious examples, conf(Bs) = |Bs|‚àí1(z,y)‚ààBs p(y = 1|z, Œ∏), and
N = |Dtest |. The ECE metric suffers from imbalanced data be-
cause the majority class dominates some bins owing to the weights
Bs/N . In order to deal with imbalanced data, we propose using
the following variant, dubbed Unweighted ECE (uECE), which is
formally defined as follows.
uECE =
1
S
| Pr(y = 1|Bs) ‚àí conf(Bs)|.
S
s =1
S
s =1
3.4.2
Selecting Metrics When Ground-Truth Labels Are Not Given.
There are three metrics [21, 25, 34] that can be applied to quantify
predictive uncertainty in the absence of ground-truth labels. These
metrics are applied to a point (i.e., expectation is not considered),
which indicates these metrics do not suffer from the imbalanced
dataset.
The first metric is known as Entropy, which intuitively measure
a state of disorder in a physical system [21]. A larger entropy value
means a higher uncertainty; Formally, this metric is defined as:
‚àí(p(y|z, Œ∏) log p(y|z, Œ∏) + (1 ‚àí p(y|z, Œ∏)) log(1 ‚àí p(y|z, Œ∏))),
(7)
where p(y|z, Œ∏) denotes the model output p(y = 1|z, Œ∏) or Eq.(3).
The second metric is known as Standard Deviation (SD), which
intuitively measures the inconsistency between the base classi-
fiers and the ensemble one [34]. A larger SD value means a higher
uncertainty. Formally, this metric is defined as:
(cid:118)(cid:117)(cid:116)
T
i =1
T
T ‚àí 1
wi
(cid:0)p(y|z, Œ∏i) ‚àí p(y|z, Œ∏)(cid:1)2
,
where Œ∏i denotes the i-th DNN model, which has a weight wi (ref.
Eq.(5)) or wi = 1/T .
The third metric is known as the KL divergence, which is an
alternative to SD [25]. A larger KL value means a higher uncertainty.
Formally, this metric is defined as:
wi(KL(p(y|z, Œ∏i)||p(y|z, Œ∏))),
where KL denotes the Kullback-Leibler divergence.
i =1
T
3.5 Answering RQs
At this point, one can apply the calibrated detectors to quantify their
predictive uncertainties. It should be clear that this methodology
can be adopted or adapted by other researchers to conduct empirical
studies with more kinds of detectors and more metrics.
4 EXPERIMENTAL RESULTS AND ANALYSIS
4.1 Experimental setup
We implement the framework using TensorFlow [2] and TensorFlow
Probability libraries [1] and run experiments on a CUDA-enabled
GTX 2080 Ti GPU. We below detail datasets and hyperparameters.
4.1.1 Datasets. We use 3 widely-used Android datasets: Drebin
[4], VirusShare [11], and Androzoo [3].
Drebin: The Drebin dataset is built before the year of 2013 and is
prepossessed by a recent study [27], which relabels the APKs using
the VirusTotal service [38] that contains more than 70 antivirus
scanners. An APK is treated as malicious if four or more scanners
say it is malicious, and benign if no scanners say it is malicious;
theoretical justification of such heuristic but widely-used practice
has yet to be made [14]. The resultant Drebin dataset contains 5,560
malicious APKs and 42,333 benign APKs.
VirusShare: VirusShare is a repository of potential malware. We
chose the APKs collected in 2013 and treat this dataset as out-of-
source in regards to the Drebin dataset. These APKs are labeled
using the same fashion as the Drebin dataset, producing 12,383
malicious APKs and 340 benign APKs, while throwing away the
Figure 2: The Androzoo examples of APKs with time [35].
2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-122015-012015-022015-032015-042015-052015-062015-072015-082015-092015-102015-112015-122016-012016-022016-032016-042016-052016-062016-072016-082016-092016-102016-112016-120200040006000800010000# of Examples34403067348541015081606687454788417663451305144415641231188818381953209825212768307729323010328934783959581863684052280630274881137775819067383263381485608724107249939959114514714412921518322121627527235431337436934447663561245227835256414881238BenwareMalware600ACSAC ‚Äô21, December 6‚Äì10, 2021, Virtual Event, USA
D. Li, T. Qiu, S. Chen, Q. Li, and S. Xu
469 APKs in the grey area (i.e., at least one, but at most three,
scanners say they are malicious).
Androzoo: Androzoo is an APK repository, including over 10 mil-
lion examples, along with their VirusTotal reports. These APKs were
crawled from the known markets (e.g., Google Play, AppChina).
Following a previous study [35], we use a subset of these APKs
spanning from January 2014 to December 2016, which includes
12,735 malicious examples and 116,993 benign examples. Figure 2
plots the monthly distribution of these examples with time [35].
4.1.2 Hyper-parameters. We present the hyper-parameters for mal-
ware detectors and then for calibration methods.
DeepDrebin [17] is an MLP, consisting of two fully-connected
hidden layers, each with 200 neurons.
MultimodalNN [22] has five headers and an integrated part, where
each header consists of two fully-connected layers of 500 neurons.
The integrated part consists of two fully-connected layers of 200
neurons.
DeepDroid [30] has an embedding layer, followed by a convolu-
tional layer and two fully-connected layers. The vocabulary of the
embedding layer has 256 words, which correspond to 256 Dalvik
opcodes; the embedding dimension is 8; the convolutional layer has
the kernels of size 8√ó8 with 64 kernels; the fully-connected layer
has 200 neurons. Limited by the GPU memory, DeepDroid can only
deal with a maximum sequence of length 700,000, meaning that
APKs with longer opcode sequences are truncated and APKs with
shorter opcode sequences are padded with 0‚Äôs (which correspond
to nop).
Droidetec [29] has an embedding layer with vocabulary size 100,000
and embedding dimension 8, the bi-directional LSTM layer with 64
units, and a fully-connected hidden layer with 200 neurons. Droide-
tec allows the maximum length of API sequence 100,000. We further
clip the gradient values into the range of [-100, 100] for Droidetec
in case of gradient explosion.
All models mentioned above use the ReLU activation function.
We also place a dropout layer with a dropout rate 0.4 before the
last fully-connected layer (i.e., the output layer). We implement
the four malware detectors by ourselves. The hyperparameters of
calibration methods are detailed below.
Vanilla and Temp scaling: We have the same settings as the mal-
ware detectors mentioned above.
MC dropout: We use a dropout layer with a dropout rate 0.4. We
add the dropout layer into the fully-connected layer, convolutional
layer, and the LSTM layer, respectively. Following a recent study
[39], we neglect the ‚Ñì2 regularization that could decline the detec-
tion accuracy. In the test phase, we sample 10 predictions for each
example.
VBI: We sample the parameters of the fully-connected layer or
the convolutional layer (i.e., weights and bias) from Gaussian dis-
tributions. A Gaussian distribution has the variables (mean and
standard deviation), which are learned via back propagation using
the reparameterization technique [6]. We do not implement VBI for
Bi-LSTM, due to the effectiveness issue [39, 44]. This means only
the last layer of Droidetec is calibrated by VBI. In the test phase,
we sample 10 predictions for each example.
Ensemble and wEnsemble: We learn 10 base instances for each
ensemble-based method.
We learn these models using the Adam optimizer with 30 epochs,
batch size 16, and learning rate 0.001. A model is selected for evalu-
ation when it achieves the highest accuracy on the validation set in
the training phase. In addition, we calculate the validation accuracy
at the end of each epoch.
4.2 Answering RQ1
In order to quantify the predictive uncertainty of malware detectors
in the absence of dataset shift, we learn the aforementioned 24
malware detectors on the Drebin dataset, by splitting it into three
disjoint sets of 60% for training, 20% for validation, and 20% for
testing.
Table 1 summarizes the results, including detection estimation us-
ing the metrics False Negative Rate (FNR), False Positive Rate (FPR),
Accuracy (Acc) or percentage of detecting benign and malicious
samples correctly, balanced Accuracy (bAcc) [8] and F1 score [36],
and uncertainty evaluation using the metrics NLL, bNLL, BSE, bBSE,
ECE, and uECE. We make four observations. First, Temp scaling
achieves the same detection accuracy as its vanilla model because it
is a post-processing method without changing the learned parame-
ters. On the other hand, MC dropout, Ensemble and wEnsemble im-
prove detection accuracy but VBI degrades detection accuracy some-
what when compared with the vanilla model. In terms of balanced
accuracy, the calibration methods do not always improve detection
accuracy because the vanilla MultimodalNN actually achieves the
highest balanced accuracy 98.61% among all those detectors. The
reason may be that MultimodalNN itself is an ensemble model (e.g.,
5 headers are equipped).
Second, ensemble methods (i.e., Ensemble and wEnsemble) re-
duce the calibration error when compared with the respective
vanilla models. Moreover, the two ensemble methods exceed the
other calibration methods in terms of the NLL, BSE and bBSE met-
rics, except for DeepDrebin (suggesting MC Dropout is best for
calibration). Third, the measurements of the balanced metrics are
notably larger than their imbalanced counterparts (e.g., bNLL vs.
NLL), because benign examples dominate the test set and malware
detectors predict benign examples more accurately than predicting
malicious ones.
Fourth, uECE shows inconsistent results in terms of bNLL and
bBSE. In order to understand the reasons, we plot the reliability dia-
gram [33], which demonstrates the difference between the fraction
of malicious examples in each bin, namely the difference between
the Pr(y = 1|Bs) in Eq.(6) and the mean of the predicted confi-
dence con f (Bs). Figure 3 plots the results of the vanilla malware
detectors, along with the number of examples in the bins. Figure 3a
shows that most examples belong to bins B1 and B5. Figure 3b says
DeepDroid achieves the lowest error (because it is closest to the
diagonal than others), and shall be best calibrated, which contracts
the ECE values in Table 1 (demonstrating that MultimodalNN is
best instead). This is because as shown in Figure 3a, most benign
examples belong to bin B1 and most malicious examples belong to
bin B5. As a comparison, uECE does not suffer from this issue.
Insight 1. Calibration methods reduce malware detection uncer-
tainty; variational Bayesian inference degrades detection accuracy
and F1 score in the absence of dataset shift; balanced metrics (i.e.,
601Can We Leverage Predictive Uncertainty to Detect Dataset Shift and Adversarial Examples in Android Malware Detection?ACSAC ‚Äô21, December 6‚Äì10, 2021, Virtual Event, USA
Table 1: Detection estimation and uncertainty evaluation of calibrated malware detectors in the absence of dataset shift.
Malware
detector
Calibration
method
DeepDrebin
MultimodalNN
DeepDroid
Droidetec
Vanilla
Temp scaling
MC Dropout