# User-Level Data Center Tomography

**Authors:**
- Neil Alexander Twigg, University of Stirling
- Marwan Fayed, University of Stirling
- Colin Perkins, University of Glasgow
- Dimitrios P. Pezaros, University of Glasgow
- Posco Tso, University of Glasgow

## Abstract
Measurement and inference in data centers present unique opportunities and challenges compared to the Internet domain. Existing tools may be affected or misled by virtualization issues. Despite facing similar scalability challenges, data centers are relatively homogeneous and symmetric, which can be advantageous for network measurement. To better evaluate our hypotheses, we have developed a unified framework for conducting various tests. Our observations support recent claims but also highlight changes in the network and additional obfuscations due to virtualization.

## Categories and Subject Descriptors
- C.2.3 [Computer-Communication Networks]: Network Operations
- C.4 [Performance of Systems]: Measurement Techniques

## Keywords
- Data Centers, Network Measurement, Tomography

## 1. Introduction
Public understanding of data center network performance and behavior is limited. A deeper understanding can facilitate research in network management, planning, protocol design, and anomaly detection. While data center operators have direct access to network equipment and statistics, the results of their measurements are often kept confidential to maintain competitive advantage. This confidentiality makes it challenging to reproduce experiments and improve network-level performance and security.

In this poster, we present our efforts to provide a client-level perspective on data center networks. We aim to identify the type and resolution of data available to paying customers and the inferences that can be made from this data. From a user's perspective, the only relevant service level is the one they experience. For example, metrics like bisection bandwidth, which describe network-wide characteristics, may not be directly observable by individual users.

## 2. Framework
To address these gaps, we have constructed a flexible framework that allows for dynamic selection and execution of measurements. The framework was built and tested on Amazon's AMI Linux for EC2 using standard C libraries, making it easily portable to other platforms.

When launched in the data center, clients first retrieve and build the latest source code from a Git repository. They then communicate with an off-site server to fetch the list of expected measurements and learn the private IP addresses of other clients as they come online. This centralized distribution point enables quick creation, modification, and distribution of experiments.

Current modules include ping- and traceroute-like measurements. To minimize timing issues and embed relevant tags, these modules use custom ICMP headers and raw sockets. We validate the measurements taken using this framework by comparing them with previous work [3] and are in discussions with Amazon to determine accuracy. Future work will focus on bottleneck bandwidth estimation and understanding the relationship between logical and physical paths.

## 3. Initial Results
Our initial tests were designed to address two main concerns:
1. The impact of layer-2 routing on the opacity of the IP layer.
2. The reliability of fast round-trip times given the granularity of the operating system.

### Network Maps
From the gathered data, we constructed topology maps. Figure 1 shows a subset of a complete map, depicting the view from a root VM to 19 other VMs in the network. These detailed maps suggest that there is sufficient IP-level detail despite layer-2 routing. One interesting observation is that the host machine never responds to expired TTL. We also inferred that each physical machine is assigned a /24 network address by comparing VM and gateway addresses.

### Round Trip Times
Our observations confirm the high variability seen in [3] but differ in other respects. In [3], the first few RTT measurements in a new set of pings between a pair of nodes were abnormally high. After removing these, all RTTs were less than 200Âµs. We observed no such initial behavior and an increase in RTTs. Figures 2a to 2c summarize our findings, separating RTTs less than 1ms (Figures 2a and 2b) from those greater than 1ms. The ratio of packets in these categories is approximately 5:1. RTTs greater than 1ms appear to be fairly constant and persistent over several seconds. We also observed negative RTT values, likely due to different timing on different cores.

### Path Length
The distribution of path lengths differs from [3], which observed all path lengths to be either 3 or 4 hops. Figure 2d shows that most paths are now 2, 4, or 5 hops long, with some spanning up to 8 hops. Our observations suggest that while the means of the underlying distributions are similar, the variances are quite different, indicating changes in the network and service levels.

## 4. References
[1] http://d253108.cs.stir.ac.uk:89  
[2] P. Gill, N. Jain, and N. Nagappan. Understanding network failures in data centers: Measurement, analysis, and implications. In Proceedings of ACM SIGCOMM, August 2011.  
[3] G. Wang and T. E. Ng. The impact of virtualization on network performance of Amazon EC2 data center. In Proceedings of the IEEE INFOCOM, March 2010.