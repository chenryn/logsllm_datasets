all classiﬁed as anomalies compared to the normal data from
other sensor nodes in the WSN.
Zhang et al. [21] classify anomalies based on the cause of
the anomaly on a local node.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.4
IEEE COMMUNICATIONS SURVEYS & TUTORIALS, ACCEPTED FOR PUBLICATION
t
n
e
m
e
r
u
s
a
e
M
r
o
s
n
e
S
1
0.8
0.6
0.4
0.2
0
0
First
Second
0.8
Type 1
Type 2
1
Type 3
Type 4
Third
Sensor Node 1
Sensor Node 2
Sensor Node 3
20
40
60
80
100
Time
t
n
e
m
e
r
u
s
a
e
M
r
o
s
n
e
S
0.6
0.4
0.2
0
0
Normal Measurement
Range
20
40
60
80
100
Time
(a) First, Second and Third Order Anomalies [12], [20]
(b) Type 1, 2, 3 and 4 Anomalies [21]
t
n
e
m
e
r
u
s
a
e
M
r
o
s
n
e
S
1
0.8
0.6
0.4
0.2
0
0
Point Anomaly
Collective
 Anomalies
Normal Measurement
Range      
Contextual
 Anomaly
t1
20
t2
t3
t4
40
60
80
100
Time
(c) Point, Contextual and Collective Anomalies [10]
Fig. 1. Different Deﬁnitions of Anomalies in WSN data sets.
(cid:129) Type 1: Incidental absolute errors: A short-term ex-
tremely high anomalous measurement
(cid:129) Type 2: Clustered absolute errors: A continuous sequence
of type 1 errors
(cid:129) Type 3: Random errors: Short-term observations not lying
within the normal threshold of observations
(cid:129) Type 4: Long term errors: A continuous sequence of type
3 errors
Fig. 1b displays the anomalies deﬁned by Zhang et al..
At time period 20 a type 1 anomaly occurs as this data
instance differs signiﬁcantly from the normal data, but lies
within the observation range. From time period 40 to 45
an extended burst of type 1 anomalies occurs. These are
termed type 2 anomalies. At time period 60, a measurement
occurs that signiﬁcantly differs from the normal data and is
outside the observation range. This is termed a type 3 anomaly.
Finally, from time period 80 to 85 an extended burst of type
3 anomalies occurs. These are termed type 4 anomalies.
In the area of security in WSNs three anomaly types,
namely point, contextual or collective anomalies [10], are used
to compare techniques [15].
(cid:129) Point anomaly: An individual data instance that is con-
sidered anomalous with respect to the data set.
(cid:129) Contextual anomaly: A data instance that is considered
an anomaly in the current context. In a different context
the same data instance might be considered normal.
(cid:129) Collective anomalies: A collection of related anomalies.
Fig. 1c displays the anomalies deﬁned by Chandola et
al. [10]. A point anomaly occurs at time period 24 where
the data instance is anomalous with respect to the entire
data set. At
time period 43 a contextual anomaly occurs
which is anomalous at this time, but would not be considered
anomalous had it occurred at time t1, t2, t3 or t4. Finally, col-
lective anomalies occur in the time period 54 – 71. Collective
anomalies are a set of data instances that exhibit a pattern,
however, they are anomalous with regard to the entire data
set.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.O’REILLY et al.: ANOMALY DETECTION IN WIRELESS SENSOR NETWORKS IN A NON-STATIONARY ENVIRONMENT
5
The three deﬁnitions of anomalies in WSNs provide an
insight into the characteristics of anomalies that can occur
in data sets. However, these deﬁnitions are not comprehensive
for all anomalies that may occur in WSNs.
III. NON-STATIONARY DATA DISTRIBUTION
In this section the terms stationary and non-stationary are
deﬁned. Assumptions made in machine learning and pattern
recognition techniques are examined and the effect that a
non-stationary data distribution has on these assumptions is
discussed. In addition, the effect that a non-stationary data
distribution has on anomaly detection in WSNs is explored.
The section concludes with examples of non-stationary data
sets from real-world WSN deployments.
A. Machine Learning and Non-Stationary Data Sets
A fundamental assumption of standard machine learning
and pattern recognition theories is that the data used in a train-
ing set are drawn from a stationary data distribution, and the
testing set will also be drawn from the same distribution [28].
Thus it is assumed that Ptrain(x) = Ptest(x) [29]. This is
often unrealistic in real-world environments [28]. A change in
the data distribution can cause a model trained with data from
a previous distribution to become suboptimal for the current
distribution. Application domains such as network monitoring,
economic and ﬁnancial data analysis generate data that are
changing in its distribution as time progresses [30]. Changes
can occur for several reasons, including changes in the funda-
mental natural process which generates the observation.
Anomaly detection in data sets has been widely examined in
the machine learning community. The main focus of attention
in WSNs has been on stationary data sets where the data
distribution is assumed to be constant over time. Algorithms
either ignore non-stationary distributions or assume that a
periodic retraining will account for change, for example [20],
[31]–[33]. Due to the assumption that a training data set and a
testing data set are drawn from a stationary data distribution, if
the data distribution alters between the drawing of the training
and testing data set, the model will not be correct for the
testing data set. This will lead to a degradation in performance
of the anomaly detector. O’Reilly et al. [34] studied a data set
in which the anomaly rate varied and showed that if the model
does not adapt to the varying rate, performance degrades.
Two approaches to the problem of anomaly detection in
a non-stationary environment can be deﬁned. One method is
to monitor the data distribution, if a change is detected the
model is retrained. Another approach is to make an assumption
that the training and test inputs have different probability
distributions, but that the conditional distribution of the output
values given the input values is not altered. This is known
as covariate shift adaptation [35]. In this survey we focus
on the former technique, the identiﬁcation of change in the
data distribution and effective and efﬁcient adaptation to the
change.
B. Stationary and Non-Stationary Processes
Alterations in the underlying phenomenon that is being ob-
served can cause changes to the data that are being generated
by the sensor nodes in a WSN. Kelly et al. [36] identiﬁed
three ways in which a non-stationary distribution may exhibit
change through the use of Bayes Theorem.
Bayes Theorem and the posterior probability states that for
a data instance x and class ω
P (ω|x) =
P (x|ω)P (ω)
P (x)
(1)
Firstly, the class priors, P (ω), may change overtime. Sec-
ondly, the distributions of the classes might change, where
P (x|ω) alters over time. Finally, the posterior distributions of
the class may change, P (ω|x) [36].
Not all changes will cause the classiﬁer to be incorrect for
the current data distribution. If the class priors, P (ω), and the
likelihood of observing a data point within a particular class,
P (x|ω), alters, the posterior distribution of class membership,
P (ω|x) might not change. This is termed virtual drift [37].
Other changes will alter the performance of the classiﬁer
that was trained using a data set from a different distribution.
Concept drift [38] is deﬁned as changes in the posterior distri-
bution of the class (concept) membership as time progresses
where Pt+1(w|x) (cid:2)= Pt(w|x) [39]. Furthermore, concept drift
is deﬁned as a gradual change to the target variable, and
concept shift is deﬁned as a more abrupt change to the target
variable [38], [40], [41].
It has been proposed that it is not necessary to differentiate
between changes to the concept and changes to the data distri-
bution, as both alterations require a model to be updated [41].
Therefore, we examine methods by which an adaptation can
be made by an anomaly detector, regardless of the nature of
the change to the data. We use the more general term non-
stationary (distribution) rather than referring to speciﬁc types
of change to the concept or data distribution. It is necessary
that effective anomaly detection algorithms are able to adapt to
non-stationary data distributions in order to construct accurate
models which minimize the error on unseen data [42]–[44].
C. Anomaly Detection in a Non-Stationary Environment
Previously, the nature of data in a non-stationary environ-
ment was discussed. Attention is now turned to the application
domain of anomaly detection and the effect of a non-stationary
distribution.
Anomaly detection differs from supervised two-class clas-
siﬁcation problems. Anomaly detection uses one-class classi-
ﬁcation where one concept class, rather than two, is deﬁned.
The purpose is to classify a data vector as either belonging
to the class, a normal data vector, or not belonging to the
class, an anomaly data vector. If we deﬁne the concept as
the target variable that the algorithm is trying to model, then
anomaly detection aims to model the concept in order to
identify data that does not belong to it. Therefore, for the
normal class N and the anomaly class A, P (ω) = P (N )
and P (A) = 1 − P (N ). The posterior probability of the
normal class membership is P (N|x), which deﬁnes the class
boundary for the normal data.
A non-stationary distribution can affect anomaly detection
in two ways:
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.6
IEEE COMMUNICATIONS SURVEYS & TUTORIALS, ACCEPTED FOR PUBLICATION
1.5
1
0.5
0
t
2
e
u
b
i
r
t
t
A
−0.5
−1
−1.5
−1.5
−1
−0.5
1.5
1
0.5
0
t
2
e
u
b
i
r
t
t
A
−0.5
−1
−1.5
−1.5
−1
−0.5
0.5
1
1.5
0
Attribute 1
(a) X, Y ∼ N (0, 0.04)
1.5
1
0.5
0
t
2
e
u
b
i
r
t
t
A
−0.5
−1
−1.5
−1.5
−1
−0.5
0.5