5
∗
35
∗
7
∗
14
61
24
∗
776
∗
34
126
128
1278
34
33
27
3
218
5
44
10
13
265
41
131
135
959
later). j¨Ak extracted the highest number of unique URL structures in 10 applica-
tions. In one application, i.e., Nibbleblog, j¨Ak, W3af, and Skipﬁsh extracted the
same number of URL structures. In the remaining two web applications, W3af
extracted the highest number of web applications. In the case of Joomla, W3af
extracted 3 URL structures more than j¨Ak, whereas in the case of Tidios, W3af
extracted 251 URLs against 166 of j¨Ak.
To interpret these results qualitatively, we sought to assess to what extent
the surfaces explored by each tool relate to the one explored by j¨Ak. A way to
measure this is to analyze the URLs extracted by j¨Ak and each of the other tools,
and to calculate the complement sets. These two sets will contain the following
URLs. The ﬁrst set contains URLs that are extracted by j¨Ak and missed by each
of the other crawlers. The second set contains the URLs that are not discovered
by j¨Ak but are extracted by the other tool. The number of URLs in each of these
sets is shown in Table 4. When compared with the other tools, on average, j¨Ak
explored a surface of the web applications which is 86 % larger than the one of
the other tools. Then, the amount of surface which j¨Ak did not explore range
from 0.5 % of Crawljax to 22 % of Skipﬁsh.
Table 4. Unique URLs discovered only by j¨Ak (+) and missed by j¨Ak (-).
Groups
Surf. discovered only by j¨Ak
Surf. missed by j¨Ak
Crawljax W3af
+85%
-18%
+98%
-0.5%
Wget Skipﬁsh
+70%
+90%
-22%
-20%
j¨Ak: Using Dynamic Analysis to Crawl and Test Modern Web Applications
313
To further understand the potential misses by j¨Ak, we manually inspected a
random sample of 1030 (15 %) of the URLs that are not discovered by j¨Ak. We
were able to identify eight classes of URLs, as shown in Table 5. URL forgery
refers to URLs which are not present in the web application but are forged by
the crawler. The vast majority of the URLs that j¨Ak “missed”, i.e., 75 % of
the URLs, are URLs that were forged by W3af and Skipﬁsh. Forging means
that these tools implement a crawling strategy which attempts to visit hidden
parts of the web application. Starting from a URL, they systematically submit
URLs in which they remove parts of the path. For example, W3af derives from
URLs like http://foo.com/p1/p2/p3, other URLs, i.e., http://foo.com/p1/p2,
http://foo.com/p1/, and http://foo.com/. It is important to notice that these
URLs are not valid URLs of the web application. For this reason, we corrected
the results in Table 3 by deducting the percentage of forged URLs. Next, the
class static resources include style-sheet documents or external JS ﬁles with a
diﬀerent document extension, e.g., .php. This is an error introduced by our URL
analysis which failed in recognizing these documents as static. The third class of
URLs (5.34 %) is the one of unsupported actions such as form submission during
crawling. Then, the fourth class contains URLs that were not extracted because
they belong to a user session diﬀerent from the one used by j¨Ak. This may be
solved by using j¨Ak in parallel with multiple user credentials. The ﬁfth class
contains URLs that are due to bugs both in j¨Ak and in Skipﬁsh. The sixth class
contains URLs that are generated while crawling. We have two types of these
URLs: URLs with timestamps and URLs generated by, for example, creating
new content in the web application. 1,36 % of the URLs, we could not ﬁnd the
origin of the URL nor the root cause. Finally, 1,26 % of the URLs are of W3af
that does not implement a depth-bounded crawling and thus might crawl the
applications deeper than other crawlers.
Detection — Finally, we measure how the improved crawling convergence trans-
lates into the detection of XSS vulnerabilities in the 13 web applications. For
these tests, we had to exclude Wget and Crawljax, as they are pure crawlers and
as such cannot discover vulnerabilities.
j¨Ak discovered XSS vulnerabilities in three of the ﬁve web applications, i.e.,
phpBB, Piwigo, and MyBB 1.8.1. However, j¨Ak could not ﬁnd known vulnerabil-
ities in OwnCloud 4 and ModX. Manual analysis revealed that the vulnerability
as described in the security note of OwnCloud 4 is not exploitable. For ModX,
j¨Ak could not discover the URL. The URL is added in the DOM tree by an
event handler. j¨Ak correctly ﬁres the events, but the code of the handler is not
executed because it veriﬁes that the target is an inner tag. This shortcoming
is the same that cause to fail the test in the C5 class of Table 1. In a regular
browser, due to the implicit rules for the propagation of events, the user will
click on the inner tag and the outer one will be executed. As a future work, we
plan to reproduce the event propagation as implement by regular browsers.
The other tools detected only known vulnerabilities in MyBB, and had issues
with false positives. Both W3af and Skipﬁsh detected the XSS vulnerability in
MyBB 1.8.1. Furthemore, in Mediawiki W3af reported 49 XSS vulnerabilities
314
G. Pellegrino et al.
and Skipﬁsh one vulnerability, respectively. However, in both cases, these were
false positives. Finally, Skipﬁsh reported 13 false positives in Gallery. In our
experiments, j¨Ak did not report any false positive. This is the result of using
dynamic analysis for the detection of XSS attacks: if an attack is successful, the
test payload is executed and visible in the dynamic trace.
Table 5. Origin of the URLs that were not discovered by j¨Ak
URL Origin
URL Forgery
Static resources
Unsupp. action
User session mgmt.
sguB
New content
nwonknU
Beyond max depth (W3af)
Total
URLs
774
57
55
53
74
17
41
13
1030
Fraction
75.15%
5.53%
5.34%
5.15%
%65.4
1.65%
%63.1
1.26%
100,00%
6 Related Work
In this section we review works closely related to our paper. We focus on two
areas: analysis of existing web application scanners, and novel ideas to improve
the current state of the art of scanners.
Bau et al. [15] and Doup´e et al. [16] presented two independent and comple-
mentary studies on the detection power of web application scanners. Both works
concluded that while web scanners are eﬀective in the detection of reﬂected XSS
and SQLi, they still poorly perform in the detection of other classes of more
sophisticated vulnerabilities. According to these works, one of the reason of these
results is the lack of support of client-side technology. Furthermore, Doup´e et al.
explored in a limited way the problem of web application coverage focusing on
the capability of scanners to perform multi-step operations. As opposed to these
works, in this paper we mainly focused on the problem of the coverage of web
applications and detailed the shortcomings of four web application scanners.
Recently, there have been new ideas to improve the state of the art of web
application scanner. These works included the support of client-side features and
explored the use of reasoning techniques together with black-box testing. The
most notable of these works are the state-aware-crawler by Doup´e et al. [14],
Crawljax by Mesbah et al. [2], AUTHSCAN by Guangdong et al. [17], and
SSOScan by Zhou et al. [18]. State-aware-crawler proposed a model inference
algorithm based on page clustering to improve the detection of higher-order
XSS and SQLi. However, this technique focus mainly on the detection of state-
changing operations and it does not take into account the dynamic features of
j¨Ak: Using Dynamic Analysis to Crawl and Test Modern Web Applications
315
client-side programs. Similarly, Crawljax proposed a model inference technique
based on “user-clickable areas” in order to crawl hidden parts of AJAX-based
web applications. However, Crawljax uses static heuristics that do not satisfac-
torily cover the dynamic interaction points between the user and the UI. As
opposed to Crawljax, j¨Ak does not rely on these heuristics and uses a tech-
nique which can detect the registration of event handlers via function hooking.
Finally, AUTHSCAN and SSOScan are black-box testing tools that focus on the
Web-based Single Sign-On functionalities integrated in web applications. AUTH-
SCAN extends the classical design veriﬁcation via model checking with the auto-
matic extraction of formal speciﬁcations from HTTP conversations. SSOScan is a
vulnerability scanner that targets only Facebook SSO integration in third-party
web applications. Neither of the two tools is a web application scanner, and they
do not support crawling web applications. As opposed to j¨Ak, the focus of these
tools is on improving the detection power of security testing tools. Nevertheless,
the proposed testing technique may be integrated into j¨Ak to detect other classes
of vulnerabilities.
A work closely related to our approach is Artemis [19]. Artemis is a JavaScript
web application testing framework which supports the generation and execution
of test cases to increase the client-side code coverage. Starting from an initial
input (e.g., event), Artemis explores the state space of the web application by
probing the program with new inputs. Inputs are generated and selected by
using diﬀerent strategies in order to maximize, e.g., code branches or number
of read/write access of object properties. At each step, Artemis resets the state
of the client and server side to a known state and continues the exploration.
From the angle of input generation, Artemis and our approach shares common
points. For example, both approaches explore the client side by ﬁring events
and observing state changes. However, Artemis and our approach diﬀer on the
assumption of the availability of the server side. While Artemis assumes complete
control of the state space of the server side, our approach does not make this
assumption and targets the exploration of a live instance of the server side.
7 Conclusion
This paper presented a novel technique to crawl web applications based on the
dynamic analysis of the client-side JavaScript program. The dynamic analysis
hooks functions of JavaScript APIs to detect the registration of events, the use
of network communication APIs, and ﬁnd dynamically-generated URLs and user
forms. This is then used by a crawler to perform the next action. The crawler
creates and builds a navigation graph which is used to chose the next action. We
presented a tool j¨Ak, which implements the presented approach. We assessed
j¨Ak and four other web-application scanners using 13 web applications. Our
experimental results show that j¨Ak can explore a surface of the web applications
which is about 86 % larger than the other tools.
Acknowledgements. This work was supported by the German Ministry for Educa-
tion and Research (BMBF) through funding for the project 13N13250, EC SPRIDE
316
G. Pellegrino et al.
and ZertApps, by the Hessian LOEWE excellence initiative within CASED, and by
the DFG within the projects RUNSECURE, TESTIFY and INTERFLOW, a project
within the DFG Priority Programme 1496 Reliably Secure Software Systems −RS 3.
References
1. Zhou, J., Ding, Y.: An analysis of URLs generated from javascript code. In: 2012
IEEE/ACIS 11th International Conference on Computer and Information Science
(ICIS), vol. 5, pp. 688–693 (2012)
2. Mesbah, A., van Deursen, A., Lenselink, S.: Crawling ajax-based web applications
through dynamic analysis of user interface state changes. ACM Trans. Web 6(1),
3:1–3:30 (2012)
3. Urgun, B.: Web Input Vector Extractor Teaser (2015). https://github.com/
bedirhan/wivet
4. Hickson, I.: A vocabulary and associated APIs for HTML and XHTML (2014).
http://dev.w3.org/html5/workers/
5. van Kesteren, A., Gregor, A., Ms2ger, Russell, A., Berjon, R.: W3C DOM4 (2015).
http://www.w3.org/TR/dom/
6. The Python Software Foundation: Python (2015). https://www.python.org/
7. Apple Inc.: The WebKit Open Source Project (2015). https://www.webkit.org/
8. Riverbank Computing Limited: PyQt - The GPL Licensed Python Bindings for
the Qt Application Framework (2015). http://pyqt.sourceforge.net/
9. Google Inc.: V8 JavaScript Engine (2015). https://code.google.com/p/v8/
10. Mozilla Foundation: SpiderMonkey (2015). https://developer.mozilla.org/en-US/
docs/Mozilla/Projects/SpiderMonkey
11. Zalewski, M.: Skipﬁsh (2015). https://code.google.com/p/skipﬁsh/
12. Riancho, A.: w3af: Web Application Attack and Audit Framework (2015). http://
w3af.org/
13. Nikˇsi´c, H., Scrivano, G.: GNU Wget (2015). http://www.gnu.org/software/wget/
14. Doup´e, A., Cavedon, L., Kruegel, C., Vigna, G.: Enemy of the state: a state-aware
black-box vulnerability scanner. In: Proceedings of the 2012 USENIX Security
Symposium (USENIX 2012), Bellevue, WA (2012)
15. Bau, J., Bursztein, E., Gupta, D., Mitchell, J.: State of the art: automated black-
box web application vulnerability testing. In: 2010 IEEE Symposium on Security
and Privacy (SP) (2010)
16. Doup´e, A., Cova, M., Vigna, G.: Why Johnny can’t pentest: an analysis of black-
box web vulnerability scanners. In: Kreibich, C., Jahnke, M. (eds.) DIMVA 2010.
LNCS, vol. 6201, pp. 111–131. Springer, Heidelberg (2010)
17. Guangdong, B., Guozhu, M., Jike, L., Sai, S.V., Prateek, S., Jun, S., Yang, L.,
Jinsong, D.: Authscan: Automatic extraction of web authentication protocols from
implementations. In: 2013 Annual Network and Distributed System Security Sym-
posium (NDSS). The Internet Society (2013)
18. Zhou, Y., Evans, D.: Ssoscan: automated testing of web applications for single
sign-on vulnerabilities. In: 23rd USENIX Security Symposium (USENIX Security
2014), pp. 495–510. USENIX Association, San Diego, CA (2014)
19. Artzi, S., Dolby, J., Jensen, S.H., Møller, A., Tip, F.: A framework for automated
testing of javascript web applications. In: Proceedings of the 33rd International
Conference on Software Engineering, ICSE 2011, pp. 571–580. ACM, New York,
NY, USA (2011). http://doi.acm.org/10.1145/1985793.1985871