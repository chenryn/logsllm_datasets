pro 1607 10.0.14393.0 64 bits). We use these old versions because
they contain vulnerabilities that have been fixed in the latest version.
In total, we collected 52 public exploits from Bochspwn and used
them in our evaluation.
The result is encouraging. Our system detected 85 vulnerabilities,
while Bochspwn can only detect 67 of them. The result is shown in
Figure 7 (Table 3 and 4 show the details.) After carefully inspect-
ing all the findings and cross-checking with the CVE information
of Microsoft from 2016 and 2018, we found that 55 of them have
CVE numbers. For the other 30, they do not have CVE numbers,
stack
stack
heap
heap
heap
heap
stack
stack
stack
stack
stack
stack
stack
stack
stack
stack
stack
stack
s
e
u
s
s
i
d
e
t
c
e
t
e
d
f
o
r
e
b
m
u
n
d
e
t
a
l
u
m
u
c
c
a
Historical CVE-2017-8678, 0x04 bytes disclosure (xmm0 to memory)
Historical CVE-2018-0970, 0x10 bytes disclosure (xmm0 to memory)
Historical CVE-2017-8462, 0x01 byte disclosure (eax to memory)
Historical CVE-2017-8708, 0x04 bytes disclosure (rax to memory)
Historical CVE-2018-0972, 0x08 bytes disclosure (ecx to memory)
Historical CVE-2018-8407, 0x10 bytes disclosure (xmm0 to memory)
Historical
Historical
Historical
Historical
Historical
Historical
Historical
Historical
Historical
Historical
Historical
Historical
0x02 bytes disclosure (r10 to memory)
0x04 bytes disclosure (eax to memory)
0x04 bytes disclosure (eax to memory)
0x04 bytes disclosure (ecx to memory)
0x04 bytes disclosure (r10 to memory)
0x04 bytes disclosure (r10 to memory)
0x04 bytes disclosure (r10 to memory)
0x04 bytes disclosure (xmm0 to memory)
0x08 bytes disclosure (rax to memory)
0x08 bytes disclosure (rax to memory)
0x08 bytes disclosure (xmm0 to memory)
0x08 bytes disclosure (xmm0 to memory)
30
20
10
0
0
Differential Replay
Taint Analysis
0.5
1
1.5
2
time elapsed in the experiment (seconds)
2.5
·105
Figure 8: Time used to detect new vulnerabilities in Table 2
by differential replay and dynamic taint analysis. We find
that differential replay can detect all of them in less than
170,000 second (47 hours), while dynamic taint analysis can
only detect 7 of them in 240,000 seconds (66 hours).
and are not publicly known. However, our manual analysis con-
firmed they are indeed kernel information leaks. We believe they
are vulnerabilities detected and patched internally by Microsoft.
This demonstrated the effectiveness of our system.
Summary TimePlayer can detect new vulnerabilities that affected
the latest versions of Windows systems. Using the same test cases,
all the vulnerabilities reported by Bochspwn could also be detected
by our system, and our system reported 18 more vulnerabilities.
Session 8D: Language SecurityCCS ’19, November 11–15, 2019, London, United Kingdom1893Table 5: Overhead of differential replay. The second column
shows the time consumed in vanilla replay, and the third
column shows the time consumed in replay with memory
poisoning and differences checking, the fourth and fifth
columns is the number of issues found in corresponding test
case.
Test Case
ReactOS_a
Chrome
cp
Chrome64
Vanilla Replay
222.85s
837.32s
1507.04s
1110.92s
Replay with
Poisoning & Checking
5,063.25s
18,962.72s
37,503.84s
24,885.625s
# of
Instructions
14,311,248,868
61,142,430,613
113,650,380,756
77,569,624,726
Issues:
stack/heap
4/2
5/3
3/2
6/3
6.2 Efficiency
One advantage of our system is leveraging the differential replay
to quickly detect the use of uninitialized variables, instead of using
taint analysis to track the data flow. In the following, we will com-
pare the efficiency of the differential replay with the taint analysis,
from the perspective of tracking uninitialized variables.
To this end, we implement a system to track the data flow using
the taint analysis (this one is called reference system). Specifically,
we use PANDA to log the execution trace, and then perform the
offline taint analysis on the trace to detect whether the variable in
the kernel space has been leaked to user space. We feed same test
cases (Table 1) to the reference system and TimePlayer one by one,
and log the time when each vulnerability was detected.
The result is shown in Figure 8. The x axis means the time elapsed
in the experiment and the y axis denotes the accumulated number
of detected vulnerabilities. TimePlayer consumed 165, 102 seconds
to detect all 34 issues, and analyzed 553, 385, 983, 467 instructions
in total. The average speed is around 3, 351, 775 instructions per
second. However, for the system using taint analysis, it consumed
more than 240, 000 seconds (66 hours) and only detected 7 of them
within that time period. This is due to high performance overhead
introduced by taint analysis on the whole system trace.
Note that, our system also leverages taint analysis to locate the
sources of variables. However, we only need to apply taint analysis
to a small part of the whole system trace, and this process usually
finishes in less than one minute (Section 6.3).
Summary The key technique, i.e., differential replay, is more ef-
ficient to detect the use of uninitialized variables than the taint
analysis.
6.3 Performance
In the following, we will show the evaluation result of the perfor-
mance of two key techniques, i.e., differential replay and symbolic
taint analysis, respectively.
Differential replay Table 5 shows the overhead of differential
replay. ReactOS_a is one test case inside the ReactOS test suits used
in our evaluation. We also used Chrome to visit multiple web pages,
and cp operation to copy and paste 1, 226 files (total size is around
40M bytes) inside the guest Windows operating system. The result
shows that, differential replay is around 22-24x slower, compared
with the vanilla replay. Note that, though the absolute speed is
slow, it’s still more efficient than dynamic taint analysis to detect
vulnerabilities (Section 6.2).
Table 6: Overhead of differential replay in parallel (four in-
stances).
Test Case
ReactOS_a
ReactOS_a_1
ReactOS_a_2
ReactOS_a_3
ReactOS_a_4
Chrome64
Chrome64_1
Chrome64_2
Chrome64_3
Chrome64_4
Replay
222.85s
59.66s
62.38s
61.81s
64.24s
1,110.92s
292.12s
214.56s
246.94s
221.52s
Replay with
Poisoning & Checking
5,063.25s
1,312.15s
1,318.15s
1,354.33s
1,313.71s
24,885.63s
7,542.66s
5,824.78s
6,608.77s
6,131.71s
# of Instructions
14,311,248,868
3,577,812,223
3,577,812,214
3,577,812,215
3,577,812,189
77,569,624,726
19,392,406,183
19,392,406,181
19,392,406,179
19,392,406,157
Table 7: Overhead of symbolic taint analysis. The second and
third columns show the time used to generate the trace, and
the trace size, respectively. The last column shows the time
used to perform the taint analysis.
Test Case
CVE-2018-8408
CVE-2018-8477
win32k!xxxInitTerminal
CVE-2019-0569
win32k!xxxInterSendMsgEx
Trace
Time
30s
16s
15s
16s
10s
Trace
Size
7.2GB
3.2GB
3.4GB
3.2GB
2.6GB
Instructions
Analysis Time
Taint
5.842s
1.145s
4.871s
1.925s
37.645s
# of
91.22M
40.15M
42.87M
40.12M
32.24M
Our system leverages an optimization strategy to replay multi-
ple instances in parallel (Section 4.3). In the following, we evaluate
the effectiveness of this optimization. We use the same test case
ReactOS_a and Chrome as in the previous experiment. Since the
number of CPU cores on the machine is four, we split recorded
traces into four instances, which are poisoned and replayed in par-
allel. The result is shown in Table 6. We can see that, the time
consumed depends on the slowest instance, i.e., the instance that
runs 1, 354.33 seconds. Compared with the reported 5, 063.25 sec-
onds without parallel replay, this optimization speeds up nearly
4x.
Symbolic taint analysis As shown in Table 7, the symbolic taint
analysis is very efficient. When a differential point is detected,
our system looks back several stack frames and replay from there
to generate detailed system trace for further taint analysis. The
second, third and fourth columns show the time used to generate
the trace, the trace size and the number of instructions inside the
trace. After that, we apply the (offline) symbolic taint analysis on
the trace. In particular, TimePlayer propagates the taint tags and
generates the symbolic expression along the trace, until it reaches
the differential point. The time is reported in the last column.
Note that, during evaluation, we set the initial value of the win-
dow size as 500, and the threshold to 5, 000 (Section 5.1). In most
cases, our system can successfully locate the source with a window
size less than 1, 500. Since we only need to consider instructions in-
side the window, the taint analysis typically will end in one minute.
Summary The differential replay is around 22-24x slower than
the vanilla replay of PANDA. However, this process can be opti-
mized using parallel replaying. The symbolic taint analysis typically
finishes in one minute for real test cases.
Session 8D: Language SecurityCCS ’19, November 11–15, 2019, London, United Kingdom18947 DISCUSSION AND LIMITATION
False positives Our system compares differences of program
states to determine the use of uninitialized variables. This operation
is performed at the checking point (Section 4.3). In our prototype
and evaluation, our system is leveraged to detect the kernel infor-
mation leak. It only considers a kernel information leak when a
memory write instruction is executed with the kernel privilege and
the destination address is in user-space area, while at the same time
there is a difference between two replay instances. This is a very con-
servative strategy, and will not introduce false positives. This has
been confirmed by our findings that all the reported leakage can be
manually confirmed (Table 2, Table 3 and Table 4).
False negatives Our system may have false negatives. That means
some vulnerabilities could be missed. This is due to the nature of
th dynamic system whose effectiveness depends on the code cov-
erage during this process. In our evaluation, we leverage ReactOS
test suits and popular programs (Table 1) to drive and record the
system’s execution. This leads to the discovery of 34 issues and
vulnerabilities. However, we did not leverage any path exploration
technique to actively trigger new paths. We believe the orthogonal
efforts on the code coverage improvement, e.g., the fuzzing testing
tools AFL [69] or KAFL [56] could be borrowed. Moreover, our sys-
tem may miss the kernel memory leak due to DMA requests. That’s
because our system does not check memory operations issued from
the DMA controller.
The value to poison memory needs to be selected carefully. That
is because in some cases, the differences caused by the poisoned
memory could be lost. For instance, if the uninitialized data (its
value is 0x0) is used to perform the bitwise-AND operation with a
constant value (0x1 for instance) and the poisoned word is 0xaa, it
will not cause any difference between the vanilla replay instance and
the poisoned replay instance, and the uninitialized variable will not
be detected. To solve this problem, we can run the program multiple
times with different data for poisoning to reduce the possibilities
of false negatives.
Performance overhead and optimizations As shown in the
evaluation, the differential replay introduces 22x-24x slowdown
compared with the vanilla replay of PANDA. Note that this slow-
down does not affect the effectiveness of our system to detect vul-
nerabilities, as demonstrated by the new vulnerabilities reported by