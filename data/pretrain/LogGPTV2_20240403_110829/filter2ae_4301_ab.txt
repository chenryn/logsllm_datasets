        else:
            print '[Info] black opcodes exists'
        # 使用数据
        white_file_list = []
        black_file_list = []
        with open('black_opcodes.txt', 'r') as f:
            for line in f:
                black_file_list.append(line.strip('\n'))
        with open('white_opcodes.txt', 'r') as f:
            for line in f:
                white_file_list.append(line.strip('\n'))
        len_white_file_list = len(white_file_list)
        len_black_file_list = len(black_file_list)
        y_white = [0] * len_white_file_list
        y_black = [1] * len_black_file_list
        X = white_file_list + black_file_list
        y = y_white + y_black
        print '[Data status] ... ↓'
        print '[Data status] X length : {}'.format(len_white_file_list + len_black_file_list)
        print '[Data status] White list length : {}'.format(len_white_file_list)
        print '[Data status] black list length : {}'.format(len_black_file_list)
        # X raw data
        # y label
        return X, y
prepare_data 做了以下几个事：
  * 把黑名单和白名单中的PHP opcode 统一生成并分别写入到两个不同的文件中。
  * 如果这两个文件已经存在，那就不再次生成了
  * 把白名单中的PHP opcode 贴上 【0】的标签
  * 把黑名单中的PHP opcode 贴上 【1】的标签
  * 最后返回所有PHP opcode 的集合数据 X（有序）
  * 返回所有PHP opcode 的标签 y（有序）
##### 第三步：编写训练函数
终于到了我们的重点节目了，编写训练函数。
在这里先简单的介绍一下[scikit-learn](http://scikit-learn.org/)中我们需要的一些使用起来很简单的对象和方法。
  * CountVectorizer
  * TfidfTransformer
  * train_test_split
  * GaussianNB
CountVectorizer 的作用是把一系列文档的集合转化成数值矩阵。
TfidfTransformer 的作用是把数值矩阵规范化为 tf 或 tf-idf 。
train_test_split的作用是“随机”分配训练集和测试集。这里的随机不是每次都随机，在参数确定的时候，每次随机的结果都是相同的。有时，为了增加训练结果的有效性，我们会用到交叉验证（cross
validations）。
GaussianNB ：Scikit-learn 对朴素贝叶斯算法的实现。朴素贝叶斯算法是常用的监督型算法。
先上写好的代码：
    def method1():
        """
        countVectorizer + TF-IDF 整理数据
        朴素贝叶斯算法生成
        :return: None
        """
        X, y = prepare_data()
        cv = CountVectorizer(ngram_range=(3, 3), decode_error="ignore", token_pattern=r'\b\w+\b')
        X = cv.fit_transform(X).toarray()
        transformer = TfidfTransformer(smooth_idf=False)
        X = transformer.fit_transform(X).toarray()
        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)
        gnb = GaussianNB()
        gnb.fit(x_train, y_train)
        joblib.dump(gnb, 'save/gnb.pkl')
        y_pred = gnb.predict(x_test)
        print 'Accuracy :{}'.format(metrics.accuracy_score(y_test, y_pred))
        print metrics.confusion_matrix(y_test, y_pred)
代码介绍：
首先，我们用了刚才写的prepare_data()函数来获取我们的数据集。然后，创建了一个CountVectorizer
对象，初始化的过程中，我们告诉CountVectorizer对象，ngram的上下限为(3,3)
【ngram_range=(3,3)】，当出现解码错误的时候，直接忽略【decode_error="ignore"】，匹配token的方式是【r"\b\w+\b"】，这样匹配我们之前用空格来隔离每个opcode
的值。
然后我们用 `cv.fit_transform(X).toarray()` 来“格式化”我们的结果，最终是一个矩阵。
接着创建一个TfidfTransformer对象，用同样的方式处理一次我们刚才得到的总数据值。
然后使用`train_test_split`函数来获取打乱的随机的测试集和训练集。这时候，黑名单中的文件和白名单中的文件排列顺序就被随机打乱了，但是X[i]
和 y[i] 的对应关系没有改变，训练集和测试集在总数聚集中分别占比60%和40%。
接下来，创建一个GaussianNB 对象，在Scikit-learn中，已经内置好的算法对象可以直接进行训练，输入内容为训练集的数据（X_train） 和
训练集的标签（y_train）。
    gnb.fit(X_train, y_train)
执行完上面这个语句以后，我们就会得到一个已经训练完成的gnb训练对象，我们用测试集(X_test) 去预测得到我们的y_pred 值（预测出来的类型）。
然后我们对比原本的 y_test 和 用训练算法得到的结果 y_pred。
    metrics.accuracy_score(y_test, y_pred)
结果即为在此训练集和测试集下的准确率。
约为97.42%
还需要计算混淆矩阵来评估分类的准确性。
    metrics.confusion_matrix(y_test, y_pred)
输出结果见上图。
编写训练函数到这里已经初具雏形。并可以拿来简单的使用了。
##### 第四步：持久化&应用
编写完训练函数，现在我们可以拿新的Webshell来挑战一下我们刚才已经训练好的gnb。
但是，如果每次检测之前，都要重新训练一次，那速度就非常的慢了，我们需要持久化我们的训练结果。
在Scikit-learn 中，我们用joblib.dump() 方法来持久化我们的训练结果，细心的读者应该发现，在method1() 中有个被注释掉的语句
    joblib.dump(gnb, 'save/gnb.pkl')
这个操作就是把我们训练好的gnb保存到save文件夹内的gnb.pkl文件中。
方面下次使用。
创建check.py
理一下思路：先实例化我们之前保存的内容，然后将新的检测内容放到gnb中进行检测，判断类型并输出。
核心代码：
        gnb = joblib.load('save/gnb.pkl')
        y_p = gnb.predict(X[-1:])
最后根据标签来判断结果，0 为 正常程序， 1 为 Webshell。
我们来进行一个简单的测试。
那么，一个简单的通过朴素贝叶斯训练算法判断Webshell的小程序就完成了。
#### 下一步？
这个小程序只是一个简单的应用，还有很多的地方可以根据需求去改进
如：
在准备数据时：
  1. 生成 opcode过程中，数据量太大无法全部放入内存中时，更换写入文件中的方式。
在编写训练方法时：
  1. 更换CountVectorizer的ngram参数，提高准确性。
  2. 增加cross validation 来增加可靠性
  3. 更换朴素贝叶斯算法为其他的算法，比如MLP、CNN（深度学习算法）等。
  4. 在训练后，得到数据与预期不符合时：
重复增量型训练，优化训练结果。
  1. 增大训练数据量
  2. 如果对PHP opcode 有深入研究的同学可以采用其他的提取特征的方法来进行训练。
  3. 选择多种训练方法，看看哪一种的效果最好，而且不会过度拟合（over fitting）。 
#### 结语
最后咱们总结一下机器学习在Webshell 检测过程中的思路和操作。
  1. 提取特征，准备数据
  2. 找到合适的算法，进行训练
  3. 检查是否符合心中预期，会不会出现过度拟合等常见的问题。
  4. 提供更多更精准的数据，或更换算法。
  5. 重复1~4
本人也是小菜鸡，在此分享一下简单的思路和方法。希望能抛砖引玉。
项目下载地址：
参考链接：
[基于机器学习的 Webshell 发现技术探索](https://mp.weixin.qq.com/s/1V0xcjH-6V5qJoJILP0pJQ
"基于机器学习的 Webshell 发现技术探索")
* * *