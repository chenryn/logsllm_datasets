748
+3.17 k
Far Right
2.26 k
+1.72 k
4.60 k
+9.76 k
1.57 k
+23.0 k
9.24 k
+1.55 k
2.96 k
+18.5 k
650
+1.47 k
2.91 k
+3.16 k
Table 6: Median (a) and mean (b) interactions per post of each type from non-misinformation (N) pages, and in alternating rows
the difference for posts from misinformation pages (delta relative to non-misinformation). Values do not add up to the overall
aggregate because computations are done independently. Photo, Facebook video and live video posts from misinformation
pages receive significantly higher median and mean engagement per post than posts from non-misinformation pages.
the independent variables and the natural log-transformed distri-
bution of views per video as the dependent variable, we found the
interaction effect of factualness was significant at the 0.05 level for
all political leanings (Table 4), and post-hoc testing confirmed the
significance of factualness in explaining differences in mean video
views for all partisanship groups.
For comparison, we show engagement with the same set of
videos in Figure 9b. Disregarding slightly left video, the trends of
the medians and means with regard to the impact of a misinfor-
mation source on video engagement are the same as they are for
video views. To understand differences in mean engagement per
video, we used a Multivariate ANOVA model similar to what we
have described previously, and found the difference in mean en-
gagement per video due to factualness was statistically significant
at the 0.05 level of confidence for all political leanings of the video
publisher (Table 4). Post-hoc testing confirmed the significance of
factualness in explaining differences in mean video engagement
for all partisanship groups.
To further investigate the similarity of video views and engage-
ment, we plot video views against engagement in Figure 9c. This
figure suggests that engagement is generally correlated with views,
and that to some extent, engagement-based metrics may serve as
a substitute for view-based metrics. However, 283 videos received
more engagement than views, out of which 246 received more re-
actions than views. Facebook users likely reacted to the videos
without watching them, as users can react only once to each post
(whereas they could comment or share multiple times). While we
could filter out these pathological cases, we cannot account for sim-
ilar effects of engaging without viewing among videos with more
views than engagement, thus views are not a suitable replacement
for the lack of impression data. While we would like to study how
many users who were shown a misinformation video also engaged
with the video, using view data might not be a good approximation.
453
Understanding Engagement with U.S. (Mis)Information News Sources on Facebook
IMC ’21, November 2–4, 2021, Virtual Event
able to analyze engagement and views, the data are entirely opaque
about content recommendations or impressions, and we cannot
disentangle the effects of Facebook’s algorithms, content attrac-
tiveness, and user behavior. For example, we were able to show
that misinformation content is more engaged with, but in order to
study whether it is truly more engaging, the rate of engagement,
we would need impression data. Furthermore, in order to study
the sources of engagement, we would need the impression data
broken down by categories such as followers, non-followers, shares,
and sponsored impressions. These data could help researchers un-
derstand whether there is a difference in rate of engagement with
misinformation based on partisanship or the type of content pro-
motion. We believe that understanding these factors is crucial to
devising effective countermeasures against misinformation.
Limitations. The results of our analysis are as imperfect and lim-
ited as the data sources that we rely on. Major threats to the validity
of our results are potential biases or omissions in the lists of news
publishers that we obtained from NewsGuard and Media Bias/Fact
Check. We were unable to discover the Facebook pages of some of
these publishers, and our data sets are limited to what is available
on CrowdTangle. When publishers delete posts from their Facebook
pages, for instance, they also disappear from CrowdTangle, which
means that highly controversial posts may be missing in our data
set. Our findings reflect the limits of the data available on Crowd-
Tangle; we were able to extract high-level trends in engagement
with misinformation news sources, but cannot derive any more
detailed data-driven explanations of these phenomena. Lastly, we
label news sources, not individual news pieces, based on their rep-
utations for factualness and partisanship as judged by NewsGuard
and Media Bias/Fact Check. Our boolean misinformation attribute
is based on a threshold instead of modeling the degree to which
news publishers vary in how much misinformation, fake news, and
conspiracy theories they spread. We do not know which percentage
of content promoted by misinformation sources contains misinfor-
mation, or how much content from a publisher is in line with its
partisanship assessment.
6 RELATED WORK
There is a large body of work on misinformation. We focus on
studies of misinformation news in online social media.
Misinformation and the 2016 election. Allcott and Gentzkow [1]
explore the economics of misinformation on Facebook during the
2016 election and measure the relative exposure of the U.S. popula-
tion to fake news. Faris et al. [11] perform a large-scale analysis of
content from media sources shared on Facebook via CrowdTangle,
and perform a network analysis. They observe that the far-right
media ecosystem is distinct from the mainstream media in linking
behavior, as well as in journalistic standards and practices. Three of
those authors, Faris, Roberts, and Benkler [4] expand this work to
additionally perform case studies of left and right partisan disinfor-
mation. They note that while individual Facebook posts spreading
both left and right disinformation generate high levels of engage-
ment, left-wing partisan media sources are much less likely than
their right-wing counterparts to repeat disinformation from other
sources. Guess, Nagler, and Tucker [15] study user sharing on Face-
book by observing user behavior directly, and matching against a
Figure 8: Bar plot of total views of videos from (mis)-
information news publishers from Far Left to Far Right,
with the number of videos posted in each group on the x
axis. In absolute terms, only videos from Far Right misin-
formation pages accumulated more views than their non-
misinformation counterparts. (Different data set than Fig-
ure 2, does not compare directly.)
4.5 Summary of Findings
In the ecosystem of U.S. news sources on Facebook, only in the
Far Right does misinformation accumulate more overall engage-
ment than non-misinformation. Across all other political lean-
ings, misinformation providers generate less engagement than non-
misinformation news sources, but sometimes still a sizeable share
of total engagement (e.g., 37.7 % in the Far Left). When looking
at the performance of individual news providers and how well
they engage with their respective follower base, the advantage of
misinformation widens to the Far Left when considering median
performance, and additionally includes Slightly Right publishers
when considering mean audience engagement. In terms of engage-
ment on a per-post basis, median engagement with posts from
misinformation providers is consistently higher across the entire
political spectrum, and for average engagement, only Slightly Left
misinformation sources are at a disadvantage compared to their
non-misinformation counterparts. Similar results hold for views
of videos posted by news pages. We also find that while there are
fewer misinformation than non-misinformation pages, individual
misinformation pages tend to have larger average follower bases.
5 DISCUSSION
Given our findings, there is no single answer to the question of
how bad is Facebook’s misinformation problem. While we found
misinformation news to be highly engaging to audiences across
the entire political spectrum, in absolute terms it is driving a sig-
nificant portion of engagement only on the extreme ends of the
spectrum, and particularly on the far right. Our methodology does
not give us any insight into why the apparent potential for more
misinformation is not exploited to the same degree outside of the
far right; this is an interesting topic for future research.
Recommendations. Facebook data available to researchers on
CrowdTangle are currently very limited in scope. While we were
454
26k7k58k337428k13k34k4k13k16k# FB videos per political leaning & (mis)information02 B4 B6 B8 B10 B12 B14 BTotal views6.82 B1.19 B3.67 B7.22 M13.5 B1.16 B1.05 B480 M845 M2.91 BNon-misinformationMisinformationNon-misinformationMisinformationIMC ’21, November 2–4, 2021, Virtual Event
Laura Edelson, Minh-Kha Nguyen, Ian Goldstein, Oana Goga, Damon McCoy, and Tobias Lauinger
(a) Video views
(b) Video engagement
(c) Video views vs. engagement
Figure 9: Views and engagement with Facebook-native and live video (separate data set). Medians and means for views and
engagement follow similar trends except for content from Slightly Left misinformation pages (they posted only 337 videos,
thus likely not reliable). The scatter plot excludes 171 videos with 0 views and an additional 5,511 videos with 0 engagement
due to the double log scale. While views and engagement appear largely correlated, outliers suggest users engaging with video
posts without viewing.
list of known purveyors of fake news. They find that fake news
makes up only a small part of of overall user sharing activity.
User engagement with misinformation. Van Bavel et al. [32]
explore psychological motivations and propose a model for un-
derstanding the reasons users might have for engaging with mis-
information. They point out that users share content even when
they do not believe it is true, to signal partisan identity. Several
others [18, 21] have also found that users engage with and share
news content they do not believe to be true. Geeng, Yee, and Roes-
ner [12] use a browser extension to inject fake news into study
participants’ Facebook timelines. They observe how participants in-
teract with misinformation when they encounter it, and document
their explanations for (not) investigating.
Misinformation ecosystem and audit studies. Closest to our
work is a large-scale study of news link sharing in reddit commu-
nities by Weld, Glenski, and Althoff [34], who use news source
ratings from Media Bias/Fact Check to annotate links with a parti-
san bias and factualness evaluation. We study a different platform,
Facebook, and various different post engagement metrics on the
official Facebook pages of news publishers instead of link sharing
in unofficial communities. Kornbluh, Goldstein, and Weiner [19] of
the German Marshal Fund use news evaluations from NewsGuard
to identify trustworthy, fake news, and manipulator news sources,
and measure their engagement over time; our work considers addi-
tional publisher/audience and per-post engagement metrics, and
contrasts publishers based on their partisanship. Glenski et al. [13]
study how users share content from deceptive news sources on
Twitter, classifying a relatively small number of 282 news providers
and monitoring how users share content from those providers. Our
work covers 2,551 news sources active on Facebook and focuses on
the publisher side instead of user characteristics and sharing behav-
ior. Hussein et al. [17] audit YouTube video recommendations and
note factors associated with YouTube’s recommendation algorithm
promoting misinformation.
Journalistic Uses of CrowdTangle. Journalists have used Crowd-
Tangle data for reporting on Facebook’s ecosystem, particularly dur-
ing election periods. In 2019, the Guardian created a dashboard [26]
showing the election videos with most engagement during the par-
liamentary election that year. Kevin Roose’s Twitter feed Facebook’s
Top 10 [28] posts daily the Facebook posts with most engagement
over the past 24 hours.
7 CONCLUSION
In this paper, we studied how much Facebook users engage with
(mis)information from U.S. news providers on Facebook. We found
that in absolute terms, Far-Right misinformation sources accumu-
late more engagement than non-misinformation sources of the same
partisanship (68.1 % of overall Far Right engagement, followed by
37.7 % on the Far Left). Even though the misinformation ecosystem
outside of the Far Right is smaller in absolute engagement terms,
misinformation also performs better than non-misinformation on
the Far Left and Slightly Right when considering how publishers
engage with their respective follower base. In terms of engagement
with posts from misinformation sources, the median is higher for all
political leanings. Our findings illustrate that the lure of misinfor-
mation is not confined to Far-Right audiences, although there are
fewer publishers outside of the Far Right to feed the misinformation
ecosystem. We hope that future research will be able to investigate
why misinformation generates more engagement.
ACKNOWLEDGMENTS
We wish to thank the employees of CrowdTangle and Facebook
who built the tools to enable our analysis. Particular thanks are
owed to Naomi Shiffman for her help working with CrowdTangle
and her insightful feedback on drafts of this work. Cybersecurity
for Democracy at NYU’s Center for Cybersecurity has been sup-
ported by Democracy Fund, Luminate, Media Democracy Fund, the
National Science Foundation under grant 1814816, Reset, and Well-
spring. This research was supported in part by the French National
Research Agency (ANR) through the ANR-17-CE23-0014 and the
MIAI@Grenoble Alpes ANR-19-P3IA-0003 grants and the European
Union’s Horizon 2020 research and innovation programme under
grant agreement No 101021377. This material is based upon work
supported by the Google Cloud Research Credits program.
455
26k7k58k337428k13k34k4k13k16k# FB videos per political leaning & (mis)information103104105106Views per FB video26k7k58k337428k13k34k4k13k16k# FB videos per political leaning & (mis)information101102103104105Engagement per FB videoNon-misinformationMisinformationUnderstanding Engagement with U.S. (Mis)Information News Sources on Facebook
IMC ’21, November 2–4, 2021, Virtual Event
REFERENCES
[1] Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake news in the
2016 election. Journal of Economic Perspectives 31, 2 (2017), 211–236.
[2] Jennifer Allen, Baird Howland, Markus Mobius, David Rothschild, and Duncan J
Watts. 2020. Evaluating the fake news problem at the scale of the information
ecosystem. Science Advances 6, 14 (April 2020).
[3] Michael Barthel, Amy Mitchell, and Jesse Holcomb. 2016. Many Americans believe
fake news is sowing confusion. https://www.journalism.org/2016/12/15/many-
americans-believe-fake-news-is-sowing-confusion/
[4] Yochai Benkler, Robert Faris, and Hal Roberts. 2018. Network propaganda: Manip-
ulation, disinformation, and radicalization in American politics. Oxford University
Press, New York. 472 pages.
[5] Lia Bozarth, Aparajita Saraf, and Ceren Budak. 2020. Higher ground? How
groundtruth labeling impacts our understanding of fake news about the 2016 U.S.
presidential nominees. In ICWSM.
[6] Media Bias/Fact Check. 2021. Media Bias/Fact Check. https://mediabiasfactcheck.
com/
[7] Media Bias/Fact Check. 2021. Media Bias/Fact Check methodology.
https:
//mediabiasfactcheck.com/methodology/
[8] CrowdTangle. 2021. Introducing Facebook and Instagram video views! https:
//www.crowdtangle.com/resources/videoviews
[9] Facebook. 2021. Why do I see suggested posts in my Facebook News Feed? Facebook.
https://www.facebook.com/help/485502912850153
[10] FactCheck. 2021. Our process. https://www.factcheck.org/our-process/
[11] Robert M. Faris, Hal Roberts, Bruce Etling, Nikki Bourassa, Ethan Zuckerman, and
Yochai Benkler. 2017. Partisanship, propaganda, and disinformation: Online media
and the 2016 U.S. presidential election. Research Paper. Berkman Klein Center for
Internet & Society. http://nrs.harvard.edu/urn-3:HUL.InstRepos:33759251
[12] Christine Geeng, Savanna Yee, and Franziska Roesner. 2020. Fake news on
Facebook and Twitter: Investigating how people (don’t) investigate. In CHI.
[13] Maria Glenski, Tim Weninger, and Svitlana Volkova. 2018. Propagation from
deceptive news sources: Who shares, how much, how evenly, and how quickly?
IEEE Transactions on Computational Social Systems 5, 4 (Dec. 2018), 1071–1082.
[14] Ted Van Green. 2020. Few Americans are confident in tech companies to prevent
misuse of their platforms in the 2020 election. https://www.pewresearch.org/fact-
tank/2020/09/09/few-americans-are-confident-in-tech-companies-to-prevent-
misuse-of-their-platforms-in-the-2020-election/
[15] Andrew Guess, Jonathan Nagler, and Joshua A Tucker. 2019. Less than you think:
Prevalence and predictors of fake news dissemination on Facebook. Science
Advances 5, 1 (Jan. 2019).
[16] Benjamin D Horne, Jeppe Nørregaard, and Sibel Adalı. 2019. Different spirals of
sameness: A study of content sharing in mainstream and alternative media. In
ICWSM. 257–266.
[17] Eslam Hussein, Prerna Juneja, and Tanushree Mitra. 2020. Measuring misinfor-
mation in video search platforms: An audit study on YouTube. In ACM CSCW.
[18] Shan Jiang, Miriam Metzger, Andrew Flanagin, and Christo Wilson. 2020. Model-
ing and measuring expressed (dis)belief in (mis)information. In ICWSM.
[19] Karen Kornbluh, Adrienne Goldstein, and Eli Weiner. 2020. New study by Digital
New Deal finds engagement with deceptive outlets higher on Facebook today than
run-up to 2016 election. Technical Report. The German Marshall Fund of the
United States. https://www.gmfus.org/blog/2020/10/12/new-study-digital-new-
deal-finds-engagement-deceptive-outlets-higher-facebook-today
[20] Merriam-Webster. 2021. Misinformation. https://www.merriam-webster.com/
dictionary/misinformation.
[21] Miriam J Metzger, Andrew Flanagin, Paul Mena, Shan Jiang, and Christo Wilson.
2021. From dark to light: The many shades of sharing misinformation online.
Media and Communication 9, 1 (2021).
[22] Adam Mosseri. 2021.
our apps.
misinformation-and-false-news
Taking action against misinformation across
https://www.facebook.com/formedia/blog/working-to-stop-
[23] Adam Mosseri. 2021. Working to stop misinformation and false news. https:
//www.facebook.com/combating-misinfo
[24] NewsGuard. 2021. NewsGuard. https://www.newsguardtech.com/
[25] NewsGuard. 2021. NewsGuard rating process and criteria.
newsguardtech.com/ratings/rating-process-criteria/
[26] David Pegg, Niamh McIntyre, and Pamela Duncan. 2019. Which election videos
are getting the most attention online? https://www.theguardian.com/politics/
2019/nov/14/which-election-videos-are-getting-the-most-attention-online
[27] Facebook Journalism Project. 2020.
How our fact-checking program
works. https://www.facebook.com/journalismproject/programs/third-party-
fact-checking/how-it-works
[28] Kevin Roose. 2020. Facebook’s Top 10. https://twitter.com/FacebooksTop10
https://www.
[29] Mattia Samory, Vartan Kesiz Abnousi, and Tanushree Mitra. 2020. Characterizing
the social media news sphere through user co-sharing practices. In ICWSM.
[30] Mark Scott. 2021. Thousands of posts around January 6 riots go missing from Face-
book transparency tool. https://www.politico.eu/article/facebook-crowdtangle-
data-january-6-capitol-hill-riots-transparency/
[31] Naomi Shiffman. 2021. CrowdTangle codebook. https://help.crowdtangle.com/
en/articles/3213537-crowdtangle-codebook
[32] Jay J Van Bavel, Elizabeth A Harris, Philip Pärnamets, Steve Rathje, Kim-
berly C Doell, and Joshua A Tucker. 2021. Political psychology in the digital
(mis)information age: A model of news belief and sharing. Social Issues and Policy
Review 15, 1 (Jan. 2021), 84–113.
[33] Clare Wardle and Hossein Derakhshan. 2021.
Information disorder:
Toward an interdisciplinary framework for research and policy mak-
ing.
https://rm.coe.int/information-disorder-toward-an-interdisciplinary-
framework-for-researc/168076277c.
[34] Galen Weld, Maria Glenski, and Tim Althoff. 2021. Political bias and factualness
in news sharing across more than 100,000 online communities. arXiv (Feb. 2021).
A APPENDIX
A.1 Appropriateness of ANOVA the Model
The overall goal of our analysis was to understand whether factual-
ness (i.e., misinformation or non-misinformation) was associated
with significant differences in engagement within the five parti-
sanship groups of news publishers. We first tested more generally
whether engagement was distributed differently between the five
partisanship groups and the binary factualness variable using the
Kolmogorov-Smirnov (KS) test. This test addresses that question
directly though comparison of the empirical CDF (rather than a
measure of centrality alone), and does not rely on the specific distri-
butional assumptions or strict sample size requirements that many
exact tests require. Using the KS test, we made pairwise compar-
isons of the ten partisanship/factualness combinations and adjusted
our p-value threshold to account for multiple comparisons. The KS
test results indicated that the distributions of the ten groups differ.
We further explored these differences by fitting a Multivariate
ANOVA model, given that our data satisfied the general assump-
tions of that model. The ANOVA model has the advantage of being
interpretable with regard to our research question and allowing us
to reason about the interaction of factualness and partisanship.
A.2 Post-Hoc Testing
In cases where the ANOVA F-statistic was significant, we could
conclude that there were in fact significant differences in mean en-
gagement, and performed post-hoc comparisons to identify which
levels of the test were significant. We applied the Tukey HSD test in
a pairwise manner across our combination groups of partisanship
and factualness, and adjusted our p-values using Bonferroni cor-
rection. Table 7 shows as an example the results for the per-page,
per-follower metric from Section 4.2. In that case, the results of the
post-hoc testing confirm the finding of significance of factualness
for explaining differences in engagement per follower in the Center
and Far Right partisanship groups from our ANOVA model. For the
three other metrics (per post, video views, and video engagement),
similar post-hoc testing with the Tukey HSD test confirmed signifi-
cance for all political leanings, likely due to the larger sample sizes
of the misinformation groups.
456
IMC ’21, November 2–4, 2021, Virtual Event
Laura Edelson, Minh-Kha Nguyen, Ian Goldstein, Oana Goga, Damon McCoy, and Tobias Lauinger
Pairwise comparison between groups Meandiff p-adj Lower Upper Reject
Center (N)
Center (N)
Center (N)
Center (N)
Center (N)
Center (N)
Center (N)
Center (N)
Center (N)
Center (M)
Center (M)
Center (M)
Center (M)
Center (M)
Center (M)
Center (M)
Center (M)
Far Left (N)
Far Left (N)
Far Left (N)
Far Left (N)
Far Left (N)
Far Left (N)
Far Left (N)
Far Left (M)
Far Left (M)
Far Left (M)
Far Left (M)
Far Left (M)
Far Left (M)
Far Right (N)
Far Right (N)
Far Right (N)
Far Right (N)
Far Right (N)
Far Right (M)
Far Right (M)
Far Right (M)
Far Right (M)
Slightly Left (N)
Slightly Left (N)
Slightly Left (N)
Slightly Left (M)
Slightly Left (M)
Slightly Right (N)
Center (M)
Far Left (N)
Far Left (M)
Far Right (N)
Far Right (M)
Slightly Left (N)
Slightly Left (M)
Slightly Right (N)
Slightly Right (M)
Far Left (N)
Far Left (M)
Far Right (N)
Far Right (M)
Slightly Left (N)
Slightly Left (M)
Slightly Right (N)
Slightly Right (M)
Far Left (M)
Far Right (N)
Far Right (M)