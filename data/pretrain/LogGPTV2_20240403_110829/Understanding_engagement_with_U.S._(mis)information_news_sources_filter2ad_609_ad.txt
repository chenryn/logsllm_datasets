### Table 6: Median (a) and Mean (b) Interactions per Post by Type from Non-Misinformation (N) Pages, and the Difference for Posts from Misinformation Pages (Delta Relative to Non-Misinformation)

| Political Leaning | Median (a) | Delta (Median) | Mean (b) | Delta (Mean) |
|------------------|-------------|----------------|-----------|---------------|
| Far Right        | 748         | +3.17 k        | 2.26 k    | +1.72 k       |
| Slightly Right   | 4.60 k      | +9.76 k        | 1.57 k    | +23.0 k       |
| Center           | 9.24 k      | +1.55 k        | 2.96 k    | +18.5 k       |
| Slightly Left    | 650         | +1.47 k        | 2.91 k    | +3.16 k       |

**Note:** The values do not add up to the overall aggregate because computations are done independently. Photo, Facebook video, and live video posts from misinformation pages receive significantly higher median and mean engagement per post than posts from non-misinformation pages.

### Analysis of Factualness and Engagement

Using a Multivariate ANOVA model with independent variables and the natural log-transformed distribution of views per video as the dependent variable, we found that the interaction effect of factualness was significant at the 0.05 level for all political leanings (Table 4). Post-hoc testing confirmed the significance of factualness in explaining differences in mean video views for all partisanship groups.

For comparison, we show engagement with the same set of videos in Figure 9b. Disregarding slightly left video, the trends of the medians and means with regard to the impact of a misinformation source on video engagement are the same as they are for video views. To understand differences in mean engagement per video, we used a Multivariate ANOVA model similar to what we have described previously, and found the difference in mean engagement per video due to factualness was statistically significant at the 0.05 level of confidence for all political leanings of the video publisher (Table 4). Post-hoc testing confirmed the significance of factualness in explaining differences in mean video engagement for all partisanship groups.

To further investigate the similarity of video views and engagement, we plot video views against engagement in Figure 9c. This figure suggests that engagement is generally correlated with views, and that to some extent, engagement-based metrics may serve as a substitute for view-based metrics. However, 283 videos received more engagement than views, out of which 246 received more reactions than views. Facebook users likely reacted to the videos without watching them, as users can react only once to each post (whereas they could comment or share multiple times). While we could filter out these pathological cases, we cannot account for similar effects of engaging without viewing among videos with more views than engagement, thus views are not a suitable replacement for the lack of impression data. Using view data might not be a good approximation for studying how many users who were shown a misinformation video also engaged with the video.

### Limitations

The results of our analysis are as imperfect and limited as the data sources that we rely on. Major threats to the validity of our results include potential biases or omissions in the lists of news publishers obtained from NewsGuard and Media Bias/Fact Check. We were unable to discover the Facebook pages of some of these publishers, and our data sets are limited to what is available on CrowdTangle. When publishers delete posts from their Facebook pages, for instance, they also disappear from CrowdTangle, which means that highly controversial posts may be missing in our data set. Our findings reflect the limits of the data available on CrowdTangle; we were able to extract high-level trends in engagement with misinformation news sources but cannot derive any more detailed data-driven explanations of these phenomena. Lastly, we label news sources, not individual news pieces, based on their reputations for factualness and partisanship as judged by NewsGuard and Media Bias/Fact Check. Our boolean misinformation attribute is based on a threshold instead of modeling the degree to which news publishers vary in how much misinformation, fake news, and conspiracy theories they spread. We do not know which percentage of content promoted by misinformation sources contains misinformation, or how much content from a publisher is in line with its partisanship assessment.

### Related Work

There is a large body of work on misinformation, and we focus on studies of misinformation news in online social media. For example, Allcott and Gentzkow [1] explore the economics of misinformation on Facebook during the 2016 election and measure the relative exposure of the U.S. population to fake news. Faris et al. [11] perform a large-scale analysis of content from media sources shared on Facebook via CrowdTangle and observe that the far-right media ecosystem is distinct from the mainstream media in linking behavior, as well as in journalistic standards and practices. Van Bavel et al. [32] explore psychological motivations and propose a model for understanding the reasons users might have for engaging with misinformation. They point out that users share content even when they do not believe it is true, to signal partisan identity.

### Summary of Findings

In the ecosystem of U.S. news sources on Facebook, only in the Far Right does misinformation accumulate more overall engagement than non-misinformation. Across all other political leanings, misinformation providers generate less engagement than non-misinformation news sources, but sometimes still a sizeable share of total engagement (e.g., 37.7% in the Far Left). When looking at the performance of individual news providers and how well they engage with their respective follower base, the advantage of misinformation widens to the Far Left when considering median performance, and additionally includes Slightly Right publishers when considering mean audience engagement. In terms of engagement on a per-post basis, median engagement with posts from misinformation providers is consistently higher across the entire political spectrum, and for average engagement, only Slightly Left misinformation sources are at a disadvantage compared to their non-misinformation counterparts. Similar results hold for views of videos posted by news pages. We also find that while there are fewer misinformation than non-misinformation pages, individual misinformation pages tend to have larger average follower bases.

### Discussion

Given our findings, there is no single answer to the question of how bad Facebook's misinformation problem is. While we found misinformation news to be highly engaging to audiences across the entire political spectrum, in absolute terms, it is driving a significant portion of engagement only on the extreme ends of the spectrum, and particularly on the far right. Our methodology does not give us any insight into why the apparent potential for more misinformation is not exploited to the same degree outside of the far right; this is an interesting topic for future research.

### Recommendations

Facebook data available to researchers on CrowdTangle are currently very limited in scope. While we were able to analyze engagement and views, the data are entirely opaque about content recommendations or impressions, and we cannot disentangle the effects of Facebook’s algorithms, content attractiveness, and user behavior. For example, we were able to show that misinformation content is more engaged with, but in order to study whether it is truly more engaging, the rate of engagement, we would need impression data. Furthermore, in order to study the sources of engagement, we would need the impression data broken down by categories such as followers, non-followers, shares, and sponsored impressions. These data could help researchers understand whether there is a difference in the rate of engagement with misinformation based on partisanship or the type of content promotion. We believe that understanding these factors is crucial to devising effective countermeasures against misinformation.

### Conclusion

In this paper, we studied how much Facebook users engage with (mis)information from U.S. news providers on Facebook. We found that in absolute terms, Far-Right misinformation sources accumulate more engagement than non-misinformation sources of the same partisanship (68.1% of overall Far Right engagement, followed by 37.7% on the Far Left). Even though the misinformation ecosystem outside of the Far Right is smaller in absolute engagement terms, misinformation also performs better than non-misinformation on the Far Left and Slightly Right when considering how publishers engage with their respective follower base. In terms of engagement with posts from misinformation sources, the median is higher for all political leanings. Our findings illustrate that the lure of misinformation is not confined to Far-Right audiences, although there are fewer publishers outside of the Far Right to feed the misinformation ecosystem. We hope that future research will be able to investigate why misinformation generates more engagement.

### Acknowledgments

We wish to thank the employees of CrowdTangle and Facebook who built the tools to enable our analysis. Particular thanks are owed to Naomi Shiffman for her help working with CrowdTangle and her insightful feedback on drafts of this work. Cybersecurity for Democracy at NYU’s Center for Cybersecurity has been supported by Democracy Fund, Luminate, Media Democracy Fund, the National Science Foundation under grant 1814816, Reset, and Wellspring. This research was supported in part by the French National Research Agency (ANR) through the ANR-17-CE23-0014 and the MIAI@Grenoble Alpes ANR-19-P3IA-0003 grants and the European Union’s Horizon 2020 research and innovation programme under grant agreement No 101021377. This material is based upon work supported by the Google Cloud Research Credits program.