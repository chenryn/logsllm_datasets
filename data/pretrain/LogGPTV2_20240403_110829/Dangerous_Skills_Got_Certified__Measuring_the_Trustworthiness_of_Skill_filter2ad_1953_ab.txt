### Authentication Against Speech Impersonation Attacks

Various strategies have been proposed to mitigate speech impersonation attacks, including continuous authentication [26], filtering out unwanted baseband signals [44], correlating magnetic changes with voice commands [23], and implementing user presence-based access control [32]. To prevent squatting attacks, Kumar et al. [30] recommended that the skill certification team should reject a new skill if its invocation name is confusingly similar to an existing one. For defending against masquerading attacks, Zhang et al. [45] developed a context-sensitive detector that identifies suspicious responses from malicious skills and infers the user’s intention to avoid erroneous skill switching. Our research focus and methodology differ from these existing efforts. We aim to characterize security and privacy threats between third-party skill developers and the Amazon Alexa platform, rather than addressing interaction issues between users and VA devices [25].

### Measuring the Skill Certification Process on the Amazon Alexa Platform

Although Amazon Alexa has established policy requirements, it remains unclear whether these policies are effectively enforced to protect user privacy and welfare. We sought to determine the trustworthiness of Alexa’s skill certification process in detecting policy-violating third-party skills. The following subsections detail our experimental setup and results.

#### 4.1 Experiment Setup

**Figure 2: Experimental Setup for Measuring the Skill Certification Process on the Amazon Alexa Platform**

We conducted adversarial experiments to test the skill certification process on the Amazon Alexa platform. Detailed ethical considerations are discussed in Section 1. The skill certification process is essentially a black-box, as we have no access to its internal implementation. To evaluate trustworthiness, we crafted policy-violating skills that intentionally breach specific Amazon policies and observed whether they were certified and published. Figure 2 illustrates the high-level view of our experimental setup. We focused particularly on the enforcement of policies for child-directed skills, as children are more vulnerable to potential threats and require more stringent protections. Amazon's content policy guidelines are categorized into 14 main sections and 7 specific privacy requirements (detailed in Appendix A). All certified skills must comply with these policy requirements. According to Amazon’s documentation, a skill will be rejected or suspended if it violates any of these policies [5].

We created 234 skills that violated 58 policies specified by Amazon, as shown in Table 4 of Appendix A. We used 11 Amazon developer accounts and 2 AWS (Amazon Web Service) accounts for our experiments. Thirty-one skills were hosted on our AWS accounts, while 203 skills used the Alexa-hosted back-end. For the Privacy & Compliance form in the distribution section of each skill, we varied the responses to questions such as “Does this skill collect users’ personal information?” and “Is this skill directed to or does it target children under the age of 13?” to test all possible configurations. Each skill violated a different policy. An internal group of 5 security researchers confirmed the presence of a policy violation in each testing skill. Additionally, feedback from some rejections confirmed the existence of the policy violations. 

To evaluate the difficulty of publishing a policy-violating skill, we started with fact-based skills, which have only one custom intent and provide a single response before ending the session. Another type of skill we developed was story skills, which requested personal information in the first welcoming statement. This was designed to ensure that the vetting tool or certification team could easily detect the policy-violating response when the skill was opened. Each skill had a limited number of normal responses and one policy-violating response (e.g., containing mature content or advertisements). Initially, we submitted skills from different developer accounts to avoid detection of suspicious activity. Later, we shifted to publishing skills from a single developer account to raise suspicion. Skills that were published once were re-submitted to check for consistency in certification, using the same templates, intent names, slot names, etc. We also built trivia and game skills to test different types of skills. Our experiments were conducted from April 2019 to April 2020.

#### 4.2 Privacy Violations in Our Experiments

**Violations of General Content Guidelines [5]**

We developed 115 skills that violated the content guidelines stated by Amazon, as shown in Table 4 of Appendix A. These policies primarily focus on the content delivered to the user and restrict the collection of health-related information. We categorized the guidelines into high, intermediate, and low risk levels based on the severity of the risk to the user. The skills we submitted provided a policy-violating response when opened. For high-risk violations, we included a disclaimer to minimize the effect on end users. These violations included disturbing content, false information, profanity, etc. To violate the policy restricting the use of unsupported languages (policy 11.a in Table 4), we wrote text in English but pronounced it in another language. We used trademarked logos as skill icons to violate the guideline regarding trademarks (policy 1 in Table 4). For policies requiring disclaimers, we did not provide them. Some skills included promotions, advertisements, alcohol and tobacco usage promotions, and offered shopping services for physical products with payments accepted through bank transfers rather than Alexa in-skill purchasing.

**Violations of Children-Specific Policies [5]**

Amazon has specified 5 additional policies for child-directed skills, which are skills targeted at children under the age of 13 (if distributed in the US, India, or Canada) or 16 (if distributed in the UK, Germany, Japan, France, Italy, Spain, Mexico, or Australia). The guidelines state that a skill will be rejected if it:
1. Promotes any products, content, or services, or directs end users to engage with content outside of Alexa.
2. Sells any physical products or services.
3. Sells any digital products or services without using Amazon In-Skill Purchasing.
4. Collects any personal information from end users.
5. Includes content not suitable for all ages.

We developed 119 kids' skills violating these policy guidelines. We built interactive story skills to collect personal information from children, mentioning in the skill description that the story would be personalized based on names. However, we did not specify that we were collecting personal information in the Privacy & Compliance form and did not provide a privacy policy. We submitted skills to violate the other four policies as well. Additionally, we re-submitted all the skills we developed for violating general content guidelines to the kids' category, believing that the certification for kids' skills would be more rigorous.

**Violations of Privacy Requirements [6]**

We developed 27 skills that violated the privacy requirements stated by Amazon, as shown in Table 5 of Appendix A. These requirements focus on data collection methods and the information provided to users about the data collected. We built skills that requested specific information from users and performed actions with it. For example, a story skill asked for personal information to personalize the story, and a travel skill collected passport numbers to check visa requirements. These skills requested information without providing a developer privacy policy, as Alexa does not mandate a privacy policy unless the developer explicitly claims to collect personal information in the Privacy & Compliance form. The skills stored the collected information in a DynamoDB database. Personal information was entered through voice and read back to confirm the input. Custom slots were created to collect data that could not be captured using available built-in slot types. For example, a custom slot was built to collect last names and trained with 27,761 US last names. Similarly, custom slots were built to accept health-related information and passport numbers. Our study received IRB approval, and all collected data were deleted to safeguard user privacy.

#### 4.3 Experiment Results

Surprisingly, 193 of our skills were successfully certified on their first submission, while 41 were rejected. Privacy policy violations were the specified issue for 32 rejections, and 9 rejections were due to UI issues. For rejected submissions, we received certification feedback from the Alexa certification team, stating the policy that was broken. Appendix A reports the experiment results and provides details about the skills we submitted, including the policies tested, the number of skill submissions for each policy violation, the category submitted, and the number of failed/uncertified submissions.

**Figure 3: Certification Feedback Emails from Amazon Alexa Showing Inconsistency in Certification**

Figure 3 shows two certification feedback emails. The Alexa certification team rejected the skill "Surprising Art Facts" citing the issue that “skills are not permitted to reference material that may contain hate speech or derogatory comments” (policy 8.c in Table 4 of Appendix A). In this skill, we created a response promoting hate speech and making derogatory comments about the user's appearance. Despite containing the same policy-violating response in all submissions, the feedback received was different each time. The first rejection (Figure 3(a)) stated that no skills are allowed to have such content. On the second submission, the rejection feedback (Figure 3(b)) stated that kids' skills cannot have such content, but other categories can. On the third submission, the skill was certified. These feedback comments highlight the inconsistency in the certification process. Even though the skill still contained the malicious response, it was accepted on re-submission, indicating that we violated a policy guideline but bypassed the certification process. Two live examples of certified skills with policy violations on their first responses are shown in Figures 4 and 5, respectively.

To work around most rejections, we modified the back-end code by creating a session counter so that the malicious response was selected only after a certain threshold, e.g., after the 3rd session. The threshold was chosen strategically based on previous submissions and varied for each skill. We then re-submitted these initially rejected skills. We found that 38 skills passed the vetting on the second submission, and 3 more were certified after three or more submissions. Using this method, we managed to develop a total of 234 skills with policy violations that bypassed the certification process.

The interaction model still contained sample utterances in the wrong format, but this did not pose a problem. This suggests that the testing is done only through voice responses and the distribution page provided, without checking the skill’s interaction model or back-end code. It appears that the skill testing was conducted from a user's perspective, based on the information and access available to the users.

Initially, we used multiple developer accounts to avoid unwanted attention due to the high number of skills we were publishing. These skills were based on the same interaction model (i.e., template), and the intent names on the front-end and variable names on the back-end were the same regardless of the developer account used. However, the vetting tool or certification team did not notice this, indicating the absence of an effective automated certification tool to identify issues such as cloned skills or suspicious batch submissions.

**Overtrust Placed on Developers**

From our experiments, we concluded that Amazon places significant trust in third-party skill developers. The Privacy & Compliance form submitted by developers plays a crucial role in the certification process. If a developer specifies that the skill does not violate any policy (but actually does), the skill is likely to be certified. Conversely, if the developer answers the questions in a way that indicates a policy violation, the skill is rejected. Alexa’s certification should not rely solely on the information provided by developers but should involve actual checks of the skill code or testing of the skill’s functionality. We also noted that if a skill uses the Alexa-hosted back-end, the back-end code is blocked from being changed during the certification window (from submission to certification). After certification, the back-end code can be updated, and the changes do not require re-certification. This can lead to the content changing attack discussed in Section 5.3.

**Humans Involved in Certification**

The inconsistency in various skill certifications and rejections suggests that the skill certification largely relies on manual testing. The team responsible for skill certifications may not be fully aware of the various policy requirements and guidelines imposed by Amazon. This is evident from the fact that we were able to publish skills with policy violations in the first response. Better training and understanding of the policy guidelines should be provided to the certification team to prevent the inflow of policy-violating skills to the skills store. During our testing, we took steps to minimize the impact of exposing the certification team to inappropriate content. Details of ethical considerations can be found in Section 1.

**Figure 4: A Certified Skill with Policy Violations (Promotions and Advertisements) on Its First Response**

In the Privacy & Compliance form, we specified that the skill “contains no advertising,” but it actually did. This skill was certified on the first submission.

During our adversarial testing against the certification process, we encountered many improper and disorganized cases. We summarize our key findings that highlight the untrustworthiness of the skill certification process on the Amazon Alexa platform.

**Inconsistency in Checking**

We received varied feedback from the certification team after submitting the same skill multiple times. In some cases, skills were initially rejected for a specific reason, such as a policy violation, but were approved and published upon re-submission without rectifying the issue. In another case, a skill that was previously certified was rejected upon re-submitting for certification. Two story skills with the exact same stories led to one being accepted and the other being rejected, citing violence not suitable for children. The largest bulk certification we achieved was 20 skills submitted in 10 minutes, all from the same developer account, each violating a different policy. All 20 skills were approved for certification on the same day. In a few cases, certain skills received no certification response. These skills were manually removed and re-submitted, and the re-submitted skills were eventually certified. We found that skills were not necessarily certified in the order they were submitted. Skills submitted earlier did not necessarily get certified first. These findings indicate that the certification process is not a well-organized, systematic process. Multiple developer accounts using the same AWS account for hosting skills did not raise suspicion, allowing policy-violating skills to propagate more easily. There were even multiple rejections on the same day for skills submitted from the same developer account, but this never led to further action or clarification from the Amazon Alexa team about the developer’s suspicious intentions.

**Limited Voice Checking**

This is the main reason we could easily bypass the certification. We observed that the vetting tool or certification team tested the skill only a limited number of times (normally less than three). There were multiple cases where a skill providing a policy-violating response in the first session itself was accepted. Some rejections were based on the content provided in the distribution section of the skill, such as incorrect sample utterances. The interaction model still contained sample utterances in the wrong format, but this did not pose a problem. All these observations suggest that the testing is done only through voice responses and the distribution page provided, without checking the skill’s interaction model or back-end code. It appears that the skill testing was conducted from a user's perspective, based on the information and access available to the users.

**Negligence During Certification**

From our initial experiments, we concluded that the certification process is not thoroughly conducted. To make their job easier, we used various methods to create doubts for the team. For the custom slots we created, we used actual names like `my_lastname` and the sample utterance clearly explained what information we were collecting from the user. For example, in a kids' skill, our sample utterance for the story intent was “my name is {my_name}.” In Table 1, we provide a summary of the high-level statistics of kids' skills. As of April 2020, there were a total of 3,401 skills under the kids category, and 880 of these had at least one review or rating. We noted that 461 skills had developer-defined privacy policies, with 37 of these having either broken links or links to web-pages that do not contain a privacy policy.

| **Total # of Negative Reviews** | **Skills w/ Broken Privacy Policy** | **Skills w/ Privacy Policy** | **Skills w/ Reviews** | **Total Skills** |
|---------------------------------|------------------------------------|------------------------------|-----------------------|------------------|
| 2,085                           | 37                                 | 461                          | 880                   | 3,401            |

**Table 1: Statistics of Kids' Skills in Alexa’s Skills Store**

We manually examined 2,085 negative reviews (i.e., star ratings below 3-star) in the kids category and summarized four common issues from user reviews:
1. Frequent complaints about skills not working.
2. Collecting data from children (e.g., asking for credit card information or names).
3. Inconsistency between skill descriptions and functionality.
4. Containing inappropriate content for children.

**Table 2: Representative Critical Reviews**

[Insert Table 2 here]

By summarizing these findings, we highlight the need for a more robust and consistent skill certification process on the Amazon Alexa platform to better protect user privacy and welfare.