authentication against speech impersonation attacks, e.g., continu-
ous authentication [26], canceling unwanted baseband signals [44],
correlating magnetic changes with voice commands [23], and user
presence-based access control [32]. To prevent squatting attacks,
Kumar et al. [30] suggested that the skill certification team should
reject a new skill if its invocation name has any confusion with
an existing one. To defend against masquerading attacks, Zhang et
al. [45] built a context-sensitive detector, which detects suspicious
responses from a malicious skill and infers the user’s intention
to avoid erroneously switching to another skill. Our focus and
methodology are different from existing research efforts. We aim at
characterizing security/privacy threats between third-party skill
developers and the Amazon Alexa platform, instead of addressing
interaction issues between users and VA devices [25].
4 MEASURING THE SKILL CERTIFICATION
PROCESS ON AMAZON ALEXA PLATFORM
Though Amazon Alexa has policy requirements in place, it is un-
clear whether these policies have been properly enforced to protect
user privacy and welfare. We are curious to know if Alexa’s skill
certification process is trustworthy in terms of its capability to de-
tect policy-violating third-party skills. In the following subsections,
we describe the details of our experimental setup and the results.
4.1 Experiment Setup
Figure 2: Experiment setup for measuring the skill certifica-
tion process in Amazon Alexa platform.
We performed “adversarial” experiments against the skill cer-
tification process of the Amazon Alexa platform. Detailed ethical
discussions are presented in Sec. 1. The skill certification process
is essentially a black-box since we have no access to its internal
implementation. For testing the trustworthiness, we craft policy-
violating skills that intentionally violate specific policies defined
by Amazon, and examine if it gets certified and published to the
store or not. Fig. 2 illustrates the high-level view of our experiment
setup. We are particularly interested in the policy enforcement for
child-directed skills. Kids are more vulnerable to such potential
threats compared to adults and skills targeted for them require
more stringent policies by VA platforms. Amazon has content pol-
icy guidelines which are categorized into 14 main sections and 7
specific privacy requirements (details in Appendix A). All certified
skills are expected to align with these policy requirements. Ama-
zon’s documentation for the Alexa Skills Kit, states that a skill will
be rejected or suspended if it violates any of these policies4.
4It states that “If Amazon determines that your skill contains, facilitates, or promotes
content that is prohibited by these policy guidelines, we will reject or suspend the
4
We crafted 234 skills that violated 58 policies specified by Ama-
zon as shown in Table 4 of Appendix A. 11 Amazon developer
accounts and 2 AWS (Amazon Web Service) accounts were used
for our experiments. 31 skills were hosted on our AWS accounts
while 203 skills used the Alexa-hosted back-end. For the Privacy &
Compliance form in the distribution section of each skill, we varied
the responses we gave for the questions asked such as “Does this
skill collect users’ personal information?” and “Is this skill directed
to or does it target children under the age of 13?” to test the ef-
fects of all possible configurations. Each skill violated a different
policy. We organized an internal group of 5 security researchers
to confirm the presence of a policy-violation in each testing skill.
In addition, the feedback given for some of the rejections we got
for our skills proved the existence of the policy violation. Since
our aim is to evaluate the level of difficulty in publishing a policy-
violating skill to the store, we started our testing with facts skills
which basically have just one custom intent. These skills give a
single response when opened and then end the session. There is
no extended branching or flow of control within the skill. Another
type of skill that we developed was story skills which asked for
personal information right in the first welcoming statement itself.
This was done to make sure that the vetting tool (or certification
team members) could easily capture the policy-violating response
when the skill is opened and no extra steps had to be taken to
reach it. Each skill has a limited number of normal responses, and a
policy-violating response (e.g., containing mature content or adver-
tisement). Initially, the skill submissions were made from different
developer accounts to evade detection of any suspicious activity.
Later, we shifted our focus to publishing skills from a single devel-
oper account to purposely raise suspicion. The skills which were
published once were re-submitted to check for the consistency in
certification, where same templates, intent names, slot names, etc,
were used for all skills. To test different types of skills, we also built
a few trivia skills and games skills in our study. Our experiments
were conducted from April 2019 to April 2020.
4.2 Privacy Violations in Our Experiments
Violations of General Content Guidelines [5]. We developed
115 skills violating the content guidelines stated by Amazon as
shown in Table 4 of Appendix A. These policies mostly focus on
the content being delivered to the user in a skill. It also restricts
the collection of health related information. We categorized the
guidelines into high, intermediate and low risk-levels according to
the severity of the risk involved in affecting a user. The skills we
submitted delivered a policy-violating response when opened. For
high-risk violations we included a disclaimer to minimize the effect
on end users. These involve disturbing content, false information,
profanity, etc. For breaking the policy of restricting the use of
languages not supported by Amazon (i.e., policy 11.a in Table 4),
we wrote the text in English in a way that it is pronounced in
the other language. We used trademarked logos as the icons for
a skill to violate the guideline regarding trademarks (i.e., policy 1
in Table 4). Certain policies required that a disclaimer needs to be
provided in a skill if it contains certain content. For these cases
submission and notify you using the e-mail address associated with your developer
account” [5].
SkillCertificationSkillsMaliciousdevelopersSkillwith policy violationSkillsStorePublishandthenpull back Examine the certification result we did not provide one. There are also skills that had promotions,
advertisements, alcohol and tobacco usage promotions, etc. Skills
also include offered shopping services for physical products with
payments accepted through a bank transfer rather than the Alexa
in-skill purchasing.
Violations of Children-Specific Policies [5]. In particular,
Amazon has specified 5 extra policies for child-directed skills which
are skills directed to children under the age of 13 (if distributed in
the US, India, or Canada) or 16 (if distributed in the UK, Germany,
Japan, France, Italy, Spain, Mexico, or Australia). The guideline
states that a skill will be rejected, 1) if it promotes any products,
content, or services, or directs end users to engage with content
outside of Alexa, 2) it sells any physical products or services, 3)
it sells any digital products or services without using Amazon
In-Skill Purchasing, 4) it collects any personal information from
end users, or 5) it includes content not suitable for all ages. We
developed 119 kids skills violating the policy guidelines. We built
interactive story skills to collect personal information from children.
We mentioned about personalizing the story based on names in
the skill description. But we did not specify that we are collecting
personal information in the Privacy & Compliance form. We did not
provide a privacy policy for these skills either. Skills were submitted
to violate the other 4 policies as well. In addition, we re-submitted
all the skills that we developed for violating the general content
guidelines to the kids category with the belief that the certification
for kids skills would be much more diligent by the team.
Violations of Privacy Requirements [6]. 27 skills that we
developed violated the privacy requirements stated by Amazon
as shown in Table 5 of Appendix A. These privacy requirements
mostly focus on the collection of data, the method of collection and
the information being provided to the users about the data collected
from them. We built skills that request particular information from
the user and do something with it. Skills that we built included
a story skill that would ask for the users personal information in
order to personalize a story for him/her, a travel skill that would
collect the users’ passport number to check if he/she requires a
visa or not, etc. These skills asked for information from users with-
out providing a developer privacy policy as Alexa doesn’t make it
mandatory to include a privacy policy unless we explicitly claim
that we collect personal information in the Privacy & Compliance
form. These skills were also capable of storing this information
collected in a DynamoDB database. The personal information was
asked to be entered through voice and was also read back to them to
confirm their input. These skills asked for the personal information
in the LaunchRequest intent itself which is the entry point of a
skill. For collecting data that could not be captured using an avail-
able built-in slot type, we created custom slots and trained them
with values that we required. For example, a custom slot was built
to collect last names and was trained with 27,761 US last names.
Similarly, custom slots were built to accept health related informa-
tion, passport numbers, etc. Our study received IRB approval. All
the collected data were deleted to safeguard the users privacy.
4.3 Experiment Results
Surprisingly, we successfully certified 193 skills on their first sub-
mission. 41 skills were rejected. Privacy policy violations were the
5
specified issue for 32 rejections while 9 rejections were due to UI
issues. For the rejected submissions, we received certification feed-
back from the Alexa certification team stating the policy that we
broke. Appendix A reports the experiment results and provides
details about the skills we submitted. These include the policies we
tested, the number of skill submissions for each policy violation, the
category it was submitted to and the number of failed/uncertified
submissions.
Figure 3: Certification feedback emails from Amazon Alexa
showing the inconsistency in certification.
Fig. 3 shows two certification feedback emails. The Alexa cer-
tification team rejected the skill “Surprising Art facts” citing the
issue that “skills are not permitted to reference material that may
contain hate speech or derogatory comments” which is specified
as policy 8.c in Table 4 of Appendix A. In this skill, we created a
response that was promoting hate speech and trying to make a
comment about the users appearance. This skill was certified on
its third submission. While it contained the same policy-violating
response in all submissions, feedback received was different for
each submission. The first rejection (see Fig 3(a)) stated that no
skills are allowed to have such content. On the second submission,
the rejection feedback (shown in Fig 3(b)) stated that kids skill can-
not have such content but the other categories can. On the third
submission, the skill was certified. These feedback comments show
an inconsistency in the certification process. Even though the skill
still had the malicious response that caused the initial rejections, it
was accepted on re-submission. This shows that we did violate one
of the policy guidelines, yet were able to bypass the certification
process. Two live examples of certified skills with policy violations
on their first responses are shown in Fig. 4 and Fig. 5, respectively.
To work around most rejections, we used the same technique of
modifying the back-end code by creating a session counter so that
the malicious response is selected only when the counter reaches
a certain threshold, e.g., after the 3rd session. The threshold was
chosen strategically according to our previous submissions and it
varied for each skill. We then re-submitted these initially rejected
skills. We found that 38 skills passed the vetting on the second sub-
mission, and 3 more were certified after three or more submissions.
Using this simple method we managed to develop a total of 234
skills with policy violations that bypassed the certification process.
c(a)(b)ccSubmitted the same skill,and received different feedback comments.information provided in the distribution section of the skill, such
as wrong sample utterances specified. The interaction model still
contained sample utterances in the wrong format but this didn’t
pose any problem. All these lead to the conclusion that the testing
is done only through voice responses and the distribution page
provided and not by checking the skill’s interaction model or the
back-end code. It appears that the skill testing was done from a
user’s perspective with checks conducted based on the information
and access of the skill available to the users.
In addition, we initially used multiple developer accounts in
order to avoid unwanted attention due to the high number of skills
we were publishing. These skills were based on the same interaction
model (i.e., template), and the intent names on the front-end and
the variable names on the back-end were all the same regardless of
the developer account used. But the vetting tool neglects this or it
did not draw the attention of the certification team, indicating the
absence of an effective automated certification tool which could
identify issues such as cloning of skills or suspicious batch skills.
Overtrust placed on developers. From our experiments, we
understood that Amazon has placed overtrust in third-party skill de-
velopers. The Privacy & Compliance form submitted by developers
plays an important role in the certification process. If a developer
specifies that the skill does not violate any policy (but actually
does), the skill gets certified with a high probability. If the devel-
oper answers the questions in a way that specifies a violation of
any policy, then the skill is rejected on submission. Alexa’s certi-
fication should not be simply based on the information provided
by developers but by actually checking the skill code or testing the
skill’s functionality. We also noticed that if a skill uses the Alexa
hosted back-end, the back-end code is blocked from being changed
during the certification window, i.e., from the time it is submitted
till the time it is certified. But after the skill is certified, the back-end
code is made available for updating again and the changes that are
made from then do not require a re-certification. This can lead to
the content changing attack discussed in Sec. 5.3.
Humans are involved in certification. The inconsistency in
various skill certifications and rejections have led us to believe
that the skill certification largely relies on manual testing. And
the team in charge of skill certifications is not completely aware
of the various policy requirements and guidelines being imposed
by Amazon. This is especially due to the fact that we were able to
publish skills that had a policy violation in the first response. A
better understanding and training of the policy guidelines should be
given to the certification team so as to prevent the inflow of policy-
violating skills to the skills store. During our testing, we took steps
to minimize the impact on the certification team being exposed to
any inappropriate content. Details of ethical consideration can be
found in Sec. 1
Figure 4: A certified skill with policy violations (promotions
and advertisements) on its first response. In the Privacy &
Compliance form, we specified the skill “contains no adver-
tising” but it actually does. This skill got certified on the first
submission.
During our adversarial testing against the certification process,
we encountered many improper and disorganized cases. We sum-
marize our key findings that lead to the untrustworthiness of skill
certification in Amazon Alexa platform.
Inconsistency in checking. We have received varied feedback
from the certification team after submitting the same skill multiple
times. In some cases, skills were initially rejected citing a certain
reason like a policy violation but the same skills on re-submission,
without rectifying the issue, got approved and published. In another
case, a skill that was certified earlier got rejected upon re-submitting
for certification. Two story skills, that had the same exact stories,
on submission led to one skill being accepted and the other being
rejected stating the issue that the story had violence which is not
suitable for children. The largest amount of bulk certifications we
were able to achieve was 20 skills submitted in 10 minutes with
all skills being from the same developer account and each skill
violating a different policy. All 20 skills were approved for certifi-
cation on the same day. In a few cases, we observed that certain
skills received no certification response. These skills were manually
removed and re-submitted. The re-submitted skills were eventually
certified. We found that skills were not necessarily certified in the
order that they were submitted. Skills that were submitted earlier
did not necessarily get certified first. These findings show that the
certification is not a well-organized systematic process. We noticed
that multiple developer accounts using the same AWS account for
hosting the skills did not raise a suspicion either. This is particularly
interesting as this would allow policy-violating skills to propagate
more easily. There were even more than one rejections on the same
day for skills submitted from the same developer account but this
never led to any further action or clarification being asked from
Amazon Alexa team about the developer’s suspicious intentions.
Limited voice checking. This is the main reason we could
easily bypass the certification. We observed that the vetting tool
(or certification team) tested the skill only for a limited number of
times (normally less than three). There were multiple cases where
the skill that provided a response with a policy violation in the
first session itself was accepted. Some rejections were based on the
6
Negligence during certification. From our initial experiments,
we understood that the certification process is not thoroughly con-
ducted. To make their job easier, we used various methods in order
to purposefully create doubts to the team. For the custom slots
that we created, we used the actual names like my_lastname and
the sample utterance also explained clearly what information we
were collecting from the user. For example, in a kids’ skill, our
sample utterance for the story intent was “my name is {my_name}
is specified. In Table 1, we provide a summary of the high-level
statistics of kids’ skills. As of April 2020, there were a total of 3,401
skills under the kids category, and 880 of these had at least one
review or rating. We noted that 461 skills had developer-defined
privacy policies, with 37 of these having either broken links or links
to web-pages that do not contain a privacy policy.
Total # of
negative
reviews
2,085
37
461
880
Skills w/
privacy policy
Skills w/
reviews
Skills w/ broken
privacy policy
Total
skills
3,401
Table 1: Statistics of kids skills in Alexa’s skills store.
We manually examined 2,085 negative reviews (i.e., star ratings
below 3-star) in the kids category, and summarized four common
issues by user reviews: 1) frequent user complaints about skills not
working. 2) collecting data from children (e.g., asking for credit
card information or names); 3) inconsistency of skill descriptions
with their functionality; and 4) containing inappropriate content
for children. Table 2 illustrates some representative critical reviews