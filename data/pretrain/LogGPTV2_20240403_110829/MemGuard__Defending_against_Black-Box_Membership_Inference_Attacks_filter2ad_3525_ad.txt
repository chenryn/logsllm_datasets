|p · д(s + r) + (1 − p) · д(s + 0) − 0.5|
p = argmin
subject to: p · d(s, s + r) + (1 − p) · d(s, s + 0) ≤ ϵ,
(22)
where the constraint means that the expected confidence score
distortion is bounded by the budget. Note that we omit the other
three constraints in Equation 2, Equation 4, and Equation 5. This
is because both of our representative noise vectors already satisfy
those constraints. Moreover, we can derive an analytical solution
to the simplified optimization problem. The analytical solution is
as follows:
(21)
p
(cid:40)0,
p =
min(
d(s,s+r) , 1.0),
ϵ
if |д(s) − 0.5| ≤ |д(s + r) − 0.5|
otherwise.
(23)
One-time randomness: If the defender randomly samples one
of the two representative noise vectors every time for the same
query data sample, then an attacker could infer the true confidence
score vector via querying the same data sample multiple times. We
consider the attacker knows our defense mechanism including the
confidence score distortion metric d, the budget ϵ, and that the
noise vector is sampled from two representative noise vectors, one
of which is 0.
Suppose the attacker queries the same data sample n times from
the target classifier. The attacker receives a confidence score vector
s1 for m times and a confidence score vector s2 for n−m times. One
confidence score vector is s + r and the other is the true confidence
score vector s. Since the attacker receives two different confidence
score vectors, the attacker knows 0 < p < 1. Moreover, given the
two confidence score vectors, the attacker can compute p according
to Equation 23 since the distance d(s, s + r) does not depend on
the ordering of s and s + r, i.e., d(s, s + r) = d(s1, s2). The attacker
can also estimate the probabilities that the defender returns the
confidence score vectors s1 and s2 as m
n , respectively.
If m
n is closer to p, then the attacker predicts that s2 is the true
confidence score vector, otherwise the attacker predicts s1 to be
the true confidence score vector.
n and n−m
To address this challenge, we propose to use one-time random-
ness when the defender samples the representative noise, with
which the defender always returns the same confidence score vec-
tor for the same query data sample. Specifically, for a query data
sample, the defender quantizes each dimension of the query data
sample and computes the hash value of the quantized data sample.
Then, the defender generates a random number p′ in the range
[0, 1] via a pseudo random number generator with the hash value
as the seed. If p′ < p, the defender adds the representative noise
vector r to the true confidence score vector, otherwise the defender
does not add noise. The random number p′ is the same for the
same query data sample, so the defender always returns the same
confidence score vector for the same query data sample. We com-
pute the hash value of the quantized query data sample as the seed
such that the attacker cannot just slightly modify the query data
sample to generate a different p′. The attacker can compute the
random number p′ as we assume the attacker knows the defense
mechanism including the hash function and pseudo random num-
ber generator. However, the attacker does not know p any more
because the defender always returns the same confidence score
vector for the same query data sample. Therefore, the attacker does
not know whether the returned confidence score vector is the true
one or not.
5 EVALUATION
5.1 Experimental Setup
5.1.1 Datasets. We use three datasets that represent different ap-
plication scenarios.
Location: This dataset was preprocessed from the Foursquare
dataset1 and we obtained it from [58]. The dataset has 5,010 data
samples with 446 binary features, each of which represents whether
a user visited a particular region or location type. The data sam-
ples are grouped into 30 clusters. This dataset represents a 30-class
classification problem, where each cluster is a class.
Texas100: This dataset is based on the Discharge Data public use
files published by the Texas Department of State Health Services.2
We obtained the preprocessed dataset from [58]. The dataset has
67, 330 data samples with 6, 170 binary features. These features
represent the external causes of injury (e.g., suicide, drug misuse),
the diagnosis, the procedures the patient underwent, and some
generic information (e.g., gender, age, and race). Similar to [58], we
focus on the 100 most frequent procedures and the classification
task is to predict a procedure for a patient using the patient’s data.
This dataset represents a 100-class classification problem.
CH-MNIST: This dataset is used for classification of different tissue
types on histology tile from patients with colorectal cancer. The
dataset contains 5, 000 images from 8 tissues. The classification
task is to predict tissue for an image, i.e., the dataset is a 8-class
classification problem. The size of each image is 64×64. We obtained
a preprocessed version from Kaggle. 3.
Dataset splits: For each dataset, we will train a target classifier,
an attack classifier, and a defense classifier. Therefore, we split
each dataset into multiple folds. Specifically, for the Location (or
CH-MNIST) dataset, we randomly sample 4 disjoint sets, each of
which includes 1,000 data samples. We denote them as D1, D2, D3,
and D4, respectively. For the Texas100 dataset, we also randomly
sample such 4 disjoint sets, but each set includes 10,000 data samples
as the Texas100 dataset is around one order of magnitude larger.
Roughly speaking, for each dataset, we use D1, D2, and D3 to learn
1https://sites.google.com/site/yangdingqi/home/foursquare-dataset
2https://www.dshs.texas.gov/THCIC/Hospitals/Download.shtm
3https://www.kaggle.com/kmader/colorectal-histology-mnist
Table 2: Neural network architecture of the target classifier
for CH-MNIST.
Table 3: Training and testing accuracies of the target classi-
fier on the three datasets.
Layer Type
Convolution
Activation
Convolution
Activation
Pooling
Convolution
Activation
Convolution
Activation
Pooling
Flatten
Fully Connected
Fully Connected
Activation
Layer Parameters
Input 64 × 64
32 × 3 × 3, strides=(1, 1), padding=same
32 × 3 × 3, strides=(1, 1)
MaxPooling(2 × 2)
32 × 3 × 3, strides=(1, 1), padding=same
ReLU
ReLU
ReLU
ReLU
32 × 3 × 3, strides=(1, 1)
MaxPooling(2 × 2)
512
8
softmax
Output
the target classifier, the attack classifier, and the defense classifier,
respectively; and we use D1 ∪ D4 to evaluate the accuracy of the
attack classifier. We will describe more details on how the sets are
used when we use them.
5.1.2 Target Classifiers. For the Location and Texas100 datasets,
we use a fully-connected neural network with 4 hidden layers as
the target classifier. The number of neurons for the four layers are
1024, 512, 256, and 128, respectively. We use the popular activation
function ReLU for the neurons in the hidden layers. The activation
function in the output layer is softmax. We adopt the cross-entropy
loss function and use Stochastic Gradient Descent (SGD) to learn
the model parameters. We train 200 epochs with a learning rate
0.01, and we decay the learning rate by 0.1 in the 150th epoch for
better convergence. For the CH-MNIST dataset, the neural network
architecture of the target classifier is shown in Table 2. Similarly,
we also adopt the cross-entropy loss function and use SGD to learn
the model parameters. We train 400 epochs with a learning rate
0.01 and decay the learning rate by 0.1 in the 350th epoch. For
each dataset, we use D1 to train the target classifier. Table 3 shows
the training and testing accuracies of the target classifiers on the
three datasets, where the testing accuracy is calculated by using
the target classifier to make predictions for the data samples that
are not in D1.
5.1.3 Membership Inference Attacks. In a membership inference
attack, an attacker trains an attack classifier, which predicts member
or non-member for a query data sample. The effectiveness of an
attack is measured by the inference accuracy of the attack classifier,
where the inference accuracy is the fraction of data samples in
D1 ∪ D4 that the attack classifier can correctly predict as member
or non-member. In particular, data samples in D1 are members of
the target classifier’s training dataset, while data samples in D4 are
non-members. We call the dataset D1 ∪ D4 evaluation dataset. We
consider two categories of state-of-the-art black-box membership
inference attacks, i.e., non-adaptive attacks and adaptive attacks. In
Training Accuracy
Testing Accuracy
Location Texas100 CH-MNIST
100.0%
60.32%
99.98%
51.59%
99.0%
72.0%
non-adaptive attacks, the attacker does not adapt its attack classifier
based on our defense, while the attacker adapts its attack classifier
based on our defense in adaptive attacks.
Non-adaptive attacks: We consider the random guessing attack
and state-of-the-art attacks as follows.
Random guessing (RG) attack. For any query data sample,
this attack predicts it to be a member of the target classifier’s train-
ing dataset with probability 0.5. The inference accuracy of the RG
attack is 0.5.
2 and D′′
Neural Network (NN) attack [56, 58]. This attack assumes
that the attacker knows the distribution of the target classifier’s
training dataset and the architecture of the target classifier. We
further split the dataset D2 into two halves denoted as D′
2 and D′′
2 ,
respectively. The attacker uses D′
2 to train a shadow classifier that
has the same neural network architecture as the target classifier.
After training the shadow classifier, the attacker calculates the con-
fidence score vectors for the data samples in D′
2 , which
are members and non-members of the shadow classifier. Then, the
attacker ranks each confidence score vector and treats the ranked
confidence score vectors of members and non-members as a “train-
ing dataset” to train an attack classifier. The attack classifier takes
a data sample’s ranked confidence score vector as an input and pre-
dicts member or non-member. For all three datasets, we consider
the attack classifier is a fully-connected neural network with three
hidden layers, which have 512, 256, and 128 neurons, respectively.
The output layer just has one neuron. The neurons in the hidden
layers use the ReLU activation function, while the neuron in the
output layer uses the sigmoid activation function. The attack clas-
sifier predicts member if and only if the neuron in the output layer
outputs a value that is larger than 0.5. We train the attack classifier
for 400 epochs with a learning rate 0.01 using SGD and decay the
learning rate by 0.1 at the 300th epoch.
Random Forest (RF) attack. This attack is the same as the
NN attack except that RF attack uses random forest as the attack
classifier, while NN uses a neural network as the attack classifier.
We use scikit-learn with the default setting to learn random forest
classifiers. We consider this RF attack to demonstrate that our
defense mechanism is still effective even if the attack classifier
and the defense classifier (a neural network) use different types of
algorithms, i.e., the noise vector that evades the defense classifier
can also evade the attack classifier even if the two classifiers use
different types of algorithms.
NSH attack [42]. Nasr, Shokri, and Houmansadr [42] proposed
this attack, which we abbreviate as NSH. This attack uses multiple
neural networks. One network operates on the confidence score
vector. Another one operates on the label which is one hot encoded.
Both networks are fully-connected and have the same number of
input dimension, i.e., the number of classes of the target classifier.
Specifically, NSH assumes the attacker knows some members and
(a) Location
(b) Texas100
(c) CH-MNIST
Figure 1: Inference accuracies of different attacks as the confidence score distortion budget (i.e., ϵ) increases.
non-members of the target classifier’s training dataset. In our exper-
iments, we assume the attacker knows 30% of data samples in D1
(i.e., members) and 30% of data samples in D4 (i.e., non-members).
The attacker uses these data samples to train the attack classifier.
We adopt the neural network architecture in [42] as the attack clas-
sifier. The remaining 70% of data samples in D1 and D4 are used to
calculate the inference accuracy of the attack classifier. We train
the attack classifier for 400 epochs with an initial learning rate 0.01
and decay the learning rate by 0.1 after 300 epochs.
Adaptive attacks: We consider two attacks that are customized to
our defense.
Adversarial training (NN-AT). One adaptive attack is to train
the attack classifier via adversarial training, which was considered
to be the most empirically robust method against adversarial exam-
ples so far [3]. We adapt the NN attack using adversarial training
and denote the adapted attack as NN-AT. Specifically, for each data
sample in D′
2 , the attacker calculates its confidence score
vector using the shadow classifier. Then, the attacker uses the Phase
I of our defense to find the representative noise vector and adds it
to the confidence score vector to obtain a noisy confidence score
vector. Finally, the attacker trains the attack classifier via treating
the true confidence score vectors and their corresponding noisy
versions of data samples in D′
2 and D′′
2 as a training dataset.
2 and D′′
Rounding (NN-R). Since our defense adds carefully crafted
small noise to the confidence score vector, an adaptive attack is to
round each confidence score before using the attack classifier to
predict member/non-member. Specifically, we consider the attacker
rounds each confidence score to be one decimal and uses the NN
attack. Note that rounding is also applied when training the NN
attack classifier. We denote this attack NN-R.
Table 4 shows the inference accuracies of different attacks when
our defense is not used. All attacks except RG have inference accu-
racies that are larger or substantially larger than 0.5.
5.1.4 Defense Setting. In our defense, we need to specify a defense
classifier and the parameters in Algorithm 1.
Defense classifier: The defender itself trains a classifier to perform
membership inference. We consider the defense classifier is a neural
network. However, since the defender does not know the attacker’s
attack classifier, we assume the defense classifier and the attack
classifier use different neural network architectures. Specifically,
we consider three different defense classifiers in order to study
the impact of defense classifier on MemGuard. The three defense
Table 4: Inference accuracies of different attacks on the