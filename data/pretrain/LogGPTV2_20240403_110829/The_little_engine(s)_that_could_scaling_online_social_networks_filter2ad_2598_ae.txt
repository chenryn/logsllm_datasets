So, we have full control of the data placement (users and
updates). We implement the Directory Service DS on top
of the same Cassandra nodes showing that we can piggy-
back on the underlying data-store infrastructure. The DS
is distributed across all servers to avoid bottlenecks.
The SPAR middle-ware for Cassandra is written using a
Thrift interface. We describe what the middle-ware does
for the canonical operation of Statusnet; retrieving the last
20 updates (tweets) for a given user. The middle-ware per-
forms three operations: 1) randomly select a Directory Ser-
vice node and request the location of the master replica of
the user by using the get primitive of Cassandra. 2) connect
to the node that hosts the master and perform a get-slice
operation to request the update-id s of the list of the 20 sta-
tus updates to be shown to the user, and ﬁnally 3) do a
multi-get to retrieve the content of all the status updates
and return to Statusnet.
As noted earlier in our evaluation we compare the per-
formance of Statusnet with the SPAR instantiation on Cas-
sandra and the standard vanilla Cassandra (with random
partition). We are interested in answering the following two
question questions: (i) What impact does SPAR have on the
response time as compared to random partitioning? and (ii)
How much does SPAR reduce network traﬃc?
To answer these two questions, we perform the following
set of experiments. We randomly select 40K users out of
the Twitter dataset and issue requests to retrieve the last
20 status updates at a rate of 100, 200, 400 and 800 requests
per second. Note that this requests are not primitive get/set
 1
 0.8
 0.6
F
D
 0.4
C
 0.2
 0
SPAR(800Req/s)
SPAR(400Req/s)
SPAR(200Req/s)
Random(400Req/s)
Random(200Req/s)
Random(100Req/s)
 10
 100
Response Time (ms)
Figure 10: Response time of SPAR on top of Cassandra.
Random
SPAR
s
p
b
M
 70
 60
 50
 40
 30
 20
 10
 0
 50
 100
 150
 200
 250
 300
 350
 400
Request Rate
Figure 11: Network activity in SPAR on top of Cassandra.
operations on Cassandra but application level requests – a
user getting the status updates from her friends.
Examining Response Times: Fig. 10 shows the re-
sponse time of SPAR and the vanilla Cassandra using ran-
dom partitioning. We can see that SPAR reduces the aver-
age response time by 77% (400 requests/second). However,
what it is most interesting is the throughput in the aggre-
gate number of request per second given a realistic quality
of service. SPAR can support 800 req/s with the 99th per-
centile response time below 100ms. Cassandra with random
partitioning can only support 1/4 of the request rate with
the same quality of service, about 200 req/s.
Why does SPAR outperform the random partitioning of
Cassandra? There are multiple reasons and we discuss each
one in brief. First, Cassandra is aﬀected by the delay of
the worst performing server. This is due to the heavy inter-
server traﬃc for remote reads.Our setting runs on Gigabit
Ethernet switches without background traﬃc and hence net-
work bandwidth is not a bottleneck, as Fig. 11 also shows.
The commodity servers, however, often hit network I/O and
CPU bottlenecks in trying to sustain high rates of remote
reads. With SPAR, as all relevant data is local by design,
remote reads are not necessary. A second and less obvious
reason for the better performance of SPAR has to do with
its improved memory hit ratio that comes as a byproduct
of the non-random partitioning of masters. Indeed, a read
for a user brings in memory from the disk the data of the
user as well as those of her friends. Given that masters
of her friends are on the same server with high likelihood,
and that reads are directed to the masters, there is a good
chance that a read for one of these friend will ﬁnd most of the
required data already in memory because of previous reads.
The random partitioning scheme of Cassandra destroys such
correlations and thus suﬀers from a lower memory hit ratio
and the consequent disk I/O penalties.
Analyzing the Network Load: Fig. 11 depicts the
aggregate network activity of the cluster under various re-
quest rates. For random partitioning, requests are spread
among multiple nodes and multi-get operations signiﬁcantly
increases the network load. Compared to a vanilla Cassan-
384dra implementation, SPAR reduces the network traﬃc by a
factor of 8 (for 400 reqs/sec).
7.2 Evaluation with MySQL
We now turn our attention to MySQL, a traditional RDBM
system. The ﬁrst question we want to answer is: can SPAR
scale an OSN application using MySQL? This is impor-
tant as it allows developers to continue using the familiar
RDBMS framework without worrying about scaling issues.
Speciﬁcally, we want to answer if we can make Statusnet
deal with the demand of Twitter as of Dec. 2008.
We use MySQL version 5.5 together with the SQL data
scheme that is provided by Statusnet [5]. The schema con-
tains SQL tables related to the users (table user and proﬁle),
the social graph (subscription), updates (notice) and the list
of updates per user (notice inbox ). We adapted our Twitter
dataset to the Statusnet data scheme, so that it contains all
information about users and status updates. We retrieve the
last 20 status updates (tweets) for a given user by perform-
ing a single query using a join on the notice and notice inbox
tables.
To stress-test our setup we use Tsung [6] and two servers
that we use to emulate the activity for thousands of con-
current users. We generate both application read requests
(retrieve the last 20 status updates for the user) and appli-
cation write requests (a user generates a new status update
and updates the inboxes for all her friends). Our experimen-
tal evaluation consists of multiple 4 minute sessions where
we query for the last status updates of a random subset of
users with a constant request rate. We make sure that every
user is queried only once per session, and that the requests
are spread evenly among servers.
Comparison to Full Replication: First, we check if
a scheme based on Full Replication can work in practice.
This would mean loading the entire Twitter dataset on all 16
servers and measuring the number of users that the system
can serve. The average 95th percentile of the response time
is 113ms for 16 req/s (1 request per second per machine),
151ms for 160 req/s, and 245ms for 320 req/s. As expected,
the 99th percentiles are even higher with 152ms for 16 req/s.
On the other hand, when we use SPAR, the cluster can serve
more than 2, 500 req/s with a 99th percentile of less than
150ms. This shows that SPAR using a MySQL data store
is able to withstand Twitter-scale read loads with a small
cluster of commodity machines, whereas a full replication
system cannot cope.
Note that this experiment shows that the same centralized
code of Statusnet that knows nothing about distributed sys-
tems can still support Twitter-scale loads when using SPAR.
Adding Writes: To further stress-test the system, we in-
troduce insertions of updates. We evaluate the eﬀect of the
insertion of 16 updates/s (1 update/s per machine). In this
part we evaluate only SPAR (with MySQL), as full repli-
cation using MySQL performs very poorly. Note that the
insertion of a new status update to the system can generate
thousands of updates, since the system needs to insert to
the notice inbox table one entry for every user that should
receive the status update (to all the followers in Twitter
terminology). How we treat these inserts is crucial for the
overall performance. A naive implementation, that performs
single or multi inserts into the notice inbox can completely
saturate the system.
We group the inserts and control the rate at which we
introduce them in the system. Under this scenario, we show
that we can achieve a 95th percentile response time below
260ms for 50 read req/s and below 380ms for 200 read req/s
while a constant rate of 16 updates/s. We should note here
that the median response time in both cases is very low,
around 2ms. Performance will only get better as we add
more machines.
In this section, we have shown that using SPAR leads to
high throughput (reqs/sec) and better scalability when com-
pared to full replication solutions and random partitioning,
at the cost a very modest replication overhead.
8. RELATED WORK
To the best of our knowledge this is the ﬁrst work to
address the problem of scalability of the data back-end for
OSNs. In this section we compare and contrast the approach
presented in this paper (and our prior work in the area [28,
29]) with related work in the literature.
Scaling Out: Scaling out web applications is one of the
key features oﬀered by Cloud providers such as Amazon EC2
and Google AppEngine. They allow developers to eﬀort-
lessly add more computing resources on demand or depend-
ing on the current load of the application [4]. This scaling
out however, is only transparent as long as the application is
stateless. This is the case of typical Web front-end layer, or
when the data back-end can be partitioned into independent
components. We deal with scaling of the application back-
end when data is not independent as in the case of OSNs by
providing means to ensure local semantics at the data level.
Key-Value Stores: Many popular OSNs today rely on
DHTs and Key-Value stores to deal with the scaling prob-
lems of the data back-end (e.g. Twitter is migrating to Cas-
sandra). While Key-Value stores do not suﬀer from scala-
bility due to their distributed design, they rely on random
partition of the data across the back-end server. This can
can lead to poor performance in the case of OSN workloads
(as we show in this paper). SPAR improves performance
multi-fold over Key-Value stores as it minimizes network
I/O by keeping all relevant data for a given request local to
the server. Keeping data local helps prevent issues like the
‘multi-get’ hole observed in the typical operation of Key-
Value stores[1].
Distributed File Systems and Databases: Distribut-
ing data for the sake of performance, availability and re-
silience has been widely studied in the ﬁle system and database
systems community. Ficus [15] and Coda [31] are distributed
ﬁle systems that replicate ﬁles for high availability. Farsite
is a distributed ﬁle system that achieves high availability
and scalability using replication [8]. Distributed RDBMS
systems like MySQL cluster and Bayou [33] allow for dis-
connected operations and provide eventual data consistency.
SPAR takes a diﬀerent approach as it does not distribute
data, but maintains it locally via replication. This approach
is more suitable for OSNs since their typical operation re-
quires fetching data from multiples servers in a regular basis.
9. CONCLUSIONS
Scaling OSNs is a hard problem because the data of users
is highly interconnected, and hence, cannot be subjected to
a clean partition. We present SPAR, a system based on
partitioning of the OSN social graph combined with a user-
level replication so that the local data semantics for all users
385is guaranteed. By local semantics, we mean that all relevant
data of the direct neighbors of a user is co-located in the
same server hosting the user. This enables queries to be
resolved locally on a server, and consequently, breaks the
dependency between users that makes scalability of OSNs
so problematic.
Preserving local semantics has many beneﬁts. First, it
enables transparent scaling of the OSN at a low cost. Sec-
ond, the performance beneﬁt in throughput (requests per
second) served increases multifold as all relevant data is lo-
cal and network I/O is avoided. Third, network traﬃc is
sharply reduced.
We designed and validated SPAR using real datasets from
three diﬀerent OSNs. We showed that replication overhead
needed to achieve local data semantics is low using SPAR.
We also demonstrated that SPAR can deal with the dynam-
ics experienced by an OSN gracefully. Further, we imple-
mented a Twitter-like application and evaluated SPAR on
top of a RDBMS (MySQL) and a Key-Value store (Cassan-
dra) using real traces from Twitter. We showed that SPAR
oﬀers signiﬁcant gains in throughput (req/s) while reducing
network traﬃc.
Acknowledgments
We thank the anonymous reviewers and our shepherd Yin
Zhang for their valuable feedback. Special thanks to Evan
Weaver, Zografoula Vagena and Ravi Sundaram for their
early comments and feedback.
10. REFERENCES
[1] Facebook’s memcached multiget hole: More machines
!= more capacity.
http://highscalability.com/blog/2009/10/26/
facebooks-memcached-multiget-hole-more-machines-
more-capacit.html.
[2] Friendster lost lead due to failure to scale.
http://highscalability.com/blog/2007/11/13/friendster-
lost-lead-because-of-a-failure-to-scale.html.
[3] Notes from scaling mysql up or out.
http://venublog.com/2008/04/16/notes-from-scaling-
mysql-up-or-out/.
[4] Rightscale. http://www.rightscale.com.
[5] Status net. http://status.net.
[6] Tsung: Distributed load testing tool.
http://tsung.erlang-projects.org/.
[7] Twitter architecture.
http://highscalability.com/scaling-twitter-making-
twitter-10000-percent-faster.
[8] A. Adya, W. J. Bolosky, M. Castro, G. Cermak,
R. Chaiken, J. R. Douceur, Jon, J. Howell, J. R.
Lorch, M. Theimer, and R. P. Wattenhofer. Farsite:
Federated, available, and reliable storage for an
incompletely trusted environment. In OSDI 02.
[9] M. Armbrust, A. Fox, R. Griﬃth, A. D. Joseph, R. H.
Katz, A. Konwinski, G. Lee, D. A. Patterson,
A. Rabkin, I. Stoica, and M. Zaharia. Above the
clouds: A berkeley view of cloud computing. Technical
Report UCB/EECS-2009-28.
[10] S. Arora, S. Rao, and U. Vazirani. Expander ﬂows,
geometric embeddings and graph partitioning. J.
ACM, 56(2):1–37, 2009.
[11] F. Benevenuto, T. Rodrigues, M. Cha, and
V. Almeida. Characterizing user behavior in online
social networks. In Proc. of IMC ’09, pages 49–62,
New York, NY, USA, 2009. ACM.
[12] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and
E. Lefebvre. Fast unfolding of communities in large
networks. J.STAT.MECH., page P10008, 2008.
[13] G. DeCandia, D. Hastorun, M. Jampani,
G. Kakulapati, A. Lakshman, A. Pilchin,
S. Sivasubramanian, P. Vosshall, and W. Vogels.
Dynamo: amazon’s highly available key-value store.
SIGOPS Oper. Syst. Rev., 41(6):205–220, 2007.
[14] M. R. Garey and D. S. Johnson. Computers and
Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman & Co., New York,
NY, USA, 1979.
[15] R. G. Guy, J. S. Heidemann, W. Mak, T. W. Page,
Jr., G. J. Popek, and D. Rothmeier. Implementation
of the Ficus replicated ﬁle system. In USENIX
Conference Proceedings, 1990.
[16] J. Hamilton. Geo-replication at facebook.
http://perspectives.mvdirona.com/2008/08/21/
GeoReplicationAtFacebook.aspx.
[17] J. Hamilton. Scaling linkedin.
http://perspectives.mvdirona.com/2008/06/08/
ScalingLinkedIn.aspx.
[18] HighScalability.com. Why are facebook, digg and
twitter so hard to scale?
http://highscalability.com/blog/2009/10/13 /why-are-
facebook-digg-and-twitter-so-hard-to-scale.html.
[19] G. Karypis and V. Kumar. A fast and high quality
multilevel scheme for partitioning irregular graphs.
SIAM J. Sci. Comput., 20(1):359–392, 1998.
[20] H. Kwak, Y. Choi, Y.-H. Eom, H. Jeong, and
S. Moon. Mining communities in networks: a solution
for consistency and its evaluation. In ACM IMC ’09.
[21] H. Kwak, C. Lee, H. Park, and S. Moon. What is
twitter, a social network or a news media? 2010.
[22] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph
evolution: Densiﬁcation and shrinking diameters.
ACM Transactions on KDD, 1:1, 2007.
[23] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W.
Mahoney. Community structure in large networks:
Natural cluster sizes and the absence of large
well-deﬁned clusters. CoRR, abs/0810.1355, 2008.
[24] N. Media. Growth of twitter.
http://blog.nielsen.com/nielsenwire/online mobile/
twitters-tweet-smell-of-success/.
[25] A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel,
and B. Bhattacharjee. Measurement and analysis of
online social networks. In ACM IMC ’07.
[26] M. Newman and J. Park. Why social networks are
diﬀerent from other types of networks. Phys. Rev. E,
68:036122, 2003.
[27] M. E. J. Newman. Modularity and community
structure in networks. PNAS, 103:8577, 2006.
[28] J. M. Pujol, V. Erramilli, and P. Rodriguez. Divide
and conquer: Partitioning online social networks.
http://arxiv.org/abs/0905.4918v1, 2009.
[29] J. M. Pujol, G. Siganos, V. Erramilli, and
P. Rodriguez. Scaling online social networks without
pains. In Proc of NETDB, 2009.
[30] J. Rothschild. High performance at massive scale -
lessons learned at facebook.
http://cns.ucsd.edu/lecturearchive09.shtml#Roth.
[31] M. Satyanarayanan. Coda: A highly available ﬁle
system for a distributed workstation environment.
IEEE Transactions on Computers, 39:447–459, 1990.
[32] F. Schneider, A. Feldmann, B. Krishnamurthy, and
W. Willinger. Understanding online social network
usage from a network perspective. In IMC ’09.
[33] D. B. Terry, M. M. Theimer, K. Petersen, A. J.
Demers, M. J. Spreitzer, and C. H. Hauser. Managing
update conﬂicts in bayou, a weakly connected
replicated storage system. In ACM SOSP ’95.
[34] B. Viswanath, A. Mislove, M. Cha, and K. P.
Gummadi. On the evolution of user interaction in
facebook. In Proc of WOSN’09).
386