Monday
e
t
u
n
i
M
/
.
q
e
R
2
1
0
2.5
2
1.5
1
0.5
r
u
o
h
r
e
p
s
t
s
e
u
q
e
r
l
a
t
o
T
0
0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Time (days)
Figure 15: API session management operations and
authentication service requests. The inner plot
shows API session requests under a DDoS.
1
0.8
0.6
F
D
C
0.4
1
0.8
F
D
C
0.6
0.4
0.2
0
10
0
Active sessions
2
10
4
10
6
10
Storage operations per session
0.2
0
0.01s
0.1s
1s
10s 60s
Session length
All sessions
Active sessions
1h
8h 1d
1w 1m
Figure 16: Distribution of session lengths and stor-
age operations per session.
Moreover, similarly to the distribution of user activity, the
inner plot of Fig. 15 shows that 80% of active sessions exhib-
ited at most 92 storage operations, whereas the remaining
20% accounted for 96.7% of all data management operations.
Therefore, the are sessions much more active than others.
A provider may beneﬁt from these observations to opti-
mize session management. That is, depending on a user’s
activity, the provider may wisely decide if a desktop client
works in a pull (cold sessions) or push (active sessions) fash-
ion to limit the number of open TCP connections [24].
8. RELATED WORK
The performance evaluation of cloud storage services is an
interesting topic inspiring several recent papers. Hill et al.
in [25] provide a quantitative analysis of the performance of
the Windows Azure Platform, including storage. Palankar
et al.
in [26] perform an extensive measurement against
Amazon S3 to elucidate whether cloud storage is suitable
for scientiﬁc Grids. Similarly, [27] presents a performance
analysis of the Amazon Web Services. Liu et. al. [18] in-
spected in depth the workload patterns of users in the con-
text of a storage system within a university campus. This
work concentrates on macroscopic storage workload metrics
and type of requests, as well as the diﬀerences in access pat-
terns of personal and shared folders. Unfortunately, these
papers provide no insights into Personal Clouds.
Perhaps surprisingly, despite their commercial popularity,
only few research works have turned attention to analyze
the performance of Personal Cloud storage services [2, 3, 5,
6, 7, 12, 28, 29]. We classify them into two categories:
Active measurements. The ﬁrst analysis of Personal
Cloud storage services we are aware is due to Hu et al. [6]
that compared Dropbox, Mozy, Carbonite and CrashPlan
storage services. However, their analysis was rather incom-
plete; the metrics provided in [6] are only backup/restore
times depending on several types of backup contents. They
also discuss potential privacy and security issues compar-
ing these vendors. The work of Hu et al. [6] was comple-
mented by Gracia-Tinedo et al. [7] that extensively studied
the REST interfaces provided by three big players in the
Personal Cloud arena, analyzing important aspects of their
QoS and potential exploitability [12].
Recently, Drago et al. [3] presented a complete framework
to benchmark Personal Cloud desktop clients. One of their
valuable contributions is to set a benchmarking framework
addressed to compare the diﬀerent data reduction techniques
implemented in desktop clients (e.g, ﬁle bundling).
Passive measurements. Drago et al. [2] presented an
external measurement of Dropbox in both a university cam-
pus and residential networks. They analyzed and character-
ized the traﬃc generated by users, as well as the workﬂow
and architecture of the service. [29] extended that measure-
ment by modeling the behavior of Dropbox users. Similarly,
Mager et al. [28] uncovered the architecture and data man-
agement of Wuala, a peer-assisted Personal Cloud.
Unfortunately, these works do not provide insights on the
provider’s metadata back-end, since this is not possible from
external vantage points. Moreover, [2, 29] study the storage
workload and user behavior on speciﬁc communities (e.g.,
university campus) that may lead to false generalizations.
In contrast, we analyze the entire population of U1.
Furthermore, Li et al.
[5] analyzed a group of Personal
Cloud users in both university and corporate environments.
Combined with numerous experiments, they studied the eﬃ-
ciency of ﬁle sync protocols, as well as the interplay between
data reduction techniques integrated in desktop clients and
users’ workload (update frequency, ﬁle compressibility).
Key diﬀerences with prior work. The main diﬀerence
with previous works is that we study in details the metadata
back-end servers of a Personal Cloud, instead of simply mea-
suring it from outside. The unique aspect of our work is that
most of our insights could have been obtained only by tak-
ing a perspective from within a data center; this goes, for
example, for the internal infrastructure of the service, or the
performance of the metadata store, to name a few.
Also, apart from guiding simulations and experiments, we
believe that the present analysis will help researchers to op-
timize several aspects of these services, such as ﬁle synchro-
nization [4, 10] and security [30], among others.
9. DISCUSSION AND CONCLUSIONS
In this paper, we focus on understanding the nature of
Personal Cloud services by presenting the internal structure
and measurement study of UbuntuOne (U1). The objec-
tives of our work are threefold: (i) to unveil the internal
operation and infrastructure of a real-world provider, (ii) to
reconﬁrm, expand and contribute observations on these sys-
tems to generalize their characteristics, and (iii) to propose
potential improvements for these systems.
This work unveils several aspects that U1 shares with
other large-scale Personal Clouds. For instance, U1 presents
clear similarities with Dropbox regarding the way of decou-
pling data and metadata of users, which seems to be a stan-
dard design for these systems [10]. Also, we found charac-
teristics in the U1 workload that reconﬁrm observations of
prior works [2, 5] regarding the relevance of ﬁle updates, the
eﬀectiveness of deduplication or the execution of user opera-
tions in long sequences, among other aspects. Therefore, our
analysis and the resulting dataset will enable researchers to
get closer to the nature of a real-world Personal Cloud.
166Thanks to the scale and back-end perspective of our study,
we expanded and contributed insights on these services. That
is, we observed that the distribution of activity across users
in U1 is even more skewed than in Dropbox [2] or that the
behavior of domestic users dominate session lengths in U1
compared to other user types (e.g., university). Among the
novelties of this work, we modeled the burstiness of user op-
erations, we analyzed the behavior of ﬁles in U1, we provided
evidences of DDoS attacks to this service, and we illustrated
the performance of the U1 metadata back-end.
An orthogonal conclusion that we extract from our study
is that understanding the behavior of users is essential to
adapt the system to its actual demands and reduce costs. In
the following, we relate some of our insights to the running
costs of U1 as well as potential optimizations, which may be
of independent interest for other large-scale systems:
Optimizing storage matters. A key problem to the survival
of U1 was the growing costs of outsourcing data storage [31],
which is directly related to the data management techniques
integrated in the system. For instance, the fact that ﬁle
updates were responsible for 18.5% of upload traﬃc in U1,
mainly due to the lack of delta updates in the desktop client,
gives an idea of the margin of improvement (§ 5.1). Actually,
we conﬁrmed that a simple optimization like ﬁle-based dedu-
plication could readily save 17% of the storage costs. This
calls to further research and the application of advanced data
reduction techniques, both at the client and server sides.
Take care of user activity. This observation is actually very
important, as we found that 1% of U1 users generated 65% of
traﬃc (§ 6.1), showing a weak form of the Pareto Principle.
That is, a very small fraction of the users represented most
of the OPEX for U1. A natural response may be to limit the
activity of free accounts, or at least to treat active users in
a more cost eﬀective way. For instance, distributed caching
systems like Memcached, data prefetching techniques, and
advanced sync deferment techniques [5] could easily cut the
operational costs down. On the other hand, U1 may bene-
ﬁt from cold/warm storage services (e.g., Amazon Glacier,
f4 [19]) to limit the costs related to most inactive users.
Security is a big concern. Another source of expense for a
Personal Cloud is related to its exploitation by malicious
parties. In fact, we found that DDoS attacks aimed at shar-
ing illegal content via U1 are indeed frequent (§ 5.4).The
risk that these attacks represent to U1 is in contrast to the
limited automation of its countermeasures. We believe that
further research is needed to integrate secure storage proto-
cols and automated countermeasures for Personal Clouds. In
fact, understanding the common behavior of users in a Per-
sonal Cloud (e.g., storage, content distribution) may provide
clues to automatically detect anomalous activities.
Acknowledgment
This work has been funded by the Spanish Ministry of Sci-
ence and Innovation through projects DELFIN (TIN-2010-
20140-C03-03) and “Servicios Cloud y Redes Comunitarias”
(TIN-2013-47245-C2-2-R) as well as by the European Union
through projects FP7 CloudSpaces (FP7−317555) and H2020
IOStack (H2020-644182). We also thank Jordi Pujol-Ahull´o
for his feedback in the latest versions of this paper.
10. REFERENCES
[1] F. Research, “The personal cloud: Transforming
personal computing, mobile, and web markets.”
http://www.forrester.com, 2011.
[2] I. Drago, M. Mellia, M. M Munafo, A. Sperotto,
R. Sadre, and A. Pras, “Inside dropbox:
understanding personal cloud storage services,” in
ACM IMC’12, 2012, pp. 481–494.
[3] I. Drago, E. Bocchi, M. Mellia, H. Slatman, and
A. Pras, “Benchmarking personal cloud storage,” in
ACM IMC’13, 2013, pp. 205–212.
[4] Z. Li, C. Wilson, Z. Jiang, Y. Liu, B. Y. Zhao, C. Jin,
Z.-L. Zhang, and Y. Dai, “Eﬃcient batched
synchronization in dropbox-like cloud storage
services,” in ACM/IFIP/USENIX Middleware’13,
2013, pp. 307–327.
[5] Z. Li, C. Jin, T. Xu, C. Wilson, Y. Liu, L. Cheng,
Y. Liu, Y. Dai, and Z.-L. Zhang, “Towards
network-level eﬃciency for cloud storage services,” in
ACM IMC’14, 2014.
[6] W. Hu, T. Yang, and J. Matthews, “The good, the
bad and the ugly of consumer cloud storage,” ACM
SIGOPS Operating Systems Review, vol. 44, no. 3, pp.
110–115, 2010.
[7] R. Gracia-Tinedo, M. S´anchez-Artigas,
A. Moreno-Mart´ınez, C. Cotes, and P. Garc´ıa-L´opez,
“Actively measuring personal cloud storage,” in IEEE
CLOUD’13, 2013, pp. 301–308.
[8] R. Sears, C. Van Ingen, and J. Gray, “To blob or not
to blob: Large object storage in a database or a
ﬁlesystem?” Microsoft Research, Tech. Rep., 2007.
[9] J. Li, N. K. Sharma, D. R. Ports, and S. D. Gribble,
“Tales of the tail: Hardware, os, and application-level
sources of tail latency,” in ACM SoCC’14, 2014.
[10] P. Garc´ıa-L´opez, S. Toda-Flores, C. Cotes-Gonz´alez,
M. S´anchez-Artigas, and J. Lenton, “Stacksync:
Bringing elasticity to dropbox-like ﬁle
synchronization,” in ACM/IFIP/USENIX
Middleware’14, 2014, pp. 49–60.
[11] “FP7 cloudspaces EU project,” http://cloudspaces.eu.
[12] R. Gracia-Tinedo, M. S´anchez-Artigas, and
P. Garc´ıa-L´opez, “Cloud-as-a-gift: Eﬀectively
exploiting personal cloud free accounts via REST
APIs,” in IEEE CLOUD’13, 2013, pp. 621–628.
[13] E. Hammer-Lahav, “The OAuth 1.0 Protocol,”
http://tools.ietf.org/html/rfc5849, 2010.
[14] M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W.
Shirriﬀ, and J. K. Ousterhout, “Measurements of a
distributed ﬁle system,” in ACM SIGOPS Operating
Systems Review, vol. 25, no. 5, 1991, pp. 198–212.
[15] N. Agrawal, W. J. Bolosky, J. R. Douceur, and J. R.
Lorch, “A ﬁve-year study of ﬁle-system metadata,”
ACM Transactions on Storage, vol. 3, no. 3, p. 9, 2007.
[16] A. W. Leung, S. Pasupathy, G. R. Goodson, and E. L.
Miller, “Measurement and analysis of large-scale
network ﬁle system workloads.” in USENIX ATC’08,
vol. 1, no. 2, 2008, pp. 5–2.
[17] W. Hsu and A. Smith, “Characteristics of i/o traﬃc in
personal computer and server workloads,” IBM
Systems Journal, vol. 42, no. 2, pp. 347–372, 2003.
[18] S. Liu, X. Huang, H. Fu, and G. Yang,
“Understanding data characteristics and access
patterns in a cloud storage system,” in IEEE/ACM
CCGrid’13, 2013, pp. 327–334.
[19] S. Muralidhar, W. Lloyd, S. Roy, C. Hill, E. Lin,
W. Liu, S. Pan, S. Shankar, V. Sivakumar, L. Tang,
and S. Kumar, “f4: Facebook’s warm blob storage
system,” in USENIX OSDI’14, 2014, pp. 383–398.
167dal.add_part_to_uploadjob
dal.delete_uploadjob
dal.get_reusable_content
dal.get_uploadjob
dal.make_content
dal.make_uploadjob
dal.set_uploadjob_multipart_id
dal.touch_uploadjob
Continues a multipart upload by adding a new chunk.
Garbage-collects the server-side state for a multipart upload, either because of commit or cancellation.
Check whether the server already has the content that is being uploaded.
Get the server-side state for a multipart upload.
Make a ﬁle entry in the metadata store (the equivalent of an inode).
Set up the server-side structure for multipart upload.
Set the requested Amazon S3 multipart upload id to the uploadjob.
Check if the client has canceled the multipart upload (garbage collection after a week).
Table 4: Upload related RPC operations that interact with the metadata store.
[20] J. Mirkovic and P. Reiher, “A taxonomy of DDoS
attack and DDoS defense mechanisms,” ACM
SIGCOMM Computer Communication Review,
vol. 34, no. 2, pp. 39–53, 2004.
[21] A.-L. Barabasi, “The origin of bursts and heavy tails
in human dynamics,” Nature, vol. 435, no. 7039, pp.
207–211, 2005.
[22] M. E. Crovella and A. Bestavros, “Self-similarity in
world wide web traﬃc: evidence and possible causes,”
IEEE/ACM Transactions on Networking, vol. 5, no. 6,
pp. 835–846, 1997.
[23] S. H¨at¨onen, A. Nyrhinen, L. Eggert, S. Strowes,
P. Sarolahti, and M. Kojo, “An experimental study of
home gateway characteristics,” in ACM IMC’10, 2010,
pp. 260–266.
[24] P. Deolasee, A. Katkar, A. Panchbudhe,
K. Ramamritham, and P. Shenoy, “Adaptive
push-pull: disseminating dynamic web data,” in ACM
WWW’01, 2001, pp. 265–274.
[25] Z. Hill, J. Li, M. Mao, A. Ruiz-Alvarez, and
M. Humphrey, “Early observations on the performance
of windows azure,” in ACM HPDC’10, 2010, pp.
367–376.
[26] M. R. Palankar, A. Iamnitchi, M. Ripeanu, and
S. Garﬁnkel, “Amazon S3 for science grids: a viable
solution?” in ACM DADC’08, 2008, pp. 55–64.
[27] A. Bergen, Y. Coady, and R. McGeer, “Client
bandwidth: The forgotten metric of online storage
providers,” in IEEE Paciﬁc Rim Conference on
Communications, Computers and Signal Processing,
2011, pp. 543–548.
[28] T. Mager, E. Biersack, and P. Michiardi, “A
measurement study of the wuala on-line storage
service,” in IEEE P2P’12, 2012, pp. 237–248.
[29] G. Gon¸calves, I. Drago, A. P. C. da Silva, A. B.
Vieira, and J. M. Almeida, “Modeling the dropbox
client behavior,” in IEEE ICC’14, vol. 14, 2014.
[30] M. Mulazzani, S. Schrittwieser, M. Leithner,
M. Huber, and E. Weippl, “Dark clouds on the
horizon: Using cloud storage as attack vector and
online slack space.” in USENIX Security, 2011.
[31] J. Silber, “Shutting down Ubuntu One ﬁle services,”
http://blog.canonical.com/2014/04/02/shutting-
down-ubuntu-one-ﬁle-services/, April 2014.
APPENDIX
A. UPLOAD MANAGEMENT IN U1
The management of ﬁle uploads is one of the most complex
parts in the U1 architecture8. Speciﬁcally, U1 resorts to the
multipart upload API oﬀered by Amazon S39. The lifecycle
8Downloads are simpler: API servers only perform a single
request to Amazon S3 for forwarding the data to the client.
9
http://docs.aws.amazon.com/AmazonS3/latest/dev/
UsingRESTAPImpUpload.html
Figure 17: Upload state machine in U1.
of an upload is closely related to this API, where several U1
RPC calls are involved (see Table 4).
Internally, U1 uses a persistent data structure called up-
loadjob that keeps the state of a multipart ﬁle transfer be-
tween the client and Amazon S3. The main objective of
multipart uploads in U1 is to provide user with a way of
interrupting/resuming large upload data transfers. upload-
job data structures are stored in the metadata store during
their life-cycle. RPC operations during the multipart upload
process guide the lifecycle of uploadjobs (see Fig. 17).
Upon the reception of an upload request, U1 ﬁrst checks
if the ﬁle content is already stored in the service, by means
of a SHA-1 hash sent by the user. If deduplication is not
applicable to the new ﬁle, a new upload begins. The API
server that handles the upload sends an RPC to create an
entry for the new ﬁle in the metadata store.
In the case of a multipart upload, the API server creates a
new uploadjob data structure to track the process. Subse-
quently, the API process requests a multipart id to Amazon
S3 that will identify the current upload until its termination.
Once the id is assigned to the uploadjob, the API server up-
loads to Amazon S3 the chunks of the ﬁle transferred by the
user (5MB), updating the state of the uploadjob.
When the upload ﬁnishes, the API server deletes the up-
loadjob data structure from the metadata store and notiﬁes
Amazon S3 about the completion of the transfer.
Finally, U1 also executes a periodic garbage-collection pro-
cess on uploadjob data structures. U1 checks if an upload-
job is older than one week (dal.touch_uploadjob). In the
aﬃrmative case, U1 assumes that the user has canceled this
multipart upload permanently and proceeds to delete the
associated uploadjob from the metadata store.
168