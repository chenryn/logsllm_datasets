uring 662 trust anchors and updating these trust anchors is
tedious at best, but this clearly becomes infeasible as the
number of secure zones moves from hundreds to millions.
A resolver may choose to only conﬁgure some of the 662
trust anchors and Figure 7 shows the percentage of secure
zones that can be veriﬁed if the resolver conﬁgures trust
anchors in a greedy manner. A resolver that conﬁgures the
top 10 trust anchors can verify data in 25% of the secure
zones. This is because a small number of zones participate
in authentication chains. By conﬁguring the trust anchor for
some zone zj, a resolver may also be able to verify data from
other secure descendants of zj. Unfortunately most zones
are not part of an authentication chain and conﬁguring the
trust anchor for zi allows the resolver to verify data from
only zi.
It is also important to note that conﬁguring a trust anchor
is not a one-time operation. Whenever a key-set that exists
in a trust anchor list is changed, the trust anchor list must
be updated. The churn in large trust anchor lists increases
operational and conﬁguration overhead.
Our veriﬁcation metric from the previous section captures
the added conﬁguration challenge. There are currently 662
trust anchors for 871 zones resulting in:
V f = 1 − 662 − 1
871
= 0.241
In an ideal deployment, there would be a single trust an-
chor and we would have a score V f = 1. Earlier monitoring
results seemed to suggest that V f was improving over time
and some longer authentication chains were formed. But un-
fortunately this improvement in V f proved to be an artifact
of testing. Several large collections of test zones were de-
ployed and connected via authentication chains. These test
zones help operators experiment with managing authentica-
tion chains, but don’t reﬂect production use and many of
these test conﬁgurations are operated by a single organiza-
tion, so true large scale inter-administration testing is still
needed.
After removing the test zones, there has been little mean-
ingful change in the V f value. For example, in October
10th, 2007 V f = 1 − 634−1
815 = 0.223. On the positive side,
a number of ccTLDs (notably se, bg, br, pr ) have deployed
DNSSEC and could be potentially become trust anchors for
large numbers of zones.
We deﬁne an island of security as a zone z and all se-
cure descendants of z that can be reached by authentica-
tion chain starting at z. Thus the size of an island is the
number of secure zones in the island4. A single zone that
deploys DNSSEC but does not coordinate authentication
chains with its parent or any of its children forms an island
of size 1. Today there are 662 distinct islands of security in
our study and 97.4% of them have size 1.
Figure 8 shows the current size of the largest islands. In
addition to island size, the number of distinct administra-
tive domains within the island is also important. We believe
Internet cryptographic systems are interesting due to both
their large size and their large number of independent ad-
ministrative authorities. For example, an island of security
that includes 60 zones operated by 60 diﬀerent organizations
requires coordinating authentication chains across diﬀerent
organization boundaries and, in our view, is more interesting
than an island operated by a single administrative domain.
To infer whether an island includes multiple administra-
tive domains, we analyzed the number of unique sets of
nameservers serving the zones in each island. If two zones
are served by the same set of name servers, we assume these
zones are operated by the same administrative domain. Fig-
ure 8 shows that among the largest observable islands, many
still consist of a relatively small number of administrative
domains. The largest island of security includes over 60 se-
cure zones, but only 1 administrative domain.
Reducing the number of required trust anchors and cre-
ating large diverse islands of security are perhaps the most
fundamental challenges facing DNSSEC deployment.
4.3 Validity
As discussed in Section 3.3, validity is distinct from veriﬁ-
cation. Due to operator errors, design ﬂaws, implementation
4If DNSSEC were fully deployed, there would be a single
island of security with the root zone as its trust anchor and
its size would be the total number of DNS zones.
 0 10 20 30 40 50 60 70 80 90 100 0 100 200 300 400 500 600 700% of Zones CoveredNumber of configured Trust AnchorsTrust Anchor Coverage CDF of all ZonesDegree of Coverage of All Zones 0 10 20 30 40 50 60 70se.8.d....arpa.har...s.net.zx.com.nln...bs.nl.176....arpa.20.....arpa.SizeIsland RankSize of Island vs. # of separate Administrative DomainsSize of IslandIndependent Admin Domainsbugs, or intentional attacks, invalid data may be veriﬁed by a
resolver (false positives). Similarly, valid data may fail a ver-
iﬁcation check (false negatives). Although, our monitoring
did not detect intentional attacks, the lack of active attacks
was not surprising given the current state of deployment.
Instead, our results focus on two areas where operational
practices lead to false negatives via broken authentication
chains and false positives where stale data can be replayed.
False Negatives: In order to authenticate zone data, a
resolver must be able to obtain the zone’s public key. The
discussion above shows most of these public keys need to be
manually conﬁgured as trust anchors. For the other public
keys that can be reached via authentication chains, we con-
sider how well these authentication chains are maintained.
In particular, a secure delegation (DS) record stored at the
parent zone must match a DNSKEY stored at the child zone.
As of January 17th, 2008, our pollers had observed 1,730 DS
records and 1,573 of these records matched DNSKEYs in the
child.
Vdeleg =
1, 573
1, 730
= 0.909
If a zone stores only one DS record for a child, and this DS
record fails to match a DNSKEY, then the authentication
chain is broken. There is no way for a resolver to verify
the child zone’s public key. The results above suggest that
9% of the authentication chains observed by our poller were
broken and data veriﬁcation would have failed for all data
in the aﬀected child zone and all its descendents.
On the other hand, if there are multiple DS records for
a child stored at the parent, it may be the case that one
authentication chain works and the other broken DS records
are simply old data that the parent has been slow to remove.
However, DNSSEC envisioned that a parent zone would have
exactly one DS record for each child. Even during a key roll
over (e.g. when the child zone changes its DNSKEY), there
is still exactly one DS record at the parent at all times.
This is accomplished by having multiple DNSKEY records
at the child and rollover procedures are described in detail
in [10]. During our study 175 zones always had exactly one
DS record at the parent and 75 zones had multiple records
at the parent.
False Positives: While we did not observe any active at-
tacks against secure zones, we did observe operational prac-
tices that would allow a misconﬁgured cache or attacker to
replay stale data. Our analysis was focused on infrastructure
records used by resolvers to navigate the DNS tree hierarchy.
Speciﬁcally, we considered whether an attacker or miscon-
ﬁgured cache might be able to replay stale DNSKEY, DS,
SOA, NS, and associated A RRsets.
Due to the potential impact of replaying these records,
we tracked changes in them and determined whether the
stale value could be replayed as described in Section 3.3.
Figure 9 breaks the set of secure zones into buckets based
on the number of stale RRsets that were associated with
the zone. Each zone is quantized based on whether is has
0, 1-10, 11-100, or more than 100 stale RRsets on each day.
The results show that for some time, zones tended to have
quite a few stale sets associated with them. The graphs
show that in December 2006 there were 10 zones with over
100 stale infrastructure records that could be intentionally
or unintentionally replayed.
This is primarily caused by zones selecting long signature
Figure 9: This ﬁgure plots the number of zones that have
either 0, 1-10, 11-100, or more than 100 stale RRsets associ-
ated with them over time.
lifetimes. For example if a DS record is signed using a one
year signature lifetime and changes only a few days later, the
stale DS record can be replayed until the year long signature
expires. Since DNSSEC includes no revocation mechanism,
selecting long signature lifetimes creates a long period where
stale data may be replayed and veriﬁed by unsuspecting re-
solvers.
Early in 2007, the zones with more than 100 Rstale began
to decline in number. In fact, currently, there are no more
than a few zones that have more than 100 stale RRsets.
This decline roughly corresponded in time with stale data
monitoring results being available on our monitoring site
and appearing on deployment mailing lists.
Based on the characterization in Section 3.3 we represent
the state of DNSSEC from January 17th, 2008 as:
Vf resh = 1 − 4, 418
22, 329
= 0.802
The longitudinal evaluation of the Vdeleg dimension, above,
shows improvement when contrasted with Vf resh from Oc-
tober 10th, 2007: Vf resh = 1 − 14,476
27,196 = 0.468. Here we see
an evident trend.
Overall, this calculates the validity as of October 10th,
2007, and then on January 17th, 2008 as a tuple:
h0.893, 0.468i → h0.909, 0.802i
The merit of these absolute values is subject to debate.
However, we present their relative values as systematic met-
rics that capture certain deployment speciﬁcs.
In this re-
gard, we note that there is a dramatic increase in fresh-
ness of DNSSEC’s validity. A qualitative interpretation of
this would indicate that signiﬁcantly fewer chances exist
for resolvers to encounter stale, or misconﬁgured data in
DNSSEC.
5. DISCUSSION AND CONCLUSIONS
Over a few years of monitoring, we have collected a vast
amount of data on DNSSEC’s deployment. Our goals have
consistently been to help inform operational practices with
actual data. For example, the timing of our discovery and
 1 10 100 100007/0609/0611/0601/0703/0705/0707/0709/0711/0701/0803/08Number of ZonesTimeDistribution of Number of Stale RRsets per zone01 - 1011 - 100> 100dissemination of RRset staleness coincided with a large drop
in its incidence. We posit that some operational groups be-
came aware of the implications of rapid re-signing of their
zones and adjusted this behavior. These simple changes help
improve the overall DNSSEC system and demonstrate the
value of distributed monitoring.
More generally, we have presented a set of metrics that
quantify the DNSSEC deployment in ways that proved quite
useful. These metrics have allowed us to collapse massive
volumes of data into a few simple quantiﬁable values whose
results helped shape further analysis and forensics surround-
ing operational failure modes. Our use of these metrics has
revealed 3 fundamental challenges: First, data in Internet
systems is not always universally available. Issues such as
PMTU limitations, transient failures, and misconﬁgurations
are a fact of life for these systems. Using our availability
metric as an indicator, we have gauged the severity of this
PMTU problem and can now design solutions.
Second, our veriﬁability metric clearly illustrates a fun-
damental challenge facing all cryptographic deployments in
the Internet; how does one obtain the the trust anchor in-
formation (e.g.
its public key) in a secure, veriﬁed, and
robust way? DNSSEC directly addressed this problem by
designing a hierarchical PKI which minimizes the neces-
sary trust anchor to one, however its design assumptions
are not congruent with the common requirement that ev-
ery party in the Internet tends to make their own decision
about whether/when they may deploy new functions. From
the facts that the Internet does not have a central author-
ity and that not everyone trusts the same parties, one may
conjecture that there may necessarily be multiple trust an-
chors, making the problem more diﬃcult. How best to solve
this cryptographic bootstrapping problem remains a critical
open question.
The DNSSEC community has taken notice of some of the
problems discussed herein and is exploring “look-aside val-
idation” (DLVs [20]) where some central site or sites verify
many public keys and become de facto authorities. We are
also working on a novel solution that uses our monitoring
apparatus as a diverse lookup infrastructure that can look
for DNSKEY consistency and provide a repository of DNSKEYs
which, although not cryptographically veriﬁed, form a con-
sistent view from multiple diverse locations and whose cor-
rectness can be double-checked by individual key owners.
Finally, even early deployment shows DNSSEC is a highly
dynamic and a continuously evolving system. Thus, its be-
haviors must be continuously monitored to capture new fail-
ures and challenges. By measuring one gets data and that
can inform a system’s design, by quantifying data one can
decipher its meaning and gauge the progress, and by mon-
itoring one is able discover problems as they arise so that
designs can be revisited.
6. ACKNOWLEDGEMENTS
This work is partially supported by the National Science
Foundation under Contract No. CNS-0524854. Opinions
and ﬁndings expressed in this paper are those of the authors
and do not necessarily reﬂect the views of NSF.
7. REFERENCES
[1] DNS anomalies and their impacts on DNS cache
servers.
http://www.nanog.org/mtg-0410/pdf/toyama.pdf.
[2] DNSSEC incident report, broadband routers.
http://www.dnssec-deployment.org/wg/materials/
20071107/dnssec_incident_en.pdf.
[3] SecSpider. http://secspider.cs.ucla.edu/.
[4] R. Arends, R. Austein, M. Larson, D. Massey, and
S. Rose. DNS Security Introduction and Requirement.
RFC 4033, March 2005.
[5] R. Arends, R. Austein, M. Larson, D. Massey, and
S. Rose. Protocol Modiﬁcations for the DNS Security
Extensions. RFC 4035, March 2005.
[6] R. Arends, R. Austein, M. Larson, D. Massey, and
S. Rose. Resource Records for the DNS Security
Extensions. RFC 4034, March 2005.
[7] D. Atkins and D. Austein. Threat Analysis of the
Domain Name System (DNS). RFC 3833, August
2004.
[8] S. M. Bellovin. Using the domain name system for
system break-ins. In Proceedings of the Fifth Usenix
Unix Security Symposium, pages 199–208, 1995.
[9] S. Kent, C. Lynn, and K. Seo. Secure border gateway
protocol (S-BGP). IEEE Journal on Selected Areas in
Communications, 18(4):582–592, 2000.
[10] O. Kolkman and R. Gieben. DNSSEC Operational
Practices. RFC 4641, NLnet Labs, September.
[11] R. Mahajan, D. Wetherall, and T. Anderson.
Understanding bgp misconﬁguration. In SIGCOMM
2002, pages 3–16, New York, NY, USA, 2002. ACM.
[12] J. Mogul and S. Deering. Path MTU Discovery. RFC
1191, DECWRL and Stanford University, November
1990.
[13] J. Ng. Extensions to BGP to Support Secure Origin
BGP (soBGP). Internet draft, Network WG, April
2004.
[14] U. of Oregon. Route Views Project.
http://www.routeviews.org.
[15] E. Osterweil, D. Massey, and L. Zhang. Observations
from the DNSSEC Deployment. In The 3rd workshop
on Secure Network Protocols (NPSec), 2007.
[16] E. Osterweil, V. Pappas, D. Massey, and L. Zhang.
Zone state revocation for dnssec. In LSAD ’07:
Proceedings of ACM Sigcomm Workshop on Large
Scale Attack Defenses, 2007.
[17] V. Pappas, Z. Xu, S. Lu, D. Massey, A. Terzis, and
L. Zhang. Impact of Conﬁguration Errors on DNS
Robustness. In ACM SIGCOMM, 2004.
[18] S. Stamm, Z. Ramzan, and M. Jakobsson. Drive-by
pharming. In ICICS, pages 495–506, 2007.
[19] A. C. Weaver. Secure sockets layer. Computer,
39(4):88–90, 2006.
[20] S. Weiler. DNSSEC lookaside validation (DLV). RFC
5074, SPARTA Inc., November 2007.