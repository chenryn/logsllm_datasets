P (a = 1) =
=
Fcode(1) · Fdata(1)
oneg · (1 − opos)
Fcode(1) · Fdata(1) + Fcode(0) · Fdata(0)
oneg · (1 − opos) + opos · (1 − oneg)
Fig. 9: Factor Graph for Each Address
Speciﬁcally,
critical property: a rewritten instruction should evaluate to
the same value(s) as its original version. This ensures all data
accesses (to the original space) are not broken. For example, a
rewritten read of rip must be patched with an offset such that
the read yields the corresponding value in the original space
as the rewritten read must be executed in the shadow space.
it performs the following code transforma-
tions. It directly patches direct jump instructions by an offset
statically computed based on the offset between the shadow
and original address spaces and the instrumentations. The
computation of such offset is standard and elided [38]. It
instruments all indirect jumps to perform a runtime address
lookup that translates the target to the shadow space. It may
throw an intentional segfault if it detects the target is not in the
shadow space, meaning the corresponding code has not been
rewritten. Client analysis instrumentation such as coverage
tracking code is inserted in the shadow space.
Handling Call Instructions to Support Data Accesses
through Return Addresses. There are programs that access
data using addresses computed from some return address on
the stack. As such, we need to ensure return addresses saved
on the stack must be those in the original space. Therefore,
STOCHFUZZ rewrites a call instruction to a push instruc-
tion which pushes a patched return address (pointing to the
original address) to the stack, followed by a jmp instruction
to the callee in the shadow space. We then instrument ret
instructions to conduct on-the-ﬂy lookup just like in handling
indirect jumps.
Our design allows keeping the control ﬂow in the shadow
space as much as possible, which can improve instruction
cache performance. An exception is callbacks from external
libraries, which cause control ﬂow to the original space, even
though it quickly jumps back to the shadow space.
Generating Random Binary Versions. Besides the afore-
mentioned transformations, STOCHFUZZ also performs the
following stochastic rewriting to generate a pool of N different
binaries (every time the rewriter is invoked). Speciﬁcally, for
addresses whose their probabilities of being data are smaller
than a threshold pθ but not 0 (i.e., not “certainly code” but
“likely code”), they have a chance of 1 − pθ to be replaced
with hlt. In our setting, we have N = 10 and pθ = 0.01.
C. Crash Analyzer
Recall that the crash analyzer needs to decide if a crash
it needs to locate and
is due to a rewriting error. If so,
repair the crash inducing rewriting error. Let S be a set of
uncertain addresses (that may be replaced with hlt), and
R(S) the execution result of a rewritten binary where all
the addresses in S are replaced with hlt. Assume R(S1)
yields an unintentional crash. To determine whether the crash
is caused by a rewriting error, the analyzer compares the results
of R(S1) and R(∅). If R(S1) = R(∅), the crash is caused by
a latent bug in the subject program, and vice versa.
Then, locating the crash inducing rewriting error can be
formalized as ﬁnding a 1-minimal subset S2 ⊆ S1, which
satisﬁes R(S2) = R(S1) and ∀ ai ∈ S2 : R(S2 \ {ai}) (cid:54)=
R(S1) [28]. Intuitively, all the addresses in S2 must be erro-
neously replaced with hlt. It can be proved by contradiction.
Assuming aj ∈ S2 is a code byte (and hence its rewriting
is correct), not replacing address aj (with hlt) should not
inﬂuence the execution result, that is R(S2 \ {aj}) = R(S2).
As R(S2) = R(S1), R(S2 \ {aj}) = R(S1), directly con-
tradicting with the 1-minimal property. Delta debugging [28]
is an efﬁcient debugging technique that guarantees to ﬁnd 1-
minimal errors. It operates in a way similar to binary search.
Details are elided.
D. Optimizations
We develop three optimizations for STOCHFUZZ, which are
directly performed on rewritten binaries without lifting to IR.
They are reusing dead registers, removing ﬂag register savings,
and removing redundant instrumentation. Details can be found
in Appendix X-A.
IV. PROBABILISTIC GUARANTEES
In this section, we study the probabilistic guarantees of
is the
STOCHFUZZ. We focus on two aspects. The ﬁrst
likelihood of rewriting errors (i.e., data bytes are mistakenly
replaced with hlt) corrupting coverage information without
triggering a crash. Note that if it triggers a crash, STOCHFUZZ
can locate and repair the error. The second is the likelihood
of instruction bytes not being replaced with hlt so that we
miss coverage information. Note there is no crash in this case
but rather some instructions are invisible to our system and
not rewritten. Our theoretical analysis shows that the former
likelihood is 0.05% and the latter is 0.01% (with a number
of conservative assumptions). They are also validated by our
experiments. Details can be found in Appendix X-B.
V. PRACTICAL CHALLENGES
We have addressed a number of practical challenges such as
supporting exception handling in C++, reducing process set up
cost, safeguarding non-crashing rewriting errors, and handling
occluded rewriting. Details can be found in Appendix X-C.
VI. EVALUATION
STOCHFUZZ is implemented from scratch with over 10, 000
lines of C code, leveraging Capstone [39] and Keystone [40]
that provide basic disassembling and assembling functionali-
ties, respectively. Our evaluation takes more than 5000 CPU
hours and is conducted on three benchmark sets, including
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:54 UTC from IEEE Xplore.  Restrictions apply. 
9667
FcodeFdataaFcodeFdata01 -onegopos1oneg1 -oposaddress aholding a data byteaaTABLE II: Soundness on Google FTS ( means failure)
Program
aﬂ-qemu ptfuzzer e9patch ddisasm STOCHFUZZ
TABLE III: Mean and standard deviation of time-to-discovery
(in minutes) for bugs in Google FTS
boringssl
freetype2
guetzli
harfbuzz
lcms
libarchive
libxml2
openssl-1.0.1f
openssl-1.0.2d
openssl-1.1.0c
openthread
sqlite
wpantund































the Google Fuzzer Test Suite (Google FTS) [17], a variant
of Google FTS which is compiled with inlined data, and
the fuzzing benchmarks from RetroWrite [16]. We compare
STOCHFUZZ with the state-of-the-art binary-only fuzzers, in-
cluding ptfuzzer, aﬂ-qemu, RetroWrite, e9patch, and ddisasm.
In addition, we use STOCHFUZZ on 7 commercial binaries
and ﬁnd 2 zero-days. We port a recent work IJON [21] on
state-based fuzzing to support stripped binaries, demonstrating
STOCHFUZZ can collect other feedback than coverage.
All the benchmarks are compiled by Clang 6.0 with their
default compilation ﬂags (“-O2” in most cases). For e9path, as
it cannot recover CFG from a stripped binary, we instrument
all the control ﬂow transfer instructions (e.g., jmp) to trace
the execution paths. For ddisasm, the version we use is 1.0.1,
and the reassembly ﬂags we use are “--no-cﬁ-directives” and
“--asm”. The reassembly of ddisasm is performed on a server
equipped with a 48-cores CPU (Intel(R) Xeon(R) Silver 4214
CPU @ 2.20GHz) and 188G main memory. All others are
conducted on a server equipped with a 12-cores CPU (Intel(R)
Core(TM) i7-8700 CPU @ 3.20GHz) and 16G main memory.
A. Evaluation on Google FTS
Google FTS is a standard benchmark widely used to evalu-
ate fuzzing techniques [4], [41], [42], consisting of 24 complex
real-world programs. We compare STOCHFUZZ with ptfuzzer,
aﬂ-qemu, e9patch, and ddisasm. We additionally compare
with two compiler-based baselines (aﬂ-gcc and aﬂ-clang-fast).
However, we cannot compare with RetroWrite on Google
FTS as RetroWrite cannot instrument stripped binaries and it
requires the binaries not written in C++, while all the binaries
are stripped in this experiment and 1/3 of them are C++ ones.
Soundness. Table II presents the overall soundness of binary-
only fuzzing solutions. The ﬁrst column shows the programs.
Columns 2-6 show whether aﬂ-qemu, ptfuzzer, e9patch, ddis-
asm, and STOCHFUZZ successfully generate binaries that the
fuzzer can execute, respectively. Note that we only present the
programs which at least one tool fails to instrument (due to
the space limitations). Speciﬁcally, aﬂ-qemu fails on libxml2
due to a known implementation bug [43], ptfuzzer fails on
4 out of the 24 programs due to unsolved issues in their
implementation [44], e9patch fails on 4 programs as these
Tool
aﬂ-gcc
aﬂ-clang-fast
aﬂ-qemu
ptfuzzer
e9patch
ddisasm
STOCHFUZZ
Tool
aﬂ-gcc
aﬂ-clang-fast
aﬂ-qemu
ptfuzzer
e9patch
ddisasm
STOCHFUZZ
guetzli
json
0.85 ± 0.63
513.25 ± 114.84
539.56 ± 240.83
0.18 ± 0.17
+∞
2.64 ± 3.56
+∞ 49.08 ± 82.35
+∞ 21.87 ± 36.21
N/A
0.67 ± 1.02
505.22 ± 93.45
363.37 ± 120.14
llvm-libcxxabi
0.08 ± 0.00
0.08 ± 0.00
0.23 ± 0.05
0.79 ± 0.25
0.35 ± 0.00
0.08 ± 0.00
0.08 ± 0.00
pcre2
re2
woff2
763.61 ± 40.44
2.21 ± 2.14
461.73 ± 219.89
3.08 ± 3.93
+∞
+∞ 42.92 ± 68.08
+∞
913.90 ± 495.42
768.91 ± 264.82
12.89 ± 0.44
12.09 ± 4.91
+∞ 67.23 ± 26.94
29.18 ± 0.19
+∞ 30.73 ± 0.28
14.60 ± 0.25
N/A
7.43 ± 0.27
2.32 ± 0.54
programs contain hand-written assembly code interleaved with
data, ddisasm fails on 9 programs which crash on the seed
inputs after reassembly due to uncertainty in their heuristics1,
and STOCHFUZZ succeeds on all the 24 programs.
Fuzzing Efﬁciency. To assess the fuzzing efﬁciency achieved
by STOCHFUZZ, we run AFL to fuzz the instrumented binaries
for 24 hours. Fig. 10 presents the total number of fuzzing
executions, where we take aﬂ-gcc as a baseline and report the
ratio of each tool to aﬂ-gcc. Larger numbers indicate better
performance. The average numbers of fuzzing executions over
the 24 programs are presented in the legend (on the top)
associated with the tools. STOCHFUZZ outperforms aﬂ-gcc
in 13 out of 24 programs. For the remaining 11 programs,
STOCHFUZZ also achieves comparable performance with aﬂ-
gcc. Aﬂ-clang-fast achieves the best performance among all
the tools, as it does instrumentation at the IR level. Compared
with it, STOCHFUZZ has 11.77% slowdown on average due to
the additional overhead of extra control ﬂow transfers (from
the original space to the shadow space) and switching between
binary versions. Ddisasm also achieves good performance.
However, due to its inherent soundness issues, it fails on 9
out of the 24 programs. Other tools have relatively higher
overhead.
Bug Finding. As Time-to-discovery (TTD) (of bugs) directly
reﬂects fuzzing effectiveness, and hence suggests instrumen-
tation effectiveness and fuzzing throughput, we additionally
conduct an experiment to show the time needed to ﬁnd the
ﬁrst bug for each tool. We run each tool three times with a
24-hour timeout. Table III shows the average TTD (in minutes)
and the standard deviation. We only report the programs for
which at least one tool can report a bug within the time bound.
The ﬁrst column presents the tools. Columns 2-4 show the
TTDs for different programs. The symbol +∞ denotes the tool
cannot discover any bug within the time bound. N/A denotes
the crash(es)2 discovered by the tool cannot be reproduced
1After being reported to the developers of ddisasm, 6 out of 9 test failures
got ﬁxed in the latest release (via strengthening heuristics). Details can be
found at https://github.com/GrammaTech/ddisasm/issues/20.
2The latest ddisasm can correctly reassemble all N/A programs.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:54 UTC from IEEE Xplore.  Restrictions apply. 
10668
Fig. 10: Total number of fuzzing executions of each tool in 24 hours. We take aﬂ-gcc as a baseline, and report the ratio of
each tool to aﬂ-gcc. In the legend, we additionally present the average number of fuzzing executions over the 24 programs.
Larger numbers indicate better performance.
by executing the non-instrumented binary. Due to their high
overhead, aﬂ-qemu, ptfuzzer, and e9patch cannot discover bugs
in multiple programs. Although ddisasm achieves good per-
formance in the programs that it can instrument, it generates
invalid crashes for some programs due to its soundness issues.
STOCHFUZZ has a similar TTD to aﬂ-gcc. This shows the
soundness and effectiveness of STOCHFUZZ.
TABLE IV: Effects of Optimizations. #B denotes the number
of basic blocks instrumented by STOCHFUZZ, #O denotes
the number of blocks where an optimization is applied at
least once, %R denotes the percentage, and %S denotes the
slowdown when disabling the optimizations
Program
%S
#B
We also collect the path coverage in 24 hours. The average
coverage for aﬂ-gcc, aﬂ-clang-fast, and STOCHFUZZ is 2572,
2239, and 2493, respectively. As other tools do not work on
all the programs, their numbers are not comparable, and hence
elided. We also omit the details due to the page limitations.
Optimization Effectiveness. Table IV presents the effects
of optimizations. The second column presents the number
of executed blocks during fuzzing. Columns 3-4, 5-6, and
7-8 present
the results for removing ﬂag register savings