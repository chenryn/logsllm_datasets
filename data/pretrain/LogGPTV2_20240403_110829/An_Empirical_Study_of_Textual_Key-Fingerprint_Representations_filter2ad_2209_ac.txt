### 4.3.1 Variables and Conditions

In the main experiment, the representation scheme is the controlled independent variable, with its values defining the experimental conditions. In the additional chunking experiment, the chunk size is the controlled independent variable instead of the representation algorithm. Throughout all tasks, we measure participants' performance speed and their accuracy in detecting attacks (speed and accuracy are the dependent variables).

Each participant was required to perform 46 comparisons in total. To identify participants who might be clicking randomly, two obviously distinct comparisons were included as attention tests. These training comparisons and attention tests were not included in the final evaluation. Based on feedback from our pre-study, we added tooltips during the training phase to provide hints for language-based approaches, informing users that spelling attacks would not occur.

We set the number of attacks to six: two obvious attacks where all bits are altered (serving as control questions) and four actual attacks with partial 80-bit preimages (one for each representation scheme). Participants who failed the control attacks were excluded from the evaluation but still received payment if they completed all tasks. The primary challenge in the study design was achieving a high attack detection rate, as most users performed comparisons correctly for the given attacker strength.

To minimize confounding factors, we standardized the font size, color, and style, using the same typeface for all fingerprint representations. Additionally, we set fixed line breaks for sentences and word lists. In the main experiment, the same chunking style was used for all representations: for (alpha)numeric approaches, a chunk consisted of four characters separated by spaces; for word lists, a line break was inserted every four words; and for generated sentences, one sentence per line was displayed. Although these design decisions may influence the comparison of representations, our pre-study results indicated a significantly lower effect size. More importantly, our primary interest was in comparing the concepts, so we did not vary any visual attributes such as font size or style. Specifically, differences resulting from the font's typeface were not evaluated. Lund's meta-analysis [25] showed no significant legibility differences between serif and sans-serif typefaces.

### Chunk-Size Testing

A question arose regarding whether the chunking of a hexadecimal string plays a more significant role compared to different representation types. Therefore, in addition to the main experiment, we conducted a second experiment with new participants to test different chunk sizes for the hexadecimal representation. We used chunk sizes ranging from 2 to 8, plus a "zero-chunk size" (eight cases), where no spaces were included. To ensure comparability, we used a similar design as in the main experiment, requiring the same number of comparisons, using the same font settings, and including the same number of attacks. Each participant was assigned four out of eight different chunk sizes randomly. All participants had to compare 46 fingerprints, with the first four considered as training comparisons, four attacks (one for each chunk size), and two control attacks with obviously distinct fingerprints.

### 4.3.2 Online Survey

The experiment was followed by an online survey to gather self-reported data and demographic information from participants. To measure perception, we asked participants to rate their agreement with statements discussed in subsection 5.2 on a 5-point Likert scale, ranging from "strongly disagree" to "strongly agree," as shown in Figure 4. Participants rated each representation type for all statements. Since users might not distinguish between different representation schemes, we provided an example from their previously completed task.

### 4.3.3 Statistical Testing

We used a common significance level of α = 0.05. To address the multiple comparisons problem, we applied the Holm-Bonferroni correction for our statistical significance tests [18]. Consequently, all reported p-values are corrected.

We tested the comparison duration using the Mann-Whitney-Wilcoxon (MWW) test (two-tailed). This significance test was chosen due to the presence of a few outliers.

### Summary Table

| Scheme | Comparison | Mean [s] | Median [s] | P-value | Standard Deviation |
|--------|------------|----------|------------|---------|-------------------|
| Hexadecimal | - | 10.0 | 1.1 | <0.001 | 6.4 |
| Hexadecimal – Base32 | - | 0.5 | -1.2 | <0.001 | 0.6 |
| ... | ... | ... | ... | ... | ... |
| Sentences – Peerio | - | 1.0 | 0.7 | <0.001 | 0.4 |

| Accuracy | P-value | False Positives | Fail Rate |
|----------|---------|-----------------|-----------|
| 0.49 | 10.44 | 0.690 | -2.09 |
| ... | ... | ... | ... |
| 0.075 | -1.07 | -2.76 | 1.48 |

This table summarizes the mean and median comparison times, p-values, and standard deviations for each representation scheme. The accuracy section includes p-values, false positives, and fail rates for each condition.