is available for reads. But the same is true for the replica
datacenters for key B, they will not be able to prepare non-
replica key A until they know it has committed. Thus, the
different sets of replicas are deadlocked and never commit. K2
sidesteps this issue with its write-only transaction algorithm.
Replicated Write-Only Transactions. The key insight behind
K2’s write-only transaction algorithm is to decouple the avail-
ability of data for remote reads from its availability for local
reads. Data should be available for remote reads immediately
and for as long as necessary to ensure remote reads can be
served without blocking. While data should be available for
local reads only when it satisﬁes the guarantees of the storage
system. This decoupling allows K2 to provide its invariant
that ensures remote reads do not block. It breaks down into
two cases: before and after a replica datacenter applies a
write to make it visible to local reads. Before a write is
applied, K2 makes it available only to remote reads through
the IncomingWrites table. This is safe since K2 ensures that
the remote read only requests a version that is already causally
consistent in the requesting datacenter. After a write is applied,
K2 keeps it in the multiversioning framework until it can be
safely garbage collected.
V. READ-ONLY TRANSACTIONS
This section completes K2’s design by describing our cache-
aware, read-only transaction algorithm. The algorithm is built
around two key insights that allow it
to often avoid any
cross-datacenter requests: cache awareness and trading a little
freshness for a lot of performance.
A. Cache Awareness
The read-only transaction algorithm exploits the temporal
locality of data access by leveraging the data cached as part
Fig. 4: A and C are non-replica keys, B is a replica key.
a1 and c1 are cached versions. A straw-man solution incurs
unnecessary remote fetches, while K2’s read-only transaction
reuses cached versions when safe.
of K2’s design. Caching values which are likely to be accessed
again soon [5], [9], [17], [28], [48] avoids unnecessary remote
fetches of the same data. For instance, after Alice uploads a
new photo (cache after write), she is likely to verify the upload
was successful by downloading it (read the cached photo).
Similarly, after Bob reads a photo (cache after remote fetch),
the same photo will likely be recommended to Bob’s friends
(read the cached photo). The beneﬁts of caching are even more
promising for real-world applications, which usually exhibit
Zipﬁan workloads, i.e., most operations are on a small subset
of the data. For instance, Facebook’s TAO caching system
reported an overall hit rate of 96.4% [16].
Caching makes it possible to avoid many cross-datacenter
requests. The challenge, however, is realizing this possibility
with our read-only transaction algorithm. Previous algorithms
focused on providing low latency and consistency. Our algo-
rithm adds the need to reuse cached values.
B. Trading Freshness for Performance
K2’s read-only transactions provide causal consistency.
Causal consistency has two properties we can leverage to
achieve better read performance. First, it does not require
a read to reﬂect the most recent updates, commonly known
as the real-time requirement in stronger consistency models,
e.g., linearizability [30]. Second, it does not require all clients
to advance their views of the system at the same rate: it is
technically causally consistent if the system keeps making a
client read at a ﬁxed timestamp that only advances when the
client issues writes. Our algorithm does much better than this
as we guarantee that clients make progress through the garbage
collection that safely discards any versions older than 5s. In
addition, in our evaluation we ﬁnd much lower staleness with
a median of no staleness at all.
With these two observations in mind, we explore the pos-
sibility of avoiding remote fetches by allowing each client
to maintain and manage its read timestamp, which could be
slightly stale but for which most non-replica items have cached
values. For instance, in Figure 4, a straw-man solution for
read-only transactions is to read at the most-recent timestamp,
12. However, this will incur two unnecessary remote fetches on
A2 and C2 since those versions are not present locally. Instead,
K2’s read-only transaction algorithm reads at timestamp 3
since both A and C have cached values at that timestamp.
A2= ∅1A1= a1A:5B2= b2B:712C1= c1C:39Logical time13C2= ∅16B1= b12K2Straw-man1 function read_txn():
2
vers[][] = [][], vals[] = []
for k in keys: /* 1st round */
vers[k] = read(k, cli.read_ts)
ts = find_ts(vers)
for k in keys:
3
4
5
6
7
8
9
10
11
12
13
14
15
for ver in vers[k]:
if ver.evt ≤ ts ≤ ver.lvt and
ver.val != null:
vals[k] = ver.val; break
if !vals.contains(k):
/* 2nd round */
vals[k] = read_by_time(k, ts)
cli.read_ts = max(cli.read_ts, ts)
for k in keys: deps.add(k, vals[k].ver)
return vals
Fig. 5: Pseudocode for read-only transaction.
C. Read-only Transaction Algorithm
Figure 5 shows the pseudocode for K2’s read-only trans-
action algorithm at a client. Each client maintains a read
timestamp read_ts, and includes this timestamp when it
sends read-only transaction requests to the servers. The client
begins with a round of parallel requests to the servers in its
local datacenter. Each server returns all visible versions of each
key in its request that are valid at or after read_ts. Each
version includes the version number, EVT, LVT, and value if
it is stored or cached locally. The LVT (latest valid time) of a
version is the latest logical time before it is overwritten by a
new version. The server returns its current logical time for LVT
if the version is the latest. The server returns an empty value
if the version or any of its earlier versions are pending. (The
empty value indicates that the a version is potentially being
modiﬁed by some ongoing write-only transactions.) The client
examines the returned versions and ﬁnds a consistent logical
time ts that minimizes cross-datacenter requests. Speciﬁcally,
find_ts examines the EVTs of all returned versions. It ﬁnds
the earliest EVT where either (1) all keys have a valid value, or
(2) all non-replica keys have a valid value, or (3) the most keys
have a valid value. This procedure for picking the effective
logical time is what makes our algorithm cache-aware.
A second round of read requests is required if a key has no
consistent version or value at ts. If the key is being modiﬁed
by pending write-only transactions earlier than ts, the server
waits for the pending transactions to commit. This waiting
does not appreciably affect latency because the longest a write-
only transaction will remain pending is a single roundtrip
within the local datacenter (from the cohorts to the coordinator
and back). Once the pending transactions commit, the server
determines the committed version at time ts, and returns
the value if it is available. If not, the server sends a remote
read request to its equivalent server in the nearest replica
datacenter to fetch the value given the key and the version
number. Our constrained replication topology and write-only
transaction algorithm ensure the requested version will be
accessible in the replica datacenter. The remote server checks
its IncomingWrites table and multiversioning framework for
the requested version, and sends its value to the requesting
server. Upon receiving the response, the local server caches the
value and replies to the client. To maintain causal consistency,
the client updates its read_ts and dependencies. It also
advances read_ts to max(read_ts, write_ts) after it
completes a local write-only transaction that returns a write
timestamp write_ts (§III-C).
VI. FAULT TOLERANCE
This section describes unimplemented extensions to K2 for
handling failures and enabling clients to switch datacenters.
These extensions are similar to prior work [38], [39].
A. Handling Failures
Server failures within a DC: Server failures are unavoidable
in practice. K2 can provide availability for a logical server
despite failures using a fault-tolerant protocol like Paxos [36]
or Chain Replication [55].
Datacenter failures: With a replication factor of
f , K2
assumes up to f − 1 replica datacenters can fail. Replicating
writes to replica datacenters can proceed if at least one replica
datacenter of each key in those writes is available. The non-
replica datacenters can send their remote read requests to
the available replica datacenters. Permanent datacenter failures
(e.g., a datacenter being destroyed by a tsunami) may lead to
data loss in K2 if a local datacenter is destroyed after replying
to a client’s write request but before successfully replicating
them to any other datacenter. This cost of achieving low
latency for local writes that return faster than inter-datacenter
latency is inevitable [38], [39]. Transient failures (e.g., tem-
porary power failures) do not result in data loss. However,
the local (temporarily failed) datacenter should replicate its
pending updates to other datacenters once it is restored.
B. Switching Datacenters
The clients of K2 are frontend servers co-located in the
same datacenters as the backend storage servers of K2 (§II-A).
These clients will continue to access their co-located servers.
The users they issue operations on behalf of, however, may
wish to switch datacenters, e.g., after ﬂying to another part
of the world. K2 can allow users to switch datacenters using
the following steps: (0) Dependencies are propagated back to
users, e.g., in an HTTP cookie. (1) When a user switches to a
new datacenter it sends its dependencies to its frontend, e.g.,
the user request includes the cookie. (2) That frontend checks
(by polling with reads) and waits until all dependencies (which
includes its last write and all its reads since the last write)
are satisﬁed by the metadata in the local datacenter. (3) That
frontend then uses the included dependencies for this user.
Steps 0 and 1 ensure the new frontend knows the dependencies
for this user. Step 2 ensures all causal dependencies are present
in the new datacenter. Step 3 ensures later operations on behalf
of this user include the correct dependencies.
VII. EVALUATION
Our evaluation compares K2 to RAD, a baseline that
directly adapts causal consistency for partial replication, and
VA
CA
60
SP
146
LDN
76
TYO 162
SG 243
CA
194
136
110
178
SP
LDN TYO
214
269
333
233
163
68
Fig. 6: Round trip latencies in ms between datacenters emu-
lated on Emulab and based on EC2 measurements.
PaRiS(cid:63), a baseline that uses a per-client cache [51],
to
understand the improvements and tradeoffs of K2’s design.
Speciﬁcally, our evaluation answers these questions:
§VII-C What improvement in latency does K2 provide?
§VII-D How does the throughput and write latency of K2
compare to the RAD baseline?
§VII-D What staleness does K2’s new read-only transaction
algorithm introduce?
A. Implementation and Baseline
K2 is implemented as a modiﬁcation to the Java codebase of
Eiger, a scalable geo-replicated storage system that provides
causal consistency [39]. The major changes in our implemen-
tation include our new algorithms for replication, write-only
transactions, read-only transactions, LRU-like cache replace-
ment policy, and garbage collection.
Replicas Across Datacenters (RAD). We use a direct adapta-
tion of scalable causal consistency to partial replication as our
baseline for comparison. We compare to RAD because it is a
reasonable adaptation of a fully-replicated causal consistency
design to a partially replicated setting. To implement RAD, we
conﬁgure Eiger to split data in each replica across datacenters,
which together form a replica group. Clients send read and
write requests directly to the datacenters in its group that hold
the relevant keys. A datacenter in a group needs to replicate
writes to its equivalent datacenters, which hold the same key
ranges, in other groups. Before committing a replicated write,
a datacenter sends dependency checks to other datacenters in
its group. It applies the replicated write once all dependencies
are satisﬁed. RAD uses Eiger’s read-only and write-only
transaction algorithms.
It is not straightforward to adapt the design of Eiger to
make efﬁcient use of a cache. Eiger’s read-only transaction
algorithm’s ﬁrst round returns the currently visible value for
each key within a replica. A local datacenter cache would only
contain previously read values and would not know if these
values were still visible. All ﬁrst round requests for non-replica
keys would thus need to contact a remote datacenter. This
precludes the possibility of achieving zero cross-datacenter
requests, which is the purpose of our cache. Thus, our RAD
baseline does not include a datacenter cache.