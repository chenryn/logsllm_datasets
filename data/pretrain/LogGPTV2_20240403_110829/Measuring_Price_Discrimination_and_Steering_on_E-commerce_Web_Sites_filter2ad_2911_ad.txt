and average price diﬀerence to analyze discrimination.
All of our analysis is always conducted relative to a con-
trol.
In all of the ﬁgures in this section, the starred (*)
feature in the key is the control. For example, in Figure 6,
all analysis is done relative to a PhantomJS instance that
does not have a user account. Each point is an average of
the given metric across all 20 queries on that day.
In total, our analysis produced >360 plots for the various
features across all 16 e-commerce cites. Overall, most of the
experiments do not reveal evidence of steering or discrimina-
tion. Thus, for the remainder of this section, we focus on the
particular features and sites where we do observe personal-
Figure 7: Price discrimination on Cheaptickets. The top result
is shown to users that are not logged-in. The bottom result is a
“Members Only” price shown to logged-in users.
ization. None of our feature tests revealed personalization
on rental car sites, so we omit them entirely.
5.2 Hotels
We begin by analyzing personalization on hotel sites. We
observe hotel sites implementing a variety of personalization
strategies, so we discuss each case separately.
Cheaptickets and Orbitz.
The ﬁrst sites that we
examine are Cheaptickets and Orbitz. These sites are ac-
tually one company, and appear to be implemented using
the same HTML structure and server-side logic. In our ex-
periments, we observe both sites personalizing hotel results
based on user accounts; for brevity we present the analysis
of Cheaptickets and omit Orbitz.
Figures 6(a) and (b) reveal that Cheaptickets serves
slightly diﬀerent sets of results to users who are logged-in
to an account, versus users who do not have an account or
who do not store cookies. Speciﬁcally, out of 25 results per
page, ≈2 are new and ≈1 is moved to a diﬀerent location
on average for logged-in users. In some cases (e.g., hotels in
Bangkok and Montreal) the diﬀerences are much larger: up
to 11 new and 11 moved results. However, the nDCG analy-
sis in Figure 6(c) indicates that these alterations do not have
an appreciable impact on the price of highly-ranked search
results. Thus, we do not observe Cheaptickets or Orbitz
steering results based on user accounts.
However, Figure 6(d) shows that logged-in users receive
diﬀerent prices on ≈5% of hotels. As shown in Figure 6(e),
the hotels with inconsistent prices are $12 cheaper on aver-
age. This demonstrates that Cheaptickets and Orbitz im-
plement price discrimination,
in favor of users who have
accounts on these sites. Manual examination reveals that
these sites oﬀer “Members Only” price reductions on certain
hotels to logged-in users. Figure 7 shows an example of this
on Cheaptickets.
Although it is not surprising that some e-commerce sites
give deals to members, our results on Cheaptickets (and
Orbitz) are important for several reasons. First, although
members-only prices may be an accepted practice, it still
qualiﬁes as price discrimination based on direct segmenta-
tion (with members being the target segment). Second, this
result conﬁrms the eﬃcacy of our methodology, i.e., we are
able to accurately identify price discrimination based on au-
tomated probes of e-commerce sites. Finally, our results
 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. Jaccard(a)-1-0.5 0 0.5 104/0504/1204/1904/2605/0305/10Avg. Kendall’s Tau(b) 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. nDCG(c) 0 5 10 15 2004/0504/1204/1904/2605/0305/10% of Items w/ Diff. Prices(d)Logged-in-20-15-10-5 004/0504/1204/1904/2605/0305/10Avg. Price Difference ($)(e)No Account*Logged-InNo Cookies313Browser
OS
0.4
FX IE8 Chr* Ctrl OSX Lin XP Win7*
0.4 0.4 0.4
0.4
0.9 1.0 1.0 0.9
1.0 0.9 0.9 1.0
1.0 0.9 0.9 1.0
1.0 0.9 0.9 1.0
1.0 0.9 0.9
0.9 1.0
0.9
0.4 0.4 0.4
0.9 0.9 0.9
1.0 1.0
1.0
Account
In No* Ctrl
0.4 0.4 0.3
1.0 1.0 1.0
0.9 0.9 0.9
0.9 0.9 0.9
0.9 0.9 0.9
0.9 0.9 0.9
1.0 1.0 1.0
1.0 1.0 1.0
0.9 0.9 0.9
1.0 1.0
1.0
Ctrl
Win7*
S
O
XP
Lin
OSX
r Ctrl
e
Chr*
s
w
IE8
o
r
FX
B
t Ctrl
c
c
No*
A
Table 4: Jaccard overlap between pairs of user feature experiments on
Expedia.
Figure 8: Clearing cookies causes a user to be placed in
a random bucket on Expedia.
Figure 9: Users in certain buckets are steered towards higher priced hotels on Expedia.
reveal the actual diﬀerences in prices oﬀered to members,
which may not otherwise be public information.
Hotels.com and Expedia.
Our analysis reveals that
Hotels.com and Expedia implement the same personaliza-
tion strategy: randomized A/B tests on users. Since these
sites are similar, we focus on Expedia and omit the details
of Hotels.com.
Initially, when we analyzed the results of our feature tests
for Expedia, we noticed that the search results received by
the control and its twin never matched. More oddly, we
also noticed that 1) the control results did match the results
received by other speciﬁc treatments, and 2) these matches
were consistent over time.
These anomalous results led us to suspect that Expedia
was randomly assigning each of our treatments to a “bucket”.
This is common practice on sites that use A/B testing: users
are randomly assigned to buckets based on their cookie,
and the characteristics of the site change depending on the
bucket you are placed in. Crucially, the mapping from cook-
ies to buckets is deterministic: a user with cookie C will be
placed into bucket B whenever they visit the site unless their
cookie changes.
To determine whether our treatments are being placed
in buckets, we generate Table 4, which shows the Jaccard
Index for 12 pairs of feature experiments on Expedia. Each
table entry is averaged over 20 queries and 10 days. For a
typical website, we would expect the control (Ctrl) in each
category to have perfect overlap (1.0) with its twin (marked
with a *). However, in this case the perfect overlaps occur
between random pairs of tests. For example, the results
for Chrome and Firefox perfectly overlap, but Chrome has
low overlap with the control, which was also Chrome. This
strongly suggests that the tests with perfect overlap have
been randomly assigned to the same bucket. In this case,
we observe three buckets: 1) {Windows 7, account control,
no account, logged-in, IE 8, Chrome}, 2) {XP, Linux, OS X,
browser control, Firefox}, and 3) {OS control}.
To conﬁrm our suspicion about Expedia, we examine the
behavior of the experimental treatment that clears its cook-
ies after every query. We propose the following hypothesis: if
Expedia is assigning users to buckets based on cookies, then
the clear cookie treatment should randomly change buckets
after each query. Our assumption is that this treatment will
receive a new, random cookie each time it queries Expedia,
and thus its corresponding bucket will change.
To test this hypothesis we plot Figure 8, which shows
the Jaccard overlap between search results received by the
clear cookie treatment, and results received by treatments
in other buckets. The x-axis corresponds to the search re-
sults from the clear cookie treatment over time; for each
page of results, we plot a point in the bucket (y-axis) that
has >0.97 Jaccard overlap with the clear cookie treatment.
If the clear cookie treatment’s results do not overlap with
results from any of the buckets, the point is placed on the
“Unknown” row. In no cases did the search results from the
clear cookie treatment have >0.97 Jaccard with more than
a single bucket, conﬁrming that the set of results returned
to each bucket are highly disjoint (see Table 4).
Figure 8 conﬁrms that the clear cookie treatment is ran-
domly assigned to a new bucket on each request. 62% of
results over time align with bucket 1, while 21% and 9%
match with buckets 2 and 3, respectively. Only 7% do not
match any known bucket. These results suggest that Expe-
dia does not assign users to buckets with equal probability.
There also appear to be time ranges where some buckets
are not assigned, e.g., bucket 3 in between 04/12 and 04/15.
We found that Hotels.com also assigns users to one of three
buckets, that the assignments are weighted, and that the
weights change over time.
Now that we understand how Expedia (and Hotels.com)
assign users to buckets, we can analyze whether users in dif-
ferent buckets receive personalized results. Figure 9 presents
Bucket 1Bucket 2Bucket 3Unknown04/0604/1304/2004/27Result Overlap (>0.97)Search Results Over Timewith Cleared Cookies 0 0.2 0.4 0.6 0.8 103/2904/0504/1204/1904/2605/0305/10Avg. Jaccard(a)-1-0.5 0 0.5 103/2904/0504/1204/1904/2605/0305/10Avg. Kendall’s Tau(b) 0 0.2 0.4 0.6 0.8 103/2904/0504/1204/1904/2605/0305/10Avg. nDCG(c)Bucket 1*Bucket 2Bucket 3314Figure 10: Priceline alters hotel search results based on a user’s click and purchase history.
the results of this analysis. We choose an account from
bucket 1 to use as a control, since bucket 1 is the most fre-
quently assigned bucket.
Two conclusions can be drawn from Figure 9. First, we
see that users are periodically shuﬄed into diﬀerent buckets.
Between 04/01 and 04/20, the control results are consistent,
i.e., Jaccard and Kendall’s τ for bucket 1 are ≈1. However,
on 04/21 the three lines change positions, implying that the
accounts have been shuﬄed to diﬀerent buckets. It is not
clear from our data how often or why this shuﬄing occurs.
The second conclusion that can be drawn from Figure 9 is
that Expedia is steering users in some buckets towards more
expensive hotels. Figures 9(a) and (b) show that users in
diﬀerent buckets receive diﬀerent results in diﬀerent orders.
For example, users in bucket 3 see >60% diﬀerent search re-
sults compared to users in other buckets. Figure 9(c) high-
lights the net eﬀect of these changes: results served to users
in buckets 1 and 2 have higher nDCG values, meaning that
the hotels at the top of the page have higher prices. We do
not observe price discrimination on Expedia or Hotels.com.
Priceline.
As depicted in Figure 10, Priceline alters
hotel search results based on the user’s history of clicks and
purchases. Figures 10(a) and (b) show that users who clicked
on or reserved low-price hotel rooms receive slightly diﬀerent
results in a much diﬀerent order, compared to users who
click on nothing, or click/reserve expensive hotel rooms. We
manually examined these search results but could not locate
any clear reasons for this reordering. The nDCG results in
Figure 10(c) conﬁrm that the reordering is not correlated
with prices. Thus, although it is clear that account history
impacts search results on Priceline, we cannot classify the
changes as steering. Furthermore, we observe no evidence of
price discrimination based on account history on Priceline.
Travelocity.
As shown in Figure 11, Travelocity alters
hotel search results for users who browse from iOS devices.
Figures 11(a) and (b) show that users browsing with Safari
on iOS receive slightly diﬀerent hotels, and in a much dif-
ferent order, than users browsing from Chrome on Android,
Safari on OS X, or other desktop browsers. Note that we
started our Android treatment at a later date than the other
treatments, speciﬁcally to determine if the observed changes
on Travelocity occurred on all mobile platforms or just iOS.
Although Figure 11(c) shows that this reordering does not
result in price steering, Figures 11(d) and (e) indicate that
Travelocity does modify prices for iOS users. Speciﬁcally,
prices fall by ≈$15 on ≈5% of hotels (or 5 out 50 per page)
for iOS users. The noise in Figure 11(e) (e.g., prices increas-
ing by $50 for Chrome and IE 8 users) is not signiﬁcant: this
result is due to a single hotel that changed price.
The takeaway from Figure 11 is that we observe evidence
consistent with price discrimination in favor of iOS users on
Travelocity. Unlike Cheaptickets and Orbitz, which clearly
mark sale-price “Members Only” deals, there is no visual
cue on Travelocity’s results that indicates prices have been
changed for iOS users. Online travel retailers have pub-
licly stated that mobile users are a high-growth customer
segment, which may explain why Travelocity oﬀers price-
incentives to iOS users [26].
5.3 General Retailers
Home Depot.
We now turn our attention to general
retail sites. Among the 10 retailers we examined, only Home
Depot revealed evidence of personalization. Similar to our
ﬁndings on Travelocity, Home Depot personalizes results for
users with mobile browsers. In fact, the Home Depot website
serves HTML with diﬀerent structure and CSS to desktop
browsers, Safari on iOS, and Chrome on Android.
Figure 11: Travelocity alters hotel search results for users of Safari on iOS, but not Chrome on Android.
 0 0.2 0.4 0.6 0.8 105/0305/1005/1705/24Avg. Jaccard(a)Control*Click LowClick HighBuy LowBuy High-1-0.5 0 0.5 105/0305/1005/1705/24Avg. Kendall’s Tau(b)Click Low and Buy Low 0 0.2 0.4 0.6 0.8 105/0305/1005/1705/24Avg. nDCG(c) 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. Jaccard(a)-1-0.5 0 0.5 104/0504/1204/1904/2605/0305/10Avg. Kendall’s Tau(b) 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. nDCG(c) 0 5 10 15 2004/0504/1204/1904/2605/0305/10% of Items w/ Diff. Prices(d)iOS Safari-20 0 20 40 6004/0504/1204/1904/2605/0305/10Avg. Price Difference ($)(e)Chrome*IE 8FirefoxSafari on OS XSafari on iOSChrome on Android315Figure 12: Home Depot alters product searches for users of mobile browsers.
Figure 12 depicts the results of our browser experiments
on Home Depot. Strangely, Home Depot serves 24 search re-
sults per page to desktop browsers and Android, but serves
48 to iOS. As shown in Figure 12(a), on most days there is
close to zero overlap between the results served to desktop
and mobile browsers. Oddly, there are days when Home De-
pot brieﬂy serves identical results to all browsers (e.g., the
spike in Figure 12(a) on 4/22). The pool of results served
to mobile browsers contains more expensive products over-
all, leading to higher nDCG scores for mobile browsers in
Figure 12(c). Note that nDCG is calculated using the top
k results on the page, which in this case is 24 to preserve
fairness between iOS and the other browsers. Thus, Home
Depot is steering users on mobile browsers towards more
expensive products.
In addition to steering, Home Depot also discriminates
against Android users. As shown in Figure 12(d), the
Android treatment consistently sees diﬀerences on ≈6% of
prices (one or two products out of 24). However, the prac-
tical impact of this discrimination is low: the average price
diﬀerential in Figure 12(e) for Android is ≈$0.41. We manu-
ally examined the search results from Home Depot and could
not determine why the Android treatment receives slightly
increased prices. Prior work has linked price discrimination
on Home Depot to changes in geolocation [30], but we con-
trol for this eﬀect in our experiments.
It is possible that the diﬀerences we observe on Home
Depot may be artifacts caused by diﬀerent server-side im-
plementations of the website for desktop and mobile users,
rather than an explicit personalization algorithm. However,
even if this is true, it still qualiﬁes as personalization ac-
cording to our deﬁnition (see § 2.3) since the diﬀerences are
deterministic and triggered by client-side state.
6. RELATED WORK