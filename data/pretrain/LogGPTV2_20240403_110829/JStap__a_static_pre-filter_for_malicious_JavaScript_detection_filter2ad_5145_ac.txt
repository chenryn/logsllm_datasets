y
y
Table 4: Benign JavaScript dataset
Source
Tranco-10k
Microsoft
Games
Web frameworks
Atom
Total
#JS
122,910
16,271
1,992
427
168
141,768
Collection
2019
2015-18
N/A
N/A
2011-18
-
Obfuscated
N/A
y
n
N/A
n
-
malicious (we discarded the other samples, which are also not rep-
resented in Table 3). Since our analysis is entirely static, it provides
a complete code coverage based on the proportion of source code
analyzed. In turn, it is unable to consider dynamically generated
JavaScript.3 To this end, we parsed each malicious file with Esprima
and automatically inlined all code passed through eval (for invoca-
tions with static strings). Thereby, we increased the code coverage
of JStap on 1,868 unique scripts, as we did not merely consider a
CallExpression node with a fixed string parameter anymore, but
the code contained in the string, possibly (depending on JStap’s
selected module) along with its control and/or data flow. Also, 1,094
samples used conditional compilation [39], which Esprima parses
as a large comment. Thus, we automatically replaced this construct
with the corresponding code for the parser to produce the actual
ASTs of such scripts.
3.1.2 Benign Dataset. As for the benign dataset (Table 4), we used
Chromium to visit the start pages of Tranco top 10,000 websites [35].4
For each visited web page, we waited for the load of the page and
observed the site for one second, to also collect dynamically gener-
ated scripts. In particular, we stored all inline scripts from the same
document in one file–keeping the order in which they are executed–
and consider all external scripts separately. This way, we obtain
122,910 unique JavaScript files. Given the fact that we extracted
JavaScript from the start pages of high-profile websites, we assume
them to be benign. Based on a study from Skolka et al. [48], over 30%
of first-party scripts are either obfuscated or minified and over 55%
of third-party scripts. In addition, we consider benign JavaScript
from Microsoft products,5 the majority of which are also obfus-
cated, which enables us to ensure that JStap does not confound
obfuscation with maliciousness. As we also own malicious scripts
from Microsoft (i.e., JScript-loaders), we are not introducing a bias
in our dataset, even if Microsoft uses custom obfuscation methods.
3We discuss some possible drawbacks induced by the static analysis in Section 4.1
4Even though we crawled Tranco top 10,000 in 2019, the scripts were not necessarily
written in 2019, such that our malicious dataset is not older than our benign dataset
5Microsoft Exchange 2016 and Microsoft Team Foundation Server 2017
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Fass et al.
Finally, we collected benign JavaScript from open source games,
web frameworks, and Atom [2]. As these samples may contain new
or specific (e.g., games) coding styles, we show that JStap does
not confound unknown or unusual structures with maliciousness
either.
3.1.3 Classifier Training. For the next sections, we built machine
learning models using the following protocol. First, we randomly
selected 10,000 JavaScript samples from our malicious dataset. We
deemed our malicious training set to be representative of the distri-
bution found in the wild due to our multiple malware sources and
random selection from an initial malicious pool with over 130,000
samples. For the benign part, we also randomly selected 10,000
samples to build a balanced model. As previously, we assume our
benign training set to be representative of the distribution found
in the wild through our multiple sources, with different coding
styles (e.g., games). In the following sections, we consider that the
remaining samples are unknown and use them to evaluate the per-
formance of the different detectors. In addition, we extracted all
features present in our training set, before randomly selecting 5,000
new unique malicious and as many benign samples, to check on
this validation dataset which of the previous features are correlated
with the classification, using the χ2 test described in Section 2.2.3.
In the remainder of this paper, we consider only these features.
We specifically chose to assemble balanced datasets, even though
in reality, benign webpages outnumber malicious ones. With Tesser-
act [42], Pendlebury et al. argue that using unrealistic assumptions
about the ratio of benign samples to malware in the data can lead
to inflated detection results. In our case, it is not an issue, since
we specifically chose metrics to evaluate the detection accuracy
of JStap on both benign and malicious samples, and not merely a
score to rate the proportion of correct predictions of our modules
(cf. Section 3.2). Finally, to limit any statistical effects from random-
ized datasets, we repeated the previous procedure five times and
averaged the detection results. Contrary to 5-fold cross-validation,
we explore more ways of partitioning data, such that each sample
is not necessarily tested only once.
3.2 JStap’s Detection Performance
First, we compare the detection performance of JStap, in terms of
true-positive (correct classification of a malicious script as mali-
cious) and true-negative (correct classification of a benign input as
benign) rates, depending on the level of the analysis, i.e., tokens,
AST, CFG, PDG-DFG, or PDG. We make, in particular, the distinc-
tion between the ngrams and the value approach (Section 2.2.2).
Specifically, we chose to compare the accuracy of the different mod-
ules over their true-positive (TPR) and true-negative rates (TNR),
and not AUC [18] or F-measure, so as to evaluate how well they
can detect benign and malicious inputs. For this reason, AUC and
F-measure would be heavily biased by the composition of our test
sets (proportion of benign and malicious samples), while we aim at
having a more realistic estimation of our modules’ accuracy both
on benign and malicious samples. Then, we conclude on the pre-
dictions’ accuracy of JStap’s modules, before justifying why they
make such accurate predictions.
3.2.1 ngrams Features. In the first scenario, we consider the ngrams
approach. As Figure 4 shows, both the true-positive (TPR) and true-
negative rates (TNR) of JStap stay constant across our five analyses.
Specifically, the TPR ranges from 98.73% (tokens) to 99.22% (CFG),
making the CFG the most reliable malicious JavaScript detector in
this configuration. As for the TNR, it ranges from 99.34% (PDG) to
99.62% (AST), meaning that the AST detects benign JavaScript best.
In terms of overall detection rate, defined as the proportion of sam-
ples correctly classified, the AST performs best with an accuracy
around 99.38%, whereas the token-based approach performs worst
with a detection rate of 99.08%, while CFG, PDG-DFG, and PDG
have similar detection rates between 99.27% and 99.28%.
As mentioned in Section 2.1.1, the lexical level of code abstrac-
tion does not use the context (in terms of syntactic structure) in
which a given token occurs (e.g., IfStatement, ForStatement,
VariableDeclaration), but merely processes JavaScript inputs
one word after the other. For example, the following two JavaScript
snippets for(i = 0; i < 5; i++) and if(i == 1) j = 2;k--;
are composed of exactly the same tokens, namely Keyword, Punc-
tuator, Identifier, Punctuator, Numeric, Punctuator,
Identifier, Punctuator, Numeric, Punctuator, Identifier,
Punctuator, Punctuator, while performing different actions. On
the contrary, the AST-based analysis leverages the JavaScript gram-
mar, which provides more insight than an analysis purely based on
tokens, and makes the distinction, e.g., between the previous for
and if constructs, hence a better detection accuracy.
Even though the AST-based approach performs better overall,
the CFG, PDG-DFG, and PDG also are reliable. Still, we observe
that the AST code representation may be slightly more informative
to distinguish benign from malicious JavaScript than the control
and data flow. We ran the same experiments where the CFG, PDG-
DFG, and PDG only followed the control and/or data dependency,
without also traversing the sub-AST corresponding to nodes with
such a control/data flow (Section 2.2.1). The TPR stays relatively
similar to the results from Figure 4, between 98.87% (PDG-DFG) and
99.33% (CFG), but the TNR decreased between 94.92% (PDG-DFG)
and 95.50% (PDG). As the control and data flow are represented
only between statement nodes, these nodes are less representative
of benign or malicious intent than the AST structure. Specifically,
we extracted the five features most representative of malicious or
benign intent, for all five ngrams modules, according to the cor-
responding random forest models [47]. Since Identifier nodes
are always part of each of these five most important features, we
highlight the importance, in terms of predictions’ accuracy, of not
just considering statement nodes (which, thereby, do not include
Identifier). Thus, adding AST information into the CFG, PDG-
DFG, and PDG improved their detection rates up to the AST stan-
dards. Still, these three approaches may inherently be limited if
there is no control or data flow present in the considered files. Out
of the 253,216 samples6 we classified, the CFG could handle on av-
erage 231,490.8 of them (91.4%), the PDG-DFG 233,484 (92.2%) and
PDG 237,415.4 (93.8%), while the token- and AST-based analyses
classified them all. Nevertheless, due to the possibility of combining
several modules (Section 3.4), JStap can still classify such samples.
6We excluded the samples used to train the model
JStap: A Static Pre-Filter for Malicious JavaScript Detection
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Figure 4: Accuracy comparison with the ngrams approach
Figure 5: Accuracy comparison with the value approach
value Features. In the second scenario, we consider the
3.2.2
value approach. Contrary to the previous ngrams variant, the TPR
and TNR are less constant across our five analyses (Figure 5), given
that considering node values increases the number of different fea-
tures. In particular, the TPR ranges from 98.44% (AST) to 99.23%
(CFG). Even though the CFG performs best to detect malicious
JavaScript, it performs worse to accurately label benign samples,
with a TNR of 95.87% compared to 99.67% for the token-based ap-
proach. The overall detection rates across the five analyses also are
more sparse than with the ngrams approach: from 97.55% for the
CFG to 99.44% for the lexical analysis, while AST, PDG-DFG, and
PDG have similar detection rates between 98.9% and 99.1%.
This time and contrary to Section 3.2.1, the lexical level of code ab-
straction leverages context information, since the value approach
takes the value of each token into consideration (Section 2.2.2).
Thus, the for and if code snippet from the ngrams approach in Sec-
tion 3.2.1 would have a different representation, which contributes–
for the reasons mentioned previously–to a better overall detection
accuracy. In particular, each token has a value by construction, while
only the Identifier and Literal nodes have one in the graph
representation. For this purpose, we mapped the non-identifier and
non-literal nodes to their nearest Identifier/Literal child, if
any (on average, only 2.8 samples did not have any Identifier
nor Literal nodes [41], representing 0.001% of our dataset). As a
consequence, the same value is used by several nodes and may not
always be informative, even though it is significant w.r.t. χ2 (Sec-
tion 2.2.3). Besides, the syntactic analyses do not benefit from the
JavaScript grammar anymore, as each feature is analyzed indepen-
dently (compared to an ngrams analysis previously). As mentioned
in Section 3.2.1, the context information was mainly responsible
for the high detection results; therefore, the lexical analysis now
performs best. To overcome the lack of context, we tried to combine
the current value approach with an n-gram analysis, by combining
pairs of (unit, value) n times, but the TNR dropped to 80%. As a
matter of fact, the features got too specific to one file and could not
be generalized over the whole dataset anymore. Last but not least,
we assume that the CFG approach does not perform as well as the
other ones, since benign and malicious developers may tend to use
similar names for nodes with control flow, as suggested by the small
number of features compared to the AST or PDG (Section 2.2.3).
3.2.3 Predictions’ Accuracy Summary of JStap’s Modules. To sum
up, each of the ten JStap modules could correctly classify our
JavaScript collection with an accuracy over 97.6%, eight modules
of which had an accuracy over 99%. For the ngrams approach, the
AST performs best, mainly due to the context information brought
by the combination of syntactic units. Similarly, the value lexical
module performs best thanks to the context information brought
by the tokens’ values. Nevertheless, the CFG, PDG-DFG, and PDG
also are very accurate ways of detecting malicious JavaScript and
add all the more semantic information into the considered features.
3.2.4 Most Important Features for Classification. To accurately dis-
tinguish benign from malicious JavaScript inputs over 97.6% of
the time, JStap leverages differences between benign and mali-
cious samples at several abstract levels (e.g., AST, CFG). Specifically,
using the way in which given lexical and syntactic units are ar-
ranged in JavaScript files, along with their frequency, provides
valuable insight to capture the salient properties of the code and
identify recurrent patterns, specific to malicious or benign intent.
For the ten JStap’s modules, we extracted the five features most
representative of malicious or benign intent, according to the cor-
responding random forest models [47]. For example, the most rep-
resentative feature for the ngrams approach and for the AST, CFG,
PDG-DFG and PDG levels is the following: [MemberExpression,
MemberExpression, Identifier, Identifier], which is in line
with the tokens’ most representative feature, namely [Punctuator,
Identifier, Punctuator, Identifier], and represents an el-
ement of the form a.b.c. We assume that this construct is rather
typical of benign samples, such as jQuery, which define several
objects with multiple properties, while our malicious files rather
store data inside simpler variables or tables. For instance, the fourth
most representative feature of the tokens value module is the
Punctuator "+", which might point to the string splitting/string
concatenation data obfuscation form [56], massively used in ma-
licious samples to evade, e.g., signatures-based detection, while
benign inputs might rather tend to avoid it, du to the resulting
performance downgrade. Similarly, the fifth most important feature
of the AST value module is (NewExpression, ’Array’), which
may this time point to the obfuscation technique where strings are
fetched from a global array.
All in all, malicious JavaScript samples try to hide their mali-
ciousness by using different obfuscation techniques, which leave
specific and recognizable traces in the source code. While benign
documents may also be obfuscated to protect code privacy and
intellectual property, they have more concerns about the perfor-
mance, and therefore use different techniques. For this reason, we
also assume that malicious code is so different from benign inputs
that the natural evolution of the code experienced over a few years
should not change the detection results. Therefore, we consider
that even if our benign (Table 4) and malicious (Table 3) datasets