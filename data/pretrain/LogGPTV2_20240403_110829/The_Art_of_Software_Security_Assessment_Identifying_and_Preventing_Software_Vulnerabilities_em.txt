The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1010 
username/password authentication most other services used at the time. Therefore, 
if attackers could forge DNS responses to make their IP addresses appear to be one 
of the hosts in this file, they could bypass authentication and log in to the target 
machine. These days, DNS names are rarely used to authenticate parties in such a 
direct manner; however, being able to forge DNS responses is still a serious issue. 
The most serious current risk is impersonation of a legitimate site. Malicious nodes 
can pose as legitimate destinations and collect authentication details or other 
sensitive data. For example, attackers could pose as a retailer that clients usually visit 
(such as www.amazon.com/). By posing as the legitimate site and fooling certain 
clients, the malicious users might be able to collect Amazon login credentials and 
credit card information from clients browsing the site. These attackers would have to 
pull a few tricks to make the spoofed site seem authentic, but they can usually fool 
most users. 
Cache Poisoning 
The original resolver algorithm specified in DNS RFCs was vulnerable to a poisoning 
attack that enabled attackers to provide malicious IP addresses for arbitrary domain 
names. Assume that attackers have control of the zone at badperson.com. A victim 
asks the attackers' name server for the A records of www.badperson.com. They can 
respond by delegating authority for the www subdomain to the hostname they want 
to poison. For example, they could include an authority section in the response with 
these NS resource records: 
www.badperson.com. NS ns1.google.com. 
www.badperson.com. NS ns2.google.com. 
www.badperson.com. NS ns3.google.com. 
www.badperson.com. NS ns4.google.com. 
Basically, the attackers are telling the victim that the subdomain 
www.badperson.com is handled by four authoritative name servers, which happen to 
be Google's name servers. The death blow comes in the additional section in the 
response, where attackers place the A resource records for the Google name servers: 
ns1.google.com A 10.20.30.40 
ns2.google.com A 10.20.30.40 
ns3.google.com A 10.20.30.40 
ns4.google.com A 10.20.30.40 
RFC 1034 says the resolution code should check that the delegation is to a "better" 
name server then the one used in the current query. In this example, the query for 
www.badperson.com. was made to the badperson.com. name server. This request is 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1011 
delegated to the Google name servers, but the packet is saying is the name servers 
are authoritative for the www.badperson.com. subdomain. This is good enough to 
pass the algorithm's "better" check. The real problem is the algorithm suggests that 
code blindly honor the supplemental A records that purport to be helpful glue. 
Vulnerable implementations of BIND circa 1997 would enter these A records into the 
cache. Any future requests by victims for a google.com. host would end up contacting 
the attackers' evil name server at 10.20.30.40. 
Windows Resolver Bug 
Windows resolvers also have a bug that allows attackers to hijack popular Web sites 
for specific targets. Say attackers have control of the zone at badperson.com. A 
victim asks their name server for the A records of www.badperson.com. This time, 
attackers can respond by delegating the authority for the com. domain to an evil 
name server under their control. The authority section might contain this NS resource 
record: 
com. NS evil.reallybad.org. 
There's no reason the victim's resolver should honor this response, as it's completely 
illogical. However, Windows cached this NS record because of an implementation bug. 
This means that later, when the resolver needs to contact a name server for the .com 
zone, it contacts evil.reallybad.org instead. Windows NT and Windows 2000 SP1 and 
SP2 were vulnerable by default to this problem, and it also affected various Symantec 
products. 
Spoofing Responses 
Most communications between DNS clients and servers occur over UDP, an unreliable 
and unauthenticated transport. ("Unauthenticated" means there's no way to verify 
that sender are who they say they are.) TCP is also an unauthenticated transport but 
to a much lesser extent. (For more information, refer to Chapter 14(? [????.]).) 
Therefore, how does a client or server know a request is from a legitimate source? 
The answer is simple: They don't, in a lot of circumstances! The traditional way of 
validating DNS responses is using the DNS ID field in the header. When a DNS client 
generates a question, it assigns an (ostensibly) random number for the ID field. When 
it receives responses, it checks that the DNS ID field matches the request. This check 
is done by verifying that the response packet has the same value in the DNS ID field 
as the query packet the client originally sent. With this information, a couple of 
attacks could be launched. One of the most obvious is a man-in-the-middle attack by 
someone in a position to observe DNS traffic. This attack is fairly easy to achieve, so 
chalk it up as a known risk and focus your attention on blind spoofing. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1012 
DNS spoofing issues affect both DNS server and client implementations because 
servers make requests on behalf of clients and usually cache results (if they are 
configured for recursion). When a server issues a DNS request to recursively resolve 
a remote host on behalf of a client, remote responses to servers could be forged and 
subsequently cached. Basically, most attacks of this nature revolve around how 
predictable an implementation's DNS ID generation algorithm is. The simplest 
implementations have fixed increments (usually of 1) for each question they generate. 
In the past, BIND (one of the premier name servers on the Internet) was vulnerable 
to this problem, as pointed out by Secure Networks Inc. and CORE (documented at 
http://attrition.org/security/advisory/nai/SNI-12.BIND.advisory). The advisory 
walks through the steps required to cache poison name servers by forging responses 
from a remote DNS server. 
Note 
In some ways, this attack is not unlike the TCP sequence number spoofing mentioned 
in Chapter 14(? [????.]), except DNS IDs need to be exact. Injecting TCP data just 
requires a sequence number within the TCP window. 
Dan Bernstein gives a great summary of the current risks of blind forgery at 
http://cr.yp.to/djbdns/forgery.html: 
An attacker from anywhere on the Internet, without access to the client network and 
without access to the server network, can also forge responses, although not so easily. 
In particular, he has to guess the query time, the DNS ID (16 bits), and the DNS 
query port (15-16 bits). The dnscache program uses a cryptographic generator for 
the ID and query port to make them extremely difficult to predict. However, 
an attacker who makes a few billion random guesses is likely to succeed at 
least once; 
tens of millions of guesses are adequate with a colliding attack; 
against BIND, a hundred thousand guesses are adequate, because BIND 
keeps using the same port for every query; and 
against old versions of BIND, a thousand guesses are adequate with a colliding 
attack. 
The lack of authentication in this protocol is a recognized problem, and steps have 
been taken to help secure it. Specifically, DNS messages can be cryptographically 
verified by using the TKEY and TSIG record types, but this method isn't yet used 
extensively (even though most implementations support it). For this reason, you 
can't assume that cryptographic verification protects an implementation from DNS ID 
prediction vulnerabilities unless the implementation you're reviewing mandates the 
use of the DNS cryptographic features. DNS ID generation algorithms based on 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1013 
known values also might not be very secure. For example, a DNS ID based on the 
time returned from the time() functions might be quite easy to guess. 
8.3.7 Summary 
This chapter has described a general process for assessing network protocol 
implementations. To supplement that process, you have also walked through 
identifying vulnerabilities in several popular protocols. Although this chapter isn't an 
exhaustive coverage of protocols, it should certainly give you a firm grasp of how to 
assess an unfamiliar implementation. You should feel comfortable with applying these 
same basic techniques to reviewing an implementation of a file format specification or 
other data-exchange method. 
8.4 Chapter 17.  Web Applications 
Chapter 17. Web Applications 
"Maybe this world is another planet's hell." 
Aldous Huxley, Brave New World 
8.4.1 Introduction 
Web applications are one of the most popular areas of modern software development; 
in fact, they might be the single biggest innovation of the dot-com era. In less than a 
decade, they've caused a simple communications protocol (HTTP) to become a 
primary means of modern interaction. The rapid uptake of Web applications is a result 
of their capability to provide convenient access to information and services in ways 
not previously possible. The downside is that Web applications have introduced a new 
array of security concerns and vulnerability classes, so you'll almost certainly be 
required to assess the security of Web applications. This task can be formidable 
because the Web exists as a loose collection of rapidly developing technologies. This 
collection often includes abstruse architectural patterns intertwined with third-party 
middleware and Web server platforms. However, you can use some basic strategies 
to cut through the dizzying array of technologies and focus on the bottom line: finding 
security vulnerabilities. Of course, much of modern Web application development is 
tied to complex third-party frameworks, so security reviewers should augment Web 
application source-code reviews with operational reviews and live testing. 
Web programming has been divided into two chapters. This chapter gives you an 
overview of the Web and HTTP, the basic design challenges facing Web developers, 
and a brief survey of Web programming technologies. Then you learn general 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1014 
strategies and techniques for auditing Web applications and operational concerns 
with the Web environment. Finally, you learn about the types of vulnerabilities that 
plague these programs and how to find them. Chapter 18(? [????.]), "Web 
Technologies," covers some popular Web development technologies and examines 
their security issues. 
8.4.2 Web Technology Overview 
Developing a Web site might seem straightforward or at least easier than developing 
a full-blown cross-platform networked application. For better or worse, Web 
technology has evolved to the point that developing a Web application is almost as 
complex as other networked services. This following paragraph is from the 
documentation for a popular open-source Web framework, Apache Struts: 
The core of the Struts framework is a flexible control layer based on standard 
technologies like Java Servlets, JavaBeans, ResourceBundles, and XML, as well as 
various Jakarta Commons packages. Struts encourages application architectures 
based on the Model 2 approach, a variation of the classic Model-View-Controller (MVC) 
design paradigm. 
Struts provides its own Controller component and integrates with other technologies 
to provide the Model and the View. For the Model, Struts can interact with standard 
data access technologies, like JDBC and EJB, as well as most any third-party 
packages, like Hibernate, iBATIS, or Object Relational Bridge. For the View, Struts 
works well with JavaServer Pages, including JSTL and JSF, as well as Velocity 
Templates, XSLT, and other presentation systems. 
If you understand all that, you can probably skip the first half of this chapter. If you 
don't, this chapter and the next cover enough ground that you'll be able to at least 
approach it. The Struts framework isn't alone in the Web space as far as complexity 
and approachability. The point is that you need to consider these details when 
reviewing enterprise-class Web applications. You need to budget a good deal of 
preparation time or find a strategy for dealing with unfamiliar and complex 
technology. The remainder of this section provides an overview of the general 
principles and common elements of the most popular web technologies. 
The Basics 
The World Wide Web (WWW) is a distributed global network of servers that publishes 
documents over various protocols, such as gopher, FTP, and HTTP. A document, or 
resource, is identified by a Uniform Resource Identifier (URI), such as 
http://www.neohapsis.com/index.html. This URI is the identifier for the HTML 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1015 
document located on the www.neohapsis.com Web server at /index.html, which can 
be retrieved via and HTTP request. 
Hypertext Markup Language (HTML) is a simple language for marking up text 
documents with tags that identify semantic structure and visual presentation. HTML is 
a Standard Generalized Markup Language (SGML) applicationthat is, a markup 
language defined in SGML. A key concept in HTML is the hyperlink, which is a 
reference to another resource on another server (given as a URI). One of the defining 
characteristics of the Web is that it's composed largely of hypertextinterconnected 
documents that reference each other via hyperlinks. 
Hypertext Transport Protocol (HTTP) is a simple protocol that Web servers use to 
make documents available to clients (discussed in more detail in "HTTP(? [????.])" 
later in this chapter). A Web client, or Web browser, connects to a Web server by 
using a TCP connection and issues a simple request for a URI path, such as 
/index.html. The server then returns this document over the connection or notifies 
the client if there has been an error condition. Web servers typically listen on port 80. 
SSL-wrapped HTTP (known as HTTPS) is typically available on port 443. 
Static Content 
The most straightforward request a Web server can broker is for a file sitting on its 
local file system or in memory. The Web server simply retrieves the file and sends it 
to the network as the HTTP response. This process is known as serving static 
content because the document is the same for every user every time it's served. 
Static content is great for data that doesn't change often, like your Star Trek Web site 
or pictures of your extensive collection of potted meat products. However, more 
complex Web sites need to be able to control the Web server's output 
programmatically. The Web server needs to create content on the fly that reacts to 
users' actions so that it can exhibit the behavior of an application. Naturally, there are 
myriad ways a programmer can interface with a Web server to create this dynamic 
content. 
CGI 
Common Gateway Interface (CGI) is one of the oldest mechanisms for creating 
dynamic Web content. A CGI program simply takes input from the Web server via 
environment variables, the command line, and standard input. This input describes 
the request the user made to the Web server. The CGI program performs some 
processing on this input, and then writes its output (usually an HTML document) to 
standard output. When a Web server receives a request for a CGI program, it simply 
forks and runs that program as a new process, and then relays the program's output 
back to the user. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1016 
CGI programs can be written in almost any language, as the only real requirement is 
the ability to write to STDOUT. Perl is a popular choice because of its string 
manipulation features, as are Python and Ruby. Here is a bare-bones CGI program in 
Perl: 
#!/usr/bin/perl 
print "Content-type: text/html\r\n\r\n"; 
print "hi!\r\n"; 
The primary disadvantage of the CGI model is that it requires a separate process for 
each Web request, which means it isn't well suited to handling heavy traffic. Modified 
interfaces are available, such as FastCGI, that allow a more lightweight 
request-handling process, but CGI-style programs are typically used for low-traffic 
applications. 
Web Server APIs 
Most Web servers provide an API that enables developers to customize the server's 
behavior. These APIs are provided by creating a shared library or dynamic link library 
(DLL) in C or C++ that's loaded into the Web server at runtime. These Web server 
extensions can be used for creating dynamic content, as Web requests can be passed 
to developer-supplied functions that process them and generate responses. These 
extensions also allow global modification of the server, so developers can perform 
analysis or processing of every request the server handles. These APIs allow far more 
customization than an interface such as CGI because Web developers can alter the 
behavior of the Web server at a very granular level by manipulating shared data 
structures and using control APIs and callbacks. Here are the common interfaces: 
Internet Server Application Programming Interface (ISAPI) Microsoft provides 
this API for extending the functionality of its Internet Information Services 
(IIS) Web server. ISAPI filters and DLLs are often found in older 
Microsoft-based Web applications, particularly in Web interfaces to 
commercial software packages. 
Netscape Server Application Programming Interface (NSAPI) Netscape's Web 
server control API can be used to extend Netscape's line of servers and Web 
proxies. It's occasionally used in older enterprise applications for global input 
validation as a first line of defense. 
Apache API This API supports extension of the Apache open-source Web 
server via modules and filters. 
Many of the other Web programming technologies discussed in this chapter are 
implemented on top of these Web server APIs. Modern Web servers are usually 
constructed in an open, modular fashion. Therefore, these extension APIs can be used 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1017 
to make changes commensurate with what you'd expect from full source-code-based 
modifications of the Web server. 
Server-Side Includes 
A Web server doesn't examine a typical static HTML document when presenting it to 
a Web browser. The server simply reads the document from memory or disk and 
sends it out over the network without looking at the document's contents. Several 
technologies are based on slightly altering this design so that the Web server inspects 
and processes the document while it serves it to the client. These technologies range 
in complexity from simple directives to the Web server, to full programming language 
interpreters embedded in the Web server. 
The simplest and oldest form of server-side document processing is server-side 
includes (SSIs), which are specially formatted tags placed in HTML pages. These 
tags are simple directives to the Web server that are followed as a document is 
presented to a user. As the Web server outputs the document, it pulls out the SSI tags 
and performs the appropriate actions. These tags provide basic functionality and can 
be used to create simple dynamic content. Most Web servers support them in some 
fashion. Take a look at a few examples of SSIs. The following command prints the 
value of the Web server variable DOCUMENT_NAME, which is the name of the requested 
document: 
The current page is  
The following SSI directs the server to retrieve the file /footer.html and replace the 
#include tag with the contents of that file: 
When the Web server parses the following tag, it runs the ls command and replaces 
the #exec tag with its results: 
As a security reviewer, SSI functionality should make your ears perk up a little. You 
learn more some handling issues with SSI in "Programmatic SSI(? [????.])" later in 
this chapter. 
Server-Side Transformation 
Storing the content of a Web site in a format other than HTML is often advantageous. 
This content might be generated by another program or tool in a common format such 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
1018 
as XML, or it might reside on a live resource, such as a database server. Web 
developers can use server-side parsing technologies to instruct the Web server to 
automatically transform content into HTML on the fly. These technologies are more 
involved than server-side includes, but they aren't as sophisticated as the more 
popular full server-side scripting implementations. 
Extensible Stylesheet Language Transformation (XSLT) is a general language 
that describes how to turn one XML document into another XML document. Web 
developers can use XSLT to tell a Web server how to transform a XML document 
containing a page's content into an HTML document that's presented to users. Say 
you have the following simple XML document describing a person: 
    Zoe 
    1 
An XSLT style sheet that describes how to turn this XML document into HTML could 
look something like this: 