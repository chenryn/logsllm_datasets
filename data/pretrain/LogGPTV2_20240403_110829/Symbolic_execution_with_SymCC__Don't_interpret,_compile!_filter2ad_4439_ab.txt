2.4 Reducing overhead
In either type of symbolic execution, IR-based and IR-less,
building symbolic expressions and passing them to the sym-
bolic backend is necessary only when computations involve
symbolic data. Otherwise, the result is completely indepen-
dent of user input and is thus irrelevant for whatever reasoning
is performed in the backend. A common optimization strat-
egy is therefore to restrict symbolic handling to computations
on symbolic data and resort to a faster execution mechanism
otherwise, a strategy that we call concreteness checks. In IR-
based implementations, symbolic interpretation of IR may
even alternate with native execution of machine code on the
real or a fast emulated CPU; angr [37], for example, follows
this approach. Implementations vary in the scope of their
concreteness checks—while QSYM [45] decides whether
to invoke the symbolic backend on a per-instruction basis,
angr [37] places hooks on relevant operations such as mem-
ory and register accesses. Falling back to a fast execution
scheme as often as possible is an important optimization,
which we also implement in SYMCC (see Section 3.4).
3 Compilation-based symbolic execution
We now describe our compilation-based approach, which dif-
fers from both conventional IR-based and IR-less symbolic
execution but combines many of their advantages. The high-
level goal of our approach is to accelerate the execution part of
symbolic execution (as outlined in Section 2.1) by compiling
symbolic handling of computations into the target program.
The rest of this section is devoted to making this statement
more precise; in the next section, we describe the actual im-
plementation.
3.1 Overview
An interpreter processes a target program instruction by in-
struction, dispatching on each opcode and performing the
required actions. A compiler, in contrast, passes over the tar-
get ahead of time and replaces each high-level instruction
with a sequence of equivalent machine-code instructions. At
execution time, the CPU can therefore run the program di-
rectly. This means that an interpreter performs work during
every execution that a compiler needs to do only once.
In the context of symbolic execution, current approaches
either interpret (in the case of IR-based implementations) or
run directly on the CPU but with an attached observer (in IR-
less implementations), performing intermittent computations
that are not part of the target program. Informally speaking,
IR-based approaches are easy to implement and maintain
but rather slow, while IR-less techniques reach a high perfor-
mance but are complex to implement. The core claim of this
paper is that we can combine the advantages of both worlds,
i.e., build a system that is easy to implement yet fast. To do so,
we compile the logic of the symbolic interpreter (or observer)
into the target program. Contrary to early implementations
of symbolic execution [6, 16, 35], we do not perform this em-
bedding at the source-code level but instead work with the
compiler’s intermediate representation, which allows us to
remain independent of the source language that the program
under test is written in, as well as independent of the target
architecture (cf. Section 7).
184    29th USENIX Security Symposium
USENIX Association
define i32 @is_double (i32, i32) {
%3 = shl nsw i32 %1 , 1
%4 = icmp eq i32 %3 , %0
%5 = zext i1 %4 to i32
ret i32 %5
}
Listing 1: An example function in LLVM bitcode. It takes
two integers and checks whether the ﬁrst is exactly twice the
second.
To get an intuition for the process, consider the example
function in Listing 1. It takes two integers and returns 1 if the
ﬁrst integer equals the double of the second, and 0 otherwise.
How would we expect compiler-based symbolic execution to
transform the program in order to capture this computation
symbolically? Listing 2 shows a possible result. The inserted
code calls out to the run-time support library, loaded in the
same process, which creates symbolic expressions and eventu-
ally passes them to the symbolic backend in order to generate
new program inputs (not shown in the example). Note that the
transformation inserting those calls happens at compile time;
at run time, the program “knows” how to inform the symbolic
backend about its computations without requiring any exter-
nal help and thus without incurring a signiﬁcant slowdown.
Figure 4 summarizes the approach; note how it contrasts with
the conventional techniques depicted in Figures 2 and 3. We
will now go over the details of the technique.
define i32 @is_double (i32, i32) {
; symbolic computation
%3 = call i8* @_sym_get_parameter_expression (i8 0)
%4 = call i8* @_sym_get_parameter_expression (i8 1)
%5 = call i8* @_sym_build_integer (i64 1)
%6 = call i8* @_sym_build_shift_left (i8* %4 , i8* %5)
%7 = call i8* @_sym_build_equal (i8* %6 , i8* %3)
%8 = call i8* @_sym_build_bool_to_bits (i8* %7)
; concrete computation (as before)
%9 = shl nsw i32 %1 , 1
%10 = icmp eq i32 %9 , %0
%11 = zext i1 %10 to i32
call void @_sym_set_return_expression (i8* %8)
ret i32 %11
}
Listing 2: Simpliﬁed instrumentation of Listing 1. The called
functions are part of the support library. The actual instru-
mentation is slightly more complex because it accounts for
the possibility of non-symbolic function parameters, in which
case the symbolic computation can be skipped.
3.2 Support library
Since we compile symbolic execution capabilities into the tar-
get program, all components of a typical symbolic execution
engine need to be available. We therefore bundle the sym-
bolic backend into a library that is used by the target program.
The library exposes entry points into the symbolic backend
Figure 4: Our compilation-based approach compiles symbolic
execution capabilities directly into the target program.
to be called from the instrumented target, e.g., functions to
build symbolic expressions and to inform the backend about
conditional jumps.
3.3 Symbolic handlers
The core of our compile-time transformation is the inser-
tion of calls to handle symbolic computations. The compiler
walks over the entire program and inserts calls to the symbolic
backend for each computation. For example, where the target
program checks the contents of two variables for equality,
the compiler inserts code to obtain symbolic expressions for
both operands, to build the resulting “equals” expression and
to associate it with the variable receiving the result (see ex-
pression %7 in Listing 2). The code is generated at compile
time and embedded into the binary. This process replaces a
lot of the symbolic handling that conventional symbolic ex-
ecution engines have to perform at run time. Our compiler
instruments the target program exactly once—afterwards, the
resulting binary can run on different inputs without the need
to repeat the instrumentation process, which is particularly ef-
fective when combined with a fuzzer. Moreover, the inserted
handling becomes an integral part of the target program, so
it is subject to the usual CPU optimizations like caching and
branch prediction.
3.4 Concreteness checks
It is important to realize that each inserted call to the run-time
support library introduces overhead: it ultimately invokes
the symbolic backend and may put load on the SMT solver.
However, involving the symbolic backend is only necessary
when a computation receives symbolic inputs. There is no
need to inform the backend of fully concrete computations—
we would only incur unnecessary overhead (as discussed in
Section 2.4). There are two stages in our compilation-based
approach where data can be identiﬁed as concrete:
Compile time Compile-time constants, such as offsets into
data structures, magic constants, or default return values
can never become symbolic at run time.
USENIX Association
29th USENIX Security Symposium    185
Compilation	to	IRBitcode	instrumentationpassCodegenerationBinary	execution			Test	casesLLVM bitcode but before the backend transforms the bitcode
into machine code. SYMCC thus needs to support the instruc-
tions and intrinsic functions of the LLVM bitcode language.
We implement the same semantics as IR-based symbolic in-
terpreters of LLVM bitcode, such as KLEE [5] and S2E [9].
In contrast to the interpreters, however, we do not perform
the symbolic computations corresponding to the bitcode in-
structions at instrumentation time but instead generate code
ahead of time that performs them during execution.2 This
means that the instrumentation step happens only once, fol-
lowed by an arbitrary number of executions. Furthermore,
the code that we inject is subject to compiler optimizations
and eventually runs as part of the target program, without
the need to switch back and forth between the target and an
interpreter or attached observer. It is for this reason that we
implemented the instrumentation logic from scratch instead
of reusing code from KLEE or others: those systems perform
run-time instrumentation whereas our implementation needs
to instrument the target at compile time.
There is a trade-off in positioning SYMCC’s pass relative to
the various optimization steps. Early in the optimizer, the bit-
code is still very similar to what the front-end emitted, which
is typically inefﬁcient but relatively simple and restricted to
a subset of the LLVM bitcode instruction set. In contrast, at
later stages of the optimizer pipeline, dead code has been op-
timized away and expensive expressions (e.g., multiplication)
have been replaced with cheaper ones (e.g., bit shifts); such
optimized code allows for less and cheaper instrumentation
but requires handling a larger portion of the instruction set. In
the current implementation, our pass runs in the middle of the
optimization pipeline, after basic optimizations like dead-code
elimination and strength reduction but before the vectorizer
(i.e., the stage that replaces loops with SIMD instructions on
supported architectures). Running our code even later could
improve the performance of compiled programs but would
complicate our implementation by requiring us to implement
symbolic handling of vector operations; we opted for imple-
mentation simplicity. It would be interesting to experiment
more with the various options of positioning SYMCC in the
optimization pipeline; we defer such improvements to future
work.
In a recent study, we found that symbolic execution is
fastest when it executes at the level of machine code, but
that SMT queries are easiest when generated based on the
higher-level semantics of an intermediate representation [32].
This is exactly the setup of SYMCC: we reason about compu-
tations at the level of LLVM bitcode, but the injected code is
compiled down to efﬁcient machine code.
Run time In many cases, however, the compiler cannot know
whether data will be concrete or symbolic at run time,
e.g., when it is read from memory: a memory cell may
contain either symbolic or concrete data, and its con-
creteness can change during the course of execution. In
those cases, we can only check at run time and prevent
invocation of the symbolic backend dynamically if all
inputs of a computation are concrete.
Consequently, in the code we generate, we omit calls to the
symbolic backend if data is known to be constant at compile
time. Moreover, in the remaining cases, we insert run-time
checks to limit backend calls to situations where at least one
input of a computation is symbolic (and thus the result may
be, too).
4 Implementation of SymCC
We now describe SYMCC, our implementation of compiler-
based symbolic execution. We built SYMCC on top of the
LLVM compiler framework [25]. Compile-time instrumen-
tation is achieved by means of a custom compiler pass, writ-
ten from scratch. It walks the LLVM bitcode produced by
the compiler frontend and inserts the code for symbolic han-
dling (as discussed in Section 3.3). The inserted code calls
the functions exported by the symbolic backend: we provide
a thin wrapper around the Z3 SMT solver [11], as well as
optional integration with the more sophisticated backend of
QSYM [45]. The compiler pass consists of roughly 1,000
lines of C++ code; the run-time support library, also written
in C++, comprises another 1,000 lines (excluding Z3 and the
optional QSYM code). The relatively small code base shows
that the approach is conceptually simple, thus decreasing the
probability of implementation bugs.
The remainder of this section describes relevant implemen-
tation details before we evaluate SYMCC in the next section.
For additional documentation of low-level internals we refer
interested readers to the complementary material included
in the source repository at http://www.s3.eurecom.fr/
tools/symbolic_execution/symcc.html.
4.1 Compile-time instrumentation
The instrumentation inserted by our compiler extension leaves
the basic behavior of the target program unmodiﬁed; it merely
enhances it with symbolic reasoning. In other words, the in-
strumented program still executes along the same path and
produces the same effects as the original program, but addi-
tionally uses the symbolic backend to generate new program
inputs that increase code coverage or possibly trigger bugs in
the target program.
Since our compiler extension is implemented as an LLVM
pass, it runs in the “middle-end” of LLVM-based compilers—
after the frontend has translated the source language into
It is sometimes argued that binary-based vulnerability
2This also distinguishes our approach from what the formal veriﬁcation
community calls symbolic compilation [42]. Symbolic compilers translate
the entire program to a symbolic representation in order to reason about all
execution paths at once, while we—like all symbolic execution systems—
defer reasoning to run time, where it is necessarily restricted to a subset of
all possible execution paths.
186    29th USENIX Security Symposium
USENIX Association
search is more effective than source-based techniques be-
cause it examines the instructions that the processor executes
instead of a higher-level representation; it can discover bugs
that are introduced during compilation. A full evaluation of
this claim is outside the scope of this paper. However, we
remark that SYMCC could address concerns about compiler-
introduced bugs by performing its instrumentation at the very
end of the optimization pipeline, just before code generation.
At this point, all compiler optimizations that may introduce
vulnerabilities have been performed, so SYMCC would in-
strument an almost ﬁnal version of the program—only the
code-generation step needs to be trusted. We have not seen
the need for such a change in practice, so we leave it to future
work.
The reader may wonder whether SYMCC is compatible
with compiler-based sanitizers, such as address sanitizer [36]
or memory sanitizer [38]. In principle, there is no problem
in combining them. Recent work by Österlund et al. shows
that sanitizer instrumentation can help to guide fuzzers [31].
We think that there is potential in the analogous application
of the idea to symbolic execution—sanitizer checks could
inform symbolic execution systems where generating new
inputs is most promising. However, our current implementa-
tion, like most concolic execution systems, separates test case
generation from input evaluation: sanitizers check whether
the current input leads to unwanted behavior, while SYMCC
generates new inputs from the current one. We leave the ex-
ploration of sanitizer-guided symbolic execution in the spirit
of Österlund et al. to future work.
4.2 Shadow memory
In general, we store the symbolic expressions associated with
data in a shadow region in memory. Our run-time support li-
brary keeps track of memory allocations in the target program
and maps them to shadow regions containing the correspond-
ing symbolic expressions that are allocated on a per-page
basis. There is, however, one special case: the expressions
corresponding to function-local variables are stored in local
variables themselves. This means that they receive the same
treatment as regular data during code generation; in particular,
the compiler’s register allocator may decide to place them in
machine registers for fast access.
It would be possible to replace our allocation-tracking
scheme with an approach where shadow memory is at a ﬁxed
offset from the memory it corresponds to. This is the tech-
nique used by popular LLVM sanitizers [36, 38]. It would
allow constant-time lookup of symbolic expressions, where
currently the lookup time is logarithmic in the number of
memory pages containing symbolic data. However, since this
number is usually very small (in our experience, below 10),