title:TTPDrill: Automatic and Accurate Extraction of Threat Actions from
Unstructured Text of CTI Sources
author:Ghaith Husari and
Ehab Al-Shaer and
Mohiuddin Ahmed and
Bill Chu and
Xi Niu
0
2
0
2
r
p
A
9
2
]
R
C
.
s
c
[
1
v
2
2
3
4
1
.
4
0
0
2
:
v
i
X
r
a
Automated Retrieval of ATT&CK Tactics and
Techniques for Cyber Threat Reports
Valentine Legoy1, Marco Caselli2, Christin Seifert1, and Andreas Peter1
1 University of Twente, 7522 NB Enschede, The Netherlands
{c.seifert, a.peter}@utwente.nl
2 Siemens Corporate Technology, 81739 Munich, Germany
PI:EMAIL
Abstract. Over the last years, threat intelligence sharing has steadily
grown, leading cybersecurity professionals to access increasingly larger
amounts of heterogeneous data. Among those, cyber attacks’ Tactics,
Techniques and Procedures (TTPs) have proven to be particularly valu-
able to characterize threat actors’ behaviors and, thus, improve defensive
countermeasures. Unfortunately, this information is often hidden within
human-readable textual reports and must be extracted manually. In this
paper, we evaluate several classiﬁcation approaches to automatically re-
trieve TTPs from unstructured text. To implement these approaches,
we take advantage of the MITRE ATT&CK framework, an open knowl-
edge base of adversarial tactics and techniques, to train classiﬁers and
label results. Finally, we present rcATT, a tool built on top of our ﬁnd-
ings and freely distributed to the security community to support cyber
threat report automated analysis.
Keywords: Automation · Cyber Threat Intelligence · ATT&CK Tactics
and Techniques · Multi-Label Classiﬁcation
1
Introduction
According to Gartner, Cyber Threat Intelligence (CTI) can be generally iden-
tiﬁed as any piece of information corresponding to “evidence-based knowledge
(...) about an existing or emerging menace”[9]. Despite lacking a precise def-
inition that comprehensively describes its properties, the security community
generally agrees on diﬀerentiating among three main types of CTI: “tactical”,
“operational” and “strategical”. Tactical CTI refers to information that is a
direct manifestation of adversary actions within a compromised system; exam-
ples are IP addresses, ﬁle hashes and other artifacts that are traces of actual
malicious activities. Operational CTI relates to more general information that
has an impact on day-to-day security decision making; for instance statistics
of ongoing cyberattack campaigns requiring security teams to deploy speciﬁc
countermeasures. Finally, strategic CTI refers to knowledge about threat actors’
capabilities and motivations, such as characteristics and behavior of advanced
persistent threats.
2
V. Legoy et al.
Over the last year, the sharing of these three types of CTI has steadily
grown. Threat information sources have increased in number, supported by the
development of standardized technologies and formats, like STIX [3]. Available
information is either open source and freely accessible over the Internet or se-
lectively provided within private feeds managed by specialized companies, like
IBM X Force [13] and OTX AlienVault [1].
It is worth noting that the amount of shared information is not equally
distributed among the three types of CTI. In fact, most sharing activities involve
tactical CTI in the form of Indicators of Compromise (IOCs). Malicious IPs and
URLs as well as signatures of malware executables cover the vast majority of CTI
exchanges while operational and, especially, strategic CTI corresponds to almost
negligible percentages despite their importance [7]. Furthermore, operational and
strategic CTI does not usually come within structured formats but is embedded
within human-readable cyber threat reports (CTRs). If structured data can be
generally handled simply by using web scrapers and parsers, unstructured data
almost always requires security experts to read and manually extract the most
relevant information. Considering the number of CTRs published each day,3
dealing with unstructured security information currently remains a cumbersome
task.
Our work aims to overcome this challenging task by simplifying the extraction
of valuable security information contained in CTRs, namely cyber threats’ Tac-
tics, Techniques and Procedures (TTPs). To achieve this, we use the ATT&CK
(Adversarial Tactics, Techniques, and Common Knowledge) framework [23], an
open knowledge base of adversarial tactics and techniques, and we aim at la-
belling each CTR with the TTPs that most likely match its content. Automat-
ing this labeling allows security experts to use the information within any given
CTR more eﬃciently and thus support the deﬁnition of prevention, detection
and mitigation methods for the related threats. Our contribution speciﬁcally
focuses on three main aspects:
– We implement and evaluate standard multi-label text classiﬁcation models
and adapt them to retrieve ATT&CK tactics and techniques from CTRs;
– We test several post-processing techniques based on the hierarchical struc-
ture of the ATT&CK framework to enhance classiﬁcation results;
– We present and distribute rcATT,4 a tool based on our ﬁndings that: 1. pre-
dicts ATT&CK tactics and techniques related to a given CTR, 2. allows the
user to correct predictions and give feedback on the classiﬁcation engine to
improve results over time, and 3. outputs the results using the Structured
Threat Information eXpression (STIX) format.
Finally, we provide a comparison with available open-source solutions showing
rcATT’s improvements over the state of the art.
3 IBM X Force and OTX AlienVault alone have an average of 15 new reports per day.
4 https://github.com/vlegoy/rcATT
Automated Retrieval of TTPs from Threat Reports
3
2 ATT&CK framework
MITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge)
framework [23] is a “globally-accessible knowledge base of adversary tactics and
techniques based on real-world observations”. Its comprehensive collection of
tactics and techniques has gained popularity over recent years and has been
integrated into popular threat information sharing technologies [3].
ATT&CK provides a structured taxonomy to describe several diﬀerent ad-
versary behaviours. It formally divides into three “technology domains”:
– “Enterprise”, which describes behaviours on standard IT systems (e.g., Linux,
Windows).
– “Mobile”, which focuses on mobile devices (e.g., Android, iOS).
– “ICS”, which relates to industrial control and, more in general, to cyber-
physical systems
Beyond these domains, ATT&CK also documents behaviours for reconnaissance
and weaponisation under the “PRE-ATT&CK” designation. Although our work
applies likewise to every domain, we will focus this paper on the ATT&CK
“Enterprise”.
The “Enterprise” ATT&CK framework is usually represented as a matrix of
tactics and techniques where:
– the tactics represent possible goals of an attacker (e.g., “Initial Access”,
“Privilege Escalation”, etc.)
– the techniques identify “how” an attacker might fulﬁll a speciﬁc goal (e.g.,
“Access Token Manipulation”, “Accessibility Features”, etc.)
Tactics and techniques are the key focus of our work as they will correspond
to the labels of our classiﬁcation.
Each technique is associated with one or more tactics. Furthermore, the “En-
terprise” ATT&CK framework collects information about recorded adversary use
of every available technique (e.g., involved threat actors, notorious malware, etc.)
and possible mitigation approaches. All this information will be used to train
our classiﬁers.
2.1 Data Characterization
The actual ATT&CK data employed in our work is stored on GitHub5 and
represented using the STIX 2.0 format [3].
Figure 1 shows the structure of the employed dataset and emphasizes the
relationships linking all available information to the related tactics and tech-
niques. Every piece of information includes several external references (e.g., usu-
ally threat reports or technical descriptions). All text linked by these references
was considered part of the dataset and used to train the classiﬁers.
5 https://github.com/mitre/cti/tree/master/enterprise-attack,
accessed
February 2020
4
V. Legoy et al.
Fig. 1: “Enterprise” ATT&CK data schema and corresponding STIX 2.0 objects
Overall, the “Enterprise” ATT&CK dataset is enriched by 1490 diﬀerent
security reports either in HTML (1311) or in PDF (179) format. We handled
and parsed documents in PDF format using state-of-the-art tools [4]. To extract
content from HTML resources, we focused only on HTML paragraph tags to
simplify the process and avoid noise. All available text was eventually checked
to remove strings that can hinder the classiﬁcation (e.g., we removed all stop
words provided in the list of the Natural Language Toolkit (NLTK) [17])
In the end, all ATT&CK tactics linked to at least 80 reports and provide a
suitable dataset for the training of the classiﬁers. Unfortunately, this was not
the same for the techniques which presented instead an imbalance (e.g., some
techniques corresponding to just one report). We omitted all techniques that
had less than 5 reports in the dataset. This ensures that we can at least train
on 4 reports and have 1 report for testing in a 5-fold cross-validation setting.
3 Methodology overview
Considering that CTRs may refer to more than one tactic (or technique) at the
same time, the machine learning problem is a multi-class multi-label approach.
We tested both general approach to multi-label classiﬁcation: i) dedicated multi-
label classiﬁers and ii) multiple single-label classiﬁers, each of which is responsible
to decide whether its class (e.g., one speciﬁc tactic) should be assigned or not.
During the evaluation, we tested several classiﬁers simultaneously compar-
ing diﬀerent text representation methods such as term-frequency (TF) and term
frequency-inverse document frequency (TF-IDF) weighting factors [19] as well
as Word2Vec [22]. Finally, we deﬁned several post-processing approaches to im-
prove classiﬁcation results. These approaches are based on properties and char-
acteristics of the “Enterprise” ATT&CK framework. For example, we leverage
the hierarchical structure of the framework by ﬁltering the classiﬁcation results
based on a coherent matching of tactics and techniques belonging to those tac-
tics. In all experiments, we performed 5-fold cross-validation.
Automated Retrieval of TTPs from Threat Reports
5
3.1 Metrics
Our evaluation metrics of choice are: precision, recall, and F0.5 score. The F0.5
score represents a weighted average between the precision π and recall ρ where
the precision is considered twice as important as the recall. The F0.5 is computed
as follows:
π =
T P
T P + F P
,
ρ =
T P
T P + F N
, F0.5 = 1 + 0.52 ·
π · ρ
0.52π + ρ
with TP being true positives, FP false positives and FN false negatives. We
chose F0.5 to emphasise precision over recall, i.e., the assigned labels need to be
correct (precision), but there might be labels we do not assign (recall). Finally,
we employ macro-averaging and micro-averaging [30].
3.2 Baseline
To simplify the analysis of diﬀerent classiﬁcation models and quickly identify
unsuccessful ones, we deﬁned a “naive” baseline by simply attributing to each
testing instance the most frequent label of the training set. Table 1 shows the
results of this baseline.
Table 1: Naive baseline based on the attribution of the most frequent label
Majority
Tactics
Techniques
Micro
Macro
Precision Recall F0.5 Precision Recall F0.5
48.72% 19.00% 37.10% 4.43% 9.09% 4.93%
9.57% 2.51% 6.11% 0.05% 0.48% 0.06%
4 Multi-label classiﬁcation
As introduced in the previous section, our evaluation consists on testing diﬀerent
text representation methods with several multi-label classiﬁcation models.
4.1 Text representation
Our primary approach is based on standard weighting functions such as term-
frequency (TF) and term frequency-inverse document frequency (TF-IDF) [19].
These weighting functions were tested employing either simple bag-of-words or
bi-grams and tri-grams. The use of TF and TF-IDF weighting functions relied
on the application of a maximum and a minimum frequency of appearance in
the overall corpus. According to the performed tests, a simple bags-of-words rep-
resentation performed better than any other grouping technique. Furthermore,
6
V. Legoy et al.
decreasing the number of features and thus selecting half of the words with high-
est TF/TF-IDF scores outperformed both the choice of a constant number of
words for each report or not limiting the number of features at all.
Besides TF and TF-IDF, our analysis includes tests based on Word2Vec [22].
In this case, Word2Vec was trained on our dataset instead of taking advantage
of the existing pre-trained versions. This was due to some inconsistencies found
in some early results. Tests with Word2Vec were performed both averaging and
summing word vectors representing the text.
4.2 Classiﬁcation models
Our analysis includes two main strategies to tackle the multi-label classiﬁcation
problem: adapting state-of-the-art classiﬁcation algorithms or approaching the
overall problem from a diﬀerent perspective. In this last case, the approaches
are: using binary relevance and training a binary classiﬁer for each label inde-
pendently [18], employing a classiﬁer chain (which is similar to binary relevance
but uses the relation between labels [26]), or adopting label power sets (basi-
cally transforming them into a multi-class problems between all labels combi-
nation [29]). A power-set model is, however, diﬃcult to apply to our case, as
we have too many labels and not enough data to cover all possible combina-
tions. Therefore, we only focus on binary relevance and classiﬁer chains and test
diﬀerent types of classiﬁers.
All implementations of multi-label classiﬁers are built on top of the Scikit-
learn library (version 0.21.3) [27]. Our tests include: multi-label K-Nearest Neigh-
bours, multi-label Decision Tree and Extra Tree techniques, and Extra Trees,
Random Forest ensemble methods for multi-label classiﬁcation. For what con-
cerns binary relevance and classiﬁer chains, we tested several linear models.
These are: Logistic Regression, Perceptron, Ridge, Bernoulli Naive Bayes classi-
ﬁer, K-Nearest Neighbors and Linear Support Vector Machine (Linear SVC). We
also included multiple tree-based models such as: Decision Tree, Extra Tree clas-
siﬁers, Random Forest, Extra Trees, AdaBoost Decision Tree, Bagging Decision
Tree and Gradient Boosting.
It is to be noted that in classiﬁcation models were the correlations between
labels is used (i.e. multi-label classiﬁers and classiﬁer chains), we predicted the
tactics and techniques together, with the best parameters possible for both label
types. In the cases in which the label relationship did not matter (i.e. binary
relevance), we split the classiﬁcation by tactics and techniques, applying to each
the best parameters possible for their category.
To avoid overﬁtting, in addition to the previously mentioned reduction of
the number of features, we tested regularising models, ﬁne-tuning the hyper-
parameters of the classiﬁers and aim at simple models.
As an attempt to solve the imbalance in the dataset, we used a random
resampling approach on the models with binary relevance, associated with TF
and TF-IDF. In the case of the tactics, the resampling method was a combination
of over- and undersampling for all tactics to have in their training set with
Automated Retrieval of TTPs from Threat Reports
7
400 positive reports and 400 negative reports. For the techniques, we randomly
sampled 125 positive reports and 500 negative reports.
4.3 Results and discussion