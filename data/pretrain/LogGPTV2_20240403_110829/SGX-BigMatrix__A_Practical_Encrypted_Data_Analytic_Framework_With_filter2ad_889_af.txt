Curve Sampling Method Applied to Model-Based Clustering. Journal of Machine
Learning Research 2 (2002), 397.
[44] Muhammad Naveed, Seny Kamara, and Charles V Wright. 2015. Inference attacks
on property-preserving encrypted databases. In Proceedings of the 22nd ACM
SIGSAC Conference on Computer and Communications Security. ACM, 644–655.
[45] John Neter, Michael H Kutner, Christopher J Nachtsheim, and William Wasser-
man. 1996. Applied linear statistical models. Vol. 4. Irwin Chicago.
[46] Olga Ohrimenko, Felix Schuster, Cédric Fournet, Aastha Mehta, Sebastian
Nowozin, Kapil Vaswani, and Manuel Costa. 2016. Oblivious Multi-Party Machine
Learning on Trusted Processors. In 25th USENIX Security Symposium (USENIX
Security 16). USENIX Association, Austin, TX, 619–636. https://www.usenix.org/
conference/usenixsecurity16/technical-sessions/presentation/ohrimenko
[47] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The
PageRank citation ranking: Bringing order to the web. Technical Report. Stanford
InfoLab.
[48] Rafael Pass, Elaine Shi, and Florian Tramer. 2016. Formal Abstractions for Attested
Execution Secure Processors. Cryptology ePrint Archive, Report 2016/1027.
(2016). http://eprint.iacr.org/2016/1027.
[49] Ashay Rane, Calvin Lin, and Mohit Tiwari. 2015. Raccoon: closing digital side-
channels through obfuscated execution. In 24th USENIX Security Symposium
(USENIX Security 15). 431–446.
[50] Felix Schuster, Manuel Costa, Cédric Fournet, Christos Gkantsidis, Marcus
Peinado, Gloria Mainar-Ruiz, and Mark Russinovich. 2015. VC3: Trustworthy
data analytics in the cloud using SGX. In Security and Privacy (SP), 2015 IEEE
Symposium on. IEEE, 38–54.
[51] George AF Seber and Alan J Lee. 2012. Linear regression analysis. Vol. 936. John
Wiley & Sons.
[52] Ming-Wei Shih, Sangho Lee, Taesoo Kim, and Marcus Peinado. 2017. T-SGX:
Eradicating controlled-channel attacks against enclave programs. In Proceedings
of the 2017 Annual Network and Distributed System Security Symposium (NDSS),
San Diego, CA.
[53] Emil Stefanov, Marten van Dijk, Elaine Shi, Christopher Fletcher, Ling Ren, Xi-
angyao Yu, and Srinivas Devadas. 2013. Path ORAM: An Extremely Simple Obliv-
ious RAM Protocol. In CCS. 299–310. https://doi.org/10.1145/2508859.2516660
[54] Stephen Tu, M Frans Kaashoek, Samuel Madden, and Nickolai Zeldovich. 2013.
Processing analytical queries over encrypted data. In Proceedings of the VLDB
Endowment, Vol. 6. VLDB Endowment, 289–300.
[55] Yuanzhong Xu, Weidong Cui, and Marcus Peinado. 2015. Controlled-Channel At-
tacks: Deterministic Side Channels for Untrusted Operating Systems. In Proceed-
ings of the 2015 IEEE Symposium on Security and Privacy (SP ’15). IEEE Computer
Society, Washington, DC, USA, 640–656. https://doi.org/10.1109/SP.2015.45
[56] Wenting Zheng, Ankur Dave, Jethro Beekman, Raluca Ada Popa, Joseph Gonzalez,
and Ion Stoica. 2017. Opaque: A Data Analytics Platform with Strong Security.
In 14th USENIX Symposium on Networked Systems Design and Implementation
(NSDI 17). USENIX Association, Boston, MA. https://www.usenix.org/conference/
nsdi17/technical-sessions/presentation/zheng
A BIGMATRIX API DESIGN
In this Appendix section we discuss more implementation details
of different important BigMatrix operations. In addition to the de-
scription of the operations, we also include the discussion of the
trace and the cost. We call the information leakage to the adver-
sary the trace. In general, the trace contains any information that
an adversary can observe from the inputs, and also entering calls
(ECalls) and out calls (OCalls) made by an enclave. The cost is the
computation and communication cost of each operation. Since, in-
dividual operations are data independent and these costs will be
the same for every possible input and given only the trace as input,
we will be able to compute the cost. During our programming lan-
guage construction, we use these cost functions to find the optimal
execution plan.
Notations. We use A[i, j] to mean the element of matrix A at ith
row jth column. A[i, j : y] indicates y number of elements of ith
row from jth column to (j + y)th column. A(p,q) represents the
block at pth row and qit column. A(p:x,q:y) means a sub-matrix of
x row blocks and y column blocks of A starting at pth row block
and qth column block.
A.1 Matrix scalar operation
Let, A be a m × n matrix that is split into p × q blocks, ⊙ be a
binary operation, v be a value, and C be the output matrix of same
dimensions. So the scalar operation can be defined as C[i, j] =
A[i, j] ⊙ v. Using BigMatrix abstraction we perform,
C(α, β) = A(α, β) ⊙ v
for all the 1 ≤ α ≤ p and 1 ≤ β ≤ q to compute desired output.
The trace of this operation consists of size of the matrices, the
block size, the sequence of read block requests of A, and the se-
quence of write block request for C. After loading a block we access
all the elements once and we do not perform any data dependent
operations. As a result, this operation is data oblivious, i.e., the
adversary will not be able to distinguish two datasets from the
traces.
A.2 Matrix element-wise operation
Let, A and B be two matrices of m × n dimension, and ⊙ be a binary
operation such as multiplication, addition, subtraction, division,
bit-wise and, bit-wise or, etc., C be the output of o operation applied
element-wise between A and B. Meaning, C[i, j] = A[i, j] ⊙ B[i, j]
for all 1 ≤ i ≤ m and 1 ≤ j ≤ n, where A[i, j] means ith row and
jth column element in matrix A.
Now, let’s assume that A, B and C is too large to fit into the
enclave memory and A, B, C are split into p × q number of blocks.
Using BigMatrix abstraction we perform
C(α, β) = A(α, β) ⊙ B(α, β)
for all the 1 ≤ α ≤ p and 1 ≤ β ≤ q to compute desired output.
The trace for this operation consists of the size of matrices, the
block size, the sequence of read requests block-by-block for A, B,
and the sequence of write request for C. Once in memory each
element is touched only once. Furthermore, we are not performing
any data dependent operations.
A.3 Matrix multiplication
Let, A be a m × p matrix, B be a p × n matrix, A be split into q × s
blocks, B be split into s × r blocks, and C be the output of AB. We
can compute C with
s
σ =1
C(α, β) =
A(α,σ)B(σ, β)
where M(x,y) indicates (x, y) block of matrix M.
The trace of this operation contains the size and block size of
A, B, and C, the sequence of read requests for matrix A, B, and the
sequence of write request for C. Similar to previous operations we
do not perform any data dependent operations so this operation is
data oblivious.
A.4 Matrix inverse
Performing matrix inverse is comparatively complicated than other
operations. Let A be a square matrix split into four blocks
F
(cid:170)(cid:174)(cid:172)
G H
A =(cid:169)(cid:173)(cid:171)E
−1 =(cid:169)(cid:173)(cid:171)E−1 + E−1
−S−1
where E and H are square matrices with dimensions m × m and
n×n, respectively. So, F and G are m×n and n×m dimension array.
The inverse can then be computed
FS−1
GE−1
GE−1 −E−1
FS−1
S−1
A
where, S = H − GE−1
F. Also, E and S must have non-zero determi-
nants. This format requires several multiplications and inverses. In
a naive implementation, we will need a large amount of temporary
memory. We can perform the following sequence of operations to
inverse a matrix with manageable memory overhead.
• We perform E−1 in place and our BigMatrix internal state is
as follows
(cid:170)(cid:174)(cid:172)
• We multiply E−1 times block F and negate the result and re-
place F with the result. BigMatrix internal state is as follows
G
H
• Next, we multiply G with −E−1
replace H, leading to BigMatrix internal state of
F and subtract from H and
F
G
H
(cid:169)(cid:173)(cid:171)E−1
(cid:170)(cid:174)(cid:172)
(cid:169)(cid:173)(cid:171)E−1 −E−1
(cid:169)(cid:173)(cid:171)E−1
(cid:169)(cid:173)(cid:171)E−1 −E−1
(cid:169)(cid:173)(cid:171)E−1 −E−1
S−1
S−1
G
G
F
(cid:170)(cid:174)(cid:172)
F
F
(cid:170)(cid:174)(cid:172)
(cid:170)(cid:174)(cid:172)
−E−1
F
H − GE−1
F
(cid:170)(cid:174)(cid:172)
(cid:169)(cid:173)(cid:171)
E−1
GE−1
−S−1
F
−E−1
S−1
(cid:170)(cid:174)(cid:172)
Here, H − GE−1
• We compute S−1 and replace S, so we have
F is S.
• Next, we compute GE−1 and replace G, so we have
G
GE−1 by multiplying the last two re-
• Now we compute S−1
sults. We negate the result and replace G, so our BigMatrix
looks like
• We multiply the off diagonal elements and add it to E−1
block, so that we have
F with S−1, replace −E−1
F and we
−S−1
FS−1
GE−1
• Finally we multiply −E−1
get the intended result.
FS−1
GE−1
(cid:169)(cid:173)(cid:171)E−1 + E−1
(cid:169)(cid:173)(cid:171)E−1 + E−1
−S−1
GE−1 −E−1
S−1
F
GE−1 −E−1
FS−1
S−1
(cid:170)(cid:174)(cid:172)
(cid:170)(cid:174)(cid:172)
We can perform these operations with temporary memory equal
to the size of input BigMatrix. Now, we have built an iterative
algorithm to perform the inverse. It starts with block (0, 0) and
in each iteration it expands inverse by one block as described in
Algorithm 1. In this algorithm we need to inverse 1 × 1 blocks. To
achieve that we use a traditional LU decomposition technique with
a fixed number of rounds depending on the size of matrix not on
the data.
Algorithm 1 Matrix inverse by block iterative method.
Require: A = Square matrix split into blocks
A(0:1,0:1) = inverse(A(0:1,0:1))
for i = 1 to number of blocks in A do
e = (0 : i, 0 : i)
f = (0 : i, i : 1)
д = (i : 1, 0 : i)
h = (i : 1, i : 1)
Af = −1 ∗ Ae ∗ Af
Ah = Ah + Aд ∗ Af
Ah = inverse(Ah)
Aд = Ah ∗ Ae
Aд = −1 ∗ Ah ∗ Aд
Ae = Ae + Af ∗ Aд
Af = −1 ∗ Af ∗ Ah
end for
The trace of the matrix inverse performed in blocks consists
of the trace of individual operations in sequences mentioned by
Algorithm 1. Similar to previous operations, this operation does
not perform any data dependent execution so it is data oblivious.
A.5 Matrix Transpose
Let, A be a matrix of dimension m × n, which is split into p × q
blocks, C be the transpose of A. C[i, j] = A[j, i] for all elements of
A. To compute C in our BigMatrix abstraction we compute
C(α, β) = transpose(A(β,α))
for 1 ≤ α ≤ p and 1 ≤ β ≤ q.
The trace of the transpose operation is the size of the matrix,
the block size, the sequence of read requests for blocks of A, and the
sequence of block write requests for block of B. Furthermore, while
in memory each element value is touched only once and we do not
perform any data dependent operation. As a result, the transpose
operation is data oblivious.
A.6 Sort and Top k
We use Bitonic Sort [17] that performs exactly the same number of
comparisons for the same size dataset. However, the comparison
function in bitonic sort needs special attentions in order to make
it data oblivious. In particular, we used registers to determine the
comparison result of two rows and swap the values accordingly. To
make our framework more practical we allow users to mention a
list of column numbers and the direction of sort for each column.
To make the overall sort operation oblivious, for each row, we read
the full column and touch all the columns, compute a flag value and
swap two rows based on the flag. For top k results, we perform the
full sort and keep only the top k results based on the given criteria.
The trace of the sort function consists of the size of input matrix,
the block size, and the sequence of read and write request for the
matrix. We take input of the sorting direction as a row vector where
each element belongs to {0, 1,−1}, 0 meaning no sorting direction, 1
meaning ascending order, and −1 meaning descending order sorting.
As a result, there is no leakage through sorting order input.
A.7 Selection