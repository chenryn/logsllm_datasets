title:Can We Leverage Predictive Uncertainty to Detect Dataset Shift and
Adversarial Examples in Android Malware Detection?
author:Deqiang Li and
Tian Qiu and
Shuo Chen and
Qianmu Li and
Shouhuai Xu
Can We Leverage Predictive Uncertainty to Detect Dataset Shift
and Adversarial Examples in Android Malware Detection?
Nanjing University of Science and
Nanjing University of Science and
Deqiang Li
Technology
Nanjing, China
PI:EMAIL
Tian Qiu
Technology
Nanjing, China
PI:EMAIL
Shuo Chen
RIKEN
Saitama, Japan
PI:EMAIL
Nanjing University of Science and
University of Colorado Colorado
Qianmu Li∗
Technology
Nanjing, China
PI:EMAIL
Shouhuai Xu
Springs
Colorado Springs, Colorado, USA
PI:EMAIL
ABSTRACT
The deep learning approach to detecting malicious software (mal-
ware) is promising but has yet to tackle the problem of dataset
shift, namely that the joint distribution of examples and their labels
associated with the test set is different from that of the training set.
This problem causes the degradation of deep learning models with-
out users’ notice. In order to alleviate the problem, one approach
is to let a classifier not only predict the label on a given example
but also present its uncertainty (or confidence) on the predicted
label, whereby a defender can decide whether to use the predicted
label or not. While intuitive and clearly important, the capabilities
and limitations of this approach have not been well understood. In
this paper, we conduct an empirical study to evaluate the quality
of predictive uncertainties of malware detectors. Specifically, we
re-design and build 24 Android malware detectors (by transforming
four off-the-shelf detectors with six calibration methods) and quan-
tify their uncertainties with nine metrics, including three metrics
dealing with data imbalance. Our main findings are: (i) predictive
uncertainty indeed helps achieve reliable malware detection in the
presence of dataset shift, but cannot cope with adversarial evasion
attacks; (ii) approximate Bayesian methods are promising to cali-
brate and generalize malware detectors to deal with dataset shift,
but cannot cope with adversarial evasion attacks; (iii) adversarial
evasion attacks can render calibration methods useless, and it is
an open problem to quantify the uncertainty associated with the
predicted labels of adversarial examples (i.e., it is not effective to
use predictive uncertainty to detect adversarial examples).
∗Also with Wuyi University.
This work is licensed under a Creative Commons Attribution International
4.0 License.
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8579-4/21/12.
https://doi.org/10.1145/3485832.3485916
CCS CONCEPTS
• Security and privacy → Malware and its mitigation; • Math-
ematics of computing → Probabilistic inference problems.
KEYWORDS
malware detection, predictive uncertainty, deep learning, dataset
shift, adversarial malware examples
ACM Reference Format:
Deqiang Li, Tian Qiu, Shuo Chen, Qianmu Li, and Shouhuai Xu. 2021. Can
We Leverage Predictive Uncertainty to Detect Dataset Shift and Adversarial
Examples in Android Malware Detection?. In Annual Computer Security
Applications Conference (ACSAC ’21), December 6–10, 2021, Virtual Event,
USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3485832.
3485916
1 INTRODUCTION
Malware is a big threat to cybersecurity. Despite tremendous efforts,
communities still suffer from this problem because the number of
malware examples consistently increases. For example, Kaspersky
[24] detected 21.6 million unique malware in 2018, 24.6 million in
2019, and 33.4 million in 2020. This highlights the necessity of au-
tomating malware detection via machine learning techniques [48].
However, a fundamental assumption made by machine learning
is that the training data distribution and the test data distribution
are identical. In practice, this assumption is often invalid because
of the dataset shift problem [37]. Note that dataset shift is related
to, but different from, the concept drift problem, which emphasizes
that the conditional distribution of the labels conditioned on the
input associated with the test data is different from its counterpart
associated with the training data [46]. While some people use these
two terms interchangeably [28, 50], we choose to use dataset shift
because it is a broader concept than concept drift.
Several approaches have been proposed for alleviating the dataset
shift problem, such as: periodically retraining malware detectors
[10, 47], extracting invariant features [49], and detecting example
anomalies [20]. One fundamental open problem is to quantify the
uncertainty that is inherent to the outcomes of malware detectors
(or the confidence a detector has in its prediction). A well-calibrated
uncertainty indicates the potential risk of the accuracy decrease
596ACSAC ’21, December 6–10, 2021, Virtual Event, USA
D. Li, T. Qiu, S. Chen, Q. Li, and S. Xu
and enables malware analysts to conduct informative decisions (i.e.,
using the predicted label or not). One may argue that a deep learn-
ing model associates its label space with a probability distribution.
However, this “as is” method is poorly calibrated and causes poor
uncertainty estimates [18]. This problem has motivated researchers
to propose multiple methods to calibrate the probabilities, such
as: variational Bayesian inference [6], Monte Carlo dropout [15],
stochastic gradient MCMC [45], and non-Bayesian methods such
as ensemble [25]. Quantifying the uncertainty associated with pre-
dicted labels (i.e., predictive uncertainty) in the presence of dataset
shift has received a due amount of attention in the context of image
classification [6, 25], medical diagnoses [26], and natural language
processing [39], but not in the context of malware detection. This
motivates us to answer the question in the title.
Our contributions. In this paper, we empirically quantify the pre-
dictive uncertainty of 24 deep learning-based Android malware
detectors. These detectors correspond to combinations of four mal-
ware detectors, which include two multiple layer perceptron based
methods (i.e., DeepDrebin [17] and MultimodalNN [22]) and one
convolutional neural network based method (DeepDroid [30]), and
the recurrent neural network based method (Droidetec [29]); and
six calibration methods, which are vanilla (no effort made for cali-
bration), temperature scaling [18], Monte Carlo (MC) dropout [15],
Variational Bayesian Inference (VBI) [39], deep ensemble [25] and
its weighted version. The vanilla and temperature scaling calibra-
tion methods belong to the post-hoc strategy, while the others are
ad-hoc and require us to transform the layers of an original neural
network in a principled manner (e.g., sampling parameters from a
learned distribution). In order to evaluate the quality of predictive
uncertainty on imbalanced datasets, we propose useful variants of
three standard metrics.
By applying the aforementioned 24 malware detectors to three
Android malware datasets, we draw the following insights: (i) We
can leverage predictive uncertainty to achieve reliable malware de-
tection in the presence of dataset shift to some extent, while noting
that a defender should trust the predicted labels with uncertainty
below a certain threshold. (ii) Approximate Bayesian methods are
promising to calibrate and generalize malware detectors to deal
with dataset shift. (iii) Adversarial evasion attacks can render cali-
bration methods useless and thus the predictive uncertainty.
We have made the code of our framework publicly available
at https://github.com/deqangss/malware-uncertainty. It is worth
mentioning that after the present paper is accepted, we became
aware of a preprint [32], which investigates how to leverage predic-
tive uncertainty to deal with false positives of Windows malware
detectors, rather than dealing with dataset shift issues.
2 PROBLEM STATEMENT
2.1 Android Apps and Malware Detection
Since Android malware is a major problem and deep learning is
a promising technique, our empirical study focuses on Android
malware detection. Recall that an Android Package Kit (APK) is a
zipped file that mainly contains: (i) AndroidManifest.xml, which de-
clares various kinds of attributes about the APK (e.g., package name,
required permissions, activities, services, hardware); (ii) classes.dex,
which contains the APK’s functionalities that can be understood by
the Java virtual machines (e.g., Android Runtime); (iii) res folder and
resources.arsc, which contain the resources used by an APK; (iv) lib
folder, which contains the native binaries compatible to different
Central Processing Unit (CPU) architectures (e.g., ARM and x86);
(v) META-INF folder, which contains the signatures of an APK. For
analysis purposes, one can unzip an APK to obtain its files in the
binary format, or disassemble an APK by using an appropriate tool
(e.g., Apktool [42] and Androguard [13]), to obtain human-readable
codes and manifest data (e.g., AndroidManifest.xml).
A malware detector is often modeled as a supervised binary
classifier that labels an example as benign (‘0’) or malicious (‘1’).
Let Z denote the example space (i.e., consisting of benign and
malicious software) and Y = {0, 1} denote the label space. Let
p∗(z, y) = p∗(y|z)p∗(z) denote the underlying joint distribution,
where z ∈ Z and y ∈ Y. The task of malware detection deals with
the conditional distribution p∗(y|z). Specifically, given a software
sample z ∈ Z, we consider Deep Neural Network (DNN) p(y =
1|z, θ), which uses the sigmoid function in its output layer, to model
p∗(y|z), where θ represents the learnable parameters. A detector
denoted by f : Z → Y, which consists of DNNs and is learned
from the training set Dtr ain, returns 1 if p(y = 1|z, θ) ≥ 0.5 and 0
otherwise. We obtain p(y = 0|z) = 1 − p(y = 1|z, θ). Moreover, let
p′(x, y) denote the underlying distribution of test dataset Dtest .
2.2 Problem Statement
There are two kinds of uncertainty associated with machine learn-
ing: epistemic vs. aleatoric [39]. The epistemic uncertainty tells us
about which region of the input space is not perceived by a model
[5]. That is, a data sample in the dense region will get a low epis-
temic uncertainty and get a high epistemic uncertainty in the sparse
region. On the other hand, the aleatoric uncertainty is triggered by
data noises and is not investigated in this paper.
It is widely assumed that p′(z, y) = p∗(z, y) in malware detec-
tion and in the broader context of machine learning. However, this
assumption does not hold in the presence of dataset shift, which re-
flects non-stationary environments (e.g., data distribution changed
over time or the presence of adversarial evasion attacks) [37]. Most
dataset shifts possibly incur changes in terms of epistemic un-
certainty (excluding label flipping or concept of input z changed
thoroughly). We consider three settings that potentially trigger the
p′(z, y) (cid:44) p∗(z, y):
• Out of source: The Dtest and Dtr ain are drawn from different
sources [12].
• Temporal covariate shift: The test data or p′(z) evolves over
time [35].
• Adversarial evasion attack: The test data is manipulated ad-
versarially; i.e., (z′, y = 1) ∼ p′(z, y = 1) is perturbed from
(z, y = 1) ∼ p∗(z, y = 1), where z′ and z have the same
functionality and ‘∼’ denotes “is sampled from” [27].
One approach to coping with these kinds of dataset shifts is to make
a malware detector additionally quantify the uncertainty associated
with a prediction so that a decision-maker decides whether to use
the prediction [25, 39, 44]. This means that a detector p(y|z, θ)
should be able to model p∗(y|z) well while predicting examples z
of (z, y) ∼ p′(z, y) with high uncertainties when (z, y) ≁ p∗(z, y),
where ≁ denotes “does not obey the distribution of”. Specifically, the
597Can We Leverage Predictive Uncertainty to Detect Dataset Shift and Adversarial Examples in Android Malware Detection?ACSAC ’21, December 6–10, 2021, Virtual Event, USA
quality of prediction uncertainty can be assessed by their confidence
scores. This can be achieved by leveraging the notion of calibration
[25]. A malware detector is said to be well-calibrated if the detector
returns a same confidence score q ∈ [0, 1] for a set of test examples
Zq ⊆ Z, in which the malware examples are distributed as q
[43]. Formally, let Pr(y = 1|Zq) denote the proportion of malware
examples in the set Zq that are indeed malicious. We have
Definition 1 (Well-calibrated malware detector, adapted
from [43]). A probabilistic malware detector p(·, θ) : Z → [0, 1] is
well-calibrated if for each confidence q ∈ [0, 1] and Zq = {z : p(y =
1|z, θ) = q,∀z ∈ Z}, it holds that Pr(y = 1|Zq) = q.
Note that Definition 1 means that for a well-calibrated detector,
the fraction of malware examples in example set Zq is indeed q. This
means that we can use q as a confidence score for quantifying un-
certainty when assessing the trustworthiness of a detector’s predic-
tions. This also implies that detection accuracy and well-calibration
are two different concepts. This is so because an accurate detector is
not necessarily well-calibrated (e.g., when correct predictions with
confidence scores near 0.5). Moreover, a well-calibrated malware
detector is also not necessarily accurate.
It would be ideal if we can rigorously quantify or prove the un-
certainty (or bounds) associated with a malware detector. However,
this turns out to be a big challenge that has yet to be tackled. This
motivates us to conduct an empirical study to characterize the un-
certainty associated with Android malware detectors. Hopefully
the empirical findings will contribute to theoretical breakthrough
in the near future.
Specifically, our empirical study is centered at the question in
title, which is further disintegrated as four Research Questions
(RQs) as follows:
tors in the absence of dataset shift?
• RQ1: What is the predictive uncertainty of malware detec-
• RQ2: What is the predictive uncertainty of malware detec-
• RQ3: What is the predictive uncertainty of malware detec-
• RQ4: What is the predictive uncertainty of malware detec-
tors with respect to out-of-source examples?
tors under temporal covariate shift?
tors under adversarial evasion attacks?
Towards answering these questions, we need to empirically study
the predictive distribution p(y|z, θ) of malware detectors in a num-
ber of scenarios.
3 EMPIRICAL ANALYSIS METHODOLOGY
In order to design a competent methodology, we need to select
malware detector that are accurate in detecting malware, select the
calibration methods that are appropriate for these detectors, and
metrics. This is important because a “blindly” designed method-
ology would suffer from incompatibility issues (e.g., integrating
different feature extractions modularly, or integrating post- and
ad-hoc calibration methods together). The methodology will be de-
signed in a modular way so that it can be extended to accommodate
other models or calibration methods in a plug-and-play fashion.
3.1 Selecting Candidate Detectors
We focus on deep learning based Android malware detectors and
more specifically choose the following four Android malware de-
tectors.
DeepDrebin [17]: It is a Multiple Layer Perceptron (MLP)-based
malware detector learned from the Drebin features [4], which are
extracted from the AndroidManifest.xml file and the classes.dex file
reviewed above. These features are represented by binary vectors,
with each element indicating the presence or absence of a feature.
MultimodalNN [22]: It is a multimodal detector that contains
five headers and an integrated part, all of which are realized by
MLP. The 5 headers respectively learn from 5 kinds of features:
(i) permission-component-environment features extracted from the
manifest file; (ii) strings (e.g., IP address); (iii) system APIs; (iv)
Dalvik opcode; and (v) ARM opcodes from native binaries. These
features are represented by their occurrence frequencies. These 5
headers produce 5 pieces of high-level representations, which are
concatenated to pass through the integrated part for classification.
DeepDroid [30]: It is Convolutional Neural Network (CNN)-based
and uses the TextCNN architecture [23]. The features are Dalvik
opcode sequences of smali codes.
Droidetec [29] is an RNN-based malware detector with the archi-
tecture of Bi-directional Long Short Term Memory (Bi-LSTM) [41].
Droidetec partitions a Function Call Graph (FCG) into sequences
according to the caller-callee relation and then concatenates these
sequences for passing through the Bi-LSTM model.
These detectors leverage several types of deep learning models.
Indeed, we also implement an end-to-end method (R2-D2 [19]) and
find it ineffectiveness due to the over-fitting issue. Therefore, we
eliminate it from our study.
3.2 Selecting Calibration Methods
In order to select calibration methods to calibrate the malware
detectors, the following two criteria can be used: (i) they are known
to be effective and (ii) they are scalable for learning a deep learning
model. In particular, the preceding highlights the importance of
the computational complexity of a calibration method. Based on
a previous study [39], these criteria lead us to select the following
six methods.
Vanilla: It serves as a baseline and means that no effort is made to
calibrate a model p(y = 1|z, θ).
Temperature scaling (Temp scaling) [18]: It is a post-processing
method that learns extra parameters to scale the logits of deep
learning models on the validation set, where logits are the input of
the activation of sigmoid.
Monte Carlo dropout (MC dropout) [15]: It technically adds a
dropout layer [40] before the input of every layer contained in a
model. This dropout operation is performed in both the training
and test phases different from the normal usage that turns dropout
on in the training phase and off in the test phase. In theory, MC
dropout is an approximate Bayesian inference, which takes as input
an example z and marginalizes the parameters θ out for returning
the predictive probability:
p(y = 1|z, Dtr ain) =
p(y = 1|z, θ)p(θ|Dtr ain)dθ
(1)
∫
598ACSAC ’21, December 6–10, 2021, Virtual Event, USA
D. Li, T. Qiu, S. Chen, Q. Li, and S. Xu
Due to the intricate neural networks, an analytical solution to ob-
taining p(θ|Dtr ain) is absent. One alternative is to approximate
p(θ|Dtr ain) via a known functional form distribution q(ω) with
variables ω, leading to p(y|z, Dtr ain) ≈ Eq(θ |ω)p(y|z, θ). Specifi-
cally, let θ = {Wi}l
i =1 be the set of l parameters of a neural network,
MC dropout defines q(ω) as:
(2)
where Mi is learnable variables, entities of Vi are sampled from
a Bernoulli distribution with the probability ri (i.e., dropout rate),
and ω = {Mi , ri}l
i =1. In the training phase, variational learning is
leveraged to look for {Mi}l
i =1 [6, 16], which minimizes the Kullback-