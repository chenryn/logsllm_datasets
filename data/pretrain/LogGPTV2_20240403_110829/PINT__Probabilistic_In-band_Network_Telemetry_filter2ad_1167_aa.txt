title:PINT: Probabilistic In-band Network Telemetry
author:Ran Ben Basat and
Sivaramakrishnan Ramanathan and
Yuliang Li and
Gianni Antichi and
Minlan Yu and
Michael Mitzenmacher
PINT: Probabilistic In-band Network Telemetry
Yuliang Li
Sivaramakrishnan Ramanathan
Ran Ben Basat
Harvard University
PI:EMAIL
University of Southern California
PI:EMAIL
Harvard University
PI:EMAIL
Gianni Antichi
Queen Mary University of London
PI:EMAIL
Minlan Yu
Harvard University
PI:EMAIL
Michael Mitzenmacher
Harvard University
PI:EMAIL
0
2
0
2
l
u
J
7
]
I
N
.
s
c
[
1
v
1
3
7
3
0
.
7
0
0
2
:
v
i
X
r
a
ABSTRACT
Commodity network devices support adding in-band telemetry mea-
surements into data packets, enabling a wide range of applications,
including network troubleshooting, congestion control, and path trac-
ing. However, including such information on packets adds significant
overhead that impacts both flow completion times and application-
level performance.
We introduce PINT, an in-band telemetry framework that bounds
the amount of information added to each packet. PINT encodes the
requested data on multiple packets, allowing per-packet overhead
limits that can be as low as one bit. We analyze PINT and prove per-
formance bounds, including cases when multiple queries are running
simultaneously. PINT is implemented in P4 and can be deployed on
network devices.Using real topologies and traffic characteristics, we
show that PINT concurrently enables applications such as congestion
control, path tracing, and computing tail latencies, using only sixteen
bits per packet, with performance comparable to the state of the art.
INTRODUCTION
1
Network telemetry is the basis for a variety of network management
applications such as network health monitoring [72], debugging [28],
fault localization [6], resource accounting and planning [56], attack
detection [27, 65], congestion control [46], load balancing [2, 41,
42], fast reroute [47], and path tracing [36]. A significant recent
advance is provided by the In-band Network Telemetry (INT) [75].
INT allows switches to add information to each packet, such as
switch ID, link utilization, or queue status, as it passes by. Such
telemetry information is then collected at the network egress point
upon the reception of the packet.
INT is readily available in programmable switches and network in-
terface cards (NICs) [8, 14, 58, 85], enabling an unprecedented level
of visibility into the data plane behavior and making this technology
attractive for real-world deployments [16, 46]. A key drawback of
INT is the overhead on packets. Since each switch adds information
to the packet, the packet byte overhead grows linearly with the path
length. Moreover, the more telemetry data needed per-switch, the
higher the overhead is: on a generic data center topology with 5 hops,
requesting two values per switch requires 48 Bytes of overhead, or
4.8% of a 1000 bytes packet (§2). When more bits used to store
telemetry data, fewer bits can be used to carry the packet payload
and stay within the maximum transmission unit (MTU). As a result,
applications may have to split a message, e.g., an RPC call, onto
multiple packets, making it harder to support the run-to-completion
model that high-performance transport and NICs need [7]. Indeed,
the overhead of INT can impact application performance, potentially
1
leading in some cases to a 25% increase and 20% degradation of
flow completion time and goodput, respectively (§2). Furthermore, it
increases processing latency at switches and might impose additional
challenges for collecting and processing the data (§2).
We would like the benefits of in-band network telemetry, but
at smaller overhead cost; in particular, we wish to minimize the
per-packet bit overhead. We design Probabilistic In-band Network
Telemetry (PINT), a probabilistic variation of INT, that provides
similar visibility as INT while bounding the per-packet overhead
according to limits set by the user. PINT allows the overhead budget
to be as low as one bit, and leverages approximation techniques
to meet it. We argue that often an approximation of the telemetry
data suffices for the consuming application. For example, telemetry-
based congestion control schemes like HPCC [46] can be tuned to
work with approximate telemetry, as we demonstrate in this paper.
In some use cases, a single bit per packet suffices.
With PINT, a query is associated with a maximum overhead al-
lowed on each packet. The requested information is probabilistically
encoded onto several different packets so that a collection of a flow’s
packets provides the relevant data. In a nutshell, while with INT
a query triggers every switch along the path to embed their own
information, PINT spreads out the information over multiple pack-
ets to minimize the per-packet overhead. The insight behind this
approach is that, for most applications, it is not required to know
all of the per-packet-per-hop information that INT collects. existing
techniques incur high overheads due to requiring perfect telemetry
information. For applications where some imperfection would be
sufficient, these techniques may incur unnecessary overheads. PINT
Is designed for precisely such applications For example, it is pos-
sible to check a flow’s path conformance [30, 56, 72], by inferring
its path from a collection of its packets. Alternatively, congestion
control or load balancing algorithms that rely on latency measure-
ments gathered by INT, e.g., HPCC [46], Clove [41] can work if
packets convey information about the path’s bottleneck, and do not
require information about all hops.
We present the PINT framework (§3) and show that it can run
several concurrent queries while bounding the per-packet bit over-
head. To that end, PINT uses each packet for a query subset with
cumulative overhead within the user-specified budget. We introduce
the techniques we used to build this solution (§4) alongside its im-
plementation on commercial programmable switches supporting
P4 (§5). Finally, we evaluate (§6) our approach with three differ-
ent use cases. The first traces a flow’s path, the second uses data
plane telemetry for congestion control, and the third estimates the
experienced median/tail latency. Using real topologies and traffic
characteristics, we show that PINT enables all of them concurrently,
Metadata value
Description
Switch ID
Ingress Port ID
Ingress Timestamp
Egress Port ID
Hop Latency
Egress Port TX utilization Current utilization of output port
Queue Occupancy
Queue Congestion Status
ID associated with the switch
Packet input port
Time when packet is received
Packet output port
Time spent within the device
The observed queue build up
Percentage of queue being used
Table 1: Example metadata values.
with only sixteen bits per packet and while providing comparable
performance to the state of the art.
In summary, the main contributions of this paper are:
• We present PINT, a novel in-band network telemetry approach
that provides fine-grained visibility while bounding the per-packet
bit overhead to a user-defined value.
• We analyze PINT and rigorously prove performance bounds.
• We evaluate PINT in on path tracing, congestion control, and
• We open source our code [1].
latency estimation, over multiple network topologies.
INT AND ITS PACKET OVERHEAD
2
INT is a framework designed to allow the collection and reporting
of network data plane status at switches, without requiring any con-
trol plane intervention. In its architectural model, designated INT
traffic sources, (e.g., the end-host networking stack, hypervisors,
NICs, or ingress switches), add an INT metadata header to pack-
ets. The header encodes telemetry instructions that are followed
by network devices on the packet’s path. These instructions tell
an INT-capable device what information to add to packets as they
transit the network. Table 1 summarizes the supported metadata
values. Finally, INT traffic sinks, e.g., egress switches or receiver
hosts, retrieve the collected results before delivering the original
packet to the application. The INT architectural model is intention-
ally generic, and hence can enable a number of high level appli-
cations, such as (1) Network troubleshooting and verification, i.e.,
microburst detection [36], packet history [30], path tracing [36],
path latency computation [34]; (2) Rate-based congestion control,
i.e., RCP [22], XCP [40], TIMELY [53]; (3) Advanced routing, i.e,
utilization-aware load balancing [2, 42].
INT imposes a non insignificant overhead on packets though. The
metadata header is defined as an 8B vector specifying the telemetry
requests. Each value is encoded with a 4B number, as defined by
the protocol [75]. As INT encodes per-hop information, the overall
overhead grows linearly with both the number of metadata val-
ues and the number of hops. For a generic data center topology
with 5 hops, the minimum space required on packet would be 28
bytes (only one metadata value per INT device), which is 2.8%
of a 1000 byte packet (e.g., RDMA has a 1000B MTU). Some
applications, such as Alibaba’s High Precision Congestion Con-
trol [46] (HPCC), require three different INT telemetry values for
each hop. Specifically, for HPCC, INT collects timestamp, egress
port tx utilization, and queue occupancy, alongside some additional
data that is not defined by the INT protocol. This would account
Figure 2: Normalized average
goodput of long flows (>10MB)
varying the network load and in-
creasing per-packet overhead.
Figure 1: Normalized average
Flow Completion Time varying
the network load and increasing
the per-packet overhead.
for around 6.8% overhead using a standard INT on a 5-hop path.1
This overhead poses several problems:
1. High packet overheads degrade application performance. The
significant per-packet overheads from INT affect both flow comple-
tion time and application-level throughput, i.e., goodput. We ran an
NS3 [76] experiment to demonstrate this. We created a 5-hop fat-tree
data center topology with 64 hosts connected through 10Gbps links.
Each host generates traffic to randomly chosen destinations with a
flow size distribution that follows a web search workload [3]. We
employed the standard ECMP routing with TCP Reno. We ran our
experiments with a range of packet overheads from 28B to 108B.
The selected overheads correspond to a 5-hop topology, with one
to five different INT values collected at each hop. Figure 1 shows
the effect of increasing overheads on the average flow completion
time (FCT) for 30% (average) and 70% (high) network utilization.
Figure 2, instead, focuses on the goodput for only the long flows,
i.e., with flow size >10 MBytes. Both graphs are normalized to the
case where no overhead is introduced on packets.
In the presence of 48 bytes overhead, which corresponds to 3.2%
of a 1500B packet (e.g., Ethernet has a 1500B MTU), the average
FCT increases by 10%, while the goodput for long flows degrades
by 10% if network utilization is approximately 70%. Further in-
creasing the overhead to 108B (7.2% of a 1500B packet) leads to
a 25% increase and 20% degradation of flow completion time and
goodput, respectively. This means that even a small amount of band-
width headroom can provide a dramatic reduction in latency [4].
The long flows’ average goodput is approximately proportional to
the residual capacity of the network. That means, at a high network
utilization, the residual capacity is low, so the extra bytes in the
header cause larger goodput degradation than the byte overhead
itself [4]. As in our example, the theoretical goodput degradation
should be around 1 − 100%−70%∗1.072
100%−70%∗1.032 ≈ 10.1% when increasing the
header overhead from 48B to 108B at around 70% network utiliza-
tion. This closely matches the experiment result, and is much larger
than the extra byte overhead (4%).
Although some data center networks employ jumbo frames to
mitigate the problem2, it is worth noting that (1) not every network
can employ jumbo frames, especially the large number of enterprise
and ISP networks; (2) some protocols might not entirely support
1HPCC reports a slightly lower (4.2%) overhead because they use customized INT.
For example, they do not use the INT header as the telemetry instructions do
not change over time.
2https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/network_mtu.html
2
 1 1.1 1.2 1.328486888108Normalized FCTOverhead (Bytes)30%70% 0 0.2 0.4 0.6 0.8 128486888108Normalized GoodputOverhead (Bytes)30%70%Description
Congestion Control with in-network support
Diagnosis of short-lived congestion events
Determine network state, i.e., queues status
Determine under-utilized network elements