# Two Days in the Life of the DNS Anycast Root Servers

**Authors:**
- Ziqian Liu
- Bradley Huffaker
- Marina Fomenkov
- Nevil Brownlee
- Kimberly C. Claffy

**Affiliations:**
- Ziqian Liu: CAIDA, University of California at San Diego and Beijing Jiaotong University
- Bradley Huffaker, Marina Fomenkov, and Kimberly C. Claffy: CAIDA, University of California at San Diego
- Nevil Brownlee: CAIDA and The University of Auckland

**Contact:**
- {ziqian, bhuffake, marina, nevil, kc}@caida.org

## Abstract
The Domain Name System (DNS) root nameservers frequently employ anycast to enhance service quality and resilience against various failures. This study examines DNS traffic collected over a two-day period in January 2006 from anycast instances of the C, F, and K root nameservers. We analyze how anycast DNS service impacts the global population of Internet users. To determine if clients are using the geographically closest instance, we examine client locations and the geographic distances between servers and clients. Our findings indicate that BGP routing often does not select the geographically closest server. We also investigate specific AS paths and cases where local instances have a higher than usual proportion of non-local clients. Overall, we conclude that anycast roots significantly localize DNS traffic, thereby improving DNS service for clients worldwide.

**Keywords:** DNS, anycast, Root Servers, BGP

## 1. Background
The Domain Name System (DNS) is a critical component of the Internet, providing mappings between domain names and corresponding IP addresses. The DNS data is stored in a distributed, tree-structured database, with each nameserver authoritative for a part of the naming tree. DNS root nameservers play a crucial role by providing authoritative referrals to nameservers for generic top-level domains (gTLDs, e.g., .com, .org) and country-code top-level domains (ccTLDs, e.g., .us, .cn).

Initially, the DNS was designed with only 13 root nameservers, which provided the bootstrap foundation for the entire system. As the Internet grew globally, this limitation became a challenge, and anycast was introduced as a solution. Anycast allows a single IP address to be announced from multiple physical locations, enabling the network to deliver requests to the closest available server. For a DNS root nameserver, anycast provides a service where clients send requests to a single address, and the network delivers the request to the nearest server in the anycast group.

An anycast group consists of instances run by the same organization, using the same IP address (service address), but located in different physical nodes. Each instance announces reachability for the same prefix/length (service supernet) via the Border Gateway Protocol (BGP). Instances can use either global or local routing policies. Local instances limit their catchment area by announcing the service supernet with a no-export attribute, while global instances do not restrict their scope, relying on BGP to determine their reach.

As of now, anycasting has been deployed for six of the 13 DNS root nameservers (C, F, I, J, K, and M). The primary goal of using anycast is to increase geographic diversity and isolate regions from failures. Additionally, it often reduces latency for local populations and makes it easier to increase DNS system capacity, helping protect against simple DoS attacks. BGP optimizes for ISP costs and AS path length, with latency and stability improvements as secondary effects.

In this study, we examine traffic at the anycast instances of the C, F, and K root nameservers and their client populations. We use geographic proximity as a proxy for latency, as latency between metropolitan areas is primarily determined by propagation delay.

## 2. Data
Measurements at the DNS root nameservers were conducted by the Internet Systems Consortium (ISC) and the DNS Operations and Analysis Research Center (OARC) in collaboration with CAIDA. The OARC DNS anycast dataset contains full-record tcpdump traces collected at the C, E, F, and K-root instances in September 2005 and January 2006. The traces mainly captured inbound traffic to each root instance, with some instances also collecting outbound traffic.

For this study, we selected the most complete dataset, the "OARC Root DNS Trace Collection January 2006." It includes concurrent traces from all four C-root instances, 33 of the 37 F-root instances, and 16 of the 17 K-root instances during the period from January 10 to January 11, 2006, UTC. The common maximum interval for all measured instances is 47.2 hours, or nearly two days.

Each of the three root nameservers we measured implements a different deployment strategy. All C-root nodes are routed globally, making its topology flat. The F-root topology is hierarchical, with two global nodes and many more widely distributed local nodes. K-root has a hybrid topology with five global and 12 local nodes, all geographically distributed. The instance locations for all roots are listed in [5].

Our target data are IPv4 UDP DNS requests to each root server's anycast IP address. Some F and K-root instances have applicable IPv6 service addresses, and we observed a few requests to these addresses. Further analysis of IPv6 DNS traffic is needed, but in this paper, we focus on IPv4 traffic. We also note that for the F and K-root instances that collected TCP traffic associated with port 53, the volume was negligible, accounting for approximately 1.3% of total bytes and 3.2% of total packets.

## 3. Traffic Differences Between Root Server Instances

### 3.1 Diurnal Pattern
Assuming that DNS traffic is primarily generated by humans rather than machines, we expect to see a clear diurnal pattern for instances that primarily attract a client base from a small geographic area. Figure 1 shows the time distribution of DNS requests to three F-root local instances: mad1, mty1, and lax1. Both mad1 and mty1 exhibit a clear diurnal pattern matching the local time, with traffic rising in the morning and falling towards midnight. However, lax1 has a distinct traffic pattern, with the peak of the request curve shifted from its local midday by approximately 8 hours. This difference suggests that a large proportion of lax1's requests come from clients who do not follow the local time of the instance, likely because they are located elsewhere. Indeed, as shown in Section 4.1, although lax1 is located in the US, approximately 90% of its clients are in Asia, generating over 70% of the total requests received by this instance.

We also studied the request time distribution of one of the global instances (not shown) and found that its curve was flatter than those of local instances. However, slight diurnal variations were still noticeable and correlated with the local time of the continent from which that global instance had the largest proportion of its clients.

### 3.2 Traffic Load
We characterized the traffic load of root server instances using two metrics: the number of requests per second averaged over our measurement interval and the total number of clients served during this interval (Figure 2). Global instances generally have higher request rates and serve larger populations than local instances, but there is significant variability in their loads. Some local instances also have high traffic loads and large client populations comparable to those of global instances. High loads may occur due to (1) a high density of Internet users in the local instance's catchment area, or (2) a topologically larger catchment area. For example, the F-root local instance ams1 peers with AMS-IX, a major Internet exchange point in Amsterdam, attracting a higher request rate and a larger number of clients than typical for a local instance. Conversely, some local instances have extremely low load levels (less than 10 pkt/s on average over two days), serving only a handful of clients, and are clearly underutilized.

## 4. Client Distribution

Figure 3 shows the client continental distribution of instances. Each bar represents one instance, arranged from left to right according to the instance longitude, in west-to-east order. Groups delimited by white gaps represent instances located in the same continent. The anycast group (root) and the city names of the instances at continent boundaries are given above the bars. Within each bar, the colored segments show the distribution of clients by continent. Global instances are marked below the bars, with the first row for F-root and the second row for K-root.