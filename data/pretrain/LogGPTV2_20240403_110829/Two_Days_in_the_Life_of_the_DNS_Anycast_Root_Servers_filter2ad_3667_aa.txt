title:Two Days in the Life of the DNS Anycast Root Servers
author:Ziqian Liu and
Bradley Huffaker and
Marina Fomenkov and
Nevil Brownlee and
Kimberly C. Claffy
Two Days in the Life of
the DNS Anycast Root Servers
Ziqian Liu2, Bradley Huﬀaker1, Marina Fomenkov1,
Nevil Brownlee3, and kc claﬀy1
1 CAIDA, University of California at San Diego
2 CAIDA and Beijing Jiaotong University
3 CAIDA and The University of Auckland
{ziqian,bhuffake,marina,nevil,kc}@caida.org
Abstract. The DNS root nameservers routinely use anycast in order
to improve their service to clients and increase their resilience against
various types of failures. We study DNS traﬃc collected over a two-day
period in January 2006 at anycast instances for the C, F and K root
nameservers. We analyze how anycast DNS service aﬀects the worldwide
population of Internet users. To determine whether clients actually use
the instance closest to them, we examine client locations for each root
instance, and the geographic distances between a server and its clients.
We ﬁnd that frequently the choice, which is entirely determined by BGP
routing, is not the geographically closest one. We also consider speciﬁc
AS paths and investigate some cases where local instances have a higher
than usual proportion of non-local clients. We conclude that overall,
anycast roots signiﬁcantly localize DNS traﬃc, thereby improving DNS
service to clients worldwide.
Keywords: DNS, anycast, Root Servers, BGP.
1 Background
The Domain Name System (DNS) [1] is a fundamental component of today’s
Internet: it provides mappings between domain names used by people and the
corresponding IP addresses required by network software. The data for this map-
ping is stored in a tree-structured distributed database where each nameserver
is authoritative for a part of the naming tree. The DNS root nameservers play
a vital role in the DNS as they provide authoritative referrals to nameservers
for generic top-level domains (gTLD, e.g. .com, .org) and country-code top-level
domains (ccTLD, e.g. .us, .cn).
When the DNS was originally designed, its global scope was not foreseen, and
as a consequence of design choices had only 13 root nameservers (“roots”) that
would provide the bootstrap foundation for the entire DNS system. As the Inter-
net grew beyond its birthplace in the US academic community to span the world
it increasingly put pressure on this limitation, at the same time also increasing
the deployment cost of any transition to a new system. Thus, anycast [2] was
S. Uhlig, K. Papagiannaki, and O. Bonaventure (Eds.): PAM 2007, LNCS 4427, pp. 125–134, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007
126
Z. Liu et al.
presented as a solution since it would allow the system to grow beyond the static
13 instances, while avoiding a change to the existing protocol. For a DNS root
nameserver, anycast provides a service whereby clients send requests to a single
address and the network delivers that request to at least one, preferably the
closest, server in the root nameserver’s anycast group [3].
We deﬁne an anycast group as a set of instances that are run by the same
organisation and use the same IP address, namely the service address, but are
physically diﬀerent nodes. Each instance announces (via the routing system)
reachability for the same preﬁx/length – the so-called service supernet – that
covers the service address and has the same origin Autonomous System (AS).
The service supernet is announced from diﬀerent instances by Border Gateway
Protocol (BGP) such that there may be multiple competing AS paths. Instances
may employ either global or local routing policy. Local instances attempt to limit
their catchment area to their immediate peers only by announcing the service
supernet with no-export attribute. Global instances make no such restriction,
allowing BGP alone to determine their global scope, but use prepending in their
AS path to decrease the likelihood of their selection over a local instance [4].
As of today, anycasting has been deployed for 6 of the 13 DNS root name-
servers, namely, for the C, F, I, J, K and M roots [5]. The primary goal of using
anycast was to increase the geographic diversity of the roots and isolate each
region from failures in other regions; as a beneﬁcial side eﬀect, local populations
often experience lower latency after an anycast instance is installed. As well,
anycast makes it easier to increase DNS system capacity, helping protect nam-
servers against simple DOS attacks. The expected performance gains depend on
BGP making the best tradeoﬀ between latency, path length and stability, and
Internet Service Provider (ISP) cost models. BGP optimizes ﬁrst ISP costs and
then Autonomous System (AS) path length, attaining any gains in latency and
stability as secondary eﬀects from this optimization.
In this study we examine traﬃc at the anycast instances of the C, F, and
K root nameservers and their client population. We substitute the geographic
proximity as a proxy for latency, since latency between metropolitan areas is
dominated by propagation delay [6].
2 Data
Measurements at the DNS root nameservers were conducted by the Internet Sys-
tems Consortium (ISC) and the DNS Operations and Analysis Research Center
(OARC) [7] in the course of their collaboration with CAIDA. DNS-OARC pro-
vides a platform for network operators and researchers to share information and
cooperate, with focus on the global DNS.
The full OARC DNS anycast dataset contains full-record tcpdump traces col-
lected at the C, E, F, and K-root instances in September 2005 and January 2006.
The traces mostly captured inbound traﬃc to each root instance, while a few
instances also collected outbound traﬃc. For this study we selected the most
complete dataset available, the “OARC Root DNS Trace Collection January
Two Days in the Life of the DNS Anycast Root Servers
127
c
e
s
/
s
t
s
e
u
q
e
R
#
 600
 500
 400
 300
 200
 100
 0
00:00
01/10
Madrid (noon)
Monterrey (noon)
LA (noon)
mad1
mty1
lax1
04:00
01/10
08:00
01/10
12:00
01/10
16:00
01/10
20:00
01/10
00:00
01/11
04:00
01/11
08:00
01/11
12:00
01/11
16:00
01/11
20:00
01/11
00:00
01/12
04:00
01/12
UTC time
Fig. 1. Diurnal patterns of the DNS traﬃc to the F-root local instances mad1
(Madrid, Spain), mty1 (Monterrey, Mexico), and lax1 (Los Angeles, US). For each
instance, the local time noon is explicitly speciﬁed with a solid vertical line. The artifact
on Jan. 10th between 4:00 and 5:00 appears because no data available for this period.
2006” [8]. It includes traces collected concurrently at all 4 C-root instances, 33
of the 37 F-root instances and 16 of the 17 K-root instances during the period
from Tue Jan 10 to Wed Jan 11 2006, UTC. A common maximum interval for
all measured instances is 47.2 hours or nearly two whole days.
Each of the three root nameservers we measured implements a diﬀerent de-
ployment strategy [9]. All nodes of C-root are routed globally, making its topol-
ogy ﬂat. The F-root topology is hierarchical: two global nodes are geographically
close, with many more widely distributed local nodes. Finally, K-root represents
a case of hybrid topology with ﬁve global and 12 local nodes, all geographically
distributed. The instance locations for all roots are listed in [5].
Our target data are IPv4 UDP DNS requests to each root server’s anycast
IP address. Some of the F and K-root instances have applicable IPv6 service
addresses, and we observed a few requests destined to these addresses. Further
analysis of the IPv6 DNS traﬃc is needed, but in this paper we focus on IPv4
traﬃc. We also note that for the F and K-root instances that collected TCP
traﬃc associated with port 53, its volume was negligible, namely, ∼1.3% of total
bytes and ∼3.2% of total packets.
3 Traﬃc Diﬀerences Between Root Server Instances
3.1 Diurnal Pattern
Assuming that DNS traﬃc is primarily generated by humans, rather than by
machines, we expect to see a clear diurnal pattern for those instances that pri-
marily attract a client base from a small geographic area. Fig. 1 shows the
time distribution of DNS requests to three F-root local instances: mad1, mty1
and lax1. Both mad1 and mty1 have a clear diurnal pattern matching the local
time, i.e. rising in the morning and falling towards midnight. However, lax1 has
a distinct traﬃc pattern, where the crest of the request curve is shifted from
its local midday by ∼8 hours. This diﬀerence suggests that a large proportion
of lax1’s requests are coming from clients who do not follow the local time
128
Z. Liu et al.
2500
2000
1500
C−root
F−root
K−root
iad1* 
ord1* 
lax1* 
c
e
s
/
s
t
s
e
u
q
e
R
#
1000
jfk1* 
500
sfo2* 
muc1 
ams1 gru1 
lga1 
pao1* 
linx* 
x 105
10
iad1* 
ams−ix* 
nap* 
denic 
tokyo* 
delhi* 
8
6
4
2
s
e
s
s
e
r
d
d
a
P
I
#
0
0
lax1* 
ord1* 
jfk1* 
pao1* 
C−root
F−root
K−root
linx*
ams−ix*
ams1 
lga1 
sfo2* 
muc1 
gru1 
nap* 
denic
tokyo*
delhi* 
0
0
10
20
30
40
50
60
Instance
(a) Average request rate
10
20
30
Instance
40
50
60
(b) Number of clients
Fig. 2. Average instance requests per second and the total number of clients.
The x-axis instance order is the same in both (a) and (b). The instances are plotted
in groups for C, F and K roots; within each group they are arranged in an increasing
request rate order. Symbol ∗ designates global instances.
of the instance, most likely, because they are located elsewhere. Indeed, as we
show in Section 4.1, although lax1 is located in the US, ∼90% of its clients are
in Asia and they generated over 70% of the total requests that this instance
received.
We also studied the request time distribution of one of the global instances
(not shown) and found that its curve was ﬂatter than those of local instances.
However, slight diurnal variations were still noticeable and correlated with the
local time of the continent from which that global instance has the largest pro-
portion of its clients.
3.2 Traﬃc Load
We characterised the traﬃc load of root server instances with two metrics: num-
ber of requests per second averaged over our measurement interval and total
number of clients served during this interval (Fig. 2). Global instances generally
have higher request rates and serve larger populations than local instances, but
there is large variability in their loads. Some local instances also have fairly high
traﬃc loads and large client populations comparable to those of the global in-
stances. Such high loads may occur because (1) the local instance’s catchment
area has a high density of Internet users that generate many requests, or (2)
its catchment area is topologically larger than normal. For example, the F-root
local instance ams1 is peering with AMS-IX, an Internet exchange point in Am-
sterdam, NL, which is one of Europe’s major exchange points. Therefore, ams1
peers with a large number of ASes via AMS-IX and attracts a higher request
rate and larger number of clients than is typical for a local instance. At the same
time, some local instances have extremely low load levels (less than 10 pkt/s on
average over two days period), serve only a handful of clients, and are clearly
underutilised.
Two Days in the Life of the DNS Anycast Root Servers
129
F: San Francisco (US)  
F: San Francisco (US)  
F: New York (US)  
F: New York (US)  
F: Santiago (CL)  
F: Santiago (CL)  
F: Auckland (NZ)  
F: Auckland (NZ)  
F: Brisbane (AU)  
F: Brisbane (AU)  
F: Tel Aviv (IL)   K: Tokyo (JP)  
F: Tel Aviv (IL)   K: Tokyo (JP)  
F: Sao Paulo (BR)  
F: Sao Paulo (BR)  
F:Johannesburg(SA)  
F:Johannesburg(SA)  
K: Reykjavik (IS)  K: Helsinki (FI)  
K: Reykjavik (IS)  K: Helsinki (FI)  
100
100
75
75
%
%
50
50
25
25
0
0
sfo2* pao1* 
sfo2* pao1* 
N.Amer S.Amer
N.Amer S.Amer
lax1* ord1* 
lax1* ord1* 
nap*
nap*
iad1* 
iad1* 
linx*
linx*
jfk1* 
jfk1* 
Oceania
Oceania
Asia
Asia
Africa
Africa
Europe
Europe
S. Amer
S. Amer
N. Amer
N. Amer
Europe
Europe
ams−ix* 
ams−ix* 
Africa
Africa
Asia
Asia
delhi*
delhi*
Oceania
Oceania
tokyo*
tokyo*
Fig. 3. Client continental distribution of instances. Each bar represents one in-
stance, and the bars are arranged from left to right according to the instance longitude,
in the west to east order. Groups delimited by white gaps represent instances located
in the same continent. The anycast group (root) and the city names of the instances
that are located at continent boundaries are given above the bars. Within each bar,
the colored segments show the distribution of clients by continent. Global instances are
marked below the bars, where the ﬁrst row is for F-root, the second row is for K-root,