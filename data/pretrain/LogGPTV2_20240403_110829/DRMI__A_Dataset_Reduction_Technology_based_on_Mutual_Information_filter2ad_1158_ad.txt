90.29%
90.08%
88.58%
100
88.09%
87.97%
87.77%
86.38%
86.13%
84.95%
60
83.27%
82.86%
82.65%
82.09%
81.80%
80.42%
ALL.
99.13%
98.27%
97.56%
96.33%
95.36%
92.84%
AD Size
-
7,291
5,000
2,000
1,000
600
600
-
93.65%
93.36%
92.50%
91.81%
-
300
-
92.15%
91.88%
91.20%
90.89%
90.23%
150
-
89.94%
89.57%
89.08%
88.67%
87.88%
100
-
86.69%
86.24%
85.41%
85.11%
84.30%
60
-
81.73%
81.47%
80.42%
80.09%
79.16%
ALL.
-
95.56%
94.69%
93.17%
92.26%
90.83%
Table 7: Evaluations of C3F2 model and DNN5 model on
MNIST. “Test Accuracy” means the substitute model accu-
racy on the test dataset. The original C3F2 model trained on
the full dataset (60,000 data) reaches 99.28% accuracy. The
original DNN5 model reaches 98.03% accuracy. The number
of “Queries” is also the size of simpliﬁed set.
Model
Method
C3F2
DNN5
min-sum
min-max
baseline
min-sum
baseline
α
1
2
4
1
2
4
-
1
2
4
-
Queries = 600
95.10%
96.54%
97.25%
96.05%
96.40%
97.34%
92.40%
87.87%
86.78%
90.11%
82.99%
Test Accuracy
Queries = 300
91.21%
94.03%
94.57%
92.43%
94.57%
94.41%
90.65%
80.80%
82.79%
83.79%
73.87%
Queries = 150
89.98%
86.59%
90.49%
88.89%
89.02%
91.12%
85.18%
68.06%
71.13%
74.77%
64.12%
73.8% top-5 accuracy and 52.7% top-1 accuracy. When the
attacker only uses 10,000 data, we can get a substitute model
with 77.0% top-5 accuracy. Results show that DRMI also
works on the ImageNet dataset.
Remark: Our DRMI also performs well on CIFAR10 and
more complex datasets like ImageNet. DRMI shows superior
performance on different datasets.
5.2.2 Different Reduction Degrees
In order to assess the relationship between reduced samples
and corresponding accuracies, we conducted an experiment
with different k for Equation 4. More speciﬁcally, we sample
training data of varying sizes (e.g., k=60, 600, or 6,000).The
accuracies are measured for each training. Figure 3 shows the
curve of the accuracy rates of substitute models with different
dataset sizes. In PRADA, we only found results below 500
samples. Compared to other methods, our curve has high
accuracy when the size is very small. It has reached 82% at
50 samples, 92% at 150, and 97% at 600. In PRADA [29],
the accuracy of 50 samples is only 75%, 82.5% at 100, and
90% at 200. In the baseline method, the accuracy is lower
than 70% at 50 samples, even 300 samples can only achieve
Figure 3: The curve of model accuracy under different dataset
sizes on MNIST.
an accuracy of 89%. This shows that DRMI can achieve high
performance with small queries. The gap between DRMI and
baseline is about 5% at 600 samples, 7.5% at 150, and more
than 10% at 50. The gap between DRMI and PRADA is about
3.3% at 200 samples, 4.8% at 100, and 7% at 50. When the
dataset size exceeds 2,000, the gap becomes smaller and is
ﬁlled when the size is larger than 20,000.
Remark: Our DRMI method can obtain a high accuracy
with a small-sized dataset. When the dataset size is greater
than 50, the smaller the dataset size, the greater the gap be-
tween other methods and ours.
As claimed in “Threat Model” at Section 3, DRMI can still
performs effectively when attackers can only access some
data (may not in the training set) that has the same distribu-
tion with the training data. It is evaluated and presented as
shown in Table 6. In this experiment, the dataset is randomly
divided into two parts (except the “60,000” row). One part can
be obtained by attackers, whose size is “AD Size” in Table 6,
and the other is used to train a target model. This guarantees
that attackers can only access the data of the same distribution
with the training dataset, not the exact training data. The row
of “60,000” is the situation when the attacker has all training
samples. From the perspective of each column, the test accu-
USENIX Association
30th USENIX Security Symposium    1909
racy only decreases slightly when the attacker has a smaller
dataset. When the attacker has only one-tenth of previous data
(from 10,000 to 1,000), the substitute model’s accuracy only
decreases 1.07%, 1.25%, 0.77%, 1.84%, 1.06% under 600,
300, 150, 100, 60 queries, respectively. When the attacker can
only get 600 samples, DRMI also obtains 91.06% accuracy
under 300 queries. The accuracy decline from 60,000 “AD
Size” to 600 is between 2.85% and 3.55% under 300, 150,
100, and 60 queries. Results show that DRMI still performs
well even when the attacker only has limited data. DRMI can
select representative data from a small dataset, and the stolen
substitute model still has a high accuracy rate.
We also do similar experiments on ImageNet in Table 5.
The dataset size of the attacker varies from 200,000 to only
10,000. DRMI can achieve 72.5% top-5 accuracy through
5,000 data when the attacker only obtains 10,000 samples.
Remark: DRMI performs well when attackers only have
a very small dataset. DRMI also does not need attackers to
know the exact training data.
In order to explore the performance of DRMI when attack-
ers obtain a different dataset that does not match the distribu-
tion of the training dataset, we choose another handwritten
digits dataset USPS [1] and present the results in Table 6.
Attackers utilize the USPS data to steal the target model
trained on MNIST. Querying 7,291 data gets a 95.56% substi-
tute model, while querying 600 representative samples using
DRMI still reaches a 93.65% model. Compared to MNIST,
using USPS data for attack only decreases 1.47%, 0.52%,
0.94%, 1.53%, 1.18% accuracy under 600, 300, 150, 100, 60
queries when attackers have 5,000 samples. Results show that
using USPS data can still attack the target model, with a bit
of accuracy decrease compared to using MNIST data.
Remark: DRMI still works well when the attacker’s dataset
does not match the distribution of the target training dataset.
5.2.3 Different Models
To evaluate its generality amongst varying models, we test
our approach against C3F2 and DNN5 models on MNIST.
Table 7 shows the results of training substitute model against
a C3F2 model and a DNN5 model, spanning from size 150
to 600. We can see that the accuracies on C3F2 and LeNet-5
(see Table 3) models are all higher than that of DNN5 model,
which is determined by model structure itself. The best C3F2
results are 7.23%, 10.78%, and 16.35% higher than the best
DNN5 results on 600, 300, and 150 size.
In addition, the accuracy under α = 1 still performs the
worst, and there is a gap with cases of α = 2 or 4. This also
demonstrates the need to give high penalties (large α) to
images with high similarity (large MI value) in the reduced
dataset. Comparing the two algorithms, “min-max” and “min-
sum” are still not far behind. On 600 dataset size of C3F2, we
improve the test accuracy up to 97.34%, which is only fewer
than two percentages away from the optimal model.
Compared to the baseline method, our approach on C3F2
has increased by 4.94, 3.92, and 5.94 percentages on 600, 300,
and 150 dataset sizes, respectively. According to the upper
limit of 99.28%, our improvement has reached 72%, 45%,
and 42% on 600, 300, and 150 dataset sizes in the improvable
space. On the DNN5 model, DRMI improves 7.12%, 9.92%,
and 10.65% accuracy than baseline on 600, 300, and 150 size.
Table 5 shows attackers adopt an Inception-v3 and a
ResNet152 network to steal the Inception-v3 target model
on ImageNet. The top-5 and top-1 accuracy are very similar
(< 2%) on the two substitute model structures.
Remark: To sum up, our approach can be applied to a wide
range of model structures (CNNs and DNNs), which proves
the excellent generalizability of our DRMI method. The at-
tacker does not need to know the target model architecture. It
is largely attributed to its model-independent property. As a
result, given a dataset, we can extract a high-quality reduced
dataset, which can be applied to different models.
Jagielski et al. [26] also focuses on extracting high-
accuracy substitute models with fewer queries. Their learning-
based extraction adopts semi-supervised learning techniques.
Here we make a comparison. On ImageNet, DRMI reaches
90.6% top-5 accuracy using about 8.3% data, and their method
achieves 86.2% top-5 accuracy using 10% data. On CIFAR10
for 4,000 queries, DRMI reaches 89.74% accuracy, better
than 86.51% in their fully supervised extraction, indicating
that the quality of our queries is higher than theirs, but worse
than 93.29% accuracy in their MixMatch extraction. This is
mainly because they not only use query data for fully super-
vised learning, but also perform semi-supervised learning on
the remaining unlabeled data in the training set.
5.3 Catalytic Effect for Black-box Attacks
We aim to answer RQ2 by evaluating how our approach fa-
cilitates black-box attacks. The accuracy evaluation proves
that our substitute model is functionally similar to the target
model. Here we evaluate the decision boundary similarity
between them through attack success rate of adversarial ex-
amples (AEs). Adversarial attacks are a major technology
to undermine the security of deep learning models. Training
substitute models has been a method of black-box adversarial
attacks. By querying the target model, attackers can obtain
class probabilities of their inputs. Then they use these data
to train a substitute model, and adopt white-box adversarial
attacks to generate AEs on it. At last, attackers use these AEs
to attack the target model and evaluate the success rate ac-
cording to the transferability of AEs. In this process, training
dataset quality and query numbers are particularly important.
Attackers need to get a high quality dataset and use fewer
queries for the target model.
Here we use our MI technique for black-box adversarial
attacks. We adopt the simpliﬁed dataset produced by the MI
method to query the target model, and train a substitute model
1910    30th USENIX Security Symposium
USENIX Association
Table 8: Transferability of adversarial examples on target
models generated by substitute models. Adversarial examples
are generated by PGD. “Transferability” means success rate
of adversarial examples on target model. “LeNet-5 (1,000)”
means the attacker only has a small dataset with 1,000 data
points. Experiments are under the same environments.
Queries
Target model
Transferability
Accuracy
50
150
200
300
600
LeNet-5
C3F2
LeNet-5 (1,000)
PRADA [29]
Practical [45]
LeNet-5
C3F2
LeNet-5 (1,000)
LeNet-5 (1,000)
PRADA