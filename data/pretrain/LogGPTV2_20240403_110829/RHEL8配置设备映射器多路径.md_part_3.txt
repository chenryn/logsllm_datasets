::: titlepage
# []{#assembly_enabling-multipathing-on-nvme-devices_configuring-device-mapper-multipath.html#assembly_enabling-multipathing-on-nvme-devices_configuring-device-mapper-multipath}第 4 章 在 NVMe 设备中启用多路径 {.title}
:::
您可以多路径通过光纤传输连接到系统的 NVMe
设备，如光纤通道(FC)。您可以在多个多路径解决方案之间进行选择。
::: section
::: titlepage
# []{#assembly_enabling-multipathing-on-nvme-devices_configuring-device-mapper-multipath.html#con_native-nvme-multipathing-and-dm-multipath_assembly_enabling-multipathing-on-nvme-devices}原生 NVMe 多路径和 DM 多路径 {.title}
:::
NVMe 设备支持原生多路径功能。在 NVMe 中配置多路径时，您可以在标准 DM
多路径框架和原生 NVMe 多路径间进行选择。
DM 多路径和原生 NVMe 多路径支持 NVMe
设备的对称命名空间访问(ANA)多路径方案。ANA
识别目标和启动器之间的优化路径，并提高性能。
当启用原生 NVMe 多路径时，它适用于所有 NVMe
设备。它可以提供更高的性能，但不包含 DM 多路径提供的所有功能。例如，原生
NVMe 多路径只支持 `故障切换`{.literal} 和 `循环`{.literal}
路径选择方法。
红帽建议您在 RHEL 8 中使用 DM 多路径作为默认多路径解决方案。
:::
::: section
::: titlepage
# []{#assembly_enabling-multipathing-on-nvme-devices_configuring-device-mapper-multipath.html#proc_enabling-native-nvme-multipathing_assembly_enabling-multipathing-on-nvme-devices}启用原生 NVMe 多路径 {.title}
:::
这个过程使用原生 NVMe 多路径解决方案在连接的 NVMe 设备中启用多路径。
::: itemizedlist
**先决条件**
-   NVMe 设备连接到您的系统。
    有关通过光纤传输连接 NVMe 的更多信息，请参阅 [NVMe over fabric
    设备概述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/overview-of-nvme-over-fabric-devicesmanaging-storage-devices){.link}。
:::
::: orderedlist
**流程**
1.  检查内核中是否启用了原生 NVMe 多路径：
    ``` screen
    # cat /sys/module/nvme_core/parameters/multipath
    ```
    命令显示以下内容之一：
    ::: variablelist
    [`N`{.literal}]{.term}
    :   禁用原生 NVMe 多路径。
    [`Y`{.literal}]{.term}
    :   启用了原生 NVMe 多路径。
    :::
2.  如果禁用了原生 NVMe 多路径，请使用以下方法之一启用它：
    ::: itemizedlist
    -   使用内核选项：
        ::: orderedlist
        1.  在内核命令行中添加 thenvme `_core.multipath=Y`{.literal}
            选项：
            ``` screen
            # grubby --update-kernel=ALL --args="nvme_core.multipath=Y"
            ```
        2.  在 64 位 IBM Z 构架中，更新引导菜单：
            ``` screen
            # zipl
            ```
        3.  重启系统。
        :::
    -   使用内核模块配置文件：
        ::: orderedlist
        1.  使用以下内容创建 `/etc/modprobe.d/nvme_core.conf`{.literal}
            配置文件：
            ``` screen
            options nvme_core multipath=Y
            ```
        2.  备份 `initramfs`{.literal} 文件系统：
            ``` screen
            # cp /boot/initramfs-$(uname -r).img \
                 /boot/initramfs-$(uname -r).bak.$(date +%m-%d-%H%M%S).img
            ```
        3.  重建 `initramfs`{.literal} 文件系统：
            ``` screen
            # dracut --force --verbose
            ```
        4.  重启系统。
        :::
    :::
3.  可选：在运行的系统中，更改 NVMe 设备的 I/O
    策略，以便在所有可用路径中分发 I/O：
    ``` screen
    # echo "round-robin" > /sys/class/nvme-subsystem/nvme-subsys0/iopolicy
    ```
4.  可选：使用 `udev`{.literal} 规则永久设置 I/O 策略。使用以下内容创建
    `/etc/udev/rules.d/71-nvme-io-policy.rules`{.literal} 文件：
    ``` screen
    ACTION=="add|change", SUBSYSTEM=="nvme-subsystem", ATTR{iopolicy}="round-robin"
    ```
:::
::: orderedlist
**验证**
1.  检查您的系统是否识别 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme0n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    ```
2.  列出所有连接的 NVMe 子系统：
    ``` screen
    # nvme list-subsys
    nvme-subsys0 - NQN=testnqn
    \
     +- nvme0 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme1 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme2 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
     +- nvme3 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
    ```
    检查活动的传输类型。例如，`nvme0 fc`{.literal}
    表示设备通过光纤通道传输连接，`nvme tcp`{.literal} 表示设备通过 TCP
    连接。
3.  如果您编辑了内核选项，请检查内核命令行中是否启用了原生 NVMe 多路径：
    ``` screen
    # cat /proc/cmdline
    BOOT_IMAGE=[...] nvme_core.multipath=Y
    ```
4.  检查 DM 多路径将 NVMe 命名空间报告为： `nvme0c0n1`{.literal} by
    `nvme0c3n1`{.literal}，[*而不是像*]{.emphasis}
    `nvme0n1 to`{.literal} `nvme3n1`{.literal} ：
    ``` screen
    # multipath -ll | grep -i nvme
    uuid.8ef20f70-f7d3-4f67-8d84-1bb16b2bfe03 [nvme]:nvme0n1 NVMe,Linux,4.18.0-2
    | `- 0:0:1    nvme0c0n1 0:0     n/a   optimized live
    | `- 0:1:1    nvme0c1n1 0:0     n/a   optimized live
    | `- 0:2:1    nvme0c2n1 0:0     n/a   optimized live
      `- 0:3:1    nvme0c3n1 0:0     n/a   optimized live
    uuid.44c782b4-4e72-4d9e-bc39-c7be0a409f22 [nvme]:nvme0n2 NVMe,Linux,4.18.0-2
    | `- 0:0:1    nvme0c0n1 0:0     n/a   optimized live
    | `- 0:1:1    nvme0c1n1 0:0     n/a   optimized live
    | `- 0:2:1    nvme0c2n1 0:0     n/a   optimized live
      `- 0:3:1    nvme0c3n1 0:0     n/a   optimized live
    ```
5.  如果您更改了 I/O 策略，请检查 `round-robin`{.literal} 是否是 NVMe
    设备中的活跃 I/O 策略：
    ``` screen
    # cat /sys/class/nvme-subsystem/nvme-subsys0/iopolicy
    round-robin
    ```
:::
::: itemizedlist
**其它资源**
-   [有关编辑内核选项的更多信息，请参阅配置内核命令行参数](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/configuring-kernel-command-line-parameters_managing-monitoring-and-updating-the-kernel){.link}。
:::
:::
::: section
::: titlepage
# []{#assembly_enabling-multipathing-on-nvme-devices_configuring-device-mapper-multipath.html#proc_enabling-dm-multipath-on-nvme-devices_assembly_enabling-multipathing-on-nvme-devices}在 NVMe 设备中启用 DM 多路径 {.title}
:::
这个过程使用 DM 多路径解决方案在连接的 NVMe 设备中启用多路径。
::: itemizedlist
**先决条件**
-   NVMe 设备连接到您的系统。
    有关通过光纤传输连接 NVMe 的更多信息，请参阅 [NVMe over fabric
    设备概述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/overview-of-nvme-over-fabric-devicesmanaging-storage-devices){.link}。
:::
::: orderedlist
**流程**
1.  检查原生 NVMe 多路径是否已禁用：
    ``` screen
    # cat /sys/module/nvme_core/parameters/multipath
    ```
    命令显示以下内容之一：
    ::: variablelist
    [`N`{.literal}]{.term}
    :   禁用原生 NVMe 多路径。
    [`Y`{.literal}]{.term}
    :   启用了原生 NVMe 多路径。
    :::
2.  如果启用了原生 NVMe 多路径，请禁用它：
    ::: orderedlist
    1.  从内核命令行中删除 thenvme `_core.multipath=Y`{.literal} 选项：
        ``` screen
        # grubby --update-kernel=ALL --remove-args="nvme_core.multipath=Y"
        ```
    2.  在 64 位 IBM Z 构架中，更新引导菜单：
        ``` screen
        # zipl
        ```
    3.  如果存在
        `nvme_core multipath=Y 行，请从 /etc/modprobe.d/nvme_core.conf`{.literal}
        文件中删除选项``{=html} nvme_core multipath=Y 行。
    4.  重启系统。
    :::
3.  确保启用了 DM 多路径：
    ``` screen
    # systemctl enable --now multipathd.service
    ```
4.  在所有可用路径上分发 I/O。在 `/etc/multipath.conf`{.literal}
    文件中添加以下内容：
    ``` screen
    device {
      vendor "NVME"
      product ".*"
      path_grouping_policy    group_by_prio
    }
    ```
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    当 DM 多路径管理 NVMe 设备
    `时，/sys/class/nvme-subsys0/iopolicy`{.literal} 配置文件对 I/O
    分发没有影响。
    :::
5.  重新载入 `multipathd`{.literal} 服务以应用配置更改：
    ``` screen
    # multipath -r
    ```
6.  备份 `initramfs`{.literal} 文件系统：
    ``` screen
    # cp /boot/initramfs-$(uname -r).img \
         /boot/initramfs-$(uname -r).bak.$(date +%m-%d-%H%M%S).img
    ```
7.  重建 `initramfs`{.literal} 文件系统：
    ``` screen
    # dracut --force --verbose
    ```
:::
::: orderedlist
**验证**
1.  检查您的系统是否识别 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme0n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme1n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme1n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme2n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme2n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme3n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme3n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    ```
2.  列出所有连接的 NVMe
    子系统。检查命令是否将其报告为例如，`nvme0n1`{.literal} by
    `nvme3n2`{.literal}，[*而不是像*]{.emphasis}
    `nvme0c0n1 to`{.literal} `nvme0c3n1`{.literal} ：
    ``` screen
    # nvme list-subsys
    nvme-subsys0 - NQN=testnqn
    \