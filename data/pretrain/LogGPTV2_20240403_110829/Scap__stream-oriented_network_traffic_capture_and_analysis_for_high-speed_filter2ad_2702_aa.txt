title:Scap: stream-oriented network traffic capture and analysis for high-speed
networks
author:Antonis Papadogiannakis and
Michalis Polychronakis and
Evangelos P. Markatos
Scap: Stream-Oriented Network Trafﬁc Capture and
Analysis for High-Speed Networks
Antonis Papadogiannakis
FORTH-ICS, Greece
PI:EMAIL
Michalis Polychronakis
Columbia University, USA
PI:EMAIL
Evangelos P. Markatos
FORTH-ICS, Greece
PI:EMAIL
ABSTRACT
Many network monitoring applications must analyze trafﬁc beyond
the network layer to allow for connection-oriented analysis, and
achieve resilience to evasion attempts based on TCP segmentation.
However, existing network trafﬁc capture frameworks provide ap-
plications with just raw packets, and leave complex operations like
ﬂow tracking and TCP stream reassembly to application develop-
ers. This gap leads to increased application complexity, longer de-
velopment time, and most importantly, reduced performance due
to excessive data copies between the packet capture subsystem and
the stream processing module.
This paper presents the Stream capture library (Scap), a network
monitoring framework built from the ground up for stream-oriented
trafﬁc processing. Based on a kernel module that directly handles
ﬂow tracking and TCP stream reassembly, Scap delivers to user-
level applications ﬂow-level statistics and reassembled streams by
minimizing data movement operations and discarding uninterest-
ing trafﬁc at early stages, while it inherently supports parallel pro-
cessing on multi-core architectures, and uses advanced capabilities
of modern network cards. Our experimental evaluation shows that
Scap can capture all streams for trafﬁc rates two times higher than
other stream reassembly libraries, and can process more than ﬁve
times higher trafﬁc loads when eight cores are used for parallel
stream processing in a pattern matching application.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Opera-
tions—Network Monitoring
Keywords
Trafﬁc Monitoring; Stream Reassembly; Packet Capturing; Packet
Filtering; Overload Control; Performance
1.
INTRODUCTION
Passive network monitoring is an indispensable mechanism for
increasing the security and understanding the performance of mod-
ern networks. For example, Network-level Intrusion Detection Sys-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC’13, October 23–25, 2013, Barcelona, Spain.
Copyright 2013 ACM 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504750.
tems (NIDS) inspect network trafﬁc to detect attacks [36, 40] and
pinpoint compromised computers [18, 43]. Similarly, trafﬁc clas-
siﬁcation tools inspect trafﬁc to identify different communication
patterns and spot potentially undesirable trafﬁc [1, 24]. To make
meaningful decisions, these monitoring applications usually ana-
lyze network trafﬁc at the transport layer and above. For instance,
NIDSs reconstruct the transport-layer data streams to detect attack
vectors spanning multiple packets, and perform trafﬁc normaliza-
tion to avoid evasion attacks [14, 19, 38].
Unfortunately, there is a gap between monitoring applications
and underlying trafﬁc capture tools: Applications increasingly need
to reason about higher-level entities and constructs such as TCP
ﬂows, HTTP headers, SQL arguments, email messages, and so on,
while trafﬁc capture frameworks still operate at the lowest possible
level:
they provide the raw—possibly duplicate, out-of-order, or
overlapping—and in some cases even irrelevant packets that reach
the monitoring interface [11, 28, 29]. Upon receiving the captured
packets at user space, monitoring applications usually perform TCP
stream reassembly using an existing library such as Libnids [2] or a
custom stream reconstruction engine [36, 40]. This results in addi-
tional memory copy operations for extracting the payloads of TCP
segments and merging them into larger stream “chunks” in contigu-
ous memory. Moreover, it misses several optimization opportuni-
ties, such as the early discarding of uninteresting packets before
system resources are spent to move them to user level, and assign-
ing different priorities to transport-layer ﬂows so that they can be
handled appropriately at lower system layers.
To bridge this gap and address the above concerns, in this pa-
per we present the Stream capture library (Scap), a uniﬁed passive
network monitoring framework built around the abstraction of the
Stream. Designed from the beginning for stream-oriented network
monitoring, Scap (i) provides the high-level functionality needed
by monitoring applications, and (ii) implements this functionality
at the most appropriate place: at user level, at kernel level, or even
at the network interface card.
To enable aggressive optimizations, we introduce the notion of
stream capture: that is, we elevate the Stream into a ﬁrst-class ob-
ject that is captured by Scap and handled by user applications. Al-
though previous work treats TCP stream reassembly as a necessary
evil [50], used mostly to avoid evasion attacks against intrusion
detection and other monitoring systems, we view streams as the
fundamental abstraction that should be exported to network moni-
toring applications, and as the right vehicle for the monitoring sys-
tem to implement aggressive optimizations all the way down to the
operating system kernel and network interface card.
To reduce the overhead of unneeded packets, Scap introduces the
notion of subzero packet copy. Inspired by zero-copy approaches
that avoid copying packets from one main memory location to an-
other, Scap not only avoids redundant packet copies, but also avoids
bringing some packets in main memory in the ﬁrst place. We show
several cases of applications that are simply not interested in some
packets, such as the tails of large ﬂows [9, 26, 27, 33]. Subzero
packet copy identiﬁes these packets and does not bring them in
main memory at all:
they are dropped by the network interface
card (NIC) before reaching the main memory.
To accommodate heavy loads, Scap introduces the notion of pri-
oritized packet loss (PPL). Under heavy load, traditional monitor-
ing systems usually drop arriving packets in a random way, severely
affecting any following stream reassembly process.
However,
these dropped packets and affected streams may be important for
the monitoring application, as they may contain an attack or other
critical information. Even carefully provisioned systems that are
capable of handling full line-rate trafﬁc can be overloaded, e.g.,
by a sophisticated attacker that sends adversarial trafﬁc to exploit
an algorithmic complexity vulnerability and intentionally overload
the system [34, 45]. Scap allows applications to (i) deﬁne different
priorities for different streams and (ii) conﬁgure threshold mecha-
nisms that give priority to new and small streams.
Scap provides a ﬂexible and expressive Application Program-
ming Interface (API) that allows programmers to conﬁgure all as-
pects of the stream capture process, perform complex per-stream
processing, and gather per-ﬂow statistics with a few lines of code.
Our design introduces two novel features: (i) it enables the early
discarding of uninteresting trafﬁc, such as the tails of long-lived
connections that belong to large ﬁle transfers, and (ii) it offers more
control for tolerating packet loss under high load through stream
priorities and best-effort reassembly. Scap also avoids the overhead
of extra memory copies by optimally placing TCP segments into
stream-speciﬁc memory regions, and supports multi-core systems
and network adapters with receive-side scaling [22] for transparent
parallelization of stream processing.
We have evaluated Scap in a 10GbE environment using real traf-
ﬁc and showed that it outperforms existing alternatives like Lib-
nids [2] and Snort’s stream reassembly [40] in a variety of sce-
narios. For instance, our results demonstrate that Scap can cap-
ture and deliver all streams with low CPU utilization for rates up
to 5.5 Gbit/s using a single core, while Libnids and Snort start
dropping packets at 2.5 Gbit/s due to increased CPU utilization
for stream reassembly at user level. A single-threaded Scap pat-
tern matching application can handle 33% higher trafﬁc rates than
Snort and Libnids, and can process three times more trafﬁc at 6
Gbit/s. When eight cores are used for parallel stream processing,
Scap can process 5.5 times higher rates with no packet loss.
The main contributions of this paper are:
1. We introduce the notion of stream capture, and present the
design, implementation, and evaluation of Scap , a stream-
oriented network trafﬁc processing framework. Scap pro-
vides a ﬂexible and expressive API that elevates streams to
ﬁrst-class objects, and uses aggressive optimizations that al-
lows it to deliver transport-layer streams for two times higher
trafﬁc rates than previous approaches.
2. We introduce subzero packet copy, a technique that takes ad-
vantage of ﬁltering capabilities of commodity NICs to not
only avoid copying uninteresting packets (such as the long
tails of large ﬂows) across different memory areas, but to
avoid bringing them in main memory altogether.
3. We introduce prioritized packet loss, a technique that enables
graceful adaptation to overload conditions by dropping pack-
ets of lower priority streams, and favoring packets that be-
long to recent and shorter streams.
2. DESIGN AND FEATURES
The design of Scap is driven by two key objectives: program-
ming expressiveness and runtime performance. In this section, we
introduce the main aspects of Scap across these two dimensions.
2.1 Subzero-Copy Packet Transfer
Several network monitoring applications [9, 26, 27, 33] are in-
terested in analyzing only the ﬁrst bytes of each connection, espe-
cially under high trafﬁc load. In this way, they analyze the more
useful (for them) part of each stream and discard a signiﬁcant per-
centage of the total trafﬁc [27]. For such applications, Scap has
incorporated the use of a cutoff threshold that truncates streams to
a user-speciﬁed size, and discards the rest of the stream (and the
respective packets) within the OS kernel or even the NIC, avoiding
unnecessary data transfers to user space. Applications can dynam-
ically adjust the cutoff size per stream, or set a different cutoff for
each stream direction, allowing for greater ﬂexibility.
Besides a stream cutoff size, monitoring applications may be in-
terested in efﬁciently discarding other types of less interesting traf-
ﬁc. Many applications often use a BPF ﬁlter [28] to deﬁne which
streams they want to process, while discarding the rest.
In case
of an overload, applications may want to discard trafﬁc from low
priority streams or deﬁne a stream overload cutoff [26, 33]. Also,
depending on the stream reassembly mode used by an application,
packets belonging to non-established TCP connections or duplicate
packets may be discarded. In all such cases, Scap can discard the
appropriate packets at an early stage within the kernel, while in
many cases packets can be discarded even earlier at the NIC.
To achieve this, Scap capitalizes on modern network interfaces
that provide ﬁltering facilities directly in hardware. For example,
Intel’s 82599 10G interface [21] supports up to 8K perfect match
and 32K signature (hash-based) Flow Director ﬁlters (FDIR). These
ﬁlters can be added and removed dynamically, within no more than
10 microseconds, and can match a packet’s source and destination
IP addresses, source and destination port numbers, protocol, and
a ﬂexible 2-byte tuple anywhere within the ﬁrst 64 bytes of the
packet. Packets that match an FDIR ﬁlter are directed to the hard-
ware queue speciﬁed by the ﬁlter. If this hardware queue is not used
by the system, the packets will be just dropped at the NIC layer, and
they will never be copied to the system’s main memory [13]. When
available, Scap uses FDIR ﬁlters to implement all above mentioned
cases of early packet discarding. Else, the uninteresting packets are
dropped within the OS kernel.
2.2 Prioritized Packet Loss
Scap introduces Prioritized Packet Loss (PPL) to enable the sys-
tem to invest its resources effectively during overload.
This is
necessary because sudden trafﬁc bursts or overload conditions may
force the packet capturing subsystem to ﬁll up its buffers and ran-
domly drop packets in a haphazard manner. Even worse, attackers
may intentionally overload the monitoring system while an attack
is in progress so as to evade detection. Previous research in NIDSs
has shown that being able to handle different ﬂows [16, 25, 34], or
different parts of each ﬂow [26,33], in different ways can enable the
system to invest its resources more effectively and signiﬁcantly im-
prove detection accuracy. PPL is a priority assignment technique
that enables user applications to deﬁne the priority of each stream
so that in case of overload, packets from low-priority streams are
the ﬁrst ones to go. User applications can also deﬁne a threshold for
the maximum stream size under overload (overload_cutoff ). Then,
packets situated beyond this threshold are the ones to be dropped.
As long as the percentage of used memory is below a user-deﬁned
threshold (called base_threshold), PPL drops no packets. When,
however, the used memory exceeds the base_threshold, PPL kicks
in: it ﬁrst divides the memory above base_threshold into n (equal
to the number of used priorities) regions using n+1 equally spaced
watermarks (i.e., watermark0, watermark1, ..., watermarkn),
where watermark0 = base_threshold and watermarkn =
memory_size. When a packet belonging to a stream with the
ith priority level arrives, PPL checks the percentage of memory
used by Scap at that time. If it is above watermarki, the packet
is dropped. Otherwise, if the percentage of memory used is be-
tween watermarki and watermarki−1, PPL makes use of the
overload_cutoff, if it has been deﬁned by the user. Then, if the
packet is located in its stream beyond the overload_cutoff byte, it
is dropped.
In this way, high priority streams, as well as newly
created and short streams if an overload_cutoff is deﬁned, will be
accommodated with higher probability.
2.3 Flexible Stream Reassembly
To support monitoring at the transport layer, Scap provides dif-
ferent modes of TCP stream reassembly. The two main objec-
tives of stream reassembly in Scap are: (i) to provide transport-
layer reassembled chunks in continuous memory regions, which
facilitates stream processing operations, and (ii) to perform pro-
tocol normalization [19, 51]. Scap currently supports two differ-
ent modes of TCP stream reassembly: SCAP_TCP_STRICT and
SCAP_TCP_FAST. In the strict mode, streams are reassembled
according to existing guidelines [14,51], offering protection against
evasion attempts based on IP/TCP fragmentation. In the fast mode,
streams are reassembled in a best-effort way, offering resilience
against packet loss caused in case of overloads. In this mode, Scap
follows the semantics of the strict mode as closely as possible, e.g.,
by handling TCP retransmissions, out-of-order packets, and over-
lapping segments. However, to accommodate for lost segments,
stream data is written without waiting for the correct next sequence
number to arrive. In that case, Scap sets a ﬂag to report that errors
occurred during the reassembly of a particular chunk.
Scap uses target-based stream reassembly to implement different
TCP reassembly policies according to different operating systems.
Scap applications can set a different reassembly policy per each
stream. This is motivated by previous work, which has shown that
stream reassembly performed in a NIDS may not be accurate [38].
For instance, the reconstructed data stream may differ from the ac-
tual data stream observed by the destination. This is due to the
different TCP reassembly policies implemented by different oper-
ating systems, e.g., when handling overlapping segments. Thus, an
attacker can exploit such differences to evade detection. Shankar
and Paxson [42] developed an active mapping solution to determine
what reassembly policy a NIDS should follow for each stream.
Similarly to Scap, Snort uses target-based stream reassembly [32]
to deﬁne the reassembly policy per host or subnet.
Scap also supports UDP: a UDP stream consists of the concate-
nation of the payloads of the arriving packets of the respective UDP
ﬂow. For other protocols without sequenced delivery, Scap return
each packet for processing without reassembly.
2.4 Parallel Processing and Locality
Scap has inherent support for multi-core systems, hiding from
the programmer the complexity of creating and managing multiple
processes or threads. This is achieved by transparently creating
a number of worker threads for user-level stream processing (typ-
ically) equal to the number of the available cores. Using afﬁnity
calls, the mapping of threads to CPU cores is practically one-to-
one. Scap also dedicates a kernel thread on each core for handling
packet reception and stream reassembly. The kernel and worker
threads running on the same core process the same streams. As
each stream is assigned to only one kernel and worker thread, all
processing of a particular stream is done on the same core, reduc-
ing, in this way, context switches, cache misses [15, 37], and inter-
thread synchronization operations. The kernel and worker threads
on each core communicate through shared memory and events: a
new event for a stream is created by the kernel thread and is han-
dled by the worker thread using a user-deﬁned callback function
for stream processing.
To balance the network trafﬁc load across multiple NIC queues
and cores, Scap uses both static hash-based approaches, such as
Receive Side Scaling (RSS) [22], and dynamic load balancing ap-
proaches, such as ﬂow director ﬁlters (FDIR) [21]. This provides
resiliency to short-term load imbalance that could adversely affect
application performance. First, Scap detects a load imbalance when
one of the cores is assigned a portion of the total streams larger than
a threshold. Then, subsequent streams assigned by RSS to this core
are re-directed with an FDIR to the core that handles the lowest
number of streams at the time.
2.5 Performance Optimizations
In case that multiple applications running on the same host moni-
tor the same trafﬁc, Scap provides all of them with a shared copy of
each stream. Thus, the stream reassembly operation is performed
only once within the kernel, instead of multiple times for each user-
level application. If applications have different conﬁgurations, e.g.,
for stream size cutoff or BPF ﬁlters, the capture system takes a best
effort approach to satisfy all requirements. For instance, it sets the
largest among the cutoff sizes for all streams, and keeps streams
that match at least one of the ﬁlters, marking the applications that
should receive each stream and their cutoff.
Performing stream reassembly in the kernel also offers signif-
icant advantages in terms of cache locality. Existing user-level
TCP stream reassembly implementations receive packets of dif-
ferent ﬂows highly interleaved, which results in poor cache local-
ity [35].
In contrast, Scap provides user-level applications with
reassembled streams instead of randomly interleaved packets, al-
lowing for improved memory locality and reduced cache misses.
3. SCAP API
The main functions of the Scap API are listed in Table 1.
3.1 Initialization
An Scap program begins with the creation of an Scap socket us-
ing scap_create(), which speciﬁes the interface to be moni-
tored. Upon successful creation, the returned scap_t descriptor
is used for all subsequent conﬁguration operations. These include
setting a BPF ﬁlter [28] to receive a subset of the trafﬁc, cutoff val-
ues for different stream classes or stream directions, the number of
worker threads for balancing stream processing among the avail-
able cores, the chunk size, the overlap size between subsequent
chunks, and an optional timeout for delivering the next chunk for
processing. The overlap argument is used when some of the last
bytes of the previous chunk are also needed in the beginning of the
next chunk, e.g., for matching a pattern that might span consecutive
chunks. The flush_timeout parameter can be used to deliver
for processing a chunk smaller than the chunk size when this time-
out passes, in case the user needs to ensure timely processing.
3.2 Stream Processing
Scap allows programmers to write and register callback func-
tions for three different types of events: stream creation, the avail-
ability of new stream data, and stream termination. Each callback
Scap Function Prototype
Description
scap_t *scap_create(const char *device, int memory_size,
Creates an Scap socket
int reassembly_mode, int need_pkts)
int scap_set_filter(scap_t *sc, char *bpf_filter)
int scap_set_cutoff(scap_t *sc, int cutoff)
int scap_add_cutoff_direction(scap_t *sc, int cutoff, int direction)
int scap_add_cutoff_class(scap_t *sc, int cutoff, char* bpf_filter)
int scap_set_worker_threads(scap_t *sc, int thread_num)
int scap_set_parameter(scap_t *sc, int parameter, int value)
int scap_dispatch_creation(scap_t *sc,
int scap_dispatch_data(scap_t *sc,
int scap_dispatch_termination(scap_t *sc,
void (*handler)(stream_t *sd))
void (*handler)(stream_t *sd))
void (*handler)(stream_t *sd))
int scap_start_capture(scap_t *sc)
void scap_discard_stream(scap_t *sc, stream_t *sd)
int scap_set_stream_cutoff(scap_t *sc, stream_t sd, int cutoff)
int scap_set_stream_priority(scap_t *sc, stream_t *sd, int priority)
int scap_set_stream_parameter(scap_t *sc, stream_t *sd,
int parameter, int value)
int scap_keep_stream_chunk(scap_t *sc, stream_t *sd)
char *scap_next_stream_packet(stream_t *sd, struct scap_pkthdr *h)
int scap_get_stats(scap_t *sc, scap_stats_t *stats)
void scap_close(scap_t *sc)
Applies a BPF ﬁlter to an Scap socket
Changes the default stream cutoff value
Sets a different cutoff value for each direction
Sets a different cutoff value for a subset of the trafﬁc
Sets the number of threads for stream processing
Changes defaults: inactivity_timeout, chunk_size,
overlap_size, ﬂush_timeout, base_threshold, overload_cutoff
Registers a callback routine for handling stream
creation events
Registers a callback routine for processing newly
arriving stream data
Registers a callback routine for handling stream
termination events
Begins stream processing
Discards the rest of a stream’s trafﬁc
Sets the cutoff value of a stream
Sets the priority of a stream