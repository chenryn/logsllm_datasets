duce these ﬁngerprints do not reveal any personally identiﬁable
information about the user of the transmitting device. We only
performed full identiﬁcation and tracking on 17 devices that
we controlled. According to our university’s IRB ofﬁce, this
work does not qualify as human subjects research.
1698
Data Analysis
We ﬁngerprint and identify devices using our BLE tracking
toolkit described in Section III. To apply this methodology
on ﬁeld-collected datasets, we ﬁrst had to determine how
many packets an attacker needs to receive from each device to
accurately ﬁngerprint and identify it. We found this threshold
by performing a controlled experiment using 20 ESP32 BLE
chipsets. We tested in varying SNR conditions from 10 to
30 dB—exactly what an attacker would typically see in the
ﬁeld—to see if the number of packets needed for ﬁngerprinting
and identiﬁcation increases when beacons have poor SNR.
Next, we identiﬁed each of the 20 devices using the algorithm
described in Section III-D. We split the captures used for
training and test as follows: 80% of the beacons were used
for training (i.e., ﬁngerprinting), and 20% for testing (i.e.,
identifying). We trained with beacons at three SNR values:
{10, 15, 25} dB. Then, we ran identiﬁcation tests with beacons
that had {10, 15, 25} dB SNR independently. We evaluated the
identiﬁcation accuracy of different training sizes with a test
size of 10 packets.
Figure 11 shows the accuracy of identifying the devices
compared to the number of training packets used for building
the device ﬁngerprints. For all SNR values, having 50 packets
for training is sufﬁcient. Many BLE devices transmit signiﬁ-
cantly more than 50 beacons a minute (Table I); therefore we
estimate an attacker only needs to isolate a mobile device for
at most one minute to get enough packets to ﬁngerprint it.
Figure 12 shows the accuracy of classifying the devices
compared to the number of packets used (the number of
training packets is ﬁxed to 50 per device). Across the tested
SNRs, an attacker only needs 10 packets to accurately identify
a device. For the rest of the ﬁeld study, we use 50 packets to
ﬁngerprint a device, and 10 packets to identify a device.
A. False Positives and False Negatives
In the following experiments, we evaluate the likelihood that
our BLE tracking toolkit confuses a device that is not a target
with a target (False Positive), and the likelihood that it does
not identify a target when it is present (False Negative).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:12 UTC from IEEE Xplore.  Restrictions apply. 
020406080100#ofpacketsusedfortraining0%20%40%60%80%100%AccuracyTestSNR101520250510152025303540#ofpacketsusedfortest0%20%40%60%80%100%AccuracyTestSNR10152025Features used
CFO only
I/Q offset only
I/Q imbalance only
All Features
FNR
FPR
2.42%
2.45%
19.84% 2.39%
32.53% 1.52%
1.21% 2.53%
TABLE II: Hardware imperfection-speciﬁc FPR and FNR.
Devices Compared
Only Apple Products
Only other Products
Apple vs other
All Devices
FNR
2.40%
2.94%
FPR
1.91%
1.15%
0.15%
1.21% 2.53%
—
TABLE III: Manufacturer-speciﬁc FPR and FNR.
repeating a similar experiment as we used to compare device
manufacturers. CFO contributes the most to identiﬁcation, as
it can have a wider range of values for different devices
compared to I/Q imperfections. I/Q imperfections alone have a
much higher FPR, but they can resolve the confusion between
devices with similar CFO. This same phenomena is also visible
our controlled lab experiments (Figure 6) where some devices
have CFO values close to each other, but their difference in
I/Q imperfection makes them distinguishable. Also, recall that
temperature can cause variation CFO while it does not have
any notable impact on I/Q imperfections. As a result, I/Q
imperfections can help identify the target when it experiences
temperature changes.
2) Effect of device model: Based on our controlled ex-
periments (Section IV-A), we expect devices from the same
manufacturer to be more likely to be confused than devices
of different manufacturers. To test this hypothesis, we used
the technique proposed in [11] to distinguish Apple products
in our dataset from other devices. About 76% (123 devices)
in the dataset are Apple products. The prominence of Apple
products in the dataset is likely because Apple enables their
BLE-based device handoff service by default on many of their
mobile products, including iPhones and Apple Watches.†
Table III shows the FPR and FNR of Apple products
compared with other products. As expected, the FPR when
comparing Apple devices with other Apple devices (1.91%)
is greater than the median FPR when comparing across all
devices (0.62%). Also, the FPR and FNR when comparing
Apple products with other devices is close to zero. This
appears to conﬁrm our hypothesis that devices from the same
manufacturer are more likely to be similar to each other than
devices from different manufactures.
3) Effect of temperature: The temperature of the devices
we observe in the ﬁeld were unlikely to experience signiﬁcant
temperature changes during the course of our data collection.
Therefore, we perform a model-based simulation to evaluate
the effect of temperature changes on FPR and FNR. Recall
that temperature changes affect CFO because of the well-
documented relationship between frequency drift of crystal
oscillators and their temperature (Section IV-A). Using the
curves in [12], we calculate the change in CFO (∆f) as
temperature drifts further from the temperature baseline when
the device was ﬁngerprinted (∆T ◦C). To ensure the target
†We collected this dataset before COVID–19 contact tracing launched.
1699
Fig. 13: Dist. of FPR a device when comparing with all others
Given the absence of ground truth of device identities in our
dataset, we relied upon the fact that BLE devices have stable
MAC addresses for ∼15 minutes (after with they re-randomize
the MAC address). Therefore, we used the MAC as ground
truth that multiple packets received were from the same device.
However, a device’s MAC address can be randomized during
our data collection, causing us to incorrectly treat the same
physical-layer ﬁngerprint as two devices. We mitigated this
problem by only considering devices that we observed during
one contiguous period of time in each location where we did
not observe any new devices, nor any devices that appear to
stop transmitting. This ﬁltering left us with 162 devices to use
for our false positive and false negative evaluation.
We
device
(MAC
every
consider
address)
i ∈ {1, 2, 3, ..., 162} as a target, and we train our classiﬁer
to ﬁnd that device’s ﬁngerprint (Section III-D). Then, for
each of the other devices, we run the classiﬁer to see if it
identiﬁes them as the target (i) device. If it does, then that is
considered a false positive. The number of false positives for
target device i divided by the total number of devices is the
False Positive Rate (FPR) for device i. Next, we ﬁngerprint
each target i and run the classiﬁer to see if it fails to identify
each device as itself. Each instance of this is a false negative.
We repeat this process for all the 162 devices (each time one
of them is selected as the target), and divide the result by the
total number of devices to compute the total False Negative
Rate (FNR). We observe our classiﬁer achieves a 2.5% FNR
across all 162 devices.
Figure 13 shows the distribution of FPR for each of the
162 devices. The median FPR of a device is only 0.62%.
Moreover, 40% of the devices were not confused with any
other device (zero FPR), which implies many devices seen
in the ﬁeld have unique physical-layer ﬁngerprints. Owning a
device with unique imperfections makes someone particularly
vulnerable to BLE tracking attacks. We also observed a small
fraction of devices had an FPR as high as 10%.
1) How imperfections contribute to identiﬁcation: Next, we
evaluate how each of the imperfections contribute to identiﬁ-
cation. Table II shows the FPR and FNR when using CFO,
I/Q offset and I/Q imbalance separately, and all together, by
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:12 UTC from IEEE Xplore.  Restrictions apply. 
0%2%4%6%8%10%12%FPR0.000.200.400.600.801.00CDFFig. 14: How oscillator temperature changes affect FPR.
is not missed even if the temperature changes are as large
as ∆T ◦C, we modiﬁed the classiﬁer to accept the device
as the target even if the CFO of the device is ∆f away
from the ﬁngerprinted CFO of the target. The consequence
of increasing the range of acceptable CFO values is that it
increases the chance of observing a device whose CFO falls
in the acceptable range, resulting in an increase in FPR.
Figure 14 presents the FPR as the change in temperature
increases. We present the results for both high-quality and
low-quality crystals (i.e., different cutting accuracies), as the
type of crystal depends on the speciﬁc device being targeted.
Temperature change causes signiﬁcantly less change in CFO
(and thus less increase in FPR) for high-quality crystals (0
minute cutting accuracy) compared to low quality crystals
(8 minute cutting accuracy). For low-quality crystals, FPR
increases rapidly as the temperature increases. If the change in
temperature is too signiﬁcant (25◦C), CFO becomes useless
for identiﬁcation: the FPR is the same as if we only used IQ
offset and IQ imbalance. In summary, temperature changes
can severely limit an attacker’s ability to track a target device.
B. Uniqueness of imperfections
Recall that across the 162 devices observed in our ﬁrst
ﬁeld evaluation dataset, we found ∼40% of the devices to be
uniquely identiﬁable. However, is natural to ask, is the same
true at large scale? If the attacker were to observe several
hundred devices over multiple days, will we see a similar
fraction of devices that are uniquely identiﬁable?
To answer this question, we performed a larger-scale ﬁeld
data collection. We placed an SDR at the exit of a room
where hundreds of different devices passed by each day. We
recorded the Apple/Google COVID–19 Exposure Notiﬁcation
BLE beacons transmitted by those devices over the course
of l0 hours on two days, separated by one week to limit the
number of duplicate devices. We computed the mean CFO
and mean I/Q offset magnitude for each BLE MAC address we
observed in the beacons. The mean hardware imperfections are
representative of the ﬁngerprint of the BLE device. To reduce
Fig. 15: Histogram of imperfections across 647 BLE devices.
the chance that we observed the same device with two or more
different MAC addresses, we ﬁltered out devices which were
observed for a duration longer than three minutes‡.
We observed 647 unique MAC addresses across the two 20
hours of data collection. Figure 15 shows the 2-Dimensional
histogram of the ﬁngerprints of these devices, namely their
CFO and I/Q offset magnitude. The number of histogram bins
were chosen so that the number of bins (2500) is signiﬁcantly
larger than the total BLE devices observed. Each bin represents
a CFO range of ∼1.3 kHz, and an I/Q offset magnitude range
of 0.00516. Devices that fall in the same bin are considered to
have indistinguishable hardware imperfections. We also show
the bounds of the 2D histogram that cover 36% (∼σ) and
67% (∼2σ) of the devices (σ because imperfections tend to
be normally distributed).
We found that 47.1% (305) of the devices were unique.
This conﬁrms that even in a larger data set, ∼40% of devices
are uniquely distinguishable. We also observed that devices
with overlaps did not overlap with many other devices. For
instance, 15% (97) of the devices had similar imperfections
with only one other device.
C. Case Study 1: Temporal tracking of many targets
Next, we conduct an experiment to evaluate how well our
toolkit can track 17 controlled targets over time, in real world
environments. These controlled targets are listed in Table IV.
Each target is isolated in an ofﬁce to capture 50 packets to
train the classiﬁer with its ﬁngerprint.
False Negative dataset: Between 2–7 days after we ﬁnger-
printed the targets, we individually took them to a different
location, and we captured their packets using a USRP N210
sniffer placed 10 ft away from the targets. We did not strictly
force the targets to have the same temperature in the ofﬁce and
food court, but both environments were air-conditioned indoor
buildings and there was nominal activity on the targets.
False Positive dataset: We evaluated the FPR for these
targets using a trace from a coffee shop from our ﬁeld datasets,
‡Apple rotates addresses every 15 mins and Android every 10 mins.
1700
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:12 UTC from IEEE Xplore.  Restrictions apply. 
13510152025Low-qualitycrystal0%2%4%6%8%10%12%FPRTemperature∆(◦C)51525High-qualitycrystal-20020CFO(KHz)0.0050.0100.0150.0200.0250.030I/Qoffsetmagnitude36%66%0246#ofdevicesFig. 16: FNR–FPR for 17 controlled targets.
#: Device
1: iPhone 10
2: iPhone 8
3: iPhone 11
4: Bose Headset
5: iWatch
6: iPhone 8
#: Device
7: iPhone 10
8: iWatch
9: iPhone 10
10: iPhone 8
11: iPhone 10
12: iWatch
#: Device
13: MacBook Pro
14: Thinkpad
15: AirPod
16: Pixel 2
17: Pixel 5
TABLE IV: 17 target devices used for this experiment and
their label numbers that are used in Figures 16 and 17.
because we knew the 17 controlled devices were not present
during that experiment.
Temporal FNR and FPR: We calculate the FNR and FPR
over time, in each 10 second interval of the captures. In each
time interval, we provide 10 packets from each MAC address
to the classiﬁer to determine if it matches any of the 17 targets’
ﬁngerprints. The FNR is the fraction of intervals where the
target was present, but was not identiﬁed, and the FPR is the
fraction of intervals where the target was not present, but was
mistakenly identiﬁed.
Results: Figure 16 shows the average FNR and FPR for
these 17 targets. The average FNR of these controlled targets
is 3.21% and the average FPR is 3.5%. Although there are
a few devices with high FNR and FPR, most devices have
distinguishable hardware imperfections, resulting in low FNR
and FPR.
Figure 17 shows the temporal patterns of false positive
occurrences for each of the 17 targets in one of the ﬁeld
traces. Each time there is a bump in a device’s horizontal line,
it means that at least one device was mistakenly identiﬁed
as being the target during that time interval. We observe that
false positives are sometimes short-lived, but often they last
for longer than one 10-second interval, possibly indicating a
device with similar hardware imperfections came within range
of the sniffer.
D. Case Study 2: Tracking a person
Finally, we describe an end-to-end tracking attack we
executed on a controlled target (a volunteer who uses an
iPhone). The attacker ﬁrst carries their SDR sniffer close to the
target device to obtain the device’s physical-layer ﬁngerprint.
1701
Fig. 17: FPR occurrences over time for each of the 17 targets.
Simultaneously, the attacker scans for nearby BLE devices
using a commonly available BLE scanner phone app, and they
record the MAC address of the BLE device with the highest
observed signal strength, which is the nearest device (i.e., the
target’s phone). Later, they use this MAC address to pick out
the target device’s packets from the raw sniffer capture. Then,
they feed these packets into the BLE tracking toolkit to train
its classiﬁer with the target device’s ﬁngerprint.
After creating the ﬁngerprint, the attacker tracks their target
by placing an SDR and laptop close to their target’s home. The
attacker can determine when the target is home by observing
when the classiﬁer running on the laptop indicates the packets
received by the SDR match the target device’s ﬁngerprint.
The attacker tracks their target for one hour, during which the
target walks inside and outside the house 2 times. Figure 18
shows the number of unique MAC addresses observed every
ten seconds during this hour. There are approximately 30 other
devices nearby that could be confused with the target.