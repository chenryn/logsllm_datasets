9:
10:
11:
12:
13:
14:
15: end while
16: return a cluster with size i.
star t = max(i − 2M, star t)
end = i
M = round(sqr t(M))
i = star t
if д(i)  0 then
д(y). Then, the samples in Sor acle with these features will be tested
against M, and KARMA will calculate the accuracy based on part of
the detection results in д(y) and the rest in д(x).
4.3 Causality Removal (Unlearning)
In this part of the section, we present the unlearning module in
Algorithm 3. First, a list of seeds is generated by the causality search
module based on the learning model, misclassified data, and training
data (Line 1–2). Next, the algorithm sets the initial value of the
working set Stmp to be Str aininд (Line 3). Starting from each seed
(Line 6), the causality determination module decides the cluster size
and forms a cluster (Line 10). The cluster, if polluted, is pushed to a
set Sclusters (Line 11–13) and removed from our working dataset
Stmp (Line 14). Then, the algorithm will repeat forming clusters
until the working set is empty (Line 5). During the repetition, if the
algorithm runs out of seeds, it randomly select a sample from the
the working set as the seed (Line 7–9).
Then, the algorithm goes through the generated list of clusters of
the training set (Line 16–20). Specifically, the algorithm calculates
the current accuracy delta (Line 17), ensures that each cluster does
cause the accuracy to increase (Line 18), and then unlearns the
cluster from M (Line 19). Next, the entire algorithm (Line 1–21)
is repeated based on a new M with polluted clusters unlearned and
a new Str aininд with polluted clusters removed. That is, the seed
finder will find a new list of seeds fed into the peak finder, and then
the algorithm will have a new list of clusters, and find the cluster to
unlearn.
The iteration of Algorithm 3 is stopped based on two possible
conditions. First, if the detection accuracy meets the expected value
of the administrator of the learning model, the algorithm will stop.
That is, the produced new learning model will be enough for use in
the view of the administrator. Second, if the algorithm cannot find
any new clusters to unlearn (or precisely no clusters that cause the
detection accuracy to increase), the algorithm will also stop. That
is, in this case, ideally, all the polluted data samples are removed
from the training set, and unlearned from the learning model. In
Causal Unlearning
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
practice, as shown in the evaluation, we may still have a small
number of samples, such as less than one or two percent of polluted
data, scattered in the training set, however such polluted samples
will have little impacts on the learning model as evident by the fact
that the detection accuracy goes back to the vanilla value. The reason
is that learning model itself is somehow robust to a small number of
erroneous data, especially when they are not in clusters.
IMPLEMENTATION
5
Our prototype implementation of KARMA framework contains 3,009
lines of Python code. In particular, the implementation of divergence
score contains 330 lines of code, the cluster forming 324 lines of
code, the core of KARMA 1,968 lines of code, and other parts (such
as the interface with learning model) 717 lines of code. The proto-
type implementation uses stand-alone files that provide APIs to be
interacted with other learning systems. The main interaction (unless
otherwise specified in Section 6.8 where we use an SVM-based
spam filter and another Bayes-based JavaScript malware detector) is
with SpamBayes [6], a naïve Bayes classifier capable of identifying
spam and ham (non-spam) emails. We choose SpamBayes, because
there is a published paper [33] documenting how to pollute Spam-
Bayes, and as shown in Section 6 we will follow the paper to launch
dictionary attacks. We believe that using the same learning system
for data pollution documented in the literature will greatly reduce
any potential bias.
6 EVALUATION
In the section of evaluation, we are going to evaluate our prototype
implementation of KARMA framework.
6.1 Attacks and Datasets
In this subsection, following the two attack scenarios described in
the threat model of Section 2, we present how we evaluate KARMA
against these two attacks. Our vanilla dataset is a publicly avail-
able spam dataset called Enron-Spam [2, 41] consisting of 517,401
emails. Nine tenths of the vanilla dataset, i.e., 465,661 emails, are
used for the vanilla training set, and the rest will be used for the
oracle and testing dataset.
Mislabelling Attacks. In this attack scenario, as detailed in Sec-
tion 2, the administrator relies on crowdsourcing workers to label
training dataset, and the attacker’s capability is limited to altering
the labels. We assume that the administrator randomly assign all the
training emails to 1,000 crowdsourcing workers, and each worker
will label about 466 emails. Among the workers, some have mali-
cious intents to mislabel emails. In the experiment, we assume that
the ratio of malicious workers ranges from 5% to 30%.We categorize
malicious workers below:
• Blind Mislabelling. A blind mislabelling worker will mislabel all
the emails assigned to her. We collect 6 polluted datasets with
different malicious worker ratios.
• Targeted Mislabelling. There are two subtypes of targeted mis-
labelling workers. First, the worker only intentionally mislabels
one class of emails, either spam as ham or ham as spam. We
collect 24 polluted datasets with different malicious worker ratios.
Second, the worker only mislabels emails with certain features
(words). In the experiment, we choose several popular words, such
as “laptop”, “schedule” and “magazine”, and collect 12 polluted
datasets.
Additionally, we also collect 3 datasets mixing blind and targeted
mislabelling: one blind and spam as ham, one blind and ham as
spam, another blind and mislabelling with certain features. In sum,
45 polluted datasets are collected from mislabelling attacks. The
pollution technique is effective as demonstrated in the final accuracy
of learning model ranging from 15.2% to 51%.
Injection (Dictionary) Attacks. In this attack scenario, as detailed
in Section 2, the attacker has more control over the spam emails
but no control over ham emails, because a honeypot collects spams
from spammers. Particularly, we launch a special injection attack,
i.e., the dictionary attack proposed by Nelson et al. [33] to generate
spams with many injected dictionary words, i.e., spams with crafted
features. The purposes of a dictionary attack are twofold. First, if
legitimate dictionary words are considered as spam features, many
ham emails with such words will be misclassified. Second, if non-
spam words are considered as spam features, the real spam features
might be buried and not selected by the learning model.
Based on the aforementioned dictionary attacks, we collect 50
polluted datasets. The polluted emails are about 0.5% to 4% of the
entire training set, and the final detection accuracy after pollution
ranges from 59.3% to 81.4%. Specifically, in each attack, an attacker
can craft spams based on four parameters: number of clusters (NC),
cluster size (CS) that is the number of emails in a cluster, number
of words (NW ) that is the length of each email, and maximum
deviation of word set between two emails in one cluster (D). In
our experiment, NC is selected randomly between 1 and 26 for
each dataset, CS randomly between 1 and 1000 for each cluster,
NW randomly between 1000 and 2000 for each cluster, and D%
randomly between 10% and 50% for each dataset.
Oracle and Testing Datasets. The samples used for oracle and
testing datasets are further divided into two parts. The first part with
25,870 emails is used as an independent, third dataset (Stest ) for
validation purpose such as measuring the accuracy of the learning
model. All the accuracies reported in this section are using this
dataset.
The second part is used to generate Sor acle , the oracle set. We
feed emails in this part (about 1/20 of the entire dataset) into each
polluted learning model, and include the misclassified emails and
one tenth of the correctly classified emails into Sor acle . Based on dif-
ferent attacks, Sor acle varies, but roughly contains 5K–15K emails.
6.2 Comparing with Learning-based Approaches
In this part of the section, we evaluate two naïve learning-based ap-
proaches and show that they cannot repair a polluted learning model
in our experiment. Specifically, the first approach is to construct a
learning model with only Sor acle , and the second is to improve the
already polluted model trained from a polluted Str aininд by learning
Sor acle incrementally.
First, let us see whether Sor acle can be used to train a learning
model with good accuracy. Because Sor acle varies for different
attacks, we randomly select five Sor acle , train a learning model,
and test the learning model against Stest . The accuracies are all
below 60%. The reason is that Sor acle is a good representation of
misclassified emails, but not correctly classified emails. In sum, the
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Y. Cao et al.
take-away for this experiment is that although Sor acle can be used
to find the polluted data and fix the learning model, Sor acle cannot
be used as a stand-alone dataset for training purposes.
Second, we incrementally learn Sor acle based on the polluted
learning model, i.e., the new training set of the model will be the
polluted dataset plus Sor acle . We then evaluate whether the polluted
learning model can be repaired. We call this naïve approach correc-
tive learning in this paper. Similar to the previous experiment, we
choose five random Sor acle for the experiment. The results show
that the accuracies are all below 70%.
The reason is as follows. Due to the small size of Sor acle , the
correctly labelled samples in Sor acle cannot negate the pollution
effects. Because there is no prior work on this corrective learning
approach and we do not have enough number of additional correctly
labeled samples, we are not sure how to determine the number of
correctly labelled samples to learn. One thing worth noting is that
the number is very challenging to determine: Fewer samples do not
negate the pollution effects, but more will cause overfitting. Further
investigation is out of scope of the paper.
Now from a high level, let us compare corrective learning and
KARMA. We believe that in this specific problem, KARMA is su-
perior to corrective learning due to the following two reasons. (i)
KARMA not only repairs the polluted model, but also helps to block
future pollution attempts, because the administrator can find the
pollution source based on the misclassification cause. As a com-
parison, corrective learning only repairs the model, but not blocks
future attempts. (ii) When the size of Sor acle increases, KARMA
only becomes more effective in repairing the polluted model, but
corrective learning may correct the polluted learning model to the
other extreme, i.e., causing overfitting.
6.3 True Positives and Negatives
In this part of the section, we evaluate the true positives and negatives
of KARMA using all the attack datasets mentioned in Section 6.1.
True positives are defined as the percentage of polluted identified,
and true negative the percentage of unpolluted remained.
The true positives of KARMA against all the datasets are higher
than 98.0% for the lowest, 99.2% for the median, and reaching
99.97% for the highest. The very high true positives prove the ef-
fectiveness of KARMA in identifying polluted samples altered or
injected by adversaries. The rest of less than 1–2% polluted data
is scattered in the training data, and has little impacts on the learn-
ing model as evident by our final detection accuracy, which is very
close to the vanilla (±0.9%) after removing the polluted shown in
Section 6.4.
The true negatives are higher than 85.5% for the lowest, 90.8% for
the median, and reaching 94.3% for the highest. Such true negatives