dlers are a generic mechanism enabling ﬁne-grained control of the
trafﬁc generated by hosts.
5. DESIGN SPACE
The previous section described scenarios where network excep-
tion handlers could be employed to better manage the network ex-
perience. The underlying principle is that network and hosts should
collaborate to better control the performance of the enterprise net-
work. Network exception handlers require information about the
network and hosts, denoted by NetworkState and HostState respec-
tively in the examples. In order to create and maintain this state,
information from the hosts needs to be distributed to the network
devices controlling the network, or vice versa, information from the
network needs to be available at the host. Both approaches have ad-
vantages and disadvantages which we now discuss.
In the network core, information about the current observed state
of the network is available with low latency. However, making the
host state information available in the network, either through in-
ference techniques or through explicit signaling, even at the ﬁrst
hop router appears expensive, if not practically infeasible. Infer-
ring applications in the network is hindered by (i) the increasing
use of encryption, (payload and even TCP-headers might not be
available with IPSEC, thus making deep packet inspection tech-
niques impractical), and (ii) other obfuscation techniques, such as
the widespread overloading of ports [12], a practice which is typ-
ical in enterprise networks. Additionally, network devices cannot
infer other host-speciﬁc context information, like the user that gen-
erated the trafﬁc, the user’s business role, or the application’s busi-
ness value. For example, network devices may be able to detect
CCs using either IP multicast or an overlay. All information is asso-
ciated with a lease, and if the set of CCs should become partitioned,
the information contributed by unreachable CCs will timeout and
be removed from the disconnected CCs.
Network exception handlers are distributed to the hosts, either
exploiting the existing infrastructure available for host management
in the enterprise or utilizing the CCs. The distribution and update
of network exception handlers is thus expected to occur with low
latency. Each host determines the set of network exception handlers
that it is required to use, and then registers the exception condition
of each with the CC. The way in which network exception handlers
are deﬁned means the exception condition is captured within the
Exception() function, and this is parameterized only by Network-
State. This means that a host can easily delegate the evaluation of
the set of Exception() functions it must use to the CC. When the CC
detects that an Exception() has triggered, it informs the host which
then executes Fire() and the registered Handler() locally. This re-
quires that the CC exposes the NetworkState to any host that cur-
rently has active exceptions. This can be optimized in many ways,
including having the hosts register ﬁlters on NetworkState to en-
sure that only relevant state-changes are propagated to the hosts.
De-activating the exceptions occurs locally at the end hosts as part
of the evaluation of the handler.
Each CC should be able to support a large-number of hosts so
that it would be feasible for a single CC, for example, to support
an average building with 500+ hosts. The overhead of checking the
exception conditions is bounded on the total number of unique net-
work exception handlers, rather than on the sum of the number of
exception handlers per host. This implies that if the same exception
handler is installed on all hosts associated with a CC, then the CC
needs only to evaluate the associated Exception() handler once, and
then trigger all connected hosts. Similarly, if any processing or ﬁl-
tering of the NetworkState is necessary, such a process will only be
performed once, irrespective of the number of clients that have reg-
istered the exception; updates will then be distributed to all hosts
with the corresponding exception handler active. As CCs are topo-
logically close in the network to hosts, communication problems
between the corresponding hosts and the CC are expected to be
rare. Similarly, failure of the CC or loss of connectivity to the CC
is easily detectable by hosts, which then would trigger an exception
handler for such a situation, that would, for example, redirect hosts
to an alternative CC.
Fig. 5 depicts the ﬂow of various types of information towards
the CC, and how the CC triggers and forwards information to its
hosts. The network information collection and aggregation is per-
formed at the CCs and the policies enforced at each host.
The two-tier architecture outlined here does not require the net-
work administrator to explicitly understand where exceptions are
being evaluated or enforced. Administrators describe the excep-
tions against a simple model of an object embodying the network
state and an object embodying the per-host state. The network ex-
ception handlers are decomposed into exception detection and a set
of actions to be enacted. We exploit this ﬂexibility to enable a sim-
ple architecture that is similar to many network-wide services that
are already deployed in enterprise networks today. A server, the
CC, performs exception detection, reducing the amount of infor-
mation that needs to ﬂow to each host, and the hosts implement
the actions. This clean separation allows for a scalable and efﬁ-
cient infrastructure that supports network exception handlers. The
following section describes in more detail how the network topol-
ogy information is extracted and aggregated, and how policy can
be implemented at the hosts.
Figure 5: Information ﬂow between the different entities of the
architecture.
that media is being streamed to a host, but devices are agnostic as
to whether the application is destined for a training video applica-
tion or a video conferencing application. An alternative would be
to implement such functionality at the edge routers but as multi-
homing becomes increasingly popular, such solutions appear quite
complicated. For example with laptops connected to both wireless
and wired infrastructure, there is no longer a single edge network
device which mediates access to the network for each host.
In contrast to implementing network exception handlers in the
network, enterprise hosts have plentiful resources available and ac-
cess to context of the network usage, including application names,
process IDs, user IDs, etc. Hosts can further implement complex
policies on local trafﬁc easily, and policies can easily be func-
tions of multiple applications generating trafﬁc to different hosts
over different network interfaces. This enables richer policies to
be speciﬁed through exception handlers. Implementing the action
at the host also implies that “unwanted” trafﬁc potentially never
enters the network. This is advantageous; for example, an admin-
istrator has the ability to deploy network exception handlers that
are ﬁred when users connect over VPN links to the enterprise, and
block a set of non-essential business applications from transmit-
ting. The enterprise would still incur the cost of the trafﬁc should
such a policy be implemented in the network, whereas a host im-
plementation would block the trafﬁc at its source. The challenge of
realizing such policies on hosts however, comes from the fact that
the network is virtually a black box to the host, and distributing all
topology and other network information to each host in a robust
way is expensive.
5.1 A Two-tier Architecture
This motivates us to advocate the implementation of network ex-
ception handlers in a two-tier architecture, similar to how existing
enterprise services, such as directory services, are implemented. A
set of servers are deployed across the network, referred to as cam-
pus controllers (CC). In many enterprise deployments these would
be collocated with servers operating other network-wide services,
such as directory services, e.g., domain controllers supporting Ac-
tive Directory. As an extreme, the CC functionality could be inte-
grated with the functionality of the edge router. Each host authen-
ticates with a CC nearby in the network, in the same way that the
host authenticates and connects to an Active Directory domain con-
troller. Each CC maintains an instance of the NetworkState object,
representing the annotated network topology. In the next section,
we describe in more detail how this NetworkState information is
maintained. Each CC is responsible for interacting with a set of
routers using for example SNMP, and aggregating the locally gath-
ered information. It then disseminates this information to all other
6. ENABLING EXCEPTION HANDLERS
In the previous section, we outlined a two-tier architecture for
supporting network exception handlers in enterprise networks. In
order for this architecture to be feasible, we need to be able to col-
lect and synthesize the dynamic network topology information. We
also need to be able to support trafﬁc shaping and similar function-
ality on hosts.
6.1 Monitoring the network topology
Network exception handlers require dynamic network informa-
tion. Enabling a diverse set of exception conditions requires a cor-
respondingly rich dataset. We believe that a complete, dynamic
link-level network topology, annotated with link capacities, costs
and loads is both feasible and also necessary to support the full
functionality that network exception handlers can offer.
Data concerning link capacities and costs is already maintained
by an enterprise, and used by the network operators for billing,
auditing and capacity planning purposes. This information is typi-
cally stored in a database and accessed via, e.g., web interfaces in
our enterprise. The information changes slowly over time, when
contracts are renegotiated or new links are provisioned. Therefore,
distributing a copy of this information to the CCs is feasible and
allows the network topology to be augmented with link capacities
and costs.
Link load data is obviously far more dynamic than link capacity
and cost information. However, this data is also available using ex-
isting mechanisms such as SNMP. Indeed, in most enterprises this
information is already required by existing network management
systems to enable capacity planning. It is thus feasible for CCs that
are topologically close to such monitoring devices to extract this
information, and then distribute it to all other CCs.
In contrast, the complete, dynamic link-level network topology
is typically not currently available. We thus discuss here how to
extract this topology information efﬁciently in the context of using
OSPF [13]. We believe that OSPF is the most common routing pro-
tocol used in enterprise networks, including our own. In general,
the techniques that we describe in the following sections should be
easy to use with other link-state protocols, such as IS-IS [14, 5].
Collection
There are several methodologies for collecting OSPF data, includ-
ing commercial solutions, such as those provided by Iptivia.5 Shaikh
and Greenberg [18] propose four basic methodologies to extract
OSPF data in order to infer the network topology for a large ISP
network. The ﬁrst two involve forming a partial or full adjacency
with a router, the third describes the host-mode where the IP mul-
ticast group used to distribute OSPF data in a broadcast network
is monitored, and the fourth is the wire-tap method where a link
between two routers is monitored.
In our two-tier architecture we assume that the CCs will collect
the OSPF data. In a large enterprise network, the network will be
conﬁgured into several OSPF areas, and it is necessary to extract
the OSPF data for each area. To increase resilience, it is best to
have multiple CCs per area. The wire-tap approach requires that
CCs have physically proximity to the link being monitored, which
may not always be feasible. The adjacency approach allows the
CCs to act as ‘dummy’ routers that do not advertise any subnets but
do participate in OSPF routing. This approach also has the advan-
tage, that using GRE [9] tunnels, an adjacency between a CC and
an arbitrary router in the network can be established, which per-
mits OSPF data collection from routers when physical proximity is
5http://www.iptivia.com/
LSDB MERGING:
1. Insert each non-summary LSA from the per-area LSDBs into the
merged LSDB, tracking all preﬁxes for which such non-summary LSAs
exist.
2. For each summary LSA, extract the single preﬁx it contains and retain
it if there is no non-summary LSA for this preﬁx in the merged LSDB.
If a summary LSA has already been retained for this preﬁx, then prefer
according to the preference relationship on LSA types 1 < 2 < 5 < 3 < 4.
3. Insert all retained summary LSAs into the network LSDB.
4. For all type-1 LSAs that are generated by each router that exists in
multiple areas, construct and insert a new type-1 LSA, and remove type-
1 LSAs with the same lsid.
Figure 6: OSPF topology synthesis process
not possible. We have successfully used the wiretap method on a
GRE tunnel conﬁgured to make an adjacency between two routers
in the network visible to us. Host-mode would also be feasible in
a network where IP multicast is supported, if the CCs support the
necessary protocols allowing them to join IP multicast groups, and
routers are suitably conﬁgured to allow them to join the relevant
multicast group.
Note that there are security implications to consider when not
using either wire-tap or host-mode since standard router OSPF im-
plementations do not permit a router to protect itself against bad
routes inserted by hosts with which it is conﬁgured to form an ad-
jacency. One of the advantages of the two-tier architecture is that
the CCs are managed dedicated servers. Allowing these servers to
act as dummy routers creates a smaller attack plane compared to
allowing arbitrary hosts to act as dummy routers.
Collecting a live feed of OSPF data from each area and shad-
owing the current link-state database (LSDB) being maintained by
routers in the area is therefore quite feasible.
Synthesis
Having collected the per-area LSDBs, they must be combined to
synthesize a global topology. While this may be conceptually straight-
forward, practical implementation details can be complicated. To
our knowledge, synthesis of global OSPF topology through merg-
ing of area LSDBs has not been described before and thus we pro-
vide here a detailed description of the procedure we followed.
The goal is to merge the LSDBs, each of which is simply a
collection of link-state advertisements (LSAs) describing the links
connected to a given link in the network. The complication arises
from the fact that a “link” in OSPF can be a point-to-point link be-
tween two interfaces, a subnet being advertised by a router, a shared
segment connecting many routers, or a subnet from a different area
being summarized into this one. To account for this complication,
we give preference to speciﬁc non-summary LSA types compared
to the less speciﬁc summarization LSAs. Further, we must also dis-
ambiguate the LSAs describing an individual router in cases where
that router exists in multiple areas. For completeness, given a set
of per-area LSDBs to merge, Fig. 6 gives a detailed OSPF-speciﬁc
description of the process.
After merging all LSDBs, the result is a single LSDB represent-
ing the entire network visible from OSPF. To gain a truly complete
picture of the network’s topology also requires the use of out-of-
band data such as the router conﬁguration ﬁles. Parsing conﬁgu-
ration ﬁles reveals information not available through any particu-
lar routing protocol such as static routes and MPLS tunnels. This
process can be automated by incorporating it into the conﬁgura-
tion management system that any large network runs to manage
its device conﬁgurations: when a device conﬁguration is changed,
the topology inference host for that part of the network should be
automatically notiﬁed so that it can re-parse the relevant conﬁgu-
rations. Other sources of routing data should also be considered
although we note that, as discussed in Section 2, enterprise net-
works typically mandate use of proxies for external Internet access
and use default routes to handle trafﬁc destined outside the enter-
prise. Thus, we are not required either to infer topology from BGP
data or to handle the high rate of updates usually observed in such
data.
6.2 Control mechanisms at the host
Policy enforcement takes place at the hosts, with the exception
handlers specifying actions to take when an exception is triggered.
In order to implement network exception handlers at the host, ac-
cess to local host state, e.g., the set of running processes, the ap-
plication names, and the ability to shape the generated trafﬁc are
both required. Modern operating systems provide extensive support
that enables this functionality. Event tracing frameworks, such as