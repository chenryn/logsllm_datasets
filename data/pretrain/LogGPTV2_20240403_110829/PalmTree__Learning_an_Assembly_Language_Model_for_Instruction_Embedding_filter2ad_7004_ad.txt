shows how we adopt different instruction embedding models to
Gemini. Since Gemini takes a feature vector for each basic block,
we use mean pooling to generate basic block embeddings based on
embeddings of the instructions in the corresponding basic block.
The architectures of our modified model and the original model
are both shown in Figure 9. We also included its original basic
block features as an additional baseline (denoted as “Gemini”) for
comparison.
(3) Despite that the testing set is considerably different from the
training set, PalmTree can still perform reasonably well and
beat the remaining schemes, demonstrating that PalmTree
can substantially boost the generalizability of downstream
tasks.
(4) All the three pre-training tasks contribute to the final model
(PalmTree) for Gemini. However, both PalmTree-M and
PalmTree-MC do not show obvious advantages over other
baselines, signifying that only the complete PalmTree with
the three training tasks can generate better embeddings than
previous approaches in this downstream task.
Figure 10: ROC curves of Gemini
The accuracy of the original Gemini is reported to be very high
(with an AUC of 0.971). However, this might be due to overfitting,
since the training and testing sets are from OpenSSL compiled by
the same compiler Clang. To really evaluate the generalizability (i.e.,
the ability to adapt to previously unseen data) of the trained models
under different inputs, we use binutils-2.26, binutils-2.30,
and coreutils-8.30 compiled by Clang as training set (237 bi-
naries in total), and used openssl-1.1.0h, openssl-1.0.1, and
glibc-2.29.1 compiled by GCC as testing set (14 binaries). In
other words, the training and testing sets are completely different
and the compilers are different too.
Table 4: AUC values of Gemini
Model
one-hot
Instruction2Vec
word2vec
Asm2Vec
AUC Model
0.745 Gemini
0.738
0.826
0.823
PalmTree-M
PalmTree-MC
PalmTree
AUC
0.866
0.864
0.866
0.921
Table 4 gives the AUC values of Gemini when different models
are used to generate its input. Figure 10 shows the ROC curves
of Gemini when different instruction embedding models are used.
Based on Table 4, we can make the following observations:
(1) Although the original paper [40] reported very encouraging
performance of Gemini, we can observe that the original
Gemini model does not generalize very well to completely
new testing data.
(2) The manually designed embedding schemes, Instruction2Vec
and one-hot vector, perform poorly, signifying that manually
selected features might be only suitable for specific tasks.
Figure 11: Instruction embedding models and EKLAVYA
Function Type Signature Inference. Function type signature
4.4.2
inference is a task of inferring the number and primitive types of
the arguments of a function. To evaluate the quality of instruction
embeddings in this task, we select EKLAVYA, an approach proposed
by Chua et al. [5]. It is based on a multi-layer GRU (Gated Recurrent
Unit) network and uses word2vec as the instruction embedding
method. According to the original paper, word2vec was pre-trained
with the whole training dataset. Then, they trained a GRU network
to infer function type signatures.
In this evaluation, we test the performances of different types
of embeddings using EKLAVYA as the downstream application.
Since the original model is not an end-to-end model, we do not
need an embedding layer between instruction embeddings and the
GRU network. We replaced the original word2vec in EKLAVYA
with one-hot encoding, Instruction2Vec, Asm2Vec, PalmTree-M,
PalmTree-MC, and PalmTree, as shown in Figure 11. Similarly, in
order to evaluate the generalizability of the trained downstream
models, we used very different training and testing sets (the same
datasets described in Section 4.4.1).
Table 5: Accuracy and Standard Deviation of EKLAVYA
Model
one-hot
Instruction2Vec
word2vec
Asm2Vec
PalmTree-M
PalmTree-MC
PalmTree
Accuracy
Standard Deviation
0.309
0.311
0.856
0.904
0.929
0.943
0.946
0.0338
0.0407
0.0884
0.0686
0.0554
0.0476
0.0475
0.00.20.40.60.81.0False positive rate0.20.40.60.81.0True positive rateone-hotInstruction2Vecword2vecAsm2VecPᴀʟᴍTʀᴇᴇ-MPᴀʟᴍTʀᴇᴇ-MCPᴀʟᴍTʀᴇᴇGeminiPALMTREE and other Instruction Embedding Modelsmov rbp, rdiOutput: Function type signituresword2vecGRUGRUGRUSession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3245heap, stack, and other. It feeds instruction raw bytes as input into a
multi-layer LSTM network to generate instruction embeddings. It
then feeds the generated instruction representations into another
multi-layer bi-directional LSTM network, which is supposed to cap-
ture the dependency between instructions and eventually predict
the memory access regions.
In our experiment, we use different kinds of instruction em-
beddings to replace the original instruction embedding generation
model in DeepVSA. We use the original training and testing datasets
of DeepVSA and compare prediction accuracy of different kinds of
embeddings. The original datasets contain raw bytes only, thus we
need to disassemble these raw bytes. After that we tokenize and
encode these disassembled instructions for training and testing. We
add an embedding layer before the LSTM network to further adjust
instruction embeddings, as shown in Figure 13.
We use part of the dataset provided by the authors of Deep-
VSA. The whole dataset provided by the authors has 13.8 million
instructions for training and 10.1 million for testing. Our dataset
has 9.6 million instructions for training and 4.8 million for testing,
due to the disassembly time costs. As explained in their paper [14],
their dataset also used Clang and GCC as compilers and had no
overlapping instructions between the training and testing datasets.
Figure 14: Loss value of DeepVSA during training
Table 6 lists the experimental results. We use Precision (P), Recall
(R), and F1 scores to measure the performance. Figure 14 depicts the
loss values of DeepVSA during training, when different instruction
embedding schemes are used as its input. From these results, we
have the following observations:
(1) PalmTree has visibly better results than the original Deep-
VSA and the other baselines in Global and Heap, and has
slightly better results in Stack and Other since other base-
lines also have scores greater than 0.9.
(2) The three training tasks of PalmTree indeed contribute to
the final result. It indicates that PalmTree indeed captures
the data flows between instructions. In comparison, the other
instruction embedding models are unable to capture data
dependency information very well.
(3) PalmTree converged faster than original DeepVSA (see Fig-
ure 14), indicating that instruction embedding model can
accelerate the training phase of downstream tasks.
Figure 12: Accuracy of EKLAVYA
Table 5 and Figure 12 presents the accuracy of EKLAVYA on the
testing dataset. Figure 15, and Figure 16 in the Appendix shows the
loss value and accuracy of EKLAVYA during training and testing.
From the results we can make the following observations:
(1) PalmTree and Asm2Vec can achieve higher accuracy than
word2vec, which is the original choice of EKLAVYA.
(2) PalmTree has the best accuracy on the testing dataset, demon-
strating that EKLAVYA when fed with PalmTree as instruc-
tion embeddings can achieve the best generalizability. More-
over, CWP contributes more (see PalmTree-MC), which im-
plies that control-flow information plays a more significant
role in EKLAVYA.
(3) Instruction2Vec performs very poorly in this evaluation, sig-
nifying that, when not done correctly, manual feature selec-
tion may disturb and mislead a downstream model.
(4) The poor results of one-hot encoding show that a good in-
struction embedding model is indeed necessary. At least in
this task, it is very difficult for the deep neural network to
learn instruction semantic through end-to-end training.
Figure 13: Instruction embedding models and the down-
stream model DeepVSA
4.4.3 Value Set Analysis. DeepVSA [14] makes use of a hierarchi-
cal LSTM network to conduct a coarse-grained value set analysis,
which characterizes memory references into regions like global,
one-hotInstruction2Vecword2vecAsm2VecPᴀʟᴍTʀᴇᴇ-MPᴀʟᴍTʀᴇᴇ-MCPᴀʟᴍTʀᴇᴇ0.40.60.81.0AccuracyPALMTREE and other Instruction Embedding Modelsmov rbp, rdiInstruction EmbeddingsEmbedding LayerEmbedding LayerLSTMLSTMLSTMLSTMLTSMLTSMLSTMLTSMLTSM0255075100125150Iterations0.00.10.20.30.40.50.60.7Loss valueone-hotInstruction2Vecword2vecAsm2VecPᴀʟᴍTʀᴇᴇ-MPᴀʟᴍTʀᴇᴇ-MCPᴀʟᴍTʀᴇᴇDeepVSASession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3246Table 6: Results of DeepVSA
Embeddings
one-hot
Instruction2Vec
word2vec
Asm2Vec
DeepVSA
PalmTree-M
PalmTree-MC
PalmTree
P
0.453
0.595
0.147
0.482
0.961
0.845
0.910
0.912
Global
R
0.670
0.726
0.535
0.557
0.738
0.732
0.755
0.805
F1
0.540
0.654
0.230
0.517
0.835
0.784
0.825
0.855
P
0.507
0.512
0.435
0.410
0.589
0.572
0.758
0.755
Heap
R
0.716
0.633
0.595
0.320
0.580
0.625
0.675
0.678
F1
0.594
0.566
0.503
0.359
0.584
0.597
0.714
0.714
P
0.959
0.932
0.802
0.928
0.974
0.963
0.965
0.974
Stack
R
0.866
0.898
0.420
0.894
0.917
0.909
0.897
0.929
F1
0.910
0.914
0.776
0.911
0.944
0.935
0.929
0.950
P
0.953
0.948
0.889
0.933
0.943
0.956
0.958
0.959
Other
R
0.965
0.946
0.863
0.964
0.976
0.969
0.988
0.983
F1
0.959
0.947
0.876
0.948
0.959
0.962
0.972
0.971
PalmTree outperforms the other instruction embedding ap-
proaches in each extrinsic evaluation. Also, PalmTree can speed
up training and further improve downstream models by provid-
ing high-quality instruction embeddings. In contrast, word2vec
and Instruction2Vec perform poorly in all the three downstream
tasks, showing that the poor quality of an instruction embedding
will adversely affect the overall performance of downstream ap-
plications.
4.5 Runtime Efficiency
In this section, we conduct an experiment to evaluate runtime
efficiencies of PalmTree and baseline approaches. First, we test the
runtime efficiencies of different instruction embedding approaches.
Second, we test the runtime efficiency of PalmTree when having
different embedding sizes. We use 64, 128, 256, and 512 as embedding
sizes, while 128 is the default setting. In the transformer encoder
of PalmTree, the width of each feed-forward hidden layer is fixed
and related to the size of the final output layer, which is 4 times of