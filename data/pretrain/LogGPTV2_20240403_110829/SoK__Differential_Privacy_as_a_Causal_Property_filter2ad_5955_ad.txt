tribute from affecting another’s, for example, preventing a
mother’s genetic condition from affecting her child’s genetic
condition. This is not the case since D1, D2, D3 represent
the data points provided as inputs to the algorithm and
not the actual attributes themselves. One could model the
actual attributes, such as genetics itself, as random variables
R1, R2, R3 where Di := Ri for all i and allow Ri to affect
Rj without changing how intervening on the Dis works. For
example, progA might be called in the following context:
def progstatus(R1, R3) :
R2 := R1
D1 := R1
D2 := R2
D3 := R3
progA(D1, D2, D3)
which does not say how the inputs R1 or R3 are set but does
model that R2 is assigned R1. We can graphically represent
these relationships as a graphical model, similar to the one
in Figure 1 with n − 2 = 1 and an intermediate variable
D representing the database put between the data points and
the output. Note that while D1 and D2 are associated, equal
in fact, neither causes the other and they can be changed
independently of one another, which can be seen from neither
being downstream from the other.
To make the above intuitions about causation formal, we
use structural equation models (SEMs). An SEM M =
(cid:104)Ven,Vex,E(cid:105) includes a set of variables partitioned into en-
dogenous (or dependent) variables Ven and background (or
exogenous, or independent) variables Vex. You can think of
the endogenous variables as being those assigned values by the
programs above and the background variables as being those
provided as inputs to the programs. M also includes a set E of
structural equations, corresponding to the assignments. Each
endogenous variable X has a structural equation X := FX ((cid:126)Y )
where FX is a possibly randomized function and (cid:126)Y is a list
st ,P [Di=di] = 0.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
361
of other variables, modeling the direct causes of X. To avoid
circularity, (cid:126)Y may not include X. We call the variables (cid:126)Y the
parents of X, denoted as pa(X).
We limit ourselves to recursive SEMs, those in which the
variables may be ordered such that all background variables
come before all endogenous variables and no variable has a
parent that comes before it in the ordering. We may view such
SEMs as similar to a program where the background variables
are inputs to the program and the ordering determines the order
of assignment statements in the program. We can make this
precise by computing the values of endogenous variables from
the values of the background variables using a method similar
to assigning a semantics to a program.
Returning to our example, let MA
The only difference is that, rather then a single value, the
inputs are assigned probability distributions over values, which
allows us to talk about the probabilities of the endogenous
variables taking on a value. Let a probabilistic SEM (cid:104)M,P(cid:105)
be an SEM M with a probability distribution P over its
background variables. We can raise the structural equations
(assignments) to work over P instead of a concrete assignment
of values. (Appendix C provides details.)
Finally, to deﬁne causation, let M be an SEM, Z be an
endogenous variable of M, and z be a value that Z can take
on. Pearl deﬁnes the sub-model M[Z:=z] to be the SEM that
results from replacing the equation Z := FZ( (cid:126)Z) in E of M
with the equation Z := z. You can think of this as using
GDB to assign a value to a variable or as aspect-oriented
programming jumping into a function to alter a variable. The
sub-model M[Z:=z] shows the effect of setting Z to z. Let
PrM,P [Y =y | do(Z:=z)] be PrM[Z:=z],P [Y =y]. This is well
deﬁned even when PrM,P [Z=z] = 0 as long as z is within
in the range of values Z that Z can take on.
st be an SEM representing
progstatus and P be the naturally occurring distribution of
data points. PrMA
st ,P [O=o] is the probability of the algo-
rithm’s output being o under P and coin ﬂips internal to
A. PrMA
st ,P [O=o | Di=pos] is that probability conditioned
st ,P [O=o | do(D1=pos)] is
upon seeing D1 = pos. PrMA
that probability given an intervention setting the value of D1
to pos, which is PrMA
st [D1:=pos] is
the program with the line assigning R1 to D1 replaced with
st ,P [O=o | do(D1=pos)] depends upon how
D1 := pos. PrMA
the intervention on D1 will ﬂow downstream to O.
This probability differs from the conditional probability in
that setting D1 to pos provides no information about Dj for
j (cid:54)= 1, whereas if D1 and Dj are associated, then seeing
the value D1 does provide information about Dj. Intuitively,
this lack of information is because the artiﬁcial setting of
D1 to pos has no causal inﬂuence on Dj due to the data
points not affecting one another and the artiﬁcial setting, by
being artiﬁcial, tells us nothing about the associations found
in the naturally occurring world. On the other hand, artiﬁcially
setting the attribute itself R1 to pos will provide information
about D2 since R1 has an effect on D2 in addition to D1.
st ,P [O=o | do(Di=di)] is
A second difference is that PrMA
deﬁned even when PrMA
Importantly, interventions on a data point Di do not model
modifying the attributes they record nor affect other inputs.
st [Di:=pos],P [O=o]. MA
Instead, interventions on Di model changing the values pro-
vided as inputs to the algorithm, which can be changed without
affecting the attributes or other inputs. This corresponds to an
atomicity property: the inputs Di are causally isolated from
one another and they can be intervened upon separately.
Making the distinction between the inputs Di and the
attributes Ri might seem nitpicky, but it is key to under-
standing DP. Recall that its motivation is to make people
comfortable with truthfully sharing data instead of withholding
it or lying, which is an acknowledgment that the inputs people
provide might not be the same as the attributes they describe.
Furthermore, that changing inputs do not change attributes or
other inputs is a reﬂection of how the program works. It is
not an implicit or hidden assumption of independence; it is a
fact about the program analyzed.
V. DIFFERENTIAL PRIVACY AS CAUSATION
Due to differential privacy’s behavior on associated inputs
and its requirement of considering zero-probability database
values, DP is not a straightforward property about the inde-
pendence or the degree of association of the database and
the algorithm’s output. The would-be conditioning upon zero-
probability values corresponds to a form of counterfactual
reasoning asking what the algorithm would have performed
had the database taken on a particular value that it might
never actually take on. Experiments with such counterfactuals,
which may never naturally occur, form the core of causation.
The behavior of DP on associated inputs corresponds to the
atomicity property found in causal reasoning, that one can
change the value of an input without changing the values of
other inputs. With these motivations, we will show that DP
is equivalent to a causal property that makes the change in a
single data point explicit.
A. With the Whole Database
We ﬁrst show an equivalence between DP and a causal
property on the whole database to echo Strong Adversary
Differential Privacy (Def. 2). To draw out the parallels between
the associative and causal properties, we quantify over all
populations as we did in Deﬁnition 2, but as we will see,
doing so is not necessary.
Let MA be an SEM modeling a slightly modiﬁed version
of progstatus that lacks the ﬁrst assignment and treats all of
any ﬁxed number of attributes Ri as inputs (i.e., as exogenous
variables) with Di := Ri. (Appendix C provides details.) We
could instead use a version of MA that also accounts for
Di possibly being assigned a value other than Ri to model
withholding an attribute’s actual value. While the proofs would
become more complex, the results would remain the same
since we only intervene on the Di and not the Ri.
Deﬁnition 5 (Universal Whole Database Intervention D.P.).
A randomized algorithm A is -differentially private as uni-
versal intervention on the whole database if for all population
distributions P, for all i, for all data points d1, ..., dn in Dn
i in D, and for all output values o,
and d(cid:48)
PrMA,P [O=o | do(D1=d1, ..., Dn=dn)]
≤ e ∗ PrMA,P [O=o | do(D1=d1, ..., Di=d(cid:48)
i, ..., Dn=dn)]
where O := A(D) and D := (cid:104)D1, ..., Dn(cid:105).
Proposition 2. Deﬁnitions 1 and 5 are equivalent.
Proof. Pearl’s Property 1 says that conditioning upon all the
parents of a variable and causally intervening upon them all
yields the same probability [42, p. 24]. Intuitively, this is for
the same reason that Strong Adversary Differential Privacy is
equivalent to DP: it blocks other paths of inﬂuence from one
data point to the output via another data point by ﬁxing all
the data points.
We can apply Property 1 since all
the Dis are be-
ing intervened upon and they make up all the parents of
D. We can apply it again on D and O. We then get
that PrMA,P [O=o | do(D1=d1, ..., Dn=dn)]
to
PrMA,P [O=o | D1=d1, ..., Dn=dn], that is to Strong Ad-
versary Differential Privacy, which we already know to be
equivalent to DP by Proposition 1.
is equal
Notice that this causal property is simpler than the as-
sociative one in that it does not need qualiﬁcations around
zero-probability data points because we can causally ﬁx data
points to values with zero probability. In fact, the population
distribution P did not matter at all since intervening upon all
the data points makes it irrelevant, intuitively by overwriting
it. For this reason, we could instead look at any population,
such as the naturally occurring one (or even elide it from the
deﬁnition altogether, as in Deﬁnition 1, if we are not too picky
about formalism). Next, we state such a simpliﬁed deﬁnition.
Deﬁnition 6 (Whole Database Intervention D.P.). Given a
population distribution P, a randomized algorithm A is -
differentially private as intervention on the whole database for
P if for all i, for all data points d1, ..., dn in Dn and d(cid:48)
i in
D, and for all output values o,
PrMA,P [O=o | do(D1=d1, ..., Dn=dn)]
≤ e ∗ PrMA,P [O=o | do(D1=d1, ..., Di=d(cid:48)
where O := A(D) and D := (cid:104)D1, ..., Dn(cid:105).
Proposition 3. Deﬁnitions 1 and 6 are equivalent.
Proof. The proof follows in the same manner as Proposition 2
since that proof applies to all population distributions P.
i, ..., Dn=dn)]
B. With a Single Data Point
Deﬁnitions 5 and 6, by ﬁxing every data point, do not
capture the local nature of the decision facing a single potential
survey participant. We can deﬁne a notion similar to DP that
uses a causal intervention on a single data point as follows:
Deﬁnition 7 (Data-point Intervention D.P.). Given a popula-
tion P, a randomized algorithm A is -differentially private
as intervention on a data point for P if for all i, for all data
points di and d(cid:48)
PrMA,P [O=o | do(Di=di)] ≤ e PrMA,P [O=o | do(Di=d(cid:48)
i)]
where O := A(D) and D := (cid:104)D1, ..., Dn(cid:105).
i in D, and for all output values o,
This deﬁnition is strictly weaker than DP. The reason is
similar to why we had to quantify over all distributions P
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
362
with Strong Adversary Differential Privacy. In both cases,
we can give a counterexample with a population P that
hides the effects of a possible value of the data point by
assigning the value a probability of zero. For the associative
deﬁnition,
the counterexample involves only a single data
point (Appendix B). However, for this causal deﬁnition, the
counterexample has to have two data points. The reason is that,
since the do operation acts on a single data point at a time, it
can ﬂush out the effects of a single zero-probability value but
not the interactions between two zero-probability values.
Proposition 4. Deﬁnition 1 implies Deﬁnition 7, but not the
other way around.
Proof. W.l.o.g., assume i = n. Assume Deﬁnition 1 holds:
PrA[A((cid:104)d1, .. ., dn−1, dn(cid:105))=o]
≤ e ∗ PrA[A((cid:104)d1, ..., dn−1, d(cid:48)
n(cid:105))=o]
n in D. This implies
n(cid:105))=o]
i=1 Di=di
i=1 Di=di
for all o in O, (cid:104)d1, ..., dn(cid:105) in Dn, and d(cid:48)
that for any P,
PrP(cid:2)∧n−1
≤ e ∗ PrP(cid:2)∧n−1
(cid:88)
PrP(cid:2)∧n−1
for all o in O, d1, ..., dn in Dn, and d(cid:48)
≤(cid:88)
e ∗ PrP(cid:2)∧n−1
(cid:88)
PrP(cid:2)∧n−1
≤ e(cid:88)
PrP(cid:2)∧n−1
(cid:3) ∗ PrA[A((cid:104)d1, ..., dn−1, dn(cid:105))=o]
(cid:3) ∗ PrA[A((cid:104)d1, ..., dn−1, d(cid:48)
(cid:3) ∗ PrA[A((cid:104)d1, . . . , dn−1, dn(cid:105))=o]
n in D. Thus,
(cid:3) ∗ PrA[A((cid:104)d1, . . . , dn−1, d(cid:48)
(cid:3) ∗ PrA[A((cid:104)d1, . . . , dn−1, dn(cid:105))=o]
(cid:3) ∗ PrA[A((cid:104)d1, . . . , dn−1, d(cid:48)
(cid:104)d1,. . . ,dn−1(cid:105)∈Dn−1
i=1 Di=di
(cid:104)d1,. . . ,dn−1(cid:105)∈Dn−1
(cid:104)d1,. . . ,dn−1(cid:105)∈Dn−1
i=1 Di=di
i=1 Di=di
i=1 Di=di
(cid:104)d1,. . . ,dn−1(cid:105)∈Dn−1
n(cid:105))=o]
PrMA,P [O=o | do(Dn=dn)] ≤ e ∗ PrMA,P [O=o | do(Dn=d(cid:48)
where the last line follows from Lemma 2 in Appendix C.
n(cid:105))=o]
Deﬁnition 7 is, however, weaker than DP. Consider the case
of a database holding two data points whose value could be 0,
1, or 2. Suppose the population P is such that PrP [D1=2] = 0
and PrP [D2=2] = 0. Consider an algorithm A such that
PrA[A((cid:104)2, 2(cid:105))=1] = 0
PrA[A((cid:104)2, 2(cid:105))=0] = 1
PrA[A((cid:104)d1, d2(cid:105))=0] = 1/2 PrA[A((cid:104)d1, d2(cid:105))=1] = 1/2
when d1 (cid:54)= 2 or d2 (cid:54)= 2. The algorithm does not satisfy
Deﬁnition 1 due to its behavior when both of the inputs are
2. However, using Lemma 2 in Appendix C,
PrMA,P [O=o | do(D1=d(cid:48)
for all o and d(cid:48)
1 since PrP [D2=2] = 0. A similar result holds
switching the roles of D1 and D2. Thus, the algorithm satisﬁes
Deﬁnition 7 for P but not Deﬁnition 1.
1)] = 1/2