If you need a list of numbers between 1 and 5 and don’t want to hardcode them, you
can use SELECT generate_series(1, 5); Numbers are helpful, but those functions
also work with temporal data. When using temporal data be aware though that you need
to specify both start and end parameter, as there is no sensible default for either. Let’s
put this to practical use. The readings in our example data end in the middle of 2020.
Reports based on that would prematurely end if they are for a whole year as shown
below:
SELECT strftime(day, '%Y-%m') AS month, avg(kwh)
FROM v_power_per_day WHERE year(day) = 2020
GROUP BY ALL ORDER BY month;
The result will look like the output below:
┌─────────┬────────────────────┐
│ month │ avg(kwh) │
│ varchar │ double │
├─────────┼────────────────────┤
│ 2020-01 │ 222.13169014084497 │
│ 2020-02 │ 133.52356321839076 │
│ 2020-03 │ 207.86670454545438 │
│ 2020-04 │ 309.7838888888888 │
│ 2020-05 │ 349.5753763440861 │
│ 2020-06 │ 337.80820512820515 │
└─────────┴────────────────────┘
If you are tasked to create a chart you might find yourself in a situation in which you
need to think about how to represent the future months. Here’s one way to use the
range() function to cover a whole year and indicate missing values as 0:
© Manning Publications Co. To comment go to liveBook
109
Listing 4.31 Using a range of dates as driving table
WITH full_year AS (
SELECT generate_series AS day
FROM generate_series( -- #1
'2020-01-01'::date,
'2020-12-31'::date, INTERVAL '1 day')
)
SELECT strftime(full_year.day, '%Y-%m') AS month,
avg(kWh) FILTER (kWh IS NOT NULL) AS actual
FROM full_year -- #2
LEFT OUTER JOIN v_power_per_day per_day -- #3
ON per_day.day = full_year.day
GROUP BY ALL ORDER BY month;
#1 A range defined from the first up to the last day of the year, in an interval of 1 day
#2 Use the output of the table function in the FROM clause as driving table
#3 Outer join the values of interest
The result is now a report for a full year, which sadly lacks values after June 2020:
┌─────────┬────────────────────┐
│ month │ actual │
│ varchar │ double │
├─────────┼────────────────────┤
│ 2020-01 │ 222.13169014084508 │
│ 2020-02 │ 133.52356321839076 │
│ 2020-03 │ 207.86670454545455 │
│ 2020-04 │ 309.7838888888888 │
│ 2020-05 │ 349.57537634408607 │
│ 2020-06 │ 337.80820512820515 │
│ 2020-07 │ │
│ 2020-08 │ │
│ 2020-09 │ │
│ 2020-10 │ │
│ 2020-11 │ │
│ 2020-12 │ │
├─────────┴────────────────────┤
│ 12 rows 2 columns │
└──────────────────────────────┘
Taking this idea one step further would be using the value of the same month in the
previous year to forecast the production value. For that you would have to join
v_power_per_day a second time using an offset of one year:
© Manning Publications Co. To comment go to liveBook
110
Listing 4.32 Projecting past data into the future
WITH full_year AS (
SELECT generate_series AS day
FROM generate_series(
'2020-01-01'::date,
'2020-12-31'::date, INTERVAL '1 day')
)
SELECT strftime(full_year.day, '%Y-%m') AS month,
avg(present.kWh) FILTER (present.kWh IS NOT NULL) AS actual,
avg(past.kWh) FILTER (past.kWh IS NOT NULL) AS forecast,
FROM full_year
LEFT OUTER JOIN v_power_per_day present
ON present.day = full_year.day
LEFT OUTER JOIN v_power_per_day past -- #1
ON past.day = full_year.day - INTERVAL '1 year'
GROUP BY ALL ORDER BY month;
#1 Joining power per day a second time but subtracting a year from the values of the generated series
The result is much more pleasant and happens to give a comparison between this year
and last year essentially for free:
┌─────────┬────────────────────┬────────────────────┐
│ month │ actual │ forecast │
│ varchar │ double │ double │
├─────────┼────────────────────┼────────────────────┤
│ 2020-01 │ 222.13169014084505 │ 161.59319248826304 │
│ 2020-02 │ 133.5235632183909 │ 111.07298850574716 │
│ 2020-03 │ 207.8667045454546 │ 150.65231060606064 │
│ 2020-04 │ 309.78388888888895 │ 316.1782222222224 │
│ 2020-05 │ 349.57537634408595 │ 325.36881720430125 │
│ 2020-06 │ 337.8082051282051 │ 351.60691056910514 │
│ 2020-07 │ │ 334.32311827956994 │
│ 2020-08 │ │ 314.928817204301 │
│ 2020-09 │ │ 289.6049999999999 │
│ 2020-10 │ │ 253.82935483870958 │
│ 2020-11 │ │ 191.3843333333334 │
│ 2020-12 │ │ 164.88628205128202 │
├─────────┴────────────────────┴────────────────────┤
│ 12 rows 3 columns │
└───────────────────────────────────────────────────┘
© Manning Publications Co. To comment go to liveBook
111
4.10 Using LATERAL joins
In section Section 43 we learned about correlated and uncorrelated sub-queries. Listing
4.5 demonstrated how an uncorrelated sub-query can be joined once with the outer
query. From a performance point of view that might be beneficial, as the sub-query
needs to be only evaluated once and the join is then performed for each row of the other
table against the memorized values.
Sometimes, however, you precisely want to evaluate the inner query for each value of
an outer query. This is where the LATERAL JOIN comes into play. You can think of it as
the inner block of a for-loop and the outer query being the control structure.
Unnesting arrays, fanning out data and similar tasks can be dealt by using LATERAL.
Assume you are interested in the intensity of the sun, how much of its energy reaches
your place at certain hours of the day, past or future. Open Meteo offers a free API that
provides a broad range of weather data, including the so-called Global Horizontal
Irradiance (GHI). That is the total amount of short-wave radiation received from above
by a surface horizontal to the ground. This value is of particular interest to photovoltaic
installations and is measured in W/m². Their API generates a JSON object that contains
two individual arrays, one with the timestamps, one with the selected values. The latter
array is the array of interest, we want to retrieve specific values for some given facts.
© Manning Publications Co. To comment go to liveBook
112
Listing 4.33 ch04/ghi_past_and_future.json, a JSON structure with past and forecast data for GHI,
from Open Meteo
{
"latitude": 50.78,
"longitude": 6.0799994,
"utc_offset_seconds": 7200,
"timezone": "Europe/Berlin",
"timezone_abbreviation": "CEST",
"elevation": 178.0,
"hourly_units": {
"time": "iso8601",
"shortwave_radiation_instant": "W/m\u00b2"
},
"hourly": {
"time": [
"2023-08-26T00:00",
"2023-08-26T01:00",
"2023-08-26T02:00",
"2023-08-26T03:00",
"2023-08-26T04:00",
"2023-08-26T05:00"
],
"shortwave_radiation_instant": [
0.0,
0.0,
0.0,
0.0,
0.0,
9.1
]
}
}
The above JSON is in the code repository of the book under
ch04/ghi_past_and_future.json. Alternatively you can get fresh data using this URL:
https://api. open-meteo. com/v1/forecast? latitude=52. 52&longitude=13.
41&hourly=shortwave_ radiation_ instant&past_ days=7
© Manning Publications Co. To comment go to liveBook
113
At first sight it might be a daunting task, using SQL to pick out the morning hours,
noon and the evening hours from that array. Let’s see how LATERAL can solve this task.
We already read in chapter 1 that DuckDB is able to process JSON and will go into more
details in chapter 5. For now, it shall be good enough to know you can select from a
JSON file like from any other table in the FROM clause. The below query generates a
series of 7 days, joins those with the hours 8, 13 and 19 (7pm) to create indexes. Those
indexes are the day number * 24 plus the desired hour of the day to find the value in the
JSON array. That index is the lateral driver for the sub-query:
INSTALL json;
LOAD json;
WITH days AS (
SELECT generate_series AS value FROM generate_series(7)
), hours AS (
SELECT unnest([8, 13, 18]) AS value
), indexes AS (
SELECT days.value * 24 + hours.value AS i
FROM days, hours
)
SELECT date_trunc('day', now()) - INTERVAL '7 days' +
INTERVAL (indexes.i || ' hours') AS ts, -- #1
ghi.v AS 'GHI in W/m^2'
FROM indexes,
LATERAL (
SELECT hourly.shortwave_radiation_instant[i+1] -- #2
AS v
FROM 'code/ch04/ghi_past_and_future.json' -- #3
) AS ghi
ORDER BY ts;
#1 Recreates a data again from the hourly index
#2 Arrays are 1 based in DuckDB (and SQL in general)
#3 DuckDB automatically detects that this string refers to a JSON files, loads and parses it
The end of August in Aachen did look like this in 2023, it was not a great month for
photovoltaics:
© Manning Publications Co. To comment go to liveBook
114
┌──────────────────────────┬──────────────┐
│ ts │ GHI in W/m^2 │
│ timestamp with time zone │ double │
├──────────────────────────┼──────────────┤
│ 2023-08-26 08:00:00+02 │ 36.0 │
│ 2023-08-26 13:00:00+02 │ 490.7 │
│ 2023-08-26 18:00:00+02 │ 2.3 │
│ 2023-08-27 08:00:00+02 │ 243.4 │
│ 2023-08-27 13:00:00+02 │ 124.3 │
│ · │ · │
│ · │ · │
│ · │ · │
│ 2023-09-01 13:00:00+02 │ 392.0 │
│ 2023-09-01 18:00:00+02 │ 0.0 │
│ 2023-09-02 08:00:00+02 │ 451.0 │
│ 2023-09-02 13:00:00+02 │ 265.0 │
│ 2023-09-02 18:00:00+02 │ 0.0 │
├──────────────────────────┴──────────────┤
│ 24 rows (10 shown) 2 columns │
└─────────────────────────────────────────┘
The sub-query can produce zero, one or more rows for each row of the driving, outer
table. In the example above, it produced one row for each outer row. In case the sub-
query produces more rows, the values of the outer row will be repeated, similar to what
a CROSS join does. In case the sub-query does not produce any value, the join won’t
produce a value either. We must apply an OUTER JOIN in this case as well. At this point
the LATERAL keyword alone is not enough, and we must use the full JOIN syntax like
this. The following query is artificial and of little value except demonstrating the syntax.
Both queries produce a series of values from 1 to 4, the outer in a step size of one, the
inner in a step size of two. We compare both values in the ON clause.
SELECT i, j
FROM generate_series(1, 4) t(i)
LEFT OUTER JOIN LATERAL (
SELECT * FROM generate_series(1, 4, 2) t(j)
) sq ON sq.j = i -- #1
ORDER BY i;
#1 While the condition is now on the outside and cannot be formulated otherwise, it is still a correlated sub-query
The result of this query looks like this:
© Manning Publications Co. To comment go to liveBook
115
┌───────┬───────┐
│ i │ j │
│ int64 │ int64 │
├───────┼───────┤
│ 1 │ 1 │
│ 2 │ │
│ 3 │ 3 │
│ 4 │ │
└───────┴───────┘
The problem with the prices in section Section 48 can be solved with a sub-query and
LATERAL JOIN, too. In essence, the subquery must return a row from the price table
that has a validity that is as close to the date in time of the sale as possible. For that to
work we cannot use a normal join, as the sub-query must produce different values for
each incoming date. Therefore, the date-column that would normally be part of the join
must move inside the sub-query. Thus, the joined sub-query now becomes correlated, or
laterally joined to the outer query. The correlation in the example below is the validity of
the price compared to the day the power production was recorded.
Listing 4.34 Comparing the ASOF join from 4.29 to a LATERAL JOIN
SELECT power.day, power.kWh,
prices.value as 'EUR/kWh'
FROM v_power_per_day power,
LATERAL ( -- #1
SELECT *
FROM prices
WHERE prices.valid_from <= power.day -- #2
ORDER BY valid_from DESC limit 1 -- #3
) AS prices
WHERE system_id = 34
ORDER BY day;
#1 Mark the sub-query as lateral thus allowing correlation
#2 Correlate by inequality
#3 While the ASOF JOIN would automatically pick the closest value for us, we must order the values ourselves when using
LATERAL
For time-series related computations with DuckDB we would most certainly use the ASOF
JOIN. LATERAL is attractive when you need to think about portability, and you probably
find more databases supporting LATERAL then ASOF JOIN. Use LATERAL in scenarios in
which you want to fan-out of a dataset to produce more rows.
© Manning Publications Co. To comment go to liveBook
116
4.11 Summary
The SQL standard has evolved a lot since the last major revision in
1992 (SQL-92) and DuckDB supports a broad range of modern SQL,
including CTEs (SQL:1999), window functions (SQL:2003), list
aggregations (SQL:2016) and more.
Grouping sets allow the computation of aggregates over multiple
groups, doing a drill-down into different levels of detail; ROLLUP and
CUBE can be used to generate subgroups or combinations of grouping
keys
DuckDB fully supports window functions, including named windows and
ranges, enabling use cases such as computing running totals, ranks and
more.
All aggregate functions, including statistic computations and
interpolations, are optimized for usage in a windowed-context.
HAVING and QUALIFY can be used to select aggregates and windows
after they have been computed, FILTER prevents unwanted data from
going into aggregates.
DuckDB includes ASOF join which is needed in use cases involving time-
series data.
DuckDB also supports LATERAL joins that help fanning out data and can
emulate loops to a certain extent.
Results can be pivoted, either with a simplified, DuckDB specific PIVOT
statement or with a more static, standard SQL approach.
© Manning Publications Co. To comment go to liveBook