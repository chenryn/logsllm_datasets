title:Rise of the HaCRS: Augmenting Autonomous Cyber Reasoning Systems with
Human Assistance
author:Yan Shoshitaishvili and
Michael Weissbacher and
Lukas Dresel and
Christopher Salls and
Ruoyu Wang and
Christopher Kruegel and
Giovanni Vigna
Augmenting Autonomous Cyber Reasoning Systems with Human Assistance
Rise of the HaCRS:
Yan Shoshitaishvili
Arizona State University
PI:EMAIL
Christopher Salls
UC Santa Barbara
PI:EMAIL
Michael Weissbacher
Northeastern University
PI:EMAIL
Ruoyu Wang
UC Santa Barbara
PI:EMAIL
Giovanni Vigna
UC Santa Barbara
PI:EMAIL
Lukas Dresel
UC Santa Barbara
PI:EMAIL
Christopher Kruegel
UC Santa Barbara
PI:EMAIL
ABSTRACT
Software permeates every aspect of our world, from our homes to
the infrastructure that provides mission-critical services.
As the size and complexity of software systems increase, the
number and sophistication of software security flaws increase as
well. The analysis of these flaws began as a manual approach, but it
soon became apparent that a manual approach alone cannot scale,
and that tools were necessary to assist human experts in this task,
resulting in a number of techniques and approaches that automated
certain aspects of the vulnerability analysis process.
Recently, DARPA carried out the Cyber Grand Challenge, a
competition among autonomous vulnerability analysis systems
designed to push the tool-assisted human-centered paradigm into
the territory of complete automation, with the hope that, by re-
moving the human factor, the analysis would be able to scale to
new heights. However, when the autonomous systems were pitted
against human experts it became clear that certain tasks, albeit
simple, could not be carried out by an autonomous system, as they
require an understanding of the logic of the application under anal-
ysis.
Based on this observation, we propose a shift in the vulner-
ability analysis paradigm, from tool-assisted human-centered to
human-assisted tool-centered. In this paradigm, the automated sys-
tem orchestrates the vulnerability analysis process, and leverages
humans (with different levels of expertise) to perform well-defined
sub-tasks, whose results are integrated in the analysis. As a result, it
is possible to scale the analysis to a larger number of programs, and,
at the same time, optimize the use of expensive human resources.
In this paper, we detail our design for a human-assisted auto-
mated vulnerability analysis system, describe its implementation
atop an open-sourced autonomous vulnerability analysis system
that participated in the Cyber Grand Challenge, and evaluate and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134069
discuss the significant improvements that non-expert human assis-
tance can offer to automated analysis approaches.
CCS CONCEPTS
• Security and privacy-Usability in security and privacy; •
Security and privacy-Vulnerability scanners;
KEYWORDS
Fuzzing, Human assistance, Cyber Reasoning Systems
1 INTRODUCTION
Software has become dominant and abundant. Software systems
support almost every aspect of our lives, from health care to fi-
nance, from power distribution to entertainment. This growth has
led to an explosion of software bugs and, more importantly, soft-
ware vulnerabilities. Because the exploitation of vulnerabilities can
have catastrophic effects, a substantial amount of effort has been
devoted to discovering these vulnerabilities before they are found
by attackers and exploited in the wild.
Traditionally, vulnerability discovery has been a heavily manual
task. Expert security researchers spend significant time analyzing
software, understanding how it works, and painstakingly sifting
it for bugs. Even though human analysts take advantage of tools
to automate some of the tasks involved in the analysis process,
the amount of software to be analyzed grows at an overwhelming
pace. As this growth reached the scalability limits of manual analy-
sis, the research community has turned its attention to automated
program analysis, with the goal of identifying and fixing software
issues on a large scale. This push has been met with significant
success, culminating thus far in the DARPA Cyber Grand Challenge
(CGC) [34], a cyber-security competition in which seven finalist
teams pitted completely autonomous systems, utilizing automated
program analysis techniques, against each other for almost four
million dollars in prize money.
By removing the human factor from the analysis process, the
competition forced the participants to codify the strategy and or-
chestration tasks that are usually performed by experts, and, at the
same time, it pushed the limits of current vulnerability analysis
techniques to handle larger, more complex problems in an efficient
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA347and resource-aware manner. These systems represented a signifi-
cant step in automated program analysis, automatically identifying
vulnerabilities and developing exploits for 20 of a total of 82 binary
programs developed for the event.
Despite the success of these systems, the underlying approaches
suffer from a number of limitations. These limitations became evi-
dent when some of the CGC autonomous systems participated in a
follow-up vulnerability analysis competition (the DEFCON CTF)
that included human teams. The autonomous systems could not
easily understand the logic underlying certain applications, and, as
a result, they could not easily produce inputs that drive them to
specific (insecure) states. However, when humans could provide
“suggestions” of inputs to the automated analysis process the results
were surprisingly good.
This experience suggested a shift in the current vulnerability anal-
ysis paradigm, from the existing tool-assisted human-centered par-
adigm to a new human-assisted tool-centered paradigm. Systems
that follow this paradigm would be able to leverage humans (with
different level of expertise) for specific well-defined tasks (e.g., tasks
that require an understanding of the application’s underlying logic),
while taking care of orchestrating the overall vulnerability analysis
process.
This shift is somewhat similar to introduction of the assembly
line in manufacturing, which allowed groups of relatively unskilled
workers to produce systems (such as cars) that had, until then,
remained the exclusive domain of specially trained engineers. Con-
ceptually, an assembly line “shaves off” small, easy tasks that can
be carried out by a large group of people, in loose collaboration, to
accomplish a complex goal.
In this paper, we explore the application of this idea to vulnerabil-
ity analysis. More precisely, we develop an approach that leverages
tasklets that can be dispatched to human analysts by an autonomous
program analysis system, such as those used in the Cyber Grand
Challenge, to help it surmount inherent drawbacks of modern pro-
gram analysis techniques (see Figure 1). We explore the question of
how much our “program analysis assembly line” empowers humans,
otherwise unskilled in the field, to contribute to program analysis,
and we evaluate the improvement that external human assistance
can bring to the effectiveness of automated vulnerability analysis1.
Our results are significant: by incorporating human assistance into
an open-source Cyber Reasoning System, we were able to boost the
number of identified bugs in our dataset by 55%, from 36 bugs (in
85 binaries) using fully-automated techniques to 56 bugs through
the use of non-expert human assistance.
In summary, this paper makes the following contributions:
• We introduce the design of a human-assisted automated vul-
nerability analysis system, in which the result of well-defined
tasklets that are delegated to human actors are integrated in
the (otherwise) autonomous analysis process. These tasklets
help automated analysis systems to bridge the “semantic gap”
in the analysis of complex applications.
• We implemented a prototype human-assisted autonomous
system on top of Mechanical Phish, a system that partici-
pated in the DARPA Cyber Grand Challenge, which we had
1In the rest of the paper, we refer to “automated vulnerability analysis” as the orches-
tration process, even though it might include tasks that are outsourced to humans.
Figure 1: Tool-assisted Human-centered Analysis vs. Human-
assisted Tool-centered Analysis.
open-sourced after the contest. To support the community
and drive the state of (semi-) automated program analysis
forward, we open-source our modifications to Mechanical
Phish.
• We experimentally evaluated the effectiveness of our tasklets
in aiding the vulnerability analysis process of our system
by leveraging the assistance of unskilled humans, showing
that significant contribution can be made without requiring
expert hackers.
In the next section, we will discuss the background of automated
program analysis and pinpoint the challenges that we hope to solve
with human-analyzed tasklets.
2 BACKGROUND
The field of vulnerability discovery has received a significant amount
of research attention. In this section, we will describe the current
state of the art of both automated and manual vulnerability dis-
covery techniques, show the challenges facing each of them, and
position our approach in the context of related work.
2.1 Fully Automated Analysis
Individual techniques have been developed for identification of
vulnerabilities [7, 12, 29], automatic exploitation [1, 13, 14], and au-
tomatic application protection [24, 35, 36]. However, until recently,
researchers did not focus on the integration of various techniques
into cohesive end-to-end systems. Over the last two years, DARPA
hosted the Cyber Grand Challenge which required contestants
to develop Cyber Reasoning Systems (CRSes). These are fully au-
tonomous machines capable of identifying, exploiting, and patching
vulnerabilities in binary code.
A Cyber Reasoning System represents the culmination of years
of research into automated binary analysis. However, being fully
autonomous, CRSes suffer from the limitations of their under-
lying techniques. These limitations were reflected in the Cyber
Grand Challenge results, in which only 20 out of the 87 vulner-
able challenges were successfully exploited by the machine con-
tenders [8, 27].
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA3482.2 Human-based Computation
While the assembly line pioneered the idea of splitting complex
physical tasks (such as the assembly of a car) into small, manage-
able micro-tasks as early as the 12th century [6], the intellectual
equivalent was not explored until modern times. This concept was
most popularized with the Manhattan Project, in which specific
computation micro-tasks were assigned to and carried out by hu-
man “computers” [17]. With the emergence of modern computing
capability, these micro-tasks came to be chiefly carried out by ma-
chines. As computers developed to the point where they could
oversee such efforts, a formal specification of the different roles
that humans and computer components can take on in computation
emerged [18, 19, 25]. This specification defines three roles:
Organization Agent. The organization agent is the overall intelli-
gence. It tracks the progress of work toward an overarching
goal, determines what should be done, and creates micro-
tasks. In the Manhattan Project, the organization agent was
the panel of scientists leading the research effort.
Innovation Agent. The innovation agent is the entity responsible
for carrying out micro-tasks defined by the organization
agent. In the Manhattan Project, the innovation agents were
the human “computers” solving computation tasks.
Selection Agent. The selection agent collates the results produced
by the innovation agents and determines which are valid.
In the Manhattan Project, this task was performed by the
scientists leading the effort.
Systems are described using three letters, depending on whether
a human or computer agent is responsible for each role. For example,
an HCH designation would imply a system with a human deciding
which tasks to execute, a computer executing them, and the human
deciding which of the results are useful. In a security context, this
might be the human specifying jobs to a symbolic execution engine,
and then analyzing its output to identify exploitable bugs in a piece
of software.
Over the last few years, the Internet has achieved enough satu-
ration to support complex combinations of human and computer
agents. For example, Amazon’s Mechanical Turk provides an API for
automatically specifying micro-tasks for human consumption [2],
usually used in a CHC context. In fact, we use Mechanical Turk
for many of our experiments in this paper. In a similar vein to
Mechanical Turk, specific-purpose platforms have been created to
leverage human effort in the pursuit of a single overarching goal.
One such platform, Galaxy Zoo [37], utilizes human-completed
micro-tasks for the classification of astronomical images, while an-
other, Foldit [11], aids protein folding algorithms by having humans
play “folding games.”
2.3 Human-Driven Automated Analysis
Because it is important to understand the interactions between
manual and automated processes in binary analysis systems, we
provide a few examples of their intersections outside of the context
of our work.
Fuzzing. Generational fuzzers, such as Peach [16], attempt to
create inputs conforming to a specification that a program is de-
signed to process. Mutational fuzzers, such as AFL [38], mutate
previously-known inputs to identify program flaws.
The most common way of creating these inputs and input speci-
fications is manually, through human effort. This results in an HCH
system – a human creates the input specification, the computer
performs the fuzzing, and a human analyzes the results.
An example of successful human-computer cooperation in bi-
nary analysis is the discovery of the Stagefright vulnerability in
the Android multimedia library. This vulnerability was found by
repeating the following steps [10]:
Organization - H. The analyst seeds a mutational fuzzer (in this
case, AFL), and starts it.
Innovation - C. The fuzzer identifies vulnerabilities in the target
application (in this case, the Android multimedia library).
Selection - H. The human collects the vulnerabilities and fixes
them so that future iterations of the full system will identify
deeper vulnerabilities.
By repeating this HCH process, the analyst was able to identify
many high-impact vulnerabilities inside the Android multimedia
library, requiring multiple patches and an eventual rewrite of the
entire library to fix [30].
2.4 Human-Assisted Automated Analysis
The Cyber Grand Challenge required a fully autonomous system
(CCC, by the definitions in Section 2.2). This necessitated the devel-
opment, by participating teams, of complex automation to handle
the organizational, innovation, and selection roles. However, we
propose that while the organizational and selection roles must be
automated to achieve high scalability, some human effort can still be
used in the innovation role to mitigate drawbacks currently impact-
ing automated program analysis techniques. That is, our intuition
is that it is possible to create a Human-assisted Cyber Reasoning
System (HaCRS) that would sparingly use human assistance to
improve its performance.
HaCRS provides a principled framework for such an integration
of manual and automated analysis. It can be modeled as a C(C|H)C
system: it does most of its work fully autonomously, but relies
on human intuition in the innovation phase, when the automated
processes get “stuck.” In this paper, we propose that limited human
assistance can be used in the scope of otherwise-automated binary
analysis systems.
Of course, leveraging humans for tasks that are otherwise dif-
ficult to automate is a well-explored field. Research in the field of
human-computer interaction (HCI) has been focusing on effectively
engaging human labor into computer systems to solve hard prob-
lems, like labeling images [31], locating objects in images [32], and
recognizing characters in images [33]. One way to raise the moti-
vation of human participants is through gamification, which has
been adopted in security for human-assisted verification [9, 20, 22].
However, the scalability of these techniques and systems are strictly
limited by the number of participants of the game, since none of
them integrates the output of human users into an autonomous
system. One exception has been explored in the context of gener-
ating inputs for Android applications, but this concept has never
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA349been investigated in the context of an otherwise-autonomous Cyber
Reasoning System [23].
HaCRS takes a different route: It treats its humans as optional
assistants, and injects their output into an autonomous cyber rea-
soning system to improve an already-scalable and fully automated
solution. In the next section, we will give an overview of our system,
followed by in-depth details and an evaluation of its improvement
over fully-autonomous systems from the Cyber Grand Challenge.
3 OVERVIEW
While DARPA’s Cyber Grand Challenge drove the integration of
cutting edge automated binary analysis techniques, it also revealed
the many limitations of these techniques. Our work on HaCRS
extends the concept of a Cyber Reasoning System by defining a
method for human interaction that compensates for many of these
limitations. Primarily, HaCRS is an autonomous Cyber Reasoning
System. However, when it identifies situations that can benefit
from human analysis, HaCRS dispatches self-contained tasklets and
assigns them to human assistants. These human assistants can vary
in skill, from abundant low-skill analysts to rare high-skill hackers.
Our HaCRS can dispatch a variety of tasklets to human assis-
tants, depending on changing requirements. Generally, each tasklet
includes a specific program that must be analyzed and a request for
specific information that the human can extract from this program.
These tasklets are created by a centralized orchestration component
and disseminated to the assistant through a Human-Automation
Link (HAL). In this paper, as an initial exploration of this idea, we
focus on human-assisted input generation, leaving the exploration
of other tasklets to future work.