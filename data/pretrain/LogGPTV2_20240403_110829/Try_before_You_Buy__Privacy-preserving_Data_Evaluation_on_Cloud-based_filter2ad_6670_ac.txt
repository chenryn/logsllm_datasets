Output: encrypted parameters CKi;
1 t ← (Iw − |Ki| + 1)/s;
2 if i == 1 then
3
4
5
6
initiate a t × t matrix CKi;
for k ∈ {0,· · · , t} do
for v ∈ {0,· · · , t} do
CKi[k][v] ← IFE.KeyGen(Ri[k][v] ∗ Ki , msk);
7 else
8
9
10
11
initiate a (t × t × |Ki| × |Ki|)matrix CKi;
for k ∈ {0,· · · , t} do
for v ∈ {0,· · · , t} do
i−1[k ∗ s :
CKi[k][v][:][:] = Ri[k][v]/R2
k ∗ s + |Ki|][v ∗ s : v ∗ s + |Ki|] × Ki;
12 return CKi;
2 = Ri−1
2 × Yi−1
2. As Zi−1
to protect the kernel of this layer. As Figure 5 shows, this layer
receives the output (activation values) from the previous layer as
input data. Particularly, the input data from the previous layer can
2. For instance, the input from
be presented as Zi
2 × Y1
2 is
2 = R1
the first convolution hidden layer is Z1
2, we cannot directly
encrypted by a matrix of random numbers Ri−1
2 and the (trans-
perform convolution computation between Zi−1
formed) kernel. To guarantee correct convolution functionality, we
also need to eliminate the effect of random numbers Ri−1 in matrix
transformation.
As Figure 5 shows, we choose a matrix of random number Ri and
transform the kernel Ki to a set of random kernel matrices {CK00,
CK01, · · · }, according to the convolution stride length s and kernel
width |Ki|. As line 11 of Algorithm 4 shows, each matrix CKkv is
randomized by different random matrices. Thus, the privacy of the
kernel is fully preserved. Furthermore, matrix transformation elim-
inates the effect of random numbers in the subarea and guarantees
the convolution functionality of this layer. Specifically, the input
2 used in the (k × v)-th stride of convolution is multiplied
data Zi−1
i−1[k ∗ s : k ∗ s + |Ki|][v ∗ s : v ∗ s + |Ki|].
by a random matrix R2
i−1[k ∗ s :
As the corresponding kernel matrix CKkv is divided by R2
k ∗ s + |Ki|][v ∗ s : v ∗ s + |Ki|], the effect of random numbers
are eliminated and thus the inner product operation between the
subarea and CKkv can be executed correctly.
In this way, after performing a series of inner product operations
between multiple subareas and {CK00, CK01, · · · }, we can obtain
the result of entire convolution computation Zi = Ri × Yi. As Ri
is a random matrix, the original convolution result Yi cannot be
derived. After running the square activation function, this layer
will output random activation values Zi
2 = R2
.
i × Y2
i
k1k2k4k3r11k4r11k3r11k2r11k1…………r11Kr12KKernel KData Xx22x21x12x11x24x23x14x13x42x43x44x41x33x31x32x34x21x23x24x22x14x13x12x11c22c21c12c11c24c23c14c13sk11sk12C11r12k4r12k3r12k2r12k1X’11 X’12 C12z23z31z21z33z11z12z31z13z22…………Z1 =  (R1 ✕ Y1)……Z12 =  (R12 ✕ Y12)Vectorize & Mat TransformationUsing R1IFE-basd Data EncIFE-based Model (Kernel) Enc IFE-based Conv. Computationzij = IFE.Decrypt(Cij, skij)Cij = IFE.Encrypt(X’ij ,mpk)skij = IFE.KeyGen(rijK, msk)z32z31z33z13z23z22z12z21z11222222222IFE can reveal convolution results over ciphertextsSend to next layerSquare Activationoutput encrypted activation valuesY1 = Conv(K, X) 265Try before You Buy
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
represented as follows:



Fpoo[i][j] =
x =i∗wp +wp,y=j∗wp +wp
x =i∗wp,y=j∗wp
Z[x][y]/w
2
p ,
(1)
Let K be the convolution kernel and X be the convoluted data.
Considering Z[x][y] = K × Xx,y, where Xx,y = [x ∗ s : x ∗ s +
wk][y ∗ s : y ∗ s + wk], we can extend Equation 1 as follows:
Fpoo[i][j] =
x =i∗wp +wp,y=j∗wp +wp
x =i∗wp,y=j∗wp
x =i∗wp +wp,y=j∗wp +wp
= (
= AV G(Xx,y) × K
x =i∗wp,y=j∗wp
(K × Xx,y)/w
2
p
Xx,y/w
p) × K
2
(2)
2
p times.
According to Equation 2, we can pack multiple image subareas
Xx,y using AV G(Xx,y). As a result, the convolution and pooling
computation can be executed in one step. Therefore, the image size,
the execution time, and the encryption time of the CNN model can
be reduced w
5 PRIVACY-PRESERVING DATA SELECTION
We first present a privacy-preserving data prediction approach that
allows the cloud to perform prediction operations with a shop-
per’s encrypted model and sellers’ encrypted data. Based on this
prediction approach, we offer a privacy-preserving data selection
protocol.
5.1 Privacy-preserving Data Prediction
Algorithm 5: Feed Forward in a Fully-connected Layer
(the i-th Hidden Layer)
Input: encrypted parameters CWi or (cid:174)CW i, the encrypted
input from the (i − 1)-th layer (if i > 1, the input is
(cid:174)Z
i−1; else if i == 1, the input is (cid:174)Cx );
2
Output: the output (cid:174)Zi of the i-th layer;
1 if i == 1 then
2
3
4 else
5
for j ∈ {0,· · · , | (cid:174)CW i|} do
(cid:174)Zi ← IFE.Decryt( (cid:174)Z
2
i−1, (cid:174)CW i[j]);
(cid:174)Zi ← CWi (cid:174)Z
2
i−1;
6 return ( (cid:174)Zi)2;
Fully-connected Layers. Algorithm 5 shows the feed forward
process in fully-connected layers. If a fully-connected layer is the
first hidden layer, it applies IFE’s functional decryption to perform
matrix computation and output (cid:174)Z1 = (cid:174)R1 × (cid:174)Y1, where (cid:174)Y1 is the origi-
nal result and (cid:174)R1 is a random number vector. If a fully-connected
layer is the i-th hidden layer (i > 1), it directly performs matrix
computation as follows.
Figure 5: Model Encryption and Convolution Computation
in a Convolution Layer (i-th Hidden Layer). Here, the convo-
lution stride is 1.
4.4 Case Study: Encrypting a CNN Model and
Data
A typical CNN model, e.g., LeNet-5 [22], consists of convolution
layers, pooling layers, and followed by fully-connected layers. We
do not provide a special encryption approach for pooling layers in
our ML encryption protocol since they are essentially convolution
layers. Here, we show how to encrypt a CNN model and input data
using our ML encryption protocol.
Model and Data Encryption. In CNN tasks, data are first input
into a convolution layer. Therefore, we utilize Algorithm 2 to en-
crypt data, which retains the functionality of convolution in ci-
phertexts. For a typical CNN model, we firstly utilize Algorithm 4
to encrypt the parameters of convolution layers and then utilize
Algorithm 3 to encrypt the parameters of fully-connected layers.
We notice that only the parameters of the first hidden layer are
encrypted by the IFE cryptographic scheme. Therefore, the encryp-
tion time and execution time of a privacy-preserving CNN model
are determined by the first hidden layer.
Packing Optimization. When using a typical CNN model to pre-
dict images, adjacent image subareas are often mapped to an el-
ement of feature map (intermediate output) by convolution and
pooling layers. Therefore, we can pack these subareas into a cipher-
text to reduce the encryption time and execution time. Here, we
consider a typical pooling layer, i.e., mean pooling layer. Let Fpoo be
the output feature map of this pooling layer, Z be the output feature
map of the previous convolution layer, wp be the pooling size. The
feature map Fpoo output by convolution and pooling layers can be
Model Enc(Mat TransformationUsing Ri and R(i-1))previous (i-1)-th hidden LayerTransformed Conv. ComputationSquare activationSend to next Layerz’12Z’22KiInput dataDirect matrix computation over ciphertexts Output encrypted activation valuesYi = Conv(Ki, Z(i-1)2) Z(i-1)2 =  (R(i-1)2 ✕ Y(i-1)2)CK00k1k2k4k3CK01CKkv = Ri[k][v]/R(i-1)[k:k+2][v:v+2]2 ✕ Kik1’k2’k4’k3’k1’’k2’’k4’’k3’’CK00 ✕Z(i-1)2 [0:2][0:2], CK01 ✕Z(i-1)2[1:3][0:2]CK10 ✕Z(i-1)2[0:2][1:3], CK11 ✕Z(i-1)2 [1:3][1:3]CK10CK11z’11z’12z’11z’12z23z31z21z33z11z12z31z13z22Zi =  (Ri ✕ Yi)Zi2 =  (Ri2 ✕ Yi2)266ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Qiyang Song1, Jiahao Cao2,3, Kun Sun1, Qi Li2,3, and Ke Xu2,3
Algorithm 6: Feed forward in a Convolution Layer (the
i-th Hidden Layer)
Input: encrypted kernel parameters CKi, the encrypted
input from the (i − 1)-th layer (if i > 1, the input is
Z2
i−1; else if i == 1, the input is CX), the kernel
width Kw and stride length s;
of the i-th layer;
for k, v ∈ {0,· · · , |CKi|} do
Zi[k][v] ← IFE.Decryt(CX[k][v][:], CKi[k][v]);
Output: the output Z2
i
1 if i == 1 then
2
3
4 else
5
6
7 return (Zi)2;
for k, v ∈ {0,· · · , |CKi|} do
i−1[k ∗ s : k ∗ s + Kw][v ∗ s :
Zi = Z2
v ∗ s + Kw] × CKi[k][v][:][:];
2
i−1 = (Ci × Wi)((cid:174)R
i−1) × (Wi (cid:174)Z
i−1)
2
2
i−1 × (cid:174)Z
i−1)
2
(cid:174)Zi = CWi (cid:174)Z
= (Ci (cid:174)R
2
= (cid:174)Ri × (cid:174)Yi
(3)
2
2
i
2
i
2
i
= (cid:174)R
i × (cid:174)Y
, where (cid:174)Y
where (cid:174)Yi is the original result and (cid:174)Ri is a random vector. After
running the square activation function, this layer outputs random
activation values (cid:174)Z
is the original output of
the i-th layer.
Convolution Layers. Algorithm 6 shows the feed forward process
in convolution layers. If a convolution layer is the first hidden layer,
it transforms convolution computation to inner product operations
between the encrypted kernel parameters and subareas of input
data, and then utilizes IFE’s functional decryption to reveal the
convolution result Z1 = R1 × Y1, where R1 is a random number
matrix, and Y1 is the original result. If a convolution layer is the
i-th hidden layer (i > 1), it also converts convolution computation
to inner product operations between the subareas Z2
of input
data and random kernel matrices CKi = {CK11,· · · }. Let Kw be
the kernel width and s be the stride length. The (k ∗ v)-th inner
product operation is as follows.
i−1,kv
Zi[k][v] =CKi[k][v][:][:] · Z2
i−1,kv
i−1,kv)
=Ri[k][v](K · Z2
=Ri[k][v]Yi[k][v],
(4)
= Z2
i−1,kv
i−1,kv[s ∗ k : s ∗ k + Kw][s ∗ v : s ∗ v + Kw]. By
where Z2
performing multiple inner product productions, this layer outputs
the convolution result Zi = Ri×Yi, where Ri is a random matrix and
Yi is the original result. Finally, this layer runs the square activation
function to output random activation values Z2
i
5.2 Data Selection
As valuable data contain much informativeness and can signifi-
cantly improve model performance, a shopper can evaluate data
informativeness to screen out potentially valuable data. Note that
i × Y2
= R2
.
i
data informativeness for a specific model can be revealed by predic-
tion performance. Therefore, the shopper can collect the prediction
values of sellers’ data to estimate data informativeness. As the shop-
per cannot access sellers’ data before the final payment, it relies on
the cloud to output the prediction values of sellers’ data. To protect
model privacy and data privacy as well as enabling prediction oper-
ations in the cloud, we leverage privacy-preserving data prediction
to propose a data selection protocol. It works as follows.
Step 1: prediction operation. First, this protocol requires the
shopper and sellers to upload their encrypted model and data into
the cloud. Second, the cloud uses the shopper’s encrypted model
to predict sellers’ encrypted data (see Section 5.1). It then sends
encrypted prediction values back to the shopper.
Step 2: data section based on prediction values. In a prediction
operation, as aforementioned, each fully-connected layer outputs a
random vector (cid:174)Zi
= (cid:174)Ri
2, and each convolution layer outputs
2
a random matrix Z2
2. Therefore, if the prediction values
= Ri
i
(cid:174)Zn
2 are from a fully-connected layer, the shopper decrypts them
by multiplying with 1/ (cid:174)Ri
2 are from a