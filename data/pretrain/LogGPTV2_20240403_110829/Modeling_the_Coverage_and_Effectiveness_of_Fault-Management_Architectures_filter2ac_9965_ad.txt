proc5:Proc
:Ntfy
:Ntfy
mom1:MT
:Ntfy
proc7:Proc
:Ntfy
dm2:MT
proc6:Proc
:AW
:SW
proc3:Proc
:AW
:AW
:SW
proc4:Proc
:AW
Server1:AT
ag3:AGT
Server2:AT
ag4:AGT
Figure 9. MAMA Model of a hierarchical management 
architecture for the system in Figure 1. mom1 is man-
ager of managers handling both dm1 and dm2.
proc1:Proc
:AW
AppA:AT
im1:MT
ag1:AGT
:Ntfy
:SW
:Ntfy
proc2:Proc
:AW
AppB:AT
im2:MT
ag2:AGT
:Ntfy
:SW
:Ntfy
:AW
:SW
:AW
:AW
:AW
:SW
:SW
:SW
dm1:MT
proc3:Proc
proc4:Proc
:SW
:SW
ag3:AGT
ag4:AGT
dm2:MT
Server1:AT
:AW
Server2:AT
:AW
Figure 10. MAMA Model of a network management 
architecture for the system in Figure 1. im1, im2 are 
integrated managers. 
Ri = 
wUserAfi UserA
,
+
wUserBfi UserB
,
where wj  represents  the  weight  of  users  of  group  j  and  fi,j
represents the throughput of users of group j corresponding
to  the  configuration  Ci.  Table  2  shows  the  distinct
operational  configurations  of  the  system,  together  with
their  probabilities  and  throughputs  for  the  four  fault
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:50 UTC from IEEE Xplore.  Restrictions apply. 
Table 1: Distinct Operational Configurations of the system in Figure 1, their probabilities for four fault 
management architectures and the associated throughput of two groups of users
Configuration 
i 
Ci = {n | node n is an entry or a service node 
that is working and is in use by the system}
Perfect 
knowledge 
assumed 
(Case 1)
Central 
Architecture 
Distributed 
Architecture 
Hierarchical 
Architecture 
Network 
Architecture 
(Case 2)
(Case 3)
(Case 4)
(Case 5)
Prob(Ci)
 Throughputs of 
users
(fi,UserA, fi,UserB)
1
2
3
4
5 {userA, userB, eA, eB, serviceA, eA-1, servi-
{userA, eA, serviceA, eA-1}
{userA, eA, serviceA, eA-2}
{userB, eB, serviceB, eB-1}
{userB, eB, serviceB, eB-2}
0.125
0.024
0.125
0.024
0.531
ceB, eB-1}
6 {userA, userB, eA, eB, serviceA, eA-2, servi-
0.100
ceB, eB-2}
System Failed Configuration
Average UserA throughput
Average UserB throughput
0.071
0.352
0.572
0.117
0.021
0.117
0.021
0.314
0.057
0.353
0.232
0.387
0.082
0.041
0.307
0.036
0.349
0.046
0.139
0.235
0.608
0.225
0.014
0.076
0.014
0.206
0.037
0.428
0.226
0.253
0.148
0.026
0.148
0.026
0.282
(0.5, 0)
(0.5, 0)
(0, 0.5)
(0, 0.5)
(0.44, 0.67)
0.049
(0.44, 0.67)
(0, 0)
0.321
0.233
0.396
management architectures and also for perfect knowledge.
the 
Figure  11  compares  the  expected  steady  state  reward
rate  of  the  layered  system  in  Figure  1  corresponding  to
each  of 
four  management  architectures,  under
variations  of  the  weight  of  the  UserB  users  relative  to
UserA. In Table 2, we observe that the variation in UserA
throughput  (given  in  second  row  from  the  bottom)  for
different  management  architectures 
the
variation  in  UserB  throughput  (given  in  the  last  row).  As
we increase the weight of UserB over UserA, the effect of
UserB  throughput  on  the  reward  rate  of  the  system
increases.  UserB  throughput  decreases  for  the  cases  in
Table  2  in  the  order  Case  3,  Case  1,  Case  5,  Case  2  and
Case 4. Thus from Figure 11, we observe that the expected
steady  state  reward  rate  of  the  system  decreases  for  the
architectures  in  the  order  distributed,  network,  centralized
and  hierarchical  with  the  increase  in  the  weight  of  UserB
over UserA. 
than 
is 
less 
The  algorithm  costs  are  different  for  analyzing  the
different  architectures.  In  the  order  of  cases  given  across
the table, the number of states in the solution state space is
256,  16384,  65536,  262144  and  65536  respectively.  The
larger state spaces arise for systems with more management
components. The execution times for obtaining the distinct
operational  configurations  of  the  system  in  Figure  1  and
their  associated  probabilities  for 
the  five  cases  are
approximately 0.2, 2, 8, 35, and 8 secs respectively. This is
for a Java implementation, measured in Windows98 hosted
by Pentium (III) processor. 
Perfect Knowledge
Central Arch
Distributed Arch
Network Arch
Hierarchical Arch
12
10
8
6
4
2
0
0.06 0.13 0.25 0.5
1
2
4
8
16
weight  o f  U s er B /weight  o f  U s erA
Figure 11. Comparison of expected steady state reward 
rate of the system in Figure 1 for the four management 
architectures.
7.  Conclusion
The  value  of  including  the  management  architecture  in
the  analysis  is  first  to  account  for  failures  and  repairs  of
managers and agents, and second to evaluate limitations in
the  detection  and 
reconfiguration  architecture.  The
algorithm  described  here  scans  the  space  of  failure
combinations  to  detect  the  reachable  configurations  of
tasks 
the  operational
configurations  of  application  tasks  (smaller  yet).  Thus  the
effort  expended  in  the  high-complexity  steps  which  prune
the  set  of  configurations  is  small.  The  need  to  explore  2N
relatively  small  set)  and 
(a 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:50 UTC from IEEE Xplore.  Restrictions apply. 
cases will limit the scalability of the approach as described
here  to  one  or  two  dozen  entities,  however  much  more
efficient pruning appears to be possible, using a non-state-
space-based approach.
In  layered  systems  there  may  be  partially  covered
failures  that  affect  the  probability  of  different  operational
configurations  thereby  affecting  the  performability  of  a
system,  as  we  have  seen  in  Section  6.2.  This  work  has
combined 
the
performability computation.
the  effect  of  partial  coverage  with 
The  examples  shown  here  considered  only  failure  of
processors and tasks. Network failures are easily included,
as well.
The  analysis  could  be  extended  to  include  delays  to
detect  failures  and  to  reconfigure,  following  the  approach
described  in  [29].  Delays  in  detection  may  be  due  to  the
length  of  a  heartbeat  interval,  or  to  a  polling  delay.  This
extension leads to a serious increase in the number of states
however,  which  may  require  approximations  or  bounding
analysis.
8.  References
for 
incorporating 
imperfect 
fault-coverage 
[1]  J.  B.  Dugan,  “Fault  trees  and  imperfect  coverage”,  IEEE
Trans. on Reliability, 38(2), June 1989, pp. 177-185.
[2]  S.  A.  Doyle,  J.  B.  Dugan  and  F.  A.  Patterson-Hine,  “A
combinatorial  approach  to  modeling  imperfect  coverage”,  IEEE
Trans. on Reliability, 44(1), March 1995, pp. 87-94.
[3]  S.  V.  Amari,  J.  B.  Dugan  and  R.  B.  Misra,  “A  separable
method 
into
combinatorial  models”,  IEEE  Trans.  on  Reliability,  48(3),  Sept
1999, pp. 267-274.
[4]  P.  Stelling,  I.  Foster,  C.  Kesselman,  C.  Lee  and  G.  von
Laszewski,  “A  fault  detection  service  for  wide  area  distributed
computations” in Proc. of 7th IEEE Symp. on High Performance
Distributed Computations, 1998, pp. 268-278.
[5]  L.  A.  Laranjeira,  “NCAPS:  Application  high  availability  in
UNIX computer clusters”, FTCS-28, June 1998, pp. 441-450.
[6]  Y.  Huang,  P.  Y.  Chung,  C.  M.  R.  Kintala,  D. Liang,  and  C.
Wang,  “NT-Swift:  Software  implemented  fault-tolerance  for
Windows-NT”, Proc.  of 2nd USENIX  WindowsNT  Symposium,
Aug. 3-5, 1998.
[7] C. M. Woodside, “Performability modelling for multi-layered
service systems”, Proc. PMCCS-3, Illinois, Sept. 1996.
[8]  O.  Das  and  C.  M.  Woodside,  “The  Fault-tolerant  layered
queueing  network  model  for  performability  of  distributed
systems”, IPDS’98, Sept. 1998, pp. 132-141.
[9]  O.  Das,  “Performance  and  dependability  analysis  of  fault-
tolerant  layered  distributed  systems”,  Master’s  thesis,  Dept.  of
Systems and Computer Engineering, Carleton University, 1998.
[10] O. Das and C. M. Woodside, “Evaluating layered distributed
software  systems  with  fault-tolerant  features”,  Performance
Evaluation, 45 (1), 2001, pp. 57-76.
[11] S. S. Gokhale, W. E. Wong, K. S. Trivedi and J. R. Horgan,
“An analytical approach to architecture-based software reliability
prediction”,  IEEE Int. Computer  Performance and  Dependability
Symposium (IPDS’98), Sept. 1998, pp. 13-22.
[12]  K.  Goseva-Popstojanova  and  K.  S.  Trivedi,  “Architecture-
based  approach  to  reliability  assessment  of  software  systems”,
Performance Evaluation, 45 (2-3), 2001, pp. 179-204.
[13] F. B. Schneider, “What good are models and what models are
good”,  in  Sape  Mullender,  Editor,  Distributed  Systems,  ACM
Press, 1993.
[14] G. Franks, S. Majumdar, J. Neilson, D. Petriu, J. Rolia, and
M.  Woodside,  “Performance  Analysis  of  Distributed  Server
Systems,” in the 6th International Conference on Software Quality
(6ICSQ), Ottawa, Ontario, 1996, pp. 15-26.
[15]  B.  R.  Haverkort,  I.  G.  Niemegeers  and  P.  Veldhuyzen  van
Zanten, “DYQNTOOL: A performability modelling tool based on
the Dynamic Queueing Network concept”, in Proc. of the 5th Int.
Conf. on Computer Perf. Eval.: Modelling Techniques and Tools,
G. Balbo, G. Serazzi, editors, North-Holland, 1992, pp. 181-195.
[16]  B.  R.  Haverkort,  “Performability  modelling  using
DYQNTOOL+”,  International Journal  of Reliability, Quality  and
Safety Engineering., 1995, pp. 383-404.
[17]  H.  Kreger,  “Java  management  extensions  for  application
management”, IBM Systems Journal, 40(1), 2001, pp. 104-129.
[18] P. Felber, R. Guerraoui and A. Schiper, “The implementation
of  a  CORBA  Object  Group  Service”,  Theory  and  Practice  of
Object Systems, 4(2), 1998, pp. 93-105.
[19]  L.  E.  Moser,  P.  M.  Melliar-Smith  and  P.  Narasimhan,  “A
fault tolerance framework for CORBA”, Proc. of 29th Annual Int.
Symposium on Fault-Tolerant Computing, 1998, pp. 150-157.
[20] Tivoli Systems Inc., 9442 Capital of Texas Highway North,
Arboretum Plaza One, Austin, Texas. See http://www.tivoli.com.
[21]  C.  Hofmeister,  R.  Nord  and  D.  Soni,  Applied  Software
Architecture, Chapter 4, Addison-Wesley, 2000.
[22]  C.  J.  Colbourn,  The  Combinatorics  of  Network  Reliability,
Oxford University Press, 87. 
[23]  F.  Stamatelopoulos,  N.  Roussopoulos  and  B.  Maglaris,
“Using a DBMS for hierarchical network management”, Engineer
Conference, NETWORLD+INTEROP’95, March 1995.
[24]  L.  N.  Cassel,  G.  Patridge  and  J.  Westcott,  “Network
management 
and
approaches”, IEEE J. on Selected Areas in Comm., 7(7), Sept. 89.
[25]  R.  Marshall,  The  Simple  Book:  An  introduction  to  Internet
Management, 2nd Edition, Prentice Hall, 1994. 
[26]  A.  Leinwand  and  K.  Fang,  Network  Management:  A
Practical Perspective, Addison-Wesley, 1993. 
[27]  J.  Herman,  “Enterprise  Management  vendors  shoot  it  out”,
Data Communications International, Nov. 1990.
[28] A. Dupuy, S. Sengupta, O. Wolfson and Y. Yemini, “Design
of the Netmate network management system”, Integrated Network
Management, Elsevier Science-North Holland, 1991.
[29] O. Das and C. M. Woodside, “Failure detection and recovery
modelling  for  multi-layered  service  systems”,  Proc.  PMCCS-5,
Erlangen, Sept. 2001.
and  protocols:  Problems 
architectures 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:50 UTC from IEEE Xplore.  Restrictions apply.