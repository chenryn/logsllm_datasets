notice that because CVE-2017-13209 has been patched, the
false positive rate of the patched versions is lower than vulner-
able versions. Similarly, CVE-2018-9412 has not been patched
and Figure 7 shows the false positive rate of patched version is
higher than the vulnerable version. However, Table VI shows
the vulnerable version function gets 0 true positives and 1 false
negative in Android Things for CVE-2017-13209. This is due
to the fact that CVE-2017-13209 has been patched in Android
Things. Therefore, when PATCHECKO uses the vulnerable
function, the deep learning model may miss the correct target
function. Intuitively, it makes sense that a known vulnerability
discovery may miss a patched function as a vulnerability.
Dynamic analysis engine. The goal of the dynamic analysis
engine is to prune the set of candidate functions. In Table VI
and Table VII, the results for the dynamic analysis engine
TABLE VIII: The ﬁnal patch detection results for PATCHECKO
in Android Things
PATCHECKO Result Patched (?)
CVE
CVE-2018-9451
0
0
CVE-2018-9340
CVE-2017-13232 (cid:2)
CVE-2018-9345
0
0
CVE-2018-9420
CVE-2017-13210 (cid:2)
(cid:2)
CVE-2018-9470
CVE-2017-13209 (cid:2)
CVE-2018-9411
0
CVE-2017-13252 (cid:2)
CVE-2017-13253 (cid:2)
0
CVE-2018-9499
CVE-2018-9424
0
CVE-2018-9491
0
CVE-2017-13278 (cid:2)
CVE-2018-9410
0
CVE-2017-13208 (cid:2)
CVE-2018-9498
0
CVE-2017-13279 (cid:2)
0
CVE-2018-9440
CVE-2018-9427
0
CVE-2017-13178
0
CVE-2017-13180 (cid:2)
CVE-2018-9412
0
CVE-2017-13182 (cid:2)
Ground Truth Patched (?)
0
0
(cid:2)
0
0
(cid:2)
0
(cid:2)
0
(cid:2)
(cid:2)
0
0
0
(cid:2)
0
(cid:2)
0
(cid:2)
0
0
0
(cid:2)
0
(cid:2)
includes only the Execution and Ranking metrics. Because
we want to reduce the number of candidate functions that
we need to perform dynamic feature proﬁling. We use the
concrete input of vulnerable functions to validate the candidate
functions. As long as the candidate functions can survive the
input validation, PATCHECKO will do dynamic feature proﬁl-
ing for the ﬁnal candidate function. For example, after deep
learning, CVE-2018-9412 still has 252 candidate functions.
For dynamic analysis, PATCHECKO arranges different inputs of
vulnerable functions to validate the candidate functions. After
validation, only 38 candidate functions remain that require
dynamic feature proﬁling–which is a much more reasonable
number. Finally, PATCHECKO calculates the function similarity
score. Table VI and Table VII show that PATCHECKO can rank
the target function in the top 3 candidates 100% of the time.
The target function is only missed for CVE-2017-13209 since
the deep learning model already misses the target function.
Patch detection. According to the differential signature, se-
mantic static features, and the results from Table VI and Ta-
ble VII, PATCHECKO generates the ﬁnal results in Table VIII.
There is only one missed classiﬁcation for the patched version
of CVE-2018-9470. The reason that this classiﬁcation was
missed was due to the fact that the only difference between
vulnerable and patched version is one integer–which is a very
minute and difﬁcult to detect.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:40 UTC from IEEE Xplore.  Restrictions apply. 
382
Limitations. If the difference is very minute between a
vulnerable function and a patched function, our similarity
measures may not catch the difference, e.g., CVE-2018-9470.
The missed classiﬁcation is due to the fact the static feature
and dynamic features do not represent the difference between
the vulnerable and patched versions of the code. A solution
would be to add more ﬁne-grained features from known vul-
nerability exploits. However, assuming the associated exploits
are available, there is a trade-off in generalizability.
E. Processing Time
Table VI and Table VII respectively list the processing
times for the deep learning detection and dynamic analysis for
vulnerable and patched functions. The deep learning detection
phase takes around 3 seconds on average. The dynamic anal-
ysis’ execution time varies depending on the number of can-
didate functions to test and the number of execution environ-
ments (program states) to replicate. For example, CVE-2017-
13208 takes much less time than CVE-2017-13182 due to the
large difference (72) in candidate functions. For the dynamic
analysis, PATCHECKO bootstraps the execution environments
that correspond to the candidate functions. These environments
are run in parallel. PATCHECKO currently parallelizes the
execution environment
testing for all candidate functions.
Future works will focus on parallelizing the candidate function
execution in each environment to further reduce the dynamic
analysis processing time.
VI. RELATED WORK
We brieﬂy survey the related work. We focus on approaches
that use code similarity for known vulnerabilities without
access to source code. Other approaches for ﬁnding unknown
vulnerabilities [3], [5], [39], [7] and source code based ap-
proaches [24], [23], [21], [20], [28] will not be discussed
in this section. We divide the related work to programming
languages and machine learning based solutions.
Programming language-based solutions. The problem of
testing whether two pieces of syntactically-different code are
semantically identical has received much attention by previous
researchers. A lot of traditional approaches are based on a
matching algorithm for the CFGs of functions. Bindiff [44] is
based on the syntax of code for node matching. At a high-level,
BinDiff starts by recovering the control ﬂow graphs of the two
binaries and then attempts to use a heuristic to normalize and
match the vertices from the graphs. For
[33], each vertex
of a CFG is represented with an expression tree. Similarity
among vertices is computed by using the edit distance between
the corresponding expression trees. Another approach that
focuses on cross-platform binary similarity [16] proposes a
graph-based methodology. It used a matching algorithm on the
CFGs of functions. The idea is to transform the binary code
into an intermediate representation. For such a representation,
the semantics of each CFG vertex are computed by using
a sampling of the code executions using random inputs. On
the theoretical side,
[17], [41] extract feature representations
from the control ﬂow graphs and encodes them into graph
embeddings to speed up the matching process. Other recent
work [31], [25] leverage similar static analysis techniques.
In comparison to static analyses, dynamic binary analysis
is another approach to detect function similarity. Binhunt [18]
implemented symbolic execution and theorem proving as a
dynamic binary differentiate to test basic blocks for semantic
differences. However,
this method only focused on basic
blocks. There is a possibility that two functions are function-
ally equivalent but they may have different basic blocks due to
compiler optimizations. Egele et. al [15] proposed a dynamic
equivalence testing primitive that achieves complete coverage
by overriding the intended program logic. It collects the
side effects of functions during execution under a controlled
randomized environment. Two functions can be similar if their
side effects are similar. However, it requires executing each
function with many different inputs–which is time-consuming.
Machine learning-based solutions. Deep learning-based
graph embedding approaches have also been used to do binary
similarity checking. The current state-of-the-art [41] looks
for the same affected functions in the complete collection of
functions in a target binary. Similarly, [29] leverages machine
code to compute the best parameters for their neural network
model. [43] and [14] both use Natural Language Processing
(NLP) to replace the manually selected features. However,
when the search space is huge,
leaves a large set
of candidate functions. PATCHECKO can integrate dynamic
analysis to prune the candidate functions and reduce false
positives.
it still
Zhang et. al [42] proposed a unique position that lever-
ages the source-level information to answer a more speciﬁc
question: whether a speciﬁc affected function is patched in
the target binary. However, it needs the source code support
as well as the aforementioned similarity checking solutions
to locate the target function. PATCHECKO uses
to help it
deep learning and dynamic binary analysis to locate the target
function and perform accurate patch detection.
VII. CONCLUSION
We presented PATCHECKO, a vulnerability assessment
framework that
leverages deep learning and hybrid static-
dynamic binary analysis to perform cross-platform binary code
similarity analysis to identify known vulnerabilities without
source code access with high accuracy. PATCHECKO then
uses a differential engine to distinguish between vulnerable
functions and patched functions. We evaluated PATCHECKO
on 25 existing CVE vulnerability functions for Google Pixel 2
smartphone and Android Things IoT ﬁrmware images on het-
erogeneous architectures - we compiled the ﬁrmware images
for multiple architectures and platforms with different com-
piler optimization levels. Our deep learning model identiﬁes
vulnerabilities with an accuracy of over 93%.
We also demonstrated how dynamic analysis of the vulner-
ability functions in a controlled environment could be used
to signiﬁcantly reduce the number of candidate functions
(i.e., eliminate false positives). PATCHECKO identiﬁes the
correct matches (candidate functions) among the top 3 ranked
outcomes 100% of the time. Furthermore, PATCHECKO’s dif-
ferential engine distinguishes between functions that are still
vulnerable and those that are patched with an accuracy of 96%.
ACKNOWLEDGMENTS
We appreciate the Ofﬁce of Naval Research (ONR) and the
National Science Foundation (NSF) for their support of our
project.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:40 UTC from IEEE Xplore.  Restrictions apply. 
383
[28] Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng, and Y. Zhong,
“Vuldeepecker: A deep learning-based system for vulnerability detec-
tion,” arXiv preprint arXiv:1801.01681, 2018.
[29] B. Liu, W. Huo, C. Zhang, W. Li, F. Li, A. Piao, and W. Zou, “αdiff:
cross-version binary code similarity detection with dnn,” in Proceedings
of the 33rd ACM/IEEE International Conference on Automated Software
Engineering. ACM, 2018, pp. 667–678.
[30] P. Mell, K. Scarfone, and S. Romanosky, “Common vulnerability scoring
system,” IEEE Security & Privacy, vol. 4, no. 6, 2006.
[31] B. H. Ng and A. Prakash, “Expose: Discovering potential binary code re-
use,” in Computer Software and Applications Conference (COMPSAC),
2013 IEEE 37th Annual.
IEEE, 2013, pp. 492–501.
[32] J. Pewny, B. Garmany, R. Gawlik, C. Rossow, and T. Holz, “Cross-
architecture bug search in binary executables,” in Security and Privacy
(SP), 2015 IEEE Symposium on.
IEEE, 2015, pp. 709–724.
[33] J. Pewny, F. Schuster, L. Bernhard, T. Holz, and C. Rossow, “Leveraging
semantic signatures for bug search in binary programs,” in Proceedings
of the 30th Annual Computer Security Applications Conference. ACM,
2014, pp. 406–415.
[34] A. O. S. Project, “Android security bulletins; available at https://source.
android.com/security/bulletin.”
[35] R. Qiao and R. Sekar, “Effective function recovery for cots binaries using
interface veriﬁcation,” Technical report, Secure Systems Lab, Stony
Brook University, Tech. Rep., 2016.
[36] Quarkslab, “Lief: Library to instrument executable formats; available at
https://lief.quarkslab.com/,” 2017.
[37] G. Rossi and M. Testa, “Euclidean versus minkowski short distance,”
Physical Review D, vol. 98, no. 5, p. 054028, 2018.
[38] E. C. R. Shin, D. Song, and R. Moazzezi, “Recognizing functions in
binaries with neural networks.” in USENIX Security Symposium, 2015,
pp. 611–626.
[39] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta,
Y. Shoshitaishvili, C. Kruegel, and G. Vigna, “Driller: Augmenting
fuzzing through selective symbolic execution.” in NDSS, vol. 16, 2016,
pp. 1–16.
[40] TechNavio. (2014) Global industrial control systems (ics) security mar-
ket 2014-2018. shorturl.at/jprIX.
[41] X. Xu, C. Liu, Q. Feng, H. Yin, L. Song, and D. Song, “Neural network-
based graph embedding for cross-platform binary code similarity detec-
tion,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security. ACM, 2017, pp. 363–376.
[42] H. Zhang and Z. Qian, “Precise and accurate patch presence test for
binaries,” in 27th USENIX Security Symposium (USENIX Security 18),
2018, pp. 887–902.
[43] F. Zuo, X. Li, Z. Zhang, P. Young, L. Luo, and Q. Zeng, “Neural machine
translation inspired binary code similarity comparison beyond function
pairs,” arXiv preprint arXiv:1808.04706, 2018.
[44] Zynamics, “Bindiff; available at https://www.zynamics.com/software.
html,” 2011.
REFERENCES
[1] “Why are android devices slow to patch?; available at https://duo.com/
blog/why-are-android-devices-slow-to-patch,” 2018.
[2] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: a system for large-
scale machine learning.” in OSDI, vol. 16, 2016, pp. 265–283.
[3] T. Avgerinos, S. K. Cha, B. L. T. Hao, and D. Brumley, “Aeg: Automatic
exploit generation,” 2011.
[4] T. Bao, J. Burket, M. Woo, R. Turner, and D. Brumley, “Byteweight:
Learning to recognize functions in binary code.” USENIX, 2014.
[5] S. K. Cha, M. Woo, and D. Brumley, “Program-adaptive mutational
fuzzing,” in Security and Privacy (SP), 2015 IEEE Symposium on.
IEEE, 2015, pp. 725–741.
[6] W. Chargin, “Tensorboard; available at https://www.tensorﬂow.org/
guide/summaries and tensorboard,” 2017.
[7] D. D. Chen, M. Woo, D. Brumley, and M. Egele, “Towards automated
dynamic analysis for linux-based embedded ﬁrmware.” in NDSS, 2016.
[8] F. Chollet et al., “Keras: Deep learning library for theano and tensor-
ﬂow,” URL: https://keras. io/k, vol. 7, no. 8, 2015.
[9] Z. L. Chua, S. Shen, P. Saxena, and Z. Liang, “Neural nets can learn
function type signatures from binaries,” in Proceedings of the 26th
USENIX Conference on Security Symposium, Security, vol. 17, 2017.
[10] C. Cimpanu, “NASA hacked because of unauthorized Raspberry Pi con-
nected to its network,” https://www.zdnet.com, 2019, [Online; accessed
12-December-2019].
[11] F. T. COMMISSION,
security updates: Understanding
the issues; available at https://www.ftc.gov/system/ﬁles/documents/
reports/mobile-security-updates-understanding-issues/mobile security
updates understanding the issues publication ﬁnal.pdf.”
“Mobile
[12] R. Cox, “Our software dependency problem; available at https://research.
swtch.com/deps,” 2019.
[13] A. Cui, M. Costello, and S. Stolfo, “When ﬁrmware modiﬁcations attack:
A case study of embedded exploitation,” 2013.
[14] S. H. Ding, B. C. Fung, and P. Charland, “Asm2vec: Boosting static rep-
resentation robustness for binary clone search against code obfuscation
and compiler optimization,” in Asm2Vec: Boosting Static Representation
Robustness for Binary Clone Search against Code Obfuscation and
Compiler Optimization.
IEEE, 2019, p. 0.
[15] M. Egele, M. Woo, P. Chapman, and D. Brumley, “Blanket execution:
Dynamic similarity testing for program binaries and components.”
USENIX, 2014.
[16] S. Eschweiler, K. Yakdan, and E. Gerhards-Padilla, “discovre: Efﬁcient
cross-architecture identiﬁcation of bugs in binary code.” in NDSS, 2016.
[17] Q. Feng, R. Zhou, C. Xu, Y. Cheng, B. Testa, and H. Yin, “Scalable
graph-based bug search for ﬁrmware images,” in Proceedings of the 2016
ACM SIGSAC Conference on Computer and Communications Security.
ACM, 2016, pp. 480–491.
[18] D. Gao, M. K. Reiter, and D. Song, “Binhunt: Automatically ﬁnding
semantic differences in binary programs,” in International Conference
on Information and Communications Security.
Springer, 2008, pp.
238–255.
[19] Hex-Rays, “Ida pro disassembler; available at https://www.hex-rays.
com/products/ida/.”
[20] X. Huo and M. Li, “Enhancing the uniﬁed features to locate buggy ﬁles
by exploiting the sequential nature of source code,” in Proceedings of the
26th International Joint Conference on Artiﬁcial Intelligence. AAAI
Press, 2017, pp. 1909–1915.
[21] X. Huo, M. Li, and Z.-H. Zhou, “Learning uniﬁed features from natural
and programming languages for locating buggy source code.” in IJCAI,
2016, pp. 1606–1612.
[22] L. C. Infrastructure, “libfuzzer: a library for coverage-guided fuzz
testing,” 2017.
[23] L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “Deckard: Scalable and
accurate tree-based detection of code clones,” in Proceedings of the 29th
international conference on Software Engineering.
IEEE Computer
Society, 2007, pp. 96–105.
[24] T. Kamiya, S. Kusumoto, and K. Inoue, “Ccﬁnder: a multilinguistic
token-based code clone detection system for large scale source code,”
IEEE Transactions on Software Engineering, vol. 28, no. 7, pp. 654–670,
2002.
[25] W. M. Khoo, A. Mycroft, and R. Anderson, “Rendezvous: A search
engine for binary code,” in Proceedings of the 10th Working Conference
on Mining Software Repositories.
IEEE Press, 2013, pp. 329–338.
[26] D. Labs, “Thirty percent of android devices susceptible to 24 critical
vulnerabilities; available at shorturl.at/dhj17.”
[27] S. R. Labs, “The android ecosystem contains a hidden patch gap;
available at https://srlabs.de/bites/android patch gap/.”
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:40 UTC from IEEE Xplore.  Restrictions apply. 
384