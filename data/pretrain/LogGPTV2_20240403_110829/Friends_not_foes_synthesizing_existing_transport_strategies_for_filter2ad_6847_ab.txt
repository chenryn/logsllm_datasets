ing across a wide range of network loads.
Arbitration: Explicit rate protocols, like PDQ [18], use ar-
bitration as their underlying transport strategy. Instead of
endpoints making decisions on their own, arbitration based
approaches [18, 24] require the switches to make the schedul-
ing decision, keeping in view all network ﬂows and their in-
dividual priorities (e.g., deadline, ﬂow size). The scheduling
decision is communicated as a rate at which ﬂows should
send data. The rate could be zero, if the ﬂow needs to be
paused because of its low priority, or it could be the full link
capacity, if the ﬂow has the highest priority. While a cen-
tralized problem in general, prior work [18, 24] shows that
arbitration can be done in a decentralized fashion – each
switch along the path of a ﬂow adds its rate to the packet
header and the minimum rate is picked by the sender for
transmitting data.
The explicit nature of arbitration based approaches en-
sures that ﬂows achieve their desired rate quickly (typically
in one RTT). Moreover, the ability to pause and unpause
ﬂows enables strict priority scheduling of ﬂows: the highest
priority ﬂow gets the full link capacity (if it can saturate
the link) while other ﬂows are paused. However, this ex-
plicit rate assignment comes with its own set of problems.
For example, calculating accurate rates for ﬂows is challeng-
ing as ﬂows could be bottlenecked at non-network resources
(e.g., source application, receiver). Another important issue
is the ﬂow switching overhead, which refers to the overhead
of pausing and unpausing ﬂows. This overhead is typically
∼1-2 RTTs, which can be signiﬁcant in scenarios involving
short ﬂows (when ﬂows last for a small duration) and at high
loads (when ﬂows need to be frequently preempted).
We illustrate the impact of ﬂow switching overhead in
a practical scenario through a simulation experiment. We
consider PDQ [18]3, which is considered the best perform-
ing arbitration based scheme, and compare its performance
with DCTCP [11]. The scenario is a repeat of the previous
intra-rack, all-to-all experiment, except the metric here is
AFCT. Figure 2 shows the AFCT as a function of network
load. At low loads, PDQ outperforms DCTCP because of
its fast convergence to the desired rate. However, at high
loads, the ﬂow switching overhead becomes signiﬁcant as
more ﬂows contend with each other, thereby requiring more
preemptions in the network. As a result, PDQ’s performance
degrades and the completion time becomes even higher than
that of DCTCP.
In-network Prioritization: In transport protocols that
use in-network prioritization (e.g., pFabric [12]), packets
carry ﬂow priorities, such as the ﬂow deadline or size, and
switches use this priority to decide which packet to sched-
ule or drop (in case of congestion). This behavior ensures
two desirable properties: work conservation, a lower priority
packet is scheduled if there is no packet of higher priority,
and preemption, when a higher priority packet arrives, it
gets precedence over a lower priority packet.
The well-known downside to in-network prioritization is
the limited number of priority queues available in switches
– typically ∼ 4-10 [24] (see Table 2). For most practical
scenarios, this number is much smaller than the number of
unique ﬂow priorities in the system. Proposals that sup-
port a large number of priority levels require changing the
network fabric [12], which makes them hard to deploy.
Another shortcoming of this strategy is that switches
make local decisions about prioritization which can lead
to sub-optimal performance in multi-link scenarios. This
is shown in Figure 3 through a simple toy example involv-
3Based on the simulator code obtained from the PDQ au-
thors. It supports all the optimizations that reduce the ﬂow
switching overhead.
 0 0.2 0.4 0.6 0.8 1 10 20 30 40 50 60 70 80 90App. ThroughputOffered load (%)pFabricD2TCPDCTCP 1 10 100 10 20 30 40 50 60 70 80 90AFCT (msec) - log ScaleOffered load (%)PDQDCTCPoritization, the arbitrator can just assign relative priorities
to the ﬂows (e.g., high priority ﬂow vs low priority ﬂow),
leaving it up to the switches to enforce this relative prior-
ity through a suitable scheduling and dropping mechanism.
The current in-network prioritization mechanisms (e.g., pri-
ority queues) provide seamless switching between ﬂows of
diﬀerent priorities, so there is no ﬂow switching overhead
and link utilization remains high during this period.
A simple example illustrates this beneﬁt. Assume we have
two ﬂows – F1 (higher priority) and F2 (lower priority).
With arbitration-only approaches, F1 is initially assigned
the entire link capacity while F2 is paused during this time.
When F1 ﬁnishes, we have to explicitly signal F2 to un-
pause. With in-network prioritization, we can just assign
these ﬂows to diﬀerent priority classes – F1 is mapped to
the high priority class while F2 is mapped to the low pri-
ority class. The switch ensures that as soon as there are
no more packets of F1 in the high priority queue, it starts
scheduling packets of F2 from the lower priority queue.
Arbitration aiding In-network Prioritization: The
small number of priority queues cause performance degra-
dation when multiple ﬂows get mapped to the high priority
queue [12]. This results in multiplexing of these ﬂows in-
stead of strict priority scheduling (i.e., one ﬂow at a time).
This problem can be avoided with the help of arbitration.
Instead of statically mapping ﬂows to queues, an arbitrator
can do a dynamic mapping. So a ﬂow’s priority queue keeps
on changing during it’s lifetime. A ﬂow whose “turn” is far
away is mapped to lower priority queue. As a ﬂow’s turn is
about to come, it moves up to a higher priority queue.
We explain this idea through a simple two queue example.
Suppose queue A (QA) is the high priority queue and queue
B (QB) is the lower priority queue. We have four ﬂows to
schedule (F1, F2, F3, and F4, with F1 having the highest pri-
ority and F4, the lowest) — as the number of ﬂows is more
than the number of queues, any static mapping of ﬂows to
queues will result in sub-optimal performance. With the
help of arbitration, we can initially map F1 to QA and the
other three ﬂows to QB. When F1 ﬁnishes, we can change
the mapping of F2 from QB to QA while ﬂows F3 and F4
are still mapped to QB. A similar process is applied when
F2 (and later on, F3) ﬁnishes. In short, the highest prior-
ity queue is used for the active, high priority ﬂow while the
lower priority queue is used primarily to keep link utiliza-
tion high (i.e., work-conservation). The example shows how
arbitration can help leverage the limited number of priority
queues without compromising on performance.
Arbitration helping Self-Adjusting Endpoints : With
arbitration-only approaches, calculating precise ﬂow rates
can be hard because the arbitrator may not have accu-
rate information about all the possible bottlenecks in the
system [18, 24]. Thus, we can end up underestimating
or overestimating the available capacity. Unfortunately,
in arbitration-only approaches, endpoints – which typically
have a better idea of path conditions – are dumb: they al-
ways transmit at the rate assigned by the arbitrator, so even
if they are in a position to detect congestion or spare capac-
ity in the network, they cannot respond.
The self-adjusting endpoint strategy naturally addresses
this problem as it constantly probes the network:
if there
is any spare capacity, it will increase its rate, and if there
is congestion, it will back oﬀ. For example, suppose there
Figure 3: Toy example illustrating problem with
pFabric.
Figure 4: Loss rate for pFabric with varying load.
Local prioritization leads to losses at high loads.
ing three ﬂows. Flow 1 has the highest priority; Flow 2 has
medium priority and ﬂow 3 has the lowest priority. Flows
1 and 2 share link B, so only ﬂow 1 can progress while ﬂow
2 should wait. A protocol like pFabric continues to send
packets of ﬂow 2 on link A even though these packets are
later dropped at link B. These unnecessary transmissions
stall ﬂow 3, which could have run in parallel with ﬂow 1 as
both ﬂows do not share any link.
The above toy example highlights a common use case
present in all-to-all traﬃc patterns (e.g., MapReduce [16],
Search) where a node typically has data to send to many
other nodes. To quantify this problem under such practi-
cal settings, we simulate the interaction between workers
and aggregators within a single rack of a search applica-
tion. Each worker-aggregator ﬂow is uniformly distributed
between [2-198] KB. We focus on the loss rate of pFabric
when the network load is increased. As shown in Figure 4,
loss rate shoots up as the load on network links is increased.
For a load of 80%, more than 40% packets are dropped.
These lost packets translate into loss of throughput as we
could have used these transmissions for packets belonging
to other ﬂows. In Section 4, we show how this high loss rate
results in poor FCT for pFabric.
2.2 Transport Strategies in Unison
We now discuss how combining these transport strategies
oﬀers a simple solution to the problems identiﬁed earlier.
In-network Prioritization complementing arbitra-
tion: The high ﬂow switching overhead of arbitration-only
approaches can be avoided with the help of in-network pri-
oritization. As today’s arbitration-only approaches,
like
PDQ [18], assume no prioritization within the network, they
have to achieve priority scheduling by communicating ex-
plicit rates to end-hosts, which takes time, and thus results
in a high ﬂow switching overhead. If we have in-network pri-
1 Src. 1 Switch  Dest. 2 Src. 2 Dest. 1 3 2 1 Link B Link A Flow 1: Src. 1  Dest. 1 (Highest Priority) Flow 2: Src. 2  Dest. 1 (Medium Priority) Flow 3: Src. 2  Dest. 2 (Lowest Priority) Stalled Flows  0 10 20 30 40 50 60 70 10 20 30 40 50 60 70 80 9095Loss Rate (%)Offered load (%)pFabricSwitch
Vendor
BCM56820 [2] Broadcom
G8264 [4]
7050S [1]
EX3300 [5]
S4810 [3]
IBM
Arista
Juniper
Dell
Num. Queues ECN
Yes
Yes
Yes
No
Yes
10
8
7
5
3
Table 2: Priority Queues and ECN support in pop-
ular commodity top-of-rack switches. The numbers
are per interface.
are two ﬂows in the system with diﬀerent priorities. The
higher priority ﬂow is assigned the full link capacity but
it is unable to use it. The lower priority ﬂow will remain
paused if we do not use self-adjusting endpoints. However, if
the endpoint uses a self-adjusting policy, it will detect spare
capacity and increase its rate until the link is saturated.
Note that arbitration also helps the self-adjusting endpoint
strategy: instead of just blindly probing the network for its
due share, a ﬂow can use information from the arbitrator to
“bootstrap” the self-adjusting behavior.
3. PASE
PASE is a transport framework that synthesizes the
three transport strategies, namely in-network Prioritization,
Arbitration, and Self-adjusting Endpoints. The underlying
design principle behind PASE is that each transport strategy
should focus on what it is best at doing, such as:
• Arbitrators
should do inter-ﬂow prioritization at
coarse time-scales. They should not be responsible
for computing precise rates or for doing ﬁne-grained
prioritization.
• Endpoints should probe for any spare link capacity on
their own, without involving any other entity. Further,
given their lack of global information, they should not
try to do inter-ﬂow prioritization (protocols that do
this have poor performance, as shown in Section 2).
• In-network prioritization mechanism should focus on
per-packet prioritization at short, sub-RTT timescales.
The goal should be to obey the decisions made by the
other two strategies while keeping the data plane sim-
ple and eﬃcient.
Given the above roles for each strategy, the high-level
working of PASE is as follows. Every source periodically
contacts the arbitrator to get its priority queue and refer-
ence rate. The arbitration decision is based on the ﬂows
currently in the system and their priorities (e.g., deadline,
ﬂow-size). As the name suggests, the reference rate is not
binding on the sources, so depending on the path conditions,
the sources may end up sending at higher or lower than this
rate (i.e., self-adjusting endpoints). A key beneﬁt of PASE
is that we do not require any changes to the data plane:
switches on the path use existing priority queues to schedule
packets and employ explicit congestion notiﬁcation (ECN)
to signal congestion. As shown in Table 2, most modern
switches support these two features.
To achieve high performance while being deployment
friendly, PASE incorporates two key components: a control
plane arbitration mechanism and an end-host transport pro-
tocol. While existing arbitration mechanisms operate in the
data plane (and hence require changes to the network fab-
ric), we implement a separate control plane for performing
arbitration in a scalable manner. To this end, we introduce
optimizations that leverage the typical tree structure of data
center topologies to reduce the overhead of arbitration. Fi-
nally, PASE’s transport protocol has an explicit notion of
reference rate and priority queues, which leads to new rate
control and loss recovery mechanisms.
In the following sections, we describe the control plane
and the end-host transport protocol of PASE, followed by
the details of its implementation.
3.1 Arbitration Control Plane
While a centralized arbitrator is an attractive option for
multiple reasons, making it work in scenarios involving short
ﬂows is still an open problem [10, 15]. Prior work [18] shows
that the problem of ﬂow arbitration can indeed be solved in
a distributed fashion: each switch along the path of a ﬂow
independently makes the arbitration decision and returns
the allocated rate for its own link, and the source can pick
the minimum rate. While prior work implements arbitration
as part of the data plane, PASE supports this as part of the
control plane because experiences with prior protocols (e.g.,