91.3
88.8
94.4
92.8
TABLE III.
SOURCE/SINK CROSS VALIDATION PSCOUT WITHOUT
PERMISSION FEATURE
We evaluated SUSI on an extended test set obtained
using the implicit-annotation technique explained in section
Section IV-E. With this technique, classiﬁcations for a method
are copied to all other methods that would lead to the same
code being executed according to the semantics of virtual
method dispatch in Java. SUSI again shows an average recall
and precision of more than 92%, see Table IV. The results are
not exactly equal because some of our features consider not
just a method’s deﬁnition but also its container, e.g., the name
of the class the method resides in. The fact that SUSI obtains
similar results despite these differences is a good indicator of
inherent consistency in the results as it shows that semantically
equal methods (i.e., ones that have not been overwritten and
are thus exposed as-is) are also recognized equally.
Recall [%] Precision [%]
Category
Sources
Sinks
Neither
Weighted Average
89.6
84.7
95.2
92.3
88.0
90.8
93.6
92.3
TABLE IV.
SOURCE/SINK CROSS VALIDATION WITH IMPLICIT
ANNOTATIONS
The classiﬁer takes about 26 minutes to classify the
complete Android 4.2 API on a MacBook Pro computer running
MacOS X version 10.7.4 on a 2.5 GHz Intel Core i5 processor
and 8 GB of memory.
As explained in Section IV-A, we experimented with various
classiﬁcation algorithms, and found that SMO performed best.
In Table V, we compare the weighted average precision for
2The available permission lists including PScout are incomplete since they
exclude permissions enforced through calls to native code.
9
Recall [%] Precision [%]
Category
ACCOUNT
BLUETOOTH
BROWSER
CALENDAR
CONTACT
DATABASE
FILE
NETWORK
NFC
SETTINGS
SYNC
UNIQUE IDENTIFIER
NO CATEGORY
Weighted Average
TABLE VI.
100.0
100.0
100.0
100.0
100.0
100.0
100.0
83.3
100.0
85.7
100.0
100.0
62.9
89.6
SOURCE CATEGORY CROSS VALIDATION
100.0
83.3
83.0
100.0
95.0
50.0
75.0
83.3
100.0
75.0
100.0
88.9
95.7
88.7
SMO, J48, and Naive Bayes, the most well-known represen-
tatives of their respective families of classiﬁers (margin, rule-
based and stochastic classiﬁer, respectively). The results were
computed on the extended training set obtained through the
implicit-annotation technique. The permission feature was not
used.
2) Validating SUSI’s Source/Sink Output: The output of
SUSI’s ﬁrst phase is a list of sources and a separate list of
sinks. In this section we verify that the precision and recall
of the cross validation in Section V-A1 is representative for
SUSI’s actual output. Since manually verifying the outputs for
the complete Android API is infeasible, we concentrate on two
APIs: The Google Cast API and the Google Mirror API.
Our manual validation of the Google Cast API results in
a precision of 96% and a recall of 99% for the sources and
a precision of 100% and recall of 88% for the sinks. The
somewhat lower recall for the sinks is due the fact this API
has only 18 sinks, out of which 16 were detected. The Google
Mirror API yields a precision of 100% and a recall of 97% for
the sources and a precision of 100% and recall of 94% for the
sinks. In result it seems that one can be rather optimistic: at
least for these APIs the precision and recall are even higher than
the ones obtained through cross validation (cf. Section V-A1).
B. RQ2: Categories for Sources and Sinks
For evaluating the categorization of sources and sinks, we
used similar techniques like the ones used for assessing the
identiﬁcation of sources and sinks in Section V-A. However,
recall that only methods identiﬁed as sources or sinks in the
ﬁrst step get categorized by SUSI.
1) Cross Validation: We use ten-fold cross validation on
our training data to assess the quality of our categorization. For
this task, we do not use the permission feature, but do apply
the implicit annotation technique from Section IV-E. Table VI
shows the cross-validation results for categorizing the sources,
while Table VII contains those for the sinks.
While SUSI achieves a very high precision and recall for
most of the categories, the results for a few categories (e.g.
Bluetooth) are considerably worse. These categories are rather
small, i.e., randomly picking training methods from the overall
Classiﬁer
Margin (SMO)
Rule-Based (J48)
Probabilistic (Naive Bayes)
Class. [%]
92.3
89.5
86.9
Avg. Recall
Source Cat. [%]
88.8
81.0
61.5
Sink Cat. [%] Class. [%]
88.4
80.2
46.6
92.3
89.4
87.1
Avg. Precision
Source Cat. [%]
89.7
81.6
61.7
Sink Cat. [%]
90.4
77.4
36.1
TABLE V.
SOURCE/SINK CLASSIFIER COMPARISON
Recall [%] Precision [%]
Category
ACCOUNT
AUDIO
BROWSER
CALENDAR
CONTACT
FILE
LOG
NETWORK
NFC
PHONE CONNECTION
PHONE STATE
SMS MMS
SYNC
SYSTEM
VOIP
NO CATEGORY
Weighted Average
TABLE VII.
100.0
100.0
100.0
100.0
100.0
100.0
71.4
88.9
100.0
85.7
100.0
100.0
100.0
89.3
100.0
70.2
88.0
SINK CATEGORY CROSS VALIDATION
85.7
100.0
50.0
100.0
91.7
60.0
100.0
72.7
100.0
75.0
100.0
96.3
80.0
80.6
66.7
97.1
85.7
set of 110,000 Android 4.2 API methods yields only few entries
belonging to such categories. Respectively, there is not much
material to train the classiﬁer on. Annotating more data (recall
that we only have category annotations for 0.4% of all methods)
would certainly improve the situation.
Categories can be ambiguous in some cases. A method to
set the MSIDN (the phone number to be sent out when placing
a call) could for instance be seen as a system setting (category
SETTINGS), but could also be considered a UNIQUE ID.
In such cases, we checked the classiﬁer’s result and updated
our training data if a misclassiﬁcation was to due semantic
ambiguity, i.e., the result would be right in both categories.
Categories that ended up empty or almost empty due to such
shifts were removed.
Categorizing the sources took about 6 minutes on our test
computer. The sinks were classiﬁed in about 3 minutes.
2) Validating SUSI’s Categorized Source/Sink Output:
Manually evaluating the categorized sources and sinks for
the Google Cast and Google Mirror APIs shows a precision
and recall of almost 100% . The precision and recall for the
Google Cast API are 100% for both sources and sinks. For
sources in the Google Mirror API the precision is 98% and the
recall is 100%. For sinks, both precision and recall are 100%.
This shows that the results from Section V-A2 also carry over
to the categorization.
C. RQ3: Sources and Sinks in Malware Apps
It is an important question to ask whether existing malware
apps already use sources and/or sinks discovered by SUSI but
10
not currently recognized by state-of-the-art program-analysis
tools. To address this question, we selected about 11,000
malware apps from Virus Share [21] and analyzed which kinds
of sources and sinks these malware samples use. Unsurprisingly,
as already found by different researchers [4], [31], [32]
current malware is leaking privacy information such as location
information or the address book.
Interestingly, however,
these samples do not only use
the standard source and sink methods commonly known to
literature, but also such ones not detected by popular program
analysis tools (see Section V-E). In total, the samples revealed
usage of more than 900 distinct source methods, all of which
can be used to obtain privacy-sensitive information. Further-
more, the samples leak data through more than 500 distinct
sink methods. The getLac() and getCid() methods used
in our motivating example (see Section II) are two of the most
commonly used methods in the LOCATION INFORMATION
category. This is partly related to the fact
that both are
called in the Google Maps Geolocation API [33], which is
used in the respective malware samples. Another example
is the getMacAddress() method in the WifiInfo class
that SUSI categorizes as NETWORK INFORMATION. This
method is among the most often called methods in this category
and is not treated as a source by many tools either. By manual
analysis of different malware samples, we found that these
source methods are not just called, but their privacy-sensitive
return values are indeed leaked to a remote web server.
Since approaches such as LeakMiner [3] create their
source and sink lists from a permission map, we also ana-
lyzed whether malware samples exploit source methods that
do not need a permission. Examples of such methods are
getSimOperatorName() in the TelephonyManager
class (returns the service provider name), getCountry()
in the Locale class, and getSimCountryIso in the
TelephonyManager class (both return the country code),
all of which are correctly classiﬁed by SUSI. By manually
analyzing the malware samples, we found that these methods
are used frequently and that this data is actually leaked to web
servers. This conﬁrms that approaches which solely rely on
the permission map for inferring sources and sinks miss data
leaks in real-world malware samples.
SUSI’s categorized output of sources and sinks for Android
4.2 (see Section V-A1) includes a lot of methods which return
privacy-sensitive information, such as the IMEI. SUSI found
that there is not only one way of accessing such information (e.g.
via getDeviceID for the IMEI). Instead, there are plenty of
wrapper methods in internal Android classes or pre-installed
apps that return the same value. One example would be the
internal GSMPhone class or the pre-installed email-application
which contains a getDeviceId() method for returning the
IMEI. These methods can only be called using explicit class
s
d
o
h
t
e
m
e
c
r
u
o
s
f
o
t
n
u
o
m
A
#
400
300
200
100
0
4
6
Bluetooth
Location
NFC
16
18
12
14
10
8