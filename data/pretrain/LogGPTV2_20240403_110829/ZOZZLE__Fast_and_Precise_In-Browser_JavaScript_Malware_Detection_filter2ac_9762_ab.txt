Implementation
In this section, we discuss the details of the ZOZZLE im-
plementation.
3.1 Overview
Much of ZOZZLE’s design and implementation has in ret-
rospect been informed by our experience with reverse
engineering and analyzing real malware found by NOZ-
ZLE. Figure 5 illustrates the major parts of the ZOZZLE
0%5%10%15%20%25%30%0102030405060708090100110120130140150160170180190200210220230240250260270280290300Fraction of Deobfuscated URLs BenignMaliciousextraction and labelingclassificationfeature selectionJS Enginefile.js/eval contextinitial featuresJavaScript fileMaliciousBenignpredictive featuresfeatures + weightsBayesian classifierBayesian classifiertrainingtrainingfilteringfilteringAST contextsbenign contextsmalicious contextsloopc0stringarchitecture. At a high level, the process evolves in three
stages: JavaScript context collection and labeling as be-
nign or malicious, feature extraction and training of a
na¨ıve Baysian classiﬁer, and ﬁnally, applying the clas-
siﬁer to a new JavaScript context to determine if it is be-
nign or malicious. In the following section, we discuss
the details of each of these stages in turn.
3.2 Training Data Extraction and Labeling
ZOZZLE makes use of a statistical classiﬁer to efﬁciently
identify malicious JavaScript. The classiﬁer needs train-
ing data to accurately classify JavaScript source, and
we describe the process we use to get that training data
here. We start by augmenting the JavaScript engine in
a browser with a “deobfuscator” that extracts and col-
lects individual fragments of JavaScript. As discussed
above, exploits are frequently buried under multiple lev-
els of JavaScript eval. Unlike Nozzle, which observes
the behavior of running JavaScript code, ZOZZLE must
be run on an unobfuscated exploit to reliably detect ma-
licious code.
While detection on obfuscated code may be possible,
examining a fully unpacked exploit is most likely to re-
sult in accurate detection. Rather than attempt to deci-
pher obfuscation techniques, we leverage the simple fact
that an exploit must unpack itself to run.
Our experiments presented in this paper involved
instrumenting the Internet Explorer browser, but we
could have used a different browser such as Firefox or
Chrome instead. Using the Detours binary instrumenta-
tion library [13], we were able to intercept calls to the
Compile function in the JavaScript engine located in the
jscript.dll library. This function is invoked when eval
is called and whenever new code is included with an
 or  tag. This allows us to observe Java-
Script code at each level of its unpacking just before it is
executed by the engine. We refer to each piece of Java-
Script code passed to the Compile function as a code con-
text. For purposes of evaluation, we write out each con-
text to disk for post-processing. In a browser-based im-
plementation, context assessment would happen on the
ﬂy.
3.3 Feature Extraction
Once we have labeled JavaScript contexts, we need to
extract features from them that are predictive of mali-
cious or benign intent. For ZOZZLE, we create features
based on the hierarchical structure of the JavaScript ab-
stract syntax tree (AST). Speciﬁcally, a feature consists
of two parts: a context in which it appears (such as a
loop, conditional, try/catch block, etc.) and the text (or
some substring) of the AST node. For a given JavaScript
context, we only track whether a feature appears or not,
and not the number of occurrences. To efﬁciently ex-
tract features from the AST, we traverse the tree from the
root, pushing AST contexts onto a stack as we descend
and popping them as we ascend.
To limit the possible number of features, we only ex-
tract features from speciﬁc nodes of the AST: expres-
sions and variable declarations. At each of the expression
and variable declarations nodes, a new feature record is
added to that script’s feature set.
If we use the text of every AST expression or variable
declaration observed in the training set as a feature for
the classiﬁer, it will perform poorly. This is because most
of these features are not informative (that is, they are not
correlated with either benign or malicious training set).
To improve classiﬁer performance, we instead pre-select
features from the training set using the χ2 statistic to
identify those features that are useful for classiﬁcation.
A pre-selected feature is added to the script’s feature set
if its text is a substring of the current AST node and the
contexts are equal. The method we used to select these
features is described in the following section.
3.4 Feature Selection
As illustrated in Figure 5, after creating an initial fea-
ture set, ZOZZLE performs a ﬁltering pass to select those
features that are likely to be most predictive. For this
purpose, we used the χ2 algorithm to test for correla-
tion. We include only those features whose presence is
correlated with the categorization of the script (benign or
malicious). The χ2 test (for one degree of freedom) is
described below:
A = malicious contexts with feature
B = benign contexts with feature
C = malicious contexts without feature
D = benign contexts without feature
χ2 =
(A ∗ D − C ∗ B)2
(A + C) ∗ (B + D) ∗ (A + B) ∗ (C + D)
We selected features with χ2 ≥ 10.83, which corre-
sponds with a 99.9% conﬁdence that the two values (fea-
ture presence and script classiﬁcation) are not indepen-
dent.
3.5 Classiﬁer Training
ZOZZLE uses a na¨ıve Bayesian classiﬁer, one of the sim-
plest statistical classiﬁers available. When using na¨ıve
Bayes, all features are assumed to be statistically inde-
pendent. While this assumption is likely incorrect, the
independence assumption has yielded good results in the
past. Because of its simplicity, this classiﬁer is efﬁcient
to train and run.
The probability assigned to label Li for code fragment
containing features F1, . . . , Fn may be computed using
Bayes rule as follows:
P (Li|F1, . . . , Fn) =
P (Li)P (F1, . . . , Fn|Li)
P (F1, . . . , Fn)
Because the denominator is constant regardless of Li we
ignore it for the remainder of the derivation. Leaving
out the denominator and repeatedly applying the rule of
conditional probability, we rewrite this as:
P (Li|F1, . . . , Fn) = P (Li)
P (Fk|F1, . . . , Fk−1, Li)
n(cid:89)
k=1
n(cid:89)
Given that features are assumed to be conditionally inde-
pendent, we can simplify this to:
P (Li|F1, . . . , Fn) = P (Li)
P (Fk|Li)
k=1
Classifying a fragment of JavaScript requires travers-
ing its AST to extract the fragment’s features, multiply-
ing the constituent probabilities of each discovered fea-
ture (actually implemented by adding log-probabilities),
and ﬁnally multiplying by the prior probability of the la-
bel. It is clear from the deﬁnition that classiﬁcation may
be performed in linear time, parameterized by the size
of the code fragment’s AST, the number of features be-
ing examined, and the number of possible labels. The
processes of collecting and hand-categorizing JavaScript
samples and training the ZOZZLE classiﬁer are detailed in
Section 4.
3.6 Fast Pattern Matching
An AST node contains a feature if the feature’s text is a
substring of the AST node. With a na¨ıve approach, each
feature must be matched independently against the node
text. To improve performance, we construct a state ma-
chine for each context that reduces the number of charac-
ter comparisons required. There is a state for each unique
character occurring at each position in the features for a
given context.
A pseudocode for the fast matching algorithm is
shown in Figure 7. State transitions are selected based
on the next character in the node text. Every state has a
bit mask with bits corresponding to features. The bits are
set only for those features that have the state’s incom-
ing character at that position. At the beginning of the
matching, a bitmap is set to all ones. This mask is AND-
ed with the mask at each state visited during matching.
At the end of matching, the bit mask contains the set of
features present in the node. This process is repeated
for each position in the node’s text, as features need not
match at the start of the node.
Example 2 An example of a state machine used for fast
pattern matching is shown in Figure 6. This string match-
ing state machine can identify three patterns: alert,
append, and insert. Assume the matcher is running on
input text appert. During execution, a bit array of size
three, called the matched list, is kept to indicate the pat-
terns that have been matched up to this point in the in-
put. This bit array starts with all bits set. From the left-
most state we follow the edge labeled with the input’s
ﬁrst character, in this case an a.
The match list is bitwise-anded with this new state’s
bit mask of 110. This process is repeated for the input
characters p, p, e. At this point, the match list contains 010
and the remaining input characters are r, t, and null (also
notated as \0). Even though a path to an end state exists
with edges for the remaining input characters, no patterns
will be matched. The next character consumed, an r,
takes the matcher to a state with mask 001 and match
list of 010. Once the match list is masked for this state,
no patterns can possibly be matched. For efﬁciency, the
matcher terminates at this point and returns the empty
match list.
The maximum number of comparisons required to
match an arbitrary input with this matcher is 17, ver-
sus 20 for na¨ıve matching (including null characters at
the ends of strings). The worst-case number of compar-
isons performed by the matcher is the total number of
distinct edge inputs at each input position. The sample
matcher has 19 edges, but at input position 3 two edges
consume the same character (’e’), and at input position 6
two edges consume the null character. In practice, we
ﬁnd that the number of comparisons is reduced signiﬁ-
cantly more than for this sample, due to the large number
of features because of the pigeonhole principle. (cid:3)
For a classiﬁer using 100 features, a single position in
the input text would require 100 character comparisons
with na¨ıve matching. Using the state machine approach,
there can be no more than 52 comparisons at each string
position (36 alphanumeric characters and 16 punctuation
symbols), giving a reduction of nearly 50%. In practice
there are even more features, and input positions do not
require matching against every possible input character.
Figure 8 clearly shows the beneﬁt of fast pattern
matching over a na¨ıve matching algorithm. The graph
shows the average number of character comparisons per-
formed per-feature using both our scheme and a naive
approach that searches an AST node’s text for each pat-
tern individually. As can be seen from the ﬁgure, the
fast matching approach has far fewer comparisons, de-
Figure 6: Fast feature matching illustrated.
matchList ← (cid:104)1, 1, . . . , 1(cid:105)
state ← 0
for all c in input do
state ← matcher.getN extState(state, c)
matchList ← matchList ∧ matcher.getM ask(state)
if matchList(cid:104)0, 0, . . . , 0(cid:105) then
return matchList
end if
end for
return matchList
Figure 7: Fast matching algorithm.
ongoing process; selected features should discriminate
between different clusters, and these clusters will likely
change over time.
3.7.2 Substring Feature Selection
For the current version of ZOZZLE, automatic feature se-
lection only considers the entire text of an AST node as
a potential feature. While simply taking all possible sub-
strings of this and treating those as possible features as
well may seem reasonable, the end result is a classiﬁer
with many more features and little (if any) improvement
in classiﬁcation accuracy.
An alternative approach would be to treat certain types
of AST nodes as “divisible” when collecting candidate
features. If the entire node text is not a good discrimi-
native feature, its component substrings can be selected
as candidate features. This avoids introducing substring
features when the full text is sufﬁciently informative, but
allows for simple patterns to be extracted from longer
text (such as %u or %u0c0c) when they are more informa-
tive than the full string. Not all AST nodes are suitable
for subdivision, however. Fragments of identiﬁers don’t
necessarily make sense, but string constants and numbers
could still be meaningful when split apart.
Figure 8: Comparisons required per-feature with na¨ıve vs. fast pattern
matching. The number of features is shown on the x axis.
3.7.3 Feature Flow
creasing asymptotically as the number of features ap-
proaches 1,500.
3.7 Future Improvements
In this section, we describe additional algorithmic im-
provements not present in our initial implementation.
3.7.1 Automatic Malware Clustering
Using the same features extracted for classiﬁcation, it
is possible to automatically cluster attacks into groups.
There are two possible approaches that exist in this
space: supervised and unsupervised clustering.
Supervised clustering would consist of hand-
categorizing attacks, which has actually already been
done for about 1,000 malicious contexts, and assigning
new scripts to one of these groups. Unsupervised
clustering would not require the initial sorting effort,
and is more likely to successfully identify new, common
attacks.
It is likely that feature selection would be an
At the moment, features are extracted only from the text
of the AST nodes in a given context. This works well for
whole-script classiﬁcation, but has yielded more limited
results for ﬁne-grained classiﬁcation (that is, to identify
that a speciﬁc part of the script is malicious). To prevent
a particular feature from appearing in a particularly infor-
mative context (such as COMMENT appearing inside a loop, a
component the Aurora exploit [19]) an attacker can sim-
ply assign this string to a variable outside the loop and
reference the variable within the loop. The idea behind
feature ﬂow is to keep a simple lookup table for iden-
tiﬁers, where both the identiﬁer name and its value are
used to extract features from an AST node.
By ignoring scoping rules and loops, we can get a rea-
sonable approximation of the features present in both the
identiﬁers and values within a given context with low
overhead. This could be taken one step further by em-
ulating simple operations on values. For example, if two
identiﬁers set to strings are added, the values of these
strings could be concatenated and then searched for fea-
tures. This would prevent attackers from hiding common
shellcode patterns using concatenation.
110001010001100010001011100100100010010001001111ainseertppndlert011100\0\0\005101520253001002003004005006007008009001,0001,1001,2001,3001,4001,500Million Comparisons per Feature Features Naïve MatchingFast Matching4 Experimental Methodology
In order to train and evaluate ZOZZLE, we created a col-
lection of malicious and benign JavaScript samples to use
as training data and for evaluation.
Gathering Malicious Samples: To gather the results
for Section 5, we ﬁrst dynamically scanned URLs with
a browser running both NOZZLE and the ZOZZLE Java-
Script deobfuscator. In this conﬁguration, when NOZZLE
detects a heap spraying exploit, we record the URL and
save to disk all JavaScript contexts seen by the deobfus-
cator. All recorded JavaScript contexts are then hand-
examined to identify those that contain any malware ele-
ments (shellcode, vulnerability, or heap-spray).
Malicious contexts can be sorted efﬁciently by ﬁrst
grouping by their md5 hash value. This dramatically re-
duces the required effort because of the lack of exploit
diversity explained ﬁrst in Section 2 and relatively few
identiﬁer-renaming schemes being employed by attack-
ers. For exploits that do appear with identiﬁer names
changed, there are still usually some identiﬁers left un-
changed (often part of the standard JavaScript API)
which can be identiﬁed using the grep utility. Finally,
hand-examination is used to handle the few remaining
unsorted exploits. Using a combination of these tech-
niques, 919 deobfuscated malicious contexts were iden-
tiﬁed and sorted in several hours.
Gathering Benign Samples: To create a set of benign
JavaScript contexts, we extracted JavaScript from the
Alexa.com top 50 URLs using the ZOZZLE deobfuscator.
The 7,976 contexts gathered from these sites were used
as our benign dataset.
Feature Selection: To evaluate ZOZZLE, we partition our
malicious and benign datasets into training and evalua-
tion data and train a classiﬁer. We then apply this classi-
ﬁer to the withheld samples and compute the false posi-
tive and negative rates. To train a classiﬁer with ZOZZLE,
we ﬁrst need a deﬁne a set of features from the code.
These features can be hand-picked, or automatically se-
lected (as described in Section 3) using the training ex-
amples. In our evaluation, we compare the performance
of classiﬁers built using hand-picked and automatically
selected features.
89
hand-
The
picked