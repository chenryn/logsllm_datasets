a group of similar payload ﬁngerprints with a single prototype ﬁngerprint, and
the algorithm proceeds with approximate clustering analysis using the prototype
ﬁngerprints instead of the original payload ﬁngerprints.
We design two experiments to evaluate the impact of Opt-2 on accuracy:
one experiment (Exp-3) using Opt-1 only, and another experiment (Exp-4) using
Opt-1 and Opt-2. Due to the quartic complexity of the original algorithm, the
overall analysis (using Opt-1 only) will get dramatically slower for analyzing
larger number of malware samples. For instance, we found it takes about one
day to analyze 1000 samples and more than ﬁve days to analyze 2000 samples
for Exp-3. In order to conduct the evaluation within reasonable amount of time,
we randomly select 70% of labeled samples from the largest 4 malware families
and conduct the two experiments for each family. We used the clustering output
generated by Exp-3 as reference, and measured the precision and recall of the
clustering output generated by Exp-4 to evaluate the accuracy impact brought
by Opt-2.
Our experiments showed that on average Exp-4 can speed up more than
95% compared to Exp-3, and the average precision and recall for the analyzed
4 families were 0.955 and 0.932. This optimization makes it feasible to apply
our algorithm to analyze a bigger scale of malware families while providing a
desirable trade-oﬀ option between speed and accuracy.
6 Experiments
In this section, we describe the data preparation procedure, and report malware
clustering results and key ﬁndings of our experiments.
6.1 Data Preparation
We obtained a large collection of potentially malicious Android apps (rang-
ing from late 2010 to early 2016) from various sources, include Google Play,
204
Y. Li et al.
VirusShare [23] and third party security companies. In order to prepare ground-
truth family labeling for the datasets, we queried the collected apps against
VirusTotal [29] around April 2016, and used the scanning results to ﬁlter out
potentially ambiguous apps.
To assign family labels to the collected malware samples, we applied the
following steps: (1) tokenized VirusTotal scanning results and normalized the
contained keywords, and then counted the total number of occurrences of
each keyword. (2) removed all the generic keywords such as Virus, Trojan,
and Malicious. (3) detected keyword aliases by calculating the edit distances
between keywords. For example, Nickyspy, Nickspy, Nicky, and Nickibot were
all consolidated into Nickispy. (4) assigned the dominant keyword as the family
label for the sample. A keyword was considered as dominant if it satisﬁed two
conditions: (a) the count of the keyword was larger than a predeﬁned threshold
t (e.g., t = 10), and (b) the count of the most popular keyword was at least twice
larger than the counts of any other keywords.
Table 2. Clearly labeled malware families
Name
Size Name
Size Name
Size Name
Size Name
Size
Dowgin
3280 Minimob
145 Erop
48
Vidro
23 Koomer
Fakeinst
3138 Gumen
145 Andup
48 Winge
19
Vmvol
Adwo
2702 Basebridge
144 Boxer
44
Penetho
19 Opfake
Plankton
1725 Gingermaster 122 Ksapp
39 Mobiletx
Wapsx
Mecor
Kuguo
Youmi
1657 Appquanta
93 Yzhc
37 Moavt
1604 Geinimi
86 Mtk
1167 Mobidash
83 Adﬂex
35
32
Tekwon
Jsmshider
790 Kyview
80 Fakeplayer 31
Cova
15
13
13
12
12
12
Uuserv
Svpeng
Steek
Spybubble 12
Nickispy
12
Fakeangry 12
Utchi
Lien
Ramnit
11
11
9
19
19
18
18
17
17
16
16
16
16
Droidkungfu
561 Pjapps
245 Bankun
75 Adrd
70 Zitmo
214 Nandrobox
65 Viser
183 Clicker
58 Fakedoc
166 Golddream
54 Stealer
Mseg
Boqx
Airpush
Smskey
Kmin
30
29
26
26
25
Badao
Spambot
Fjcon
Faketimer
Bgserv
158 Androrat
49 Updtkiller 24 Mmarketpay 15
Although our malware labeling process may look similar to AVclass [27], we
developed the approach independently without the knowledge of the AVclass;
and both work was ﬁnished around the same time. The unlabeled samples were
not included in the malware dataset for clustering analysis. In summary, we
collected 19,725 labeled malware samples from 68 diﬀerent families, and the
detailed breakup of the malware samples is shown in Table 2.
Besides the above labeled malware dataset, we also collected Android
Genome malware samples [34] to obtain an optimal clustering threshold, and
randomly selected a list of 10,000 benign samples from AndroZoo [1] to evaluate
the accuracy of the library removal procedure. In particular, we selected benign
Android Malware Clustering Through Malicious Payload Mining
205
apps that were created around the same time (before Jan 1st, 2016) as most of
the labeled malware samples, and their latest (Mar 2017) VirusTotal re-scanning
results showed no malicious labels.
6.2 Feature Collision Analysis
The accuracy of the proposed clustering system and the correctness of the recon-
structed malicious payloads relies on the assumption that unique features will
be mapped to unique bit locations within the bit-vector ﬁngerprint. Feature col-
lision is directly impacted by two parameters: an n-gram size, and a bit-vector
ﬁngerprint size. To evaluate a feature collision rate, we varied the n-gram size
(2 and 4) and the bit-vector ﬁngerprint size, and then measured how many
unique features were mapped to the same single bit position, i.e., feature colli-
sion. Figure 5 illustrates feature collision with regard to diﬀerent n-gram sizes
and ﬁngerprint sizes.
The graph shows that feature collision occurs more frequently when the ﬁn-
gerprint size is small. The total number of unique features depends on the n-
gram size. For the labeled malware, it was about 4.1 million for 2-gram features,
and 14.4 million for 4-gram features. And for the benign dataset, it was about
15.2 million for 2-gram features, and 45.3 million for 4-gram features. Accord-
ing to the pigeonhole principle, when putting N unique features into M buckets,
with N > M, at least one bucket would contain more than one unique features.
This means that we need to set the bit-vector ﬁngerprint size larger than the
total number of unique features to reduce feature collision. Therefore, we set the
default n-gram size to be 2 and default ﬁngerprint size to be 1024 KB which pro-
vides 8,388,608 unique bit positions. With the above conﬁguration, the unique
feature per bit value was reduced to 0.49 to process the labeled malware dataset.
Notice that the complete feature space is unlimited for our system due to the
inclusion of arbitrary string values, however the true unique features contained
in a certain dataset will be limited.
Fig. 5. Random feature collision status
Fig. 6. Benign apps lib removal accuracy
206
Y. Li et al.
6.3 Library Removal Accuracy
Besides the random feature collision discussed in the previous section, it is also
possible that feature collision may happen between the app code and the irrel-
evant versions of the library code. To evaluate the library removal accuracy, we
assumed the libraries used in benign samples were not purposefully manipulated,
and measured the precision (e.g., how much of the removed code is true library
code) and recall (e.g., how much of the true library code is removed) of library
code removal results for the prepared benign samples. Particularly, we consid-
ered the code that were deﬁned under the oﬃcial library names in the benign
samples as ground truth library code, and created the true library code ﬁnger-
print fptrue by mapping all the features from the true library code to a bit-vector
ﬁngerprint. After removing the library code from each app, we identiﬁed the bit
positions that were presented in the original app ﬁngerprint and were removed
subsequently; and used the identiﬁed bit positions to generate removed library
code ﬁngerprint fpremoved. Using the containment ratio calculation function as
discussed in Sect. 3.1, library removal precision Plib is deﬁned as S(fptrue∧fpremoved)
,
, where S(·) denotes
and library removal recall Rlib is deﬁned as S(fptrue∧fpremoved)
the number of 1-bits in the bit-vector.
S(fpremoved)
S(fptrue)
Figure 6 depicts the library removal precision and recall for the benign apps.
We observed that 9,215 benign apps contained at least one legitimate library,
and the median values for precision and recall were 0.94, 0.95, respectively. We
manually inspected certain corner cases with poor precision or recall. The poor
precision cases were due to incomplete true library code extraction, e.g., an older
version of Admob library contained obfuscated version of code which were not
under com.google domain, thus not counted as true library code. The poor
recall cases were due to excessive true library code inclusion, e.g., all the code
of the Androidify app was deﬁned under com.google domain which made the
distinction of library code obscure.
6.4 Malware Clustering Results
In order to select an optimal clustering threshold for the system and assess the
performance comparing with other known Android malware clustering system,
we ﬁrst applied our clustering system on the Android Genome malware dataset.
We used the classical precision and recall [2,12,14,19,22,30] measurements to
evaluate the accuracy of clustering results. Figure 7 describes the clustering pre-
cision and recall results with various thresholds.
The highest F-measure score was 0.82 with precision of 0.90 and recall of 0.75
when the clustering threshold was 0.85. We set the default clustering threshold
value to be 0.85 for subsequent clustering analysis. As a reference, ClusThe-
Droid [17] achieved precision of 0.74 and recall of 0.73 while clustering 939 of
Android Genome malware samples.
Note that the clustering outputs produced by our system is per sub-version
instead of per family, therefore it is more challenging to properly obtain ﬁne-
grained ground truth labels to evaluate the accuracy. In fact, this was the main
Android Malware Clustering Through Malicious Payload Mining
207
Datasets Samples Clusters Precision Recall
D1
D2
D3
D4
D5
D6
D7
D8
D9
D10
1064
1462
1708
1039
2277
1066
1256
1680
2074
1612
33
27
29
31
29
30
29
29
31
31
0.977
0.987
0.985
0.971
0.988
0.971
0.985
0.985
0.996
0.992
0.972
0.964
0.978
0.960
0.989
0.919
0.981
0.980
0.858
0.989
Fig. 7. Clustering results of Android
Genome malware dataset
Fig. 8. Clustering results of diﬀerent
sub-version datasets
reason for a bit low recall of our system with respect to coarse-grained ground
truth labels, e.g., one Android malware family samples might contain multiple
versions of malicious payloads. While reviewing the clustering results, we noticed
that 13 families of the Genome dataset contained more than one versions of
malicious payloads. For example, Basebridge contained 7 versions of malicious
payloads with threshold of 0.85.
Therefore, we separated the labeled malware samples into sub-versions using
the clustering system, and further designed several experiments to evaluate the
clustering results with manually veriﬁed sub-version ground-truth. We manu-
ally veriﬁed the correctness of the sub-version cluster results. For the generated
sub-version clusters, we ﬁrst checked if the extracted payload was the indeed
malicious. Since each version of the extracted payloads usually had similar class
names and Dalvik code sequences, the maliciousness of the extracted payload can
be spotted by checking the extracted class names (e.g., similar pseudo-random
pattern). In case the class names were not enough to determine its malicious-
ness, we then went through the reconstructed code segments and checked if
there were any suspicious activities or behaviors, such as stealthily sending out
premium SMS. After verifying the maliciousness of the extracted payload, we
then randomly selected 3 samples from each sub-version group, and checked if
the selected apps contained the same version malicious payload. Out of 19,725
malware samples that were labeled with 68 families, we obtained a total of 260