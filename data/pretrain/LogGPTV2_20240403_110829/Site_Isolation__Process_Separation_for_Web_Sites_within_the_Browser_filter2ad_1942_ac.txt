### Security Model and Process Consolidation

Our security model mandates that a renderer process must never host documents from different sites, but it can be shared among separate instances of documents from the same site. This is particularly beneficial for users who often keep multiple tabs open, as it allows for process sharing across those tabs.

To minimize the number of processes, we have implemented a process consolidation policy. When creating an out-of-process iframe, this policy checks for an existing same-site process. For example, if a document embeds an `example.com` iframe and another tab already contains an `example.com` frame (either an iframe or a main frame), they are consolidated into the same process. This approach reduces overhead by sacrificing some performance isolation and failure containment, as a slow frame could affect other same-site frames in the same process. However, this trade-off is generally worthwhile for iframes, which typically require fewer resources than main frames.

The same consolidation policy can be applied to main frames, but doing so unconditionally is not ideal. When resource-intensive documents from a site are loaded in multiple tabs, using a single process for all of them can lead to bloated, poorly performing processes. Instead, we apply process consolidation for same-site main frames only after exceeding a soft process limit that approximates memory pressure. Below this limit, main frames in independent tabs do not share processes; above the limit, new frames start reusing same-site processes when possible. The threshold is calculated based on the performance characteristics of the machine. It's important to note that Site Isolation cannot support a hard process limit because the number of sites in the browser may always exceed it.

### Avoiding Non-essential Isolation

Some web content, such as iframes with `data:` URLs or sandboxed same-site iframes, is assigned to an opaque origin without crossing a site boundary. These cases could use separate processes, but we choose to keep them in the same process as an optimization, focusing on true cross-site content.

Other design decisions to reduce process count include isolating at a site granularity rather than origin, keeping cross-site images in-process, and allowing extensions to share processes with each other. Future improvements in these areas are discussed in Section 6.3.

### Reducing the Cost of Process Swaps

Section 3.3 implies that many more navigations must create a new process. To mitigate the latency, we:
1. Start the process in parallel with the network request.
2. Run the old document’s unload handler in the background after the new document is created in the new process.

However, in cases where documents load quickly from the cache (e.g., back/forward navigations), adding process creation latency can significantly slow down the process. To address this, we maintain a warmed-up spare renderer process that can be immediately used by a new navigation to any site. When a spare process is locked to a site and used, a new one is created in the background, similar to process pre-creation optimizations in OP2 [23]. To control memory overhead, we avoid spare processes on low-memory devices, during system memory pressure, or when the browser exceeds the soft process limit.

### Deployment

Deploying Site Isolation in a production browser is challenging due to its highly disruptive nature, affecting significant portions of the browser. Enabling it all at once would pose a high risk of functional regressions. Therefore, we deployed incrementally along two axes: isolation targets and users.

Before launching full Site Isolation, we shipped two milestones to enable process isolation for selective targets:
1. **Extensions**: As the first use of out-of-process iframes, we isolated web iframes embedded inside extension pages and vice versa [50]. This provided a meaningful security improvement, keeping malicious web content out of higher-privileged extension processes. It affected only about 1% of all page loads, reducing the risk of widespread functional regressions.
2. **Selective Isolation**: We created an enterprise policy allowing administrators to optionally isolate a set of manually selected high-value web sites [6].

These preliminary isolation modes provided valuable bug reports and performance data, and demonstrated how some form of isolation can be deployed in environments where full Site Isolation may still be prohibitively expensive, such as on mobile devices.

We also deployed each milestone incrementally to users. All feature work was developed behind an opt-in flag, and we recruited early adopters who provided bug reports. For each milestone (including full Site Isolation), we used Chrome’s A/B testing mechanism [13], initially deploying to a certain percentage of users to monitor performance and stability data.

### Evaluation

To evaluate the effectiveness and practicality of deploying Site Isolation, we address the following questions:
1. How well does Site Isolation upgrade existing security practices to mitigate renderer exploit attacks?
2. How effectively does Site Isolation mitigate transient execution attacks compared to other web browser mitigation strategies?
3. What is the performance impact of Site Isolation in practice?
4. How well does Site Isolation preserve compatibility with existing web content?

#### Mitigating Renderer Vulnerabilities

We added numerous enforcements to Chrome (version 76) to prevent a compromised renderer from accessing cross-site data. This section evaluates these enforcements from the perspective of web developers, specifically asking which existing web security practices have been transparently upgraded to defend against renderer exploit attackers, who have complete control over the renderer process.

**New Protections:**
- **Authentication**: HttpOnly cookies are not delivered to renderer processes, and `document.cookie` is restricted based on a process’s site. Similarly, the password manager only reveals passwords based on a process’s site.
- **Cross-origin messaging**: Both `postMessage` and `BroadcastChannel` messages are only delivered to processes if their sites match the target origin, ensuring that confidential data in the message does not leak to other compromised renderers. Source origins are also verified to ensure incoming messages are trustworthy.
- **Anti-clickjacking**: `X-Frame-Options` is enforced in the browser process, and CSP `frame-ancestors` is enforced in the embedded frame’s renderer process. In both cases, a compromised renderer process cannot bypass these policies to embed a cross-site document.
- **Keeping data confidential**: Many sites use HTML, XML, and JSON to transfer sensitive data. This data is now protected from cross-site renderer processes if it is filtered by CORB (e.g., has a `nosniff` header or can be sniffed), per Section 3.5.
- **Storage and permissions**: Data stored on the client (e.g., in `localStorage`) and permissions granted to a site (e.g., microphone access) are not available to processes for other sites.

**Potential Protections:**
- **Anti-CSRF**: CSRF tokens remain protected from other renderers if they are only present in responses protected by CORB. Origin headers and SameSite cookies can also be used for CSRF defenses, but our enforcement implementation is still in progress.
- **Embedding untrusted documents**: The behavioral restrictions of `iframe` sandbox (e.g., creating new windows or dialogs, navigating other frames) and Feature-Policy are currently enforced in the renderer process, allowing compromised renderers to bypass them. If sandboxed iframes are given separate processes, many of these restrictions could be enforced in the browser process.

**Renderer Vulnerability Analysis:**
We analyzed security bugs reported for Chrome from 2014 to 2018 (extending the analysis by Moroz et al [41]) and found 94 UXSS-like bugs that allow an attacker to bypass the SOP and access contents of cross-origin documents. Site Isolation mitigates such bugs by construction, subject to the limitations discussed in Section 2.2. Similar analyses in prior studies have shown that isolating web principals in different processes prevents a significant number of cross-origin bypasses [19, 63, 68].

In the six months after Site Isolation was deployed in mid-2018, Chrome received only 2 SOP bypass bug reports, also mitigated by Site Isolation (compared to 9 reports in the prior six months). The team continues to welcome and fix such reports, as they still have value on mobile devices where Site Isolation is not yet deployed. We also believe that attention will shift to other classes of bugs seen during this post-launch period, including:
- **Bypassing Site Isolation**: Bugs that exploit flaws in the process assignment or other browser process logic to force cross-site documents to share a process, or to bypass the enforcement logic. For example, we fixed a reported bug where incorrect handling of blob URLs created in opaque origins allowed an attacker to share a victim site’s renderer process.
- **Targeting non-isolated data**: 14 bugs allowed an attacker to steal cross-site images or media, which are not isolated in our architecture, e.g., by exploiting memory corruption bugs or via timing attacks.
- **Cross-process attacks**: 5 bugs are side channel attacks that rely on timing events that work even across processes, such as a frame’s `onload` event, to reveal information about the frame.

In general, we find that Site Isolation significantly improves robustness to renderer exploit attackers, protecting users’ web accounts and lowering the severity of renderer vulnerabilities.

#### Mitigating Transient Execution Attacks

Transient execution attacks represent memory disclosure attackers from Section 2, where lying to the browser process is not possible. Thus, Site Isolation mitigations here depend on process isolation and CORB, but not the enforcements in Section 3.6. This section compares the various web browser mitigation strategies for such attacks, evaluating their effectiveness against known variants.

**Strategy Comparison:**
Web browser vendors have pursued three types of strategies to mitigate transient execution attacks on the web, each with varying strengths and weaknesses.

1. **Reducing the availability of precise timers**: Most browsers attempted to reduce the availability of precise timers that could be used for attacks [14, 39, 48, 67]. This strategy focuses on the most commonly understood exploitation approach for Spectre and Meltdown attacks: a Flush+Reload cache timing attack [75]. This strategy assumes the timing attack will be difficult to perform without precise timers. Most major browsers reduced the granularity of APIs like `performance.now` to 20 microseconds or even 1 millisecond, introduced jitter to timer results, and removed implicit sources of precise time, such as SharedArrayBuffers [59]. This strategy applies whether the attack targets data inside the process or outside of it, but it has several weaknesses:
   - **Incomplete**: There are many ways to build a precise timer [35, 58], making it difficult to enumerate and adjust all sources of time in the platform.
   - **Amplification**: It is possible to amplify the cache timing result to the point of being effective even with coarse-grained timers [25, 37, 58].
   - **Impact on legitimate applications**: Coarsening timers hurts web developers who need precise time for powerful web applications. Disabling SharedArrayBuffers was particularly unfortunate, as it disrupted web applications that relied on them (e.g., AutoCAD).
   - **Limited scope**: Cache timing attacks are only one of several ways to leak information from transient execution, so this approach may be insufficient for preventing data leaks [8].

2. **Modifications to the JavaScript compiler and runtime**: Browser vendors pursued modifications to the JavaScript compiler and runtime to prevent JavaScript code from accessing victim data speculatively [37, 48, 65]. This involved array index masking and pointer poisoning to limit out-of-bounds access, lfence instructions as barriers to speculation, and similar approaches. The motivation for this strategy is to disrupt all “speculation gadgets” to avoid leaking data within and across process boundaries. Unfortunately, there are many variants of transient execution attacks [8], and it is difficult for a compiler to prevent all the ways an attack might be expressed [37]. This is especially true for variants like Spectre-STL (also known as Variant 4), where store-to-load forwarding can be used to leak data [28], or Meltdown-RW, which targets in-process data accessed after a CPU exception [8]. Additionally, some of these mitigations have large performance overheads on certain workloads (up to 15%) [37, 65], which risk slowing down legitimate applications. The difficulty in maintaining a complete defense combined with the performance cost led Chrome’s JavaScript team to conclude that this approach was ultimately impractical [37, 49].

3. **Site Isolation**: Site Isolation offers a third strategy. Rather than targeting the cache timing attack or disrupting speculation, Site Isolation assumes that transient execution attacks may be possible within a given OS process and instead attempts to move data worth stealing outside of the attacker’s address space, much like kernel defenses against Meltdown-US [15, 24].

**Variant Mitigation:**
Canella et al [8] present a systematic evaluation of transient execution attacks and defenses, which we use to evaluate Site Isolation. Spectre attacks rely on branch mispredictions or data dependencies, while Meltdown attacks rely on transient execution after a CPU exception [8]. Table 1 shows how both types of attacks can target data inside or outside the attacker’s process, making both Spectre and Meltdown relevant to consider when mitigating memory disclosure attacks.

Site Isolation mitigates same-address-space attacks by avoiding putting vulnerable data in the same renderer process as a malicious principal. This targets the most practical variants of transient execution attacks, for which an attacker has a large degree of control over the behavior of the process (relative to attacks that target another process). Site Isolation does not depend on the absence of precise timers for its effectiveness.