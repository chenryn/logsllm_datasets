Our security model dictates that a renderer process may
never contain documents hosted at different sites, but a pro-
cess may still be shared across separate instances of doc-
uments from the same site. Fortunately, many users keep
several tabs open, which presents an opportunity for process
sharing across those tabs.
To reduce the process count, we have implemented a pro-
cess consolidation policy that looks for an existing same-site
process when creating an out-of-process iframe. For exam-
ple, when a document embeds an example.com iframe and
another browser tab already contains another example.com
frame (either an iframe or a main frame), we consolidate
them in the same process. This policy is a trade-off that
avoids process overhead by reducing performance isolation
and failure containment: a slow frame could slow down or
crash other same-site frames in the process. We found that
this trade-off is worthwhile for iframes, which tend to require
fewer resources than main frames.
The same policy could also be applied to main frames, but
doing this unconditionally is not desirable: when resource-
heavy documents from a site are loaded in several tabs, using
a single process for all of them leads to bloated processes
that perform poorly. Instead, we use process consolidation
for same-site main frames only after crossing a soft process
limit that approximates memory pressure. When the number
of processes is below this limit, main frames in independent
tabs don’t share processes; when above the limit, all new
frames start reusing same-site processes when possible. Our
threshold is calculated based on performance characteristics
of a given machine. Note that Site Isolation cannot support
a hard process limit, because the number of sites present in
the browser may always exceed it.
4.1.2 Avoiding Non-essential Isolation
Some web content is assigned to an opaque origin [29] with-
out crossing a site boundary, such as iframes with data:
URLs or sandboxed same-site iframes. These could utilize
separate processes, but we choose to keep these cases in-
process as an optimization, focusing our attention on true
cross-site content.
Other design decisions that help reduce process count in-
clude isolating at a site granularity rather than origin, keep-
ing cross-site images in-process, and allowing extensions to
share processes with each other. Section 6.3 discusses im-
proving isolation in these cases in the future.
4.1.3 Reducing the Cost of Process Swaps
Section 3.3 implies that many more navigations must create
a new process. We mask some of this latency by (1) starting
the process in parallel with the network request, and (2) run-
ning the old document’s unload handler in the background
after the new document is created in the new process.
However, in some cases (e.g., back/forward navigations)
documents may load very quickly from the cache. These
cases can be signiﬁcantly slowed by adding process creation
latency. To address this, we maintain a warmed-up spare
renderer process, which may be used immediately by a new
navigation to any site. When a spare process is locked to a
site and used, a new one is created in the background, similar
to process pre-creation optimizations in OP2 [23]. To control
memory overhead, we avoid spare processes on low memory
devices, when the system experiences memory pressure, or
when the browser goes over the soft process limit.
4.2 Deployment
Shipping Site Isolation in a production browser is challeng-
ing.
It is a highly disruptive architecture change affecting
signiﬁcant portions of the browser, so enabling it all at once
would pose a high risk of functional regressions. Hence, we
deployed incrementally along two axes: isolation targets and
users. Before launching full Site Isolation, we shipped two
milestones to enable process isolation for selective targets:
1. Extensions. As the ﬁrst use of out-of-process iframes
from Section 3.4, we isolated web iframes embedded
inside extension pages, and vice versa [50]. This pro-
vided a meaningful security improvement, keeping ma-
USENIX Association
28th USENIX Security Symposium    1667
licious web content out of higher-privileged extension
processes. It also affected only about 1% of all page
loads, reducing the risk of widespread functional re-
gressions.
2. Selective isolation. We created an enterprise policy al-
lowing administrators to optionally isolate a set of man-
ually selected high-value web sites [6].
Deploying these preliminary isolation modes provided a
valuable source of bug reports and performance data (e.g., at
least 24 early issues reported from enterprise policy users).
These modes also show how some form of isolation may be
deployed in environments where full Site Isolation may still
be prohibitively expensive, such as on mobile devices.
We also deployed each of these milestones incrementally
to users. All feature work was developed behind an opt-
in ﬂag, and we recruited early adopters who provided bug
reports. For each milestone (including full Site Isolation),
we also took advantage of Chrome’s A/B testing mecha-
nism [13], initially deploying to only a certain percentage
of users to monitor performance and stability data.
5 Evaluation
To evaluate the effectiveness and practicality of deploying
Site Isolation, we answer the following questions: (1) How
well does Site Isolation upgrade existing security practices to
mitigate renderer exploit attacks? (2) How effectively does
Site Isolation mitigate transient execution attacks, compared
to other web browser mitigation strategies? (3) What is the
performance impact of Site Isolation in practice? (4) How
well does Site Isolation preserve compatibility with exist-
ing web content? Our ﬁndings have allowed us to success-
fully deploy Site Isolation to all desktop and laptop users of
Google Chrome.
5.1 Mitigating Renderer Vulnerabilities
We have added numerous enforcements to Chrome (version
76) to prevent a compromised renderer from accessing cross-
site data.3 This section evaluates these enforcements from
the perspective of web developers. Speciﬁcally, we ask
which existing web security practices have been transpar-
ently upgraded to defend against renderer exploit attackers,
who have complete control over the renderer process.
New Protections. The following web developer practices
were vulnerable to renderer exploit attackers before Site Iso-
lation but are now robust.
• Authentication. HttpOnly cookies are not delivered to
renderer processes, and document.cookie is restricted
based on a process’s site. Similarly, the password man-
ager only reveals passwords based on a process’s site.
• Cross-origin messaging. Both postMessage and
BroadcastChannel messages are only delivered to
3A list of these enforcements is included in Appendix C.
processes if their sites match the target origin, ensur-
ing that conﬁdential data in the message does not leak
to other compromised renderers. Source origins are also
veriﬁed so that incoming messages are trustworthy.
• Anti-clickjacking. X-Frame-Options is enforced in
the browser process, and CSP frame-ancestors is en-
forced in the embedded frame’s renderer process.
In
both cases, a compromised renderer process cannot by-
pass these policies to embed a cross-site document.
• Keeping data conﬁdential. Many sites use HTML,
XML, and JSON to transfer sensitive data. This data
is now protected from cross-site renderer processes if it
is ﬁltered by CORB (e.g., has a nosniff header or can
be sniffed), per Section 3.5.
• Storage and permissions. Data stored on the client
(e.g., in localStorage) and permissions granted to a
site (e.g., microphone access) are not available to pro-
cesses for other sites.
Potential Protections. The Site Isolation architecture
should be capable of upgrading the following practices to
mitigate compromised renderers as well, but our current im-
plementation does not yet fully cover them.
• Anti-CSRF. CSRF [3] tokens remain protected from
other renderers if they are only present in responses pro-
tected by CORB. Origin headers and SameSite cook-
ies can also be used for CSRF defenses, but our enforce-
ment implementation is still in progress.
• Embedding untrusted documents. The behavioral
restrictions of iframe sandbox (e.g., creating new
windows or dialogs, navigating other frames) and
Feature-Policy are currently enforced in the ren-
derer process, allowing compromised renderers to by-
pass them. If sandboxed iframes are given separate pro-
cesses, many of these restrictions could happen in the
browser process.
Renderer Vulnerability Analysis. We also analyzed secu-
rity bugs reported for Chrome for 2014-2018 (extending the
analysis by Moroz et al [41]) and found 94 UXSS-like bugs
that allow an attacker to bypass the SOP and access contents
of cross-origin documents. Site Isolation mitigates such bugs
by construction, subject to the limitations discussed in Sec-
tion 2.2. Similar analyses in prior studies have also shown
that isolating web principals in different processes prevents
a signiﬁcant number of cross-origin bypasses [19, 63, 68].
In the six months after Site Isolation was deployed in mid-
2018, Chrome has received only 2 SOP bypass bug reports,
also mitigated by Site Isolation (compared to 9 reports in
the prior six months). The team continues to welcome and
ﬁx such reports, since they still have value on mobile devices
where Site Isolation is not yet deployed. We also believe that
going forward, attention will shift to other classes of bugs
seen during this post-launch period, including:
1668    28th USENIX Security Symposium
USENIX Association
• Bypassing Site Isolation. These bugs exploit ﬂaws in
the process assignment or other browser process logic
to force cross-site documents to share a process, or to
bypass the enforcement logic. For example, we ﬁxed
a reported bug where incorrect handling of blob URLs
created in opaque origins allowed an attacker to share a
victim site’s renderer process.
• Targeting non-isolated data. For example, 14 bugs al-
lowed an attacker to steal cross-site images or media,
which are not isolated in our architecture, e.g., by ex-
ploiting memory corruption bugs or via timing attacks.
• Cross-process attacks. For example, 5 bugs are side
channel attacks that rely on timing events that work
even across processes, such as a frame’s onload event,
to reveal information about the frame.
In general, we ﬁnd that Site Isolation signiﬁcantly im-
proves robustness to renderer exploit attackers, protecting
users’ web accounts and lowering the severity of renderer
vulnerabilities.
5.2 Mitigating Transient Execution Attacks
Transient execution attacks represent memory disclosure at-
tackers from Section 2, where lying to the browser process
is not possible. Thus, Site Isolation mitigations here depend
on process isolation and CORB, but not the enforcements in
Section 3.6. This section compares the various web browser
mitigation strategies for such attacks, evaluating their effec-
tiveness against known variants.
Strategy Comparison. Web browser vendors have pursued
three types of strategies to mitigate transient execution at-
tacks on the web, with varying strengths and weaknesses.
First, most browsers attempted to reduce the availabil-
ity of precise timers that could be used for attacks [14,
39, 48, 67]. This focuses on the most commonly under-
stood exploitation approach for Spectre and Meltdown at-
tacks: a Flush+Reload cache timing attack [75]. This strat-
egy assumes the timing attack will be difﬁcult to perform
without precise timers. Most major browsers reduced the
granularity of APIs like performance.now to 20 microsec-
onds or even 1 millisecond, introduced jitter to timer results,
and even removed implicit sources of precise time, such as
SharedArrayBuffers [59]. This strategy applies whether
the attack targets data inside the process or outside of it, but
it has a number of weaknesses that limit its effectiveness:
• It is likely incomplete: there are a wide variety of ways
to build a precise timer [35, 58], making it difﬁcult to
enumerate and adjust all sources of time in the platform.
• It is possible to amplify the cache timing result to
the point of being effective even with coarse-grained
timers [25, 37, 58].
• Coarsening timers hurts web developers who have a le-
gitimate need for precise time to build powerful web
applications. Disabling SharedArrayBuffers was a
particularly unfortunate consequence of this strategy,
since it disrupted web applications that relied on them
(e.g., AutoCAD).
• Cache timing attacks are only one of several ways to
leak information from transient execution, so this ap-
proach may be insufﬁcient for preventing data leaks [8].
As a result, we do not view coarsening timers or disabling
SharedArrayBuffers as an effective strategy for mitigat-
ing transient execution attacks.
Second, browser vendors pursued modiﬁcations to the
JavaScript compiler and runtime to prevent JavaScript code
from accessing victim data speculatively [37, 48, 65]. This
involved array index masking and pointer poisoning to limit
out of bounds access, lfence instructions as barriers to
speculation, and similar approaches. The motivation for
this strategy is to disrupt all “speculation gadgets” to avoid
leaking data within and across process boundaries. Un-
fortunately, there are an increasingly large number of vari-
ants of transient execution attacks [8], and it is difﬁcult for
a compiler to prevent all the ways an attack might be ex-
pressed [37]. This is especially true for variants like Spectre-
STL (also known as Variant 4), where store-to-load forward-
ing can be used to leak data [28], or Meltdown-RW which
targets in-process data accessed after a CPU exception [8].
Additionally, some of these mitigations have large perfor-
mance overheads on certain workloads (up to 15%) [37, 65],
which risk slowing down legitimate applications. The dif-
ﬁculty to maintain a complete defense combined with the
performance cost led Chrome’s JavaScript team to conclude
that this approach was ultimately impractical [37, 49].
Site Isolation offers a third strategy. Rather than targeting
the cache timing attack or disrupting speculation, Site Isola-
tion assumes that transient execution attacks may be possible
within a given OS process and instead attempts to move data
worth stealing outside of the attacker’s address space, much
like kernel defenses against Meltdown-US [15, 24].
Variant Mitigation. Canella et al [8] present a systematic
evaluation of transient execution attacks and defenses, which
we use to evaluate Site Isolation. Spectre attacks rely on
branch mispredictions or data dependencies, while Melt-
down attacks rely on transient execution after a CPU excep-
tion [8]. Table 1 shows how both types of attacks are able to
target data inside or outside the attacker’s process, and thus
both Spectre and Meltdown are relevant to consider when
mitigating memory disclosure attacks.
Site Isolation mitigates same-address-space attacks by
avoiding putting vulnerable data in the same renderer pro-
cess as a malicious principal. This targets the most practical
variants of transient execution attacks, for which an attacker
has a large degree of control over the behavior of the process
(relative to attacks that target another process). Site Isola-
tion does not depend on the absence of precise timers for
USENIX Association
28th USENIX Security Symposium    1669
Inside Process Outside Process
n
o
i
t
a
l
o
s
I
e
t
i
S
s
r
e
m
T
i
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35)
-
-
-
-
-
-
-
-
Attack
Spectre-PHT
Spectre-BTB
Spectre-RSB
Spectre-STL
Meltdown-US
Meltdown-P
Meltdown-GP
Meltdown-NM
n
o
i