a
l
e
R
0
−2
−4
−6
−8
−10
−3
L2-LR
L1-LR
−1
−2
SVM-RHL
SVM-SHL
0
1
2
3
True hyperparameter value (log10)
True hyperparameter value (log10)
True hyperparameter value (log10)
True hyperparameter value (log10)
(a) Regression
(b) Classiﬁcation
(a) Diabetes
(b) Iris
Fig. 5: Effectiveness of our hyperparameter stealing attacks for
a) a three-layer neural network regression algorithm and b) a
three-layer neural network classiﬁcation algorithm.
Fig. 7: Effectiveness of our hyperparameter stealing attacks
when model parameters are unknown but stolen by model pa-
rameter stealing attacks. (a) Regression algorithms on Diabetes
and (b) Classiﬁcation algorithms on Iris.
ˆλ
Δ
0.020
0.016
0.012
0.008
0.004
0.000
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Δw
Fig. 6: Effectiveness of our hyperparameter stealing attacks for
RR when the model parameters deviate from the optimal ones.
We have two key observations. First, our attacks can accu-
rately estimate the hyperparameter for all learning algorithms
we studied and for a wide range of hyperparameter values. Sec-
ond, we observe that our attacks can more accurately estimate
the hyperparameter for Ridge Regression (RR) and Kernel
Ridge Regression (KRR) than for other learning algorithms.
This is because RR and KRR have analytical solutions for
model parameters, and thus the learnt model parameters are
the exact minima of the objective functions. In contrast, other
learning algorithms we studied do not have analytical solutions
for model parameters, and their learnt model parameters are
relatively further away from the corresponding minima of
the objective functions. Therefore, our attacks have larger
estimation errors for these learning algorithms.
In practice, a learner may use approximate solutions for
RR and KRR because computing the exact optimal solutions
may be computationally expensive. We evaluate the impact of
such approximate solutions on the accuracy of hyperparameter
stealing attacks, and compare the results with those predicted
by our Theorem 2. Speciﬁcally, we use the RR algorithm,
adopt the Diabetes dataset, and set the true hyperparameter to
be 1. We ﬁrst compute the optimal model parameters for RR.
Then, we modify a model parameter by Δw and estimate the
hyperparameter by our attack. Figure 6 shows the estimation
error Δˆλ as a function of Δw (we show the absolute estimation
error instead of relative estimation error in order to compare
the results with Theorem 2). We observe that when Δw is very
small, the estimation error Δˆλ is a linear function of Δw. As
Δw becomes larger, Δˆλ increases quadratically with Δw. Our
observation is consistent with Theorem 2, which shows that the
estimation error is linear to the difference between the learnt
model parameters and the minimum closest to them when the
difference is very small.
3) Experimental Results for Unknown Model Parameters:
Our hyperparameter stealing attacks are still applicable when
the model parameters are unknown to an attacker, e.g., for
black-box MLaaS platforms such as Amazon Machine Learn-
ing. Speciﬁcally, the attacker can ﬁrst use the equation-solving-
based model parameter stealing attacks proposed in [54] to
learn the model parameters and then perform our hyperparam-
eter stealing attacks. Our Theorem 2 bounds the estimation
error of hyperparameters with respect to the difference between
the stolen model parameters and the closest minimum of the
objective function of the ML algorithm.
We also empirically evaluate the effectiveness of our at-
tacks when model parameters are unknown. For instance, Fig-
ure 7 shows the relative estimation errors of hyperparameters
for regression algorithms and classiﬁcation algorithms, when
the model parameters are unknown but stolen by the model
parameter stealing attacks [54]. For simplicity, we only show
results on the Diabetes dataset for regression algorithms and
on the Iris dataset for classiﬁcation algorithms, but results
on other datasets are similar. Note that LASSO requires the
hyperparameter to be smaller than a certain threshold as
we discussed in the above, and thus some data points are
missing for LASSO. We ﬁnd that we can still accurately steal
the hyperparameters. The reason is that the model parameter
stealing attacks can accurately steal the model parameters.
4) Summary: Via empirical evaluations, we have the fol-
lowing observations. First, our attacks can accurately estimate
the hyperparameter for all ML algorithms we studied. Second,
our attacks can more accurately estimate the hyperparameter
for ML algorithms that have analytical solutions of the model
parameters. Third, via combining with model parameter steal-
ing attacks, our attacks can accurately estimate the hyperpa-
rameter even if the model parameters are unknown.
C. Implications for MLaaS
We show that a user can use our hyperparamter stealing
attacks to learn an accurate model through a machine-learning-
as-a-service (MLaaS) platform with much less costs. While
different MLaaS platforms have different paradigms, we con-
sider an MLaaS platform (e.g., Amazon Machine Learning [1],
Microsoft Azure Machine Learning [25]) that charges a user
according to the amount of computation that
the MLaaS
platform performed to learn the model, and supports two
43
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
protocols for a user to learn a model. In the ﬁrst protocol
(denoted as Protocol I), the user uploads a training dataset to
the MLaaS platform and speciﬁes a learning algorithm; the
MLaaS platform learns the hyperparameter using proprietary
algorithms and learns the model parameters with the learnt
hyperparameter; and then (optionally) the model parameters
are sent back to the user. When the model parameters are
not sent back to the user, the MLaaS is called black-box.
The MLaaS platform (e.g., a black-box platform) does not
share the learnt hyperparameter value with the user considering
intellectual property and algorithm conﬁdentiality.
In Protocol I, learning the hyperparameter is often the
most time-consuming and costly part, because it more or less
involves cross-validation. In practice, some users might already
have appropriate hyperparameters through domain knowledge.
Therefore, the MLaaS platform provides a second protocol
(denoted as Protocol II), in which the user uploads a training
dataset to the MLaaS platform, deﬁnes a hyperparameter value,
and speciﬁes a learning algorithm, and then the MLaaS plat-
form produces the model parameters for the given hyperparam-
eter. Protocol II helps users learn models with less economical
costs when they already have good hyperparameters. We note
that Amazon Machine Learning and Microsoft Azure Machine
Learning support the two protocols.
1) Learning an Accurate Model with Less Costs: We
demonstrate that a user can use our hyperparameter stealing
attacks to learn a model
through MLaaS with much less
economical costs without sacriﬁcing model performance. In
particular, we assume the user does not have a good hyperpa-
rameter yet. We compare the following three methods to learn
a model through MLaaS. By default, we assume the MLaaS
shares the model parameters with the user. If not, the user can
use model parameter stealing attacks [54] to steal them.
Method 1 (M1): The user leverages Protocol I supported
by the MLaaS platform to learn the model. Speciﬁcally, the
user uploads the training dataset to the MLaaS platform and
speciﬁes a learning algorithm. The MLaaS platform learns the
hyperparameter and then learns the model parameters using
the learnt hyperparameter. The user then downloads the model
parameters.
Method 2 (M2):
In order to save economical costs, the
user samples p% of the training dataset uniformly at random
and then uses Protocol I to learn a model over the sampled
subset of the training dataset. We expect that this method is
less computationally expensive than M1, but it may sacriﬁce
performance of the learnt model.
Method 3 (M3):
In this method, the user uses our hyperpa-
rameter stealing attacks. Speciﬁcally, the user ﬁrst samples q%
of the training dataset uniformly at random. Second, the user
learns a model over the sampled training dataset through the
MLaaS via Protocol I. We note that, for big data, even a very
small fraction (e.g., 1%) of the training dataset could be too
large for the user to process locally, so we consider the user
uses the MLaaS. Third, the user estimates the hyperparamter
learnt by the MLaaS using our hyperparameter stealing attacks.
Fourth, the user re-learns a model over the entire training
dataset through the MLaaS via Protocol II. We call this strategy
“Train-Steal-Retrain”.
2) Comparing the Three Methods Empirically: We ﬁrst
show simulation results of the three methods. For these simu-
lation results, we assume model parameters are known to the
user. In the next subsection, we compare the three methods on
Amazon Machine Learning, a real-world MLaaS platform.
Setup: For each dataset in Table II, we randomly split it
into two halves, which are used as the training dataset and the
testing dataset, respectively. We consider the MLaaS learns the
hyperparameter through 5-fold cross-validation on the training
dataset. We measure the performance of the learnt model
through mean square error (MSE) (for regression models) or
accuracy (ACC) (for classiﬁcation models). MSE and ACC
are formally deﬁned in Section II. Speciﬁcally, we use M1
as a baseline; then we measure the relative MSE (or ACC)
error of M2 and M3 over M1. For example, the relative MSE
. Moreover, we also
error of M3 is deﬁned as
measure the speedup of M2 and M3 over M1 with respect to
the overall amount of computation required to learn the model.
Note that we also include the computation required to steal the
hyperparameter for M3.
|MSEM 3−MSEM 1|
MSEM 1
M3 vs. M1: Figure 8 compares M3 with M1 with respect
to model performance (measured by relative performance of
M3 over M1) and speedup as we sample a larger fraction
of training dataset (i.e., q gets larger), where the regression
algorithm is RR and the classiﬁcation algorithm is SVM-SHL.
Other learning algorithms and datasets have similar results, so
we omit them for conciseness.
We observe that M3 can learn a model that is as accurate as
the model learnt by M1, while saving a signiﬁcant amount of
computation. Speciﬁcally, for RR on the dataset UJIndoorLoc,
when we sample 3% of training dataset, M3 learns a model
that has almost 0 relative MSE error over M1, but M3 is around
8 times faster than M1. This means that the user can learn an
accurate model using M3 with much less economic costs, when
the MLaaS platform charges the user according to the amount
of computation. For the SVM-SHL algorithm on the Bank
dataset, M3 can learn a model that has almost 0 relative ACC
error over M1 and is around 15 times faster than M1, when we
sample 1% of training dataset. The reason why M3 and M1
can learn models with similar performances is that learning the
hyperparameter using a subset of the training dataset changes
it slightly and the learning algorithms are relatively robust to
small variations of the hyperparameter.
Moreover, we observe that the speedup of M3 over M1
is more signiﬁcant when the training dataset becomes larger.
Figure 9 shows the speedup of M3 over M1 on binary-class
training datasets with different sizes, where each class is
synthesized via a Gaussian distribution with 10 dimensions.
Entries of the mean vectors of the two Gaussian distributions
are all 1’s and all -1’s, respectively. Entries of the covariance
matrix of the two Gaussian distributions are generated from the
standard Gaussian distribution. We select the parameter q% in
M3 such that the relative ACC error is smaller than 0.1%, i.e.,
M3 learns a model as accurately as M1. The speedup of M3
over M1 is more signiﬁcant as the training dataset gets larger.
This is because the process of learning the hyperparameter
has a computational complexity that is higher than linear.
M1 learns the hyperparameter over the entire training dataset,
while M3 learns it on a sampled subset of training dataset.
44
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
r
o
r
r
e
E
S
M
e
v
i
t
a
l
e
R
0.10
0.08
0.06
0.04
0.02
0.00
p
u
d
e
e
p
S
20
16
12
8
4
0
M3 over M1
1
2
3
4
5
6
7
8
9
10
3
2
1
10
q%, percentage of training instances
6
7
8
9
4
5
r
o
r
r
e
C
C
A
e
v
i
t
a
l
e
R
0.10
0.08
0.06
0.04
0.02
0.00
p
u
d
e
e
p
S
15
12
9
6
3
0
M3 over M1
1
2
3
4
5
6
7
8
9
10
3
2
1
10
q%, percentage of training instances
4
5
6
7
8
9
r
o
r
r
e
E
S
M
e
v
i
t
a
l
e
R
0.30
0.24
0.18
0.12
0.06
0.00
p
u
d
e
e
p
S
60
48
36
24
12
0
M2 over M1
M3 over M1
1
2
3
4
5
6
7
8
9
10
3
2
1
10
p%, percentage of training instances
4
5
6
7
8
9
r
o
r
r
e
C
C
A
e
v
i
t
a
l
e
R
0.30
0.24
0.18
0.12
0.06
0.00
p
u
d
e
e
p
S
25
20
15
10
5
0
M2 over M1
M3 over M1
1
2
3
4
5
6
7
8
9
10
3
2
1
10
p%, percentage of training instances
4
5
6
7
8
9
(a)
(b)
(a)
(b)
Fig. 8: M3 vs. M1. (a) Relative MSE error and speedup of
M3 over M1 for RR on the dataset UJIndoorLoc. (b) Relative
ACC error and speedup of M3 over M1 for SVM-SHL on the
dataset Bank.
Fig. 10: M3 vs. M2. (a) Relative MSE error and speedup of
M3 and M2 over M1 for RR on the dataset UJIndoorLoc. (b)
Relative ACC error and speedup of M3 and M2 over M1 for
SVM-SHL on the dataset Bank.
M3 over M1
300
240
180
120
60
p
u
d
e
e
p
S
0
3
7
Number of training instances (log10)
4
5