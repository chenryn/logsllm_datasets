ularly the number of different locations from where the developers performed
their work require additional research in order to get any generalized conclu-
sions about this insight.
5.4. RQ4: Using only process metrics, are we able to model accu-
rately the expected level of complexity variance after a refac-
toring task?
To answer this RQ, we used not only the metrics from Table A.8, but
also the ones from Table A.9. During our analysis, it was clear that process
extendedmetrics,representingthecommandsissuedbyeachdeveloper/team,
added significant predictive power to the models computed. Therefore, to
predict the expected software cyclomatic complexity we needed to include
individual commands frequencies in addition to the process metrics used in
previous RQ. By doing this we were able to achieve models with higher
accuracy and good ROC values. However, in general, these models have
lower accuracy than the ones in RQ3.
Table 6 shows the top five models computed to predict the complexity
levelgainsobtainedafterarefactoringsession,eitherusingadedicatedplugin
or simply by using Eclipse features.
Observation 11: Locally Weighted Learning combined with a
Decision Table outperforms Random Forrest. Contrary to the pre-
vious RQ, in this case the best model is not based on a Random Forrest
algorithm. However, the latter show as the second best model in terms of ac-
curacy. The Locally Weighted Learning method uses an instance-based algo-
rithm to assign instance weights which are then used by a specified weighted
instances handler. It uses a stack of methods, initially a cluster like mech-
anism such as the LinearNNSearch and then a Decision Table to classify
the outcome. This shows up at no surprise since Decision Tables use the
simplest hypothesis spaces possible and usually outperform state-of-the-art
classification algorithms.
35
Table 6: Detailed Model Evaluation
Model TP FP Pre. Rec. F-M. MCC ROC PRC
Model 1, LWL/LinearNNSearch/DecisionTable, Accuracy = 94.36%
LOW 0.968 0.000 1.000 0.968 0.984 0.972 0.991 0.992
MEDIUM 1.000 0.095 0.879 1.000 0.935 0.892 0.994 0.992
HIGH 0.727 0.000 1.000 0.727 0.842 0.832 0.992 0.967
Weighted Avg. 0.944 0.039 0.950 0.944 0.942 0.917 0.993 0.988
Model 2, Bagging/RandomForest, Accuracy = 83.09%
LOW 0.839 0.075 0.897 0.839 0.867 0.771 0.938 0.94
MEDIUM 0.828 0.095 0.857 0.828 0.842 0.737 0.971 0.945
HIGH 0.818 0.083 0.643 0.818 0.720 0.668 0.971 0.827
Weighted Avg. 0.831 0.085 0.841 0.831 0.834 0.741 0.957 0.926
Model 3, KStar, Accuracy = 78.87%
LOW 0.935 0.225 0.763 0.935 0.841 0.707 0.948 0.951
MEDIUM 0.862 0.143 0.806 0.862 0.833 0.713 0.945 0.915
HIGH 0.182 0.000 1.000 0.182 0.308 0.398 0.982 0.904
Weighted Avg. 0.789 0.157 0.818 0.789 0.755 0.661 0.952 0.929
Model 4, RandomCommittee/REPTree, Accuracy = 74.64%
LOW 0.903 0.300 0.700 0.903 0.789 0.603 0.895 0.873
MEDIUM 0.759 0.143 0.786 0.759 0.772 0.619 0.886 0.847
HIGH 0.273 0.000 1.000 0.273 0.429 0.491 0.932 0.738
Weighted Avg. 0.746 0.189 0.781 0.746 0.726 0.592 0.897 0.842
Model 5, LWL/LinearNNSearch/DecisionTable, Accuracy = 71.83%
LOW 0.871 0.300 0.692 0.871 0.771 0.569 0.843 0.803
MEDIUM 0.759 0.190 0.733 0.759 0.746 0.565 0.800 0.729
HIGH 0.182 0.000 1.000 0.182 0.308 0.398 0.823 0.541
Weighted Avg. 0.718 0.209 0.757 0.718 0.689 0.541 0.822 0.732
TP-True Positive, FP-False Positive, Pre-Precision, Rec-Recall,
F-M-F-Measure, MCC-Matthews Correlation Coefficient,
ROC-Receiver Operating Characteristic, PRC-Precision-Recall Curve,
LOW-Low level of Cyclomatic Complexity,
MEDIUM-Medium level of Cyclomatic Complexity,
HIGH-High level of Cyclomatic Complexity,
W. Avg-Weighted Average
Observation 12: Teams with LOW level of software complexity
gains are frequently spotted with higher F-Measure and ROC val-
ues. Our models perform better in detecting subjects achieving low levels of
complexity reduction. These are the most critical cases, as such, a software
development project manager can quickly detect the teams or individuals
responsible for those outcomes and implement actions to bring the project
under acceptable quality parameters.
36
Observation13: Processextendedmetricshaveingeneralhigher
importance than process standard metrics. From Figure 11 we can un-
derstand that 18 out of 30 metrics are related with the commands issued by
the developers. In general, these metrics have also higher importance in the
models. It is not surprising to find methods and class extraction commands
in the top of the list, with ≈86% and ≈56% importance, respectively. It was
however unexpected to find project export actions being so relevant (≈70%).
100.00% serutaeF
Eclipse.View.Feature.Envy
86.09%
Refactor...Java.Extract.Method
70.67%
File.Export
65.66%
NFILES
59.94%
NSS
58.24%
EVTS
57.48%
NOT
56.67%
Refactor...Java.Extract.Class...
52.35%
EC
48.85%
File.Save
47.85%
Eclipse.Editor.File.Close
47.75%
Eclipse.Editor.File.Editing
47.55%
NOA
47.05%
NCS
46.18%
Eclipse.View.Type.Checking
44.84%
PCC
43.95%
File.Import
43.72%
SES
43.13%
Eclipse.View.Package.Explorer
42.15%
Eclipse.Editor.File.Open
39.68%
NCOM
39.14%
PCCPF
38.59%
Edit.Paste
37.42%
Eclipse.View.God.Class
36.73%
Eclipse.View.Long.Method
36.62%
Eclipse.View.Duplicated.Code
36.31%
Edit.Undo
34.25%
NVER
34.09%
Refactor...Java.Move...Refactoring
33.64%
Edit.Copy
0 25 50 75 100
Importance
Figure 11: Feature importance for models on Table 6 (Top 30 only)
37
6. Threats to Validity
The following types of validity issues were considered when interpreting
the results from this article.
6.1. Construct Validity
We acknowledge that this work was supported by an academic environ-
ment and using subjects with different maturity and skills which we did not
assessed deeply upfront. Additionally, although some work has been done
in this domain, we are still just scratching the surface in mining develop-
ers’ activities using process mining tools. Some of these tools are not ready
yet to automate the complete flow of: collecting data, discover processes,
compute metrics and export results. As a consequence, the referred tasks
were mostly done manually, thus, introducing margin for errors in the data
metrics used in the experiment. To soften this, and to reduced the risk of
having incoherent data, we implemented the validation of metric values from
multiple perspectives. Another possible threat is related with the event data
pre-processing tasks before using the Process Mining tools to discover the
processes and associated metrics. Events were stored initially in a database,
and from there, queries were issued to filter, aggregate and select some pro-
cess related metrics. We used all the best practices in filtering and querying
the data. However, there is always a small chance for the existence of an
imprecise query which may have produced incorrect results and therefore,
impacted our data analysis.
6.2. Internal Validity
We used a cluster analysis technique supported by the Elbow and Silhou-
ette methods. This was used to partition the subjects according to different
software and process cyclomatic complexity levels. Even if this is a valid
approach, other strategies could have been followed, thus, results could vary
depending on the alternative methods used, since the models computed to
address RQ3 and RQ4 make use of this data partition approach. As men-
tioned earlier, our population was not very large and we had to use it for
training and test purposes. As such, our prediction models were all trained
using k-fold cross validation and using feature selection methods.
6.3. External Validity
We understood from the beginning there was a real possibility that events
collected and stored in CSV/JSON files on developers’ devices could be man-
ually changed. We tried to mitigate this threat of having data tampering by
using a hash function on each event at the moment of their creation. As such,
38
each event contain not only information about the IDE activities, but also
a hash code introduced as a new property in the event for later comparison
with original event data. For additional precautions regarding data losses,
weimplementedalso real-timeeventstreaming fromthe IDEto acloudevent
hub.
Our initial dataset contains events collected from a group of teams when
performing an academic exercise. Each user was provided with a username
and password to enter in the Eclipse Plugin. With this approach, we can
easily know which user was working on each part of the software and their
role in the whole development process. However, we cannot guarantee that
each developer used indeed their own username. This does not cause any
invalid results in the number of activities for example, but may introduce
some bias in the number of developers per team18.
6.4. Conclusion Validity
Weperformedanexperimentusingdatafrom71softwareteamsexecuting
well defined refactoring tasks. This involved 320 sessions of work from 117
developers. Since this is a moderate population size for this type of analysis,
we acknowledge this may be a threat to generalize conclusions or make bold
assertions. The Spearmmans’ correlation, a nonparametric measure (there-
fore having less statistical power) of the strength and direction of association
thatexistsbetweentwovariables, wasdoneon32and39teamsforautomatic
and manual refactoring tasks respectively. These figures, although valid, are
close to the minimum admissible number of subjects for this type of analysis.
Nevertheless, the insights we unveil in this study should be able to trigger
additional research in order to confirm or invalidate our initial findings.
7. Conclusion
7.1. Main conclusions
Software maintenance activities, such as refactoring, are said to be very
impacted by software complexity. Researchers are measuring software prod-
uct and processes complexities for a long time and the methods used are
frequently debated in the software development realm. However, the com-
prehension on the links between these two dimensions has a long journey
ahead.
In this work, we tried to understand deeper the liaison of process and
software complexity. Moreover, we assessed if process driven metrics and
18A metric used on almost all RQs and identified as having a high importance
39
IDE issued commands are suitable to build valid models to predict differ-
ent refactoring methods and/or the expected levels of software cyclomatic
complexity variance on development sessions.
We mined the software metrics from a software product after a software
quality improvement task was given to a group of developers organized in
teams. At the same time we collected events from those developers during
the change activities they have performed within the IDE.
To the best of our knowledge, this is the first study where, using proven
process mining methods, process metrics were gathered and combined with
product metrics in order to understand deeper the liaison of product and
processdimensions,particularlythecyclomaticcomplexities. Furthermore,it
brings to the attention of researchers the possibility to adopt process metrics
extractedfromtheIDEusageasawaytocomplementorevenreplaceproduct
metrics in modeling the development process.
We can’t compare our study to any previous works, however, with a
small set of features, we were able to unveil important correlations between
productandprocessdimensionsandobtaingoodmodelsintermsofaccuracy
and ROC when predicting the type of refactoring done or the expected level
of cyclomatic complexity variance after multiple sessions of development. We
used a refactoring task as our main use case, however, by taking a snapshot
ofproductandprocessmetricsindifferentmomentsintime, onecanmeasure
other development practices the same way.
7.2. Relevance for practitioners
Thisapproach canbe particularlyrelevant incases whereproduct metrics
are not available or are difficult to obtain. It can be also a valid approach to
measure and monitor productivity within and between software teams. As
we showed by analyzing the sessions complexity and the software cyclomatic
complexity variance, non efficient teams can easily be detected. Our method
easily support real-time data collection from individuals located in differ-
ent geographic zones and with a multitude of development environments.
Because the data collection is not dependent on code repositories and is de-
coupled from check-ins and/or commits, process and code analysis can be
performed before repositories are updated. Development organizations can
leverage this approach to apply conformance checking methods to verify the
adherence of developers’ practices with internally prescribed development
processes. This facilitates mainly the detection of low performance practices
and may trigger quick correction actions from project managers.
40
7.3. Limitations
We are aware that in this work we used only events from the IDE usage.
This limits the generalization of the current method. However, our approach,
although valid on it’s own, may be used to complement project management
analysis based on other repositories. Events from tools containing informa-
tion about the documentation, project management decisions, communica-
tion between developers and managers, Q & A services, test suites and bug
tracking systems, together with our method and metrics can build more ro-