### 5.4. RQ4: Using Only Process Metrics, Are We Able to Model Accurately the Expected Level of Complexity Variance After a Refactoring Task?

To address this research question (RQ), we utilized not only the metrics from Table A.8 but also those from Table A.9. Our analysis revealed that process-extended metrics, which represent the commands issued by each developer or team, significantly enhanced the predictive power of our models. Therefore, to predict the expected software cyclomatic complexity, we needed to include individual command frequencies in addition to the process metrics used in previous RQs. By incorporating these additional metrics, we were able to achieve models with higher accuracy and good ROC values. However, in general, these models had lower accuracy compared to those in RQ3.

**Table 6: Detailed Model Evaluation**

| Model | TP | FP | Precision | Recall | F-Measure | MCC | ROC | PRC |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **Model 1, LWL/LinearNNSearch/DecisionTable, Accuracy = 94.36%** |  |  |  |  |  |  |  |  |
| LOW | 0.968 | 0.000 | 1.000 | 0.968 | 0.984 | 0.972 | 0.991 | 0.992 |
| MEDIUM | 1.000 | 0.095 | 0.879 | 1.000 | 0.935 | 0.892 | 0.994 | 0.992 |
| HIGH | 0.727 | 0.000 | 1.000 | 0.727 | 0.842 | 0.832 | 0.992 | 0.967 |
| Weighted Avg. | 0.944 | 0.039 | 0.950 | 0.944 | 0.942 | 0.917 | 0.993 | 0.988 |
| **Model 2, Bagging/RandomForest, Accuracy = 83.09%** |  |  |  |  |  |  |  |  |
| LOW | 0.839 | 0.075 | 0.897 | 0.839 | 0.867 | 0.771 | 0.938 | 0.94 |
| MEDIUM | 0.828 | 0.095 | 0.857 | 0.828 | 0.842 | 0.737 | 0.971 | 0.945 |
| HIGH | 0.818 | 0.083 | 0.643 | 0.818 | 0.720 | 0.668 | 0.971 | 0.827 |
| Weighted Avg. | 0.831 | 0.085 | 0.841 | 0.831 | 0.834 | 0.741 | 0.957 | 0.926 |
| **Model 3, KStar, Accuracy = 78.87%** |  |  |  |  |  |  |  |  |
| LOW | 0.935 | 0.225 | 0.763 | 0.935 | 0.841 | 0.707 | 0.948 | 0.951 |
| MEDIUM | 0.862 | 0.143 | 0.806 | 0.862 | 0.833 | 0.713 | 0.945 | 0.915 |
| HIGH | 0.182 | 0.000 | 1.000 | 0.182 | 0.308 | 0.398 | 0.982 | 0.904 |
| Weighted Avg. | 0.789 | 0.157 | 0.818 | 0.789 | 0.755 | 0.661 | 0.952 | 0.929 |
| **Model 4, RandomCommittee/REPTree, Accuracy = 74.64%** |  |  |  |  |  |  |  |  |
| LOW | 0.903 | 0.300 | 0.700 | 0.903 | 0.789 | 0.603 | 0.895 | 0.873 |
| MEDIUM | 0.759 | 0.143 | 0.786 | 0.759 | 0.772 | 0.619 | 0.886 | 0.847 |
| HIGH | 0.273 | 0.000 | 1.000 | 0.273 | 0.429 | 0.491 | 0.932 | 0.738 |
| Weighted Avg. | 0.746 | 0.189 | 0.781 | 0.746 | 0.726 | 0.592 | 0.897 | 0.842 |
| **Model 5, LWL/LinearNNSearch/DecisionTable, Accuracy = 71.83%** |  |  |  |  |  |  |  |  |
| LOW | 0.871 | 0.300 | 0.692 | 0.871 | 0.771 | 0.569 | 0.843 | 0.803 |
| MEDIUM | 0.759 | 0.190 | 0.733 | 0.759 | 0.746 | 0.565 | 0.800 | 0.729 |
| HIGH | 0.182 | 0.000 | 1.000 | 0.182 | 0.308 | 0.398 | 0.823 | 0.541 |
| Weighted Avg. | 0.718 | 0.209 | 0.757 | 0.718 | 0.689 | 0.541 | 0.822 | 0.732 |

**Observation 11: Locally Weighted Learning Combined with a Decision Table Outperforms Random Forest.**
Contrary to the previous RQ, in this case, the best model is not based on a Random Forest algorithm. The Locally Weighted Learning (LWL) method, combined with a Decision Table, outperformed the Random Forest. The LWL method uses an instance-based algorithm to assign instance weights, which are then used by a specified weighted instances handler. It employs a stack of methods, initially a cluster-like mechanism such as the LinearNNSearch, followed by a Decision Table to classify the outcome. This result is not surprising, as Decision Tables use the simplest hypothesis spaces possible and often outperform state-of-the-art classification algorithms.

**Observation 12: Teams with Low Levels of Software Complexity Gains Show Higher F-Measure and ROC Values.**
Our models perform better in detecting subjects achieving low levels of complexity reduction. These are the most critical cases, as a software development project manager can quickly identify the teams or individuals responsible for these outcomes and implement corrective actions to bring the project under acceptable quality parameters.

**Observation 13: Process-Extended Metrics Have Generally Higher Importance than Standard Process Metrics.**
From Figure 11, it is evident that 18 out of 30 metrics are related to the commands issued by the developers. In general, these metrics have higher importance in the models. It is not surprising to find methods and class extraction commands at the top of the list, with approximately 86% and 56% importance, respectively. However, it was unexpected to find project export actions being so relevant (approximately 70%).

**Figure 11: Feature Importance for Models in Table 6 (Top 30 Only)**

**6. Threats to Validity**

**6.1. Construct Validity**
This work was conducted in an academic environment with subjects of varying maturity and skills, which we did not assess deeply upfront. Additionally, while some progress has been made in mining developers' activities using process mining tools, many of these tools are not yet fully automated. As a result, tasks such as collecting data, discovering processes, computing metrics, and exporting results were mostly done manually, introducing a margin for errors. To mitigate this, we validated metric values from multiple perspectives. Another potential threat is related to the event data pre-processing tasks before using the Process Mining tools. Although we followed best practices, there is always a small chance of imprecise queries that may have produced incorrect results, impacting our data analysis.

**6.2. Internal Validity**
We used a cluster analysis technique supported by the Elbow and Silhouette methods to partition the subjects according to different software and process cyclomatic complexity levels. While this is a valid approach, other strategies could have been used, potentially leading to different results. Our population was not very large, and we had to use it for both training and testing. Therefore, our prediction models were all trained using k-fold cross-validation and feature selection methods.

**6.3. External Validity**
There was a real possibility that events collected and stored in CSV/JSON files on developers' devices could be manually changed. We mitigated this threat by using a hash function on each event at the moment of its creation. Each event contains not only information about the IDE activities but also a hash code for later comparison with the original event data. For additional precautions against data loss, we implemented real-time event streaming from the IDE to a cloud event hub.

Our initial dataset contains events collected from a group of teams performing an academic exercise. Each user was provided with a username and password to enter the Eclipse Plugin. This approach allows us to easily track which user worked on each part of the software and their role in the development process. However, we cannot guarantee that each developer used their own username, which may introduce some bias in the number of developers per team.

**6.4. Conclusion Validity**
We performed an experiment using data from 71 software teams executing well-defined refactoring tasks, involving 320 sessions of work from 117 developers. Given the moderate population size, we acknowledge that this may be a threat to generalizing conclusions or making bold assertions. The Spearman's correlation, a nonparametric measure, was done on 32 and 39 teams for automatic and manual refactoring tasks, respectively. These figures, although valid, are close to the minimum admissible number of subjects for this type of analysis. Nevertheless, the insights we unveil in this study should trigger additional research to confirm or invalidate our initial findings.

**7. Conclusion**

**7.1. Main Conclusions**
Software maintenance activities, such as refactoring, are heavily impacted by software complexity. Researchers have long measured software product and process complexities, and the methods used are frequently debated. In this work, we aimed to deepen our understanding of the relationship between process and software complexity. We assessed whether process-driven metrics and IDE-issued commands are suitable for building valid models to predict different refactoring methods and the expected levels of software cyclomatic complexity variance during development sessions.

We mined software metrics from a software product after a software quality improvement task was given to a group of developers organized in teams. Simultaneously, we collected events from those developers during their change activities within the IDE.

To the best of our knowledge, this is the first study to use proven process mining methods to gather and combine process metrics with product metrics to understand the relationship between product and process dimensions, particularly cyclomatic complexities. This study highlights the potential for researchers to adopt process metrics extracted from IDE usage as a way to complement or even replace product metrics in modeling the development process.

While we cannot compare our study to any previous works, with a small set of features, we were able to uncover important correlations between product and process dimensions and obtain good models in terms of accuracy and ROC when predicting the type of refactoring done or the expected level of cyclomatic complexity variance after multiple development sessions. We used a refactoring task as our main use case, but by taking snapshots of product and process metrics at different times, one can measure other development practices similarly.

**7.2. Relevance for Practitioners**
This approach is particularly relevant in cases where product metrics are not available or difficult to obtain. It can also be a valid approach to measure and monitor productivity within and between software teams. By analyzing session complexity and software cyclomatic complexity variance, inefficient teams can be easily detected. Our method supports real-time data collection from individuals located in different geographic zones and using various development environments. Because the data collection is not dependent on code repositories and is decoupled from check-ins and/or commits, process and code analysis can be performed before repositories are updated. Development organizations can leverage this approach to apply conformance checking methods to verify the adherence of developers' practices with internally prescribed development processes. This facilitates the detection of low-performance practices and may trigger quick corrective actions from project managers.

**7.3. Limitations**
We are aware that in this work, we used only events from IDE usage, which limits the generalization of the current method. However, our approach, though valid on its own, can be used to complement project management analysis based on other repositories. Events from tools containing information about documentation, project management decisions, communication between developers and managers, Q&A services, test suites, and bug tracking systems, combined with our method and metrics, can build more robust and comprehensive models.