in Table 4.18. To compute the p-value we assumed that methods with
data augmentation and method without it performs similarly. We per-
formed two samples Student t-test to see whether this hypothesis is
true or not. We also perform a Wilcoxon test to analyze if there is a sta-
tistical difference in general. The result of this test is used to produce
the critical difference diagram in Figure 4.7. This diagram shows a bar
between methods that have no statistical differences, the x-axis repre-
sent the average ranking of these methods given in 4.17. With this test
we cannot conclude that there is a statistical difference in terms of per-
formances in general in our algorithm even if on each problem except
one we have noticed a signiﬁcant statistical difference.
CHAPTER 4. RESULTS
51
Dataset
Avila
Hand posture
Segmentation
Spam base
Sensorless
Bank credit
Wine quality
Htru
Statistical difference
(cid:88)
x
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
0.26
p-value
1.01 × 10−5
2.09 × 10−16
4.78 × 10−23
4.28 × 10−17
2.62 × 10−8
1.77 × 10−8
4.09 × 10−17
Table 4.18: Augmented data tree versus other classic algorithms (Stu-
dent t-test)
Figure 4.7: Critical difference diagram on data augmentation (The hor-
izontal axis correspond to the average rank)
Even if these results are promising they induce two problems. First,
to create the augmented model, we have to train a complex model on a
large amount of data which can cost a lot of time. During experiments,
ﬁnding the best parameters for different models has taken more than
one hour for the biggest dataset. Secondly ,that there is a lot of con-
ﬁgurations where the obtained result is not really different from the
single decision tree ones on the raw dataset. Moreover, there is no
clear pattern between the different parameters of data augmentation
and it seems to be very problem-related. For example, for the dataset
Avila with 20000 datapoints, the best results are achieved by using
a small augmentation 1 but with a large variance. The wine quality
dataset, which only has 2000 points, needs a high data augmentation
and a small variance. Last but not least, with the dataset Sensorless
(58000 points) best results are obtained by using a medium factor of
augmentation and a medium variance. As a reminder, the variance
factor is computed based on the variance of the dataset that is why we
can compare the variance factors of the data augmentation on differ-
ent datasets. These three examples clearly show that the choice in the
parameters is not predictable at all using a simple rule of thumb. The
52
CHAPTER 4. RESULTS
person who wants to use this technique has to perform another round
of parameters tuning but, this time, on the method that he wants to
choose to augment its dataset.
Chapter 5
Discussion
5.1 Decision trees answer to ethical problems
in machine learning
Ethic is currently at the heart of major machine learning discussions.
[28]. The need to understand how a model makes a decision, and how
it is built, grows at the same pace as integration of machine learning in
our daily lives. For example, automatic suggestion on social networks
could possibly comfort individual in their believes. A person who of-
ten consults pages about medicine and particular diseases could be
redirected to other pages that conﬁrm their auto-diagnostic. Another
example is the COMPAS algorithm that assesses a score to individu-
als to evaluate their recidivism potential [29]. It can potentially hide
some bias that are not acceptable. Some scandals have risen when it
has been proved that the COMPAS algorithm tended to give a differ-
ent score to different ethnic groups. This particular issue cannot occur
with decision tree algorithm since it is possible to inspect just after the
learning step the global picture of the produced models and determine
if some rules are not ethical. Therefore, it is possible to identify and
correct faster bias in a model. This problem is not totally addressed
by the local explainability given by LIME [5] or SHAP [6] because one
have to verify that all examples have no bias in it and it is impossible
to perform such algorithm on the actual volume of data in industrial
applications.
53
54
CHAPTER 5. DISCUSSION
5.2 Decision trees improvement in machine
learning
This work is mainly focused on decision tree improvement. It could
seem that this topic is not at the center of the actual research. In fact,
looking at the main conferences deep learning and new methods such
as gradient boosting occupy a large part of their proceedings. But
we were convinced that improvements could still be done on the this
model which is at the base of complex models such as Random Forest
[1] or XGboost[2]. Nevertheless black box models are not the only one
usable in applied machine learning. An important number of indus-
trial use cases of machine learning needs explainability at the heart of
their models and using decision trees could be an interesting option.
Though, research is still done to improve decision trees, and they are
often oriented towards producing a fairer framework for artiﬁcial in-
telligence. For example Zhang and Ntoutsi [30] have proposed a new
correcting factor in the decision tree splitting criterion, which is in fa-
vor of fair decision to reduce the bias introduced by some datasets
using some ethnic or gender factors. Moreover some research is in-
spired by the idea that imbalanced dataset are a common use case in
machine learning that is not very well handled by decision trees. One
way to handle it is to add weight to samples from minority classes, but
research have been done to ﬁnd a way to incorporate the bias directly
in the splitting function [31].
5.3 Transfer learning in machine learning
The performance improvement using complex model such as random
forest or XGboost can clearly be seen as a transfer learning method. We
succeeded to incorporate knowledge from the more accurate model
into a simple one like decision trees. This technique is also present
in deep neural networks. One complex neural network is trained to
accomplish a speciﬁc task in image recognition or natural language
processing and then the ﬁrst layer of the network could be used to
accomplish new task in a more accurate way [32]. In this thesis the
knowledge transfer between the two models is done via samples la-
beled with the powerful model. Furthermore this create a knowledge
transfer from a black box model into a simple decision tree which is
CHAPTER 5. DISCUSSION
55
completely white box. We have seen in Chapter 4 that it is possible in
more than 80% of our datasets to gain from XGboost or Random Forest
some percent of accuracy.
5.4 Sustainability
This work on decision trees fully ﬁts in the actual trends and needs in
machine learning. In fact, for the use of machine learning in industrial
application, explainability has an important role. Moreover the cur-
rent complex models that are developed, especially in deep learning,
are not sustainable in terms of ecology.
5.4.1 The need of explainability
It seems unthinkable to integrate an artiﬁcial intelligence in an indus-
trial application without a model which can provide explanations to
the end users.
It is hard to design a culprit when the decision was
taken by an AI, these kind of issues are well-known by autonomous
car developers. That is why the last years have seen the emergence of
several tools like LIME [5] or SHAP [6] to explain new black box mod-
els such as XGboost [2] or neural networks. This thesis focuses mainly
on an algorithm that is explainable by design. We are convinced that
reusing some old techniques and trying to improve them to ﬁt indus-
trial needs of explainability could be an interesting path to follow. In
this thesis we have increased the explainability power of decision trees
by reducing the number of nodes using new splitting technique for cat-
egorical values. We have also shown that we could have performance
in terms of accuracy that are quite similar to state-of-the-art algorithm.
5.4.2 Ecological impact of decision trees
We think that using basic methods such as decision trees that needs
less computation power than other methods can be a step forward to
a more sustainable AI. In fact, state-of-the-art models such as those
which has been used by Google in AlphaGo [33] consume up to 1
megaWatt and they are composed of hundreds of graphics processing
units and thousands of central processing units. The ecological im-
pact is double, ﬁrstly when the important amount of hardware is built
using a lot of rare metal, secondly a lot of energy is used when the
56
CHAPTER 5. DISCUSSION
algorithm learn and predict something. With the increase uses of arti-
ﬁcial intelligence for industrial purposes, computational optimisation
becomes an important issue because using a complex model at large
scale will have a huge ecological impact. That is why we think that
using machine learning technique that are less consuming in terms of
computation becomes a real needs. Exploring some old techniques
and trying to improving them, keeping in mind the computational
cost of each improvements seems to be one possible way to follow to
make a more sustainable AI. Another possible way is to develop more
transfer learning which avoid a lot of computation when dealing with
similar problems.
5.5 Limitations
5.5.1 Explainability
On the different datasets we used we have successfully decreased the
number of nodes in produced decision trees. But the number of nodes
is not the only metric to measure explainability, it is much more than
that. To estimate it clearly, it has to be shown to a population of ex-
perts to determine whether the produced model is simple and accurate
or not. The explainability has an important part of subjectivity. One
could think of it as how to describe a model mathematically whereas
another one about its comprehensibility for everyone. A limitation of
our approach is that since we did not have enough time to have expert
opinions for each domain for each dataset, we had to use a more sim-
plistic metric of explainability.
We only considered the global explainability and currently in science
the trend is on local explainability. The real needs of explainability is
to give a reason in the case that the system which embed an AI had
a failure. For example in the case of an accident with an autonomous
car, the idea is to identify where and when the failure happened. Algo-
rithms such as SHAP [6] or LIME [5] could provide some explanation
on the reasons which lead to a particular decision. Classically, using
SHAP on a deep neural network that has been trained for image classi-
ﬁcation reveals which parts of the picture are important for a particular
pair of example and decision.
Last but not least, there is currently no common agreement on how we
can precisely deﬁne the explainability of a model. Some researchers try
CHAPTER 5. DISCUSSION
57
to gather several ideas to ﬁnd a deﬁnition of it and some performance
indicators to measure it.
5.5.2 Global performance
Our study mainly focuses on the quality of prediction of decision trees
and the way to handle categorical values in it. We add to gather several
datasets to benchmark the produced methods. Because our task was
very speciﬁc and not common we could not rely on a classic bench-
mark database. In ﬁelds such as computer vision or natural processing
there are various [34] [35] [36] benchmarks that are commonly used to
compare new algorithm. The research of datasets which match the re-
quirements of our problem, that is to say with categorical features with
a lot of dimensions took an signiﬁcant amount of time. To provide a
result that an algorithm perform signiﬁcantly better than an other one
in general requires an signiﬁcant amount of experiments and datasets.
That is why we cannot conclude that our method is better in general
even if it is statistically better on several cases.
5.6 Future work
Experiments have raised some interesting questions about data aug-
mentation, tree merging and categorical split. This section will address
what could be done as a further works around this thesis.
5.6.1 Categorical split
Categorical split has raised a problematic about how to create and train
decision tree. The new proposed method has been able to increase the
compactness of learned decision trees, but at the cost of training time.
To go further, one should reduce the time consumed by the categorical
split. In fact, experiments show that even if one uses very complex
methods on the categorical split, it does not increase performances in
terms of accuracy. Nevertheless, it could be interesting to go deeper
in optimizing the building of a decision tree especially, by inspecting
what would happen in the case of splitting on numerical values. In
fact, most of the algorithms do an exhaustive search on all the possible
values of a feature. When the number of samples is relatively high, this
task is very costly. The problem persists if one tries to test one split
58
CHAPTER 5. DISCUSSION
every n samples. The variance, the mean and the class distribution
can be computed incrementally, only the ﬁrst computation of entropy
is in the order of the number of samples, each other one is just one
operation. An idea could be to try random split values, because, in
this case, no sorts are needed to search the best place to split. This
method already exists and it is called extremely random decision tree
[37]. We think that better improvements can be done on the increasing
performance part.
5.6.2 Toward increasing performance of decision tree
During the thesis, only some methods have been tested for the data
augmentation, and only some models have been used to label new
data. An idea could be to add some weights to the generated points
in order to focus only on some particular points. Some experiments
that we made while trying to make data augmentations show that al-
most every time models can achieve similar accuracies with only 10
% of the data. In this case, the data augmentation could be used to
generate new points that are maybe easier to classify (i.e. with a great
score) and to bring new information toward the simple model. As seen
previously, the biggest issue that we have in this problem is the com-
putation time of very complex models. There is only a gain of a few
percents of the accuracy. A huge pipeline has to be set to preserve the
explainability of the decision tree: ﬁrst training a complex model like
xgboost then create new samples and generate a model on top of it.
Another way that we did not explore is to use the data at each level
of the decision tree, that is to say, to pass the whole dataset to each
child and compute the second best split, to split on it, and so on. This
method has already been studied in information fuzzy network. The
result is not presented as a tree but as a graph, and at each level, the
feature which is used for the split is dropped in order to avoid du-
plicates in the network. An improvement which is possible on this
method is to not drop the feature when it is used, and to add a sort of
coefﬁcient which penalizes the values around the splitting points.
A last solution that has been considered, but not explored enough
to make a proper section on it, is to merge the behaviours of differ-
ent trees. In fact this idea could possibly solve two main problems of
decision trees:
CHAPTER 5. DISCUSSION
59
• A relatively poor accuracy compared to state-of-the-art models
• The impossibility to learn on huge datasets.
It is motivated by the possible estimation of the importance of fea-
tures in Random Forest. We could possibly guess among all the data
which feature the algorithm is going to split ﬁrst. Moreover, we have
a set of possible split values for each feature, so the idea could be to
select the split values randomly or by using a smart manner like a
density estimation. To avoid consecutive splits on the same features, a
mechanism could be implemented to rank the potential next features
and to update the ranking when splitting on a particular one.
Chapter 6
Conclusion
Experiments on the categorical splits have revealed that it is pos-
sible to signiﬁcantly improve the compactness of a decision tree. The
compactness could be considered as one of the performance indicators
to measure explainability, using new splitting function on categorical
values. Furthermore, we adapted the method for the regression task
using a clustering of the output value of a decision tree. This clustering
can be adaptive and change through the construction of the decision
tree. This adaptability enhances the split precision and allows to be
closer to the real distribution of the output values of the sub-dataset in
each nodes. The use of this method in categorical split does not bring
increase of the mean squared error. However, this improvement is at
the cost of computation time and it can also lead to overﬁtting. We also
succeeded in improving the accuracy of single decision trees using the
power of random forest and xgboost. The best method to perform data
augmentation labeling new samples with with black boxed algorithm
seems to be problem dependant. Nevertheless, in 7 out of 8 datasets
this method brings a signiﬁcant improvement in terms of accuracy.
60
Bibliography
[1] L Breiman. “Random forests”. In: Machine learning 45.1 (2001),
pp. 5–32.
[2] T Chen and C Guestrin. “Xgboost: A scalable tree boosting sys-
tem”. In: Proceedings of the 22nd acm sigkdd international conference
on knowledge discovery and data mining. 2016, pp. 785–794.
[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton. “Imagenet classi-
ﬁcation with deep convolutional neural networks”. In: Advances
in neural information processing systems. 2012, pp. 1097–1105.
[4] M. Sundermeyer, R. Schlüter, and H. Ney. “LSTM neural net-
works for language modeling”. In: Thirteenth annual conference of
the international speech communication association. 2012.
[5] M.T. Ribeiro, S. Singh, and C. Guestrin. “"Why Should I Trust
You?": Explaining the Predictions of Any Classiﬁer”. In: Proceed-
ings of the 22nd ACM SIGKDD international conference on knowl-
edge discovery and data mining. 2016, pp. 1135–1144.
[6] S. M Lundberg and S. Lee. “A uniﬁed approach to interpreting
model predictions”. In: Advances in neural information processing
systems. 2017, pp. 4765–4774.
J. R. Quinlan. “Induction of decision trees”. In: Machine learning
1.1 (1986), pp. 81–106.
[7]
[8] L Breiman et al. Classiﬁcation and Regression Trees; 2017. Rout-
ledge.
J R Quinlan. C4. 5: programs for machine learning. Elsevier, 2014.
[9]
[10] F Pedregosa et al. “Scikit-learn: Machine Learning in Python”.
In: Journal of machine learning research 12 (2011).
[11] M Kuhn. The caret Package. 2009.
61
62
BIBLIOGRAPHY
[12] T. Miller. “Explanation in artiﬁcial intelligence: Insights from the
[13]
social sciences”. In: Artiﬁcial Intelligence 267 (2019), pp. 1–38.
J Mingers. “An empirical comparison of pruning methods for
decision tree induction”. In: Machine learning 4.2 (1989), pp. 227–
243.
[14] R Kohavi. “Scaling up the accuracy of naive-bayes classiﬁers: A
decision-tree hybrid.” In: Kdd. Vol. 96. 1996, pp. 202–207.
[15] N. Landwehr, M. Hall, and E. Frank. “Logistic model trees”. In:
Machine learning 59.1-2 (2005), pp. 161–205.
[16] Gian-Carlo Rota. “The number of partitions of a set”. In: The
American Mathematical Monthly 71.5 (1964), pp. 498–504.
[17] Svante Wold, Kim Esbensen, and Paul Geladi. “Principal compo-
nent analysis”. In: Chemometrics and intelligent laboratory systems
2.1-3 (1987), pp. 37–52.
[18] D Coppersmith, S.J. Hong, and J. RM Hosking. “Partitioning
nominal attributes in decision trees”. In: Data Mining and Knowl-
edge Discovery 3.2 (1999), pp. 197–217.
[19] Y Yang, I.G. Morillo, and T.K. Hospedales. “Deep neural deci-
sion trees”. In: arXiv (2018).
[20] S C Wong et al. “Understanding data augmentation for classiﬁ-
cation: when to warp?” In: 2016 international conference on digital
image computing: techniques and applications (DICTA). IEEE. 2016,
pp. 1–6.
[21] P Domingos. “Unifying instance-based and rule-based induc-
tion”. In: Machine Learning 24.2 (1996), pp. 141–168.
[22] P Domingos. “Knowledge acquisition from examples via multi-
ple models”. In: MACHINE LEARNING-INTERNATIONAL WORK-
SHOP THEN CONFERENCE-. 1997, pp. 98–106.
J. C. Schlimmer and D. Fisher. “A case study of incremental con-
cept induction”. In: AAAI. Vol. 86. 1986, pp. 496–501.
[23]
[24] E. Scornet, G. Biau, J.P. Vert, et al. “Consistency of random forests”.
In: The Annals of Statistics 43.4 (2015), pp. 1716–1741.
[25] D.H. Ruben. Explaining explanation. Routledge, 2015.
[26] D. Dua and C. Graff. UCI Machine Learning Repository. 2017.
BIBLIOGRAPHY
63
[27] N. E. Day. “Estimating the components of a mixture of normal
distributions”. In: Biometrika 56.3 (1969), pp. 463–474.
[28] High-Level Expert Group on AI. Ethics guidelines for trustworthy
AI. eng. Report. European Commission, Apr. 2019.
[29] D. Leah Kehl and S. A. Kessler. “Algorithms in the criminal jus-
tice system: Assessing the use of risk assessments in sentencing”.
In: (2017).
[30] W. Zhang and E. Ntoutsi. “FAHT: an adaptive fairness-aware
decision tree classiﬁer”. In: arXiv (2019).
[31] P.S. Akash et al. “Inter-node Hellinger Distance based Decision
Tree”. In: Proceedings of the Twenty-Eighth International Joint Con-
ference on Artiﬁcial Intelligence, IJCAI-19. 2019.
[32] H.C. Shin et al. “Deep convolutional neural networks for computer-
aided detection: CNN architectures, dataset characteristics and
transfer learning”. In: IEEE transactions on medical imaging 35.5
(2016), pp. 1285–1298.
[33] David Silver et al. “Mastering the game of Go with deep neural
networks and tree search”. In: Nature 529 (2016), pp. 484–503.
URL: http://www.nature.com/nature/journal/v529/
n7587/full/nature16961.html.
[34] A. Wang et al. “GLUE: A Multi-Task Benchmark and Analysis
Platform for Natural Language Understanding”. 2018.
[35] D. Jia et al. “ImageNet: A large-scale hierarchical image database”.
In: 2009 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida,
USA. IEEE Computer Society, 2009, pp. 248–255.
[36] Y. LeCun and C. Cortes. “MNIST handwritten digit database”.
In: (2010).
[37] P. Geurts, D. Ernst, and L. Wehenkel. “Extremely randomized
trees”. In: Machine learning 63.1 (2006), pp. 3–42.
TRITA -EECS-EX-2020:254
www.kth.se