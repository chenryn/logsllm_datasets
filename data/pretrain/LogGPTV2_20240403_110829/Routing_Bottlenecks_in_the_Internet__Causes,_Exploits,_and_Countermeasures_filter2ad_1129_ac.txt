11The CAIDA’s Ark project uses 86 monitors distributed over
81 cities in 37 countries and performs traceroute to all routed
/24’s. For consistent comparison, we use the Ark dataset
that was measured on the same day when PlanetLab dataset
was obtained and select a subset of the measured traces in
the Ark dataset that has the same AS distribution of hosts
used in the PlanetLab dataset.
5
 0%10%20%30%40%50%60%70%80%90%100%N/DIXPinter (N/D)inter (Tier2-Tier3)inter (Tier2-Tier2)inter (Tier1-Tier3)inter (Tier1-Tier2)inter (Tier1-Tier1)intra (Tier2)intra (Tier1)Percentage of link types(N/D: not determined)               0         5         10        15Router−hop distance  Country1Country2Country3Country4Country5Country6Country7Country8Country9Country10Country11Country12Country13Country14Country15AverageStdev               00.20.40.60.811.2     conn. degradation (Ark) conn. degradation (PlanetLab)Country1Country2Country3Country4Country5Country6Country7Country8Country9Country10Country11Country12Country13Country14Country15Figure 8: Maximum rank with sample size (cid:21) 50 for
each of the 15 countries.
etLab nodes for both cases.12 In most countries, the ratios of
the two connectivity degradations are slightly higher or very
close to 1, which means that the bottlenecks of the Planet-
Lab dataset also become the bottlenecks of the independent
Ark dataset. This result con(cid:12)rms the independence of the
bottleneck-link (cid:13)ooding results of the choice of route-source
distribution [24].
3.2 Sufﬁciency of Link-Sample Size
Another common pitfall in Internet measurement discov-
ering statistical properties of datasets is the lack of sample
size; that is, it is possible that the sample size is not su(cid:14)-
ciently large to detect possible deviations from a discovered
distribution. For extracting reliable parameter estimates,
the rule of thumb is that one needs to collect at least 50
samples for each value of element [8]. Fig. 8 shows the
maximum rank of the links (ordered by decreasing link oc-
currence) that are observed with at least 50 link samples for
the 15 countries in our measurement. The (cid:12)gure shows that
for all 15 countries, all the high ranked links (i.e., rank (cid:20)
100) are observed with more than 50 link samples and thus
the parameter estimates based on these links (i.e., (cid:11) and (cid:12)
in Fig. 2) are statistically sound.
Fig. 9 con(cid:12)rms that we have collected a su(cid:14)cient number
of link samples. In this (cid:12)gure, we measure the normalized
link occurrence with the various sizes of disjoint D and ob-
serve how the link occurrence in the high rank region (i.e.,
rank (cid:20) 100) converges. We can conclude that jDj = 1,000
is su(cid:14)cient to discover the power-law distribution in rank
(cid:20) 100 because it displays the same power-law distribution
in the range as that observed with smaller size for D; i.e.,
jDj < 1; 000. Thus, with a relatively small number of mea-
surements one can learn the power-law distribution of the
few but frequently observed high-rank links.
3.3 Traceroute Accuracy
traceroute is a very commonly used but frequently misused
network monitoring tool. We discuss common pitfalls in
analyzing traceroute results and describe how we avoid them.
Inaccurate alias resolution: In many previous ‘topol-
ogy measurement’ studies, it is extremely important to ac-
curately infer the group of interfaces located in the same
router (or alias resolution) because its accuracy dramatically
a(cid:11)ects the resulting network topology [42, 34]. Highly ac-
curate alias resolution still remains an open problem. Our
measurements do not need alias resolution because we do
not measure any router-level topology, but only layer-3 links
12For detailed strategy to select the links from routing bot-
tlenecks, viz., Section 4.2.
(i.e., interfaces) and routes that use those links.
Inaccurate representation of load-balanced routes:
Ordinary traceroute does not accurately capture load-balanced
links and thus specially crafted traceroute-like tools (e.g.,
Paris traceroute [6]) are needed to discover these links. Our
measurement does not need to discover load-balanced links
because they cannot become the routing bottlenecks.
In-
stead, we perform ordinary traceroute multiple times (e.g., 6
traceroutes in our measurement) for the same source-destination
pair and ignore the links that do not always appear in mul-
tiple routes.
Inconsistent returned IPs: In response to traceroute,
common router implementations return the address of the
incoming interface where packets enter the router. How-
ever, very few router models return the outgoing interface
used to forward ICMP messages back to the host launch-
ing traceroute [32, 33] and thus create measurement errors.
However, our routing bottleneck measurement is not a(cid:11)ected
by this router behavior because (1) most of the identi(cid:12)ed
router models that return outgoing interfaces are likely to
be in small ASes since they are mostly Linux-based soft-
ware routers or lower-end routers [32], and (2) we remove
all load-balanced links that can be created by the routers
which return outgoing interfaces [33].
Hidden links in MPLS tunnels: Some routers in MPLS
tunnels might not respond to traceroute and this might cause
serious measurement errors [54]. However, according to a re-
cent measurement study in 2012 [10], in the current Internet,
nearly all (i.e., more than 95%) links in MPLS tunnels are
visible to traceroute since most current routers implement
RFC4950 ICMP extension and/or ttl-propagate option to
respond to traceroute [10].
4. ROUTING-BOTTLENECK EXPLOITS
Bottleneck links provide a very attractive target for link-
(cid:13)ooding attacks [7, 24]. After identifying a routing bottle-
neck, an adversary chooses a set of links in it and (cid:13)oods
them. In this section we discuss the selection of such links,
the expected degradation in connectivity to the targeted
hosts D, and the scaling property of the link-(cid:13)ooding at-
tacks. To measure the strength of a link-(cid:13)ooding attack,
we (cid:12)rst de(cid:12)ne an ideal attack that completely disconnects
all routes from sources S to selected hosts of destinations
D. Then, we de(cid:12)ne lower-strength attacks that nevertheless
degrade connectivity signi(cid:12)cantly.
4.1 Connectivity Disconnection Attacks
Let S be the 250 PlanetLab nodes and D the 1,000 ran-
domly selected hosts in the target region; e.g., a country or a
city. For e(cid:14)cient disconnection attacks, the adversary needs
to (cid:13)ood only non-redundant links; that is, (cid:13)ooding of a link
should disconnect routes that have not been disconnected
by the other already (cid:13)ooded links. Link redundancy can be
avoided by (cid:13)ooding the mincut of the routes from S to D,
namely the minimum set of links whose removal disconnects
all the routes from S to D, which is denoted by M (S; D).
Finding M (S; D) can be formulated as the set cover prob-
lem13: given a set of element U = f1; 2;(cid:1)(cid:1)(cid:1) ; mg (called the
13Notice that our mincut is not the same as a graph-theoretic
mincut; our mincut is a set of links that cover all routes
to chosen nodes whereas the graph-theoretic mincut is a set
of physical link cuts for an arbitrary network partitioning.
6
    050100150200250300350400450   Max rank withsample size ≥ 50Country1Country2Country3Country4Country5Country6Country7Country8Country9Country10Country11Country12Country13Country14Country15Figure 9: Normalized link occurrence/rank in traced routes to 1,000 randomly selected hosts in 3 countries.
degradation metric, which we call the degradation ratio, as
follows:
(cid:14)(S;D)(B) =
number of routes that traverse B
number of routes from S to D
;
(2)
where the exploitable bottleneck B is the set of links that are
(cid:13)ooded by an attack.15 B is a subset of the mincut M (S; D),
and its size, jBj, or the number of links to (cid:13)ood, is deter-
mined by an adversary’s capability. Clearly, the maximum
number of links that an adversary can (cid:13)ood is directly pro-
portional to the maximum amount of tra(cid:14)c generated by
attack sources controlled by the adversary; e.g., botnets or
ampli(cid:12)cation servers. Here, we assume that the required
bandwidth to (cid:13)ood a single link is 40 Gbps16 and thus the
adversary should create 40 (cid:2) n Gbps attack bandwidth to
(cid:13)ood n links concurrently.
Fig. 11 shows the expected degradation ratio calculated
for each of the 15 countries for varying number of links to
(cid:13)ood, or jBj. These countries are ordered by increasing the
averaged degradation ratio over 1 (cid:20) jBj (cid:20) 50. By de(cid:12)nition,
the degradation ratio for B (i.e., (cid:14)(S;D)(B)) is the sum of
normalized occurrences of the links in B. Thus, degradation
ratio can be accurately modeled by the cumulative distribu-
tion function (CDF) of Zipf-Mandelbrot distribution since
the normalized link occurrence follows Zipf-Mandelbrot dis-
tribution. Parameters (cid:11) and (cid:12) are listed in the plot. We
observe that the ordering of the degradation ratio in Fig.
11 is exactly the same as the ordering of the values of (cid:11)
of the 15 countries in Fig. 2. That is, countries with low
(cid:11) (i.e., less skewed distribution) have low degradation ratio
(i.e., less vulnerable to (cid:13)ooding attacks) and countries with
high (cid:11) (i.e., more skewed distribution) have high degrada-
tion ratio (i.e., more vulnerable to (cid:13)ooding attacks). This
con(cid:12)rms that the skew of the link-occurrence distribution
(or (cid:11) in Zipf-Mandelbrot distribution) is a good indicator of
the vulnerability of target regions to link-(cid:13)ooding attacks.
Fig. 11 also shows that the adversary can easily achieve
signi(cid:12)cant degradation ratio (e.g., 40% - 82%) when (cid:13)ooding
only few bottleneck links; e.g., 20 links. Given the prolifer-
ation of tra(cid:14)c ampli(cid:12)cation attacks achieving hundreds of
Gbps or the extremely low costs of botnets, (cid:13)ooding sev-
eral tens of bottleneck links of selected hosts in di(cid:11)erent
countries around the world seems very practical. (We also
illustrate the degradation ratio for 15 major cities in Fig. 15
in Appendix A.)
15The de(cid:12)nition of degradation ratio is similar to that pre-
sented in [24].
16Links with larger physical capacity (e.g., 100 Gbps) are re-
cently introduced in the Internet backbone but the majority
of backbone links are equipped with less than or equal to 40
Gbps of capacity [23].
Figure 10: Measured sizes of mincuts, M (S; D), and
exploitable bottlenecks for given degradation ratios
(cid:14)C , B(S;D)((cid:14)C ), for varying size of D in 3 countries.
universe) and a set K of n sets whose union equals the uni-
verse, the problem is to identify the smallest subset of K
whose union equals the universe. Thus, our mincut problem
can be formulated as follows: the set of all routes we want
to disconnect is the universe, U; all IP-layer links are the
sets in K, each of which contains a subset of routes in U ,
and their union equals U; the problem is to (cid:12)nd the smallest
set of links whose union equals U . Since the set cover prob-
lem is NP-hard, we run a greedy algorithm [19] to calculate
M (S; D). The greedy algorithm, which is similar to the one
used to (cid:12)nd critical links in the Cross(cid:12)re attack [24], itera-
tively selects (and virtually cuts) each link in M (S; D) until
all the routes from S to D are disconnected. To be speci(cid:12)c,
at each iteration, the algorithm picks the most e(cid:11)ective link
within the not-yet-selected links.
Our experiments show that (cid:13)ooding an entire mincut,
M (S; D), in any of the (cid:12)fteen countries and cities selected
would be rather unrealistic; e.g., approximately 83 Tbps
would be required to (cid:13)ood a mincut of 2,066 links with 40
Gbps link capacity for a (cid:13)ooding attack against 1,000 servers
in Country1.14 Worse yet, Fig. 10 (red line) shows that the
mincut size, jM (S; D)j, grows as jDj grows. This implies
that any practical link-(cid:13)ooding attack that disconnects all
the hosts of a target region will also be impractical because
the mincut would likely be much larger than M (S; D). How-
ever, as we show in the next section, an adversary does not
need to (cid:13)ood an entire mincut to degrade connectivity of D
hosts of a targeted region substantially.
4.2 Connectivity Degradation Attacks
Feasible yet powerful connectivity-degradation attacks would
(cid:13)ood much smaller sets of links to achieve substantial con-
nectivity degradation to the routes from S to D. To mea-
sure the strength of such attacks we de(cid:12)ne a connectivity-
Thus, one cannot use well-known polynomial-time mincut
algorithms in graph theory [48] for our purpose.
14Note that in calculating M (S; D) we exclude network links
that are directly connected to the hosts in S or D and thus
jM (S; D)j can be larger than jSj or jDj.
7
10010110210310−510−410−310−210−1100Country1← high                     low →Rank of LinksNormalized Link Occurrence10010110210310−510−410−310−210−1100Country8← high                     low →Rank of LinksNormalized Link Occurrence10010110210310−510−410−310−210−1100Country15← high                     low →Rank of LinksNormalized Link Occurrence  Legend:|D| = 20|D| = 100|D| = 200|D| = 500|D| = 1,000|D||M(S,D)| vs. |B(S,D)(δC)|2005007001000100101102103Country12005007001000100101102103Country82005007001000100101102103Country15  Legend:|M(S,D)||B(S,D)(0.7)||B(S,D)(0.5)|Figure 11: Calculated degradation-ratio/number-of-links-to-(cid:13)ood for 1,000 servers in 15 countries.
4.2.1 Bottlenecks Sizes
The size of an exploitable bottleneck selected for an attack
clearly depends on the chosen degradation ratio (cid:14)C sought
by an adversary. This ratio is de(cid:12)ned as:
B(S;D)((cid:14)C ) = minimum jBj; such that (cid:14)(S;D)(B) (cid:21) (cid:14)C : (3)
Exploitable bottlenecks, B(S;D)((cid:14)C ), are substantially smaller
than their corresponding mincuts, M (S; D). Fig. 10 shows
the set sizes of the mincuts and the exploitable bottlenecks
for given (cid:14)C of 0.7 and 0.5 for varying sizes of D. The plots in
the three countries show that jM (S; D)j is one to two orders
of magnitude larger than jB(S;D)((cid:14)C )j in the entire range of
measured jDj and (cid:14)C . In other words, the attack against the
exploitable bottlenecks requires a much lower adversary’s
(cid:13)ooding capability while achieving substantial connectivity
degradation (e.g., 70%). Figures 11 and 15 (Appendix A)
illustrate the bottleneck sizes for di(cid:11)erent degradation ratios
in 15 countries and cities around the world.
4.2.2 Scaling the Attack Targets
Our experiments suggest that an adversary needs not scale
routing measurements and attack tra(cid:14)c much beyond those
illustrated in this paper for much larger target-host sets (i.e.,
jDj (cid:29) 1; 000) in a chosen region to obtain connectivity-
degradation ratios in the range illustrated in this paper.
This is the case following two reasons. First, our measure-
ments for multiple disjoint sets of selected hosts in a target
region yield the same power-law distribution for di(cid:11)erent un-
related sizes of D; viz., Fig. 9. Hence, increasing the number
of routes from S to a much larger D will not increase the
size of the bottlenecks appreciably. In fact, we have already
noted that, unlike the size of mincuts, jM (S; D)j, the size
of the observed bottlenecks for a chosen degradation ratio
(cid:14)C , jB(S;D)((cid:14)C )j, does not change as jDj increases, as shown
in Fig. 10. Second, we have shown the routing-bottleneck
discovery is independent of the choice of S, where jSj (cid:29) jBj;
viz., Section 3.1. This implies that, to (cid:13)ood the few addi-
tional bottleneck links necessary for a much increased target
set D, an adversary needs not increase the size of S and at-