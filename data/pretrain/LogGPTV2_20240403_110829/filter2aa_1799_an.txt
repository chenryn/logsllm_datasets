Always
Target processor is idle
High
Always
Target processor is idle
Additionally, Table 8-8 describes the various DPC adjustment variables and their default values, as 
set by using the SystemDpcBehaviorInformation system information class.
TABLE 8-8 DPC interrupt generation variables
Variable
Definition
Default Override Value
Number of DPCs queued before an interrupt will be 
sent even for Medium or below DPCs
4
DpcQueueDepth
Number of DPCs per clock tick where low DPCs will 
not cause a local interrupt to be generated
3
MinimumDpcRate
Number of DPCs per clock tick before the maximum 
DPC queue depth is decremented if DPCs are pending 
but no interrupt was generated
20
IdealDpcRate
Number of clock ticks before the maximum DPC 
20
AdjustDpcThreshold
Because user-mode threads execute at low IRQL, the chances are good that a DPC will interrupt 
-
faults, or create or wait for dispatcher objects (explained later in this chapter). They can, however, ac-
cess nonpaged system memory addresses, because system address space is always mapped regardless 
of what the current process is. 
CHAPTER 8 System mechanisms
57
Because all user-mode memory is pageable and the DPC executes in an arbitrary process context, 
DPC code should never access user-mode memory in any way. On systems that support Supervisor 
Mode Access Protection (SMAP) or Privileged Access Neven (PAN), Windows activates these features 
for the duration of the DPC queue processing (and routine execution), ensuring that any user-mode 
memory access will immediately result in a bugcheck.
Another side effect of DPCs interrupting the execution of threads is that they end up “stealing” 
from the run time of the thread; while the scheduler thinks that the current thread is executing, a DPC 
is executing instead. In Chapter 4, Part 1, we discussed mechanisms that the scheduler uses to make 
up for this lost time by tracking the precise number of CPU cycles that a thread has been running and 
deducting DPC and ISR time, when applicable. 
wall time (also sometimes called clock time—the real-life passage of time) is still 
being spent on something else. Imagine a user currently streaming their favorite song off the Internet: 
If a DPC were to take 2 seconds to run, those 2 seconds would result in the music skipping or repeat-
ing in a small loop. Similar impacts can be felt on video streaming or even keyboard and mouse input. 
Because of this, DPCs are a primary cause for perceived system unresponsiveness of client systems or 
workstation workloads because even the highest-priority thread will be interrupted by a running DPC. 
threaded DPCs. Threaded DPCs, 
as their name implies, function by executing the DPC routine at passive level on a real-time priority 
(priority 31) thread. This allows the DPC to preempt most user-mode threads (because most application 
and other priority 31 threads to preempt the routine. 
The threaded DPC mechanism is enabled by default, but you can disable it by adding a DWORD val-
ue named ThreadDpcEnable
KeInitializeThreadedDpc API, which sets the DPC internal type to ThreadedDpcObject. Because threaded 
DPCs can be disabled, driver developers who make use of threaded DPCs must write their routines 
following the same rules as for nonthreaded DPC routines and cannot access paged memory, perform 
dispatcher waits, or make assumptions about the IRQL level at which they are executing. In addition, 
they must not use the KeAcquire/ReleaseSpinLockAtDpcLevel APIs because the functions assume the 
CPU is at dispatch level. Instead, threaded DPCs must use KeAcquire/ReleaseSpinLockForDpc, which 
performs the appropriate action after checking the current IRQL. 
system administrator. As such, the vast majority of DPCs still execute nonthreaded and can result in 
perceived system lag. Windows employs a vast arsenal of performance tracking mechanisms to diag-
through performance counters, as well as through precise ETW tracing.
58 
CHAPTER 8 System mechanisms
EXPERIMENT: Monitoring DPC activity
You can use Process Explorer to monitor DPC activity by opening the System Information dialog 
box and switching to the CPU tab, where it lists the number of interrupts and DPCs executed each 
time Process Explorer refreshes the display (1 second by default):
EXPERIMENT: Monitoring DPC activity
You can use Process Explorer to monitor DPC activity by opening the System Information dialog 
box and switching to the CPU tab, where it lists the number of interrupts and DPCs executed each 
time Process Explorer refreshes the display (1 second by default):
CHAPTER 8 System mechanisms
59
with Dpc, such as DpcRequestRate, DpcLastCount, DpcTime, and DpcData (which contains the 
DpcQueueDepth and DpcCount for both nonthreaded and threaded DPCs). Additionally, newer 
versions of Windows also include an IsrDpcStats_ISRDPCSTATS 
nonthreaded) versus the number that have executed:
lkd> dx new { QueuedDpcCount = @$prcb->DpcData[0].DpcCount + @$prcb->DpcData[1].DpcCount, 
ExecutedDpcCount = ((nt!_ISRDPCSTATS*)@$prcb->IsrDpcStats)->DpcCount },d 
    QueuedDpcCount   : 3370380 
    ExecutedDpcCount : 1766914 [Type: unsigned __int64]
The discrepancy you see in the example output is expected; drivers might have queued a DPC 
that was already in the queue, a condition that Windows handles safely. Additionally, a DPC 
execute on a different processor, such as when the driver uses KeSetTargetProcessorDpc (the API 
allows a driver to target the DPC to a particular processor.)
DPCTimeout, DpcWatchdogPeriod, and DpcWatchdogProfileOffset. 
The DPC Watchdog is responsible for monitoring all execution of code at DISPATCH_LEVEL or 
above, where a drop in IRQL has not been registered for quite some time. The DPC Timeout, on the 
20 seconds, and all DISPATCH_LEVEL (and above) execution times out after 2 minutes. Both limits are 
DPCTimeout
limit, whereas the DpcWatchdogPeriod controls the combined execution of all the code running at 
high IRQL). When these thresholds are hit, the system will either bugcheck with DPC_WATCHDOG_
VIOLATION (indicating which of the situations was encountered), or, if a kernel debugger is attached, 
raise an assertion that can be continued.
Driver developers who want to do their part in avoiding these situations can use the 
KeQueryDpcWatchdogInformation
KeShouldYieldProcessor API takes these values (and other system state values) into 
consideration and returns to the driver a hint used for making a decision whether to continue its DPC 
work later, or if possible, drop the IRQL back to PASSIVE_LEVEL-
ing, but the driver was holding a lock or synchronizing with a DPC in some way).
On the latest builds of Windows 10, each PRCB also contains a DPC Runtime History Table 
(DpcRuntimeHistoryHashTable
functions that have recently executed and the amount of CPU cycles that they spent running. When 
with Dpc, such as DpcRequestRate, DpcLastCount, 
DpcLastCount, 
DpcLastCount DpcTime, and DpcData (which contains the 
DpcQueueDepth and DpcCount for both nonthreaded and threaded DPCs). Additionally, newer 
DpcCount for both nonthreaded and threaded DPCs). Additionally, newer 
DpcCount
versions of Windows also include an IsrDpcStats_ISRDPCSTATS
nonthreaded) versus the number that have executed:
lkd> dx new { QueuedDpcCount = @$prcb->DpcData[0].DpcCount + @$prcb->DpcData[1].DpcCount, 
ExecutedDpcCount = ((nt!_ISRDPCSTATS*)@$prcb->IsrDpcStats)->DpcCount },d
    QueuedDpcCount   : 3370380
    ExecutedDpcCount : 1766914 [Type: unsigned __int64]
The discrepancy you see in the example output is expected; drivers might have queued a DPC 
that was already in the queue, a condition that Windows handles safely. Additionally, a DPC 
execute on a different processor, such as when the driver uses KeSetTargetProcessorDpc (the API 
KeSetTargetProcessorDpc (the API 
KeSetTargetProcessorDpc
allows a driver to target the DPC to a particular processor.)
60 
CHAPTER 8 System mechanisms
access to a UI tool, but more importantly, this data is also now used by the kernel.
When a driver developer queues a DPC through KeInsertQueueDpc, the API will enumerate the 
LongDpcRuntimeThreshold regis-
this is the case, the LongDpcPresentDpcData structure mentioned earlier.
thread), the kernel now also creates a DPC Delegate Thread. These are highly unique threads that 
thread selection algorithms. They are merely kept in the back pocket of the kernel for its own purposes. 
delegate threads. Note that in this case, these threads have a real Thread ID (TID), and the Processor 
column should be treated as such for them.
FIGURE 8-18 The DPC delegate threads on a 16-CPU system.
CHAPTER 8 System mechanisms
61
Whenever the kernel is dispatching DPCs, it checks whether the DPC queue depth has passed the 
threshold of such long-running 
by looking at the properties of the currently executing thread: Is it idle? Is it a real-time thread? Does 
kernel may decide to schedule the DPC delegate thread instead, essentially swapping the DPC from its 
thread-starving position into a dedicated thread, which has the highest priority possible (still execut-
ing at DISPATCH_LEVEL). This gives a chance to the old preempted thread (or any other thread in the 
standby list) to be rescheduled to some other CPU. 
This mechanism is similar to the Threaded DPCs we explained earlier, with some exceptions. The 
delegate thread still runs at DISPATCH_LEVEL. Indeed, when it is created and started in phase 1 of the 
NT kernel initialization (see Chapter 12 for more details), it raises its own IRQL to DISPATCH level, saves 
it in the WaitIrql
a context switch to another standby or ready thread (via the KiSwapThread routine.) Thus, the delegate 
DPCs provide an automatic balancing action that the system takes, instead of an opt-in that driver 
developers must judiciously leverage on their own.  
If you have a newer Windows 10 system with this capability, you can run the following command in 
the kernel debugger to take a look at how often the delegate thread was needed, which you can infer 
from the amount of context switches that have occurred since boot:
lkd> dx @$cursession.Processes[0].Threads.Where(t => t.KernelObject.ThreadName-> 
ToDisplayString().Contains("DPC Delegate Thread")).Select(t => t.KernelObject.Tcb.
ContextSwitches),d 
    [44]
: 2138 [Type: unsigned long] 
    [52]
: 4 [Type: unsigned long] 
    [60]
: 11 [Type: unsigned long] 
    [68]
: 6 [Type: unsigned long] 
    [76]
: 13 [Type: unsigned long] 
    [84]
: 3 [Type: unsigned long] 
    [92]
: 16 [Type: unsigned long] 
    [100]
: 19 [Type: unsigned long] 
    [108]
: 2 [Type: unsigned long] 
    [116]
: 1 [Type: unsigned long] 
    [124]
: 2 [Type: unsigned long] 
    [132]
: 2 [Type: unsigned long] 
    [140]
: 3 [Type: unsigned long] 
    [148]
: 2 [Type: unsigned long] 
    [156]
: 1 [Type: unsigned long] 
    [164]
: 1 [Type: unsigned long]
Asynchronous procedure call interrupts
Asynchronous procedure calls (APCs) provide a way for user programs and system code to execute 
in the context of a particular user thread (and hence a particular process address space). Because 
APCs are queued to execute in the context of a particular thread, they are subject to thread schedul-
ing rules and do not operate within the same environment as DPCs—namely, they do not operate at 
DISPATCH_LEVEL and can be preempted by higher priority threads, perform blocking waits, and access 
pageable memory. 
62 
CHAPTER 8 System mechanisms
That being said, because APCs are still a type of software interrupt, they must somehow still be able 
APC_LEVEL
operate under the same restrictions as a DPC, there are still certain limitations imposed that developers 
APCs are described by a kernel control object, called an APC object. APCs waiting to execute reside 
in one of two kernel-managed APC queues. Unlike the DPC queues, which are per-processor (and di-
vided into threaded and nonthreaded), the APC queues are per-thread—with each thread having two 
APC queues: one for kernel APCs and one for user APCs. 
When asked to queue an APC, the kernel looks at the mode (user or kernel) of the APC and then 
inserts it into the appropriate queue belonging to the thread that will execute the APC routine. Before 
When an APC is queued against a thread, that thread may be in one of the three following situations:
I 
The thread is currently running (and may even be the current thread).
I 
The thread is currently waiting.
I 
The thread is doing something else (ready, standby, and so on).
alertable 
state whenever performing a wait. Unless APCs have been completely disabled for a thread, for kernel 
APCs, this state is ignored—the APC always aborts the wait, with consequences that will be explained 
user APCs however, the thread is interrupted only if the wait was alertable and 
instantiated on behalf of a user-mode component or if there are other pending user APCs that already 
started aborting the wait (which would happen if there were lots of processors trying to queue an APC 
to the same thread). 
either perform an alertable wait or go through a ring transition or context switch that revisits the User 
raising the IRQL to APC_LEVEL, notifying the processor that it must look at the kernel APC queue of its 
currently running thread. And, in both scenarios, if the thread was doing “something else,” some transi-
tion that takes it into either the running or waiting state needs to occur. As a practical result of this, 
We mentioned that APCs could be disabled for a thread, outside of the previously described scenar-
being to simply keep their IRQL at APC_LEVEL or above while executing some piece of code. Because 
-
plained, if the processor is already at APC_LEVEL (or higher), the interrupt is masked out. Therefore, it is 
only once the IRQL has dropped to PASSIVE_LEVEL that the pending interrupt is delivered, causing the 
APC to execute. 
CHAPTER 8 System mechanisms
63
The second mechanism, which is strongly preferred because it avoids changing interrupt controller 
state, is to use the kernel API KeEnterGuardedRegion, pairing it with KeLeaveGuardedRegion when you 
want to restore APC delivery back to the thread. These APIs are recursive and can be called multiple 
times in a nested fashion. It is safe to context switch to another thread while still in such a region 
SpecialApcDisable and 
not per-processor state. 
Similarly, context switches can occur while at APC_LEVEL, even though this is per-processor state. 
WaitIrql and then sets the processor 
IRQL to the WaitIrql of the new incoming thread (which could be PASSIVE_LEVEL). This creates an 
Such a possibility is common and entirely normal, proving that when it comes to thread execution, the 
scheduler outweighs any IRQL considerations. It is only by raising to DISPATCH_LEVEL, which disables 
thread preemption, that IRQLs supersede the scheduler. Since APC_LEVEL is the only IRQL that ends up 
behaving this way, it is often called a thread-local IRQL
approximation for the behavior described herein.
Regardless of how APCs are disabled by a kernel developer, one rule is paramount: Code can neither 
return to user mode with the APC at anything above PASSIVE_LEVEL nor can SpecialApcDisable be set 
to anything but 0. Such situations result in an immediate bugcheck, typically meaning some driver has 
forgotten to release a lock or leave its guarded region.
In addition to two APC modes, there are two types of APCs for each mode—normal APCs and spe-
cial APCs—both of which behave differently depending on the mode. We describe each combination:
I 
Special Kernel APC This combination results in an APC that is always inserted at the tail of
all other existing special kernel APCs in the APC queue but before any normal kernel APCs. The
kernel routine receives a pointer to the arguments and to the normal routine of the APC and
operates at APC_LEVEL, where it can choose to queue a new, normal APC.
I 
Normal Kernel APC This type of APC is always inserted at the tail end of the APC queue, al-
lowing for a special kernel APC to queue a new normal kernel APC that will execute soon there-
after, as described in the earlier example. These kinds of APCs can not only be disabled through
the mechanisms presented earlier but also through a third API called KeEnterCriticalRegion
(paired with KeLeaveCriticalRegion), which updates the KernelApcDisable counter in KTHREAD
but not SpecialApcDisable.
I 
kernel routine at APC_LEVEL, sending it pointers to the argu-
ments and the normal routine
drop the IRQL to PASSIVE_LEVEL and execute the normal routine as well, with the input argu-
ments passed in by value this time. Once the normal routine returns, the IRQL is raised back to
APC_LEVEL again.
I 
Normal User APC This typical combination causes the APC to be inserted at the tail of the
APC queue and for the kernel routineAPC_LEVEL in the same way as the
preceding bullet. If a normal routine is still present, then the APC is prepared for user-mode
64 
CHAPTER 8 System mechanisms
delivery (obviously, at PASSIVE_LEVEL) through the creation of a trap frame and exception 
frame that will eventually cause the user-mode APC dispatcher in Ntdll.dll to take control of the 
thread once back in user mode, and which will call the supplied user pointer. Once the user-
mode APC returns, the dispatcher uses the NtContinue or NtContinueEx system call to return to 
the original trap frame. 
I 
Note that if the kernel routine ended up clearing out the normal routine, then the thread, if
alerted, loses that state, and, conversely, if not alerted, becomes alerted, and the user APC
performed by the KeTestAlertThread
executed in user mode, even though the kernel routine cancelled the dispatch.
I 
Special User APC This combination of APC is a recent addition to newer builds of Windows 10
and generalizes a special dispensation that was done for the thread termination APC such that
(noncurrent) thread requires the use of an APC, but it must also only occur once all kernel-mode
quite well, but it would mean that a user-mode developer could avoid termination by perform-
kernel routine of a User 
APC was KiSchedulerApcTerminate. In this situation, the User APC was recognized as being “special” 
pending” state was always set, which forced execution of the APC at the next user-mode ring transi-