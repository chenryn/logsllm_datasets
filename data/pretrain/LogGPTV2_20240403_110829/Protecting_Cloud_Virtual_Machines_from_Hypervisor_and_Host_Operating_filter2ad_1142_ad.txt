mentation [45] ported to run in EL2 performs the decryption.
We include only two small yet sufﬁcient crypto libraries in EL2
to keep the TCB small. This limits the number of crypto al-
gorithms, but avoids including comprehensive but excessively
large crypto libraries such as OpenSSL. HypSec leverages
AES to support encrypted VM migration and snapshot, and
ensures only encrypted VM data is exposed to the hostvisor.
HypSec’s hardware requirements can also be satisﬁed on
Intel’s x86 architecture by using Virtual Machine Extensions
(VMX) [35] and the IOMMU. Existing x86 hypervisors can be
retroﬁtted to run the corevisor in VMX root operation which
allows control of virtualization features for deprivileging the
hostvisor. The hostvisor runs in VMX non-root operation to
provide resource management and virtual I/O. The corevisor
protects VM execution state by managing a Virtual-Machine
Control Structure (VMCS) per CPU, and VM memory by using
Extended Page Tables (EPT) and controlling the IOMMU.
5 Security Analysis
We present ﬁve properties of the HypSec architecture, then
discuss how their combination provides a set of security
properties regarding HypSec’s ability to protect the integrity
and conﬁdentiality of VM data.
Property 1. HypSec’s corevisor is trusted during the system’s
lifetime against remote attackers.
HypSec leverages hardware secure boot to ensure only the
signed and trusted HypSec binary can be booted. This prevents
an attacker from trying to boot or reboot the system to force it to
load a malicious corevisor. The hostvisor securely installs the
corevisor during the boot process before network access and
serial input service are available. Thus, remote attackers can-
not compromise the hostvisor prior to or during the installation
of the corevisor. The corevisor protects itself after initializa-
tion. It runs in a privileged CPU mode using a separate address
space from the hostvisor and the VMs. The corevisor has full
control of the hardware including the virtualization features
that prevent attackers from disabling its VM protection. The
corevisor also protects its page tables so an attacker cannot
map executable memory to the corevisor’s address space.
Property 2. HypSec ensures only trusted VM images can be
booted on VMs.
Based on Property 1, the trusted corevisor veriﬁes the signa-
tures of the VM images loaded to VM memory before they are
booted. The public keys and signatures are stored using TEE
APIs for persistent secure storage. A compromised hostvisor
therefore cannot replace a veriﬁed VM with a malicious one.
Property 3. HypSec isolates a given VM’s memory from all
other VMs and the hostvisor.
Based on Property 1, HypSec prevents the hostvisor and
a given VM from accessing memory owned by other VMs.
The corevisor tracks ownership of physical pages and enforces
inter-VM memory isolation using nested paging hardware. A
compromised hostvisor could control a DMA capable device
to attempt to access VM memory or compromise the corevisor.
However, the corevisor controls the IOMMU and its page
tables, so the hostvisor cannot access corevisor or VM memory
via DMA. VM pages reclaimed by the hostvisor are scrubbed
by the corevisor, so they do not leak VM data. HypSec also
protects the integrity of VM nested page tables. The corevisor
manages shadow page tables for VMs. The MMU can only
walk the shadow page tables residing in a protected memory
region only accessible to the corevisor. The corevisor manages
USENIX Association
28th USENIX Security Symposium    1365
and veriﬁes updates to the shadow page tables to protect VM
memory mappings.
Property 4. HypSec protects a given VM’s CPU registers
from the hostvisor and all other VMs.
HypSec protects VM CPU registers by only granting
the trusted corevisor (Property 1) full access to them. The
hostvisor cannot access VM registers without permission.
Attackers cannot compromise VM execution ﬂow since only
the corevisor can update VM registers including program
counter (PC), link register (LR), and TTBR.
Property 5. HypSec protects the conﬁdentiality of a given
VM’s I/O data against the hostvisor and all other VMs assum-
ing the VM employs an end-to-end approach to secure I/O.
Based on Properties 3 and 4, HypSec protects any I/O en-
cryption keys loaded to VM CPU registers or memory, so a
compromised hostvisor cannot steal these keys to decrypt en-
crypted I/O data. The same protection holds against other VMs.
Property 6. HypSec protects the conﬁdentiality and integrity
of a given VM’s I/O data against the hostvisor and all other
VMs assuming the VM employs an end-to-end approach to
secure I/O and the I/O can be veriﬁed before it permanently
modiﬁes the VM’s I/O data.
Using the reasoning in Property 5 with the additional
assumption that I/O can be veriﬁed before it permanently
modiﬁes I/O data, HypSec also protects the integrity of VM
I/O data, as any tampered data will be detected and can be
discarded. For example, a network endpoint receiving I/O
from a VM over an encrypted channel with authentication can
detect modiﬁcations of the I/O data by any intermediary such
as the hostvisor. If veriﬁcation is not possible, then HypSec
cannot prevent compromises of data availability that result in
destruction of I/O data, which can affect data integrity. As an
example, HypSec cannot prevent an attacker from arbitrarily
destroying a VM’s I/O data by blindly overwriting all or parts
of a VM’s local disk image; both the VM’s availability and
integrity are compromised since the data is destroyed. Secure
disk backups can protect against permanent data loss.
Property 7. Assuming a VM takes an end-to-end approach
for securing its I/O, HypSec protects the conﬁdentiality of all
of the VM’s data against a remote attacker, including if the
attacker compromises any other VMs or the hostvisor itself.
Based on Properties 1, 3, and 4, a remote attacker cannot
compromise the corevisor, and any compromise of the
hostvisor or another VM cannot allow the attacker to access
VM data stored in CPU registers or memory. This combined
with Property 5 allows HypSec to ensure the conﬁdentiality
of all of the VM’s data.
Property 8. Under the assumption that a VM takes an
end-to-end approach for securing its I/O and I/O can be
veriﬁed before it permanently modiﬁes any VM data, HypSec
protects the integrity of all of the VM’s data against a remote
attacker, including if the attacker compromises any other VMs
or the hostvisor itself.
Based on Properties 1, 3, and 4, HypSec ensures a remote
attacker cannot compromise the corevisor, and that any
compromise of the hostvisor or another VM cannot allow the
attacker to access VM data stored in CPU registers or memory,
thereby preserving VM CPU and memory data integrity.
This combined with Property 6 allows HypSec to ensure the
integrity of all of the VM’s data.
Property 9. If the hypervisor is benign and responsible for
handling I/O, HypSec protects the conﬁdentiality and integrity
of all of the VM’s data against any compromises of other VMs.
If both the hostvisor and corevisor are not compromised
and the hostvisor is responsible for handling I/O, then the
conﬁdentiality and integrity of a VM’s I/O data will be
protected against other VMs. This combined with Properties 3
and 4 allows HypSec to ensure the conﬁdentiality and integrity
of all of the VM’s data. This guarantee is equivalent to what
is provided by a traditional hypervisor such as KVM.
6 Experimental Results
We quantify the performance and TCB of HypSec compared to
other approaches, and demonstrate HypSec’s ability to protect
VM conﬁdentiality and integrity. All of our experiments were
run on ARM server hardware with VE support, speciﬁcally
a 64-bit ARMv8 AMD Seattle (Rev.B0) server with 8
Cortex-A57 CPU cores, 16 GB of RAM, a 512 GB SATA3
HDD for storage, an AMD 10 GbE (AMD XGBE) NIC device,
and an IOMMU (SMMU-401) to support control over DMA
devices and direct device assignment. The hardware did not
support ARM Virtualization Host Extensions [20, 21]. For
client-server experiments, the clients ran on an x86 machine
with 24 Intel Xeon CPU 2.20 GHz cores and 96 GB RAM. The
clients and the server communicated via a 10 GbE unsaturated
network connection.
To provide comparable measurements across the ap-
proaches, we kept the software environments across all
platforms as uniform as possible. We compared KVM with
our HypSec modiﬁcations versus standard KVM, both in
Linux 4.18 with QEMU 2.3.50. In both cases, KVM was
conﬁgured with its standard VHOST virtio network, and with
cache=none for its virtual block storage devices [30, 47, 77].
All hosts and VMs used Ubuntu 16.04 with the same Linux
4.18 kernel, except for HypSec changes. All VMs used par-
avirtualized I/O, typical of cloud infrastructure deployments
such as Amazon EC2.
We ran benchmarks both natively on the hardware and in
VMs. Each physical or VM instance was conﬁgured as a 4-way
SMP with 12 GB of RAM to provide a common basis for com-
parison. This involved two conﬁgurations: (1) native Linux
capped at 4 cores and 12 GB RAM, and (2) a VM using KVM
with 8 cores and 16 GB RAM with the VM capped at 4 virtual
CPUs (VCPUs) and 12 GB RAM. We measure multi-core con-
ﬁgurations to reﬂect real-world server deployments. For VMs,
1366    28th USENIX Security Symposium
USENIX Association
Name
Hypercall
I/O Kernel
I/O User
Virtual IPI
Description
Transition from the VM to the hypervisor and
return to the VM without doing any work in the
hypervisor. Measures bidirectional base transition
cost of hypervisor operations.
Trap from the VM to the emulated interrupt controller
in the hypervisor OS kernel, and then return to
the VM. Measures a frequent operation for many
device drivers and baseline for accessing I/O devices
supported by the hypervisor OS kernel.
Trap from the VM to the emulated UART in QEMU
and then return to the VM. Measures base cost of
operations that access I/O devices emulated in the
hypervisor OS user space.
Issue a virtual IPI from a VCPU to another VCPU
running on a different PCPU, both PCPUs executing
VM code. Measures time between sending the
virtual IPI until the receiving VCPU handles it, a
frequent operation in multi-core OSes.
Name
Kernbench Compilation of
Description
the Linux 4.9 kernel using
Netperf
Apache
Hackbench
allnoconfig for ARM with GCC 5.4.0.
hackbench [66] using Unix domain sockets and 100
process groups running in 500 loops.
netperf v2.6.0 [41] running netserver on the
server and the client with its default parameters in
three modes: TCP_STREAM (throughput), TCP_-
MAERTS (throughput), and TCP_RR (latency).
running
Apache
ApacheBench [80] v2.3 on the remote client,
which measures number of handled requests per
second when serving the 41 KB index.html ﬁle of
the GCC 4.4 manual using 100 concurrent requests.
Memcached memcached v1.4.25 using the memtier benchmark
v2.4.18 Web
server
MySQL
v1.2.3 with its default parameters.
MySQL v14.14 (distrib 5.7.24) running SysBench
v.0.4.12 using the default conﬁguration with 200
parallel transactions.
Table 2: Microbenchmarks
Table 4: Application Benchmarks
we pinned each VCPU to a speciﬁc physical CPU (PCPU) and
ensured that no other work was scheduled on that PCPU. All
of the host’s device interrupts and processes were assigned to
run on other PCPUs. For client-server benchmarks, the clients
ran natively on Linux and used the full hardware available.
6.1 Microbenchmark Results
We ﬁrst ran microbenchmarks to quantify the cost of
low-level hypervisor operations. We used the KVM unit
test framework [48] listed in Table 2 to measure the cost of
transitioning between the VM and the hypervisor, initiating
a VM-to-hypervisor OS kernel I/O request, emulating user
space I/O with QEMU, and sending virtual IPIs. We slightly
modiﬁed the test framework to measure the cost of virtual IPIs
and to obtain cycle counts on ARM to ensure detailed results
by conﬁguring the VM with direct access to the cycle counter.
Microbenchmark KVM HypSec
Hypercall
3,202
4,563
I/O Kernel
10,704
I/O User
Virtual IPI
10,047
2,896
3,831
9,288
8,816
Table 3: Microbenchmark Measurements (cycles)
Table 3 shows the microbenchmarks measured in cycles
for both standard KVM and HypSec. HypSec introduces
roughly 5% to 19% overhead over KVM. HypSec does not
increase the number of traps in the operations we measured.
The corevisor interposes on exisiting traps to add additional
logic to protect VM data, so the cost is relatively small. The
I/O Kernel, I/O User, and Virtual IPI measurements show
relatively higher overhead than Hypercall on HypSec because
of the cost involved to secure data transfers between the VM
and hostvisor for I/O and interrupt virtualization.
6.2 Application Workload Results
Next we ran real application workloads to evaluate HypSec
compared to standard KVM. Table 4 lists the workloads which
are a mix of widely-used CPU and I/O intensive benchmarks.
To evaluate VM performance with end-to-end I/O protection,
we used ﬁve conﬁgurations: (1) Native unmodiﬁed Linux host
kernel without Full Disk Encryption, (2) Unmodiﬁed KVM
and guest kernel without FDE (KVM), (3) Unmodiﬁed KVM
and guest kernel with FDE (KVM-FDE), (4) HypSec and par-
avirtualized guest kernel without FDE (HypSec), (5) HypSec
and paravirtualized guest kernel with FDE (HypSec-FDE).
For FDE, we use dm-crypt to create a LUKS-encrypted root
partition of the VM ﬁlesystem. We measure with and without
FDE to separately quantify its extra costs. We leveraged the
TLS/SSL support in Apache and MySQL and evaluated VM
performance on HypSec with end-to-end network encryption.
Figure 4 shows the relative overhead of executing in a VM
in our four VM conﬁgurations compared to natively. We
normalize the results so that a value of 1.00 means the same
performance as native hardware. Lower numbers mean less
overhead. The performance on real application workloads
shows modest overhead overall for HypSec compared to
standard KVM. The overhead for HypSec in many cases is
less than 10%, even with FDE enabled.
The worst overhead for HypSec occurs for some of the net-
work workloads. Our current implementation of the front-end
network virtio driver applies grant/revoke hypercalls on a per
transaction basis to make data available to the back-end driver
in the hostvisor. Therefore, HypSec’s performance is sub-
optimal in workloads where the virtio driver can batch multiple
transactions without trapping to the hypervisor, most notably
in TCP_MAERTS. TCP_MAERTS measures the bandwidth
of a VM sending packets to a client. The virtio driver batches
multiple sends to avoid traps to hypervisor, while in the imple-
mentation measured in the paper, the driver traps additionally
USENIX Association
28th USENIX Security Symposium    1367
LOC
Components