two ways to retrieve the location information of tweets: namely
from the user proﬁles and from the message content itself. The
message itself is a more valuable source because it presumably pro-
vides the location directly related to performance issues. Therefore,
we ﬁrst rely on the location information in messages. If there is no
such information, then we consider the location in the user pro-
ﬁle. Because both proﬁle and messages are human language inputs
without ﬁxed structure, the location information could be missing
290Table 2: The delay between the customers experience the inci-
dents and the customers report the incidents.
Delay
Tweets
Tickets
> 1 day
0 day
1 day
98.3% 1.2%
0.5%
38.4% 21.2% 40.4%
or incomplete. There are three general categories of location in-
formation identiﬁed in our analysis: city + state, state only, others
(e.g. city name only, street). Table 1 depicts the breakdown across
these categories of the location information obtained from the user
proﬁles and tweets analyzed. We observe that a large proportion
of tweets have no location information at all - 37% of proﬁles have
no location information and 69% of Twitter messages do not con-
tain location speciﬁc information. It reduces the number of valid
tweets for later correlation purposes. In contrast, the location infor-
mation in tickets is well organized based on the market regions. A
single market could include multiple states (e.g. Georgia + South
Carolina) or cover just a part of one populous state (e.g. Northern
California). Different data sources have different granularity of lo-
cation information. Therefore, we use the most detailed common
location for correlation purposes in Section 3.3 .
3.2.3 Timeliness
There is inherently a delay between the time when a customer ex-
periences an incident and the time when the customer reports that
incident. The reporting time is deﬁned here to be the timestamp of
when the Twitter message was posted or when the customer called
customer care. It can be directly retrieved from the data. On the
other hand, the time associated with when a customer experiences
an incident is often hard to determine - if it is available, it will
be embedded within a Twitter message or in a ticket’s description.
For example, in Twitter, the message may reference “a couple of
call drops today"; in tickets, the description may mention“no ser-
vice since yesterday". We extract such information from the tweets
and tickets in a simple way: we identify the timing patterns like
“now, today, yesterday, 3 days, 2010-04-03, this morning". We
observed that over 90.1% of our collected tweets do have such tim-
ing information.
In comparison, fewer than 23.7% of ticket de-
scriptions contain explicit timing information. We then computed
differences between the times when the issues were reported and
the estimated incident times as extracted from the tweet and ticket
contents. The resulting distribution is illustrated in Table 2. We
observe that most tweets are posted on the same day as the cus-
tomers experience the performance issues. In contrast, most tickets
are opened one or more days after the incident being reported. The
result indicates that Twitter users respond much faster than those
reporting issues via customer care. However, we should notice that
these two groups of users may care about different kinds of perfor-
mance issues. Therefore we cannot conclude that Tweets are faster
than tickets for any particular incident.
3.3 Correlation Results
In this section, we correlate both performance related tweets and
tickets with events reported in the major network incidents report.
Figure 3 shows the time series of the incidents, tweets and tickets.
Different from Figure 1, performance related tweets demonstrate
only a weak diurnal pattern as shown in Figure 3. This implies
that most of these Twitter messages are incident-driven. But cus-
tomer tickets still have the strong diurnal pattern, because of the
customers’ calling behavior and aforementioned delay in incident
reporting.
To understand if there is a strong correlation between incident re-
ports and customer feedback, we conduct a statistic correlation [12]
between the incidents and tweets/tickets series, for each location.
We compute Pearson’s coefﬁcient of correlation and conduct sig-
niﬁcance tests. Unfortunately, the test result shows no strong corre-
lation between incidents and customers’ experiences. One possible
reason is the time lag from the time when the incidents start to the
time when users report performance problems. Correction of this
time lag by some time shifting should be applied for the correla-
tion tests. The challenge is that the time lags vary case by case and
cannot be compensated systematically. We will leave the design of
new statistical signiﬁcance test methodology under this particular
situation for the future work.
Instead of using statistical correlation, we use the incidents in the
report as the ground truth and investigate whether these incidents
are reported within the collected tweets/tickets. More speciﬁcally,
if we observe tweets / tickers during the incident and occurring
within the same region (e.g. east, west, central, etc.), we deem these
tweets or tickets to be associated with the incidents. We verify
the incidents which last less than 3 hours. The reason to verify
relatively short-term incidents is that we have more conﬁdence with
the joins when the time window is limited.
In other words, the
chance of falsely joining an incident to independent tickets/tweets
is low. Because of this ﬁltering process, the results we observe
during the correlation are valid for relatively short term incidents.
We ﬁnd that 55.6% of incidents can be found in tweets, and only
37.0% of them found in customer care tickets.3 One interesting
observation is that the matches found in tickets can also be found in
tweets. Moreover, we use the number of associated tweets/tickets
divided by the accumulated duration of incidents to describe the
chance of observing tweets/tickets when incidents happen, denoted
as c1. Similarly, the chance of observing tweets/tickets when no
incident happens, say c2, can be measured as the number of un-
associated tweets/tickets divided by the accumulated duration of
non-incidents during the measurement period. We ﬁnd that the ratio
c1/c2 is 8.3 for tweets and 6.8 for tickets. This suggests that the
chance of having user feedback during an incident is signiﬁcantly
higher than that when there are no ongoing incidents.
In Section 3.2.3 we quantitatively studied the delay from the time
when customers experience the service impact till the time when
customers report the issue. In fact, there is another delay between
the time when incidents take place and the time when users ex-
perience problems. However, the time when users experience the
incidents is not an accurate timestamp, as we have discussed in
Section 3.2.3. Therefore, we now quantitatively analyze the total
delay from the time when the incidents start till the time when cus-
tomers report the problem. Figure 4 shows the box statistics of two
delay distributions. The bottom and top of the box are the 25th
and 75th percentile, and the band near the middle of the box is the
50th percentile (the median). The ends of the whiskers represent
the minimum and maximum value. The maximum value in both
cases is approximately 80 minutes. Note that we ﬁltered out all
incidents exceeding 3 hours, which thus serves as the upper bound
of the delay. We ﬁnd that Twitter users respond approximately 10
minutes faster than customer who call the customer care team in
general. The fastest Twitter users response is in several minutes.
The implication of this observation is that it may be possible to
utilize Twitter users’ feedback to observe the impact of network
performance issues more timely than using customer tickets.
Finally, let us revisit Figure 3. There are obvious spikes on
Day 11 for both Twitter and customer care data, even though there
3Due to limited coarse location information, there are some poten-
tial mismatch cases or false positive.
291s
t
n
e
d
c
n
i
i
f
o
r
e
b
m
u
N
s
e
g
a
s
s
e
m
f
o
r
e
b
m
u
N
s
t
e
k
c
i
t
f
o
r
e
b
m
u
N
0
1
6
2
Daily incidents
Date
Twitter (Performance related)
Day 4
Day 9
Time (UTC)
Day 14
Customer care tickets (Performance related)
Day 4
Day 9
Time (UTC)
Day 14
Figure 3: The time series of incidents, tweets and tickets. Due to privacy issues, the concrete numbers of incidents and tickets are not
reported.
)
s
e
t
u
n
m
i
(
y
a
e
D
l
0
8
0
6
0
4
0
2
0
Tickets
Twitter
Figure 4: The CDF of delay distribution.
are very few incidents recorded in the same period of time. More
speciﬁcally, there are many complaints from both Twitter and cus-
tomer care regarding call drops during 8 PM to 10 PM in the central
area. This may be an indication of certain short term network prob-
lems at that time, but yet none were reported in the major incident
reports covering the area.
4. RELATED WORK
There have been a number of studies regarding the social net-
work. However, most of them focus on the social network itself,
e.g., users behavior [18, 13, 20, 2], the impact on network per-
formance [21], community evolution [7, 14], information propa-
gation [23, 5, 4], privacy issues [8, 6]. Very few studies focus on
showing the value of social network content. Vieweg et al. shows
the microblogging such as Twitter can contribute to the situation
awareness during natural hazards events like ﬂoods and ﬁre [22].
Motoyama et al. use Twitter data to infer on-line Internet service
availability [19].
Correlating across data sources is a common methodology used
in anomaly detection [15, 9, 3] and network problem diagnosis [12,
10, 17, 16, 11]. Most of these papers focus on the derivation of
statistics methods. Our study in this paper takes the ﬁrst step to re-
veal the possibility of utilizing social media content as a new source
to understand user experience of mobile networks.
2925. CONCLUSION AND FUTURE WORK
We have presented the primary study of exploiting the social net-
work content for network performance monitoring. Our data sug-
gest that users’ feedback regarding network incidents is often ob-
served in twitter messages in a timely fashion. Therefore, it is a
complementary source for understanding network performance is-
sues and their impact on user experience.
As future work, we plan to apply advanced natural language pro-
cessing techniques to better understand tweets, as well as tickets
and incident reports. For example, it is important to advance the
technique for intelligently extracting performance related issues
from tweets. It would also be interesting to quantify the severity
level of performance related tweets by the scale of the responses
and the sentiment in the messages (e.g.
to detect urgent issues
based on users’ attitude).
6. REFERENCES
[1] comScore 2009 US Digital Year in Review, 2009.
http://www.comscore.com/Press_Events/
Presentations_Whitepapers/2010/The_2009_
U.S._Digital_Year_in_Review.
[2] Fabrício Benevenuto, Tiago Rodrigues, Meeyoung Cha, and
Virgílio A. F. Almeida. Characterizing user behavior in
online social networks. In Internet Measurement Conference,
pages 49–62, 2009.
[3] Daniela Brauckhoff, Xenofontas Dimitropoulos, Arno
Wagner, and Kavè Salamatian. Anomaly extraction in
backbone networks using association rules. In Proc. ACM
IMC, 2009.
[4] Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and
Krishna P. Gummadi. Measuring User Inﬂuence in Twitter:
The Million Follower Fallacy. In In Proceedings of the 4th
International AAAI Conference on Weblogs and Social
Media (ICWSM).
[5] Meeyoung Cha, Alan Mislove, and Krishna P. Gummadi. A
measurement-driven analysis of information propagation in
the ﬂickr social network. In WWW ’09: Proceedings of the
18th international conference on World wide web, pages
721–730, New York, NY, USA, 2009. ACM.
[6] Catherine Dwyer, Starr Roxanne Hiltz, and Katia Passerini.
Trust and privacy concern within social networking sites: A
comparison of facebook and myspace. In Proceedings of the
Thirteenth Americas Conference on Information Systems (
AMCIS 2007), 2007. Paper 339.
[7] Sanchit Garg, Trinabh Gupta, Niklas Carlsson, and Anirban
Mahanti. Evolution of an online social aggregation network:
an empirical study. In IMC ’09: Proceedings of the 9th ACM
SIGCOMM conference on Internet measurement conference,
pages 315–321, New York, NY, USA, 2009. ACM.
[8] Ralph Gross and Alessandro Acquisti. Information revelation
and privacy in online social networks. In WPES ’05:
Proceedings of the 2005 ACM workshop on Privacy in the
electronic society, pages 71–80, New York, NY, USA, 2005.
ACM.
[9] Yiyi Huang, Nick Feamster, Anukool Lakhina, and Jim (Jun)
Xu. Diagnosing network disruptions with network-wide
analysis. SIGMETRICS Perform. Eval. Rev., 35(1):61–72,
2007.
[10] Srikanth Kandula, Ranveer Chandra, and Dina Katabi. What
is going on? learning communication rules in edge networks.
In Proc. ACM SIGCOMM, 2008.
[11] Srikanth Kandula, Ratul Mahajan, Patrick Verkaik, Sharad
Agarwal, Jitendra Padhye, and Paramvir Bahl. Detailed
diagnosis in enterprise networks. In Proc. ACM SIGCOMM,
2009.
[12] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren.
Detection and localization of network blackholes. In Proc.
INFOCOM, 2007.
[13] Balachander Krishnamurthy, Phillipa Gill, and Martin Arlitt.
A few chirps about twitter. In WOSP ’08: Proceedings of the
ﬁrst workshop on Online social networks, pages 19–24, New
York, NY, USA, 2008. ACM.
[14] Haewoon Kwak, Yoonchan Choi, Young-Ho Eom, Hawoong
Jeong, and Sue Moon. Mining communities in networks: a
solution for consistency and its evaluation. In IMC ’09:
Proceedings of the 9th ACM SIGCOMM conference on
Internet measurement conference, pages 301–314, New
York, NY, USA, 2009. ACM.
[15] Anukool Lakhina, Mark Crovella, and Christophe Diot.
Mining anomalies using trafﬁc feature distributions. In
SIGCOMM ’05: Proceedings of the 2005 conference on
Applications, technologies, architectures, and protocols for
computer communications, pages 217–228, New York, NY,
USA, 2005. ACM.
[16] A. Mahimkar, Z. Ge, , A. Shaikh, J. Wang J. Yates, Y. Zhang,
, and Q. Zhao. Towards Automated Performance Diagnosis
in a Large IPTV Network. In Proc. ACM SIGCOMM, 2009.
[17] A. Mahimkar, J. Yates, Y. Zhang, A. Shaikh, J. Wang, Z. Ge,
and C. T. Ee. Troubleshooting chronic conditions in large ip
networks. In Proc. ACM CoNEXT, 2008.
[18] Alan Mislove, Massimiliano Marcon, Krishna P. Gummadi,
Peter Druschel, and Bobby Bhattacharjee. Measurement and
analysis of online social networks. In IMC ’07: Proceedings
of the 7th ACM SIGCOMM conference on Internet
measurement, pages 29–42, New York, NY, USA, 2007.
ACM.
[19] Marti Motoyama, Brendan Meeder, Kirill Levchenko,
Geoffrey M. Voelker, and Stefan Savage. Measuring online
service availability using twitter. In USENIX 3rd Workshop
on Online Social Networks (WOSN), 2010.
[20] Atif Nazir, Saqib Raza, Dhruv Gupta, Chen-Nee Chuah, and
Balachander Krishnamurthy. Network level footprints of
facebook applications. In IMC ’09: Proceedings of the 9th
ACM SIGCOMM conference on Internet measurement
conference, pages 63–75, New York, NY, USA, 2009. ACM.
[21] Fabian Schneider, Anja Feldmann, Balachander
Krishnamurthy, and Walter Willinger. Understanding online
social network usage from a network perspective. In Anja
Feldmann and Laurent Mathy, editors, Internet Measurement
Conference, pages 35–48. ACM, 2009.
[22] Sarah Vieweg, Amanda L. Hughes, Kate Starbird, and Leysia
Palen. Microblogging during two natural hazards events:
what twitter may contribute to situational awareness. In CHI
’10: Proceedings of the 28th international conference on
Human factors in computing systems, pages 1079–1088,
New York, NY, USA, 2010. ACM.
[23] Bimal Viswanath, Alan Mislove, Meeyoung Cha, and
Krishna P. Gummadi. On the evolution of user interaction in
facebook. In WOSN ’09: Proceedings of the 2nd ACM
workshop on Online social networks, pages 37–42, New
York, NY, USA, 2009. ACM.
293