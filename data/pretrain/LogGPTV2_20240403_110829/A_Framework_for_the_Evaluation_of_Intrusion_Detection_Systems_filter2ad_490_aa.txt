title:A Framework for the Evaluation of Intrusion Detection Systems
author:Alvaro A. C&apos;ardenas and
John S. Baras and
Karl Seamon
A Framework for the Evaluation of Intrusion Detection Systems
Alvaro A. C´ardenas
John S. Baras Karl Seamon
Department of Electrical and Computer Engineering
∗
and The Institute of Systems Research
University of Maryland, College Park
{acardena,baras,kks}@isr.umd.edu
Abstract
Classiﬁcation accuracy in intrusion detection systems
(IDSs) deals with such fundamental problems as how to
compare two or more IDSs, how to evaluate the perfor-
mance of an IDS, and how to determine the best conﬁgu-
ration of the IDS. In an effort to analyze and solve these
related problems, evaluation metrics such as the Bayesian
detection rate, the expected cost, the sensitivity and the in-
trusion detection capability have been introduced. In this
paper, we study the advantages and disadvantages of each
of these performance metrics and analyze them in a uniﬁed
framework. Additionally, we introduce the intrusion detec-
tion operating characteristic (IDOC) curves as a new IDS
performance tradeoff which combines in an intuitive way
the variables that are more relevant to the intrusion detec-
tion evaluation problem. We also introduce a formal frame-
work for reasoning about the performance of an IDS and
the proposed metrics against adaptive adversaries. We pro-
vide simulations and experimental results to illustrate the
beneﬁts of the proposed framework.
1. Introduction
Consider a company that, in an effort to improve its in-
formation technology security infrastructure, wants to pur-
chase either intrusion detector 1 (I DS 1) or intrusion de-
tector 2 (I DS 2). Furthermore, suppose that the algorithms
used by each IDS are kept private and therefore the only
way to determine the performance of each IDS (unless
some reverse engineering is done [15]) is through empirical
tests determining how many intrusions are detected by each
scheme while providing an acceptable level of false alarms.
Suppose these tests show with high conﬁdence that I DS 1
detects one-tenth more attacks than I DS 2 but at the cost of
This material is based upon work supported by the U.S. Army Re-
search Ofﬁce under Award No. DAAD19-01-1-0494 to the University of
Maryland College Park.
∗
producing one hundred times more false alarms. The com-
pany needs to decide based on these estimates, which IDS
will provide the best return of investment for their needs and
their operational environment.
This general problem is more concisely stated as the
intrusion detection evaluation problem, and its solution
usually depends on several factors. The most basic of
these factors are the false alarm rate and the detection
rate, and their tradeoff can be intuitively analyzed with the
help of the receiver operating characteristic (ROC) curve
[16, 17, 35, 7, 14]. However, as pointed out in [3, 9, 10],
the information provided by the detection rate and the false
alarm rate alone might not be enough to provide a good
evaluation of the performance of an IDS. Therefore, the
evaluation metrics need to consider the environment the IDS
is going to operate in, such as the maintenance costs and the
hostility of the operating environment (the likelihood of an
attack). In an effort to provide such an evaluation method,
several performance metrics such as the Bayesian detection
rate [3], expected cost [9], sensitivity [6] and intrusion de-
tection capability [10], have been proposed in the literature.
Yet despite the fact that each of these performance met-
rics makes their own contribution to the analysis of intrusion
detection systems, they are rarely applied in the literature
when proposing a new IDS. It is our belief that the lack
of widespread adoption of these metrics stems from two
main reasons. Firstly, each metric is proposed in a different
framework (e.g. information theory, decision theory, cryp-
tography etc.) and in a seemingly ad hoc manner. Therefore
an objective comparison between the metrics is very difﬁ-
cult.
The second reason is that the proposed metrics usually
assume the knowledge of some uncertain parameters like
the likelihood of an attack, or the costs of false alarms and
missed detections. Moreover, these uncertain parameters
can also change during the operation of an IDS. Therefore
the evaluation of an IDS under some (wrongly) estimated
parameters might not be of much value.
More importantly, there does not exist a security model
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:52:29 UTC from IEEE Xplore.  Restrictions apply. 
for the evaluation of intrusion detection systems. Several
researchers have pointed out the need to include the resis-
tance against attacks as part of the evaluation of an IDS
[25, 27, 11, 34, 29, 30, 13]. However, the traditional evalu-
ation metrics are based on ideas mainly developed for non-
security related ﬁelds and therefore, they do not take into
account the role of an adversary and the evaluation of the
system against this adversary. In particular, it is important
to realize that when we borrow tools from other ﬁelds, they
come with a set of assumptions that might not hold in an ad-
versarial setting, because the ﬁrst thing that the intruder will
do is violate the sets of assumptions that the IDS is relying
on for proper operation.
1.1. Our Contributions
In this paper, we introduce a framework for the eval-
uation of IDSs in order to address the concerns raised in
the previous section. First, we identify the intrusion de-
tection evaluation problem as a multi-criteria optimization
problem. This framework will let us compare several of the
previously proposed metrics in a uniﬁed manner. To this
end, we recall that there are in general two ways to solve a
multi-criteria optimization problem. The ﬁrst approach is to
combine the criteria to be optimized in a single optimization
problem. We then show how the intrusion detection capabil-
ity, the expected cost and the sensitivity metrics all fall into
this category. The second approach to solve a multi-criteria
optimization problem is to evaluate a tradeoff curve. We
show how the Bayesian rates and the ROC curve analysis
are examples of this approach.
To address the uncertainty of the parameters assumed in
each of the metrics, we then present a graphical approach
that allows the comparison of the IDS metrics for a wide
range of uncertain parameters. For the single optimization
problem we show how the concept of isolines can capture in
a single value (the slope of the isoline) the uncertainties like
the likelihood of an attack and the operational costs of the
IDS. For the tradeoff curve approach, we introduce a new
tradeoff curve we call the intrusion detector operating char-
acteristic (IDOC). We believe the IDOC curve combines in
a single graph all the relevant (and intuitive) parameters that
affect the practical performance of an IDS.
Finally, we introduce a robust evaluation approach in or-
der to deal with the adversarial environment the IDS is de-
ployed in.
In particular, we do not want to ﬁnd the best
performing IDS on average, but the IDS that performs best
against the worst type of attacks. To that end we extend
our graphical approach presented in section 4 to model the
attacks against an IDS. In particular, we show how to ﬁnd
the best performing IDS against the worst type of attacks.
This framework will allow us to reason about the security of
the IDS evaluation and the proposed metric against adaptive
adversaries.
In an effort to make this evaluation framework accessible
to other researchers and in order to complement our presen-
tation, we started the development of a software application
available at [2] to implement the graphical approach for the
expected cost and our new IDOC analysis curves. We hope
this tool can grow to become a valuable resource for re-
search in intrusion detection.
2. Notation and Deﬁnitions
In this section we present the basic notation and deﬁni-
tions which we use throughout this paper.
We assume that the input to an intrusion detection sys-
tem is a feature-vector x ∈ X . The elements of x can in-
clude basic attributes like the duration of a connection, the
protocol type, the service used etc. It can also include spe-
ciﬁc attributes selected with domain knowledge such as the
number of failed logins, or if a superuser command was at-
tempted. Examples of x used in intrusion detection are se-
quences of system calls [8], sequences of user commands
[26], connection attempts to local hosts [12], proportion of
accesses (in terms of TCP or UDP packets) to a given port
of a machine over a ﬁxed period of time [19] etc.
Let I denote whether a given instance x was generated
by an intrusion (represented by I = 1 or simply I) or not
(denoted as I = 0 or equivalently ¬I). Also let A denote
whether the output of an IDS is an alarm (denoted by A = 1
or simply A) or not (denoted by A = 0, or equivalently ¬A).
An IDS can then be deﬁned as an algorithm I DS that re-
ceives a continuous data stream of computer event features
X = {x[1],x[2], . . . ,} and classiﬁes each input x[ j] as being
either a normal event or an attack i.e. I DS : X →{A,¬A}.
In this paper we do not address how the IDS is designed.
Our focus will be on how to evaluate the performance of a
given IDS.
Intrusion detection systems are commonly classiﬁed
as either misuse detection schemes or anomaly detection
schemes. Misuse detection systems use a number of attack
signatures describing attacks; if an event feature x matches
one of the signatures, an alarm is raised. Anomaly detec-
tion schemes on the other hand rely on proﬁles or models of
the normal operation of the system. Deviations from these
established models raise alarms.
The empirical results of a test for an IDS are usually
recorded in terms of how many attacks were detected and
how many false alarms were produced by the IDS, in a data
set containing both normal data and attack data. The per-
centage of alarms out of the total number of normal events
monitored is referred to as the false alarm rate (or the prob-
ability of false alarm), whereas the percentage of detected
attacks out of the total attacks is called the detection rate
(or probability of detection) of the IDS. In general we de-
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:52:29 UTC from IEEE Xplore.  Restrictions apply. 
note the probability of false alarm and the probability of
detection (respectively) as:
PFA ≡ Pr[A = 1|I = 0]
PD ≡ Pr[A = 1|I = 1] (1)
and
State of the system
Detector’s report
No Alarm (A=0) Alarm (A=1)
No Intrusion (I = 0)
Intrusion (A = 1)
C(0,0)
C(1,0)
C(0,1)
C(1,1)
These empirical results are sometimes shown with the
help of the ROC curve; a graph whose x-axis is the false
alarm rate and whose y-axis is the detection rate. The
graphs of misuse detection schemes generally correspond
to a single point denoting the performance of the detector.
Anomaly detection schemes on the other hand, usually have
a monitored statistic which is compared to a threshold τ
in order to determine if an alarm should be raised or not.
Therefore their ROC curve is obtained as a parametric plot
of the probability of false alarm (PFA) versus the probability
of detection (PD) (with parameter τ) as in [16, 17, 35, 7, 14].
3. Evaluation Metrics
In this section we ﬁrst introduce metrics that have been
proposed in previous work. Then we discuss how we can
use these metrics to evaluate the IDS by using two general
approaches, that is the expected cost and the tradeoff ap-
proach.
In the expected cost approach, we give intuition
of the expected cost metric by relating all the uncertain pa-
rameters (such as the probability of an attack) to a single
line that allows the IDS operator to easily ﬁnd the optimal
tradeoff. In the second approach, we identify the main pa-
rameters that affect the quality of the performance of the
IDS. This will allow us to later introduce a new evaluation
method that we believe better captures the effect of these
parameters than all previously proposed methods.
3.1. Background Work
Expected Cost.
In this section we present the expected
cost of an IDS by combining some of the ideas originally
presented in [9] and [28]. The expected cost is used as an
evaluation method for IDSs in order to assess the investment
of an IDS in a given IT security infrastructure. In addition
to the rates of detection and false alarm, the expected cost of
an IDS can also depend on the hostility of the environment,
the IDS operational costs, and the expected damage done
by security breaches.
A quantitative measure of the consequences of the output
of the IDS to a given event, which can be an intrusion or not
are the costs shown in Table 1. Here C(0,1) corresponds
to the cost of responding as though there was an intrusion
when there is none, C(1,0) corresponds to the cost of failing
to respond to an intrusion, C(1,1) is the cost of acting upon
an intrusion when it is detected (which can be deﬁned as
a negative value and therefore be considered as a proﬁt for
using the IDS), and C(0,0) is the cost of not reacting to
Table 1. Costs of the IDS reports given the
state of the system
a non-intrusion (which can also be deﬁned as a proﬁt, or
simply left as zero.)
Adding costs to the different outcomes of the IDS is a
way to generalize the usual tradeoff between the probability
of false alarm and the probability of detection to a tradeoff
between the expected cost for a non-intrusion
R(0,PFA) ≡ C(0,0)(1− PFA) +C(0,1)PFA
and the expected cost for an intrusion
R(1, PD) ≡ C(1,0)(1− PD) +C(1,1)PD
It is clear that if we only penalize errors of classiﬁcation
with unit costs (i.e. if C(0,0) = C(1,1) = 0 and C(0,1) =
C(1,0) = 1) the expected cost for non-intrusion and the
expected cost for intrusion become respectively, the false
alarm rate and the detection rate.
The question of how to select the optimal tradeoff be-
tween the expected costs is still open. However, if we let the
hostility of the environment be quantiﬁed by the likelihood
of an intrusion p ≡ Pr[I = 1] (also known as the base-rate
[3]), we can average the expected non-intrusion and intru-
sion costs to give the overall expected cost of the IDS:
E[C(I, A)] = R(0,PFA)(1− p) + R(1, PD)p
(2)
It should be pointed out that R() and E[C(I, A)] are also
known as the risk and Bayesian risk functions (respectively)
in Bayesian decision theory.
Given an IDS, the costs from Table 1 and the likelihood
of an attack p, the problem now is to ﬁnd the optimal trade-
off between PD and PFA in such a way that E[C(I, A)] is
minimized.
The Intrusion Detection Capability.
The main motiva-
tion for introducing the intrusion detection capability CID as
an evaluation metric originates from the fact that the costs in
Table 1 are chosen in a subjective way [10]. Therefore the
authors propose the use of the intrusion detection capability
as an objective metric motivated by information theory:
CID = I(I; A)
H(I)
(3)
where I and H respectively denote the mutual information
and the entropy [5]. The H(I) term in the denominator is
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:52:29 UTC from IEEE Xplore.  Restrictions apply. 
a normalizing factor so that the value of CID will always
be in the [0,1] interval. The intuition behind this metric is
that by ﬁne tuning an IDS based on CID we are ﬁnding the
operating point that minimizes the uncertainty of whether
an arbitrary input event x was generated by an intrusion or
not.
The main drawback of CID is that it obscures the intuition
that is to be expected when evaluating the performance of
an IDS. This is because the notion of reducing the uncer-
tainty of an attack is difﬁcult to quantify in practical values
of interest such as false alarms or detection rates. Informa-
tion theory has been very useful in communications because
the entropy and mutual information can be linked to prac-
tical quantities, like the number of bits saved by compres-
sion (source coding) or the number of bits of redundancy re-
quired for reliable communications (channel coding). How-
ever it is not clear how these metrics can be related to quan-
tities of interest for the operator of an IDS.
The Base-Rate Fallacy and Predictive Value Metrics.
In [3] Axelsson pointed out that one of the causes for the
large amount of false alarms that intrusion detectors gen-
erate is the enormous difference between the amount of
normal events compared to the small amount of intrusion
events. Intuitively, the base-rate fallacy states that because
the likelihood of an attack is very small, even if an IDS
ﬁres an alarm, the likelihood of having an intrusion remains
relatively small. Formally, when we compute the posterior
probability of intrusion (a quantity known as the Bayesian
detection rate, or the positive predictive value (PPV)) given
that the IDS ﬁred an alarm, we obtain:
PPV ≡ Pr[I = 1|A = 1]
Pr[A = 1|I = 1]Pr[I = 1]
=
=
Pr[A = 1|I = 1]Pr[I = 1] + Pr[A = 1|I = 0]Pr[I = 0]
(PD − PFA)p + PFA
(4)
PD p
Therefore, if the rate of incidence of an attack is very