I'm using Keras 2.1.1 and Tensorflow 1.4, Python 3.6, Windows 7.
I'm attempting transfer learning using the Inception model.  
The code is straight from the Keras Application API, just a few tweaks (using
my data).
Here is the code
    from keras.preprocessing import image```
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Dense, GlobalAveragePooling2D
    from keras import backend as K
    from keras import optimizers
    img_width, img_height = 299, 299
    train_data_dir = r'C:\Users\Moondra\Desktop\Keras Applications\data\train'
    total_samples = 13581
    batch_size = 3
    epochs = 5
    train_datagen = ImageDataGenerator(
    rescale = 1./255,
    horizontal_flip = True,
    zoom_range = 0.1,
    rotation_range=15)
    train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size = (img_height, img_width),
    batch_size = batch_size, 
    class_mode = 'categorical')  #class_mode = 'categorical'
    # create the base pre-trained model
    base_model = InceptionV3(weights='imagenet', include_top=False)
    # add a global spatial average pooling layer
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    # let's add a fully-connected layer
    x = Dense(1024, activation='relu')(x)
    # and a logistic layer -- let's say we have 200 classes
    predictions = Dense(12, activation='softmax')(x)
    # this is the model we will train
    model = Model(input=base_model.input, output=predictions)
    # first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional InceptionV3 layers
    for layer in base_model.layers:
        layer.trainable = False
    # compile the model (should be done *after* setting layers to non-trainable)
    model.compile(optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics = ['accuracy'])
    # train the model on the new data for a few epochs
    model.fit_generator(
    train_generator,
    steps_per_epoch = 20,
    epochs = epochs)
    # at this point, the top layers are well trained and we can start fine-tuning
    # convolutional layers from inception V3. We will freeze the bottom N layers
    # and train the remaining top layers.
    # let's visualize layer names and layer indices to see how many layers
    # we should freeze:
    for i, layer in enumerate(base_model.layers):
       print(i, layer.name)
    # we chose to train the top 2 inception blocks, i.e. we will freeze
    # the first 172 layers and unfreeze the rest:
    for layer in model.layers[:249]:
       layer.trainable = False
    for layer in model.layers[249:]:
       layer.trainable = True
    # we need to recompile the model for these modifications to take effect
    # we use SGD with a low learning rate
    from keras.optimizers import SGD
    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics = ['accuracy'])
    # we train our model again (this time fine-tuning the top 2 inception blocks
    # alongside the top Dense layers
    model.fit_generator(
    train_generator,
    steps_per_epoch = 25,
    epochs = epochs)`
    Output is
    Found 13581 images belonging to 12 classes.
    Warning (from warnings module):
      File "C:\Users\Moondra\Desktop\Keras Applications\keras_transfer_learning_inception_problem_one_epoch.py", line 44
        model = Model(input=base_model.input, output=predictions)
    UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
    Epoch 1/5
     1/20 [>.............................] - ETA: 38s - loss: 2.8652 - acc: 0.0000e+00
     3/20 [===>..........................] - ETA: 12s - loss: 2.6107 - acc: 0.1111    
     4/20 [=====>........................] - ETA: 8s - loss: 2.6454 - acc: 0.0833 
     5/20 [======>.......................] - ETA: 6s - loss: 2.6483 - acc: 0.0667
     6/20 [========>.....................] - ETA: 5s - loss: 2.6863 - acc: 0.0556
     7/20 [=========>....................] - ETA: 4s - loss: 2.6230 - acc: 0.0952
     8/20 [===========>..................] - ETA: 3s - loss: 2.6212 - acc: 0.0833
     9/20 [============>.................] - ETA: 3s - loss: 2.6192 - acc: 0.1111
    10/20 [==============>...............] - ETA: 2s - loss: 2.6223 - acc: 0.1000
    11/20 [===============>..............] - ETA: 2s - loss: 2.6626 - acc: 0.0909
    12/20 [=================>............] - ETA: 2s - loss: 2.6562 - acc: 0.1111
    13/20 [==================>...........] - ETA: 1s - loss: 2.6436 - acc: 0.1282
    14/20 [====================>.........] - ETA: 1s - loss: 2.6319 - acc: 0.1190
    15/20 [=====================>........] - ETA: 1s - loss: 2.6343 - acc: 0.1111
    Warning (from warnings module):
      File "C:\Users\Moondra\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\callbacks.py", line 116
        % delta_t_median)
    UserWarning: Method on_batch_end() is slow compared to the batch update (0.102000). Check your callbacks.
    16/20 [=======================>......] - ETA: 0s - loss: 2.6310 - acc: 0.1042
    17/20 [========================>.....] - ETA: 0s - loss: 2.6207 - acc: 0.1176
    18/20 [==========================>...] - ETA: 0s - loss: 2.6063 - acc: 0.1296
    19/20 [===========================>..] - ETA: 0s - loss: 2.6056 - acc: 0.1228
    It just hangs at the 19/20.
    I already asked on stack overflow but no help.
    https://stackoverflow.com/questions/47382952/cant-get-past-first-epoch-just-hangs-keras-transfer-learning-inception