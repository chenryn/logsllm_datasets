::: para
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
**自选软件包：**
::: para
这些软件包位于自选频道中。从自选频道安装这些软件包前，请查看
[覆盖范围详情](https://access.redhat.com/site/support/offerings/production/scope_moredetail)。有关订阅自选频道的信息，请查看
Red Hat
知识库解决方案，[如何访问自选及辅助频道](https://access.redhat.com/site/solutions/392003)。
:::
-   ::: para
    `dapl、dapl-devel 及 dapl-utils`{.option} --- 为 RDMA 提供不同于
    Verbs API 的 API。这些软件包中均包含运行时组件及开发组件。
    :::
-   ::: para
    `openmpi、mvapich2 及 mvapich2-psm`{.option} --- 可使用 RDMA 通讯的
    MPI 栈。写入这些栈的用户空间应用程序不一定会知道发生的 RDMA 通讯。
    :::
:::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Configuring_the_Base_RDMA_Subsystem}9.3. 配置基础 RDMA 子系统 {.title}
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-The_RDMA_Package_Installation}9.3.1. RDMA 软件包安装 {.title}
:::
::: para
The [rdma]{.package} package is not part of the default install package
set. If the InfiniBand package group was not selected during install,
the [rdma]{.package} package (as well as a number of others as listed in
the previous section) can be installed after the initial installation is
complete. If it was not installed at machine installation time and
instead was installed manually later, then it is necessary to rebuild
the `initramfs`{.systemitem} images using [**dracut**]{.application} in
order for it to function fully as intended. Issue the following commands
as `root`{.systemitem}:
``` screen
~]# yum install rdma
dracut -f
```
:::
::: para
Startup of the `rdma`{.systemitem} service is automatic. When RDMA
capable hardware, whether InfiniBand or iWARP or RoCE/IBoE is detected,
[**udev**]{.application} instructs `systemd`{.systemitem} to start the
`rdma`{.systemitem} service. Users need not enable the
`rdma`{.systemitem} service, but they can if they want to force it on
all the time. To do that, issue the following command:
``` screen
~]# systemctl enable rdma
```
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Configuration_of_the_rdma.conf_file}9.3.2. rdma.conf file 文件配置 {.title}
:::
::: para
The `rdma`{.systemitem} service reads `/etc/rdma/rdma.conf`{.filename}
to find out which kernel-level and user-level RDMA protocols the
administrator wants to be loaded by default. Users should edit this file
to turn various drivers on or off.
:::
::: para
可启用和禁用的各个驱动程序为：
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    `IPoIB`{.option} --- 这是 `IP`{.systemitem} 网络模拟层，以便
    `IP`{.systemitem} 应用程序在 InfiniBand 网络 中运行。
    :::
-   ::: para
    `SRP`{.option} --- This is the SCSI Request Protocol. It allows a
    machine to mount a remote drive or drive array that is exported via
    the `SRP`{.systemitem} protocol on the machine as though it were a
    local hard disk.
    :::
-   ::: para
    `SRPT`{.option} --- This is the target mode, or server mode, of the
    `SRP`{.systemitem} protocol. This loads the kernel support necessary
    for exporting a drive or drive array for other machines to mount as
    though it were local on their machine. Further configuration of the
    target mode support is required before any devices will actually be
    exported. See the documentation in the [targetd]{.package} and
    [targetcli]{.package} packages for further information.
    :::
-   ::: para
    `ISER`{.option} --- 这个是用于 Linux 内核常规 iSCSI
    层的底层驱动程序，可通过 InfiniBand 网络为 iSCSI 设备提供传输。
    :::
-   ::: para
    `RDS`{.option} --- This is the Reliable Datagram Service in the
    Linux kernel. It is not enabled in Red Hat Enterprise Linux 7
    kernels and so cannot be loaded.
    :::
:::
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Usage_of_70-persistent-ipoib.rules}9.3.3. 70-persistent-ipoib.rules 用法 {.title}
:::
::: para
[rdma]{.package} 软件包提供
`/etc/udev.d/rules.d/70-persistent-ipoib.rules`{.filename} 文件。这个
[**udev**]{.application} 规则文件可用来修改 IPoIB 设备的默认名称（比如
`ib0`{.literal} 和
`ib1`{.literal}），以便提供更有描述性的名称。用户必须编辑此文件以便确定如何命名其文件。首先，请找到要重命名文件的
GUID 地址：
``` screen
~]$ ip link show ib0
8: ib0: >BROADCAST,MULTICAST,UP,LOWER_UP
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Relaxing_memlock_restrictions_for_users}9.3.4. 为用户解除 memlock 限制 {.title}
:::
::: para
RDMA
通讯需要所要连接计算机中的物理内存（就是说不允许该内核在计算机启动运行的可用内存短缺时将该内存交换到要页面文件）。锁定内存一般是非常特殊的操作。要让
`root`{.systemitem} 以外的用户运行大 RDMA 程序，则需要增大非
`root`{.systemitem} 用户在系统中可锁定的内存。方法是在
`/etc/security/limits.d/`{.filename} 目录中添加有以下内容的文件：
``` screen
~]$ more /etc/security/limits.d/rdma.conf
# configuration for rdma tuning
*       soft    memlock         unlimited
*       hard    memlock         unlimited
# rdma tuning end
```
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Configuring_Mellanox_cards_for_Ethernet_operation}9.3.5. 为以太网操作配置 Mellanox 卡 {.title}
:::
::: para
Mellanox 中的某些硬件既可在 InfiniBand
模式中运行，也可在以太网模式中运行。这些网卡通常默认是
InfiniBand。用户可将这些网卡设定为以太网模式。目前只支持在 ConnectX
产品线硬件（使用 [**mlx4**]{.application}
驱动程序）中设定模式。要设定此模式。用户应按照
`/etc/rdma/mlx4.conf`{.filename} 中的说明为其给定的硬件找到正确的 PCI
设备 ID。然后使用该设备 ID
及要求使用的端口类型中该文件中生成一行内容。然后应重新构建其
`initramfs`{.systemitem}，以便确定将更新的端口设置复制到
`initramfs`{.systemitem}。
:::
::: para
端口类型设定完毕后，如果一个或两个端口均设定为
Ethernet，那么用户会在其日志中看到这样的信息：`mlx4_core 0000:05:00.0: Requested port type for port 1 is not supported on this HCA`{.computeroutput}。这很正常，不会影响操作。负责设定端口类型的脚本不可能知道该驱动程序何时会完成内部从端口
2 到所需类型，而且从该脚本发出请求切换端口 2 到此切换完成前，尝试将端口
1
设定为不同的类型的请求都会被拒绝。该脚本会不断重试直到该命令成功，或者直到其超过超时值，后者表示该端口切换一直没有完成。
:::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Configuring_the_Subnet_Manager}9.4. 配置子网管理器 {.title}
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Determining_Necessity}9.4.1. 确定必要性 {.title}
:::
::: para
Most InfiniBand switches come with an embedded subnet manager. However,
if a more up to date subnet manager is required than the one in the
switch firmware, or if more complete control than the switch manager
allows is required, Red Hat Enterprise Linux 7 includes the
[**opensm**]{.application} subnet manager. All InfiniBand networks
[**must**]{.bold .bold} have a subnet manager running for the network to
function. This is true even when doing a simple network of two machines
with no switch and the cards are plugged in back to back, a subnet
manager is required for the link on the cards to come up. It is possible
to have more than one, in which case one will act as master, and any
other subnet managers will act as slaves that will take over should the
master subnet manager fail.
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Configuring_the_opensm_master_configuration_file}9.4.2. 配置 opensm 主配置文件 {.title}
:::
::: para
The [**opensm**]{.application} program keeps its master configuration
file in `/etc/rdma/opensm.conf`{.filename}. Users may edit this file at
any time and edits will be kept on upgrade. There is extensive
documentation of the options in the file itself. However, for the two
most common edits needed, setting the GUID to bind to and the PRIORITY
to run with, it is highly recommended that the `opensm.conf`{.filename}
file is not edited but instead edit `/etc/sysconfig/opensm`{.filename}.
If there are no edits to the base `/etc/rdma/opensm.conf`{.filename}
file, it will get upgraded whenever the [opensm]{.package} package is
upgraded. As new options are added to this file regularly, this makes it
easier to keep the current configuration up to date. If the
`opensm.conf`{.filename} file has been changed, then on upgrade, it
might be necessary to merge new options into the edited file.
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Configuring_the_opensm_startup_options}9.4.3. 配置 opensm 启动选项 {.title}
:::
::: para
`/etc/sysconfig/opensm`{.filename}
文件中的选项控制子网管理器的实际启动方式，以及启动的子网管理器副本数量。例如：在双端口
InfiniBand
卡中，会将每个端口与独立的物理网络相连，就是说需要在每个端口中都有一个运行的子网管理器副本。[**opensm**]{.application}
子网管理器只会在管理应用程序实例的一个子网，且必须为每个需要管理的子网启动一次。另外，如果有一个以上的
[**opensm**]{.application}
服务器，则需要为每台服务器设置优先顺序，以决定哪些是从属服务器，哪些是主服务器。
:::
::: para
`/etc/sysconfig/opensm`{.filename}
文件是用来设定子网管理器优先顺序并控制子网管理器所绑定 GUID
的简单工具。`/etc/sysconfig/opensm`{.filename}
文件本身包括其选项的详尽说明。用户只需要阅读该文件并按照其说明操作即可启用
[**opensm**]{.application} 的故障转移及多结构操作功能。
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Creating_a_P_Key_definition}9.4.4. 创建 P_Key 定义 {.title}
:::
::: para
默认情况下，`opensm.conf`{.filename} 会寻找文件
`/etc/rdma/partitions.conf`{.filename}
以获取要在其中创建结构的分区列表。所有结构必须包含 `0x7fff`{.literal}
子网，且所有交换机及所有主机都必须属于那个结构。可在该结构外另行创建其他分区，且所有主机及交换机不一定是那些附加分区的成员。这样就可以让管理员在
InfiniBand 结构中创建类似以太网 VLAN 的子网。如果使用给定速率（比如
40Gbps）定义某个分区，而在该网络中有一台主机无法达到
40Gbps，则那台主机即使有授权也无法加入该分区，因为它无法满足速度要求，因此建议将分区速率设定为所有有权限加入该分区主机中的最慢速度主机。如果某些主机子网需要较快的速度，则可使用较高的速度创建不同分区。
:::
::: para
以下分区文件会得到默认 `0x7fff`{.literal} 分区（速度降低 10
Gbps）以及速度为 40 Gbps 的分区 `0x0002`{.literal}：
``` screen
~]$ more /etc/rdma/partitions.conf
# For reference:
# IPv4 IANA reserved multicast addresses:
#   http://www.iana.org/assignments/multicast-addresses/multicast-addresses.txt
# IPv6 IANA reserved multicast addresses:
#   http://www.iana.org/assignments/ipv6-multicast-addresses/ipv6-multicast-addresses.xml
#
# mtu =
#   1 = 256
#   2 = 512
#   3 = 1024
#   4 = 2048
#   5 = 4096
#
# rate =
#   2  =   2.5 GBit/s
#   3  =  10   GBit/s
#   4  =  30   GBit/s
#   5  =   5   GBit/s
#   6  =  20   GBit/s
#   7  =  40   GBit/s
#   8  =  60   GBit/s
#   9  =  80   GBit/s
#   10 = 120   GBit/s
Default=0x7fff, rate=3, mtu=4, scope=2, defmember=full: