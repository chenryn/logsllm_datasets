2.0
Bandwidth (Mbits/sec)
2.5
Figure 6: Heterogeneous client bandwidth experiments with 50 LAN
clients, all good. The fraction of the server (c = 10 requests/s) allocated
to the ten clients in category i, with bandwidth 0.5 · i Mbits/s, is close
to the ideal proportional allocation.
For the byte cost, we measure the number of bytes uploaded for
served requests—the “price”—as recorded by the thinner. Figure 5
shows the average of this measurement for good and bad clients
and also plots the theoretical average price, (G + B)/c, from §3.3,
which is labeled “Upper Bound”. The actual price is lower than this
theoretical one because the clients do not consume all of their band-
width, for reasons that we now describe. We consider the different
values of c in turn.
For c = 50, each good client spends an average of 1.25 Mbits/s
(determined by tallying the total bits spent by good clients over the
experiment). This average is less than the 2 Mbits/s access link be-
cause of a quiescent period between when a good client ﬁrst issues
a request and when the thinner replies, asking for payment. This pe-
riod is roughly 0.35 seconds, the length owing to a long backlog at
the thinner of requests and payment bytes. When not in a quiescent
period, a good client consumes most of its access link, delivering
1.8 Mbits/s on average, inferred by dividing the average good client
payment (Figure 5) by the average time spent paying (Figure 4).
Bad clients, in contrast, keep multiple requests outstanding so do
not have “down time”. For c = 50, their payments are 1.7 Mbits/s
on average. They actually deliver slightly more than this number
but occasionally “waste” bytes. This wastage happens when a bad
client establishes a payment channel but—because its outbound
bandwidth is nearly fully utilized—fails to deliver the accompa-
nying request. Meanwhile, the thinner accepts payment for 10 sec-
onds, at which point it times out the payment channel.
The c = 100 case is similar to c = 50, except bad clients see a
higher price than good ones. The reason is as follows. Bad clients
waste bytes, as just described. In this case, however, some of the
requests actually arrive before the 10 seconds have elapsed—but
long after the client has paid enough to win the auction. In those in-
stances, bad clients overpay hugely, increasing their average price.
For c = 200, clients do not have to pay much because the server
is lightly loaded. In fact, good and bad clients often encounter a
price of zero, though bad clients again overpay sometimes.
7.4 Empirical Adversarial Advantage
As just discussed, bad clients are able to deliver more bytes than
good clients in our experiments. As a result of this disparity, the
server does not achieve the ideal of a bandwidth-proportional allo-
cation. This effect was visible in §7.2.
To better understand this adversarial advantage, we ask, What is
the minimum value of c at which all of the good demand is satis-
ﬁed? To answer this question, we experimented with the same con-
d
e
t
a
c
o
l
l
a
r
e
v
r
e
s
f
o
n
o
i
t
c
a
r
F
 0.3
 0.2
 0.1
All-good expt
All-bad expt
Ideal for both expts
 0
 0
 100
 200
 300
RTT (ms)
 400
 500
Figure 7: Two sets of heterogeneous client RTT experiments with 50
LAN clients, all good or all bad. The fraction of the server (c = 10 re-
quests/s) captured by the 10 clients in category i, with RTT 100 · i ms,
varies for good clients. In contrast, bad clients’ RTTs don’t matter be-
cause they open multiple connections.
ﬁguration as above (G = B = 50 Mbits/s; 50 clients) but for more
values of c. We found that all of the good demand is satisﬁed at
c = 115, which is only 15% more provisioning than cid, the capac-
ity needed under exact proportional allocation. We conclude that
a bad client can cheat the proportional allocation mechanism but
only to a limited extent—at least under our model of bad behavior.
We now revisit that model. We chose w = 20 to be conservative:
for other values of w between 1 and 60 (again, B = G, c = 100),
the bad clients capture less of the server. (We hypothesize that for
w > 20, the damage from wasted bytes exceeds the beneﬁt from
no quiescence.) However, the qualitative model does have weak-
nesses. For example, our bad clients sometimes overpay (as dis-
cussed in §7.3), and a truly pessimal bad client would not. Never-
theless, the analysis in §3.4 shows that bad clients cannot do much
better than the na¨ıve behavior that we model.
7.5 Heterogeneous Network Conditions
We now investigate the server’s allocation for different client band-
widths and RTTs. We begin with bandwidth. We assign 50 clients
to 5 categories. The 10 clients in category i (1 ≤ i ≤ 5) have band-
width 0.5 · i Mbits/s and are connected to the thinner over a LAN.
All clients are good. The server has capacity c = 10 requests/s. Fig-
ure 6 shows that the resulting server allocation to each category is
close to the bandwidth-proportional ideal.
We now consider RTT, hypothesizing that the RTT between a
good client and the thinner will affect the allocation, for two rea-
sons. First, at low prices, TCP’s ramp-up means that clients with
longer RTTs will take longer to pay. Second, and more importantly,
each request has at least one associated quiescent period (see §7.1
and §7.3), the length of which depends on RTT. In contrast, bad
clients have multiple requests outstanding so do not have “down
time” and will not be much affected by their RTT to the thinner.
To test this hypothesis, we assign 50 clients to 5 categories. The
10 clients in category i (1 ≤ i ≤ 5) have RTT = 100 · i ms to the
thinner, giving a wide range of RTTs. All clients have bandwidth 2
Mbits/s, and c = 10 requests/s. We experiment with two cases: all
clients good and all bad. Figure 7 conﬁrms our hypothesis: good
clients with longer RTTs get a smaller share of the server while for
bad clients, RTT matters little. This result may seem unfortunate,
but the effect is limited: for example, in this experiment, no good
client gets more than double or less than half the ideal.
Actual fraction of ‘bottleneck service’ to good
Actual fraction of ‘bottleneck service’ to bad
Ideal fraction of ‘bottleneck service’ to good
Ideal fraction of ‘bottleneck service’ to bad
Ideal fraction served: bottlenecked good
Actual fraction served: bottlenecked good
n
o
i
t
c
a
r
F
 1
 0.8
 0.6
 0.4
 0.2
 0
5 good, 25 bad
25 good, 5 bad
Number of clients behind shared bottleneck
15 good,15 bad
Figure 8: Server allocation when good and bad clients share a bottle-
neck link, l. “Bottleneck service” refers to the portion of the server cap-
tured by all of the clients behind l. The actual breakdown of this portion
(left bar) is worse for the good clients than the bandwidth-proportional
allocation (middle bar) because bad clients “hog” l. The right bar fur-
ther quantiﬁes this effect.
7.6 Good and Bad Clients Sharing a Bottleneck
When good clients share a bottleneck link with bad ones, good re-
quests can be “crowded out” by bad ones before reaching the thin-
ner (see §4.2). We quantify this observation with an experiment that
uses the following topology: 30 clients, each with a bandwidth of
2 Mbits/s, connect to the thinner through a common link, l. The
bandwidth of l is 40 Mbits/s. l is a bottleneck because the clients
behind l can generate 60 Mbits/s. Also, 10 good and 10 bad clients,
each with a bandwidth of 2 Mbits/s, connect to the thinner directly
through a LAN. The server’s capacity is c = 50 requests/s. We vary
the number of good and bad clients behind l.
In all cases, the clients behind l together capture half of the
server’s capacity (as expected, given the topology). We measure
how this “server half” is allocated to the good and bad clients be-
hind l. We also measure, of the good requests that originate behind
l, what fraction receive service. Figure 8 depicts these measure-
ments and compares them to the bandwidth-proportional ideals.2
The effect on good clients, visible in the ﬁgure, will likely be more
pronounced when the bottleneck’s bandwidth is a smaller fraction
of the combined bandwidth behind it.
Impact of Speak-up on Other Trafﬁc
7.7
We now consider how speak-up affects other trafﬁc, speciﬁcally
what happens when a TCP endpoint, H, shares a bottleneck link,
m, with clients that are currently uploading dummy bytes. The case
when H is a TCP sender is straightforward: m will be shared among
H’s transfer and the speak-up uploads. When H is a TCP receiver,
the extra trafﬁc from speak-up affects H in two ways. First, ACKs
from H will be lost (and delayed) more often than without speak-
up. Second, for request-response protocols (e.g., HTTP), H’s re-
quest can be delayed. Here, we investigate these effects on HTTP
downloads.
We experiment with the following setup: 10 good speak-up
clients share a bottleneck link, m, with H, a host that runs the HTTP
client wget. m has a bandwidth of 1 Mbit/s and one-way delay
100 ms. Each of the 11 clients has a bandwidth of 2 Mbits/s. On
the other side of m are the thinner (fronting a server with c = 2
2For the ﬁrst measurement, the ideal is simply the fraction of good
and bad clients behind l. For the second measurement, the ideal pre-
sumes that the non-bottlenecked clients each have 2 Mbits/s of band-
width and that the clients behind l have 2( 40
60 ) Mbits/s.
)
s
d
n
o
c
e
s
(
y
c
n
e
t
a
l
d
n
e
-
o
t
-
d
n
E
 4
 3
 2
 1
 0
Without speak-up
With speak-up
 1
 10
 100
Size of HTTP Transfer (KBytes)
Figure 9: Effect on an HTTP client of sharing a bottleneck link with
speak-up clients. Graph shows means and standard deviations of end-
to-end HTTP download latency with and without speak-up running, for
various HTTP transfer sizes (which are shown on a log scale).
requests/s) and a separate Web server, S . In each experiment, H
downloads a ﬁle from S 100 times.
Figure 9 shows the means and standard deviations of the down-
load latency for various ﬁle sizes, with and without the speak-up
trafﬁc. There is signiﬁcant “collateral damage” to “innocently by-
standing” Web transfers here: download times inﬂate by almost
6× for a 1 Kbyte (single packet) transfer and by almost 4.5× for
64 Kbyte transfers. However, this experiment is quite pessimistic:
the RTTs are large, the bottleneck bandwidth is highly restrictive
(roughly 20× smaller than the demand), and the server capacity is
low. While speak-up is clearly the exacerbating factor in this exper-
iment, speak-up will not have this effect on every link.
8 RELATED WORK
In this section, we ﬁrst survey related work in the context of com-
paring speak-up to other defenses against application-level DDoS
attacks. (For other attacks and defenses, see the survey by Mirkovic
and Reiher [28] and the bibliographies in [21,29,51].) We then dis-
cuss how and when to combine speak-up with other defenses.
8.1 Comparisons to Related Work
Using the taxonomy in §1 (massive over-provisioning, detect and
block, currency), speak-up is a currency scheme. The currency con-
cept was pioneered by Dwork and Naor [12] in the context of spam
defense. Others have done work in the same spirit [1,6,7,11,20,25,
49]; these approaches are often called proof-of-work schemes.
We ﬁrst proposed bandwidth as a currency in a workshop pa-
per [48]. In contrast to [48], this paper gives a viable mechanism
and an implementation, evaluation, and analysis of that mechanism;
presents a solution to the “unequal requests” case; and considers
context and alternate DDoS defenses much more completely.
We do not know of another proposal to use bandwidth as a cur-
rency. However, the authors of [17, 39] describe a solution to DoS