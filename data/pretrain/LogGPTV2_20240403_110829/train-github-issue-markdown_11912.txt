Looks like ONNX support does not support dataparallel models (in particular,
model that is converted after nn.DataParallel(model).
Here is a simple piece of code to repro the issue:
    import os
    import sys
    import numpy as np
    import pandas as pd
    import torch
    import torchvision.models as models
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    import torch.nn.init as init
    import time
    from torch.optim.lr_scheduler import ReduceLROnPlateau
    from torch.autograd import Variable
    import torchvision.transforms as transforms
    from torch.utils.data import DataLoader, Dataset
    from sklearn.metrics.ranking import roc_auc_score
    from sklearn.model_selection import train_test_split
    from PIL import Image
    import multiprocessing
    model = models.densenet.densenet121(pretrained=True)
    model = nn.DataParallel(model)
    dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()
    model = model.cuda()
    torch.onnx.export(model, dummy_input, "test.onnx", verbose=False, export_params=True)
The error message is:
    ---------------------------------------------------------------------------
    RuntimeError                              Traceback (most recent call last)
     in ()
        386 dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()
        387 model = model.cuda()
    --> 388 torch.onnx.export(model, dummy_input, "test.onnx", verbose=False, export_params=True)
        389 
        390 #model.load_state_dict(chkpt['state_dict'])
    ~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training)
         73             only, so you will generally not need to set this to True.
         74     """
    ---> 75     _export(model, args, f, export_params, verbose, training)
         76 
         77 
    ~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in _export(model, args, f, export_params, verbose, training)
        120                            "something weird is happening in your model!")
        121 
    --> 122     _optimize_trace(trace)
        123     if verbose:
        124         print(trace)
    ~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in _optimize_trace(trace)
         79     torch._C._jit_pass_peephole(trace)
         80     torch._C._jit_pass_lint(trace)
    ---> 81     torch._C._jit_pass_onnx(trace)
         82     torch._C._jit_pass_lint(trace)
         83     torch._C._jit_pass_onnx_peephole(trace)
    RuntimeError: untraced buffer
However if we remove the `model = nn.DataParallel(model)` line, i.e. change
the code to the following:
    model = models.densenet.densenet121(pretrained=True)
    dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()
    model = model.cuda()
    torch.onnx.export(model, dummy_input, "test.onnx", verbose=False, export_params=True)
It at least exports the model.
I hope either we fix this bug (i.e. also support the model that is converted
using nn.DataParallel(model), or provide an explicit error message saying
multiple GPU model is not supported.
BTW - for those who still want to export such a model, here is the workaround:  
https://stackoverflow.com/questions/44230907/keyerror-unexpected-key-module-
encoder-embedding-weight-in-state-dict  
i.e. still loading the single GPU model and when loading the weights, change
the dict name to exclude the string "module".
When submitting a bug report, please include the following information (where
relevant):
  * OS: Linux
  * PyTorch version: 0.3.1
  * How you installed PyTorch (conda, pip, source): pip
  * Python version: 3.5
  * CUDA/cuDNN version: 8.0/cuDNN 6
  * GPU models and configuration: Tesla K80
  * GCC version (if compiling from source): N/A