### Content Serving and Performance Expectations

We expect high performance from Netflix's setup, as it can leverage the in-kernel TLS implementation with `sendfile` (§ 2.1), unlike stock FreeBSD. However, the semantics differ from plaintext cases. In-place encryption is not feasible as it would invalidate buffer cache entries, necessitating out-of-place encryption, which increases memory and Last-Level Cache (LLC) footprints.

To avoid CPU saturation on our client systems, we emulate the TLS overhead by encrypting and authenticating data with dummy keys before transmission. The HTTP headers remain in plaintext, allowing the client to parse the HTTP response and count received data without additional decryption. We believe this setup closely approximates the actual TLS protocol overheads, especially given that the initial TLS handshake's overhead is negligible for video streaming.

### Memory Access Patterns and Encryption Overheads

For Atlas, we used the internal OpenSSL GCM API, which leverages ISA extensions like AESNI and PCLMUL to accelerate AES-128-GCM. To ensure a fair comparison, we modified the Netflix stack to allow plaintext transmission of HTTP headers while encrypting the data. The Netflix implementation supports different encryption backends, including offloading to PCIe hardware. Our experiments include results using Intel’s ISA-L library, which uses ISA extensions and non-temporal instructions to reduce LLC pressure.

Figures 13a and 13b show network throughput and CPU utilization for encrypted traffic with zero and 100% buffer cache hit ratios. With over 4,000 connections, Atlas achieves higher throughput than Netflix, ~72Gb/s compared to ~68Gb/s. For uncacheable workloads, Atlas achieves 50% more throughput using only four cores, while Netflix saturates all CPU cores, reducing available cycles for encryption and network processing.

With fewer than 2,000 active connections, Atlas shows slightly suboptimal throughput due to similar reasons as with plaintext. As active connections increase, both systems exhibit a small performance degradation, expected when a resource (CPU) is saturated. Increased requests build deeper queues and put more pressure on memory, but the reduction is minimal, and both systems handle overload gracefully.

### Memory Throughput Analysis

Memory throughput measurements reveal significant differences between the two systems. Atlas reaches ~110Gb/s read throughput, a ~43% increase from the plaintext case. Netflix, however, requires ~175Gb/s for cached workloads and ~127Gb/s for uncacheable workloads with over 4,000 concurrent connections. Despite the uncacheable workload triggering more memory traffic, the ratio of memory read throughput to network throughput remains at 2.6. Atlas consistently outperforms Netflix in memory traffic efficiency, requiring 1.5× the network throughput compared to 2.6× for Netflix.

The Atlas memory read results indicate that retaining all data in the LLC for the entire TX pipeline is challenging, though possible for 2,000 concurrent connections. The increased memory write throughput in Atlas is likely due to dirty cache line evictions of encrypted data after NIC DMA. Under heavy load, some data is evicted to main memory and must be re-read, either by the CPU or the NIC during DMA.

### New Design Principles

We developed diskmap and the Atlas stack to explore the boundaries of performance through software specialization and microarchitectural awareness. The resulting prototype shows significant improvements over conventional designs. Many design principles are reusable and can be applied within current network and storage stacks.

#### Storage Stack Design

New non-volatile storage technologies reduce the need for DRAM-based buffer caches. On-demand data retrieval, even without DRAM presence, is now feasible and efficient. Optimizing LLC use by DMA is crucial to avoid memory bandwidth bottlenecks. If the aggregate bandwidth-delay product fits within the LLC, DRAM accesses can be minimized. This requires careful latency management across I/O and compute paths, discouraging deferred processing.

#### Latency Minimization

Integrating control loops to minimize latency is essential. Unbounded latency due to thread handoffs or large queues between protocol-stack layers is unacceptable, as it increases effective latency and limits the work fitting into the LLC. Userspace I/O frameworks, like netmap, facilitate latency minimization by providing fine-grained notifications and minimizing in-kernel work loops.

#### Zero-Copy and Data Movement

Zero-copy operation has long been a goal in network stacks, but attention must also be paid to implied data movement in the hardware. Tools like hardware performance counters are less effective as data copying and cache interactions move further from the processor pipeline. Atlas successfully reduces DRAM use in favor of on-package cache and fast flash, avoiding unnecessary volatile memory loading.

### Related Work

Previous research has focused on optimizing system call overheads and redundant data copies. IO-Lite, FlexSC, and Megapipe have shown significant performance improvements by unifying data management, system call batching, and bidirectional per-core pipes. Netmap and DPDK provide high-throughput network I/O by exposing DMA memory to userspace and using kernel-bypass techniques.

Microkernel designs like Mach and Exokernel reduce shared subsystems, enabling low-level hardware access. User-level network stacks, such as mTCP and IX, demonstrate dramatic throughput and latency improvements over conventional kernel stacks.

### Conclusions

Atlas, a high-performance video streaming stack, outperforms conventional and state-of-the-art implementations by leveraging OS-bypass. Traditional server designs with buffer caches suffer under typical video streaming workloads. Atlas directly includes storage in the network fast path, achieving tighter control over the complete I/O pipeline and making more efficient use of memory and CPU cycles.

### Acknowledgements

We thank Drew Gallatin from Netflix for his comments and assistance, Navdeep Parhaar from Chelsio for arranging 40GbE NICs, and Jim Harris from Intel’s Storage Division, Serafeim Mellos, our anonymous reviewers, and Keith Winstein for their insightful comments. This work was supported by a Google PhD Fellowship and a NetApp Faculty Fellowship.

### References

[1] M. Accetta, R. Baron, D. Golub, R. Rashid, A. Tevanian, and M. Young. Mach: A New Kernel Foundation for UNIX Development. Technical report, Computer Science Department, Carnegie Mellon University, August 1986.

[2] Adobe HTTP Dynamic Streaming. http://wwwimages.adobe.com/content/dam/Adobe/en/devnet/hds/pdfs/adobe-hds-specification.pdf.

[3] N. Amit, M. Ben-Yehuda, D. Tsafrir, and A. Schuster. vIOMMU: Efficient IOMMU Emulation. In Proceedings of the 2011 USENIX Conference on USENIX Annual Technical Conference, USENIXATC’11, pages 6–6, Berkeley, CA, USA, 2011. USENIX Association.

[4] BBC Digital Media Distribution: How we improved throughput by 4x. http://www.bbc.co.uk/blogs/internet/entries/.

[5] A. Belay, G. Prekas, M. Primorac, A. Klimovic, S. Grossman, C. Kozyrakis, and E. Bugnion. The IX Operating System: Combining Low Latency, High Throughput, and Efficiency in a Protected Dataplane. ACM Trans. Comput. Syst., 34(4):11:1–11:39, Dec. 2016.

[6] B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. E. Fiuczynski, D. Becker, C. Chambers, and S. Eggers. Extensibility Safety and Performance in the SPIN Operating System. In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, SOSP ’95, pages 267–283, New York, NY, USA, 1995. ACM.

[7] S. Boyd-Wickizer, H. Chen, R. Chen, Y. Mao, F. Kaashoek, R. Morris, A. Pesterev, L. Stein, M. Wu, Y. Dai, Y. Zhang, and Z. Zhang. Corey: An Operating System for Many Cores. In Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation, OSDI’08, pages 43–57, Berkeley, CA, USA, 2008. USENIX Association.