of serving content. We expect high performance from Netflix’
setup, since unlike stock FreeBSD, it can still take advantage of
sendfile with the in-kernel TLS implementation (§ 2.1). The
semantics, however, are different from the plaintext case. In-place
encryption is not an option as it would invalidate the buffer cache en-
tries, so the stack needs to encrypt the data out-of-place, increasing
the memory and LLC footprint.
To avoid our tests being impacted by CPU saturation on our client
systems, rather than implementing a full TLS layer we have decided
to emulate the TLS overhead by doing encryption and authentication
of the data with dummy keys before it is actually transmitted. The
HTTP headers are still transmitted in plaintext, so that the client
software can parse the HTTP response and count the received data
without needing to spend additional CPU cycles decrypting data,
but the server encrypts everything else as normal. We believe that
this setup closely approximates the actual TLS protocol’s overheads,
especially given that the initial TLS handshake’s overhead will be
negligible for flow durations encountered with video streaming.
221
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Ilias Marinos, Robert N.M. Watson, Mark Handley, and Randall R. Stewart
(a) Delayed notifications incur extra memory
writes. No memory reads, but extra writes.
(b) Heavy load and netmap batching result in
LLC eviction. One extra memory read and write.
(c) Contention for DDIO portion of LLC evicts
DMA’ed data. Two extra reads and writes.
Figure 14: Principal sub-optimal Atlas memory access patterns for encrypted traffic.
For Atlas we used the internal OpenSSL GCM API that takes
advantage of ISA extensions. This uses AESNI instructions for en-
cryption and the PCLMUL instruction for ghash, so as to accelerate
AES 128bit in Galois Counter Mode [28] (AES128-GCM). For a fair
comparison, we modified the Netflix stack to implement a similar
design: in particular, we have modified the Netflix implementation to
allow plaintext transmission of HTTP headers, which are passed to
the kernel as a parameter to the sendfile syscall, while the data
are still encrypted. The Netflix implementation allows the use of
different backends for encryption including support for offloading en-
cryption to special PCIe hardware. Our experiments include results
with, according to Netflix, the most optimized software-only imple-
mentation: Intel’s ISA-L library, which not only uses ISA extensions
to accelerate crypto, but also utilizes non-temporal instructions to
reduce pressure on the CPU’s Last Level Cache.
Figures 13a and 13b show network throughput and CPU utiliza-
tion while serving encrypted traffic with a zero and 100% buffer
cache hit ratios. When serving more than 4,000 connections, Atlas
achieves higher throughput than Netflix when the buffer hit ratio is
100%, ~72Gb/s as opposed to ~68Gb/s peak throughput for Netflix.
When the workload is not cacheable Atlas, achieves 50% more
network throughput than Netflix, while only using four cores. Netflix
saturates all the CPU cores even when no disk activity is required, so
uncacheable traffic caused storage stack overhead to be introduced,
fewer CPU cycles are available for encryption and network protocol
processing, greatly reducing throughput.
With under 2,000 active connections, we again see slightly sub-
optimal Atlas throughput for the same reasons as with plaintext. As
active connections increase, all three curves demonstrate a small per-
formance degradation . This is to be expected when a resource—the
CPU in this case—is saturated. Increasing the number of requests
can only hinder performance by building deeper queues in stacks
and by putting more pressure on memory. However, the reduction is
small and both systems handle overload gracefully.
Measuring memory throughput while serving such workloads
reveals a big difference between the two systems (Figures 13c and
13d). Clearly encryption affects memory throughput: Atlas memory
read throughput reaches ~110Gb/s, roughly a ~43% increase com-
pared to the plaintext case. Netflix, however, requires ~175Gb/s of
read throughput when serving the cached workload. When serving
the uncachable workload it requires about ~127Gb/s for more than
4,000 concurrent connections. This might seem counter-intuitive
since the uncacheable workload should trigger more memory traffic
due to LLC/memory pressure, but if we look at Fig. 13e, the ratio
of memory read throughput to network throughput is actually un-
changed at 2.6. For the whole range of connections benchmarked,
Atlas remains more effective than Netflix in terms of memory traffic
efficiency, requiring 1.5× the network throughput as opposed to
2.6× for Netflix.
The Atlas memory read results indicate that it was not possible to
retain all the data in the LLC for the full duration of the TX pipeline,
from disk to encryption to NIC for most workloads, though it is
often possible for 2,000 concurent connections when the memory
access pattern in Fig. 14a dominates. We believe that the increased
memory write throughput observed for Atlas in Figure 13d is related
to dirty cache line evictions of encrypted data, which occur after
the NIC has finished DMAing the encrypted data out; this does not
affect performance. Under heavy load a fraction of the data was
evicted to main memory and has to be re-read, either by the CPU
while encrypting, or by the NIC during DMA for transmission, or
both. The pattern in Fig. 14b is primarily due to a small amount
of extra latency introduced by netmap batching, combined with
heavy pressure on the LLC. The extra eviction in Fig. 14c prior to
encryption is responsible for the LLC misses in Fig. 13f, and occurs
because to avoid DMA thrashing the LLC, only a fraction of the
LLC is available for DDIO. Once this is exhausted, new DMAs will
evict older DMA buffers if the stack is even slightly slow getting
round to encrypting them.
5 NEW DESIGN PRINCIPLES
We developed diskmap and the clean-slate Atlas stack to explore the
boundaries of achievable performance through a blend of software
specialization and microarchitectural awareness. The resulting pro-
totype exhibits significant performance improvements over conven-
tional designs. However, and perhaps counterintuitively, we believe
that many of the resulting design principles are reusable, and could
be applied within current network- and storage-stack designs.
Reduced latency and increased bandwidth for storage, aris-
ing out of new non-volatile storage technologies, fundamentally
change the dynamic in storage-stack design. Previously, substan-
tial investment in CPU to improve disk layout decisions and mask
spindle latency was justified, and the use of DRAM to prefetch and
cache on-disk contents offered significant improvements in both
222
DRAM	LLC	NIC	AES	TCP	CPU	stale	buﬀers	re-use	buﬀer	NVMe	DRAM	LLC	NIC	AES	TCP	CPU	TCP	Packets	re-use	buﬀer	NVMe	DRAM	LLC	NIC	AES	TCP	CPU	TCP	Packets	re-use	buﬀer	NVMe	Disk|Crypt|Net: rethinking the stack for high-performance video streaming
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
latency and bandwidth [25]. Now, the argument for an in-DRAM
buffer cache is dramatically reduced, as on-demand retrieval of data
(e.g., on receiving a TCP ACK opening the congestion window),
regardless of existing presence in DRAM, is not only feasible, but
may also be more efficient than buffered designs.
Optimizing Last-Level Cache (LLC) use by DMA must be
a key design goal to avoid being bottlenecked on memory band-
width. A key insight here is that if the aggregate bandwidth-delay
product across the I/O subsystem (e.g., from storage DMA receive
through to NIC DMA send) can fit within the LLC, DRAM accesses
can be largely eliminated. This requires careful bounding of latency
across the I/O and compute path, proportionally decreasing the prod-
uct, which discourages designs that defer processing – e.g., those
that might place inbound DMA from disk, encryption, and outbound
DMA to the NIC in different threads – an increasing design choice
made to better utilize multiple hardware threads. In the Netflix stack,
substantial effort is gone to mitigate cache misses, including use
of prefetch instructions and non-temporal loads and stores around
AES operations. Ironically, these mitigations may have the effect
of further increasing the degree to which higher latency causes the
bandwidth-delay product to exceed LLC size. This optimization goal
also places pressure on copying designs: copies from a buffer cache
to encrypted per-packet memory doubles the cache footprint, halving
the bandwidth-delay product that can be processed on a package.
Integrating control loops to minimize latency therefore also
becomes a key concern, as latency reduction requires a “process-to-
completion” across control loops in I/O and encryption. Allowing
unbounded latency due to handoffs between threads, or even in using
larger queues between protocol-stack layers, is unacceptable, as it
will increase effective latency, in turn increasing the bandwidth-delay
product, limiting the work that can fit into the LLC.
Userspace I/O frameworks also suffer from latency problems,
as they have typically been designed to maximize batching and
asynchrony in order to mitigate system-call expense. Unlike netmap,
diskmap facilitates latency minimization by allowing user code to
have fine-grained notification of memory being returned for reuse,
and by minimizing in-kernel work loops that otherwise increase LLC
utilization. This is critical to ensuring that “free” memory in the LLC
is reused, rather than unnecessarily spilling its unused contents from
the LLC to DRAM by allocating further memory.
Zero-copy is not just about reducing software memory copies.
While zero-copy operation has long been a goal in network-stack de-
signs, attention has primarily been paid to data movement performed
directly through the architecture – e.g., by avoiding unnecessary
memory copies as data is passed between user and kernel space,
or between kernel subsystems. It is clear from our research that, to
achieve peak performance, system programmers must also eliminate
or mitigate implied data movement in the hardware – with a special
focus on memory-subsystem and I/O behavior where data copying
in the microarchitecture or by DMA engines comes at extremely
high cost that must be carefully managed. This is made especially
challenging by the relative opacity of critical performance behav-
iors: as data copying and cache interactions move further from the
processor pipeline, tools such as hardware performance counters
become decreasingly able to attribute costs to the originating parties.
For example, no hardware that we had access to was able to at-
tribute cache-line allocation to specific DMA sources, which would
have allowed a more thorough analysis of NIC vs. NVMe cache
interactions.
Larger than DRAM-size workloads are important for two rea-
sons: a long tail of content used by large audiences (e.g., with respect
to video and software updates), and also because DRAM is an un-
economical form of storage due to high cost and energy use as com-
pared to flash memory. The Atlas design successfully deemphasizes
DRAM use in favor of on-package cache and fast flash, avoiding
loading content into volatile memory for longer than necessary.
Netflix has already begun to explore applying some of these de-
sign principles to their FreeBSD-based network stack. A key concern
to reduce memory bandwidth utilization has been to improve the ef-
ficiency of cache use, which has to date been accomplished through
careful use of prefetching and non-temporal operations. These in
fact prove harmful compared to a more optimal design such as Atlas
due to increasing the effective bandwidth-delay product. Reducing
cache inefficiency by eliminating the buffer cache is challenging in
the current software environment, especially when some key content
sees high levels of reuse. However, reducing latency between storage
DMA and encryption is plausible, by shifting data encryption close
to storage I/O completions, avoiding redundant detours to DRAM.
6 RELATED WORK
We briefly discuss previous work related to Atlas.
Conventional Stack Optimizations: System call overheads and
redundant data copies have been previously identified as a bottle-
neck of conventional OSes. Multiple past studies have focussed on
optimizing OS primitives to achieve better system performance. IO-
Lite [24] unifies data management between userspace applications
and kernel subsystems, utilizing page-based mechanisms to safely
share data. FlexSC [31] provides system call batching by allowing
userspace and kernel to share the system call pages, avoiding that
way CPU cache pollution. Megapipe [13] demonstrates significant
performance improvements by employing system call batching, and
introducing a bidirectional per-core pipe for data and event exchange
between kernel and userspace. Past research has shown that main-
taining flow affinity, and minimizing sharing is key to achieving
network IO scalability on multicore systems [7, 8, 26].
Userspace I/O frameworks: Netmap [29] implements high-
throughput network I/O, by exposing DMA memory to userspace
and relying on batching to substantially reduce context switch over-
head. Similarly, Intel’s DPDK [11] utilizes kernel-bypass to provide
user-level access to the network interface using hardware virtualiza-
tion. Intel’s Storage Performance Development Kit (SPDK) [32] is
a contemporary effort to diskmap, that implements a high through-
put and low latency NVMe storage I/O framework, by running the
NVMe device drivers fully in userspace. Unlike SPDK, diskmap
does not fully expose NVMe devices to userspace (e.g., device door-
bells, administrative queue pairs): the OS kernel is still mediating for
device administrative operations, and DMA operations are abstracted
with system calls for protection.
Microkernels and User-level Stacks: Microkernel designs such
as Mach [1], shift core services to userspace. The Exokernel [12] and
SPIN [6] reduce shared subsystems to enable userspace-accessible
low-level interfaces for hardware access. Recently, inspired by mi-
crokernel services, multiple user-level network stacks leveraged
223
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Ilias Marinos, Robert N.M. Watson, Mark Handley, and Randall R. Stewart
[11] Intel Data Plane Development Kit. http://dpdk.org.
[12] D. R. Engler, M. F. Kaashoek, and J. O’Toole, Jr. Exokernel: An Operating System
Architecture for Application-level Resource Management. In Proceedings of the
Fifteenth ACM Symposium on Operating Systems Principles, SOSP ’95, pages
251–266, New York, NY, USA, 1995. ACM.
[13] S. Han, S. Marshall, B.-G. Chun, and S. Ratnasamy. MegaPipe: a new program-
ming interface for scalable network I/O. In Proceedings of the 10th USENIX
conference on Operating Systems Design and Implementation, pages 135–148.
USENIX Association, 2012.
[14] HTTP Live Streaming.
https://tools.ietf.org/html/draft- pantos- http- live-
streaming-23.
[8] S. Boyd-Wickizer, A. T. Clements, Y. Mao, A. Pesterev, M. F. Kaashoek, R. Morris,
and N. Zeldovich. An Analysis of Linux Scalability to Many Cores. In Proceedings
of the 9th USENIX Conference on Operating Systems Design and Implementation,
OSDI’10, pages 1–16, Berkeley, CA, USA, 2010. USENIX Association.
[9] Chelsio 40GbE Netmap Performance. http://www.chelsio.com/wp- content/
uploads/resources/T5-40Gb-FreeBSD-Netmap.pdf.
[10] Intel Data Direct IO. http://www.intel.com/content/www/us/en/io/data-direct-i-o-
technology.html.
[15] M. Honda, F. Huici, C. Raiciu, J. Araujo, and L. Rizzo. Rekindling Network
Protocol Innovation with User-level Stacks. SIGCOMM Comput. Commun. Rev.,
44(2):52–58, Apr. 2014.
[16] E. Y. Jeong, S. Woo, M. Jamshed, H. Jeong, S. Ihm, D. Han, and K. Park. mTCP:
A Highly Scalable User-level TCP Stack for Multicore Systems. In Proceedings of
the 11th USENIX Conference on Networked Systems Design and Implementation,
NSDI’14, pages 489–502, Berkeley, CA, USA, 2014. USENIX Association.
[17] J. Lemon. Kqueue - A Generic and Scalable Event Notification Facility.
In
Proceedings of the FREENIX Track: 2001 USENIX Annual Technical Conference,
pages 141–153, Berkeley, CA, USA, 2001. USENIX Association.
[18] I. Marinos, R. N. Watson, and M. Handley. Network Stack Specialization for
Performance. In Proceedings of the 2014 ACM Conference on SIGCOMM, SIG-
COMM ’14, pages 175–186, New York, NY, USA, 2014. ACM.
[19] A. Markuze, A. Morrison, and D. Tsafrir. True IOMMU Protection from DMA
Attacks: When Copy is Faster Than Zero Copy. In Proceedings of the Twenty-First
International Conference on Architectural Support for Programming Languages
and Operating Systems, ASPLOS ’16, pages 249–262, New York, NY, USA, 2016.
ACM.
[20] Dynamic adaptive streaming over HTTP (DASH) — Part 1: Media presenta-
ISO/IEC 23009-1 (http:
tion description and segment formats, April 2012.
//standards.iso.org/ittf/PubliclyAvailableStandards).
[21] Netflix Appliance Software. https://openconnect.netflix.com/en/software/.
[22] NVM Express Specification 1.2.1. http://www.nvmexpress.org/specifications/.
[23] Intel P3608 NVME drive. http://www.intel.com/content/www/us/en/solid-state-
drives/solid-state-drives-dc-p3608-series.html.
[24] V. S. Pai, P. Druschel, and W. Zwaenepoel. IO-Lite: A unified I/O buffering and
caching system. Operating systems review, 33:15–28, 1998.
[25] R. H. Patterson, G. A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. Informed
Prefetching and Caching. In Proceedings of the Fifteenth ACM Symposium on
Operating Systems Principles, SOSP ’95, pages 79–95, New York, NY, USA,
1995. ACM.
[26] A. Pesterev, J. Strauss, N. Zeldovich, and R. T. Morris.
Improving Network
In Proceedings of the 7th ACM
Connection Locality on Multicore Systems.
European Conference on Computer Systems, EuroSys ’12, pages 337–350, New
York, NY, USA, 2012. ACM.
[27] S. Peter, J. Li, I. Zhang, D. R. K. Ports, D. Woos, A. Krishnamurthy, T. Anderson,
and T. Roscoe. Arrakis: The Operating System is the Control Plane. In Proceedings
of the 11th USENIX Conference on Operating Systems Design and Implementation,
OSDI’14, pages 1–16, Berkeley, CA, USA, 2014. USENIX Association.
[28] RFC5288: AES Galois Counter Mode (GCM) Cipher Suites for TLS.
[29] L. Rizzo. Netmap: A Novel Framework for Fast Packet I/O. In Proceedings of the
2012 USENIX Conference on Annual Technical Conference, USENIX ATC’12,
pages 9–9, Berkeley, CA, USA, 2012. USENIX Association.
[30] Sandvine 2015 Global Internet Phenomena Report. https://www.sandvine.com/
downloads / general / global - internet - phenomena / 2015 / global - internet -
phenomena-report-latin-america-and-north-america.pdf.
[31] L. Soares and M. Stumm. FlexSC: Flexible System Call Scheduling with
Exception-less System Calls. In Proceedings of the 9th USENIX Conference
on Operating Systems Design and Implementation, OSDI’10, pages 33–46, Berke-
ley, CA, USA, 2010. USENIX Association.
[32] Intel Storage Performance Development Kit. http://www.spdk.io.
[33] R. Stewart, J.-M. Gurney, and S. Long. Optimizing TLS for high–bandwidth
applications in FreeBSD. In Proc. Asia BSD conference, 2015.
OS-bypass to demonstrate dramatic throughput and latency improve-
ments over the conventional kernel stacks [15, 16, 18].
Dataplane and Research OSes: Arrakis [27] is a research op-
erating system that leverages hardware virtualization to efficiently
decouple the control and data planes. Diskmap was partially inspired
by Arrakis, which first applied the idea of fast and safe user-level
storage data plane. Similarly, IX [5] uses hardware virtualization to
enforce safety, while utilizing zerocopy APIs and adaptive batching
to achieve high performance network IO.
7 CONCLUSIONS
In this paper we presented Atlas, a high-performance video stream-
ing stack which leverages OS-bypass and outperforms conventional
and state-of-the-art implementations. Through measurement of the
Netflix stack, we show how traditional server designs that feature a
buffer cache to hide I/O latency suffer when serving typical video
streaming workloads. Based on these insights, we show how to build
a stack that directly includes storage in the network fast path.
Finally, we discuss the highly asynchronous nature of the con-
ventional stack’s components, and how it contributes to lengthening
I/O datapaths, while wasting opportunities for exploiting microar-
chitectural properties. We show how, using a specialized design, it
is possible to achieve tighter control over the complete I/O pipeline
from the disk up to network hardware, achieving high throughput
and making more efficient use of memory and CPU cycles on con-
temporary microarchitectures.
8 ACKNOWLEDGEMENTS
We thank Drew Gallatin from Netflix for his comments and invalu-
able assistance with the Netflix stack and workloads. Additionally,
we gratefully acknowledge Navdeep Parhaar from Chelsio for arrang-
ing 40GbE NICs for us, and assisting us with TSO support for the
netmap driver. Finally, we would also like to thank Jim Harris from
Intel’s Storage Division, Serafeim Mellos, our anonymous reviewers,
and our shepherd Keith Winstein for their insightful comments.
This work was supported by a Google PhD Fellowship, and a
NetApp Faculty Fellowship.
REFERENCES
[1] M. Accetta, R. Baron, D. Golub, R. Rashid, A. Tevanian, and M. Young. Mach:
A New Kernel Foundation for UNIX Development. Technical report, Computer
Science Department, Carnegie Mellon University, August 1986.
[2] Adobe HTTP Dynamic Streaming. http://wwwimages.adobe.com/content/dam/
Adobe/en/devnet/hds/pdfs/adobe-hds-specification.pdf.
[3] N. Amit, M. Ben-Yehuda, D. Tsafrir, and A. Schuster. vIOMMU: Efficient
IOMMU Emulation. In Proceedings of the 2011 USENIX Conference on USENIX
Annual Technical Conference, USENIXATC’11, pages 6–6, Berkeley, CA, USA,
2011. USENIX Association.
[4] BBC Digital Media Distribution: How we improved throughput by 4x. http:
//www.bbc.co.uk/blogs/internet/entries/.
[5] A. Belay, G. Prekas, M. Primorac, A. Klimovic, S. Grossman, C. Kozyrakis,
and E. Bugnion. The IX Operating System: Combining Low Latency, High
Throughput, and Efficiency in a Protected Dataplane. ACM Trans. Comput. Syst.,
34(4):11:1–11:39, Dec. 2016.
[6] B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. E. Fiuczynski, D. Becker,
C. Chambers, and S. Eggers. Extensibility Safety and Performance in the SPIN
Operating System. In Proceedings of the Fifteenth ACM Symposium on Operating
Systems Principles, SOSP ’95, pages 267–283, New York, NY, USA, 1995. ACM.
[7] S. Boyd-Wickizer, H. Chen, R. Chen, Y. Mao, F. Kaashoek, R. Morris, A. Pesterev,
L. Stein, M. Wu, Y. Dai, Y. Zhang, and Z. Zhang. Corey: An Operating System
for Many Cores. In Proceedings of the 8th USENIX Conference on Operating
Systems Design and Implementation, OSDI’08, pages 43–57, Berkeley, CA, USA,
2008. USENIX Association.
224