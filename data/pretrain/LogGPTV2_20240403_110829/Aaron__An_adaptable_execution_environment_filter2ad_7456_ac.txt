checked executions. At start, the original system is loaded
moderately. All requests are being processed using runtime
checks. As soon as load increases at time t1, the fraction
of checked execution drops to zero. Once the request rate
returns to its original level at time t2, requests are processed
using runtime checks again. The delay between time t2 and
the time at which the rate of checked executions rebounds
again is explained by the large number of pending requests
in the system. Once the backlog of requests is cleared, the
fraction of checked execution is 1.0 again.
G. Dynamic Adaptation of Used Software Variants
We present the overhead for the different diversiﬁcation
methods in Figure 10. The overhead is normalized to the
execution time of the version without runtime checks.
Although the absolute numbers are not a decisive factor
in this work, their relationship to each other is important:
Assertion, OverAllocate and in most cases also NullWrite
exhibit very low overhead. A typical application reads mem-
ory more often than it writes to it. Consequently, NullRead
employs more runtime checks than NullWrite; thus it is more
expensive.
The overhead of SWIFT and SWIFTCFC is signiﬁcantly
higher than of all other diversiﬁcation methods. Actually it is
even higher than reported in the literature [4]. This is due to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:53 UTC from IEEE Xplore.  Restrictions apply. 
0100020003000Workload[Requests per second]0500100015002000Throughput[Requests per second]NativeAaronMD502500500075001000012500Workload[Requests per second]100020003000400050006000700080009000Throughput[Requests per second]NativeAaronLibPNG025005000750010000Workload[Requests per second]0100020003000400050006000Throughput[Requests per second]NativeAaronWordcount0250005000075000100000Workload[Requests per second]01000020000300004000050000600007000080000Throughput[Requests per second]NativeAaronZoologist0306090120150180210Time [s]010002000ThroughputAaronNativet1t20306090120150180210Time [s]0.00.20.40.60.81.0Fraction ofchecked executionst1t2418Aaron does not necessitate additional hardware resources; it
only exploits spare CPU cycles that are otherwise wasted.
Runtime checks are a more pragmatic approach to han-
dle arbitrary failures:
logging for replay [22, 23], mon-
itoring [24, 25], data-ﬂow integrity [7], hardware error
detection [4], anomaly based checking [26, 10], input ﬁl-
tering [11, 27], and distributed assertions [12] to name a
few. Runtime checks can be automatically applied by the
compiler. In related work, the runtime checks are deployed
on honeypots and systems where performance does not
matter. In contrast, Aaron targets deployed systems with
throughput critical applications. Aaron switches on checking
only if it does not affect throughput.
If Aaron detects an error that it cannot tolerate, it drops the
currently processed request. More sophisticated recovery ap-
proaches like Rx [9] or MicroReboot [28] could be combined
with Aaron. Furthermore, our prototype does not provide
strong sandboxing for request processing, yet. Again,
it
would be possible to integrate known sandboxing approaches
like SafeDrive [29] or XFI [30] into Aaron.
Aaron was built with existing frameworks for distributed
computing in mind. It could be ported to MapReduce [31],
DryadLINQ [32] or similar approaches. In MapReduce
applications, Aaron could exploit
the not so uncommon
performance heterogeneity [33].
There are several options to speedup runtime checking.
The approach closest to ours is a feedback controller to
limit the overhead of watchdogs in distributed embedded
systems [34]. The feedback controller dynamically switches
the watchdog on and off to adjust performance. Instead
of embedded systems, we target data centers, which have
different characteristics.
Parallelizing runtime checks is another approach to reduce
the perceived performance overhead of runtime checks [35,
36, 17]. These approaches distribute load across additional
CPU cores. In contrast
to
changing workloads dynamically.
they do not adapt
to Aaron,
Finally, sampling can be used to balance the false positive
and false negative rate [10]. This system is complementary
to Aaron.
V. FUTURE WORK
In the future, we plan to extend Aaron in 3 directions:
First, using dynamic stack rewriting [17], Aaron could
switch between different variants during the processing of
a single task. This would enable Aaron to operate on
applications that are not task-oriented. Second, we want
to extend Aaron into the hardware domain by replicating
applications dynamically. Depending upon the load of the
cluster environment, replication can be used to increase fault
coverage even further. In contrast to traditional replication
approaches, we want to downscale replication if necessitated
by throughput demands. Third, we plan to explore new met-
rics to decide whether runtime checks should be employed.
Figure 10.
applications.
Slowdown of software diversiﬁcation methods for different
Figure 11. Fraction of executed checked versions.
our implementation: We do not use any of the optimizations
presented in [4].
Developing highly-efﬁcient diversiﬁcation methods is or-
thogonal to Aaron, and not scope of this work. Instead,
Aaron combines existing methods to increase the detection
rate of latent errors.
The fraction at which certain alternatives are executed
depends on the workload and the utilization of the system.
The distribution for Wordcount is shown in Figure 11. As
the utilization increases, Aaron adapts the runtime checking
by choosing cheaper variants more frequently.
IV. RELATED WORK
Many approaches to fault detection demand signiﬁcant
additional resources from the system. They are expensive in
terms of hardware and software. For example, the replicated
state machine approach using Byzantine fault tolerant (BFT)
protocols [19, 20] requires a minimum of 3F + 1 machines
to tolerate F independent faults. A different approach is
lockstep execution. Orchestra uses software-based lockstep
execution to detect errors [21]. Similar to Aaron, it applies
automatic software diversity. In contrast to redundant exe-
cution approaches like e.g. BFT protocols and Orchestra,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:53 UTC from IEEE Xplore.  Restrictions apply. 
AssertionOverAllocateNullWriteNullReadNullDerefSWIFTSWIFTCFC124816Slowdown MD5LibPNGWordcountZoologist0250005000075000100000Tasks per second0.00.10.20.30.40.50.60.70.80.91.0Fraction of checked executionsAssertionNativeNullDerefNullReadNullWriteOverAllocateSWIFTSWIFTCFC419Processing costs ﬂuctuate within one cluster and also across
multiple different clusters. Scheduling runtime checks only
if processing is cheap appears to be the next natural step for
Aaron.
VI. CONCLUSION
For current server systems, the difference between power
consumption at average load and peak load is small. Assum-
ing an average load of 50%, using the remaining computing
resources cost only about 14% (in our setup) in terms of
power consumption.
Aaron exploits these spare cycles and schedules auto-
matically diversiﬁed software variants to maximize fault
coverage. Aaron’s on-demand error checking has no inﬂu-
ence on the performance of the system, neither in terms
of throughput nor in terms of responsiveness. The runtime
overhead of Aaron is determined by the overheads of the
different diversiﬁcation methods only. Aaron enables the
usage of automatic software diversiﬁcation methods, even
though the overheads of the methods themselves might
be prohibitively high. It adapts failure coverage depending
on the current load situation. This adaptivity enables the
usage of Aaron in deployed systems without generating
prohibitively high costs.
ACKNOWLEDGEMENTS
The authors thank Martin Nowack for valuable discus-
sions and Figure 1. Parts of this research were funded in the
context of the SRT-15 project by the European Commission
under the Seventh Framework Program (FP7) with grant
agreement number 257843.
REFERENCES
[1] B. Schroeder and G. A. Gibson, “A large-scale study
of failures in high-performance computing systems,”
in Proceedings of
the International Conference on
Dependable Systems and Networks (DSN), 2006.
[2] R. W. M. Jones and P. H. J. Kelly, “Backwards-
compatible bounds checking for arrays and pointers
in C programs,” in Proceedings of
the 3rd Inter-
national Workshop on Automatic Debugging (AADE-
BUG), 1997.
[3] C. Wang, H. seop Kim, Y. Wu, and V. Ying, “Compiler-
managed software-based redundant multi-threading for
transient fault detection,” in Proceedings of the Interna-
tional Symposium on Code Generation and Optimiza-
tion (CGO), 2007.
[4] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, and
D. I. August, “SWIFT: Software implemented fault tol-
erance,” in Proceedings of the International Symposium
on Code generation and optimization (CGO), 2005.
[5] U. Schiffel, M. S¨ußkraut, and C. Fetzer, “An-encoding
compiler: Building safety-critical systems with com-
modity hardware,” in Proceedings of the 28th Interna-
tional Conference on Computer Safety, Reliability, and
Security (SAFECOMP), 2009.
[6] B. Randell, “System structure for software fault toler-
ance,” in Proceedings of the International Conference
on Reliable Software, 1975.
[7] M. Castro, M. Costa, and T. L. Harris, “Securing soft-
ware by enforcing data-ﬂow integrity,” in Proceedings
of the 7th Symposium on Operating Systems Design
and Implementation (OSDI), 2006.
[8] G. Novark, E. D. Berger, and B. G. Zorn, “Extermina-
tor: automatically correcting memory errors with high
probability,” in Proceedings of the 2007 ACM SIG-
PLAN Conference on Programming Language Design
and Implementation (PLDI), 2007.
[9] F. Qin, J. Tucek, J. Sundaresan, and Y. Zhou, “Rx:
treating bugs as allergies – a safe method to survive
software failures,” in Proceedings of the 20th ACM
Symposium on Operating Systems Principles (SOSP),
2005.
[10] M. W. Stephenson, R. Rangan, E. Yashchin, and E. V.
Hensbergen, “Statistically regulating program behavior
via mainstream computing,” in Proceedings of the 8th
Annual IEEE/ACM International Symposium on Code
Generation and Optimization (CGO), 2010.
[11] M. Costa, M. Castro, L. Zhou, L. Zhang, and
M. Peinado, “Bouncer: securing software by blocking
bad input,” in Proceedings of twenty-ﬁrst ACM SIGOPS
symposium on Operating systems principles (SOSP),
2007.
[12] X. Liu, Z. Guo, X. Wang, F. Chen, X. Lian, J. Tang,
M. Wu, F. M. Kaashoek, and Z. Zhang, “D3s: De-
bugging deployed distributed systems,” in Proceedings
of the 5th USENIX Symposium on Networked Systems
Design and Implementation (NSDI), 2008.
[13] C. Lattner and V. Adve, “LLVM: A compilation frame-
work for lifelong program analysis & transformation,”
in Proceedings of
the International Symposium on
Code Generation and Optimization (CGO), 2004.
[14] M. Rinard, C. Cadar, D. Dumitran, D. M. Roy, T. Leu,
and W. S. Beebee, Jr., “Enhancing server availability
and security through failure-oblivious computing,” in
Proceedings of the 6th Symposium on Operating Sys-
tems Design & Implementation (OSDI), 2004.
[15] G. A. Reis, J. Chang, and D. I. August, “Automatic
instruction-level software-only recovery,” IEEE Micro,
vol. 27, pp. 36–47, Jan 2007.
[16] U. H¨olzle and L. A. Barroso, The Datacenter as
a Computer: An Introduction to the Design of
Warehouse-Scale Machines, 1st ed.
Morgan and
Claypool Publishers, 2009.
[17] M. S¨ußkraut, S. Weigert, T. Knauth, U. Schiffel,
M. Meinhold, and C. Fetzer, “Prospect: A compiler
framework for speculative parallelization,” in Proceed-
ings of The 8th International Symposium on Code
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:53 UTC from IEEE Xplore.  Restrictions apply. 
420nals, M. Harren, G. C. Necula, and E. A. Brewer,
“Safedrive: Safe and recoverable extensions using
language-based techniques,” in 7th Symposium on Op-
erating Systems Design and Implementation (OSDI),
2006.
´U. Erlingsson, M. Abadi, M. Vrable, M. Budiu, and
G. C. Necula, “Xﬁ: Software guards for system ad-
dress spaces,” in 7th Symposium on Operating Systems
Design and Implementation (OSDI), 2006.
[30]
[31] J. Dean and S. Ghemawat, “MapReduce: Simpliﬁed
data processing on large clusters,” in 6th Symposium on
Operating System Design and Implementation (OSDI),
2004.
[32] Y. Yu, M. Isard, D. Fetterly, M. Budiu, lfar Erlingsson,
P. K. Gunda, and J. Currey, “Dryadlinq: A system
for general-purpose distributed data-parallel computing
using a high-level language.” in 8th USENIX Sympo-
sium on Operating Systems Design and Implementation
(OSDI), 2008.
[33] M. Zaharia, A. Konwinski, A. D. Joseph, R. H. Katz,
and I. Stoica, “Improving MapReduce performance in
heterogeneous environments,” in 8th USENIX Sympo-
sium on Operating Systems Design and Implementation
(OSDI), 2008.
[34] K. Liang, X. Zhou, K. Zhang, and R. Sheng, “An
adaptive performance management method for failure
detection,” in Proceedings of the 9th ACIS Interna-
tional Conference on Software Engineering, Artiﬁ-
cial Intelligence, Networking, and Parallel/Distributed
Computing, 2008.
[35] E. B. Nightingale, D. Peek, P. M. Chen, and J. Flinn,
“Parallelizing security checks on commodity hard-
ware,” in Proceedings of the 13th International Con-
ference on Architectural Support
for Programming
Languages and Operating Systems (ASPLOS), 2008.
[36] M. S¨ußkraut, S. Weigert, U. Schiffel, T. Knauth,
M. Nowack, D. B. de Brum, and C. Fetzer, “Specula-
tion for parallelizing runtime checks,” in Proceedings
of the 11th International Symposium on Stabilization,
Safety, and Security of Distributed Systems (SSS), 2009.
Generation and Optimization (CGO), 2010.
[18] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed,
“Zookeeper: wait-free coordination for internet-scale
systems,” in Proceedings of the USENIX annual tech-
nical conference 2010 (USENIXATC), 2010.
[19] M. Abd-El-Malek, G. R. Ganger, G. R. Goodson, M. K.
Reiter, and J. J. Wylie, “Fault-scalable byzantine fault-
tolerant services,” in Proceedings of the 20th ACM
Symposium on Operating Systems Principles (SOSP),
2005.
[20] A. Clement, E. Wong, L. Alvisi, M. Dahlin, and
M. Marchetti, “Making byzantine fault tolerant systems
tolerate byzantine faults,” in Proceedings of the 6th
USENIX Symposium on Networked Systems Design and
Implementation (NSDI), 2009.
[21] B. Salamat, T. Jackson, A. Gal, and M. Franz, “Or-
chestra:
intrusion detection using parallel execution
and monitoring of program variants in user-space,” in
EuroSys ’09: Proceedings of the fourth ACM european
conference on Computer systems.
New York, NY,
USA: ACM, 2009, pp. 33–46.
[22] X. Liu, “Wids checker: Combating bugs in distributed
systems,” in Proceedings of the 4th USENIX Sympo-
sium on Networked Systems Design and Implementa-
tion (NSDI), 2007.
[23] D. Geels, G. Altekar, P. Maniatis, T. Roscoe, and I. Sto-
ica, “Friday: Global comprehension for distributed re-
play,” in Proceedings of the 4th USENIX Symposium
on Networked Systems Design and Implementation
(NSDI), 2007.
[24] I. Cohen, M. Goldszmidt, T. Kelly, and J. Symons,
“Correlating instrumentation data to system states: A
building block for automated diagnosis and control,”
in Proceedings of the 6th Symposium on Operating
System Design and Implementation (OSDI), 2004.
[25] Z. Li, M. Zhang, Z. Zhu, Y. Chen, A. Greenberg, and
Y.-M. Wang, “Webprophet: Automating performance
prediction for web services,” in Proceedings of the 7th
USENIX Symposium on Networked Systems Design and
Implementation (NSDI), 2010.
[26] A. Depoutovitch and M. Stumm, “Software error early
detection system based on run-time statistical analysis
of function return values,” in 1st Workshop on Hot
Topics in Autonomic Computing, 2006.
[27] S. K. Cha, I. Moraru, J. Jang, J. Truelove, D. Brumley,
and D. G. Andersen, “Splitscreen: enabling efﬁcient,
distributed malware detection,” in Proceedings of the
7th USENIX Conference on Networked Systems Design
and Implementation (NSDI), 2010.
[28] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman,
and A. Fox, “Microreboot - a technique for cheap
recovery,” in 6th Symposium on Operating System
Design and Implementation (OSDI), 2004.
[29] F. Zhou, J. Condit, Z. R. Anderson, I. Bagrak, R. En-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:53 UTC from IEEE Xplore.  Restrictions apply. 
421