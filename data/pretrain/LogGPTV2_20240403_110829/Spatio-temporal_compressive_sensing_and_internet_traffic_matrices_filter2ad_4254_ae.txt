SRMF
Tomo−SRMF
0.2
0.4
0.6
0.8
TM element loss probability
(a) Abilene
Figure 5: Comparison for different loss models.
1
E
A
M
N
0.5
0.4
0.3
0.2
0.1
0
0
0.2
0.4
0.6
0.8
TM element loss probability
(b) Commercial
0.5
0.4
0.3
0.2
0.1
E
A
M
N
1
0
0
0.2
0.4
0.6
0.8
TM element loss probability
(c) GÉANT
1
Figure 6: Network tomography performance. Note, loss probability=1 corresponds to the traditional network tomography problem.
4.6 Computational Times
We measure the computation times of SRMF (KNN takes a small
amount of additional time) on matrices of various sizes. The com-
putation time is linear in the number of matrix elements, and quite
reasonable. A 200× 1000 matrix (with rank r = 10 used in the de-
composition) can be approximated in less than 3.5 seconds (using
Matlab on a 2.66 GHz Linux machine). Meanwhile, the computa-
tion time with respect to r is reasonably modeled by O(r2).
5. APPLICATIONS
The previous section considered the simple problem of interpo-
lation. We now consider different applications of matrix interpo-
lation, where the meaning or importance of the missing values are
determined by the application in question.
5.1 Tomography
A special case of the our approach is the network tomography
problem of inferring a TM from link-load measurements. In the
previous cases, the constraints come from direct measurements. In
network tomography the constraints are given by (1). However, it is
common to have some combination of these sets of measurements.
So it is desirable to combine them to obtain the best possible ap-
proximation to the TM. In this case, we can simply deﬁne A to
incorporate both (1) and (3), resulting in a combined penalty term
of the form ||A(LRT ) − Y ||2
Due to lack of space, we do not compare all possible algorithms
for TM estimation, but concentrate on two simple and relatively
well known algorithms. Further performance improvements might
be obtained by using more recently developed algorithms (e.g.,
[9]), but the insights are still useful. The two existing algorithms
we consider are the gravity model and Tomo-gravity. The gravity
model [33] is a simple rank-1 approximation to a single TM. It is
known to be a poor estimator of real TMs, but it has been success-
fully used as the ﬁrst step in the Tomo-gravity [33] algorithm. The
F + ||(LRT − D). ∗ M||2
F .
Tomo-gravity
Base
SRMF
Tomo-SRMF
Abilene
0.197 / 0.197
0.321 / 0.233
0.280 / 0.204
0.227 / 0.155
Commercial
0.292 / 0.292
0.566 / 0.380
0.483 / 0.285
0.288 / 0.203
GÉANT
0.441 / 0.439
1.198 / 0.489
1.185 / 0.516
0.433 / 0.240
Table 3: Network tomography performance: the ﬁrst number
is the performance where we have no direct TM measurements,
the second shows where we measure only 0.5% of the elements.
latter is yet another regularization based on the Kullback-Leibler
divergence between the gravity model and the measurements.
The notable feature (for the purpose of this paper) of the gravity
model and Tomo-gravity is that neither involve temporal informa-
tion. They operate purely on a TM snapshot. The gravity model
is based purely on row and column sums of the TM (snapshot) and
so has no need (or method) for incorporating additional informa-
tion. However, Tomo-gravity is just a standard regularization, and
so additional measurement equations can be easily added.
In this section we compare these algorithms against three alter-
natives: the baseline approximation, SRMF, and Tomo-SRMF. In
Figure 6 we show the performance of the algorithms with respect
to the proportion of the TM elements that are missing, but note that
in addition to direct measurement of the matrices, we assume we
can measure all of the link loads on the networks. So in this ﬁgure,
100% data loss corresponds to the standard network tomography
problem. As this part of the ﬁgure is important, but relatively hard
to read, we have duplicated key performance metrics in Table 3.
First, note that the gravity model is poor enough that its results
lie off the scale. The baseline technique is the second worst in
most cases, but is still much better than the gravity model. Sec-
ond, SRMF performs poorly at the pure network tomography task
where no direct measurements are available. However, if even a
few (as few as 0.5%) of the TM elements are directly observed,
then SRMF’s performance improves dramatically, whereas Tomo-
gravity’s performance improves roughly linearly with respect to the
275SRMF
Seasonal 1NN
Base
0.5
0.4
0.3
0.2
0.1
E
A
M
N
0
0
0.2
0.4
0.6
prediction length
(a) Abilene
0.8
1
E
A
M
N
0.5
0.4
0.3
0.2
0.1
0
0
0.5
0.4
0.3
0.2
0.1
E
A
M
N
0.8
1
0
0
0.2
0.4
0.6
0.8
1
prediction length
(c) GÉANT
0.2
0.4
0.6
prediction length
(b) Commercial
Figure 7: Network prediction performance.
increase in information. Finally, by combining SRMF and Tomo-
gravity, Tomo-SRMF gets the best of both worlds and signiﬁcantly
outperforms each individual method by itself. Figure 6 and Table 3
show the improvements.
Note that Soule et al. [25] also propose to incorporate ﬂow-level
measurements in TM estimation. Compared with their “third gen-
eration” TM estimation methods, Tomo-SRMF has two key advan-
tages: (i) it does not require any expensive calibration phase in
which entire TMs are directly measured, and (ii) it is highly ac-
curate and can reduce the error of Tomo-gravity by half with only
0.5-5% observed TM elements (whereas 10-20% directly observed
TM elements are required according to [25]).
5.2 Prediction
In this section we consider the behavior of SRMF with respect to
TM prediction. We do so by dividing our data into two segments, an
initial training segment up to some time t, and then a test segment
over which we try to predict the TM.
Prediction is rather different from the general problem of inter-
polation. Several techniques (SRSVD and NMF) just fail. KNN
does not work well because there are no temporally “near” neigh-
bors, and no spatial neighbors at all. However, if we can use the
temporal pattern in the data more creatively we can make progress.
For instance, rather than using a simple nearest neighbors tech-
nique, we use seasonal nearest neighbors. TMs show strong di-
urnal patterns, and so it is not surprising that offsetting neighbors
by the 24 hours period has beneﬁts. In essence, the seasonal nearest
neighbor approach assumes that today’s trafﬁc has the same pattern
as yesterdays.
Likewise for SRMF, we do not need to use the spatial constraint
matrix, as an entire slab of the data is missing (the future data we
are trying to predict). However, to allow a fair comparison with
seasonal nearest neighbors, we also use seasonality in construct-
ing our T matrix. We construct a difference matrix, but where the
interval between the differences is 24 hours.
Figure 7 shows the results with respect to the proportion of data
being predicted. Note that SRMF outperforms the other techniques,
and further that SRMF’s performance degrades very slowly as the
length of data being predicted increases (and the training data gets
correspondingly smaller). This shows that typical TMs exhibit tem-
poral regularity and SRMF can effectively take advantage of it.
5.3 Anomaly Detection
A common task in network operations is ﬁnding problems. There
are speciﬁc tools for ﬁnding some problems (e.g., SNMP is com-
monly used to ﬁnd link failures), and other problems such as spe-
ciﬁc attacks can be characterized by a signature, which signals
the attack. However, both of the above approaches rely on pre-
knowledge of the problems that we will encounter. There is a com-
plementary need to ﬁnd unanticipated problems in networks.
Such problems cannot be characterized before-hand, and so the
method commonly used to detect such anomalies is to ﬁnd sig-
niﬁcant differences from historical observations. Most approaches
involve some transformation of the data followed by outlier detec-
tion [30]. Common examples include simple ﬁltering of the data,
Fourier transform, wavelets, or PCA. The transform is aimed at
separating the “normal” space of historical events, from the anoma-
lous space. Techniques such as PCA do this explicitly, while oth-
ers rely on commonly observed properties. For example, Fourier
techniques rely on the normal data primarily inhabiting a low- to
mid-frequency space, so that anomalies involve high-frequencies
such as those incurred by a rapid change. Outlier detection can be
performed by taking the normal model of the data, and comparing
its values at each time point with the real data, and then seeking
points where the difference exceeds some threshold T .
In this section we will compare several approaches to anomaly
detection. To keep things simple so that we can gain an intuitive
understanding of the various properties of different approaches, we
will consider only three algorithms one temporal, one spatial, and
our spatio-temporal approach. The three approaches we use are
1. Differencing: Differencing is a standard time-series technique
to remove linear trends (typical trafﬁc data are non-stationary,
and over periods of minutes to hours can often be reasonably
approximated as having a linear trend). Differencing also high-
lights sudden changes, such as we would see in a trafﬁc spike
or a level shift [30]. Implicitly, differencing is using the data
from the previous time step as a model for the current time,
and so it has not received a great deal of consideration in the
networking literature, but it provides a simple temporal bench-
mark against which we can gain some intuition. We can write
the differencing operator as postmultiplication of X by T =
T oeplitz(0, 1,−1), a purely temporal operation that makes no
use of spatial correlations between TM elements.
2. PCA/SVD: PCA/SVD has received much interest for network
anomaly detection in recent years [12–14, 21, 29, 30], and is
the only common spatial technique for anomaly detection. As
noted earlier, PCA/SVD is applied by choosing the rank r of
the normal subspace (based on the power contained in the ﬁrst r
singular values), and projecting the input data X into the abnor-
mal subspace, where artifacts are then tested for size. Implic-
itly, we are looking at the difference between the normal model
of the data created by the low-rank SVD approximation and the
data itself. Intuitively, the process builds a (simple) model from
the historical relationships between TM elements. New time
points are compared to see if they satisfy this relationship. If
not, they are declared to be anomalies.
It is a purely spatial
technique, since reordering of the data in time (the columns of
X) has no effect on the results. Interestingly, compressive sens-
ing ideas have already appeared in the context of PCA based
anomaly detection [12], though in that context the goal was to
reduce the volume of data transmitted to a NOC, and the miss-
ing data could be controlled, whereas in our context the missing
data are out of our control.
3. SRMF: In this context we apply SRMF directly to the trafﬁc
data including the anomalies, much as one would with SVD.
Our technique, however, is truly spatio-temporal as the model
that we create involves both the spatial and temporal properties
276of the underlying TM. The low-rank approximation is then used
as a model for the normal data, and the differences between this
and the real trafﬁc are used to signal anomalies. Once again,
we use the standard method of thresholding these differences
to detect outliers.
We will compare each of these algorithms using simulations. Ring-
berg et al. [21] explain in detail why simulation should be used for
accurate comparisons of anomaly detection techniques.
In brief
their reasons are: (i) accurate and complete ground truth informa-
tion is needed to form both false-alarm and detection probability
estimates (both are needed for comparisons, as one by itself can be
entirely misleading); (ii) many more results are needed (than one
can obtain from data) to form accurate estimates of probabilities,
and (iii) simulation allows one to vary parameters (say the anomaly
size) to study their effects. Simulation is necessary, but not sufﬁ-
cient for validation, so we expect that further work is needed on this
type of anomaly detection before it is used by network operators.
Our approach to simulation is intended to highlight the features