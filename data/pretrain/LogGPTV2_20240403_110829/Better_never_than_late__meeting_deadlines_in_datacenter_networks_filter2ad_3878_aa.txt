title:Better never than late: meeting deadlines in datacenter networks
author:Christo Wilson and
Hitesh Ballani and
Thomas Karagiannis and
Antony I. T. Rowstron
Better Never than Late: Meeting Deadlines in
Datacenter Networks
Christo Wilson
Hitesh Ballani
Thomas Karagiannis
Ant Rowstron
PI:EMAIL PI:EMAIL PI:EMAIL PI:EMAIL
Microsoft Research
Cambridge, UK
ABSTRACT
The soft real-time nature of large scale web applications in
today’s datacenters, combined with their distributed work-
ﬂow, leads to deadlines being associated with the datacenter
application traﬃc. A network ﬂow is useful, and contributes
to application throughput and operator revenue if, and only
if, it completes within its deadline. Today’s transport pro-
tocols (TCP included), given their Internet origins, are ag-
nostic to such ﬂow deadlines. Instead, they strive to share
network resources fairly. We show that this can hurt appli-
cation performance.
Motivated by these observations, and other (previously
known) deﬁciencies of TCP in the datacenter environment,
this paper presents the design and implementation of D3, a
deadline-aware control protocol that is customized for the
datacenter environment. D3 uses explicit rate control to
apportion bandwidth according to ﬂow deadlines. Evalua-
tion from a 19-node, two-tier datacenter testbed shows that
D3, even without any deadline information, easily outper-
forms TCP in terms of short ﬂow latency and burst toler-
ance. Further, by utilizing deadline information, D3 eﬀec-
tively doubles the peak load that the datacenter network can
support.
Categories and Subject Descriptors: C.2.2 [Computer-
Communication Networks]: Network Protocols
General Terms: Algorithms, Design, Performance
Keywords: Online services, Datacenter, SLA, Deadline,
rate control
1.
INTRODUCTION
The proliferation of datacenters over the past few years
has been primarily driven by the rapid emergence of user-
facing online services. Web search, retail, advertisement,
social networking and recommendation systems represent a
few prominent examples of such services.
While very diﬀerent in functionality, these services share a
couple of common underlying themes. First is their soft real
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’11, August 15–19, 2011, Toronto, Ontario, Canada.
Copyright 2011 ACM 978-1-4503-0797-0/11/08 ...$10.00.
time nature resulting from the need to serve users in a timely
fashion. Consequently, today’s online services have service
level agreements (SLAs) baked into their operation [10,16,24].
User requests are to be satisﬁed within a speciﬁed latency
target; when the time expires, responses, irrespective of their
completeness, are shipped out. However, the completeness
of the responses directly governs their quality and in turn,
operator revenue [17]. Second, the mantra of horizontal scal-
ability entails that online services have a partition-aggregate
workﬂow with user requests being partitioned amongst (mul-
tiple) layers of workers whose results are then aggregated to
form the response [10].
The combination of latency targets for datacenter appli-
cations and their distributed workﬂow has implications for
traﬃc inside the datacenter. Application latency targets cas-
cade to targets for workers at each layer; targets in the region
of 10 to 100ms are common [4], which in turn, yield targets
for network communication between the workers. Speciﬁ-
cally, for any network ﬂow initiated by these workers, there
is an associated deadline. The ﬂow is useful and contributes
to the application throughput if, and only if, it completes
within the deadline.
Today’s datacenter networks, given their Internet origins,
are oblivious to any such implications of the application de-
sign. Speciﬁcally, the congestion control (TCP) and ﬂow
scheduling mechanisms (FIFO queuing) used in datacenters
are unaware of ﬂow deadlines and hence, strive to optimize
network-level metrics: maximize network throughput while
achieving fairness. This mismatch can severely impact appli-
cation performance; this is best illustrated through a couple
of simple examples:
– Case for unfair sharing: Consider two ﬂows that share
a bottleneck link; one ﬂow has a tighter deadline than the
other. As shown in ﬁgure 1, with today’s setup, TCP strives
for fairness and the ﬂows ﬁnish at similar times.1 How-
ever, only one ﬂow makes its deadline and is included in the
user response. Apart from hurting application performance,
this wastes valuable network resources on a non-contributing
ﬂow. Alternatively, given explicit information about ﬂow
deadlines, the network can distribute bandwidth unequally
to meet the deadlines.
– Case for ﬂow quenching: Consider a common application
setting involving multiple servers responding to an aggre-
gator simultaneously. The resulting network ﬂows share a
bottleneck link and have the same deadline. Further, as-
1We use TCP as a running example since it is used in data-
centers today. However, our arguments apply to other TCP
variants and proposals like DCTCP [4], XCP [19], etc.
50Status Quo
Deadline aware
Flows
f2
f1
X
f2
f1
d2
Time
d1
d1
d2
Time
Figure 1: Two ﬂows (f1, f2) with diﬀerent deadlines
(d1, d2). The thickness of a ﬂow line represents the
rate allocated to it. Awareness of deadlines can be
used to ensure they are met.
Status Quo
Deadline aware
Flows
x
x
x
x
x
x
through explicit rate control [11,19], D3 explores the feasibil-
ity of exploiting deadline information to control the rate at
which endhosts introduce traﬃc in the network. Speciﬁcally,
applications expose the ﬂow deadline and size information
at ﬂow initiation time. Endhosts use this information to
request rates from routers along the data path to the des-
tination. Routers thus allocate sending rates to ﬂows to
greedily satisfy as many deadlines as possible.
Despite the fact that ﬂows get assigned diﬀerent rates,
instead of fair share, D3 does not require routers to maintain
per-ﬂow state. By capitalizing on the nature of trust in
the datacenter environment, both the state regarding ﬂow
sending rates and rate policing are delegated to endhosts.
Routers only maintain simple aggregate counters. Further,
our design ensures that rate assignments behave like “leases”
instead of reservations, and are thus unaﬀected by host or
router failures.
To this eﬀect, this paper makes three main contributions:
d
Time
d
Time
• We present the case for utilizing ﬂow deadline informa-
tion to apportion bandwidth in datacenters.
Figure 2: Multiple ﬂows with the same deadline (d).
The bottleneck capacity cannot satisfy the deadline
if all six ﬂows proceed. Quenching one ﬂow ensures
that the others ﬁnish before the deadline.
sume that congestion on the bottleneck link is such that the
aggregate capacity available to these ﬂows is not enough to
ﬁnish all the ﬂows before the deadline. Figure 2 shows that
with today’s setup, all ﬂows will receive their fair share of
the bandwidth, ﬁnish at similar times and hence, miss the
deadline. This, in turn, results in an empty response to the
end user. Given ﬂow deadlines, it may be possible to deter-
mine that the network is congested and quench some ﬂows
to ensure that the remaining ﬂows do meet the deadline.
Both these examples reﬂect a tension between the func-
tionality oﬀered by a deadline agnostic network and appli-
cation goals. This tension is above and beyond the known
deﬁciencies of TCP in the datacenter environment. These in-
clude the incast problem resulting from bursts of ﬂows [8,23],
and the queuing and buﬀer pressure induced by a traﬃc mix
that includes long ﬂows [4].
However, the goal of a deadline-aware datacenter network
poses unique challenges:
1. Deadlines are associated with ﬂows, not packets. All
packets of a ﬂow need to arrive before the deadline.
2. Deadlines for ﬂows can vary signiﬁcantly. For example,
online services like Bing and Google include ﬂows with a
continuum of deadlines (including some that do not have
a deadline). Further, datacenters host multiple services
with diverse traﬃc patterns. This, combined with the
previous challenge, rules out traditional scheduling solu-
tions, such as simple prioritization of ﬂows based on their
length and deadlines (EDF scheduling [21]).
3. Most ﬂows are very short (<50KB) and RTTs minimal
(≈300µsec). Consequently, reaction time-scales are short,
and centralized, or heavy weight mechanisms to reserve
bandwidth for ﬂows are impractical.
In this paper, we present D3, a Deadline-Driven Delivery
control protocol, that addresses the aforementioned chal-
lenges. Inspired by proposals to manage network congestion
• We present the design, implementation and evaluation of
D3, a congestion control protocol that makes datacenter
networks deadline aware. Results from our testbed de-
ployment show that D3 can eﬀectively double the peak
load that a datacenter can support.
• We show that apart from being deadline-aware, D3 per-
forms well as a congestion control protocol for datacenter
networks in its own right. Even without any deadline in-
formation, D3 outperforms TCP in supporting the mix
of short and long ﬂows observed in datacenter networks.
While we are convinced of the beneﬁts of tailoring dat-
acenter network design to the soft real time nature of dat-
acenter applications, we also realize that the design space
for such a deadline-aware network is vast. There exists a
large body of work for satisfying application demands in the
Internet. While we discuss these proposals in Section 3, the
D3 design presented here is heavily shaped by the peculiar-
ities of the datacenter environment — its challenges (lots
of very short ﬂows, tiny RTTs, etc.) and luxuries (trusted
environment, limited legacy concerns, etc.). We believe that
D3 represents a good ﬁrst step towards a datacenter network
stack that is optimized for application requirements and not
a retroﬁtted Internet design.
2. BACKGROUND: TODAY’S
DATACENTERS
In this section, we provide a characterization of today’s
datacenters, highlighting speciﬁc features that inﬂuence D3
design.
2.1 Datacenter applications
Partition-aggregate. Today’s large-scale, user facing web
applications achieve horizontal scalability by partitioning
the task of responding to users amongst worker machines
(possibly at multiple layers). This partition-aggregate struc-
ture is shown in Figure 3, and applies to many web applica-
tions like search [4], social networks [6], and recommenda-
tion systems [10]. Even data processing services like MapRe-
duce [9] and Dryad [18] follow this model.
51)
)
%
%
(
(
t
t
e
e
m
m
s
s
e
e
n
n
i
i
l
l
d
d
a
a
e
e
D
D
 100
 100
 95
 95
 90
 90
 85
 85
 80
 80
 75
 75
 70
 70
 10
 10
 20
 20
 30
 30
 40
 40
 50
 50
Mean Deadline (ms)
Mean Deadline (ms)
Figure 4: Deadlines met (%) based on datacenter
ﬂow completion times.
with the impact of such ineﬃciencies [4], and this might
mean artiﬁcially limiting the peak load a datacenter can sup-
port so that application SLAs are met. Hence, a signiﬁcant
fraction of ﬂow deadlines are missed in today’s datacenters.
Flow sizes. Since deadlines are associated with ﬂows, all
packets of a ﬂow need to arrive before its deadline. Thus,
a-priori knowledge of the ﬂow size is important.
Indeed,
for most interactive web applications today, the size of net-
work ﬂows initiated by workers and aggregators is known in
advance. As a speciﬁc example, in web search, queries to
workers are ﬁxed in size while responses essentially include
the top-k matching index records (where k is speciﬁed in the
query). Thus, the size of the response ﬂow is known to the
application code at the worker even before it begins process-
ing. The same holds for many other building block services
like key-value stores [6,10], data processing [9,18], etc. Even
for applications where this condition does not hold, the ap-
plication designer can typically provide a good estimate of
the expected ﬂow sizes. Hence, many web applications have
knowledge of the ﬂow size at ﬂow initiation time.
2.2 TCP in Datacenters
The problems resulting from the use of TCP in datacen-
ters are well documented [8,23]. Bursts of concurrent ﬂows
that are all too common with the partition aggregate appli-
cation structure can cause a severe drop in network through-
put (incast). Beyond this, the presence of long ﬂows, when
combined with TCP’s tendency to drive queues to losses,
hurts the latency of query-response ﬂows. These problems
have placed artiﬁcial limitations on application design, with
designers resorting to modifying application workﬂow to ad-
dress the problems [4].2
Motivated by these issues, recent proposals have devel-
oped novel congestion control protocols or even moved to
UDP to address the problems [4,22]. These protocols aim
to ensure: (i) low latency for short ﬂows, even in the face
of bursts, and (ii) good utilization for long ﬂows. We con-
cur that this should be the baseline for any new datacenter
transport protocol and aim to achieve these goals. We fur-
ther argue that these minimum requirements should ideally
be combined with the ability to ensure that the largest pos-
sible fraction of ﬂows meet their deadlines. Finally, many
application level solutions (like SEDA [25]) deal with vari-
able application load. However, they are not well suited for
network/transport problems since the datacenter network is
shared amongst multiple applications. Further, applications
do not have visibility into the network state (congestion, fail-
2Restrictions on the fan-out factor for aggregators and the
size of the response ﬂows from workers to aggregator are a
couple of examples of the limitations imposed.
Figure 3: An example of the partition aggregate
model with the associated component deadlines in
the parentheses.
Application deadlines. The interactive nature of web
applications means that latency is key. Customer studies
guide the time in which users need to be responded to [20],
and after accounting for wide-area network and rendering
delays, applications typically have an SLA of 200-300ms to
complete their operation and ship out the user reply [10].
These application SLAs lead to SLAs for workers at each
layer of the partition aggregate hierarchy. The worker SLAs
mean that network ﬂows carrying queries and responses to
and from the workers have deadlines (see Figure 3). Any
network ﬂow that does not complete by its deadline is not
included in the response and typically hurts the response
quality, not to mention wastes network bandwidth. Ulti-
mately, this aﬀects operator revenue; for example, an added
latency of 100ms costs Amazon 1% of sales [17]. Hence,
network ﬂows have deadlines and these deadlines are (im-
plicitly) baked into all stages of the application operation.
Deadline variability. Worker processing times can vary
signiﬁcantly. For instance, with search, workers operate on
diﬀerent parts of the index, execute diﬀerent search algo-
rithms, return varying number of response items, etc. This
translates to varying ﬂow deadlines. Datacenter traﬃc itself
further contributes to deadline variability. We mentioned
the short query and response ﬂows between workers. Be-
yond this, much datacenter traﬃc comprises of time sensi-
tive, short messages (50KB to 1MB) that update the con-
trol state at the workers and long background ﬂows (5KB to
50MB) that move fresh data to the workers [4]. Of course,
the precise traﬃc pattern varies across applications and dat-
acenters. A Google datacenter running MapReduce jobs will
have primarily long ﬂows, some of which may carry dead-
lines. Hence, today’s datacenters have a diverse mix of ﬂows
with widely varying deadlines. Some ﬂows, like background
ﬂows, may not have deadlines.
Missed deadlines. To quantify the prevalence of missed
deadlines in today’s datacenters, we use measurements of
ﬂow completion times in datacenters [4]. We were unable
to obtain ﬂow deadline information, and resort to captur-
ing their high variability by assuming that deadlines are ex-
ponentially distributed. Figure 4 shows the percentage of
ﬂows that meet their deadlines with varying mean for the
deadline distribution. We ﬁnd that even with lax deadlines
(mean=40ms), more than 7% of ﬂows miss their deadline.
Our evaluation (section 6) shows that this results from net-
work ineﬃciencies. Application designers struggle to cope
52)
)
%
%
(
(
t
t
u
u
p
p
h
h
g
g
u
u
o
o
r
r
h