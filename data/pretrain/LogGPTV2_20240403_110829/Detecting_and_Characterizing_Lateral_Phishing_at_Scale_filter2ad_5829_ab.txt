Across these enterprises, 25 organizations have 100 or fewer
user accounts, 34 have between 101–1000 accounts, and 33
have over 1000 accounts. Real-estate, technology, and edu-
cation constitute the three most common industries in our
dataset, with 15, 13, and 13 enterprises respectively; Figures 1
and 2 show the distribution of the economic sectors and sizes
of our dataset’s organizations, broken down by exploratory
organizations versus test organizations (§ 3.2).
USENIX Association
28th USENIX Security Symposium    1275
agricultureconsumereducationenergyentertainmentfinancialfoodgovernmenthealthindustrialsnon-profitpeople servicesreal-estatetechnologytourismtransportation02468101214# of organizationsExploratory orgsTest orgsFigure 2: Breakdown of the organization sizes across our dataset’s
52 exploratory organizations versus the 40 test organizations.
3.1 Schema
The organizations in our dataset use Ofﬁce 365 as their email
provider. At a high level, each email object contains: a unique
Ofﬁce 365 identiﬁer; the email’s metadata (SMTP header
information), which describes properties such as the email’s
sent timestamp, recipients, purported sender, and subject; and
the email’s body, the contents of the email message in full
HTML formatting. Ofﬁce 365’s documentation describes the
full schema of each email object [29]. Additionally, for each
organization, we have a set of veriﬁed domains: domains
which the organization has declared that it owns.
3.2 Dataset Size
Our dataset consists of 113,083,695 unique, employee-sent
emails. To ensure our detection techniques generalized (Sec-
tion 5.1), we split our data into a training dataset of emails
from 52 ‘exploratory organizations’ during April–June 2018,
and a test dataset covering July–October 2018 from 92 or-
ganizations. Our test dataset consists of emails from the 52
exploratory organizations (but from a later, disjoint time pe-
riod than our training dataset), plus data from an additional,
held-out set of 40 ‘test organizations’. We selected the 40 test
organizations via a random sample that we performed prior
to analyzing any data. Our training dataset has 25,670,264
emails, and our test dataset has 87,413,431 emails. Both sets
of organizations cover a diverse range of industries and sizes
as shown in Figures 1 and 2. The exploratory organizations
span a total of 89,267 user mailboxes that sent or received
email, and the test organizations have 138,752 mailboxes
(based on the data from October 2018).1
3.3 Ground truth
Our set of lateral phishing emails comes from two sources:
(1) attack emails reported to Barracuda by an organization’s
security administrators, as well as attacks reported by users
to their organization or directly to Barracuda, and (2) emails
1The number of mailboxes is an upper bound on the number of employees
due to the use of mailing lists and aliases.
Figure 3: An anonymized screenshot of the web page that a phishing
URL in a lateral phishing email led to.
ﬂagged by our detector (§4), which we manually reviewed
and labeled before including.
At a high-level, to manually label an email as phishing, or
not, we examined its message content, Ofﬁce 365 metadata,
and Internet Message Headers [33] to determine whether the
email contained phishing content, and whether the email came
from a compromised account (versus an external account,
which we do not treat as lateral phishing). For example, if the
Ofﬁce 365 metadata showed that a copy of the email resided
in the employee’s Sent Items folder, or if its headers showed
that the email passed the corresponding SPF or DKIM [9]
checks, then we considered the email to be lateral phishing.
Appendix §A.1 describes our labeling procedure in detail.
Additionally, for a small sample of URLs in these lateral
phishing emails, employees at Barracuda accessed the phish-
ing URL in a VM-contained browser to better understand
the end goals of the attack. To minimize potential harm and
side effects, these employees only visited phishing URLs
which contained no unique identiﬁers (i.e., no random strings
or user/organization information in the URL path). To han-
dle any phishing URLs that resided on URL-shortening do-
mains, we used one of Barracuda’s URL-expansion APIs
that their production services already apply to email URLs,
and only visited suspected phishing links that expanded to a
non-side-effect URL. Most phishing URLs we explored led
to a SafeBrowsing interstitial webpage, likely reﬂecting our
use of historical emails, rather than what users would have
encountered contemporaneously. However, more recent mali-
cious URLs consistently led to credential phishing websites
designed to look like a legitimate Ofﬁce 365 login page (the
email service provider used by our study’s organizations); Fig-
ure 3 shows an anonymized example of one phishing website.
In total, our dataset contains 1,902 lateral phishing emails
(unique by subject, sender, and sent-time), sent by 154 hi-
jacked employee accounts from 33 organizations. 1,694 of
these emails were reported by users, with the remainder found
solely by our detector (§ 4); our detector also ﬁnds many of the
1276    28th USENIX Security Symposium
USENIX Association
= 1,001 mailboxes05101520253035# of organizationsExploratory OrgsTest orgsuser-reported attacks as well (§ 5). Among the user-reported
attacks, 40 emails (from 12 hijacked accounts) contained a
fake wire transfer or malicious attachment, while the remain-
ing 1,862 emails used a malicious URL.
We focus our detection strategy on URL-based phishing,
given the prevalence of this attack vector. This focus means
that our analysis and detection techniques do not reﬂect the
full space of lateral phishing attacks. Despite this limitation,
our dataset’s attacks span dozens of organizations, enabling
us to study a prevalent class of enterprise phishing that poses
an important threat in its own right.
4 Detecting Lateral Phishing
Adopting the lateral attacker threat model deﬁned by Ho et
al. [18], we focus on phishing emails sent by a compromised
employee account, where the attack embeds a malicious URL
as the exploit (e.g., leading the user to a phishing webpage).
We explored three strategies for detecting lateral phishing
attacks, but ultimately found that one of the strategies detected
nearly all of the attacks identiﬁed by all three approaches. At
a high level, the two less fruitful strategies detected attacks
by looking for emails that contained (1) a rare URL and (2)
a message whose text seemed likely to be used for phishing
(e.g., similar text to a known phishing attack). Because our
primary detection strategy detected all-but-two of the attacks
found by the other strategies, while ﬁnding over ten times as
many attacks, we defer discussion of the two less successful
approaches to our extended technical report [17]; below, we
focus on exploring the more effective strategy in detail. In our
evaluation, we include the two additional attacks found by the
alternative approaches as false negatives for our detector.
Overview: We examined the user-reported lateral phishing
incidents in our training dataset (April–June 2018) to iden-
tify widespread themes and behaviors that we could leverage
in our detector. Grouping this set of attacks by the hijacked
account (ATO) that sent them, we found that 95% of these
ATOs sent phishing emails to 25 or more distinct recipients.2
This prevalent behavior, along with additional feature ideas
inspired by the lure-exploit detection framework [18], pro-
vide the basis for our detection strategy. In the remainder of
this section, we describe the features our detector uses, the
intuition behind these features, and our detector’s machine
learning procedure for classifying emails.
Our techniques provide neither an all-encompassing ap-
proach to ﬁnding every attack, nor guaranteed robustness
against motivated adversaries trying to evade detection. How-
ever, we show in Section 5 that our approach ﬁnds hundreds
of lateral phishing emails across dozens of real-world organi-
zations, while incurring a low volume of false positives.
2To assess the generalizability of our approach, our evaluation uses a
withheld dataset, from a later timeframe and with new organizations (§ 5).
Features: Our detector extracts three sets of features. The ﬁrst
set consists of two features that target the popular behavior we
observed earlier: contacting many recipients. Given an email,
we ﬁrst extract the number of unique recipients across the
email’s To, CC, and BCC headers. Additionally, we compute
the Jaccard similarity of this email’s recipient set to the closest
set of historical recipients seen in any employee-sent email
from the preceding month. We refer to this latter (similarity)
feature as the email’s recipient likelihood score.
The next two sets of features draw upon the lure-exploit
phishing framework proposed by Ho et al. [18]. This frame-
work posits that phishing emails contain two necessary com-
ponents: a ‘lure’, which convinces the victim to believe the
phishing email and perform some action; and an ‘exploit’: the
malicious action the victim should execute. Their work ﬁnds
that using features that target both of these two components
signiﬁcantly improves a detector’s performance.
To characterize whether a new email contains a poten-
tial phishing lure, our detector extracts a single, lightweight
boolean feature based on the email’s text. Speciﬁcally, Bar-
racuda provided us with a set of roughly 150 keywords and
phrases that frequently occur in phishing attacks. They de-
veloped this set of ‘phishy’ keywords by extracting the link
text from several hundred real-world phishing emails (both
external and lateral phishing) and selecting the (normalized)
text that occurred most frequently among these attacks. The-
matically, these suspicious keywords convey a call to action
that entices the recipient to click a link. For our ‘lure’ feature,
we extract a boolean value that indicates whether an email
contains any of these phishy keywords.
Finally, we complete our detector’s feature set by extracting
two features that capture whether an email might contain an
exploit. Since our work focuses on URL-based attacks, this
set of features reﬂects whether the email contains a potentially
dangerous URL.
First, for each email, we extract a global URL reputation
feature that quantiﬁes the rarest URL an email contains. Given
an email, we extract all URLs from the email’s body and ig-
nore URLs if they fall under two categories: we exclude all
URLs whose domain is listed on the organization’s veriﬁed
domain list (§ 3.1), and we also exclude all URLs whose
displayed, hyperlinked text exactly matches the URL of the
hyperlink’s underlying destination. For example, in Listing 1’s
attack, the displayed text of the phishing hyperlink was “Click
Here”, which does not match the hyperlink’s destination (the
phishing site), so our procedure would keep this URL. In
contrast, Alice’s signature from Listing 1 might contain a
link to her personal website, e.g., www.alice.com; our pro-
cedure would ignore this URL, since the displayed text of
www.alice.com matches the hyperlink’s destination.
This latter ﬁltering criteria makes the assumption that a
phishing URL will attempt to obfuscate itself, and will not
display the true underlying destination directly to the user.
After these ﬁltering steps, we extract a numerical feature by
USENIX Association
28th USENIX Security Symposium    1277
mapping each remaining URL to its registered domain, and
then looking up each domain’s ranking on the Cisco Um-
brella Top 1 Million sites [20];3 for any unlisted domain, we
assign it a default ranking of 10 million. We treat two spe-
cial cases differently. For URLs on shortener domains, our
detector attempts to recursively resolve the shortlink to its
ﬁnal destination. If this resolution succeeds, we use the global
ranking of the ﬁnal URL’s domain; otherwise, we treat the
URL as coming from an unranked domain (10 million). For
URLs on content hosting sites (e.g., Google Drive or Share-
point), we have no good way to determine its suspiciousness
without fetching the content and analyzing it (an action that
has several practical hurdles). As a result, we treat all URLs
on content hosting sites as if they reside on unranked domains.
After ranking each URL’s domain, we set the email’s global
URL reputation feature to be the worst (highest) domain rank-
ing among its URLs. Intuitively, we expect that phishers will
rarely host phishing pages on popular sites, so a higher global
URL reputation indicates a more suspicious email. In prin-
ciple a motivated adversary could evade this feature; e.g., if
an adversary can compromise one of the organization’s ver-
iﬁed domains, they can host their phishing URL from this
compromised site and avoid an accurate ranking. However,
we found no such instances in our set of user-reported lateral
phishing. Additionally, since the goal of this paper is to begin
exploring practical detection techniques, and develop a large
set of lateral phishing incidents for our analysis, this feature
sufﬁces for our needs.
In addition to this global reputation metric, we extract a
local metric that characterizes the rareness of a URL with
respect to the domains of URLs that an organization’s em-
ployees typically send. Given a set of URLs embedded within
an email, we map each URL to its fully-qualiﬁed domain
name (FQDN) and count the number of days from the preced-
ing month where at least one employee-sent email included a
URL on the FQDN. We then take the minimum value across
all of an email’s URLs; we call this minimum value the lo-
cal URL reputation feature. Intuitively, suspicious URLs will
have both a low global reputation and a low local reputation.
However, our evaluation (§ 5.2) ﬁnds that this local URL
reputation feature adds little value: URLs with a low local
URL reputation value almost always have a low global URL
reputation value, and vice versa.
Classiﬁcation: To label an email as phishing or not, we
trained a Random Forest classiﬁer [45] with the aforemen-
tioned features. To train our classiﬁer, we take all user-
reported lateral phishing emails in our training dataset, and
combine them with a set of likely-benign emails. We generate
this set of “benign” emails by randomly sampling a subset
of the training window’s emails that have not been reported
as phishing; we sample 200 of these benign emails for each
3We use a list fetched in early March 2018 for our feature extraction, but
in practice, one could use a continuously updated list.
attack email to form our set of benign emails for training. Fol-
lowing standard machine learning practices, we selected both
the hyperparameters for our classiﬁer and the exact downsam-
pling ratio (200:1) using cross-validation on this training data.
Appendix A.2 describes our training procedure in more detail.
Once we have a trained classiﬁer, given a new email, our
detector extracts its features, feeds the features into this clas-
siﬁer, and outputs the classiﬁer’s decision.
5 Evaluation
In this section we evaluate our lateral phishing detector. We
ﬁrst describe our testing methodology, and then show how
well the detector performs on millions of emails from over 90
organizations. Overall, our detector has a high detection rate,
generates few false positives, and detects many new attacks.
5.1 Methodology
Establishing Generalizability: As described earlier in Sec-
tion 3.2, we split our dataset into two disjoint segments: a
training dataset consisting of emails from the 52 exploratory
organizations during April–June 2018 and a test dataset from
92 enterprises during July–October 2018; in § 5.2, we show
that our detector’s performance remains the same if our test
dataset contains only the emails from the 40 withheld test
organizations. Given these two datasets, we ﬁrst trained our
classiﬁer and tuned its hyperparameters via cross validation
on our training dataset (Appendix A.2). Next, to compute our
evaluation results, we ran our detector on each month of the
held-out test dataset. To simulate a classiﬁer in production,
we followed standard machine learning practices and used
a continuous learning procedure to update our detector each
month [38]. Namely, at the end of each month, we aggregated
the user-reported and detector-discovered phishing emails
from all previous months into a new set of phishing ‘training’
data; and, we aggregated our original set of randomly sampled
benign emails with our detector’s false positives from all pre-
vious months to form a new benign ‘training’ dataset. We then
trained a new model on this aggregated training dataset and
used this updated model to classify the subsequent month’s
data. However, to ensure that any tuning or knowledge we
derived from the training dataset did not bias or overﬁt our
classiﬁer, we did not alter any of the model’s hyperparameters
or features during our evaluation on the test dataset.
Our evaluation’s temporal-split between the training and
test datasets, along with the introduction of new data from
randomly withheld organizations into the test dataset, follows
best practices that recommend this approach over a random-
ized cross-validation evaluation [2, 31, 34]. A completely ran-
domized evaluation (e.g., cross-validation) risks training on
data from the future and testing on the past, which might lead
us to overestimate the detector’s effectiveness. In contrast,
1278    28th USENIX Security Symposium
USENIX Association
Metric