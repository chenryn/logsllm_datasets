r have densities y±
r )) obey
r )−
y+
r (Qpdr q(Y +
r )) = pdr y−
r (Qpdr q(Y −
r ))
(11)
We use the above expression to compute the bounds and con-
sider four cases. For cases (a)–(c) we plot the actual CDF on the
unmeasured path, together with the CDF bound in Figure 2.
(a) Homogeneous Delay. m1 = 1.0, m2 = 1.1, m3 = 1.2, m4 =
1.3. The delay on path r1 is somewhat underestimated, but
then large delays only very rarely occur.
(b) High Delay on Unmeasured Path, Low Delay Elsewhere. m1 =
10, m2 = 1.1, m3 = 1.2, m4 = 1.3. The low delays on links
not included in the unmeasured path allow fairly close esti-
mation of the delay distribution on r1.
(c) High Delay on Unmeasured Path, Some High Delay Else-
where. m1 = 10, m2 = 11, m3 = 1.2, m4 = 1.3. Although
elevation of delay on r1 is detected, the amount is somewhat
underestimated due to the presence of high delay on one of
the measured paths; this parallels the remarks following The-
orem 1.
(d) Low Delay on Unmeasured Path, Some High Delay Else-
where. m1 = 1.0, m2 = 11, m3 = 1.2, m4 = 1.3. The results
are similar to the homogeneous case; the presence of high
delay elsewhere in the network does not further perturb the
delay bound.
If this delay bound estimates are to be used for raising alarms
based on crossing threshold levels, it may be desirable to adjust
alarm thresholds based on the spatial distribution of measured path
delays. Speciﬁcally, case (c) above illustrates that when higher de-
lays are encountered on a path in S−
r , a lower alarm threshold may
be used in order to compensate for the partial “obscuring” of the
delay on the unmeasured path. In situations exempliﬁed by cases
(a) and (b), no adjustment to the threshold is needed, since there are
no measured paths with high delay (so in particular, none in S−
r ).
5. EXPERIMENTAL TESTBED
We implemented a tool to perform multi-objective probing, called
SLAM (SLA Monitor). SLAM sends UDP packets in a one-way
manner between a sender and receiver. It consists of about 2,000
lines of C++, including code to implement the loss, delay, and de-
lay variation probe modules. The implementation is extensible and
can accommodate other discrete-time probe algorithms. In this sec-
tion, we describe the controlled laboratory environment in which
we evaluated SLAM. We considered two topologies, shown in Fig-
ure 3. Each setup consisted of commodity workstation end hosts
and commercial IP routers.
The ﬁrst topology (Figure 3a) was set up in a dumbbell-like con-
ﬁguration. We used 10 workstations on each side of the bottleneck
m1 = 1.0, m2 = 1.1, m3 = 1.2, m4 = 1.3
m1 = 10, m2 = 1.1, m3 = 1.2, m4 = 1.3
m1 = 10, m2 = 11, m3 = 1.2, m4 = 1.3
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
c
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
c
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
bound
actual cdf
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
c
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
bound
actual cdf
bound
actual cdf
 0
 5  10  15  20  25  30  35  40  45  50
 0
 5  10  15  20  25  30  35  40  45  50
 0
 5  10  15  20  25  30  35  40  45  50
delay on path r1 = (e1,e3)
delay on path r1 = (e1,e3)
delay on path r1 = (e1,e3)
2: Example bounds on the inferred delay distribution. (a) Left: homogeneous delay; (b) Center: high delay on unmeasured path; (c) Right:
high delay on unmeasured path and some others.
OC3 for producing background trafﬁc and one workstation at each
side to run SLAM. Background trafﬁc and probe trafﬁc ﬂowed over
separate paths through a Cisco 6500 enterprise router (hop A) and
was multiplexed onto a bottleneck OC3 (155 Mb/s) link at a Cisco
GSR 12000 (hop B). Packets exited the OC3 via another Cisco GSR
12000 (hop C) and passed to receiving hosts via a Cisco 6500 (hop
D). NetPath [5] was used between hops C and D to emulate prop-
agation delays for the background trafﬁc hosts in the testbed. We
used a uniform distribution of delays with a mean of 50 msec, mini-
mum of 20 msec, and maximum of 80 msec. The bottleneck output
queue at the Cisco GSR at hop B was conﬁgured to perform tail
drop with a maximum of about 50 msec of buffer space.
The second topology (Figure 3b) was set up in a star-like con-
ﬁguration. We used 12 hosts on each side of the setup (6 at top,
6 at bottom) to generate trafﬁc over links e1 (OC12–622 Mb/s), e2
(OC48–2.488 Gb/s), e3 (OC3) , and e4 (OC3) making up the star.
An additional host conﬁgured at each corner ran SLAM. Aggre-
gation routers (Cisco 6500’s at hops A and E) were conﬁgured to
direct trafﬁc over four primary conﬁgured paths, r1–r4, as shown
in the ﬁgure. In addition, trafﬁc ﬂowed over path (e1, e2) to create
sufﬁcient load on e1 to include queueing delay and loss. SLAM
probes ﬂowed over the four primary trafﬁc paths to monitor delay,
loss, and delay variation. SLAM was also conﬁgured to monitor
paths (e1, e6), (e2, e6), (e5, e3), and (e5, e4). Only probe trafﬁc tra-
versed links e5 and e6, thus it was assumed that these additional
probe measurements were sufﬁcient to separately measure charac-
teristics on links e1, e2, e3, and e4. As with the dumbbell topol-
ogy, NetPath [5] was used to emulate propagation delays for the
background trafﬁc hosts in the testbed. We used a uniform distri-
bution of delays with a mean of 50 msec, minimum of 20 msec,
and maximum of 80 msec. Each queue was conﬁgured to perform
tail drop. Using the notation (r, e) = B to denote the output queue
at router r on to link e in msec, buffer size conﬁgurations were fol-
lows: (v1, e1) ≈ 25 msec, (v2, e2) ≈ 12.5 msec, (vc, e3) ≈ 50 msec,
and (vc, e4) ≈ 100 msec.
Each workstation used in our experiments had a Pentium 4 pro-
cessor running at 2GHz or better, with at least 1 GB RAM and an
Intel Pro/1000 network interface card and was conﬁgured to run
either FreeBSD 5.4 or Linux 2.6. The SLAM hosts were conﬁg-
ured with a default installation of FreeBSD 5.4. The SLAM work-
stations used a Stratum 0 NTP server conﬁgured with a TrueTime
GPS card for synchronization. We used the software developed by
Corell et al. [20] to provide accurate timestamps for SLAM. All
management trafﬁc for the two topological conﬁgurations ﬂowed
over separate network paths (not pictured in either ﬁgure).
A critical aspect of our laboratory environment is the ability to
measure a reliable basis for comparison for our experiments. For
the dumbbell topology, optical splitters were attached to the links
traffic generator hosts
...
Cisco 6500
GE
Si
GE
Cisco 12000
Cisco 12000
OC3
SLAm host
DAG monitor
traffic generator hosts
Cisco 6500
...
GE
GE
Si
GE
NetPath delay
emulation system
SLAm host
hop identifier
A
B
C
D
(a) Dumbbell topology. Probes and cross trafﬁc are multiplexed
onto a bottleneck OC3 (155Mb/s) link where queueing delay
and loss occurs.
traffic generator hosts
and SLAm host
Cisco 12000
v1
GE
Cisco 6500
Si
GE
GE
GE
GE
Cisco 12000
traffic generator hosts
and SLAm host
v3
e3
OC3
e6
e4
e1
OC12
Cisco 12000
e5
e2
vc
OC48
OC3
GE
GE
GE
GE
primary traffic paths
Cisco 6500
Si
NetPath delay
emulation system
r1=(e1,e3)
r2=(e1,e4)
r3=(e2,e3)
r4=(e2,e4)
GE
DAG monitor
DAG monitor
traffic generator hosts
and SLAm host
v2
v4
Cisco 12000
Cisco 12000
traffic generator hosts
and SLAm host
hop identifier
A
B
C
D
E
(b) Star topology. Probes and cross trafﬁc follow paths r1, r2, r3, and r4,
shown in the ﬁgure.
3: Laboratory testbeds.
between hops A and B and to the link between hops B and C and
synchronized Endace DAG 4.3 (Gigabit Ethernet) and 3.8 (OC3)
passive monitoring cards were used to capture packet traces enter-
ing and leaving the bottleneck node. For the star topology, optical
splitters were attached to the Gigabit ethernet links entering the
core star topology (just after hop A), and exiting the star (just be-
fore hop E). We used synchronized DAG 4.3 passive monitoring
cards to capture packet traces entering and leaving the star setup.
By comparing packet header information, we were able to identify
which packets were lost along each path. Furthermore, these cards
provide synchronization of better than one microsecond allowing
precise delay measurement through the bottleneck router.
We used four background trafﬁc scenarios for experiments using
the dumbbell setup. For the ﬁrst scenario, we used Iperf [38] to
produce constant-bit rate (CBR) UDP trafﬁc for creating a series
of approximately constant duration (about 65 msec) loss episodes,
spaced randomly at exponential intervals with a mean of 10 sec-
onds over a 10 minute period. We found that short loss episodes
were difﬁcult to consistently produce with Iperf, thus the duration
we used was a compromise between a desire for short episodes and
the ability to predictably produce them. The second scenario con-
sisted of 100 long-lived TCP sources run over a 10 minute period.
For the ﬁnal two scenarios, we used Harpoon [34] with a heavy-
tailed ﬁle size distribution to create self-similar trafﬁc approximat-
ing a mix of web-like and peer-to-peer trafﬁc commonly seen in
today’s networks. We used two different offered loads of 60% and
75% of the bottleneck OC3. Since good performance cannot be
guaranteed when resources are oversubscribed, SLAs often contain
clauses to allow discarding performance measurements if utiliza-
tion exceeds a given threshold [33]. Thus, we chose these offered
loads to reﬂect relatively high, yet acceptable average loads in light
of this practice. Experiments using the self-similar trafﬁc scenario
were run for 15 minutes. For all scenarios, we discarded the ﬁrst
and last 30 seconds of the traces.
For the star setup, we used three background trafﬁc scenarios in
our experiments. For the ﬁrst scenario, we used Iperf [38] to pro-
duce CBR UDP trafﬁc over the four primary trafﬁc paths to create
a series of approximately constant duration loss episodes at (vc, e3)
and (vc, e4). We used an additional Iperf ﬂow over path (e1, e2) to
produce a series of loss episodes at (v1, e1). All loss episodes were
spaced at exponential intervals with a mean of 10 seconds, and the
test duration was 10 minutes. The second scenario consisted of
long-lived TCP sources conﬁgured to use all four primary trafﬁc
paths plus path (e1, e2). There were at least 100 trafﬁc sources con-
ﬁgured to use each path, and the test duration was 10 minutes. In
the third scenario, we used Harpoon [34] with a heavy-tailed ﬁle
size distribution to create self-similar trafﬁc as in scenarios three
and four for the dumbbell topology. Trafﬁc sources were conﬁg-
ured to produce approximate average loads of 65% on link e1, 15%
on link e2, 75% on link e3, and 60% on link e4, and the test duration
was 15 minutes. For all scenarios, we discarded the ﬁrst and last 30
seconds of the traces. Finally, we note that while maximum queue-
ing delays at (v2, e2) were non-zero for all three trafﬁc scenarios,
no loss occurred at (v2, e2).
6. EVALUATION
We now describe the experimental evaluation of SLAM using
the testbed described above. We examine the accuracy of SLAM’s
delay and loss estimates, comparing its results with estimates ob-
tained using standard IPPM methodologies [7, 8], which are based
on Poisson-modulated probes. We also compare the DV matrix
metric with other standard methodologies [21, 32].
SLAM Measurement Overhead
6.1
Two important implementation decisions were made in the SLAM
probe sender. First, the scheduler must accommodate estimation
techniques that use multi-packet probes, such as the loss rate esti-
mation method we use. Second, the scheduler must arbitrate among
probe modules that may use different packet sizes. At present, the
smallest packet size scheduled to be sent at a given time slot is used.
An effect of the implementation decision for probe packet sizes
is that the overall bandwidth requirement for the multi-objective
stream is less than the aggregate bandwidth requirement for indi-
vidual probe modules if used separately. One concern with this
implementation decision is the issue of packet size dependence in
the measurement technique. For delay and delay variation, packet
sizes should be small to keep bandwidth requirements low. For de-
lay variation, the packet size should closely match that used by a
codec referred to in the G.107 and related standards so that the E-
model formulas can be directly used [1]. We use 48 bytes at an
interval of 30 msec in our evaluation below, which approximates
the G.723.1 codec. For delay, another concern is the relative differ-
ence between end-to-end transmission and propagation delays. In
situations where propagation delay is large relative to transmission
delay, the packet size can be small since the transmission delays
along a path contribute little to the overall delay. In cases where
the opposite situation holds, packet sizes should be large enough
to estimate delays experienced by packets of average size. In our
evaluation described below, we use 100 byte packets for delay es-
timation. For loss estimation packet sizes, the key consideration
is that multi-packet probes should admit accurate instantaneous in-
dications of congestion.
In previous work [35], a packet size of
600 bytes was used and was found to be a reasonable balance be-
tween limiting measurement impact while still obtaining accurate
congestion indications. We veriﬁed this previous ﬁnding and leave
a detailed analysis for future work.
In the experiments below, we ﬁx SLAM probe parameters as
shown in Table 2. In prior work, ploss = 0.3 was found to give good
loss characteristic estimates [35]. We veriﬁed the results regarding
the setting of the parameter ploss but omit detailed results in this
paper. We experimented with a range of values for pdelay from 0.01
to 0.5 (mean probe intervals from 5 msec to about 500 msec) and
found that estimation accuracy for SLAM is virtually unchanged
over the range of parameter settings except those below about 0.02
(above about 200 msec mean probe spacing). We do not include
detailed results in this paper due to space limitations. For delay
variation, we used a packet size of 48 bytes sent at periodic intervals
of 30 msec. We used a stream length k of 100 probes in computing
the DV matrix metric.
2: SLAM parameters used in evaluation experiments. For all ex-
periments, we set the discrete time interval for the scheduler to be
5 msec.
Loss
Delay