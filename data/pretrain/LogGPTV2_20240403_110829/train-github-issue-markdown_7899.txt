## üêõ Bug
Model I am using (Bert, XLNet....): Bert
Language I am using the model on (English, Chinese....): English
The problem arise when using:
  * the official example scripts: modeling_bert.py
The tasks I am working on is:
  * my own task or dataset: fine-tuning Bert with added new tokens to vocabulary
## To Reproduce
Steps to reproduce the behavior:
Running "run_lm_finetuning.py" with added tokens to vocabulary.
    new_vocab_list = ['token_1', 'token_2', 'token_3']
    tokenizer.add_tokens(new_vocab_list)
    logger.info("vocabulary size after adding: " + str(len(tokenizer)))
    model.resize_token_embeddings(len(tokenizer))
    logger.info("size of model.cls.predictions.bias: " + str(len(model.cls.predictions.bias)))
## Expected behavior
  * The result should be:  
vocabulary size after adding: 31119  
size of model.cls.predictions.bias: 31119
  * But actually the result is:  
vocabulary size after adding: 31119  
size of model.cls.predictions.bias: 31116
## Environment
  * OS: Ubuntu
  * Python version: 3.6
  * PyTorch version: 1.3.1
  * PyTorch Transformers version (or branch): 2.2.1
  * Using GPU: yes
  * Distributed or parallel setup: no
## Additional context
I have found the problem to be: for BERT model, the class
"BertLMPredictionHead" has two separate attributes "decoder" and "bias". When
adding new tokens, the code "model.resize_token_embeddings(len(tokenizer))"
only updates the size of "decoder" and its bias if it has (this bias is
different from the "BertLMPredictionHead.bias"). The attribute
"BertLMPredictionHead.bias" is not updated and therefore, causes the error.
I have added the updating-bias code in my "modeling_bert.py". And if you want,
I can merge my branch to your code. However, if I misunderstand something,
please notice me too.
Thank you very much for your code base.