⎡
⎣gx
gy
gz
(4)
3) we then use extended Kalman ﬁlter (EKF) to perform
sensor data fusion, which is widely used for state estimation
and tracking due to its robustness in nonlinear dynamic en-
vironments [52]. The EKF method takes time-varying drift
into account via deﬁning an error metric and updating covari-
ance metric iteratively to minimize this error. Speciﬁcally, the
system state vector xxx of EKF in our work is given as Eq 5.
xxx = [qqqT ,wwwT ]T = [q0,q1,q2,q3,
∂(φ)
∂(t)
,
∂(θ)
∂(t)
,
∂(ψ)
∂(t)
]T
(5)
the
denotes
transpose
∂(t) , ∂(ψ)
operator, wwwT =
where
T
[ ∂(θ)
∂(t) , ∂(ψ)
∂(t) ], which are estimated values with Eq. 4.
qqq is the quaternion (four-element vector), which can be
acquired based on the relationship between Euler Angles and
quaternion as shown in Eq. 6.
φc
2 cos θc
φc
2 cos θc
φc
2 sin θc
φc
2 cos θc
ψc
2 cos
2
ψc
2 cos
2
ψc
2 cos
2
ψc
2 sin
2
φc
2 sin θc
φc
2 sin θc
φc
2 cos θc
φc
2 sin θc
⎤
⎥⎥⎥⎦ (6)
⎡
⎢⎢⎢⎣
cos
sin
cos
cos
+ sin
− cos
+ sin
− sin
2 sin
2 sin
2 sin
2 cos
⎤
⎥⎥⎦ =
ψc
2
ψc
2
ψc
2
ψc
2
q0
q1
q2
q3
⎡
⎢⎢⎣
qqq =
where φc, θc, and ψc are estimated with the fusion of both
accelerometer and magnetometer based on Eq. 1, 2, and 3.
q1, q2, q3, q4 are elements of the unit quaternion.
USENIX Association
29th USENIX Security Symposium    2221
Table 1: Time- and frequency-domain features and their normalized ﬁsher’s scores.
Domain
Feature
Description
e
m
T
i
y
c
n
e
u
q
e
r
F
Mean
Standard deviation
Relative standard deviation
Sum of absolute differences
Absolute energy
Autocorrelation
Spectral centroid
Spectral spread
Spectral skewness
Spectral kurtosis
Power spectral density
Spectral entropy
The mean of the time series.
The standard deviation of the time series.
The extent of variability in relation to its mean.
The sum over the absolute value of consecutive changes in
the time series.
The absolute energy of the time series.
The autocorrelation of the time series.
The center of mass of the spectrum is located.
The average spread of the spectrum in relation to its cen-
troid.
The measurement of the asymmetry of the probability dis-
tribution of a real-valued random variable about its mean.
The shape of a probability distribution.
Average of distribution of power into frequency compo-
nents.
The complexity of the signal in the frequency domain.
(cid:3),φφφ,θθθ,ψψψ)
Normalized Fisher Score of
(aaax,aaay,aaaz,aaa
(0.45,0.01,0.22,0.68,0.86,0.84,0.84)
(0.24,0.56,0.31,0.41,0.58,0.32,0.74)
(0.34,0.15,0.12,0.56,0.71,0.64,0.82)
(0.32,0.27,0.72,0.52,0.53,0.72,0.78)
(0.63,0.98,0.85,0.57,0.72,0.57,0.37)
(0.00,0.14,0.15,0.21,0.94,0.62,0.64)
(0.34,0.21,0.38,0.12,0.78, 0.98,0.78)
(0.66,0.36,0.32,0.78,0.46,0.82,0.96)
(0.85,0.45,0.58,0.84,0.56,0.85,1.00)
(0.34,0.17,0.70,0.86,0.62,0.51,0.42)
(0.90,0.71,0.86,0.26,0.85,0.68,0.82)
(0.94,0.32,0.82,0.21,0.96,0.82,0.89)
We compute accurate quaternions, where the detailed steps
are presented in Appendix B due to the page limit. Finally,
rotation angles can be computed based on Eq. 7.
⎧⎪⎨
⎪⎩
arctan ( 2q2q3+2q0q1
−1
γ =
θ = −arcsin (2q1q3 − 2q0q2)
ψ =
+2q2
3
2q2
0
arctan ( 2q1q2+2q0q3
−1
+2q2
1
2q2
0
)
)
(7)
The outcome of characterizing ﬁngertip-touch behaviors is
(cid:3),φφφ,θθθ,ψψψ), where each of element
represented as (aaax,aaay,aaaz,aaa
is an n-dimensional vector.
4 Feature Extraction
We present two methods to extract discriminative features
from ﬁngertip-touch behaviors.
4.1 Time- and Frequency-domain Features
We extract features in the time- and frequency-domain from
(cid:3),φφφ,θθθ,ψψψ). As shown in Table 1, we extract six sta-
(aaax,aaay,aaaz,aaa
tistical features in the time domain, including mean, standard
deviation, relative standard deviation, sum of absolute differ-
ences, absolute energy, and autocorrelation. In addition, we
apply fast Fourier transform and extract another six features
in the frequency domain. These features include spectral cen-
troid, spread, skewness, kurtosis, power density, and entropy.
These time- and frequency-domain features are widely used
for time series analysis [24, 44, 46].
Selected Features. We computed the Fisher’s scores [35] for
all aforementioned 84 features with 45,000 data points col-
lected from 90 users to select the most discriminative features.
As the results show in Table 1, the features from rotation an-
gle have higher Fisher’s score than features from acceleration.
Features with a normalized Fisher’s score higher than 0.6 are
selected. The output of features extraction and selection in
time and time-domain is a 43-dimensional feature vector.
4.2 CNN-based Feature Learning
Besides the extracted time- and frequency-domain features,
we also resort to CNN-based feature learning. To this end, we
ﬁrst apply STFT and convert the time series data (e,g., aaax) to
a two-dimensional power spectral density matrix. Then, we
concatenate these matrices and rely on CNN models to extract
features from them. Figure 3 shows three users’ spectrograms
(cid:3), θθθ, φφφ, ψψψ, which are visual representations
from aaax, aaay, aaaz, aaa
of power spectral density matrices.
The basic idea of feature learning with CNN is to lever-
age the output of the model’s intermediate layer as features
thanks to the powerful feature representation of deep learning
method [13, 67]. In particular, we train the CNN model to
distinguish different users with collected FINAUTH data, and
employ the ﬁrst k layers of the trained model as the feature
extractor. Even though the model is trained from a limited
dataset, it can be used to extract generalized features because
of the feature learning ability of CNN, which is also known
as transfer learning [77].
Table 2 shows the structure of our used CNN model. We
use leaky rectiﬁed linear units (Leaky ReLu) as the activation
functions for two-dimensional convolution (Conv2d) layers
and fully connected (FC) layers, since it can tackle the van-
ishing gradient problem during the model training phase [50].
For the pooling layers, we use the max-pooling method to
down-sample the input, which controls over-ﬁtting and saves
computational costs by reducing the number of parameters
for training. To avoid over-ﬁtting, we add dropout layers af-
ter each pooling layer. Furthermore, we also consider batch
normalization (BN) layers to normalize the output of the pre-
vious layer, which accelerates model training and increases
2222    29th USENIX Security Symposium
USENIX Association
(a) User A.
(b) User B.
(c) User C.
Figure 3: Characterized ﬁngertip-touch behaviors of three users under STFT. From left to right, spectrograms of aaax, aaay, aaaz, aaa
φφφ, ψψψ.
(cid:3), θθθ,
the stability of the model. The softmax layer is added as the
last layer for prediction, which outputs the categorical proba-
bility distribution of each class. Speciﬁcally, the kernel size
of Conv2d and pooling layers is set as 3× 3 and 2× 2 respec-
tively, because of their better non-linear feature representation
gaining popularity in start-of-art models [36, 38, 67]. The de-
tailed output shape and the number of parameters of each layer
are given as Table 2. The total model contains 202,974 pa-
rameters, including 202,438 trainable and 536 non-trainable
parameters.
5 Authentication With One-class Classiﬁers
In real-world ﬁngerprint authentication settings, the training
dataset only contains the legitimate user’s data points. There-
fore, it is a one-class classiﬁcation problem. We use four
methods to proﬁle the legitimate user: i) Pearson correlation
coefﬁcient-based similarity comparison (PCC), ii) one-class
support vector machine (OC-SVM), iii) local outlier factor
(LOF), and iv) isolation forest (IF).
PCC is a similarity metric to measure the linear correla-
tion between two variables. The coefﬁcient is between +1
and -1, where +1/-1 denotes a total positive or negative linear
Table 2: The structure of base CNN model.
# Layer
Layer Type
1
2
3
4
5
6
7
8
9
10
11
12
Conv2d + LeakyReLu
Conv2d + LeakyReLu
Pooling + Dropout +BN
Conv2d + LeakyReLu
Conv2d + LeakyReLu
Pooling + Dropout +BN
Conv2d + LeakyReLu
Conv2d + LeakyReLu
Pooling + Dropout +BN
Flatten
FC+LeakyReLu
FC+ Softmax
Output Shape
62× 126× 24
60× 124× 24
30× 62× 24
28× 60× 48
26× 58× 48
13× 29× 48
11× 27× 16
9× 25× 16
4× 12× 16
768
180
90
# Para
1,536
5,208
96
10,416
20,784
192
6,928
2,320
64
0
139,140
16,290
correlation, and 0 represents none linear correlation. Specif-
ically, after feature extraction, we compute the mean PCC
between the extracted feature vector and ﬁngertip-touch tem-
plates (i.e., saved feature vector during the register phase).
The computed mean PCC is then used to decide whether the
user is authorized.
OC-SVM, an extended algorithm of SVM, maps data points
into high-dimensional feature space with the kernel func-
USENIX Association
29th USENIX Security Symposium    2223
Table 3: Summary of the compiled datasets
Dataset Week of Collection
1
1 †, 8 and 9 ‡