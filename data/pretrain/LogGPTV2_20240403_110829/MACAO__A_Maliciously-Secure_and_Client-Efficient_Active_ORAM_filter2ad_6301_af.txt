Circuit-ORAM
Πprf
rss
Path-ORAM
S3ORAM
Πspdz
Ring-ORAM
Πrss
Πprf
spdz
Circuit-ORAM
Πprf
rss
Path-ORAM
S3ORAM
Πspdz
Ring-ORAM
40
35
30
25
20
15
10
5
)
c
e
s
(
y
a
l
e
D
26
28
210
0
20
22
24
26
28
210
22
24
|DB| (GB)
(a) Block size |b|= 4 KB
|DB| (GB)
(b) Block size |b|= 256 KB
Fig. 13: End-to-end delay of MACAO schemes and their counterparts.
3) Overall Result: We present in Figure 13 the end-to-end
delay of MACAO schemes compared with selected counter-
parts with 4 KB and 255 KB block sizes and database sizes
from 1 GB to 1 TB. In the home network setting, all MACAO
schemes outperformed Path-ORAM and Circuit-ORAM in all
testing cases, especially when the block size was large (i.e.,
256 KB). Speciﬁcally, Path-ORAM and Circuit-ORAM took
369 ms to 650 ms and 625 ms to 1.2 s to access a 4 KB
block, respectively, whereas MACAO schemes took 198 ms to
336 ms. All MACAO schemes (except Πrss) were also faster
than Ring-ORAM for 4 KB block access. For 256 KB block
access, the performance gap between MACAO and single-
server ORAM schemes signiﬁcantly increased since MACAO
featured the constant client-bandwidth blowup. In particular,
Path-ORAM, Circuit-ORAM and Ring-ORAM took 16 s to
32 s, 17 s to 34 s and 12 s to 24 s, respectively, for each
256 KB-block access, whereas MACAO schemes only took
3.3 s to 5.5 s. This resulted in MACAO being up to 7× faster
than single-server ORAM schemes.
On the other hand, the performance of MACAO schemes
was comparable to S3ORAM, where S3ORAM took 312 ms
to 451 ms per 4 KB-block access, and 1.78 s to 3.11 s per
256 KB-block access, respectively. Πspdz scheme was faster
than S3ORAM for 4 KB-block access since it operated on two
servers (vs. 3 in S3ORAM) with small amount of data, and
the retrieval phase of MACAO incurred less number of blocks
to be computed than S3ORAM (O(log N ) vs. O(log2 N )).
We notice that Πspdz operates on the preprocessing model,
where their online access operation performance depends on
the availability of authenticated matrix multiplication shares
computed in the ofﬂine phase. For 256 KB-block access,
S3ORAM was approximately 1.5 times faster than MACAO
schemes. This is mainly because MACAO schemes perform the
computation on the information-theoretic MAC components,
whose size is equal to the block size. Notice that S3ORAM
does not have the MAC and it does not offer integrity and
security against the malicious adversary.
One might also observe from Figure 13 that the bandwidth
reduction trick in §IV-E1 signiﬁcantly lowered the end-to-
rss and Πprf
end delays of MACAO schemes (denoted as Πprf
spdz
schemes). This trick allowed us to reduce the performance gap
between the MACAO schemes using RSS and SPDZ when
the amount of data to be transmitted was large as in the
256 KB-block access. The price to pay for such efﬁciency
is the reduction from information-theoretic to computational
security. To aid more understanding, we provide the detailed
cost of MACAO schemes in the following section.
4) Cost Breakdown: We decomposed the delay of MACAO
schemes to investigate cost factors that impact the perfor-
mance. As shown in Figure 14, there were four main sources
of delay including client processing, server processing, client-
server communication and server-server communication.
Client-side processing. MACAO schemes incurred very low
computation at the client-side thereby, making them the ideal
choice for resource-limited clients such as mobile devices. The
client main task was to generate shares of the retrieval query
and permutation matrices for eviction by invoking pseudo/true-
random number generator. The client recovered the accessed
block and veriﬁed its integrity by performing some modular
additions and multiplications. All these operations are very
lightweight, all of which cost less than 4 ms and 40 ms for
4 KB and 256 KB block size on 1 TB database, respectively.
Disk I/O access. We disabled default caching mechanisms
[52] to minimize the impact of random access sequence on the
I/O latency. The disk access contributed a small amount to the
delay of MACAO schemes due to the following reasons. The
MACAO structure was stored on a network-based storage unit
called EBS with 2.1 Gbps throughput. Meanwhile, the amount
of data to be loaded per retrieval was small, which was only
4|b|(H + 1) KB, where |b| ∈ {4, 256} and H ∈ {11, . . . , 27}
for up to 1 TB of outsourced data. In MACAO schemes, the
disk I/O access only impacted the retrieval, but not eviction.
This is because MACAO schemes follow the deterministic
eviction, where the data along the eviction path can be pre-
loaded into the memory before the push-down operation.
Hence, the data can be read directly from the cache if needed,
given that they were processed in the previous operations but
have not been written to the disk yet.
Server computation. This contributed a large portion to the
total delay, mostly due to the matrix multiplication in the
eviction phase. The server computation in Πrss was higher than
in Πspdz since it incurred more number of additions than Πspdz
for each homomorphic multiplication.
Client-server communication. MACAO schemes feature a
11
)
B
G
(
|
B
D
|
spdz
rss
210
spdz
rss
spdz
rss
28
26
spdz
rss
24
22
spdz
rss
20
spdz
rss
0
100
)
B
G
(
|
B
D
|
spdz
rss
210
spdz
rss
spdz
rss
28
26
spdz
rss
24
22
spdz
rss
20
spdz
rss
400
500
0
1,000 2,000 3,000 4,000 5,000 6,000 7,000 8,000
Client-server Communication
Inter-server Communication
Delay (ms)
Inter-server Communication Saved by Reduced Bandwidth Trick
(b) |b|= 256 KB
Client Computation
Client-server Communication Saved by Reduced Bandwidth Trick
Server Computation
200
300
Delay (ms)
Server Disk I/O
(a) |b|= 4 KB
Fig. 14: Cost breakdown of MACAO schemes.
constant client-bandwidth blowup similar to S3ORAM. There-
fore, only the query size and the eviction matrix size increased
when the database size increased while the number of data
blocks to be transmitted remained the same. Therefore, al-
though it was one of the most signiﬁcant factors contributing
to the total delay, the client-server communication cost of
MACAO schemes was likely to remain the same when in-
creasing the database size as shown in Figure 14, where most
of the time was spent to download/upload a constant number
authenticated shares of the data blocks. Πspdz incurred less
client-server communication delay than Πrss because it only
needs two servers, instead of three. Figure 14 also shows that
the bandwidth reduction trick signiﬁcantly reduced the client
communication delay (the green bar with red ﬁlled pattern).
This trick allows the client to send the authenticated share
to only one server, thereby making the client-communication
overhead of Πrss and Πspdz schemes almost the same.
Server-server communication.
is the second smallest
portion of the total delay. We can also see that the bandwidth
reduction trick also helped to reduce the server-server com-
munication in Πrss scheme (the yellow bar with red pattern in
Figure 14). The Πrss scheme had higher inter-server communi-
cation delay than Πspdz since three servers communicated with
each other, compared with only two in Πspdz.
Storage overhead. MACAO schemes harness the eviction
strategy in [66] so that they incur a constant server storage
blowup. In Πrss, every server stores two authenticated shares
of the ORAM tree so that storage overhead per server is 8|DB|
(i.e., 4 times blowup from [66]). On the other hand, every
server in Πspdz stores one authenticated shared ORAM tree,
and therefore, the server storage overhead is 4|DB| (2 times
blowup from [66]). Regarding the client storage, Πrss schemes
add an extra of O(N log log N ) bits to the storage overhead of
[66], which is analytically |N| log2 N + log2(log2 N ) + 80|b|
in total. Empirically, with 1 TB DB and 256 KB block size,
the client storage overhead of Πrss, Πspdz is 33.23 MB. With
1 TB DB and 4 KB block size, it is 1.33 GB.
It
5) MACAO Performance with Varying Privacy Levels: We
conducted an experiment to evaluate the performance of Πspdz
and Πprf
spdz schemes under higher privacy levels by increasing
the number of servers. Due to RSS, we did not evaluate Πrss
and Πprf
rss since they incur signiﬁcant server storage for high
privacy levels. Figure 15 outlines the delay of Πspdz and Πprf
spdz
Πspdz (4 KB)
Πspdz (256 KB)
Πprf
Πprf
spdz (4 KB)
spdz (256 KB)
)
c
e
s
(
y
a
l
e
D
12
10
8
6
4
2
0
1
2
3
4
5
Privacy level (t)
Fig. 15: End-to-end delay with varied privacy levels.
scheme under 1 TB database with 4 KB and 256 KB block
sizes. When increasing the number of servers, Πprf
spdz incurred
much less delay than Πspdz, especially in the 256 KB block
size setting, since the client only sent data to one server while
the other servers generated authenticated shares on their own.
VI. RELATED WORKS
Goldreich was the ﬁrst to propose the ORAM concept for