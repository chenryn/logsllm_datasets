in C and leverage
Setup. We implement ReluVal
OpenBLAS1 to enable efﬁcient matrix multiplications. We
evaluate ReluVal on a Linux server running Ubuntu 16.04
with 16 CPU cores and 256GB memory.
Parallelization. One unique advantage of ReluVal over
other security property checking systems like Reluplex is
that the interval arithmetic in the setting of verifying DNN
is highly parallelizable by nature. During the process of
iterative interval reﬁnement, newly created input ranges
during iterative reﬁnement can be checked independently.
This feature allows us to create as many threads as pos-
sible, each taking care of a speciﬁc input range, to gain
signiﬁcant speedup by distributing different input ranges
to different workers.
However, there are two key challenges that required
solving to fully leverage the beneﬁts of parallelization.
First, as shown in Section 5.2, the bisection tree is often
not balanced leading to substantially different running
times for different threads. We found that often several
laggard threads slow down the computation, i.e., most of
the available workers stay idle while only a few workers
keep on reﬁning the intervals. Second, as it is hard to
predict the depth of the bisection tree for any sub-interval
in advance, starting a new thread for each sub-interval
may result in high scheduling overhead. To solve these
two problems, we develop a dynamic thread rebalancing
algorithm that can identify the potentially deeper parts of
the bisection tree and efﬁciently redistribute those parts
among other workers.
Outward rounding. The large number of ﬂoating ma-
trix multiplications in a DNN can potentially lead to se-
1http://www.openblas.net/
Figure 5: A bisection tree with split depth of n. Each
node represents a bisected sub-interval.
Algorithm 2 Backward propagation for gradient interval
Inputs:
network ← tested neural network
R ← gradient mask
1: // initialize upper and lower gradient bounds
2: gup = glow = weights[lastLayer];
3: for layer = numlayer-1 to 1 do
for 1 to layerSize[layer] do
4:
5:
6:
7:
8:
9:
10: return g;
// g is an interval containing gup and glow
// interval hadamard product
g=R[layer]
// interval matrix multiplication
g=weights[layer]
(cid:4)
g;
(cid:5)
g;
Jacobian matrix is completely determined by the weight
parameters, which is independent of the input. A ReLU
node’s gradient can either be 0 for negative input or 1 for
positive input. We use intervals to track and propagate
the bounds on the gradients of the ReLU nodes during
backward propagation as shown in Algorithm 2.
We further use the estimated gradient interval to com-
pute the smear function for an input feature [26, 27]:
Si(X) = max1≤ j≤d|Ji j|w(Xj), where Ji j denotes the gradi-
ent of input Xj for output Yi. For each reﬁnement step, we
bisect the Xj with the highest smear value to reduce the
over-approximation error as shown in Algorithm 3.
(2) Monotonicity. Computing the Jacobian matrix also
helps us to reason about the monotonicity property of the
output for a given input interval. In particular, for the
cases where the partial derivative of ∂ Fi
is always positive
Xj
or negative for the given the input interval X, we can
simply replace the interval Xj with two concrete value
Xj and Xj. Because, as the DNN output is monotonic in
that input interval, it is impossible for any intermediate
value to cause a violation without either Xj or Xj cause a
USENIX Association
27th USENIX Security Symposium    1607
vere precision drops after rounding [15]. For example,
assume that the output of one neuron is [0.00000001,
0.00000002]. If the ﬂoating-point precision is e− 7, then
it is automatically rounded up to [0.0,0.0]. After one
layer propagation with a weight parameter of 1000, the
correct output should be [0.00001, 0.00002]. However,
after rounding, the output will incorrectly become [0.0,
0.0]. As the interval propagates through the neural net-
work, more errors will accumulate and signiﬁcantly affect
the output precision. In fact, our tests show that some
adversarial examples reported by Reluplex [25] are false
positives due to such rounding problem.
To avoid such issues, we adopt outward rounding in
ReluVal. In particular, for every newly calculated interval
or symbolic intervals [x,x], we always round the bounds
outward to ensure the computed output range is always
a sound overestimation of the true output range. We
implement outward rounding with 32-bit ﬂoats. We ﬁnd
that this precision is enough for verifying properties of
ACAS Xu models, though it can easily be extended to
64-bit double.
7 Evaluation
7.1 Evaluation Setup
In the evaluation, we consider two general categories of
DNNs, deployed for handling two different tasks.
The ﬁrst category is airborne collision avoidance sys-
tem (ACAS) crucial for alerting and preventing the colli-
sions between aircrafts. We focus our evaluation on the
ACAS Xu model for collision avoidance in unmanned
aircrafts [28].
The second category includes the models deployed to
recognize hand-written digit from the MNIST dataset.
Our preliminary results demonstrate that ReluVal can also
scale to larger networks that the solver-based veriﬁcation
tools often struggle to check.
ACAS Xu. The ACAS Xu system consists of forty-ﬁve
different NN models. Each network is composed of an
input layer taking ﬁve inputs, an output layer generating
ﬁve outputs, and six hidden layers with each containing
ﬁfty neurons. As shown in Figure 6, ﬁve inputs include
{ρ,θ ,ψ,vown,vint}. In particular, ρ denotes the distance
between ownship and intruder, θ denotes the heading
direction angle of ownship relative to the intruder, ψ
denotes the heading direction angle of the intruder relative
to ownship, vown is the speed of ownship, and vint is the
speed of intruder. Output of the NN includes {COC, weak
left, weak right, strong left, strong right}. COC denotes
clear of conﬂicts, weak left means heading left with angle
1.5o/s, weak right means heading right with angle 1.5o/s,
strong left is heading left with angle 3.0o/s, and strong
right denotes heading right with angle 3.0o/s. Each output
in NN corresponds to the score for this action (minimal
for the best).
Figure 6: Horizontal view of ACAS Xu operating scenarios.
MNIST. For classifying hand-written digits, we test a
neural network with 784 inputs, 10 outputs and two hid-
den layers. Each intermediate layer has 512 neurons. On
the MNIST test data set, it can achieve 98.28% accuracy
for classiﬁcation.
7.2 Performance on ACAS Xu Models
In this section, we ﬁrst present a detailed comparison of
ReluVal and Reluplex in terms of the veriﬁcation perfor-
mance. Then, we compare ReluVal with a state-of-the-art
adversarial attack on DNNs, Carlini-Wagner [7], showing
that on average ReluVal can consistently ﬁnd 50% more
adversarial examples. Finally, we show that ReluVal can
accurately narrow down all possible adversarial ranges
and therefore provide more insights on the distribution of
adversarial corner-cases.
Comparison to Reluplex. Table 1 compares the time
taken by ReluVal with that of Reluplex for verifying ten
original properties described in their paper [25]. In ad-
dition, we include the experimental results for ﬁve new
security properties. The detailed description of each prop-
erty is in the Appendix. Table 1 shows that ReluVal
always outperforms Reluplex at checking all ﬁfteen se-
curity properties. For the properties on which Reluplex
times out, ReluVal is able to terminate in signiﬁcantly
shorter time. On average, ReluVal achieves up to 200×
speedup over ReluPlex.
Finding adversarial inputs. In terms of the number of
adversarial examples detected, ReluVal also outperforms
the popular attacks using gradients to ﬁnd adversarial
examples. Here, we compare ReluVal to the Carlini and
Wagner (CW) attack [7], a state-of-the-art gradient-based
attack that minimizes specialized CW loss function.
As gradient-based attacks start from a seed input and
iteratively looking for adversarial examples, the choice
of seeds may highly inﬂuence the success of the attack
at ﬁnding adversarial inputs. Therefore, we try differ-
1608    27th USENIX Security Symposium
USENIX Association
Source
Properties
Networks
Security
Properties
from [25]
Reluplex Time (sec)
>443,560.73*
123,420.40
35,040.28
13,919.51
23,212.52
220,330.82
>86400.0*
43,200.01
116,441.97
23,683.07
4,394.91
2,556.28
>172,800.0*
>172,810.86*
31,328.26
* Reluplex use different timeout thresholds for different properties.
45
34∗2
42
42
1
1
1
1
1
1
1
1
1
2
2
φ1
φ2
φ3
φ4
φ5
φ6
φ7
φ8
φ9
φ10
φ11
φ12
φ13
φ14
φ15
Additional
Security
Properties
ReluVal Time (sec)
14,603.27
117,243.26
19,018.90
441.97
216.88
46.59
9,240.29
40.41
15,639.52
10.94
27.89
0.104
148.21
288.98
876.80
Speedup
>30×
1×
2×
32×
107×
4729×
>9×
1069×
7×
2165×
158×
24580×
>1166×
>598×
36×
Table 1: ReluVal’s performance at verifying properties of ACAS Xu compared with Reluplex. φ1 to φ10 are the
properties proposed in Reluplex [25]. φ11 to φ15 are our additional properties.
# Seeds
50
40
30
20
10
CW CW Miss
24/40
21/40
17/40
10/40
6/40
40.0%
47.5%
58.5%
75.0%
85.0%
ReluVal
40/40
40/40
40/40
40/40
40/40
ReluVal Miss
0%
0%
0%
0%
0%
Table 2: The number of adversarial inputs CW can ﬁnd
compared to ReluVal on 40 adversarial ACAS Xu proper-
ties. The third column shows the percentage of adversarial
properties CW failed to ﬁnd.
P
S1
S2
S3