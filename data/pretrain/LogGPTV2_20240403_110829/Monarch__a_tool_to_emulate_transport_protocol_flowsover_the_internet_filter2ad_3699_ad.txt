cause is unstationarity in the network, and not a problem
with the Monarch emulation.
estimates between successive TCP measurements along the
same paths. This points to the eﬃciency of our Monarch
implementation; despite the additional packet processing
overhead in our interposing proxy, we add negligible over-
head to the packet latencies.
Second, queueing delays
show a much larger variation or unstationarity over time
compared to minimum and maximum RTTs. The reason
for these large relative diﬀerences is that the absolute values
are very low. Over 76% of queueing delay estimates are
below 10 milliseconds. Hence, even a small 1-millisecond
variation corresponds to a 10% diﬀerence.
Packet loss: Finally, we investigate the loss rates in the
ﬂows. We note that both Monarch and TCP senders re-
transmit packets that they perceive to be lost, which might
be diﬀerent from the packets that were actually lost. For
example, TCP might mistake massive packet reordering for
a loss and trigger a retransmission. Our interest here is in
the perceived loss rates of these ﬂows, so we use the packet
retransmission rate for loss rate.
Figure 10 shows cumulative distributions of retransmis-
sion rates for both Monarch and TCP ﬂows. 75% of all
Monarch ﬂows and 88% of all TCP ﬂows do not contain any
retransmissions and therefore do not perceive packet loss.
Thus, packet retransmissions do not aﬀect a majority of
both Monarch and TCP ﬂows. Of the ﬂows that do contain
retransmissions, Monarch shows a clearly higher retransmis-
sion rate than TCP. This is expected because Monarch ﬂows
s
w
o
l
f
f
o
n
o
i
t
c
a
r
F
1
0.8
0.6
0.4
0.2
0
TCP
Monarch
0%
1%
2%
3%
4%
5%
Packet retransmissions
Figure 10: Retransmissions per ﬂow for Monarch and
TCP: Monarch shows more retransmissions because it must
retransmit packets for losses in both upstream and down-
stream directions, while TCP needs to retransmit only pack-
ets lost on the downstream.
must retransmit packets for losses in both upstream and
downstream directions, while TCP needs to retransmit only
packets lost on the downstream, due to cumulative acknowl-
edgments.
Summary: Our analysis shows that Monarch can accu-
rately emulate TCP ﬂows with respect to ﬂow-level proper-
ties such as throughput, latency, and queueing delay. How-
ever, Monarch’s inability to distinguish between upstream
and downstream packet loss causes it to over-estimate packet
loss. The impact of this inaccuracy is limited to the small
fraction of ﬂows that see upstream packet loss.
4.3 Reliability of self-diagnosis
In the previous section, we showed that the primary source
of inaccuracy in a Monarch emulation is upstream packet
loss. In this section, our goal is to show that Monarch’s self-
diagnosis feature (Section 3.3) can reliably detect upstream
packet loss, and thus, warn the user of potential inaccura-
cies.
We tested this feature on the Monarch ﬂows in our Plan-
etLab trace. For each ﬂow, we compared the tcpdump traces
from the sender and the receiver to determine how many
packets had actually been lost on the downstream and the
upstream. Then we compared the results to the output of
Monarch’s self-diagnosis for that ﬂow; recall that this uses
only the sender-side trace.
s
w
o
l
f
f
o
n
o
i
t
c
a
r
F
1
0.96
0.92
0.88
0.84
0.8
Unkown (inferred)
Upstream (measured)
Upstream (inferred)
Downstream (measured)
Downstream (inferred)
Total (inferred, measured)
0% 1% 2% 3% 4% 5% 6% 7% 8% 9% 10%
Loss rate
Figure 11: Self-diagnosis is accurate: The number of
downstream and upstream losses inferred by Monarch’s self-
diagnosis matches the actual measurements. Only a small
number of losses cannot be classiﬁed as either downstream
or upstream.
Result
Broadband
Router
Confirmed
Inaccuracte
Indeterminate
Traces total
13,168
1,130
1,344
15,642
84.2 %
7.2 %
8.6 %
100.0 %
2,317
164
295
2,776
83.5 %
5.9 %
10.6 %
100.0 %
Table 5: Monarch is accurate over real Internet paths:
In the majority of the ﬂows, self-diagnosis did not detect
any inaccuracies. Note that even when an inaccuracy is
reported, its impact on ﬂow-level metrics such as throughput
may be quite small.
Figure 11 shows the results. Self-diagnosis could not dis-
tinguish between all upstream and downstream losses (see
Section 3.3) for a very small number of ﬂows (less than 2%).
In these cases, Monarch printed a warning. For the majority
of ﬂows for which self-diagnosis could infer the loss rates, the
measured and the inferred loss rates match extremely well
in both upstream and downstream directions. As expected,
the total loss rate plots are identical.
We conclude that Monarch’s self-diagnosis can reliably
detect the major source of inaccuracy in an emulated ﬂow.
4.4 Accuracy over the Internet at large
In the previous two sections, we showed that upstream
loss is the most important source of inaccuracies in Monarch
emulations, and that Monarch’s self-diagnosis can reliably
detect the presence of upstream loss. Our goal in this section
is to show that upstream losses are rare even when Monarch
is used over real Internet paths.
We ran Monarch’s self-diagnosis over our two Internet
traces; the ﬁrst trace consists of 15, 642 ﬂows to 4, 805 broad-
band hosts, and the second trace contains 2, 776 ﬂows to 697
Internet routers. Table 5 summarizes our results. About
10% of the traces could not be analyzed by Monarch. In the
broadband dataset, 7.1% of the traces did not contain usable
IPIDs (8.3% in the router dataset), and 1.5% (2.3%) con-
tained a loss that could not be classiﬁed as either upstream
or downstream. In either of these cases, self-diagnosis was
aborted immediately.
Overall, 84.2% of the broadband traces and 83.5% of the
router traces were conﬁrmed by self-diagnosis because nei-
ther upstream losses nor signiﬁcant reordering errors were
detected. This includes the 15.8% (24.9%) of the traces that
contained only minor reordering errors that would not have
changed the number of duplicate ACKs, and therefore would
not have aﬀected any packet transmissions. Only 7.2% of the
broadband traces were reported as inaccurate; for the router
traces, the fraction was only 5.9%.
We conclude that a majority of our ﬂows to Internet hosts
did not suﬀer from upstream packet loss or signiﬁcant re-
ordering, the two primary sources of inaccuracy in Monarch.
This suggests that Monarch can be used to accurately em-
ulate TCP ﬂows to a large number of Internet hosts. More-
over, our results show that the IPID-based self-diagnosis is
applicable in most cases.
4.5 Summary
In this section, we showed that Monarch is accurate: its
emulated TCP ﬂows behave similarly to real TCP ﬂows with
respect to both packet-level and ﬂow-level metrics. We also
showed that the most important source of error in Monarch’s
ﬂows is upstream packet loss, and that this can be reliably
detected by Monarch’s built-in self-diagnosis. Further, our
examination of large sets of Monarch ﬂows to various Inter-
net hosts, including hundreds of routers and thousands of
broadband hosts, revealed that less than 10% of these ﬂows
suﬀer from upstream packet loss. From this, we conclude
that Monarch can accurately emulate TCP ﬂows to a large
number of Internet hosts.
5. APPLICATIONS
Monarch’s ability to evaluate transport protocol designs
over large portions of the Internet enables new measurement
studies and applications. We used Monarch to conduct three
diﬀerent types of measurement experiments. In this section,
we describe these experiments and present some preliminary
results from them to illustrate their potential beneﬁts.
5.1 Evaluating different transport protocols
New transport protocol designs [3, 10, 47, 49] continue to
be proposed as the Internet and its workloads change over
time. However, even extensive simulation-based evaluations
face skepticism whether their results would translate to the
real world. The resulting uncertainty around how well these
protocols would compete with existing deployed protocols
hinders their actual deployment. With Monarch, researchers
can evaluate their new protocol designs over actual Internet
paths.
We used Monarch to compare three diﬀerent TCP conges-
tion control algorithms implemented 4 in the Linux 2.6.16.11
kernel: NewReno [12], BIC [49], and Vegas [8].
In our
experiment, we emulated 500kB data transfers from a lo-
cal machine to several hosts in broadband (cable and DSL)
ISPs, using each of the three congestion control algorithms
in turn.5 We examined the traces generated by Monarch for
diﬀerences in protocol behavior.
Figure 12 shows the diﬀerence between the algorithms
over a single, but typical path. The graphs show how the
congestion window (CWND) and the round-trip time (RTT)
4Note that the Linux implementation of TCP protocols may
diﬀer signiﬁcantly from their reference implementation or
their standard speciﬁcation.
5In Linux 2.6 kernels, it is possible to switch between diﬀer-
ent TCP congestion control algorithms at runtime.
)
s
d
n
o
c
e
s
i
l
l
i
m
(
T
T
R
500
400
300
200
100
0
0
0.5
1
RTT
2
2.5
1.5
3
Time (seconds)
100
)
t
e
k
c
a
p
(
D
N
W
C
)
s
d
n
o
c
e
s
i
l
l
i
m
(
T
T
R
80
60
40
20
0
500
400
300
200
100
0
CWND
3.5
4
4.5
0
0.5
1
RTT
2
2.5
1.5
3
Time (seconds)
100
)
s
t
e
k
c
a
p
(
D
N
W
C
)
s
d
n
o
c
e
s
i
l
l
i
m
(
T
T
R
80
60
40
20
0
500
400
300
200
100
0
CWND
3.5
4
4.5
RTT
CWND
0
0.5
1
2
2.5
1.5
3
Time (seconds)
3.5
4
4.5
100
)
s
t
e
k
c
a
p
(
D
N
W
C
80
60
40
20
0
(a)
(b)
(c)
Figure 12: Comparing the performance of diﬀerent TCP protocols over an Internet path between a host in Germany
and a host in the BTOpenWorld DSL network: variation of packet round-trip times (RTT) and the congestion window
(CWND) over the duration of a ﬂow using (a) NewReno, (b) BIC, and (c) Vegas. The steep drops in RTT and CWND
values are due to packet losses. Compared to Reno, BIC shows higher RTTs and losses, while Vegas shows lower RTTs and
losses.
evolve over the duration of the transfer. All ﬂows begin in
the slow-start phase, where the CWND increases rapidly un-
til the ﬂow loses a packet and enters the congestion avoid-
ance phase. The TCP NewReno graph shows that the RTT
increases from 44ms at the beginning to well over 300ms be-
fore it loses a packet. This suggests the presence of a long
router queue at the congested link on this broadband Inter-
net path. TCP BIC, which has been adopted as the default
TCP protocol by Linux since kernel version 2.6.7, shows a
similar pattern but ramps up the congestion window much
faster after each loss, which results in even higher queueing
delays and packet losses. In contrast to NewReno and BIC,
Vegas enters a stable state with a round-trip time of about
100ms without suﬀering a single loss.
Our experiment shows that TCP BIC, the default conges-
tion control algorithm in Linux, exhibits the worst perfor-
mance both in terms of packet delay and packet loss. This
is not particularly surprising because BIC is designed for
Internet paths that have a high bandwidth-delay product.
In contrast, our measurement path includes a broadband
link with relatively low bandwidth. However, since many
hosts today use broadband Internet connections, it might
be important to improve BIC’s performance over broadband
networks.
Our Monarch results, while preliminary, show the impor-
tance of understanding the behavior of new protocols over a
variety of real network paths before deploying them widely.
5.2 Inferring network path properties