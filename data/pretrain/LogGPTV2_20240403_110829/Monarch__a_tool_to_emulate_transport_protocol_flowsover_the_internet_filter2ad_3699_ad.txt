### Unstationarity in the Network and Monarch Emulation

The primary cause of the observed unstationarity is the network itself, rather than an issue with the Monarch emulation. The consistency in estimates between successive TCP measurements along the same paths indicates the efficiency of our Monarch implementation. Despite the additional packet processing overhead introduced by our interposing proxy, the impact on packet latencies is negligible.

### Queueing Delays

Queueing delays exhibit a much larger variation or unstationarity over time compared to minimum and maximum Round-Trip Times (RTTs). This significant relative difference is due to the low absolute values of queueing delays. Over 76% of queueing delay estimates are below 10 milliseconds. Consequently, even a small 1-millisecond variation corresponds to a 10% difference.

### Packet Loss

Finally, we investigate the loss rates in the flows. Both Monarch and TCP senders retransmit packets that they perceive to be lost, which may differ from the actual lost packets. For example, TCP might misinterpret massive packet reordering as a loss and trigger a retransmission. Our interest here is in the perceived loss rates, so we use the packet retransmission rate as a proxy for the loss rate.

Figure 10 shows the cumulative distributions of retransmission rates for both Monarch and TCP flows. 75% of all Monarch flows and 88% of all TCP flows do not contain any retransmissions and, therefore, do not perceive packet loss. Thus, packet retransmissions do not affect a majority of both Monarch and TCP flows. Of the flows that do contain retransmissions, Monarch shows a higher retransmission rate than TCP. This is expected because Monarch must retransmit packets for losses in both upstream and downstream directions, while TCP only needs to retransmit packets lost on the downstream, due to cumulative acknowledgments.

### Summary of Flow-Level Properties

Our analysis demonstrates that Monarch can accurately emulate TCP flows with respect to flow-level properties such as throughput, latency, and queueing delay. However, Monarch's inability to distinguish between upstream and downstream packet loss causes it to over-estimate packet loss. The impact of this inaccuracy is limited to the small fraction of flows that experience upstream packet loss.

### Reliability of Self-Diagnosis

In the previous section, we showed that the primary source of inaccuracy in a Monarch emulation is upstream packet loss. In this section, we aim to demonstrate that Monarch’s self-diagnosis feature (Section 3.3) can reliably detect upstream packet loss, thereby warning the user of potential inaccuracies.

We tested this feature on the Monarch flows in our PlanetLab trace. For each flow, we compared the tcpdump traces from the sender and the receiver to determine how many packets had actually been lost on the downstream and the upstream. We then compared these results to the output of Monarch’s self-diagnosis for that flow, which uses only the sender-side trace.

Figure 11 shows the results. Self-diagnosis could not distinguish between all upstream and downstream losses for a very small number of flows (less than 2%). In these cases, Monarch printed a warning. For the majority of flows where self-diagnosis could infer the loss rates, the measured and inferred loss rates matched extremely well in both upstream and downstream directions. As expected, the total loss rate plots are identical.

We conclude that Monarch’s self-diagnosis can reliably detect the major source of inaccuracy in an emulated flow.

### Accuracy Over the Internet at Large

In the previous two sections, we demonstrated that upstream loss is the most important source of inaccuracies in Monarch emulations and that Monarch’s self-diagnosis can reliably detect the presence of upstream loss. Our goal in this section is to show that upstream losses are rare even when Monarch is used over real Internet paths.

We ran Monarch’s self-diagnosis over two Internet traces: the first trace consists of 15,642 flows to 4,805 broadband hosts, and the second trace contains 2,776 flows to 697 Internet routers. Table 5 summarizes our results. About 10% of the traces could not be analyzed by Monarch. In the broadband dataset, 7.1% of the traces did not contain usable IPIDs (8.3% in the router dataset), and 1.5% (2.3%) contained a loss that could not be classified as either upstream or downstream. In these cases, self-diagnosis was aborted immediately.

Overall, 84.2% of the broadband traces and 83.5% of the router traces were confirmed by self-diagnosis because neither upstream losses nor significant reordering errors were detected. This includes the 15.8% (24.9%) of the traces that contained only minor reordering errors that would not have changed the number of duplicate ACKs and, therefore, would not have affected any packet transmissions. Only 7.2% of the broadband traces and 5.9% of the router traces were reported as inaccurate.

We conclude that a majority of our flows to Internet hosts did not suffer from upstream packet loss or significant reordering, the two primary sources of inaccuracy in Monarch. This suggests that Monarch can be used to accurately emulate TCP flows to a large number of Internet hosts. Moreover, our results show that the IPID-based self-diagnosis is applicable in most cases.

### Summary

In this section, we showed that Monarch is accurate: its emulated TCP flows behave similarly to real TCP flows with respect to both packet-level and flow-level metrics. We also demonstrated that the most important source of error in Monarch’s flows is upstream packet loss, which can be reliably detected by Monarch’s built-in self-diagnosis. Further, our examination of large sets of Monarch flows to various Internet hosts, including hundreds of routers and thousands of broadband hosts, revealed that less than 10% of these flows suffer from upstream packet loss. From this, we conclude that Monarch can accurately emulate TCP flows to a large number of Internet hosts.

### Applications

Monarch’s ability to evaluate transport protocol designs over large portions of the Internet enables new measurement studies and applications. We used Monarch to conduct three different types of measurement experiments. In this section, we describe these experiments and present some preliminary results to illustrate their potential benefits.

#### Evaluating Different Transport Protocols

New transport protocol designs [3, 10, 47, 49] continue to be proposed as the Internet and its workloads change over time. However, even extensive simulation-based evaluations face skepticism about whether their results would translate to the real world. The resulting uncertainty around how well these protocols would compete with existing deployed protocols hinders their actual deployment. With Monarch, researchers can evaluate their new protocol designs over actual Internet paths.

We used Monarch to compare three different TCP congestion control algorithms implemented in the Linux 2.6.16.11 kernel: NewReno [12], BIC [49], and Vegas [8]. In our experiment, we emulated 500kB data transfers from a local machine to several hosts in broadband (cable and DSL) ISPs, using each of the three congestion control algorithms in turn. We examined the traces generated by Monarch for differences in protocol behavior.

Figure 12 shows the difference between the algorithms over a single, but typical path. The graphs depict the evolution of the congestion window (CWND) and the round-trip time (RTT) over the duration of the transfer. All flows begin in the slow-start phase, where the CWND increases rapidly until the flow loses a packet and enters the congestion avoidance phase. The TCP NewReno graph shows that the RTT increases from 44ms at the beginning to well over 300ms before it loses a packet, suggesting the presence of a long router queue at the congested link on this broadband Internet path. TCP BIC, which has been adopted as the default TCP protocol by Linux since kernel version 2.6.7, shows a similar pattern but ramps up the congestion window much faster after each loss, resulting in even higher queueing delays and packet losses. In contrast, Vegas enters a stable state with a round-trip time of about 100ms without suffering a single loss.

Our experiment shows that TCP BIC, the default congestion control algorithm in Linux, exhibits the worst performance in terms of packet delay and packet loss. This is not surprising because BIC is designed for Internet paths with a high bandwidth-delay product. In contrast, our measurement path includes a broadband link with relatively low bandwidth. However, since many hosts today use broadband Internet connections, it might be important to improve BIC’s performance over broadband networks.

Our preliminary Monarch results highlight the importance of understanding the behavior of new protocols over a variety of real network paths before deploying them widely.

#### Inferring Network Path Properties

[Continued in the next section]