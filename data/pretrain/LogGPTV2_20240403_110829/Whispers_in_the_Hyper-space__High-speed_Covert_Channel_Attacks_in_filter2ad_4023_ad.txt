In addition, we strengthen the communication proto-
col’s resilience to clock drifting and scheduling inter-
ruption by employing data framing. We break the data
into segments of ﬁxed-length bits, and frame each seg-
ment with a start-and-stop pattern. The beneﬁts of data
framing are twofold. First, when the sender detects trans-
mission interruption, instead of retransmitting the whole
piece of data, only the affected data frame is retried. Sec-
ond, some data will inevitably be lost during transmis-
sion. With data framing, the receiver can easily local-
ize the erasure errors and handle them well through the
Reed-Solomon coding.
The ﬁnalized protocol with all the improvements in
place is presented in Algorithm 4.
5 Evaluation
We evaluate the exploitability of memory bus covert
channels by implementing the reliable Cross–VM com-
munication protocol, and demonstrate covert channel at-
tacks on our in-house testbed server, as well as on the
Amazon EC2 cloud.
Algorithm 4 Reliable Timing-based Memory Bus Channel Protocol
MExoticS, MExoticR: Exotic memory regions for the sender and the receiver, respectively.
DSend, DRecv: Data to transmit and receive, respectively.
Sender Prepares DSend by:
{DMSend[]: Segmented encoded data to send}
RSSend := ReedSolomonEncode(DSend);
FDSend[] := Break RSSend into segments;
DMSend[] := DiffManchesterEncode(FDSend[]);
Receiver Recovers DRecv by:
{DMRecv[]: Segmented encoded data received}
FDRecv[] := DiffManchesterDecode(DMRecv[]);
RSRecv := Concatenate FDRecv[];
DRecv := ReedSolomonDecode(RSRecv);
Sending Encoded Data in a Frame:
{Data: A segment of encoded data to send}
{FrmHead, FrmFoot: Unique bit patterns
{signifying start and end of frame, respectively}
Result := SendBits(FrmHead);
if Result is not Aborted then
Result := SendBits(Data);
if Result is not Aborted then
{Ignore error in sending footer}
SendBits(FrmFoot);
return Succeed;
end if
end if
return Retry;
Sending a Block of Bits:
{Block: A block of bits to send}
{Base1, Base0: Mean contention-free access
{time for sending bit 1 and 0, respectively}
for each Bit in Block do
if Bit = 1 then
for an amount of time do
Timed atomic operation with MExoticS;
end for
Latency := Mean(AccessTime) − Base1;
else
for an amount of time do
Timed uncached memory access;
end for
Latency := Mean(AccessTime) − Base0;
end if
if Latency  T hreshold then
Bit := 1; {Bus is contended}
else
Bit := 0; {Bus is contention-free}
end if
{Detect sender de-schedule}
if too many consecutive 0 or 1 bits then
{Sender not running}
Sleep for some time;
{Sleep makes sender abort, then we abort}
return Aborted;
end if
end for
return Succeed;
5.1
In-house Experiments
We launch covert channel attacks on our virtualization
server equipped with the latest generation x86 platform
(i.e., with no shared memory bus). The experimental
setup is simple and realistic. We create two Linux VMs,
namely VM-1 and VM-2, each with a single virtual
processor and 512 MB of memory. The covert channel
sender runs as an unprivileged user program on VM-1,
while the covert channel receiver runs on VM-2, also as
an unprivileged user program.
We ﬁrst conduct a quick proﬁling to determine the op-
timal data frame size and error correction strength. And
we ﬁnd out that a data frame size of 32 bits (includ-
ing an 8 bit preamble), and a ratio of 4 parity symbols
(bytes) per 4 data bytes works well. Effectively, each data
frame transmits 8 bits of preamble, 12 bits of data, and
12 bits of parity, yielding an efﬁciency of 37.5%. In or-
der to minimize the impact of burst errors, such as multi-
ple frame losses, we group 48 data and parity bytes, and
randomly distribute them across 16 data frames using a
linear congruential generator (LCG).
We then assess the capacity (i.e., bandwidth and error
rate) of the covert channel by performing a series of data
transmissions using these parameters. For each transmis-
sion, a one kilobyte data block is sent from the sender to
the receiver. With 50 repeated transmissions, we observe
a stable transmission rate of 746.8±10.1 bps. Data errors
are observed, but at a very low rate of 0.09%.
5.2 Amazon EC2 Experiments
We prepare the Amazon EC2 experiments by spawning
physically co-hosted Linux VMs. Thanks to the opera-
tional experiences presented in [18, 30], using only two
accounts, we successfully uncover two pairs of physi-
cally co-hosted VMs (micro instances) in four groups of
40 VMs (i.e. each group consists of 20 VMs spawned by
each account). Information disclosed in /proc/cpuinfo
shows that these servers use the shared-memory-bus plat-
form, one generation older than our testbed server used
in the previous experiment.
Similar to our in-house experiments, we ﬁrst conduct
a quick proﬁling to determine the optimal data frame
size and error correction strength. Compared to our in-
house system proﬁles, memory bus channels on Ama-
zon EC2 VMs have a higher tendency of clock de-
synchronization. We compensate for this deﬁciency by
reducing the data frame size to 24 bits. The error cor-
rection strength of 4 parity symbols per 4 data bytes still
works well. And the overall transmission efﬁciency thus
becomes 33.3%.
We again perform a series of data transmissions and
measure the bandwidth and error rates. Our initial results
Ϳ
Ɛ
Ɖ
ď
;

Ğ
ƚ
Ă
Z
Ŷ
Ž
ŝ
Ɛ
Ɛ
ŝ

ŵ
Ɛ
Ŷ
Ă
ƌ
d
ϱϬϬ
ϰϱϬ
ϰϬϬ
ϯϱϬ
ϯϬϬ
ϮϱϬ
ϮϬϬ
ϭϱϬ
ϭϬϬ
ϱϬ
Ϭ
ĂŶĚǁŝĚƚŚ
ƌƌŽƌZĂƚĞ
Ϯϭ͘ϱϲй
Ϭ͘ϯϵй
Ϯ͘ϴϭй
ĞƐƚ
ĞŐƌĂĚĞĚ
EŽŝƐǇ
ϯϬй
Ϯϱй
ϮϬй
ϭϱй
ϭϬй
ϱй
Ϭй
Ğ
ƚ
Ă
Z

ƌ
Ž
ƌ
ƌ


Figure 5: Memory Bus Channel Capacities in EC2

are astonishingly good. A transmission rate of 343.5 ±
66.1 bps is achieved, with error rate of 0.39%. However,
as we continue to repeat the measurements, we observe
an interesting phenomenon. As illustrated in Figure 5,
three distinct channel performances are observed through
our experiment. The best performance is achieved dur-
ing the initial 12–15 transmissions. After that, for the
next 5–8 transmissions, the performance degrades. The
bandwidth slightly reduces, and the error rate slightly in-
creases. Finally, for the rest of the transmissions, the per-
formance becomes very bad. While the bandwidth is still
comparable to that of the best performance, the error rate
becomes unacceptably high.
By repeating this experiment, we uncover that the
three-staged behavior can be repeatedly observed after
leaving both VMs idle for a long period of time (e.g.,
one hour). Therefore, we believe that the cause of this
behavior can be explained by scheduler preemption [29]
as discussed in [30]. During the initial transmissions,
the virtual processors of VMs at both the sender and
receiver sides have high scheduling priorities, and thus
they are very likely to be executed in parallel, resulting
in a very high channel performance. Then, the sender
VM’s virtual processor consumes all its scheduling cred-
its and is throttled back by the Xen scheduler, causing the
channel performance to degrade. Soon after that, the re-
ceiver VM’s virtual processor also uses up its scheduling
credits. Since both the sender and receiver are throttled
back, their communication is heavily interrupted. This
“offensive” scheduling pattern subjects the communica-
tion channel to heavy random erasures beyond the cor-
rection capability of the FEC mechanism.
Fortunately, our communication protocol is designed
to handle very unreliable channels. We adapt to the
scheduler preemption by tuning two parameters to be
more “defensive”. First, we increase the ratio of parity
bits to 4 parity symbols per 2 data bytes. Although it re-
duces transmission efﬁciency by 11.1%, the error correc-
tion capability of our FEC is increased by 33.3%. Sec-
ond, we reduce the transmission symbol rate by about
20%. By lengthening the duration of the receiving conﬁr-
Ϳ
Ɛ
Ɖ
ď
;

Ğ
ƚ
Ă
Z
Ŷ
Ž
ŝ
Ɛ
Ɛ
ŝ

ŵ
Ɛ
Ŷ
Ă
ƌ
d
ƌĞĚŝƚƐ
ĞƉůĞƚĞĚ
ƌĞĚŝƚƐ
ZĞƉůĞŶŝƐŚĞĚ
ϯϬϬ
ϮϱϬ
ϮϬϬ
ϭϱϬ
ϭϬϬ
ϱϬ
Ϭ
ϰϬϬ
ϱϬϬ
ϲϬϬ
ϳϬϬ
ϴϬϬ
ϵϬϬ
ϭϬϬϬ
dƌĂŶƐĨĞƌWƌŽŐƌĞƐƐ;ǇƚĞƐͿ

Figure 6: Reliable Transmission with Adaptive Rates
mation, we effectively increase the probability of discov-
ering scheduling interruptions. After the parameter ad-
justment, we can achieve a transmission rate of 107.9 ±
39.9 bps, with an error rate of 0.75%, even under sched-
uler preemption.
Figure 6 depicts the adjusted communication proto-
col in action. During the ﬁrst period of preemption-free
scheduling, the transmission rate can be as high as 250
bps. However, when preemption starts, the sender re-
sponds to frequent transmission failures with increased
retries, allowing the receiver continue to receive and de-
code data without uncorrectable error. And correspond-
ingly, the transmission rate drops to below 50 bps. Fi-
nally, when the harsh scheduling condition is alleviated,
the transmission rate is automatically restored. The capa-
bility of adaptively adjusting transmission rates to chan-
nel conditions, evidences the versatility of our reliable
communication protocol.
6 Discussion
In this section, we ﬁrst reassess the threat of covert chan-
nel attacks based on our experimental results. Then, we
discuss possible means to mitigate the covert channel at-
tacks in virtualized environments.
However, this additional requirement does not signiﬁ-
cantly reduce the usefulness of covert channels in data
theft attacks.
Data theft attacks are normally launched in two steps,
inﬁltration and exﬁltration. In the inﬁltration step, attack-
ers leverage multiple attack vectors, such as buffer over-
ﬂow [4], VM image pollution [2, 26], and various social