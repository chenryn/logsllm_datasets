title:Restoring compromised privacy in micro-data disclosure
author:Lei Zhang and
Alexander Brodsky and
Sushil Jajodia
Restoring Compromised Privacy in Micro-data Disclosure
Lei Zhang1
Alexander Brodsky1,2
Sushil Jajodia1,3
1Center for Secure Information Systems, George Mason University, Fairfax, VA 22030
2Department of Computer Science, George Mason University, Fairfax, VA 22030
3The MITRE Corporation, 7515 Coleshire Drive, Mclean, VA 22102
{lzhang8, brodsky, jajodia}@gmu.edu
ABSTRACT
Studied in this paper is the problem of restoring compro-
mised privacy for micro-data disclosure with multiple dis-
closed views. The property of γ-privacy is proposed, which
requires that the probability of an individual to be associ-
ated with a sensitive value must be bounded by γ in a possi-
ble table which is randomly selected from a set of tables that
would lead the same disclosed answers. For the restricted
case of a single disclosed view, the γ-privacy is shown to
be equivalent to recursive ( γ
1−γ , 2)-Diversity, which is not
deﬁned for multiple disclosed views. The problem of decid-
ing on γ-privacy for a set of disclosed views is proven to be
#P-complete. To mitigate the high computational complex-
ity, the property of γ-privacy is relaxed to be satisﬁed with
(, θ) conﬁdence, i.e., that the probability of disclosing a sen-
sitive value of an individual must be bounded by γ +  with
statistical conﬁdence θ. A Monte Carlo-based algorithm is
proposed to check the relaxed property in O((λλ(cid:48))4) time
for constant  and θ, where λ is the number of tuples in the
original table and λ(cid:48) is the number diﬀerent sensitive values
in the original table. Restoring compromised privacy using
additional disclosed views is studied. Heuristic polynomial
time algorithms are proposed based on enumerating and
checking additional disclosed views. A preliminary exper-
imental study is conducted on real-life medical data, which
demonstrates that the proposed polynomial algorithms re-
store privacy in up to 60% of compromised disclosures.
Categories and Subject Descriptors
H.2.7 [DATABASE MANAGEMENT]: Database Ad-
ministration—Security, integrity, and protection
General Terms
Security
Keywords
Data Privacy, Micro-data Disclosure
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASIACCS’10 April 13–16, 2010, Beijing, China.
Copyright 2010 ACM 978-1-60558-936-7 ...$10.00.
1.
INTRODUCTION
We have been faced with an information explosion in re-
cent years. Search and data mining have been extensively
used to help information consumers to ﬁlter and absorb the
information to which they are exposed. However, the vul-
nerability of privacy protection has not been adequately ad-
dressed and remains a major concern. It is alarming that,
for example, the medical information of a person can be eas-
ily identiﬁed just by linking a public voters’ list with some
anonymized datum in medical research ([29, 30]). There
has been a considerable body of research on how to mitigate
such a rising threat on privacy, i.e., to prevent the rela-
tion between individuals and sensitive attributes from being
identiﬁed ([29, 30, 3, 23, 34, 2]). The standard way to pre-
serve privacy is based on monitoring the entire information
disclosure process and rejecting any request with a potential
privacy violation.
However, there are multiple reasons for failure in prevent-
ing privacy violations. First, multiple sources may release in-
formation on individuals without centralized disclosure con-
trol.
In this case, even if information disclosed by each
source preserves privacy, the combined information may vi-
olate privacy. For example, as shown in ([30]), anonymized
disclosure of medical records from a hospital, combined with
general information from a voters’ list, can be used to in-
fer medical conditions of individuals, which clearly violates
their privacy. Second, the requirements for privacy may in-
crease over time due, for example, to new legislation, while
sensitive information has already been disclosed. Third, pri-
vacy violation may also arise in practice due to exceptions
in information disclosure control.
To address the problem, we investigate the following ques-
tion in this paper. When a desired privacy property is al-
ready compromised due to accidental information disclosure,
is there a way to restore it? At ﬁrst glance, the answer seems
to be negative, because the information disclosure process is
irreversible. That is, the more information about secret data
is disclosed, the more constrained the set of “possible worlds”
becomes.
However, for privacy properties that are related to prob-
ability distribution or entropy, there is a way to restore the
compromised privacy under certain conditions. Although we
cannot reverse the disclosure process, we may still be able
to restore the compromised privacy by disclosing more true
information.
More generally, the question we investigate in this pa-
per is as follows. Given a set of information objects that
jointly violate a desired privacy property, can we extend it
36to a superset that satisﬁes the same privacy property. Note
that this question is important not only for restoring acci-
dentally compromised privacy as described above, but also
for the centralized disclosure control to provide better data
availability. For example, if the centralized disclosure con-
trol cannot release a collection of data because of privacy
violation, we may be able to disclose the data collection in
conjunction with additional data; if disclosed together, the
privacy can be preserved.
It is worth noting, before giving a concrete example, that
apparently the claim above seems very counter-intuitive.
However, as we will also explain in the following example,
we are not contradicting with the common intuition at all,
i.e., in general, more disclosed true information should al-
ways lead to worse privacy. The truth is, in an information
disclosure problem where privacy of a small group of people
does not have enough protection due to some well-deﬁned
regulations, there may exist another group of people whose
privacy is “over-protected”. We observe that in this case,
it is possible to sacriﬁce the “over-protected” privacy of the
latter group of people, as long as they meet the regulations
and, in return, the privacy of the former group of people
can be restored. At the same time, the privacy protection
in general is still getting worse.
Intuitively, by disclosing
more true information, we will always decrease the privacy
protection in general but may be able to increase the privacy
protection on a small part of the entire group.
A Motivating Example
We limit our scope here to the problem of micro-data dis-
closure and we consider only generalization-based view dis-
closure. Some other techniques like data swapping and per-
turbation ([14, 28, 22, 33]) that can be also utilized in the
micro-data problem, as discussed in the related work, are
not covered in this paper. To explain the basic idea, con-
sider a medical information disclosure example. A table of
patients’ medical information is shown in Table 1, part of
which is publicly accessible through, say, a voters’ list, shown
in Table 2.
Condition
Heart Disease
Name Sex Age Employer
M
ABC, Inc.
Alan
ABC, Inc.
M
Bob
ABC, Inc. Viral Infection
Clark
M
Donald M
ABC, Inc.
ABC, Inc. Viral Infection
F
Ellen
ABC, Inc.
F
Fen
F
ABC, Inc.
23
24
25
26
27
28
28
Garcia
SARS
SARS
SARS
Flu
Table 1: Patient Information Table
Name Sex Age Employer
M
Alan
ABC, Inc.
ABC, Inc.
M
Bob
ABC, Inc.
Clark
M
ABC, Inc.
Donald M
ABC, Inc.
F
Ellen
Fen
F
ABC, Inc.
ABC, Inc.
F
23
24
25
26
27
28
28
Garcia
Table 2: Information from Voters’ List
Now assume one view of Table 1 is disclosed by a data au-
thority (e.g., hospital) upon request as shown in Table 3(A).
Note that this view is generalized enough not to reveal the
individuals’ medical conditions, which are considered sensi-
tive. More formally, Table 3(A) satisﬁes the property of Re-
cursive (1,2)-Diversity ([3]). This means that, in any group
of individuals present in the view and indistinguishable by
non-sensitive attributes (i.e., four tuples in the view having
the same Sex Attribute “Male” form a group), the maximum
frequency of any sensitive value cannot exceed 0.5. To cal-
culate the frequency of a sensitive value, the number of oc-
currences of the sensitive value in the multiset of sensitive
values, which are associated with the group, are counted and
then divided by the size of the multiset. In the example, the
condition SARS has the maximum appearance frequency of
0.5.
Condition
Sex
M Heart Disease
M
M
M Viral Infection
SARS
SARS
Flu
Condition
Age
26˜28
26˜28
26˜28
26˜28 Viral Infection
SARS
SARS
(A)Male-Cond in ABC, Inc.
(B)Age-Cond in ABC, Inc.
(ﬁrst four tuples)
(last four tuples)
Table 3: Two Disclosed Views of Table 1
Recall that we want to prevent any individual’s medical
condition from being identiﬁed through the combined dis-
closed and public information. As we will prove in Sec-
tion 2, the satisfaction of Recursive (1,2)-Diversity by Ta-
ble 3(A) guarantees that any adversary cannot win the fol-
lowing game to infer any individual’s medical condition with
the probability of 0.5. Assume the adversary has a random
oracle ([6]) whose output domain is the set of all possible
patient information tables that yield the same result as the
original Table 1 in both the disclosed views (Table 3(A)) and
public information (Table 2), if computed the same way. For
example, Table 4 could be one of the possible outputs. Intu-
itively, a random oracle will respond to every query with a
random response chosen uniformly from its output domain.
The adversary is interested to know whether an individual,
id, that appears in Table 2, is associated with a medical con-
dition s. She will query the random oracle for an answer.
The adversary wins if in the outcome (a possible table), id is
associated with s. We say that there is a violation of privacy
if, for any individual and medical condition, the probability
for the adversary to win this game is higher than 0.5 or
another pre-established bound. Clearly, the adversary can
estimate the probability of winning the game within any
desired statistical conﬁdence interval by playing the game
suﬃciently many times.
In this example, the output domain of the adversary’s
random oracle is rather simple. It can be shown that the
probability of Alan to have SARS, Heart Disease, or V iral
Inf ection in an outcome is 0.5, 0.25, or 0.25, respectively.
The same result applies to Bob, Clark, and Donald.
Similar to Table 3(A), Table 3(B) presents another gen-
eralized view that also satisﬁes the desired Recursive (1,2)-
Diversity. That is, by disclosing Table 3(B) (and also Ta-
ble 2) to the adversary, we will not have a violation of pri-
vacy. However, if the adversary gets both Table 3(A) and
Table 3(B), privacy will be violated. Note that, a property
37Condition
Name Sex Age Employer
ABC, Inc.
M
Alan
M
Heart Disease
Bob
ABC, Inc.
ABC, Inc. Viral Infection
Clark
M
ABC, Inc.
Donald M
ABC, Inc.
F
Ellen
ABC, Inc.
F
Fen
F
ABC, Inc.
SARS
(any)
(any)
(any)
23
24
25
26
27
28