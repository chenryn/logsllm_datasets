program states explored at runtime but that do not show up as
new coverage, i.e., meaningful interleavings missed by alias
coverage. KRACE forgoes the opportunities to check data races
in those cases and is a trade-off made in favor of expanding
the coverage with efficiency.
ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
FS
btrfs
btrfs
btrfs
btrfs
btrfs
btrfs
btrfs
btrfs
btrfs
btrfs
btrfs
ext4
ext4
ext4
ext4
VFS
VFS
VFS
VFS
VFS
VFS
VFS
VFS
Racing access
heap struct: cur_trans->state
heap struct: cur_trans->aborted
heap struct: delayed_rsv->full
heap struct: sb->s_flags
global variable: buffers
heap struct: inode->i_mode
heap struct: inode->i_atime
heap struct: BTRFS_I(inode)->disk_i_size
heap struct: root->last_log_commit
heap struct: free_space_ctl->free_space
heap struct: cache->item.used
heap struct: inode->i_mtime
heap struct: inode->i_state
heap struct: ext4_dir_entry_2->inode
heap array: ei->i_data[block]
heap string: name in link_path_walk
heap struct: inode->i_state
heap struct: inode->i_wb_list
heap struct: inode->i_flag
heap struct: inode->i_opflags
heap struct: file->f_mode
heap struct: file->f_pos
heap struct: file->f_ra.ra_pages
Status
pending
harmful
harmful
benign
harmful
benign
harmful
harmful
harmful
benign
harmful
benign
benign
benign
harmful
pending
benign
benign
benign
benign
benign*
pending
harmful
TABLE I: List of data races found and reported by KRACE so far.
Status of benign* means that it is a benign race according to the
execution paths we submitted, but the kernel developers suspect that
there might be other paths leading to potentially harmful cases.
A. Data races in popular file systems
Across intermittent fuzzing runs on two popular kernel file
systems (btrfs and ext4) during two months, KRACE found
and reported 23 new data races, of which nine have been
confirmed to be harmful, 11 are benign, and the rest of them
are still under investigation, as listed in Table I. Note that
besides bugs in concrete file systems, KRACE also finds data
races in the virtual file system (VFS) layer, which might affect
all file systems in the kernel.
Consequence. Based on our preliminary investigation, only
one bug (#5) is likely to cause immediate effects (null-
pointer dereference) when triggered. Others are likely to cause
performance degradation or specification violations, but we do
not see a simple path toward memory errors. This also means
that relying on bug signals such as KASan reports or kernel
panics might not be sufficient to find data races.
B. Fuzzing characteristics
Coverage growth. The growth patterns for both branch and
alias coverage are plotted in Figure 11 (for btrfs) and Figure 12
(for ext4). There are several interesting observations:
Alias coverage size. Although branch coverage for the two
file systems grow into roughly the same level (25K vs 20K),
compared with ext4, btrfs has a significantly larger alias
coverage bitmap, (60K vs 9K). Given that the number of user
threads is the same (3 threads), the difference is caused by
the level of concurrency inherent in btrfs and ext4 design.
As shown in Figure 14, btrfs uses at least 22 background
threads and each thread may additionally fork more helper
threads, while the only background thread for ext4 is the
jbd2 journaling thread. In other words, btrfs is inherently
more concurrent than ext4, and dividing works among more
Fig. 10: Implementation of the QEMU VM-based fuzzing executor
in KRACE. The VM instance and the host have three communication
channels: 1) private memory mapping, which contains the test case
program to be executed by the VM and the seed quality report
generated by KRACE runtime; 2) globally shared memory mapping,
which contains the coverage bitmaps globally available to the host
and all VM instances; 3) file sharing under the 9p protocol for sharing
of large files, including the file system image and the execution log.
E. Implementation
KRACE’s code base is divided into two parts: 1) compile-
time preparation, including annotations to the kernel source
code (in the form of kernel patches), an LLVM instrumentation
pass, and the KRACE library compiled into the kernel that
provides coverage tracking and logging at runtime; and 2) a
VM-based fuzzing loop that evolves test cases, executes them
in QEMU VMs, and checks for data races. The complexity
of each component is described in Table III and an overview
of the runtime executor is shown in Figure 10. Due to space
constraints, more details can be found in §D.
VII. EVALUATION
In this section, we evaluate KRACE as a whole as well
as per each component. In particular, we show the overall
effectiveness of KRACE by listing previously unknown data
races found (§VII-A); provide a comprehensive view of
KRACE’s performance characteristics, e.g., speed, scalability,
etc., as a file system fuzzer (§VII-B); justify major design
decisions with controlled experiments (§VII-C); and compare
KRACE against recent OS and data race fuzzers (§VII-D).
Experiment setup. We evaluate KRACE on a two-socket, 24-
core machine running Fedora 29 with Intel Xeon E5-2687W
(3.0GHz) and 256GB memory. All performance evaluations
are done on Linux v5.4-rc5, although the main fuzzer runs
intermittently across versions from v5.3. We build the kernel
core with minimal components but enable as many features as
possible for the btrfs and ext4 file system modules. For all
evaluations, the fuzzing starts with an empty file system image
created from the mkfs.* utilities. We run 24 VM instances in
parallel for fuzzing and each VM runs a three-thread seed.
11
Fig. 11: Evaluation of the coverage growth of KRACE when fuzzing
the btrfs file system for a week (168 hours) with various settings.
Fig. 13: Evaluation of seed execution and analysis time in KRACE
with a varying number of syscalls in the seed
The overhead mainly comes from memory access instrumen-
tation, as every memory access is now turned into a function
call where atomic operations are performed and synchronized,
not only with respect to all other threads on the VM, but also
against all threads across all VMs, as the thread is updating the
global bitmap on the host directly (implicitly handled by the
QEMU ivshmem module). As a result, further optimizations are
possible. For example, a VM instance may accumulate coverage
locally and update the global bitmap in batches instead of on
every memory access.
It is, however, debatable whether the overhead is detrimental
to KRACE as a fuzzer since lower overhead simply means that
the coverage growth will converge and saturates faster. In our
opinion, we consider the overhead caused by tracking more
coverage (including alias coverage) as a trade-off between
execution speed and seed quality. A fuzzer with fast executions
may waste resources in non-interesting test cases, while a