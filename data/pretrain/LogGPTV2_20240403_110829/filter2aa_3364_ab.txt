### Obfuscation Techniques and Their Detection

When used individually, obfuscation techniques can effectively conceal the true functionality of a script. However, when combined, they significantly complicate the task of detecting and analyzing malicious code.

### Web Crawling for JavaScript Analysis

To ensure our research was not purely theoretical, we needed to gather real-world examples of JavaScript. We decided to focus on websites with a high volume of user-generated content, hypothesizing that a crawl starting from MySpace would yield a rich sample set.

#### Using Heritrix for Web Crawling

We utilized the open-source Heritrix web crawler, developed by the Internet Archive. Heritrix is designed for large-scale web crawling and offers extensive configuration options. For those familiar with Java, setting up Heritrix was straightforward, and we quickly began collecting JavaScript samples.

**Key Concepts in Heritrix:**
- **Profiles:** Define all aspects of the web crawler's configuration.
- **Jobs:** Based on Profiles, Jobs can override inherited settings and specify seed URIs (initial starting points).
- **Frontier:** Maintains the overall state of the web crawl, allowing Jobs to be paused and resumed.
- **Custom Workflows:** Use Crawl Operator, Crawl Organization, and Crawl Job Recipient properties for fine-grained control over search strategies and URI selection rules.
- **Robots.txt Policies:** Configurable policies to respect site-specific `robots.txt` files.
- **Resource Consumption:** Extensive options to limit resource usage.

**Modifying Default Settings:**
Before running a Job, we had to modify the `User-Agent` and `From` HTTP headers to meet Heritrix's criteria. The `User-Agent` must follow a specific format, and the `From` header must contain a valid email address. We created a new profile named "Caffeine Monkey" based on the default Profile, with custom values for these headers.

### Data Analysis and Reporting

With the JavaScript samples indexed in our database, we developed several Python classes to automate the analysis process. Each sample was run through the Caffeine Monkey engine, and its runtime log was analyzed to generate execution statistics.

#### Pre-processing and Indexing

Heritrix stores collected content in ARC files, each typically growing to about 100MB and containing thousands of documents. To efficiently extract JavaScript documents, we used and extended a collection of Perl scripts from the University of Michigan. These scripts were modified to index the ARC files into a MySQL database, including two new tables: one for Heritrix Jobs and another for storing analysis results.

**Database Schema:**
- **URI:** The URL of the retrieved document.
- **Heritrix Job ID:** The Job that collected the URI.
- **MIME Content-Type:** The type of content.
- **HTTP Response Code:** The 3-digit response code from the server.
- **ARC File:** The file containing the retrieved document.
- **Index and Length:** Used to extract the document from the ARC file.
- **Timestamp:** The time of retrieval.

### Function Call Analysis

We analyzed the function calls in both known malicious scripts (labeled as "Monkey Chow #1" to "#4") and the top JavaScript domains from our MySpace crawl. The absolute number of function calls is less important than the ratios between them.

**Function Call Ratios:**
- **Malicious Scripts:** Three out of four samples showed similar ratios of object instantiations, element instantiations, `eval()` calls, and string instantiations.
- **Top JS Sites:** Similar ratios were observed across the nine top domains, suggesting a consistent pattern.

**Future Research:**
Further research could involve analyzing larger samples of malicious JavaScript to confirm these trends and explore their implications for detection and prevention.

---

This revised version aims to provide a clearer, more structured, and professional presentation of the original text.