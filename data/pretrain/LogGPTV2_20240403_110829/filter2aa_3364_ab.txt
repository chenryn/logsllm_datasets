Used alone, each of these techniques can be effective at obfuscating the true functionality of 
 script. Combined together, they clearly make the job of effective detection very difficult. 
a
Along Came a Spider… 
Of course, without evidence of these techniques being actively used in the wild our 
research would be purely theoretical. With this in mind we set out to crawl selected 
portions of the web, focusing on sites with a large amount of user created content. 
Intuitively, we thought that a crawl originating from www.myspace.com would yield an 
interesting sample of JavaScript. 
Not wanting to reinvent the wheel, the open source Heritrix software package was 
employed. Heritrix is an Internet‐scale web crawler developed by the Internet Archive for 
their own use. Setup was straight forward for those with experience using Java, and before 
we knew it we were collecting JavaScript from the wild. 
Heritrix is designed around the concepts of Profiles and Jobs. A Profile is used to define all 
aspects of web crawler configuration. A Job is based on a Profile and can override 
configuration options inherited from the Profile. A Job specifies the seed URIs which are the 
initial starting points of the web crawl. Jobs are queued for processing and will be picked 
up by an idle crawler thread. The overall state of a web crawl is maintained internally as a 
Frontier. Jobs can be paused and resumed by recovering the Job state from a previously 
saved Frontier. 
Heritrix supports custom workflows through the use of the Crawl Operator, Crawl 
Organization, and Crawl Job Recipient properties. Fine‐grained control is given over search 
strategies (e.g. “Deep”, “Broad”) and the rules used to select or reject URIs to pursue. 
Several policies for respecting site robots.txt files are configurable. Extensive 
configuration options to limit resource consumption are available. 
Before being able to run a Job using the default Profile we were forced to modify the values 
of the User-Agent and From HTTP headers to meet Heritrix’s restrictive criteria. The 
User-Agent is required to follow a form like “Mozilla/5.0 (compatible; heritrix/1.12.1 
+PROJECT_URL_HERE)” and the From header must contain a valid email address. We 
created a new profile named “Caffeine Monkey” that was based on the default Profile but 
defined our particular values for the User-Agent and From headers. Being the good 
Internet citizens that we are, we configured our crawler to identify itself as belonging to 
Feinstein & Peck 
6 
Black Hat USA 2007
Analysis and Reporting 
With all the archive files from our MySpace crawl indexed in our database, it was relatively 
straight forward to retrieve all the JavaScript documents for analysis. Several Python 
classes were created to automate the process of analyzing the collected scripts. Each 
JavaScript sample was run through the Caffeine Monkey engine and its runtime log was 
analyzed. Statistics on runtime execution were generated from the log and stored in the 
Pre­processing and Indexing 
Heritrix saves the content it collects in an archival file format referred to by its three 
character file extension, ARC. By default each archive file grows to about 100MB and can 
contain thousands of spidered documents. We needed a way to efficiently pull the 
JavaScript documents out of the archive files in order to subsequently analyze them using 
the Caffeine Monkey engine. 
Fortunately an individual at the University of Michigan had already written a rough 
collection of Perl scripts for working with Heritrix archive files. One of these scripts was 
designed to index a collection of archive files into a MySQL database containing a single 
table. We extended the rudimentary database schema to encompass two new tables: the 
first representing the Heritrix Jobs that collect the archives, and the second for storing the 
results from analyzing the JavaScript samples. The existing Perl scripting was modified, 
adding support for the concept of Heritrix Jobs and the new job database table, as well 
fixing several bugs. 
The relevant details of each URI that was retrieved were stored in the database. These 
included: 
• URI 
• 
as retrieved as a part of 
Heritrix Job the URI w
• MIME Content‐Type 
• 
e server providing the URI 
3‐digit HTTP response code received from th
• ARC file containing the retrieved document 
• Index into the ARC file of the start of the retrieved document 
• 
gth, used along with the file index to extract the retrieved document 
Content‐Len
• Timestamp 
database. The Caffeine Monkey engine hooks a number of interesting functions. For each 
script run through the engine, function call statistics were captured. 
At the time of this writing, we had not identified any malicious JavaScripts among the 364 
samples collected through our MySpace web crawl. In order to get samples of known 
malicious scripts we sent requests to several other security researchers. In all, we received 
four good samples which we labeled Monkey Chow #1 through #4. 
Function Call Analysis of "Bad" Scripts
0
5
10
15
20
25
30
35
40
45
object_instance
element_instance
escape
eval
string_instance/50
document_write
Chow #1
Chow #2
Chow #3
Chow #4
Note that the numbers for string instantiations are scaled by a factor of 1/50 in order to 
yield better chart scaling. There are orders of magnitude more string instantiations than 
the other categories because string concatenation is a popular operator, and because string 
concatenation creates three new string instances for each use of the “+” operator. 
What is important to look at is not the absolute number of times each function is called, but 
rather the ratios of the function call counts to one another. For three out of the four 
samples above, the ratios between the count of object instantiations, element 
instantiations, calls to eval(), and string instantiations are strikingly similar. Future 
research might involve analyzing larger samples of malicious JavaScript to see if these 
Feinstein & Peck 
7 
Black Hat USA 2007
trends hold. 
Based on the results of our MySpace web crawl, we grouped the collected JavaScript URIs 
by their domain name fragment. We then sorted the domain names by the number of 
JavaScript documents retrieved from each. Myspace.com was the top domain by the 
number of JavaScript documents collected, yielding a sample size of twenty‐seven. 
Grouping the domains by the top six counts of JavaScript documents yielded nine domains 
because of ties. hillaryclinton.com rounded out the top nine, with five JavaScripts collected 
from the domain. For each of the top domains we generated aggregate statistics and 
harted the results is the same fashion as our Monkey Chow samples. 
c
Function Call Analysis of Top JS Sites
0
50
100
150
200
250
300
350
400
myspace.com
fastclick.net
evite.com
object_instance
element_instance
escape
eval
string_instance/50
document_write
m
frig
st
hill
uchmusic.com
photofile.ru
youtube.com
htcatalog.com
ore.yahoo.net
aryclinton.com
Again, the absolute magnitudes of the function call counts are not as important as the ratios 
between them. There does appear to be similar ratios of function calls across the nine 
domains sampled. At this point we charted our monkey chow samples against our analysis 
f the top JavaScript domains from our MySpace crawl. 
o
Feinstein & Peck 
8 
Black Hat USA 2007
Function Call Analysis (Combined)
0
50
100
150
200
250
300