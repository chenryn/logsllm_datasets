Algorithm 1 Recursive Partitioning and Summarization
RPS sanitize(D, R)
return post process(RPS recursion(D, R))
RPS recursion(D, R)
(Stop, Info) ← stopping condition(D, R)
if Stop then
return summarize(D, R)
end if
(R1, R2) ← balanced partition(D, R)
return (Info, RPS recursion(D, R1), RPS recursion(D, R2))
stopping condition(D, R)
return (whether to stop partitioning the region R, aux-
iliary information about R)
summarize(D, R)
return a multiset of tuples that summarize the tuples in
D that are in region R
balanced partition(D, R)
return the results of partitioning R into two subregions
post process(Tree)
return dataset based on the tree resulted from RPS
Running the algorithm on a dataset D and an initial range
R0 results in a series of partitions, which form a partition
tree. The root of the tree is R0. Each internal node has
two child nodes which represent two non-overlapping sub-
regions that together form the region represented by the
parent node. Running this algorithm also results in a se-
ries of queries about D. More speciﬁcally, on each internal
node corresponding to a region R, two queries are executed:
stopping condition(D, R) and partition(D, R), and on each
leaf node corresponding to a region R, two queries are exe-
cuted: stopping condition(D, R) and summarize(D, R).
Because changing one tuple in a dataset aﬀects only
queries along one path in the tree from a leaf node to the
root,an RPS algorithm satisﬁes ǫ-diﬀerential privacy if for
any partition tree generated by the algorithm, all queries
satisfy diﬀerential privacy, and the total privacy budgets
consumed along any path is bounded by ǫ. This follows
from the sequential and parallel composition properties of
diﬀerential privacy. [23]
An RPS can be very eﬃcient. If all the three subroutines
run in time linear in the number of tuples in the region, then
the algorithm takes time O(N log N ).
3.2 Instantiating the RPS framework
We now study how to instantiate this RPS framework
into concrete algorithms. One needs to provide the im-
plementation of three functions: stopping condition(D, R),
partition(D, R), and summarize(D, R). We want these func-
tions to satisfy diﬀerential privacy relative to D. One fur-
ther needs to provide a way to allocate a privacy budget to
answer these queries.
Partitioning. Partitioning is the most important step.
One approach is to ﬁrst choose an attribute (one dimension
in the space), then query the median value among tuples in
the region, and ﬁnally partition the region at the median
point. To answer the medium query, one can use the ap-
proach based on smooth sensitivity. This method, however,
has a number of drawbacks. First, it is unclear how to best
choose the attribute on which to partition. Second, it is
unclear how to deal with attributes with categorical values,
where the concept of median does not apply.
We propose using the exponential mechanism to partition
the dataset. This method can be adapted to both continuous
and discrete attributes. For continues attributes, we choose
a granularity, and consider all multiples of the granularity.
For instance, given a domain range [0, 100] and granularity
1, we consider partitions along 1, 2, . . . to 100. For discrete
attributes, we can either consider all possible partitions of
the attribute domain if the domain is suﬃciently small. Al-
ternatively, we can consider an order (possibly random) over
the domain and partition linearly along this order. Such de-
cisions can be made based on the database schema.
Another advantage of using the exponential method is
that, with one query we can consider all possible ways of
partitioning along diﬀerent dimensions at once instead of
choosing one dimension a priori. For example, suppose that
we have only two attributes: education and salary. Here
we would consider all possible ways to partition along ed-
ucation and all possible ways to partition along salary. By
considering the union of such partitions as the output of the
partition mechanism, we can use the exponential mechanism
to choose the best partition along both dimensions.
An interesting challenge is what quality function to use.
In general, we choose a quality function that would give
higher preference to an even split. When even split is the
only objective, the quality of the partition (R1, R2) ∈ R is
deﬁned as follows:
q(d, (r1, r2)) =
n − |r1 − r2|
4
(2)
where r1 and r2 are the number of elements in R1 and R2
respectively, and n = r1 + r2.
Removing one record will change the size of one of the
partitions by 1; hence the sensitivity of q is
∆q ≤
n − |r1 − r2|
4
−
(n − 1) − (|r1 − r2| + 1)
4
=
1
2
We have found that diﬀerent application domains may
beneﬁt from diﬀerent choices of the quality function. We
discuss such choices when discussing diﬀerent application
domains in Sections 4 and 5.
Furthermore, the running time is proportional to the size
of the attribute domain, which can be limited to a constant,
and is linear to the size of the dataset in the region.
Choosing a stopping condition. To determine whether
one should stop further partitioning a region R or not, sev-
eral methods can be used. The ﬁrst is to stop when a cer-
tain depth is reached. This has two beneﬁts. One is that
one can allocate the privacy budget to be used in each level
using the knowledge of the maximum depth. The other is
that this requires no query on D and consumes no privacy
budget; hence one can conserve the privacy budget for other
queries.
Alternatively, we can stop when the region contains too
few tuples. To determine whether this is the case, the al-
gorithm would issue a count query to determine how many
tuples D has in the region R and decide to stop if the result
is lower than a threshold. Because count queries have low
sensitivities, the standard method of adding Laplace noise
is accurate enough for this purpose.
We can also stop when the size of the region is small
enough. For example, if the region already contains a sin-
gle possible value, i.e., all tuples are already the same, then
it makes no sense to perform additional partitioning. Even
when some attributes are continuous, perhaps one would
consider a region to be small enough so that consuming ad-
ditional privacy budget in further partitioning is no longer
beneﬁcial. Using this stopping condition also consumes no
privacy budget.
In our implementation, we choose to use a combination
of the above methods. A maximum depth limit is used. In
addition, the algorithm also stops when the region is narrow
enough, and when the noisy count returns a number smaller
than a threshold (we experimented with 1, 3, 5, 10). In our
experiments, we evaluate the choice of the threshold on the
utility of the resulting dataset.
Summarizing the resulting partitions. When the stop-
ping condition is reached, we need to summarize the tuples
in the region R. To do this, we need to determine how many
tuples to generate in the region and then generate these tu-
ples based on the region R.
There are two alternatives to determine how many tuples
to generate in the region. The ﬁrst is to generate based on
the depth and the size of the dataset. For example, if the
current depth is d, and the dataset size is N , we generate
N/2d tuples in R. This approach consumes no privacy bud-
get. The other approach is to issue another count query to
get the number c of tuples that are in R, which can be an-
swered using all remaining privacy budget, as this is the last
query along this path.
To generate a tuple in R, a natural approach is to gener-
ate each attribute independently from other attributes. For
an ordered attribute, several alternatives exist. One could
sample a random value in the region, choose the mid point
(mean) between the two boundary values, and in some cases
(such as when dealing with node degree sequences in Sec-
tion 5) choosing one end of the region turns out to be the
best approach. For a categorical attribute, we simple use
the set of all values in the region. Alternatively, for each
attribute, one could randomly choose a value in the range.
Allocating the privacy budget. Many strategies can
be applied in allocating the privacy budget among diﬀerent
queries, depending on whether partitioning, checking stop-
ping condition, and summarizing need to consume the pri-
vacy budget, and the relative importance of diﬀerent sub-
routines for an application. In our experiments, we choose
to leave half the privacy budget for the ﬁnal query needed to
summarize a leaf region, in order to get a reasonably accu-
rate count, and the other half for partitioning and checking
stopping condition. Other alternatives are possible. An-
other intriguing idea is how to split the privacy budget for
each level between partitioning and checking stopping con-
dition.
It may be beneﬁcial that on shallower levels, one
does not check for stopping conditions and use all budget
D
M
E
 25
 20
 15
 10
 5
 0
depth = 5
depth = 10
depth = 20
depth = 50
depth = 100
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
Epsilon
(a) Depth
D
M
E
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
count = 3
count = 5
count = 10
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
Epsilon
(b) Noisy Stopping Count
Figure 1: Utility of a single attribute under diﬀer-
ent instantiations of RPS that diﬀer on ǫ, depth, and
stopping count. The graphs plot ǫ vs Earth Mover’s Dis-
tance to the original distribution. Smaller values are better.
for partitioning.
The relative eﬀectiveness of diﬀerent instantiations of RPS
algorithms likely depends on the input dataset and the pri-
vacy budget at hand. In a data publishing scenario, the data
curator may want to experiment with diﬀerent settings and
choose the most desirable output to publish.
4. ANONYMIZING
SIONAL DATA
MULTIDIMEN-
We evaluate RPS on multidimensional data. A multidi-
mensional dataset, D, is a set of records. Each record con-
sists of a list of values corresponding to a set of attributes,
or dimensions. The applications of this for such types of
data are wide and varied and include Census data, medi-
cal records, and many others. In this section, we ﬁrst show
the general applicability of our approach by anonymizing a
synthetic dataset under diﬀerent instantiations of the RPS
algorithm. We then apply the framework to a general cen-
sus dataset: the UCI Machine Learning Adult dataset [1],
which has been used in several studies on privacy preserv-
ing data publishing. We study the eﬃcacy of our approach
by examining characteristics of the anonymized dataset as
compared to the original and by measuring accuracy of data
mining workloads. We repeat each experiment 5 times and
report the mean with the standard error.
4.1 Results on Synthetic Data
To synthesize data, we sampled points from a predeﬁned
space. We ﬁrst set to evaluate the ability of the algorithm to
preserve the distribution of each attribute (dimension) in the
data. Hence, we sampled a set of 10,000 points in R from a
normal distribution with µ = 50 and σ = 25. This is similar
to the experimental approach used to evaluate Mondrian
[20]. To evaluate how well the distribution is preserved, we
measured the Earth Mover’s Distance between the original
dataset and the anonymized counterpart.
The Earth Mover’s Distance (EMD) is also referred to as
the 1st Mallows distance or 1st Wasserstein distance. Given
two empirical probability distributions, the EMD measures
the minimum cost of turning one distribution into another.
Informally, we can view one distribution as a mass of dirt
over the space, and the other as a collection of holes in the
same space. EMD measures the least amount of work needed
to ﬁll the holes with dirt; where work refers to the amount
of dirt times the distance by which it is moved. EMD can
be calculated by solving the transportation problem. More
formally, for two sorted numerical datasets X and Y in R,
y
t
i
r
a
l
i
m
S
i
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65
 0.6
depth = 5
depth = 10
depth = 20
depth = 50
depth = 100
 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5
Epsilon
(a) Depth
y
t
i
r
a
l
i
m
S
i
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65
 0.6
count = 3
count = 5
count = 10
 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5
Epsilon
y
t
i
r
a
l
i
m
S
i
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7