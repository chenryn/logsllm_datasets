[-report]//在终端上显示文件系统的基本信息
[-safemode enter|leave|get|wait]//Hadoop的安全模式及相关维护；在安全模式中系统是只读的，数据块也不可以删除或复制
[-refreshNodes][-finalizeUpgrade]//重新读取hosts和exclude文件，将新的被允许加入到集群中的DataNode连入，同时断开与那些从集群出去的DataNode的连接
[-upgradeProgress status|details|force]//获得当前系统的升级状态、细节，或者强制执行升级过程
[-metasave filename]//保存NameNode的主要数据结构到指定目录下
[-setQuota＜quota＞＜dirname＞……＜dirname＞]//为每个目录设定配额
[-clrQuota＜dirname＞……＜dirname＞]//清除这些目录的配额
[-setSpaceQuota＜quota＞＜dirname＞……＜dirname＞]//为每个目录设置配额空间
[-clrSpaceQuota＜dirname＞……＜dirname＞]//清除这些目录的配额空间
[-help[cmd]]//显示命令的帮助信息
fsck。fsck在HDFS中被用来检查系统中的不一致情况。比如某文件只有目录，但数据块已经丢失或副本数目不足。与Linux不同，这个命令只用于检测，不能进行修复。其使用方法如下：
hadoop fsck[GENERIC_OPTIONS]＜path＞[-move|-delete|-openforwrite][-files
[-blocks[-locations|-racks]]]
//＜path＞检查的起始目录
//-move移动受损文件到/lost+found
//-delete删除受损文件
//-openforwrite在终端上显示被写打开的文件
//-files在终端上显示正被检查的文件
//-blocks在终端上显示块信息
//-location在终端上显示每个块的位置
//-rack显示DataNode的网络拓扑结构图
fs：fs可以说是HDFS最常用的命令，这是一个高度类似Linux文件系统的命令集。你可以使用这些命令查看HDFS上的目录结构文件、上传和下载文件、创建文件夹、复制文件等。其使用方法如下：
hadoop fs[genericOptions]
[-ls＜path＞]//显示目标路径当前目录下的所有文件
[-lsr＜path＞]//递归显示目标路径下的所有目录及文件（深度优先）
[-du＜path＞]//以字节为单位显示目录中所有文件的大小，或该文件的大小（如果目标为文件）
[-dus＜path＞]//以字节为单位显示目标文件大小（用于查看文件夹大小）
[-count[-q]＜path＞]//将目录的大小、包含文件（包括文件）个数的信息输出到屏幕（标准stdout）
[-mv＜src＞＜dst＞]//把文件或目录移动到目标路径，这个命令允许同时移动多个文件，但是只允许移动到一个目标路径中，参数中的最后一个文件夹即为目标路径
[-cp＜src＞＜dst＞]//复制文件或目录到目标路径，这个命令允许同时复制多个文件，如果复制多个文件，目标路径必须是文件夹
[-rm[-skipTrash]＜path＞]//删除文件，这个命令不能删除文件夹
[-rmr[-skipTrash]＜path＞]//删除文件夹及其下的所有文件
[-expunge]
[-put＜localsrc＞……＜dst＞]//从本地文件系统上传文件到HDFS中
[-copyFromLocal＜localsrc＞……＜dst＞]//与put相同
[-moveFromLocal＜localsrc＞……＜dst＞]//与put相同，但是文件上传之后会从本地文件系统中移除
[-get[-ignoreCrc][-crc]＜src＞＜localdst＞]//复制文件到本地文件系统。这个命令可以选择是否忽视校验和，忽视校验和下载主要用于挽救那些已经发生错误的文件
[-getmerge＜src＞＜localdst＞[addnl]]//将源目录中的所有文件进行排序并写入目标文件中，文件之间以换行符分隔
[-cat＜src＞]//在终端显示（标准输出stdout）文件中的内容，类似Linux系统中的cat
[-text＜src＞]
[-copyToLocal[-ignoreCrc][-crc]＜src＞＜localdst＞]//与get相同
[-moveToLocal[-crc]＜src＞＜localdst＞]
[-mkdir＜path＞]//创建文件夹
[-setrep[-R][-w]＜rep＞＜path/file＞]//改变一个文件的副本个数。参数-R可以递归地对该目录下的所有文件做统一操作
[-touchz＜path＞]//类似Linux中的touch，创建一个空文件
[-test-[ezd]＜path＞]//将源文件输出为文本格式显示到终端上，通过这个命令可以查看TextRecordInputStream（SequenceFile等）或zip文件
[-stat[format]＜path＞]//以指定格式返回路径的信息
[-tail[-f]＜file＞]//在终端上显示（标注输出stdout）文件的最后1kb内容。-f选项的行为与
Linux中一致，会持续检测新添加到文件中的内容，这在查看日志文件时会显得非常方便
[-chmod[-R]＜MODE[，MODE]……|OCTALMODE＞PATH……]//改变文件的权限，只有文件的所有者或是超级用户才能使用这个命令。-R可以递归地改变文件夹内的所有文件的权限
[-chown[-R][OWNER][：[GROUP]]PATH……]//改变文件的拥有者，-R可以递归地改变文件夹内所有文件的拥有者。同样，这个命令只有超级用户才能使用
[-chgrp[-R]GROUP PATH……]//改变文件所属的组，-R可以递归地改变文件夹内所有文件所属的组。这个命令必须是超级用户才能使用
[-help[cmd]]//这是命令的帮助信息
在这些命令中，参数＜path＞的完整格式是hdfs：//NameNodeIP：port/，比如你的NameNode地址是192.168.0.1，端口是9000，那么，如果想访问HDFS上路径为/user/root/hello的文件，则需要输入的地址是hdfs：//192.168.0.1：9000/user/root/hello。在Hadoop中，如果参数＜path＞没有NameNodeIP，那么会默认按照core-site.xml中属性fs.default.name的设置，附加“/user/你的用户名”作为路径，这是为了方便使用以及对不同用户进行区分。
9.8 WebHDFS
本章前面的部分讲解了HDFS相关的内容，重点集中在如何使用shell下Hadoop的命令和HDFS的Java API来管理HDFS。这一小节将讲解Hadoop 1.0版本中新增加的WebHDFS，即通过Web命令来管理HDFS。
 9.8.1 WebHDFS的配置
WebHDFS的原理是使用curl命令向指定的Hadoop集群对外接口发送页面请求，Hadoop集群的网络接口接收到请求之后，会将命令中的URL解析成HDFS上的对应文件或者文件夹，URL后面的参数解析成命令、用户、权限、缓存大小等参数。待完成相应的操作之后，将结果发还给执行curl命令的客户端，并显示执行信息或者错误信息。那么要使用WebHDFS，首先就必须在期望使用WebHDFS的客户端安装curl软件包。在Ubuntu下执行简单的apt-get install curl命令，apt包管理器就会自动从系统指定的源地址下载curl并安装。待安装结束之后，在终端输入curl-V可以查看是否安装成功。
在客户端安装好curl软件包之后，还需要修改Hadoop集群的配置，使其开放WebHDFS服务。具体操作是：停止Hadoop所有服务之后，配置hdfs-site.xml中的dfs.webhdfs.enabled, dfs.web.authentication.kerberos.principal, dfs.web.authentication.kerberos.keytab这三个属性为适当的值，其中第一个属性值应配置为true，代表启动webHDFS服务，后面两个代表使用webHDFS时采用的用户认证方法，这里为了简单起见并没有设置，后面的命令也都采用Hadoop的启动用户Ubuntu来发送命令。配置结束之后再启动Hadoop所有的服务，这样就可以使用WebHDFS来管理Hadoop集群了。
9.8.2 WebHDFS命令
上一小节讲了如何配置WebHDFS，这一小节我们将详细介绍WebHDFS命令的组织方式和具体的命令。
1.WebHDFS命令一般形式
在这一部分的开始就讲了WebHDFS实际上是用curl命令来发送管理的命令，所以WebHDFS的命令组织和curl命令组织类似。一般为下面的格式：
curl[-i/-X/-u/-T][PUT]"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?[user.name=＜user＞＆]＆op=＜operation＞＆[doas=＜user＞]……"
在这个命令里面，引号前面的部分是curl自己的参数，需要大家自行了解；后面网页形式的内容代表着操作的指令、参数和路径。其中http：//＜HOST＞：＜PORT＞。代表需要将命令发送的地址和端口，也就是Hadoop集群服务器的IP地址和HDFS端口（默认是50070）。在这个地址之后的部分/webhdfs/v1/＜PATH＞代表着需要操作的远程HDFS集群上的路径，比如/webhdfs/v1/user/ubuntu/input，就代表着HDFS上/user/ubuntu/input这个目录。引号中再往后的内容就是操作的指令和参数了，其中最重要的是op参数，代表着具体的操作指令，接下来的内容我们会详细讲解。
2.文件和路径操作
创建文件并写入内容：
c u r l-i-X P U T"h t t p：//＜H O S T＞：＜P O R T＞/w e b h d f s/v 1/＜P A T H＞?o p=C R E A T E overwrite=＜true|false＞][＆blocksize=＜LONG＞][＆replication=＜SHORT＞]
[＆permission=＜OCTAL＞][＆buffersize=＜INT＞]"
使用上述命令之后，会返回一个location，它包括了已创建文件所在的DataNode地址及创建路径。下面就可以将文件内容发送到所显示DataNode对应路径下的文件内，命令如下：
curl-i-X PUT-T＜LOCAL_FILE＞"http：//＜DAtANODE＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=CREATE……"
文件追加内容，首先使用下面的命令获取待追加内容文件所在的地址：
curl-i-X POST"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=APPEND[＆buffersize=＜INT＞]"
再结合返回内容的location信息，追加内容，命令如下：
curl-i-X POST-T＜LOCAL_FILE＞"http：//＜DAtANODE＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=APPEND……"
打开并读取文件内容，使用下面的命令打开远程HDFS上的文件并读取内容：
curl-i-L"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=OPEN
[＆offset=＜LONG＞][＆length=＜LONG＞][＆buffersize=＜INT＞]"
需要注意的是，这个命令首先会返回文件所在的location信息，然后打印文件的具体内容。
创建文件夹：
curl-i-X PUT"http：//＜HOST＞：＜PORT＞/＜PATH＞?op=MKDIRS[＆permission=＜OCTAL＞]"
重命名文件夹或文件：
curl-i-X PUT"＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=RENAME＆destination=＜PATH＞"
删除文件夹或者文件：
curl-i-X DELETE"http：//＜host＞：＜port＞/webhdfs/v1/＜path＞?op=DELETE
[＆recursive=＜true|false＞]"
查看文件夹或文件信息：
curl-i"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=GETFILESTATUS"
列举文件夹内容：
curl-i"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=LISTSTATUS"
3.其他文件系统操作
获取文件夹统计信息：
curl-i"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=GETCONTENTSUMMARY"
这个命令主要返回一下文件夹信息：文件夹个数、文件个数、总字长和总大小等。
获取文件校验和：
curl-i"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=GETFILECHECKSUM"
主要返回校验算法、校验字符串和字符串长度。
获取当前web用户的主目录：
curl-i"http：//＜HOST＞：＜PORT＞/webhdfs/v1/?op=GETHOMEDIRECTORY"
设置权限：
curl-i-X PUT"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=SETPERMISSION
[＆permission=＜OCTAL＞]"
设置文件夹或文件属主属性：
curl-i-X PUT"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=SETOWNER
[＆owner=＜USER＞][＆group=＜GROUP＞]"
设置备份数量：
curl-i-X PUT"http：//＜HOST＞：＜PORT＞/webhdfs/v1/＜PATH＞?op=SETREPLICATION
[＆replication=＜SHORT＞]"
4.常见错误
在使用WebHDFS时经常会抛出一些异常，但是从异常的信息大体都能分析出问题所在，下面介绍几种常见的异常和分析。
（1）Illegal Argument Exception
这种异常出现的返回信息如下：
HTTP/1.1 400 Bad Request
Content-Type：application/json
Transfer-Encoding：chunked
{
"RemoteException"：
{
"exception"："IllegalArgumentException"，
"javaClassName"："java.lang.IllegalArgumentException"，
"message"："Invalid value for webhdfs parameter\"permission\"：……"
}
}
从异常信息可以很明显看出是命令的参数不对，这就需要用户仔细检查自己的参数是否有输入错误或拼写错误。
（2）Security Exception
这种异常出现的返回信息如下：
HTTP/1.1 401 Unauthorized
Content-Type：application/json
Transfer-Encoding：chunked
{
"RemoteException"：
{
"exception"："SecurityException"，
"javaClassName"："java.lang.SecurityException"，
"message"："Failed to obtain user group information：……"
}
}
出现这种异常的原因是提交命令的用户应通过认证，这就需要用户先提交认证信息。
（3）File Not Found Exception
这种异常出现的返回信息如下：
HTTP/1.1 404 Not Found
Content-Type：application/json
Transfer-Encoding：chunked
{