induce loss of packets destined to services and applications,
and packet loss also degrades the efﬁcacy of the monitoring
system itself, so we consider these solutions unacceptable.
Moreover, these strawman solutions do not achieve the full
expressivity of our event deﬁnitions: for example, they do
not track losses and bursts that require keeping per ﬂow state
(Section 3). Modifying them to do so would add complexity,
which might result in much higher losses at line rates.
Design Requirements. These results and the strawman de-
signs help identify several requirements for TPM design.
First, both match-ﬁrst and match-later, even with a state-of-
the-art matching implementation, induce loss because the
overhead of matching the packet to a ﬁlter often exceeds
the 70ns budget available to process a single packet at the
rate of 14.8 Mpps 64-byte packets on a 10G NIC. This im-
plies that more efﬁcient per-packet processing (phase one) is
necessary. We also impose the requirement that the system
should degrade gracefully under DoS attacks, since an attack
can disrupt monitoring and attacks on cloud providers are
not uncommon [39]. Moreover, with match-ﬁrst, any delay
in processing a packet will add queueing delay to process-
ing subsequent packets: a monitoring system should, ideally,
impose small and bounded delay.
Second, we observed that match-ﬁrst scales worse than
match-later because it incurs 3× higher TLB and 30% higher
cache misses (match-later exhibits much more locality be-
cause it performs all packet related actions at once). Thus,
cache and TLB efﬁciency is a crucial design requirement for
being able to scale trigger processing to full line rate.
5.2 Our Approach
In Trumpet, TPM splits the monitoring functions into the
following two phases: (a) Match and scatter in which in-
coming packets are matched to a 5-tuple ﬂow5 (the ﬁnest
flow_granularity speciﬁcation allowed), which stores
per packet counts/other statistics per ﬂow (this scatters statis-
tics across ﬂows), and (b) Gather-test-and-report, which runs
at the speciﬁed trigger time granularity, gathers the statistics
to the right flow_granularity, evaluates the predicate and
reports to TEM when the predicate evaluates to true.
Partitioning the processing in this manner enables Trumpet
to satisfy the requirements discussed in the previous subsec-
tion:
• As we discuss below, this partitioning permits the design
of data structures that promote cache and TLB efﬁciency,
which, as we discuss above, is crucial for performance. Fur-
thermore, the balance of CPU overhead between the two
phases permits efﬁcient packet processing, without com-
promising expressivity: in the ﬁrst phase, we can minimize
matching overhead by caching lookups.
• Small and bounded delays can be achieved by co-
operatively scheduling these phases (which avoids
the second phase is queue-
synchronization overhead):
adaptive and runs only when the NIC queue is empty.
• Match and scatter per 5-tuple ﬂows allows us to track per
ﬂow statistics such as loss and burst and separating that
from the gather-test-and-report phase let us compute the
statistics once and share these among multiple triggers
matching the same ﬂow, but at different ﬂow granularities.
• This partitioning also localizes the two processing bot-
tlenecks in the system: packet processing and gathering
statistics. As we discuss later, this allows us to design
safeguards for the system to degrade gracefully under at-
tacks, avoiding packet loss completely while maintaining
the ﬁdelity of statistics gathering.
5.3 Data Structures in Trumpet
Trumpet uses four data structures (Figure 3): a ﬂow table
to store statistics of per-packet variables for the ﬂow, a trigger
repository contains all the triggers, a trigger index for fast
matching, and a ﬁlter table for DoS-resilience. We describe
the latter two data structures in detail later.
The ﬂow table is a hash table, keyed on the ﬂow’s 5-tuple,
that keeps, for each ﬂow, only the statistics required for the
triggers that match the ﬂow (Figure 3). Thus, for example, if
5Unlike match-ﬁrst, which matches against triggers deﬁned at multiple
granularities.
[52], which is also used in Open vSwitch [46]. The tuple
search algorithm uses the observation that there are only
a limited number of patterns in these wildcard ﬁlters (e.g.,
only 32 preﬁx lengths for the IP preﬁx). The trigger index
consists of multiple hash tables, one for each pattern, each
of which stores the ﬁlters and the corresponding triggers for
each ﬁlter. Searching in each hash table involves masking
packet header ﬁelds to match the hash table’s pattern (e.g.,
for a table deﬁned on a /24 preﬁx, we mask out the lower 8
bits of the IP address), then hashing the result to obtain the
matched triggers. Tuple search memory usage is linear to the
number of triggers, and its update time is constant.
Performance Optimizations. Since packet processing im-
poses a limited time budget, we use several optimizations
to reduce computation and increase cache efﬁciency. For
performance, software switches often read a batch of packets
from the NIC. When we process this batch, we use two forms
of cache prefetching to reduce packet processing delay: (1)
prefetching packet headers to L1 cache (lines 2-3) and (2)
prefetching ﬂow table entries (lines 4-7). Data center appli-
cations have been observed to generate a burst of packets on
the same ﬂow [31], so we cache the result of the last ﬂow
table lookup (lines 5, 9). To minimize the impact of TLB
misses, we store the ﬂow table in huge pages. In Section 7, we
demonstrate that each optimization is critical for Trumpet’s
performance.
5.5 Phase 2: Gather, Test, and Report
This phase gathers all statistics from the ﬂow table entries
into the flow_granularity speciﬁed for each trigger (re-
call that the ﬂow-table stores statistics at 5-tuple granularity,
but a trigger may be deﬁned on coarser ﬂow granularities,
like dstIP/24). Then, it evaluates the predicate, and reports
all the predicates that evaluate to true to the TEM.
The simplest implementation, which runs this entire phase
in one sweep of triggers, can result in large packet delays or
even packet loss, since packets might be queued or dropped
in the NIC while this phase is ongoing. Scheduling this phase
is one of the trickier aspects of Trumpet’s design.
At a high-level, our implementation works as follows.
Time is divided into epochs of size T , which is the great-
est common factor of the time granularities of all the triggers.
Trumpet supports a T as small as 10 milliseconds. In each
ﬂow table entry, we double-buffer the statistics (like volumes,
loss, burst, etc.): one buffer collects statistics for the odd-
numbered epochs, another for even-numbered epochs. In the
i-th epoch, we gather statistics from the i− 1-th epoch. Thus,
double-buffering gives us the ﬂexibility to interleave trigger
sweeps with packet processing.
We schedule this gathering sweep in a queue-adaptive
fashion (Algorithm 2). When the NIC queue is empty, we
run a sweep step for a bounded time (Algorithm 3). Because
of Trumpet’s careful overall design, it is always able to stay
ahead of incoming packets so that these sweeps are never
starved (Section 7). This bound determines the delay imposed
by the measurement system, and can be conﬁgured. In our
experiments, we could bound delay to less than 10 µs (Figure
4). Each invocation of this algorithm processes some number
Figure 3: Two-stage approach in Trumpet
a ﬂow only matches a single trigger whose predicate is ex-
pressed in terms of volume (payload size), the ﬂow table does
not track other per-packet variables (loss and burst, round-trip
times, congestion windows, etc.). The variables can be shared
among triggers, and the TEM tells the TPM which variables
to track based on static analysis of event descriptions at trig-
ger installation. The ﬂow table maintains enough memory to
store most per-packet statistics, but some, like loss and burst
indicators are stored in dynamically allocated data structures.
Finally, the TPM maintains a statically allocated overﬂow
pool to deal with hash collisions. The trigger repository not
only contains the deﬁnition and state of triggers, but also
tracks a list of 5-tuple ﬂows that match each trigger. In a later
section, we discuss how we optimize the layout of these data
structures to increase cache efﬁciency.
foreach Packet p do
prefetch(p)
foreach Packet p do
if p.ﬂow != lastpacket.ﬂow then
Algorithm 1: Processing packets in Trumpet
1 Function processPackets(packetBatch)
2
3
4
5
6
7
8
9
10
11
12
13
14
p.hash = calculateHash()
prefetchFlowEntry(p)
e = ﬂowTable.ﬁnd(p)
if e == NULL then
if p.ﬂow != lastpacket.ﬂow then
foreach Packet p do
e = ﬂowTable.add(p)
triggers = triggerMatcher.match(p)
e.summaryBitarray =
bitarray(triggers.summaries.id)
15
16
17
18
if e.lastUpdate < epoch then
e.resetSummaries()
e.updateSummaries(p)
forwardPacket(p)
5.4 Phase 1: Match and Scatter
In this phase, Algorithm 1 runs over every packet. It looks
up the packet’s 5-tuple in the ﬂow-table and updates the
per-packet statistics. If the lookup fails, we use a matching
algorithm (lines 11-14) to match the packet to one or more
triggers (a ﬂow may, in general, match more than one trigger).
trigger matching using tuple search. Matching
Fast
a packet header against trigger ﬁlters is an instance of
multi-dimensional matching with wildcard rules. For this,
we build a trigger index based on the tuple search algorithm
StatisticsbitarrayTrigger repositoryTrigger1: filter, time-interval, flows, state, ...1st packetof a flowTrigger2: filter, time-interval, flows, state, ...Triggern: filter, time-interval, flows, state, ...Statistics bufferFlow table5 tupleStatistics bitarrayLast resetPeriodic updatePacketsElevated flowFilter tablePhase 1Phase 2Trigger indexFlowsif time to sweep then
Algorithm 2: Trumpet main loop
1 Function mainLoop(timeBudget)
2
3
4
5
6
7
if last sweep is not ﬁnished then
while Packets in NIC queue do
sweep(timeBudget)
startSweep()
processPackets(batches of 16 packets)
of triggers from the trigger repository (lines 4-5 in Algorithm
3). This processing essentially gathers all the statistics from
the ﬂow entries (recall that each trigger entry has a list of
matched ﬂow entries).
ﬂowNum = processFlowList(t)
t.ﬂowNum += ﬂowNum
b = b - (t.avgUpdateTime × ﬂowNum)
if b ≤0 then
if passedTime ≥ timeBudget then
foreach t.FlowList do
b = timeBudget / 10
foreach t = nextTrigger() do
Algorithm 3: Periodic triggers sweeping
1 Function sweep(timeBudget)
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
reset(t)
t.lastUpdate = epoch
updateAvgUpdateTimes(passedTime)
if t.condition(t) then
b = timeBudget / 10
report(t)
saveSweepState()
updateAvgUpdateTime(passedTime)
return
if epoch - t.lastUpdate ≥ t.timeInterval then
Once all the ﬂows for a trigger have been processed, the
algorithm tests the predicate and reports to the TEM if the
predicate evaluates to true (lines 14-18 in Algorithm 3). How-
ever, while processing a trigger, the processing bound may
be exceeded: in this case, we save the sweep state (line 10),
and resume the sweep at that point when the NIC queue is
next empty. For each trigger, during a sweep step, instead
of computing elapsed time after processing each ﬂow entry,
which can be expensive (∼100 cycles), we only compute the
actual elapsed time when the estimated elapsed time exceeds
a small fraction of the budget (lines 7-13). The estimate is
based on the number of ﬂows processed for this trigger so far.
Our approach assumes we can complete sweeping all of the
triggers within one epoch (10ms): in Section 7.2, we demon-
strate that we can achieve this for 4K triggers on one core
for a 10G NIC with small packets at full line rate even under