from the original ones. This puts Prεεch’s effectiveness at
the risk of the arms race between the voice cloning system’s
performance and the adversary’s strength. This limitation is
addressed by voice conversion at the cost of transcription
utility. We quantify these utility-privacy trade-offs in Sec. 7.
Number of CSPs used for transcription: As discussed
above, employing multiple CSPs lowers the monetary cost
incurred. However, as shown in Table 1, AWS has a higher
WER than Google. Hence, using both the CSPs results in
lower overall utility than just using Google’s cloud service.
4.6 Voice Conversion
Below, we discuss the two main categories of VC systems,
highlighting their privacy-utility trade-offs.
4.6.1 One-to-One Voice Conversion
One-to-one VC maps a predeﬁned source speaker voice to a
target speaker voice. In Prεεch, we use sprocket [21], which
is based on spectral conversion using a Gaussian mixture
USENIX Association
29th USENIX Security Symposium    2711
an enrollment phase to get the source speaker’s voice sam-
ples, thereby limiting the scalability of Prεεch for previously
unseen speakers. Second, one-to-one VC does not provide per-
fect indistinguishability. These two limitations are mitigated
by applying many-to-one VC (Sec. 7.4.1).
Figure 4: An illustration of the many-to-one VC pipeline.
5 End-to-End Threat Analysis
model (GMM). Sprocket’s training phase takes three steps: (1)
acoustic features extraction of the source and target speakers
samples, (2) time-alignment of the source and target features,
and (3) GMM model training. During conversion, sprocket
extracts the acoustic features of the new utterances, converts
them using the learned GMM model, and generates the target
waveform. Prεεch applies sprocket to convert the voice of all
source speakers, including the synthesized dummy segments,
into the same target speaker voice.
4.6.2 Many-to-One Voice Conversion
For perfect voice privacy, the VC system should (1) map any
voice (even if previously unseen) to the same target voice, (2)
not leak any distinguishing acoustic features, and (3) operate
on speech containing multiple speakers. To this end, Prεεch
deploys the two-stage many-to-one VC [44] mechanism. As
shown in Fig. 4, the ﬁrst stage is a phoneme classiﬁer that
transfers the speech utterance into phonetic posterior grams
(PPG) matrix. A PPG is a time-aligned phonetic class [44],
where a phoneme is the visual representation of a speech
sound. Thus, the phoneme classiﬁer removes the speaker-
identifying acoustic features by mapping the spoken content
into speaker-independent labels. In the second stage, a speech
synthesizer converts the PPGs into the target voice.
The PPGs intermediate stage is irreversible and speaker-
independent. It guarantees that the converted dummy seg-
ments Sd and converted original segments S cannot be distin-
guished from each other. However, the actual implementation
of the system carries many challenges. The ﬁrst stage is a per-
formance bottle-neck as it needs large phonetically aligned
training data to generalize to new unseen voices. We over-
come this challenge by generating a custom training speech
dataset with aligned phonemes as described in Sec. 6.
4.6.3 Control Knobs
The aforementioned VC techniques present an interesting
utility-usability-privacy trade-off. The one-to-one VC tech-
nique gives better accuracy than many-to-one VC since it is
trained for a speciﬁc predeﬁned set of source speakers (de-
tails in Sec. 7.4.1). However, this utility gain comes at the
price of usability and privacy. First, unlike many-to-one VC,
sprocket needs parallel training data – a set of utterances spo-
ken by both the source and target speakers. Hence, it requires
In this section, we go over the end-to-end system design of
Prεεch and identify potential privacy vulnerabilities.
Voice Privacy: Many-to-one VC removes all the iden-
tifying features from S, like the speakers’ voices, background
noise, and recording hardware, thereby protecting voice
privacy.
Textual Privacy: For sensitive word scrubbing,
the
best-case scenario from a privacy point of view is to have the
user spell out the entire keyword list. However, due to its
high usability overhead, Prεεch uses NER instead to identify
named entities automatically from T OSP
. In Sec. 7.3.1, we
empirically show that Prεεch can achieve near-perfect true
positive rate in identifying the segments containing sensitive
words. However, this is only an empirical result and is dataset
dependent.
S
Our main defense against statistical analysis on the text
is the DP guarantee on the word histogram. This DP guar-
antee would break down if the adversary can distinguish the
dummy segments from the true segments. Many-to-one VC
technique, by design, ensures that both sets of segments have
the same acoustic features. However, the possibility of dis-
tinguishing them based on their textual features still remains.
To address this threat, we rely on state-of-the-art NLP models
with low perplexity (log-likelihood) scores to generate the
dummy text corpus. The low perplexity scores ensure that
the auto-generated text is as close as possible to the natural
language generated by humans [19, 36]. Although there is no
formal guarantee about the adversary’s ability to distinguish
dummy and true segments based on their textual features,
we have empirically analyzed this threat in Sec. 7.3.3 and
Sec. 7.3.4. We leverage state-of-the-art NLP techniques to
mount attacks on the dummy segments. Our results show that
the adversary fails to distinguish between the dummy and true
segments. However, the extent of such robustness is based on
the efﬁcacy of state-of-the-art NLP techniques.
Word correlations can also weaken the DP guarantee (d−w,
if w is the maximum size of word groups with high correla-
tion). This can be addressed by either increasing d or consid-
ering n-gram (n = w) word histograms. However, this would
increase the requisite amount of dummy segments.
Long segments can also be a source of privacy vulnerability
as each segment contains more contextual information. Hence,
in the prototype Prεεch presented in the paper, we use short
segments that contain at most two non-stop words.
2712    29th USENIX Security Symposium
USENIX Association
PhonemeClassifierphnts(ms)zh0ih20s40sil60ih100z115Speech SynthesisSourcespeakerTargetspeakerphonemesAnother weakness is related to vocabulary estimation, espe-
cially if some of the distribution-tail words are deemed to be
sensitive. Prεεch provides no formal guarantees on the words
that do not belong to V . Although our empirical evaluation
shows that the OSP has a very high accuracy for the weighted
estimation of V (Sec. 7.3.2), some sensitive distribution-tail
words might still be missed due to the OSP’s transcription
errors. Additionally, our formal DP guarantee holds only for
the word histogram (BOW ) on V . Textual analysis models
other than BOW are empirically evaluated in Sec. 7.3.3 and
Sec. 7.3.4.
Finally, if the CSP can reorder the segments (even partially
since the speech ﬁle it receives contains dummy segments
as well), it will be able to distinguish the dummy segments
from the true ones and hence, learn the textual content of the
ﬁle. For this again, we show empirically that current NLP
techniques fail to reorder the segments (Sec. 7.3.4) even in
the worst-case setting where all the segments go to one CSP.
However, as before, this is an empirical result only.
Formal Privacy Guarantee: For a speech ﬁle S, Prεεch pro-
vides perfect voice privacy (when using many-to-one VC) and
an (ε,δ)-DP guarantee on the word histogram for the vocabu-
lary considered (BOW ), under the assumption that the dummy
segments are indistinguishable from the true segments.
6 Implementation
In this section, we describe the implementation details of
Prεεch’s building blocks (shown in Fig. 1).
Segmentation: We implement the two-level hierarchical seg-
mentation algorithm described in Sec. 4.3. The silence detec-
tion based segmentation is implemented using the Python py-
dub package7. We used Praat8 to extract the pitch information
required for the second level of the segmentation algorithm.
Sensitive Keyword Scrubbing: We use the NLP Python
framework spaCy 9 for named entity recognition (NER) from
the text. The keyword lists per each dataset can be found in
the full paper [3] . We employ PocketSphinx10 for keyword
spotting, a lightweight ASR that can detect keywords from
continuous speech. It takes a list of words (in the text) and
their respective sensitivity thresholds and returns segments
that contain speech matching the words. PocketSphinx is
a generic system that can detect any keyword speciﬁed in
runtime; it is not trained on a pre-deﬁned list of keywords and
requires no per-user training or enrollment.
Generating Dummy Segments: We use the open source
implementation 11 of OpenAI’s state-of-the-art NLP language
model, GPT 2 [36], to generate the noise corpus.
7https://pypi.org/project/pydub/
8http://www.fon.hum.uva.nl/praat/
9https://github.com/explosion/spaCy
10https://github.com/cmusphinx/pocketsphinx
11https://github.com/huggingface/transformers
Using this predictive model, we generate a large corpus
representing the vocabulary of the evaluation datasets. An
example of the generated text is available in the full paper [3] .
To generate the dummy segments, we segment each document
at the same level as the speech segmentation algorithm. We
build a hash table associating each vocabulary word with the
segments that contain it. Prεεch uses a dummy segment only
once per CSP to prevent it from identifying repetitions.
Text-to-Speech: We use the multi-speaker (voice cloning)
TTS synthesizer [20] to generate the speech ﬁles correspond-
ing to the dummy segments. We use a pre-existing system
implementation and pretrained models 12.
One-to-One Voice Conversion: We use the open-source
sprocket software 13. As described in Sec. 4.6.1, sprocket
requires a parallel training data and the target voice should
be uniﬁed for all source speakers. For the VCTK datasets, we
use speaker p306 as the target voice. Since we also evaluate
Prεεch on non-standard datasets (Facebook and Carpenter
cases), we had to construct the parallel training data for their
source speakers. For this, we use TTS to generate the required
target voice training utterances in a single synthetic voice.
Many-to-One Voice Conversion: We utilize pre-existing ar-
chitectures and hyperparameters 14 for the two-stage many-to-
one VC [44] mechanism, shown in Fig. 4. The ﬁrst network,
net1, is trained on a set of {raw speech, aligned phoneme
labels} samples from a multi-speaker corpus, where the la-
bels are the set of 61 phonemes from the TIMIT dataset. The
only corpus that has a manual transcription of speech to the
phonemes’ level is the TIMIT dataset – a limited dataset. We
found that training net1 on TIMIT alone results in an infe-
rior WER performance. For better generalization, we augment
the training set by automatically generating phoneme-aligned
transcriptions of standard ASR corpora. We use the Montreal
Forced Aligner 15 to generate the aligned phonemes on Lib-
riSpeech and TED-LIUM [38] datasets. The second network,
net2, synthesizes the phonemes into the target speaker’s voice.
It is trained on a set of {PPGs, raw speech} pairs from the
target speaker’s voice. We use the trained net1 to generate the
PPGs data for training net2. As such, we only need speech
samples of the target speaker to train net2. This procedure also
allows net2 to account for net1’s errors. We use Ljspeech16
as the target voice for its relatively large size – 24 hours of
speech from a single female.
7 Evaluation
We evaluate how well Prεεch meets the design objectives of
Sec. 4. Speciﬁcally, we aim to answer the following questions:
12https://github.com/CorentinJ/Real-Time-Voice-Cloning
13https://github.com/k2kobayashi/sprocket
14https://github.com/andabi/deep-voice-conversion
15https://montreal-forced-aligner.readthedocs.io/en/latest/
16https://keithito.com/LJ-Speech-Dataset/
USENIX Association
29th USENIX Security Symposium    2713
Datasets
VCTK p266
VCTK p262
Facebook1
Facebook2
Facebook3
Carpenter1
Carpenter2
Cloning
5.15 (80.73%)
4.53 (71.63%)
8.26 (66.59%)
9.75 (63.36%)
14.93 (51.40%)
14.43 (44.18%)
13.53 (65.93%)
One-to-One
16.55 (38.06%)
7.39 (53.73%)
14.60 (40.94%)
18.27 (31.34%)
23.25 (24.32%)
23.88 (7.62%)
33.71 (15.11%)
Many-to-One
21.92 (17.96%)
10.82 (32.25%)
20.30 (17.88%)
19.44 (26.94%)
27.06 (11.91%)
22.63 (12.46%)
38.90 (2.04%)
OSP
26.72
15.97
24.72
26.61
30.72
25.85
39.71
Table 2: WER (%) of end-to-end Prεεch which represents the accumulative
effect of segmentation, SWS, and different settings of voice privacy and its
relative improvement in (%) over OSP (Deep Speech).
Figure 5: ROC curve for sensitive words detec-
tion at different values of the sensitivity score.
(Q1.) Does Prεεch preserve the transcription utility?
(Q2.) Does Prεεch protect the speakers’ voice biometrics?
(Q3.) Does Prεεch protect the textual content of the speech?
(Q4.) Does the different control knobs provide substantial
ﬂexibility in the utility-usability-privacy spectrum?
We answer the ﬁrst three questions for a prototype imple-
mentation of Prεεch that provides the maximum degree of
formal privacy and hence, the least utility. For evaluating Q4,
we relax the privacy guarantee to obtain utility and usability
improvements.
Prototype Prεεch: For the prototype Prεεch presented in
the paper: (1) segmentation length is adjusted to ensure that
each segment contains at most two non-stop words (2) noisy
segments are generated via the GPT2 language model (3) a
single CSP (Google) is utilized (4) many-to-one VC is applied
to both the dummy and true segments.
7.1 Q1. Transcription Utility
We assess the transcription WER after deploying end-to-end
Prεεch on the non-standard datasets. Recall that Table 1 in
Sec. 2.2 shows the baseline WER performance of the CSP
and OSP before applying Prεεch.
WER Analysis: Column 4 in Table 2 shows the end-to-end
WER for the prototype Prεεch which represents the accu-
mulative effect of segmentation, SWS, and many-to-one VC.
Although VC is the main contributor to Prεεch’s WER, as is
evident from Sec. 7.4.1 and Sec. 7.3.1, there are two main ob-
servations. First, many-to-one VC is superior to Deep Speech.
Speciﬁcally, Prεεch’s relative improvement over Deep Speech
ranges from 11.91% to 32.25% over the evaluation datasets
(except for Carpenter2). Recall that we trained the VC system
using standard ASR corpora, while we evaluate the WER on
non-standard cases. Still, Prεεch’s WER is superior to that
of Deep Speech, which has been trained through hundreds
of hours of speech data. Second, Prεεch does not have the
same performance for all the datasets. This observation arises
again from the lack of diversity in our VC training set. For
example, the speaker in Carpenter 1 speaks loudly, allowing
VC to perform well. On the other hand, the second speaker
(Carpenter 2) is not as clear or loud, which results in an in-
ferior VC performance. This observation is consistent with
Deep Speech as well.
Our experiments show that these results can be improved by
adding samples of the source speaker voice to the training
pipeline of net1 and net2. We chose not to go with this ap-
proach as this limits the usability of the system, and in such a
case sprocket (Sec. 7.4.1) would be a better choice.
7.2 Q2. Voice Biometric Privacy
To test the voice biometric privacy, we conduct two experi-
ments using the voice analysis APIs (details in Sec. 3.1). In
the ﬁrst experiment, we assess the CSP’s ability to separate
speech belonging to different speakers after Prεεch applies
the VC system. On our multi-speaker datasets, IBM diariza-
tion API concludes that there is only one speaker present.
Furthermore, we run the diarization API after adding the
dummy segments (after TTS and VC). Again, the API detects