purpose of our experiment, we need the two forwarders to have
different /24 prefixes but share the same /16 prefix. We find 164
resolvers with appropriate forwarders and thus amenable to this
technique.
Finally, when the resolution path includes hidden resolvers (see
Section 8.2 for details on our hidden resolver detection), we attempt
to use two open forwarders that are behind two hidden resolvers
that are in different /24 prefixes and share a /16 prefix to deliver
120
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Rami Al-Dalky, Michael Rabinovich, and Kyle Schomp
our queries. There are 7 resolvers with suitable hidden resolvers
for this technique.
In summary, across the above cases, we are able to deliver two
successive queries to 203 resolvers such that the resolver treats the
two queries as if they arrived from clients in different /24 but the
same /16 address blocks. In addition, for the 32 resolvers amenable
to our first measurement technique, we can explore their handling
of ECS prefixes of arbitrary lengths. One of these 32 resolvers turned
out to be in Google’s IP address space, although it is not part of
Google Public DNS according to their published list of IP blocks
used for this purpose or observed in the CDN dataset – and was no
longer accessible by the time we wanted to notify Google3.
Overall, of 278 non-Google egress resolvers in the Scan dataset,
there were 76 recursive resolvers that we could not study, including
64 that did not have appropriate forwarders or hidden resolvers
and 12 for which forwarders became unavailable in the time lag
(two weeks) between the scan and the experiment in this section.
6.3.2 Results. Using the above techniques, we conduct two experi-
ments. First, we deliver pairs of successive queries with different /24
but the same /16 ECS prefixes for our own hostname to each recur-
sive resolver, and when they arrive at our authoritative nameserver,
return the scopes /24, /16, and /0. To avoid cached records from one
trial affecting other trials, we use a unique hostname for each trial.
If the recursive resolver honors scope restrictions as prescribed in
the RFC, it will not use the cached record from the first query to
answer the second query for scope /24 but will reuse the cached
record for scope /16 and /0. Thus, with a compliant resolver, we ex-
pect our authoritative nameserver to see both queries for scope/24
and only the first query for scopes /16 and /0. Second, for the 32
resolvers which accept an arbitrary ECS prefix from our queries,
we explore how these resolvers handle ECS prefixes and scopes
that are longer and shorter than 24.
Based on these experiments, we classify the resolvers into one
of the following categories:
• We find 76 resolvers with correct behavior: they honor the
scope from the authoritative answers and never submit ECS
prefixes longer than 24 to authoritative nameserver, even when
the resolvers accept arbitrary ECS prefixes from the clients, and
even when these prefixes are longer than /24 (in which case
they truncate the excessive bits).
This is proper behavior as, according to the RFC recommenda-
tion, recursive resolvers should not convey more than 24 bits
in ECS prefixes to preserve the client privacy. The resolvers in
this category include 9 recursive resolvers that accept arbitrary
ECS prefixes. For these resolvers, we are able to also test that
they enforce an RFC stipulation that the scope length in the
responses cannot exceed the source prefix length in the query.
3At the time of the study, Google’s anycast front-end did not accept ECS options
from incoming external queries and instead derived ECS prefix from the IP address
of the sender of the query. However, during the preparation of the camera version,
we noticed that Google has returned to its previous behavior reported in [30] in its
interactions with non-whitelisted authoritative nameservers, namely that it passes the
ECS submitted by the external queries along to our authoritative nameserver. Thus,
we could use our first technique to directly measure the behavior of at least some
of Google’s egress resolvers and confirm that they exhibits the same ECS caching
behavior as the resolver reported in the paper, i.e., that it shows the correct ECS
behavior.
121
All 9 resolvers correctly apply scope length 24 to control the
reuse of their cached records, even when we return a greater
scope length. Finally, these 9 resolvers include the one Google
resolver that we can study.
• On the other hand, we find 103 recursive resolvers, or over
half of all recursive resolvers we could study, that don’t control
caching based on scope at all: they reuse cached responses irre-
spective of the clients’ addresses, as if they did not understand
ECS. This is particularly interesting because either (i) recursive
resolver adds ECS to queries but then ignores it or (ii) some
hidden resolver adds ECS and the resolver does not understand
it at all.
• Among the 32 resolvers willing to accept arbitrary ECS prefixes
from the queries sent to them, we find 15 open resolvers accept-
ing ECS prefixes longer than /24, and caching the responses
based on correspondingly longer scopes. This behavior runs
counter to the RFC recommendation on client privacy.
• Conversely, another 8 resolvers among the 32 resolvers accept-
ing arbitrary ECS prefixes cache responses based on subnet
granularity coarser than /24. Specifically, these resolvers impose
the maximum cacheable prefix length of 22. When receiving
queries with source prefix length longer than 22, the recursive
resolver only conveys the first 22 bits to our authoritative name-
server. In addition, they impose scope length 22 to control the
reuse of their cached records even when we return greater scope
length. Such behavior can be lead to highly suboptimal user-to-
edge-server mapping with some CDNs (as we will discuss in
Section 8.3).
• Finally, we find one misconfigured resolver that sends an ECS
prefix from a private address block (10.0.0.0/8) even after receiv-
ing answers from our authoritative nameserver indicating its
ECS support, and even when relaying queries from forwarders
that share with the resolver the /24 address prefix (so there are
no privacy issues). Moreover, from coordinated queries to two
forwarders using this resolver, we observe that this resolver
does not handle the ECS scope properly, as it does not cache
(or does not reuse) responses with scope prefix length zero.
7 ECS IMPACT ON CACHING
ECS facilitates fine-grained server selection by DNS, with responses
tailored to different clients. In doing so, it limits the resolver’s ability
to reuse a cached DNS record to serve multiple clients and thus can
increase the cache size needed to avoid evictions (since multiple
records for the same question can co-exist in cache if the question
came from different client IP prefixes) and reduce the cache hit
rate. This section quantifies these tendencies. Using the Public
Resolver/CDN dataset, we examine the ECS impact on cache size of
a large number of resolvers4 but only considering accesses of the
CDN-accelerated content. Using the All-Names Resolver dataset,
we measure the caching impact of ECS (on both the cache size and
hit rate) due to all DNS queries/responses that carry ECS but only
considering a single resolver.
4As a reminder, while the dataset represents queries from a single major public resolu-
tion service, there are a large number of individual egress resolvers in the dataset. We
consider cache blow up per each such resolver.
A Look at the ECS Behavior of DNS Resolvers
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Figure 1: Distribution of the blow-up factor in the cache size
for various TTL values.
Figure 2: Cache size blow-up factor for random fractions of
the client IP addresses.
7.1 Resolver Cache Size
As mentioned, we expect that when a recursive resolver enables
support for ECS, the size of its cache state will grow. We assess
by how much the cache grows using two datasets that offer com-
plementary perspectives. We use the Public Resolver/CDN dataset,
which records interactions between a major resolution service and
a major CDN, to assess the increase in resolver cache sizes due to
accesses of the CDN-accelerated Internet content. We complement
these results with the analysis of the All-Names Resolver dataset,
which allows us to quantify the increase of a resolver cache size
needed to store responses in all interactions that involve ECS, in-
cluding all authoritative DNS servers that support ECS. We use
trace-driven simulations to conduct these analyses. In our simu-
lations, we assume that the recursive resolvers adhere to the TTL
value returned by the authoritative nameserver (i.e., retain the
records for no longer than the TTL) and do not evict records from
the cache before they expire.
We begin with the Public Resolver/CDN dataset. The public DNS
service in question employs a large number of individual egress
resolvers. Our logs contain 2370 different recursive resolver IP
addresses with varying traffic volume per IP address. We assume
each resolver maintains its own isolated cache (no cache sharing
among the resolvers). The CDN and the public DNS service involved
both support ECS in their DNS interactions, and the dataset includes
the ECS options (both source prefixes of the queries and scope prefix
lengths of the responses) exchanged in the DNS interactions.
To simulate the cache of the resolvers without ECS, we replay the
logs disregarding the ECS information. In other words, we assume
that once a given resolver records the answer for a given query, any
subsequent queries would be answered from the cache, irrespective
of the client, for the duration of the TTL, with only the initial
answer occupying cache space. To simulate the cache with ECS, we
replay the logs while obeying the ECS source and scope prefixes
listed. The authoritative nameservers always return a 20 second
TTL in responses and our simulated recursive resolvers remove
records from cache at that TTL, i.e., 20 seconds after insertion.
For each recursive resolver, we calculate the cache blow-up fac-
tor as the maximum cache size with ECS at any time during the
simulation divided by the maximum cache size without ECS. Fig-
ure 1 shows the CDF of the blow-up factor for all the recursive
resolvers in the “20 Sec. TTL” line. We find that the maximum
cache size blow-up factor is 15.95 and 50% of the resolvers have a
blow-up factor of more than 4, meaning that at peak cache usage
the resolvers held 4x more records from the CDN with ECS than
they would have held without ECS.
Next, we note that TTL also has an impact on cache size. The
CDN records in our simulation have a TTL of 20 seconds but many
DNS records in the wild have longer TTLs. We repeat the simula-
tions using TTLs of 40 and 60 seconds in the remaining lines of
Figure 1. The maximum cache size blow-up factor grows to 23.68
with 40 second TTLs and 29.85 with 60 second TTLs. We anticipate
that the cache size blow-up factor will continue to increase with
TTL values greater than 60. Supporting ECS increases query vol-
ume for both recursive resolvers and authoritative nameservers [6].
Increasing TTL values to reduce the load on authoritative name-
servers, however, will further exacerbate the impact of ECS on the
recursive resolvers’ cache size.
The above results demonstrate the ECS impact on recursive
resolver cache size due to accesses to content delivered by a single
CDN. However, recursive resolvers resolve hostnames for many
domains, supported by different CDNs with varying ECS and TTL
behavior. Next, we study the impact on the cache size of a single
recursive resolver considering its interactions with all authoritative
nameservers with which the resolver exchanges ECS information.
We use the logs from the All-Names Resolver dataset to run trace-
driven simulations of the resolver cache similar to above, using
real-life authoritative ECS and TTL information as they appear in
the log. We find that the cache size blow-up factor for this resolver
is 4.3. Note that the cache size blow-up factor is calculated only
on records that carry ECS. The recursive resolver can send queries
without ECS and/or receive responses without ECS, thus the blow-
up factor on the overall resolver cache may be smaller than what
we report here.
As part of an anycast DNS resolution service, the resolver in
the All-Names Resolver dataset receives DNS queries from a large
122
024681012141618202224262800.10.20.30.40.50.60.70.80.91Cache Blow FactorCDF  20 Sec. TTL40 Sec. TTL60 Sec. TTL1020304050607080901001.522.533.544.5Percentage of Client IP AddressesCache Blow-up FactorIMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Rami Al-Dalky, Michael Rabinovich, and Kyle Schomp
answers) and when obeying the ECS scope restrictions from the log.
The figure was obtained by trace-driven simulation of the logged
queries while obeying all authoritative TTLs from the log. Each
data point reflects the average values of three runs, using different
seeds for random client selection.
The results show a drop in hit rate due to ECS by more than
half, for all client populations. For the full client population, the
hit rate declines from around 76% to around 30%. Moreover, the hit
rate in the presence of ECS increases much slower than without
ECS as client population grows. The latter fact can be explained by
contradictory effects of the client population growth on the hit rate
under ECS. On one hand, as the number of clients grows, popular
hostnames are more widely shared, leading to higher hit rate (this
is the effect captured by the growing hit rate without ECS). On the
other hand, the larger client population is likely to be fragmented
among more /24 blocks, which would depress the hit rate in the
presence of ECS. It appears that the two tendencies largely cancel
each other.
8 ECS PITFALLS
ECS was proposed to improve the performance of end-users by
enabling authoritative nameservers to tailor a response based on
the topological location of the client’s subnet. However, we find
several types of real-life resolver setups that interact with ECS in
ways that diminish or negate its benefits and in fact can turn ECS
from a facilitator to an obstacle to proximity-based server selection.
8.1 Using Non-Routable ECS Prefixes
From our Scan dataset, we observe that a fraction of ECS-enabled
queries arrive at our authoritative nameserver with non-routable
IP addresses in client subnet information. Specifically, we observe
queries with the loopback and self-assigned address prefixes (most
commonly 127.0.0.1/32, 127.0.0.0/24, and 169.254.252.0/24). There
were 33 resolvers from 6 ASes with this behavior, including 27
resolvers from a single AS5. We suspected this may not be part of
the probing behavior we observed in the CDN dataset (Section 6.1)
because we observe these ECS prefixes repeatedly from the same
resolvers despite our authoritative nameserver responding to each
query with the ECS option. After investigating ECS functionality
of several resolvers, we found (and confirmed with the PowerDNS
community) that the PowerDNS recursor software [24] can exhibit
this behavior under some scenarios. Specifically, a private IP ad-
dress or loopback address is sent in an ECS query if (i) the resolver
receives a query from a client with source prefix length of 0 (in
which case, the RFC stipulates the resolver must either not include
any ECS option at all or include its own address information, al-
though in view of our findings the RFC should be more specific and
say explicitly this must be the public address used by the resolver to
send the query) or (ii) the resolver receives (ii) the resolver receives
a query from a client whose IP address is not whitelisted for ECS
usage.
We investigate whether sending such information to authorita-
tive nameservers can cause a confusion at the server side. We send
5Since, as discussed later, we found this behavior to be problematic, we notified the
administrators of the AS containing the majority of these resolvers of the issue.
Figure 3: Cache hit rate for various random fraction of the
client IP addresses.
distributed set of clients. As noted in Section 4, it has 76.2K clients
in 12.3K /24 IPv4 and 2.8K /48 IPv6 client subnets. We would like
to assess the blow-up factor for recursive resolvers with smaller
client populations. To this end, we simulate a public resolver with a
smaller client population by randomly sampling client IP addresses
in the dataset. Using a random subset of clients reflects public
resolver usage, where clients individually decide to use this resolver
regardless of their subnet.
We re-run our simulations only using queries from the random
samples of client IP addresses in the dataset, where we vary the
fraction of all clients in the sample. For each fraction of clients, we
run simulations with three different random samples of client IP
addresses and report the average values.
Figure 2 shows the cache size blow-up factor for different frac-
tions of the client IP addresses. As we can see from the figure, there
is a clear relation between the blow-up factor in cache size and
the increase in the client population because having a diverse set
of clients results in caching several copies of the response for the
same question if the clients’ IP addresses are not covered by the
same ECS scope. In fact, the curve does not appear to flatten as the
fraction of clients reaches 100%. Thus, busier resolvers, with more
clients, than the resolver from the All-Names Resolver dataset are
likely to experience even larger blow-up factors.
In summary, large TTL values and a diverse client population
would result in a large increase of the cache size recursive resolvers
would need if they were to preserve low rates of premature cache
evictions observed recently [27]. Thus, supporting ECS entails re-
source cost for the operators who need to weigh these costs against
the benefits to clients’ quality of experience in deciding on ECS
adoption.