Sources并不支持0.9.x的E2E和DFO模式。唯一支持的是BE（best
effort，尽力而为），尽管1.x的可靠性保证对于从0.9.x传输过来并且已经存在channel里面的Events是有效的。
:::
::: hint
::: title
Hint
:::
虽然数据进入了Flume
1.x的channel之后是适用1.x的可靠性保证，但是从0.9.x到1.x的时候只是BE保证，既然只有BE的保证，也就是说
[Legacy Sources](#legacy-sources)
不算是可靠的传输。对于这种跨版本的部署使用行为要慎重。
:::
必需的参数已用 **粗体** 标明。
##### Avro Legacy Source
  属性                           默认值        解释
  ------------------------------ ------------- ----------------------------------------------------------------------------------------------------------------------------------------------
  **channels**                   \--           与Source绑定的channel，多个用空格分开
  **type**                       \--           组件类型，这个是： `org.apache.flume.source.avroLegacy.AvroLegacySource`
  **host**                       \--           要监听的hostname或者IP地址
  **port**                       \--           要监听的端口
  selector.type selector.\*      replicating   可选值：`replicating` 或 `multiplexing` ，分别表示： 复制、多路复用 channel选择器的相关属性，具体属性根据设定的 *selector.type* 值不同而不同
  interceptors interceptors.\*   \--           该source所使用的拦截器，多个用空格分开 拦截器相关的属性配
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.avroLegacy.AvroLegacySource
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
```
##### Thrift Legacy Source
  属性                           默认值        解释
  ------------------------------ ------------- ----------------------------------------------------------------------------------
  **channels**                   \--           与Source绑定的channel，多个用空格分开
  **type**                       \--           组件类型，这个是： `org.apache.flume.source.thriftLegacy.ThriftLegacySource`
  **host**                       \--           要监听的hostname或者IP地址
  **port** selector.type         \--           要监听的端口 可选值：`replicating` 或 `multiplexing` ，分别表示： 复制、多路复用
  selector.\*                    replicating   channel选择器的相关属性，具体属性根据设定的 *selector.type* 值不同而不同
  interceptors interceptors.\*   \--           该source所使用的拦截器，多个用空格分开 拦截器相关的属性配
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.thriftLegacy.ThriftLegacySource
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
```
#### Custom Source
你可以自己写一个Source接口的实现类。启动Flume时候必须把你自定义Source所依赖的其他类配置进Agent的classpath内。custom
source在写配置文件的type时候填你的全限定类名。
::: hint
::: title
Hint
:::
如果前面章节的那些Source都无法满足你的需求，你可以写一个自定义的Source，与你见过的其他框架的自定义组件写法如出一辙，实现个接口而已，然后把你写的类打成jar包，连同依赖的jar包一同配置进Flume的classpath。
后面章节中的自定义Sink、自定义Channel等都是一样的步骤，不再赘述。
:::
  属性                           默认值        解释
  ------------------------------ ------------- ----------------------------------------------------------------------------------------------------------------------------------------------
  **channels**                   \--           与Source绑定的channel，多个用空格分开
  **type**                       \--           组件类型，这个填你自己Source的全限定类名
  selector.type selector.\*      replicating   可选值：`replicating` 或 `multiplexing` ，分别表示： 复制、多路复用 channel选择器的相关属性，具体属性根据设定的 *selector.type* 值不同而不同
  interceptors interceptors.\*   \--           该source所使用的拦截器，多个用空格分开 拦截器相关的属性配
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.example.MySource
a1.sources.r1.channels = c1
```
#### Scribe Source
::: hint
::: title
Hint
:::
这里先说一句，Scribe是Facebook出的一个实时的日志聚合系统，我在之前没有听说过也没有使用过它，
从Scribe项目的Github文档里面了解到它在2013年就已经停止更新和支持了，貌似现在已经没有新的用户选择使用它了，所以Scribe
Source这一节了解一下就行了。
:::
Scribe
是另外一个类似于Flume的数据收集系统。为了对接现有的Scribe可以使用ScribeSource
，它是基于Thrift
的兼容传输协议，如何部署Scribe请参考Facebook提供的文档。
必需的参数已用 **粗体** 标明。
  属性                                      默认值     解释
  ----------------------------------------- ---------- -------------------------------------------------------------------------------------------------------------------------------------------------------------
  **type**                                  \--        组件类型，这个是： `org.apache.flume.source.scribe.ScribeSource`
  port                                      1499       Scribe 的端口
  maxReadBufferBytes                        16384000   Thrift 默认的FrameBuffer 大小
  workerThreads selector.type selector.\*   5          Thrift的线程数 可选值：`replicating` 或 `multiplexing` ，分别表示： 复制、多路复用 channel选择器的相关属性，具体属性根据设定的 *selector.type* 值不同而不同
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.scribe.ScribeSource
a1.sources.r1.port = 1463
a1.sources.r1.workerThreads = 5
a1.sources.r1.channels = c1
```
### Flume Sinks
#### HDFS Sink
这个Sink将Event写入Hadoop分布式文件系统（也就是HDFS）。
目前支持创建文本和序列文件。 它支持两种文件类型的压缩。
可以根据写入的时间、文件大小或Event数量定期滚动文件（关闭当前文件并创建新文件）。
它还可以根据Event自带的时间戳或系统时间等属性对数据进行分区。
存储文件的HDFS目录路径可以使用格式转义符，会由HDFS
Sink进行动态地替换，以生成用于存储Event的目录或文件名。
使用此Sink需要安装hadoop，
以便Flume可以使用Hadoop的客户端与HDFS集群进行通信。 注意，
**需要使用支持sync() 调用的Hadoop版本** 。
以下是支持的转义符：
  转义符           解释
  ---------------- ----------------------------------------------------------------------------------------------------------------------
  %{host}          Event header中key为host的值。这个host可以是任意的key，只要header中有就能读取，比如%{aabc}将读取header中key为aabc的值
  %t               毫秒值的时间戳（同 System.currentTimeMillis() 方法）
  %a               星期的缩写（Mon、Tue等）
  %A               星期的全拼（Monday、 Tuesday等）
  %b               月份的缩写（Jan、 Feb等）
  %B               月份的全拼（January、February等）
  %c               日期和时间（Thu Feb 14 23:05:25 2019）
  %d               月份中的天（00到31）
  %e               月份中的天（1到31）
  %D               日期，与%m/%d/%y相同 ，例如：02/09/19
  %H               小时（00到23）
  %I               小时（01到12）
  %j               年中的天数（001到366）
  %k               小时（0到23），注意跟 %H的区别
  %m               月份（01到12）
  %n               月份（1到12）
  %M               分钟（00到59）
  %p               am或者pm
  %s               unix时间戳，是秒值。比如2019/2/14 18:15:49的unix时间戳是：1550139349
  %S               秒（00到59）
  %y               一年中的最后两位数（00到99），比如1998年的%y就是98
  %Y               年（2010这种格式）
  %z               数字时区（比如：-0400）
  %\[localhost\]   Agent实例所在主机的hostname
  %\[IP\]          Agent实例所在主机的IP
  %\[FQDN\]        Agent实例所在主机的规范hostname
注意，%\[localhost\], %\[IP\] 和
%\[FQDN\]这三个转义符实际上都是用java的API来获取的，在一些网络环境下可能会获取失败。
正在打开的文件会在名称末尾加上".tmp"的后缀。文件关闭后，会自动删除此扩展名。这样容易排除目录中的那些已完成的文件。
必需的参数已用 **粗体** 标明。
::: note
::: title
Note
:::
对于所有与时间相关的转义字符，Event
header中必须存在带有"timestamp"键的属性（除非 *hdfs.useLocalTimeStamp*
设置为 `true` ）。快速自动添加此时间戳的一种方法是使用
[时间戳添加拦截器](#时间戳添加拦截器) 。
:::
+--------------------------+--------------+--------------------------+
| 属性名                   | 默认值       | 解释                     |
+==========================+==============+==========================+
| **channel**              | \--          | 与 Sink 连接的 channel   |
+--------------------------+--------------+--------------------------+
| **type**                 | \--          | 组件类型，这个是：       |
|                          |              | `hdfs`                   |
+--------------------------+--------------+--------------------------+
| **hdfs.path**            | \--          | HDFS                     |
|                          |              | 目录路径（例如：hdfs://n |
|                          |              | amenode/flume/webdata/） |
+--------------------------+--------------+--------------------------+
| hdfs.filePrefix          | FlumeData    | Flume在HDFS文件          |
|                          |              | 夹下创建新文件的固定前缀 |
+--------------------------+--------------+--------------------------+
| hdfs.fileSuffix          | \--          | Flume在HDFS文件夹        |
|                          |              | 下创建新文件的后缀（比如 |
|                          |              | ：.avro，注意这个"."不会 |
|                          |              | 自动添加，需要显式配置） |
+--------------------------+--------------+--------------------------+
| hdfs.inUsePrefix         | \--          | Flume正在写入            |
|                          |              | 的临时文件前缀，默认没有 |
+--------------------------+--------------+--------------------------+
| hdfs.inUseSuffix         | .tmp         | Flu                      |
|                          |              | me正在写入的临时文件后缀 |
+--------------------------+--------------+--------------------------+
| hdfs.emptyInUseSuffix    | false        | 如果设置为 `false`       |
|                          |              | 上面的                   |
|                          |              | `hdfs.inUseSuffix`       |
|                          |              | 参数在写                 |
|                          |              | 入文件时会生效，并且写入 |
|                          |              | 完成后会在目标文件上移除 |
|                          |              | `hdfs.inUseSuffix`       |
|                          |              | 配置的后缀。如果设置为   |
|                          |              | `true` 则上面的          |
|                          |              | `hdfs.inUseSuffix`       |
|                          |              | 参数会被忽略             |
|                          |              | ，写文件时不会带任何后缀 |
+--------------------------+--------------+--------------------------+
| hdfs.rollInterval        | 30           | 当前文件写入             |
|                          |              | 达到该值时间后触发滚动创 |
|                          |              | 建新文件（0表示不按照时  |
|                          |              | 间来分割文件），单位：秒 |
+--------------------------+--------------+--------------------------+
| hdfs.rollSize            | 1024         | 当前文件写入达到         |
|                          |              | 该大小后触发滚动创建新文 |
|                          |              | 件（0表示不根据文件大小  |
|                          |              | 来分割文件），单位：字节 |
+--------------------------+--------------+--------------------------+
| hdfs.rollCount           | 10           | 当前文件写入E            |
|                          |              | vent达到该数量后触发滚动 |
|                          |              | 创建新文件（0表示不根据  |
|                          |              | Event 数量来分割文件）   |
+--------------------------+--------------+--------------------------+
| hdfs.idleTimeout         | 0            | 关闭非活动文             |
|                          |              | 件的超时时间（0表示禁用  |
|                          |              | 自动关闭文件），单位：秒 |
+--------------------------+--------------+--------------------------+
| hdfs.batchSize           | 100          | 向 HDFS                  |
|                          |              | 写入内容时每次批量操作的 |
|                          |              | Event 数量               |
+--------------------------+--------------+--------------------------+
| hdfs.codeC               | \--          | 压缩算法。可选值：`gzip` |
|                          |              | 、 `bzip2` 、 `lzo` 、   |
|                          |              | `` lzop` 、 ``snappy\`\` |
+--------------------------+--------------+--------------------------+
| hdfs.fileType            | SequenceFile | 文件格式，目前支持：     |