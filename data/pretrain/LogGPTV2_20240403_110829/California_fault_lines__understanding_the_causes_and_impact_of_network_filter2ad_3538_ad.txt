### 6.2 Failure Characteristics

#### 6.2.1 Annualized Link Downtime, Time to Repair, and Time Between Failures

The cumulative distribution functions (CDFs) for individual failure events, by network, are shown in Figure 8. The x-axes are on a logarithmic scale.

**Figure 8: CDFs of individual failure events, by network, for links in operation 30 days or more. (Log-scale x-axes.)**

- **(a) Annualized link downtime**
- **(b) Time to repair**
- **(c) Time between failures**

Our findings align with those of prior studies. Both Watson et al. [33] and Markopoulou et al. [23] also find that link flapping is a predominant source of instability. It is unlikely that all three studies reflect anomalous networks; instead, we suggest that short time scale and oscillatory behavior may simply be "normal" in large networks. Thus, network protocols and routing algorithms should be designed to handle flapping as a common case.

#### 6.2.2 Total Downtime within Flapping Episodes

Figure 9 shows the total downtime within a flapping episode. The cumulative distribution of individual repair times is also depicted, with a sharp spike in the DC network due to a single highly periodic flapping link. Notably, over 70% of the failures in the CPE and HPR networks last less than 10 seconds. In the DC network, 70% of failures last less than 100 seconds, with a median duration of 13 seconds.

**Figure 9: Total downtime within a flapping episode.**

#### 6.2.3 Grouping of Failures

We have considered each link failure independently so far. However, as discussed in Section 4.2.2, we also group link failures into larger events based on temporal correlation. Specifically, we aggregate simultaneous failures into PoP or router failures and combine back-to-back failures into flapping episodes. For the CENIC network, the former are relatively infrequent, so we focus on the latter.

**Figure 8(c)** plots the CDF of time between failure events on a single link. We draw a vertical line at 10 minutes, which serves as our definition of "flapping": two or more consecutive failure events on the same link separated by less than 10 minutes are grouped together into a larger flapping episode. More than 50% of all flapping episodes consist of only two failures, but 5% of the episodes contain more than 19 individual failures (not shown).

**Figure 9** shows the amount of downtime within flapping episodes—note that this is not the duration of the episode, only the periods within the episode when the link was actually down. Comparing to **Figure 8(b)**, we see that flapping episodes, on the whole, are more disruptive than typical failure events.

### 6.3 Causes of Failure

Now that we have quantified how often failure occurs, we turn our attention to its causes. We consider whether particular types of links are more likely to fail and then examine instances where operators explicitly place blame.

#### 6.3.1 Link Type

Each constituent CENIC network is composed of different link technologies, including Ethernet, SONET, and serial lines. **Figure 10** breaks down the individual failure events by the type of hardware involved.

- **(a) Annualized link downtime**
- **(b) Time to repair**
- **(c) Time between failures**

**Figure 10** suggests that Ethernet links are more reliable than other technologies. While Ethernet failures are not as quick to repair as serial lines, they are far less frequent. This is partly due to Ethernet's predominance for short-haul links, which are less exposed to external failure processes.

**Figure 11** presents a similar breakdown, separating links into intra-PoP and long haul. **Figure 11(a)** shows a clear separation in reliability, with intra-PoP links being markedly more available than long-haul links. This may be due to the fact that many intra-PoP links are carried over Ethernet. Comparing **Figures 11(b) and 11(c)** to **Figures 10(b) and 10(c)** suggests that long-haul failures are dominated by serial links.

- **(a) Annualized link downtime**
- **(b) Time to repair**
- **(c) Time between failures**

#### 6.3.2 Labeled Causes

For a subset of link failures, we are able to annotate them with information regarding their causes by matching them to administrator notices. We matched 5,237 out of 19,046 events to such notices, accounting for 37.5% of the total downtime. **Figure 12** shows the breakdown of these events according to the stated cause. The plurality of failure events are due to software upgrades, with hardware upgrades being the next most frequent cause.

**Figure 13** shows that while hardware-related events account for the majority of the downtime, software upgrades are responsible for much less of the total downtime. External factors, including power disruptions, have a more significant footprint. The data is also summarized in **Table 6**.

**Table 5** provides some basic statistics regarding the duration of individual failure events for each category. Most events are short, but the median hardware and power outages are substantially longer—over twenty minutes. Almost all categories have heavy tails, causing the average failure duration to be an order of magnitude longer than the median.

In addition to identifying the cause of the failure, administrator notices also indicate whether the failure is anticipated or "scheduled." While most of the failure events found in the administrator announcements are scheduled, most of the actual downtime can be attributed to unexpected failures—likely because operators take care to limit planned downtime. The median planned outage lasts less than 5 minutes. Interestingly, it appears that network operators are frequently not notified by external entities ahead of incidents that impact the network's operation.

### 6.4 Failure Impact

In general, it is extremely difficult for us to determine from the failure log what, if any, impact a failure had on users of the network. For the set of events annotated with administrator notices, however, we can report if the notice explicitly stated whether the event was supposed to have an impact on the network. The third column of **Table 6** indicates what fraction of the events were supposed to have some impact—however brief—on the network. In almost all cases, the operators indicate some link downtime may result. This phenomenon may be due to self-selection on the part of the operators. Non-impacting failure events, especially unscheduled ones, seem far less likely to motivate an operator to issue a notice.