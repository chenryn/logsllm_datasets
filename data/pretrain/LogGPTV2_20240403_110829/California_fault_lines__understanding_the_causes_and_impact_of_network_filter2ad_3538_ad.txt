50%
40%
30%
20%
10%
0%
DC
CPE
HPR
flapping
isolated
DC
CPE
HPR
10s
1m
10m
1h
6h
1 day
1 week
1s
10s
1m
10m
1h
6h
1s
10s
1m
10m
1h
6h 1 day 1 week
1 year
(a) Annualized link downtime.
(b) Time to repair.
(c) Time between failures.
Figure 8: CDFs of individual failure events, by network, for links in operation 30 days or more. (Log-scale x-axes.)
Again, our ﬁndings reinforce those of prior studies. Both Wat-
son et al. [33] and Markopoulou et al. [23] also ﬁnd that link ﬂap-
ping is a predominant source of instability. It is unlikely that all
three studies reﬂect anomalous networks and instead we suggest
that short time scale and oscillatory behavior may simply be “nor-
mal” in large networks. Thus, network protocols and routing algo-
rithms should be prepared to handling ﬂapping as a common case.
6.3 Causes of failure
Now that we have quantiﬁed how often failure occurs, we turn
our attention to its causes. We consider whether particular types of
links are more likely to fail, and then examine the instances where
operators explicitly place blame.
DC
CPE
HPR
1s
10s
1m
10m
1h
6h
Figure 9: Total downtime within a ﬂapping episode.
mulative distribution of individual repair times. The sharp spike
in the DC network is due to a single highly periodic ﬂapping link.
Other than this anomaly, the most striking feature of the graph is
the low failure durations. Remarkably, over 70% of the failures in
the CPE and HPR networks last less than 10 seconds. In the DC
network, 70% of failures last less than 100 seconds, with a median
duration of 13 seconds.
6.2.4 Grouping
So far, we have considered each link failure independently. As
discussed in Section 4.2.2, however, we also group link failures into
larger events based upon temporal correlation. In particular, we ag-
gregate simultaneous failures, when appropriate, into PoP or router
failures, and combine back-to-back failures into ﬂapping episodes.
In the case of the CENIC network, however, the former are rela-
tively infrequent, so we focus exclusively on the latter.
Figure 8(c) plots the CDF of time between failure events on a
single link. We draw a vertical line at 10 minutes, which serves as
our deﬁnition of “ﬂapping:” two or more consecutive failure events
on the same link separated by less than 10 minutes are grouped to-
gether into a larger ﬂapping episode. 10 minutes is just past the
knee of the curve for each network—the distributions appear mem-
oryless for longer intervals. More than 50% of all ﬂapping episodes
constructed in this manner consist of only two failures, but 5% of
the episodes contain more than 19 individual failures (not shown).
Figure 9 shows the amount of downtime within ﬂapping episodes—
note that this is not the duration of the episode, only the periods
within the episode when the link was actually down. Comparing to
Figure 8(b), we see that ﬂapping episodes, on the whole, are more
disruptive than typical failure events.
6.3.1 Link type
Each constituent CENIC network is composed of a number of
different link technologies, including Ethernet, SONET, and serial
lines. Figure 10 breaks down the individual failure events not by
network (c.f. Figure 8), but instead by the type of hardware in-
volved. Figure 10(a) suggests that Ethernet links are more reliable
than other technologies. Figure 10(b) shows that while Ethernet
failures are not as quick to repair as serial lines, they are far less
frequent (Figure 10(c)). This is undoubtedly in part due to Ether-
net’s predominance for short-haul links, which are less exposed to
external failure processes.
Figure 11 presents a similar breakdown, separating links into
intra-PoP and long haul. Perhaps not surprisingly, Figure 11(a)
shows a clear separation in reliability, with intra-PoP links being
markedly more available than long-haul links. This may be due to
the fact that many intra-PoP links are carried over Ethernet; indeed,
comparing Figures 11(b) and 11(c) to Figures 10(b) and 10(c) sug-
gests that long-haul failures are dominated by serial links.
6.3.2 Labeled causes
For a subset of link failures, we are able to annotate them with
information regarding their causes by matching them to adminis-
trator notices. We were able to match 5,237 (out of 19,046) events
to such a notice, accounting for 37.5% of the total downtime. Fig-
ure 12 shows the breakdown of these events according to the stated
cause. The plurality of failure events are due to software upgrades,
with hardware upgrades the next most frequent cause. Figure 13,
however, shows that while hardware-related events account for the
lion’s share of the downtime, software upgrades are responsible for
much less of the total downtime; indeed, external factors including
power disruptions have a more signiﬁcant footprint. The data is
also summarized in Table 6.
Table 5 provides some basic statistics regarding the duration
of individual failure events for each category. Most events are
323100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Serial
SONET
Ether
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Serial
SONET
Ether
Serial
SONET
Ethernet
1m
10m
1h
6h
1 day
1 week
1s
10s
1m
10m
1h
6h
10m
1h
6h
1 day
1 week 1 month
1 year
(a) Annualized link downtime.
(b) Time to repair.
(c) Time between failures.
Figure 10: CDFs of individual failure events, by link hardware type, for links in operation 30 days or more. (Log-scale x-axes.)
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
1250
1000
750
500
250
0
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Intra−pop
Long−haul
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Intra−pop
Long−haul
Intra−pop
Long−haul
1m
10m
1h
6h
1 day
1 week
1s
10s
1m
10m
1h
6h
10m
1h
6h
1 day
1 week 1 month
1 year
(a) Annualized link downtime.
(b) Time to repair.
(c) Time between failures.
Figure 11: CDFs of individual failure events, by link class, for links in operation 30 days or more. (Log-scale x-axes.)
Unscheduled
Scheduled
Unscheduled
Scheduled
50 d
40 d
30 d
20 d
10 d
0 d
External
Hardware
Software
Config
Power
Other
Unknown
External
Hardware
Software
Config
Power
Other
Unknown
Figure 12: Failure events that matched administrator notices
during the entire measurement period, broken down by cause.
Figure 13: Cumulative downtime of the failures that matched
administrator notices over the entire measurement period, cat-
egorized by failure.
short, but the median hardware and power outages are substantially
longer—over twenty minutes. Almost all categories have heavy
tails, however, which cause the average failure duration to be an
order of magnitude longer than the median.
In addition to identifying the cause of the failure, administra-
tor notices also indicate whether or not the failure is anticipated or
“scheduled”. While most of the failure events found in the admin-
istrator announcements are scheduled, most of the actual downtime
can be attributed to unexpected failures—likely because the opera-
tors take care to make sure planned downtime is as limited as pos-
sible. Indeed, the median planned outage lasts less than 5 minutes
(not shown). Interestingly, it appears network operators frequently
are not notiﬁed by external entities ahead of incidents that impact
the network’s operation.
6.4 Failure impact
In general, it is extremely difﬁcult for us to tell from the failure
log what—if any—impact a failure had on users of the network.
For the set of events that are annotated with administrator notices,
however, we can report if the notice explicitly stated whether or
not the event was supposed to have an impact on the network. The
third column of Table 6 indicates what fraction of the events were
supposed to have some impact—however brief—on the network. In
almost all cases, the operators indicate some link downtime may re-
sult. This phenomenon is perhaps due to self selection on the part of
the operators, however. Non-impacting failure events—especially
unscheduled ones—seem far less likely to motivate an operator to