35s
2m57s
6m20s
1m5s
3m26s
2m54s
55s
1m43s
1m24s
1h10m
5h12m
6h21m
2m10s
7m57s
8m40s
1h10m
4h49m
5h50m
Table 2: Reproduction of earlier results in crash reproduction
in greybox fuzzers. We manually select the target and show
the mean time-to-exposure.
In Table 2, we present a comparison of ParmeSan,
AFLGo, and HawkEye on crash reproduction of known bugs
in OpenSSL and Binutils. We manually target the point in
the code that causes the crash, and let the fuzzers generate
inputs to reproduce the crash (i.e., ParmeSan skips its target
acquisition step). We use the same input seeds as presented
in [8], consisting of a single ﬁle with a single newline char-
acter. As shown in the table, ParmeSan outperforms both
HawkEye and AFLGo in reproducing these bugs in all cases.
For most, ParmeSan is more than twice as fast, while in the
worst case (CVE-2016-4490), it is still more than 30% faster
at reproducing the bug than AFLGo. Adding DFA informa-
tion allows ParmeSan to focus on solving conditionals, both
on the way to the target and of the target itself—leading to a
more targeted mutation strategy (fewer executions needed),
allowing for faster crash reproduction. We conclude that
ParmeSan signiﬁcantly improves the state-of-the-art time-to-
exposure (TTE) of bugs even for traditional directed fuzzing.
8.2 Coverage-guided fuzzers
We now show that our fuzzing strategy ﬁnds (many) bugs
faster than state-of-the-art coverage-guided fuzzers. We
speciﬁcally compare against Angora, which we found to be
the fastest open-source competitor on the dataset considered,
faster for instance than QSYM [46]. Note that if we target
all the conditionals in the program, the behavior of ParmeSan
is very similar to Angora. Comparing against Angora gives
us a good picture of the effectiveness of targeting points ob-
tained from our sanitizer-based analysis stage.
To show that sanitizer-guided fuzzing can efﬁciently ﬁnd
real-world bugs, we evaluate ParmeSan on the Google
fuzzer-test-suite [22]. This dataset contains a number of
known bugs, coverage benchmarks, and assertion checks for
23 real-world libraries. We show that ParmeSan is able to
trigger the same bugs as coverage-oriented fuzzers in sig-
niﬁcantly less time. In this suite, we always use ASan for
ParmeSan’s target acquisition step, as it is very powerful and
detects some of the most common memory errors.
In all benchmarks, we use the seeds provided by the suite
as the initial corpus. Since the dataset contains a number of
hard-to-trigger bugs, we run the experiments with a timeout
of 48 hours, to give the fuzzers a chance at reaching these
bugs. For example, it takes Angora on average 47 hours to
trigger the integer overﬂow in freetype2 . Furthermore, the
suite adds runtime sanitizers to each application to detect the
bugs. We compile and run every program with the default
parameters used in the suite.
Table 3 shows the mean time-to-exposure (TTE) of a num-
ber of bugs from the Google fuzzer-test-suite dataset. We
emphasize that we evaluated the entire test suite, but for
brevity left out 11 bugs that no fuzzer could ﬁnd within
48 hours, as well as the openthread set with its 12 very
easy to ﬁnd bugs which did not have any outlying results
(of course, we did include them in our geomean calculation
to avoid skewing the results). The evaluation is split into
two parts. The ﬁrst part, whole pipeline, uses the whole
ParmeSan pipeline with automatic target acquisition using
ASan. We compare ParmeSan against baseline Angora (i.e.,
no targets) and sanitizer-guided AFLGo (i.e., provided with
USENIX Association
29th USENIX Security Symposium    2297
Prog
Type Runs
Mean. TTE
Comment
AFLGo (p)
Angora (p)
ParmeSan
boringssl
c-ares
freetype2
libarchive
pcre2
lcms
UAF
BO
IO
UAF
BO
BO
ML
BO
ML
openssl-1.0.1f BO
openssl-1.0.1f ML
ML
BO
BO
libssh
libxml2
libxml2
proj4
re2
woff2
5s
7
2h32m 0.004
0.04
Whole pipeline
30
30
5
30
30
30
30
30
30
30
30
30
30
30
25m 0.006
6m 0.002
1h12m 0.004
3m10s 0.002
51m 0.007
30m 0.005
50m 0.003
1m 0.012
7m30s 0.002
47m 0.002
45m 0.004
288%
Manual targeting
45m 0.005
1s
0.12
47h 0.018
15m 0.003
2m 0.006
22m 0.001
32s 0.008
20m 0.001
20m 0.001
5m 0.04
0.11
40s
0.03
1m40s
21m 0.004
15m 0.006
37%
Geomean ParmeSan beneﬁt
libpng
libpng
libjpeg-turbo
⋆
⋆
⋆
⋆
AE
AE
AE
openssl-1.0.2d AE
Geomean ParmeSan beneﬁt
freetype2
guetzli
harfbuzz
30
30
30
30
30
30
30
30
json
1h8m 0.003
2m 0.003
2m 0.005
0.21
2s
45m 0.000
(45m) 0.000
(30s) 0.002
(42s) 0.003
(1s)
0.83
(10m) 0.005
5h 0.000 (2h20m) 0.005
(3m) 0.005
7m 0.004
1m10s 0.001
0.04
(15s)
422%
90%
25m crypto/asn1/asn1_lib.c:459
1s CVE-2016-5180
43h cf2_doFlex.
8m src/pcre2_match.c:5968
41s src/cmsintrp.c:642
13m archive_read_support_format_warc.c:537
50s
11m CVE-2015-8317
17m memleak. valid.c:952
3m4s CVE-2014-0160. OpenSSL 10.0.1f
37s crypto/mem.c:308
1m26s
12m35s
8m
10m jdmarker.c:659
20s pngread.c:738
34s pngrutil.c:3182
1s ttgload.c:1710
5m
1h10m
1m
10s CVE-2015-3193
Table 3: Time-to-exposure on the Google fuzzer-test-suite. For the tests under manual target, there is no actual bug, here we
manually target the site (i.e., no target acquisition phase). Statistically signiﬁcant Mann-Whitney U test p-values (p < 0:05) are
highlighted. 7= not found, = not available. In all cases, we use ASan for target acquisition. UAF=use-after-free, BO=buffer
overﬂow, IO=integer overﬂow, ML=memory leak, AE=assertion error
the same targets as ParmeSan). We see that ParmeSan out-
performs both AFLGo and Angora signiﬁcantly, with a ge-
omean speedup in TTE of 288% and 37% respectively.
In the second part, we manually target a number of known
hard-to-reach sites. These benchmarks from the suite check
whether fuzzers are able to cover hard-to-reach sites or trig-
ger assertion errors. Since in these cases there is no bug to be
found, using a sanitizer-guided approach makes little sense.
Instead, we show the effect of making the fuzzer directed.
As these targets have to be selected manually, we consider
the comparison against Angora to be unfair and only include
the results as an indication how much directed fuzzing can
help in such scenarios.
Interestingly, Angora beats AFLGo in every benchmark
on the whole suite. The main cause for this is that Angora
has access to DFA information which allows it to cover new
branches much more quickly than the AFL-based strategy
used by AFLGo. Note that some of our results when compar-
ing ParmeSan against Angora are not statistically signiﬁcant
(Mann-Whitney p-value (cid:21) 0.05). All of these are bugs that
are either triggered in a short amount of time (and thus have
a large variance in the measurements), or are memory leaks
(for which the immediate cause is independent of the targets
retrieved by our target acquisition component, as we discuss
in the next section). On the libssh benchmark, ParmeSan
performs worse than Angora. This happens due to the fact
that the bug is often triggered at a point when a lot of new
coverage is found in one go. Due to our lazysan optimiza-
tion, ASan is not enabled when this new coverage is trig-
gered, causing ParmeSan to detect the bug later when it actu-
ally tries to ﬂip the branch that causes the sanitizer error. As
Table 7 shows, ParmeSan without the lazysan optimization
is faster at ﬁnding this particular bug. Note that the variance
in this test case is very high, and, as such, the result is not
statistically signiﬁcant.
In Table 4, we present branch coverage at the time-of-
exposure (TTE) for ParmeSan and 4 different state-of-the-
art fuzzers: AFLGo [4], NEUZZ [40], QSYM [46], and An-
gora [9]. In this experiment, we run all the fuzzers with 1
instance, except QSYM which uses 2 AFL instances and one
QSYM instance (as per the setup suggested by the authors)
inside a Docker container that has been allocated one CPU.
Note that we do not include the required preprocessing time
for NEUZZ and ParmeSan in the results. For ParmeSan, the
2298    29th USENIX Security Symposium
USENIX Association
NEUZZ
QSYM
3s
7
280
7
2h32m
5s
7
Type Runs AFLGo
Prog
2281
UAF 10
boringssl
202
10
BO
c-ares
IO
5
freetype2
7
9023
UAF 10
pcre2
BO
10
1079
lcms
4870
10
BO
libarchive
365
10
ML
libssh
5780
10
BO
libxml2
ML
10
5755
libxml2
550
10
openssl-1.0.1f BO
1250
10
openssl-1.0.1f ML
82
10
ML
proj4
5172
BO
re2
10
woff2
BO
10
91
woff2
50
OOM 10
Geomean diff
Angora
45m
2510
2520 1h20m 2670 3h20m
1s
270
275
20s
47h
7 57330
7
15m
16m 32430 1h20m 30111
25m 31220
2m
2890
6m
2876
1m50s
22m
6208
5945 1h20m
1h12m
32s
341
43s
419
3m10s
20m
25m 12789
51m
5071
7576
20m
19m 11260 1h10m 10580
30m 10644
5m
793
50m
720
1m
40s
83 1m40s
7m30s
21m
47m
45m
15m
20s
2m
+16% +288% +40% +81% +95% +867% +33% +37%
853 5h25m
4570
23m
86 10m5s
50m 7610
2h
98
41m
53 1m45s
814 10m12s
40s
717
83
1m55s
5178
3231
7
7m
7
631 2m32s
2h5m
94 31m20s
22s
50
ParmeSan
1850
200
49320
8761
540
4123
123
2701
2554
543
709
80
25m
1s
43h
8m
41s
13m
50s
11m
17m