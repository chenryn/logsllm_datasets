detection systems. Although Snort has been recently released with an improved
detection engine that addresses some of these issues, its parallelization eﬀorts are
limited to searching strings in packet payloads and the discriminating features
are chosen based on domain knowledge. This limits the general applicability of
the solution and forfeits potential gains by processing all features in parallel.
Similar to the revised detection engine of Snort, we attempt to mitigate the
performance problem by changing the comparison mechanism from a rule-to-rule
to a feature-to-feature approach. Instead of dealing with each rule individually,
all rules are combined in a large set and partitioned (or clustered) based on
their speciﬁcations for the diﬀerent features. By considering a single feature at
a time, we partition all rules of a set into subsets. In this clustering process, all
rules that specify identical values for this feature are put into the same subset.
The clustering process is then performed recursively on all subsets until each
subset contains only a single rule or there are no more features left to split the
remaining rules into further subsets. In contrast to the Snort engine, our solution
is applicable to diﬀerent kinds of signature-based systems and not limited to
input from the network. It requires no domain speciﬁc feature selection and is
capable of performing parallel checks for all features.
4 Decision Tree
The subset structure obtained by the partitioning of the rule set can also be
represented as a decision tree. Given this representation, the set that initially
contains all rules can be considered as the tree’s root node while its children
are the direct subsets created by partitioning the rule set according to the ﬁrst
feature. Each subset is associated with a node in the tree. When a node contains
more than one rule, these rules are subsequently partitioned and the node is
labeled with the feature that has been used for this partitioning step. An arrow
that leads from a node to its child is annotated with the value of the feature
that is speciﬁed by all the rules in this child node. Every leaf node of the tree
contains only a single rule or a number of rules that can not be distinguished by
any feature. Rules are indistinguishable when they are identically with respect
to all the features used for the clustering process.
Consider the following example with four rules and three features. A rule
speciﬁes a network packet from a certain source address to a certain destina-
tion address and destination port. The source and destination address features
have the type IPv4 address while the destination port feature is of type short
integer.
(#) Source Address --> Destination Address : Destination Port
(1) 192.168.0.1 --> 192.168.0.2 : 23
(2) 192.168.0.1 --> 192.168.0.3 : 23
(3) 192.168.0.1 --> 192.168.0.3 : 25
(4) 192.168.0.4 --> 192.168.0.5 : 80
Using Decision Trees to Improve Signature-Based Intrusion Detection
179
A possible decision tree is shown in Figure 1. In order to create this tree, the
rules have been partitioned on the basis of the three features, from left to right,
starting with the source address. When the IDS attempts to ﬁnd the matching
rules for an input data item, the detection process commences at the root of
the tree. The label of the node determines the next feature that needs to be
examined. Depending on the actual value of that feature, the appropriate child
node is selected (using the annotations of the arrows leading to all children). As
the rule set has been partitioned by the respective feature, it is only necessary
to continue detection at a single child node.
{ 1, 2, 3, 4 }
Source Address
192.168.0.1
192.168.0.4
{ 1, 2, 3 }
Destination Address
{ 4 }
192.168.0.2
192.168.0.3
{ 1 }
{ 2, 3 }
Destination Port
23
25
{ 2 }
{ 3 }
Fig. 1. Decision Tree.
When the detection process eventually terminates at a leaf node, all rules as-
sociated with this node are potential matches. However, it might still be necessary
to check additional features. To be precise, all features that are speciﬁed by the
potentially matching rules but that have not been previously used by the clus-
tering process to partition any node on the path from the root to this leaf must
be evaluated at this point. Consider Rule 1 in the leftmost leaf node in Figure 1.
Both, source address and destination address have been used by the clustering
process on the path between this node and the root, but not the destination
port. When a packet which has been sent from 192.168.0.1 to 192.168.0.2 is
evaluated as input element, the detection process eventually terminates at the
leaf node with Rule 1. Although this rule becomes a potential match, it is still
possible that the packet was directed to a diﬀerent port than 23. Therefore, the
destination port has to be checked additionally. Our implementation solves this
problem by simply expanding the tree for all deﬁned features that have not been
180
C. Kruegel and T. Toth
used so far. This only requires the ability to further ‘partition’ a node with only
one rule, a step that results in a single child node.
At any time, when the detection process cannot ﬁnd a successor node with a
speciﬁcation that matches the actual value of the input element under consider-
ation (i.e., an arrow with a proper annotation), there is no matching rule. This
allows the matching process to exit immediately.
4.1 Decision Tree Construction
The decision tree is built in a top-down manner. At each non-leaf node, that is
for every (sub)set of rules, one has to select a feature that is used for extending
the tree (i.e., partitioning the corresponding rules). Obviously, features that are
not deﬁned by at least one rule are ignored in this process as a split would simply
yield a single successor node with the exactly same set of rules. In addition, all
features that have been used previously for partitioning at any node on the path
from the node currently under consideration to the root are excluded as well.
A split on the basis of such a feature would also result in only a single child
node with exactly the same rules. This is because of the partitioning at the
predecessor node, which guarantees that only rules that specify identical values
for that feature are present at each child node.
The choice of the feature used to split a subset has an important impact on
the shape and the depth of the resulting decision tree. As each node on the path
from the root to a leaf node accounts for a check that is required for every input
element, it is important to minimize the depth of the decision tree. An optimal
tree would consist of only two levels - the root node and leaves, each with only
a single rule. This would allow the detection process to identify a matching rule
by examining only a single feature.
As an example of the impact of feature selection, consider the decision tree
of Figure 2 which has been built from the same four rules introduced above. By
using the destination port as the ﬁrst selection feature, the resulting tree has a
maximum depth of only two and consists of six nodes instead of seven.
In order to create an optimized decision tree, we utilize a variant of ID3 [11,
12], a well-known clustering algorithm applied in machine learning. This algo-
rithm builds a decision tree from a classiﬁed set of data items with diﬀerent
features using the notion of information gain. The information gain of an at-
tribute or feature is the expected reduction in entropy (i.e., disorder) caused by
partitioning the set using this attribute. The entropy of the partitioned data is
calculated by weighting the entropy of each partition by its size relative to the
original set. The entropy ES of a set S of rules is calculated by the following
Formula 1.
Smax(cid:1)
ES =
−pi log2(pi)
(1)
where pi is the proportion of examples of category i in S. Smax denotes the total
number of diﬀerent categories. In our case, each rule itself is considered to be a
i=1
Using Decision Trees to Improve Signature-Based Intrusion Detection
181
{ 1, 2, 3, 4 }
Destination Port
23
25
80
{ 1, 2 }
Destination Address
{ 3 }
{ 4 }
192.168.0.2
192.168.0.3
{ 1 }
{ 2 }
Fig. 2. Optimized Decision Tree.
category of its own, therefore Smax is the total number of rules. When S is a set
of n rules, pi is equal to 1
n and the equation above becomes
ES =
n(cid:1)
i=1
− 1
n log2(
n) = − log2(
1
1
n) = log2(n)
(2)
The notion of entropy could be easily extended to incorporate domain speciﬁc
know-ledge. Instead of assigning the same weight to each rule (that is, 1
n for
each one of the n rules), it is possible to give higher weights to rules that are
more likely to trigger. This results in a tree that is optimized toward a certain,
expected input.
Given the result about entropy in Formula 2 above, the information gain G
for a rule set S and a feature F can be derived as shown in Formula 3.
|Sv|
|S| log2(|Sv|)
|Sv|
|S| ESv = log2(|S|) − (cid:1)
G(S,F ) = ES − (cid:1)
(3)
v=Val(F )
v=Val(F )
In this equation, Val(F ) represents the set of diﬀerent values of feature F that
are speciﬁed by rules in S. Variable v iterates over this set. Sv are the subsets
of S that contain all rules with an identical speciﬁcation for feature F . |S| and
|Sv| represent the number of elements in the rule sets S and Sv, respectively.
ID3 performs local optimization by choosing the most discriminating fea-
ture, i.e., the one with the highest information gain, for the rule sets at each
node. Nevertheless, no optimal results are guaranteed as it might be necessary
to choose a non-local optimum at some point to achieve the globally best out-
come. Unfortunately, creating a minimal decision tree that is consistent with a
set of data is NP-hard.
182
C. Kruegel and T. Toth
4.2 Non-trivial Feature Deﬁnitions
So far, we have not considered the situation of a rule that completely omits the
speciﬁcation of a certain feature or deﬁnes multiple values for it (e.g., instead of
a single integer, a whole interval is given). As not deﬁning a feature is equivalent
to specifying the feature’s whole value domain, we only consider the deﬁnition
of multiple values. Notice that it is sometimes not possible to enumerate the
value domain of a feature (such as ﬂoating point numbers) explicitly. This can
be easily solved by specifying intervals instead of single values.
When a certain rule speciﬁes multiple values for a property, there can be
a potential overlap with a value deﬁned by another rule. As the partitioning
process can only put two rules into the same subset when both specify the exact
same value for the feature used to split, this poses a problem. The solution is
to put both rules into one set and annotate the arrow with the value that the
two have in common and additionally put the rule which deﬁnes multiple values
into another set, labeling the arrow leading to that node with the value(s) that
only that rules speciﬁes.
Obviously, this basic idea can be extended to multiple rules with many over-
lapping deﬁnitions. The value domain of the feature used for splitting is par-
titioned into distinct intersections of all the values which are speciﬁed by the
rules. Then, for each rule, a copy is put into every intersection that is associated
with a value deﬁned by that rule. Consider the example rules that have been
previously introduced and change the second rule to one that allows an arbitrary
destination port as shown below.
(2) 192.168.0.1 --> 192.168.0.3 : any
The decision tree that results when the destination port feature is used to
partition the root node is shown in Figure 3. The value domain [0, 216-1] of
destination port has been divided into the seven intersections represented by the
following intervals [0,22], 23, 24, 25, [26,79], 80 and [81, 216-1]. Rules that deﬁne
the appropriate values are put into the successor nodes with the corresponding
arrow labels. Notice that a packet sent from 192.168.0.1 to 192.168.0.3 and
port 25 satisﬁes the constraints of both rules, number 2 and 3. This fact is
reﬂected by the leaf node in the center of the diagram that holds two rules but
cannot be partitioned any further.
The total number of rules in all node’s successors does not necessarily need to
be equal to the number of rules in the ancestor node (as one might expect when
a set is partitioned). This has eﬀects on the size of the decision tree as well as on
the function that chooses the optimal feature for tree construction. When many
rules need to be processed and each only deﬁnes a few of all possible features,
the size of the tree can become large. To keep the size manageable, one can trade
execution speed during the detection process for a reduced size of the decision
tree. This is achieved by dividing the rule set into several subsets and building
separate trees for each set. During detection, every input element has then to be
processed by all trees sequentially. For our detection engine implementation, we
Using Decision Trees to Improve Signature-Based Intrusion Detection
183
{ 1, 2, 3, 4 }
Destination Port
0:22
23
24