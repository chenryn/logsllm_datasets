title:How Unique is Your .onion?: An Analysis of the Fingerprintability
of Tor Onion Services
author:Rebekah Overdorf and
Mark Ju&apos;arez and
Gunes Acar and
Rachel Greenstadt and
Claudia D&apos;ıaz
An Analysis of the Fingerprintability of Tor Onion Services
How Unique is Your .onion?
Rebekah Overdorf
Drexel University
Philadelphia, Pennsylvania
PI:EMAIL
Marc Juarez
ESAT-COSIC and imec KU Leuven
Leuven, Belgium
PI:EMAIL
Gunes Acar
imec-COSIC KU Leuven
Leuven, Belgium
PI:EMAIL
7
1
0
2
p
e
S
0
2
]
R
C
.
s
c
[
2
v
5
7
4
8
0
.
8
0
7
1
:
v
i
X
r
a
Rachel Greenstadt
Drexel University
Philadelphia, Pennsylvania
PI:EMAIL
Claudia Diaz
imec-COSIC KU Leuven
Leuven, Belgium
PI:EMAIL
ABSTRACT
Recent studies have shown that Tor onion (hidden) service websites
are particularly vulnerable to website fingerprinting attacks due to
their limited number and sensitive nature. In this work we present
a multi-level feature analysis of onion site fingerprintability, con-
sidering three state-of-the-art website fingerprinting methods and
482 Tor onion services, making this the largest analysis of this kind
completed on onion services to date.
Prior studies typically report average performance results for
a given website fingerprinting method or countermeasure. We in-
vestigate which sites are more or less vulnerable to fingerprinting
and which features make them so. We find that there is a high
variability in the rate at which sites are classified (and misclassified)
by these attacks, implying that average performance figures may
not be informative of the risks that website fingerprinting attacks
pose to particular sites.
We analyze the features exploited by the different website finger-
printing methods and discuss what makes onion service sites more
or less easily identifiable, both in terms of their traffic traces as well
as their webpage design. We study misclassifications to understand
how onion services sites can be redesigned to be less vulnerable to
website fingerprinting attacks. Our results also inform the design
of website fingerprinting countermeasures and their evaluation
considering disparate impact across sites.
CCS CONCEPTS
• Security and privacy → Pseudonymity, anonymity and un-
traceability; Privacy-preserving protocols; Network security;
KEYWORDS
Website fingerprinting, Tor, anonymous communications systems,
web privacy
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134005
1 INTRODUCTION
Website fingerprinting attacks apply supervised classifiers to
network traffic traces to identify patterns that are unique to a web
page. These attacks can circumvent the protection afforded by en-
cryption [7, 13, 19, 25] and the metadata protection of anonymity
systems such as Tor [9, 12]. To carry out the attack the adversary
first visits the websites, records the network traffic of his own visits,
and extracts from it a template or fingerprint for each site. Later,
when the victim user connects to the site (possibly through Tor),
the adversary observes the victim’s traffic and compares it to the
previously recorded templates, trying to find a match. Website fin-
gerprinting can be deployed by adversaries with modest resources
who have access to the communications between the user and the
Tor entry guard. There are many entities in a position to access this
communication, including wireless router owners, local network
administrators or eavesdroppers, Internet Service Providers (ISPs),
and Autonomous Systems (ASes), among other network intermedi-
aries.
Despite the high success rates initially reported by website finger-
printing attacks [6, 27], their practicality in the real-world remains
uncertain. A 2014 study showed that the success of the attacks is
significantly lower in realistic scenarios than what is reported by
evaluations done under artificial laboratory conditions [15]. More-
over, using a very large world of websites, Panchenko et al. showed
that website fingerprinting attacks do not scale to the size of the
Web [21], meaning that, in practice, it is very hard for an adversary
to use this attack to recover the browsing history of a Tor user.
Kwon et al. demonstrated, however, that a website fingerprinting
adversary can reliably distinguish onion service connections from
other Tor connections [17]. This substantially reduces the number
of sites to consider when only targeting onion services, as the uni-
verse of onion services is orders of magnitude smaller than the web,
which makes website fingerprinting attacks potentially effective in
practice. In addition, onion services are used to host sensitive con-
tent such as whistleblowing platforms and activist blogs, making
website fingerprinting attacks on this sites particularly attractive,
and potentially very damaging [8]. For these reasons, we focus our
analysis on onion services rather than the whole web.
In this work we choose to model the set of onion services as a
closed world. Our dataset contains as many landing pages of the
hidden service world as was possible for us to collect at the time.
After removing pages with errors and pages that are duplicates
of other sites, we were left with a sanitized dataset of 482 out
of the 1,363 onion services that were crawled. While the exact
size of the complete onion service world cannot be known with
certainty, onionscan was able to find 4,400 onion services on their
latest scan (this number is not sanitized for faulty or duplicated
sites) [18]. This indicates that our set, while incomplete, contains
a significant portion of the onion service world. We consider that
an actual attacker can compile an exhaustive list of onion services,
which would effectively yield a closed world scenario, since, once
the adversary establishes that a user is visiting a onion service, the
onion service in question will be one on the adversary’s list. We
note that closed world models are not realistic when considering
the entire web, rather than just onion services.
Prior evaluations of website fingerprinting attacks and defenses
report aggregate metrics such as average classifier accuracy. How-
ever, we find that some websites have significantly more distinctive
fingerprints than others across classifiers, and that average metrics
such as overall classifier accuracy cannot capture this diversity.
In this work, we study what we call the fingerprintability of web-
sites and investigate what makes a page more vulnerable to website
fingerprinting. This issue has practical relevance because adver-
saries interested in identifying visits to a particularly sensitive site
may not care about the accuracy of the classifier for other sites, and
thus the fingerprintability of that specific site matters. Similarly,
the administrators of onion services likely care more about the
vulnerability of their users to fingerprinting attacks, rather than the
average vulnerability of a onion services to the attack. We extract
lessons from our analysis to provide recommendations to onion
service designers to better protect their sites against website finger-
printing attacks, including an analysis of a high profile SecureDrop
instance.
The contributions of this study are:
Large .onion study. 1 We collected the largest dataset of onion
services for website fingerprinting to date and evaluated the perfor-
mance of three state-of-the-art classifiers in successfully identifying
onion service sites. For comparison, previous studies considered
worlds of 30 [11] or 50 [8, 17] onion services, an order of magnitude
smaller than our study, that analyses 482 onion services.
Fingerprintability matters. While the average accuracy achie-
ved by the classifiers is 80%, we found that some sites are consis-
tently misclassified by all of the methods tested in this work, while
others are consistently identified correctly, and yet others provide
mixed results. In particular, 47% of sites in our data set are classified
with greater than 95% accuracy, while 16% of sites were classified
with less than 50% accuracy. Throughout this paper, we use the
term fingerprintable to mean how many of the visits are correctly
classified. Depending on the requirements of the specific analysis,
we use different ways to distinguish more and less fingerprintable
sites. This includes comparing top 50 sites to bottom 50 sites or
taking sites with F1  0.66 as more fingerprintable.
Errors made by different methods are correlated. Fully 31%
of misclassified instances were misclassified by all three classifiers.
1This data along with the code used for analysis in this work is available at
https://cosic.esat.kuleuven.be/fingerprintability/
This implies that weaknesses of the individual classifiers cannot be
fully overcome using ensemble methods. We nonetheless propose
an ensemble that combines all three classifiers, slightly improving
the results offered by the best individual classifier.
Novel feature analysis method. We present a method for an-
alyzing fingerprintability that considers the relationship between
the inter-class variance and intra-class variance of features across
sites. The results of this analysis explain which features make a site
fingerprintable, independently of the classifier used.
Size matters. We show that size-based features are the most
important in identifying websites and that when sites are misclassi-
fied, they are typically confused with sites of comparable size. We
show that large sites are consistently classified with high accuracy.
Dynamism matters for small sites. While large sites are very
fingerprintable, some small sites are harder than others to classify.
We find that misclassified small sites tend to have more variance,
and that features related to size variability are more distinguishing
in sets of small sites. Put simply, smaller sites that change the most
between visits are the hardest to identify.
Analysis of site-level features. Site-level features are website
design features that cannot be (directly) observed in the encrypted
stream of traffic but can be tweaked by the onion service operators.
We identify which site-level features influence fingerprintability
and we provide insights into how onion services can be made more
robust against website fingerprinting attacks.
Insights for Adversarial Learning. Website fingerprinting is
a dynamic, adversarial learning problem in which the attacker aims
to classify a traffic trace and the defender aims to camouflage it,
by inducing misclassifications or poisoning the learning system.
In the parlance of adversarial learning [2], we have conducted an
exploratory attack against three different approaches, to help site
owners and the Tor network design better causative attacks. A
causative attack is an attack against a machine learning system
that manipulates the training data of a classifier. Most adversarial
learning approaches in the literature consider the adversary to
be the evader of the learning system, not the learner. However,
this is not the case in website fingerprinting nor in many other
privacy problems. For this reason, most adversarial learning studies
investigate an attack on a specific learning algorithm and feature
set. In contrast, we study the three top-performing learners and
introduce a classifier-independent feature analysis method to study
the learnability of a particular class (a web page).
2 BACKGROUND AND RELATED WORK
Encryption alone does not hide source and destination IP ad-
dresses, which can reveal the identities of the users and visited
website. Anonymous communications systems such as Tor [9]
route communications through multiple relays, concealing the des-
tination server’s address from network adversaries. Moreover, Tor
supports onion services which can be reached through Tor while
concealing the location and network address of the server.
Website fingerprinting is a traffic analysis attack that allows an at-
tacker to recover the browsing history of a user from encrypted and
anonymized streams. Prior work has studied the effectiveness of this
attack on HTTPS [7], encrypted web proxies [13, 25], OpenSSH [19],
VPNs [12], and various anonymity systems such as Tor and JAP [12].
We focus on Tor because it is, with more than two million daily
users [1], the most popular anonymous communications system.
In website fingerprinting the adversary is a network eavesdrop-
per who can identify the user by her IP address, but who does not
know which website the user is visiting (see Figure 1). The attacker
cannot decrypt the communication, but can record the network
packets generated by the activity of the user. To guess the web
page that the user has downloaded, the attacker compares the traf-
fic recorded from the user with that of his own visits to a set of
websites. The best match is found using a statistical classifier.
Website fingerprinting attacks are based on supervised classifiers
where the training instances are constructed from the traffic samples
or traces the adversary collects browsing sites of interest with
with Tor, and the test samples are traces presumably captured
from Tor users’ traffic. Next, we will give an overview of website
fingerprinting attacks that have been proposed in the literature.
2.1 Attacks against Tor
In 2009, Herrmann et al. proposed the first website fingerprinting
attack against Tor, based on a Naive Bayes classifier and frequency
distributions of packet lengths [12]. Their study only achieved an
average accuracy of 3% for 775 websites, but their attack was im-
proved by Panchenko et al. who used a Support Vector Machine
(SVM) and extracted additional features from traffic bursts to clas-
sify Herrmann et al.’s dataset with more than 50% accuracy [22].
Panchenko et al.’s study was also the first to perform an open-
world evaluation of website fingerprinting attacks [22]. Prior work
relied on a closed-world assumption, which assumes that the uni-
verse of possible pages is small enough that adversary can train the
classifier on all sites. The open-world evaluation is appropriate for
a web environment as it accounts for users visiting pages that the
classifier has not been trained on. Based on Herrman et al.’s dataset,
Cai et al. [6] achieved more than 70% accuracy in an open-world
setting. Wang and Goldberg’s [27] approach obtained over 90%
accuracy for 1,000 sites in an open world setting.
The results reported by these attacks were criticized for using
experimental conditions that gave unrealistic advantages to the
adversary, compared to real attack settings [15]. However, new
Figure 1: The client visits an onion service site over the Tor
network. The adversary has access to the (encrypted) link
between the client and the entry to the Tor network. For clar-
ity, we have omitted the six-hop circuit between the client
and the onion service. The attacker cannot observe traffic
beyond the entry node.
techniques have been shown to overcome some of those limitations,
suggesting that attacks may be successful in the wild [28].
Even though an open-world is a more realistic evaluation setting
than a closed world for the web, our evaluation considers a closed
world because: i) the universe of onion services is small enough that
is feasible for an adversary to build a database of fingerprints for
all existing onion services; and ii) we are interested in the best-case
scenario for the adversary because we evaluate the vulnerability to
website fingerprinting from a defender’s point of view.
As in most prior work on website fingerprinting, we only con-
sider the homepages of the websites and not inner pages within a
website. We justify this for onion services by arguing that, given
their unusable naming system and their shallowness in terms of
not having a deep structure, it is reasonable to assume that visitors