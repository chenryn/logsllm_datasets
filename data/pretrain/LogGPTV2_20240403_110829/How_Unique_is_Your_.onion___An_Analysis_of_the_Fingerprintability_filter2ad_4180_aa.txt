# Title: How Unique is Your .onion?: An Analysis of the Fingerprintability of Tor Onion Services

## Authors:
- Rebekah Overdorf, Drexel University, Philadelphia, Pennsylvania, USA
- Marc Juarez, ESAT-COSIC and imec KU Leuven, Leuven, Belgium
- Gunes Acar, imec-COSIC KU Leuven, Leuven, Belgium
- Rachel Greenstadt, Drexel University, Philadelphia, Pennsylvania, USA
- Claudia Diaz, imec-COSIC KU Leuven, Leuven, Belgium

## Abstract
Recent studies have shown that Tor onion (hidden) service websites are particularly vulnerable to website fingerprinting attacks due to their limited number and sensitive nature. In this work, we present a multi-level feature analysis of onion site fingerprintability, considering three state-of-the-art website fingerprinting methods and 482 Tor onion services, making this the largest analysis of its kind completed on onion services to date.

Prior studies typically report average performance results for a given website fingerprinting method or countermeasure. We investigate which sites are more or less vulnerable to fingerprinting and which features make them so. Our findings indicate a high variability in the rate at which sites are classified (and misclassified) by these attacks, implying that average performance figures may not be informative of the risks that website fingerprinting attacks pose to particular sites.

We analyze the features exploited by different website fingerprinting methods and discuss what makes onion service sites more or less easily identifiable, both in terms of their traffic traces and webpage design. We study misclassifications to understand how onion services can be redesigned to be less vulnerable to website fingerprinting attacks. Our results also inform the design of website fingerprinting countermeasures and their evaluation, considering disparate impacts across sites.

## Keywords
Website fingerprinting, Tor, anonymous communications systems, web privacy

## 1. Introduction
Website fingerprinting attacks use supervised classifiers to identify patterns in network traffic traces that are unique to a web page. These attacks can circumvent the protection provided by encryption and the metadata protection of anonymity systems such as Tor. To carry out the attack, the adversary first visits the websites, records the network traffic, and extracts a template or fingerprint for each site. Later, when the victim user connects to the site (possibly through Tor), the adversary observes the victim’s traffic and compares it to the previously recorded templates, trying to find a match. Website fingerprinting can be deployed by adversaries with modest resources who have access to the communication between the user and the Tor entry guard. Potential adversaries include wireless router owners, local network administrators, Internet Service Providers (ISPs), and Autonomous Systems (ASes).

Despite the high success rates initially reported by website fingerprinting attacks, their practicality in real-world scenarios remains uncertain. A 2014 study showed that the success of these attacks is significantly lower in realistic scenarios than in artificial laboratory conditions. Moreover, Panchenko et al. demonstrated that website fingerprinting attacks do not scale to the size of the Web, making it very hard for an adversary to use this attack to recover the browsing history of a Tor user. However, Kwon et al. showed that a website fingerprinting adversary can reliably distinguish onion service connections from other Tor connections, substantially reducing the number of sites to consider. This makes website fingerprinting attacks potentially effective in practice, especially since onion services host sensitive content such as whistleblowing platforms and activist blogs.

In this work, we model the set of onion services as a closed world. Our dataset contains 482 landing pages out of the 1,363 onion services that were crawled. While the exact size of the complete onion service world cannot be known with certainty, onionscan was able to find 4,400 onion services in their latest scan. This indicates that our set, while incomplete, contains a significant portion of the onion service world. We consider that an actual attacker can compile an exhaustive list of onion services, effectively yielding a closed world scenario.

Prior evaluations of website fingerprinting attacks and defenses report aggregate metrics such as average classifier accuracy. However, we find that some websites have significantly more distinctive fingerprints than others, and that average metrics cannot capture this diversity. In this work, we study the fingerprintability of websites and investigate what makes a page more vulnerable to website fingerprinting. This issue has practical relevance because adversaries interested in identifying visits to a particularly sensitive site may not care about the accuracy of the classifier for other sites. Similarly, the administrators of onion services likely care more about the vulnerability of their users to fingerprinting attacks rather than the average vulnerability of onion services to the attack. We extract lessons from our analysis to provide recommendations to onion service designers to better protect their sites against website fingerprinting attacks, including an analysis of a high-profile SecureDrop instance.

### Contributions of This Study
- **Large .onion Study**: We collected the largest dataset of onion services for website fingerprinting to date and evaluated the performance of three state-of-the-art classifiers in successfully identifying onion service sites. Previous studies considered worlds of 30 or 50 onion services, an order of magnitude smaller than our study, which analyzes 482 onion services.
- **Fingerprintability Matters**: While the average accuracy achieved by the classifiers is 80%, we found that some sites are consistently misclassified by all of the methods tested, while others are consistently identified correctly, and yet others provide mixed results. Specifically, 47% of sites in our dataset are classified with greater than 95% accuracy, while 16% of sites were classified with less than 50% accuracy.
- **Correlated Errors**: 31% of misclassified instances were misclassified by all three classifiers, implying that weaknesses of the individual classifiers cannot be fully overcome using ensemble methods. We nonetheless propose an ensemble that combines all three classifiers, slightly improving the results offered by the best individual classifier.
- **Novel Feature Analysis Method**: We present a method for analyzing fingerprintability that considers the relationship between the inter-class variance and intra-class variance of features across sites. The results of this analysis explain which features make a site fingerprintable, independently of the classifier used.
- **Size Matters**: We show that size-based features are the most important in identifying websites and that when sites are misclassified, they are typically confused with sites of comparable size. Large sites are consistently classified with high accuracy.
- **Dynamism Matters for Small Sites**: While large sites are very fingerprintable, some small sites are harder to classify. We find that misclassified small sites tend to have more variance, and that features related to size variability are more distinguishing in sets of small sites.
- **Analysis of Site-Level Features**: We identify which site-level features influence fingerprintability and provide insights into how onion services can be made more robust against website fingerprinting attacks.
- **Insights for Adversarial Learning**: Website fingerprinting is a dynamic, adversarial learning problem. We have conducted an exploratory attack against three different approaches to help site owners and the Tor network design better causative attacks.

## 2. Background and Related Work
Encryption alone does not hide source and destination IP addresses, which can reveal the identities of users and visited websites. Anonymous communications systems such as Tor route communications through multiple relays, concealing the destination server’s address from network adversaries. Tor also supports onion services, which can be reached through Tor while concealing the location and network address of the server.

Website fingerprinting is a traffic analysis attack that allows an attacker to recover the browsing history of a user from encrypted and anonymized streams. Prior work has studied the effectiveness of this attack on HTTPS, encrypted web proxies, OpenSSH, VPNs, and various anonymity systems such as Tor and JAP. We focus on Tor because it is the most popular anonymous communications system, with more than two million daily users.

In website fingerprinting, the adversary is a network eavesdropper who can identify the user by her IP address but does not know which website the user is visiting. The attacker cannot decrypt the communication but can record the network packets generated by the user's activity. To guess the web page that the user has downloaded, the attacker compares the traffic recorded from the user with that of his own visits to a set of websites. The best match is found using a statistical classifier.

### 2.1 Attacks against Tor
In 2009, Herrmann et al. proposed the first website fingerprinting attack against Tor, based on a Naive Bayes classifier and frequency distributions of packet lengths. Their study only achieved an average accuracy of 3% for 775 websites, but their attack was improved by Panchenko et al., who used a Support Vector Machine (SVM) and extracted additional features from traffic bursts to classify Herrmann et al.’s dataset with more than 50% accuracy. Panchenko et al.’s study was also the first to perform an open-world evaluation of website fingerprinting attacks. Prior work relied on a closed-world assumption, which assumes that the universe of possible pages is small enough that the adversary can train the classifier on all sites. The open-world evaluation is appropriate for a web environment as it accounts for users visiting pages that the classifier has not been trained on. Based on Herrman et al.’s dataset, Cai et al. achieved more than 70% accuracy in an open-world setting, and Wang and Goldberg’s approach obtained over 90% accuracy for 1,000 sites in an open-world setting.

The results reported by these attacks were criticized for using experimental conditions that gave unrealistic advantages to the adversary, compared to real attack settings. However, new techniques have been shown to overcome some of those limitations, suggesting that attacks may be successful in the wild. Even though an open-world is a more realistic evaluation setting than a closed world for the web, our evaluation considers a closed world because: i) the universe of onion services is small enough that it is feasible for an adversary to build a database of fingerprints for all existing onion services; and ii) we are interested in the best-case scenario for the adversary because we evaluate the vulnerability to website fingerprinting from a defender’s point of view.

As in most prior work on website fingerprinting, we only consider the homepages of the websites and not inner pages within a website. We justify this for onion services by arguing that, given their unusable naming system and shallowness in terms of structure, it is reasonable to assume that visitors primarily interact with the homepage.