Each site in M-Lab’s platform is conﬁgured to launch a Paris tracer-
oute toward every client that initiates any TCP-based measurement
to the M-Lab test server. For each NDT measurement a client initi-
ates, the server should launch a Paris traceroute toward the client. In
M-Lab’s initial Paris traceroute deployment, the infrastructure ran
this traceroute service as a single-threaded process; consequently, if
the server was performing a traceroute to client c1 while client c2
generated a new NDT measurement, the server would not perform
a traceroute toward client c2.3 The platform also does not explicitly
associate an NDT measurement with its corresponding Paris tracer-
oute; the only way to match NDT measurements to corresponding
traceroutes is to search the data for Paris traceroutes that were ex-
ecuted closely after a client ran an NDT measurement. To perform
this association in the data, we matched NDT tests run by each
client with the ﬁrst traceroute from the server to that same client
within a 10-minute window after the NDT test. With this window,
the available traceroutes during May 2015 allowed us to match 71%
(527,480 out of 743,780 NDT tests) from clients to M-Lab servers
(with both endpoints in the U.S.). If we relaxed the matching win-
dow to allow traceroutes either before or after the NDT test, we
were able to match 87% of NDT tests with traceroutes. Given that
the incomplete matching was a known issue for the M-Lab team,
we analyzed the NDT and Paris traceroute data from March 2017
to check whether the matching fraction had improved. We found
that in March 2017, we were able to match about the same fraction,
76% of NDT tests (4,689,239 out of 6,185,394) from U.S. M-Lab
servers to U.S. clients using a window of 10 minutes after the NDT
test.
Another limitation of M-Lab’s traceroute support is that that
traceroutes are only in one direction (server to client). Clients usu-
ally run the NDT client using the web-browser implementation,
where the client cannot traceroute to the server because traceroute
requires lower level access to sockets. Consequently, paths from
clients to M-Lab servers are not visible in this data set.
4.2 Investigating Assumption # 2: Are servers and
clients in adjacent ASes?
Using the corpus of traceroute data from M-Lab in May 2015 that
we matched with NDT tests (Section 4.1), we investigated Assump-
tion 2, i.e., that server and client ASes were generally directly con-
nected. We extracted traces from all U.S. M-Lab servers to clients
in 12 major U.S. ISPs listed in the Measuring Broadband America
report [20]. To identify clients in various ISPs, we used the preﬁx-
to-AS mapping from CAIDA’s AS-rank project [12], which used
public BGP data from 1-5 May 2015.
Identifying interdomain links in traceroute:
In order to identify whether the server AS and client AS are directly
connected or not, it is necessary to identify the AS boundaries in the
traceroutes. Identifying the interdomain link between the server AS
and client AS in a traceroute path faces several challenges [25]. One
is that in a transition between ASes A and B, the interdomain link
interfaces could be numbered out of either A or B’s address space.
2Section 3.2 of that paper: “Inferring Interdomain Links”.
3M-Lab corrected this issue in 2016 [24], which however led to further issues described
in Section 2.2.
Challenges in Inferring Internet Congestion
IMC ’17, November 1–3, 2017, London, United Kingdom
1 hop
2 hops
2+ hops
117k 89k
56k
59k
13k
1k
39k
6k
4k
were directly connected, or whether there were additional interdo-
main links between the server and the client AS. We considered
sibling ASes as the same AS hop using information from CAIDA’s
AS-to-Organization dataset [13]. To obtain sibling AS lists for the
client ASes, we used a manually curated list of sibling ASes for
each of the top U.S. ISPs that we considered. To curate this list, we
used CAIDA’s AS-to-Organization dataset, Hurricane Electric’s set
of BGP tools [22] and then manually inspected the resulting set to
remove false positives (ASes that were not siblings).
s
t
s
e
t
f
o
n
o
i
t
c
a
r
F
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
C
A
T
V
C
C
C
o
m
T
T
W
C
c
a
s
t
e
riz
o
n
e
h
n
t
u
r
yli
n
a
rt
e
r
k
ISP
o
x
F
r
o
W
i
n
d
n
ti
e
r
s
tr
e
a
m
Time Warner Cable
Figure 1: AS hops traversed in traceroute paths to clients in
9 large access ISPs running measurements on M-Lab in May
2015. The number above each bar denotes the number of tracer-
outes matched with NDT tests from M-Lab servers toward that
ISP. The largest U.S. ISPs are mostly directly connected to the
ASes hosting M-Lab servers. Charter, Cox, and Frontier are
notable for having a smaller fraction of measurements that tra-
verse just one AS hop. Assumption 2 does not always hold for
these ISPs.
There are further challenges such as third party addresses that ap-
pear in traceroute that may confuse the identiﬁcation of AS bound-
aries. Finally, we do not control the source or the destination of the
traceroute, and therefore cannot perform additional measurements.
Fortunately, a recent effort by Marder et al. [28] focused on ex-
actly the problem of inferring interdomain links in a set of tracer-
outes that have already been collected. The MAP-IT algorithm
works on the basic premise that a single traceroute is insufﬁcient
to identify the AS borders that were traversed. Instead, collating to-
gether multiple traceroutes provides more constraints that can be
used to infer which interfaces represent an interdomain link. The
MAP-IT algorithm uses traceroutes along with additional infor-
mation such as preﬁx-AS mappings, AS relationship data, AS-to-
Organization data, and list of IXP preﬁxes to infer each AS bound-
ary. This approach effectively handles the challenges posed by nam-
ing point-to-point interfaces from /30 or /31 preﬁxes, minimizes
the impact of third-party addresses and load balancing, and corrects
for mistaken inferences due to low visibility of certain interfaces in
traceroute paths. Marder et al. showed that the algorithm achieved
more than 90% accuracy on the datasets they tested.
We processed the entire set of matched traceroutes from May
2015 through the MAP-IT [28] algorithm. In addition to the
traceroute-derived adjacencies, we used CAIDA’s preﬁx-AS map-
ping derived from BGP routing tables from May 1-5, 2015,
CAIDA’s AS-Organization mapping [13] from July 2015 (the clos-
est available snapshot to May 2015), and a list of IXP preﬁxes ob-
tained from peeringDB [34] and PCH [32] as input to MAP-IT. For
each Paris traceroute from the server to client, we then used the in-
ference from MAP-IT to determine if the server AS and client AS
ISP
Comcast
AT&T
Verizon
CenturyLink
Charter
Cox
Cablevision
Frontier
Suddenlink
Windstream
Mediacom
Number of subscribers (Q3 2015)
23,329,000
15,778,000
13,313,000
9,228,000
6,048,000
5,572,000
4,300,000
2,809,000
2,444,000
1,467,000
1,095,100
1,085,000
Table 1: Broadband access providers in the United States with
more than one million subscribers as of Q3 2015 (retrieved from
Wikipedia [2] page history)
Analyzing connectivity between Server and Client ASes:
We found that 82% of the 383k traces we could analyze toward the
12 ISPs had the server AS connected directly to the client AS. How-
ever, this fraction varied considerably by ISP: 91% for AT&T, 96%
for Comcast, 82% for CenturyLink, 86% for Verizon, 75% for Time
Warner Cable, but only 37% for Charter, 39% for Cox, and 47% for
Frontier (Figure 1). In particular, Charter, Cox, and Frontier were all
in the top 10 ISPs in the U.S. in Q3 2015, yet had a much smaller
fraction of tests that traversed a single AS hop from the server to
client. Table 1 lists the broadband access providers in the US with
more than one million subscribers as of Q3 2015. Correlating these
numbers with Figure 1, we ﬁnd that the top 5 broadband providers
— Comcast, AT&T, Time Warner Cable, Verizon and CenturyLink
— had a high fraction (greater than 80% for all except Time Warner,
and greater than 90% for Comcast and AT&T) of observed paths
with just one AS hop from the server to client. The fractions were
lower for ISPs ranked between 5 and 10 (Charter, Cox, and Fron-
tier). Windstream was ranked 11 in terms of subscribers in Q3 2015,
and had only 6% of tests that traversed a single AS hop.
It is important to note that M-Lab servers are hosted in commer-
cial networks; the connectivity between those networks and broad-
band access providers is driven by the economic incentives of those
ASes, and all networks hosting M-Lab servers may not choose to
connect directly with all access providers, as we observed in our
data. Indeed, we ﬁnd that even for the top 5 ISPs, there is a small
fraction of tests that traverse one (or even two) AS hops between
server and client. These cases are due to M-Lab servers in networks
that do not have direct peering agreements with those access ISPs.
IMC ’17, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
Summary: We conclude that the assumption of direct connection
between server AS and client AS during May 2015 appeared to be
true for the top 5 U.S. residential broadband access providers as of
2015, and not always true for 3 of the top 10 providers. Clearly,
when analyzing NDT tests between a given server and client AS,
care must be taken to ensure that the server and client AS are di-
rectly connected, using traceroutes and a technique to identify AS
boundaries in traceroutes. Given the dynamic nature of AS-level
interconnection, these conditions merit periodic re-examination.
4.3
Investigating Assumption # 3: diversity of
interconnection to access providers
As discussed in Section 3.1, the simpliﬁed AS-level tomographic
approach used in the original M-Lab report [27] implicitly assumes
that either a) all measurements between that server to clients in the
access AS traverse a single IP link or router-level interdomain in-
terconnection, or b) that all IP links or router-level interconnections
traversed by those measurements are similar in performance. These
assumptions are required because ideally tests should not be aggre-
gated across multiple links; if they are aggregated, they should be
across links that are likely to behave similarly. Claffy et al. [14]
discuss that interdomain congestion often shows regional effects.
Consequently, aggregating tests across links is particularly prob-
lematic if those links are in different geographical regions, as they
could vary widely in terms of diurnal throughput patterns. The M-
Lab service uses proximity-based server selection to try to ensure
a client performs its measurement to the geographically closest M-
Lab server. We investigate the validity of Assumption 3 by explor-
ing the topological diversity of interconnection links between an
M-Lab server and NDT clients in an access ISP AS, i.e., the set of
IP-level interdomain links traversed in tests from a server to client.
Identiﬁcation of IP-level interdomain links
In Section 4.2, we used MAP-IT to identify the interdomain links
in traceroute paths from May 2015 for the purposes of AS adja-
cency analysis. We reuse that same dataset to investigate the diver-
sity of router-level interconnection, as it contains all the informa-
tion necessary to identify the IP-level interdomain link traversed
in a traceroute path. Speciﬁcally, for NDT tests (which could be
matched with a corresponding Paris traceroute) from a server in
AS S to clients in access AS A, we examine the traceroutes and
determine (using MAP-IT) which IP-level interdomain links those
traceroutes traversed.
Fine-grained link-level topological analysis
Our results conﬁrmed that AS-level aggregation of measurements
masked the diversity of interconnection between ASes. Table 2 lists
the number of interdomain links observed from an M-Lab server in
Atlanta hosted by Level 3 to 6 access ISPs, and the number of NDT
measurements performed across all observed interconnections with
that access ISP in May 2015. The third column lists the number of
NDT tests that traversed each interdomain IP link between Level3
and that ISP. Only a single ISP, Frontier, has a signiﬁcant number of
tests (107) that cross a single interdomain IP link. All paths to other
ISPs either have a small representation of measurements (< 100),
or cross multiple interdomain IP links. Distribution of measure-
ments across interdomain links is not uniform. Comcast’s AS22909
Client ISP (ASN)
# Links
# NDT tests per link
Comcast (AS7922)
Comcast (AS7725)
Comcast (AS22909)
AT&T (AS7018)
Verizon (AS701)
Verizon (AS6167)
Cox (AS22773)
Frontier (AS5650)
CenturyLink (AS209)
2
1
1
14
8
2
39
1
4
1759,8
1650
1130
2395,820,770,216,137,
25,21,19,19,17,17,8,2,1
548,62,54,42,20,2,1,1
3,3
total 817, max 378
107
383,39,22,1
Table 2: Interdomain links to top U.S. ISPs seen by M-Lab
server atl01 (Level 3) in Atlanta (May 2015), with the number of
tests traversing each link. We only show the top 3 ASN borders
from Level 3 to Comcast with the highest number of tests – in
reality the data showed 18 unique AS-level links between Level
3 and Comcast, and 30 unique IP-level interdomain links dis-
tributed across these ASNs. Distribution of tests across interdo-
main links is not uniform: Comcast’s AS22909 had 1,130 tests
traversing one interdomain IP link, while Comcast’s AS7922
had 1767 tests traversing two interdomain IP links.
had 1,130 measurements traversing one interdomain IP link, while
Comcast’s AS7922 had a total of 1767 measurements traversing
two interdomain IP links (with an uneven distribution). Overall, we
found that the tests to Comcast traversed 18 different AS-level links
between Level3 and Comcast, comprising 30 unique IP links. We
found that a majority of measurements (2395) to AT&T (AS7018)
traversed a single IP link in Atlanta (we found the geographic lo-
cation using reverse DNS lookups of the inferred interdomain hop),
the next highest number (820) crossed an IP link in Washington DC,
and 770 measurements crossed an IP link in New York City. There-
fore the assumption that measurements aggregated at the AS level
reﬂect a single connection between the server and client ASes in a
given geographic region does not hold in all cases. The observed ge-
ographical spread of the interdomain links is especially problematic
given the possibility of regional congestion effects [14].
A limitation of the MAP-IT algorithm is that it does not operate
at the router level, and hence cannot reveal the presence of parallel
IP links between the same pair of border routers. We used DNS
names to resolve interdomain IP links into router-level intercon-
nects for the 39 inferred interdomain links from Level 3 to Cox
(AS22773), which seemed an abnormally high number. Of those
39, 12 interdomain interfaces in the level3.net domain had DNS
names “COX-COMMUNI.edge5.Dallas3.Level3.net” that hinted
that they were parallel links to Cox from the same Level3 router
in Dallas. Another 5 IP links with the same DNS name “COX-
COMMUNI.ear1.SanJose3.Level3.net” indicate that these were on
a single router in San Jose. DNS entries indicated that there were
two more groups of parallel links in Washington D.C. (7 links) and
Los Angeles (9 links).
Summary
Based on our analysis, we conclude that aggregating NDT through-
put measurement results at an AS granularity masks the fact that
Challenges in Inferring Internet Congestion
IMC ’17, November 1–3, 2017, London, United Kingdom
different measurements could cross different IP-level links, some-