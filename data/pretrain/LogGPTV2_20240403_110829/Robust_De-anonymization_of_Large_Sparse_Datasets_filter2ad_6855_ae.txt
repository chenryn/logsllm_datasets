is not the only or even the most important threat. A
water-cooler conversation with an ofﬁce colleague about
her cinematographic likes and dislikes may yield enough
information, especially if at least a few of the movies
mentioned are outside the top 100 most rated Netﬂix
movies. This information can also be gleaned from per-
sonal blogs, Google searches, and so on.
One possible source of a large number of per-
sonal movie ratings is the Internet Movie Database
(IMDb) [17]. We expect that for Netﬂix subscribers who
use IMDb, there is a strong correlation between their pri-
vate Netﬂix ratings and their public IMDb ratings.8 Our
attack does not require that all movies rated by the sub-
scriber in the Netﬂix system be also rated in IMDb, or
vice versa.
In many cases, even a handful of movies
that are rated by a subscriber in both services would
be sufﬁcient to identify his or her record in the Net-
ﬂix Prize dataset (if present among the released records)
with enough statistical conﬁdence to rule out the possi-
bility of a false match except for a negligible probability.
Due to the restrictions on crawling IMDb imposed by
IMDb’s terms of service (of course, a real adversary may
not comply with these restrictions), we worked with a
very small sample of around 50 IMDb users. Our results
should thus be viewed as a proof of concept. They do
not imply anything about the percentage of IMDb users
who can be identiﬁed in the Netﬂix Prize dataset.
The auxiliary information obtained from IMDb is
quite noisy. First, a signiﬁcant fraction of the movies
rated on IMDb are not in Netﬂix, and vice versa, e.g.,
8We are not claiming that a large fraction of Netﬂix subscribers use
IMDb, or that many IMDb users use Netﬂix.
122
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
Figure 10. Effect of increasing error in
Aux.
Figure 11. Entropic de-anonymization:
same parameters as in Fig. 6.
movies that have not been released in the US. Second,
some of the ratings on IMDb are missing (i.e., the user
entered only a comment, not a numerical rating). Such
data are still useful for de-anonymization because an av-
erage user has rated only a tiny fraction of all movies, so
the mere fact that a person has watched a given movie
tremendously reduces the number of anonymous Netﬂix
records that could possibly belong to that user. Finally,
IMDb users among Netﬂix subscribers fall into a con-
tinuum of categories with respect to rating dates, sepa-
rated by two extremes: some meticulously rate movies
on both IMDb and Netﬂix at the same time, and others
rate them whenever they have free time (which means
the dates may not be correlated at all). Somewhat off-
setting these disadvantages is the fact that we can use all
of the user’s ratings publicly available on IMDb.
Because we have no “oracle” to tell us whether the
record our algorithm has found in the Netﬂix Prize
dataset based on the ratings of some IMDb user indeed
belongs to that user, we need to guarantee a very low
false positive rate. Given our small sample of IMDb
users, our algorithm identiﬁed the records of two users
in the Netﬂix Prize dataset with eccentricities of around
28 and 15, respectively. These are exceptionally strong
matches, which are highly unlikely to be false posi-
tives: the records in questions are 28 standard devia-
tions (respectively, 15 standard deviations) away from
the second-best candidate.
Interestingly, the ﬁrst user
was de-anonymized mainly from the ratings and the sec-
ond mainly from the dates. For nearly all the other IMDb
users we tested, the eccentricity was no more than 2.
Let us summarize what our algorithm achieves.
Given a user’s public IMDb ratings, which the user
posted voluntarily to reveal some of his (or her; but we’ll
use the male pronoun without loss of generality) movie
likes and dislikes, we discover all ratings that he entered
privately into the Netﬂix system. Why would someone
who rates movies on IMDb—often under his or her real
name—care about privacy of his Netﬂix ratings? Con-
sider the information that we have been able to deduce
by locating one of these users’ entire movie viewing his-
tory in the Netﬂix Prize dataset and that cannot be de-
duced from his public IMDb ratings.
First, his political orientation may be revealed by his
strong opinions about “Power and Terror: Noam Chom-
sky in Our Times” and “Fahrenheit 9/11,” and his reli-
gious views by his ratings on “Jesus of Nazareth” and
“The Gospel of John.” Even though one should not
make inferences solely from someone’s movie prefer-
ences, in many workplaces and social settings opinions
about movies with predominantly gay themes such as
“Bent” and “Queer as folk” (both present and rated in
this person’s Netﬂix record) would be considered sensi-
tive. In any case, it should be for the individual and not
for Netﬂix to decide whether to reveal them publicly.
6 Conclusions
We have presented a de-anonymization methodol-
ogy for sparse micro-data, and demonstrated its prac-
123
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
the purpose of the data release is precisely to foster com-
putations on the data that have not even been foreseen at
the time of release 9, and are vastly more sophisticated
than the computations that we know how to perform in
a privacy-preserving manner.
An intriguing possibility was suggested by Matthew
to release the
Wright via personal communication:
the column identiﬁers (i.e., movie
records without
It is
names in the case of the Netﬂix Prize dataset).
not clear how much worse the current data mining al-
gorithms would perform under this restriction. Further-
more, this does not appear to make de-anonymization
impossible, but merely harder. Nevertheless, it is an in-
teresting countermeasure to investigate.
Acknowledgements. This material is based upon work
supported by the NSF grant IIS-0534198, and the ARO
grant W911NF-06-1-0316.
The authors would like to thank Ilya Mironov for
many insightful suggestions and discussions and Matt
Wright for suggesting an interesting anonymization
technique. We are also grateful to Justin Brickell,
Shuchi Chawla, Jason Davis, Cynthia Dwork, and Frank
McSherry for productive conversations.
References
[1] N. Adam and J. Worthmann.
Security-control
methods for statistical databases: A comparative
study. ACM Computing Surveys, 21(4), 1989.
[2] C. Aggarwal. On k-anonymity and the curse of
dimensionality. In VLDB, 2005.
[3] R. Agrawal and R. Srikant. Privacy-preserving
data mining. In SIGMOD, 2000.
[4] C. Anderson. The Long Tail: Why the Future of
Business Is Selling Less of More. Hyperion, 2006.
[5] A. Blum, C. Dwork, F. McSherry, and K. Nissim.
Practical privacy: The SuLQ framework. In PODS,
2005.
[6] A. Blum, K. Ligett, and A. Roth. A learning theory
In
approach to non-interactive database privacy.
STOC, 2008.
[7] E. Brynjolfsson, Y. Hu, and M. Smith. Consumer
surplus in the digital economy. Management Sci-
ence, 49(11), 2003.
9As of February 2008, the current best algorithm in the Netﬂix
Prize competition is a combination of 107 different techniques.
Figure 12. Effect of knowing approxi-
mate number of movies rated by victim
(±50%). Adversary knows approximate
ratings (±1) and dates (14-day error).
tical applicability by showing how to de-anonymize
movie viewing records released in the Netﬂix Prize
dataset. Our de-anonymization algorithm Scoreboard-
RH works under very general assumptions about the
distribution from which the data are drawn, and is ro-
bust to data perturbation and mistakes in the adversary’s
knowledge. Therefore, we expect that it can be success-
fully used against any dataset containing anonymous
multi-dimensional records such as individual transac-
tions, preferences, and so on.
We conjecture that the amount of perturbation that
must be applied to the data to defeat our algorithm will
completely destroy their utility for collaborative ﬁlter-
ing. Sanitization techniques from the k-anonymity liter-
ature such as generalization and suppression [27, 9, 20]
do not provide meaningful privacy guarantees, and in
any case fail on high-dimensional data. Furthermore, for
most records simply knowing which columns are non-
null reveals as much information as knowing the speciﬁc
values of these columns. Therefore, any technique such
as generalization and suppression which leaves sensitive
attributes untouched does not help.
Other possible countermeasures include interactive
mechanisms for privacy-protecting data mining such
as [5, 12], as well as more recent non-interactive tech-
niques [6]. Both support only limited classes of com-
putations such as statistical queries and learning halfs-
paces. By contrast, in scenarios such as the Netﬂix Prize,
124
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
[8] S. Chawla, C. Dwork, F. McSherry, A. Smith, and
H. Wee. Towards privacy in public databases. In
TCC, 2005.
[9] V. Ciriani, S. De Capitani di Vimercati, S. Foresti,
and P. Samarati. k-anonymity. Secure Data Man-
agement in Decentralized Systems, 2007.
[10] C. D´ıaz, S. Seys, J. Claessens, and B. Preneel. To-
wards measuring anonymity. In PET, 2003.
[11] C. Dwork. Differential privacy. In ICALP, 2006.
[12] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data anal-
ysis. In TCC, 2006.
[13] Electronic Privacy Information Center. The Video
Privacy Protection Act (VPPA). http://epic.
org/privacy/vppa/, 2002.
[14] D. Frankowski, D. Cosley, S. Sen, L. Terveen, and
J. Riedl. You are what you say: privacy risks of
public mentions. In SIGIR, 2006.
[15] K. Hafner. And if you liked the movie, a Netﬂix
contest may reward you handsomely. New York
Times, Oct 2 2006.
[16] S. Hansell. AOL removes search data on vast group
of web users. New York Times, Aug 8 2006.
[17] IMDb. The Internet Movie Database. http://
www.imdb.com/, 2007.
[18] J. L. W. V. Jensen. Sur les fonctions convexes et les
in´egalit´es entre les valeurs moyennes. Acta Math-
ematica, 30(1), 1906.
[19] J. Leskovec, L. Adamic, and B. Huberman. The
dynamics of viral marketing. In EC, 2006.
[20] A. Machanavajjhala, J. Gehrke, D. Kifer, and
M. Venkitasubramaniam. l-diversity: Privacy be-
yond k-anonymity. In ICDE, 2006.
[21] A. Machanavajjhala, D. Martin, D. Kifer,
J. Gehrke, and J. Halpern. Worst case background
knowledge. In ICDE, 2007.
[22] B. Malin and L. Sweeney. How (not) to protect
genomic data privacy in a distributed network: us-
ing trail re-identiﬁcation to evaluate and design
anonymity protection systems. J. of Biomedical In-
formatics, 37(3), 2004.
[23] Netﬂix.
FAQ.
Netﬂix Prize:
http://
www.netflixprize.com/faq, Downloaded
on Oct 17 2006.
[24] A. Serjantov and G. Danezis. Towards an informa-
tion theoretic metric for anonymity. In PET, 2003.
[25] L. Sweeney. Weaving technology and policy to-
J. of Law,
gether to maintain conﬁdentiality.
Medicine and Ethics, 25(2–3), 1997.
[26] L. Sweeney.
Achieving k-anonymity privacy
protection using generalization and suppression.
International J. of Uncertainty, Fuzziness and
Knowledge-based Systems, 10(5), 2002.
[27] L. Sweeney. k-anonymity: A model for protect-
ing privacy. International J. of Uncertainty, Fuzzi-
ness and Knowledge-based Systems, 10(5):557–
570, 2002.
[28] J. Thornton. Collaborative ﬁltering research pa-
http://jamesthornton.com/cf/,
pers.
2006.
A Glossary of terms
Symbol Meaning
D
Database
ˆD
Released sample
N
Number of rows
M
Number of columns
m
Size of aux
X
Domain of attributes
⊥
Null attribute
supp(.)
Set of non-null attributes in a row/column
Sim
Similarity measure
Aux
Auxiliary information sampler
aux
Auxiliary information
Score
Scoring function

Sparsity threshold
δ
Sparsity probability
θ
Closeness of de-anonymized record
ω
Probability that de-anonymization succeeds
r, r(cid:1)
Record
Π
P.d.f over records
HS
Shannon entropy
H
De-anonymization entropy
φ
Eccentricity
125
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply.