32 MB
Additionally, exFAT implements certain features previously available only
in NTFS, such as support for access control lists (ACLs) and transactions
(called Transaction-Safe FAT, or TFAT). While the Windows Embedded CE
implementation of exFAT includes these features, the version of exFAT in
Windows does not.
 Note
ReadyBoost (described in Chapter 5 of Part 1, “Memory Management”)
can work with exFAT-formatted flash drives to support cache files much
larger than 4 GB.
NTFS
As noted at the beginning of the chapter, the NTFS file system is one of the
native file system formats of Windows. NTFS uses 64-bit cluster numbers.
This capacity gives NTFS the ability to address volumes of up to 16
exaclusters; however, Windows limits the size of an NTFS volume to that
addressable with 32-bit clusters, which is slightly less than 8 petabytes (using
2 MB clusters). Table 11-5 shows the default cluster sizes for NTFS volumes.
(You can override the default when you format an NTFS volume.) NTFS also
supports 232–1 files per volume. The NTFS format allows for files that are 16
exabytes in size, but the implementation limits the maximum file size to 16
TB.
Table 11-5 Default cluster sizes for NTFS volumes
Volume Size
Default Cluster Size
<7 MB
Not supported
7 MB–16 TB
4 KB
16 TB–32 TB
8 KB
32 TB–64 TB
16 KB
64 TB–128 TB
32 KB
128 TB–256 TB
64 KB
256 TB–512 TB
128 KB
512 TB–1024 TB
256 KB
1 PB–2 PB
512 KB
2 PB–4 PB
1 MB
4 PB–8 PB
2 MB
NTFS includes a number of advanced features, such as file and directory
security, alternate data streams, disk quotas, sparse files, file compression,
symbolic (soft) and hard links, support for transactional semantics, junction
points, and encryption. One of its most significant features is recoverability.
If a system is halted unexpectedly, the metadata of a FAT volume can be left
in an inconsistent state, leading to the corruption of large amounts of file and
directory data. NTFS logs changes to metadata in a transactional manner so
that file system structures can be repaired to a consistent state with no loss of
file or directory structure information. (File data can be lost unless the user is
using TxF, which is covered later in this chapter.) Additionally, the NTFS
driver in Windows also implements self-healing, a mechanism through which
it makes most minor repairs to corruption of file system on-disk structures
while Windows is running and without requiring a reboot.
 Note
At the time of this writing, the common physical sector size of disk
devices is 4 KB. Even for these disk devices, for compatibility reasons,
the storage stack exposes to file system drivers a logical sector size of 512
bytes. The calculation performed by the NTFS driver to determine the
correct size of the cluster uses logical sector sizes rather than the actual
physical size.
Starting with Windows 10, NTFS supports DAX volumes natively. (DAX
volumes are discussed later in this chapter, in the “DAX volumes” section.)
The NTFS file system driver also supports I/O to this kind of volume using
large pages. Mapping a file that resides on a DAX volume using large pages
is possible in two ways: NTFS can automatically align the file to a 2-MB
cluster boundary, or the volume can be formatted using a 2-MB cluster size.
ReFS
The Resilient File System (ReFS) is another file system that Windows
supports natively. It has been designed primarily for large storage servers
with the goal to overcome some limitations of NTFS, like its lack of online
self-healing or volume repair or the nonsupport for file snapshots. ReFS is a
“write-to-new” file system, which means that volume metadata is always
updated by writing new data to the underlying medium and by marking the
old metadata as deleted. The lower level of the ReFS file system (which
understands the on-disk data structure) uses an object store library, called
Minstore, that provides a key-value table interface to its callers. Minstore is
similar to a modern database engine, is portable, and uses different data
structures and algorithms compared to NTFS. (Minstore uses B+ trees.)
One of the important design goals of ReFS was to be able to support huge
volumes (that could have been created by Storage Spaces). Like NTFS, ReFS
uses 64-bit cluster numbers and can address volumes of up 16 exaclusters.
ReFS has no limitation on the size of the addressable values, so,
theoretically, ReFS is able to manage volumes of up to 1 yottabyte (using 64
KB cluster sizes).
Unlike NTFS, Minstore doesn’t need a central location to store its own
metadata on the volume (although the object table could be considered
somewhat centralized) and has no limitations on addressable values, so there
is no need to support many different sized clusters. ReFS supports only 4 KB
and 64 KB cluster sizes. ReFS, at the time of this writing, does not support
DAX volumes.
We describe NTFS and ReFS data structures and their advanced features
in detail later in this chapter.
File system driver architecture
File system drivers (FSDs) manage file system formats. Although FSDs run
in kernel mode, they differ in a number of ways from standard kernel-mode
drivers. Perhaps most significant, they must register as an FSD with the I/O
manager, and they interact more extensively with the memory manager. For
enhanced performance, file system drivers also usually rely on the services of
the cache manager. Thus, they use a superset of the exported Ntoskrnl.exe
functions that standard drivers use. Just as for standard kernel-mode drivers,
you must have the Windows Driver Kit (WDK) to build file system drivers.
(See Chapter 1, “Concepts and Tools,” in Part 1 and
http://www.microsoft.com/whdc/devtools/wdk for more information on the
WDK.)
Windows has two different types of FSDs:
■    Local FSDs manage volumes directly connected to the computer.
■    Network FSDs allow users to access data volumes connected to
remote computers.
Local FSDs
Local FSDs include Ntfs.sys, Refs.sys, Refsv1.sys, Fastfat.sys, Exfat.sys,
Udfs.sys, Cdfs.sys, and the RAW FSD (integrated in Ntoskrnl.exe). Figure
11-17 shows a simplified view of how local FSDs interact with the I/O
manager and storage device drivers. A local FSD is responsible for
registering with the I/O manager. Once the FSD is registered, the I/O
manager can call on it to perform volume recognition when applications or
the system initially access the volumes. Volume recognition involves an
examination of a volume’s boot sector and often, as a consistency check, the
file system metadata. If none of the registered file systems recognizes the
volume, the system assigns the RAW file system driver to the volume and
then displays a dialog box to the user asking if the volume should be
formatted. If the user chooses not to format the volume, the RAW file system
driver provides access to the volume, but only at the sector level—in other
words, the user can only read or write complete sectors.
Figure 11-17 Local FSD.
The goal of file system recognition is to allow the system to have an
additional option for a valid but unrecognized file system other than RAW.
To achieve this, the system defines a fixed data structure type
(FILE_SYSTEM_RECOGNITION_STRUCTURE) that is written to the first
sector on the volume. This data structure, if present, would be recognized by
the operating system, which would then notify the user that the volume
contains a valid but unrecognized file system. The system will still load the
RAW file system on the volume, but it will not prompt the user to format the
volume. A user application or kernel-mode driver might ask for a copy of the
FILE_SYSTEM_RECOGNITION_STRUCTURE by using the new file system
I/O control code FSCTL_QUERY_FILE_SYSTEM_RECOGNITION.
The first sector of every Windows-supported file system format is reserved
as the volume’s boot sector. A boot sector contains enough information so
that a local FSD can both identify the volume on which the sector resides as
containing a format that the FSD manages and locate any other metadata
necessary to identify where metadata is stored on the volume.
When a local FSD (shown in Figure 11-17) recognizes a volume, it creates
a device object that represents the mounted file system format. The I/O
manager makes a connection through the volume parameter block (VPB)
between the volume’s device object (which is created by a storage device
driver) and the device object that the FSD created. The VPB’s connection
results in the I/O manager redirecting I/O requests targeted at the volume
device object to the FSD device object.
To improve performance, local FSDs usually use the cache manager to
cache file system data, including metadata. FSDs also integrate with the
memory manager so that mapped files are implemented correctly. For
example, FSDs must query the memory manager whenever an application
attempts to truncate a file to verify that no processes have mapped the part of
the file beyond the truncation point. (See Chapter 5 of Part 1 for more
information on the memory manager.) Windows doesn’t permit file data that
is mapped by an application to be deleted either through truncation or file
deletion.
Local FSDs also support file system dismount operations, which permit
the system to disconnect the FSD from the volume object. A dismount occurs
whenever an application requires raw access to the on-disk contents of a
volume or the media associated with a volume is changed. The first time an
application accesses the media after a dismount, the I/O manager reinitiates a
volume mount operation for the media.
Remote FSDs
Each remote FSD consists of two components: a client and a server. A client-
side remote FSD allows applications to access remote files and directories.
The client FSD component accepts I/O requests from applications and
translates them into network file system protocol commands (such as SMB)
that the FSD sends across the network to a server-side component, which is a
remote FSD. A server-side FSD listens for commands coming from a
network connection and fulfills them by issuing I/O requests to the local FSD
that manages the volume on which the file or directory that the command is
intended for resides.
Windows includes a client-side remote FSD named LANMan Redirector
(usually referred to as just the redirector) and a server-side remote FSD
named LANMan Server (%SystemRoot%\System32\Drivers\Srv2.sys).
Figure 11-18 shows the relationship between a client accessing files remotely
from a server through the redirector and server FSDs.
Figure 11-18 Common Internet File System file sharing.
Windows relies on the Common Internet File System (CIFS) protocol to
format messages exchanged between the redirector and the server. CIFS is a
version of Microsoft’s Server Message Block (SMB) protocol. (For more
information on SMB, go to https://docs.microsoft.com/en-
us/windows/win32/fileio/microsoft-smb-protocol-and-cifs-protocol-
overview.)
Like local FSDs, client-side remote FSDs usually use cache manager
services to locally cache file data belonging to remote files and directories,
and in such cases both must implement a distributed locking mechanism on
the client as well as the server. SMB client-side remote FSDs implement a
distributed cache coherency protocol, called oplock (opportunistic locking),
so that the data an application sees when it accesses a remote file is the same
as the data applications running on other computers that are accessing the
same file see. Third-party file systems may choose to use the oplock
protocol, or they may implement their own protocol. Although server-side
remote FSDs participate in maintaining cache coherency across their clients,
they don’t cache data from the local FSDs because local FSDs cache their
own data.
It is fundamental that whenever a resource can be shared between multiple,
simultaneous accessors, a serialization mechanism must be provided to
arbitrate writes to that resource to ensure that only one accessor is writing to
the resource at any given time. Without this mechanism, the resource may be
corrupted. The locking mechanisms used by all file servers implementing the
SMB protocol are the oplock and the lease. Which mechanism is used
depends on the capabilities of both the server and the client, with the lease
being the preferred mechanism.
Oplocks
The oplock functionality is implemented in the file system run-time library
(FsRtlXxx functions) and may be used by any file system driver. The client of
a remote file server uses an oplock to dynamically determine which client-
side caching strategy to use to minimize network traffic. An oplock is
requested on a file residing on a share, by the file system driver or redirector,
on behalf of an application when it attempts to open a file. The granting of an
oplock allows the client to cache the file rather than send every read or write
to the file server across the network. For example, a client could open a file
for exclusive access, allowing the client to cache all reads and writes to the
file, and then copy the updates to the file server when the file is closed. In
contrast, if the server does not grant an oplock to a client, all reads and writes
must be sent to the server.
Once an oplock has been granted, a client may then start caching the file,
with the type of oplock determining what type of caching is allowed. An
oplock is not necessarily held until a client is finished with the file, and it
may be broken at any time if the server receives an operation that is
incompatible with the existing granted locks. This implies that the client must
be able to quickly react to the break of the oplock and change its caching
strategy dynamically.
Prior to SMB 2.1, there were four types of oplocks:
■    Level 1, exclusive access This lock allows a client to open a file for
exclusive access. The client may perform read-ahead buffering and
read or write caching.
■    Level 2, shared access This lock allows multiple, simultaneous
readers of a file and no writers. The client may perform read-ahead
buffering and read caching of file data and attributes. A write to the
file will cause the holders of the lock to be notified that the lock has
been broken.
■    Batch, exclusive access This lock takes its name from the locking
used when processing batch (.bat) files, which are opened and closed
to process each line within the file. The client may keep a file open on
the server, even though the application has (perhaps temporarily)
closed the file. This lock supports read, write, and handle caching.
■    Filter, exclusive access This lock provides applications and file
system filters with a mechanism to give up the lock when other clients
try to access the same file, but unlike a Level 2 lock, the file cannot be
opened for delete access, and the other client will not receive a
sharing violation. This lock supports read and write caching.
In the simplest terms, if multiple client systems are all caching the same
file shared by a server, then as long as every application accessing the file
(from any client or the server) tries only to read the file, those reads can be
satisfied from each system’s local cache. This drastically reduces the network
traffic because the contents of the file aren’t sent to each system from the
server. Locking information must still be exchanged between the client
systems and the server, but this requires very low network bandwidth.
However, if even one of the clients opens the file for read and write access
(or exclusive write), then none of the clients can use their local caches and all
I/O to the file must go immediately to the server, even if the file is never
written. (Lock modes are based upon how the file is opened, not individual
I/O requests.)
An example, shown in Figure 11-19, will help illustrate oplock operation.
The server automatically grants a Level 1 oplock to the first client to open a
server file for access. The redirector on the client caches the file data for both
reads and writes in the file cache of the client machine. If a second client
opens the file, it too requests a Level 1 oplock. However, because there are
now two clients accessing the same file, the server must take steps to present
a consistent view of the file’s data to both clients. If the first client has
written to the file, as is the case in Figure 11-19, the server revokes its oplock
and grants neither client an oplock. When the first client’s oplock is revoked,
or broken, the client flushes any data it has cached for the file back to the
server.
Figure 11-19 Oplock example.
If the first client hadn’t written to the file, the first client’s oplock would
have been broken to a Level 2 oplock, which is the same type of oplock the
server would grant to the second client. Now both clients can cache reads,
but if either writes to the file, the server revokes their oplocks so that
noncached operation commences. Once oplocks are broken, they aren’t
granted again for the same open instance of a file. However, if a client closes
a file and then reopens it, the server reassesses what level of oplock to grant
the client based on which other clients have the file open and whether at least
one of them has written to the file.
EXPERIMENT: Viewing the list of registered file
systems
When the I/O manager loads a device driver into memory, it
typically names the driver object it creates to represent the driver so
that it’s placed in the \Driver object manager directory. The driver
objects for any driver the I/O manager loads that have a Type
attribute value of SERVICE_FILE_SYSTEM_DRIVER (2) are
placed in the \FileSystem directory by the I/O manager. Thus, using
a tool such as WinObj (from Sysinternals), you can see the file
systems that have registered on a system, as shown in the following
screenshot. Note that file system filter drivers will also show up in
this list. Filter drivers are described later in this section.
Another way to see registered file systems is to run the System
Information viewer. Run Msinfo32 from the Start menu’s Run
dialog box and select System Drivers under Software
Environment. Sort the list of drivers by clicking the Type column,
and drivers with a Type attribute of
SERVICE_FILE_SYSTEM_DRIVER group together.
Note that just because a driver registers as a file system driver
type doesn’t mean that it is a local or remote FSD. For example,
Npfs (Named Pipe File System) is a driver that implements named
pipes through a file system-like private namespace. As mentioned
previously, this list will also include file system filter drivers.
Leases
Prior to SMB 2.1, the SMB protocol assumed an error-free network
connection between the client and the server and did not tolerate network
disconnections caused by transient network failures, server reboot, or cluster
failovers. When a network disconnect event was received by the client, it
orphaned all handles opened to the affected server(s), and all subsequent I/O
operations on the orphaned handles were failed. Similarly, the server would
release all opened handles and resources associated with the disconnected
user session. This behavior resulted in applications losing state and in
unnecessary network traffic.
In SMB 2.1, the concept of a lease is introduced as a new type of client
caching mechanism, similar to an oplock. The purpose of a lease and an
oplock is the same, but a lease provides greater flexibility and much better
performance.
■    Read (R), shared access Allows multiple simultaneous readers of a
file, and no writers. This lease allows the client to perform read-ahead
buffering and read caching.
■    Read-Handle (RH), shared access This is similar to the Level 2
oplock, with the added benefit of allowing the client to keep a file
open on the server even though the accessor on the client has closed
the file. (The cache manager will lazily flush the unwritten data and
purge the unmodified cache pages based on memory availability.)
This is superior to a Level 2 oplock because the lease does not need to
be broken between opens and closes of the file handle. (In this
respect, it provides semantics similar to the Batch oplock.) This type
of lease is especially useful for files that are repeatedly opened and
closed because the cache is not invalidated when the file is closed and
refilled when the file is opened again, providing a big improvement in
performance for complex I/O intensive applications.
■    Read-Write (RW), exclusive access This lease allows a client to
open a file for exclusive access. This lock allows the client to perform
read-ahead buffering and read or write caching.
■    Read-Write-Handle (RWH), exclusive access This lock allows a
client to open a file for exclusive access. This lease supports read,
write, and handle caching (similar to the Read-Handle lease).
Another advantage that a lease has over an oplock is that a file may be
cached, even when there are multiple handles opened to the file on the client.
(This is a common behavior in many applications.) This is implemented
through the use of a lease key (implemented using a GUID), which is created
by the client and associated with the File Control Block (FCB) for the cached
file, allowing all handles to the same file to share the same lease state, which
provides caching by file rather than caching by handle. Prior to the
introduction of the lease, the oplock was broken whenever a new handle was
opened to the file, even from the same client. Figure 11-20 shows the oplock
behavior, and Figure 11-21 shows the new lease behavior.
Figure 11-20 Oplock with multiple handles from the same client.
Figure 11-21 Lease with multiple handles from the same client.
Prior to SMB 2.1, oplocks could only be granted or broken, but leases can
also be converted. For example, a Read lease may be converted to a Read-
Write lease, which greatly reduces network traffic because the cache for a
particular file does not need to be invalidated and refilled, as would be the
case with an oplock break (of the Level 2 oplock), followed by the request
and grant of a Level 1 oplock.
File system operations
Applications and the system access files in two ways: directly, via file I/O
functions (such as ReadFile and WriteFile), and indirectly, by reading or
writing a portion of their address space that represents a mapped file section.