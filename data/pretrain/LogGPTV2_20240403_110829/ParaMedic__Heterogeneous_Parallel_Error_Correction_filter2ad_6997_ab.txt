single error detect, single error correct (SEDSEC) capability,
using Hamming codes [20]. Abbreviations are Address ECC
(AE), Old Data ECC (ODE), Unused (U), Load (L) and Store
(S). Load and store bits are placed such that they are the ﬁrst
bit seen in a packet when read in reverse on an unroll.
stores, so as to ensure that memory values are correct for the
register checkpoint we are reverting to. We can therefore only
reallocate a load-store log segment once all prior segments
have been successfully validated.
The cases in which this new constraint can affect perfor-
mance are those where utilisation of the checker cores is
reduced, and thus the main core has to be stalled for longer,
waiting for load-store log space. This is only signiﬁcant when
the runtime length of each check is highly variable. By keeping
them at a similar instruction length in any given phase, the
additional slowdown can be minimised.
D. Register Checkpoints
For detection alone, for each segment of the load-store log, we
must store a checkpoint of the register ﬁle as the main core
saw it at the end of that particular sequence of instructions.
If this matches what the checker core produces, then there
is no error. However, to perform error correction, we must
store and be able to roll back to the checkpoint at
the
start of a segment (i.e., the checkpoint at the end of the
previous segment). This increases the length of time we must
keep the starting register checkpoint before overwriting it—
essentially the previous segment’s register checkpoint cannot
be overwritten until the following segment commits. However,
the previous segment’s undo log can still be ﬁlled while the
current segment is being checked, and so no slowdown is
caused by this: if the main core must stall waiting to be able
to overwrite a register checkpoint, it would also stall waiting
for the subsequent load-store log segment to become free.
ECC is unnecessary on register checkpoints while the asso-
ciated partition is being checked. This is because information
redundancy is achieved by the two execution copies, one from
the main core and one from the checker core. Still, since we
may need to roll back to a register checkpoint after it has
been checked, due to a later error, a window of vulnerability
is introduced. We can mitigate this with ECC on the register
checkpoints but, crucially, this is only necessary once the
204
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
5
Committed Error
Speculative Error
Time Committed Time Data
Addr
Time Data Addr
0
7
8
8
11
0
1
1
7
7
0
0
0
0x84
0x84
0
0
0
0xBE
0xBE
0
5
3
3
9
0
0x43
0x84
0x84
0x84
0
0xA4
0xBE
0xBE
0xBE
Fig. 4: An example of hard error tracking, using the execution
of ﬁgure 3 as a starting point. A speculative error occurs at
time 5, and is checked against the current committed error, but
does not match. When the earlier time 3 error is checked, this
replaces the value in the speculative-error register. Finally,
when time 3 is committed, this data goes into the committed-
error register. The program is re-executed, and the same error
is detected, triggering hard error recovery and maintaining
forward progress.
updated correctly even after rollbacks. An example of this is
shown in ﬁgure 4.
If no other main cores are available we can still recover from
hard faults by effectively checking the code on two different
checker cores. We store the data and address of the committed
error, using the value from the ﬁrst checker core. We then
restart execution, ensuring the new start point is scheduled for
checking on a different core. If the next error matches, it is
assumed that the main core is incorrect, and the checker cores
correct. The program is rolled back to immediately before the
error occurred, the correct operation proceeds, and execution
is continued on the main core, with the register ﬁle copied
from a checker core immediately after the operation.
G. Context Switches
As with all interrupts, context switches trigger new check-
points, to avoid replaying them in the middle of a check. On
a rollback, this behaves correctly: the stores are rolled back for
the process that has been switched to, followed by the stores
for the earlier process, as a result of the timestamps for each
core having a total ordering regardless of process.
If we have to roll back an incorrect computation, on
the successive attempt at running the computation a context
switch may be triggered at a different point. However, this
should cause no issues, as the second execution is checked
independently of the ﬁrst failed attempt.
One complication with rolling back stores is that the load-
store log structure tracks store addresses in the virtual address
space, to avoid having to translate before checking. This means
that to write back to the correct physical location, the rollback
hardware must have access to the TLB, and also we must
store a process ID per partitioned load-store log segment. As
we switch entry on a context switch, we only have to store a
single process ID for each segment.
H. Kernel Code
In addition to user-space code, ParaMedic must also check
kernel-level code to maintain correct execution. This is also
Fig. 3: Example of an error being rolled back, followed by
discovery of an earlier error. Though it would not harm cor-
rectness to repeat roll back of entries marked as “R”, treating
them as committed prevents redundant work. Abbreviations are
Committed (C), Validating (V), Error (E), Undoing (U), Rolled
back (R), Filling (F).
F. Hard Faults
Transient errors are unlikely to occur twice in the same place,
so we can successfully recover from them by returning the
system to a consistent state and beginning execution again
with associated checking, which should succeed the second
time. However, this is not the case with hard faults, either in
the core itself or within the checker hardware of a core, where
the same error will be detected twice. Further complicating this
is that the same hard fault may manifest in different locations
in a checker log even if we assume the log is in program order,
due to non-deterministic scheduling. This makes a hard fault
difﬁcult to distinguish from a soft fault.
Our solution aims to maintain forward progress in the
presence of hard faults by keeping track of the address/data
pair of the most recent error detected in a previous-error log.
The address is either a memory location or an architectural
register, depending on whether the fault was detected in
the register checkpoint or within a memory instruction. On
detection of an error we check this pair and if there is a match
then we move to a different checker core for the subsequent re-
execution after rollback. If that doesn’t succeed in correcting
the issue, we can then move to different main core and checker
unit for the next attempt.
To succeed in detecting hard faults, this scheme ensures
the error written back to the tracking hardware is the ﬁrst
error in a sequence. Before we update the previous-error log,
all prior checks must have been completed. We do this by
keeping both a speculative-error register and a committed-
error register. The former is updated whenever an error occurs
that has an earlier timestamp than the existing error. The
latter is updated when the committed timestamp goes past
the timestamp of the speculative-error register. All new errors
are compared with the committed-error register. This won’t
necessarily identify all hard errors as being permanent, but
is sufﬁcient to maintain forward progress. As the timestamp
is monotonically increasing, the committed-error register is
205
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
separated from user-space code by register checkpoints that are
taken when any transfer to kernel mode occurs (e.g., system
calls or hardware interrupts). Kernel-level registers must also
be stored in the necessary checkpoints in this case. The error
detection process works similarly, as all data the kernel-level
code must read will also be forwarded in the load-store log.
While checker cores themselves do not need access to kernel
memory while performing this check, as loads and stores are
forwarded, kernel-level code rollback does need to be able to
write to kernel memory. As data may have propagated to the
kernel code from unprivileged code and vice versa through,
for example, system call arguments, we roll back both kinds
of checkpoint in the detection of an error in either. In effect,
we treat the two identically.
I. Writes Outside the System
To avoid having to report errors and corrections to other
systems, a fault tolerant system should only allow correct
results to escape from its sphere of replication. We therefore
need to make sure that all data that leaves the system has
been checked. Similarly, all unrepeatable reads and writes
(for example, to some IO devices) must be non-speculative
in terms of errors. To do this, we must stop the system,
issue a check, and wait until the check has completed before
issuing communications with other systems, or more general
unrepeatable reads and writes. We consider all uncacheable
data as coming from an unrepeatable operation and force
checks to complete before the operation succeeds.
This could introduce a signiﬁcant delay in the execution
of such operations, when the number of instructions between
checkpoints is large. However, if they are infrequent, this can-
not cause a signiﬁcant performance loss. If they are frequent,
then each checkpoint will be small and the latency between
each operation will thus be similarly small. Performance will
in effect be limited by the checker cores, as the enforced
checkpoints will cause every instruction between nonspecula-
tive reads/writes to be in the same checkpoint. However, since
such code is likely to be IO-bound, this is unlikely to reduce
performance signiﬁcantly, as the smaller cores will likely be
able to keep up with larger ones. We have two cases: buffered
IO only results in overhead at the single point of the DMA
request, rather than on all IO operations, and unbuffered IO
is not compute bound so can be adequately serviced despite
lacking parallelism in ParaMedic in this scenario. Still, we
come up with a scheme to ameliorate these issues, and others,
using dynamic checkpoint scheduling, in section IV-D.
J. Summary
This section has dealt with the properties of ParaMedic
necessary to develop a heterogeneous parallel error-correction
scheme, under the assumption that a system has a single
main core. The load-store log must be extended to include
overwritten data, so we can roll back writes on detection of an
error. These old values must be covered by ECC, to make sure
rolled back values are correct, but most of this data can be re-
covered from the memory system. A stricter commit ordering
(cid:16)(cid:12)(cid:17)(cid:17)(cid:18)(cid:11)(cid:11)(cid:14)(cid:19)(cid:20)(cid:21)(cid:18)(cid:17)(cid:14)(cid:22)(cid:11)(cid:23)(cid:17)(cid:24)(cid:25)(cid:20)(cid:3)(cid:26)
(cid:10)(cid:24)(cid:14)(cid:27)(cid:28)(cid:29)(cid:23)(cid:11)(cid:18)(cid:30)(cid:14)(cid:20)(cid:21)(cid:18)(cid:17)(cid:14)(cid:22)(cid:11)(cid:23)(cid:17)(cid:24)(cid:25)(cid:20)(cid:4)(cid:6)
(cid:10)(cid:11)(cid:12)(cid:13)(cid:14) (cid:1)(cid:2)(cid:15)(cid:1)
(cid:3)(cid:4)(cid:20)(cid:31)(cid:20)(cid:3)(cid:26)
(cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:5) (cid:6)(cid:7)(cid:8)(cid:4)(cid:5)(cid:9)(cid:10)(cid:8)(cid:11) (cid:12)(cid:9)(cid:10)(cid:9)(cid:4)
(cid:1)
(cid:1)
(cid:1)
(cid:1)(cid:2)(cid:3)(cid:1)
(cid:1)(cid:2)(cid:4)(cid:1)
(cid:1)(cid:2)(cid:6)(cid:1)
(cid:3)(cid:4)
(cid:3)(cid:1)
(cid:7)(cid:8)(cid:9)
(cid:5)
(cid:5)
(cid:10)
Fig. 5: An example store into the L1 cache. At the current
timestamp, 93, a write miss occurs. We cannot evict the line
at address 0x80 because its timestamp (89) is more recent than
the committed timestamp (85), and so potentially-incorrect
data could be written out to the L2 cache.
needs to be placed on the checkpoints taken for each parallel
check, to allow sequential rollback of data. We can correct
hard faults by tracking the repetition of error observations, to
guarantee forward progress, and to give the illusion of a fault-
free system to the outside world, IO operations are delayed
until all previous instructions are checked. The next section
deals with error correction for multicore CPUs.
IV. MULTICORE CORRECTION
The solution presented in the previous section works for
single core processors. However, when multiple main cores
are included in a system the problem of correcting errors
becomes more complicated because we have to know far data
has propagated when committing and on an error.
As an example, suppose a check fails and ParaMedic rolls
back to an earlier checkpoint. How do we know which other
cores have seen data affected by the rollback, since they will
also need to be rolled back to a correct state? Conversely, how
do we know that all data used by a particular load-store log
segment has been validated, so it is safe to commit the current
entry? What ordering should we apply when two load-store log
segments read data generated by the other? How do we know
the order in which stores should be undone when we have
multiple load-store logs being written to concurrently?
We solve all these issues by mandating that data must be
checked before communication can occur. If a core cannot see
uncommitted data from another core, errors are independent,
and failed checks can be rolled back independently. However,
we need to minimise the overheads of enforcing checks-
before-communication. The following section discusses tech-
niques used to achieve this, while still allowing high perfor-
mance even for workloads with communication between cores.
A. Cacheline Timestamps
To prevent data propagation to other cores until it has been
checked, we keep modiﬁed data within private caches until it
has been committed. A timestamp, local to each core rather
than to each process, is stored per cacheline in the L1 cache
and data is only allowed to leave if its line hasn’t been modi-
ﬁed or if the line’s timestamp is less than or equal to the most
recently committed timestamp. This means that modiﬁed data
cannot be evicted through the cache’s replacement policy or
206
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
coherence requests (either invalidating the line or changing to
shared state). An example is given in ﬁgure 5. As uncommitted
data cannot escape to shared caches, we do not need to extend
this scheme further than the L1, reducing the storage overhead.
If a check fails, we roll back and undo stores by overwriting
new cached data with its old value, leaving the timestamp
unchanged (i.e., we do not undo the timestamp update that
occurred with the store that is being reverted). As the failed
checks are marked as ready to commit, but are not committed,
this behaves correctly if an earlier check subsequently fails.
When all previous checks have been completed, the timestamp
will be below the last committed timestamp, and can thus
be considered committed. This is a safe overapproximation:
we may falsely mark data that was written before any
uncommitted timestamps as uncommitted, however we still
guarantee forward progress, as once all previous timestamps
have committed, execution will be able to continue.
B. Data Eviction
To prevent unchecked data escaping the L1, we can only evict
data once it has been committed. In cases where the data
must be evicted, we pause the main core until the data has
been checked. If the timestamp of the data is the same as
that currently executing on the core, we immediately issue a
checkpoint to start checking the data, to avoid deadlock.
As the core must stall at
this point, we wish to do it
as infrequently as possible. We thus bias against evicting
uncommitted data by favouring eviction of other data within
the cache set ﬁrst. We stop the core if all cache lines within
a set are uncommitted and modiﬁed, so one must be evicted.
A 36KiB load-store log can hold between 1,500 and 4,500
entries, approximately, assuming 64-bit words and depending
on the ratio of loads to stores in the log. By comparison, a
typical 32KiB data cache can store approximately 4,000 64-
bit words, or 500 cache lines. However, multiple accesses
to the same location take up multiple entries in the load-
store log. Thus, provided there is some temporal or spatial
locality, uncommitted data is unlikely to be evicted from the
L1 as a result of a capacity miss. However, conﬂict misses in
low-associativity caches are possible. These can be mitigated
by increasing the associativity of the cache, or by using an
eviction buffer for uncommitted writes, which is stored in
order of timestamp.
C. Coherence Requests
In addition to eviction by other data and through invalidation,
requests from the cache coherence protocol can also force data
to be written to either the upper level caches, or the private
caches of other cores. If a request is observed to a cache line
that is currently uncommitted, the response must be delayed
until the relevant check completes. This means that writes
become visible in commit-order of checkpoints. If the write
timestamp in the local cache is the same as the timestamp
currently executing on the main core, again it is necessary to
issue an early checkpoint. The coherence request further takes