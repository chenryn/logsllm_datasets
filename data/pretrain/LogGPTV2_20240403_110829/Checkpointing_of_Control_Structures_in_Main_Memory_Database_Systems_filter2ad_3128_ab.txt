•  After system initialization, the database server sends the
• 
image of all the control structures to the image keeper.
In the following processing, a client/service acquires a
mutex and then performs operations on the control struc-
tures. On each write operation, any changes to the data
are stored in the local buffer.
•  After all updates are successfully finished, the mutex is
released, and the client/service delivers the buffered in-
crements to the image keeper for maintaining the up-to-
date checkpoint of the control structures.
•  Upon a crash while the mutex is held, the cleanup ser-
vice requests from the image keeper the latest checkpoint
data and restores the corrupted control structures.
Handling mutex overlaps. In some cases, a single section of
control structures is protected by multiple mutexes. To prop-
erly handle this scenario, the image keeper maintains, in addi-
tion to the checkpoint, the mapping between a mutex and the
data section(s) protected by this mutex. To assist in the map-
ping, the checkpoint increments sent to the image keeper pig-
gyback the mutex id and the information necessary to identify
the correct sections in the control structures.
Figure 3 depicts an example configuration of control structure
images kept in the image keeper and illustrates the mapping
of mutexes to control structures. With this mapping, over-
lapped data sections can be protected and their consistency
with the corresponding copies in SysDB can be preserved,
even when multiple mutexes are acquired at the same time.
Handling data access without mutex protection. The pro-
posed algorithm works correctly as long as all updates to con-
trol structures are performed within the mutex blocks. There
are, however, cases in which control structures are updated
directly, without mutex protection, e.g. during database ini-
tialization (when it is assumed that no processes try to access
the database). While this example is a rather benign case,
practice shows that application developers often make some-
what arbitrary decisions and allow the accessing of control
structures without mutex protection3. Handling such scenar-
ios would require (i) locating, in the application code, all the
places of potential updates outside mutex blocks and (ii)
augmenting the implementation to ensure the checkpoint in
the image keeper is up-to-date. This can be difficult given the
size and complexity of real-world applications, such as our
target database system. Delta checkpointing, discussed next,
is an attempt to alleviate this problem.
data
structure 1
data
structure 2
data
structure n
.
.
.
mutex 1
mutex 2
.
.
.
mutex m-1
mutex m
Figure 3: Control Structure Images
Delta Checkpointing
5.2
The delta checkpointing algorithm is based on the following
assumption:
In a correctly implemented system, any access
to control structures outside the mutex blocks, after system
initialization, does not violate data consistency. Conse-
quently, crashes outside the mutex blocks do not cause data
inconsistency, and sections of control structures not updated
by any currently executing mutex block are always consis-
tent4. As a result, the image keeper does not need to maintain
a copy of all control structures (as in incremental checkpoint-
ing, discussed in the previous section). It is sufficient to pre-
serve data sections modified (plus information on the type
and parameters of the update operation) while executing a
given mutex block. In other words, the algorithm only needs
to recognize, collect, and send to the image keeper the modi-
fied data section, delta (delta is a before-image). Upon a fail-
ure of a client/service while executing a mutex block, the
primary copy of the control structures still exists in the shared
memory. The entire image of the related structures (base) can
then be delivered to the image keeper. Using delta and base,
the image keeper computes the original (at the time of enter-
ing the mutex block) control structures image (orig =
base+delta) and sends it back to the cleanup process for re-
covering the data. The possible updates include data inser-
tion, deletion, and replacement. In summary, the algorithm
includes two basic steps:
(1) When a client/service acquires a mutex, it sends the delta
to the image keeper. The delta's content depends on the
update to be performed in the mutex block.
(2) Upon a crash while the mutex is held, the cleanup ser-
vice sends the base to the image keeper. The image
keeper merges the base with the saved delta and gener-
ates the valid image of control structures. The cleanup
service requests from the image keeper the regenerated
image and restores the corrupted control structures.
3 Identification of all cases of updates outside mutex blocks would require
reviewing/profiling the entire code base. We could not do this due to limited
access to the database.
4 Under this assumption, incremental checkpointing (Section 5.1) still needs
to determine all locations in the code where SysDB is updated.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:36:17 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm
Similarities
Differences
Table 1: Comparison of Incremental and Delta Checkpointing Algorithms
Incremental Checkpointing
Delta Checkpointing
Close checkpointing architecture; small overhead; no checkpoint taken for read-only access; same way of handling mutex
overlap and access to external devices while holding a mutex.
1. At start time – an image of all control structures is stored
as an initial checkpoint;
2. At runtime – a post-transaction (upon transaction comple-
tion) state of the control structure(s) accessed by each write
transaction is collected and merged with the current check-
point;
3. At recovery time – the checkpointed image of control struc-
tures is directly loaded to the shared memory from the image
keeper;
4. Must checkpoint data updates due to operations within and
outside mutex blocks.
1. At start time – an initial checkpoint is an empty data set,
i.e., no need to store any control structures;
2. At runtime – a pre-transaction (before data updates occur)
state of control structure(s) accessed by a given transaction
(write or read-only) is preserved as a current (delta) check-
point;
3. At recovery time – the image of control structures from the
shared memory is merged with the latest delta stored in the
image keeper;
4. Must checkpoint only updates due to operations within
mutex blocks.
Image Keeper
Observe that the image keeper merges the base with the latest
delta it receives. To avoid using the wrong delta in the case
of recovery, it is important to send out a delta at each mutex
acquisition, even if the delta is empty, i.e., no changes to the
control structures were performed during the current mutex
block.
5.3
The image keeper is a separate element within the ARMOR
process that collects and maintains checkpoint data represent-
ing the correct state of control structures. It is a passive com-
ponent, which means that it is only invoked by the incoming
messages (checkpoint updates) and that it performs proper
actions according to the received messages. Figure 4 illus-
trates the basic structure of the image keeper, which consists
of (i) a set of memory blocks for preserving images of control
data structures (control structure images) and (ii) manage-
ment support (manager) for updates of the checkpoint and
recovery actions in response to client failures. The image
keeper communicates with the database processes by means
of the ARMOR communication channel.
The control structure images in Figure 4 represent a memory
pool that stores the images of control structures in SysDB.
Different mutexes map their corresponding data sections into
the copy of control structures in the image keeper (Figure 3).
ARMOR
communication
channel
checkpointing msg
control structure image
ARMOR Microkernel
.
. .
Element 1
Element 2
Manager
Control
Structure
Images
Image Keeper
Figure 4: Structure of the Image Keeper
Performance Evaluation
6
This section presents performance measurements of the pro-
totype implementation of the ARMOR-based incremental
checkpointing scheme applied to the target database system5.
The testbed consists of a Sun Blade 100 workstation running
the Solaris 8 operating system on top of a 500MHz UltraS-
PARC-II CPU with 128MB of memory. The measurements
are conducted in (i) error-free scenarios, in which normal
operation of the database under a synthetic workload mimics
actual database activity, and (ii) error-recovery scenarios, in
which the database recovers from the checkpoint after a fail-
ure while executing transactions issued by synthetic clients.
While checkpointing is applied in the context of the file table
mutex, the proposed solution applies to other mutexes as well.
6.1
Performance of ARMOR-based Incremental
Checkpointing in Error-Free Scenarios
Workload. Each workload invocation involves execution of a
sequence of transactions, which arrive with a predefined fre-
quency, and each transaction is represented as a set of opera-
tions of variable execution time. Some of the operations need
to acquire the file table mutex (mutex) to preserve mutual
exclusion in accessing shared data by multiple clients.
Operations associated with mutex acquisition can be either
data write (write) or data read (read). The operation pattern
within a transaction is a sequence of alternate reads and
writes, e.g., read – write – read – write – …. The sleep func-
tion is used to emulate the execution time of operations that
(i) do not require mutex acquisition (mutex-free operations),
(ii) occur when the mutex is held (mutex operations), or (iii)
represent idle time, i.e., the period after completion of a cur-
rent transaction and before arrival of the next.
Parameters. The workload is flexible and can be configured
to mimic actual execution scenarios. The tunable workload
parameters (and other experiment settings) are as follows:
•  Transaction frequency (freq) – number of transactions
arriving within one second.
•  Number
of mutex
acquisitions
per
transaction
(num_acq).
•  Percentage of read-only operations (read_per) – fraction
of mutex acquisitions for read-only operations.
•  Mutex operation time (mutex_op) – processing time
while holding the mutex, i.e., the interval between the
time the mutex is granted and the time it is released.
5 Due to the limited time for accessing the target system, we provide meas-
urements only for incremental checkpointing. Because checkpoint data
transmission time dominates the performance of both schemes in error-free
scenarios and both algorithms transmit a similar amount of data, it is ex-
pected that the performance of the two schemes is similar.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:36:17 UTC from IEEE Xplore.  Restrictions apply. 
•  Mutex-free operation time (other_op) – processing time
for mutex-free operations within the transaction.
•  Delivered data – the amount of checkpointed data.
In
the case of the file table mutex, the typical data to be
checkpointed is a single file entry in the file table
(approx. 3700 bytes). In all measurements, the size of the
checkpointed data is assumed to be 4000 bytes.
•  Experiment duration – time duration of the experiment.
The transaction frequency should satisfy the following re-
quirement for experiments to run correctly:
1/freq >= mutex_op*num_acq + other_op +
time{get/release mutexes+checkpointing}
as
are
follows:
Results. The workload configuration parameters for perform-
ance measurements
num_acq=5,
mutex_op=0.002s, other_op=0.03s, delivered data=4000
bytes, experiment duration=20s.
Table 2 shows the time per transaction (with 95% confidence
intervals) for four transaction frequencies and with read-only
percentages (read_per) ranging from 0% to 100%. The trans-
action time includes mutex-free operations, mutex operations,
mutex acquisition/release, and checkpointing time. Table 3
depicts the performance overhead of checkpointing per trans-
action. The case of read_per=100% is the one without
checkpointing, and hence, it is the baseline against which the
overheads in other scenarios are computed and compared.
Table 2:Transaction Time [s]
freq
(1/s)
0.75
1.25
1.75
2.25
0
0.109
±.0001
0.115
±.0095
0.124
±.0122
0.130
±.0116
20
0.102
±.0001
0.104
±.0027
0.115
±.0070
0.121
±.0068
read_per(%)
60
40
0.102
±.0002
0.103
±.0008
0.107
±.0039
0.113
±.0067
0.102
±.0002
0.104
±.0017
0.104
±.0026
0.103
±.0015
80
0.101
±.0002
0.102
±.0007
0.105