in the query results returned. The naive solution to run the pre-
processing preparation of the database each time it changes, brings
prohibitive efﬁciency cost. We would like to avoid processing each
record in the database for updates that affect a small fraction of it.
From a different point of view, though, the updates of the database
can be considered private information of the server and thus the in-
formation about what records have been changed is a privacy leak-
age to any other party (in our case to the IS who holds the Bloom
ﬁlter search structures ). This type of leakage comes inherent with
the efﬁciency requirement we posed above — if the update pro-
cessing does not touch a record, clearly it has not been modiﬁed.
Therefore, we accept the update pattern leakage as a necessary pri-
vacy trade-off for usable cost of the updates.
Now we look at the speciﬁc information that changes at the IS in
the SADS scheme, and consider whether it has leakage beyond the
update pattern of the documents:
• Bloom ﬁlters: As we discussed before, if we use the same hash
function for the Bloom ﬁlters of all documents, then the search
structures reveal the similarities between documents. In the case of
an update this would be indicative to what fraction of the content
of the document has been changed.
If, however, each BF has a
different set if hash functions, the update of a document would also
include a selection of a new set of hash functions for its BF as well.
The only information that the IS could derive from the update will
be the change of the length of the document based on the number of
1’s in the BF. However, this information can be obtained also from
the length of the encrypted document that the IS is storing. In both
cases, we can eliminate this leakage by padding the documents.
• Encrypted documents — Each encrypted document stored at
the IS index is re-encrypted with a key that the IS obtains in an
oblivious transfer execution with the data owner.
If an existing
document is modiﬁed, and the server encrypts it with the same key,
then the IS can also use the same re-encryption key. If the server is
adding a new document to his database, though, he should generate
a new key of type k” (i.e., the encryption keys that the documents
are encrypted with after the re-encryption of the IS). The guaran-
tee that we want to provide is that the server does not know the
permutation image of the key k(cid:48)(cid:48) that is used in the re-encryption
of the document. We also want to avoid executing oblivious trans-
fer for each document, which results in a complexity greater than
the database size. Each permutation over n + 1 elements can be
presented as the product of a permutation over the ﬁrst n of the
elements and a transposition of one of the n elements and the last
unused element. Thus, it is sufﬁcient to execute a protocol where
the IS obtains re-encryption keys for the new document and for a
random document from the rest. Intuitively this guarantees that the
new re-encryption key could be assigned to any of the old docu-
ments or the new one, and if it was used for a previously existing
document, then the new one receives the re-encryption key that was
released from that document.
5.3 Optimizations
During the preprocessing stage, for each database document a
Bloom ﬁlter containing its keywords is generated.
In the SADS
scheme, adding a keyword to the BF of a document involves en-
crypting the keyword under the server’s key. Thus, preprocessing
documents containing the same keyword incurs repeated effort. In
order to avoid this unnecessary preprocessing cost, we can cache
the BF indices for keywords. This avoids some recomputation, but
requires additional storage space. Whether to do this, and how
much to cache, depends on the nature of the documents and re-
peat frequency. This is also applicable in the case when multiple
hash functions are used where the preprocessing of a keyword is
not identical but shares a common and expensive intermediary re-
sult that can be reused. The caching capability we implement uses
LRU removal policy.
In addition, SADS preprocesses each item of the dataset inde-
pendently (i.e., computes the BF search structure for it), and fur-
thermore, it handles the elements of each item separately (each
word/value is inserted into the Bloom ﬁler after a cryptographic
transformation). This computational independence makes for sim-
ple and robust parallelization. The search phase, especially when
using multiple hash functions, also permits parallelization of the
computation of the search indices for a query. We used the open
source Threading Building Blocks library [21] to implement the
parallelization optimization. It is easy to use and well-integrated
with C++. After analyzing the source code we found out that there
is just one integer counter that we need to synchronize among the
different threads: the Bloom ﬁlters counter. It took roughly 10 lines
of code to parallelize the entire preprocessing phase – similar for
the search phase too.
6. EVALUATION
To evaluate the practicality of our proposed extensions we imple-
mented them in SADS (roughly 4 Klocs of C++ code in total) and
we performed a number of measurements using realistic datasets:
(i) the email dataset that was made public after the Enron scan-
dal [31] and (ii) a synthetic dataset with personal information for
100K persons. The Enron dataset consists of about half a million
emails with an average size of 900 bytes after stemming . During
the preprocessing phase of SADS, a distinct Bloom ﬁlter for each
email was created. Then, each of the email ﬁles was tokenized and
the tokens where stored in the corresponding Bloom ﬁlter, after
they were properly encrypted. The format of the second dataset is
more close to a database than a collection of documents. Its schema
consists of a single table with 51 attributes of three types: strings
(ﬁrst name, last name, etc.), numbers (height, SSN, etc.) and ﬁle
links (ﬁngerprint, private key, security image, etc.) and it is stored
in a ﬂat CSV (Comma Separated Value) ﬁle. The total size of that
dataset, along with the ﬁles pointed in the records, is 51GB and the
average size for a record is 512KB. During the preprocessing phase
we created a distinct Bloom ﬁlter for each record and each of the at-
tribute values where inserted after it was preﬁxed with the attribute
name (“name_value”) and properly encrypted. In both cases, we
conﬁgured the BF parameters so as the false positive rate would be
less than 10−6.
The experimental evaluation setup was comprised by two servers
and a client laptop. The servers had two four-quad Intel Xeon
2.5GHz CPUs, 16 GB of RAM, two 500 GB hard disk drives, and
a 1 Gbit ethernet interface. The laptop was equipped with an In-
tel Core2 Duo 2.20GHz CPU, 4 GB of RAM, a 220 GB hard disk
drive, and a 100 Mbit ethernet interface. All of them were con-
nected through a Gigabit switch; they all ran a 64-bit ﬂavor of the
Ubuntu operating system. QR and IS were running on each of the
servers, the queries were performed from the laptop. When Doc-
ument Retrieval was enabled, the File Server was running on the
same host with the IS.
6.1 Memory Consumption
Along with the timing measurements, we also monitored the
memory consumption of the extended SADS system to determine
scaling limits. We found out that the only signiﬁcant factor was
the type of Bloom ﬁlter storage. Bloom ﬁlters are stored either
sequentially in a ﬂat ﬁle or transposed using the slicing optimiza-
tion. In the sequential storage case memory usage was constant; it
grew consistently with the dataset size in the slicing case, because
the structures are kept in memory and written to ﬁles at the end.
During the search phase, both the client and the QR used a small,
constant amount of memory (∼2MB). On the other hand, the IS’s
memory usage grew with the dataset size. In the sequential stor-
age case, the ﬁle was mmap’ed; the amount of memory used was
the Bloom ﬁlter size in bytes times the number of BFs (e.g. 1KB
* 50K = 50MB). When the slicing optimization was enabled, we
saw higher memory usage, ∼109MB for the same dataset. That
was most likely due to the extensive use of C++ vectors, which we
can further optimize in the case of much larger databases where the
available RAM may become an issue.
6.2
Implementation Optimizations
We performed experiments using variable-sized subsets of both
datasets while changing the size of the cache. As for the Enron
dataset, we show that a good cache size is 5K keywords. This gives
us a ∼90% hit ratio, while reducing the preprocessing time for 50K
emails from 2h to 10m. Performing the same experiments for the
synthetic dataset yielded slightly worse results, as some attribute
values are unique. However, using a 10K keywords cache the hit
ratio was 50% on the full dataset, which still is a signiﬁcant gain.
We measured the speedup of the preprocessing phase on the full
datasets, while increasing the number of threads. As we expected,
the speedup grew linearly until the number of threads reached the
number of cores in our servers – that is eight. When the number
of threads was more than the CPU cores, the speedup slightly de-
Statistics
Column
min/ avg/ max
Query Time
msec (stdev)
# of Ranges
# (stdev)
Age
Height
Weight
SSN
38/ 95
2/
58/
78
67/
90/ 175/ 280
1M/3.9G/6.5G
1,529 (244)
959 (123)
1,979 (345)
12,783 (805)
4.3 (0.6)
2.8 (0.4)
5.8 (0.9)
38.0 (2.4)
* the time for a single keyword query is ~400 msec
Table 1: Range queries timing on integer attributes of the syn-
thetic dataset (100K records – 51GB).
dataset size. That is because for each document we have to recal-
culate the hash functions and recompute the Bloom ﬁlter indices.
Finally, we see that taking advantage of the commonly used mul-
ticore architectures does increase the performance of the search in
the multiple hashing scheme. More precisely, the speedup when
we used 8 threads on our 8-core servers was from 1.3 to almost
4 for the dataset sizes shown in the Figure 5. Thus, although the
multiple hash functions feature increases the computation factor,
we can amortize a great part of it by executing it in parallel.
It
is also worth noting that the multiple hash functions plus parallel
searching conﬁguration provides better performance that the orig-
inal conﬁguration, while on the same time it improves the privacy
guarantees.
Next, we evaluate the performance overhead of the multiple hash
functions in boolean queries, and more precisely OR queries. To
optimize the normal case – i.e., when the slicing optimization is
not enabled – we skip BFs that already contain one of the search
terms. That way we avoid searching over and over on Bloom Filters
that already match the OR query thus reducing the overall search-
ing time, especially when the search terms are frequent. Figure 6
shows the search time for OR queries under different SADS con-
ﬁgurations. Each cluster of bars is for a different dataset size; each
bar is for a different term count in the boolean OR query. The ﬁrst
bar is for two terms, the second for three, and the last two for four
and ﬁve, respectively. The fact that the search time in each cluster
grows sub-linearly to the number of terms clearly shows the perfor-
mance gain.
6.4 Range Queries
The implementation of the range queries extension on top of
SADS translates a range to a variable-sized OR query. In the av-
erage case, the number of the terms in the OR query depends on
the size of the numbers in the range and the size of the range it-
self. To evaluate the practicality of that approach, we measured the
time for performing range queries over the numeric attributes of
the synthetic dataset. These are age, height, weight and SSN. The
range of the values of these attributes relative small, except for the
SSN which spans from one million to a few billions (ﬁrst column
of Table 1). For each of the attributes, we calculated ten ranges
that each match about 1/10 of the dataset. SADS was conﬁgured
to use multiple hash functions and the parallel search optimization
was enabled. Table 1 shows the average query time over the ten
queries for each attribute, along with the average number of bi-
nary ranges that each query was translated to. In most of the cases,
where the ranges are translated to a few binary ranges, the average
range query time is low enough to be considered practical. On the
other hand, the SSN attribute demonstrates the disadvantage of our
range queries extension when dealing with big values. Still, the
performance is not prohibitive, but, clearly, our range query exten-
sion yields better results for values that range from a few tens to a
few thousands.
Figure 5: Average query time under different SADS conﬁgura-
tions using the Enron dataset.
Figure 6: Average OR-query time under different SADS conﬁg-
urations using the Enron dataset. Each cluster is for a different
dataset size and each bar is for a different term count (from 2
to 5).
clined, most probably due to thread scheduling overhead. Perfor-
mance results for the parallelized search phase are presented in the
next section.
6.3 Search Performance
The introduction of the multiple hash functions feature in SADS
poses a trade-off between efﬁciency and privacy. Not only because
of the higher computation overhead it adds but also because it is in-
compatible with the slicing optimization. In this section we explore
in detail the effects of the multiple hash function scheme and also
how parallel search could help amortize some of the performance
penalty.
Figure 5 shows the comparison for four different conﬁgurations
of SADS: (i) original, (ii) original with the slicing optimization en-
abled, (iii) using multiple hash functions and (iv) using multiple
hash functions and parallel searching together. The search time re-
ported in this ﬁgure is the total time elapsed from the point when
the client issues the query to the QR until it receives the set of
matching document IDs if any — no document retrieval. As ex-
pected, the average query time grows linearly using the original
SADS conﬁguration, as the actual search is done linearly over all
the Bloom ﬁlters. Next, we can see that the slicing optimization
greatly reduces search time to a point that it seems almost constant
across different dataset sizes. Using the multiple hash functions
feature we do get better privacy guarantees, but at the cost of in-
creased search time by another factor that is proportional to the
 0 50 100 150 200 250 300 350 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000Time (ms)Dataset Size (files)OriginalMultipleSlicingParallel + Multiple 0 200 400 600 800 1000 1200 1400 1600Time (ms)Dataset Size (files)SlicingOriginalMultiple5000040000300002000010000Figure 7: Average time for retrieving documents anonymously,
compared to retrieving them non-anonymously using ssh ﬁle
transfer. Average size of ﬁles being transferred was 27.8 Mb
Figure 8: Comparison between the extended SADS and
MySQL.
6.5 Document Retrieval
We implemented document retrieval using PH-SAEP and stan-
dard RSA signatures to sign query results. Using PH-SAEP puts a
(likely over-restrictive) limit on the length of plaintext values. To
handle this, we encrypt larger ﬁles using AES private key encryp-
tion, and store the key encrypted with PH-SAEP as a header in the
encrypted ﬁle. The ﬁles can thus be read by decrypting the header
with the appropriate PH-SAEP key and using the result to decrypt
the content of the ﬁle. We preprocess the ﬁles in a way that provides
an intermediate party with AES encrypted ﬁles under different AES
keys and encryptions of these AES keys under some permutation of
the keys k1”, . . . kn”. The client will receive as results from the in-
termediary party the encrypted ﬁles, the encrypted AES keys, and
the indices of the keys k” used for their encryptions. When he re-
ceives the decryption keys k” from the server, the client will ﬁrst
decrypt the AES keys and then use them to decrypt the remainder
of the ﬁles.
Figure 7 shows the average time to retrieve documents using our
scheme versus the number of documents being retrieved. This is
shown in comparison to a non-privacy-preserving SSH-based ﬁle
transfer. As we can see, our scheme adds very little overhead com-