255
748
187
N/A 5504
CPE
128
228
54
595
4202
Table 1: Summary of the CENIC network dataset.
Mar
6 15:55:46 lax-core1.cenic.net 767:
RP/0/RP1/CPU0: Mar 6 16:56:08.660: IS-IS[237]:
ROUTING-ISIS-4-ADJCHANGE: Adjacency to
lax-core2 (TenGigE0/2/0/7) (L2) Up, Restarted
⊲
⊲
⊲
Figure 3: A syslog message generated by a Cisco CRS-8/S
router (split into multiple lines to ﬁt). The message indicates
that IS-IS routing protocol has transitioned the lax-core1
link to the lax-core2 router on interface TenGigE0/2/0/7
to the up state.
This message is to alert you that the CENIC
network engineering team has scheduled
PLANNED MAINTENANCE:
START 0001 PDT, FRI 8/17/07
END 0200 PDT, FRI 8/17/07
SCOPE:
IMPACT: Loss of power redundancy at Level 3/
Power breaker upgrade
Triangle Court, Sacramento
COMMENTS
CENIC Engineering team has scheduled
remote-hands at Level 3/ Triangle Court,
Sacramento to swap out breakers.
⊲
⊲
⊲
⊲
⊲
Figure 4: An operational announcement. Announcements are
a combination of ﬁxed-format elements (START and END) and
free-form text.
2009. Figure 2 shows an example of an interface description
for a Cisco 12410-series router.
• Syslog messages. All CENIC network routers are conﬁgured
to send syslog [21] messages over the network itself to a cen-
tral server located at the Tustin (TUS) hub site. The messages
announce link failures at the physical link layer, link proto-
col layer, and network layer (IS-IS), covering the ﬁrst three
layers of the network protocol hierarchy. Unlike many local
logs, messages in the centralized syslog are timestamped to
only whole-second granularity. We obtained an archive of
these messages from November 2004 to December 2009, of
which 217,498 pertained to the networks in this study. Un-
fortunately 176 days of syslog data ( 9/23/2007 to 3/17/2008)
are absent from the archive. Figure 3 shows a syslog message
generated by IS-IS.
• Administrator notices. We also obtained archives of two
mailing lists used to disseminate announcements about the
network. Together the mailing lists contained 7465 an-
nouncements covering 3505 distinct events from November
2004 to December 2009. Figure 4 shows a typical adminis-
trator notice.
317Configs
Syslog
Email
BGP
Link
Extractor
Failure
Extractor
Classifier
Isolation
Detector
Correlator
Event
Detector
Network
Stats
Event
History
Event
Verifier
Verified
Events
Figure 5: Our failure event reconstruction work ﬂow. The BGP
validation process is described in Section 5.
Finally, in order to help validate our conclusions about when fail-
ures occur, we extract BGP announcements relating to the CENIC
networks from the Route Views Project [31]. In particular, we col-
lect all BGP messages received by the Peering and Internet Ex-
change (PAIX) listener in Palo Alto, California that pertain to ad-
dress blocks belonging to or serviced by the CENIC network. Note
that our analyses do not depend on the BGP data—we instead use it
as ground-truth regarding the small subset of externally visible fail-
ures. Table 1 provides a brief overview of the cumulative dataset
that we consider for the remainder of the paper.
4. METHODOLOGY
The main goal of our work is to develop a general procedure to
mine the three “low quality” sources of historical data to construct
a crisp timeline of failures, where each failure event is annotated
with a start and end time, set of involved links, and, if possible, a
potential cause. Moreover, where appropriate, we seek to aggre-
gate multiple simultaneous link failures into larger events such as
router and point-of-presence (PoP) failures, and coalesce frequent
back-to-back link failures into enclosing ﬂapping episodes. In ad-
dition to this annotated failure timeline, we also produce statistical
information about link lifetimes that serves as input to our analysis
(Section 6). Figure 5 depicts the extraction process, as well as our
validation experiments discussed in the next section.
4.1 Recovering the topology
Before beginning to catalog failures, we must ﬁrst build a topo-
logical model of the network under study. While it is possible that a
map may be readily available (indeed, the current CENIC topology
is available on the Web1), we instead choose to infer the topology
from the historical data. Our reasons are two-fold: First, previous
work has shown that topological databases rapidly become out of
date as operators change the physical topology to increase capacity
or in response to failures [17]. Second, we need to cross-correlate
syslog information with physical network entities; extracting the
actual identiﬁers used in the conﬁguration ﬁles signiﬁcantly sim-
pliﬁes this task.
We begin by attacking the latter issue. In particular, we map en-
tities described in the syslog ﬁles to individual routers and links in
the network topology. This process is not entirely straightforward
however, as layer-3 syslog messages identify both endpoint routers
of a link, but only the particular interface for the router that gener-
ates the syslog message. In several cases this is insufﬁcient to fully
describe a link, for example when two routers have multiple links
1http://noc.cenic.org/maps/
between them. To accurately identify the interfaces at both ends of
a link, we consult the router conﬁgurations. Each conﬁguration ﬁle
describes the kinds of interfaces present on the router and how they
are conﬁgured; Figure 2 shows an example interface description.
Our collection of router conﬁguration ﬁles is not just a single
snapshot in time, but rather a series of conﬁguration ﬁles for each
router, where each ﬁle version is annotated with its update time.
Thus, router conﬁgurations give us a meaningful way to deﬁne link
“lifetime” as the period of time between its ﬁrst mention in a con-
ﬁguration ﬁle and its last.
We identify the ports associated with each link using a straight-
forward iterative process similar to previous work on on extracting
global network state from conﬁguration ﬁles [13, 14]. For each ac-
tive interface running IS-IS, we determine the set of IP addresses
on the same subnet. The overwhelmingly common case is that an
interface’s subnet is 255.255.255.254 (i.e., a point-to-point link)
making it obvious which interfaces are communicating with each
other. An important caveat is that IP addresses are often changed
and re-used on different routers, so it is critical to allow interfaces
to be part of multiple different links throughout the analysis.
4.2 Identifying failures
Armed with the set of links in the network, we process the fail-
ure history of the network in several steps. We begin with the sys-
log archive under the assumption that it contains an accurate—if
incomplete—enumeration of link failures.
4.2.1 Deﬁning failure
For our purposes, a failure is any event that causes a routing-state
change (layer-3) syslog message to be recorded. As a result, our re-
constructed event history reﬂects the routing state of the network,
i.e., a link is considered to have failed whenever a router refuses to
send trafﬁc over it. As such, our event history may not accurately
capture the physical state of the network components. For example,
a router may refuse to route trafﬁc over a link because a hold-down
timer has yet to expire rather than because of an actual disconnec-
tion. We deﬁne the duration of a failure event to extend from the
ﬁrst layer-3 “down” message in syslog (we may receive messages
from routers at both ends of the link) to the ﬁrst “up” referring to
the same link.
Recall that syslog also contains failure messages generated at
the physical link layer and at the link protocol layer. We choose to
focus on the network layer, as opposed to the link layer, because
it more faithfully captures the state we are interested in, namely
whether the link can be used to carry trafﬁc. The bias is, of course,
one-sided: if the physical layer reports the link is “down,” then it
is necessarily also “down” at the network layer; on the other hand,
a link may be “up” at the physical layer, but not at the network
layer (e.g., an Ethernet link plugged into a switch with incorrectly
conﬁgured VLANs).
4.2.2 Grouping
Once individual failure events have been identiﬁed, we further
consider whether failure events overlap. We deﬁne simultaneous
failures to be two or more failures on distinct links occurring or
healing within 15 seconds of each other. In keeping with the liter-
ature [23], we identify three types of simultaneous failures: router-
related, PoP-related, and other. A simultaneous failure is deemed
router-related if all involved links share a common router, PoP-
related if all links share a common PoP but not a common router,
and “other” if the failures has no common PoPs.
In addition to grouping simultaneous failures across multiple
links, we also aggregate back-to-back failure events on a single
318Classiﬁcation
Example causes or explanations
Power
Hardware
External
Software
Conﬁguration
Other
Unknown
“City-wide power failure”, “UPS failure”
“Replacing line card” , “Replacing optical ampliﬁer”
Failure of non-CENIC equipment (e.g., leased ﬁber)
“Upgrading IOS”
“Modifying IGP metrics” , “adding IPv6 capability”
“DoS attack” , “ﬂooded machine room”
Failures with unknown or unreported causes
Table 2: Operational announcement failure classiﬁcation.
link into an enclosing ﬂapping event. Link ﬂapping has long been
understood as a challenge for routing protocols [33]. Based on our
experience with the CENIC dataset in this study, we liberally deﬁne
ﬂapping as two or more up/down state changes where the down-to-
up periods last no longer than 10 minutes. (We justify our particular
parameter choice in Section 6.2.4.)
4.2.3 Dealing with loss
In contrast to Markopoulou et al. [23], who used a specialized
IS-IS listener, we glean routing state information entirely from sys-
log messages generated by the routers themselves. Unfortunately,
because of the unreliable nature syslog’s UDP-based transport, not
all router log messages make it to the syslog server. As a result, it is
common to hear about a failure from one side of a link but not the
other. For this reason, we consider a link down if at least one end-
point reports it being down, and up if at least one endpoint reports
it coming up.
It is also common to see two “up” messages with no interven-
ing “down” message, and vice versa. In one instance, for exam-
ple, the syslog shows a link between the LAX and RIV routers in
the HPR network fail (reported “down” by RIV but not LAX), and
then, 36 days later, the same link is reported down by LAX, with no
intervening messages about the link. We discard such anomalous
periods—between consecutive “up-up” or “down-down” messages
where it was impossible to infer when a link changed state—from
the dataset. We choose this conservative approach in order to fa-
vor correctness over completeness. In the case of our dataset, the
excluded time periods account for 12.6% of the link-hours on HPR
links, 9.5% of the link-hours on DC links, and 16% of the link-
hours on CPE links.
4.3 Categorizing failures
So far, we have determined when failures occur and how long
they last, but nothing more. Inferring the probable cause of these
failures requires additional inference and additional data.
Over and above the syslog entries, operational announcement
archives contain a wealth of information that, when available, can
turn simple failures into well described events (for example, see
Figure 4). After manually reviewing a number of announcements,
we observed that most events can be categorized into a small num-
ber of classes.
We classify the administrator notices into seven categories, listed
in Table 2. We manually labeled each announcement based on
matching keywords, phrases, and regular expressions. In some in-
stances, there may be multiple announcements pertaining to the
same failure event. Grouping these multiple announcements into
a single event requires some piece of information to be repeated
in each announcement. Luckily, the ﬁrst announcement about an
event contains the start time of the event in an easy to identify
and parse format. From there, each additional announcement ei-
ther contains the original announcement or restates the start time of
the event. Also the ﬁnal announcement contains the time that the
event ofﬁcially ended.
Armed with failure start and end times from syslog as well as
failure causes tagged with start time and end time from the op-
erational announcements archives, we use temporal correlation to
match failures (computed by processing the syslog) with potential
causes (based on administrator notices). To ﬁnd matches, we widen
the start and end times from the operational announcements by ﬁf-
teen minutes to compensate for potential issues with clock syn-
chronization, conservative time windows, and delayed reporting.
Unfortunately, blindly assigning causes to syslog failures that fall
within an announcement’s window leads to a large number of false
positives. To minimize such errors we extract router names or ab-
breviations from announcement messages and ensure that at least
one router in a link was mentioned in the message before match-
ing it to a corresponding syslog-based inference. For our CENIC
dataset, we discard 1,335 of the 2,511 messages (53%) that, while
contemporaneous with a failure event in syslog, do not explicitly
mention a router involved in the failure. It is likely that manual
inspection could salvage a signiﬁcant percentage of these.
5. VALIDATION
Unfortunately, there is no free lunch. Syslog data was never in-
tended to be used for our purposes; consequently certain omissions
and ambiguities are inevitable. Validating network failure data in
general is challenging, and especially so when dealing with events
ﬁve years in the past. Thus, we take an opportunistic approach,
checking for consistency against data we do have with an under-
standing that there will be noise and errors that reﬂect the different
vantage points between these diverse data sources. In particular,
our approach has two major shortcomings: it is neither complete
nor 100% accurate: there are likely to be failures that our log does
not include, and it may be the case that failures we do include are
spurious, misclassiﬁed, or improperly timestamped. We discuss the
potential biases that result from our choice of data source, as well
what we did to validate our results and help quantify our errors.
5.1 Measurement bias
As discussed earlier, some link status change messages may
be missing from the syslog due to its unreliable nature. Thus, a
“down” link state transition may not have a preceding “up” or vice
versa. In our use to date we have found that such gaps are rela-
tively minor (accounting for less than 13% of link time) but this
could also be an artifact of our particular network and hardware.
Additionally, our deﬁnition of link failure is based on adjacency
status reported by the underlying routing protocol. For example, to
ensure connectivity, the IS-IS protocol requires routers to send and
receive hello messages. By default, a router sends a hello message
once every ten seconds and declares a link disconnected if no hello
message is received for thirty seconds. Hence, we may under-count
failure duration by up to thirty seconds per failure. Conversely, IS-