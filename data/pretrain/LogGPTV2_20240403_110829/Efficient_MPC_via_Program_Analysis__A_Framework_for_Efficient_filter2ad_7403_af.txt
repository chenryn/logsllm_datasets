18
The ﬁnal schedule constructed from the MPC-source abstraction in Fig. 3(a) is shown in Fig. 3(c).
We stress that the natural schedule, which we construct in the implementation, is only a step towards a
solution. We believe that one can exploit the well-behaved MPC-source representation, and data dependences
on MPC-source, to construct provably optimal schedules. We will explore this direction in future work.
6.3 Uniformly Parallel Schedule
In this section, we describe a restriction on parallelization—namely, we consider schedules that have a
property we call uniformly parallelization. This restriction captures natural schedules, and it also enables
computing an optimal protocol assignment that takes advantage of amortized costs. Similarly to §5.3, if
we constrain IP Parallel(S) to the same ast and yst for all st that map to the same n ∈ CMPC(S), then
the optimal solution of IP CMPC(S) is the optimal solution of IP Parallel(S). Below we formalize the uniform
parallelization restriction. In Appendix E we argue that a natural schedule as described in §6.2, meets
the uniform parallelization restriction, and therefore, the protocol assignment that minimizes IP Linear(S)
minimizes IP Parallel(S) for a natural schedule.
First, we extend the concretization function γ, to work on n ∈ CMPC(S): γ(n) = { st ∈ Linear(S) | α(st) =
n }. We note that we abuse notation by allowing an ill-formed domain of γ. The restriction has two compo-
nents:
1. All γ(n) are uniformly allocated across N hyper-nodes (parallel nodes) in Parallel(S). That is, each one of
N st nodes. As a result, we can amortize execution costs of the st nodes
n with each st ∈ γ(n).
|γ(n)|
N , and associate the same (potentially amortized) costs cA
the N hyper-nodes contains
based on
These costs can be extracted from a generalized cost model.
n and cY
|γ(n)|
M
M , and associate the same (potentially amortized) conversion costs cY 2A
2. All γ((d, u)) are uniformly allocated across M hyper-nodes. Again, each one of the M hyper-nodes contains
|γ((d,u))|
std nodes. As a result, we can amortize conversion costs of the std ∈ γ((d, u)) nodes based on
|γ((d,u))|
d(e) —also extracted
from the generalized cost model—with each std ∈ γ((d, u)), where e = min cut(d, u). Notably, the cost
of converting d depends on what the min-cut edge e happens to be.
By Lemma 4 (see Appendix D.2), if (d, u) subsumes (d, u(cid:48)), then γ((d, u(cid:48))) ⊆ γ((d, u)), i.e., only a
subset of the deﬁnitions std are part of def-use chains that end at u(cid:48)’s. Thus, γ((d, u(cid:48))) is amortized over a
smaller number of parallel executions, and therefore, individual conversion cost cY 2A
d(e(cid:48)) may be higher than
d(e) . If conversions are not required at (d, u), but they are required at (d, u(cid:48)),
individual conversion cost cY 2A
those conversions contribute higher cost, namely cY 2A
Linear(S) is an extreme case of a uniformly parallel schedule: all parallel nodes are of size 1, and all costs
d(e(cid:48)), than (d, u).
d(e) and cA2Y
are the sequential costs.
Theorem 4. The protocol assignment that minimizes
(cid:80)
n (an · cA
(cid:80)
n · wn + yn · cY
n · wn)
e · we · cA2Y
d(e) )
+
e · we · cY 2A
d(e) + zd
d,e (xd
(cid:80)
st (ast · cA
st) +(cid:80)
st + yst · cY
st(xst · cY 2A
st + zst · cA2Y
st
)
also minimizes
This is argued exactly as in §5.3.
7 Implementation and Benchmarks
In this section we discuss our implementation and experimental results. The section is organized as follows:
§7.1 presents an overview of our implementation—the analysis and OPA solver, and §7.2 describes our
19
Figure 4. Implementation Overview: The analysis takes a Java program as input and outputs a def-use graph (along
with related information). The linear program takes as input analysis information and costs, and outputs the optimal
assignment.
Table 1. Running Times of Analysis and Integer Program (rounded to nearest integer, median of 10 executions).
Lines of Code
Time (secs)
Benchmark
Java MPC- MPC Analysis Integer
Program
GCD
Biometric Matching
Modular Exponentiation
Private Set Intersection (PSI)
Histogram
MiniONN (MNIST)
k-means
DB-Merge (500 + 500)
DB-Join (50 x 50)
DB-Join (25 x 200)
Cryptonets (Square)
36
55
43
40
102
196
121
77
83
103
103
Source Nodes†
10
7
19
2
24
114
36
26
33
43
39
55
112
112
75
160
696
331
192
189
225
331
18
19
18
18
18
23
19
19
19
19
19
1
1
1
1
1
4
2
1
1
1
1
† MPC-Source nodes may translate to several gates, e.g. in the running GCD example, the MUX on line 25 is translates
to 2 * LEN gates.
experiments. §7.3 details how we calculated costs for the cost model and discusses the implications of our
method. §7.4 concludes with a detailed examination of our results, and a comparison with existing works.
For brevity, we may refer, collectively, to our implementation of the analysis and OPA solver as the toolchain
or the tool.
7.1 The Toolchain
Our techniques are generically applicable to MPC-source, which can be deﬁned on any high-level language,
i.e., any language that can be transformed into IMP-SSA form is a candidate for our analysis. In our
experiments we chose Java as the high level language for our system. Following the methodology introduced
in the previous sections, we restrict our benchmarks to an IMP-style subset of Java that can be translated
to MPC-source. This yields the following restrictions which are standard in MPC compilers [BNP08; SR18;
Fra+14]: 1) function calls are statically resolvable, i.e. no polymorphism, 2) there is no recursion, 3) loops
have statically known bounds, and 4) arrays have statically known sizes.
Additionally, we restrict data types to unsigned integers (for both scalars and arrays). We note that this
restriction does not entail loss of generality. If the underlying compiler supports additional data types, the
analysis can easily be extended to handle those data types. The OPA solver itself will remain unchanged.
However, costs for operations on the additional data types will have to be collected (the OPA solver needs
20
prog.java AnalysisLinear Programanalysis (json) assignments (text) costs (json) costs-scriptcosts for all operations). In fact, one future research direction is to integrate our toolchain into a feature-rich
hybrid protocol compiler such as CGMC-GC [Fra+14].
We used Soot [Val+99] for performing our program analysis. Soot is a popular program analysis framework
for Java and Android. It provides an SSA form called Shimple. It also provides out-of-the-box support for
function inlining, loop detection and basic def-use analysis, which facilitate translation of Java to IMP-SSA,
and subsequently to MPC-source.
We used MATLAB’s Optimization Toolbox to write a linear program that takes analysis information as
input (along with costs) and outputs an optimal mixed protocol assignment for the speciﬁed two protocols.
Figure 4 presents an overview of our system. The analysis takes a Java program as input. Using Soot,
it transforms the input program into SSA form (Shimple), then inlines the function calls. The program is
now transformed into MPC-source. (A mapping from Shimple operations to MPC gates is deﬁned inside
the analysis, and there is no need to explicitly perform this transformation.) We analyze MPC-source and
generate the linear program. We then pass the linear program to MATLAB and solve it using its built-in
LP solver.
7.2 Our Experiments
OPA is parameterized by the cost model and its optimality is with respect to the underlying costs. We detail
how we obtained cost for our experiments in Section §7.3. Using our toolchain with these costs we run the
following experiments on a Core i6-6500 3.2 GHz computer with 16GB of RAM: For each benchmark, and
each pair of protocols from {πA, πB, πY} (i.e. for each of {πA, πB }, {πA, πY } and {πB, πY }), we plugged
in our corresponding costs to derive the linear program. We used the solutions of the corresponding linear
programs to obtain the optimal 2-out-of-3 protocol assignment by keeping the one with the overall minimum
value for the objective function. The results of the experiment are summarized in Table 1.
7.3 Calculating Costs
Calculating accurate costs is of high importance for the usability of any protocol assignment tool. As discussed
in §3.5, one can instantiate the cost model with diﬀerent cost values depending on the setting. Following the
trend in the hybrid MPC literature [DSZ15; MR18; B¨us+18] we focused on running time in our experiments.
Ideally, we would reuse cost tables from existing works for the most accurate comparison. Unfortunately, not
all costs are reported, and the actual code that runs the experiment is not released at the time of writing.
Therefore, we use the following methodology to calculate runtime costs of diﬀerent instructions and share
conversions.
To compute costs for an operation OP in the unamortized setting, we use a circuit with two inputs a, b
from Alice and Bob, a gate OP that operates on a, b and a reconstruction gate that reconstructs to both
parties. To facilitate comparison with ABY [DSZ15], we obtain the circuits by use of the public interface
of ABY [DSZ15] without modifying the internal code, i.e., we use ABY’s circuit creation mechanism in a
black-box manner. We run this circuit 1000 times and average the total time reported by ABY [DSZ15].
For n parallel/amortized operations, we have a circuit with n copies of each gate (as described above, in the
unamortized setting), making n OP gates execute in parallel. Observe that the unamortized setting is exactly
n = 1. We create and run experiments for n = {1, 2, 5, 10, 25, 50, 100, 200, 300, 500, 800}.
An important factor that aﬀects the run time of MPC is the communication network. Therefore, one
usually investigates two common scenarios: execution over a Local Area Network (LAN) vs. over Wide Area
Network (WAN). There are two types of experiments one can do to estimate the eﬀect of the network, namely
execute the protocol over a real LAN or WAN [DSZ15], or use a network simulator [MR18; B¨us+18]. As
our goal is mainly to demonstrate our toolbox and compare to existing results, we used the latter method.
We note that in either case, existing benchmarks demonstrate that although the network type aﬀects the
absolute running time of the protocols, in almost all cases it does not aﬀect the actual optimal assignment.
This trend is conﬁrmed in our simulation experiments.
Following the methodology used in [MR18; B¨us+18] we used Linux’s Traﬃc Control tc to simulate
the network. We used the same parameters as in [MR18]: LAN (i.e., bandwidth=10gbps, burst=250mbps,
21
Table 2. Instruction cost, in micro-seconds. Averaged over 100 executions except when n = 1 where it is averaged
over 1000 executions. (32bit)
Inst
n (Simulated LAN) n (Simulated WAN)
1 100