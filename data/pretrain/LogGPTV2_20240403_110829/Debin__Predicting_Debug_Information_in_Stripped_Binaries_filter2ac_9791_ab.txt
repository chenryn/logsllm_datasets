formation is lost during stripping. Registers that store variable val-
ues are one example of unknown nodes. The second set of program
elements are those whose property is already present in the binary
code and Debin does not need to infer this information. These
elements are marked with blue color. Examples of known nodes
are constants and instructions.
For certain kinds of elements, it is easy to determine whether
they are unknown or known with fixed rules. For instance, the con-
stants 0 and 1, and the instruction mov, are known and are marked
with blue in Figure 2(c). However, such rules cannot always deter-
mine if a registers or a memory offset stores a variable. In general,
variables in source code can be mapped to registers or memory
offsets in binary code, but not every register and memory offset cor-
responds to a variable in the source as memory and registers may be
allocated by the compiler only for temporary use. For example, the
memory offset mem[ESP] at line 10 of Figure 2(b) temporarily holds
the return address of the function, but mem[ESP+4] at line 2 stores
the variable n. Compilers leverage sophisticated techniques for al-
locating machine resources, a process that is generally irreversible
and often differs among different compilers and versions. There-
fore, manually constructing rules for finding register-allocated and
memory-allocated variables is difficult.
Debin infers properties of registers and memory offsets allo-
cated for variables since they capture crucial debug information
such as variable names. For temporarily allocated registers and
memory offsets, Debin treats these as known nodes and does not
predict their name or type. We formalize the challenge of recov-
ering variables as a binary classification problem. Debin trains a
classifier from millions of registers and memory offsets that are
labeled whether they represent variables. Then, this classifier is
used to recover source-level variables from low-level registers and
memory offsets. In our example, EDX.2, EDX.3 and EAX.3 are rec-
ognized as register-allocated variables and their nodes are marked
with unknown. Unlike these, EDX.1 is not recovered as a variable
so it is marked as known. The variable recovery classification model
is based on Extremely randomized Trees [28], a variant of Random
Forests [16]. We discuss this model in more detail in Section 3.1.
Build Dependency Graph.
In the next step, Debin builds an
undirected dependency graph where nodes are extracted code el-
ements in BAP-IR and edges represent the relationships between
these code elements. In Figure 2(d), we depict a partial dependency
graph for our example.
Two kinds of relationships are built upon program elements
extracted in prior steps. The first are pairwise relationships which
link two nodes. Note that there could be multiple relationships
between two nodes with different labels. For instance, EDX.3 and
EDX.2 are connected by the edge dep-EDX-EDX to formally express
their dependency relationship by the statement EDX.3=EDX.2+1.
Similarly, the cond-NE-EDX-ECX relationship between EDX.3 and
ECX.1 indicates their values are compared by a “not equal” con-
ditional expression. Note that these two relationships are both
encoded with the register location (EDX and ECX) of the connected
nodes in order to capture how compilers allocate and manipulate
registers.
The second kind of relationship is a factor relationship. Factor
relationships enhance the expressiveness of our model because
they can connect multiple nodes. For instance, the grey diamond
in Figure 2(d) involves the constant node 1 as well as two reg-
ister variables EDX.3 and EDX.2 (because they are all operands
of the statement EDX.3=EDX.2+1). In fact, this BAP-IR statement
corresponds to the ++i statement in the original C source code.
Factor relationship are helpful for capturing patterns such as the
one above.
In general, relationships in dependency graphs originate from
the semantic behaviors of the binary code elements extracted by
our analysis. Extracted elements and relationships are formally
defined in Section 4. The dependency graph represents a Condi-
tional Random Field (CRF) [37], a probabilistic graphical model that
captures relationships between nodes and enables joint predictions
over them. We elaborate on the details of this model in Section 3.2.
Probabilistic Prediction.
In this step, Debin assigns a property
value to every unknown node. Debin performs what is known
as Maximum a Posterior (MAP) inference over the dependency
graph – it jointly predicts a globally optimal assignment for all
unknown nodes, based on the structure of the graph. Next, we il-
lustrate how the prediction works on our example. In Figure 2(e),
we show the same graph as in Figure 2(d), but with the predicted
names for all unknown nodes and with three additional tables asso-
ciated with the three relationships. Each table is a mapping from a
possible assignment of the nodes connected by the relationship to
a weight value. The weight values are learned a-priori during the
training phase (discussed in Section 3.2) and intuitively represent
the likelihood of an assignment. The goal of the MAP inference
algorithm is to find a globally optimal assignment that maximizes
the summation of weights.
For the top-most table, the assignment of i and n results in a
score of 0.5, which is the highest score locally in this table. Similarly,
a locally optimal assignment of i and i is achieved for the bottom-
most table. However, for the table in the middle, Debin does not
select the most likely assignment because the only feasible choice is
i and i according to the other two assignments. Hence, the overall
MAP assignment will have the (highest) score of 1.6. Note that
if the assignment of p and p was selected from the middle table,
the selection from the other two tables will result in a total score
of only 1.0. The final MAP inference results for all names in our
example are shown in Figure 2(e).
Output Debug Information. Debin encodes all predicted prop-
erties along with the recorded location information in the format of
DWARF [8] debug information (a standard adopted by many com-
pilers, debuggers and security analysis tools) and outputs a binary
augmented with debug information. For simplicity, we visualize the
recovered debug information for our example in Figure 2(f). First,
Debin predicts names and types of functions and variables, as listed
in the table. Second, the boundary of function sum is recovered as
shown by yellow color, thanks to the ByteWeight [14] plugin of
BAP. Finally, the location of every variable in the assembly code
is also rebuilt, as indicated by the one-to-many colored mapping
from variables to registers and memory offsets. For example, vari-
able n, colored purple , has type int and is located in memory
offset 4(%esp) at address 80483f2 and in register %ecx at address
8048405.
Scope and Limitations. We focus on inferring names and types
of variables and functions. Additionally, Debin reconstructs pro-
gram variables based on the predicted names and maps them to
low-level locations in registers and memory offsets. We do not con-
sider debug information such as line numbers as it is only applicable
when source code is actually available. In terms of finer prediction
granularity, while we handle 17 different types, an interesting item
for future work would be predicting the contents of struct and
union types.
We note that Debin learns patterns of compiler behavior from
training samples to make predictions. When input binaries are not
entirely compiler-generated, e.g., the binary is obfuscated or gen-
erated from human-written assembly, the accuracy of Debin may
be lower. For example, Popov et al. [41] introduce a method that
modifies program control flow and inserts traps to impair disassem-
bly, which can affect an important preliminary step of Debin (and
other binary analyzers). Those issues are beyond the scope of this
paper. An interesting future direction would be to train and test
Debin with obfuscated samples and study how Debin generalizes
upon these. Finally, while our tool already supports ELF binaries
on x86, x64 and ARM, it can be extended to other binary formats
and operating systems such as the Windows PE format.
3 PROBABILISTIC MODELS
In this section, we introduce the probabilistic models used by Debin.
We first discuss our variable recovery techniques based on Ex-
tremely randomized Trees classification and then present the Condi-
tional Random Field model used for structured prediction of binary
code properties.
3.1 Variable Recovery
We consider the problem of predicting whether a low-level register
or a memory offset maps to a variable at the source-level. This vari-
able can be either a local variable, a global variable or a function
argument (we do not classify the particular kind). We formulate
this prediction problem as a binary classification task for regis-
ters and memory offsets. Every register or memory offset can be
represented by a feature vector where every feature is extracted
from the stripped binary. The binary classification model takes the
feature vector as input and outputs a boolean value that signifies
whether the input register or memory offset is a variable. We will
later discuss how the feature vector is constructed in Section 4.3.
We leverage Extremely randomized Trees (ET), an ensemble of De-
cision Trees, as our binary classification model. In the following,
we first introduce Decision Trees, followed by the ET model.
Decision Tree. A Decision Tree is a tree-based model that takes a
feature vector as input and outputs a classification label. Each non-
leaf node of the tree contains a test on one feature value from the
feature vector and splits further the testing space according to the
test result. Each leaf node represents a label. The input propagates
from the root to a leaf node of the tree, where the final classification
decision is made. Typical decision tree training algorithms build
the tree from the root to the leaves by selecting features and values
that split the data into subtrees such that a given measure (e.g.,
information gain [28]) is maximized.
Extremely Randomized Trees. The Extremely randomized Trees
(ET) is a tree ensemble learning model proposed by Geurts et al. [28].
Here, at training time, a large number of decision trees are con-
structed. At testing time, the ET model outputs the most commonly
predicted label from the decision trees constructed at training time.
Next, we outline several technical details of the ET learning
algorithm. At every step of splitting the training data into sub-
trees, a feature from a random subset of S features in the feature
vector is selected (this step is similar to the popular Random Forests
learning algorithm [16]). ET differs from Random Forests mainly
in two aspects. First, ET learning utilizes the complete training
set instead of sub-samples to build each decision tree. Second and
most importantly, for each of the S features, a split value is selected
randomly by the ET algorithm. Then, the split maximizing the
information gain is selected among the S random splits from the
prior step. These randomization and ensemble techniques make
ET a powerful tool for classification. Further, randomized selection
of the split value reduces computation time and model variance,
making classification fast and stable.
The outcome of this step is a mapping where every register
and memory offset is assigned an unknown label if the classifier
predicted that the element is mapped to a variable, and a known label
otherwise.
3.2 Binary Code Properties Prediction
Given the variable mapping computed above, our goal now is to
assign the most likely name or type to each unknown node. This is
achieved through structured prediction using a model known as
Conditional Random Fields (CRFs).
We formalize the relevant elements in a binary as a vector of
random variables V = (v1, . . . , v|V | ). The assignment of every ran-
dom variable in V ranges over a set Labels that contains all possible
names and types seen during training. Without loss of general-
ity, we further define the vectors of unknown and known elements
as U = (v1, . . . , v|U | ) and K = (v|U |+1, . . . , v|V | ), respectively.
Then, to find the most likely properties Uopt for the unknown nodes,
we perform a Maximum a Posteriori (MAP) query on the CRF prob-
abilistic model P over the random variables U (discussed later) as
follows:
Uopt = argmax
U ′∈ Ω
P (U = U
′ | K = K
′
)
where Ω ⊆ Labels |U | denotes the set of possible assignments
of properties to unknown elements, U ′ is an assignment to the
unknown elements and K′ ∈ Labels |K | is an assignment to known el-
ements, which is fixed. Note that in this conditional probability,
the unknown elements are predicted jointly, which means that they
may influence each other. After the query, the values of unknown el-
ements are changed according to the most likely assignment while
the known elements remain fixed. To perform the MAP query, a
trained CRF model is required. We discuss the details of CRFs,
MAP Inference and the training algorithms in the remainder of this
section.
Dependency Graph. A dependency graph for the input program
is an undirected factor graph G = (V , E, F ) where V is the vector of
random variables that represent program elements, E ⊆ V ×V ×Rels
is the set of edges where an edge e = (vm, vn, rel ) denotes a
particular pairwise relationship named rel between two program
elements vm and vn (we assume m < n), and F is a set of factors
where each factor connects one or multiple program elements.
Figure 2(d) shows an example of a dependency graph.
Feature Functions.
In our work, there are two kinds of feature
functions: pairwise feature functions and factor feature functions.
For pairwise feature functions, we first define helper functions
ψi : Labels × Labels × Rels → R. After assigning two values to the
two nodes linked by edge e = (vm, vn, rel ), we can apply ψi on
this edge, obtain a real number. The returned value can be viewed
as a strength measure on the presence of certain nodes-relationship
triplets. We define the ψi function templates with respect to dif-
ferent pairwise relationships defined later in Section 4.4. Then, in
the training phase, our model can instantiate the templates with
observed edges to obtain multiple different ψi functions.
Given a dependency factor graph G = (V , E, F ) and an assign-
ment to all program elements (U ′, K′), the pairwise feature function
(cid:88)
fi is defined as follows (via the ith helper function ψi):
′
fi (U
, K
′
) =
ψi ((U
(vm, vn, r el )∈E
′
′
′
′
, K
)m, (U
, K
)n, rel )
where (U ′, K′)m means the mth element of vector (U ′, K′). The
pairwise feature function fi applies function ψi on all edges of the
graph G and computes a summation of the return values. Intuitively,
it can be viewed as examining the overall effect of the function ψi
on the entire graph.
We apply the concept of a factor feature function from Factor
Graphs [27] in our CRF model. For factor feature functions, we de-
fine other helper functions φj : Labels+ → R. Intuitively, the func-
tion φj takes as input a set of assigned values to nodes connected
by a factor and returns a real value that captures how good the
assignment is. Further, given the dependency graph G = (V , E, F )
and its assignment, we define a factor feature function fj:
′
′
fj (U
, K
) =
′
′
φj ((U
, K
)I )
(cid:88)
I ∈F
where I is a set containing the indices of program elements of a
factor and (U ′, K′)I returns the ordered list of values in (U ′, K′)
indexed by I. The factor feature function iterates through all the
factors in the graph and also computes an overall influence.
Score Functions. Every feature function is associated with a
weight wl , which is computed by the learning algorithm. Based on
the weights and feature functions defined above, we can score a
predicted assignment by:
′
n(cid:88)
′
′
′
score(U
, K
) =
wl fl (U
, K
)
l
where fl is either a pairwise feature function or a factor feature
function and n is the total number of features.
Conditional Random Field. A Conditional Random Field (CRF)
is a probabilistic graphical model for representing a conditional
probability distribution. In our case, the definition of the distribution
is as follows:
P (U = U
′ | K = K
′
) =
exp(score(U
, K
))
′
′
1
Z (K′)
n(cid:88)
l
=
1
Z (K′)
exp(
′
, K
′
))
wl fl (U
where Z (K′) = (cid:80)
U ′′∈Ω exp(score(U ′′, K′)) is a constant which
ensures the probabilities of all assignments sum to 1.
MAP Inference. As mentioned earlier, to find the most likely
assignment to program elements, we maximize the function:
Uopt = argmax
U ′∈ Ω
P (U = U
′ | K = K
′
)
In the setting of CRF, this function can be rewritten as:
Uopt = argmax
U ′∈ Ω
1
Z (K′)
exp(score(U
′
, K
′
))
Because Z (K′) is a constant, we can omit it and work with the
following function instead:
Uopt = argmax
U ′∈ Ω
score(U
′
′
, K
)
Indeed, for a MAP inference query, our goal is to find the optimal
Uopt that leads to the highest score computed according to the
definition above.
In practice, computing this query with exact search is NP-hard
and thus computationally prohibitive. To solve this problem, we
leverage a scalable greedy algorithm, available in the open source
Nice2Predict framework [5]. This algorithm works by selecting a