for 
clouds, 
is  significant 
ranging 
to 
cloud 
redundancy 
for  maintaining 
business  continuity.  There  are  several  existing  disaster 
from 
recovery  mechanisms 
geographical 
storage 
redundancy[10] 
redundancy[11].  Geographical 
approach 
replicates  data  between  datacenters  some  distance 
apart[10].  Cloud  storage  redundancy  approach  requires 
each  storage  to  be  equipped  with  at  least  three  different 
geographical locations[11]. Moreover, some existing tools 
such  as  Zerto[21]  and  Yuruware[22]  can  support  disaster 
recovery  with  in  clouds.  For  example,  Zerto  enables 
replication across a heterogeneous range of storage devices 
and  protocols  for  virtual  IT  environments  based  on  a 
hypervisor-level  replication  solution[21].  And  Yuruware 
enables cross-region replication for cloud applications[22]. 
Disaster recovery involves two phases: one is the disaster 
recovery design phase where disaster recovery architecture 
and  a  disaster  recovery  plan  are  generated,  and  the  other 
one  is  the  disaster  recovery  realization  phase,  where  the 
disaster  is  mitigated  according  to  the  DR  plan.  One 
concern  about  disaster  recovery  is  the  efficiency  of 
replicating  datacenter  or  storage  from  one  platform  to 
another,  especially  for  cross-region  replication.  Another 
concern is how to retrieve the changed states and data in 
the previous data center or storage and move them into the 
backup site.  
C.  Virtual Machine Replication 
Virtual  machine  replication 
failure[5].  Replicating  virtual  machines 
is  needed  for  cloud 
applications  because  of  the  uncertainty  and  instability  of 
cloud resources. Virtual machine replication mechanism is 
also widely used in disaster recovery and fault tolerance[5]. 
Virtual machine replication in cloud requires that systems  
be  constructed  with  redundant  virtual  machines  that  are 
capable  of  maintaining  and  switching  to  backups  in  the 
face  of 
is 
encouraged  to  be  asynchronously  done  because  this  will 
have fewer interrupts on the cloud applications themselves. 
For  stateful  machines,  it  is  also  required  that  only  the 
changed states and incremental data should be replicated to 
the  machine  replicas.  One  example  of  asynchronous  VM 
replication  strategy  which  fulfills  those  requirements  is 
Remus[12], which encapsulates protected software in VM 
and asynchronously propagates changed state to a backup 
host  at  high  frequencies.  The  technology  behind  Remus 
has  been  used  in  some  of  today’s  commodity  cloud 
applications,  such  as  some  e-business  websites.  Virtual 
machine  replication  strategy  can  be  designed  in  cloud 
systems  design  phase  and  take  place  at  systems  runtime 
phase.  One  challenge  in  asynchronous  virtual  machine 
replication 
replication 
frequency  and  the  avoidance  of  influencing  the  servicing 
system. 
the  determination  of 
the 
is 
D.  Fault-Tolerance 
the 
from 
generated 
functionally 
equivalent  programs 
same 
Fault-tolerance  of  cloud  services  has  received  great 
attention  for  many  years,  although  there  is  still  a  strong 
research  focus  on  how 
to  do  fault-tolerance  more 
efficiently  for  different  types  of  cloud  services.  The  idea 
behind  fault-tolerance  is  to  mask  faults  occurring  in  a 
system  instead  of  removing  them.  Three  existing  fault-
tolerance  strategies  are:  Recovery  Block[13],  N-version 
Programming[14]  and  Parallel[15].  Recovery  block  is  a 
means  of  structuring  redundant  program  modules,  where 
standby  components  will  be  invoked  if  the  primary 
component  fails[13].  N-version  Programming  allows 
multiple 
to  be 
independently 
initial 
specifications,  and 
the  functionally  equivalent  cloud 
components are invoked in parallel and the final result is 
determined  by  majority  voting[15].  For  Parallel,  the 
multiple functional equivalent components will be invoked 
in parallel and the first returned response will be used as 
the  final  result[15].  In  particular,  it  is  recommended  that 
not all  the components  within  a  cloud  system  need  to be 
equipped  with  fault-tolerance  mechanism.  For  example, 
FTCloud[15]  proposes  a  component  ranking  based  fault-
tolerance framework for cloud applications. FTCloud first 
invocation  structures  and 
employs 
invocation 
significant 
components  in  a  cloud  application,  and  then  the  optimal 
fault-tolerance  strategy  (RB,  NVP,  or  Parallel)  will  be 
automatically selected for each significant component. By 
doing  so,  the  reliability  and  efficiency  of  fault-tolerant 
cloud  applications  have  been  improved.  Fault-tolerance 
strategy  is  designed  during  system  design  phase.  Fault-
tolerance  itself  will  be  triggered  (e.g.  backup  component 
switching) during system runtime phase. The challenge is 
how 
the  service  downtime  since 
downtime could happen during the switching between two 
sites. 
to  further  reduce 
the  component 
frequencies 
identify 
the 
to 
E.  Recovery for Cloud Internal Protocols 
cloud 
internal 
recovery 
protocols 
For  cloud  internal  protocols,  such  as  the  internal 
processes  of  HDFS[16],  recoverability  is  enabled  by 
specifying  recovery  specifications.  One  typical  paradigm 
for 
is 
FATE&DESTINI[16],  a  framework  for  cloud  recovery 
testing.  With  FATE&DESTINI,  recovery  actions  are 
specified clearly, concisely and precisely by using datalog 
rules[16]  in  the  domain  of  cloud  internal  protocols.  For 
example, a datalog rule would ask the service to create a 
missing  file  on  a  certain  node.  The  recovery  action  is 
comprised of a serious of datalog rules that composite the 
logic  of  recovery.  One  drawback  of  FATE&DESTINI  is 
that it has a strong assumption on the visibility and control 
level provided by cloud, while in fact cloud platform only 
provides  consumers  with  very  limited  visibility  and 
control[1]. It is clear that FATE&DESTINI is intended for 
698
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:21:46 UTC from IEEE Xplore.  Restrictions apply. 
the design phase of cloud internal protocols which can be 
part  of  cloud  systems  or  cloud  normal  activities.    Since 
datalog rules cannot be executed without being translated 
into runnable codes, FATE&DESTINI is not intended for 
the  runtime  phase  of  cloud  internal  protocols  unless  the 
datalog  rules  are  translated  into  real  codes.  The  main 
concern  with  FATE&DESTINI  is  that  it  assumes  a  high 
visibility and control on cloud platform, while in fact cloud 
platforms  such  as  AWS  EC2  only  provides  us  with  very 
limited visibility and indirect controls[1]. 
F.  Test Driven Scripts 
consumer-initiated 
cloud 
to  make 
there  are  several  ways 
DevOps  scripts  such  as  Chef[23]  can  be  used  for 
implementing 
sporadic 
operations, such as continuous deployment or upgrade. In 
order for the scripts to be more reliable, operation scripts 
can  be  written  in  a  test  driven  manner[7].  In  scripts  like 
Chef, 
test  driven 
infrastructure,  for  instance, by  using  scripts  mini tests[7]. 
Mini tests are usually utilized to test the functionality and 
availability of a module in the whole script infrastructure. 
For  example,  a  mini  test  for  a  module  of  shutting  down 
Tomcat  service  can  be  conducted  to  check  if  Tomcat 
service will really be shut down successfully. Additionally, 
by  using  mini  test  the  errors  in  scripts  could  be  detected 
and fixed in a timely fashion by the operator. Mini tests are 
performed  in  the  test  bed  and  are  integrated  into  the 
structure  of  the  scripts  during  the  scripts  design  phase. 
However,  the  test  bed  is  different  from  the  actual 
operation’s  run  bed,  and  mini  tests  cannot  guarantee  a 
successful  script  execution  at  runtime.  Moreover,  test 
driven scripts will introduce the overhead of implementing 
the mini tests scripts and running the mini test. 
G.  Cloud Operations Exceptions Handling 
One  way  to  handle  operations  errors  at  runtime  is  by 
using exception handling[23][24]. For example, Chef uses 
exception handlers to implement error handling logics. In 
Asgard[24],  one  exception  handling  technique  is  to 
gracefully  exit  from  the  operation  in  the  face  of  errors. 
Although  exception  handling  is  triggered  during  the 
runtime phase of scripts or operations, the detailed logic in 
exception  handling  has  to  be  implemented  during  the 
scripts  and  operation  design  phase.  One  of  the  biggest 
challenges  about  cloud  exception  handling,  according  to 
[25], is that exception handling in cloud should deal with 
cross-platform and cross-language exceptions in a unified 
manner. Another challenge of exception handling is that it 
usually only has local information and has no access to the 
global information about the whole environment. 
H.  Recovery for Cloud Operations as Transactions 
Recovery strategies for transactions, e.g. long running 
transactions[17],  usually  involve  backward  recovery  and 
forward  recovery[17].  Backward  recovery  refers  to  the 
strategy which first revert the current erroneous state to a 
previous  correct  state  before  attempting  to  continue 
execution.  Forward  recovery  attempts  to  correct  the 
current  erroneous  state  and 
then  continues  normal 
execution.  One  form  of  backward  recovery  is  called 
rewind & replay[26], which makes the system go back to 
the  previous  consistent  state  and  replay  the  step  in  the 
699
transaction. One form of forward recovery in long running 
transactions  is  called  compensation[17],  which  means  to 
attempt  to  correct  the  state  of  a  system  given  some 
knowledge  of  the  previous  actions  of  the  system[17]. 
When  conducting  sporadic  operations  such  as  rolling 
upgrade  for  a  large-scale  cloud  application,  backward 
recovery  and  forward  recovery  can  be  employed  for 
recovering  from  errors  happening  during  the  operations. 
These recovery strategies are designed and implemented in 
the design phase of operations, and take effect during the 
operations  runtime  when  failure  occurs.  The  most 
challenging part in this recovery strategy is cloud system 
state  reachability  checking[19]  as  well  as  making 
checkpointing more efficient.  
I.  Recovery for Cloud Operations as Process Models 
When  analyzing  and  modeling 
sporadic  cloud 
operations  as  processes,  several  existing  process-oriented 
recovery  methods[2][18]  can  be  utilized  for  recovery. 
Generally,  process-oriented  recovery  methods  are  similar 
to  transaction-oriented  recovery  methods.  For  example, 
operation  undo  and  redo[18]  is  similar  to  rewind  and 
replay.  One  difference  between  existing  process-oriented 
recovery  methods  and  existing 
transaction-oriented 
recovery  methods  is  that  the  recovery  points  should  be 
determined prior to implementing the recovery actions. For 
example, [8] presented a method to determine the recovery 
points  for  sporadic  operation  on  cloud  applications.  For 
recovery after a particular recovery point, several recovery 
mechanisms such as checkpoint-based undo and redo[18] 
or reparation[19] can be adopted. These recovery methods 
are non-intrusive. That is, they do not require changing the 
original  code  of  the  operation  and  can  function  as  an 
external recovery service. Hence, this recovery strategy is 
targeted for operations runtime phase and provides a real 
time  non-intrusive  recovery,  but  its  architecture  and 
functionality are designed during operations design phase. 
One challenge behind this strategy is that the checkpoints 