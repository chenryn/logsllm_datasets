{none, simple, EffiCuts}
{x, log(x)}
{1000, 5000, 15000}
{100, 500, inf}
10000000
60000
fully-connected
tanh
{256x256, 512x512}
{true, false}
0.00005
1.0
0.01
0.3
10.0
0.01
30
1000
Table 2: NeuroCuts hyperparameters. Values in curly braces
denote a space of values searched over during evaluation.
We found that the most sensitive hyperparameter is the top-
node partitioning, which greatly affects the structure of the
search problem. It is also important to ensure that the roll-
out timestep limit and model used are sufficiently large for
the problem.
construction, and the objective is minimizing classification time
and memory usage.
Structured data in deep learning. There have many recent pro-
posals towards applying deep learning to process and generate
tree and graph data structures [11, 58, 60, 61, 63, 66]. NeuroCuts
sidesteps the need to explicitly process graphs, instead exploiting
the structure of the problem to encode agent state into a compact
fixed-length representation.
Deep reinforcement learning. Deep RL leverages the modeling
capacity of deep neural networks to extend classical RL to domains
with large, high-dimensional state and action spaces. DQN [36, 37,
56] is one of the earliest successes of Deep RL, and shows how to
learn control policies from high-dimensional sensory inputs and
achieve human-level performance in Atari 2600 games. A3C, PPO,
and IMPALA [7, 35, 43] scale actor-critic algorithms to leverage
many parallel workers. AlphaGo [44], AlphaGo Zero [46] and Alp-
haZero [45] show that Deep RL algorithms can achieve superhuman
performance in many challenging games like Go, chess and shogi.
Deep RL has also been applied to many other domains like natural
language processing [28] and robotics [25–27]. NeuroCuts works
in a discrete environment and applies Deep RL to learn decision
trees for packet classification.
Deep learning for networking and systems. Recently there has
been an uptake in applying deep learning to networking and sys-
tems problems [4, 6, 16, 33, 34, 54, 62, 64, 65]. NAS [62] utilizes
client computation and deep neural networks to improve the video
quality independent to the available bandwidth. Pensieve [34] gen-
erates adaptive bitrate algorithms using Deep RL without relying
on pre-programmed models or assumptions about the environment.
Valadarsky et al. [54] applies Deep RL to learn network routing.
Chinchali et al. [4] uses Deep RL for traffic scheduling in cellu-
lar networks. AuTO [3] scales Deep RL for datacenter-scale traffic
optimization. There are also many solutions that apply deep rein-
forcement learning to congestion control [6, 16, 64] and resource
management [33]. We explore the application of Deep RL to packet
classification, and propose a new algorithm to learn decision trees
with succinct encoding and scalable training mechanisms.
8 CONCLUSION
We present NeuroCuts, a simple and effective Deep RL formulation
of the packet classification problem. NeuroCuts provides signifi-
cant improvements on classification time and memory footprint
compared to state-of-the-art algorithms. It can easily incorporate
pre-engineered heuristics to leverage their domain knowledge, op-
timize for flexible objectives, and generates decision trees which
are easy to test and deploy in any environment.
We hope NeuroCuts can inspire a new generation of learning-
based algorithms for packet classification. As a concrete example,
NeuroCuts currently optimizes for the worst-case classification
time or memory footprint. By considering a specific traffic pattern,
NeuroCuts can be extended to other objectives such as average
classification time. This would allow NeuroCuts to not only opti-
mize for a specific classifier but also for a specific traffic pattern in
a given deployment.
267
Neural Packet Classification
SIGCOMM ’19, August 19–23, 2019, Beijing, China
REFERENCES
[1] Florin Baboescu, Sumeet Singh, and George Varghese. 2003. Packet classification
for core routers: Is there an alternative to CAMs?. In IEEE INFOCOM.
[2] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schul-
man, Jie Tang, and Wojciech Zaremba. 2016. OpenAI gym. arXiv preprint
arXiv:1606.01540 (2016).
[3] Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. AuTO: Scaling Deep
Reinforcement Learning for Datacenter-Scale Automatic Traffic Optimization. In
ACM SIGCOMM.
[4] Sandeep Chinchali, Pan Hu, Tianshu Chu, Manu Sharma, Manu Bansal, Rakesh
Misra, Marco Pavone, and Sachin Katti. 2018. Cellular network traffic scheduling
with deep reinforcement learning. In AAAI.
[5] Ekin Dogus Cubuk, Barret Zoph, Dandelion Mané, Vijay Vasudevan, and Quoc V.
Le. 2018. AutoAugment: Learning Augmentation Policies from Data. CoRR
(2018).
[6] Mo Dong, Tong Meng, Doron Zarchy, Engin Arslan, Yossi Gilad, Brighten Godfrey,
and Michael Schapira. 2018. PCC Vivace: Online-Learning Congestion Control.
In USENIX NSDI.
[7] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih,
Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Legg Shane,
and Kavukcuoglu Koray. 2018.
IMPALA: Scalable distributed Deep-RL with
importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561
(2018).
[8] Kousha Etessami and Mihalis Yannakakis. 2005. Recursive Markov decision pro-
cesses and recursive stochastic games. In International Colloquium on Automata,
Languages, and Programming. Springer, 891–903.
[9] Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He, Zachary
Kaden, Vivek Narayanan, and Xiaohui Ye. 2018. Horizon: Facebook’s Open Source
Applied Reinforcement Learning Platform. arXiv preprint arXiv:1811.00260 (2018).
[10] Alex Graves, Abdel-Rahman Mohamed, and Geoffrey Hinton. 2013. Speech
recognition with deep recurrent neural networks. In ICASSP.
[11] Arthur Guez, Théophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol
Vinyals, Daan Wierstra, Rémi Munos, and David Silver. 2018. Learning to search
with MCTSnets. arXiv preprint arXiv:1802.04697 (2018).
[12] Pankaj Gupta and Nick McKeown. 1999. Packet classification on multiple fields.
SIGCOMM CCR (1999).
[13] Pankaj Gupta and Nick McKeown. 1999. Packet classification using hierarchical
intelligent cuttings. In Hot Interconnects.
[14] Pankaj Gupta and Nick McKeown. 2001. Algorithms for packet classification.
(2001).
[15] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,
and David Meger. 2017. Deep Reinforcement Learning that Matters. CoRR (2017).
[16] Nathan Jay, Noga H. Rotman, P. Godfrey, Michael Schapira, and Aviv Tamar. 2018.
Internet Congestion Control via Deep Reinforcement Learning. arXiv preprint
arXiv:1810.03259 (2018).
[17] Kirill Kogan, Sergey Nikolenko, Ori Rottenstreich, William Culhane, and Patrick
Eugster. 2014. SAX-PAC (scalable and expressive packet classification). In SIG-
COMM CCR.
[18] AN Kolmogorov and NA Dmitriev. 1947. Stochastic branching processes. In
Doklady Akademi Nauk SSSR, Vol. 56. 7–10.
[19] Vijay R. Konda and John N. Tsitsiklis. 2000. Actor-critic algorithms. In Advances
in neural information processing systems.
[20] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo.
2015. Deep Neural Decision Forests. In The IEEE International Conference on
Computer Vision (ICCV).
[21] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.
The case for learned index structures. In SIGMOD.
[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. Imagenet classifica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems.
[23] Karthik Lakshminarayanan, Anand Rangarajan, and Srinivasan Venkatachary.
2005. Algorithms for advanced packet classification with ternary CAMs. In
SIGCOMM CCR.
[24] John Langford and Tong Zhang. 2008. The epoch-greedy algorithm for multi-
armed bandits with side information. In Advances in neural information processing
systems. 817–824.
[25] Ian Lenz, Honglak Lee, and Ashutosh Saxena. 2015. Deep learning for detecting
robotic grasps. The International Journal of Robotics Research (2015).
[26] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. 2016. End-to-end
training of deep visuomotor policies. The Journal of Machine Learning Research
(2016).
[27] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen.
2018. Learning hand-eye coordination for robotic grasping with deep learning
and large-scale data collection. The International Journal of Robotics Research
(2018).
[28] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky.
2016. Deep reinforcement learning for dialogue generation. arXiv preprint
arXiv:1606.01541 (2016).
[29] Wenjun Li, Xianfeng Li, Hui Li, and Gaogang Xie. 2018. CutSplit: A Decision-
Tree Combining Cutting and Splitting for Scalable Packet Classification. In IEEE
INFOCOM.
[30] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Gold-
berg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. 2018. RLlib: Abstractions
for distributed reinforcement learning. In ICML.
[31] Alex X Liu, Chad R Meiners, and Yun Zhou. 2008. All-match based complete
redundancy removal for packet classifiers in TCAMs. In IEEE INFOCOM.
[32] Yadi Ma and Suman Banerjee. 2012. A smart pre-classifier to reduce power
consumption of TCAMs for multi-dimensional packet classification. In ACM
SIGCOMM.
[33] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. 2016.
Resource management with deep reinforcement learning. In ACM SIGCOMM
HotNets Workshop.
[34] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neural adaptive
video streaming with pensieve. In ACM SIGCOMM.
[35] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timo-
thy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchro-
nous methods for deep reinforcement learning. In ICML.
[36] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing Atari with
deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).
[37] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness,
Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg
Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen
King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015.
Human-level control through deep reinforcement learning. Nature (2015).
[38] Mohammad Norouzi, Maxwell Collins, Matthew A. Johnson, David J. Fleet, and
Pushmeet Kohli. 2015. Efficient non-greedy optimization of decision trees. In
Advances in Neural Information Processing Systems. 1729–1737.
[39] Stanley R Pliska. 1976. Optimization of multitype branching processes. Manage-
ment Science 23, 2 (1976), 117–124.
[40] Yaxuan Qi, Lianghong Xu, Baohua Yang, Yibo Xue, and Jun Li. 2009. Packet
classification algorithms: From theory to practice. In IEEE INFOCOM.
[41] Yun R. Qu, Hao H. Zhang, Shijie Zhou, and Viktor K. Prasanna. 2015. Opti-
mizing many-field packet classification on FPGA, multi-core general purpose
processor, and GPU. In ACM/IEEE Symposium on Architectures for Networking
and Communications Systems.
[42] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
2015. High-dimensional continuous control using generalized advantage estima-
tion. arXiv preprint arXiv:1506.02438 (2015).
[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[44] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalch-
brenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,
Graepel Thore, and Demis Hassabis. 2016. Mastering the game of Go with deep
neural networks and tree search. Nature (2016).
[45] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Grae-
pel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. 2018. A general
reinforcement learning algorithm that masters chess, shogi, and Go through
self-play. Science (2018).
[46] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche,
Thore Graepel, and Demis Hassabis. 2017. Mastering the game of Go without
human knowledge. Nature (2017).
[47] Sumeet Singh, Florin Baboescu, George Varghese, and Jia Wang. 2003. Packet
classification using multidimensional cutting. In ACM SIGCOMM.
[48] Ed Spitznagel, David Taylor, and Jonathan Turner. 2003. Packet classification
using extended TCAMs. In IEEE ICNP.
[49] Venkatachary Srinivasan, Subhash Suri, and George Varghese. 1999. Packet
classification using tuple space search. In SIGCOMM CCR.
[50] Weibin Sun and Robert Ricci. 2013. Fast and flexible: parallel packet processing
with GPUs and click. In ACM/IEEE Symposium on Architectures for Networking
and Communications Systems.
[51] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning
with neural networks. In Advances in neural information processing systems.
[52] David E. Taylor and Jonathan S Turner. 2005. ClassBench: A packet classification
benchmark. In IEEE INFOCOM.
[53] David E. Taylor and Jonathan S Turner. 2005. Scalable packet classification using
distributed crossproducing of field labels. In IEEE INFOCOM.
[54] Asaf Valadarsky, Michael Schapira, Dafna Shahaf, and Aviv Tamar. 2017. Learning
to route with Deep RL. In NIPS Deep Reinforcement Learning Symposium.
268
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica
[55] Balajee Vamanan, Gwendolyn Voskuilen, and T. N. Vijaykumar. 2010. EffiCuts:
Optimizing Packet Classification for Memory and Throughput. In ACM SIG-
COMM.
[61] Jianchao Yang, Shuicheng Yang, Yun Fu, Xuelong Li, and Thomas Huang. 2008.
Non-negative graph embedding. In CVPR.
[62] Hyunho Yeo, Youngmok Jung, Jaehong Kim, Jinwoo Shin, and Dongsu Han. 2018.
[56] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep Reinforcement
Neural adaptive content-aware internet video delivery. In USENIX OSDI.
Learning with Double Q-Learning. In AAAI.
[57] Matteo Varvello, Rafael Laufer, Feixiong Zhang, and TV Lakshman. 2016. Multi-
layer packet classification with graphics processing units. IEEE/ACM Transactions
on Networking (2016).
[58] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge
Graph Embedding by Translating on Hyperplanes.. In AAAI.
[63] Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. 2018. Graph
Convolutional Policy Network for Goal-Directed Molecular Graph Generation.
arXiv preprint arXiv:1806.02473 (2018).
[64] Yasir Zaki, Thomas Pötsch, Jay Chen, Lakshminarayanan Subramanian, and
Carmelita Görg. 2015. Adaptive congestion control for unpredictable cellular
networks. In SIGCOMM CCR.
[59] Zheng Xiong, Wenpeng Zhang, and Wenwu Zhu. 2017. Learning decision trees
[65] Ying Zheng, Ziyu Liu, Xinyu You, Yuedong Xu, and Junchen Jiang. 2018. Demys-
with reinforcement learning. In NIPS Workshop on Meta-Learning.
[60] Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang Yang, and
Stephen Lin. 2007. Graph embedding and extensions: A general framework for
dimensionality reduction.
IEEE transactions on pattern analysis and machine
intelligence (2007).
tifying Deep Learning in Networking. In ACM SIGCOMM APNet Workshop.
[66] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong
Sun. 2018. Graph Neural Networks: A Review of Methods and Applications.
arXiv preprint arXiv:1812.08434 (2018).
269