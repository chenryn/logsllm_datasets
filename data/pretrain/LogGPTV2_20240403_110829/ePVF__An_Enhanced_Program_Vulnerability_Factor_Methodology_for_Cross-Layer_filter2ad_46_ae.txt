i
r
i
p
m
E
1.0
0.8
0.6
0.4
0.2
0.0
F
D
C
l
a
c
i
r
i
p
m
E
1.0
0.8
0.6
0.4
0.2
0.0
epvf
pvf
0.2
0.4
0.6
0.8
1.0
ePVF or PVF values of nw
0.2
0.4
0.6
0.8
1.0
ePVF or PVF values of lud
Fig. 12: The Ô¨Ågure presents the CDF for the ePVF and PVF values
of registers used in each instruction of nw (left) and lud (right)
benchmarks. PVF values for most instructions are clustered around 1
and thus can not inform protection mechanisms based on instruction
level protection.
(cid:2)
ePVFinst =
register in inst (ACEBits - CrashBits)
T otal bits in inst
(3)
Evaluation Methodology. We evaluate the coverage of the
above schemes through fault injection experiments. We only
consider the Ô¨Åve benchmarks whose SDC rates were higher
than 10% in Figure 9 (i.e. mm, pathÔ¨Ånder, hotspot, lud and
nw) so as to discriminate the effects of the two schemes better.
Further, we run the fault injection campaigns with different
inputs than the ones we used to get the ePVF values (these
are much larger in size) to get stable performance numbers.
Evaluation Results. Figure 13 shows the SDC rate of the
original application (no protection), the SDC rate when using
hot-path protection, and the SDC rate when using ePVF-
informed protection, given a performance overhead bound of
24%4. Overall, we Ô¨Ånd that ePVF based protection does better
than hot-path based protection, and reduces the SDC rate from
20% to 7% (geometric mean), while hot-path based protection
reduces it to about 10%. Thus, ePVF based protection does
30% better than hot-path based protection, on average across
benchmarks, showing that it has better discriminative power
than execution frequencies for protection. Further, we Ô¨Ånd that
ePVF-based protection outperforms hot-path based protection
for all benchmarks except hotspot. This is due to the presence
of many control-Ô¨Çow structures in hotspot all of which are
marked as sensitive by ePVF though they do not cause SDCs.
VI. DISCUSSION
A. Scalability is an important issue as most applications
will likely generate ACE graphs with billions of vertices. The
ACE-graph sampling technique we describe in ¬ßIV-E offers
a signiÔ¨Åcant speedup for applications that contain repetitive
patterns. We believe that scaling to handle larger applications
is a matter of good engineering and not a fundamental barrier
for the following reasons. First, the current ePVF infrastructure
(including building/processing the DDG) is implemented in
Python. A tuned C/C++ implementation will likely be orders
4We considered performance overheads of 8%, 16% and 24% as well. We
report here only the results using 24% overhead due to space constraints - the
qualitative results were similar all cases.
177
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:04 UTC from IEEE Xplore.  Restrictions apply. 


	
 
	

30%
25%
20%
15%
10%
5%
0%
mm
lud
nw
pathfinder
hotspot
geomean
However, in Section IV-B, we found that our implementation
of the ePVF methodology may yield false positives i.e., it may
identify a failure as a crash when in fact it is an SDC. This
occurs because of differences between the program‚Äôs memory
structures in the golden run (on which the ePVF analysis is
based) and the fault injected runs. However, the differences
are very small in practice (at most 8% in our experiments). A
more robust implementation can address this difference.
Fig. 13: SDC rates for the original application (left bars) and when
using hotpath (center) and ePVF-based protection (right) for an
overhead bound of 24%. Error bars present 95% conÔ¨Ådence intervals.
VII. RELATED WORK
of magnitude faster and consume less memory. Second, the
most time-consuming portion of the ePVF analysis is running
the crash and propagation models. These start from each
load/store individually, and search along their backward slices.
This process is trivially parallelizable (threads can be assigned
to one backward slice each with minimum coordination re-
quired). Additionally the work allocated for each thread (i.e.,
the size of the backward slice) scales sub-linearly with the
size of the graph. Third, if the DDG does not Ô¨Åt in memory,
it can be partitioned to support the parallel backward slice
exploration suggested above.
B. Sources of Inaccuracy. ¬ßIV-C shows that ePVF is a
much closer upper bound than PVF for the SDC rate of an
application. However, ePVF still overestimates the SDC rate,
in some cases by a signiÔ¨Åcant amount. This overestimate is
generated mainly by the following three factors:
1. Lucky loads: ePVF assumes that any fault that causes
a load to deviate from its intended source address (but still
stays within the bounds of the program‚Äôs allocated memory)
will lead to an SDC. However, as prior work has found [32],
this is not always true. For example, the value loaded from
the incorrect address may still be correct, and hence have no
effect on the program. The likelihood of this case increases
if the value loaded is 0, as memory typically has large areas
initialized to zeroes [32].
2. Y-branches: Y-branches are branches that do not affect the
outcome of the application even when the program executes
the wrong part of a branch due to a fault [33]. The ePVF
analysis assumes that all branches lead to SDCs if Ô¨Çipped.
However, only about 20% of branch Ô¨Çips lead to SDCs in
practice, as prior work has found [33].
3. Application-speciÔ¨Åc correctness checks: Similar to PVF,
the ePVF model, considers as ACE bits all bits that lead to
visible changes to the application output. Some of these faults,
however, may be characterized as benign by application-
speciÔ¨Åc correctness checks (e.g., based on precision thresholds
for Ô¨Çoating-point computations).
C. Conservativeness: While ePVF may overestimate the
SDC rate, it will never underestimate it (barring the case
below). This is because ePVF conservatively labels every non-
crash causing operation as potentially leading to an SDC.
Being conservative is important as it can drive decisions about
how much state to protect in the worst-case for the application.
it
There has been a considerable amount of work on esti-
mating the error resilience of a program either through fault
injection, or through vulnerability analysis techniques. The
main advantage of fault-injection is that
is simple and
allows distinguishing between different failure outcomes, yet
has limited predictive power and is slow. The main advantage
of vulnerability analysis is that it has predictive power and
is faster, but does not distinguish between different kinds of
failures. The main question we ask in this paper is whether it
is possible to combine the advantages of the two approaches
by building an architecture-neutral vulnerability analysis tech-
nique to distinguish different failure outcomes, and especially
SDCs. Therefore, we use fault injection to gather the ground
truth of the error resilience characteristics of an application
and compare it with the result of the ePVF methodology.
Biswas et al. [34] separate the overall AVF of processor
structures into SDC AVF and Detected Unrecoverable Errors
(DUE) AVF by considering whether bit-level error protection
mechanisms such as ECC or parity are enabled in those
structures. While DUE is similar to the notion of crash in
this paper, DUE is deÔ¨Åned at the hardware level only and does
not consider software-level mechanisms. Further, like AVF, the
DUE-AVF is highly hardware dependent.
Bronovetsky et al. [35] use standard machine learning
algorithms to predict the vulnerability proÔ¨Åles of different
routines under soft errors, to understand the vulnerability of
the full applications. However, their technique is conÔ¨Åned to
linear algebra applications. Lu et al. [36] and Laguna et al. [37]
identify SDC-causing code regions through a combination of
static analysis and machine learning. However, their technique
does not provide foundational understanding behind why some
faults cause SDCs and others do not. A common issue with
machine learning techniques is that they require extensive
training with representative data, which analytical techniques
do not.
Finally, Yu et al. [38] introduce a novel resilience metric
called data vulnerability factor (DVF) to quantify the vulner-
ability of individual data structures. By combining the DVF
of different data structures, the vulnerability of an application
can be evaluated. While useful, this technique requires the
program to be written in a domain speciÔ¨Åc language, that is
restricted in terms of its expressiveness. Further, DVF does not
distinguish between crash-causing errors and other errors.
178
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:04 UTC from IEEE Xplore.  Restrictions apply. 
VIII. SUMMARY
This paper presents ePVF, a methodology to extend the
PVF analysis by distinguishing crash-causing bits from the
ACE bits as as to get a tighter bound on SDC rate. Our
methodology consists of two models: (1) a propagation model
to predict the dependent bits of memory address calculations
based on a range propagation analysis, and (2) a crash model
to predict the platform-speciÔ¨Åc behaviour of program crashes.
We implement the ePVF methodology in the LLVM compiler,
and evaluate its accuracy. The results show that ePVF can
predict crashes with high conÔ¨Ådence (89% recall and 92%
precision on average). Further, ePVF signiÔ¨Åcantly lowers the
upper bound of the estimated SDC rate of a program (61% on
average), compared to the original PVF. Finally, we present
a use-case for this methodology: an ePVF-informed selective
duplication technique, which leads to 30% lower SDCs than
hot-path instruction duplication.
While we have focused on using ePVF methodology for
SDC rate estimation and reduction in software, there are two
other uses in the future. First, it can be used to determine
which architectural structures are more likely to cause SDCs,
and selectively protect these structures through hardware tech-
niques such as selective ECC. Second, the ePVF methodology
can be used to determine the total number of crash-causing
bits in the program and inform a fault-tolerance mechanism
for crash-causing faults (e.g. checkpointing).
ACKNOWLEDGEMENT
We thank Vilas Sridharan for his insightful feedback on this
work. We also thank the reviewers of DSN 2016 for their
feedback which helped improve this work. This work was
funded in part by Discovery grants from the Natural Science
and Engineering Research Council (NSERC).
REFERENCES
[1] C. Constantinescu, ‚ÄúTrends and challenges in vlsi circuit reliability,‚Äù in
IEEE MICRO, 2003.
[2] T. Karnik and P. Hazucha, ‚ÄúCharacterization of soft errors caused
by single event upsets in cmos processes,‚Äù Dependable and Secure
Computing, IEEE Transactions on, vol. 1, no. 2, April 2004.
[3] L. Tan, S. L. Song, P. Wu, Z. Chen, R. Ge, and D. J. Kerbyson,
‚ÄúInvestigating the interplay between energy efÔ¨Åciency and resilience in
high performance computing,‚Äù in IPDPS.
IEEE, 2015, pp. 786‚Äì796.
[4] K. S. Yim, C. Pham, M. Saleheen, Z. Kalbarczyk, and R. Iyer, ‚ÄúHauberk:
Lightweight silent data corruption error detector for gpgpu,‚Äù in IPDPS,
2011.
[5] W. Gu, Z. Kalbarczyk, and R. Iyer, ‚ÄúError sensitivity of the linux kernel
executing on powerpc g4 and pentium 4 processors,‚Äù in DSN 2003.
[6] B. Atkinson, N. DeBardeleben, Q. Guan, R. Robey, and W. M. Jones,
‚ÄúFault injection experiments with the clamr hydrodynamics mini-app,‚Äù
in 2014 ISSREW.
[7] C. da Lu and D. Reed, ‚ÄúAssessing fault sensitivity in mpi applications,‚Äù
in Supercomputing, 2004. Proceedings of the ACM/IEEE SC2004.
[8] V. Sridharan and D. Kaeli, ‚ÄúEliminating microarchitectural dependency
from architectural vulnerability,‚Äù in HPCA 2009., 2009, pp. 117‚Äì128.
[9] C. Lattner and V. Adve, ‚ÄúLLVM: a compilation framework for lifelong
program analysis & transformation,‚Äù in CGO, ser. CGO ‚Äô04, 2004.
[10] J. Wei, A. Thomas, G. Li, and K. Pattabiraman, ‚ÄúQuantifying the
accuracy of high-level fault injection techniques for hardware faults,‚Äù
in DSN, June 2014.
[11] A. Meixner, M. Bauer, and D. Sorin, ‚ÄúArgus: Low-cost, comprehensive
error detection in simple cores,‚Äù in Microarchitecture, 2007. MICRO
2007. 40th Annual IEEE/ACM International Symposium on, Dec 2007.
[12] M. de Kruijf, S. Nomura, and K. Sankaralingam, ‚ÄúRelax: An architec-
tural framework for software recovery of hardware faults,‚Äù in ISCA 14,
pp. 497‚Äì508.
[13] S. Feng, S. Gupta, A. Ansari, and S. Mahlke, ‚ÄúShoestring: Probabilistic
soft error reliability on the cheap,‚Äù SIGPLAN Not., vol. 45, no. 3, Mar.
[14] G. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. August, ‚ÄúSWIFT:
Software implemented fault tolerance,‚Äù in CGO, 2005, pp. 243‚Äì254.
[15] D. S. Khudia and S. A. Mahlke, ‚ÄúHarnessing Soft Computations for
Low-Budget Fault Tolerance.‚Äù MICRO, pp. 319‚Äì330, 2014.
[16] M.-C. Hsueh, T. Tsai, and R. Iyer, ‚ÄúFault injection techniques and tools,‚Äù
Computer, vol. 30, no. 4, pp. 75‚Äì82, 1997.
[17] D. Stott, B. Floering, D. Burke, Z. Kalbarczpk, and R. Iyer, ‚ÄúNftape:
a framework for assessing dependability in distributed systems with
lightweight fault injectors,‚Äù in IPDPS 2000, 2000, pp. 91 ‚Äì100.
[18] J. Aidemark, J. Vinter, P. Folkesson, and J. Karlsson, ‚ÄúGooÔ¨Å: generic
object-oriented fault injection tool,‚Äù in DSN, 2001, pp. 83‚Äì88.
[19] J. Carreira, H. Madeira, and J. Silva, ‚ÄúXception: a technique for
the experimental evaluation of dependability in modern computers,‚Äù
Software Engineering, IEEE Transactions on, Feb 1998.
[20] S. K. S. Hari, S. V. Adve, H. Naeimi, and P. Ramachandran, ‚ÄúRelyzer:
exploiting application-level fault equivalence to analyze application
resiliency to transient faults,‚Äù in ASPLOS 2012.
[21] S. Hari, R. Venkatagiri, S. Adve, and H. Naeimi, ‚ÄúGanges: Gang error
simulation for hardware resiliency evaluation,‚Äù in ISCA 2014, 2014.
[22] S. Mukherjee, C. Weaver, J. Emer, S. Reinhardt, and T. Austin, ‚ÄúMeasur-
ing architectural vulnerability factors,‚Äù in IEEE MICRO, vol. 23, no. 6.
[23] H. Cho, S. Mirkhani, C.-Y. Cher, J. Abraham, and S. Mitra, ‚ÄúQuantitative
evaluation of soft error injection techniques for robust system design,‚Äù
in DAC, 2013 50th ACM/EDAC/IEEE, May, pp. 1‚Äì10.
[24] J. Wei and K. Pattabiraman, ‚ÄúBLOCKWATCH: Leveraging similarity in
parallel programs for error detection,‚Äù in DSN, 2012.
[25] Q. Lu, K. Pattabiraman, M. S. Gupta, and J. A. Rivers, ‚ÄúSdctune: A
model for predicting the sdc proneness of an application for conÔ¨Ågurable
protection,‚Äù in CASE 2014. New York, New York, USA: ACM Press,
2014, pp. 1‚Äì10.
[26] F. Ayatolahi, B. Sangchoolie, R. Johansson, and J. Karlsson, ‚ÄúA study
of the impact of single bit-Ô¨Çip and double bit-Ô¨Çip errors on program
execution,‚Äù in Computer Safety, Reliability, and Security, 2013, vol.
8153, pp. 265‚Äì276.
[27] H. Agrawal and J. R. Horgan, ‚ÄúDynamic program slicing,‚Äù in Proceed-
ings of Programming Language Design and Implementation (PLDI),
New York, NY, USA, 1990.
[28] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee, and
K. Skadron, ‚ÄúRodinia: A benchmark suite for heterogeneous computing,‚Äù
in IISWC, ser. IISWC ‚Äô09.
[29] I. Karlin, A. Bhatele, J. Keasler, B. L. Chamberlain, J. Cohen, Z. DeVito,
R. Haque, D. Laney, E. Luke, F. Wang, D. Richards, M. Schulz,
and C. Still, ‚ÄúExploring traditional and emerging parallel programming
models using a proxy application,‚Äù in IEEE IPDPS 2013, Boston, USA.
[30] I. Karlin, J. Keasler, and R. Neely, ‚ÄúLulesh 2.0 updates and changes,‚Äù
Tech. Rep. LLNL-TR-641973, August 2013.
[31] S. K. S. Hari, S. V. Adve, and H. Naeimi, ‚ÄúLow-cost program-level
detectors for reducing silent data corruptions,‚Äù in 2012 42nd DSN.
IEEE, 2012, pp. 1‚Äì12.
[32] J. Cook and C. Zilles, ‚ÄúA characterization of instruction-level error
derating and its implications for error detection,‚Äù in DSN, 2008.
[33] N. J. Wang, A. Mahesri, and S. J. Patel, ‚ÄúExamining ace analysis
reliability estimates using fault-injection,‚Äù in ISCA ‚Äô07, 2007.
[34] A. Biswas, P. Racunas, R. Cheveresan, J. Emer, S. S. Mukherjee, and
R. Rangan, ‚ÄúComputing architectural vulnerability factors for address-
based structures,‚Äù SIGARCH Comput. Archit. News, vol. 33, no. 2, May.
[35] G. Bronevetsky, B. de Supinski, and M. Schulz, ‚ÄúA foundation for
the accurate prediction of the soft error vulnerability of scientiÔ¨Åc
applications,‚Äù in SELSE, 2009.
[36] Q. Lu, K. Pattabiraman, M. S. Gupta, and J. A. Rivers, ‚ÄúSdctune: A
model for predicting the sdc proneness of an application for conÔ¨Ågurable
protection,‚Äù in CASE, 2014.
[37] I. Laguna, M. Schulz, D. F. Richards, J. Calhoun, and L. Olson,
‚ÄúIpas: Intelligent protection against silent output corruption in scientiÔ¨Åc
applications,‚Äù ser. CGO 2016, 2016.
[38] L. Yu, D. Li, S. Mittal, and J. S. Vetter, ‚ÄúQuantitatively modeling
application resilience with the data vulnerability factor,‚Äù in SC, 2014.
179
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:04 UTC from IEEE Xplore.  Restrictions apply.