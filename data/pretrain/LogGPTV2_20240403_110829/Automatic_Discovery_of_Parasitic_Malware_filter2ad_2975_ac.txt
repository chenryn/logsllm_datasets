108
A. Srivastava and J. Giﬃn
process or driver and adds it to the list of malicious code. Finally, it identi-
ﬁes other benign processes infected by the malware by searching again within
the host attribution sensor’s records. The correlation engine periodically purges
records generated by the network and host attribution sensors.
5 Low-Level Implementation Details
Pyren´ee is an operating prototype implemented for Windows XP SP2 victim
systems hosted in virtual machines by the Xen hypervisor version 3.2. The high-
privilege VM executing our software runs Fedora Core 9. Implementing Pyren´ee
for Windows victim systems required technical solutions to challenging, low-level
problems.
5.1 Fast Network Flow Discovery
The network attribution sensor intercepts all inbound and outbound network
ﬂows of untrusted virtual machines. To intercept packets at a fast rate, we de-
ployed the sensor’s packet ﬁlter inside the trusted VM’s kernel-space; we de-
veloped the packet ﬁlter as a Linux kernel module. To capture packets before
they exit the network, we set up our untrusted VMs to use a virtual network
interface bridged to the trusted VM. We inserted a hook into a bridge-based
packet ﬁltering framework called ebtables [4] to view packets crossing the bridge.
Whenever the sensor’s kernel component receives a TCP SYN packet from the
hook, it notiﬁes the userspace component to perform introspection.
5.2 Introspection
The network attribution sensor identiﬁes local processes that are the endpoints
of network ﬂows via virtual machine introspection. This requires the sensor’s
userspace component to inspect the runtime state of the victim system’s kernel
MODULE_ENTRY
MODULE_ENTRY
MODULE_ENTRY
Drivers:
tcpip.sys
Linked list iteration
TCBTable
Pointer
Source IP
Source Port
Destination IP
Destination Port
Process ID
Linked list iteration
Source IP
Source Port
Destination IP
Destination Port
Process ID
Source IP
Source Port
Destination IP
Destination Port
Process ID
EPROCESS
EPROCESS
EPROCESS
Processes:
ID
Name
ID
Name
ID
Name
PID
Match
Return:
Process name
from EPROCESS
Linked list iteration
Fig. 3. Network connection to host-level process correlation in Windows
Automatic Discovery of Parasitic Malware
109
31
Handle Identiﬁer
EPROCESS
0
10
10
9
Handle Table
Pointer
Pointer
Entry
Object
Header
Fig. 4. Handle resolution in Windows converts a 32-bit handle identiﬁer into a structure
for the object referenced by the handle. Resolution operates in a manner similar to
physical address resolution via page tables.
state. Unfortunately, Windows does not store network port and process name
information in a single structure. A network driver (tcpip.sys) manages net-
work connection related information. To locate the data structure corresponding
to tcpip.sys, the sensor’s userspace component iterates across the kernel’s list
of loaded drivers to ﬁnd the structure’s memory address. The driver maintains
a pointer to a structure called TCBTable, which in turn points to a linked list
of objects containing network ports and process IDs for open connections. To
convert the process ID to a process name, the component iterates across the
guest kernel’s linked list of running processes. Figure 3 illustrates the complete
process of resolving a network connection to a host-level process name.
The correlation engine uses VMI across handle tables to identify the names
of processes that receive DLL or thread injection from other, potentially mali-
cious, software. The engine knows handle identiﬁers because the host attribution
sensor observes IN parameters to the Windows system calls used as part of an
injection, and these parameters include handles. All handles used by a process
are maintained by the Windows kernel in handle tables, which are structured as
shown in Figure 4.
To resolve a handle to a process name, the correlation engine uses the handle
to ﬁnd the corresponding EPROCESS data structure in the Windows kernel mem-
ory. Since it knows the process ID of an injecting process, the engine can ﬁnd
that process’ handle table. It searches the table for the speciﬁc object identiﬁer
recorded by the host attribution sensor. As a pleasant side-eﬀect, this inspection
of the handle table will additionally reveal the collection of ﬁles and registries
currently open to the possibly malicious injecting process.
5.3 System Call Interpositioning and Parameter Extraction
Pyren´ee’s host attribution sensor requires information about system calls used
as part of DLL or thread injection. We developed a system call interpositioning
framework deployable in Xen; this framework supports inspection of both IN and
OUT system call parameters. An IN parameter’s value is passed by the caller of
a system call while an OUT parameter’s value is ﬁlled after the execution of the
system call inside the kernel.
110
A. Srivastava and J. Giﬃn
Windows XP uses the fast x86 system-call instruction SYSENTER. This instruc-
tion transfers control to a system-call dispatch routine at an address speciﬁed
in the IA32 SYSENTER EIP register. Unfortunately, the Intel VTx hardware vir-
tualization design does not allow the execution of SYSENTER to cause a VM to
exit out to the hypervisor. As a result, our host attribution sensor must forcibly
gain execution control at the beginning of a system call. It alters the contents
of IA32 SYSENTER EIP to contain a memory address that is not allocated to the
guest OS. When a guest application executes SYSENTER, execution will fault to
the hypervisor, and hence to our code, due to the invalid control-ﬂow target.
Inside the hypervisor, the sensor processes all faults due to its manipulation of
the register value. It records the system call number (stored in the eax register),
and it uses the edx register value to locate system-call parameters stored on the
kernel stack. The sensor extracts IN parameters with a series of guest memory
read operations. It uses the FS segment selector to ﬁnd the Win32 thread infor-
mation block (TIB) containing the currently-executing process’ ID and thread
ID. It then modiﬁes the instruction pointer value to point at the original address
of the system-call dispatch routine and re-executes the faulted instruction.
We use a two-step procedure to extract values of OUT parameters at system-
call return. In the ﬁrst step, we record the value present in an OUT parameter at
the beginning of the system call. Since OUT parameters are passed by reference,
the stored value is a pointer. In order to know when a system call’s execution
has completed inside the kernel, we modify the return address of an executing
thread inside the kernel with a new address that is not assigned to the guest
OS. This modiﬁcation occurs when intercepting the entry of the system call. In
the second step, a thread returning to usermode at the completion of a system
call will fault due to our manipulation. As before, the hypervisor receives the
fault. Pyren´ee reads the values of OUT parameters, restores the original return
address, and re-executes the faulting instruction. By the end of the second step,
the host attribution sensor has values for both the IN and OUT system-call
parameters.
5.4 Address Space Construction and Switching
We create isolated address space for untrusted drivers using the Xen hypervisor
and the Windows XP 32-bit guest operating system, though our design is general
and applicable to other operating systems and hypervisors. We allocate memory
for UPT page tables transparent to the guest OS inside the hypervisor. We
then map untrusted driver code pages into the UPT and trusted kernel and
driver code into the TPT. We mark all untrusted driver code pages in TPT as
non-executable and non-writable and mark all trusted code pages in UPT as
non-executable, non-writable, and non-readable.
Pyren´ee switches between the two address spaces depending upon the execu-
tion context. It manipulates the CR3 register: a hardware register that points to
the current page tables used by memory management hardware and inaccessi-
ble to any guest OS. When an untrusted driver invokes a kernel API, execution
faults into the hypervisor due the non-executable kernel code in the UPT. Inside
Automatic Discovery of Parasitic Malware
111
the hypervisor, Pyren´ee veriﬁes the legitimacy of the control ﬂow by checking
whether the entry point into the TPT is valid. If the entry point is valid, it
switches the address space by storing the value of TPT CR3, the trusted page
table base, into CR3. If the entry point is not valid, Pyren´ee records this behav-
ior as an attack and raises an alarm. Similarly, control ﬂow transfers from TPT
to UPT fault because untrusted driver code pages are marked non-executable
inside the TPT. On this fault, Pyren´ee switches the address space by storing the
untrusted page table base, UPT CR3, in the CR3 register.
Pyren´ee identiﬁes the legitimate entry points into the TPT by ﬁnding the
kernel and trusted drivers’ exported functions. These exported functions’ names
and addresses are generated from the PDB ﬁles available from Microsoft’s symbol
server. Pyren´ee keeps this information in the hypervisor for the host-attribution
sensor.
5.5 Interception of Driver Loading
Pyren´ee requires knowledge of drivers’ load addresses to map their code pages
into either the UPT or TPT. Since Windows dynamically allocates memory
for all drivers, these addresses change. Moreover, Windows uses multiple mech-
anisms to load drivers. Pyren´ee intercepts all driver loading mechanisms. It
rewrites the kernel’s binary code on driver loading paths automatically at run-
time. It modiﬁes the direct call instruction to the ObInsertObject kernel func-
tion by changing its target to point to a location in the guest which is not
assigned to the guest VM; it stores the original target. With this design, during
the driver loading process execution faults into the hypervisor. On the fault,
Pyren´ee extracts the driver’s load address securely from the driver object and
resumes the execution at the original target location. This design provides com-
plete interpositioning of driver loading.
6 Evaluation
We tested our prototype implementation of Pyren´ee to evaluate its ability to
appropriately identify malicious software on infected systems, its performance,
and its avoidance of false positives. To generate alerts notifying the correlation
engine of suspicious network activity in our test environment, we ran a network
simulator that acted as a network-based IDS.
6.1 User-Level Malware Identiﬁcation
We tested Pyren´ee’s ability to detect process-to-process parasitic behaviors with
the recent Conficker worm [38]. Conﬁcker employs DLL injection to infect be-
nign processes running on the victim system. We executed Conﬁcker inside a test
VM monitored by Pyren´ee and connected to a network overseen by our NIDS
simulator. When executed, the worm ran as a process called rundll32.exe. The
112
A. Srivastava and J. Giﬃn
host attribution sensor recorded DLL injection behavior from rundll32.exe
targeting speciﬁc svchost processes.
When our NIDS simulator sent the IP addresses and port numbers for out-
bound malicious traﬃc to Pyren´ee’s correlation engine, the engine then deter-
mined what malicious code on the host was responsible. It searched the network
attribution sensor’s data to extract the name of the process bound to the con-
nection’s source port, here svchost.exe. It then searched the host attribution
sensor’s data and found that svchost.exe was the victim of a parasitic DLL
injection from rundll32.exe. The correlation engine also found the names of
other executables infected by the malware, and it generated a complete listing
that could be sent to a security administrator.
We repeated these tests with the Adclicker.BA trojan and successfully de-
tected its parasitic behavior.
6.2 Kernel-Level Malware Identiﬁcation
We evaluated Pyren´ee’s ability to detect kernel-level parasitism by testing it with
the recent Storm worm [23]. Storm is kernel-level malware that exhibits parasitic
behaviors by injecting malicious DLLs into the benign services.exe process,
causing services.exe to launch DDoS attacks. We loaded Storm’s malicious
driver in the test VM. Since the driver is untrusted, Pyren´ee loaded it into the
separate isolated address space. On the execution of the driver’s code, all kernel
APIs invoked by the driver were veriﬁed and logged by Pyren´ee’s host attribution
sensor. The sensor found that the driver was performing injection via APCs, and
it recorded both the parasitic behavior and the victim process.
When our network simulator ﬂagged the traﬃc made by services.exe, the
correlation engine gathered the data collected by the host and network attri-
bution sensors. The network attribution sensor determined services.exe to be
the end-point of the connection, and the host attribution sensor identiﬁed the
parasitism of the malicious driver.
6.3 Performance
We designed Pyren´ee to operate at runtime, so its performance cost on an end
user’s system must remain low. We tested our prototype on an Intel Core 2
Quad 2.66 GHz system. We assigned 1 GB of memory to the untrusted Win-
dows XP SP2 VM and 3 GB combined to the Xen hypervisor and the high-
privilege Fedora Core 9 VM. We carried out CPU and memory experiments
using a Windows benchmark tool called PassMark Performance Test [24]. We
measured networking overheads using IBM Page Detailer [13] and wget. Our
experiments measured Pyren´ee’s overhead during benign operations, during ac-
tive parasitic attacks, and during the isolation of a heavily-used driver in the
UPT. We executed all measurements ﬁve times and present here the median
values.
First, we measured Pyren´ee’s overhead on CPU-bound and memory inten-
sive operations. Tables 3 and 4 list a collection of benchmark measurements for
Automatic Discovery of Parasitic Malware
113
Table 3. Results of CPU performance tests for unmonitored execution and for
Pyren´ee’s monitoring with and without parasitic behaviors present; higher absolute
measurements are better. Percentages indicate performance loss.
Parasitic Behavior
Operations
Integer Math (MOps/sec)
Floating Point Math (MOps/sec)
Compression (KB/sec)
Encryption (MB/sec)
String Sorting (Thousand strings/sec)
Unmonitored Present % Absent %
124.8 1.34
92.5 26.88
6.17
439.5