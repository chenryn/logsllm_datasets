Cluster trafﬁc is bursty, but most of the time utilizes a fraction
of network capacity (Fig. 13). We measure throughput over 100 µs
time intervals on one production server. 25% of these intervals have
no ingress trafﬁc, 25% have no egress trafﬁc, and only 10% of these
intervals have aggregate trafﬁc exceeding 2 Gbits/s.
The production workload changes gently over time scales of tens
of minutes, enabling comparative testing when schemes are applied
in sequence. The live experiment ran for 135 minutes: the rack
started in baseline mode, switched to Fastpass at 30 minutes, and
back to baseline at 110 minutes.
)
s
/
s
t
i
b
G
(
t
u
p
h
g
u
o
r
h
t
r
e
t
i
b
r
A
TX
RX
0.6
0.4
0.2
0.0
0
50
Network throughput (Gbits/s)
100
150
200
Figure 11: The arbiter requires 0.5 Gbits/s TX and 0.3 Gbits/s RX
bandwidth to schedule 150 Gbits/s: around 0.3% of network trafﬁc.
Figure 13: Distribution of the sending and receiving rates of one
production server per 100 microsecond interval over a 60 second
trace.
Experiment H: production results. Fig. 14 shows that the 99th
percentile web request service time using Fastpass is very similar to
the baseline’s. The three clusters pertain to groups of machines that
were assigned different load by the load-balancer. Fig. 15 shows
the cluster’s load as the experiment progressed, showing gentle
oscillations in load. Fastpass was able to handle the load without
triggering the aggressive load-reduction.
Fastpass reduced TCP retransmissions by 2–2.5⇥ (Fig. 16). We
believe the remaining packet loss is due to trafﬁc exiting the rack,
where Fastpass is not used to keep switch queues low. Extending the
Fastpass scheduling boundary should further reduce this packet loss.
8. DISCUSSION
8.1 Large deployments
A single arbiter should be able to handle hundreds to thousands of
endpoints. At larger scales, however, a single arbiter’s computational
and network throughput become bottlenecks, and several arbiters
would need to cooperate.
A hierarchical approach might be useful: an arbiter within each
cluster would send its demands for intra-cluster trafﬁc to a core-
arbiter, which would decide which timeslots each cluster may use,
and what paths packets at these timeslots must follow. The cluster
arbiters would then pass on these allocations to endpoints.
A core arbiter would have to handle a large volume of trafﬁc, so
allocating at MTU-size granularity would not be computationally
feasible. Instead, it could allocate timeslots in bulk, and trust cluster
arbiters to assign individual timeslots fairly among endpoints.
An alternative for large deployments could be the use of spe-
cialized hardware. An FPGA or ASIC implementation of timeslot
allocation and path selection algorithms would likely allow a single
arbiter to support much larger clusters.
(cid:1)(cid:2)(cid:1)(cid:3)(cid:1)(cid:4)(cid:1)(cid:1)(cid:3)(cid:5)(cid:5)(cid:1)(cid:6)(cid:5)(cid:2)(cid:1)(cid:1)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:9)(cid:15)(cid:16)(cid:15)(cid:17)(cid:18)(cid:9)(cid:15)(cid:11)(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:12)(cid:11)(cid:24)(cid:8)(cid:25)(cid:25)(cid:15)(cid:19)(cid:26)(cid:9)(cid:15)(cid:27)(cid:8)(cid:28)(cid:8)(cid:12)(cid:9)(cid:15)(cid:27)(cid:8)(cid:25)(cid:16)(cid:11)(cid:9)(cid:20)(cid:1)(cid:25)(cid:22)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:4)(cid:3)(cid:2)(cid:29)(cid:30)(cid:31)(cid:1)(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:1)(cid:2)(cid:4)(cid:1)(cid:1)(cid:2)(cid:5)(cid:4)(cid:6)(cid:2)(cid:1)(cid:1)(cid:6)(cid:1)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:11)(cid:6)(cid:1)(cid:1)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:11)(cid:6)(cid:13)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:11)(cid:6)(cid:1)(cid:13)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:11)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:15)(cid:20)(cid:18)(cid:10)(cid:20)(cid:21)(cid:16)(cid:6)(cid:1)(cid:1)(cid:1)(cid:11)(cid:9)(cid:22)(cid:10)(cid:21)(cid:16)(cid:23)(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:14)(cid:30)i
d
n
o
c
e
s
r
e
p
e
d
o
n
r
e
p
i
s
n
o
s
s
m
s
n
a
r
t
e
r
t
e
k
c
a
p
n
a
d
e
M
i
6
4
2
0
baseline
fastpass
0
2000
4000
Time (seconds)
6000
Figure 14: 99th percentile web request ser-
vice time vs. server load in production trafﬁc.
Fastpass shows a similar latency proﬁle as
baseline.
Figure 15: Live trafﬁc server load as a func-
tion of time. Fastpass is shown in the middle
with baseline before and after. The offered
load oscillates gently with time.
Figure 16: Median server TCP retransmis-
sion rate during the live experiment. Fast-
pass (middle) maintains a 2.5⇥ lower rate of
retransmissions than baseline (left and right).
8.2 Routing packets along selected paths
Packets must be made to follow the paths allocated to them by the
arbiter. Routers typically support IP source routing only in software,
if at all, rendering it too slow for practical use. Static routing [38] and
policy based routing using VLANs are feasible, but could interfere
with existing BGP conﬁgurations, making them less suitable for
existing clusters. Tunneling (e.g. IP-in-IP or GRE) entails a small
throughput overhead, but is supported in hardware by many switch
chipsets making it a viable option [18].
Finally, routing along a speciﬁc path can be achieved by what
we term ECMP spooﬁng. ECMP spooﬁng modiﬁes ﬁelds in the
packet header (e.g., source port) to speciﬁc values that will cause
each switch to route the packet to the desired egress, given the other
ﬁelds in the packet header and the known ECMP hash function. The
receiver can then convert the modiﬁed ﬁelds to their original values,
stored elsewhere in the packet.
8.3 Scheduling support in NICs
Fastpass enqueues packets into NIC queues at precise times, using
high resolution timers. This frequent timer processing increases
CPU overhead, and introduces operating-system jitter (e.g., due to
interrupts). These timers would not be necessary if NICs implement
support for precise packet scheduling: packets could be enqueued
when Fastpass receives an allocation. A “send-at” ﬁeld in the packet
descriptor would indicate the desired send time.
8.4 Small packet workloads
A current limitation of Fastpass is that it allocates at the granularity
of timeslots, so if an endpoint has less than a full timeslot worth
of data to send, network bandwidth is left unused. This waste is
reduced or eliminated when an endpoint sends many small packets to
the same destination, which are batched together (§6.1). Workloads
that send tiny amounts of data to a large number of destinations still
waste bandwidth. A possible mitigation is for the arbiter to divide
some timeslots into smaller fragments and allocate these fragments
to the smaller packets.
9. RELATED WORK
Several systems use centralized controllers to get better load bal-
ance and network sharing, but they work at “control-plane” granular-
ity, which doesn’t provide control over packet latencies or allocations
over small time scales.
Hedera [5] discovers elephant ﬂows by gathering switch statistics,
then reroutes elephants for better load balancing. It aims for high
throughput, and has no mechanism to reduce network latency.
Datacenter TDMA [35] and Mordia [17] use gathered statistics
to estimate future demand, then compute a set of matchings that are
applied in sequence to timeslots of duration on the order of 100 µs.
Both schemes target elephant ﬂows; short ﬂows are delegated to
other means.
Orchestra [11] coordinates transfers in the shufﬂe stage of MapRe-
duce/Dryad so all transfers ﬁnish roughly together, by assigning
weights to each transfer. Orchestra is an application-level mecha-
nism; the actual transfers use TCP.
SWAN [19] frequently reconﬁgures the network’s data plane to
match demand from different services. All non-interactive services
coordinate with the central SWAN controller, which plans paths
and bandwidth and implements those paths by updating forwarding
tables in network devices.
Distributed approaches usually set out to solve a restricted data-
center problem: minimizing Flow Completion Time (FCT), meeting
deadlines, balancing load, reducing queueing, or sharing the net-
work. To our knowledge, no previous scheme provides a general
platform to support all these features, and some schemes perform
sub-optimally because they lack complete knowledge of network
conditions.
DCTCP [6] and HULL [7] reduce switch queueing, but increase
convergence time to a fair-share of the capacity, and do not eliminate
queueing delay.
In MATE [15] and DARD [38], ingress nodes reroute trafﬁc self-
ishly between paths until converging to a conﬁguration that provides
good load balance across paths.
In multi-tenant datacenters, Seawall [32] provides weighted fair
sharing of network links, and EyeQ [22] enforces trafﬁc constraints.
Schemes such as pFabric [8] and PDQ [20] modify the switches to
reduce ﬂow completion time, while D3 [36] and PDQ minimize ﬂow
completion times to meet deadlines. These switch modiﬁcations
raise the barrier to adoption because they need to be done across the
network. PDQ and pFabric use a distributed approximation of the
shortest remaining ﬂow ﬁrst policy, which Fastpass can implement
in the arbiter.
Differentiated Services (DiffServ, RFC2474) provides a dis-
tributed mechanism for different classes of trafﬁc to travel via distinct
queues in routers. The number of DiffServ Code Points available
is limited, and in practice operational concerns restrict the number
of classes even further. Most commonly, there are classes for bulk
trafﬁc and latency-sensitive trafﬁc, but not a whole lot more.
(cid:1)(cid:2)(cid:3)(cid:4)(cid:3)(cid:3)(cid:4)(cid:2)(cid:3)(cid:3)(cid:2)(cid:3)(cid:3)(cid:3)(cid:3)(cid:5)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:5)(cid:2)(cid:3)(cid:3)(cid:3)(cid:3)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:8)(cid:10)(cid:11)(cid:8)(cid:12)(cid:13)(cid:14)(cid:11)(cid:15)(cid:13)(cid:10)(cid:16)(cid:17)(cid:13)(cid:7)(cid:8)(cid:18)(cid:7)(cid:19)(cid:15)(cid:7)(cid:8)(cid:19)(cid:7)(cid:20)(cid:12)(cid:21)(cid:22)(cid:23)(cid:24)(cid:24)(cid:10)(cid:11)(cid:15)(cid:7)(cid:8)(cid:20)(cid:7)(cid:21)(cid:10)(cid:18)(cid:25)(cid:7)(cid:26)(cid:7)(cid:27)(cid:8)(cid:7)(cid:17)(cid:13)(cid:7)(cid:19)(cid:10)(cid:19)(cid:7)(cid:8)(cid:9)(cid:18)(cid:20)(cid:7)(cid:10)(cid:18)(cid:28)(cid:7)(cid:16)(cid:28)(cid:19)(cid:23)(cid:27)(cid:29)(cid:19)(cid:7)(cid:25)(cid:18)(cid:21)(cid:7)(cid:30)(cid:29)(cid:19)(cid:10)(cid:15)(cid:29)(cid:19)(cid:19)(cid:1)(cid:2)(cid:1)(cid:3)(cid:1)(cid:1)(cid:3)(cid:2)(cid:1)(cid:4)(cid:1)(cid:1)(cid:1)(cid:4)(cid:1)(cid:1)(cid:1)(cid:5)(cid:1)(cid:1)(cid:1)(cid:6)(cid:1)(cid:1)(cid:1)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:10)(cid:13)(cid:14)(cid:15)(cid:16)(cid:12)(cid:17)(cid:18)(cid:19)(cid:13)(cid:20)(cid:21)(cid:22)(cid:23)(cid:14)(cid:24)(cid:25)(cid:22)(cid:26)(cid:24)(cid:21)(cid:11)(cid:3)(cid:1)(cid:1)(cid:1)(cid:12)(cid:14)(cid:27)(cid:28)(cid:24)(cid:10)(cid:23)(cid:8)(cid:10)(cid:12)(cid:26)(cid:10)(cid:23)(cid:12)(cid:10)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)10. CONCLUSION
We showed how to design and implement a datacenter network
in which a centralized arbiter determines the times at which each
packet should be transmitted and the path it should take. Our results
show that compared to the baseline network, the throughput penalty
is small but queueing delays reduce dramatically, ﬂows share re-
sources more fairly and converge quickly, and the software arbiter
implementation scales to multiple cores and handles an aggregate
data rate of 2.21 Terabits/s.
Even with such a seemingly heavy-handed approach that incurs a
little extra latency to and from the arbiter, tail packet latencies and
the number of retransmitted packets reduce compared to the status
quo, thanks to the global control exerted by the arbiter. Our results
show that Fastpass opens up new possibilities for high-performance,
tightly-integrated, predictable network applications in datacenters,
even with commodity routers and switches.
We hope we have persuaded the reader that centralized control at
the granularity of individual packets, achieved by viewing the entire
network as a large switch, is both practical and beneﬁcial.
Acknowledgements
We are grateful to Omar Baldonado and Sanjeev Kumar of Facebook
for their enthusiastic support of this collaboration, Petr Lapukhov