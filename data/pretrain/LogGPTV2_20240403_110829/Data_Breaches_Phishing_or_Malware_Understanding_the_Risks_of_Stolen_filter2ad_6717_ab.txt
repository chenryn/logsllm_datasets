26,208
107,343,690
1,921
–
1,799,553,568
leaks
1,666
1,304
557
258
Table 3: Top 20 largest credential leaks in our dataset and
the fraction of inverted (or existing plaintext) passwords.
Classification and verification. After identifying and apply-
ing the optimal parser for a document, we scan over the produced
columns to determine if they correspond to a leak. All leaks must
have a column containing email addresses and a column containing
a password (or password hash). We use a regular expression to
identify which column contains an email address for every row.
While not required, we also detect IP addresses using regular ex-
pressions; and User-Agent strings and mailing addresses based on
a keyword dictionary (e.g., Mozilla, France). Verifying whether a
candidate document contains a password is more challenging. To
handle the possibility of MD5 and SHA-1 hashed passwords, we
detect columns of fixed-length strings consisting entirely of hexa-
decimal characters. Plaintext passwords lack this typical structure.
Here, we use a logistic regression that models character n-grams to
identify likely password columns. We emphasize this learning ap-
proach takes into account every string in a column simultaneously
rather than operating on a row by row basis.
To train our password classifier, we manually parsed and la-
beled the columns of plaintext credential leaks in our private forum
dataset. We used all password columns as positive samples, and
all other columns (e.g., usernames, and any other data) as neg-
ative examples. We then featurized every column into a binary
vector of character n-grams. To determine the size of the n-grams
and a threshold on the quantity of n-grams to include, we ran a
grid search using 10-fold cross validation on the training data. Our
search considered n-grams of length of 1 to 10 and feature vec-
tors that included the top 1,000 to 100,000 most frequent n-grams,
excluding common n-grams shared by both classes. Our final clas-
sifier consists of n-grams of length 2 to 5, and a feature vector of
the top 10,000 such n-grams per class. To test our classifier, we
manually labeled 230 candidate documents: 157 contained stolen
credentials and 73 did not. Our classifier correctly classified 93.9% of
test documents. The classifier favors precision over recall, whereby
it failed to identify an existing password column in 8.3% of leaks
and misidentified a password column in 3.1% of documents.
Training and testing aside, we apply our classifier to every can-
didate document and drop any document that fails to contain an
email and potential hashed password, or password detected by the
classifier. We present a breakdown of confirmed credential leaks per
collection source in Table 2. In total, our automated collection iden-
tifies 3,527 documents from public sources which combined contain
123,055,697 emails and passwords. In comparison, we managed to
acquire 1.7 billion passwords from just 258 leaks on private forums,
indicating there still is a gap between public and private sets. Most
Rank Source
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
–
Unknown p
MySpace p
Badoo
Adobe⋄
LinkedIn
VK p
Tumblr∗
Dropbox†
Zoosk
IMesh‡
LastFM
Fling p
Neopets p
Mate1 p
Unknown p
000webhost p
Taobao p
NexusMods p
Unknown p
Unknown p
Total
Number of
credentials
558,862,722
322,014,681
125,322,081
123,947,902
112,322,695
76,865,954
73,355,694
68,669,208
57,085,529
51,283,424
41,631,844
40,724,332
35,822,980
27,383,966
26,351,372
15,249,241
15,051,549
6,759,631
5,728,163
4,901,088
1,922,609,265
Plaintext
after inversion
100.0%
100.0%
33.0%
0.0%
85.6%
99.6%
0.0%
0.0%
68.2%
0.0%
85.4%
100.0%
100.0%
100.0%
99.8%
100.0%
100.0%
100.0%
99.7%
100.0%
76.0%
p Password leak acquired in plaintext format; no dictionary attack
required.
⋄ Adobe passwords were encrypted and could not be reversed.
∗ Tumblr passwords were salted SHA-1. Salt was not present in
the leak acquired.
† Dropbox passwords were a mixture of bcrypt and salted SHA-1.
Salt was not present in the leak acquired.
‡ IMesh passwords were salted MD5. Salt was not present in the
leak acquired.
public leaks are small: 48% contain fewer than 1,000 credentials,
and 86% fewer than 10,000. We list the the largest 20 confirmed
leaks in our dataset in Table 3.
Inverting passwords. Based on the character length and distri-
bution of passwords in each confirmed leak, we estimate 14.8% of
all passwords in our dataset are hashed using MD5 and 9.8% are
SHA-1. We attempt to invert these passwords using a dictionary
Table 4: Top 10 passwords across all plaintext leaks.
Top
123456
password
123456789
abc123
password1
homelesspa
111111
qwerty
12345678
1234567
Number of
Percent of
Credentials Credentials
Rank Passwords
0.35%
1
0.15%
2
0.12%
3
0.10%
4
0.05%
5
6∗
0.05%
0.05%
7
0.05%
8
0.05%
9
10
0.04%
∗ This was the most common password in the MySpace
credential leak, but appears to be automatically generated
as all email addresses begin with “msmhomelessartist".
6,387,184
2,759,747
2,249,344
985,709
888,836
855,477
855,257
829,835
828,848
740,464
of 3,416,701,663 keywords. We source this list by combining non-
hashed passwords identified in the previous stages of our pipeline
with supplemental dictionaries curated by Weakpass3 and Crack-
station.4 In total, we successfully invert 35.8% of hashed passwords.
We note this low hit rate may result for two reasons. First, black-
market actors may have previously inverted credential leaks and
uploaded a new leak file with both discovered plaintext passwords
and any remaining uninverted hashes. In this case, we are only
iterating on the previous inversion step. Secondly, this approach
fails when applied to salted passwords (as was the case for Dropbox,
Tumblr, and IMesh). We summarize our final dataset, post-inversion
in Table 3, with the most popular passwords listed in Table 4.
3.2 Phishing Kits & Victims
Through an undisclosed source, we obtain a sample of 10,037 phish-
ing kits (including the PHP and HTML source code) and 3,779,664
usernames and passwords belonging to victims of those kits along
with the time they were phished. Both the kits and victims were
identified over the course of March, 2016–March, 2017. Leveraging
this data, we develop a pipeline to understand the life cycle of phish-
ing kits and the volume of potential victims they deceive as shown
in Figure 2. Our pipeline hinges on the observation that phishing
kits frequently use email as a mechanism for reporting stolen cre-
dentials (discussed previously in Section 2). Using our phishing kit
corpus (➊), we first statically analyze the source code of each kit to
extract its email template used to report stolen credentials (➋). We
then develop rules to match the subject and body of these messages
(➌) and finally adapt Gmail’s anti-abuse classification pipeline to
identify all inbound messages containing stolen credentials (➍).
Template extraction. All of the phishing kits in our corpus use
PHP’s mail () command to report stolen credentials to an exfiltra-
tion point. We provide an example in Figure 3. This particular kit
prompts a victim for their username and password before populat-
ing a message template containing the stolen information, which
3https://weakpass.com/
4https://crackstation.net/
Table 5: Breakdown of the top five email providers used by
miscreants as exfiltration points to receive stolen creden-
tials.
Phishing Kits
Keyloggers
Mail
provider
Gmail
Yahoo
Yandex
Hotmail
Outlook
Other
Popularity
72.3%
6.8%
5.1%
4.2%
2.2%
9.4%
Mail
provider
Gmail
Yandex
Mail.ru
Hotmail
Zoho
Other
Popularity
39.0%
12.3%
8.5%
3.6%
1.3%
35.3%
is then mailed to a hardcoded email. Using static analysis, we au-
tomatically identify calls to mail () and then determine the string
values supplied to each variable of the call. This search handles
all include operations as well as nested variable instantiations. In
the end, we output a template that includes the target email (e.g.,
exfiltration point), the message’s subject, and the message’s body
stripped of variables the kit determines at run-time. We provide a
breakdown of email providers for 7,780 unique exfiltration points
hard coded into kits in Table 5, of which 72.3% relate to gmail.com.
This heavy use indicates our pipeline should detect a significant
amount of all messages related to stolen credentials.
Rule generation. The next phase of our pipeline tokenizes the
message templates and outputs a set of rules to match the subject
and body of inbound emails that contain stolen credentials along
with a maximum expected message size. We opt for rules rather
than identifying the exfiltration points in our kit corpus because a
single kit may be re-configured to use multiple exfiltration points
(potentially by multiple actors). For the sample kit in Figure 3, a
message would match the template if its subject contained Result
from Gmail, while its body contained Username, Vict!m Info, and all
other strings from the template’s $message that are not run-time
variables. In total, we generate 7,325 rules which cover our 10,037
kits. As an initial trial to evaluate false positives, we applied these
to a corpus of 100,000 messages unrelated to phishing kit templates,
of which there were 0 false positives.
Email flagging. We modify Gmail’s anti-abuse detection sys-
tems to apply our rules to all inbound messages over the course
of March, 2016–March, 2017 (the same period over which the kits
were collected) to identify the exfiltration points receiving stolen
credentials, the volume of messages each account receives, and the
volume of messages per kit template. We require any exfiltration
point to receive at least 20 stolen credentials before we include it in
our study. In total, we flag 12,449,036 messages (excluding 0.8% fil-
tered due to lacking at least 20 credentials) sent to 19,311 exfiltration
points. We caution this is a strict underestimate of messages gener-
ated by phishing kits as our coverage of kits is non-exhaustive and
kits may use non-Gmail addresses or even non-SMTP mechanisms
to report stolen credentials (as discussed in Section 2).
Figure 2: Framework for identifying inbound messages that contain credentials stolen by phishing kits and keyloggers.
$subject = "Result from Gmail";
$message .= "------------Gmail Info-----------n";
$message .= "Username : ".$gmailuser."\n";
$message .= "Password : ".$gmailpassword."\n";
$message .= "------------Vict!m Info----------n";
$message .= "Client IP : ".$ip."\n";
$message .= "Browser :".$browserAgent."\n";
$message .= "country : ".$country."\n";
$message .= "-----Created BY Dropbox Wire-----n";
mail("PI:EMAIL", $subject, $message);
Figure 3: Sample phishing kit that collects a victim’s user-