Practical
LeNet-5
C3F2
PRADA
Practical
LeNet-5
C3F2
PRADA
Practical
LeNet-5
C3F2
PRADA
Practical
LeNet-5 (1,000)
LeNet-5 (1,000)
66.06%
48.80%
42.62%
22%
19%
68.32%
69.64%
54.45%
29%
27%
69.15%
70.13%
57.90%
31%
28%
69.80%
76.37%
60.70%
39%
33%
71.98%
78.51%
65.74%
49%
39%
82.27%
80.96%
80.40%
75%
65%
92.13%
91.12%
90.08%
89%
81.20%
93.27%
92.18%
91.13%
90%
85%
94.34%
94.57%
91.76%
91%
87%
96.49%
97.34%
94.50%
94%
90%
based on class probability information we obtained. Then
we use the PGD [37] (projected gradient descent) method
to generate targeted AEs on the substitute model. Finally,
we apply targeted AEs which could successfully attack the
substitute model to the target model, and evaluate its attack
success rate. We choose the optimal model trained on the
full dataset as the target model. PGD is an enhanced version
of FGSM [19]. It is essentially projected gradient descent
on negative loss function [37]. PGD can easily control the
size of perturbations and is fast to compute. We import the
PGD method from the foolbox [4] library. We set the upper
perturbation (ε) limit to 128/255 after several attempts. As ε
increases, the attack effect gets better, but as ε continues to
increase, the effect does not change signiﬁcantly. During an
attack process, we randomly select 5,000 seed samples in the
test dataset as a benchmark. For each sample, we generate 9
targeted AEs that are misclassiﬁed into all other categories by
the substitute model. There are totally 45,000 targeted AEs,
which take about 1.5 hours to generate (averagely 0.12s for
one AE). Compared with untargeted AEs, targeted AEs not
only make misclassiﬁcations, but also lead into the speciﬁed
categories, which are more difﬁcult for generation.
Through attacking the target model with AEs, we calculate
transferability and draw confusion matrices. In Table 8, we
evaluate LeNet-5 and C3F2 model structures and compare
Figure 4: Confusion matrices of targeted adversarial examples
attacking the target LeNet-5 model.
Figure 5: Confusion matrices of targeted adversarial examples
attacking the target C3F2 model.
with state-of-the-art PRADA [29] and Practical [45]. The ex-
perimental environment is on the MNIST dataset. AEs are
from 5,000 randomly selected normal samples in the test
dataset. The upper perturbation size is set as 128/255. In
DRMI, the AEs transferability reaches 66% under only 50
queries, while 22% in PRADA. Our approach is nearly three
times as much as them. The accuracy of our substitute model
is also 7% higher than them. As the number of queries in-
creases, the transferability also increases, and both our trans-
ferability and accuracy are higher than PRADA. In 150, 200,
and 300 queries, DRMI under LeNet-5 model all increase 3%
accuracy than PRADA, and our attack success rates achieve
68%, 69%, and 70%, respectively, and increase 39%, 38%,
and 30% than PRADA. Under 600 queries, our targeted AEs
attack success rate is as high as 72%, 23 percentages higher
than PRADA. In PRADA, the transferability reaches 64.64%
under 3,200 queries. Even though the attacker only has a very
small dataset (only 1,000 samples), DRMI still raises 20%,
25%, 21% in transferability, outperforming PRADA under
50, 150, 300 queries, and also has a better model accuracy.
DRMI of LeNet-5(1,000) also raises 15.40%, 8.88%, 6.13%,
4.76%, 4.50% model accuracy and 23.62%, 27.45%, 29.90%,
27.70%, 26.74% attack success rate under 50, 150, 200, 300,
600 queries than Practical [45], respectively.
Among these model structures, the C3F2 model has a
USENIX Association
30th USENIX Security Symposium    1911
higher attack success rate than LeNet-5 in most cases except
50 queries. With 50 queries, C3F2 reaches 48.80% transfer-
ability, two times as much as PRADA, but lower than LeNet-5.
While with 150 and 200 queries, the success rate of C3F2 is
slightly higher than that of LeNet-5, nearly 70%. Until 300
and 600 queries, C3F2 model reaches 76.37% and 78.51%
transferability, both nearly 7 percentages higher than LeNet-
5, and almost 30 percentages higher than PRADA with 600
queries. Results are affected by the model’s complexity since
C3F2 has one more convolutional layer than LeNet-5.
Remark: 1) Transferability increases as the query num-
ber increases. 2) Larger ε helps transferability of AEs to a
certain extent. 3) More complex model structure has better
transferability.
Figure 4 shows the confusion matrices of targeted AEs
attacking target model with structure LeNet-5 under 50 and
600 queries. The value in i-th row, j-th column represents
the number of samples whose original label is i and which is
classiﬁed into j. The diagonal elements are failed attack num-
bers. Other elements are succeeded attack samples. Lighter
color means larger value. As we can see in 50 queries, the
(2,2) element is the lightest, which means many adversarial
samples generated by images of label 2 did not succeed in the
attack. Although the total attack success rate is 66% under 50
queries, the success rate is 36.4% for label 2. In 600 queries,
we improve this situation. The (2,2) element turns darker and
the success rate reaches 57.0% for label 2. In Figure 5, we can
see the confusion matrices of targeted AEs attacking C3F2
under 150 and 600 queries. It achieves 69.6% accuracy at 150
queries, but AEs from different labels also have very different
transferability. Label 2 still performs worst, its attack success
rate is 44.8%. Label 3, 5, 7 also perform not well. Label 1
attacks best, whose AEs achieve 95.4% attack success rate.
Confusion matrix under 600 queries contains a higher success
rate of 78.5%. The (2,2) element is not so bright as in 150
queries. The attack success rate of label 2 achieves 53.9%.
Label 1 also attacks best with 97.9% success rate. We can
ﬁnd that adversarial samples of label 2 are the most difﬁcult
to attack successfully. For other labels, we can intuitively feel
that our attack success rate is high.
Remark: Different labels have different attack success rates
of AEs. This is because different category has different bound-
aries, causing different density of AEs. This phenomenon is
ubiquitous and does not affect the results, where the success
rate is averaged on all labels.
We also generated untargeted adversarial examples to at-
tack the target model, and compare with the state-of-the-art
Gradient Estimation (GE) [7] in Table 9. GE queries the target
model for 196 times and utilizes the acquired information to
generate 1,000 untargeted AEs in 11s. These AEs achieve a
61.5% attack success rate on the target model. In DRMI, we
use 150 queries to generate 1,000 untargeted AEs in 2 min-
utes. We achieve 71.3% success rate on LeNet-5 and 73.2%
on C3F2. Our DRMI still improves the attack success rate by
Table 9: Attack success rates of untargeted AEs between
DRMI and Gradient Estimation [7] on the MNIST dataset.
We set ε (max perturbation) as 0.3, and test the success rate
of 1,000 untargeted AEs for each experiment.
Method
Attack Success
Queries
Time per AE(s)
Gradient Estimation [7]
DRMI on LeNet-5
DRMI on C3F2
61.5%
71.3%
73.2%
196
150
150
0.011
0.126
0.113
√
Table 10: Attack success rates of untargeted AEs on the Ima-
0.001· D, and D is
geNet dataset. We set perturbation ε as
the input dimension (≈ 270,000) [48].
Method
NES [25]
AutoZoom [59]
P-RGF [11]
DRMI
Attack Success
Queries
95.5%
85.4%
96.5%
96.6%
1718
2443
1119
1100
11.7% than GE with even 46 fewer queries. The extra time
is affordable. We can generate an AE in only about 0.12s.
This comparison shows DRMI also performs effectively in a
untargeted attack. Moreover, we perform untargeted attacks
on the ImageNet dataset using Inception-v3 as shown in Ta-
ble 10. One thousand images are randomly selected from the
test set for evaluation. This experiment adopts the PGD [37]
attack under L2-norm. Results show that DRMI outperforms
NES [25] and AutoZoom [59], and has similar performance
with P-RGF [11].
Remark: Through these experiments, our substitute mod-
els have achieved a higher transferability with fewer queries,
outperforming the state-of-the-art approaches. It proves that
our substitute models generated by DRMI can accurately im-
itate the decision boundaries of the target model, and thereby
facilitate black-box attacks (e.g., adversarial attacks) against
deep learning.
Interpretability of Data Reduction
5.4
Training data can be reduced without losing too much ac-
curacy, which implies the existence of redundancy in data.
Therefore, data reduction can be regarded as redundancy elim-
inating. To answer RQ3, we implement another three metrics
to measure data redundancy: correlation matrix, class proba-
bility of prediction, and trace of activated neurons. With these
metrics, we evaluate their effectiveness in the same manner,
and provide a number of insights on interpretability.
5.4.1 CMAL: Correlation Matrix
Correlation matrix reﬂects the overall correlation among
data samples, and is a measure of data polymerization as a
whole [65]. For a data point x = [x1,x2, ...,xn]T, its correlation
1912    30th USENIX Security Symposium
USENIX Association
Table 11: Comparison between CMAL and DRMI on the
MNIST dataset.
Method
Dataset Size
Test Accuracy
Test Loss
Epoch
CMAL
DRMI
CMAL
DRMI
CMAL
DRMI
600 (1%)
600 (1%)
300 (0.5%)
300 (0.5%)
150 (0.25%)
150 (0.25%)
90.16%
96.41%
89.81%
94.14%
83.32%
92.13%
0.6071
0.1961
0.5392
0.2475
0.7078
0.2604
30
30
20
20
15
15
Table 12: Effectiveness with class probability on MNIST.
LCP means that data has low class probability, and the model
classiﬁes it correctly with low conﬁdence.
Method
Dataset Size
Test Accuracy
Test Loss
Epoch
HCP
LCP
DRMI
K-Means
PCA + K-Means
K-Means
PCA + K-Means
DRMI
K-Means
PCA + K-Means
DRMI
600 (1%)
600 (1%)
600 (1%)
600 (1%)
600 (1%)
300 (0.5%)
300 (0.5%)
300 (0.5%)
150 (0.25%)
150 (0.25%)
150 (0.25%)
90.96%