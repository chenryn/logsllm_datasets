### Figures and Descriptions
**Figure 7:** Reduction in user-perceived query latency with the use of multiple hash functions.

**Figure 8:** Latency savings due to landmark ordering used in CAN construction.

**Figure 9:** Effect of Uniform Partitioning feature on a CAN with 65,536 nodes, 3 dimensions, and 1 reality.

### Addressing the "Hot Spot" Problem on the Web
In Section 3.8, we discuss caching and replication techniques that can be used to mitigate the "hot spot" problem in CANs.

### Uniform Partitioning
If the total volume of the entire coordinate space is \( V_T \) and \( n \) is the total number of nodes in the system, then a perfect partitioning of the space among the \( n \) nodes would assign a zone of volume \( \frac{V_T}{n} \) to each node. We denote this volume as \( V \). We conducted simulations with \( n \) nodes, both with and without the uniform partitioning feature. At the end of each run, we computed the volume of the zone assigned to each node. Figure 9 plots different possible volumes in terms of \( V \) on the X-axis and shows the percentage of the total number of nodes (Y-axis) that were assigned zones of a particular volume. From the plot, we can see that without the uniform partitioning feature, a little over 40% of the nodes are assigned to zones with volume \( V \), compared to almost 90% with this feature. The largest zone volume drops from \( 10V \) to \( 2V \). As expected, the partitioning of the space further improves with increasing dimensions.

### Caching and Replication Techniques for "Hot Spot" Management
As with files on the Web, certain (key, value) pairs in a CAN are likely to be accessed much more frequently than others, thus overloading nodes that hold these popular data keys. To make very popular data keys widely available, we borrow some of the caching and replication techniques commonly applied to the Web.

- **Caching:** In addition to its primary data store (i.e., those data keys that hash into its coordinate zone), a CAN node maintains a cache of the data keys it recently accessed. Before forwarding a request for a data key towards its destination, a node first checks whether the requested data key is in its own cache. If so, it can satisfy the request without forwarding it any further. Thus, the number of caches from which a data key can be served grows in direct proportion to its popularity, and the act of requesting a data key makes it more widely available.
  
- **Replication:** A node that finds it is being overloaded by requests for a particular data key can replicate the data key at each of its neighboring nodes. Replication is an active pushing out of popular data keys, as opposed to caching, which is a natural consequence of requesting a data key. A popular data key is thus eventually replicated within a region surrounding the original storage node. A node holding a replica of a requested data key can, with a certain probability, choose to either satisfy the request or forward it on its way, thereby spreading the load over the entire region rather than just along the periphery.

As with all such schemes, cached and replicated data keys should have an associated time-to-live (TTL) field and be eventually expired from the cache.

### Design Review
Sections 2 and 3 described and evaluated individual CAN design components. The evaluation of our CAN recovery algorithms (using both large-scale and smaller-scale ns simulations) is presented in [18]. Here, we briefly recap our design parameters and metrics, summarize the effect of each parameter on the different metrics, and quantify the performance gains achieved by the cumulative effect of all the features.

#### Metrics
- **Path length:** The number of (application-level) hops required to route between two points in the coordinate space.
- **Neighbor-state:** The number of CAN nodes for which an individual node must retain state.
- **Latency:** Both the end-to-end latency of the total routing path between two points in the coordinate space and the per-hop latency (latency of individual application-level hops obtained by dividing the end-to-end latency by the path length).
- **Volume:** The volume of the zone to which a node is assigned, indicative of the request and storage load a node must handle.
- **Routing fault tolerance:** The availability of multiple paths between two points in the CAN.
- **Hash table availability:** Adequate replication of a (key, value) entry to withstand the loss of one or more replicas.

#### Key Design Parameters
- **Dimensionality of the virtual coordinate space:** \( d \)
- **Number of realities:** \( r \)
- **Number of peer nodes per zone:** \( p \)
- **Number of hash functions (i.e., number of points per reality at which a (key, value) pair is stored):** \( k \)
- **Use of the RTT-weighted routing metric**
- **Use of the uniform partitioning feature described in Section 3.7**

In some cases, the effect of a design parameter on certain metrics can be directly inferred from the algorithm; in all other cases, we resorted to simulation. Table 3 summarizes the relationship between the different parameters and metrics. A table entry marked “-” indicates that the given parameter has no significant effect on that metric, while “+” and “-” indicate an increase and decrease, respectively, in that measure caused by an increase in the corresponding parameter. The figure numbers included in certain table entries refer to the corresponding simulation results.

### Cumulative Effect of All Features
To measure the cumulative effect of all the above features, we selected a system size of \( n = 262,144 \) nodes and compared two algorithms:
1. A “bare bones” CAN that does not utilize most of our additional design features.
2. A “knobs-on-full” CAN making full use of our added features (without the landmark ordering feature from Section 3.7).

The topology used for this test is a Transit-Stub topology with a delay of 100ms on intra-transit links, 10ms on stub-transit links, and 1ms on intra-stub links. Tables 4 and 5 list the values of the parameters and metrics for each test.

We find these results encouraging as they demonstrate that for a system with over 260,000 nodes, we can route with a latency that is well within a factor of two of the underlying network latency. The number of neighbors that a node must maintain to achieve this is approximately 30 (27.1 + 2.95), which is high but not necessarily unreasonable. The biggest gain comes from increasing the number of dimensions, which lowers the path length from 198 to approximately 5 hops. However, we can see that the latency reduction heuristics play an important role; without latency heuristics, the end-to-end latency would be close to 198 * 115ms.

We repeated the above “knobs-on-full” simulation and varied the system size \( n \) from 16,384 to 262,144. In scaling the CAN system, we scaled the topology by adding more CAN nodes to the edges of the topology without scaling the backbone topology itself. This effectively grows the density at the edges of the topology. We found that as \( n \) grows, the total path latency grows even more slowly than \( n^{1/d} \) (with \( d = 10 \) in this case) because although the path length grows slowly as \( n^{1/10} \) (from 4.56 hops with 16,384 nodes to 5.0 with 262,144 nodes), the latency of the additional hops is lower than the average latency since the added hops are along low-latency links at the edges of the network.

Extrapolating this scaling trend and making the pessimistic assumption that the total latency grows with the increase in path length (i.e., as \( n^{1/10} \)), we could potentially scale the size of the system by another factor of 10, reaching a system size of close to a billion nodes, before seeing the path latency increase to within a factor of four of the underlying network latency.

### Effect of Link Delay Distributions
To better understand the effect of link delay distributions on the above results, we repeated the “knobs-on-full” test for different delay distributions on the Transit-Stub topologies. We used the following topologies:
- **H(100, 10, 1):** A Transit-Stub topology with a hierarchical link delay assignment of 100ms on intra-transit links, 10ms on transit-stub links, and 1ms on intra-stub links. This is the topology used in the above “knobs-on-full” test.
- **H(20, 5, 2):** A Transit-Stub topology with a hierarchical link delay assignment of 20ms on intra-transit links, 5ms on transit-stub links, and 2ms on intra-stub links.

Table 3: Effect of design parameters on different metrics

| Parameter | Path Length | Neighbor-State | Latency | Volume | Routing Fault Tolerance | Hash Table Availability |
|-----------|-------------|----------------|---------|--------|------------------------|-------------------------|
| \( d \)    | -           | -              | -       | -      | +                      | +                       |
| \( r \)    | -           | -              | -       | -      | +                      | +                       |
| \( p \)    | -           | +              | -       | -      | +                      | +                       |
| \( k \)    | -           | -              | -       | -      | +                      | +                       |
| RTT-weighted routing | - | - | - | - | + | + |
| Uniform partitioning | - | - | - | - | + | + |

Table 4: CAN parameters

| Parameter | "Bare Bones" CAN | "Knobs On Full" CAN |
|-----------|------------------|---------------------|
| \( d \)    | 2                | 10                  |
| \( r \)    | 1                | 1                   |
| \( p \)    | 0                | 4                   |
| \( k \)    | 1                | 1                   |
| RTT-weighted routing | OFF | ON |
| Uniform partitioning | OFF | ON |
| Landmark ordering | OFF | OFF |

Table 5: CAN Performance Results

| Metric | "Bare Bones" CAN | "Knobs On Full" CAN |
|---------|------------------|---------------------|
| Path length | 198.0 | 5.0 |
| # Neighbors | 4.57 | 27.1 + 2.95 |
| IP latency | 115.9ms | 82.4ms |
| CAN path latency | 23,008ms | 135.29ms |

Figure 10: Effect of link delay distribution on CAN latency

- **H(100, 10, 1):** A Transit-Stub topology with a hierarchical link delay assignment of 100ms on intra-transit links, 10ms on transit-stub links, and 1ms on intra-stub links.
- **H(20, 5, 2):** A Transit-Stub topology with a hierarchical link delay assignment of 20ms on intra-transit links, 5ms on transit-stub links, and 2ms on intra-stub links.