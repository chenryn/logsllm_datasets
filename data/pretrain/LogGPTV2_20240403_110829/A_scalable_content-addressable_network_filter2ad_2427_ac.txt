4V
8V
Figure 7: Reduction in user-perceived
query latency with the use of multiple
hash functions
Figure 8: Latency savings due to land-
mark ordering used in CAN construction
Figure 9: Effect of Uniform Partitioning
feature on a CAN with 65,536 nodes, 3
dimensions and 1 reality
“hot spot” problem on the Web. In Section 3.8 we discuss caching
and replication techniques that can be used to ease this hot spot
problem in CANs.
If the total volume of the entire coordinate space were VT and n
the total number of nodes in the system then a perfect partitioning
of the space among the n nodes would assign a zone of volume
VT /n to each node. We use V to denote VT /n. We ran simulations
with  nodes both with and without this uniform partitioning fea-
ture. At the end of each run, we compute the volume of the zone
assigned to each node. Figure 9 plots different possible volumes
in terms of V on the X axis and shows the percentage of the total
number of nodes (Y axis) that were assigned zones of a particular
volume. From the plot, we can see that without the uniform parti-
tioning feature a little over 40% of the nodes are assigned to zones
with volume V as compared to almost 90% with this feature and
the largest zone volume drops from V to V . Not surprisingly,
the partitioning of the space further improves with increasing di-
mensions.
3.8 Caching and Replication techniques for
“hot spot” management
As with ﬁles in the Web, certain (key,value) pairs in a CAN are
likely to be far more frequently accessed than others, thus overload-
ing nodes that hold these popular data keys. To make very popular
data keys widely available, we borrow some of the caching and
replication techniques commonly applied to the Web.
(cid:15) Caching: In addition to its primary data store (i.e. those data
keys that hash into its coordinate zone), a CAN node main-
tains a cache of the data keys it recently accessed. Before
forwarding a request for a data key towards its destination,
a node ﬁrst checks whether the requested data key is in its
own cache and if so, can itself satisfy the request without
forwarding it any further. Thus, the number of caches from
which a data key can be served grows in direct proportion to
its popularity and the very act of requesting a data key makes
it more widely available.
(cid:15) Replication: A node that ﬁnds it is being overloaded by re-
quests for a particular data key can replicate the data key
at each of its neighboring nodes. Replication is thus an ac-
tive pushing out of popular data keys as opposed to caching,
which is a natural consequence of requesting a data key. A
popular data key is thus eventually replicated within a re-
gion surrounding the original storage node. A node holding
a replica of a requested data key can, with a certain prob-
ability, choose to either satisfy the request or forward it on
its way thereby causing the load to be spread over the entire
region rather than just along the periphery.
As with all such schemes, cached and replicated data keys should
have an associated time-to-live ﬁeld and be eventually expired from
the cache.
4. DESIGN REVIEW
Sections 2 and 3 described and evaluated individual CAN design
components. The evaluation of our CAN recovery algorithms (us-
ing both large scale and smaller scale ns simulations), are presented
in [18]. Here we brieﬂy recap our design parameters and metrics,
summarize the effect of each parameter on the different metrics and
quantify the performance gains achieved by the cumulative effect
of all the features.
We used the following metrics to evaluate system performance:
(cid:15) Path length: the number of (application-level) hops required
to route between two points in the coordinate space.
(cid:15) Neighbor-state: the number of CAN nodes for which an in-
dividual node must retain state.
(cid:15) Latency: we consider both the end-to-end latency of the to-
tal routing path between two points in the coordinate space
and the per-hop latency, i.e., latency of individual application
level hops obtained by dividing the end-to-end latency by the
path length.
(cid:15) Volume: the volume of the zone to which a node is assigned,
that is indicative of the request and storage load a node must
handle.
(cid:15) Routing fault tolerance: the availability of multiple paths
between two points in the CAN.
(cid:15) Hash table availability: adequate replication of a (key,value)
entry to withstand the loss of one or more replicas.
The key design parameters affecting system performance are:
(cid:15) dimensionality of the virtual coordinate space: d
(cid:15) number of realities: r
(cid:15) number of peer nodes per zone: p
Parameter
“bare bones”
“knobs on full”
d
r
p
k
RTT weighted
routing metric
Uniform
partitioning
Landmark
ordering
CAN
2
1
0
1
OFF
OFF
OFF
CAN
10
1
4
1
ON
ON
OFF
Table 4: CAN parameters
(cid:15) number of hash functions (i.e. number of points per reality
at which a (key,value) pair is stored): k
(cid:15) use of the RTT-weighted routing metric
(cid:15) use of the uniform partitioning feature described in Section 3.7
In some cases, the effect of a design parameter on certain met-
rics can be directly inferred from the algorithm; in all other cases
we resorted to simulation. Table 3 summarizes the relationship be-
tween the different parameters and metrics. A table entry marked
“-” indicates that the given parameter has no signiﬁcant effect on
that metric, while " and # indicate an increase and decrease respec-
tively in that measure caused by an increase in the corresponding
parameter. The ﬁgure numbers included in certain table entries re-
fer to the corresponding simulation results.
To measure the cumulative effect of all the above features, we se-
lected a system size of n= nodes and compared two algorithms:
1. a “bare bones” CAN that does not utilize most of our addi-
tional design features
2. a “knobs-on-full” CAN making full use of our added features
(without the landmark ordering feature from Section 3.7)
The topology used for this test is a Transit-Stub topology with
a delay of 100ms on intra-transit links, 10ms on stub-transit links
and 1ms on intra-stub links (i.e. 100ms on links that connect two
transit nodes, 10ms on links that connect a transit node to a stub
node and so forth). Tables 4 and 5 list the values of the parameters
and metrics for each test. 6
We ﬁnd these results encouraging as they demonstrate that for a
system with over 260,000 nodes we can route with a latency that is
well within a factor of two of the underlying network latency. The
number of neighbors that a node must maintain to achieve this is
approximately 30 (27.1 + 2.95) which is deﬁnitely on the high side
but not necessarily unreasonable. The biggest gain comes from
increasing the number of dimensions, which lowers the path length
from 198 to approximately 5 hops. However, we can see that the
latency reduction heuristics play an important role; without latency
heuristics, the end-to-end latency would be close to  (cid:2) ms (#
hops (cid:2) # latency-per-hop).
We repeated the above “knobs-on-full” simulation and varied the
system size n from  to .
In scaling the CAN system, we
scaled the topology by scaling the number of CAN nodes added
The reason the IP latency is 82ms (in the “knobs-on-full” test)
instead of 115ms is not because the average latency of the physical
network is lower but because our CAN algorithm (because of the
use of zone overloading and RTT-weighted routing) automatically
retrieves an entry from the closest replica. 82ms represents the
average IP network level latency from the retrieving node to this
closest replica.
Metric
path length
# neighbors
# peers
IP latency
CAN path latency
“bare bones” CAN “knobs on full CAN”
198.0
4.57
0
115.9ms
23,008ms
5.0
27.1
2.95
82.4ms
135.29ms
Table 5: CAN Performance Results
h
c
t
e
r
t
S
y
c
n
e
t
a
L
3.2
3
2.8
2.6
2.4
2.2
2
1.8
1.6
1.4
16K
H(100,10,1)
H(20,5,2)
R(10,50)
10xH(20,5,2)
32K
65K
131K
Number of nodes
Figure 10: Effect of link delay distribution on CAN latency
to the edges of the topology without scaling the backbone topol-
ogy itself. This effectively grows the density at the edges of the
topology. We found, that as n grows, the total path latency grows
even more slowly than n=d (with d =   in this case) because al-
though the path length grows slowly as n=  (from 4.56 hops with
 nodes to 5.0 with  hops) the latency of the additional hops
is lower than the average latency since the added hops are along
low-latency links at the edges of the network.
Extrapolating this scaling trend and making the pessimistic as-
sumption that the total latency grows with the increase in path
length (i.e., as n= ) we could potentially scale the size of the
system by another  , reaching a system size of close to a billion
nodes, before seeing the path latency increase to within a factor of
four of the underlying network latency.
To better understand the effect of link delay distributions on the
above results, we repeated the “knobs-on-full” test for different de-
lay distributions on the Transit-Stub topologies. We used the fol-
lowing topologies:
(cid:15) H(  ;  ; ): A Transit-Stub topology with a hierarchical
link delay assignment of 100ms on intra-transit links, 10ms
on transit-stub links and 1ms on intra-stub links. This is the
topology used in the above “knobs-on-full” test.
(cid:15) H( ; ; ): A Transit-Stub topology with a hierarchical link
delay assignment of 20ms on intra-transit links, 5ms on transit-
stub links and 2ms on intra-stub links.
n
u
m
b
e
r
o
f
h
a
s
h
f
u
n
c
t
i
o
n
s
:
k
u
s
e
o
f
R
T
T
-
w
e
i
g
h
t
e
d
r
o
u
t
i
n
g
-
-
-
-
#
#
(
ﬁ
g
:
7
)
-
(
d
u
e
t
o
r
e
d
u
c
e
d
p
e
r
-
h
o
p
#
(
t
a
b
l
e
:
1
)
-
m
e
t
r
i
c
f
e
a
t
u
r
e
u
s
e
o
f
u
n
i
f
o
r
m
p
a
r
t
i
t
i
o
n
i
n
g
v
a
r
i
a
n
c
e
r
e
d
u
c
e
d
v
a
r
i
a
n
c
e
r
e
d
u
c
e
d
-
l
a
t
e
n
c
y
)
-
r
e
d
u
c
e
d
v
a
r
i
a
n
c
e
(
ﬁ
g
:
9
)
T
a
b
l
e
3
:
E
f
f
e
c
t
o
f
d
e
s
i
g
n
p
a