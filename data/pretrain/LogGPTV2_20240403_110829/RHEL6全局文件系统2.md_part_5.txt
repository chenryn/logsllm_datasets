::: para
安全加强 Linux（SELinux）是为大多数情况推荐的安全选项，但不支持在 GFS2
中使用。SELinux 保存每个文件系统目标所使用的扩展属性的信息。GFS2
可以读取、写入并维护这些扩展属性，但也会极大降低其速度。因此必须在 GFS2
文件系统中关闭 SELinux。
:::
:::
::: section
::: titlepage
## [⁠]{#ch-considerations.html#s2-NFS-GFS-Issues}2.5.5. 使用 GFS2 设定 NFS {.title}
:::
::: para
由于 GFS2 锁定子系统额外的复杂性及其集群本质，使用 GFS2 设置 NFS
需要注意很多方面，并要格外小心。本小节论述了您在使用 GFS2 文件系统配置
NFS 时应该注意的问题。
:::
::: {.warning xmlns:d="http://docbook.org/ns/docbook"}
::: admonition_header
**警告**
:::
::: admonition
::: para
如果使用 GFS2 导出 NFS，且该 NFS 客户端程序使用 POSIX 锁，那么就必须使用
`localflocks`{.literal} 选项挂载该文件系统。预期的效果是这样可以强制
POSIX 锁从每台服务器转到本地：即非集群，且各自独立。（如果 GFS2 尝试从
NFS 跨集群中的节点使用 POSIX 锁，目前还有很多问题需要解决。）对于在 NFS
客户端中运行的程序，本地的 POSIX
锁意味着如果是从两台服务器中挂载两个客户端，则它们可以同时持有同一锁。如果所有客户端都使用一台服务器挂载
NFS，那么就不存在不同服务器单独提供同一锁定的问题。如果您不确定是否要使用
`localflocks`{.literal}
选项挂载您的文件系统，则不要使用该选项。在集群的环境中使用锁总是会更安全一些。
:::
:::
:::
::: para
除锁定注意事项外，您还应在使用 GFS2 文件系统配置 NFS
服务时注意以下问题：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    Red Hat 只支持使用有以下特征并附带 active/passive 配置的 NFSv3
    系统配置 Red Hat High Availability Add-On：
    :::
    ::: itemizedlist
    -   ::: para
        后端文件系统是一个在 2 到 16 个节点集群中运行的 GFS2 文件系统。
        :::
    -   ::: para
        将 NFSv3 服务器被定义为一次从独立集群节点中导出整个 GFS2
        文件系统的服务。
        :::
    -   ::: para
        NFS 服务器可以在从一个集群节点到另一个节点（active/passive
        配置）间进行故障切换。
        :::
    -   ::: para
        [*除非*]{.emphasis}通过 NFS 服务器，否则不允许任何对 GFS2
        文件系统的访问。这包括本地 GFS2 文件系统访问以及所有通过 Samba
        或者集群的 Samba 的访问。
        :::
    -   ::: para
        该系统中没有 NFS 额度支持。
        :::
    :::
    ::: para
    这个配置为该文件系统提供 HA，并减少系统停机时间，因为当 NFS
    服务器从一个节点到另一个节点失败时，失败的节点不需要执行
    `fsck`{.command} 命令。
    :::
-   ::: para
    GFS2 的 NFS 导出中 `fsid=`{.literal} NFS 选项是强制的。
    :::
-   ::: para
    如果您的集群出现问题（例如：该集群变得额度不足且 fencing
    无法工作），则会停止集群的逻辑卷以及 GFS2
    文件系统，且在该集群有足够额度前不可能有任何访问。您在决定是否使用简单的故障切换解决方案（比如：在这个步骤中规定的方法是否最适合您的系统）时考虑这个可能性。
    :::
:::
:::
::: section
::: titlepage
## [⁠]{#ch-considerations.html#s1-Samba-gfs2}2.5.6. 通过 GFS2 进行的 Samba（SMB 或者 Windows）文件服务 {.title}
:::
::: para
从 Red Hat Enterprise Linux 6.2 发行本开始，您可以在有 CTDB 的 GFS2
文件系统中使用 Samba（SMB 或者 Windows）文件服务，该文件系统应允许
active/active 配置。有关集群的 Samba
配置的详情请查看*《集群管理》*文档。
:::
::: para
目前尚不支持同时访问 Samba 中与 Samba 以外共享的数据。目前不支持 GFS2
集群租赁，该服务可延迟 Samba 文件服务。
:::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-considerations.html#s1-backups-gfs2}2.6. 文件系统备份 {.title}
:::
::: para
常规备份您的 GFS2
文件系统以防万一很重要，不要考虑文件系统的大小。很多系统管理员感到很安全是因为他们使用
RAID、multipath、镜像、快照以及其他形式的冗余，但永远没有足够安全这个说法。
:::
::: para
生成备份可能会有问题，因为备份一个节点或者一组节点的过程通常包括按顺序读取整个文件系统。如果在单一节点中进行，则该节点将在缓存中保留所有信息直到集群中的其他节点开始请求锁定。在集群中运行此类备份程序是可对性能产生负面影响的操作。
:::
::: para
备份完成后立即放弃缓存，这样可减少其他节点重新获得其集群锁/缓存所有权所需时间。但这仍不是最佳方法，因为其他节点将停止缓存备份进程开始前就已开始的缓存。您可以在备份完成后使用以下命令放弃缓存：
:::
``` screen
echo -n 3 > /proc/sys/vm/drop_caches
```
::: para
如果该集群在获得每个节点备份其各自拥有的文件就会更迅速，因为这样就将该任务分配到节点中进行。您还可以使用在没有具体节点的目录中使用
`rsync`{.command} 命令的脚本达到此目的。
:::
::: para
备份 GFS2 的最佳方法是在 SAN
中创建硬件快照，将该快照放到另一个系统中，并在那里进行备份。该备份系统应使用
`-o lockproto=lock_nolock`{.command} 挂载该快照，因为它不在同一集群中。
:::
:::
::: section
::: titlepage
# [⁠]{#ch-considerations.html#s1-hardware-gfs2}2.7. 硬件注意事项 {.title}
:::
::: para
您应在部署 GFS2 文件系统时注意以下硬件注意事项。
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    使用高质量存储选项
    :::
    ::: para
    GFS2 可以在便宜的共享存储中运行，比如 iSCSI 或者
    FCoE，但使用带较大缓存容量的较高质量的存储可获得更好的性能。Red Hat
    在使用光纤连接的 SAN
    存储中进行大多数质量、健全以及性能测试。基本原则是在部署某个产品前进行测试总是要好些。
    :::
-   ::: para
    部署前测试网络设备
    :::
    ::: para
    更高质量、更快速的网络设备可让集群沟通和 GFS2
    运行更迅速，且更可靠。但您不一定要购买昂贵的硬件。有些昂贵的网络交换机有传送多播数据包的问题，这些数据包是用来传递
    `fcntl`{.literal}
    锁（flocks），而较便宜的日用网络交换机有时更迅速且可靠。最好是在将是被部署到产品中时对其进行测试。
    :::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-considerations.html#s1-customer-portal}2.8. 性能问题：查看 Red Hat 客户门户网站 {.title}
:::
::: para
For information on best practices for deploying and upgrading Red Hat
Enterprise Linux clusters using the High Availability Add-On and Red Hat
Global File System 2 (GFS2) refer to the article \"Red Hat Enterprise
Linux Cluster, High Availability, and GFS Deployment Best Practices\" on
Red Hat Customer Portal at
.
:::
:::
::: section
::: titlepage
# [⁠]{#ch-considerations.html#s1-ov-lockbounce}2.9. GFS2 节点锁定 {.title}
:::
[]{#ch-considerations.html#idm140546324619376 .indexterm}
::: para
要获得最佳 GFS2
文件系统性能，则需要理解其操作的基本原理。单节点文件系统与缓存一同使用，其目的是在频繁使用请求的数据时可消除磁盘访问延迟。Linux
页面缓存（以及缓冲缓存）提供这个缓存功能。
:::
::: para
使用 GFS2，每个节点都可有其自身的页面缓存，该缓存中包含 on-disk
数据的一部分。GFS2 使用 *glocks*（发音为
gee-locks）锁定机制维护节点间缓存的完整性。glock
子系统提供缓存管理功能，该功能使用*分布式锁管理器*（DLM）部署作为基础沟通层。
:::
::: para
glocks
在每个内节点中为缓存提供保护，因此在每个内节点中都有一个锁定用来控制缓冲层。如果为那个
glock 赋予共享模式（DLM 锁定模式：PR），那么那个 glock
保护下的数据可同时被一个或者多个节点缓存，这样多有节点就都有到该数据的本地访问。
:::
::: para
如果为 glock 赋予专用模式（DLM
锁定模式：EX），那么只有一个节点可缓存那个 glock
保护的数据。所有修改数据的操作（例如 `write`{.command}
系统调用）都使用这个模式。
:::
::: para
如果另一个节点请求 glock，但无法立刻获得，那么 DLM
会向该节点发送一条信息，或者向目前使用 glock
并妨碍新的请求的节点发送信息，要求它们释放其锁定。释放
glock（大多数文件系统操作标准）需要很长时间。释放共享 glock
只需要使该缓存无效，相对缓冲的数据来说速度较快。
:::
::: para
释放专用 glock 需要 log
flush，并向磁盘写回所有更改的数据，之后要使每个共享的 glock 失效。
:::
::: para
单一节点文件系统与 GFS2 之间的区别在于单一节点文件系统只有一个缓存，而
GFS2
在每个节点中都有独立的缓存。在这两种情况下，对缓冲数据访问的延迟程度类似，但如果另一个节点之前缓冲了同样的数据，GFS2
对非缓冲数据访问的延迟要大得多。
:::
::: {.note xmlns:d="http://docbook.org/ns/docbook"}
::: admonition_header
**注意**
:::
::: admonition
::: para
Due to the way in which GFS2\'s caching is implemented the best
performance is obtained when either of the following takes place:
:::
::: itemizedlist
-   ::: para
    在所有节点中都使用只读方式使用内节点。
    :::
-   ::: para
    只在单一节点中写入或者修改内节点。
    :::
:::
::: para
请注意：在创建和删除文件的过程中插入和删除目录条目也算在内，因为要写入到目录内节点。
:::
::: para
也可能不遵守这个规则，但并不常发生。过度忽略这个规则会对性能有严重影响。
:::
::: para
如果您在 GFS2 中使用 read/write 映射 `mmap`{.command}()
某个文件，但只读取它，那么这只计为读取。而在 GFS 中会将其计为写入，因此
GFS2 使用 `mmap`{.command}() I/O 时更灵活。
:::
::: para
如果您没有设定 `noatime`{.literal} `mount`{.command}
参数，那么读取也会导致写入来更新文件时间戳。文件建议所有 GFS2
用户应该使用 `noatime`{.literal} 挂载，除非对 `atime`{.literal}
有具体要求。
:::
:::
:::
::: section
::: titlepage
## [⁠]{#ch-considerations.html#posix_lock_issues}2.9.1. Posix 锁定问题 {.title}
:::
[]{#ch-considerations.html#idm140546259828992 .indexterm}
::: para
使用 Posix 锁定时要注意以下问题：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    使用 Flokcs 将获得比使用 Posix 锁更迅速的处理。
    :::
-   ::: para
    GFS2 中使用 Posix 锁的程序应避免使用 `GETLK`{.literal}
    功能，因为在集群环境中，进程 ID 可能是用于该集群的不同节点。
    :::
:::
:::
::: section
::: titlepage
## [⁠]{#ch-considerations.html#gfs2_performance_tuning}2.9.2. 使用 GFS2 调节性能 {.title}
:::
[]{#ch-considerations.html#idm140546262350320
.indexterm}[]{#ch-considerations.html#idm140546262349360 .indexterm}
::: para
通常可以更改出问题的程序保存其数据的方法，这样可获得可观的性能优势。
:::
::: para
典型的问题程序例子有电子邮件服务器。通常会制定包含每个用户的文件的 spool
目录（`mbox`{.literal}），或者为每个用户创建一个目录，其中有包含所有信息的文件（`maildir`{.literal}）。当有来自
IMAP
的请求时，理想的情况是为每个用户赋予一个对特定节点的亲和性。那么将会使用那个节点中的缓存处理他们查看和删除电子邮件信息的请求。显然，如果那个节点失败，那么可在不同的节点中重启该会话。
:::
::: para
When mail arrives via SMTP, then again the individual nodes can be set
up so as to pass a certain user\'s mail to a particular node by default.
If the default node is not up, then the message can be saved directly
into the user\'s mail spool by the receiving node. Again this design is
intended to keep particular sets of files cached on just one node in the
normal case, but to allow direct access in the case of node failure.
:::
::: para
This setup allows the best use of GFS2\'s page cache and also makes
failures transparent to the application, whether `imap`{.literal} or
`smtp`{.literal}.
:::
::: para
备份通常是另一个令人纠结的问题。如果可能，最好直接从节点备份每个节点的工作集合，这样可缓存具体的内节点组。如果您可以定期运行的备份脚本，且与在
GFS2
中运行的应用程序反应时间完全一致，那么很有可能集群无法最有效地使用页面缓存。
:::
::: para
Obviously, if you are in the (enviable) position of being able to stop
the application in order to perform a backup, then this won\'t be a
problem. On the other hand, if a backup is run from just one node, then
after it has completed a large portion of the file system will be cached
on that node, with a performance penalty for subsequent accesses from
other nodes. This can be mitigated to a certain extent by dropping the
VFS page cache on the backup node after the backup has completed with
following command:
:::
``` screen
echo -n 3 >/proc/sys/vm/drop_caches
```
::: para
但这并不是一个好的解决方案，最好的方案是保证在每个节点中的工作集合要么是共享的（大多数为集群内只读），要么是从单一节点的大量访问。
:::
:::
::: section
::: titlepage