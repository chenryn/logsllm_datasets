### Figure 21: Normalized MPKI of SPEC CPU 2006 Benchmark Cases

**Figure 21** illustrates the normalized MPKI (Misses Per Kilo Instructions) for various types of randomized caches, using the static CEASE as the baseline. The following observations can be made:

- **Skewed Caches with Moderate Partitions**: Skewed caches with a moderate number of partitions (e.g., 2, 4, 8, 16) show a slight reduction in MPKI, but this reduction is marginal (less than 0.5%).
- **Performance Impact with More Partitions**: When the number of partitions exceeds eight, MPKI begins to increase, leading to performance degradation. This suggests that while skewed caches can offer some benefits, they are not the optimal solution.
- **Multi-Step Relocation (MS)**: Utilizing multi-step relocation (MS) reduces MPKI by approximately 0.05%, with the most significant benefit observed in skewed caches with only two partitions (0.08%). This aligns with our earlier estimations in Figure 12.
- **CEASER LLCs with Additional Features**:
  - **Remapping by Evictions (EV)**: Periodically remapping by evictions introduces an additional 0.08% MPKI.
  - **Attack Detection (DT)**: Enabling attack detection adds another 0.11% MPKI.
  - **Combining MS, EV, and DT**: Adopting multi-step relocation (MS) in conjunction with EV and DT reduces the overall overhead to a negligible 0.007%.

This result indicates that when all proposed enhancements (DT+EV+MS) are applied to CEASER, the randomized set-associative cache remains secure without significant performance loss.

### B. Logic and Memory Overhead

The memory overhead of randomized caches has been previously analyzed in [16], [17]. Here, we focus on the additional overhead introduced by the new ideas. We use a single-core Rocket-Chip (lowRISC ver. 0.4) [52] as the base configuration. The LLC (L2 cache) consumes around 22% of the processor's logic and 99% of its SRAM (excluding outer AXI buses and devices).

- **Remap Tracker**: To support remaps, a remap tracker is added to the LLC, which originally includes two access trackers and one writeback tracker. This addition incurs an extra area overhead of approximately 7.6% of the processor's logic (34% of the LLC's logic). Although this overhead is relatively high, it is necessary.
- **Eviction-Based Remapping**: Remapping by evictions rather than accesses introduces no additional area overhead.
- **Multi-Step Relocation**: The overhead for supporting multi-step relocation is minimal, requiring only the addition of a port to the remap tracker and modifications to its state machine.
- **Attack Detection**: A hardware prototype of the detector was developed, which completes each round of detection in 2K cycles (less than the sample period of 4K LLC accesses). By reducing the precision of intermediate results and using adder/shifter instead of multiplier/divider, the detection error remains within 5% compared to the software implementation. The area overhead after place and route is around 0.8% of the processor's logic and 0.4% of its SRAM (3.5% of the LLC's logic and 0.4% of its SRAM), both of which are marginal.

### VIII. Discussion

#### New Cache Designs
Since the introduction of randomized skewed caches, two new designs have been proposed, both promoting the use of set-associative caches:
- **Two-Level Dynamic Randomization (TLDR)** [19]: This design strengthens CEASER by adding an indirection table (iTable) for an additional layer of randomization. An address is first mapped to an iTable entry, which is then mapped to a random cache set. This approach claims to provide higher randomness and reduce remap-related performance loss.
- **PhantomCache** [21]: This design places incoming cache blocks in one of multiple randomly selected cache sets, increasing randomness and allowing the use of LRU for the entire cache set. Both TLDR and PhantomCache can safely defeat GE attacks, but their effectiveness against CT and PPT attacks requires further investigation.
- **Doblas** [20]: This extends cache randomization from the LLC to L1 caches using simple randomization functions.

#### Performance Evaluation
The performance results of existing cache randomization designs are based on various Gem5 simulations [16]–[18], [21], [42]. These simulations are slow, limiting the total number of instructions that can be simulated in a reasonable time, which in turn constrains the coverage of representative workloads [60], [61]. Our choice of using the fast (event-driven and timeless) Spike simulation allows us to simulate up to 100-400 times more instructions, significantly increasing the coverage of representative workloads. However, this limits the performance evaluation to miss rate, leaving the overhead on CPU execution time unstated. We believe this is a reasonable trade-off.

After the encryption algorithm used in CEASER was found problematic [34], there is no consensus on which encryption algorithm should be adopted. The extra delays introduced by cache randomization remain a challenge, and the estimation of CPU execution time is already inaccurate for any comparison between designs, even if the slowest Gem5 OoO model [42] is used. Cache miss rate is the only frequently used and unbiased metric available.

#### Attack Detection
Run-time detection of cache side-channel attacks using existing performance counters (pfc) [58], [62]–[65] has shown to be effective in detecting persistent attacks by software. Some approaches use machine learning to increase detection accuracy [62], [64], but they are constrained by the limited information available from pfc. The concentration of cache accesses on target cache sets during the exploitation phase has long been known [22], [58], [66]. Recent hardware detectors with set-level granularity begin to utilize this pattern [67], [68]. Most exploit the cyclic pattern between an attacker and their victim [65], [67], [68]. To the best of our knowledge, we are the first to exploit the unique set distribution of cache evictions during the search for eviction sets in randomized caches. Whether an attacker can evade detection by slowing down and hiding behind background noise remains an open question.

#### New Attacks
Purnal et al. [30] improve the original PPT attack by introducing a prune phase and correctly point out the possibility of using partially congruent eviction sets to launch covert channel attacks on ScatterCache [30]. Our simulation and analysis of PPT are based on Purnal’s work, but with our own optimized prune method, as it is not clearly described in [30]. Our experiments show that PPT attacks fail on randomized skewed caches because the accumulated number of LLC evictions always surpasses the proposed remap period. Recently, Purnal et al. have further improved PPT by optimizing the pruning and profiling method [69]. We need to evaluate these new optimizations and decide whether attack detection is also needed for randomized skewed caches. In another concurrent work [70], Bourgeat et al. analyze the end-to-end security impact of utilizing partially congruent eviction sets. They find that attackers may opt for eviction sets with lower eviction rates to increase the chance of information leakage, and reducing the remap period can significantly increase the cost of attacks. These findings complement our work.

### IX. Conclusion

We have identified several issues with the hypotheses and implementations in the latest randomized skewed caches:
- The potential use of cache flush instructions in conflict-based attacks has been overlooked.
- The concept of minimal eviction sets no longer applies to randomized skewed caches.
- Attackers do not need to use eviction sets with 99% eviction rates.
- Measuring the remap period by LLC accesses is flawed.

As a result, existing randomized skewed caches remain vulnerable to conflict-based cache side-channel attacks. We propose several defense ideas to address these issues:
- Measure the remap period by LLC evictions rather than accesses, and further reduce the period.
- Adopt ZCache-like multi-step relocation to minimize the number of cache blocks evicted during the remap process.

Our experiments show that all newly discovered vulnerabilities are fixed within the current performance budget. We also claim that randomized set-associative caches can be sufficiently strengthened with reasonable overhead using a simple attack detection mechanism. Compared to randomized skewed caches, randomized set-associative caches are better candidates for future commercial processors.

### Acknowledgments

The authors would like to thank the anonymous reviewers for their valuable comments. This work was supported by the National Natural Science Foundation of China under grant No. 61802402 and No. 61802397, the CAS Pioneer Hundred Talents Program, and internal grants from the Institute of Information Engineering, CAS. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding parties.