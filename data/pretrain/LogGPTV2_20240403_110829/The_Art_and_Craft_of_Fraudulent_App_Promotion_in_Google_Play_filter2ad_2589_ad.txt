the same device for those reviews.” Proof of work details in § 5.12.
• Improper VPN use. P10 also blamed VPN: “One safe way is, login
from normal IP, then write review from VPN. If you login using VPN,
Google will detect this as fraud.”
• Improper app use. P12 said that Google deletes reviews if the
users “do not care to use the app and keep it installed for more days.”
More details in the app retention part of § 5.5.
• Extended account use. P3, P9, P18 report that using the same
account to write many reviews in a short time, may trigger red(cid:30)ags.
• Mis(cid:27)res of Google fraud detection. P6 blames it on Google: “Some-
times genuine reviews get deleted and sometimes multiple reviews
from same devices don’t get deleted.”
User account validation. P2 and P3 said that they prefer to use
e-mail to validate user accounts. P3 also said that Google may force
them to use phone numbers. Only P16 claimed that “we use virtual
phone numbers and Google accepts them.” All others said that they
use real phone numbers to validate accounts.
Real phone numbers require access to SIM cards, which can
be expensive. However, participants revealed ingenious solutions
to bypass this limitation. For instance, P3, P10, P11 and P17 use
friends and family: P3 said that “We use our friends and family phone
numbers. For example, I meet a friend on the road, I ask him to check
the message and I use his phone number to verify an account.” P10 said
that “In Bangladesh one person can buy as many as 20 SIM cards using
his credentials. [..] For example, for my 450 Gmail accounts I have
used at least 200 phone numbers.” P5 mentioned that he borrowed
SIM cards from friends. P7 and P15 use phone number veri(cid:27)cation
services. Concretely, P7 said “we pay other people to get a one-time
code from their mobile SMS to verify those accounts.” P13 said that
they purchase user accounts that are already validated.
Several participants reported limitations on phone number reuse.
For instance, P3 and P8 said that one number could be used for
3–5 accounts but not immediately, while P1 said that “between two
veri(cid:27)cation using the same number, we have to wait at least 3 months.”
Review without install. When asked, P5, P10, P13 and P18 said
that one can review an app without its prior installation from a
device on which the account is logged in [20]: “Click on install then
stop installing immediately. The app would not be installed but it will
allow us to write reviews.” We have tested this claim and veri(cid:27)ed that
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2443it works as suggested. This vulnerability breaks Google’s intended
security design [20] and facilitates the creation of fake reviews by
reducing the amount of resources needed from the ASO worker.
App installation and use. 14 participants claimed to wait, open,
or even use the app before reviewing it. P5 and P9 wait a few hours
before reviewing the installed app. P9 claimed to also use it for
5–10 minutes. P6 and P8 claimed to open the app 1–2 times before
reviewing. P7 claimed to use the app as a normal user. P10, P13 and
P16 claimed to keep the app open for 3–15 minutes before writing
the review. P12, P14, P17 and P18 claimed to recommend to their
online and organic teams to open the app for a few minutes and
even use it before reviewing. P4 said “We try to navigate all the
pages of the app before writing the reviews.”
All the participants admitted to perform retention installs. P10
said that this is required to prevent (cid:27)ltering: “Google takes 72 hours
to verify the review. If you delete the app in this period, Google will
drop the review.” Most participants said that they keep the app for a
few days after reviewing it: 1 day (P1, P5 and P15), 2–3 days (P4,
P5, P8, P10, P13, P14), 1–2 weeks (P17), and 7 days – 2 months (P2).
P4 said that his workers keep the app until they need the space.
Upvote, Downvote. 6 of the 18 participants (P5, P7, P10, P14, P15,
P16) said that they upvote reviews written by their team from other
accounts. P7 said “We upvote the reviews put by our team and also
other reviews which are positive.”
P10 said that his team downvote negative reviews of the apps
they target, in order to trigger Google’s (cid:27)ltering mechanism, thus
remove those reviews. P7 said “We provide upvote and downvote
services to move positive reviews to the top and negative to bottom.”
Singleton accounts. P1, P2, P7, P10, P13, and P15 said they worked
on jobs where they had to create accounts just to post one review
and then to abandon them. P1 and P2 said that the cost of such
reviews is higher, $8 and $10 respectively. The reason for this is
due to the e(cid:29)ort to create an account, which will not be amortized
over multiple fake review posting activities. The reason given by
the participants for being requested to do this is that Google does
not (cid:27)lter reviews posted by singleton accounts, since its fraud
detection module needs more information to build a reputation for
the account.
Account blending. 12 participants claimed to have seen jobs that
required only the use of old accounts. However, P1, P2, P7, P10, P11,
P13, P15 said that they have worked on jobs where they only used
fresh accounts. P10 said that “We do it because Google always keeps
the reviews received from new accounts.” P1, P2, P7, P16, P18 said
that they regularly use a mix of old and new accounts. In § 5.13 we
report account creation and purchase strategies.
Noisy reviews. P2, P3, P5, P7, P10, P13, P15 and P16 said that they
do not review other apps to avoid detection. Of the physically co-
located teams, only P1 said that they review products for which
they have not been hired, which they pick at random. 7 participants
with online and organic team members (P4, P6, P8, P11, P14, P17,
P18) said that their online team members do review other apps,
which they normally use in their real life. P4 said “That’s why we
use real users. We don’t need to follow any strategy. The real users’
behaviors serve the purpose of authenticity. We always instruct them
to use other popular apps from their accounts.”
Device reset. P10 said that before logging in to an account, they
(cid:30)ush the virtual device and change its MAC address. After using the
account and virtual device pair for a few days to install and review
apps, they log out and repeat the process with another account.
They then leave the previous account unused for 1–1.5 months:
“after that interval, Google does not check that the new login is from
the same MAC address as the previous one.” P13 similarly claim to
stay logged in to the account for 3 days, then they reset the device
(using cccleaner) before logging in to the next account.
VPN use. P1, P3, P5, P13, P15 admitted to use VPNs, while the other
10 explicitly claimed to not use them. P3 said “We use VPN or proxy
only when it is required in the job speci(cid:27)cation. For example, if I need
to install from USA, we have to use USA proxy server. (sic)”
Emulator use. P10 and P13 said that their teams use virtual devices
running in laptops. The others claimed to use mobile devices or
have access to real users equipped with mobile devices.
Summary. Several of our interview participants con(cid:27)rmed several
observations proposed in previous work: (1) ASO workers adjust
their behaviors to avoid detection [24, 36, 44, 68, 74, 74], includ-
ing using VPNs [58, 85], and mobile device emulators running on
PCs [58, 81, 96]. However, P10 noted that improper use of VPNs
can also trigger fraud (cid:27)ltering. (2) ASO workers also write gen-
uine reviews, for products for which they have not been hired [24,
36, 38, 52, 74, 89]. However, this is only supported by participants
who claimed to recruit and use organic ASO workers. (3) Some
participants claimed to upvote their own reviews [68]. (4) Some par-
ticipants also report using singleton accounts [63, 74, 76, 92, 100].
We however report a surprising motivation for this, which is not
convenience, but rather a fraud detection strategy that exploits
cold-start problems of Google’s fraud detector.
Further, we identi(cid:27)ed new black hat ASO behaviors, that include
downvoting negative reviews to promote their (cid:27)ltering by Google,
and the unexpected bene(cid:27)ts of using singleton accounts. Partici-
pants revealed ingenious solutions to bypass Google-imposed veri-
(cid:27)cations, and validate the user accounts that they control, with real
phone numbers. They provide circumstantial support for previous
work studying the underlying technical and (cid:27)nancial capabilities
of social network fraudsters [84].
Several participants reported the ability to bypass Google’s check
of preventing reviews without prior app installation. However, to
avoid (cid:27)ltering, all participants said they use a combination of app
interaction, delaying of review posting, and retention installs.
Further, we conjecture that the claimed use of a blend of older
with newly created accounts, enables ASO workers to replenish
or increase their base of accounts controlled, build the reputation
of older accounts, and reduce chances of detection of lockstep
behaviors (§ 5.8) and the use of singleton accounts.
5.6 Review Burst vs. Campaign Length
We now present (cid:27)ndings on the timing of the review process. 16
interview participants claimed to have seen jobs (1–45 in the past
month) that specify how many reviews per day the workers should
post. For instance, P5 said that “Most buyers don’t want to get all
the reviews in a single day. They want a slow rate, like 2–3 reviews
each day. To maintain this rate, they provide the review text on a
daily basis.” However, P6 also said that “some developers with money
don’t care whether reviews stay or not. They just need the number of
reviews, quality doesn’t matter. They just want short-time business.”
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2444(a)
(b)
Figure 8: (a) Per-worker distribution of the number of reviews per day for each targeted app. (b) Per-worker distribution of
time di(cid:29)erence in hours between consecutive reviews posted within one day for targeted apps. F7, F9, F10, F16, F27, F28, F31,
tend to post more reviews per day, in bursts. F1, F3, F19, F20, F23, F29, F35, F37-39 post few daily reviews, but in bursts. Others
like F6, F11, F13, F23-F26, F32, F33 post few daily reviews, but space them through the day (post one every 8-9 hours).
P1, P3 and P5 reported that they suggest to the hiring developers,
the rate of posting reviews. P5 said “If the developer asks for 30
reviews each day, I have to warn him that it’s harmful to his app as
Google may detect this as fake. Then I’ll suggest to him that I will
take 10 days to provide 30 reviews.” Most participants suggest 2–3
reviews per day, but some (e.g., P11, P14, P17, P18) recommend
higher numbers, up to 30–40 reviews per day (P14).
Several participants suggested that the number of recommended
daily reviews is a function of the app’s existing review count. Con-
cretely, P6 said, “for new apps with less installs, it is better not to
provide many reviews each day. But for popular apps, 20–50 reviews
each day would be acceptable.”
P10 revealed a di(cid:29)erent strategy: “We provide a slow rate at the
beginning. Like 1 review per day, or 5 reviews in 6–7 days. After 10
reviews we start posting 2 reviews each day. After 150 reviews we can
provide 3–4 reviews each day.”
All the participants except P4 said that they have seen ASO jobs
that require a duration for the review posting campaign. P6 and
P15 said that this is rare, and that developers are more concerned
about the total number of reviews. However, P2 said almost all the
jobs he has seen in the past month, mention the campaign length.
In the past month, 5 participants have seen 3–5 such jobs, 4 have
seen 6–10 jobs, and 5 have seen 11–35 such jobs. 12 participants
reported longest seen required campaigns of 1–6 months, and 6
participants reported campaigns of 7–18 months.
Quantitative Investigation. Figure 8(a) shows the per-worker,
violin-shaped distribution of the number of reviews per day, posted
from accounts controlled by the 39 ASO workers, for each targeted
app. Figure 8(b) shows the violin plots for the distributions of the
inter-review times (only those posted within the same day). We
observe several participants, e.g., F7, F9, F10, F16, F27, F28, F31, who
tend to post more reviews per day, an do so in bursts. We also see
participants, who even though write fewer reviews per day, still
tend to post them in bursts (F1, F3, F19, F20, F23, F29, F35, F37-39).
However, as also reported by the interview participants, we also
found ASO workers who post only a small number of reviews per
day and space them well through the day. Notably, F6, F11, F13,
F23-F26, F32, F33 have a mean inter-review time of 8-9 hours.
Further, we call a worker’s active interval for an app, the time
span (in days) between the worker’s last and (cid:27)rst review for the app
from accounts that we know he controls. Figure 9 shows the per-
worker active interval distribution over the 316 apps that received
Figure 9: Per-worker distribution of active intervals (in days)
over apps targeted. Each point represents the active interval
of an ASO worker for an app. We observe workers who have
posted reviews for certain apps, for more than 1 year, and
up to more than 4 years.
at least 10 reviews from the 39 participants. Some ASO workers
were often active for more than 1 year for an app.
Summary. We found ASO workers who post fake reviews in rapid
bursts in both our qualitative and quantitative investigations. This is
consistent with assumptions made in previous fraud detection work,
e.g., [26, 28, 32, 36–38, 40, 42, 43, 51, 52, 59–61, 63, 65, 92, 95, 96,
100]. However, multiple interview participants have revealed both
developer and ASO worker assumptions that Google (cid:30)ags review
bursts. Some participants also claimed to push back on developers
who asks for many daily reviews. Our quantitative analysis reveals
ASO workers whose behavior is consistent with these statements.
Interview participants further revealed avoidance techniques that
include (adaptive) rate control.
Rate control implies longer campaigns, as workers need more
time to post their review quota. This is further supported by state-
ments made by several interview participants and by evidence we
extract from the quantitative investigation.
5.7 Accounts Per Device Strategies
Participants revealed mixed strategies for the number of accounts
used on a device, and the number of reviews that they publish from
a single device. P10, P11, P13, P18 said that they only log in to
one account at a time, on any device that they control. P18 has 30
devices and 30 accounts, and a 1-to-1 mapping between accounts
and devices. P11 said that “If we provide multiple reviews from one
device, Google will keep only one review for that device.” P5, P6, P7, P8,
P9, P15 and P16 claimed to log in to multiple accounts (2–5) from a
single device and also instruct their remote workers to do the same.
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2445To investigate lockstep behaviors in the gold standard fraud
data (§ 4.2), we used frequent itemset mining [23, 64] to discover
sets of apps that are co-reviewed by many accounts in the same
or similar order. Intuitively, a set of apps reviewed by the same,
many user accounts, is said to be “frequent”. More formally, let A =
{a1, a2, . . . , an} be a set of apps, and let U = {u1, u2, . . . , um} be a
set of users in Google Play. We say that a set A ⊆ A is s−frequent
if |{u∈U;A⊆Tu }|
≥ s where Tu = {a ∈ A; a is reviewed by u}.
|U|
We used the A-priori algorithm [23, 57], to (cid:27)nd per-worker max-
imal frequent itemsets: frequent itemsets for which none of their
immediate supersets are frequent. 25 of the 39 participants had
maximal frequent itemsets with s = 0.5. That is, they used at least
half of their accounts to review common subsets of apps.
Figure 10 shows lockstep matrices for two of the ASO workers.
In the lockstep matrix Mij of a worker, columns are user accounts
controlled by the worker and rows are apps reviewed from those
accounts. Mij ∈ [nw] denotes the chronological order of the review