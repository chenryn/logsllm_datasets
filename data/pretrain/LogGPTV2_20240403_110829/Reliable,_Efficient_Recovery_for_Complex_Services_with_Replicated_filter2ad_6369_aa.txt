title:Reliable, Efficient Recovery for Complex Services with Replicated
Subsystems
author:Edward Tremel and
Sagar Jha and
Weijia Song and
David Chu and
Ken Birman
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Reliable, Efﬁcient Recovery for Complex Services
with Replicated Subsystems
Edward Tremel, Sagar Jha, Weijia Song, David Chu, and Ken Birman
Cornell University, Ithaca, NY, USA
ensure high availability, replicas must be placed into distinct
failure correlation sets.
Abstract—Applications with internal substructure are common
in the cloud, where many systems are organized as independently
logged and replicated subsystems that interact via ﬂows of objects
or some form of RPC. Restarting such an application is difﬁcult: a
restart algorithm needs to efﬁciently provision the subsystems by
mapping them to nodes with needed data and compute resources,
while simultaneously guaranteeing that replicas are in distinct
failure domains. Additional failures can occur during recovery,
hence the restart process must itself be a restartable procedure.
In this paper we present an algorithm for efﬁciently restarting a
service composed of sharded subsystems, each using a replicated
state machine model, into a state that (1) has the same fault-
tolerance guarantees as the running system, (2) satisﬁes resource
constraints and has all needed data to restart into a consistent
state, (3) makes safe decisions about which updates to preserve
from the logged state, (4) ensures that the restarted state will
be mutually consistent across all subsystems and shards, and (5)
ensures that no committed updates will be lost. If restart is not
currently possible, the algorithm will await additional resources,
then retry.
I. INTRODUCTION
We are seeing a shift from a query-dominated cloud in
which most operations are read-only and use data acquired
out-of-band, to a real-time control cloud, hosting increasingly
complex online applications, in which near-continuous avail-
ability is important. Such needs arise in stream processing for
banking and ﬁnance, IoT systems that monitor sensors and
control robots or other devices, smart homes, smart power
grids, smart highways, and cities that dynamically manage
trafﬁc ﬂows, etc. These applications often have multiple sub-
systems that interact, and that bring safety requirements which
include the need for fault-tolerance and consistency in the
underlying data-management infrastructure.
Traditional transactional database methods scale poorly if
applied naively [1]. Our work adopts a state machine replica-
tion model, using key-value sharding for scaling. Such models
are relatively easy to program against and hence increasingly
popular, but pose challenges when crashes occur.
To maintain the basic obligations of the state machine
replication methodology, updates must be applied to replicas
exactly once, in the same order, and should be durable despite
damage a failure may have done. For a given replication factor
the system should also guarantee recoverability if fewer than
that number of crashes occur. Subsystems may being further
constraints: numbers of cores, amounts of memory, etc. A
further consideration is that datacenter hardware can exhibit
correlated failures due to shared resource dependencies. To
Performance considerations further shape the design of
modern cloud systems, which often migrate artiﬁcially intelli-
gent behavior into the edge [2]. This may entail use of machine
learned models for decision-making or event classiﬁcation, as
well as real-time learning in which new models are trained
from the incoming data stream. For example, a smart highway
might need to learn the behavior of vehicles, and adapt the
acquired models as vehicles change their behavior over time.
The large data sizes (photos, videos, radar, lidar) and intense
time pressure (guidance is of little value if based on stale data)
compel the use of accelerators, such as RDMA (which ofﬂoads
data transport to hardware and achieves zero-copy transfers),
NVM (which offers durable memory-mapped storage), GPU
and TPU, and FPGA, without which applications would often
be unable to meet performance demands [3, 4, 5].
The Derecho library [3] was created to support this new
class of demanding edge applications. Derecho models the
application as a collection of subgroups where each subgroup
is partitioned into shards (subgroups can overlap, but shards of
the same subgroup are disjoint). Each shard is a replicated state
machine. The membership of the entire system is managed
in a top-level group, which consists of all the nodes in the
system. Figure 1 shows an example application. Derecho
makes several key design decisions that are necessary to
achieve high performance:
• Consensus off the critical path: Derecho adopts a virtual
synchrony approach [6]. The top-level group membership
moves through epochs (or views) where each epoch is a
failure-free run of the system. Each failure triggers a re-
conﬁguration (or view change) of the group membership.
The view change involves agreement on pending updates
and recomputation of the membership of each shard.
• Update all, read any single replica: An update is only
committed in a shard if it has been logged at every non-
failed member. Every replica has full state, enabling fast
single-replica queries that do not interfere with updates.
In this model, a shard can survive the failure of all but one
member without losing any committed updates. This is in
contrast to quorum-based protocols [7], where it sufﬁces
to update a majority of replicas, but where a query or
a restart involves merging state from multiple replicas.
Moreover, Derecho pipelines updates, such that each log
consists of a preﬁx of committed updates followed by
a sufﬁx of pending updates. A reconﬁguration results in
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00035
172
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
Sensors, other 
external “clients” 
Load-balancing subgroup 
Shards 
Update logs 
“Front-end” subgroup 
Multicast-only subgroups 
“Back-end” subgroup 
Fig. 1: A Derecho service spanning 16 (or more) machines
and containing several subsystems that employ a mix of point-
to-point RPC and multicast. The ovals represent subgroups
and shards within which data is replicated. Independent use
of state machine replication isn’t sufﬁcient: after a shutdown,
components must restart in a mutually-consistent state.
a distributed log cleanup where updates that cannot be
committed are discarded.
• Distributed logs: For safety, each shard member needs to
log updates before they are committed. In this class of
services, the “state of the application” is decentralized.
Services sometimes shut down and must later be restarted,
for example when the application is migrated to different
nodes, software is updated or the datacenter as a whole
is serviced. Clearly we must recover each individual SMR
subgroup or shard, but notice that the recovered states also
need to correspond to a state the service as a whole could
have experienced, while also preserving every committed
update. This obligation is not unique to Derecho: systems like
vCorfu [8] (the multi-log version of Corfu [9]) and Ceph [10]
also have multiple subsystems that use sharding. Nonetheless,
the problem has not previously been studied. For example,
although the Derecho paper is detailed,
it focuses on the
efﬁciency of its protocols, their mapping to RDMA, and the
resulting performance.
There are several factors that make restarting non-trivial:
• Failures during restart complicate the problem. We need
to ensure safety under all circumstances and restore the
system to a consistent, running state, equivalent to the
last committed state before total failure.
• Some nodes that were once part of the system may never
recover. Moreover, some restarting nodes may have failed
in a view preceding the last view before the restart, in
which case they will not be aware of the last member-
ship of the top-level group. We need to determine the
conditions under which a restart is possible and reconcile
incomplete logs stored by shard members.
• We need to satisfy application constraints related to
deployment. For example, shards may require that the
members belong to different failure regions of the data-
center, impose a minimum on the number of members,
and specify hardware conﬁgurations (such as number of
cores, amount of memory, GPUs, etc).
The restart process should also be highly efﬁcient to min-
imize application downtime. Thus we need to minimize the
data transferred during restart and optimize data movement.
In this paper, we describe our restart algorithm for such sys-
tems, with conﬁgurable parameters as follows. Our algorithm
requires the restarting service to designate a restart leader; it
can be any restarting node. We model the failure characteristics
of the nodes by organizing them into failure correlation sets.
The application speciﬁes the minimum number of failure
correlation sets that the members of a shard should come
from, for each shard of every subgroup. The application pro-
vides mappings from nodes to failure correlation sets through
conﬁguration ﬁles, making the process highly ﬂexible; it can
choose to distinguish nodes that belong to different racks or
different regions of the datacenter altogether.
Our paper makes the following contributions:
1) Characterization of the state recovery problem for ser-
vices composed of stateful subsystems, including a def-
inition of correct recovery for replicated state machines
that share a conﬁguration manager.
2) An algorithm for restarting such a system from durable
logs, including reasoning that argues why the algorithm
is safe in the presence of any number of crashes, and
live as long as any quorum of the last live conﬁguration
eventually restarts.
3) An algorithm that provably assigns nodes to shards in a
way that satisﬁes deployment constraints and minimizes
state transfer.
4) An experimental evaluation showing that a structured
service can be recovered quickly and efﬁciently using
this algorithm.
An implementation is available in the Derecho system.
In section II, we describe the restart problem at length,
discussing our desirable goals for any algorithm that solves
it. In section III, we discuss our restart algorithm and the
accompanying algorithm for assigning nodes to shards while
satisfying deployment constraints. In section IV, we reason
about the correctness of the restart algorithm and prove the
node assignment algorithm correct. We show the feasibility
of our approach in section V and discuss related work in
section VI. Finally, we summarize our ﬁndings and conclude
in section VII.
II. PROBLEM DESCRIPTION
The essence of our problem is that independent recovery
of state-machine replicated components is not sufﬁcient. SMR
guarantees that a service with 2f + 1 members can tolerate
f crash failures. However a complex service with multiple
subsystems and shards has many notions of f. For the service
as a whole, Derecho’s virtually synchronous membership
protocol requires that half the members remain active from
one view to the next; this prevents “split brain” behavior. But
notice that in Figure 1 some shards have as few as 2 members.
A log instance could be lost in a crash, hence such a shard
must not accept updates if even a single member has crashed.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
173
We can distinguish two cases. One involves continued
activity of a service that experiences some failures, but in
which many nodes remain operational. This form of partial
failure has been treated by prior work, including the Derecho
paper. In summary, if the partial failure creates a state in which
safety cannot be maintained, the service must halt (“wedge”)
and cannot install a new view or accept further updates until
the damage has been repaired.
The second case is our focus here: a full shutdown, which
may not have been graceful (the service may not have
been warned). To restart, we must ﬁrst determine the ﬁnal
membership of the entire service, and the mapping of those
nodes to their shard memberships in the restarted service.
Then we must determine whether all the needed durable state
is available, since recovery cannot continue if portions of
the state are lacking, even for a single shard. Furthermore,
intelligent choices must be made about the mapping of nodes
to shard roles in the restarted service. On the one hand, this
must respect constraints. Yet to maximize efﬁciency it is also
desireable to minimize “role changes” that entail copying
potentially large amounts of data from node to node.
In what follows, we will describe the restart problem and
our algorithm for its solution in terms of a more generic
system, with the hope that our techniques will be useful even
in systems where Derecho is not employed.
A. System Setup
We consider a distributed system of nodes (i.e. processes)
organized into subgroups partitioned into shards, in which
each shard implements a virtually synchronous replicated state
machine. In general, we will refer to a shard without men-
tioning which subgroup it belongs to, unless the distinction
is important for clarity. Each shard maintains a durable log
of totally ordered updates to its partition of the system state,
and an update is considered committed once it is logged at
every replica in the current view. As is standard in the virtual
synchrony approach [6], each update records the view in which
it was delivered. Also, each reconﬁguration (view change)
event requires every node to commit to an epoch termination
decision which must contain, at a minimum, the highest update
sequence number that can be committed in each shard, as well
as the ID of the view that it terminates.
We believe this model to be quite general. Obviously, it is
a natural ﬁt for services implemented using Derecho, but it
can also be applied to the materialized stream abstraction in
vCorfu [8]. A vCorfu stream abstracts the action of applying
a sequence of updates to a single replicated object (what we
would call a shard). Moreover, vCorfu has multiple subsys-
tems: it stores the system’s conﬁguration in a separate layout
server, rather than having replicas store their own conﬁgura-
tion. Turning to the Ceph ﬁle system [10], we ﬁnd a meta-
data service, a cluster mapping service, and a sharded SMR-
replicated object store (RADOS). Again, the requirements are
analogous to the ones we described for Derecho, with the
cluster map playing the role of the view. To our knowledge,
neither vCorfu nor Ceph currently addresses the issue of
consistency across different shards and subsystems in the event
of a full shutdown; our methods would thus strengthen the
recoverability guarantees offered by these systems.
B. The Restart Problem
Our task is to ensure that the committed state of this system
can be recovered in the event that every node in the system
crashes in a transient way. This could be the result of a power
failure or network disconnection, or an externally-mandated
shutdown caused by datacenter management policies. When
the system begins restarting after such a failure, we can assume
that most of the nodes that crashed will resume functioning
and can participate in the restart process. However, some nodes
may remain failed. The system should be able to restart as
long as enough of its former members participate in the restart
process to guarantee that its state has been correctly restored.
Speciﬁcally, we need to restore the system to a consistent,
running state, that is equivalent to its last committed state
before the total failure. The restarted system must also have the
same fault tolerance guarantees as it did before. This means
that the restarted distributed service must (1) include every
update in every shard that had reached a durably-committed
state before the crash, (2) adopt a conﬁguration that is the
result of a valid view-change procedure from the system’s last
installed conﬁguration, and (3) assign nodes to shards such that
each shard meets its constraint of having nodes from different
failure correlation sets.
The service must also be resilient against failures during the
restart process, since the same transient crashes that caused it
to stop can also occur during restart. It must tolerate the failure
of any node in the system, detect it, and revert the system to a
safe state until recovery can continue. Recovery must be able
to continue from any intermediary state.
We assume that some simple external process triggers the
restart procedure, such as a datacenter-management system
that re-runs each interrupted program after a shutdown event.
As a preliminary design choice, we will also assume that the