0.014
40.953
39.388
58.375
0.451
0.036*
0.004*
0.022*
0.044*
Table 3: Final linear regression model of teams’ ship scores,
indicating how many points each selected factor adds to the
total score. Overall eﬀect size f 2 = 0.163.
est, followed by static submissions, and then those written
in C/C++ (which has the largest minimum size).
4.5 Code quality measures
Now we turn to measures of a build-it submission’s quality—
in terms of its correctness and security—based on how it held
up under scrutiny by break-it teams.
Resilience. The total build-it score is the sum of ship score,
just discussed, and resilience. Resilience is a non-positive
score that derives from break-it teams’ test cases that prove
the presence of defects. Builders may increase this score
during the ﬁx-it phase, as ﬁxes prevent double-counting test
cases that identify the same defect (see §2.1).
Unfortunately, upon studying the data we found that a
large percentage of build-it teams opted not to ﬁx any bugs
reported against their code, forgoing the scoring advantage
of doing so. We can see this in Figure 4, which graphs the
resilience scores (Y-axis) of all teams, ordered by score, for
the two contests. The circles in the plot indicate teams that
ﬁxed at least one bug, whereas the triangles indicate teams
that ﬁxed no bugs. We can see that, overwhelmingly, the
teams with the lower resilience scores did not ﬁx any bugs.
We further conﬁrmed that ﬁxing, or not, was a dominant
factor by running a regression on resilience score that in-
cluded ﬁx-it phase participation as a factor (not shown).
Overall, teams ﬁxed an average of 34.5% of bugs in Spring
Figure 4: Final resilience scores, ordered by team, and plot-
ted for each contest problem. Build-it teams who did not
bother to ﬁx bugs generally had lower scores.
Spring 2015 Fall 2015
Bug reports submitted
Bug reports accepted
Fixes submitted
Bugs addressed by ﬁxes
24,796
9,482
375
2,252
3,701
2,482
166
966
Table 4: Break-it teams in each contest submitted bug re-
ports, which were judged by the automated oracle. Build-it
teams then submitted ﬁxes, each of which could potentially
address multiple bug reports.
2015 and 45.3% of bugs in Fall 2015. Counting only “active”
ﬁxers who ﬁxed at least one bug, these averages were 56.9%
and 72.5% respectively.
Table 4 digs a little further into the situation. It shows
that of the bug reports deemed acceptable by the oracle (the
second row), submitted ﬁxes (row 3) addressed only 23% of
those from Spring 2015 and 38% of those from Fall 2015
(row 4 divided by row 2).
This situation is disappointing, as we cannot treat re-
silience score as a good measure of code quality (when added
to ship score). Our hypothesis is that participants were not
suﬃciently incentivized to ﬁx bugs, for two reasons. First,
teams that are suﬃciently far from the lead may have cho-
sen to ﬁx no bugs because winning was unlikely. Second,
for MOOC students, once a minimum score is achieved they
were assured to pass; it may be that ﬁxing (many) bugs
was unnecessary for attaining this minimum score. We are
exploring alternative reward structures that more strongly
incentivize all teams to ﬁx all (duplicated) bugs.
Presence of security bugs. While resilience score is not
suﬃciently meaningful, a useful alternative is the likelihood
that a build-it submission contains a security-relevant bug;
by this we mean any submission against which at least one
crash, privacy, or integrity defect is demonstrated. In this
model we used logistic regression over the same set of factors
as the ship model.
Table 5 lists the results of this logistic regression; the co-
eﬃcients represent the change in log likelihood associated
Factor
Coef. Exp(coef )
SE p-value
Fall 2015
# Languages known
Lines of code
Dynamically typed
Statically typed
MOOC
5.692
-0.184
0.001
-0.751
-2.138
2.872
296.395
0.832
1.001
0.472
0.118
17.674
1.374 <0.001*
0.086
0.033*
0.030*
0.0003
0.393
0.879
0.016*
0.889
1.672
0.086
Table 5: Final logistic regression model, measuring log like-
lihood of a security bug being found in a team’s submission.
with each factor. Negative coeﬃcients indicate lower likeli-
hood of ﬁnding a security bug. For categorical factors, the
exponential of the coeﬃcient (Exp(coef)) indicates roughly
how strongly that factor being true aﬀects the likelihood
relative to the baseline category.12 For numeric factors, the
exponential indicates how the likelihood changes with each
unit change in that factor.
Fall 2015 implementations were 296× as likely as Spring
2015 implementations to have a discovered security bug.13
We hypothesize this is due to the increased security design
space in the ATM problem as compared to the gallery prob-
lem. Although it is easier to demonstrate a security error
in the gallery problem, the ATM problem allows for a much
more powerful adversary (the MITM) that can interact with
the implementation; breakers often took advantage of this
capability, as discussed in §5.
The model also shows that C/C++ implementations were
more likely to contain an identiﬁed security bug than either
static or dynamic implementations. For static languages,
this eﬀect is signiﬁcant and indicates that a C/C++ program
was about 8.5× (that is, 1/0.118) as likely to contain an
identiﬁed bug. This eﬀect is clear in Figure 5, which plots
the fraction of implementations that contain a security bug,
broken down by language type and contest problem. Of the
16 C/C++ submissions (see Figure 2), 12 of them had a
security bug: 5/9 for Spring 2015 and 7/7 for Fall 2015.
All 5 of the buggy implementations from Spring 2015 had a
crash defect, and this was the only security-related problem
for three of them; none of the Fall 2015 implementations had
crash defects.
If we reclassify crash defects as not security relevant and
rerun the model we ﬁnd that the impact due to language cat-
egory is no longer statistically signiﬁcant. This may indicate
that lack of memory safety is the main disadvantage to using
C/C++ from a security perspective, and thus a memory-safe
C/C++ could be of signiﬁcant value. Figure 6 shows how
many security bugs of each type (memory safety, integrity,
privacy) were found in each language category, across both
contests. This ﬁgure reports bugs before uniﬁcation dur-
ing the ﬁx-it phase, and is of course aﬀected by diﬀerences
among teams’ skills and language choices in the two contests,
but it provides a high-level perspective.
Our model shows that teams that knew more unique lan-
guages (even if they did not use those languages in their sub-
mission) performed slightly better, about 1.2× for each lan-
12In cases (such as the Fall 2015 contest) where the rate of security
bug discovery is close to 100%, the change in log likelihood starts
to approach inﬁnity, somewhat distorting this coeﬃcient upwards.
13This coeﬃcient is somewhat exaggerated (see prior footnote),
but the diﬀerence between contests is large and signiﬁcant.
Figure 5: The fraction of teams in whose submission a secu-
rity bug was found, for each contest and language category.
Figure 6: How many of each type of security bug were found,
across both contests, for each language category. Counts are
normalized by the number of qualiﬁed Build-it submissions
in each language category.
guage known. Additional LOC in an implementation were
also associated with a very small increase in the presence of
an identiﬁed security bug.
Finally, the model shows two factors that played a role
in the outcome, but not in a statistically signiﬁcant way:
using a dynamically typed language, and participating in
the MOOC. We see the eﬀect of the former in Figure 5. For
the latter, the eﬀect size is quite large; it is possible that the
MOOC security training played a role.
4.6 Breaking success
Now we turn our attention to break-it team performance,
i.e., how eﬀective teams were at ﬁnding defects in others’
submissions. First, we consider how and why teams per-
formed as indicated by their (normalized) break-it score
prior to the ﬁx-it phase. We do this to measure a team’s
raw output, ignoring whether other teams found the same
bug (which we cannot assess with conﬁdence due to the lack
of ﬁx-it phase participation per §4.5). This data set includes
108 teams that participated in the break-it phase in Spring
and Fall 2015. We also model which factors contributed
to security bug count, or how many total security bugs a
break-it team found. Doing this disregards a break-it team’s
eﬀort at ﬁnding correctness bugs.
Factor
Coef.
SE p-value
Fall 2015
# Team members
Knowledge of C
Coding experience
Build participant
-2406.89
430.01
-1591.02
99.24
1534.13
685.73 <0.001*
193.22
0.028*
0.117
1006.13
0.056
51.29
995.87
0.127
Table 6: Final linear regression model of teams’ break-it
scores, indicating how many points each selected factor adds
to the total score. Overall eﬀect size f 2 = 0.039.
We model both break-it score and security bug count us-
ing several of the same potential factors as discussed pre-
viously, but applied to the breaking team rather than the
building team. In particular, we include which contest they
participated in, whether they were MOOC participants, the
number of break-it Team members, average team-member
Coding experience, average team-member Knowledge
of C, and unique Languages known by the break-it team
members. We also add two new potential factors:
Build participant: Whether the breaking team also
qualiﬁed during the build-it phase.
Advanced techniques: Whether the breaking team re-
ported using software analysis or fuzzing to aid in bug ﬁnd-
ing. Teams that only used manual inspection and testing
are categorized as false. 26 break-it teams (24%) reported
using advanced techniques.
For these two initial models, our potential factors provide
eight degrees of freedom; again assuming power of 0.75, this
yields a prospective eﬀect size f 2 = 0.136, indicating we
could again expect to ﬁnd eﬀects of roughly medium size by
Cohen’s heuristic [7].
Break score. The model considering break-it score is given
in Table 6. It shows that teams with more members per-
formed better, with an average of 430 additional points per
team member. Auditing code for errors is an easily paral-
lelized task, so teams with more members could divide their
eﬀort and achieve better coverage. Recall that having more
team members did not help build-it teams (see Tables 3
and 5); this makes sense as development requires more co-
ordination, especially during the early stages.
The model also indicates that Spring 2015 teams per-
formed signiﬁcantly better than Fall 2015 teams. Figure 7
illustrates that correctness bugs, despite being worth fewer
points than security bugs, dominate overall break-it scores
for Spring 2015. In Fall 2015 the scores are more evenly dis-
tributed between correctness and security bugs. This out-
come is not surprising to us, as it was somewhat by design.
The Spring 2015 problem deﬁnes a rich command-line inter-
face with many opportunities for subtle errors that break-it
teams can target.
It also allowed a break-it team to sub-
mit up to 10 correctness bugs per build-it submission. To
nudge teams toward ﬁnding more security-relevant bugs, we
reduced the submission limit from 10 to 5, and designed the
Fall 2015 interface to be far simpler.
Interestingly, making use of advanced analysis techniques
did not factor into the ﬁnal model; i.e., such techniques did
not provide a meaningful advantage. This makes sense when
we consider that such techniques tend to ﬁnd generic errors
such as crashes, bounds violations, or null pointer derefer-
(a) Spring 2015
(b) Fall 2015
Figure 7: Scores of break-it teams prior to the ﬁx-it phase,
broken down by points from security and correctness bugs.
The ﬁnal score of the break-it team (after ﬁx-it phase) is
noted as a dot. Note the diﬀerent ranges in the y-axes; in
general, the Spring 2015 contest (secure log problem) had
higher scores for breaking.
Factor
Coef.
SE p-value
Fall 2015
# Team members
Build participant
3.847
1.218
5.430
1.486
0.417
2.116
0.011*
0.004*
0.012*
Table 7: Final linear regression modeling the count of se-
curity bugs found by each team. Coeﬃcients indicate how
many security bugs each factor adds to the count. Overall
eﬀect size f 2 = 0.035.
ences. Security violations for our problems are more seman-
tic, e.g., involving incorrect design or use of cryptography.
Many correctness bugs were non-generic too, e.g., involving
incorrect argument processing or mishandling of inconsistent
or incorrect inputs.
Being a build participant and having more coding expe-
rience is identiﬁed as a postive factor in the break-it score,
according to the model, but neither is statistically signiﬁ-
cant (though they are close to the threshold). Interestingly,
knowledge of C is identiﬁed as a strongly negative factor in
break-it score (though again, not statistically signiﬁcant).
Looking closely at the results, we ﬁnd that lack of C knowl-
edge is extremely uncommon, but that the handful of teams
in this category did unusually well. However, there are to
few of them for the result to be signiﬁcant.
Security bugs found. We next consider breaking success
as measured by the count of security bugs a breaking team
level cryptographic libraries with few “knobs” that allow for
incorrect usage[3].
One implementation of the ATM problem, written in Python,
made use of the SSL PKI infrastructure. The implementa-
tion used generated SSL private keys to establish a root of
trust that authenticated the atm program to the bank pro-
gram. Both the atm and bank required that the connection
be signed with the certiﬁcate generated at runtime. Both
the bank and the atm implemented their communication pro-
tocol as plain text then wrapped in HTTPS. This put the
contestant on good footing; to ﬁnd bugs in this system, other
contestants would need to break the security of OpenSSL.
Another implementation, also for the ATM problem, writ-
ten in Java, used the NaCl library. This library intentionally
provides a very high level API to “box” and “unbox” secret
values, freeing the user from dangerous choices. As above,
to break this system, other contestants would need to ﬁrst
break the security of NaCl.
An implementation of the log reader problem, also written
in Java, achieved success using a high level API. They used
the BouncyCastle library to construct a valid encrypt-then-
MAC scheme over the entire log ﬁle.
5.2 Failure Stories
The failure modes for build-it submissions are distributed