s
r
e
s
U
f
o
F
D
P
 18
 16
 14
 12
 10
 8
 6
 4
 2
 0
ratio = 0.03
 0.2
 0.4
 0.6
 0.8
 1
User’s ratio of active lifetime
Figure 15: The growth of user population
in our dataset over time.
Figure 16: # of whispers and replies by
new and old users per week.
Figure 17: User’s active lifetime over stay-
ing time in our dataset.
Then we further examine these pairs co-located in nearby areas
(i.e. distance  20%). However, as more and more users transition from
new to “existing users,” content generation by existing users does
not grow signiﬁcantly over time. This conﬁrms our intuition that a
certain portion of users are disengaging over time.
143Per-user Active Period.
Next, we focus on individual users
and examine how long users stay active before they disengage.
More speciﬁcally, we compute their active “lifetime” (timespan
between their ﬁrst and last posts) over their staying time in the
dataset (timespan between a user’s ﬁrst post and the last date of our
data collection). Given our focus on long-term activity, we exclude
users who just recently joined during the last the month of our data
collection. Thus for Figure 17, we only consider users who have
been in our dataset for at least one month (70.3% of all users).
Figure 17 shows the distribution of user’s ratio of active lifetime
(PDF). Users are clearly clustered into two extremes: one major
cluster around an extremely low ratio (0.03), representing those
who quickly turned inactive in 1 or 2 days after their ﬁrst post;
another major cluster around 1.00, representing users who remain
active for their entire time in the dataset (at least 1 month). Sim-
ilar patterns have also been observed in other user generated con-
tent (UGC) networks, such as blogs and Q&A services [17].
If
we set a threshold for active ratio at 0.03, these “try and leave”
users account for 30% of all users. This explains our observation
in Figure 16—because a signiﬁcant portion of users become inac-
tive quickly, the overall content posting rate remains stable despite
a signiﬁcant number of new users joining the network.
5.2 Predicting User Engagement
A key observation of the above analysis is that Whisper users
tend to fall into one of two behavioral extremes—either staying
active for a long time, or quickly turning inactive (Figure 17). The
bimodal nature of the distribution hints at the potential to classify
users into the two clusters.
Here, we experiment with machine learning (ML) classiﬁers to
determine if we can predict long term user engagement based on
their early behavior after their ﬁrst post (in our dataset). We seek to
answer three key questions: First, is this prediction even possible?
Second, what ML models produce the most accurate predictions?
Third, what early-day signals can most strongly indicate a user’s
intention to leave?
We take three steps to answer the above questions. First, we col-
lect a set of behavioral features based on users’ activities in their
ﬁrst X days on Whisper, ideally with a small value for X. Second,
we use these features to build different machine learning classiﬁers
to predict long term user engagement. Finally, we run feature se-
lection to determine the features that provide the best early signals
indicating which users might disengage.
Features. We explore multiple different classes of features (20
features in all) to proﬁle users’ behavior during their ﬁrst X days.
Out of these, we will select the most essential features.
• Content posting features (F1-F7). 7 features: user’s number
of total posts, number of whispers, number of replies, num-
ber of deleted whispers, and number of days with at least one
post/whisper/reply.
• Interaction features (F8-F15). 8 features: ratio of replies in
total posts, number of acquaintances, number of bi-directional
acquaintances, outgoing replies over all replies, maximum
number of interactions with the same user, ratio of whis-
pers with replies, and average number of replies and likes
per whisper.
• Temporal features (F16-F17): 2 features: average delay be-
fore ﬁrst reply to user’s whisper; average delay of user’s
replies to other users’ whispers.
• Activity trend (F18-F20): 3 features: we equally split each
user’s ﬁrst X days into three buckets and record the num-
ber of posts in each bucket (F irst, M iddle and Last). We
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
 100
 90
 80
 70
 60
 50
 100
 90
 80
 70
 60
 50
All Features
Top Features
1 day
3 days
7 days
Data from first x days
C
U
A
 1
 0.9
 0.8
 0.7
 0.6
 0.5
All Features
Top Features
1 day
3 days
7 days
Data from first x days
(a) Predicting Inactive vs. Active (RF)
All Features
Top Features
1 day
3 days
7 days
C
U
A
 1
 0.9
 0.8
 0.7
 0.6
 0.5
All Features
Top Features
1 day
3 days
7 days
Data from first x days
(b) Predicting Inactive vs. Active (SVM)
Data from first x days
Figure 18: Prediction result using Random Forests and SVM.
The model performance is evaluated by accuracy (left) and
Area under ROC curve (right) .
compute 2 features as M iddle
F irst . Finally, whether
the number of posts decreases monotonically across the three
buckets.
F irst and Last
Classiﬁer Experiments.
To build a training set for our classi-
ﬁers, we focus on users that have at least a month’s worth of activity
history in our dataset (730K users). We select a set of “short-term”
users who tried the app for 1-2 days and quickly disengaged (no
more posts). Using results from Figure 17, we randomly sample
50K users from those whose active lifetime ratio  0.03 to form the Active set.
Our goal is to classify the two sets of users solely based on users’
activities in their ﬁrst X days, and we use 1, 3 and 7 as values of X.
We build multiple machine learning classiﬁers including Random
Forests (RF), Support Vector Machine (SVM) and Bayes Network
(BN), using implementations of these algorithms in WEKA [19]
with default parameters. For each experiment, we run 10-fold cross
validation and report classiﬁcation accuracy and area under ROC
curve (AUC). Accuracy refers to the ratio of correctly predicted in-
stances over all instances. AUC is another widely used metric, with
higher AUC indicating stronger prediction power. For instance,
AUC > 0.5 means the prediction is better than random guessing.
The experiment results with Random Forests and SVM are shown
in Figure 18. The Bayesian results closely match those of SVM,
thus we omit them for brevity. We make two key observations.
First, behavioral features are effective in predicting future engage-
ment. The accuracy is high (75%) even when only using users’
ﬁrst-day data (RF). This conﬁrms that users’ early actions can act
as indicators of their future activity. If we include a week’s worth of
data, we can achieve accuracy up to 85%. Second, we ﬁnd different
classiﬁers achieve similar performance given 7 days of data. How-
ever, their results diverge when they are constrained to using less
data (e.g., 1-day). With less data, Random Forests produce more
accurate predictions than SVM and Bayesian networks.
Feature Selection.
Finally, we seek to identify the most power-
ful signals to predict a user’s long-term engagement. To ﬁnd the an-
swer, we perform feature selection on the 20 features. More speciﬁ-
144Rank
1
2
3
4
5
6
7
8
Observation Time Frame
1 day
Interact-F9 (0.15)
Interact-F11 (0.12)
Interact-F10 (0.11)
Interact-F12 (0.11)
Trend-F18 (0.05)
Interact-F15 (0.04)
Post-F1 (0.04)
Interact-F8 (0.04)
3 days
Post-F5 (0.27)
Trend-F19 (0.18)
Post-F6 (0.18)
Interact-F9 (0.16)
Post-F1 (0.16)
Post-F7 (0.13)
Interact-F15 (0.12)
Interact-F11 (0.12)
7 days
Post-F5 (0.46)
Post-F6 (0.31)
Trend-F19 (0.28)
Post-F1 (0.27)
Post-F7 (0.23)
Trend-F20 (0.21)
Interact-F15 (0.21)
Post-F2 (0.19)
Table 3: The top 8 feature and its categories ranked by infor-
mation gain (values shown in parentheses).
cally, we rank features based on Information Gain [18], which mea-
sures feature’s distinguishing power over the two classes of data.
We list the top 8 features in Table 3. As expected, prediction power
varies signiﬁcantly, and information gain drops off quickly (partic-
ularly for 1 day) after the top 4 features. To validate their prediction
power, we repeat each experiment with only their top 4 features.
The results in Figure 18 show that the top 4 features achieve most
of the accuracy of the entire classiﬁer, but with much less complex-
ity.
Then we take a closer look at the top features. First, we note that
the 1-day classiﬁer relies on different set of features compared with
3- and 7-day classiﬁers. The 1-day models rely heavily on interac-
tion features. Intuitively, the model predicts whether a user will
stay engaged based on how actively the user participates in social
interactions. If a user received many replies or actively replied to
others on her ﬁrst day, there’s a high chance for this user to stay
longer. For 3- and 7-day models, we ﬁnd that the key features shift
to user’s content posting and activity trend features. This means
once we monitor the users for a longer period, the user’s inten-
tion to stay or leave can be more accurately reﬂected in her posting
frequency and volume, and whether that activity is declining over
time.
Engaging Users with Notiﬁcations.
Stimulating user engage-
ment is a key goal for any new service. One tool Whisper has
already deployed is push notiﬁcations that deliver the “whisper of
the day” to users’ mobile device every evening between 7 and 9pm.
The exact notiﬁcation time varies each day and between Android
and iOS devices. To examine the impact of these notiﬁcations, we
conduct a small experiment. We monitor the notiﬁcation time on
5 different phones every day for 6 days. We look at user activity
in the Whisper stream for 5 minute and 10 minute intervals follow-
ing the notiﬁcations, and ﬁnd no statistically signiﬁcant increase in
new replies or whispers compared to other 5 or 10 minute windows
between 7 and 9pm. This means that while these notiﬁcations may
serve to engage users to read popular whispers, there is no signiﬁ-
cant increase in new whispers or replies as a result.
6. CONTENT MODERATION IN WHISPER
Anonymity facilitates free speech, but also inevitably fosters abu-
sive content and behavior [21, 35]. Like other anonymous commu-
nities, Whisper faces the same challenge of dealing with abusive
content (e.g., nudity, pornography or obscenity) in their network.
In addition to a crowdsourcing-based user reporting mechanism,
Whisper also has dedicated employees to moderate whispers [16].
Our basic measurements (§3.2) also suggest this has a signiﬁcant
impact on the system, as we observed a large volume of whispers
(>1.7 million) has been deleted during the 3 months of our study.
The ratio of Whisper’s deleted content (18%) is much higher than
traditional social networks like Twitter (<4%) [1, 30].
Topic
Sexting (36)
Selﬁe (7)
Chat (7)
Topic
Emotion (17)
Religion (10)
Entertain. (8)
Top 50 Keywords Most Related to Deleted Whispers
sext, wood, naughty, kinky, sexting, bj,
threesome,
dirty, role, fwb, panties, vibrator, bi, inches, lesbians,
hookup, hairy, nipples, freaky, boobs, fantasy, fantasies,
dare, trade, oral, takers, sugar, strings, experiment, cu-