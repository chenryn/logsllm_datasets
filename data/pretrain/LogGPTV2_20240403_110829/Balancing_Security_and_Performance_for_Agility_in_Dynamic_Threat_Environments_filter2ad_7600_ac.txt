k
7
6
5
4
3
2
1
0
0
0.1
0.2
0.3
0.4
0.5
P
FP
0.6
0.7
0.8
0.9
1
Fig. 2. Scaling of the number of hash functions (k) with the probability of
false positives (PF P ) for a bloom ﬁlter under optimal assumptions.
Using 221 as the representatative system output size, we
model the operation of the sliding window as proceeding byte-
by-byte (i.e., sliding forward by two hexadecimal characters
on each window advance through the data), and neglecting
the ﬁnal 7 bytes, as a valid system address must be made up
from 8 bytes. This calculation gives 104 Bloom ﬁlter queries
per system output inspected. Further, we assume our system is
instantiated on a workstation with time duration per operation
of 100 ns, a reasonable performance number for a standard PC
workstation.
We then calculate Li,s, the latency cost of searching a
single system output i for the presence of memory disclosure
as,
Li,s = k ∗ Z ∗ F,
(19)
with k = 4 the number of hash functions used in the Bloom
ﬁlter, which gives the scaling performance of the Bloom ﬁlter
612
algorithm, Z = 100 ns/op the time duration per operation
on the hypothetical system, and F = 100 queries/output the
characteristic number of search queries that must be performed
per system output payload inspected to scan the entire data
payload for memory disclosures. Together these values give
Li,s = 0.04 ms, holding PF P = 0.1 throughout the remainder
of this study unless otherwise noted.
To explore the sensitivity of model outputs to the value
of the learning rate (α) and and discount factor (γ) we
performed a factorial sweep over possible values for α and γ,
the output of which is displayed in Fig. 3 in terms of simulated
system security. We observe from Fig. 3 that for α values
less than approximately 0.4, the value of the discount factor
(γ) has minimal effect on overall security score obtained by
the defender. As the learning rate (α) is increased beyond 0.4
the discount factor becomes increasingly important in overall
system performance, with γ values between approximately 0.6
and 0.8 partially compensating for suboptimal tendencies in
learned defender policy caused by a high α value. We adopt
the values of α = 0.2 and γ = 0.2 throughout the remainder
of this study.
1
0.9
0.8
0.7
0.6
0.3
0.5
0 . 1
0 . 2
0.3
0.5
0.4
0.7
0 .6
0.8
0 . 1
0 .2
0.4
α
0.5
0.7
0.4
0.3
0.2
0.1
0
0
0
.
8
0.7
0.6
0.6
0.8
0.8
0.2
0
.
9
0.7
0.6
0.4
γ
0.3
0.5
0.4
0.6
0.7
0.8
0
.
4
0
.
5
6
.
0
0
.
7
0.8
0
.
2
0
.
3
0.4
0
.
5
0
.
6
7
.
0
0.8
0.9
0.7
0.6
0 . 8
0.6
0.8
1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
System security, (Eqt. 6) as a function of Q-learning parameters α
Fig. 3.
and γ. Color indicates the level of security achieved for a given value of α
and γ, with redder colors indicating a higher level of security achieved. The
mapping of colors to numerical results is given to the right of the ﬁgure.
Metrics quantifying the performance of the defender’s
learned policy, including security and performance, are reset
periodically to facilitate a more accurate and timely char-
acterization of the policy’s current performance against the
dynamic attacker. We select our chosen window size based on
an analysis of the effect window size has on the stability of
system effectiveness metrics across simulation runs.
Figure 4 shows the standard deviation of the mean security
score recorded on the ﬁnal step of 100 replicate simulation
runs. From these and other similar results we select a window
size of 200 simulation steps for the resetting of system
effectiveness metrics.
Table I summarizes the model parameter settings used
throughout the simulation studies detailed in the next section.
613
0.35
0.3
0.25
0.2
0.15
0.1
0.05
]
y
t
i
r
u
c
e
s
[
n
o
i
t
a
i
v
e
D
d
r
a
d
n
a
t
S
0
0
500
1500
Window Size [system output count]
1000
2000
Fig. 4. Standard deviation of security as a function of metric window size. The
displayed results are averages taken over 100 replicate runs of the simulation
with a (randomized) attack strength of 0.50.
TABLE I.
SUMMARY OF BASELINE MODEL PARAMETER VALUES
EMPLOYED THROUGHOUT THIS STUDY UNLESS OTHERWISE INDICATED IN
THE TEXT.
Parameter
W
PF N
Li,m
Li,s
α
γ
D
C
Meaning
Metric window duration
Probability of false positives
Latency cost of mitigation
Latency cost of output search
Q-learning learning rate
Q-learning discount factor
Simulation duration (steps)
Exploration decay constant
Value
200 model steps
0.1
101 ms
0.04 ms
0.20
0.20
2000
2000
B. Results
The purpose of our studies here are to begin to characterize
the behavior of the proposed Q-learning architecture and
provide preliminary guidance for users as to desirable model
parameter values in various contexts. Due to space constraints,
a complete characterization of model behavior is beyond the
scope of the current work, but will be a topic that is addressed
in future works.
that
We characterize the behavior of our Q-learning architecture
when facing an attacker that causes memory disclosures uni-
formly at random at a ﬁxed overall rate. A given attack strategy
is characterized by a single parameter, Ap,
takes on
values between 0 and 1 and characterizes the probability that a
memory disclosure will be contained within any given system
output. For efﬁciency, a single system output is produced on
each simulation step. Simulations are run to 2000 steps, simu-
lating 2000 system outputs for which the defender must decide
to inspect or pass. Each attacker parameterization faces the
defender in 100 repeated encounters (i.e., 100 replicate runs) of
2000 simulation steps each. Summary statistics characterizing
defender success are collected and aggregated over each of the
replicate runs.
Figure 5 shows the ﬁnal security (blue) and performance
(red) scores averaged over 100 replicate runs for a defender
facing the attacker described above with Ap = 0.50. The x-
axis in Fig. 5 gives the value of the normalized beneﬁt term,
BN (deﬁned in Eqt. 18), parameterizing the adaptive defense.
For small beneﬁt values the system obtains high performance
scores and low security scores. On the other hand, large beneﬁt
values lead to high security scores by the defender and low
performance scores.
0.07
y
t
i
r
u
c
e
S
1.00
0.75
l
e
u
a
V
0.50
0.25
0.00
0
f
o
0.06
n
o
i
t
i
0.05
a
v
e
D
d
r
a
d
n
a
S
t
0.04
0
1
Normalized Benefit