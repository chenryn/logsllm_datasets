like the one served on mss.ca. This indicates that the ScanBox tool
has been extensively customized by the APT actors to work on dif-
ferent targets.
Once the reconnaissance is done, information collected is passed
Figure 2: Similarities in reconnaissance scripts over functionalities (upper matrices),
and software tools (lower matrices) across different watering holes through three user
agents, Internet Explorer (1), Chrome (2), Firefox (3). Tmpit is used here to indicate
scripts found in online templates. SiteB is a submission of one of ScanBox’s recon-
naissance scripts on VirusTotal [63] which does not have the corresponding watering
hole.
on to the C&C centers through GET and POST requests (Table 9).
We found such C&C domains include legitimate domains (e.g. ntxeye.
com), malicious domains (e.g. foundationssl.com) and static IP ad-
dresses. DDNS was also used in some ScanBox attacks reported by
URLQuery, a free web scanning tool. Further, we found ScanBox
encrypts some URL arguments and periodically probe the C&C
with status update requests when they are being visited.
Of particular interest here is mss.ca, which was found to be com-
promised as early as late 2014, according to URLQuery. Our dy-
namic crawler started crawling the website in Nov,2014 using uni-
versity campus IP addresses. The monitoring failed to report any
suspicious activities. However, after moving our dynamic crawler
to a supercomputing center in San Diego, CA, we were surprised to
ﬁnd ScanBox reconnaissance code in the HTTP trafﬁc. This could
indicate that the actors behind the attack might be targeting industry
users.
Live inﬁltration. gokbayrak.com in our dataset was also reported
to be a watering hole by a blog [30], which was presumably com-
promised with a ScanBox framework. In our research, we further
performed live monitoring on the domain and found that it rotated
its redirections over a number of suspicious third-party domains.
!!!!!(1)!!!!!!!!!!!(2)!!!!!!!!!!!!(3)!One of them, theguardian.com.tw, was available for purchase and
we bought it anonymously in an effort to sink-hole the domain.
Speciﬁcally, we hosted the domain on the Amazon cloud using an
Apache web server to log all its trafﬁc, from June 26 to August
16th, 2015.
By analyzing the collected trafﬁc logs and checking for request
referrals, we discovered two more unreported watering holes: ibsahq.
org for the International Buddhism Sangha Association, and HNN.
hkfor a Chinese news agency. These two sites turn out to have sim-
ilar infections as gokbayrak.com, but redirect their visitors to other
malicious domains, as indicated by the IoCs in Table 5.
Altogether, the sinkhole collected HTTP trafﬁc for 3 months
from around 7K unique IP addresses (i.e. victims), mostly from
Turkey, Taiwan and USA.
4.3 Recap
Our research shows that the APT actors are extremely active in
the arena of politically oriented websites, repeatedly compromis-
ing those sites and utilizing all kinds of techniques (e.g., cookie)
to track down the visitors and recover their identities (e.g., through
JSONP vulnerabilities). A weakness of such a politically minded
attack, however, is its targeting at a more generic audience, i.e.,
anyone who visits the website, and therefore can be less stealthy
than the industry speciﬁc attacks, in which the watering hole may
unleash its attack code only toward the individuals from a certain
organization. Therefore, a web scanner tuned to the unique fea-
tures of this type of websites could lead to new discoveries and
raise the bar for this type of attacks. In the meantime, our research
shows that the target sites are often less protected, compared with
the industry sites. Enhancing their owners’ security awareness and
getting help from professionals will certain make the attacks less
likely to happen.
When it comes to attacks aiming at industry targets, the use of
legitimate intermediaries becomes an interesting feature. As men-
tioned earlier, we consider this trick to be an evasion technique,
particularly in a corporate environment where a redirection to an
unknown external domain could cause a red ﬂag to be raised. How-
ever, what can still be found here could be an unusual relation
(e.g., redirection) between two unrelated, though both legitimate
domains visited by an organization’s employees. Such a relation, if
rarely observed before, could become sufﬁciently unusual to war-
rant a close look at the domains involved. Again, new technologies
leveraging such observations should be further investigated in the
follow up research.
5. RELATED WORK
Understanding web site compromise. In terms of the risk factors
associated with web site compromise, a recent study showed that
sites using certain content management systems (e.g., WordPress
or Joomla) or running particular web servers are at higher risk [61].
Regarding attackers’ objectives for compromising legitimate sites,
the study by Moore et al. [49] reveals attackers’ strategy of using
search engines to hunt vulnerable sites and compromise them for
later use in Phishing campaigns. The work by John et al. [36] and
Leontiadis et al. [41] elaborate on the abuse of compromised sites
for boosting the search rankings of malicious sites owned by attack-
ers. The attackers’ behavior after compromising legitimate sites is
also thoroughly studied by Canali et al. [26]. Interestingly, it has
been shown that many web hosting providers are not responding in
a timely manner to compromise [27] and thus attackers can lever-
age a compromised site for a long time for criminal activities.
Outlier detection. To redirect visitors to malicious sites or directly
drop malware on visitors’ machines, the attacker has to manipulate
the web content delivered to the site visitors, introducing inevitable
changes. Our system identiﬁes such changes and highlights the
ones that represent outliers with respect to the observed historical
distribution of the web site structure. Different techniques can be
used for this purpose [20, 25, 50]. We use a simple outlier detec-
tion method based on probabilistic models and conﬁdence intervals
applied to the site’s rate-of-change that proves to be effective in
capturing the types of site changes we are interested in.
Pre-ﬁltering systems. EvilSeed [35] is a pre-ﬁlter that works by
generating search queries for identifying other web pages similar
to the known malicious ones. Eyeson, however, does not need a
seed of malicious pages and is able to ﬁnd unknown ones. Addi-
tionally, Eyeson outperforms Evilseed in toxicity, later discussed in
Appendix A. Prophiler [28] and Delta [24] are content-based pre-
ﬁltering systems that look into a combination of static features or
changes in page structures to determine the pages owned or com-
promised by attackers. These approaches are more heavyweight
and can be evaded if page content is obfuscated. On the contrary,
Eyeson is built to proﬁle the evolution of a target by only looking
at lightweight features from its HTTP headers, which enables us to
inspect targets at a large scale.
Advanced Persistent Threats. APTs are well-funded and care-
fully orchestrated targeted campaigns posing serious risks to vari-
ous commercial and governmental organizations, naturally attract-
ing attention from both the security industry and academic com-
munity. The existing work mainly focuses on dissecting APT cam-
paigns [46, 39, 42, 60]. In addition, several mechanisms have been
developed to assess the threat of targeted malware [33] and link
different attacks [40]. The campaigns investigated by these stud-
ies leverage spear-phishing emails to inﬁltrate the victim organi-
zations, while our study looks into another venue becoming more
popular among malicious actors, watering hole attacks.
6. CONCLUSION
Our work contributes towards the understanding and mitigation
of an emerging infection vector, strategic website site compromise,
increasingly used for delivering malware in initial stages of a tar-
geted campaign. By analyzing over 5 years of data from archive.org
and carefully labeling ground truth using public sources, we dis-
covered 17 watering holes never reported before, including a high
impact politically minded attack, and shed new light on APT ac-
tors’ motivations, strategies and techniques. Looking forward, we
believe that our new ﬁndings will inspire the follow up research
on this emerging type of targeted attacks. Further study is also ex-
pected to enhance our methodology Eyeson, exploring the potential
of running it as a pre-ﬁltering system for organizations under the
APT threat.
Acknowledgements
We thank our reviewers for their insightful comments. This work
was supported in part by National Science Foundation under grant
CNS-1223477, CNS-1223495, CNS-1527141 and CNS-1408874.
Part of the work was done during Sumayah Alrwais’s internship
at RSA. We thank Kent Backman from RSA FirstWatch and Todd
Leetham from EMC CIRT team for their generous help in provid-
ing the list of compromised sites and investigations. We are grateful
to Ronald L. Rivest, Kevin Bowers and Robin Norris for their feed-
back and suggestions. We also thank Xiaorui Pan for his help with
the investigations and analysis of watering holes. Any opinions,
ﬁndings, conclusions or recommendations expressed in this paper
same time. For example, an attacker exploiting the vulnerability
of Java plugin can deliver the payload through previously seen ﬁle
name or URL pattern, but a new content type jar will be inevitably
observed from the trafﬁc and the attack will be detected by Eyeson.
However, as the proﬁle is based solely on the HTTP header, the
system will only detect a compromise if it leads to changes in the
HTTP requests. For example, if the malicious payload is injected
into an existing website resource such as a home page, the change
to the page will not be captured. But when that change triggers
another new HTTP request to an uncommon destination, it will be
detected. Even though for almost all the cases we are aware of,
a compromised website does bring in observable changes to the
visitor’s HTTP requests, we acknowledge that an attack could be
carefully designed to avoid any signiﬁcant change to requests. One
such example is that the legitimate Adobe Flash ﬁle owned by the
targeted site is tampered to include drive-by-download code.
We argue that this type of compromise is inelastic for the at-
tacker’s operation and more likely to expose attacker’s traces, since
the attacker has to keep presence on the compromised web host
if he wants to adjust the payload or pause the attack, which fre-
quently happens during the attack campaign. Besides, the design
and implementation of Eyeson can be extended to foil such type
of attack. The HTTP responses from the visited site can be pro-
ﬁled separately using features like ﬁle size and the anomalies can
be identiﬁed through the same change point analysis.
In a nutshell. Eyeson’s preliminary evaluation on the archive HTTP
trafﬁc shows that the technique is accurate enough to serve a pre-
ﬁltering system, though the validation on the whole dataset was too
complicated to yield a conclusion. It is important to point out here
that the design of Eyeson makes it very suitable for operating under
today’s corporate environment. This is because for the subject of
a targeted attack, which is typically a large organization, the trafﬁc
its network produces is simply too large to be collected and ana-
lyzed efﬁciently. For example, the company we are working with
has on average 120K hosts visiting at least 600K external domains
every day; logging even part of the HTTP headers generated by
the visits takes 662 gigabytes of storage space daily. As a result,
only a small amount of information for each visit can be logged.
Eyeson was designed to take advantage of a common organization
network product (namely a web proxy system) without requiring
additional data to be collected. It can perform persistent monitor-
ing of a large number of strategic websites using only partial HTTP
header information such as URL, referrer, content type and cookie.
The outputs of the system can be delivered to the incident response
team for further evaluation. For this purpose, further research is ex-
pected to understand its effectiveness in real corporate settings and
enhance the technique with new APT features, including the ones
learnt from our study.
B. ARCHIVE DATA
To monitor the changes that happen to a website over a long pe-
riod of time, we leveraged archive.org, a system that implements a
dynamic crawler to crawl a list of URLs intermittently and main-
tains the snapshot for each visited URL. To visit an archived snap-
shot on archive.org, one can render the corresponding archive URL
in a browser which in turns renders the archived visit and all of its
embedded archived URLs at the time the snapshot was captured by
the archive.
For example, to visit a snapshot of forbes.com captured on the
28th Nov, 2014, one can browse the URL http://web.archive.org/
web/20141128132335/http://www.forbes.com/ which we refer to
as an archive URL. Table 12 shows a sample of the generated
Figure 3: FP and FN rates at different proﬁle sizes and conﬁdences.
do not necessarily reﬂect the views of the NSF.
Appendices
A. EYESON 2.0
The system we developed, in its current state, only served as
a measurement methodology in our research, which helped us go
through a large amount of HTTP trafﬁc to quickly focus on a small
set of highly likely watering hole cases. However, Eyeson does
have the potential to be deployed in a corporate environment as a
pre-ﬁltering mechanism, after proper improvements and organiza-
tion performance evaluations as discussed below.
Evaluation. Using ground truth collected and described in Section
3.2, we bootstrapped the proﬁle for each monitored URL with 10
visits (after initial 5 visits, the follow-up 5 for collecting change
rates), and ran Eyeson over all the snapshots gathered in the order
of their dates with conﬁdences ranging from 90%, increasing by
1%, until 99% and found that the false positive ranged from 19.7%
to 19.3% while the false negatives stayed the same at zero as shown
in Figure 3.
Running Eyeson on the larger set of collected HTTP trafﬁc with
a 95% conﬁdence interval, as discussed earlier in Section 3.3, re-
sulted in alerting 56.3K monitored URLs ,shown in Table 10. Upon
validating results with our post ﬁltering process through blacklists,
clustering and manual analysis, described in Section 3.3, we were
able to conﬁrm the compromise of 8.2% of those alerted monitored
URLs.
Furthermore, we evaluate our validated results using a toxicity
metric indicating the fraction of malicious alerts out of all alerts
for each type of alert generated. Over a 5 year period of collected
URL visits, Eyeson had a toxicity level of 4.1% of the total alerted
visits. To evaluate the potential of Eyeson as a preﬁlter we compare
its toxicity to that of Evilseed [35],a preﬁlter that generates search
queries to ﬁnd malicious pages using a set of labeled malicious
pages as seed. As shown in Table 10, we ﬁnd that Eyeson outper-
forms EvilSeed in terms of toxicity where Evilseed has a toxicity
level of 1.12% in malicious domains found vs 3.4% by Eyeson. It
is worth noting here the data sets collected by Eyeson and Evil seed
span different observation periods where Evilseed covers 25 days
and Eyeson 5 years but still has a much higher toxicity levels.
Evasion. The set of proﬁle features capture the anomalies in HTTP
requests when a site is compromised and visited by the user. Though
an attacker can manipulate the trafﬁc to make one feature look le-
gitimate, it is quite difﬁcult for her to maneuver all features at the
Visits
Changes
Type
SnapShots
Monitored URLs
Monitored FQDNs
Generalized URLs
All FQDNs
External FQDNs
Static IPs
# Proﬁling Alerts
1.7M
56.3K
35K
2.7M
48.7K
17.6K
456
#Validated Alerts
69.8K
4.6K
3.2K
53.2K
1.6K