In terms of the high anomaly amplitude experiment
(aanomaly = 0.158), BasisDetect declares 90% fewer false
alarms compared with the Distributed Spatial methodology
and almost 99% fewer false alarms than the PCA approach,
in order to ﬁnd all the injected anomalies.
The second synthetic experiment tested anomaly detec-
tion performance on 30 network snapshots for a single
anomaly injected into each snapshot, with each having vary-
ing anomaly amplitude levels ranging from aanomaly =
0.0316 to aanomaly = 1.
In Figure 8 we see the results
of detecting injected anomalies across all 24 test network
snapshots with anomalies of various amplitude (holding out
6 of the network snapshots as training data for BasisDe-
tect). Again, our BasisDetect methodology has uniformly
better performance than both the Distributed Spatial and
PCA anomaly detection methodologies. Selected results
461BasisDetect
BasisDetect w/o residual
BasisDetect w/o penalty
Table 4: Synthetic Traﬃc Matrices for Varying
Anomaly Amplitude - Number of false alarms de-
clared for a percentage of the true anomalies de-
tected.
4
x 10
5
4
3
2
1
l
l
s
m
r
a
A
e
s
a
F
d
e
i
f
i
s
s
a
C
l
f
o
r
e
b
m
u
N
Methodology
PCA
Spatial
BasisDetect
Percentage of True Anomalies Found
70% 80% 90%
4,266
7,325
2,365
577
43
1,673
100%
13,966
7,977
2,716
5,874
924
166
Table 5: Abilene Network Data - Number of false
alarms declared for a percentage of the PCA anoma-
lies detected.
Methodology
Spatial
BasisDetect
Percentage of PCA Anomalies Found
70% 80% 90%
1,287
563
327
747
100%
4,564
2,746
733
495
ments examined network data with 1,008 time-series sam-
ples, we examined data with 16,384 and 24,576 time-series
samples for the ﬁrst and second synthetic experiments re-
spectively. While the increase in the experiment size results
in a signiﬁcantly greater number of absolute false alarms
declared, we feel that the larger experiments oﬀer a greater
understanding as to the performance characteristics of the
three anomaly detection methodologies.
8.5 Abilene Real-World Network Data
Finally, we test the performance of the BasisDetect frame-
work on the Abilene real-world dataset. As mentioned in
Section 2, there is no ground truth labeling of anomalies
for this dataset.
Instead, here we will use one of the two
prior network-wide anomaly detection methodologies (PCA)
to classify the most obvious anomalies for that methodol-
ogy. Using these classiﬁed anomalies as the ground truth,
we compare how the new BasisDetect methodology per-
forms in comparison with the other competing network-wide
anomaly detection methodology (e.g., the Distributed Spa-
tial methodology).
Using the PCA network-wide anomaly detection tech-
nique, we classify the 15 most dominant anomalies in the
Abilene data. The performance of both the BasisDetect
framework and the Distributed Spatial approach can be
seen in Figure 9 on detecting these PCA classiﬁed anoma-
lies using 5-way Cross Validation (thus expanding the to-
tal anomalies considered to 70 in the dataset). As seen in
the ﬁgure, our BasisDetect methodology is detecting these
PCA classiﬁed anomalies with lower false alarm rate than
the Distributed Spatial approach, with almost 40% fewer
false alarms declared to detect all the PCA anomalies. A
speciﬁc breakdown of the false alarm rate can be seen in
Table 5.
0
0
50
100
150
200
250
300
350
Number of Detected Anomalies
Figure 5: Tuning parameter performance experi-
ment, examination of how well the BasisDetect algo-
rithm performs as each of the tuning parameters are
removed. Using the full BasisDetect algorithm (γ, ρ
learned from training set), BasisDetect w/o residual
(γ learned from training set, ρ = 0), and BasisDetect
w/o penalty (ρ learned from training set, γ = 0)
highlighted in Table 4 show that with respect to the best
competing methodology (the Distributed Spatial method-
ology), our BasisDetect algorithm declares over 65% fewer
false alarms in order to detect all of the true anomalies. In
comparison with the PCA algorithm, we ﬁnd over 80% fewer
false alarms in order to discover all of the true anomalies.
l
s
m
r
a
A
e
s
a
F
d
e
l
i
f
i
s
s
a
C
l
f
o
r
e
b
m
u
N
14000
12000
10000
8000
6000
4000
2000
0
0
BasisDetect
Spatial
PCA
40
10
Number of Detected Anomalies
20
30
50
Figure 8: Synthetic Traﬃc Matrices with Varying
Anomaly Amplitude - False Alarms declared for a
speciﬁed level of true anomaly detection for the
three network-wide detection methodologies (PCA,
Distributed Spatial, BasisDetect).
One surprising result of both synthetic experiments is the
very large number of false alarms declared in contrast with
prior published results in [1, 2]. Note that our experiments
consider a signiﬁcantly larger dataset. While prior experi-
9. CONCLUSIONS AND FUTURE WORK
The ability to detect anomalies accurately and in a timely
fashion in large networks would be a signiﬁcant beneﬁt in
day to day network operations. It can be argued that cur-
462l
d
e
r
a
c
e
D
s
m
r
a
A
e
s
a
F
l
l
f
o
r
e
b
m
u
N
5000
4000
3000
2000
1000
0
0
BasisDetect
Spatial
10
20
30
40
50
60
70
Number of PCA Anomalies Detected
Figure 9: Abilene Real-World Network Data - Using
15 anomalies found by the PCA methodology, the
false alarm rates are displayed for both BasisDetect
and the Distributed Spatial methodology.
rent methods are not yet suﬃciently capable based on the
fact that they are not widely used today. The objective of
our work is to develop an anomaly detection capability that
is suﬃciently accurate to be considered practical for opera-
tional deployment and use.
In this paper we present the BasisDetect framework, a
model-based methodology for anomaly detection. We build
temporal models by applying a novel basis pursuit algorithm
to the key components of learned anomalies from a small
training set. These temporal models are extended from
anomaly detection on a single node to the network-wide con-
text by applying a higher reasoning framework. This com-
bined approach has additional beneﬁts, such as the ability to
be applied ﬂexibly to a wide variety of data, extensibility to
include a variety of ﬁlter functions, and low computational
complexity.
We test and evaluate the BasisDetect methodology us-
ing both empirical and synthetic network data with labeled
anomalies. In the single node case, when we compare with
standard time-series based methods, our method identiﬁes
all of the labeled anomalies with over 50% fewer false alarms
declared compared with the competing methodologies.
In
the case of unlabeled real-world network wide data, we show
considerable improvements in detecting anomalies declared
by previous network-wide anomaly detection methodologies.
Finally, we use synthetic traces to examine in detail the sen-
sitivity of our method over a range of anomalies to show
that to ﬁnd all of the labeled anomalies, our methodology
will declare 65% fewer false anomalies than the best com-
peting methodology. The results show that the model-based
method is highly eﬀective even when large amounts of noise
are present.
The implication of our results is that our model-based
methodology is indeed feasible for event detection in an op-
erational environment. While we show that models from a
small set of ﬁlters can be eﬀective, we have not investigated
optimizations of the ﬁlters that could enhance their abil-
ity. Furthermore, there are many practical issues revolving
around where and how data is gathered in a network that
must be systematically addressed to facilitate application of
our methods. Ultimately, the best test of an anomaly detec-
tor is in a live environment. In the future, we plan to work
closely with several network operations groups to deploy and
test our techniques.
10. ACKNOWLEDGMENTS
This work was supported in part by NSF grants CNS-
0716460, CNS-0831427 and CNS-0905186. Also support was
provided by ARC Grant DP0665427. Any opinions, ﬁndings,
conclusions or other recommendations expressed in this ma-
terial are those of the authors and do not necessarily reﬂect
the view of the NSF or ARC. We would also AT&T, Tech-
nicolor, Abilene and G´EANT networks for providing data.
11. REFERENCES
[1] A. Lakhina, M. Crovella, and C. Diot, “Diagnoising
Network-Wide Traﬃc Anomalies,” in Proceedings of
ACM SIGCOMM Conference, Portland, OR, August
2004.
[2] P. Chhabra, C. Scott, E. Kolaczyk, and M. Crovella,
“Distributed Spatial Anomaly Detection,” in
Proceedings of IEEE INFOCOM Conference, Phoenix,
AZ, March 2008.
[3] A. Lakhina, M. Crovella, and C. Diot,
“Characterization of Network-Wide Anomalies in
Traﬃc Flows,” in Proceedings of ACM SIGCOMM
Internet Measurement Conference, Taormina, Sicily,
Italy, October 2004.
[4] A. Lakhina, K. Papagiannaki, M. Crovella, C. Diot,
E. D. Kolaczyk, and N. Taft, “Structural Analysis of
Network Traﬃc Flows,” in ACM SIGMETRICS /
Performance, 2004.
[5] P. Barford, J. Kline, D. Plonka, and A. Ron, “A Signal
Analysis of Network Traﬃc Anomalies,” in Proceedings
of ACM SIGCOMM Internet Measurement Workshop,
Marseilles, France, November 2002.
[6] Y. Zhang, Z. Ge, M. Roughan, and A. Greenberg,
“Network Anomography,” in Proceedings of the
Internet Measurement Conference, Berkeley, CA,
USA, October 2005.
[7] M. Roughan, T. Griﬃn, M. Mao, A. Greenberg, and
B. Freeman, “IP forwarding anomalies and improving
their detection using multiple data sources,” in ACM
SIGCOMM Workshop on Network Troubleshooting
(NetTS), Portland, OR, September 2004, pp. 307–312.
[8] S. H. Steiner, “Grouped Data Exponentially Weighted
Moving Average Control Charts,” Applied Statistics,
vol. 47, no. 2, 1998.
[9] H. Ringberg, A. Soule, J. Rexford, and C. Diot,
“Sensitivity of PCA for Traﬃc Anomaly Detection,” in
Proceedings of ACM SIGMETRICS Conference, San
Diego, CA, June 2007.
[10] B. Rubinstein, B. Nelson, L. Huang, A. Joseph,
S. Lau, S. Rao, N. Taft, and J. Tygar, “ANTIDOTE:
Understanding and Defending Against Poisoning of
Anomaly Detectors,” in Proceedings of ACM
SIGCOMM Internet Measurements Conference,
Chicago, Illinois, November 2009.
[11] Y. Benjamini and Y. Hochberg, “Controlling the False
Discovery Rate,” in Journal of the Royal Statistical
Society B, vol. 57, no. 1, 1995, pp. 289–300.
463[12] R. Miller, in Simultaneous Statistical Inference.
[19] H. Ringberg, M. Roughan, and J. Rexford, “The Need
Springer-Verlag, 1991.
[13] L. Huang, X. Nguyen, M. Garofalakis, J. Hellerstein,
M. Jordan, M.Joseph, and N.Taft.,
“Communication-Eﬃcient Online Detection of
Network-Wide Anomalies,” in Proceedings of IEEE
INFOCOM Conference, Anchorage, Alaska, May 2007.
[14] Y. Liu, L. Zhang, and Y. Guan, “A Distributed Data
Streaming Algorithm for Network-Wide Traﬃc
Anomaly Detection,” SIGMETRICS Performance
Evaluation Review, vol. 37, no. 2, pp. 81–82, 2009.
[15] C. Scott and E. Kolaczyk, “Nonparametric Assessment
of Contamination in Multivariate Data using
Generalized Quantile Sets and FDR,” in Accepted for
Publication in J. Computational and Graphical
Statistics, 2007.
[16] S. Chen, D. Donoho, and M. Saunders, “Atomic
Decomposition by Basis Pursuit,” in SIAM Journal of
Scientiﬁc Computing, vol. 20, 1998, pp. 33–61.
For Simulation In Evaluating Anomaly Detectors,”
ACM SIGCOMM CCR, vol. 38, no. 1, pp. 55–59,
January 2008.
[20] Y. Zhang, M. Roughan, W. Willinger, and L. Qui,
“Spatio-Temporal Compressive Sensing and Internet
Traﬃc Matrices,” in Proceedings of ACM SIGCOMM
Conference, Barcellona, Spain, August 2009, pp.
267–278.
[21] “Geant Project Website,” http://www.geant.net/.
[22] A. Markopoulou, G. Iannaccone, S. Bhattacharrya,
C.-N. Chuah, and C. Diot, “Characterization of
failures in an IP backbone,” in Proceedings of IEEE
INFOCOM Conference, Hong Kong, China, March
2004.
[23] D. Oppenheimer, A. Ganapathi, and D. A. Patterson,
“Why do Internet services fail, and what can be done
about it?” in 4th Usenix Symposium on Internet
Technologies and Systems (USITS’03), 2003.
[17] G. Davis, S. Mallat, and M. Avellaneda, “Greedy
[24] L. Wasserman, “All of Nonparametric Statistics,” in
Adaptive Approximation,” in Journal of Constructive
Approximation, vol. 13, 1997, pp. 57–98.
[18] P. Huggins and S. Zucker, “Greedy Basis Pursuit,” in
IEEE Transactions on Signal Processing, vol. 55,
no. 7, July 2007, pp. 3760–3771.
Springer Texts, 2006.
464