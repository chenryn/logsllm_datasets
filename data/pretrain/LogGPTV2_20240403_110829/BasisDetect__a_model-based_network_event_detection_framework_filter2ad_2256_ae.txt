### High Anomaly Amplitude Experiment

In the high anomaly amplitude experiment (with an anomaly amplitude of 0.158), BasisDetect declared 90% fewer false alarms compared to the Distributed Spatial methodology and almost 99% fewer false alarms than the PCA approach, while still detecting all the injected anomalies.

### Second Synthetic Experiment

The second synthetic experiment evaluated the anomaly detection performance on 30 network snapshots, with a single anomaly injected into each snapshot. The anomaly amplitudes ranged from 0.0316 to 1. Figure 8 presents the results of detecting injected anomalies across all 24 test network snapshots (with 6 snapshots held out as training data for BasisDetect). Once again, BasisDetect outperformed both the Distributed Spatial and PCA methodologies in terms of false alarm rates.

### Synthetic Traffic Matrices for Varying Anomaly Amplitude

Table 4 shows the number of false alarms declared for different percentages of true anomalies detected using synthetic traffic matrices with varying anomaly amplitudes.

| Methodology | Percentage of True Anomalies Found | Number of False Alarms |
|-------------|------------------------------------|------------------------|
| PCA         | 70%                                | 4,266                  |
|             | 80%                                | 7,325                  |
|             | 90%                                | 10,000                 |
|             | 100%                               | 13,966                 |
| Spatial     | 70%                                | 577                    |
|             | 80%                                | 924                    |
|             | 90%                                | 1,673                  |
|             | 100%                               | 7,977                  |
| BasisDetect | 70%                                | 2,365                  |
|             | 80%                                | 43                     |
|             | 90%                                | 1,673                  |
|             | 100%                               | 2,716                  |

### Abilene Network Data

Table 5 provides the number of false alarms declared for different percentages of PCA-detected anomalies in the Abilene network data.

| Methodology | Percentage of PCA Anomalies Found | Number of False Alarms |
|-------------|------------------------------------|------------------------|
| Spatial     | 70%                                | 1,287                  |
|             | 80%                                | 747                    |
|             | 90%                                | 327                    |
|             | 100%                               | 4,564                  |
| BasisDetect | 70%                                | 563                    |
|             | 80%                                | 2,746                  |
|             | 90%                                | 733                    |
|             | 100%                               | 495                    |

### Large-Scale Experiments

In our experiments, we examined network data with 1,008 time-series samples for the first synthetic experiment and 16,384 and 24,576 time-series samples for the second synthetic experiment. While the larger dataset size led to a significantly greater number of absolute false alarms, it also provided a more comprehensive understanding of the performance characteristics of the three anomaly detection methodologies.

### Abilene Real-World Network Data

Finally, we tested the performance of the BasisDetect framework on the Abilene real-world dataset. Since there is no ground truth labeling of anomalies for this dataset, we used the PCA methodology to classify the 15 most dominant anomalies. Using these classified anomalies as the ground truth, we compared the performance of BasisDetect with the Distributed Spatial methodology. Figure 9 shows that BasisDetect detected these PCA-classified anomalies with a lower false alarm rate, declaring almost 40% fewer false alarms than the Distributed Spatial approach.

### Conclusions and Future Work

Accurate and timely anomaly detection in large networks is crucial for day-to-day network operations. Current methods are not yet sufficiently capable for widespread use. Our work aims to develop an anomaly detection capability that is accurate enough for practical operational deployment.

In this paper, we present the BasisDetect framework, a model-based methodology for anomaly detection. We build temporal models by applying a novel basis pursuit algorithm to the key components of learned anomalies from a small training set. These models are extended to the network-wide context using a higher reasoning framework, offering flexibility, extensibility, and low computational complexity.

We tested and evaluated BasisDetect using both empirical and synthetic network data with labeled anomalies. In the single-node case, BasisDetect identified all labeled anomalies with over 50% fewer false alarms compared to competing methodologies. For unlabeled real-world network-wide data, BasisDetect showed significant improvements in detecting anomalies. Synthetic traces further demonstrated that BasisDetect declares 65% fewer false alarms than the best competing methodology to find all labeled anomalies, even in the presence of large amounts of noise.

Our results indicate that the model-based methodology is feasible for event detection in an operational environment. Future work will involve optimizing filters and addressing practical issues related to data gathering in networks. We plan to collaborate with several network operations groups to deploy and test our techniques in a live environment.

### Acknowledgments

This work was supported in part by NSF grants CNS-0716460, CNS-0831427, and CNS-0905186, and ARC Grant DP0665427. We thank AT&T, Technicolor, Abilene, and GÉANT networks for providing data.

### References

[1] A. Lakhina, M. Crovella, and C. Diot, “Diagnosing Network-Wide Traffic Anomalies,” in Proceedings of ACM SIGCOMM Conference, Portland, OR, August 2004.

[2] P. Chhabra, C. Scott, E. Kolaczyk, and M. Crovella, “Distributed Spatial Anomaly Detection,” in Proceedings of IEEE INFOCOM Conference, Phoenix, AZ, March 2008.

[3] A. Lakhina, M. Crovella, and C. Diot, “Characterization of Network-Wide Anomalies in Traffic Flows,” in Proceedings of ACM SIGCOMM Internet Measurement Conference, Taormina, Sicily, Italy, October 2004.

[4] A. Lakhina, K. Papagiannaki, M. Crovella, C. Diot, E. D. Kolaczyk, and N. Taft, “Structural Analysis of Network Traffic Flows,” in ACM SIGMETRICS / Performance, 2004.

[5] P. Barford, J. Kline, D. Plonka, and A. Ron, “A Signal Analysis of Network Traffic Anomalies,” in Proceedings of ACM SIGCOMM Internet Measurement Workshop, Marseilles, France, November 2002.

[6] Y. Zhang, Z. Ge, M. Roughan, and A. Greenberg, “Network Anomography,” in Proceedings of the Internet Measurement Conference, Berkeley, CA, USA, October 2005.

[7] M. Roughan, T. Griffin, M. Mao, A. Greenberg, and B. Freeman, “IP Forwarding Anomalies and Improving Their Detection Using Multiple Data Sources,” in ACM SIGCOMM Workshop on Network Troubleshooting (NetTS), Portland, OR, September 2004, pp. 307–312.

[8] S. H. Steiner, “Grouped Data Exponentially Weighted Moving Average Control Charts,” Applied Statistics, vol. 47, no. 2, 1998.

[9] H. Ringberg, A. Soule, J. Rexford, and C. Diot, “Sensitivity of PCA for Traffic Anomaly Detection,” in Proceedings of ACM SIGMETRICS Conference, San Diego, CA, June 2007.

[10] B. Rubinstein, B. Nelson, L. Huang, A. Joseph, S. Lau, S. Rao, N. Taft, and J. Tygar, “ANTIDOTE: Understanding and Defending Against Poisoning of Anomaly Detectors,” in Proceedings of ACM SIGCOMM Internet Measurements Conference, Chicago, Illinois, November 2009.

[11] Y. Benjamini and Y. Hochberg, “Controlling the False Discovery Rate,” in Journal of the Royal Statistical Society B, vol. 57, no. 1, 1995, pp. 289–300.

[12] R. Miller, in Simultaneous Statistical Inference, Springer-Verlag, 1991.

[13] L. Huang, X. Nguyen, M. Garofalakis, J. Hellerstein, M. Jordan, M. Joseph, and N. Taft, “Communication-Efficient Online Detection of Network-Wide Anomalies,” in Proceedings of IEEE INFOCOM Conference, Anchorage, Alaska, May 2007.

[14] Y. Liu, L. Zhang, and Y. Guan, “A Distributed Data Streaming Algorithm for Network-Wide Traffic Anomaly Detection,” SIGMETRICS Performance Evaluation Review, vol. 37, no. 2, pp. 81–82, 2009.

[15] C. Scott and E. Kolaczyk, “Nonparametric Assessment of Contamination in Multivariate Data Using Generalized Quantile Sets and FDR,” in Accepted for Publication in J. Computational and Graphical Statistics, 2007.

[16] S. Chen, D. Donoho, and M. Saunders, “Atomic Decomposition by Basis Pursuit,” in SIAM Journal of Scientific Computing, vol. 20, 1998, pp. 33–61.

[17] G. Davis, S. Mallat, and M. Avellaneda, “Greedy Adaptive Approximation,” in Journal of Constructive Approximation, vol. 13, 1997, pp. 57–98.

[18] P. Huggins and S. Zucker, “Greedy Basis Pursuit,” in IEEE Transactions on Signal Processing, vol. 55, no. 7, July 2007, pp. 3760–3771.

[19] H. Ringberg, M. Roughan, and J. Rexford, “The Need for Simulation in Evaluating Anomaly Detectors,” ACM SIGCOMM CCR, vol. 38, no. 1, pp. 55–59, January 2008.

[20] Y. Zhang, M. Roughan, W. Willinger, and L. Qui, “Spatio-Temporal Compressive Sensing and Internet Traffic Matrices,” in Proceedings of ACM SIGCOMM Conference, Barcelona, Spain, August 2009, pp. 267–278.

[21] “GÉANT Project Website,” http://www.geant.net/.

[22] A. Markopoulou, G. Iannaccone, S. Bhattacharrya, C.-N. Chuah, and C. Diot, “Characterization of Failures in an IP Backbone,” in Proceedings of IEEE INFOCOM Conference, Hong Kong, China, March 2004.

[23] D. Oppenheimer, A. Ganapathi, and D. A. Patterson, “Why Do Internet Services Fail, and What Can Be Done About It?” in 4th Usenix Symposium on Internet Technologies and Systems (USITS’03), 2003.

[24] L. Wasserman, “All of Nonparametric Statistics,” in Springer Texts, 2006.