 2  4  8  16 32 64
99.9%
99.99%
99.999%
 2  4  8  16 32 64
 2  4  8  16 32 64
 2  4  8  16 32 64
Number of threads
Figure 7: Latency distribution percentiles (lower is better).
action size increases, the probability of having conﬂicting
158
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:27 UTC from IEEE Xplore.  Restrictions apply. 
transactions increases. ESTM and TinySTM perform well
when contention is very low, i.e., a single threaded execution
or a few swaps per transaction. However, for large transac-
tions, as contention increases, their performance drops while
our OneFile performs well, particularly OneFile wait-free
because it aggregates multiple operations, thus reducing the
average number of writes (DCAS) per transaction.
We also include a variant of the SPS microbenchmark
which allocates memory. Every entry in the array points to a
small object containing two TMType. A swap
of two entries in this array allocates a new object, installs the
new pointer, and de-allocates the previous object. Results are
shown in Fig. 3. The OneFile algorithms perform well when
memory allocation is involved. In contrast, performance
of both TinySTM and ESTM reduces, making OneFile
favorable even when contention is lower than the SPS
microbenchmark without allocations.
In Fig. 4 we depict results for the queue data structure:
linked-list based (left side) and array-based (right hand side).
We executed 108 pairs of enqueues and dequeues for each
data point. OneFile performs better than both ESTM and
TinySTM for the linked list based, but slower than ad-hoc
algorithms. Still, OneFile allows for linearizable traversals
of the queue, is signiﬁcantly simpler to design, maintain,
customize, and use, making it a reasonable choice in many
practical cases.
The following plots compare different set data structures,
with the update ratio being shown at
the top of each
graph. An update operation is composed of two consecutive
transactions, a removal followed by an insertion, whereas
a read operation is composed of two consecutive read-only
transactions, each executes a search for an existing random
key.
Fig. 5 shows linked list sets under six different update
ratio workloads. In the 100% update workload (top left-
most plot), the OneFile beat the other STMs and the Harris
lock-free implementations for the 2 threads and 4 threads
scenarios. As the number of threads increases, the Harris
lists are capable of doing more concurrent insertion and
removal operations, allowing them to scale, giving them
advantage over all the others. As the ratio of read-only
operations increases, the advantage of OneFile extends up
to a larger number of threads. Due to not having a read-
set, the OneFile STMs excel in the read-mostly workloads
where they can match or surpass the hand-made lock-free
linked lists [54].
With a tree with 104 keys (Fig. 6), OneFile has several
scenarios where it surpasses the lock-free tree [56], namely
with low thread count or in over-subscription.
Fig. 7 shows six different percentiles of the latency dis-
tribution, when running on a high-core count CPU, namely,
a dual-socket 2.10 GHz Intel Xeon E5-2683 (“Broadwell”)
with a total of 32 hyper-threaded cores (64 HW threads). The
plots are in log-log scale, with the vertical axis representing
the time in microseconds it takes to complete a transaction.
In this microbenchmark there is an array of 64 counters
where each transaction increments all of the counters, al-
ternating between incrementing the counters starting from
left to right, and on the next transaction incrementing right
to left. This workload implies a strong serialization of
the transactions and causes most STMs to have starvation
effects. For example, on the 90% percentile plot, for 2
threads, with the wait-free OneFile 90% of the transactions
take 9 microseconds or less to complete, while with ESTM
take 243 microseconds or less to complete, and with Tiny
STM take 848 microseconds or less to complete. The wait-
free OneFile STM has a signiﬁcant advantage at the tail of
the distribution, having at least 100× lower (better) latency
from the 99.9% percentile onwards, as soon two or more
threads attempts to execute update transactions, reaching a
peak improvement of 1,000× over ESTM and 10,000× over
TinySTM. These results highlight the importance of wait-
freedom for tail latency, a relevant characteristic for network
operating systems and other soft real-time applications.
B. Evaluation of NVM techniques
For fairness and practicality reasons, we did not evaluate
PTMs that require specialized hardware [10], nor did we
consider PTMs that do not provide durable linearizable
transactions [61], [62]. Comparisons were made with PMDK
(libpmemobj++), RomulusLog and RomulusLR, described
in §II. NVM was emulated by allocating a ﬁle in the
/dev/shm directory that is mounted on DRAM.
We executed the persistent variant of the SPS [11],
[58]–[60] benchmark mentioned previously. This benchmark
allows us to understand the performance proﬁle of the PTMs
under different transaction sizes. Each swap exchanges the
values of two randomly selected entries of an array of one
million 64-bit integers, hence modifying two memory words
in PM. As shown in Fig. 8, the lock-free OneFile-PTM
surpasses the other PTMs for large transactions with over-
subscription.
We implemented three persistent sets using the above
PTMs, namely, a singly linked list set, a red-black tree set,
and a hash set, shown in Fig. 9, 10 and 11 respectively.
The throughput of the OneFile PTMs is penalized due to
the load interposition. On a singly linked list data structure,
most of the time is spent traversing the list, imposing a
call to the load() interposed method for every traversed
node. On RomulusLog and PMDK this is implemented as
a regular load, while on RomulusLR it contains a check
for which memory region to traverse and adjust the regular
load accordingly, both approaches being extremely fast. For
the OneFile PTMs the load interposition is signiﬁcantly
more complex, requiring two acquire-loads and possibly a
lookup on the write-set, affecting the overall throughput.
Despite this disadvantage, OneFile is capable of beating
RomulusLog and RomulusLR for the majority of the runs
159
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:27 UTC from IEEE Xplore.  Restrictions apply. 
Legend for non-volatile memory (all graphs of §V.B)
OF-LF
OF-WF
RomL
RomLR
PMDK
Persistent SPS integer swap
1 thread
 14
 12
 10
 8
 6
 4
 2
 0
 1  2  4  8  32  128
2 threads
4 threads
 1  2  4  8  32  128
 1  2  4  8  32  128
16 threads
32 threads
64 threads
10
8
6
4
2
0
 1  2  4  8  32  128
Number of swaps per transaction
Figure 8: Persistent SPS integer microbenchmark.
Persistent linked list sets with 103 keys
100%
50%
11
10%
33
)
s
/
6
0
1
×
(
s
n
o
i
t
a
r
e
p
O
3
2
1
0
 1  2  4  8  32  128
 1  2  4  8  32  128
Figure 10: Persistent red-black tree sets with 106 keys.
)
s
/
6
0
1
×
(
s
p
a
w
S
)
s
/
6
0
1
×
(
s
p
a
w
S
)
s
/
6
0
1
×
(
s
n
o
i
t
a
r
e
p
O
)
s
/
6
0
1
×
(
s
n
o
i
t
a
r
e
p
O
0.6
0.4
0.2
0
4
3
2
1
0
 4  16  32  48  64
 4  16  32  48  64
 4  16  32  48  64
1%
0.1%
88
6060
0%
 4  16  32  48  64
 4  16  32  48  64
Number of threads
 4  16  32  48  64
Figure 9: Persistent linked list-based sets with 103 keys.
in the 1% and 0.1% workloads, due to the non-blocking
progress. The OneFile PTMs overtake PMDK on nearly all
workloads.
On update transactions, due to the overhead on redo-log
techniques added by the lookup in the log, traversing less
nodes implies a smaller overhead. On a balanced tree ﬁlled
sequentially with one million keys, seen in Fig. 10, the
number of traversed nodes is ∼20, with the OneFile PTMs
having high enough throughput to overtake PMDK in all
scenarios. The OneFile PTMs match and slightly exceed
the Romulus variants for the 0% update workload, and
surpass Romulus in over-subscription for the 0.1% workload.
This happens because in the Romulus variants, in over-
subscription, a thread holding the lock may be preempted,
blocking progress for all other threads, which does not
happen in OneFile due to the lock-free progress.
0.3
)
s
/
6
0
1
×
(
s
n
o
i
t
a
r
e
p
O
0.2
0.1
0
)
s
/
6
0
1
×
(
s
n
o
i
t
a
r
e
p
O
10
5
0
Persistent red-black tree sets with 106 keys
100%
50%
10%
1.41.4
 4  16  32  48  64
 4  16  32  48  64
 4  16  32  48  64
1%
0.1%
2525
3838
0%
 4  16  32  48  64
 4  16  32  48  64
Number of threads
 4  16  32  48  64
Persistent resizable hash table sets with 103 keys
10%
100%
50%
1414
 4  16  32  48  64
 4  16  32  48  64
 4  16  32  48  64
1%
0.1%
200200
850850
0%
 4  16  32  48  64
 4  16  32  48  64
Number of threads
 4  16  32  48  64
Figure 11: Persistent hash table sets with 103 keys.
6
)
s
/
0
1
×
(
s
n
o
i
t
a
r
e
p
O
125
100
75
50
25
0
Linked list-based queue
 4
) FHMP
s
/
6
0
1
×
(
s
n
o
i
t
a
r
e
p
O
 3
 2
 1
 0
 1
 4