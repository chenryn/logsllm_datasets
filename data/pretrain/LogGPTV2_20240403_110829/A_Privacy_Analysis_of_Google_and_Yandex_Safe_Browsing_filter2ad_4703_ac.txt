that share one decomposition and hence one common preﬁx,
while the other common preﬁx is due to the collision on
truncated digests. Finally, the last type of collisions (Type III)
appears when completely unrelated URLs generate the same
preﬁxes. The latter may occur again due to collisions on the
truncated digests. In the following, by a Type I URL, we mean
a URL that generates a Type I collision with a given URL. We
similarly deﬁne Type II and Type III URLs for a given URL.
TABLE V: An example with different possible collisions.
Target URL
URL
a.b.c
Type I
g.a.b.c
Type II
Type III
g.b.c
d.e.f
Decomposition
a.b.c/
b.c/
g.a.b.c/
a.b.c/
b.c/
g.b.c/
b.c/
d.e.f/
e.f/
Preﬁxes
A
B
C
A
B
A
B
A
B
Coll. Type
of URLs hosted on the domain. In general, the smaller is
the number of decompositions per URL,
the lower is the
probability that Type I and Type II URLs exist. Moreover,
a Type II URL exists only if the number of decompositions
on a domain is larger than 232. We later show that no Type
II URL exists by empirically estimating the distribution of
decompositions over domains. As a result, the ambiguity in
the re-identiﬁcation can only arise due to Type I collisions. In
the following, we discuss the problem of URL re-identiﬁcation
with a focus on URLs that admit Type I collisions.
B. URL Re-identiﬁcation
We note that a target URL with few decompositions has
a very low probability to yield Type I collisions, and hence
it can be easily re-identiﬁed. In case of URLs with large
number of decompositions, the server would require more than
2 preﬁxes per URL to remove the ambiguity. Nevertheless,
the SB provider can still determine the common sub-domain
visited by the client using only 2 preﬁxes. This information
may often sufﬁce to identify suspicious behavior when the
domain in question pertains to speciﬁc traits such as pedophilia
or terrorism. It is pertinent to highlight that the SB service pro-
vided by WOT collects the domains visited by its clients [26].
Hence, in the scenario when GSB and YSB servers receive
multiple preﬁxes for a URL, the privacy achieved is the same
as that ensured by services such as WOT.
Now, let us further analyze the problem of re-identifying
URLs for which Type I collisions occur. To this end, we
consider an illustrative example of a domain b.c that hosts
a URL a.b.c/1 and its decompositions (see Table VI). We
assume that these are the only URLs on the domain. The URL
generates four decompositions. Two of these decompositions
include the domain name ’a’ as a sub-domain while the
remaining two do not. These decompositions yield preﬁxes
say A, B, C and D respectively. We note that the considered
URL is only a slightly simpliﬁed form of the most general
URL, where the query part of the URL has been removed [14].
Therefore, it largely represents all the canonicalized URLs.
TABLE VI: A sample URL on b.c with its 4 decompositions.
URL
a.b.c/1
Decompositions
a.b.c/1
a.b.c/
b.c/1
b.c/
Preﬁx
A
B
C
D
In order to illustrate the different possible collisions, we
present a set of examples in Table V. We assume that the client
visits the target URL a.b.c and hence the server receives the
corresponding two preﬁxes, say A and B. The server using
these preﬁxes must determine the exact URL visited by the
client. The next 3 URLs exemplify the different collisions.
Clearly, P[Type I] > P[Type II] > P[Type III], where
P[X] denotes the probability of an event X. Under the
uniformity assumption of hash functions, a Type III collision
264 . We note that for
is highly unlikely, with a probability of
Type I and Type II collisions to occur, the URLs must share
at least one common decomposition. The probability of these
collisions hence depends on the number of decompositions
1
We analyze the following three cases depending on the
preﬁxes sent by the client to the SB server:
• Case 1. (A, B) generate hits: If the server receives these
preﬁxes, it can be sure that the client has visited the URL
that corresponds to the ﬁrst preﬁx A, i.e, a.b.c/1. The
probability that the re-identiﬁcation fails is P[Type III] =
. This holds because we assume that the domain b.c
1
264
hosts only 4 URLs, hence the probability that
the re-
identiﬁcation fails is the same as the probability of ﬁnding
a Type III URL for preﬁxes (A, B).
• Case 2. (C, D) generate hits: In this case, the possible
URLs that the client could have visited are: a.b.c/1,
352
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:43:08 UTC from IEEE Xplore.  Restrictions apply. 
a.b.c/ or b.c/1. These URLs correspond to preﬁxes
A, B and C respectively. Hence, in order to remove the
ambiguity and re-identify the exact URL visited by the
client, the SB provider would include additional preﬁxes
in the local database. If it includes the preﬁx A, in addition
to C and D, then it can learn whether the client visited the
URL a.b.c/1 or b.c/1. More precisely, if the client
visits a.b.c/1 then preﬁxes A, C and D will be sent to
the server, while if the client visits b.c/1, then only C and
D will be sent. Similarly, in order to distinguish whether
the client visits a.b.c/ or b.c/, the SB provider would
additionally include the preﬁx B.
• Case 3. One of {A, B} × {C, D} generates hits: If the
preﬁx A creates a hit, then the visited URL is a.b.c/1,
while if the preﬁx B creates a hit then the client has either
visited a.b.c/1 or a.b.c/. As in the previous case,
the re-identiﬁcation requires the SB provider to include an
additional preﬁx A in the preﬁx database.
As a general rule, all the decompositions that appear before
the ﬁrst preﬁx are possible candidates for re-identiﬁcation.
Consequently, lower-level domain names and URL paths can
be re-identiﬁed with a higher certainty than the ones at a higher
level. To this end, we consider the case of leaf URLs on a
domain. We call a URL on a given domain a leaf, if it does not
belong to the set of decompositions of any other URL hosted
on the domain. A leaf URL can also be identiﬁed as a leaf
node in the domain hierarchy (see Fig. 4). Type I collisions for
these URLs can be easily eliminated during re-identiﬁcation
with the help of only two preﬁxes. The ﬁrst preﬁx corresponds
to that of the URL itself, while the other one may arbitrarily
correspond to any of its decompositions. In the example of
Table VI, the URL a.b.c/1 is a leaf URL on the domain
b.c, and hence it can be re-identiﬁed using preﬁxes (A, B).
a.b.c
a.b.c/1
a.b.c/2
b.c
a.b.c/3
d.b.c
a.b.c/3/3.1
a.b.c/3/3.2
Fig. 4: A sample domain hierarchy. Blue nodes are leaf URLs.
Clearly, only non-leaf URLs contribute to Type I collisions.
Hence, in order to re-identify non-leaf nodes, one must include
more than 2 preﬁxes per node. Our observations further
raise the question of the distribution of decompositions over
domains that we explore in the following section.
C. Statistics on Decompositions
In the following, we support our analysis of the previous
section using extensive experiments to estimate the distribution
of URLs, decompositions and collisions over domains.
Our experiments have been performed on the web corpus
provided by Common Crawl [6]. Common Crawl is an open
repository of web crawl data collected over the last 7 years.
It contains raw web page data, metadata and text extractions.
For our experiments, we have used the most recent corpus of
April 2015. This crawl archive is over 168 TB in size and
holds more than 2.11 billion web pages. We note that the
data included in Common Crawl is not exact. For instance,
the maximum number of URLs hosted on a domain is of
the order of 105. However, there are several domains such
as wikipedia.org which host over a billion URLs. This peak is
due to the fact that crawlers do not systematically collect more
pages per site than this bound due to limitations imposed by the
server. One may also ﬁnd small-sized domains for which the
crawl archive does not include all web page data, in particular
all the URLs on the domain. Despite this bias, the corpus
allows us to obtain a global estimate of the size of the web
and determine the distribution of URLs and decompositions.
It is worth noticing that popular domains often host more
URLs than the non-popular ones. This generally implies
that the number of unique URL decompositions and even-
tual collisions on popular domains are larger than those on
random/non-popular ones. We hence consider two datasets
in our experiments. Our ﬁrst dataset contains web pages on
the 1 million most popular domains of Alexa [27]. We also
collected 1 million random domains from Common crawl
and then recovered web pages hosted on these domains. This
forms our second dataset. The number of URLs and the total
number of decompositions provided by these datasets is given
in Table VII. Our dataset on popular domains contains around
1.2 billion URLs, while the one on random domains includes
over 427 million URLs. URLs in the Alexa dataset yield
around 1.4 billion unique decompositions in total, while the
random dataset generates around 1 billion decompositions.
TABLE VII: Our datasets.
Dataset
Alexa
Random
#Domains
1,000,000
1,000,000
#URLs
1,164,781,417
427,675,207
#Decompositions
1,398,540,752
1,020,641,929
Fig. 5a presents the number of URLs hosted on the domains
belonging to these two datasets. Clearly, the Alexa domains
host larger number of URLs than the random domains. The
most number of URLs hosted by a domain from either of
the datasets is around 2.7 × 105. We also note that around
61% of the domains in the random dataset are single page
domains. Fig. 5b presents the cumulative fraction of URLs for
the two datasets. Our results show that for the Alexa dataset,
only 19,000 domains cover 80% of all the URLs while, for the
random dataset, only 10,000 domains span the same percentage
of URLs. These results give strong empirical evidence (due to
a large and random dataset) to previous results by Huberman
and Adamic [28] that demonstrate that the number of web
pages per site is distributed according to a power law. This
implies that on a log-log scale the number of pages per site
should fall on a straight line. For the random dataset, we ﬁt a
power-law distribution of the form:
where, xmin = 1 and the parameter α is estimated as ˆα:
(cid:8)
x
xmin
(cid:9)−α
(cid:16)−1
,
p(x) =
α − 1
xmin
(cid:14)
n(cid:15)
ˆα = 1 + n
= 1.312,
ln
xi
xmin
i=1
353
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:43:08 UTC from IEEE Xplore.  Restrictions apply. 
106
105
104
103
102
101
100
t
s
o
h
e
h
t
n
o
s
L
R
U
f
o
r
e
b
m
u
N
s
n
o
i
t
i
s
o
p
m
o
c
e
d
#
25
20
15
10
5
0
n
o
i
t
c
a
r
f
L
R
U
e
v
i
t
a
l
u
m
u
C
1
0.8
0.6
0.4
0.2
0
107
106
105
104
103
102
101
100
t
s
o
h
e
h
t
n
o
.
p
m
o
c
e
d
.
q
i
n
U
#
Alexa hosts
Random hosts
Alexa hosts
Random hosts
Alexa hosts
Random hosts
100
101
102
103
Hosts
104
105
106
100
101
102
103
Hosts
104
105