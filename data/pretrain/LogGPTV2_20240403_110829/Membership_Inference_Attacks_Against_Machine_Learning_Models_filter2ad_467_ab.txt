{i}
records (x{i}, y{i}
)target. A data record x
target is the input to
{i}
the model, and y
target is the true label that can take values
from a set of classes of size ctarget. The output of the target
model is a probability vector of size ctarget. The elements of
this vector are in [0, 1] and sum up to 1.
Let fattack() be the attack model. Its input xattack is com-
posed of a correctly labeled record and a prediction vector
of size ctarget. Since the goal of the attack is decisional
membership inference, the attack model is a binary classiﬁer
with two output classes, “in” and “out.”
},
Figure 1 illustrates our end-to-end attack process. For a
labeled record (x, y), we use the target model to compute
the prediction vector y = ftarget(x). The distribution of y
(classiﬁcation conﬁdence values) depends heavily on the true
class of x. This is why we pass the true label y of x in
addition to the model’s prediction vector y to the attack
model. Given how the probabilities in y are distributed around
y,
the attack model computes the membership probability
Pr{(x, y) ∈ Dtrain
the probability that ((x, y), y)
belongs to the “in” class or, equivalently, that x is in the
training dataset of ftarget().
The main challenge is how to train the attack model to
distinguish members from non-members of the target model’s
training dataset when the attacker has no information about the
internal parameters of the target model and only limited query
access to it through the public API. To solve this conundrum,
we developed a shadow training technique that lets us train
the attack model on proxy targets for which we do know the
training dataset and can thus perform supervised training.
i.e.,
target
B. Shadow models
The attacker creates k shadow models f i
shadow(). Each
shadow model i is trained on a dataset Dtrain
shadow i of the same
format as and distributed similarly to the target model’s train-
ing dataset. These shadow training datasets can be generated
using one of methods described in Section V-C. We assume
that the datasets used for training the shadow models are
disjoint from the private dataset used to train the target model
(∀i, Dtrain
target = ∅). This is the worst case for the
attacker; the attack will perform even better if the training
datasets happen to overlap.
shadow i ∩ Dtrain
The shadow models must be trained in a similar way to
the target model. This is easy if the target’s training algorithm
6
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
c
(cid:2) initialize a record randomly
y ← ftarget(x)
if yc ≥ y∗
c then
(cid:2) query the target model
(cid:2) accept the record
if rand()  conf min and c = arg max(y) then
x ← RANDRECORD(.)
← 0
y∗
j ← 0
k ← kmax
for iteration = 1··· itermax do
Algorithm 1 Data synthesis using the target model
1: procedure SYNTHESIZE(class : c)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
end for
25:
return ⊥
26:
27: end procedure
end if
end if
x∗ ← x
← yc
y∗
j ← 0
j ← j + 1
if j > rejmax then (cid:2) many consecutive rejects
end if
x ← RANDRECORD(x∗, k) (cid:2) randomize k features
k ← max(kmin,(cid:8)k/2(cid:9))
j ← 0
c
else
end if
(cid:2) failed to synthesize
(e.g., neural networks, SVM, logistic regression) and model
structure (e.g., the wiring of a neural network) are known.
Machine learning as a service is more challenging. Here the
type and structure of the target model are not known, but
the attacker can use exactly the same service (e.g., Google
Prediction API) to train the shadow model as was used to
train the target model—see Figure 2.
The more shadow models, the more accurate the attack
model will be. As described in Section V-D, the attack model
is trained to recognize differences in shadow models’ behavior
when these models operate on inputs from their own training
datasets versus inputs they did not encounter during training.
Therefore, more shadow models provide more training fodder
for the attack model.
C. Generating training data for shadow models
To train shadow models, the attacker needs training data
that is distributed similarly to the target model’s training data.
We developed several methods for generating such data.
Model-based synthesis.
If the attacker does not have real
training data nor any statistics about its distribution, he can
generate synthetic training data for the shadow models using
the target model itself. The intuition is that records that are
classiﬁed by the target model with high conﬁdence should
be statistically similar to the target’s training dataset and thus
provide good fodder for shadow models.
The synthesis process runs in two phases: (1) search, using
a hill-climbing algorithm, the space of possible data records
to ﬁnd inputs that are classiﬁed by the target model with high
conﬁdence; (2) sample synthetic data from these records. After
this process synthesizes a record, the attacker can repeat it until
the training dataset for shadow models is full.
See Algorithm 1 for the pseudocode of our synthesis
procedure. First, ﬁx class c for which the attacker wants to
generate synthetic data. The ﬁrst phase is an iterative process.
Start by randomly initializing a data record x. Assuming that
the attacker knows only the syntactic format of data records,
sample the value for each feature uniformly at random from
among all possible values of that feature. In each iteration,
propose a new record. A proposed record is accepted only
if it increases the hill-climbing objective: the probability of
being classiﬁed by the target model as class c.
Each iteration involves proposing a new candidate record by
changing k randomly selected features of the latest accepted
record x∗. This is done by ﬂipping binary features or resam-
pling new values for features of other types. We initialize k to
kmax and divide it by 2 when rejmax subsequent proposals
are rejected. This controls the diameter of search around the
accepted record in order to propose a new record. We set the
minimum value of k to kmin. This controls the speed of the
search for new records with a potentially higher classiﬁcation
probability yc.
The second, sampling phase starts when the target model’s
probability yc that the proposed data record is classiﬁed as
belonging to class c is larger than the probabilities for all
other classes and also larger than a threshold conf min. This
ensures that the predicted label for the record is c, and that the
target model is sufﬁciently conﬁdent in its label prediction. We
select such record for the synthetic dataset with probability y∗
and, if selection fails, repeat until a record is selected.
c
This synthesis procedure works only if the adversary can
efﬁciently explore the space of possible inputs and discover
inputs that are classiﬁed by the target model with high conﬁ-
dence. For example, it may not work if the inputs are high-
resolution images and the target model performs a complex
image classiﬁcation task.
Statistics-based synthesis. The attacker may have some statis-
tical information about the population from which the target
model’s training data was drawn. For example, the attacker
may have prior knowledge of the marginal distributions of
different features. In our experiments, we generate synthetic
training records for the shadow models by independently
sampling the value of each feature from its own marginal
distribution. The resulting attack models are very effective.
Noisy real data. The attacker may have access to some data
that is similar to the target model’s training data and can be
considered as a “noisy” version thereof. In our experiments
with location datasets, we simulate this by ﬂipping the (bi-
nary) values of 10% or 20% randomly selected features, then
7
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
(data record, class label)
predict(data)
(prediction, class label, “in” / “out”)
Shadow Training Set 1
Shadow Model 1
“in” Prediction Set 1
Shadow Test Set 1
“out” Prediction Set 1
·
·
·
·
·
·
·
·
·
Shadow Training Set k
Shadow Model k
“in” Prediction Set k
train()
Shadow Test Set k
“out” Prediction Set k
Attack Training Set
Attack Model
Fig. 3: Training the attack model on the inputs and outputs of the shadow models. For all records in the training dataset of a shadow model,
we query the model and obtain the output. These output vectors are labeled “in” and added to the attack model’s training dataset. We also
query the shadow model with a test dataset disjoint from its training dataset. The outputs on this set are labeled “out” and also added to the
attack model’s training dataset. Having constructed a dataset that reﬂects the black-box behavior of the shadow models on their training and
test datasets, we train a collection of ctarget attack models, one per each output class of the target model.
training our shadow models on the resulting noisy dataset.
This scenario models the case where the training data for the
target and shadow models are not sampled from exactly the
same population, or else sampled in a non-uniform way.
D. Training the attack model
The main idea behind our shadow training technique is that
similar models trained on relatively similar data records using
the same service behave in a similar way. This observation is
empirically borne out by our experiments in the rest of this
paper. Our results show that learning how to infer membership
in shadow models’ training datasets (for which we know the
ground truth and can easily compute the cost function during
supervised training) produces an attack model that successfully
infers membership in the target model’s training dataset, too.
We query each shadow model with its own training dataset
and with a disjoint test set of the same size. The outputs on
the training dataset are labeled “in,” the rest are labeled “out.”
Now, the attacker has a dataset of records, the corresponding
outputs of the shadow models, and the in/out labels. The
objective of the attack model is to infer the labels from the
records and corresponding outputs.
Figure 3 shows how to train the attack model. For all
(x, y) ∈ Dtrain
shadow i, compute the prediction vector y =
shadow(x) and add the record (y, y, in) to the attack training
f i
set Dtrain
shadow i be a set of records disjoint from the
training set of the ith shadow model. Then, ∀(x, y) ∈ Dtest
shadow i
shadow(x) and add the
compute the prediction vector y = f i
record (y, y, out) to the attack training set Dtrain
attack. Finally,
attack into ctarget partitions, each associated with a
split Dtrain
different class label. For each label y, train a separate model
that, given y, predicts the in or out membership status for x.
attack. Let Dtest
If we use model-based synthesis from Section V-C, all of
the raw training data for the attack model is drawn from
the records that are classiﬁed by the target model with high
conﬁdence. This is true, however, both for the records used in
the shadow models’ training datasets and for the test records
left out of these datasets. Therefore, it is not the case that
the attack model simply learns to recognize inputs that are
classiﬁed with high conﬁdence. Instead, it learns to perform
a much subtler task: how to distinguish between the training
inputs classiﬁed with high conﬁdence and other, non-training
inputs that are also classiﬁed with high conﬁdence.
In effect, we convert the problem of recognizing the com-
plex relationship between members of the training dataset and
the model’s output into a binary classiﬁcation problem. Binary
classiﬁcation is a standard machine learning task, thus we can
use any state-of-the-art machine learning framework or service
to build the attack model. Our approach is independent of the
speciﬁc method used for attack model training. For example,
in Section VI we construct the attack model using neural
networks and also using the same black-box Google Prediction
API that we are attacking, in which case we have no control
over the model structure, model parameters, or training meta-
parameters—but still obtain a working attack model.
VI. EVALUATION
We ﬁrst describe the datasets that we use for evaluation,
followed by the description of the target models and our exper-
imental setup. We then present the results of our membership
inference attacks in several settings and study in detail how and
why the attacks work against different datasets and machine
learning platforms.
8
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
A. Data
CIFAR. CIFAR-10 and CIFAR-100 are benchmark datasets
used to evaluate image recognition algorithms [24]. CIFAR-10
is composed of 32×32 color images in 10 classes, with 6, 000
images per class. In total, there are 50, 000 training images
and 10, 000 test images. CIFAR-100 has the same format as
CIFAR-10, but it has 100 classes containing 600 images each.
There are 500 training images and 100 testing images per
class. We use different fractions of this dataset in our attack
experiments to show the effect of the training dataset size on
the accuracy of the attack.
Purchases. Our purchase dataset is based on Kaggle’s “ac-
quire valued shoppers” challenge dataset that contains shop-
ping histories for several thousand individuals.6 The purpose
of the challenge is to design accurate coupon promotion
strategies. Each user record contains his or her transactions
over a year. The transactions include many ﬁelds such as
product name, store chain, quantity, and date of purchase.
For our experiments, we derived a simpliﬁed purchase
dataset (with 197, 324 records), where each record consists
of 600 binary features. Each feature corresponds to a product
and represents whether the user has purchased it or not. To
design our classiﬁcation tasks, we ﬁrst cluster the records
into multiple classes, each representing a different purchase
style. In our experiments, we use 5 different classiﬁcation
tasks with a different number of classes {2, 10, 20, 50, 100}.
The classiﬁcation task is to predict the purchase style of a
user given the 600-feature vector. We use 10, 000 randomly
selected records from the purchase dataset to train the target
model. The rest of the dataset contributes to the test set and
(if necessary) the training sets of the shadow models.
Locations. We created a location dataset from the publicly
available set of mobile users’ location “check-ins” in the
Foursquare social network, restricted to the Bangkok area
and collected from April 2012 to September 2013 [36].7 The
check-in dataset contains 11, 592 users and 119, 744 locations,
for a total of 1, 136, 481 check-ins. We ﬁltered out users with
fewer than 25 check-ins and venues with fewer than 100 visits,
which left us with 5, 010 user proﬁles. For each location venue,
we have the geographical position as well as its location type
(e.g., Indian restaurant, fast food, etc.). The total number of
location types is 128. We partition the Bangkok map into areas
of size 0.5km × 0.5km, yielding 318 regions for which we
have at least one user check-in.
Each record in the resulting dataset has 446 binary features,
representing whether the user visited a certain region or
location type, i.e., the user’s semantic and geographical proﬁle.
The classiﬁcation task is similar to the purchase dataset. We
cluster the location dataset into 30 classes, each representing
a different geosocial type. The classiﬁcation task is to predict
the user’s geosocial type given his or her record. We use 1, 600
randomly selected records to train the target model. The rest
6https://kaggle.com/c/acquire-valued-shoppers-challenge/data
7https://sites.google.com/site/yangdingqi/home/foursquare-dataset
of the dataset contributes to the test set and (if necessary) the
training sets of the shadow models.
Texas hospital stays. This dataset is based on the Hospital
Discharge Data public use ﬁles with information about inpa-
tients stays in several health facilities,8 released by the Texas
Department of State Health Services from 2006 to 2009. Each
record contains four main groups of attributes: the external
causes of injury (e.g., suicide, drug misuse), the diagnosis
(e.g., schizophrenia, illegal abortion), the procedures the pa-
tient underwent (e.g., surgery) and some generic information
such as the gender, age, race, hospital id, and length of stay.
Our classiﬁcation task is to predict the patient’s main proce-
dure based on the attributes other than secondary procedures.
We focus on the 100 most frequent procedures. The resulting
dataset has 67, 330 records and 6, 170 binary features. We use
10, 000 randomly selected records to train the target model.
Note that our experiments do not involve re-identiﬁcation
of known individuals and fully comply with the data use
agreement for the original Public Use Data File.
MNIST. This is a dataset of 70, 000 handwritten digits
formatted as 32 × 32 images and normalized so that
the
digits are located at the center of the image.9 We use 10, 000
randomly selected images to train the target model.
UCI Adult (Census Income). This dataset includes 48, 842
records with 14 attributes such as age, gender, education,
marital status, occupation, working hours, and native country.
The (binary) classiﬁcation task is to predict if a person makes
over $50K a year based on the census attributes.10 We use
10, 000 randomly selected records to train the target model.
B. Target models
We evaluated our inference attacks on three types of target
models: two constructed by cloud-based “machine learning as
a service” platforms and one we implemented locally. In all
cases, our attacks treat the models as black boxes. For the
cloud services, we do not know the type or structure of the
models they create, nor the values of the hyper-parameters
used during the training process.
Machine learning as a service. The ﬁrst cloud-based machine
learning service in our study is Google Prediction API. With
this service, the user uploads a dataset and obtains an API
for querying the resulting model. There are no conﬁguration
parameters that can be changed by the user.
The other cloud service is Amazon ML. The user cannot
choose the type of the model but can control a few meta-
parameters. In our experiments, we varied the maximum num-
ber of passes over the training data and L2 regularization
amount. The former determines the number of training epochs
and controls the convergence of model training; its default
value is 10. The latter tunes how much regularization is per-
formed on the model parameters in order to avoid overﬁtting.
8https://www.dshs.texas.gov/THCIC/Hospitals/Download.shtm