ture and its addressing are isolated from cache systems. In the CDN
architecture, as indicated by Figure 6, every server participates in the
distributed cache. Both internal addressing schemes, and distributed
filesystems are untouched.
DNS changes are trivial; opportunities and challenges shift to
policies. Our experience shows that necessary changes to DNS
are relatively small, and otherwise eclipsed by planning and execu-
tion costs associated with changes to hostname-IP bindings, sockets,
ECMP, quality of service routing and resourcing. In addition, poli-
cies can be designed and executed independent and irrespective of
IP addresses. This contrasts with the convention of designating ad-
dresses in DNS records and systems as proxies for a desired policy.
In our evaluation deployment the policy is expressed as datacenter
locations and account type. A match on both attributes then triggers
randomized addressing‚Äîhostnames are completely ignored. Queries
that do not match are resolved as normal. This approach enabled
code changes to be deployed globally, so that a single codebase
could be maintained during evaluation instead of two.
One question that has emerged is how best to design and allow
more expressive policies? Safe and verifiable policy expression and
processing is left for future work.
‚ÄúShared fate benefit, shared load‚Äù Our architecture also facilitates
provisioning. Different hostnames will incur different loads that may
change over time and are hard to predict a priori. Since IP addresses
are shared between hostnames and services, and IP addresses can be
associated with physical resources, the choice of address can affect
wider services. Given any set of candidate addresses, our system‚Äôs
answer to ‚Äúwhich IP addresses should be assigned?‚Äù is all of them.
This idea is borne out of one non-standard performance metric‚Äî
measures of load per IP address equalize. The effect is shown by
Figure 7, which plots dual measures of requests-per-IP (ùë¶1-axes)
and bytes-per-IP (ùë¶2-axes) in a medium-sized datacenter before and
after our architecture is deployed. Notably, the equalization emerges
without a priori engineering or post-analysis. This is an artefact of
the address randomization, itself.
Figure 7a shows dual measures of per-IP load before our system
was deployed. Measurements are drawn from 1% samples taken
over the two most loaded prefixes (out of 18) in a medium-sized
datacenter over a 24 hour period in June 2020. The 8192 individual
IP addresses are sorted on the ùë•-axis from most- to least-loaded by
number of requests per address indicated by the ùë¶1-axis. Correspond-
ing values for bytes transferred over each address are plotted in place,
and indicated by the ùë¶2-axis. The differences between most and least
loaded addresses are ‚àº4‚Äì6 orders of magnitude. Excluded from Fig-
ure 7a is the observation that these differences only increases when
including the remaining 16 prefixes allocated to hostnames that fall
under our test policy.
The same measurements captured during our deployment reveal
a sharp contrast. Recall from ¬ß3.2 that our deployment responds
to each DNS query, independent of the hostname, with an address
selected at random from the address pool. Figure 7b shows that
random addressing within a /20 reduces the load gaps to less than
440
1001011021031041051031041051061071081091010No. of Requests per IPBytes Transferred per IPIP Addresses in 2 /20s sorted by loadRequests (y1-axis)Bytes (y2-axis)100101102103104105103104105106107108109No. of Requests per IPBytes Transferred per IPIP Addresses in single /20 sorted by loadRequests (y1-axis)Bytes (y2-axis)104105107108109No. of Requests per IPBytes Transferred per IPIP Addresses in /24 sorted by loadRequests (y1-axis)Bytes (y2-axis)The Ties that un-Bind: Decoupling IP from web services and sockets. . .
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
2 and 3 orders of magnitude in the same day of week 24hr period.
Figure 7c shows that random addressing from a smaller /24 closes
the load gaps to near uniform levels of magnitude, and to factor of
less than 2 in absolute terms.
Absent randomization, reliable a priori predictions about the pop-
ularity or usage of hostnames is difficult, if not impossible. This has
consequences on most aspects of provisioning. The use of all ad-
dresses in a set of any size precludes the need to plan or provision on
the basis of popularity. Instead, a natural and effective load balancer
emerges by simultaneously yet dynamically binding all hostnames to
all available addresses in uniformly distributed manner, irrespective
of request patterns and demands.
Implications and Potential Pitfalls
4.4
For the sake of completeness we raise the following implications
of IP randomization, identify concerns, and resolve most. Later we
show that placing services behind a single address works, resolves
any potential pitfalls, and may improve application performance.
TTL Values and Individual Behaviors Any dependency on DNS
responses to somehow shape ensuing request patterns should be
avoided. This is because DNS responses are cached both at recursive
and local client stub resolvers. The subsequent traffic generated by
any single authoritative response is determined by the number and
behaviour of downstream resolvers and clients. For well-behaved
resolvers, TTLs provide some granularity of control. Recent ev-
idence suggests, however, that resolvers commonly modify TTL
values [46, 47]. Even so, agile IPs do open opportunities to investi-
gate resolver behaviors by changing TTL at the authoritative DNS
and comparing addresses that are returned with addresses used to
connect. We revisit this observation in ¬ß6.
Denying Denial of Service (DoS) Denial of service attacks can
target IP addresses (layer-4 attack) or hostnames (layer-7 attack).
Identifying the layer of the attack may, on appearance, seem more
difficult if randomizing IPs. We argue the root of the challenges lay,
not with randomization, but with the multiplexing of many hosts
onto individual IPs that exist independently of our scheme.
We also posit that an agile addressing architecture is a natural
fit for DoS mitigation for two reasons. First, the total load for all
affected services is immediately shared equally by all reachable
servers. Since our architecture operates at layer-3, separately and
isolated from L4 and L7 balancers, the impact of attacks is less
pronounced at higher protocol layers.
In addition, many DoS mitigations at CDN scale work by tem-
porarily changing hostnames to address assignments, and dropping
or directing packets destined to the affected IP addresses elsewhere.
In this way, DoS mitigations need the ability to quickly assign differ-
ent IP addresses to names and servers. We exploit this observation
in ¬ß6 to sketch a DoS mitigation mechanism built using addressing
agility to quickly identify DoS targets and mitigate attacks.
HTTP Connection Coalescing HTTP/2 [7] supports connection
reuse, and permits resource requests on domains that differ from
the domain used to connect. There are two conditions under which
connection reuse for a target resource is allowed:
(1) The target resource URI authority matches that of the certifi-
cate associated with the connection.
(2) The URI host resolves to the same IP address as the given
connection.5
IP address randomization may prohibit the second condition. There
are several reasons why this may not be a problem in practice. First,
browsers and operating system stacks are increasingly narrowing
the context in which connection state may be shared. Connection
reuse is often limited to a single process and, depending on appli-
cation characteristics, within certain contexts of that process (e.g.,
tab-isolation). In ¬ß5.2 we show opportunities potentially missed
by randomization across many addresses are instead exploited by
use of one address. Note that HTTP/3 [8] does not require IP ad-
dress matching, so randomization would not affect coalescing those
connections.
Non-TLS or HTTP based Services Applying our architecture
more broadly than to TLS and HTTP(S) presents a potentially sep-
arate yet surmountable set of challenges. One service that might
be adversely affected by randomized IPs is ssh, which maintains a
known_hosts file that stores the hostname-to-IP address mapping,
and issues a warning when the IP address used to connect is different
than is stored in the file. This association, while understandable, is
outdated and already broken given that many DNS records presently
return more than one IP address.
5 ONE ADDRESS TO SERVE THEM ALL
If millions of names and services can be hosted behind ‚àº4K ad-
dresses, and 256 addresses, then why not 1 address? We conducted
one additional experiment to evaluate feasibility.
Map everything to a single IP address Our deployment indicates
that as more services and machines reuse the same IP address, the
benefits continue to improve. We show that extending this to its
logical conclusion‚Äîa single IP address for the entire set of services‚Äî
is feasible, perhaps even preferable.
In early 2021 we parameterized a 24-hour trial at the datacenter
used in Figure 7 to return a ‚Äúrandom‚Äù IP address from a set of size 1
(i.e., a /32). Its success has motivated an extended trial‚Äîfrom June
2021 and scheduled to continue after final submission, this medium-
sized datacenter has been serving the same 20+M hostnames and
services with a randomized address set of 1.
We stress that all available system and performance metrics re-
mained unchanged. This experiment suggests that, services and
hostnames being equal, an autonomous system‚Äôs content and hosting
services can be bound to a single IP address. The result is newfound
levels of flexibility and granularity of control that enable entirely
new systems and visibility, as described in ¬ß6. Below we argue that
one-address also amplifies all benefits that are ascribed to random-
ization over a larger set, and additionally resolves potential pitfalls
of randomization identified in ¬ß4.4.
5.1 Why one address works, too
The explanations in ¬ß4.3 equally apply to one address. It is instructive
to restate those reasons in this context, and clarify differences.
5Not all browsers implement this check the same. Some browsers require the IP ad-
dress set for the new host to contain the IP address used for the existing connection,
whereas others assume transitive properties among IP address sets that have intersecting
addresses between them.
441
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Fayed, et al.
5.2 Upstream Implications and Opportunities
One-address is functionally equivalent to both conventional address-
ing and IP randomization. Even so, and in contrast to IP randomiza-
tion, one-address does raise the possibility of upstream effects that
deserve consideration. None have presented in our evaluations so
far, nor in communications with upstream networks.
Upstream Routing Errors are Immediate and Total
If all traffic
arrives on a single address (or prefix), then upstream routing errors
affects all content equally. This criticism has two responses. First,
this is an address diversity problem that exists independent of the way
addresses are used or the scale of operation, as are resolutions. For
example, one solution is to allocate two addresses in DNS records
from non-overlapping prefixes [15].
Alongside, our architecture provides fast mitigation. The per-
query rate of address change (see ¬ß4) means that addresses can be
changed en masse by changing affected address pool attached to any
policy. The changes will be immediate for new queries, and cached
records will update in a time that is upper-bounded by TTL.
Upstream DoS Protections The concentration of prefix- or many
prefixes-wide traffic to a single address might be mistakenly inter-
preted by an upstream observer as a denial of service attack. An
ISP could conceivably respond by triggering a well-meaning protec-
tion in the form of rate-limiting, or block. This can be prevented by
means of communication and community engagement.
Port exhaustion in IPv4 NAT is accelerated, as is a push to
IPv6 From the client-side, the number of permissible concurrent
connections to one-address is upper-bounded by the size of a trans-
port protocol‚Äôs port field. For TCP this is no longer an issue [21, 42].
In UDP (QUIC), however, the only way to reuse ports is with
SO_REUSEPORT. This could cause carrier-grade NATs to exhaust
available UDP ports, or lose the IP+port pair uniqueness that is
required for address translation. One option may be to incorporate
other transport-protocol headers into the NAT binding function, such
as QUIC connection identifiers, but these are decreasingly available
as encrypted transport becomes increasingly dominant.
To the best of our knowledge this is the only drawback to one-
address, and is also immediately resolved by migrating to IPv6.
Opportunities opened by one-address at the Client Running a
CDN on a single IP address may enable the fascinating new space
of client-side optimizations that arise as a result. Standard tasks
like DNS lookups and establishing TCP connections can comprise
large fraction of page load times (7% and 53%, respectively) [65].
When all content is served from the same IP address, a client can
potentially avoid these performance hits.
For example, one-address could be used to reduce stresses on
DNS. CDNs commonly use low DNS TTLs to permit rapid load
rebalancing. Under one-address, a CDN can adopt long-lived ex-
piries akin to root DNS servers, thereby extending cache duration
and reducing frequency of client DNS requests. An interesting area
of future work is to preemptively inform clients that the one-address
CDN hosts a particular domain. One possibility is to explore a new
form of HTTP resource hint that merely indicates the one-address
CDN host (making a DNS lookup unnecessary). Another possibility
is to use compact filters to represent all of the hostnames hosted by
the CDN [41]. We leave such optimizations for future work.
Figure 8: Preliminary evidence of connection coalescing: Placing
all services on one IP address increases requests per connection
relative to address to name bindings used in the rest of the CDN.
Routing and traffic engineering are unchanged, IP space is liber-
ated This argument is identical to IP randomization. One in-use IP
address has no effect on inter-domain routing, which admits a maxi-
mum of /24 and /56 prefix lengths. Once inside the autonomous sys-
tem, a packet is correctly forwarded to its destination. One-address
also makes it easier to reason about IP space and traffic engineering‚Äî
an otherwise challenging endeavours at CDN-scale.
The reachability of innumerable services behind one-address also
has implications for smaller or non-CDN operators: (i) Few ad-
dresses are required to function, even one; irrespective, (ii) standard