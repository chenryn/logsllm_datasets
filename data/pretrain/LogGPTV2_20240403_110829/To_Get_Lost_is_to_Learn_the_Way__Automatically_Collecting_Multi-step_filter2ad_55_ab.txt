queries. It inputs them into the search engine and search forms on
social media to widely collect corresponding URLs. Some social
media do not always provide comprehensive search results due to
a minimum required search function; thus, the module also uses a
search engine to collect social-media postings. Finally, it outputs
the URLs collected from the search results and links scraped from
social media postings as input for the web-crawling module.
3.2 Web-Crawling Module
The web-crawling module automates a web browser to recursively
crawl a URL collected by the landing-page-collection module and
outputs a WebTree as a crawling result. Figure 3 shows a conceptual
model of a WebTree representing sequences of web pages derived
from the landing page and visited by the web-clawing module.
The web-crawling module starts from the landing page, clicks on
multiple lure elements on the web pages, and recursively follows
multiple web pages derived from the landing page. The depth in-
dicates the recursion count of web crawls. The depth increases
when this module reaches a web page that completes loading and
is waiting for browser interaction. This module uses Selenium and
our original browser extension to automatically control and mon-
itor a web browser. For the prototype of our system, we chose
Google Chrome as a browser, but Selenium can also control other
web browsers; thus, the web-crawling module can use different
browsers. In the following section, we describe two components of
the web-crawling module: selecting and operating.
Selecting Component. The selecting component collects a
3.2.1
lure element that causes web navigation leading to SE attacks by
Figure 3: Conceptual model of WebTree.
analyzing an HTML source code and a screenshot of a web page.
As mentioned in Section 2.1, a word representing the category or
action of an element tends to be used for the lure element’s DOM
attributes, text content, and the text drawn inside the button graphic,
for example, “download” in “download-btn” of the class attribute
and “click” in “Click Now” of the text drawn inside a clickable button.
To select elements containing such keywords as lure elements, the
selecting component parses an HTML source code and executes
image processing of a web page’s screenshot. The purpose of the
selecting component is not to accurately detect elements leading to
SE attacks but to select possible lure elements to reduce the number
of elements with which to interact. By following only selected
elements, the web-crawling module can efficiently reach diverse
SE attacks. Note that there could be multiple lure elements on the
same web page; thus, this component analyzes all elements on
the web page. The reason the selecting component also executes
image processing is to complement the acquisition of character
strings drawn in the button image (i.e., img element), which cannot
be acquired from the HTML source code. This component also
identifies lure elements by their shape such as the triangular video
play button.
We explain a statistical method of preparing keywords for se-
lecting lure elements. We compare elements that have actually
redirected users to SE attacks (lure elements) with other elements
that have not redirected users to any SE attacks (non lure elements)
and extract words specific to lure elements. More specifically, we
extract attribute, text content, and strings drawn on buttons from
the collected elements and divide these words into two documents:
a document of lure elements and one of non lure elements. We
then calculate the term frequency-inverse document frequency (tf-
idf) of the two documents and manually choose words that have
high tf-idf values from the lure-element document. The process of
keyword selection is shown in Section 4.2.
In the analysis of HTML source codes, if an element matches at
least one of the following four rules, this component determines it
to be a lure element.
• one of the keywords is used in the element’s text content.
• a keyword is set in id, class, or alt DOM attributes.
• a keyword is used as the file name indicated by the URL of
• An executable file (e.g., .exe or .dmg) or a compressed file
(e.g., .zip or .rar) is used as a link extension.
the link (a element) or image (img element).
In the analysis of image processing, this component extracts
character strings written in each element from the screenshot and
matches keywords used in the HTML source code analysis. To iden-
tify the coordinates and size of buttons from the screenshot, this
!"#$%#&’("&)!"#$%"&’()*%+,-$#&$.+’(+/.+$0*"$1!"#$%"&’()*%+,2321!+4.567!+4.568!+4.569!+4.56:!"#$%"&’()*%+,;&%#&&;(4&?+(,.+>5(0-44"&;1=>&;(4&?+(,0-&;1Session 8: Web Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan397component leverages OpenCV to find rectangle contours repre-
senting the button areas. It also uses optical character recognition
(OCR) using Tesseract OCR [7] to extract character strings from
the rectangles the component found. This component executes key-
word matching with extracted character strings and determines an
element containing one of the keywords in the area to be a lure
element. To acquire video play buttons as lure elements, the module
also finds a triangle contour pointing right. Finally, the component
outputs multiple lure elements that may lead to SE attacks from
the web page.
3.2.2 Operating Component. The operating component executes
browser interactions (i.e., clicking on lure elements), monitors web
navigation, and constructs a WebTree. It simulates clicking on lure
elements with the CTRL key pressed to open the web page in a new
browser tab because the current page may be transferred to another
web page by simple clicking. As a result, links or popup windows
can be opened in new tabs without changing the original tab. To
simulate unintended clicks described in Section 2.2, the operating
module also clicks a body element, body element with context click,
and the browser’s back button. When the new tab is opened, the
selecting component finds lure elements again, and the operating
component executes browser interactions with a depth-first order,
unless it reaches a predetermined maximum depth. We explain the
maximum depth we used in the following experiment in Section 4.2.
The operating component also monitors web navigation. To mon-
itor JavaScript function calls, this component hooks the existing
JavaScript function so that it can detect the executed JavaScript
function name and its argument. The JavaScript functions to be
monitored by this component are alert(), window.open(), and
the installation function of the browser extension (e.g., chrome.
webstore.install()). The function alert() is frequently used
in SE attacks that threaten a user by suddenly displaying dialog
with messages inducing user anxiety. The window.open() function
opens a new browser window and is used for popup advertisements.
This component also hooks the installation function of the browser
extension and detects what type of browser extension was installed
from the argument. This component also monitors URL redirec-
tion, which navigates a user to another URL. URL redirection is
divided into client-side redirection and server-side redirection. A
web browser may conduct client-side redirection such as JavaScript
function location.href when this component clicks the lure ele-
ment. On the other hand, a web server conducts server-side redi-
rection to navigate to another web page before loading a web page.
This component monitors the URLs the browser passed during
server-side redirection to identify the server that navigates users
to SE attacks, such as advertising providers.
The operating component conducts browser interactions and
monitors web navigation until it finishes clicking on all selected lure
elements. This component aggregates information from sequences
of web pages (i.e., screenshots, the HTML source codes of web pages,
browser interactions, and web navigation) and finally outputs a
WebTree as input for the SE-detection module.
3.3 SE-Detection Module
The SE-detection module extracts features from a WebTree output
by the web-crawling module and identifies multi-step SE attacks
using a classification model. This module first extracts sequences
Figure 4: Example of extracting features from a sequence.
from the WebTree. A sequence is defined as a series of rendered
web pages from the landing page (a root node) to the last pages
(leaf nodes). Note that the sequence does not represent a URL redi-
rection chain (an automatic process of forwarding a user to another
URL multiple times) but a series of displayed web pages through
user interaction. This module then extracts features from each se-
quence that reaches web pages of depth of two or more. Unlike
conventional methods that examine structural similarity of URL
redirection chains [22, 30, 32], this module extracts features spe-
cific to multi-step SE attacks from the entire sequence: contents of
web pages, browser interactions that trigger page transitions, and
web navigation. Finally, it identifies whether the last page of each
sequence is the SE page using a classifier and outputs URLs of the
detected web pages. Ground truth data for identifying SE attacks is
explained in Section 4.3.
Feature Extraction. To classify web pages that trick users into
3.3.1
interacting, it is common to use information that can be acquired af-
ter visiting the web page, such as image and HTML features [17, 27].
However, if a classifier uses such features, it cannot detect an SE
page similar to the legitimate page, such as a fake software-update
web page that closely resembles a legitimate Flash update page or
fake infection-alert page using the logo of security vendors. There-
fore, we designed feature vectors using not only features extracted
from a single web page but also all features extracted from the
entire sequence. Specifically, it analyzes the last page of the se-
quence, page before the last page (previous page), and the entire
sequence, as shown in Fig. 4. Table 10 shows features extracted
from each sequence and grouped into the three phases of SE at-
tacks: user attraction, browser interaction, and web navigation. To
the best of our knowledge, StraySheep is the first system that
automatically collects these features from the entire sequence by re-
cursively crawling web pages from the landing page. In terms of the
user-attraction-based features, StraySheep extracts appearance,
meaning of document, and structure of HTML from the last and
previous pages. It then finds features based on browser interaction,
such as actions performed on the web pages and lure elements from
the previous page and entire sequence. The SE-detection module
also analyzes web navigation that occurred on the last page, pre-
vious page, and entire sequence. We explain a feature extraction
method for each SE-attack phase below in detail.
User Attraction The appearance of a web page and the semantic
properties of text content include the intention of the attacker to
trick a user. The HTML document structure is also an important
indicator for analyzing the similarity of web pages using the same
document template. The SE-detection module extracts image and
linguistic features from the last and previous pages of the sequence.
It also calculates an HTML tag histogram, RGB color histogram, and
the length of the text field from both the last and previous pages. To
extract image features, we use AKAZE [9], which is a bag-of-visual
words algorithm that detects local image features. The SE-detection
ADCLanding pagePrevious pageLast pageSequenceBSession 8: Web Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan398module extracts 128 dimensional image features from the screen-
shots of the last and previous pages using a trained model we pre-
viously constructed. We use Doc2Vec [1] as a document-modelling
algorithm to extract linguistic features. The module extracts the
300 dimensional features from text content of the last and previous
pages by using a doc2vec model trained beforehand. The text con-
tent of a web-page document is extracted by cleaning out HTML
tags from an HTML source code. The SE-detection module also
calculates a histogram of the RGB (red, green, and blue) values
of the screenshot with ten bins for each color and a histogram of
HTML tags of the text content. This module uses up to 40 HTML
tags (e.g., a div, and img) frequently appearing on web pages we
collected in advance. It counts the number of characters in the text
content.
Browser Interaction The SE-detection module analyzes lure ele-
ments and actions that caused SE attacks. To extract features from
browser interactions, this module counts the number of left clicks
and unintended clicks (body clicks, body-context clicks, and back-
button clicks) the web-crawling module performed in the sequence.
This module also counts the types of clicked lure elements (a and
iframe) in the sequence and determines the size (x,y) and coordi-
nates (width, height) of lure elements on the previous page.
Web Navigation The SE-detection module analyzes browser events
that occurred as a result of browser interaction. File downloads
and extension install indicate events that are directly related to SE
attacks such as malware downloads and unwanted extension in-
stalls. Since SE attacks are often delivered via advertising providers,
redirection has characteristics unique to SE attacks. The method of
navigation (e.g., redirection and popup window) is important for
analyzing SE attacks. This module determines whether file down-
loads and extension installs occurred on the last and previous pages.
It counts the times popup windows were displayed and the number
of URLs observed during server-side and client-side redirection. It
also checks the number of displayed alert dialogues and the length
of the sequence, i.e., crawling depth.
3.3.2 Classifier. We combine the features extracted from sequences
to create features vectors and construct a binary classifier to iden-
tify SE web pages. We use Random Forest as a learning algorithm
because we can measure the importance of each feature that con-
tributes to the classification. Evaluation results compared with other
algorithms are given in Section 4.5.
4 EVALUATION
We evaluated the three modules of StraySheep (landing-page-
collection, web-crawling, and SE-detection). We first evaluated
the qualitative advantage of StraySheep by comparing it with
previous systems for collecting SE attacks. We then evaluated the
effectiveness of the landing-page-collection module by comparing
its two collection methods (search engine and social media) to
three baseline URL-collection methods in terms of the number of
landing pages leading to SE attacks and total visited malicious pages
and domain names. Also, we conducted a crawling experiment to
determine the efficiency of the web-crawling module by comparing
its crawling method with two baseline crawling methods in terms
of the number of malicious domain names reached per unit of time.
Finally, we confirmed the effectiveness of the SE-detection module
in terms of detection accuracy.
4.1 Qualitative Evaluation
We qualitatively compared StraySheep with the previous systems
to collect SE attacks from five perspectives. Table 1 summarizes the
results.
Collecting method. The previous systems [24, 25] for passively
observing HTTP traffic to analyze SE attacks can only collect at-
tacks triggered by users’ real download events. On the other hand,
actively crawling arbitrary web pages with StraySheep enables us
to proactively detect SE attacks before many users reach the web
pages.
Interacting with elements. To observe multi-step SE attacks, we
need to interact with HTML elements and recursively follow page
transitions. Surveylance [17] is a system to detect survey gateways,
which are landing pages displaying survey requests, and interact
with their survey content and survey publisher sites. A system
proposed by Rafique et al. [27] detects free live streaming (FLIS)
pages and interacts with overlay video ads on them. While these
systems focus on survey scams or FLIS services, StraySheep can
collect various SE attacks and observe different types of survey
scams originating from web pages deeper than the landing pages
(see Section 5.1).
Extracting features. As stated in Section 3.3, StraySheep extracts
features such as images, HTML structures, and linguistic context
from reached web pages and analyzes sequences to accurately detect
multi-step SE attacks. As shown in Table 1, none of the previous
systems use all the features used in StraySheep.
Source of landing-page collection. StraySheep collects landing
pages from two common platforms: search engines and social media.
StraySheep is the only system that uses both platforms.
Type of SE attacks to collect. While the previous systems are
limited to detecting a specific attack, StraySheep collects various
multi-step SE attacks by following lure elements on each web page.
In summary, StraySheep is the first system to collect multi-step
SE attacks not limited to specific attacks by recursively following
multiple lure elements on web pages. StraySheep also detects
multi-step SE attacks by extracting various types of features from
reached web pages and sequences.
4.2 Experimental Setup
We implemented StraySheep for Google Chrome 69 with Ubuntu
16.04. It simultaneously ran up to 32 instances on a virtual machine
assigned with Intel Xeon 32 logical processors and 256-GB RAM.
For the browser setting, a user agent was set as Google Chrome
of Windows 7, and browser cookies were reset for every landing-
page access. Our crawling experiment spanned from November
to December 2018, and StraySheep used a single IP address. We
need to set a timeout for performance evaluation because the two
baseline web-crawling modules mentioned in Section 4.4 require an
enormous amount of time (a few weeks at most) to complete web
crawling. About 90% of web crawling conducted with StraySheep
finished within an hour in our preliminary experiment (similar
results are shown in Fig. 5); therefore, we set the timeout to one
hour. To find the best maximum depth for collecting the most
malicious domain names when we used the timeout, we changed
the depth from two to six. The number of malicious domain names
monotonically increased up to depth four and decreased as the
Session 8: Web Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan399Table 1: Comparison between proposed and previous systems
Collecting method
Interacting with HTML elements
StraySheep
Surveylance [17] Rafique et al. [27]
ROBOVIC [23]
Active
Active
Active
(cid:71)(cid:35) (survey filling) (cid:71)(cid:35) (overlay video ad) (cid:35)
(cid:32)
Following multiple lure elements on web page (cid:32)
(cid:35)
(cid:35)
(cid:32)
(cid:32)
(cid:35)
(cid:32)
(cid:35)
(cid:32)
(cid:71)(cid:35) (heuristic)
(cid:32)
(cid:32)
(cid:71)(cid:35) (heuristic)
(cid:35)
(cid:32)
Search engine, social media Search engine
Parked domain, URL shortener Search engine
All multi-step SE attacks
Survey scams
(cid:32): Fully Covered,(cid:71)(cid:35): Partially Covered,(cid:35): Not Covered
Active
(cid:35)
(cid:32)
(cid:32)
(cid:32)
(cid:35)
Search engine