表中。
if done:
episode_number += 1
game_number = 0
epx.extend(xs)
epy.extend(ys)
disR = tl.rein.discount_episode_rewards(rs, gamma)
disR -= np.mean(disR)
disR /= np.std(disR)
epr.extend(disR)
xs, ys, rs = [], [], []
智能体在进行了很多局游戏，并收集了足够的数据之后，就可以开始更新了。我们使用交叉
熵损失和梯度下降方法来计算各参数的梯度，之后将梯度应用在相应的参数上，并结束更新。
if episode_number
print(’batch over...... updating parameters......’)
with tf.GradientTape() as tape:
_prob = model(epx)
_loss = tl.rein.cross_entropy_reward_loss(_prob, epy, disR)
grad = tape.gradient(_loss, train_weights)
optimizer.apply_gradients(zip(grad, train_weights))
epx, epy, epr = [], [], []
以上内容描述了主要工作，之后的代码主要用于显示训练相关数据，以便更好地观察训练走
势。我们可以使用滑动平均来计算每个片段的运行奖励，以降低数据抖动的程度，方便观察趋势。
最后，做完这些内容后别忘了重置环境，因为此时当前片段已经结束了。
# if episode_number
# tl.files.save_npz(network.all_params, name=model_file_name + ’.npz’)
running_reward = reward_sum if running_reward is None else running_reward * 0.99
+ reward_sum * 0.01
print(’resetting env. episode reward total was {}. running mean:
{}’.format(reward_sum, running_reward))
167
第5章 策略梯度
reward_sum = 0
observation = env.reset()
prev_x = None
if reward != 0:
print(
( ’episode
(episode_number, game_number, time.time() - start_time, reward)
), (’’ if reward == -1 else ’ !!!!!!!!’)
)
start_time = time.time()
game_number += 1
CartPole
这个例子中，算法和Pong的一样。我们可以考虑将整个算法放入一个类中，并将各部分代
码写入对应的函数。这样可以使得代码更为简洁易读。PolicyGradient类的结构如下所示：
class PolicyGradient:
def __init__(self, state_dim, action_num, learning_rate=0.02, gamma=0.99):
# 类初始化。创建模型、优化器和需要的变量
......
def get_action(self, s, greedy=False): # 基于动作分布选择动作
......
def store_transition(self, s, a, r): # 存储从环境中采样的交互数据
......
def learn(self): # 使用存储的数据进行学习和更新
......
def _discount_and_norm_rewards(self): # 计算折扣化回报并进行标准化处理
......
def save(self): # 存储模型
......
def load(self): # 载入模型
......
初始化函数先后创建了一些变量、模型并选择Adam作为策略优化器。在代码中，我们可以
看出这里的策略网络只有一层隐藏层。
168
5.10 策略梯度代码例子
def __init__(self, state_dim, action_num, learning_rate=0.02, gamma=0.99):
self.gamma = gamma
self.state_buffer, self.action_buffer, self.reward_buffer = [], [], []
input_layer = tl.layers.Input([None, state_dim], tf.float32)
layer = tl.layers.Dense(
n_units=30, act=tf.nn.tanh, W_init=tf.random_normal_initializer(mean=0,
stddev=0.3),
b_init=tf.constant_initializer(0.1)
)(input_layer)
all_act = tl.layers.Dense(
n_units=action_num, act=None, W_init=tf.random_normal_initializer(mean=0,
stddev=0.3),
b_init=tf.constant_initializer(0.1)
)(layer)
self.model = tl.models.Model(inputs=input_layer, outputs=all_act)
self.model.train()
self.optimizer = tf.optimizers.Adam(learning_rate)
在初始化策略网络之后，我们可以通过 get_action()函数计算某状态下各动作的概率。通
过设置’greedy=True’，可以直接输出概率最高的动作。
def get_action(self, s, greedy=False):
_logits = self.model(np.array([s], np.float32))
_probs = tf.nn.softmax(_logits).numpy()
if greedy:
return np.argmax(_probs.ravel())
return tl.rein.choice_action_by_probs(_probs.ravel())
但此时，我们选择的动作可能并不好。只有通过不断学习之后，网络才能做出越来越好的判
断。每次的学习过程由 learn()函数完成，这部分函数的代码基本也和Pong例子中一样。我们
使用标准化后的折扣化奖励和交叉熵损失来更新模型。在每次更新后，学过的转移数据将被丢弃。
def learn(self):
# 计算标准化后的折扣化奖励
discounted_ep_rs_norm = self._discount_and_norm_rewards()
169
第5章 策略梯度
with tf.GradientTape() as tape:
_logits = self.model(np.vstack(self.ep_obs))
neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=_logits,
labels=np.array(self.ep_as))
loss = tf.reduce_mean(neg_log_prob * discounted_ep_rs_norm)
grad = tape.gradient(loss, self.model.trainable_weights)
self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))
self.ep_obs, self.ep_as, self.ep_rs = [], [], [] # 清空片段数据
return discounted_ep_rs_norm
learn()函数需要使用智能体与环境交互得到的采样数据。因此我们需要使用 store_tran-
sition()来存储交互过程中的每个状态、动作和奖励。
def store_transition(self, s, a, r):
self.ep_obs.append(np.array([s], np.float32))
self.ep_as.append(a)
self.ep_rs.append(r)
策略梯度算法使用蒙特卡罗方法。因此，我们需要计算折扣化回报，并对回报进行标准化，
也有助于学习。
def _discount_and_norm_rewards(self):
# 计算折扣化片段奖励
discounted_ep_rs = np.zeros_like(self.ep_rs)
running_add = 0
for t in reversed(range(0, len(self.ep_rs))):
running_add = running_add * self.gamma + self.ep_rs[t]
discounted_ep_rs[t] = running_add
# 标准化片段奖励
discounted_ep_rs -= np.mean(discounted_ep_rs)
discounted_ep_rs /= np.std(discounted_ep_rs)
return discounted_ep_rs
和Pong的代码一样，我们先准备好环境和算法。在创建好环境之后，我们产生一个名为agent
的 PolicyGradient类的实例。
env = gym.make(ENV_ID).unwrapped
170
5.10 策略梯度代码例子
# 通过设置随机种子，可以复现一些运行情况
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
env.seed(RANDOM_SEED)
agent = PolicyGradient(
action_num=env.action_space.n,
state_dim=env.observation_space.shape[0],
)
t0 = time.time()
在训练模式中，我们使用模型输出的动作来和环境进行交互，之后存储转移数据并在每个片
段更新策略。为了简化代码，智能体将在每局结束时直接进行更新。
if args.train:
all_episode_reward = []
for episode in range(TRAIN_EPISODES):
# 重置环境
state = env.reset()
episode_reward = 0
for step in range(MAX_STEPS): # 在一个片段中
if RENDER:
env.render()
# 选择动作
action = agent.get_action(state)
# 与环境交互
next_state, reward, done, info = env.step(action)
# 存储转移数据
agent.store_transition(state, action, reward)
state = next_state
episode_reward += reward
# 如果环境返回 done 为 True，则跳出循环
if done:
break
# 在每局游戏结束时进行更新
agent.learn()
print(
171
第5章 策略梯度
’Training | Episode: {}/{} | Episode Reward: {:.0f} | Running Time:
{:.4f}’.format(
episode + 1, TRAIN_EPISODES, episode_reward,
time.time() - t0))
我们可以在每局游戏结束后的部分增加一些代码，以便更好地显示训练过程。我们显示每个
回合的总奖励和通过滑动平均计算的运行奖励。之后可以绘制运行奖励以便更好地观察训练趋
势。最后，存储训练好的模型。
agent.save()
plt.plot(all_episode_reward)
if not os.path.exists(’image’):
os.makedirs(’image’)
plt.savefig(os.path.join(’image’, ’pg.png’))
如果我们使用测试模式，则过程更为简单，只需要载入预训练的模型，再用它和环境进行交
互即可。
if args.test: