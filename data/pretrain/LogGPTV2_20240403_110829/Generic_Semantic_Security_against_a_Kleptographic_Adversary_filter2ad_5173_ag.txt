we can bound the distinguishing advantage by the probability that a collision appears in q queries
when sample from S. We leave the full proof to Sec. C in the appendix.
30
Proposition B.1. For any CPA-secure (public-key) encryption scheme, for any public function Φ,
the adversary A shown in Figure 15 can learn the plaintext with probability 1 given the ciphertext
generated by Enczimpl even if the randomness generation is separated and immunized by a random
oracle. Furthermore, suppose RGimpl outputs (cid:96) bits of randomness; then the detection advantage is
q2/2(cid:96)−4 + negl(λ) for all ppt watchdogs making q queries during the interrogation, assuming PRF is a
secure pseudorandom function.
Proof. The eﬀectiveness of A is straightforward, since the randomness generated by RGimpl makes
the ciphertext to be distinguishable using the PRF, thus the adversary A who knows the backdoor
recovers the message bit perfectly.
Next, we will argue that no oﬄine watchdog can notice the subversion, particularly RGimpl. (All
Game-0 is the game that W is interacting with RGimpl.
Game-1 is the same as Game-0 except that the PRF used in RGimpl is replaced with a random
Game-2 is the same as Game-1 except that RGimpl resamples, if W notices a collision in the
other components are honestly implemented). We deﬁne the game sequence as follows:
function R.
responses to the q queries.
Game-3 is the same as Game-2 except that RGimpl is replaced with RGspec.
Game-4 is the same as Game-3 except removing the resampling condition.
Analyzing the gaps, we see: Game-0 and Game-1 are indistinguishable because of the PRF
security; Game-1 and Game-2 are identical, if there is no collision among the q queries in Game-2.
In Game 2, for any r0, the probability that R(c0) = 0∧ R(c1) = 1 is 1/4, where c0, c1 are ciphertexts
encrypting 0,1 respectively using Φ(r0) as the coin. Suppose S is the set that contains all the values
that satisﬁes R(c0) = 0∧ R(c1) = 1, then the expected size of S, E[|S|] = 2(cid:96)/4 = 2(cid:96)−2. It follows that
with negligible probability, |S| ≤ E[|S|]
2 = 2(cid:96)−3. Then the probability that there exists a collision
among q samples from S would be bounded by q2/|S| ≤ q2/2(cid:96)−3. Game-2 and Game-3 is identical,
since the responses can either appear in a random subset or the whole space. Game-3 and Game-4
are identical except there is collision when sampling q uniform points. The probability of such
collision exist is q2/2(cid:96). Combining them above, we have the proposition.
Theorem 3.3. For any randomized algorithm G, consider the speciﬁcation Gspec := (RGspec,dGspec),
where RGspec generates λ = λ(n) bits of uniform randomness and dGspec is deterministic. Let (RG0
spec,RG1
be the double-split speciﬁcation of RGspec as above. If (1) RG0
spec(1λ) output λ uniform
bits; (2) Φspec takes r0 ◦ r1 as input, and outputs r (so it maps strings of length 2k to strings of length k)
(see Fig. 5), then Gspec is stego-free with a trusted amalgamation (according to Def. A.2 in appendix. A.2).
Here Φspec is modeled as a random oracle, and Φ0impl, Φ1impl are executed independently.
spec(1λ) and RG1
spec, Φspec)
Proof. The watchdogs will be a combination of the ones in Theorem 3.1 and Lemma 2.3 to guarantee
unpredictability of RGbimpl and the overwhelming consistency for deterministic algorithms with a
public input distribution. Here dGspec is a deterministic algorithm and the output of RGspec × IG
would be the input distribution to dGspec.
We here only describe brieﬂy about the game changes.
Game-0 is the same as Figure 12, the adversary A prepares every piece of the implementation,
and the challenger simply calls them and passes the inputs to the next implementation as deﬁned
(doing the basic amalgamation).
Game-1 is the same as Game-0 except that the randomness r is uniformly sampled by C.
31
Game-2 is the same as Game-1 except that dGimpl is replaced with dGspec.
Note that in Game-0, it corresponds to b = 0, while in Game-2, every implementation of the
algorithm (except input generation) is now the speciﬁcation, it corresponds to b = 1.
From Theorem 3.1, with a trusted amalgamation, the output from the implementation RGimpl :=
impl, Φimpl) is pseudorandom to the adversary A who made RGimpl. Thus in the security
(RG0
impl,RG1
game deﬁned in Figure 12, we can let the challenger simply generate r uniformly to reach Game-1.
From Lemma 2.3, dGspec will be a deterministic algorithm with a public input distribution,
thus dGimpl would be consistent with dGspec with an overwhelming probability when inputs are
sampled according to RGspec × IG, thus Game-2 can be reached with only a negligible gap.
Once in Game-2, all components are speciﬁcation.
Remark B.1. We remark that for this particular attack we assume that the implementation has access
to a public key—this yields an intuitively natural attack against a encryption scheme which permits
full recovery of the message. However, the basic structure of the attack can be adapted to randomized
algorithms in generality.
32
C Omitted Proofs
Theorem 3.3. For any randomized algorithm G, consider the speciﬁcation Gspec := (RGspec,dGspec),
where RGspec generates λ = λ(n) bits of uniform randomness and dGspec is deterministic. Let (RG0
spec,RG1
be the double-split speciﬁcation of RGspec as above. If (1) RG0
spec(1λ) output λ uniform
bits; (2) Φspec takes r0 ◦ r1 as input, and outputs r (so it maps strings of length 2k to strings of length k)
(see Fig. 5), then Gspec is stego-free with a trusted amalgamation (according to Def. A.2 in appendix. A.2).
Here Φspec is modeled as a random oracle, and Φ0impl, Φ1impl are executed independently.
spec(1λ) and RG1
spec, Φspec)
Proof. The watchdogs will be a combination of the ones in Theorem 3.1 and Lemma 2.3 to guarantee
unpredictability of RGbimpl and the overwhelming consistency for deterministic algorithms with a
public input distribution. Here dGspec is a deterministic algorithm and the output of RGspec × IG
would be the input distribution to dGspec.
We here only describe brieﬂy about the game changes.
Game-0 is the same as Figure 12, the adversary A prepares every piece of the implementation,
and the challenger simply calls them and passes the inputs to the next implementation as deﬁned
(doing the basic amalgamation).
Game-1 is the same as Game-0 except that the randomness r is uniformly sampled by C.
Game-2 is the same as Game-1 except that dGimpl is replaced with dGspec.
Note that in Game-0, it corresponds to b = 0, while in Game-2, every implementation of the
algorithm (except input generation) is now the speciﬁcation, it corresponds to b = 1.
From Theorem 3.1, with a trusted amalgamation, the output from the implementation RGimpl :=
impl, Φimpl) is pseudorandom to the adversary A who made RGimpl. Thus in the security
(RG0
impl,RG1
game deﬁned in Figure 12, we can let the challenger simply generate r uniformly to reach Game-1.
From Lemma 2.3, dGspec will be a deterministic algorithm with a public input distribution,
thus dGimpl would be consistent with dGspec with an overwhelming probability when inputs are
sampled according to RGspec × IG, thus Game-2 can be reached with only a negligible gap.
Once in Game-2, all components are speciﬁcation.
D Purifying randomness in the standard model
The split-and-amalgamate paradigm proposed in Section 3.2 assumes the speciﬁcation of the
immunization function Φspec to be modeled as a random oracle. Intuitively, it seems hard to
remove this assumption when the randomness generation is split into only two segments, since
a ppt oﬄine watchdog can guarantee at most a ω(log λ) entropy in the output of each RGbimpl for
b = 0,1, (and the concrete lower bound of entropy is not even clear). Nevertheless, it is interesting
to consider immunization in the standard model if we are willing to have more than constant
number of components.
A simple approach via multi-splitting. Observe that suppose the randomness generation imple-
mented by the adversary only outputs one bit, the watchdog (who checks whether there is a bit
appears signiﬁcantly more frequently than the other after drawing certain amount of samples) can
guarantee that the bit only has a negligible bias. If not, suppose the bias for outputting bit value
1 is 1/2 + δ for a noticeable δ, then according to the Chernoﬀ bound, after the watchdog draws
enough samples, he will notice that there are signiﬁcantly more 1s appear which is abnormal if the
speciﬁcation is used which outputs a uniform bit.
33
With the above observation, we can simply extract one bit from a RG1
now split the randomness generation into n copies RG1
independently draw one bit from each of them.
impl from each draw. If we
impl, . . . ,RGnimpl, we can collect n-bits if we
Security analysis. We now analyze how these n-bits are distributed. Suppose the output distribution
of RGiimpl is denoted by Di, the user ﬁrst samples di ← Di, and outputs only the ﬁrst bit of di; each
such bit has bias at most 1/2 + , for a negligible function . We denote each bit using the random
variable X1, . . . , Xn. It follows that for any particular value b1, . . . , bn ∈ {0,1}n, as 1 + x
n)n ≤ ex we have
Pr[(X1, . . . , Xn) = (b1, . . . , bn)] ≤ (1/2 + )n ≤ 1
2n e2n,
that if Yn( ¯x) ≤ Un( ¯x)(1 + t) for all ¯x ∈ {0,1}n, the statistical distance satisﬁes:
Let Un be the uniform distribution over {0,1}n, and Yn be the distribution of X1 . . . Xn. Recall
(cid:88)
¯x
(cid:88)
¯x
(cid:107)Yn − Un(cid:107) =
1
2
|Yn( ¯x)− Un( ¯x)| ≤ 1
2
Un( ¯x)· t =
t
2
.
Putting back the value t = e2n−1, we conclude that the statistical distance between the distribution
of X1 . . . Xn and the uniform distribution over n-bit strings, is no more than 2n ≤ negl(λ), the
inequality holds because ex − 1 ≤ 2x for x < 1/2. Summarizing above, we have:
Proposition D.1. Suppose the randomness generation speciﬁcation RGspec is deﬁned as (RG1
spec, . . . ,RGnspec),
where each RGispec is supposed to output uniform n bits, and Φspec : {0,1}n2 → {0,1}n simply outputs the
ﬁrst bit of each n-bit block. Such RGspec is stego-free with a trusted amalgamation.
Remark D.1. We keep the immunization function for consistency with the general model. In this simple
multi-splitting, we may not even need Φspec; we can deﬁne each RGispec to output only 1 random bit.
A more eﬃcient construction. The above construction is extremely simple, but with a price that
the randomness generation has to be split into n pieces if the output is n-bit long. Note that there
are at least c log n bits entropy (for any constant c ≥ 2) in the output of each RGiimpl. This comes
from the guarantee of a watchdog that makes O(n2c) queries during the interrogation. Moreover,
the bitstring can be stretched (if it is close to uniform) to a polynomially longer output using a
PRG. 8
We may use more powerful machineries of a strong randomness extractor together with a
pseudorandom generator. The intuition is to ﬁrst get a short seed and then use the power of
the extractor to bootstrap the randomness generation by a factor of log n since we can get at
least this many uniform bits from the output of each RGiimpl. After collecting O(n) bits (for
some small ), we can apply the PRG to stretch it to O(n) bits. Suppose RGspec now is deﬁned as
(RG1
spec, . . . ,RG(cid:96)spec, Φspec), where (cid:96) = n/ log n+log n for some small . We describe the immunization
function Φspec : {0,1}n·(cid:96) → {0,1}n below, when inputing r1, . . . , r(cid:96) (generated by RGiimpl respectively).
• First, Φspec takes r1, . . . , rlog n and returns the ﬁrst bit of each ri and obtains a log n-bit uniform
string denoted by s0.
8We remark that here the PRG is just one deterministic algorithm, and it is associated with a public input distribution.
Such PRG can be based on e.g., some ﬁxed hard function. This is in contrast with the (backdoored) PRG that requires
some randomly generated public parameters P P , and has the danger of containing backdoors in P P [DGG+15, RTYZ15].
34
• Then, Φspec applies a seeded strong extractor Ext to
rlog n+1, . . . , r(cid:96) respectively using s0 as a seed. It obtains (cid:96) log n = n uniform bits, (log n bits
from each of them), which are denoted by s1.
• Last, we apply a pseudorandom generator PRG on s1 to stretch it to n bits, and output those
as the ﬁnal random coin.
Security analysis. Let us ﬁrst assume Φspec is not subverted. In the ﬁrst step, s0 would be close to
uniform due to Proposition D.1. In the second step, since Ext is a strong extractor, the output of
Ext(s0, ri) is close to uniform and independent of s0, via the simple union bound, s1 is also negligibly
close to uniform. Last step follows easily because of the PRG property.
Next, observe that although Φspec here is a complex function, it is still a deterministic algorithm,
and it is with a public input distribution. Following Lemma 2.3, the watchdog can guarantee that
the subverted implementation Φimpl will be consistent with the speciﬁcation with an overwhelming
probability when the input is sampled from RG1
impl × . . .× RG(cid:96)impl.
Proposition D.2. There exists a speciﬁcation for the randomness generation that outputs n bits is
stego-free with the trusted amalgamation and O(n/ log n) segments for any constant .
E Further Applications
We show above how we can salvage randomized algorithms, in particular randomized encryption
schemes, when they are under subversion. While the technique we develop is used to generically
eliminate subliminal channels which has its own interest. In this section, we give two more
examples to illustrate how the techniques can be applied to broader settings. We ﬁrst show how to
circumvent the impossibility result of [DGG+15] for publicly immunizing the output of a potentially
subverted PRG (called backdoored PRG). Then we show how our decomposition-then-amalgamate
paradigm can be used to simulate an intuitively more powerful semi-private model proposed in
[DGG+15]. In such a model, there is a uniformly random string r generated by a trusted party and r
is made public only after the adversary provides the implementations. This will motivate follow-up
works to consider the power of the semi-private model, then realize it using our paradigm.
Public immunization of output of a backdoored PRG. In a backdoored PRG, the stretch algorithm
takes a random seed and also a public key (generated by the adversary) as inputs. The goal of
the adversary is to ensure that the output of the PRG looks uniform to everyone except herself by
providing a malicious public key. In particular, it captured the notorious subversion attack on
the Dual_EC PRG. Unfortunately, as shown in [DGG+15], publicly immunizing the outputs of the
backdoored PRG using a random oracle is impossible, i.e., the adversary can still distinguish the
immunized string from a random string. The reason is similar to the attack shown in Section 3.1
since the subverted stretch algorithm can make random oracle queries during the execution. Russell
et al. proposed a way to sidestep this impossibility by randomizing the public key [RTYZ15] with
the caveat that the speciﬁcation of the key generation should output uniform group elements. As
pointed out by [DGG+15], immunizing the output of the PRG is preferable as one does not have to
pay particular attention to the key/parameter generation.
It is not hard to see our split-and-amalgamate paradigm can be applied here to immunize the
output of a backdoored PRG. We present a simpliﬁed version that the PRG is stateless, it is not hard
generalize it to allow iterations to capture the Dual_EC PRG, and we defer that to the full version.
35
Moreover, if we allow to split into more pieces, we can immunize the output in the standard model.
Even in the semi-private model with a trusted randomness, the known result uses either random
oracle or UCE, neither of which can be explicitly instantiated so far.
pk
s0
pk
s1
G0spec
G1spec
r0
r1
Φspec
r
Figure 16: Immunizing backdoored PRG.
Security analysis. It is almost the same as the analysis of Theorem 3.1, if we can show (i) For each
sb, it is unpredictable to G1−bimpl; (ii) Gimpl(s0)||Gimpl(s1) is unpredictable to the adversary who makes
the implementations. This is easy to see since the watchdog can check the entropy contained in the
output r0, r1 respectively by feeding PRG with random seed and pk. We defer the proof to the full
version.
Simulating the semi-private model. Semi-private model was introduced in [DGG+15], where a
uniformly random string r is generated by a trusted party and r is made public only after the big
brother provides the implementations. It was shown in [DGG+15] that in such a model, one can
bypass the impossibility of public immunization of backdoored PRG output. We may expect more
interesting results in this model due to the power of this extra trusted randomness.
Using our technique, we could ask the adversary to provide a stego-free speciﬁcation of
randomness generation, for example, we may ask the adversary to implement RG0
impl, Φimpl,
and the user can use them to generate bits that look uniform even to the adversary. In this way, we
can simulate the semi-private model.
impl,RG1
36