input vectors are independent. The 128-bit output of MD5 is then
considered as four independent 32-bit digests which can support
Bloom ﬁlters of dimension up to four. Router implementations re-
quiring higher performance are likely to prefer other universal hash
families speciﬁcally tailored to hardware implementation [11]. A
simple family amenable to fast hardware implementation could be
constructed by computing a CRC modulo a random member of the
set of indivisible polynomials over Z2k .
In order to ensure hash independence, each router periodically gen-
erates a set of k independent input vectors and uses them to select k
digest functions needed for the Bloom ﬁlter from the family of uni-
versal hashes. These input vectors are computed using a pseudo-
random number generator which is independently seeded at each
router. For increased robustness against adversarial trafﬁc, the input
vectors are changed each time the digest table is paged, resulting in
independence not only across routers but also across time periods.
The size of the digest bit vector, or digest table, varies with the
total trafﬁc capacity of the router; faster routers need larger vectors
for the same time period. Similarly, the optimum number of hash
functions varies with the size of the bit vector. Routers with tight
memory constraints can compute additional digest functions and
provide the same false-positive rates as those who compute fewer
digests but provide a larger bit vector.
Figure 8 depicts a possible implementation of a SPIE Data Genera-
tion Agent in hardware for use on high-speed routers. A full discus-
sion of the details of the architecture and an analysis of its perfor-
mance were presented previously [20]. Brieﬂy, each interface card
in the router is outﬁtted with an Interface Tap which computes mul-
tiple independent digests of each packet as it is forwarded. These
digests are passed to a separate SPIE processor (implemented either
in a line card form factor or as an external unit) which stores them
as described above in digest tables for speciﬁc time periods.
Line Cards
SPIE Card (or Box)
S32
S32
S32
S32
S32
...
FIFO
RAM 
MUX
2k-bit RAM
Sk
Ring Buffer DRAM
t
readout 
every 
R ms
+
...
Time
=t
...
Readout 
by 
Control 
Processor
t-P s
Signature Taps
Signature Aggregation
History Memory
Figure 8: A sample SPIE DGA hardware implementation for high-
speed routers.
As time passes, the forwarded trafﬁc will begin to ﬁll the digest ta-
bles and they must be paged out before they become over-saturated,
resulting in unacceptable false-positive rates. The tables are stored
in a history buffer implemented as a large ring buffer. Digest tables
can then be transferred by a separate control processor to SCARs
while they are stored in the ring buffer.
7 ANALYSIS
There are several tradeoffs involved when determining the optimum
amount of resources to dedicate to SPIE on an individual router or
the network as a whole. SPIE’s resource requirements can be ex-
pressed in terms of two quantities:
the number of packet digest
functions used by the Bloom ﬁlter, and the amount of memory used
to store packet digests. Similarly, SPIE’s performance can be char-
acterized in two orthogonal dimensions. The ﬁrst is the length of
time for which packet digests are kept. Queries can only be issued
while the digests are cached; unless requested by a SCAR within a
reasonable amount of time, the DGAs will discard the digest tables
in order to make room for more recent ones. The second is the ac-
curacy of the candidate attack graphs which can be measured in the
number of false positives in the graph returned by SPIE.
Both of these metrics can be controlled by adjusting operational
parameters. In particular, the more memory available for storing
packet digests, the longer the time queries can be issued. Similarly,
digest tables with lower false-positive rates yield more accurate at-
tack graphs. Hence, we wish to characterize the performance of
SPIE in terms of the amount of available memory and digest table
performance.
7.1 False positives
We ﬁrst relate the rate of false positives in an attack graph to the
rate of false positives in an individual digest table. This relationship
depends on the actual network topology and trafﬁc being forwarded
at the time. We can, however, make some simplifying assumptions
in order to derive an upper bound on the number of false positives
as a function of digest table performance.
7.1.1 Theoretical bounds
Suppose, for example, each router whose neighbors have degree
at most d ensures its digest tables have a false-positive rate of at
most P = p/d, where 0 ≤ p/d ≤ 1 (p is just an arbitrary tuning
factor). A simplistic analysis shows that for any true attack graph G
with n nodes, the attack graph returned by SPIE will have at most
np/(1 − p) extra nodes in expectation.
The false-positive rate of a digest table varies over time, depending
on the trafﬁc load at the router and the amount of time since it was
paged. Similarly, if the tables are paged on a strict schedule based
on maximum link capacity, and the actual trafﬁc load is less, digest
tables will never reach their rated capacity. Hence, the analytic re-
sult is a worst case bound since the digest table performs strictly
better while it is only partially full. Furthermore, our analysis as-
sumes the set of neighbors at each node is disjoint which is not true
in real networks. It seems reasonable to expect, therefore, that the
false-positive rate over real topologies with actual utilization rates
would be substantially lower.
For the purposes of this discussion, we arbitrarily select a false-
positive rate of n/7, resulting in no more than 5 additional nodes
in expectation for a path length of over 32 nodes (approaching the
diameter of the Internet) according to our theoretical model. Using
the bound above, p = 1/8 seems a reasonable starting point and we
turn to considering its effectiveness in practice.
7.1.2 Simulation results
In order to relate false-positive rate to digest table performance in
real topologies, we have run extensive simulations using the actual
network topology of a national tier-one ISP made up of roughly 70
backbone routers with links ranging from T-1 to OC-3. We obtained
a topology snapshot and average link utilization data for the ISP’s
network backbone for a week-long period toward the end of 2000,
sampled using periodic SNMP queries, and averaged over the week.
We simulated an attack by randomly selecting a source and vic-
tim, and sending 1000 attack packets at a constant rate between
them. Each packet is recorded by every intermediate router along
the path from source to destination. A traceback is then simulated
starting at the victim router and (hopefully) proceeding toward the
source. Uniformly distributed background trafﬁc is simulated by
selecting a ﬁxed maximum false-positive rate, P , for the digest ta-
ble at each off-path router. (Real background trafﬁc is not uniform,
which would result in slight dependencies in the false-positive rates
between routers, but we believe that this represents a reasonable
starting point.) In order to accurately model performance with real
trafﬁc loads, the effective false-positive rate is scaled by the ob-
served trafﬁc load at each router.
For clarity, we consider a non-transformed packet with only one
source and one destination. Preliminary experiments with multi-
ple sources (as might be expected in a distributed denial of service
(DDoS) attack) indicate false positives scale linearly with respect to
the size of the attack graph, which is the union of the attack paths
for each copy of the packet. We do not, however, consider this case
in the experiments presented here. (A DDoS attack sending iden-
tical packets from multiple sources only aids SPIE in its task. A
wise attacker would instead send distinct packets from each source,
forcing the victim to trace each packet individually.)
s
s
s
s
e
e
e
e
v
v
v
v
i
i
i
i
t
t
t
t
i
i
i
i
s
s
s
s
o
o
o
o
P
P
P
P
e
e
e
e
s
s
s
s
a
a
a
a
F
F
F
F
l
l
l
l
f
f
f
f
o
o
o
o
r
r
r
r
e
e
e
e
b
b
b
b
m
m
m
m
u
u
u
u
N
N
N
N
.
.
.
.
g
g
g
g
v
v
v
v
A
A
A
A
1
1
1
1
0.8
0.8
0.8
0.8
0.6
0.6
0.6
0.6
0.4
0.4
0.4
0.4
0.2
0.2
0.2
0.2
0
0
0
0
0
0
0
0
Theoretical bound
Theoretical bound
Theoretical bound
Theoretical bound
[100% util.] P=1/(8*degree)
[100% util.] P=1/(8*degree)
[100% util.] P=1/(8*degree)
P=1/8
P=1/8
P=1/(8*degree)
5
5
5
5
10
10