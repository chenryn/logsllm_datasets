### System Specifications

**System A:**
- **Memory:** 64GB per node
- **Processors:**
  - Xeon E52670
  - 2x Xeon Phi per node
- **Memory Protection:**
  - ECC-protected CPU and Xeon Phi memory
- **Storage:**
  - 26.4PB, 1.1TB/s
  - Data Disks: RAID 6
  - Index Disks: RAID 1+0
  - OSS: Active-Active HA pair
  - MDS: Active-Passive HA pair
- **Network:**
  - 9.6GB/s, Cray Gemini 3D Torus
  - 16-bit links with adaptive packet CRC, load balancing, quiesce, and reroute
- **Local IO:**
  - 500 GB (SATA), 100MB/s
- **Network IO:**
  - 1.4PB, 23GB/s
  - Data Disks: RAID 6
  - Index Disks: RAID 1+0
  - OSS: Active-Active HA pair
  - MDS: Active-Passive HA pair
  - 5GB/s, Fat Tree, Infiniband with Forward Error Correction (FEC)

**System B:**
- **Nodes:**
  - 22,636XE (CPU only) and 4,228XK (CPU+GPU)
- **Memory:** 64GB per node
- **Processors:**
  - AMD 6276 Interlagos
  - NVIDIA GK110
- **Memory Protection:**
  - Chipkill-protected CPU memory modules
  - ECC-protected GPU memory modules
- **Storage:**
  - 26.4PB, 1.1TB/s
  - Data Disks: RAID 6
  - Index Disks: RAID 1+0
  - OSS: Active-Active HA pair
  - MDS: Active-Passive HA pair
- **Network:**
  - 9.6GB/s, Cray Gemini 3D Torus
  - 16-bit links with adaptive packet CRC, load balancing, quiesce, and reroute
- **Local IO:**
  - 500 GB (SATA), 100MB/s
- **Network IO:**
  - 1.4PB, 23GB/s
  - Data Disks: RAID 6
  - Index Disks: RAID 1+0
  - OSS: Active-Active HA pair
  - MDS: Active-Passive HA pair
  - 5GB/s, Fat Tree, Infiniband with Forward Error Correction (FEC)

**Location:**
- System B is hosted at the University of Illinois at Urbana-Champaign.

### Job Submission Systems

**System A:**
- Jobs can be flagged as shared or non-shared.
- Shared jobs can be executed together with other jobs on the same node.
- Walltime restriction: 336 hours.
- Job termination status (exit code) is captured by TORQUE [71].

**System B:**
- All jobs execute in non-shared mode, without co-location with another job on the same node.
- Walltime restriction: 48 hours.
- Job termination status (exit code) is captured by both TORQUE [71] and ALPS [39].

### Job Characteristics

**1. Job Node-seconds:**
- Product of the number of nodes and the wallclock time (in seconds) for which the job executes.
- On both systems, 50% of the jobs run for less than ~10^3 node-seconds (i.e., less than 16 minutes).
- On System B, jobs can run up to ~10^9 node-seconds (i.e., more than 1 year).

**2. Job Size:**
- Most jobs on both systems are single-node jobs:
  - System A: 85%
  - System B: 74%
- These single-node jobs contribute:
  - System A: 34% by node-seconds
  - System B: 5% by node-seconds
- Scale of jobs submitted on System B is significantly larger, with more than 20% of the jobs (by node-seconds) executing on 2,000 nodes or more.

### Data Description

The data used in this paper falls into three broad categories:
1. **Job Scheduler Accounting Logs:**
   - Both systems use TORQUE [71].
   - Records contain event details, timestamps, submitting user or group, and resources requested and used.
   - For System A, raw TORQUE logs were processed.
   - For System B, Integrated System Console (ISC) [28] is used to parse and store job records and associated metrics.

2. **Resource Utilization Stats:**
   - System A uses TACC Stats [23] with a 5-minute granularity.
   - System B uses LDMS [6] with a 1-minute granularity.

3. **Node Failure Reports:**
   - System A maintains records of planned outages, reboots, and alerts from Sensu [5] and Nagios [4].
   - System B uses ISC [28] for detailed information about each job and node.

### Job Categories Based on Exit Statuses

Exit reasons are classified into the following categories:
1. **Success:** Applications complete without errors.
2. **Walltime:** Applications do not complete within the allocated wall clock time.
3. **User:** Failures due to issues originating from the submitter or developer, including misconfiguration, command errors, missing files, and wrong permissions.
4. **System:** Application termination due to system hardware or software errors.
5. **User/System:** Ambiguous errors that could be due to either system or user issues.

### Implications for System Design

- **System A:**
  - More frequent job failures due to system issues (5.3%).
  - Lower reliability compared to System B.
- **System B:**
  - More reliable with fewer system-related failures (3.6%).
  - Uses advanced resiliency features like chipkill-enabled memory modules.
  - Dedicated maintenance staff for daily monitoring and management.

### Effect of Resource Usage on Job Failures

This section studies the influence of resource usage on job failure due to system errors, considering:
- Memory
- Local and remote I/O
- Network
- Job node-seconds

**Tail-usage:**
- Total amount of resource consumed (or rate of resource consumption for I/O and network) by the job in the last measurement window before failure.
- System A: 5-minute window
- System B: 1-minute window

**Key Findings:**
- Job failure rate is correlated with tail resource utilization rather than aggregate resource utilization.
- Analysis focuses on system-related issues to highlight deficiencies in the underlying system architecture.
- User-related errors are not considered due to their unpredictable nature.

By focusing on tail resource utilization, we aim to better understand the root causes of job failures and identify areas for administrative modifications.