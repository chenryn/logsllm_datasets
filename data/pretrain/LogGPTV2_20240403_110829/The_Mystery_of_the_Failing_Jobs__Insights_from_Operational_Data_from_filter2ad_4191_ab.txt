64GB/node,
Xeon E52670 + 2x Xeon
Phi/node,
ECC-protected
CPU & Xeon Phi memory
System B
22,636XE (CPU only) and
4,228XK (CPU+GPU) nodes,
64GB/node, AMD 6276 In-
terlagos, NVIDIA GK110,
Chipkill-protected CPU mem-
ory modules, ECC-protected
GPU memory modules
26.4PB, 1.1TB/s, Data Disks:
RAID 6, Index Disks: RAID
1+0, OSS : Active-Active HA
pair, MDS: Active-Passive
HA pair
[7], Packet:
9.6GB/s, Cray Gemini 3D
Torus
16-bit
links: adaptive
packet CRC,
load
routers:
balancing,
quiesce and reroute
Local IO 500 GB (SATA), 100MB/s Not present
Network
IO
Data
1.4PB,
23GB/s,
Disks: RAID 6,
Index
Disks: RAID 1+0, OSS
: Active-Active HA pair,
MDS: Active-Passive HA
pair
5GB/s, Fat Tree, Inﬁniband
Forward Error Correction
(FEC)
Network
and System B is hosted at University of Illinois at Urbana-
Champaign.
Job Submission System: In System A, jobs can be ﬂagged
by their submitters as shared or non-shared; the former means
that the job can be executed together with other jobs on the
same node. In System B, all jobs execute in non-shared mode,
without any co-location with another job on the same node.
System B puts a 48-hour walltime restriction, whereas System
A has a restriction of 336 hours. A job termination status (exit
code) is captured by the TORQUE [71] log in both systems,
while for System B, ALPS [39] also captures the exit code of
each application within a job.
Job Characteristics: This section compares and contrasts the
job characteristics of System A and System B in terms of i)
job node-seconds and ii) job size.
1) Job Node-seconds: It is the product of the number of
nodes and the wallclock time (in seconds) for which the
job executes. On both System A and System B, 50% of
the jobs run for less than ∼ 103 node-seconds (i.e., less
than 16 minutes), however on System B the jobs run up
to ∼ 109 node-seconds i.e., more than 1 year (refer
Fig. 1a).
2) Job Size: Most of the jobs on System A and System
B are single-node jobs, 85% and 74% respectively.
However, these jobs contribute just 34% (System A) and
5% (System B) by node-seconds. Furthermore, scale of
jobs submitted on System B is signiﬁcantly larger than
System A where more than 20% of the jobs (by node-
seconds) execute on 2K nodes or more (refer Fig. 1b).
III. DESCRIPTION OF DATA
job scheduler accounting logs,
The data used in this paper falls under three broad cat-
job-level resource
egories:
utilization logs, and node-level health monitoring logs.
Job Accounting: Both System A and System B use
TORQUE [71]. These records contain the event being recorded
(e.g., queuing, job start, job end), corresponding timestamps,
the submitting user or group, and resources requested and
used. For System A, we processed the raw TORQUE logs,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
160
range -11 to 271. A successful job has exit status of 0 and any
other exit status can be either walltime or denotes unsuccessful
job termination, with each exit status value representing a
different error.
Exit reasons are classiﬁed into the following categories: (i)
Success, for applications completing without any errors, (ii)
Walltime, for applications not completing within the allocated
wall clocktime, (iii) User, for applications that fail due to
issues that originate from the submitter of the job or the
developer of the code. These include mis-conﬁguration of job
script or compilation/execution environment setup, job user
action (such as a control-C signal), command errors, missing
module/ﬁle/directory, and wrong permissions, (iv) System,
where an application is terminated due to system hardware or
software errors, and (v) User/System, when it is not possible
to disambiguate whether the error occurred due to system or
user issues, e.g., SIGTERM signal can be issued both by user
(through asssertions) or scheduler (on failing health check).
A job exiting due to walltime does not necessarily mean
loss of production hours. Most of the jobs (especially large-
scale jobs) depend on checkpoint-restart mechanisms to start
from previously checkpointed state. On System B, developers
can trap the kill signal issued by the scheduler on expiry of
requested walltime and write a checkpoint ﬁle before exiting.
Since, there is no information available to us about application-
level checkpointing events, we ignore walltime jobs from the
rest of the study as we cannot disambiguate jobs that wrote a
checkpoint before being terminated (good case and little work
is lost) from the jobs that did not.
Table IV shows the category distribution for both the
systems for different execution environments (shared or non-
shared) and job types (single or multi). On System A, a
signiﬁcant number of jobs failed due to system related errors
(5.3%) whereas most common failure category on System B
is user (3.6%). Only 61.8% (System A) and 64.0% (System B)
of the multi-node jobs in non-shared environment completed
successfully. On System A a higher 87.6% of the shared multi-
node jobs completed successfully but their number is too small
(1.0% overall) to draw any conclusion. Walltime category jobs
contributed to 33% and 43% of total compute hours for System
A and System B respectively. We ﬁnd empirically through a
sub-sampled set that most of these jobs checkpoint frequently.
However, even here, there is possible loss of computation
due to the time gap between the last checkpoint and program
termination by the scheduler.
To investigate further, we also studied the node downtime
and uptime distribution using the node failure reports main-
tained by admins of these systems. We observed System B has
lower continuous downtime (95th percentile value: 24 hours
for System A vs 20 hours for System B) and higher continuous
uptime (95th percentile value: 31 days for System A vs 92 days
for System B).
Implications for System Design: We ﬁnd that on the smaller
scale system (System A), job failures caused by system issues
are more frequent (53% for System A vs 4% for System B
of all failures). This is because System B is more reliable
(a) Comparing job node-seconds (#nodes × wallclock time )
(b) Comparing percentage by node-seconds
Fig. 1: Characteristics of jobs on System A (in brown, left in each
subﬁgure) and System B (in blue, right in each subﬁgure).
however System B uses Integrated System Console (ISC) [28]
to parse and store the job records and its associated metrics
(performance and failure) in its database.
Resource Utilization Stats: System A uses TACC Stats [23]
and System B uses light-weight distributed metric service
(LDMS) [6] for collecting resource utilization values. TACC
stat on System A is conﬁgured to collect data for each node at
5-minute granularity, whereas LDMS is conﬁgured at 1-minute
granularity on System B.
Node Failure Reports: System A maintains a record of
planned system outages, reboots, or alerts on unreachable
nodes from the Sensu [5] and Nagios [4] monitoring frame-
works. System B uses ISC [28] for recording in-depth infor-
mation about each job and node of the system.
IV. JOB CATEGORIES BASED ON EXIT STATUSES
In this analysis, we use the exit code information to ﬁnd
the probable job failure cause and categorize them based on
the approach used in LogDiver [51]. On a TORQUE-based
system, a job upon termination returns an exit code in the
TABLE IV: Job categories based on exit codes. Percentages in
brackets are based on the total node-seconds
A
m
e
t
s
y
S
B
m
e
t
s
y
S
Category
Success
System
User
User/System
Walltime
Total
Success
System
User
User/System
Walltime
Total
Environment & Job Type
shared
non-shared
overall
multi
single
multi
8.8 %
7.2%
6.1%
16.1%
407k
single
93.1% 87.6% 87.6% 61.8 % 86.1% (48.4%)
5.3% (4.0%)
2.7%
3.3%(12.9%)
1.6%
1.3% (1.3%)
0.6%
2.0%
4.0% (33.4%)
1,125k
6.5%
3.5%
0.4%
2.0%
1,348k
91.6% 64.0% 84.4% (44.4 %)
0.10%
3.8%
1.2%
3.7%
1,640k
0.3% (1.4%)
3.6% (2.7%)
3.6% (8.0%)
8.0% (43.4%)
6.5%
2.2%
0.2%
3.5%
28k
-
-
-
-
-
-
1.0%
3.0%
0.8%
20.4%
579k
2,219k
2,908k
-
-
-
-
-
-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
161
by design. It uses expensive Cray HPC solutions and has
better resiliency features compared to System A (such as use
of chipkill-enabled memory modules over ECC). Additionally,
unlike System A, System B has dedicated maintenance staff
who monitor and manage system on a daily basis.
V. EFFECT OF RESOURCE USAGES ON JOB FAILURES
The section studies the inﬂuence of resource usage
on job failure due to system errors. We consider the 5
primary kinds of resources, both local and remote, namely,
memory, local and remote IO, network, and job node-seconds.
We calculate failure rate after removing all debug as well
as walltimed jobs. We remove the walltimed jobs because
these are not considered failed jobs and debug jobs are not
representative of production workloads. We deﬁne job failure
rate as the fraction of jobs that fail due to system-related
issues. Here we focus on system-related issues to reveal any
deﬁciencies in the underlying system architecture. Examples
are: insufﬁcient main memory per node, unsuitable ﬁle system,
slow remote ﬁle system, or insufﬁcient network speed. The
purpose of the analysis is to highlight which modules in the
system architecture is causing job failures and hence requires
administrative modiﬁcations. User-related errors happen due to
factors that have no discernible pattern, such as, correctness
bugs or misconﬁgurations in the user code. Hence, we do not
consider job failures due to user or user/system issues.
Tail-usage: It is the total amount of resource consumed (or
rate of resource consumption for I/O and network) by the job
in the last measurement window before failure. For System A
measurement window is ﬁve minutes whereas for System B
the measurement window is one minute.
Errors in high-speed HPC systems quickly propagate
through the system, hence resource utilization values shortly
before application failure provide a better understanding of
the failure reason than resource utilization throughout
the
application run. We empirically validate that the job failure
rate is correlated with the tail resource utilization rather than
aggregate resource utilization (if there is any relation between
that resource and failures). Therefore,
in this section, all
analyses are conducted using tail resource utilization values.