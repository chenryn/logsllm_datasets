which describes a combination of adversarial capabilities.
Note that when combined with a claim to form a security prop-
erty, threat models are implicitly negated, see Section 3.3.2.
Thus a security property is a statement that the protocol pro-
vides the guarantees of the claim C we consider, unless the
adversary has access to the capabilities described in t. We
have already deﬁned the claims we consider in the previous
subsection. We now describe how we formulate the threat
model t.
3.3.1 Adversary Capabilities
Intuitively, our threat models can each be expressed as a com-
bination of atomic adversarial capabilities. We summarize
these capabilities in Figure 3 and explain their meaning here.
The symbol active denotes that the adversary is active. A
passive adversary can only read, drop, and reorder messages,
but not modify, send, or replay messages. R denotes a reveal
or compromise of some key, and comes in two ﬂavors: one
where the reveal occurs before the time of the claim (e.g.,
R<
psk) and one where the reveal can occur at any time (e.g.,
Rpsk). D refers to dishonest key generation. Namely, Dpki
expresses that the keys received by anyone as pre-messages,
for instance through a PKI, can be dishonestly generated;
i.e., no assurance is provided of their well-formedness and
received keys can be, e.g., gs−1 or g. Drs expresses that the
peer’s static public key could be dishonestly generated.
Note that there is no Ds or De as we assume that the actor’s
private static and ephemeral keys were honestly generated.
However, we only make this assumption for the actor, that
is the honest agent for which a security guarantee must be
provided, and not for other actors, most notably the actor’s
peer (see Drs, Dre, and Dpki).
These capabilities capture a realistic class of adversarial
USENIX Association
29th USENIX Security Symposium    1865
capabilities, including the ability to compromise the private
state of the local or remote party, interfere with the application
layer authentication system, and register malicious agents
with the adversary’s choice of key. In [17, Section 2.2.4], we
provide a formal interpretation in our model of each of these
capabilities, which we lack the space to explore here.
Anonymity claims: The anonymity claims are substantially
more complex to model and analyze as they rely on obser-
vational equivalence (see Section 2.2). Such properties are
well-known to be computationally much more expensive to
analyze than trace properties.
For this reason, we analyze anonymity claims with re-
spect to a strict subset of adversary capabilities, namely
Aa = {Rrs,Rpsk, active} instead of A. The adversary does
not have access to other capabilities: ephemeral keys cannot
be revealed, the PKI is honest, and the peer of the role whose
identity we try to hide always receives honest static keys. We
also assume that there is at most one initiator and one recipi-
ent, and that Difﬁe Hellman operations are implemented on a
prime order group.
Albeit strict, these restrictions still allow us to gain useful
insights about Noise’s anonymity guaranties as we will see in
Section 5.4.
3.3.2 Threat Models
We now model an adversary who possesses a given subset
of these capabilities, including all or none of them. We ﬁrst
describe how these capabilities can be combined into a threat
model and afterward how they can be used to evaluate a secu-
rity property.
Deﬁnition 3. Let A be the set of adversary capabilities given
in Figure 3. We deﬁne the set of threat models, denoted by
T , to be the (subset of) propositional logic formulas, built
from A, ∧, ∨, and the bottom element ⊥, which represents
an empty threat model.
Note that when combined with a claim to form a security
property, threat models are implicitly negated. That is, a threat
model does not describe the adversary’s permitted capabili-
ties, but rather its excluded capabilities, i.e., the threat model
determines under what circumstances the claim is not required
to hold. Hence the empty threat model ⊥ affords the attacker
the most power as the claim must hold in all circumstances
(and vice versa for the maximal threat model). However, not
all combinations of capabilities are meaningful. For example,
rs ∧ Rrs is intuitively equivalent to threat model R<
R<
rs as re-
vealing the key prior to the claim also satisﬁes the requirement
to reveal the key at any point. We deﬁne a notion of redun-
dancy that we use to eliminate such redundant threat models.
Deﬁnition 4. We deﬁne (cid:22) to be the smallest reﬂexive and
x (cid:22) Rx
transitive relation over threat models containing: R<
for x ∈ {e, re, s, rs, psk}, Dpki (cid:22) Drs, and Dx (cid:22) active,
for x ∈ {rs, re, pki}.
For t1,t2 ∈ T , we say that t1 subsumes t2 when t1 (cid:22) t2.
We can use this to reduce the number of relevant threat
models using the following result.
Theorem 1. Let C be a claim, and t1,t2 ∈ T be such that
t1 (cid:22) t2. If C holds in threat model t1 then it also holds in t2.
Our ordering (cid:22) induces a partial order on the set of threat
models T , and thus yields an equivalence relation (cid:39) deﬁned
as t1 (cid:39) t2 when t1 (cid:22) t2 and t2 (cid:22) t1. We denote as ¯T the quotient
of the set of threat models by (cid:39). The set ¯T represents the set
of distinct threat models that we will consider.
¯T is still large: it contains more than 1012 elements. Al-
though this indicates how ﬁne-grained our analysis is, this
large number poses two problems. First, the raw results of
evaluating these threat models against each claim would be
beyond human comprehension. Therefore, we develop a tech-
nique to condense these results into a single summary state-
ment without any loss of precision. Second, evaluating all
of these threat models would take substantial computational
resources. We address this problem in Section 5.
3.4 Finding the Strongest Threat Model
We now show that for a given claim, there exists a unique
element of ¯T that subsumes exactly those threat models under
which the claim holds. This allows us to summarize succinctly
the conditions under which a claim holds.
Theorem 2 (Strongest threat model). Let C be a claim. Let
¯T1(C) be the set of threat models under which C is true. There
exists a unique element in ¯T1(C) that subsumes all other threat
models in ¯T1(C). We denote this element by B(C) and call it
the Strongest Threat Model (STM) for C.
Without loss of generality, we can represent the unique
STM by a representative of B(C) in Disjunctive Normal Form
(DNF), wherein there are sequence of clauses connected by
disjunctions (∨), and each clause is composed of conjunc-
tions (∧) of adversarial capabilities. Each clause corresponds
directly to a minimal set of capabilities required for the adver-
sary to violate the security claim. Informally we will refer to
this representative in DNF as the STM.
Example 4. Secrecy of the third payload of I1N from the
perspective of the initiator (called claimer) holds under the
strongest threat model: Rre∨Dre∨ (Re∧Rs). This is equiva-
lent to the following statement, where p denotes the payload:
Trigger(p) =⇒ Secret(p)∨ (Rre ∨ Dre ∨ (Re ∧ Rs))
Which means one of the following must be true:
• The secrecy claim on the payload p holds, i.e., Secret(p).
• The adversary compromised the peer’s ephemeral key.
• The peer’s ephemeral key was generated by the adversary.
• The adversary compromised both the
claimer’s
ephemeral and static key (as modeled in the conjunct
Re ∧ Rs).
1866    29th USENIX Security Symposium
USENIX Association
These cases cover all possible attacks using combinations
of atomic adversarial capabilities. Furthermore, each case
is minimal, e.g., in the fourth case it must be that no attack
is possible if the adversary only compromises the claimer’s
ephemeral key (Re) but not the claimer’s static key (Rs).
Consequently, for each claim C (secrecy, non-injective
agreement, injective agreement, and anonymity), we can con-
dense the result to a single threat model B(C), which sum-
maries the exact capabilities the adversary needs to violate
the property given by C. This reduces our set of results for all
claims to where they can be inspected by hand.
We now exemplify how this choice of threat models, com-
bined with security claims, is expressive enough to encode
well-known standard security notions.
rs ∨ R<
Example 5. Secrecy under R<
s captures a form of PFS
where payload secrecy holds unless the actor’s or the actor’s
peer’s static, private key is compromised before the claim.
Example 6 (KCI resistance). Key Compromise Imperson-
ation (KCI) resistance can be modeled as injective agreement
under the threat model Re ∨ Rre ∨ Dre ∨ Rrs ∨ Drs. In
plain English, agreement holds unless the actor’s ephemeral
key is compromised, or an asymmetric key of the actor’s
peer is either compromised or was in fact generated by the
adversary. Hence even if the actor’s static key is compromised,
agreement still holds.
However, we must still compute this STM. Naively, a brute
force strategy enumerating all threat models in ¯T would suf-
ﬁce, where we submit all proof obligations as lemmas to
Tamarin for each claim. However, this would yield more than
1012 proof obligations per handshake, message, and security
claim. We reﬁne this approach so that computation is man-
ageable in Section 4.1.
4 Vacarme
In the previous section, we described the security properties
we consider. We now present our tool, called Vacarme, which
is available at [18], and how we evaluated it.
Vacarme can take any two-way Noise pattern and computes
the STM for each of its messages and security claim. Vacarme
builds upon Tamarin [25] by ﬁrst converting the pattern into
a set of Tamarin proof obligations, running Tamarin on them,
and ﬁnally analyzing the results.
4.1 Performance optimizations
Our methodology involves generating one Tamarin proof obli-
gation (lemma) for each claim and each threat model. How-
ever, as noted previously, a naive brute force approach in-
voking Tamarin for each of them would require prohibitive
computational resources. Instead, we use several techniques
to reduce the overall computation time.
First, we employ static analysis to reduce the number of
proof obligations required (Section 4.1.1). A runtime frame-
work, described in Section 4.1.2, uses dynamic analysis to
minimize the invocations to Tamarin given the results of al-
ready examined proof obligations. To further reduce analysis
time, we developed a dedicated, but handshake-independent,
provably sound Tamarin heuristic, which reduces Tamarin’s
proof search, that we describe in Section 4.1.3.
4.1.1 Static Analysis
We start with several a priori observations that reduce the
number of Tamarin invocations.
Threat Models and Handshakes: Taking the quotient of T
by (cid:39) reduces the number of distinct conjunctions of atomic
capabilities in A from 16,384, to 1,701. We explain and prove
in [17] how and why results for conjuncts are enough to
compute the STMs. We can also consider how elements of A
interact with the claim under consideration. For example, if a
claim considers a point in the protocol where an ephemeral
key for a party has not yet been instantiated, we need not
consider this key’s reveal. Similarly, where the handshake
pattern has no pre-messages, Dpki gives the adversary no
additional power.
Threat Models and Claims: Next, we note that when ana-
lyzing agreement, a passive adversary cannot make use of any
knowledge gained to affect the views or actions of the other
participants, because they cannot insert their own messages.
Hence we need not consider key reveals whilst considering
agreement for passive adversaries. Similarly, for a passive ad-
versary attempting to violate a secrecy property, the timing of
a key reveal does not change the adversary’s ultimate knowl-
edge set, which means that there is no difference between the
timed and untimed variants of a reveal, and we can infer the
result for one by evaluating the other. Furthermore, revealing
a key after a claim does not increase the adversary’s ability
to violate non-injective agreement at the time of the claim.
Trivial Attacks: In many threat models, the adversary
may have enough knowledge to perform a trivial attack
on a handshake. For example, if the adversary completely
compromises the peer’s state, then secrecy and agreement
properties no longer hold. Similarly, if the adversary learns
any PSKs present and at least one private key for each Difﬁe
Hellman operation, they can compute the session key. We
generalize these observations into a wider category of trivial
attacks, which are important for our tool’s efﬁciency. For
such cases, we can immediately conclude that the claim is
false based on a simple static analysis.
The above observations allow us to immediately infer that
over 99% of proof obligations are false. This leaves us with,
on average, only 63 proof obligations per remaining claim,
as opposed to the 16,384 naive ones. There is a varying
number of claims per pattern, depending on, e.g., its number
of messages. Overall, this leaves us with about 410,000 proof
USENIX Association
29th USENIX Security Symposium    1867
obligations for 53 patterns. Note that we show in [17] that
all our reductions are sound in that we have formally proven
that they not impact the actual results.
4.1.2 Dynamic Analysis
Our static analysis techniques substantially reduce the num-
ber of proof obligations, but the required computational effort
would still be substantial. However, there are further rela-
tionships between these tasks that we can exploit. We lift our
deﬁnition of subsumption (Deﬁnition 4) from threat models to
proof obligations in the natural way. Namely, the subsumption
relation is the smallest reﬂexive, transitive relation such that:
• if t (cid:22) t(cid:48) (t is a stronger threat model than t(cid:48)) then for any
claim C, the lemma ‘C holds in t’ subsumes the lemma C
holds in t(cid:48);
• for any given threat model, message, and set of keys,
injective agreement subsumes non-injective agreement;
and
• for any given threat model and message, if S and S(cid:48) are
sets of keys where S ⊆ S(cid:48), then non-injective agreement
on this message and keys S(cid:48) subsumes non-injective agree-
ment on the same message and keys S.
The resulting subsumption over-approximates, but does not
coincide with, entailment, i.e., for lemmas P1,P2, if subsump-
tion relates P1 and P2, and P1 is true then P2 must also be true.
We can now consider the proof obligations (written as
Tamarin lemmas, combining a claim and a threat model) we
submit to Tamarin as nodes in a directed acyclic graph, whose
edges are determined by the subsumption relationship which
we can statically compute. In order to calculate the STM
under which a claim holds, we must label each node in this
graph with True or False. However, we can use the subsump-
tion relationship to speed up this labeling. For example, if a
property is true, then all weaker properties must also be true.
Likewise, if there is a counterexample for a property, then
that counterexample also holds for any stronger property.
We still have to choose a tree traversal strategy, i.e., in
which order we perform the individual proof obligations.
We designed a dedicated heuristic that approximates the ex-
pected payoff, i.e., how many tasks we could save from anal-
ysis. Overall, this reduces the number of proof obligations
from 410,000 to 150,000. Again, the reduction is provably
sound [17].
of open constraints in order to determine which constraints
should be prioritized. We improve upon that by additionally
examining the entire constraint system from a global view.
Thanks to this extra ﬂexibility, we are able to design a
heuristic that delays the introduction of new identities or ses-
sions into a trace. We ensure that all constraints concerning
the already present sessions are ﬁrst satisﬁed, before we con-
sider constraints that might require the introduction of a new
party. This ensures that we ﬁnd straightforward contradictions
early on, before investigating more complex scenarios. In this
work, we can ensure this condition syntactically by inspecting
the constraints output by Tamarin.
We stress that our oracle is handshake-independent and
does not impact the validity of our results: Tamarin remains
sound, and for trace properties, complete.
4.2 Toolchain and Evaluation
Toolchain: Vacarme’s core consists of 5k lines of Rust. First,
a generator converts any Noise pattern into a set of Tamarin
input ﬁles that describe the protocol and all proof obligations
using aforementioned static analysis. Given these Tamarin
models, our runtime framework, a combination of Python
and bash scripts, runs Tamarin with our oracle using the
aforementioned dynamic analysis. The complete toolchain is
push-button: given any Noise handshake, written in the Noise
syntax as in Figure 1, or any Noise handshake name from
the speciﬁcation, it returns a table of STMs for all claims
and messages. We also provide tool support to interpret the
results and compare handshakes, as explained in Section 5.
Evaluation: While Vacarme can take an arbitrary two-way
Noise pattern as input, we ran it on all such patterns that are
listed in the speciﬁcation, both for evaluating our tool and for
interpreting the analysis results. To determine the STM for
secrecy and agreement properties for the 53 two-way Noise
patterns described in the speciﬁcation, Vacarme required a
total of 150,000 lemma evaluations, requiring 74 CPU-days
on cores ranging from 2.2 to 2.6 GHz, and a peak requirement
of 75 GB RAM. Anonymity proofs for a relevant subset of
46 patterns took another 97 CPU-days, with a peak memory
usage of 125 GB RAM. The complete results and the tool
to reproduce them are available at [18], and a large subset is