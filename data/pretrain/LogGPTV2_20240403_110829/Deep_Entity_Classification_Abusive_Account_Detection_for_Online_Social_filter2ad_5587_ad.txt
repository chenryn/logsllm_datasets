from the second stage.
6.3.1 First Stage: Low Precision Training
The objective of the ﬁrst training stage is to reduce the high-
dimensional vector of aggregated raw deep features to a low-
dimensional embedding vector. This dimensionality reduction
is done through the training of a multi-task deep neural net-
work model [6] using our approximate label data. Each sample
in the training data has a vector of labels where each label
corresponds to a task, and each task corresponds to classiﬁca-
USENIX Association
30th USENIX Security Symposium    4105
tion of a sub-type of abusive accounts on Facebook. After the
training has converged, we take the outputs of the last hidden
layer of the neural network as the learned low-dimensional
embeddings.
For our implementation, we use a neural network model
with 3 fully connected hidden layers having 512, 64, and
32 neurons respectively. For each task, the model outputs a
probability using a sigmoid activation function. The inputs
are normalized using a Box-Cox transformation. We trained
the model using PyTorch [42] for an epoch using per-task
binary cross entropy and an Adagrad optimizer [11], with a
learning rate of 0.01.
6.3.2 Second Stage: High Precision Training
We leverage a technique from transfer learning [41] and ex-
tract the last hidden layer’s output from the ﬁrst stage model
as the input for the second stage. We train the second stage
(GBDT model) with high precision human-labeled data to
classify abusive accounts regardless of the sub-types of vio-
lations. The scores output by the GBDT model are the ﬁnal
DEC classiﬁcation scores.
Our implementation of the GBDT model uses an ensemble
of 7 trees with a maximum depth of 4. We trained the model
with a company-internal gradient boosting framework similar
to XGBoost [56], using penalized stochastic gradient boosting,
with a learning rate of 0.03 and a feature sampling rate of 0.2.
7 Evaluation
In this section we evaluate the performance of our MS-MTL
approach and the DEC system as a whole. Speciﬁcally we
analyze three abusive account models:
1. A behavioral-only model, which represents traditional de-
tection techniques employed by OSNs;
2. DEC as a single multi-task neural network (“Single Stage,”
SS), and
3. DEC with MS-MTL.
We performed our evaluation on active accounts on Face-
book. These accounts have already gone through multiple
early-stage security systems such as registration or login-time
actioning, but have not yet gone through full behavioral (i.e.,
activity- and content-based) detection. We also investigate
adversarial adaptation, in particular looking at the stability of
DEC’s precision and recall over time.
7.1 Datasets
Table 3 summarizes the dataset used for our experiments and
evaluation of DEC.
Training Data. We test DEC’s performance on production
Facebook data. We consider four types of abusive accounts
(tasks) in our MS-MTL implementation: fake, compromised,
spam, and scam. We split the abuse types into these four differ-
ent categories for two reasons. First, they are violating differ-
ent policies of Facebook, which causes the detected accounts
Table 3: Datasets: Number and composition of labels used
for our training and evaluation. The longitudinal dataset is
measured in # of samples per day.
Training
Dataset
Fake
Comp.
Spam
Scam
Benign
Abusive
Benign
Evaluation
Dataset
Abusive
Benign
Longitudinal
Label Type
Approximate
Approximate
Approximate
Approximate
Approximate
Human
Human
Label Type
Human
Human
Human
Training
Stage
First
First
First
First
First
Second
Second
# Samples
3.0× 107
7.8× 105
6.2× 105
6.2× 105
2.6× 108
1.2× 105
1.2× 105
Evaluation
Mechanism # Samples
3.0× 104
3.0× 104
2.0× 104/day
Ofﬂine
Ofﬂine
Online
to be actioned on by separate enforcement systems, each em-
ploying distinct appeals ﬂows. Second, the positive samples
of different abuse types are not homogeneous by nature. For
example, fake accounts are largely driven by scripted creation,
while compromised accounts usually result from malware or
phishing. The behavioral patterns and social connections of
these accounts are distinctive for each abuse type, lending
themselves well to different “tasks” in our formulation.
We maintain separate datasets of approximate (lower-
precision) and human labels. The quantity of approximate
labels is signiﬁcantly larger than human labels. The ﬁrst train-
ing stage uses four approximate datasets of abusive accounts
and one of benign accounts, while the second stage requires
only human-reviewed accounts labeled as abusive or benign.
The approximately labeled data comes from three sources:
1. User reports: Users on Facebook can report other users
as abusive. This source is noisy [19], but appropriate as
low-precision labels for the ﬁrst stage of training.
2. Rule-based systems: Outside of DEC, there are other
existing enforcement rules on Facebook. We take users
caught by these enforcements, categorized by the type of
abuse, as an additional approximate label source. Some
examples of users labeled by rule-based systems include:
• Users sending friend requests too quickly;
• Users with multiple items of content deleted by spam-
• Users distributing links to known phishing domains.
In total, rule-based systems account for more than half of
our abusive account labels.
detection systems;
3. Discovered attacks: It’s common to have “waves” of
scripted attacks on OSNs, such as malware or phishing
attacks. When Facebook notices such a wave they can
identify a “signature” for the accounts involved and use
the signature as an approximate label for our ﬁrst stage.
These discovered attacks comprise approximately 10% of
our abusive account labels.
All of the above sources provide noisy, low-precision abuse
data. While inappropriate for full system training, they are apt
4106    30th USENIX Security Symposium
USENIX Association
for the ﬁrst stage of training. For the ﬁrst stage, we construct
a set of benign users by randomly sampling active users and
excluding those contained in approximate abuse dataset.
In contrast, we generate training data for the second stage
by having human labellers employed by Facebook manually
review randomly sampled users on the platform. Accounts
labeled as abusive are used as positive samples for training,
and accounts labeled as benign are negative samples.
Evaluation Data. To evaluate DEC’s performance, we create
an evaluation dataset of accounts by sampling active users
from Facebook. These are users that have already passed
through several early-stage abuse detection systems, and as
such contain the hardest abusive accounts to classify. We per-
form manual human labeling of a large number of randomly
selected accounts using the same methodology and process
that Facebook uses for ground truth measurement. We then
randomly select 3× 104 accounts labeled abusive and 3× 104
accounts labeled benign for ofﬂine evaluation.
7.2 Model Evaluation
We use three different models to evaluate the performance of
our DEC approach (single stage and with MS-MTL) both in
isolation, and in comparison to traditional techniques. Note
that the objective of DEC is to identify accounts committing
a wide spectrum of abuse types. This approach goes beyond
traditional Sybil defense techniques which primarily focus on
detecting fake accounts.
A summary of these models, their training data, and their
evaluation data can be found in Table 4. The three models we
compare are:
1. Behavioral: This GBDT model classiﬁes accounts based
only on the direct behavioral features of each account (e.g.,
number of friends), and outputs whether the account is
abusive (regardless of the speciﬁc abuse type). Thus, this
model does not use deep features and is not multi-task.
Since the number of behavioral features is relatively small,
we train the model with the human labeled dataset. This
model is representative of traditional ML based detection
techniques used in OSNs, similar to the system described
by Stein et al. [48]. By operating on an evaluation dataset
drawn from active accounts on Facebook that have already
undergone early-stage remediation, adding this behavioral
(later-stage) system is representative of an end-to-end so-
lution. We employ a GBDT architecture with an ensemble
of 200 trees of depth of 16, each with 32 leaf nodes.
2. DEC-SS: This model uses the DEC approach outlined in
this paper to extract deep features, but does not leverage the
MS-MTL learning approach. A single deep neural network
model is trained by combining all the approximate data
across multiple tasks. If a user is identiﬁed as violating
by any one of the included tasks, we consider this as a
positive sample. Because of the huge number of features
extracted by DEC, the quantity of human labeled data is
too small to be used for training.
Figure 4: Comparison of ROC curves for different models
on evaluation data. Both DEC models (single stage and with
MS-MTL) perform signiﬁcantly better than the behavioral
model at all points in the curve.
3. DEC-MS-MTL: This is is the complete end-to-end frame-
work and model described in Section 6. It combines the
DEC-only approach with MS-MTL.
Outside of this evaluation section, references to DEC without
a MS-MTL or SS qualiﬁer refer to DEC MS-MTL.
7.3 Performance Comparisons
We compare various metrics based on the results of above
three models.
7.3.1 ROC Curves
Figure 4 examines the ROC performance of all three models.
ROC curves capture the trade-off in a classiﬁer between false
positives and false negatives. For all operating points on the
curve, the DEC models (both MS-MTL and SS) perform
signiﬁcantly better than a behavioral-only approach—by as
much as 20%, depending on the operating point. From a ROC
perspective, both DEC models perform similarly.
While ROC curves are important measures of the effective-
ness of models, they are inherently scaleless, as the x-axis
considers only ground-truth negatives and the y-axis considers
only ground-truth positives. If the dataset is being classiﬁed is
imbalanced, as is the case with abusive accounts (there are sig-
niﬁcantly more benign accounts than abusive accounts), ROC
curves may not capture the actual operating performance of
classiﬁcation systems—particularly precision, a critical mea-
sure in accessing abuse detection systems.
7.3.2 Precision and Recall
Figure 5 compares the precision and recall of the models.
We ﬁnd the behavioral model is unable to obtain precision
above 0.95 and has very poor recall throughout the precision
range. Both DEC models perform signiﬁcantly better than the
behavioral model, being able to achieve a higher precision
and have signiﬁcantly higher recall at all relevant operating
USENIX Association
30th USENIX Security Symposium    4107
Table 4: Comparisons of the three evaluation models’ type, training features, training data, and evaluation data.
Name
Behaviorial
DEC- SS
Model
GBDT
Multi-Task DNN
DEC- MS-MTL Multi-Task DNN + GBDT
Training Features
Account behavior features (∼ 102)
DEC deep features (∼ 104)
DEC deep features (∼ 104)
Training Data
Human labels
Approximate labels
Approximate labels+human labels
Evaluation Data
Human labels
Human labels
Human labels
Table 5: Comparison of the area under the curve (AUC) and
recall at precision 0.95 for different models on evaluation data.
The DEC with MS-MTL model achieves the best result by a
signiﬁcant margin, a nearly 30% absolute improvement. The
behavioral model is unable to obtain precision 0.95.
Model
Behavioral
DEC- SS
DEC- MS-MTL
AUC Recall @ Precision 0.95
0.81
0.89
0.90
NA
0.22
0.50
DEC both single stage and with MS-MTL have similar AUC
performance, adding MS-MTL more than doubles the model
recall, increasing it from 22% to 50%. This increased perfor-
mance, both over behavioral and over DEC without MS-MTL,
enables signiﬁcantly better real-world impact when deployed
in production.
7.4 Results In Production Environment
Building on our design and evaluation of DEC (with MS-
MTL), we deployed the system into production at Facebook.
The system not only identiﬁed abusive accounts, but also trig-
gered user-facing systems to take action on the accounts iden-
tiﬁed. To assess the model’s real-world impact and longevity,
we evaluate our system in production by looking at the stabil-
ity of precision and recall over time.
Precision Over Time. Figure 6 examines the 3-day moving
average of the precision of our DEC with MS-MTL system
in production at Facebook. As with our prior evaluation, we
obtain ground truth for our measurements by relying on man-
ual human labeling of a random sample of accounts classiﬁed
as abusive by DEC. We ﬁnd that the precision of the system
is stable, with the precision never dropping below 0.97, and
frequently being higher than 0.98.
Recall Over Time. We examine the stability of our produc-
tion DEC-MS-MTL model’s recall by considering its false
negative rate (FNR), where FNR = 1−recall. Using a longitu-