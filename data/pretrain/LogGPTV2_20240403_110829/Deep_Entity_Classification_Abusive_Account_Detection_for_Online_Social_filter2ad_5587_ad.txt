### 6.3.1 First Stage: Low Precision Training

The primary objective of the first training stage is to reduce the high-dimensional vector of aggregated raw deep features to a low-dimensional embedding vector. This dimensionality reduction is achieved by training a multi-task deep neural network model [6] using our approximate label data. Each sample in the training data is associated with a vector of labels, where each label corresponds to a specific task. Each task, in turn, corresponds to the classification of a sub-type of abusive accounts on Facebook. Once the training has converged, we use the outputs of the last hidden layer of the neural network as the learned low-dimensional embeddings.

For our implementation, we utilized a neural network model with three fully connected hidden layers, containing 512, 64, and 32 neurons, respectively. For each task, the model outputs a probability using a sigmoid activation function. The inputs are normalized using a Box-Cox transformation. We trained the model using PyTorch [42] for one epoch, employing per-task binary cross-entropy loss and an Adagrad optimizer [11] with a learning rate of 0.01.

### 6.3.2 Second Stage: High Precision Training

In the second stage, we employ a transfer learning technique [41] and use the output from the last hidden layer of the first-stage model as input. We then train a Gradient Boosting Decision Tree (GBDT) model with high-precision, human-labeled data to classify abusive accounts, regardless of the sub-types of violations. The scores output by the GBDT model are the final DEC classification scores.

Our GBDT model uses an ensemble of seven trees with a maximum depth of four. We trained the model using a company-internal gradient boosting framework similar to XGBoost [56], with penalized stochastic gradient boosting, a learning rate of 0.03, and a feature sampling rate of 0.2.

### 7 Evaluation

In this section, we evaluate the performance of our Multi-Stage Multi-Task Learning (MS-MTL) approach and the DEC system as a whole. Specifically, we analyze three abusive account models:

1. A behavioral-only model, which represents traditional detection techniques employed by Online Social Networks (OSNs);
2. DEC as a single multi-task neural network ("Single Stage," SS); and
3. DEC with MS-MTL.

We performed our evaluation on active accounts on Facebook. These accounts have already passed through multiple early-stage security systems such as registration or login-time actioning but have not yet undergone full behavioral (i.e., activity- and content-based) detection. We also investigate adversarial adaptation, particularly the stability of DEC’s precision and recall over time.

#### 7.1 Datasets

Table 3 summarizes the datasets used for our experiments and evaluation of DEC.

**Training Data:**
- We test DEC’s performance on production Facebook data.
- We consider four types of abusive accounts (tasks) in our MS-MTL implementation: fake, compromised, spam, and scam.
- We split the abuse types into these four different categories for two reasons:
  - They violate different policies of Facebook, leading to separate enforcement systems and distinct appeals flows.
  - The positive samples of different abuse types are not homogeneous. For example, fake accounts are often created by scripts, while compromised accounts result from malware or phishing. The behavioral patterns and social connections of these accounts are distinctive for each abuse type.

**Table 3: Datasets: Number and composition of labels used for our training and evaluation. The longitudinal dataset is measured in # of samples per day.**

| **Training Dataset** | **Fake** | **Comp.** | **Spam** | **Scam** | **Benign** | **Abusive** | **Benign** |
|----------------------|----------|-----------|----------|----------|------------|-------------|------------|
| **Evaluation Dataset** | **Abusive** | **Benign** | **Longitudinal** |
| **Label Type** | **Approximate** | **Approximate** | **Approximate** | **Approximate** | **Approximate** | **Human** | **Human** |
| **Label Type** | **Human** | **Human** | **Human** |
| **Training Stage** | **First** | **First** | **First** | **First** | **First** | **Second** | **Second** |
| **# Samples** | 3.0×10^7 | 7.8×10^5 | 6.2×10^5 | 6.2×10^5 | 2.6×10^8 | 1.2×10^5 | 1.2×10^5 |
| **Evaluation Mechanism # Samples** | 3.0×10^4 | 3.0×10^4 | 2.0×10^4/day | **Offline** | **Offline** | **Online** |

We maintain separate datasets of approximate (lower-precision) and human-labeled data. The quantity of approximate labels is significantly larger than human-labeled data. The first training stage uses four approximate datasets of abusive accounts and one of benign accounts, while the second stage requires only human-reviewed accounts labeled as abusive or benign.

The approximately labeled data comes from three sources:
1. **User reports:** Users can report other users as abusive. This source is noisy [19] but appropriate as low-precision labels for the first stage of training.
2. **Rule-based systems:** Outside of DEC, there are other existing enforcement rules on Facebook. We take users caught by these enforcements, categorized by the type of abuse, as an additional approximate label source. Examples include:
   - Users sending friend requests too quickly.
   - Users with multiple items of content deleted by spam filters.
   - Users distributing links to known phishing domains.
   - In total, rule-based systems account for more than half of our abusive account labels.
3. **Discovered attacks:** It is common to have "waves" of scripted attacks on OSNs, such as malware or phishing attacks. When Facebook notices such a wave, they identify a "signature" for the accounts involved and use the signature as an approximate label for our first stage. These discovered attacks comprise approximately 10% of our abusive account labels.

All of the above sources provide noisy, low-precision abuse data. While inappropriate for full system training, they are suitable for the first stage of training. For the first stage, we construct a set of benign users by randomly sampling active users and excluding those contained in the approximate abuse dataset.

For the second stage, we generate training data by having human labelers employed by Facebook manually review randomly sampled users on the platform. Accounts labeled as abusive are used as positive samples for training, and accounts labeled as benign are negative samples.

**Evaluation Data:**
To evaluate DEC’s performance, we create an evaluation dataset of accounts by sampling active users from Facebook. These are users that have already passed through several early-stage abuse detection systems, making them the hardest abusive accounts to classify. We perform manual human labeling of a large number of randomly selected accounts using the same methodology and process that Facebook uses for ground truth measurement. We then randomly select 3×10^4 accounts labeled as abusive and 3×10^4 accounts labeled as benign for offline evaluation.

#### 7.2 Model Evaluation

We use three different models to evaluate the performance of our DEC approach (single stage and with MS-MTL) both in isolation and in comparison to traditional techniques. Note that the objective of DEC is to identify accounts committing a wide spectrum of abuse types, going beyond traditional Sybil defense techniques which primarily focus on detecting fake accounts.

A summary of these models, their training data, and their evaluation data can be found in Table 4. The three models we compare are:

1. **Behavioral:**
   - **Model:** GBDT
   - **Training Features:** Account behavior features (∼10^2)
   - **Training Data:** Human labels
   - **Evaluation Data:** Human labels
   - This model classifies accounts based only on direct behavioral features (e.g., number of friends) and outputs whether the account is abusive (regardless of the specific abuse type). It does not use deep features and is not multi-task. We train the model with the human-labeled dataset, representative of traditional ML-based detection techniques used in OSNs, similar to the system described by Stein et al. [48]. We employ a GBDT architecture with an ensemble of 200 trees of depth 16, each with 32 leaf nodes.

2. **DEC-SS:**
   - **Model:** Multi-Task DNN
   - **Training Features:** DEC deep features (∼10^4)
   - **Training Data:** Approximate labels
   - **Evaluation Data:** Human labels
   - This model uses the DEC approach outlined in this paper to extract deep features but does not leverage the MS-MTL learning approach. A single deep neural network model is trained by combining all the approximate data across multiple tasks. If a user is identified as violating any one of the included tasks, it is considered a positive sample. Due to the large number of features extracted by DEC, the quantity of human-labeled data is too small to be used for training.

3. **DEC-MS-MTL:**
   - **Model:** Multi-Task DNN + GBDT
   - **Training Features:** DEC deep features (∼10^4)
   - **Training Data:** Approximate labels + human labels
   - **Evaluation Data:** Human labels
   - This is the complete end-to-end framework and model described in Section 6, combining the DEC-only approach with MS-MTL.

Outside of this evaluation section, references to DEC without a MS-MTL or SS qualifier refer to DEC-MS-MTL.

#### 7.3 Performance Comparisons

We compare various metrics based on the results of the above three models.

##### 7.3.1 ROC Curves

Figure 4 examines the ROC performance of all three models. ROC curves capture the trade-off between false positives and false negatives. For all operating points on the curve, the DEC models (both MS-MTL and SS) perform significantly better than a behavioral-only approach—by as much as 20%, depending on the operating point. From a ROC perspective, both DEC models perform similarly.

While ROC curves are important measures of the effectiveness of models, they are inherently scaleless, as the x-axis considers only ground-truth negatives and the y-axis considers only ground-truth positives. If the dataset is imbalanced, as is the case with abusive accounts (there are significantly more benign accounts than abusive accounts), ROC curves may not capture the actual operating performance of classification systems—particularly precision, a critical measure in abuse detection systems.

##### 7.3.2 Precision and Recall

Figure 5 compares the precision and recall of the models. We find that the behavioral model is unable to obtain precision above 0.95 and has very poor recall throughout the precision range. Both DEC models perform significantly better than the behavioral model, achieving higher precision and significantly higher recall at all relevant operating points.

**Table 5: Comparison of the area under the curve (AUC) and recall at precision 0.95 for different models on evaluation data.**

| **Model** | **AUC** | **Recall @ Precision 0.95** |
|-----------|---------|-----------------------------|
| **Behavioral** | 0.81 | NA |
| **DEC-SS** | 0.89 | 0.22 |
| **DEC-MS-MTL** | 0.90 | 0.50 |

The DEC-MS-MTL model achieves the best result by a significant margin, with a nearly 30% absolute improvement. The behavioral model is unable to obtain precision 0.95. Both DEC single stage and with MS-MTL have similar AUC performance, but adding MS-MTL more than doubles the model recall, increasing it from 22% to 50%. This increased performance, both over the behavioral model and over DEC without MS-MTL, enables significantly better real-world impact when deployed in production.

#### 7.4 Results in Production Environment

Building on our design and evaluation of DEC (with MS-MTL), we deployed the system into production at Facebook. The system not only identified abusive accounts but also triggered user-facing systems to take action on the identified accounts. To assess the model’s real-world impact and longevity, we evaluate our system in production by looking at the stability of precision and recall over time.

**Precision Over Time:**
Figure 6 examines the 3-day moving average of the precision of our DEC-MS-MTL system in production at Facebook. As with our prior evaluation, we obtain ground truth for our measurements by relying on manual human labeling of a random sample of accounts classified as abusive by DEC. We find that the precision of the system is stable, with the precision never dropping below 0.97 and frequently being higher than 0.98.

**Recall Over Time:**
We examine the stability of our production DEC-MS-MTL model’s recall by considering its false negative rate (FNR), where FNR = 1 - recall. Using a longitudinal dataset, we track the FNR over time to ensure the model's performance remains consistent.