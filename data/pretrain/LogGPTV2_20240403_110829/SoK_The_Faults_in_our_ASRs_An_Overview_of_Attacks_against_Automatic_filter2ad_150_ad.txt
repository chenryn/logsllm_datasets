Label
Label
Label
Label
Label
Label
Label
Distribution
Medium
L
L
L
L
L
L
L
L,A
L,A
L,A
L
L,T
A
A
L,A
L,A
L,A
L
?
?
?
?
?
?
?
?
?
?
?
?
?
18 hours
4.6 hours
seconds
seconds
32 hours
Table II: The table2 provides an overview of the current progress of the adversarial attacks against ASR and SI systems.
“?”: Authors provide no information in paper. “”: Will work. “P,W,S” = Phoneme, Word, Sentence. “L,A,T” = Over-Line,
Over-Air, Over-Telephony-Network.
VPS
Attacked
VPS
Internals
Distance
(Approximate)
Acoustic
Equipment
Acoustic
Environment
Transferability
Attack
Type
Direct
Direct
Direct
Direct
Direct
Direct
Direct
Direct
Direct
Direct
Indirect
Indirect
Attack Name
Carlini et al. [6]
HVC (1) [82]
Houdini [72]
-
-
-
-
-
-
Abdoli et al. [76]
Schonherr et al. [75]
M. Azalnot et al. [79]
Taori et al. [71]
HVC (2) [82]
1
1
1
1
1
1
1
2
3
5
1
1
1
1
9
4
6
12
RNN
RNN
RNN
HMM
RNN
RNN
RNN
HMM
Commander Song [41]
Devil’s Whisper [78]
Kreuk et al. [73]
Qin et al. [74]
Yakura et al. [77]
-
-
-
-
-
-
-


-

?
-
-




Table III: The table shows the current progress of the adversarial attacks against ASR and SI systems.“?”: Authors provide no
information in paper.“”: Will work. We sent each of the authors of the above papers emails regarding their papers and have
included the responses with “” in the table. “?”: Authors did not test it and are not sure if it will work. “+”: Authors did
not test it and believe it will work. “-”: Authors did not test it and believe it will not work.“”: Authors did not respond to
correspondence but we believe it will not work.
Cocaine Noodles [81]
Dolphin Attack [42]
Light Commands [80]
Kenansville Attack [29]
-
-
-
-
-
-



-

?
-
-

NA


-
-
-
-
-
-
-
?

-

?
-
-

NA


Miscellaneous
Miscellaneous
Miscellaneous
Miscellaneous
30 cm
150 cm
110 m
N/A
1 ft
Signal Processing
Signal Processing
0.5 meters
1.5 meters
5-200 cm
Abdullah et al. [7]
RNN,HMM,?
CNN
CNN
RNN
0.5 meters
?
?
?
?
-

?
RNN,?
RNN,?
ASR




-













SI
-
-
+

+
-
-
-
-
?

?
-
-




White-box attacks have made signiﬁcant contributions in
the space of adversarial ML. However, the authors generally
do not provide metrics to help identify the advantages of
their attacks over existing ones. These include metrics such
as max number of queries, time to produce an adversarial
audio sample etc. Generally, the attack methods make similar
assumptions (e.g., input/output granularity, goals, knowledge,
medium etc.), but exploit different ASR architectures. For
example, Carlini et al. [6] exploit DeepSpeech (CNN-RNN),
while Schonherr et al. [75] and Commander Song [41] both
target Kaldi (DNN-HMM).
2) Clean Attacks and Mediums: Clean audio attacks pro-
duce audio samples that sound clear to humans but are
mistranscribed by the ASR (Section IV-D). Although these
attacks have been demonstrated Over-Line, they have only
seen limited success over other mediums (Table III). There
are only a few attacks that can work Over-Air [41], [74], [77].
These are constrained as they require white-box knowledge of
the target, physical access to the victim’s acoustic equipment,
can take hours to generate, have limited physical range and
are sensitive to background noise. Similarly, there is only a
single attack [29] that can function Over-Telephony-Network.
However, it can not produce a targeted transcription.
Success Over-Line does not translate into success over other
mediums. There are a plethora of factors that contribute to
this limitation (Section IV-D3). These interfere with the ad-
versary generated perturbations, effectively blunting the attack.
Though these factors have a negligible impact on the ASR
understanding of a benign audio sample, these can frustrate the
efforts of the attacker. To enable clean attacks Over-Air and
Over-Telephony-Network, researchers can beneﬁt from char-
acterizing and overcoming the various sources of interference.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
738
3) Signal Processing Attacks vs Rest:
In comparison to
the other attack types, signal processing attacks have shown
greater promise, speciﬁcally in black-box settings. These at-
tacks have achieved the same goals as traditional optimization
attacks, speciﬁcally for noise [7] and untargeted attacks [29].
However, unlike their counterparts [82], signal processing
attacks are more efﬁcient as they require less than 15 queries
and a few seconds to generate an attack sample. These attacks
assume black-box access, which has allowed them to be suc-
cessfully tested against more target VPSes than existing works.
However, clean targeted attacks based on signal processing
have not yet been demonstrated.
4) Attacks against SIs: Only a tiny subset of the existing
work has focused on attacking SI systems [73], [7], [29]
(Table III). This is a result of the similarity of the underlying
architectures of the SIs and ASRs. Generally, an attack against
ASR can be used wholesale to exploit an SI. This is true
for both signal processing and optimization attacks. This is
because, as discussed in Section II-C2, pipelines for systems
often constitute the same stages. If an attack exploits the
stage in the pipeline shared by both ASRs and SIs, it will
succeed against both. This is corroborated by the examples of
the same attacks being successfully used, as is, against both
systems. Signal processing attack papers [7], [29] have already
demonstrated this ability to exploit both ASRs and SIs.
5) Indirect Optimization Attacks: Indirect optimization at-
tacks attempt to generate adversarial samples by repeatedly
querying the model [79], [71] in a black-box setting (Table III).
These attacks have been demonstrated only at the word [79]
granularity. One might incorrectly assume that indirect attacks
can be used against real-world systems (e.g., Alexa) as these
do not require perfect knowledge of the target, but
these
attacks have limited for a number of reasons. First, these
attacks require hundreds of thousands of queries to generate
a single attack audio sample. This makes attack execution
difﬁcult as most ASR’s charge users a fee for each query [10].
Second, the audio samples generated using these attacks can
only be used over-line and have not been demonstrated over
other mediums. This is primarily due to the sensitivity of the
samples to loss/distortion (Section IV-D3). Third, these attacks
have not been demonstrated at the sentence level in a black-
box setting. The only attack [71] that comes close still requires
information about the distributions, and is therefore grey box.
Overcoming these limitations is a direction for future research.
6) Optimization Attacks Do Not Guarantee Success: Opti-
mization attacks explore the decision spaces using algorithms
like gradient descent. One well-known drawback of such
algorithms is that they can get stuck in local minima. This
means that it might not be possible to successfully perturb
every benign audio sample to evade the target VPS. In con-
trast, signal processing attacks have been free of this speciﬁc
limitation. These attacks have been able to guarantee attack
success for any benign audio sample.
ing attacks are model dependent as each exploits a unique
component of the target VPS. For example, Carlini et al.
[6]
exploit the CTC component of the DeepSpeech ASR [83],
which does not exist in the Kaldi ASR [25]. It is important
for future researchers to develop model agnostic attacks, since
newer ASR systems with distinct architectures and improved
accuracies are being introduced every day. In this highly
variable and continuously changing space, any attack that only
works against a single VPS type can quickly become obsolete.
VI. DEFENSE AND DETECTION TAXONOMY
We present a taxonomy to help categorize the space of
defenses and detection methods.
A. Attacker Type:
There are two types of attackers:
1) Non-adaptive: This attacker does not have any knowl-
edge of the target’s defense strategy and parameters. The
minimal criteria a viable defense strategy needs to meet is
to be able prevent non-adaptive attackers.
2) Adaptive: The attacker has full knowledge of the the
type and parameters of the defense strategy being employed.
Using this knowledge, the attacker can modify their attack
strategy in hopes of overcoming the defender. A strong defense
can prevent successful exploitation even in the presence of an
adaptive attacker.
B. Adversarial Cost:
1) Resources: A defense might force an attacker to expend
greater resources to produce adversarial audio samples. For
example, an attacker would be forced to make additional
queries to the model. A defense is strong if it can increase the
resources required for a viable attack by a signiﬁcant margin.
2) Distortion: The goal of an attacker is to control the
degree of audible distortion introduced due to perturbations.
For example, in order to circumvent VPSes used in telephony
surveillance system, the adversary has the dual aims of evading
the VPS and ensuring low audible distortion. Large distortions
might make the audio message difﬁcult
to understand. A
defense strategy might aim to force the attacker to increase
the audible distortion needed to successfully create an attack
sample. Adding too much distortion will make the attack audio
sample difﬁcult to understand, preventing the second aim.
A strong defense will force the attacker to add signiﬁcant