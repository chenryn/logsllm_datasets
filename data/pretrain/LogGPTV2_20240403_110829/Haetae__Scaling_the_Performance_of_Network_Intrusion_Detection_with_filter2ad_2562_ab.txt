40
Receive Decode Flow mgmt Stream Detect
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
s
e
l
c
y
c
e
t
u
p
m
o
c
f
o
e
g
a
s
U
64
256
1024
1514
Packet size (Bytes)
(a)
(b)
Fig. 2. Performance bottleneck analysis of Baseline Suricata: (a) Throughputs of the
Aho-Corasick algorithm over varying numbers of TILE-Gx72 cores, (b) CPU usage
breakdown of Suricata modules over various packet size
However, if we generate packets over the network, the overall performance
drops by more than 40 Gbps. This means that modules other than pattern match-
ing must be optimized for overall performance improvement. To reveal a detailed
use of processing cycles, we measure the fraction of compute cycles spent on each
NIDS module. The results in Fig. 2(b) show that tasks other than pattern match-
ing (i.e., the detect module) take up 28 to 72 % of total processing cycles, depend-
ing on the packet size. The tile usage for the non-pattern matching portion is a
ﬁxed overhead per packet as the fraction gets higher for smaller packets.
Our detailed code-level analysis reveals that these cycles are mostly used to
process packet metadata. They include the operations, such as decoding the pro-
tocol of each packet, managing concurrent ﬂows, and reassembling TCP streams
for each incoming packet. In this work, we focus on improving the performance of
these operations, since the overall NIDS performance often depends on the per-
formance of these operations while leveraging the unique hardware-level features
of TILE-Gx72.
3.2 Our Approach
Our strategy for a high-performance NIDS is two folds. First, we need to par-
allelize pattern matching as much as possible to give the most compute cycles
to the performance-critical operation. This aﬀects the basic architecture of the
NIDS, which will be discussed in more detail in the next section. Second, we
need to reduce the overhead of the per-packet operation as much as possible.
For the latter, we exploit the special hardware provided by the TILE-Gx72 plat-
form. More speciﬁcally, our system leverages mPIPE and TRIO for oﬄoading
some of the heavy operations from regular tiles. mPIPE is originally designed to
evenly distribute the incoming packets to tiles by their ﬂows, but we extend it to
perform per-packet metadata operations to reduce the overhead on regular tiles.
The key challenge here is that the oﬄoaded features need to be carefully cho-
sen because the mPIPE processors provide limited compute power and memory
Haetae: Scaling the Performance of Network Intrusion Detection
95
access privilege. TRIO is mostly used to communicate with the host-side CPU
for monitoring the application behavior. We extend the TRIO module to pass
the analyzing workload to the host side when the TILE platform experiences a
high load. That is, we run a host-side NIDS for extra ﬂow analysis. The chal-
lenge here is to make eﬃcient PCIe transfers to pass the ﬂows and to dynamically
determine when to deliver the ﬂows to the host side. We explain the design in
more detail in the next section.
4 Design
In this section, we provide the base design of Haetae, and describe three opti-
mizations: mPIPE computation oﬄoading, lightweight metadata structure, and
dynamic host-side ﬂow analysis.
4.1 Parallel NIDS Engine Architecture
Haetae adopts the multi-threaded parallel architecture where each thread is run-
ning a separate NIDS engine, similar to [23]. Each NIDS engine is pinned to a
tile, and repeats running all NIDS tasks in sequence from receive to output mod-
ules. This is in contrast to the pipelining architecture used by earlier TILE-based
Suricata [24] where each core is dedicated to perform one or a few modules and
the input packets go through multiple cores for analysis. Pipelining is adopted
by earlier versions of open-source Suricata, but it suﬀers from a few fundamental
limitations. First, it is diﬃcult to determine the number of cores that should be
assigned for each module. Since the computation need of each module varies for
diﬀerent traﬃc patterns, it is hard to balance the load across cores. Even when
one module becomes a bottleneck, processing cores allocated for other modules
cannot help alleviate the load of the busy module. This leads to load imbalance
and ineﬃcient usage of computation cycles. Second, pipelining tends to increase
inter-core communication and lock contention, which is costly in a high-speed
NIDS. Since an NIDS is heavily memory-bound, eﬀective cache usage is critical
for good performance. In pipelining, however, packet metadata and payload have
to be accessed by multiple cores, which would increase CPU cache bouncing and
reduce the cache hits. Also, concurrent access to the shared packet metadata
would require expensive locks, which could waste processing cycles.
To support our design, we modify baseline Suricata to eliminate any shared
data structures, such as the ﬂow table. Each thread maintains its own ﬂow table
while it removes all locks needed to access the shared table entry. Incoming
packets are distributed to one of the tiles by their ﬂows, and a thread on each
tile analyzes the forwarded ﬂows without any intervention by other threads. Since
each thread only needs to maintain a small amount of ﬂow ranges, dividing the
huge ﬂow table into multiple pieces for each thread is not a big trade-oﬀ. Thus,
this shared-nothing architecture ensures high scalability while it simpliﬁes the
implementation, debugging, and conﬁguration of an NIDS.
96
J. Nam et al.
One potential concern with this design is that each core may not receive the
equal amount of packets or ﬂows from the NICs. However, recent measurements
in a real ISP show that a simple ﬂow-based load balancing scheme like RSS more
or less evenly distributes the ﬂows among the processing cores [35]. According
to the study, the maximum diﬀerence in the number of processed ﬂows per each
core on a 16-core server is within 0.2 % of all ﬂows at any given time with real
traﬃc. This implies that the randomness of IP addresses and port numbers used
in real traﬃc is suﬃcient to distribute the packet load evenly among the tiles.
4.2 MPIPE Computation Oﬄoading
With the highly-scalable system architecture in place, we now focus on opti-
mizing per-tile NIDS operations. Speciﬁcally, we reduce the packet processing
overhead on a tile by oﬄoading some common computations to the mPIPE pro-
grammable hardware module. When a packet arrives at a network interface,
mPIPE allocates a packet descriptor and a buﬀer for the packet content. The
packet descriptor has packet metadata such as timestamps, size, pointer to the
packet content as well as some reserved space for custom processing. After packet
reception, the software packet classiﬁer in mPIPE distributes the packet descrip-
tors to one of the tile queues, and the tile accesses the packet content with the
packet descriptor. mPIPE allows the developers to replace the packet classiﬁer
with their custom code to change the default module behavior.
Programming in mPIPE, however, is not straightforward due to a number
of hardware restrictions. First, in the case of mPIPE, it allows only 100 com-
pute cycles per packet to execute the custom code at line rate. Second, the
reserved space in the packet descriptor is limited to 28 bytes, which could be too
small to perform intensive computations. Third, mPIPE embedded processors
are designed mainly for packet classiﬁcation with a limited instruction set and
programming libraries. They consist of 10 low-powered 16-bit processors, which
do not allow ﬂexible operations such as subroutines, non-scalar data types (e.g.,
structs and pointers), and division (remainder) operations.
Given these constraints, Haetae oﬄoads two common packet processing tasks
of an NIDS: packet protocol decoding and hash computation for ﬂow table
lookup. We choose these two functions for mPIPE oﬄoading since they should
run for every packet but do not maintain any state. Also, they are relatively sim-
ple to implement in mPIPE while they save a large number of compute cycles
on each tile.
Figure 3 shows how the customized mPIPE module executes protocol decod-
ing and ﬂow hash computation. A newly-arriving packet goes through packet
decoding and ﬂow hash functions, saving results to the reserved area of an
mPIPE packet descriptor. Out of 28 bytes of total output space, 12 bytes are
used for holding the packet address information (e.g., source and destination
addresses and port numbers) and 4 bytes are used to save a 32-bit ﬂow hash
result. The remaining 12 bytes are employed as a bit array to encode various
information: whether it is an IPv4 or IPv6 packet, whether it is a TCP or
UDP packet, the length of a TCP header in the case of the TCP packet, etc.
Haetae: Scaling the Performance of Network Intrusion Detection
97
mPIPE packet processing processor
Packet
Decoding 
function
Flow hash 
function
IPv4/6 TCP
TCP hdr len
…
1 1 1 0 0 0 1 0
…
Hash value
IPs / ports
Bit array
H(p)
Packet descriptor
Load balancer
Cache coherent
TILE memory
Original path
New path
Tiles
Fig. 3. Design of the mPIPE engine with decoding and hash computations
Each bit can indicate multiple meanings depending on protocols. After these
functions, a load balancer determines which tile should handle the packet, and
the packet descriptor along with the packet is directly passed onto the L2 cache
of the tile that handles the packet, a feature similar to Intel data direct I/O [6].
As a result, each NIDS thread can proceed with the pre-processed packets and
avoids memory access latencies.
Our micro-benchmarks show that mPIPE oﬄoading improves the perfor-
mance of the decode and ﬂow management modules by 15 to 128 % (in Sect. 6).
Since these are per-packet operations, the cycle savings are more signiﬁcant with
smaller packets.
4.3 Lightweight Metadata Structure
mPIPE computation oﬄoading conﬁrms that reducing the overhead of per-
packet operation greatly improves the performance of the overall NIDS. The
root cause for performance improvement is reduced memory access and enhanced
cache access eﬃciency. More eﬃcient cache utilization leads to a smaller number
of memory accesses, which minimizes the wasted cycles due to memory stalls. If
the reduced memory access is a part of per-packet operation, the overall savings
could be signiﬁcant since a high-speed NIDS has to handle a large number of
packets in a unit time.
To further reduce the overhead of per-packet memory operation, we simplify
the packet metadata structure of baseline Suricata. Suricata’s packet metadata
structure is bloated since it has added support for many network and transport-
layer protocols over time. For example, the current data structure includes packet
I/O information (e.g., PCAP [10], PF RING [11], mPIPE), network-layer meta-
data (e.g., IPv4, IPv6, ICMP, IGMP) and transport-layer metadata (e.g., TCP,
UDP, SCTP). The resulting packet metadata structure is huge (1,920 bytes),
which is not only overkill for small packets but also severely degrades the cache
utilization due to redundant memory access. Also, the initialization cost for
metadata structure (e.g., memset() function calls) would be expensive.
98
J. Nam et al.
To address these concerns, we modify the packet metadata structure. First,
we remove the data ﬁelds for unused packet I/O engines. Second, we separate the
data ﬁelds for protocols into two groups: those that belong to frequently-used
protocols such as TCP, UDP, and ICMP and the rest that belong to rarely-used
protocols such as SCTP, PPP, and GRE. We move the data ﬁelds for the latter
into a separate data structure, and adds a pointer to it to the original structure.
If an arriving packet belongs to one of rarely-used protocols, we dynamically
allocate a structure and populate the data ﬁelds for the protocol. With these
optimizations, the packet metadata structure is reduced to 384 bytes, ﬁve times
smaller than the original size. Our proﬁling results ﬁnd that the overall number
of cache misses is reduced by 54 % due to lightweight metadata structures.
4.4 Flow Oﬄoading to Host-Side CPU
Since TILE-Gx72 is typically attached to a commodity server machine, we could
improve the NIDS performance further if we harness the host-side CPU for
intrusion detection. The TILE-Gx72 platform provides a TRIO module that
allows communication with the host machine. We exploit this hardware feature
to oﬄoad extra ﬂows beyond the capacity of the TILE processors to the host-side
CPU.
The net performance increase by host-side ﬂow oﬄoading largely depends on
two factors: (i) how fast the TILE platform transfers the packets to the host
machine over its PCIe interface, and (ii) the pattern matching performance of
the host-side NIDS. In our case, we use a machine containing two Intel E5-2690
CPUs (2.90 GHz, 16 cores in total) that run Kargus with only CPUs [23]. Since
the performance of the Aho-Corasick algorithm in Kargus is about 2 Gbps per
CPU core [23], the host-side NIDS performance would not be an issue given the
8-lane PCIev2 interface (with 32 Gbps maximum bandwidth in theory) employed
by the TILE platform.
We ﬁrst describe how we optimize the TRIO module to eﬃciently trans-
fer packets to the host side, and explain which packets should be selected for
oﬄoading. Also, we determine when the packets should be oﬄoaded to the host
machine to maximize the performance of both sides.
Eﬃcient PCIe Communication. Baseline Suricata provides only rudimen-
tary host oﬄoading support mainly used for remote message logging; since the
TILE platform does not have built-in secondary storage, it periodically dis-
patches the batched log messages from its output module to the host-side storage
via its TRIO module. Since log transmission does not require a high bandwidth,
the stock TRIO module in the baseline Suricata code is not optimized for high-
speed data transfer. First, the module does not exploit zero-copy DMA support.
Second, it does not exercise parallel I/O in PCIe transactions, incurring a heavy
contention in the shared ring buﬀer. Our measurement shows that the stock
TRIO module achieves only 5.7 Gbps of PCIe data transfer throughput at best
out of the theoretical maximum of 32 Gbps.
Haetae: Scaling the Performance of Network Intrusion Detection
99
TILE platform
TILE memory
TRIO module
Host
Tiles
(1)
(2)
Tiles
Tiles
TRIO queue 1
TRIO queue 2
(4)
(3)
e
l
u
d
o
m
x
R
Core 1
Core 2
TRIO queue 16
Core 16
Steps:
Ring buffers
(1) Batching packets
(3) Notifying the update of the ring buffer
(4) Reading offloaded packets using the raw DMA API
(2) Updating a ring buffer
Fig. 4. Design of the oﬄoading module with the TRIO engine
We exploit three features in the TRIO module to maximize the PCIe trans-
fer performance for high host-side NIDS throughput. First, we develop a zero-
copying oﬄoading module with the raw-DMA API provided by the TRIO engine.
The raw-DMA API ensures low-latency data transfer between the TILE platform
and the host. It requires physically-contiguous buﬀers to map the TILE memory
to the host-side address space. For zero-copy data transfer, we pre-allocate shared
packet buﬀers at initialization of Suricata, which are later used by mPIPE for
packet reception. Packets that need to be oﬄoaded are then transferred via the
TRIO module without additional memory copying, which greatly saves compute
cycles. Second, we reduce the contention to the shared ring buﬀer by increas-
ing the number of TRIO queues. The baseline version uses a single ring buﬀer,
which produces severe contention among the tiles. We increase the number to
16, which is the maximum supported by our TILE platform. This allows parallel
queue access both from tiles and CPU cores. Finally, we oﬄoad multiple packets
in a batch to amortize the cost incurred due to per-packet PCIe transfer. Our
packet oﬄoading scheme is shown in Fig. 4. We ﬁnd that these optimizations are
very eﬀective, improving the performance of PCIe transfer by 5 to 28 times over
the stock version.
Dynamic Flow Oﬄoading. We design the TRIO oﬄoading module to fully
beneﬁt from the hardware advantage of the TILE platform. We make the TILE
platform handle as much traﬃc as possible to minimize the power consumption
and the analyzing latency. To determine when to oﬄoad the packets to the host
side, each tile monitors whether it is being under pressure by checking the queue
size in mPIPE. A large build-up in the queue indicates that the incoming load