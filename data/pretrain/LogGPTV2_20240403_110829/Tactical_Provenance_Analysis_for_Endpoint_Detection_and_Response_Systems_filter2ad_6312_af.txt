accurate TPGs from the skeleton graph for future alerts.
it
X. CONCLUSION
In this work, we propose a viable solution for incorporating
data provenance into commercial EDR tools. We use the
notion of tactical provenance to reason about causally related
threat alerts, and then encode those related alerts into a
tactical provenance graph (TPG). We leverage the TPG for
risk assessment of the EDR-generated threat alerts and for
system log reduction. We incorporated our prototype system,
RapSheet, into the Symantec EDR tool. Our evaluation results
over an enterprise dataset show that RapSheet improves the
threat detection accuracy of the Symantec EDR. Moreover,
our log reduction technique dramatically reduces the overhead
associated with long-term system log storage while preserving
causal links between existing and future alerts.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:41:13 UTC from IEEE Xplore.  Restrictions apply. 
1184
ACKNOWLEDGMENT
We thank our shepherd, Guofei Gu, and the anonymous
reviewers for their comments and suggestions. We also thank
Akul Goyal, Riccardo Paccagnella, and Ben Ujcich for feed-
back on early drafts of this paper, as well as all members
of the NortonLifeLock Research Group. Wajih Ul Hassan
was partially supported by the Symantec Graduate Fellowship.
This work was supported in part by the NSF under contracts
CNS-16-57534 and CNS-17-50024. Any opinions, ﬁndings,
conclusions, or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the
views of their employers or the sponsors.
REFERENCES
[1] “Target Missed Warnings in Epic Hack of Credit Card Data,” https:
//bloom.bg/2KjElxM, 2019.
[2] “Equifax Says Cyberattack May Have Affected
143 Million
in the U.S.” https://www.nytimes.com/2017/09/07/business/equifax-
cyberattack.html, 2017.
[3] “Inside the Cyberattack That Shocked the US Government,” https:
//www.wired.com/2016/10/inside-cyberattack-shocked-us-government/,
2016.
[4] “Whats in a name? TTPs in Info Sec,” https://posts.specterops.io/whats-
in-a-name-ttps-in-info-sec-14f24480ddcc, 2019.
[5] “The Critical Role of Endpoint Detection and Response,” https://bit.ly/
39NrNwo, 2019.
[6] “MITRE ATT&CK,” https://attack.mitre.org, 2019.
[7] “Why MITRE ATT&CK Matters,” https://symantec-blogs.broadcom.
com/blogs/expert-perspectives/why-mitre-attck-matters.
[8] “Experts advocate for ATT&CK,” https://www.cyberscoop.com/mitre-
attck-framework-experts-advocate/.
[9] “ATT&CK Evaluations,” https://attackevals.mitre.org/.
[10] “Endpoint Detection and Response Solutions Market,” https://www.
gartner.com/reviews/market/endpoint-detection-and-response-solutions,
2019.
[11] “File Deletion,” https://attack.mitre.org/techniques/T1107/, 2019.
[12] “Automated
Incident Response: Respond
Every Alert,”
to
https://swimlane.com/blog/automated-incident-response-respond-
every-alert/, 2019.
[13] “New Research from Advanced Threat Analytics,” https://prn.to/
2uTiaK6, 2019.
[14] G. P. Spathoulas and S. K. Katsikas, “Using a fuzzy inference system to
reduce false positives in intrusion detection,” in International Conference
on Systems, Signals and Image Processing, 2009.
[15] “How Many Alerts is Too Many to Handle?” https://www2.ﬁreeye.com/
StopTheNoise-IDC-Numbers-Game-Special-Report.html, 2019.
[16] “An ESG Research Insights Report,” http://pages.siemplify.co/rs/182-
SXA-457/images/ESG-Research-Report.pdf.
[17] “Splunk,” https://www.splunk.com.
[18] “About purging reports,” https://support.symantec.com/us/en/article.
howto129116.html, 2019.
[19] “Evaluating Endpoint Products,” https://redcanary.com/blog/evaluating-
endpoint-products-in-a-crowded-confusing-market/, 2018.
[20] A. Bates, W. U. Hassan, K. Butler, A. Dobra, B. Reaves, P. Cable,
T. Moyer, and N. Schear, “Transparent web service auditing via network
provenance functions,” in WWW, 2017.
[21] A. Bates, D. Tian, K. R. B. Butler, and T. Moyer, “Trustworthy whole-
system provenance for the Linux kernel,” in USENIX Security, 2015.
[22] M. N. Hossain, S. M. Milajerdi, J. Wang, B. Eshete, R. Gjomemo,
R. Sekar, S. D. Stoller, and V. Venkatakrishnan, “SLEUTH: Real-
time attack scenario reconstruction from COTS audit data,” in USENIX
Security, 2017.
[23] Y. Kwon, F. Wang, W. Wang, K. H. Lee, W.-C. Lee, S. Ma, X. Zhang,
D. Xu, S. Jha, G. Ciocarlie et al., “MCI: Modeling-based causality
inference in audit logging for attack investigation,” in NDSS, 2018.
[24] K. H. Lee, X. Zhang, and D. Xu, “High accuracy attack provenance via
binary-based execution partition,” in NDSS, 2013.
[25] S. Ma, K. H. Lee, C. H. Kim, J. Rhee, X. Zhang, and D. Xu, “Accurate,
low cost and instrumentation-free security audit logging for Windows,”
in ACSAC. ACM, 2015.
[26] S. Ma, J. Zhai, F. Wang, K. H. Lee, X. Zhang, and D. Xu, “MPI:
Multiple perspective attack investigation with semantic aware execution
partitioning,” in USENIX Security, 2017.
[27] W. U. Hassan, M. A. Noureddine, P. Datta, and A. Bates, “OmegaLog:
High-ﬁdelity attack investigation via transparent multi-layer log analy-
sis,” in NDSS, 2020.
[28] S. M. Milajerdi, B. Eshete, R. Gjomemo, and V. Venkatakrishnan,
“Poirot: Aligning attack behavior with kernel audit records for cyber
threat hunting,” in CCS, 2019.
[29] W. U. Hassan, M. Lemay, N. Aguse, A. Bates, and T. Moyer, “Towards
scalable cluster auditing through grammatical inference over provenance
graphs,” in NDSS, 2018.
[30] K. H. Lee, X. Zhang, and D. Xu, “LogGC: Garbage collecting audit
log,” in CCS, 2013.
[31] Y. Liu, M. Zhang, D. Li, K. Jee, Z. Li, Z. Wu, J. Rhee, and P. Mittal,
“Towards a timely causality analysis for enterprise security,” in NDSS,
2018.
[32] S. Ma, X. Zhang, and D. Xu, “ProTracer: Towards practical provenance
tracing by alternating between logging and tainting,” in NDSS, 2016.
[33] T. Pasquier, X. Han, T. Moyer, A. Bates, O. Hermant, D. Eyers, J. Bacon,
and M. Seltzer, “Runtime analysis of whole-system provenance,” in CCS.
ACM, 2018.
[34] Z. Xu, Z. Wu, Z. Li, K. Jee, J. Rhee, X. Xiao, F. Xu, H. Wang, and
G. Jiang, “High ﬁdelity data reduction for big data security dependency
analyses,” in CCS, 2016.
[35] S. Ma, J. Zhai, Y. Kwon, K. H. Lee, X. Zhang, G. Ciocarlie, A. Gehani,
V. Yegneswaran, D. Xu, and S. Jha, “Kernel-supported cost-effective
audit logging for causality tracking,” in USENIX ATC, 2018.
[36] Y. Tang, D. Li, Z. Li, M. Zhang, K. Jee, X. Xiao, Z. Wu, J. Rhee,
F. Xu, and Q. Li, “Nodemerge: Template based efﬁcient data reduction
for big-data causality analysis,” in CCS. ACM, 2018.
[37] M. N. Hossain, J. Wang, R. Sekar, and S. D. Stoller, “Dependence-
preserving data compaction for scalable forensic analysis,” in USENIX
Security Symposium, 2018.
[38] W. U. Hassan, S. Guo, D. Li, Z. Chen, K. Jee, Z. Li, and A. Bates,
“NoDoze: Combatting threat alert fatigue with automated provenance
triage,” in NDSS, 2019.
[39] Q. Wang, W. U. Hassan, D. Li, K. Jee, X. Yu, K. Zou, J. Rhee, Z. Chen,
W. Cheng, C. Gunter, and H. Chen, “You are what you do: Hunting
stealthy malware via data provenance analysis,” 2020.
[40] X. Han, T. Pasquier, A. Bates, J. Mickens, and M. Seltzer, “UNICORN:
Runtime provenance-based detector for advanced persistent threats,” in
NDSS, 2020.
[41] A. Bates and W. U. Hassan, “Can data provenance put an end to the
data breach?” IEEE Security Privacy, vol. 17, no. 4, pp. 88–93, July
2019.
[42] K. Pei, Z. Gu, B. Saltaformaggio, S. Ma, F. Wang, Z. Zhang, L. Si,
X. Zhang, and D. Xu, “HERCULE: Attack story reconstruction via
community discovery on correlated log graph,” in ACSAC. ACM, 2016.
[43] S. M. Milajerdi, R. Gjomemo, B. Eshete, R. Sekar, and V. Venkatakr-
ishnan, “HOLMES: Real-time APT detection through correlation of
suspicious information ﬂows,” in IEEE S&P, 2019.
[44] “Threat-based
Defense,”
https://www.mitre.org/capabilities/
cybersecurity/threat-based-defense, 2019.
[45] E. M. Hutchins, M. J. Cloppert, and R. M. Amin, “Intelligence-driven
computer network defense informed by analysis of adversary campaigns
and intrusion kill chains,” Leading Issues in Information Warfare &
Security Research, vol. 1, no. 1, p. 80, 2011.
[46] S. T. King and P. M. Chen, “Backtracking intrusions,” in SOSP. ACM,
2003.
[47] “Windows Event Tracing,” https://docs.microsoft.com/en-us/windows/
desktop/ETW/event-tracing-portal.
[48] “The Linux audit daemon,” https://linux.die.net/man/8/auditd.
[49] “MITRE Matrix,” https://attack.mitre.org/matrices/enterprise/.
[50] “APT 29 - Put up your Dukes,” https://www.anomali.com/blog/apt-29-
put-up-your-dukes, 2019.
[51] “APT29,” https://attack.mitre.org/groups/G0016/, 2019.
[52] “CrowdStrike,” https://www.crowdstrike.com/.
[53] Airbus Cyber Security, “APT Kill Chain,” https://airbus-cyber-security.
com/apt-kill-chain-part-2-global-view/, 2018.
[54] R. Paccagnella, P. Datta, W. U. Hassan, C. W. Fletcher, A. Bates,
A. Miller, and D. Tian, “Custos: Practical tamper-evident auditing of
operating systems using trusted execution,” in NDSS, 2020.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:41:13 UTC from IEEE Xplore.  Restrictions apply. 
1185
[55] W. Zhou, Q. Fei, A. Narayan, A. Haeberlen, B. T. Loo, and M. Sherr,
“Secure Network Provenance,” in SOSP, 2011.
[56] “Endgame - Endpoint Protection,” https://www.endgame.com/sites/
default/ﬁles/architecturesolutionbrief.pdf, 2019.
[57] “Endpoint Security in Todays Threat Environment,” https://ziften.com/
wp-content/uploads/2016/12/UserMode Whitepaper.pdf, 2019.
[58] “Monitoring ALPC Messages,” http://blogs.microsoft.co.il/pavely/2017/
02/12/monitoring-alpc-messages/, 2017.
[59] L. Lamport, “Time, clocks, and the ordering of events in a distributed
system,” Commun. ACM, vol. 21, no. 7, pp. 558–565, Jul. 1978.
[Online]. Available: http://doi.acm.org/10.1145/359545.359563
[60] “Common Attack Pattern Enumeration and Classiﬁcation,” https://capec.
[61] MITRE, “Cyber Threat Intelligence Repository,” https://github.com/
[62] “Registry Run Keys / Startup Folder,” https://attack.mitre.org/techniques/
mitre.org, 2019.
mitre/cti.
T1060/, 2019.
[63] “CAPEC-270: Modiﬁcation of Registry Run Keys,” https://capec.mitre.
org/data/deﬁnitions/163.html, 2019.
[64] “Apache TinkerPop,” http://tinkerpop.apache.org/, 2019.
[65] “RedisGraph - a graph database module for Redis,” https://oss.redislabs.
com/redisgraph/, 2019.
[66] H. Lim, D. Han, D. G. Andersen, and M. Kaminsky, “Mica: A holistic
approach to fast in-memory key-value storage.” USENIX, 2014.
[67] MITRE, “Technology Transfer: CALDERA,” https://www.mitre.org/
research/technology-transfer/open-source-software/caldera.
[68] “APT3,” https://attack.mitre.org/groups/G0022/, 2019.
[69] A. Valdes and K. Skinner, “Probabilistic alert correlation,” in Interna-
tional Workshop on Recent Advances in Intrusion Detection. Springer,
2001, pp. 54–68.
[70] W. Wang and T. E. Daniels, “A graph based approach toward network
forensics analysis,” TISSEC, 2008.
[71] H. Debar and A. Wespi, “Aggregation and correlation of intrusion-
detection alerts,” in International Workshop on Recent Advances in
Intrusion Detection. Springer, 2001, pp. 85–103.
[72] Y. Shen and G. Stringhini, “Attack2vec: Leveraging temporal word
embeddings to understand the evolution of cyberattacks,” in USENIX
Security, 2019.
[73] G. Gu, P. Porras, V. Yegneswaran, and M. Fong, “BotHunter: Detecting
malware infection through ids-driven dialog correlation,” in USENIX
Security Symposium, 2007.
[74] “Endpoint Monitoring & Security,” https://logrhythm.com/solutions/
security/endpoint-threat-detection/, 2019.
[75] “What is SIEM?” https://logz.io/blog/what-is-siem/, 2019.
[76] A. Bates, K. Butler, A. Haeberlen, M. Sherr, and W. Zhou, “Let SDN
Be Your Eyes: Secure Forensics in Data Center Networks,” in SENT,
2014.
[77] Y. Wu, A. Chen, and L. T. X. Phan, “Zeno: Diagnosing performance
problems with temporal provenance,” in NSDI, 2019.
[78] A. Chen, Y. Wu, A. Haeberlen, W. Zhou, and B. T. Loo, “The good,
the bad, and the differences: Better network diagnostics with differential
provenance,” in ACM SIGCOMM, 2016.
[79] Y. Wu, M. Zhao, A. Haeberlen, W. Zhou, and B. T. Loo, “Diagnosing
missing events in distributed systems with negative provenance,” ACM
SIGCOMM Computer Communication Review, vol. 44, no. 4, pp. 383–
394, 2014.
[80] W. Zhou, M. Sherr, T. Tao, X. Li, B. T. Loo, and Y. Mao, “Efﬁcient
querying and maintenance of network provenance at internet-scale,” in
ACM SIGMOD, 2010.
[81] W. Zhou, S. Mapara, Y. Ren, Y. Li, A. Haeberlen, Z. Ives, B. T. Loo,
and M. Sherr, “Distributed time-aware provenance,” Proceedings of the
VLDB Endowment, pp. 49–60, 2012.
[82] A. Gehani and D. Tariq, “SPADE: Support for provenance auditing in
distributed environments,” in Middleware, 2012.
APPENDIX A
DATASET CHARACTERIZATION
In this section, we characterize dataset that we used in our
evaluation. We collected 40M system monitoring event from
34 hosts in a real-world enterprise environment. These host
machines were used by employees daily for web browsing,
software coding and compilation, quality assurance testing,
project management, and other routine business tasks. We
used 67 total alert rules to detect various MITRE ATT&CK
techniques in our experiments. Of these rules, some were
written by us, while the other were included by default in
the Symantec EDR software.
(cid:1)Ctr Panr
ICnms
Firn
Discovnty
NCwtkShPtn
Discovnty
NCwtk(cid:1)oaa
Discovnty
AccouaC
Discovnty
NCwtkShPtn
RnmovPr
ShotCcuC
ModifcPCioa
(cid:1)ompirnd
HTMLFirn
IasCPrrRooC
(cid:1)ntCifcPCn
(cid:1)tndnaCiPr
Dumpiag
 tocnss
IajncCioa
SysCnmUsnt
Discovnty
100%
80%
DncodnFirns
NCwtk(cid:1)oafg
otIa(cid:2)o
Discovnty
ImPgnFirn
ExncOpCioas
WiaAdmia
ShPtns
SysTimn
Discovnty
RnmoCn
Sntvicns
RnmoCnSys
Discovnty
Rngsvt32 Dnvnropnt
UCiriCins
VPrid
AccouaCs
Sntvicn
ExncuCioa
WiaMgmC
IasCtumnaCPCioa
RngisCty
RuaKnys
SysIa(cid:2)o
Discovnty
Modi(cid:2)y
Sntvicn
IadicPCot
RnmovPr
RngisCty
 ntmWnPkanss
RnmoCn
DnskCop
 toCocor
Btowsnt
ExCnasioas
Nnw
Sntvicn
Schndurnd
TPsk
SysSntvicn
Discovnty
 owntShnrr RnmoCnFirn
(cid:1)opy
SctipCiag (cid:1)hPagnFirn
AssociPCioa
Ruadrr32
DPCP
(cid:1)omptnssnd
(cid:1)OM
HijPckiag
60%
40%
e
v
i
t
i
s
o
P
e
u
r
T

20%
0%
1
10
100
1,000
10,000
TotalNumberofObservations
Fig. 13: Number of matched MITRE ATT&CK techniques during
our evaluation with their true positive rates.
First, we look at how often the various MITRE ATT&CK
technique and tactic rules caused alerts on the hosts in our
experiment. Figure 13 shows which MITRE ATT&CK tech-
niques were matched, how many times, and what proportion
of the alerts for each technique were related to a true attack.
We can see from the ﬁgure that rules for techniques like
RunDLL32 (T1085) and Scripting (T1064) generated many
alerts, but have very low true positive rates since these
techniques are commonly used for benign purposes. On the
other hand, techniques like “Change File Association” (T1042)
and “System Service Discovery” (T1007) were triggered many
times and have high true positive rate because these techniques
usually only happen during malicious activity. Thus, these
techniques can be strong indication of an attack campaign.
]
l
d
e
a
c
s
-
g
o
l
[

t
n
u
o
C
100000
10000
1000
100
10
1
defense-evasion
lateral-m ove m ent
initial-access
privilege-escalation
FalseAlarms