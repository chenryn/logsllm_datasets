 20
 15
 10
 5
 0
Jan 22
Feb 22 Mar 22
Apr 21
1
2
3
4
5
6
Backup
7
8
9
10
 0
 0.05
 0.1
 0.15
 0.2
Leakage Rate (%)
Backup
(a) FSL dataset
(b) Synthetic dataset
Fig. 4. Experiment A.2 (Inference rate in ciphertext-only mode).
Fig. 5.
known-plaintext mode).
Experiment A.3 (Inference rate in
three parameters and vary the remaining parameter to identify
the largest inference rate in each case.
Figure 3(a) ﬁrst shows the impact of u, in which we ﬁx
v = 20 and w = 100,000. The inference rate decreases with
u. For example, when u = 5, the attack can successfully infer
10.28% of plaintext chunks, while the inference rate drops to
7.37% when u increases to 20. The reason is that a larger u
implies that incorrect ciphertext-plaintext chunk pairs are more
likely to be included into the inferred set during initialization,
thereby compromising the inference accuracy.
Figure 3(b) next shows the impact of v, in which we ﬁx
u = 10 and w = 100,000. Initially, the inference rate increases
with v as the underlying frequency analysis infers more
ciphertext-plaintext chunk pairs in each iteration. It hits the
maximum value at about 10% when v = 30. When v increases
to 40, the inference rate drops slightly to about 9.52%. The
reason is that some incorrectly inferred ciphertext-plaintext
chunk pairs are also included into G, which compromises the
inference rate.
Figure 3(c) ﬁnally shows the impact of w, in which we ﬁx
u = 10 and v = 20. A larger w increases the inference rate,
since G can hold more ciphertext-plaintext chunk pairs across
iterations. We observe that when w increases beyond 200,000,
the inference rate becomes steady at about 10.2%.
Experiment A.2 (Inference rate in ciphertext-only mode):
We now compare both basic and locality-based attacks in
ciphertext-only mode. From Experiment A.1, we select u = 5,
v = 30, and w = 200,000 as default parameters, as they give
the highest possible inference rate for the locality-based attack.
We ﬁrst consider the FSL dataset. We choose each of the
prior FSL backups on January 22, February 22, March 22,
and April 21 as auxiliary information, and we launch attacks
to infer the original plaintext chunks in the latest backup
on May 21. Figure 4 shows the inference rate versus the
prior backup. As expected, the inference rate of both attacks
increases as we use more recent non-latest backups as auxiliary
information, since a more recent backup has higher content
redundancy with the target latest backup. We observe that the
basic attack is ineffective in all cases, as the inference rate
is no more than 0.0001%. On the other hand, the locality-
based attack can achieve a signiﬁcantly high inference rate.
For example, if we use the most recent non-latest backup on
April 21 as auxiliary information, the inference rate of the
locality-based attack can reach as high as 17.8%.
We now consider the synthetic dataset. We use the initial
snapshot (which is publicly available) as auxiliary information.
We then infer the original plaintext chunks in each of the
following synthetic backups. Figure 4(b) shows the inference
rates of both the basic and locality-based attacks over the
sequence of backups. The locality-based attack is again more
severe than the basic attack, whose inference rate is less than
0.2%. For example, the locality-based attack can infer 12.93%
of original plaintext chunks in the ﬁrst backup, while that
of the basic attack is only 0.19%. After ten backups, since
more chunks have been updated since the initial snapshot, the
inference rates of the locality-based and basic attacks drop to
6% and 0.0007%, respectively. Nevertheless, we observe that
the locality-based attack always incurs a higher inference rate
than the basic attack.
Experiment A.3 (Inference rate in known-plaintext mode):
We further evaluate the severity of the locality-based attack
in known-plaintext mode. To quantify the amount of leakage
about the latest backup (see Section III), we deﬁne the leakage
rate as the ratio of the number of ciphertext-plaintext chunk
pairs known by the adversary to the total number of ciphertext
chunks in the latest backup. We consider the medium case of
the attack: for the FSL dataset, we choose the middle version
of the backup on March 22 as auxiliary information to infer the
latest backup on May 21; for the synthetic dataset, we use the
initial snapshot as auxiliary information to infer the 5th backup
snapshot. We conﬁgure u = 5, v = 30, and w = 500,000. Note
that we increase w to 500,000 (as compared to w = 200,000
in Experiment A.2), since we ﬁnd that the attack in known-
plaintext mode can infer much more ciphertext-plaintext chunk
pairs across iterations. Thus, we choose a larger w to include
them into the inferred set.
Figure 5 shows the inference rate versus the leakage rate
about the target backup being inferred, which we vary from
0% to 0.2%. The slight increase in the leakage rate can lead
to a signiﬁcant increase in the inference rate. For example,
when the leakage rate increases from 0 to 0.2%, the inference
rate increases from 11.09% to 27.14% and from 10.34% to
28.32% for the FSL and synthetic datasets, respectively.
VI. DEFENSE
The deterministic nature of encrypted deduplication dis-
closes the frequency distribution of the underlying plaintext
chunks, thereby making frequency analysis feasible. To defend
against frequency analysis, we consider a MinHash encryption
scheme that encrypts each copy of identical plaintext chunks
into possibly different ciphertext chunks, so as to hide the
frequency distribution of original chunks, while ensuring that
the storage efﬁciency is only slightly degraded.
Overview: MinHash encryption builds on Broder’s theorem
[12], which quantiﬁes the similarity of two sets of elements:
Broder’s theorem [12]: Consider two sets of elements S1
and S2. Let U = |S1 ∪ S2| be the number of elements in
the union of S1 and S2, H be a hash function that is chosen
uniformly at random from a min-wise independent family of
permutations, and min{H(S)} be the minimum element hash
of S. Then Pr[min{H(S1)} = min{H(S2)}] = |S1∩S2|
|S1∪S2| .
Broder’s theorem states that
if two sets share a large
fraction of common elements (i.e., they are highly similar),
then the probability that both sets share the same minimum
hash element is also high. Since two backups from the same
data source are expected to be highly similar and share a
large number of identical chunks [45], MinHash encryption
leverages this property to perform encrypted deduplication in
a different way from the original MLE [8], [9]. We emphasize
Algorithm 3 MinHash Encryption
Initialize C
Partition M into segments
for each segment S do
1: procedure MINHASH ENCRYPTION(M)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end procedure
h ← minimum ﬁngerprint of all chunks in S
KS ← segment-based key derived from h
for each chunk M ∈ S do
C ←ENCRYPT(KS , M )
Add C into C
end for
end for
return C
that previous deduplication approaches also leverage Broder’s
theorem to minimize the memory usage of the ﬁngerprint
index in plaintext deduplication [10], [47] or key generation
overhead in server-aided MLE [38]. Thus, we do not claim
the novelty of the design of MinHash encryption. Instead, our
contribution is to demonstrate that it can effectively defend
against frequency analysis.
Algorithm details: MinHash encryption works as follows,
as shown in Algorithm 3. It takes a sequence of plaintext
chunks M as input, and returns a sequence of ciphertext
chunks C as output. It ﬁrst partitions an original stream of
plaintext chunks M into a set of large-size data units called
segments (Line 3), each of which is composed of a sequence
of plaintext chunks. One possible way of such partitioning is
to put a segment boundary at the chunk boundary if a chunk’s
modulo hash equals some speciﬁed pattern, so that segment
boundaries align with chunk boundaries [30]; we elaborate the
implementation details in Section VII-A. For each segment S,
MinHash encryption computes the minimum ﬁngerprint h of
all chunks in S; here, we use the ﬁngerprint value as the hash
value of each chunk. It then derives the segment-based key KS
based on h (Line 6), for example, by querying a key manager
as in DupLESS [8] to defend against brute-force attacks (see
Section II-B). It then encrypts each chunk in S using KS and
adds the resulting ciphertext chunk to C (Lines 7-10). Finally,
it returns C (Line 12).
Why MinHash encryption is effective: MinHash encryption
preserves deduplication effectiveness. The rationale is that
segments are highly similar, so their minimum ﬁngerprints
and hence the segment-based keys are likely to be the same.
Also, the similar segments share a large fraction of identical
plaintext chunks, which will likely be encrypted by the same
segment-based keys into identical ciphertext chunks that can
be deduplicated. Since identical plaintext chunks may reside
in different segments that have different segment-based keys,
their resulting ciphertext chunks become different and cannot
be deduplicated. Thus, we can only achieve near-exact dedu-
plication and expect a slight degradation of storage efﬁciency.
Nevertheless, our main observation is that such near-exact
deduplication provides resilience against frequency analysis.
Since an identical chunk can now be encrypted into multiple
distinct ciphertext chunks, this breaks the deterministic nature
of encrypted deduplication. More importantly, even though it
affects only a small number of identical chunks, it is sufﬁcient
to disturb the overall frequency rank of ciphertext chunks and
make frequency analysis ineffective.
VII. DEFENSE EVALUATION
In this section, we present trace-driven evaluation results on
the effectiveness of MinHash encryption, using the same FSL
and synthetic datasets in our attack evaluation (see Section V).
A. Implementation
Since the FSL dataset does not contain actual content,
instead of performing actual encryption, we simulate MinHash
encryption by directly operating on chunk ﬁngerprints.
if (i) the size of each segment
First, we need to identify segment boundaries based on
chunk ﬁngerprints. We follow the variable-size segmentation
scheme in [30]. Speciﬁcally,
the segmentation scheme is
conﬁgured by the minimum, average, and maximum segment
sizes. It places a segment boundary at the end of a chunk
ﬁngerprint
the
minimum segment size, and (ii) the chunk ﬁngerprint modulo
a pre-deﬁned divisor (which determines the average segment
size) is equal to some constant (e.g., −1), or the inclusion of
the chunk makes the segment size larger than the maximum
segment size. In our evaluation, we vary the average segment
size, and ﬁx the minimum and maximum segment sizes as half
and double of the average segment size, respectively.
is at
least
We mimic the encryption process as follows. We ﬁrst
calculate the minimum chunk ﬁngerprint h of each segment.
We then concatenate h with each chunk ﬁngerprint in the
segment and compute the MD5 hash of the concatenation.
We also truncate hash result to keep only the ﬁrst 48 bits, so
as to be consistent with the ﬁngerprint size in the original
FSL dataset. The truncated hash result can be viewed as
the ﬁngerprint of the ciphertext chunk. We can easily check
that identical plaintext chunks under the same h will lead to
identical ciphertext chunks that can be deduplicated.
B. Results
Experiment B.1 (Robustness against leakage): We ﬁrst
evaluate the effectiveness of MinHash encryption against the
locality-based attack. We vary the average segment size as
512KB, 1MB, and 2MB (while the average chunk size is
8KB). We use the same parameter setting as in Experiment A.3
and evaluate the inference rate versus the leakage rate.
Figure 6(a) ﬁrst shows the results of the FSL dataset. In
ciphertext-only mode (i.e., the leakage rate is zero), MinHash
encryption suppresses the inference rate to almost zero. In
particular, when the segment sizes are 512KB, 1MB, and 2MB,
the locality-based attack can only successfully infer 6, 6, and 3
out of around 37 million ciphertext-plaintext chunk pairs. The
main reason is the frequency rank of the ciphertext chunks
has been disturbed by MinHash encryption (as explained in
Section VI). In known-plaintext mode (i.e., the leakage rate
is greater than zero), the inference rate increases with the
 0.5
)
 0.4
 0.3
 0.2
%
(
e
t
a
R
e
c
n
e
r
e
f
n
I
 0.1
512KB
1MB
2MB
512KB
1MB
2MB
 8
)
%
(
e
t
a
R
e
c
n
e
r
e
f
n
I
 6
 4
 2
 0
 0
 0
 0.05  0.1  0.15  0.2
Leakage Rate (%)
 0
 0.05
 0.15
Leakage Rate (%)
 0.1
 0.2
(a) FSL dataset
(b) Synthetic dataset
Fig. 6. Experiment B.1 (Robustness against leakage).
leakage rate, but remains very small. For example, when the
leakage rate increases to 0.2%, the inference rates are only
0.41%, 0.39%, and 0.38% when the average segment sizes
are 512KB, 1MB, and 2MB, respectively. We observe that a
larger segment size implies a slightly smaller inference rate.
We conjecture the reason is that a larger segment size implies a
smaller probability that two different segments share the same
minimum chunk ﬁngerprint in this dataset. Thus, identical
plaintext chunks are more likely to be encrypted into multiple
distinct ciphertext chunks, and the frequency rank of ciphertext
chunks is more disturbed.