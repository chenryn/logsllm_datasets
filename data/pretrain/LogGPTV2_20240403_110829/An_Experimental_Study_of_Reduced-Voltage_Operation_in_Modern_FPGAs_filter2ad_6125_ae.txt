http://zedboard.org/product/zedboard. 2020.
[8] A. Bacha et al. Dynamic reduction of voltage margins by leveraging
on-chip ECC in Itanium II processors. In ISCA, 2013.
[9] A. Bacha et al. Using ECC feedback to guide voltage speculation in
low-voltage processors. In MICRO, 2014.
[10] R. Bertran et al. Voltage noise in multi-core processors: Empirical
characterization and optimization opportunities. In MICRO, 2014.
[11] S. Borkar et al. Design challenges of technology scaling. IEEE Micro,
1999.
[12] A. Boutros et al. You can not improve what you do not measure:
FPGA vs. ASIC efﬁciency gaps for convolutional neural network
inference. ACM TRETS, 2018.
[13] R. Brewer et al. The Impact of Proton-Induced Single Events on
Image Classiﬁcation in a Neuromorphic Computing Architecture.
TNS, 2019.
[14] Y. Cai et al. Threshold voltage distribution in MLC NAND ﬂash
memory: Characterization, analysis, and modeling. In DATE, 2013.
[15] Y. Cai et al. Read disturb errors in MLC NAND ﬂash memory:
Characterization, mitigation, and recovery. In DSN, 2015.
[16] Y. Cai et al. Error characterization, mitigation, and recovery in ﬂash-
memory-based solid-state drives. Proceedings of the IEEE, 2017.
[17] N. Chandramoorthy et al. Resilient Low Voltage Accelerators for
High Energy Efﬁciency. In HPCA, 2019.
[18] K. Chang et al. Understanding reduced-voltage operation in mod-
ern DRAM devices: Experimental characterization, analysis, and
mechanisms. SIGMETRICS, 2017.
[19] K. Chang et al. Voltron: Understanding and Exploiting the Voltage-
Latency-Reliability Trade-Offs in Modern DRAM Chips to Improve
Energy Efﬁciency. arXiv:1805.03175, 2018.
[20] A. Chatzidimitriou et al. Assessing the Effects of Low Voltage in
Branch Prediction Units. In ISPASS, 2019.
[21] T. Chen et al. DianNao: A small-footprint high-throughput accelera-
tor for ubiquitous machine-learning. In ISCA, 2014.
[22] Y. Chen et al. Eyeriss: A spatial architecture for energy-efﬁcient
dataﬂow for convolutional neural networks. In ISCA, 2016.
[23] Z. Chishti et al. Improving cache lifetime reliability at ultra-low
voltages. In MICRO, 2009.
[24] A. Cristal et al. LEGaTO: ﬁrst steps towards energy-efﬁcient toolset
for heterogeneous computing. In SAMOS, 2018.
[25] A. Cristal et al. LEGaTO: towards energy-efﬁcient, secure, fault-
tolerant toolset for heterogeneous computing. In CF, 2018.
[26] C. Deng et al. PermDNN: Efﬁcient compressed DNN architecture
with permuted diagonal matrices. In MICRO, 2018.
[27] D. Ernst et al. Razor: A low-power pipeline based on circuit-level
timing speculation. In MICRO, 2003.
[28] M. Feldman. https://www.top500.org/news/good-times-for-fpga-
[30] K. Givaki et al. On the Resilience of Deep Learning for Reduced-
voltage FPGAs. In PDP, 2020.
[31] D. Gizopoulos et al. Modern Hardware Margins: CPUs, GPUs,
FPGAs Recent System-Level Studies. In IOLTS, 2019.
[32] K. Guo et al. A survey of FPGA-based neural network accelerator.
[33] S. Han et al. Learning both weights and connections for efﬁcient
[34] S. Han et al. EIE: efﬁcient inference engine on compressed deep
arXiv:1712.08934, 2017.
neural network. In NIPS, 2015.
neural network. In ISCA, 2016.
2016.
[35] K. He et al. Deep residual learning for image recognition. In CVPR,
[36] Y. He et al. Channel pruning for accelerating very deep neural
networks. In ICCV, 2017.
[37] P. Hill et al. DeftNN: Addressing bottlenecks for dnn execution on
gpus via synapse vector elimination and near-compute data ﬁssion.
In MICRO, 2017.
[38] K. Himanshu et al. A 320 mV 56 μW 411 GOPs/W Ultra-Low
Voltage Motion Estimation Accelerator in 65 nm CMOS. JSSC,
2009.
[39] W. Huang et al. Temperature-Aware Architecture: Lessons and
Opportunities. IEEE Micro, 2011.
[40] S. Jha et al. Kayotee: A fault injection-based system to assess the
safety and reliability of autonomous vehicles to faults and errors.
arXiv:1907.01024, 2019.
[41] S. Jhaet et al. ML-based fault injection for autonomous vehicles: a
case for Bayesian fault injection. In DSN, 2019.
[42] N. Jouppi et al. In-datacenter performance analysis of a tensor pro-
cessing unit. In ISCA, 2017.
[43] M. Kaliorakis et al. Statistical analysis of multicore CPUs operation
in scaled voltage conditions. CAL, 2018.
[44] S. Karandikar et al. FireSim: FPGA-accelerated cycle-exact scale-out
system simulation in the public cloud. In ISCA, 2018.
[45] B. Khaleghi et al. FPGA Energy Efﬁciency by Leveraging Thermal
Margin. arXiv:1911.07187, 2019.
[46] F. Khorasani et al. In-register parameter caching for dynamic neural
nets with virtual persistent processor specialization. In MICRO, 2018.
[47] N. Sung Kim et al. Leakage current: Moore’s law meets static power.
Computer, 2003.
[48] S. Kim et al. MATIC: Learning around errors for efﬁcient low-voltage
neural network accelerators. In DATE, 2018.
[49] YD. Kim et al. Compression of deep convolutional neural networks
for fast and low power mobile applications. arXiv:1511.06530, 2015.
EDEN: Enabling Energy-Efﬁcient, High-
Performance Deep Neural Network Inference Using Approximate
DRAM. In MICRO, 2019.
[50] S. Koppula et al.
[51] A. Krizhevsky et al. ImageNet classiﬁcation with deep convolutional
neural networks. In NIPS, 2012.
[52] A. Lavin et al. Fast algorithms for convolutional neural networks. In
CVPR, 2016.
[53] Y. LeCun et al. Deep learning. Nature, 2015.
[54] S. Kyu Lee et al. A 16-nm Always-On DNN Processor With Adaptive
Clocking and Multi-Cycle Banked SRAMs. JSSC, 2019.
[55] J. Leng et al. GPU voltage noise: Characterization and hierarchical
smoothing of spatial and temporal voltage noise interference in GPU
architectures. In HPCA, 2015.
[56] J. Leng et al. Safe limits on voltage reduction efﬁciency in GPUs: a
direct measurement approach. In MICRO, 2015.
[57] J. Leng et al. Asymmetric Resilience: Exploiting Task-Level Idem-
potency for Transient Error Recovery in Accelerator-Based Systems.
In HPCA, 2020.
[58] G. Li et al. Understanding error propagation in deep learning neural
network (DNN) accelerators and applications. In SC, 2017.
[59] H. Li et al. On-Chip Memory Technology Design Space Explorations
for Mobile Deep Neural Network Accelerators. In DAC, 2019.
[60] Z. Li et al. E-RNN: Design optimization for efﬁcient recurrent neural
networks in FPGAs. In HPCA, 2019.
[61] F. Libano et al. Selective Hardening for Neural Networks in FPGAs.
TNS, 2018.
[62] F. Libano et al. Understanding the Impact of Quantization, Accuracy,
and Radiation on the Reliability of Convolutional Neural Networks
on FPGAs. TNS, 2020.
[63] Y. Liu et al. Fault injection attack on deep neural network. In ICCAD,
[64] Y. Ma et al. Optimizing the convolution operation to accelerate deep
neural networks on FPGA. TVLSI, 2018.
[65] MaxIntegrated. https://www.maximintegrated.com. 2019.
[66] T. Miller et al.
VRSync: Characterizing and eliminating
synchronization-induced voltage emergencies in many-core proces-
sors. In ISCA, 2012.
[67] P. Molchanov et al. Pruning convolutional neural networks for re-
source efﬁcient inference. arXiv:1611.06440, 2016.
[68] B. Moons et al. A 0.26-to-10 TOPs/W subword-parallel dynamic-
voltage-accuracy-frequency-scalable convolutional neural network
processor in 28nm FDSOI. In ISSCC, 2017.
[29] J. Fowers et al. A conﬁgurable cloud-scale DNN processor for real-
2017.
enthusiasts/. 2019.
time AI. In ISCA, 2018.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:26:55 UTC from IEEE Xplore.  Restrictions apply. 
148
[69] A. Moradi et al. Side-channel leakage through static power. In CHES,
[105] Y. Shen et al. Maximizing CNN accelerator efﬁciency through re-
2014.
[70] M. Hadi Mottaghi et al. Aging Mitigation in FPGAs Considering
Delay, Power, and Temperature. TR, 2019.
[71] H. Nakahara et al. A batch normalization free binarized convolutional
deep neural network on an FPGA. In FPGA, 2017.
[72] K. Neshatpour et al. Enhancing Power, Performance, and Energy
Efﬁciency in Chip Multiprocessors Exploiting Inverse Thermal De-
pendence. TVLSI, 2018.
[73] E. Nurvitadhi et al. Accelerating binarized neural networks: Compar-
ison of FPGA, CPU, GPU, and ASIC. In FPT, 2016.
[74] E. Nurvitadhi et al. Why Compete When You Can Work Together:
FPGA-ASIC Integration for Persistent RNNs. In FCCM, 2019.
[75] P. Pandey et al. GreenTPU: Improving Timing Error Resilience of a
Near-Threshold Tensor Processing Unit. In DAC, 2019.
[76] G. Papadimitriou et al. Harnessing voltage margins for energy efﬁ-
ciency in multicore CPUs. In MICRO, 2017.
[77] G. Papadimitriou et al. Voltage margins identiﬁcation on commercial
x86-64 multicore microprocessors. In IOLTS, 2017.
[78] G. Papadimitriou et al. Adaptive Voltage/Frequency Scaling and Core
Allocation for Balanced Energy and Performance on Multicore CPUs.
In HPCA, 2019.
[79] G. Papadimitriou et al. Exceeding Conservative Limits: A Consoli-
dated Analysis on Modern Hardware Margins. TDMR, 2020.
[80] A. Parashar et al. SCNN: An accelerator for compressed-sparse
convolutional neural networks. In ISCA, 2017.
[81] K. Parasyris et al. A Framework for Evaluating Software on Reduced
Margins Hardware. In DSN, 2018.
[82] J. Park et al. Scale-out acceleration for machine learning. In MICRO,
[83] Power Management Bus (PMBus). https://pmbus.org/. 2020.
[84] A. Putnam et al. A reconﬁgurable fabric for accelerating large-scale
datacenter services. In ISCA, 2014.
[85] J. Qiu et al. Going deeper with embedded FPGA platform for Convo-
lutional Neural Network. In FPGA, 2016.
2017.
neural network accelerators. In ISCA, 2016.
[87] B. Reagen et al. Ares: A framework for quantifying the resilience of
deep neural networks. In DAC, 2018.
[88] M. Riera et al. Computation reuse in DNNs by exploiting input
similarity. In ISCA, 2018.
[89] A. Roelke et al. Pre-RTL Voltage and Power Optimization for Low-
Cost, Thermally Challenged Multicore Chips. In ICCD, 2017.
[90] S. Salamat et al. Workload-aware opportunistic energy efﬁciency in
multi-FPGA platforms. arXiv:1908.06519, 2019.
[91] B. Salami. Aggressive undervolting of FPGAs: power & reliability
trade-offs. Ph.D. Dissertation, Universitat Politècnica de Catalunya
(UPC), 2018.
[92] B. Salami et al. HATCH: hash table caching in hardware for efﬁcient
relational join on FPGA. In FCCM, 2015.
[93] B. Salami et al. Accelerating hash-based query processing operations
on FPGAs by a hash table caching technique. In CARLA, 2016.
[94] B. Salami et al. AxleDB: A novel programmable query processing
platform on FPGA. MICPRO, 2017.
[95] B. Salami et al. A Demo of FPGA Aggressive Voltage Downscaling:
Power and Reliability Tradeoffs. In FPL, 2018.
[96] B. Salami et al. Comprehensive evaluation of supply voltage under-
scaling in fpga on-chip memories. In MICRO, 2018.
[97] B. Salami et al. Fault Characterization Through FPGA Undervolting.
In FPL, 2018.
[98] B. Salami et al. On the resilience of RTL NN accelerators: Fault
characterization and mitigation. In SBAC-PAD, 2018.
[99] B. Salami et al. Evaluating Built-in ECC of FPGA on-chip Memories
for the Mitigation of Undervolting Faults. In PDP, 2019.
[100] B. Salami et al. LEGaTO: Low-Energy, Segure, and Resilient Toolset
for Heterogeneous Computing. In DATE, 2020.
[101] H. Sharma et al. From high-level deep neural models to FPGAs. In
MICRO, 2016.
source partitioning. In ISCA, 2017.
[106] K. Simonyan et al. Very deep convolutional networks for large-scale
image recognition. arXiv:1409.1556, 2014.
[107] N. Suda et al. Throughput-optimized OpenCL-based FPGA accelera-
tor for large-scale convolutional neural networks. In FPGA, 2016.
[108] K. Swaminathan et al. Bravo: Balanced reliability-aware voltage
optimization. In HPCA, 2017.
[109] V. Sze et al. Efﬁcient processing of deep neural networks: A tutorial
and survey. Proceedings of the IEEE, 2017.
[110] C. Szegedy et al. Going deeper with convolutions. In CVPR, 2015.
[111] Z. Tang et al. The Impact of GPU DVFS on the Energy and Perfor-
mance of Deep Learning: an Empirical Study. In ACM e-Energy,
2019.
[112] M. Garay Trindade et al. Assessment of a Hardware-Implemented
Machine Learning Technique under Neutron Irradiation. TNS, 2019.
[113] A. Uht et al. Going beyond worst-case specs with TEAtime. Com-
puter, 2004.
[114] A. Vaishnav et al. A survey on FPGA virtualization. In FPL, 2018.
[115] X. Wang et al. Bit Prudent In-Cache Acceleration of Deep Convolu-
tional Neural Networks. In HPCA, 2019.
[116] P. Whatmough et al.
14.3 A 28nm SoC with a 1.2 GHz
568nJ/prediction sparse deep-neural-network engine with> 0.1 timing
error rate tolerance for IoT applications. In ISSCC, 2017.
[117] P. Whatmough et al. DNN Engine: A 28-nm timing-error tolerant
sparse deep neural network processor for IoT applications. JSSC,
2018.
[118] C. Wilkerson et al. Trading off cache capacity for reliability to enable
low voltage operation. ISCA, 2008.
[119] C. Wilkerson et al. Reducing cache power with low-cost, multi-bit
error-correcting codes. In ISCA, 2010.
[120] Q. Xiao et al. Exploring heterogeneous algorithms for accelerating
deep convolutional neural networks on FPGAs. In DAC, 2017.
[121] Xilinx.
7
Series
FPGAs Memory
Resources
https://www.xilinx.com/support/documentation/user_guides/ug473
_7Series_Memory_Resources.pdf. 2019.
DNNDK
Guide
https://www.xilinx.com/support/documentation/user_guides/ug1327-
dnndk-user-guide.pdf. 2019.
User
Guide
https://www.xilinx.com/support/documentation/ip_documentation/
dpu/v3_1/pg338-dpu.pdf. 2019.
Product
DPU
UltraScale Architecture Memory Resources
https://www.xilinx.com/support/documentation/user_guides/ug573-
ultrascale-memory-resources.pdf. 2019.
IP
[123] Xilinx.
[124] Xilinx.
[125] Xilinx.
Zynq UltraScale+ MPSoC ZCU102 Evaluation Kit
https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-
g.html, 2019.
[126] G. Yalcin et al. Exploiting a fast and simple ECC for scaling supply
voltage in Level-1 caches. In IOLTS, 2014.
[127] G. Yalcin et al. Exploring Energy Reduction in Future Technology
Nodes via Voltage Scaling with Application to 10nm. In PDP, 2016.
[128] L. Yang et al. SRAM voltage scaling for energy-efﬁcient convolu-
tional neural networks. In ISQED, 2017.
[129] R. Yazdani et al. The dark side of DNN pruning. In ISCA, 2018.
[130] M. Yufei et al. Optimizing loop operation and dataﬂow in FPGA
acceleration of deep convolutional neural networks. In FPGA, 2017.
[131] C. Zhang et al. Optimizing FPGA-based accelerator design for deep
convolutional neural networks. In FPGA, 2015.
[132] J. Zhang et al. Thundervolt: enabling aggressive voltage underscal-
ing and timing error resilience for energy efﬁcient deep learning
accelerators. In DAC, 2018.
[133] J. Jun Zhang et al. Analyzing and mitigating the impact of permanent
faults on a systolic array based neural network accelerator. In VTS,
2018.
[134] S. Zhang et al. Cambricon-x: An accelerator for sparse neural net-
works. In MICRO, 2016.
[135] X. Zhang et al. Shufﬂenet: An extremely efﬁcient convolutional
neural network for mobile devices. In CVPR, 2018.
[136] A. Zhou et al. Incremental network quantization: Towards lossless
CNNs with low-precision weights. arXiv:1702.03044, 2017.
[137] Z. Zhu et al. A Conﬁgurable Multi-Precision CNN Computing Frame-
work Based on Single Bit RRAM. In DAC, 2019.
[138] A. Zou et al. Voltage-Stacked GPUs: A Control Theory Driven Cross-
Layer Solution for Practical Voltage Stacking in GPUs. In MICRO,
2018.
[86] B. Reagen et al. Minerva: Enabling low-power, highly-accurate deep
[122] Xilinx.
[102] H. Sharma et al. Bit fusion: Bit-level dynamically composable
architecture for accelerating deep neural network. In ISCA, 2018.
[103] L. Shen et al. Fast Voltage Transients on FPGAs: Impact and Mitiga-
tion Strategies. In FCCM, 2019.
[104] Y. Shen et al. Escher: A CNN accelerator with ﬂexible buffering to
minimize off-chip transfer. In FCCM, 2017.
149
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:26:55 UTC from IEEE Xplore.  Restrictions apply.