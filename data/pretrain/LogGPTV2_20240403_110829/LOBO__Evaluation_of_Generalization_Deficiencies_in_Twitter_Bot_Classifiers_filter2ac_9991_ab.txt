Fake followers-FSF
Fake followers-INT
S
T
Fake followers-TWT
V
HoneyPot bots (Darpa)
W Attack on Ben Nimmo
X
Attack on Brian Krebs
—
2.75
7.67
96.79
92.35
99.37
94.10
98.28
100.00
66.08
97.81
20.89
10.90
1.32
0.00
100.00
100.00
95.34
27.69
59.09
83.05
Size
— 357,000
500,000
700,000
721
551
3,320
458
872
1
283
977
387
534
229
26
33
64
624
2,521
1,558
728
0.04
0.09
0.90
0.85
0.96
0.87
0.93
0.85
0.60
0.90
0.21
0.13
0.02
0.00
0.96
0.95
0.89
0.30
0.54
0.78
General dataset consisting of users and
several classes of bots or botnets
Bot Ci
User C
Randomly split into train/test 
(with or without balancing/subsampling)
Train Bci
Test Bci
Train Uc
Test Uc
First Training round
LOBO Model
Full Model
Train {Bci - Bct}
Train Uc
Train Bci
Classiﬁer testing on
target class
Train Uc
LOBO Model
Bot Ct
Full Model
Test Ct
Figure 2: Abstract representation of the LOBO test. The clas-
sier gets trained on all bot classes BCi except the target
class BCt , and then tested on the target class to assess how
well the classier generalizes.
can be seen in Table 1, which shows the average botometer scores
for each of the bot classes. To evaluate whether this tool would be
able to predict the dataset, we provide another metric which is the
percentage of the queried accounts that receive a botometer score
over 0.5 . Because of rate limiting, we only collected botometer
scores for up to 1,000 randomly selected accounts belonging to
each of the classes.
We can see that many of these bot classes are overwhelmingly
classied as users, for example, only 2.75% of the Bursty Bots are
classied as bots, and less than 10% of DeBot bots (dataset C) are
classied as bots. Both of them with average botometer scores less
than 0.1, indicating that they are “very likely” to be users.
Dierent bot classes will achieve reasonable and even perfect
performance on this bot detection task, but variability between bot
classes is very clear.
4 METHODOLOGY - THE LOBO TEST
Evaluating bot classiers faces an important challenge. For obvious
reasons, we are unable to evaluate the performance on the bots that
we haven’t seen. This is a real problem, as bots mutate all the time,
and botmasters are actively and creatively trying to get around any
form of detection (which sometimes means suspension from the
service).
In this paper we propose a new form of testing accuracy for
generalization of bot detection strategies. We call it the LOBO
test, for Leave One Botnet Out. It derives inspiration from cross
validation where a section of the available data is kept out, and
used for testing on N number of “fold.” Then, the section of the data
used for testing changes on each fold.
140
Table 1: Dierent bot datasets, their identiers, botometer
metrics, and number of accounts collected for each of them
3.2 Aggregated Bot Dataset
Our aggregated bot dataset is over 1.5 million bots with all their
available tweets. To the best of our knowledge, this is by far the
largest bot dataset that has been analyzed in research. It contains
bots from several dierent sources, including content polluters,
fake followers, silent accounts, phishing bots, and political bots
(albeit, in a wide array of quantities for each class).
3.3 User Dataset
To contrast against the bot datasets, we face the problem of nding
a suitable real-user dataset of similar size. While we could just
randomly sample Twitter to get an equal amount of users, this
methodology might result in including a small amount of bots in our
real-user class. To minimize this issue, we use crawling techniques
that attempt to give us an unbiased sample of the general real-user
population in Twitter. Please note that this methodology does not
guarantee that no bots will be represented in this dataset: it is just
a way of minimizing their presence.
We begin with a real user as a seed, and follow his outgoing
connections (friends only, not followers). We manually verify that
each of the users in the rst level are real users. We use up to 4
steps and obtain over 1 million English speaking users. We assume
that a real user is unlikely to follow bots. A similar approach was
used in [18]. While there might be a few bots in this dataset, the
vast majority of it must be real users.
3.4 Botometer Scores
Botometer [15] (previously botornot) is a public API that provides a
score based on whether an account is likely to be a bot or a user. It
has been used to verify bot accounts in other research [9]. For our
dierent bot classes, botometer does not perform well enough. This
LOBO – Evaluation of Generalization Deficiencies
in Twier Bot Classifiers
The LOBO test was created to assess whether a classier can
detect a bot class, which we will call the target class, by training
only with other bot classes, explicitly without any direct knowledge
of the target class itself. It was conceived strictly in the context of a
binary classication between bots and users, to specically address
the variety of bot classes that such a classier would face.
We assume the LOBO test to be a proxy for generalization. For
example, let us take the Bursty bots as the target class. We train
the classier with all other datasets except dataset B, then we test
the classier against dataset B. If the classier performs well on
the Bursty bots when it hasn’t actually trained on them, then it
has “generalized” from the seen bot classes to this target class. A
owchart of how the test should be applied can be seen in Fig 2.
Train-Test split. We now face the need to compare between a
classier that has been trained with and without the target class.
Addressing this need, we decided to use a 70-30 train-test split
instead of cross validation. Each bot class is randomly and indepen-
dently sampled so that 70% of each bot class is in the training set
and 30% in the testing set. This is to ensure that the smallest classes
will still be represented and tested properly. This strategy allows
us to test a single bot class using the 30% test data for that specic
class, which has never been seen by the classier.
One could argue that comparing accuracy on 30% of the target
class with accuracy tested on 100% of the target class is misleading.
However, this split is not strictly part of the LOBO test, we nd it
useful to test on at least 30% of each bot class to provide context
to the performance of the classier on the target class when it has
already trained on it.
5 FEATURES FOR CLASSIFICATION
This section describes the features to be used in our classier. Most
of these features have been used in research before and are relatively
common place. We placed specic importance on not including
graph information. Twitter imposes strong rate limits on graph
information, making it very time consuming to collect the followers
of a popular user (e.g., a celebrity or politician) from Twitter’s API.
Additionally, any real time implementation of this classier would
need to depend on a few API calls, which does not bode well with
the inclusion of graph information.
Table 2 shows all the features that are used in the classiers
analysed in this paper. Also, the way the datasets at hand were
nally prepared for feature extraction is illustrated in Fig. 1. Next,
we dene some of the features that might not be straight forward.
5.1 User Features
We include several features obtainable directly from a user prole.
Seconds active and days active is the number of seconds and days
between account creation and last obtainable tweet.
Their maximum value is 1 month and 3 years respectively, to
account for dierences in collection dates. While these two features
might seem redundant, seconds active is able to detect bots that
tweet immediately after creation date and then fall silent, which
have been detected in large numbers [17, 18]. "Days active" is better
suited to address how long a user has been tweeting. Merging them
in a single feature would possibly lose some of the details. Total
tweet count is the number of lifetime tweets the account has created
141
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
or retweeted, and this number comes directly from the prole and
is not limited to the 3200 tweets that we get through the API. It
includes tweets that have been deleted. We consider these prole
features because they can be obtained from a single API call to the
user prole (which includes the user’s last tweet).
5.2 Tweet Features
We use the tweets and their text obtained from users to extract
useful features. The most basic ones such as number of tweets and
retweets, average tweet and retweet length, etc., are considered,
and other more elaborate features are computed and used.
Hashtags. Hashtags are a way of grouping topics within Twitter.
We have created features from the number of hashtags in analyzed
tweets and retweets, number of unique hashtags, and the unique
hashtag ratio. Unique hashtags are included to account for the
dierence between accounts tweeting many times using a small
set of hashtags against people who are involved and tweeting over
many dierent hashtags.
Mentions. Mentions are a way of publicly addressing another user.
They are also commonly abused by spammers to generate engage-
ment with unsuspecting users. We use the number of mentions in
tweets and in retweets as features. We also include the number of
unique mentions as a feature.
Edit Distance. To account for tweets that are equal or with small
variations, we use the edit distance between tweets and retweets
of a user. The edit distance (or Levenshtein distance) between two
strings is the minimum number of one-character edits to turn one
string into the other. Because of processing time, we only evaluate
this feature for the last 200 tweets and the last 200 retweets of each
user; each of these tweets is compared to the rest, and then the
mean of the distances is computed.
Geolocation. Depending on user preference, each tweet can have
geolocation embedded, consisting of latitude and longitude. We use
the number of tweets that are geolocated, as well as the percentage
of the analyzed tweets that have this information, as features.
Tweet Sources. When apps are used to publish tweets through
Twitter’s API, an app publisher needs to dene the "source" of the
tweet. We use the number of unique sources used to publish the
tweets as a feature. While some older botnets rely on using a single
source to publish all of their tweets [17, 18], other botnets may use
as many sources as possible to confuse detection eorts. Regardless
of the assumption, we calculate this feature for each of the users in
our dataset.
Favorites. Marking a tweet as a favorite or “liking” it, is an action
a user can take to endorse a specic tweet. We use the number
of tweets a user has liked as a feature. However, we also include
how many of a user’s tweets have been marked as favorites by
other users, and the ratio between liked tweets and analyzed tweets
(or favorites per tweet). This summarizes both directions of the
endorsement: how much the user being analyzed endorses other
users, and how much other users endorse the user at hand.
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Juan Echeverrï£¡a et al.
User ID
Friend To Follower Ratio
Seconds active
Prole description length
# Geolocated tweets
Avg. Edit distance (Rtw)
Avg. Edit distance (Twt)
# Tweets analyzed
# APIs used
# Unique mentions
# Retweets analyzed
User Features
# Followers
# User favorites
Days active
# Friends
username length
Total tweet count
Tweet Features
% of Geolocated Tweets
# Hashtags
# URLs
# Favorites
# Unique hashtags
Avg. tweet length
# Mentions (Rtw)
# Mentions (Twt)
URLs (per tweet)
Favorites (per tweet)
Unique hashtag ratio
Avg. retweet length
Table 2: Features employed for classication.
6 EXPERIMENTS
In this section, we describe the experimental setups used, machine
learning classiers applied and results extracted using the LOBO
test under dierent scenarios.
6.1 Subsampling
Dataset with class size  30k In real life, botnets will come in
varying sizes. Furthermore, the amount of data that will be available
for training each bot class will vary even more. As an easy example,
the bot classes analyzed in this paper range from tens of accounts to
hundreds of thousands. To provide an opportunity for our smallest
bot classes, in this experimental setup we limit the numbers of the
three larger bot classes. With this in mind, we include only 30,000
randomly sampled bot accounts from each of these datasets (A, B,
and C). However, we include all of the bots in datasets D-X for a
total of ⇠ 105,000 bots. The reasoning behind this is to not allow
our three large datasets to exceed 100 times the largest of our other,
smaller, datasets.
To contrast these bots against users, we randomly sample our
real user dataset to only include 105,000 accounts. The aggregation
of these 105k users and 105k bots will be referred to as the dataset
with class size  30k, C30K for short.
Dataset with class size = 500 Dataset C30K is still quite imbal-
anced, having classes with 30,000 bots and classes with 26 bots.
To measure the eect of bot classes without being biased by their
size, we create another bot dataset. This balanced sub-sampled bot
dataset contains 500 random instances from each of the bot classes
that have over 500 accounts in them. This means a few of the bot
datasets have been excluded, but choosing 500 as the size still allows
us to have 14 bot datasets to evaluate on. This bot dataset is made
of 7,000 bot instances.
To contrast against this bot dataset, we add an equivalent number
of users from our user dataset. The aggregation of both of these
datasets will be referred to as the dataset with class size = 500, or
C500. Please note that this dataset is created on the y every time
it is needed. Fig. 1 shows the data collection ow from the bot class
identication to our C30K and C500 datasets.
142
Algorithm
LGBM
XGBC
Random Forest
DecissionTree
AdaBoost
Acc.
97.84%
95.91%
97.02%
95.99%
94.29%
AUC
0.98
0.96
0.97
0.96
0.94
C30K
C500
Acc.
93.93%
91.24%
91.54%
86.93%