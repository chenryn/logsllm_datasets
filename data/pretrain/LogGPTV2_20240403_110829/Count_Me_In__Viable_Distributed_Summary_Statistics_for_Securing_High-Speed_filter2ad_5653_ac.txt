c
S
5000
Feb 24
Mar 03
Mar 10
Mar 17
r
u
o
h
r
e
p
s
t
s
o
h
d
e
n
n
a
c
s
e
g
a
r
e
v
A
15000
10000
5000
0
09
809
1
8
1
809
1
809
1
809
1
809
1
809
1
Fig. 3. Incoming address scans per hour
from 2014-02-19 to 2014-03-20 at IU
Fig. 4. Aggregate count of incoming addr.
scans from 2014-02-19 to 2014-03-20 at IU.
4.2 Brute-force Login Detection
A common type of attack concerns brute-forcing accounts by trying a large num-
ber of username and password combinations. We implemented scripts to detect
such attacks for the FTP and SSH protocols. For FTP, the script counts the
number of failed FTP authentication attempts and generates an alarm when it
sees more than, by default, 20 attempts from a speciﬁc source to a particular
destination host within 15 minutes. For SSH, Bro provides a heuristic that de-
termines if a login succeeded or failed, based on the volume of data exchanged
as well as the number of packets seen during the session. Our script counts the
number of times this heuristic reports a successful login and triggers an alarm
when that number exceeds 30 in a 30 minute interval. A number of sites, includ-
ing Indiana University, are currently running the brute-force detection scripts in
their production setups.
332
J. Amann, S. Hall, and R. Sommer
4.3 SQL Injection Detection
We also created script for detecting automated SQL injection attacks, using a
similar thresholding approach as above. When targeting a web server, attackers
often iterate through a large library of canned injection URIs within a short time
frame. To detect this kind of attack, we ﬁrst wrote a regular expression that
matches typical injection URIs (e.g., /site.php?site=5’ and 1=1 and ”=’).4
We then set up two summary statistic instances. Both count the number of times
the regular expression matches. For the ﬁrst instance, the key is the source IP
address while for the second we use the destination address. In other words, the
ﬁrst identiﬁes attack sources (independent of how many victims each targets),
and the second reports servers under attack (independent of the number of their
attackers). In addition, both summary statistic instances also apply an additional
Sample reducer, which keeps 5 URIs that have matched the regular expression.
Once one of the instances hits a conﬁgured threshold of matching requests (50
in 5 minutes by default), the detector triggers an alert email that summarizes
the detected SQL injection attack, including the 5 URIs as additional context.
4.4 Traceroute detection
Traceroute detection constitutes another use-case for the summary statistics
framework. While a traceroute does usually not pose a direct security threat, it
may indicate reconnaissance preceding an attack. Traceroutes are however chal-
lenging to identify in clustered monitoring setups where traﬃc is load-balanced
across diﬀerent monitoring systems according to its 5-tuple of addresses, ports,
and protocol. As the ICMP packets belonging to one execution will often arrive
at diﬀerent nodes, no single node can spot it by itself.
For our detector, we use a single summary statistic instance with two reducers.
One of them counts the number of packets per host pair with TTLs lower than
10. The second counts the number of ICMP Time Exceeded messages relating
to the same hosts.
We consider a traceroute to be in progress if we see at least one low-TTL
packet between a pair of hosts along with at least three matching ICMP Time
Exceeded messages. Leveraging the summary statistics framework allows to de-
ﬁne such a logic at a semantic level with a single if-statement, without needing
to consider the underlying traﬃc splitting any further. We validated this scheme
by running it on the Bro cluster of the National Center for Supercomputing
Applications at the University of Illinois, manually executing traceroutes and
sampling the corresponding reports during normal operation. Ignoring our own
activity, the large number of otherwise incoming traceroutes we saw (more than
2,000 a day) surprised us. Many of them turned out to be targeting a local
content management system.
4 This turns out harder than it sounds: We have developed, and continuously reﬁned,
this regular expression for more than 5 years now by regularly evaluating network
traﬃc and adding new cases as we discovered them. The expression has a size of
more than 1,500 characters today.
Count Me In: Viable Distributed Summary Statistics
333
4.5 Top-k
As examples of “top-k” measurements, we wrote a script that tracks (i) the top-10
source and destination hosts exhibiting the most established TCP connections;
(ii) the top-10 second-level domains in DNS queries; and (iii) the top-10 Host
header values present in HTTP requests.5 We consider only outgoing traﬃc and
calculate rankings over both 10-minute and 1-hour intervals.
Table 1. Top-10 outgoing DNS 2nd-level lookups and HTTP Host values (19-3-2014,
15:15–16:15)
DNS domain
.akamai.net
.akamaiedge.net
.berkeley.edu
.amazonaws.com
.google.com
.akadns.net
.yuerengu.com.cn
.cloudfront.net
.spameatingmonkey.net
.ustiming.org
Total DNS req. (exact)
Upper bound 
123,293 0
111,760 0
87,539 0
77,521 0
72,156 0
70,284 0
62,996 0
59,607 0
56,673 0
56,513 0
10,985,712
 HTTP host
0 b.scorecardresearch.com
0 www.google-analytics.com
0 pagead2.googlesyndication.com
0 ib.adnxs.com
0 ad.doubleclick.net
0 pixel.quantserve.com
0 www.google.com
0 i1.ytimg.com
Upper bound
276,592
185,150
158,938
148,584
137,474
135,519
92,210
60,234
57,089 142 googleads.g.doubleclick.net
38,108 719 setiboincdata.ssl.berkeley.edu
4,220,837
Total HTTP requests (exact)
For demonstration purposes we ran this script on a 28-node Bro research clus-
ter operating at the University of California, Berkeley; monitoring the campus’
2x10 GE uplink connections [25]. Daytime volume averages between 3-4 Gb/s to-
tal. Table 1 shows a snapshot of the 1-hour DNS/HTTP statistics from an early
Monday afternoon. Recall that the top-k calculation uses a probabilistic data
structure and, hence, the results represent estimates. The table includes what
the algorithm reported as upper bounds for the number of times it encountered
each value. In addition, the table also shows the corresponding uncertainty ;
subtracting  from the upper bound gives the lower bound. This means that,
e.g., a DNS request for .ustiming.org was encountered between 37,389 and
38,108 times. We see that generally the error rates remain very low, consider-
ing the large amount of traﬃc with high numbers of unique DNS domains and
HTTP hosts (154,859 and 100,269, respectively, during the shown time interval;
calculated independently from logs). For these measurements, we conﬁgured the
probabilistic algorithm to keep at most 1,000 diﬀerent values in memory for each
summary statistic at any point of time.
5 The Host headers provides an application-level view of popular web sites, vs. just
looking at IP addresses. Web site addresses have become quite meaningless today
with many services running on generic cloud infrastructure.
334
J. Amann, S. Hall, and R. Sommer
4.6 Traﬃc Matrix
The summary statistics framework can also be used to compute traﬃc matrices,
such as for breaking down overall volume by subnets. To demonstrate this, we
created a small Bro script which sets up a single summary statistics framework
instance using two reducers tracking the volume of incoming and outgoing traﬃc
by source, respectively. Additionally, the reducers deﬁne a key normalization
function, which maps the source address of each individual observation to the
containing /24 network in which the host resides. We deployed the top-k script
on the Berkeley research cluster discussed in §4.5. Table 2 shows the output for
the 5 (anonymized) subnets with the largest amount of total traﬃc during the
observed one-hour period, out of 502 unique local subnets encountered.
4.7 Real-Time Visualization
As our ﬁnal application, we extended the previous “top-k” setup to visualize the
results in real-time. See Fig. 5 for a screenshot. Internally, the extended Bro
script uses the intermediate value update mechanism of the summary statistics
framework to get current values every 15 seconds. It then sends the aggregated
valued to Bro’s logging framework, which supports a number of diﬀerent out-
put formats including TSV ﬁles and databases. For this application, we added
support for Apache’s ActiveMQ message queuing framework so that Bro can
send the values directly to an ActiveMQ server. We created an HTML page that
uses JavaScript for visualizing the values via a persistent WebSocket connection.
After each update, the value changes are immediately reﬂected in the browser
window.
In
Bytes
Subnet
Out Total
UCB Subnet A 124G 56.0G 180G
UCB Subnet B 123G 22.7G 146G
UCB Subnet C 39.7G 48.1G 87.9G
UCB Subnet D 23.3G 2.15G 25.5G
UCB Subnet E 18.6G 1.19G 19.8G
Table 2. UCB Top-5 local subnets
by total traﬃc (28-3-2014, 11:41–
12:41)
5 Evaluation
Fig. 5. Screenshot top-10 HTTP hosts (by
headers) live visualization (4-4-2014, 9:28)
In this section we evaluate the overhead introduced by the summary statistics
framework in terms of computation, memory, and communication. Our objective
concerns ensuring that the implementation provides the performance necessary
Count Me In: Viable Distributed Summary Statistics
335
to operate in large-scale distributed environments. We focus on two applica-
tions: Top-k (§4.5), as the most resource-intensive application; and scan detec-
tion (§4.1), which stresses the inter-node communication the most.
5.1 Correctness
We ﬁrst brieﬂy double-check correctness of the summary statistics framework’s
calculations. While not directly an issue for the simpler calculations, the proba-
bilistic data structures by design introduce errors into their results, along with
worst-case bounds derived from their mathematical foundation. In Table 1, we
show top-k results along with their error margins for a 1-hour measurement pe-
riod in a large-scale cluster setup (see §4.5). We cross-check the reported num-
bers by calculating the actual top-k lists oﬄine out of the log ﬁles that the
Bro cluster produced during the same execution. We ﬁnd that despite using the
memory-eﬃcient probabilistic data structure: (i) the summary statistics frame-
work correctly identiﬁes all entries in the right order in all but two cases, and
(ii) all the actual values indeed fall within the given error margin. Regarding
the former, the two exceptions concern the top sources. During our measurement
the counts for 8 of the top 10 IP addresses were very close to each other. In both
cases, the reported uncertainty  (see §4.5) was greater than the diﬀerence to
the next values. Hence, a user can indeed conclude from the numbers that while
the reported ordering might not be fully correct, it must be closely matching the
actual activity.
5.2 Computational Overhead
Internally, the summary statistics framework is a complex module consisting of
several hundred lines of Bro script code for the basic framework, separate scripts
for the plugins, and low-level core support for the probabilistic data structures.
For evaluating the computational overhead that this extension introduces, we
captured a packet trace of about 20-minutes at the Internet uplink of UC Berke-
ley (see §4.6). To keep the volume manageable we recorded just a subset of the
total traﬃc, corresponding to what one node of the Bro research cluster pro-
28 of all ﬂows).6 The resulting trace includes 19.8 M packets and
cesses (i.e., 1
516 K ﬂows, at a total volume of about 15 GB.
We measure CPU load with three diﬀerent conﬁgurations: (i) Bro’s default
setup with the summary statistics framework disabled; (ii) enabling the scan
detector from §4.1; and (iii) enabling the top-k script from §4.5; For each conﬁg-
uration, we measure CPU utilization per 1 sec trace interval. The trace is replayed
using the pseudo-realtime mode [23] of Bro, which was created to facilitate the
realistic playback of packet-traces.
6 In other words, we assess the performance overhead for one worker node. We do
not examine the CPU overhead of the manager node merging the data structures as
that system is typically not CPU-bound and has suﬃcient head-room for additional
operations.
336
J. Amann, S. Hall, and R. Sommer
y
t
i
s
n
e
d
y