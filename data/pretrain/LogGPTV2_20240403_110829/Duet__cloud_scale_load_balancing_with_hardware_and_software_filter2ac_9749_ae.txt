Microbenchmark results in §7.2 showed that DUET can handle
HMux failures well – the VIPs fall back to SMux, and the disrup-
tion to the VIP trafﬁc is minimal. In §8.2, we considered the num-
ber of SMuxes DUET needs to cope with failures. We now consider
the bigger picture – what impact does failures of several switches,
or even a container have on overall trafﬁc?
We consider the same failure model as was used in §8.2 – a con-
tainer or up to 3 switches can fail simultaneously. We evaluate fail-
ure resilience of DUET by measuring the maximum link utilization
under these two scenarios: failure of a randomly selected container,
or 3 randomly selected switches.
A random switch failure affects link trafﬁc load in two ways.
It causes the trafﬁc of the VIPs assigned to the failed switch to
be shifted to the backstop SMuxes, and other through trafﬁc to be
shifted to the alternative path. A container failure affects the trafﬁc
in more complicated ways: it not only causes all the switches inside
to be disconnected, but also makes all the trafﬁc with sources and
destinations (DIPs) inside to disappear.
Figure 19 shows the measured maximum link utilization during
the two failure scenarios in the 10 experiments. We see that as
expected, link failures can result in transient congestion. However,
 1 10 100 1000 100001.252.5510Number of SMuxesTraffic (Tbps)Duet(10G)DuetAnanta(10G)Ananta1021031045k10k15kLatency (usec)Number of SMuxesAnantaDuet 1 10 100 10001.252.5510Number of SMuxesTraffic (Tbps)DuetRandom 0 0.2 0.4 0.6 0.8 1 1.21.252.5510Max. Link UtilizationTraffic (Tbps)NormalSwitch-failContainer-fail36(a) Trafﬁc load-balanced by HMux
(b) Trafﬁc shufﬂed during migration
Figure 20: Effectiveness of different migration algorithms.
(c) Number of SMux
the utilization increase of any link in the network is no more than
16%, and hence is comfortably absorbed by the 20% bandwidth
reservation made in the VIP assignment algorithm. Interestingly,
the single container failure (with 44 switches inside) often results
in less congestion than 3-switch failure. This can be explained by
two reasons: (1) any trafﬁc with source and sinks (DIPs) inside the
container has disappeared, and (2) all the rest trafﬁc which have
sources or sinks outside the container are not shifted to other paths
as their paths do not go through any switch inside the container.
8.6 VIP Migration
In this section, we evaluate the effectiveness of DUET’s VIP
migration algorithm, Sticky (§4.2). We set the threshold to be
δ = 0.05, i.e., a VIP will migrate to a new assignment only if
doing so reduces the MRU by 5%.
We compare Sticky with Non-sticky, which calculates the new
assignment from scratch based on current trafﬁc matrix (§4.1), but
migrates all the VIPs at the same time through SMuxes to avoid
the memory deadlock problem. We evaluate these two schemes by
re-running the 3-hour trafﬁc trace, where we reassign and migrate
the VIPs for Sticky and Non-sticky every 10 minutes. The total VIP
trafﬁc varies between 6.2 to 7.1 Tbps in this trace.
Effectiveness: We ﬁrst compare the portion of total trafﬁc that
are handled by the HMuxes under the two assignment schemes –
the larger the portion, the more effective the assignment algorithm.
Here, we also compare Sticky and Non-sticky against One-time al-
gorithm, which assigns the VIPs at time 0 sec, and never change
it. Figure 20(a) shows the results over the duration of the trace.
First, as expected, while the portion of trafﬁc handled by HMuxes
started out the same, the initial assignment which is used by One-
time throughout the trace, gradually loses its effectiveness, and re-
sults in only 60-89% (average 75.2%) of the total being handled by
HMuxes. In contrast, Sticky and Non-sticky handle 86-99.9% (av-
erage 95.3%) of the trafﬁc in HMuxes, from continuously adapting
to the trafﬁc dynamics. Second, even though Sticky only migrates
VIPs that reduce the MRU by at least 5%, it is as effective as Non-
sticky in maximizing the trafﬁc assigned to HMuxes. In particular,
it handles 86-99.7% trafﬁc (average 95.1%) in HMuxes, which is
almost identical to the 87-99.9% trafﬁc (average 95.67%) handled
by HMuxes under Non-sticky.
Trafﬁc shufﬂed: Next, we compare the fraction of the total
VIP trafﬁc migrated under Sticky and Non-sticky– the less trafﬁc
are migrated, the fewer SMuxes need to be reserved as stepping
stone. Figure 20(b) shows that migration using Non-sticky results in
reshufﬂing almost 25-46% (average 37.4%) of the total VIP trafﬁc
each time throughout the trace duration, compared to only 0.7-4.4%
(average 3.5%) under Sticky. Such a drastic reduction in the trafﬁc
shufﬂed under Sticky is attributed to its simple ﬁltering scheme: a
VIP is only migrated if it improves the MRU by 5%.
Number of SMuxes: Figure 20(c) shows the number of SMuxes
needed by Sticky and Non-sticky. Additionally, we also calculate
the SMuxes needed without migration (marked as No-migration) as
well as number of SMuxes needed in Ananta considering the SMux
capacity to 3.6Gbps. The number of SMuxes needed in Sticky
and Non-sticky is calculated as maximum of SMuxes needed for
VIP trafﬁc, failure and transition trafﬁc. It can be seen that, Non-
sticky always requires more SMuxes compared to No-migration
and Sticky, showing that Sticky does not increase the number of
SMuxes to handle the trafﬁc during migration.
9. DISCUSSION
Why are there empty entries in switch tables? DUET uses
empty entries in the host table, ECMP table, and tunneling table
in switches to implement HMux. Several reasons contribute to the
abundance of such free resources in our production datacenter. The
host table of ToR switches has only a few dozen entries for the
hosts within each rack, and that of the rest of the switches is mostly
empty. The ECMP table of switches is mostly empty because of
the hierarchical DC network topology, where each switch has a
small number of outgoing links among which all outgoing trafﬁc
is split via ECMP. The tunneling table is mostly free since few on-
line services use encapsulation other than load balancing itself. We
acknowledge that other DCs may have a different setup, but we
believe that our design will be applicable in common cases.
VIP assignment: While the greedy VIP assignment algorithm
described in §4 works well in our scenarios, we believe that it can
be improved. The VIP assignment problem resembles bin pack-
ing problem, which has many sophisticated solutions. We plan to
study them in future. Also, while we consider VIPs in order of traf-
ﬁc, other orderings are possible (e.g., consider VIPs with latency
sensitive trafﬁc ﬁrst).
Failover and Migration: DUET relies on SMuxes to simplify
failover and migration. As hinted in §3.3, it may be possible to
handle failover and migration by replicating VIP entries in multi-
ple HMuxes. We continue to investigate this approach, although
our initial exploration shows that the resulting design is far more
complex than our current design.
10. RELATED WORK
To the best of our knowledge, DUET is a novel approach to build-
ing a performant, low-cost, organically scalable load balancer. We
 0.5 0.75 1 0 30 60 90 120 150 180% traffic on HMuxTime (min)Non-stickyStickyOne-time 0 50 100 0 30 60 90 120 150 180% migrated trafficTime (min)Non-stickySticky 1 10 100 1000 100001.252.5510Number of SMuxesTraffic (Tbps)No-migrationStickyNon-stickyAnanta37are not aware of any load balancing architecture that fuses switch-
based load balancer with the software load balancers. However,
there has been much work on load balancers, and we brieﬂy review
it here.
Load balancer: Traditional hardware load balancers [4, 1] are
expensive and typically only provide 1+1 availability. DUET is
much more cost effective, and provides enhanced availability by
using SMuxes as a backstop. Importantly, compared to traditional
load balancers, DUET gives us control over very important vantage
point in our cloud infrastructure.
We have already discussed Ananta [17] software load balancer
extensively. Other software-based load balancers [5, 6, 7] are also
available, but they lack the scalability and availability of Ananta, as
shown in [17]. Embrane [3] promises scalability, but suffers from
the same fundamental limitations of the software load balancer.
OpenFlow based load balancer: Two recent proposals focus
on using OpenFlow switches for load balancing. In [20], authors
present a preliminary design for a load balancing architecture us-
ing OpenFlow switches. They focus on minimizing the number
of wildcard rules. The paper, perhaps because it is a preliminary
design, ignores many key issues such as handling switch failures.
Plug-n-Serve [15] is another preliminary design that uses Open-
Flow switches to load balance web servers deployed in unstruc-
tured, enterprise networks. DUET is very different from these ap-
proaches. DUET uses a combined hardware and software approach.
DUET does not rely on OpenFlow support. DUET is designed for
data center networks, and pays careful attention to handling nu-
merous practical issues including various types of failures and VIP
migration to adapt to network dynamics.
Partitioning OpenFlow rules: Researchers have also proposed
using OpenFlow switches for a variety of other purposes. For ex-
ample, DIFANE [22] uses some switches in the data center to cache
rules, and act as authoritative switches. While a load balancing ar-
chitecture can be built on top of DIFANE, the focus of the paper is
very different from DUET. In vCRIB [16] authors propose to of-
ﬂoad some of the trafﬁc management rules from host agent to ToR
switches, as well as to other host agents. Their goal is to ensure
resource-aware and trafﬁc-aware placement of rules. While vCRIB
also faces problems such as managing network dynamics (e.g., VM
migration), their main focus is quite different than DUET.
SDN architecture and middleboxes: Similar to DUET, re-
searchers have leveraged SDN architecture in the context of mid-
dleboxes to achieve policy enforcement and veriﬁcation [18, 12],
which is again a different goal than DUET.
Improving single server performance: Researchers have sub-
stantially improved packet processing capabilities on commodity
servers [23, 11], which could potentially improve SMux perfor-
mance. But, these improvements are unlikely to bridge the differ-
ences in packet processing capabilities between HMux and SMux
for the load balancer workload.
Lastly, several algorithms for calculating ﬂow hashes (e.g., re-
silient hashing [2], cuckoo-hashing [23]) offer a wide variety of
trade-offs. We do not review them here, although DUET can lever-
age any advances in this ﬁeld.
11. CONCLUSION
DUET is a new distributed hybrid load balancer designed to pro-
vide high capacity, low latency, high availability, and high ﬂexibil-
ity at low cost. The DUET design was motivated by two key ob-
servations: (1) software load balancers offer high availability and
high ﬂexibility but suffer high latency and low capacity per load
balancer, and (2) commodity switches have ample spare resources
and now also support programmability needed to implement load
balancing functionality. The DUET architecture seamlessly inte-
grates the switch-based load balancer design with a small deploy-
ment of software load balancer. We evaluate DUET using a pro-
totype implementation and extensive simulations using traces from
our production DC. Our evaluation shows that DUET provides 10x
more capacity than a software load balancer, at a fraction of its cost,
while reducing the latency by over 10x, and can quickly adapt to
network dynamics including failures.
Acknowledgements
We thank the members from Microsoft Azure team, especially
Chao Zhang, for their help in shaping DUET. We also thank the
reviewers and our shepherd Ali Ghodsi for their helpful feedback.
12. REFERENCES
[1] A10 networks ax series. http://www.a10networks.com.
[2] Broadcom smart hashing.
http://http://www.broadcom.com/collateral/wp/
StrataXGS_SmartSwitch-WP200-R.pdf.
[3] Embrane. http://www.embrane.com.
[4] F5 load balancer. http://www.f5.com.
[5] Ha proxy load balancer. http://haproxy.1wt.eu.
[6] Loadbalancer.org virtual appliance.
http://www.load-balancer.org.
[7] Netscalar vpx virtual appliance. http://www.citrix.com.
[8] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan. Data center TCP
(DCTCP). In SIGCOMM, 2010.
[9] P. Bodík, I. Menache, M. Chowdhury, P. Mani, D. A. Maltz, and
I. Stoica. Surviving failures in bandwidth-constrained datacenters. In
SIGCOMM, 2012.
[10] C. Chekuri and S. Khanna. On multi-dimensional packing problems.
In SODA, 1999.
[11] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall,
G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy.
Routebricks: Exploiting parallelism to scale software routers. In
SOSP, 2009.
[12] S. Fayazbakhsh, V. Sekar, M. Yu, and J. Mogul. Flowtags: Enforcing
network-wide policies in the presence of dynamic middlebox actions.
Proc. HotSDN, 2013.
[13] P. Gill, N. Jain, and N. Nagappan. Understanding network failures in
data centers: measurement, analysis, and implications. In ACM
SIGCOMM CCR, 2011.
[14] J. Hamilton. The cost of latency. http://perspectives.
mvdirona.com/2009/10/31/TheCostOfLatency.aspx.
[15] N. Handigol, S. Seetharaman, M. Flajslik, N. McKeown, and
R. Johari. Plug-n-serve: Load-balancing web trafﬁc using openﬂow.
ACM SIGCOMM Demo, 2009.
[16] M. Moshref, M. Yu, A. Sharma, and R. Govindan. Scalable rule
management for data centers. In NSDI, 2013.
[17] P. Patel et al. Ananta: Cloud scale load balancing. In SIGCOMM,
2013.
[18] Z. A. Qazi, C.-C. Tu, L. Chiang, R. Miao, V. Sekar, and M. Yu.
Simple-fying middlebox policy enforcement using sdn. In
SIGCOMM, 2013.
[19] L. Ravindranath, J. padhye, R. Mahajan, and H. Balakrishnan.
Timecard: Controlling User-Perceieved Delays in Server-based
Mobile Applications. In SOSP, 2013.
[20] R. Wang, D. Butnariu, and J. Rexford. Openﬂow-based server load
balancing gone wild. In Usenix HotICE, 2011.
[21] X. Wu, D. Turner, C.-C. Chen, D. A. Maltz, X. Yang, L. Yuan, and
M. Zhang. Netpilot: automating datacenter network failure
mitigation. ACM SIGCOMM CCR, 2012.
[22] M. Yu, J. Rexford, M. J. Freedman, and J. Wang. Scalable ﬂow-based
networking with difane. In SIGCOMM, 2010.
[23] D. Zhou, B. Fan, H. Lim, M. Kaminsky, and D. G. Andersen.
Scalable, high performance ethernet forwarding with cuckooswitch.
In CoNext, 2013.
38