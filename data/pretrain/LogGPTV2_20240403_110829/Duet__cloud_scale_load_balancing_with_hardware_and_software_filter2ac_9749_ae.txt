### Microbenchmark Results and Failure Resilience

The microbenchmark results in §7.2 demonstrated that DUET can effectively handle HMux failures, as VIPs seamlessly fall back to SMux with minimal disruption to VIP traffic. In §8.2, we examined the number of SMuxes required by DUET to manage failures. Now, we will explore the broader impact of multiple switch failures or even an entire container failure on overall traffic.

We use the same failure model as in §8.2, where a container or up to three switches can fail simultaneously. We evaluate DUET's failure resilience by measuring the maximum link utilization under these two scenarios: the failure of a randomly selected container or the failure of three randomly selected switches.

A random switch failure affects link traffic load in two ways:
1. It shifts the traffic of the VIPs assigned to the failed switch to the backstop SMuxes.
2. It reroutes other through traffic to alternative paths.

A container failure, on the other hand, has more complex implications:
- It disconnects all the switches within the container.
- It eliminates all traffic with sources and destinations (DIPs) inside the container.

Figure 19 illustrates the measured maximum link utilization during the two failure scenarios across ten experiments. As expected, link failures can cause transient congestion. However, the increase in link utilization is no more than 16%, which is well within the 20% bandwidth reservation made in the VIP assignment algorithm. Interestingly, a single container failure (with 44 switches inside) often results in less congestion than a three-switch failure. This can be attributed to two factors:
1. The disappearance of any traffic with sources and sinks (DIPs) inside the container.
2. The remaining traffic, which has sources or sinks outside the container, does not need to be rerouted as their paths do not go through any switch inside the container.

### VIP Migration

In this section, we evaluate the effectiveness of DUET’s VIP migration algorithm, Sticky (§4.2). We set the threshold to δ = 0.05, meaning a VIP will migrate to a new assignment only if doing so reduces the MRU by at least 5%.

We compare Sticky with Non-sticky, which recalculates the new assignment from scratch based on the current traffic matrix (§4.1) but migrates all VIPs at once through SMuxes to avoid memory deadlock. We assess these two schemes by re-running a three-hour traffic trace, where we reassign and migrate VIPs every 10 minutes. The total VIP traffic varies between 6.2 to 7.1 Tbps in this trace.

#### Effectiveness

We first compare the portion of total traffic handled by HMuxes under the two assignment schemes. A larger portion indicates a more effective assignment algorithm. We also compare Sticky and Non-sticky against the One-time algorithm, which assigns VIPs at time 0 sec and never changes them. Figure 20(a) shows the results over the duration of the trace.

As expected, while the portion of traffic handled by HMuxes started out the same, the initial assignment used by One-time gradually loses its effectiveness, resulting in only 60-89% (average 75.2%) of the total traffic being handled by HMuxes. In contrast, Sticky and Non-sticky handle 86-99.9% (average 95.3%) of the traffic in HMuxes, continuously adapting to traffic dynamics. Even though Sticky only migrates VIPs that reduce the MRU by at least 5%, it is as effective as Non-sticky in maximizing the traffic assigned to HMuxes. Specifically, it handles 86-99.7% (average 95.1%) of the traffic in HMuxes, nearly identical to the 87-99.9% (average 95.67%) handled by Non-sticky.

#### Traffic Shuffled

Next, we compare the fraction of total VIP traffic migrated under Sticky and Non-sticky. Less traffic migration means fewer SMuxes need to be reserved as stepping stones. Figure 20(b) shows that Non-sticky results in reshuffling 25-46% (average 37.4%) of the total VIP traffic each time throughout the trace, compared to only 0.7-4.4% (average 3.5%) under Sticky. The significant reduction in traffic shuffled under Sticky is due to its simple filtering scheme, where a VIP is only migrated if it improves the MRU by at least 5%.

#### Number of SMuxes

Figure 20(c) shows the number of SMuxes needed by Sticky and Non-sticky. Additionally, we calculate the SMuxes needed without migration (marked as No-migration) and the number of SMuxes needed in Ananta, considering the SMux capacity to be 3.6 Gbps. The number of SMuxes needed for Sticky and Non-sticky is calculated as the maximum of SMuxes needed for VIP traffic, failure, and transition traffic. It is evident that Non-sticky always requires more SMuxes compared to No-migration and Sticky, indicating that Sticky does not increase the number of SMuxes needed to handle traffic during migration.

### Discussion

#### Empty Entries in Switch Tables

DUET uses empty entries in the host table, ECMP table, and tunneling table in switches to implement HMux. Several reasons contribute to the abundance of such free resources in our production datacenter:
- The host table of ToR switches has only a few dozen entries for the hosts within each rack, and the rest of the switches' tables are mostly empty.
- The ECMP table is mostly empty due to the hierarchical DC network topology, where each switch has a small number of outgoing links, and all outgoing traffic is split via ECMP.
- The tunneling table is mostly free since few online services use encapsulation other than load balancing itself.

While other data centers may have different setups, we believe our design will be applicable in common cases.

#### VIP Assignment

The greedy VIP assignment algorithm described in §4 works well in our scenarios, but there is room for improvement. The VIP assignment problem resembles the bin-packing problem, which has many sophisticated solutions. We plan to study these in the future. Additionally, while we consider VIPs in order of traffic, other orderings are possible (e.g., prioritizing VIPs with latency-sensitive traffic).

#### Failover and Migration

DUET relies on SMuxes to simplify failover and migration. As hinted in §3.3, it may be possible to handle failover and migration by replicating VIP entries in multiple HMuxes. We continue to investigate this approach, although our initial exploration shows that the resulting design is far more complex than our current design.

### Related Work

To the best of our knowledge, DUET is a novel approach to building a performant, low-cost, and organically scalable load balancer. We are not aware of any load balancing architecture that fuses switch-based and software load balancers. However, there has been much work on load balancers, and we briefly review it here.

#### Load Balancer

Traditional hardware load balancers [4, 1] are expensive and typically provide only 1+1 availability. DUET is much more cost-effective and provides enhanced availability by using SMuxes as a backstop. Compared to traditional load balancers, DUET gives us control over a critical vantage point in our cloud infrastructure.

We have extensively discussed the Ananta [17] software load balancer. Other software-based load balancers [5, 6, 7] are available but lack the scalability and availability of Ananta, as shown in [17]. Embrane [3] promises scalability but suffers from the same fundamental limitations as other software load balancers.

#### OpenFlow-Based Load Balancer

Two recent proposals focus on using OpenFlow switches for load balancing. In [20], authors present a preliminary design for a load balancing architecture using OpenFlow switches, focusing on minimizing the number of wildcard rules. However, the paper ignores key issues such as handling switch failures. Plug-n-Serve [15] is another preliminary design that uses OpenFlow switches to load balance web servers deployed in unstructured, enterprise networks. DUET is very different from these approaches, using a combined hardware and software approach without relying on OpenFlow support. DUET is designed for data center networks and pays careful attention to handling various practical issues, including different types of failures and VIP migration to adapt to network dynamics.

#### Partitioning OpenFlow Rules

Researchers have proposed using OpenFlow switches for various purposes. For example, DIFANE [22] uses some switches in the data center to cache rules and act as authoritative switches. While a load balancing architecture can be built on top of DIFANE, the focus of the paper is different from DUET. In vCRIB [16], authors propose offloading some traffic management rules from host agents to ToR switches and other host agents, aiming to ensure resource-aware and traffic-aware placement of rules. Although vCRIB faces problems such as managing network dynamics (e.g., VM migration), its main focus is quite different from DUET.

#### SDN Architecture and Middleboxes

Similar to DUET, researchers have leveraged SDN architecture in the context of middleboxes to achieve policy enforcement and verification [18, 12], which is a different goal than DUET.

#### Improving Single Server Performance

Researchers have significantly improved packet processing capabilities on commodity servers [23, 11], which could potentially improve SMux performance. However, these improvements are unlikely to bridge the differences in packet processing capabilities between HMux and SMux for the load balancer workload.

Lastly, several algorithms for calculating flow hashes (e.g., resilient hashing [2], cuckoo-hashing [23]) offer a wide variety of trade-offs. We do not review them here, although DUET can leverage any advances in this field.

### Conclusion

DUET is a new distributed hybrid load balancer designed to provide high capacity, low latency, high availability, and high flexibility at low cost. The DUET design was motivated by two key observations:
1. Software load balancers offer high availability and high flexibility but suffer from high latency and low capacity per load balancer.
2. Commodity switches have ample spare resources and now support programmability needed to implement load balancing functionality.

The DUET architecture seamlessly integrates the switch-based load balancer design with a small deployment of software load balancers. We evaluate DUET using a prototype implementation and extensive simulations using traces from our production DC. Our evaluation shows that DUET provides 10x more capacity than a software load balancer, at a fraction of its cost, while reducing latency by over 10x and can quickly adapt to network dynamics, including failures.

### Acknowledgements

We thank the members from Microsoft Azure team, especially Chao Zhang, for their help in shaping DUET. We also thank the reviewers and our shepherd Ali Ghodsi for their helpful feedback.

### References

[1] A10 Networks AX Series. http://www.a10networks.com.
[2] Broadcom Smart Hashing. http://www.broadcom.com/collateral/wp/StrataXGS_SmartSwitch-WP200-R.pdf.
[3] Embrane. http://www.embrane.com.
[4] F5 Load Balancer. http://www.f5.com.
[5] HAProxy Load Balancer. http://haproxy.1wt.eu.
[6] LoadBalancer.org Virtual Appliance. http://www.load-balancer.org.
[7] NetScaler VPX Virtual Appliance. http://www.citrix.com.
[8] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data Center TCP (DCTCP). In SIGCOMM, 2010.
[9] P. Bodík, I. Menache, M. Chowdhury, P. Mani, D. A. Maltz, and I. Stoica. Surviving failures in bandwidth-constrained datacenters. In SIGCOMM, 2012.
[10] C. Chekuri and S. Khanna. On multi-dimensional packing problems. In SODA, 1999.
[11] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall, G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy. RouteBricks: Exploiting parallelism to scale software routers. In SOSP, 2009.
[12] S. Fayazbakhsh, V. Sekar, M. Yu, and J. Mogul. FlowTags: Enforcing network-wide policies in the presence of dynamic middlebox actions. Proc. HotSDN, 2013.
[13] P. Gill, N. Jain, and N. Nagappan. Understanding network failures in data centers: measurement, analysis, and implications. In ACM SIGCOMM CCR, 2011.
[14] J. Hamilton. The Cost of Latency. http://perspectives.mvdirona.com/2009/10/31/TheCostOfLatency.aspx.
[15] N. Handigol, S. Seetharaman, M. Flajslik, N. McKeown, and R. Johari. Plug-n-Serve: Load-balancing web traffic using OpenFlow. ACM SIGCOMM Demo, 2009.
[16] M. Moshref, M. Yu, A. Sharma, and R. Govindan. Scalable rule management for data centers. In NSDI, 2013.
[17] P. Patel et al. Ananta: Cloud-scale load balancing. In SIGCOMM, 2013.
[18] Z. A. Qazi, C.-C. Tu, L. Chiang, R. Miao, V. Sekar, and M. Yu. Simple-fying middlebox policy enforcement using SDN. In SIGCOMM, 2013.
[19] L. Ravindranath, J. Padhye, R. Mahajan, and H. Balakrishnan. Timecard: Controlling User-Perceived Delays in Server-based Mobile Applications. In SOSP, 2013.
[20] R. Wang, D. Butnariu, and J. Rexford. OpenFlow-based server load balancing gone wild. In Usenix HotICE, 2011.
[21] X. Wu, D. Turner, C.-C. Chen, D. A. Maltz, X. Yang, L. Yuan, and M. Zhang. NetPilot: Automating datacenter network failure mitigation. ACM SIGCOMM CCR, 2012.
[22] M. Yu, J. Rexford, M. J. Freedman, and J. Wang. Scalable flow-based networking with DIFANE. In SIGCOMM, 2010.
[23] D. Zhou, B. Fan, H. Lim, M. Kaminsky, and D. G. Andersen. Scalable, high-performance Ethernet forwarding with CuckooSwitch. In CoNext, 2013.