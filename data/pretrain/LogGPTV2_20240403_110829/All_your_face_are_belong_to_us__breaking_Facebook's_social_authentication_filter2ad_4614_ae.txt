6. REMEDIATION AND LIMITATIONS
Facebook has already devised some mechanisms that aim at hin-
dering casual attackers and the practices presented in this paper. We
explain why these mechanisms are not very effective or have some
drawbacks that make them impractical. We continue with some
proposed modiﬁcations to SA to make it safer based on the insights
we obtained through our experiments.
6.1 Compromise Prevention and Notiﬁcation
Facebook has recently deployed some security features that can
help further defend against stolen credentials being used for com-
promising accounts. However, these mechanisms are opt-in and
disabled by default. Therefore, users may not have them enabled,
and will remain susceptible to the threat that we study in this paper.
First, users can add certain devices to a list of recognized, trusted
devices. Whenever a user logs in from an unrecognized device,
a security token is sent to the owner’s mobile phone. This token
must be entered in the log-in form for the user to be successfully
logged in. This security setting, called login approval, follows the
traditional second-token authentication scheme and only works in
combination with the recognized, trusted devices feature. This ap-
proach can completely deter our attack, because it implements a
truly-strong, two-factor authentication: The adversary would need
physical access to the user’s mobile phone to obtain the security
token and successfully login.
Second, a user who fails to complete an SA challenge is redi-
rected to an alert page, upon the next successful login, which re-
ports the attempted login, and shows the time and place informa-
tion. Unfortunately, if the adversary manages to solve the SA test in
a subsequent attempt, he will be redirected to the notiﬁcation page
and the account owner will never see the alert. In addition to the
default notiﬁcation, users may enable an optional login-notiﬁcation
feature: Whenever their account is accessed, an alert message is
sent via text or email message. This notiﬁcation feature does not
prevent an adversary from logging in and, therefore, does not pre-
vent our attack, which takes less than one minute. Furthermore, if
the adversary has compromised the email account—which is not an
unrealistic assumption, as users may reuse their credentials across
406
services—he can delete the notiﬁcation email.
If that is not the
case, the adversary will still have access until the owner changes
the password and terminates any illegal active sessions.
Moreover, these mechanisms present three additional drawbacks.
First, users must link their mobile phone number to their Facebook
account, which many may not wish to do. Second, and more im-
portantly, users typically access their account from many devices
some of which may be public (e.g., computers in libraries or their
workplace).
In this case, adding all these devices to the list of
trusted devices is both impractical and insecure, and users will not
wish to receive alerts every time they log in from one of those ma-
chines. Finally, involving the cellular network may result in mone-
tary charges, a factor which could seriously discourage users from
opting in to the mechanism.
6.2 Slowing Down the Attacker
When the attacker is prompted with an SA challenge, he must
solve a CAPTCHA before the actual SA test. Although this topic
falls outside the scope of this paper, it is worth noticing that solving
a CAPTCHA is trivial and only takes a human a few seconds. In ad-
dition, as previous work [4, 5, 7] has shown, breaking CAPTCHAs
automatically is feasible and, in many cases, easy. Furthermore,
it is well known that adversaries can perform laundry attacks [2,
13] and crowd-source the solution of CAPTCHAs. In conclusion,
CAPTCHAs may create a technical obstacle to automated attacks,
but they should not be considered a deﬁnitive countermeasure.
The presence of suggested names in SA tests is the major dis-
advantage of the current implementation as it greatly limits the
search space for adversaries. By removing suggestions, there is
a high probability of face-recognition software returning multiple
users with similar conﬁdence scores. Also, the time needed for
face recognition might increase for certain systems although, as we
have shown, cloud-based face recognition systems are unlikely to
be seriously affected. On the downside, it will be harder for users
to identify their friends and the system will be less usable as one
would have to manually type the exact names of his friends. Auto-
matic “type ahead” features may lessen the burden, although they
are still vulnerable to exhaustive enumeration.
6.3 SA revisited
Designing effective and usable CAPTCHAs [6] is as hard as de-
signing effective and usable authentication schemes that exploit so-
cial knowledge [17]. The downside of CAPTCHAs is that they are
either too easy for machines or too difﬁcult for humans. This paper
and previous work show that the main weakness of social-based au-
thentication schemes is that the knowledge needed to solve them is
too public: Ironically, the purpose of social networks and the nature
of human beings is to share knowledge. However, we believe that
SA tests could be more secure yet still solvable by humans.
Facebook can build SA tests from photos showing human faces
that fail or achieve very low conﬁdence scores in Facebook’s own
face recognition system. Photos may contain faces of people wear-
ing glasses, with masks on or slightly turned away from the camera.
Humans are able to recognize their friends in the general image of
a person, the environment, or the event. On the other hand, face-
recognition algorithms have a hard time matching these human and
social characteristics across different photos. In terms of feasibility,
Facebook can piggy-back on users uploading photos as part of their
daily routine and prompt them to tag a person for which no face has
been detected, thus creating the necessary labeled dataset for gen-
erating SA tests. Even if an adversary is able to capture that photo
and the tag provided by the user, chances are his face recognition
algorithm will fail to ﬁnd a resemblance with other photos of the
same person. Also, if the adversary carries out an informed training
process this might introduce unwanted noise which will reduce the
overall accuracy of the classiﬁer.
7. RELATED WORK
Previous work showed that information available in users’ pro-
ﬁles in social networks can be used to break authentication mecha-
nisms, or deduce information that threatens their privacy. A study
performed by Rabkin [21] attempted to assess the security proper-
ties of personal knowledge questions that are used for fallback au-
thentication. In §2.5 we discuss a similar study, although focused
on Facebook SA. Rabkin argues that since such mechanisms owe
their strength to the hardness of an information-retrieval problem,
in the era of online social networks and the vast availability of per-
sonal information, their security is diminishing. In this study 12%
of the sampled security questions from online banking sites is auto-
matically attackable (i.e., the answers are on a user’s proﬁle).
Polakis et al. [20] demonstrate and evaluate how names extracted
from OSNs can be used to harvest e-mail addresses as a ﬁrst step for
personalized phishing campaigns. Using information from Face-
book, Twitter and Google Buzz, over 40% of the users’ e-mail ad-
dresses could be directly mapped to their social proﬁles. A similar
study was conducted by Balduzzi et al. [3]. They focus on the abil-
ity to use search utilities in social networking sites as an oracle; an
attacker can search for an e-mail address, and if a user proﬁle has
been registered with that address, the proﬁle is returned. Thus, the
attacker can map e-mail addresses to social proﬁles.
The work most related to this paper is a recent study by Kim et
al. [17], already discussed in §2.4. They formally quantify the ad-
vantage an attacker has against SA tests when he is already inside
the victim’s social circle. The researchers thus demonstrate that SA
is ineffective against one’s close friends and family or highly con-
nected social sub-networks such as universities. However, in this
paper we extend the threat model to incorporate any attacker lo-
cated outside the victim’s social circle. Furthermore, we implement
a proof-of-concept infrastructure, and use publicly available infor-
mation to quantify the effectiveness of such attacks. Thus, we are
able to show the true extent to which SA is susceptible to automated
attacks. Previous work [4, 5, 19, 24] has proved the feasibility of po-
sitioning one’s self among a target’s social circle using a mix of
active and passive [14] techniques ranging from social engineering
(e.g., attractive fake proﬁles) to forgetful users accepting friendship
requests from fake proﬁles of people they are already linked. As
such, the proposed countermeasures by Kim et al. [17] for a more
secure social authentication mechanism remain equally vulnerable
to our attack. Finally, we also present a theoretical estimation of
the attack surface based on empirical data from our experiments as
well as those reported by previous studies.
Boshmaf et al. [5] explore the feasibility of socialbots inﬁltrating
social networks, and operate a Socialbot Network in Facebook for
8 weeks. A core aspect of their operation is the creation of new
accounts that follow a stealthy behavior and try to imitate human
users. These actions are complimentary to our attack, as a deter-
mined attacker can use them to inﬁltrate the social circles of his
victims and expand the attack surface by gaining access to private
photos. This will result in much higher percentages of solved SA
tests. Gao et al. [12] found that 97% of malicious accounts were
compromised accounts of legitimate users. This reﬂects the impor-
tance of social authentication as a mechanism for preventing attack-
ers from taking over user accounts using stolen credentials. Accord-
ingly, in this paper we explore the feasibility of an automated attack
that breaks SA tests though the use of face recognition techniques.
Our results validate the effectiveness of our attack even when the
attacker uses only publicly available information.
407
8. CONCLUSIONS
In this paper we pointed out the security weaknesses of using
social authentication as part of a two-factor authentication scheme,
focusing on Facebook’s deployment. We found that if an attacker
manages to acquire the ﬁrst factor (password), he can access, on av-
erage, 42% of the data used to generate the second factor, thus, gain-
ing the ability to identify randomly selected photos of the victim’s
friends. Given that information, we managed to solve 22% of the
real Facebook SA tests presented to us during our experiments and
gain a signiﬁcant advantage to an additional 56% of the tests with
answers for more than half of pages of each test. We have designed
an automated social authentication breaking system, to demonstrate
the feasibility of carrying out large-scale attacks against social au-
thentication with minimal effort on behalf of an attacker. Our ex-
perimental evaluation has shown that widely available face recogni-
tion software and services can be effectively utilized to break social
authentication tests with high accuracy. Overall we argue that Face-
book should reconsider its threat model and re-evaluate the security
measures taken against it.
Acknowledgements
We thank the anonymous reviewers for their valuable comments
and Alessandro Frossi for his support. This paper was supported
in part by the FP7 project SysSec funded by the EU Commission
under grant agreement no 257007, the Marie Curie Reintegration
Grant project PASS, the ForToo Project of the Directorate General
Home Affairs, and ONR MURI N00014-07-1-0907. Any opinions,
ﬁndings, and conclusions or recommendations expressed in this ma-
terial are those of the author(s) and do not necessarily reﬂect the
views of ONR or the US Government.
9. REFERENCES
[1] A. Acquisti, R. Gross, and F. Stutzman. Faces of Facebook:
How the largest real ID database in the world came to be.
BlackHat USA, 2011, http://www.heinz.cmu.edu/
~acquisti/face-recognition-study-FAQ/
acquisti-faces-BLACKHAT-draft.pdf.
[2] E. Athanasopoulos and S. Antonatos. Enhanced CAPTCHAs:
Using animation to tell humans and computers apart. In
Proceedings of the 10th IFIP Open Conference on
Communications and Multimedia Security. Springer, 2006.
[3] M. Balduzzi, C. Platzer, T. Holz, E. Kirda, D. Balzarotti, and
C. Kruegel. Abusing social networks for automated user
proﬁling. In Proceedings of the 13th International
Conference on Recent Advances in Intrusion Detection.
Springer, 2010.
[4] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda. All your
contacts are belong to us: automated identity theft attacks on
social networks. In Proceedings of the 18th International
Conference on World Wide Web. ACM, 2009.
[8] M. Dantone, L. Bossard, T. Quack, and L. V. Gool.
Augmented faces. In Proceedings of the 13th IEEE
International Workshop on Mobile Vision. IEEE, 2011.
[9] R. Dey, Z. Jelveh, and K. Ross. Facebook users have become
much more private: A large-scale study. In Proceedings of
the 4th IEEE International Workshop on Security and Social
Networking. IEEE, 2012.
[10] R. Dhamija, J. D. Tygar, and M. Hearst. Why phishing works.
In Proceedings of the SIGCHI conference on Human Factors
in computing systems. ACM, 2006.
[11] R. Dunbar. Grooming, Gossip, and the Evolution of
Language. Harvard University Press, 1998.
[12] H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Y. Zhao.
Detecting and characterizing social spam campaigns. In
Proceedings of the 10th Annual Conference on Internet
Measurement. ACM, 2010.
[13] C. Herley. The plight of the targeted attacker in a world of
scale. In Proceedings of the Ninth Workshop on the
Economics of Information Security, 2010.
[14] D. Irani, M. Balduzzi, D. Balzarotti, E. Kirda, and C. Pu.
Reverse social engineering attacks in online social networks.
In Proceedings of the 8th International Conference on
Detection of Intrusions and Malware, and Vulnerability
Assessment. Springer, 2011.
[15] D. Jacoby. Facebook Security Phishing Attack In The Wild.
Retrieved on January 2012 from
http://www.securelist.com/en/blog/208193325/
Facebook_Security_Phishing_Attack_In_The_Wild.
[16] M. Jakobsson and J. Ratkiewicz. Designing ethical phishing
experiments: A study of (ROT13) rOnl query features. In
Proceedings of the 15th International Conference on World
Wide Web. ACM, 2006.
[17] H. Kim, J. Tang, and R. Anderson. Social authentication:
harder than it looks. In Proceedings of the 2012
Cryptography and Data Security conference. Springer, 2012.
[18] M. Madejski, M. Johnson, and S. M. Bellovin. A study of
privacy settings errors in an online social network. In
Proceedings of the 4th IEEE International Workshop on
Security and Social Networking. IEEE, 2012.
[19] F. Nagle and L. Singh. Can friends be trusted? Exploring
privacy in online social networks. In Proceedings of the 2009
International Conference on Advances in Social Network
Analysis and Mining. IEEE, 2009.
[20] I. Polakis, G. Kontaxis, S. Antonatos, E. Gessiou, T. Petsas,
and E. P. Markatos. Using social networks to harvest email
addresses. In Proceedings of the 9th Annual ACM Workshop
on Privacy in the Electronic Society. ACM, 2010.
[21] A. Rabkin. Personal knowledge questions for fallback
authentication: Security questions in the era of Facebook. In
Proceedings of the 4th Symposium on Usable Privacy and
Security. ACM, 2008.
[5] Y. Boshmaf, I. Muslukhov, K. Beznosov, and M. Ripeanu.
[22] A. Shulman. The underground credentials market. Computer
The socialbot network: when bots socialize for fame and
money. In Proceedings of the Annual Computer Security
Applications Conference. ACM, 2011.
[6] E. Bursztein, S. Bethard, C. Fabry, J. C. Mitchell, and
D. Jurafsky. How good are humans at solving CAPTCHAs?
A large scale evaluation. In Proceedings of the 2010 IEEE
Symposium on Security and Privacy. IEEE, 2010.
[7] E. Bursztein, M. Martin, and J. C. Mitchell. Text-based
CAPTCHA strengths and weaknesses. In Proceedings of the
18th ACM Conference on Computer and Communications
Security. ACM, 2011.
Fraud & Security, (3), 2010.
[23] J. Staddon and A. Swerdlow. Public vs. publicized: content
use trends and privacy expectations. In Proceedings of the
6th USENIX Conference on Hot Topics in Security. USENIX,
2011.
[24] B. E. Ur and V. Ganapathy. Evaluating attack ampliﬁcation
in online social networks. In Proceedings of the 2009 Web
2.0 Security and Privacy Workshop.
408