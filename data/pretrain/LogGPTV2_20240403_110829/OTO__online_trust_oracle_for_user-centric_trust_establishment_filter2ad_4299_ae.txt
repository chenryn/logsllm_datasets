.05
µ
.91
.64
σ ¯X
.05
.05
µ
.79
.60
σ ¯X
.05
.05
µ
.79
.57
σ ¯X
.05
.05
Usefulness
min:1, max:5
Annoyance
min:1, max:5
µ
4.24
4.10
σ
.58
.72
µ
3.75
3.17
σ
1.02
1.10
F (1, 51) = 4.03
F (1, 425) = .18
F (1, 425) = 12.49
F (1, 294) = 8.86
F (1, 294) = 11.09
F (1, 57) = .64
F (1, 57) = 4.40
p < .0001
p = .67
p = .0005
p = .003
p = .001
p = .43
p = .04
OTO
SSF
Results
µ: mean, σ ¯X : standard error, σ: standard deviation
There was a statistically signiﬁcant diﬀerence between
OTO and SSF conditions for TN scenarios, as shown in Ta-
ble 9. Hence, Hypothesis 2 is valid and users do indeed make
signiﬁcantly better trust decisions with OTO compared to
SSF.
False Positive (FP).
In the FP case the OS identiﬁes
legitimate software as malicious. We tested three FP sce-
narios (“Kaspersky”, “Rkill”, and “MindMaple”) given the
following hypothesis:
Hypothesis 3. Compared to SSF, OTO enables users to
download legitimate software that the OS mistakenly detects
as malicious.
There was a statistically signiﬁcant diﬀerence between
OTO and SSF for FP scenarios, as shown in Table 9. Hence,
for cases where browsers miscategorize legitimate software as
malicious, OTO’s trustworthy and suspicious evidence sig-
niﬁcantly helps users make correct and informed trust deci-
sions.
False Negative (FN).
In the FN case, the OS fails to de-
tect malware. For three FN scenarios (“HDD Diagnostics”,
“ActiveX codec”, and “Adobe ﬂash update”), our hypothesis
is the following:
Hypothesis 4. Compared to SSF, OTO prevents users
from downloading malware that the OS does not detect.
We found a statistically signiﬁcant diﬀerence between OTO
and SSF for FN scenarios (see Table 9). Hence, for cases in
which browsers fail to detect malware, the trustworthy and
suspicious evidence provided in the OTO interface signiﬁ-
cantly helps users make correct trust decisions.
Discussion. The marginal interaction eﬀect (F (3, 517) =
2.04, p = .10) suggests that the TN case was especially
helped by OTO. It also shows that the FN and FP cases
were helped by OTO, but not the TP case. Hence, OTO’s
approach of providing positive and negative trust evidence
helps people make correct trust decisions, even if the inter-
face misclassiﬁes software.
6.5 Usability Analysis
Usefulness. During the post-test of both SSF and OTO
studies, we posed a 5-point Likert scale question to measure
how useful participants found the corresponding interface,
with the following hypothesis:
Hypothesis 5. Users ﬁnd the OTO interface at least as
useful as the SSF interface.
There was no statistically signiﬁcant diﬀerence between
two interfaces for usefulness (see Table 9), hence satisfying
our hypothesis.
Annoyance. We asked participants how much they were
annoyed by given interface. Our hypothesis was as follows:
Hypothesis 6. Since the OTO interface can potentially
contain more information than the SSF interface, users may
ﬁnd the OTO interface more annoying than the SSF inter-
face.
Figure 10: Mean and standard deviation of 20 forms of trust
evidence using 5-point Likert scales. Participants rated how they
consider each evidence as helpful in validating the software legiti-
macy on a 5-point Likert scale.
We were able to ﬁnd a statistically signiﬁcant diﬀerence
between two interfaces, nullifying Hypothesis 6.
In other
words, the participants found OTO more comfortable to use.
6.6 Desired List of Evidence
At the end of each study, we gave participants a list of
pieces of evidence for assessing software legitimacy (same
as what is presented in Table 4), and asked them to rank
the usefulness of each piece of evidence on a 5-point Likert
scale. Figure 10 shows the mean and the standard deviation
of each piece of evidence.
In general, participants reported the robust and scalable
evidence as helpful for trust assessments, except information
about the developer and the category of software. In partic-
ular, participants found the evidence suggested by security
experts helpful.
7. RELATED WORK
In this section we discuss related research covering various
aspects of security warning design and online user behavior:
user mental models of security risks, mitigating the eﬀect of
habituation, and user assessments of content credibility.
7.1 User Mental Models
Prior research has demonstrated the importance of a user’s
mental model of security risks. Sunshine et al. have analyzed
how risk perception plays a large role in how people respond
to SSL warning messages, which are used to notify users
of a potential Man-in-the-Middle attack [17]. Their results
show that, in general, all the warning signs did not prevent
many of the users from exhibiting unsafe behavior, even for
security experts who perceived the warnings as a lower risk
than, for example, a mismatched certiﬁcate. This may be
an even bigger issue for those not familiar with computing
security and privacy because their perception of risk may
401be even lower. For example, these users may be unaware
of the incentives for cyber-criminals or the scalable nature
of online attacks and therefore do not perceive a signiﬁcant
personal risk.
Similarly, Bravo-Lillo et al. have addressed psychological
responses to warning messages in terms of how users per-
ceive and react to computer alerts [6]. Using various warn-
ings from popular operating systems, their study revealed
that many users have the “wrong” mental model for many
computer warnings. For example, understanding SSL warn-
ings is diﬃcult if one does not know about certiﬁcates or
Man-in-the-Middle attacks. The authors argue that warn-
ings should be a third line of defense, after designing out the
risk and guarding against the risk.
Wash illustrates that home users tend to follow some (but
not all) pieces of security advice from experts based on the
identiﬁcation of folk models of security threats [19]. The
author ﬁnds that some users with certain folk models (e.g.,
viruses are generally bad, viruses are buggy software) believe
that the avoidance of intentional downloading and execution
is enough protect them from virus infection. Based on the
analysis of eight folk models on viruses, hackers, and bot-
nets, the author suggests that security technologies should
not only focus on actionable advice, but also clearly explain
potential threats that users may face.
Motiee has explored how to create informative contents to
Microsoft Windows 7 User Account Control warnings [14].
More speciﬁcally, the author focuses on the information con-
tent to help users assess risk and correctly respond to warn-
ings. Based on the user study, the author states that the
most understandable and useful pieces of content for users
are as follows: program name, origin, description, certiﬁ-
cation, changes to apply and result of antivirus scan. The
author also suggests selecting a context-based subset of the
content to avoid habituation. However, the majority of the
contents is not robust against spooﬁng attacks, thus ques-
tioning the eﬀectiveness of the warnings.
We address the requirement of a correct mental model
for security risk assessments by incorporating robust tradi-
tional evidence (e.g., ﬁle origin information) with evidence
that may be more familiar to novice computer users (e.g.,
OSN data and reviews from authorities) and presenting the
information in an intuitive way such that users can make
informed trust decisions without heavily relying on the se-
curity assessment by the underlying operating system.
7.2 Habituation
The research community has studied the eﬀects of ha-
bituation of users to warning dialogs. Egelman et al. have
analyzed the eﬀectiveness of browser warnings [11]. Specif-
ically, they compare the eﬀectiveness of passive and active
warnings in getting users to avoid spear phishing attacks.
They found that the majority of users (97%) were deceived
by least one of the phishing messages. Many users simply
did not notice warning signs. However, 79% of the partic-
ipants presented with active warnings heeded them. This
result conﬁrms that habituation is a major road block for
creating eﬀective warning messages.
To mitigate the tendency of users who ignore security di-
alogs, Brustoloni et al. have explored the use of polymorphic
and audited dialogs to defend against risky email attach-
ments [7]. Since a polymorphic dialog continually changes,
users are forced to examine the dialog even if their goal is
to bypass it as quickly as possible. Auditing involves warn-
ing users that their responses will be forwarded to an au-
ditor and their account may be quarantined based on those
responses. Their results show that untrained users accept
signiﬁcantly less unjustiﬁed risk using polymorphic and au-
dited dialogs as compared to conventional dialogs, but at
the cost of usability: Expert users who already know what
decisions they want to make may ﬁnd a constantly changing
interface annoying. Furthermore, it may be challenging to
enforce an auditing policy in cases where there is no central
authority (e.g., home or public network).
OTO mitigates habituation by varying the color of warn-
ing dialog boxes based on the severity of the risks. Moreover,
every software download generates a unique set of evidence,
encouraging users to read the warnings.
7.3 Assessing Credibility Online
Systems in the past have looked at other domains outside
of software installation. For example, Schwarz and Morris
have explored augmenting web pages and search results with
visualizations presenting credibility features [16]. However,
the credibility features, which were selected by the authors,
are highly subjective and can often times provide security
ﬂaws. For example, the “popularity by experts” feature,
which the study subjects found to be most useful, is ques-
tionable in terms of how the system selects the experts.
Fogg et al. have evaluated the credibility of two web-
sites covering similar topics by applying the Prominence-
Interpretation theory [12]. They found that the design of
the site was the most frequently utilized aspect for evaluat-
ing credibility, followed by information structure and infor-
mation focus. They also found that content is a factor that
aﬀects prominence as people notice diﬀerent aspects when
they examine diﬀerent types of sites.
8. CONCLUSION
Users are often confronted with vexing trust decisions of
whether to download a given piece of software. Unfortu-
nately, automated techniques have proven inadequate so far,
as malware continues to thrive. One solution is to provide
users with additional trust evidence to enable them to make
better trust decisions.
IE’s SmartScreen Filter (SSF) is a
promising step in this direction. As we discover in the two
user studies discussed in this paper, experts generally agree
what additional trust evidence is required for making trust
decisions, and novice users can make better trust decisions
with such information, even improving on SSF with sta-
tistical signiﬁcance. We hope that these results encourage
further research in this important area to ultimately enable
users to make correct trust decisions with high conﬁdence.
9. ACKNOWLEDGMENTS
We gratefully thank Sara Kiesler for her insightful feed-
back and help with statistical analyses, and anonymous re-
viewers for their valuable comments.
This research was supported by CyLab at Carnegie Mellon
under grants DAAD19-02-1-0389, and W911NF-09-1-0273
from the Army Research Oﬃce, by support from NSF un-
der awards CCF-0424422, CNS-1040801, and IGERT Dge-
0903659, and by Singapore National Research Foundation
under its International Research Centre @ Singapore Fund-
ing Initiative and administered by the IDM Programme Of-
402ﬁce. The views and conclusions contained here are those
of the authors and should not be interpreted as necessar-
ily representing the oﬃcial policies or endorsements, either
expressed or implied, of ARO, CMU, NSF, or the U.S. Gov-
ernment or any of its agencies.
10. REFERENCES
[1] Microsoft Security Intelligence Report. http:
//download.microsoft.com/download/0/3/3/
0331766E-3FC4-44E5-B1CA-2BDEB58211B8/
Microsoft_Security_Intelligence_Report_
volume_11_English.pdf, 2011.
Constructing Trustworthy Trust Indicators. PhD
thesis, Carnegie Mellon University, 2009.
[11] S. Egelman, L. F. Cranor, and J. Hong. You’ve been
warned: an empirical study of the eﬀectiveness of web
browser phishing warnings. In Proceedings of the 26th
annual SIGCHI conference on Human factors in
computing systems, 2008.
[12] B. Fogg, C. Soohoo, D. R. Danielson, L. Marable,
J. Stanford, and E. R. Tauber. How Do Users Evaluate
the Credibility of Web Sites? A Study with Over
2,500 Participants. In Proceedings of the Conference
on Designing for User Experiences (DUX), 2003.
[2] Sophos Security Threat Report 2011. http:
[13] C. Kuo. Reduction of End User Errors in the Design
//www.sophos.com/sophos/docs/eng/papers/
sophos-security-threat-report-2011-wpna.
pdf, 2011.
[3] Sophos Security Threat Report 2012.
http://www.sophos.com/medialibrary/PDFs/
other/SophosSecurityThreatReport2012.pdf,
2012.
[4] This Is Watson. IBM Journals of Research and
Development, May/Jul 2012.
[5] P. Ayyavu and C. Jensen. Integrating User Feedback
with Heuristic Security and Privacy Management
Systems. In Proceedings of Proceedings of the annual
SIGCHI conference on Human factors in computing
systems, 2011.
[6] C. Bravo-Lillo, L. F. Cranor, J. S. Downs, and
S. Komanduri. Bridging the Gap in Computer
Security Warnings. IEEE Security and Privacy, 2011.
[7] J. C. Brustoloni and R. Villamarin-Salomon.
Improving Security Decisions with Polymorphic and
Audited Dialogs. In Proceedings of Symposium on
Usable Privacy and Security (SOUPS), 2007.
[8] R. Dhamija, J. Tygar, and M. Hearst. Why Phishing
Works. In Proceedings of the annual SIGCHI
conference on Human factors in computing systems,
2006.
[9] J. S. Downs, M. B. Holbrook, and L. F. Cranor.
Decision Strategies and Susceptibility to Phishing. In
Proceedings of Symposium on Usable Privacy and
Security (SOUPS), 2006.
[10] S. Egelman. Trust Me: Design Patterns for
of Scalable, Secure Communication. PhD thesis,
Carnegie Mellon University, 2008.
[14] S. Motiee. Towards Supporting Users in Assessing the
Risk in Privilege Elevation. Master’s thesis, The
University of British Columbia, 2011.
[15] P. O’Kane, S. Sezer, and K. McLaughlin. Obfuscation:
The Hidden Malware. IEEE Security & Privacy
Magazine, Sept. 2011.
[16] J. Schwarz and M. R. Morris. Augmenting web pages
and search results to help people ﬁnd trustworthy
information online. In Proceedings of the annual
SIGCHI conference on Human factors in computing
systems, 2011.
[17] J. Sunshine, S. Egelman, H. Almuhimedi, N. Atri, and
L. F. Cranor. Crying wolf: an empirical study of ssl
warning eﬀectiveness. In Proceedings of the 18th
conference on USENIX security symposium, 2009.
[18] A. Vishwanath, T. Herath, R. Chen, J. Wang, and
H. R. Rao. Why do people get phished? Testing
individual diﬀerences in phishing vulnerability within
an integrated, information processing model. Decision
Support Systems, 2011.
[19] R. Wash. Folk Models of Home Computer Security. In
Proceedings of the Symposium on Usable Privacy and
Security (SOUPS), 2010.
[20] M. S. Wogalter. Handbook of Warnings, chapter
Communication-Human Information Processing
(C-HIP) Model, pages 51–61. Lawrence Erlbaum
Associates, 2006.
403