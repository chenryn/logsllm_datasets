title:DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection
in Security Applications
author:Dongqi Han and
Zhiliang Wang and
Wenqi Chen and
Ying Zhong and
Su Wang and
Han Zhang and
Jiahai Yang and
Xingang Shi and
Xia Yin
DeepAID: Interpreting and Improving Deep Learning-based
Anomaly Detection in Security Applications
Dongqi Han1, Zhiliang Wang1, Wenqi Chen1, Ying Zhong1, Su Wang2, Han Zhang1, Jiahai Yang1,
1Institute for Network Sciences and Cyberspace, BNRist, Tsinghua University, Beijing, China
2Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China
Xingang Shi1, and Xia Yin2
{handq19, chenwq19, zhongy18, wangsu17}@mails.tsinghua.edu.cn, {wzl, yang, shixg}@cernet.edu.cn, {zhhan,yxia}@tsinghua.edu.cn
ABSTRACT
Unsupervised Deep Learning (DL) techniques have been widely
used in various security-related anomaly detection applications,
owing to the great promise of being able to detect unforeseen threats
and the superior performance provided by Deep Neural Networks
(DNN). However, the lack of interpretability creates key barriers
to the adoption of DL models in practice. Unfortunately, existing
interpretation approaches are proposed for supervised learning
models and/or non-security domains, which are unadaptable for
unsupervised DL models and fail to satisfy special requirements in
security domains.
In this paper, we propose DeepAID, a general framework aiming
to (1) interpret DL-based anomaly detection systems in security
domains, and (2) improve the practicality of these systems based on
the interpretations. We first propose a novel interpretation method
for unsupervised DNNs by formulating and solving well-designed
optimization problems with special constraints for security domains.
Then, we provide several applications based on our Interpreter as
well as a model-based extension Distiller to improve security sys-
tems by solving domain-specific problems. We apply DeepAID over
three types of security-related anomaly detection systems and ex-
tensively evaluate our Interpreter with representative prior works.
Experimental results show that DeepAID can provide high-quality
interpretations for unsupervised DL models while meeting several
special requirements in security domains. We also provide several
use cases to show that DeepAID can help security operators to un-
derstand model decisions, diagnose system mistakes, give feedback
to models, and reduce false positives.
CCS CONCEPTS
‚Ä¢ Computing methodologies ‚Üí Anomaly detection; ‚Ä¢ Secu-
rity and privacy ‚Üí Intrusion detection systems; File system
security.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea
¬© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484589
KEYWORDS
Deep Learning Interpretability; Anomaly Detection; Deep Learning-
based Security Applications
ACM Reference Format:
Dongqi Han, Zhiliang Wang, Wenqi Chen, Ying Zhong, Su Wang, Han Zhang,
Jiahai Yang, Xingang Shi, and Xia Yin. 2021. DeepAID: Interpreting and
Improving Deep Learning-based Anomaly Detection in Security Applica-
tions. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and
Communications Security (CCS ‚Äô21), November 15‚Äì19, 2021, Virtual Event,
Republic of Korea. ACM, New York, NY, USA, 21 pages. https://doi.org/10.
1145/3460120.3484589
1 INTRODUCTION
Anomaly detection has been widely used in diverse security applica-
tions [9]. Security-related anomaly detection systems are desired to
be capable of detecting unforeseen threats such as zero-day attacks.
To achieve this goal, unsupervised learning with only normal data,
also known as ‚Äúzero-positive‚Äù learning [11], becomes more promis-
ing since it does not need to fit any known threats (i.e., abnormal
data) during training, compared with supervised methods.
Recently, deep learning (DL) has completely surpassed tradi-
tional methods in various domains, owing to the strong ability to
extract high-quality patterns and fit complex functions [53]. Con-
sequently, unsupervised deep learning models are more desirable
in security domains for owing both the ability to detect unfore-
seen anomalies and high detection accuracy. So far, unsupervised
deep learning models have been applied in various security-related
anomaly detection applications, such as network intrusion detec-
tion [35], system log anomaly detection [12], advanced persistent
threat (APT) detection [6], domain generation algorithm (DGA)
detection [45] and web attack detection [51].
While demonstrated great promise and superior performance,
DL models, specifically Deep Neural Networks (DNN) are lack of
transparency and interpretability of their decisions. The black-box
reputation creates key barriers to the adoption of DL models in
practice, especially in security-related domains. Firstly, it is hard to
establish trust on the system decision from simple binary (abnormal
or normal) results without sufficient reasons and credible evidence.
Secondly, black-box DL-based systems are difficult to incorporate
with expert knowledge, troubleshoot and debug decision mistakes
or system errors. Thirdly, reducing false positives (FP) is the most
challenging issue for anomaly detection systems in practice. It is
impossible to update and adjust DL models to reduce FPs without
understanding how the models work. As a result, security operators
are confused with over-simplified model feedback, hesitated to
Session 12A: Applications and Privacy of ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3197trust the model decisions, shriveled towards model mistakes, and
overwhelmed by tons of meaningless false positives.
In recent years, several studies have attempted to develop tech-
niques for interpreting decisions of DL models by pinpointing a
small subset of features which are most influential for the final de-
cision [37]. However, they are not directly applicable to DL-based
anomaly detection in security applications for two reasons. Firstly,
existing studies such as [20, 33] mostly focus on interpreting DL
models for supervised classification rather than unsupervised anom-
aly detection. Since the mechanisms of these two types of learning
methods are fundamentally different, directly applying prior tech-
niques is improper and ineffective, as validated by our experiments
in ¬ß6.2. Secondly, most prior methods are designed for non-security
domains such as computer vision [16, 44, 62, 65] to understand
the mechanism of DNNs. Instead, security practitioners are more
concerned about how to design more reliable and practical systems
based on interpretations. Besides, studies have shown that prior
methods failed to be adopted in security domains due to their poor
performance [14, 54].
Our Work. In this paper, we develop DeepAID, a general frame-
work to interpret and improve DL-based anomaly detection in secu-
rity applications. The high-level design goals of DeepAID include
(1) developing a novel interpretation approach for unsupervised
DL models that meets several special requirements in security do-
mains (such as high-fidelity, human-readable, stable, robust, and
high-speed), as well as (2) solving several domain-specific problems
of security systems (such as decision understanding, model diag-
nosing and adjusting, and reducing FPs). To this end, we develop
two techniques in DeepAID referred to as Interpreter and Distiller.
DeepAID Designs. Interpreter provides interpretations of certain
anomalies in unsupervised DL models to help security practitioners
understand why anomalies happen. Specifically, we formulate the
interpretation of an anomaly as solving an optimization problem
of searching a normal ‚Äúreference‚Äù, and pinpoint the most effective
deference between it and the anomaly. We propose several tech-
niques in formulating and solving the optimization problem to
ensure that our interpretation can meet the special concerns of
security domains. Based on the interpretations from Interpreter, we
additionally propose a model-based extension Distiller to improve
security systems. We ‚Äúdistill‚Äù high-level heuristics of black-box DL
models and expert feedback of the interpretations into a rather
simple model (specifically, finite-state machines (FSM)). Compared
with the original black-box DL model, security practitioners can
more easily understand and modify the simple FSM-based Distiller.
Implementation and Evaluations. We separate security-related
anomaly detection systems into three types according to the struc-
ture of source data: tabular, time-series, and graph data. Then we pro-
vide prototype implementations of DeepAID Interpreter over three
representative security systems (Kitsune[35], DeepLog[12] and
GLGV[6]), and Distiller over tabular data based systems (Kitsune).
We extensively evaluate our Interpreter with representative prior
methods. Experimental results show that DeepAID can provide
high-quality interpretations for unsupervised DL models while
meeting the special requirements of security domains. We also
provide several use cases to show that DeepAID can help security
operators to understand model decisions, diagnose system mistakes,
give feedback to models, and reduce false positives.
Contributions. This study makes the following contributions:
‚Ä¢ We propose DeepAID, a general framework for interpreting and
improving DL-based anomaly detection in security applications,
which consists of two key techniques: Interpreter provides high-
fidelity, human-readable, stable, robust, and fast interpretations
for unsupervised DNNs. Distiller serves as an extension of Inter-
preter to further improve the practicality of security systems.
‚Ä¢ We provide prototype implementations of DeepAID Interpreter
over three types of DL-based anomaly detection applications in
security domains using tabular, time-series, and graph data, as
well as Distiller for tabular data based security applications1.
‚Ä¢ We conduct several experiments to demonstrate that our inter-
pretation outperforms existing methods with respect to fidelity,
stability, robustness, and efficiency.
‚Ä¢ We introduce several use cases and improvements of security
applications based on DeepAID such as understanding model
decisions, model diagnosing and feedback, and reducing FPs.
The rest of the paper is organized as follows: In ¬ß2, we provide
backgrounds on DL interpretability and unsupervised DL for anom-
aly detection in security applications, as well as the overview of
DeepAID design. We introduce the motivation of this work in ¬ß3.
¬ß4 introduces the problem formulation for interpreting unsuper-
vised DL models and its instantiations on three types of systems.
¬ß5 introduces the model-based extension Distiller. In ¬ß6, we exten-
sively evaluate the performance of interpreters and showcase how
DeepAID solves some critical problems of security systems. We
make several discussions in ¬ß7 and introduce related work in ¬ß8. ¬ß9
concludes this study.
2 BACKGROUND AND OVERVIEW
In this section, we first introduce prior techniques for DL inter-
pretability (¬ß2.1). Then, two kinds of unsupervised DL for anomaly
detection are introduced (¬ß2.2). Next, we introduce the pipeline and
three types of security-related anomaly detection systems (¬ß2.3).
Finally, the overview of DeepAID is introduced (¬ß2.4).
2.1 Local Interpretations of DNNs
Local interpretation refers to interpreting certain decisions/outputs
of DL models, which is the most prevailing form in the broad domain
of DL Interpretability. We introduce three major categories of local
interpretations and representative interpreters as follows.
Approximation-based Interpretations. These approaches use
a rather simple and interpretable model to locally approximate
original DNNs with respect to certain decisions. They commonly
make the assumption that although the mapping functions in DNNs
are extremely complex, the decision boundary of a specific sample
can be simply approximated (e.g., by linear models). Then, the
interpretation of the sample is provided by the simple model. For
example, LIME[42] uses a set of neighbors of the interpreted sample
to train an interpretable linear regression model to fit the local
boundary of the original DNN. LEMNA [20] is another approach
dedicated to Recurrent Neural Networks (RNN) in security domains.
Unlike LIME, the surrogate interpretable model used in LEMNA is a
non-linear mixture regression model. In addition, LEMNA leverages
1The implementation of DeepAID is available at: https://github.com/dongtsi/DeepAID
Session 12A: Applications and Privacy of ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3198Figure 1: The overview of our work.
fused Lasso to cope with the feature dependence problem in RNNs.
COIN [31] supports interpreting unsupervised models by training a
surrogate linear SVM from interpreted anomalies and normal data.
Perturbation-based Interpretations. These approaches make
perturbations on the input neurons and observe corresponding
changes in model prediction. Impactful input features with small
perturbations are more likely to induce major change in the out-
puts of DNNs [10, 15, 16, 65]. CADE [58] provides a more rational
method for unsupervised interpretation of concept drift samples
by perturbing the inputs and observing the distance changes to the
nearest centroid in the latent space.
Back Propagation-based Interpretations. The core idea of back
propagation-based approaches is to propagate the decision-related
information from the output layer to the input layer of DNNs. The
simple approach is to compute gradients of the output relative to
the input based on the back propagation mechanism of DNNs [46].
However, gradients-based approaches have proven to be unreliable
for corner cases and noisy gradients. Several improvements have
been studied to solve the problem [3, 44, 48, 50]. Among them,
DeepLIFT [44] outperforms others by defining a reference in the
input space and computing the back propagate changes in neu-
ron activation to compute the contribution of change between the
reference and interpreted sample.
2.2 Unsupervised DL for Anomaly Detection
Unsupervised deep learning for anomaly detection is trained with
purely normal data, thus are also called ‚Äúzero-positive‚Äù learning [11].
Existing approaches enable DNNs to learn the distribution of nor-
mal data and find anomalies that are deviated from the distribution,
which can be divided into the following two types:
Reconstruction-based Learning. This kind of learning approach-
es are achieved by generative DL models, such as Autoencoder
[35, 56, 63] and Generative Adversarial Networks (GAN) [59]. Let
ùëìùëÖ : RùëÅ ‚Üí RùëÅ represent generative DL models mapping the input
to the output space with the same dimensionality of ùëÅ . The train-
ing process of these models is to minimize the distance between
normal data ùë• and its output through DNNs denoted with ùëì (ùë•).
Then, reconstruction-based models can detect anomalies by com-
pute the reconstruction error (such as mean square error (MSE))
between given inputs and outputs. The intuition is that DNNs can
learn the distribution of normal data after training, thus are more
Figure 2: The workflow of applying
DeepAID on security systems.
likely to reconstruct normal data with small errors, while failing to
reconstruct anomalies that differ from the normal data.
Prediction-based Learning. This kind of learning approaches
are usually used for time-series or sequential data and achieved by
RNNs or Long short-term memory (LSTM) [11, 12]. Given a normal
time-series x = ùë•1, ùë•2, ..., ùë•ùë° with the length of ùë°, prediction-based