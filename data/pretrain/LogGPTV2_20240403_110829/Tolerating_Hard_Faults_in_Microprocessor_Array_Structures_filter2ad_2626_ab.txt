henceforth  focus  the  discussion  on  one  speciﬁc  array 
structure  from  each  of  the  two  categories:  the  reorder 
buffer (ROB) and the branch history table (BHT). The 
ROB and BHT are representative of the kinds of array 
structures  found  in  modern  microprocessors,  and  thus 
the arguments and results in this paper apply broadly. 
Reorder Buffer. The  ROB  is  a  circular  buffer  that  is 
used  in  dynamically  scheduled  (a.k.a.  “out-of-order”) 
processors to implement precise exceptions by ensuring 
that instructions are committed in program order. There 
is an entry in the ROB for each in-ﬂight instruction, and 
there are pointers to the head and tail entries in the ROB. 
An entry is added to the tail of the ROB once it has been 
decoded  and  is  ready  to  be  scheduled.  An  entry  is 
removed from the head of the ROB when it is ready to 
be  committed.  We  focus  on  processors  that  perform 
explicit register renaming with a map table, such as the 
Pentium4 [9] and the Alpha 21364 [8], in which an ROB 
entry contains the physical register tags for the destina-
tion register and the register that can be freed when this 
instruction commits, plus some other status bits. 
ROB sizes are on the order of 32-128 entries, which 
is large enough to have a non-negligible probability of a 
hard  fault.  The  ROB  is  a  buffer  which  cannot  be  ran-
domly addressed, and we leverage this constraint in our 
remapper  implementation.  The  ROB  has  a  high  archi-
tectural  vulnerability  factor  [15],  in  that  a  fault  in  an 
entry is likely to cause an incorrect execution. A fault in 
an  ROB  entry  is  not  guaranteed  to  cause  an  incorrect 
execution  for  its  instruction,  though,  since  the  fault 
might not change the data (i.e., logical masking) or the 
ROB entry might correspond to a squashed instruction 
(i.e., functional masking). 
Branch History Table. The  BHT  is  a  table  that  is 
accessed  during  branch  prediction.  Common  two-level 
branch predictor designs [25] use some combination of 
the branch program counter (PC) and the branch history 
register (BHR) to index into a BHT. The BHR is a k-bit
shift  register  that  contains  the  results  of  the  past  k
branches.  The  indexed  BHT  entry  contains  the  predic-
tion (i.e., taken or not taken, but not the destination). A 
typical BHT entry is a 2-bit saturating counter [21] that 
is  incremented  (decremented)  when  the  corresponding 
branch is taken (not taken). A BHT value of 00 or 01 (10 
or 11) is interpreted as a not-taken (taken) prediction. 
BHRs  and/or  BHTs  can  be  either  local  (one  per 
branch  PC),  global  (shared  across  all  branch  PCs),  or 
shared (by sets of branch PCs). In this paper, we focus 
on  the  gshare  two-level  predictor  [14],  in  which  the 
BHT is indexed by the exclusive-OR of the branch PC 
pointer
advance
logic
begin_buffer
pointer
advance
logic
end_buffer
Check row
2nd faulty row
1st faulty row
fault map
0
0
0
1
0
1
0
spare
spare
General
Purpose
spares
fault information
buffer size
buffer size
advancement
FIGURE 2.
Self-Repair for Buffers
and a global BHR. Since the BHT is a table, our remap-
per implementation for it is fairly similar to the logical 
abstraction presented earlier. The BHT has an architec-
tural vulnerability factor of zero, in that no fault in it can 
ever lead to incorrect execution. Thus, DIVA will never 
detect  faults  in  it.  However,  a  BHT  fault  can  lead  to 
incorrect branch predictions, which can degrade perfor-
mance.
4.1  Tolerating Detected Faults 
While  remapping  with  a  level  of  indirection  is 
straightforward in the abstract, implementing it in a high 
performance  microprocessor  pipeline  requires  careful 
consideration.  We  now  present  remapper  implementa-
tions for the ROB and BHT.
ROB Remapper. In buffer structures, as in the case of 
the ROB, the address of the data to be accessed is deter-
mined at the time of the access. Typically, two pointers 
are  used  to  mark  the  head  and  the  tail  location  of  the 
active  rows.  When  a  new  is  added,  the  tail  pointer  is 
advanced  and  the  corresponding  address  becomes  the 
physical address of the data. Similarly, when an entry is 
removed, the head pointer is advanced. Thus, the physi-
cal  as  well  as  logical  address  of  the  data  is  abstracted 
and  all  rows  have  the  same  functionality.  Thus,  the 
faulty row can easily be mapped out by modifying the 
pointer advancement logic when a hard fault is detected.
Figure 2 illustrates the implementation of the self-repair 
mechanism  for  buffers,  with  SRAS  hardware  high-
lighted in gray. SRAS uses a fault map bit-array to track 
faulty  rows.  If  a  row  is  determined  to  contain  a  hard 
fault, the corresponding bit in the fault map is modiﬁed. 
The fault map is used by the pointer advancement circuit 
to determine how far the pointer needs to be advanced. 
Once  the  pointer  is  updated  accordingly,  reads  and 
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:42 UTC from IEEE Xplore.  Restrictions apply. 
Check row
data in
s
s
e
r
d
d
A
e
d
o
c
e
D
0
0
0
1
0
0
0
0
0
0
0
0
1
0
Fault/spare
match map
Remap
Logic
2nd faulty row
1st faulty row
data
out
read/write 
enable
Spare replacing
1st faulty row
Spare replacing
2nd faulty row
read/write 
enable
FIGURE 3.
Self-Repair for Tables
writes  of  the  buffer  entries  proceed  unmodiﬁed.  Since 
the pre-processing for pointer advancement can be done 
off the critical path, the proposed modiﬁcation does not 
impact the read or write access time. 
In order to avoid a reduction in the effective buffer 
size  due  to  hard  faults,  spare  rows  can  be  used.  Since 
there is no need to replace the faulty row with any par-
ticular spare row, the detection of the faulty row prompts 
incrementing the total buffer size by one (by adding the 
spare) while maintaining the same effective size. SRAS 
can tolerate as many hard faults as there are spares with-
out any degradation of buffer performance. If the num-
ber of faulty rows exceed the number of spare rows, then 
the effective buffer size is allowed to shrink, resulting in 
graceful degradation of the buffer performance. Assum-
ing that adding one or two to the pointers does not dra-
matically change timing or power consumption, the only 
overhead  of  this  repair  mechanism  is  the  small  addi-
tional  area  taken  by  the  fault  map  and  the  additional 
power  consumed  for  pointer  pre-processing,  updating 
fault  map  entries,  and  updating 
the  buffer  size. 
Section 4.3  discusses  the  overall  overhead  of  the  com-
plete SRAS architecture in more detail. 
BHT Remapper. In  tables,  the  logical  address  of  the 
data  is  determined  by  the  program  execution  prior  to 
accessing the data. Since rows do not have equal func-
tionality in tables, a faulty row needs to be replaced by a 
speciﬁc spare row. In this case, we need a logical indi-
rection to map out the faulty rows. This problem is quite 
similar to the memory repair problem, and many on-line 
repair  mechanisms  have  been  proposed  [6,  13].  How-
ever,  in  microprocessor  array  structures,  logic  inserted 
into  the  critical  path  directly  impacts  performance,  so 
we  must  implement  a  timing-efﬁcient  repair  mecha-
nism. In SRAS, we distribute spare rows over sub-arrays 
of the table, and a spare can only replace a row within its 
own sub-array. This choice may make the use of spares 
inefﬁcient for highly localized faults, but it enables tim-
ing  efﬁcient  implementation  of  the  repair  logic,  as 
shown  in  Figure 3.  Once  again,  hardware  for  SRAS  is 
shown in gray. 
Similar  to  the  buffer  case,  we  keep  the  fault  map 
information  in  a  table.  However,  we  use  an  extended 
fault map which also stores the faulty-row/spare match-
ing  information.  If  a  row  is  identiﬁed  faulty  and  an 
unused  spare  is  found  to  replace  it,  the  corresponding 
entry  of  the  fault/spare  match  map  is  set  to  1.  The 
address  decode  logic,  which  is  present  in  all  tables, 
enables a row of the table to be read or written by gener-
ating  the  individual  read/write  enable  signals  for  the 
table rows. During a read or write access, these signals 
are modiﬁed by the remap logic to generate the updated 
read/write enable signals for the table entries as well as 
the read/write enable signals for the spare entries. The 
remap logic consists of (nxk) 2-input AND gates and k
n-input OR gates, where n is the size of the subarray and 
k is the number of spares assigned to that subarray. Once 
a  read/write  signal  is  initiated  by  the  address  decode 
logic,  this  signal  is  “AND”ed  with  the  corresponding 
entries of the fault/spare match map. If an entry is “1”, 
that  spare  replaces  the  row  currently  accessed.  In  this 
case, the spare replacing the faulty row will get activated 
for  the  access.  To  disable  access  to  the  faulty  row,  the 
bits in a row of the fault/spare match map are “NOR”ed 
and this signal is “AND”ed with the original read/write 
enable signal.
In  all  cases,  SRAS  will  add  two  gate  delays  (one 
OR and one AND gate delay) to the table access time. 
Since  the  additional  level  of  indirection  for  accessing 
the  physical  table  entries  is  on  the  critical  path,  this 
additional time cannot be ignored. In order to avoid set-
up or hold time violations, we very conservatively use a 
second  pipeline  stage  to  access  the  table  entries.  This 
additional  pipeline  stage  will  impose  a  penalty  in  the 
normal  mode  of  operation.  While  we  expect  that  the 
actual  performance  penalty  would  be  far  less  than  a 
pipeline  stage  (e.g.,  if  BHT  access  latency  is  not  the 
determining factor in pipeline stage latency), we choose 
this  pessimistic  design  point  as  a  lower  bound  on 
SRAS’s  beneﬁt.  In  Section 5,  we  run  experiments  to 
assess the impact of this additional pipeline stage on the 
execution time in the absence of hard faults.
4.2  Detecting and Diagnosing Faults 
Detection and diagnosis is the same for both tables 
and buffers. While we logically need only k check rows 
in a k-way superscalar processor to detect and diagnose 
faults,  the  SRAS  implementation  may  necessitate  hav-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:42 UTC from IEEE Xplore.  Restrictions apply. 
ing  even  more  check  rows.  Having  only  k  check  rows 
could lead to an unreasonably long delay to transfer the 
data along wires from one end of the array to the other. 
Wire delays are already a problem in multi-GHz micro-
processors—for example, the Intel Pentium4 has multi-
ple pipeline stages allocated strictly to wire delay—and 
we cannot ignore them in our design. A simple option is 
to divide the array into sub-arrays, each of which has k
check rows. 
4.3  SRAS Costs
The  cost  of  a  fault  tolerance  scheme  has  three 
aspects:  hardware  (area)  overhead,  performance  (tim-
ing)  overhead,  and  power  consumption  overhead.  For 
aggressive  microprocessor  architectures,  the  perfor-
mance overhead during fault-free execution is often the 
most critical parameter. 
In order to keep the performance overhead at a min-
imum,  buffers  and  tables  are  handled  differently  in 
SRAS. The distinct nature of buffers that makes all of 
their rows have equal functionality enables a no-timing-
overhead  implementation.  Tables,  however,  require  a 
deﬁnitive logical address for the data, which results in a 
need for an additional level of indirection. This indirec-
tion results in two gate delays in access times (e.g., for 
the  Pentium4,  an  inverter  delay  is  about  1-2%  of  the 
clock period [23]). As discussed in Section 4.1, we very 
conservatively add a pipeline stage for access to tables. 
The additional pipeline stage results in increased latency 
and  an  increased  number  of  stalls,  and  we  evaluate  its 
performance overhead in Section 5.
The increase in power consumption in SRAS stems 
mostly from increased data read/write activity due to the 
check rows. Since the write/read activity is doubled, the 
dynamic power consumption in the array structures will 
roughly  be  doubled  as  well.  If  power  consumption  is 
still a concern, accesses to check rows can be reduced at 
the expense of increasing the fault detection latency. 
Finally,  the  area  overhead  of  SRAS  mostly  stems 
from the spare rows (including spare check rows), since 
there is only one logic circuit for repair and check for 
the entire structure. Thus, there is an engineering trade-
off between availability and the area overhead incurred 
for spare rows. 
4.4  Limitations of this Implementation
The implementation of SRAS in this paper does not 
tolerate all microprocessor faults. We divide these untol-
erated faults into three categories. First, SRAS does not 