### Client Error and Resource Mismatch

In the event of a client error, the cause is typically a malformed request or a request for a resource that does not exist on one of the machines. If one machine responds with success while the other responds with an error, the machine that responded with success should be taken offline for recovery. This is because it should not have responded positively to a malformed request or a request for a non-existent resource.

### Meaningful Combinations of Status Codes

Another meaningful combination is 200/300 (success/redirection). This indicates that one web server (the one responding with success) sent back different content than the other, which responded with redirection. This suggests that the client already had the requested content, and the content has likely changed on the first web server, possibly due to defacement. In this case, the server that responded with success should be taken offline.

A third meaningful combination is 300/400. This indicates that one web server sent a redirect status code, while the other sent a client error. The possible causes are similar to the 200/400 case. The web server that responded with 300 should be taken offline.

Finally, there is the possibility that one web server responds while the other does not. A timeout (typically less than a second) is used to identify this scenario, resulting in the non-responding web server being taken offline.

### Handling Other Status Code Combinations

Other combinations of different status codes are generally rare and are ignored. If these combinations occur but are not caused by an intrusion, service will continue after failover, appearing uninterrupted to users. This approach helps avoid self-inflicted denial of service due to false positives from the detection mechanism.

### Additional Detection Mechanisms

In addition to status code comparison, we use "Wrapper" technology to protect the application’s environment (file system and process space) from the application itself or from malicious agents. This serves as both a detection and prevention mechanism, as attempts to violate policy are reported to the MAC, which may initiate failover even if the attempt did not succeed.

### Software Isolation

Requests are not immediately passed to the application servers. Instead, they are intercepted by a proxy on the primary server and sent to the MAC. The MAC calls the `AllowRequest` function in the Content Filter with the new request. The Content Filter maintains a list of bad requests, identified by the Forensics Agent after previous intrusions, and uses generalization rules to determine if a new request is similar to a known bad request. If the request matches or is similar to a bad request, the Content Filter returns true, allowing the request; otherwise, it returns false, blocking the request. In the latter case, the request is not sent to the backup, nor is it processed by the primary.

### Exploit Mitigation through Diversity

As described in the "Detection" section, COTS-provided diversity is a key part of HACQIT's design. This not only aids in detection through comparison but also supports our isolation strategy. Most attacks exploit specific vulnerabilities in software products, such as web servers or operating systems. An exploit that works against one product is unlikely to work against another. Therefore, using two different web servers on the primary and backup ensures that an exploit successful on one server is unlikely to propagate to the other, even if the request containing the exploit is passed to both.

### Random Rejuvenation

It is possible for an intrusion to become part of a legitimate process (e.g., creating a new thread that remains dormant indefinitely). In such cases, detection mechanisms may not identify the failure until the malicious thread activates and attempts to cause damage. Random rejuvenation is a countermeasure for this type of intrusion. The MAC randomly initiates failovers with an average interval set through the Policy Editor. This minimizes the effectiveness of "stealth" attacks that remain dormant before causing identifiable errors. Typically, this interval is set to a few hours or more.

### Host Firewall Configuration

On each server, a host firewall is installed. It allows access to the server from the OOB machine through authorized programs and ports, and from the gateway machine through port 80. Similarly, it allows access to the OOB machine from the server through authorized programs and ports. To change this configuration, a user must enter a password at the server’s keyboard. An attacker would need system privileges and the ability to remove the host firewall to gain access to the OOB machine. Creating an unauthorized process to remove a file would trigger an "Unhealthy" state by the HM.

### Recovery Mechanism

The wrappers on our web servers mediate every attempt to access the file system, allowing only policy-authorized operations. Even if a process is compromised by a buffer overflow attack, the wrapper continues to mediate file access and prevent unauthorized operations. HACQIT incorporates multiple, layered defenses to detect and recover from unauthorized file accesses due to wrapper failure or other unknown vulnerabilities. This capability, called Continual Recovery, is analogous to online repair for hardware fault tolerance.

Continual Recovery performs the following actions:
1. If an unauthorized process is detected, the HM sets the server state to "Unhealthy" and notifies the MAC, which then takes the server offline and initiates failover.
2. If the unauthorized process created a file, the HM removes the file.
3. If a file is modified or deleted by an unauthorized process, the HM retrieves a copy of the file from the OOB machine to replace the modified or deleted one. The copy is valid as of the last failover.

After failover, the wrapped processes are terminated, and an integrity check via TripWire is performed on the allowed executable files and the directory of system data files. If no integrity violations are found, the MAC is notified that the server is "Healthy." When the server is promoted to an online spare or backup, its data files are re-synchronized with the files from the previous instantiation of process pairs.

### Demonstration

We will demonstrate prevention and isolation, error detection, failfast failover, attack learning, and attack generalization. The demonstration will consist of the following steps:
1. Launch attacks from one laptop while maintaining a simulated stress load on the HACQIT web server. The critical application is a dynamic message board maintained by the web server, which is read and written from a web browser on the laptop. Before the first attack, we will demonstrate the message board application and the persistence of its dynamic data.
2. Attack with a sequence of recent, well-known exploits, such as Code Red I, Code Red II, and their variants.
3. Attack with Code Red I. This will not be prevented, as the corrective software patch will not have been applied to the OS and IIS, but the status code comparison will cause the primary to failover. A new process pair will then be active, with the previous backup server promoted to primary, and the online spare promoted to backup.
4. Use the log from the laptop to show that there was no apparent interruption of service to other users of the critical application, and use a browser to show that dynamic data on the message board has persisted across the failover.
5. Show the HACQIT status display on the OOB machine to see that after the failover caused by the initial attack with Code Red I, the MAC requested that forensics be started.
6. Launch Code Red I again. No failover will occur. The HACQIT cluster status display will show that the request was blocked because the Content Filter would not allow it.
7. Attack HACQIT with variants of Code Red I, including Code Red II and an arbitrary variant. These requests will be blocked. The Content Filter log will show they were blocked because they were generalizations of Code Red I, not because the cluster had been attacked by them before.
8. Send a legitimate request that references the Index Server resource, which contains the vulnerability Code Red I exploits, and show that legitimate requests to this resource are not arbitrarily blocked. The particular request will contain a long query string shorter than the minimum required to cause a buffer overflow. We will receive a benign status code.
9. Launch another attack, this time with a non-buffer-overflow attack. It will cause failover, but when repeated will be blocked.
10. Insert a floppy disk with an executable file on one of the servers and copy the file to the hard disk. It will be deleted. Start the executable from the floppy disk; the process will be killed, and failover will occur.

At the time of paper submission, these capabilities had already been demonstrated in our lab with multiple exploits, including Code Red I, Code Red II, and many variants.

### Conclusion

Our work suggests at least three potential benefits:
1. For a bounded problem space (no anonymous users), it should be possible to use techniques from the fault tolerance field, suitably modified, to increase the availability of our systems in the face of concerted cyber attacks.
2. For a problem space with less restrictive assumptions, it should be possible to significantly improve how fast and how cheaply we can recover from intrusions with the implementation of continual online repair.
3. For the general case (any server on the Internet), it should be possible to prevent repeated attacks from succeeding, even when the attacks can be varied, with a combination of attack learning and generalization as part of a control loop that filters out bad requests.

Of course, these benefits come at the cost of additional hardware and administration. Our efforts are designed to minimize the likelihood of intrusion propagation, though not to provably eliminate it. This aligns with the best advice for building secure systems, which uses risk analysis rather than absolute security policies to evaluate system security.

### References

1. J. Gray, A. Avizienis, T. Barclay, L. Spainhower, and T. A. Gregg, “IBM S/390 Parallel Enterprise Server G5: A Historical Perspective,” IBM reprint, 0018-8646/99, 1999.
2. Department of Defense Trusted Security Evaluation Criteria, DOD 5200.28-STD, Library No. S225,711, December 1985.
3. J. Gray and D. Slutz, Microsoft TerraServer: A Spatial Data Warehouse, MS-TR-99-29, Microsoft Research, Advanced Technology Division, One Microsoft Way, Redmond WA 98052, June 1999.
4. J. Gray and A. Reuter, Transaction Processing: Concepts and Techniques, San Francisco, CA: Morgan Kaufmann Publishers, 1993.
5. “The N-Version Approach to Fault-Tolerant Software,” Transactions on Software Engineering, Vol. SE-22, No. 12, pp. 1491-1501, December 1985.
6. S. S. Brilliant, J. C. Knight, and N. G. Leveson, “Analysis of Faults in an N-Version Software Experiment,” IEEE Transactions on Software Engineering, Vol. SE-16, No. 2, February 1990.
7. B. W. Lampson, “Computer Security in the Real World,” Annual Security Applications Conference, 2000, http://www.acsac.org/2000/papers/lampson.pdf.
8. R. Balzer and N. Goldman, “Mediating Connectors,” Proceedings of the 19th IEEE International Conference on Distributed Computing Systems, Austin, Texas, May 31-June 4, 1999, IEEE Computer Society Press, pp. 73-77.
9. Hypertext Transfer Protocol—HTTP/1.1 (RFC 2616), http://www.w3.org/Protocols/rfc2616/rfc2616.html.