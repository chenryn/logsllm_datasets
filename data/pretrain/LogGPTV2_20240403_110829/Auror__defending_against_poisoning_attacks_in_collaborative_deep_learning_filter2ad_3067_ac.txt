(cid:10)
(cid:11)
(cid:12)
(cid:3)
(cid:4)
(cid:5)
(cid:6)
(cid:7)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:12)
(cid:3)
(cid:4)
(cid:5)
(cid:6)
(cid:7)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:12)
(cid:3)
(cid:4)
(cid:5)
(cid:6)
(cid:7)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:12)
(cid:3)
(cid:4)
(cid:5)
(cid:6)
(cid:7)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:12)
(cid:3)
(cid:4)
(cid:5)
(cid:6)
(cid:7)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:12)
Figure 4: Attack success rate for classifying different pairs of source and target digits when fraction of malicious users is 30%, 20%
and 10% respectively. The entry in each row represents the source label while the entry in each column represents the target label.
The gray value in each cube represents the success rate. Black represents 0%, while white represents 100% attack success rate. The
success rate is the highest when the malicious ratio is 30%.
Fraction of
Malicious users (%)
10%
20%
30%
Accuracy Drop
(%)
Min Max Avg
9
2
7
16
24
15
15
27
68
Table 1: Average accuracy drop, maximum accuracy drop and
minimum accuracy drop for MNIST due to poisoning attacks
as compared to benign model.
function in the network. For the output layer, the activation func-
tion is a Softmax function that converts the output of previous layer
to a digit between 0 to 9. We divide the 60,000 training samples
among 30 users, each of them having 2000 samples. Every user
computes the local model based on these 2000 training samples.
Attack Strategy. To measure the severity of targeted poisoning
attacks on above outlined system, we perform several experiments
by varying the fraction of malicious users f in this setting. The
malicious users range from 10% to 30% of all the users as we con-
sider f < n/2. In each experiment, the malicious users poison
their training data with the goal of inﬂuencing the global model
such that it classiﬁes a source digit (e.g., 1) as a target digit (e.g.,
3). Each malicious user extracts all the samples corresponding to
the source digit from its training data and ignores the images for
the remaining digits. Further, it replicates the source digit images
to occupy the entire training space of the user (2000 samples) and
mislabels them as the target digit. The malicious user uses this poi-
soned training data to compute the gradients from its local model.
We run the experiments for all the possible sources (1-10) and tar-
get (1-10) digits.
Attack Findings. To measure the attack success rate, we test the
global model with the samples for each digit in the test dataset.
Figure 4 shows the attack success rate for classifying each of the
source digits (e.g., 0) as the other digits (1 to 9) when fraction of
malicious users is 30%, 20% and 10%. The graph uses different
level of gray value with white for the largest value (90% and above)
and black for the lowest value (0%) to represent the attack success
rate. We observe that when the fraction of malicious users is 30%
the attack success rate for all the combinations vary between 89%
to 100% with 96% average attack success rate. For 20% malicious
users, we observe that the attack success rate reduces for speciﬁc
targets but is still high for majority of source to target digits with
91% average attack success rate. The success rate greatly varies
for different targets when the malicious fraction is reduced to 10%.
For instance, the attack success rate is only 3% for mislabeling 4
as 0, while 94% for mislabeling 0 as 6. Other digits demonstrate a
similar behavior. This phenomenon is an extrapolation of the effect
due to distance between the hyperplane of source and target digits
as explained in [34]. The average attack success rate increases from
65% to 96% with the increase in the fraction of malicious users
from 10% to 30%.
Lastly, we test all the 10,000 test samples and measure the ac-
curacy of the global model. In our experiments, the accuracy of
the model when trained under completely benign training data is
86%. Table 1 shows the accuracy drop for the global model due
to poisoning of the training data as compared to benign dataset for
different fraction of malicious users. The maximum accuracy drop
is as high as 68% (e.g. mislabeling 7 to 1) and minimum is 15%. In
addition, the average accuracy drop for various targeted poisoning
attack is 24%, 16% and 9% for 30%, 20% and 10% of malicious
users respectively. This shows that the accuracy drop increases with
increase in the fraction of malicious users. Hence, our experiments
demonstrate that targeted poisoning attacks not only inﬂuence the
classiﬁcation of the source data but also affect the overall accuracy
of the model.
4.2 German Trafﬁc Sign Benchmarks
Dataset. We use GTSRB dataset of German trafﬁc signs [40], an-
other popular benchmark for deep learning problem. The dataset
provides 39,209 training images in 43 classes and 12,630 test im-
ages in random order. Unlike MNIST, the images of trafﬁc signs
are RGB images and are stored in PPM format. The size of images
varies from 15 × 15 to 250 × 250. To eliminate the inﬂuence of
size, we reshape all images into 32 × 32 format. In addition, we
standardize the images with zero mean and unit variance. Hence,
each feature has a ﬂoating point value between 0 and 1. Among
all training set, we randomly choose 39,000 training samples and
divide them into 30 users, each of them having 1300 samples. Each
user computes its local model based on these 1300 training images.
513of dataset as compared to benign model. The model trained under
benign dataset exhibits an accuracy of 84%. As the total number of
classes is more (43), the accuracy drop is smaller for the GTSRB
dataset as compared to the MNIST dataset which as 10 classes.
Thus, even though the attacker can achieve signiﬁcant success for
a targeted misclassiﬁcation, the overall classiﬁcation for other im-
ages is fairly accurate. Speciﬁcally, the accuracy drop for case 1
and case 2 is 9% and 10% when the malicious ratio is 30%.
Result 1: Targeted poisoning attacks are signiﬁcantly effective in
indirect collaborative deep learning setting regardless of masking
the essential features of the training data and restricted poisoning
capacity of the adversary.
5. DESIGN
Our AUROR defense against poisoning attacks is deployable at
the training phase before the ﬁnal global model is generated. AU-
ROR ﬁlters the malicious users before creating the ﬁnal model, us-
ing the following steps.
Identifying Indicative Features. In designing AUROR, the ﬁrst
step is to identify the indicative features that show an anomalous
distribution under attack setting. It collects all the masked features
from users and compares each of these features. In this step, we
use KMeans algorithm to divide all users into two clusters for any
given masked features for ﬁrst 10 epochs since features for ﬁrst 10
epochs have enough difference between malicious users and benign
users and decrease the computation. However, any other clustering
algorithm can be used instead. We then calculate the distance be-
tween the centers of these clusters. If the distance exceeds a certain
limit α, we consider the masked features as indicative features. The
threshold α determines whether the two clusters are distinct from
each other. We choose α = 0.02 for MNIST dataset and for GT-
SRB α = 0.0045.
Identifying Malicious Users. The second step in AUROR is to
identify the malicious users based on the indicative features. For
each indicative feature, the users are divided into different clusters.
Every cluster with number of users which is smaller than n/2 are
marked as suspicious clusters, since the attackers are in the mi-
nority. The users that appear in suspicious clusters for more than
τ = 50% of the total indicative features are conﬁrmed as malicious
users. The threshold τ ensures that the benign users that show a
similar distribution as the malicious users within statistical error
are not labeled as malicious.
Training Global Model. The server excludes the input values from
the malicious users identiﬁed in the previous step and trains the
global model on the remaining masked features. The training pro-
cedure varies based on the underlying learning algorithm used in
the application. For the cases we illustrate in our paper, we use
privacy preserving deep learning architecture (PPDL) to retrain the
model.
Implementation. We implement the prototype of AUROR in Python.
The implementation contains a total of 5401 lines calculated us-
ing CLOC tool available on Linux. We use Theano package [12]
to realize the back-propagation procedure in deep learning. To
implement the collaborative setting for these applications, we use
multiprocessing module in Python to run users’ programs asyn-
chronously. We create a server process to aggregate masked fea-
tures submitted by each user and compute the ﬁnal global model.
6. AUROR EVALUATION
To evaluate the efﬁcacy of AUROR, we apply our defense to
the targeted poisoning attacks performed on image recognition sys-
Figure 5: Poisoning the dataset to classify a sign of bicycle
crossing as a sign of wild animal crossing (above) and classify a
sign of 20 km/h maximum speed limit at 80 km/h (below)
Fraction of
Malicious
Users (%)
10
20
30
Attack Success
Rate (%)
Bicycle
to Wild
Animal
31
18
79
20 km/h
to 80 km/h
26
17
61
Accuracy Drop
(%)
20 km/h
to 80 km/h
Bicycle
to Wild
Animal
2
3
9
2
6
10
Table 2: Attack success rate and accuracy drop for mislabeling
a sign of bicycle crossing as a sign of wild animal crossing and
20 km/h as 80 km/h in GTSRB dataset
Network Architecture. We use convolutional neural network (CNN)
to train the classiﬁer for trafﬁc signs. For CNN architecture, there
are two convolutional layers, two pooling layers, one fully-connected
layer and a ﬁnal output layer. The ﬁrst convolutional layer consists
of 64 ﬁlters with 2 × 2 and the second one with 16 ﬁlters with the
same size as the ﬁrst one. After each convolutional layer, there is
one pooling layer followed with a ﬁlter of same size 2 × 2. Be-
fore the output layer, there is a fully-connected layer consisting of
64 nodes with ReLU function as their activation function, which is
similar with the hidden layer of MLP architecture. For the output
layer, the activation function is a Softmax function that converts the
output of previous layer to a label between 0 to 42.
Attack Strategy. Since the number of classes for GTSRB is more
than 40, we randomly perform two pairwise target poisoning at-
tacks — classifying a cyclists crossing sign as a wild animals cross-
ing sign and classifying the sign of 20 km/h maximum speed limit
as 80 km/h (as shown in the Figure 5). The attack strategy is sim-
ilar to that of MNIST dataset where the malicious users replicate
the source image and mislabel them as the target image.
Attack Findings. Table 2 shows the attack success rate and accu-
racy drop for varying levels of malicious ratio when we mislabel a
sign of bicycle crossing as a sign of wild animal crossing (case 1)
and a sign of 20 km/h maximum speed limit as 80 km/h (case
2) respectively. The success rate for case 1 and case 2 is 79% and
61% separately for 30% malicious ratio and reduces with the de-
crease in the malicious ratio, although the attack success rate with
10% of malicious users is a little higher than when fraction of ma-
licious users is 20%. This shows that the attacker can poison the
dataset to achieve a targeted attack. However, the success rate dif-
fers depending on the source and target images that the attacker
selects.
The accuracy drop is small in the ﬁnal model due to poisoning
5141 · 10
−3
5 · 10
−4
0
−5 · 10
−4
e
u
l
a
v
t
n
e
i
d
a
r
g
e
g
a
r
e
v
A
e
u
l