TPC-C benchmark, relative to the cost of a stand-alone server.
shows, most of the power is consumed by the servers
when idle. The remaining power is mostly used to execute
transactions.
Fig. 3 presents the energy costs of the protocols (e.g.,
in units of money), relative to the cost of the stand-
alone server at 250 TPS, or about 55% to 75% of the
maximum load supported. After the stand-alone server,
PBR is the cheapest of all: it consumes respectively 3.3%
and 2.7% less energy than SMR and DUR. Although these
differences are modest, they may represent a large cost
difference when thousands of instances of these protocols
are deployed in large data centers.
V. ENHANCING ENERGY EFFICIENCY
We present two techniques that enhance the energy
efﬁciency of replication: one improves the performance
of DUR, the other decreases the energy consumption of
PBR. These algorithmic improvements provide a higher
average efﬁciency than the previously considered proto-
cols and match the peak efﬁciency of DUR. Although we
present these approaches in the context of DUR and PBR
respectively, these techniques are general enough to be
applied to other protocols.
A. Increasing Performance
With DUR, the updates of transactions that pass certi-
ﬁcation are applied to the database in the order deﬁned
by atomic broadcast. As a consequence, this process is
sequential and prone to be the bottleneck. In general,
applying updates must be done in the same order at
all replicas, otherwise the state of replicas may diverge.
Consider two transactions Ti, i ∈ {1, 2}, that read an item
xi and set an item y to xi. Transaction T1 is delivered ﬁrst
and passes certiﬁcation since it is the ﬁrst transaction to
be certiﬁed. Transaction T2 is delivered next and passes
certiﬁcation as well since T2 does not read y. Replica r1
applies the update of T1 followed by the one of T2 while
replica r2 applies the updates in the opposite order. In this
example, r1 ﬁnishes the execution with a value of y equal
to x2 whereas on r2 y has a value of x1.
Not all transactions update the same data items how-
ever. If the ﬁnal state of the database does not depend
on the order in which the updates of two transactions
are applied, then the updates of these transactions can
be applied in parallel. More precisely, we say that two
transactions T1 and T2 commute if, and only if, applying
the updates of T1 followed by the updates of T2 leads to
the same state as applying them in the opposite order.
We call the DUR protocol that applies the updates of
commutative transactions in parallel DURP U , for DUR
with parallel updates. In DURP U ,
the certiﬁcation of
transactions is done sequentially in the order deﬁned
by atomic broadcast, as before, and we rely on the
parameters of stored procedures to determine which trans-
actions contain commutative updates. Once a transaction
T passes certiﬁcation, we determine if T commutes with
the transactions currently updating the database. If it is
the case, we apply T ’s updates concurrently with the other
transactions. Otherwise, we delay the application of T ’s
updates until it is safe to do it.
Applying commutative updates in parallel does not
violate strict serializability since certiﬁcation requires
transactions to read up-to-date data items. Hence, any
execution e of DURP U is equivalent
to a sequential
execution e(cid:3) of the same set of committed transactions,
ordered in the same way as they are certiﬁed in e. In
particular, consider an execution where two commutative
transactions Ti, i ∈ {1, 2}, read and write item xi. The
updates of T1 are applied before those of T2 at replica r1;
updates of T1 and T2 are applied in the opposite order at
r2. A transaction T3 executing at r1 reads x1 from T1,
but reads x2 before T2 updates x2. Another transaction
T4 executing at r2 reads x2 from T2 but reads x1 before
T1 updates x1 (the updates of T1 and T2 are applied in
opposite orders at r1 and r2). In this execution, the cycle
of transaction dependencies T1 → T3 → T2 → T4 → T1
is avoided because both T3 and T4 abort. At the time of
their certiﬁcations, T3 read an outdated version of x2 and
T4 read an old version of x1.
Applying transaction updates in parallel can also be
used in PBR and SMR. With PBR, this lets backups
apply commutative transactions in parallel. In the case of
SMR, the presented technique cannot directly be applied.
Recall
that with this replication protocol, all replicas
execute transactions in their entirety. In this context, two
transactions can be executed in parallel if, and only if,
their order of execution does not affect the resulting state
nor does it change the transactions’ outputs.
B. Reducing Power
Improving performance is one way to increase en-
ergy efﬁciency, lowering energy consumption is another.
PBR improves on SMR by letting the primary execute
transactions and only applying transaction updates on the
413413413
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply. 
backups. As a result, PBR consumes less energy than
SMR. With PBR, only the state of the primary is necessary
to execute transactions in the absence of failures. Based on
this observation, we develop a technique that removes the
necessity to maintain state at the backups in the normal
case.
In PBR∗, backups append update statements coming
from the primary to a log instead of applying them to
the database to save energy—the remainder of the normal
case protocol is identical to PBR. When the primary fails,
the backup elected as new primary must apply the updates
in its log to rebuild the state (updates that commute can
be applied concurrently). To reduce recovery time and
to bound the size of the log, backups truncate their log
periodically. To do so, two options exist: either backups
apply the update statements present in their log, as done
during recovery, or they can receive a database snapshot
directly from the primary. The second option may be
preferable in situations where most updates overwrite
existing data items and the database size only grows
moderately. In the event of a backup failure, a fresh new
server is added to the group before receiving a snapshot
of the database from the primary.
A similar technique can be employed with SMR to
lower energy requirements. Care must be taken to ensure
that failures of the replica holding the state, the active
replica, is detected and replaced by one of the passive
replicas. Clients that did not receive an answer to their
submitted transaction notify the new active replica to
obtain the result of the transaction execution.
Finally, we note that letting a single replica maintain
state with DUR could be achieved in a similar way as with
SMR. The passive replicas would append transactions that
passed certiﬁcation to a log. Doing so would remove the
ability of DUR to execute transactions at multiple replicas
in parallel however.
C. Evaluation
We measure the energy efﬁciency of DURP U and PBR∗
under the TPC-C benchmark. The setup is identical to the
one of Section IV.
In Fig. 4, various metrics are presented as a function
of the load, as previously. Thanks to its ability to apply
commutative updates in parallel to the database, DURP U
improves on the throughput of DUR and reaches 471
TPS (Fig. 4(a)). Power consumption and CPU utilization
with DURP U is slightly higher than DUR (Fig. 4(c)
and Fig. 4(d)) because DURP U aborts more transactions
than DUR (Fig. 2(a) and 4(a)). Despite this, DURP U
provides higher energy efﬁciency than DUR with up to
7 clients. Beyond this point, the two protocols have the
same efﬁciency.
Because appending transaction updates to a log requires
less work than applying them to the database, PBR∗ exe-
cutes transactions faster than PBR and slightly improves
the maximum attained throughput. In addition, PBR∗
decreases power consumption for a maximum difference
of 5.5% with PBR (Fig. 4(c)). As a consequence, PBR∗
provides better energy efﬁciency than PBR across all
loads.
Although neither DURP U nor PBR∗ improve the peak
energy efﬁciency of the considered replication protocols,
they improve the average efﬁciency. PBR∗ and DURP U
provide an average of 1.81 and 1.6 TPS per Watt respec-
tively, compared to 1.56 TPS per Watt for DUR, the third
best protocol considering the average efﬁciency.
With PBR∗, we also measure the time it takes for the
primary to send a snapshot of the database to the backup
so that the backup can truncate its log. With 9 warehouses,
it takes the primary 18.9 seconds to save the database to
disk (the snapshot is 1123.5 MBs) and 10.4 seconds to
transfer the snapshot to the backup. The entire process
requires 5,460 Joules. If the average throughput is 250
TPS and log truncation happens once per day, sending a
database snapshot of 1.1 GB to one backup reduces the
energy efﬁciency of PBR∗ by 0.03%.
VI. A HYBRID APPROACH
hyb,
In this section, we explore the possibility of using
low-power devices to reduce energy requirements without
excessively compromising performance. Our approach,
denoted as PBR∗
it combines software
is hybrid:
techniques with a heterogeneous hardware deployment to
maximize energy efﬁciency. PBR∗
hyb builds upon PBR∗
and deploys backups on low-power devices, thereby im-
plementing a low-power log. To offer satisfactory per-
formance,
the primary runs on a powerful multi-core
machine.
Low-power devices typically consume a few Watts
at peak load. This allows us to stripe the log on the
backups for maximum performance with little energy
overhead. More speciﬁcally, the primary sends the up-
dates at sequence number i to backup i mod n, where
backups are numbered from 0 to n − 1. Arguably, this
decreases the fault-tolerance of the replicated system since
any backup failure triggers the recovery protocol due to
the unavailability of the log. Nevertheless, the software
running at the backups is signiﬁcantly simpler than the
one executing at the primary (which includes transaction
execution). Backups are thus less prone to software bugs
that would lead to a crash.
hyb normal case operation is identical to PBR∗.
Recovery requires special care, as we describe next. After
the failed primary has been replaced,2 the new primary
obtains the latest checkpoint of the database as well as
the log from the backups (both the log and the database
snapshot are striped to reduce state transfer time). The new
In PBR∗
2With PBR∗
hyb, a failed primary is always replaced by a powerful
multi-core machine to maintain high performance.
414414414
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply. 
)
s
m
(
y
c
n
e
t
a
L
25
20
15
10
5
0
)
W
(
r
e
w
o
P
l
a
t
o
T
240
230
220
210
200
190
180
PBR –•– PBR
∗
–+– DUR – – DURP U – –
11
40
39
37
21
28
13
0
200
100
500
Committed transactions per second
300
400
600
(a)
PBR –•– PBR
∗
–+– DUR – – DURP U – –
)
W
/
S
P
T
(
y
c
n
e
i
c
ﬁ
f
E
y
g
r
e
n
E
)
%
(
U
P
C
l
a
t
o
T
PBR –•– PBR
∗
–+– DUR – – DURP U – –
2.4
1.8
1.2
0.6
0
0
200
100
500
Committed transactions per second
300
400
600
(b)
PBR –•– PBR
∗
–+– DUR – – DURP U – –
700
560
420
280
140
0
200
100
500
Committed transactions per second
300
400
600
0
0
200
100
500
Committed transactions per second
300
400
600
(c)
(d)
Figure 4. The performance and costs of the improved replication protocols under the TPC-C benchmark. Graph (a) contains the
percentage of aborted transactions for PBR
and DURP U (only percentages larger than zero are reported).
∗
primary applies all updates statements in the log before
resuming normal operations. Upon a backup failure, the
faulty backup is replaced before the primary records a
new database snapshot on the striped log. After backups
learn that the snapshot has been successfully recorded,
they ﬂush their log.
A. Evaluation
We measure the energy efﬁciency of PBR∗
hyb on the
TPC-C benchmark, conﬁgured with 9 warehouses as
previously. The striped log is deployed on three Raspberry
PIs (adding a fourth Raspberry provides a negligible
speedup). Each Raspberry PI is a 700MHz ARM with
512 MB of memory and a 100Mbit/s network interface.
The log could also be implemented by other low-power
devices such as a dual-ported drive. We chose Raspberry
PIs due to their low power consumption, affordability, and
their ﬂexibility (they can run a full-ﬂedged Linux). The
primary is a quad-core Intel i7 2.2 GHz with 16 GB of
memory. Running a stand-alone server on a Raspberry PI
with a scaled-down version of TPC-C resulted in a peak
throughput of only 4.1 TPS for a power consumption of
2 Watts. The results below show that PBR∗
hyb allows to
more than double this energy efﬁciency while sustaining
a much higher throughput. In all experiments, clients run
on a separate machine.
In Fig. 5, we compare a stand-alone server running on
the quad-core machine to PBR∗
hyb using the same metrics
as before. We omit the percentage of aborted transactions
as it was always lower than 1%. In each graph, we plot
one metric as a function of the load. We consider between
1 and 8 clients.
PBR∗
hyb does not attain the maximal
throughput of
a stand-alone server (Fig. 5(a)). At 320 TPS, no more
load is supported. Surprisingly, backups show a CPU
utilization of only 66% at this throughput. After inves-
tigating the cause of this behavior, we observed that it
was due to a combination of a higher transaction latency
and lock contention. Due to its higher latency, PBR∗
hyb
reaches a lower throughput
than a stand-alone server
with an identical number of clients. With eight clients
no more throughput is supported due to lock contention.
We experimentally veriﬁed this hypothesis by purposely
violating strict serializability and allowing the primary to
respond to the clients directly after executing transactions
415415415
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply. 
30
24
18
12
6
0
80
70
60
50
40