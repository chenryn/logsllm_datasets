### TPC-C Benchmark and Energy Consumption Analysis

The TPC-C benchmark, relative to the cost of a stand-alone server, reveals that most of the power is consumed by the servers even when they are idle. The remaining power is primarily used to execute transactions.

Figure 3 presents the energy costs of various protocols (in units of money) relative to the cost of a stand-alone server at 250 TPS, or about 55% to 75% of the maximum supported load. After the stand-alone server, PBR is the most cost-effective: it consumes 3.3% and 2.7% less energy than SMR and DUR, respectively. Although these differences are modest, they can represent significant cost savings when thousands of instances of these protocols are deployed in large data centers.

### Enhancing Energy Efficiency

We present two techniques to enhance the energy efficiency of replication: one improves the performance of DUR, while the other reduces the energy consumption of PBR. These algorithmic improvements provide higher average efficiency than the previously considered protocols and match the peak efficiency of DUR. Although we discuss these approaches in the context of DUR and PBR, they are general enough to be applied to other protocols.

#### A. Increasing Performance

In DUR, transaction updates that pass certification are applied to the database in the order defined by atomic broadcast, making the process sequential and prone to bottlenecks. Generally, updates must be applied in the same order at all replicas to avoid state divergence.

Consider two transactions \( T_1 \) and \( T_2 \) that read an item \( x_i \) and set an item \( y \) to \( x_i \). If \( T_1 \) is delivered first and passes certification, and \( T_2 \) is delivered next and also passes certification, then replica \( r_1 \) applies the update of \( T_1 \) followed by \( T_2 \), while replica \( r_2 \) applies them in the opposite order. In this example, \( r_1 \) ends with \( y \) equal to \( x_2 \), whereas \( r_2 \) has \( y \) equal to \( x_1 \).

Not all transactions update the same data items. If the final state of the database does not depend on the order of updates, then these updates can be applied in parallel. Specifically, two transactions \( T_1 \) and \( T_2 \) commute if applying the updates of \( T_1 \) followed by \( T_2 \) results in the same state as applying them in the opposite order.

We call the DUR protocol that applies the updates of commutative transactions in parallel DURPU (DUR with parallel updates). In DURPU, the certification of transactions is done sequentially as before, and we use the parameters of stored procedures to determine which transactions contain commutative updates. Once a transaction \( T \) passes certification, we check if \( T \) commutes with the transactions currently updating the database. If so, we apply \( T \)'s updates concurrently; otherwise, we delay the application until it is safe.

Applying commutative updates in parallel does not violate strict serializability because certification requires transactions to read up-to-date data items. Any execution \( e \) of DURPU is equivalent to a sequential execution \( e' \) of the same set of committed transactions, ordered as they are certified in \( e \). For example, consider an execution where two commutative transactions \( T_1 \) and \( T_2 \) read and write item \( x_i \). The updates of \( T_1 \) are applied before those of \( T_2 \) at replica \( r_1 \); updates of \( T_1 \) and \( T_2 \) are applied in the opposite order at \( r_2 \). A transaction \( T_3 \) executing at \( r_1 \) reads \( x_1 \) from \( T_1 \), but reads \( x_2 \) before \( T_2 \) updates \( x_2 \). Another transaction \( T_4 \) executing at \( r_2 \) reads \( x_2 \) from \( T_2 \) but reads \( x_1 \) before \( T_1 \) updates \( x_1 \). In this execution, the cycle of transaction dependencies \( T_1 \rightarrow T_3 \rightarrow T_2 \rightarrow T_4 \rightarrow T_1 \) is avoided because both \( T_3 \) and \( T_4 \) abort. At the time of their certifications, \( T_3 \) read an outdated version of \( x_2 \) and \( T_4 \) read an old version of \( x_1 \).

Parallel application of transaction updates can also be used in PBR and SMR. With PBR, backups can apply commutative transactions in parallel. In the case of SMR, this technique cannot be directly applied because all replicas execute transactions in their entirety. Two transactions can be executed in parallel if their order of execution does not affect the resulting state or change the transactions' outputs.

#### B. Reducing Power

Improving performance is one way to increase energy efficiency, and lowering energy consumption is another. PBR improves on SMR by allowing the primary to execute transactions and only applying transaction updates on the backups, thus consuming less energy than SMR. With PBR, only the state of the primary is necessary to execute transactions in the absence of failures. Based on this observation, we develop a technique that removes the necessity to maintain state at the backups in normal operation.

In PBR*, backups append update statements from the primary to a log instead of applying them to the database, saving energy. The remainder of the normal case protocol is identical to PBR. When the primary fails, the backup elected as the new primary must apply the updates in its log to rebuild the state (commutative updates can be applied concurrently). To reduce recovery time and bound the log size, backups periodically truncate their log. They can either apply the update statements in the log, as done during recovery, or receive a database snapshot directly from the primary. The second option is preferable when most updates overwrite existing data items and the database size grows moderately. In the event of a backup failure, a fresh server is added to the group and receives a snapshot from the primary.

A similar technique can be employed with SMR to lower energy requirements, ensuring that failures of the active replica are detected and replaced by a passive replica. Clients that did not receive an answer to their submitted transaction notify the new active replica to obtain the result.

Finally, letting a single replica maintain state with DUR could be achieved similarly to SMR. Passive replicas would append transactions that passed certification to a log, but this would remove DUR's ability to execute transactions in parallel at multiple replicas.

### Evaluation

We measure the energy efficiency of DURPU and PBR* under the TPC-C benchmark, using the same setup as in Section IV.

Figure 4 presents various metrics as a function of the load. Thanks to its ability to apply commutative updates in parallel, DURPU improves the throughput of DUR, reaching 471 TPS (Fig. 4(a)). Power consumption and CPU utilization with DURPU are slightly higher than DUR (Fig. 4(c) and Fig. 4(d)) because DURPU aborts more transactions (Fig. 2(a) and Fig. 4(a)). Despite this, DURPU provides higher energy efficiency than DUR with up to 7 clients. Beyond this point, the two protocols have the same efficiency.

Appending transaction updates to a log requires less work than applying them to the database, so PBR* executes transactions faster than PBR and slightly improves the maximum attained throughput. Additionally, PBR* decreases power consumption by up to 5.5% compared to PBR (Fig. 4(c)). Consequently, PBR* provides better energy efficiency than PBR across all loads.

Although neither DURPU nor PBR* improve the peak energy efficiency of the considered replication protocols, they enhance the average efficiency. PBR* and DURPU provide an average of 1.81 and 1.6 TPS per Watt, respectively, compared to 1.56 TPS per Watt for DUR, the third-best protocol in terms of average efficiency.

With PBR*, we also measure the time it takes for the primary to send a snapshot of the database to the backup to truncate the log. With 9 warehouses, it takes the primary 18.9 seconds to save the database to disk (the snapshot is 1123.5 MB) and 10.4 seconds to transfer the snapshot to the backup. The entire process requires 5,460 Joules. If the average throughput is 250 TPS and log truncation happens once per day, sending a 1.1 GB database snapshot to one backup reduces the energy efficiency of PBR* by 0.03%.

### A Hybrid Approach

In this section, we explore the possibility of using low-power devices to reduce energy requirements without excessively compromising performance. Our approach, denoted as PBR*hyb, combines software techniques with a heterogeneous hardware deployment to maximize energy efficiency. PBR*hyb builds upon PBR* and deploys backups on low-power devices, implementing a low-power log. The primary runs on a powerful multi-core machine to offer satisfactory performance.

Low-power devices typically consume a few Watts at peak load, allowing us to stripe the log on the backups for maximum performance with little energy overhead. Specifically, the primary sends updates at sequence number \( i \) to backup \( i \mod n \), where backups are numbered from 0 to \( n-1 \). This decreases fault tolerance since any backup failure triggers the recovery protocol due to the unavailability of the log. However, the software running on backups is simpler than the one executing at the primary, making backups less prone to software bugs that lead to crashes.

PBR*hyb's normal case operation is identical to PBR*. Recovery requires special care. After the failed primary is replaced, the new primary obtains the latest checkpoint of the database and the log from the backups (both the log and the database snapshot are striped to reduce state transfer time). The new primary applies all updates in the log before resuming normal operations. Upon a backup failure, the faulty backup is replaced, and the primary records a new database snapshot on the striped log. Backups then flush their log after learning that the snapshot has been successfully recorded.

#### A. Evaluation

We measure the energy efficiency of PBR*hyb on the TPC-C benchmark, configured with 9 warehouses. The striped log is deployed on three Raspberry Pis (adding a fourth Raspberry Pi provides negligible speedup). Each Raspberry Pi is a 700MHz ARM with 512 MB of memory and a 100Mbit/s network interface. We chose Raspberry Pis due to their low power consumption, affordability, and flexibility (they can run a full-fledged Linux). The primary is a quad-core Intel i7 2.2 GHz with 16 GB of memory. Running a stand-alone server on a Raspberry Pi with a scaled-down version of TPC-C resulted in a peak throughput of only 4.1 TPS for a power consumption of 2 Watts. The results below show that PBR*hyb allows for more than double this energy efficiency while sustaining a much higher throughput. In all experiments, clients run on a separate machine.

Figure 5 compares a stand-alone server running on the quad-core machine to PBR*hyb using the same metrics as before. We omit the percentage of aborted transactions as it was always lower than 1%. In each graph, we plot one metric as a function of the load, considering between 1 and 8 clients.

PBR*hyb does not attain the maximal throughput of a stand-alone server (Fig. 5(a)). At 320 TPS, no more load is supported. Surprisingly, backups show a CPU utilization of only 66% at this throughput. After investigating, we found that this behavior was due to a combination of higher transaction latency and lock contention. Due to its higher latency, PBR*hyb reaches a lower throughput than a stand-alone server with the same number of clients. With eight clients, no more throughput is supported due to lock contention. We experimentally verified this hypothesis by violating strict serializability and allowing the primary to respond to clients directly after executing transactions.