Therefore, an application need not implement its own mechanism 
to handle load imbalance in the application layer.  
*  A  chunk  of  packet  buffers  is  a  group  of  packet  buffers  that 
is  efficient.  WireCAP  employs 
(3)  With  WireCAP, 
(2)  WireCAP 
typically occupy physically contiguous memory. 
(4)  With  WireCAP,  the  performance  of  a  packet  processing 
application can be improved. WireCAP provides large ring buffer 
pools in the kernel to accommodate captured packets and supports 
zero-copy packet capture and delivery. Therefore, an application 
need not copy captured packets from low-level ring buffer pools 
and store them into its own set of buffers in user space. Instead, 
the application can use ring buffer pools as its own data buffers, 
and process the captured packets directly from there. This strategy 
helps to improve application performance. 
(5) WireCAP implements a packet transmit function that allows 
captured packets to be forwarded, potentially after being analyzed 
in flight. Thus, WireCAP can be used to support middlebox-type 
applications. 
Ultimately,  WireCAP  is  intended  to  advance  the  state-of-the-art 
for packet capture engines on commodity multicore systems. Our 
design is unique in the sense that we seek to address off-the-wire 
packet  capture  concerns  in  conjunction  with  packet  delivery 
issues to the application. WireCAP potentially paves the way for 
advancement of packet analysis tools to deal with emerging high-
speed networks. 
  We  implemented  WireCAP  in  Linux.  It  consists  of  a  kernel-
mode driver and a user-mode library. The optimized kernel-mode 
driver  manages  commodity  NICs  and  provides  WireCAP’s  low-
level packet capture and transmit services. The user-mode library 
is  Libpcap-compatible  [20],  which  provides  a  standard  interface 
for  low-level  network  access  and  allows  existing  network 
monitoring  applications  to  use  WireCAP  without  changes.  We 
evaluated WireCAP with several experiments. Experiments have 
demonstrated 
that  WireCAP  achieves  better  performance 
compared with existing packet captures engines. For example, in 
our packet capture experiments, WireCAP can capture and deliver 
100%  of  the  network  traffic  to  applications  without  loss  while 
existing  packet  capture  engines  (e.g.,  PF_RING,  NETMAP,  and 
DNA) suffer a packet drop rate ranging from 20% to 40% under 
the same conditions.  
2 Background and Motivation 
2.1  Problems  with  existing  packet  capture 
engines  
  When a commodity system with a commodity NIC is used for 
packet capture, the NIC is put into promiscuous mode to capture 
network packets. A packet capture engine manages the NIC and 
provides packet capture services to user-space applications. 
Capturing  packets  from  the  wire.  Packet  capture  is  a  special 
case of packet reception [21]. Typically, a commodity NIC can be 
logically partitioned into one or multiple receive queues. For each 
receive  queue,  the  NIC  maintains  a  ring  of  receive  descriptors, 
called a receive ring. The number of receive descriptors in a ring 
is device-dependent. For example, an Intel 82599-based 10 GigE 
NIC has 8192 receive descriptors [22]. Assuming that the NIC is 
configured with n receive queues, each receive queue will have, at 
most,  a  ring  size  of  8192 / n
#$ .  A  receive  descriptor  must  be 
initialized  and  pre-allocated  with  an  empty  ring  buffer  in  host 
memory—in the ready state—to receive a packet. Before packet 
capture  begins,  the  packet  capture  engine  performs  this  receive 
ring initialization. When network packets arrive, the NIC moves 
packets from the wire to the ring buffers via direct memory access 
(DMA).  Subsequently,  applications  can  access  the  received 
packets  through  the  OS  services  provided  by  the  packet  capture 
engine.  Receive  descriptors  are  used  circularly  and  repeatedly. 
The  packet  capture  engine  needs  to  reinitialize  a  used  receive 
!"
descriptor  and  reload  it  with  an  empty  ring  buffer  as  quickly  as 
possible because incoming packets will be dropped if the receive 
descriptors in the ready state aren’t available. 
Existing packet capture engines. The protocol stack of a general 
purpose OS can provide standard packet capture services through 
raw sockets (e.g., PF_PACKET). However, because the protocol 
stack is designed to support a wide range of network applications, 
it  is  not  optimized  for  packet  capture.  Consequently,  the 
performance  is  inadequate  for  packet  capture  in  high-speed 
networks. A number of packet capture engines, such as PF_RING 
[14], NETMAP [15], and DNA [16], have been developed. These 
packet  capture  engines  essentially  bypass  the  standard  protocol 
stack and achieve improved performance through techniques such 
as  pre-allocated 
large  packet  buffer,  packet-level  batch 
processing,  and  zero-copy  [15,  23].  Existing  packet  capture 
engines  can  be  classified  into  two  types,  depending  on  how 
captured packets are delivered from ring buffers to applications: 
  Type-I Packet Capture Engine, represented by PF_RING, pre-
allocates  a  large  packet  buffer  in  kernel  space  for  each  receive 
ring. The large packet buffer consists of fixed-size cells. Each cell 
corresponds to a ring buffer and is assigned to a receive descriptor 
in the ring. A receive descriptor and its assigned ring buffer are 1-
to-1 mapped to one another. The ring buffers are used circularly 
and repeatedly; a used receive descriptor is refilled with the same 
assigned  ring  buffer.  In  addition,  PF_RING  allocates  an 
intermediate  data  buffer,  termed  pf_ring,  within  the  kernel  and 
uses  it  as  data  capture  buffer  for  each  receive  ring.  When  the 
network packets arrive, the NIC moves the packets from the wire 
to  the  ring  buffers  via  DMA.  Subsequently,  the  packet  capture 
engine  copies  packets  from  the  ring  buffers  to  pf_ring  (for 
example, using NAPI polling in Linux [21]). Finally, a user-space 
application  accesses  captured  packets  from  pf_ring.  To  improve 
performance, pf_ring is memory-mapped into the address space of 
the user-space application to avoid copying.  
  Type-II  Packet  Capture  Engine,  represented  by  DNA  and 
NETMAP.  Like  PF_RING,  DNA  and  NETMAP  implement  a 
similar  pre-allocated  large  packet  buffer  scheme  for  ring  buffer 
operations. DNA and NETMAP expose shadow copies of receive 
rings to user-space applications. The ring buffers of a receive ring 
are  memory-mapped  into  the  address  space  of  a  user-space 
application. The ring buffers not only are used to receive packets 
but are also employed as data capture buffer to temporarily store 
the packets. When network packets arrive, the NIC moves packets 
from  the  wire  to  the  ring  buffers  via  DMA.  A  user-space 
application  accesses  packets  directly  from  the  memory-mapped 
ring  buffers.  The  advantage  of  this  design  is  that  it  avoids  the 
costs of unnecessary data movement. 
The problems. A Type-I packet capture engine requires at least 
one copy to move a packet from the NIC ring into the user space. 
At  high  packet  rates,  excessive  data  copying  results  in  poor 
performance.  In  addition,  it  may  suffer  the  receive  livelock 
problem in user context [24].  
  A  Type-II  packet  capture  engine  has 
limited  buffering 
capability. For DNA or NETMAP, a received packet is kept in a 
NIC ring buffer until it is consumed. During this period, the ring 
buffer and its associated receive descriptor cannot be released and 
reinitialized. Because a NIC ring has a limited number of receive 
descriptors,  the  receive  descriptors  in  the  ready  state  can  be 
rapidly  depleted  if  the  received  packets  are  not  consumed  in  a 
timely  manner.  Consequently,  subsequent  packets  would  be 
dropped. 
397
that 
load 
imbalance  occurs  frequently 
  Neither  type  of  packet  capture  engine  has  an  effective 
offloading  mechanism  to  address  the  long-term  load  imbalance 
problem. 
2.2 Experimental proof 
  We conduct two experiments to support our claims. In the first, 
we  demonstrate 
in 
multicore systems. In the second, we demonstrate the deficiencies 
of  existing  packet  capture  engines,  including  PF_RING,  DNA, 
and NETMAP.  
Experiment tools. The first tool is called queue_profiler. It is a 
single-threaded  application  that  captures  packets  from  a  specific 
receive queue and counts the number of packets captured every 10 
ms.  
  The second tool is called pkt_handler and is single-threaded. It 
captures and processes packets from a specific queue and executes 
a  repeating  while  loop.  In  each  loop,  a  packet  is  captured  and 
applied  with  a  Berkeley  Packet  Filter  (BPF)  [25]  x  times  before 
being  discarded.  By  varying  x,  we  simulate  different  packet-
processing  rates  of  real  applications  during  monitoring.  In  our 
experiments, the BPF filter “131.225.2 and UDP” is used, and x is 
set  to  0  and  300,  respectively.  With  x=0,  no  packet-processing 
load is actually applied. We use this value to evaluate whether a 
packet capture engine can capture packets at wire speed with no 
loss. With x=300, the packet-processing rate of a single 2.4 GHz 
CPU is 38,844 p/s. The value 300 was selected to emulate a heavy 
load application such as snort [1], which is capable of sustaining 
similar packet-processing rates [26]. 
Experiment data. To ensure that the evaluations are practical and 
repeatable, we capture traffic from the Fermilab border router for 
use  as  experiment  data.  The  experiment  data  includes  5  million 
packets and lasts for approximately 32 seconds. 
Experiment configuration. The experiment system is depicted in 
Figure  2.  It  is  a  non-uniform  memory  access  (NUMA)  system 
with  two  NUMA  nodes—node  0  and  node1.  Each  NUMA  node 
features an Intel E5-2690 processor (P0 or P1). The system has 32 
GB of system memory and the system I/O bus is PCIE-Gen3. In 
additional,  two  Intel  82599-based  10  GigE  NICs—NIC1  and 
NIC2—are installed in NUMA node 0. A traffic generator capable 
of generating traffic at wire speed or replaying captured traffic at 
the speed exactly as recorded is connected directly to NIC1. NIC2 
is not used in this experiment. 
  The  system  runs  a  64-bit  Scientific  Linux  6.0  server 
distribution.  For  packet  capture  engines,  we  use  driver 
pf_ring_5.5.2  (PF_RING)  [14],  ixgbe-3.10.16-DNA  (DNA)  [16], 
and 20131019-netmap (NETMAP) [15]. 
0
e
d
o
MemN
NIC1
NIC2
P0
I
O
H
P1
I
O
H
1
e
d
o
N
Mem
Figure 2. Experiment system 
Experiment 1. The traffic generator replays the captured data at 
the speed exactly as recorded. NIC1 is configured with six receive 
queues (queue 0 – 5), with each queue tied to a distinct core (core 
0 – 5). A separate queue_profiler is launched to profile traffic for 
398
"
s
t
e
k
c
a
P
"
f
o
"
r
e
b
m
u
N
3000"
2500"
2000"
1500"
1000"
500"
0"
0"
Receive"queue"0"
Receive"queue"3"
5"
20"
25"
35"
Time"(Second)"<"Traﬃc"is"binned"in"10ms"interval"
10"
15"
30"
Figure 3. Load imbalance 
each queue. We use DNA as the packet capture engine. No packet 
drops occur in the experiment. Figure 3 presents the time-series of 
the  number  of  packets  received  during  a  short  time  interval  for 
queue  0  and  3,  respectively.  The  figure  shows  that  a  load 
imbalance can occur routinely in a multicore system because the 
Intel NIC distributes packets to cores based on a per-flow policy 
that assigns packets of the same flow to the same core [22]. We 
observed  both  short-term  load  imbalance  (short-term  bursts  of 
packets)  and  long-term  load  imbalance  (queue  0  receives  much 
more  traffic  than  queue  3).  Because  network  traffic  is  bursty  by 
nature  and  TCP  is  the  dominant  transport  protocol,  a  short-term 
load imbalance occurs more frequently in practice. When we vary 
the number of receive queues, similar load imbalance patterns are 
observed. 
Experiment  2.  The  traffic  generator  replays  the  same  captured 
data  as  in  experiment  1.  Again,  NIC1  is  configured  with  six 
receive  queues  (queue  0  –  5),  and  each  queue  tied  to  a  distinct 
core (core 0 – 5) on which a separate pkt_handler is launched to 
capture and process the traffic for each queue. For pkt_handler, x 
is set to 0 and 300, respectively. The CPU speed is set at 2.4 GHz. 
We  vary  the  packet  capture  engines  in  the  experiments,  using 
PF_RING  (mode  2),  DNA,  and  NETMAP,  respectively.  Each 
NIC  receive  ring  is  configured  with  a  size  of  1,024.  For 
PF_RING, the size of pf_ring buffer is set to 10,240. We calculate 
the  packet  capture  drop  rate  and  the  packet  delivery  drop  rate. 
Note: because PF_PACKET’s performance is too poor compared 
with 
include 
PF_PACKET  in  our  experiments.  Readers  can  refer  to  [9]  for 
PF_PACKET evaluation. 
  With  x=0,  each  pkt_handler  does  not  incur  any  packet-
processing  load.  In  the  experiments,  we  did  not  observe  any 
packet drops for DNA or NETMAP. However, we observed that 
PF-RING suffers small packet capture drops at queue 3.  
  With  x=300,  pkt_handler  emulates  a  heavy  load  application. 
Table 1 lists the packet drop rates for queue 0 and 3, respectively. 
At  queue  0,  the  incoming  packet  rate  sustains  approximately 
80,000 p/s from 10s to 35s (Figure 3, Note: the traffic is binned in 
10ms interval), which far exceeds the packet-processing speed of 
a 2.4 GHz CPU (38,844 p/s). Because the pkt_handler at queue 0 
is  over-flooded  by  incoming  packets,  all  packet  capture  engines 
suffer  substantial  packet  drops.  DNA  and  NETMAP  suffer 
substantial packet capture drops because they use ring buffers as 
data  capture  buffers,  which  has  a  limited  buffering  capability. 
PF_RING  avoids  packet  capture  drops  but  suffers  substantial 
packet delivery drops due to both the inability of the application 
to keep pace with the packet capture rate and the receive livelock 
problem [24]. In addition, because PF_RING requires at least one 
copy  to  move  a  packet  from  the  NIC  ring  into  the  user  space, 
PF_RING  incurs  higher  packet  capture  costs  than  DNA  and 
NETMAP. Consequently, PF_RING suffers higher overall packet 
drops  than  DNA  and  NETMAP.  Table  1  demonstrates  that  the 
these  packet  capture  engines,  we  do  not 
existing  packet  capture  engines  lack  an  offloading  capability  to 
migrate traffic to less busy or idle cores, where other thread would 
process it. 
  At  queue  3,  the  incoming  packet  rate  sustains  approximately 
20,000 p/s between 1s and 32s (Figure 3). While this rate is less 
than the packet processing speed of a 2.4 GHz CPU (38,844 p/s), 
DNA  and  NETMAP  still  suffer  significant  packet  capture  drops 
due  to  the  limited  buffing  capability  for  dealing  with  short-term 
bursts  of  packets.  For  example,  during  the  time  interval  [3.86s 
3.97s], 2724 packets are sent to queue 3. Because pkt_handler can 
process only 10ms * 38,844 p/s = 388 packets during the interval 
and because the ring buffers can temporarily buffer 1024 packets 
at  most,  packet  capture  drops  inevitably  occur.  PF-RING  also 
suffers small packet capture drops. 
Table 1. Packet drop rates 
Receive Queue 0: 
Packet Capture Drops 
Packet Delivery Drops 
Receive Queue 3: 
Packet Capture Drops 
Packet Delivery Drops 
NETMAP  DNA  PF_RING 
46.5% 
0% 
50.1% 
0% 