device. Each sample has three metadata: Internet connection tech-
nology, ISP, and US state. A record contains UDP ping loss rate (min.
across measured servers) and total traffic (bytes sent and received),
reflecting client’s aggregate Internet usage.
Google Cluster Usage Traces (GCUT): This dataset [90] con-
tains usage traces of a Google Cluster of 12.5k machines over 29
days in May 2011. We use the logs containing measurements of
task resource usage, and the exit code of each task. Once the task
starts, the system measures its resource usage (e.g. CPU usage,
6From a privacy perspective, model and data sharing may suffer similar information
leakage risks [53], but this may be a pragmatic choice some providers can make
nonetheless.
7Our choice of using public datasets is to enable others to independently validate and
reproduce our work.
471
memory usage) per second, and logs aggregated statistics every 5
minutes (e.g., mean, maximum). Those resource usage values are
the measurements. When the task ends, its end event type (e.g. FAIL,
FINISH, KILL) is also logged. Each task has one end event type,
which we treat as an metadata.
More details of the datasets (e.g. dataset schema) are attached in
Appendix A.
5.1.2 Baselines. We only compare DG to the baselines in §2.2
that are general—the machine-learned models. (In the interest of
reproducibility, we provide complete configuration details for these
models in Appendix B.)
Hidden Markov models (HMM) (§2.2): While HMMs have been
used for generating time series data, there is no natural way to
jointly generate metadata and time series in HMMs. Hence, we
infer a separate multinomial distribution for the metadata. During
generation, metadata are randomly drawn from the multinomial
distribution on training data, independently of the time series.
Nonlinear auto-regressive (AR) (§2.2): Traditional AR models
can only learn to generate measurements. In order to jointly learn
to generate metadata and measurements, we design the following
more advanced version of AR: we learn a function f such that Rt =
f (A, Rt−1, Rt−2, ..., Rt−p). To boost the accuracy of this baseline,
we use a multi-layer perceptron version of f . During generation, A
is randomly drawn from the multinomial distribution on training
data, and the first record R1 is drawn a Gaussian distribution learned
from training data.
In this model, we
Recurrent neural networks (RNN) (§2.2):
train an RNN via teacher forcing [111] by feeding in the true time
series at every time step and predicting the value of the time series
at the next time step. Once trained, the RNN can be used to gen-
erate the time series by using its predicted output as the input for
the next time step. A traditional RNN can only learn to generate
measurements. We design an extended RNN takes metadata A as
an additional input. During generation, A is randomly drawn from
the multinomial distribution on training data, and the first record
R1 is drawn a Gaussian distribution learned from training data.
Naive GAN (§3.3.2): We include the naive GAN architecture (MLP
generator and discriminator) in all our evaluations.
TimeGAN [117]: Note that the state-of-the-art TimeGAN [116]
does not jointly generate metadata and high-dimensional time se-
ries of different lengths, so several of our evaluations cannot be run
on TimeGAN. However, we modified the TimeGAN implementa-
tion directly [116] to run on the WWT dataset (without metadata)
and compared against it.
RCGAN [35]: RCGAN does not generate metadata, and only deals
with time series of the same length, so again, several of our evalua-
tions cannot be run on RCGAN. To make a comparison, we used the
version without conditioning (called RGAN [35]) from the official
implementation [34] and evaluate it on the WWT dataset (without
metadata).
Market Simulator [17]: We also compare against a VAE-based
approach [17] designed to generate synthetic financial market data,
since its code is publicly available.
5.1.3 Metrics. Evaluating GAN fidelity is notoriously difficult [72,
114]; the most widely-accepted metrics are designed for labelled
image data [55, 92] and cannot be applied to our datasets. Moreover,
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
t
n
u
o
C
Task duration (seconds)
Figure 9: Histogram of task duration for the Google Cluster
Usage Traces. RNN-generated data misses the second mode,
but DoppelGANger captures it.
generation, though its performance may be due to other architec-
tural differences [35, 117]. E.g., WWT is an order of magnitude
longer than the time series it evaluates on [34, 116].
Another aspect of learning temporal correlations is generating
time series of the right length. Figure 9 shows the duration of tasks
in the GCUT dataset for real and synthetic datasets generated by
DG and RNN. Note that TimeGAN generates time series of different
lengths by first generating time series of a maximum length and
then truncating according to the empirical length distribution from
the training data [116]. Hence we do not compare against TimeGAN
because the comparison is not meaningful; it perfectly reproduces
the empirical length distribution, but not because the generator is
learning to reproduce time series lengths.
DG’s length distribution fits the real data well, capturing the
bimodal pattern in real data, whereas RNN fails. Other baselines
are even worse at capturing the length distribution (Appendix C).
We observe this regularly; while DG captures multiple data modes,
our baselines tend to capture one at best. This may be due to the
naive randomness in the other baselines. RNNs and AR models
incorporate too little randomness, causing them to learn simpli-
fied duration distributions; HMMs instead are too random: they
maintain too little state to generate meaningful results.
Cross-measurement correlation: To evaluate correlations between
the dimensions of our measurements, we computed the Pearson cor-
relation between the CPU and memory measurements of generated
samples from the GCUT dataset. Figure 8 shows the CDF of these
correlation coefficients for different time series. We observe that
DG much more closely mirrors the true auto-correlation coefficient
distribution than any of our baselines.
Measurement distribution: As discussed in §4.3 and Figure 7, DG
captures the distribution of (max+min)/2 of page views in WWT
dataset. As a comparison, TimeGAN and RCGAN have much worse
fidelity. TimeGAN captures the two modes in the distribution, but
fails to capture the tails. RCGAN does not learn the distribution
at all. In fact, we find that RCGAN has severe mode collapse in
this dataset: all the generated values are close to -1. Some possible
reasons might be: (1) The maximum sequence length experimented
in RCGAN is 30 [34], whereas the sequence length in WWT is 550,
which is much more difficult; (2) RCGAN used different numbers of
generator and discriminator updates per step in different datasets
Figure 8: CDF of Pearson correlation between CPU rate and
assigned memory usage from GCUT.
numeric metrics do not always capture the qualitative problems of
generative models. We therefore evaluate DG with a combination
of qualitative and quantitative microbenchmarks and downstream
tasks that are tailored to each of our datasets. Our microbenchmarks
evaluate how closely a statistic of the generated data matches the
real data. E.g., the statistics could be attribute distributions or au-
tocorrelations, and the similarity can be evaluated qualitatively or
by computing an appropriate distance metric (e.g., mean square
error, Jensen-Shannon divergence). Our downstream tasks use the
synthetic data to reason about the real data, e.g., attribute predic-
tion or algorithm comparison. In line with the recommendations of
[72], these tasks can be evaluated with quantitative, task-specific
metrics like prediction accuracy. Each metric is explained in more
detail inline.
5.2 Results
In line with prior recommenda-
Structural characterization:
tions [79], we explore how DG captures structural data properties
like temporal correlations, metadata distributions, and metadata-
measurement joint distributions.8
Temporal correlations: To show how DG captures temporal corre-
lations, Figure 1 shows the average autocorrelation for the WWT
dataset for real and synthetic datasets (discussed in §2.2). As men-
tioned before, the real data has a short-term weekly correlation and
a long-term annual correlation. DG captures both, as evidenced
by the periodic weekly spikes and the local peak at roughly the
1-year mark, unlike our baseline approaches. It also exhibits a 91.2%
lower mean square error from the true data autocorrelation than
the closest baseline (RCGAN).
The fact that DG captures these correlations is surprising, par-
ticularly since we are using an RNN generator. Typically, RNNs
are able to reliably generate time series of length around 20, while
the length of WWT measurements is 550. We believe this is due
to a combination of adversarial training (not typically used for
RNNs) and our batch generation. Empirically, eliminating either
feature hurts the learned autocorrelation. TimeGAN and RCGAN,
for instance, use RNNs and adversarial training but does not batch
8 Such properties are sometimes ignored in the ML literature in favor of downstream
performance metrics; however, in systems and networking, we argue such microbench-
marks are important.
472
−1.0−0.50.00.51.0Pearson correlation0.00.20.40.60.81.0CDFDoppelGANgerRealRealDoppelGANgerRNNARHMMNaive GAN010203040500500010000RealDoppelGANger01020304050050001000015000RealRNNIMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
t
n
u
o
C
End Event Type
Figure 10: Histograms of end event types from GCUT.
DoppelGANger AR RNN HMM Naive GAN
DSL
Cable
0.68
0.74
1.34 2.33
6.57 2.46
3.46
7.98
1.14
0.87
Table 2: Wasserstein-1 distance of total bandwidth distribu-
tion of DSL and cable users. Lower is better.
[34]. We directly take the hyper-parameters from the longest se-
quence length experiment in RCGAN’s code [34], but other fine-
tuned hyper-parameters might give better results. Note that unlike
RCGAN, DG is able to achieve good results in our experiments with-
out tuning the numbers of generator and discriminator updates.
Metadata distribution: Learning correct metadata distributions is
necessary for learning measurement-metadata correlations. As men-
tioned in §5.1.2, for our HMM, AR, and RNN baselines, metadata
are randomly drawn from the multinomial distribution on training
data because there is no clear way to jointly generate metadata
and measurements. Hence, they trivially learn a perfect metadata
distribution. Figure 10 shows that DG is also able to mimic the
real distribution of end event type distribution in GCUT dataset,
while naive GANs miss a category entirely; this appears to be due
to mode collapse, which we mitigate with our second discriminator.
Results on other datasets are in Appendix C.
Measurement-metadata correlations: Although our HMM, AR, and
RNN baselines learn perfect metadata distributions, it is substan-
tially more challenging (and important) to learn the joint metadata-
measurement distribution. To illustrate this, we compute the CDF
of total bandwidth for DSL and cable users in MBA dataset. Table 2
shows the Wasserstein-1 distance between the generated CDFs and
the ground truth,9 showing that DG is closest to the real distribution.
CDF figures are attached in Appendix C.
DG does not overfit: In the context of GANs, overfitting is equiva-
lent to memorizing the training data, which is a common concern
with GANs [7, 88]. To evaluate this, we ran an experiment inspired
by the methodology of [7]: for a given generated DG sample, we
find its nearest samples in the training data. We observe signifi-
cant differences (both in square error and qualitatively) between
the generated samples and the nearest neighbors on all datasets,
suggesting that DG is not memorizing. Examples can be found in
Appendix C.
9 Wasserstein-1 distance is the integrated absolute error between 2 CDFs.
Figure 11: Mean square error of autocorrelation of the
daily page views v.s. number of training samples for WWT
dataset. For each training set size, 5 independent runs are
executed and their MSE are plotted in the figure. The line
connects the median MSE of the 5 independent runs.
Method
RNN
AR
HMM
Naive GAN
TimeGAN
RCGAN
MarketSimulator
DoppelGANger
DoppelGANger (500 training samples)
MSE
0.1222
0.2777
0.6030
0.0190
0.2384
0.0103
0.0324
0.0009
0.0024
Table 3: Mean square error (MSE) of autocorrelation of the
daily page views for WWT dataset (i.e. quantitative presen-
tation of Figure 1). Each model is trained with multiple in-
dependent runs, and the median MSE among the runs is pre-
sented. Except the last row, all models are trained with 50000
training samples.
Resource costs: DG has two main costs: training data and training
time/computation. In Figure 11, we plot the mean square error (MSE)
between the generated samples’ autocorrelations and the real data’s
autocorrelations on the WWT dataset as a function of training set
size. MSE is sensitive to training set size—it decreases by 60% as
the training data grows by 2 orders of magnitude. However, Table 3
shows that DG trained on 500 data points (the size that gives DG the
worst performance) still outperforms all baselines trained on 50,000
samples in autocorrelation MSE. Figure 11 also illustrates variability
between models; due to GAN training instability, different GAN
models with the same hyperaparameters can have different fidelity
metrics. Such training failures can typically be detected early in
the training proccess.
With regards to training time, Table 4 lists the training time
for DG and other baselines. All models were trained on a single
NVIDIA Tesla V100 GPU with 16GB GPU memory and an Intel
Xeon Gold 6148 CPU with 24GB RAM. These implementations have
not been optimized for performance at all, but we find that on the
WWT dataset, DG requires 17 hours on average to train, which is
3.4× slower than the fastest benchmark (Naive GAN) and 15.2×
faster than the slowest benchmark (TimeGAN).
Predictive modeling: Given time series measurements, users may