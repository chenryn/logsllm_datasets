f the test run is repeated with the same random seed so that the task kills are in the same order, there is ialization between the kills and the fake user traffic. Therefore, there’s no guarantee that the actual pre‐viously observed code path will now be exercised again.
Chapter 17: Testing for ReliabilityEngineers who use the testing infrastructure want to know if their code—usually a tiny fraction of all the source behind a given test run—is broken. Often, not being broken implies that any observed failures can be blamed on someone else’s code. In other words, the engineer wants to know if their code has an unanticipated race con‐dition that makes the test flaky (or more flaky than the test already was due to other factors).Testing Deadlines
Most tests are simple, in the sense that they run as a self-contained hermetic binary that fits in a small compute container for a few seconds. These tests give engineers interactive feedback about mistakes before the engineer switches context to the next bug or task.Tests that require orchestration across many binaries and/or across a fleet that has many containers tend to have startup times measured in seconds. Such tests are usu‐ally unable to offer interactive feedback, so they can be classified as batch tests. Instead of saying “don’t close the editor tab” to the engineer, these test failures are say‐ing “this code is not ready for review” to the code reviewer.The informal deadline for the test is the point at which the engineer makes the next context switch. Test results are best given to the engineer before he or she switches context, because otherwise the next context may involve XKCD compiling.14Suppose an engineer is working on a service with over 21,000 simple tests and occa‐sionally proposes a patch against the service’s codebase. To test the patch, you want to compare the vector of pass/fail results from the codebase before the patch with the vector of results from the codebase after the patch. A favorable comparison of those two vectors provisionally qualifies the codebase as releasable. This qualification cre‐ates an incentive to run the many release and integration tests, as well as other dis‐tributed binary tests that examine scaling of the system (in case the patch uses significantly more local compute resources) and complexity (in case the patch creates a superlinear workload elsewhere).At what rate can you incorrectly flag a user’s patch as damaging by miscalculating environmental flakiness? It seems likely that users would vehemently complain if 1 in 10 patches is rejected. But a rejection of 1 patch among 100 perfect patches might go without comment.
14 See .
Testing at Scale  |  19714 See .
Testing at Scale  |  197
This means you’re interested in the 42,000th root (one for each defined test before the patch, and one for each defined test after the patch) of 0.99 (the fraction of patches that can be rejected). This calculation:
0 . 99 1 
2 × 21000
suggests that those individual tests must run correctly over 99.9999% of the time. Hmm.
Pushing to ProductionWhile production configuration management is commonly kept in a source control repository, configuration is often separate from the developer source code. Similarly, the software testing infrastructure often can’t see production configuration. Even if the two are located in the same repository, changes for configuration management are made in branches and/or a segregated directory tree that test automation has his‐torically ignored.In a legacy corporate environment where software engineers develop binaries and throw them over the wall to the administrators who update the servers, segregation of testing infrastructure and production configuration is at best annoying, and at worst can damage reliability and agility. Such segregation might also lead to tool duplica‐tion. In a nominally integrated Ops environment, this segregation degrades resiliency because it creates subtle inconsistencies between the behavior for the two sets of tools. This segregation also limits project velocity because of commit races between the ver‐sioning systems.In the SRE model, the impact of segregating testing infrastructure from production configuration is appreciably worse, as it prevents relating the model describing pro‐duction to the model describing the application behavior. This discrepancy impacts engineers who want to find statistical inconsistencies in expectations at development time. However, this segregation doesn’t slow down development so much as prevent the system architecture from changing, because there is no way to eliminate migra‐tion risk.Consider a scenario of unified versioning and unified testing, so that the SRE meth‐odology is applicable. What impact would the failure of a distributed architecture migration have? A fair amount of testing will probably occur. So far, it’s assumed that a software engineer would likely accept the test system giving the wrong answer 1 time in 10 or so. What risk are you willing to take with the migration if you know that testing may return a false negative and the situation could become really exciting, really quickly? Clearly, some areas of test coverage need a higher level of paranoia198  |  Chapter 17: Testing for Reliability
than others. This distinction can be generalized: some test failures are indicative of a larger impact risk than other test failures.
Expect Testing FailNot too long ago, a software product might have released once per year. Its binaries were generated by a compiler toolchain over many hours or days, and most of the testing was performed by humans against manually written instructions. This release process was inefficient, but there was little need to automate it. The release effort was dominated by documentation, data migration, user retraining, and other factors. Mean Time Between Failure (MTBF) for those releases was one year, no matter how much testing took place. So many changes happened per release that some user-visible breakage was bound to be hiding in the software. Effectively, the reliability data from the previous release was irrelevant for the next release.Effective API/ABI management tools and interpreted languages that scale to large amounts of code now support building and executing a new software version every few minutes. In principle, a sufficiently large army of humans15 could complete test‐ing on each new version using the methods described earlier and achieve the same quality bar for each incremental version. Even though ultimately only the same tests are applied to the same code, that final software version has higher quality in the resulting release that ships annually. This is because in addition to the annual ver‐sions, the intermediate versions of the code are also being tested. Using intermediates, you can unambiguously map problems found during testing back to their underlying causes and be confident that the whole issue, and not just the limited symptom that was exposed, is fixed. This principle of a shorter feedback cycle is equally effective when applied to automated test coverage.If you let users try more versions of the software during the year, the MTBF suffers because there are more opportunities for user-visible breakage. However, you can also discover areas that would benefit from additional test coverage. If these tests are implemented, each improvement protects against some future failure. Careful relia‐bility management combines the limits on uncertainty due to test coverage with the limits on user-visible faults in order to adjust the release cadence. This combination maximizes the knowledge that you gain from operations and end users. These gains drive test coverage and, in turn, product release velocity.If an SRE modifies a configuration file or adjusts an automation tool’s strategy (as opposed to implementing a user feature), the engineering work matches the same conceptual model. When you are defining a release cadence based on reliability, it often makes sense to segment the reliability budget by functionality, or (more con‐
15 Perhaps acquired through Mechanical Turk or similar services.Testing at Scale  |  199
veniently) by team. In such a scenario, the feature engineering team aims to achieve a given uncertainty limit that affects their goal release cadence. The SRE team has a separate budget with its own associated uncertainty, and thus an upper limit on their release rate.In order to remain reliable and to avoid scaling the number of SREs supporting a ser‐vice linearly, the production environment has to run mostly unattended. To remain unattended, the environment must be resilient against minor faults. When a major event that demands manual SRE intervention occurs, the tools used by SRE must be suitably tested. Otherwise, that intervention decreases confidence that historical data is applicable to the near future. The reduction in confidence requires waiting for an analysis of monitoring data in order to eliminate the uncertainty incurred. Whereas the previous discussion in “Testing Scalable Tools” on page 193 focused on how to meet the opportunity of test coverage for an SRE tool, here you see that testing deter‐mines how often it is appropriate to use that tool against production.Configuration files generally exist because changing the configuration is faster than rebuilding a tool. This low latency is often a factor in keeping MTTR low. However, these same files are also changed frequently for reasons that don’t need that reduced latency. When viewed from the point of view of reliability:• A configuration file that exists to keep MTTR low, and is only modified when there’s a failure, has a release cadence slower than the MTBF. There can be a fair amount of uncertainty as to whether a given manual edit is actually truly optimal without the edit impacting the overall site reliability.• A configuration file that changes more than once per user-facing application release (for example, because it holds release state) can be a major risk if these changes are not treated the same as application releases. If testing and monitor‐ing coverage of that configuration file is not considerably better than that of the user application, that file will dominate site reliability in a negative way.One method of handling configuration files is to make sure that every configuration file is categorized under only one of the options in the preceding bulleted list, and to somehow enforce that rule. Should you take the latter strategy, make sure of the following:
• Each configuration file has enough test coverage to support regular routine 	editing.
• Before releases, file edits are somewhat delayed while waiting for release testing.• Provide a break-glass mechanism to push the file live before completing the test‐ing. Since breaking the glass impairs reliability, it’s generally a good idea to make the break noisy by (for example) filing a bug requesting a more robust resolution for next time.
200  |  Chapter 17: Testing for Reliability
Break-Glass and TestingYou can implement a break-glass mechanism to disable release testing. Doing so means that whoever makes a hurried manual edit isn’t told about any mistakes until the real user impact is reported by monitoring. It’s better to leave the tests running, associate the early push event with the pending testing event, and (as soon as possi‐ble) back-annotate the push with any broken tests. This way, a flawed manual push can be quickly followed by another (hopefully less flawed) manual push. Ideally, that break-glass mechanism automatically boosts the priority of those release tests so that they can preempt the routine incremental validation and coverage workload that the test infrastructure is already processing.Integration
In addition to unit testing a configuration file to mitigate its risk to reliability, it’s also important to consider integration testing configuration files. The contents of the con‐figuration file are (for testing purposes) potentially hostile content to the interpreter reading the configuration. Interpreted languages such as Python are commonly used for configuration files because their interpreters can be embedded, and some simple sandboxing is available to protect against nonmalicious coding errors.Writing your configuration files in an interpreted language is risky, as this approach is fraught with latent failures that are hard to definitively address. Because loading con‐tent actually consists of executing a program, there’s no inherent upper limit on how inefficient loading can be. In addition to any other testing, you should pair this type of integration testing with careful deadline checking on all integration test methods in order to label tests that do not run to completion in a reasonable amount of time as failed.If the configuration is instead written as text in a custom syntax, every category of test needs separate coverage from scratch. Using an existing syntax such as YAML in combination with a heavily tested parser like Python’s safe_load removes some of the toil incurred by the configuration file. Careful choice of syntax and parser can ensure there’s a hard upper limit on how long the loading operation can take. How‐ever, the implementer needs to address schema faults, and most simple strategies for doing so don’t have an upper bound on runtime. Even worse, these strategies tend not to be robustly unit tested.Testing at Scale  |  201
The benefit of using protocol buffers16 is that the schema is defined in advance and automatically checked at load time, removing even more of the toil, yet still offering the bounded runtime.The role of SRE generally includes writing systems engineering tools17 (if no one else is already writing them) and adding robust validation with test coverage. All tools can behave unexpectedly due to bugs not caught by testing, so defense in depth is advisa‐ble. When one tool behaves unexpectedly, engineers need to be as confident as possi‐ble that most of their other tools are working correctly and can therefore mitigate or resolve the side effects of that misbehavior. A key element of delivering site reliability is finding each anticipated form of misbehavior and making sure that some test (or another tool’s tested input validator) reports that misbehavior. The tool that finds the problem might not be able to fix or even stop it, but should at least report the prob‐lem before a catastrophic outage occurs.For example, consider the configured list of all users (such as /etc/passwd on a non-networked Unix-style machine) and imagine an edit that unintentionally causes the parser to stop after parsing only half of the file. Because recently created users haven’t loaded, the machine will most likely continue to run without problem, and many users may not notice the fault. The tool that maintains home directories can easily notice the mismatch between the actual directories present and those implied by the (partial) user list and urgently report the discrepancy. This tool’s value lies in report‐ing the problem, and it should avoid attempting to remediate on its own (by deleting lots of user data).Production Probes
Given that testing specifies acceptable behavior in the face of known data, while mon‐itoring confirms acceptable behavior in the face of unknown user data, it would seem that major sources of risk—both the known and the unknown—are covered by the combination of testing and monitoring. Unfortunately, actual risk is more complicated.Known good requests should work, while known bad requests should error. Imple‐menting both kinds of coverage as an integration test is generally a good idea. You can replay the same bank of test requests as a release test. Splitting the known good requests into those that can be replayed against production and those that can’t yields three sets of requests:
16 See .16 See .
17 Not because software engineers shouldn’t write them. Tools that cross between technology verticals and span abstraction layers tend to have weak associations with many software teams and a slightly stronger association with systems teams.
202  |  Chapter 17: Testing for Reliability
• Known bad requests
• Known good requests that can be replayed against production• Known good requests that can’t be replayed against production
You can use each set as both integration and release tests. Most of these tests can also be used as monitoring probes.
It would seem to be superfluous and, in principle, pointless to deploy such monitor‐ing because these exact same requests have already been tried two other ways. How‐ever, those two ways were different for a few reasons:• The release test probably wrapped the integrated server with a frontend and a 	fake backend.
• The probe test probably wrapped the release binary with a load balancing 	frontend and a separate scalable persistent backend.
• Frontends and backends probably have independent release cycles. It’s likely that the schedules for those cycles occur at different rates (due to their adaptive release cadences).Therefore, the monitoring probe running in production is a configuration that wasn’t previously tested.
Those probes should never fail, but what does it mean if they do fail? Either the frontend API (from the load balancer) or the backend API (to the persistent store) is not equivalent between the production and release environments. Unless you already know why the production and release environments aren’t equivalent, the site is likely broken.The same production updater that gradually replaces the application also gradually replaces the probes so that all four combinations of old-or-new probes sending requests to old-or-new applications are being continuously generated. That updater can detect when one of the four combinations is generating errors and roll back to the last known good state. Usually, the updater expects each newly started application instance to be unhealthy for a short time as it prepares to start receiving lots of user traffic. If the probes are already inspected as part of the readiness check, the update safely fails indefinitely, and no user traffic is ever routed to the new version. The update remains paused until engineers have time and inclination to diagnose the fault condition and then encourage the production updater to cleanly roll back.This production test by probe does indeed offer protection to the site, plus clear feed‐back to the engineers. The earlier that feedback is given to engineers, the more useful it is. It’s also preferable that the test is automated so that the delivery of warnings to engineers is scalable.
Testing at Scale  |  203Assume that each component has the older software version that’s being replaced and the newer version that’s rolling out (now or very soon). The newer version might be talking to the old version’s peer, which forces it to use the deprecated API. Or the older version might be talking to a peer’s newer version, using the API which (at the time the older version was released) didn’t work properly yet. But it works now, hon‐est! You’d better hope those tests for future compatibility (which are running as moni‐toring probes) had good API coverage.Fake Backend Versions
When implementing release tests, the fake backend is often maintained by the peer service’s engineering team and merely referenced as a build dependency. The her‐metic test that is executed by the testing infrastructure always combines the fake backend and the test frontend at the same build point in the revision control history.That build dependency may be providing a runnable hermetic binary and, ideally, the engineering team maintaining it cuts a release of that fake backend binary at the same time they cut their main backend application and their probes. If that backend release is available, it might be worthwhile to include hermetic frontend release tests (without the fake backend binary) in the frontend release package.Your monitoring should be aware of all release versions on both sides of a given ser‐vice interface between two peers. This setup ensures that retrieving every combina‐tion of the two releases and determining whether the test still passes doesn’t take much extra configuration. This monitoring doesn’t have to happen continuously—you only need to run new combinations that are the result of either team cutting a new release. Such problems don’t have to block that new release itself.On the other hand, rollout automation should ideally block the associated production rollout until the problematic combinations are no longer possible. Similarly, the peer team’s automation may consider draining (and upgrading) the replicas that haven’t yet moved from a problematic combination.
ConclusionTesting is one of the most profitable investments engineers can make to improve the reliability of their product. Testing isn’t an activity that happens once or twice in the lifecycle of a project; it’s continuous. The amount of effort required to write good tests is substantial, as is the effort to build and maintain infrastructure that promotes a strong testing culture. You can’t fix a problem until you understand it, and in engi‐neering, you can only understand a problem by measuring it. The methodologies and techniques in this chapter provide a solid foundation for measuring faults and uncer‐tainty in a software system, and help engineers reason about the reliability of software as it’s written and released to users.204  |  Chapter 17: Testing for Reliability
CHAPTER 18
Software Engineering in SRE
Written by Dave Helstroom and Trisha Weir with Evan Leonard and Kurt Delimon 
Edited by Kavita Guliani
Ask someone to name a Google software engineering effort and they’ll likely list a consumer-facing product like Gmail or Maps; some might even mention underlying infrastructure such as Bigtable or Colossus. But in truth, there is a massive amount of behind-the-scenes software engineering that consumers never see. A number of those products are developed within SRE.Google’s production environment is—by some measures—one of the most complex machines humanity has ever built. SREs have firsthand experience with the intrica‐cies of production, making them uniquely well suited to develop the appropriate tools to solve internal problems and use cases related to keeping production running. The majority of these tools are related to the overall directive of maintaining uptime and keeping latency low, but take many forms: examples include binary rollout mecha‐nisms, monitoring, or a development environment built on dynamic server composi‐tion. Overall, these SRE-developed tools are full-fledged software engineering projects, distinct from one-off solutions and quick hacks, and the SREs who develop them have adopted a product-based mindset that takes both internal customers and a roadmap for future plans into account.Why Is Software Engineering Within SRE Important?
In many ways, the vast scale of Google production has necessitated internal software development, because few third-party tools are designed at sufficient scale for Goo‐gle’s needs. The company’s history of successful software projects has led us to appre‐ciate the benefits of developing directly within SRE.
205205
SREs are in a unique position to effectively develop internal software for a number of reasons:
• The breadth and depth of Google-specific production knowledge within the SRE organization allows its engineers to design and create software with the appropri‐ate considerations for dimensions such as scalability, graceful degradation during failure, and the ability to easily interface with other infrastructure or tools.• Because SREs are embedded in the subject matter, they easily understand the 	needs and requirements of the tool being developed.
• A direct relationship with the intended user—fellow SREs—results in frank and high-signal user feedback. Releasing a tool to an internal audience with high familiarity with the problem space means that a development team can launch and iterate more quickly. Internal users are typically more understanding when it comes to minimal UI and other alpha product issues.From a purely pragmatic standpoint, Google clearly benefits from having engineers with SRE experience developing software. By deliberate design, the growth rate of SRE-supported services exceeds the growth rate of the SRE organization; one of SRE’s guiding principles is that “team size should not scale directly with service growth.”Achieving linear team growth in the face of exponential service growth requires per‐petual automation work and efforts to streamline tools, processes, and other aspects of a service that introduce inefficiency into the day-to-day operation of production. Having the people with direct experience running production systems developing the tools that will ultimately contribute to uptime and latency goals makes a lot of sense.On the flip side, individual SREs, as well as the broader SRE organization, also benefit from SRE-driven software development.
Fully fledged software development projects within SRE provide career development opportunities for SREs, as well as an outlet for engineers who don’t want their coding skills to get rusty. Long-term project work provides much-needed balance to inter‐rupts and on-call work, and can provide job satisfaction for engineers who want their careers to maintain a balance between software engineering and systems engineering.Beyond the design of automation tools and other efforts to reduce the workload for engineers in SRE, software development projects can further benefit the SRE organi‐zation by attracting and helping to retain engineers with a broad variety of skills. The desirability of team diversity is doubly true for SRE, where a variety of backgrounds and problem-solving approaches can help prevent blind spots. To this end, Google always strives to staff its SRE teams with a mix of engineers with traditional software development experience and engineers with systems engineering experience.206  |  Chapter 18: Software Engineering in SRE
Auxon Case Study: Project Background and Problem Space
This case study examines Auxon, a powerful tool developed within SRE to automate capacity planning for services running in Google production. To best understand how Auxon was conceived and the problems it addresses, we’ll first examine the problem space associated with capacity planning, and the difficulties that traditional approaches to this task present for services at Google and across the industry as a whole. For more context on how Google uses the terms service and cluster, see Chap‐ter 2.Traditional Capacity Planning
There are myriad tactics for capacity planning of compute resources (see [Hix15a]), but the majority of these approaches boil down to a cycle that can be approximated as follows:
1) Collect demand forecasts.
How many resources are needed? When and where are these resources needed?
• Uses the best data we have available today to plan into the future• Typically covers anywhere from several quarters to years
2) Devise build and allocation plans.
Given this forecasted outlook, what’s the best way to meet this demand with additional supply of resources? How much supply, and in what locations?
3) Review and sign off on plan.
Is the forecast reasonable? Does the plan line up with budgetary, product-level, and technical considerations?4) Deploy and configure resources.
Once resources eventually arrive (potentially in phases over the course of some defined period of time), which services get to use the resources? How do I make typically lower-level resources (CPU, disk, etc.) useful for services?It bears stressing that capacity planning is a neverending cycle: assumptions change, deployments slip, and budgets are cut, resulting in revision upon revision of The Plan. And each revision has trickle-down effects that must propagate throughout the plans of all subsequent quarters. For example, a shortfall this quarter must be made up in future quarters. Traditional capacity planning uses demand as a key driver, and manually shapes supply to fit demand in response to each change.Auxon Case Study: Project Background and Problem Space  |  207
Brittle by nature
Traditional capacity planning produces a resource allocation plan that can be disrup‐ted by any seemingly minor change. For example:
• A service undergoes a decrease in efficiency, and needs more resources than 	expected to serve the same demand.
• Customer adoption rates increase, resulting in an increase in projected demand.• The delivery date for a new cluster of compute resources slips.
• A product decision about a performance goal changes the shape of the required service deployment (the service’s footprint) and the amount of required resources.Minor changes require cross-checking the entire allocation plan to make sure that the plan is still feasible; larger changes (such as delayed resource delivery or product strategy changes) potentially require re-creating the plan from scratch. A delivery slippage in a single cluster might impact the redundancy or latency requirements of multiple services: resource allocations in other clusters must be increased to make up for the slippage, and these and any other changes would have to propagate through‐out the plan.Also, consider that the capacity plan for any given quarter (or other time frame) is based on the expected outcome of the capacity plans of previous quarters, meaning that a change in any one quarter results in work to update subsequent quarters.
Laborious and impreciseFor many teams, the process of collecting the data necessary to generate demand forecasts is slow and error-prone. And when it is time to find capacity to meet this future demand, not all resources are equally suitable. For example, if latency require‐ments mean that a service must commit to serve user demand on the same continent as the user, obtaining additional resources in North America won’t alleviate a capacity shortfall in Asia. Every forecast has constraints, or parameters around how it can be fulfilled; constraints are fundamentally related to intent, which is discussed in the next section.Mapping constrained resource requests into allocations of actual resources from the available capacity is equally slow: it’s both complex and tedious to bin pack requests into limited space by hand, or to find solutions that fit a limited budget.This process may already paint a grim picture, but to make matters worse, the tools it requires are typically unreliable or cumbersome. Spreadsheets suffer severely from scalability problems and have limited error-checking abilities. Data becomes stale, and tracking changes becomes difficult. Teams often are forced to make simplifying
208  |  Chapter 18: Software Engineering in SREassumptions and reduce the complexity of their requirements, simply to render maintaining adequate capacity a tractable problem.When service owners face the challenges of fitting a series of requests for capacity from various services into the resources available to them, in a manner that meets the various constraints a service may have, additional imprecision ensues. Bin packing is an NP-hard problem that is difficult for human beings to compute by hand. Further‐more, the capacity request from a service is generally an inflexible set of demand requirements: X cores in cluster Y. The reasons why X cores or Y cluster are needed, and any degrees of freedom around those parameters, are long lost by the time the request reaches a human trying to fit a list of demands into available supply.The net result is a massive expenditure of human effort to come up with a bin pack‐ing that is approximate, at best. The process is brittle to change, and there are no known bounds on an optimal solution.
Our Solution: Intent-Based Capacity Planning
Specify the requirements, not the implementation.At Google, many teams have moved to an approach we call Intent-based Capacity Planning. The basic premise of this approach is to programmatically encode the dependencies and parameters (intent) of a service’s needs, and use that encoding to autogenerate an allocation plan that details which resources go to which service, in which cluster. If demand, supply, or service requirements change, we can simply autogenerate a new plan in response to the changed parameters, which is now the new best distribution of resources.With a service’s true requirements and flexibility captured, the capacity plan is now dramatically more nimble in the face of change, and we can reach an optimal solution that meets as many parameters as possible. With bin packing delegated to computers, human toil is drastically reduced, and service owners can focus on high-order priori‐ties like SLOs, production dependencies, and service infrastructure requirements, as opposed to low-level scrounging for resources.As an added benefit, using computational optimization to map from intent to imple‐mentation achieves much greater precision, ultimately resulting in cost savings to the organization. Bin packing is still far from a solved problem, because certain types are still considered NP-hard; however, today’s algorithms can solve to a known optimal solution.
Intent-Based Capacity PlanningIntent-Based Capacity Planning
Intent is the rationale for how a service owner wants to run their service. Moving from concrete resource demands to motivating reasons in order to arrive at the true
Intent-Based Capacity Planning  |  209
capacity planning intent often requires several layers of abstraction. Consider the fol‐lowing chain of abstraction:
1) “I want 50 cores in clusters X, Y, and Z for service Foo.”This is an explicit resource request. But…why do we need this many resources specifically in these particular clusters?
2) “I want a 50-core footprint in any 3 clusters in geographic region YYY for service Foo.”This request introduces more degrees of freedom and is potentially easier to ful‐fill, although it doesn’t explain the reasoning behind its requirements. But…why do we need this quantity of resources, and why 3 footprints?3) “I want to meet service Foo’s demand in each geographic region, and have N + 2 redundancy.”
	Suddenly greater flexibility is introduced and we can understand at a more	“human” level what happens if service Foo does not receive these resources. 	But…why do we need N + 2 for service Foo?
4) “I want to run service Foo at 5 nines of reliability.”This is a more abstract requirement, and the ramification if the requirement isn’t met becomes clear: reliability will suffer. And we have even greater flexibility here: perhaps running at N + 2 is not actually sufficient or optimal for this ser‐vice, and some other deployment plan would be more suitable.So what level of intent should be used by intent-driven capacity planning? Ideally, all levels of intent should be supported together, with services benefiting the more they shift to specifying intent versus implementation. In Google’s experience, services tend to achieve the best wins as they cross to step 3: good degrees of flexibility are avail‐able, and the ramifications of this request are in higher-level and understandable terms. Particularly sophisticated services may aim for step 4.Precursors to Intent
What information do we need in order to capture a service’s intent? Enter dependen‐cies, performance metrics, and prioritization.
DependenciesServices at Google depend on many other infrastructure and user-facing services, and these dependencies heavily influence where a service can be placed. For example, imagine user-facing service Foo, which depends upon Bar, an infrastructure storage service. Foo expresses a requirement that Bar must be located within 30 milliseconds of network latency of Foo. This requirement has important repercussions for where we place both Foo and Bar, and intent-driven capacity planning must take these con‐straints into account.210  |  Chapter 18: Software Engineering in SRE
Furthermore, production dependencies are nested: to build upon the preceding example, imagine service Bar has its own dependencies on Baz, a lower-level dis‐tributed storage service, and Qux, an application management service. Therefore, where we can now place Foo depends on where we can place Bar, Baz, and Qux. A given set of production dependencies can be shared, possibly with different stipula‐tions around intent.Performance metrics
Demand for one service trickles down to result in demand for one or more other services. Understanding the chain of dependencies helps formulate the general scope of the bin packing problem, but we still need more information about expected resource usage. How many compute resources does service Foo need to serve N user queries? For every N queries of service Foo, how many Mbps of data do we expect for service Bar?Performance metrics are the glue between dependencies. They convert from one or more higher-level resource type(s) to one or more lower-level resource type(s). Deriving appropriate performance metrics for a service can involve load testing and resource usage monitoring.
Prioritization