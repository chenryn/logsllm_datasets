parted users will not be replaced. For these user nodes, the key
server changes them to null nodes (see the left ﬁgure of Figure 3
for an example). If all of the children of a node are null nodes, the
key server changes the node to null node as well. On the other hand,
if J > L, the key server ﬁrst replaces the L departed users with L
of the newly joined users. However, the key server still needs to
insert the remaining J − L new users. For insertion, three strate-
gies have been investigated to achieve different tradeoffs among the
aforementioned three objectives:
• Strategy 1.
In this strategy, to add the remaining J − L
new users, the key server ﬁrst splits the L replaced nodes
to add the remaining new users.
If splitting the newly re-
placed nodes is still not enough to add all of the remaining
new users (i.e. J > d· L), the key server splits the leaf nodes
from left to right and adds new users (see the right ﬁgure of
Figure 3 for an example). The advantage of this approach is
that it reduces the number of encrypted keys because it ﬁrst
splits the replaced user nodes. The disadvantage is that if the
user nodes of some users are changed, the key server will
need to provide new IDs individually to these users in addi-
tion to newly joined users. We notice that such notiﬁcation
will increase key server bandwidth overhead.
• Strategy 2. This strategy, which we proposed and investi-
gated in [11], achieves a smaller number of encrypted keys
than that of Strategy 1. With this strategy, the key server cre-
ates a tree with new users at its leaf nodes and grafts the tree
under a departed user node with the smallest height. This
strategy, however, does not keep the key tree as balanced as
Strategy 1. On the other hand, with this strategy, the ID of
at most one remaining user is modiﬁed; therefore, the key
server only needs to provide new IDs to at most one remain-
ing user in addition to newly joined users.
• Strategy 3. This strategy, which we proposed and investi-
gated in [26], was designed to make it efﬁcient for remaining
users to identify the encrypted keys that they need. With this
strategy, the key server ﬁrst replaces the null nodes that have
IDs between d · m + 1 and d · m + d with newly joined
users, where m is the ID of the last node in the key tree that
is neither a user node nor a null node. If there are still ex-
tra joins, starting with the user node with ID m + 1, the key
server splits a user node to add d children, moves the content
of the user node to its left-most child, and adds d − 1 new
user nodes. The key server repeats this process until all new
users are added to the key tree. A disadvantage of this strat-
egy is that it generates a slightly larger number of encrypted
keys. The advantage of this strategy, however, is that if the
key server multicasts m, the ID of the last node that is nei-
ther a user node nor a key node, in a rekey message, each
remaining user will be able to independently derive the ID
of its user node even if the structure of the key tree has been
modiﬁed. For an explanation of how each user, whose ID has
changed, determines its new ID, please see [26].
Comparing the three strategies to process the J > L case, our
evaluation shows that the difference in terms of the size of rekey
subtree is small. Therefore, we report analytical results below for
Strategy 3 only.
After updating the key tree, the key server makes a copy of the
key tree, and marks the states of key nodes in the duplicated key
tree. The nodes are marked with one of the following four states:
Unchanged, Join, Leave, and Replace.
We ﬁrst mark the states of user nodes: 1) A user node is marked
Unchanged unless it is changed by the following rules. 2) A user
node of a departed user is marked Leave if the node is not replaced;
otherwise, it is marked Replace. 3) A user node is marked Join if
it is a replacement for a null node or it is split from a previous user
node.
We then mark the states of other key nodes: 1) If all the children
of a key node are marked Leave, we mark it Leave and remove
all of its children. 2) Otherwise, if all of its children are marked
Unchanged, we mark it Unchanged, and remove all of its children.
3) Otherwise, if all of its children are marked Unchanged or Join,
we mark it as Join, create a virtual node, which contains the old key
of the key node, and use it to replace all of its Unchanged children.
4) Otherwise, if the node has at least one Leave or Replace child,
we mark it as Replace.
We call the pruned subtree rekey subtree, and we observe that
each edge in the rekey subtree corresponds to an encryption: par-
ent node encrypted by child node. The detail of how to traverse a
rekey subtree to generate a rekey message will be investigated in
Section 3.1.
The running complexity of our marking algorithm is O((J +
L) log N ). Our benchmark shows that on a Sun Ultra Sparc I with
167MHz CPU, the marking algorithm takes less than 4.5 ms for
N = 1024, and less than 10 ms for N = 4096. On the other hand,
according to our benchmark, the running time of a batch rekeying
algorithm based on boolean function minimization [4] can take tens
of seconds at similar group sizes.
2.5 Worst scenario analysis
We analyze the worst scenario and average scenario performance
of batch rekeying based upon Strategy 3.
(An analysis of batch
rekeying based upon Strategy 2 was presented in [11].) The metric
we use is the number of encrypted keys. In this subsection, we will
show that even if we consider the worst number of encrypted keys
to rekey L leave requests, assuming no joins in a batch, batch rekey-
ing can still have large beneﬁt. From our previous discussion, we
know that it is because of forward access control that makes rekey
encoding difﬁcult; therefore quantifying the beneﬁt of batch rekey-
ing under this scenario can be instructive. For results on worst case
performance of other cases, we refer the interested reader to [11].
We present the average performance in next section.
Consider a balanced tree with degree d and height h. We know
that there are N = dh leaf nodes. Suppose L of the users leave. We
observe that the worst scenario happens when the departed users are
evenly distributed on the tree leaf nodes, and therefore, the number
of overlapped encryptions is the minimum.
Without delving into the detail of analysis, assuming L = dl,
where L ≤ N/d, we derive that the worst number of encrypted
keys is:
Encworst(N, L) = Ld logd
N
L +
L − d
d − 1
(1)
On the other hand, in individual rekeying, a single departed user
costs d logd
N. Suppose the L requests are processed individu-
ally, then there will be about a total of Ld logd
N encrypted keys.
Comparing with Equation (1), we observe that the difference is
L. When L is large, the beneﬁt of batch rekeying can be
Ld logd
substantial. When L ≥ N/d, more edges in the rekey subtree will
be pruned, and the savings become even larger.
2.6 Average scenario analysis
Let Enc(N, J, L) denote the average number of encrypted keys
when J join and L leave requests are processed for an N user key
tree. To simplify the analysis, we assume that the key tree is bal-
N denote
anced at the beginning of a batch, and we let h = logd
the height of the key tree. Also, we assume that the departed users
are uniformly distributed over the tree leaf nodes. The scenario
that users have different leave probabilities can be utilized to fur-
ther improve performance, for example, by using a Huffman type
of tree to minimize the number of encrypted keys. However, such
exploration and analysis are beyond the scope of this paper.
Since our batch rekeying algorithm depends on the relationship
between J and L, our analytical results also depend on the relation-
ship between J and L. By considering the number of times that a
key node belongs to a rekey subtree, we derive the following ana-
lytical expressions for the average number of encrypted keys [25]:
• J = L:
Enc1 (N, J, L) = d
h−1
Xl=0
dl(1 − C J
N−N0
C J
N
)
where N0 = N/dl.
• J  L:
Enc3 (N, J, L) = (cid:10) d·(J−L)
d−1
h−1
l−1
d
Xi=0
Xl=0
J
(cid:11)+
(d(1 − C J
N−N0
)+
C J
N
· 1(J − L − dN1)·
N/dl+1 (cid:11) + 1})
C
N−N0
CJ
min{d,(cid:10) J−L−dN1
N
where N0 = N/dl, N1 = i · N0, 1(x) = 1 if x > 0;
otherwise, 1(x) = 0.
N=4096, d=4
computed
simulated
Enc(N, J, L)
10000
8000
6000
4000
2000
0
0
1000
2000
J
3000
4000
4000
2000
3000
L
1000
Figure 4: Enc(N, J, L) by analysis and simulation
Next, we plot our analytical results. Figure 4 shows the values of
Enc(N, J, L) for N = 4096 and a wide range of J and L values.
We have plotted both simulation results (controlled by achieving a
conﬁdence interval of 5%) and our analytical results; our analytical
results match simulations well and they are indistinguishable in the
ﬁgure. From Figure 4, we observe that for a ﬁxed L, Enc(N, J, L)
grows linearly. This is expected because in our marking algorithm,
joins replace leaves and thus the rekey subtree grows linearly with
the number of joins. For a ﬁxed J, Enc(N, J, L) ﬁrst increases
(because more leaves means more keys to be changed), then de-
creases (because now some keys can be pruned from the rekey sub-
tree).
N=4096, d=4
batch rekeying
individual rekeying
#encrypted keys
120000
80000
40000
0
0
1000
2000
J
3000
4000
4000
2000
3000
L
1000
Figure 5: Batch vs. individual rekeying
Next, using the analytical expressions above, we consider the
performance gains of batch rekeying when the average number of
encrypted keys is used as performance metric. Figure 5 compares
batch rekeying and individual rekeying for a wide range of J and
L values. From Figure 5, we observe that the difference between
batch and individual rekeying can be large. For J = 400, L = 400,
batch rekeying generates 2547 encrypted keys, while individual
rekeying generates 12000 encrypted keys, which is about 4 times
larger; for J = 0, L = 400, batch generates 2146 encrypted
keys, and individual generates 9600 encrypted keys; for J = 400,
L = 0, batch generates 583 encrypted keys, and individual gen-
erates 2400 encrypted keys. The difference becomes even larger
when J and L become larger.
3. PROVIDING
RELIABLE REKEY TRANSPORT
A rekey subtree generated by the rekey encoding component is
sent to the rekey transport component for delivery. We investigate
two issues for rekey transport:
1. What are the special characteristics of the rekey transport
workload?
2. Given the workload, how to provide reliability to the rekey
packets, and what is the performance?
3.1 Rekey transport workload
For an encoding algorithm based on key trees, we know that each
user only needs to receive the encrypted keys that are on the path
from its individual key to the new group key. To avoid the overhead
of unicasting individually to each user its encrypted keys, however,
the key server partitions the users into a small number of subgroups
(we consider one subgroup using one multicast channel in this pa-
per), combines the encrypted keys for the subgroup of users into a
rekey message, which may be partitioned into several rekey pack-
ets if the keys cannot ﬁt into one packet, and multicasts the rekey
message to all of the users in the subgroup. Instead of receiving all
of the packets in a rekey message, however, each user only needs
to receive those packets that contain its encrypted keys. As a re-
sult, the rekey packets that each user needs will depend on how
encrypted keys are assigned into rekey packets. Therefore, our in-
vestigation of rekey transport workload is to address the following
questions: 1) How to assign the encrypted keys in a rekey subtree
of a subgroup of users into packets? 2) Given the assignment al-
gorithm, how many packets a user needs to receive? What is the
average? What is the variance? and 3) How many packets are there
in a rekey message?
3.1.1 Key assignment algorithms
To improve the performance of rekey transport protocol, it is de-
sired that a key assignment algorithm reduces the number of pack-
ets zr that a user r needs to receive. Moreover, the overhead of
rekey transport also depends on the users with the largest num-
bers of packets to receive. Thus, it is desired that a key assign-
ment algorithm also reduces the variance of {zr}. Given these re-
quirements, we consider three key assignment algorithms: Breadth
First Assignment (BFA), Depth First Assignment (DFA), and Re-
cursive BFA (R-BFA). The common characteristic of these three
key assignment algorithms is that they do not duplicate encrypted
keys, that is, each encrypted key is assigned into only one packet.
In [26], we have also proposed and investigated a different algo-
rithm called User-oriented Assignment (UKA). The advantage of
the UKA algorithm is that it assigns all of the encrypted keys for a
user into the same packet, and therefore each user needs to receive
only one packet, that is, zr = 1 for all receivers. The disadvan-
tage of this algorithm, however, is that some encrypted keys are
duplicated into several packets, and such duplications can domi-
nate bandwidth overhead, especially when MTU is small or when
receiver loss rates are low.
For BFA and DFA, the key server traverses a rekey subtree using
either breadth ﬁrst or depth ﬁrst order, and assigns sequentially the
encrypted keys into packets. By horizontally scanning a rekey sub-
tree, BFA collects keys from different users in a round-robin man-
ner. This “fairness” for each user reduces the variance of {zr}. On
the other hand, BFA spreads the keys of a user into multiple pack-
ets, and increases the average of {zr}. By vertically tracing along
a path, DFA ﬁrst collects the keys for one user, and then goes to the
next user. Thus, we expect that the average of {zr} is smaller for
DFA. However, since the shared encrypted keys are assigned to the
users processed earlier, such bias causes larger variance of {zr}.
To gain the beneﬁts of both BFA and DFA, we consider R-BFA.
Figure 6 shows our R-BFA algorithm. This algorithm starts by call-
ing R-BFA(root), where root is the root node of a rekey subtree.
Algorithm R-BFA (node id)
✄ node id uniquely identiﬁes a node in rekey subtree.
✄ pkt is a global variable, denoting a rekey packet.
✄ family(i) is the set containing i and its immediate children.
1. Q ← create a local FIFO queue
2. put node id into Q
3. while (Q is not empty)
i ← pop the head element head(Q)
if pkt has the space to contain family(node id)
then put all the children of i into Q sequentially
else pkt ← generate a new rekey packet
put all the encrypted keys of family(i) into pkt
call R-BFA (i)
while (Q is not empty)
i ← pop head(Q)
call R-BFA (i)
Figure 6: Recursive BFA (R-BFA) algorithm
To better understand R-BFA, we compare its bahavior with that
of BFA. When there is still space in the current packet, R-BFA
behaves just like BFA, and thus has performance in terms of vari-
ance similar to that of BFA. However, when the current packet is
full and a new packet is created, instead of continuing horizontally
scanning on the global rekey subtree (as BFA will do), R-BFA does
BFA within a local subtree. Thus, R-BFA puts more related keys
together and reduces the average value of {zr} compared with that
of BFA. Figure 7 illustrates the basic idea of R-BFA.
P1
P2
P3
P4
P5
Figure 7: An illustration of the R-BFA algorithm
3.1.2 Comparison of assignment algorithms
We next verify that compared with BFA and DFA, R-BFA per-
forms well in terms of both the average and the variance of {zr}.
First, we consider the average value of {zr}. Figure 8 plots
the results of BFA, DFA and R-BFA for rekey subtrees with 2048