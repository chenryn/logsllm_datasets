c
A
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
(a) Cora
100
80
60
40
20
0
100
80
60
40
20
0
100
80
60
40
20
0
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
N
C
G
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
T
A
G
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
E
G
A
S
h
p
a
r
G
)
%
(
y
c
a
r
u
c
c
A
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
(b) Pubmed
100
80
60
40
20
0
100
80
60
40
20
0
100
80
60
40
20
0
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
N
C
G
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
T
A
G
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
E
G
A
S
h
p
a
r
G
)
%
(
y
c
a
r
u
c
c
A
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
(c) Facebook
100
80
60
40
20
0
100
80
60
40
20
0
100
80
60
40
20
0
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
𝜖𝑦 = 1
𝜖𝑦 = 2
𝜖𝑦 = 3
𝜖𝑦 = ∞
0.01 0.1 1.0 2.0 3.0 ∞
𝜖𝑥
(d) LastFM
Figure 3: Comparison of LPGNN’s performance with different GNN model under varying feature and label privacy budgets.
sensitivity to the features, and thus it degrades more than the other
two models when the features are highly noisy. On the contrary,
a model like GCN only uses node features in the GCN aggregator
function (Eq. 12) and thus can better tolerate the noisy features.
GraphSAGE averages neighboring node features and then appends
the self feature vector to the aggregation, and therefore it is not as
dependent as GAT on the features. Nevertheless, GAT could also
achieve comparable results for 𝜖𝑥 ≥ 1 on all the datasets.
Analyzing the multi-bit mechanism. In Table 2, we com-
pared the performance of our multi-bit mechanism (denoted as
MB) against 1-bit mechanism (1B), Laplace mechanism (LP), and
Analytic Gaussian mechanism (AG) [5]. The 1-bit mechanism [11],
is obtained by setting 𝑚 = 𝑑 in Algorithm 1. The Laplace and Gauss-
ian mechanisms are two classic mechanisms that respectively add
a zero-mean Laplace and Gaussian noise to the data with a noise
variance calibrated based on the privacy budget, and are widely
used for both single value and multidimensional data perturbation.
Note that the Gaussian mechanism satisfies a relaxed version of
𝜖-LDP , namely (𝜖, 𝛿)-LDP, which (loosely speaking) means that
it satisfies 𝜖-LDP with probability at least 1 − 𝛿 for 𝛿 > 0. Here,
we use the Analytic Gaussian mechanism [5], the optimized ver-
sion of the standard Gaussian mechanism, with 𝛿 = 1−10. As all
these mechanisms are used for feature perturbation, we set the
label privacy budget 𝜖𝑦 = ∞ and only consider their performance
under different 𝜖𝑥 ∈ {0.01, 0.1, 1, 2}. According to the results, our
multi-bit mechanism consistently outperforms the other mecha-
nisms in classification accuracy almost in all cases, especially under
smaller privacy budgets. For instance, at 𝜖𝑥 = 0.01, MB performs
over 8%, 2%, 7%, and 12% better than the second-best mechanism
AG on Cora, Pubmed, Facebook, and LastFM, respectively. This is
mainly because the variance of our optimized multi-bit mechanism
is lower than the other three, resulting in a more accurate estima-
tion. Simultaneously, our mechanism is also efficient in terms of the
communication overhead, requiring only two bits per feature. In
contrast, the Gaussian mechanism’s output is real-valued, usually
taking 32 bits per feature (more or less, depending on the precision)
to transmit a floating-point number.
To verify that using node features in a privacy-preserving man-
ner has an added value in practice, in Table 3, we compare our
multi-bit features with several ad-hoc feature vectors that can be
used instead of the private features to train the GNN without any
additional privacy cost. Ones is the all-one feature vector, Ohd is
the one-hot encoding of the node’s degree, as in [59], and Rnd is
randomly initialized node features between 0 and 1. To have a fair
comparison, we set the feature dimension of all the methods equal
to the private features. We set 𝜖𝑦 = 1 and compare the LPGNN’s
result with multi-bit encoded features under 𝜖𝑥 ∈ {0.01, 0.1, 1}. We
observe that LPGNN, with the multi-bit mechanism, even under
the minimum privacy budget of 0.01, significantly outperforms the
ad-hoc baselines in all cases, with an improvement ranging from
around 7% on Facebook to over 20% on Pubmed comparing to the
best performing ad-hoc baseline. Note that even though perturbed
features under very small 𝜖𝑥 are noisier and becomes similar to Rnd,
with the help of KProp, the resulting aggregation could estimate
– even if poorly – the true aggregation, which might be enough
for the GNN to distinguish between different neighborhoods. But
in the case of Rnd, the aggregations carry no information about
neighborhoods as the features are random, so the accuracy is worse
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2138Table 2: Accuracy of LPGNN with different LDP mechanisms
(𝜖𝑦 = ∞)
Table 3: Accuracy of LPGNN with different features
(𝜖𝑦 = 1)
Dataset Mech.
Cora
Pubmed
1b
lp
ag
mb
1b
lp
ag
mb
Facebook 1b
lp
ag
mb
LastFm
1b
lp
ag
mb
𝜖𝑥 = 0.01
45.8 ± 3.3
43.2 ± 3.1
59.7 ± 2.3
68.0 ± 2.9
76.2 ± 0.6
76.6 ± 0.5
76.4 ± 0.6
78.9 ± 0.7
57.0 ± 3.4
54.2 ± 2.9
78.2 ± 1.4
85.8 ± 0.4
40.5 ± 7.4
43.4 ± 5.7
63.6 ± 2.4
75.6 ± 1.6
𝜖𝑥 = 0.1
62.3 ± 1.5
57.8 ± 2.3
62.7 ± 2.8
64.6 ± 3.2
74.8 ± 0.7
75.2 ± 1.0
81.5 ± 0.3
82.7 ± 0.2
76.3 ± 1.6
72.5 ± 2.1
85.6 ± 0.7
91.0 ± 0.4
56.2 ± 2.1
50.5 ± 2.7
75.1 ± 1.9
85.3 ± 0.4
𝜖𝑥 = 1
59.9 ± 2.7
61.9 ± 3.1
67.5 ± 3.0
83.9 ± 0.4
81.8 ± 0.4
81.9 ± 0.4
82.9 ± 0.2
82.9 ± 0.2
86.1 ± 0.6
85.4 ± 0.4
92.0 ± 0.1
92.7 ± 0.1
75.5 ± 2.5
73.1 ± 2.9
67.7 ± 4.2
84.9 ± 0.8
𝜖𝑥 = 2
58.5 ± 2.9
58.1 ± 2.1
77.2 ± 1.9
84.0 ± 0.3