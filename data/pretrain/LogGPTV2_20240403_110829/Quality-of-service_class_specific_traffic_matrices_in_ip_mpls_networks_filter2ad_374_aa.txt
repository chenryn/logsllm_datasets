title:Quality-of-service class specific traffic matrices in ip/mpls networks
author:Stefan Schnitter and
Franz Hartleb and
Martin Horneffer
Quality-Of-Service Class Specific Traffic Matrices  
in IP/MPLS Networks 
Deutsche Telekom, T-Systems 
Deutsche Telekom, T-Systems 
Franz Hartleb 
D-64295 Darmstadt 
+49 6151 9372726 
Stefan Schnitter 
D-64295 Darmstadt 
+49 6151 9378521 
PI:EMAIL 
PI:EMAIL
Martin Horneffer 
Deutsche Telekom, T-Com 
Hammer Str. 216-226 
D-48165 Münster 
+49 251 7985208 
traffic  matrices 
ABSTRACT 
In  this  paper  we  consider  the  problem  of  determining  traffic 
matrices  for  end-to-end  demands  in  an  IP/MPLS  network  that 
supports  multiple  quality  of  service  (QoS)  classes.  More 
precisely, we want to determine the set of traffic matrices Ti for 
each  QoS  class  i  separately.  Ti  contains  average  bandwidth 
levels  for  QoS  class  i  for  every  pair  of  routers  within  the 
network.  We  propose  a  new  method  for  obtaining  QoS  class 
specific 
that  combines  estimation  and 
measurement methods:  We take advantage of the fact that the 
total traffic matrix can be measured precisely in MPLS networks 
using  either 
the  LDP  or  RSVP-TE  protocol.  These 
measurements  can  then  be  used  in  a  mathematical  model  to 
improve estimation methods – known as network tomography – 
that estimate QoS class specific traffic matrices from QoS class 
specific link utilizations. In addition to the mathematical model, 
we present results of the proposed method from its application 
in Deutsche Telekom’s global IP/MPLS backbone network and 
we  show  that  the  estimation  accuracy  (mean  relative  error)  is 
improved by a factor of 2.5 compared to results from network 
tomogravity.  We  investigate  the  structure  of  the  estimated 
traffic matrices for the different QoS classes and motivate in this 
paper why QoS class specific traffic matrices will be essential 
for  efficient  network  planning  and  network  engineering  in  the 
future. 
Categories and Subject Descriptors 
C.4  [Performance  of  Systems]:  Measurement  techniques, 
[Computer-Communication 
Modeling 
Networks]: Network Operations.  
General Terms: Algorithms, Measurement. 
Keywords: Traffic Matrices, MPLS, LDP, QoS. 
1.  INTRODUCTION 
Traffic matrices – or origin-destination (o-d) matrices – contain 
end  to  end  traffic  demands  between  each  pair  of  nodes  in  a 
given IP network. In this paper we are interested in 15 minutes 
average traffic demands between routers. For an ISP, there are 
multiple  reasons  why  the  availability  of  good  quality  traffic 
Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for 
personal  or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. 
IMC07, October 24-26, 2007, San Diego, California, USA. 
Copyright 2007 ACM 978-1-59593-908-1/07/0010...$5.00. 
techniques.  C.2.3 
include  network  planning  and 
matrices  is  essential:  Important  tasks  for  an  ISP  that  require 
traffic  matrices 
traffic 
engineering. They are needed to perform simulations of failure 
scenarios and network extension scenarios as well as for (IGP) 
routing  optimization.  For  some  tasks  –  especially  in  network 
planning – traffic matrices are needed on the level of PoPs but 
they  can  generally  be  obtained  by  aggregating  router  level 
traffic matrices. 
Already the task of traffic matrix generation is difficult for the 
total  traffic  within  a  network  and  may  require  estimation 
methods  or  complex  measurement  infrastructures.  But  as  true 
multi-service  networks  become  reality  and  traffic  classes  with 
different service requirements exist, the need for traffic matrices 
per  class-of-service  (CoS)  is  getting  stronger:  In  order  to  plan 
and operate a multi-service IP/MPLS  network economically, an 
ISP needs to incorporate QoS class specific traffic matrices into 
its planning and engineering tasks. Possible scenarios include: 
•  Different service level agreements for the QoS classes 
require a different bandwidth dimensioning. For each 
QoS  class  the  actual  traffic  must  never  exceed  the 
bandwidth  guaranteed  to  that  class  by  the  scheduler. 
Only the best effort class can be allowed to use up to 
the  physical  link  bandwidth  without  violating  given 
service level agreements. 
The use of links with low delay (e.g. direct links from 
Europe  to  Asia  –  not  via  America)  only  for  traffic 
from certain QoS classes. 
Failure  simulations  in  order  to  validate  certain  QoS 
concepts (e.g. to make sure that non-best-effort traffic 
remains below a certain link utilization threshold)    
• 
• 
While applications with high QoS requirements such as IPTV or 
VoIP  are  growing  to  a  significant  contribution  to  the  overall 
traffic  demand,  a  service  provider  needs  to  incorporate  QoS 
class  specific  traffic  data  into  its  planning  and  engineering 
processes.   
In this paper we consider only unicast traffic matrices. Multicast 
traffic will generally be divided into different QoS classes, too, 
and  has  to  be  measured  separately.  For  distribution  platforms 
with  fixed  traffic  sources  and  sinks  (e.g.  IPTV  within  the 
backbone network) this may be possible from the knowledge of 
the source traffic and the multicast tree. 
For (unicast) traffic matrices, a variety of methods have already 
been  investigated,  both  for  traffic  matrix  estimation  and  for 
traffic matrix measurement. One method that is widely deployed 
for 
level 
measurements is Cisco’s Netflow [1]. But the implementation is 
rather complex and issues that arise in practice include:  
traffic  matrix  measurement  based  on  IP  flow 
• 
•  Netflow  availability  and  performance  depend  on  the 
line card types in use – in a service provider’s network 
there is usually a large number of different hardware 
types in use. Thus only a partial measurement might 
be possible. 
IP flow measurements generally use packet sampling, 
so 
the 
measurement  accuracy  (low  sampling  rates)  and 
performance (high sampling rates). Some router types 
are not even able to provide anything than very high 
sampling rates. 
Large  effort  for  aggregation  of  the  flows  that  are 
exported by the routers. 
to  make  between 
tradeoff 
is  a 
there 
• 
Well-established  estimations  methods  for  traffic  matrices  are 
gravity estimation [10], where a traffic demand from s to t is set 
proportional  to  the  total  outgoing  traffic  of  s  and  the  total 
incoming traffic of t or network tomography, where end-to-end 
demands  are  estimated  from  link  utilizations  –  see  [12].  The 
combination  of  those  two  estimation  methods  is  commonly 
referred to as network tomogravity – see [13] for a survey and 
comparison  of  different  estimation  methods.  It  is  obvious  that 
those estimation methods can also be used to estimate QoS class 
specific traffic matrices if the necessary input data (mostly link 
utilizations) are available per QoS class. 
In networks that use multi-protocol label switching - MPLS [2] 
– to forward packets there are additional methods to measure the 
total  traffic  matrix.  If  RSVP-TE  [3]  is  used,  a  full  mesh  of 
tunnels can be deployed and counters for those tunnels exist to 
measure the traffic matrix. If LDP [4] is used to distribute the 
label information in a network, LDP statistics of the routers can 
be used to compute traffic matrices on a router level – see [5], 
[6].  The  LDP  method  results  in  a  very  high  measurement 
accuracy (for example, there is no sampling involved) while the 
measurement complexity is very low: The measurement is based 
on  aggregated  forwarding  equivalence  classes  (FEC)  that  are 
introduced in MPLS/LDP and is not based on the IP flow level. 
However, the LDP method can only be used for the network’s 
total  traffic  matrix  and  not  per  QoS  class.  For  Deutsche 
Telekom’s  IP/MPLS  backbone  network  the  LDP  method  is 
currently used to compute total traffic matrices to support IGP 
metric optimization (see [9] for a theoretical survey or [8] for a 
discussion  on  the  practical  implementation)  and  network 
planning. 
Dependent  on  the  existing  measurement  infrastructure,  the 
network topology or the deployed protocols, it may be easier to 
obtain  traffic  matrices  for  the  total  traffic  within  the  network 
than QoS class specific traffic matrices. This is why we propose 
a  model  for  QoS  class  specific  traffic  matrices  that  combines 
estimation methods with total traffic matrix measurements.   
This  paper  is  organized  as  follows:  Section  2  describes  the 
mathematical model used to estimate QoS class specific traffic 
matrices  and  section  3  gives  numerical  results  from  the 
application of this model to a part of Deutsche Telekom’s global 
IP/MPLS backbone network.   
2.  QOS TRAFFIC MATRIX MODEL 
2.1  Notations for Tomogravity Model 
If  our  network  has  n  nodes  and  m  links  we  denote  by  x  the 
vector  of  link  utilization  and t the vector representation of the 
jit ⋅
 (i,j=1,…,n) contains the traffic from node 
traffic matrix, i.e. 
i to node j. From the network topology and the IGP metrics in 
use, we can construct the network’s routing matrix A. The entry 
, ∈⋅ jika
]1,0[
  (i,j=1,…,n;  k=1,…,m)  of  A  contains  the  part  of 
jit ⋅
 on link k. The routing matrix A results from a 
the demand 
shortest-path calculation with respect to the given IGP metrics – 
if the network makes no use of equal cost path splitting (ECMP) 
, ∈⋅ jika
  holds.  The  relation  between  traffic  matrix,  link 
utilizations  and  routing  matrix  is  then  described  by  the 
following equation: 
(R) 
The  tomography  estimation  of  the  traffic  matrix  t*  can  be 
constructed by solving the following system of equations for a 
given  routing  matrix  A,  a  given  vector  of  measured  link 
utilizations  x  and  a  given  initial  estimate  Et
  for  the  traffic 
matrix: 
A t = x. 
}1,0{
*
(TG) 
→−
−
→
The solution of (TG) is given by        
At
||
ts
t
||..
x
||
Et
||
*
min
min
*
t
=
t
E
(~
tAxA
⋅+
⋅−
)
E
where 
~
TUSVA
⋅=
−1
⋅
denotes the Moore-Penrose-Inverse (also pseudo inverse) of the 
routing  matrix  A  and  is  constructed  with  a  singular  value 
decomposition (SVD) of A: 
=
TVSUA
⋅
⋅
We  use  the  SVD  routines  contained  in  LAPACK  [14]  for  this 
purpose  –  iterative  methods  for  large  scale  SVD computations 
are also available – see [15].    
If a gravity estimation is used as an initial estimate tE, (TG) is 
referred to as tomogravity method. The solution of (TG) can be 
interpreted as a (orthogonal) projection of the initial solution tE 
onto  the  subspace  of  all  traffic  matrices  that  satisfy  the  link 
utilization  restrictions  (R),  i.e.  we  calculate  that  admissible 
traffic  matrix  that  is  closest  to  tE.  For  tomogravity  estimation 
methods a gravity estimate is chosen for tE – one possibility to 
improve the gravity estimate is the use of a generalized gravity 
model as given in [12] which takes into account the knowledge 
of where peering traffic enters the network. The assumption that 
there is no transit peering traffic in the network then restricts the 
number  of  possible  traffic  demand  combinations  and  thereby 
improves the estimate. 
One method to improve the numerical stability of the solution of 
the  system  (TG)  is  to  remove  parallel  links  from  the  network 
topology.  Service  providers  often  use  parallel  links  in  their 
networks to increase capacity before they move to a technology 
with  higher  capacity  (e.g.  two  or  three  2.5Gbit/s  links  before 
installing on 10Gbit/s link). Those parallel links have the same 
IGP metric so that they are utilized equally when using ECMP. 
In practice, the traffic is not shared entirely equal depending on 
which hashing algorithm the routers use for load sharing – that 
means 
the  corresponding 
components of the utilization vector x.  On the other hand the 
routing  matrix  A  introduces  theoretical  load  sharing  properties 
there  are  differences 
that 
in 
The  estimation  accuracy  of  (TGQ)  will  depend  highly  on  the 
size of the subset of links that have QoS specific link utilizations 
available.  Also  it  should  be  noted  that  (TGQ)  can  result  in 
traffic matrix estimations with negative entries since (TGQ) has 
no  condition  for  Qt
  being  nonnegative.  We  apply  a  simple 
iteration scheme to assure positive solutions whose convergence 
is discussed in the following section. 
Compared to an application of the tomogravity method (TG) for 
each QoS class separately (TGQ) introduces further restrictions: 
the sum of the QoS class specific traffic demands must equal the 
total  traffic  for  the  respective  traffic  relation.  These  additional 
restrictions should improve the estimation quality of the method. 
Section 3 investigates the improvement of the estimation quality 
for a concrete example with realistic data.     
Model (RQ) applies the same routing for all QoS classes but it 
can  be  easily  extended  to  QoS  specific  routing  schemes  if  we 
replace  Aˆ  by QoS specific routing matrices 
)(ˆ lA   (l = 1,…,q). 
3.  NUMERICAL RESULTS 
The  QoS-tomography  model  (TGQ)  is  applied  to  an  example 
network  with  26  nodes  and  190  edges  (Figure  2).  We  assume 
four QoS classes: voice, low loss, low delay, and best effort. 
In  the  following  section,  we  discuss  two  problems:  estimation 
convergence and estimation accuracy. The third example shows 
estimation  results  based  on  QoS  link  utilization  measurements 
from our backbone network. The numerical results focus on the 
differences  in  the  demand  structures  of  the  QoS  class  specific 
traffic  matrices.  Therefore  traffic  matrices  for  one  given  (15-
minute) time interval are compared. 