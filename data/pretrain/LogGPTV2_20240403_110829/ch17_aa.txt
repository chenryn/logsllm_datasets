数据耐久性工程
James Cowling, Dropbox, Inc.
   可靠性对于 SRE 有如生存和呼吸一样重要，但对许多工程师而言，可靠性是可用性的代名词：“我们要保持网站正常运行吗？”然而，可靠性是一个多方面的问题，其中极为重要的一部分是持久性：“我们如何避免丢失或破坏数据？”
对于任何存储用户数据的公司来说，耐久性工程都至关重要。大多数公司都能在停机期间生存下来，但很少公司能够承受失去相当一部分用户数据。然而，在耐用系统方面积累专门知识尤其具有挑战性；随着公司的发展，随着系统的发展，大多数公司都会提高可用性，但单一的耐久性错误可能是公司终结的事件。因此，提前投入精力了解实际耐久性威胁以及如何针对这些威胁进行设计非常重要。
复制是给表买的保险
 如果不想丢失数据，则应存储数据的多个副本。你可能不需要一本书来告诉你这些。我们将范范讨论这点，因为当涉及到耐久性时，这实际上只是基本要求。备份
  备份数据。备份的好处是，它们在逻辑和物理上与主数据存储脱节：导致数据库状态丢失或损坏的操作错误可能不会影响备份。理想情况下，这些应同时存储在本地基础结构和异地，以便提供快速的本地访问且可以防止本地物理灾难影响。
但是，备份有一些主要限制，尤其是在恢复时间和数据新鲜度方面。这两种因素都可能导致你遭受比预期更多的数据丢失或停机。
恢复
 从备份还原状态可能需要很长时间，特别是如果最近你还没有练习过从备份中恢复。事实上，如果最近没有测试备份，它们甚至可能根本无法工作！
在 Dropbox 的早期，我们沮丧地发现，在灾难性故障后，重新启动生产数据库需要 8 小时。即使所有数据都完好无损，但因为我们将 MySQL 的 innodb_max_dirty_pages_pct 参数设置得过高，导致 MySQL 在崩溃恢复期间只能扫描重做日志，因而需要 8 小时才能上线。我们当时运行的是 MySQL 5.1；对于版本 5.5 及版本以上，问题要小得多。幸运的是，这个数据库不是关键数据库，所以我们能够在几个小时内完全绕过数据库使dropbox.com重新联机，但这肯定是一个警钟。自那时以来，我们的运维成熟度得到显著提高，我在本章后面概述了这一点。新鲜度
 备份表示以前的快照，并且通常会导致从它们还原时丢失最近的数据。通常你希望同时存储完整快照备份以快速恢复和历史版本控制，以及更快速状态的增量备份，从而尽量避免过时数据反复存储。但是，任何比这更强大的保证都需要一个实际的复制协议。
复制
  数据库复制技术包括异步复制到半同步复制到完整的quorum或consensus协议。复制策略的选择将通过可以容忍的不一致性程度以及对数据库的性能要求来决定。 尽管复制通常对于持久性至关重要，但它也可能是不一致的来源。 你必须注意异步复制方案，以确保不会从副本数据库中错误地读取过时数据。 因为主服务器延迟而对副本数据库的提升也可能导致永久性数据丢失。通常，公司将运行一个主数据库和两个副本库，然后假设持久性问题已解决。更精细的存储系统通常需要更精细的复制机制，尤其是在存储开销成为严重问题时。对于像 Dropbox 这样的存储海量数据的公司，必须有比基本复制更有效的提供持久性的方法。有的公司采用了纠删码等技术，将编码的冗余数据块存储在多个磁盘上，从而实现更高的持久性，并降低存储开销。Dropbox 使用纠删码技术的变体，这些变体旨在跨多个地理区域分发数据，同时最大限度地减少从常规磁盘故障中恢复所需的跨区域网络带宽。
估计耐久性
  无论选择何种复制技术，你可能会遇到一个直接的问题，“我实际拥有多少耐久性？”你是否需要一个数据库副本？还是两个？或者更多个？
 估计耐久性的一个便捷方法是将一些数字输入Markov 模型。让我们在这里引入一些简单的数学，所以要么潜心读下去，要么干脆跳过本节。假设每个磁盘都有估定的平均故障时间（MTTF，以小时为单位），并且我们的运维流程需要替换和重新复制故障磁盘，以平均恢复时间（MTTR，以小时为单位）为限制。我们分别用和来表示故障率和恢复率。这意味着我们假设每个磁盘以平均每小时 λ 个的故障率失败，并且每个磁盘故障都以每小时 μ 个的恢复率替换。
对于给定的复制方案，假设我们的复制组中有n个磁盘，如果我们同时损坏超过m个磁盘，则丢失数据；例如，对于三向数据库复制，我们的n = 3 且 m = 2；对于 RS(9,6) 究删码，我们的 n = 9 且 m = 3。
我们可以采用这些变量并建模 Markov 链（如#durability_markov_model所示），其中每个状态表示组中的给定失败数，状态之间的转换表示磁盘失败或正在恢复的速率。
耐久性 Markov 模型耐久性 Markov 模型
在此模型中，从第一个状态到第二个状态的流等于nλ，因为还剩下n个磁盘，每个磁盘都以每小时 λ 的失败率失败。从第二个状态返回第一个状态的流等于 μ，因为我们只有一个故障磁盘，该磁盘以每小时 μ 恢复率恢复，等等。
此模型中的数据丢失率相当于从最接近数据丢失的状态转换到数据丢失状态的速率。这个速度可以计算为
每小时 R(loss) 复制组的数据丢失率。
这种简化的故障模型允许我们代入一些数字，并估计丢失数据的可能性。
假设我们的磁盘的年化故障率 （AFR） 为 3%。我们可以使用 AFR 计算 MTTF（这大约是）；在这种情况下，MTTF = 287795 小时。假设我们有一些相当不错的运维工具，可以在故障后 24 小时内替换和重新复制磁盘，因此MTTR = 24。如果我们采用三向数据复制，我们的 n = 3，m = 2，, and .把以上这些数据代入方程中，我们得到R(loss) = 每小时 7.25 × 10–14 的数据丢失故障或每年 6.35 ×10–10 个故障。这意味着给定复制组在给定年份中是安全的，概率为 0.999999994。
等等，这很安全，对吧？
嗯，9个9 的耐久性是相当体面的。但是，如果复制组数量众多，则失败的可能性会比这更大，因为虽然每个组都有 9个9 的耐久性，但总体而言，整体上更有可能失败。如果你是大型消费类存储公司，则可能需要进一步推动耐用性。比如减少恢复延迟，或购买更好的磁盘，或增加复制系数，则很容易将持久性数字推得更高。Dropbox在理论上的耐久性数字远远超过了二十四个9，因为引入了一些奇妙的究删码和自动磁盘恢复系统。根据这些模型，几乎不可能丢失数据。这是否意味着我们可以回家睡大觉？
不幸的是这还不够 ...
你的耐久性估计只是上限！不幸的是这还不够 ...
你的耐久性估计只是上限！