Timer expiration
As we said, one of the main tasks of the ISR associated with the interrupt that the clock source gener-
ates is to keep track of system time, which is mainly done by the KeUpdateSystemTime routine. Its sec-
ond job is to keep track of logical run time, such as process/thread execution times and the system tick 
time, which is the underlying number used by APIs such as GetTickCount that developers use to time 
operations in their applications. This part of the work is performed by KeUpdateRunTime. Before doing 
any of that work, however, KeUpdateRunTime checks whether any timers have expired. 
Windows timers can be either absolute timers, which implies a distinct expiration time in the future, 
or relative timers, which contain a negative expiration value used as a positive offset from the current 
time during timer insertion. Internally, all timers are converted to an absolute expiration time, although 
the system keeps track of whether this is the “true” absolute time or a converted relative time. This dif-
ference is important in certain scenarios, such as Daylight Savings Time (or even manual clock changes). 
but a relative timer—say, one set to expire “in two hours”—would not feel the effect of the clock 
the kernel reprograms the absolute time associated with relative timers to match the new settings.
-
tiples, each multiple of the system time that a timer could be associated with is an index called a hand, 
which is stored in the timer object's dispatcher header. Windows used that fact to organize all driver 
and application timers into linked lists based on an array where each entry corresponds to a possible 
multiple of the system time. Because modern versions of Windows 10 no longer necessarily run on a 
periodic tick (due to the dynamic tick
46 bits of the due time (which is in 100 ns units). This gives each hand an approximate “time” of 28 ms. 
hands could have expiring timers, Windows can no longer just check the current hand. Instead, a bit-
the bitmap and checked during every clock interrupt.
Regardless of method, these 256 linked lists live in what is called the timer table—which is in the 
PRCB—enabling each processor to perform its own independent timer expiration without needing to 
tables, for a total of 512 linked lists.
Because each processor has its own timer table, each processor also does its own timer expiration 
= [15]    - msedge.exe        - 10000    - 0         =
= [16]    - msedge.exe        - 10000    - 0         =
= [17]    - msedge.exe        - 10000    - 0         =
= [18]    - msedge.exe        - 10000    - 0         =
= [19]    - SearchApp.exe     - 40000    - 0         =
====================================================== 
CHAPTER 8 System mechanisms
71
-
tion time to avoid any incoherent state. Therefore, to determine whether a clock has expired, it is only 
necessary to check if there are any timers on the linked list associated with the current hand. 
31
0
255
CPU 0
Timer Table
Timer Hand
0
31
0
255
CPU 1
Timer Table
Timer Hand
0
Driver
Process
Timer 1
Timer 2
Timer 3
Timer 4
FIGURE 8-19 Example of per-processor timer lists.
Although updating counters and checking a linked list are fast operations, going through every 
timer and expiring it is a potentially costly operation—keep in mind that all this work is currently being 
performed at CLOCK_LEVEL, an exceptionally elevated IRQL. Similar to how a driver ISR queues a DPC 
draining mechanism knows timers need expiration. Likewise, when updating process/thread runtime, if 
the clock ISR determines that a thread has expired its quantum, it also queues a DPC software interrupt 
processing of run-time updates because each processor is running a different thread and has different 
DPCs are provided primarily for device drivers, but the kernel uses them, too. The kernel most fre-
quently uses a DPC to handle quantum expiration. At every tick of the system clock, an interrupt occurs 
at clock IRQL. The clock interrupt handler (running at clock IRQL) updates the system time and then 
decrements a counter that tracks how long the current thread has run. When the counter reaches 0, the 
priority task that should be done at DPC/dispatch IRQL. The clock interrupt handler queues a DPC to 
interrupt has a lower priority than do device interrupts, any pending device interrupts that surface 
before the clock interrupt completes are handled before the DPC interrupt occurs.
Once the IRQL eventually drops back to DISPATCH_LEVEL, as part of DPC processing, these two 
72 
CHAPTER 8 System mechanisms
TABLE 8-10 
KPRCB Field
Type
Description
LastTimerHand
Index (up to 256)
The last timer hand that was processed by this processor. In recent 
builds, part of TimerTable because there are now two tables.
ClockOwner
Boolean
Indicates whether the current processor is the clock owner.
TimerTable
List heads for the timer table lists (256, or 512 on more recent builds).
DpcNormalTimerExpiration
Bit
Indicates that a DISPATCH_LEVEL interrupt has been raised to request 
timer expiration.
Chapter 4 of Part 1 covers the actions related to thread scheduling and quantum expiration. Here, 
we look at the timer expiration work. Because the timers are linked together by hand, the expira-
tion code (executed by the DPC associated with the PRCB in the TimerExpirationDpc
KiTimerExpirationDpc) parses this list from head to tail. (At insertion time, the timers nearest to the 
within this hand.) There are two primary tasks to expiring a timer:
I 
The timer is treated as a dispatcher synchronization object (threads are waiting on the timer as
part of a timeout or directly as part of a wait). The wait-testing and wait-satisfaction algorithms
will be run on the timer. This work is described in a later section on synchronization in this chap-
ter. This is how user-mode applications, and some drivers, make use of timers.
I 
The timer is treated as a control object associated with a DPC callback routine that executes
when the timer expires. This method is reserved only for drivers and enables very low latency
response to timer expiration. (The wait/dispatcher method requires all the extra logic of wait
signaling.) Additionally, because timer expiration itself executes at DISPATCH_LEVEL, where
DPCs also run, it is perfectly suited as a timer callback.
As each processor wakes up to handle the clock interval timer to perform system-time and run-time 
processing, it therefore also processes timer expirations after a slight latency/delay in which the IRQL 
drops from CLOCK_LEVEL to DISPATCH_LEVEL
-
tion processing that might occur if the processor had associated timers.
Time Interrupt
Time
Software Timer Expiration
Processor 1
Processor 0
Time
FIGURE 8-20 Timer expiration.
CHAPTER 8 System mechanisms
73
Processor selection
A critical determination that must be made when a timer is inserted is to pick the appropriate table to 
timer serial-
ization is disabled. If it is, it then checks whether the timer has a DPC associated with its expiration, and 
If the timer has no DPC associated with it, or if the DPC has not been bound to a processor, the kernel 
-
tion on core parking, see Chapter 4 of Part 1.) If the current processor is parked, it picks the next closest 
neighboring unparked processor in the same NUMA node; otherwise, the current processor is used. 
This behavior is intended to improve performance and scalability on server systems that make use 
of Hyper-V, although it can improve performance on any heavily loaded system. As system timers pile 
with the execution of timer expiration code, which increases latency and can even cause heavy delays 
or missed DPCs. Additionally, timer expiration can start competing with DPCs typically associated with 
driver interrupt processing, such as network packet code, causing systemwide slowdowns. This process 
is exacerbated in a Hyper-V scenario, where CPU 0 must process the timers and DPCs associated with 
potentially numerous virtual machines, each with their own timers and associated devices.
-
tion load is fully distributed among unparked logical processors. The timer object stores its associated 
processor number in the dispatcher header on 32-bit systems and in the object itself on 64-bit systems.
Timers Queue on CPU 0
Timers Queued on Current CPU
CPU
0
CPU
1
CPU
2
CPU
3
CPU
0
CPU
1
CPU
2
CPU
3
FIGURE 8-21 Timer queuing behaviors.
much. Additionally, it makes each timer expiration event (such as a clock tick) more complex because a 
processor may have gone idle but still have had timers associated with it, meaning that the processor(s) 
asynchronous behaviors in timer expiration, which may not always be desired. This complexity makes 
can ultimately remain to manage the clock. Therefore, on client systems, timer serialization is enabled if 
Modern Standby is available, which causes the kernel to choose CPU 0 no matter what. This allows CPU 
0 to behave as the default clock owner—the processor that will always be active to pick up clock inter-
rupts (more on this later). 
74 
CHAPTER 8 System mechanisms
Note This behavior is controlled by the kernel variable KiSerializeTimerExpiration, which is 
initialized based on a registry setting whose value is different between a server and client 
installation. By modifying or creating the value SerializeTimerExpiration
other than 0 or 1, serialization can be disabled, enabling timers to be distributed among 
processors. Deleting the value, or keeping it as 0, allows the kernel to make the decision 
based on Modern Standby availability, and setting it to 1 permanently enables serialization 
even on non-Modern Standby systems.
EXPERIMENT: Listing system timers
You can use the kernel debugger to dump all the current registered timers on the system, as well as 
information on the DPC associated with each timer (if any). See the following output for a sample:
0: kd> !timer 
Dump system timers 
Interrupt time: 250fdc0f 00000000 [12/21/2020 03:30:27.739] 
PROCESSOR 0 (nt!_KTIMER_TABLE fffff8011bea6d80 - Type 0 - High precision) 
List Timer             Interrupt Low/High Fire Time
DPC/thread 
PROCESSOR 0 (nt!_KTIMER_TABLE fffff8011bea6d80 - Type 1 - Standard) 
List Timer             Interrupt Low/High Fire Time
DPC/thread 
 1 ffffdb08d6b2f0b0   0807e1fb 80000000 [
NEVER
] thread ffffdb08d748f480 
 4 ffffdb08d7837a20   6810de65 00000008 [12/21/2020 04:29:36.127]  
 6 ffffdb08d2cfc6b0   4c18f0d1 00000000 [12/21/2020 03:31:33.230] netbt!TimerExpiry 
(DPC @ ffffdb08d2cfc670) 
   fffff8011fd3d8a8 A fc19cdd1 00589a19 [ 1/ 1/2100 00:00:00.054] nt!ExpCenturyDpcRoutine 
(DPC @ fffff8011fd3d868) 
 7 ffffdb08d8640440   3b22a3a3 00000000 [12/21/2020 03:31:04.772] thread ffffdb08d85f2080 
   ffffdb08d0fef300   7723f6b5 00000001 [12/21/2020 03:39:54.941]  
FLTMGR!FltpIrpCtrlStackProfilerTimer (DPC @ ffffdb08d0fef340) 
11 fffff8011fcffe70   6c2d7643 00000000 [12/21/2020 03:32:27.052] nt!KdpTimeSlipDpcRoutine 
(DPC @ fffff8011fcffe30) 
   ffffdb08d75f0180   c42fec8e 00000000 [12/21/2020 03:34:54.707] thread ffffdb08d75f0080 
14 fffff80123475420   283baec0 00000000 [12/21/2020 03:30:33.060] tcpip!IppTimeout 
(DPC @ fffff80123475460) 
. . .  
58 ffffdb08d863e280 P 3fec06d0 00000000 [12/21/2020 03:31:12.803] thread ffffdb08d8730080 
   fffff8011fd3d948 A 90eb4dd1 00000887 [ 1/ 1/2021 00:00:00.054] nt!ExpNextYearDpcRoutine 
(DPC @ fffff8011fd3d908) 
. . .  
104 ffffdb08d27e6d78 P 25a25441 00000000 [12/21/2020 03:30:28.699]
tcpip!TcpPeriodicTimeoutHandler (DPC @ ffffdb08d27e6d38) 
    ffffdb08d27e6f10 P 25a25441 00000000 [12/21/2020 03:30:28.699]  
tcpip!TcpPeriodicTimeoutHandler (DPC @ ffffdb08d27e6ed0) 
106 ffffdb08d29db048 P 251210d3 00000000 [12/21/2020 03:30:27.754] 
CLASSPNP!ClasspCleanupPacketTimerDpc (DPC @ ffffdb08d29db088) 
EXPERIMENT: Listing system timers
You can use the kernel debugger to dump all the current registered timers on the system, as well as
information on the DPC associated with each timer (if any). See the following output for a sample:
0: kd> !timer
Dump system timers
Interrupt time: 250fdc0f 00000000 [12/21/2020 03:30:27.739]
PROCESSOR 0 (nt!_KTIMER_TABLE fffff8011bea6d80 - Type 0 - High precision)
List Timer             Interrupt Low/High Fire Time
DPC/thread
PROCESSOR 0 (nt!_KTIMER_TABLE fffff8011bea6d80 - Type 1 - Standard)
List Timer             Interrupt Low/High Fire Time
DPC/thread
 1 ffffdb08d6b2f0b0   0807e1fb 80000000 [
NEVER
] thread ffffdb08d748f480
 4 ffffdb08d7837a20   6810de65 00000008 [12/21/2020 04:29:36.127]  
 6 ffffdb08d2cfc6b0   4c18f0d1 00000000 [12/21/2020 03:31:33.230] netbt!TimerExpiry 
(DPC @ ffffdb08d2cfc670) 
   fffff8011fd3d8a8 A fc19cdd1 00589a19 [ 1/ 1/2100 00:00:00.054] nt!ExpCenturyDpcRoutine 
(DPC @ fffff8011fd3d868) 
 7 ffffdb08d8640440   3b22a3a3 00000000 [12/21/2020 03:31:04.772] thread ffffdb08d85f2080 
   ffffdb08d0fef300   7723f6b5 00000001 [12/21/2020 03:39:54.941]  
FLTMGR!FltpIrpCtrlStackProfilerTimer (DPC @ ffffdb08d0fef340) 
11 fffff8011fcffe70   6c2d7643 00000000 [12/21/2020 03:32:27.052] nt!KdpTimeSlipDpcRoutine
(DPC @ fffff8011fcffe30) 
   ffffdb08d75f0180   c42fec8e 00000000 [12/21/2020 03:34:54.707] thread ffffdb08d75f0080 
14 fffff80123475420   283baec0 00000000 [12/21/2020 03:30:33.060] tcpip!IppTimeout 
(DPC @ fffff80123475460) 
. . . 
58 ffffdb08d863e280 P 3fec06d0 00000000 [12/21/2020 03:31:12.803] thread ffffdb08d8730080 
   fffff8011fd3d948 A 90eb4dd1 00000887 [ 1/ 1/2021 00:00:00.054] nt!ExpNextYearDpcRoutine
(DPC @ fffff8011fd3d908) 
. . . 
104 ffffdb08d27e6d78 P 25a25441 00000000 [12/21/2020 03:30:28.699]
tcpip!TcpPeriodicTimeoutHandler (DPC @ ffffdb08d27e6d38) 
    ffffdb08d27e6f10 P 25a25441 00000000 [12/21/2020 03:30:28.699]  
tcpip!TcpPeriodicTimeoutHandler (DPC @ ffffdb08d27e6ed0) 
106 ffffdb08d29db048 P 251210d3 00000000 [12/21/2020 03:30:27.754] 
CLASSPNP!ClasspCleanupPacketTimerDpc (DPC @ ffffdb08d29db088) 
CHAPTER 8 System mechanisms
75
    fffff80122e9d110   258f6e00 00000000 [12/21/2020 03:30:28.575] 
Ntfs!NtfsVolumeCheckpointDpc (DPC @ fffff80122e9d0d0) 
108 fffff8011c6e6560    19b1caef 00000002 [12/21/2020 03:44:27.661] 
tm!TmpCheckForProgressDpcRoutine (DPC @ fffff8011c6e65a0) 
111 ffffdb08d27d5540 P  25920ab5 00000000 [12/21/2020 03:30:28.592]  
storport!RaidUnitPendingDpcRoutine (DPC @ ffffdb08d27d5580) 
    ffffdb08d27da540 P  25920ab5 00000000 [12/21/2020 03:30:28.592]  
storport!RaidUnitPendingDpcRoutine (DPC @ ffffdb08d27da580) 
. . .  
Total Timers: 221, Maximum List: 8 
Current Hand: 139
In this example, which has been shortened for space reasons, there are multiple driver-
associated timers, due to expire shortly, associated with the Netbt.sys and Tcpip.sys drivers (both 
related to networking), as well as Ntfs, the storage controller driver drivers. There are also back-
ground housekeeping timers due to expire, such as those related to power management, ETW, 
kernel-mode timers that are used for wait dispatching. You can use !thread on the thread point-
ers to verify this. 
that checks for Daylight Savings Time time-zone changes, the timer that checks for the arrival 
of the upcoming year, and the timer that checks for entry into the next century. One can easily 
locate them based on their typically distant expiration time, unless this experiment is performed 
on the eve of one of these events.
Intelligent timer tick distribution
wakes up several times (the solid arrows) even when there are no associated expiring timers (the dotted 
arrows). Although that behavior is required as long as processor 1 is running (to update the thread/pro-
cess run times and scheduling state), what if processor 1 is idle (and has no expiring timers)? Does it still 
need to handle the clock interrupt? Because the only other work required that was referenced earlier is to 
keeping processor (in this case, processor 0) and allow other processors to remain in their sleep state; if 
they wake, any time-related adjustments can be performed by resynchronizing with processor 0.
Windows does, in fact, make this realization (internally called intelligent timer tick distribution), 
to handle its expiring timers, creating a much larger gap (sleeping period). The kernel uses a variable 
KiPendingTimerBitmaps
processors need to receive a clock interval for the given timer hand (clock-tick interval). It can then 
    fffff80122e9d110   258f6e00 00000000 [12/21/2020 03:30:28.575] 
Ntfs!NtfsVolumeCheckpointDpc (DPC @ fffff80122e9d0d0) 
108 fffff8011c6e6560    19b1caef 00000002 [12/21/2020 03:44:27.661] 
tm!TmpCheckForProgressDpcRoutine (DPC @ fffff8011c6e65a0) 
111 ffffdb08d27d5540 P  25920ab5 00000000 [12/21/2020 03:30:28.592]  
storport!RaidUnitPendingDpcRoutine (DPC @ ffffdb08d27d5580) 
    ffffdb08d27da540 P  25920ab5 00000000 [12/21/2020 03:30:28.592]  
storport!RaidUnitPendingDpcRoutine (DPC @ ffffdb08d27da580) 
. . . 
Total Timers: 221, Maximum List: 8