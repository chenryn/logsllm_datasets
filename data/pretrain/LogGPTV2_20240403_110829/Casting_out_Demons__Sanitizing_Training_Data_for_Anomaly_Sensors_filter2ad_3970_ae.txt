ing 0.5 would have been also enough). As we can observe in
Table 5, the FP rate has improved and the detection rate re-
mains at 100%. The improvement of the FP rate is reﬂected
in the size of the cross-sanitized models (see Table 6).
In terms of computational performance, as expected, the
indirect model differencing is more expensive than the di-
rect model differencing (see Table 7). There is a tradeoff
between how fast the cross-sanitization needs to be done
and how high the FP rate is. If a higher FP rate is allowed,
a quicker cross-sanitization can be applied by using the di-
rect differencing; otherwise the best solution is the indirect
model differencing.
Finally, any of the methods can be reﬁned using feed-
back from a shadow sensor, but this process would intro-
duce more computational effort. The beneﬁt of using a
shadow sensor is that we can cross-sanitize the model to
optimize the FP rate.
Table 7. Time to cross-sanitize for direct and
indirect model differencing
Method
direct
indirect
www1
13.98s
1966.68s
www
26.35s
1732.32s
lists
16.84s
685.81s
5.2 Polymorphic Attacks
Polymorphic attacks are of increasing concern. To test
against such attacks, we used a popular polymorphic en-
gine, CLET [6] to generate samples of polymorphic shell-
code.
In these experiments, we assume that an attacker
tries to perform a training attack using a polymorphic vector
(which implies that the exploit would include polymorphic
shellcode). For the experiments, we used 2100 samples of
shellcode generated with CLET. We used 100 micro-models
with a three hour granularity derived from our dataset for
www1. We poisoned each micro-model with 20 samples of
shellcode. We also poisoned the data set from which the
sanitized model is built with the remaining 100 shellcode
samples.
We rebuilt the sanitized model with our system. In the
voting strategy, all of the micro-models found the 100 shell-
code samples as being anomalous, given that, on average,
82% of the grams from 100 samples were found abnormal
by the micro-models. After the sanitized model was com-
puted, we tested it against the testing dataset of 100 hours.
As expected, the performance results were identical with
the ones given when the sanitized model was constructed
without any shellcode samples. These experiments indicate
that most common polymorphic attacks can be handled by
the local sanitization architecture.
6 Related work
Our approach for sanitizing training datasets shares ele-
ments with ensemble methods, which are reviewed in [7]. It
is important to note that, while most of these methods tra-
ditionally fall in the category of supervised learning algo-
rithms, due the applications of our work (e.g. real network
trafﬁc), we are forced to use unlabeled training data.
In
particular, we construct a set of classiﬁers and then classify
the new data points using a (weighted) vote. We generate
AD models from slices of the training data, thus manipulat-
ing the training examples presented to the learning method.
Another similar machine learning approach is that of Bag-
ging predictors [2], which uses a learning algorithm with
a training set that consists of a sample of m training ex-
amples drawn randomly for the initial data set. The cross-
validated committees method [19] proposes to construct a
training model by leaving out disjoint subsets of the train-
ing data. ADABoost [11] generates multiple hypothesis
and maintains a set of weights over the training example.
Each iteration invokes the learning algorithm to minimize
the weighted error and returns a hypothesis, which is used
in a ﬁnal weighted vote.
One of the most similar work to ours is MetaCost [8],
which is an algorithm that implements cost-sensitive classi-
ﬁcation. Instead of modifying an error minimization classi-
ﬁcation procedure, it views the classiﬁer as a black box, the
same as we do, and wraps the procedure around it in order to
reduce the loss. MetaCost estimates the class probabilities
and relabels the training examples such that the expected
cost of predicting new labels is minimized. Finally it builds
a new model based on the relabeled data. Our algorithm
also labels the training examples and ignores the abnormal
ones in the training process.
We have previously explored the feasibility of clean-
ing trafﬁc [5]. That work contains a very limited analysis
and did not address the problem of long-lasting attacks in
the training data.
In contrast, this paper performs an ex-
tended analysis on larger and more realistic data sets to help
conﬁrm the hypothesis that cleaning is possible. We also
present alternatives that can be used when the local archi-
tecture fails due to long lasting training attacks.
JAM [24] focuses on developing and evaluating a range
of learning strategies for fraud detection.
That work
presents methods for “meta-learning” by computing sets
of “base classiﬁers” over various partitions or sampling of
the training data. The combining algorithms proposed are
called “class-combiner” or “stacking” and they are built
based on work presented in [3] and [31]. The exchange
of abnormal models was used in [24] for commercial fraud
detection. Their results show that fraud detection systems
can be substantially improved by combining multiple mod-
els of fraudulent transactions shared among banks. We ap-
ply a similar idea in the case of network trafﬁc content-
based anomaly detection in order to cross-sanitize the nor-
mal model.
The perceived utility of anomaly detection is based on
the assumption that malicious inputs rarely occur during
the normal operation of the system. Because a system can
evolve over time, it is also likely that new non-malicious
inputs will be seen [10]. Perhaps more troubling, Fogla
and Lee [9] have shown how to evade anomaly classiﬁers
by constructing polymorphic exploits that blend with nor-
93
mal trafﬁc (a sophisticated form of mimicry attack [27]),
and Song et al. [23] have improved on this technique and
shown that content–based approaches may not work against
all polymorphic threats, since many approaches often ﬁx on
speciﬁc byte patterns [17].
7 Conclusions
Due to recent advances in polymorphic attacks, we be-
lieve that the research community should make a con-
certed effort to revive the use of content–based anomaly
detection as a ﬁrst-class defensive technique. To that end,
we introduce a novel sanitization technique that signiﬁ-
cantly improves the detection performance of out-of-the-
box anomaly detection (AD) sensors. We are the ﬁrst to
introduce the notion of micro-models: models of “normal”
trained on small slices of the training data set. Using sim-
ple weighted voting schemes, we signiﬁcantly improve the
quality of unlabeled training data by making it as “attack-
free” and “regular” as possible. Our approach is straightfor-
ward and general, and we believe it can be applied to a wide
range of unmodiﬁed AD sensors (because it interacts with
the training data rather than the AD algorithm) without in-
curring signiﬁcant additional computational cost other than
in the initial training phase.
The experimental results indicate that our system can
serve both as a stand-alone sensor and as an efﬁcient and
accurate online packet classiﬁer using a shadow sensor. Fur-
thermore, the alerts generated by the “sanitized” AD model
represent a small fraction of the total trafﬁc. The model
detects approximately 5 times more attack packets than the
unsanitized AD model. In addition, the AD system can de-
tect more threats both online and after an actual attack, since
the AD training data are attack-free. In case local sanitiza-
tion is evaded, we extend our methodology to support shar-
ing models of abnormal trafﬁc among collaborating sites.
A site can cross-sanitize its local training data based on the
remote models. Our results show that if the collaborating
sites were targeted by the same attack and they were able to
capture it in their abnormal models, the detection rate can
be improved up to 100%.
Obtaining anomaly sensors from the research commu-
nity is a difﬁcult process; most sensors are heavily protected
IP or are under active development. We are, however, cur-
rently investigating two additional sensors: one based on
libanomaly and one based on the pH system [22]. We
plan to investigate how to clean training data for these algo-
rithms to help show that our approach extends to other AD
sensors.
Acknowledgements
We would like to thank Yingbo Song for helpful feed-
back and Wei-Jen Li and Vanessa Frias-Martinez for inter-
esting discussions. This material is based on research spon-
sored by the Air Force Research Laboratory under agree-
ment number FA8750-06-2-0221, the Army Research Of-
ﬁce under grant No. DA W911NF-04-1-0442, and by the
National Science Foundation under NSF grants CNS-06-
27473 and CNS-04-26623. We authorize the U.S. Govern-
ment to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon.
Any opinions, ﬁndings, conclusions, or recommendations
expressed in this material are those of the authors and do not
necessarily reﬂect the views of the National Science Foun-
dation.
References
[1] K. G. Anagnostakis, S. Sidiroglou, P. Akritidis, K. Xinidis,
E. Markatos, and A. D. Keromytis. Detecting Targeted At-
tacks Using Shadow Honeypots. In Proceedings of the 14th
USENIX Security Symposium, August 2005.
[2] L. Breiman. Bagging Predictors. Machine Learning,
24(2):123–140, 1996.
[3] P. K. Chan and S. J. Stolfo. Experiments in Multistrategy
In Proceedings of the sec-
Learning by Meta-Learning.
ond international conference on information and knowledge
management, pages 314–323, Washington, DC, 1993.
[4] J. R. Crandall, Z. Su, S. F. Wu, and F. T. Chong. On Deriv-
ing Unknown Vulnerabilities from Zero-Day Polymorphic
and Metamorphic Worm Exploits. In ACM Conference on
Computer and Communications Security, Alexandria, VA,
2005.
[5] G. F. Cretu, A. Stavrou, S. J. Stolfo, and A. D. Keromytis.
Improving the Forensic Utility of
In Workshop on Hot Topics
Data Sanitization:
Anomaly Detection Systems.
in System Dependability (HotDep), 2007.
[6] T. Detristan, T. Ulenspiegel, Y. Malcom, and M. S. von
Underduk. Polymorphic Shellcode Engine Using Spectrum
Analysis. Phrack, 11(61-9), 2003.
[7] T. G. Dietterich. Ensemble Methods in Machine Learning.
Lecture Notes in Computer Science, 1857:1–15, 2000.
[8] P. Domingos. Metacost: A general method for making clas-
In Knowledge Discovery and Data
siﬁers cost-sensitive.
Mining, pages 155–164, 1999.
[9] P. Fogla and W. Lee. Evading Network Anomaly Detection
Systems: Formal Reasoning and Practical Techniques.
In
Proceedings of the 13th ACM Conference on Computer and
Communications Security (CCS), pages 59–68, 2006.
[10] S. Forrest, A. Somayaji, and D. Ackley. Building Diverse
Computer Systems. In Proceedings of the 6th Workshop on
Hot Topics in Operating Systems, pages 67–72, 1997.
[11] Y. Freund and R. E. Schapire. A decision-theoretic general-
ization of on-line learning and an application to boosting. In
European Conference on Computational Learning Theory,
pages 23–37, 1995.
94
[29] K. Wang, J. J. Parekh, and S. J. Stolfo. Anagram: A Con-
tent Anomaly Detector Resistant to Mimicry Attack. In Pro-
ceedings of the Symposium on Recent Advances in Intrusion
Detection (RAID), September 2006.
[30] K. Wang and S. J. Stolfo. Anomalous Payload-based Net-
work Intrusion Detection. In Proceedings of the Symposium
on Recent Advances in Intrusion Detection (RAID), Septem-
ber 2004.
[31] D. Wolpert. Stacked Generalization. In Neural Networks,
volume 5, pages 241–259, 1992.
[12] S. S. Janak Parekh, Ke Wang. Privacy-preserving payload-
based correlation for accurate malicious trafﬁc detection. In
SIGCOMM Workshop on Large Scale Attack Defense, 2006.
[13] C. Kruegel, T. Toth, and E. Kirda. Service Speciﬁc Anomaly
Detection for Network Intrusion Detection. In Symposium
on Applied Computing (SAC), Madrid, Spain, 2002.
[14] R. P. Lippmann and J. Haines. Analysis and Results of the
1999 DARPA Off-Line Intrusion Detection Evaluation. In
Proceedings of the Recent Advances in Intrusion Detection
(RAID 2000), pages 162–182, 2000.
[15] J. McHugh. Testing Intrusion Detection Systems: A Cri-
tique of the 1998 and 1999 DARPA Intrusion Detection Sys-
tem Evaluations as Performed by Lincoln Laboratory. ACM
TISSEC, 3(4):262–291, 2000.
[16] D. Moore
and C. Shannon.
The Spread
of
http://www.
the Code Red Worm (CRv2).
caida.org/analysis/security/code-red/
coderedv2 analysis.xml.
[17] J. Newsome, B. Karp, and D. Song. Polygraph: Automat-
In
ically Generating Signatures for Polymorphic Worms.
IEEE Security and Privacy, Oakland, CA, 2005.
[18] J. J. Parekh. Privacy-Preserving Distributed Event Corrob-
oration. PhD thesis, Columbia University, 2007.
[19] B. Parmanto, M. P. W., and H. R. Doyle. Improving Com-
mittee Diagnosis with Resampling Techniques. Advances in
Neural Information Processing Systems, 8:882–888, 1996.
[20] H. Patil and C. N. Fischer. Efﬁcient Turn-time Monitoring
Using Shadow Processing. In Proceedings of the 2nd Inter-
national Workshop on Automated and Algorithmic Debug-
ging, 1995.
[21] S. Sidiroglou, M. E. Locasto, S. W. Boyd, and A. D.
Keromytis. Building a Reactive Immune System for Soft-
ware Services.
In Proceedings of the USENIX Technical
Conference, June 2005.
[22] A. Somayaji and S. Forrest. Automated Response Using
System-Call Delays. In Proceedings of the 9th USENIX Se-
curity Symposium, August 2000.
[23] Y. Song, M. E. Locasto, A. Stavrou, A. D. Keromytis, and
S. J. Stolfo. On the Infeasibility of Modeling Polymorphic
Shellcode. In ACM Computer and Communications Security
Conference (CCS), 2007.
[24] S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan.
Cost-based Modeling for Fraud and Intrusion Detection: Re-
sults from the JAM Project. In Proceedings of the DARPA
Information Survivability Conference and Exposition (DIS-
CEX), 2000.
[25] K. M. Tan and R. A. Maxion. Why 6? Deﬁning the Oper-
ational Limits of stide, an Anomaly-Based Intrusion Detec-
tor. In Proceedings of the IEEE Symposium on Security and
Privacy, pages 188–201, May 2002.
[26] C. Taylor and C. Gates. Challenging the Anomaly Detection
Paradigm: A Provocative Discussion. In Proceedings of the
15th New Security Paradigms Workshop (NSPW), pages 21–
29, September 2006.
[27] D. Wagner and P. Soto. Mimicry Attacks on Host-Based
Intrusion Detection Systems. In ACM CCS, 2002.
[28] K. Wang, G. Cretu, and S. J. Stolfo. Anomalous Payload-
based Worm Detection and Signature Generation. In Pro-
ceedings of the Symposium on Recent Advances in Intrusion
Detection (RAID), September 2005.
95