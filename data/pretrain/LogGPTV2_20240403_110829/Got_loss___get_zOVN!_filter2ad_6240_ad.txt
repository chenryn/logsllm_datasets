plementations exhibited similar response times. The phys-
ical link has a constant service rate, so that TCP was able
to ﬁnd the proper congestion window to avoid most losses.
On the desktop machines, the vSwitch could support up
to 1.45 Gbps of traﬃc without losses, compared with the
256 Mbps for the laptop machines. However, the maximum
bandwidth through the vSwitch was limited to the 1 Gbps
of the physical link, which was the bottleneck in this case.
Accordingly, we measured loss ratios of less than 0.02%. En-
abling losslessness on such a conﬁguration brings no addi-
tional beneﬁts. However, this result validates the eﬃciency
of our implementation.
In the 1-server setup, the zOVN switch was consistently
better than the lossy VALE switch across all runs. The
Linux Bridge exhibited performance variabilities (up to +19%
improvement for the 16 MTU responses over zOVN, but as
much as –65% degradation over zOVN for 128 MTU re-
sponses). The architecture of the Linux Bridge requires one
extra copy for each packet sent or received. This extra over-
head slows down the workers reducing the pressure on the
vSwitch, thereby reducing packet losses. In the 2-server sce-
nario, the extra overhead was hidden by the physical link
bottleneck.
5.3 Lab-Scale Experiments
Next, we deployed zOVN over the two testbeds described
in Figure 9. We ran a PA workload using 32 VMs with the
System Type
CPU
Total cores
Clock speed [GHz]
Memory [GB]
Physical machines
VMs/machine
Data network
Physical switch
Control network
Linux kernel
Testbed 1
Lenovo T60p
Laptops
1x Intel Core 2
T7600
2
2.33
3
8
4
Testbed 2
IBM System x3550 M4
Rack Servers
2x Intel Xeon
E5-2690
16
2.90
96
4
16
1G Ethernet
HP 1810-8G
wireless
3.0.3 64-bit
10G CEE
IBM RackSwitch G8264
1G wired
3.0.3 64-bit
Figure 9: Real implementation testbeds.
same methodology and ﬂow sizes as in the previous para-
graph.
In addition, we varied the TCP version between
NewReno, Vegas and Cubic. As shown in Figure 9, each
physical machine has two network interfaces. The PA traﬃc
that is subject to measurements ﬂows through an isolated
data network. The workers, aggregators and background
traﬃc generators are started and killed through a separate
control network, which is also used to conﬁgure the data
network before each run and to gather the statistics at the
end without interfering with the experiments.
Testbed 1: Laptops. In Figure 10a and 10b, we report
the mean completion time and performance gain of zero-
loss (Z) over lossy (L). The zero-loss conﬁguration has ﬂow
control enabled both in the physical and the virtual network,
whereas the lossy conﬁguration has no ﬂow control in any
of the two networks. The mean ﬂow completion time was
reduced by a factor of up to 19.1×. The highest beneﬁt was
achieved for ﬂow sizes between 6 KB and 48 KB (4 and 32
packets). For very small ﬂows, the total size of all worker
responses was too small to cause any buﬀer overﬂow. For
long ﬂows, the losses were recovered through fast-retransmit
and selective acknowledgments. All TCP versions performed
about equally.
In Figure 10c and 10d, we report the same metrics, but
with background traﬃc.
In this scenario, each VM hosts
an additional traﬃc generator producing background ﬂows.
The generator chooses a random uniformly distributed desti-
nation, then it sends to it a TCP ﬂow with the length drawn
from the distribution in Figure 8a. Afterward, the generator
sleeps according to the background ﬂow inter-arrival distri-
bution shown in Figure 8b. Both the PA and the background
ﬂows use the same TCP version. The gain is smaller than
in the previous scenario, because the background ﬂows also
beneﬁt from losslessness obtaining a higher throughput. In
particular, the congestion window of NewReno and Cubic
are kept open due to the absence of losses. On the other
hand, the latency sensitive Vegas injects background traﬃc
at a lower rate, thus the completion times are shorter.
Testbed 2: Rack Servers. We repeat the above exper-
iments on 4 rack servers with a 10G CEE network. Each
100101102100101102103Mean Completion Time [ms]Query Response [Pkts]LinuxBridgeVALEzOVN100101102103100101102103Mean Completion Time [ms]Query Response [Pkts]LinuxBridgeVALEzOVN 0 0.2 0.4 0.6 0.8 110-510-410-310-210-1100101102Background CDFFlow Sizes [MB]TCPUDP 0 0.2 0.4 0.6 0.8 1100101102103104CDFInter Arrivals [us]BackgroundPartition/AggregateControlNetworkPhysical SwitchPhysical machine 1VM k...VM 1Data NICvSwitch...Physical machine 2VM k...VM 1Data NICvSwitchPhysical machine NVM k...VM 1Data NICvSwitchCommandsStatisticsCtrl NICCtrl NICCtrl NIC430(a) w/o bkgd.
(b) w/o bkgd.
(c) with bkgd.
(d) with bkgd.
Figure 10: Testbed 1 results: 32 VMs PA running on 8 laptops.
(a) w/o bkgd.
(b) w/o bkgd.
(c) with bkgd.
(d) with bkgd.
Figure 11: Testbed 2 results: 32 VMs PA running on 4 rack servers.
server hosts 16 VMs: 8 for PA traﬃc and 8 VMs for gener-
ating background traﬃc. We studied four ﬂow control con-
ﬁgurations: no ﬂow control (LL), ﬂow control activated in
the physical network (LZ), ﬂow control activated in the vir-
tual network (ZL), and ﬂow control activated in both (ZZ).
The mean completion times and gains over LL are reported
in Figure 11a and 11b. The mean completion times are re-
duced by a factor up to 15.95×, similar to the laptop experi-
ments. Although the server CPUs have more resources than
the laptop CPUs, they have to handle more VMs and more
traﬃc from a 10× faster network. Activating ﬂow control
only in the physical network (LZ) showed no major bene-
ﬁt in this scenario, where the primary bottleneck is in the
vSwitches. Also, enabling ﬂow control only in the vSwitch
(ZL) shifted the drop point from the virtual to the physical
domain. Finally, in Figure 11c and 11d, we repeated the ex-
periments with background traﬃc, conﬁrming the ﬁndings
from Testbed 1.
5.4 Simulation Experiments
To ﬁnalize our validation, we implemented a model of the
zOVN system on top of the OMNeT++ network simulator.
The simulator models a 10G CEE fabric at frame level with
generic input-buﬀered output-queued switches. As the TCP
models implemented in OMNeT++, as well as those from
NS2/3, are highly simpliﬁed, we ported the TCP stack from
a FreeBSD v9 kernel into this simulator with only minimal
changes, most of them related to memory management. As
we focus on the network, we did not model the endnode
CPUs, assuming that the endnodes can process the seg-
ments as fast as they arrive, and that the applications can
reply immediately. The stack adds only a ﬁxed delay to
each segment, calibrated from our prior hardware experi-
ments. Even if idealized, these assumptions are consistent
with our network-centric methodology. The simulator also
incorporates a thin UDP layer used for background ﬂows
performing simple segmentation and encapsulation of the
application data.
The zOVN model performs switching and bridging in the
same way as in the testbed experiment. However, here
we chose a diﬀerent encapsulation size of 54B, reﬂecting
a VXLAN-type encapsulation: 18B outer Ethernet header
+ 20B outer IP header + 8B UDP header + 8B VXLAN
header. To avoid fragmentation, we decreased the MTU
value accordingly from 1500B to 1446B. Modern CEE hard-
ware is able to increase its physical MTUs, thus preserving
the default settings.
The simulated network topology is shown in Figure 4. It
consists of 256 servers, distributed in 16 chassis, and inter-
connected through a three-layer fat tree. Clients attached
to the up-links inject HTTP queries that are served by the
VMs residing on each virtualized server. The queries were
generated according to the inter-arrival times shown in Fig-
ure 8b. Each server hosts 3 VMs, one HLA, one MLA and
one worker. The client query reaches a randomly chosen
HLA that in turns chooses 16 MLAs, one in each chassis.
Each MLA contacts all worker VMs from the same chas-
sis. The messages exchanged between the HLA, MLAs and
workers have a ﬁxed size of 20KB.
Figure 12 compares the mean completion times and the
5- and 95-percentiles for diﬀerent ﬂow control conﬁgurations
under no, light, and heavy background traﬃc. We stud-
ied the four ﬂow control conﬁgurations introduced above
(LL, LZ, ZL, and ZZ) and the same three TCP versions as
before. Enabling ﬂow control in only one network (either
physical or virtual) is not beneﬁcial, because packet losses
100101102103104100101102103Mean Completion Time [ms]Query Response [Pkts]Cubic LCubic ZNReno LNReno ZVegas LVegas Z 0 5 10 15 20 25100101102103Gain Ratio over LossyQuery Response [Pkts]CubicNRenoVegas100101102103104100101102103Mean Completion Time [ms]Query Response [Pkts]Cubic LCubic ZNReno LNReno ZVegas LVegas Z 0 5 10 15 20 25100101102103Gain Ratio over LossyQuery Response [Pkts]CubicNRenoVegas100101102103CubicLLLZZLZZ100101102103Mean Completion Time [ms]New Reno100101102103100101102103VegasQuery Response [Pkts] 5 10 15 20 25CubicLLLZZLZZ 5 10 15 20 25Gain Ratio Over LLNew Reno 0 5 10 15 20 25100101102103VegasQuery Response [Pkts]100101102103CubicLLLZZLZZ100101102103Mean Completion Time [ms]New Reno100101102103100101102103VegasQuery Response [Pkts] 2 4 6 8 10CubicLLLZZLZZ 2 4 6 8 10Gain Ratio Over LLNew Reno 0 2 4 6 8 10100101102103VegasQuery Response [Pkts]431(a) w/o bkgd
(b) TCP bkgd
(c) UDP bkgd
Figure 12: Simulation results: 768 VMs PA with 256 servers.
are merely shifted from one domain to the other. However,
the eﬀects were not altogether identical, because the virtual
ﬂow control still beneﬁted inter-VM communications on the
same host. Therefore, enabling only the virtual ﬂow con-
trol (ZL) still led to a performance improvement, although
smaller than in the ZZ case. Enabling both ﬂow controls
(ZZ) achieved signiﬁcant gains, similar to those observed in
the testbed: a reduction in FCT of up to 10.1× with Cubic,
and no background ﬂows. When adding light background
traﬃc, we observed similar gain decreases. However, a new
insight is that in the presence of heavy UDP background
traﬃc, enabling ﬂow control will harm performance. In this
case, the uncooperative background UDP packets did no
longer get dropped and, consequently, hogged link capacity
and harmed the foreground PA workload traﬃc. These re-
sults conﬁrmed the need to segregate the traﬃc into PFC
priorities with true resource separation and scheduling. It
may also suggest the need for a layer-2 congestion manage-
ment loop as in [18].
With background traﬃc, Vegas outperformed NewReno
and Cubic, conﬁrming the results obtained on the testbed
setups.
In the case without background traﬃc Vegas was
again better. Nonetheless, on the testbeds, all TCP versions
produced similar results. The diﬀerence here is due to the
more complex communication pattern with more hops, as
more ﬂows share the same path. This causes longer queues,
especially in the core switches. The longer delays are de-
tected by Vegas, which will reduce its congestion window,
thus obtaining shorter completion times.
6. DISCUSSION
Here we review the main takeaways from the results pre-
sented in this paper. Using zOVN’s experimental platform,
we demonstrated both absence of packet drops – in support
of converged storage and HPC applications – and improved
ﬂow completion time (FCT) performance. Thus, we have
achieved our primary objective of reconciling performance
with losslessness for overlay virtual networks.
Is lossless ﬂow control more relevant for physical or for vir-
tual networks? Having tested all four combinations of lossy
and lossless physical and virtual ﬂow control both in our
testbed and in simulations, we found that contiguous end-
to-end ﬂow control, hop-by-hop within each domain, yields
the largest reductions in FCT: PA over zOVN with 32 vir-
tual workers distributed across four physical rack servers
achieved up to 15-fold peak speedup. Relevant to on-line and
data-intensive workloads in general, the highest speedups
recorded are for ﬂows between 6 and 50 KB. Unexpectedly, if
a suboptimal choice between ﬂow control in either the phys-
ical or the virtual network must still be made, the latter is
better for FCT performance, as demonstrated by the results
for ZL vs. LZ in Figure 12. As noted initially, this situation
entails a paradoxical twist: Although CEE and InﬁniBand
fabrics have already implemented the costlier (buﬀers, logic,
and signaling) hardware ﬂow control, this remains practi-
cally non-existent in today’s virtual networks - despite much
lower implementation eﬀorts.
Are our modest experimental platforms relevant for hun-
dreds of blade-based racks and top-of-rack switches with 40-
100 Gbps uplinks? While the deﬁnitive answer would entail
a multi-million dollar datacenter setup, we are conﬁdent in
the relevance of our admittedly limited prototype platforms.
Thin and embedded low-power CPUs as used in microservers
as well as fully virtualized, and hence loaded, “fat” CPUs
are likely to exhibit qualitatively similar behaviors as these
measured on our two testbeds.
During zOVN experiments we consistently observed how
the loss ratio is inﬂuenced by the CPU/network speed ra-
tio. On the transmit side, a fast Intel Xeon2 CPU can easily
overload a slower 1G network, producing more losses in the
vSwitch than a slower CPU (Intel Core 2) with the same
1G NIC does. On the other hand, on the receive side, a
fast 10G network coupled with a loaded Intel Xeon CPU
produces more drops than the 1G network with the same