71.80%
adv-train
acc
N.A.
adv-test
acc
N.A.
ver-train
acc
N.A.
ver-test
acc
N.A.
inference
acc (IB)
71.50%
inference
acc (IA)
N.A.
inference
acc (IV)
N.A.
0.25/255
100%
73.10%
99.99%
69.84%
99.99%
68.18%
76.13%
0.5/255
99.98%
69.29%
99.98%
64.51%
99.97%
60.89%
77.06%
0.75/255
100%
65.25%
99.95%
59.49%
99.85%
54.71%
77.99%
1/255
99.78%
63.96%
99.44%
57.06%
98.61%
50.74%
76.30%
1.25/255
98.46%
61.79%
97.30%
53.76%
95.36%
46.70%
74.07%
1.5/255
96.33%
60.97%
94.27%
51.72%
90.19%
44.23%
71.08%
76.18%
77.36%
78.50%
77.05%
75.10%
72.29%
76.04%
77.09%
78.20%
77.16%
75.41%
72.69%
at the beginning, robustness performance gets improved, and we
also have a higher membership inference accuracy. However, when
the model capacity is too large, the robustness performance and
the membership inference accuracy begin decreasing, since now
the verified robust loss becomes too loose.
6.3 Reducing the Size of Training Set
In this subsection, we further prove our hypothesis by showing
that when the size of the training set is reduced so that the model
can fit well on the reduced dataset, the verifiable defense method
indeed leads to an increased membership inference accuracy.
We choose the duality-based verifiable defense method [61, 62]
and train the CIFAR10 classifier with a normal ResNet architecture:
3 groups of residual layers with output channel numbers (16, 32,
64) and only 1 residual unit for each group. The whole CIFAR10
training set have too many points to be robustly fitted with the
verifiable defense algorithm: the robust CIFAR10 classifier [62] with
ϵ = 2/255 has the train accuracy below 70%. Therefore, we select a
subset of the training data to robustly train the model by randomly
choosing 1000 (20%) training images for each class label. We vary
the perturbation budget value (ϵ) in order to observe when the
model capacity is not large enough to fit on this partial CIFAR10
set using the verifiable training algorithm [61].
We show the obtained results in Table 10, where the natural
model has a low test accuracy (below 75%) and high privacy leakage
Figure 4: Verified train accuracy and membership inference
accuracy using inference strategy IV for robust Yale Face
classifiers [16] with varying capacities. The model with a ca-
pacity scale of s contains 4 convolution blocks with output
channel numbers (s, 2s, 4s, 8s), as described in Section 4.
.
(inference accuracy is 71.50%) since we only use 20% training exam-
ples to learn the classifier. By using the verifiable defense method
[61], the verifiably robust models have increased member-
ship inference accuracy values, for all ϵ values. We can also
see that when increasing the ϵ values, at the beginning, the ro-
bust model is more and more susceptible to membership inference
attacks (inference accuracy increases from 71.50% to 78.50%). How-
ever, beyond a threshold of ϵ = 1/255, the inference accuracy starts
to decrease, since a higher ϵ requires a model with a larger capacity
to fit well on the training data.
7 DISCUSSIONS
In this section, we first evaluate the success of membership infer-
ence attacks when the adversary does not know the l∞ perturbation
constraints of robust models. Second we discuss potential coun-
termeasures, including temperature scaling and regularization, to
reduce privacy risks. Finally, we discuss the relationship between
training data privacy and model robustness.
7.1 Membership Inference Attacks with
Varying Perturbation Constraints
Our experiments so far considered an adversary with prior knowl-
edge of the robust model’s l∞ perturbation constraint. Next, we
evaluate privacy leakage of robust models in the absence of such
prior knowledge by varying perturbation budgets used in the mem-
bership inference attack. Specifically, we perform membership in-
ference attacks IA with varying perturbation constraints against
robust Yale Face classifiers [33, 50, 66], which are robustly trained
with the l∞ perturbation budget (ϵ) of 8/255.
We present the membership inference results in Figure 5, where
the inference strategy IA with the perturbation budget of 0 is equiv-
alent to the inference strategy IB. In general, we observe a higher
membership inference accuracy when the perturbation budget used
in the inference attack is close to the robust model’s exact perturba-
tion constraint. An attack perturbation budget that is very small will
Figure 5: Membership inference accuracy on robust Yale
Face classifiers [33, 50, 66] trained with the l∞ perturbation
constraint of 8/255. The privacy leakage is evaluated via the
inference strategy IA based on adversarial examples gener-
ated with varying perturbation budgets.
not fully utilize the classifier’s structural characteristics, leading to
high robustness performance for adversarial examples generated
from both training and test data. On the other hand, a very large
perturbation budget leads to low accuracy on adversarial examples
generated from both training training data and test data. Both of
these scenarios will reduce the success of membership inference
attacks.
Based on results shown in Figure 5, the adversary does not need
to know the exact value of robust model’s l∞ perturbation budget:
approximate knowledge of ϵ suffices to achieve high membership
inference accuracy. Furthermore, the adversary can leverage the
shadow training technique (with shadow training set) [47] in prac-
tice to compute the best attack parameters (the perturbation budget
and the threshold value), and then use the inferred parameters
against the target model. The best perturbation budget may not
even be same as the exact ϵ value of robust model. For example,
we obtain the highest membership inference accuracy by setting
ϵ as 9/255 for the PGD-Based Adv-Train Yale Face classifier [33],
and 10/255 for the other two robust classifiers [50, 66]. We observe
similar results for Fashion-MNIST and CIFAR10 datasets, which are
presented in Appendix D.
7.2 Potential Countermeasures
We discuss potential countermeasures that can reduce the risk of
membership inference attacks while maintaining model robustness.
7.2.1 Temperature scaling. Our membership inference strategies
leverage the difference between the prediction confidence of the
target model on its training set and test set. Thus, a straightforward
mitigation method is to reduce this difference by applying temper-
ature scaling on logits [17]. The temperature scaling method was
shown to be effective to reduce privacy risk for natural (baseline)
models by Shokri et al. [47], while we are studying its effect for
robust models here.
Temperature scaling is a post-processing calibration technique
for machine learning models that divides logits by the tempera-
ture, T , before the softmax function. Now the model prediction
probability can be expressed as
where T = 1 corresponds to original model prediction. By setting
T > 1, the prediction confidence F(x)y is reduced, and when T →
∞, the prediction output is close to uniform and independent of
the input, thus leaking no membership information while making
the model useless for prediction.
F(x)i =
k−1
exp(д(x)i/T)
j =0 exp(д(x)j/T) ,
(21)
the logits: two multivariate Gaussian distributions for the logits of
benign examples and adversarial examples are computed, and l1
distances between two mean vectors and two covariance matrices
are added into the training loss.
Table 11: Membership inference attacks against robust mod-
els [33], where the perturbation budget ϵ is 8/255 for the Yale
Face datset, and 0.1 for the Fashion-MNIST dataset. When us-
ing DA, we modify the robust training algorithm by adding
the regularization loss proposed by Song et al. [51].
Dataset
Yale Face
Yale Face
Fashion
MNIST
Fashion
MNIST
no
yes
no
yes
using
DA [51]?
train
acc
test
acc
adv-train
acc
99.89%
96.69%
99.00%
adv-test
acc
77.63%
inference
acc (IB)
61.69%
inference
acc (IA)
68.83%
99.32%
94.75%
99.26%
88.52%
60.73%
99.93%
90.88%
96.91%
68.06%
58.32%
88.97%
86.98%
81.59%
78.65%
51.19%
63.14%
64.49%
51.49%
Figure 6: Membership inference accuracy on robust Yale
Face and Fashion-MNIST classifiers [33] with varying soft-
max temperature values [17].
We apply the temperature scaling technique on the robust Yale
Face and Fashion-MNIST classifiers using the PGD-based adversar-
ial training defense [33] and investigate its effect on membership
inference. We present membership inference results (both IB and
IA) for varying temperature values (while maintaining the same
classification accuracy) in Figure 6. We can see that increasing the
temperature value decreases the membership inference accuracy.
7.2.2 Regularization to improve robustness generalization. Regular-
ization techniques such as parameter norm penalties and dropout
[54], are typically used during the training process to solve over-
fitting issues for machine learning models. Shokri et al. [47] and
Salem et al. [41] validate their effectiveness against membership
inference attacks. Furthermore, Nasr et al. [36] propose to measure
the performance of membership inference attack at each training
step and use the measurement as a new regularizer.
The above mitigation strategies are effective regardless of natural
or robust machine learning models. For the robust models, we
can also rely on the regularization approach, which improves the
model’s robustness generalization. This can mitigate membership
inference attacks, since a poor robustness generalization leads to a
severe privacy risk. We study the method proposed by Song et al.
[51] to improve model’s robustness generalization and explore its
performance against membership inference attacks.
The regularization method in [51] performs domain adaptation
(DA) [57] for the benign examples and adversarial examples on
We apply this DA-based regularization approach on the PGD-
based adversarial training defense [33] to investigate its effective-
ness against membership inference attacks. We list the experimental
results both with and without the use of DA regularization for Yale
Face and Fashion-MNIST datasets in Table 11. We can see that the
DA-based regularization can decrease the gap between adversarial
train accuracy and adversarial test accuracy (robust generalization
error), leading to a reduction in membership inference risk.
7.3 Privacy vs Robustness
We have shown that there exists a conflict between privacy of train-
ing data and model robustness: all six robust training algorithms
that we tested increase models’ robustness against adversarial exam-
ples, but also make them more susceptible to membership inference
attacks, compared with the natural training algorithm. Here, we
provide further insights on how general this relationship between
membership inference and adversarial robustness is.
7.3.1 Beyond image classification. Our experimental evaluation so
far focused on the image classification domain. Next, we evaluate
the privacy leakage of a robust model in a domain different than
image classification to observe whether the conflict between privacy
and robustness still holds.
We choose the UCI Human Activity Recognition (HAR) dataset
[3], which contains measurements of a smartphone’s accelerometer
and gyroscope values while the participants holding it performed
one of six activities (walking, walking upstairs, walking downstairs,
sitting, standing, and laying). The dataset has 7,352 training samples
and 2,947 test samples. Each sample is a 561-feature vector with
time and frequency domain variables of smartphone sensor values,
and all features are normalized and bounded within [-1,1].
To train the classifiers, we use a 3-layer fully connected neural
network with 1,000, 100, and 6 neurons respectively. For robust
training, we follow Wong and Kolter [61] by using the l∞ pertur-
bation constraint with the size of 0.05, and apply the PGD-based
adversarial training [33]. The results for membership inference
Table 12: Membership inference attacks against natural and
empirically robust models [33] on the HAR dataset with a l∞
perturbation constraint ϵ = 0.05. Based on Equation (16), the
natural model has an inference advantage of 10.72%, while
the robust model has an inference advantage of 20.26%.
Training
method
Natural
train
acc
100%
test
acc
adv-train
acc
96.61%
33.56%
adv-test
acc
29.69%
inference
acc (IB)
55.36%
PGD-Based
Adv-Train [33]
96.10%
92.53%
92.51%
73.84%
58.29%
inference
acc (IA)
55.03%
60.13%
attacks against the robust classifier and its naturally trained coun-
terpart are presented in Table 12. We can see that the robust train-
ing algorithm still leaks more membership information: the robust
model has a 2× membership inference advantage (Equation (16))
over the natural model.
Is the conflict a fundamental principle? It is difficult to judge
7.3.2
whether the privacy-robustness conflict is fundamental or not: will a
robust training algorithm inevitably increase the model risk against
membership inference attacks, compared to the natural training
algorithm? On the one hand, there is no direct tension between
privacy of training data and model robustness. We have shown in
Section 5.2.2 that the privacy leakage of robust model is related to
its generalization error in the adversarial setting. The regulariza-
tion method in Section 7.2.2, which improves the adversarial test
accuracy and decreases the generalization error, indeed helps to
decrease the membership inference accuracy.
On the other hand, our analysis verifies that state-of-the-art ro-
bust training algorithms [16, 33, 34, 50, 61, 66] magnify the influence
of training data on the model by minimizing the loss over a lp ball
of each training point, leading to more training data memorization.
In addition, we find that a recently-proposed robust training algo-
rithm [29], which adds a noise layer for robustness, also leads to an
increase of membership inference accuracy in Appendix E. These
robust training algorithms do not achieve good generalization of
robustness performance [42, 51]. For example, even the regularized
Yale Face classifier in Table 11 has a generalization error of 11%