the application level does not offer perfect control of the network
or protection against misbehaving hosts, it still gives considerable
Figure 4: Orchestra architecture. An Inter-Transfer Controller
(ITC) manages Transfer Controllers (TCs) for the active trans-
fers. Each TC can choose among multiple transfer mechanisms
depending on data size, number of nodes, and other factors.
The ITC performs inter-transfer scheduling.
Table 1: Coordination throughout the Orchestra hierarchy.
ﬂexibility to improve job performance. We therefore took this ap-
proach as a sweet-spot between utility and deployability.
In the next few sections, we present the components of Orches-
tra in more detail. First, we discuss inter-transfer scheduling and
the interaction between TCs and the ITC in Section 4. Sections 5
and 6 then discuss two efﬁcient transfer mechanisms that take ad-
vantage of the global control provided by Orchestra. For broadcast,
we present Cornet, a BitTorrent-like scheme that can take into ac-
count the cluster topology. For shufﬂe, we present Weighted Shuf-
ﬂe Scheduling (WSS), an optimal shufﬂe scheduling algorithm.
4
Typically, large computer clusters are multi-user environments where
hundreds of jobs run simultaneously [29, 43]. As a result, there
are usually multiple concurrent data transfers. In existing clusters,
without any transfer-aware supervising mechanism in place, ﬂows
from each transfer get some share of the network as allocated by
TCP’s proportional sharing. However, this approach can lead to
extended transfer completion times and inﬂexibility in enforcing
scheduling policies. Consider two simple examples:
Inter-Transfer Scheduling
10306090Number of machines050100150200250Time (s)ComputationCommunicationTC (broadcast) TC (broadcast) HDFS Tree Cornet HDFS Tree Cornet TC (shuffle) Hadoop shuffle WSS shuffle broadcast broadcast ITC Fair sharing FIFO Priority Component Coordination Activity Inter-Transfer Controller (ITC) - Implement cross-transfer scheduling policies (e.g., priority, FIFO etc.) - Periodically update and notify active transfers of their shares of the network Transfer Controller (TC) - Select the best algorithm for a transfer given its share and operating regime Cornet Broadcast TC - Use network topology information to minimize cross-rack communication - Control neighbors for each participant Weighted Shuffle Scheduling TC - Assign flow rates to optimize shuffle completion time • Scheduling policies: Suppose that a high-priority job, such as a
report for a key customer, is submitted to a MapReduce cluster.
A cluster scheduler like Quincy [29] may quickly assign CPUs
and memory to the new job, but the job’s ﬂows will still expe-
rience fair sharing with other jobs’ ﬂows at the network level.
• Completion times: Suppose that three jobs start shufﬂing equal
amounts of data at the same time. With fair sharing among
the ﬂows, the transfers will all complete in time 3t, where t is
the time it takes one shufﬂe to ﬁnish uncontested. In contrast,
with FIFO scheduling across transfers, it is well-known that the
transfers will ﬁnish faster on average, at times t, 2t and 3t.
Orchestra can implement scheduling policies at the transfer level
through the Inter-Transfer Controller (ITC). The main design ques-
tion is what mechanism to use for controlling scheduling across the
transfers. We have chosen to use weighted fair sharing at the clus-
ter level: each transfer is assigned a weight, and each congested
link in the network is shared proportionally to the weights of the
transfers using that link. As a mechanism, weighted fair sharing is
ﬂexible enough to emulate several policies, including priority and
FIFO. In addition, it is attractive because it can be implemented at
the hosts, without changes to routers and switches.
When an application wishes to perform a transfer in Orchestra,
it invokes an API that launches a TC for that transfer. The TC
registers with the ITC to obtain its share of the network. The ITC
periodically consults a scheduling policy (e.g., FIFO, priority) to
assign shares to the active transfers, and sends these to the TCs.
Each TC can divide its share among its source-destination pairs
as it wishes (e.g., to choose a distribution graph for a broadcast).
The ITC also updates the transfers’ shares periodically as the set of
active transfers changes. Finally, each TC unregisters itself when
its transfer ends. Note that we assume a cooperative environment,
where all the jobs use TCs and obey the ITC.
We have implemented fair sharing, FIFO, and priority policies
to demonstrate the ﬂexibility of Orchestra. In the long term, how-
ever, operators would likely integrate Orchestra’s transfer schedul-
ing into a job scheduler such as Quincy [29].
In the rest of this section, we discuss our prototype implemen-
tation of weighted sharing using TCP ﬂow counts (§4.1) and the
scalability and fault tolerance of the ITC (§4.2).
4.1 Weighted Flow Assignment (WFA)
To illustrate the beneﬁts of transfer-level scheduling, we imple-
mented a prototype that approximates weighted fair sharing by al-
lowing different transfers to use different numbers of TCP connec-
tions per host and relies on TCP’s AIMD fair sharing behavior. We
refer to this strategy as Weighted Flow Assignment (WFA). Note
that WFA is not a bulletproof solution for inter-transfer scheduling,
but rather a proof of concept for the Orchestra architecture. Our
main contribution is the architecture itself. In a full implementa-
tion, we believe that a cross-ﬂow congestion control scheme such
as Seawall [38] would improve ﬂexibility and robustness. Seawall
performs weighted max-min fair sharing between applications that
use an arbitrary number of TCP and UDP ﬂows using a shim layer
on the end hosts and no changes to routers and switches.1
In WFA, there is a ﬁxed number, F , of permissible TCP ﬂows
per host (e.g., 100) that can be allocated among the transfers. Each
transfer i has a weight wi allocated by the scheduling policy in
(cid:101) TCP
connections, where the sum is over all the transfers using that host.
the ITC. On each host, transfer i is allowed to use F(cid:100) wi(cid:80) wj
1In Orchestra, we can actually implement Seawall’s congestion
control scheme directly in the application instead of using a shim,
because we control the shufﬂe and broadcast implementations.
Suppose that two MapReduce jobs (A and B) sharing the nodes of a
cluster are both performing shufﬂes. If we allow each job to use 50
TCP connections per host, then they will get roughly equal shares
of the bandwidth of each link due to TCP’s AIMD behavior. On
the other hand, if we allowed job A to use 75 TCP ﬂows per host
and job B to use 25 TCP ﬂows per host, then job A would receive
a larger share of the available bandwidth.2
Our implementation also divides all data to be transferred into
chunks, so that a TC can shut down or launch new ﬂows rapidly
when its share in a host changes. In addition, for simplicity, we give
transfers a 1.5× higher cap for sending ﬂows than for receiving on
each host, so that a TC does not need to micromanage its ﬂows to
have sender and receiver counts match up exactly.
We found that our ﬂow count approach works naturally for both
broadcast and shufﬂe. In a BitTorrent-like broadcast scheme, nodes
already have multiple peers, so we simply control the number of
concurrent senders that each node can receive from.
In shufﬂe
transfers, existing systems already open multiple connections per
receiver to balance the load across the senders, and we show in Sec-
tion 6 that having more connections only improves performance.
WFA has some limitations—for example, it does not share a link
used by different numbers of sender/receiver pairs from different
transfers in the correct ratio. However, it works well enough to il-
lustrate the beneﬁts of inter-transfer scheduling. In particular, WFA
will work well on a full bisection bandwidth network, where the
outgoing links from the nodes are the only congestion points.
4.2 Fault Tolerance and Scalability
Because the ITC has fairly minimal responsibilities (notifying each
TC its share), it can be made both fault tolerant and scalable. The
ITC stores only soft state (the list of active transfers), so a hot
standby can quickly recover this state if it crashes (by having TCs
reconnect). Furthermore, existing transfers can continue while the
ITC is down. The number of active transfers is no more than several
hundred in our 3000-node Facebook trace, indicating that scalabil-
ity should not be a problem. In addition, a periodic update interval
of one second is sufﬁcient for setting shares, because most transfers
last seconds to minutes.
5 Broadcast Transfers
Cluster applications often need to send large pieces of data to mul-
tiple machines. For example, in the collaborative ﬁltering algo-
rithm in Section 2, broadcasting an O(100 MB) parameter vector
quickly became a scaling bottleneck. In addition, distributing ﬁles
to perform a fragment-replicate join3 in Hadoop [6], rolling out
software updates [8], and deploying VM images [7] are some other
use cases where the same data must be sent to a large number of
machines. In this section, we discuss current mechanisms for im-
plementing broadcast in datacenters and identify several of their
limitations (§5.1). We then present Cornet, a BitTorrent-like pro-
tocol designed speciﬁcally for datacenters that can outperform the
default Hadoop implementation by 4.5× (§5.2). Lastly, we present
a topology-aware variant of Cornet that leverages global control to
further improve performance by up to 2× (§5.3).
5.1 Existing Solutions
One of the most common broadcast solutions in existing cluster
computing frameworks involves writing the data to a shared ﬁle
system (e.g., HDFS [2], NFS) and reading it later from that cen-
tralized storage. In Hadoop, both Pig’s fragment-replicate join im-
2Modulo the dependence of TCP fairness on round-trip times.
3This is a join between a small table and a large table where the
small table is broadcast to all the map tasks.
plementation [6] and the DistributedCache API for deploying code
and data ﬁles with a job use this solution. This is likely done out of
a lack of other readily available options. Unfortunately, as the num-
ber of receivers grows, the centralized storage system can quickly
become a bottleneck, as we observed in Section 2.
To eliminate the centralized bottleneck, some systems use d-ary
distribution trees rooted at the source node. Data is divided into
blocks that are passed along the tree. As soon as a node ﬁnishes
receiving the complete data, it can become the root of a separate
tree. d is sometimes set to 1 to form a chain instead of a tree
(e.g., in LANTorrent [7] and in the protocol for writing blocks in
HDFS [2]). Unfortunately, tree and chain schemes suffer from two
limitations. First, in a tree with d > 1, the sending capacity of the
leaf nodes (which are at least half the nodes) is not utilized. Sec-
ond, a slow node or link will slow down its entire subtree, which is
problematic at large scales due to the prevalence of stragglers [15].
This effect is especially apparent in chains.
Unstructured data distribution mechanisms like BitTorrent [4],
traditionally used in the Internet, address these drawbacks by pro-
viding scalability, fault-tolerance, and high throughput in heteroge-
neous and dynamic networks. Recognizing these qualities, Twitter
has built Murder [8], a wrapper over the BitTornado [3] implemen-
tation of BitTorrent, to deploy software to its servers.
5.2 Cornet
Cornet is a BitTorrent-like protocol optimized for datacenters. In
particular, Cornet takes advantage of the high-speed and low-latency
connections in datacenter networks, the absence of selﬁsh peers,
and the fact that there is no malicious data corruption. By leverag-
ing these properties, Cornet can outperform BitTorrent implemen-
tations for the Internet by up to 4.5×.
Cornet differs from BitTorrent in three main aspects:
• Unlike BitTorrent, which splits ﬁles into blocks and subdivides
blocks into small chunks with sizes of up to 256 KB, Cornet
only splits data into large blocks (4 MB by default).
• While in BitTorrent some peers (leechers) do not contribute to
the transfer and leave as soon as they ﬁnish the download, in
Cornet, each node contributes its full capacity over the full du-
ration of the transfer. Thus, Cornet does not include a tit-for-tat
scheme to incentivize nodes.
• Cornet does not employ expensive SHA1 operations on each
data block to ensure data integrity; instead, it performs a single
integrity check over the whole data.
Cornet also employs a cap on the number of simultaneous con-
nections to improve performance.4 When a peer is sending to the
maximum number of recipients, it puts further requests into a queue
until one of the sending slots becomes available. This ensures faster
service times for the small number of connected peers and allows
them to ﬁnish quickly to join the session as the latest sources for
the blocks they just received.
During broadcast, receivers explicitly request for speciﬁc blocks
from their counterparts. However, during the initial stage, the source
of a Cornet broadcast sends out at least one copy of each block in a
round-robin fashion before duplicating any block.
The TC for Cornet is similar to a BitTorrent tracker in that it
assigns a set of peers to each node. However, unlike BitTorrent,
each node requests new peers every second. This allows the TC to
adapt to the topology and optimize the transfer, as we discuss next.
4The default limits for the number of receive and send slots per
node are 8 and 12, respectively.
5.3 Topology-Aware Cornet
Many datacenters employ hierarchical network topologies with over-
subscription ratios as high as 5 [21, 27], where transfer times be-
tween two nodes on the same rack are signiﬁcantly lower than be-
tween nodes on different racks. To take network topology into ac-
count, we have developed two extensions to Cornet.
CornetTopology In this case, we assume that the network topol-
ogy is known in advance, which is appropriate, for example, in
private datacenters. In CornetTopology, the TC has a conﬁgura-
tion database that speciﬁes locality groups, e.g., which rack each
node is in. When a receiver requests for a new set of peers from
the TC, instead of choosing among all possible recipients (as in
vanilla Cornet), the TC gives priority to nodes on the same rack
as the receiver. Essentially, each rack forms its individual swarm
with minimal cross-rack communication. The results in Section 7.2
show that CornetTopology can reduce broadcast time by 50%.
CornetClustering In cloud environments, users have no control
over machine placements, and cloud providers do not disclose any
information regarding network topology. Even if the initial place-
ments were given out, VM migrations in the background could in-
validate this information. For these cases, we have developed Cor-
netClustering, whose goal is to infer and exploit the underlying net-
work topology. It starts off without any topology information like
the vanilla Cornet. Throughout the course of an application’s life-
time, as more and more broadcasts happen, the TC records block
transfer times between different pairs of receivers and uses a learn-
ing algorithm to infer the rack-level topology. Once we infer the
topology, we use the same mechanism as in CornetTopology. The
TC keeps recalculating the inference periodically to keep an up-
dated view of the network.
The inference procedure consists of four steps. In the ﬁrst step,
we record node-to-node block transfer times. We use this data to
construct an n × n distance matrix D, where n is the number of
receiver nodes, and the entries are the median block transfer times
between a pair of nodes. In the second step, we infer the missing
entries in the distance matrix using a version of the nonnegative
matrix factorization procedure of Mao and Saul [33]. After com-
pleting the matrix D, we project the nodes onto a two-dimensional