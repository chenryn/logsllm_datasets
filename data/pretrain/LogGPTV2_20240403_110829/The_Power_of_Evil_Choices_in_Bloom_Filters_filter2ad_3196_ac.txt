)} such that Ix(cid:2) ∩ Ix (cid:2)= φ . Consequently,
{h1(x(cid:2)
deleting the item x(cid:2) from the ﬁlter would decrease at least
one counter for x and hence the item x also gets deleted. The
(cid:4)k
probability to ﬁnd such an x(cid:2) is given by:
), . . . , hk(x(cid:2)
(cid:3)
1 −
W − |Ix|
m
.
We highlight that deletion of an item may require other
deletions. These deletions may remove several other items
from the ﬁlter as a side effect.
To summarize, our attacks against Bloom ﬁlter can be
classiﬁed depending on the bit of the ﬁlter targeted by the
adversary. The chosen-insertion adversary generates pre-image
on the 0 of a Bloom ﬁlter. The query-only adversary generates
pre-image on the 1 of a Bloom ﬁlter and sends queries for
which the bits at the ﬁrst k − 1 indexes are set to 1, while
the last index could be either 0 or 1. The deletion adversary
targets certain 1 in the ﬁlter. In each case, we consider brute
105105
force search: an item is selected at random and its k indexes
are computed. If the bit in the ﬁlter at any of these indexes is
already set to 1 or 0 depending on the adversary, the item is
discarded and a new one is tried.
Feasibility of our attacks: We highlight that our attacks
on Bloom ﬁlter based applications are feasible since either
non-cryptographic hash functions are used or cryptographic
digests are truncated. While non-cryptographic hashes can be
easily broken, truncation of cryptographic digests drastically
reduces the attack complexity. Furthermore, the probability of
success of our attacks is higher than the generally accepted
(second) pre-image attacks on hash functions. Table I presents
the summary of the success probability of our attacks.
TABLE I: Comparative summary of our attacks for a Bloom
ﬁlter of Hamming weight W .
Attack
Probability
(Second) pre-image (hash function)
(Second) pre-image (Bloom ﬁlter)
Pollution
False positive forgery
Dummy query
Deletion
(cid:3)k ≤ (cid:2)
(cid:3)k
k
m
1
2(cid:2)
(cid:2)
(cid:4)
W
m
m−W
(cid:5)
k
mk
(cid:3)k
(cid:5)
1
2
W
k−1
(cid:2)
(cid:4)
(cid:3)k ≤ (cid:2)
W
m
(m−W )·
mk
W −k
(cid:4)
1 −
m
(cid:5)k ≤ 1 −
(cid:4)
W −|Ix|
m
(cid:5)k
From the computed probabilities (see Table I), it is possible
to order each attack in terms of their computational feasibility.
The pollution attack has the highest success probability. The
most difﬁcult attack is the deletion one. Between these two
attacks, lies the difﬁculty of mounting query attacks which
depend on the number of items inserted in the ﬁlter.
In order to put our adversary models to test and measure
the impact of our attacks, in the following sections we focus on
three critical applications of Bloom ﬁlters: web spider, spam
ﬁlter and web cache.
V. BLINDING SOME SPECIES OF (WEB)SPIDERS
A spider, also known as a robot or a crawler is a mecha-
nism for bulk discovering or downloading of publicly available
web pages from the world wide web. Recently, crawlers have
been appositely employed to design automatic tools to track
the web for personalized topics and notify clients of pages
that match these topics [18]. Interested readers may consult
the survey by Olston and Najork [19] for more applications
and further details.
In this section, we ﬁrst present SCRAPY [20], a high
level screen scraping and web crawling framework, used to
crawl websites and extract structured data from their pages.
It has been employed for wide range of applications, varying
from data mining, monitoring, to information processing and
automated testing, among others1. In the sequel, we present
attacks against SCRAPY.
1http://scrapy.org/companies/
106106
A. Scrapy: simpliﬁed architecture
SCRAPY extracts data from HTML and XML sources
with the help of XPath and provides built-in support for
sanitizing the scraped data using a collection of reusable ﬁlters
shared between the spiders. SCRAPY is capable of performing
crawling at different scales and depths. For instance, it can
crawl speciﬁc domains or pages related to a given topic of
interest.
Execution of a crawling process recursively performs the
following steps. These steps are common to any spider and
hence are not speciﬁc to SCRAPY.
1) Select a URL form the list of scheduled ones.
2) Fetch the URL.
3) Archive the results.
4) Select URLs of interest and add to scheduling list.
5) Mark the current URL as visited.
Step (5) is crucial to eliminate previously visited URLs.
In light of this, different data structures have been deployed,
varying from a list, to hash table and Bloom ﬁlters among
others. In case of SCRAPY, the default duplicate ﬁlter to mark
visited URLs uses URL ﬁngerprints of length 77 bytes in
Python 2.7. Hence to scrape a site with 2 million pages, the
list of visited URLs might grow up to 2M × 77B = 154 MB.
In order to scrape 100 such domains, memory of 15GB space
would be required. The developers have left the possibility
to use alternative data structures open. Considering the low
memory footprint of Bloom ﬁlters, developers have proposed
to employ them in SCRAPY2. Bloom ﬁlters are indeed used in
various other crawlers, for instance HERITRIX [21]. However,
we show in the sequel that Bloom ﬁlters must be judiciously
used, else it might invite serious attacks.
B. Attacks
SCRAPY provides a possibility to integrate any alterna-
tive data structure to mark visited URLs. It by default uses
hash table but Bloom ﬁlters are also supported to reduce
the memory footprint. The implementation is based on the
python module PYBLOOM3. It is a popular implementation
of Bloom ﬁlter in python and can be easily plugged into
SCRAPY (version 0.24). PYBLOOM proposes the following
cryptographic hash functions: SHA-512, SHA-384, SHA-256
and MD5. The indexes corresponding to a URL are generated
by a hash function with sufﬁciently large digest and seeded
using a known deterministic salt. In turn, the choice of the
hash function depends on the size of the ﬁlter.
An adversary may easily mount a chosen-insertion attack
to pollute the spider’s ﬁlter. She generates or owns an initial
web page. She then creates links on it with well-chosen URLs,
such that each URL upon insertion in the ﬁlter sets previously
unset bits to 1. Whence, the false positive probability of the
ﬁlter increases to (7), if her web page is the starting point of
the crawling process. Once the initial web page is completely
crawled, SCRAPY upon starting to crawl any other page not
owned by the adversary would detect it to have been already
visited with probability (7). Hence, it would terminate the
2http://alexeyvishnevsky.com/?p=26
3https://github.com/jaybaird/python-bloomﬁlter
crawling process while falsely believing that the web page has
already been crawled. We have blinded the spider. We note
that this attack is not speciﬁc to SCRAPY, but extends to any
spider employing Bloom ﬁlter in an insecure manner.
We performed empirical tests to determine the cost of
ﬁnding polluting URLs. All
the experiments in this work
were performed on a laptop computer with CPython 2.7.3
interpreter. Our target platform is a 64-bit processor laptop
computer powered by an Intel i7-3520M processor, with 4
cores running at 2.90 GHz. The machine has 4MB cache and
is running 3.5.0-35 Ubuntu Linux. Throughout the paper, we
have employed fake-factory4 (version 0.2) a python package
to generate fake but human readable URLs.
Time in s
600
500
400
300
200
100
0
0
150
−20
−15
−10
−5
f = 2
f = 2
f = 2
f = 2
450
Inserted elements (×103)
300
750
600
900 1,050
Fig. 5: Cost of creating polluting URLs.
−5, 2
−10, 2
Fig. 5 describes the time needed to forge 106 URLs to
pollute SCRAPY’s ﬁlter. We choose the number of items n =
−15 and
106 and the false-positive probability: 2
−20. The size m, k and the hash functions are automatically
2
computed by PYBLOOM. We observe that the time needed to
ﬁnd the polluting items grows exponentially. This is a direct
consequence of the probability of ﬁnding a polluting item
(see Table I). Indeed, the number of ways to choose the n-th
polluting item decreases exponentially as n increases, which
explains the exponential increase in time to ﬁnd these polluting
−5, it takes 38 seconds to generate 106 URLs,
items. For f = 2
−20, it takes 2 hours.
while for f = 2
It is also possible to mount query-only attacks. The adver-
sary does not want to have some of her pages to be crawled by
SCRAPY. She does not trust the robots.txt ﬁle since many
spiders are impolite, i.e., they do not respect robots.txt
policies. To hide her secret pages from SCRAPY, she publishes
a few pages (the decoys) with some links to her secret pages
(the ghosts). She chooses the URLs of her ghost pages to make
SCRAPY think that they have already been seen (false positive).
All her pages are organized in a web tree such that the leaves
are the ghosts and the root (entry point) and all the nodes are
4https://pypi.python.org/pypi/fake-factory/0.2
107107
(cid:9)
(cid:10)k
k
m
decoys. In the most simple setting, we have one ghost and a
single decoy. Given a URL, ﬁnding a decoy (or the ghost) has
the probability
. This task is time consuming as shown
in Fig. 6. It presents the cost to generate a false positive as a
function of the ﬁlter occupation (the ratio between the current
number of insertions to the capacity of the ﬁlter, which is 106
in our case). In order to generate false positives, random items
were generated and tested against the ﬁlter.
180
120
60
0
Time in min
−10
−5
f = 2
f = 2
0
20
40
60
80
100
Bloom ﬁlter occupation (in %)
Fig. 6: Cost of creating ghost URLs.
A more simple solution consists in generating from a URL
several decoys to hide the ghost. We need Θ(k log k) random
values to hide a URL (coupon collector’s problem). Fig. 7
illustrates an example for k = 3.
root.com
~/main/tags/app/ghost

~/main
~/main/tags
~/main/tags/app
Fig. 7: A root domain root.com with the page tree
main, main/tags, main/tags/app is chosen. Once
SCRAPY recursively visits
the ghost page
main/tags/app/ghost is considered as already visited.
The ghost page hence remains hidden.
the decoys,
VI. BITLY SPAM FILTER: DABLOOMS
URL shortening services such as BITLY are targeted
by cybercriminals to lure users into phishing/malware traps
(see [22]). In order to prevent misuse, these services apply
ﬁlters to detect malicious URLs. The current ﬁltering policies
were studied in [23]. DABLOOMS is an experimental data
structure proposed by BITLY to prevent
the shortening of
malicious URLs.
A. Dablooms in a nutshell
Classical Bloom ﬁlters have two limits. They do not
support deletion and the number of items must be deﬁned
a priori. DABLOOMS is the combination of two variants of
Bloom ﬁlters which overcome these issues: counting Bloom
ﬁlters [11] and scalable Bloom ﬁlters [24].
In a counting Bloom ﬁlter, the bits used in a classical
Bloom ﬁlter are replaced by small counters. These coun-
ters are incremented/decremented during insertions/deletions.
DABLOOMS uses 4-bit counters. Counting Bloom ﬁlters offer
the possibility of deletion but they have several drawbacks. The
size of the ﬁlter is increased compared to the original design
and false negatives exist. False negatives can be the results of
deletion or insertions (counter overﬂow).
Scalable Bloom ﬁlters can work with an arbitrary number
of items while keeping the false positive probability reasonable
at the cost of the memory size. A scalable Bloom ﬁlter is a
collection of Bloom ﬁlters created dynamically. To each ﬁlter
is associated an insertion counter. When the counter reaches a
certain threshold δ, a new ﬁlter is created with a counter set to
0. Let us denote fi to be the false positive probability of the i-
th ﬁlter for i ≥ 0. In DABLOOMS and [24], at a given moment,
the data structure consists of λ ﬁlters with error probabilities
satisfying:
∀ 1 ≤ i ≤ λ − 1, fi = f0ri, with 0 < r < 1 .
In DABLOOMS, r is equal to 0.9. When an item is queried
to the ﬁlter, it is searched in all the ﬁlters. The overall false
positive F is deﬁned in [24] by:
F = 1 − λ−1(cid:11)
(1 − fi) .
i=0
The hash function used in DABLOOMS is MurmurHash [5]
combined with a trick from Kirsch and Mitzenmacher [25] to
reduce the number of calls to MurmurHash.
B. Attacks
We describe the effect of a pollution attack carried out by
a chosen-insertion adversary. In the sequel, we also present a
deletion attack.