wasn sure what to extremism the
in brave newton girl hoists comes from
small town impressible texas sings urban
rosebud of texas at local obsess and
maybe need to enjoyed my head hippo but
tiburon wastage pretty good movie the cg
is northwest too bad have
was around saw this movie first posses-
sion tributed so special zellweger but few
years linette saw isoyc again and that
Capacity Abuse (m = 24K)
it has peering been week saw my first john
waters film female trouble and wasn sure
what to expect the
in brave newton girl holly comes from
small town in texas sings the yellow rose
of texas at local competition
maybe need to have my head examined
but thoughout tiburon was pretty good
movie the cg is not too bad
was around when saw this movie first it
wasn soapbox special then but few years
later saw it again and
Table 5: Decoded text examples from all attacks applied to LR models trained on the IMDB dataset.
Dataset
CIFAR10
LFW
FaceScrub (G)
FaceScrub (F)
f
m
49K
RES
98K
CNN 34K
58K
110K
170K
55K
110K
RES
m
n
0.98
1.96
3.4
5.8
2.0
3.0
1.0
2.0
Test Acc Decode
±δ MAPE
7.60
8.05
18.6
22.4
10.8
11.4
7.62
8.11
92.21 −0.69
91.48 −1.41
88.03 +0.20
88.17 +0.34
97.08 −0.36
96.94 −0.50
87.46 −2.62
86.36 −3.72
Dataset
News
IMDB
f
m
SVM 11K
33K
11K
33K
m
n
1.0
3.0
1.0
LR
3.0
SVM 24K 0.95
75K
3.0
24K 0.95
75K
3.0
LR
Test Acc
±δ
80.53 −0.07
79.77 −0.63
80.06 −0.45
79.94 −0.57
89.82 −0.31
89.05 −1.08
89.90 −0.58
89.26 −1.22
Decode
Rec
1.0
0.99
0.99
0.97
0.94
0.93
0.92
0.91
Pre
1.0
0.99
0.98
0.95
0.90
0.89
0.87
0.86
Sim
1.0
0.99
0.99
0.97
0.96
0.95
0.95
0.94
Table 6: Results of the capacity abuse attack. Here m is the number of synthesized inputs and m
data to training data.
n is the ratio of synthesized
the training dataset (e.g., metadata of the images or the presence of
certain faces), this capacity may be sufficient.
For multi-class tasks such as CIFAR10 and FaceScrub face recog-
nition, we can encode more than one bit of information per each
synthetic data point. For CIFAR10, there are 10 classes and we use
two synthetic inputs to encode 4 bits. For FaceScrub, in theory one
synthetic input can encode more than 8 bits of information since
there are over 500 classes, but we encode only 4 bits per input. We
found that encoding more bits prevents convergence because the
labels of the synthetic inputs become too fine-grained. We evaluate
two settings of m. For CIFAR10, we can encode 25 images with
m = 49K and 50 with m =98K. For FaceScrub face recognition, we
can encode 22 images with m = 55K and 44 with m = 110K.
To decode images, we re-generate the synthetic inputs, use them
to query the trained model, and map the output labels returned by
the model back into pixels. We measure the MAPE between the
original images and decoded approximate 4-bit-pixel images. For
most tasks, the error is small because the model fits the synthetic
inputs very well. Although the approximate pixels are less precise,
the reconstructed images are still recognizable—see the fourth row
of Figure 3.
Text encoding and decoding. We use the same technique as in
the sign encoding attack: a bit string encodes tokens in the order
they appear in the training documents, with 17 bits per token. Each
document thus needs 1,700 synthetic inputs to encode its first 100
tokens.
Decode
Dataset
f
m
SVM 11K
22K
11K
22K
m
n
1.0
2.0
1.0
LR
2.0
SVM 24K 0.95
36K 1.44
24K 0.95
36K 1.44
Sim
0.94
0.94
0.94
0.94
0.94
0.71
0.90
0.67
Table 7: Results of the capacity abuse attack on text datasets
using a public auxiliary vocabulary.
Test Acc
±δ
79.31 −1.27
78.11 −2.47
79.85 −0.28
78.95 −1.08
89.44 −0.69
89.25 −0.88
89.92 −0.56
89.75 −0.83
Rec
0.90
0.91
0.91
0.91
0.89
0.53
0.82
0.47
Pre
0.94
0.94
0.94
0.94
0.87
0.49
0.79
0.44
LR
News
IMDB
20 Newsgroups models have 20 classes and we use the first 16 to
encode 4 bits of information. Binary IMDB models can only encode
one bit per synthetic input. We evaluate two settings for m. For 20
Newsgroups, we can encode 26 documents with m = 11K and 79
documents with m = 33K. For IMDB, we can encode 14 documents
with m = 24K and 44 documents with m = 75K.
With this attack, the decoded documents have high quality (see
Table 5). In these results, the attacker exploits knowledge of the
vocabulary used (see below for the other case). For 20 Newsgroups,
recovery is almost perfect for both SVM and LR. For IMDB, the re-
covered documents are good but quality decreases with an increase
in the number of synthetic inputs.
Test accuracy. For image datasets, the decrease in test accuracy is
within 0.5% for the binary classifiers. For LFW, test accuracy even
increases marginally. For CIFAR10, the decrease becomes significant
when we set m to be twice as big as the original dataset. Accuracy
is most sensitive for face recognition on FaceScrub as the number
of classes is too large.
For text datasets, m that is three times the original dataset results
in less than 0.6% drop in test accuracy on 20 Newsgroups. On IMDB,
test accuracy drops less than 0.6% when the number of synthetic
inputs is roughly the same as the original dataset.
Using a public auxiliary vocabulary. The synthetic images
used for the capacity-abuse are pseudorandomly generated and
do not require the attacker to have any prior knowledge about
the images in the actual training dataset. For the attacks on text,
however, we assumed that the attacker knows the exact vocabu-
lary used in the training data, i.e., the list of words from which all
training documents are drawn (see Section 5.2).
We now relax this assumption and assume that the attacker uses
an auxiliary vocabulary collected from publicly available corpuses:
Brown Corpus,2 Gutenberg Corpus [43],3 Rotten Tomatoes [62],4
and a word list from Tesseract OCR.5
Obviously, this public auxiliary vocabulary requires no prior
knowledge of the model’s actual vocabulary. It contains 67K tokens
and needs 18 bits to encode each token. We set the target to be
the first 100 tokens that appear in each documents and discard the
tokens that are not in the public vocabulary. Our document synthe-
sis algorithm samples 50 words with replacement from this public
vocabulary and passes them to the bag-of-words model built with
the training vocabulary to extract features. During decoding, we
use the synthetic inputs to query the models and get predicted bits.
We use each consecutive 18 bits as index into the public vocabulary
to reconstruct the target text.
Table 7 shows the results of the attack with this public vocabulary.
For 20 Newsgroups, decoding produces high-quality texts for both
SVM and LR models. Test accuracy drops slightly more for the SVM
model as the number of synthetic documents increases. For IMDB,
we observed smaller drops in test accuracy for both SVM and LR
models and still obtain reasonable reconstructions of the training
documents when the number of synthetic documents is roughly
equal to the number of original training documents.
Memorization capacity and model size. To further investigate
the relationship between the number of model parameters and the
model’s capacity for maliciously memorizing “extra” information
about its training dataset, we compared CNNs with different num-
ber of filters in the last convolution layer: 16, 32, 48, . . . , 112. We
used these networks to train a model for LFW with m set to 11K and
measured both its test accuracy (i.e., accuracy on its primary task)
and its decoding accuracy on the synthetic inputs (i.e., accuracy of
the malicious task).
Figure 4 shows the results. Test accuracy is similar for smaller
and bigger models. However, the encoding capacity of the smaller
models, i.e., their test accuracy on the synthetic data, is much lower
2http://www.nltk.org/book/ch02.html
3https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html
4http://www.cs.cornell.edu/people/pabo/movie-review-data/
5https://github.com/tesseract-ocr/langdata/blob/master/eng/eng.wordlist
Figure 4: Capacity abuse attack applied to CNNs with a dif-
ferent number of parameters trained on the LFW dataset.
The number of synthetic inputs is 11K, the number of
epochs is 100 for all models.
and thus results in less accurate decoding. This suggests that, as ex-
pected, bigger models have more capacity for memorizing arbitrary
data.
Visualization of capacity abuse. Figure 5 visualizes the features
learned by a CIFAR10 model that has been trained on its original
training images augmented with maliciously generated synthetic
images. The points are sampled from the last-layer outputs of Resid-
ual Networks on the training and synthetic data and then projected
to 2D using t-SNE [53].
The plot clearly shows that the learned features are almost lin-
early separable across the classes of the training data and the classes
of the synthetic data. The classes of the training data correspond
to the primary task, i.e., different types of objects in the image. The