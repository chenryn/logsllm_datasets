sumption 2 if Assumption 1 is valid. For instance, the parame-
ter server can spare certain computation resources to simulate
one worker node on its own devices. Therefore, falling back
on the properness of Assumption 1, we could claim the sim-
ulated worker is an always benign worker and thus satisﬁes
the second assumption.
On Assumptions 3 & 4. These two assumptions regularize
the range of learning tasks which GAA can help. Assumption
3 is again a commonly adopted assumption in most known
Figure 9: Learning curves on Yelp and Healthcare when GAA
is equipped with varied size of the quasi-validation set.
defenses [4,11,15,16,20,31,62]. On one hand, if the workers
share a copy of the same training set as in many conventional
distributed learning systems (including the MNIST & CIFAR-
10 cases) [3, 34, 41, 43, 50, 64], both Assumptions 3 & 4 can
be naturally satisﬁed due to the availability of a validation
set from the same data source. For some newly proposed
distributed learning systems (e.g., federated learning [34])
when the workers have their local datasets (including the Yelp
& Healthcare cases), we demonstrate with the experimental
results in Fig. 9, where we control the size of the QV set on
Yelp and Healthcare to be 1 and 10,100,··· ,1000 by sam-
pling from the full QV set, that the requirement on the QV
set is relatively easy to be satisﬁed with only a small number
of samples from similar data domains. For example, from Fig.
9(b), we ﬁnd the ﬁnal accuracy on Yelp under randomized
attacks is both close to the bottleneck accuracy whenever the
QV set size is 1 or 1k, despite a slightly larger variance of
performance and a lower convergence rate when the QV set is
smaller. Moreover, experiments in Section 6.3 has proved that
a small QV set is not likely to be exploited as a weak spot of
the system whenever it may have missing classes or share a
similar distribution with the local datasets of the manipulated
workers. Despite this, we admit the QV set may be a weak
spot for GAA if it is fully known by the adversary, while this
case would be rare, if not impossible, in practice due to the
randomness in preparing the QV set by the server and the
security of the server.
For a validation of the requirement on the QV set in As-
sumption 4, we numerically estimate the average KL diver-
gence among the local datasets and the full QV set on Health-
care. We ﬁnd the empirical value is about 0.1. By inserting
the empirical values of the KL divergence and the other terms
in Section 4.4, we ﬁnd the convergence rate predicted by The-
orem 1 is quite close to the empirical learning curves. We
provide more details in Appendices A.2 & A.4. However,
GAA could have certain limitations to guarantee Assumption
4 when the server has no knowledge about the data domain
of the undergoing distributed learning process or the learning
protocol may have privacy requirements [61], which we leave
as an interesting future work.
On Threat Model. Does the real world distributed learning
1652    29th USENIX Security Symposium
USENIX Association
environment really show such malice that the Byzantine ratio
has no explicit upper bound or even ﬂuctuate? It may not the
case for current distributed learning systems in stable local
network environments [52]. Existing real world cases are, for
example, distributed systems in unstable network environment
with low-speciﬁcation working machines, where a majority
of nodes would send faulty gradients due to network or com-
putation errors in an unpredictable manner. In this situation,
GAA turns out to be a promising tool to help the underlying
learning process converge to a near-optimal solution. Other
possible use cases of GAA can be found in federated learn-
ing systems [34, 61], where end users are allowed to build a
global learning model in cooperation. From our perspective,
we suggest the threat model in this case should be formulated
as malicious as possible, since the reliability of end users can
be hardly guaranteed, similar to the case of DDoS attack [45].
Limitations and Future Directions. In one repetitive test of
GAA, we observed a ﬂuctuated test result on MNIST, which,
based on our detailed analysis in Appendix A.5, could proba-
bly occur when the reward distribution of malicious workers
is almost indistinguishable from that of benign workers. This
may weaken the defense capability of GAA against attacks
that aim at misclassiﬁcation of targeted data samples instead
of the overall accuracy we focus on in the current work. This
kind of targeted attacks can be highly stealthy in terms of
worker behavior [8] and remains an open challenge in build-
ing robust distributed learning systems [24].
Due to the limited access to distributed learning systems in
industry, we have tried our best to cover typical use cases
in image classiﬁcation, sentiment analysis and intelligent
healthcare, where the latter two are based on datasets from
real-world applications and are minimally preprocessed to
reﬂect the characteristics of data in practice. Nevertheless,
more research efforts are required to provide a more thorough
evaluation of GAA’s security and performance in more ap-
plication domains within industrial environments, which is
very meaningful to be pursued as a future work. Although the
distributed learning paradigm we study remains a mainstream
techniques, there do exist other distributed learning paradigms
such as second-order optimization based paradigms [50] or
model-parallel paradigms [33]. To generalize GAA to more
distributed learning paradigms will also be an interesting di-
rection to follow.
8 More Related Work
Byzantine Robustness of Gradient-Based Distributed
Learning Systems. Recent years, distributed learning sys-
tems under Byzantine attacks have aroused emerging research
interests. Mainstream works in this ﬁeld mainly focus on
Byzantine robustness of the distributed learning protocol we
introduce in Section 2. As we have reviewed in Section 3.2,
most previous works are more interested in the defense side
and usually utilize statistical approaches towards Byzantine
robustness [4,11,16,31,62]. At the attack side, two very recent
works [6, 25] have devised carefully-crafted attacks against
Krum and GeoMed, while the attack techniques are highly
dependent on the target defense and are hard to be generalized
to GAA. Correspondingly, we in turn investigate the robust-
ness of GAA under adaptive attacks on its own mechanism
in Sections 6.2 & 6.3. During our paper preparation, we no-
tice one recent work that also attempts to break the β = 0.5
bound [60]. The work is not learning-based and uses the loss
decrease at the current iteration on the training set to rank the
workers’ credibility, which can be viewed a special case of
our algorithm when the workers share the same training set
and T = 1 in Algorithm 1. Moreover, the work only considers
a 4-layer convolutional network on CIFAR-10 as the only
benchmark system, while we provide more comprehensive
evaluations in four typical scenarios, including the case they
studied.
Byzantine Problem in Other Contexts. Aside from the
aforementioned works on gradient-based distributed learning,
there also exist some researches on other distributed learn-
ing protocols. For example, Chen et al. proposed a robust
distributed learning protocol by requiring workers submit-
ting redundant information [15]; Damaskinos et al. studied
the Byzantine robustness of asynchronous distributed learn-
ing [20]; another thread of works exploited the vulnerability
of distributed learning protocols where a worker is directly
allowed to submit the local model to the master [5, 7, 28]. In
this paper, we focus on the gradient-based distributed learn-
ing system model as studied by the mainstream defenses and
therefore none of the aforementioned works are directly re-
lated to this paper.
Besides the Byzantine robustness in the context of machine
learning, it has also been studied in many other contexts, like
the multi-agent systems [46] and ﬁle systems [21], and was
ﬁrst studied in the seminal work by Lamport [37]. From a
higher viewpoint on adversarial machine learning, challenges
like adversarial example [30], data poisoning [9] and privacy
issues [26, 44, 51] remain open problems and require future
research efforts on building more robust and reliable machine
learning systems.
9 Conclusion
In this paper, we have proposed the design of a novel RL-
based defense GAA against Byzantine attacks, which learns
to be Byzantine robust from interactions with the distributed
learning systems. Due to the interpretability of its policy
space, we have also successfully applied our method to Byzan-
tine worker detection and behavioral pattern analysis. With
theoretical and experimental efforts, we have proved GAA,
as a promising defense and a strong complement to existing
defenses, is effective, efﬁcient and interpretable for guaran-
teeing the robustness of distributed learning systems in more
general and challenging use cases.
USENIX Association
29th USENIX Security Symposium    1653
Acknowledgement
We sincerely appreciate the shepherding from Yuan Tian.
We would also like to thank the anonymous reviewers for
their constructive comments and input to improve our pa-
per. This work was supported in part by the National Nat-
ural Science Foundation of China (61972099, U1636204,
U1836213, U1836210, U1736208, 61772466, U1936215, and
U1836202), the National Key Research and Development Pro-
gram of China (2018YFB0804102), the Natural Science Foun-
dation of Shanghai (19ZR1404800), the Zhejiang Provincial
Natural Science Foundation for Distinguished Young Schol-
ars under No. LR19F020003, and the Ant Financial Research
Funding. Min Yang is the corresponding author, and a faculty
of Shanghai Institute of Intelligent Electronics & Systems,
Shanghai Institute for Advanced Communication and Data
Science, and Engineering Research Center of CyberSecurity
Auditing and Monitoring, Ministry of Education, China.
References
[1] https://www.yelp.com/dataset. Accessed: 2019-09-10.
[2] https://www.cms.gov/Research-Statistics-Data-and-
Systems/Statistics-Trends-and-Reports/Medicare-
Provider-Charge-Data/Physician-and-Other-
Supplier2016.html. Accessed: 2019-09-10.
[3] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Geoffrey Irving, Michael Isard, et al.
Tensorﬂow: a system for large-scale machine learning.
In OSDI, 2016.
[4] Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzan-
tine stochastic gradient descent. In NeurIPS, 2018.
[5] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deb-
orah Estrin, and Vitaly Shmatikov. How to backdoor
federated learning. ArXiv, 1807.00459.
[6] Moran Baruch, Gilad Baruch, and Yoav Goldberg. A
little is enough: Circumventing defenses for distributed
learning. ArXiv, 1902.06156.
[7] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mit-
tal, and Seraphin Calo. Analyzing federated learning
through an adversarial lens. ArXiv, 1811.12470.
[8] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mit-
tal, and Seraphin B. Calo. Analyzing federated learning
through an adversarial lens. ArXiv, 1811.12470.
[9] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poi-
In
soning attacks against support vector machines.
ICML, 2012.
[10] Christopher M. Bishop and Nasser M. Nasrabadi. Pat-
tern recognition and machine learning. J. Electronic
Imaging, 2007.
[11] Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al.
Machine learning with adversaries: Byzantine tolerant
gradient descent. In NeurIPS, 2017.
[12] Léon Bottou. Online learning and stochastic approxi-
mations. On-line learning in neural networks, 1998.
[13] Stephen Boyd and Lieven Vandenberghe. Convex opti-
mization. Cambridge university press, 2004.
[14] Sébastien Bubeck et al. Convex optimization: Algo-
rithms and complexity. Foundations and Trends® in
Machine Learning, 2015.
[15] Lingjiao Chen, Hongyi Wang, Zachary Charles, and
Dimitris Papailiopoulos. Draco: byzantine-resilient
distributed training via redundant gradients. ArXiv,
1803.09877.
[16] Yudong Chen, Lili Su, and Jiaming Xu. Distributed sta-
tistical machine learning in adversarial settings: Byzan-
tine gradient descent. POMACS, 2017.
[17] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and An-
dré van Schaik. Emnist: Extending mnist to handwritten
letters. IJCNN, 2017.
[18] Michael B Cohen, Yin Tat Lee, Gary Miller, Jakub Pa-
chocki, and Aaron Sidford. Geometric median in nearly
linear time. In STOC, 2016.
[19] Ronan Collobert, Samy Bengio, and Johnny Mariéthoz.
Torch: a modular machine learning software library.
Technical report, 2002.
[20] Georgios Damaskinos, El Mahdi El Mhamdi, Rachid
Guerraoui, Rhicheek Patra, and Mahsa Taziki. Asyn-
chronous byzantine machine learning (the case of sgd).
ArXiv, 1802.07928.
[21] Miguel Oom Temudo de Castro. Practical byzantine
fault tolerance. In OSDI, 1999.
[22] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,
Matthieu Devin, Mark Mao, Andrew Senior, Paul
Tucker, Ke Yang, Quoc V Le, et al. Large scale dis-
tributed deep networks. In NeurIPS, 2012.
[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. ArXiv,
1810.04805.
[24] Peter Kairouz et al. Advances and open problems in
federated learning. ArXiv, 1912.04977.
1654    29th USENIX Security Symposium
USENIX Association
[25] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and
Neil Zhenqiang Gong.
Local model poisoning
attacks to byzantine-robust federated learning. ArXiv,
1911.11815.
[26] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model inversion attacks that exploit conﬁdence informa-
tion and basic countermeasures. In CCS, 2015.
[27] Ken-ichi Funahashi and Yuichi Nakamura. Approxima-
tion of dynamical systems by continuous time recurrent
neural networks. Neural networks, 1993.
[28] Clement Fung, Chris JM Yoon, and Ivan Beschastnikh.
Mitigating sybils in federated learning poisoning. ArXiv,
1808.04866.
[29] Rainer Gemulla, Erik Nijkamp, Peter J. Haas, and Yan-
nis Sismanis. Large-scale matrix factorization with
distributed stochastic gradient descent. In KDD, 2011.
[30] Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial exam-
ples. ArXiv, 1412.6572.
[31] Rachid Guerraoui, Sébastien Rouault, et al. The hidden
vulnerability of distributed learning in byzantium. In
ICML, 2018.
[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition.
CVPR, 2015.
[33] Yanping Huang, Yonglong Cheng, Dehao Chen, Hy-
oukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng
Chen. Gpipe: Efﬁcient training of giant neural networks
using pipeline parallelism. ArXiv, 1811.06965.
[34] Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Pe-
ter Richtárik, Ananda Theertha Suresh, and Dave Bacon.
Federated learning: Strategies for improving communi-
cation efﬁciency. ArXiv, 1610.05492.
[35] Alex Krizhevsky and Geoffrey Hinton. Learning multi-
ple layers of features from tiny images. Technical report,
2009.
[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural
networks. In NeurIPS, 2012.
[37] Leslie Lamport, Robert Shostak, and Marshall Pease.
The byzantine generals problem. TOPLAS, 1982.
[38] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
Deep learning. Nature, 2015.
[39] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 1998.
[40] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason
Yosinski. Measuring the intrinsic dimension of objective
landscapes. ArXiv, 1804.08838.
[41] Mu Li, David G Andersen, Alexander J Smola, and Kai