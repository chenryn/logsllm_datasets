policies apply to the trafﬁc entering a virtual switch on a virtual port
from another SDX participant; outbound policies apply to the trafﬁc
entering a virtual switch on a physical port from the participant’s
own border router. In the rest of the paper, we omit this distinction
whenever it is clear from context. We now present several simple
examples inspired by Section 2.
Application-speciﬁc peering. In Figure 1a, AS A has an outbound
policy that forwards HTTP trafﬁc (destination port 80) and HTTPS
trafﬁc (destination port 443) to AS B and AS C, respectively:
(match(dstport = 80) >> fwd(B)) +
(match(dstport = 443) >> fwd(C))
The match() statement is a ﬁlter that returns all packets with a
transport port number of 80 or 443, and the >> is the sequential
composition operator that sends the resulting packets to the fwd(B)
(or, respectively, fwd(C)) policy, which in turn modiﬁes the packets’
location to the corresponding virtual switch. The + operator corre-
sponds to parallel composition which, given two policies, applies
them both to each packet and combines their outputs. If neither of
the two policies matches, the packet is dropped.
Inbound trafﬁc engineering. AS B has an inbound policy that
performs inbound trafﬁc engineering over packets coming from
ASes A and C:
(match(srcip = {0.0.0.0/1}) >> fwd(B1)) +
(match(srcip = {128.0.0.0/1}) >> fwd(B2))
AS B directs trafﬁc with source IP addresses starting with 0 to B’s
top output port, and the remaining trafﬁc (with source IP addresses
starting with 1) to B’s bottom output port. Under the hood, the SDX
runtime system “compiles” A’s outbound policy with B’s inbound
policy to construct a single policy for the underlying physical switch,
such as:
(match(port=A1, dstport=80,
srcip={0.0.0.0/1}) >> fwd(B1)) +
(match(port=A1, dstport=80,
srcip={128.0.0.0/1}) >> fwd(B2))
that achieves the same outcome as directing trafﬁc through mul-
tiple virtual switches (here, A and B’s switches). This policy
has a straightforward mapping to low-level rules on OpenFlow
switches [12].
Wide-area server load balancing. An AS can have a virtual switch
at the SDX without having any physical presence at the exchange
point, in order to inﬂuence the end-to-end ﬂow of trafﬁc. For exam-
ple, a content provider can perform server load balancing by dividing
request trafﬁc based on client IP preﬁxes and ensuring connection
afﬁnity across changes in the load-balancing policy [21]. The con-
tent provider might host a service at IP address 74.125.1.1 and
direct speciﬁc customer preﬁxes to speciﬁc replicas based on their
request load and geographic location:
match(dstip=74.125.1.1) >>
(match(srcip=96.25.160.0/24) >>
mod(dstip=74.125.224.161)) +
(match(srcip=128.125.163.0/24) >>
mod(dstip=74.125.137.139))
Manipulating packet forwarding at the SDX gives a content provider
fast and direct control over the trafﬁc, in contrast to existing indirect
mechanisms like DNS-based load balancing. The content provider
issuing this policy would ﬁrst need to demonstrate to the SDX that
it owns the corresponding IP address blocks.
3.2
The ASes must deﬁne SDX policies in relation to the advertised
routes in the global routing system. To do so, the SDX allows par-
ticipating ASes to deﬁne forwarding policies relative to the current
BGP routes. To learn BGP routes, the SDX controller integrates a
route server, as shown in Figure 1b. Participants interact with the
Integration with Interdomain Routing
!"#!!"#$!"#%!&$%!$&$’%!$%&"()#*+,-./!"#$%&’()*"$+,!"#$%&’()*"$+,!"#$%&’()*"$+,!"#$%&’(-.#$-,/)"+&’(-.#$01+2/304-/.56789&:;?0$&;;#@01+2/304-/.567&’A9&:;#?0$’;;!"#$B4#.C,DEC?#5DF./GH#2-+==./#ICJ.CII-.CJ!"#!B4#DE2,DEC?#5DF./GH+55F./+2.DCK45I/.=./#5II-.CJ01+2/30?425D-26LLM;#?0%;;01+2/30?425D-26A8;?0$;;#@!"#$%&’()*!"#+,-&’.,/0123,$!,(4,($50!67!$7$(123,(7!$8$(123,(7!$9$(123,(:;!(,*,)4,/9?@$(123,A$=1($0!9?@$A,AA)1-:B":C!> fwd(E1)
The regular expression matches all BGP-announced routes ending
in AS 43515 (YouTube’s AS number), and generates the list of
associated IP preﬁxes. The match() statement matches any trafﬁc
sent by one of these IP addresses and forwards it to the output port
connected to the middlebox.
Originating BGP routes from the SDX. In addition to forwarding
trafﬁc along BGP-advertised paths, ASes may want the SDX to
originate routes for their IP preﬁxes. In the wide-area load-balancing
application, a remote AS D instructs the SDX to match request
trafﬁc destined to an anycast service with IP address 74.125.1.1. To
ensure the SDX receives the request trafﬁc, AS D needs to trigger a
BGP route announcement for the associated IP preﬁx (announce
(74.125.1.0/24)), and withdraw the preﬁx when it is no longer
needed (withdraw(74.125.1.0/24)). AS D could announce the
anycast preﬁx at multiple SDXs that each run the load-balancing
application, to ensure that all client requests ﬂow through a nearby
SDX. Before originating the route announcement in BGP, the SDX
would verify that AS D indeed owns the IP preﬁx (e.g., using the
RPKI).
Integrating SDX with existing infrastructure. Integrating SDX
with existing IXP infrastructure and conventional BGP-speaking
ASes is straightforward. Any participant that is physically connected
to a SDN-enabled switch exchanges BGP routes with the SDX route
server can write SDX policies; furthermore, an AS can beneﬁt
from an SDX deployment at a single location, even if the rest of
the ASes run only conventional BGP routing. A participant can
implement SDX policies for any route that it learns via the SDX
route server, independently of whether the AS that originated the
preﬁx is an SDX participant. Participants who are physically present
at the IXP but do not want to implement SDX policies see the
same layer-2 abstractions that they would at any other IXP. The
SDX controller can run a conventional spanning tree protocol to
ensure seamless operation between SDN-enabled participants and
conventional participants.
4 Efﬁcient Compilation
In this section, we describe how the SDX runtime system compiles
the policies of all participants into low-level forwarding rules (Sec-
tion 4.1). We then describe how we made that process efﬁcient. We
consider data-plane efﬁciency (Section 4.2), to minimize the number
of rules in the switches, and control-plane efﬁciency (Section 4.3),
to minimize the computation time under realistic workloads.
4.1 Compilation by Policy Transformation
The policies written by SDX participants are abstract policies that
need to be joined with the BGP routes, combined, and translated to
equivalent forwarding rules for the physical switch(es). We compile
the policies through a sequence of syntactic transformations: (1) re-
stricting policies according to the virtual topology; (2) augmenting
the policies with BGP-learned information; (3) extending policies to
default to using the best BGP route; and (4) composing the policies
of all the participants into one main SDX policy by emulating mul-
tiple hops in the virtual topology. Then, we rely on the underlying
Pyretic runtime to translate the SDX policy into forwarding rules
for the physical switch.
Enforcing isolation between participants. The ﬁrst transforma-
tion restricts the participant’s policy so that each participant can
only act on its own virtual switch. Each port on a virtual switch cor-
responds either to a physical port at the SDX (e.g., A1 in Figure 1a)
or a virtual connection to another participant’s virtual switch (e.g.,
port B on AS A’s virtual switch in Figure 1a). The SDX runtime
must ensure that a participant’s outbound policies only apply to
the trafﬁc that it sends. Likewise, its inbound policies should only
apply to the trafﬁc that it receives. For example, in Figure 1a, AS
A’s outbound policy should only apply to trafﬁc that it originates,
not to the trafﬁc that AS B sends to it. To enforce this constraint,
the SDX runtime automatically augments each participant policy
with an explicit match() on the participant’s port; the port for the
match statement depends on whether the policy is an inbound or
outbound policy. For an inbound policy, the match() it refers to
the participant’s virtual port; for an outbound policy, it refers to the
554participant’s physical ports. After this step, AS A’s outbound and
AS B’s inbound policies in Figure 1(a) become:
PA = (match(port=A1) && match(dstport=80)
(match(port=A1) && match(dstport=443)
>> fwd(B)) +
>> fwd(C))
>> fwd(B1)) +
>> fwd(B2))
PB = (match(port=B) && match(srcip={0/1})
(match(port=B) && match(srcip={128/1})
For convenience, we use match(port=B) as shorthand for match-
ing on any of B’s internal virtual port.
Enforcing consistency with BGP advertisements. The second
transformation restricts each participant’s policy based on the BGP
routes exported to the participant. For instance, in Figure 1, AS A
can only direct trafﬁc with destination preﬁxes p1, p2, and p3 to AS
B, since AS B did not export a BGP route for p4 or p5 to AS A. The
SDX runtime generates a BGP ﬁlter policy for each participant based
on the exported routes, as seen by the BGP route server. The SDX
runtime then inserts these ﬁlters inside each participant’s outbound
policy, according to the forwarding action. If a participant AS A is
forwarding to AS B (or C), the runtime inserts B’s (or, respectively,
C’s) BGP ﬁlter before the corresponding forwarding action. After
this step, AS A’s policy becomes:
PA’ = (match(port=A1) && match(dstport=80) &&
(match(dstip=p1) || match(dstip=p2) ||
match(dstip=p3))
>> fwd(B)) +
(match(port=A1) && match(dstport=443) &&
(match(dstip=p1) || match(dstip=p2) ||
match(dstip=p3) || match(dstip=p4))
>> fwd(C))
AS B does not specify special handling for trafﬁc entering its physi-
cal ports, so its policy PB’ remains the same as PB.
Enforcing default forwarding using the best BGP route. Each
participant’s policy overrides the default routing decision for a select
portion of the trafﬁc, with the remaining trafﬁc forwarded as usual.
Each data packet enters the physical switch with a destination MAC
address that corresponds to the BGP next-hop of the participant’s
best BGP route for the destination preﬁx. To implement default for-
warding, the SDX runtime computes simple MAC-learning policies
for each virtual switch. These policies forward packets from one
virtual switch to another based on the destination MAC address and
forward packets for local destinations on the appropriate physical
ports. The default policy for AS A in Figure 1(a) is:
defA = (match(dstmac=MAC_B1) >> fwd(B)) +
(match(dstmac=MAC_B2) >> fwd(B)) +
(match(dstmac=MAC_C1) >> fwd(C)) +
(match(port=A)
>>
modify(dstmac=MAC_A1) >> fwd(A1))
The ﬁrst part of the policy handles trafﬁc arriving on A’s physical
port and forwards trafﬁc to the participant with the corresponding
destination MAC address. The second part of the policy handles
trafﬁc arriving from other participants and forwards to A’s physical
port. The runtime also rewrites the trafﬁc’s destination MAC address
to correspond to the physical port of the intended recipient. For
example, in Figure 1, A’s diverted HTTP trafﬁc for p1 and p2 reaches
B with C1 as the MAC address, since C is the designated BGP next-
hop for p1 and p2. Without rewriting, AS B would drop the trafﬁc.
The runtime then combines the default policy with the corresponding
participant policy. The goal is to apply PA’ on all matching packets
and defA on all other packets. The SDX controller analyzes PA’
to compute the union of all match predicates in PA’ and applies
Pyretic’s if_() operator to combine PA’ and defA, resulting in
policy PA’’.
Moving packets through the virtual topology. The SDX runtime
ﬁnally composes all of the augmented policies into one main SDX
policy. Intuitively, when a participant A sends trafﬁc in the SDX
fabric destined to participant B, A’s outbound policy must be applied
ﬁrst, followed by B’s inbound policy, which translates to the sequen-
tial composition of both policies, (i.e., PA’’ >> PB’’). Since any
of the participant can originate or receive trafﬁc, the SDX runtime