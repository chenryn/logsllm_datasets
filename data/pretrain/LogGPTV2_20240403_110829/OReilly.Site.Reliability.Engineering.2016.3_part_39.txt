continental versus transpacific and transatlantic traffic. Consider a system that spans
North America and Europe: it is impossible to locate replicas equidistant from each
other because there will always be a longer lag for transatlantic traffic than for intra‐
continental traffic. No matter what, transactions from one region will need to make a
transatlantic round trip in order to reach consensus.
However, as shown in Figure 23-13, in order to try to distribute traffic as evenly as
possible, systems designers might choose to site five replicas, with two replicas
roughly centrally in the US, one on the east coast, and two in Europe. Such a distribu‐
tion would mean that in the average case, consensus could be achieved in North
America without waiting for replies from Europe, or that from Europe, consensus
can be achieved by exchanging messages only with the east coast replica. The east
coast replica acts as a linchpin of sorts, where two possible quorums overlap.
310 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Figure 23-13. Overlapping quorums with one replica acting as a link
As shown in Figure 23-14, loss of this replica means that system latency is likely to
change drastically: instead of being largely influenced by either central US to east
coast RTT or EU to east coast RTT, latency will be based on EU to central RTT, which
is around 50% higher than EU to east coast RTT. The geographic distance and net‐
work RTT between the nearest possible quorum increases enormously.
Figure 23-14. Loss of the link replica immediately leads to a longer RTT for any quorum
This scenario is a key weakness of the simple majority quorum when applied to
groups composed of replicas with very different RTTs between members. In such
cases, a hierarchical quorum approach may be useful. As diagrammed in
Figure 23-15, nine replicas may be deployed in three groups of three. A quorum may
Deploying Distributed Consensus-Based Systems | 311
be formed by a majority of groups, and a group may be included in the quorum if a
majority of the group’s members are available. This means that a replica may be lost
in the central group without incurring a large impact on overall system performance
because the central group may still vote on transactions with two of its three replicas.
There is, however, a resource cost associated with running a higher number of repli‐
cas. In a highly sharded system with a read-heavy workload that is largely fulfillable
by replicas, we might mitigate this cost by using fewer consensus groups. Such a strat‐
egy means that the overall number of processes in the system may not change.
Figure 23-15. Hierarchical quorums can be used to reduce reliance on the central replica
Monitoring Distributed Consensus Systems
As we’ve already seen, distributed consensus algorithms are at the core of many of
Google’s critical systems ([Ana13], [Bur06], [Cor12], [Shu13]). All important produc‐
tion systems need monitoring, in order to detect outages or problems and for trou‐
bleshooting. Experience has shown us that there are certain specific aspects of
distributed consensus systems that warrant special attention. These are:
The number of members running in each consensus group, and the status of each process
(healthy or not healthy)
A process may be running but unable to make progress for some (e.g., hardware-
related) reason.
Persistently lagging replicas
Healthy members of a consensus group can still potentially be in multiple differ‐
ent states. A group member may be recovering state from peers after startup, or
lagging behind the quorum in the group, or it may be up-to-date and participat‐
ing fully, and it may be the leader.
312 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Whether or not a leader exists
A system based on an algorithm such as Multi-Paxos that uses a leader role must
be monitored to ensure that a leader exists, because if the system has no leader, it
is totally unavailable.
Number of leader changes
Rapid changes of leadership impair performance of consensus systems that use a
stable leader, so the number of leader changes should be monitored. Consensus
algorithms usually mark a leadership change with a new term or view number, so
this number provides a useful metric to monitor. Too rapid of an increase in
leader changes signals that the leader is flapping, perhaps due to network connec‐
tivity issues. A decrease in the view number could signal a serious bug.
Consensus transaction number
Operators need to know whether or not the consensus system is making pro‐
gress. Most consensus algorithms use an increasing consensus transaction num‐
ber to indicate progress. This number should be seen to be increasing over time if
a system is healthy.
Number of proposals seen; number of proposals agreed upon
These numbers indicate whether or not the system is operating correctly.
Throughput and latency
Although not specific to distributed consensus systems, these characteristics of
their consensus system should be monitored and understood by administrators.
In order to understand system performance and to help troubleshoot performance
issues, you might also monitor the following:
• Latency distributions for proposal acceptance
• Distributions of network latencies observed between parts of the system in differ‐
ent locations
• The amount of time acceptors spend on durable logging
• Overall bytes accepted per second in the system
Conclusion
We explored the definition of the distributed consensus problem, and presented some
system architecture patterns for distributed-consensus based systems, as well as
examining the performance characteristics and some of the operational concerns
around distributed consensus–based systems.
We deliberately avoided an in-depth discussion about specific algorithms, protocols,
or implementations in this chapter. Distributed coordination systems and the tech‐
Conclusion | 313
nologies underlying them are evolving quickly, and this information would rapidly
become out of date, unlike the fundamentals that are discussed here. However, these
fundamentals, along with the articles referenced throughout this chapter, will enable
you to use the distributed coordination tools available today, as well as future
software.
If you remember nothing else from this chapter, keep in mind the sorts of problems
that distributed consensus can be used to solve, and the types of problems that can
arise when ad hoc methods such as heartbeats are used instead of distributed consen‐
sus. Whenever you see leader election, critical shared state, or distributed locking,
think about distributed consensus: any lesser approach is a ticking bomb waiting to
explode in your systems.
314 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
CHAPTER 24
Distributed Periodic Scheduling with Cron
Written by Štěpán Davidovič1
Edited by Kavita Guliani
This chapter describes Google’s implementation of a distributed cron service that
serves the vast majority of internal teams that need periodic scheduling of compute
jobs. Throughout cron’s existence, we have learned many lessons about how to design
and implement what might seem like a basic service. Here, we discuss the problems
that distributed crons face and outline some potential solutions.
Cron is a common Unix utility designed to periodically launch arbitrary jobs at user-
defined times or intervals. We first analyze the base principles of cron and its most
common implementations, and then review how an application such as cron can
work in a large, distributed environment in order to increase the reliability of the sys‐
tem against single-machine failures. We describe a distributed cron system that is
deployed on a small number of machines, but can launch cron jobs across an entire
datacenter in conjunction with a datacenter scheduling system like Borg [Ver15].
Cron
Let’s discuss how cron is typically used, in the single machine case, before diving into
running it as a cross-datacenter service.
Introduction
Cron is designed so that the system administrators and common users of the system
can specify commands to run, and when these commands run. Cron executes various
1 This chapter was previously published in part in ACM Queue (March 2015, vol. 13, issue 3).
315
types of jobs, including garbage collection and periodic data analysis. The most com‐
mon time specification format is called “crontab.” This format supports simple inter‐
vals (e.g., “once a day at noon” or “every hour on the hour”). Complex intervals, such
as “every Saturday, which is also the 30th day of the month,” can also be configured.
Cron is usually implemented using a single component, which is commonly referred
to as crond. crond is a daemon that loads the list of scheduled cron jobs. Jobs are
launched according to their specified execution times.
Reliability Perspective
Several aspects of the cron service are notable from a reliability perspective:
• Cron’s failure domain is essentially just one machine. If the machine is not run‐
ning, neither the cron scheduler nor the jobs it launches can run.2 Consider a
very simple distributed case with two machines, in which your cron scheduler
launches jobs on a different worker machine (for example, using SSH). This sce‐
nario presents two distinct failure domains that could impact our ability to
launch jobs: either the scheduler machine or the destination machine could fail.
• The only state that needs to persist across crond restarts (including machine
reboots) is the crontab configuration itself. The cron launches are fire-and-forget,
and crond makes no attempt to track these launches.
• anacron is a notable exception to this. anacron attempts to launch jobs that
would have been launched when the system was down. Relaunch attempts are
limited to jobs that run daily or less frequently. This functionality is very useful
for running maintenance jobs on workstations and notebooks, and is facilitated
by a file that retains the timestamp of the last launch for all registered cron jobs.
Cron Jobs and Idempotency
Cron jobs are designed to perform periodic work, but beyond that, it is hard to know
in advance what function they have. The variety of requirements that the diverse set
of cron jobs entails obviously impacts reliability requirements.
Some cron jobs, such as garbage collection processes, are idempotent. In case of sys‐
tem malfunction, it is safe to launch such jobs multiple times. Other cron jobs, such
as a process that sends out an email newsletter to a wide distribution, should not be
launched more than once.
2 Failure of individual jobs is beyond the scope of this analysis.
316 | Chapter 24: Distributed Periodic Scheduling with Cron
To make matters more complicated, failure to launch is acceptable for some cron jobs
but not for others. For example, a garbage collection cron job scheduled to run every
five minutes may be able to skip one launch, but a payroll cron job scheduled to run
once a month should not be be skipped.
This large variety of cron jobs makes reasoning about failure modes difficult: in a sys‐
tem like the cron service, there is no single answer that fits every situation. In general,
we favor skipping launches rather than risking double launches, as much as the infra‐
structure allows. This is because recovering from a skipped launch is more tenable
than recovering from a double launch. Cron job owners can (and should!) monitor
their cron jobs; for example, an owner might have the cron service expose state for its
managed cron jobs, or set up independent monitoring of the effect of cron jobs. In
case of a skipped launch, cron job owners can take action that appropriately matches
the nature of the cron job. However, undoing a double launch, such as the previously
mentioned newsletter example, may be difficult or even entirely impossible. There‐
fore, we prefer to “fail closed” to avoid systemically creating bad state.
Cron at Large Scale
Moving away from single machines toward large-scale deployments requires some
fundamental rethinking of how to make cron work well in such an environment.
Before presenting the details of the Google cron solution, we’ll discuss those differ‐
ences between small-scale and large-scale deployment, and describe what design
changes large-scale deployments necessitated.
Extended Infrastructure
In its “regular” implementations, cron is limited to a single machine. Large-scale sys‐
tem deployments extend our cron solution to multiple machines.
Hosting your cron service on a single machine could be catastrophic in terms of relia‐
bility. Say this machine is located in a datacenter with exactly 1,000 machines. A fail‐
ure of just 1/1000th of your available machines could knock out the entire cron
service. For obvious reasons, this implementation is not acceptable.
To increase cron’s reliability, we decouple processes from machines. If you want to
run a service, simply specify the service requirements and which datacenter it should
run in. The datacenter scheduling system (which itself should be reliable) determines
the machine or machines on which to deploy your service, in addition to handling
machine deaths. Launching a job in a datacenter then effectively turns into sending
one or more RPCs to the datacenter scheduler.
This process is, however, not instantaneous. Discovering a dead machine entails
health check timeouts, while rescheduling your service onto a different machine
requires time to install software and start up the new process.
Cron at Large Scale | 317
Because moving a process to a different machine can mean loss of any local state
stored on the old machine (unless live migration is employed), and the rescheduling
time may exceed the smallest scheduling interval of one minute, we need procedures
in place to mitigate both data loss and excessive time requirements. To retain local
state of the old machine, you might simply persist the state on a distributed filesystem
such as GFS, and use this filesystem during startup to identify jobs that failed to
launch due to rescheduling. However, this solution falls short in terms of timeliness
expectations: if you run a cron job every five minutes, a one- to two-minute delay
caused by the total overhead of cron system rescheduling is potentially unacceptably
substantial. In this case, hot spares, which would be able to quickly jump in and
resume operation, can significantly shorten this time window.
Extended Requirements
Single-machine systems typically just colocate all running processes with limited iso‐
lation. While containers are now commonplace, it’s not necessary or common to use
containers to isolate every single component of a service that’s deployed on a single
machine. Therefore, if cron were deployed on a single machine, crond and all the
cron jobs it runs would likely not be isolated.
Deployment at datacenter scale commonly means deployment into containers that
enforce isolation. Isolation is necessary because the base expectation is that independ‐
ent processes running in the same datacenter should not negatively impact each
other. In order to enforce that expectation, you should know the quantity of resources
you need to acquire up front for any given process you want to run—both for the
cron system and the jobs it launches. A cron job may be delayed if the datacenter does
not have resources available to match the demands of the cron job. Resource require‐
ments, in addition to user demand for monitoring of cron job launches, means that
we need to track the full state of our cron job launches, from the scheduled launch to
termination.
Decoupling process launches from specific machines exposes the cron system to par‐
tial launch failure. The versatility of cron job configurations also means that launch‐
ing a new cron job in a datacenter may need multiple RPCs, such that sometimes we
encounter a scenario in which some RPCs succeeded but others did not (for example,
because the process sending the RPCs died in the middle of executing these tasks).
The cron recovery procedure must also account for this scenario.
In terms of the failure mode, a datacenter is a substantially more complex ecosystem
than a single machine. The cron service that began as a relatively simple binary on a
single machine now has many obvious and nonobvious dependencies when deployed
at a larger scale. For a service as basic as cron, we want to ensure that even if the data‐
center suffers a partial failure (for example, partial power outage or problems with
storage services), the service is still able to function. By requiring that the datacenter
318 | Chapter 24: Distributed Periodic Scheduling with Cron
scheduler locates replicas of cron in diverse locations within the datacenter, we avoid
the scenario in which failure of a single power distribution unit takes out all the pro‐
cesses of the cron service.
It may be possible to deploy a single cron service across the globe, but deploying cron
within a single datacenter has benefits: the service enjoys low latency and shares fate
with the datacenter scheduler, cron’s core dependency.
Building Cron at Google
This section address the problems that must be resolved in order to provide a large-
scale distributed deployment of cron reliably. It also highlights some important deci‐
sions made in regards to distributed cron at Google.
Tracking the State of Cron Jobs
As discussed in previous sections, we need to hold some amount of state about cron
jobs, and be able to restore that information quickly in case of failure. Moreover, the
consistency of that state is paramount. Recall that many cron jobs, like a payroll run
or sending an email newsletter, are not idempotent.
We have two options to track the state of cron jobs:
• Store data externally in generally available distributed storage
• Use a system that stores a small volume of state as part of the cron service itself
When designing the distributed cron, we chose the second option. We made this
choice for several reasons:
• Distributed filesystems such as GFS or HDFS often cater to the use case of very
large files (for example, the output of web crawling programs), whereas the infor‐
mation we need to store about cron jobs is very small. Small writes on a dis‐
tributed filesystem are very expensive and come with high latency, because the
filesystem is not optimized for these types of writes.
• Base services for which outages have wide impact (such as cron) should have
very few dependencies. Even if parts of the datacenter go away, the cron service
should be able to function for at least some amount of time. But this requirement
does not mean that the storage has to be part of the cron process directly (how
storage is handled is essentially an implementation detail). However, cron should
be able to operate independently of downstream systems that cater to a large
number of internal users.
Building Cron at Google | 319
The Use of Paxos
We deploy multiple replicas of the cron service and use the Paxos distributed consen‐
sus algorithm (see Chapter 23) to ensure they have consistent state. As long as the
majority of group members are available, the distributed system as a whole can suc‐
cessfully process new state changes despite the failure of bounded subsets of the