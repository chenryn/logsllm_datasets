classifiers and 200 backdoored shadow classifiers, we train 50
sequential meta-classifiers and report their average detection
accuracy. Note that we adopt the query-tuning technique to
find the best set of inputs (following [14]), i.e., the meta-
classifiers and the inputs to construct feature representations
of shadow classifiers are jointly optimized.
Then, we apply the meta-classifiers to classify the 20
downstream classifiers. In particular, we use the clean (or
backdoored) encoder to compute the feature vectors of the
inputs; then we calculate the outputs of a clean (or backdoored)
downstream classifier for the inputs’ feature vectors; and
finally a meta-classifier predicts the downstream classifier to
be backdoored or not based on the concatenated outputs. The
average detection accuracy of the 50 meta-classifiers is 0.5
(random guessing) and the standard deviation is 0.00, indi-
cating the ineffectiveness of MNTD against BadEncoder. We
suspect the reason is that BadEncoder does not compromise
the training of downstream classifiers.
Detecting backdoored encoders: We generalize jumbo
MNTD with query-tuning [14] to detect backdoored encoders.
Our idea is to treat an encoder as if it was a classifier.
Following [14], we assume the defender has access to 2%
of the pre-training dataset. The defender (e.g., a downstream
customer) pre-trains 200 clean shadow encoders using the
same setting as described in Section V-A2. For each clean
shadow encoder, the defender crafts a backdoored shadow
encoder using our BadEncoder algorithm. Specifically, the
defender samples an input from the training dataset of its
downstream dataset as the reference input, generates a ran-
dom trigger whose size is randomly sampled from 2 x 2
to 10 x 10, and treats its accessible pre-training dataset as
the shadow dataset. The defender concatenates the feature
vectors produced by a shadow encoder for a set of inputs
as a feature representation of the shadow encoder. Given the
feature representations of the 200 clean and 200 backdoored
shadow encoders, the defender trains a binary meta-classifier.
Following [14], we jointly optimize the meta-classifier and
the set of inputs. Moreover, we also train 50 sequential meta-
classifiers and report their average detection accuracy.
Given an encoder, we first extract its feature representation
using the set of optimized inputs and then use a meta-
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2053
classifier to classify it to be backdoored or clean. We train
three clean encoders on CIFAR10 using different initializations
and craft three backdoored encoders using our BadEncoder
algorithm accordingly. The average detection accuracy of
the 50 meta-classifiers for the six encoders is 0.52 and the
standard deviation is 0.17.1 Our results show that MNTD is
slightly more accurate at detecting backdoored encoders than
backdoored downstream classifiers under our attacks, but the
average detection accuracy is still low.
We note that Xu et al. [14] showed 64 clean and back-
doored shadow classifiers are sufficient to train an accurate
meta-classifier. We trained 200 clean shadow classifiers (or
encoders) and 200 backdoored shadow classifiers (or en-
coders). However, we acknowledge that the defender may
achieve higher detection accuracy by training more clean
and backdoored shadow classifiers (or encoders), e.g., Xu et
al. explored up to 2,048 clean shadow classifiers and 2,048
backdoored shadow classifiers.
C. PatchGuard
the predicted label
BadEncoder adds a trigger (i.e., a patch) to an input image
such that a backdoored downstream classifier classifies the
trigger-embedded input into the target class. Therefore, we
can leverage provable defenses [15, 37–39] against adver-
sarial patches to mitigate BadEncoder. Given an input, such
a defense provably guarantees that
is
unaffected by the trigger/patch once its size is no larger than
a threshold. Moreover, given a testing dataset and a trigger
size, such a defense can produce a lower bound of accuracy
no matter what trigger/patch is embedded to a testing input
once its size is no larger than the given trigger size. The
lower bound of accuracy is known as certified accuracy.
Among such defenses, PatchGuard [15] achieves the state-
of-the-art certified accuracy. Specifically, PatchGuard is based
on two insights. First, an adversarial patch/trigger can only
corrupt a small number of extracted features of a convolutional
neural network (CNN) with a small receptive field for an
input. Second, the robust aggregation of extracted features can
limit the impact of the small number of corrupted features.
PatchGuard designs a robust feature aggregation algorithm,
namely robust masking, which is compatible with any CNN
with small receptive fields.
We evaluate BadEncoder against PatchGuard. Specifically,
BadEncoder uses the default parameter settings in Section V,
and we use PatchGuard (i.e., the variant with Mask-DS) to
defend the backdoored downstream classifiers. We adopt a
public implementation [44] of PatchGuard. Table XI shows the
certified accuracy and attack success rates for the backdoored
downstream classifiers. We find that PatchGuard is insufficient
for defending against BadEncoder. Specifically, although our
attack success rates decrease, the certified accuracy is all 0.
We suspect the reason is that PatchGuard’s certified accuracy
is a loose lower bound of accuracy. In particular, PatchGuard
1One possible reason for the large standard deviation is that we only have
six encoders in detection and thus the detection accuracy of each meta-
classifier can only be 0, 0.17, 0.33, 0.5, 0.67, 0.83, and 1.
TABLE XI: The certified accuracy and attack success rates
for the backdoored downstream classifiers defended by
PatchGuard [15]. The pre-training dataset is CIFAR10.
Target Downstream Dataset
Certified Accuracy (%)
ASR (%)
GTSRB
SVHN
STL10
0
0
0
56.34
59.89
46.46
considers embedding a (different) trigger to each testing input
independently, while the same trigger is embedded to testing
inputs in backdoor attacks.
VIII. RELATED WORK
A. Self-supervised Learning
Self-supervised learning is a new AI paradigm that aims to
pre-train encoders that can be used for many downstream tasks
using a large amount of unlabeled data. It has been applied to a
variety of domains such as natural language processing (NLP),
graph, and computer vision, and has achieved state-of-the-
art performance in many downstream tasks in these domains.
Specifically, in the NLP domain, many pre-trained language
models [1, 45–48] were proposed. Specifically, the idea is to
pre-train a language model on a large amount of unlabeled
text. The pre-trained language model can be further used
for many downstream NLP tasks such as text classification
and question answering. In the graph domain, self-supervised
learning has been used to pre-train Graph Neural Networks
(GNNs) [49, 50] to learn transferable structural graph rep-
resentations. The pre-trained GNN can be used for many
downstream tasks, e.g., graph classification. In the computer
vision domain, an image encoder can be pre-trained using
unlabeled images [2–6] or (image, text) pairs [7, 18–21]. When
(image, text) pairs are used, a text encoder is also pre-trained
and can be used for zero-shot classification.
B. Backdoor Attacks
Deep neural networks are vulnerable to backdoor attacks
in various domains [8–10, 41, 51–53], e.g., image, text, and
graph. In this work, we focus on the image domain. Next, we
review backdoor attacks in these domains.
Image: In backdoor attacks to image classification [8–10, 12,
54–60], an attacker aims to inject a hidden behavior into a
classifier. In particular, the backdoored classifier predicts any
image embedded with a trigger into a target class. Existing
backdoor attacks [8–10, 12] directly inject a backdoor into a
classifier. For instance, BadNets [8] injects a backdoor into
a classifier via poisoning its training images, i.e., adding a
backdoor trigger to the training inputs and changing their
labels to the target class. Liu et al. [10] proposed to first invert
a classifier to generate a backdoor trigger, and then inject the
backdoor into the classifier via retraining it.
Yao et al. [12] proposed latent backdoor attack (LBA) to
transfer learning. When extended to self-supervised learning,
LBA injects a backdoor into a teacher classifier built based on
the image encoder and a labeled dataset similar to the target
downstream dataset. When the backdoored teacher classifier is
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2054
used to fine-tune a student classifier for the target downstream
task, the student classifier inherits the backdoor behavior. The
key differences between LBA and our BadEncoder are as
follows. First, LBA requires a large labeled dataset for the
target downstream task. In contrast, our BadEncoder only
requires a few reference inputs from the target class and an
arbitrary unlabeled shadow dataset. Second, as demonstrated
by our experiments, even if such labeled data is available,
LBA achieves suboptimal attack effectiveness when extended
to self-supervised learning. Third, our BadEncoder can attack
multiple target downstream tasks simultaneously while LBA
only attacks a single target downstream task by design.
We note that Carlini et al. [61] recently proposed data
poisoning and backdoor attacks to self-supervised learning,
which is concurrent to ours. Their attacks poison the pre-
training dataset, but they were designed for (image, text) pairs
based self-supervised learning.
Text: Some studies [51, 62, 63] showed that natural language
classifiers are also vulnerable to backdoor attacks. For in-
stance, Zhang et al. [63] proposed backdoor attacks to pre-
trained language models. These studies are different from
ours as we focus on image encoders. Moreover, Zhang et al.
require an attacker to have substantial knowledge about the
downstream tasks. For instance, an attacker requires access to
a subset of the training dataset of the target downstream task.
In contrast, our attack does not require such information.
Graph: Backdoor attacks have also been studied for graph
classification [41, 53]. For instance, Zhang et al. [41] de-
veloped a subgraph based backdoor attack to GNNs. Xi et
al. [53] also proposed a subgraph based backdoor attack, which
considers both topological structures and descriptive features
when designing backdoor triggers.
We note that some studies [64–66] proposed targeted data
poisoning attacks that involve a feature extractor (i.e., pre-
trained image encoder in our context). For instance, Ji et
al. [65] modified a feature extractor such that a downstream
classifier predicts a particular attacker-chosen clean target
input as attacker-chosen target class. Shafahi et al. [66] adds
perturbation to some training inputs in the target class in the
target downstream dataset such that the clean feature extractor
produces similar feature vectors for the perturbed training
inputs and the attacker-chosen clean target inputs; and then
the downstream classifier trained using the perturbed training
inputs will predict the clean target inputs as the target class.
The key difference with our work is that our attack injects
backdoors into the image encoder such that a backdoored
downstream classifier predicts the target class for any input
embedded with a pre-defined trigger.
C. Defenses against Backdoor Attacks
Defenses against backdoor attacks can be categorized into
empirical defenses [13, 14, 32, 34–36, 67–71] and provable
defenses [15, 37–41, 72].
Empirical defenses: A family of empirical defenses [13, 14,
32, 34–36] aim to detect whether there is a backdoor (or a
trigger) in a classifier (or an input). For instance, Wang et
al. [13] proposed Neural Cleanse [13] which tries to detect
whether a classifier (i.e., a downstream classifier in our con-
text) is backdoored or not. In particular, they first try to reverse
engineer a trigger for each possible class and then use anomaly
detection to predict whether the classifier is backdoored or not.
Liu et al. [35] proposed to detect backdoor via analyzing the
behaviors of a neuron under different levels of stimulation.
Gao et al. [34] proposed STRIP, which predicts an input
has a trigger embedded if the predicted labels for randomly
perturbed versions of the input have small entropy. Another
family of empirical defenses [13, 33] try to remove the
backdoor in a classifier. For instance, Liu et al. [33] proposed
to first prune the neurons that are less informative and then
fine-tune the pruned classifier to remove the backdoor. Our
results show that Neural Cleanse and MNTD, two state-of-the-
art empirical defenses, cannot detect our backdoor attacks.
Provable defenses: Provable defenses [15, 37–41, 72–74]
can provide provable robustness guarantees against backdoor
attacks. In particular, given an input,
these defenses can
provably guarantee that the predicted label remains unchanged
when the trigger size is smaller than a threshold. For instance,
Wang et al. [40] leveraged randomized smoothing to mitigate
backdoor attacks and found that existing randomized smooth-
ing techniques provide limited provable robustness guarantees.
Since backdoor attacks embed a trigger/patch to a testing
input, provable defenses against adversarial patches can be
used to mitigate them. Chiang et al. [37] proposed the first
provable defense against adversarial patches, which leverages
interval bound propagation. Recently, Xiang et al. [15] pro-
posed PatchGuard which achieves the state-of-the-art certified
accuracy against adversarial patches. However, our experi-
mental results indicate that PatchGuard provides insufficient
robustness guarantees under our attacks.
IX. CONCLUSION AND FUTURE WORK
In this work, we show that pre-trained image encoders in
self-supervised learning are vulnerable to backdoor attacks.
Injecting backdoors to image encoders can be formulated as
an optimization problem, which can be solved by a gradient
descent based method. We also find that existing defenses
against backdoor attacks are insufficient to defend against
our attack. Interesting future work includes: 1) generalizing
our attack to self-supervised learning in other domains, e.g.,
natural language processing and graph, 2) developing new
defenses to defend against our attack, and 3) studying how to
pre-train an encoder (assume the encoder is clean) such that
the downstream classifiers built based on the encoder are more
robust against conventional backdoor attacks that compromise
the training of downstream classifiers [8–10].
ACKNOWLEDGEMENTS
We thank the anonymous reviewers and our shepherd Fabio
Pierazzi for constructive comments. This work was supported
by NSF under Grant No. 1937786 and the Army Research
Office under Grant No. W911NF2110182.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2055
visual representations by solving jigsaw puzzles,” in
ECCV, 2016.
[18] N. Srivastava, R. Salakhutdinov et al., “Multimodal
learning with deep boltzmann machines.” in NeurIPS,
2012.
[19] A. Joulin, L. Van Der Maaten, A. Jabri, and N. Vasilache,
“Learning visual features from large weakly supervised
data,” in ECCV, 2016.
[20] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,
K. Ni, D. Poland, D. Borth, and L.-J. Li, “Yfcc100m:
The new data in multimedia research,” Communications
of the ACM, vol. 59, no. 2, pp. 64–73, 2016.
[21] G. Li, N. Duan, Y. Fang, M. Gong, and D. Jiang,
“Unicoder-vl: A universal encoder for vision and lan-
guage by cross-modal pre-training,” in AAAI, 2020.
[22] A. Krizhevsky, “Learning multiple layers of features
from tiny images,” Tech Report, 2009.
[23] A. Coates, A. Ng, and H. Lee, “An analysis of single-
layer networks in unsupervised feature learning,” in AIS-
TATS, 2011.
[24] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel,
“Man vs. computer: Benchmarking machine learning
algorithms for traffic sign recognition,” Neural networks,
vol. 32, pp. 323–332, 2012.
[25] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and
A. Y. Ng, “Reading digits in natural images with unsu-
pervised feature learning,” in NIPS Workshop on Deep
Learning and Unsupervised Feature Learning, 2011.
[26] L. Bossard, M. Guillaumin, and L. Van Gool, “Food-
101 – mining discriminative components with random
forests,” in ECCV, 2014.
[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual
learning for image recognition,” in CVPR, 2016.
[28] “SimCLR,” https://github.com/google-research/simclr.
[29] “SimCLR
SimCLR.
https://github.com/leftthomas/
PyTorch,”
[30] “LBA,” http://sandlab.cs.uchicago.edu/latent/.
[31] “CLIP,” https://github.com/openai/CLIP.
[32] B. Tran, J. Li, and A. Madry, “Spectral signatures in
backdoor attacks,” in NeurIPS, 2018.
REFERENCES
[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
“Bert: Pre-training of deep bidirectional transformers for
language understanding,” in NAACL, 2019.
[2] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality
reduction by learning an invariant mapping,” in CVPR,
2006.
[3] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Mo-
mentum contrast for unsupervised visual representation
learning,” in CVPR, 2020.
[4] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,
“A simple framework for contrastive learning of visual
representations,” in ICML, 2020.
[5] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Gre-
wal, P. Bachman, A. Trischler, and Y. Bengio, “Learning
deep representations by mutual information estimation
and maximization,” in ICLR, 2019.
[6] J.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. H.
Richemond, E. Buchatskaya, C. Doersch, B. A. Pires,
Z. D. Guo, M. G. Azar et al., “Bootstrap your own latent:
A new approach to self-supervised learning,” in NeurIPS,
2020.
[7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark
et al., “Learning transferable visual models from natural
language supervision,” arXiv, 2021.
[8] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identify-
ing vulnerabilities in the machine learning model supply
chain,” IEEE Access, 2017.
[9] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted
backdoor attacks on deep learning systems using data
poisoning,” arXiv preprint arXiv:1712.05526, 2017.
[10] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang,
and X. Zhang, “Trojaning attack on neural networks,” in
NDSS, 2018.
[11] E. Bagdasaryan and V. Shmatikov, “Blind backdoors in
deep learning models,” in Usenix Security, 2021.
[12] Y. Yao, H. Li, H. Zheng, and B. Y. Zhao, “Latent
backdoor attacks on deep neural networks,” in CCS,
2019.
[13] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng,
and B. Y. Zhao, “Neural cleanse: Identifying and miti-
gating backdoor attacks in neural networks,” in IEEE S&
P, 2019.
[14] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and
B. Li, “Detecting ai trojans using meta neural analysis,”
in IEEE S & P, 2021.
[15] C. Xiang, A. N. Bhagoji, V. Sehwag, and P. Mit-
tal, “Patchguard: Provable defense against adversarial
patches using masks on small receptive fields,” in Usenix
Security, 2021.
[16] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and