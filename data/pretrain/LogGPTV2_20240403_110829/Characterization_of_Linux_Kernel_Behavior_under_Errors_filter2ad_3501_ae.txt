75.0%
12.5%
12.5%
NULL pointer
Bad paging request
0.8%
1.4%
3.0%
drivers
100%
45.5%
NULL pointer
fs
36.4%
18.2%
Bad paging request
kernel
93.2%
24.4%
38.3%
Invalid opcode
kernel
0.9%
kernel
1.5%
lib
mm
30.2%
5.3%
16.7%
22.2%
61.1%
(d)
92.9%
0.3%
30.3%
32.9%
26.5%
kernel
10.3%
Invalid opcode
kernel
General 
Protection fault
1.6%
lib
mm
33.3%
16.7%
General 
Protection fault
1.8%
50.0%
(e)
Figure 8: Error Propagation 
Error Propagation 
7.4 
The Linux kernel is a classical monolithic architecture, which 
means that kernel subsystems are tightly related to each other, 
even  though  most  of  kernel  components  are  accessed  via 
well-defined  interfaces.  In  this  section,  we  study  the  error 
propagation  between  the  location  of  an  error  injection  and 
that of the system crash. Figure 8 provides error propagation 
statistics for the two subsystems fs (the top three graphs) and 
kernel  (the  bottom  three  graphs)  for  each  of  the  three  error 
injection campaigns4.
The first node on each graph  refers to the  faulted  subsystem 
(i.e., the subsystem where an error is injected). The outgoing 
arcs  (transitions)  indicate  error  propagation  paths  (including 
the self loop).  The end node of each transition corresponds to 
the subsystem where the crash occurred.  The final transition 
from  the  end  nodes  indicates  the  type  of  the  crash.    For  ex-
ample,  Figure  8(a)  captures  crash  propagation  paths  for  fs
subsystem – 89.4% of all crashes happen inside fs subsystem, 
5.7%  of  injected  errors  propagate  to  and  crash  in  the  kernel
subsystem, and 38.6% of these crashes are due to an invalid 
operand.  Below,  we  summarize  the  major  findings  from  the 
error propagation analysis: 
•  The overall percentage of error propagation is small (less 
than  10%).  Approximately,  90%  of  crashes  occur  inside  the 
subsystem into which the error was injected. This observation 
agrees  with  the  results  from  earlier  studies  on  UNIX  system 
behavior,  where  it  has  been  shown  that  8%  of  errors  propa-
gate between  the subsystems  [14], but it is less than that  for 
the  Tandem  Guardian  operating  system,  where  18%  of  soft-
ware design errors caused error propagation [17].5
4 The analysis of error propagation has been also conducted for the other two 
target  subsystems  (i.e.,  arch  and  mm).  Due  to  space  limitations,  we  only 
report data for the fs and kernel subsystems. 
5 The study on the impact of transient errors (including error propagation) in 
LynxOS indicates that from 1% to 4.4% of errors propagate (with the higher 
percentages observed for application faults) [18].  
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. 
•  Errors  injected  into  the  fs  subsystem  have  the  highest 
probability  of  propagating.  In  particular,  the  primary  error 
(crash)  propagation  path  (5.7%)  is  to  the  kernel  subsystem. 
Recovery  from  crash  requires  reboot  the  system  (around  4 
minutes as shown in Section 7.1), which may have a signifi-
cant negative impact on system availability.  
•  Several critical error propagation paths can be identified, 
e.g., from fs to kernel (graphs (a) to (c)) and from kernel to 
fs and from kernel to mm (graphs (d) to (f) in Figure 8).  
•  Closer analysis of the propagation patterns indicates that 
it  is  feasible  to  identify  strategic  locations  for  embedding 
additional assertions in the source code of a given subsystem 
to detect errors and, hence, prevent error propagation. In this 
scenario,  when  an  assertion  fires,  an  appropriate  recovery 
action (e.g., termination of an offending application process) 
can be initiated to avoid a kernel crash. Doing so can signifi-
cantly  reduce  system  downtime  and  can  allow  achieving 
high availability. (Placing of assertion based on error propa-
gation analysis has been also suggested in [11]). 
As mentioned earlier, we encounter nine catastrophic kernel 
crashes,  which  require  reformatting  the  whole  file  system. 
The example analyzed in Section 7.1 shows that an error in-
jected  to  the  mm  subsystem  propagates  to  the  fs  subsystem 
and  makes the file system  unusable. For example, an asser-
tion can be embedded into the kernel code for checking the 
relationship between the variable index and the inode->i_size .
8  Experimental  Results:  Not  Manifested  Errors 
and Fail Silence Violations 
The  pie-charts  in  Figure  4  show  that  30-50%  of  activated 
errors  do  not  affect  kernel  or  application  functionality  (Not 
Manifested category). A closer case analysis of the examples 
from  this  category  reveals  that  the  possible  causes  include 
redundancy/optimization  coding  at  C  source  code  level  and 
instruction-inherent  factors.  Examples  of  such  cases  are  il-
lustrated below.  
Redundancy in C Source Code Level. The following piece of 
code is taken from the function reschedule_idle().
212 static void reschedule_idle(struct 
task_struct * p) 
213 { 
214 #ifdef CONFIG_SMP 
……
/* shortcut if the woken up task’s last 
* CPU is idle now. */
best_cpu = p->processor; 
223     if (can_schedule(p, best_cpu)) {
Valid  but  Incorrect  Branch  reverses  the  direction  of  the  if
statement in line 223. It turns out that in the single processor 
machine, can_schedule is always true. Consequently without 
error  injection,  the  body  of  if  is  taken,  which  simply  re-
schedules process p on the same processor and returns. With 
an error injected, the body of if is not taken, and since there 
is only one cpu, p is still scheduled onto the same processor. 
Not  Manifested  Errors  in  the  Random  Branch  Error  Cam-
paign. Not Manifested errors in campaign B (Random Branch 
Error) reach 47% of all activated cases (note that Not Mani-
fested errors in campaigns A and C constitute only 33% of all 
activated  errors).  This  difference  can  be  explained  by  the 
intrinsic features of the Linux kernel implementation, namely 
the  fact  that  in  most  of  the  execution  scenarios,  a  given 
branch  is  not  taken  (i.e.,  the  instruction  immediately  follow-
ing the branch is executed). The functionality of the branch is 
virtually the same as a nop instruction. Table 6 shows exam-
ples of Not Manifested errors observed in campaign B.
Fail Silence Violations in the campaign C (valid but incorrect 
branch,  in  Figure  4)  constitute  9.9%  of  all  activated  errors. 
This percentage is substantially higher than those in other two 
error injection campaigns (2% and 0.8% for campaigns A and 
B,  respectively).  Here,  the  error-checking  scheme  of  the 
Linux kernel plays an important role. When the control flow 
takes a valid but incorrect execution direction, the kernel de-
tects an error and returns with an error code to the user appli-
cation  program.  This  represents  a  fail  silence  violation  sce-
nario, since the kernel propagates incorrect data (i.e., notifica-
tion about an error) to the application. 
/* Seeks are not allowed on 
45
pipes.*/
46     ret = -ESPIPE; 
47     read = 0; 
48     if (ppos != &filp->f_pos) 
49       goto out_nolock; 
……
129 out_nolock: 
130     if (read) 
131       ret = read; 
132
133     UPDATE_ATIME(inode); 
134         return ret; 
135 }
The code on the 
right is taken from 
the function 
pipe_read(). In 
line 48, the func-
tion performs a 
check (ppos != 
&filp->f_pos), and 
if there is an error 
the control flow 
moves to line 129, 
which returns 
with. an error code (-ESPIPE). In campaign C, the condition 
of the if statement at line 48 is reversed. The kernel (falsely) 
detects an error and returns the error code. 
9  Conclusions 
This  paper  describes  a  series  of  fault/error  injection  experi-
ments  conducted  on  the  Linux  operating  system.  Using  a 
software-implemented  kernel  error  injector  and  instrumenta-
tion of the Linux kernel, we conduct extensive fault injection 
campaigns on selected kernel subsystems arch, fs, kernel and 
mm.  The  goal  is  to  analyze  and  quantify  the  response  of  the 
operating  system  to  a  variety  of  failure  scenarios  with  par-
ticular focus on detailed analysis of kernel crashes. Key find-
ings from the experiments are summarized below. 
•  Most  (95%) of  the  crashes  are  due  to  four  major  causes 
including  unable  to  handle  kernel  null  pointer  dereference, 
unable to handle a kernel paging request, general protection 
faults, and invalid operands. 
•  Nine errors in the kernel resulted in crashes (most severe 
crash category) which required reformatting the file system. 
The  process  of  bringing  up  the  system  can  take  nearly  an 
hour.  
•  Less  than  10%  of  the  crashes  are  associated  with  fault 
propagation  and  nearly  60%  of  crashes  latencies  are  within 
10 cycles. 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. 
Table 6: Causes of Not Manifested Errors in Random Branch Error Injection Campaign 
No 
Original Code 
After Inject Error 
Cause of Not Manifested Errors 
1 
2 
3
Binary 
Assembly 
Binary 
Assembly 
74 56   
je  c01144f4 
7c 56 
jl  c01144f4 
Before error is injected, the status flag is “greater”, thus je (jump if equal) will 
not be taken. After error is injected, jl (jump if less) is still not taken. 
0f84ed  
000000   
je c013a9bd 
0f80ed 
00 00 00 
jo c013a9bd 
Before  error  is  injected,  the  status  flag  is  “less”,  thus  je  will  not  be  taken. 
After error is injected, jo (jump if overflow) is still not taken. 
7456 
je  c0132548 
34 56 
xor $0x56,%al 
The error injected changes je to xor which alters the content of register %al. 
However, the instruction followed is: “mov %ecx, %eax” which assigns cor-
rect value to %eax. Thus the error does not manifest. 
No. 
Before error injection 
(machine code/assembly) 
After error injection 
(machine code/assembly)  
Description 
Table 7: Example Case Studies of Crash Causes 
1
2
3
4
85 d2       test   edx, edx 
75 28       jne    c014c7f1 
31 d2       xor    edx, edx 
…
     movzbl 0x1b (edx), eax 
85 d2       test   edx, edx 
74 28       je    c014c7f1 
31 d2       xor   edx, edx 
…
      movzbl 0x1b(edx), eax 
Unable to handle kernel NULL pointer at 0000001b 
8b 51 0c   mov 0xc(%ecx),%edx 
39 5d 0c   cmp  %ebx, 0xc(%ebp) 
8d 04 82   lea (%edx,%eax,4),  
8b 11       mov    (%ecx),%edx 
0c 39       or        $0x39, %al 
5d            pop      %ebp 
0c 8d       or         $0x8d, %al 
04 82       add       $0x82, %al 
89 45 c0  mov    %eax, 
                     0xffffffc0(%ebp) 
Unable to handle kernel page request at virtual address ffffffce 
                                            %eax 
89 45 c0   mov    %eax,0xffffffc0 
                                          (%ebp) 
Before error is injected, edx is 0x0, jne (jump if not equal) is not 
taken;
After error is injected, je (jump if equal) is taken; control flow 
goes to execute movzbl, which attempts to access data pointed by 
NULL pointer (stored in edx).  
An error makes the original three instructions (mov, cmp, lea) to 
be interpreted as a sequence of five instructions (mov,or,pop,or, 
add).  
Pop modifies ebp register, which causes the mov instruction to 
access an incorrect memory location at address ffffffce. 
8b 5d bc     mov
          0xffffffbc(%ebp),%ebx 
cb             lret 
5d             pop %ebp 
bc              in (%dx), %al
General protection fault 
Original mov instruction is corrupted to lret, which causes general 
protection fault.
74 08            je     c010510c  
0f 0b             ud2a    
75 08            jne     c010510c
0f 0b             ud2a    
invalid opcode 
C code:  if (!PageLocked(page))  BUG();
Valid but Incorrect Branch error  makes control flow go to BUG() 
which is ud2a (invalid opcode exception). 
Acknowledgments 
This work was supported in part by a MARCO Program grant 
SC #1010168/PC#2001-CT-888 Carnegie Mellon and in part 
by  NSF  grant  CCR  99-02026.  We  thank  Fran  Baker  for  her 
insightful editing of our manuscript. 
References 
[1] 
J. Arlat, et al., “Dependability of COTS Microkernel-Based Systems,” 
IEEE Transactions on Computers, 51(2), 2002. 
J.  Barton,  et  al.,  “Fault  Injection  Experiments  Using  FIAT, IEEE 
Transactions on Computers, 39(4), 1990. 
[2] 
[3]  M.  Beck,  et  al.,  “Linux  Kernel  Internals,”  Second  Edition,  Addison-
Wesley, 1998. 
[4]  K. Buchacker, V. Sieh, “Framework for Testing the Fault-Tolerance of 
Systems  Including  OS  and  Network  Aspects,”  Proc.  3rd  Intl.  High-
Assurance Systems Engineering Symposium, 2001. 
J.  Carreira,  H. Madeira, and  J.  Silva,  “Xception:  A  Technique  for  the 
Evaluation  of  Dependability  in  Modern  Computers,”  IEEE  Transac-
tions on Software Engineering, 24(2), 1998. 
[5] 
[6]  G.  Carrette, 
“CRASHME:  Random 
Input  Testing,”  1996, 
http://people.delphiforums.com/gjc/crashme.html 
[7]  H.  Cha,  et  al.,  “A  Gate-level  Simulation  Environment  for  Alpha-
Particle-Induced  Transient  Faults,  IEEE  Transactions  on  Computers,
45(11), 1996. 
[8]  G. Choi, R. Iyer and D. Saab, “Fault Behavior Dictionary for Simula-
tion of Device-level  Transients,”  Proc.  IEEE  InternationalConf.  Com-
puter-Aided Design, 1993. 
[9]  A. Chou, et al., “An Empirical Study of Operating Systems Errors,” In 
[11]  M.  Hiller,  et  al.,  “On  the  Placement  of  Software  Mechanism  for 
Detection of Data Errors,” in DSN-02, 2002.
[12]  M. Hsueh, T. Tsai, and R. Iyer, “Fault Injection Techniques and Tools 
IEEE Computer, 30(4), 1997. 
[13]  R.  Iyer,  D.  Rossetti,  M.  Hsueh,  “Maesurement  and  Modeling  of 
Computer  Reliability  as  Affected  by  System  Activity,”  ACM 
Transactions on Computer Systems, Vol.4, No.3, 1986.
[14]  W.  Kao,  et al, “FINE:  A  Fault  Injection and Monitoring Environment 
for Tracing the UNIX System Behavior Under Faults,” IEEE Trans. on 
Software Engineering, 19(11), 1993. 
[15]  P.  Koopman,  J.  DeVale,  “The  Exception  Handling  Effectiveness  of 
POSIX Operating Systems,” IEEE Transactions on Software Engineer-
ing, 26(9), 2000. 
[16]  N.  Kropp  et  al.,  “Automated  Robustness  Testing  of  Off-the-Shelf 
Software Components,” Proc. FTCS-28, 1998.
[17]  I. Lee and R. Iyer, “Faults, Symptoms, and Software Fault Tolerance in 
Tandem GUARDIAN90 Operating System,” Proc. FTCS-23, 1993. 
[18]  H.  Maderia,  et  al.,  “Experimental  evaluation  of  a  COTS  system  for 
space applications,” in DSN-02, 2002.
[19]  B.  P.  Miller,  et  al.,  “A  Re-examination  of  the  Reliability  of  UNIX 
Utilities and Services,” Tech. Rep., University of Wisconsin, 2000. 
[20]  Built-in Kernel Debugger (KDB), http://oss.sgi.com/projects/kdb/ 
[21]  Kernel Profiling (kernprof), http://oss.sgi.com/projects/kernprof/ 
[22]  Linux RAS Package, 
http://oss.software.ibm.com/linux/projects/linuxras/ 
[23]  M. Sullivan and R. Chillarege, “Software Defects and Their Impact on 
System Availability – A Study of Field Failures in Operating Systems,” 
Proc. FTCS-21, 1991. 
[24]  UnixBench, www.tux.org/pub/tux/benchmarks/System/unixbench 
[25]  D. Wilder, “LKCD Installation and Configuration,” 2002. 
Proc. of 18th ACM Symp. on Operating systems principles, 2001. 
http://lkcd.sourceforge.net/ 
[10]  M.  Godfrey  and  Q.  Tu,  “Evolution in  Open  Source  Software:  A  Case 
Study,” Proc. Intl. Conference on Software Maintenance, 2000. 
[26]  J. Xu, Z. Kalbarczyk, R. Iyer, “Networked Windows NT System Field 
Failure Data Analysis,” Proc. of Pacific Rim Intl' Symp. on Dependable 
Computing, 1999. 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply.