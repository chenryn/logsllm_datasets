structure of the snippet it analyses; therefore it is, e.g., unable to
infer that a traditionally reserved word (if ) is not always used as a
Keyword, but can be used as an Identifier (Table 1, line 3).
2.1.2 Abstract Syntax Tree (AST). Contrary to the previous tokens,
the AST describes the syntactic structure of an input sample, as it
rests upon the JavaScript grammar [15]. In particular, we use the
parser Esprima [23], which can produce up to 69 different syntactic
units, referred to as nodes. Inner nodes represent operators such as
VariableDeclaration, AssignmentExpression or IfStatement,
while the leaf nodes are operands, e.g., Identifier, Literal or
EmptyStatement. As an illustration, Figure 2 shows the Esprima
AST obtained from the code snippet of Listing 1 (for legibility
reasons, the variables’ names and values appear in the paper’s
graphical representations, but they are not part of the graphs). This
time, the construct x.if is recognized as a MemberExpression with
x and if being correctly labeled as Identifier. Still, the AST only
retains information about the nesting of programming constructs to
form the source code but does not contain any semantic information
such as control or data flow.
2.1.3 Control Flow Graph (CFG). Contrary to the AST, the CFG
allows to reason about the conditions that have to be met for specific
program’s paths to be taken. To this end, statements (predicates
and non-predicates) are represented by nodes that are connected
by labeled and directed edges to represent flow of control.
Since the Esprima AST comprises not only statements but also
non-statement and still non-terminal nodes, as shown in Figure 2,
we construct the CFG over the previous AST (we refer to the exten-
sion of the AST with control flow edges as CFG) so as not to lose
the relationships between nodes, which are both non-statement
and non terminal. We use the JavaScript grammar [15] to determine
which nodes are statements and which ones are not. Nevertheless,
we consider that a SwitchCase and a ConditionalExpression
are both statements, in order to indicate the conditional flow of
control originating from these two nodes. Then, we traverse the
AST depth-first pre-order and define two labels to link statement
Figure 2: AST corresponding to the code of Listing 1
Figure 3: AST of Listing 1 extended with control flow (red
dotted edges) and data flow (blue dashed edges)
1 x. if = 1;
2 var y = 1;
3 if (x. if == 1) {d = y ;}
Listing 1: JavaScript code example
nodes with a control flow dependency. The label e is used for edges
originating from non-predicate statements, while edges originating
from predicates are labeled with a boolean, standing for the value
the predicate has to evaluate to, for this path in the graph to be
chosen. Contrary to the AST of Figure 2, Figure 3 (considering only
the control flow edges) shows an execution path difference when
the if condition is true, and when it is not. Still, the CFG does not
enable to reason about the order in which statements are executed.
2.1.4 Program Dependency Graph (PDG). To this end, we build a
PDG [20] by adding data flow information to the previous CFG.
We connect statements with a directed data dependency edge iff a
variable (also including object and function) defined or modified at
the source node is used at the destination node, taking into account
its reaching definition. Since this code representation captures the
data and control flow between the different program components,
it is not influenced by arbitrary sequencing choices made by the
programmer. Contrary to the AST of Figure 2, Figure 3 indicates
the order in which statements from Listing 1 should be executed
(for legibility reasons, we drew the data dependencies between
ProgramExpressionStatementVariableDeclarationIfStatementAssignmentExpressionMemberExpressionLiteralIdentifierIdentifierxif1VariableDeclaratorIdentifierLiteraly1BinaryExpressionBlockStatementMemberExpressionLiteralIdentifierIdentifierxif1ExpressionStatementAssignmentExpressionIdentifierIdentifierdyExpressionStatementAssignmentExpressionMemberExpressionLiteralIdentifierIdentifierxIdentifierdataif1xVariableDeclarationVariableDeclaratorIdentifierLiteralyIdentifierdata1yIfStatementBinaryExpressionBlockStatementTrueMemberExpressionLiteralIdentifierif1ExpressionStatementeAssignmentExpressionIdentifierdACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Fass et al.
leaf nodes instead of their corresponding nearest statement nodes),
e.g., as shown by the data flow, lines 1 and 2 are executed before
line 3; still, we could swap lines 1 and 2 without altering the code
semantics.
Table 2: Number of relevant features per module
Tokens
602
24,912
AST
11,050
45,159
CFG
18,105
36,961
PDG-DFG
17,997
45,566
PDG
24,706
46,375
ngrams
value
static code analysis levels, and two ways of representing features
extracted from these different code representations.
N-Gram Features. To identify specific patterns in JavaScript doc-
uments, in the first scenario, we move a fixed-length window
of n symbols over each lexical or syntactic unit previously ex-
tracted, to get every sub-sequence of length n (n-grams) at each
position. For example, the first 2-grams of Table 1 are: (Identifier,
Punctuator), (Punctuator, Keyword) and (Keyword, Punctu-
ator). The use of n-grams feature enables a representation of how
the lexical and syntactic units were originally arranged in the ana-
lyzed JavaScript files, and is an effective means for abstracting the
files [32, 34, 36, 53, 54]. Thus, we build n-grams upon the lexical
units and the features previously extracted from the AST, CFG,
PDG-DFG and PDG (Section 2.2.1). We empirically evaluated differ-
ent n values, and selected n = 4, which provides the best trade-off
between detection accuracy and run-time performance. In the fol-
lowing, we use the keyword ngrams to refer to the 4-gram features
we built as described above.
Node Value Features. In the second scenario, we do not use n-
gram features, but combine each lexical unit with their correspond-
ing value (as presented in Table 1) and each syntactic unit extracted
from the AST, CFG, PDG-DFG, and PDG with their corresponding
Identifier/Literal value. For example, the first features of Fig-
ure 2 are (ExpressionStatement, x) and (AssignmentExpression,
x). In the following, we use the keyword value to refer to the fea-
tures combining lexical or syntactic units with their corresponding
value, as described above.
Features Space. JavaScript samples sharing several features
2.2.3
with the same frequency present similarities with one another,
while files with different features have a more dissimilar content.
Hence, analyzing the frequency of the features previously extracted
(ngrams and value, Section 2.2.2) is an indicator to accurately de-
termine if a given input is either benign or malicious.
To compare the frequency of the features appearing in several
JavaScript files, we construct a vector space such that each feature
is associated with one consistent dimension, and its corresponding
frequency is stored at this position in the vector. To limit the size
of the vector space, which has a direct impact on the performance,
we use the χ2 test to check for correlation. We select only features
for which χ2 ≥ 6.63, meaning that feature’s presence and script
classification are correlated with a confidence of 99% [52]. Table 2
presents the number of features considered for each of the ten
JStap modules based on our training set, which we describe in
Section 3.1.3. For the ngrams variant, there are more statistically
representative features when the complexity of the code represen-
tation structure increases, as complex graph structures lead here
to more edges. This also holds for the value approach, except for
the CFG traversal, for which we both have fewer representative
In particular, our PDG implementation respects JavaScript’s scop-
ing rules, makes the distinction between function declarations–a
standalone construct defining named function variables–and func-
tion expressions–functions that are part of larger expressions–and
handles lexical scoping. Also, we connect the function call nodes
to the corresponding function definition nodes with a data depen-
dency, thus defining the PDG at the program level [57].
2.2 Features Extraction
Once JStap built abstract code representations to analyze JavaScript
samples, we extract lexical units and traverse the different graphs
to collect syntactic units. Subsequently and for each code repre-
sentation, we consider (independently) n-gram features and the
combination of the extracted units with their corresponding node’s
value (variable’s name). Finally, learning components take the fre-
quency of such features as input for the classification process.
2.2.1 Graph Traversal. As far as the lexical analysis is concerned,
we already extracted lexical units (tokens) in Section 2.1.1. For the
AST, CFG and PDG, we need to traverse each graph by following
its specific edges to extract the name of each node (referred to as
a syntactic unit). Specifically, a depth-first pre-order traversal of
Figure 2 gives the following syntactic units: ExpressionStatement,
AssignmentExpression, MemberExpression, [...] Identifier (the
Program node just represents the root and does not have any syn-
tactic meaning). For the CFG, we also traverse the AST but only
store nodes linked by a control flow edge (i.e., the e, True and
False labels), e.g., on Figure 3: IfStatement, BlockStatement and
ExpressionStatement. In practice, considering only statement
nodes is not informative enough to distinguish benign from ma-
licious JavaScript inputs though (due to them linking the same
statements with one another, cf. Section 3.2.1). To add more con-
text information, we also traverse the sub-AST of each node with
a control flow once. For example, in Figure 3, JStap reports the
IfStatement node and traverses its sub-AST before following
the control flow and traversing the BlockStatement, then the
ExpressionStatement nodes. This time, we do not traverse their
corresponding sub-ASTs, as we already did it, while handling the
IfStatement node.2 Finally, the process is similar for the PDG,
with consideration of the data flow. In the following, we use the
term PDG-DFG (for Data Flow Graph) to refer to this traversal only
along data flow edges, and the term PDG to refer to the PDG tra-
versal through the data flow edges, followed by a second traversal
along the control flow edges.
Features Analysis. For the five previous abstract code rep-
2.2.2
resentations, namely tokens, AST, CFG, PDG-DFG, and PDG, we
(independently) consider n-gram features and the combination of
the extracted units with their corresponding node values to build
features. JStap therefore contains ten modules, with five different
2At the end, we retain the following units: IfStatement, BinaryExpression,
MemberExpression, Identifier, Identifier, Literal, BlockStatement,
ExpressionStatement,
Identifier,
BlockStatement, ExpressionStatement
AssignmentExpression,
Identifier,
JStap: A Static Pre-Filter for Malicious JavaScript Detection
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
features and fewer features in general than for the AST or PDG. We
assume that it comes from benign and malicious actors using more
similar variables name in statements with a control flow than in
other statements. This is confirmed to some extent in Section 3.2.2,
where this approach does not perform as well as the other ones. Fi-
nally, we store the frequency of each feature in Compressed Sparse
Row (CSR matrix) [10] to efficiently represent non-zero values.
2.3 Learning and Classification
The learning-based detection completes the design of our system.
We first build and leverage the CSR matrix of a representative and
balanced set of both benign and malicious JavaScript files to train
our classifier, as presented in Section 3.1.3. We empirically evaluated
several off-the-shelf systems (Bernoulli naive Bayes, multinomial
naive Bayes, Support Vector Machine (SVM), and random forest),
and selected random forest, which provided the most reliable de-
tection results, with the best true-positive and true-negative rates.
3 COMPREHENSIVE EVALUATION
In this section, we outline the results of our evaluation of JStap. In
particular, we leverage a high-quality dataset from various sources,
totaling over 270,000 unique samples. First, we study and justify the
detection rate of all ten modules of JStap, comparing them with one
another and analyzing their high(er) detection performance, before
comparing our implementations with closely-related work, and
explaining why our systems perform better. Finally, we analyze the
detection accuracy of a detector combining the predictions of three
JStap modules, before evaluating the overall run-time performance.
3.1 Experimental Protocol
The experimental evaluation of our approach rests upon two exten-
sive datasets, with a total size over 6.2 GB. The first one contains
131,448 SHA1-unique malicious JavaScript samples and the second
one 141,768 unique benign files. We used these two datasets to both
train and test our random forest classifier on.
3.1.1 Malicious Dataset. Our malicious dataset (Table 3) is a collec-
tion of samples mainly provided by the German Federal Office for
Information Security (BSI) [8]. These samples have been labeled as
malicious based on a score provided by the combination of antivirus
systems, malware scanners, and a dynamic analysis. To reduce pos-
sible similarities between samples from the same source, we got the
malware collection of Hynek Petrak (Hynek) [43], exploit kits from
Kafeine DNC (DNC) [27] and GeeksOnSecurity (GoS) [21], and
additional samples from VirusTotal [50]. This way, our malicious
dataset contains different samples performing various activities. For
example, we have JScript-loaders leading to, e.g., drive-by download
or ransomware attacks, and exploit kits (e.g., Blackhole, Donxref,
RIG) targeting vulnerabilities in old versions of Java, Adobe Flash
or Adobe Reader plugins, also trying to exploit old browsers ver-
sions. Most of these samples are obfuscated, e.g., through string
manipulation, dynamic arrays, encoding obfuscation [56]. Even
though the samples are labeled by their sources, in some cases, we
extracted JavaScript from HTML documents and thereby had to
ensure that the maliciousness lay in the script and was not, e.g.,
contained in an SWF bundle. For this purpose, we manually ana-
lyzed our 19,942 extracted JavaScript samples, 15,475 of which are
Table 3: Malicious JavaScript dataset
Source
BSI
Hynek
DNC
GoS
VirusTotal
Total
#JS
83,361
29,558
12,982
2,491
3,056
131,448
Creation
2017-2018
2015-17
2014-18
2017
2018
2014-18
Obfuscated
y
y
y
y