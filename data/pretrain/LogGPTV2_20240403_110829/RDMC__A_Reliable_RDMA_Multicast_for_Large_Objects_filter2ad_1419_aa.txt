title:RDMC: A Reliable RDMA Multicast for Large Objects
author:Jonathan Behrens and
Sagar Jha and
Ken Birman and
Edward Tremel
2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
RDMC: A Reliable RDMA Multicast for Large Objects
Jonathan Behrens1,2, Sagar Jha1, Ken Birman1, Edward Tremel1
1Department of Computer Science, Cornell University
2MIT CSAIL
Introduction
Abstract
Multicast patterns are common in cloud computing and data-
center settings. Applications and infrastructure tools such as
Spark frequently move large objects around, update ﬁles repli-
cated to multiple nodes, or push new versions of programs to
compute nodes. Some applications use replication directly, for
example to increase fault-tolerance or achieve parallelism. Im-
plementations of Paxos, block chains and other libraries of-
ten employ a hand-built reliable multicast as a primitive. Yet
operating systems continue to be focused on point-to-point
communication solutions such as TCP. Our system, RDMC
(RDMA Multicast), offers reliable multicast functionality con-
structed from RDMA unicast. We discuss design choices,
present a theoretical analysis of RDMC’s robustness to delays
and slow network links, and report on experiments that evalu-
ate RDMC over Mellanox RDMA.
1
Datacenter loads are heavily dominated by data copying de-
lays, often from a source node to two or more destinations.
By 2011, distributed ﬁle systems like Cosmos (Microsoft),
GFS (Google), and HDFS (Hadoop) handled many petabytes
of writes per day (hundreds of Gb/s) [6], and the throughput
is surely far higher today. Many ﬁles are replicated to multi-
ple storage servers [8]. The latency of this process determines
overall write performance for end-user applications. At Face-
book, Hadoop traces show that for jobs with reduce phases,
the transfer of data between successive phases represents 33%
of total run time [4]. Google’s Borg has a median task startup
latency of around 25 seconds (about 80% devoted to package
installation) with upwards of 10,000 tasks starting per minute
in some cells [22]. In some cases, copying VM images and in-
put ﬁles takes substantially more time than computation [19].
Despite the importance of fast replication, effective general-
purpose solutions are lacking. Today, cloud middleware sys-
tems typically push new data to nodes in ways that make one
copy at a time. Content sharing is often handled through an
intermediary caching or a key-value layer, which scales well
but introduces extra delay and copying. In parallel platforms
like Hadoop the scheduler often can anticipate that a collection
of tasks will read the same ﬁle, yet unless the data happens to
be cached locally, it will be moved point-to-point as each task
opens and accesses that ﬁle. Cloud systems could substan-
tially improve efﬁciency by recognizing such interactions as
instances of a common pattern. Doing so makes it possible
to recover network bandwidth and CPU time currently lost to
extraneous transfers and unneeded copying. For time-critical
uses, such a primitive would reduce staleness.
Our RDMA multicast protocol, RDMC, solves this prob-
lem, offering higher speed with sharply lower resource utiliza-
tion. RDMC is inexpensive to instantiate, and offers a relia-
bility semantic analogous to that of N side-by-side TCP links,
one per receiver. The protocol is also robust to disruption and
offers fair division of bandwidth, as we demonstrate using ex-
periments that expose RDMC to scheduling delays, link con-
gestion, and overlapping delivery patterns.
RDMC can also be extended to offer stronger semantics.
In work reported elsewhere, we describe Derecho [9]: a new
open-source software library layered over RDMC that sup-
ports atomic multicast as well as a classic durable Paxos. To
gain these properties, Derecho introduces a small delay, during
which receivers buffer messages and exchange status informa-
tion. Delivery occurs when RDMC messages are known to
have reached all destinations. No loss of bandwidth is experi-
enced, and the added delay is surprisingly small.
The contributions of the present paper are as follows:
• We describe RDMC in detail, showing how it maps mul-
ticast transfers to an efﬁcient pattern of RDMA unicast
operations.
• We undertake an extensive evaluation of the system.
• We show that RDMC is robust to scheduling and network
delays and discuss options for recovering in the rare event
of a failed transfer.
• We argue that because RDMC generates a deterministic
block transfer pattern, it offers a stepping stone towards
ofﬂoading reliable multicast directly onto the NIC.
2 Background on RDMA
RDMA (remote direct memory access) is a zero-copy commu-
nication standard. It has been used for many years on Inﬁni-
band, but is now also working robustly on standard datacenter
Ethernet [15, 25].
RDMA is a user-space networking solution, accessed via
lock-free data structures shared between user
queue pairs:
code and the network controller (NIC), consisting of a send
queue and a receive queue. RDMA supports several modes of
2158-3927/18/$31.00 Â©2018 IEEE
DOI 10.1109/DSN.2018.00020
71
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
operation. RDMC makes use of reliable two-sided RDMA op-
erations, which behave similarly to TCP. With this mode, the
sender and receiver bind their respective queue pairs together,
creating a session fully implemented by the NIC endpoints. A
send is issued by posting a memory region to the send queue,
and a process indicates its readiness to receive by posting a
memory region to the receive queue. The sender NIC will
then transmit the data, awaiting a hardware-level ack. After
a speciﬁed timeout, the NIC retries; after a speciﬁed number
of retries, it breaks the connection and reports failure (as ex-
plained below, RDMC won’t start to send unless the receiver is
ready, hence a broken connection indicates a genuine network
or endpoint failure). Once a send and the matching receive are
posted, the data is copied directly from the sender’s memory to
the receiver’s designated location, reliably and at the full rate
the hardware can support. A completion queue reports out-
comes. End-to-end software resending or acknowledgments
are not needed: either the hardware delivers the correct data
(in FIFO order) and reports success, or the connection breaks.
If processes P and Q wish to set up a two-sided RDMA
connection, they must ﬁrst exchange a form of key (RDMA
lacks the equivalent of the TCP listen operation, and has no
hardware-layer 3-way handshake). RDMC can support multi-
ple overlapping sessions, and they can be created as needed,
hence the need to exchange keys can arise without warning.
To minimize delay, RDMC creates a full N ∗ N set of TCP
connections during bootstrap, then uses them for RDMA con-
nection setup and failure reporting, as explained below.
RDMA offers several additional modes: a one-sided read
and write mode (Q authorizes P to directly access some mem-
ory region), data-inlining, unreliable point-to-point datagrams,
and an unreliable multicast. These features are intended for
small transfers, and because RDMC focuses on large transfers
we did not ﬁnd them useful, with one exception: as each re-
ceiver becomes ready to accept an incoming transfer, it does a
a one-sided write to tell the sender, which starts sending only
after all are prepared.
Evolution of RDMA NIC programmability. There is grow-
ing interest in programmable network devices. For RDMA
NICs, this may introduce new request-ordering options.
Today’s RDMA NICs guarantee two forms of ordering: (1)
requests enqueued on a single send or receive queue will be
performed in FIFO order (2) a receive completion occurs only
after the incoming transfer is ﬁnished. Mellanox’s CORE-
Direct [14] feature proposes a third form of request ordering:
it is possible to enqueue an RDMA send that will wait both
until the prior request has completed, as well as for comple-
tion of some other RDMA send or receive, possibly even on
a different queue pair. In cases where a node Q needs to re-
lay data received from P to another node R, this avoids the
software delay at Q to issue the relay operation after the re-
ceive is complete. We believe that CORE-Direct is just one
of what will eventually be a wide range of new RDMA NIC
programmability features.
RDMC was designed to anticipate this trend, although the
hardware functionality isn’t fully mature yet and hence seri-
ous evaluation of the potential will require additional work.
RDMC can precompute data-ﬂow graphs describing the full
pattern of data movement at the outset of each multicast send.
Members of a replication group could thus post data-ﬂow
graphs at the start of a transfer, linked by cross-node send/re-
ceive dependencies. The hardware would then carry out the
whole transfer without further help. Ofﬂoading would elimi-
nate the need for any software actions, but creates an interest-
ing scheduling puzzle: if operations are performed as soon as
they become possible, priority inversions could arise, whereby
an urgent operation is delayed by one that actually has sub-
stantial scheduling slack. As these new hardware capabilities
mature, we hope to explore such questions.
3 High level RDMC summary
We implemented RDMC using the two-sided RDMA opera-
tions described above. The basic requirement is to create a
pattern of RDMA unicasts that would efﬁciently perform the
desired multicast. In the discussion that follows, the term mes-
sage refers to the entire end-user object being transmitted: it
could be hundreds of megabytes or even gigabytes in size.
Small messages are sent as a single block, while large mes-
sages are sent as a series of blocks: this permits relaying pat-
terns in which receivers simultaneously function as senders.
The beneﬁt of relaying is that it permits full use of both the
incoming and outgoing bandwidth of the receiver NICs.
In
contrast, protocols that send objects using a single large uni-
cast transfer are limited: any given node can use its NIC in just
one direction at a time.
This yields a framework that operates as follows:
1. For each RDMC transfer, the sender and receivers ﬁrst
create an overlay mesh of multi-way bindings: an RDMC
group. This occurs out of band, using TCP as a boot-
strapping protocol. RDMC is lightweight and can sup-
port large numbers of overlapping groups, but to mini-
mize bootstrap delay, applications that will perform re-
peated transfers should reuse groups when feasible.
2. Each transfer occurs as a series of reliable unicast RDMA
transfers, with no retransmission. RDMC computes se-
quences of sends and receives at the outset and queues
them up to run as asynchronously as possible. As noted
earlier, it should eventually be feasible to ofﬂoad the en-
tire sequence to a programmable NIC.
3. On the receive side, RDMC notiﬁes the user application
of an incoming message, and it must post a buffer of the
correct size into which bytes are received.
4. Sends complete in the order they were initiated. Incom-
ing messages are guaranteed to not be be corrupted, to
arrive in sender order, and will not be duplicated.
5. RDMA apportions bandwidth fairly if there are several
active transfers in one NIC. RDMC extends this property,
offering fairness for overlapping groups.
6. If an RDMA connection fails,
the non-crashed end-
point(s) learn of the event from their NICs. RDMC re-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
72
// Create a new group with the designated members (first member is the root).
bool create_group(int group_number,
vector members,
function incoming_message_callback,
function message_completion_callback);
// Destroy the group, and deallocate associated resources.
void destroy_group(int group_number);
// Attempt to send a message to the group. Will fail if not the root.
bool send(int group_number, char* data, int size);
Figure 1: RDMC library interface
lays these notiﬁcations, so that all survivors eventually
learn of the event. The application can then self-repair by
closing the old RDMC session and initiating a new one.
4 System Design
4.1 External API
Figure 1 shows the RDMC interface, omitting conﬁguration
parameters like block size. The send and destroy group
functions are self-explanatory. The create group func-
tion is called concurrently (with identical membership in-
formation) by all group members; we use the out-of-
band TCP connections mentioned earlier to initiate this
step. create group takes two callback functions, which
will be used to notify the application of events.
The
incoming message callback is triggered by receivers
when a new transfer is started, and is also used to obtain a
memory region to write the message into. Memory registra-
tion is expensive, hence we perform this step during startup,
before any communication activity occurs.
The message completion callback triggers once a message
send/receive is locally complete and the associated memory
region can be reused. Notice that this might happen before
other receivers have ﬁnished getting the message, or even after
other receivers have failed.
Within a group, only one node (the “root”) is allowed to
send data. However, an application is free to create multi-
ple groups with identical membership but different senders.
Note that group membership is static once created: to change
a group’s membership or root the application should destroy
the group and create a new one.
4.2 Architectural Details
RDMC runs as a user-space library. Figure 2 shows an
overview of its architecture.
Initialization. When the application ﬁrst launches, its mem-
bers must initialize RDMC. At this point, RDMC creates the
mesh of TCP connections mentioned earlier, registers memory,
creates a single RDMA completion queue, and prepares other
internal data structures. Later, during runtime, all RDMC ses-
sions share a single completion queue and thread, reducing
overheads. To avoid polling when no I/O is ocuring, the com-
Sender 
Application 
Memory 
region 
)
(
d
n
e
s
RDMC 
NIC 
Receiver 
Application 
Memory 
region 
r
e
g
i
s
t
e
r
RDMC 
NIC 
Receiver 
Application 
Memory 
region  
C
a
l
l