Figure 6: Performance evaluation of pessimistic single-node transactions under TPC-C and YCSB benchmarks. YCSB
performance is evaluated with a write heavy (20 % reads) and a read heavy (80 % reads) workload.
Figure 7: Performance evaluation of optimistic single-node transaction under TPC-C and YCSB benchmarks. YCSB
performance is evaluated with a write heavy (20 % reads) and a read heavy (80 % reads) workload.
Enc adds minimal overhead compared to the non-encrypted
versions. Further, SCONE’s overheads are reasonable. TREATY
w/o Enc has roughly 1.6× slowdown compared to RocksDB
while TREATY w/ Enc has 2× slowdown. Lastly, the stabi-
lization period seems not to have great impact on the overall
throughput. We experience a 2.1× slowdown compared to
RocksDB. Regarding the latency, we see that all TREATY
SCONE versions do not scale as good as the native execution.
However,
the latency of SCONE systems is equivalent or
smaller to the natively executed versions. This behavior is
reasonable since the native versions are “saturated” to 64
clients while the SCONE versions to 32 clients.
Additionally, Figure 6 shows the throughput and latency
of all 6 systems for the two YCSB workloads. YCSB’s
conﬁguration, in contrast to TPC-C, present little conﬂicts.
That said, for the read-heavy workload, encryption, adds a
throughput overhead of 1.3× and 2.7× compared to native and
SCONE versions respectively while the respective overheads
for latency are 1.6× and 4.6×. For the write-heavy workload,
we have 1.2× and 2.8× slowdown to native and SCONE
versions compared with RocksDB. The latency overheads are
1.5× and 4.7× respectively. Similarly to TPC-C, TREATY’s
stabilization function does not impact performance dramat-
ically. We experience 3.5× slowdown for the read-heavy
workload and 3.2× slowdown with respect to RocksDB for
the write-heavy workload compared to when de-activating the
stabilization mechanism. Further, especially for the read-heavy
workload, we ﬁnd out that TREATY w/ Enc w/ Stab takes
advantage of the “idle” (stabilization) time to improve the
scalability; TREATY w/ Enc w/ Stab becomes saturated in 64
clients while the otherversions are saturated in 32 clients.
Optimistic Txs. Figure 7 shows that TREATY w/ Enc w/ Stab
performs 5× and 4× worse compared to the native RocksDB
for TPC-C and YCSB, respectively. We see that TREATY’s sta-
bilization does not incur extra throughput overhead compared
to the TREATY w/ Enc as the system, thanks to our userspace
ﬁber scheduler, continues to process requests. TREATY w/ Enc
w/ Stab’s compared to TREATY w/ Enc experiences roughly
10 % latency overhead. Further, we notice that TREATY w/ Enc
w/ Stab’s saturation point under YCSB is 128 clients while
RocksDB’s one is 32. TREATY shows similar overheads as
SPEICHER [31] which is the most related system.
E. Network Library for Txs
We evaluate the performance of TREATY’s networking
library using iPerf against six baselines: eRPC (SCONE), eRPC
(native), iPerf-UDP (native), iPerf-UDP (SCONE), iPerf-TCP
(native), and iPerf-TCP (SCONE). All native (eRPC and iPerf)
versions do not provide any security. Additionally, SCONE
(eRPC and iPerf) versions do not secure network layer; we
only use the secure message format for TREATY-networking.
Note that iPerf build with SCONE is optimized w.r.t to SGX
since SCONE uses the async syscalls [26] for performance.
For the sockets (native and SCONE), we use iPerf to
measure the throughput. For the eRPC versions and TREATY-
networking, we implement a client-server model with eRPC to
implement iPerf. Our experiments saturate network bandwidth
where we compare the performance with different packet sizes.
iPerf supports TCP and UDP, eRPC supports only UDP.
Figure 8 shows the throughput in network bandwidth for
all seven systems discussed (TREATY networking, eRPC
(SCONE), eRPC (native),
iPerf-UDP
iPerf-UDP (native),
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
23
Slowdown
Slowdown
Version
TREATY w/o Enc
2.0×
Table I: Recovery overheads w.r.t. native recovery.
1.5×
Version
TREATY
Figure 8: Throughput in network bandwidth of TREATY-
networking, eRPC (native and SCONE),
iPerf-TCP and
iPerf-UDP (native and SCONE).
(SCONE), iPerf-TCP (native), and iPerf-TCP (SCONE)). We
see that eRPC is comparable to iPerf-TCP while iPerf-UDP
performs poorly. Especially for large messages (> MTU),
UDP throughput equals zero as many messages are dropped.
In contrast to UDP, TCP performs equivalently and better than
eRPC. We deduct this to the fact that TCP is optimized for high
speed bulk transfers and, additionally, the entire TCP/IP stack
processing is frequently ofﬂoaded to the network controller.
For small and medium packets sizes that are still smaller
than the MTU (1460 B), we observe performance differences
between eRPC and iPerf-TCP. Especially, for packet sizes
of 256 B and 1024 B, eRPC shows roughly 30 % and 22 %
slowdown respectively compared to iPerf-TCP. For larger mes-
sages, both eRPC and iPerf-TCP perform almost equivalently.
We deduce two core conclusions: (a) SCONE’s overhead is
signiﬁcant—SCONE deteriorates up to 8× iPerf-TCP (SCONE)
while up to 4× eRPC; and (b), due to the amount of syscalls,
eRPC in SCONE performs up to 1.5× faster than iPerf-TCP
(SCONE). As discussed, syscalls execution in the enclave
incurs heavy overheads. Note the bigger the packet size is,
the worse the performance becomes. Lastly, we see that
TREATY network stack which also fully secures the network
and includes the encryption overheads performs equivalently
to iPerf-TCP (SCONE) that do not provide any security. As a
result, iPerf-TCP (SCONE) is an inappropriate design.
F. Recovery Protocol
We next evaluate the overheads of TREATY recovery w/ and
w/o Enc compared with native recovery. We construct logs of
800K entries each that lead to log sizes of 69 MiB and 91 MiB
for the non-encrypted and encrypted entries respectively. In
this experiment we use relatively small log entries (e.g 100 B
per log entry) which is the worse case for TREATY as: (i) we
have more syscalls, and (ii) we have more decryption calls.
Table I shows that TREATY recovery without decryption
costs incurs roughly 1.5× slowdown compared to the native
recovery. Further, encryption increases the overheads by up to
2× slower than the native recovery.
IX. RELATED WORK
Conﬁdential computing frameworks [23], [25], [47], [77]
use TEEs to build secure systems [31], [78]–[85]. TREATY
leverages SCONE to build the ﬁrst secure distributed transac-
tional KV storage system with TEEs.
Secure systems for cloud computing [75], [80], [86]–[95]
offer different security properties, interfaces, threat model, and
security enforcement mechanisms. EnclaveDB [80] is the most
related work. In contrast to TREATY, it (1) is a single-node
in-memory system (w/o persistence and distribution), (2) runs
in emulated h/w and, (3) assumes unlimited enclaves. TREATY
targets a distributed storage system, where we extend the
security properties to storage and network and overcome the
limitations of TEEs. Other storage systems vary on hardware,
security guarantees and interfaces: KV APIs [31], [78], [96]
and ﬁlesystems [97]–[99]. Precursor [94] combines SGX with
RDMA ofﬂoading the cryptographic operations to clients. In
contrast, TREATY provides distribution, persistency and Txs.
Secure distributed storage systems [100]–[102] provide
consistency, durability, availability and integrity. Cloud-
Proof [102], as TREATY, distrusts the cloud provider but it
requires (1) clients to guarantee these security properties and
(2) a trusted proxy which limits scalability. TREATY leverages
TEEs to avoid such limitations.
Other distributed systems [2], [27]–[29] deploy RDMA as
TREATY. However, we target security which is more chal-
lenging; DMA connections for direct I/O are not allowed by
TEEs. ShieldBox [83] uses DPDK to overcome this limitation,
but it targets only layer 2 in the OSI model which is limiting
for distributed systems. SPEICHER [31] uses SPDK [71] for
direct I/O to the SSDs. rkt-io [49] provides a library OS in
the enclave including a full network stack. We build on these
advancements to build a secure direct network I/O mechanism
for TEEs with which we design a 2PC protocol.
X. CONCLUSION
In this paper, we present TREATY, a secure distributed
transactional KV store for untrusted cloud environments.
TREATY offers high-performance serializable Txs with strong
security properties.We achieve these design goals by building
on hardware-assisted secure Txs with SGX and designing a
distributed 2PC protocol with a direct I/O network library
based on eRPC. Further, we design a stabilization protocol
for Txs using an asynchronous trusted counter interface along
with a distributed attestation service. We implement an end-
to-end secure Tx processing system from the ground-up based
on RocksDB/SPEICHER as the underlying storage engine. Our
evaluation with the YCSB and TPC-C shows reasonable over-
heads for TREATY, while it provides strong security properties.
Software artifact. TREATY is publicly available: https://
github.com/TUM-DSE/Treaty.
Acknowledgements. We thank our shepherd, Prof. Fernando
Pedone. We also thank Dr. Le Quoc Do, Dimitris Stavrakakis
and Prof. Jana Giceva for their helpful comments. This work
was supported in parts by a Microsoft Research PhD Fellow-
ship and Huawei Research, UK RISE and BaCaTeC Grants.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
24
[21] Microsoft
Azure,
“Azure
conﬁdential
computing,”
https://azure.microsoft.com/en-us/solutions/conﬁdential-compute/,
last accessed: Jan, 2021.
[22] “Introducing Google
Computing with
Conﬁdential VMs,” https://cloud.google.com/blog/products/identity-
security/introducing-google-cloud-conﬁdential-computing-with-
conﬁdential-vms,
Available:
introducing-google-cloud-conﬁdential-computing-with-conﬁdential-vms
[Online].
https://cloud.google.com/blog/products/identity-security/
Conﬁdential
accessed:
Cloud
2021.
Jan,
last
[23] S. Arnautov, B. Trach, F. Gregor, T. Knauth, A. Martin, C. Priebe,
J. Lind, D. Muthukumaran, D. O’Keeffe, M. L. Stillwell, D. Goltzsche,
D. Eyers, R. Kapitza, P. Pietzuch, and C. Fetzer, “SCONE: Secure
Linux Containers with Intel SGX,” in Proceedings of the 12th USENIX
Symposium on Operating Systems Design and Implementation (OSDI),
2016.
[24] S. Shinde, D. Le Tien, S. Tople, and P. Saxena, “PANOPLY: Low-
TCB Linux Applications with SGX Enclaves,” in Proceedings of the
Network and Distributed System Security Symposium (NDSS), 2017.
[25] C.-C. Tsai, D. E. Porter, and M. Vij, “Graphene-SGX: A practical
library OS for unmodiﬁed applications on SGX,” in Proceedings of
the USENIX Annual Technical Conference (USENIX ATC), 2017.
[26] L. Soares and M. Stumm, “FlexSC: Flexible System Call Scheduling
with Exception-less System Calls,” in Proceedings of the 9th USENIX
Symposium on Operating Systems Design and Implementation (OSDI),
2010.
[27] A. Kalia, M. Kaminsky, and D. G. Andersen, “FaSST: Fast, Scalable
and Simple Distributed Transactions with Two-Sided (RDMA) Data-
gram RPCs,” in 12th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 16), 2016.
[28] A. Shamis, M. Renzelmann, S. Novakovic, G. Chatzopoulos, A. Drago-
jevi´c, D. Narayanan, and M. Castro, “Fast General Distributed Trans-
actions with Opacity,” in Proceedings of the 2019 International Con-
ference on Management of Data (SIGMOD’19), 2019.
[29] X. Wei, J. Shi, Y. Chen, R. Chen, and H. Chen, Fast In-Memory
Transaction Processing Using RDMA and HTM, 2015.
[30] “Intel,
“SGX documentation:
sgx create monotonic
counter”,”
https://software.intel.com/en-us/sgx-sdk-dev-reference-sgx-create-
monotonic-counter/, last accessed: Dec, 2018.
[31] M. Bailleu, J. Thalheim, P. Bhatotia, C. Fetzer, M. Honda, and
K. Vaswani, “SPEICHER: Securing lsm-based key-value stores using
shielded execution,” in 17th USENIX Conference on File and Storage
Technologies (FAST), 2019.
[32] F. Gregor, W. Ozga, S. Vaucher, R. Pires, D. L. Quoc, S. Arnautov,
A. Martin, V. Schiavoni, P. Felber, and C. Fetzer, “Trust Management
as a Service: Enabling Trusted Execution in the Face of Byzantine
Stakeholders,” in 50th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks (DSN 2020), 2020.
[33] C. H. Papadimitriou, “The serializability of concurrent database
updates,” J. ACM, vol. 26, no. 4, p. 631–653, Oct. 1979. [Online].
Available: https://doi.org/10.1145/322154.322158
rocksdb
[34] “How
adoption?”
big
is
https://rocksdb.org/docs/support/faq.html, last accessed: May 2021.
[35] “CockroachDB,” https://www.cockroachlabs.com/, last accessed: May
2021.
Jan, 2021.
[36] A. Kalia, M. Kaminsky, and D. Andersen, “Datacenter RPCs can be
General and Fast,” in 16th USENIX Symposium on Networked Systems
Design and Implementation (NSDI), 2019.
[37] “TPC-C,” http://www.tpc.org/tpcc/, April 4, 2022.
[38] “YCSB,” https://github.com/brianfrankcooper/YCSB,
last accessed:
[39] A. Lakshman and P. Malik, “Cassandra: structured storage system
on a p2p network,” in Proceedings of the 28th ACM Symposium on
Principles of distributed computing (PODC). ACM, 2009.
[40] “MongoDB,” https://www.mongodb.com/, last accessed: May 2021.
[41] “Couchbase,” https://www.couchbase.com/, last accessed: May 2021.
[42] L. Bindschaedler, A. Goel, and W. Zwaenepoel, “Hailstorm: Disag-
gregated compute and storage for distributed lsm-based databases,” in
Proceedings of the Twenty-Fifth International Conference on Archi-
tectural Support for Programming Languages and Operating Systems
(ASPLOS ’20), 2020.
[43] “LevelDB,” http://leveldb.org/, last accessed: Dec, 2018.
[44] “Apache HBase,” https://hbase.apache.org/, last accessed: May 2021.
[45] “Apache AsterixDB,” https://asterixdb.apache.org/, last accessed: May
REFERENCES
[1] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman,
A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels, “Dynamo:
Amazon’s highly available key-value store,” ACM SIGOPS Operating
Systems Review (SIGOPS), 2007.
[2] A. Dragojevi´c, D. Narayanan, M. Castro, and O. Hodson, “FaRM: Fast
remote memory,” in 11th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 14), 2014.
[3] “RocksDB, A persistent key-value store,” https://rocksdb.org/,
last
accessed: Dec, 2018.
[4] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman,
S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild, W. Hsieh, S. Kan-
thak, E. Kogan, H. Li, A. Lloyd, S. Melnik, D. Mwaura, D. Nagle,
S. Quinlan, R. Rao, L. Rolig, Y. Saito, M. Szymaniak, C. Taylor,
R. Wang, and D. Woodford, “Spanner: Google’s Globally Distributed
Database,” 2013.
[5] Amazon,
“Amazon
S3
Cloud
Object
Storage,”
https://aws.amazon.com/s3, last accessed: Dec, 2018.
[6] Microsoft, “Azure Blob Storage,” https://azure.microsoft.com/en-
us/services/storage/blobs, last accessed: Dec, 2018.
[7] Google, “Cloud Storage,” http://www.cloud.google.com/storage, 2017,
last accessed: Dec, 2018. [Online]. Available: https://cloud.google.
com/storage/
[8] Dell,
“Elastic
https://www.dellemc.com/en-
us/storage/ecs/, 2017, last accessed: Dec, 2018. [Online]. Available:
https://www.dellemc.com/en-us/storage/ecs/index.htm