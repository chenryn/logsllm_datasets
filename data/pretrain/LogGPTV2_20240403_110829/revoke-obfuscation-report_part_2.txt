The crucial insight is that obfuscated code looks nothing like regular code. Nobody looking at code like
this would consider it normal:
So rather than (or in addition to) detecting known signatures, we must enrich our detection capabilities
by post-­‐processing Script Block and command-­‐line logs to look for signs of obfuscation. If we find
obfuscated PowerShell, then we can kick off a secondary investigation to determine its purpose and
intent.
Character Frequency Analysis
One of the first pieces of insight we can take action on is based on character frequency. For example,
the canonical obfuscation built into the Metasploit Framework uses random characters for all variable
and function names.
If we analyze the entropy and letter frequency distribution of the variable names as done in the blog
post, Detecting Obfuscated PowerShell, we can see that we can get some pretty strong signals from this
approach.
The MSF-­‐based stager had the most randomness in its variable names, and only 24% of the script came
from the top four letters. For the “normal” scripts in that small-­‐scale experiment, the top four letters
accounted for 35% or more of the script.
Cosine Similarity
We can take this character frequency approach even further. This approach is explained in the blog post,
More Detecting Obfuscated PowerShell. Rather than analyze the frequency of the top four letters, we
can analyze the frequency of each letter in a script. For example, here is an example of the average
character frequency as computed from all of PoshCode.org, a popular script sharing platform:
When we compare that to the character frequency of some obfuscated samples, there is clearly a
significant difference:
One approach used frequently in the information retrieval community to rank and compare lists of
numbers is called Cosine or Vector similarity. Rather than lists of character frequencies, the information
retrieval community instead often creates lists – called Feature Vectors – based on other items of
interest. For example, the number of occurrences of certain words, paragraph lengths, number of
internal links, number of external links and more.
Cosine similarity builds on the same math that we use to measure the angle between two lines in
geometry. As described on Wikipedia’s Cosine Similarity article:
The information retrieval community extends this concept past two or three numbers (representing the
two or three coordinate representation of a typical line) to hundreds or even thousands of elements.
We can apply exactly this technique on a feature vector represented by character frequencies.
When we mix in two of our obfuscation samples with a random selection of 20 other scripts from
PoshCode, the lack of similarity with the average character distribution really becomes obvious.
When we graph the similarity all of the scripts in PoshCode against its average character frequency, we
can see a clear clustering that we should be able to use to trigger further investigation.
In ad-­‐hoc experimentation, a similarity score lower than 0.8 can provide a good starting score for further
investigation. However, this sample size was limited – and a rigorous evaluation of this approach was
not possible without further data.
Building the PowerShell Corpus
Two major areas of weakness with the initial Cosine Similarity investigation were:
1) Lack of variety. Scripts shared in PoshCode have some degree of variety, but don’t represent the
full breadth of author experiences and scenarios in the PowerShell ecosystem. PoshCode
actively encourages copying, modifying, and re-­‐sharing existing scripts, so there were many
examples that were redundant with respect to other scripts.
2) Lack of labeled data. Without an exhaustive labeling of which scripts were in fact obfuscated vs.
which were not, the accuracy of Cosine Similarity cannot be measured. Despite having a
reasonable number of false positives when all of the items below a similarity value of 0.8 were
reviewed, it was not possible to measure how many obfuscated scripts were missed by having a
similarity score greater than 0.8.
To build a much more robust and representative PowerShell Corpus, we downloaded approximately
408,000 PowerShell scripts and modules:
-­‐ All GitHub projects detected as having the ‘PowerShell’ language (383.7k files)
-­‐ All PowerShell Gallery modules (17.5k files)
-­‐ All PoshCode scripts (3.4k files)
-­‐ A large selection of GitHub gists detected as having the ‘PowerShell’ language (1.7k files)
-­‐ All Technet scripts (1.5k files)
To create a set of labeled data, we:
-­‐ Manually reviewed approximately 7,000 scripts from this corpus
-­‐ Incorporated 383 submissions from the Underhanded PowerShell Contest
-­‐ Generated 3,200 scripts by having Invoke-­‐Obfuscation apply random amounts of obfuscation to
scripts from the existing corpus.
-­‐ Generated 788 scripts by having ISE Steroids’ obfuscation tool apply random amounts of
obfuscation to scripts from the existing corpus.
-­‐ Generated 200 scripts by having Invoke-­‐CradleCrafter apply random amounts of obfuscation to
scripts from the existing corpus.
This manual and automatic labeling provided a training set of 5674 “known clean” files, as well as 5590
“known obfuscated” files. With this labeled data, we were then able to evaluate the empirical ability of
the Cosine Similarity approach. Wikipedia’s Precision and Recall article is a great starting point for
further discussion of these measurements.
Cosine Similarity produced great precision (89% of the items it considered obfuscated were in fact
obfuscated), but suffered from poor recall (at scale, it only detected 37% of what was obfuscated). The
F score, which incorporates both of these metrics, demonstrates the relatively poor overall
1
performance.
In addition to the issue with false negatives, a significant vulnerability in the Cosine Similarity approach
is its susceptibility to character frequency tampering. No matter the inherent character frequency of an
obfuscated PowerShell script, it would be trivial to pad that script with comments, strings, or variable
names that restored the character frequency of the script back to the industry average.
Leveraging the PowerShell Tokenizer and AST
Using character frequency as a feature vector on PowerShell scripts is a good start. However, the
PowerShell engine includes two extremely powerful features to give tool authors deeper insight into the
structure of PowerShell scripts: the PowerShell Tokenizer, and the PowerShell Abstract Syntax Tree
(AST). These features are commonly used to enable syntax highlighting support for PowerShell editors
(such as the PowerShell ISE and Visual Studio Code), as well as advanced code analysis features such as
the detection of unused variables.
The System.Management.Automation.Language.Parser class provides access to both the tokenization of
a PowerShell script and the tree-­‐like representation of the script.
Tokenization provides access to PowerShell’s initial basic extraction of comments, variable names,
command names, operators, and more:
PowerShell’s parser additionally creates a tree-­‐like representation of the script, called the Abstract
Syntax Tree. This representation provides access to rich structural data about the script, such as the
nesting of commands within script blocks, variables used in parameter arguments, and more:
Logistic Regression with Gradient Descent
With this advanced access to the structure of any given PowerShell script, we can begin to extract
features much more descriptive of a script’s composition than its character frequency alone. As part of
this investigation, we wrote feature extractors to calculate and summarize 4098 unique script
characteristics, including:
- Distribution of AST types
- Distribution of language operators
- Assignment, binary, invocation, …
- Array size ranges
- Statistics within each AST type
- Character frequency, entropy, length (max, min, median, mean, mode, range),
whitespace density, character casing, …
- Statistics of command names, .NET methods, variables…
Rather than apply Cosine Similarity to this feature vector, we instead took a classification approach. This
classification approach directly identifies the likelihood that a script is obfuscated, rather than use
another metric (like similarity being greater than a certain number) to determine that fact.
A common approach to classification of feature vectors is to apply a linear regression. Most statistical
and mathematical packages offer built-­‐in functionality to create a linear regression. Excel is one popular
choice. A linear regression is based on the simple concept that you take each feature, multiply it by a
weight, and then add all of those results together. If the result is over a certain amount, then the sample
is considered part of the target classification (i.e.: “obfuscated”), while otherwise it is not.
In pseudo-­‐code, this looks similar to:
More important features will end up having higher weights than less important features. To keep the
result within a reasonable range of values and also to enable some slightly non-­‐linear distributions, it is
common to apply the Logit function to this calculation. Together, this forms the basis of a Logistic
Regression.
The final step in creating the PowerShell Obfuscation classifier is to decide on the appropriate weighting
for each of the 4098 unique script features.
One approach is to do it manually, but that’s error-­‐prone and time consuming. Another approach is to
employ a Gradient Descent algorithm, which is the approach we took.
The fundamental concept behind the Gradient Descent algorithm is similar to the back-­‐propagation
approach used in neural networks. For any given script that you have labeled data for, you can calculate
the error between the actual result and the expected result. Based on this error, you can adjust all of the
individual feature weights based on how much they contributed to the error. If the weight for a specific
feature was large, it played a large role in the calculation of the actual result. Therefore, the weight will
be adjusted downward more aggressively than feature weights that played a relatively more minor role.
When you apply this error correction approach over many thousands of iterations over the labeled
training data set, the Gradient Descent algorithm will generate a set of weights for the feature vector
that minimizes error as much as possible.
When doing this training, it is critical to separate the set of data used for training from the set of data
used to calculate the actual results. Without this partitioning, a given weight vector can become over-­‐
trained to the point that it is extremely accurate on the training data, but loses a great deal of fidelity on
unseen data. We followed common best practice and used a randomized 80% of the data for training,
and 20% of the data for validation.
Results
By leveraging the incredible structure exposed through PowerShell’s parser APIs and AST, the
PowerShell Obfuscation classifier proved extremely robust against in-­‐the-­‐wild obfuscated PowerShell.
We categorized in-­‐the-­‐wild obfuscated PowerShell as scripts output from Invoke-­‐Obfuscation, Invoke-­‐
CradleCrafter, ISE Steroids, and the Underhanded PowerShell contest:
This classifier has an Accuracy and F1 score of 96% and 95%, respectively. It is 10 times better at finding
obfuscated content (illustrated by the decrease in False Negatives) than character frequency analysis
alone, and has half of the false positives.
Deep Content Scanning
When performing an incident response, it is sometimes helpful to cast a wider net: to investigate more
deeply than you would have otherwise because the likelihood for malicious content is high. For
example, this script isn’t malicious or obfuscated, but clearly relies on uncommon scripting techniques:
In the heat of an incident response, you’d probably rather double-­‐check scripts like that, so we labeled
all scripts that contained unusual or content like this as part of our labeling effort. We included scripts
with regions of complicated regular expressions, odd concentrations of script blocks, operators, and
more. Given the variety of scripts in deep content scanning, the classifier still performs admirably at this
much more complicated task:
It returns nearly 90% of the scripts that somebody might consider “unusual”, with a still very reasonable
5% false positive rate. This returned set of results includes 97% of “in the wild” obfuscated PowerShell.
Other Algorithms
The linear regression and gradient descent algorithm was written in C#, heavily leveraging the prior
work done by Dr. James McCaffrey in his MSDN column, Test Run – Gradient Descent Training Using C#.
However, Azure Machine learning makes it incredibly easy to compare this algorithm to several others,
including Perceptron, Boosted Decision Tree, and Support Vector Machine techniques.
For both the “In the Wild” and “Deep” data sets, this implementation gets nearly identical results to the
Azure Machine Learning implementation of Logistic Regression. Boosted Decision Trees produce similar
results, while the Perceptron and Support Vector Machine approaches performed much more poorly on
this data set.
Revoke-­‐Obfuscation
A production-­‐ready implementation of this classifier is now available in the GitHub project, Revoke-­‐
Obfuscation.
Revoke-­‐Obfuscation has been used in numerous Mandiant investigations. It supports analysis against
files, PowerShell event logs, URLs, and even raw content streams. It also supports rich whitelisting
behaviour to reduce the false positive rate on known scripts.
Authors
Daniel Bohannon
Daniel Bohannon (@DanielHBohannon) is a Senior Applied Security Researcher at Mandiant. He is the
author of Invoke-­‐Obfuscation and Invoke-­‐CradleCrafter. He specializes in Obfuscation, evasion, and
detection techniques.
Lee Holmes
Lee Holmes (@Lee_Holmes) is the lead security architect of Azure Management at Microsoft. He is the
author of the Windows PowerShell Cookbook, and an original member of the PowerShell development
team.