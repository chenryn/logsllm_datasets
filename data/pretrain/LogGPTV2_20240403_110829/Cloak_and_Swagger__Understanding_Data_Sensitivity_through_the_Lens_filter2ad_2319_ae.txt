missed if we relied only on the conventional assumptions.
Not all the words identiﬁed as characteristic of anonymous
answers, and therefore potentially sensitive, carry a negative
connotation. There are several positive words, such as: laughing,
gorgeous, righteous, faithful, forgiveness, and several neutral
words, such as acquiring, feel, admissions, committee, bidding.
503
This suggests that purely sentiment analysis-based methods [47]
that rely on the sentiment of the item being shared would not
be successful at predicting the item’s sensitivity.
Finally, besides serving as an additional conﬁrmation of the
hypothesis that sensitivity is nuanced, for which we found
evidence via the analysis of topics and contexts in Section IV,
the ability to build a vocabulary of potentially sensitive words
is valuable in its own right. For example, in scenarios when
users are sharing posts for which an accurate topic inference
is not feasible (e.g., due to the short length of a post or lack
of time or resources for manual labeling of its topic), having
a vocabulary of potentially sensitive words for that application
can power a cheap and easy-to-implement “Are you sure?”-type
feature with high potential gain for user privacy.
VI. TOWARDS AUTOMATED SENSITIVITY PREDICTION
In this section, we explore the possibility of training a
machine learning classiﬁer capable of warning users when
they are about to follow a potentially sensitive question or to
share or disclose something sensitive.
A. Question Sensitivity Prediction
To evaluate the possibility of predicting a question’s potential
sensitivity, we consider questions from contexts identiﬁed in
Section IV-A2 whose answer anonymity ratio, A(C), exceeds
the average by 2 standard deviations. We further limit the set
of questions to those 15,466 that have at least 6 answers, since
our goal is to predict a question’s sensitivity, and an accurate
computation of the anonymity rate among the question’s
answers is unlikely for questions with few answers. We label
a question as sensitive if the fraction of anonymous answers
to its total answers is at least 0.32, i.e., 2 standard deviations
above the average. The label was chosen in such a way as to
roughly correspond to a 95% conﬁdence interval [43].
Following the common machine learning practice, we
randomly partition the data into two datasets: one for training
and one for evaluation. The evaluation dataset consists of
1,000 questions in order to allow for a 0.1% precision in the
evaluation. The training dataset contains the remaining 14,466
questions. We note that given our question sensitivity labeling,
21.5% of the questions in the evaluation dataset are considered
sensitive, which establishes the baseline at 78.5%10.
We experiment with soft-margin classiﬁers, linear and SVM
classiﬁers, as they have been shown to be the most effective
on NLP tasks that involve short text, such as Twitter sentiment
analysis [47]. We use exhaustive search to evaluate the best
method to convert the words in the dataset into features (e.g.,
with or without stop word removal, with or without stemming,
using unigrams or bigrams, etc.), converging on no stop word
removal, no stemming, and use of bigrams as the transformation
that yields the best accuracy when used in conjunction with a
linear classiﬁer.11 We experimented with four distinct types of
10An algorithm that always predicts that the question is not sensitive will
achieve a 78.5% accuracy.
11We did not perform an analogous exhaustive search for the SVM classiﬁer
due to its prohibitive computational cost.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:00:12 UTC from IEEE Xplore.  Restrictions apply. 
the bigram feature representations, namely: binary, occurrence
count, term frequency, and TF-IDF, and concluded that the
frequency representation works best. For the linear classiﬁer,
we tested various regularization modes, including L1 and L2.
For the SVM classiﬁer with an RBF kernel we performed a
grid search to determine the optimal gamma and cost.
Table V presents the outcome of attempts to predict a
question’s sensitivity when each of the trained models is tested
on the evaluation set. Overall, the best accuracy achieved is
80.4%, which represents a slight improvement relative to the
baseline of 78.5%. Even with a small training sample and
noise due to “Search Engine Privacy”, our machine learning
predictions of question sensitivity outperform the baseline.
However, our results also suggest that relying purely on the
content may not be sufﬁcient and more information needs
to be factored when evaluating the potential sensitivity of
sharing something. We discuss several candidates for additional
information, such as a person-speciﬁc sensitivity measure
and the nuance of sensitivity depending on a person in
Sections VII-A and VII-B.
Algorithm
Linear classiﬁer
SVM linear kernel
SVM RBF kernel
Parameters
–
c=0.0029
c=850 g=0.01
Accuracy
80.4%
79.9%
80.2%
PERFORMANCE OF ALGORITHMS PREDICTING QUESTION SENSITIVITY
B. Answer Sensitivity Prediction
TABLE V
We run a set of experiments similar to the ones described
in the previous section in order to assess whether it is possible
to predict the sensitivity of an answer from its context and
content. We limit our consideration to answers that contain at
least 80 characters, which signiﬁcantly decreases the number
of answers, and experiment with two datasets. The ﬁrst one,
S, contains 3,660 answers to the questions that were labeled
as sensitive in the question sensitivity experiment above. The
second one, A, contains 151,825 answers to questions from
the 1,525 contexts analyzed in Section IV-A1. As above, we
randomly partition our data into a training and evaluation sets,
with 1,000 answers in the evaluation datasets to allow for a
0.1% precision in the evaluation.
Algorithm
Linear classiﬁer
SVM linear kernel
SVM RBF kernel
Parameters
L1
c=385
c=2 g= 0.00195
Accuracy
62.3%
63.1%
61.7%
PERFORMANCE OF ALGORITHMS PREDICTING ANSWER SENSITIVITY, S
TABLE VI
CORPUS
Class
Anonymous
Non-anonymous
Precision
0.63
0.61
TABLE VII
Recall
0.22
0.90
PRECISION AND RECALL FOR THE VARIOUS CLASSES, S CORPUS
In the evaluation subset of S, the fraction of anonymous
answers is 42.2%, setting a 57.8% baseline (using an algorithm
that always predicts an answer will be non-anonymous).
504
Table VI reports the performance accuracy of our answer
anonymity predictor for the evaluation part of S, with the
best algorithm12 achieving an accuracy of 63.1%, which is
5.3% above the baseline. When evaluating precision and recall,
reported in Table VII, the following conclusions emerge: ﬁrst,
predictions of anonymous and non-anonymous class have
roughly the same precision which suggests that content provides
information in both directions. Second, the weakest part of the
prediction is the recall for the anonymous class: barely 2 out of
10 anonymous answers are correctly classiﬁed by the algorithm.
This indicates that the biggest area of potential improvement
lies in ﬁnding additional features to improve anonymous recall.
In the evaluation subset of A, the fraction of anonymous
answers is 16.5%, setting a 83.5% baseline. Table VIII
reports the performance accuracy of our answer anonymity
predictor using the Linear classiﬁer13, with the algorithm
achieving 88.0% accuracy, which is 4.5% above the baseline.
As was the case in question sensitivity prediction, our
answer sensitivity prediction results are able to beat
the
baseline performance even when given a small
training
set and in the presence of noise due to “Search Engine
Privacy”. The results highlight another important direction
for improving classiﬁcation quality: the need for additional
training data, as the hypothesis that
the quality of the
prediction will
improve with increase in the amount of
data available for training is supported by the observation
that our performance is better on the larger corpus, A, than on S.
Algorithm
Linear classiﬁer
Parameters
L1
Accuracy
88.0%
PERFORMANCE OF ALGORITHM PREDICTING ANSWER SENSITIVITY, A
TABLE VIII
CORPUS
Overall the experiments related to sensitivity prediction
support our hypothesis that it is possible to use a data-driven
approach of learning based on users’ use of privacy-enhancing
features, in order to provide better privacy protections for
them. On the other hand, the accuracies of our classiﬁers also
strongly suggest that predicting what is sensitive is a complex
and nuanced problem that could beneﬁt from additional features
and better training data.
VII. DISCUSSION
A. Limitations of the Study due to Dataset Choice
The dataset we collected and used for our study of content
sensitivity has several limitations, with implications for ability
to generalize the conclusions made on its basis to other
populations and other services and for the kind of statistical
analyses and machine learning models that are feasible to
perform on it.
Firstly, although Quora has a real name policy and many
users answer questions on Quora in order to build their
12Removing stop words, performing stemming, using unigrams and repre-
13SVM kernel models were not built due to their prohibitive computational
senting words as binary features.
costs on such a large corpus.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:00:12 UTC from IEEE Xplore.  Restrictions apply. 
reputations as an expert in certain topics, and therefore, have
an incentive to use a real name, some may be creating
accounts using names other than their real one. Answering
using an account with a fake name is analogous to answering
anonymously from the perspective of risks we consider; hence,
although anecdotal evidence suggests most users use their real
name14, our ﬁndings are limited by the extent to which Quora
succeeds in enforcing the real names policy.
Secondly, although the Quora user base is fairly large and
diverse15, it may not be representative of users of other Internet
services. Furthermore, the true privacy paranoids are unlikely
to post on Quora or on any other online service. Therefore, our
inferences can be effectively applied to improve the privacy
of Quora’s products, and can serve as a starting point for
discussion on sensitivity, but would need additional service-
speciﬁc research in order to be properly generalized to other
services and other populations of users.
Thirdly, as discussed in Section III-C,
the reasons for
exercising anonymity choices on Quora may vary, and are
not limited to data sensitivity. However, both previous work
on user regrets about posting online [20] and Quora users’
self-report on usage of anonymity [40] suggest that content
sensitivity may be one of the signiﬁcant motivating factors.
Therefore, although one certainly cannot equate anonymity
with sensitivity, we believe that anonymity is a strong indicator
of potential sensitivity and our ﬁndings could serve as a starting
point for further research on the topic.
Fourthly, unlike Quora itself, we do not have a user-level
view of each user’s anonymous and non-anonymous answers.
Our inability to include user-speciﬁc features, such as gender
or tendency for anonymous answering, likely signiﬁcantly
hampers the quality of the anonymity predictors we can build16.
In practice, Quora has access to such information and would
not be subject to the same limitations were it to attempt to
learn privacy preferences based on its data or build features
that could help prevent regret. Furthermore, lack of a user-level
view prevents us from studying the potential differences in
preferences due to gender, age, location, etc.
Finally, our dataset quality is limited by the quality and
reach of the crawler we used. We cannot be sure that we
collected a complete snapshot of Quora, that our parsing of
the question page was perfect17, or that the access we have
to the followers of a question through the follower grid is
representative of all its followers. Another limitation due to
the crawler used relates to the Search Engine Privacy feature
of Quora. The inferences we make are based on both truly
14Many users link to their Facebook and Twitter accounts in their Quora
proﬁles.
15A recent press interview suggests that Quora has been experiencing a
healthy user growth in 2012-2013 [48]. Statistics provided by web trafﬁc
analytics companies Alexa [49], Compete [50], and trafﬁcestimate [51],
estimate that Quora has ∼1 million unique monthly visitors, and ∼30 million
total monthly visits. The user base is dominated by visitors from India and
the United States, who together account for more than 60% of the total trafﬁc.
16In a related scenario of analyzing online content, [52] ﬁnds that author
features have a strong discriminative power.
17We observed several answers that were blurred out with images or only
partially collected by the crawl, which we omitted.
anonymously written answers and those made anonymous due
to the search engine privacy setting of the writing user. As
described in Section IV-B2, we mitigate this limitation by
choosing analyses whose inferences are minimally affected
by such noise. We limit our word analyses and some of our
machine learning analyses to data from contexts which exhibit
elevated level of anonymity – ﬁrmly placing them above noise
that may be due to search engine privacy.
In spite of these limitations, we are able to make informative
inferences and develop sensitivity predictors which outperform
the baseline prediction rates. This suggests that in practice, the
service providers who are not constrained by the limitations
we face, should be able to both better understand their users’
privacy preferences and build predictors that enable them to
improve users’ privacy related experiences through introduction
of appropriate nudges or defaults.
B. Content Sensitivity is Subjective
As pointed out by privacy experts in [6], determining
content sensitivity is a complex problem. Content sensitivity
depends not only on the content but also on the context,
i.e., who is sharing the information and when, where and
with whom they are sharing it, along with what they are
sharing. Individuals may have widely differing anonymity
and sensitivity preferences, depending on their personalities,
cultural or religious backgrounds, experiences, etc. Consider
the following examples of Quora questions and answers that
illustrate that individual people may be making choices that
differ from those that would be expected from most users:
• The question, “Selﬁshness: What is the most selﬁsh thing
you have ever done?”18, has 12 (10 without search engine
privacy) anonymous answers out of 18 total answers.
However, one user gave the following very personal answer
non-anonymously, and even provided a link to her Facebook
account19: “Thought that my husband and 2 young children
could wait a year while I enjoyed, for the ﬁrst time in my
life, my job. At the end of the year, my marriage was in a
shambles, and my eldest daughter was dead.”
• The question, “Why do homeless people wear so much
clothing?”20, has 6 (2 without search engine privacy)
anonymous answers out of 9 total answers. The following
answer was provided anonymously, though there isn’t
anything obviously sensitive in it – “I always assumed
the reason they usually wear clothing in layers is because
they have no storage facility to stash them. They always
say: dress in layers in SF. Seriously, it can be 30 degrees
in the morning (or colder) and 70 in the afternoon. In
addition the extra layers are versatile, they can double as
blankets and pillows. Also, many homeless people have
issues with hoarding. Obviously you can’t be a hoarder if
18https://www.quora.com/Selﬁshness/What-is-the-most-selﬁsh-thing-you-
19We believe this user is not using her real name.
20https://www.quora.com/Homelessness/Why-do-homeless-people-wear-
have-ever-done
so-much-clothing
505
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:00:12 UTC from IEEE Xplore.  Restrictions apply. 
you are homeless but frequently they “collect” stuff and
hang on to it.”
• A question related to murders, “What does it feel like
to murder someone?”21, may be expected to have many
anonymous answers. However, only 1 out of the 9 answers
for it is anonymous.
• The question trying to understand reasons for anonymous
answers, “What drives people to contribute anonymous
answers on Quora?”22, also contains many anonymous
answers – 21 (17 without search engine privacy) of the 27.
C. Correlation with a User Survey
We initiate a study that aims to compare our behavioral
data-driven ﬁndings with survey-based ones, via a short user
survey using Google Consumer Surveys [12], a new public
tool that enables anyone to quickly and cheaply run surveys
online. To provide an (imperfect) parallel with our study of
sensitivity based on Quora anonymity choices, we posed the
questions:
1) Of the following topics, which ones would you be
comfortable writing about online using your real name?
2) Of the following topics, which ones would you be
comfortable writing about online anonymously?
The topics included in the choices were: Prostitution, Recre-
ational Drugs, Depression, Friendship, Government Leaders
and Politicians, Religion and Beliefs (high anonymity ratio
according to analysis in Section IV), and Mobile Phone and
Superhero Films (low anonymity ratio). Selection of more than
one answer was permitted, along with the option ”None of the
above”. The topic presentation order was randomized.
Figure 7 presents the results based on 1,500 responses
received for each question, with respondents chosen to be
representative of the US Internet population (via the quota
method provided by [12]). The results highlight the difﬁculty of
eliciting user privacy preferences and sensitivities, as although
participation in online sharing platforms such as Twitter and
Tumblr is skyrocketing,
the vast majority of respondents
indicated they would not be comfortable writing online even
about the seemingly innocuous topic of Mobile Phones. On the
other hand, they give support to the validity and promise of our
proposed approach: ﬁrstly, for most topics, the respondents’
indicated comfort level is higher when assuming they’d be
answering anonymously rather than with their real name,
supporting our hypothesis that anonymity choices may be
indicative of sensitivity. Secondly, the ranking of topics by
percentage of respondents who’d be comfortable writing about
it is different, but not radically so, from the one derived in
Section IV. This suggests that behavioral-data driven analyses