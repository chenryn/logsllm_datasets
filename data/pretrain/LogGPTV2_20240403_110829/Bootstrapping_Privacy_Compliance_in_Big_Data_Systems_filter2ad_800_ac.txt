the IP address, which from the lattice in Fig. 3c includes a
team in the ads organization, one in the search organization,
and some individuals in the research organization. The policy
then explicitly denies access to interns. Note that the layered
speciﬁcation (explicitly) breaks ties — both the allow and the
nested deny clause apply to data dependency nodes labeled
with AccessByRole Dave, but the more deeply nested DENY
clause takes precedence. We show in the formal semantics
section how all LEGALEASE policies can be interpreted un-
ambiguously.
(a) Policy datatype lattice
(b) Typestate lat-
tice
(c) DataType lattice (product of 4a and 4b); only
part of lattice shown.
Fig. 4. Concept lattice construction for the DataType attribute.
4) DataType Attribute: Lastly, we deﬁne the DataType at-
tribute and concept lattice. The interesting aspect is the notion
of increasing or decreasing the sensitiveness of a datatype
(e.g., encryption decreases sensitiveness, opting-out increases).
Since policy may disallow use of data at one sensitiveness level
and allow use at another, there is a need to track a limited
history of the policy datatype. We track history with a notion
we call typestate (deﬁned below).
a) Policy datatypes: The policy datatypes are organized
in a lattice (Fig. 4a). For example, IP address is both a
unique identiﬁer as well as a form of location information. The
ordering relationship (≤T ) deﬁnes t ≤T t′ if t “is of type” t′.
e.g., IPAddress ≤T UniqueID and IPAddress ≤T Location.
b) Limited typestate: The typestate is a limited way of
tracking history. Consider the typestate :OptOut, which we
use to refer to data from users that have opted-out of certain
products, or :Expired that tracks data past its retention time and
scheduled for deletion (highly sensitive). The GROK mapper
determines the typestate of nodes in the data dependency graph
as deﬁned in Section V. Fig. 4b lists some other typestates that
we use in our deployment. These typestates are organized in
a lattice ordered by the “is less sensitive than” relation (≤S);
332
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:58:21 UTC from IEEE Xplore.  Restrictions apply. 
the sensitiveness levels are decided by privacy champions.
c) Combining policy datatypes and typestates: The con-
cept lattice (D) for the DataType attribute is deﬁned over tuples
of the form t:s where t is picked from the lattice of policy
datatypes, and s is picked from the lattice of typestates. The
ordering relationship (≤) for set element t:s ∈ D is deﬁned
as t:s ≤ t′:s′ iff t ≤T t′ ∧ s ≤S s′. Intuitively it is the lattice
formed by ﬂattening the result of replacing each element of
one lattice with a copy of the other lattice. Fig. 4c shows a part
of the DataType lattice formed by replacing the :, :Encrypted,
and :Truncated elements from the typestate lattice with the
policy datatype lattice; black lines encode the ≤T ordering
relationship, and grey lines encode ≤S.
E. Formal Semantics
Our third goal
is to enable compositional reasoning of
policies. We now present the formal semantics of LEGALEASE
that satisﬁes this goal.
The semantics uses vectors of attributes. We use the notation
T (with suitable superscripts) to denote a vector of sets of
lattice elements (representing the label of a node in the data
dependency graph or a clause during policy evaluation), and
the notation Tx to denote the value of attribute x in T , which
is a set of lattice elements. Recall that LEGALEASE policies
are checked at each graph node. Each graph node G is labeled
with a vector T G. Similarly, policy clauses contain a vector
T C .
In order to deﬁne how policies are checked, we deﬁne the
partial order ⊑ over vectors of sets of lattice elements, as
pointwise ordering over all the attributes in the vector. More
precisely, we deﬁne T ⊑ T ′ iff ∀x.Tx ⊑x T ′
x, where ⊑x is
.v ≤x v′ and ≤x is the partial order
deﬁned as ∀v∈Tx ∃v′∈T ′
associated with the attribute x. Using DeMorgan’s law, we
have that T 6⊑ T ′ iff ∃x.Tx 6⊑x T ′
x. Intuitively, a policy clause
ALLOW T C applies to a graph node labeled with a vector T G
if T G ⊑ T C .
x
v ∧x v′|v′ ∈ T ′
We also deﬁne T ⊓T ′ pointwise, where for each x, Tx ⊓x T ′
x
x}. We use the notation ⊥ ∈ T as
is {Wv∈Tx
shorthand for ∃x.⊥ ∈ Tx. A policy clause DENY T C applies
to a graph node labeled with a vector T G if ⊥ 6∈ T G ⊓ T C ,
which intuitively means that the overlap between the denied
elements T G and the node labels T C is not empty.
Missing attributes: If for some attribute x the set of lattice
elements is not speciﬁed, it is taken to be a singleton set with
top (⊤), as missing attributes imply all.
Finally, the judgment C allows T G, read as policy clause
C allows data dependency graph node labeled with attribute
set T G, characterizes which nodes are allowed. Similarly, the
judgment C denies T G characterizes which graph nodes
are denied. These two judgments, deﬁned formally in Table
III, provide a recursive procedure to check whether a data
dependency graph node satisﬁes a policy, given its attributes.
Intuitively, a graph node is allowed by an ALLOW clause if
and only if the clause applies and is allowed by each exception
(rules A1-A3 in Table III). A graph node is denied by a DENY
T G 6⊑ T C
ALLOW T C EXCEPT D1 · · · Dm denies T G
T G ⊑ T C ∃iDi denies T G
ALLOW T C EXCEPT D1 · · · Dm denies T G
T G ⊑ T C ∀iDi allows T G
ALLOW T C EXCEPT D1 · · · Dm allows T G
⊥ ∈ T G ⊓ T C
DENY T C EXCEPT A1 · · · Am allows T G
⊥ 6∈ T G ⊓ T C ∃iAi allows T G ⊓ T C
DENY T C EXCEPT A1 · · · Am allows T G
⊥ 6∈ T G ⊓ T C ∀iAi denies T G ⊓ T C
DENY T C EXCEPT A1 · · · Am denies T G
TABLE III
INFERENCE RULES FOR LEGALEASE
(A1 )
(A2 )
(A3 )
(D1 )
(D2 )
(D3 )
clause if and only if the clause applies and is denied by each
exception (rules D1-D3).
As an example, consider the following policy clause that al-
lows everything except for the use of IPAddress and AccountID
in the same program. Note that individually, using either may
be allowed.
ALLOW DataType ⊤
EXCEPT
DENY DataType IPAddress, AccountID
For ease of exposition, we demonstrate policy evaluation
only for the DataType attribute here and elide DataType
subscripts. The DataType attribute of top level clause is {⊤}
and as for any T G, T G ⊑ {⊤}, one of rules A2 or A3
applies and we need to check the exception. The exception
has DataType attribute T C = {IPAddress, AccountID}. Now
consider two nodes with its DataType attributes being T G
1 =
{IPAddress} and T G
2 = {IPAddress, AccountID}. In the ﬁrst
case T C ⊓ T G = {IPAddress, ⊥}, and therefore, the node is
allowed by the policy. In the second case, on the other hand,
T C ⊓ T G = {IPAddress, AccountID}, and therefore, the node
is denied by the policy.
F. LEGALEASE Properties
We now use the formal deﬁnition of LEGALEASE to state
some of its properties. Appendix A of [20] contains a more
detailed discussion and proofs.
The ﬁrst two are safety properties that ensure that checking
is deﬁned uniquely for each policy and graph node.
Proposition 1 (Totality): For each vector of sets of lattice
elements T , and policy clause C, C allows T or C denies T .
Proposition 2 (Unicity): For each vector of sets of lattice
elements T , and policy clause C, C allows T and C
denies T , are not both true.
We then show that LEGALEASE respects a syntactic notion
of weakening. Our notion of weakening captures the standard
333
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:58:21 UTC from IEEE Xplore.  Restrictions apply. 
T C
ALLOW T C
2
1 ⊑ T C
1 (cid:22) ALLOW T C
2
C1 (cid:22) C2 C2 (cid:22) C3
C1 (cid:22) C3
ALLOW T C EXCEPT D1 · · · Dn (cid:22) ALLOW T C EXCEPT D1 · · · D′
n
Dn (cid:22) D′
n
SELECTED RULES FOR WEAKENING LEGALEASE POLICIES
TABLE IV
modes of editing a policy in LEGALEASE i.e., relaxing a
clause, weakening an exception or removing an exception.
Table IV contains selected rules for deﬁning the weakening
relation (cid:22). The intuitive idea that weakening a policy should
make it more permissive, is stated as follows:
Proposition 3 (Monotonicity): If C1 (cid:22) C2, then for any
T G, C1 allows T G implies that C2 allows T G and C2
denies T G implies C1 denies T G.
Fig. 5. A coarse-grained GROK data ﬂow graph over users, ﬁles, and jobs.
GROK conﬁdence values to rank violations so that we can
direct auditor attention to violations it is more certain about.
IV. DATA INVENTORY
B. GROK System
Inference rules of LEGALEASE assume a data dependency
graph labeled with the domain-speciﬁc attributes. Constructing
such a graph in reality is a difﬁcult process. In this section we
present GROK which constructs and labels a data dependency
graph over big data pipelines with data ﬂow, storage, access,
and purpose labels with minimal manual effort.
A. Design Goals
Our primary goal in designing the GROK mapper is to scale
to the needs of a large big data system, i.e., it must scale to
millions of lines of ever-changing source code, storing data in
tens of millions of data ﬁles.
a) Exhaustive and up-to-date: We target Map-Reduce-
like big data systems that store data and run processing jobs.
Such systems have complete visibility into all data, accesses,
and processing. Any data entering (or leaving) the system
can do so only though a narrow set of upload (or download)
APIs that require users to authenticate themselves to the
system. Similarly, all jobs run on the system are submitted
by authenticated users.
b) Bootstrapping: At the scale in which we are inter-
ested, bootstrapping a GROK is highly non-trivial. We cannot
assume extra effort on the part of the developer (e.g., labeling
schema-elements with policy datatypes). There are thousands
of developers and any change affecting more than a few tens
requires creating a new organizational process (awareness,
trainings, code reviews, process audits, and so on). Thus,
we are constrained in using purely automated approaches or
approaches involving a small team to bootstrap a labeled data
ﬂow graph.
c) Veriﬁable and robust: As a result of bootstrapping
without any developer effort, it is inevitable that there will
be false-positives and false-negatives in our attribute labels.
At a minimum we must be able to verify the correctness of
inferred labels. At the same time, it is unreasonable to assume
that the team will be able to verify correctness of all labels.
We therefore expose an explicit conﬁdence-level associated
with any attribute label. Our privacy policy checker uses the
A GROK data ﬂow graph (Fig. 5) contains a node for every
principal (processes, data stores, entities) handling data in a
system, and a directed edge between principals when data
ﬂows from one to another. Nodes are labeled with the domain-
speciﬁc attributes mentioned in the previous section (callouts
in the ﬁgure). The graph is updated with new nodes and
edges as new principals and data ﬂows are encountered. GROK
associates a conﬁdence score with each attribute label (labeled
in the ﬁgure as high or low). Conﬁdence values are based on
how the attribute value was inferred.
Granularity: The ﬁner the granularity of GROK, the more
precision with which it can track data ﬂows, but the higher
the scalability cost of using that information. For example,
at a ﬁne granularity, there may be a process node for every
line of executable code in every job; a data store node for
every ﬁle; and an entity node for every human accessing the
system. At coarse granularity there may be one process node
for every job run on the cluster; one data store node for every
logical separation of data (e.g., sub-directory); and one entity
node for every functional team. Having one node per sub-
directory is more scalable, but it conﬂates ﬁle level attributes
that may not otherwise appear together on the same node thus
trading off precision. Ultimately, the required precision is a
function of the privacy policy — if the policy “We will not
store account information with advertising data” is interpreted
as not storing in the same sub-directory, then a coarse-grained
GROK is precise enough.
Our GROK prototype is at a ﬁner granularity than the exam-
ples above. There is a data node for each individual column in
each ﬁle that contains tabular data, a process node for every
ﬁeld in every sub-expression in a statement of code, and entity
nodes for each computer a user connects to the system with.
For scalable use of the graph, we dynamically materialize a
coarse-grained view by combining related columns (at the cost
of precision), but allow any algorithm access to the underlying
ﬁne-grained graph as needed. By scalably targeting the ﬁnest
granularity, we allow policy interpretations to change over time
without having to change GROK.
334
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:58:21 UTC from IEEE Xplore.  Restrictions apply. 
Clicks =
EXTRACT GUID, ClientIP
FROM “/adsdata/clicks/20131113” ;
UserAgents =
EXTRACT GUID, UserAgent
FROM “/adsdata/uadata/20131113” ;
Suspect =
SELECT Encrypt(ClientIP, “...”) AS EncryptedIP
FROM Clicks INNER JOIN UserAgents ON GUID
WHERE MaybeFraud(UserAgent);
OUTPUT Suspect TO “/users/alice/output” ;
Fig. 7. Coarse-grained labeled data ﬂow graph nodes for Fig. 6.
Fig. 6. Example of big data analysis code written in Scope [9].
D. Data Flow Edges and Labeling Nodes
C. Language Restrictions in Big Data Languages
Before we can describe how we bootstrap GROK, we explain
semantics common to the three languages commonly used in
industry for big data analysis — Hive, Dremel, and Scope.
All three languages have the same basic data abstraction of a
table. A table is a rectangle of data with a ﬁxed number of
columns, and an arbitrary number of rows. Each column has a
name and a type (e.g., int, string). Jobs are a sequence of SQL-
like expressions. Each expression of the language operates on
one or more tables, and returns a resultant table that may be
used in other expressions. Thus, the result of every expression
(or sub-expression) in the program is also a table with named