6 More Optimizations for ML Applications
In this section, we will discuss several optimizations for key
components in the machine learning (ML) applications and
how they are connected. Then, we describe how to support
various types of ML algorithms by extending TensorFlow [1].
6.1 Optimizing Matrix Multiplication
By generalizing the Freivalds algorithm [31], we propose a
ZK protocol to prove matrix multiplication with dimension
n× n over a ﬁeld Fq (for any prime q ≥ 2), which only needs
to prove n private multiplications rather than n3 using a naive
algorithm. Since the intuition of the protocol has been dis-
cussed in Section 3.3, we directly present the ZK protocol in
the FauthZK-hybrid model in Figure 10.
Protocol ΠMatMul
and C ∈ Fn×(cid:96)
Inputs: A prover P and a veriﬁer V have three authen-
ticated matrices [A], [B] and [C], where A ∈ Fn×m
,B ∈
Fm×(cid:96)
q
Protocol execution: P proves in zero-knowledge that
A· B = C holds by interacting with V as follows:
1. V samples uuu ← (Fqk )n,vvv ← (Fqk )(cid:96), and then sends
q
q
.
them to P .
2. P and V compute [xxx](cid:62) := uuu(cid:62) · [A] and [yyy] := [B]· vvv
locally (this can be also done by calling the (lincomb)
command of FauthZK). Both parties also compute
[z] := uuu(cid:62) · [C]· vvv.
3. The parties compute [z(cid:48)] := [xxx](cid:62) · [yyy] by calling func-
tionality FauthZK, where z(cid:48) = xxx(cid:62) · yyy.
4. Both parties execute the CheckZero procedure on
[z] − [z(cid:48)] to verify that z = z(cid:48). If the check fails, V
outputs false and aborts; otherwise, it outputs true.
Figure 10: Zero-knowledge protocol for proving matrix
multiplication in the FauthZK-hybrid model. Before run-
ning this protocol, P and V have computed the authenticated
values on all entries in the matrices to be proven by calling
the (input) command of FauthZK.
In the following theorem, we prove the security of this
protocol, where we refer the reader to [52] for the standard
ZK functionality. The detailed proof of this theorem can be
found in the full version [53].
Theorem 5. Protocol ΠMatMul shown in Figure 10 UC-
realizes the standard ZK functionality FZK in the presence of
a static, malicious adversary with soundness error 3/qk in
the FauthZK-hybrid model.
Further optimizations. We can further optimize the protocol
shown in Figure 10 by letting the veriﬁer send a random seed
to the prover and then the two parties compute uuu and vvv by
applying a random oracle to the seed.
In protocol ΠMatMul, the parties compute [z(cid:48)] = [xxx](cid:62) · [yyy]
by calling the (mult) command of FauthZK. This require
communication of O(mlogq) bits. To optimize the com-
munication cost, we can deﬁne a multivariate polynomial
f (xxx,yyy,z) = xxx(cid:62) · yyy − z, and then prove knowledge of xxx,yyy,z
such that f (xxx,yyy,z) = 0 using the latest ZK protocol [54].
This optimization can reduce the communication cost to only
O(k logq) bits, independent of m.
USENIX Association
30th USENIX Security Symposium    511
6.2 Support Fixed-Point and Floating-Point
There are many non-linear operations in typical ML algo-
rithms, including ReLU, Max Pooling, Sigmoid, SoftMax, etc.
These operations are complicated to compute, and may of-
ten cause some accuracy loss when values are represented
as ﬁxed-point numbers. In our implementation, we support
native IEEE-754 single-precision number in ZK proofs so
that we can obtain maximum accuracy.
Encoding signed, ﬁxed-point numbers. Linear layers and
non-linear layers appear alternately. It is crucial to encode
data in Fp for linear layers so that we can enjoy our highly
efﬁcient matrix-multiplication protocol described as above.
For non-linear layers, data is encoded as ﬂoating-point num-
bers. To eventually convert between ﬂoating-point numbers
and elements over Fp, we need to ﬁnd a way to encode signed,
ﬁxed-point numbers into Fp, and execute an encoding proce-
dure in another direction.
Given a prime p > 2, we can deﬁne an encoding proce-
dure from Z to Fp as Encode(x ∈ Z) = (x mod p), where
an integer lies in [−(p− 1)/2, (p− 1)/2]. Note that ﬁeld ele-
ments over Fp are represented in [0, p−1]. The corresponding
decoding procedure is described as follows:
x,
x− p,
x ≤ (p− 1)/2
x > (p− 1)/2
(cid:40)
Decode(x ∈ Fp) =
Now given a ﬁxed-point number x and a precision param-
eter s ∈ N, we can encode x into Fp by Encode((cid:98)2s · x(cid:99)). If
(cid:98)2s · x(cid:99) ∈ [−(p−1)/2, (p−1)/2], there is almost no accuracy
loss. Encoding in another direction from elements over Fp
to ﬁxed-point numbers can be executed in a straightforward
inverse process. There is one caveat: since we will perform
matrix multiplication after non-linear layers, it is important
to leave enough slack so that the precision does not overﬂow.
In our implementation, we use a Mersenne prime p = 261 − 1
and encode ﬁxed-point numbers into a 30-bit range (where
s = 16). Since in our application, the numbers never reach
close to the 30-bit range, this ensures that the matrix multipli-
cation would not overﬂow.
Converting between ﬂoating-point and ﬁxed-point num-
bers. With values encoded as ﬁxed-point numbers, we could
convert between these ﬁxed-point numbers and their bi-
nary representation via the arithmetic-Boolean conversion as
shown in Section 4 and the encoding procedure described as
above. Thus, the remaining task is to design efﬁcient Boolean
circuits for conversions between ﬁxed-point and ﬂoating-point
numbers. We use the single-precision circuits in EMP [51],
where the operations conform with the IEEE-754 standard.
To perform the conversion from a ﬂoating-point number to
a ﬁxed-point number, we follow the deﬁnition of IEEE-754.
The key components are private logical left shift and right
shift, each of which is implemented using a (nlogn)-sized
circuit when shifting n bits. This procedure takes about 580
Figure 11: Integration with TensorFlow. Static and dynamic
passes used in Rosetta to connect TensorFlow and our ZK
protocol.
AND gates for n = 61. However, we found that converting
from ﬁxed-point to ﬂoating-point numbers is about 3× slower,
since private logical right shift is done with n = 61, but private
logical left shift is handled with n = 24 (deﬁned by IEEE-754).
To close the efﬁciency gap between two directions, we can
let the prover provide a converted ﬂoating-point number as
an extended witness on-demand, and then only prove in zero-
knowledge the conversion from a ﬂoating-point number to a
ﬁxed-point number.
6.3
Integrating with TensorFlow
We integrate the algorithms into Rosetta [22], which is an
efﬁcient and easy-to-use framework based on TensorFlow [1].
Speciﬁcally, we implemented our ZK backend protocol in
C++ to maintain high efﬁciency and integrated to the back-
end of TensorFlow. In this way, developers could use simple
interfaces in the frontend (in Python) to build complicated
machine learning models without knowing details of the un-
derlying cryptographic protocols. As a result, one can reuse
the original code and interfaces of TensorFlow, and import
an additional package to enable our ZK protocol. Below we
discuss details of our integration. The main components of
Rosetta are static and dynamic passes described as follows.
Static pass. In the frontend of TensorFlow, developers could
write a model with Python language. The underlying com-
piler will convert the model into a graph, which consists of
nodes and edges. Nodes are actually different operators, and
edges are inputs/outputs of operators with speciﬁc data types
(e.g., int and float). Static pass, described in Figure 11,
is implemented in our framework, which additionally turns
this graph into an abstract secure graph. Secure graph dif-
fers from the original graph in edges and nodes. Particularly,
all the edges in secure graph are string type, which will
contain the input and output information of each operator
implemented with the underlying protocol (e.g., authenticated
values in our ZK protocol). This is designed to be applicable
to various cryptographic algorithms or protocols. Secure op-
erators additionally specify the edges to be either public or
private according to applications. The nodes in secure graph
represent secure operators as shown in Figure 11.
512    30th USENIX Security Symposium
USENIX Association
50 Mbps 200 Mbps 500 Mbps 1 Gbps
7 Performance Evaluation
Conversions
45 µs
49 µs
55 µs
46 µs
46 µs
107 µs
109 µs
56 µs
50 µs
49 µs
34 µs
38 µs
55 µs
46 µs
46 µs
Machine Learning (ML) Functions
1.6 ms
0.4 ms
185 µs
161 ms
257 ms
1.6 ms
0.5 ms
262 µs
157 ms
261 ms
29 µs
33 µs
55 µs
46 µs
46 µs
1.6 ms
0.4 ms
188 µs
171 ms
269 ms
A2B
B2A
C2A
Fix2Float
Float2Fix
Sigmoid
Max Pooling
2.1 ms
1.6 ms
908 µs
209 ms
SoftMax-10
Batch Norm 415 ms
ReLU
185 ms
1.39 s
10.63 s
186 ms
1.48 s
11.30 s
Matrix Multiplications
361 ms
185 ms
MatMult-512
2.42 s
1.37 s
MatMult-1024
MatMult-2048 15.19 s
10.39 s
Table 2: Performance of the basic building blocks. The
dimension of Max Pooling is 2 × 2. The dimension of
Batch Normalization is [1,16,16,4], which stands for the
batch size, height, weight and channels. For ML functions,
the inputs and outputs are authenticated values in Fp with
p = 261 − 1. The performance result assumes that the inputs
and outputs are all private to the veriﬁer.
Dynamic pass. The graph will be executed by TensorFlow
in the backend when data is fed, and the string-type data will
ﬂow across the graph. Dynamic pass shown in Figure 11 is de-
signed to integrate the graph execution with our ZK protocol.
When handling a speciﬁc operator (e.g., matrix multiplica-
tion), dynamic pass will ﬁrst convert the string-type data into
ZK-friendly authenticated values (i.e., ZK type in Figure 11),
and then call the underlying ZK protocol for this operator
and get the authenticated output. Finally, dynamic pass will
convert the resulting authenticated values back to string-type
data, such that the data can be handled by TensorFlow and
passed to the next operator. The universal composability of
our protocol ensures that our approach is secure. To make
sure all operators can be composed together directly as well
as reduce the memory overhead, we encode the inputs and
outputs of all operators into authenticated values over Fp.
Extendibility. In addition to our ZK protocols, Rosetta [22]
is also capable of integrating with other cryptographic pro-
tocols and algorithms, such as MPC and homomorphic en-
cryption. It is feasible to support mixed protocols between
ZK proofs and MPC, where we will leave as a future work.
In this section, we benchmark the speed of Mystique and
how it performs on large-scale ML-inspired applications. We
used three neural network models: LeNet-52 (5 layers, 62000
model parameters), ResNet-50 (50 layers, 23.5 million model
parameters), and ResNet-101 (101 layers, 42.5 million model
parameters). All experimental results are obtained by run-
ning the protocol over two Amazon EC2 machines of type
m5.2xlarge, each with 32 GB memory. We use all CPU
resources but only a fraction of the memory. The largest ex-
ample is for ResNet-101 that uses 12 GB of memory. Our
implementations use the latest sVOLE-based protocol [54]
as the underlying ZK proof. All our implementations achieve
the computational security parameter λ = 128 and statistical
security parameter ρ ≥ 40.
7.1 Benchmarking Our Building Blocks
We test the performance of our key building blocks discussed
in this paper and summarized the results in Table 2. From this