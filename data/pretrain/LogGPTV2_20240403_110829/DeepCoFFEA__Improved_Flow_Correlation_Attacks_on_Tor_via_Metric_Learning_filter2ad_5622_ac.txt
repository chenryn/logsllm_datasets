IV. DEEPCOFFEA ATTACKS
In this section, ﬁrst, we detail how we extended previous
work [26], [28] to train the DeepCoFFEA FENs to generate
Tor and exit ﬂow embeddings. Next, we describe several meth-
ods to compute whether two embedded vectors are correlated
and discuss the architecture of the DeepCoFFEA FENs and
the hyperparameters involved.
A. Feature Embedding Networks for Correlation Study
To develop FENs for use in the DeepCoFFEA attack, we
started with the TF network architecture [28] and adapted it
for the ﬂow correlation attack model. As a preliminary step,
we tried using their networks directly (that is, having the A,P,
and N networks all share weights) when trained with Tor ﬂow,
exit ﬂow, and exit ﬂow triplets, but as expected we found that
triplet loss did not decrease with training. We then made the
following key changes to improve this initial result.
Two Different Networks. Using the TF architecture,
the
triplet loss did not converge, due to two factors. First, Tor
and exit ﬂows are different trafﬁc collected at different points
in that the Tor trace is collected between the client and guard
node and the exit trafﬁc is collected between the exit relay and
the web server. Second, Tor traces contained a relatively lower
number of packets per window than exit traces, requiring dif-
ferent input dimensions for the two networks. Thus, in contrast
to FaceNet [26] and TF, we adopted two separate models: one
for the A network and another common model to the P and
N networks. This approach led to reduction of the initial loss
value, and further, helped achieve decreasing loss curves with
training. However, we still ended up with an overall small drop
in the training loss. For further improvement, we modiﬁed the
triplet generator, as described next.
Triplet Epoch Generator. The TF implementation chose
triplets from positive and negative examples without regard
to whether a particular negative had already been used in a
previous input, which led to many exit ﬂows being selected
both as a positive (p) and as a negative (n). This quickly
froze the triplet loss at some value because some ﬂow pairs
were used interchangeably to both maximize and minimize the
correlation in the triplet loss function. To resolve this issue,
we divided the exit ﬂows into two sets and implemented the
triplet generator to choose p from one set and n from the other
set. In this way, we guaranteed that p and n were always
different within a batch. However, we found that we could
obtain a better loss curve when that guarantee was extended
to an epoch. Note that we set 128 batches for an epoch in all
experiments in the paper. Thus, we kept shufﬂing the exit trace
set and dividing it into two separate pools for every epoch.
With this epoch generator, we were able to reach a training
loss value closer to zero.
Loss Function. The DeepCoFFEA FENs were trained to
minimize the following triplet loss function:
role to train FENs
since “easy” negatives
max(0, corr(G(a), H(n)) − corr(G(a), H(p)) + α)
In other words,
the goal of FENs is to learn embed-
dings G and H that satisfy corr(G(a), H(n)) + α <
corr(G(a), H(p)). The “negative” triplet n plays an im-
portant
in
which under
the current network parameters we already
have corr(G(a), H(n)) + α < corr(G(a), H(p)) will not
contribute to the loss while “hard” negatives in which
corr(G(a), H(p)) < corr(G(a), H(n)) will always lead to
positive loss. Thus, we used “semi-hard negatives” in which
corr(G(a), H(n)) < corr(G(a), H(p)) + α. That is, semi-
hard negatives are “hard enough” that they contribute to the
loss but “easy enough” that adjusting the parameters can push
the loss to 0. As such, by tuning α, we could adjust the
margin that was enforced between positive and negative pairs.
In the following section we describe how we chose the cosine
similarity for the correlation metric (i.e., corr); we chose α
using hyper-parameter tuning, as described in Section V-C.
B. Correlation Methodology
Based on G and H, we extract
the Tor and exit ﬂow
embeddings, G(ti,w) and H(xi,w) in which ti are the Tor
ﬂows, and xi are exit ﬂows, fi,w is the w-th window of ﬂow fi,
and 0≤ i < n, and 1 ≤ w ≤ k for n ﬂow pairs and k windows.
Then, we compute the correlation values, d(G(ti,w), H(xj,w))
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1919
Fig. 2. ROC with different evaluation methods (Note that x-axis is log scale
and RG is Random Guess.
Σn
for each 0 ≤ i, j < n, and each window w ∈ 1, . . . , k, for
each of the n2 potential ﬂow pairings. Third, based on these
correlation values, we record a 1 vote if d(G(ti,w), H(xj,w))
is high enough and 0 otherwise. Finally, we decide that t
and x are correlated if they received at least k − (cid:96) 1 votes.
We considered three different approaches to computing these
similarity “votes”: the softmax function, k-NN classiﬁers, and
cosine similarity. In this section, we describe these options.
Softmax. We applied the softmax function, which normalizes
the embedding into a vector following a probability distribu-
tion with a sum of 1, to the feature embeddings. Based on this
probability vector, we could determine a predicted “label” by
taking the logit with the highest probability. To investigate the
Tor and exit ﬂow pair (t, x), we computed the top-h labels for
t and x based on argsort(soft(G(t))) and argsort(soft(H(x)))
in which soft(xi)= exi
j=1exj . If the matched ﬂow appeared in
the top labels, we assume that it is possibly correlated ﬂow.
By varying h from 2 to 31, we plot TPRs against FPRs.
k-NN with Clustering. We trained k-NN classiﬁers using the
Tor ﬂows and tested them using the exit ﬂows. We further
trained the classiﬁers using the exit ﬂows and tested them
using the Tor ﬂows to ﬁnd the best setting. For either direction,
we ﬁrst had to label the ﬂows in the training set before training.
We explored k-means [30] and Spectral clustering [31] to
label training ﬂows. For both clusters, we varied k from 2
to 295, resulting in 2-295 labels to be trained. After training
k-NN models using Tor (or exit) ﬂows labeled by clustering,
we evaluated the models using exit (or Tor) ﬂows. We then
conducted the pair-wise comparison over the predicted labels
of exit ﬂows and labels of Tor ﬂows decided by the clustering
algorithm. If the labels are the same in the correlated ﬂows,
they are TPs. If they are the same in the uncorrelated ﬂows,
they are FPs. Based on the preliminary experiments, we
decided to train k-NN models using Tor ﬂows and further
test them using exit ﬂows. We also empirically chose Spectral
clustering with the cosine similarity to label the Tor ﬂows and
computed TPRs and FPRs at various k from 2 to 295.
Cosine Similarity. The cosine similarity measures the co-
sine of the angle between two vectors projected in a multi-
dimensional space. It captures the angle, not magnitude, such
as the Euclidean distance. Since the FENs were trained using
triplet loss based on the cosine similarity (cos), we naturally
Fig. 3. Example DeepCoFFEA Scenario: In this example, we had ten (ti, xi)
ﬂow pairs and ﬁve windows (W1,...,W5). First, we performed the non-
overlapping window partition to generate two training sets, Ttr and Xtr, and
ten testing sets, Tte1,..,Tte5,Xte1,..Xte5. Then, we trained the DeepCoFFEA
feature embedding network (FEN) with Ttr and Xtr and generated the feature
embedding vectors using A and P/N models for each testing set, (Ttew ,Xtew )
where w=1,...,5. We then computed the pairwise cosine similarity scores for
each testing window and voted with 1 if the score was greater than τ or 0
otherwise. Finally, we aggregated those results and determined that the ﬂow
pair was correlated if it had at least four 1 votes.
studied this similarity score as the similarity metric for Deep-
CoFFEA. That is, we computed the similarity scores for each
window w of all Tor and exit embedding pairs, (ti,w, xj,w), in
which 0 ≤ i, j < n for n testing ﬂow pairs. For each pair, if
cos(G(ti,w), H(xj,w)) ≥ τ for some threshold τ, we recorded
a vote of 1 and 0 otherwise. By varying τ, we drew the ROC
plot. We present how we chose τ in Section V-E.
As shown in Figure 2,
the cosine similarity approach
outperformed other methodologies. The cosine similarity was
clearly more effective to distinguish the correlated ﬂows
from uncorrelated ﬂows than the softmax-based distinction
and the clustering did not extract effective correlation labels.
We adopted the cosine similarity when evaluating DeepCoF-
FEA throughout the remainder of the paper.
C. FEN Architecture
As shown in previous work [5], [6], [25], [25], CNNs
typically learn more useful features for analysis of Tor trafﬁc.
Thus, we further explored two different architectures, one
based on 1D convolutional (Conv) layers and the other based
on 2D Conv layers. We adopted the DF [5] and DeepCorr [25]
architectures for these two architectures since they have been
effective in generating website ﬁngerprints based on traces
between the client and the guard node and correlational ﬂow
features based on inﬂow and outﬂow to the Tor network. We
empirically concluded that the 1D CNN-based DF architecture
performed better; more speciﬁcally, we were unable to reduce
the triplet loss below 0.01 with the DeepCorr architecture.
As shown in Figure 3, the two FEN models are learned,
and in the testing phase, the A network maps inputs from Tor
ﬂows to feature embedding vectors, while the P/N network
maps inputs from exit ﬂows to feature embedding vectors.
Each FEN consists of four 1D Conv blocks, including two 1D
Conv layers, followed by one max pooling layer. After that,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1920
there was a fully connected output layer, which generated the
feature embedding. We chose the input and output dimensions,
optimizer, and learning rate based on parameter optimization
as shown in Table II.
Finally, we outline an example DeepCoFFEA evaluation
scenario in Figure 3.
V. DEEPCOFFEA EXPERIMENT DETAILS
In this section, we detail the experimental settings including
the dataset, features, window partition, hyper-parameter opti-
mization, and metrics to evaluate DeepCoFFEA in Section VI.
A. Input Preprocessing
Data Collection. To the best of our knowledge, Nasr, Bahra-
mali, and Houmansadr [25] collected the most comprehensive
ﬂow correlation dataset collected on the Tor network and
reﬂects the effect of different circuit usage and time gaps
between training and testing data. After reparsing the raw cap-
tures of DeepCorr set instead of using the preprocessed data in
which the outgoing and incoming packets were separated, we
selectively chose the ﬂow pairs to ensure that all connection
destinations are unique, resulting in 12,503 ﬂow pairs. We call
this set the DeepCorr dataset.
However, we were interested in evaluating additional scenar-
ios that could not be addressed with this dataset alone, includ-
ing a training/testing gap longer than three months, training
and testing sets with no mutual circuits, and the ability to
test with a large non-training set. Thus, we collected our own
dataset (the DCF set) mimicking the collection methodology
devised by DeepCorr, with some small differences.
First, we used physical machines to run both Tor clients
and the SOCKS proxy servers collecting exit ﬂows while the
original method ran clients inside VMs. Second, we crawled
60,084 Alexa websites using 1,051 batches in which the
circuits were rebuilt every batch. We captured for a full 60
seconds for every sample to ensure that any dynamic content
loaded after the initial page load was included in the capture.
Third, we captured packets at the Ethernet layer while Deep-
Corr collected it at the IP layer. This change aimed to remove
over-MTU-sized packets that appeared in the DeepCorr set
and better resemble trafﬁc as it would be seen on the wire. To
provide transferability between datasets we combined adjacent
packets with IPD values of 0 (e.g., the packets arrived/sent
at the same moment) when training our model. Lastly, we
ﬁltered out ﬂows whose packet counts were less than 70, which
was shorter than DeepCorr which applied 300, 400, and 500
when evaluating the model against traces with 300, 400, and
500 packets, respectively. In this way, the DCF set caused
more ﬂows to be padded or truncated than the DeepCorr set.
As shown in Figure 11 in Appendix D, we explored various
ﬁlters to remove shorter trafﬁc whose length was lower than
70, 300, and 500, and the DeepCoFFEA performance was not
signiﬁcantly affected by the size of the ﬁlter.
Among 60,084 ﬂow pairs, we selected pairs whose per-
window packet count was greater than 10, which led to 42,489
pairs in total. Note that to show the model transferability across
different circuits and sites, all 42,489 connections involved
a unique destination. This ensures that there should be no
overlapping destinations and circuit usage between training
and testing sets.
In addition, we also collected 13,000 ﬂows using the obfs4
Pluggable Transport for the evaluation against defended ﬂows.
Furthermore, we captured some older ﬂows crawled in April
2020 for the experimental scenario in which there is a long-
time gap between training and testing data.
To further evaluate DeepCoFFEA against light-weight web-
site ﬁngerprinting defenses including WTF-PAD [32] and
FRONT [33], we also simulated defended ﬂows by using
ofﬁcial implementations shared by researchers [34], [35].
Window Pooling. In contrast to DeepCorr, in which n corre-
lated ﬂow pairs could be used to create up to n2 input pairs,
in DeepCoFFEA each correlated ﬂow pair can only produce at
most one triplet, creating many fewer training examples for the
FEN models. Instead of feeding all possible pairs to FENs, for
each anchor point, we selected one semi-hard negative that was
more useful through triplet mining. For example, if we have
three ﬂow pairs, (t1,x1), (t2,x2), and (t3,x3), our approach
results in three input pairs such as [(t1,x1,x2), (t2,x2,x1),
(t3,x3,x2)]. Based on our epoch generator, only the ﬁrst and
third triplets can appear in the same epoch.
By partitioning the ﬂows into k windows, we were able
to pool the correlated pairs across windows, increasing the
FEN training set size by a factor of k. More speciﬁcally,
we divided each ﬂow based on a predeﬁned time interval
chosen during the tuning (Section V-C), and constructed the
training set. Based on 10,000 pairs of training ﬂows, we built
a training set using all k ﬂow windows, resulting in k· 10, 000
correlated ﬂow pairs. In contrast, we constructed k testing
sets separately for each window in which there are n pairs
(2094 ≤ n ≤ 10, 000). Thus, this setup resulted in testing
DeepCoFFEA using n Tor and exit ﬂows across k windows.
We detail the window partitioning in Section V-B.
Features. As in DeepCorr [25], we used inter-packet delay
(IPD) and packet size information to construct the feature
vectors from the ﬂows. Since we chose 1D CNN models
for DeepCoFFEA, we constructed one-dimensional arrays,
vi = [Ii||Si] for the bi-directional Tor and exit ﬂows by
concatenating the vector of IPDs and packet sizes. Here, the
vector Ii consists of upstream IPDs (I u) and downstream IPDs
(-I d) and the vector Si is comprised of upstream packet sizes
(Su) and downstream packet sizes (-Sd).
i ||I d
i ||I d
i ||Sd
i ||Su
i ||Sd
i ||Su
i ] or [I u
We also evaluated DeepCoFFEA based on other combina-
tions, such as vi = [I u
i ] and
these feature vectors led to much worse performance, perhaps
because the local interleaving of upstream and downstream
trafﬁc allowed the (local) CNN ﬁlters to extract more relevant
features. Similarly, we tried training FENs based on feature
sets using only the IPD vectors or the packet size vectors and
found that these were less effective as well.
Lastly, we evaluated DeepCoFFEA using features based
only on packet sizes. The cosine similarity scores between
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1921
Fig. 4. Overlapping window partition.
(a) ROC for different δ applied to the
window partition when loss ≈ 0.006.
Fig. 5. DeepCoFFEA parameter tuning (Note that x-axis is log scale and RG:
Random guessing).
(b) ROC for various voting thresholds
when loss ≈ 0.006.
correlated ﬂow pairs were higher when considering both