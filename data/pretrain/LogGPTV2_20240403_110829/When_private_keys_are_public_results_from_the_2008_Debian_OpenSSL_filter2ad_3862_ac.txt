### 6.2 Generating Weak Keys

Analyzing the data requires identifying which servers were using weak keys, which in turn necessitates a list of weak keys. The process ID (PID) entropy is limited to just 15 bits, but other parameters, described below, must also be considered.

Previous efforts to generate weak keys have used `getpid` interposition via `LD_PRELOAD`, but this approach does not scale well. Instead, we created a patch for OpenSSL 0.9.8h that allows us to simulate the relevant conditions and used the patched version to generate our corpus of weak keys.

Since the binary representation in memory of certain values is added to the entropy pool rather than a canonical representation, our key generation must account for the target platform’s endianness and native word size. Additionally, the presence of a file called `.rnd` in the user’s home directory affects the behavior of OpenSSL’s command-line utilities. If the file is present, its contents are added to the entropy pool. Therefore, we must generate two sets of keys: one assuming the presence of `.rnd` and one assuming its absence. Due to the Debian bug, the contents of the randomness file do not matter; all 1024-byte files produce the same result. When `.rnd` is missing, versions of OpenSSL before and after 0.9.8f exhibit different behavior, which we must also account for. Debian-derived distributions shipped versions with both behaviors, so we must consider both.

**Note:** This process required several iterations to get right. We initially anticipated that OpenSSL would time out on its own, but this only applies to failed TCP connections. Once the TCP connection succeeds, it can stall indefinitely, which was an unexpected issue. This type of stall occurred frequently enough that we had to parallelize our probes and use an alarm to kill stalled probes. Earlier versions of OpenSSL will add their `struct stat` to the pool regardless of whether the `stat` call succeeds.

We generated keys for each of 32,768 PIDs, on each of three platforms (little-endian 32-bit, big-endian 32-bit, and little-endian 64-bit), for each of three `.rnd` conditions (present, missing with old behavior, missing with new behavior). This resulted in a total of 294,912 keys per key size.

We generated all 294,912 keys for every common key size: 512, 768, 1024, 1536, 2048, 3072, 4096, and 8192 bits. Additionally, we generated all 294,912 keys for various uncommon key sizes encountered in our survey, such as 1000 bits and 1023 bits. However, none of the odd-sized weak keys matched a certificate in our survey.

### 6.3 Processing the Data

To reduce the data (over 30 GB for 173 days) to a manageable form, we created a map from IP-time pairs to certificates. By analyzing each certificate once, we could determine most host properties of interest several orders of magnitude more quickly.

Once we know the status of each host on each day, we still face the challenge of how to analyze that data. A major complication is the natural turnover of certificates. We would expect to see hosts eventually get unaffected certificates even if administrators do not take explicit action, due to the ordinary software upgrade and certificate expiry-and-replacement cycle. A new key generated on an upgraded server will automatically replace the old weak key. Even if the server has not been upgraded (which Rescorla’s results [16] suggest is often the case), the major CAs that dominate our survey attempted to detect vulnerable keys and would refuse to reissue the certificate. Thus, certificate expiration forces the replacement of vulnerable keys; we aim to disentangle this effect from deliberate fixing.

Biology and epidemiology have developed extensive survival analysis techniques to deal with situations where members of a population gradually transition from one state to another (traditionally transitioning from alive to dead, hence the morbid name). The general idea is to look at the hazard function \( h(t) \), which represents the probability of undergoing the transitional event at any period in time, and to compare the hazard functions between different groups. Techniques are also available for dealing with censored population members: those who disappear from view before undergoing the event. A good introduction to these techniques can be found in Kleinbaum [5]. We used Thierry Thernau’s `survival` package for R [14].

**Note for non-statistician readers:** Different statistical packages often use subtly different algorithms even for well-understood operations. Therefore, it is important to document the package used for completeness.

One difficulty here is determining what we treat as an "individual" for the purpose of survival analysis, the host or the certificate. Unfortunately, neither is entirely satisfactory. We observed multiple machines exhibiting the same certificate (in the most extreme case, a single Akamai certificate appeared on 241 distinct hosts). In some cases, two hosts displaying the same certificate would fix on different days or one would disappear without being fixed. Thus, treating machines as the basic unit is problematic.

We adopted the following strategy to deal with these inconsistencies: we grouped all hosts displaying the same initial certificate into a single unit, called a host-certificate (HC), with an "event" assigned based on the final event observed from the HC. For example, if Host A upgraded at time 1 and Host B stopped responding while still unupgraded at time 2, we reported the event as "Censored, 2". On the other hand, if Host A upgraded at time 2 and the last contact with Host B was at time 1, but it was vulnerable at that time, we reported the event as "Fixed, 2". Generally, groups of hosts with the same certificate behaved similarly, so other methodologies would likely have yielded similar results.

Like our initial decision of whether a host is vulnerable, our measurements of host certificate parameters (e.g., self-signed, key size, etc.) are based on our initial contact with a host. For instance, if a host had a vulnerable 1024-bit key, then transitioned to a vulnerable 2048-bit key, and then transitioned to a secure 1024-bit key, we would consider it to be a 1024-bit host.

In 22 cases, we saw hosts that had previously exhibited secure certificates suddenly start to display compromised certificates ("spontaneous generation"). We ignored these hosts.

For demographic information (e.g., cipher suite support), we identify which units (hosts or HCs) we are working with.

### 7. Survey Results

Our main survey contacted 59,100 hosts. Of those, 51,838 answered at one time or another, with an average of 48,555 hosts answering on any given day. During the course of the survey, we observed 751 vulnerable hosts (473 HCs). 507 of these hosts (241 HCs) were fixed during the survey period, with the remainder of the hosts either vulnerable on the final day (n = 206) or not responding (n = 38).

#### 7.1 Demographics

**Certificate Churn.** Even in the absence of security vulnerabilities, CA-issued certificates must typically be reissued every 1 or 2 years. During the 173-day course of our study, 17,579 (34%) of the hosts changed their certificates. As shown in Figure 4, vulnerable certificates are changed at a significantly different rate (p < .001; log-rank test) than other certificates. Qualitatively, this rate is much faster at the beginning of the survey period and then slows down, possibly becoming slightly slower (though we have no statistical tests confirming this) than the baseline past 150 days.

**Figure 4: Rate of Certificate Churn**
```
0   50  100  150
0.4 0.5 0.6 0.7 0.8 0.9 1.0
Days since first measurement
Fraction of HCs Unchanged
Not Vulnerable
Vulnerable
```

While the graph of churn of vulnerable certificates in Figure 4 and the graph of fixing of vulnerable certificates in Figure 1 are similar-looking, they represent different transitional events. If a host's certificate changes but the keys in both the old and new certificates are weak, this represents churn (so it is represented by a drop in Figure 4) but not fixing (so it is not represented by a drop in Figure 1). There were 34 such cases. In 16 of these cases, a certificate was renewed by the same CA and on the same weak key. In 8 more, certificates were issued by a new CA but on the same weak key. In 3 more, renewed certificates were issued by the same CA on a new key, with both the old and new keys weak. In 2 more, certificates were issued by a new CA and on a new weak key. In 2 cases, we saw several overlapping certificates on the same weak key. (In 4 of the cases above, we eventually saw a further new certificate with a good key.) In another 2 cases, self-signed certificates were updated with new, still-weak keys. In the last, spectacular case, a certificate was updated 55 times, with increasingly many CN fields giving a crude form of name-based virtual hosting; the first 10 certificates were all weak. We interpret the evidence above to mean that while the CAs do some checking (as discussed in Section 7.2), they either do not always check or they miss some weak keys.

**Key Lengths.** The vast majority of the HCs (approximately 93%) displayed 1024-bit RSA keys. The remainder were predominantly 512-bit (2%) and 2048-bit (4%) RSA keys, with a scattering of other key sizes. This is more or less as we expected: 1024 bits is the default key size output by most popular key-generation tools, such as the `mkcert.sh` tool included with Mod SSL.

The distribution we observe of key sizes roughly agrees with the results of Lee et al.’s survey [9], which in the November 2006 survey reported 88% 1024-bit keys, 4% 512-bit keys, and 6% 2048-bit keys. Both these surveys differ substantially from Murray’s in 2000 [12], which found 70% 1024-bit keys, 23% 512-bit keys, and almost no 2048-bit keys.

**Certificate Authorities.** Only a small fraction of HCs had self-signed certificates (2%). Most HCs (93%) displayed certificates from CAs which had more than 100 certificates in our total sample. This is strikingly different from reports by some other researchers. In particular, Netcraft [13] in January 2008 reported approximately one quarter of the certificates in their survey as being self-signed. We attribute this difference to our sampling methodology, which is biased towards servers that are heavily used, because it is those to which we will observe network traffic. As an additional datapoint, Murray gathered his list of servers by querying a search engine with various search terms and, in 2000, found less than 3% self-signed certificates.

To test this hypothesis, we portscanned randomly chosen machines on the Internet (using nmap’s `-iR` flag). Several days of scanning recovered 20,214 hosts that accept connections on port 443; we then ran our survey tool against these, obtaining 19,299 certificates. Of these, 8,417 (44%) were self-signed. This clearly indicates that sampling methodology matters—if we want to talk about SSL servers as a group, we must first define what group we are interested in. In particular, it appears that commonly used (and hence "important") servers are more likely to have third-party certificates—thus allowing scalable authentication to arbitrary users—than would be suggested by simple random sampling.

**Figure 5: Distribution of Major Certificate Authorities by the Number of HCs Showing That Certificate**
```
VeriSignThawteEquifaxGoDaddyUSERTRUSTRSANetwork SolutionsEntrustStarfieldDigiCertAkamaiComodoCybertrustMicrosoftSecureTrustIPSDODTrusted SecureXRAMPGlobalSignSomeOrganizatioTorAllVulnerable
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
Process ID
Number of Hosts
0   5000 10000 15000 20000 25000 30000
0   5   10  15  20  25  30
```

Because vulnerability in this case was out of the control of the user, we would expect the initial demographics of vulnerable certificates to be similar to those of non-vulnerable certificates. We indeed find this for the distribution of key lengths. However, when we examine the distribution of CAs, as seen in Figure 5, we find that the distribution is quite different: VeriSign certificates are underrepresented in the population of vulnerable certificates. There are two plausible hypotheses about this difference: either VeriSign customers were less likely to use Debian and Debian-derived distributions, and thus less likely to be affected, or VeriSign customers upgraded faster, and so significantly more had upgraded by the time we started our survey. In Section 7.5, we discuss some evidence against the latter possibility.

**Cipher Suite Support.** As discussed in Section 4.1, servers that have weak long-lived keys but support DHE connections provide a higher degree of confidentiality against passive analysis than those with weak long-lived keys that support only RSA ciphersuites. Of the 746 hosts vulnerable on the first day of our study, 357 (48%) negotiated DHE with our OpenSSL client, indicating that they would likely negotiate DHE with a compatible browser. These comprise approximately one-fifth of the market: Firefox will negotiate DHE with RSA certificates; Safari offers it, but below non-DHE suites that Mod SSL supports; and Internet Explorer does not support DHE_RSA at all. Compared to the vulnerable servers, a smaller fraction of all the hosts we surveyed on day one (30%) negotiated DHE with our client. By contrast, Lee et al. reported 58% penetration for DHE_RSA; we believe that some servers that support DHE nevertheless preferred another cipher suite from the list presented by our client, and that this partly explains the discrepancy. To verify this guess, we would have needed to make multiple connections to each server with different lists of supported cipher suites.

For the same reason, we do not have direct measurements for the level of support of symmetric algorithms. However, approximately 44% of the servers we surveyed on day one negotiated AES with our client, and more may support it, roughly consistent with Lee et al.’s report of 57% AES support. Amazingly, we found 18 hosts that negotiated an export cipher suite and 12 that negotiated single-DES.

**Measuring Server Characteristics.** Because the keys generated by OpenSSL depend on the state of the machine doing the key generation, we can remotely measure some properties of the server (or, more properly, the machine that generated the keys, though these are typically the same) that are ordinarily difficult to obtain. The first property of interest is the PID of the process that generated the key. We naively expected that users would usually generate their keys shortly after boot, thus biasing the PID towards small numbers. However, while our data shows some evidence of biasing, the effect is not particularly strong, as shown in Figure 6, which displays the process ID histogram.

Some researchers have suggested that cloud-based systems are particularly vulnerable to such attacks because they start in a known state that is accessible to attackers [2]. Our findings have negative implications for the feasibility of such attacks.