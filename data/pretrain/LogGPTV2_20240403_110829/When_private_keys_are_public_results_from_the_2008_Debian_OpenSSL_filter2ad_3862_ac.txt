entire set, representing connections to each server in our set
on approximately a daily basis, is upwards of 30 GB
6.2 Generating Weak Keys
Analyzing the data requires identifying which servers were
using weak keys, which in turn requires a list of weak keys.
There is just 15 bits’ worth of process ID entropy, but other
parameters, described below, must also be accounted for.
Previous weak-key generation eﬀorts have used getpid in-
terposition via LD_PRELOAD, but this approach does not scale.
We instead created a patch to OpenSSL 0.9.8h that allows
each of the relevant conditions to be simulated, and used the
patched version to generate our corpus of weak keys.
Because the binary representation in memory of certain
values is added to the entropy pool, not a canonical repre-
sentation, our key generation must account for the target
platform’s endianness and native word size. In addition, the
presence of a ﬁle called .rnd in the user’s home directory
aﬀects the behavior of OpenSSL’s command-line utilities. If
it is present, its contents are added to the entropy pool. Ac-
cordingly, we must generate two sets of keys: ones assuming
the presence of .rnd, one its absence. (Because of the Debian
bug, the contents of the randomness ﬁle are not consulted;
all 1024-byte ﬁles produce the same result.) When .rnd is
missing, versions of OpenSSL before and after 0.9.8f have
diﬀerent behavior that we must again account for.6 Debian-
derived distributions shipped versions with both behaviors,
so we must account for both.
5This took several iterations to get right. We had antici-
pated that OpenSSL would time out on its own, but this
only applies to failed TCP connects. It will stall indeﬁnitely
once the TCP connect succeeded, which was an unpleasant
surprise. This sort of stall happened often enough that we
ended up having to parallelize our probes and use an alarm
to kill stalled probes.
6Earlier versions will add their struct stat to the pool
whether or not the stat call succeeds.
19We generated keys for each of 32768 pids, on each of three
platforms (little-endian 32-bit, big-endian 32-bit, and little-
endian 64-bit), for each of three .rnd conditions (present;
missing, old behavior; missing, new behavior). This is a
total of 294,912 keys per key size.
We generated all 294,912 keys for every common keysize:
512, 768, 1024, 1536, 2048, 3072, 4096, and 8192 bits. In
addition, we generated all 294,912 keys for various oddball
keysizes we encountered in our survey, such as 1000 bits and
1023 bits. In the end, though, none of the odd-sized weak
keys matched a certiﬁcate in our survey.
6.3 Processing the Data
To reduce the data (30+ GB for 173 days) to manage-
able form, we created a map from IP-time pairs to certiﬁ-
cates. By analyzing each certiﬁcate once, we could deter-
mine most host properties we are concerned with7 several
orders of magnitude more quickly.
Once we know the status of each host on each day, we
are still faced with the problem of how to analyze that data.
A major complication is the natural turnover of certiﬁcates:
we would expect to see hosts eventually get unaﬀected cer-
tiﬁcates even if administrators do not take explicit action to
request a new certiﬁcate, due to the ordinary software up-
grade and certiﬁcate expiry-and-replacement cycle: A new
key generated on an upgraded server will automatically re-
place the old weak key. Even if the server has not upgraded
(which Rescorla’s results [16] suggest is often the case), the
major CAs which dominate our survey attempted to detect
vulnerable keys and would refuse to reissue the certiﬁcate.
Thus, certiﬁcate expiration acts to force replacement of vul-
nerable keys; we would like to disentangle this eﬀect from
deliberate ﬁxing.
Biology and epidemiology have developed an extensive ar-
ray of survival analysis techniques designed to deal with
situations like this where members of a population gradu-
ally undergoes a transition from one state to another (tradi-
tionally transitioning from alive to dead, hence the morbid
name). The general idea is to look at the hazard function
h(t) which represents the probability of undergoing the tran-
sitional event at any period in time, and to compare the
hazard functions between diﬀerent groups. Techniques are
also available for dealing with censored population members:
those who disappear from view before undergoing the event.
A good introduction to these techniques may be found in
Kleinbaum [5]. We used Thierry Thernau’s survival [17]
package for R [14].8
One diﬃculty here is determining what we treat as an “in-
dividual” for the purpose of survival analysis, the host or the
certiﬁcate. Unfortunately, neither is entirely satisfactory.
We observed a number of cases of multiple machines exhibit-
ing the same certiﬁcate (in the most extreme case, a single
Akamai certiﬁcate appeared on 241 distinct hosts). In some
cases, two hosts displaying the same certiﬁcate would ﬁx on
diﬀerent days or one would disappear without being ﬁxed.
Thus treating machines as the basic unit is problematic.
We adopted the following strategy to deal with these in-
consistencies: we grouped all hosts displaying the same ini-
7Server cipher suite support is the notable exception.
8Note for non-statistician readers: diﬀerent statistical pack-
ages often use subtly diﬀerent algorithms even for well-
understood operations. Thus, it is important for complete-
ness to document the package used.
tial certiﬁcate into a single unit, called a host-cert (HC),
with an “event” assigned based on the ﬁnal event observed
from the HC: If A upgraded at time 1 and B stopped re-
sponding still unupgraded at time 2, we reported the event
“Censored, 2”; on the other hand, if A upgraded at time 2
and the last contact with B was at time 1 but it was vulner-
able at that time, we reported the event as “Fixed, 2”. In
general, groups of hosts with the same certiﬁcate behaved
similarly, so other methodologies would likely have yielded
similar results.
Like our initial decision of whether a host is vulnerable,
our measurements of host certiﬁcate parameters (e.g., self-
signed, key size, etc.) are based on our initial contact with
a host. For instance, if a host had a vulnerable 1024-bit
key, then transitioned to a vulnerable 2048-bit key, and then
transitioned to a secure 1024-bit key, we would consider it
to be a 1024-bit host.
In 22 cases, we saw hosts which had previously exhibited
secure certiﬁcates suddenly start to display compromised
certiﬁcates (“spontaneous generation”). We ignored these
hosts.
For demographic information (e.g., cipher suite support),
we identify which units (hosts or HCs) we are working in.
7. SURVEY RESULTS
Our main survey contacted 59100 hosts. Of those, 51838
answered at one time or another, with an average of 48555
hosts answering on any given day. During the course of the
survey, we observed 751 vulnerable hosts (473 HCs). 507 of
these hosts (241 HCs) were ﬁxed during the survey period,
with the remainder of the hosts either vulnerable on the ﬁnal
day (n = 206) or not responding (n = 38).
7.1 Demographics
Certiﬁcate Churn. Even in the absence of security vul-
nerabilities, CA-issued certiﬁcates must typically be reis-
sued every 1 or 2 years. During the 173-day course of our
study, 17579 (34%) of the hosts changed their certiﬁcates.
As shown — for HCs — in Figure 4, vulnerable certiﬁcates
are changed at a signiﬁcantly diﬀerent rate (p < .001; log-
rank test) than other certiﬁcates. Qualitatively, this rate is
much faster at the beginning of the survey period and then
slows down and may be slightly slower (though we have no
statistical tests conﬁrming that it becomes slower) than the
baseline out past 150 or so days.
Figure 4: Rate of certiﬁcate churn
0501001500.40.50.60.70.80.91.0Days since first measurementFraction of HCs UnchangedNot VulnerableVulnerable20While the graph of churn of vulnerable certiﬁcates in Fig-
ure 4 and the graph of ﬁxing of vulnerable certiﬁcates in
Figure 1 are similar-looking, they represent diﬀerent tran-
sitional events. If a host’s certiﬁcate changes but the keys
in both the old and new certiﬁcate are weak, this repre-
sents churn (so it is represented by a drop in Figure 4) but
not ﬁxing (so it is not represented by a drop in Figure 1).
There were 34 such cases. In 16 of these cases, a certiﬁcate
was renewed by the same CA and on the same weak key.
In 8 more, certiﬁcates were issued by a new CA but on the
same weak key. In 3 more, renewed certiﬁcates were issued
by the same CA on a new key, with both the old and new
keys weak. In 2 more, certiﬁcates were issued by a new CA
and on a new weak key. In 2 cases, we saw several overlap-
ping certiﬁcates on the same weak key. (In 4 of the cases
above, we eventually saw a further new certiﬁcate with a
good key.) In another 2 cases, self-signed certiﬁcates were
updated with new, still-weak keys. In the last, spectacular
case, a certiﬁcate was updated 55 times, with increasingly
many CN ﬁelds giving a crude form of name-based virtual
hosting; the ﬁrst 10 certiﬁcates were all weak. We interpret
the evidence above to mean that while the CAs do some
checking (as discussed in Section 7.2), they either do not
always check or they miss some weak keys.
Key Lengths. The vast majority of the HCs (approxi-
mately 93%) displayed 1024-bit RSA keys. The remainder
of the were predominately 512-bit (2%) and 2048-bit (4%)
RSA keys with a scattering of other key sizes. This is more
or less as we expected: 1024 bits is the default key size
output by most popular key-generation tools, such as the
mkcert.sh tool included with Mod SSL.
The distribution we observe of key sizes roughly agrees
with the results of Lee et al.’s survey [9], which in the
November 2006 survey reported 88% 1024-bit keys, 4% 512-
bit keys, and 6% 2048-bit keys; both these surveys diﬀer sub-
stantially from Murray’s in 2000 [12], which found 70% 1024-
bit keys, 23% 512-bit keys, and almost no 2048-bit keys.
Certiﬁcate Authorities. Only a small fraction of HCs
had self-signed certiﬁcates (2%). Most HCs (93%) displayed
certiﬁcates from CAs which had more than 100 certiﬁcates
in our total sample.9 This is strikingly diﬀerent from re-
ports by some other researchers. In particular, Netcraft [13]
in January 2008 reported approximately one quarter of the
certiﬁcates in their survey as being self-signed. We attribute
this diﬀerence to our sampling methodology, which is biased
towards servers which are heavily used, because it is those
to which we will observe network traﬃc. As an additional
datapoint, Murray gathered his list of servers by querying a
search engine with various search terms and, in 2000, found
less than 3% self-signed certiﬁcates.
To test this hypothesis, we portscanned randomly chosen
machines on the Internet (using nmap’s -iR ﬂag). Several
days of scanning recovered 20,214 hosts that accept connec-
tions on port 443; we then ran our survey tool against these,
obtaining 19,299 certiﬁcates. Of these, 8,417 (44%) were
self-signed. This seems to clearly indicate that sampling
methodology matters — if we want to talk about SSL servers
as a group we must ﬁrst deﬁne what group we are interested
in. In particular, it appears that commonly used (and hence
“important”) servers are more likely to have third-party cer-
9Note: to determine this 100 threshold we counted all cer-
tiﬁcates we saw, not just the ﬁrst one for a given HC.
tiﬁcates — thus allowing scalable authentication to arbitrary
users — than would be suggested by simple random sam-
pling.
Figure 5 shows the distribution of major certiﬁcate au-
thorities by the number of HCs showing that certiﬁcate.
The names here do not map 1-1 to X.509 issuerName val-
ues. Rather, we merged all CAs with the same brand name
into a single bin: e.g., “VeriSign Class 3 Secure Server CA”
and “VeriSign International Server CA — Class 3” are both
in the “VeriSign” bin. This actually underestimates the in-
ﬂuence of VeriSign, because VeriSign, Thawte, and Equifax
are all owned by VeriSign and collectively they dominate the
market. However, because each brand oﬀers a somewhat dif-
ferent user experience, we have chosen to break out the CAs
by brand. Note also that some of these CAs (e.g., Tor) have
fewer than 100 certiﬁcates in a given day’s data but have
more than 100 certiﬁcates in aggregate.
Figure 5: Distribution of CAs (day 1)
Figure 6: Distribution of Process IDs by cert
Because vulnerability in this case was out of the control
of the user, we would expect the initial demographics of vul-
nerable certiﬁcates to be similar to those of non-vulnerable
certiﬁcates. We indeed ﬁnd this for the distribution of key
lengths. But when we examine the distribution of CAs, as
seen in Figure 5, we ﬁnd that the distribution is quite diﬀer-
ent: VeriSign certiﬁcates are underrepresented in the pop-
ulation of vulnerable certiﬁcates. There are two plausible
hypotheses about this diﬀerence: either VeriSign customers
were less likely to use Debian and Debian-derived distribu-
tions, and were thus less likely to be aﬀected, or VeriSign
customers upgraded faster and so signiﬁcantly more had up-
graded by the time we started our survey. In Section 7.5 we
discuss some evidence against the latter possibility.
VeriSignThawteEquifaxGoDaddyUSERTRUSTRSANetwork SolutionsEntrustStarfieldDigiCertAkamaiComodoCybertrustMicrosoftSecureTrustIPSDODTrusted SecureXRAMPGlobalSignSomeOrganizatio   TorAllVulnerable0.000.050.100.150.200.250.300.35Process IDNumber of Hosts05000100001500020000250003000005101520253021Cipher Suite Support. As discussed in Section 4.1, those
servers that have weak long-lived keys but support DHE con-
nections provide a higher degree of conﬁdentiality against
passive analysis than those with weak long-lived keys that
support only RSA ciphersuites. Of the 746 hosts vulnerable
on the ﬁrst day of our study, 357 (48%) negotiated DHE
with our OpenSSL client, indicating that they would likely
negotiate DHE with a compatible browser. These comprise
approximately one ﬁfth of the market: Firefox will negoti-
ate DHE with RSA certiﬁcates; Safari oﬀers it, but below
non-DHE suites that Mod SSL supports; and Internet Ex-
plorer does not support DHE_RSA at all.10 Compared to the
vulnerable servers, a smaller fraction of all the hosts we sur-
veyed on day one (30%) negotiated DHE with our client. By
contrast, Lee et al. reported 58% penetration for DHE_RSA;
we believe that some servers that support DHE neverthe-
less preferred another cipher suite from the list presented by
our client, and that this partly explains the discrepancy. To
verify this guess, we would have needed to make multiple
connections to each server with diﬀerent lists of supported
cipher suites.
For the same reason, we do not have direct measurements
for the level of support of symmetric algorithms. However,
approximately 44% of the servers we surveyed on day one
negotiated AES with our client, and more may support it,
roughly consistent with Lee et al.’s report of 57% AES sup-
port. Amazingly, we found 18 hosts that negotiated an ex-
port cipher suite and 12 that negotiated single-DES.
Measuring Server Characteristics. Because the keys
generated by OpenSSL depend on the state of the machine
doing the key generation, we can remotely measure some
properties of the server (or, more properly, the machine that
generated the keys, though these are typically the same)
that are ordinarily diﬃcult to obtain. The ﬁrst property
of interest is the pid of the process that generated the key.
We naively expected that the users would usually generate
their keys shortly after boot, thus biasing the pid towards
small numbers. However, while our data shows evidence of
some biasing, the eﬀect is not particularly strong, as shown
in Figure 6, which displays the process ID histogram.
Some researchers have suggested that cloud-based sys-
tems are particularly vulnerable to such attacks because
they start in a known state which is accessible to attack-
ers [2]. Our ﬁndings have negative implications for the fea-