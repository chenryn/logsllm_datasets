running, there should be none of them; in case the Windows Defender Application Guard (WDAG) 
role is installed, there could be an existing VMMEM process instance, which hosts the preloaded 
WDAG container. (This kind of VM is described later in the “VA-backed virtual machines” section.) In 
case a VMMEM process instance exists, you should take note of its process ID (PID).
Open the Hyper-V Manager by typing Hyper-V Manager in the Cortana search box and start 
your virtual machine. After the VM has been started and the guest operating system has success-
fully booted, switch back to the Task Manager and search for a new VMMEM process. If you click 
the new VMMEM process and expand the User Name column, you can see that the process has 
been associated with a token owned by a user named as the VM’s GUID. You can obtain your VM’s 
GUID by executing the following command in an administrative PowerShell window (replace the 
term “” with the name of your VM):
Get-VM -VmName "" | ft VMName, VmId
EXPERIMENT: Playing with the root scheduler
The NT scheduler decides when to select and run a virtual processor belonging to a VM and for 
how long. This experiment demonstrates what we have discussed previously: All the VP dis-
patch threads execute in the context of the VMMEM process, created by the VID driver. For the 
experiment, you need a workstation with at least Windows 10 April 2018 update (RS4) installed, 
along with the Hyper-V role enabled and a VM with any operating system installed ready for use. 
The procedure for creating a VM is explained in detail here: https://docs.microsoft.com/en-us/
virtualization/hyper-v-on-windows/quick-start/quick-create-virtual-machine.
First, you should verify that the root scheduler is enabled. Details on the procedure are avail-
able in the “Controlling the hypervisor’s scheduler type” experiment earlier in this chapter. The 
VM used for testing should be powered down.
Open the Task Manager by right-clicking on the task bar and selecting Task Manager, click the 
Task Manager, click the 
Task Manager
Details sheet, and verify how many VMMEM processes are currently active. In case no VMs are 
running, there should be none of them; in case the Windows Defender Application Guard (WDAG) 
role is installed, there could be an existing VMMEM process instance, which hosts the preloaded 
WDAG container. (This kind of VM is described later in the “VA-backed virtual machines” section.) In 
case a VMMEM process instance exists, you should take note of its process ID (PID).
Open the Hyper-V Manager by typing Hyper-V Manager in the Cortana search box and start 
Hyper-V Manager in the Cortana search box and start 
Hyper-V Manager
your virtual machine. After the VM has been started and the guest operating system has success-
fully booted, switch back to the Task Manager and search for a new VMMEM process. If you click 
the new VMMEM process and expand the User Name column, you can see that the process has 
been associated with a token owned by a user named as the VM’s GUID. You can obtain your VM’s 
GUID by executing the following command in an administrative PowerShell window (replace the 
term “” with the name of your VM):
Get-VM -VmName "" | ft VMName, VmId
298 
CHAPTER 9 Virtualization technologies
The VM ID and the VMMEM process’s user name should be the same, as shown in the follow-
ing figure.
Install Process Explorer (by downloading it from https://docs.microsoft.com/en-us/sysin-
ternals/downloads/process-explorer), and run it as administrator. Search the PID of the correct 
VMMEM process identified in the previous step (27312 in the example), right-click it, and select 
Suspend”. The CPU tab of the VMMEM process should now show “Suspended” instead of the 
correct CPU time.
If you switch back to the VM, you will find that it is unresponsive and completely stuck. This is 
because you have suspended the process hosting the dispatch threads of all the virtual proces-
sors belonging to the VM. This prevented the NT kernel from scheduling those threads, which 
won’t allow the WinHvr driver to emit the needed HvDispatchVp hypercall used to resume the 
VP execution.
If you right-click the suspended VMMEM and select Resume, your VM resumes its execution 
and continues to run correctly.
The VM ID and the VMMEM process’s user name should be the same, as shown in the follow-
ing figure.
Install Process Explorer (by downloading it from https://docs.microsoft.com/en-us/sysin-
ternals/downloads/process-explorer), and run it as administrator. Search the PID of the correct 
ternals/downloads/process-explorer), and run it as administrator. Search the PID of the correct 
ternals/downloads/process-explorer
VMMEM process identified in the previous step (27312 in the example), right-click it, and select 
Suspend”. The CPU tab of the VMMEM process should now show “Suspended” instead of the 
correct CPU time.
If you switch back to the VM, you will find that it is unresponsive and completely stuck. This is
because you have suspended the process hosting the dispatch threads of all the virtual proces-
sors belonging to the VM. This prevented the NT kernel from scheduling those threads, which
won’t allow the WinHvr driver to emit the needed HvDispatchVp hypercall used to resume the 
VP execution.
If you right-click the suspended VMMEM and select Resume, your VM resumes its execution 
and continues to run correctly.
CHAPTER 9 Virtualization technologies
299
Hypercalls and the hypervisor TLFS
Hypercalls provide a mechanism to the operating system running in the root or the in the child parti-
tion to request services from the hypervisor. Hypercalls have a well-defined set of input and output 
parameters. The hypervisor Top Level Functional Specification (TLFS) is available online (https://docs 
.microsoft.com/en-us/virtualization/hyper-v-on-windows/reference/tlfs); it defines the different call-
ing conventions used while specifying those parameters. Furthermore, it lists all the publicly available 
hypervisor features, partition’s properties, hypervisor, and VSM interfaces. 
Hypercalls are available because of a platform-dependent opcode (VMCALL for Intel systems, 
VMMCALL for AMD, HVC for ARM64) which, when invoked, always cause a VM_EXIT into the hypervi-
sor. VM_EXITs are events that cause the hypervisor to restart to execute its own code in the hypervisor 
privilege level, which is higher than any other software running in the system (except for firmware’s 
SMM context), while the VP is suspended. VM_EXIT events can be generated from various reasons. In 
the platform-specific VMCS (or VMCB) opaque data structure the hardware maintains an index that 
specifies the exit reason for the VM_EXIT. The hypervisor gets the index, and, in case of an exit caused 
by a hypercall, reads the hypercall input value specified by the caller (generally from a CPU’s general-
purpose register—RCX in the case of 64-bit Intel and AMD systems). The hypercall input value (see 
Figure 9-16) is a 64-bit value that specifies the hypercall code, its properties, and the calling convention 
used for the hypercall. Three kinds of calling conventions are available:
I 
Standard hypercalls Store the input and output parameters on 8-byte aligned guest physical
addresses (GPAs). The OS passes the two addresses via general-purposes registers (RDX and R8
on Intel and AMD 64-bit systems).
I 
Fast hypercalls Usually don’t allow output parameters and employ the two general-purpose
registers used in standard hypercalls to pass only input parameters to the hypervisor (up to
16 bytes in size).
I 
Extended fast hypercalls (or XMM fast hypercalls) Similar to fast hypercalls, but these use an
additional six floating-point registers to allow the caller to pass input parameters up to 112 bytes
in size.
RsvdZ
(4 bits)
63:60
RsvdZ
(4 bits)
47:44
RsvdZ
(5 bits)
31:27
Fast
(1 bit)
16
Rep start index
(12 bits)
59:48
Variable
header size
(9 bits)
26:17
Call Code
(16 bits)
15:0
Rep count
(12 bits)
43:32
FIGURE 9-16 The hypercall input value (from the hypervisor TLFS).
There are two classes of hypercalls: simple and rep (which stands for “repeat”). A simple hypercall 
performs a single operation and has a fixed-size set of input and output parameters. A rep hypercall 
acts like a series of simple hypercalls. When a caller initially invokes a rep hypercall, it specifies a rep 
count that indicates the number of elements in the input or output parameter list. Callers also specify 
a rep start index that indicates the next input or output element that should be consumed. 
300 
CHAPTER 9 Virtualization technologies
All hypercalls return another 64-bit value called hypercall result value (see Figure 9-17). Generally, 
the result value describes the operation’s outcome, and, for rep hypercalls, the total number of com-
pleted repetition.
Rsvd
(20 bits)
63:40
Rsvd
(16 bits)
31:16
Rep 
complete
(12 bits)
43:32
Result
(16 bits)
15:0
FIGURE 9-17 The hypercall result value (from the hypervisor TLFS).
Hypercalls could take some time to be completed. Keeping a physical CPU that doesn‘t receive 
interrupts can be dangerous for the host OS. For example, Windows has a mechanism that detects 
whether a CPU has not received its clock tick interrupt for a period of time longer than 16 milliseconds. 
If this condition is detected, the system is suddenly stopped with a BSOD. The hypervisor therefore 
relies on a hypercall continuation mechanism for some hypercalls, including all rep hypercall forms. If 
a hypercall isn’t able to complete within the prescribed time limit (usually 50 microseconds), control is 
returned back to the caller (through an operation called VM_ENTRY), but the instruction pointer is not 
advanced past the instruction that invoked the hypercall. This allows pending interrupts to be handled 
and other virtual processors to be scheduled. When the original calling thread resumes execution, it 
will re-execute the hypercall instruction and make forward progress toward completing the operation.
A driver usually never emits a hypercall directly through the platform-dependent opcode. 
Instead, it uses services exposed by the Windows hypervisor interface driver, which is available in 
two different versions:
I 
WinHvr.sys Loaded at system startup if the OS is running in the root partition and exposes
hypercalls available in both the root and child partition.
I 
WinHv.sys Loaded only when the OS is running in a child partition. It exposes hypercalls
available in the child partition only.
Routines and data structures exported by the Windows hypervisor interface driver are extensively 
used by the virtualization stack, especially by the VID driver, which, as we have already introduced, 
covers a key role in the functionality of the entire Hyper-V platform.
Intercepts
The root partition should be able to create a virtual environment that allows an unmodified guest OS, 
which was written to execute on physical hardware, to run in a hypervisor’s guest partition. Such legacy 
guests may attempt to access physical devices that do not exist in a hypervisor partition (for example, 
by accessing certain I/O ports or by writing to specific MSRs). For these cases, the hypervisor provides 
the host intercepts facility; when a VP of a guest VM executes certain instructions or generates certain 
exceptions, the authorized root partition can intercept the event and alter the effect of the intercepted 
instruction such that, to the child, it mirrors the expected behavior in physical hardware. 
CHAPTER 9 Virtualization technologies
301
When an intercept event occurs in a child partition, its VP is suspended, and an intercept message 
is sent to the root partition by the Synthetic Interrupt Controller (SynIC; see the following section 
for more details) from the hypervisor. The message is received thanks to the hypervisor’s Synthetic 
ISR (Interrupt Service Routine), which the NT kernel installs during phase 0 of its startup only in case 
the system is enlightened and running under the hypervisor (see Chapter 12 for more details). The 
hypervisor synthetic ISR (KiHvInterrupt), usually installed on vector 0x30, transfers its execution 
to an external callback, which the VID driver has registered when it started (through the exposed 
HvlRegisterInterruptCallback NT kernel API).
The VID driver is an intercept driver, meaning that it is able to register host intercepts with the 
hypervisor and thus receives all the intercept events that occur on child partitions. After the partition 
is initialized, the WM Worker process registers intercepts for various components of the virtualization 
stack. (For example, the virtual motherboard registers I/O intercepts for each virtual COM ports of the 
VM.) It sends an IOCTL to the VID driver, which uses the HvInstallIntercept hypercall to install the inter-
cept on the child partition. When the child partition raises an intercept, the hypervisor suspends the VP 
and injects a synthetic interrupt in the root partition, which is managed by the KiHvInterrupt ISR. The 
latter routine transfers the execution to the registered VID Intercept callback, which manages the event 
and restarts the VP by clearing the intercept suspend synthetic register of the suspended VP.
The hypervisor supports the interception of the following events in the child partition:
I 
Access to I/O ports (read or write)
I 
Access to VP’s MSR (read or write)
I 
Execution of CPUID instruction
I 
Exceptions
I 
Accesses to general purposes registers
I 
Hypercalls
The synthetic interrupt controller (SynIC)
The hypervisor virtualizes interrupts and exceptions for both the root and guest partitions through 
the synthetic interrupt controller (SynIC), which is an extension of a virtualized local APIC (see the Intel 
or AMD software developer manual for more details about the APIC). The SynIC is responsible for 
dispatching virtual interrupts to virtual processors (VPs). Interrupts delivered to a partition fall into two 
categories: external and synthetic (also known as internal or simply virtual interrupts). External inter-
rupts originate from other partitions or devices; synthetic interrupts are originated from the hypervisor 
itself and are targeted to a partition’s VP.
When a VP in a partition is created, the hypervisor creates and initializes a SynIC for each supported 
VTL. It then starts the VTL 0’s SynIC, which means that it enables the virtualization of a physical CPU’s 
302 
CHAPTER 9 Virtualization technologies
APIC in the VMCS (or VMCB) hardware data structure. The hypervisor supports three kinds of APIC 
virtualization while dealing with external hardware interrupts:
I 
In standard configuration, the APIC is virtualized through the event injection hardware support.
This means that every time a partition accesses the VP’s local APIC registers, I/O ports, or MSRs
(in the case of x2APIC), it produces a VMEXIT, causing hypervisor codes to dispatch the inter-
rupt through the SynIC, which eventually “injects” an event to the correct guest VP by manipu-
lating VMCS/VMCB opaque fields (after it goes through the logic similar to a physical APIC,
which determines whether the interrupt can be delivered).
I 
The APIC emulation mode works similar to the standard configuration. Every physical inter-
rupt sent by the hardware (usually through the IOAPIC) still causes a VMEXIT, but the hypervi-
sor does not have to inject any event. Instead, it manipulates a virtual-APIC page used by the
processor to virtualize certain access to the APIC registers. When the hypervisor wants to inject
an event, it simply manipulates some virtual registers mapped in the virtual-APIC page. The
event is delivered by the hardware when a VMENTRY happens. At the same time, if a guest VP
manipulates certain parts of its local APIC, it does not produce any VMEXIT, but the modifica-
tion will be stored in the virtual-APIC page.
I 
Posted interrupts allow certain kinds of external interrupts to be delivered directly in the guest
partition without producing any VMEXIT. This allows direct access devices to be mapped directly
in the child partition without incurring any performance penalties caused by the VMEXITs. The
physical processor processes the virtual interrupts by directly recording them as pending on the
virtual-APIC page. (For more details, consult the Intel or AMD software developer manual.)
When the hypervisor starts a processor, it usually initializes the synthetic interrupt controller module 
for the physical processor (represented by a CPU_PLS data structure). The SynIC module of the physical 
processor is an array of an interrupt’s descriptors, which make the connection between a physical inter-
rupt and a virtual interrupt. A hypervisor interrupt descriptor (IDT entry), as shown in Figure 9-18, contains 
the data needed for the SynIC to correctly dispatch the interrupt, in particular the entity the interrupt is 
delivered to (a partition, the hypervisor, a spurious interrupt), the target VP (root, a child, multiple VPs, or 
a synthetic interrupt), the interrupt vector, the target VTL, and some other interrupt characteristics.
Dispatch Type
Target VP & VTL
Virtual Vector
Interrupt
Characteristics
Hypervisor
Reserved
FIGURE 9-18 The hypervisor physical interrupt descriptor.
CHAPTER 9 Virtualization technologies
303
In default configurations, all the interrupts are delivered to the root partition in VTL 0 or to the 
hypervisor itself (in the second case, the interrupt entry is Hypervisor Reserved). External interrupts 
can be delivered to a guest partition only when a direct access device is mapped into a child partition; 
NVMe devices are a good example. 
Every time the thread backing a VP is selected for being executed, the hypervisor checks whether 
one (or more) synthetic interrupt needs to be delivered. As discussed previously, synthetic interrupts 
aren’t generated by any hardware; they’re usually generated from the hypervisor itself (under certain 
conditions), and they are still managed by the SynIC, which is able to inject the virtual interrupt to the 
correct VP. Even though they’re extensively used by the NT kernel (the enlightened clock timer is a 
good example), synthetic interrupts are fundamental for the Virtual Secure Mode (VSM). We discuss 
them in in the section “The Secure Kernel” later in this chapter. 
The root partition can send a customized virtual interrupt to a child by using the HvAssertVirtualInterrupt 
hypercall (documented in the TLFS).
Inter-partition communication
The synthetic interrupt controller also has the important role of providing inter-partition communica-
tion facilities to the virtual machines. The hypervisor provides two principal mechanisms for one parti-
tion to communicate with another: messages and events. In both cases, the notifications are sent to the 
target VP using synthetic interrupts. Messages and events are sent from a source partition to a target 
partition through a preallocated connection, which is associated with a destination port. 
One of the most important components that uses the inter-partition communication services pro-
vided by the SynIC is VMBus. (VMBus architecture is discussed in the “Virtualization stack” section later 
in this chapter.) The VMBus root driver (Vmbusr.sys) in the root allocates a port ID (ports are identified 
by a 32-bit ID) and creates a port in the child partition by emitting the HvCreatePort hypercall through 
the services provided by the WinHv driver. 
A port is allocated in the hypervisor from the receiver’s memory pool. When a port is created, the 
hypervisor allocates sixteen message buffers from the port memory. The message buffers are main-
tained in a queue associated with a SINT (synthetic interrupt source) in the virtual processor’s SynIC. 