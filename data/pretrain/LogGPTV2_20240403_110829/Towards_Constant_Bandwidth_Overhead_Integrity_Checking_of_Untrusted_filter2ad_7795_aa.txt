title:Towards Constant Bandwidth Overhead Integrity Checking of Untrusted
Data
author:Dwaine E. Clarke and
G. Edward Suh and
Blaise Gassend and
Ajay Sudan and
Marten van Dijk and
Srinivas Devadas
Towards Constant Bandwidth Overhead
Integrity Checking of Untrusted Data
Dwaine Clarke, G. Edward Suh, Blaise Gassend, Ajay Sudan, Marten van Dijk, Srinivas Devadas
Massachusetts Institute of Technology
Computer Science and Artiﬁcial Intelligence Laboratory
{declarke, suh, gassend, ajaytoo, marten, devadas}@mit.edu
Cambridge, MA 02139
Abstract
We present an adaptive tree-log scheme to improve the
performance of checking the integrity of arbitrarily-large
untrusted data, when using only a small ﬁxed-sized trusted
state. Currently, hash trees are used to check the data. In
many systems that use hash trees, programs perform many
data operations before performing a critical operation that
exports a result outside of the program’s execution environ-
ment. The adaptive tree-log scheme we present uses this
observation to harness the power of the constant runtime
bandwidth overhead of a log-based scheme. For all pro-
grams, the adaptive tree-log scheme’s bandwidth overhead
is guaranteed to never be worse than a parameterizable
worst case bound. Furthermore, for all programs, as the av-
erage number of times the program accesses data between
critical operations increases, the adaptive tree-log scheme’s
bandwidth overhead moves from a logarithmic to a constant
bandwidth overhead.
1. Introduction
This paper studies the problem of checking the integrity
of operations performed on an arbitrarily-large amount of
untrusted data, when using only a small ﬁxed-sized trusted
state. Commonly, hash trees [1] are used to check the in-
tegrity of the operations. The hash tree checks data each
time it is accessed and has a logarithmic bandwidth over-
head as an extra logarithmic number of hashes must be read
each time the data is accessed.
One proposed use of a hash tree is in a single-chip se-
cure processor [8, 10, 12], where it is used to check the in-
tegrity of external memory. A secure processor can be used
to help license software programs, where it seeks to pro-
vide the programs with private, tamper-evident execution
environments.
In such an application, an adversary’s job
is to get the processor to unintentionally sign incorrect re-
sults or unintentionally reveal private instructions or private
data in plaintext. Thus, assuming covert channels are pro-
tected by techniques such as memory obfuscation [5, 10],
with regards to security, the critical instructions are the in-
structions that export plaintext outside of the program’s ex-
ecution environment, such as the instructions that sign cer-
tiﬁcates certifying program results and the instructions that
export plaintext data to the user’s display. It is common for
programs to perform millions of instructions, and perform
millions of memory accesses, before performing a critical
instruction. As long as the sequence of memory operations
is checked when the critical instruction is performed, it is
not necessary to check each memory operation as it is per-
formed and using a hash tree to check the memory may be
causing unnecessary overhead.
In [2, 11], a new scheme, referred to as a log-hash
scheme, was introduced to securely check memory. Intu-
itively, the processor maintains a “write log” and a “read
log” of its write and read operations to the external mem-
ory. At runtime, the processor updates the logs with a min-
imal constant-sized bandwidth overhead so that it can ver-
ify the integrity of a sequence of operations at a later time.
To maintain the logs in a small ﬁxed-sized trusted space in
the processor, the processor uses incremental multiset hash
functions [2] to update the logs. When the processor needs
to check a sequence of its operations, it performs a sepa-
rate integrity-check operation using the logs. The integrity-
check operation is performed when the program performs
a critical instruction: a critical instruction acts as a signal
indicating when it is necessary to perform the integrity-
check operation. (Theoretically, the hash tree checks each
memory operation as it is performed. However, in a secure
processor implementation, because the latency of verifying
values from memory can be large, the processor “specu-
latively” uses instructions and data that have not yet been
veriﬁed, performing the integrity veriﬁcation in the back-
Proceedings of the 2005 IEEE Symposium on Security and Privacy (S&P’05) 
1081-6011/05 $ 20.00 IEEE
ground. Whenever a critical instruction occurs, the proces-
sor waits for all of the integrity veriﬁcation to be completed
before performing the critical instruction. Thus, the notion
of a critical instruction that acts as signal indicating that a
sequence of operations must be veriﬁed is already present
in secure processor hash tree implementations.)
While the log-hash scheme does not incur the logarith-
mic bandwidth overhead of the hash tree, its integrity-check
operation needs to read all of the memory that was used
since the beginning of the program’s execution. When
integrity-checks are infrequent, the number of memory op-
erations performed by the program between checks is large
and the amortized cost of the integrity-check operation is
very small. The bandwidth overhead of the log-hash scheme
is mainly its constant-sized runtime bandwidth overhead,
which is small. This leads the log-hash scheme to perform
very well and to signiﬁcantly outperform the hash tree when
integrity-checks are infrequent. However, when integrity
checks are frequent, the program just uses a small subset
of addresses that are protected by the log-hash scheme be-
tween the checks. The amortized cost of the integrity-check
operation is large. As a result, the performance of the log-
hash scheme is not good and is much worse than that of the
hash tree. Thus, though the log-hash scheme performs very
well when checks are infrequent, it cannot be widely-used
because its performance is poor when checks are frequent.
In this paper, we introduce secure tree-log integrity
checking. This hybrid scheme of the hash tree and log-hash
schemes captures the best features of both schemes. The
untrusted data is originally protected by the tree, and sub-
sets of it can be optionally and dynamically moved from the
tree to the log-hash scheme. When the log-hash scheme is
used, only the addresses of the data that have been moved to
the log-hash scheme since the last log-hash integrity check
need to be read to perform the next log-hash integrity check,
instead of reading all of the addresses that the program used
since the beginning of its execution. This optimizes the log-
hash scheme, facilitating much more frequent log-hash in-
tegrity checks, making the log-hash approach more widely-
applicable.
The tree-log scheme we present has three features.
Firstly, the scheme adaptively chooses a tree-log strategy
for the program that indicates how the program should use
the tree-log scheme when the program is run. This al-
lows programs to be run unmodiﬁed and still beneﬁt from
the tree-log scheme’s features. Secondly, even though the
scheme is adaptive, it is able to provide a guarantee on its
worst case performance such that, for all programs, the per-
formance of the scheme is guaranteed to never be worse
than a parameterizable worst case bound. The third fea-
ture is that, for all programs, as the average number of per
data program operations (total number of program data op-
erations/total number of data accessed) between critical op-
erations increases, the performance of the tree-log integrity
checking moves from a logarithmic to a constant bandwidth
overhead.
With regards to the second feature, the worst-case bound
is a parameter to the adaptive tree-log scheme. The bound
is expressed relative to the bandwidth overhead of the hash
tree, if the hash tree had been used to check the integrity
of the data during the program’s execution. For instance,
if the bound is set at 10%, then, for all programs, the tree-
log bandwidth overhead is guaranteed to be less than 1.1
times the hash tree bandwidth overhead. This feature is im-
portant because it allows the adaptive tree-log scheme to be
turned on by default in applications. To provide the bound,
we introduce the notion of a reserve to determine when data
should just be kept in the tree and to regulate the rate at
which data is added to the log-hash scheme. The adaptive
tree-log scheme is able to provide the bound even when no
assumptions are made about the program’s access patterns
and even when the processor uses a cache, about which min-
imal assumptions are made (the cache only needs to have a
deterministic cache replacement policy, such as the least re-
cently used (LRU) policy).
With regards to the third feature, the adaptive tree-log
scheme is able to approach a constant bandwidth data in-
tegrity checking overhead because it can use the optimized
log-hash scheme to check sequences of data operations be-
fore a critical operation is performed. The longer the se-
quence, the more the data that the tree-log scheme moves
from the tree to the log-hash scheme and the more the over-
head approaches the constant-runtime overhead of the log-
hash scheme. As programs typically perform many data
operations before performing a critical operation, there are
large classes of programs that will be able to take advan-
tage of this feature to improve their data integrity check-
ing performance. (We note that we are actually stating the
third feature a bit imprecisely in this section. After we have
described the adaptive tree-log scheme, we will state the
feature more precisely for the case without caching in Sec-
tion 6.3, and modify the theoretical claims on the feature for
the case with caching in Section 6.4.)
While the paper is primarily focused on providing the
theoretical foundation for the adaptive tree-log scheme, we
present some experimental results showing that the band-
width overhead can be signiﬁcantly reduced when the adap-
tive tree-log scheme is used, compared to when a hash tree
is used. In light of the algorithm’s features and the results,
we provide a discussion in Appendix C on tradeoffs that a
system designer may consider making when implementing
the scheme in his system.
Hash trees have been implemented in both software and
hardware applications. For simplicity, throughout this pa-
per, we will use secure processors and memory integrity
checking as our example application. However, the adap-
Proceedings of the 2005 IEEE Symposium on Security and Privacy (S&P’05) 
1081-6011/05 $ 20.00 IEEE
FSM
cache
store
load
checker
ﬁxed-sized
state
write
read
RAM
trusted
untrusted
Figure 1. Model
tive tree-log algorithm can be implemented anywhere hash
trees are currently being used to check untrusted data. The
application can experience a signiﬁcant beneﬁt if programs
can perform sequences of data operations before perform-
ing a critical operation.
The paper is organized as follows.
Section 2 de-
scribes related work. Section 3 presents our model. Sec-
tion 4 presents background information on memory in-
tegrity checking:
it describes the hash-tree and log-hash
checkers. Section 5 details our tree-log checker. Section 6
describes our adaptive tree-log checker. Section 7 provides
an experimental evaluation of the adaptive tree-log checker.
Section 8 concludes the paper. The appendices provide var-
ious supplemental material.
2. Related Work
The use of a hash tree (also known as a Merkle tree [9])
to check the integrity of untrusted memory was introduced
by Blum et al. [1]. The paper also introduced a log-based
scheme to check the correctness of memory. The log-based
scheme in [1] could detect random errors, but it was not se-
cure against active adversaries. The log-hash scheme that
the tree-log scheme uses is secure against an active adver-
sary.
It is also more efﬁcient than the log-based scheme
in [1] because time stamps can be smaller without increas-
ing the frequency of checks. Log-based schemes, by them-
selves, are not general enough because they do not per-
form well when integrity checks are frequent. The tree-log
scheme can use the tree when checks are frequent and move
data from the tree to the log-hash scheme as sequences of
operations are performed to take advantage of the constant
runtime bandwidth overhead of the log-hash scheme.
Hall and Jutla [6] propose parallelizable authentication
trees. In a standard hash tree, the hash nodes along the path
from the leaf to the root can be veriﬁed in parallel. Paral-
lelizable authentication trees also allow the nodes to be up-
dated in parallel on store operations. The log-hash scheme
could be integrated into these trees in a manner similar to
how we integrate it into a standard hash tree. However, the
principal point is that trees still incur a logarithmic band-
width overhead, whereas our tree-log scheme can reduce
the overhead to a constant bandwidth overhead.
3. Model
Figure 1 illustrates the model we use. There is a checker
that keeps and maintains some small, ﬁxed-sized, trusted
state. The untrusted RAM (main memory) is arbitrarily
large. The ﬁnite state machine (FSM) generates loads and
stores and the checker updates its trusted state on each FSM
load or store to the untrusted RAM. The checker uses its
trusted state to verify the integrity of the untrusted RAM.
The FSM may also maintain a ﬁxed-sized trusted cache.
The cache is initially empty, and the FSM stores data that
it frequently accesses in the cache. Data that is loaded into
the cache is checked by the checker and can be trusted by
the FSM.
The FSM is the unmodiﬁed processor running a user
program. The processor can have an on-chip cache. The
checker is special hardware that is added to the processor.
The trusted computing base (TCB) consists of the FSM with
its cache and the checker with its trusted state.
The problem that this paper addresses is that of check-
ing if the untrusted RAM behaves like valid RAM. RAM
behaves like valid RAM if the data value that the checker
reads from a particular address is the same data value that
the checker most recently wrote to that address.
In our model, the untrusted RAM is assumed to be ac-
tively controlled by an adversary. The adversary can per-
form any software or hardware-based attack on the RAM.
The untrusted RAM may not behave like valid RAM if the
RAM has malfunctioned because of errors, or if the data
stored has somehow been altered by the adversary. We are
interested in detecting whether the RAM has been behaving
correctly (like valid RAM) during the execution of the FSM.
The adversary could corrupt the entire contents of the RAM
and there is no general way of recovering from tampering
other than restarting the program execution from scratch;
thus, we do not consider recovery methods in this paper.
For this problem, a simple approach such as calculating
a message authentication code (MAC) of the data value and
Proceedings of the 2005 IEEE Symposium on Security and Privacy (S&P’05) 
1081-6011/05 $ 20.00 IEEE
root = h(h1.h2)
h1 = h(V1.V2)
h2 = h(V3.V4)
V1
V2
V3
V4
Figure 2. A binary hash tree.
address, writing the (data value, MAC) pair to the address
and using the MAC to check the data value on each read,
does not work. The approach does not prevent replay at-
tacks: an adversary can replace the (data value, MAC) pair
currently at an address with a different pair that was previ-
ously written to the address.
We deﬁne a critical operation as one that will break the
security of the system if the FSM performs it before the in-
tegrity of all the previous operations on the untrusted RAM
is veriﬁed. The checker must verify whether the RAM has
been behaving correctly (like valid RAM) when the FSM
performs a critical operation. Thus, the FSM implicitly de-
termines when it is necessary to perform checks based on
when it performs a critical operation. It is not necessary to
check each FSM memory operation as long as the checker
checks the sequence of FSM memory operations when the
FSM performs a critical operation.
4. Background
4.1. Hash Tree
The scheme with which we compare our work is in-
tegrity checking using hash trees. Figure 2 illustrates a
hash tree. The data values are located at the leaves of the
tree. Each internal node contains a collision resistant hash
of the concatenation of the data that is in each one of its
children. The root of the tree is stored in the trusted state in
the checker where it cannot be tampered with.
To check the integrity of a node, the checker: 1) reads the
node and its siblings, 2) concatenates their data together, 3)
hashes the concatenated data and 4) checks that the resultant
hash matches the hash in the parent. The steps are repeated
on the parent node, and on its parent node, all the way to the