title:Global Feature Analysis and Comparative Evaluation of Freestyle In-Air-Handwriting
Passcode for User Authentication
author:Duo Lu and
Yuli Deng and
Dijiang Huang
Global Feature Analysis and Comparative Evaluation of
Freestyle In-Air-Handwriting Passcode for User Authentication
Duo Lu∗
PI:EMAIL
Rider University
Lawrence, New Jersey, USA
Yuli Deng
PI:EMAIL
Arizona State University
Tempe, Arizona, USA
Dijiang Huang†
PI:EMAIL
Arizona State University
Tempe, Arizona, USA
ABSTRACT
Freestyle in-air-handwriting passcode-based user authentication
methods address the needs for Virtual Reality (VR) / Augmented
Reality (AR) headsets, wearable devices, and game consoles where
a physical keyboard cannot be provided for typing a password, but
a gesture input interface is readily available. Such an authentication
system can capture the hand movement of writing a passcode string
in the air and verify the user identity using both the writing content
(like a password) and the writing style (like a behavior biometric
trait). However, distinguishing handwriting signals from different
users is challenging in signal processing, feature extraction, and
matching. In this paper, we provide a detailed analysis of the global
features of in-air-handwriting signals and a comparative evaluation
of such a user authentication framework. Also, we build a prototype
system with two different types of hand motion capture devices,
collect two datasets, and conduct an extensive evaluation.
CCS CONCEPTS
• Security and privacy → Authentication; Graphical / visual
passwords; Multi-factor authentication.
KEYWORDS
In-Air-Handwriting, User Authentication, Gesture Input Interface
ACM Reference Format:
Duo Lu, Yuli Deng, and Dijiang Huang. 2021. Global Feature Analysis and
Comparative Evaluation of Freestyle In-Air-Handwriting Passcode for User
Authentication. In Annual Computer Security Applications Conference (AC-
SAC ’21), December 6–10, 2021, Virtual Event, USA. ACM, New York, NY,
USA, 14 pages. https://doi.org/10.1145/3485832.3485906
1 INTRODUCTION
Gesture interfaces are generally considered the next generation
of Human-Computer Interface (HCI). These novel computing plat-
forms equipped with gesture interfaces are gaining popularity in
∗The author Duo Lu is also affiliated with Arizona State University, PI:EMAIL,
and this work is done when Duo Lu is a PhD student at Arizona State University.
†This research is sponsored by NSF grant CNS-1925709
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8579-4/21/12...$15.00
https://doi.org/10.1145/3485832.3485906
consumer electronics [16], gaming (e.g., Sony PSVR, Facebook Ocu-
lus Quest), and education markets (e.g., Microsoft HoloLens). For
example, users can play VR games and interact with virtual objects
using direct hand movements captured with handheld controllers
or cameras. However, on these platforms, presenting a physical
keyboard or touchscreen is usually impractical, and a virtual key-
board is inconvenient to use without contact feedback. Meanwhile,
many applications need to authenticate a user to unlock the device
or log in to an account to access private data and services, which
typically requires the user to type a password.
A freestyle in-air-handwriting passcode can be used for authen-
tication by leveraging the native gesture user interface. Like a
password-based system, the user is asked to "write a passcode
string" instead of "type a password." The difference is that a pass-
word only contains typed discrete alphanumerical symbols and
special characters individually distinguished. At the same time, the
content of a freestyle in-air-handwriting passcode can be a string
in any language, a signature, or a piece of a doodle as long as the
user can naturally reproduce it but difficult to mimic by others.
Meanwhile, a password is typically hashed and matched with a
stored hash bit-by-bit. Still, a freestyle in-air-handwriting passcode
is captured as a signal and compared with a template for authenti-
cation, in a way closer to signature verification. Thus, we call it a
“passcode” rather than a “password”. Besides the passcode content,
the hand movement is also influenced by the writing style and the
hand geometry, which provides better security and flexibility. For
example, when the passcode content is leaked, such an authenti-
cation system can still defend imposters to a certain extent based
on the handwriting style (like a signature but different from a pass-
word). Meanwhile, it has the desired characteristics of a password,
such as changeability, revocability, and privacy. The content of the
in-air-handwriting can be changed and is not necessarily linked to
a person, which is different from a person biometric trait such as a
fingerprint. Additionally, the coronavirus outbreak has presented
new issues to the extensively deployed fingerprint and facial recog-
nition applications. People wear facial masks and avoid touching
a shared physical device, which also brings interest to gesture-
based authentication methods such as freestyle in-air-handwriting
passcodes.
There are three major technical challenges due to the inherent
characteristics of freestyle in-air-handwriting. The first one is lim-
ited sensor capability, i.e., accurate tracking of fast hand movement
with enough resolution is difficult using affordable consumer elec-
tronic motion capture sensor. Also, there is no standard on the
sensor characteristics or data format. The second one is informa-
tion fuzziness, i.e., there are small variations even for the same user
writing the same passcode multiple times. How to tolerate these
468ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Duo Lu, Yuli Deng, and Dijiang Huang
variations and distinguish legitimate users from imposters at the
same time is not well-understood. Third, limited data availability,
i.e., the authentication system can only collect a few samples at
registration (typically two to five) for usability reasons. Meanwhile,
collecting a dataset for developing data-driven matching algorithms
is also expensive. Furthermore, it is difficult to compare different
algorithms from different research groups because they usually use
their own datasets. In this paper, we provide a feature analysis and
comparative evaluation with an authentication framework devel-
oped by us to address these challenges. Our contributions are as
follows:
1) We designed a unified model of freestyle in-air-handwriting
passcode signals and implemented an authentication framework
based on it. The framework supports two different types of inexpen-
sive hand motion capture devices (a glove with an inertial sensor
and a 3D stereo camera) and multiple authentication algorithms
for different scenarios.
2) We collected two datasets from 180 users and 10 impostors,
conducted an in-depth analysis of the in-air-handwriting signals,
and compared the results with two types of sensor devices with the
same group of users writing the same content for passcodes. Our
datasets will be made publicly available1.
The remainder of the paper is organized as follows. Section
2 shows related works on gesture-based authentication systems,
and section 3 presents the architecture of our framework. Section
4 provides an analysis of various types of features of the in-air-
handwriting signal. Section 5 describes our proposed authentication
algorithm. Section 6 shows the empirical evaluation results, and
section 7 concludes.
2 RELATED WORK
As a branch of Natural User Interface (NUI) based authentication
methods [27], authentication with freestyle gestures [7] generally
considers two types of uses two types of devices: inertial sensors
[23][15][8][11][24][2][21] or 3D cameras [31][36][35][28][5]. A few
early works use dedicated devices with an accelerometer to track
simple hand motions such as shake [23] or swing [15]. They demon-
strate the feasibility, but they also show that these motions have
limited information to distinguish different users. Instead, com-
plicated hand movements like in-air-handwriting passcode or in-
air-signature can be captured using the inertial measurement unit
(IMU) on a smartphone [11], or a handheld remote controller [2]
for authentication. However, the behavior of writing by holding
a device is different from writing with a pen. Freestyle handwrit-
ing using the fingertip in the air can be captured by a wearable
data glove with an IMU [21], a smartwatch [24], or a camera-based
gesture interface with the bare hand. Such cameras include the
Microsoft Kinect [31][36][35], the Google Glass [28], or the Leap
Motion controller [5][1][6]. Typically, the user’s writing content is
determined by the user, like a password or a signature, and the users
are not required to move the hand to help the system convert the
motion signal to discrete symbols. Hence, it is called a “freestyle in-
air-handwriting passcode” instead of a type of graphical password
[34][30][25]. Since signals from different devices have different
formats and their performance cannot be directly compared. This
1https://github.com/duolu/fmkit
Figure 1: Authentication framework architecture.
paper proposes a unified framework that can accommodate both
the inertial sensor and the Leap Motion controller. We also show
their performances are comparable. Moreover, there is also a signif-
icant amount of research works in 2D online signature verification
[32][13] and multitouch gesture authentication [26][37]. We do not
refer to or compare with any of them because the writing behavior
using a pen with the support of a surface is fundamentally different
from writing using the index finger in the air.
The authentication algorithms in existing works include template
matching in the temporal domain [31], comparison of frequency or
statistical properties [23][38], and machine learning [5][6]. Formu-
lating the authentication problem into a matching or binary classi-
fication problem is straightforward but existing works generally
do not provide a detailed feature analysis to explain the underlying
mechanism. Currently, template matching using the Dynamic Time
Warp (DTW) algorithm [4] is considered the best candidate [2]
due to its effectiveness, simplicity, and efficiency. With the limited
amount of training data at registration, a deep neural network [20]
or Hidden Markov Model (HMM) [2] may face difficulties in train-
ing, and their performance has limitations. Because matching or
classification is done on signals with inherent fuzziness and noise,
such an authentication system is similar to a biometric recognizer,
especially an online signature verification system [12]. As a result,
performance metrics for biometrics systems are generally used in
the evaluation. Besides the motion signal itself, the hand geometry
[10][29] can be also used to build a multiple-factor authentication
system [6][14]. Fusion multiple features can be done in different
levels [22] [9]. Our proposed framework supports both score-level
fusion and feature-level fusion. Besides, we implemented several
algorithms and compared their performance with the same dataset.
We also provide an analysis of the features to explain how different
design ideas work.
3 SYSTEM MODEL
Our proposed authentication framework consists of seven compo-
nents (shown in Figure 1): a hand motion tracking device, client
software for signal preprocessing, an account database, three types
of feature extractors, and a decision-maker.
3.1 Hand Motion Capture Devices
The hand movement is captured by a Leap Motion controller [33]
and a custom-made data glove with an Inertial Measurement Unit
(IMU), shown in Figure 2 (c) and (d). The camera device has its
469Global Feature Analysis and Comparative Evaluation of Freestyle In-Air-Handwriting Passcode for User Authentication
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 2: An example of the motion signal and the two types of hand motion capture devices.
own hand tracking software, and it can provide 3D coordinates of
each joint of the hand and fingers in millimeter-level precision at
around 110 Hz. It is placed on a desk. However, we discover that
the tracking results are not always reliable, especially when the
hand moves quickly or when the index finger points to certain
directions (e.g., if the index finger is pointing upward, it will be
occluded by the palm from the perspective of the camera). Hence,
the center of the palm is tracked for robustness. The glove device
has a microelectromechanical IMU on the tip of the index finger,
which measures acceleration and angular speed at 100 Hz.
The captured in-air-handwriting signal is modeled as the six-
degree-of-freedom rigid body motion of a point in 3D space, includ-
ing position, speed, acceleration, orientation, angular speed, and
angular acceleration, in total 18 axes. With this model, both devices
generate signals of the same format, enabling us to apply the same
analysis methodology. An example of a signal is shown in Figure 2
(a), and the corresponding trajectory is shown in Figure 2 (b), which
is obtained by tracking a user writing “123456” in the air. Note that
this user deliberately writes the passcode from left to right in a
legible way, and we intentionally colored each digit for illustration
purposes. Most users write every letter or character at the same
place very fast in an illegible way, like a signature, because it is
more natural to write, and it is harder for others to mimic.
3.2 Signal Preprocessing
The following preprocessing steps are applied to a raw signal di-
rectly obtained from the devices:
(1) Trim the start and end when the hand does not move.
(2) Down-sample the signal to 50 Hz with linear interpolation.
(3) Normalize the signal reference frame. We first calculate the
average position of the signal and set it as the origin. Then,
we compute the average hand pointing direction using the
trimmed signal and set it as the x-axis. After that, for the
camera device, we use the skeleton shape of the palm to
calculate a vector pointing to the left of the palm, and set it
as the y-axis; for the glove device, we use this x-axis and the
gravity vector to set up the XOZ plane and find a vector per-
pendicular to the plane as the y-axis. Finally, we transform
each sample to this new reference frame.
(4) Low-pass filter with a threshold frequency of 10 Hz.
(5) Normalize the samples of each sensor axis to zero mean and
unit variance.
The position is directly tracked for the camera device, and the
orientation is estimated by the palm facing direction and hand
pointing direction. The orientation is represented by three Euler
angles in roll, pitch, and yaw. Higher-order states such as speed
and acceleration are derived by differentiation of the position and
orientation. The orientation is estimated from the angular speed
for the glove device, while the position and speed are estimated by
inertial dead reckoning with a regularizer. The regularizer works
like a virtual attractive force to the origin to prevent unbounded
drift caused by sensor bias and noise in integration. Although the
obtained position does not follow the actual hand movement trajec-
tory, it is still useful for matching because the same regularization
applies to all signals.
The normalization in step 3 and step 5 are crucial since they re-
move the uncertainty of the hand posture relative to the sensor and
the variation of the overall handwriting movement intensity. After
preprocessing, a hand motion signal contains a multi-dimensional
time series, denoted as a l × d matrix R in general. d is the number
of sensor axes (18 in our case). l is the signal length, i.e., the number
of samples along one axis, typically from 100 to 500. All axes have
the same number of samples synchronized in time. Given a certain
sampling time i and a specific sensor axis j, the sample value is
denoted as Rij.
3.3 Account Database and Procedures
The account database contains a collection of accounts, and each
account contains a tuple of . The ID is just a unique
number assigned by the system to each account. The templates
are generated at registration based on each feature (detailed in the
“template generation” steps in sections 4 and 5).
At registration, the user must create an account by creating a
passcode and writing the passcode in the air with five repetitions.
A collection of five signals are generated, which is denoted as the
registration signal set {R(1), R(2), ..., R(5)}.
In this paper, we focus on the authentication problem. The user
is asked to provide an account ID at login and then write a passcode
in the air for authentication, similar to a password-based system.
We assume the account ID is available, such as remembered by the
client software or recognized by the in-air-handwriting of an ID
string [20]. After finishing writing the passcode, the user keeps the
hand still for one second as a termination sign to the system, and
a signal is generated, denoted as the login request signal R. Then
a set of matching algorithms process the signal R and calculates
a distance score δ using the templates (detailed in the “matching”
steps in section 4 and section 5). After that, the authentication
system makes a decision based on the score δ and a threshold
470ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Duo Lu, Yuli Deng, and Dijiang Huang
Figure 3: Examples of warping paths, signal alignment, and alignment analysis results. For subplot (c), (d), (e), the first row
is obtained using the data from the camera device, and the second row is obtained using the data from the glove device. The
same rule applies to other figures with two rows with similar content. Subplot (f) is obtained using the camera device.
(typically configured by an administrator). If the score δ is greater
than the threshold, output “reject”; otherwise, output “accept”.
To facilitate analysis, given a specific account A and a login
request signal R, we define the class label c of R in three cases:
1) c = same: both the request signal R and the templates are
generated by the owner of account A writing the same content. For
example, R may be obtained through legitimate user login.
2) c = collision: R is generated by a different user other than the
owner of account A but both R and the templates of account A
are generated by writing the same content. For example, R may