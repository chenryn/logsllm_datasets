1† We assume that the memory peripheral circuits and cell array account for 10% and 90% of the total 
die area and PCM is 4X as dense as DRAM [2]. For the hybrid memory consisting of 2GB MLC-PCM 
and 64MB DRAM, there are 8G (2GB× 8bits per byte / 2 bits per cell = 8G) MLCs and 512M DRAM 
cells.  Assuming  each  MLC  occupies  an  area  of  A,  the  total  area  of  the  hybrid  memory  is  9×109A 
(8G×A/0.9  +  512M×4A/0.9  =  9×109A).  The  area  required  by  8MB  DRAM  is  64M ×   4A/0.9  = 
0.3×109A. As a result, the area overhead is about 3% (0.3×109A / 9×109A=3.3%). 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:46 UTC from IEEE Xplore.  Restrictions apply. 
202Instead of operating in the MLC mode for all page frames in the 
PCM  storage,  we  selectively  switch  the  mode  of  page  frames 
from the MLC mode to the SLC mode to trade the capacity for an 
improved  read  reliability.  Note  that  there  are  both  SLC-  and 
MLC- memory regions in the PCM storage and consequently the 
aggregated capacity varies, depending on the size of each region. 
To  minimize  the  performance  degradation  due  to  the  reduced 
capacity  of 
the  reference 
characteristics to it and invoke the mode switching only when the 
reference shows a drift-sensitive behavior.  
the  PCM  storage,  we  monitor 
the  entire 
initialization  sequence  of 
A multi-level PCM cell stores multiple states by exploiting 
additional  states  provided  through  partially  amophizing  phase 
change material. By only programming MLCs to fully amorphous 
(e.g.  “00”)  and  fully  crystalline  states  (e.g.  “11”),  a  multi-level 
cell  can  act  as  a  single-level  cell  and  a  binary  “0”  and  “1”  is 
represented  by  state  “00”  and  “11”  respectively.  To  support  the 
hybrid SLC/MLC design, a slight modification in PCM peripheral 
circuitry is required. For the write driver functional block, when 
the  target  memory  region  operates  in  the  SLC  mode,  only  the 
partial  or 
the  MLC 
programming mechanism is performed and the subsequent SCU 
sequence  is  disabled.  This  is  because  the  PCM  cells  can  be 
programmed  to  either  the  highest-resistance  state  or  the  lowest-
resistance states when only the initialization sequence is applied. 
During the read operations, data stored in PCM cells are detected 
by  sense  amplifiers  as  if  all  cells  operate  in  the  MLC  mode. 
Doing  so  eliminates  the  hardware  overhead  for  modifying  the 
sense amplifier circuitry. If the cells operate in the SLC mode, the 
actual data is extracted by discarding every other bit in the data. 
Otherwise, the whole data is returned without modification. Note 
that although the bandwidth of accessing SLC-pages is as half as 
that  of  accessing  MLC-pages, 
is 
negligible  because  of  twofold:  1)  the  technique  of  returning  the 
critical data first is widely adopted in memory design; and 2) we 
observe that only a small fraction of total pages in PCM operate 
in  SLC  mode.  Once  a  page  is  converted  to  SLC  mode, 
inversion/rotation  is  disabled  for  memory  updates  on  SLC  page 
frames,  since  they  are  completely  immune  to  resistance  drift. 
Nevertheless  inversion/rotation  is  enabled  before  the  conversion 
takes place, since a period of time is required for collecting the 
run-time memory reference characteristics before identifying the 
pages that need to be converted to SLC mode.  
the  performance 
impact 
To keep tracking the mode of page frames in PCM storage, 
we  use  a  single  bit,  SLC_MLC_MODE,  for  each  page  as  an 
indicator of its operating mode (e.g. SLC_MLC_MODE=1 for the 
SLC mode, SLC_MLC_MODE=0 for the MLC mode). The bit is 
stored  in  the  page  table  entry  and  managed  by  the  OS.  Upon  a 
memory  access  request  is  sent  to  PCM  storage,  the  bit  is 
forwarded  to  the  memory  controller  to  determine  the  proper 
operations  required  to  perform  read/write  in  either  the  MLC  or 
SLC  mode.  When  a  page  frame  is  first  allocated  by  the  OS  in 
PCM storage, its memory storage mode is set to the MLC mode 
by  default  to  maximize  the  memory  capacity.  To  identify  the 
pages in PCM storage that can benefit from mode switching, we 
associate 
and 
Avg_read_to_write_latency, with each page to track the average 
read-to-write  latency  of  the  page.  The  Last_write_cycle  is 
updated  upon  a  write  reference  on  a  page  and  it  records  the 
timestamp  of  the  most  recent  write.  For  each  read  reference, 
Avg_read_to_write_latency  is  updated  to  record  the  average 
number of cycles elapsed between the current timestamp and the 
Last_write_cycle 
counters, 
two 
threshold, 
the  memory  reference  behavior  of 
timestamp  of  the  most  recent  write  stored  in  Last_write_cycle. 
When  the  Avg_read_to_write_latency  is  greater  than  a  pre-
defined 
the 
associated page is classified to be drift-sensitive. In this study, we 
choose  a  threshold  of  1E12  cycles,  which  is  found  to  be  most 
effective  by  our  simulations.  A  mode  switch  is  initiated  by 
migrating the page to a new physical location in the PCM storage, 
where two physical memory pages (i.e. 32K MLCs) operating in 
the  SLC  mode  are  created.  Page  migrations  are  performed 
transparently  to  the  program  with  the  aid  of  the  OS,  which  is 
responsible for maintaining TLB coherence, copying the page to 
its target address (we emulate this migration in our simulation by 
invoking a bcopy() routine), flushing the cache lines belonging to 
the pages to be migrated. For all Read_to_write_latency counters, 
they are halved periodically by shifting right one bit. As a context 
switch takes place every 10-200ms in a normal Linux OS, we use 
10ms  as  our  interval  to  make  our  scheme  aware  of  the  altered 
memory  reference  behavior  due  to  running  different  programs 
caused by a context switch. In addition to page allocation, we also 
augment  the  page  replacement  policy  by  using  a  second-chance 
algorithm  to  minimize  the  capacity  loss  due  to  the  proposed 
hybrid  design  when  there  aren’t  plenty  of  free  memory  frames 
available.  More  specifically,  we  use  the  default  OS  page 
reclaiming algorithm to select the top ten pages that are ready to 
be replaced. The page operating in the SLC mode is chosen to be 
freed  or  written  back  to  disk  if  it  is  dirty.  To  deal  with  the 
addressing issue in the variable PCM storage capacity, the entire 
physical memory is addressed as if it is MLC-only PCM. During 
the  address  translation,  a  virtual  address  is  first  translated  to  a 
physical  address  as  if  the  memory  is  MLC-only.  Then  the  page 
offset bits of the physical address are left-shifted by one bit, if the 
SLC_MLC_MODE is set, to obtain the correct physical address.  
Converting  a  page  frame  from  the  MLC  to  SLC  mode 
improves  reliability,  but  at  the  cost  of  reduced  capacity. 
Nevertheless,  the  lower  error  rate  of  the  SLC  mode  may  be 
capable  of  mitigating  memory  latency  by  reducing  the  delay 
caused  by  correcting  errors,  thereby  offsetting  the  performance 
degradation  due  to  smaller  capacity.  The  primary  hardware 
overhead for implementing the hybrid SLC/MLC design is the die 
area required for two counters associated with each page frame. 
We  estimated  that  the  area  overhead  is  about  5%2†  of  total 
memory  die  area,  assuming  DRAM-based  storage  is  used.  The 
access latency and power consumption (per access) are estimated 
to be 35ns and 120mW respectively. All of these overheads have 
been taken into account in our evaluations.  
C. 
Temperature Aware Page Allocation 
 Temperature has been shown to have a significant impact on 
resistance  drift  [13].  Upon  a  page  allocation  request,  a 
conventional  OS  virtual  memory  management  scheme  allocates 
the  next  available  page(s)  without  taking  the  temperature  of  the 
physical memory location into consideration. The key idea of our 
temperature-aware  page  allocation  scheme  is  to  collect  the 
temperature distribution profile of PCM through a thermal sensor 
network and favor free pages in lower temperature regions than 
those  in  higher  temperature  regions.  Moreover,  as  pages  can 
2† As each 4KB page requires two counters, a total of 1M (2GB / 4KB per page × 2 counter per page = 
1M ) counters are needed for a memory of 2GB capacity. We assume that each counter is 128 bits and 
the  peripheral  circuits  account  for  40%  of  total  area  due  to  the  complexity  of  arithmetic  operations 
performed  on  these  counters.  Assuming  each  MLC  occupies  an  area  of  A,  PCM  is  4X  as  dense  as 
DRAM,  thereby  each  DRAM  occupying  an  area  of  4A.  As  a  results,  the  overall  area  overhead  is 
4.5×108×A ( (64bits per counter ×  4A per DRAM cell × 1M counters ) / (1-40%) = 4.5×108A), leading 
to a 5% ( 4.5×108A / 9×109A = 5%) area overhead. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:46 UTC from IEEE Xplore.  Restrictions apply. 
203operate in either SLC or MLC mode, we use page allocation to 
map drift sensitive pages (e.g. pages operating in the MLC mode) 
to  regions  that  have  low  temperature  and  place  drift  tolerant 
pages  (e.g.  pages  operating  in  the  SLC  mode)  to  regions  that 
exhibit high temperature. Note that we assume a 3D die-stacked 
on-chip memory design in this case and the same technique can 
be applied to an off-chip design. 
CORE
CORE
CORE
CORE
r
e
x
e
p
l
i
t
l
u
M
g
o
a
n
A
l
e
d
o
d
V
i
r
e
t
r
e
v
n
o
C
D
A
/
OS
Memory Management
Runtime temperature
Physical Pages
Temperature
Page #0000-8191
Page #8192-16383
350
370
7-bit temperature 
reading
Shared L2 Cache
Temperature 1
Temperature 2
Temperature 3
Temperature 90
t
p
u
r
r
e
n
t
I
Allocate MLC pages on low-
temperature regions, or SLC pages 
on high-temperature regions 
Figure 7. An overview of the thermal sensor network and  
temperature-aware page allocation 
To  obtain  the  runtime  temperature  profile  of  a  PCM  layer, 
we adopt a thermal sensor design similar to those used in the Intel 
Core  Duo  [20].  Figure  7  shows  an  overview  of  thermal  sensor 
network  architecture.  The  sensing  device  is  an  analog  thermal 
diode and the voltage across the diode exhibits strong temperature 
dependence. This sensing device occupies little area and allows a 
large  number  of  thermal  diodes  to  be  placed  across  the  die. 
Although  the  thermal  sensors  are  scattered  across  the  entire 
memory die, the sensor data processing is centralized. An analog 
multiplexer is used to probe all sensing devices and therefore the 
temperature at different parts of the die can be collected. An A/D 
circuit is used to convert the analog signal from each sensor into a 
digital  reading,  which  is  assumed  to  be  7  digits  representing  a 
temperature  range  from  0ºC  to  128ºC  in  our  study.  To  achieve 
measurement accuracy, each sensor is calibrated at test time. This 
sensor  implementation  is  capable  of  providing  accuracy  levels 
around 1ºC, which is sufficient for our thermal monitoring. Note 
that  the  temperature  is  not  evenly  distributed  across  the  entire 
memory  layer.  For  example,  the  temperature  distribution  across 
the memory regions on top of core areas can vary significantly, 
while  those  regions  atop  caches  is  likely  to  show  a  small 
variation.  To  capture  the  spatial  variation  in  temperature  and 
minimize  the  number  of  sensing  devices  required,  we  adopt  a 
non-uniform sensor placement approach. After dividing the floor 
plan of the PCM layer uniformly into grids shown in Figure 7, a 
thermal diode is placed in the center of each grid cell that is on 
top of core area, while a thermal diode is placed in the center of a 
memory region covering 4 grid cells, if the region is physically 
located on top of L2 cache. To determine the size of grid cell, we 
performed  thermal  analysis  across  all  simulated  workloads  and 
adopted a gird size of 1.5mm by 1.5mm. This grid size is able to 
ensure that the temperature error within any given cell is smaller 
than 5ºC, since the variation in the resistance drift is small when 
the temperature varies less than 5ºC [13]. Due to the small size of 
the  sensing  device,  we  deploy  a  total  of  90  on-chip  thermal 
sensors  on  the  PCM  layer.  As  temperature  doesn’t  change 
significantly over time, we assume a sampling interval of 1s.  
After  collecting  the  run-time  temperature,  the  data  is  fed 
back  to  the  OS  through  an  interrupt  mechanism  and  they  are 
consulted  by  the  OS  memory  management  module  during  page 
allocation, as is shown in Figure 7. Due to its low frequency (e.g. 
1Hz),  the  performance  degradation  caused  by  the  periodic 
interrupt  is  negligible.  A  page  allocation  request  is  generated 
when a page fault exception occurs since the OS delays allocating 
dynamic memory to user processes until the page is referenced. A 
typical technique used in Linux is the buddy memory allocation 
technique which groups free pages into multiple lists of memory 
blocks  and  the  memory  blocks  in  each  list  contain  a  variable 
number  of  contiguous  page  frames.  To  satisfy  a  memory 
allocation  request,  a  list  that  contains  large  enough  memory 
blocks  is  traversed  and  a  random  memory  block  in  this  list  is 
selected  and  returned  for  allocation.  To  augment  the  page 
allocation  scheme  with  the  capability  of  temperature  awareness, 
the temperature of the memory regions that each memory block 
belongs  to  is  obtained  by  looking  up  the  collected  runtime 
temperature. After traversing the list, the memory block that has 
the  lowest  temperature  is  returned  to  satisfy  the  memory 
allocation  request.  Beside  page  faults,  page  allocation  requests 
are also triggered due to page migrations. As described in Section 
IV.B, the proposed hybrid SLC/MLC design may initiate a mode 
switch by migrating a page operating in the MLC mode to a new 
location where memory cells will operate in the SLC mode. As 
SLC  is  insensitive  to  resistance  drift,  free  pages  in  the  high 
temperature  regions  are  favored  over  those  in  low  temperature 
regions  in  this  case.  Doing  so  provides  more  opportunity  for 
pages in MLC to be allocated in low temperature regions.  
V. EXPERIMENTAL METHODOLOGY 
In  this  Section,  we  describe  our  experimental  methodology 
for  evaluating  the  benefits  of  the  proposed  techniques.  We 
simulated  a  quad  core  system  with  a  shared  2MB  cache  and  a 
DRAM/PCM  hybrid  memory.  Each  core  is  an  out-of-order 
processor  with  parameters  listed  in  Table  1.  For  the  hybrid 
memory, the small DRAM buffer is on the processor layer, which 
is  managed  by  the  memory  controller  and  is  transparent  to  the 
OS,  while  the  MLC-PCM  storage  is  stacked  on-top  of  the 
processor layer and managed by the OS. For the MLC-PCM, we 
assume  2GHz  frequency  and  90nm  technology  with  a  supply 
voltage  of  1.6V.  Table  2  summarizes  the  timing  and  power 
characteristics  of  the  modeled  MLC-PCM.  We  evaluated  five 
MLC designs with different resistance distributions (i.e. Margin0, 
Margin0.5, Margin1.0, Margin1.5 and Margin2.0), which provide 
a readout current margin of 0uA, 0.5uA, 1.0uA, 1.5uA and 2.0uA 
respectively.  For  each  margin  size, 
the  minimum,  mean, 
maximum  resistance  values  for  each  logic  state  along  with  its 
RESET and SET current are shown in Table 2. These values are 
obtained  using  a  one-dimensional  heat  conduction  model  [16] 
that calculates the minimum programming current required for a 
successful  write  operation.  To  evaluate  our  techniques,  we  ran 
applications  from  SPEC  2000  and  NAS  parallel  benchmark 
suites.  These  benchmarks  are  listed  in  Table  3  along  with  the 
drift-induced error rate when they run standalone. We selected 20 
programs  from  SPEC  2000  with  reference  inputs  and  used  7 
programs from NAS Parallel Benchmarks Version 3.2 with Class 
“C”  input  data  set.  All  benchmarks  are  compiled  on  an  x86 
platform  using  GCC  or  FORTRAN  compiler  with  optimization 
level –O3. To form 4-threaded multiprogramming workloads, we 
first  categorized  all  benchmarks  into  high-error-rate  (error 
rate>3%),  moderate-error-rate  (3%<error  rate<1%)  and  low-
error-rate  (error  rate<1%)  groups.  In  Table  4,  the  High-, 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:46 UTC from IEEE Xplore.  Restrictions apply. 
204Moderate- and Low- error-rate workloads (H1-H3, M1-M3, L1-
L3) consist of four benchmarks exclusively from each category. 
All  caches  and  memories  are  warmed  up  before  entering  the 
detailed  simulation  mode  and  our  multiprogramming  workloads 
exhibit  a  high  cache  misses  on  both  L2  and  DRAM  buffer. 
Therefore,  there  is  sufficient  amount  of  access  traffic  to  MLC-
PCM  in  our  experiments.  We  applied  sampling  and  fast-
forwarding techniques in our simulations to capture the impact of 
resistance drift at a large time scale.  
For  performance  and  power  evaluation,  we  developed  a 
framework  based  on  a  heavily  extended  full-system  simulator, 
PTLSim/X  [21],  integrated  with  a  modified  DRAMSim  [22] 
memory  model,  a  resistance  drift  model  and  the  Wattch  Power 
Model [23]. PTLSim/X is a cycle accurate simulator supporting 
the  x86  instruction  set  architecture.  We  extended  PTLSim/X  to 
model a 2- level write back cache, contention for memory buses 
and  bus  traffic.  To  model  the  latency  and  energy  of  PCM,  we 
enhanced the DRAMSim timing module to model PCM-specific 
peripheral  structures  and  extended  its  power  module  for  PCM 
energy  estimation.  Table  2  lists  the  modeled  PCM  timing  and 
power parameters. To capture the behavior of resistance change, 
we  used  the  drift  model  (detailed  in  Section  III.A),  which 
estimates  the  post-drift  resistance  values  based  on  memory 
reference patterns and run-time on-chip temperature, to calculate 
the  readout  error  rate.  We  assume  that  an  advanced  error 
TABLE 1. BASELINE MACHINE CONFIGURATION 
Parameter 
Frequency  
Width  
IQ 
ITLB 
Branch Pred. 
BTB 
RAS 
L1 I-Cache 
ROB  
LDQ 
STQ 
Int. ALU 
FP ALU 
DTLB 
L1 D-Cache 
L2 Cache 
Hybrid Memory 
Write Buffer 
Configuration 
2GHz 
4-wide fetch/decode/issue/commit 
64 entries  
128 entries, 4-way 
2K entries Gshare, 10-bit global history   
2K entries, 4-way 
32 entries RAS  
64KB, 4-way, 64 Byte/line, 2 ports, 3 cycle  
128 entries 
48 entries  
32 entries 
4 I-ALU, 2 I-MUL/DIV, 1 Load/Store 
2 FP-ALU, 2 FP-MUL/DIV/SQRT 
256 entries, 4-way 
64KB, 4-way, 64 Byte/line, 2 ports, 3 cycle  
Shared 2MB, 8-way, 64 Byte/line, 12 cycle  
DRAM buffer (64MB), MLC-PCM(2GB 
effective capacity, 8 banks)   
32 entries, 64B per entry 
TABLE 3. BENCHMARKS USED TO FORM WORKLOADS 
Benchmark 
Error 
Rate 
20.17%  UA 
11.21%  mcf 
gap 
8.64% 
applu 
7.96% 
7.88%  mgrid 
3.85% 
3.14%  Crafty 
3.08%  BT 
3.05% 
perlbmk 
facerece 
Error 
Rate 
2.97%  EP 
1.78%  mesa 
1.91%  FT 
1.61%  MG 
vpr 
1.53% 
1.42%  CG 
1.27%  LU 
1.07% 
1.06% 
Benchmark  Error 
Rate 
0.95% 
0.92% 
0.91% 
0.03% 
0.02% 
0.02% 
0.02% 
0.01% 
0.01% 
fma3d 
galgel 
Benchmark 
gzip 
bzip 
art 
swim 
equake 
ammp 
lucas 
sixtrack 
vortex 
correction  code  (ECC),  Bose-Chaudhuri-Hocquenghem  (BCH) 
code  [24],  is  employed  in  our  MLC-PCM  to  investigate  the 
impact  of  readout  error  rate  on  performance.  This  BCH  code  is 
assumed to be able to correct an arbitrary number of errors with 
an extra 2ns latency to correct each additional error. Nevertheless, 
correcting a larger number of errors demands more check bits to 
be employed at the design stage of ECC, leading to an increased 