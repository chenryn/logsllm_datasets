gaze metrics during phishing detection and malware warnings
tasks in a near-realistic environment.
Relevant
to our study is the fNIRS study of phishing
detection by Neupane et al. [33]. They used fNIRS to measure
neural activities when users were identifying real and phishing
websites, and built neural cues based machine learning models
to predict real and phishing website. Unlike this study, we use
the fNIRS system to measure the neural behavior in a different
application paradigm – voice security.
There are some neuroscience studies on voice recognition
(e.g., [3], [6], [7], [20]). Belin et al. [6] studied the brain
areas involved in voice recognition. Formisano et al. [20] in
their fMRI study showed the feasibility of decoding speech
content and speaker identity from observation of auditory
cortical activation patterns of the listener. Similar to our study,
Bethman et al. [7] studied how human brain processes voices
of a famous familiar speaker and an unfamiliar speaker. In
contrast
these studies, our research focuses on the
security aspect of voice impersonation attack and explains why
users fail to identify fake voices, and evaluates the privacy
issues related to the BCI devices.
to all
C. The Premise of our Hypothesis
Prior researchers [25], [32]–[34] have also conducted stud-
ies to understand why people fall for fake artifacts (e.g., fake
images, fake websites) analyzing their neural signals. Huang et
al. [25] had conducted a study to understand how human brain
reacts when they are asked to identify if a given Rembrandt
painting was a real or a fake. They reported differences in
neural activation at left and right visual areas when users were
viewing real and fake Rembrandt paintings (see Figure 2).
Neupane et al. had also conducted studies [32]–[34] to analyze
the neural activations measured using different neuroimaging
devices (e.g., EEG [32], fNIRS [33] and fMRI [34]) when
users were subjected to phishing attacks. The users were asked
to identify if a given website was real or fake and their
neural signals were concurrently measured when they were
viewing these websites. They reported differences in the brain
activations when users were processing real and fake websites.
3
detection task. We set-up our study to test the hypothesis that
the brain areas related to the real-fake decision making should
be activated differently when users are listening to the original
and fake voices of a speaker as well. We, therefore, set-up
the fNIRS headset conﬁguration to measure the frontal and
temporo-parietal brain regions reported in these previous “real-
fake detection” studies. Next, we also try to automatically infer
the type of voice (real or morphed voice) a user is listening to
based on the fNIRS-measured neural signals.
D. Voice Synthesis
Voice synthesis is commonly used in text-to-speech sys-
tems. The quality of the synthesized voice is judged by its
intelligibility and its similarity to a targeted human voice. Tra-
ditionally, it is known to be challenging for a voice synthesizer
to produce a natural human speech (without noticeable noise)
artiﬁcially and make it indistinguishable from the human voice
[18]. Additionally, voice synthesizers require a huge amount of
data from a target speaker to learn the phonemes and generate
a synthesized speech of the target speaker.
However, newer techniques have emerged that may do a
much better job of synthesizing a voice. Voice morphing (also
referred to as voice conversion and voice transformation) is one
such technique to generate a more naturalistic (human-type)
voices with fewer samples. Voice morphing software takes
some samples of a speech spoken by a source speaker and
produces a sound as if it was spoken by the target speaker by
mapping between the spectral features of the source speaker’s
and the target speaker’s voice [42]. Previous research [30],
[38] has reported via behavioral studies that the morphed voice
attack may be successful against users with high probability.
In our study, we followed a methodology described in [30] and
used the CMU Festvox voice converter [19], an academic off-
the-shelf voice morphing tool, to generate the morphed voices
of the target speakers.
E. Threat Model
In our work, we study how access to a few voice samples
of a speaker can be used to launch attacks on human-based
speaker veriﬁcation systems. We assume that an attacker has
collected a few voice samples previously spoken by the target
victim with or without her consent. The attacker can get such
voice samples from any public speeches made by the victim
or by stealthily following the victim and recording audio as
he speaks with other people. The attacker then uses the voice
samples to train a model of a morphing engine (we followed
the procedures mentioned in [30] for morphing engine), such
that the model creates the victim’s voice for any (new) arbitrary
speech spoken by the attacker. This morphed speech can now
be used to attack human-based speaker veriﬁcation systems.
The attackers can either make a fake phone call, or leave voice
messages, or produce a fake video with the victim’s voice in
it and post it online.
F. Terminology and Attack Description
Victim Speakers: These are the speakers whose voices were
manipulated to launch voice impersonation attacks on partici-
pants in our study.
Fig. 1. Activation in right middle frontal gyri (RMFG), right inferior frotal
gyri (RIFG), left inferior parietal lobule (LIPL), left precentral gyrus, right
cerebellum, and left cingulate gyrus is dependent on whether or not the
participant was viewing an image of a genuine website (real vs. fake) [34].
TABLE II.
BRAIN AREAS ACTIVATED IN PHISHING TASK [34] AND
THEIR CORRESPONDING FUNCTIONS
Brain Areas
Orbitofrontal area
Middle frontal, inferior
frontal,
and
inferior
parietal areas
Right cerebellum and
left precentral gyrus
Occipital cortex
Functions
Decision-making; judgment
Working memory
Feedforward and feedback projections
Visual processing, search
Speciﬁcally, Neupane et al. [34] revealed statistically sig-
niﬁcant activity in several areas of the brain that are critical and
speciﬁc to making “real” or “fake” judgments. For the websites
those the participants identiﬁed as fake (contrasted with real),
participants activated right middle, inferior, and orbital frontal
gyri, and left inferior parietal lobule. The functions governed
by these areas are listed in Table II. On the other hand, when
real websites were identiﬁed, participants showed increased
activity in several regions, such as the left precentral gyrus,
right cerebellum, and the occipital cortex (see Figure 1).
Neupane et al. [33] also explored a feasibility of fake website
detection system based on fNIRS-measured neural cues, where
they were able to obtain the best area under the curve of 76%.
Fig. 2. Activation in the visual areas, calcarine sulcus (CS), is dependent on
whether or not the participant was viewing an image of a genuine Rembrandt
(real vs. fake) [25].
Similar to the tasks of identifying real and fake websites
or images, the speaker legitimacy detection task also involves
real-fake decision making and hence we hypothesized that
similar areas should be activated in the speaker legitimacy
4
Familiar Speakers: In our study, we used the voice samples
of Oprah Winfrey and Morgan Freeman to represent the set of
familiar speakers. The reason for choosing these celebrities
in our case study is to leverage their distinct and unique
voice texture and people’s pre-existing familiarity with their
voices. The participants were also asked to answer if they
were familiar with the voice of these celebrities as “a yes/no
question” before their participation.
Brieﬂy Familiar Speakers: In our study, these are the speakers
whom the participants did not know before the study and were
only familiarized during the experimental task only to establish
brief familiarity. The brieﬂy familiar speakers represent a set
of people with whom the users have previously interacted only
for a short-term (e.g., a brief conversation at a conference).
Different Speaker Attack: Different speakers are the arbitrary
people who attempt to use their own voice to impersonate as
the victim speaker. In our study, using a different speaker’s
voice, replacing the voice of a legitimate speaker to fool the
participants, is referred to as the different speaker attack.
Morphed Voice Attack: Attackers can create spoofed voice
using speech morphing techniques to impersonate the voice of
a victim user, referred to as a morphed voice. In our study, the
use of such a voice to deceive other users to extract their private
information is therefore called the morphed voice attack.
Speaker Legitimacy Detection: In our study, this represents the
act of identifying whether the given voice sample belongs to
the original speaker or is generated by a morphed engine. The
different speaker attack is used as a baseline for the morphing
attack, since it is expected that people might be able to detect
the different speaker attack well.
III. STUDY DESIGN & DATA COLLECTION
In this section, we present the design of our experimental
task, the set-up involving fNIRS, and the protocol we followed
for data collection with human participants.
A. Ethical and Safety Considerations
Our study was approved by our university’s institutional
review board. We ensured the participation in the study was
strictly voluntary and the participants were informed of the
option to withdraw from the study at any point in time. We
obtained an informed consent from the participants and made
sure they were comfortable during the experiment. We also fol-
lowed the standard best practices to protect the conﬁdentiality
and privacy of participant’s data (task responses and fNIRS
data).
B. Design of the Voice Recognition Task
The study design for our voice recognition task followed
the one employed in recent task performance study of speaker
veriﬁcation [30]. However, unlike [30], our study captured
neural signals in addition to the task performance data. We
designed our experiment to test the following hypothesis:
Hypothesis 1 The activation in frontopolar and temporopari-
etal areas, which covers most of the regions activated in previ-
ous studies (see Section II-C), will be high when participants
are listening to the morphed voice of a speaker compared to
the original voice of the speaker.
To test this hypothesis, we used original, morphed, and dif-
ferent voices for two types of speakers – familiar speakers and
brieﬂy-familiar speakers(see Section II-F). The participants
were asked regarding their familiarity with these speakers’
voices as “a yes/no question” before the experiment. During
the experiment, the participants were asked to identify the
real (legitimate) and fake (illegitimate) voice of a speaker. We
assumed the real voices may impose more trust compared to
the fake voices. The failure of users in detecting such attacks
would demonstrate a vulnerability of numerous real-world
scenarios that rely (implicitly) on human speaker veriﬁcation.
Stimuli Creation: Voice Conversion: We followed the
methodology similar to the one reported in [30] to create our
dataset. For familiar speakers, we collected the voice samples
of two popular celebrities, Oprah Winfrey and Morgan Free-
man, available from the Internet. For unfamiliar speakers, we
recruited participants via Amazon Mechanical Turk (MTurk).
We asked twenty American speakers in MTurk to record the
speech of these two celebrities in their own voices. They were
asked to read the same sentences the celebrities had spoken in
the recorded audio and also to imitate the celebrities’ speaking
style, pace and emotion. We fed these original voice samples of
the celebrities and the recorded voice samples from one male
and one female speaker among these twenty speakers to the
CMU Festvox voice converter [19] to generate the morphed
voices of Morgan Freeman and Oprah Winfrey.
In line with the terminology used in [30], in our study, the
original recording of a victim speaker’s voice is referred to
as a “real/original voice”. For each speaker, the fake voices
created using speech morphing techniques is referred as a
“morphed voice”, and recorded speech in other speaker’s
voices is referred as a “different speaker”.
Experiment Design: We used an event-related (ER) design for
our study. In ER design, each trial is presented as an event with
longer inter-trial-interval and we can isolate fNIRS response to
each item separately [37]. We familiarized the participants with
the voice a of victim speaker for 1-minute. We could not let the
participants replay the original audio during familiarization as
it would have made the experiment longer. As the experiment
gets longer, fatigue effects may set in, eventually affecting the
quality of the brain signals recorded.. After familiarization, we
presented 12 randomly selected speech samples (4 in original
speaker’s voice voice, 4 in morphed voice and 4 in different
speaker’s voice and asked participants to identify legitimate
and fake voices of the victim speakers. The process was
repeated for four victim speakers (2 familiar speakers, and
2 brieﬂy familiar speakers) randomly. Following [30], our
study participants were not informed about voice morphing
to prevent explicit priming in the security task. In real life
also, people may have to decide a speaker’s legitimacy without
knowing the voices may have been morphed.
The experiment started with the Firefox browser loading
the instructions page (specifying the tasks the participants are
supposed to perform) for 30 seconds. This was followed by
a rest page of 14 sec (+ sign shown at the center of a blank
page) during which participants were asked to relax. We next
5
Fig. 3. The ﬂow diagram depicts the presentation of trials in the experiment. The participants were familiarized with a speaker, and were asked to recognize
the short voice samples presented next as real or fake.
played a speech sample of a victim speaker for 60 sec, and
then showed a rest page for 8 sec. This was followed by
12 trials, each 20 sec long. Each trial consisted of a voice
(corresponding to a fake/real voice of the speaker) played for
15 sec, followed by a 5 sec long response page. The response
page had a dialog box with the question, “Do you think the
voice is of the original speaker?” and the “Yes” and “No”
buttons. The participants answered the question using mouse
input. A rest page of 8 sec was loaded after each trial. Rest
trials are considered as windows of baseline brain activity. The
process was repeated for four speakers and the experiment
ended with the goodbye note of 5 sec. The system recorded the
participant’s neural data, responses and response time. Figure
3 depicts the ﬂow diagram.
C. Study Protocol
Our study followed a within-subject design, whereby all
participants performed the same set of (randomized) trials
corresponding to the voice recognition task.
Recruitment and Preparation Phase: We recruited twenty
healthy participants from the broader university community
(including students and staff) by distributing the study ad-
vertisements across our university’s campus. We asked the
participants about their familiarity with Morgan Freeman’s and
Oprah Winfrey’s voices, i.e., if the participants have heard the
celebrities’ voices before and could recognize those voices,
along with participants’ age, gender, and educational back-
ground in the pre-test questionnaire. Of the 20 participants,
10 were male and 10 were female. All were English speaking
participants in the age range of 19-36 years with a mean age of
24.5 years. Table III summarizes the demographic information
of our participants.
Previous power analysis studies have found 20 to be an
optimal number of participants for such studies. For instance,
statistical power analysis of ER-design fMRI studies have
demonstrated that 80% of clusters of activation proved repro-
ducible with a sample size of 20 subjects [31]. Both fMRI
and fNIRS are based on hemodynamic response of the BOLD
principle, so we assumed similar power analysis for fMRI
and fNIRS. Also previous fNIRS studies have shown that
fNIRS measurements are reliable with 12 participants [35].
TABLE III.
DEMOGRAPHICS
Participant Size N = 20
Gender(%)
Male
Female
Age(%)
18-22
22-26
27-31
31+
Handedness(%)
Right-Handed
Left-Handed
Background(%)
High School
Bachelor’s
Masters
Doctorate
Others
50%
50%
20%
55%
20%
5%
90%
10%
10%