uneven object will fail it; second, the method does not consider
variation in illumination. Kollreider, et al., [13] developed a
method for detecting the positions and velocities of facial
components by using model-based Gabor feature classiﬁcation
and optical ﬂow pattern matching. However, its performance
was impaired when keen edges were present on the face (e.g.,
spectacles or a beard). The authors admitted that the system is
only error-free if the data contain only horizontal movements.
Findling et al. [8] achieved liveness detection by combining
camera images and movement sensor data. They captured
multiple views of a face while a smartphone was moved.
Li et al. [17] measured the consistency in device movement
detected using inertial sensors and changes in the perspective
of a head captured on video. However, both methods were
demonstrated to be compromised by Xu et al. [27], who
constructed virtual models of victims on the basis of publicly
available photographs of them.
Less adversaries have attempted to launch 3D dynamic
attacks. They can reconstruct a 3D model of a victim’s
AttacksStatic2D Static3D StaticDynamic2D Dynamic3D Dynamicface [27] in virtual settings but hardly fabricate them in real
scenes. We illustrate the difﬁculties in launching 3D dynamic
attacks using the following three examples: First, building a
ﬂexible screen that can be molded into the shape of a face is
expensive and may fail because the reﬂectance from a screen
differs from that of a face. Second, 3D printing a soft mask is
impractical, being limited by the printing materials available
(see Section V-D for a fuller explanation). Third, building
an android is infeasible and intricate and would involve face
animation, precision control, and skin fabrication. Additionally,
building an android is costly, particularly a delicate android
face.
On the basis of the above discussion, we observe that the
current threats are principally 2D dynamic attacks because
static attacks have been effectively neutralized and 3D dynamic
attacks are hard to launch.
III. ADVERSARY MODEL
In this section, we present our proposed adversary model
and assumptions.
We assume adversaries’ goal is to bypass the face authen-
tication systems by impersonating victims, and the objective
of our proposed methods is to raise the bar for such successful
attacks signiﬁcantly. As will be demonstrated in the limitation
part (section IX), powerful adversaries could bypass our secu-
rity system, but the cost would be much higher than is currently
the case. Particularly, they need to purchase or build special
devices that can do all of the following operations within the
period when the camera scanning a single row: (1) capture
and recognize the randomized challenges, (2) forge responses
depending on the random challenges, and (3) present the forged
responses. For this, adversaries require high-speed cameras,
powerful computers, high-speed I/Os, and a specialized screen
with fast refreshing rate, etc. Therefore, it is difﬁcult to attack
our system.
Adversaries can also launch 3D dynamic attacks, such as
undergoing cosmetic surgery, disguising their faces, or coerc-
ing a victims twin into assisting them. However, launching a
successful 3D dynamic attack is much more difﬁcult than using
existing methods of MFF attack; crucially, identifying such
an attack would be challenging even for humans and would
constitute a Turing Test problem, which is beyond the scope of
this paper. But in either case, our original goal is achieved by
having increased the bar for successful attacks signiﬁcantly.
Our method relies on the integrity of front-end devices; that
is, that the camera and the hosting system that presents random
challenges and captures responses have not been compromised.
If this cannot be guaranteed, adversaries could learn the
algorithm used to generate random challenges and generate
fake but correct responses beforehand, thus undermining our
system. We believe that assuming the integrity of front-end
devices is reasonable in real-world settings, considering that in
many places the front-end devices can be effectively protected
and their integrity guaranteed (e.g., ATMs and door access
controls). We cannot assume or rely on the integrity of smart-
phones, however. Our proposed techniques are general and can
easily be deployed on different hardware platforms, including
but not limited to smartphones. For simplicity, we choose to
build a prototype and conduct evaluations on smartphones, but
Fig. 3: Architecture of Face Flashing.
this is only for demonstration purposes. If the integrity of a
smartphone can be guaranteed, by using a trusted platform
module or Samsung KNOX hardware assistance, for example,
our techniques can be deployed on them; otherwise this
should be avoided and the proposed techniques are not tied
to smartphones.
IV. FACE FLASHING
Face Flashing is a challenge-response protocol designed
for liveness detection. In this section, we elaborate its detailed
processes and key techniques to leverage ﬂashing and reﬂec-
tion.
A. Protocol Processes
The proposed protocol contains seven components, which
are illustrated in Fig 3, and eight steps are required to complete
once challenge-response procedure where the challenge is
ﬂashing light and the response is the light reﬂected from the
face.
•
Step 1: Generation of parameters. Parameters are
produced by the Generator of the Liveness Detection
Module running on the back-end server, which works
closely with the front-end devices. Such parameters
include seed and N. Seed controls how random chal-
lenges are generated, and N determines the total
number of challenges to be used. Communication of
parameters between the back-end server and front-
end devices is protected by standard protocols such
as HTTP and TLS.
Step 2: Initialization of front-end devices. After
receiving the required parameters, the front-end de-
vices initialize their internal data structures and start
to capture videos from the subject being authenticated
by turning on the Camera.
Step 3: Presentation of a challenge. Once initialized,
the front-end devices begin to generate challenges
according to the received parameters. Essentially, a
challenge is a picture displayed on a screen during
one refresh period; light is emitted by the screen onto
the subjects face. The challenge can be of two types:
•
•
4
ScreenGeneratorCameraTime VerifierFace ExtractorFace VerifierInitializeVideoFaceDataParametersFlashReflectionLiveness Detection ModuleFront-end DevicesParametersExpressionDetectorFace•
•
•
•
•
a background challenge, which displays a pure color,
and a lighting challenge, which displays a lit area over
the background color. More details are speciﬁed in
subsequent sections.
Step 4: Collection of response. The response is the
light that is reﬂected immediately by the subjects face.
We collect the response through the camera that has
already been activated (in Step 2).
Step 5: Repetition of challenge-response. Our pro-
tocol repeats Step 3 and 4 for N times. This repetition
is designed to collect enough responses to ensure
high robustness and security so that legitimate users
always pass whereas adversaries are blocked even
if they accidentally guess out the correct challenges
beforehand.
Step 6: Timing veriﬁcation. Timing is the most
crucial security guarantee provided by our protocol
and is the fundamental distinction between genuine
and fake responses. Genuine responses are the light
reﬂected from a human face and are generated through
a physical process that occurs simultaneously over all
points and at the speed of light (i.e., zero delay).
Counterfeit responses, however, would be calculated
and presented sequentially, pixel by pixel (or row by
row), through one or more pipelines. Thus, counterfeit
responses would result in detectable delays. We detect
delays among all the responses to verify their integrity.
Step 7: Face veriﬁcation. The legality of the face
is veriﬁed by leveraging neural network that incor-
porates with both the shape and textual characters
extracted from the face. This veriﬁcation is necessary
because without it our protocol is insufﬁciently strong
to prevent from MFF attacks, and face veriﬁcation
prolongs the time required by adversaries to forge
a response, which makes the difference from benign
response more obvious. The details are provided in
Section IV-B.
Step 8: Expression veriﬁcation. The ability to make
expressions indicates liveness. We verify this ability
by ascertaining whether the detected expression is the
one requested. Speciﬁcally, technology from [22] is
embedded in our prototype for detecting and recog-
nizing human expressions.
Details of Step 8 are omitted in this paper so that we
can focus on our two crucial steps: timing and face veriﬁ-
cations. However, expression detection has been satisfactorily
developed and is critical to our focus. Additionally, Step 8
is indispensable because it integrates our security boundary,
which is elucidated in Section V. The face extraction detailed
in the next section is designed so that our two veriﬁcation
techniques are compatible with this expression detection.
B. Key Techniques
The security guarantees of our proposed protocol are built
on the timing as well as the unique features extracted from
the reﬂected lights. In the followings, we will ﬁrst introduce
the model of light reﬂection, then our algorithm for extracting
faces from video frames, and veriﬁcations on time and face.
1) Model of Light Reﬂection: Consider an image Irgb =
{Ir, Ig, Ib} that is taken from a linear RGB color camera with
black level corrected and saturated pixels removed. The value
of Ic, c ∈ {r, g, b} for a Lambertian surface at pixel position x
is equal to the integral of the product of the illuminant spectral
power distribution E(x, λ), the reﬂectance R(x, λ) and the
sensor response function Sc(λ):
(cid:90)
Ic(x) =
E(x, λ)R(x, λ)Sc(λ)dλ, c ∈ {r, g, b}
Ω
where λ is the wavelength, and Ω is the wavelength range of
all visible spectrum supported by camera sensor. From the Von
Kries coefﬁcient law [3], a simpliﬁed diagonal model is given
by:
Ic = Ec × Rc, c ∈ {r, g, b}
Exploiting this model, by controlling the outside illuminant
E, we can get the reﬂectance of the object. Speciﬁcally, when
Ec for x and y are the same, then
Ic(x)
Ic(y)
=
Rc(x)
Rc(y)
, c ∈ {r, g, b}
(1)
This means the lights captured by camera sensor at
two
different pixels x and y are proportional to the reﬂectance of
that two pixels.
Similarly, for the same pixel point x,
different illuminant lights Ec1 and Ec2, then:
if applying two
Ic1(x)
Ic2(x)
=
Ec1(x)
Ec2(x)
, c1, c2 ∈ {r, g, b}
(2)
In other words, the reﬂected light captured by the camera in a
certain pixel is proportional to the incoming light of the same
pixel.
Implications of above equations. Eq.(1) and Eq.(2) are
simple but powerful. They are the foundations of our live-
ness detection protocols. Eq.(1) allows us to derive relative
reﬂectance for two different pixels from the proportion of
captured light from these two pixels. The reﬂectance is deter-
mined by the characteristics of the human face, including its
texture and 3D shape. Leveraging Eq.(1), we can extract these
characteristics from the captured pixels and further feed them
to a neural network to determine how similar the subject’s face
is to a real human face.
Eq.(2) states that for a given position, when the incoming
light changes,
the reﬂected light captured by the camera
changes proportionally, and crucially, such changes can be
regarded as “simultaneously” to the emission of the incoming
light because light reﬂection occurs at the speed of light.
Leveraging Eq.(2), we can infer the challenge from the current
received response and detect whether a delay occurs between
the response and the challenge.
2) Face Extraction: To do our veriﬁcations, we need to
locate the face and extract it. Furthermore, our veriﬁcations
must be performed on regularized faces where pixels in
different frames with the same coordinate represent the same
point on the face. Concretely, when a user’s face is performing
expressions as instructed, it produces head movements and
hand tremors. Thus, using only face detection technology is
insufﬁcient; we must also employ a face alignment algorithm
5
that ascertains the location of every landmark on the face and
neutralizes the impacts from movements. Using the alignment
results, we can regularize the frames as we desired, and
the regularized frames also ensure that our veriﬁcations are
compatible with the expression detector.
First, We designed Algorithm 1 to quickly extract
the
face rectangle from every frame. In Algorithm 1, track(·)
is our face tracking algorithm [7]. It uses the current frame
as the input and employs previously stored frames and face
rectangles to estimate the location of the face rectangle in the
current frame. The algorithm outputs the estimated rectangle
and a conﬁdence degree, ρ. When it is small (ρ < 0.6), we
regard the estimated rectangle as unreliable and subsequently
use detect(·), our face detection algorithm [28], to redetect
the face and ascertain its location. We employ this iterative
process because the face detection algorithm is precise but
slow, whereas the face tracking algorithm is fast but may lose
track of the face. Additionally, the face tracking algorithm is
used to obtain the transformation relationship between faces in
adjacent frames, which facilitates our evaluation of robustness
(Sec VI-D).
Algorithm 1 Algorithm to extract the face.
INPUT: V ideo
OUTPUT: {Fj}
1: for f rame in V ideo do
2:
3:
4:
5:
6:
7:
8: end for
Rect, ρ = track(f rame)
if Rect = ∅ or ρ < 0.6 then
Rect = detect(f rame)
Rect → track(·)
end if
Fj = f rame(Rect)
After obtaining face rectangles, {Fj}, we exploit face
alignment algorithm to estimate the location of 106 facial
landmarks [29] on every rectangle. The locations of these
landmarks are shown in Figure 4.
Fig. 4: 106 landmarks.
(cid:80)
= argminT ||T ˜Lj − Lmean||2
j Lj(cid:80)
(cid:34)x1 x2
(cid:35)
j 1
···
···
···
x106
y106
1
=
y1
1
y2
1
where Lmean =
Tj
˜Lj
where T is a 3 × 3 matrix contains rotation and shifting
coefﬁcients. We select the best T as Tj that minimizes the
L2 distance between the regularization target Tmean and ˜Lj,
the homogeneous matrix of the coordinate matrix. After that,
we regularize the j-th frame by applying the transformation
matrix Tj to every pair of coordinates and extract the cen-
tering 1280x720 rectangle containing the face. For the sake
of simplicity, we use ”frame” to represent these regularized
frames containing only the face 1.
3) Timing Veriﬁcation: Our timing veriﬁcation is built on
the nature of how camera and screen work. Basically, both
of them follow the same scheme: refreshing pixel by pixel.
Detailedly, after ﬁnishing refreshing one line or column, they
move to the beginning of next line or column and perform
the scanning repeatedly. We can simply suppose an image
is displayed on screen line by line and captured by camera
column by column, ignoring the time gap between refreshing
adjacent pixels within one line or column that is much smaller
than the time needed to jump to the next line or column. In
other words, as to update any speciﬁc line on the screen, it
has to wait for a complete frame cycle until all other lines
have been scanned. Similarly, when a camera is capturing an
image, it also has to wait for a frame cycle to refresh a certain
column.
One example is given in Fig 5 to better explain the interest-
ing phenomenon that is leveraged for our timing veriﬁcation.
Fig 5a shows a screen that is just changing the displaying color
from Red to Green. Since it is scanning horizontally from top
to bottom, the upper part is now updated to Green but the
lower part is still previous color Red. The captured image of