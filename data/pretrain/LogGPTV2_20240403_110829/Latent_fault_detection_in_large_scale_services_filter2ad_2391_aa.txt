title:Latent fault detection in large scale services
author:Moshe Gabel and
Assaf Schuster and
Ran Gilad-Bachrach and
Nikolaj Bjørner
Latent Fault Detection in Large Scale Services
Moshe Gabel, Assaf Schuster
Department of Computer Science
Technion – Israel Institute of Technology
Haifa, Israel
{mgabel,assaf}@cs.technion.ac.il
Ran-Gilad Bachrach, Nikolaj Bjørner
Microsoft Research
Microsoft
Redmond, WA, USA
{rang,nbjorner}@microsoft.com
Abstract—Unexpected machine failures, with their resulting
service outages and data loss, pose challenges to datacenter man-
agement. Existing failure detection techniques rely on domain
knowledge, precious (often unavailable) training data, textual
console logs, or intrusive service modiﬁcations.
We hypothesize that many machine failures are not a result of
abrupt changes but rather a result of a long period of degraded
performance. This is conﬁrmed in our experiments, in which over
20% of machine failures were preceded by such latent faults.
We propose a proactive approach for failure prevention. We
present a novel framework for statistical latent fault detection
using only ordinary machine counters collected as standard
practice. We demonstrate three detection methods within this
framework. Derived tests are domain-independent and unsuper-
vised, require neither background information nor tuning, and
scale to very large services. We prove strong guarantees on the
false positive rates of our tests.
Index Terms—fault detection; web services; statistical analysis;
distributed computing; statistical learning
I. INTRODUCTION
For large scale services comprising thousands of computers,
it
is unreasonable to assume that so many machines are
working properly and are well conﬁgured [1], [2]. Unnoticed
faults might accumulate to the point where redundancy and
fail-over mechanisms break. Therefore, early detection and
handling of latent faults is essential for preventing failures
and increasing the reliability of these services.
A latent fault is machine behavior that is indicative of a
fault, or could eventually result in a fault. This work provides
evidence that latent faults are common. We show that these
faults can be detected using domain independent techniques,
and with high precision. This enables a proactive approach [3]:
machine failures can be predicted and handled effectively and
automatically without service outages or data loss.
Machines are usually monitored by collecting and analyzing
performance counters [4], [5], [6], [7]. Hundreds of counters
per machine are reported by the various service layers, from
service-speciﬁc information (such as the number of queries for
a database) to general information (such as CPU usage). The
large number of machines and counters in datacenters makes
manual monitoring impractical.
Existing automated techniques for detecting failures are
mostly rule-based. A set of watchdogs [6], [3] is deﬁned.
In most cases, a watchdog monitors a single counter on a
single machine or service: the temperature of the CPU or
free disk space, for example. Whenever a predeﬁned threshold
is crossed, an action is triggered. These actions range from
notifying the system operator to automatic recovery attempts.
Rule-based failure detection suffers from several key prob-
lems. Thresholds must be made low enough that faults will
not go unnoticed. At
the same time they should be set
high enough to avoid spurious detections. However, since the
workload changes over time, no ﬁxed threshold is adequate.
Moreover, different services, or even different versions of the
same service, may have different operating points. Therefore,
maintaining the rules requires constant, manual adjustments,
often done only after a “postmortem” examination.
Others have noticed the shortcomings of these rule-based
approaches. [8], [9] proposed training a detector on historic
annotated data. However, such approaches fall short due to
the difﬁculty in obtaining this data, as well as the sensitivity
of these approaches to deviations in workloads and changes
in the service itself. Others proposed injecting code into the
monitored service to periodically examine it [1]. This approach
is intrusive and hence prohibitive in many cases.
More ﬂexible, unsupervised approaches to failure detection
have been proposed for high performance computing (HPC).
[10], [11], [12] analyze textual console logs to detect system or
machine failures by examining occurrence of log messages. In
this work, we focus on large scale online services. This setting
differs from HPC in several key aspects. Console logs are
impractical in high-volume services for bandwidth and perfor-
mance reasons: transactions are very short, time-sensitive, and
rapid. Thus, in many environments, nodes periodically report
aggregates in numerical counters. Console log analysis fails
in this setting: console logs are non-existent (or very limited),
and periodically-reported aggregates exhibit no difference in
rate for faulty, slow or misconﬁgured machines. Rather, it
is their values that matter. Moreover, console logs originate
at application code and hence expose software bugs. We are
interested in counters collected from all layers to reveal both
software and hardware problems.
The challenge in designing a latent fault detection mecha-
nism is to make it agile enough to handle the variations in a
service and the differences between services. It should also be
non-intrusive yet correctly detect as many faults as possible
with only a few false alarms. As far as we know, we are
the ﬁrst to propose a framework and methods that address
all these issues simultaneously using aggregated numerical
counters normally collected by datacenters.
978-1-4673-1625-5/12/$31.00 ©2012 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:20:08 UTC from IEEE Xplore.  Restrictions apply. 
Contribution
A. Framework Overview
We focus on machine behavior that is indicative of a fault,
or could eventually result in a fault. We call this behavior
a latent fault. Not all machine failures are the outcome of
latent faults. Power outages and malicious attacks, for instance,
can occur instantaneously, with no visible machine-related
warning. However, even our most conservative estimates show
that at least 20% of machine failures have a long incubation
period during which the machine is already deviating in its
behavior but is not yet failing (Sec. IV-E).
We develop a domain independent framework for identi-
fying latent faults (Sec. II). Our framework is unsupervised
and non-intrusive, and requires no background information.
Typically, a scalable service will use (a small number of) large
collections of machines of similar hardware, conﬁguration,
and load. Consequently, the main idea in this work is to
use standard numerical counter readings in order to compare
similar machines performing similar tasks in the same time
frames, similar to [13], [10]. A machine whose performance
deviates from the others is ﬂagged as suspicious.
To compare machines’ behavior, the framework uses tests
that take the counter readings as input. Any reasonable test can
be plugged in, including non-statistical tests. We demonstrate
three tests within the framework and provide strong theoret-
ical guarantees on their false detection rates (Sec. III). We
use those tests to demonstrate the merits of the framework
on several production services of various sizes and natures,
including large scale services, as well as a service that uses
virtual machines (Sec .IV).
Our technique is agile: we demonstrate its ability to work
efﬁciently on different services with no need for tuning, yet
still guaranteeing false positive rate. Moreover, changes in the
workload or even changes to the service itself do not affect
its performance: in our experiments, suspicious machines that
switched services and workloads remained suspicious.
The rest of the paper is organized as follows: We ﬁrst
describe the problem and out general framework in Sec. II.
In Sec. III we use the framework to develop three speciﬁc
tests. In Sec. IV we discuss our empirical evaluation on several
production services. We survey related work in Sec. V, and
ﬁnally summarize our results and their implication in Sec. VI.
II. FRAMEWORK
Large-scale services are often made reliable and scalable
by means of replication. That is, the service is replicated on
multiple machines with a load balancing process that splits
the workload. Therefore, similar to [13], [10], we expect all
machines that perform the same role, using similar hardware
and conﬁguration, to exhibit similar behavior. Whenever we
see a machine that consistently differs from the rest, we ﬂag it
as suspicious for a latent fault. As we show in our experiments,
this procedure ﬂags latent faults weeks before the actual failure
occurs.
To compare machine operation, we use performance coun-
ters. Machines in datacenters often periodically report and
log a wide range of performance counters. These counters
are collected from the hardware (e.g., temperature), the op-
erating system (e.g., number of threads), the runtime system
(e.g., garbage collected), and from application layers (e.g.,
transactions completed). Hundreds of counters are collected
at each machine. More counters can be speciﬁed by the
system administrator, or the application developer, at will. Our
framework is intentionally agnostic: it assumes no domain
knowledge, and treats all counters equally. Figure 5 shows
several examples of such counters from several machines
across a single day.
We model the problem as follows: there are M machines
each reporting C performance counters at every time unit. We
denote the vector of counter values for machine m at time
t as x(m, t). The hypothesis is that the inspected machine
is working properly and hence the statistical process that
generated this vector for machine m is the same statistical
process that generated the vector for any other machine m(cid:48).
However, if we see that the vector x(m, t) for machine m is
notably different from the vectors of other machines, we reject
the hypothesis and ﬂag the machine m as suspicious for a
latent fault. (Below we simply say the machine is suspicious.)
After some common preprocessing (see Section II-D), the
framework incorporates pluggable tests (aka outlier detection
methods) to compare machine operation. At any point t, the
input x(t) to the test S consists of the vectors x(m, t) for
all machines m. The test S(m, x(t)) analyzes the data and
assigns a score (either a scalar or a vector) to machine m at
time t. x and x(cid:48) denote sets of inputs x(m, t) and x(cid:48)(m, t),
respectively, for all m and t.
The framework generates a wrapper around the test, which
guarantees its statistical performance. Essentially, the scores
for machine m are aggregated over time, so that eventually
the norm of the aggregated scores converges, and is used to
compute a p-value for m. The longer the allowed time period
for aggregating the scores is, the more sensitive the test will
be. At the same time, aggregating over long periods of time
creates latencies in the detection process. Therefore, in our
experiments, we have aggregated data over 24 hour intervals,
as a compromise between sensitivity and latency.
The p-value for a machine m is a bound on the probability
that a random healthy machine would exhibit such aberrant
counter values. If the p-value falls below a predeﬁned signiﬁ-
cance level α, the null hypothesis is rejected, and the machine
is ﬂagged as suspicious. In Section II-E we present the general
analysis used to compute the p-value from aggregated test
scores.
Given a test S, and a signiﬁcance level α > 0, we can
present the framework as follows:
1) Preprocess the data as described in Section II-D (can be
done once, after collecting some data; see below);
2) Compute for every machine m the vector vm =
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:20:08 UTC from IEEE Xplore.  Restrictions apply. 
(cid:80)
1
T
t S(m, x(t)) (integration phase);
3) Using the vectors vm, compute p-values p(m);
4) Report every machine with p(m) < α as suspicious.
To demonstrate the power of the framework, we describe
three test implementations in Sec. III.
B. Notation
The cardinality of a set G is denoted by |G|, while for
a scalar s, we use |s| as the absolute value of s. The L2
norm of a vector y is (cid:107)y(cid:107), and y · y(cid:48) is the inner product
of y and y(cid:48). M denotes the set of all machines in a test,
m, m(cid:48), m∗ denote speciﬁc machines, and M = |M| denotes
the number of machines. C is the set of all counters selected by
the preprocessing algorithm, c denotes a speciﬁc counter, and
C = |C|. T are the time points where counters are sampled
during preprocessing (for instance, every 5 minutes for 24
hours in our experiments), t, t(cid:48) denote speciﬁc time points,
and T = |T |.
C. Framework Assumptions
In modeling the problem we make several reasonable as-
sumptions (see, e.g., [13], [10]) that we will now make explicit.
While these assumptions might not hold in every environment,
they do hold in many cases, including the setups considered
in Section IV.
• The majority of machines are working properly at any
given point in time.
• Machines are homogeneous, meaning they perform a
similar task and use similar hardware and software. (If
this is not the case, then we can often split the collection
of machines to a few large homogeneous clusters.)
• On average, workload is balanced across all machines.
• Counters are ordinal and are reported at the same rate.
• Counter values are memoryless in the sense that they de-
pend only on the current time period (and are independent
of the identity of the machine).
Formally, we assume that x(m, t) is a realization of a
random variable X(t) whenever machine m is working prop-
erly. Since all machines perform the same task, and since
the load balancer attempts to split the load evenly between
the machines, the homogeneous assumption implies that we
should expect x(m, t) to show similar behavior. We do expect
to see changes over time, due to changes in the workload, for
example. However, we expect these changes to be similarly
reﬂected in all machines.
D. Preprocessing
Clearly, our model is simpliﬁed, and in practice not all of
its assumptions about counters hold. Thus the importance of
the preprocessing algorithm: it eliminates artifacts, normalizes
the data, and automatically discards counters that violate
assumptions and hinder comparison. Since we do not assume
any domain knowledge, preprocessing treats all counters sim-
ilarly, regardless of type. Furthermore, preprocessing is fully
automatic and is not tuned to the speciﬁc nature of the service
analyzed.
Not all counters are reported at a ﬁxed rate, and even
periodic counters might have different periods. Non-periodic
and infrequent counters hinder comparison because at any
given time their values are usually unknown for most ma-
chines. They may also bias statistical tests. Such counters are
typically event-driven, and have a different number of reports
on different machines; hence they are automatically detected
by looking at the variability of the reporting rate and are
removed by the preprocessing.
Additionally, some counters violate the assumption of being
memoryless. For example, a counter that reports the time since
the last machine reboot cannot be considered memoryless.
Such counters usually provide no insight into the correct or
normal behavior because they exhibit different behavior on
different machines. Consequently, preprocessing drops those
counters. Automatic detection of such counters is performed
similarly to the detection of event-driven counters, by looking
at the variability of counter means across different machines.
The process of dropping counters is particularly important
when monitoring virtual machines. It eliminates counters
reﬂecting cross-talk between machines running in the same
physical host. In our experiments, after the above ﬁltering
operations, we were typically left with more than one hundred
useful counters (over two hundred in some systems); see Table
IV.
Preprocessing also samples counters at equal time intervals
(5 minutes in our implementation), so that machines can
be compared at those time points. Finally, the counters are
normalized to have a zero mean and a unit variance in order
to eliminate artifacts of scaling and numerical instabilities.
There are many possible ways to measure variability. Our
implementation is based on normalized median absolute de-
viation. The particulars are not critical to the framework, and
were omitted for lack of space.
E. Framework Analysis
(cid:80)
In this section we show how the p-values (step 3 in the
framework) are computed. We use two methods to compute
these values. The ﬁrst method assumes the expected value
of the scoring function is known when all machines work