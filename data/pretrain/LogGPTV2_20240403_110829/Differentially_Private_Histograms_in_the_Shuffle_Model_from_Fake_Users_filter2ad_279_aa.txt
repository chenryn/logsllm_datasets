title:Differentially Private Histograms in the Shuffle Model from Fake Users
author:Albert Cheu and
Maxim Zhilyaev
4
1
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
Differentially Private Histograms
in the Shufﬂe Model from Fake Users
Albert Cheu
Department of Computer Science
Georgetown University
Email: PI:EMAIL
Maxim Zhilyaev
Meta Privacy
Email: PI:EMAIL
Abstract—There has been much recent work in the shufﬂe
model of differential privacy, particularly for approximate d-bin
histograms. While these protocols achieve low error, the number
of messages sent by each user—the message complexity—has
so far scaled with d or the privacy parameters. The message
complexity is an informative predictor of a shufﬂe protocol’s
resource consumption. We present a protocol whose message
complexity is two when there are sufﬁciently many users. The
protocol essentially pairs each row in the dataset with a fake
row and performs a simple randomization on all rows. We show
that the error introduced by the protocol is small, using rigorous
analysis as well as experiments on real-world data. We also prove
that corrupt users have a relatively low impact on our protocol’s
estimates.
I. INTRODUCTION
Given that statistical computations often involve data sourced
from human users, an analyst could execute differentially
private algorithms in the central model (also called centrally
private algorithms). Originally deﬁned by Dwork, McSherry,
Nissim, and Smith [12], these algorithms provide quantiﬁable
protection to data contributors at a small price in terms of
accuracy. As an example, there exists an (ε, δ)-centrally private
algorithm that computes d-bin histograms from n users up to
maximum ((cid:96)∞) error O( 1
εn log 1
δ ) [6].
We focus on computing accurate histograms since they allow
approximate top-t selection, the set of t data values that occur
most frequently in a population. One application is smart-
phone autocomplete. Because devices are resource constrained,
a keyboard offers word corrections from a smaller pool than
the entire vocabulary. To obtain a list of the most common
words, user devices could participate in a differentially private
computation that estimates word frequencies.
Users contributing to a centrally private algorithm need to
trust that the analyst correctly executes the algorithm and does
not leak their data. To collect data from less trusting users,
analysts can instead implement locally private protocols: each
user applies a differentially private algorithm on their data and
sends a message containing the algorithm’s output to the analyst.
This weaker trust assumption comes at a price: there are lower
bounds that show locally private protocols have signiﬁcantly
more error than the centrally private counterparts. Returning to
the histogram example, Bassily & Smith show (ε, o(1/n))-local
privacy incurs a maximum error of Ω( 1
ε
(cid:113) log d
n ) [4].
Figure 1: Diagram of the shufﬂe model. Like the local model,
users individually randomize their data to protect against an
untrusted analyzer. But unlike the local model, there is a shufﬂer
which anonymizes messages.
Originating with work by Bittau et al. and Cheu et al.
[5, 10], shufﬂe privacy has emerged as an appealing middle-
ground. Here, we assume that there is a service called the
shufﬂer that uniformly permutes user messages. The output of
the shufﬂer must satisfy (ε, δ)-differential privacy. Intuitively,
if each user generates a locally private message, then the
anonymity provided by the shufﬂer should “amplify” the
privacy guarantee. Put another way, o( 1
n ) histogram
ε
error may be possible for the same privacy parameters as in
the local model.
(cid:113) log d
As shown in Figure 1, a user can send multiple messages
to the shufﬂer. And a user does not need to produce these
messages in a differentially private manner, since we only
require that the output of the shufﬂer is differentially private.
This ﬂexibility is leveraged by the histogram protocol of Balcer
& Cheu [2], where each user sends d + 1 messages and the
maximum error is O( 1
δ ) for δ = O(1/n). This is within
a 1/ε factor of the central model result. Alternative histogram
protocols in the shufﬂe model have been introduced by Ghazi,
Golowich, Kumar, Pagh, and Velingker [16] and by Ghazi,
Kumar, Manurangsi, and Pagh [17]. As shown in Table I,
these protocols demand fewer messages from each user (in
expectation) than the protocol from [2].
ε2n log 1
We will use message complexity to refer to the number of
messages sent by each user and communication complexity to
refer to total number of bits consumed by those messages. The
message complexity is necessary to have a complete picture
of a protocol’s resource consumption. For starters, the amount
of randomness needed to perform the shufﬂe is a function of
© 2022, Albert Cheu. Under license to IEEE.
DOI 10.1109/SP46214.2022.00055
440
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:25:20 UTC from IEEE Xplore.  Restrictions apply. 
User 𝑛User 2User 1𝑦1,1Shuffler𝑆𝑅(𝑥1)𝑥1Analyzer𝐴𝑦2,1𝑅(𝑥2)𝑥2𝑦𝑛,1𝑅(𝑥𝑛)𝑥𝑛Raw dataRandomizerMessages𝑦81,1𝑦27,2𝑦4,1… … … … 𝑦1,2𝑦2,2𝑦𝑛,2the message complexity but not the length of each message.
Furthermore, two protocols P,P(cid:48) with the same communication
complexity can incur different costs, since the physical delivery
of a message over a network in a secure fashion requires
overhead. If P sends more messages than P(cid:48), the computing
cost of transmitting messages is larger for P, since it needs to
perform cryptographic operations on each message. Also, the
bandwidth overhead is larger for P, due to both encryption
and physical network protocols such as TCP/IP.
In light of the above, one can ask the following question:
Are there shufﬂe private protocols for histograms
that have low message complexity but still provide
estimates that are competitive with prior work?
Given the distributed nature of local and shufﬂe protocols,
they are impacted by users who deviate from the intended
behavior. In the local privacy literature, there is research
on manipulation attacks where corrupted users aim to skew
estimates and tests by sending carefully crafted messages. One
baseline attack is to simply feed wrong inputs into the protocol,
but the prior work has shown that there are attacks against
locally private protocols that introduce signiﬁcantly worse error
(see e.g. Cao, Jia, and Gong [7] and Cheu, Smith, and Ullman
[11] and citations within). Here, we investigate manipulation
against shufﬂe private protocols. Speciﬁcally,
Are there shufﬂe private protocols for histograms
that are robust to manipulation by users?
A. Our Contributions
Our primary contribution is a shufﬂe private protocol for
histograms that answers both questions in the afﬁrmative. For
a large range of n, the message complexity can be as small as
two. We also show that one consequence of the low message
complexity is robustness to manipulation.
any positive integer. Each message is d bits.
Section III contains the full speciﬁcation and analysis of our
protocol, but we give an overview of the core features in the
theorem below.
Theorem I.1 (Informal). For any privacy parameters ε = O(1),
δ < 1/100, and number of users n = Ω(log d + 1
δ ),
ε2 log 1
there is an (ε, δ)-differentially private shufﬂe protocol that
approximates d-bin histograms with the following properties
i. The message complexity is k + 1, where k can be set to
ii. The maxmimum error of any bin estimate is f (k) ·
δ ) with probability 9/10,1 where
n ·
iii. m corrupted users can skew an estimate by at most m
O( log d
f (k) monotonically approaches 1 from above.
(k + 1) · f (k).
We unpack this theorem. Parts i and ii show that the protocol
allows for a tradeoff between message complexity and the
measurement accuracy, since increasing k reduces a scaling
factor f (k). This may not be signiﬁcant for large n, but it could
log d log 1
(cid:113)
n + 1
εn
1We use 9/10 as a target success probability throughout this work, but it
can be changed to any other constant without affecting the asymptotic analysis.
be useful for smaller n (e.g. the target population of a health
survey can consist of much fewer subjects than the dictionary-
building example). Re-scaling k by a factor of c will naturally
increase the transmission cost by c but the trafﬁc remains
feasible since n is small. Thus, we can improve accuracy
without altering the privacy guarantee.
Meanwhile, Part iii bounds the impact of any manipulation
attack. Each corrupt user in our protocol can introduce O( 1
n )
bias whenever k· f (k) = O(1). For comparison, we also prove
that a protocol by Ghazi et al. [16] suffers Ω( 1
δ ) bias
per corrupt user.
ε2 log 1
n · 1
In Section IV, we describe experiments on text sampled
from Twitter to evaluate our protocol’s accuracy (absent an
attack). The error introduced by our protocol to the histogram
is consistent with our theoretical bounds. We also show that
the top-t items in the output of the protocol are consistent
with those in the raw dataset, for several choices of t. The
experimental results of our protocol compare favorably to that
of [2]. We also show that the protocol in [17] produces more
accurate estimates than ours but at high cost.
We also study variants of our main protocol. In Section
V, we describe how to exponentially reduce the protocol’s
communication complexity. The price is an increased message
complexity and a mildly increased error. Appendix C analyzes
the special case where k = 0 so that each user transmits only
one message. We combine steps taken by Ghazi et al. [16] with
the state-of-the art ampliﬁcation lemma by Feldman, McMillan,
and Talwar [15]. This alternative protocol’s maximum error is
proportional to 1/n3/4 instead of 1/n.
Techniques: Each user in our protocol ﬁrst encodes their data
∈ [d] as a binary string ∈ {0, 1}d with a single set bit. They
then ﬂip each bit independently with probability q. Next, they
repeat this bit ﬂipping on k all-zero strings, which corresponds
to introducing k fake users with null data. The user sends all
k + 1 randomized strings to the shufﬂer. The analyzer receives
the shufﬂed set of messages and then computes a simple linear
function on the binary sums. Much of our work is devoted
to choosing q so that the nk messages from the fake users
provide differential privacy for the actual users.
Our technique to reduce communication complexity proceeds
in two stages. We ﬁrst make the simple observation that a binary
string with known length is equivalent to a list of the indices
of set bits. By construction, a message generated by our local
randomizer is a binary string where the number of such indices
has expectation O(dq). Our choice of q is proportional to
1/n, so this alternative representation is very effective when n
approaches or exceeds d.
The small n regime motivates the second round of compres-
sion, which adapts the count-min sketching technique. Given
a random hash function, we can reduce the size of the domain
d to some ˆd, so we run our histogram protocol on the hashed
data. Collisions with user data are possible, but if we sample
enough hash functions, it is likely that there is a hash for each
domain element that free of such collisions. We remark that
Ghazi et al. [16] build a speciﬁc histogram protocol out of
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:25:20 UTC from IEEE Xplore.  Restrictions apply. 
441
count-min, while our approach is general enough to work with
arbitrary histogram protocols.
B. Related Work
We brieﬂy recap prior work on private histograms in the
shufﬂe model. Cheu, Smith, Ullman, Zeber, and Zhilyaev
[10] rigorously deﬁne the shufﬂe model and give a histogram
protocol that requires d messages per user. Balcer & Cheu [2]
give a different protocol with the same asymptotic message
complexity but with maximum error independent of d. These
two results are presented in the ﬁrst row of Table I.
We note that Cheu et al. [10] and Balcer & Cheu [2] use the
same design pattern: equate a histogram with a composition
of d counts and then privatize those counts using binomial
noise. This technique is not used by our core protocol or
the protocol based on Hadamard response by Ghazi et al.
[16]. In these works, users generate exactly one message
from their data and then sample messages from a data-
independent distribution. Privacy proofs for these protocols rely
on bespoke multinomial distributions, instead of composition of
the binomial mechanism. Our work differs from [16] because
we explore what is possible with low message complexity
while they optimize communication complexity.
[16] also give a protocol based on the ampliﬁcation lemma
from Balle, Bell, Gascón, and Nissim [3]. Our analysis in
Appendix C uses a more recent ampliﬁcation lemma.
In follow-up work, Ghazi et al. [17] give a protocol where
the message complexity shrinks as n increases. Our protocol
has the same property but at a faster rate. Speciﬁcally, our
message complexity is two when n is logarithmic in d while
the prior work requires n to be linear in d.
Manipulation attacks have previously been studied in the
context of local privacy. Ambainis, Jakobsson, and Lipmaa
[1] as well as Moran and Naor [19] study the vulnerability
of randomized response to these attacks. Work by Cao, Jia,
and Gong [7] also consider attacks against histogram and
heavy hitter protocols. Cheu, Smith, and Ullman [11] show
that powerful attacks are inevitable for any locally private
protocol. In particular, these attacks are stronger when the
privacy guarantee is stronger or the data dimension is larger.
may not hold for every (cid:126)x ∼ (cid:126)x (cid:48) ∈ X n. In these cases, we
will disambiguate by saying it satisﬁes differential privacy for
inputs from X .
Because this deﬁnition assumes that the algorithm M has
“central” access to compute on the entire raw dataset, we
sometimes call this central differential privacy. Two properties
about differentially private algorithms will be useful. First,
privacy is preserved under post-processing.
Fact II.2. For (ε, δ)-differentially private algorithm M :
X n → Z and randomized algorithm f : Z → Z(cid:48), f ◦ M
is (ε, δ)-differentially private.
This means that any computation based solely on the output
of a differentially private function does not affect the privacy
guarantee. Refer to Prop. 2.1 in the text by Dwork and Roth [13]
for a proof. The second property is closure under composition.
Fact II.3. For (ε1, δ1)-differentially private M1 and (ε2, δ2)-
differentially private M2, M3 deﬁned by M3((cid:126)x) =
(M1((cid:126)x),M2((cid:126)x)) is (ε1 + ε2, δ1 + δ2)-differentially private.
Fact