title:Stealing Links from Graph Neural Networks
author:Xinlei He and
Jinyuan Jia and
Michael Backes and
Neil Zhenqiang Gong and
Yang Zhang
Stealing Links from Graph Neural Networks
Xinlei He, CISPA Helmholtz Center for Information Security; Jinyuan Jia, 
Duke University; Michael Backes, CISPA Helmholtz Center for 
Information Security; Neil Zhenqiang Gong, Duke University; 
Yang Zhang, CISPA Helmholtz Center for Information Security
https://www.usenix.org/conference/usenixsecurity21/presentation/he-xinlei
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Stealing Links from Graph Neural Networks
Xinlei He1
Jinyuan Jia2 Michael Backes1 Neil Zhenqiang Gong2 Yang Zhang1
1CISPA Helmholtz Center for Information Security 2Duke University
Abstract
Graph data, such as chemical networks and social networks,
may be deemed conﬁdential/private because the data owner
often spends lots of resources collecting the data or the data
contains sensitive information, e.g., social relationships. Re-
cently, neural networks were extended to graph data, which
are known as graph neural networks (GNNs). Due to their
superior performance, GNNs have many applications, such as
healthcare analytics, recommender systems, and fraud detec-
tion. In this work, we propose the ﬁrst attacks to steal a graph
from the outputs of a GNN model that is trained on the graph.
Speciﬁcally, given a black-box access to a GNN model, our at-
tacks can infer whether there exists a link between any pair of
nodes in the graph used to train the model. We call our attacks
link stealing attacks. We propose a threat model to system-
atically characterize an adversary’s background knowledge
along three dimensions which in total leads to a comprehen-
sive taxonomy of 8 different link stealing attacks. We propose
multiple novel methods to realize these 8 attacks. Extensive
experiments on 8 real-world datasets show that our attacks
are effective at stealing links, e.g., AUC (area under the ROC
curve) is above 0.95 in multiple cases. Our results indicate
that the outputs of a GNN model reveal rich information about
the structure of the graph used to train the model.
1 Introduction
Graph is a powerful tool to model the complex relationships
between entities. For instance, in healthcare analytics, protein-
protein interactions can be modeled as a graph (called a chem-
ical network); and a social network can be modeled as a graph,
where nodes are users and edges indicate certain social re-
lationships among them. A graph may be treated as a data
owner’s intellectual property because the data owner may
spend a lot of resources collecting the graph, e.g., collecting
a chemical network often involves expensive and resource-
consuming chemical experiments. Moreover, a graph may
also contain sensitive user information, e.g., private social
relationships among users.
Recently, a family of machine learning techniques known
as graph neural networks (GNNs) was proposed to analyze
graphs. We consider GNNs for node classiﬁcation. Specif-
ically, given a graph, attributes of each node in the graph,
and a small number of node labels, a GNN model is trained
and can predict the label of each remaining unlabeled node.
Due to their superior performance, we have seen growing
applications of GNNs in various domains, such as healthcare
analytics [18, 22], recommender systems [19], and fraud de-
tection [65]. However, the security and privacy implications
of training GNNs on graphs are largely unexplored.
Our Contributions. In this work, we take the ﬁrst step to
study the security and privacy implications of training GNNs
on graphs. In particular, we propose the ﬁrst attacks to steal a
graph from the outputs of a GNN model trained on the graph.
We call our attacks link stealing attacks. Speciﬁcally, given a
black-box access to a target GNN model, our attacks aim to
predict whether there exists a link between any pair of nodes
in the graph used to train the target GNN model. Our attacks
reveal serious concerns on the intellectual property, conﬁden-
tiality, and/or privacy of graphs when training GNNs on them.
For instance, our attacks violate the intellectual property of
the data owner when it spends lots of resources collecting the
graph; and our attacks violate user privacy when the graph
contains sensitive social relationships among users [2, 23].
Adversary’s Background Knowledge: We refer to the graph
and nodes’ attributes used to train the target GNN model
as the target dataset. We characterize an adversary’s back-
ground knowledge along three dimensions, including the tar-
get dataset’s nodes’ attributes, the target dataset’s partial
graph, and an auxiliary dataset (called shadow dataset) which
also contains its own graph and nodes’ attributes. An adver-
sary may or may not have access to each of the three dimen-
sions. Therefore, we obtain a comprehensive taxonomy of a
threat model, in which adversaries can have 8 different types
of background knowledge.
Attack Methodology: We design an attack for each of the 8 dif-
ferent types of background knowledge, i.e., we propose 8 link
stealing attacks in total. The key intuition of our attacks is that
USENIX Association
30th USENIX Security Symposium    2669
two nodes are more likely to be linked if they share more sim-
ilar attributes and/or predictions from the target GNN model.
For instance, when the adversary only has the target dataset’s
nodes’ attributes, we design an unsupervised attack by calcu-
lating the distance between two nodes’ attributes. When the
target dataset’s partial graph is available, we use supervised
learning to train a binary classiﬁer as our attack model with
features summarized from two nodes’ attributes and predic-
tions obtained from the black-box access to the target GNN
model. When the adversary has a shadow dataset, we propose
a transferring attack which transfers the knowledge from the
shadow dataset to the target dataset to mount our attack.
Evaluation: We evaluate our 8 attacks using 8 real-world
datasets. First, extensive experiments show that our attacks
can effectively steal links. In particular, our attacks achieve
high AUCs (area under the ROC curve). This demonstrates
that the predictions of a target GNN model encode rich in-
formation about the structure of a graph that is used to train
the model, and our attacks can exploit them to steal the graph
structure. Second, we observe that more background knowl-
edge leads to better attack performance in general. For in-
stance, on the Citeseer dataset [35], when an adversary has
all the three dimensions of the background knowledge, our
attack achieves 0.977 AUC. On the same dataset, when the
adversary only has nodes’ attributes, the AUC is 0.878. Third,
we ﬁnd that the three dimensions of background knowledge
have different impacts on our attacks. Speciﬁcally, the target
dataset’s partial graph has the strongest impact followed by
nodes’ attributes, the shadow dataset, on the other hand, has
the weakest impact. Fourth, our transferring attack can achieve
high AUCs. Speciﬁcally, our transferring attack achieves bet-
ter performance if the shadow dataset comes from the same
domain as the target dataset, e.g., both of them are chemical
networks. We believe this is due to the fact that graphs from
the same domain have similar structures, which leads to less
information loss during transferring. Fifth, our attacks out-
perform conventional link prediction methods [24, 40], which
aim to predict links between nodes based on a partial graph.
In summary, we make the following contributions.
• We propose the ﬁrst link stealing attacks against graph
neural networks.
• We propose a threat model to comprehensively charac-
terize an adversary’s background knowledge along three
dimensions. Moreover, we propose 8 link stealing attacks
for adversaries with different background knowledge.
• We extensively evaluate our 8 attacks on 8 real-world
datasets. Our results show that our attacks can steal links
from a GNN model effectively.
2 Graph Neural Networks
Many important real-world datasets come in the form of
graphs or networks, e.g., social networks, knowledge graph,
and chemical networks. Therefore, it is urgent to develop
machine learning algorithms to fully utilize graph data. To
this end, a new family of machine learning algorithms, i.e.,
graph neural networks (GNNs), has been proposed and shown
superior performance in various tasks [1, 14, 35, 62].
Training a GNN Model. Given a graph, attributes for each
node in the graph, and a small number of labeled nodes, GNN
trains a neural network to predict labels of the remaining
unlabeled nodes via analyzing the graph structure and node
attributes. Formally, we deﬁne the target dataset as D =
(A,F ), where A is the adjacency matrix of the graph and F
contains all nodes’ attributes. Speciﬁcally, Auv is an element
in A: If there exists an edge between node u and node v,
then Auv = 1, otherwise Auv = 0. Moreover, Fu represents the
attributes of u. V is a set containing all nodes in the graph.
Note that we consider undirected graphs in this paper, i.e.,
∀u,v ∈ V ,Auv = Avu.
A GNN method iteratively updates a node’s features via
aggregating its neighbors’ features using a neural network,
whose last layer predicts labels for nodes. Different GNN
methods use slightly different aggregation rules. For instance,
graph convolutional network (GCN), the most representative
and well-established GNN method [35], uses a multi-layer
neural network whose architecture is determined by the graph
structure. Speciﬁcally, each layer obeys the following propa-
gation rule to aggregate the neighboring features:
H(k+1) = σ( ˜Q − 1
2 ˜A ˜Q − 1
2 H(k)W (k)),
(1)
where ˜A = A +I is the adjacency matrix of the graph with self-
connection added, i.e., I is the identity matrix. ˜Q − 1
2 is
the symmetric normalized adjacency matrix and ˜Quu = ∑u ˜Auv.
Moreover, W (k) is the trainable weight matrix of the kth layer
and σ(·) is the activation function to introduce non-linearity,
such as ReLU. As the input layer, we have H(0) = F . When
the GCN uses a two-layer neural network, the GCN model
can be described as follows:
2 ˜A ˜Q − 1
softmax( ˜Q − 1
2 ˜A ˜Q − 1
2 σ( ˜Q − 1
2 ˜A ˜Q − 1
2 F W (0))W (1)).
(2)
Note that in most of the paper, we focus on two-layer GCN.
Later, we show that our attack can be also performed on other
types of GNNs, including GraphSAGE [27] and GAT [62]
(see Section 5).
Prediction in a GNN Model. Since all nodes’ attributes and
the whole graph have been fed into the GNN model in the
training phase to predict the label of a node, we only need
to provide the node’s ID to the trained model and obtain the
prediction result. We assume the prediction result is a poste-
rior distribution (called posteriors) over the possible labels
2670    30th USENIX Security Symposium
USENIX Association
Table 1: List of notations.
Notation
D
A
A∗
F
V
f
g
f (u)
g(u)
D(cid:48)
f (cid:48)
g(cid:48)
K
d(·,·)
Ψ(·,·)
e( f (u))
Description
Target dataset
Graph of D represented as adjacency matrix
Partial graph of D
Nodes’ attributes of D
Set of nodes of D
Target model
Reference model
u’s posteriors from the target model
u’s posteriors from the reference model
Shadow dataset
Shadow target model
Shadow reference model
Adversary’s knowledge
Distance metric
Pairwise vector operations
Entropy of f (u)
for the node. Our work shows that such posteriors reveal rich
information about the graph structure: As mentioned before,
a GNN essentially learns a node’s features via aggregating
its neighbors’ features, if two nodes are connected, then their
posteriors should be similar. We leverage this to build our
attack models. We further use f to denote the target GNN
model and f (u) to represent the posteriors of node u. For pre-
sentation purposes, we summarize the notations introduced
here and in the following sections in Table 1.
3 Problem Formulation
In this section, we ﬁrst propose a threat model to characterize
an adversary’s background knowledge. Then, we formally
deﬁne our link stealing attack.
3.1 Threat Model
Adversary’s Goal. An adversary’s goal is to infer whether a
given pair of nodes u and v are connected in the target dataset.
Inferring links between nodes leads to a severe privacy threat
when the links represent sensitive relationship between users
in the context of social networks. Moreover, links may be con-
ﬁdential and viewed as a model owner’s intellectual property
because the model owner may spend lots of resources col-
lecting the links, e.g., it requires expensive medical/chemical
experiments to determine the interaction/link between two
molecules in a chemical network. Therefore, inferring links
may also compromise a model owner’s intellectual property.
Adversary’s Background Knowledge. First, we assume an
adversary has a black-box access to the target GNN model.
In other words, the adversary can only obtain nodes’ posteri-
ors by querying the target model f . This is the most difﬁcult
setting for the adversary [52, 54, 56]. An adversary can have
a black-box access to a GNN model when an organization
uses GNN tools from another organization (viewed as an
adversary) or the GNN model prediction results are shared
among different departments within the same organization.
For instance, suppose a social network service provider lever-
ages another company’s tool to train a GNN model for fake-
account detection, the provider often needs to send the predic-
tion results of (some) nodes to the company for debugging or
reﬁning purposes. In such a scenario, the security company
essentially has a black-box access to the GNN model. Note
that the graph structure is already revealed to the adversary if
she has a white-box access to the target GNN model as the
GNN model architecture is often based on the graph structure.
Then, we characterize an adversary’s background knowl-
edge along three dimensions:
• Target Dataset’s Nodes’ Attributes, denoted by F .
This background knowledge characterizes whether the
adversary knows nodes’ attributes F in D. We also as-
sume that the adversary knows labels of a small subset
of nodes.
• Target Dataset’s Partial Graph, denoted by A∗. This
dimension characterizes whether the adversary knows
a subset of links in the target dataset D. Since the goal
of link stealing attack is to infer whether there exists an
edge/link between a pair of nodes, the partial graph can
be used as ground truth edges to train the adversary’s
attack model.
• A Shadow Dataset, denoted by D(cid:48). This is a dataset