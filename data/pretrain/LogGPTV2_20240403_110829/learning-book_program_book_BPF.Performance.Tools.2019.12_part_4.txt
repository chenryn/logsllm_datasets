Figure 17-10: grafana-pcp-live screenshot, Copyright 2019 C Grafana Labs
Figures 17-11 to 17-14: Grafana screenshots, Copyright 2019 C Grafana Labs
SupplementalMaterialandReferences
Readlers are encouraged to visit the website for this book:
http://www.brendangregg.com/bpf-performance-tools-book.html
All the tools contained in the book, as well as book errata and reader feedback, can be
downloaded from this site.
Many of the tools discussed in this book are also in source code repositories where they are
maintained and enhanced. Refer to these repositories for the latest versions of these tools:
https:/github.com/iovisor/bcc
https://github.com/iovisor/bpftrace
These repositories also contain detailed reference guides and tutorials, which I created and the
BPF community maintains and updates
---
## Page 33
ConventionsUsedinThisBook
This book discusses different types of technology, and the way it presents material provides more
context.
For tool output, bold text indicates the command that was executed or, in some cases, highlights
something of interest. A hash prompt (#) signifies that the command or tool has been run as the
root user (administrator). For example:
+ id
uid=0 (coot) gid=0 (root)gcougs=0 (root)
A dollar prompt (S) signifies running the command or tool as a non-root user:
PT
u1d=1000 (bgxegg1 g1d=1000 (bgregg) gzoups=1000 (bgzegg1 , 4 (adn) , 2T (sudo)
Some prompts include a directory name prefix to show the working directory
bpftrace/too1ss  /biolateney.bt
Italic is used to highlight new terms, and is sometimes used to show placeholder text.
u 6q uous °unu o sataatud quaeamb 1o ssaooe poor annbar qooq stu u sjoo atp jo iso
repeated use of hash prompts. If you are not root, one way to execute tools as root is to prefix
them with sudo for the sudo(8) command (super-user do).
shell expansions. It is a good habit to form. For example:
Some commands are executed in single quotation marks to prevent unnecessary (albeit unlikely)
 funccount 'vfs_*"
A Linux command name or system call is followed by the man page chapter enclosed in
parentheses—for example, the Is(1) command, the read(2) system call, and the funccount(8)
system administration command. Empty parentheses signify function calls from a programming
language—for example, the vfs_read( kernel function. When commands with arguments are
includled in paragraphs, they use a monospace font.
Command output that is truncated includes an ellipsis in square brackets (..]). A single line
containing ^C indicates that Ctrl-C was typed to terminate the program.
Bibliography references for websites are numbered: e.g., [123]
---
## Page 34
Acknowledgments
Many people have worked on building all the components necessary for the BPF tracing tools
t u suagqord aqisuaqaaduou Suajos :annosqo taas deu suonqiuo au epot xom o
uauodtuo xadtuoo saqo ro raguan uoonnsu ueqoog sadto xosaueay Supen paua
Such work is often not understood and goes unsung. But the end result of their labors is the
BPF tools you're about to run. Many of which were written by me, which might give the unfair
impression that I wrote them single-handedlly, but I’'m really building upon many different
technologies and the work of so many others. I'd like to acknowledge and thank them for their
work, as well as thank others who also contributed to this book.
Technologies and their authors include:
eBPF: Thanks to Alexei Starovoitov (Facebook; formerly PLUMgrid) and Daniel Borkmann
(Isovalent; formerly Cisco, Red Hat) for creating the technology, leading development,
maintaining the BPF kernel code, and pursuing their vision for eBPE Thanks to all the other
eBPF contributors, in particular David S. Miller (Red Hat for supporting and improving the
technology. At the time of writing, there is a BPF community of 249 different contributors
to BPF kernel code with a total of 3,224 commits since 2014. After Daniel and Alexei,
the current top contributors based on commit counts are: Jakub Kicinski (Netronome),
Yonghong Song (Facebook), Martin KaFai Lau (Facebook), John Fastabend (Isovalent;
formerly Intel), Quentin Monnet (Netronome),Jesper Dangaard Brouer (Red Hat), Andrey
Ignatov (Facebook), and Stanislav Fomichev (Google).
 BCC: Thanks to Brenden Blanco (VMware; formerly PLUMgrid) for creating and developing
BCC. Major contributors include Sasha Goldshtein (Google; formerly SELA), Yonghong Song
(Facebook; formerly PLUMgrid), Teng Qin (Facebook), Paul Chaignon (Orange), Vicent Marti
(github), Mark Drayton (Facebook), Allan McAleavy (Sky), and Gary Ching-Pang Lin (SUSE).
• bpftrace: Thanks to Alastair Robertson (Yellowbrick Data; formerly G-Research, Cisco)
for creating bpftrace and insisting on quality code and extensive tests. Thanks to all other
bpftrace contributors so fat, especially Matheus Marchini (Netflix; formerly Shtima),
Willian Gasper (Shtima), Dale Hamel (Shopify), Augusto Mecking Caringi (Red Hat), and
Dan Xu (Facebook).
• ply: Thanks to Tobias Waldekranz for developing the first high-level tracer built upon BPE
 LLVM: Thanks to Alexei Starovoitov, Chandler Carruth (Google), Yonghong Song, and
others, for their work on the BPF backend for LLVM, which BCC and bpftrace is built upon.
kprobes: Thanks to all those that designed, developed, and worked on kernel dynamic
instrumentation for Linux, which is used extensively throughout this book. They include
Richard Moore (IBM), Suparna Bhattacharya (IBM), Vamsi Krishna Sangavarapu (IBM),
Prasanna S. Panchamukhi (IBM), Ananth N Mavinakayanahalli (IBM), James Keniston
(IBM), Naveen N Rao (IBM), Hien Nguyen (IBM), Masami Hiramatsu (Linaro; formerly
Hitachi), Rusty Lynch (Intel), Anil Keshavamurthy (Intel), Rusty Russell, Will Cohen (Red
Hat), and David S. Miller (Red Hat).
uprobes: Thanks to Srikar Dronamraju (IBM), Jim Keniston, and Oleg Nesterov (Red
Hat) for developing user-level dynamic instrumentation for Linux, and Peter Zijlstra for
technical review.
---
## Page 35
xxiv Acknowledgments
tracepoints: Thanks to Mathieu Desnoyers (EfficiOS) for his contributions to Linux tracing.
In particular, Mathieu developed and drove static tracepoints to be accepted in the kernel, 
making it possible to build stable tracing tools and applications.
 perf: Thanks to Arnaldo Carvalho de Melo (Red Hat) for his work on the perf(1) utility,
which added kernel capabilities that BPF tools make use of.
• Ftrace: Thanks to Steven Rostedt (VMware; formerly Red Hat) for Ftrace and his other
contributions to tracing, Ftrace has aided BPF tracing development, as where possible I’ve
cross-checked tool output with equivalents in Ftrace. Tom Zanussi (Intel) has recently been
contributing with Ftrace hist triggers.
• (Classic) BPF: Thanks to Van Jacobson and Steve McCanne.
 Dynamic instrumentation: Thanks to professor Barton Miller (University of Wisconsin
Madison) and his then-student Jeffrey Hollingsworth for founding the field of dynamic
instrumentation in 1992 [Hollingsworth 94], which has been the killer feature driving the
adoption of DTrace, SystemTap, BCC, bpftrace, and other dynamic tracers. Most of the tools
in this book are based on dynamic instrumentation (those that use kprobes and uprobes)
•LTT: Thanks to Karim Yaghmour and Michel R. Dagenais for developing the first Linux
tracer, LTT in 1999, Also thanks to Karim for his unrelenting push for tracing in the Linux
community, building support for later tracers.
• Dprobes: Thanks to Richard J. Moore and his team at IBM for developing the first dynamic
instrumentation technology for Linux, DProbes, in 2000, which led to the kprobes
technology we used today.
•System'Tap: While SystemTap is not used in this book, the work by Frank Ch. Eigler (Red
Hat) and others on SystemiTap has greatly improved the field of Linux tracing. They were
Supen pauaq qm s8nq zagunooua pue seare matu oqu Susen xnurI ysnd o isig uao
technologies.
•ktap: Thanks to Jovi Zhangwei for ktap, a high-level tracer that helped build support in
Linux for VM-based tracers.
•Also thanks to the Sun Microsystems engineers Bryan Cantrill, Mike Shapiro, and Adam
Leventhal, for their outstanding work in developing the first widely-used dynamic
instrumentation technology: DTrace, launched in 2005. Thanks to Sun marketing,
evangelists, sales, and many others inside and outside of Sun, for helping make DTrace
known worldwide, helping drive demand for similar tracers in Linux.
Thanks to the many others not listed here who have also contributed to these technologies over
the years.
Apart from creating these technologies, many of the same people have helped with this book:
Daniel Borkmann provided amazing technical feedback and suggestions for several chapters,
and Alexei Starovoitov also provided critical feedback and axdvice for the eBPF kernel content (as
well as writing the Foreword). Alastair Robertson provided input on the bpftrace chapter, and
Yonhgong Song provided feedback for the BTF content while he was developing BTF.
s aop suognquoo pue xoeqpaa peopuqoa apssod adoad Kuetu peq aeuq op apeunuog ane a
book, most of whom have had an active role in the development of BPF-related technologies.
---
## Page 36
Acknowledgments
Thanks to: Matheus Marchini (Neflix), Paul Chaignon (Orange), Dale Hamel (Shopify), Ames
Ather (Netflix), Martin Spier (Netflix), Brian W. Kernighan (Google),Joel Fernandes (Google),
Jesper Brouer (Red Hat), Greg Dunn (AWS), Julia Evans (Stripe), Toke Heiland-Jorgensen (Red Hat),
Stanislav Kozina (Red Hat), Jiri Olsa (Red Hat), Jens Axboe (Facebook), Jon Haslam (Facebook),
Andri Nakryiko (Facebook), Sargun Dhillon (Netflix), Alex Maestretti (Netflix),Joseph Lynch
(Netflix), Richard Elling (Viking Enterperise Solutions), Bruce Curtis (Netfix), and Javier Honduvilla
Coto (Facebook). Many sections have been rewritten, added, and improved thanks to all their
help. I also had some help on a couple of sections from Mathieu Desnoyers (EfficiOS) and Masami
Hiramatsu (Linaro). Claire Black also provided a final check and feedback for many chapters.
My colleague Jason Koch wrote much of the Other Tools chapter, and provided feedback on
almost every chapter in the book (hand-annotated on a printed copy about two inches thick.)
The Linux kernel is complicated and ever-changing, and I appreciate the stellar work by Jonathan
Corbet and Jake Edge of Iwn.net for summarizing so many deep topics. Many of their articles are
referenced in the Bibliography.
pue Dg au um sansst tuxg pure saumqeay Aueu Susppe paunbau ose seu xooq stu Suaqdtuo
bpftrace front-ends. Myself and others have written thousands of lines of code to make the tools
in this book possible. A special thanks to Matheus Marchini, Willian Gasper, Dale Hamel, Dan Xu,
and Augusto Caringi for timely fixes.
Thanks to my current and former Netflix managers, Ed Hunter and Coburn Watson, for their
support of my BPF work while at Netflix. Also thanks to my colleagues on the OS team, Scott
Emmons, Brian Moyles, and Gabrielle Munoz, for helping to get BCC and bpftrace installed on
production servers at Netflix, from which I was able to fetch many example screenshots.
Thanks to Deirdre Straughan (AWS), now my wife, for her professional technical editing and
suggestions, and general support of yet another book. My writing has greatly improved thanks to
her help over the years. And thanks to my son Mitchell for support and sacrifices while I was busy
with the book.
This book is inspired by the DTrace book written by myself and Jim Mauro. Jim's hard work
to make the DTrace book a success, and our endless discussions on book structure and tool
presentation, have contributed to the quality of this book. Jim has also made many direct
contributions to this book. Thanks, Jim, for everything.
And a special thanks to Senior Edlitor Greg Doench at Pearson for his help and enthusiasm for this
project.
Working on this book has been an enormous privilege, providing me the opportunity to showcase
BPF observability. Of the 156 tools in this book, 1 developed 135 of them, including 89 new tools
for this book (there are over 100 new tools, counting variants, although it was never my intent
to hit that milestone!). Creating these new tools required research, configuration of application
environments and client workloads, experimentation, and testing. It has been exhausting at
times, but it is satisfying to complete, knowing that these tools will be valuable to so many,
Brendan Gregs
San Jose, California (formerly Sydney, Australia)
November 2019
---
## Page 37
AbouttheAuthor
Brendan Gregg, Netflix senior performance engineer, is a major contributor to BPF (eBPF) who
has helped develop and maintain both main BPF front-ends, pioneered BPF's use for observability,
and created dozens of BPF-based performance analysis tools. His books include the best-seller
Systerms Performance: Enterprise ansl thte Cloud.
---
## Page 38
hapter
Introduction
izes technologies, and demonstrates
some BPF performance tools. These technologies will be e
explained in more detail in the following
chapters.
1.1WhatAreBPFandeBPF?
BPF stands for Berkeley Packet Filter, an obscure technology first developed in 1992 that improved
the performance of packet capture tools [McCanne 92]. In 2013, Alexei Starovoitov proposed
a major rewrite of BPF [2], which was further developed by Alexei and Daniel Borkmann and
included in the Linux kernel in 2014 [3]. This turned BPF into a general-purpose execution
engine that can be used for a variety of things, including the creation of advanced performance
analysis tools.
BPF can be difficult to explain precisely because it can do so much. It provides a way to run mini
programs on a wide variety of kernel and application events. If you are familiar with JavaScript,
suasa 1asmosq uo suezload rupur unu o4 asqam e smoe duoseae[ speus atuos aos Aeu nof
such as mouse clicks, enabling a wide variety of web-based applications. BPF allows the kernel
to run mini programs on system and application events, such as disk I/O, thereby enabling new
system technologies. It makes the kernel fully programmable, empowering users (including non-
kernel developers) to customize and control their systems in order to solve real-world problems.
BPF is a flexible and efficient technology composed of an instruction set, storage objects, and
helper functions. It can be considered a virtual machine due to its virtual instruction set speci
fication. These instructions are executed by a Linux kernel BPF runtime, which includes an
tion. BPF instructions must first pass through a verifier that checks for safety, ensuring that the
interpreter and a JIT compiler for turning BPF instructions into native instructions for execu-
BPF program will not crash or corrupt the kernel (it doesn’t, however, prevent the end user from
writing illogical programs that may execute but not make sense). The components of BPF are
explained in detail in Chapter 2.
So fat, the three main uses of BPF are networking, observability, and security. This book focuses on
observability (tracing).
---
## Page 39
Chapter 1 Introduction
Extended BPF is often abbreviated as eBPF, but the official abbreviation is still BPF without
the °e,” so throughout this book I use BPF to refer to extended BPF. The kernel contains only
one execution engine, BPF (extended BPF), whtich runs both extended BPF and °classic* BPF
programs.1
1.2What AreTracing,Snooping,Sampling,Profiling
andObservability?
These are all terms used to classify analysis techniques and tools.
Tracing is event-based recordingthe type of instrumentation that these BPF tools use. You
spuooau *aduexa sog (t)aoens xnurI 'soop Bupen asodnd-jepods aos pasn peae aneq eu
and prints system call events. There are many tools that do not trace events, but instead measure
events using fixed statistical counters and then print summaries; Linux top(1) is an example.
A hallmark of a tracer is its ability to record raw events and event metadata. Such data can be
voluminous, and it may need to be post-processed into summaries. Programmatic tracers, which
BPF makes poible, can run small programs on the events to do custom on-the-fly statistical
summaries or other actions, to avoid costly post-processing.
While strace(1) has “trace”d in its name, not all tracers do. tcpdump(8), for example, is another
specialized tracer for network packets. (Perhaps it should have been named tcptrace?) The Solaris
operating system had its own version of tcpdump called snoop(1M)*, so named because it was
used to snoop network packets. I was first to develop and publish many tracing tools, and did
aea  so Aoouua _Sudoous, au pasn (4qeara sdepad) I auum sepos uo os
tools. This is why we now have execsnoop(8), opensnoop(8), biosnoop(8), etc. Snooping, event
dumping, and tracing usually refer to the same thing. These tools are covered in later chapters.
BPF when used for observability.
Apart from tool names, the term tracing is also used, especially by kernel developers, to describe
Sampling tools take a subset of measurements to paint a coarse picture of the target; this is also
known as creating a profile or profiling. There is a BPF tool called profile(8) that takes timer-based
samples of running code. For example, it can sample every 10 milliseconds, or, put differently, it
opad naq eq st suardues po slequeape uy (d A1asa uo) puooas aad sadures oot axes ue)
larger set of events. A disadvantage is that sampling provides only a rough picture and can miss
mance overhead can be lower than that of tracers, since they only measure one out of a much
events.
Observability refers to understanding a system through observation, and classifies the tools
that accomplish this. These tools includes tracing tools, sampling tools, and tools based on fixed
counters. It does not include benchmark tools, which modify the state of the system by perform
ing a workload experiment. The BPF tools in this book are observability tools, and they use BPF
for programmatic tracing.
1 Classic BPF programs which refers to the orginsl 8PF (McCnne 92] are automatialy migrated to the eended BPF
engine by the kemel for exeoution. Classic BPF is also not being developed further.
2 For Solaris, section 1M of the man psges is for maintenance and administration commands (section 8 on Linux)
---
## Page 40
1.3 What Are BCC, bpftrace, and I0 Visor?
1.3
3WhatAreBcC,bpftrace,andIoVisor?
It is extremely tedious to code BPF instructions directly, so front ends have been developed that
provide higher-level languages; the main ones for tracing are BCC and bpftrace.
BCC
bpftrace
Python
lua
C++
libboc
libbpf
kernel
Event
Sources
BPF
Figure 1-1 BCC, bpftrace, and BPF
BCC (BPF Compiler Collection) was the first higher-level tracing framework developed for BPE It
provides a C programming environment for writing kernel BPF code and other languages for the
user-level interface: Python, Lua, and C+. It is also the origin of the libbcc and current libbpf
libraries,2 whtich provide functions for instrumenting events with BPF programs. The BCC reposi-
tory also contains more than 70 BPF tools for performance analysis and troubleshooting. You can
install BCC on your system and then run the tools provided, without needing to write any BCC
coxde yourself. This book will give you a tour of many of these tools.
dojasap rog asentue paaa-u8tq ‘asodnd-gepoods e sapaod eq pua quong ramatu e st aoengdq
ing BPF tools. bpftrace code is so concise that tool source code is usually included in this book, to
show what the tool is instrumenting and how it is procesed. bpftrace is built upon the libbcc and
libbpf libraries.
BCC and bpftrace are pictured in Figure 1-1. They are complementary: Whereas bpftrace is ideal
for powerful one-liners and custom short scripts, BCC is better suited for complex scripts and
daemons, and can make use of other libraries, For example, many of the Python BCC tools use the
Python argparse library to provide complex and fine control of tool command line arguments.
pue ugtamu8t aq o pau8isap s # [s] suaudopaaap u s Kjd paeo ‘pua quorg ddg saqouy
require minimal dependencies, which makes it a good fit for embedded Linux environments,
If ply is better suited to your environment than bpftrace, you will nonetheless find this book
3 The first libpf wss devel
loped by Wang Nan for use with perf [4], libbpf is now part of the kemel source
---
## Page 41
4
Chapter 1 Introduction
useful as a guide for what you can analyze with BPE Dozens of the bpftrace tools in this book can
be executed using ply after switching to ply’s syntax. (A future version of ply may support the
bpftrace syntax directly.) This book focuses on bpftrace because it has had more development and
has all the features needed to analyze all targets.
BCC and bpftrace do not live in the kernel code base but in a Linux Foundation project on github
called IO Visor. Their repositories are:
https://github.com/iovisor/bcc
https://github.com/iovisor/bpftrace
Throughout this book I use the term BPF tracing to refer to both BCC and bpftrace tools.
1.4AFirstLookatBCC:QuickWins
Let’s cut to the chase and look at some tool output for some quick wins. The following tool traces
new processes and prints a one-line summary for each one as it begins. This particular tool,
execsnoop(8) from BCC, works by tracing the execve(2) system call, which is an exec(2) variant
(hence its name). Installation of BCC tools is covered in Chapter 4, and later chapters will intro-
duce these tools in more detail.
execsnoop
PCOMX
PID
PPID
RET ARGS
run
129834469
 0 /run
(seg
12983
4469
0/b1n/bash
svstst
12985
12984
0/command/svstat /service/httpd
per1
12986
12984
0 /usz/bin/per1 -e $1=<>μ$1=~/ (\d+) sec/÷print $1110
sd
12988 12987
0 /bin/ps -ppid 1 =o pid, cmd, args
dox6
12989 12987
0 /bln/grep org apache, catalina
sed
1299012987
0/bin/sed s/~ *//;
cut
12991
12987
 g- p- n/uq/xsn/ 0
xsrgs
12992
12987
0/usz/bin/xargs
echo
12993
0/bL.n/echo
mkdic
12994 12983
0/bin/nkdir =v =p /dats/tomcat
nkd1
sdidleqen/veouo/sdde/ d- a- arpxs/vra/ 0
The output reveals which processes were executed while tracing: processes that may be so short-
lived that they are invisible to other tools. There are many lines of output, showing standard Unix
utilities: ps(1), grep(1), sed(1), cut(1), etc. What you can’t see just from looking at this output on
---
## Page 42
1.4 A First Look at BCC: Quick Wins
stamp column:
execsnoop -t
TIHE (s)PCOMH
PID
PPID
RET ARGS
0 , 437
run
15524
4469
0/run
0 . 438
bash
15524
4469
0/bin/bash
0.440
15526
15525