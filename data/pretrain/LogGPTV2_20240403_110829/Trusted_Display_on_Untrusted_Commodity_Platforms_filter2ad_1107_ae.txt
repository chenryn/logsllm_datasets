As shown in Table 4(a), the TD add-on modules (i.e., trap-
per, CPU instruction emulator, fast communication chan-
nel) add a total of 1300 SLOC, which is minuscule num-
ber compared to existing code bases of similar functionality.
For example, CPU instruction emulator contributes most of
this code, and yet it is an order of magnitude smaller than
the code base of general purpose CPU instruction emulation
tools; e.g., diStorm3.312 has 11141 SLOC. To implement this
emulator we stripped the CPU instruction emulator of the
Xen hypervisor (i.e., 4159 SLoC of Xen-4.5.0) of unneces-
sary code and shrank its size by 73.80%; i.e., from 4159 to
1090 SLOC. Were we to use diStorm3.3 instead, we would
have bloated our micro-hypervisor’s code size by over 38%
(11141/(30,243-1,090)) and undoubtedly invalidated the ex-
isting formal assurances of XMHF.
Table 4(b) shows the code size of the TDK. The access me-
diation code for the GPU uses most of the code base since
it contains both the mediation mechanism (1741 SLoC) and
policy (1124 SLoC). A large portion of the code in these
modules (i.e., 2227 SLoC) can be reused across diﬀerent
GPUs, including all the utility code and helper functions
of other modules in the TDK. In particular, supporting dif-
ferent GPUs of the same vendor only requires minimal code
modiﬁcation because the GPU object changes are incremen-
tal; e.g., IronLake speciﬁc code in TDK takes only 178 SLoC.
In contrast, the code size of full GPU virtualization ap-
proaches [46, 45] is much larger. It contains a Xen hypervisor
of 263K SLOC [56] and a privileged root domain that has
over 10M SLOC.
5.2 Performance on micro-benchmarks
In this section, we measure the performance overhead of
commodity OS/Apps and SecApps during a trusted-display
session. Speciﬁcally, we measure the overhead of the access
mediation and access emulation components of the TDK to
evaluate their impact on untrusted commodity OS/Apps.
We also evaluate the overhead added by screen overlays to
SecApps. Finally, we illustrate the overhead incurred by the
TD add-on component of the trusted-display service.
12https://code.google.com/p/distorm/
998Table 6: Overhead of GPU address space separation.
Initialization
GGTT
Shadowing
40.25 ms
Modify
GGTT’
0.04 µs
Run-time
Apply
Invariants
0.10 µs
Page Table
Switch13
11.60 µs
Access mediation. The run-time overhead of access me-
diation is small, though its magnitude varies for diﬀerent
GPU objects. The mediation of access to GPU conﬁgu-
ration registers, GPU page tables, and GPU data adds a
modest performance penalty to commodity OS/Apps dur-
ing SecApp run-time. As shown in Table 5, the TDK spends
2.61 µs on mediating access to a GPU conﬁguration regis-
ter, and 2.69 µs on mediating access to a new GPU page
table mapping, on average. However, the TDK does not
spend any time on GPU data mediation on the ﬂy, because
the sensitive GPU data has been remapped by TD add-on.
On average, access mediation for GPU commands takes 8.86
µs. We note that GPU commands mediation overhead may
vary; e.g. mediation of the GPU batch-buﬀer start command
may require access veriﬁcation for the entire GPU command
batch buﬀer. However, in most cases, batch-buﬀer media-
tion by TDK code is unnecessary since the GPU hardware
protection mechanisms can be used instead.
We also measured the performance of GPU address-space
separation and GPU command protection since they are
important components of the access-mediation mechanism.
Table 6 shows the overhead of GPU address space separa-
tion added to untrusted OS/Apps. GGTT shadowing incurs
the overwhelming portion of the overhead since it needs to
parse every mapping in GGTT and construct the mapping in
GGTT’. Fortunately, this overhead is incurred only at TDK
initialization, and subsequent SecApps display operations
do not require GGTT shadowing. In contrast, the run-time
overhead incurred by untrusted OS/Apps is small, as shown
in Table 6. For GPU command protection, the TDK im-
plementation uses the GPU privilege protection mechanism
and takes 0.07 µs on the average to de-privilege a single
GPU batch buﬀer.
Access emulation.
Similar to access mediation, TDK’s
access emulation has a small runtime overhead, which varies
for diﬀerent GPU objects. For example, TDK’s overhead for
emulating access to GPU page tables is 0.24 µs, on average.
We do not measure access emulation costs of GPU data,
GPU commands, nor GPU conﬁguration registers. The rea-
son for this is that GPU data accesses are not intercepted
by TDK or TD add-on and their access emulation is done
by memory remapping. Although the overhead of GPU
command-access and conﬁguration-register emulation varies
widely with the types of objects and functions used, accesses
to these objects are either infrequent or cause a small over-
head.
Screen overlay.
As anticipated, our experiments con-
ﬁrm that hardware overlays have much better performance
than software overlays. Our test SecApp uses a 100 * 100
display area size with a screen resolution of 1200 * 800. Soft-
ware overlays take 4.10 ms to process a display request of
this SecApp. In contrast, hardware overlays take only 0.03
13We evaluate Haswell GPUs instead, because Intel’s open
source GPU drivers support local page tables on GPUs
newer than IronLake [2]
Table 7: TD add-on overhead.
CPU Instruction
Fast Commun-
ication Channel
3.60 µs
Trapper
11.79 µs
Emulator
5.43 µs
ms, which decreases the overhead of handling a display re-
quest in software by 99.27%. Software overlays decrease
CPU performance for SecApps as the screen resolution in-
creases. For example, software overlays takes 42.59 ms on
displays with 4096 * 2160 resolution, which are becoming in-
creasingly popular. Such an overhead would cause excessive
frame rate drops at 60Hz or higher display refresh cycles,
which would result in visually choppy display images. We
note that the overhead of software overlays increases when
performed in GPU memory due to the reading and writing
of frame buﬀers by the CPU.
TD Add-on. The TD add-on component takes 20.82 µs
to trap and operate on a single sensitive GPU object access
by untrusted OS/Apps. Table 7 illustrates the overhead
breakdown.
First, the trapper takes 11.79 µs, on average, to inter-
cept an MMIO access to GPU object and resume untrusted
OS/Apps execution. (Note that this measurement does not
include the PIO access interception.) This overhead is large
due to the round-trip context switch between the mHV and
untrusted OS/Apps. However, new hardware architectures
make the trapper’s overhead negligible.
Second, the CPU instruction emulator takes 5.43 µs, on
the average, to parse the trapped CPU instruction due to
accessing sensitive GPU objects from untrusted OS/Apps.
However, this emulation overhead is well amortized since
native GPU drivers [2, 6, 4] tend to use a single uniﬁed
function for conﬁguration updates of each type of GPU ob-
jects. Thus, the CPU instruction emulator can cache the
emulation result for future use.
Third, our evaluation results show that the fast communi-
cation channels are much more eﬃcient than switching be-
tween untrusted OS and TDK. In our implementation, the
fast channel takes 3.60 µs for the communication round-trip
between mHV and TDK when they run on diﬀerent cores.
In contrast, when the mHV needs to switch to TDK and
back on the same core, the overhead is 722.42 µs. As a re-
sult, fast communication channels on multi-core systems can
reduce the communication overhead by 99.50%.
5.3 Performance on macro-benchmarks
We use Linux GPU benchmarks to evaluate the perfor-
mance impact of the trusted-display service on commod-
ity OS/Apps software. Speciﬁcally, we use 3D benchmarks
from Phoronix Test Suite [5], including OpenArena, Urban-
Terror, and Nexuiz. We also use Cairo-perf-trace [1] 2D
benchmarks, including ﬁrefox-scrolling (”Firefox”), gnome-
system-monitor (”Gnome”), and midori-zoomed (”Midori”).
However, we did not run LightsMark in 3D benchmark or
ﬁrefox-asteroids in 2D benchmark as in previous studies [46],
because these workloads have not been available to us.
Our evaluation uses three settings that are designed to dis-
tinguish the overhead caused by the underlying mHV from
that caused by the trusted display service. These settings
are: Ubuntu 12.04 with no security component added
(“native”), mHV running without the trusted-display service
(”TD oﬀ”), and a SecApp using the trusted-display service
999ability and maintenance [30, 19].
In contrast, our system
does not require any modiﬁcation of widely available com-
modity OSes.
Other approaches provide trusted display by exclusively
assigning GPU to SecApp. Recent work [48, 51] uses the de-
vice pass-through feature of modern chipsets [8, 24] for this
assignment. Other work [33, 11] isolates the GPU with a
system’s TCB. Recent implementations of trusted path [55,
56] also isolate communication channels from SecApps to
GPU hardware. However, once assigned to a SecApp, the
GPU cannot be accessed by untrusted commodity OS/App
code until the device is re-assigned to that code. Thus,
a commodity OS/App cannot display its content during
SecApp’s exclusive use of the trusted display, unless the
OS/App trusts SecApps unconditionally. Our system solves
this problem by allowing untrusted OS/Apps and SecApps
to use GPU display function at the same time. As a re-
sult, commodity OS/Apps do not need to rely on external
SecApps for display services.
GPU virtualization.
GPU virtualization can provide
trusted-display services by running SecApps in a privileged
domain and untrusted OS/Apps in an unprivileged domain.
The privileged domain can emulate the GPU display func-
tion in software [44, 28] for the untrusted OS/Apps. How-
ever, other GPU functions, such as image-processing emu-
lation, are extremely diﬃcult to implement in software and
take advantage of this setup due to their inherent complex-
ity [46, 13]. As a result, GPU emulation cannot provide all
GPU functions to the untrusted OS/Apps, and hence this
approach is incompatible with commodity software. Smow-
ton [43] paravirtualizes the user-level graphics software stack
to provide added GPU functions to untrusted OS/Apps. Un-
fortunately, this type of approach requires graphics software
stack modiﬁcation inside untrusted OS/Apps, and hence is
incompatible with commodity OS software.
Full GPU virtualization approaches [46, 45] expose all
GPU objects to unprivileged VMs access, and hence al-
low untrusted OS/Apps to use unmodiﬁed GPU drivers.
However, these approaches share all GPU functions between
privileged domain and unprivileged domain, and hence re-
quire complex mediation and provide only low assurance.
Existing full GPU virtualization approaches are subject to
GPU address space isolation attacks, and hence are inade-
quate for trusted-display services. Furthermore, full GPU
virtualization requires extensive emulation of accesses to a
large number of GPU objects, in order to retain compatibil-
ity with the VMs that share the GPU. Our system solves the
problem by sharing only the GPU display function between
the untrusted OS/Apps and the SecApps. Thus, it needs
to mediate only accesses to GPU objects that aﬀect trusted
display’s security. Hence it needs to emulate accesses to a
much smaller set of GPU objects, which helps minimize the
trusted code base for high assurance system development.
Special devices.
High-bandwidth digital content pro-
tection (HDCP) [31, 40] employs cryptographic methods
to protect display content transmitted from GPU to physi-
cal monitor. HDCP requires encryption/decryption circuits,
and hence hardware modiﬁcation of both GPUs and phys-
ical monitors. Similarly, Hoekstra et al. [21] also require
crypto support in GPU to provide trusted display.
Intel
Identity Protection Technology with Protection Transaction
Display is also reported to rely on special CPU and GPU
features [22]. In contrast, our system does not require any
Figure 5: Performance of the trusted display service
on 2D and 3D benchmarks.
running on the top of mHV (”TD on”). The native setting
does not load any of out trusted code; i.e., neither the mHV
nor trusted-display service code. In the TD oﬀ setting, both
the mHV and TD add-on code are loaded, but the TD add-
on code is never invoked because the TDK is not loaded.
Thus, whatever performance overhead arises in the TD oﬀ
setting it is overwhelmingly caused by the security services
of the unoptimized mHV. The TD on setting measures the
overhead of the trusted-display service over and above that
of mHV, and hence is of primary interest.
Figure 5 shows that the TD on setting achieves an av-
erage 53.74% of native performance, while TD oﬀ achieves
59.13%. Thus, the trusted-display service is responsible for
only 10% of the overhead whereas the unoptimized mHV is
largely responsible for the rest. We believe that the perfor-
mance of the mHV (i.e., XMHF) can be improved signiﬁ-
cantly with a modest engineering eﬀort; e.g., mapping large
pages instead of small pages in the nested page table for
CPU physical memory access control, and hence decreasing
the page-walking overhead and frequency of nested page ta-
ble use. We also believe that new hardware architectures,
such as Intel’s SGX, will make the hypervisor overhead neg-
ligible.
Furthermore, the data of Figure 6 show that most frame
jitter is caused by the unoptimized mHV, and the trusted-
display service does not increase the amount of frame jitter.
We obtained these data by measuring the frame latencies
of the OpenArena workload using the tools provided by the
Phoronix Test Suite. These data show that the frame la-
tencies of TD on and TD oﬀ settings are similar, whereas
those of the native and TD oﬀ settings are diﬀerent. Specif-
ically, the standard deviations are 6.62, 14.69, 14.49 for Fig-
ure 6(a), Figure 6(b), Figure 6(c), respectively.
6. RELATED WORK
6.1 Trusted Display
GPU isolation.
Several previous approaches provide
trusted display services using security kernels. For exam-
ple, Nitpicker [17], EWS [42] and Trusted X [15] support a
trusted windowing system. Glider [41] could also be used to
provide a trusted display service since it isolates GPU ob-
jects in the security kernel. However, these approaches are
unsuitable for unmodiﬁed commodity OSes, because security
kernels are object-code incompatible with native commod-
ity OSes. Past research eﬀorts to restructure commodity
OSes to support high-assurance security kernels have failed
to meet stringent marketplace requirements of timely avail-
72.73%47.82%79.86%40.80%66.54%74.24%49.74%91.83%41.43%82.61%40%60%80%100%Native PerformanceTD offTD_on14.70%14.94%0%20%OpenArenaUrbanTerrorNexuizFirefoxMidoriGnome% of N1000(a) Native
(b) TD oﬀ
(c) TD on
Figure 6: Latency evaluation of OpenArena. The vertical axis represents latency in milliseconds and the
horizonal axis represents frame index.
modiﬁcation of existing commodity hardware. It could also
use HDCP to defend against certain hardware attacks; e.g.,
malicious physical monitors.