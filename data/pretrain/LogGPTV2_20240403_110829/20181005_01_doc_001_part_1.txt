CLOUD
VFP
A Virtual Switch Platform for Host SDN in the Public Cloud
DANIEL FIRESTONE
Daniel Firestone is the Tech The Virtual Filtering Platform (VFP) is a cloud-scale programmable
Lead and Manager for the virtual switch providing scalable SDN policy to one of the world’s
Azure Host Networking group
largest clouds, Microsoft Azure. It was designed from the ground up
at Microsoft. His team builds
to handle the programmability needs of Azure’s many SDN applications, the
the Azure virtual switch,
scalability needs of deployments of millions of servers, and to deliver the
which serves as the datapath for Azure
virtual networks, as well as SmartNIC, the fastest virtual networks in the public cloud to Azure’s VMs through hard-
Azure platform for offloading host network ware offloads.
functions to reconfigurable FPGA hardware
We, the VFP team, describe here our goals and motivations in building VFP,
and Azure’s RDMA stack. Before Azure, Daniel
did his undergraduate studies at MIT. fstone@ VFP’s design, and lessons we learned from production deployments. We also
microsoft.com compare our design with that of other popular host SDN technologies such
as OpenFlow [2] and Open vSwitch (OVS) [3] to show how our constraints
in the public cloud can differ from those of popular open source projects. We
believe these lessons can benefit the SDN community at large. More details
of our design can be found in our recent NSDI paper [1].
The rise of public cloud workloads, such as Amazon Web Services, Microsoft Azure, and
Google Cloud Platform, has created a new scale of datacenter computing, with vendors regu-
larly reporting server counts in the millions. These vendors not only have to provide scale
and high density of VMs to customers, but must provide rich network semantics, such as
private virtual networks with customer supplied address spaces, scalable L4 load balancers,
security groups and ACLs, virtual routing tables, bandwidth metering, QoS, and more. This
policy is sufficiently complex that it isn’t feasible to implement at scale in traditional switch
hardware.
Instead this is often implemented using Software-Defined Networking (SDN) on the VM
hosts, in the virtual switch (vswitch) connecting VMs to the network, which scales well
with the number of servers and allows the physical network to be simple, scalable, and very
fast. As a large public cloud provider, Azure has built its cloud network on host-based SDN
technologies. Much of the focus around SDN in recent years has been on building scalable
and flexible network controllers and services—however, the design of the programmable
vswitch is equally important. It has the dual and often conflicting requirements of a highly
programmable dataplane, with high performance and low overhead. VFP is our solution to
these problems.
Design Goals and Rationale
As a motivating example for VFP, we consider a simple scenario requiring four host policies
used for O(1M) VM hosts in a cloud. Each policy is programmed by its own SDN control-
ler and requires both high performance and SR-IOV offload support: the first is virtual
networking, allowing a customer to define their own private network with their own IP
addresses, despite running on shared multi-tenant infrastructure. Our virtual networks
(VNETs) are based on the design from VL2 [4]. Second is an L4 (TCP/UDP connection)
load balancer based on Ananta [5], which scales by running the load balancing NAT in the
vswitch on end hosts, leaving the in-network load balancers stateless and scalable. We also
6 FALL 2017 VOL. 42, NO. 3 www.usenix.org
CLOUD
VFP: A Virtual Switch Platform for Host SDN in the Public Cloud
include a stateful firewall and per-destination traffic metering via a virtual tunnel endpoint (VTEP) schema in OVSDB, rather
for billing. than rules specifying which packets to encapsulate (encap) and
decapsulate (decap) and how to do so.
Originally, we built independent networking drivers for each
of these host functions. As host networking became our main We prefer instead to base all functionality on the MAT model,
tool for virtualization policy, we decided to create VFP in 2011 trying to push as much logic as possible into the controllers
because this model wasn’t scaling. Instead, we created a single while leaving the core dataplane in the vswitch. For instance,
platform based on the Match-Action Table (MAT) model popu- rather than a schema that defines what a VNET is, a VNET can
larized by projects such as OpenFlow. be implemented using programmable encap and decap rules
matching appropriate conditions, leaving the definition of a
Original Goals VNET in the controller. We’ve found this greatly reduces the
Our original goals for the VFP project were as follows: need to continuously extend the dataplane every time the defini-
tion of a VNET changes.
1. Provide a programming model allowing for multiple simultane-
ous, independent network controllers to program network appli-
Later Goals Based on Production Lessons
cations, minimizing cross-controller dependencies.
Based on lessons from initial deployments of VFP, we added the
Implementations of OpenFlow and similar MAT models often following goals for VFPv2, a major update in 2013-14, mostly
assume a single distributed network controller that owns pro- around serviceability and performance:
gramming the switch. Our experience is that this model doesn’t
fit cloud development of SDN—instead, independent teams often 1. Provide a serviceability model allowing for frequent deployments
build new network controllers and agents for those applications. and updates without requiring reboots or interrupting VM con-
This model reduces complex dependencies, scales better, and is nectivity for stateful flows, and strong service monitoring.
more serviceable than adding logic to existing controllers. We As our scale grew dramatically to over O(1M) hosts, more con-
needed a design that not only allows controllers to independently trollers built apps on top of VFP, more engineers joined us, and
create and program flow tables, but enforces good layering and we found more demand than ever for frequent updates, both fea-
boundaries between them (e.g., disallows rules to have arbitrary tures and bug fixes. In Infrastructure as a Service (IaaS) models,
GOTOs to other tables) so that new controllers can be developed we also found customers were not tolerant of taking downtime
to add functionality without old controllers needing to take their for individual VMs for updates.
behavior into account.
2. Provide very high packet rates, even with a large number of
2. Provide a MAT programming model capable of using connections tables and rules, via extensive caching.
as a base primitive, rather than just packets—stateful rules as
Over time we found more and more network controllers being
first-class objects.
built as the host SDN model became more popular, and soon
OpenFlow’s original MAT model derives historically from pro-
we had deployments with large numbers of flow tables (10+),
gramming switching or routing ASICs, and assumes that packet
each with many rules, reducing performance as packets had to
classification is stateless. However, we found our controllers
traverse each table. At the same time, VM density on hosts was
required policies for connections, not just packets—for example,
increasing, pushing us from 1G to 10G to 40G and even faster
end users often found it more useful to secure their VMs using
NICs. We needed to find a way to scale to more policy without
stateful access control lists (ACLs) (e.g., allowing outbound
impacting performance and concluded we needed to perform
connections but not inbound ones) rather than stateless ACLs
compilation of flow actions across tables, and use extensive
used in commercial switches. Controllers also needed NAT (e.g.,
flow caching such that packets on existing flows would match
Ananta) and other stateful policies. Stateful policy is more trac-
precompiled actions without having to traverse tables.
table in soft switches than in ASIC ones, and we believe a MAT
model should take advantage of that. 3. Implement an efficient mechanism to offload flow policy to pro-
grammable NICs, without assuming complex rule processing.
3. Provide a programming model that allows controllers to define
their own policy and actions, rather than implementing fixed sets As we scaled to 40G+ NICs, we wanted to offload policy to NICs
of network policies for predefined scenarios. themselves to support SR-IOV, which lets NICs indicate packets
Due to limitations of the MAT model provided by OpenFlow directly to VMs without going through the host. However, as
(historically, a limited set of actions, limited rule scalability, controllers created more flow tables with more rules, we con-
and no table typing), OpenFlow switches such as OVS have cluded that directly offloading those tables would require pro-
added virtualization functionality outside of the MAT model. hibitively expensive hardware resources for server-class NICs.
For example, constructing virtual networks is accomplished Instead we wanted an offload model that would work well with
www.usenix.org FALL 2017 VOL. 42, NO. 3 7
CLOUD
VFP: A Virtual Switch Platform for Host SDN in the Public Cloud
Figure 1: Overview of VFP design
Figure 3: Example VFP layers with boundaries
our precompiled exact-match flows, requiring hardware to only
support a large table of cached flows in DRAM and our associ- Programming Model
ated action language.
VFP’s core programming model is based on a hierarchy of VFP
objects that controllers can create and program to specify their
VFP Overview
SDN policy, with ports containing layers of policy made up of
Figure 1 shows a model of the VFP design, which is described in
groups of rules.
subsequent sections. VFP operates on top of Hyper-V’s exten-
sible switch as a packet filter. Its programming model is based
Layers
on layers, MATs that support a multi-controller model. VFP’s
VFP divides a port’s policy into layers. Layers are the basic
packet processor includes a fastpath through Unified Flow
Match Action Tables that controllers use to specify their policy.
Tables and a classifier used to match rules in the MAT layers.
They can be created and managed separately by different con-
The core VFP model assumes a switch with multiple ports that trollers. Logically, packets into a VM go through each layer one
are connected to virtual NICs (VNICs). VFP filters traffic from by one, matching rules in each based on the state of the packet
a VNIC to the switch, and from the switch to a VNIC. All VFP after the action performed in the previous layer, with returning
policy is attached to a specific port. From the perspective of a packets coming back in the opposite direction.
VM with a VNIC attached to a port, ingress traffic to the switch
Figure 3 shows layers for our SDN deployment example. A VNET
is considered to be “outbound” traffic from the VM, and egress
layer creates a customer address (CA) / physical address (PA)
traffic from the switch is considered to be “inbound” traffic to
boundary by having encapsulation rules on the outbound path
the VM. VFP’s API and its policies are based on the inbound/
and decapsulation rules on the inbound path. In addition, an
outbound model.
ACL layer for a stateful firewall sits above our Ananta NAT
layer. The security controller, having placed it here with respect
to those boundaries, knows that it can program policies match-
ing dynamic IP addresses (DIPs) of VMs in CA space. Finally, a
metering layer used for billing sits at the top next to the VM, where
it can meter traffic exactly as the customer in the VM sees it.
Figure 2: VFP objects: layers, groups, and rules
Figure 4: A layer with a stateful flow
8 FALL 2017 VOL. 42, NO. 3 www.usenix.org
CLOUD
VFP: A Virtual Switch Platform for Host SDN in the Public Cloud
Header Parameters
Ethernet (L2) Source MAC, Dest MAC
Source IP, Dest IP, ToS
IP (L3)
(DSCP+ EC )
Encapsulation Type Tenant
Encapsulation (L4)
ID, Entropy (Optional)
Source Port, Dest Port, TCP
TCP/UDP (L4) Flags (note: does not support
Push/Pop)
Table 1: Valid parameters for each header type
Action Notes
Figure 5: Example conditions and actions Pop Remove this header.
Layering also gives us a good model on which to implement Push this header onto the packet. All header
stateful policy. We keep flow state on a layer with a hash table Push parameters for creating the new header are
tracking all TCP, UDP, or RDMA connections in either direction. specified.
When a stateful rule is matched, it creates both an inbound and
Modify this header. All header parameters
outbound flow in the layer flow tables, with appropriate actions