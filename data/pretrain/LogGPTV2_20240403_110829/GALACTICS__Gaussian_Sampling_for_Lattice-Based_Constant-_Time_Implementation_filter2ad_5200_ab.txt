2 mod p, µ)
Fig. 3. Description of the BLISS verification algorithm.
2 A TIMING ATTACK ON BLISS
We use statistical learning techniques to recover the second part s2 of the secret key by using either
PCA or either Phase Retrieval algorithm. The main difficulties come from the final compression
that adds a lot of noise to the samples. For some BLISS parameters, the noise is too high for the first
5
Table 1. Concrete parameters for BLISS.
BLISS–
n
q
δ1, δ2
d
κ
α
0
256
7681
.55 , .15
5
12
0.5
I
512
12289
.3 , 0
10
23
1.0
II
512
12289
.3 , 0
10
23
0.5
III
512
12289
.42 , .03
9
30
0.7
IV
512
12289
.45, .06
8
39
0.55
1: function SampleBernCosh(x)
x ← |x|
2:
Sample a ← SampleBernExp(x)
3:
Sample b ← B1/2
4:
Sample c ← SampleBernExp(x)
5:
if ¬a ∧ (b ∨ c) then restart
6:
7:
return a
8: end function
Fig. 4. 1/cosh Bernoulli sampling with countermeasure.
attack to succeed in a complete key recovery. The second attack first uses a maximum likelihood
principle to recover an estimate of the of absolute value of the scalar product given the timing
information. Then, a phase retrieval algorithm is run. However, since the noise is high and the
problem is non-convex, the initialization phase of the gradient descent is crucial. To this end, we
develop a new and refined initialization process improving [29]. Finally, we use a lattice reduction
to remove a few errors on s2.
2.1 Leakage of the cosh sampler
Suppose that, as suggested by the countermeasures of [22], the exponential sampler SampleBern-
Exp is constant time. From the specification of SampleBernCosh and following [16], a natural
implementation of this function would be given as the pseudocode of Figure 4. However, there still
exists a timing leakage from this implementation of the hyperbolic cosine sampler.
to the probability of the expression ¬a ∧ (b ∨ c) to be false, which is
Indeed, by definition of the function SampleBernCosh, the probability of outputting a is equal
p(S, c, z) = 1 − Pr(¬a) Pr(b ∨ c)
2σ 2 (cid:17)(cid:32)
− |⟨z,Sc⟩|
= 1 −(cid:16)1 − e
= 1 − (1 − Pr(a))(1 − Pr(¬b ∧ ¬c))
− |⟨z,Sc⟩|
2σ 2
2
1 − 1 − e
(cid:33)
1 + e
=
− |⟨z,Sc⟩|
σ 2
2
.
Hence by measuring the differences in computation time, one can derive traces that shape (z, c, t),
where t ∈ N is the number of restarts performed before outputting the value a. In the following of
this section, we describe two ways to exploit this leakage, leading to a full key recovery.
6
T end if
i · ciz∗
i
if ti = 0 then W ← ciz∗
1: Collect m traces (zi, ci, ti)
2: for i = 0 to m do
3:
4: end for
5: S ←$ N(0, 1)n; S ← S∥s0 ∥
6: for i = 0 to K then
S ← W −1S; S ← S∥s0 ∥
7:
8: end for
9: return round(
)
S∥S∥N
Fig. 5. First timing attack on Bernoulli sampler.
i
i wiwT
i
(wi = zic∗
in constructing the empirical covariance matrix W =
2.2 Spectral attack with samples with t = 0
Remark that if a trace satisfies t = 0, then it is likely for the geometric distribution parameter
p(S, c, z) to be large, since for t = 0, the likelihood function is exactly p. Therefore, for such a sample,
⟨z, Sc⟩ should be close to zero, i.e., S should be close to orthogonal to the vector zc∗, where c∗ is the
adjoint of c: ⟨z, Sc⟩ = ⟨zc∗, S⟩.
If the vector S was actually orthogonal to each of these zc∗ then it would be enough to collect
sufficiently of them so that they generate an hyperplane H of the ambient space Rn and return
the unique (up to sign) vector of H⊥ of norm compatible with the specification of BLISS (secret
vectors in BLISS all have the same known norm by construction). This would practically translate
) for a series of trace
(zi , ci , 0) and get a basis of its kernel. Remark now that since the secret is not actually orthogonal
to these vectors, the obtained matrix is not singular. To overcome this difficulty we thus do not
seek a vector in the kernel but instead in the eigenspace associated with the smallest eigenvalue of
W . This technique can be seen as a continuous relaxation of the kernel computation in the ideal
case. It translates directly into pseudocode in Figure 5, where the computation of the eigenvector
is performed iteratively and N = ⌈δ1n⌉ + 4⌈δ2n⌉ is the norm of the secret key. Remark that this
technique does not recover exactly the secret but an approximate solution over the reals. To recover
the secret we need to find the closest integral vector to the output candidate, which is simply
done by rounding each coefficient to the nearest integral elements. In addition, remark that by the
contruction of the public key from the secret one, recovering solely s2 is sufficient to reconstruct
the full secret key. Hence the rounding can be carried to 2Z on the second part of the eigenvector
to conclude, as s2 has its coefficients equal to 0, ±2 or ±4 by construction.
2.3 A timing attack by phase retrieval
Exploiting the leakage described in Section 2.1 boils down to retrieve S up to sign from a family
of values of the shape (zi , ci , ti) where ti is sampled under a geometric distribution of parameter
p(S, ci , zi). A natural approach would then consist in starting by estimating the values of p(S, ci , zi)
for each trace (ci , zi , ti), yielding a (noisy) estimate of the absolute value of the inner product
|⟨zi , Sci⟩| = |⟨zic∗
i , S⟩|. In a second time we then fall back on retrieving S from samples of the form
(|⟨wi , S⟩|, wi). This is an instance of so-called (noisy) phase retrieval problem.
First phase: estimation of the phases. In order to get a (noisy) evaluation of the phases,
2.3.1
we devise an estimator of maximum likelihood. Set Li(ω) to be the logarithm of the probability
Pr[|⟨S, wi⟩| = x|t = ω]. We then set the estimator yi to be the arguments of the maximum of Li(ti)
for each trace. Such a computation is classically done using Bayes’ theorem and seeking for critical
values from the derivates of Li(ω).
7
1: A ← [w1 | · · · | wm]
2: s0 ←$ N(0, 1)n
3: for i = 0 to K then
s0 ← AT diag(y1, . . . , ym)As0
4:
s0 ← (AT A)−1s0
5:
6: s0 ← s0∥s0 ∥
7: end for
8: s0 ← s0∥s0 ∥ N
9: return rounding(s0)
Fig. 6. Spectral initializer algorithm.
Second phase: solving the phase retrieval instance. Phase retrieval aims at solving quadratic
2.3.2
equations of the shape
|⟨S, wi⟩|2 = yi
i = 1, . . . , m,
where S is the decision variable, the wi are known sampling vectors and the yi ∈ R are the phase
measurements. The noisy version of this problem consists in retrieving the variable S from noisy
quadratic equations:
|⟨S, wi⟩|2 + ei = yi
i = 1, . . . , m,
for ei independents (usually gaussian) random variables. This problem has been widely studied in
the fields of statistical learning and the most common approach to tackle it consists of a two-step
strategy:
i yiwiwT
i
positive definite symmetric matrix
Initialization via spectral method. First, find a candidate vector s0 that is sufficiently close to
2.3.3
the actual solution to make the second step converges towards the actual solution. The usual way to
initialize the candidate vector can be seen as a generalization of the principal component analysis
(PCA): the initial guess is given via a spectral method; in short, s0 is the leading eigenvector of the
. The intuition behind this method is to remark that
the secret vector will have a greater inner product with the test vectors wi which have a small
angle with it. Hence we want to extract the direction of the wi for which the inner product is
the largest, that is, favorizing the components inducing high yi’s. This corresponds to extract the
largest eigenvalue of the Gram-matrix of the wi, normalized by a diagonal matrix of yi. It is nothing
more than a principal componant analysis on the test vectors wi. In practice, we use a slightly
different version of the (iterative version of the) spectral initializer, outlined in Figure 6, which
provides slightly better practical results than the classical method of [10]. N(0, 1) is the centered
normal reduced distribution, K is a constant, set sufficiently large.
2.3.4 The descent phase. Once an initialization vector is found, we iteratively try to make it closer
to the actual secret by a series of updates like in a gradient descent scheme. Note that in the problem
of phase retrieval the problem is non-convex so that a direct gradient descent would not be directly
applicable. As stated in [10], the phase retrieval problem can be stated as a minimization problem:
ℓ(yr , |⟨wr , x⟩|2), z ∈ Rn,
(1)
where ℓ is a distance function over the reals (such as the Euclidean distance ℓ2(a, b) = (a − b)2).
The corresponding descent, called Wirtinger flow, is then simply stated in Figure 7 where t (cid:55)→ µt is
a step function, which has to be experimentally tailored to optimize the convergence. The value
ϵ > 0 is a small constant that determines the desired precision of the solution.
m
r =1
minimize
1
2m
8
m∥s0 ∥2m
µt
1: t ← 0
2: do
3:
4:
5: while ∥st − st +1∥ > ϵ
6: return S
st +1 ← st −
t ← t + 1
r =1(|⟨wr , st⟩|2 − yr)(wr wt
r)st
Fig. 7. Wirtinger flow descent algorithm.
yi ←(cid:0)argmaxxLi(ti)(cid:1)2
1: Collect m traces (zi, ci, ti)
2: for i = 0 to m do
3:
4: end for
5: s0 ← Spectral initialization (Figure 6)
6: S ← Descent(s0) (Figure 7)
7: return S
Fig. 8. Timing attack on the Bernoulli sampler.
It is well known that minimizing non-convex objectives, which may have very many stationary
points is in general NP-hard. Nonetheless if the initialization s0 is sufficiently accurate, then the
sequence si will converge toward a solution to the problem given by Equation (1).
As in the first attack, the descent algorithm does not directly give an integral solution to the
retrieval problem, so that we eventually need to round the coefficients before outputting the
solution.
The full outline of the attack is given in Figure 8.
2.4 Reducing the number of samples by error localization and dimension reduction
By the inherent noisy nature of the problem, if not enough samples are used to mount the attack,
the recovery might fail on a certain amount of bits. In such a case one cannot figure a priori where
these errors are and would be forced to enumerate the possible errors, using, for instance, the
hybrid MiTM technique of Howgrave-Graham [25]. Since the dimension (n = 512) is large, such an
approach becomes quickly untractable as the number of errors is greater than 8.
However, as the final step of both of the attacks consists of a coefficient-wise rounding, we can
study the distance of each coefficient to 2Z. Heuristically since the descent is supposed ultimately
to converge to the secret, the retrieved coefficients should be close to 2Z. Hence if some of them
are far from this lattice, we can consider them as problematic coefficients and likely to be prone to
induce an error after rounding. Suppose that we discriminate these problematic coefficients in a
finite set T and that each coefficient outside T is correctly retrieved by rounding. Then we can find
the correct value of the coefficients in T by lattice reduction in dimension slightly larger than |T |
by the exploitation of dimension reduction techniques described in [21].
If this dimension is sufficiently small (less than 100 for typical computers), this approach allows
to still perform a full key recovery in cases where the sole descent algorithm would have led to
some errors.
2.5 Practicality of the attacks and discussion
We provide the attack scripts in [4] and summarize in Table 2 the number of samples required to
perform a full key recovery with both of the attacks. The first column corresponds to the first attack
9