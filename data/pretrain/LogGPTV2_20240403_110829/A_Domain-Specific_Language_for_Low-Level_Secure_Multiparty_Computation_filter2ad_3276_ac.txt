x2i + 1
i=0 xi = (cid:81)∞
1/ f = 1/(1 − x) = (cid:80)∞
This already gives us a suitable ﬂoating-point representation for
2 f ∈ (1/2, 1). The case
every f ∈ (1/2, 1) because in such case 1
when the fractional part of the input is 1/2 or close to it is self-
correcting as our algorithm rounds the result down and prevents
it from overﬂowing. This gives us the recipe for computing the
inverse of a ﬂoating-point number. We ﬁrst compute 1 − f and
interpret it as a ﬁxed-point number with a single binary digit be-
fore the radix point. To do that we divide − f by 2 (negation and
division computed for an unsigned integer). A ﬁxed-point format
with a single binary digit before the radix point allows us to repre-
sent values in the range [0, 2) and the extra digit is needed because
inverse yields us a value in the range [1, 2).
By setting x = 1 − f we can compute 1/ f using the equality
. Evaluating just the
ﬁrst k terms of the product gives the maximum error of about 2−2k
at x = 1/2. This means that for a single-precision ﬂoating-point
number it is suﬃcient to only compute the ﬁrst 5 terms. To get the
fractional part of the result all that is left to do is to evaluate that
expression on ﬁxed-point numbers. Note that this approximates
1/ f as a ﬁxed-point number with one digit before the radix point,
but reinterpreting that as a ﬁxed-point number with no digits before
the radix point yields the approximation for 1/(2 f ).
To ﬁnd 1/ f we need to compute powers of x = 1 − f and multi-
ply the terms incremented by one to approximate the wanted value.
Therefore, ﬁxed-point multiplication is needed. Let u and v be
(1 + n)-bit ﬁxed-point numbers with a single digit before the radix.
To compute the product u ∗ v (assuming that the result does not
overﬂow) we extend both of the numbers to 1 + 2n bits, multiply
them and then cut away n least signiﬁcant digits of the result. This
is a rather expensive operation: to extend the numbers we need to
compute their overﬂow bits and to cut away least signiﬁcant digits
we again need to check if those digits overﬂow. The overﬂow bits
have to be computed because extending a number to a larger one
has the same problem that we already faced with extending a single
bit integer to a larger one. One of the ways to compute the overﬂow
bit is to tailor the bit extraction protocol for this purpose (a more
eﬃcient method is provided in [8]).
If we know, ahead of time, that we are performing some multipli-
cations in a row, for example, when computing a product of several
numbers, we can optimize the computation by eliminating the need
to extend the numbers before every multiplication. If we know that
we are performing exactly r multiplications on u we can instead
immediately extend it to 1 + (1 + r)n bits and on every successive
multiplication remove the lowest n bits.
The implementation for ﬂoating-point inverse is presented in List-
ing 8. We have used but not deﬁned various helper functions:
a) publicShiftr for shifting an additively shared value right by
some public value; b) choice for obliviously choosing between two
type float[n,m] = uint[1] * uint[n] * uint[m]
def bias : unit -> uint[m]
def floatInv : n > m > 3 => float[n,m] -> float[n,m] =
\N -> let
(s, f, e) = N
x = publicShiftr (-f) 1 // x = 1 - f
f’ = fixInv x
e’ = share((bias() + 1)  0 => uint[n] -> uint[n] = \x ->
let
x : uint[n + 5*(n - 1)] = extend x
one = share (1  0 =>
uint[n+(r+1)*(n-1)] ->
uint[n+(r+2)*(n-1)] -> uint[n] =
\acc xPow -> let
xPow : uint[n+(r+1)*(n-1)] = cut (square xPow)
one = 1 << ‘(n - 1)
acc = cut (mult acc (xPow + one))
in if (r == 0) acc else fixInv acc xPow
Listing 8: Floating-point reciprocal protocol
additively shared integers; c) share for sharing a public value by
having two of the parties pick 0 as their shares; d) cut for cutting
away some least signiﬁcant bits of an additively shared integer; and
e) extend for converting an additively shared integer to a larger one.
Floating-point numbers are represented by a triple consisting of a
1-bit sign, n-bit fractional part and m-bit exponent. The type syn-
onym float[n,m] is provided for this. The function bias returns
the bias for an m-bit exponent (we omit the deﬁnition), the function
fixInv computes the inverse of an n-bit ﬁxed-point number with a
single digit before the radix point and ﬁnally floatInv computes
the inverse of a ﬂoating point number.
If the input had a fractional part very close to 1 then 1/(2 f ) is
very close to 1/2. During the computation this may be rounded
down and the highest bit can become 0 resulting in a denormalized
ﬂoat. To avoid this, we need to check the highest bit of the to-be
fractional part – we denote it with b. This is computed by shifting
the fractional part right by n−1 bits. If the highest bit turns out to be
0 then we know that the input had a fractional part very close to 1
and the result was rounded down too much during the computation.
In this case we correct both the resulting fraction and the exponent.
Initially we implemented the reciprocal protocol using ﬁxed-
point polynomial evaluation technique as in [23]. However, the
protocol DSL enabled us to rapidly try out diﬀerent implementa-
tions and optimizations and we quickly found out that the approach
presented here is superior to polynomial evaluation both in speed
and in precision. Implementing the protocol in optimized manner
in our C++ framework would have been a major undertaking.
3. THE CORE PROTOCOL LANGUAGE
In this section we will formalize the core of the protocol DSL.
The code examples presented previously do not match the syntax
provided here perfectly but can be translated to the core language
Size literals
Party nr.
Sources
Expressions
c
p
q
e
∈ N0
∈ N1
::= p | Prev | Next
::= x | λx. e | Λα. e
|
e1 e2 | e τ | let x = e1 in e2
|
if C then e1 else e2
|
case: e1, . . . , ek | e from q
M ::=  | def x : σ = e M
::= ε | C1 ∧ C2 | s1 ∼ s2 | s1 < s2
::= α | unit | bit | arr[τ, s] | τ1 → τ2
|
::= ∀α.C ⇒ τ
c | s1 + s2 | s1 ∗ s2 | s1/s2
Programs
Constraints C
τ, s
Monotypes
Size types
Polytypes
σ
Figure 1: Syntax of the core protocol DSL
with relatively little eﬀort. For some constructs we have provided
syntactic rewriting rules. A major diﬀerence is that the core lan-
guage does not infer type parameters automatically and expects ex-
plicit type applications. In the compiler implementation the type
arguments are inferred whenever possible during type checking.
The syntax of the language is presented in Fig. 1. Expressions e
of the language include the standard constructs for lambda calcu-
lus: variables, function applications, lambda-abstractions and let-
expressions. In addition to that the language includes conditional
expressions over size predicates, case-expressions for branching
depending on the computing party, and from-expressions for per-
forming network communication. An expression e is always evalu-
ated by a set of computing parties that may communicate between
each other. By default, all the parties evaluate the same expres-
sion, but every computing party does not always hold a result for
the given expression. We will see that case-expressions allow the
computation to branch depending on the evaluating party but may
also omit the value for some parties. The let-expressions are sim-
ilar to those in ML. They are not recursive and shadow (override)
previous variable deﬁnitions with the same name.
The from-expression e from q is used for network communica-
tion and states that the current evaluating party gets the value of e
from source q. For example, for the second computing party the ex-
pression x from 1 evaluates to the ﬁrst computing party’s value of
the variable x. The source q is not only restricted to concrete parties
but may also denote the next or the previous computing party.
The conditional expression if C then e1 else e2 evaluates to e1
if type constraint C holds and otherwise evaluates the expression e2.
When type checking a branch the fact that C does or does not hold
may be used depending on the branch. The protocol DSL compiles
to an intermediate representation (IR) with no branching constructs,
meaning that the source code may only contain loops that are stat-
ically bounded. The mixture of supporting type-level integers and
providing the ability to branch over them facilitates writing recur-
sive code and cleanly segregates values that might only be dynam-
ically known (regular values) from values that are deﬁnitely stati-
cally known (types). Recall that Sharemind protocols are always
instantiated to concrete bit-widths.
The case-expression case: e1, . . . , ek evaluates to ei for the i-th
party where k is the number of computing parties. This construct
looks quite diﬀerent to what we have seen in the examples above
but high-level code can be straightforwardly trasnalted to this form.
If a case-expression has any uncovered cases we can add them by
mapping to undeﬁned values. As an example, a case expression
with no branches has no value for any of the computing parties (it
is undeﬁned everywhere). A set of parties can be implemented by
binding the expression to a fresh variable, replacing the expressing
with the variable and duplicating that branch for each party.
A program M of the language consists of a sequence of variable
deﬁnitions. All top-level bindings must be annotated with types
and the deﬁnitions may be mutually recursive unlike the regular
let-expressions.
3.1 Type system
The type system of the language is inspired by Cryptol [31]. We
have opted for strict and static type checking with type inference: a
classic Hindley-Milner type system [13] extended with type con-
straints (predicates) over type-level natural numbers. A regular
type τ is either a variable α, the unit type having only a single
value, the bit type having two values 0 and 1, an array arr[τ1, s]
of length s containing elements of type τ1, or a function τ1 → τ2
taking arguments of type τ1 and returning values of type τ2. Be-
cause most protocols operate on integer values we use uint[n] as
a synonym for an array of n bits for the sake of conciseness and
readability. The language additionally supports n-ary tuples and
data structures, but we have omitted them here as they are a rela-
tively straightforward addition to the language. The protocol DSL
has two diﬀerent kinds of types: regular data types, and size types
for denoting lengths of arrays. A size type s is either a variable n, a
natural number c ∈ N0, or an arithmetic expression of size types.
The type system is simply an instantiation of OutsideIn(X) where
X has been chosen to be integer constraints. The general type
checking algorithm is described in [41]. The type checking of the
protocol DSL is made easier by our requirement to annotate all
polymorphic types: top-level bindings must be annotated and no
generalization is performed when type checking let-expressions.
Note that the type system of the language is deﬁnitely not com-
plete: in order to type check arbitrary programs we need to be able
to solve arbitrary (non-linear) systems of equations. In practise this
has not turned out to be a hindrance as almost all of the constraints
are very simple and easily dispatched by Z3 [16] SMT solver.
3.2 Semantics
The semantics of the language is relatively straightforward by
exploiting the fact that programs of the language must always ter-
minate. There are two kinds of values in the protocol DSL: func-
tional values (either value or type abstraction), and tuples of primi-
tive values where the i-th component denotes the value that the i-th
party has. When we say that some value is undeﬁned everywhere
we mean that it is a tuple consisting of bottom values ⊥.
The semantics is in small-step style. The transition rules are ei-
ther from expression to another e p−→ e(cid:48) or from expression to a
value e p−→ v. All of the transitions are annotated with probabilities
(omitted if equal to 1). The meaning of constraints (cid:126)C(cid:127) ∈ {0, 1} is
deﬁned in the obvious manner.
The evaluation rules for the three party case are given in Fig. 2.
Mostly they are straightforward lambda-calculus rules: we evalu-
ate expressions under the evaluation context C and substitute vari-
ables in case of function application and let-expression. Evaluation
is performed strictly except for lambda (or type) abstractions and
if-expressions. The from-expression rearranges the components
of the tuple (in syntax we have i ≡ (cid:104)i, i, i(cid:105), Next ≡ (cid:104)2, 3, 1(cid:105) and
Prev ≡ (cid:104)3, 1, 2(cid:105)). For case-expression all subexpressions are eval-
uated and then correct components are picked out of the branches.
The generation of a random bit by three parties chooses each pos-
sible set of three bits with equal probability.
Implementing this semantics would result in an extremely ineﬃ-
cient evaluator. It would constantly compute values that will never
be used (due to the case-construct dropping them) and often prop-
e
C[e]
(λx. e)v → e[x (cid:55)→ v]
p−→ e(cid:48)
p−→ C[e(cid:48)]
(Λα. e)τ → e[α (cid:55)→ τ]
{v1, v2, v3} from (cid:104)i, j, k(cid:105) → {vi, v j, vk}