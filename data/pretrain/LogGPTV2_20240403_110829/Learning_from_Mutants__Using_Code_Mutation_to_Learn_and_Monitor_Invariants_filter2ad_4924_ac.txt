0, s(cid:48)
x + (d/t) of T r;
Run simulator S on conﬁguration π for d time
units to yield trace T r(cid:48);
π(cid:48)(cid:48) := (cid:104)s(cid:48)(cid:48)
0 , s(cid:48)(cid:48)
d/t of T r(cid:48);
if π(cid:48) (cid:54)= π(cid:48)(cid:48) then
1 , . . .(cid:105) for all sensor values s(cid:48)(cid:48)
i at row
i at row
N e := N e ∪ {(π, π(cid:48))}
x := x + 1;
21 return P o, N e;
not cause an immediate change. It is crucial not to mislabel
normal data as abnormal—additional ﬁltering is required.
Applied to SWaT. Algorithm 3 summarises how feature
vectors are collected from the SWaT simulator and its mutants.
Collecting positive feature vectors is very simple: all possible
pairs of physical states (π, π(cid:48)) are extracted from the normal
traces. For each pair (π, π(cid:48)) extracted from the abnormal
traces, the unmodiﬁed simulator is run on π for d time units:
if the unmodiﬁed simulator leads to a state distinguishable
from π(cid:48), the original pair is collected as a negative feature
vector; if it leads to a state that is indistinguishable from
π(cid:48), it is discarded (since the mutation had no effect). In the
case of SWaT, its simulator is deterministic, allowing for this
judgement to be made easily. (For data from the testbed, some
acceptable level of tolerance would need to be deﬁned.)
Sub-step (iii): Learning. Once the feature vectors are col-
lected, a supervised ML algorithm can be applied to learn a
model.
Applied to SWaT. For the SWaT simulator, we choose to
apply SVM as our supervised ML approach since it is fully
automatic, with well-developed active learning strategies, and
good library support (we use LIBSVM [22]). Furthermore,
SVM has expressive kernels and has often been successfully
applied to time series prediction [23]. Based on the training
data, SVM attempts to learn the (unknown) boundary that sep-
arates it. Different classiﬁcation functions exist for expressing
this boundary, ranging from ones that attempt to ﬁnd a simple
linear separation between the data, to non-linear solutions
based on RBF (we compare different classiﬁcation functions
for SWaT in Section IV-A). For the purpose of validating the
classiﬁer and assessing its generalisability, it is important to
train it on only a portion of the feature vectors, reserving a
portion of the data for testing. We randomly select 70% of the
feature vectors to use as the training set, reserving the rest for
evaluation.
We remark that SVM can struggle to learn a reasonable
classiﬁer if the data is very unbalanced. This is the case for the
SWaT simulator: we have just one simulator for normal data,
but potentially inﬁnite mutant simulators for generating ab-
normal data. To ensure balance, we undersample the negative
feature vectors. Let NP o denote the number of positive feature
vectors and NN e the number of negative feature vectors we
collected. We partition the negative feature vectors into subsets
of size NN e/NP o (rounded up to the nearest integer), and
randomly select a feature vector from each one. This leads to
an undersampled set of negative feature vectors that is roughly
the same size as the positive feature vector set.
C. Third Step: Validating the Classiﬁer
At this point, we have collected normal and abnormal data,
processed it into positive and negative feature vectors, and
learnt a classiﬁer by applying a supervised ML approach. This
ﬁnal step is to determine whether or not there is evidence
that the learnt model can be considered a physical invariant
of the CPS. It consists of the following two sub-steps: (i)
applying standard ML cross-validation to assess how well the
classiﬁer generalises; and (ii) apply SMC to determine whether
or not there is statistical evidence that the classiﬁer does indeed
characterise an invariant property of the system.
Sub-step (i): Cross-Validation. Our ﬁrst validation method is
to apply standard ML k-fold cross validation (with e.g. k = 5)
to assess how well the classiﬁer generalises. This technique
computes the average accuracy of k different classiﬁers, each
obtained by partitioning the training set
into k segments,
training on k − 1, and validating on the segment remaining
(repeating with respect to different validation partitions).
Sub-step (ii): Statistical Model Checking. The second valida-
tion method applies SMC, a standard technique for verifying
general stochastic systems [8]. The variant we use observes
executions of the system (i.e. traces of sensor data), and applies
hypothesis testing to determine whether or not the executions
provide statistical evidence of the learnt model being an invari-
ant of the system. SMC estimates the probability of correctness
rather than guaranteeing it outright. It is simple to apply, since
it only requires that we can execute the (unmodiﬁed) system
and collect data traces. It treats the system as a black box, and
thus does not require a model [24].
Given some classiﬁer φ for a system S, we apply SMC
to determine whether or not φ is an invariant of S with a
probability greater or equal to some threshold θ, i.e. whether φ
correctly classiﬁes the traces of S as normal with a probability
greater than θ. Note that the usefulness of invariants is a
separate question, addressed in Section IV-E. A classiﬁer
that always labels normal and abnormal data as normal, for
example, is an invariant, but not a useful one for detecting
attacks.
Applied to SWaT. In the case of the SWaT simulator, we
generate a normal data trace from a new, distinct initial conﬁg-
uration, and collect the positive feature vectors from it. Next,
we randomly sample feature vectors from this set, evaluate
them with our classiﬁer, and apply SPRT as our hypothesis test
to determine whether or not there is statistical evidence that
the classiﬁer labels them correctly (setting the error bounds at
a standard level of 0.05) with accuracy greater than some θ. If
further data is required, we sample additional positive feature
vectors from another distinct initial conﬁguration. We remark
that we choose θ to be the accuracy of the best classiﬁer
we train in our evaluation (Section IV-D). These steps are
repeated several times, each with data from additional new
initial conﬁgurations.
IV. EVALUATION
We evaluate our approach through experiments intended to
answer the following research questions (RQs):
• RQ1: What kind of classiﬁcation function do we need?
• RQ2: How large should the time interval
in feature
vectors be?
• RQ3: How many mutants do we need?
• RQ4: Is our model a physical invariant of the system?
• RQ5: Is our model useful for detecting attacks?
RQ1–3 consider the effects of different parameters on the
performance of our learnt models, in particular, the classi-
ﬁcation function (linear, polynomial, or RBF), the different
time intervals for constructing feature vectors, and the number
of mutants to collect abnormal
traces from. We take the
best classiﬁer from these experiments, and assess for RQ4
whether or not there is statistical evidence that the model
characterises an invariant of the system. Finally, for RQ5, we
investigate whether or not the model is useful for detecting
various different attacks that manipulate the network and PLC
programs.
All the experiments in the following were performed on the
SWaT simulator [13]. The mutation and learning framework
we built for this simulator (as described in Section III) is avail-
able to download [13], and uses version 3.22 of LIBSVM [22]
to apply SVM to our feature vectors1.
A. RQ1: What kind of classiﬁcation function do we need?
Our ﬁrst experiment is to determine which of the main
SVM-based classiﬁcation functions—linear, polynomial (de-
gree 3), or RBF—we should use in order to learn models
with an acceptable level of accuracy. Intuitively, a simple
model is more useful for human interpretation, but it may not
be expressive enough to achieve high classiﬁcation accuracy.
First, we generate 700 mutant simulators, of which 91 are
effective (i.e.
led to some abnormal behaviour). From 20
initial conﬁgurations of the SWaT simulator, as described in
Section III-A, we generate 30 minute traces (at 5ms intervals)
of normal and abnormal data from the original simulator and
mutant simulators respectively. From these data traces, we
collect 1.68 ∗ 106 feature vectors with a 250ms time interval
type, using undersampling to account for the larger quantity
of abnormal data (see Section III-B). These vectors are then
randomly divided into two parts: 70% for training, and 30%
for testing. SVM is applied to the training vectors to learn
three separate linear, polynomial, and RBF classiﬁers.
Table II presents a comparison between the three classiﬁers
learnt in the experiment. We report two types of accuracy. The
accuracy column reports how many of the held-out feature
vectors (i.e. the 30% of the collected feature vectors held
out for testing) are labelled correctly by the classiﬁer. The
cross-validation accuracy is the result of applying k-fold
cross-validation (with k = 5) to the training set: this is the
average accuracy of ﬁve different classiﬁers, each obtained
by partitioning the training set
training on four
partitions, and validating on the ﬁfth (then repeating with a
different validation partition). This measure helps to assess
how well our classiﬁer generalises. Sensitivity expresses the
proportion of positives that are correctly classiﬁed as such;
speciﬁcity is the same but for negatives. Across all four
measures, a higher percentage is better.
into ﬁve,
From our results, it is clear that the RBF-based classiﬁer far
outperforms the other two options. While RBF scores highly
across all measures, the other classiﬁcation functions lag far
behind at around 60 to 70%; they are much too simple for
the datasets we are considering. Intuitively, we believe linear
or polynomial classiﬁers are insufﬁcient because readings
of different sensors in SWaT are correlated in complicated
ways which are beyond the expressiveness of these kinds
of classiﬁers. Given this outcome, we choose RBF as our
classiﬁcation function.
B. RQ2: How large should the time interval in feature vectors
be?
Our second experiment assesses the effect on accuracy
of using different time intervals in the feature vectors. As
discussed before, a feature vector is of the form (π, π(cid:48)) where
π denotes the water tank levels at a certain time and π(cid:48) denotes
1Additional rounds of experiments on different mutants were performed
post-publication. The results [13] are consistent with our conclusions here.
COMPARISON OF CLASSIFICATION FUNCTIONS
TABLE II
type
SVM-linear
SVM-polynomial
SVM-RBF
accuracy
63.34%
67.10%
91.05%
cross-validation accuracy
64.12%
68.32%
90.99%
sensitivity
66.44%
speciﬁcity
60.23%
74.92%
99.28%
51.67%
82.82%
the levels after d time units. Intuitively, using these feature
vectors, the learnt model characterises the effects of mutants
after d time units. On the one hand, an abnormal system
behaviour is more observable if this interval d is larger (as the
modiﬁed PLC control program has more time to take effect).
On the other hand, having an interval that is too large runs
the risk of reporting abnormal behaviours too late and thus
potentially resulting in some safety violation.
Table III presents the results of a comparison of accuracy
and cross-validation accuracy (both deﬁned as for RQ1) across
classiﬁers based on 100, 150, . . . 300 ms time intervals. SVM-
RBF was used as the classiﬁcation function, and abnormal data
was generated from 700 mutants.
The results match the intuition mentioned earlier, although
the accuracy stabilises much more quickly than we initially
expected (at around 150ms time intervals). The time interval
of 250ms has, very slightly, the best accuracy, so we continue
to use it in the remaining experiments.
C. RQ3: How many mutants do we need?
Our third experiment assesses the effect on accuracy from
using different numbers of mutant simulators to generate
abnormal data. We are motivated to ﬁnd the point at which
accuracy stabilises, in order to avoid the unnecessary compu-
tational overhead associated with larger numbers of mutants.
Table IV presents a comparison of accuracy and cross-
validation accuracy (both deﬁned as for RQ1) across classi-
ﬁers learnt from the data generated by 300, 400, 500, 600, and
700 mutants. Our mutant sets are inclusive, i.e. the set of 700
mutants includes all the mutants in the set of 600 in addition
to 100 distinct ones. We also list how many of the generated
mutants are effective, in the sense that they can be compiled,
run, and cause some abnormal physical effect with respect to
at least one of the initial conﬁgurations. We used SVM-RBF
as the classiﬁcation function, collecting feature vectors (see
Section III-B) with a time interval of 250ms.
The results indicate that both accuracy and cross-validation
accuracy start to stabilise in the 90s from 500 mutants (62
effective mutants) onwards. It also shows that with fewer
mutants (e.g. 300 mutants / 23 effective mutants) it is difﬁcult
to learn a classiﬁer with acceptable accuracy. Given the results,
we choose 600 as our standard number of mutants to generate.
D. RQ4: Is our model a physical invariant of the system?
Our fourth experiment is to establish whether or not there
is statistical evidence supporting that the learnt model is a
(physical) invariant of the system, i.e. it correctly classiﬁes
the data in normal traces as normal with accuracy greater or
equal to some threshold θ. We perform SMC as described in
Section III-C, sampling positive feature vectors derived from
a new and distinct initial conﬁguration, setting the acceptable
error bounds at a standard level of 0.05, and setting the
threshold as θ = 91.04% (i.e. the accuracy of the classiﬁer
learnt from 600 mutants and a feature vector interval of
250ms). Our implementation performs hypothesis testing using
SPRT, randomly sampling feature vectors and applying the
classiﬁer until SPRT’s stopping criteria are met. If the sampled
data is not enough, we sample additional feature vectors from
the traces of additional new initial conﬁgurations.
Our SMC implementation repeated the overall steps above
ﬁve times, each with normal data derived from a different
distinct initial conﬁguration (falling within normal operational
ranges). In each run, our classiﬁer passed, without requiring
data to be sampled from traces of additional conﬁgurations.
This provides some evidence that the classiﬁer is an invariant
of the SWaT simulator. This is not surprising: in Section IV-A
we found that the sensitivity of the classiﬁer was very high
(99.28%), i.e. the proportion of positive feature vectors that
it classiﬁed as such was very high. Our SMC implementation
evaluates for the same property but seeks statistical evidence.
E. RQ5: Is our model useful for detecting attacks?
Our ﬁnal experiment assesses whether our learnt invariant
is effective at detecting different kinds of attacks, i.e. whether
it classiﬁes feature vectors as negative once an attack has
been launched. First, we investigate network attacks, in which
an attacker is assumed to be able to manipulate network
packets containing sensor readings (read by PLCs) and signals
(read by actuators). Second, we investigate code-modiﬁcation
attacks (i.e. manipulations of the PLC programs), by randomly
modifying the different PLC programs in the simulator and de-
termining whether any resulting physical effects are detected.
If able to detect the latter kind of attacks, the invariant can be
seen as physically attesting the integrity of the PLC code.
Network attacks. Table V presents a list of network attacks
that we implemented in the SWaT simulator, and the results
of our invariant’s attempts at classifying them. Our attacks
are from a benchmark of attacks that were performed on the