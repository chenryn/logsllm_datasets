OT-based
CrypTFlow2
GAZELLE
DELPHI
CrypTFlow2
OT-based
CrypTFlow2
Comm.
(MB)
17.45
617
116.6
2,108
22.8
718.5
150
5,063.7
54
2,033.9
354
6,292.1
297.1
10,489
1,831
13,104
603.1
22,199.4
3,582.8
23,857
873.1
29,433
5,141
32,804
Network Model: ResNet-101
Network Model: ResNet-152
TABLE X.
RUNTIME COST OF CLASSIC MODEL.
Approach
SecureML
MiniONN
GAZELLE
DELPHI
CrypTFlow2
Comm.
(MB)
0.21
4.4
0.21
84
12.4
Network Model: MLP
LAN (ms)
Time
31.9
14.1
15
204.5
246
Speedup
2.6×
1×
1×
3.1×
2.3×
WAN (ms)
Time
79.3
227.6
84.9
3658.3
780.6
Speedup
1.5×
1×
1×
1×
1.2×
B. Performance with Classic Networks
In this section, we benchmark the GALA performance
on a 4-layer Multi Layer Perceptron (MLP)11 which is also
adopted in other privacy preserving frameworks including
GAZELLE, SecureML [48] and MiniONN [44] as a baseline
network, as well as state-of-the-art neural network models
including AlexNet [40], VGG [62], ResNet-18 [32], ResNet-
50 [32], ResNet-101 [32], and ResNet-152 [32]. We use
MNIST dataset [3] for the MLP and CIFAR-10 dataset [1]
for state-of-the-art networks.
Table IX shows computation complexity of the proposed
GALA compared with GAZELLE. We can see that GALA
reduces GAZELLE’s Perm by 34×, 31×, 30×, 47×, 39×,
and 36× for AlexNet, VGG, ResNet-18, ResNet-50, ResNet-
101, and ResNet-152, respectively. The fundamental base for
this speedup lies in GALA’s deep optimization for HE-based
linear computation. We also notice that GALA achieves limited
11The network structure is 784-128-128-10.
reduction of Perm in MLP (from 70 to 55). This is due to
the small ratio between the number of slots in the ciphertext
and output dimension in each layer, i.e, n
, which limits the
no
performance gain. The limited gain is also observed in Table X
which shows the system speedup of GALA over GAZELLE,
CrypTFlow2, DELPHI, SecureML and MiniONN. Speciﬁcally,
GALA boosts CrypTFlow2 by 2.3× in the LAN setting.
SecureML also gains 2.6× in the LAN setting. Meanwhile,
GALA’s performance is similar to GAZELLE and MiniONN.
The is due to the relatively small network size and noticeable
communication overhead (i.e., the large round time in total
compared with computation cost). Nevertheless, none of the
competing schemes achieves a better performance than GALA.
It
that
is worth pointing out
the MLP network is not
widely adopted in practical scenarios. On the other hand, as
the state-of-the-art deep neural networks utilize large channels
and small-size kernels to capture data features while the size
of feature maps is large, GALA is especially effective for
accelerating such large state-of-the-art network models.
Table XI shows the runtime of GAZELLE, DELPHI and
CrypTFlow2, and the speedup of GALA on top of each. By
reducing HE operations, especially Perm operations, GALA
achieves noticeable boost over the GAZELLE, DELPHI and
CrypTFlow2 frameworks. Speciﬁcally, the results show that
GALA boosts GAZELLE by 2.5× (from 11s to 4.3s), 2.7×
(from 18s to 6.5s), 3.2× (from 43s to 13s), 8.3× (from 276s
to 33s), 7.7× (from 486s to 62s), and 7.5× (from 659s to
87s) in LAN setting, on AlexNet, VGG, ResNet-18, ResNet-
12
TABLE XII.
PERCENTAGES OF LINEAR COMPUTATION IN
STATE-OF-THE-ART NEURAL NETWORK MODELS.
Networks
AlexNet
VGG
ResNet-18
ResNet-50
ResNet-101
ResNet-152
GAZELLE
97.7
98.2
98.3
98.5
98.4
98
DELPHI
76.9
77.9
75.1
55.2
53.2
52
CrypTFlow2
98.7
98.8
98.6
96.8
96.5
96.4
Plaintext
98.5
98.1
98.9
97.9
98.3
98.4
50, ResNet-101, and ResNet-152, respectively.
CrypTFlow2 (CCS’20) is the latest framework for pri-
vacy preserved neural networks. It optimizes the nonlinear
operations of DELPHI, and adopts a similar HE scheme of
DELPHI for linear operations. GALA is an efﬁcient plug-and-
play module to optimize the linear operations of CrypTFlow2.
As shown in the Tables VI and VIII, GALA’s optimization
of linear operations can further boost CrypTFlow2 by 700×
and 7.4× for matrix-vector multiplication and convolution
in the LAN setting, respectively. This speedup stems from
GALA’s streamlined HE calculation compared with the one
of CrypTFlow2. Slow-down is observed in the WAN setting,
but CrypTFlow2 can still gain up to 6.5× speedup for convo-
lution due to the computation-intensive nature for large input
channels with small kernel dimensions featured in state-of-the-
art network models. As for the overall system speedup, GALA
can boost CrypTFlow2 by 6.5×, 6×, 5.7×, 4.5×, 4.2×, and
4.1× in LAN, and by 4.8×, 4.6×, 4.3×, 2.9×, 2.8×, and 2.7×
in WAN, based on the aforementioned network architectures.
It might appear anti-intuitive that while CrypTFlow2 is
a more recent system than DELPHI, the speedup of GALA
over DELPHI is smaller than its speedup over CrypTFlow2.
This is because CrypTFlow2 has optimized the nonlinear part
of DELPHI, signiﬁcantly reducing its runtime. As a result,
the runtime of linear operations in CrypTFlow2 accounts for
a very high percentage as illustrated in Table XII. Hence
CrypTFlow2 can beneﬁt more from GALA’s optimization of
linear computation, resulting in a higher speedup in terms of
the overall runtime. It is worth pointing out that the ability
to accelerate CrypTFlow2 is highly desirable since it is the
latest privacy-preserving framework. Meanwhile, we also show
GALA’s speedup on top of the OT-based CrypTFlow2 which
relies on OT to complete the linear computation. As signiﬁcant
communication cost,
is involved in
OT,
the overhead of linear computation, especially in the
WAN setting, increases compared with HE-based CrypTFlow2,
which results in greater speedup achieved by GALA.
including round cost,
Next we examine the runtime breakdown of different layers
for those six state-of-the-art networks as shown in Fig. 9,
which allows detailed observation. Note that the layer indexing
here is slightly different from the original plaintext model
for the sake of HE operations, e.g., the nonlinear activation
or pooling following a convolution operation is counted as a
separate layer. The x-axis of each subﬁgure in Fig. 9 shows
the layer index of a sequence of linear (convolution or matrix-
vector multiplication) and nonlinear (activation or pooling)
layers that constitute each network model. The y-axis plots the
accumulated running time (milliseconds) up to a layer, and the
speedup of GALA over GAZELLE in each layer.
For example, Fig. 9 (a) illustrates the result for AlexNet.
TABLE XIII.
ACCURACY WITH FLOATING AND FIXED POINT IN
STATE-OF-THE-ART NEURAL NETWORK MODELS. TOP-1 ACCURACY: ONLY
THE PREDICTION WITH THE HIGHEST PROBABILITY IS A TRUE LABEL;
TOP-5 ACCURACY: ANY ONE OF THE FIVE MODEL PREDICTIONS WITH
HIGHER PROBABILITY IS A TRUE LABEL.
Network Models
AlexNet
VGG
ResNet-18
ResNet-50
ResNet-101
ResNet-152
Top1
Fix-point
Floating-point
Top1
Top5
Top5
78.89% 97.32% 78.43% 97.26%
92.09% 99.72% 92.05% 99.68%
93.33% 99.82% 93.21% 99.81%
93.86% 99.85% 93.86% 99.84%
94.16% 99.79% 94.12% 99.79%
94.23% 99.81% 94.15% 99.79%
The most time-consuming computation in GAZELLE is in
layer “6”, “8” and “10”, which are all convolution com-
putations. This is evidenced by the large jump of runtime
from these layers to the next layer. GALA decreases the time
for these linear computations by nearly 3×. Meanwhile, the
nonlinear layers (activation/pooling) have a speedup of 1, as
GALA has the same computation cost as GAZELLE in those
layers. Since the nonlinear computation contributes to only
a small portion of the total cost, it does not signiﬁcantly
affect the overall performance gain of GALA that focuses
on accelerating the linear computation. Note that GALA does
not beneﬁt much in the ﬁrst layer of AlexNet, i.e., the ﬁrst
convolution, as the input has only three channels. However,
the speedup for the following more costly convolutions allows
GALA to effectively reduce the overall cost. A similar obser-
vation can be seen from the result on VGG. As for the four
ResNets frameworks, the most signiﬁcant performance gain
stems from the convolution with 1×1 kernels. As ResNets
the blocks with multiple 1×1 convolution kernels,
repeat
GALA effectively accelerates this type of convolution due to
its deeply optimized linear computation mechanism (see details
in Sec. III-B), thus reducing the overall runtime. Similar trend
is observed for DELPHI and CrypTFlow2.
It is also worth mentioning that GALA focuses on optimiz-
ing the HE-based linear operations only and can be integrated
into a baseline model (such as GAZELLE, CryptFlow2, or
DELPHI). The proposed approach does not introduce approx-
imation. Hence it does not result in any accuracy loss com-
pared to the baseline privacy preserved model. Furthermore,
compared with the original plaintext model, the only possible
accuracy loss in GALA comes from the quantiﬁcation of
ﬂoating point numbers to ﬁxed point numbers in the HE op-
erations. Such quantiﬁcation is indispensable in all HE-based
frameworks including CryptFlow2. From our experiments, the
model accuracy loss due to quantiﬁcation is negligible, as
shown in Table XIII.
V. CONCLUSION AND FURTHER DISCUSSIONS
This paper has focused on a deep optimization on the HE-
based linear computation in privacy-preserved neural networks.
It aims to minimize the Perm operations, thus substantially
reducing the overall computation time. To this end, we have
proposed GALA: Greedy computAtion for Linear Algebra,
which views the HE-based linear computation as a series of
Homomorphic Add, Mult and Perm operations and chooses
the least expensive operation in each linear computation step
to reduce the overall cost. GALA has made the following
13
Fig. 9. Layer-wise accumulated runtime and GALA speedup over GAZELLA on different networks: (a) AlexNet; (b) VGG; (c) ResNet-18; (d) ResNet-50; (e)
ResNet-101; (f) ResNet-152. The bar with values on the left y-axis indicates speedup, and the curve with values on the right y-axis indicates the accumulated
runtime. The layers with speedup of 1 are nonlinear layers.
contributions: (1) It has introduced a row-wise weight matrix
encoding with combined share generation (i.e., row-encoding-
share-RaS (Rotated and Sum)) to reduce the Perm operations
for dot product. (2) It has designed a ﬁrst-Add-second-Perm
approach (named kernel grouping) to reduce the Perm oper-
ations for convolution. As such, GALA efﬁciently reduces
the cost for the HE-based linear computation, which is a
critical building block in almost all of the recent frameworks
for privacy-preserved neural networks, including GAZELLE,
DELPHI, and CrypTFlow2. With its deep optimization of
the HE-based linear computation, GALA can be a plug-and-
play module integrated into these systems to further boost
their efﬁciency. Our experiments show that GALA achieves
a signiﬁcant speedup up to 700× for the dot product and
14× for the convolution computation under different data
dimensions. Meanwhile, GALA demonstrates an encouraging
runtime boost by 2.5×, 2.7×, 3.2×, 8.3×, 7.7×, and 7.5×
over GAZELLE and 6.5×, 6×, 5.7×, 4.5×, 4.2×, and 4.1×
over CrypTFlow2, on AlexNet, VGG, ResNet-18, ResNet-50,
ResNet-101, and ResNet-152, respectively.
14
It
is worth pointing out
that even with the signiﬁcant
progress toward privacy preserved machine learning in recent