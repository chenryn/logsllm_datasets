28
28
All (132)
x
28
#
exp
552
325
113
468
2400
The  sensitivity  of  the  robustness  measure  to 
the
parameter  corruption  technique  can  be  further  analyzed,
using a bit-flip parameter corruption technique, referred to
as  FL4.  We  use  it  here  to  corrupt  the  same  set  of  75
parameters in a systematic  way (i.e.,  flipping  the  32  bits
of  each  parameter  considered).  This  leads 
to  2400
corrupted values (i.e.,  2400 experiments). The results  are
given  in  Figure  4  for Windows 2000.  This  figure shows
that  the  OS  robustness  is  very  similar  using  the  two
parameter  corruption  techniques,  which  confirms  our
previous work on fault representativeness [20].
We conclude that  the  results  obtained for a  subset  of
system calls related to the  most  frequently used functions
of  Windows  (corresponding  to  Processes  and  Threads,
File 
and
Configuration  Manager)  are  similar  to  those  obtained
Input/Output,  Memory  Management 
when considering all  system  calls.  This  is  why  we  have
targeted these four functions for the Windows family.
OS Hang/Panic
OS Exception
11.4%
0.0%
OS Hang/Panic
OS Exception
10.6%
0.0%
OS Error Code
34.1%
Windows 2000
No Signaling
54.5%
OS Error Code
44.0%
Windows 2000
No Signaling
45.4%
     Selective substitution (552 exp.)          Systematic bit-flip (2400 exp.)
Figure 4: Sensitivity to corruption technique
3.2.2.  OS  Reaction  Time.  Table  6  completes  the
information provided in Table 3.  It  gives  the  OS  reaction
time  with  respect  to  OS  outcomes  after  execution  of  a
corrupted system  call.  It  can be seen  that  i)  the  time  to
issue  an error code is  very short  and  comparable for  the
three systems, ii) the time to signal an exception is  higher
than that of error code return, but  it  is  still  acceptable for
Windows NT4 and XP,  but very large for Windows  2000
and iii)  the  largest execution  time  is  obtained  when  the
OS does not signal the error (SNS).
Table 6: Detailed OS reaction times
Windows 2000
Windows NT4
SD
Mean
Mean
SD
28 µs
22 µs
18 µs
17 µs
2978 µs
138 µs
86 µs
973 µs
281 µs 2013 µs
203 µs
4147 µs
Windows XP
SD
Mean
17 µs
23 µs
162 µs
108 µs
165 µs
204 µs
Error code
Exception
No signaling
The  very  high  standard  deviation  (SD)  is  due  to  a
large variation around the mean. As  an example,  Figure  4
shows  this  variation  in  the  case  of  SNS.  This  figure
identifies the system calls that led  to  SNS  with  the  mean
execution  time  of  each  of  them.  The  large  standard
deviation is mainly due to two system calls.
µs
a
b
Windows NT4 Windows 2000 Windows XP
a: 10416 µs
b: 8205 µs
600
500
400
300
200
100
0
GlobalAlloc
G etStartupInfoA
G etProcessVersion
G etExitCodeThread
DuplicateHandle
G etPrivateProfileStringA
CreateThread
G etPrivateProfileIntA
CreateRe m oteThread
FreeEnviron m entStrings W
GlobalFree
GlobalUnlock
LocalAlloc
IsBadReadPtr
IsBad W ritePtr
GlobalLock
LocalReAlloc
LocalFree
SetThreadPriority
VirtualAllocEx
ReadFile
W riteFile
Figure 5: OS reaction time in case of SNS
3.2.3  OS  Restart Time.  Careful analysis  of  the  collected
data revealed a correlation between the system  restart time
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:02 UTC from IEEE Xplore.  Restrictions apply. 
and  the  state  of  the  workload.  When  the  workload  is
completed,  the  mean  restart  time  is  very  close  to  τres
(obtained without fault injection), and  when the  workload
is aborted or hangs, the restart time is  8%  to  18%  higher.
Indeed, the  number of  experiments  that  led  to  workload
abort/hang  was  respectively  101,  107  and  128  for
Windows NT4, 2000 and XP. Even  though  Windows XP
had induced more workload abort/hang outcomes,  it  still
has the lowest system restart time as indicated in  Table 7.
The latter gives in rows 1  and 2  the  restart times  without
faults, τres, and in presence of  faults,  Tres, and refines in
the  last  two  rows  Tres  according  to  the  workload  state,
irrespective of the OS outcome.
Table 7: Restart time and workload state
Wind. NT4 Wind.  2000 Wind.  XP
92 s
96 s
95 s
102 s
105 s
109 s
106 s
123 s
74 s
80 s
76 s
90 s
τres
Tres
Tres after WL completion
Tres after WL abort/hang
4.Conclusion
In this  paper we have briefly presented a dependability
benchmark  for  OSs  and  an  example  of  implementation
prototype, then we have used the  prototype to  benchmark
Windows NT4, 2000 and XP.
The benchmark addresses the user perspective. The OS
is  considered  as  a  black  box  and  the  only  required
information  is  its  description  in  terms  of  services  and
functions (system calls). We emphasize the OS robustness
as regards application induced erroneous behavior.
The comparison of  the  three OSs  showed that  i)  they
are equivalent from the robustness  point  of  view and that
ii) Windows  XP  has  the  shortest  reaction  and  restart
times.  Detailed  information  provided  by  the  current
benchmark  prototype 
the
benchmark  measures  and  confirmed 
the  benchmark
measure results.  Sensitivity  analyses  with  respect  to  the
parameter corruption technique showed that,  even though
for each  OS  the  robustness  is  slightly  impacted  by  the
technique used, the three OSs are impacted similarly.
refinement  of 
allowed 
Finally,  the  results  obtained  showed  that  using  a
reduced  set  of  experiments  (113)  targeting  only  out-of-
range data led  to  results  similar  to  those  obtained  from
the 552 initial experiments targeting additionally  incorrect
data  and  addresses.  If  this  is  confirmed  for  other  OS
families,  this  would  divide  the  benchmark  execution
duration 
the  number  of
experiments) by almost 5, which is substantial.
is  proportional 
(that 
to 
References
[1]
T.  K.  Tsai,  R.  K.  Iyer  and  D.  Jewitt,  “An  Approach  Towards
Benchmarking  of  Fault-Tolerant  Commercial  Systems”,  in  Proc.  26th
Int.  Symp.  on  Fault-Tolerant  Computing  (FTCS-26),  Sendai,  Japan,
1996, pp. 314-323.
[2] A.  Brown,  “Availability  Benchmarking  of  a  Database  System”,
EECS Computer Science Division,  University  of  California  at  Berkley,
2002.
[3]
J.  Zhu,  J.  Mauro  and  I.  Pramanick,  “R3  -  A  Framwork  for
Availability Benchmarking”,  in  Int.  Conf.  on  Dependable  Systems  and
Networks (DSN 2003), San Francisco, CA, USA, 2003, pp. B-86-87.
[4] K.  Kanoun,  J.  Arlat,  D.  J.  G.  Costa,  M.  Dal Cin,  P.  Gil,  J.-C.
Laprie,  H.  Madeira  and  N.  Suri,  “DBench  –  Dependability
Benchmarking”,  in  Supplement  of  the  2001  Int.  Conf.  on  Dependable
Systems and Networks (DSN-2001), Göteborg, Sweden, 2001,  pp. D.12-
15.
[5] K. Kanoun, H. Madeira, Y. Crouzet, M. Dal  Cin,  F.  Moreira  and
Ruiz J.-C, “DBench Dependability Benchmarks”, LAAS-report  no.  04-
120, 2004.
[6] A.  Kalakech,  T.  Jarboui,  A.  Arlat,  Y.  Crouzet  and  K.  Kanoun,
“Benchmarking Operating Systems Dependability:  Windows  as  a  Case
Study”,  in  2004  Pacific  Rim  International  Symposium  on  Dependable
Computing (PRDC 2004), Papeete, Polynesia, 2004, pp. 262-271.
[7] A.  Mukherjee  and  D.  P.  Siewiorek,  “Measuring  Software
Dependability  by  Robustness  Benchmarking”,  IEEE  Transactions  of
Software Engineering, vol. 23, no. 6, pp. 366-378, 1997.
[8]
P.  Koopman  and  J.  DeVale,  “Comparing  the  Robustness  of
POSIX Operating Systems”, in Proc.  29th  Int.  Symp.  on  Fault-Tolerant
Computing (FTCS-29), Madison, WI, USA, 1999, pp. 30-37.
[9] C.  Shelton,  P.  Koopman  and  K.  Devale,  “Robustness  Testing  of
the Microsoft Win32 API”, in. Int. Conference  on  Dependable  Systems
and Networks (DSN’2000), New York, NY, USA, 2000, pp. 261-270.
[10] J.  Durães  and  H.  Madeira,  “Characterization  of  Operating
Systems Behavior  in  the  Presence  of  Faulty  Drivers  through  Software
Fault  Emulation”,  in  2002  Pacific  Rim  Int.  Sym.  on  Dependable
Computing, Tsukuba City, Ibaraki, Japan, 2002, pp. 201-209.
[11] A.  Chou,  J.  Yang,  B.  Chelf,  S.  Hallem  and  D.  Engler,  “An
Empirical  Study  of  Operating  Systems  Errors”,  in  Proc.  18th  ACM
Symp.  on  Operating  Systems  Principles  (SOSP-2001),  Banff,  AL,
Canada, 2001, pp. 73-88.
[12] A.  Albinet,  J.  Arlat  and  J.-C.  Fabre,  “Characterization  of  the
Impact  of  Faulty  Drivers  on  the  Robustness  of  the  Linux  Kernel”,  in
Int. Conf. on Dependable Systems and Networks  (DSN  2004),  Florence,
Italy, 2004.
[13] D. A. Solomon and M. E.  Russinovich,  Inside  Microsoft  Windows
2000, Third Edition, 2000.
[14] TPC-C, TPC Benchmark  C,  Standard  Specification  5.1,  available
at http://www.tpc.org/tpcc/. 2002.
[15] M.  Vieira  and  H.  Madeira,  “Definition  of  Faultloads  Based  on
Operator Faults  for  DBMS  Recovery  Benchmarking”,  in  2002  Pacific
Rim International Symposium on Dependable Computing,  Tsukuba  city,
Ibaraki, Japan, 2002.
[16] K. Buchacker, M. Dal Cin, H. J. Höxer, R. Karch, V. Sieh and  O.
Tschäche,  “Reproducible  Dependability  Benchmarking  Experiments
Based  on  Unambiguous  Benchmark  Setup  Descriptions”,  in  Int.  Conf.
on  Dependable  Systems  and  Networks,  San  Francisco,  Ca,  2003,
pp. 469-478.
[17] P.  J.  Koopman,  J.  Sung,  C.  Dingman,  D.  P.  Siewiorek  and  T.
Marz, “Comparing Operating  Systems  using  Robustness  Benchmarks”,
in  Proc.  16th  Int.  Symp.  on  Reliable  Distributed  Systems  (SRDS-16),
Durham, NC, USA, 1997, pp. 72-79.
[18] G.  Hunt  and  D.  Brubaher,  “Detours:  Binary  Interception  of
Win32  Functions”,  in  3rd  USENIX  Windows  NT  Symposium,  Seattle,
Washington, USA, 1999, pp. 135-144.
[19] M.  Vieira  and  H.  Madeira,  “A  Dependability  Benchmark  for
OLTP  Application  Environments”,  in  29th  Int.  Conference  on  Very
Large Data Bases (VLDB 2003), Berlin, Germany, 2003, pp. 742-753.
[20] T.  Jarboui,  J.  Arlat,  Y.  Crouzet,  K.  Kanoun  and  T.  Marteau,
“Analysis of the Effects of Real and Injected Software Faults:  Linux  as
a  Case  Study”,  in.  2002  Pacific  Rim  Int.  Symposium  on  Dependable
Computing (PRDC 2002), Tsukuba, Japan, 2002, pp. 51-58.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:02 UTC from IEEE Xplore.  Restrictions apply.