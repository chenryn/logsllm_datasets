the implemented system obtains very good results, detecting all the synthetic
anomalies.
References
1. Lakhina, A., Crovella, M., Diot, C.: Characterization of network-wide anomalies in
traﬃc ﬂows. In: ACM Internet Measurement Conference, pp. 201–206 (2004)
2. Lakhina, A.: Diagnosing network-wide traﬃc anomalies. In: ACM SIGCOMM, pp.
219–230 (2004)
3. The Internet2 Network, http://www.internet2.edu/network/
Detecting the Onset of Infection for Secure
Hosts
Kui Xu1, Qiang Ma2, and Danfeng (Daphne) Yao1
1 Department of Computer Science, Virginia Tech
{xmenxk,danfeng}@cs.vt.edu
2 Department of Computer Science, Rutgers University
PI:EMAIL
Abstract. Software ﬂaws in applications such as a browser may be ex-
ploited by attackers to launch drive-by-download (DBD), which has be-
come the major vector of malware infection. We describe a host-based
detection approach against DBDs by correlating the behaviors of human-
user related to ﬁle systems. Our approach involves capturing keyboard
and mouse inputs of a user, and correlating these input events to ﬁle-
downloading events. We describe a real-time monitoring system called
DeWare that is capable of accurately detecting the onset of malware
infection by identifying the illegal download-and-execute patterns.
Analysis based on the arrival methods of top 100 malware infecting the most
number of systems discovered that 53% of infections are through download [1]. In
another study, 450,000 out of 4.5 millions URLs were found to contain drive-by-
download exploits that may be due to advertisement, third-party contents, and
user-contributed contents [2]. Drive-by-download (DBD) attacks exploit the vul-
nerabilities in a browser or its external components to stealthily fetch malicious
executables from remote malware-hosting server without proper permission of
the user.
We present DeWare – a host-based security tool for detecting the onset of
malware infection at real time, especially drive-by-download attacks. Deware
is application independent, thus it is capable of performing host-wide moni-
toring beyond the browser. DeWare’s detection is based on observing stealthy
download-and-execute pattern, which is a behavior virtually all active malware
exibits at its onset.
However, the main technical challenge to successful DBD detection is to tell
DBDs apart from legal downloads. Our solution is based on monitoring relevant
ﬁle-system events and correlating them with user inputs at the kernel level. In
contrast to DBDs, legitimate user download activities are triggered by explicit
user requests. Also, browser itself may automatically fetch and create temporary
ﬁles which are not directly associated with user actions. To that end, we grant
browser access to limited folders with additional restrictions.
Security and attack models. We assume that the browser and its components
are not secure and may have software vulnerabilities. The operating system
is assumed to be trusted and secure, and thus the kernel-level monitoring of
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 492–493, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
Detecting the Onset of Infection for Secure Hosts
493
ﬁle-system events and user inputs yields trusted information. The integrity of
ﬁle systems deﬁned in our model refers to the enforcement of user-intended or
user-authorized ﬁle-system activities; the detection and prevention of malware-
initiated tampering.
DeWare Archietecture Overview. The DeWare monitoring system is de-
signed to utilize a combination of three techniques, including input logger, sys-
tem monitor, and execution monitor. Following are the main components.
– Input logger that intercepts user inputs at the kernel level with timestamp
and process information (i.e., to which process the inputs go to). User in-
puts are viewed as trusted seeds in our analysis, which are used to identify
legitimate system behaviors.
– System logger which intercepts system calls for ﬁle creations, and probes
kernel data structures to gather process information. Timestamps can be
obtained from input logger at runtime to perform temporal correlation.
– Access control framework that speciﬁes (1)accessible area: where an applica-
tion is allowed to make ﬁle creations, (2)downloadable area: places a user can
download ﬁles into via an application.
– Execution monitor which gives additional inspection to areas where access
is granted to an application or user downloads.
Capturing all ﬁle-creation events related to processes generates an overwhelm-
ingly large number of false alarms. The purpose of our access control framework
is to reduce the white noise, by granting a process access to certain folders, which
are deﬁned as accessible area. For example, Temporary Internet Files folder is
modiﬁable by IE – in contrast, system folder is not. Execution monitor is to
prevent malware executables from being run at accessible area.
Prototype Implementation in Windows Our implementation and experi-
ments are built with Minispy, a kernel driver for Windows operating systems. It
is able to monitor all events where system is requesting to open a handle to a ﬁle
object or device object, and further ﬁnd out the ﬁle creations. Logged activities
are reported to user mode where the access control policy, input correlation, ﬁle
extension check are performed. We record user inputs at the kernel level through
hooks SetWindowsHookex provided by Windows OS. The execution monitor is re-
alized with Microsoft PsTools and the process tracking in local security settings.
We have carried out a study with 22 users to collect real-world user download
behavior data. We will also use DeWare to evaluate a large number of both
legitimate and malware-hosting websites for testing its detection accuracy.
References
1. Macky Cruz. Most Abused Infection Vector,
http://blog.trendmicro.com/most-abused-infection-vector/
2. Provos, N., McNamee, D., Mavrommatis, P., Wang, K., Modadugu, N.: The ghost
in the browser analysis of web-based malware. In: Hot-Bots 2007: Proceedings of
the First Conference on First Workshop on Hot Topics in Understanding Botnets.
USENIX Association, Berkeley (2007)
Eliminating Human Specification in Static Analysis* 
Ying Kong, Yuqing Zhang**, and Qixu Liu 
National Computer Network Intrusion Protection Center, GUCAS, Beijing 100049, China 
Tel.: +86-10-88256218; Fax: +86-10-88256218 
PI:EMAIL 
State Key Laboratory of Information Security, GUCAS, Beijing 100049, China 
Abstract. We present a totally automatic static analysis approach for detecting 
code injection vulnerabilities in web applications on top of JSP/servlet frame-
work.  Our  approach  incorporates  origin  and  destination  information  of  data 
passing in information flows, and developer’s beliefs on vulnerable information 
flows extracted via statistical analysis and pattern recognition technique, to in-
fer specifications for flaws without any human participation. According to ex-
periment, our algorithm is proved to be able to cover the most comprehensive 
range of attack vectors and lessen the manual labor greatly. 
Published  static  approaches  for  detecting  code  injection  vulnerabilities  depend  on 
human work heavily to specify flaws and to build auditing model. This leads to much 
omission in tagging attack vectors due to the erratic nature of human judgment, fur-
thermore,  the  omission  in  flaw  report.  In  this  paper,  we  present  a  novel  approach 
named injection vulnerability checking tool (IVCT) to solve this problem. 
We consider the attack against code injection vulnerability as an improper commu-
nication procedure among three components including the  front-end  web server, the 
back-end database and the underlying operating system. Return from method invoked 
on web server forms the message, and is finally accepted by another method invoked 
on one of the three components. We treat the former method as taint source, and the 
latter as vulnerable receiver. Data flow of the message, which starts with taint source 
and ends at vulnerable receiver, is regarded as possible candidate of vulnerable flow 
in this paper. Such model covers the most comprehensive range of attack vectors. 
IVCT framework consists of four phases,  which are illustrated in Fig 1. We take 
advantage  of  the  slicing  technique  [1]  described  in  [2]  to  track  propagation  of  un-
trusted  input,  and  enhance  the  dataflow  analysis  with  indirect  propagation  which 
models the relationship between the data passing into and out of a library method and 
abstracts away the concrete operations on data therein. Such abstraction is based on 
the  insight  that  most  library  code  won’t  modify  data  structure  from  customer  code. 
Before tracking, just those sensitive components’ jar paths are required be specified in 
advance  to  locate  the  candidate  information  flows.  During  tracking,  we  can  collect 
tainted information propagated via library invocation directly instead of tracking into 
the  implementation.  For  example,  in  the  statement  “str2=a.fun(str1)”,  data  “str1”  
* This work is supported by the National Natural Science Foundation of China under Grant No. 
60970140, No.60773135 and No.90718007. 
** Corresponding author. 
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 494–495, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Eliminating Human Specification in Static Analysis 
495 
Table 1. Experimental Results Comparing with 
TAJ and bddbddb 
Fig. 1. IVCT Workflow 
passes into library invocation “a.fun”, then reference variable “a” and “str2” will be 
treated as tainted data passing out of the invocation. Such enhancement are expected 
to simplify the tracking process, and hence to improve the scalability. 
We  manually  inspected  two  web  applications  “Webgoat5.3RC”  and  “blojsom-
3.3b”, both of which are used by tools TAJ in Tripp [2] and bddbddb in [3] for ex-
periment data. In the analysis, IDE “Eclipse” is utilized to locate grammar element of 
java  code,  the  rest  operations  are  rigorously  adhered  to  IVCT’s  instructions.  There-
fore, no human judgments have been involved into the inspection. According to the 
experimental results illustrated in Table 1, our approach is proved to be better in two 
factors.  First,  no  human  participation  is  required  by  IVCT.  In  contrast,  TAJ  and 
bddbddb require checkers to read the libraries used by targeted web applications thor-
oughly  to  flag  taint  sources  and  sinks.  Second,  IVCT  captures  more  vulnerabilities 
with fewer false positives. We own the bigger number to the fact that IVCT’s candi-
date flows cover all the attack vectors. In fact, every method returning variable possi-
ble to carry string value in web server library is potential taint source, but TAJ limits 
taint  source  only  in  form  input  and  upload  file  data.  Additionally,  variables  propa-
gated by sinks’ reference variables are potential vulnerable receivers. However, such 
propagation is ignored by both [2] and [3]. In the future, we plan to implement our 
approach in a tool to be used in real code. In addition, try to extract other beliefs bur-
ied in program code which can be used as flaw specification. 
References 
1.  Sridharan, M., Fink, S.J., Bodik, R.: Thin slicing. In: ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation, vol. 42(6), pp. 112–122 (2007) 
2.  Tripp, O., Pistoia, M., Fink, S., Sridharan, M., Weisman, O.: TAJ: Effective Taint Analysis 
of Web Applications. In: ACM SIGPLAN Conference on Programming Language Design 
and Implementation, pp. 87–97 (2009) 
3.  Livshits, V.B., Lam, M.S.: Finding security vulnerabilities in Java applications with static 
analysis. In: The 14th USENIX Security Symposium, pp. 271–286 (2005) 
Evaluation of the Common Dataset Used in
Anti-Malware Engineering Workshop 2009
Hosoi Takurou and Kanta Matsuura
Institute of Industrial Science, The University of Tokyo
4-6-1, Komaba, Meguro-ku, Tokyo 153-8585, Japan
Abstract. Anti-Malware Engineering Workshop 2009 provided a com-
mon dataset for all the authors there. In order to understand research-
promotion eﬀects in the network-security community, we evaluate the
dataset through observations and a questionnaire.
Keywords: malware, evaluation dataset, network security.
1 Introduction
Evaluation by using datasets is a major approach in network security due to the
diﬃculty of theoretical evaluation. If a common dataset is available, we can have
more reliable comparison among diﬀerent technologies. And if the dataset is bet-
ter maintained, the absolute quality of each evaluation gets better. A Japanese
domestic workshop on network security, called anti-Malware engineering Work-
Shop 2009 (MWS2009) [3], was challengingly designed in a way that all the 28
authors should use a common dataset (CCC DATAset 2009 [2]). In order to un-
derstand eﬀects of this challenge on research promotion, we evaluate the dataset
through observations and a questionnaire.
2 Observations
A well-known example of commonly available datasets is DARPA dataset [1]
which is basically for intrusion-detection evaluation. By contrast, CCC DATAset
2009 has a more comprehensive nature with the following three classes of data:
(S) malware specimen information (on 10 malwares),
(T) attack traﬃc data (by 2 hosts, 2 days long), and
(A) attack source log (by 94 hosts, 1 year long).
These data were captured at 94 honeypots during one year from the middle of
2008 to the middle of 2009, and were provided along with the dataset used in
the previous year’s edition of MWS. This comprehensiveness is an advantage;
the more researchers join the chellenging workshop, the higher the productivity
of the challenge is.
Another remarkable feature of the dataset is operational eﬀorts for organiz-
ing the workshop (e.g. carefully-designed contracts among stakeholders). The
realization of the workshop itself and its sustainability (in fact, the Japanese
community is preparing MWS2010) suggests beneﬁts from this feature.
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 496–497, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
Evaluation of the Common Dataset in MWS2009
497
3 Questionnaire-Based Evaluation
3.1 Questionnaire
We sent a questionnaire to all the users of the dataset, and received 27 responses.
The questionnaire consists of 89 questions, which are on the role of replying
person and other administrative aspects (8 questions), on technical aspects in
general (14 questions), on the data of class (S) (17 questions), on the data of
class (T) (29 questions), and on the data of class (A) (21 questions). The large
number of questions on technical aspect were designed in a systematic manner;
many of them ask “expectation before use” as well as “evaluation after use”,
and “absolute evaluation considering their demand” as well as “comparison of
the absolute evaluation with their own dataset (i.e. not the common dataset but
the dataset which the researcher prepared by themselves)”.
3.2 Result
Due to the page limitation, we here show some remarkable results only.
The rate of deployment: The ratio of the number of users who used each
class of data to the number of users who planned to use them before starting
their research are: (cid:17)8/11(cid:18) for the class (S), (cid:17)17/20(cid:18) for the class (T), and
(cid:17)10/13(cid:18) for the class (A). It should be noted that the ratio is (cid:17)9/16(cid:18) for
responses from researchers who used data of multiple classes. The importance
of dataset comprehensiveness is thus suggested.
The usefulness of the dataset: Regarding the usefulness of the dataset of
each class, the negative answers are very few: 1 out of 8 for class (S), 0 out
of 17 for class (T), and 0 out of 10 for class (A). The high productivity of
the project is thus suggested.
4 Concluding Remarks
Through observations and a questionnaire-based evaluation, we found that CCC
DATAset 2009 has many good features and is supported by participating re-
searchers. It is suggested that the comprehensiveness of the dataset brings a
large impact. In the poster, more details will be described.
References
1. DARPA intrusion detection evaluation dataset, http://www.ll.mit.edu/mission/
communications/ist/corpora/ideval/data/index.html
2. Hatada, M., Nakatsuru, Y., Terada, M., Shinoda, Y.: Dataset for anti-malware re-
search and research achievements shared at the workshop. In: Computer Security
Symposium 2009 (CSS 2009), Anti-Malware Engineering WorkShop 2009 (MWS
2009), IPSJ, Japan, pp. 1–8 (2009) (in Japanese)
3. anti-Malware engineering WorkShop 2009 (MWS 2009),
http://www.iwsec.org/mws/2009/en.html
Inferring Protocol State Machine from
Real-World Trace
Yipeng Wang12, Zhibin Zhang1, and Li Guo1
1 Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
2 Graduate University, Chinese Academy of Sciences, Beijing, China
PI:EMAIL
speciﬁcations are helpful
Abstract. Application-level protocol
for
network security management, including intrusion detection, intrusion
prevention and detecting malicious code. However, current methods for
obtaining unknown protocol speciﬁcations highly rely on manual oper-
ations, such as reverse engineering. This poster provides a novel insight
into inferring a protocol state machine from real-world trace of a applica-
tion. The chief feature of our method is that it has no priori knowledge
of protocol format, and our technique is based on the statistical na-
ture of the protocol speciﬁcations. We evaluate our approach with text
and binary protocols, our experimental results demonstrate our proposed
method has a good performance in practice.
1 Introduction and System Architecture
Finding protocol speciﬁcations is a crucial issue in network security, and detailed
knowledge of a protocol speciﬁcation is helpful in many network security appli-
cations, such as intrusion detection systems and vulnerability discovery etc. In
the context of extracting protocol speciﬁcations, inferring the protocol state ma-
chine plays a more important role in practice. ScriptGen [1] is an attempt to infer
protocol state machine from network traﬃc. However, the proposed technique is
limited for no generalization.
This poster provides a novel insight into inferring a protocol state machine
from real-world packet trace of an application. Moveover, we propose a system
that can automatically extract protocol state machine for stateful network pro-
tocols from Internet traﬃc. The input to our system is real-world trace of a
speciﬁc application, and the output to our system is the protocol state machine
of the speciﬁc application. Furthermore, our system has the following features,
(a) no knowledge of protocol format, (b) appropriate for both text and binary
protocols, (c) the protocol state machine we inferred is of good quality.
The objective of our system is to infer the speciﬁcations of a protocol that is
used for communication between diﬀerent hosts. To this end, our system carries
on the whole process in four phases, which are shown as follows:
Network data collection. In this phase, network traﬃc of a speciﬁc application
(such as SMTP, DNS etc.) is collected carefully. In this poster, The method of
collecting packets under speciﬁc transport layer port is adopted.
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 498–499, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
Inferring Protocol State Machine from Real-World Trace
499
EHLO
HELO
/
q1
q0
MAIL 
FROM
:
q2
:
M
O
R
F
L
I
A
M
q6
RCPT 
TO: <
<
q3
RCPT TO: 
DATA
q4
T
N
E
T
N
O
C
a
q0
a
q1
d
e
RSET
q5
QUIT
q7