session keys ensures the forward security of the exchanged
messages. That is, an attacker A cannot acquire the session
key K once the migration protocol is terminated, even if it
gains full control of S and all the exchanged messages be-
tween S and D.
To acquire the key, A has to compromise D.
As such, our protocol construction satisﬁes the VM-vTPM
conﬁdentiality and integrity requirement (Requirement 1).
Furthermore, since the public key certiﬁcate of D (and S,
respectively) is extended in the PCRs during its integrity
veriﬁcation, S can ensure that the measured PCRs corre-
spond to the physical machine of D.
This prevents A
from presenting measurements performed on another ma-
chine and claiming that they pertain to the machine of D
(or S, respectively); in this case, this misbehavior will be
5
A variant scheme for linking the public key to the PCRs
relies on the use of special TLS certiﬁcate extensions—which
might, however, increase the size of trusted computing base
(TCB) [21].
6
Recall in this case that S can securely delete K at the end
of the migration process.
7
Linking the PCR measurements to D cannot be achieved
solely by the use of the AIK of D. This is because the
AIK does not contain any information that could be used
for identifying the entity to which it was issued (in this case,
S or D) [1].
S (Source)
D (Destination)
Derive session key K
Start of TLS Session
Mutual authentication
Key exchange
Attestation of S and D
Pick Ns ∈ {0, 1}n
{Ns}
K
Derive session key K
m1 = SignAIK (P CR || Ns)
S veriﬁes D platform integrity. A similar attestation of S is also performed by D.
Verify Ns
K
{m1
}
If these veriﬁcations fail, the protocol is aborted.
VM-vTPM Transfer
Suspend and lock
the VM and its vTPM
m2 = V M||vT P M||Nd
Delete VM and vTPM
{Nd
}
K
{m2
}
K
{DELET E}
K
{RESUM E}
K
Pick Nd ∈ {0, 1}n
Verify Nd.
Verify VM integrity
Import VM-vTPM
Resume VM and its vTPM
Delete K
Delete K
End of TLS Session
Figure 3: Sketch of a secure VM-vTPM migration protocol. The session key K is derived using key exchange
protocols (e.g., TLS handshake) and used only for the current protocol instance. S and D verify each other’s integrity
by examining PCRs signed using an AIK key obtained from a Privacy CA. SignX (Y ) refers to the signature of Y using
key X, n is a security parameter.
directly detected by S. Since we assume that A cannot
modify software on S and/or D during the migration pro-
cess, little can be done by A to convince S that its machine
is “honest”, while it hosts in reality malicious/compromised
software (Requirement 3). Given that the integrity of S is
also veriﬁed, the authenticity of the migration initiation can
also be ensured. This is the case since D veriﬁes that it is
interacting with an “honest” source and therefore can trust
that S will abide by the protocol speciﬁcation. This also in-
cludes trusting that S (i) will initiate the migration process
upon the request of P (Requirement 2) and (ii) will securely
delete the key K at the end of the migration process [22,23].
Note that this trust does not extend to the contents of the
migrated VM. Indeed, while S might be “honest”, the VM
itself might be compromised by A prior to the start of the
migration process. This use-case is countered by requiring
that D checks the integrity of the migrated VM after the
data transfer. As a result, D can ensure that only correct
VMs can be migrated to its environment, thus conforming
with Requirement 3 (Section 2).
4. PRACTICAL CONSIDERATIONS
So far, we have discussed secure VM-vTPM migration in
the context of VM migration using the suspend- transfer-
resume paradigm where the vTPM is a process inside the
VM itself. Here, we discuss several challenges in extending
it to alternative VM migration mechanisms (e.g., live mi-
gration) and vTPM architectures (e.g., each vTPM inside
a separate VM, all vTPMs in a separate privileged VM).
The performance of secure VM-vTPM migration is critical
to its adoption and optimizing it requires understanding the
nature of the overhead that it imposes. For this purpose,
we present a prototype implementation and an initial study
of the overhead in terms of end-to-end migration time and
CPU usage.
4.1 Feasibility Study
In order to demonstrate the feasibility of our secure VM-
vTPM migration protocol, we implemented a prototype and
integrated it in the Xen hypervisor [9]. Our implementation
emulates suspended VM-vTPM migration where each VM
runs its own vTPM in the user space. We also show pre-
liminary performance results on the overhead incurred by
securing VM-vTPM migration in terms of end-to-end mi-
gration time and CPU usage. Finally, we discuss additional
protocol characteristics and possible optimizations.
Implementation setup
In our implementation, we considered a private cloud vir-
tual environment similar to the one described in Figure 1.
We used two Thinkpad W510 (1.73 GHz, 8 GB RAM) ma-
chines with identical hardware as migration source (S) and
destination (D). Both machines were conﬁgured to use the
64-bit Xen hypervisor (version 4.0.2-rc2-pre). The virtual
machines (VM) running on S and D had their conﬁgura-
tion ﬁles, disk and swap spaces on a separate NFS shared
server. Each VM instance had its own virtual TPM run-
ning in user space. S, D and the NFS server were on the
same 1 GB Ethernet local network (LAN). Therefore, the
migration protocol only transfers the VM-vTPM RAM im-
(cid:1)(cid:3)(cid:2)
/
/
o
o
/
/
o
o
o
o
/
/
o
o
/
/
(cid:25)
(cid:28)
(cid:27)
(cid:33)
(cid:1)
(cid:29)
(cid:30)
(cid:27)
(cid:33)
(cid:21)
(cid:32)
(cid:20)
(cid:1)
(cid:27)
(cid:1)
(cid:22)
(cid:31)
(cid:26)
(cid:28)
(cid:24)
(cid:29)
(cid:25)
(cid:35)
(cid:30)
(cid:35)
(cid:24)
(cid:29)
(cid:14)
(cid:33)
(cid:5)(cid:7)(cid:2)
(cid:5)(cid:2)(cid:2)
(cid:4)(cid:7)(cid:2)
(cid:4)(cid:2)(cid:2)
(cid:3)(cid:7)(cid:2)
(cid:3)(cid:2)(cid:2)
(cid:7)(cid:2)
(cid:2)
(cid:15)(cid:29)(cid:32)(cid:25)(cid:23)
(cid:3)(cid:4)(cid:9)(cid:35)(cid:10)(cid:14)(cid:18)
(cid:17)(cid:12)(cid:6)
(cid:5)(cid:13)(cid:14)(cid:18)
(cid:7)(cid:3)(cid:4)
(cid:3)(cid:2)(cid:4)(cid:6)
(cid:3)(cid:7)(cid:5)(cid:8)
(cid:19)(cid:16)(cid:1)(cid:17)(cid:10)(cid:16)(cid:1)(cid:32)(cid:27)(cid:34)(cid:25)(cid:1)(cid:20)(cid:16)(cid:11)(cid:21)
(cid:4)(cid:2)(cid:6)(cid:9)
Figure 4: End-to-end VM-vTPM migration downtime
for diﬀerent encryption ciphers and VM RAM sizes. The
results are validated over ﬁve independent migration
protocol executions. We also show the corresponding
95% conﬁdence intervals.
age. We note that this conﬁguration is common in private
cloud environments [24, 25].
Our implementation leverages on the Python-based Xen
VM suspension and resumption. We used the TPM emula-
tor [26] as a vTPM and executed it within the VM itself.
We integrated OpenSSL (version 0.9.8o) to establish an au-
thentic and secure channel and used the Privacy CA [27] to
obtain AIKs for remote attestation. For simplicity, we only
used boot time integrity measurements during attestation.
We also implemented an insecure version of VM migration
which simply transfers the VM-vTPM using the standard
TCP socket interface. This implementation was used for
comparison. Our source code is open-source and available
for download at [10].
Preliminary results
One of the most common metrics used for evaluating the
performance of migration protocols is the total end-to-end
VM migration downtime [24, 25, 28].
It is deﬁned as the
time elapsed between the initiation of the migration proto-
col at the source and the completion of the VM migration at
the destination. It is indicative of the actual VM downtime
perceived by the end user. We measured the end-to-end
migration time for diﬀerent VM RAM sizes and encryption
ciphers. Figure 4 shows the results validated with ﬁve inde-
pendent migration protocol executions. For VM RAM sizes
of 1 GB (typical in private cloud environments), the secure
protocol with 128-AES encryption completed within approx-
imately 110 s, that is 20% slower than the insecure version
(88 s). We note that the absolute values can only signiﬁ-
cantly improve if the industry-standard virtualized hardware
is used.
We further investigated the underlying overheads in the
migration process by considering the CPU usage. The CPU
time consumed by a process is indicative of the actual over-
head imposed on the system as well as the contributions of
(cid:1)(cid:3)(cid:2)
the diﬀerent components. We used the Google CPU proﬁler,
part of the Google-perf tools [29] to instrument and measure
the CPU usage during migration. Figures 5(a) and 5(b)
show the distribution of CPU time between I/O and cryp-
tographic operations measured for diﬀerent VM RAM sizes
and encryption ciphers. We notice that most of the time
is spent in cryptographic (encryption, secure hashing) and
optimized SSL I/O operations during the secure transfer of
the VM RAM image. Note that this overhead is common to
all secure solutions. We should also note that the overhead
on Privacy CA and key regeneration in our design is inde-
pendent of the number of migrated vTPM keys. In terms
of the overhead due to attestation, this overhead depends
on the type of properties being measured and the size of
the measurement target. We note that the time required to
measure hypervisor static properties is typically of the order
of tens of milliseconds [19, 20].
The incurred security overhead may be tolerable in ap-
plications without strict timing constraints (e.g., email). In
the case of time sensitive applications (e.g., video stream-
ing), migration process optimizations would be required in
order to reduce the total security overhead. These would in-
clude hardware and software as well as cryptographic-related
optimizations.
4.2 Discussion
In order to reduce the total migration downtime of time
sensitive applications running on a VM, one direction is to
consider performing live VM-vTPM migration [24, 25, 28].
However, secure live migration of VMs with vTPMs requires
synchronizing VM-vTPM state during the transfer. Given
that each vTPM (in our implementation) runs as a process
within its own VM, existing memory synchronization tech-
niques can be used to synchronize the VM-vTPM state on
the source and destination during migration. This allows the
resumption of the VM on the destination before the com-
plete transfer of VM-vTPM state [24]. We note however
that live migration of the vTPM may require speciﬁc VM
memory partitioning in order to ensure that the vTPM state
is transferred at once to avoid state corruption. The feasi-
bility of such a secure VM-vTPM live migration approach