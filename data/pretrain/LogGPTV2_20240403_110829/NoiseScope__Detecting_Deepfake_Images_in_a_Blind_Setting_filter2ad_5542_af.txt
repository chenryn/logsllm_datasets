### Evaluation of Countermeasures Against NoiseScope

We have previously discussed the robustness of NoiseScope against various countermeasures, including the use of the defenderâ€™s filter. In this section, we summarize our findings on the effectiveness of a range of challenging countermeasures.

#### Resilience to Compression Attacks
NoiseScope demonstrates resilience against compression attacks, which are considered particularly challenging in prior work. This robustness is a significant advantage, as it ensures that NoiseScope can maintain its detection capabilities even when images are compressed.

#### Effective Recovery Schemes
We recommend effective recovery schemes for different types of post-processing attacks, such as Wavelet-noising, adding noise, and Gamma correction. These schemes help to mitigate the impact of these attacks on the detection performance of NoiseScope.

#### Unsuccessful Countermeasures
Countermeasures that significantly degrade image quality, such as excessive blurring or histogram equalization, are generally ineffective. These methods can be considered unsuccessful, as they render the images unusable for most practical purposes. Online platforms, such as news and social media sites, can reject images that show signs of excessive post-processing.

#### Ongoing Work
There is ongoing research into detecting image manipulations and post-processing. For example, Adobe has recently developed new tools to detect Photoshopped images [76]. NoiseScope can leverage such tools and implement appropriate recovery measures to enhance its detection pipeline, making it more resilient to a variety of countermeasures.

### Conclusion
Advancements in deep learning have significantly improved the capabilities of generative models, enabling the creation of photorealistic images or deepfakes that can be used for malicious purposes, such as spreading fake news or creating fake accounts. In this work, we introduce NoiseScope, a method for detecting deepfakes in a blind setting, without any prior access to fake images or their generative models. The key idea is to exploit unique patterns left behind by generative models during the creation of fake images.

Our method was evaluated on 11 diverse deepfake datasets, covering 4 high-quality generative models, and achieved over 90% F1 score in detecting fake images. We also analyzed the resilience of NoiseScope against a range of countermeasures, demonstrating its effectiveness in real-world scenarios.

### References
[References listed here]

### Image Samples
The following figures illustrate examples of fake samples from various generative models and the effects of different countermeasures on these images.

- **Figure 8**: Fake samples from BigGAN-DogLV [36].
- **Figure 9**: Fake samples from BigGAN-DogHV [36].
- **Figure 10**: Fake samples from BigGAN-BurgLV [36].
- **Figure 11**: Fake samples from BigGAN-BurgHV [36].
- **Figure 12**: Fake samples from CycleGAN-Zebra [86].
- **Figure 13**: Fake samples from CycleGAN-Winter [86].
- **Figure 14**: Fake samples from PGGAN-Tower [67].
- **Figure 15**: Fake samples from StyleGAN-Bed [14].
- **Figure 16**: Fake samples from StyleGAN-Face1 [15].
- **Figure 17**: Fake samples from StyleGAN-Face2 [28].
- **Figure 18**: Fake samples from PGGAN-Face [67].

**Effects of Countermeasures:**

- **Figure 20**: Samples from CycleGAN-Zebra [86] that evaded detection when blurred. The top row shows the images before blurring, and the bottom row shows the images after blurring.
- **Figure 21**: Samples from BigGAN-DogHV [36] that evaded detection when blurred. The top row shows the images before blurring, and the bottom row shows the images after blurring.
- **Figure 22**: Samples from StyleGAN-Face2 [28] that evaded detection when subjected to histogram equalization. The top row shows the images before equalizing, and the bottom row shows the images after equalizing.
- **Figure 23**: Model fingerprints from StyleGAN-Face2 [28], before (left) and after (right) applying JPEG compression.
- **Figure 19**: Image samples from StyleGAN-Face2 [28] subjected to a fingerprint spoofing attack against an increasing number of residual spaces. From left to right, we present (a) the original image, (b) the image spoofed against the Wavelet residual space, (c) the image spoofed against the Wavelet and Blur residual spaces, and (d) the image spoofed against the Wavelet, Blur, and Laplacian residual spaces.

These figures provide visual evidence of the effectiveness of NoiseScope and the challenges posed by various countermeasures.