venient to use a histogram with geometric bin ranges, i.e., the range of values corre-
sponding to jth bin in the histogram is a times the range of the (j − 1)th bin, for
2. Thus, the histogram bins were
some factor a. In our experiments, a was set to
[0, 1), [1, 2), [2, 4), [4, 7), [7, 11), [11, 17), [17, 25) and so on.
√
As with other anomaly detection techniques, our approach consists of a training
period, followed by a detection period. During the training period, a histogram Ht
representing the frequency distribution observed during the training period is computed
and stored. For detection, the histogram Hd computed during detection is compared
with the histogram Ht. An anomaly is ﬂagged if Hd is “more” than Ht. The notion of
“more” can be deﬁned in multiple ways, but we need some thing that can be computed
efﬁciently, and moreover, represents a clear and signiﬁcant change from Hd. For this
reason, we compare the highest non-zero bin HN ZBd in Hd with the highest non-
zero bin HN ZBt computed during training. The severity of the anomaly is deﬁned to
be HN ZBd − HN ZBt, provided HN ZBd > HN ZBt. Otherwise, no anomaly is
ﬂagged. The condition HN ZBd > HN ZBt reﬂects our bias for detecting increased
email trafﬁc, as opposed to detecting a reduction in email trafﬁc. Note that with this
simple threshold criteria, there is no need to maintain entire histograms, but only the
highest nonzero bins. More complex threshold criteria may take into account the entire
histogram Ht to derive a threshold, so it is useful to compute and maintain histograms
during training. During detection, however, the potential beneﬁts of having the extra
information will likely be more than offset by the additional storage needed to maintain
histograms.
By choosing different values for the time window w, we can capture statistical
information at different time scales. A small value of w will enable fast detection of
intense attacks, as such attacks can be detected with a delay of the order of w. However,
a slow but sustained attack may not be detected using a small time window. This is
because there can be much more burstiness in email trafﬁc at shorter time scales than
larger time scales. Such burstiness means that the peak frequencies observed at shorter
time scales will be much higher than average values, thus making it difﬁcult to detect
small increases in trafﬁc. Since burstiness at higher time scales tends to be smaller, the
difference between peak and average is smaller in those time scales, thus making it easier
to detect modest increases in trafﬁc. For this reason, we use several different time scales
in our experiment, starting from 0.8 seconds and increasing to about 83 minutes, with
each time scale being three to ﬁve times the previous one.
The above discussion separates the training phase from the detection phase. In a live
system, user behaviors evolve over time, and this must be accommodated. The usual
An Approach for Detecting Self-propagating Email Using Anomaly Detection
61
technique used in anomaly detection systems is to continuously train the system, while
ensuring that (a) very old behaviors are “aged” out of the training proﬁle, and (b) very
recent behaviors do not signiﬁcantly alter this proﬁle. This technique can be incorporated
into our approach as well, but we did not pursue this avenue as the change would have
no direct effect on the results reported in this paper.
3 Experiment I
The primary goal of the ﬁrst set of experiments was to study the effectiveness of our
approach for detecting self-propagating email viruses. In particular, we wanted to study
the false alarm rate and detection latency as the stealthiness of the virus is changed. This
experiment us based on simple models of user behavior. (More complex and realistic
user models are considered in Experiment II.)
One obvious way to study the effectiveness of the approach is to install it on a real
mail server, such as the mail server in a university or a large company. Apart from is-
sues of privacy that need to be addressed in such experiments, there is another serious
impediment to such an approach: it is not practical to introduce viruses into such sys-
tems for the purpose of experimentation: it would seriously impact email service in the
organization. Given the critical role that email has begun to assume in practically every
large organization, such an approach is impractical.
Even if we were able to introduce such viruses in a real email system, existing email
viruses are rather noisy: as soon as they are read, they send copies of themselves to
all (or most) users in the address book. This causes a sharp spurt in email generation
rate in the system, and would be immediately detected by our approach. To pose any
challenge to our approach or to assess its capabilities, we would have to create new email
viruses, which would be a signiﬁcant task by itself. Therefore our experiment is based
on simulation. Below, we describe the simulation environment, and proceed to present
the results of the experiments.
An important aspect of these experiments is that the training, as well as detection
took place in an unsupervised setting. No attempt was made to tune or reﬁne the anomaly
detector based on observed results. Such tuning or reﬁnement could further improve the
results.
3.1 Experimental Setup
For this experiment, we simulated an intranet with several hundred users. Three sizes of
the intranet were considered: 400 users, 800 users and 1600 users. Our simulation could
have been based on actual mail servers and mail clients that were driven by simulated
users. However, the realism in the simulation is almost totally determined by the model
used for user behavior, and is largely unaffected whether real email clients or mail servers
were used3. On the other hand, leaving out real mail servers and clients in a simulation has
several important beneﬁts. First, we do not need a large testbed consisting of hundreds
3 The only condition when the presence of real mail clients and servers can become important
is when the system gets overloaded, due to propagation of email virus. In our experiments, the
virus was always detected well before there was any signiﬁcant increase in email trafﬁc, and
hence the absence of actual email servers and clients is unlikely to have affected the results we
obtained.
62
A. Gupta and R. Sekar
of computers, real or virtual. Second, a light-weight simulation that avoids real mail
servers and clients can complete in seconds instead of taking hours.
Our simulation used discrete time, where each cycle of simulation was chosen to
correspond to roughly 0.2 seconds. This is a rather arbitrary number — our main con-
cern in this context was to choose a small enough granularity that the results would be
essentially the same as with a simulation based on continuous time.
Email users are modeled as Poisson processes, reading or sending emails at random
during each simulation cycle. Speciﬁcally, in a single simulation cycle, the probability
of a user sending email was set at 0.0006 and the probability of checking email was set at
0.0003. This means that users send out emails with a mean interval of about 5 minutes,
and that they check emails with a mean interval of about 10 minutes. The recipients for
each mail was determined at random, and the number of recipients was chosen using
a positive normal distribution with a mean of 1 and standard deviation of 2. Whereas
sending of mails was assumed to take place one at a time, email reading was modeled
as a batch process — each attempt to read email reads most of the emails queued for
the user. Moreover, for each message, the user randomly chooses to reply to the sender,
reply to all recipients, or not reply at all. We have used identical models for all users in
this experiment, while the experiment described in Section 4 uses a non-uniform model
where different user behaviors are different.
In this experiment, we wanted to model not only the viruses prevalent today, all of
which propagate very rapidly, but also stealthy viruses. For stealth, viruses may employ
a combination of the following techniques:
– low propagation factor, i.e., when the virus is read, it does not cause generation of
emails to a large number of users, such as the set of names in the address book of
the reader. A high propagation factor makes the virus much more noticeable.
– long incubation period, the delay between when the virus is read and the time it
causes propagation of email is large. The long delay makes it difﬁcult to associate
the propagation with the virus.
– polymorphism, the virus modiﬁes itself, so that the emails generated do not look
like the virus that was read. For additional stealth, the virus can propagate non-virus
carrying emails as well as those carrying the virus.
– matching user behavior, i.e., the virus avoids sending out emails with a large recipient
list, instead partitioning such messages into multiple ones with recipient lists of the
size observed on normal messages.
– randomization, i.e., all of the above techniques are randomized — for instance, the
incubation period is a random number over a range. Similarly, the propagation factor
is a random number.
Of these techniques, polymorphism does not affect our approach, as it is not based on
email content. Among the rest, propagation factor and incubation period were found to
have the maximum impact on detection effectiveness, while randomization had modest
effect. Matching of user behavior seemed to have no effect. Thus, our results discussion
considers only two of the above factors: propagation factor and incubation period.
An Approach for Detecting Self-propagating Email Using Anomaly Detection
63
Effects of Fanout on Detection Times
Effect of Incubation Period on Detection Times
10000
1000
)
s
d
n
o
c
e
S
10000
1000
)
s
d
n
o
c
e
S
(
s
e
m
T
n
o
i
i
t
c
e
t
e
D
100
10
1
1
Fanout = 04
Fanout = 08
Fanout = 16
Fanout = 32
Fanout = 64
10
1000
Incubation Period (Seconds)
100
10000
(
s
e
m
T
n
o
i
i
t
c
e
t
e
D
100
10
1
1
Legend
Incub.Time = 0000s
Incub.Time = 0004s
Incub.Time = 0040s
Incub.Time = 0100s
Incub.Time = 0400s
Incub.Time = 0800s
Incub.Time = 1600s
10
100
Propagation Factor (Fanout)
Fig. 2. Detection time as a function of incubation period and propagation factor.
Infected Machines at Detection Time, Size = 400 Clients
i
s
e
n
h
c
a
M
d
e
t
c
e
f
n
I
f
o
e
g
a
t
n
e
c
r
e
P
100
80
60
40
20
0
1
Fanout = 04