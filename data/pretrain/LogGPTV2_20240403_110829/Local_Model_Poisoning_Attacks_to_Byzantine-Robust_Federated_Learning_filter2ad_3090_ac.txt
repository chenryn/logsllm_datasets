2
3.3 Attacking Trimmed Mean
Suppose wi j is the jth before-attack local model parameter on
the ith worker device and w j is the jth before-attack global
model parameter in the current iteration. We discuss how we
craft each local model parameter on the compromised worker
devices. We denote by wmax, j and wmin, j the maximum and
minimum of the jth local model parameters on the benign
worker devices, i.e., wmax, j=max{w(c+1) j,w(c+2) j,··· ,wm j}
and wmin, j=min{w(c+1) j,w(c+2) j,··· ,wm j}.
Full knowledge: Theoretically, we can show that the follow-
ing attack can maximize the directed deviations of the global
model (i.e., an optimal solution to the optimization problem
in Equation 1): if s j = −1, then we use any c numbers that
are larger than wmax, j as the jth local model parameters on
the c compromised worker devices, otherwise we use any c
numbers that are smaller than wmin, j as the jth local model
parameters on the c compromised worker devices.
Intuitively, our attack crafts the compromised local models
based on the maximum or minimum benign local model pa-
rameters, depending on which one deviates the global model
towards the inverse of the direction along which the global
model would change without attacks. The sampled c numbers
should be close to wmax, j or wmin, j to avoid being outliers
and being detected easily. Therefore, when implementing
the attack, if s j = −1, then we randomly sample the c num-
bers in the interval [wmax, j,b· wmax, j] (when wmax, j > 0) or
1628    29th USENIX Security Symposium
USENIX Association
[wmax, j,wmax, j/b] (when wmax, j ≤ 0), otherwise we randomly
sample the c numbers in the interval [wmin, j/b,wmin, j] (when
wmin, j > 0) or [b·wmin, j,wmin, j] (when wmin, j ≤ 0). Our attack
does not depend on b once b > 1. In our experiments, we set
b = 2.
Partial knowledge: An attacker faces two challenges in the
partial knowledge scenario. First, the attacker does not know
the changing direction variable s j because the attacker does
not know the local models on the benign worker devices.
Second, for the same reason, the attacker does not know the
maximum wmax, j and minimum wmin, j of the benign local
model parameters. Like Krum, to address the ﬁrst challenge,
we estimate the changing direction variables using the local
models on the compromised worker devices.
One naive strategy to address the second challenge is to use
a very large number as wmax, j or a very small number as wmin, j.
However, if we craft the compromised local models based on
wmax, j or wmin, j that are far away from their true values, the
crafted local models may be outliers and the master device
may detect the compromised local models easily. Therefore,
we propose to estimate wmax, j and wmin, j using the before-
attack local model parameters on the compromised worker
devices. In particular, the attacker can compute the mean
μ j and standard deviation σ j of each jth parameter on the
compromised worker devices.
Based on the assumption that each jth parameters of the be-
nign worker devices are samples from a Gaussian distribution
with mean μ j and standard deviation σ j, we can estimate that
wmax, j is smaller than μ j + 3σ j or μ j + 4σ j with large prob-
abilities; and wmin, j is larger than μ j − 4σ j or μ j − 3σ j with
large probabilities. Therefore, when s j is estimated to be −1,
we sample c numbers from the interval [μ j +3σ j,μ j +4σ j] as
the jth parameter of the c compromised local models, which
means that the crafted compromised local model parameters
are larger than the maximum of the benign local model pa-
rameters with a high probability (e.g., 0.898 – 0.998 when
m = 100 and c = 20 under the Gaussian distribution assump-
tion). When s j is estimated to be 1, we sample c numbers from
the interval [μ j − 4σ j,μ j − 3σ j] as the jth parameter of the
c compromised local models, which means that the crafted
compromised local model parameters are smaller than the
minimum of the benign local model parameters with a high
probability. The jth model parameters on the benign worker
devices may not accurately follow a Gaussian distribution.
However, our attacks are still effective empirically.
3.4 Attacking Median
We use the same attacks for trimmed mean to attack the me-
dian aggregation rule. For instance, in the full knowledge
scenario, we randomly sample the c numbers in the inter-
val [wmax, j,b · wmax, j] or [wmax, j,wmax, j/b] if s j = −1, oth-
erwise we randomly sample the c numbers in the interval
[wmin, j/b,wmin, j] or [b· wmin, j,wmin, j].
4 Evaluation
We evaluate the effectiveness of our attacks using multiple
datasets in different scenarios, e.g., the impact of different
parameters and known vs. unknown aggregation rules. More-
over, we compare our attacks with existing attacks.
4.1 Experimental Setup
Datasets: We consider four datasets: MNIST, Fashion-
MNIST, CH-MNIST [31]2 and Breast Cancer Wisconsin (Di-
agnostic) [18]. MNIST and Fashion-MNIST each includes
60,000 training examples and 10,000 testing examples, where
each example is an 28×28 grayscale image. Both datasets
are 10-class classiﬁcation problems. The CH-MNIST dataset
consists of 5000 images of histology tiles from patients with
colorectal cancer. The dataset is an 8-class classiﬁcation prob-
lem. Each image has 64×64 grayscale pixels. We randomly
select 4000 images as the training examples and use the re-
maining 1000 as the testing examples. The Breast Cancer
Wisconsin (Diagnostic) dataset is a binary classiﬁcation prob-
lem to diagnose whether a person has breast cancer. The
dataset contains 569 examples, each of which has 30 features
describing the characteristics of a person’s cell nuclei. We
randomly select 455 (80%) examples as the training examples,
and use the remaining 114 examples as the testing examples.
Machine learning classiﬁers: We consider the following
classiﬁers.
Multi-class logistic regression (LR). The considered ag-
gregation rules have theoretical guarantees for the error rate
of LR classiﬁer.
Deep neural networks (DNN). For MNIST, Fashion-
MNIST, and Breast Cancer Wisconsin (Diagnostic), we use a
DNN with the architecture described in Table 7a in Appendix.
We use ResNet20 [28] for CH-MNIST. Our DNN architecture
does not necessarily achieve the smallest error rates for the
considered datasets, as our goal is not to search for the best
DNN architecture. Our goal is to show that our attacks can
increase the testing error rates of the learnt DNN classiﬁers.
Compared attacks: We compare the following attacks.
Gaussian attack. This attack randomly crafts the local
models on the compromised worker devices. Speciﬁcally,
for each jth model parameter, we estimate a Gaussian dis-
tribution using the before-attack local models on all worker
devices. Then, for each compromised worker device, we sam-
ple a number from the Gaussian distribution and treat it as the
jth parameter of the local model on the compromised worker
device. We use this Gaussian attack to show that crafting com-
promised local models randomly can not effectively attack
the Byzantine-robust aggregation rules.
2We use a pre-processed version from https://www.kaggle.com/
kmader/colorectal-histology-mnist#hmnist_64_64_L.csv.
USENIX Association
29th USENIX Security Symposium    1629
Table 1: Default setting for key parameters.
Table 2: Testing error rates of various attacks.
Parameter
Description
m
c
p
ε
β
Number of worker devices.
Number of compromised worker devices.
Degree of Non-IID.
Distance parameter for Krum attacks.
Parameter of trimmed mean.
Value
100
20
0.5
0.01
c
Label ﬂipping attack. This is a data poisoning attack that
does not require knowledge of the training data distribution.
On each compromised worker device, this attack ﬂips the
label of each training instance. Speciﬁcally, we ﬂip a label l as
L−l−1, where L is the number of classes in the classiﬁcation
problem and l = 0,1,··· ,L− 1.
Back-gradient optimization based attack [43]. This is
the state-of-the-art untargeted data poisoning attack for multi-
class classiﬁers. We note that this attack is not scalable and
thus we compare our attacks with this attack on a subset of
MNIST separately. The results are shown in Section 4.4.
Full knowledge attack or partial knowledge attack. Our
attack when the attacker knows the local models on all worker
devices or the compromised ones.
Parameter setting: We describe parameter setting for the
federated learning algorithms and our attacks. Table 1 sum-
marizes the default setting for key parameters. We use
MXNet [12] to implement federated learning and attacks.
We repeat each experiment for 50 trials and report the average
results. We observed that the variances are very small, so we
omit them for simplicity.
Federated learning algorithms. By default, we assume
m = 100 worker devices; each worker device applies one
round of stochastic gradient descent to update its local model;
and the master device aggregates local models from all worker
devices. One unique characteristic of federated learning is
that the local training datasets on different devices may not be
independently and identically distributed (i.e., non-IID) [39].
We simulate federated learning with different non-IID training
data distributions. Suppose we have L classes in the classiﬁca-
tion problem, e.g., L = 10 for the MNIST and Fashion-MNIST
datasets, and L = 8 for the CH-MNIST dataset. We evenly
split the worker devices into L groups. We model non-IID
federated learning by assigning a training instance with label
l to the lth group with probability p, where p > 0. A higher
p indicates a higher degree of non-IID. For convenience, we
call the probability p degree of non-IID. Unless otherwise
mentioned, we set p = 0.5.
We set 500 iterations for the LR classiﬁer on MNIST; we
set 2,000 iterations for the DNN classiﬁers on all four datasets;
and we set the batch size to be 32 in stochastic gradient de-
scent, except that we set the batch size to be 64 for Fashion-
MNIST as such setting leads to a more accurate model. The
trimmed mean aggregation rule prunes the largest and small-
est β parameters, where c ≤ β  1. Our attacks do not depend on b once b > 1.
We set b = 2. Unless otherwise mentioned, we assume that
attacker manipulates the local models on the compromised
worker devices in each iteration.
4.2 Results for Known Aggregation Rule
Our attacks are effective: Table 2 shows the testing error
rates of the compared attacks on the four datasets. First, these
results show that our attacks are effective and substantially
outperform existing attacks, i.e., our attacks result in higher er-
1630    29th USENIX Security Symposium
USENIX Association
(a) Krum
(b) Trimmed mean
(c) Median
(d) Krum
(e) Trimmed mean
(f) Median
Figure 2: Testing error rates for different attacks as we have more compromised worker devices on MNIST. (a)-(c): LR classiﬁer
and (d)-(f): DNN classiﬁer.
ror rates. For instance, when dataset is MNIST, classiﬁer is LR,
and aggregation rule is Krum, our partial knowledge attack in-
creases the error rate from 0.14 to 0.72 (around 400% relative
increase). Gaussian attacks only increase the error rates in sev-
eral cases, e.g., median aggregation rule for Fashion-MNIST,
and trimmed mean and median for CH-MNIST. Label ﬂip-
ping attacks can increase the error rates for DNN classiﬁers
in some cases but have limited success for LR classiﬁers.
Second, Krum is less robust to our attacks than trimmed
mean and median, except on Breast Cancer Wisconsin (Di-
agnostic) where Krum is comparable to median. A possible
reason why trimmed mean and median outperform Krum is
that Krum picks one local model as the global model, while
trimmed mean and median aggregate multiple local models to
update the global model (the median selects one local model
parameter for each model parameter, but the selected parame-
ters may be from different local models). Trimmed mean is
more robust to our attacks in some cases while median is more
robust in other cases. Third, we observe that the error rates
may depend on the data dimension. For instance, MNIST and
Fashion-MNIST have 784 dimensions, CH-MNIST has 4096
dimensions, and Breast Cancer Wisconsin (Diagnostic) has
30 dimensions. For the DNN classiﬁers, the error rates are
higher on CH-MNIST than on other datasets in most cases,
while the error rates are lower on Breast Cancer Wisconsin
(Diagnostic) than on other datasets in most cases.
We note that federated learning may have higher error rate
than centralized learning, even if robustness feature is not