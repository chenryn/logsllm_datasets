mits a fresh public key to a veriﬁable shuﬄe, the servers
jointly pick one re-encrypted key at random from the shuf-
ﬂed output to be a new Nym, and discard all other keys.
For eﬃciency, we would prefer to generate fresh Nyms in
batches, amortizing the cost of the shuﬄe across lotteries
for many Nyms, while ensuring that each Nym is assigned
independently. One conceivable approach is for the servers
to mint a batch of e-cash “coins” [10], encrypt each coin to a
random key chosen from the shuﬄe’s output, and ﬁnally run
a veriﬁable DC-nets round [13, 27], with one slot per coin,
allowing each coin’s “winner” to spend the coin and publish
a fresh public Nym key. We leave detailed exploration of this
challenge to future work.
Extending Nyms Across Epochs.
To enable Nyms to persist beyond one epoch, clients can
use each winning ticket from a Nym lottery either to publish
a fresh Nym key, or to re-publish an old Nym key, eﬀectively
“reviving” the old Nym and giving it a transmission slot in
the new epoch. A lottery winner might even publish another
user’s public Nym key, eﬀectively delegating the winning
ticket’s share of bandwidth in the new epoch an arbitrary
Nym whose content the lottery winner ﬁnds interesting.
When a client revives its own Nym in a new epoch, the
client must ensure that the set of users participating in the
Nym lottery is consistent with the existing Nym’s anonymity
policy: e.g., that the user’s buddies are also online. If some
participants go oﬄine during a Nym lottery, to avoid policy-
checking races the servers must restart the lottery, enabling
clients to re-check the new participant set before exposing
an old Nym in the new epoch.
4.3 Implementing the Policy Oracle
Implementing the Policy Oracle as a single independent
server would require that all clients trust the Policy Oracle
server to implement their requested attack mitigation poli-
cies correctly. Although a bad Policy Oracle server cannot
directly de-anonymize users since it does not know which
users own each Nym, it could—by intent or negligence—
simply make intersection attacks easy. For this reason, Bud-
dies leverages the anytrust server model that the underlying
Anonymizer already uses, running a virtual replica of the
Policy Oracle in “lock-step” on each of the anonymization
servers. The servers use standard distributed accountabil-
ity techniques [28] to cross-check each others’ computation
of Policy Oracle decisions, halting progress and raising an
alarm if any server deviates from an agreed-upon determin-
istic algorithm implementing the Policy Oracle. These tech-
niques apply readily to the Policy Oracle precisely because
it architecturally has access to no sensitive state, and hence
all of its state may be safely replicated.
Identifying Users to the Policy Oracle.
Buddies’ Anonymizer shares the set of users currently on-
line in each round with the Policy Oracle, which implies that
we must treat these online sets as “public information” that
the adversary may also obtain. As discussed in Section 2.2,
however, revealing actual user identities or locators for this
purpose, such as users’ public keys or IP addresses, risks
strengthening a weak adversary into an “omniscient” adver-
sary for intersection attack purposes.
Buddies addresses this problem by permitting clients to
authenticate with the servers via linkable ring signatures [24,
35]. To connect, each client generates a cryptographic proof
that it holds the private key corresponding to one of a ring
of public keys, without revealing which key the client holds.
In addition, the client generates and proves the correctness
of a linkage tag, which has a 1-to-1 relationship with the
client’s private key, but is cryptographically unlinkable to
any of the public keys without knowledge of the correspond-
1159ing private keys. The servers track which clients are online
via their linkage tags, and provide only the list of online
tags to the Policy Oracle in each round, so the Policy Ora-
cle can simulate an adversary’s intersection attacks without
knowing which actual users are online each round.
Of course, the server that a client connects to directly
can associate the client’s network-level IP address with its
linkage tag, and a compromised server may share this infor-
mation with an adversary. This linkage information does not
help a global passive adversary, who by deﬁnition obtains all
the same information the Policy Oracle obtains merely by
monitoring the network, but may help weaker adversaries
perform intersection attacks against those users who con-
nect via compromised servers. We see this risk as equivalent
to the risk clients run of connecting to a compromised “entry
relay” in existing anonymity systems [15, 18]. Compromised
servers are just one of the many avenues through which we
must assume an adversary might monitor the network.
4.4 Malicious Users and Sybil Attacks
While Buddies can measure, and optionally enforce a lower
bound on, the number of users comprising a Nym’s possi-
nymity or indinymity set, Buddies cannot guarantee that all
those users are providing useful anonymity. In particular,
if the owner of a Nym N has speciﬁed a policy mandating
a minimum buddy-set size of K, but up to F other clients
may be colluding with the adversary, then N ’s owner may
have to assume that its actual minimum anonymity set size
may be as little as K − F , if all F bad clients happen to—
or somehow arrange to—land in the same buddy set as N ’s
owner. Since in practical systems we don’t expect users to
have a reliable way of “knowing” how many other clients are
conspiring against them, we treat F as an unknown variable
that users may simply have to “guess” and factor into their
choices of possinymity or indinymity lower-bounds. In this
respect Buddies is no diﬀerent from any other anonymity
system some of whose users may be compromised.
Reducing vulnerability to malicious clients may be an ar-
gument in favor of random buddy-set formation (Section 3.2).
Randomized policies may oﬀer some guarantee that the ma-
licious users present in a Nym’s initial user set become “evenly
distributed” among buddy-sets. In any preferential, “repu-
tation-based” formation scheme, if the attacker can learn or
correctly guess the general “level of reliability” of a Nym N ’s
true owner—which may well be inferable from N ’s posting
record—then the attacker’s compromised nodes might delib-
erately exhibit a similar level of reliability in hopes of getting
clustered together in the owner’s buddy set. For such attacks
to succeed, however, the malicious users must be present at
N ’s creation in order to fall in N ’s possinymity set in the
ﬁrst place, and the attacker must adjust their reliability pro-
ﬁle after learning “enough” about N ’s owner, but before too
many buddy set splits have already occurred for N . Thus, if
the Policy Oracle builds up user reputation information in a
relatively conservative, long-term fashion across the users’
entire histories (e.g., from before N appeared), this may
make it diﬃcult for an attacker to “steer” malicious users’
reliability proﬁles “late in the game” to implement a cluster
attack N . Clustering attacks nevertheless present a risk that
more randomized buddy set formation policies may reduce.
As in any distributed system, an attacker may be able to
amplify the eﬀective numbers of malicious clients via Sybil
attacks [19], creating many fake user identities. Buddies ad-
dresses this risk by requiring users to be authenticated—
via linkable ring signatures as detailed above—as owners of
“real” identities in some Sybil attack resistant identity space.
Buddies is agnostic as to the exact nature of this public iden-
tity space or how it is made resistant to Sybil attacks. The
current prototype simply is deﬁned for “closed” groups, de-
ﬁned by a static roster of public keys listing all members,
so the group is exactly as Sybil attack resistant as whatever
method the group’s creater uses to form the roster. To sup-
port open-ended groups, Buddies could build on one of the
many Sybil attack resistance schemes, such as those based
on social networks [48, 54]—or could simply rate-limit Sybil
attacks via some “barrier to entry,” e.g., requiring users to
solve a CAPTCHA or receive a phone callback to “register”
an unknown public key for participation.
5. EVALUATION
We now evaluate Buddies’ utility, using data we collected
from popular public IRC (Internet Relay Chat) chat rooms
on EFnet servers. After introducing our data collection and
simulation approach, we ﬁrst explore “ideal” metrics quanti-
fying levels of anonymity achievable in principle under given
conditions—metrics that depend only on the user behavior
dataset and not on any particular Buddies policy or loss mit-
igation algorithm. Next, we apply these traces to an event-
based Buddies simulator to evaluate more realistic policies
against these ideals. We consider na¨ıve anonymous posting
without Buddies, then posting under policies that enforce
minimum buddy-set sizes, and policies that attempt to maxi-
mize possinymity. Finally, we analyze the overheads Buddies
induces in the context of Dissent.
5.1 Datasets and Simulation Methodology
To evaluate Buddies’ utility, we use traces taken from
popular public IRC (Internet Relay Chat) chat rooms on
EFnet servers. Unlike web traﬃc [53], IRC logs record par-
ticipants online status in addition to activity or transmission
of messages. The online status becomes critical for systems
in which inactive but online users submit cover traﬃc, such
as Buddies assumes. While BitTorrent traﬃc [2] supports
similar conventions as IRC, online status and activity, user
behavior focuses transferring data between peers, which does
not make a strong correlation to the need for long term in-
tersection resistant Nyms. Furthermore, the buddy system
focuses on anonymous group communication systems that
reveal all anonymous cleartexts to all users with at least all
servers privy to the input; such behavior maps better to IRC
and BitTorrent traﬃc than to Web traﬃc.
We monitored 100 (a limitation imposed by EFnet) of
the most active EFnet-based IRC rooms, for over a month
dating from November 26th through December 30th, 2012.
For each room, we obtained the following for each member:
joins, leaves, nickname changes, and messages. Anecdotally,
we found that users often temporarily disconnect from IRC
without IRC recognizing the disconnection. This creates a
period of time in which the user must use a secondary nick-
name, then switch back to the original nickname once IRC
recognizes the disconnect. Unfortunately we have no statis-
tics on average disconnection time, but we were able to iden-
tify these scenarios and “ﬁx” the data such that the aﬀected
users appear to be continuously online.
Figure 2 visually illustrates the trace collected from one
sample IRC room, football, plotting the time period a given
1160(a) Members sorted by total time online
(b) Members sorted by ﬁrst appearance online
Figure 2: Visualization of user online periods over one month in EFnet’s football IRC chatroom.
user was online as a horizontal line. Figure 2(a) sorts the
1207 unique users observed during the trace vertically by
total time the user was online during the observation period.
This graph shows that the online forum has a “core” set of
around 300 users who stay online most of the time, while the
remaining users come online for shorter periods at varying
times, with denser vertical stripes showing periodic patterns
(e.g., football games). Figure 2(b) shows the same data
with users sorted by ﬁrst appearance online: again about
300 users were online already at the start of the trace, while
new users appear subsequently at a fairly constant rate—
with a fraction of these “late arrivals” remaining online for
the remaining trace period.
To analyze this data we implemented an event-driven Bud-
dies simulator in Python, cleverly called the Anonymity Sim-
ulator (AS). AS plays the role of users, the Anonymizer,
and the Policy Oracle. As input the AS takes an IRC trace,
time between rounds, system-wide buddy and possinymity
set sizes, and buddy set formation policies. We primarily fo-
cus on random buddy set formation policies and user online
times. For the latter, the AS can either use an initial pe-
riod of the trace, a bootstrap period, or use a deity mode
and review the entire data set. To better compare apples-to-
apples, random, bootstrapped, and deity mode evaluations
all began at the same time in the trace.
5.2 Ideal Anonymity Analysis
We ﬁrst use our IRC traces to explore upper bounds on
the anonymity we expect to be achievable in any system
resistant to intersection attacks, under a trace-driven sce-
nario, but independent of particular anonymity mechanisms
or policies. These experiments depend only on analysis of
the IRC data, and do not depend on Buddies’ design or the
Buddies simulator. This analysis serves to deepen our un-
derstanding of user behavior in realistic online forums, and
to establish realistic expectations of what a system such as
Buddies might achieve in principle. We ﬁrst consider ano-
nymity potentially achievable for low-latency communica-
tion using pseudonyms of varying lifetimes, then focus on
long-lived pseudonyms in communication scenarios that can
tolerate varying oﬄine times in members of anonymity sets.
Low-latency pseudonyms of varying lifetime.
We focus on the football dataset, considering all online
periods of all 1207 users appearing in the trace as visualized
in Figure 2. We treat each contiguous online period lasting
at least time x as representing a pseudonym with lifetime
x, and compute an ideal anonymity set for that pseudonym
as the total number of users also contiguously online dur-
ing that pseudonym’s lifetime. This analysis pessimistically
eliminates from the anonymity set users with any oﬄine pe-
riod, however brief, during the pseudonym’s lifetime.
Figure 3(a) summarizes the distribution of these ideal an-
onymity set sizes, for pseudonym lifetimes varying on the
log-scale x-axis. Pseudonyms used for up to about one hour
reliably achieve anonymity sets of at least 250 members, and
sometimes up to 375 members—between 20% and 30% of the
total user population observed—suggesting that substantial
resistance to intersection attack may be achievable in large
forums for short-lived pseudonyms. Achievable anonymity
under these assumptions falls oﬀ rapidly as pseudonym life-
time increases further, however.
Long-lived pseudonyms tolerant of oﬄine periods.
Applications that demand truly long-lived pseudonyms,