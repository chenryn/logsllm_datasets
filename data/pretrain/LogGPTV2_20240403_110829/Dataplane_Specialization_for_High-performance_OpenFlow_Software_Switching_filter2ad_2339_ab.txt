which no fast specialized classiﬁcation strategies apply [19].
Indirect datapath. Indirect datapath designs adopt the ven-
erable fast-path/slow-path separation principle [46]. Here,
the switch maintains multiple “views” of the pipeline: the
fast path is constituted by one or more increasingly compact
Figure 2: Open vSwitch datapaths.
and incomplete pipeline representations, while a complete
representation serves as the slow path, used as a fallback
when the fast path cannot decide on the fate of a packet. This
way, only the ﬁrst packet of a ﬂow is subjected to full-scale
slow path pipeline processing, the resulting ﬂow-speciﬁc rules
and actions are registered in the fast path classiﬁer and the
rest of the ﬂow’s packets take the fast path without having
to recurse to the slow path again. In fact, the fast path func-
tions as a ﬂow cache, storing the forwarding decisions for
recent ﬂows. Aggressive ﬂow caching, consequently, allows
to amortize the cost of packet classiﬁcation over subsequent
packets of a ﬂow [26–28], contributing to increased perfor-
mance without loss of expressiveness and genericity (see
OVS [19], 6WINDGate [47]).
2.2 A Sophisticated Indirect Pipeline: OVS
The most popular OpenFlow software switch implemen-
tation today is undoubtedly Open vSwitch (OVS, [19]). In
this paper, we use OVS to showcase the pros and cons of the
indirect datapath approach. Indeed, OVS brings this design
to the extreme by adapting a datapath hierarchy of as many
as four levels (see Fig. 2).
Microﬂow cache: per-transport-connection exact match
store. The microﬂow cache stores the forwarding decisions
for the least recently seen transport connections in a very
fast collision-free hash. Since exact matching occurs over
all relevant tuple ﬁelds, essentially any change in the packet
header inside an established ﬂow (e.g., the IP TTL ﬁeld) re-
sults in a cache miss. The microﬂow cache is managed by
the second-level cache, the megaﬂow cache (see below), in
that the microﬂow cache indexes into the megaﬂow cache
and megaﬂow cache hits trigger a microﬂow cache update.
Megaﬂow cache: wildcard match store for trafﬁc aggre-
gates. The second-level megaﬂow cache allows to bundle
multiple microﬂows into a single megaﬂow aggregate and
impose common forwarding behavior to the entire bundle,
saving on cache entries (e.g., by applying the same action
to all incoming HTTP connections, regardless of the source
TCP port) at the cost of increased processing time. The
megaﬂow cache uses a tuple space search strategy [23]: ﬂow
entries are divided into groups based on the combination of
header ﬁelds they match on and every group matches on
only the ﬁelds relevant for the group. In practice, this en-
tails linearly iterating over a list of key/mask pairs for each
541
packet. The megaﬂow cache is managed by the third data-
path level (vswitchd, see next) reactively: packets miss-
ing the cache are encapsulated and sent to vswitchd that
returns the packet with the actions to be applied and up-
dates the cache accordingly. Since the megaﬂow cache does
not “know” about ﬂow priorities, matches can never over-
lap and so megaﬂows must be disjoint. Consequently, all
header ﬁelds from all ﬂow entries a packet traverses, those
that caused a match as well as those higher priority ones that
did not, need to be taken into consideration in tuple space
search. Note also that OVS uses a single megaﬂow cache
for the entire pipeline, hence cache entries collapse together
the behavior from all relevant ﬂow tables.
vswitchd: complete OpenFlow pipeline. The penul-
timate level is a fully blown realization of the OpenFlow
pipeline. Besides the use of standard techniques like multi-
threading, read-copy-update (RCU), and extensive batching,
OVS adopts a number of clever tricks to improve tuple space
search and megaﬂow cache management, like tuple prior-
ity sorting (to cut down on pipeline stage iterations), staged
lookup (per-protocol-layer caches for opportunistic early exit),
preﬁx tracking (optimal preﬁx masks for IP address range
tracking), and classiﬁer partitioning (by metadata ﬁelds).
OpenFlow controller. Somewhat unconventionally, we con-
sider the controller as the highest level of the OVS datapath
hierarchy as it fulﬁlls precisely the same role for vswitchd
as vswitchd does for the megaﬂow cache: it manages en-
tries at the next lower level of the datapath hierarchy plus
serves as a last resort for packets missing that level.
2.3 The Case Against Flow Caching
Indirect pipelines proved a remarkably successful imple-
mentation strategy to break down the complexity of Open-
Flow software packet processing. Yet, we argue that inherent
to general purpose ﬂow caches are a number of under-the-
hood limitations, which not only signiﬁcantly curtail achiev-
able switch performance but also raise a number of deep ar-
chitectural concerns.
Unpredictable packet processing. Flow caches work best
when trafﬁc exhibits sufﬁcient spatial locality (header ﬁelds
vary in a small range and thus only a few megaﬂows are
enough to cover all trafﬁc) and temporal locality (ﬂows’ pack-
ets are ﬁnely spaced in time to keep cache entries warm)
and/or when the pipeline omits high-entropy header ﬁelds
[26–28]. But only a single ﬁne-grained rule is enough to
“punch a hole” in all aggregates, leading to heavy megaﬂow
fragmentation [13,19]. We found that even the same packets
and the same pipeline can yield vastly different ﬂow caches
depending on the packet arrival sequence (see Fig. 3). Cor-
respondingly, in a ﬂow-cache-oriented architecture it is very
difﬁcult to reliably predict when and how a particular packet
will traverse the switch dataplane.
Performance artifacts. As long as ﬂow caching assump-
tions hold, indirect datapaths work reliably and efﬁciently.
Failing these, however, brings undue cache thrashing and
packets recurring to the slow path. This is then perceived by
the user as perplexing throughput drops, latency spikes, and
542
(a) ﬂow table
# decimal binary
1 190 10111110
2 189 10111101
3 187 10111011
4 183 10110111
5 175 10101111
6 159 10011111
7 191 10111111
(b) pkt port seq 1
# decimal binary
1 191 10111111
2 190 10111110
3 189 10111101
4 187 10111011
5 183 10110111
6 175 10101111
7 159 10011111
(c) pkt port seq 2
Figure 3: The ﬂow table (a) yields 7 megaﬂow cache entries
when the TCP destination port arrivals are as of seq 1 (b) (for
each zero bit in positions 2, . . ., 8), while if destination port
191 arrives ﬁrst as of seq 2 (c) then only a single entry arises
(matching at position 2, covering all subsequent packets).
downright service interruptions, on various hard-to-predict
combinations of ﬂow tables and input trafﬁc [29–34].
Vexing cache management complexity. The ﬂip side of
ﬂow caching is the complexity of managing the cache. First,
computing “good” megaﬂows is already a very hard prob-
lem [29, 33] (cf. Fig. 3). But updating cache contents when
changes to the switch conﬁguration are made is also rather
onerous, due to the difﬁculty of tracking exactly which cache
entries are affected by a change and need invalidation2. En-
suring cache coherence across switch threads, furthermore,
necessitates ﬁne-grained locking, impeding multi-core scal-
ability. This complexity can also make dataplane debugging
cumbersome and conceal software bugs and security ﬂaws.
Opens the door to malicious attacks. It is well-known that
caches are inherently vulnerable to a wide spectrum of secu-
rity threats, like cache poisoning or cache overﬂow [35], and
may leak information through side-channel attacks [48]. An
attacker may easily spawn a denial-of-service attack, by ex-
ecuting timing attacks on an OpenFlow switch to infer ﬂow
table contents and crafting a malicious packet trace to over-
ﬂow ﬂow caches. This is especially troublesome for cloud
infrastructure switches because only a single misbehaving
user can produce a widespread service disruption, by ex-
ploiting that shared ﬂow caches break tenant isolation and
create a coupling between logical pipelines (see Section 4.3).
Brittle architectural constraints. Flow caches bring along
some insidious but inevitable architectural choices. For in-
stance, OVS must recreate essentially the entire functional-
ity of the OpenFlow protocol at the vswitchd–megaﬂow-
cache interface, complete with ﬂow entry management and
packet in/out operations, and it does this in a rather inefﬁ-
cient reactive way, forcing packets to the slow path in or-
der to populate the caches. But ﬂow caching may also hin-
der dataplane innovation, since cache semantics must be ﬁt
piecemeal to new proposals like NOSIX [34] or P4 [49].
3. DATAPLANE SPECIALIZATION
ESWITCH is a new OpenFlow switch architecture we cre-
ated with the aim to discover the design space beyond gen-
eral purpose switch fast paths. ESWITCH occupies just the
opposite extreme of the spectrum: it fully embraces the con-
2OVS adopts the brute-force strategy to invalidate the entire
cache after essentially all changes [19].
cept of dataplane specialization and makes the datapath de-
pendent on, and actually carefully tailored to, the conﬁgured
OpenFlow pipeline.
We cast dataplane specialization as the process to com-
pile from a declarative description of the OpenFlow pipeline
into an efﬁcient machine code representation, together with
a runtime that effectively realizes the switch using the com-
piled datapath as the fast path. ESWITCH builds on the in-
sight that OpenFlow pipelines usually combine only a small
selection of simple but generic patterns into complex dat-
aplane programs and, accordingly, they can be represented
in terms of a few simple templates. ESWITCH then uses
template-based code generation to derive the customized dat-
it ﬁrst applies ﬂow-table analysis to decompose a
apath:
pipeline into a series of templates, followed by template spe-
cialization to patch pre-compiled templates with ﬂow keys,
and ﬁnally a linking phase to relocate jump pointers, com-
bining template code fragments into a single binary.
3.1 Templates
ESWITCH pipeline compilation revolves around the con-
cept of templates. A template in this context is some unit
of common OpenFlow packet processing behavior that ad-
mits a simple and composable machine code implementation
out of which more complex functionality can be constructed.
ESWITCH differentiates between packet parser templates,
matcher templates, ﬂow table templates, and action templates.
Packet parser templates are used, as the name suggests,
to generate code that transforms packet headers into an in-
ternal representation that can be subjected to rule matching.
ESWITCH separates header parsing at layer boundaries: it
includes a separate L2, L3, and L4 parser. The motivation
is to save on parsing for layers that do not participate in
ﬂow formation: e.g., for pure L2 MAC forwarding it is com-
pletely superﬂuous to parse L3 and L4 header ﬁelds for each
packet, L3 routing in turn can omit parsing L4 headers alto-
gether, etc. Note that parsing is incremental; the L3 header
parser composes an L2 parser to ﬁnd the starting position of
the L3 header and the L4 parser composes both parsers.
The general functionality of parser templates is as below;
a protocol bitmask (stored in register r15) is used to mark
the presence of a particular protocol header in the packet and
then each protocol’s header position is saved into a register.
PROTOCOL_PARSER: 
L2_PARSER:
L3_PARSER:
L4_PARSER:
Matcher templates allow matching on header ﬁelds; a sep-
arate template belongs to every ﬁeld deﬁned in the Open-
Flow speciﬁcation [2]. For instance, the rule ip_dst =
ADDR/MASK would be represented with the below template:
macro IP_DST_ADDR_MATCHER(ADDR,MASK):
mov r12, 
mov r13, 
mov r14, 
mov
xor
and
jne
eax,[r13+0x10]
eax,ADDR
eax,MASK
ADDR_NEXT_FLOW
; IP dst address in eax
; match ADDR
; apply MASK
; no match: next entry
The rule tcp_dst=PORT is matched as follows:
macro TCP_DST_PORT_MATCHER(PORT):
543
cmp
jne
[r14+0x2],PORT
ADDR_NEXT_FLOW
; port field equals key?
; no match: next entry
Note that actual ﬂow keys will be patched into the tem-
plates in the template specialization step, while jump point-
ers will be resolved during linking (see Section 3.3).
Flow table templates capture common ﬂow table patterns.
Our design has settled with four elemental table templates:
the direct code, the compound hash, the LPM, and the linked
list templates (see Fig. 4). Further table templates, like range
search for port matches, can easily be added in the future.
The direct code template is a faithful machine code repre-
sentation of the classiﬁcation rules in a ﬂow table. ESWITCH
uses this template when the ﬂow table does not contain enough