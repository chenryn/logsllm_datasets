the AI parameter
the MD parameter
congestion epoch that includes n > 1 rounds of AI and one
round of MD in each epoch, we have
cwndi(M ) = β · [ cwndi(M − 1) + n · αi ].
Eventually, each ﬂow i achieves a congestion window that
is proportional to its AI parameter, αi. Indeed, the ratio of
the congestion windows of the two ﬂows approaches α1/α2
for large values of M , and n > 1:
cwnd1(M )
cwnd2(M )
=
cwnd1(M − 1)/n + α1
cwnd2(M − 1)/n + α2
cwnd1(M − 2)/n2 + α1/n + α1
cwnd2(M − 2)/n2 + α1/n + α2
=
= · · · → α1
α2
.
Hence, to allocate bandwidth fairly among two ﬂows, we
need to scale each ﬂow’s AI parameter αi using its own RTT.
For this purpose, we use tρ as a common-base RTT for all the
ﬂows. Thus, the new AI scaling parameter, αrate, becomes
For AI : αrate ← αs · rtt
tρ
= α · (
rtt
tρ
)2.
(8)
3.5 Summary of Parameters
Table 1 summarizes the set of VCP router/end-host pa-
rameters and their typical values. We note that throughout
all the simulations reported in this paper, we use the same
parameter values. This suggests that VCP is robust in a
large variety of environments.
4. PERFORMANCE EVALUATION
In this section, we use extensive ns2 simulations to evalu-
ate the performance of VCP for a wide range of network sce-
narios [19] including varying the link capacities in the range
[100Kbps, 5Gbps], round trip times in the range [1ms, 1.5s],
numbers of long-lived, FTP-like ﬂows in the range [1, 1000],
and arrival rates of short-lived, web-like ﬂows in the range
[1s−1, 1000s−1]. We always use two-way traﬃc with conges-
tion resulted in the reverse path. The bottleneck buﬀer size
is set to the bandwidth-delay product, or two packets per
ﬂow, whichever is larger. The data packet size is 1000 bytes,
while the ACK packet is 40 bytes. All simulations are run
for at least 120s to ensure that the system has reached its
steady state. The average utilization statistics neglect the
ﬁrst 20% of simulation time. For all the time-series graphs,
utilization and throughput are averaged over 500ms inter-
val, while queue length and congestion window are sampled
every 10ms. We use a ﬁxed set of VCP parameters listed in
Table 1 for all the simulations in this paper.
Figure 3: VCP with the bottleneck capacity ranging from 100Kbps to 5Gbps. It achieves high utilization and almost
no packet loss with decreasing bottleneck queue as the capacity increases. Note the logarithmic scale of the x-axis in
this ﬁgure and the next one.
Figure 4: VCP with the round-trip propagation delay ranging from 1ms to 1500ms. It is able to achieve reasonably
high utilization, low persistent queue and no packet loss.
Figure 5: VCP with the number of long-lived, FTP-like ﬂows ranging from 1 to 1000. It achieves high utilization
with more bursty bottleneck queue for higher number of FTP ﬂows.
These simulation results demonstrate that, for a wide
range of scenarios, VCP is able to achieve exponential con-
vergence to high utilization, low persistent queue, negligible
packet drop rate and reasonable fairness, except its signiﬁ-
cantly slower fairness convergence speed compared to XCP.
4.1 One Bottleneck
We ﬁrst evaluate the performance of VCP for the sim-
ple case of a single bottleneck link shared by multiple VCP
ﬂows. We study the eﬀect of varying the link capacity, the
round-trip times, the number of ﬂows on the performance of
VCP. The basic setting is a 150Mbps link with 80ms RTT
where the forward and reverse path each has 50 FTP ﬂows.
We evaluate the impact of each network parameter in isola-
tion while retaining the others as the basic setting.
Impact of Bottleneck Capacity: As illustrated in Fig-
ure 3, we observe that VCP achieves high utilization (≥ 93%)
across a wide range of bottleneck link capacities varying
from 100Kbps to 5Gbps. The utilization gap in comparison
to XCP is at most 7% across the entire bandwidth range.
Additionally, as we scale the bandwidth of the link, the aver-
age (maximal) queue length decreases to about 0.01% (1%)
buﬀer size. The absolute persistent queue length is very
small for higher capacities, leading to negligible packet drop
rates (zero packet drops for many cases). At extremely low
capacities, e.g., 100Kbps (per-ﬂow BDP of 0.02 packets),
the bottleneck average queue signiﬁcantly increases to 50%
of the buﬀer size, resulting in roughly 0.6% packet loss. This
happens because the AI parameter setting (α = 1.0) is too
large for such low capacities.
Impact of Feedback Delay: We ﬁx the bottleneck ca-
pacity at 150Mbps and vary the round-trip propagation de-
lay from 1ms to 1500ms. As shown in Figure 4, we no-
tice that in most cases, the bottleneck utilization is higher
than 90%, and the average (maximal) queue is less than 5%
(15%) of the buﬀer size. We also observe that the RTT
parameter scaling is sensitive to very low values of RTT
(e.g., 1ms), thereby causing the average (maximal) queue
length to grow to about 15% (45%) of the buﬀer size. For
the RTT values larger than 800ms, VCP obtains lower uti-
lization (85%∼94%) since the link load factor measurement
interval tρ = 200ms is much less than the RTTs of the ﬂows.
As a result, the load condition measured in each tρ shows
variations due to the bursty nature of window-based control.
This can be compensated by increasing tρ; but the trade-oﬀ
is that the link load measurement will be less responsive
causing the queue length to grow. In all these cases, we did
not observe any packet drops in VCP.
Impact of Number of Long-lived Flows: With an in-
crease in the number of forward FTP ﬂows, we notice that
the traﬃc gets more bursty, as shown by the increasing trend
of the bottleneck maximal queue. However, even when the
network is very heavily multiplexed by 1000 ﬂows (i.e., the
average per-ﬂow BDP equals to only 1.5 packets), the maxi-
mal queue is still less than 38% of the buﬀer size. The aver-
age queue is consistently less than 5% buﬀer size as shown in
Figure 5 across all these cases. For the heavily multiplexed
cases, VCP even slightly outperforms XCP.
Impact of Short-lived Traﬃc: To study VCP’s perfor-
mance in the presence of variability and burstiness in ﬂow
0.60.70.80.91.01.1 0.1 1 10 100 1000Bottleneck UtilizationBottleneck Capacity (Mbps)VCP UtilizationXCP Utilization0%10%20%30%40%50% 0.1 1 10 100 1000Bottleneck Queue (% Buf)Bottleneck Capacity (Mbps)VCP Avg QueueXCP Avg Queue0.0%0.2%0.4%0.6%0.8%1.0% 0.1 1 10 100 1000Bottleneck Drops (% Pkt Sent)Bottleneck Capacity (Mbps)VCP Drop RateXCP Drop Rate0.60.70.80.91.01.1 1 10 100 1000Bottleneck UtilizationRound−trip Propagation Delay (ms)VCP UtilizationXCP Utilization0%5%10%15%20% 1 10 100 1000Bottleneck Queue (% Buf)Round−trip Propagation Delay (ms)VCP Avg QueueXCP Avg Queue0.0%0.2%0.4%0.6%0.8%1.0% 1 10 100 1000Bottleneck Drops (% Pkt Sent)Round−trip Propagation Delay (ms)VCP Drop RateXCP Drop Rate0.60.70.80.91.01.1 0 200 400 600 800 1000Bottleneck UtilizationNum of Long−lived FlowsVCP UtilizationXCP Utilization0%5%10%15%20%25%30% 0 200 400 600 800 1000Bottleneck Queue (% Buf)Num of Long−lived Flows  VCP Avg Queue  XCP Avg Queue0.0%0.2%0.4%0.6%0.8%1.0% 0 200 400 600 800 1000Bottleneck Drops (% Pkt Sent)Num of Long−lived FlowsVCP Drop RateXCP Drop RateFigure 6: Similar to XCP, VCP remains eﬃcient with low persistent queue and zero packet loss given the short-lived,
web-like ﬂows arriving/departing at a rate from 1/ s to 1000/ s.
Figure 7: VCP with multiple congested bottlenecks. For either all the links have the same capacity (100Mbps), or the
middle link #4 has lower capacity (50Mbps) than the others, VCP consistently achieves high utilization, low persistent
queue and zero packet drop on all the bottlenecks.
Figure 8: To some extent, VCP distributes bandwidth fairly among competing ﬂows with either equal or diﬀerent
RTTs. In all the case, it maintains high utilization, keeps small queue and drops no packet.
arrivals, we add web traﬃc into the network. These ﬂows
arrive according to the Poisson process, with the average
arrival rate varying from 1/ s to 1000/ s. Their transfer size
obeys the Pareto distribution with an average of 30 pack-
ets. This setting is consistent with the real-world web traf-
ﬁc model [11]. As shown by Figure 6, the bottleneck always
maintains high utilization with small queue lengths and zero
packet drops, similar to XCP.
In summary, we note that across a wide range of net-
work conﬁgurations with a single bottleneck link, VCP can
achieve comparable performance as XCP including high uti-
lization, low persistent queues, and negligible packet drops.
All these results are achieved with a ﬁxed set of parameters
shown in Table 1.
4.2 Multiple Bottlenecks
Next, we study the performance of VCP with a more com-
plex topology of multiple bottlenecks. For this purpose, we
use a typical parking-lot topology with seven links. All the
links have a 20ms one-way propagation delay. There are 50
FTP ﬂows traversing all the links in the forward direction,
and 50 FTP ﬂows in the reverse direction as well. In addi-
tion, each individual link has 5 cross FTP ﬂows traversing
in the forward direction. We run two simulations. First, all
the links have 100Mbps capacity. Second, the middle link
#4 has the smallest capacity of only 50Mbps, while all the
others have the same capacity of 100Mbps.
Figure 7 shows that for both cases, VCP performs as good
as in the single-bottleneck scenarios. For the ﬁrst case, VCP
achieves 94% average utilization, less than 0.2%-buﬀer-size
average queue length and zero packet drops at all the bot-
tlenecks. When we lower the capacity of the middle link, its
average utilization increases slightly to 96%, with the largest
maximal queue representing only 6.4% buﬀer size. In com-
parison to XCP, one key diﬀerence is that VCP penalizes
long ﬂows more than short ﬂows. For example, in the sec-
ond case, VCP allocates 0.39Mbps to each long ﬂow, and
4.96Mbps to each cross ﬂow that passes the middle link;
while all these ﬂows get about 0.85Mbps under XCP. We
discuss the reason behind this in Section 5.
4.3 Fairness
TCP ﬂows with diﬀerent RTTs achieve bandwidth allo-
cation that is proportional to 1/ rttz where 1 ≤ z ≤ 2 [44].
VCP alleviates this issue to some extent. Here we look at
the RTT-fairness of VCP. We have 30 FTP ﬂows sharing a
single 90Mbps bottleneck, with 30 FTP ﬂows on the reverse
path. We perform three sets of simulations: (a) the same
RTT; (b) small RTT diﬀerence; (c) huge RTT diﬀerence.
We will see that VCP is able to allocate bottleneck band-
width fairly among competing ﬂows, as long as their RTTs
are not signiﬁcantly diﬀerent. This capability degrades as
the RTT heterogeneity increases.
In the case where all the ﬂows have a common RTT or
have a small RTT diﬀerence, VCP achieves a near-even dis-
tribution of the capacity among the competing ﬂows (refer to
Figure 8). However, when the ﬂows have signiﬁcantly diﬀer-
ent RTTs, VCP does not distribute the bandwidth fairly be-
tween the ﬂows that have huge RTT variation (with through-
put ratio of up to 5). This fairness discrepancy occurs due to
0.60.70.80.91.01.1 0 200 400 600 800 1000Bottleneck UtilizationMice Arrival Rate (/s)VCP UtilizationXCP Utilization0%5%10%15%20%25%30% 0 200 400 600 800 1000Bottleneck Queue (% Buf)Mice Arrival Rate (/s)  VCP Avg Queue  XCP Avg Queue0.0%0.2%0.4%0.6%0.8%1.0% 0 200 400 600 800 1000Bottleneck Drops (% Pkt Sent)Mice Arrival Rate (/s)VCP Drop RateXCP Drop Rate 0.92 0.93 0.94 0.95 0.96 0.97 0.98 1 2 3 4 5 6 7Bottleneck UtilizationBottleneck ID     Same Bandwidth, UtilizationDifferent Bandwidth, Utilization0.0%0.1%0.2%0.3%0.4%0.5%0.6% 1 2 3 4 5 6 7Bottleneck Queue (% Buf)Bottleneck ID     Same Bandwidth, Avg QueueDifferent Bandwidth, Avg Queue0.0%0.2%0.4%0.6%0.8%1.0% 1 2 3 4 5 6 7Bottleneck Drops (% Pkt Sent)Bottleneck ID     Same Bandwidth, Drop RateDifferent Bandwidth, Drop Rate 0 1 2 3 4 5 0 5 10 15 20 25 30Flow Throughput (Mbps)Flow IDEqual RTT (40ms)Different RTT (40−156ms)Very Different RTT (40−330ms) 0 0.2 0.4 0.6 0.8 1 1.2 0 20 40 60 80 100 120Bottleneck UtilizationTime (sec)Equal RTT (40ms)Different RTT (40−156ms)Very Different RTT (40−330ms) 0 500 1000 1500 2000 0 20 40 60 80 100 120Bottleneck Queue (packets)Time (sec)Very Different RTT (40−330ms)Figure 9: VCP converges onto good fairness, high utilization and small queue. However, its fairness convergence
takes signiﬁcantly longer time than XCP.
Figure 10: VCP is robust against and responsive to sudden, considerable traﬃc demand changes, and at the same
time maintains low persistent bottleneck queue.
the following reason. A ﬂow with a very high RTT is bound
to have high values for their MI and AI parameters due to
parameter scaling (see Section 3.4). Due to practical oper-
ability constraints, we place artiﬁcial bounds on the actual
values of these parameters (speciﬁcally the MI parameter)
to prevent sudden bursts from VCP ﬂows which can cause
the persistent queue length at the bottleneck link to increase
substantially. These bounds restrict the throughput of ﬂows
with very high RTTs.
4.4 Dynamics
All the previous simulations focus on the steady-state be-
havior of VCP. Now, we investigate its short-term dynamics.
Convergence Behavior: To study the convergence be-
havior of VCP, we revert to the single bottleneck link with
a bandwidth of 45Mbps where we introduce 5 ﬂows into the
system, one after another, with starting times separated by
100s. We also set the RTT values of the ﬁve ﬂows to dif-
ferent values. The reverse path has 5 ﬂows that are always
active. Figure 9 illustrates that VCP reallocates bandwidth
to new ﬂows whenever they come in without aﬀecting its
high utilization or causing large instantaneous queue. (All
the ﬁgures of queue dynamics in this paper use the router
buﬀer size to scale their queue-length axis.) However, VCP
takes a much longer time than XCP to converge to the fair
allocation. We theoretically quantify the fairness conver-
gence speed for VCP in Theorem 4 in Section 5.
Sudden Demand Change: We illustrate how VCP re-
acts to sudden changes in traﬃc demand using a simple sim-
ulation. Consider an initial setting of 50 forward FTP ﬂows
with varying RTTs (uniformly chosen in the range [50ms,
150ms]) sharing a 200Mbps bottleneck link. There are 50
FTP ﬂows on the reverse path. At t=80s, 150 new forward
FTP ﬂows become active; then they leave at 160s. Figure 10
clearly shows that VCP can adapt sudden ﬂuctuations in the
traﬃc demand. (The left ﬁgure draws the congestion win-
dow dynamics for four randomly chosen ﬂows.) When the
new ﬂows enter the system, the ﬂows adjust their rates to
the new fair share while maintaining the link at high uti-
lization. At t=160s, when three-fourths of the ﬂows depart
creating a sudden drop in the utilization, the system quickly
discovers this and ramps up to 95% utilization in about 5
seconds. Notice that during the adjustment period, the bot-
tleneck queue remains much lower than its full size. This
simulation shows that VCP is responsive to sudden, signiﬁ-
cant decreases/increases in the available bandwidth. This is
no surprise because VCP switches to the MI mode which by
nature can track any bandwidth change in logarithmic time
(see Theorem 3 in Section 5).
We have also performed a variety of other simulations
to show VCP’s ability to provide bandwidth diﬀerentiation.
Due to the limited space we are unable to present the results
here. We refer the reader to our technical report for more
details [66].
5. A FLUID MODEL
To obtain insight into the behavior of VCP, in this section,
we consider a simple ﬂuid model, and analyze its stability
and fairness properties. We also analyze VCP’s eﬃciency
and fairness convergence speed.
Our model approximates the behavior of VCP using a
load-factor guided algorithm which combines the MI and AI
steps of VCP as described in (2) and (3) in Section 3.3:
˙wi(t) =
1
T
· [ wi(t) · ξ(ρ(t)) + α ]
(9)
with the MI parameter
ξ(ρ(t)) = κ · 1 − ρ(t)
ρ(t)
,
(10)
where κ > 0 is the stability coeﬃcient of the MI parameter.
In the remainder of this section we will refer to this model
as the MIAIMD model. It assumes inﬁnite router buﬀers,
and that end-hosts know the exact value of the load factor
ρ(t), as computed by the routers.
We start our analysis by considering a single bottleneck
link traversed by N ﬂows that have the same RTT, T . As
shown in Figure 11, the load factor ρ(t) received by the
source at a time t is computed based on the sender’s rate at
time t − T ,
 0 5 10 15 20 25 30 35 40 45 0 100 200 300 400 500 600Flow Throughput (Mbps)Time (sec)Flow 1 (rtt = 40ms)Flow 2 (rtt = 50ms)Flow 3 (rtt = 60ms)Flow 4 (rtt = 70ms)Flow 5 (rtt = 80ms) 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 600Bottleneck UtilizationTime (sec) 0 50 100 150 200 250 300 0 100 200 300 400 500 600Bottleneck Queue (packets)Time (sec) 0 10 20 30 40 50 60 70 80 90 0 50 100 150 200Congestion Window (packets)Time (sec)RTT: 60ms −− 158ms 0 10 20 30 40 50 60 70 80 90 0 50 100 150 200Congestion Window (packets)Time (sec)RTT: 60ms −− 158ms 0 10 20 30 40 50 60 70 80 90 0 50 100 150 200Congestion Window (packets)Time (sec)RTT: 60ms −− 158ms 0 10 20 30 40 50 60 70 80 90 0 50 100 150 200Congestion Window (packets)Time (sec)RTT: 60ms −− 158ms 0 0.2 0.4 0.6 0.8 1 0 50 100 150 200Bottleneck UtilizationTime (sec) 0 500 1000 1500 2000 2500 0 50 100 150 200Bottleneck Queue (packets)Time (sec)Figure 11: A simpliﬁed VCP model. The source sending
rate at time t − T is used by the router to calculate a
load factor ρ, which is echoed back from the destination
to the source at time t. Then the source adjusts its MI