# Title: Tempest: Soft State Replication in the Service Tier

## Authors
Tudor Marian, Mahesh Balakrishnan, Ken Birman, Robbert van Renesse  
Department of Computer Science, Cornell University, Ithaca, NY 14853  
{tudorm, mahesh, ken, rvr}@cs.cornell.edu

## Abstract
Soft state in the middle tier is crucial for enabling scalable and responsive three-tier service architectures. While soft state can be reconstructed upon failure, replicating it across multiple service instances is essential for rapid fail-over and high availability. Current techniques for storing and managing replicated soft state often require mapping data structures to different abstractions, such as database records, which can be difficult and introduce inefficiencies. 

Tempest is a system that provides programmers with data structures similar to conventional Java Collections but are automatically replicated. We evaluate Tempest against alternatives like in-memory databases and demonstrate that Tempest scales well in real-world service architectures.

## 1. Introduction
Service-Oriented Architectures (SOAs) have become the preferred paradigm for structuring large datacenter-hosted systems. Many contemporary large-scale applications, such as online stores, search engines, enterprise software, and financial infrastructure, are built using SOAs. The canonical design for such systems is a three-tier architecture: 
- **First Tier:** Load-balancing proxies that distribute requests.
- **Second Tier:** Stateless service logic that processes requests.
- **Third Tier:** Durable databases or filesystems that store persistent data.

Soft state in the service tier is key to building highly responsive and scalable SOAs. Soft state refers to data that does not need to be stored durably and can be reconstructed at some cost. Examples include short-lived user sessions, stored aggregates, transformations on large datasets, and general-purpose write-through caches for files and database records. Third-tier constructs are extremely fault-tolerant but slow and expensive, and soft state is typically used to limit their role in performance-critical data paths. For instance, an online travel service might use the memory of the service instance to store intermediate choices made by a user during the booking process, so that only the final transaction hits the third-tier database.

In this paper, we address the availability of soft state stored in the service tier. When soft state is lost or made unavailable due to service instance crashes and overloads, reconstructing it through user interaction or third-tier re-access can be time-consuming and resource-intensive. Replicating soft state provides two critical capabilities:
- Rapid fail-over to other instances during crashes.
- Fine-grained load-balancing across instances to prevent overload.

For example, a user request can be transparently redirected during a crash or overload to a different service instance that has up-to-date session context, without requiring the user to log in again.

Various options exist for adding high availability to programs that manipulate soft state, including clustered application servers, messaging toolkits, and collocated in-memory databases. However, these options require developers to write code in "state-aware" ways, mapping data structures to special replication-aware containers, replicated state-machine stores, and database-style records. Such mapping must be done carefully to avoid performance issues, such as severe locking contention. 

The natural way for programmers to store and manage soft state in a service is to use conventional in-memory data structures like hash tables or linked lists. In this paper, we present Tempest, a Java runtime library designed for easy storage and replication of service-level soft state. Tempest provides developers with TempestCollections: custom data structures that look similar to conventional Java Collections. Data stored in these structures is transparently and fully replicated across multiple machines, providing fail-over and load-balancing for soft (in-memory) state with zero extra effort by the developer.

Under the hood, Tempest uses a fast but unreliable IP multicast operation to spread/broadcast invocations to multiple service instances and then uses gossip-based reconciliation to maintain replica consistency in the face of faults and overloads. Additional adaptive mechanisms are used to maintain high responsiveness during failures.

High-performance in-memory databases are extensively used to store soft state in currently deployed systems, but we show that Tempest outperforms them in large-scale SOA settings. Real-world SOAs often involve many services interacting with each other to perform complex tasks. For example, a first-tier front-end might contact a hundred second-tier services to assemble a webpage. Each service is potentially contacted in parallel by a large number of load-balancing first-tier front-ends. Tempest scales in both the number of front-ends querying a single service and the number of services being queried by a single front-end. In contrast, in-memory databases fail to scale in these dimensions due to contention, large latency variations, and inefficiencies in cross-process interactions between the service and the database.

The contributions of this paper are as follows:
- We present a Java runtime library that exposes data structures to programmers that are transparently replicated across multiple nodes.
- We describe the gossip-based mechanisms used within the system for rapidly replicating data and speeding up access to it.
- We evaluate Tempest on two datacenter-style testbeds: the Emulab testbed at Utah and a 252-node cluster at Cornell. We show that Tempest maintains rapid responsiveness under heavy loads and outperforms in-memory and on-disk databases while scaling in two important dimensions: the number of front-ends accessing a single service and the number of services composing a single response.

## 2. The TempestCollection Abstraction

### 2.1 Service Model
Services are self-contained entities designed to support interoperable machine-to-machine interaction over a network. Each service exposes an API through which a set of methods can be invoked by clients, and each service offers its own quality of service and availability guarantees. For example, consider the interface of a shopping cart service as listed in Figure 1.

```java
public interface ShoppingCartIF extends Iterable {
    int add(String itemSymbol, int count);
    int remove(String itemSymbol, int count);
    int update(String itemSymbol, int count);
    int check(String itemSymbol);
}
```

Figure 1. 'Shopping Cart' service interface.

Add, remove, and update methods change the state and are classified as update operations. Check is a read operation; it retrieves the current number of items in the shopping cart for the symbol of interest. Clients issue add/remove/update and check requests against the service, and the service processes each request and returns a reply. This simple example can be extended to services like item browsing history, product availability, product rating, or caching services.

We assume that business logic is collocated with soft state stored in the memory of the service instance. Storing shopping cart information in-memory allows the service to handle a large quantity of browsing traffic that would otherwise reach the third tier. A developer implementing the shopping cart service in Java could use different data structures to store the state of the cart, such as a hash table to store mappings between item identifiers and corresponding counts.

Service state is modified by updates sent through its interface. In a conventional three-tier setup, this refers to database state hidden by the service, but in our case, it includes soft state maintained by the service. In the shopping cart example, items are added to or subtracted from the cart. The implementation of a service as a Java application running on a single node is prone to crashes, overloads, and slowdowns. Our goal is to transparently replicate a service on multiple nodes while retaining the programming ease and familiarity of Java's built-in Collection data structures. Accordingly, we provide developers with TempestCollections: data structures very similar to vanilla Collections but providing automatic replication of the data stored in them.

### 2.2 TempestCollection: Syntax and Semantics
TempestCollections are syntactically identical to standard Java Collections. For example, a TempestHashtable exposes get and put methods, while a TempestSet has add and remove methods. Like most Java Collections, objects stored in a TempestCollection cannot be modified in place. To change a field inside an object stored in a TempestSet, the programmer must remove the object, modify it, and then re-insert it into the set.

This is a common programming idiom within the Java Collections framework. For example, Java TreeSets provide ordered iteration over their elements, and changing the value of an item in-place can push the TreeSet into an inconsistent state by modifying the outcome of comparison operations. Programmers are expected to change values by removal, modification, and re-insertion if they want the TreeSet to remain consistent and ordered.

Many Collections involve comparisons through `equals` and `compareTo` methods, such as HashMaps, TreeSets, or HashSets, and do not allow safe in-place modification of objects stored within them. In this respect, TempestCollections offer identical semantics.

To prevent accidental modification of stored items, TempestCollections implement by-value parameter passing: deep clones of added objects are stored within the TempestCollection, and clones of stored objects are returned by accessor functions. For example, calling `put(K, A)` on a TempestHashMap will result in a clone `A'` being stored within the collection, and calling `get(K)` will return `A''` to the programmer.

However, the Tempest runtime can alter the contents of TempestCollections by adding and/or removing items to keep collections consistent across replicas. TempestCollections provide eventual consistency, meaning all replicas converge to the same set of objects. An implication of this model is that the programmer is not provided with ACID transactions; however, this is not a major limitation for soft state management. In many soft state applications, data stored within structures is naturally immutable, such as a browsing history service that stores a list of item identifiers. For others, updates do not depend on the current state, such as a map from user identifiers to last viewed items. Even if the soft state is manipulated with arbitrary operations, it is expected by definition to not have strong semanticsâ€”the user is always asked to verify the contents of a shopping cart or the final itinerary of a travel plan before committing to it.

To summarize, TempestCollections are data structures exposing interfaces identical to those in the Java Collections framework and supporting similar semantics by not allowing in-place modifications of stored objects. The sole deviation, aside from the weak consistency implications, is that Tempest enforces object immutability by passing parameters by-value, which can result in services operating on stale data. By deliberately choosing a weaker consistency model, we had more opportunities to provide a massively scalable solution, requiring developers to understand and account for unreliable soft state.

## 3. Tempest Architecture
In this section, we describe the mechanisms used to implement replicated TempestCollections. Tempest services reside on second-tier servers, with a single server representing the primary replica and multiple secondary replicas. The architecture is shown in Figure 2.

[Insert Figure 2 here]

### 3.1 Data Replication
Tempest uses a combination of IP multicast and gossip-based reconciliation to replicate data across multiple service instances. IP multicast is used to quickly spread broadcast invocations to multiple service instances, while gossip-based reconciliation ensures that all replicas remain consistent even in the presence of faults and overloads. Adaptive mechanisms are employed to maintain high responsiveness during failures.

### 3.2 Consistency and Performance
TempestCollections provide eventual consistency, ensuring that all replicas converge to the same set of objects. This model is suitable for soft state management, where strong consistency is not a requirement. By using a weaker consistency model, Tempest achieves better scalability and performance compared to in-memory databases, which suffer from contention, large latency variations, and inefficiencies in cross-process interactions.

## 4. Evaluation
We evaluated Tempest on two datacenter-style testbeds: the Emulab testbed at Utah and a 252-node cluster at Cornell. Our results show that Tempest maintains rapid responsiveness under heavy loads and outperforms in-memory and on-disk databases in two important dimensions: the number of front-ends accessing a single service and the number of services composing a single response.

### 4.1 Testbed Setup
- **Emulab Testbed:** Used to simulate a variety of network conditions and failure scenarios.
- **Cornell Cluster:** Used to evaluate performance and scalability under realistic workloads.

### 4.2 Results
- **Responsiveness:** Tempest maintains low latency and high throughput even under heavy loads.
- **Scalability:** Tempest scales well with the number of front-ends and services, outperforming in-memory and on-disk databases.
- **Fault Tolerance:** Tempest demonstrates robust fail-over and load-balancing capabilities, ensuring high availability and reliability.

## Conclusion
Tempest is a Java runtime library that provides developers with data structures similar to conventional Java Collections but with automatic replication across multiple nodes. By using a combination of IP multicast and gossip-based reconciliation, Tempest ensures eventual consistency and high performance in large-scale SOA settings. Our evaluation on two datacenter-style testbeds demonstrates that Tempest outperforms in-memory and on-disk databases in terms of responsiveness, scalability, and fault tolerance.