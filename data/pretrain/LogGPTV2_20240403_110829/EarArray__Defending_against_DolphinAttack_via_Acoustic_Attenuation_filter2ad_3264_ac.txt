As shown in Fig. 7, these curves correspond to a speaker
changing from 0◦ to 360◦ with a step of 30◦ on the x-y
plane and launching baseband signals directly or modulated
the baseband on carrier signals. The variance curve of audible
signals ranges from 0.17 to 0.5. However, as for AM modulated
signals, we can see the energy variance ranges from 0.65 to
1 when the distance is 30 cm. With the distance between the
speaker and the microphone increases, the variation range of
variance curve is between 0.15 and 0.62, for high-frequency
5
(a) d = 30cm            (b) d = 60cm     (c) d = 90cm(d) d = 120cm    (e) d = 150cm     (f) d = 180cm(a) d = 30cm               (b) d = 60cm                (c) d = 90cm(d) d = 120cm               (e) d = 150cm             (f) d = 180cmFig. 7. Veriﬁcation of acoustic attenuation. Under the setting of Fig. 4, acoustic attenuation is calculated as variance of the band power of sound signals from
ﬁve microphones under various settings, e.g., distance between sound source and the voice assistant, angles, and carrier frequency of ultrasound. The attack is
launched at different distances range from 30 cm to 180 cm. Clear difference of ultrasound and audible sound has been shown as contrast.
sound, the variation ranges from 0.57 to 1. The difference
between acoustic signal and high-frequency modulated signal
is signiﬁcant.
In summary, we observe that the diffraction attenuation in-
creases with acoustic frequency. As the acoustic frequency in-
creases, the sound ﬁeld becomes signiﬁcantly spatial uniform.
Thus, the difference of high-frequency ultrasound received by
the ﬁve microphones is higher than low-frequency sound.
(VAD) algorithm [19]. VAD is a common method of detecting
the presence or absence of speech in sound signals. To detect
and delete non-speech intervals, a band-power-based detection
algorithm is used. To calculate band power of each channel, we
ﬁrst compute power spectral density (PSD) based on Welch’s
method which reduces noise in the estimated power spectra
and then compute the band power in the given frequency range.
The equation of VAD can be expressed as:
(cid:40) yn(t), pn > thres
IV. DESIGN OF EA RAR R A Y
y(t) =
In this section, we introduce the design of EarArray to
detect inaudible voice commands based on acoustic attenua-
tion.
A. Overview
Fig. 8 shows the overview of the system architecture. The
voice commands are ﬁrst captured by the built-in microphone
array on a device, e.g., the Echo dot and the audio signals are
then fed into our EarArray system. Finally, the EarArray
system will output the detection result i.e., whether the com-
mand is a DolphinAttack signal or from a human user. To
achieve the above purpose, we have designed EarArray and
it mainly consists of three major components, which are 1)
Audio signal preprocessing, 2) Feature extraction and 3) Attack
detection & localization.
B. Audio Signal Preprocessing
The audio signal preprocessing module is used to ﬁlter the
noise in the input signals from multiple microphones and then
prepare audio samples with a speciﬁc length for the feature
extraction module.
Signal denoising. Due to sound interference from the
environment,
the signals from microphones are noisy. To
improve the SNR of the signal, we exploit a band-pass ﬁlter to
get rid of interference from unwanted frequencies. Considering
the fact that the typical frequency of human sound is from 50
Hz to 2 kHz, we set the cut-off frequencies of the band-pass
ﬁlter as 50 Hz and 2 kHz respectively in our design.
Voice activity detection and segmentation. The sound
signal after microphones are a sequence of speech signals
interleaved with non-speech signals. To further improve the
quality of the sound signal, we choose to abandon the non-
speech signal intervals. To do this, we ﬁrst detect non-speech
signal
intervals by exploiting the voice activity detection
N on − speech, pn ≤ thres
(8)
Where, y(t) denotes the voice signal after VAD, yn(t) denotes
the nth segment, pn denotes the band power of the nth
segment. thre can be expressed as:
thres = λ1 ∗ max(p) + λ2 ∗ min(p), p = p1, p2, ...pn
(9)
Where, λ1 and λ2 can be set to 0.04 and 3.
Speciﬁcally, we divide a whole sound signal into several
segmentations with a step of 400 ms and the overlap of each
frame is set to 200 ms. For each signal segmentation, we
calculate its power of speciﬁc frequency band (50–2000 Hz)
and discard those whose band power lower than a threshold.
By doing this, the non-speech signal segmentations can
be removed and only speech-related signal segmentations are
kept,
the
VAD algorithm is applied on the channel with the highest
band power since all channels are almost synchronized, the
non-speech signal of the other channels can be abandoned
according to the highest band power channel.
the process is shown in Fig. 9. And note that,
C. Feature Extraction
For the segmentations from the audio signals, we inves-
tigate the features representing the spatial inhomogeneity of
sound. The preliminary analysis using band power variance
computed by ﬁve channel signals indicate that
the sound
characteristics generated by pure-tone signal can be clearly
distinguished from pure-tone AM signals. In the next, we
calculate three representative features, the feature extraction
process depicted in Fig. 9.
Range and standard deviation of band power. As the
speech signal is a narrow frequency bandwidth signal, we use
the frequency band power to indicate the sound intensity on
ﬁve channels. The range of band power can be expressed as:
6
(a) d = 30cm(b) d = 60cm(c) d = 90cm(d) d = 120cm(e) d = 150cm(f) d = 180cmFig. 8. The workﬂow of EarArray. The audio signals are ﬁrst captured from multiple microphones on a device and then fed into the detection component
which includes pre-processing, feature extraction, and attack detection. Attack detection results including attack source orientation will be output.
Fig. 9. Pre-processing workﬂow. VAD algorithm is used to detect the speech and non-speech segments, and then the speech segments are concatenated. Our
window size is 0.4 s, and the overlap between each window is set to 0.13 s. The features can be obtained by calculating the energy of each speech window in
the same column as the Figure depicts.
range = max(P ) − min(P ), P = {p1, p2...pm}
(10)
Where, pm denotes the band power of the mth channel. In
the same way, we can also get the standard deviation of band
power std.
Pearson Correlation Coefﬁcient. Besides range, std, we
use the Pearson correlation coefﬁcient corr between two
spectra to estimate the uniform of the sound signals instead of
using time-domain signals, that is because the phase difference
between any two channels’ signal will affect
the Pearson
correlation coefﬁcient. In the frequency domain, the phase
difference can be eliminated. As we have 5 channel signals,
we choose the pair of signals with the biggest difference in
energy to get corr.
Finally, we obtain 3 different features range, std, corr
to represent the uniform of measured sound ﬁeld. To show
the feasibility of using the 3 features to detect
inaudible
voice commands, we calculate the features of inaudible voice
commands and normal voice commands and show the results
in Fig. 11, from which we can ﬁnd that the two different
types of features are distinguishable and the gap between them
is obvious. Thus, these features can represent the difference
between sound signal distribution.
D. Attack Detection and Localization
Machine (SVM) as the classiﬁer in our design considering its
simplicity and low-cost in terms of computation. We collect
multi-channel voice segmentations in the off-line training
phase as training samples,
the voice samples include two
types of voice commands: 1) Inaudible voice commands with
different carrier frequencies, e.g., 25 kHz, 40 kHz, etc.,; 2)
Audible voice commands. Both of the samples are collected
by the specially-designed microphone-array device, as shown
in Fig. 10. The traces are collected at different
locations
around the sound source. After training with these samples,
the characteristics of the two sound signals will be registered
in the trained model.
In the process of the detection phase, features of the
unknown label voice samples will be calculated and ﬁnally
classiﬁed according to the trained model.
2) Orientation Localization: After detecting there is an
inaudible voice command attack, EarArray can also report
the orientation of the attacker. Almost all of the popular
smart speakers support sound source localization based on the
TDoA algorithm, this kind algorithm can work well in sound
source localization when the signal-to-ratio (SNR) is high.
However, the signal in some channels is too weak to apply the
TDoA algorithm effectively as the serious attenuation of high-
frequency inaudible voice commands. Thus, the performance
of the method will dramatically decrease under attack.
1) Attack Detection: EarArray utilizes a machine learn-
ing (ML) based method to detect the inaudible voice com-
mands with the above features. We choose Support Vector
To overcome the above localization challenge facing an
inaudible voice command attack, we propose a band-power-
based localization method for each microphone channel to
7
De-noisingSegmentationAttack or notOrientationVoice Activity DetectionEarArraySystemML-based DetectionPre-processingFeature ExtractionRange of BPRSS-aidedLocalizationResultsDetectionStandard VarianceCorrelation CoefficientSmart DeviceMulti-channelFig. 10. The experimental setup. Three self-made ultrasonic speaker arrays with center frequencies of 25, 32, 40 kHz. A hardware signal modulator is used.
Fig. 11.
The feasibility to detect inaudible attacks with the three features
which including range, standard deviation (std), and Pearson Correlation
Coefﬁcient (corr) of band power
infer the direction of the attacker. As the band power in each
channel is sensitive to distance and obstacles, the microphone
facing toward sound source has the strongest signal strength
and vice versa. Thus we regard the attacker is from the
direction where the microphone has the strongest power. As
a fact, the attacker orientation can be located in a quite large
angle because we cannot have a very ﬁne-grained signal power
estimation. Thus, in EarArray the localization module only
outputs a coarse direction, namely, “North”, “South”, “West”
or “East” and the user can further identify the location of the
attacker by looking at the structure of the house like windows
and walls. We evaluate the performance of such a coarse
localization in the Sec. V.
V.
IMPLEMENTATION AND EVALUATION
In this section, we start with the introduction of our
specially-designed prototype of a microphone array to better
evaluate the performance of EarArray and then elaborate on
the evaluation.
A. Implementation
To verify the performance of EarArray, we design a
special 3-D microphone array as we don’t have permission to
record multi-channel sound on popular smart devices. Imagine
Fig. 12.
(a) The specially-designed prototype of a microphone array which
supports 9 channels, and we use 5 microphones in this paper; (b) The prototype
of a smartphone with three microphones.
that as the acoustic wave encounters the smart speaker, the
backside will produce a shadow region, and with increasing
sound frequency, the shadow will become more signiﬁcant,
and this kind of spatial property of the sound ﬁeld will be
completely captured by the 3-D microphone array. A data
acquisition card is used to collect ﬁve channel signals for sub-
sequent analysis. To play the inaudible voice commands, we
design 3 ultrasound transducer arrays with center frequencies
of 25, 32, 40 kHz which are shown in Fig. 10(b).
As we can see from Fig. 12(a), the height of the cylinder
body of the microphone array is 15 cm and its diameter is 10
cm, the distance between the sound inlet of each microphone
and the top edge is set to 1.2 cm. Five ADMP401 MEMS
microphones [23] were used in our prototype. The microphone
array can be applied to fully measure the sound ﬁeld instead of
all microphones are located on the top plane. In particular, we
use one microphone located on the center of the top surface
of the cylinder, which is like the Echo microphone placement
in Fig. 3. The other four microphones are uniformly located
around the curve of the top surface. To evaluate the perfor-
mance of EarArray on commercial smartphones, we also
design and implement a prototype with only 3 microphones,
whose size is 15 × 7.5 × 1cm. The three microphones are
located on the top, back surface, and bottom side respectively,
8
rangecorrstd00.20.50.40.10.60.80.210.30.410.5Inaudible voiceAudible voiceTABLE II.
THE LIST OF VOICE COMMANDS USED IN THE EXPERIMENT.
Speaker
TTS & Volunteers ∗
Voice command
How is the weather today
Turn on airplane mode
Call 1 2 3 4 5
Facetime 6 7 8 9 0
Read my new messages
∗ In the experiment, we totally recruited 25 volunteers
(5 females and 20 males, aged between 20 and 29)
as shown in Fig. 12(b).
Using the specially designed smart speaker, we conducted
experiments to evaluate the effectiveness of EarArray in
terms of various factors, including carrier frequencies, attack
distances, angles, background noise, and voice commands
types. We also use the smartphone prototype to evaluate the
performance of EarArray on existing hardware with three
microphones. According to these experiments, we demonstrate
that EarArray can detect the inaudible voice commands
with accuracy of above 99%, meanwhile,
the accuracy of
localization is up to 97.89%. We summarize the main results
as follows:
•
•
•
EarArray shows the detection accuracy can be up
to 99% in various conditions and positions.
EarArray can achieve localization accuracy as high
as 97.89%.
EarArray is robust in terms of attack parameters,
i.e., attack distance, ambient noise, the angle between
a smart speaker and the attacker, etc.
B. Experimental Settings
Hardware setup. The experimental setup is shown in
Fig. 4 and Fig. 10. The benchtop transmitter is used for
modulating the voice commands played by a smartphone and
then emitting the inaudible voice commands with 3 narrow
bandwidth ultrasonic transducer arrays (The center frequencies
are 25, 32, 40 kHz respectively). The low-frequency audible
voice commands will be played by Bluetooth speaker JBL
GO controlled by an iPhone X. In our experiments, we use
the designed microphone array as the victim device. And the
positions of ultrasonic speaker and JBL GO are controlled by
ϕ, θ, and R as shown in Fig. 4.
Voice commands. We recruited 25 volunteers including 5
females and 20 males whose ages range between 20 to 29.
The volunteers were required to speak the 5 voice commands