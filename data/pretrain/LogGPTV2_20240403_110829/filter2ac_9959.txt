# Measured Comparative Performance of TCP Stacks

**Authors:**
- Sam Jansen<sup>1</sup>
- Anthony McGregor<sup>1,2</sup>

**Affiliations:**
1. Department of Computer Science, The University of Waikato
2. National Laboratory for Applied Network Research (NLANR)<sup>*</sup>

---

## Abstract

This extended abstract presents findings on the measured performance of various TCP stacks. We have observed significant differences in the TCP implementations found in Linux, FreeBSD, OpenBSD, and Windows XP.

---

## 1. Introduction

Implementations of a protocol can vary in many respects, including their performance. Several factors contribute to this variation. When implementing an Internet protocol, developers refer to the protocol's specifications, such as RFCs. These specifications are often written in natural language and may contain ambiguities. Some aspects of the protocol's behavior are left to the implementer's discretion. Even with well-specified protocols, implementations do not always meet the specification correctly due to logic errors or misinterpretations. In some cases, decisions that violate the specification are made to achieve better performance.

In 1997, Paxson analyzed TCP by developing a tool to automatically analyze large amounts of trace data [1]. This tool was successful in identifying implementation issues in various TCP variants of the time. However, it has a significant limitation: the code needs to be updated and customized for each TCP implementation being studied.

Previous research on TCP performance has focused on specific types of congestion control [2], sometimes under specific conditions such as mobile ad hoc networks [3], lossy radio links [4], or ATM [5]. Studies comparing different TCP variants, such as New Reno, Vegas, and Westwood+ [6], have also been conducted. Paxson's research involved TCP stacks from 1997, including Solaris 2.4, NetBSD 1.0, Linux 1.0, Windows 95, and Windows NT. Since then, TCP has evolved significantly. This paper focuses on TCP implementations used in 2004 and 2005.

We hypothesize that modern TCP implementations will perform correctly under congestion, regardless of their BSD lineage, in contrast to Paxson's findings. Additionally, we believe that TCP implementations have diversified sufficiently to show significant differences in measured performance, whether they are of BSD lineage or not. We use a test-bed network called the WAND Emulation Network, which is described in the next section. Some measured TCP performance results are presented in Section 3.

<sup>*</sup>NLANR Measurement and Network Analysis Group (NLANR/MNA) is supported by the National Science Foundation (NSF) under cooperative agreement no. ANI-0129677.

---

## 2. Emulation Network

The WAND Network Research Group has built a network of 24 machines dedicated to network testing. The machines are configured to have a control network connecting them to a central control machine and an emulation network, which can be reconfigured by changing patch panels. Each machine has one Ethernet card connected to the control network and one Ethernet card connected to the emulation network, which has four ports for router machines. This setup allows the creation of arbitrary network topologies with a maximum speed of 100 Mbit/s.

All machines are connected through a central switch to the control machine and have serial connections to the same machine. To simulate link delay and bandwidth limits, FreeBSD Dummynet [7] routers are used.

The control machine can install operating system images onto the machines on the emulation network in less than five minutes, enabling the testing of various operating systems in a short time span. Images of Linux, FreeBSD, OpenBSD, Solaris, and Windows XP are available.

Scripts can be written to run commands on the machines on the emulation network and send their output back to the control machine, allowing for the design, execution, and recording of tests on the control machine.

---

## 3. TCP Performance

The following tests were performed using the following operating systems: Linux, FreeBSD, OpenBSD, and Windows XP with Service Pack 2.

### 3.1 Bidirectional Random Loss

This section presents a study of TCP performance under random loss in both directions, where both data and acknowledgment packets are dropped randomly using a uniform model. Random loss is interesting to study because Lakshman and Madhow [8] report that it is a simple model for transient congestion and is relevant in the context of networks with multimedia traffic.

Figure 1 shows the topology used in this test. The bottleneck link is configured with a propagation delay of 100 ms and a bandwidth limit of 2 Mb/s. Router R1 drops packets randomly using Dummynet's packet loss rate option. The goodput over a single TCP stream from host H1 to H2 is measured. Goodput is the amount of data successfully read from the TCP socket by the application at the receiving end of a TCP connection. Hosts H3 and H4 are unused. Each test lasted 60 seconds.

![Test Network Setup](fig1.png)

Table 1 shows the recorded performance in kilobits per second under 5% bidirectional loss. The four numbers in the table are recorded goodput: minimum, mean, maximum, and standard deviation. For each network stack, the test was run 100 times. All tests were run with kernel parameters set to their defaults. Increasing the buffer sizes of any of the stacks studied made little difference, even though Windows XP defaults to only 8 kB (compared to up to 64 kB on other operating systems). While there is variation from run to run, it is small compared to the mean. Measurements with SACK turned on and off showed that SACK increases performance by just over 5% in this scenario.

| TCP Implementation | Min (kbps) | Mean (kbps) | Max (kbps) | SD (kbps) |
|---------------------|------------|-------------|------------|-----------|
| Linux 2.6.10        | 164.3      | 213.9       | 287.6      | 22.7      |
| Linux 2.4.27        | 136.7      | 176.2       | 225.0      | 17.1      |
| FreeBSD 5.3         | 128.7      | 162.8       | 219.0      | 19.5      |
| FreeBSD 5.2.1       | 89.9       | 137.3       | 191.0      | 21.6      |
| Windows XP SP2      |            |             |            |           |
| OpenBSD 3.5         | 63.8       | 117.9       | 166.8      | 22.1      |

### 3.2 Reverse Path Congestion

The test network topology is the same as presented in the previous section (Figure 1). No artificial loss is added by routers R1 or R2, but host H4 sends data over a single TCP stream to host H3. The TCP stream from host H1 to host H2 is measured. The buffer sizes on routers R1 and R2 are set to 8 packets, and the bottleneck link is set at 2 Mb/s with a 50 ms delay. This configuration allows R2 to be congested by the TCP stream from H4 to H3, which in turn congests the acknowledgments of the measured TCP stream. H3 and H4 use Linux 2.4.27, while the operating system on H1 and H2 varies. The stacks are configured as in Section 3.1, except for the size of the TCP socket buffers. The TCP socket buffer size for both receive and send buffers is set to 64 kB for all network stacks in the test.

Table 2 shows the measured goodput at host H2 in kilobits per second. The variation between stacks is not as large as in the previous section, but there is still a significant difference of 32% between the lowest and highest.

| TCP Implementation | Min (kbps) | Mean (kbps) | Max (kbps) | SD (kbps) |
|---------------------|------------|-------------|------------|-----------|
| Linux 2.4.27        | 1220       | 1296        | 1375       | 33.3      |
| FreeBSD 5.3         | 1099       | 1205        | 1289       | 48.6      |
| FreeBSD 5.2.1       | 906        | 1024        | 1152       | 58.5      |
| Windows XP          |            |             |            |           |
| OpenBSD 3.5         | 1273       | 1352        | 1438       | 40.4      |

---

## 4. Summary

This abstract demonstrates that there are significant differences in the measured performance of the TCP stacks studied: Linux, FreeBSD, OpenBSD, and Windows XP. During bidirectional random loss, the Linux TCP stack achieves the highest goodput. In this scenario, OpenBSD achieves just over half the goodput measured with Linux 2.4 and 2.6 kernels, while Windows XP achieves only 64% of the goodput measured with Linux. Windows XP is additionally limited by its default TCP window sizes, which are very small by today's standards.

Further analysis of these results is not presented due to space constraints.

---

## References

1. Paxson, V.: Automated packet trace analysis of TCP implementations. In: SIGCOMM. (1997) 167–179
2. Fall, K., Floyd, S.: Comparison of Tahoe, Reno, and SACK TCP (1995)
3. Holland, G., Vaidya, N.H.: Analysis of TCP performance over mobile ad hoc networks. In: Mobile Computing and Networking. (1999) 219–230
4. Kumar, A.: Comparative performance analysis of versions of TCP in a local network with a lossy link. IEEE/ACM Transactions on Networking 6 (1998) 485–498
5. Comer, D., Lin, J.: TCP buffering and performance over an ATM network (1995)
6. Grieco, L.A., Mascolo, S.: Performance evaluation and comparison of Westwood+, New Reno, and Vegas TCP congestion control. SIGCOMM Comput. Commun. Rev. 34 (2004) 25–38
7. Rizzo, L.: Dummynet: a simple approach to the evaluation of network protocols. ACM Computer Communication Review 27 (1997) 31–41
8. Lakshman, T.V., Madhow, U.: The performance of TCP/IP for networks with high bandwidth-delay products and random loss. IEEE/ACM Transactions on Networking 5 (1997)

C. Dovrolis (Ed.): PAM 2005, LNCS 3431, pp. 329–332, 2005.
c Springer-Verlag Berlin Heidelberg 2005