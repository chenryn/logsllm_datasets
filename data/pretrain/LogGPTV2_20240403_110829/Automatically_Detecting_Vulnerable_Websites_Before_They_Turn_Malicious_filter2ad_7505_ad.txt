side of the window are discarded by applying a coefﬁ-
cient of 0. Various functions such as linear and expo-
nential may be used to generate coefﬁcients that scale
observations and grant additional emphasis on recent ob-
servations of a feature.
5 Experimental results
We evaluate here both our dynamic feature extraction al-
gorithm, and the overall performance of our classiﬁer, by
providing ROC curves.
634  23rd USENIX Security Symposium 
USENIX Association
10
Feature
meta{’content’: ’WordPress 3.2.1’, ’name’: ’generator’}
ul{’class’: [’xoxo’, ’blogroll’]}
You can start editing here.
meta{’content’: ’WordPress 3.3.1’, ’name’: ’generator’}
/all in one seo pack
span{’class’: [’breadcrumbs’, ’pathway’]}
If comments are open, but there are no comments.
div{’id’: ’content disclaimer’}
Stat.
0.0569
0.0446
0.0421
0.0268
0.0252
0.0226
0.0222
0.0039
Table 2: Selection of the top features after processing
the ﬁrst 90,000 examples. These features are a chosen
subset of the top 100 features determined by the system
after 90,000 examples had been observed and using win-
dowing with a window size of 15,000 examples and lin-
ear attenuation.
5.1 Dynamic Feature Extraction
We analyzed dynamic features by logging the values of
the statistic AAC2 after adding every example to the sys-
tem. We selected a few particular features from a very
large set of candidates to serve as examples and to guide
intuition regarding dynamic feature extraction. The pro-
cess of feature extraction could be performed indepen-
dently of classiﬁcation and was run multiple times under
different conditions to explore the effect of different pa-
rameters such as the use of windowing and attenuation.
l
e
u
a
V
2
C
C
A
 0.16
 0.14
 0.12
 0.1
 0.08
 0.06
 0.04
 0.02
 0
 0
Wordpress 2.9.2
Wordpress 3.2.1
Wordpress 3.3.1
Wordpress 3.5.1
 50000  100000  150000  200000  250000  300000  350000  400000
Sample #
Table 2 shows a selection of the 200 tags with highest
statistic value after 90,000 examples had been passed to
the system using a window size of 15,000 examples and a
linear weighting scheme. A meaningful feature, i.e., with
a large statistic value, is either a feature whose presence
is relatively frequent among examples of malicious sites,
or whose presence is frequent among benign sites. Of
the 15,000 sites in the window used for generating the
table, there were 2,692 malicious sites, and 12,308 be-
nign ones. The feature ul{’class’: [’xoxo’, ’blogroll’]}
was observed in 736 malicious sites and 1,027 benign
ones (461.34 malicious, 538.32 benign after attenuation)
making it relatively more frequent in malicious sites. The
feature div{’id’: ’content disclaimer’} was observed in
no malicious sites and 62 benign ones (47.88 benign af-
ter attenuation) making it more frequent in benign sites.
After manual inspection, we determined that this feature
corresponded to a domain parking page where no other
content was hosted on the domain.
l
e
u
a
V
2
C
C
A
 0.08
 0.07
 0.06
 0.05
 0.04
 0.03
 0.02
 0.01
 0
 0
1k Window
5k Window
15k Window
50k Window
100k Window
 50000  100000  150000  200000  250000  300000  350000
 400000
Sample #
Figure 9: Statistic value for meta{’content’: ’Word-
Press 3.3.1’, ’name’:
’generator’} over time. The
statistic was computed over the experiment using win-
dow sizes of 1,000, 5,000, 15,000, 50,000 and 100,000
samples and uniform weighting.
Statistic value for various tags corre-
Figure 8:
sponding to different version of the Wordpress con-
tent management system.
Dynamic feature extraction is essential, as it allows
the system to automatically identify features useful in
predicting if a domain will become malicious; this au-
tomation is imperative in a concept-drifting domain. For
example, Figure 8 shows the computed statistic value for
various features that correspond directly to different ver-
sions of the Wordpress CMS. Over time, the usefulness
of different features changes. In general, as new versions
of a CMS are released, or new exploits are found for ex-
isting ones, or completely new CMSs are developed, the
set of the features most useful for learning will be con-
stantly evolving.
l
e
u
a
V
2
C
C
A
 0.1
 0.09
 0.08
 0.07
 0.06
 0.05
 0.04
 0.03
 0.02
 0.01
 0
 0
Uniform Weighting
Linear Weighting
Exponential Weighting
 50000  100000  150000  200000  250000  300000  350000  400000
Sample #
Figure 10: Statistic value for meta{’content’: ’Word-
Press 3.3.1’, ’name’:
’generator’} over time. The
statistic was computed over the experiment using a win-
dow size of 15,000 samples and various weighting tech-
niques.
USENIX Association  
23rd USENIX Security Symposium  635
11
 0.016
 0.014
 0.012
 0.01
 0.008
 0.006
 0.004
 0.002
l
e
u
a
V
2
C
C
A
 0
 0
 50000  100000  150000  200000  250000  300000  350000
Sample #
l
e
u
a
V
2
C
C
A
 0.05
 0.045
 0.04
 0.035
 0.03
 0.025
 0.02
 0.015
 0.01
 0.005
 0
 0
 50000  100000  150000  200000  250000  300000  350000  400000
Sample #
Statistic value for div{’id’:
’con-
Figure 11:
tent disclaimer’} over time.
The statistic was com-
puted over the experiment using a window of size 15,000
samples and linear attenuation.
Figure 12: Statistic value for meta{’content’: ’Word-
Press 3.3.1’, ’name’: ’generator’} over time.
The
statistic was computed over the experiment using a win-
dow of size 15,000 samples and linear attenuation.
The calculation of the ACC2 statistic for a feature
at a particular time is parameterized by the window
size and by a weighting scheme. As an example, Fig-
ure 9 shows the value of the statistic computed for the
tag meta{’content’: ’WordPress 3.3.1’, ’name’: ’gen-
erator’} over the experiment using different window
sizes. When using a window, we compute the statistic by
only considering examples that occurred within that win-
dow. We made passes over the data using window sizes
of 1,000, 5,000, 15,000, 50,000 and 100,000 samples,
which approximately correspond to 3 days, 2 weeks,
7 weeks, 24 weeks, and 48 weeks respectively.
A small window size generates a statistic value ex-
tremely sensitive to a few observations whereas a large
window size yields a relatively insensitive statistic value.
The window size thus yields a performance trade-off.
If the statistic value for a feature is computed with a
very small window, then the feature is prone to being in-
correctly identiﬁed as meaningful, but will correctly be
identiﬁed as meaningful with very low latency as only a
few observations are needed. A large window will result
in less errors regarding the usefulness of a feature but
will create a higher latency.
Figure 10 shows the effect of varying the weighting
scheme with a constant window size. Using a weight-
ing scheme gives higher weight to more recent examples
and the effect is very similar to simply decreasing the
window size. There is almost no difference between ex-
ponential and linear decay.
Features belonging to positive (malicious) and nega-
tive (benign) examples often carry with them their own
characteristics. The statistic values of negative examples
tend to be relatively constant and time-invariant as the
example in Figure 11 shows. These are generally fea-
tures that indicate a lack of interesting content and there-
fore a lack of malicious content—for instance, domain
parking pages. Conversely, the statistic value of positive
examples tend to contain a large spike as evidenced by
the example in Figure 12. The features correspond to
vulnerable software and spike when an attack campaign
exploiting that vulnerability is launched. Occasionally,
additional spikes are observed, presumably correspond-
ing to subsequent campaigns against unpatched software.
A design consideration when working with dynamic
features is whether or not it is appropriate to use features
that were highly ranked at some point in the past in ad-
dition to features that are currently highly ranked. As
discussed above, negative features tend to be relatively
constant and less affected, unlike positive features which
ﬂuctuate wildly. These positive features tend to indicate
the presence of software with a known vulnerability that
may continue to be exploited in the future.
Since it may happen that a feature will be useful in
the future, as long as computational resources are avail-
able, better classiﬁcation performance can be achieved
by including past features in addition to the current top
performing features. The result of including past features
is that in situations where attack campaigns are launched
against previously observed CMSs, the features useful
for identifying such sites do not need to be learned again.
5.2 Classiﬁcation performance
We ran the system with three different conﬁgurations to
understand and evaluate the impact that different con-
ﬁgurations had on overall performance. We send input
to our ensemble of classiﬁers as “blocks,” i.e., a set of
websites to be used as examples. The ﬁrst conﬁguration
generated content features from the very ﬁrst block of
the input stream but did not recompute them after that.
The second conﬁguration recomputed features from ev-
ery block in the input stream but did not use past features
which did not currently have a top statistic value. The
third conﬁguration used dynamic features in addition to
all features that had been used in the past.
For all conﬁgurations, we used a block size of 10,000
examples for retraining the ensemble of C4.5 classiﬁers.
636  23rd USENIX Security Symposium 
USENIX Association
12
We also used a window size of 10,000 samples when
computing the statistic value of features, and we relied
on features with the top 100 statistic values. We gener-
ated ROC curves by oversampling the minority class by
100% and 200% and undersampling the majority class
by 100%, 200%, 300%, and 500%. We ran each combi-
nation of over- and undersampling as its own experiment,
resulting in a total of 10 experiments for each conﬁgura-
tion. The true positive rate and false positive rate2 for
each experiment is taken as the average of the true pos-
itive and false positive rates for each block, that is, each
block in the input stream to the system is tested on before
being trained on, and the rates are taken as the average
over the tested blocks.
 1
 0.8
 0.6
 0.4
 0.2
s
e
v
i
t
i
s
o
P
e
u
r
T
 0
 0
Best operating point
All Dynamic Features
Top Dynamic Features Only
No Dynamic Features
 0.2
 0.4
 0.6
 0.8
 1
False Positives
Figure 13: ROC plot for three different strategies of
dynamic features. The classiﬁer was run using three
different conﬁgurations for dynamic features. The ﬁrst
conﬁguration corresponds to classiﬁers trained on both
current and past top features; the second corresponds
to classiﬁers trained using only current top features; the
third corresponds to classiﬁers trained using the top fea-
tures from the ﬁrst 5,000 samples.
Figure 13 shows the ROC curves generated for the
three conﬁgurations described. The points resulting from
the experiments have been linearly connected to form the
curves. One can see that the conﬁguration which used
past features performed the best, followed by the con-
ﬁguration which used only current top dynamic features
and the conﬁguration which did not use dynamic features
at all. The best operating point appears to achieve a true
positive rate of 66% and a false positive rate of 17%.
The conﬁguration which did not use dynamic features
ended up selecting a feature set which was heavily biased
by the contents of ﬁrst block in the input data stream.
While the features selected were useful on learning the
ﬁrst block, they did not generalize well to future exam-
ples since the distribution of pages that were observed
had changed. This is a problem faced by all such sys-
tems in this setting that are deployed using a static set
2The false negative rate and true negative rates are simple comple-
ments of the respective positive rates.
of features, unless the features set is fully expressive of
the page content, i.e., all changes in the page content are
able to be uniquely identiﬁed by a corresponding change
in the feature values, then the features will eventually be-
come less useful in classiﬁcation as the distribution of
pages changes.
The conﬁguration which only used the current top dy-
namic features also performed relatively poorly. To un-
derstand why this is the case, we can see that in Fig-
ures 11 and 12 some features have a statistic value which
oscillates to reﬂect the change in usefulness of the fea-
ture due to the time varying input distribution. One can
also see that when a feature becomes useful, the corre-
sponding increase in the statistic value lags behind since
a few instances of the feature need to be observed be-
fore the statistic can obtain a high value again. During