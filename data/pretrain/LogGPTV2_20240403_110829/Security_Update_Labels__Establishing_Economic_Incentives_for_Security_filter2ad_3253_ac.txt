for sec. updates
Levels
e100, e120, e140, e160
HD, Full-HD
110°, 130°, 150°
25fps, 30fps, 50fps
yes, no
none, until 12/2020 (2 years),
until 12/2024 (6 years)
none, within 10 days, within 30
days
e100, e120, e140, e160
1 year, 2 years, 3 years
±0.2°C, ±0.3°C, ±0.5°C
yes, no
yes, no
none, until 12/2020 (2 years),
until 12/2024 (6 years)
none, within 10 days, within 30
days
to get an overall score for each product attribute. The higher
the overall score, the more important is the attribute.
Data Collection: We recruited 30 crowdworkers for this
preliminary survey who were each paid e0.75 (reward equiv-
alent for 5 minutes). In the screening of the collected data,
we found that one participant clicked the middle option for
almost all attributes and was six times faster than the average
participant. This person was excluded from the evaluation [82].
Results: The ﬁnal set of participants consisted of 29 respon-
dents (18 female, 11 male) in the age between 20 and 63 years
(µ = 35.3, σ = 11.5). The results are reported in Appendix,
Table VIII. In summary, the product attributes that are per-
ceived as most important for smart home cameras were price,
resolution, ﬁeld of vision, frame rate, and zoom function. For
smart weather stations, the most important product attributes
were price, battery lifetime, precision, rain and wind sensor,
and expandability for multiple rooms. In Appendix, Table VIII,
we listed ‘solar panel for energy generation’ as second most
important product attribute for smart weather stations. How-
ever, in line with prior research [83], we refrained from this
attribute because it correlates with ‘battery lifetime’, and the
correlated attributes bear the risk of threatening the study’s
validity. We decided to include ‘battery lifetime’ as nearly
all smart weather stations use batteries, while solar panels are
only a rare feature in this product category. In addition to these
attributes, we added the availability and provisioning time of
security updates introduced in Section III-B. The ﬁnal list of
attributes is shown in Table I.
Attribute Levels: For each attribute, a discrete number of
levels had to be speciﬁed. The number of attribute levels
should be as low as possible and does not need to cover
the full feature range of the attribute. As recommended [84],
we deﬁned two to four levels to keep the complexity of the
conjoint analysis low. Similarly to the speciﬁcation of the
attributes, there is no golden standard in deﬁning the attribute
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:03:38 UTC from IEEE Xplore.  Restrictions apply. 
435
levels. We decided to specify the levels with reasonable values
that we acquired through the examination of the most popular
products in both product categories available on Amazon as of
October 2018. Thereby, we took care to avoid the speciﬁcation
of extreme values that are considered outliers.
We considered price levels in the realistic price ranges on
the market. We found smart home cameras by 13 manufac-
turers with prices between e50 and e179 (around e110 on
average), and smart weather stations by 10 manufacturers in
the range between e54 and e176 (around e129 on average).
To enhance the comparability, we deﬁned the same price levels
for both product categories. Furthermore, we decided not to
use prices below e100 as the threshold between a two-ﬁgure
price and a three-ﬁgure price might bias the importance of the
price attribute considerably towards two-ﬁgure prices. Also,
we consistently used 0-ending prices [85], [86] and ﬁnally
chose four price levels with an equal spacing between e100
and e160. For all further product attributes, we chose two to
three levels that reﬂect the attribute span of products on the
market. All attribute levels are summarized in Table I.
Focus Group: As the attributes of the security update label
are unknown to consumers, we run a focus group to gain an
intuitive description of the security update label’s attributes
and their levels. The focus group consisted of 8 participants (5
females and 3 males) in the age between 19 and 54 years (µ =
33.5, σ = 12.9) without professional cybersecurity background.
Two participants had a professional IT background. Thus, we
paid attention that these two participants did not dominate
the discussion. We rewarded each participant with e10 for a
one hour session. We started the focus group by establishing
the participants’ prior experience with IoT consumer products,
their awareness of security problems in these products, as well
as their experience with security updates in general. Then, we
introduced the idea of the security update labels and asked
the participants to write down how they would explain the
attributes to family and friends. Each participant presented
their explanations and we discussed them with the group.
time of security updates’ (German: ‘Bereitstellungszeit von
Sicherheitsupdates’).
To exclude confusing proﬁles that occur
from certain
combinations of availability and provisioning time, e.g., the
manufacturer does not provide security updates but offers a
provisioning time of 30 days, we only allowed for meaningful
combinations of both attributes.
VI. CONJOINT ANALYSIS
We decided on a choice-based conjoint (CBC) analysis as
this variant is the de-facto conjoint data collection standard in
marketing research [87] (cf. Section II-C). Although CBC’s
data collection is considered less efﬁcient than other conjoint
data collection methods, it provides a better predictor of real-
world in-market behavior [88]. Furthermore, it allows a “no
choice”-option that also contributes valuable information, i.e.,
that all options are unattractive.
A. Method
We used Lighthouse Studio by Sawtooth Software [87] for
survey setup and data analysis. Lighthouse Studio is a well-
established and validated tool for conjoint analysis [89], [90].
To avoid fatigue, each respondent evaluated only one of
the two product categories, whereas the product category
was assigned randomly to the respondents. Upon starting
the survey, general information about the context, privacy of
collected data, and the scope of the survey were presented. The
respondents expected a survey about a smart home product.
Then, the particular product category was introduced with a
short explanation about its features and exemplary product
pictures. We asked if the respondent is familiar with this
product category and if she owns such a product.
In line with previous research [91], [92], we explained
all attributes shown in Table I (except price) with a short
description, to raise their comprehension and prevent misun-
derstandings. To validate the comprehension, the respondent
answered a quiz that included a question for each attribute. For
example, for the availability attribute, we asked “What does
the availability of security updates specify?” with possible
answers: (a) “for how long the manufacturer guarantees to
provide security updates”, (b) “for how long the device is
allowed to be used”, (c) “for how long the device guarantees
to be protected against hacker attacks”. While the ﬁrst answer
was correct in this example, the order of possible answers
was randomly permuted in the questionnaire. If the respondent
chose a wrong answer, the correct answer was explained again.
Also, the quiz did not follow the order of how the attributes
were presented before to rule out learning effects.
After the respondent became familiar with the attributes,
the choice tasks for the conjoint analysis were explained: The
respondent is presented with four product proﬁles and has
to decide for the most attractive option. All product proﬁles
are described by an attribute level for each product attribute
(including the security update label attributes) in plain text.
In addition to the set of product alternatives, there is always
a “no choice”-option that can be chosen in case none of the
Based on the focus group discussion, we presented the avail-
ability period in the conjoint analysis survey as ‘availability of
security updates’ (German: ‘Verf¨ugbarkeit von Sicherheitsup-
dates’) with a ﬁxed end date, e.g., ‘until 12/2020’, and with the
relative period of time until this date (e.g. ‘2 years’) to reduce
the cognitive effort for the respondents. Although the relative
period is not part of the proposed label, the reduction of
cognitive load was important as the respondents will compare
availability attributes in 10 choice tasks. The levels of the
availability attribute were chosen to reﬂect realistic conditions:
There is always a level of non-availability (i.e., manufacturer
does not guarantee security updates), a level similar to the
usual warranty period of this class of products (i.e., 2 years),
and a third level that exceeds the usual warranty period and is
more oriented on the realistic lifetime of the product (i.e., 6
years). The provisioning time attribute was deﬁned to reﬂect
non-availability, a rather fast (and ideal) period of 10 days, and
a slower (and more realistic) period of 30 days. We denoted
the provisioning time attribute in the survey as ‘provision
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:03:38 UTC from IEEE Xplore.  Restrictions apply. 
436
four proﬁles is desirable. An exemplary choice task is depicted
in Appendix, Figure 2. Then, each respondent performed ten
choice tasks including eight randomly-generated tasks, which
were individual for each respondent, and two ﬁxed holdout
tasks, which were identical for all. The ﬁxed holdout tasks
were later used to validate the attribute preference model. This
model is based on the eight randomly generated tasks and
should be similar to the model based on the holdout tasks in
order to ensure internal consistency of the conjoint analysis
(Section VI-F). The respondents were not aware whether a
choice task is randomly generated or ﬁxed. Prior research [93]
concluded that the order of the attributes affects the choice
behavior. Therefore, we randomly permuted the order of all 7
attributes for each respondent but kept the same order for a
particular respondent as it might be confusing otherwise.
After the respondents performed the choice tasks, we mea-
sured the perceived security risk for the particular product
category by facilitating the perceived security risk scale from
Section V-A. Finally, we used psychometric scales to measure
the respondents’ privacy concerns with the Internet [59] as
well as their security behavior intentions [60].
Next, we included control questions to assess whether the
respondents had difﬁculties in understanding the survey, had
been distracted, and if they took the choice tasks seriously.
These questions were used to identify unmotivated respon-
dents that we later excluded from the analysis. Finally, we
collected demographic data: gender, year of birth, vocational
qualiﬁcation, professional IT background, and net income.
This data was used to determine the representativeness of
the sample as well as for the segmentation. To ensure that
the respondents stayed focused, we included a number of
motivational statements in the questionnaire.
B. Pilot Study
The questionnaire was developed in multiple iterative
rounds. After completing the ﬁnal draft, we collected feed-
back from seven experts from academic and market research
institutes. We asked whether they understood the attributes
and tasks, and if anything could be misleading. Using their
feedback, we re-worded some instructions. Finally, we tested
the questionnaire with 60 crowdworkers to check that the ques-
tionnaire is working as expected, and to calculate the average
task completion time needed to determine the compensation.
C. Sample Size
i.e., number of
The sample size,
respondents for our
questionnaire, was chosen as a trade-off between increasing
costs and decreasing sampling errors. Sampling error arises if
the samples of respondents do not represent the population.
Practical guidelines [94] on CBC analyses recommend at
least 300 respondents for studies without segmentation. If a
segmentation analysis is desired, as is the case with our study,
then a minimum of 200 respondents per subgroup is advised.
Since we aimed for a comparison of up to three subgroups,
which is a usual conﬁguration in a segmentation analysis, we
TABLE II: Demographic data of conjoint analysis sample
compared with the German population.
All
Gender
Age (in years)
Vocational
qualiﬁcation
Monthly net
income (in e)
female
male
3rd option
18-24
25-29
30-49
50-64
none
vocational
academic
none
less than 900
900 to 1,500
1,500 to 2,600
more than 2,600
yes
no
Sample
1,466
640
805
10
339
288
432
404
156
582
728
52
288
296
377
210
249
1166
44.0%
55.3%
0.7%
23.1%
19.6%
29.5%
27.5%
10.6%
39.7%
49.7%
4.3%
23.5%
24.2%
30.8%
17.2%
17.6%
82.4%
Population
49.8%a
50.2%a
12.9%a
9.6%a
18.8%a
58.7%a
22.8%a
60.2%a
17.0%a
17.9%b
24.3%b
22.7%b
22.4%b
10.2%b
Professional
IT background
aCensus 2011 (age 18-65) [95],
Missing answers are ignored for percent proportioning.
bMikrozensus 2014 (all ages) [96]
decided to recruit around 800 respondents for each product
category, and thus, 1,600 respondents in total.
In the pilot study, the average time to answer the question-
naire was 8 minutes. Thus, we paid crowdworkers e1.20. We
ensured that respondents of the prestudies could not participate
in the main survey.
D. Sample Characteristics
We collected the data within a week in mid-December
2018. After a screening, we excluded 154 (9.5%) of the
1,620 collected data sets. We excluded 70 data sets due to
low task completion times (within less than half of the pilot
study’s average time), 48 data sets due to indications in
the control questions, 19 data sets due to suspected multi-
participation (same IP address and user agent), 16 data sets
that answered more than two quiz questions wrong, and 3
data sets of respondents under 18 years. The ﬁnal sample
included 1466 data sets (640 female, 805 male) in the age
between 18 and 65 years (µ = 33.8, σ = 11.2). Details of the
demographic data are presented in Table II. In comparison to
the German population, the sample is biased towards males
and high-educated persons. Furthermore, people in the age of
50 and above are underrepresented in this sample. However,
the sample aligns to the target group of consumers interested
in IoT consumer products, which is likewise biased towards
males, age group 25-34, and higher incomes [97].
E. Results
In total, 731 respondents evaluated the product category
‘smart home camera’ and 735 respondents assessed the prod-
uct category ‘smart weather station’. The high difference in
the security risk perception between both product categories
was conﬁrmed: While the smart weather station achieved an
average perceived security risk score of 3.65, the smart home
camera achieved an average score of 5.50. This difference
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:03:38 UTC from IEEE Xplore.  Restrictions apply. 
437
TABLE III: Relative importance of product attributes.
Smart Home Camera (n=731)
µ [%] σ [%]
Smart Weather Station (n=735)
Attribute
µ [%] σ [%]
Rank Attribute
1. Availability1
30.57
15.12
2. Price
3. Provisioning time2 13.98
12.05
4. Resolution
10.71
5. Frame rate
8.89
6. Field of view
7. Zoom function
8.68
1Availability of security updates, 2Provisioning time for security updates
20.37
16.64
Price
Rain/wind sensor
16.37