the number of bits flipped)
2.2 Calibration against beam experiment
SFI is a simulation based methodology and therefore a
calibration against a real world experiment such as a
1-4244-2398-9/08/$20.00 ©2008 IEEE
124
DSN 2008: Ramachandran et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:06 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
proton beam experiment is helpful to validate its accuracy.
Table 2 shows the percentage of flips that vanished,
were corrected, caused checkstops outcomes. The close
match between the results validates SFI for POWER6. The
proton beam experiment is described in previous work
[3,15]. The main benefit of SFI over beam experiments is
the controllability of injections and the ability to observe
the
In addition, multiple
concurrent copies of the simulation environment can be
run relatively easily, which is not the case with the beam
experiments.
complete RAS response.
Category
Total flips
Vanished
Corrected
Checkstop
SFI
28815
95.48%
3.62%
0.90%
Proton Beam
1748
95.89%
3.51%
0.60%
Table 2: Error state proportions for SFI and Proton Beam
experiments.
3. Results
3.1 Micro-architectural SER resilience
focused on individual
Since the beam cannot be
components, the SER resilience of the individual micro(cid:173)
architectural components cannot be determined using the
beam experiment. We
injection
experiments on individual micro-architecture components
within the full system AWAN model to study the derating
that arises from each micro-architecture unit. Figure 3
presents these results.
performed
fault
Unit), FPU (Floating Point Unit), LSU (Load Store Unit),
RUT (Recovery Unit) and Core (Pervasive Logic). The
relative outcomes among those recorded during simulation
of bit flips namely; Vanished, Recovered, Hangs, and
Checkstop are shown. We notice a high rate of
architecture-level derating of Soft Errors. However, the
outcome of the unmasked faults is dependent on the
micro-architecture component and is widely varied across
the different units. Bit-flips in a total of 20k latches (out
of "'350k total latches) were studied. Owing to the large
fraction of bits flipped, the results showed a variation of
less than 0.9% as a percentage of the mean for that
category, leading to a high confidence in the statistical
stability of these numbers for bit-flips in a different set of
latches.
Figure 3 shows that a high fraction of the injected bit(cid:173)
flips are masked by the architecture. On an average, 95%
of the injected faults are masked by the architecture.
However, the variation of this derating across the different
units is markedly different. Noticeably, the Recovery Unit
(RUT) has the lowest fraction of injected faults that
vanish. This is because we have only injected the latches
in the RUT. A large portion of the RUT consists of arrays
which are protected. Also, the RUT is sensitive to faults in
its control
logic resulting in only about 92% of the
injected faults vanishing without any effect.
From the above data, we infer that the SER resilience of
the different units are markedly different and are
significantly impacted by the functionality of the different
units. However,
this data cannot be used to directly
interpret the relative vulnerabilities of the different units
as each unit has a different number of latches.
100%
::::I
! 80%
'c
§ 60%
.t:
c
.2
~ 40%
:scoo
20%
0%
IFU
• Vanished
IOU
FXU
o Recoveries
FPU
LSU'
D Hangs
Core
RUT
• Checkstops
Figure 3: SER of different micro-architecture units.
The bit-flips in the latches of different units are
categorized in Figure 3 according to their outcome
recorded. Results are shown for the IFU (Instruction Fetch
Unit), IOU (Instruction Decode Unit), FXU (Fixed Point
Recoveries
Hangs
.IFU
0 IOU
0 FXU
ID FPU
a LSU
Checkstops
I!!I RUT
[] Core
Figure 4: Contribution of each unit to the total percentage
of recoveries, hangs, checkstops categories.
Figure 4 shows the contribution from each unit towards
the total recoveries, hangs and checkstops seen. The data
is calculated from Figure 3 by taking the number of
latches in each unit into account.
1-4244-2398-9/08/$20.00 ©2008 IEEE
125
DSN 2008: Ramachandran et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:06 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
From figure 4, we see that the contribution towards
recoveries is highest from the LSU. The normalized
numbers help us to identify the relative vulnerabilities of
the different units more accurately.
Since the LSU has the highest number of latch bits that
are flipped across all the components, it has the maximum
number of recoveries. Additionally, all units have a non(cid:173)
zero contribution to the recoveries due to the existence of
checking hardware. For example every unit in the IBM
POWER6 core has the ability to detect a fault and invoke
recovery through a retry with the help of the Recovery
Unit (RUT). Faults in the RUT and the Core (pervasive)
logic have the highest
individual contribution to the
checkstops and hangs seen. Checkers that detects hangs
and watchdog timer violations are present in the core
pervasive logic and these result in hangs or unrecoverable
checkstops. If the Recovery Unit sees a fault when it
performs recovery, it results in a core checkstop that can
be recovered through software.
the SFI
This in-depth analysis into the SER characteristics of
the different micro-arch components stems from the
ability of
framework to perform focused
statistically significant bit-flips in a short duration.
It is
hard to have such a high degree of controllability and
observability with real-world beam experiments, making
SFI an invaluable tool in SER estimation.
3.2 Identifying the SER of different latches
A typical processor core contains various types of latches,
including latches that are used in scan-only mode, pipeline
latches, latches that make up the register files, etc. It is
important to understand the SER susceptibility of these
latch types to identify areas where additional
different
Such
design effort
analyses will help to improve the quality of the processors.
is needed to improve reliability.
Figure 5 presents the results reflecting the SER of the
different latch types. The GPTR (General Purpose Test
Register latch) and the MODE latches correspond to
latches that are used in a scan-only mode. The REGFILE
latches correspond to latches in the various register files
while the FUNC latches correspond to the latches in the
various pipeline stages. Depending on the location of
these latches in the architecture, two latches that are of the
same type may have different derating as they perform
functions. Since the previous
different architectural
section examined
functional
differences, in this section, we focus on understanding the
differences of the SER resilience between the different
latch types. Hence we aggregate the data for each latch
type,
functions.
Approximately 10% of the latches in each scan chain were
injected with faults, resulting in significantly low variation
across different runs.
ignoring the
effects of
differences
in their
the
these
100.0%
97.5%
enco
~ 95.0%
:5'
'0 92.5%
CD=.!
eCD
; 90.0%
Q.
87.50/0
85.0%
MODE
GPTR
REGFILE
FUNC
Scanring
• Vanished
o Recovered
o Hang
IICheckstop
Figure 5: SER of different types of latches.
Figure 5 shows that the scan-only latches have a larger
system level impact than the functional latches. MODE
and GPTR latches are scan-only latches while the other
latches are used in normal operation and are hence read(cid:173)
write latches. The results motivate the hardening of scan(cid:173)
only latches in the core. Since scan-only latches are read(cid:173)
only latches,
their data remains persistent through the
execution making them highly intrusive to application
execution. Bit-flips in functional
latches, on the other
hand, may be over-written as these are read-write latches.
Hence, a flip in a read-write latch (REGFILE and FUNC
in Figure 5) is more likely to vanish (about 95% for both
cases).
SFI thus helps identify such vulnerabilities in latches
early in the design process thus providing opportunities
for improved design choices. This is due to the high
controllability and observability of the methodology, in
addition to performing statistically significant bit-flips to
draw the right conclusions.
3.3 Effectiveness of hardware checkers
In order to deal with the heightened SER concerns,
modem processors also deploy several forms of hardware
checkers that detect the presence of a bit-flip. For example
the IBM POWER6 has several such hardware checkers
that check for abnormal execution of the hardware.
Owing to the controllability and observability of the
design and state latches using SFI, we were able to
evaluate the effect of these checkers on the SER of the
processor by disabling and enabling checkers in various
parts of the core through masking of checkers. Table 3
presents these results.
The flexibility of SFI helps identify the effectiveness of
these checkers in the core. Table 3 shows the effect of
adding low-level hardware checkers to the core in order to
detect abnormal execution caused due to SER. Raw
represents the SER in the absence of any checkers and
1-4244-2398-9/08/$20.00 ©2008 IEEE
126
DSN 2008: Ramachandran et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:06 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Check represents the SER in the presence of these low(cid:173)
level hardware checkers.
Type
Raw
Vanish
98.8%
Rec
0%
Hangs
1.2%
Chk
0%
1.5%
1.5%
Check
95.9%
1.1%
Table 3: Understanding the effect of checkers
In the presence of hardware checkers, we see an
increase in the number of recovery and checkstop events
seen by the processor. The number of recoveries increases,
indicating that the checkers are very effective at catching
the effect of the faults and correcting for them. The
reduction in the number of vanished bit flips is due to the
fact that some of the errors in the Raw mode were not
being caught by the processor. These are caught by the
checkers and become checkstops and hangs. The checkers
are therefore very effective at improving the quality of the
design.
to optimally allocate
resources to provide soft error protection.
5. References
and apportion any additional
[1]
S. Mitra, N. Seifert, M. Zhang, Q. Shi and K. S. Kim, "Robust
System Design with Built-In Soft Error Resilience,"
IEEE
Transactions on Computers, vol. 38, no. 2, Feb. 2005.
[3]
[2] M. Mack, W. Sauer, S. Swaney, and B. Mealey, "IBM POWER6
reliability," IBM Journal of Research and Development. Vol. 51,
No. 6,2007.
J. Kellington, R. McBeth, P. Sanda, and R. Kalla, "IBM POWER6
Processor Soft Error Tolerance Analysis Using Proton Irradiation,"
in Workshop on Silicon Effects ofLogic - System Effects (SELSE),
2007.
[4] C.
[5]
SER
"Neutron
Constantinescu,
Characterization
of
Microprocessors," in Proceedings of International Conference on
Dependable Systems and Networks (DSN), 2005.
S. Cakici, P. Sanda, K. Wright, 1. Day, S. Swaney, and, E. Cannon,
"Proton Irradiation Studies Single Event Upsets in IBM POWER5
System," in Workshop on Silicon Effects ofLogic - System Effects
(SELSE),2006.
[6] G. Kanawati, N. Kanawati, andJ. Abraham, "FERRARI: A Flexible
IEEE
Injection System,"
Software-Based Fault
Transactions on Computer, vol. 44, 1995.
and Error
4. Conclusions
[7] T. Tsai and R. Iyer, "FTAPE: A Fault Injection Tool to Measure
Fault Tolerance," presented at Computing in Aerospace, 1995.
The ever-increasing SER on modem processors is a major
concern for processor designers today. In order to first
understand this problem, several techniques that inject bit(cid:173)
flips in today's processors to study SER effects have been
proposed. However, one main draw-back of the current
techniques is the slow speed of software simulation, which
results in erroneous conclusions from using workloads
that are not representative of the real world, or from
studying too few bit-flips.
This paper presents Statistical Fault Injection (SFI), a
tool that uses accelerated hardware based emulation to
deal with shortcomings of traditional fault simulation
methods. Through the use of full-system models for SER
studies, SFI allows characterization of the effects of soft
errors on different parts of the modem processor. This
paper presents our ability to perform an in-depth study of
the RAS characteristics in processors like the IBM
POWER6
through
extensive fault injections in the core of the processor and
provide feedback to the designers.
development
processor
during
As designers devise new techniques to circumvent the
problems caused by SER, techniques such as SFI that aid
in accurate SER characterization become more important.
The flexibility offered by such tools allows designers to
perform in-depth studies to understand the derating of
these errors by various layers to logic and use this derating
to their advantage. Current and future work involves fault
injections in the periphery of the core, such as the I/O
subsystem, memory subsystem and so on. Future core and
system designs will need to be power efficient and
therefore require careful analysis of soft error sensitivities
in Proceedings of
[8] N. Wang, 1. Quek, T. Rafacz and S. Patel, "Characterizing the
Effects of Transient Faults on a High-Performance Processor
Pipeline,"
on
Dependable Systems and Networks (DSN), 2004.
J. M Ludden, et. a1. "Functional Verification of the POWER4
microprocessor and POWER4 multiprocessor systems," in IBM
Journal on Research and Development, Vol 46, No. 1 Jan, 2002.
International Conference
[9]
[10] M. Wazlowski et al "Verification Strategy for the Blue Gene/L
chip," IBM Journal on Research and Development", Vol 49, No.
2/3,2005
[11] A. Biswas, R. Cheveresan, J. Emer, S. Mukherjee, and R. Rangan,
"Computing architectural vulnerability factors for address-based
structures," in Proceedings ofInternational Sympsium on Computer
Architecture (lSCA), 2005.
[12] X. Li, S. Adve, P. Bose, and J. Rivers, "SoftArch: An Architecture(cid:173)
Level Tool
in
Proceedings of International Conference on Dependable Systems
and Networks (DSN), 2005.
for Modeling and Analyzing Soft Errors,"
[13] H, Nguyen, Y. Yagil, N. Seifert, and M. Reitsma, "Chip-Level Soft
Error Estimation Model," IEEE Transactions on Device and
Materials Reliability, Vol.5, No.3, 2005.
[14] S. Mukherjee, C. Weaver, 1. Emer, S. Reinhardt, and T. Austin, "A
Systematic Methodology
the Architectural
Vulnerability Factors for a High-Performance Microprocessor," in
Proceedings of International Sympsoium on Microarchitecture
(MICRO),2003.
to Compute
[15] P. Sanda et aI., "IBM POWER6 Processor Soft Error Resilience,"
IBM Journal of Research and Development, to appear, 2008.
[16] P. Kudva, J. Kellington, P. Sanda, R. McBeth, 1. Schumann, and R.
Kalla, "Fault Injection Verification of IBM POWER6 Soft Error
Resilience," in Workshop on Architectural Support for Gigascale
Integration (ASGI), 2007.
[17] T. Tsai et aI., "Stress-Based and Path-Based Fault Injection," IEEE
Transactions on Computers, Vol 48, No 11, 1999.
[18] K. Reick, P. Sanda, S. Swaney, 1. Kellington, M. Mack, M. Floyd,
and D. Henderson, "Fault-Tolerant Design of the IBM POWER6
Microprocessor," IEEE Micro, to appear, 2008.
1-4244-2398-9/08/$20.00 ©2008 IEEE
127
DSN 2008: Ramachandran et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:06 UTC from IEEE Xplore.  Restrictions apply.