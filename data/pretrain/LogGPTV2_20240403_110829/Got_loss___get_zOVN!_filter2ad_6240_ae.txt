CPU does. As TX is network-limited, a fast network is ben-
eﬁcial on the TX side – but hurts performance on the RX
side – whereas a fast CPU is beneﬁcial on the RX side –
processor-limited – while it hurts the TX side. In conclu-
sion, a diﬀerent CPU/network speed ratio is not a viable
substitute for a correct implementation of ﬂow control in
the virtual network.
7. RELATED WORK
In recent years, the TCP incast and ﬂow completion time
performance of Partition-Aggregate applications has been
extensively analyzed. For example, [15, 39] suggest a 10-
1000× retransmission timeout reduction. Other proposals
achieve sizable ﬂow completion time reductions for typical
datacenter workloads using new single-path [8, 9, 41, 38] or
multi-path [42, 24, 36, 7] transports. These are coupled with
deadline-aware or agnostic schedulers and per-ﬂow queuing.
Related to our work and to [22, 18], DeTail [42] identiﬁes
packet loss in physical networks as one of the three main is-
sues. The authors enable ﬂow control, i.e., PFC, and intro-
 0 10 20 30 40 50 60 70 80 90 100NewRenoVegasCubicMean Completion Time [ms]LLLZZLZZ 0 10 20 30 40 50 60 70 80 90 100NewRenoVegasCubicMean Completion Time [ms]LLLZZLZZ 0 100 200 300 400 500 600 700 800 900NewRenoVegasCubicMean Completion Time [ms]LLLZZLZZ432duce a new multi-path congestion management scheme tar-
geted against ﬂash hotspots typical of Partition-Aggregate
workloads. They also employ explicit congestion notiﬁcation
(ECN) against persistent congestion. DeTail uses a modiﬁed
version of NewReno to reduce ﬂow completion time by 50%
at the 99.9-percentile, but does not address virtual overlays.
pFabric [10] re-evaluates the end-to-end argument. It in-
troduces a “deconstructed” light transport stack resident
in the end node and re-designed speciﬁcally for latency-
sensitive datacenter applications. Furthermore, a greedy
scheduler implements a deadline-aware global scheduling and
a simpliﬁed retransmission scheme recovers losses. By re-
placing both the TCP stack and the standard datacenter
fabric, this scheme achieves near-ideal performance for short
ﬂows. Open issues are the scalability to datacenter-scale
port counts, costs of replacing commodity fabrics and TCP
version, fairness, and compatibility with the lossless con-
verged datacenter applications.
DCTCP [8] uses a modiﬁed ECN feedback loop with a
multibit feedback estimator ﬁltering the incoming ECN str-
eam. This compensates the stiﬀ active queue management
in the congestion point detector with a smooth congestion
window reduction function reminiscent of QCN’s rate de-
crease. DCTCP reduces the ﬂow completion time by 29%,
however, as a deadline-agnostic TCP it misses about 7% of
the deadlines. D3 [41] is a deadline-aware ﬁrst-come ﬁrst-
reserved non-TCP transport. Its performance comes at the
cost of priority inversions for about 33% of the requests [38]
and a new protocol stack. PDQ [24] introduces a multi-path
preemptive scheduling layer for meeting ﬂow deadlines using
a FIFO taildrop similar to D3. By allocating resources to
the most critical ﬂows ﬁrst, PDQ improves on D3, RCP and
TCP by circa 30%. As it is not TCP, its fairness remains to
be studied. D2TCP [38] improves on D3 and DCTCP, with
which it shares common features in the ECN ﬁlter, by penal-
izing the window size with a gamma factor. Thus, it provides
iterative feedback to near-deadline ﬂows and prevents con-
gestive collapse. This deadline-aware TCP-friendly proposal
yields 75% and 50% fewer deadline misses than DCTCP and
D3, respectively. Hedera and MP-TPC [7, 23, 31] propose
multi-path TCP versions optimized for load balancing and
persistent congestion. However, short ﬂows with fewer than
10 packets or FCT-sensitive applications do not beneﬁt, de-
spite the complexity of introducing new sub-sequence num-
bers in the multi-path TCP loop.
8. CONCLUDING REMARKS
Fabric-level per-lane ﬂow control to prevent packet loss
due to contention and transient congestion has long been
the signature feature of high-end networks and HPC inter-
connects. The recent introduction of CEE priority ﬂow con-
trol has now made it a commodity. In spite of the advances
at layer-2, we have shown that present virtual overlays lag
behind. Congestion, whether inherent in the traﬃc pattern
or as an artifact of transient CPU overloads, is still han-
dled here by dropping packets, thus breaking convergence
requirements, degrading performance, and wasting CPU and
network resources.
We provided ﬁrst evidence that, for latency-sensitive vir-
tualized datacenter applications, packet loss is a costly sin-
gularity in terms of performance. To remedy this situation,
we have identiﬁed the origins of packet drops across the en-
tire virtualized communication stack, and then designed and
implemented a fully lossless virtual network prototype.
Based on the experimental results using our prototype
implementations and also larger-scale simulations, we have
demonstrated average FCT improvements of one order of
magnitude. Additional takeaways are that (i) packet loss
in virtualized datacenters is even costlier than previously
studied in physical networking; (ii) FCT performance of
Partition-Aggregate workloads is greatly improved by loss-
lessness in the virtualized network; (iii) commodity CEE
fabrics and standard TCP stacks still have untapped per-
formance beneﬁts. Furthermore, zOVN can be orthogonally
composed with other schemes for functional or performance
enhancements on layers 2 to 5.
Acknowledgements
We are deeply indebted to the anonymous reviewers and our
shepherd, Amin Vahdat for their helpful feedback; Rami
Cohen and Katherine Barabash from IBM2 HRL for the
DOVE-related help; Andreea Anghel for the ﬁrst OVN loss
experiments; Anees Shaikh, Renato Recio, Vijoy Pandey,
Vinit Jain, Keshav Kamble, Martin Schmatz and Casimer
DeCusatis for their kind support, comments and feedback.
9. REFERENCES
[1] Iperf. URL: http://iperf.sourceforge.net.
[2] Linux Bridge. URL: http://www.linuxfoundation.
org/collaborate/workgroups/networking/bridge.
[3] Open vSwitch. URL: http://openvswitch.org.
[4] QEMU-KVM. URL: http://www.linux-kvm.org.
[5] Fabric convergence with lossless Ethernet and Fibre
Channel over Ethernet (FCoE), 2008. URL:
http://www.bladenetwork.net/userfiles/file/
PDFs/WP_Fabric_Convergence.pdf.
[6] P802.1Qbb/D2.3 - Virtual Bridged Local Area
Networks - Amendment: Priority-based Flow Control,
2011. URL:
http://www.ieee802.org/1/pages/802.1bb.html.
[7] M. Al-Fares, S. Radhakrishnan, B. Raghavan,
N. Huang, and A. Vahdat. Hedera: Dynamic Flow
Scheduling for Data Center Networks. In Proc. NSDI
2010, San Jose, CA, April 2010.
[8] M. Alizadeh, A. Greenberg, D. A. Maltz, et al.
DCTCP: Eﬃcient Packet Transport for the
Commoditized Data Center. In Proc. ACM
SIGCOMM 2010, New Delhi, India, August 2010.
[9] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar,
A. Vahdat, and M. Yasuda. Less is More: Trading a
little Bandwidth for Ultra-Low Latency in the Data
Center. In Proc. NSDI 2012, San Jose, CA, April 2012.
[10] M. Alizadeh, S. Yang, S. Katti, N. McKeown, et al.
Deconstructing Datacenter Packet Transport. In Proc.
HotNets 2012, Redmond, WA.
2Intel and Intel Xeon are registered trademarks of Intel Cor-
poration or its subsidiaries in the United States and other
countries. IBM is a trademark of International Business Ma-
chines Corporation, registered in many jurisdictions world-
wide. Linux is a registered trademark of Linus Torvalds in
the United States, other countries, or both. Other prod-
uct or service names may be trademarks or service marks of
IBM or other companies.
433[11] K. Barabash, R. Cohen, D. Hadas, V. Jain, et al. A
[27] N. McKeown, T. Anderson, H. Balakrishnan,
Case for Overlays in DCN Virtualization. In Proc.
DCCAVES’11, San Francisco, CA.
[12] P. Baran. On Distributed Communications Networks.
IEEE Transactions on Communications, 12(1):1–9,
March 1964.
[13] R. Birke, D. Crisan, K. Barabash, A. Levin,
C. DeCusatis, C. Minkenberg, and M. Gusat.
Partition/Aggregate in Commodity 10G Ethernet
Software-Deﬁned Networking. In Proc. HPSR 2012,
Belgrade, Serbia, June 2012.
[14] M. S. Blumenthal and D. D. Clark. Rethinking the
Design of the Internet: The End-to-End Arguments
vs. the Brave New World. ACM Transactions on
Internet Technology, 1(1):70–109, August 2001.
[15] Y. Chen, R. Griﬃth, J. Liu, R. H. Katz, and A. D.
Joseph. Understanding TCP Incast Throughput
Collapse in Datacenter Networks. In Proc. WREN
2009, Barcelona, Spain, August 2009.
[16] D. Cohen, T. Talpey, A. Kanevsky, et al. Remote
Direct Memory Access over the Converged Enhanced
Ethernet Fabric: Evaluating the Options. In Proc.
HOTI 2009, New York, NY, August 2009.
G. Parulkar, et al. OpenFlow: Enabling Innovation in
Campus Networks. ACM SIGCOMM Computer
Communication Review, 38(2):69–74, April 2008.
[28] J. Mudigonda, P. Yalagandula, J. C. Mogul, et al.
NetLord: A Scalable Multi-Tenant Network
Architecture for Virtualized Datacenters. In Proc.
ACM SIGCOMM 2011, Toronto, Canada.
[29] B. Pfaﬀ, B. Lantz, B. Heller, C. Barker, et al.
OpenFlow Switch Speciﬁcation Version 1.1.0.
Speciﬁcation, Stanford University, February 2011.
URL: http://www.openflow.org/documents/
openflow-spec-v1.1.0.pdf.
[30] G. Pﬁster and V. Norton. Hot Spot Contention and
Combining in Multistage Interconnection Networks.
IEEE Transactions on Computers, C-34(10):943–948,
October 1985.
[31] C. Raiciu, S. Barre, and C. Pluntke. Improving
Datacenter Performance and Robustness with
Multipath TCP. In Proc. ACM SIGCOMM 2011,
Toronto, Canada, August 2011.
[32] L. Rizzo. netmap: A Novel Framework for Fast Packet
I/O. In Proc. USENIX ATC 2012, Boston, MA.
[17] R. Cohen, K. Barabash, B. Rochwerger, L. Schour,
[33] L. Rizzo and G. Lettieri. VALE, a Switched Ethernet
D. Crisan, R. Birke, C. Minkenberg, M. Gusat, et al.
An Intent-based Approach for Network Virtualization.
In Proc. IFIP/IEEE IM 2013, Ghent, Belgium.
[18] D. Crisan, A. S. Anghel, R. Birke, C. Minkenberg, and
M. Gusat. Short and Fat: TCP Performance in CEE
Datacenter Networks. In Proc. HOTI 2011, Santa
Clara, CA, August 2011.
[19] W. Dally and B. Towles. Principles and Practices of
Interconnection Networks, Chapter 13. Morgan
Kaufmann Publishers Inc., San Francisco, CA, 2003.
[20] N. Dukkipati and N. McKeown. Why
Flow-Completion Time is the Right Metric for
Congestion Control. ACM SIGCOMM CCR,
36(1):59–62, January 2006.
[21] H. Grover, D. Rao, D. Farinacci, and V. Moreno.
Overlay Transport Virtualization. Internet draft,
IETF, July 2011.
[22] M. Gusat, D. Crisan, C. Minkenberg, and
C. DeCusatis. R3C2: Reactive Route and Rate
Control for CEE. In Proc. HOTI 2010, Mountain
View, CA, August 2010.
[23] H. Han, S. Shakkottai, C. V. Hollot, R. Srikant, and
D. Towsley. Multi-Path TCP: A Joint Congestion
Control and Routing Scheme to Exploit Path
Diversity in the Internet. IEEE/ACM Transactions on
Networking, 14(6):1260–1271, December 2006.
[24] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing
Flows Quickly with Preemptive Scheduling. In Proc.
ACM SIGCOMM 2012, Helsinky, Finland.
[25] S. Kandula, D. Katabi, S. Sinha, and A. Berger.
Dynamic Load Balancing Without Packet Reordering.
ACM SIGCOMM Computer Communication Review,
37(2):53–62, April 2007.
[26] M. Mahalingam, D. Dutt, K. Duda, et al. VXLAN: A
Framework for Overlaying Virtualized Layer 2
Networks over Layer 3 Networks. Internet draft,
IETF, August 2011.
for Virtual Machines. In Proc. CoNEXT 2012, Nice,
France, December 2012.
[34] R. Russell. virtio: Towards a De-Facto Standard For
Virtual I/O Devices. ACM SIGOPS Operating System
Review, 42(5):95–103, July 2008.
[35] J. H. Saltzer, D. P. Reed, and D. D. Clark.
End-to-End Arguments in System Design. ACM
Transactions on Computer Systems, 2(4):277–288,
November 1984.
[36] M. Scharf and T. Banniza. MCTCP: A Multipath
Transport Shim Layer. In Proc. IEEE GLOBECOM
2011, Houston, TX, December 2011.
[37] M. Sridharan, K. Duda, I. Ganga, A. Greenberg, et al.
NVGRE: Network Virtualization using Generic
Routing Encapsulation. Internet draft, IETF,
September 2011.
[38] B. Vamanan, J. Hasan, and T. N. Vijaykumar.
Deadline-Aware Datacenter TCP (D2TCP). In Proc.
ACM SIGCOMM 2012, Helsinki, Finland.
[39] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat,
D. G. Andersen, G. R. Ganger, G. A. Gibson, and
B. Mueller. Safe and Eﬀective Fine-grained TCP
Retransmissions for Datacenter Communication. In
Proc. ACM SIGCOMM 2009, Barcelona, Spain.
[40] G. Wang and T. S. E. Ng. The Impact of
Virtualization on Network Performance of Amazon
EC2 Data Center. In Proc. INFOCOM 2010, San
Diego, CA, March 2010.
[41] C. Wilson, H. Ballani, T. Karagiannis, and
A. Rowstron. Better Never than Late: Meeting
Deadlines in Datacenter Networks. In Proc. ACM
SIGCOMM 2011, Toronto, Canada, August 2011.
[42] D. Zats, T. Das, P. Mohan, D. Borthakur, and
R. Katz. DeTail: Reducing the Flow Completion Time
Tail in Datacenter Networks. In Proc. ACM
SIGCOMM 2012, Helsinky, Finland, August 2012.
434