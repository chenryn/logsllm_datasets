We will experiment with attacks based on influence functions,
which we extend to subpopulations (they were originally used for
targeted poisoning). We also propose a Gradient Optimization attack
or GO, in which we relax the quadratic approximation to a linear
approximation, trading off effectiveness for increased efficiency.
This approximation assumes the poisoned model is computed as a
single gradient step from an unpoisoned model, where the gradient
is computed on both the clean training set 𝑋, 𝑌 = {𝑥𝑖, 𝑦𝑖}𝑛
𝑖=1 and
poisoned training set 𝑋𝑝, 𝑌𝑝 = {𝑥𝑝
𝑖 , 𝑦𝑝
𝑖 }𝑚
𝑖=1:
∇𝜃 𝐿(𝑥𝑖, 𝑦𝑖, 𝜃𝑜𝑙𝑑)
∇𝜃 𝐿(𝑥𝑝
𝑖 , 𝑦𝑝
𝑖 , 𝜃𝑜𝑙𝑑).
(4)
𝜃𝑛𝑒𝑤 = 𝜃𝑜𝑙𝑑 − 𝜂
𝑛 + 𝑚
− 𝜂
𝑛 + 𝑚
𝑛
𝑚
𝑖=1
𝑖=1
We hope to induce a modification 𝜃𝑛𝑒𝑤 − 𝜃𝑜𝑙𝑑 that will most
increase the loss on a target dataset 𝑋𝑎, 𝑌𝑎 selected from 𝐷𝑎𝑢𝑥. As
𝜂 → 0, the best such modification is −∇𝜃 𝐿(𝑋𝑎, 𝑌𝑎, 𝜃). Then our
goal is to maximize 𝑂𝑏 𝑗(𝑋𝑝, 𝑌𝑝) = −∇𝜃 𝐿(𝑋𝑎, 𝑌𝑎, 𝜃)𝑇 (𝜃𝑛𝑒𝑤 − 𝜃𝑜𝑙𝑑).
Combining this with Equation (4), our optimization process is:
𝑋𝑝, 𝑌𝑝 = arg max
𝑋𝑝,𝑌𝑝
∇𝜃 𝐿(𝑋𝑎, 𝑌𝑎, 𝜃𝑜𝑙𝑑)∇𝜃 𝐿(𝑋𝑝, 𝑌𝑝, 𝜃𝑜𝑙𝑑).
To solve this, we follow standard procedure by using gradient
descent. We start with a poisoning set generated by label flipping,
and run 50 steps of gradient descent to update 𝑋𝑝, 𝑌𝑝. The gradient
update on ∇𝑋𝑝 for this optimization procedure is
−∇𝜃 𝐿(𝑋𝑎, 𝑌𝑎, 𝜃𝑜𝑙𝑑)𝑇 ∇𝑋𝑝∇𝜃 𝐿(𝑋𝑝, 𝑌𝑝, 𝜃𝑜𝑙𝑑),
Auxiliary Data CLUSTERMATCHFEATUREMATCHSubpopulation SelectionPoisoning Attack GenerationClass tClass cLabel FlippingInitFinalAttack Optimization 1.Influence 2.Gradient OptPoisoned Data Clean Data Training Algorithm Machine Learning Model Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3109Algorithm 3 Generic Subpopulation Attack. In this work, 𝐾𝑎𝑑𝑣
consists only of the dataset 𝐷𝑎𝑢𝑥.
Input: Adversarial knowledge 𝐾𝑎𝑑𝑣, attack size 𝑚
FilterFunctions = MakeFilterFunctions(𝐾𝑎𝑑𝑣)
Algorithm 1 or 2
F = SelectFilterFunction(FilterFunctions, 𝐾𝑎𝑑𝑣)
return GenerateAttack(F , 𝑚, 𝐾𝑎𝑑𝑣)
⊲ e.g.,
⊲ see Section 4.2
which is equivalent to approximating the influence function up-
date with 𝐻𝜃 = 𝐼. Intuitively, this allows us to maintain good
performance without the expensive Hessian operations required
for influence functions. There are many other optimization-based
procedures for poisoning attacks (e.g., [4, 21, 60]), differing in ob-
jective function, threat model, and initialization strategy. It may be
possible to adapt these strategies to subpopulation attacks, as well.
A gradient-based optimization approach assumes the features
can be modified as continuous values. For tabular datasets with cat-
egorical features and feature dependencies, or text datasets, these
continuous modifications are difficult, and even generating adver-
sarial examples requires a significant amount of work. The original
influence attack [30] was tested only on image data. For this rea-
son, we implement optimization-based attacks based on influence
and gradient optimization for image datasets only. We believe that
extending existing optimization attacks to text or tabular data [31]
is an interesting avenue of future work.
4.3 Subpopulation Attack Framework
The full attack operates in three distinct steps. It starts by
identifying subpopulations in the data with FeatureMatch or
ClusterMatch, continues by selecting a target subpopulation, and
ultimately generating the actual poisoned points to inject into the
training set, as shown in Figure 1 and Algorithm 3. In order to gen-
erate the contaminants, the adversary could use a basic, yet generic,
approach such as label flipping, or employ a stronger model-specific
approach like influence functions or gradient optimization. Simi-
larly, different clustering methods could be leveraged to identify
potential victim subpopulations in the first step. Due to the modular-
ity of the framework, each component (e.g., subpopulation selection,
poisoning points generation) may be improved separately, and in
ways that may more closely match specific domains of interest.
5 ATTACK EXPERIMENTS
In this section, we explore the threat of subpopulation attacks
on real datasets. We first explore the effectiveness of the label
flipping attack in the end-to-end training scenario, comparing
FeatureMatch and ClusterMatch for subpopulation selection.
Then, we show that label flipping subpopulation attacks are also
effective in transfer learning scenarios. We run experiments on four
datasets from three modalities, to demonstrate the generality of our
attack. For three of these datasets, we measure the attack’s success
on both small and large models. Large models are fine tuning all
the layers of the neural networks and have much larger capacity
(e.g., 134 million parameters for VGG-16). We also evaluate the two
optimization attacks on the face recognition dataset, investigate the
variance in subpopulations’ target damage, and the transferability
of representations (transferability results appear in the Appendix).
We believe the breadth of our experiments provides compelling
evidence that subpopulation attacks are a useful and practical threat
model for poisoning attacks against ML.
We evaluate our attacks using a general approach. We partition
standard datasets into a training set 𝐷, an auxiliary set 𝐷𝑎𝑢𝑥, and a
test set 𝐷𝑡, all being disjoint. The adversary only ever has access to
𝐷𝑎𝑢𝑥. The adversary uses 𝐷𝑎𝑢𝑥 to generate subpopulations, train-
ing a surrogate model when necessary for ClusterMatch. The
adversary generates the poisoned data 𝐷𝑝, and the model is trained
on 𝐷 ∪ 𝐷𝑝, and the target damage is evaluated only on test points
from 𝐷𝑡 belonging to the target subpopulation.
5.1 Datasets and Models
Here we will provide a brief overview of the various datasets, mod-
els, and hyperparameters used for our experiments.
CIFAR-10. CIFAR-10 [35] is a medium-size image classification
dataset of 32x32x3 images split into 50000 training images and
10000 test images belonging to one of 10 classes. We use CIFAR-
10’s standard split, splitting the train set into 25000 points for 𝐷
and 25000 points for 𝐷𝑎𝑢𝑥. We use two ML classifiers: Conv and
VGG-FT. Conv is a small convolutional neural network, consisting
of a convolutional layer, three blocks consisting of 2 convolutional
layers and an average pooling layer each, and a final convolution
and mean pooling layer, trained with Adam at a learning rate of
0.001. VGG-FT fine tunes all layers of a VGG-16 model pretrained
on ImageNet for 12 epochs with Adam with a learning rate of 0.001.
UTKFace. UTKFace [81] is a facial recognition dataset, annotated
with gender, age, and race (the races included are White, Black,
Asian, Indian, and Other, which contains Latino and Middle Eastern
images). We use it for gender classification, removing children
under age 15 to improve performance, leaving 20054 images. We
then split it into 𝐷, 𝐷𝑎𝑢𝑥, and 𝐷𝑡 with 7000, 7000, and 6054 images,
respectively. For UTKFace, we use two ML models: VGG-LL and
VGG-FT. VGG-LL only trains the last layer of a VGG-16 model [63],
pretrained on the ImageNet dataset [15] (8K parameters), while
VGG-FT trains all layers (134M parameters). For both, we train for
12 epochs with Adam, using a learning rate of 0.001 for VGG-LL
and 0.0001 for VGG-FT. For both, we use an ℓ2 regularization of
0.01 on the classification layer to mitigate overfitting.
IMDB Reviews. The IMDB movie review dataset [44] consists of
50000 reviews of popular movies left by users on the IMDB1 web-
site, together with the reviews’ scores. The dataset is often used
for binary sentiment classification, predicting whether the review
expresses a positive or negative sentiment. Given the rise in popu-
larity of pre-trained models (BERT [16], GPT-2 [54], XLNet [79]) for
natural language modeling, and the high variance of the data used
to train them, they provide a perfect target for subpopulation at-
tacks. For this dataset we split the training set into 12500 points for
𝐷 and 12500 for 𝐷𝑎𝑢𝑥. We use BERT for our experiments, followed
by a single classifier layer. Our small model, BERT-LL, only fine-
tunes BERT’s final transformer block and the last layer (classifier),
while our large model, BERT-FT, fine-tunes all of BERT’s trans-
former blocks together with the classifier. Both models are trained
on vectors of 256 tokens, for 4 epochs, with a learning rate of 10−5
1https://www.imdb.com/
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3110Dataset Worst Clean Acc
UTKFace
VGG-LL
UCI Adult
10
5
1
10
5
1
0.846
0.837
Target Damage
𝛼 = 0.5
0.054
0.094
0.400
0.103
0.143
0.311
𝛼 = 1
0.086
0.140
0.400
0.148
0.21
0.467
𝛼 = 2
0.144
0.192
0.400
0.16
0.195
0.250
2: Clean accuracy and target damage
Table
for
FeatureMatch attacks with label flipping, reported
over the worst 1, 5, and 10 subpopulations.
Task
Worst-5
Worst-10
Input
0.187
0.157
Layer 1
0.290
0.257
Layer 2
0.313
0.270
Layer 3
0.305
0.283
Layer 4
0.327
0.297
Table 3: Results for ClusterMatch with Label Flipping on
CIFAR-10 + Conv using six different layers for clustering.
Both worst-5 and worst-10 results improve monotonically
in the layer number, indicating that the more useful the rep-
resentation is to the model, the easier it is to attack, as well.
and mini batch size of 8. These models use the same architecture
and implementation from the Huggingface Transformers library
[75]. They are both based on a pre-trained bert-base-uncased
instance [16], with 12 transformer blocks (110M parameters), and
one linear layer for classification (1538 parameters).
UCI Adult. UCI Adult [18] consists of 48843 rows, where the goal
is to use demographic information to predict whether a person’s
income is above $50𝐾 a year. We drop the ’education’, ’native-
country’, and ’fnlwgt’ columns due to significant correlation with
other columns, and apply one-hot encoding to categorical columns.
For UCI Adult, we use a feed-forward neural network with one hid-
den layer of 10 ReLU units, trained for a maximum of 3000 iterations
using scikit-learn [51] default settings for all other parameters.
5.2 Label Flipping on End-to-end Training
We demonstrate that subpopulation attacks based on label flipping
are effective for end-to-end training of neural networks, which has
been a difficult setting for poisoning attacks on neural networks.
Backdoor poisoning attacks need a fairly large amount of poisoning
data to be effective [23], while targeted attacks against neural net-
works trained end-to-end need on the order of 50 points to attack a
single point at testing [21, 60]. We investigate our attacks on CIFAR-
10 and UCI Adult. We train Conv on CIFAR-10 and the UCI Adult
model end-to-end. We test FeatureMatch on UCI Adult, with the
combination of education level, race, and gender as annotations.
We also use KMeans for ClusterMatch on both UCI Adult and
CIFAR-10 with 𝑘 = 100 clusters (we discuss briefly results for other
values of 𝑘). For UCI Adult, due to large variance in FeatureMatch
subpopulation sizes, we remove subpopulations with greater than
100 or less than 10 data points. We report FeatureMatch results
in Table 2, and ClusterMatch results in Table 4.
Effectiveness of Attacks. FeatureMatch causes a target dam-
age of 25% for one subpopulation on UCI Adult and an average
of 19.5% over five subpopulations, when attacked with a poison-
ing rate of 𝛼 = 2. The attacked groups all have comparable ages,
genders, and races, impacting the group fairness of the classifier.
There are some results where increasing 𝛼 does not strictly increase
attack performance, due likely to unintuitive effects from optimiz-
ing nonconvex loss functions (all of our results with convex loss
functions have monotone performance increases).
We can only run FeatureMatch on datasets with annotations:
UTKface and UCI Adult. We find ClusterMatch is nearly always
more effective than FeatureMatch. On UCI Adult, with a poison-
ing rate of 𝛼 = 2, one subpopulation reaches 66.7% target damage
(with 48 poisoned points), and five subpopulations reach an average
of 36.7% target damage (with an average of 45 poisoned points). On
CIFAR-10, the attack is very effective, reaching 23.6% target damage
on one subpopulation at only a poisoning rate of 𝛼 = .5 and 53.5%
at 𝛼 = 2. The collateral is also low, on average 1.41% for the top 5
subpopulations by target damage. This is convincing evidence that
ClusterMatch’s ability to leverage the data to construct subpop-
ulations allows it to produce more effective subpopulations.
Optimizing ClusterMatch. We experiment with designing
good ClusterMatch clusters on CIFAR-10, by running KMeans
to produce 100 subpopulations at six different layers of Conv. For
all layers, we project to 10 dimensions using PCA. We find that
ClusterMatch is most effective when using clusters constructed
using the last layer of Conv. Notably, the top 5 clusters using
the last layer representation achieve 32.7% target damage, while
ClusterMatch using the input features only reaches 18.7% target
damage, as shown in Table 3. There is a monotonic increase in the
target damage as the layer number increases, indicating, intuitively,
that the more useful representations to the model are more effective
for subpopulation attacks, as well. We use this insight for all of our
remaining experiments.
Subpopulation size. We also varied the number of clusters in
{50, 100, 200, 400}, and found little difference between these values.
If the number of clusters is too small, however, the impact on overall
accuracy may be noticeable, and if the number of clusters is too
large, the subpopulation impacted will consist of very few points,
making the impact of the attack itself limited (at 400 subpopulations,
for example, some subpopulations do not appear in the test set).
We decided to report all results for 𝑘 = 100 clusters.
5.3 Label Flipping on Transfer Learning
We evaluate the effectiveness of Label Flipping subpopulation at-
tacks on transfer learning, on CIFAR-10, UTKFace, and IMDB. We
run FeatureMatch on VGG-LL on UTKFace, using the combina-
tion of race and bucketed ages (bucketed into [15, 30], [30, 45],
[45, 60], [60,∞)) as the annotations, presenting results in Table 2.
We present results for ClusterMatch with 100 clusters generated
with KMeans, on VGG-LL on UTKFace and BERT-LL in Table 4,
and present results for large models in Table 5.
We find FeatureMatch is effective on UTKFace, reaching a
target damage of 40% for one subpopulation and an average of
19.2% over five subpopulations, both at a poisoning rate of 𝛼 = 2.
However, ClusterMatch reaches a target damage of 55.6% on
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3111Dataset Worst Clean Acc
Target Damage
Subpop
0.846
0.889
0.803