layers for a batch of private data 𝑋𝑝𝑟𝑖𝑣 (i.e., 𝑓 (𝑋𝑝𝑟𝑖𝑣)) to the server.
The server propagates this remote activation through the layers 𝑠
and computes the loss. Then, a gradient-descent-based optimization
is locally applied to 𝑠. To complete the round, the server sends the
gradient up to the input layer of 𝑠 to the client that continues the
back-propagation locally on 𝑓 .
In the case of supervised loss functions, the protocol requires
the client to share the labels with the server. To avoid that, split
learning can be reformulated to support loss function computation
on the client-side (Figure 1b). Here, the activation of the last layer
of 𝑠 is sent to the client that computes the loss function4, sending
the gradient back to the server that continues the back-propagation
as in the previous protocol.
Split learning supports the training of multiple clients by imple-
menting a sequential turn-based training protocol. Here, clients are
placed in a circular list and interact with the server in a round-robin
fashion. On each turn, a client performs one or more iterations of
4The client can also apply additional layers before the loss computation.
the distributed back-propagation (i.e., Figure 1) by locally modi-
fying the weights of 𝑓 . Then, the client sends the new 𝑓 to the
next client that repeats the procedure. As stated in [25], the train-
ing process, for suitable assumptions, is functionally equivalent to
the one induced by the standard, centralized training procedure.
That is, clients converge to the same network that they would have
achieved by training a model on the aggregated training sets.
To overcome the sequential nature of the training process, ex-
tensions of split learning have been proposed [31, 51, 54]. More
prominently, in [51], split learning is combined with federated learn-
ing (i.e., splitfed learning) to yield a more scalable training protocol.
Here, the server handles the forward signal of the clients’ network
in parallel (without aggregating them) and updates the weights
of 𝑠. The clients receive the gradient signals and update their local
models in parallel. Then, they perform federated learning to con-
verge to a global 𝑓 before the next iteration of split learning. This
process requires an additional server that is different from the one
hosting 𝑠.5
Split learning gained particular interest due to its efficiency and
simplicity. Namely, it reduces the required bandwidth significantly
when compared with other approaches such as federated learn-
ing [49, 57]. Certainly, for large neural networks, intermediate
activation for a layer is consistently more compact than the net-
work’s gradients or weights for the full network. Furthermore,
the computational burden for the clients is smaller than the one
caused by federated learning. Indeed, clients perform forward/back-
ward propagation on a small portion of the network rather than
on the whole. This allows split learning to be successfully applied
to the Internet of Things (IoT) and edge-device machine learning
settings [22, 34, 39].
2.2.1 On the security of Split learning. Split learning has been
proposed as a privacy-preserving implementation of collaborative
learning [5, 25, 42, 55, 56]. In split learning, users’ data privacy
relies on the fact that raw training instances are never shared; only
“smashed data” induced from those instances are sent to the server.
The main advantage of split learning in terms of security is
that it can hide information about the model’s architecture and
hyper-parameters. Namely, the server performs its task ignoring
the architecture of 𝑓 or its weights. As assumed in previous
works [5, 25, 42, 56], this split is designed to protect the intel-
lectual property of the shared model and to reduce the risk
5Otherwise, a single server would access both 𝑓 and 𝑠, violating the security of
the protocol.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2115of inference attacks perpetrated by a malicious server. As a
matter of fact, in this setup, the server cannot execute a standard
model inversion attack [18, 63] since it does not have access to the
clients’ network and cannot make blackbox queries to it.
We will show that these assumptions are false, and the split
learning framework presents several vulnerabilities that allow an
attacker to subvert the training protocol and recover clients’ train-
ing instances. The most pervasive vulnerability of the framework
is the server’s entrusted ability to control the learning process of
the clients’ network. A malicious server can guide 𝑓 towards func-
tional states that can be easily exploited to recover 𝑋𝑝𝑟𝑖𝑣 data from
𝑓 (𝑋𝑝𝑟𝑖𝑣). The main issue is that a neural network is a differentiable,
smooth function that is naturally predisposed to be functionally
inverted. There is no much that can be achieved by splitting it other
than a form of security through obscurity, which is notoriously
inadequate since it gives only a false sense of security.
In the next section, we empirically demonstrate how a mali-
cious server can exploit the split learning framework’s inherent
shortcomings to disclose clients’ private training sets completely.
Furthermore, in Section 5, we demonstrate that split learning does
not protect honest clients from malicious ones, even when the
server is honest.
3 FEATURE-SPACE HIJACKING ATTACK
Here, we introduce our main attack against the split learning train-
ing protocol—the Feature-space hijacking attack (FSHA). We start
in Section 3.1 by detailing the threat model. Then, Section 3.2 intro-
duces the core intuition behind the attack, as well as its formaliza-
tion. Section 3.3 covers the pragmatic aspects of the attack, demon-
strating its effectiveness. Section 3.5 extends the FSHA framework
to property inference attacks.
3.1 Threat model
We assume that the attacker does not have information on the
clients participating in the distributed training, except those re-
quired to run the split learning protocol. The attacker has no in-
formation on the architecture of 𝑓 and its weights. Moreover, the
attacker ignores the task on which the distributed model is trained.
However, the adversary knows a dataset 𝑋𝑝𝑢𝑏 that captures the
same domain of the clients’ training sets (a “shadow dataset” [48]).
For instance, if the model is trained on face images, 𝑋𝑝𝑢𝑏 is also
composed of face images. Nevertheless, no intersection between
private training sets and 𝑋𝑝𝑢𝑏 is required. This assumption is con-
gruent with previous attacks against collaborative inference [29],
and makes our threat model more realistic and less restrictive than
the ones adopted in other related works [55, 58], where the adver-
sary is assumed to have direct access to leaked pairs of smashed
data and private data.
3.2 Attack foundations
As discussed in Section 2.2.1, the main vulnerability of split learning
resides in the fact that the server has control over the learning pro-
cess of the clients’ network. Indeed, even ignoring the architecture
of 𝑓 and its weights, an adversary can forge a suitable gradient and
force 𝑓 to converge to an arbitrary target function chosen by the
attacker. In doing so, the attacker can induce certain properties in
the smashed data generated by the clients, enabling inference or
reconstruction attacks on the underlying private data.
Here, we present a general framework that implements this
attack procedure. In such a framework, the malicious server substi-
tutes the original learning task chosen by the clients with a new
objective that shapes, on purpose, the codomain/feature-space of 𝑓 .6
During the attack, the server exploits its control on the training
process to hijack 𝑓 and steer it towards a specific, target feature
space ˜Z that is appositely crafted. Once 𝑓 maps into ˜Z, the attacker
can recover the private training instances by locally inverting the
known feature space.
Such an attack encompasses two phases: (1) a setup phase where
the server hijacks the learning process of 𝑓 , and (2) a subsequent
inference phase where the server can freely recover the smashed
data sent from the clients. Hereafter, we refer to this procedure as
Feature-space Hijacking Attack, FSHA for short.
Setup phase. The setup phase occurs over multiple training iter-
ations of split learning and is logically divided into two concurrent
steps, which are depicted in Figures 2a and 2b. In this phase of the
attack, the server trains three different networks; namely, ˜𝑓 , ˜𝑓 −1,
and 𝐷. These serve very distinct roles; more precisely:
• ˜𝑓 : is a pilot network that dynamically defines the target fea-
ture space ˜Z for the client’s network 𝑓 . As 𝑓 , ˜𝑓 is a mapping
between the data space and a target feature space ˜Z, where
| ˜𝑓 (𝑥)| = |𝑓 (𝑥)| = 𝑘.
• ˜𝑓 −1: is an approximation of the inverse function of ˜𝑓 . During
˜𝑓
the training, we use it to guarantee the invertibility of
and recover the private data from smashed data during the
inference phase.
• 𝐷: is a discriminator [23] that indirectly guides 𝑓 to learn
a mapping between the private data and the feature space
defined from ˜𝑓 . Ultimately, this is the network that substi-
tutes 𝑠 in the protocol (e.g., Figure 1), and that defines the
gradient which is sent to the client during the distributed
training process.
The setup procedure also requires an unlabeled dataset 𝑋𝑝𝑢𝑏 that
is used to train the three attacker’s networks. Observe that this is
the only knowledge of the clients’ setup that the attacker requires.
The effect of 𝑋𝑝𝑢𝑏 on the attack performance will be analyzed in
the next section.
As mentioned before, at every training iteration of split learning
(i.e., when a client sends smashed data to the server), the malicious
server trains the three networks in two concurrent steps, which
are depicted in Figures 2a and 2b. The server starts by sampling
a batch from 𝑋𝑝𝑢𝑏 that uses to jointly train ˜𝑓 and ˜𝑓 −1. Here, the
server optimizes the weights of ˜𝑓 and ˜𝑓 −1 to make the networks
converge towards an auto-encoding function i.e., ˜𝑓 −1( ˜𝑓 (𝑥)) = 𝑥.
This is achieved by minimizing the loss function:
L ˜𝑓 , ˜𝑓 −1 = 𝑑( ˜𝑓 −1( ˜𝑓 (𝑋𝑝𝑢𝑏)), 𝑋𝑝𝑢𝑏),
(1)
6The client’s network 𝑓 can be seen as a mapping between a data space X
(i.e., where training instances are defined) and a feature space Z (i.e., where smashed
data are defined).
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2116log(1 − 𝐷( ˜𝑓 (𝑋𝑝𝑢𝑏))) + 𝑙𝑜𝑔(𝐷(𝑓 (𝑋𝑝𝑟𝑖𝑣)))
𝑑( ˜𝑓 −1( ˜𝑓 (𝑋𝑝𝑢𝑏)), 𝑋𝑝𝑢𝑏)
𝑙𝑜𝑔(1 − 𝐷(𝑓 (𝑋𝑝𝑟𝑖𝑣)))
Server
Server
˜𝑓 −1
˜𝑓
𝑋𝑝𝑢𝑏
𝐷
𝑓
𝑋𝑝𝑟𝑖𝑣
˜𝑓 −1
˜𝑓
𝑋𝑝𝑢𝑏
𝐷
𝑓
𝑋𝑝𝑟𝑖𝑣
Server
˜𝑋𝑝𝑟𝑖𝑣
˜𝑓 −1
𝑓
𝑋𝑝𝑟𝑖𝑣
(a) Attacker’s training procedure.
(b) Client’s training procedure.
(c) Inference procedure.
Figure 2: Schematic representation of the setup and inference process of the feature-space hijacking attack. In the scheme,
opaque rectangles depict the neural networks actively taking part to the training. Instead, more transparent rectangles are
networks that may participate to the forward propagation but do not modify their weights.
where 𝑑 is a suitable distance function, e.g., the Mean Squared
Error (MSE).
Concurrently, also the network 𝐷 is trained. This is a discrimi-
nator [23] that is trained to distinguish between the feature
space induced from ˜𝑓 and the one induced from the client’s
network 𝑓 . The network 𝐷 takes as input ˜𝑓 (𝑋𝑝𝑢𝑏) or 𝑓 (𝑋𝑝𝑟𝑖𝑣)
(i.e., the smashed data) and is trained to assign high probability
to the former and low probability to the latter. More formally, at
each training iteration, the weights of 𝐷 are tuned to minimize the
following loss function:
L𝐷 = log(1 − 𝐷( ˜𝑓 (𝑋𝑝𝑢𝑏))) + log(𝐷(𝑓 (𝑋𝑝𝑟𝑖𝑣))).
(2)
After each local training step for 𝐷, the malicious server can then
train the network 𝑓 by sending a suitable gradient signal to the
remote client performing the training iteration. In particular, this
gradient is forged by using 𝐷 to construct an adversarial loss func-
tion for 𝑓 ; namely:
L𝑓 = log(1 − 𝐷(𝑓 (𝑋𝑝𝑟𝑖𝑣))).
(3)
That is, 𝑓 is trained to maximize the probability of being miss-
classified from the discriminator 𝐷. In other words, we require
the client’s network to learn a mapping to a feature space that
is indistinguishable from the one of ˜𝑓 . Ideally, this loss serves as
a proxy for the more direct and optimal loss function: 𝑑(𝑓 (𝑥),
˜𝑓 (𝑥)).
However, the attacker has no control over the input of 𝑓 and must
overcome the lack of knowledge about 𝑥 by relying upon an ad-
versarial training procedure that promotes a topological matching
between feature spaces rather than a functional equivalence be-
tween networks.
Attack inference phase. After a suitable number of setup iter-
ations, the network 𝑓 reaches a state that allows the attacker to
recover the private training instances from the smashed data. Here,
thanks to the adversarial training, the codomain of 𝑓 overlaps with
the one of ˜𝑓 . The latter feature space is known to the attacker who
can trivially recover 𝑋𝑝𝑟𝑖𝑣 from the smashed data by applying the
inverse network ˜𝑓 −1. Indeed, as the network 𝑓 is now mapping the
data space into the feature space ˜Z, the network ˜𝑓 −1 can be used
to map the feature space ˜Z back to the data space, that is:
˜𝑋𝑝𝑟𝑖𝑣 = ˜𝑓 −1(𝑓 (𝑋𝑝𝑟𝑖𝑣)),
where ˜𝑋𝑝𝑟𝑖𝑣 is a suitable approximation of the private training
instances 𝑋𝑝𝑟𝑖𝑣. This procedure is depicted in Figure 2c. The quality
of the obtained reconstruction will be assessed later in the paper.
We emphasize that the feature-space hijacking attack performs
identically on the private-label version of the protocol, e.g., Fig-
ure 1b. In this case, at each training step, the server sends arbitrary
forged inputs to the clients’ final layers and ignores the gradient
produced as a response, hijacking the learning process of 𝑓 as in the
previous instance. More generally, in the case of multiple vertical
splits, a malicious party can always perform the attack despite its
position in the stack. Basically, the attacker can just ignore the
received gradient and replace it with the crafted one, leaving the
underlying splits to propagate the injected adversarial task. Ad-
ditionally, the effectiveness of the attack does not depend on the
number of participating clients.
In the same way, the feature-space hijacking attack equally ap-
plies to extensions of split learning such as Splitfed learning [51].
Indeed, in this protocol, the server still maintains control of the
learning process of 𝑓 . The only difference is in how the latter is
updated and synchronized among the clients. Interestingly, the at-
tack can be potentially more effective as the server receives bigger
batches of smashed data that can be used to smooth the learning
process of the discriminator.
In the next section, we implement the feature-space hijacking
attack, and we empirically demonstrate its effectiveness on various