field-insensitive pointer analysis results in an over-approximated
sensitive data domain, but provides a simpler implementation alter-
native and does not require widening and aligning of the individual
fields of structures.
Using the results of the pointer analysis, we populate two maps:
pointsToMap, which maps pointers to their possible targets, and
pointsFromMap, which maps objects to pointers that may point to
them. Once the results of the pointer analysis and the value flow
analysis are available, we construct the equivalence class for the
sensitive pointers and objects. The pseudo-code for this process is
provided in Algorithm 1 in the appendix.
5.1.2 Value Flow Analysis. As discussed in Section 4.3, data of
objects marked as sensitive may be copied and stored to other
objects (sink sites). Given that these objects must remain encrypted
in memory, we perform interprocedural value flow analysis to find
them and instrument them appropriately.
The LLVM instructions LoadInst and StoreInst are used to
read from and write to memory, respectively. For the purposes
of our value flow analysis, we track the flows that begin from a
LoadInst reading a sensitive object, and terminate in a StoreInst
writing to a non-sensitive object. As discussed earlier, SVF repre-
sents the constraints required for points-to analysis in the form
of a constraint graph, which it then solves to resolve the targets
of every pointer in the program. We leverage this graph, and add
edges corresponding to the value flows caused by LoadInst and
StoreInst. After the points-to analysis is complete, we perform a
breadth-first graph traversal to derive the sensitive sink sites. Be-
cause the solution of the SVF constraint graph contains the targets
of function pointers, we can trivially track inter-procedural value
flows even in the presence of function pointers.
The results of the pointer analysis are used to track indirect
sensitive value flows through pointers. We first find which pointers
might point to sensitive objects. Then, we perform value flow anal-
ysis on the values defined by LoadInst instructions that perform
an indirect memory read using these pointers.
5.1.3 Partially Sensitive Complex Objects. Field-sensitive points-
to and value flow analysis may cause individual fields of struct-
type objects to become sensitive. This creates a problem because
these individual fields are often smaller than 128-bits long. One
Figure 2: Overview of our sensitive data protection approach as implemented in LLVM.
solution could be to align all fields of such a partially-sensitive
struct-type objects to 128 bits, but this has the risk of degrading
cache performance, as the individual fields of the objects would be
spaced further away in memory. To minimize performance impact,
we align only those fields of a struct that are sensitive to 128-bit
boundaries, and also pad them to 128-bit size.
A second challenge is the use of the sizeof operator, which
allows the programmer to retrieve the allocation size of an object
in memory. This operator is lowered by the Clang frontend into
constants according to the object size, before the IR is constructed.
With our approach, however, the correct size of partially sensitive
objects becomes available only after alignment and padding, which
is performed at the IR level. We address this issue by modifying
the Clang frontend to append each instruction that uses a sizeof
operator with custom metadata (passed on to the IR level) that
includes the struct type on which the sizeof operator was applied.
Once the points-to and value flow analysis have completed, we
revisit these instructions and recalculate the sizes of any struct-
type objects based on the alignment and padding of their sensitive
fields. We then fix up the constants corresponding to the sizeof
operators with the recalculated sizes.
5.1.4 Memory Encryption Transformations. The Sensitive Data Do-
main contains the set of memory objects that must be kept en-
crypted in memory. This set includes global objects, objects on the
heap, and objects on the stack, which in LLVM IR are represented by
GlobalVariable, CallInst, and AllocaInst class objects, respec-
tively. First, we use the pointsFromMap provided by the pointer
analysis to find all sensitive pointers that might refer to these ob-
jects. Then, we collect all LoadInst and StoreInst instructions
that read and write to sensitive objects in memory, either directly
or via sensitive pointers. These instructions must be rewritten to
decrypt or encrypt the sensitive objects. We present the details of
this code transformation phase in Section 5.1.5 below.
To apply these cryptographic transformations, objects must be
128-bit aligned, while global variables with default initializers, con-
stant values, and environment variables within the sensitive data
domain, must be initialized to the correct encrypted values. We
handle these special cases by adding the correct transformations to
the IR. To ensure that sensitive values read from memory remain
protected during the subsequent stages of the compilation process,
we add a SENSITIVE metadata tag to the values defined by the
LoadInst instructions, which is propagated to the LLVM backend.
5.1.5 Hardware-accelerated AES and Key Protection. Intel proces-
sors provide the aesenc, aesenclast, aesdec, and aesdeclast
instructions (as part of the AES-NI extensions) to speed up AES oper-
ations. The latest Intel processors also support the Streaming SIMD
Extensions (SSE) [6] and the more recent, Advanced Vector Exten-
sions (AVX) [56]. Intel SSE provides 32 128-bit registers (XMM0–
XMM15), and AVX widens them to 256 bits (YMM0—YMM15). Intel
SSE also includes instructions for writing and reading individual
8/16/32/64 bytes from XMM registers (pinsrb, pinsrw, pinsrd,
pinsrq, and pextrb, pextrw, pextrd, pextrq, respectively). Sim-
ilarly, AVX includes instructions for reading individual 128-bit
chunks from the YMM registers (vinserti128 and vextracti128).
We use these instructions to perform cryptographic operations
oblivious to memory leakage. Using a 128-bit key with AES requires
10 processing rounds, each consuming four words (128 bits) from
the key schedule (derived from the initial 128-bit key), also referred
to as “round keys.” Before any round-based processing begins, the
input value is XOR-ed with the first four words of the key sched-
ule (for a total of 11 four-word keys). To avoid the overhead of
generating the round keys from scratch before each AES opera-
tion, they should ideally be pre-generated from the initial secret
key, and stored in registers [45, 58]. To protect these round keys
from memory disclosure vulnerabilities, the code that loads them
into registers is placed on its own 4KB page, which is zeroed out
immediately upon its execution.
Storing all the round keys in registers would require 22 128-bit
registers. Processors with AVX support provide access to 16 256-bit
registers, which can be accessed independently as 32 128-bit regis-
ters. However, Libc and other libraries rely on XMM registers to
perform optimizations such as loop unrolling. To maintain compat-
ibility with such optimizations, we use only the 15 YMM registers
to store all ten expanded encryption round keys, a subset (four)
of the expanded decryption round keys, and the single XOR key.
Decryption round keys are the inverse of the encryption round
keys, and Intel provides the aesimc instruction to compute the
decryption round key, given its encryption counterpart. Per Intel’s
Protected executableGold linkerPointer analysisValue flow analysisInstruction selectionRegister allocationIR with AESinstrumentationLink Time Optimization: IR-level analysis and transformationsLLVM BackendStatic library with LTO objectsClangfrontendAnnotated C libraryClangfrontendAnnotated C source codeAES transformMerged IRllvm-ardocumentation, we use this instruction to compute the remaining
six decryption round keys on the fly as needed.
As noted earlier (Section 4.4), we use the 128-bit XMM0 register
as our decrypted data cache. We use the SSE instructions to read
or write individual values in an already decrypted block stored in
XMM0, and load the AES round keys into the XMM1 register.
The logic for loading the keys into registers is encapsulated in
a function named populate_keys. To effortlessly rotate the keys
upon each new program invocation, we rely on the binary analysis
and rewriting capabilities of Pyelftools [21], which we used to
implement a custom program that replaces all instances of the
old encryption and decryption keys with new user-provided (or
randomly generated) values.
5.1.6 Handling Common Libc Functions. Functions such as strcpy,
strlen, strcmp, and their memory counterparts memcpy, memcpy,
and memset, are utility functions that are invoked with a variety
of arguments. Some of these arguments are sensitive, while others
are not. If we were to mark the arguments to these functions as
sensitive, then any invocation with even a single sensitive argument
would require the other non-sensitive arguments to also be included
in the Sensitive Data Domain, as discussed in Section 4.2.1. This
would cause these other arguments to also be marked as sensitive
and be encrypted in memory. This would increase the performance
overhead as they would have to be decrypted to be computed on.
We solve this challenge by providing custom sensitive and non-
sensitive implementations of these commonly used functions. For
example, if the Libc function strlen is invoked at two places, once
with a sensitive string, and once with a non-sensitive string, the
first instance will invoke the sensitive implementation of strlen.
This version decrypts every byte of the string, as it checks for
the NULL termination character. The other invocation will invoke
the vanilla implementation of strlen. This approach prevents the
over-approximation of the Sensitive Data Domain, and the resulting
additional performance overhead.
5.2 LLVM Backend
The LLVM backend lowers the IR to assembly code. We propagate
the sensitive metadata associated with every sensitive IR value
through the different phases of this lowering process. Instruction
selection and register allocation are two critical phases of this low-
ering step. Using the sensitive metadata, we made a number of
modifications to these phases to guarantee that sensitive data re-
mains encrypted in memory. We discuss each in turn.
Instruction Selection. One of the requirements of encrypting
5.2.1
sensitive data in memory is that no instruction can directly operate
on in-memory operands. However, the x86 architecture supports in-
memory operands for arithmetic and logical instructions. Directly
accessing in-memory encrypted operands, without decrypting and
storing them in registers first, will give incorrect results for the
operation. Based on our experimentation, we observed that LLVM’s
FastISel instruction selection algorithm prefers the selection of
instructions with in-register operands, over those with in-memory
operands. However, to ensure the absolute correctness of our imple-
mentation, we modified FastISel to select arithmetic and logical
instructions with solely in-register operands.
5.2.2 Register Allocation. Registers in LLVM’s IR are virtual and
infinite. As the IR is lowered to architecture-specific instructions,
virtual registers are mapped to physical architecture-specific regis-
ters. Due to the limited number of physical registers, values stored
in them may be spilled to memory. We use the metadata collected
during the memory encryption transformation (described in Sec-
tion 5.1.4), to track the virtual registers that contain sensitive values.
LLVM’s FastRegAlloc register allocation algorithm maps each
virtual register to a slot on the stack. When register pressure in-
creases, it selects a virtual register to spill on the respective stack
slot. We modified LLVM’s FastRegAlloc to encrypt the values
stored in sensitive virtual registers before spilling them their desig-
nated stack slots, and re-encrypt them when they are restored.
6 EXPERIMENTAL EVALUATION
To investigate the performance overhead of the proposed approach,
we evaluated our prototype with stress-test microbenchmarks and
five real-world applications. In the microbenchmarks, we annotate
all data used for computation as sensitive, whereas in the real-world
applications, we mark only data that is critical from a security per-
spective as sensitive. To illustrate the impact of pointer analysis
accuracy on performance, in case of the real-world applications,
we evaluate both our simpler field-insensitive implementation, as
well as the more fine-grained field-sensitive implementation. Note
that pointer analysis accuracy does not have an impact on the mi-
crobenchmarks, in which all data is marked as sensitive. In addition,
we performed experiments to verify that sensitive data always re-
main encrypted in memory, and to demonstrate how this thwarts
Spectre attacks.
Our testbed consists of a server with an Intel Xeon E3-1240 v6
processor, and a client with an Intel Xeon E5-2620 v4 processor.
Both machines run Ubuntu 16.04.3 LTS, and use Glibc version 2.23.
Single-machine benchmarks were run on the server machine.
6.1 Microbenchmarks
As discussed in Section 2, previous works on data space random-
ization [24] rely on XOR-based transformation to protect all in-
memory data. In the face of memory leakage vulnerabilities, how-
ever, strong encryption must be used to ensure data confidentiality.
Unfortunately, unrestrictedly encrypting all data in memory re-
sults in a prohibitively high runtime overhead, which we set out to
explore with a pair of worst-case microbenchmarks.
The first program computes the sum of ten billion randomly gen-
erated 64-bit integers, which are stored in a dynamically allocated
buffer that is annotated as sensitive. We measured the average CPU
user time to compute the sum across multiple repetitions, which
resulted in a runtime overhead of 390%. By inspecting the output
of the the value flow and pointer analysis, we observed that 95% of
all memory read and write operations across the whole code access
sensitive memory regions. These accesses are the main source of
the runtime overhead due to cryptographic operations.
The second program uses the quicksort algorithm on ten bil-
lion randomly generated 64-bit integers. The key difference of this
benchmark from the previous one is that its memory access pattern
is more random. We observe that close to 96% of all memory reads
and writes access sensitive memory regions. However, due to the
Table 1: Fraction of instrumented instructions among all
memory-related instructions in the code, and all memory-
related instructions executed.
Application
Code
Execution
Field Field Field Field
Sen.
Ins.
Ins.
Sen.
MbedTLS SSL Server
Lighttpd with ModAuth
Memcached with Auth.
ssh-agent
Minisign
31%
20%
0.1%
17%
28%
11%
5%
0.1%
8%
14%
26%
26%
∼0%
8%
55%
15%
11%
∼0%