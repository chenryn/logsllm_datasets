either have to wait until connectivity was allowed again or
spend money bribing Litya’s network administrators to gain
access to the network for a certain amount of time. The
teams had full knowledge of the Snort version and conﬁgu-
ration, thus, they could predict if their traﬃc would generate
an alert. Connecting Snort to the ﬁrewall forced the teams
to come up with novel ways to circumvent an IDS.
3.3.3 Botnet
Bribing Litya’s network administrators for access opened
up the network for a limited amount of time (proportional to
the amount of money used to bribe). To remain connected
to the network, the teams needed to run a bot, which we
provided 2 hours before the competition. This bot would
connect to a mothership every 30 seconds and while the bot
was connected to the mothership it would drain money from
the team at a rate of 6 money per minute. As long as the bot
remained connected to the mothership, the team had money,
and the team didn’t generate any Snort alerts, they could
access the services. The two means of connecting to the
network (bot connection or bribing) forced teams to make
strategic decisions about when to connect, when to attack,
how to attack, and when to bribe (spend money). These
strategic decisions added another dimension to the iCTF
competition: Teams had to decide the proper allocation of
money (bot connection or bribing) to maximize their access
to the network and thus maximize points.
Like a real-world bot, these machines were “compromised”
and had 3vilSh3ll [15], a backdoor bind connect, running on
port 8000. This allowed anyone to connect on port 8000,
supply the password: hacked, and obtain a root shell. The
idea was to encourage teams to be careful about their ﬁre-
wall, and force them to defensively select what traﬃc they
allowed into their network.
3.3.4 Challenges
To gain money to bribe the Litya network administrators,
as well as allow the mothership to steal money and remain
connected to the network, teams needed to solve challenges.
We created 33 challenges to provide multiple ways to earn
money, but also to oﬀer opportunities to test and improve
Service
LityaBook
LityaHot
icbmd
StormLog
StolenCC
SecureJava
IdreamOfJeannie
WeirdTCP
MostWanted
OvertCovert
Vulnerability
Cross-Site Scripting
Session Fixation
Foam Rocket Firing
Oﬀ-By-One Overﬂow
Perl’s open abuse
Broken Crypto
Java JNI Oﬀ-By-One Error
TCP IP Spooﬁng
SQL-Injection
Format String
Table 1: Brief description of vulnerable services.
the network became stuck (no eligible transitions) then the
network was reset.
For example, running the CARGODSTR mission, Fig-
ure 1 and Figure 2a, would involve ﬁrst placing a token in
its “Start” position. As T1 is the only eligible transition to
ﬁre, it is chosen, and this information is leaked to the teams.
The token moves from “Start” to “Ship.” Because services
S8 and S3 are associated with the “Ship” state, and it has a
token, they are active. After a pause of one to two minutes,
the next time-step occurs. Once again, there is only one
eligible transition, T2. It is chosen, and the token on “Ship”
moves to “Validate Cargo.” Now, services S8 and S3 are no
longer active, but service S1 becomes active. After another
pause, the process repeats, but with two eligible transitions,
T4 and T5. One of these is randomly chosen, say T5, and the
token moves. This process repeats until the end of the com-
petition. A visualization of the execution of all four missions
throughout the iCTF competition is available1.
From this example of the execution of the CARGODSTR
mission, the teams received the sequence: T1 T2 T5. With
only this information, they had to reverse engineer the state
of the mission to ﬁnd out which services were active. The
teams would then attack only the active services.
In the
CARGODSTR mission, this is simple because the transi-
tions are unique, however this is not the case for all the
missions, as shown in the SEDAFER mission (Figure 2d).
3.3.7 Flags
A ﬂag was a sequence of hexadecimal values preﬁxed with
FLG and was speciﬁc to a service. Flags were not directly
accessible: a service must be compromised to access the as-
sociated ﬂag. Therefore, ﬂags are used by the participants
as proof that, at a certain time, they were able to compro-
mise a speciﬁc service. On each step of the service that
executed the Petri nets, a new ﬂag speciﬁc to each service
was distributed to the corresponding service. Each ﬂag con-
tained (cryptographically) the service that it belonged to,
the state of the service (active or not), and a timestamp sig-
nifying when the ﬂag was created. Thus, when a ﬂag was
submitted by a team, the ﬂag submission service had all the
necessary information to determine the ﬂag’s validity (ﬂags
were valid for 5 minutes).
3.3.8 Vulnerable Services
There were 10 services in the iCTF, each service could be
exploited only once per Petri net execution round; exploiting
a service when it was not active resulted in an equal amount
of negative points. Thus, to win the competition it was
essential to understand and follow the missions. Table 1
1http://ictf.cs.ucsb.edu/data/ictf2010/final.gif
Figure 1: CARGODSTR mission that was distributed to the
teams.
diﬀerent skills, from cryptanalysis to forensics, program and
network analysis.
Scoreboard
3.3.5
In a capture the ﬂag competition, a scoreboard showcas-
ing the current status and ranking of each team is vital.
For the iCTF competition, we also needed to show the con-
nection status of each team; if they were connected to the
network, and why they were disconnected: Either from an
IDS alert, lack of a bot connection, or lack of money. The
scoreboard also showed the history of each team’s money
and points. The scoreboard is a very important piece of
the infrastructure, because it provides immediate feedback
to the teams about the success (or failure) of their attacks.
Unfortunately, we had some glitches in our scoreboard that
we will discuss in Section 5.
3.3.6 Missions
The day before the competition each team received an
email containing a link to four pictures. Each picture con-
tained a description of a Cyber Situational Awareness mis-
sion, in the form of a hand-drawn Petri net. Figure 1 shows
one of these missions, the CARGODSTR mission.
In the
Petri net, all of the transitions were named (although not
unique across the missions and even within some missions),
as were most of the states. Some of the states were as-
sociated with one or more of the 10 services (S0-S9). For
example, the “Receive” state in the lower right of Figure 1 is
associated with services S7 and S9. The four Petri net mis-
sions given to the teams are graphically shown in Figure 2.
A service that we ran executed the Petri nets by inserting
a token in each of the “Start” states, and running each Petri
net separately. At each time-step, for each of the missions,
one of the eligible transitions (a transition where all inputs
had tokens) was randomly chosen to ﬁre. Then, the token
was consumed on all the inputs to the chosen transitions,
and a token was placed on all the outputs. The four chosen
transitions (one from each mission) were leaked to the teams
after each time-step. Then, after each mission was executed,
the service suspended for a random amount of time between
one and two minutes, and the process repeated until the
end of the competition. If a token was in an “End” state, or
(a) CARGODSTR
(b) COMSAT
(c) DRIVEBY
Figure 2: Graphical representation of the missions given to the teams. The teams were actually given formats similar to
Figure 1. Not shown here are the associations of the services to states in the Perti nets.
(d) SEDAFER
Reorder/ModifyT3Accept CargoT8T6T7EndSea DistributionT10ReceiveT12Air DistributionT11BillT13Validate CargoT4T5StartT1Land DistributionT9ShipT2Discussion & EvaluationT7EndStartT1PlanningT9T8Reporting & AggregationT5T6Satellite Image AcquisitionT2T3DistributionT10Image ProcessingT4Video ProcessingT9Blackhat SEOT3EndDeliver AttackT9T10Search Engine Result AnalysisT4T5StartFailure AnalysisT1Detect Clean-upT12Establish Drive-byT11T13T7T2Attack FailedT8ReevalCiola tuitT11Mobradi LosT13Trobor/MoastT5EndMotados BurolnT0T9MorakniT4T3KaveritT7Nocom AstrizT8Kobaric JevT7T6StartT0Cinoja CuesT3T6T5Secuz MintaT1Moar TuhT10T2T11Simpt DvT9Mim SecvzT12Locab BlutunicSligtirT1T2T8Dokrat BeritT4brieﬂy summarizes the services. We direct the interested
reader to Appendix A for an extended description of the
services.
4. DATA ANALYSIS
In addition to being an excellent learning exercise for the
teams involved, a security competition, if properly designed,
can be a great source of data that is diﬃcult to obtain in
other contexts. In the iCTF competition, we created a game
scenario to generate a Cyber Situational Awareness dataset.
Traﬃc collected during a security competition can be eas-
ier to analyze than real-world traﬃc, because there is more
information about the network and participants in the com-
petition. For example, all teams are identiﬁed, the vul-
nerable services are known, and there is no “noise traﬃc.”
Of course, a dataset collected in such controlled conditions
also suﬀers from a lack of realism and is limited in scope.
Nonetheless, the data collected during this competition is
the ﬁrst publicly available dataset that allows researchers to
correlate attacks with the missions being carried out.
The iCTF competition generated 37 gigabytes of network
traﬃc and complete information about services broken, chal-
lenges solved, ﬂags submitted, bribes paid, IDS alerts, and
bot connections. This data is made freely available2.
As this is the ﬁrst Cyber Situational Awareness dataset,
many possibilities exist for its use in Situational Awareness
research. One example would be using the dataset to train
a host-based CSA intrusion detection system that could use
more restrictive rules for a rule-based system (or tighter
thresholds in an anomaly-based system) when a service is
critical to a mission. One can also think of extending a
host-based IDS to a network CSA intrusion detection sys-
tem that understands not only the criticality of the services,
but also their dependencies and relationships. Another ex-
ample is the visualization of a network’s activity with CSA
in mind that helps a system administrator know which ser-
vices are currently critical and which will become critical
soon, helping them defend their network.
The ﬁrewall, bribing, bot, and money/points system can
be viewed in a game theory light. The teams had to decide
on the best way to allocate a scarce resource (money) to ac-
cess the network and potentially win the game. The teams
could perform any combination of bot connection and/or
bribing to access the network. Further research could in-
vestigate how the choice of resource allocation aﬀected each
team’s ﬁnal result.
4.1 Description of Results
One problem with designing and implementing a novel
competition is that teams may not understand the rules.
This was a concern during the design of the iCTF competi-
tion. We worried that the novel aspects of the competition,
especially the Petri net mission model, would be too com-
plex for the teams to understand. However, when the ﬁrst
ﬂags were submitted at 13:29, and subsequently when teams
started submitting ﬂags only for active services, it became
apparent that many teams understood the competition.
Of the 72 teams, 39 submitted a ﬂag, with 872 ﬂags sub-
mitted in total. 48% may seem like a low number, however
this means that almost half the teams broke at least one
2http://ictf.cs.ucsb.edu/data/ictf2010/
Service
Total
Active
Inact. % Inact.
Teams Flags/Team
MostWanted
OvertCovert
IdreamOf.
WeirdTCP
LityaBook
icbmd
StolenCC
680
97
49
24
16
5
1
562
82
37
23
12
3
0
118
15
12
1
4
2
1
17
15
24
4
25
40
100
38
6
6
2
3
1
1
17.895
16.167
8.167
12
5.333
5
1
Table 2: Flags submitted per service.
service. Many of the 39 teams submitted multiple ﬂags, in-
dicating that they understood the Petri net mission model.
At 17:00, “Plaid Parliament of Pwning” (PPP) of Carnegie
Mellon University, took ﬁrst place with 24,000 points. PPP
submitted a total of 93 ﬂags, with only 3 inactive ﬂags
(thus generating negative points), by compromising Idream-
OfJeannie, MostWanted, and OvertCovert. Because PPP
was able to compromise three services as well as understand
the Petri net model (as evidenced by the submission of only
three negative ﬂags), they won ﬁrst place.
Overall the teams exploited 7 of the 10 services:
icbmd,
IdreamOfJeannie, LityaBook, MostWanted, OvertCovert,
StolenCC, and WeirdTCP. We believe this is because we
underestimated the diﬃculty of the other 3 services. Secure-
Java and StormLog required a complex, multi-step process
that proved too diﬃcult for the teams to exploit. The teams
also had trouble understanding the steps involved to exploit
the session ﬁxation vulnerability in LityaHot.
Table 2 describes the number of ﬂags submitted for each
service. The “Total” column is the total number of ﬂags sub-
mitted for the service, “Active” and “Inact.” are the number
of ﬂags that were submitted when a service was active or
inactive. “% Inact.” is the percent of ﬂag submissions when
the service was inactive. “Teams” shows the number of teams
that submitted ﬂags for the service and “Flags/Team” shows
the average number of ﬂags submitted per team.
MostWanted was the most exploited service, with 680 to-
tal ﬂags submitted, followed by OvertCovert, with 97 ﬂags
submitted. It is clear that we did not estimate the diﬃculty
of the services correctly, and, as evidenced by the number of
teams that broke it, MostWanted was the easiest. Because
the teams did not know the diﬃculty of the services, some
luck is involved when teams decide which service to analyze
ﬁrst.
When we decided to create a complex competition, we
knew that not every team would have the skills, experience,
and luck to exploit a service and understand the Petri net