we use three metrics: false positive rate (FPR), false negative rate (FNR) and
accuracy (ACC), where F P R = F P/(F P + T N), F N R = F N/(T P + F N), and
ACC = (T P + T N)/(T P + T N + F P + F N), respectively. TP is true positive,
FP is false positive, TN is true negative and FN is false negative. We also show
the performance of the best detection model through the ROC curve, which can
be used to determine the best combination of true and false positive rates.
Table 4. Classiﬁcation accuracy. The means and standard deviations are calculated
using 10-times 10-fold cross-validation tests for each machine learning algorithm.
Machine learning Algorithm ACC
mean std
FPR
mean std
FNR
mean std
SVM
0.661
0.041
0.059
0.072
0.372
0.048
RandomForest
0.933 0.014 0.083 0.033 0.053 0.036
KNN
DecisionTrees
AdaBoost
0.894
0.020
0.162
0.027
0.050
0.022
0.902
0.020
0.091
0.035
0.100
0.033
0.918
0.022
0.100
0.030
0.066
0.034
Evaluation Result. Table 4 lists the accuracy of diﬀerent machine learning
algorithms used by PADetective. Most of these algorithms discover the PA
attackers with high accuracy and low false negative or false positive rate. Among
the ﬁve machine learning algorithms we tested, RandomForest achieves the high-
est accuracy (i.e., 0.933) with the lowest false positive (i.e., 0.083) and false neg-
ative (i.e., 0.053) rates. Moreover, its standard deviations of the accuracy, false
positive rate, and false negative rate of RandomForest are also low, indicating
that RandomForest can identify promotional attackers eﬀectively. We use the
grid search to determine the best parameter for RandomForest, and ﬁnd that 50
is the optimal number of trees. Based on these results, we select RandomForest
as our detection model.
To better understand the root causes of false negative rate and false positive
rate in our system, we conduct error analysis with manual inspection. It turns out
that PADetective failed to detect the PA attackers who had posted reviews for a
Characterizing Promotional Attacks in Mobile App Store
123
Fig. 8. Evaluation of detection model using test set.
period of two years or longer. On the other hand, PADetective wrongly ﬂagged
the legitimate reviewers whose behaviors were similar to a PA attacker (e.g., their
reviews seemed to be fake, but the apps were not ﬂagged as malware/adware by
VirusTotal). Note that advanced malware may evade the online virus checkers.
Finally, using the optimized RandomForest algorithm, we test PADetective’s
accuracy using the test dataset. Figure 8 shows that it can achieve 90% true
positive rate with low false positive rate of 5.8%.
5 Promtional Attacks in the Wild
Using PADetective, we examined a large-scale data collected from the Google
Play Store, and found 289, 000 potential PA attackers from 2,605,068 reviewers.
Table 5 summarizes the number of reviewers/apps detected by PADetective. The
number of unique malicious apps reviewed by the potential PA attackers was
20,906, accounting for approximately 65% of the malicious apps reviewed by all
observed reviewers. Many malicious apps having reviews were associated with
the potential PA attackers. Moreover, the majority of malicious apps detected by
VirusTotal had no user reviews. It may be due to the fact that the malicious apps
were detected and deleted by mobile app stores in the early stage of distribution,
and hence there are no comments on such apps. Another possibility is that
mobile app stores deleted both malicious apps and their information including
reviews simultaneously, and therefore we can not collect the reviews. We ranked
Table 5. Statistics of detected promotional attackers and apps. “–” indicates that we
were not able to perform the evaluation due to the lack of resources.
All observed reviewers
Potential promotional
attackers
Detected promotional
attackers with high
probability
# reviewers # apps # malicious apps # apps deleted by app store
2, 605, 068
289, 000
234, 139
135, 989
32, 367
20, 906
–
–
1, 000
2, 904
486
148
124
B. Sun et al.
the reviewers in descending order according to the probability of being a PA
attacker, and investigated top 1,000 reviewers. The top 1,000 reviewers posted
reviews for 2, 904 of apps, which include 486 of malicious apps and 148 of apps
deleted by the app store for some reasons, e.g., malware or potentially harmful
apps.
Among the 1,000 promotional attackers, 136 reviewers (13.6%) posted
reviews only for malicious apps or the deleted apps. We found that other detected
reviewers posted reviews for not only malicious apps, but also for apps that were
not regarded as malware/adware by VirusTotal. We acknowledge that using the
online virus checkers might lead to false detection, and leave the checking of
those undetected apps in future work.
Figure 9 shows the top 10 categories of the apps reviewed by PA attackers.
Three categories (approximately 15% in total) are related to games, which was
the primary target of the PAs. To study the impact of apps promoted by PA
attackers, Fig. 10 illustrates the top 10 number of installs of the apps reviewed
by PA attackers. It shows that the majority of such apps do not have many
installs. This observation indicates that PAs are used when the app is not so
popular. There may be other reasons that the data was captured when the PA
was just launched (i.e., not yet ﬁnished).
We also investigate whether the detected PA attackers can be used to discover
malicious apps. More precisely, we compare the time when the PA attackers
posted reviews on malicious apps and the time when the malicious app was
ﬁrst submitted to VirusTotal. If all the posting times are earlier than the ﬁrst
submission time, then our PA detection scheme has the potential to identify
malicious apps that have not been listed in Virustotal. We examine the top
241 detected PA attackers who only reviewed malicious apps, and ﬁnd that 72
of them reviewed malicious apps before these malicious apps were detected by
VirusTotal. Among all the apps reviewed by these 72 promotional attackers, 217
apps were labeled as malicious app by VirusTotal. It is worth noting that other
apps reviewed by the PA attackers might also be suspicious.
Fig. 9. Top 10 categories of apps
reviewed by the detected promotional
attackers.
Fig. 10. Top 10 number of installs for
apps reviewed by the detected promo-
tional attackers.
Characterizing Promotional Attacks in Mobile App Store
125
6 Discussion
This section discusses some limitations of PADetective and future research
directions.
Evasion. Advanced attackers may evade the PADetective system by employ-
ing lots of user accounts with diﬀerent names and/or mimicking the reviewing
behaviors of normal users. It is worth noting that such evasion strategies require
much more resources and eﬀorts. For example, attackers may acquire lots of fake
user accounts and use each account to just post one comment in order to degrade
the detection accuracy of PADetective. However, since mobile app stores (e.g.,
Google Play) usually adopt advanced techniques [10] to deter automated account
registration, it will cost the attackers lots of resources and eﬀorts to create many
accounts and it does not beneﬁt the attackers if these accounts are just used to
post one comment. Note that the primary goal of the attackers is to increase the
success rate of attacks with lower costs [16]. Even if an attacker aﬀords to adopt
such an expensive approach, the stakeholders of mobile app stores can enhance
PADetective with additional information about each account, such as IP address
which could be correlated with user accounts to detect malicious users [24]. The
attackers may also mimic the reviewing behaviors of normal users by writing
short/long reviews, reviewing both legitimate and malicious apps, adjusting the
posting time, and etc. It will also signiﬁcantly increase the cost of attacks. We
leave the challenge of diﬀerentiating such advanced attacks and human reviewers
in future work.
Number of apps reviewed by each reviewer. PADetective does not consider
reviewers who posted comments for only one or two apps. This constraint origi-
nates from the fact that computing some features such as entropy or coeﬃcient
variants require more than two samples. In this work, we empirically set the num-
ber as 3 because increasing the number was not sensitive to the ﬁnal outcomes.
Since attackers usually employ the accounts to post a number of comments as
we discussed above, we believe that this number is reasonable to capture promo-
tional attackers. As the number of apps reviewed by a reviewer may exceed the
threshold, 3, over time, PADetective could identify them by continuously collect-
ing and analyzing the comments. We will construct a real-time detection system
for fetching and examining UGC and the metadata continuously in future work.
7 Related Work
Review Analysis. Kong et al. [13] designed AutoREB to automatically identify
users’ concerns on the security and privacy of mobile apps. They applied the
relevance feedback technique for the semantic analysis of user reviews and then
associated the results of the user review analysis to the apps’ behaviors by using
the crowd-sourcing technique. Mukherjee et al. [17,18] proposed new approaches
to detect fake reviewer groups from Amazon product reviews. They ﬁrst used a
frequent itemset mining method to identify a set of candidate groups, and then
126
B. Sun et al.
adopted several behavioral models based on the relationships among groups such
as the review posting time and similarities. Fu et al. [11] proposed WisCom to
provide important insights for end-users, developers, and potentially the entire
mobile app ecosystem. They leveraged sentiment analysis, topic model analysis,
and time-series analysis to examine over 13 M user reviews.
Rating Analysis. Xie et al. [22] proposed a new method for discovering colluded
reviewers in app stores. They built a relation graph based on the ratings and the
deviations of the ratings, and applied a graph cluster algorithm to detect collu-
sion groups. Oh et al. [19] developed an algorithm that calculates the conﬁdence
score of each app. Market operators can replace the average rating of each app
with the conﬁdence score to defend against rating promotion/demotion attacks.
Lim et al. [15] devised an approach to measure the degree of spam for each
reviewer based on the rating behaviors, and evaluated them using an Amazon
review dataset.
Among previous works mentioned above, [17,18,22] are closely related to
our work. The major diﬀerences between PADetective and Xie et al. [22] is
the scalability. More precisely, their system is not scalable because it is not
possible to build a tie graph of large-scale dataset in physical memory. Moreover,
they performed the evaluation on a small and local dataset (200 apps collected
from the china apple store). In contrast, since our detection model uses static
features, our system can conduct large-scale analysis. Moreover, we investigate
the prevalence of PAs in the oﬃcial Android app store by collecting information
on more than 1 M apps. The method of review analysis is the main diﬀerence
between PADetective and [17,18]. Since they aimed to identify copy reviews used
by spammers, their method only extracts the similar reviews in keyword level,
e.g., “good app” and “good apps”. Since users can express the same opinion using
diﬀerent words and expressions, e.g., “nice app” and “good app”, we leveraged
the state-of-the-art NLP technique called Paragraph vector [14] to extract similar
reviews at the semantic level for better accuracy.
8 Conclusion
In this study, we developed PADetective to detect PA attackers in mobile app
stores using UGC and metadata as well as machine-learning techniques. The
large-scale evaluation revealed that we can exploit the PA attackers identiﬁed by
PADetective to discover potentially malicious apps eﬀectively and eﬃciently. We
believe that this research sheds a new light on the analysis of UGC and metadata
of app stores as a complementary channel to ﬁnd malicious apps for enhancing
the widely used anti-malware tools or for market operators and malware analysts.
Acknowledgements. A part of this work was supported by JSPS Grant-in-Aid for
Scientiﬁc Research (KAKENHI) B, Grant number JP16H02832. A part of this work
was also supported by a Grant for Non-Japanese Researchers from the NEC C&C
Foundation and a Waseda University Grant for Special Research Projects (Project
number: 2016S-055).
Characterizing Promotional Attacks in Mobile App Store
127
References
1. Developer policy center. http://goo.gl/yA0qUb
2. Feature selection. http://scikit-learn.org/stable/modules/feature selection.html
3. gensim:topic modelling for humans. https://radimrehurek.com/gensim/
4. Google play reviews collection service. https://play.google.com/store/getreviews
5. Natural language toolkit. http://www.nltk.org
6. scikit-learn:machine learning in python. http://scikit-learn.org/stable/
7. Textblob: Simpliﬁed text processing. http://textblob.readthedocs.io/en/dev/
8. Virustotal- free online virus, malware and url scanner. https://www.virustotal.com
9. The FTC’s endorsement guides: What people are asking (2015). http://goo.gl/
3875GT
10. El Ahmad, A.S., Yan, J., Ng, W.-Y.: Captcha design: color, usability, and security.
IEEE Internet Comput. 16(2), 44–51 (2012)
11. Fu, B., Lin, J., Li, L., Faloutsos, C., Hong, J.I., Sadeh, N.M.: Why people hate
your app: making sense of user feedback in a mobile app store. In: Proceedings of
the ACM KDD (2013)
12. Ganguly, R.: App. store optimization - a crucial piece of the mobile app marketing
puzzle (2013). https://blog.kissmetrics.com/app-store-optimization/
13. Kong, D., Cen, L., Jin, H.: AUTOREB: automatically understanding the review-to-
behavior ﬁdelity in android applications. In: Proceedings of the ACM CCS (2015)
14. Le, Q.V., Mikolov, T.: Distributed representations of sentences and documents. In:
Proceedings of the ICML (2014)
15. Lim, E., Nguyen, V., Jindal, N., Liu, B., Lauw, H.W.: Detecting product review
spammers using rating behaviors. In: Proceedings of the ACM CIKM (2010)
16. Liu, B., Nath, S., Govindan, R., Liu, J.: DECAF: detecting and characterizing ad
fraud in mobile apps. In: Proceedings of the NSDI (2014)
17. Mukherjee, A., Liu, B., Glance, N.S.: Spotting fake reviewer groups in consumer
reviews. In: Proceedings of the WWW (2012)
18. Mukherjee, A., Liu, B., Wang, J., Glance, N.S., Jindal, N.: Detecting group review
spam. In: Proceedings of the WWW (2011)
19. Oh, H., Kim, S., Park, S., Zhou, M.: Can you trust online ratings? A mutual
reinforcement model for trustworthy online rating systems. IEEE Trans. Syst. Man
Cybern. Syst. 45(12), 1564–1576 (2015)
20. Statista Inc.: Number of apps available in leading app stores as of June 2016.
http://goo.gl/JnBkmY
21. Viennot, N., Garcia, E., Nieh, J.: A measurement study of google play. In: Pro-
ceedings of the ACM SIGMETRICS (2014)
22. Xie, Z., Zhu, S.: Grouptie: toward hidden collusion group discovery in app stores.
In: Proceedings of the ACM WiSec (2014)
23. Xie, Z., Zhu, S.: Appwatcher: unveiling the underground market of trading mobile
app reviews. In: Proceedings of the ACM WiSec (2015)
24. Zhao, Y., Xie, Y., Yu, F., Ke, Q., Yu, Y., Chen, Y., Gillum, E.: Botgraph: large
scale spamming botnet detection. In: Proceedings of the NSDI (2009)