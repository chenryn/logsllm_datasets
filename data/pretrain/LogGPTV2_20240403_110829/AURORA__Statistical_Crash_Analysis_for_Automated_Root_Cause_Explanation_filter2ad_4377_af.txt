USENIX Association
this is caused by the crash exploration producing an insuf-
ﬁciently diverse set of test cases. This applies particularly
to any input that was originally found by a grammar-based
fuzzer since AFL’s type of mutations may fail to produce suf-
ﬁciently diverse inputs for such targets [30]. We expect that
better fuzzing techniques will improve the ability to generate
more suitable corpora. Yet, no matter how good the fuzzer
is, in the end, pinpointing a single root cause will remain an
elusive target for automated approaches: even a human expert
often fails to identify a single location responsible for a bug.
Relying on a fuzzer illustrates another pitfall: We require
that bugs can be reproduced within a fuzzing setup. Therefore,
bugs in distributed or heavily concurrent systems currently
cannot be analyzed properly by our approach. However, this is
a limitation of the underlying fuzzer rather than AURORA: Our
analysis would scale to complex systems spanning multiple
processes and interacting components; our statistical model
can easily deal with systems where critical data is passed and
serialized by various means, including networks or databases,
where traditional analysis techniques like taint tracking fail.
In some cases, the predicates that we generate might not
be precise enough. While this situation did not happen during
our evaluation, hypothetically, there may exist bugs that can
only be explained by predicates spanning multiple locations.
For example, one could imagine a bug caused by using an
uninitialized value, which is only triggered if two particular
conditions are met: The program avoids taking a path ini-
tializing the value and later takes a path where the value is
accessed. Our single-location predicates fail to capture that
the bug behavior is reliant on two locations. We leave extend-
ing our approach to more complex and compound predicates
as an interesting question for future work.
Last, our system requires a certain computation time to
identify and explain root causes. In some cases, AURORA ran
for up to 17 hours (including 12 hours for crash exploration).
We argue that this is not a problem, as our system is used
in combination with normal fuzzing. Thus, an additional 17
hours of fuzzing will hardly incur a signiﬁcant cost for typ-
ical testing setups. Since it took us multiple person-days to
pinpoint the root cause for some of the bugs we analyzed,
making the integration of our fully automated approach into
the fuzzing pipeline seems feasible.
An integration to fuzzing could beneﬁt the fuzzer: Success-
ful fuzzing runs often produce a large number of crashing
inputs, many of which trigger the same crash. To save an
analyst from being overwhelmed, various heuristics are de-
ployed to identify equivalent inputs. Most rely on the input’s
coverage proﬁle or stack hashing where the last n entries of
the call stack are hashed [42]. Unfortunately, both techniques
have been shown to be imprecise, i. e., to report far too many
distinct bugs, while sometimes even joining distinct bugs into
one equivalence class [42]. Given an automated approach ca-
pable of identifying the root cause such as ours, it is possible
to bucket crashing inputs according to their root cause. To
this end, one could pick some random crashing input, iden-
tify its root cause and then check for all remaining crashing
inputs whether the predicate holds true. Each crashing input
for which the predicate is evaluated to true is then collected
in one bucket. For the remaining inputs, the process could be
repeated until all crashing inputs have been sorted into their
respective equivalence classes.
In some cases, such as closed-source binaries or a limited
amount of developer time, manually implementing ﬁxes may
be impossible. An automated approach to providing (tem-
porary) patches may be desirable. Our approach could be
extended to patch the root cause predicate into the binary
such that—at the point of the root cause—any input crashing
the binary leads to a graceful exit rather than a potentially
exploitable crash.
8 Related Work
In the following, we focus on works related closest to ours,
primarily statistical and automated approaches.
Spectrum-based Fault Localization. Closest related to
our work are so-called spectrum-based, i. e., code coverage-
based, fault localization techniques [34]. In other words, these
approaches attempt to pinpoint program elements (on dif-
ferent levels, e. g., single statements, basic blocks or func-
tions) that cause bugs. To this end, they require multiple in-
puts for the program, some of which must crash while others
may not. Often, they use test suites provided by developers
and depend on the source code being available. For instance,
Zhang et. al. [60] describe an approach to root cause identi-
ﬁcation targeting the Java Virtual Machine: ﬁrst, they locate
the non-crashing input from provided test suite whose con-
trol ﬂow paths beginning overlaps the most with the one of
the crashing input under investigation. Then, they determine
the location of the ﬁrst deviation, which they report as the
root cause. Overall, most approaches either use some met-
ric [26,27,41,54,55] to identify and rank possible root causes
or rely on statistical techniques [43, 44, 46].
As a sub-category of spectrum-based fault localization,
techniques based on statistical approaches use predicates to
reason about provided inputs. Predicate-based techniques are
used to isolate bugs [43] or to pinpoint the root cause of
bugs [44, 46, 60]. These approaches typically require source
code and mostly rely on inputs provided by test suites.
While our work is similar to such approaches with re-
spect to sampling predicates and statistically isolating the
root cause, our approach does not require access to source
code since it solely works on the binary level. Furthermore,
our analysis synthesizes domain-speciﬁc predicates tailored to
the observed behavior of a program. Also, we do not require
any test suite but rely on a fuzzer to generate test cases. This
provides our approach with a more diversiﬁed set of inputs,
allowing for more ﬁne-grained analysis.
USENIX Association
29th USENIX Security Symposium    249
Reverse Execution. A large number of works [32, 33, 38,
45, 57] investigate the problem of analyzing a crash, typi-
cally starting from a core dump. To this end, they reverse-
execute the program, reconstructing the data ﬂow leading to
the crash. To achieve this, CREDAL [56] uses a program’s
source code to automatically enrich the core dump analysis
with information aiding an analyst in ﬁnding memory cor-
ruption vulnerabilities. Further reducing the manual effort
needed, POMP requires a control-ﬂow trace and crash dump,
then uses backward taint analysis [57] to reverse the data
ﬂow, identifying program statements contributing to a crash.
In a similar vein but for a different application scenario—
core dumps sampled on an OS level—RETRACER [33] uses
backward taint analysis without a trace to reconstruct func-
tions on the stack contributing to a crash. Improving upon
RETRACER, Cui et. al. [32] developed REPT, an reverse
debugger that introduces an error correction mechanism to
reconstruct execution history, thereby recovering data ﬂow
leading to a crash. To overcome inaccuracies, Guo et. al. [38]
propose a deep-learning-based approach based on value-set
analysis to address the problem of memory aliasing.
While sharing the goal of identifying instructions causing
a crash, AURORA differs from these approaches by design.
Reverse execution starts from a crash dump, reversing the
data-ﬂow, thereby providing an analyst with concrete assem-
bly instructions contributing to a bug. While these approaches
are useful in scenarios where a crash is not reproducible, we
argue that most of them are limited to correctly identify bugs
that exhibit a direct data dependency between root cause and
crashing location. While REPT does not rely on such a de-
pendency, it integrates into an interactive debugging session
rather than providing a list of potential root cause predicates;
thus, it is orthogonal to our approach. Moreover, AURORA
uses a statistical analysis to generate predicates that not only
pinpoint the root cause but also add an explanation describing
how crashing inputs behave at these code locations. Further-
more, since we do not perform a concrete analysis of the
underlying code, AURORA can spot vulnerabilities with no
direct data dependencies.
9 Conclusion
In this paper, we introduced and evaluated a novel binary-only
approach to automated root cause explanation. In contrast to
other approaches that identify program instructions related
to a program crash, we additionally provide semantic expla-
nations of how these locations differ in crashing runs from
non-crashing runs. Our evaluation shows that we are able to
spot root causes for complex bugs such as type confusions
where previous approaches failed. Given debug information,
our approach is capable of enriching the analysis’ results
with additional information. We conclude that AURORA is a
helpful addition to identify and understand the root cause of
diverse bugs.
Acknowledgements
We would like to thank our shepherd Trent Jaeger and the
anonymous reviewers for their valuable comments and sug-
gestions. We also thank Nils Bars, Thorsten Eisenhofer and
Tobias Scharnowski for their helpful feedback. Additionally,
we thank Julius Basler and Marcel Bathke for their valuable
support during the evaluation. This work was supported by
the Deutsche Forschungsgemeinschaft (DFG, German Re-
search Foundation) under Germany’s Excellence Strategy –
EXC-2092 CASA – 390781972. In addition, this project has
received funding from the European Union’s Horizon 2020 re-
search and innovation programme under grant agreement No
786669 (ReAct). This paper reﬂects only the authors’ view.
The Research Executive Agency is not responsible for any
use that may be made of the information it contains.
References
[1] mruby heap buffer overﬂow (CVE-2018-10191). https:
//github.com/mruby/mruby/issues/3995.
[2] Lua heap buffer overﬂow.
bugs.html#5.0-2.
https://www.lua.org/
[3] Perl heap buffer overﬂow. https://github.com/Perl/
perl5/issues/17384.
[4] screen heap buffer overﬂow. https://seclists.org/
oss-sec/2020/q1/65.
[5] readelf heap buffer overﬂow (CVE-2019-9077). https:
//sourceware.org/bugzilla/show_bug.cgi?id=
24243.
[6] mruby heap buffer overﬂow (CVE-2018-12248). https:
//github.com/mruby/mruby/issues/4038.
[7] objdump heap over ﬂow (CVE-2017-9746). https:
//sourceware.org/bugzilla/show_bug.cgi?id=
21580.
[8] patch
heap
buffer
overﬂow.
//savannah.gnu.org/bugs/?func=
detailitem&item_id=54558.
https:
[9] Python heap buffer overﬂow (CVE-2016-5636). https:
//bugs.python.org/issue26171.
[10] tcpdump heap buffer overﬂow (CVE-2017-16808).
https://github.com/the-tcpdump-group/
tcpdump/issues/645.
[11] NASM nullpointer dereference (CVE-2018-16517).
https://nafiez.github.io/security/2018/09/
18/nasm-null.html.
250    29th USENIX Security Symposium
USENIX Association
[12] Bash segmentation fault.
https://lists.gnu.org/
archive/html/bug-bash/2018-07/msg00044.html.
[13] Bash segmentation fault.
https://lists.gnu.org/
archive/html/bug-bash/2018-07/msg00042.html.
[29] Cornelius Aschermann, Sergej Schumilo, Tim Blazytko,
Robert Gawlik, and Thorsten Holz. REDQUEEN:
Fuzzing with input-to-state correspondence. In Sym-
posium on Network and Distributed System Security
(NDSS), 2019.
[14] Python
segmentation
fault.
bugs.python.org/issue31530.
https://
[15] nm stack buffer overﬂow. https://sourceware.org/
bugzilla/show_bug.cgi?id=21670.
[16] mruby type confusion.
reports/185041.
[17] Python type confusion.
reports/116286.
https://hackerone.com/
https://hackerone.com/
[18] Xpdf
uninitialized
variable.
https://
forum.xpdfreader.com/viewtopic.php?f=3&t=
41890.
[19] mruby uninitialized variable. https://github.com/
mruby/mruby/issues/3947.
[20] PHP uninitialized variable (CVE-2019-11038). https:
//bugs.php.net/bug.php?id=77973.
[21] libzip use-after-free (CVE-2017-12858).
https:
//blogs.gentoo.org/ago/2017/09/01/libzip-use-
after-free-in-_zip_buffer_free-zip_buffer-
c/.
[22] mruby use-after-free (CVE-2018-10199). https://
github.com/mruby/mruby/issues/4001.
[23] NASM use-after-free. https://bugzilla.nasm.us/
show_bug.cgi?id=3392556.
[24] Sleuthkit use-after-free.
https://github.com/
sleuthkit/sleuthkit/issues/905.
[25] Lua use-after-free (CVE-2019-6706).
https:
//security-tracker.debian.org/tracker/CVE-
2019-6706.
[26] Rui Abreu, Peter Zoeteweij, Rob Golsteijn, and Arjan
J. C. van Gemund. A practical evaluation of spectrum-
based fault localization. Journal of Systems and Soft-
ware, 82(11):1780–1792, 2009.
[27] Rui Abreu, Peter Zoeteweij, and Arjan J. C. van Gemund.
Localizing software faults simultaneously. In Interna-
tional Conference on Quality Software, 2009.
[28] Cornelius Aschermann, Tommaso Frassetto, Thorsten
Holz, Patrick Jauernig, Ahmad-Reza Sadeghi, and
Daniel Teuchert. Nautilus: Fishing for deep bugs with
grammars. In Symposium on Network and Distributed
System Security (NDSS), 2019.
[30] Tim Blazytko, Cornelius Aschermann, Moritz Schlögel,
Ali Abbasi, Sergej Schumilo, Simon Wörner, and
Thorsten Holz. GRIMOIRE: Synthesizing structure
while fuzzing. In USENIX Security Symposium, 2019.
[31] Peng Chen and Hao Chen. Angora: Efﬁcient fuzzing by
principled search. In IEEE Symposium on Security and
Privacy, 2018.
[32] Weidong Cui, Xinyang Ge, Baris Kasikci, Ben Niu, Upa-
manyu Sharma, Ruoyu Wang, and Insu Yun. REPT:
Reverse debugging of failures in deployed software. In
Symposium on Operating Systems Design and Imple-
mentation (OSDI). USENIX Association, 2018.
[33] Weidong Cui, Marcus Peinado, Sang Kil Cha, Yanick
Fratantonio, and Vasileios P. Kemerlis. RETracer: Triag-
ing crashes by reverse execution from partial memory
dumps. In International Conference on Software Engi-
neering (ICSE), 2016.
[34] Higor Amario de Souza, Marcos Lordello Chaim, and
Fabio Kon. Spectrum-based software fault localization:
A survey of techniques, advances, and challenges. CoRR,
abs/1607.04347, 2016.
[35] Michael Eddington. Peach fuzzer: Discover unknown
vulnerabilities. https://www.peach.tech/.
[36] Free Software Foundation. GNU Binutils. https://
www.gnu.org/software/binutils/.
[37] Google.
Announcing OSS-Fuzz: Continu-
https:
ous fuzzing for open source software.
//testing.googleblog.com/2016/12/announcing-
oss-fuzz-continuous-fuzzing.html.
[38] Wenbo Guo, Dongliang Mu, Xinyu Xing, Min Du, and
Dawn Song. DEEPVSA: Facilitating value-set analysis
with deep learning for postmortem program analysis. In
USENIX Security Symposium, 2019.
[39] Roberto Ierusalimschy, Luiz Henrique De Figueiredo,
and Waldemar Celes Filho. The implementation of Lua
5.0. J. UCS, 11(7):1159–1176, 2005.
[40] Intel Corporation.
Pin – a dynamic binary instru-
https://software.intel.com/
mentation tool.
en-us/articles/pin-a-dynamic-binary-
instrumentation-tool.
USENIX Association
29th USENIX Security Symposium    251
[41] James A. Jones and Mary Jean Harrold. Empirical
evaluation of the tarantula automatic fault-localization
technique. In IEEE/ACM International Conference on
Automated Software Engineering (ASE), 2005.
[51] Konstantin Serebryany, Derek Bruening, Alexander
Potapenko, and Dmitriy Vyukov. AddressSanitizer: A
fast address sanity checker. In USENIX Annual Techni-
cal Conference, 2012.
[42] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei,
and Michael Hicks. Evaluating fuzz testing. In ACM
Conference on Computer and Communications Security
(CCS), 2018.
[52] Evgeniy Stepanov and Konstantin Serebryany. Memo-
rySanitizer: fast detector of uninitialized memory use in
C++. In IEEE/ACM International Symposium on Code
Generation and Optimization (CGO), 2015.
[43] Ben Liblit, Mayur Naik, Alice X. Zheng, Alexander
Aiken, and Michael I. Jordan. Scalable statistical bug
isolation. In ACM SIGPLAN Conference on Program-
ming Language Design and Implementation (PLDI),
2005.
[44] Chao Liu, Xifeng Yan, Long Fei, Jiawei Han, and
Samuel P. Midkiff. SOBER: statistical model-based
bug localization. In Proceedings of the 10th European
Software Engineering Conference held jointly with 13th
ACM SIGSOFT International Symposium on Founda-
tions of Software Engineering, 2005.
[45] Dongliang Mu, Yunlan Du, Jianhao Xu, Jun Xu, Xinyu
Xing, Bing Mao, and Peng Liu. POMP++: Facilitating
postmortem program diagnosis with value-set analysis.
IEEE Transactions on Software Engineering, 2019.
[46] Piramanayagam Arumuga Nainar, Ting Chen, Jake
Rosin, and Ben Liblit. Statistical debugging using com-
pound boolean predicates. In International Symposium
on Software Testing and Analysis (ISSTA), 2007.
[47] Hui Peng, Yan Shoshitaishvili, and Mathias Payer. T-
In IEEE
Fuzz: fuzzing by program transformation.
Symposium on Security and Privacy, 2018.
[48] Van-Thuan Pham, Marcel Böhme, Andrew E Santosa,
Alexandru R˘azvan C˘aciulescu, and Abhik Roychoud-
hury. Smart greybox fuzzing, 2018.
[49] Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Co-
jocar, Cristiano Giuffrida, and Herbert Bos. VUzzer:
Application-aware evolutionary fuzzing. In Symposium
on Network and Distributed System Security (NDSS),
February 2017.
[50] Jukka Ruohonen and Kalle Rindell. Empirical notes on
the interaction between continuous kernel fuzzing and
development. arXiv preprint arXiv:1909.02441, 2019.
[53] Nick Stephens, John Grosen, Christopher Salls, Andrew
Dutcher, Ruoyu Wang, Jacopo Corbetta, Yan Shoshi-
taishvili, Christopher Kruegel, and Giovanni Vigna.
Driller: Augmenting fuzzing through selective symbolic
execution. In Symposium on Network and Distributed
System Security (NDSS), 2016.
[54] Xiaoyuan Xie, Tsong Yueh Chen, and Baowen Xu. Iso-
lating suspiciousness from spectrum-based fault local-
ization techniques. In International Conference on Qual-
ity Software, 2010.
[55] Jian Xu, Zhenyu Zhang, W. K. Chan, T. H. Tse, and
Shanping Li. A general noise-reduction framework
for fault localization of Java programs. Information &
Software Technology, 55(5), 2013.
[56] Jun Xu, Dongliang Mu, Ping Chen, Xinyu Xing, Pei
Wang, and Peng Liu. CREDAL: towards locating a
memory corruption vulnerability with your core dump.
In ACM Conference on Computer and Communications
Security (CCS), 2016.
[57] Jun Xu, Dongliang Mu, Xinyu Xing, Peng Liu, Ping
Chen, and Bing Mao. Postmortem program analy-
sis with hardware-enhanced post-crash artifacts.
In
USENIX Security Symposium, 2017.
[58] Michael Zalewski.
aﬂ-fuzz: crash exploration
https://lcamtuf.blogspot.com/2014/11/
mode.
afl-fuzz-crash-exploration-mode.html.
[59] Michał Zalewski.
american fuzzy lop.
lcamtuf.coredump.cx/afl/.
http://
[60] Yongle Zhang, Kirk Rodrigues, Yu Luo, Michael Stumm,
and Ding Yuan. The inﬂection point hypothesis: a prin-
cipled debugging approach for locating the root cause
of a failure. In ACM Symposium on Operating Systems
Principles (SOSP), 2019.
252    29th USENIX Security Symposium
USENIX Association