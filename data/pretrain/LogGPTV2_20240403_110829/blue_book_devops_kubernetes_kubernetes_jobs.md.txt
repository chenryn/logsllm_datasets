---
title: Kubernetes jobs
date: 20201111
author: Lyz
---
[Kubernetes
jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/) creates
one or more Pods and ensures that a specified number of them successfully
terminate. As pods successfully complete, the Job tracks the successful
completions. When a specified number of successful completions is reached, the
task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.
[Cronjobs](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs)
creates Jobs on a repeating schedule.
This example CronJob manifest prints the current time and a hello message every
minute:
```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
```
To deploy cronjobs you can use the [bambash helm
chart](https://github.com/bambash/helm-cronjobs).
Check [the kubectl commands to interact with jobs](kubectl_commands.md#jobs-and-cronjobs).
# Debugging job logs
To obtain the logs of a completed or failed job, you need to:
* Locate the cronjob you want to debug: `kubectl get cronjobs -n cronjobs`.
* Locate the associated job: `kubectl get jobs -n cronjobs`.
* Locate the associated pod: `kubectl get pods -n cronjobs`.
If the pod still exists, you can execute `kubectl logs -n cronjobs {{ pod_name
}}`. If the pod doesn't exist anymore, you need to search the pod in your log
centralizer solution.
## [Rerunning failed jobs](https://serverfault.com/questions/809632/is-it-possible-to-rerun-kubernetes-job)
If you have a job that has failed after the 6 default retries, it will show up
in your monitorization forever, to fix it, you can manually trigger the job
yourself with:
```bash
kubectl get job "your-job" -o json \
    | jq 'del(.spec.selector)' \
    | jq 'del(.spec.template.metadata.labels)' \
    | kubectl replace --force -f -
```
## [Manually creating a job from a cronjob](https://github.com/kubernetes/kubernetes/issues/47538)
```bash
kubectl create job {{ job_name }} -n {{ namespace }} \
    --from=cronjobs/{{ cronjob_name}}
```
# [Monitorization of cronjobs](https://medium.com/@tristan_96324/prometheus-k8s-cronjob-alerts-94bee7b90511)
Alerting of traditional Unix cronjobs meant sending an email if the job failed.
Most job scheduling systems that have followed have provided the same
experience, Kubernetes does not. One approach to alerting jobs is to
use the Prometheus push gateway, allowing us to push richer metrics than the
success/failure status. This approach has itâ€™s downsides; we have to update the code
for our jobs, we also have to explicitly configure a push gateway location and
update it if it changes (a burden alleviated by the pull based metrics for long
lived workloads). You can use tools such as Sentry, but it will also require
changes to the jobs.
Jobs are powerful things allowing us to implement several different workflows,
the combination of options can be overwhelming compared to a traditional Unix
cron job. This variety makes it difficult to establish one simple rule for
alerting failed jobs. Things get easier if we restrict ourselves to a
subset of possible options. We will focus on non-concurrent jobs.
The relationship between cronjobs and jobs makes the task ahead difficult. To
make our life easier we will put one requirement on the jobs we create, they
will have to include a label that associates them with the original cronjob.
Below we present an example of our ideal cronjob (which matches what the [helm
chart](https://github.com/bambash/helm-cronjobs) deploys):
```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: our-task
spec:
  schedule: "*/5 * * * *"
  successfulJobsHistoryLimit: 3
  concurrencyPolicy: Forbid
  jobTemplate:
    metadata:
      labels:
        cron: our-task #  0
    for: 1m
    annotations:
      description: '{{ $labels.cronjob }} last run has failed {{ $value }} times.'
```
We use the `kube_cronjob_labels` here to merge in labels from the original
cronjob.