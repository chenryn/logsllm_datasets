Our interviews suggest that training and certification are particularly important when
lives are at stake. For example, Mike Doherty described how lifeguards complete a
rigorous training certification, in addition to a periodic recertification process. Cour‐
ses include fitness components (e.g., a lifeguard must be able to hold someone heavier
than themselves with shoulders out of the water), technical components like first aid
and CPR, and operational elements (e.g., if a lifeguard enters the water, how do other
team members respond?). Every facility also has site-specific training, because life‐
guarding in a pool is markedly different from lifeguarding on a lakeside beach or on
the ocean.
Focus on Detailed Requirements Gathering and Design
Some of the engineers we interviewed discussed the importance of detailed require‐
ments gathering and design docs. This practice was particularly important when
working with medical devices. In many of these cases, actual use or maintenance of
the equipment doesn’t fall within the purview of product designers. Thus, usage and
maintenance requirements must be gathered from other sources.
For example, according to Erik Gross, laser eye surgery machines are designed to be
as foolproof as possible. Thus, soliciting requirements from the surgeons who
actually use these machines and the technicians responsible for maintaining them is
particularly important. In another example, former defense contractor Peter Dahl
described a very detailed design culture in which creating a new defense system com‐
monly entailed an entire year of design, followed by just three weeks of writing the
code to actualize the design. Both of these examples are markedly different from
Google’s launch and iterate culture, which promotes a much faster rate of change at a
calculated risk. Other industries (e.g., the medical industry and the military, as previ‐
ously discussed) have very different pressures, risk appetites, and requirements, and
their processes are very much informed by these circumstances.
464 | Chapter 33: Lessons Learned from Other Industries
Defense in Depth and Breadth
In the nuclear power industry, defense in depth is a key element to preparedness
[IAEA12]. Nuclear reactors feature redundancy on all systems and implement a
design methodology that mandates fallback systems behind primary systems in case
of failure. The system is designed with multiple layers of protection, including a final
physical barrier to radioactive release around the plant itself. Defense in depth is par‐
ticularly important in the nuclear industry due to the zero tolerance for failures and
incidents.
Postmortem Culture
Corrective and preventative action (CAPA)4 is a well-known concept for improving
reliability that focuses on the systematic investigation of root causes of identified
issues or risks in order to prevent recurrence. This principle is embodied by SRE’s
strong culture of blameless postmortems. When something goes wrong (and given
the scale, complexity, and rapid rate of change at Google, something inevitably will go
wrong), it’s important to evaluate all of the following:
• What happened
• The effectiveness of the response
• What we would do differently next time
• What actions will be taken to make sure a particular incident doesn’t happen
again
This exercise is undertaken without pointing fingers at any individual. Instead of
assigning blame, it is far more important to figure out what went wrong, and how, as
an organization, we will rally to ensure it doesn’t happen again. Dwelling on who
might have caused the outage is counterproductive. Postmortems are conducted after
incidents and published across SRE teams so that all can benefit from the lessons
learned.
Our interviews uncovered that many industries perform a version of the postmortem
(although many do not use this specific moniker, for obvious reasons). The motiva‐
tion behind these exercises appears to be the main differentiator among industry
practices.
Many industries are heavily regulated and are held accountable by specific govern‐
ment authorities when something goes wrong. Such regulation is especially ingrained
when the stakes of failure are high (e.g., lives are at stake). Relevant government agen‐
4 https://en.wikipedia.org/wiki/Corrective_and_preventive_action
Postmortem Culture | 465
cies include the FCC (telecommunications), FAA (aviation), OSHA (the manufactur‐
ing and chemical industries), FDA (medical devices), and the various National
Competent Authorities in the EU.5 The nuclear power and transportation industries
are also heavily regulated.
Safety considerations are another motivating factor behind postmortems. In the man‐
ufacturing and chemical industries, the risk of injury or death is ever-present due to
the nature of the conditions required to produce the final product (high temperature,
pressure, toxicity, and corrosivity, to name a few). For example, Alcoa features a note‐
worthy safety culture. Former CEO Paul O’Neill required staff to notify him within 24
hours of any injury that lost a worker day. He even distributed his home phone num‐
ber to workers on the factory floor so that they could personally alert him to safety
concerns.6
The stakes are so high in the manufacturing and chemical industries that even “near
misses”—when a given event could have caused serious harm, but did not—are care‐
fully scrutinized. These scenarios function as a type of preemptive postmortem.
According to VM Brasseur in a talk given at YAPC NA 2015, “There are multiple near
misses in just about every disaster and business crisis, and typically they’re ignored at
the time they occur. Latent error, plus an enabling condition, equals things not work‐
ing quite the way you planned” [Bra15]. Near misses are effectively disasters waiting
to happen. For example, scenarios in which a worker doesn’t follow the standard
operating procedure, an employee jumps out of the way at the last second to avoid a
splash, or a spill on the staircase isn’t cleaned up, all represent near misses and oppor‐
tunities to learn and improve. Next time, the employee and the company might not
be so lucky. The United Kingdom’s CHIRP (Confidential Reporting Programme for
Aviation and Maritime) seeks to raise awareness about such incidents across the
industry by providing a central reporting point where aviation and maritime person‐
nel can report near misses confidentially. Reports and analyses of these near misses
are then published in periodic newsletters.
Lifeguarding has a deeply embedded culture of post-incident analysis and action
planning. Mike Doherty quips, “If a lifeguard’s feet go in the water, there will be
paperwork!” A detailed write-up is required after any incident at the pool or on the
beach. In the case of serious incidents, the team collectively examines the incident
end to end, discussing what went right and what went wrong. Operational changes
are then made based on these findings, and training is often scheduled to help people
build confidence around their ability to handle a similar incident in the future. In
cases of particularly shocking or traumatic incidents, a counselor is brought on site to
help staff cope with the psychological aftermath. The lifeguards may have been well
5 https://en.wikipedia.org/wiki/Competent_authority
6 http://ehstoday.com/safety/nsc-2013-oneill-exemplifies-safety-leadership.
466 | Chapter 33: Lessons Learned from Other Industries
prepared for what happened in practice, but might feel like they haven’t done an ade‐
quate job. Similar to Google, lifeguarding embraces a culture of blameless incident
analysis. Incidents are chaotic, and many factors contribute to any given incident. In
this field, it’s not helpful to place blame on a single individual.
Automating Away Repetitive Work and Operational
Overhead
At their core, Google’s Site Reliability Engineers are software engineers with a low tol‐
erance for repetitive reactive work. It is strongly ingrained in our culture to avoid
repeating an operation that doesn’t add value to a service. If a task can be automated
away, why would you run a system on repetitive work that is of low value? Automa‐
tion lowers operational overhead and frees up time for our engineers to proactively
assess and improve the services they support.
The industries that we surveyed were mixed in terms of if, how, and why they
embraced automation. Certain industries trusted humans more than machines. Dur‐
ing the tenure of our industry veteran, the US nuclear Navy eschewed automation in
favor of a series of interlocks and administrative procedures. For example, according
to Jeff Stevenson, operating a valve required an operator, a supervisor, and a crew
member on the phone with the engineering watch officer tasked with monitoring the
response to the action taken. These operations were very manual due to concern that
an automated system might not spot a problem that a human would definitely notice.
Operations on a submarine are ruled by a trusted human decision chain—a series of
people, rather than one individual. The nuclear Navy was also concerned that auto‐
mation and computers move so rapidly that they are all too capable of committing a
large, irreparable mistake. When you are dealing with nuclear reactors, a slow and
steady methodical approach is more important than accomplishing a task quickly.
According to John Li, the proprietary trading industry has become increasingly cau‐
tious in its application of automation in recent years. Experience has shown that
incorrectly configured automation can inflict significant damage and incur a great
deal of financial loss in a very short period of time. For example, in 2012 Knight Cap‐
ital Group encountered a “software glitch” that led to a loss of $440M in just a few
hours.7 Similarly, in 2010 the US stock market experienced a Flash Crash that was
ultimately blamed on a rogue trader attempting to manipulate the market with auto‐
mated means. While the market was quick to recover, the Flash Crash resulted in a
7 See “FACTS, Section B” for the discussion of Knight and Power Peg software in [Sec13].
Automating Away Repetitive Work and Operational Overhead | 467
loss on the magnitude of trillions of dollars in just 30 minutes.8 Computers can exe‐
cute tasks very quickly, and speed can be a negative if these tasks are configured
incorrectly.
In contrast, some companies embrace automation precisely because computers act
more quickly than people. According to Eddie Kennedy, efficiency and monetary sav‐
ings are key in the manufacturing industry, and automation provides a means to
accomplish tasks more efficiently and cost-effectively. Furthermore, automation is
generally more reliable and repeatable than work conducted manually by humans,
which means that it produces higher-quality standards and tighter tolerances. Dan
Sheridan discussed automation as deployed in the UK nuclear industry. Here, a rule
of thumb dictates that if a plant is required to respond to a given situation in less than
30 minutes, that response must be automated.
In Matt Toia’s experience, the aviation industry applies automation selectively. For
example, operational failover is performed automatically, but when it comes to cer‐
tain other tasks, the industry trusts automation only when it’s verified by a human.
While the industry employs a good deal of automatic monitoring, actual air-traffic–
control-system implementations must be manually inspected by humans.
According to Erik Gross, automation has been quite effective in reducing user error
in laser eye surgery. Before LASIK surgery is performed, the doctor measures the
patient using a refractive eye test. Originally, the doctor would type in the numbers
and press a button, and the laser would go to work correcting the patient’s vision.
However, data entry errors could be a big issue. This process also entailed the possi‐
bility of mixing up patient data or jumbling numbers for the left and right eye.
Automation now greatly lessens the chance that humans make a mistake that impacts
someone’s vision. A computerized sanity check of manually entered data was the first
major automated improvement: if a human operator inputs measurements outside an
expected range, automation promptly and prominently flags this case as unusual.
Other automated improvements followed this development: now the iris is photo‐
graphed during the preliminary refractive eye test. When it’s time to perform the sur‐
gery, the iris of the patient is automatically matched to the iris in the photo, thus
eliminating the possibility of mixing up patient data. When this automated solution
was implemented, an entire class of medical errors disappeared.
8 “Regulators blame computer algorithm for stock market ‘flash crash’,” Computerworld, http://www.computer
world.com/article/2516076/financial-it/regulators-blame-computer-algorithm-for-stock-market—flash-
crash-.html.
468 | Chapter 33: Lessons Learned from Other Industries
Structured and Rational Decision Making
At Google in general, and in Site Reliability Engineering in particular, data is critical.
The team aspires to make decisions in a structured and rational way by ensuring that:
• The basis for the decision is agreed upon advance, rather than justified ex post
facto
• The inputs to the decision are clear
• Any assumptions are explicitly stated
• Data-driven decisions win over decisions based on feelings, hunches, or the opin‐
ion of the most senior employee in the room
Google SRE operates under the baseline assumption that everyone on the team:
• Has the best interests of a service’s users at heart
• Can figure out how to proceed based on the data available
Decisions should be informed rather than prescriptive, and are made without defer‐
ence to personal opinions—even that of the most-senior person in the room, who
Eric Schmidt and Jonathan Rosenberg dub the “HiPPO,” for “Highest-Paid Person’s
Opinion” [Sch14].
Decision making in different industries varies widely. We learned that some indus‐
tries use an approach of if it ain’t broke, don’t fix it…ever. Industries featuring systems
whose design entailed much thought and effort are often characterized by a reluc‐
tance to change the underlying technology. For example, the telecom industry still
uses long-distance switches that were implemented in the 1980s. Why do they rely on
technology developed a few decades ago? These switches “are pretty much bulletproof
and massively redundant,” according to Gus Hartmann. As reported by Dan Sheri‐
dan, the nuclear industry is similarly slow to change. All decisions are underpinned
by the thought: if it works now, don’t change it.
Many industries heavily focus on playbooks and procedures rather than open-ended
problem solving. Every humanly conceivable scenario is captured in a checklist or in
“the binder.” When something goes wrong, this resource is the authoritative source
for how to react. This prescriptive approach works for industries that evolve and
develop relatively slowly, because the scenarios of what could go wrong are not con‐
stantly evolving due to system updates or changes. This approach is also common in
industries in which the skill level of the workers may be limited, and the best way to
make sure that people will respond appropriately in an emergency is to provide a
simple, clear set of instructions.
Structured and Rational Decision Making | 469
Other industries also take a clear, data-driven approach to decision making. In Eddie
Kennedy’s experience, research and manufacturing environments are characterized
by a rigorous experimentation culture that relies heavily on formulating and testing
hypotheses. These industries regularly conduct controlled experiments to make sure
that a given change yields the expected result at a statistically significant level and that
nothing unexpected occurs. Changes are only implemented when data yielded by the
experiment supports the decision.
Finally, some industries, like proprietary trading, divide decision making to better
manage risk. According to John Li, this industry features an enforcement team sepa‐
rate from the traders to ensure that undue risks aren’t taken in pursuit of achieving a
profit. The enforcement team is responsible for monitoring events on the floor and
halting trading if events spin out of hand. If a system abnormality occurs, the
enforcement team’s first response is to shut down the system. As put by John Li, “If
we aren’t trading, we aren’t losing money. We aren’t making money either, but at least
we aren’t losing money.” Only the enforcement team can bring the system back up,
despite how excruciating a delay might seem to traders who are missing a potentially
profitable opportunity.
Conclusions
Many of the principles that are core to Site Reliability Engineering at Google are evi‐
dent across a wide range of industries. The lessons already learned by well-established
industries likely inspired some of the practices in use at Google today.
A main takeaway of our cross-industry survey was that in many parts of its software
business, Google has a higher appetite for velocity than players in most other indus‐
tries. The ability to move or change quickly must be weighed against the differing
implications of a failure. In the nuclear, aviation, or medical industries, for example,
people could be injured or even die in the event of an outage or failure. When the
stakes are high, a conservative approach to achieving high reliability is warranted.
At Google, we constantly walk a tightrope between user expectations for high reliabil‐
ity versus a laser-sharp focus on rapid change and innovation. While Google is
incredibly serious about reliability, we must adapt our approaches to our high rate of
change. As discussed in earlier chapters, many of our software businesses such as
Search make conscious decisions as to how reliable “reliable enough” really is.
470 | Chapter 33: Lessons Learned from Other Industries
Google has that flexibility in most of our software products and services, which oper‐
ate in an environment in which lives are not directly at risk if something goes wrong.
Therefore, we’re able to use tools such as error budgets (“Motivation for Error Budg‐
ets” on page 33) as a means to “fund” a culture of innovation and calculated risk tak‐
ing. In essence, Google has adapted known reliability principles that were in many
cases developed and honed in other industries to create its own unique reliability cul‐
ture, one that addresses a complicated equation that balances scale, complexity, and
velocity with high reliability.
Conclusions | 471
CHAPTER 34
Conclusion
Written by Benjamin Lutch1
Edited by Betsy Beyer
I read through this book with enormous pride. From the time I began working at
Excite in the early ’90s, where my group was a sort of neanderthal SRE group dubbed
“Software Operations,” I’ve spent my career fumbling through the process of building
systems. In light of my experiences over the years in the tech industry, it’s amazing to
see how the idea of SRE took root at Google and evolved so quickly. SRE has grown
from a few hundred engineers when I joined Google in 2006 to over 1,000 people
today, spread over a dozen sites and running what I think is the most interesting
computing infrastructure on the planet.
So what has enabled the SRE organization at Google to evolve over the past decade to
maintain this massive infrastructure in an intelligent, efficient, and scalable way? I
think that the key to the overwhelming success of SRE is the nature of the principles
by which it operates.
SRE teams are constructed so that our engineers divide their time between two
equally important types of work. SREs staff on-call shifts, which entail putting our
hands around the systems, observing where and how these systems break, and under‐
standing challenges such as how to best scale them. But we also have time to then
reflect and decide what to build in order to make those systems easier to manage. In