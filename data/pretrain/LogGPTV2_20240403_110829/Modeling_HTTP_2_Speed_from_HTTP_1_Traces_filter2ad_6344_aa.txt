title:Modeling HTTP/2 Speed from HTTP/1 Traces
author:Kyriakos Zarifis and
Mark Holland and
Manish Jain and
Ethan Katz-Bassett and
Ramesh Govindan
Modeling HTTP/2 Speed from HTTP/1 Traces
Kyriakos Zariﬁs1(B), Mark Holland2, Manish Jain2, Ethan Katz-Bassett1,
and Ramesh Govindan1
1 University of Southern California, Los Angeles, USA
PI:EMAIL
2 Akamai Technologies, Cambridge, USA
Abstract. With the standardization of HTTP/2, content providerswant
to understand the beneﬁts and pitfalls of transitioning to the new stan-
dard. Using a large dataset of HTTP/1.1 resource timing data from pro-
duction traﬃc on Akamai’s CDN, and a model of HTTP/2 behavior, we
obtain the distribution of performance diﬀerences between the protocol
versions for nearly 280,000 downloads. We ﬁnd that HTTP/2 provides
signiﬁcant performanceimprovements in the tail, and, for websites for
which HTTP/2 does not improve median performance, we explore how
optimizations like prioritization and push can improve performance, and
how these improvements relate to page structure.
1 Introduction
HTTP/2 will soon supplant HTTP/1.1 as the IETF standard for the delivery
of web traﬃc and is already supported by major browsers and some content
providers [12]. The design of HTTP/2 has been motivated by concerns about
the performance of HTTP/1.1. The aspect of web performance most relevant
to end-users is page load time (PLT), which has been shown to correlate with
content provider revenue, so content providers have gone to great lengths to
optimize it. HTTP/2 is a step in that direction: it multiplexes objects on a
single TCP connection, permits clients to specify priorities, and allows servers
to push content speculatively.
Several prior studies have shown mixed results on the performance diﬀerence
between HTTP/1.1 and HTTP/2 [7,11,14]. The relative performance of these
two protocols has been hard to assess because modern web pages have complex
dependencies between objects, and can contain objects hosted on diﬀerent sites.
Many of these prior studies are focused on lab environments, and some have not
used real browsers as test agents, which can restrict visibility into browser-side
tasks like resource parsing, execution or rendering time.
This has motivated us to study the performance of HTTP/2 using data
collected from live page views by real end-users. Our study uses HTTP/1.1
Resource Timing [2] data collected from a broad set of customers on a major
CDN (Akamai). The data we collect consists of detailed timing breakdowns for
the base page and each embedded resource on a small sample of all page views.
c(cid:2) Springer International Publishing Switzerland 2016
T. Karagiannis and X. Dimitropoulos (Eds.): PAM 2016, LNCS 9631, pp. 233–247, 2016.
DOI: 10.1007/978-3-319-30505-9 18
234
K. Zariﬁs et al.
Contributions. The ﬁrst contribution of this paper is a model, called rt-h2,
that takes the resource timing data for a single HTTP/1.1 page view, and esti-
mates the diﬀerence in page load times for that page view between HTTP/1.1
and HTTP/2. To do this, rt-h2 models four important components of HTTP/2:
multiplexing, push, prioritization, and frame interleaving. rt-h2 also contains a
model of TCP that is reasonably accurate for Web transfers.
Our second contribution is to explore the PLT diﬀerences between HTTP/1.1
and HTTP/2 from nearly 280,000 page views of customers of Akamai. Of these,
we select 55 distinct websites which have a signiﬁcant number of instrumented
page views and explore the relative performance under zero packet loss. In this
setting, page structure and the diversity of the client base (in terms of loca-
tion, browser type, etc.) should determine performance. We found that roughly
60 % of the time HTTP/2 has smaller PLT and 28 % of the time it has negli-
gible impact, but there are websites for which more often than others it leads
to performance degradation. We explored two optimizations, prioritization and
push. Push provided more improvement for cases where HTTP/2 was already
beneﬁcial, and both helped the cases that saw degradation with HTTP/2.
Taken together, our ﬁndings indicate that CDNs should start experimenting
with HTTP/2 at scale, as it can have beneﬁts for many clients of their customers.
2 Background and Approach
A typical web page consists of tens of resources fetched from many diﬀerent
servers. Many of these objects have dependencies between them. A base page
HTML ﬁle is downloaded before sub-resources can be requested. Once sub-
resources are downloaded and parsed, they can trigger the downloads of other
resources. The user-perceived latency in loading a web page is a complex combi-
nation of the time taken to download, parse and render (if needed) its resources.
HTTP/1.1. The original HTTP/1.0 speciﬁcation only allowed for one response-
request stream to be transferred per TCP connection. HTTP/1.1 added the
ability to re-use connections, but required that there be only one request in
ﬂight on a TCP channel at a time. It also added pipelining, which is rarely
employed [1], so we ignore it in this paper.
Subsequent optimizations enabled parallel downloads by opening multiple
concurrent connections to the server. Browsers typically limit themselves to six
parallel connections per hostname. To leverage this to achieve faster downloads,
domain sharding, is used to partition objects across diﬀerent hostnames.
HTTP/2. HTTP/2 allows multiple, concurrent requests to be outstanding on
the same TCP connection. This prevents the case where a resource that the
browser is ready to load is forced to wait for an idle connection. It also allows for
explicit prioritization of the delivery of resources. For example, when a server
has received a request for both an image and a Javascript object, and it has both
ready to deliver, the protocol allows (but does not mandate) that the Javascript
be given priority for the connection. This prioritization facilitates parallelization
Modeling HTTP/2 Speed from HTTP/1 Traces
235
of client processing and downloading. It also provides a mechanism for a server to
push content to a client without receiving a request from it. While the standard
does not specify best practices for pushing objects, the intent of this mechanism
is to enable servers to keep the pipe to the client as busy as possible.
Page Load Time. Both HTTP/1.1 and HTTP/2 contain performance opti-
mizations whose goal is to reduce PLT. Recent web performance studies have
converged upon an operational deﬁnition of PLT [10], which is when the browser
ﬁres the onLoad event.
Understanding Relative Performance: Challenges. HTTP/2 contains sev-
eral optimizations that should result in better performance than HTTP/1.*, but
these performance beneﬁts may not always be realized in practice. First, while
mechanisms for prioritization and push are deﬁned in the standard, actual per-
formance improvements may depend upon the speciﬁc policies that Web servers
implement for these optimizations. Second, interactions with TCP can limit the
performance advantages of HTTP/2. Compared with when objects are retrieved
over parallel connections, the congestion window on a single multiplexed chan-
nel grows more slowly. Moreover, parallel connections are more forgiving of loss:
when a drop occurs in a stream, it will only trigger recovery on that stream.
Our Approach. We use Resource Timing [2] data collected using Javascript
from a broad set of customers on Akamai’s CDN. When enabled by a customer,
Akamai servers insert a small body of Javascript into 1 % of this customer’s
pages as they are delivered to end users. The script triggers the monitoring of
per-resource timing information, which includes the start/end timestamps for:
DNS lookup, TCP connection setup, TLS handshake if any, request sent to the
server, and start and end from response from the server [2]. The script then
encodes that information into a trie structure and delivers it to an Akamai
back-end system. Over a selected one-week period we observed data for about
44,000 distinct base-page hostnames and 3.4 million distinct base-page URLs.
Unlike prior work [7,11,14], our data consists of detailed timing breakdowns
for the base page and each embedded resource from real clients. From this infor-
mation, we obtain realistic network delays and browser side processing and ren-
dering delays for the complete set of resources in a page, and are able to assess
PLTs as reported by browser onLoad events.
However, our dataset captures HTTP/1.* downloads, so the primary chal-
lenge we face in the paper is how to predict the page load performance for this
dataset under HTTP/2. Using a real HTTP/2 deployment on the CDN is, at
the moment, not an option, because of the complexity and scale of the endeavor.
So, we resort to using a model of HTTP/2, as described in the next section.
3 The RT-H2 Model
Input. The input to rt-h2 is the Resource Timing (RT) data for a single
HTTP/1.1 download of a website from a real client. The input can be visu-
alized as a waterfall. Figure 1 (left) illustrates a simpliﬁed waterfall for a page
236
K. Zariﬁs et al.
Fig. 1. Transformation of an HTTP/1.1 waterfall to an HTTP/2 waterfall (Color ﬁgure
online).
downloaded via HTTP/1.1, containing seven objects: a base HTML page, one
CSS ﬁle, 3 Javascript ﬁles and 3 images. The HTML ﬁle is downloaded ﬁrst and
it is parsed as it is being downloaded. So, even before 1.html completes, the
browser has determined that 2.css, 3.js and 4.png need to be downloaded
next. These three resources depend on 1.html, and that HTML page is said to
be a parent of these resources. However, not all of these resources can be imme-
diately downloaded: most browsers limit the number of parallel connections to
a given website, and Fig. 1 (left) shows a simpliﬁed example with at most two
parallel connections. Therefore, only the download of 2.css is initiated, and
other objects are blocked until 1.html has downloaded.
The waterfall diagram also illustrates three other important features that can
be gleaned from RT data. First, when 3.js completes, it triggers the download
of 5.js. The time between the completion of 3.js’s download and the request
for 5.js represents the processing time for 3.js. The processing time for 5.js
is also visible since that Javascript triggers the downloads of two images 6.png
and 7.png. Second, 5.js is an example of 3rd-party content (3PC). Examples of
3PC include ads, tags, analytics, and external JavaScript ﬁles that can trigger
the download of other 3PC or origin content. Finally, the dashed line in Fig. 1
runs through objects that represent the critical path in the waterfall. The critical
path of the waterfall demarcates objects whose download and processing times
determine the PLT. The browser’s OnLoad() event is triggered after 6.png is
downloaded, so 6.png and its ancestors are on the critical path.
The waterfall also explicitly contains four kinds of download timing infor-
mation. The blue boxes (Request), mark the time from when the object was
requested by the client to the time when the ﬁrst byte of the object was received.
The red boxes (Download), mark the time from when the ﬁrst byte of the object
was received by the browser, to when the last byte of the object was received.
The gray boxes (Blocked) represent the duration of time between when an object
could have been retrieved and when the request was actually made. The green
boxes (3PC ) demarcate the retrieval of third-party content.
Real waterfalls have the same elements as in Fig. 1, but can be signiﬁcantly
more complex, with hundreds of objects, several levels of dependencies, and
multiple sources of third-party content.
Output. The output of rt-h2 is a transformed version of the input waterfall
produced by applying the features of HTTP/2 on a real HTTP/1.1 waterfall
and the percentage change in PLT, which we denote ΔP LT .
Modeling HTTP/2 Speed from HTTP/1 Traces
237
Fig. 2. rt-h2 Components
Components of RT-H2. Both HTTP protocol versions are complex, and
HTTP/2 contains many optional features with unspeciﬁed policies or best prac-
tices. rt-h2 is designed to be able to explore what-if scenarios of diﬀerent com-
binations of policies or optional features. It models HTTP/2 in a layered fashion
and has several components, as shown in Fig. 2. We describe each of these below.
Multiplexing. In HTTP/2, a client maintains a single TCP channel with any
one server, on which resources are multiplexed. rt-h2’s multiplexing compo-
nent, which operates at the object level, analyzes the input waterfall to deter-
mine which objects can be multiplexed. With HTTP/2, any resource with a
URI covered by the certiﬁcate of the origin can reuse the same channel. rt-h2
parses resource URLs and looks for patterns resembling the base page URL.
Because strict string matching does not cover all the cases (www.example.com
and img.xmpl.com can in fact be the same origin), we also assume that any
hostname that serves more than 5 resources in the same download must be ori-
gin content. The output of the multiplexing component is a collection of sets of
objects that can be multiplexed by HTTP/2 because they come from the same
server.
Browser-cached resources are not included in the multiplexer’s output. RT
data does not explicitly mark cached resources, so rt-h2 determines an object
is cached if its retrieval time in the original waterfall is less than 10 ms. For
cached objects, rt-h2 preserves the timing from the original waterfall. rt-h2 also
preserves the duration of 3PC resources which are not on the origin connection.
Push. This component emulates the ability of an HTTP/2 server to proactively
send resources to a client without the client having to request them. Push can
keep the pipe to the client full. For example, in Fig. 1 (left), 2.css, 3.js and
4.png could be served by an HTTP/2 server as soon as 1.html is requested,
rather than waiting for the client to request them.