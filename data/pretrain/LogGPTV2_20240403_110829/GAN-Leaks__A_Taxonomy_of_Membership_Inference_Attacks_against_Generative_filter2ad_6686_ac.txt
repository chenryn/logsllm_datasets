### Optimizing GAN Performance and Data Preprocessing

To enhance GAN performance, we select a maximum of 20,000 images, center-crop them, and resize them to 64 × 64 pixels before initiating the GAN training.

#### MIMIC-III Dataset
MIMIC-III [34] is a publicly available Electronic Health Records (EHR) database containing medical records of 46,520 intensive care unit (ICU) patients. We follow the preprocessing steps outlined in [12], where each patient is represented by a 1,071-dimensional binary feature vector. After filtering out patients with repeated vector representations, we obtain 41,307 unique samples.

#### Instagram New-York Dataset
The Instagram New-York [6] dataset includes check-ins at various locations in New York from 2013 to 2017. We filter out users with fewer than 100 check-ins, resulting in 34,336 remaining samples. For sample representation, we first select 2,024 evenly-distributed time stamps. We then concatenate the longitude and latitude values of the check-in location at each time stamp, yielding a 4,048-dimensional vector for each sample. The longitude and latitude values are either retrieved from the dataset or linearly interpolated from the available neighboring time stamps. We perform zero-mean normalization before GAN training.

### Victim GAN Models
We select the following GAN models as victim models due to their strong performance in generating images and other data representations:
- PGGAN [36]
- WGANGP [23]
- DCGAN [54]
- MEDGAN [12]
- VAEGAN [8]

Ensuring high-quality, well-trained GANs is crucial because attackers are more likely to target such models for practical effectiveness. Previous works [25, 29] only provided qualitative results for their victim GANs. In contrast, Hayes et al. [25] did not show visually pleasing generated results on the Labeled Faces in the Wild (LFW) dataset [31]. We present better qualitative results for different GANs on CelebA (Figure 4) and provide quantitative evaluations using the Fréchet Inception Distance (FID) metric (Table 3). A smaller FID indicates that the generated image set is more realistic and closer to real-world data distribution.

### Attack Evaluation
The proposed membership inference attack is formulated as a binary classification problem with a threshold \(\epsilon\) (Equation 14). By varying \(\epsilon\), we measure the area under the receiver operating characteristic curve (AUC-ROC) to evaluate the attack performance.

### Analysis Study
We analyze the attack performance across two dimensions: GAN training set size and random vs. identity-based selection for the GAN training set. Additional dimensions specific to the white-box attack are detailed in Section 6.5.

#### 6.2.1 GAN Training Set Size
The training set size significantly affects the degree of overfitting in GAN training. Smaller training sets make GANs more prone to memorizing individual training images, increasing vulnerability to membership inference attacks. Additionally, the training set size impacts the privacy cost computation for differential privacy. We exclude DCGAN and VAEGAN from this evaluation due to their unstable training with small datasets.

#### 6.2.2 Random vs. Identity-Based Selection for GAN Training Set
Different levels of difficulty exist for membership inference attacks. For example, CelebA contains person identity information, allowing us to design attack difficulty by composing the GAN training set based on identity or randomly. In one case, all images of selected individuals are included (identity-based). In the other case, images are randomly selected, potentially including some but not all images of an individual (random-based). The identity-based case is easier for attackers, as it provides a larger margin between membership and non-membership image sets. We evaluate both schemes on CelebA for a comprehensive and fair comparison, in line with previous work [25].

### Full Black-box Attack Evaluation
We start by evaluating our preliminary low-skill black-box attack model to gauge the overall difficulty of the problem.

#### 6.3.1 Performance w.r.t. GAN Training Set Size
Figures 5(a) to 5(c) plot the attack performance against different GAN models on the three datasets. The attack performs well when the training set is small, as seen in CelebA, where the AUC-ROC for both PGGAN and WGANGP exceeds 0.95 with up to 512 images. As the training set size increases, the attacks become less effective due to reduced overfitting and increased generalization. PGGAN becomes more vulnerable than WGANGP on CelebA with larger training sizes, while WGANGP remains consistently more vulnerable than MEDGAN on MIMIC-III, regardless of the training size.

#### 6.3.2 Performance w.r.t. GAN Training Set Selection
Figure 6(a) shows the attack performance with respect to training set selection schemes on four victim GAN models. All GAN models are more vulnerable when the training set is selected based on identity. DCGAN and VAEGAN are more resistant, with AUC-ROC values only marginally above 0.5 (random guess baseline), possibly due to their poor generation quality.

### Partial Black-box Attack Evaluation
#### 6.4.1 Performance w.r.t. GAN Training Set Selection
Figure 6(b) compares the attack performance on four victim GAN models. Similar to the full black-box attack, all models are more vulnerable to identity-based selection. DCGAN remains the most resistant, likely due to its inferior generation quality.

#### 6.4.2 Comparison to Full Black-box Attack
Comparing Figures 6(a) and 6(b), the attack performance improves significantly from the full black-box to the partial black-box setting. This improvement is attributed to better reconstruction of query samples via optimization, indicating that providing the input interface to a generator increases privacy risk.

### White-box Attack Evaluation
We further investigate the scenario where the victim generator is published in a white-box manner, commonly studied in privacy-preserving data generation [2, 7, 11, 35, 66, 73].

#### 6.5.1 Performance w.r.t. GAN Training Set Size
Figures 5(d) to 5(f) plot the attack performance against different GAN models on the three datasets with varying training set sizes. The attack becomes less effective as the training set size increases, similar to the black-box setting. For CelebA, the attack remains effective with 20,000 training samples, while for MIMIC-III and Instagram, this number decreases to 8,192 and 2,048, respectively. The strong similarity between member and non-member samples in these non-image datasets increases the attack difficulty.

#### 6.5.2 Performance w.r.t. GAN Training Set Selection
Figure 6(c) shows comparisons against four victim GAN models. Our attack is more effective when the GAN training set is composed according to identity, similar to the full and partial black-box settings.

#### 6.5.3 Comparison to Full and Partial Black-box Attacks
For membership inference attacks, it is crucial to determine whether the white-box attack is more effective than black-box ones. Against generative models, the white-box attack is much more effective, with AUC-ROC values increasing by at least 0.03 when changing from full black-box to white-box. Compared to the partial black-box attack, the white-box attack achieves better performance against PGGAN and VAEGAN, with lower computational costs. Therefore, publicizing model parameters (white-box setting) incurs a high privacy breach risk.

### Performance Gain from Attack Calibration
We perform calibration on all settings. For full and partial black-box settings, we use a PGGAN trained on the LFW face dataset [31] as the generic reference model for calibrating all victim models trained on CelebA. Similarly, for MIMIC-III, we use WGANGP as the reference model for MedGAN and vice versa.

#### 6.6.1 Calibration Results
Figures 7 and 8 compare attack performance before and after applying calibration. The AUC-ROC values improve consistently across all GAN architectures and settings. The white-box attack calibration yields the greatest performance gain, especially significant for VAEGAN, with an AUC-ROC increase of 0.2 after calibration.

### Comparison to Baseline Attacks
We compare our calibrated attack to two recent membership inference attack baselines: Hayes et al. [25] (LOGAN) and Hilprecht et al. [29] (MC, Monte Carlo sampling method).

#### 6.7.1 Comparison Results
Figures 9, 10, and 11 show the comparisons across several datasets, victim models, training set sizes, and numbers of query images. Our low-skill attack consistently outperforms MC and LOGAN on non-image datasets and achieves comparable performance to LOGAN on CelebA with a simpler, learning-free implementation. Our white-box and partial black-box attacks outperform the other full black-box attacks, highlighting the high privacy breach risk of publicizing the generator or even just the input to the generator.

### Defense Mechanism
We investigate the most effective defense mechanism against MIA applicable to GANs: differential private (DP) stochastic gradient descent [1]. This involves clipping the per-sample gradient by its \(L_2\) norm and adding calibrated random noise to inject stochasticity for privacy protection. However, this comes at the cost of increased computational complexity and utility deterioration.

#### 6.8.1 DP Defense Results
Figures 12(a) and 12(b) show the attack performance with and without DP defense. DP consistently reduces the AUC-ROC in all settings but leads to a 10× slower training and higher FID (Table 3). For a pleasing level of utility, the noise scale must be limited, which does not fully defend against the membership inference attack. For all settings, our attack still outperforms the random guess baseline (AUC-ROC = 0.5).

### Summary
In conclusion, the vulnerability of models under MIA heavily relies on the attackers' knowledge about the victim models. Releasing the discriminator (full model) results in a high risk of privacy breach, as the discriminator is explicitly trained to maximize the margin between training and generated sets, leading to accurate confidence scores for membership inference. Our calibrated white-box attack outperforms baseline methods in more knowledgeable settings, emphasizing the need for robust privacy-preserving mechanisms in GAN deployment.