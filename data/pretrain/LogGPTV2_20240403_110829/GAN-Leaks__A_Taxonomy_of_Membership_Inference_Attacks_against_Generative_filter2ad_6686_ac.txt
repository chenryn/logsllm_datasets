which benefits GAN performance. We select at most 20k images,
center-crop them, and resize them to 64 √ó 64 before GAN training.
MIMIC-III [34] is a public Electronic Health Records (EHR) data-
base containing medical records of 46, 520 intensive care unit (ICU)
patients. We follow the same procedure as in [12] to pre-process
the data, where each patient is represented by a 1071-dimensional
binary feature vector. We filter out patients with repeated vector
presentations and yield 41, 307 unique samples.
Instagram New-York [6] contains Instagram users‚Äô check-ins at
various locations in New York at different time stamps from 2013
to 2017. We filter out users with less than 100 check-ins and yield
34, 336 remaining samples. For sample representation, we first select
2, 024 evenly-distributed time stamps. We then concatenate the
longitude and latitude values of the check-in location at each time
stamp, and yield a 4048-dimensional vector for each sample. The
longitude and latitude values are either retrieved from the dataset
or linearly interpolated from the available neighboring time stamps.
We then perform zero-mean normalization before GAN training.
Victim GAN Models: We select PGGAN [36], WGANGP [23], DC-
GAN [54], MEDGAN [12], and VAEGAN [8] into the victim model
set, considering their pleasing performance on generating images
and/or other data representations.
It is important to guarantee the high quality of well-trained
GANs because attackers are more likely to target high-quality GANs
with practical effectiveness. We noticed previous works [25, 29]
only show qualitative results of their victim GANs. In particular,
Hayes et al. [25] did not show visually pleasing generated results on
the Labeled Faces in the Wild (LFW) dataset [31]. Rather, we present
better qualitative results of different GANs on CelebA (Figure 4),
GP
24.26
VAE-
GAN GAN
35.40
53.08
SOTA PGGAN
w/ DP
ref
7.40
15.63
PG- WGAN- DC-
GAN
FID 14.86
Table 3: FID for different GAN models trained on CelebA.
‚ÄúSOTA ref‚Äù represents the state-of-the-art result reported
in [10] over 128 √ó 128 ImageNet ILSVRC 2012 dataset [56].
‚Äúw/ DP‚Äù represents the GAN model with DP privacy protec-
tion [1] (see Section 6.8).
and further present the corresponding quantitative evaluation in
terms of Fr√©chet Inception Distance (FID) metric [28] (Table 3).
A smaller FID indicates the generated image set is more realistic
and closer to real-world data distribution. We show that our GAN
models are in a reasonable range to the state of the art.
Attack Evaluation: The proposed membership inference attack
is formulated as a binary classification given a threshold ùúñ in Equa-
tion 14. Through varying ùúñ, we measure the area under the receiver
operating characteristic curve (AUCROC) to evaluate the attack
performance.
6.2 Analysis Study
We first list two dimensions of analysis study across attack settings.
There are also some other dimensions specifically for the white-box
attack, which are elaborated in Section 6.5.
6.2.1 GAN Training Set Size. Training set size is highly related to
the degree of overfitting of GAN training. A GAN model trained
with a smaller size tends to more easily memorize individual train-
ing images and is thus more vulnerable to membership inference
attack. Moreover, training set size is the main factor that affects
the privacy cost computation for differential privacy. Therefore, we
evaluate the attack performance w.r.t. training set size. We exclude
DCGAN and VAEGAN from evaluation since they yield unstable
training for small training sets.
6.2.2 Random v.s. Identity-based Selection for GAN Training Set.
There are different levels of difficulty for membership inference
attack. For example, CelebA contains person identity information
and we can design attack difficulty by composing GAN training
set based on identity or not. In one case, we include all images
of the selected individuals for training(identity). In the other case,
we ignore identity information and randomly select images for
(a)
(d)
(b)
(e)
(c)
(f)
Figure 5: Full black-box attack (the first row) and white-box attack (the second row) performance w.r.t. GAN training set size.
training(random), i.e., it is possible that some images for an indi-
vidual are in the training dataset while some are not. The former
case is relatively easier to attackers with a larger margin between
membership image set and non-membership image set. In line with
previous work [25], we evaluate these two kinds of training set
selection schemes on CelebA for a complete and fair comparison.
6.3 Evaluation on Full Black-box Attack
We start with evaluating our preliminary low-skill black-box attack
model in order to gain a sense of the difficulty of the whole problem.
6.3.1 Performance w.r.t. GAN Training Set Size. Figure 5(a) to Fig-
ure 5(c) plot the attack performance against different GAN models
on the three datasets. As shown in the plots, the attack performs
sufficiently well when the training set is small for all three datasets.
For instance, on CelebA, when the training set contains up to 512
images, attacker‚Äôs AUCROC on both PGGAN and WGANGP are
above 0.95. This indicates an almost perfect attack and a serious
privacy breach. For larger training sets, however, the attacks be-
come less effective as the degree of overfitting decreases and GAN‚Äôs
capability shifts from memorization to generalization. It is also
consistent with the objective of GAN, i.e., to model the underlying
distribution of the whole population instead of fitting a particular
data sample. Hence, the collection of more data for GAN training
can reduce privacy breach of individual samples. Moreover, PG-
GAN becomes more vulnerable than WGANGP on CelebA when
the training size becomes larger. WGANGP is consistently more
vulnerable than MEDGAN on MIMIC-III regardless of training size.
6.3.2 Performance w.r.t. GAN Training Set Selection. Figure 6(a)
shows the attack performance w.r.t. training set selection schemes
on four victim GAN models when fixing the training set size. We
observe that, consistently, all the GAN models are more vulnerable
when the training set is selected based on identity. Hence, more at-
tention needs to be paid to an identity-based privacy breach, which
is more likely to happen than an instance-based privacy breach.
Moreover, when compared among different victim GAN models,
DCGAN and VAEGAN are more resistant against the full black-box
attack with AUCROC only marginally above 0.5 (random guess
baseline). This may be attributed to the poor generation quality
of DCGAN and VAEGAN (Table 3), as it indicates that a certain
amount of data samples can not be well represented by the victim
model and thus the reconstruction error will be a less accurate
approximation of the true membership probability in Equation 2.
6.4 Evaluation on Partial Black-box Attack
6.4.1 Performance w.r.t. GAN Training Set Selection. Figure 6(b)
shows the comparison on four victim GAN models. Similar to the
case of the full black-box attack (Section 6.3), we find that all models
become more vulnerable to identity-based selection. Still, DCGAN
is the most resistant victim against membership inference in both
training set selection schemes, probably due to its inferior genera-
tion quality.
6.4.2 Comparison to Full Black-box Attack. Comparing between
Figure 6(a) and Figure 6(b), the attack performance against each
GAN model consistently and significantly improves from black-box
6412825651210242048409620ksizeofdataset0.50.60.70.80.91.0AUCROCCelebA(fullblack-box)PGGANWGANGP641282565121024204840968192sizeofdataset0.50.60.70.80.91.0AUCROCMIMIC-III(fullblack-box)WGANGPMEDGAN64128256512102420484096819210ksizeofdataset0.50.60.70.80.91.0AUCROCInstagram(fullblack-box)WGANGP6412825651210242048409620ksizeofdataset0.50.60.70.80.91.0AUCROCCelebA(white-box)PGGANWGANGP641282565121024204840968192sizeofdataset0.50.60.70.80.91.0AUCROCMIMIC-III(white-box)WGANGPMEDGAN64128256512102420484096819210ksizeofdataset0.50.60.70.80.91.0AUCROCInstagram(white-box)WGANGP(a)
(b)
(c)
Figure 6: Attack performance on the random v.s. identity-based training set selection (CelebA with size=20k).
setting to partial black-box setting. We attribute this improvement
to a better reconstruction of query samples found by the attacker via
optimization. Hence, we conclude that providing the input interface
to a generator suffers from an increased privacy risk.
6.5 Evaluation on White-box Attack
We further investigate the case where the victim generator is pub-
lished in a white-box manner. This scenario is commonly studied in
the field of privacy preserving data generation [2, 7, 11, 35, 66, 73],
where our approach can serve as a simple and interpretable frame-
work for empirically quantifying the privacy leakage. As the opti-
mization in the white-box attack involves more technical details, we
conduct additional analysis study and sanity check in this setting.
See Appendix C.1 for more details.
6.5.1 Performance w.r.t. GAN Training Set Size. Figure 5(d) to Fig-
ure 5(f) plot the attack performance against different GAN models
on the three datasets when varying training set size. We find that
the attack becomes less effective as the training set becomes larger,
similar to that in the black-box setting. For CelebA, the attack re-
mains effective for 20k training samples, while for MIMIC-III and
Instagram, this number decreases to 8192 and 2048, respectively.
The strong similarity between the member and non-member in
these two non-image datasets increases the difficulty of attack,
which explains the deteriorated effectiveness of the attack model.
6.5.2 Performance w.r.t. GAN Training Set Selection. Figure 6(c)
shows the comparisons against four victim GAN models. Our attack
is much more effective when composing GAN training set according
to identity, which is similar to those in the full and partial black-box
settings.
6.5.3 Comparison to Full and Partial Black-box Attacks. For mem-
bership inference attack, it is an important question whether or
to what extent the white-box attack is more effective than the
black-box ones. For discriminative (classification) models, recent lit-
erature reports that the state-of-the-art black-box attack performs
almost as well as the white-box attack [51, 57]. In contrast, we
find that against generative models the white-box attack is much
more effective. Comparisons across subfigures in Figure 6 show
that the AUCROC values increase by at least 0.03 when changing
from full black-box to white-box setting. Compared to the partial
black-box attack, the white-box attack achieves noticeably better
performance against PGGAN and VAEGAN. Moreover, conducting
the white-box attack requires much less computation cost than
conducting the partial black-box attack. Therefore, we conclude
that publicizing model parameters (white-box setting) does incur
high privacy breach risk.
6.6 Performance Gain from Attack Calibration
We perform calibration on all the settings. Note that for full and
partial black-box settings, attackers do not have prior knowledge
of victim model architectures. We thus train a PGGAN on LFW
face dataset [31] and use it as the generic reference model for
calibrating all victim models trained on CelebA in the black-box
settings. Similarly, for MIMIC-III, we use WGANGP as the reference
model for MedGAN and vice versa. In other words, we have to
guarantee that our calibrated attacks strictly follow the black-box
assumption.
Figure 7 compares attack performance on CelebA before and
after applying calibration. The AUCROC values are improved con-
sistently across all the GAN architectures in all the settings. In
general, the white-box attack calibration yields the greatest perfor-
mance gain. Moreover, the improvement is especially significant
when attacking against VAEGAN, as the AUCROC value increases
by 0.2 after applying calibration.
Figure 8 compares attack performance on the other two non-
image datasets. The performance is also consistently boosted for
all training set sizes after calibration.
6.7 Comparison to Baseline Attacks
We compare our calibrated attack to two recent membership infer-
ence attack baselines: Hayes et al. [25] (denoted as LOGAN) and
Hilprecht et al. [29] (denoted as MC, standing for their proposed
Monte Carlo sampling method). As described in our taxonomy
(Section 4), LOGAN includes a full black-box attack model and a
discriminator-accessible attack model against GANs. The latter is
regarded as the most knowledgeable but unrealistic setting because
the discriminator in GAN is usually not accessible in practice. But
we still compare to both settings for the completeness of our taxon-
omy and experiments. MC includes a full black-box attack against
GANs and a full-model-accessible attack against VAEs. We evaluate
PGGANWGANGPDCGANVAEGAN0.40.50.6AUCROCCelebA(fullblack-box)randomidentityPGGANWGANGPDCGANVAEGAN0.40.50.6AUCROCCelebA(partialblack-box)randomidentityPGGANWGANGPDCGANVAEGAN0.40.50.6AUCROCCelebA(white-box)randomidentity(a)
(b)
(c)
Figure 7: Attack performance before and after calibration on CelebA (size=20k).
(a)
(b)
(c)
Figure 8: Attack performance before and after calibration for non-image datasets w.r.t. GAN training set sizes.
Figure 9: Comparison of different attacks on CelebA. See Ta-
ble 12 in Appendix for quantitative results.
our generic attack model on both GANs and VAEs for a complete
comparison, though we mainly focus on GANs in this work. Note
that, to the best of our knowledge, there does not exist any attack
against GANs in the partial black-box or white-box settings.
Figure 9, Figure 10, and Figure 11 show the comparisons, consid-
ering several datasets, victim models, training set sizes, numbers
of query images (full black-box), and different attack settings. We
skip MC on the non-image datasets as it is not directly applicable
in terms of their distance calculation. Our findings are as follows.
Figure 10: Full black-box attack performance against PG-
GAN on CelebA w.r.t. ùëò in Equation 5, the number of gen-
erated samples. See Table 14 in Appendix for quantitative
results.
In the black-box setting, our low-skill attack consistently out-
performs MC and outperforms LOGAN on the non-image datasets.
It also achieves comparable performance to LOGAN on CelebA but
with a much simpler and learning-free implementation.
Our white-box and even partial black-box attacks consistently
outperform the other full black-box attacks. Hence, publicizing the
PGGANWGANGPDCGANVAEGAN0.50.60.70.8AUCROCCelebA(fullblack-box)beforecalibrationaftercalibrationPGGANWGANGPDCGANVAEGAN0.50.60.70.8AUCROCCelebA(partialblack-box)beforecalibrationaftercalibrationPGGANWGANGPDCGANVAEGAN0.50.60.70.8AUCROCCelebA(white-box)beforecalibrationaftercalibration641282565121024204840968192sizeofdataset0.50.60.70.80.91.0AUCROCMIMIC-III(WGANGP)fullblack-boxfullblack-boxcalibratedwhite-boxwhite-boxcalibrated641282565121024204840968192sizeofdataset0.50.60.70.80.91.0AUCROCMIMIC-III(MEDGAN)fullblack-boxfullblack-boxcalibratedwhite-boxwhite-boxcalibrated64128256512102420484096819210ksizeofdataset0.50.60.70.80.91.0AUCROCInstagram(WGANGP)fullblack-boxfullblack-boxcalibratedwhite-boxwhite-boxcalibratedPGGANWGANGPDCGANVAEGANVAE0.40.50.60.70.80.91.0AUCROCCelebAfullblack-box(LOGAN)fullblack-box(MC)fullblack-box(ourscalibrated)partialblack-box(ourscalibrated)white-box(ourscalibrated)accessiblefullmodel(LOGAN/MC)64128256512102420484096819215k20k40k60k80k100knumberofgeneratedsamples0.480.500.520.540.560.580.60AUCROCCelebA(PGGAN)fullblack-box(LOGAN)fullblack-box(MC)fullblack-box(ourscalibrated)(a)
(b)
(c)
Figure 11: Comparison of different attacks on the other two non-image datasets w.r.t. GAN training set size. See Table 13 in
Appendix for quantitative results.
generator or even just the input to the generator can lead to a con-
siderably higher risk of privacy breach. With a complete spectrum
of performance across settings, they bridge the performance gap
between the highly constrained full black-box attack and the un-
realistic discriminator-accessible attack. Moreover, our proposed
white-box attack model is of practical value for the differential
privacy community.
Assuming the accessibility of discriminator (full model) normally
results in the most effective attack. This can be explained by the
fact that the discriminator is explicitly trained to maximize the
margin between training set (membership samples) and generated
set (a subset of non-membership samples), which eventually yields
very accurate confidence scores for membership inference. Surpris-
ingly, our calibrated white-box attack even outperforms baseline
methods in more knowledgeable settings, i.e., LOGAN (accessible
discriminator) for VAEGAN and MC (accessible full model) for VAE.
This shows that when data coverage is explicitly enforced, which
probably leads to overfitting and data memorization if not properly
regularized, our attack models are highly effectively and achieve
superior performance with a more realistic assumption.
6.8 Defense
We investigate the most effective defense mechanism against MIA
to date that is applicable to GANs [7, 25, 66, 73], i.e., the differ-
ential private (DP) stochastic gradient descent [1]. The algorithm
can be summarized into two steps. First, the per-sample gradient
computed at each training iteration is clipped by its ùêø2 norm with
a pre-defined threshold. Subsequently, calibrated random noise is
added to the gradient in order to inject stochasticity for protecting
privacy. In this scheme, however, privacy protection is at the cost
of computational complexity and utility deterioration, i.e., slower
training and lower generation quality.
We conduct attacks against PGGAN on CelebA, which has been
defended by DP. We skip the other cases because DP always de-
teriorates generation quality to an unacceptable level. The hyper-
parameters are selected through the grid search. We fix the norm
threshold to 1.0 (average gradient norm magnitude during pre-
training) and the noise scale to 10‚àí4 (the largest value with which
we obtain samples of good visual quality). However, this results in
(a)
(b)
Figure 12: (a) Attack performance against PGGAN on CelebA
with or without DP defense. (b)Attack performance against
PGGAN on CelebA with or without DP defense, w.r.t. GAN
training set size. We fix all the other control factors (train-
ing iterations, batch size, noise scale, norm threshold) apart
from training set size, which results in less privacy guaran-
tee for a smaller dataset.
high ùúñ values (> 1010 for a default value of ùõø = 10‚àí5), while it still
reduces the effectiveness of the membership inference attack.
641282565121024204840968192sizeofdataset0.30.40.50.60.70.80.91.0AUCROCMIMIC-III(WGANGP)fullblack-box(LOGAN)fullblack-box(ourscalibrated)white-box(ourscalibrated)accessiblediscriminator(LOGAN)641282565121024204840968192sizeofdataset0.30.40.50.60.70.80.91.0AUCROCMIMIC-III(MEDGAN)fullblack-box(LOGAN)fullblack-box(ourscalibrated)white-box(ourscalibrated)accessiblediscriminator(LOGAN)641282565121024204840968192sizeofdataset0.30.40.50.60.70.80.91.0AUCROCInstagram(WGANGP)fullblack-box(LOGAN)fullblack-box(ourscalibrated)white-box(ourscalibrated)accessiblediscriminator(LOGAN)fullblack-boxcalibratedpartialblack-boxcalibratedwhite-boxcalibrated0.50.60.70.8AUCROCCelebAw/oDPw/DP64128256512102420484096sizeofdataset0.50.60.70.80.91.0AUCROCCelebAfullblack-boxw/oDPfullblack-boxw/DPwhite-boxw/oDPwhite-boxw/DPFigure 12(a) and Figure 12(b) depict the attack performance in
different settings. We observe a consistent decrease in AUCROC
in all the settings. Therefore, DP is effective in general against our
attack. However, applying DP into training leads to a much higher
computation cost (10√ó slower) in practice due to the per-sample
gradient modification. Moreover, DP results in a deterioration of
GAN utility, which is witnessed by an increasing FID (comparing
the last and second columns in Table 3). Moreover, for obtaining
a pleasing level of utility, the noise scale has to be limited to a
small value, which, in turn, cannot defend the membership infer-
ence attack completely. For example, for all the settings, our attack
still achieves better performance than the random guess baseline
(AUCROC = 0.5).
6.9 Summary
Before ending this section, we show a few insights over the ex-
periment results and list practical considerations relevant to the
deployment of GANs and potential privacy breaches.
‚Ä¢ The vulnerability of models under MIA heavily relies on the
attackers‚Äô knowledge about victim models. Releasing the
discriminator (full model) results in an exceptionally high
risk of privacy breach, which can be explained by the fact