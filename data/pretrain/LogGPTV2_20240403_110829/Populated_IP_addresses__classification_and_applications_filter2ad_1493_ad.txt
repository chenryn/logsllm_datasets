Figure 6: Classiﬁcation Performance. (a) PIPMiner performance against data length, (b) Pure history-based approach perfor-
mance, predicting PIP labels using all available prior reputation score records. (c) PIPMiner performance against the number
of the PIPs for the training, (d) PIPMiner performance against the amount of malicious traﬃc. Accuracy=(tn+tp)/all;
Precision=tp/(tp+fp); Recall=tp/(tp+fn).
series. In this model, the weighting factors for each older
data point decreases exponentially. Figure 6b shows that
this baseline approach achieves only 82% accuracy with the
best smoothing factor using all history information. More-
over, unlike our machine learning based approach, the base-
line cannot be applied to the PIPs without labels. The
machine-learning approach essentially learns the good/bad
behavior patterns from many other PIPs, hence we do not
require long activity histories.
Service Scale: PIPMiner is designed for analyzing PIP
behaviors. One question is whether PIPMiner is useful for
only very large service providers? Can we deploy PIPMiner
for small-scale online services that have only a few PIPs? To
answer these questions, we conduct an experiment to artiﬁ-
cially remove part of the source IP addresses in the Hotmail
trace randomly. As shown in Figure 6c, we demonstrate that
PIPMiner still retains its high classiﬁcation accuracy with
only a few hundred labeled samples, which is fewer than
0.2% of the labeled PIPs in the trace. This suggests that
PIPMiner is useful across a wider variety of business sizes.
Robustness to Attacks: With any detection scheme,
adversaries will naturally try to invent methods to evade
detection. The simplest choice would be to hide behind
anonymized networks such as ToR [9]. However, such ef-
fort is not very eﬀective because ToR’s throughput is low
and ToR IP addresses are public. Therefore, it is very easy
for service providers to pay special attention to the requests
coming from ToR IPs. Due to these constraints, we ﬁnd that
attackers have chosen to use open proxies or set up their
own proxies to conduct malicious activities, rather than us-
ing ToR in our data. Such observation exactly motivates
our work to classify PIPs.
Although the individual features of PIPMiner may be
gamed, it is diﬃcult for an attacker to evade the combi-
nations of them. A malicious PIP that evades our classiﬁ-
cation (i.e., below the threshold of being classiﬁed as bad)
may still be given a lower score by PIPMiner as long as
it fails to emulate normal user behaviors for some features.
The resulting score can be used in conjunction with other
detection techniques to improve the detection conﬁdence.
Moreover, PIPMiner raises the bar signiﬁcantly for an at-
tacker to evade the detection. The population and time se-
ries features will drive attackers to reduce their attack scale
in order to approximate the behaviors from a real user pop-
ulation. To validate that PIPMiner can retain high accuracy
when attackers reduce their attack scales, we synthetically
remove traﬃc sent by classiﬁed malicious PIPs in both train-
ing and testing data. Figure 6d shows that PIPMiner is
robust to such evasion strategies.
4.3 Validation of Unlabeled Cases
For the PIPs with known labels, we have shown that our
classiﬁer distinguishes good and bad ones accurately. It re-
mains unclear whether we can accurately classify the unla-
beled PIPs, which comprise 42.8% of the total PIPs. They
are particularly hard to classify as many accounts on these
PIPs are new with little history or few activities, so even the
reputation data cannot help to generate their labels.
In the lack of ground truth for verifying the unlabeled
PIPs, we combine information from both the user registra-
tion month and future reputation for validation.
User Registration Month:
Intuitively, good PIPs are
shared by many diﬀerent normal users and hence exhibit
a diverse distribution in the user registration months. For
bad PIPs, as their requests are mostly generated by attacker-
created malicious accounts, it is highly likely that most of
these accounts were created in a short time window. To
quantify whether our labels are correct, for each PIP we
place the corresponding accounts into bins based on their
registration month. Figure 7 shows the CDF of the percent-
age of accounts in the top bin, which has the largest number
of accounts. Overall, we observe very diﬀerent distributions
between the classiﬁed good and bad PIPs. For the PIPs
that we classify as good, only 4% of them have over 45%
accounts registered in the same month. For the PIPs that
we classify as bad, in contract, around 82% of them have
at least 45% accounts registered in the same month. The
distinct account behaviors suggest that the majority of the
unlabeled PIPs are classiﬁed correctly.
We further validate the results using future reputation.
Future reputation is a score reported by the reputation sys-
tem months after our dataset’s collection time. It gives the
reputation system an opportunity to observe user activities
for an extensive period of time, thus has a better chance
of recognizing previously unclassiﬁed accounts, either good
or bad. In particular, we use the reputation score of July
2011, which provides us with an updated user reputation
 0.75 0.8 0.85 0.9 0.95 1 6 12 24 50 100 200 400 800 1600 3200The maximal number of userrequests per PIP that areused to calculate featuresAccuracyPrecisionRecall 0.75 0.8 0.85 0.9 0.95 1 0.1 0.3 0.5 0.7 0.9EWMA smoothing factorAccuracyPrecisionRecall 0.75 0.8 0.85 0.9 0.95 1 10 100 1000 10000The number of labeled PIPs thatare used to train a classiﬁerAccuracyPrecisionRecall 0.75 0.8 0.85 0.9 0.95 10.01 %0.1 %1 %10 %100 %The amount of malicious traﬃcsent from identiﬁed bad PIPs[relative to raw data]AccuracyPrecisionRecall336Figure 7: The cumulative fractions of the percentage of ac-
counts that registered in the top 1 month. The function is
plotted over initially unlabeled PIPs.
Future
reputation
Accounts on Accounts on
good PIPs
bad PIPs
Good
Intermediate
Bad
Deleted
Unknown
61.6%
7.4%
7.5%
20.6%
2.9%
4.1%
5.3%
10.7%
75.4%
4.5%
Table 6: The future reputation (as of July, 2011) of PIPs
that do not have labels in August, 2010.
score almost one year after the testing dataset’s creation
time (August, 2010).
Table 6 summarizes the evaluation results based on fu-
ture reputation. For the accounts on the initially unlabeled
PIPs that we classiﬁed as good, around 62% of them are
recognized as good after one year. For the accounts on the
initially unlabeled PIPs that we classiﬁed as bad, around
86.1% of them are either deleted or marked as malicious af-
ter one year. We notice that a large portion of the accounts
that were classiﬁed as bad had been deleted during the year
and no longer exist in the updated reputation system. An
account can be deleted if it displays malicious behaviors
against user terms, or if there are no activities for a long
period of time. The violation of terms includes spamming
and phishing activities, reported by other mail servers.
These results demonstrate that PIPMiner can classify PIP
addresses accurately. When combining information collected
from multiple PIP addresses, we can recognize malicious ac-
counts months before the existing system does, with a pre-
cision higher than 98.6% (§5.2).
5. APPLICATIONS
In this section, we investigate two applications of PIPs.
First, we apply the PIP list to the Windows Live ID sign-
up abuse problem to ﬂag malicious accounts right on the
day of their sign-ups (§5.1). Second, we apply the PIP list
to the Hotmail malicious user sign-in problem and detect
malicious accounts months before the Hotmail reputation
system does (§5.2).
5.1 Windows Live ID Sign-up Abuse Problem
Our ﬁrst application is to ﬂag malicious (i.e., bot-generated)
sign-ups using the PIP information. Since there is very little
information about accounts available at the sign-up time, it
is challenging to correctly distinguish all malicious accounts
from good ones, given that they are all new. The ﬂags that
oﬀered by PIP addresses could be used as a feature or a
start-up reputation to detect malicious users more quickly
and limit their damages.
Case
Original
New
# Users
[million]
% Good
1.0
2.8
19.0%
1.4%
% Inter-
mediate
39.9%
20.3%
% Bad
41.1%
78.3%
Table 7: Comparative study of our PIP list (ﬁrst row) and
the newly appeared “PIP-like” list (second row).
PIP
# Users
information [thousand]
% Good
% Inter-
mediate
% Bad
Good
Abused
Normal
Bad
216.0
397.5
410.5
10.6%
39.1%
3.9%
35.3%
34.0%
48.1%
54.1%
26.9%
48.0%
Table 8: User reputation distribution on diﬀerent types of
PIPs.
We apply the 1.7 million PIP list derived from the Hotmail
login log of August 2010 to the Windows Live ID sign-up
trace in the same month. We study the sign-up behavior on
two types of the PIP addresses. The ﬁrst is the 1.7 million
derived PIPs. The second is the set of IP addresses that have
more than 20 sign-ups from the Windows Live ID system,
but they are not included in the 1.7 million PIPs. We refer
to the second set as the newly appeared “PIP-like” addresses.
As there is little information available at the sign-up time
to distinguish good sign-ups from bad ones, we focus on the
sign-ups related to Hotmail2 and use the Hotmail reputation
trace in July, 2011 (after 11 months) to determine whether
a particular sign-up account was malicious or not.
Sign-ups from newly appeared PIP-like addresses:
Table 7 shows a comparative study of our derived PIP list
and the newly appeared PIP-like addresses. Although both
lists are associated with a large number of malicious sign-
ups, the fraction of bad sign-ups from PIP-like addresses
is signiﬁcantly higher (78.3% versus 41.1%) than that from
our derived PIP list. Only 1.4% of the sign-ups establish
good reputation scores after one year. This shows that good
PIPs usually have activities across multiple services, as they
are used by many normal users who have diverse interests.
The PIP-like addresses that are dominantly used by just
one service are more suspicious. They are more likely to be
attacker-controlled IPs and leveraged by attackers to con-
duct attacks to a speciﬁc service.
It is hard and also ex-
pensive for attackers to simulate normal user requests to all
possible Web services. Hence, correlating activities across
multiple services could yield a better PIP recognition accu-
racy and also attack-detection ability.
Sign-ups from our PIP addresses: Blindly applying
our PIP list to the sign-up data, we observe a mixed user
reputation (19.0% good, 41.1% bad, remaining as interme-
diate as shown in Table 7). To further classify the good
sign-ups from bad ones, we leverage PIP address properties
provided by our system: the goodness score and the number
of good users.
The results are summarized in Table 8. For the PIPs with
low goodness scores, not surprisingly, only 3.9% of the sign-
ups remain good after 11 months. For the PIPs with high
2Windows Live ID is a uniﬁed ID, supporting multiple ser-
vices such as Hotmail, Window Live Messenger, Xbox.
 0 0.2 0.4 0.6 0.8 1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Cumulativefraction of PIPs% of accounts registered in the top monthClassiﬁed as goodClassiﬁed as mixedClassiﬁed as bad337Actual
Our prediction
(after 11 months) Good
155K
106K
Good
Bad
Bad
78K
2506K
Table 9: Using PIPs to predict user reputation right at the
day of the sign-ups.
scores in the Hotmail login log, however, they could still be
temporarily abused by attackers in the sign-up log. Here,
we detect abuse scenarios by looking at the sign-up rate to
the sign-in rate. If the ratio is larger than a threshold (set to
0.1 in the current experiment as the sign-up rate is usually
a magnitude smaller than the sign-in rate from practical ex-
perience), we think that the IP address is potentially abused
by attackers.
For the PIPs with high goodness scores, around 35% of
them are potentially abused. Sign-ups from abused PIPs
have much lower reputation scores compared to non-abused
good PIPs. Among the 216K sign-ups from abused PIPs,
we observe that only 10.6% sign-ups have good reputation
scores. In contrast, almost 40% of the sign-ups from non-
abused PIPs have good reputation scores and 34.0% are in-
termediate cases. Although we still observe 26.9% of the
sign-ups from good PIPs display malicious behavior later,
these could be low rate malicious sign-ups that escape the
abuse detection system. As it is essentially hard to com-
pletely stop attackers from signing up malicious accounts
for spamming, the ability to push attackers into a low rate
mode and limit the number of their malicious accounts is of
great value to the service provider.
Using the goodness scores of PIP addresses and the abuse
detection method, we introduce the following sign-up early-
warning system that ﬂags new users right on the day of the
sign-ups. We label users related to the “normal” PIPs as po-
tentially good and label users related to the other cases as
potentially bad. Using this method, we successfully detect
more than 3 million bad sign-ups as shown in Table 9. We
have a precision of 97.0% in detecting malicious accounts
by misclassifying a small number of good users (i.e., false
positives). We also have a high recall of 96.0% by accept-
ing a moderate number of false negatives. The prediction of
good users is less accurate, as 40.6% of them turn out to be
malicious. While our techniques does not achieve a perfect
accuracy, they provide early signals on the day of sign-ups
and can be used in conjunction with previously proposed
techniques (e.g., the Hotmail user reputation system, the
anomaly detection methods) to speed up or improve conﬁ-
dence in malicious user detection. We leave the comparison
to Quova’s proxy list to Appendix B.
5.2 Hotmail Malicious Account Detection
In this section, we apply our PIP list to another appli-
cation — the Hotmail malicious user sign-in problem. We
use bad PIPs to help perform postmortem forensic analy-
sis to identify malicious accounts that slipped through the
Hotmail user reputation system.
Intuitively, there should not be many good users (with