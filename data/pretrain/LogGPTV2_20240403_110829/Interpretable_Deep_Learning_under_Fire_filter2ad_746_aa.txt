# Interpretable Deep Learning under Fire

**Authors:**
- Xinyang Zhang, Pennsylvania State University
- Ningfei Wang, University of California, Irvine
- Hua Shen, Pennsylvania State University
- Shouling Ji, Zhejiang University and Alibaba-ZJU Joint Institute of Frontier Technologies
- Xiapu Luo, Hong Kong Polytechnic University
- Ting Wang, Pennsylvania State University

**Publication:**
- Proceedings of the 29th USENIX Security Symposium
- August 12â€“14, 2020
- ISBN: 978-1-939133-17-5
- Open access sponsored by USENIX
- [Link to the paper](https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-xinyang)

## Abstract

Providing explanations for deep neural network (DNN) models is crucial in security-sensitive domains. Numerous interpretation models have been proposed to help users understand how DNNs make decisions. Improved interpretability is believed to enhance security by involving humans in the decision-making process. However, due to their data-driven nature, these interpretation models are potentially vulnerable to malicious manipulations, an area that has not been well-studied.

In this work, we conduct the first systematic study on the security of interpretable deep learning systems (IDLSes). We demonstrate that existing IDLSes are highly vulnerable to adversarial attacks. Specifically, we introduce ADV2, a new class of attacks that generate adversarial inputs to mislead both the target DNN and its coupled interpretation model. Through empirical evaluations on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we show that ADV2 allows adversaries to arbitrarily control both the prediction and interpretation of an input. We identify the prediction-interpretation gap as a root cause of this vulnerability, where the DNN and its interpretation model are often misaligned. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it into an adversarial training framework. Our findings provide insights into designing and operating IDLSes more securely and informatively, leading to several promising research directions.

## 1. Introduction

Recent advances in deep learning have led to breakthroughs in various machine learning tasks, such as image classification, natural language processing, and even playing Go. These advancements have enabled use cases previously considered experimental. However, the state-of-the-art performance of DNNs often comes at the cost of interpretability. The high non-linearity and nested architectures of DNNs make it challenging to intuitively understand their decision-making processes. This lack of interpretability is a significant drawback for applications where understanding the reasoning behind decisions is critical. Additionally, DNNs are inherently vulnerable to adversarial inputs, which can cause them to malfunction, leading to unpredictable behavior and limiting their use in security-sensitive domains.

To address these issues, there has been extensive research on improving DNN interpretability through model-level and instance-level explanations. For example, attribution maps highlight the most informative parts of an input with respect to its classification, revealing causal relationships. Such interpretability helps in model debugging, digesting security analysis results, and detecting adversarial inputs. In Figure 1, we illustrate benign, regular adversarial, and dual adversarial inputs and their corresponding interpretations using ResNet (classifier) and CAM (interpreter).

### Notation

| Symbol | Definition |
|--------|------------|
| \( f, g \) | Target classifier, interpreter |
| \( x^{\circ}, x^{*} \) | Benign, adversarial input |
| \( c_t, m_t \) | Adversary's target class, interpretation |
| \( x[i] \) | i-th dimension of \( x \) |
| \( \varepsilon \) | Perturbation magnitude bound |
| \( \| \cdot \| \) | Vector norm |
| \( \alpha \) | Learning rate |
| \( \ell_{\text{int}}, \ell_{\text{prd}}, \ell_{\text{adv}} \) | Interpretation, prediction, overall loss |

### Roadmap

- **Section 2:** Introduces fundamental concepts.
- **Section 3:** Presents the ADV2 attack and details its implementation against four major types of interpreters.
- **Section 4:** Empirically evaluates the effectiveness of ADV2.
- **Section 5:** Explores the fundamental causes of the attack vulnerability and discusses possible countermeasures.
- **Section 6:** Surveys relevant literature.

This work represents the first systematic study on the security vulnerabilities of existing IDLSes. Our findings provide valuable insights into designing and operating IDLSes in a more secure and informative manner, opening up several promising research directions.