title:Interpretable Deep Learning under Fire
author:Xinyang Zhang and
Ningfei Wang and
Hua Shen and
Shouling Ji and
Xiapu Luo and
Ting Wang
Interpretable Deep Learning under Fire
Xinyang Zhang, Pennsylvania State University; Ningfei Wang, University of 
California Irvine; Hua Shen, Pennsylvania State University; Shouling Ji, Zhejiang 
University and Alibaba-ZJU Joint Institute of Frontier Technologies; Xiapu Luo, 
Hong Kong Polytechnic University; Ting Wang, Pennsylvania State University
https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-xinyang
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Interpretable Deep Learning under Fire
Xinyang Zhang∗ Ningfei Wang(cid:63) Hua Shen∗
∗Pennsylvania State University
Shouling Ji† Xiapu Luo‡
(cid:63)University of California Irvine
Ting Wang∗
†Zhejiang University and Alibaba-ZJU Joint Institute of Frontier Technologies
‡Hong Kong Polytechnic University
Abstract
Providing explanations for deep neural network (DNN)
models is crucial for their use in security-sensitive domains.
A plethora of interpretation models have been proposed to
help users understand the inner workings of DNNs: how does
a DNN arrive at a speciﬁc decision for a given input? The
improved interpretability is believed to offer a sense of se-
curity by involving human in the decision-making process.
Yet, due to its data-driven nature, the interpretability itself
is potentially susceptible to malicious manipulations, about
which little is known thus far.
Here we bridge this gap by conducting the ﬁrst systematic
study on the security of interpretable deep learning systems
(IDLSes). We show that existing IDLSes are highly vulner-
able to adversarial manipulations. Speciﬁcally, we present
ADV2, a new class of attacks that generate adversarial inputs
not only misleading target DNNs but also deceiving their
coupled interpretation models. Through empirical evaluation
against four major types of IDLSes on benchmark datasets
and in security-critical applications (e.g., skin cancer diag-
nosis), we demonstrate that with ADV2 the adversary is able
to arbitrarily designate an input’s prediction and interpreta-
tion. Further, with both analytical and empirical evidence, we
identify the prediction-interpretation gap as one root cause
of this vulnerability – a DNN and its interpretation model
are often misaligned, resulting in the possibility of exploiting
both models simultaneously. Finally, we explore potential
countermeasures against ADV2, including leveraging its low
transferability and incorporating it in an adversarial training
framework. Our ﬁndings shed light on designing and operat-
ing IDLSes in a more secure and informative fashion, leading
to several promising research directions.
1 Introduction
The recent advances in deep learning have led to break-
throughs in many long-standing machine learning tasks (e.g.,
image classiﬁcation [22], natural language processing [54],
and even playing Go [49]), enabling use cases previously
considered strictly experimental.
However, the state-of-the-art performance of deep neural
network (DNN) models is often achieved at the cost of in-
terpretability. It is challenging to intuitively understand the
Figure 1: Sample (a) benign, (b) regular adversarial, and (c) dual
adversarial inputs and interpretations on ResNet [22] (classiﬁer) and
CAM [64] (interpreter).
inference of complicated DNNs – how does a DNN arrive
at a speciﬁc decision for a given input – due to their high
non-linearity and nested architectures. This is a major draw-
back for applications in which the interpretability of decisions
is a critical prerequisite, while simple black-box predictions
cannot be trusted by default. Another drawback of DNNs
is their inherent vulnerability to adversarial inputs – mali-
ciously crafted samples to trigger target DNNs to malfunc-
tion [9,28,56] – which leads to unpredictable model behaviors
and hinders their use in security-sensitive domains.
The drawbacks have spurred intensive research on improv-
ing the DNN interpretability via providing explanations at
either model-level [26,45,63] or instance-level [10,16,43,50].
For example, in Figure 1 (a), an attribution map highlights an
input’s most informative part with respect to its classiﬁcation,
revealing their causal relationship. Such interpretability helps
users understand the inner workings of DNNs, enabling use
cases including model debugging [39], digesting security anal-
ysis results [20], and detecting adversarial inputs [13]. For
instance, in Figure 1 (b), an adversarial input, which causes
the target DNN to deviate from its normal behavior, gener-
ates an attribution map highly distinguishable from its benign
counterpart, and is thus easily detectable.
As illustrated in Figure 2, a DNN model (classiﬁer), cou-
pled with an interpretation model (interpreter), forms an in-
USENIX Association
29th USENIX Security Symposium    1659
(a) Benign(b) AdversarialInput InterpretationInput Interpretation(c) Dual AdversarialNotation
f , g
x◦, x∗
ct, mt
x[i]
ε
(cid:107)·(cid:107)
α
(cid:96)int, (cid:96)prd, (cid:96)adv
Deﬁnition
target classiﬁer, interpreter
benign, adversarial input
adversary’s target class, interpretation
i-th dimension of x
perturbation magnitude bound
vector norm
interpretation, prediction, overall loss
learning rate
Table 1. Symbols and notations.
ADV2 in training interpreters. We show that AID effectively
reduces the prediction-interpretation gap and potentially helps
improve the robustness of interpreters against ADV2.
To our best knowledge, this work represents the ﬁrst sys-
tematic study on the security vulnerability of existing IDLSes.
We believe our ﬁndings shed light on designing and operating
IDLSes in a more secure and informative manner.
Roadmap. The remainder of the paper proceeds as follows.
§ 2 introduces fundamental concepts; § 3 presents the ADV2
attack and details its implementation against four major types
of interpreters; § 4 empirically evaluates its effectiveness; § 5
explores the fundamental causes of the attack vulnerability
and discusses possible countermeasures; § 6 surveys relevant