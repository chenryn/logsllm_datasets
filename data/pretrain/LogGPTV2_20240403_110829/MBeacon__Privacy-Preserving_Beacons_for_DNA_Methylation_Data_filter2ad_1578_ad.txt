gI(cid:48)(z1 − ∆, z2 + ∆). Therefore, we rely on the fact that noise
values yi are Laplace distributed (formally, LAP( 2c∆
)) and use
2
Inequalities 17 and 18 to prove it.
Pr[ρ = yi] ≤ e
Pr[ρ = yi] ≤ e
2
c Pr[ρ = vi + 2∆]
c Pr[ρ = vi − 2∆]
2
(17)
(18)
To prove Inequality 16, we use the fact that z1 and z2 are
sampled from LAP( ∆
1
).
In the end, by combining Inequalities 14, 15 and 16, we
prove Theorem 1 as follows:
−∞
−∞
=
Pr[A(I) =
(cid:90) ∞
(cid:90) ∞
−∞
fI(z1, z2)gI(z1, z2)dz1dz2
≤
−→
R ]
Pr[ρ1 = z1 ∧ ρ2 = z2]
(cid:90) ∞
(cid:90) ∞
(cid:90) ∞
(cid:90) ∞
−∞
fI(cid:48)(z1 + ∆, z2 − ∆)e22gI(cid:48)(z1 + ∆, z2 − ∆)dz1dz2
Pr[ρ1 = z(cid:48)
=e21+22
−∞
−∞
2)dz(cid:48)
fI(cid:48)(z(cid:48)
1dz(cid:48)
1, z(cid:48)
2)gI(cid:48)(z(cid:48)
1, z(cid:48)
−→
=e2(1+2) Pr[A(I(cid:48)) =
R ]
e21 Pr[ρ1 = z1 + ∆ ∧ ρ2 = z2 − ∆]
1 ∧ ρ2 = z(cid:48)
2]
2
(cid:4)
The purpose of Algorithm 1 is to answer whether the
database is approximated well by the background knowledge
in a differentially private way. To output a Beacon answer
of the format “Yes, such data is available” resp. “No, such
data is not available”, we need to remove the background
knowledge from Algorithm 1’s answer. This is performed by
Algorithm 2, which preserves the differential privacy of the
answer. Intuitively, the transformation maintains differential
privacy due to the composition and post-processing theorems.
However, these theorems are not directly applicable due to our
database format. Therefore, we prove the following theorem.
Theorem 2. Algorithm 2 is 2(1 + 2)-differentially private.
Proof. Once the prior frequency P is ﬁxed, the output of
Algorithm 2 only depends on the output of Algorithm 1,
namely, whether the prior is correct or has to be ﬂipped.
Formally, we describe this as follows.
−→R ∈ { “Yes”, “No” }l of Algo-
First, ﬁxing any output
rithm 2 on Q = (cid:104)q1, ..., ql(cid:105), we have:
Pr[B(T, 1, 2, c, Q, I, P) =
Pr[B(T, 1, 2, c, Q, I(cid:48), P) =
As Algorithm 2 is deterministic, we have:
Pr[A(T, 1, 2, c, Q, I, P) =
Pr[A(T, 1, 2, c, Q, I(cid:48), P) =
∗ =
−→R]
−→R]
−→
R ]
−→
R ]
= ∗
= ∗
Algorithm 1 is 2(1 + 2)-differentially private, thus:
∗ ≤ e2(1+2)
(cid:4)
Notice that, for technical reasons, we disassemble our
proposed method into two stages. However, one can of course
perform both stages at once and directly output the MBeacon
response. Since we assume the prior frequency is publicly
known, we do not have to add noise to its result, which yields
Condition 10 above.
Setting the Parameters. We have shown that A is 2(1 + 2)-
differentially private to make the connection between privacy-
sensitive and less privacy-sensitive queries as well as the
connection to the sparse vector technique visible. However,
for tuning parameters, it is desirable to have only a single
privacy parameter  in addition to the budget c. Lyu et al. [21]
showed that the ratio 1 : 2 = 1 : (2c) 2
3 maximizes utility,
while preserving  = 1 + 2. We adopt Lyu’s ratio between 1
and 2. The sensitivity ∆ is 1 in our case, since removing a
participant’s entry from the database or changing it can affect
the bin count by at most one. For a given privacy parameter
and using ∆ = 1, we set:

2
(2c) 2
3 + 1
2 = (2c)
1 =
2
3 1
Application to other Domains. We emphasize that SVT2 is a
general differential privacy mechanism, and can be applied in
other cases beyond MBeacon: SVT2 is useful for comparing
a database to a general belief in a differentially-private way.
Moreover, comparing two databases is possible using Algo-
rithm 1 since it applies noise to both databases α and β. In
the future, we plan to apply SVT2 to other data domains, such
as location data [27], [46], social network data [23], [47], and
other types of biomedical data [4].
VII. RESEARCHER UTILITY
The goal of the MBeacon system is to facilitate biomedical
data sharing among the research community. Therefore, we
quantify the utility of MBeacon as the ability of a legitimate
researcher to ﬁnd methylation data of interest.
the researcher already has multiple proﬁles in P on her site,
denoted by P (cid:48) with P (cid:48) ⊂ P . Then, her goal is to ﬁnd those
MBeacons with methylation proﬁles from P\P (cid:48). A central
assumption here is that methylation proﬁles in P are similar
to each other.
As the MBeacon system only supports queries on single
methylation positions, the researcher also relies on the LR test
to ﬁnd MBeacons that contain patients in P . Moreover, there
often exist measurement errors when collecting methylation
values. To increase the reliability of her LR test, the researcher
further averages all the methylation proﬁles in P (cid:48).
Ideally, the researcher queries a MBeacon BP only contain-
ing patients of interest. To simulate a more realistic case, we
assume the existence of another population D the researcher
is not interested in. Notice that D might also be a mixture
of populations. The researcher tries to distinguish a MBeacon
BD containing no patients of interest from a MBeacon BP,D
containing some patients of interest. In the worst case, there
are only a few patients from P in BP,D. In that case, the
contribution of patients from P is small and may be hidden
due to the SVT2 protection.
To get the lower bound of the MBeacon utility, we concen-
trate on this worst case scenario. Figure 2a depicts a graphical
summary of the researcher setup. The researcher achieves a
true positive if the MBeacon she selects contains some proﬁles
in P . A false positive indicates that the MBeacon she ﬁnds
does not have the data of her interest. True negative and
false negative are deﬁned accordingly. Given these numbers,
in particular the true-positive and false-positive rates, we can
derive the AUC as our core utility metric.
Attack Scenarios. In order to ﬁnd a good trade-off between
utility and privacy, we have to evaluate the attacker’s success
under the same scenario as the researcher. The attacker’s goal
is to detect with high probability whether a target is part of
the MBeacon database or not. Of course, the attacker does not
know whether she is querying a MBeacon of the form BP,D
or BD, similar to the researcher not knowing the distinction
a priori. Moreover, the attacker’s target might be a patient in
D or in P . We refer to such an attacker as “full” attacker; a
graphical overview is displayed in Figure 2b.
The evaluation of the “full” attacker is comparable to the
researcher evaluation, but not to existing works [36], [29], [1],
[45], where the MBeacon and the victim are from one uniform
dataset. Therefore, we additionally model an attacker querying
only BD and trying to infer whether a victim in D is part of the
MBeacon. We refer to this second attacker as the “standard”
attacker, since it is the same as the one considered in Section V.
VIII. DEFENSE EVALUATION
We evaluate our defense mechanism SVT2 in this section
with respect to the attack performance and utility as deﬁned
in Section VII.
A. Experimental Setup
Concretely, a researcher is interested in methylation proﬁles
of people with a certain phenotype or disease. We use the
set P to represent all these methylation proﬁles. Moreover,
For the set of researcher’s interest, P , we use Ependy-
moma, which contains data from 48 patients. For the set D
the researcher is not interested in, we use either GBM, IBD
9
p
BP,D
p(cid:48)
p
BP,D
P (cid:48)
BD
p(cid:48)
d
d(cid:48)
d
d(cid:48)
BD
P
D
(a) The researcher knows patient(s) from P (cid:48) and is interested in
patients from P in BP,D, shown exempliﬁed by patient p. The
researcher’s task is to ﬁnd that BD is not interesting for research, while
BP,D is interesting. We focus on the worst-case of the researcher by
assuming P being a minority in BP,D to give a lower bound on utility.
P
D
(b) The attacker either queries BP,D or BD (without knowing which
one), and might have a target p from P outside the MBeacon, a target
p(cid:48) from P in the MBeacon or a target d resp. d(cid:48) from D in resp.
outside of the MBeacon. To compare side-by-side with the researcher,
we again assume P to be a minority in BP,D.
Fig. 2. A graphical overview on the general utility setup for researcher (a) and the general utility setup for the attacker (b).
CD or IBD UC as before, forming three different types of
MBeacons.
Each of these MBeacons consists of a certain number of
patients in P , we test 7 different choices for this number
including 1, 3, 5, 10, 13, 15 and 20. The remaining patients
are randomly sampled from the respective D such that a total
size of 60 is reached. Moreover, we sample randomly 60
patients from the respective D to construct BD. We simulate
5 researchers querying each pair of corresponding MBeacons
BP,D and BD. The researcher possesses P (cid:48) containing 5
randomly sampled patients in P that are not used in the
MBeacon.13 As mentioned in Section VII,
the researcher
averages these patients’ proﬁles to reduce measurement errors.
The whole sampling process is repeated 10 times to ensure the
observations are not due to randomness.
For the attacker simulations, we re-use the MBeacons we
constructed before for the researcher, but sample test patients
differently. The “full” attacker has access to only a single
patient. We randomly sample 12 patients from each of BP,D
and BD as the ones in the MBeacon. Accordingly, we sample
24 patients from P ∪ D as the patients that are outside the
MBeacon. Since we assume throughout the experiments that
patients in P are the minority, we use only up to a third of
patients in P and the remainder in D. As before, we repeat
random sampling 10 times. The “best” attacker does not have
access to BP,D and, consequently, does not get test patients in
P . Instead, we sample 24 test patients from BD and 24 test
patients from D \ BD for each of the BD MBeacons.
We assume both researchers and attackers have access to
the mean and standard deviation of the general population, that
we estimate by a union of all our available datasets as before.
These means and standard deviations are used to carry out
13If the researcher averages fewer patients, the performance could decrease
slightly since individual, non-disease related changes in the patients’ methy-
lation values become more pronounced in the search.
LR tests and rank queries, up to 250,000 queries are allowed
per researcher resp. attacker. Moreover, both researchers and
attackers sort
their queries based on expected information
gain as explained in Section IV and used in the previous
experiments in Section V.
To sum up, we test three different choices for D, and 7
different numbers of patients from P in BP,D, simulate 5
researchers querying each of the MBeacons and re-sample the
experiments 10 times, so simulate in total 2,100 researchers.
Due to the attackers not averaging over multiple patients,
we can simulate more membership inference attacks: 10,500
carried out by the “full” and the “standard” attacker each.
B. Evaluation of SVT2
First, we evaluate the inﬂuence of the number of patients in
P in the MBeacons of type BP,D. We observe that if there are
5 or more patients of interest, the researcher’s performance is
maximized. The “full” attacker, however, suffers from more
patients in P , probably due to the higher variance in the
MBeacon.
Second, we focus on SVT2. Our protection mechanism has
three parameters: a threshold T determining how many patients
have to be in the respective bin to answer “Yes”, as well as
the privacy parameter  and the query budget c, which both
calibrate the noise.
The Privacy Budget. We aim for parameters that drop
the “standard” attackers’ performance to about 0.5 AUC,
equivalent to random guessing, while minimizing the noise.
Moreover, exceeding the query budget is something MBeacon
providers would want to avoid, because the MBeacon has to
stop answering in that case. Therefore, we choose a budget that
is never exceeded in our simulations. The researchers and the
two different types of attackers (“standard” and “full”) are all
simulated separately, so our budget has to be sufﬁcient for 50
attackers submitting 12,500,000 (50×250,000) queries in total.
10
(a)
(b)
(c)
Fig. 3. Comparison of researchers’ and attackers’ performances in unprotected MBeacon (black, abbreviated as “unpr.”) and protected MBeacon (red) using
GBM (left), IBD UC (middle) and IBD CD (right) as D using up to 100,000 queries. Additionally, we plot the researchers’ performances for 1,000 queries in
blue (unprotected) and magenta (protected). AUCs with values smaller than 0.5 are displayed as 0.5.
(a)
(b)
(c)
Fig. 4. Comparison of researchers’ and attackers’ performances when setting T = 3 in unprotected MBeacon (black, abbreviated as “unpr.”) and protected
MBeacon (red) using GBM (left), IBD UC (middle) and IBD CD (right) as D using up to 100,000 queries. Additionally, we plot the researchers’ performances
for 1,000 queries in blue (unprotected) and magenta (protected). AUCs with values smaller than 0.5 are displayed as 0.5.
Notice that not all of those queries are expected to be unique
and not all of them fall into the category of privacy-sensitive
queries for which the budget must be reduced.
the MBeacon answers “Yes” if there is at
Threshold T = 1. We start with the default threshold T = 1,
i.e.,
least one
patient’s methylation value in the queried bin. A budget of
c = 600, 000 is sufﬁcient for our simulations. This might
seem large at ﬁrst glance, but notice that, having 10 bins, there
are 300,000×10 different queries that can be asked, so our c
corresponds to about 20% of them. Due to space constraints,
we report the privacy level that we found as a suitable trade-off
c = 0.05. We report the privacy
between privacy and utility at 
levels as in [34].
As shown in Figure 3, the privacy level is sufﬁcient to drop
the “standard” attackers’ performance to less than 0.6 AUC
which shows that the privacy threat can be mitigated success-
fully. In the more realistic “full” attacker scenario, however,
the attacker’s performance is higher, which is explained by
the fact that membership attacks with patients from P against
the BD MBeacon are most successful. Nevertheless, we see a
signiﬁcant drop in performance due to the application of SVT2.
The researcher’s performance is still good with 0.8 AUC