title:PrivacyBuDe: Privacy Buckets Demo Tight Differential Privacy Guarantees
made Simple
author:David M. Sommer and
Sebastian Meiser and
Esfandiar Mohammadi
DEMO: PrivacyBuDe: Privacy Buckets Demo
Tight Differential Privacy Guarantees made Simple
David Sommer
Sebastian Meiser
Esfandiar Mohammadi
ETH Zurich
Zurich, Switzerland
PI:EMAIL
University College London
London, United Kingdom
PI:EMAIL
ETH Zurich
Zurich, Switzerland
PI:EMAIL
ABSTRACT
Computing differential privacy guarantees is an important task for
a wide variety of applications. The tighter the guarantees are, the
more difficult it seems to be to compute them: naive bounds are
simple additions, whereas modern composition theorems require
either a search over a potentially confusing parameter space (for
the composition theorem of Kairouz, Oh, and Visvanath), require
computing many moments (for Rényi DP) or finding parameters
of a function that limits the privacy loss (for concentrated DP).
The best known approach for tight differential privacy bounds,
called Privacy Buckets, provides requires running a fairly complex
implementation of numerical approximations.
In this work, we provide an easy-to-use interface for computing
state-of-the-art differential privacy guarantees by simply accessing
a website. Guarantees for the widely used Laplace mechanism and
for the similarly popular Gauss mechanism can be computed by
simply stating the scale parameter of the noise, the sensitivity, and
the number of compositions. Privacy guarantees for more complex
distributions can be computed by uploading two histograms.
This work bridges the gap between the best known theoretical
results for computing differential privacy guarantees and privacy
analysts and users benefiting from such guarantees.
CCS CONCEPTS
• Security and privacy → Usability in security and privacy;
KEYWORDS
differential privacy; r-fold composition; scientific outreach
ACM Reference Format:
David Sommer, Sebastian Meiser, and Esfandiar Mohammadi. 2018. DEMO:
PrivacyBuDe: Privacy Buckets Demo, Tight Differential Privacy Guarantees
made Simple. In 2018 ACM SIGSAC Conference on Computer and Communi-
cations Security (CCS ’18), October 15–19, 2018, Toronto, ON, Canada. ACM,
New York, NY, USA, 3 pages. https://doi.org/10.1145/3243734.3278485
INTRODUCTION
1
A wide range of applications depends on a good assessment of
privacy guarantees: releasing statistical information on potentially
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5693-0/18/10.
https://doi.org/10.1145/3243734.3278485
private data, anonymous communication, time-sensitive computa-
tions on sensitive data, machine learning, and numerous other tasks
that involve processing private data. In many of these applications
the notion of approximate differential privacy (ADP), quantified by
two parameters ε and δ, is used to give a bound on the impact on
privacy. The smaller ε and δ, the better the privacy guarantees, but
this typically comes at a cost in terms of the utility of the mecha-
nism: more noise has to be added, less queries can be made before
a privacy-budget is exceeded, or more resources have to be used to
hide timing leakage.
A rich line of research in the area of differential privacy [1, 3–5, 7]
has been devoted to improving this trade-off between utility and
privacy by providing tighter bounds on ε and δ. Such tighter bounds
lower the bar for applying privacy-preserving mechanisms, reduce
the costs to achieve a desired privacy target and generally help to
get a better grasp at the impact of personal data on computations.
Nevertheless, we observe a worrying trend in this regard: Al-
though reasonably tight bounds for differential privacy are known
since years, many practical applications still use the naive com-
position bounds originally presented for approximate differential
privacy or at best the demonstrably loose advanced composition
theorem [4].1 The reason for this gap between research and practice
is simple: multiplying the values for ε and δ with the number of
observables is very simple, whereas applying tight privacy guar-
antees is not. Modern ADP bounds require not only a very good
understanding of the literature, but also to twiddle and optimize
bound-specific parameters.
The generic composition theorem of Kairouz, Oh, and Visvanath
[5], e.g., requires an optimization of a combination of some param-
eter i and initial ε and δ values. Applying that theorem additionally
requires understanding that ε and δ depend on the choice of mech-
anism and come with their own additional degree of freedom. Such
a deep understanding is crucial for computing a reasonably tight
bound with this theorem. Even more tight bounds, based on Rényi
divergence [7], require to compute the moment generating func-
tions of the privacy loss—a useful representation of leakage2—of the
mechanism in question, while guarantees on concentrated differ-
ential privacy guarantees [1, 3] can lead to very tight ADP bounds
as soon as a Gaussian over-approximation for the mechanism in
question’s privacy loss can be found.
All of these techniques are tedious to work with. We have in-
troduced privacy buckets [6] as a numerical method for computing
tight upper and lower bounds on ADP, providing yet another and,
in our eyes, often preferable method for computing ADP guarantees.
1We have recently shown [6, Figure 3] that these bounds can be off by as much as 4
orders of magnitude in the estimated value for δ , for a given ε.
2Privacy loss has first been introduced as confidence gain in [2].
DemonstrationCCS’18, October 15-19, 2018, Toronto, ON, Canada2192However, properly setting up the scripts required to compute ADP
guarantees with privacy buckets is challenging in its own right. We
expect privacy researchers to be able to easily deal with this com-
plexity, but we cannot expect every potential beneficiary of good
ADP bounds to sufficiently understand the math and engineering
challenges required.
Thus, we consider it important to bridge the gap between tight
bounds (important for everyone working with ADP) and the prac-
tical applicability of these bounds for every-day purposes.
Contribution. In this practical work we present an efficient and
easy-to-use application of the best known technique for computing
ADP guarantees. Our work bridges the gap between cutting-edge
research and practice, enabling anyone interested in privacy to
compute tight differential privacy guarantees with a simple web-
based application, which we call PrivacyBuDe [9].
Our tool is publicly available and can be used with a few clicks.
Commonly used mechanisms are readily available and we further-
more allow for arbitrary histograms of output distributions to be
analyzed. Our source code is openly available [8] and ready to be
deployed and adjusted by other researchers and privacy enthusiasts
for other webpages.
2 BACKGROUND – DIFFERENTIAL PRIVACY
AND PRIVACY BUCKETS
Approximate differential privacy is captured by two parameters: ε,
which we for simplicity call the deniability parameter captures the
multiplicative privacy leakage of a mechanism and is typically con-
sidered the main privacy parameter, whereas δ limits the probability
mass of any events not captured by ε.3
Figure 1: The interface of our tool PrivacyBuDe.
Definition 1 ((Tight) ADP). Two distributions A and B over the
universe U are (ε, δ)-ADP, if for every set S ⊆ U,
PA(S) ≤ eε PB(S) + δ(ε) and PB(S) ≤ eε PA(S) + δ(ε),
where PA(x) denotes the probability of the event x in A and PB(x)
denotes the probability of the event x in B. We call A and B tightly
(ε, δ(ε))-ADP if they are (ε, δ(ε))-ADP, and ∀δ′ ≤ δ(ε) such that A
and B are (ε, δ′)-ADP we have δ(ε) = δ′.
We showed [6] that considering two distributions is sufficient for
generally capturing differential privacy of mechanisms. We refer to
a discussion in this work [6, Section 2.1] for a comprehensive dis-
cussion of how distributions relate to mechanisms. For illustration
consider the simplest and most common case where we analyze the
output of a mechanism M that privately computes a function f as
M(x) = f (x) + N and N is the noise we add. In this case, we can
consider the output distributions of M(0) and M(∆f ) to get ADP
guarantees, where ∆f is the sensitivity of f . In particular, beyond
the sensitivity ∆f , we can abstract away from the result f (x).
The same work introduces privacy buckets, a novel numerical
way of computing ADP guarantees for two arbitrary distributions.
For the remainder of this work, we do not require any knowledge
of how exactly these bounds are computed. Instead, we treat them
as a black box B(A, B) that outputs two functions δ(·) and δlow(·)
with the following properties: for every ε ∈ [0,∞) we know that A
3More generally, the parameter δ can also capture the probability mass that does not
obey the deniability parameter ε.
Figure 2: PrivacyBuDe’s results for computing ADP guaran-
tees for the Laplace mechanism with scale = 1 and n = 8.
and B are provably (ε, δ(ε))-ADP but not (ε, δlow(ε))-ADP, i.e., δ(ε)
provides an upper bound and δlow(ε) provides a lower bound for
ADP.
3
PRIVACYBUDE– A WEBSITE FOR
COMPUTING DP GUARANTEES
Our main contribution is PrivacyBuDe, a practical platform for
computing ADP guarantees.
3.1 ADP for Laplace / Gauss mechanism
PrivacyBuDe readily enables computing ADP guarantees under
r-fold sequential composition of the Laplace mechanism, the Gauss
DemonstrationCCS’18, October 15-19, 2018, Toronto, ON, Canada2193what we want. Specifically, we often want ε to be around
0.1 or smaller. One may argue that for some applications
eε ≈ 2.7 (and thus ε ≤ 1) is still tolerable, but cases in
which eε ≥ 22026 (with ε ≥ 10) is reasonable are rare: the
adversary has a 22026 : 1 chance of guessing correctly. As a
consequence, we plot ε up to at most the value of 1 in our
graphs created by PrivacyBuDe.
• δ is the probability mass that may exceed the bound given
by ε. A simple example for a case in which δ is important
occurs when the distributions don’t have the same support,
i.e., when there are events that, although rare, can completely
violate privacy. We typically want to limit δ to the realm of
10−4 or below. Another good sanity check is that for datasets
with N rows or data-points, we want δ < 1
N .
5 CONCLUSION
In this practical demo, we presented PrivacyBuDe, an easy to use
web-based tool for computing tight ADP guarantees. PrivacyBuDe
is pre-configured to compute bounds for the most popular mech-
anisms for achieving differential privacy: the Laplace mechanism
and the Gauss mechanism. Moreover, PrivacyBuDe allows users to
upload a pair of histograms to compute ADP guarantees for other
distributions.
This work is an effort of scientific outreach aimed at rendering
cutting-edge ADP guarantees easily accessible to privacy analysts
and other interested users. We provided the source-code of our
work [8].
ACKNOWLEDGEMENT
This work has been partially supported by the European Com-
mission through H2020-DS-2014-653497 PANORAMIX, the EPSRC
Grant EP/M013-286/1, and the Zurich Information Security Center
(ZISC).
REFERENCES
[1] M. Bun and T. Steinke. Concentrated Differential Privacy: Simplifications, Ex-
tensions, and Lower Bounds. In Theory of Cryptography (TCC), pages 635–658.
Springer, 2016.
[2] I. Dinur and K. Nissim. Revealing Information While Preserving Privacy.
In
Proceedings of the Twenty-second ACM SIGMOD-SIGACT-SIGART Symposium on
Principles of Database Systems (PODS), pages 202–210. ACM, 2003.
[3] C. Dwork and G. N. Rothblum. Concentrated Differential Privacy. CoRR,
abs/1603.01887, 2016.
[4] C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting and differential privacy. In
2010 51st Annual IEEE Symposium on Foundations of Computer Science, pages 51–60.
IEEE, 2010.
[5] P. Kairouz, S. Oh, and P. Viswanath. The composition theorem for differential
privacy. IEEE Transactions on Information Theory, 63(6):4037–4049, 2017.
[6] S. Meiser and E. Mohammadi. Tight on Budget? Tight Bounds for r-Fold Approxi-
mate Differential Privacy. In Proceedings of the 25th ACM Conference on Computer
and Communications Security. ACM, 2018.
[7] I. Mironov. Renyi Differential Privacy. In Proceedings of the 30th IEEE Computer
Security Foundations Symposium (CSF), pages 263–275. IEEE, 2017.
[8] D. Sommer, E. Mohammadi, and S. Meiser. Implementation of privacy buckets,
including FAQ. https://github.com/sommerda/privacybuckets.
[9] D. Sommer, E. Mohammadi, and S. Meiser. Our openly available tool for computing
ADP. http://www.privacybuckets.space.
Figure 3: PrivacyBuDe shows the progress of its computa-
tions and intermediate information in real-time.
mechanism, or a custom mechanism. Figure 1 depicts PrivacyBuDe’s
interface, and Figure 2 shows results that have been exemplarily
computed for the Laplace mechanism (scale = 1, n = 8). Figure 3 is
a screenshot of a progress window that informs about the current
state of the computation in real-time.
The Laplace mechanism applies Laplace noise with scale pa-
rameter λ to the output of a sensitive function f . Since this mecha-
nism was suggested in the very first work that introduced differen-
tial privacy, it is widely used in practice.
The Gauss mechanism applies Gauss noise with variance σ 2 to
the output of a sensitive function f . The Gauss mechanism is often
used in privacy preserving applications due to its better behavior
under truncation: the Gauss distribution falls off much faster than
the Laplace distribution and thus in many cases introduces a smaller
privacy loss δ when truncated.
3.2 ADP for two arbitrary histograms
In addition to the popular Laplace mechanism and Gauss mecha-
nism, we provide a simple interface for computing ADP bounds
for other mechanisms. To use this interface, the user only needs to
provide two histograms H0 and H1 of the outputs of the mechanism
M in question on two worst-case inputs, or, more generally, his-
tograms of the two (worst-case) distributions the user is interested
in analyzing.
4 WHICH PRIVACY PARAMETERS ARE OK?
Tuning and choosing the privacy parameters ε and δ that are still
acceptable is difficult in and by itself. To make our tool more ap-
plicable and to provide some guideline for non-experts, we give
intuition for ε and δ and rules of thumb for its values.
• ε is the logarithm of the multiplicative difference between
outcomes based on two similar inputs. In very simple terms:
we formalize that an adversary should not be able to tell two
cases apart by requiring that any outcome is at most eε times
as likely in one case as in the other case. The adversary has
a eε : 1 chance of guessing correctly. For values of ε close
to 0, both cases are almost equally likely, which is typically
DemonstrationCCS’18, October 15-19, 2018, Toronto, ON, Canada2194