Two clustering criteria. Merging two classes into one cluster
have two effects:
• Coverage loss, since fault-induced misclassfications between
the two classes cannot be detected any more. Hence, we
prefer merging classes with small values in risk matrix R.
We use 𝐶𝑂𝑉𝑙𝑜𝑠𝑠 and 𝑂𝑠𝑎𝑣𝑒 to denote such effects, which are used
in Algorithm 1.
Updating R and C. After selecting two classes or clusters to merge,
we should update the R and C accordingly. Assuming cluster 𝐺𝑖
and 𝐺 𝑗 (𝑖 < 𝑗) are to be merged, we first move classes in cluster
𝐺 𝑗 to 𝐺𝑖, as 𝐺𝑖 = 𝐺𝑖 𝐺 𝑗, delete 𝐺 𝑗 and re-assign the cluster label
for the rest of clusters. Then we update the risk and inconsistency
values of 𝐺𝑖 (𝑖-th row and 𝑖-th column of R and C) as the sum of
the corresponding values of two clusters. Lastly, we delete the 𝑗-th
row and 𝑗-th column in R and C. In this way, we aggregate the risk
probability and inconsistency values of two merged clusters while
preserving the property of the matrices defined in Section 3.
Clustering scheme. We apply a hierarchical clustering algorithm—
agglomerative class clustering, which is illustrated in Algorithm 1.
Specifically, each class of the original task starts in its own cluster.
Then, we search for two clusters with the smallest coverage loss in
R. Note that R (see Figure 3) is usually sparse, in case of multiple
occurrences of the minimum value, we choose the one with the
largest overhead savings from C, which in turn improves the model
consistency between the task model and the checker DNN. Next, we
merge the two selected clusters into one cluster and update R and C.
The above procedure iterates in a bottom-up manner until all classes
of the original task are merged as a single cluster. Consequently,
the clustering results can be presented in a dendrogram with 𝐾 = 2
to 𝐾 = 𝑁 − 1 clustering candidates, which enables later exploration
for the optimal simplified task for a given checker DNN. Figure 4
shows an example of the iterative clustering procedure with five
classes and the generated dendrogram.
5.2 Search Strategy
After obtaining candidate tasks from Algorithm 1, we evaluate the
corresponding overhead savings and coverage loss with fault injec-
tion experiments. As the number of candidate tasks 𝑁 − 1 increases
linearly with the number of original classes, we can evaluate them
efficiently. Afterwards, we have a list of pareto-optimal checker
DNN designs with various coverage/overhead trade-offs. We then
choose the optimal final DNN design.
6 EXPERIMENTAL RESULTS
In this section, we demonstrate the effectiveness of DeepDyve. First,
we present our experimental setup in Section 6.1. After that, our
results that architecture exploration facilitates to find an optimized
checker DNN architecture in Section 6.2, and task simplification
further saves the overhead of DeepDyve in Section 6.3, respectively.
We show DeepDyve outperforms existing solutions in Section 6.4.
At last, in Section 6.5, we discuss the impact of model accuracy
through a case study on CIFAR-100.
6.1 Setup
Datasets. We demonstrate the effectiveness of DeepDyve on four
widely used image classification datasets: CIFAR-10 [19], The Ger-
man Traffic Sign Recognition Benchmark (GTSRB) [39], CIFAR-
100 [19], and Tiny-ImageNet [21]. CIFAR-10 and CIFAR-100 datasets
Figure 5: Risk probability matrices of different models under four million fault injections.
Table 3: Datasets and Task Models.
#Classes
10
43
100
200
Task Model
ResNet-152
ResNet-34
ResNet-152
WideResNet-101
Accuracy
96.15%
99.40%
80.11%
85.20%
FLOPs
3.75 G
1.16 G
3.75 G
22.84 G
Parameter
58.22 M
21.30 M
58.22 M
126.89 M
Dataset
CIFAR-10
GTSRB
CIFAR-100
Tiny-ImageNet
contain 50,000 training images and 10,000 test images, and they
have 10 and 100 classes, respectively. GTSRB has 43 classes of dif-
ferent traffic signs. It has 39,209 training images and 12,630 test
images in total. Tiny-ImageNet is a 200-class natural image dataset
sub-sampled from ImageNet dataset and it contains 100,000 training
and 10,000 validation images.
Models. Table 3 shows the task DNN used by each dataset. For
CIFAR-10, the task DNN model is a ResNet-152 with an accuracy of
95.16%. For GTSRB, the task DNN we use is a ResNet-34 model with
an accuracy of 98.6%. The ResNet-152 for CIFAR-100 has an accu-
racy of 80.11%. The task model for Tiny-ImageNet is WideResNet-
101, and its accuracy is 85.20%. Please note that for Tiny-ImageNet,
we use pre-trained weights on the ImageNet dataset and fine-tune
them to obtain high accuracy. We use Pytorch profiling tool "thop"2
to quantify model GFLOPs and parameters. We quantize all DNN
parameters into 8-bit integers (INT-8) following a uniform affine
quantitizer [18].
Fault Model. In our experiment, we use two types of fault injection:
random fault injection and BitFlip Attack (BFA) [33]. For random
fault injection, in each simulation run, we randomly flip 𝑛 bits in
the model and pass one randomly selected image to the DNN model
for inference. BFA proposed in [33] is the state-of-art fault injection
attack on DNN models. It can crash the DNN system by injecting
a small number of bit-flips by searching the most vulnerable bit
progressively. A failure occurs when the predicted label is different
from the one obtained in the fault-free case.
Risk Impact Martrix. In practice, the risk impact matrix should
be determined by system designers after conducting application-
specific risk analysis. One practical solution would be categorizing
the risk impact into a few risk levels and filing the matrix accord-
ingly. In our experiments. we simulate the impact matrix with two
configurations.
• Uniform Impact, where all entries are ones. It represents
that the risk impact among all classes are equal. When the
risks of different classes do not have significant differences,
the uniform impact matrix can be used for simplicity.
• Non-uniform Impact, where the risk impact values are set
to two different levels. As classes with the low precision are
not trustworthy themselves and hence have low risk, in this
configuration, we assign those classes with the lowest 25%
precision with impact 1 and the others with 100.
Risk Probability Matrix. We obtain the risk probability matrix
and the failure coverage of the checker DNN with random fault
injection experiments. Figure 5 shows the risk probability matrices
of different state-of-art model architectures trained on CIFAR-10.
We perform 4 million fault injections to obtain the result in this
figure3. For visualization purposes, the probability matrix is divided
by the maximum element. The sum of all elements in the original
probability matrix is 1.
We can observe that the risk probability matrix is more task-
related than model-related. The probability distributions are very
similar across different DNN models on the same CIFAR-10 task.
For example, in Figure 5 (a), the value between dog and cat is the
highest one with fault injections. It is also true for Figure 5 (b) and
(c). This matrix will be used in the task simplification process.
6.2 Effectiveness of Architecture Exploration
In this part, we study the effectiveness of the architecture explo-
ration.
Figure 6 shows the consistency, indicated by blue points and
approximated by blue curve, and the computational overhead, indi-
cated by red points and connected by the red curve, of checker DNN
models with different sizes trained by DeepDyve. Please note that
we only investigate five checker model sizes for Tiny-ImageNet due
to high training effort for this dataset, including the pre-training
on ImageNet dataset and fine-tuning on Tiny-Imagenet. For all the
four datasets, the consistency between the task and checker models
2https://github.com/Lyken17/pytorch-OpCounter
3More fault injections are performed, and the results are similar.
planecarbirdcatdeerdogfroghorseshiptruck0.000.000.300.160.020.070.090.270.340.150.170.000.000.000.000.000.020.000.100.300.120.040.000.610.330.270.390.180.100.110.240.340.160.000.281.000.490.130.100.000.010.000.430.170.000.510.060.200.040.010.000.000.150.800.180.000.120.450.090.050.110.090.160.600.000.050.000.000.010.060.150.000.340.090.060.320.050.000.010.000.180.090.050.130.000.150.280.000.000.160.040.630.000.050.060.000.000.000.210.000.000.040.480.200.080.070.260.040.310.130.230.000.000.000.000.000.000.020.330.430.760.000.000.210.150.150.250.260.050.010.190.000.190.000.321.000.620.530.210.100.020.000.110.140.000.120.260.040.080.000.380.000.090.850.320.000.040.480.020.000.030.000.340.240.130.100.000.000.010.000.160.000.140.390.480.170.000.000.010.000.750.230.040.050.030.050.010.000.000.310.100.600.020.200.070.100.000.000.140.00planecarbirdcatdeerdogfroghorseshiptruck0.000.050.120.110.090.030.070.060.220.090.080.000.010.000.000.000.020.030.120.180.270.000.000.320.150.270.340.030.040.000.130.030.480.000.231.000.400.200.070.040.060.000.240.490.000.220.180.160.000.050.050.060.100.720.230.000.270.260.000.040.020.010.290.300.220.100.000.080.040.030.010.000.180.150.170.350.050.000.020.040.270.150.010.160.030.050.010.040.000.080.160.370.000.040.020.000.010.020.170.000.00.20.40.60.81.0planecarbirdcatdeerdogfroghorseshiptruck(a) ResNet-152(b) VGG-16(c) MobileNetPredicted LabelTrue LabelFigure 6: Consistency v.s. overhead in the architecture exploration stage.
Table 4: Task simplification further shrink the overhead.
Dataset
CIFAR-10
GTSRB
CIFAR-100
Tiny-ImageNet
Impact Matrix
non-uniform
uniform
non-uniform
uniform
non-uniform
uniform
non-uniform
uniform
Start
Consistency
91.62%
98.75%
75.82%
79.19%
After TaskSim
Before TaskSim.
k
O(C) Wcov
O(C) Wcov
8
86.12%
6.88%
86.94%
9.18%
9
9.11%
75.90%
9.18%
75.90%
23
98.23%
1.94%
98.46%
2.68%
33
2.68%
2.56%
98.15%
98.15%
83
27.48% 67.29% 24.02% 66.92%
27.48% 74.33% 27.42% 74.33%
99
36.91% 76.40% 35.56% 75.19% 186
36.91% 78.03% 36.84% 78.02% 198
improves with the increasing size of checker DNNs. Also, as we can
observe, there is a turning point on the computational overhead
curve. Before the turning point, the re-computation dominates the
computational overhead O(C), and after which, the checker models’
computational cost dominates the O(C).
First, compared to the architecture with the highest consistency
(i.e., duplication), the optimized architecture can greatly reduce the
overhead. For CIFAR-10, GTSRB, CIFAR-100, and Tiny-ImageNet,
91.82%, 97.32%, 72.52% and 63.09% overhead can be saved with 8.38%,
1.25%, 24.18% and 20.81% consistency degradation, respectively
(see Table 4). We also observe that the consistency values vary for
different datasets. After manual checking, we found the consistency
values of the optimized architecture on the training set are almost
100%, but it generalizes differently during inference for the four
data sets.
that the optimal point is 3√︃ 𝑎
Second, the relation between consistencies, architecture sizes,
and the optimal point is well captured by THEOREM 4.1 . For
example, the resulting consistency function for CIFAR-10 is 𝑓 (𝛼) =
− 0.003
𝛼 + 0.94, and hence the optimal point is when 𝛼 = 0.11 (recall
𝑥 ). This is compatible with the red
curve where 𝛼 = 0.11 almost renders the minimal computational
overhead. Similarly, the consistency function for GTSRB is 𝑓 (𝛼) =