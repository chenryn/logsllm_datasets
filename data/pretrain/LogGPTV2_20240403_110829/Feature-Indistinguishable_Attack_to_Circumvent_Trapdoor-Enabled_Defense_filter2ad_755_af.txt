64
94.0% 94.8% 95.6% 98.3% 97.5% 97.7%
96.2% 96.2% 96.3% 99.5% 99.5% 99.5%
15.7
63.6
30.9
61.7
16.0
31.9
32
Table 7: ASR and GSR of FIA with single-category P-TeD at
5% FPR when the generation layer (𝐿𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛) mismatches
the detection layer (𝐿𝐷𝑒𝑡𝑒𝑐𝑡𝑖𝑜𝑛).
Table 9: FIA’s ASR with and without the drive-away loss
when P-TeD protects single category and all categories at 5%
FPR.
Dataset
MNIST
CIFAR10
GTSRB
LDetection
dense
atten_1
dense_1
dense_1
LGeneration
max_pool_1
activation_18
conv_5
max_pool_2
max_pool_3 max_pool_2
ASR GSR
95.0% 83.1%
93.8% 87.7%
99.1% 94.8%
97.0% 91.0%
88.1% 82.8%
MNIST
GTSRB
CIFAR10
With / Without Drive-away Loss (%)
Single Category
94.8% / 92.4%
96.9% / 82.3%
100% / 100%
All Categories
97.5% / 96.8%
99.6% / 93.7%
100% / 100%
issue can be alleviated by increasing the bound or using a more
accurate estimate of trapdoor signatures.
Since PGD can be readily detected by the trapdoored defense,
adversarial examples generated with PGD should have a more
accurate estimate of trapdoor signatures than adversarial examples
with FIA. We can replace the positive adversarial examples used
in FIA+G with those generated by PGD (without querying the
trapdoored defense since the detection rate is very high for PGD).
This FIA variant is denoted as FIA+GP (FIA+G with PGD to generate
positive adversarial examples) in Table 6. The rightmost column
in Table 6 reports the experimental result for FIA+GP. We can
see that ASR improves signicantly over FIA+G for the last two
layers tested but very small for the two early layers tested. This
is because adversarial examples generated with PGD deviate from
trapdoor signatures more and more when the generation layer
(i.e., the detection layer) moves backward, eventually there is no
dierence between adversarial examples generated with PGD and
FIA in estimating trapdoor signatures.
We note that the trapdoored defense becomes less eective in
detecting adversarial examples when the detection layer is signi-
cantly away from the penultimate layer.
6.6 Layer Mismatch
In previous experiments, the generation layer and the detection
layer are actually the same layer. We have conducted experiments
to study FIA’s performance when the generate layer mismatches
the detection layer. In these experiments, we deliberately let the
generation layer before the detection layer. Table 7 shows the ex-
perimental results on MNIST, CIFAR10, and GTSRB when P-TeD is
used to protect a single category at 5% FPR. We can see from the
table that FIA maintains a high ASR (above 88%) and a reasonably
high GSR (above 82%) when mismatch occurs.
6.7 Ablation Study
We have also conducted an “ablation study” of the impact of hyper-
parameters and the drive-away loss on FIA’s performance. The
parameters that are determined by experimental data, such as target
F 𝐶𝑡
𝐿 , constraint boundary 𝑐𝑝, etc. are excluded from this study
since they cannot be manually set. Only the hyper-parameters that
might have a signicant impact are reported in the paper. Hyper-
parameters not reported can deviate from the empirical values we
used in our experimental evaluation with a minor or negligible
impact on FIA’s performance.
Table 8 shows FIA’s performance on MNIST with P-TeD pro-
tecting single category and all categories at 5% FPR when a single
batch with dierent batch sizes is used in the preparation phase.
The resulting average number of queries in this phase is also re-
ported in the table. From the table, we can see that the batch size
has negligible impact on FIA’s performance. Similar results are also
observed with other datasets. This result indicate that a small batch
size can be used in the preparation to reduce the number of queries.
We have also studied the performance when the drive-away
loss term is removed. Table 9 reports FIA’s ASR with and without
the drive-away loss when P-TeD protects single category and all
categories at 5% FPR. We can see from the table that the drive-away
loss has a small or negligible impact for MNIST and CIFAR10 (at or
below +2.4%), but a reasonably signicant impact for GTSRB (up
to +14.6%). Nevertheless, the ASR without the drive-away loss is at
least reasonably high for all the tested datasets.
7 DISCUSSION
7.1 Fortifying TeD?
A question naturally arises: is it possible to fortify TeD to thwart
FIA? For the FIA described in Section 5, we can fortify TeD with
random trapdoor signatures and detection layers: to protect a target
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3170Table 10: Detection rates of FIA and fortied FIA on GTSRB
with P-TeD and fortied P-TeD (with two trapdoors or two
detection layers) protecting all categories at 5% FPR.
Attack
Defense
2 Trapdoors
2 Layers
TeD
3.80%
0.44% / 6.11%
FIA
Fortied-FIA
Fortied-TeD Fortied-TeD
75.41%
99.98%
6.83%
8.09%
category, we inject multiple trapdoors and detect at multiple layers,
each time using a trapdoor and a detection layer; and we change
them from time to time. If a single trapdoor or a single layer is
used when FIA queries TeD in the preparation phase, only the
activated trapdoor signature or detection layer is explored by FIA.
Adversarial examples crafted by FIA may be detected when an
unexplored trapdoor or detection layer is used.
The above fortied TeD can be evaded with a fortied FIA scheme
by querying TeD repeatedly using the complete FIA scheme until all
trapdoor signatures and detection layers are explored, and then FIA
applies all positive adversarial examples to drive simultaneously at
all layers determined to be potentially used as the detection layer.
This is similar to the FIA scheme to attack TeD with randomly
sampled neurons and multiple trapdoor signatures per category
that is described in Section 5.3.
We have conducted experiments to study the performance with
fortied TeD and fortied FIA. Table 10 reports the experimental re-
sults on GTSRB when the fortied TeD uses two trapdoors at layer
max_pool_3 or two detection layers (dense_1 and max_pool_2) to
protect all categories at 5% FPR, with one used in FIA’s preparation
phase and the other used at detecting crafted adversarial exam-
ples. The results with the original TeD and FIA are also reported
in Table 10. We can see from the table that the detection rate of
the original TeD on the original FIA is 3.8%. The detection rate is
boosted to 75.41% when two trapdoors at layer max_pool_3 are
used in the fortied TeD: one trapdoor is used at FIA’s preparation
phase and the other is used at evaluating the detection rate on
crafted adversarial examples. The detection rates of the original
TeD at layer dense_1 and layer max_pool_2 are 0.44% and 6.11%,
respectively. The detection rate is boosted to 99.98% when the forti-
ed TeD uses layer dense_1 at FIA’s preparation phase and layer
max_pool_2 to evaluate the detection rate with crafted adversarial
examples. When the fortied FIA is used to attack the fortied TeD,
the detection rate reduces to 6.83% and 8.09% when the fortied
TeD uses the two trapdoors or the two detection layers. From the ex-
perimental results, we believe that it is almost impossible to fortify
the trapdoored defense to eectively detect FIA-like attacks.
category to drive into. In general, untargeted adversarial exam-
ples are easier to craft than targeted adversarial examples. One
can always use the aforementioned approach to convert a targeted
adversarial attack into an untargeted adversarial attack.
The current FIA relies on an over-simplied assumption of con-
vex region to simplify the FIA scheme. This assumption can be
relaxed if we adopt the approach proposed in [30], which uses
neural networks to model layer-wise and category-wise feature
distributions. This approach should be much more general and
powerful than the current FIA version, resulting in more powerful
adversarial attacks with better indistinguishability from benign
target samples. This will be our future work.
Unlike existing adversarial attacks, our proposed adversarial at-
tack aims to make generated adversarial examples indistinguishable
in the feature space from benign samples in the target category. We
argue that this is a more powerful approach than existing adver-
sarial attacks. This indistinguishability makes crafted adversarial
examples likely much harder to detect than existing adversarial
attacks. Our study proves the feasibility of this approach. We expect
that FIA should be able to circumvent other powerful methods to
detect adversarial examples. This will also be our future work.
With more powerful adversarial attacks like FIA, we need to nd
an eective defense to thwart them. Developing eective defense
against FIA-like attacks will be our future work too.
8 CONCLUSION
The recently proposed Trapdoor-enabled Detection (TeD) [51] can
eectively detect existing state-of-the-art adversarial attacks. To
circumvent TeD, we present a novel black-box adversarial attack,
called Feature-Indistinguishable Attack (FIA), which aims to gen-
erate adversarial examples indistinguishable in the feature space
from benign examples in the target category. It jointly minimizes
the distance to the expectation of feature vectors of benign exam-
ples in the target category and maximizes distances to positive
adversarial examples generated to query TeD in the preparation
phase. A constraint is used to ensure that the feature vector of a
generated adversarial example is within the distribution of feature
vectors of benign examples in the target category. Our extensive
empirical evaluation indicates that our proposed FIA can eectively
circumvent the strongest defense provided by TeD and its improved
variant, Projection-based Trapdoor-enabled Detection (P-TeD), which
is also proposed in this paper. To the best of our knowledge, FIA is
the rst adversarial attack that aims to craft adversarial examples
indistinguishable from benign target examples in the feature space.
Adversarial examples crafted with this approach should be much
harder to detect. It opens a door for developing much more power-
ful adversarial attacks. It will also inspire researchers to develop
more eective defense against FIA-like attacks.
7.2 FIA for Untargeted Adversarial Examples
and Future Work
FIA is described under the context of crafting targeted adversarial
examples. With minor modications, it can also be used to craft
untargeted adversarial examples. For example, to generate an untar-
geted adversarial example, we can compare the input with benign
samples in dierent categories in the feature space to nd a closest
ACKNOWLEDGMENTS
This work was supported in part by National Natural Science Foun-
dation of China under Grant No. 61771211, Fundamental Research
Funds for the Central Universities under Grant No. 2017KFYXJJ064,
the Wuhan Applied Foundational Frontier Project under Grant No.
2020010601012188, and the Guangdong Provincial Key R&D Plan
Project under Grant No. 2019B010139001.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3171REFERENCES
[1] Anish Athalye, Nicholas Carlini, and David A. Wagner. 2018. Obfuscated Gra-
dients Give a False Sense of Security: Circumventing Defenses to Adversarial
Examples. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 (Proceedings
of Machine Learning Research), Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.
PMLR, 274–283. http://proceedings.mlr.press/v80/athalye18a.html
[2] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis,
Aditya Nori, and Antonio Criminisi. 2016. Measuring neural net robustness with
constraints. In 30th Conference on Neural Information Processing Systems (NIPS
2016).
[3] Avishek Joey Bose and Parham Aarabi. 2018. Adversarial attacks on face detectors
using neural net based constrained optimization. In 2018 IEEE 20th International
Workshop on Multimedia Signal Processing (MMSP). IEEE, 1–6.
[4] Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2018. Decision-based
adversarial attacks: Reliable attacks against black-box machine learning models.
In International Conference on Learning Representations ICLR.
[5] Jacob Buckman, Aurko Roy, Colin Rael, and Ian Goodfellow. 2018. Thermometer
encoding: One hot way to resist adversarial examples. In International Conference
on Learning Representations.
[6] Nicholas Carlini. 2020. A partial break of the honeypots defense to catch adver-
sarial attacks. arXiv preprint arXiv:2009.10975 (2020).
[7] Nicholas Carlini and David Wagner. 2016. Defensive distillation is not robust to
adversarial examples. arXiv preprint arXiv:1607.04311 (2016).
[8] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not eas-
ily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM
Workshop on Articial Intelligence and Security. 3–14.
[9] Nicholas Carlini and David Wagner. 2017. Magnet and "ecient defenses against
adversarial attacks" are not robust to adversarial examples. arXiv preprint
arXiv:1711.08478 (2017).
[10] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. In 2017 IEEE Symposium on Security and Privacy, SP 2017,
San Jose, CA, USA, May 22-26, 2017. IEEE Computer Society, 39–57. https:
//doi.org/10.1109/SP.2017.49
[11] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. 2018.
EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples.
In Proceedings of the Thirty-Second AAAI Conference on Articial Intelligence,
(AAAI-18), Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI Press,
10–17. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16893
[12] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau. 2018.
Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector.
In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases. Springer, 52–68.