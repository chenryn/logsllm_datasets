No.1 result, all others are linked to SEO pages of SSP.
pages were modiﬁed to ensure there is no input box
that accepts user’s sensitive information (e.g., credit card
number) when a real user accidentally visits our site.
Server logging functions were turned on to record infor-
mation of incoming requests like time, user agent, IP ad-
dress, and etc. We ﬁltered out logs that do not belong to
search crawlers using user agent patterns. Then, we reg-
istered an account in SSP, uploaded the homepage link
, and started an advertising campaign of 14 days (from
Nov 1st, 2015 to Nov 14th, 2015) for our testing web
site.
Exploring spider pool infrastructure. Several days af-
ter starting our campaign, we searched the URL of our
site in Google. The results are shown in Figure 3. Our
URL was listed in the results, together with URLs of
SEO pages set up by the spider pool. This ﬁnding mo-
tivates us to explore the whole spider pool infrastructure
by iteratively sending queries to Google, which neverthe-
less turns out to be very time-consuming, as described
in Appendix A. Therefore, we start to explore the in-
frastructure through other means. As discussed in Sec-
tion 2.1, well-structured web site (e.g., a site providing
sitemap ﬁle) can be visited by crawlers frequently and
indexed timely. We sampled several SEO domains and
found this was also followed by SSP: a sitemap.html
ﬁle was put under the root directory of every SEO do-
main to instruct the crawler to ﬁnd other SEO or cus-
tomer domains. An example of the sitemap ﬁle is shown
in Figure 4. This feature motivates us to build a dedicated
crawler which follows sitemap ﬁle to excavate sites be-
longing to SSP.
First, we feed a previously discovered SEO domain as
a seed into Q (the queue of domains to be searched). Our
crawler starts from requesting the sitemap.html under
root folder and extracts all the links inside 
tags. The domains of the links which have not been vis-
ited will be appended to Q. The sitemap ﬁle is refreshed
twice, to include domains added dynamically by the SEO
toolkit. The crawler then visits the next domain in Q,
checking if the domain is a SEO domain and extracting
all other domain names, until there is no more domain
left. With this approach, we are able to harvest a large
number of domains in SSP under moderate time.
USENIX Association  
25th USENIX Security Symposium  249
5
  摄影包排名-网站地图-apple-iw.cn 
… 
北京摄影培训指南 
同学聚会摄影 
全国摄影工作室 
成都 摄影 培训 机构 
圣枫莎摄影工作室 
摄影作品集封面欣赏 
… 
Keywords about photo shooting 
Wildcard subdomain 
Figure 4: An example of sitemap ﬁle.
3.3 Features of Spider Pool
The preliminary exploration offers us a partial view of
SSP, and we manually examined the domains included
by SSP (we demonstrate how to automatically classify
the domains in Section 3.4). In particular, we select the
domains used for SEO and present the interesting ﬁnd-
ings below:
Wildcard DNS usage. As expected, wildcard DNS
plays an important role in spider pool and greatly im-
proves its scalability: the total number of SEO FQDNs
is up to 44,054 but the number of SEO domains is only
514 5. In other words, the size of the spider pool is in-
ﬂated 86 times through this technique. We also ﬁnd that
DGA (Domain generation algorithm) is incorporated in
domain generation: most of the fabricated subdomain
names are 5-10 characters ﬁlled with random letters and
digits (see domain names in Figure 2).
Content generation. The effects of wildcard DNS on
SEO have been discussed by SEO community. In fact,
replicating same pages under different subdomains will
delay the indexing of legitimate content as Google con-
siders the crawl budget is wasted on these site [14, 33].
However, if pages on subdomains are all distinctive, the
site would not be penalized [3]. Therefore, spider pool
site dynamically renders page content for each visit.
To understand how the content is generated, we in-
spected a copy of spider pool toolkit shared for free on
a public cloud drive 6. The SEO pages generated by this
toolkit resembles the SEO pages of SSP.
In fact, the
toolkit includes a dictionary of long-tail keywords and
text scraped from trending ﬁctions, and randomly assem-
bles keywords and sentences to construct the SEO pages.
To make the page look more legitimate, the toolkit em-
5We fold the FQDN to domain name using Public Sufﬁx List [36].
For instance, www.abcd.example.com is folded to example.com.
6http://pan.baidu.com
Keywords related to melon seeds 
Text combined from fiction and keywords 
Figure 5: An example of SEO pages within SSP.
beds images from the local storage. Another interesting
discovery is that cloaking technique is not used, so the
same contents are presented to both search crawlers and
normal browsers. We veriﬁed that by manipulating the
user-agent string of our crawler. Figure 5 shows an ex-
ample of the SEO page.
Link structure. Each spider pool site provides a sitemap
page (sitemap.html) to direct search crawler to visit
other sites according to its intention. By connecting
the discovered spider pool domains under linking rela-
tions, we ﬁnd that a strongly connected graph can be con-
structed in which there exists a route from any domain to
another. In addition, most of the links on a SEO page are
swapped per visit. To a search crawler, such link struc-
ture looks like an ever-changing labyrinth: every step the
crawler makes refreshes the whole link topology, so the
crawler is always trapped till arriving at customer’s site.
Leveraging this design, the customer’s site connected to
the spider pool can be indexed much faster and updates
on the site can be timely reﬂected in search results. For
a site running illegal business, such feature is quite ben-
250  25th USENIX Security Symposium 
USENIX Association
6
How to make fake CET-4/6 certificate 
Figure 6: An example of site free-ride.
eﬁcial as it allows the site to appear quickly in search
results before being taken down.
Domains controlled by adversary are limited and a
search crawler can realize when it is trapped by checking
if it keeps visiting the same set of domains. However,
due to wildcard-DNS techniques, loop detection is much
more difﬁcult as a random FQDN can be easily generated
and inserted into the path at any time.
Site free-ride. We also discovered a new type of pro-
motion method developed to show customer’s message
in search result. This is achieved through crafting a
search URL under a popular site with message ﬁlled
in query string and injecting it into the spider pool.
Since some popular sites targeted by adversary display
the query string in their search result no matter whether
meaningful content is returned or not, the message em-
bedded in query string will be fed into search crawler.
Also due to the site’s high PR value, the indexing pro-
cess is much faster and message is more likely to ap-
pear in top of search results. As an example, we found
one Amazon search URL 7 in a SEO page which con-
tains a message for an illegal business (selling counter-
feit certiﬁcate) with contact information (QQ number,
one of the most popular instant messenger in China).
Querying with the relevant keywords (“certificate
for CET-4/6 qq” translated from Chinese) in Google
returns the customer’s message at the top spot of search
results (see Figure 6).
We call this SEO trick “site free-ride”, as the reputa-
tion of a popular site is abused by blackhat SEO while
that site is neither compromised nor spammed at all,
which makes detection rather difﬁcult. In Section 5.3,
we show many top sites are misused in this way.
3.4 Classifying Spider Pool Domains
Following the trail of sitemap, different types of domains
are encountered, including SEO domains, customer site
domains, domains abused for message promotion and
other innocent domains. Though they can be distin-
guished through manual investigation, such approach is
not scalable. Therefore, we developed a classiﬁer to dif-
ferentiate these cases automatically.
We started from identifying SEO domains. We found
7http://www.amazon.com/s?ie=utf8&page=1&rh=
[MESSAGE]
7
that SEO domains are all powered by wildcard DNS. In
addition, the page content is changed per visit while the
content from other types of sites is much more stable.
Therefore, we limited the scope to wildcard DNS domain
and compare the sitemap ﬁles for two consecutive visits
to determine if the domain was a SEO domain.
A problem we need to address here is how to measure
the difference between two visits. We ﬁrst attempted
to measure the text difference between the two HTML
pages, but we encountered a large number of false pos-
itives caused by dynamic content, e.g., online advertise-
ments. We improve this method by considering the dif-
ference of only hyperlinks between two pages. To dis-
card small changes on URLs of hyperlinks customized to
visitors, we normalize the URLs by removing the query
parameters and values. Taking two pages PA and PB as
an example, assuming the set of unique hyperlink URLs
in PA and PB are HA and HB, we compute |HA−HB|
and
|HA|
|HA−HB|
respectively and then compare the maximum
value to a threshold MinH.
If the maximum value is
larger than MinH, the domain is labeled as a SEO do-
main. We set MinH to 20% based on the empirical tests
and it renders good accuracy.
|HB|
The above method worked well for exploring SSP. In
Section 5.2, we tested 20 other spider pools using the
same detector with small changes. We found the home-
page is also used to guide search crawlers by some spider
pools. Therefore, the homepage was also inspected by
the detector.
After picking out SEO domains, our tool classiﬁes the
remaining domains to the following three types:
• The sites abused for message promotion are iden-
tiﬁed ﬁrst by using the URL patterns related to
search queries. By inspecting a large corpus of
message URLs, we identify 45 URL patterns, like
/search/, and use them to match all remaining
URLs.
• Then, the innocent sites are identiﬁed by match-
ing their domain names with Alexa top 1M site list.
Though some of them may use spider pool, we be-
lieve the chance is small if they want to keep good
reputation.
• The sites remained are classiﬁed as customer sites.
Our detection mechanism only relies on the feature on
link structure and is robust against different templates
and languages used by SEO pages. To evade our de-
tector, the adversary could suppress the change rate of
the SEO page for each visit, which however will make
the link structure less dynamic and more likely to be de-
tected by search engines. Another evasion is to disable
wildcard DNS support on SEO domains. Although we
USENIX Association  
25th USENIX Security Symposium  251
have found some spider pools are beginning to apply this
change, the majority of the SEO domains are wildcard
DNS powered, suggesting the adversaries are not plan-
ning to give up the beneﬁts brought by wildcard DNS.
4 Detecting SEO Domains through DNS
Scanning
Motivated by the ﬁndings from inﬁltrating SSP, we im-
plemented a scanner aiming to discover and measure
SEO domains of spider pool from an internet-wide view.
Though the detector described in Section 3 is effective in
enumerating SEO domains of spider pool, it highly de-
pends on the seeded domains and the discovery is still
limited.
Instead, we launched large-scale DNS scan-
ning to identify wildcard DNS domains and crawled their
sitemaps and homepages. Then, we applied differential
analysis to detect SEO domains, using the heuristics de-
scribed in Section 3.4. The process is elaborated as fol-
lows.
4.1 Data Source
We cluster the SEO domains employed by SSP according
to their TLDs and SLDs and examine their popularity.
Unsurprisingly, a large number of domains are under old
generic TLD (gTLD) like .com and country-code TLD
(ccTLD) like .cn. Interestingly, we also found a notice-
able number of domains under new gTLDs [45] like .xyz
and generic SLDs like .com.cn [9]. In the end, we de-
cide to scan domains under all these categories described
above.
For each TLD or SLD under study, we attempt to gain
access to its DNS zone ﬁle ﬁrst. If zone ﬁle is not avail-
able, we resorted to the domain collection offered by
third party and passive DNS data source. For .com, we
downloaded zone ﬁle from Verisign in Dec 2015 through
its TLD Zone File Access Program [47]. The unique
count of domains is over 125 million and we sample
2 million (1.6%) for study. We also apply for the ac-
cess to zone ﬁles of new gTLDs and several old TLDs
through Centralized Zone Data Service (CZDS) man-
aged by ICANN [23]. We obtained zone ﬁles from 10
registries for most popular new gTLDs in Dec 2015. The
.cn zone ﬁle was retrieved from viewdns.info [48] in
Dec 2015 (the zone ﬁle has over 5.5M domains, account-
ing for around 64% of all 8.6M .cn domains [12]). For
the remaining TLDs, we searched passive DNS database
provided by Farsight Security [41] and extracted do-
mains from all A records within 2015. The list of TLDs
and the statistics are shown in Table 1 8.
8We did not ﬁnd SEO domains under .gov, .edu, .edu.cn and
.gov.cn in SSP. They are only scanned for comparison.
4.2 Detection System
To detect wildcard domains, we implement a DNS scan-
ner to probe all domains (13.5 million) in our list. For
any name (e.g., a.com) in the list, our scanner issues
a wildcard A record query (*.a.com) to its authorita-
tive server directly, bypassing resolvers provided by local
ISP. If we get a valid IP (same as the one used by a.com),
we consider that the domain supports wildcard. To pre-
vent ISP’s DNS wildcard injection with its web portal’s
IP, we detect such behavior before our large scale prob-
ing, and ﬁlter out such IP address used by local ISP.
It is challenging to handle such large volume of do-
mains we collected. To optimize the performance, we
choose to issue DNS queries and process responses asyn-
chronously.
In particular, the scanner runs one thread
unremittingly issuing DNS requests in UDP and another
thread picking up matched DNS responses from incom-
ing trafﬁc. Our task was able to ﬁnish within 120 hours
and we report the result in Table 1.
We obtained 2.4 million wildcard domains (17.8% of
all scanned domains) after the scanning process and the
next step is to determine which domains are used for spi-
der pool. To this end, we use crawler to visit the sitemap
(or homepage when sitemap is not available) of the root
folder of each wildcard domain twice, then compare the
hyperlinks using the same heuristics described in Sec-
tion 3.4. The domains showing different set of hyper-
links are considered SEO domains. In the end, we cap-
tured 458K (19.1% of all wildcard DNS domains) such
domains in total. This result suggests an unnegligible
proportion of domains have conﬁgured wildcard DNS for
SEO purposes.
We further classify the detected SEO domains based
on how the pages are linked to customer’s content and
identify three different types. All of them can be gen-
erated from the templates of known spider pool toolk-
its [22]. We elaborate each type in Appendix B. The
number under each type is reported in #iframe, #link and
#redir columns in Table 1.
Veriﬁcation. Our detection system discovered a large
amount of SEO domains and we want to know how ac-
curate the outcome is. Counting the true positives among
them accurately is nearly impossible because there is no
off-the-shelf detection system we can use as the oracle
and it will take massive human efforts to examine all of
them. As an alternative, we sample 1,000 SEO domains
at maximum per TLD/SLD and evaluate them to estimate
the accuracy. We manually look into the title, page text
and link structure, and compare them with known SEO
pages from SSP. We consider one as true positive if these
features resemble. We ﬁnd the highest false-positive rate
(FPR) per TLD/SLD occurs on .com, which is 1.2%.
The main reason for the false positives is that many links
252  25th USENIX Security Symposium 
USENIX Association
8
Table 1: Data source for DNS probing and the scanning results. “Zone ﬁle*” means the domains are sampled from all domains in
zone ﬁle. #All SEO is the number of all detected SEO domains. It is also the sum of #iframe, #link and #redir.
Category
Old TLD
New gTLD
SLD
Total
TLD&SLD
cc
cn
com
edu
gov
pw
xyz
top
wang
win
club
science
ren
link
party
click
ac.cn
edu.cn
gov.cn
org.cn
com.cn
net.cn
-
Data Source
Passive DNS
Domain List
Zone ﬁle*
Passive DNS
Passive DNS
Passive DNS
Zone ﬁle
Zone ﬁle
Zone ﬁle
Zone ﬁle
Zone ﬁle
Zone ﬁle
Zone ﬁle
Zone ﬁle
Zone ﬁle
Zone ﬁle
Passive DNS
Passive DNS
Passive DNS
Passive DNS
Passive DNS
Passive DNS
#All
99,934
4,107,679
2,000,000
3,619
1,215
246,775
1,695,430
1,033,644
629,757
570,091
535,576
249,187
237,372
213,449
217,508
181,673
86,291
1,347
12,364
53,492
1,093,580
234,039
#Wildcards
40,334
371,829
751,877
353
130
146,984
309,983
127,532
49,528
56,020
108,665
44,544
11,023
40,719
68,239
43,914
56,944
173
1,745
5,206
152,751
12,445
#All SEO
10,320
222,390
37,568
0
0
43,561
45,769
9,842
6,597
22,115
2,163
2,271
121
490
3,650
1,059
24,270
0
24
805
23,264