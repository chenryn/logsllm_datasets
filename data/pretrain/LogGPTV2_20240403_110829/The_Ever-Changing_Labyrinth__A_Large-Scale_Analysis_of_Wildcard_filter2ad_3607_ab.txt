### No. 1 Result, All Others Linked to SEO Pages of SSP

The pages were modified to ensure that no input box accepts sensitive user information (e.g., credit card numbers) when a real user accidentally visits our site. Server logging functions were enabled to record incoming request information such as time, user agent, IP address, and more. We filtered out logs that did not belong to search crawlers using user agent patterns. Then, we registered an account in the Search Spider Pool (SSP), uploaded the homepage link, and initiated a 14-day advertising campaign (from November 1, 2015, to November 14, 2015) for our test website.

### Exploring Spider Pool Infrastructure

Several days after starting our campaign, we searched the URL of our site on Google. The results, shown in Figure 3, listed our URL along with URLs of SEO pages set up by the spider pool. This finding motivated us to explore the entire spider pool infrastructure by iteratively sending queries to Google, which proved to be very time-consuming, as detailed in Appendix A. Therefore, we began exploring the infrastructure through other means.

As discussed in Section 2.1, well-structured websites (e.g., those providing a sitemap file) are frequently visited by crawlers and indexed in a timely manner. We sampled several SEO domains and found that the SSP also follows this practice: a `sitemap.html` file is placed in the root directory of each SEO domain to guide the crawler to other SEO or customer domains. An example of the sitemap file is shown in Figure 4. This feature inspired us to build a dedicated crawler that follows the sitemap to uncover sites belonging to the SSP.

### Crawler Implementation

We initialized the crawler by feeding a previously discovered SEO domain as a seed into a queue (Q). The crawler starts by requesting the `sitemap.html` file under the root folder and extracting all links within `<a>` tags. The domains of these links, if not yet visited, are added to Q. The sitemap file is refreshed twice to include dynamically added domains by the SEO toolkit. The crawler then visits the next domain in Q, checks if it is an SEO domain, and extracts all other domain names until no more domains remain. This approach allows us to efficiently harvest a large number of domains in the SSP.

### Features of Spider Pool

Our preliminary exploration provided a partial view of the SSP, and we manually examined the included domains (we demonstrate how to automatically classify these domains in Section 3.4). Specifically, we focused on the domains used for SEO and present the following findings:

#### Wildcard DNS Usage
As expected, wildcard DNS plays a crucial role in the spider pool, significantly improving its scalability. The total number of SEO Fully Qualified Domain Names (FQDNs) is 44,054, but the number of unique SEO domains is only 514.5. This technique inflates the size of the spider pool by 86 times. We also found that a Domain Generation Algorithm (DGA) is used in domain generation, with most fabricated subdomain names being 5-10 characters long, filled with random letters and digits (see domain names in Figure 2).

#### Content Generation
The impact of wildcard DNS on SEO has been discussed by the SEO community. Replicating the same pages under different subdomains can delay the indexing of legitimate content as Google considers the crawl budget wasted on these sites [14, 33]. However, if the pages on subdomains are distinct, the site is not penalized [3]. Therefore, the spider pool site dynamically renders page content for each visit.

To understand how the content is generated, we inspected a copy of the spider pool toolkit shared on a public cloud drive. The SEO pages generated by this toolkit resemble those of the SSP. The toolkit includes a dictionary of long-tail keywords and text scraped from trending fictions, and randomly assembles keywords and sentences to construct the SEO pages. To make the page look more legitimate, the toolkit embeds images from local storage. Another interesting discovery is that cloaking techniques are not used, so the same content is presented to both search crawlers and normal browsers. We verified this by manipulating the user-agent string of our crawler. Figure 5 shows an example of an SEO page.

#### Link Structure
Each spider pool site provides a sitemap page (`sitemap.html`) to direct search crawlers to visit other sites according to its intention. By connecting the discovered spider pool domains under linking relations, we found that a strongly connected graph can be constructed, where there exists a route from any domain to another. Additionally, most links on an SEO page are swapped per visit. To a search crawler, this link structure appears as an ever-changing labyrinth: every step the crawler makes refreshes the whole link topology, trapping the crawler until it reaches the customer's site. This design allows the customer's site connected to the spider pool to be indexed much faster, and updates on the site are promptly reflected in search results. For a site running illegal business, this feature is particularly beneficial as it allows the site to appear quickly in search results before being taken down.

#### Site Free-Ride
We also discovered a new type of promotion method developed to show the customer's message in search results. This is achieved by crafting a search URL under a popular site with the message filled in the query string and injecting it into the spider pool. Since some targeted popular sites display the query string in their search results regardless of whether meaningful content is returned, the embedded message will be fed into the search crawler. Due to the high PageRank (PR) value of these sites, the indexing process is much faster, and the message is more likely to appear at the top of search results. For example, we found an Amazon search URL in an SEO page containing a message for an illegal business (selling counterfeit certificates) with contact information (QQ number, one of the most popular instant messengers in China). Querying with the relevant keywords ("certificate for CET-4/6 qq" translated from Chinese) in Google returns the customer's message at the top spot of search results (see Figure 6).

We call this SEO trick "site free-ride," as the reputation of a popular site is abused by blackhat SEO while the site itself is neither compromised nor spammed, making detection difficult. In Section 5.3, we show that many top sites are misused in this way.

### Classifying Spider Pool Domains

Following the trail of the sitemap, we encountered different types of domains, including SEO domains, customer site domains, domains abused for message promotion, and other innocent domains. Although they can be distinguished through manual investigation, this approach is not scalable. Therefore, we developed a classifier to differentiate these cases automatically.

We started by identifying SEO domains. We found that all SEO domains use wildcard DNS, and the page content changes per visit, while the content from other types of sites is more stable. Therefore, we limited the scope to wildcard DNS domains and compared the sitemap files for two consecutive visits to determine if the domain was an SEO domain.

A challenge we faced was measuring the difference between two visits. Initially, we attempted to measure the text difference between the two HTML pages, but this resulted in many false positives due to dynamic content like online advertisements. We improved this method by considering only the differences in hyperlinks between the two pages. To discard small changes in URLs customized for visitors, we normalized the URLs by removing query parameters and values. Taking two pages PA and PB as an example, assuming the sets of unique hyperlink URLs in PA and PB are HA and HB, we computed |HA - HB| and |HB - HA|, and then compared the maximum value to a threshold MinH. If the maximum value is larger than MinH, the domain is labeled as an SEO domain. Based on empirical tests, we set MinH to 20%, which yielded good accuracy.

This method worked well for exploring the SSP. In Section 5.2, we tested 20 other spider pools using the same detector with minor adjustments. We found that some spider pools also use the homepage to guide search crawlers. Therefore, the homepage was also inspected by the detector.

After identifying SEO domains, our tool classifies the remaining domains into three types:
- **Sites Abused for Message Promotion**: Identified using URL patterns related to search queries. By inspecting a large corpus of message URLs, we identified 45 URL patterns, like `/search/`, and used them to match all remaining URLs.
- **Innocent Sites**: Identified by matching their domain names with the Alexa top 1M site list. Although some may use spider pools, we believe the chance is small if they want to maintain a good reputation.
- **Customer Sites**: The remaining sites are classified as customer sites.

Our detection mechanism relies on the features of the link structure and is robust against different templates and languages used by SEO pages. To evade our detector, adversaries could suppress the change rate of the SEO page for each visit, but this would make the link structure less dynamic and more likely to be detected by search engines. Another evasion method is to disable wildcard DNS support on SEO domains. Although we have found some spider pools beginning to apply this change, the majority of SEO domains still use wildcard DNS, suggesting that adversaries are not planning to give up the benefits brought by wildcard DNS.

### Detecting SEO Domains through DNS Scanning

Motivated by the findings from infiltrating the SSP, we implemented a scanner to discover and measure SEO domains of the spider pool from an internet-wide perspective. While the detector described in Section 3 is effective in enumerating SEO domains, it highly depends on seeded domains and has limitations. Instead, we launched large-scale DNS scanning to identify wildcard DNS domains and crawled their sitemaps and homepages. We then applied differential analysis to detect SEO domains using the heuristics described in Section 3.4. The process is elaborated as follows.

#### Data Source
We clustered the SEO domains employed by the SSP according to their Top-Level Domains (TLDs) and Second-Level Domains (SLDs) and examined their popularity. Unsurprisingly, a large number of domains are under old generic TLDs (gTLDs) like `.com` and country-code TLDs (ccTLDs) like `.cn`. Interestingly, we also found a noticeable number of domains under new gTLDs like `.xyz` and generic SLDs like `.com.cn`. We decided to scan domains under all these categories.

For each TLD or SLD under study, we first attempted to gain access to its DNS zone file. If the zone file was not available, we resorted to third-party domain collections and passive DNS data sources. For `.com`, we downloaded the zone file from Verisign in December 2015 through its TLD Zone File Access Program. The unique count of domains was over 125 million, and we sampled 2 million (1.6%) for study. We also applied for access to zone files of new gTLDs and several old TLDs through the Centralized Zone Data Service (CZDS) managed by ICANN. We obtained zone files from 10 registries for the most popular new gTLDs in December 2015. The `.cn` zone file was retrieved from viewdns.info in December 2015 (the zone file has over 5.5 million domains, accounting for around 64% of all 8.6 million `.cn` domains). For the remaining TLDs, we searched the passive DNS database provided by Farsight Security and extracted domains from all A records within 2015. The list of TLDs and the statistics are shown in Table 1.

#### Detection System
To detect wildcard domains, we implemented a DNS scanner to probe all 13.5 million domains in our list. For any name (e.g., `a.com`) in the list, our scanner issues a wildcard A record query (`*.a.com`) to its authoritative server directly, bypassing resolvers provided by local ISPs. If we get a valid IP (same as the one used by `a.com`), we consider that the domain supports wildcard. To prevent ISP's DNS wildcard injection with its web portal's IP, we detected such behavior before large-scale probing and filtered out such IP addresses used by local ISPs.

Handling such a large volume of domains is challenging. To optimize performance, we issued DNS queries and processed responses asynchronously. Specifically, the scanner runs one thread continuously issuing DNS requests in UDP and another thread picking up matched DNS responses from incoming traffic. Our task was completed within 120 hours, and the results are reported in Table 1.

We obtained 2.4 million wildcard domains (17.8% of all scanned domains) after the scanning process. The next step was to determine which domains are used for the spider pool. To this end, we used a crawler to visit the sitemap (or homepage when the sitemap is not available) of the root folder of each wildcard domain twice, then compared the hyperlinks using the same heuristics described in Section 3.4. The domains showing different sets of hyperlinks were considered SEO domains. In the end, we captured 458K (19.1% of all wildcard DNS domains) such domains in total. This result suggests a significant proportion of domains have configured wildcard DNS for SEO purposes.

We further classified the detected SEO domains based on how the pages are linked to the customer's content and identified three different types. All of them can be generated from the templates of known spider pool toolkits. We elaborate on each type in Appendix B. The number under each type is reported in the `#iframe`, `#link`, and `#redir` columns in Table 1.

#### Verification
Our detection system discovered a large amount of SEO domains, and we wanted to know how accurate the outcome is. Counting the true positives accurately is nearly impossible because there is no off-the-shelf detection system we can use as the oracle, and it would take massive human effort to examine all of them. As an alternative, we sampled up to 1,000 SEO domains per TLD/SLD and evaluated them to estimate the accuracy. We manually looked into the title, page text, and link structure, and compared them with known SEO pages from the SSP. We considered a domain as a true positive if these features resembled. We found the highest false-positive rate (FPR) per TLD/SLD occurred on `.com`, which was 1.2%. The main reason for the false positives was that many links on legitimate sites also change frequently.

**Table 1: Data source for DNS probing and the scanning results. "Zone file*" means the domains are sampled from all domains in the zone file. #All SEO is the number of all detected SEO domains. It is also the sum of #iframe, #link, and #redir.**

| Category | Old TLD | New gTLD | SLD | Total |
|----------|---------|----------|-----|-------|
| TLD&SLD  | cc      | cn       | com | edu   |
|          | gov     | pw       | xyz | top   |
|          | wang    | win      | club| science|
|          | ren     | link     | party| click |
|          | ac.cn   | edu.cn   | gov.cn| org.cn|
|          | com.cn  | net.cn   | -   | -     |

| Data Source | Passive DNS | Domain List | Zone file* | Passive DNS | Passive DNS | Passive DNS | Zone file | Zone file | Zone file | Zone file | Zone file | Zone file | Zone file | Zone file | Zone file | Zone file | Zone file | Passive DNS | Passive DNS | Passive DNS | Passive DNS | Passive DNS | Passive DNS |
|-------------|-------------|-------------|------------|-------------|-------------|-------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-------------|-------------|-------------|-------------|-------------|-------------|

| #All        | 99,934      | 4,107,679   | 2,000,000  | 3,619       | 1,215       | 246,775     | 1,695,430 | 1,033,644 | 629,757   | 570,091   | 535,576   | 249,187   | 237,372   | 213,449   | 217,508   | 181,673   | 86,291     | 1,347       | 12,364      | 53,492      | 1,093,580   | 234,039     |

| #Wildcards  | 40,334      | 371,829     | 751,877    | 353         | 130         | 146,984     | 309,983   | 127,532   | 49,528    | 56,020    | 108,665   | 44,544    | 11,023    | 40,719    | 68,239    | 43,914    | 56,944     | 173         | 1,745       | 5,206       | 152,751     | 12,445      |

| #All SEO    | 10,320      | 222,390     | 37,568     | 0           | 0           | 43,561      | 45,769    | 9,842     | 6,597     | 22,115    | 2,163     | 2,271     | 121       | 490       | 3,650     | 1,059     | 24,270     | 0           | 24          | 805         | 23,264      |

This comprehensive approach allowed us to effectively identify and classify SEO domains, providing valuable insights into the operations and structure of the spider pool.