is elucidated in Section 6.2.
The safeguards are instituted in response to threats. Each threat
has associated with it a set of ﬁles that may be affected by it. The
grouping is orthogonal to any operating system attributes.
If an
intrusion detector determines the need to take precautions against
a threat, one course of action is to safeguard the relevant ﬁles. A
protection group serves as the data structure used to track a subset
of ﬁles that are always affected together, regardless of what the
current threat may be. Hence, a single threat may affect a number
of protection groups.
Since protection groups are deﬁned independent of any other ﬁle
attributes, they can be deﬁned as arbitrary sets. This allows ﬁles
that are unlikely to be affected by a threat not be safeguarded. The
property also ensures that subsystems and applications that do not
utilize data that is threatened can continue operating normally.
When a threat appears and a set of ﬁles must be protected, it is im-
perative that the safeguards be instituted in as short a time period
as possible. The longer it takes, the more damage can be effected
in the interim. The use of protection groups allows the system to
perform a small number of key deletion operations on the groups’
meta-data, rather than a large number of operations on all the con-
stituent ﬁles.
6.2 Assurance
When the system is under threat of penetration, data must be safe-
guarded. The aim is to guarantee three properties for the data -
conﬁdentiality, integrity and availability - that will hold even after
a successful attack.
6.2.1 Conﬁdentiality
Each ﬁle that is part of a protection group is kept encrypted in a
symmetric cipher with its own unique key, which serves as a cryp-
tographic capability. This capability in turn is kept encrypted with
the group’s public key. If an attacker is likely to compromise the
system in a manner that threatens the protection group of the ﬁle,
the private key of the group will be deleted. This will prevent the
ﬁle’s cryptographic capability from being decrypted. Without the
ﬁle’s capability accessible, the ﬁle’s conﬁdentiality is guaranteed.
When an application seeks to use the ﬁle and its protection group
has not been threatened, then the runtime environment is able to
transparently enable access to the ﬁle by retrieving its cryptographic
capability (which can be unsealed with the extant protection group
private key), decrypting the ﬁle and then opening the temporary
decrypted version. When the application is done with the ﬁle, it is
re-encrypted and the temporary version deleted from the system.
If a ﬁle was still in use at the time of the penetration, along with the
deletion of the group’s private key, the temporary decrypted version

of the ﬁle will be deleted. Any changes made since it was last
opened would be lost, but its conﬁdentiality would be maintained.
6.2.2 Integrity
In order to be able to verify the integrity of a ﬁle, a cryptographic
hash of the contents of the ﬁle is maintained with the ﬁle’s meta-
data. To prevent the hash from being manipulated without autho-
rization, it is always sealed with the ﬁle’s protection group’s public
key before it is stored. If the ﬁle is changed after being opened,
then the hash of the new version must be computed, sealed with
the protection group’s public key, and stored in the ﬁle’s meta-data.
When a ﬁle is opened, either for reading or writing, the ﬁle’s hash
is computed and compared to the one stored in the meta-data (after
unsealing the stored hash using the protection group’s private key).
If the hashes match, the ﬁle’s integrity is deemed to have been ver-
iﬁed.
If an intrusion detector determines that a protection group is threat-
ened, it deletes the group’s public key. Once this has been done, any
changes that an attacker makes to a ﬁle will be detectable. Since
the protection group’s public key is no longer present, it is not pos-
sible to seal the hash of the changed version of the ﬁle. When the
ﬁle is accessed subsequently, the fact that the computed hash does
not match the stored hash (after it has been unsealed with the pro-
tection group’s private key), signals that the ﬁle’s integrity has been
compromised.
If a ﬁle was in use when an intrusion occurs, the integrity of any
changes that were made since the ﬁle was opened will not be recorded.
This is due to the fact that the decrypted version of the ﬁle will be
deleted without re-encrypting it (since that would introduce an un-
acceptable delay which an attacker may be able to exploit), and
hence since the changes will be lost there is no question of veri-
fying their integrity. If the attacker does not alter the ﬁle, it will
remain in the state that it was before the last time it was used and
its integrity can be veriﬁed. If the attacker alters it, the integrity
check will fail.
6.2.3 Availability
The goal of guaranteeing the availability of data in the face of an
attack is usually managed by instituting a regular backup regimen.
When a system penetration is detected, data from a backup prior
to the intrusion is extracted and used to replace the tainted version.
This is an inherently synchronous process, bringing with it a nec-
essary tradeoff. Increasing the frequency of the backup decreases
temporal extent of data loss. However, it also imposes an increased
overhead. These two factors must be balanced. In addition, either
all the ﬁles are backed up or the entire ﬁlesystem must be inspected
to search for ﬁles that have changed. This fact places a lower bound
on the time to effect a single backup. The bound grows with the size
of the ﬁlesystem, a quantity that continues to increase with time.
We address the issue through the use of an asynchronous approach.
We incorporate functionality in the runtime environment which copies
changes made in ﬁles to a remote node. If an attack is subsequently
deemed to have occurred, the prior state of any ﬁle that has been
changed can be computed using the sequence of changes that have
been copied over.
When a ﬁle is accessed, a copy of the original version is main-
tained. After the ﬁle is closed, the runtime determines if the ﬁle has
been written to. If it has, a delta is computed between the original
version and the new version. A hash of the delta is computed and
sealed with the ﬁle’s protection group’s public key. The delta it-
self is encrypted with the ﬁle’s cryptographic capability. Thus, the
modiﬁcations are provided the same conﬁdentiality and integrity
guarantees as the original ﬁle. The sealed hash and the delta are
placed in a temporary location on the disk. A separate process syn-
chronizes the deltas with a remote node.
6.3 Virtual Layer
In order to implement the changes needed to provide the data se-
curity guarantees in a manner that is transparent to extant applica-
tions, it was necessary to introduce them in the operating system
itself. There are two possible approaches. The ﬁrst option is to
modify the ﬁlesystem itself, altering its data structures to include
the new meta-data needed, along with the cryptographic transfor-
mations that use the auxiliary protection information. The alterna-
tive approach is to introduce the functionality as a virtual layer over
an existing ﬁlesystem. When runtime environment calls are made
to operate on ﬁles, they can be intercepted and the new transforma-
tions effected if required, making calls to the native ﬁlesystem as
needed.
With the latter approach, there is a further choice of where to store
the security related meta-data. One option is store both the meta-
data and the actual content in the native ﬁlesystem version of the
ﬁle. The other option is to maintain the meta-data separately. We
opted to use the virtual layer approach with the meta-data stored
separately. Described below are some of the factors that were in-
volved in making the choice.
Using either a different native ﬁlesystem format or a virtual layer
with the meta-data stored with the data within a single ﬁle in the
native ﬁlesystem has several limitations. It will not be backward-
compatible with any extant data stored in a currently deployed ﬁlesys-
tem due to differing formats. All that data will have to be copied
over. The functionality provided by any attributes stored in the
meta-data of the old ﬁlesystem will either be lost or have to be re-
implemented. The new ﬁlesystem will not be inter-operable with
any other runtime system that does not have support for the new ﬁle
format. Additionally, the resulting system will not be extensible -
that is if new attributes are to be added to the meta-data of each ﬁle,
they can not be inserted for each ﬁle without rewriting the entire
ﬁlesystem.
Maintaining the meta-data separately brings with it the advantage
of being able to add new ﬁelds for existent ﬁles with little cost.
For example, to add functionality to retrieve a ﬁle’s cryptographic
capability dynamically from a remote capability server if it is not
present, new ﬁelds would be needed to store the capability server’s
location. The cost to introduce the ﬁeld into the meta-data stored
separately would be proportional to writing out all the meta-data,
and would not incur the cost of having to write out all the data
stored in the ﬁlesystem as well.
Using a virtual layer approach with the meta-data stored separately
from the ﬁles has the disadvantage that the native ﬁlesystem’s syn-
chronization of the meta-data can not be leveraged. However, this
is addressed by limiting the use of shared data structures that must
be locked - they are used only when a ﬁle is opened and closed,
not when it is read or written. Therefore the overhead introduced is
minimal.
The approach of storing the meta-data with the data has the ad-
vantage of allowing ﬁles to be transported from one ﬁlesystem to

another, even across different hosts, and yet retain their protection
proﬁle so that they may potentially be accessed independently of
the resource in which they reside. Since we are focused on a sin-
gle host operating environment, this did not provide a signiﬁcant
advantage.
6.4 Protection Granularity
Another choice that must be made is the granularity at which cryp-
tographic operations are to be performed. Cryptographic ﬁle sys-
tem projects, such as those described in Section 4, either encrypt
or decrypt an entire ﬁle or just a block at a time. Operating at ﬁle
granularity results in a performance impact when opening and clos-
ing a ﬁle, while operating at block granularity introduces overhead
for read and write operations.
6.4.1 Transaction Contract
Once an intrusion has been detected, it is necessary to use the se-
quence of replicated deltas to undo the changes made by the at-
tacker so as to return the system to an untainted state. If the cryp-
tographic operations (and implicitly the contract of the transaction
between the application and the storage subsystem) were at block
granularity, then the semantics of the recovered state would be un-
clear. To see why this is true, consider the following case. Assume
that when a block is no longer being written to, the system will re-
encrypt it and commit the changes to a remote node. Now consider
the implications when an application does a write which spans mul-
tiple blocks, some of which have been re-encrypted and replicated
at the point in time that a likely intrusion is detected. The response
subsystem will delete the relevant keys, making the writes to the
remaining blocks unauthenticated. After recovery, the ﬁle will con-
tain some blocks containing part of the write operation and some
blocks reﬂecting the earlier state of the ﬁle. This leaves the ﬁle in
an unusable state. It is preferable to be able to ensure that either
the entire write can be authentically committed or the ﬁle can be
reverted to the prior state.
6.4.2 Common Case
In addition to the issue of the semantics, we consider the implica-
tion for performance. Reading and writing are far more common
operations in a typical workload. As a result, it is reasonable to
optimize this case at the expense of the case of opening and closing
a ﬁle. We therefore opt to encrypt, decrypt and compute hashes at
ﬁle granularity. Below we further describe the tradeoff involved in
the choice.
Performing cryptographic operations at ﬁle granularity results in
the fact that opening and closing a ﬁle, which is an O(1) operation
in a traditional ﬁlesystem, becomes an O(n) operation, where n
is the length of the ﬁle. This is because the entire ﬁle must be
decrypted and its integrity veriﬁed when opening the ﬁle. Similarly,
the ﬁle must be encrypted and its hash computed when closing the
ﬁle. If blocks of size b are used and cryptographic operations are
performed at block granularity, then open and close operations have
O(b) = O(1) cost. One method to address this issue is to ﬁx an
upper limit on the size of ﬁle that may be protected, say k. The
complexity of opening and closing a ﬁle is then O(k) = O(1) if
the k is a constant.
The advantage of performing operations at ﬁle granularity mani-
fests when ﬁles are being read and written. Operating at block gran-
ularity introduces the latency of decryption and encryption during
reads and writes. If operations are performed at ﬁle granularity,
an unencrypted version is used during read and write operations so
there is no cryptographic overhead. When the workload used in-
volves concurrent accesses of ﬁles by multiple processes or there
exists signiﬁcant locality of reference, then the fact that reads and
writes have no extra cost in this approach results in a performance
advantage over the block granularity approach. If a signiﬁcant por-
tion of the ﬁle is used, then operating at ﬁle granularity approx-
imates the use of an optimal pre-caching policy that has perfect
lookahead, coupled with an inﬁnite size cache.
If the following conditions are all true for the ﬁles in the workload,
then using block granularity would have been preferable - a very
small fraction of each ﬁle is used (since operating on the entire ﬁle
would add signiﬁcant overhead), the ﬁle is not reused (since block
granularity reuse is much more expensive as cryptographic opera-
tions must be effected on each use), the ﬁle is not used by concur-
rent processes (since there is no extra cryptographic cost added for
all processes after the ﬁrst that use the ﬁle).
IMPLEMENTATION
7.
We now describe the organization of the meta-data used, the tool
Group Manager used to manipulate it manually, and the runtime
subsystem Capability Manager that transparently manages it for
applications.
7.1 Meta-data
Each ﬁle that is protected by RICE has several attributes that are
stored in an instance of the ObjectMetaData data structure. These
include:
objectLocation The location of the ﬁle in the ﬁlesystem at the
time of protection.
objectGroup The protection group to which the ﬁle belongs.
instances The number of concurrently open instances of that cur-
rently exist.
decrypted The location of a temporarily unencrypted version (if
one exists) of the ﬁle.
pristine The location of a temporary copy of the ﬁle in the state
that it was when the ﬁle was opened, before any writes oc-
curred. It only exists if a ﬁle is currently open and serves as
a baseline against which deltas of the ﬁle can be computed.
sealedCapability The cryptographic capability (symmetric key)
used to encrypt the ﬁle, wrapped in the public key of the
protection group of which it is a member.
capability The value of the unsealed cryptographic capability, which
is only present while the ﬁle is open.
currentCheckpoint A counter used to indicate the position in the
sequence of deltas that are computed each time a ﬁle is closed
after changes have been made.
computeDelta The value serves as the equivalent of a dirty bit on a
page. It indicates whether any instance of the ﬁle was opened
for writing, in which case a delta must be computed when it
is closed.
sealedHash The cryptographic hash of the ﬁle as it was when it
was last closed, kept sealed in the protection group’s public
key.

idempotency Each instance of an open ﬁle has a unique hash as-
sociated with it that is stored in this set.
Each protection group is stored in an instance of the ObjectGroup