title:A Quantitative Study of Video Duplicate Levels in YouTube
author:Yao Liu and
Sam Blasiak and
Weijun Xiao and
Zhenhua Li and
Songqing Chen
A Quantitative Study of Video Duplicate
Levels in YouTube
Yao Liu1(B), Sam Blasiak2, Weijun Xiao3, Zhenhua Li4, and Songqing Chen2
1 SUNY Binghamton, Binghamton, USA
PI:EMAIL
2 George Mason University, Fairfax, USA
{sblasiak,sqchen}@cs.gmu.edu
3 Virginia Commonwealth University, Richmond, USA
PI:EMAIL
4 Tsinghua University, Beijing, China
PI:EMAIL
Abstract. The popularity of video sharing services has increased expo-
nentially in recent years, but this popularity is accompanied by challenges
associated with the tremendous scale of user bases and massive amounts
of video data. A known ineﬃciency of video sharing services with user-
uploaded content is widespread video duplication. These duplicate videos
are often of diﬀerent aspect ratios, can contain overlays or additional
borders, or can be excerpted from a longer, original video, and thus can
be diﬃcult to detect. The proliferation of duplicate videos can have an
impact at many levels, and accurate assessment of duplicate levels is a
critical step toward mitigating their eﬀects on both video sharing ser-
vices and network infrastructure.
In this work, we combine video sampling methods, automated video
comparison techniques, and manual validation to estimate duplicate lev-
els within large collections of videos. The combined strategies yield a
31.7 % estimated video duplicate ratio across all YouTube videos, with
24.0 % storage occupied by duplicates. These high duplicate ratios moti-
vate the need for further examination of the systems-level tradeoﬀs asso-
ciated with video deduplication versus storing large number of duplicates.
1 Introduction
User generated video content has exponentially increased in the recent years.
For example, YouTube, Dailymotion, and Vimeo are among the most popular
websites for uploading and sharing user generated content (UGC). YouTube
alone has gained massive popularity: it attracts more than 1 billion users every
month, more than 100 h of uploaded video each minute, and more than 1 million
creators make money from videos that they have uploaded [3]. We estimate
that there are more than 849 million videos on YouTube (Sect. 5.2). According
to Sandvine, YouTube generates 13.19 % of all downstream ﬁxed access traﬃc
(e.g., cable network) and 17.61 % of all downstream mobile data traﬃc in North
America during peak hours [2].
c(cid:2) Springer International Publishing Switzerland 2015
J. Mirkovic and Y. Liu (Eds.): PAM 2015, LNCS 8995, pp. 235–248, 2015.
DOI: 10.1007/978-3-319-15509-8 18
236
Y. Liu et al.
Unlike video on-demand service providers such as Netﬂix, which contracts
with a limited number video providers, UGC websites attract large numbers of
video uploaders. This high diversity of uploaders poses a unique challenge for
these UGC video sharing websites: Videos can be uploaded in diﬀerent incarna-
tions by diﬀerent users, leading to duplicates in the video database. While dupli-
cates that occur at the exact byte-level can be captured by the video sharing
service using cryptographic hashes, user-generated (near-)duplicate videos are
often uploaded in diﬀerent encodings, have diﬀerent aspect ratios, can contain
overlays or additional borders, or could be excerpted from a longer, original
video. As a result, they are assigned their own unique IDs in the video database.
Note that duplicates should not be confused with multiple transcoded versions
generated by a video sharing service to support streaming at diﬀerent band-
widths and to diﬀerent devices. These transcoded versions are associated with a
same video ID in the video database.
The proliferation of duplicate videos could impact many aspects of datacen-
ter and network operations and, as a result, have negative eﬀects on the user
experience. From the video server’s perspective, duplicate videos could increase
data storage, power, and therefore overall costs of data center operations. Fur-
ther, duplicate videos have the potential to harm caching systems, degrading
cache eﬃciency by taking up space that could be used for unique content and
increasing the amount of data that must be sent over the network to in-network
caching systems. These ineﬃciencies could be passed on to the user in the form of
duplicated search results, longer startup delays, and interrupted streaming [17].
Although it is well known that duplication occurs in today’s UGC video
sharing websites, little is known about its precise level. Work to more-accurately
determine duplicate levels is necessary because, although deduplication proce-
dures can improve the overall eﬃciency of a video sharing system, deduplication
itself could also be costly. Quantifying the level of duplication is therefore critical
for determining whether eﬀort to deduplicate, or otherwise mitigate the eﬀect of
duplicates, would be worthwhile.
As YouTube is the largest UGC video system today, we choose it as repre-
sentative of similar services, and measure its level of duplication. In the process
of conducting these measurements, we make the following contributions:
– We employ a novel combination of video sampling methods, automated video
comparison techniques, and manual validation to estimate duplication levels
in large-scale video sharing services.
– Using these methods, we estimate that the duplicate ratio of YouTube videos
is 31.7 % and that 24.0 % of YouTube’s total video storage space is occupied
by duplicates.
The remainder of the paper is organized as follows. Sections 2 and 3 discuss
the motivation of this study and related work, respectively. Section 4 describes
our duplicate estimation technique. We report our results in Sect. 5. Finally,
Sect. 6 concludes this work.
A Quantitative Study of Video Duplicate Levels in YouTube
237
2 Motivation
Anyone who has watched videos on YouTube, or any other video sharing service,
has certainly noticed that near-duplicates of the same video often appear in the
search results or are recommended as related videos. These impressions, however,
are not useful toward making recommendations for taking action to mitigate any
potential eﬃciency loss resulting from unnecessary duplication.
In preliminary work, we performed a small-scale assessment of 50 queries for
the titles of 50 popular YouTube videos from a randomly selected set (Sect. 4.1).
Manual assessment of these videos produced a rough estimate of a 42 % dupli-
cate ratio.
Viewing a small number of individual search results, however, is unlikely to
yield good estimates of the prevalence of duplicates across a video sharing ser-
vice’s entire database. The huge number of videos stored within services such as
YouTube also indicates that manually comparing videos to estimate duplicate
ratio is infeasible. This intractability motivates the need for a larger scale assess-
ment, assist in determining the necessity of and formulating further systems to
conduct video deduplication.
3 Related Work
Data deduplication. Data duplication is common in storage systems. Dedupli-
cation operates by detecting duplicates and storing only a single copy of a given
chunk of data. It is typically conducted on exact byte-level duplicates [4,6,10,
18,23]. Detecting exact duplicates is often performed using cryptographic-hash
based approaches (e.g., SHA1) to create an index for fast lookups. These cryp-
tographic hash-based approaches, however, are inappropriate for detecting near-
duplicate videos (i.e., videos that appear the same or very similar to a human
viewer). This unsuitability is due to the fact that video ﬁles almost always con-
tain signiﬁcant diﬀerences at the byte-level even though the visual content of a
video may be replicated (due to changes in encoding, altered resolutions, image-
level editing, or temporal editing).
Near-duplicate video detection. The computer vision community has pro-
posed a variety of strategies for detecting near-duplicate videos [5,9,19]. Two
main types of tools have been developed. The ﬁrst is the local image descrip-
tor [13,14,21], which describes small sections within an image/keyframe. The
second is the global descriptor [7,16,20,22], which can be used to summarize
the entire contents of an image or video. An approach for video duplicate detec-
tion that can employ either local or global descriptors is called Dynamic Time
Warping (DTW) [15]. DTW is a technique used to measure distance between
two sequences where a distance can be deﬁned between sequence elements. DTW
operates by aligning elements from a pair of sequences, A and B. Speciﬁcally,
DTW aligns each element from sequence A to a similar element in sequence B
with the constraint that no changes in ordering can occur (see Fig. 1).
238
Y. Liu et al.
Video deduplication. The rapid growth of video content on the Internet and
its corresponding storage cost have recently drawn much attention to the task
of video deduplication. For example, Kathpal et al. found that multiple copies
(versions) of the same video in diﬀerent encodings and formats frequently exist.
The authors proposed to save space by conducting on-the-ﬂy transcoding to only
retain the copy with the highest quality [11]. Shen and Akella proposed a video-
centric proxy cache, iProxy. iProxy stores the frequency domain information of
the video’s key frames in an Information-Bound Reference (IBR) table. iProxy
improves the cache hit rate by mapping videos with the same IBR to a single
cache entry and dynamically transcodes the video during playback [17]. How-
ever, both works [11,17] only deal with duplicates introduced by a limited set of
transformations, e.g., quantization, resizing, and diﬀerent formats and encodings.
Other forms of transformation, such as excerption, concatenation, and splicing,
would not be detected or deduplicated. Katiyar and Weissman proposed ViD-
eDup, which uses clustering-based “similarity detection” and performs dedupli-
cation by storing the centroid-videos with the highest perceptual-quality [12].
However, since only the centroid of a set of “similar” videos are stored, restored
video may no longer represent the original visual content.
Long Video
Long Video
Skip
o
e
d
V
i
t
r
o
h
S
o
e
d
V
i
t
r
o
h
S
M
atc
h
Fig. 1. The standard Dynamic Time
Warping algorithm aligns all
frames
of both videos. Red squares represent
aligned video frames.
Fig. 2. The modiﬁed version of DTW
used in this study aligns all elements
from the shorter video and can skip
frames from the longer video.
4 Methodology
Our set of techniques for video duplicate assessment are applied in the following
steps:
Step 1: We use random preﬁx sampling [8] to sample YouTube videos uniformly
and at random. We refer to this set of sampled videos as sampled videos.
Step 2: We then search for the title of each sampled video using the text-based
search engine of YouTube, which returns a list of relevant videos. We refer to
these relevant videos as searched videos. These searched videos are used as
a candidate set of duplicates.
A Quantitative Study of Video Duplicate Levels in YouTube
239
Step 3: For each (sampled video, searched video) pair, we calculate a simi-
larity score which accounts for temporally shifted frames. This score is used to
determine whether the searched video is a duplicate of the sampled video.
Step 4: For each pair of duplicates whose score is below a threshold, we conduct
a manual comparison step to eliminate false positives.
In the rest of this section, we explain each step of our technique in detail for
assessing duplicate levels in YouTube.
4.1 Random Sampling of Videos
In order to uniformly sample YouTube videos, we use the random preﬁx sampling
method proposed by Zhou et al. [8]. Random preﬁx sampling involves querying
the YouTube search engine with a randomly selected video ID (VID) preﬁx. The
returned query results are existing videos whose VIDs match this random preﬁx.
According to Zhou et al., with a preﬁx length of ﬁve (“-” being the last/ﬁfth
symbol in the preﬁx), all existing VIDs that match the preﬁx can be returned in
one query. Therefore, during the sampling procedure, we randomly generate a
ﬁxed number, Npref ix, 5-character long preﬁxes. (In this work, we set Npref ix to
1,000.) In the remainder of the paper, we refer to the videos selected by random
preﬁx sampling as sampled videos. We make the important assumption that
the set of sampled videos contains no duplicates. We validate this assumption
through both theoretical and experimental analysis in Sect. 5.2.
4.2 Selection of Candidate Duplicate Pairs
The next step involves pruning the number of video pairs that must be assessed
with a computationally intensive duplicate detection method. We perform this
pruning step by leveraging the metadata-based search engines provided by many
video sharing services. In UGC sites, metadata can be an especially good source
for retrieving duplicates because uploaders of these duplicates are incentivised
to label their videos with metadata to indicate similarity to original popular
content, thereby attracting a larger number of views.
We extract each sampled video’s title and use it to query the YouTube
search engine. This query returns a collection of videos with metadata related to
the sampled video’s title. Because this set of videos may still be too large to
eﬀectively process with DTW, we rely on the ranking capability of YouTube’s
metadata-based search engine to further ﬁlter videos. In particular, we record
the top 100 results from each query. Some queries only return fewer than 100
results, and on average, we collected 82 searched videos for each sampled
video. We refer to this set of videos returned from this search procedure as
searched videos. Pairs of sampled videos and searched videos are sent to
our DTW-based algorithm for duplicate assessment.
240
Y. Liu et al.
4.3 Comparing Sampled and Searched Video Pairs
For comparison, we download both the sampled video and searched video
ﬁles from YouTube. YouTube usually encodes videos into multiple versions using
diﬀerent codecs, resolutions, and quantization levels to support streaming at dif-
ferent bandwidths and to diﬀerent devices. We retrieve only the H.264
Baseline/AAC/MP4/360p version as we ﬁnd this version is most often available.
After retrieving a set of searched videos associated with every sampled
video, we use FFmpeg [1] to extract images/frames from the video at one sec-
ond intervals. Note that we cannot use keyframes (i.e., I-frames) for compar-
ison, as in related work [20], because the interval between keyframes can vary
between videos. To detect pairs of duplicates, we employ a method based on
Dynamic Time Warping (DTW) [15]. Like DTW, our duplicate matching system
attempts to align frames from pairs of videos. However, we expect shorter videos