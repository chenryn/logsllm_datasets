	} 
}
• 重命名字段
当 expression 参数中使用 del 方法删除一个已存在的字段，并在 field 参数中指定另 一个不存在的字段名称时，可以实现字段的重命名。配置示例：
{ 
	"type": "evaluate", 
	"field": ".renamed_field","field": ".renamed_field", 
	"expression": "del(.to_be_renamed_field)" }
• 删除字段
当 expression 参数中使用 del 方法删除一个已存在的字段，并在 field 参数中不指定 任何字段名称时，可以实现字段的删除。配置示例：
{ 
	"type": "evaluate", 
	"field": "", 
	"expression": "del(.to_be_deleted_field)" }
script类型
script类型的处理逻辑可以实现和 evaluate 类型的处理逻辑同样的字段处理。除此之外，你可 以通过直接使用完整的  脚本实现更复杂的字段处理逻辑。字段处理脚本 存放在 source 参数中。
配置示例，从表示文件完整路径的字段 .file_path 中获取文件名，并存放在 .file_name 字段中：
{ 
	"type": "script", 
	"source": "path_items, err = split(.file_path, \"/\")\n.file_name = path_items[-1]"
	信息 
关于 field ， expression 以及 source 参数中的语法：
• Vector Remap Language 的总体介绍，参见文档 。• Event 中的字段的使用方式，参见文档 。
版本：2.13.0-beta
Filter
filtertransform 方法用于根据 event 中的字段的内容过滤 event。不能满足过滤条件的 event 将被丢弃，不再被输出到下游的 sink 组件。
配置示例
{ 
	"type": "filter", 
	"conf": { 
	"source": ".status == 500" 
	}"source": ".status == 500" 
	} 
}
配置参数
source
必填参数。过滤条件的表达式，结果为 true 时表示满足过滤条件，否则表示不满足过滤条件。
信息
• 关于 source 参数中对 event 中的字段的使用方式，参见文档 
。
• 关于 source 参数中的表达式语法，参见文档 
。
版本：2.13.0-beta
Regex_extract
regex_extracttransform 方法用于根据正则表达式匹配规则，从 event 的内容中提取出新字
段。
配置示例
{ 
	"type": "regex_extract", 
	"conf": { 
	"expressions": [ 
	"^(?P\\d+/\\d+/\\d+ \\d+:\\d+:\\d+ \\+\\d+) ", 	"^(?P\\d+) "] 
	} 
}
配置参数
expressions
必填参数。数组中可以有多个用于新字段提取的正则表达式，这些正则表达式依次被用来匹配 event 的内容，直至第一次成功匹配或没有任何匹配后才结束提取过程。如有成功匹配，event 中将加入所有
提取出的字段。
	版本：2.13.0-beta 
Sink 组 组件
Sink 组件用于接收上游的 pipeline 组件或者 source 组件输出的 event，并将 event 输出到指定的
炎凰数据平台，或者其他数据存储或服务。
📄炎凰数据平台
yh sink 组件用于将 event 输出到指定的炎凰数据平台的数据集。
📄 File
file sink 组件用于将 event 输出到指定路径下的文件。
📄 Kafka
kafka sink 组件用于将 event 输出到指定 Kafka 服务的 topic。版本：2.13.0-beta
炎凰数据平台
yhsink 组件用于将 event 输出到指定的炎凰数据平台的数据集。当使用此 sink 组件时，DataScale 服务将从配置文件 config/datascale.toml 或者相关环境变量中获取目标炎凰数 据平台的数据接入方式和相关信息（参见文档 Backend 配置）。
配置示例
{ 
	"name": "my_yh_sink", 
	"inputs": [ "my_pipeline" ], 
	"type": "yh", 
	"conf": { 
	"event_set": "my_event_set" 
	} 
}
配置参数
event_set
必填参数。炎凰数据平台数据集的名称，event 将被存入该数据集。
版本：2.13.0-beta
File
filesink 组件用于将 event 输出到指定路径下的文件。File
filesink 组件用于将 event 输出到指定路径下的文件。
配置示例
{ 
	"name": "my_file_sink", 
	"inputs": [ "my_pipeline" ], 
	"type": "file", 
	"conf": { 
	"path": "/tmp/output.log" 
	} 
}
配置参数
path
必填参数。 Event 将被写入指定路径的文件。文件路径中也可以使用 strftime 的格式字符。
例如，根据当前时间将 event 写入不同文件：
{ 
"name": "my_file_sink", 
"inputs": [ "my_pipeline" ], 
"type": "file", 
"conf": {
	信息
• 关于 strftime 中的格式字符，参见文档 。
• 运行 DataScale 服务的账号需要对 path 所指定的目录或文件具有写权限。版本：2.13.0-beta
Kafka
kafkasink 组件用于将 event 输出到指定 Kafka 服务的 topic。
配置示例
{ 
	"name": "my_kafka_sink", 
	"inputs": [ "my_pipeline" ], 
	"type": "kafka", 
	"conf": { 
	"bootstrap_servers": "localhost:9094", 	"topic": "my_topic" 
	} 
}
配置参数
bootstrap_servers
必填参数。 Kafka 服务器的地址。如有多个服务器地址，地址之间用逗号分隔。
topic
必填参数。 Kafka topic 的名称，event 将被输出到该 topic。
版本：2.13.0-beta
监控 DataScale 服务版本：2.13.0-beta
监控 DataScale 服务
DataScale 服务运行过程中产生的日志和指标数据可以用于监控服务的运行状态，以及诊断服务的 异常状态。
日志
DataScale 服务 务日志
DataScale 服务日志默认保存在安装目录下的 logs/datascale.log 文件。
如需修改日志级别、日志文件路径、rotation 方法等设置，可以修改配置文件 config/
datascale.toml中的logger部分：
[logger] 
console = false 
path = "logs/" 
file = "datascale.log" 
level = "info" 
max_size_mb = 10 
max_backups = 30 
max_age = 365
信息
各个参数的功能参见 config/datascale.toml 文件中的详细注释。内置数据采集器日志
• Vector 日志
作为 DataScale 默认的内置数据采集器，Vector 的日志默认保存在安装目录下的 logs/
vector.log文件。
如需修改日志级别、日志文件路径、rotation 方法等设置，可以修改配置文件 config/
datascale.toml中的vector.logger部分：
[vector.logger] 
path = "logs/" 
file = "vector.log" 
level = "info" 
max_size_mb = 100 
max_backups = 3 
max_age = 365
信息
各个参数的功能参见 config/datascale.toml 文件中的详细注释。
自定义 义采集器错误 错误日志
当使用自定义采集器作为 dataflow 中的 source 组件采集数据时，其输出到 STDERR 的日志都作 为错误日志被保存在独立的日志文件中。关于如何查看自定义采集器错误日志，参见 开发 Collector 。
指标
通过 HTTP 调用可以获得 DataScale 服务的指标数据，调用地址为 http:///metrics。
DataScale 服务的指标数据分为几种类别：
• DataScale 服务信息，如版本、运行模式等
• DataScale 服务运行所在的主机的指标数据，如 CPU、内存、磁盘、网络等的使用情况• Dataflow 的执行情况，如各个组件处理的 event 量等
信息
如果 “发送内部指标” 功能被打开（参见 安装 DataScale），DataScale 除去执行指定的数据 采集任务，也会将服务自身的指标数据发送至目标炎凰数据平台。
版本：2.13.0-beta
命令行工具版本：2.13.0-beta
命令行工具
可执行文件 ./bin/datascale 位于 DataScale 的安装路径下。该可执行文件不但可以用于启 动 DataScale 服务，也提供了一系列其他命令行工具：
$ ./bin/datascale --help 
Usage: 
	datascale [command]
Available Commands: 
help        Help about any command run         Run dataflow 
start       Start the service 
version     Print version
Flags:
	-c, --config-dir string   directory path for config file [env: DATASCALE_CONFIG_DIR]-h, --help                help for datascale
Use "datascale [command] --help" for more information about a command.
信息
使用 ./bin/datascale 执行任何命令时，默认会从同一个安装目录下的 ./config/ 目 录中读取配置文件。如果需要从其他路径下读取配置文件，可以使用命令行参数 --config-dir（或者对应的环境变量）指定。
命令 
run 
run命令可以在不启动 DataScale 服务的情况下执行一个指定的 dataflow，通常用于调试
dataflow 的配置。例如，文件 ./test_dataflow.json 中配置了一个 dataflow ，运行该
dataflow 的方法是：dataflow 的方法是：
$ ./bin/datascale run --dataflow-file ./test_dataflow.json
run命令执行 dataflow 时如果发生任何错误，错误信息将会被打印到控制台，便于及时发现问
题。
此外，如果仅需要调试 dataflow 中的某一 source 组件或者 pipeline 组件，可以在不修改 dataflow
配置文件的情况下直接运行 dataflow 中的指定组件。例如，文件 ./test_dataflow.json 中配
置了一个名为 log_file_source 的 source 组件， run 命令可以只运行 dataflow 中的该组
件，并将该组件输出的 event 打印到控制台：
$ ./bin/datascale run --dataflow-file ./test_dataflow.json --dataflow-component log_file_source
信息
更多关于 dataflow 以及 dataflow 组件的介绍，参见 配置 dataflow。
start 
启动 DataScale 服务。
信息
更多关于启动 DataScale 服务的介绍，参见 安装 DataScale。
version
查看当前安装的 DataScale 的版本信息。
设置环 环境变 变量