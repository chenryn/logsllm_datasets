2
1
0
Target members
Target non-members
0
20
60
40
Epoch
80
0
20
60
40
Epoch
80
0
20
60
40
Epoch
80
(a)
(b)
(c)
Fig. 7: The impact of the global active gradient ascent attack on the target model’s training process. Figures show the gradient
norms of various instances (Purchase100 dataset) during the training phase, while the target instances are under attack.
TABLE XI: Dataset sizes in the federated learning experiments
Parties’ Datasets
Datasets
Training
Test
CIFAR100
Texas100
Puchase100
30,000
8,000
10,000
10,000
70,000
50,000
Training
Members
15,000
4,000
5,000
Inference Attack Model
Training
Test
Test
Non-members Members
Non-members
5,000
4,000
5,000
5,000
4,000
5,000
5,000
4,000
5,000
TABLE XII: The accuracy of the passive global attacker in the
federated setting when the attacker uses various training epochs.
(CIFAR100-Alexnet)
Observed Epochs
5, 10, 15, 20, 25
10, 20, 30, 40, 50
50, 100, 150, 200, 250
100, 150, 200, 250, 300
Attack Accuracy
57.4%
76.5%
79.5%
85.1%
TABLE XIII: The accuracy of the passive local attacker for different
numbers of participants. (CIFAR100-Alexnet)
Number of Participants
Attack Accuracy
2
3
4
5
89.0%
78.1%
76.7%
67.2%
dataset we use epochs [100, 150, 200, 250, 300]. When the
attacker has access to several training epochs in the CIFAR100
target models, he achieves a high membership attack accuracy.
In Texas100 and Purchase100 datasets, however, the accuracy
of the attack decreases compare to the stand-alone setting.
This is due to the fact that averaging in the federated learning
scenarios will reduce the impact of each individual party.
The Passive Local Attacker: A local attacker cannot observe
the model updates of the participants; he can only observe the
aggregate model parameters. We use the same attack model
architecture as that of the global attack. In our experiments,
there are four participants (including the local attacker). The
goal of the attacker is to learn if a target input has been a
member of the training data of any other participants. Table X
shows the accuracy of our attack on various datasets. As
expected, a local attack has a lower accuracy compared to the
global attack; this is because the local attacker observes the
aggregate model parameters of all participants, which limits
the extent of membership leakage. The accuracy of the local
attacker degrades for larger numbers of participants. This is
shown in Table XIII for the CIFAR100 on Alexnet model.
E. Federated Learning Settings: Active Inference Attacks
Table X shows the results of attacks on federated learning.
(cid:24)(cid:22)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
The Gradient Ascent Attacker:
In this scenario, the attacker
adversarially manipulates the learning process to improve the
membership inference accuracy. The active attack is described
in Section II-C. We evaluate the attack accuracy on predicting
the membership of 100 randomly sampled member instances,
from the target model, and 100 non-member instances. For
all such target instances (whose membership is unknown to
the attacker), the attacker updates their data features towards
ascending the gradients of the global model (in case of the
global attack) or the local model (in the case of a local attack).
Figure 7 compares the last-layer gradient norm of the target
model for different data points. As Figure 7a shows, when
the attacker ascends on the gradients of the target instances,
the gradient norm of the target members will be very similar
to that of non-target member instances in various training
epochs. On the other hand, this is not true for the non-member
instances as shown in Figure 7b.
Intuitively,
this is because applying the gradient ascent
algorithm on a member instance will trigger the target model
to try to minimize its loss by descending in the direction
of the model’s gradient for those instances (and therefore
nullify the effect of the attacker’s ascent). For target non-
member instances, however,
the model will not explicitly
change their gradient, as they do not inﬂuence the training
loss function. The attacker repeats gradient ascend algorithm
for each epoch of the training, therefore, the gradient of the
model will keep increasing on such non-member instances.
Figure 7c depicts the resulted distinction between the gradient
norm of the member and non-member target instances. The
active gradient ascend attacker forces the target model
to
behave drastically different between target member and target
non-member instances which makes the membership inference
attack easier. As a result, compared to the passive global
attacker we see that the active attack can noticeably gain higher
accuracy. In the local case, the accuracy is lower than the
global attack due to the observation of aggregated parameters
from multiple participants.
The Isolating Attacker: The parameter aggregation in the
federated learning scenario negatively inﬂuences the accuracy
of the membership inference attacks. An active global attacker
can overcome this problem by isolating a target participant,
and creating a local view of the network for it. In this
scenario, the attacker does not send the aggregate parameters
of all parties to the target party. Instead, the attacker isolates
the target participant and segregates the target participant’s
learning process.
When the attacker isolates the target participant, then the
target participant’s model does not get aggregated with the
parameters of other parties. As a result, it stores more infor-
mation about its training dataset in the model. Thus, simply
isolating the training of a target model signiﬁcantly increases
the attack accuracy. We can apply the isolating method to
the gradient ascent attacker and further improve the attacker
accuracy. See Table X for all the results.
V. RELATED WORK
Investigating different
inference attacks on deep neural
networks is an active area of research.
A. Membership Inference Attacks
Multiple research papers have studied membership inference
attacks in a black-box setting [6], [16], [7]. Homer et al. [4]
performed one of the ﬁrst membership inference attacks on
genomic data. Shokri et al. [6] showed that an ML model’s
output has distinguishable properties about its training data,
which could be exploited by the adversary’s inference model.
They introduced shadow models that mimic the behavior of
the target model, which are used by the attacker to train
the attack model. Salem et al. [17] extended the attacks of
Shokri et al. [6] and showed empirically that it is possible
to use a single shadow model (instead of several shadow
models used in [6]) to perform the same attack. They further
demonstrated that even if the attacker does not have access
to the target model’s training data, she can use statistical
properties of outputs (e.g., entropy) to perform membership in-
ference. Yeom et al. [7] demonstrated the relationship between
overﬁtting and membership inference attacks. Hayes et al. [18]
used generative adversarial networks to perform membership
attacks on generative models.
Melis et al. [19] developed a new set of membership
inference attacks for the collaborative learning. The attack
assumes that the participants update the central server after
each mini-batch, as opposed to updating after each training
epoch [20], [21]. Also, the proposed membership inference
attack is designed exclusively for models that use explicit
word embeddings (which reveal the set of words used in the
training sentences in a mini-batch) with very small training
mini-batches.
In this paper, we evaluate standard learning mechanisms
for deep learning and standard target models for various
architectures. We showed that our attacks work even if we
use pre-trained, state-of-the-art target models.
Differential privacy [22], [23] has been used as a strong
defense mechanism against inference attacks in the context of
machine learning [24], [25], [26], [27]. Several works [28],
[29], [30] have shown that by using adversarial training, one
can ﬁnd a better trade-off between privacy and model accuracy.
However, the focus of this line of work is on the membership
inference attack in the black-box setting.
B. Other Inference Attacks
An attacker with additional information about the training
data distribution can perform various types of inference at-
tacks. Input inference [31], attribute inference [32], parameter
inference [33], [34], and side-channel attacks [35] are several
examples of such attacks. Ateniese et al. [36] show that an
adversary with access to the parameters of machine learning
models such as Support Vector Machines (SVM) or Hidden
Markov Models (HMM) [37] can extract valuable information
about the training data (e.g., the accent of the speakers in
speech recognition models).
(cid:24)(cid:22)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
VI. CONCLUSIONS
We designed and evaluated novel white-box membership
inference attacks against neural network models by exploiting
the privacy vulnerabilities of the stochastic gradient descent
algorithm. We demonstrated our attacks in the stand-alone and
federated settings, with respect to passive and active inference
attackers, and assuming different adversary prior knowledge.
We showed that even well-generalized models are signiﬁcantly
susceptible to such white-box membership inference attacks.
Our work did not investigate theoretical bounds on the privacy
leakage of deep learning in the white-box setting, which would
remain as a topic of future research.
ACKNOWLEDGEMENTS
This work was supported in part by the NSF grant CNS-
1525642, as well as the Singapore Ministry of Education
Academic Research Fund Tier 1, R-252-000-660-133. Reza
Shokri would like to acknowledge the support of NVIDIA
Corporation with the donation of a Titan Xp GPU which was
used for this research.
REFERENCES
[1] C. Dwork, A. Smith, T. Steinke, and J. Ullman, “Exposed! a survey of
attacks on private data,” 2017.
[2] I. Dinur and K. Nissim, “Revealing information while preserving pri-
vacy,” in Proceedings of the twenty-second ACM SIGMOD-SIGACT-
SIGART symposium on Principles of database systems. ACM, 2003,
pp. 202–210.
[3] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou, “Learning your
identity and disease from research papers: information leaks in genome
wide association study,” in Proceedings of the 16th ACM conference on
Computer and communications security. ACM, 2009, pp. 534–544.
[4] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe,
J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig,
“Resolving individuals contributing trace amounts of dna to highly
complex mixtures using high-density snp genotyping microarrays,” PLoS
genetics, vol. 4, no. 8, p. e1000167, 2008.
[5] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan, “Robust
traceability from trace amounts,” in Foundations of Computer Science
(FOCS), 2015 IEEE 56th Annual Symposium on.
IEEE, 2015, pp.
650–669.
[6] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership
inference attacks against machine learning models,” in Security and
Privacy (SP), 2017 IEEE Symposium on, 2017.
[7] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in
machine learning: Analyzing the connection to overﬁtting,” in IEEE
Computer Security Foundations Symposium, 2018.
[8] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, and
D. Bacon, “Federated learning: Strategies for improving communication
efﬁciency,” arXiv preprint arXiv:1610.05492, 2016.
[9] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
Citeseer, Tech. Rep., 2009.
[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.
[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[12] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks.” in CVPR, vol. 1, no. 2, 2017, p. 3.
[13] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning, 2016, vol. 1.
[14] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and
computing, vol. 17, no. 4, pp. 395–416, 2007.
[15] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understand-
ing deep learning requires rethinking generalization,” arXiv preprint
arXiv:1611.03530, 2016.
[16] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A.
Gunter, and K. Chen, “Understanding membership inferences on well-
generalized learning models,” arXiv preprint arXiv:1802.04889, 2018.
[17] A. Salem, Y. Zhang, M. Humbert, M. Fritz, and M. Backes, “Ml-leaks:
Model and data independent membership inference attacks and defenses
on machine learning models,” arXiv preprint arXiv:1806.01246, 2018.
[18] J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro, “Logan: evalu-
ating privacy leakage of generative models using generative adversarial
networks,” arXiv preprint arXiv:1705.07663, 2017.
[19] L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting
unintended feature leakage in collaborative learning,” arXiv preprint
arXiv:1805.04049, 2018.
[20] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in
Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security. ACM, 2015, pp. 1310–1321.
[21] H. B. McMahan, E. Moore, D. Ramage, S. Hampson et al.,
“Communication-efﬁcient learning of deep networks from decentralized
data,” arXiv preprint arXiv:1602.05629, 2016.
[22] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise
to sensitivity in private data analysis,” in Theory of Cryptography
Conference. Springer, 2006, pp. 265–284.
[23] C. Dwork, A. Roth et al., “The algorithmic foundations of differential
privacy,” Foundations and Trends R(cid:4) in Theoretical Computer Science,
vol. 9, no. 3–4, pp. 211–407, 2014.
[24] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differentially private
empirical risk minimization,” Journal of Machine Learning Research,
vol. 12, no. Mar, pp. 1069–1109, 2011.
[25] R. Bassily, A. Smith, and A. Thakurta, “Private empirical risk mini-
mization: Efﬁcient algorithms and tight error bounds,” in Foundations
of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on.
IEEE, 2014, pp. 464–473.
[26] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, “Deep learning with differential privacy,” in
Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2016, pp. 308–318.
[27] N. Papernot, S. Song, I. Mironov, A. Raghunathan, K. Talwar, and
´U. Erlingsson, “Scalable private learning with pate,” arXiv preprint
arXiv:1802.08908, 2018.
[28] M. Nasr, R. Shokri, and A. Houmansadr, “Machine learning with
membership privacy using adversarial regularization,” in Proceedings of
the 2018 ACM SIGSAC Conference on Computer and Communications
Security. ACM, 2018, pp. 634–646.
[29] J. Hamm, “Minimax ﬁlter: learning to preserve privacy from inference
attacks,” The Journal of Machine Learning Research, vol. 18, no. 1, pp.
4704–4734, 2017.
[30] C. Huang, P. Kairouz, X. Chen, L. Sankar, and R. Rajagopal, “Generative
adversarial privacy,” arXiv preprint arXiv:1807.05306, 2018.
[31] M. Fredrikson, S. Jha, and T. Ristenpart, “Model
inversion attacks
that exploit conﬁdence information and basic countermeasures,” in
Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2015, pp. 1322–1333.
[32] N. Carlini, C. Liu, J. Kos, ´U. Erlingsson, and D. Song, “The secret
sharer: Measuring unintended neural network memorization & extracting
secrets,” arXiv preprint arXiv:1802.08232, 2018.
[33] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in USENIX Security, 2016.
[34] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine
learning,” arXiv preprint arXiv:1802.05351, 2018.
[35] L. Wei, Y. Liu, B. Luo, Y. Li, and Q. Xu, “I know what you see: Power
side-channel attack on convolutional neural network accelerators,” arXiv
preprint arXiv:1803.05847, 2018.
[36] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and
G. Felici, “Hacking smart machines with smarter ones: How to extract
meaningful data from machine learning classiﬁers,” International Jour-
nal of Security and Networks, vol. 10, no. 3, pp. 137–150, 2015.
[37] C. Robert, Machine learning, a probabilistic perspective.
Taylor &
Francis, 2014.
(cid:24)(cid:22)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
APPENDIX A
ARCHITECTURE OF THE ATTACK MODEL
TABLE XIV: Attack model layer sizes
Name
Layers
Output Component
2 Fully Connected Layers
Label Component
2 Fully Connected Layers
Loss Component
2 Fully Connected Layers
Gradient Component
Convolutional Layer
2 Fully Connected Layers
Encoder Component
4 Fully Connected Layers
Decoder Component
2 Fully Connected Layers
Details
Sizes: 128, 64
Activation: ReLU
Dropout: 0.2
Sizes: 128, 64
Activation: ReLU
Dropout: 0.2
Sizes: 128, 64
Activation: ReLU
Dropout: 0.2
Kernels: 1000
Kernel size: 1× Next layer
Stride:1
Dropout: 0.2
Sizes: 128, 64
Activation: ReLU
Dropout: 0.2
Sizes: 256, 128, 64, 1
Activation: ReLU
Dropout: 0.2
Sizes: 64, 4
Activation: ReLU
Dropout: 0.2
(cid:24)(cid:22)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply.