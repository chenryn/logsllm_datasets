nodes, we add one edge for each perturbation under the unnotice-
ability constraint (see line 13-20 in Alg. 2). Besides, we consider
the nodes that are not included in the ℎ-hop subgraph. In this case,
we add two edges simultaneously and force the nodes, outside of
the ℎ-hop subgraph, to become the common neighbours under the
unnoticeability constraint (see line 21-26 in Alg. 2).
Regarding the search time complexity, the best-case complex-
ity is max(O(|Γℎ(𝑥)|), O(|Γℎ(𝑦)|)), which can be achieved when
only considering link-hidden attack. The worst-case complexity
is O(|𝑉𝑠| × (|Γℎ−1(𝑥)| + |Γℎ−1(𝑦)|)), which is equal to the average
complexity of Alg. 1. The benefit of Alg. 2 is more significant in
applications where the adversary is more focusing on hiding links.
5 EXPERIMENTAL EVALUATION
We have conducted an extensive array of experiments to evaluate
our proposed methods, including our greedy algorithm (GGSP)
and its efficient variation (OGSP). Our results show that both of
them are able to reduce the availability of the SEAL framework
significantly, achieving strong performance on various datasets.
More importantly, our experimental results have also shown that
our adversarial attacks mounted based on SEAL can be readily
transferred to several existing heuristics in the literature. We run
the experiments 5 times and then use the average attack success
rate (ASR) and the average AUC as our evaluation metrics. To make
direct comparisons, we use the same model architectures as SEAL
shown in Table 3, where 𝑘 is set to ensure that 60% of the subgraph
nodes are larger than 𝑘 [34, 35].
Datasets. We have selected four datasets as the benchmarks to
evaluate our methods. The datasets statistics are given in Table 2.
USAir is a network of US Airlines [5], which average node degree
is 12.81. NS is a collaboration network of researchers in network
science [24], which average node degree is 3.45. Celegans [32] is a
neural network of C.elegans, which average node degree is 14.46
and PB is a network of US political blogs [1], which average node
degree is 27.36.
Similar to SEAL, we split the existent links randomly into a
positive training set (80%) and testing set (10%). As for negative
Figure 3: An illustration of effective edge perturbations: add
perturbations in its global graph (left) and corresponding ef-
fects in its subgraph (right), where the red edges denote the
ineffective perturbations, the blue edges indicate the effec-
tive perturbations, bold dash lines represent edge deletion,
and bold solid lines denote addition.
𝑥 𝑦 ) = 1 then
//decrease common neighbours
for {∀𝑢, 𝑣 ∈ 𝑉𝑠 ∩ {Γℎ(𝑥) ∩ Γℎ(𝑦)}} ∧ {𝑒(𝑢, 𝑣) = 1} do
if Λ(G(𝑡), G(𝑡) − 𝑒(𝑢, 𝑣)) < 0.004 then
G′ ← G(𝑡) − 𝑒(𝑢, 𝑣);
𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ← 𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ∪ {G′};
end if
end for
//increase common neighbours
for ∀𝑢 ∈ 𝑉𝑠 ∩ {Γℎ(𝑥)/Γℎ(𝑦)} ∧ 𝑣 ∈ {Γℎ−1(𝑦)/Γℎ(𝑥)}; do
if Λ(G(𝑡), G(𝑡) + 𝑒(𝑢, 𝑣)) < 0.004} ∧ (𝑢, 𝑣) ≠ (𝑥, 𝑦) then
Algorithm 2 Optimized Graph Structure Perturbation (OGSP)
1: Input: Graph G(𝑡)(V(𝑡), E(𝑡)), ℎ-hop subgraph 𝐺 (𝑡)
𝑥 𝑦 .
2: 𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ← ∅;
𝑥 𝑦 , 𝑋 (0)
3: if 𝐹(𝐴(0)
4:
5:
6:
7:
8:
9:
10:
11:
12: else
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
end if
end for
for ∀𝑢 ∈ 𝑉𝑠 ∩ {Γℎ(𝑦)/Γℎ(𝑥)} ∧ 𝑣 ∈ {Γℎ−1(𝑥)/Γℎ(𝑦)}; do
if Λ(G(𝑡), G(𝑡) + 𝑒(𝑢, 𝑣)) < 0.004} ∧ (𝑢, 𝑣) ≠ (𝑥, 𝑦) then
end if
end for
for ∀𝑣 ∈ 𝑉𝑠/{Γℎ(𝑥) ∪ Γℎ(𝑦)}, ∀𝑖 ∈ Γℎ−1(𝑥), and ∀𝑗 ∈
Γℎ−1(𝑦) do
G′ ← G(𝑡) + 𝑒(𝑢, 𝑣);
𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ← 𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ∪ {G′};
G′ ← G(𝑡) + 𝑒(𝑢, 𝑣);
𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ← 𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ∪ {G′};
if Λ(G(𝑡), G(𝑡) + 𝑒(𝑖, 𝑣) + 𝑒(𝑣, 𝑗) < 0.004 then
G′ ← G(𝑡) + 𝑒(𝑖, 𝑣) + 𝑒(𝑣, 𝑗)
𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ← 𝑆𝑠𝑡𝑟𝑢𝑐𝑡 ∪ {G′};
26:
27:
28:
29:
30:
31: end if
32: Return: 𝑆𝑠𝑡𝑟𝑢𝑐𝑡
end if
end for
300 millions of Amazon customer accounts in 2018 as reported 1.
1https://expandedramblings.com/index.php/amazon-statistics/
xy?xy?Enclosing subgraph extractionTable 2: Dataset statistics and AUC using SEAL
Network #nodes/ #edges #training/ #testing AUC
USAir
NS
Celegans
PB
332/2, 126
1, 589/2, 742
297/2, 148
1, 222/16, 714
3, 400/424
4, 386/548
3, 436/428
26, 742/3, 342
0.959
0.959
0.885
0.940
sets, we randomly sample an equal number of non-existent links
as the negative training set and testing set, respectively. We retrain
SEAL for 50 epochs for each dataset, and select the model with the
smallest loss on 10% validation data; these pre-trained models are
used as our target models to mount attacks.
Note that, we remove the edges between the two target nodes in
the enclosing subgraphs while we train graph neural network, as did
in [34]. This is because these edges would contain the link existence
information, while is not available in the enclosing subgraphs of
testing links. As observed, we report the model AUC on clean data
in Table 2.
We report success if the attack produces an adversarial example
with the incorrect prediction within the perturbation bound Δ, and
the associated perturbed graph still satisfies the unnoticeability
constraint. In our experiments, we set Δ as the target link degree,
which is the sum of degrees of two nodes. This is inspired by the
observation that high-degree links are harder to attack than the
low-degree ones.
Within the testing set, we select 10 links with the highest pre-
diction margin, including 5 positive links and 5 negative links, i.e.,
they clearly are correct predictions (best-set). We also select 10 links
with the lowest prediction margin (but still correctly predicted),
including 5 positive links and 5 negative links (worst-set). Finally,
we select 20 links randomly sampled from the links that are cor-
rectly predicted, including 10 positive links and 10 negative links,
respectively (random-set). These will serve as the target links for our
attack. By default, the average ASR and AUC are reported according
to the prediction results of the links randomly selected.
Table 3: Model architecture of SEAL
Layer Type
Parameter
4 Graph Convolutions + Tanh
32, 32, 32, 1 channels
Max-K SortPooling
1-D Convolution + ReLU
1-D Convolution + ReLU
Dense Layers
Log-Softmax
k
16 output channels,
filter size 2, step size 2
32 output channels,
filter size 5, step size 1
128 units
2 channels
5.1 Attacks on SEAL
We start by analyzing both of our two algorithms, GGSP and OGSP,
by inspecting their influences on link prediction performance of
SEAL (with full knowledge of the network graph). In Table 4, we
report the average ASR and average AUC over 5 runs when per-
forming attacks on SEAL. For each run, we use the random-set as
our target links. We can see GGSP achieves very high ASR on NS —
100%, and its AUC degrades to 0.000. Even OGSP has an attack per-
formance degradation on this dataset, it still can decline its model
AUC to 0.038.
In Fig. 5, we report how our methods affect different testing sets
(best, worst and random). We define the prediction margin as the
difference between the ground truth label probability with SEAL
and the target label probability. The smaller prediction margin
indicates better attack performance (margin less than 0 indicates a
successful attack). As observed in Fig. 5, most of the average margins
are under 0, represented as ‘star’. best-set is harder to attack as its
initial prediction margin are large (typically close to 1). Overall,
it still can achieve reasonably good attack performance with our
algorithms.
Table 4: Average attack success rate/Average AUC
Data
USAir
NS
Celegans
PB
GGSP
OGSP
96.3%/0.050
100%/0.000
87.5%/0.133
82.5%/0.207
98%/0.000
79%/0.038
82%/0.166
78%/0.294
Furthermore, to inspect and compare the time cost of our two
attack algorithms, we also report the average attack time per link
across different datasets. As Fig. 4 shows, OGSP is way more ef-
ficient than GGSP; it can even be approximately 10× faster on
Celegans. Note that our time cost is averaged over the target links,
containing 50% positive links and 50% negative links. We suspect
that OGSP can achieve even better performance at runtime while
the adversary is more focused on hiding links.
Figure 4: Average attack time per link: GGSP vs. OGSP.
GGSP produces better results. As observed, even with only 25%
perturbable nodes, our algorithm can still achieve a high ASR.
Table 5: Average ASR (GGSP)
|𝑉𝑠|/𝑁
0.25
0.50
0.75
1.00
USAir
NS
Celegans
PB
66.3%
91.3%
92.5%
96.3%
97.5%
100%
100%
100%
72.5%
81.3%
84.3%
87.5%
69.9%
79.3%
81.2%
82.5%
We further report how the target link surrounding structure
affects the attack performance. We run three sets of experiments in
USAir when the adversary can perturb 25% of the entire node set.
The target links are categorized according to their link degrees. As
seen in Table 6, the target links with higher degrees achieve lower
ASRs, indicating that higher-degree links are harder to attack. The
ASR of the target links in the range [1 : 30) can achieve as high as
90.96%.
Table 6: ASR: Target link surrounding structure complexity
Degree range
#target links
Attack success rate
[1 : 30)
188
90.96%
[31 : 90)
[90 : ∞)
93
78
64.52%
43.58%
Inspecting an adversarial example. Fig. 6 illustrates a real adver-
sarial example mounted for NS. We first randomly select a pair of
nodes as the target nodes (grey); the link (dash line) is to be pre-
dicted using the pre-trained SEAL. Fig. 6a shows its 1-hop enclosing
subgraph; the link is initially predicted as 𝑐 = 0 by SEAL, indicating
the link is negative. Fig. 6a shows its 1-hop enclosing subgraph with
our attack method. The edge in blue is suggested to be added by
our attack. Even just with this edge addition, the link is predicted
as positive.
Transferability of attacks. Note that, our overall objective is to
offer a comprehensive study on the ability of an “adversary" to ma-
nipulate link prediction via adversarial machine learning. For this
purpose, we analyze the transferability of the adversarial attack,
generated based on SEAL, to existing heuristics, including com-
mon neighbors (CN), Jaccard [20], preference attachment (PA) [4],
Adam-Adar (AA) [2], resource allocation (RA) [36], Katz index [19],