title:Efficient Coflow Scheduling Without Prior Knowledge
author:Mosharaf Chowdhury and
Ion Stoica
Efﬁcient Coﬂow Scheduling
Without Prior Knowledge
Mosharaf Chowdhury, Ion Stoica
UC Berkeley
{mosharaf, istoica}@cs.berkeley.edu
ABSTRACT
Inter-coﬂow scheduling improves application-level commu-
nication performance in data-parallel clusters. However, ex-
isting efﬁcient schedulers require a priori coﬂow informa-
tion and ignore cluster dynamics like pipelining, task fail-
ures, and speculative executions, which limit their applica-
bility. Schedulers without prior knowledge compromise on
performance to avoid head-of-line blocking. In this paper,
we present Aalo that strikes a balance and efﬁciently sched-
ules coﬂows without prior knowledge.
Aalo employs Discretized Coﬂow-Aware Least-Attained
Service (D-CLAS) to separate coﬂows into a small num-
ber of priority queues based on how much they have al-
ready sent across the cluster. By performing prioritization
across queues and by scheduling coﬂows in the FIFO order
within each queue, Aalo’s non-clairvoyant scheduler reduces
coﬂow completion times while guaranteeing starvation free-
dom. EC2 deployments and trace-driven simulations show
that communication stages complete 1.93× faster on aver-
age and 3.59× faster at the 95th percentile using Aalo in
comparison to per-ﬂow mechanisms. Aalo’s performance is
comparable to that of solutions using prior knowledge, and
Aalo outperforms them in presence of cluster dynamics.
CCS Concepts
•Networks → Cloud computing;
Keywords
Coﬂow; data-intensive applications; datacenter networks
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’15, August 17 - 21, 2015, London, United Kingdom
© 2015 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787480
1 Introduction
Communication is crucial for analytics at scale [19, 8, 12,
20, 25]. Yet, until recently, researchers and practitioners have
largely overlooked application-level requirements when im-
proving network-level metrics like ﬂow-level fairness and
ﬂow completion time (FCT) [29, 10, 16, 8, 14]. The coﬂow
abstraction [18] bridges this gap by exposing application-
level semantics to the network. It builds upon the all-or-
nothing property observed in many aspects of data-parallel
computing like task scheduling [51, 12] and distributed
cache allocation [11]; for the network, it means all ﬂows
must complete for the completion of a communication stage.
Indeed, decreasing a coﬂow’s completion time (CCT) can
lead to faster completion of corresponding job [20, 25, 19].
However, inter-coﬂow scheduling to minimize the aver-
age CCT is NP-hard [20]. Existing FIFO-based solutions,
e.g., Baraat [25] and Orchestra [19], compromise on perfor-
mance by multiplexing coﬂows to avoid head-of-line block-
ing. Varys [20] improves performance using heuristics like
smallest-bottleneck-ﬁrst and smallest-total-size-ﬁrst, but it
assumes complete prior knowledge of coﬂow characteristics
like the number of ﬂows, their sizes, and endpoints.
Unfortunately, in many cases, coﬂow characteristics are
unknown a priori. Multi-stage jobs use pipelining between
successive computation stages [30, 22, 46, 3] – i.e., data is
transferred as soon as it is generated – making it hard to
know the size of each ﬂow. Moreover, a single stage may
consist of multiple waves [11],1 preventing all ﬂows within a
coﬂow from starting together. Finally, task failures and spec-
ulation [50, 30, 24] result in redundant ﬂows; meaning, the
exact number of ﬂows or their endpoints cannot be deter-
mined until a coﬂow has completed. Consequently, coﬂow
schedulers that rely on prior knowledge remain inapplicable
to a large number of use cases.
In this paper, we present a coordinated inter-coﬂow sched-
uler – called Coﬂow-Aware Least-Attained Service (CLAS)
– to minimize the average CCT without any prior knowledge
of coﬂow characteristics. CLAS generalizes the classic least-
attained service (LAS) scheduling discipline [45] to coﬂows.
However, instead of independently considering the number
of bytes sent by each ﬂow, CLAS takes into account the total
1A wave is deﬁned as the set of parallel tasks from the same stage of a job
that have been scheduled together.
393(c) Per-ﬂow fairness
(d) Decentralized LAS
(a) Datacenter fabric
(b) Coﬂow arrival times
(e) CLAS
(f) The optimal schedule
Figure 1: Online coﬂow scheduling over a 3 × 3 datacenter fabric with three ingress/egress ports (a). Flows in ingress ports are organized
by destinations and color-coded by coﬂows – C1 in orange/light, C2 in blue/dark, and C3 in black. Coﬂows arrive online over time (b).
Assuming each port can transfer one unit of data in one time unit, (c)–(f) depict the allocations of ingress port capacities (vertical axis) for
different mechanisms: The average CCT for (c) per-ﬂow fairness is 5.33 time units; (d) decentralized LAS is 5 time units; (e) CLAS with
instant coordination is 4 time units; and (f) the optimal schedule is 3.67 time units.
number of bytes sent by all the ﬂows of a coﬂow. In partic-
ular, CLAS assigns each coﬂow a priority that is decreasing
in the total number of bytes the coﬂow has already sent. As
a result, smaller coﬂows have higher priorities than larger
ones, which helps in reducing the average CCT. Note that
for heavy-tailed distributions of coﬂow sizes, CLAS approx-
imates the smallest-total-size-ﬁrst heuristic,2 which has been
shown to work well for realistic workloads [20].
For light-tailed distributions of coﬂow sizes, however, a
straightforward implementation of CLAS can lead to ﬁne-
grained sharing,3 which is known to be suboptimal for min-
imizing the average CCT [19, 25, 20]. The optimal schedule
in such cases is FIFO [25].
We address this dilemma by discretizing coﬂow priorities.
Instead of decreasing a coﬂow’s priority based on every byte
it sends, we decrease its priority only when the number of
bytes it has sent exceeds some predeﬁned thresholds. We call
this discipline Discretized CLAS, or D-CLAS for short (§4).
In particular, we use exponentially-spaced thresholds, where
the i-th threshold equals bi, (b > 1).
We implement D-CLAS using a multi-level scheduler,
where each queue maintains all the coﬂows with the same
priority. Within each queue, coﬂows follow the FIFO or-
der. Across queues, we use weighted fair queuing at the
coﬂow granularity, where weights are based on the queues’
priorities. Using weighted sharing, instead of strict priori-
ties, avoids starvation because each queue is guaranteed to
receive some non-zero service. By approximating FIFO (as
in Baraat [25]) for light-tailed coﬂows and smallest-coﬂow-
2Under the heavy-tailed distribution assumption, the number of bytes al-
ready sent is a good predictor of the actual coﬂow size [41].
3Consider two identical coﬂows, CA and CB, that start at the same time.
As soon as we send data from coﬂow CA, its priority will decrease, and we
will have to schedule coﬂow CB. Thus, both coﬂows will continuously be
interleaved and will ﬁnish roughly at the same time – both taking twice as
much time as a single coﬂow in isolation.
ﬁrst (as in Varys [20]) for heavy-tailed coﬂows, the proposed
scheduler works well in practice.
We have implemented D-CLAS in Aalo,4 a system that
supports coﬂow dependencies and pipelines, and works well
in presence of cluster dynamics like multi-wave scheduling
(§5). Aalo requires no prior knowledge of coﬂow charac-
teristics, e.g., coﬂow size, number of ﬂows in the coﬂow,
or its endpoints. While Aalo needs to track the total num-
ber of bytes sent by a coﬂow to update its priority,5 this
requires only loose coordination as priority thresholds are
coarse. Moreover, coﬂows whose total sizes are smaller than
the ﬁrst priority threshold require no coordination. Aalo runs
without any changes to the network or user jobs, and data-
parallel applications require minimal changes to use it (§6).
We deployed Aalo on a 100-machine EC2 cluster and
evaluated it by replaying production traces from Facebook
and with TPC-DS [6] queries (§7). Aalo improved CCTs
both on average (up to 2.25×) and at high percentiles (2.93×
at the 95th percentile) w.r.t. per-ﬂow fair sharing, which
decreased corresponding job completion times. Aalo’s av-
erage improvements were within 12% of Varys for single-
stage, single-wave coﬂows, and it outperformed Varys for
multi-stage, multi-wave coﬂows by up to 3.7× by removing
artiﬁcial barriers and through dependency-aware schedul-
ing. In trace-driven simulations, we found Aalo to perform
2.7× better than per-ﬂow fair sharing and up to 16× bet-
ter than fully decentralized solutions that suffer signiﬁcantly
due to the lack of coordination. Simulations show that Aalo
performs well across a wide range of parameter space and
coﬂow distributions.
We discuss current limitations and relevant future research
in Section 8 and compare Aalo to related work in Section 9.
4In Bangla, Aalo (pronounced \'ä-l¯o\) means light.
5As stated by Theorem A.1 in Appendix A, any coﬂow scheduler’s perfor-
mance can drop dramatically in the absence of coordination.
1!2!3!1!2!3!Ingress Ports!(Machine Uplinks)!Egress Ports!(Machine Downlinks)!DC Fabric!3!2!4!Arrival Time!C1!0!C2!1!C3!0!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!C1!C2!C3!Coﬂow Arrival Time!0!1!0!3!1!2!3!1!2!3!Ingress Ports!(Machine Uplinks)!Egress Ports!(Machine Downlinks)!DC Fabric!3!2!4!Arrival Time!C1!0!C2!1!C3!0!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!C1!C2!C3!Coﬂow Arrival Time!0!1!0!3!3!1!2!3!1!2!3!Ingress Ports!(Machine Uplinks)!Egress Ports!(Machine Downlinks)!DC Fabric!3!2!4!Arrival Time!C1!0!C2!1!C3!0!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!C1!C2!C3!Arrival Time!0!1!0!3!1!2!3!1!2!3!Ingress Ports!(Machine Uplinks)!Egress Ports!(Machine Downlinks)!DC Fabric!3!2!4!Arrival Time!C1!0!C2!1!C3!0!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!C1!C2!C3!Arrival Time!0!1!0!3!1!2!3!1!2!3!Ingress Ports!(Machine Uplinks)!Egress Ports!(Machine Downlinks)!DC Fabric!3!2!4!Arrival Time!C1!0!C2!1!C3!0!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!C1!C2!C3!Arrival Time!0!1!0!3!1!2!3!1!2!3!Ingress Ports!(Machine Uplinks)!Egress Ports!(Machine Downlinks)!DC Fabric!3!2!4!Arrival Time!C1!0!C2!1!C3!0!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!3!6!P2!P1!Time!C1!C2!C3!Arrival Time!0!1!0!3942 Motivation
Before presenting our design, it is important to understand
the challenges and opportunities in non-clairvoyant coﬂow
scheduling for data-parallel directed acyclic graphs (DAGs).
2.1 Background
Non-Clairvoyant Coﬂows A coﬂow is a collection of par-
allel ﬂows with distributed endpoints, and it completes af-
ter all its ﬂows have completed [18, 20, 19]. Jobs with
one coﬂow ﬁnish faster when coﬂows complete faster [20].
Data-parallel DAGs [30, 50, 46, 2, 3] with multiple stages
can be represented by multiple coﬂows with dependencies
between them. However, push-forward pipelining between
subsequent computation stages of a DAG [22, 30, 46, 3] re-
moves barriers at the end of coﬂows, and knowing ﬂow sizes
becomes infeasible. Due to multi-wave scheduling [11], all
ﬂows of a coﬂow do not start at the same time either.
Hence, unlike existing work [19, 20, 25], we do not as-
sume anything about a coﬂow’s characteristics like the num-
ber of ﬂows, endpoints, or waves, the size of each ﬂow, their
arrival times, or the presence of barriers.
Non-Blocking Fabric In our analysis, we abstract out the
entire datacenter fabric as one non-blocking switch [10, 15,
20, 9, 31, 26] and consider machine uplinks and downlinks
as the only sources of contention (Figure 1a). This model is
attractive for its simplicity, and recent advances in datacenter
fabrics [9, 28, 40] make it practical as well. However, we
use this abstraction only to simplify our analysis; we do not
require nor enforce this in our evaluation (§7).
2.2 Challenges
An efﬁcient non-clairvoyant [39] coﬂow scheduler must ad-
dress two primary challenges:
1. Scheduling without complete knowledge: Without a
priori knowledge of coﬂows, heuristics like smallest-
bottleneck-ﬁrst [20] are inapplicable – one cannot sched-
ule coﬂows based on unknown bottlenecks. Worse, re-
dundant ﬂows from restarted and speculative tasks un-
predictably affect a coﬂow’s structure and bottlenecks.
While FIFO-based schedulers (e.g., FIFO-LM in Baraat
[25]) do not need complete knowledge, they multiplex to
avoid head-of-line blocking, losing performance.
2. Need for coordination: Coordination is the key to per-
formance in coﬂow scheduling. We show analytically
(Theorem A.1) and empirically (§7.2.1, §7.6) that fully
decentralized schedulers like Baraat [25] can perform
poorly in data-parallel clusters because local-only ob-
servations are poor indicators of CCTs of large coﬂows.
Fully centralized solutions like Varys [20], on the con-
trary, introduce high overheads for small coﬂows.
2.3 Potential Gains
Given the advantages of coﬂow scheduling and the inabil-
ity of clairvoyant schedulers to support dynamic coﬂow
modiﬁcations and dependencies, a loosely-coordinated non-
clairvoyant coﬂow scheduler can strike a balance between
performance and ﬂexibility.
Figure 2: Aalo architecture. Computation frameworks interact with
their local Aalo daemons using a client library, and the daemons
periodically coordinate to determine the global ordering of coﬂows.
Consider the example in Figure 1 that compares three
non-clairvoyant mechanisms against the optimal clairvoyant
schedule. Per-ﬂow fair sharing (Figure 1c) ensures max-min
fairness in each link, but it suffers by ignoring coﬂows [19,
20]. Applying least-attained service (LAS) [42, 45, 14] in a
decentralized manner (Figure 1d) does not help, because lo-
cal observations cannot predict a coﬂow’s actual size – e.g.,
it shares P1 equally between C1 and C3, being oblivious to
C1’s ﬂow in P2. The FIFO-LM schedule [25] would be at
least as bad. Taking the total size of coﬂows into account
through global coordination signiﬁcantly decreases the aver-
age CCT (Figure 1e). The optimal solution (Figure 1f) ex-
ploits complete knowledge for the minimum average CCT.
The FIFO schedule [19] would have resulted in a lower av-
erage CCT (4.67 time units) than decentralized LAS if C3