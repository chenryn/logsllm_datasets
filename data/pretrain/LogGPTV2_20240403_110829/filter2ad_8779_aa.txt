# tips
笔者最近在学习机器学习相关的知识，看到一些不错的文章。但都是英文版的，所以在看之余干脆翻译一下，这样也方便自己后面温习。下面这篇文章非常的长，很多地方笔者也还不是很明白。所以翻译有错误的地方，望大家斧正。当然，英文好的朋友建议看了译文之后再去看一下原文，应该会有不同的理解。  
原文：[The Unreasonable Effectiveness of Recurrent Neural
Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/ "The
Unreasonable Effectiveness of Recurrent Neural Networks")
# 开始
循环神经网络-RNNs 有一些神奇的地方。我还记得我训练第一个用于[Image
Captioning](https://cs.stanford.edu/people/karpathy/deepimagesent/ "Image
Captioning")的RNN时，对我的第一个Baby模型（任意选择一些超参数）训练十几分钟之后，便开始呈现出看起来很不错的图像描述，这些描述已经处在有真正意义的边缘上了。有时，模型的简单程度与你获得结果的质量进行对比，已超乎你的预期了，这种情况只是其中一次。在当时，这个结果让我大吃一惊的原因是，大家普遍认为RNN应该是非常难训练的（实际上，在有了更多的实验之后，我得出了相反的结论）。时间向前推进一年，这一年我一直在训练RNN，也多次见证了RNN的稳定性与强大之处。但RNN神奇的输出仍然让我感到惊讶。这篇文章将和你分享这其中的奥秘。
> 我们将训练一个逐字生成文本的RNN，并同时思考“这怎么可能做到呢”
顺便说一句，与这篇文章一起，我也在 [Github](https://github.com/karpathy/char-rnn "Github")
上发布了相关代码。允许你训练基于多层LSTM的character-level
语言模型。你给它提供一大块文本，它会学习，然后生成一次一个字符的文本。你也可以使用它来重现我的实验。现在，我们来看看什么是RNN。
# 递归神经网络（Recurrent Neural Networks）
**序列（Sequences）** 。我猜你可能想知道：是什么让 Recurrent Networks 如何特别？Vanilla
神经网络（以及Convolutional
网络）的一个明显的缺陷是它们的API太受限制：它们接收固定大小的矢量作为输入（比如：图像），并产生固定大小的矢量作为输出（比如：不同类别的概率）不仅如此：这些模型执行映射时使用固定数量的计算步骤（比如：模型中的层数）。recurrent
nets 令人兴奋的核心原因是它允许我们对矢量序列进行操作：输入，输出或者最常见情况下的序列。下面列举一些具体例子：  
图中每一个矩形代表一个向量，箭头表示函数（例如：矩阵乘法）。输入向量位红色，输出向量位蓝色，绿色向量表示RNN的状态（它会变得越来越多）。从左到右：(1)
没有RNN的Vanilla 模式，从固定大小的输入到固定大小的输出（例如：图像分类）。(2) 序列输出（例如：image
captioning采用拍摄图像，并输出单词的句子）。(3) 序列输入（例如：情感分析，其中给定的句子被分类为表达正面或者负面情绪）。(4)
序列输入与序列输出（例如：机器翻译。RNN用英语读取一个句子，然后用法语输出一个句子）。(5)
同步序列输入与输出（例如：视频分类，我们希望标记视频的每一帧）。需要注意的是，在每种情况下，长度序列都没有预先指定约束，因为循环变化的部分（绿色块）是固定的，并且可以根据需要应用多次。
正如您所料，这种序列化的操作与通过固定数量的计算步骤相比更为强大，因此对于那些渴望构建更智能系统的人来说也更具吸引力。此外，正如我们稍后将要看到的，RNN将输入向量,状态向量和固定（但已学习）的函数进行组合以产生新的状态向量。在编程术语中，这可以解释为运行一个具有某些输入和一些内部变量所关联的固定程序。从这个角度来看，RNN本质上是在描述程序本身。事实上，众所周知，RNN从某种意义上来说是[Turing-Complete](http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf
"Turing-Complete") ，它们可以模拟任意具有适当权重的程序。但与神经网络的通用逼近定理一样，你不需要理解太多。
> 如果说训练vanilla 神经网络是在优化函数，那么训练recurrent nets就是在优化程序。
**没有序列时的顺序处理**
。你可能为认为将序列作为输入或输出的情况相对较少，但要意识到的重要一点是，即使你的输入/输出是固定向量，仍然可以使用这种强大的序列化形式处理它们。例如，下图展示了来自[DeepMind](https://deepmind.com/
"DeepMind")的两篇非常好的论文的结果。1图，是一种循环网络策略的算法，它的关注点是遍历整张图像；特别是，它学会了从左到右读出门牌号（[Ba
et.al](https://arxiv.org/abs/1412.7755 "Ba et.al")）。2图，是一个通过学习顺序添加颜色到canvas
来生成数字图像的循环网络（[Gregor et.al](https://arxiv.org/abs/1502.04623 "Gregor et.al")
）。
需要注意的是，即使你的数据不是序列形式，你仍然可以制定和训练强大的模型，让它学习按顺序处理。就像你正在学习一个有状态，可以处理有固定大小数据的程序。
**RNN 计算**
。这些东西是如何运作的呢？核心的部分，RNN有一个看似简单的API：它们用来接收输入向量X，并为你提供输出向量Y。但是，关键的是这个输出向量的内容不仅会受到你输入的影响，还会受到你过去输入的整个历史记录的影响。如果作为一个类编写，RNN的API由一个step函数组成：
    rnn = RNN()
    y = rnn.step(x) # x is an input vector, y is the RNN's output vector
RNN类具有一些内部状态，每次step 函数调用它都会更新。在最简单的情况下，该状态由单个隐藏向量h 组成。下面是Vanilla RNN中 step
函数的实现:
    class RNN:
      # ...
      def step(self, x):
        # update the hidden state
        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
        # compute the output vector
        y = np.dot(self.W_hy, self.h)
        return y
上面说明了vanilla RNN 的正向传递过程。该RNN的参数是三个矩阵W_hh,W_xh,W_hy。隐藏状态 self.h 用零向量初始化。函数
np.tanh
实现非线性的方式将激活单元压缩到[-1,1]区间。简要说明下它是如何工作的：tanh内部有两个部分：一个基于先前的隐藏状态，另一个基于当前的输入。Numpy
中np.dot是矩阵乘法。这两个中间体相互作用再加成，然后被tanh压缩到新的状态向量中。用数学形式来表示就是：
其中tanh被应用到向量中的每个元素上。
我们用随机数初始化RNN的矩阵，并且在训练期间的大量工作用于找到产生理想行为的矩阵，其中用一些损失函数进行衡量，该函数表示你输入序列“x”时希望看到哪种的“y”输出作为响应。  
**深入理解**
。RNN是神经网络，如果你进行深度学习，并开始把模型想煎饼一样堆叠起来，每一个部分都能独自工作的很好（如果做的正确）。例如，我们可以形成如下的2层循环网络：
    y1 = rnn1.step(x)
    y = rnn2.step(y1)
换句话说，我们有两个单独的RNN，一个RNN正在接收输入向量，第二个RNN正在接收第一个RNN的输出作为其输入。除非这些RNN都不知道或在意-其实这些输入输出都只是向量，在反向传播期间，一些梯度会每个模块中传递。  
异想天开一下。我想简单提一下，在实践中，我们大多数人使用的模型与我上面提到的称为Long Short-Term Memory (LSTM)
网络的模型略有不同。LSTM是一种特殊类型的recurrent
network，由于其更强大的更新方程和一些吸引人的反向传播特性，在实践中效果更好。我不会详细介绍，但我所说的关于RNN的所有内容都是一样的，除了计算更新的数学表达式（self.h
= ... 这一行）变得更为复杂。从这里开始，我将交替使用术语“RNN / LSTM”，但本文中的所有实验都使用LSTM模型。
# Character-Level 语言模型
好，我们知道RNN是什么，为什么让人激动，以及它们是如何工作的。现在，我们通过一个有趣的应用程序来看看：我们将训练RNN character-level
语言模型。意思是，我们将给RNN一大块文本，并要求它根据已有的一系列字符的序列中对下一个字符的概率分布进行建模。这将允许我们一次生成一个字符的新文本。  
举一个例子，假设我们有4个可能的字母“helo”，并且想要在训练序列“hello”上训练RNN。这个训练序列实际上是4个单独的训练样例。1.“e”出现的概率来自于给出的“h”的情况下；2.“i”出现的概率应该是在“he”出现的情况下；3.“i”也应该是在“hel”的情况下。4.“o”应该是在“hell”出现情况下。  
具体来说，我们将使用1-k编码将每个字符编码为矢量（即，除了词汇表中字符所对应索引处的下标是1，其他位置都是零）  
并使用step函数将它们一次一个的送到RNN中。然后我们观察一系列4维输出向量（一个字符一维），我们将其解释为RNN当前分配给每一个字符在序列中下一次出现的可信度。下图：
$$
这是一个4维输入输出层的RNN示例，隐藏层有3个单元。上图，显示了当RNN被输入字符“hell”时，正向传播的情况。输出层包含RNN为下一个字符分配的可信度（词汇表是“h，e，l，o”）。我们希望绿色数值高，红色数值低。
$$
例如，我们看到在RNN接收字符“h”的第一个时间步骤中，它将1.0的可信度分配给下一个字母为“h”，2.2给字母“e”，-3.0给“l”和4.1给“o”。在我们的训练数据（字符串“hello”）中，下一个正确的字母应是“e”，所以我们希望增加“e”（绿色）的可信度并降低其他字母（红色）的可信度。同样地，我们在4个时间步骤中的每一个都有一个期望的目标角色，我们希望神经网络为其分配更大的可信度。因为RNN完全由可微分运算组成，我们可以运行反向传播算法（来自微积分的链规则的递归应用），以确定我们应该调整每个权重的方向以增加正确目标的分数
（绿色粗体数字）。然后我们可以执行一个 **参数更新**
，它在此梯度方向上轻微推动每个向量的权重。如果我们在参数更新后将相同的输入送到RNN，我们会发现正确字符的分数（例如第一时间步中的“e”）会略高（例如2.3而不是2.2），并且分数不正确的字符会略低一些。然后，我们反复多次重复此过程，直到网络收敛并且其预测最终与训练数据一致，因为接下来始终能预测正确的字符。  
更技术性的解释是们同时在每个输出矢量上使用标准Softmax分类器（通常也称为交叉熵损失）。RNN使用小批量随机梯度下降进行训练，我喜欢使用RMSProp或Adam（每参数自适应学习速率方法）来稳定更新。  
还要注意，第一次输入字符“l”时，目标是“l”，但第二次是目标是“o”。因此，RNN不能单独依赖输入，必须使用其循环连接来跟踪上下文以完成此任务。  
**测试期间** ，我们将一个字符输入到RNN中，然后分析下一个可能出现的字符的分布。我们从这个分布中取样，然后将其反馈回来获取下一个字母。
重复这个过程，你正在采样文本！现在让我们在不同的数据集上训练RNN，看看会发生什么。  
为了进一步说明，我还编写了[ minimal character-level RNN language model in
Python/numpy](https://gist.github.com/karpathy/d4dee566867f8291f086 " minimal
character-level RNN language model in
Python/numpy")。它只有大约100行，如果你在阅读代码方面比文本更好，它可以给出简洁，具体和有用的上述摘要。我们现在将深入研究使用更高效的Lua
/ Torch代码库生成的示例结果。
# Paul Graham generator
让我们首先尝试一个小的英语数据集作为健全性检查。我最喜欢的有趣数据集是[Paul Graham’s
的文章集锦](http://www.paulgraham.com/articles.html "Paul Graham’s
的文章集锦")。基本的想法是，这些文章中有充满智慧，但不幸的是， Paul Graham是一个 慢条斯理的创作者。
如果我们能够按需提供想要的知识，那不是很好吗？ 这就是RNN的用武之地。  
数据集：将PG近五年的文章合成一个约1MB大的文本文件，包含了大约100万个字符（顺便说一下，这被认为是一个非常小的数据集）。模型：让我们训练一个2层LSTM，其中有512个隐藏节点（约350万个参数），每层dropout设为0.5。
我们每一次将训练100个例子，并在长度为100个字符的时间内截断并反向传播。在这样的设置下，TITAN Z
GPU上的一批需要大约0.46秒（这可以用50字符BPTT减少一半，性能成本可以忽略不计）。不用多说，让我们看看来自RNN的样本：
> The surprised in investors weren’t going to raise money. I’m not the company
> with the time there are all interesting quickly, don’t have to get off the
> same programmers. There’s a super-angel round fundraising, why do you can
> do. If you have a different physical investment are become in people who
> reduced in a startup with the way to argument the acquirer could see them
> just that you’re also the founders will part of users’ affords that and an
> alternation to the idea.  
>  [2] Don’t work at first member to see the way kids will seem in advance of
> a bad successful startup. And if you have to act the big company too.
好吧，这个RNN模型完全不能取代PG，但要注意的是RNN是从头开始学习英语并且是在使用的小数据集上（包括你在哪里放逗号，撇号和空格）。我也喜欢它学会支持自己的论点（例如上面的[2]）。有时它还能提供有那么一丝见解的东西，比如：“a
company is a meeting to think to
investors”。如果您想了解更多信息，[这里有50K字符样本](https://cs.stanford.edu/people/karpathy/char-rnn/pg.txt "这里有50K字符样本")。  
**temperature参数** 。我们可以调整采样过程中Softmax函数的temperature参数。将 T
从1降低到更低的数值（例如0.5）使RNN更有信心，但也意味着生成的样本更加保守。相反，较高的 T
将带来更多的多样性，但代价是更多的错误（例如拼写错误等）。将 T 设为接近0的值会产生最像PG说过的话，如：
> is that they were all the same thing that was a startup is that they were
> all the same thing that was a startup is that they were all the same thing
> that was a startup is that they were all the same
译者注：temperature是Softmax函数中的一个参数，默认为1。当 T
很大时，即趋于正无穷时，所有的激活值对应的激活概率趋近于相同（激活概率差异性较小）；而当 T 很低时，即趋于0时，不同的激活值对应的激活概率差异也就越大  
参考：[Softmax Layer](https://www.jianshu.com/p/cb93d5e39bca "Softmax Layer")
# 莎士比亚
经过上面实验，看起来我们可以学习拼写英语单词了。但是如果数据中有更多的结构和风格呢？为了检查这一点，我下载了莎士比亚的所有作品并将它们连接成一个（4.4MB）文件。我们现在可以负担得起培训更大的神经网络，在这种情况下，我们可以尝试使用3层的LSTM隐藏层，每层512个节点。在我们训练它几个小时后，我们获得了以下样本：
PANDARUS:  
Alas, I think he shall be come approached and the day  
When little srain would be attain'd into being never fed,  
And who is but a chain and subjects of his death,  
I should not sleep.
Second Senator:  
They are away this miseries, produced upon my soul,  
Breaking and strongly should be buried, when I perish  
The earth and thoughts of many states.
DUKE VINCENTIO:  
Well, your wit is in the care of side and that.
Second Lord:  
They would be ruled after this chamber, and  
my fair nues begun out of the fact, to be conveyed,  
Whose noble souls I'll have the heart of the wars.