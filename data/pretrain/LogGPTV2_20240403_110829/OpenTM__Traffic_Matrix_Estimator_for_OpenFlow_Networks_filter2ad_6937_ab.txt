-H
6
-H
8
1000
2000
3000
time(s)
4000
5000
6000
(b)
Fig. 1. (a) Testbed topology consisting of Hi: host machines, Si: OpenFlow switches,
and Li: loss emulators. (b) Measured average ﬂow rate for each of the ﬁve OD pairs in
the network.
emulate 1% packet loss. For the purpose of evaluation, OpenTM logs the ﬂow
statistics of all switches every ﬁve seconds for a duration of two hours; we then
analyze the data oﬄine. Note that this is not the case in the real application
where the switches are queried according to the querying scheme chosen by the
network operator. Since we are interested in studying the system in equilibrium,
we remove the ﬁrst 15 minutes of our data as the warm up period.
We start with a very basic question: How fast does OpenTM rate measure-
ments converge? For an element f in the TM, let us assume we have t queries.
The i-th query, ri, is the average rate from the beginning of the measurement
up to that point in time. We deﬁne the convergence point c as the ﬁrst query for
which all the proceeding queries are within 3% of the overall mean rate. We note
that since the rate is assumed to be stationary in this system5, such an average
exists. Also, by a simple application of the Central Limit Theorem, we can show
that the queries will converge to the real average as we increase the number
of queries. Roughly speaking, the convergence point is when the estimated rate
becomes and remains very close to the overall average rate that we are trying to
estimate.
Figure 1(b) shows the average rate over time for each of the ﬁve OD pairs. The
measurement is performed at the last switch in each path with a querying interval
of only ﬁve seconds. We can see that in all cases convergence point, marked by
vertical arrows in the graph, is within 50 seconds, or just 10 queries. Note that
OD pairs H1-H10 and H2-H9 receive the least rate as they are traversing the
longest path in the topology through three loss emulators hence experiencing
5 This assumption does not break the generality of our results. We make the stationar-
ity assumption in order to have a well-deﬁned notion of convergence time. However,
as long as the changes in system are slower than our convergence rate, OpenTM
gives a close estimate of the TM.
OpenTM: Traﬃc Matrix Estimator for OpenFlow Networks
207
88
87.8
87.6
87.4
87.2
87
86.8
86.6
86.4
86.2
)
s
p
b
M
(
e
t
a
r
86
2300
Last Switch
Uniform Random
Non-uniform Random
Round Robin
Least Loaded
2320
2340
time(s)
2360
2380
2400
1
0.9
0.8
0.7
0.6
F
D
C
0.5
0.4
0.3
0.2
0.1
0
-1.5
-1
-0.5
Last Switch
Uniform Random
Non-uniform Random
Round Robin
Least Loaded
1
1.5
2
0
0.5
Difference in rate(Mbps)
(a)
(b)
Fig. 2. (a) Comparing the querying strategies methods for traﬃc between H1 and H10
with a querying interval of 5 seconds. (b) CDF of diﬀerence between querying strategies
and the last switch’s traﬃc.
3% drop rate. On the other hand, H3-H4, H5-H6, and H7-H8 only experience
1% drop rate and thus have higher rates.
In the next set of experiments, we compare diﬀerent querying strategies pro-
posed in Section 2. Figure 2(a) shows the measured average throughput versus
time for traﬃc between H1 and H10 when each of the querying strategies are
used and the querying interval is ﬁve seconds. To make it more visible, the plot
is zoomed in and only shows 100 seconds of the measurement. From the ﬁgure,
it appears that the least loaded method is almost always reporting a rate higher
than the last switch method and having the largest estimation error. This is
because in our setup, the least loaded switches are mostly the ﬁrst switch in a
ﬂow’s path which is before all three drop emulators and hence this method suf-
fers from the most inaccuracy. This is not necessarily the case in other network
topologies and it is dependent on the traﬃc load over the switches.
To better illustrate the diﬀerence between querying strategies, Figure 2(b)
shows the CDF of diﬀerences between each querying strategy and the last
switch’s rate for traﬃc between H1 and H10 when each of the querying strate-
gies are used and the querying interval is ﬁve seconds. The ideal case is to
always measure the last switch’s rate and hence having zero diﬀerence and it
is shown by a vertical line at zero in the graph. As expected from the analysis
presented in Section 5, the ﬁgure shows that the non-uniform random querying
method has the best performance, as it tends to query switches closer to the
destination with higher probability. Both the round-robin and uniform random
querying methods are performing very close to each other, and worse than the
non-uniform querying method. As mentioned above, the least loaded method is
performing the worse in this case, since in our setup the ﬁrst switch on the path
is almost always the least loaded switch.
208
A. Tootoonchian, M. Ghobadi, and Y. Ganjali
Finally, we note that the overall diﬀerence between all these schemes is rela-
tively small. In fact, the maximum diﬀerence between the best and worst query-
ing schemes is about 2 Mbps, which is about 2.3% of the actual rate (86 Mbps)
between H1 and H10. This observation suggests that when we do not need ex-
tremely accurate TM, any of these querying schemes can be used. Clearly, in
this case the least loaded scheme might be the preferred scheme as it minimizes
the maximum load among switches. For higher-accuracy requirements, however,
one might want to use schemes which favor the last few switches on the path.
5 Analysis
In this section we analytically compare the querying strategies proposed in Sec-
tion 2 in terms of their accuracy in estimating the ﬂow rates between source and
destination. Intuitively, as long as there are no packet drops in the network, all
measurements from switches should be the same6, and thus all querying strate-
gies should be very close to each other; our experiments conﬁrm this. However,
when there are packet drops in the system, we expect to see diﬀerences in the
various querying schemes proposed before.
Let us consider a topology similar to Figure 1(a). We are interested in ﬁnding
the expected value of rate of a given ﬂow f. We denote the link between switches
Si and Si+1 by ei and the measured rate corresponding to f over ei by ri. If
ei has a drop rate d, then the rate measured at ei+1 will be ≤ ri × (1 − d).
Assuming there are M uniform randomly distributed congestion points in the
network, each with a drop rate of d, we can ﬁnd the expected rate for each
querying strategy as follows. Note that here for simplicity we assume that all
links have equal drop probability of d.
Querying the last switch before the destination. We deﬁne the rate be-
tween an OD pair as the rate seen by the destination. Assuming negligible packet
drops on the link connecting the last switch to the destination node, querying
the last switch must give us the rate as seen by the destination regardless of net-
work conditions. We use this rate as the baseline for comparing with randomized
querying techniques presented below.
Uniform random querying. We ﬁrst consider the simple case where there is
only one congested link in the network and call the measured rate by this method
at a time slot i as Rr(i) and the rate at the last switch by Rt(i). There are two
possible cases: (1) if the randomly selected switch is between the congested link
and the last switch before the destination, then rate scene by the selected switch
is same as the rate at the last switch; Rr(i) = Rt(i) (2) if the selected switch
is between the source and the congested link, then rate at the selected switch is
higher than the rate at the last hop switch before the destination. In particular,
Rr(i) = Rt(i)
1−d . Assuming that the congested link
is placed uniformly random over the path then each of the above cases has an
1−d . Hence, Rt(i) ≤ Rr(i) ≤ Rt(i)
6 We ignore the diﬀerence caused by the delay between switches.
OpenTM: Traﬃc Matrix Estimator for OpenFlow Networks
209
(cid:2)N
equal probability of one half. If we take the average rate over N queries, the
expected rate Rr =
Rr(i)/N, will lie exactly in between the two cases;
i.e., Rr = 0.5 × (Rt + Rt/(1 − d)).
Similarly, if there are M congestion points in the network then we have
Rt(i) ≤ Rr(i) ≤ Rt(i)/(1− d)M and if we assume that the congestion points are
distributed uniformly over the path, then the probability of Rr(i) = Rt(i)
(1−d)m is
M+1, where 0 ≤ m ≤ M is the number of congestion points that the ﬂow has
traversed before reaching the querying switch. Hence,
i=1
1
Rur =
Rt
M + 1
M(cid:3)
m=0
1
(1 − d)m
=
Rt
M + 1
× 1 − (1 − d)M +1
d(1 − d)M
(1)
Non-uniform random querying. In this method, we generate two random
numbers i and j, 1 ≤ i, j ≤ N, where N is the number of switches in a ﬂow’s
path and query the switch with ID equal to max(i, j), assuming the switch with
larger ID is the one closer to the destination. With same assumptions as the
above and in the case that there are M congestion points in the network we
have (M + 1)2 cases and if we take the average over N queries for large N, the
expected average rate will be:
Rnr =
Rt
(M + 1)2
(cid:4)
1 + 2
M−1(cid:3)
m=0
M − m
(1 − d)m
(cid:5)
+
1
(1 − d)M
(2)
Round-Robin querying. The expected value of average rate for the round-
robin querying method, Rrr, is similar to the uniform random method since on
M+1 of queries will have rate Rt(i)
average
1
1−d
and so on.
M+1 of queries will have rate Rt(i),
1
Least-loaded switch querying. The performance of least-loaded switch query-
ing highly depends on packet processing power of network switches, as well as
how network load is distributed amongst them. If switches have equal processing
power and load this scheme will perform very similar to uniform random query-
ing. However, in the worst case, the least loaded switch might be the ﬁrst switch
on the path in which case it will lead to the worst case estimation of the rate.
6 Conclusion
This paper presents OpenTM, a traﬃc matrix estimator for OpenFlow net-
works. OpenTM derives the TM of an OpenFlow network in real-time with high
accuracy using direct measurements without packet sampling. OpenTM evenly
distributes the statistic queries among all the switches in the network and thus
imposes the least overhead on the network. Our evaluation in a testbed using
OpenTM implemented as a NOX application shows that OpenTM derives an
accurate TM within 10 switch querying intervals, which is extremely faster than
existing TM estimation techniques. Despite the limitations of our evaluation
and the need for more comprehensive evaluation, we believe OpenTM can be
deployed in OpenFlow networks with a very negligible overhead.
210
A. Tootoonchian, M. Ghobadi, and Y. Ganjali
Acknowledgments
This work was partly supported by Cisco Systems and a grant from NSERC.
NEC Corporation kindly provided us with the OpenFlow switches. We would
also like to thank Bianca Schroeder and the anonymous reviewers for the their
valuable feedback.
References
1. Zhao, Q., Ge, Z., Wang, J., Xu, J.: Robust traﬃc matrix estimation with imper-
fect information: Making use of multiple data sources. SIGMETRICS Performance
Evaluation Review 34(1), 133–144 (2006)
2. Vardi, Y.: Network tomography: Estimating source-destination traﬃc intensities
from link data. Journal of the American Statistical Association 91(433), 365–377
(1996)
3. Nucci, A., Diot, C.: Design of IGP link weight changes for estimation of traﬃc
matrices. In: Proceedings of the 2004 Conference on Computer Communications
(2004)
4. Feldmann, A., Greenberg, A., Lund, C., Reingold, N., Rexford, J., True, F.: De-
riving traﬃc demands for operational IP networks: Methodology and experience.
IEEE/ACM Transactions on Networking 9(3), 265–280 (2001)
5. Papagiannaki, K., Taft, N., Lakhina, A.: A distributed approach to measure IP
traﬃc matrices. In: Proceedings of the 4th ACM SIGCOMM Conference on Inter-
net Measurement (2004)
6. Medina, A., Taft, N., Salamatian, K., Bhattacharyya, S., Diot, C.: Traﬃc matrix
estimation: Existing techniques and new directions. SIGCOMM Computer Com-
munication Review 32(4), 161–174 (2002)
7. McKeown, N., Anderson, T., Balakrishnan, H., Parulkar, G., Peterson, L., Rexford,
J., Shenker, S., Turner, J.: OpenFlow: Enabling innovation in campus networks.
SIGCOMM Computer Communication Review 38(2), 69–74 (2008)
8. Gude, N., Koponen, T., Pettit, J., Pfaﬀ, B., Casado, M., McKeown, N., Shenker,
S.: NOX: Towards an operating system for networks. SIGCOMM Computer Com-
munication Review 38(3), 105–110 (2008)
9. Medina, A., Fraleigh, C., Taft, N., Bhattacharrya, S., Diot, C.: A taxonomy of IP
traﬃc matrices. In: SPIE ITCOM: Scalability and Traﬃc Control in IP Networks
II, Boston (August 2002)
10. Pang, R., Allman, M., Bennett, M., Lee, J., Paxson, V., Tierney, B.: A ﬁrst look at
modern enterprise traﬃc. In: Proceedings of the 5th ACM SIGCOMM Conference
on Internet Measurement, Berkeley, CA, USA, p. 2 (2005)
11. Naous, J., Erickson, D., Covington, G.A., Appenzeller, G., McKeown, N.: Im-
plementing an OpenFlow switch on the NetFPGA platform. In: Franklin, M.A.,
Panda, D.K., Stiliadis, D. (eds.) ANCS, pp. 1–9. ACM, New York (2008)
12. Hemminger, S.: Network emulation with NetEm. In: Linux Conference, Australia
(April 2005)