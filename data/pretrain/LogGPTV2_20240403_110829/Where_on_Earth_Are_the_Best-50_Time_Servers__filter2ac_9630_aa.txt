title:Where on Earth Are the Best-50 Time Servers?
author:Yi Cao and
Darryl Veitch
Where on Earth Are the Best-50
Time Servers?
Yi Cao(B) and Darryl Veitch
School of Electrical and Data Engineering, University of Technology Sydney,
{Yi.Cao,Darryl.Veitch}@uts.edu.au
Sydney, Australia
Abstract. We present a list of the Best-50 public IPv4 time servers by
mining a high-resolution dataset of Stratum-1 servers for Availability,
Stratum Constancy, Leap Performance, and Clock Error, broken down
by continent. We ﬁnd that a server with ideal leap performance, high
availability, and low stratum variation is often clock error-free, but this
is no guarantee. We discuss the relevance and lifetime of our ﬁndings,
the scalability of our approach, and implications for load balancing and
server ranking.
Keywords: Leap second · NTP · Stratum-1 server ·
Network measurement · LI bits · UTC · Load balancing ·
Clock synchronization
1 Introduction
A high proportion of the global computer population achieves its time synchro-
nization via public time servers accessed by the NTP protocol. Such servers are
hierarchical in that a Stratum-s (or S-s) timeserver itself synchronizes to a Stra-
tum s − 1 server. Anchoring the system are the Stratum-1 time servers, which
have local access to reference hardware.
Clients rely on their server’s notion of time, however, as we describe below,
server quality varies in important ways, often with no warning being delivered to
clients. It would clearly be of interest to map out server quality across the Inter-
net, both for its own sake, and also to inform client server selection. However, it
is not immediately clear how this could be achieved at scale, and reliably, across
the latency noise of the Internet.
Recently the problem of server health monitoring has begun to receive atten-
tion, in particular regarding the small but critical Stratum-1 class. Techniques,
described in [5,18], have been developed for the unambiguous detection of errors
in server clock timestamps, even from vantage points where the path to the server
is both long in terms of Round Trip Time (RTT), and noisy. In [18], studying
around 100 servers, it was found that signiﬁcant errors are not rare, being found
in a surprisingly high proportion of popular public servers, including many from
National Laboratories. Errors can be both large in magnitude (10’s to 100’s of
c(cid:2) Springer Nature Switzerland AG 2019
D. Choﬀnes and M. Barcellos (Eds.): PAM 2019, LNCS 11419, pp. 101–115, 2019.
https://doi.org/10.1007/978-3-030-15986-3_7
102
Y. Cao and D. Veitch
milliseconds and even beyond) and long lasting (from hours to days and even
continuously over months), or both. In [17] a similar server set was analyzed with
respect to their leap second performance, and recently [5], using a new and much
larger data set, looked at both server clock error and protocol failures during the
end-2016 leap second. In these servers, which include all those Stratum-1 servers
employed in the widely used NTP Pool service [11], only 37.3% were found to
perform adequately.
In this paper we mine the IPv4 data set, available at [4], used in [5]. We
evaluate quality according to four dimensions: server Availability, behaviour sur-
rounding a Leap Second (a stress test for both NTP protocol compliance and
clock behaviour), Stratum Constancy, and ﬁnally, severity of server Clock Errors.
We limit our list to 50 members, and within this group servers are not explicitly
ranked. Instead, because of the importance to clients of the RTT to its server,
a key factor in synchronization performance in practice (though not necessarily
in theory, see [16]) due to its correlation with path asymmetry, congestion and
loss, we structure our results in a per-continent then per-country breakdown.
There are a number of arguments for a ‘Best-50’. One is for direct use by
measurement specialists, in particular operators of measurement infrastructures
[1,2,14], who require servers of both high availability and high accuracy. Another
is to highlight the server health issue. Quantifying best practice increases aware-
ness of ongoing problems, and provides the context (and an incentive) for eﬀorts
to improve the system and to track performance over time. A third goal is to
explore concretely a number of quality metrics, and how they relate to actual,
veriﬁable errors in server timing. Although there have been some papers survey-
ing network timing performance [6–10], we believe this is the ﬁrst attempt to
accurately identify the best servers, using diverse metrics.
After providing background in Sect. 2 and an overview in Sect. 3, the main
results are presented in Sect. 4. Section 5 discusses their signiﬁcance, limitations,
and implications for the deﬁnition and use of a server quality rank, with reference
to load balancing services including NTP Pool. We conclude in Sect. 6.
2 Background
We summarize the experimental setup, data set and server list (see [5] for full
details). We then summarize the operation of the NTP Pool service.
2.1 The Experiment
The experiment covered a 64 day period from Nov. 16 2016 to Feb. 2 2017, includ-
ing the end-2016 leap second. For each server in a target server list in parallel, an
independent instance of a request–response exchange daemon, using a per-server
customized polling period as close to τ = 1 s as possible, was launched.
For an NTP packet i which successfully completes its round-trip from the
client to server and back, a 4-tuple stamp {Ta,i, Tb,i, Te,i, Tf,i} of timestamps
is recorded. Here Tb,i, Te,i are the (incoming and outgoing respectively) UTC
Where on Earth Are the Best-50 Time Servers?
103
timestamps made by the server. These are extracted from the returning NTP
packet header, along with the Leap Indicator (LI) bits and the server Stratum
ﬁeld. The timestamps Ta,i, Tf,i are of passively tapped NTP packets, hardware
timestamped using high performance Endace DAG 7.5G4 capture cards, whose
hardware clocks are disciplined to a rubidium atomic clock, itself locked to a roof
mounted GPS receiver. The error in the client side timestamps measurement is
therefore sub-microsecond and is ignored here.
The IPv4 servers studied came from ﬁve sources:
Org:
Pool: S-1 servers participating in the NTP Pool Project
LBL: S-1 servers caught at the Lawrence Berkeley Laboratory border router
Au:
Misc: miscellaneous servers of interest.
The servers which returned useful data, 459 in total, are broken down by source
in Table 1 (the sets overlap). Of the AU servers, 6 are in fact private and will
be excluded from the ﬁnal results. Table 2 provides a geographical breakdown.
The low values for AF, AN and SA reﬂect the immaturity of Internet timing
infrastructure across these continents.
the set of Australian public facing S-1 servers (plus 6 private)
the public S-1 URL list maintained at ntp.org
Table 1. Server source breakdown.
Table 2. Continental breakdown of servers.
Population Org Pool LBL Au Misc
Population AF AN AS EU NA OC SA
#
%
197 258 257 14 10
43
56
56
3
2
#
%
1
0
0.2 0
50 203 169 29 7
0.9 44.2 36.8 6.3 1.5
2.2 NTP Pool
The NTP Pool Project [11] provides a load balancing and convenient conﬁgu-
ration service for millions of NTP clients, by supplying a set of URLs resolved
via a tailored DNS server, to members of a pool of participating volunteer NTP
servers of various strata.
Users can access at pool.ntp.org the complete worldwide pool, or subsets
thereof at #.pool.ntp.org, where # is one of {0,1,2,3}. These subsets are inﬂu-
enced by client geo-location but otherwise random, and refresh every hour [12].
The full details of how server subsets are selected is not documented.
A degree of client-control is supported via CONT.pool.ntp.org: continental
zone pools where CONT is one of {africa, antarctica, asia, europe, north-america,
oceania, south-america}, and CY-coded country pools at CY.pool.ntp.org, and
#. preﬁxed subsets of these [13].
For the pool associated to a given client at a particular time, the system uses
DNS round robin to resolve URL queries to the IP address of a server in that
pool. NTP Pool includes a monitoring system which queries the pool servers,
scoring their performance based in NTP packet ﬁelds including {oﬀset, stratum,
LI, RTT, noresponse}. Servers are evaluated periodically and only those with a
score above 10 are made available.
104
Y. Cao and D. Veitch
3 Server Characterization
We characterize servers according to the following four criteria or dimensions.
Availability. This simple but critical criterion is measured by the ratio of
response packets received to request packets sent. This will underestimate the
true availability, because of packet loss and reachability failure in the network.
Stratum Constancy. Possible stratum values range from S = 0 (unsynchro-
nized), to S = 1, 2 . . . 16. A Stratum-1 server may change stratum if its hardware
reference has a problem, if the system has a reboot, or if its synchronization
daemon/algorithm decides it would prefer an remote reference, and stratum
values of 0, 2, 3 or even higher could result. We measure the ‘Stratum-1 down-
time’ (S1Downtime) as the proportion of response packets which report a stra-
tum other than 1. Values of S1Downtime close to zero suggest a well managed
Stratum-1 server in a stable environment. We also record the list of all stratum
values ever seen.
Leap Performance. Leap Second events are a stress test for servers, both in
terms of the detailed clock performance (does it jump cleanly by exactly 1 second
at exactly the right time, and nothing else?) and protocol compliance (does it
set the LI bits in accordance with the standard?). This question was studied in
detail for each server in the list in [5]. Here we classify servers according to a
subset of the characterization deﬁned there, as:
Ideal: no observed clock error linked to the leap second, ideal protocol behaviour;
Adequate: no clock error, compliant protocol behaviour;
Clock Good: no evidence of clock error about the leap,
where Ideal⊂ Adequate⊂ Clock Good⊂ All. For convenience, we add two more
classes by set diﬀerence:
Clock Good Only (CGO): Clock-Good\Adequate;
Clock Not Good (CNG): All\Clock-Good.
Although leap seconds are rare, they occur regularly. If a server handles them
poorly, the impact can be severe, for example taking weeks to jump, or never.
Clock Errors/Anomalies. Our approach is based on the methodology we
pioneered in [18] for the remote detection and measurement of server errors. It
uses baseline analysis of the RTT timeseries to identify changes in the ‘Error’
]
s
m
[
E
40
20
0
-20
i
30.5278
30.569
t [day]
30.6101
30.6513
Fig. 1. Server errors cause E(i) to deviate from its true underlying value (green line).
(Color ﬁgure online)
Where on Earth Are the Best-50 Time Servers?
105
All
AF
AS
EU
NA
OC
SA
96
97
98
99
100
20
40
60
80
100
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
95
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
Fig. 2. CDF of Availability (in %) over all servers (black), and per-continent.
i −D↓
time series Ei = (D↑
i )/2 due to server errors, rather than the alternatives of
i = Tf,i−Te,i
path routing changes and/or congestion. Here D↑
are the empirical outgoing and incoming delays to the server. An example of a
server error zone, beginning at around t = 30.544 days, is given in Fig. 1.
i = Tb,i−Ta,i and D↓
We have improved the methodology of [18] by (i) replacing non-linear ﬁltering
based congestion suppression (which can be fooled in certain circumstances) with
strict RTT bounding, (ii) systematically recording not only error sizes but also
the precise locations of all error zones, (iii) increasing the granularity of error
frequency reporting: we classify servers according to the number of errors as:
Good: no errors; Rare: less than one error per week; Common: more than one
error per week, but not High; and High: continuous stretches of error covering
at least 25% of the trace. In [18] R and C were combined into R.
Since the selection of error zones is performed manually (due to the need to
disambiguate from complex routing, congestion and error scenarios), the detec-
tion process is very labor intensive. It is essential however for our purposes here
where, unlike [18], we evaluate not only error presence and representative size
but also how often the server is in error (see Errtime below).
3.1 Server Overview
We provide some context by examining the ﬁrst three of the above dimensions
over all servers.
Figure 2 shows the Cumulative Distribution Function (CDF) of availability
for all servers. Availability is good overall, with 80% of servers having values
exceeding 95%, and over half exceeding 99%. The per-continent results show
lower availability for regions further from the testbed in Sydney, Oceania. This
can be explained through a measurement bias due to higher loss rates over longer
paths leading to lower apparent availability.
106
Y. Cao and D. Veitch
Fig. 3. Relationship between the Stratum classes. Symbols denote servers in the Best-
50, red symbols denote those with server errors. (Color ﬁgure online)
The leap performance results over all servers appear in Table 3. Only 37%
exhibit Adequate behavior, necessary to allow their clients to navigate a leap
second without incident.
Table 3. Leap performance summary.
All CGO CNG Clock Good Adequate Ideal
# 459 134
% 100
29
154
34
305
66
171
37
36
8
Figure 3 provides a pertinent classiﬁcation of servers according to strata. In
the Constant class only one stratum value is ever seen (not always Stratum-1!),
in Bi only two, and in Unsync at least one response carries Stratum-0. We see
that 154 servers (34%) have constant strata, and the majority of the 305 that
do not, 254 or 83%, announced themselves as unsynchronized at least once.
Overall 137 servers (30%) announce themselves as Stratum-1 in each and
every response. This appears as a discrete mass of weight 0.3 at the origin in the
S1Downtime CDF in Fig. 4, which shows that servers which are not Constant
have a wide variety of S1Downtime values.
4 The Best-50 Servers
What we would ideally like is clear: to ﬁnd servers that are always available, and
that have no detectable clock errors. However, to determine the latter implies a
prior detailed examination, which is too labour intensive using our server error
methodology and tools to deal with 459 servers, each with up to 2 months of
high resolution data, each with potentially a large number of errors.
Where on Earth Are the Best-50 Time Servers?
107
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
All
AF
AS
EU
NA
OC
SA
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.01
0.02
0.03
0.04
0.05
20
40
60
80
100
Fig. 4. CDF of S1Downtime (in %) over all servers (black), and per-continent.
Accordingly, our approach is to ﬁrst assemble a list of ostensibly high qual-
ity servers using the dimensions of Availability and Stratum Constancy that
are readily calculated, and Leap Performance, available from prior work, and to
apply the Clock Error analysis on this much smaller number of servers, which
moreover are likely to be simpler to analyse. In this way we approximate the
ideal above in a scalable way (see Sect. 5), with a practically appropriate bias
toward servers with stable management (high Stratum Constancy) and compe-
tent conﬁguration and performance during high stress (Leap Performance).
More precisely we proceed as follows. For Availability, we seek servers that are
almost always available, with due allowance for measurement bias due to packet
loss. Based on Fig. 2 we believe a cutoﬀ of 97% is safe. For Leap Performance, we
insist that servers are in the Adequate class. Next, we use S1Downtime to order
the servers that pass the above two criteria. Our Best-50 servers are then deﬁned
as the ﬁrst 50 servers in this ordering (starting from the zero S1Downtime end)
whose Clock Error class is either G or R.
Server errors in a given server are further quantiﬁed through the metrics of
Size (the median over all error zones of the error range over that zone), and
Errtime (the proportion of the trace taken up by error zones).
The resulting Best-50 servers are given in Table 4. Within each continent
group, servers are ordered according to country code ﬁrst, and then lexicograph-
ically according to their URL. The mapping from URL to IP address is provided
in the Appendix.
Beyond the identities of the servers themselves and their geographical break-
down, the most important observation from the table is the fact that even excel-
lent performance under each of Availability, Stratum Constancy and Leap Per-
formance does not mean that the server is error free. Indeed, out of 15 servers
with detected server errors, 9 give no warning of this with a S1Downtime of
zero, yet have Sizes ranging from 2.1 to 1000 ms, albeit with Errtime being gen-
erally low (0.9 s in the hour on average in the worst case of 0.025%). The worst
108
Y. Cao and D. Veitch
Table 4. Best-50 public timeservers organised by continent, country, and URL. Cyan
URLs marks National Laboratory servers.
CONT
AF
AN
OC
EU
NA
AS
SA
0
0
0
0
0