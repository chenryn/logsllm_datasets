is access to data stored on passcode-locked devices. However, two
events led to a major conflict with the FBI over this issue.
some VoIP services, and there have been several (unsuccessful) attempts to amend the
law to include access to Internet services as well.
8For example, in opposing one (ultimately successful) CALEA rulemaking effort in
2004, the Electronic Frontier Foundation (EFF) argued in terms that clearly echo today:
“Building in ’back doors’ for law enforcement is likely to increase the market demand
for foreign competitor offerings to U.S. software and hardware products. If the FCC
enacts the tentative rules, it will impair innovation and drive Internet development
offshore. Another possible problem would be U.S.-based criminals or terrorists import-
ing gray-market equipment that is ’CALEA-free.’ CALEA-driven mandates will cause
technologies to be developed overseas to circumvent U.S. surveillance capability” [34].
9These concerns were particularly exacerbated by evidence that the NSA may have
manipulated at least one key NIST cryptographic standard to support their collection
interests [40].
The first was a 2015 case in which a New York magistrate judge
refused to issue a government-requested order requiring Apple
to assist in unlocking a seized iPhone, arguing that existing legal
process was insufficient to demand Apple’s compliance. Although
the finding was not precedential, Apple largely stopped complying
with such requests at this point.10
The second issue was technical. Starting with iOS8, Apple con-
figured its phones to encrypt most data by default and to make
decrypting this data rely, in part, on a key derived from the pass-
code. As a consequence, Apple was no longer technically capable
of directly extracting content from passcode-locked phones. Thus,
in addition to the policy change driven by the New York ruling,
Apple’s change in technology further solidified their position.
This conflict came to a head in December of 2015, when the
FBI recovered a phone used by one of the shooters in the widely-
publicized San Bernardino mass shooting. The device, which ran
iOS9, was encrypted, passcode-locked and configured to erase its
contents after 10 invalid login attempts. The Department of Justice
obtained a court order directing Apple to create modified firmware
that would bypass the 10-trials restriction (as well as an inter-
trial delay feature) such that the FBI could perform a brute force
passcode-guessing attack to unlock the phone. More important than
the details of the firmware itself, the government needed Apple to
digitally sign this update so it would be accepted by the device.11
Apple objected on a range of legal grounds, including claims that the
order exceeded statutory authority and a First Amendment theory
that demanding the creation of new code constituted compelled
speech [4].
Before the case could be decided, the FBI obtained an indepen-
dent means of opening the phone (a zero-day vulnerability in iOS
which, by some estimates, may have cost as much as $1M [57]).
The Department of Justice then withdrew its request. Today, it is
unclear if the vulnerability used by the FBI to unlock this phone is
still present in modern iPhones or not. However, in the time since,
there are at least two public vendors who sell tools or services
capable of opening the latest iPhones: mobile forensic software
provider Cellebrite, which offers to unlock iPhones via a service,
and Greyshift, which offers a product (GreyKey) that exploits an iOS
vulnerability to allow brute force password guessing [10, 51]. As
mentioned earlier, today such vulnerability-based systems, funded
by public dollars, represent a central element of the existing status
quo for lawful access.
2.2 Research
Developing and understanding the technical tradeoffs between dif-
ferent approaches to the lawful access question requires significant
expertise; it is precisely the complexity of the issue that makes the
research community a key party to any debate.
In the mid-1990’s in the heat of the conflict around the Clipper
proposal, the cryptography community generated a broad range of
10Today, Apple will still provide access to the data stored on locked phones for which
it is technically capable of doing so (iOS versions before 8.0) but only recognizes legal
process for this service under California’s CalECPA statute [17].
11In the rough design submitted to the court, the signature would have covered code
that validated phone-unique identifiers (e.g., the IMEI) and hence the particular piece
of signed binary code would not have been effective when used against any other
phone [6].
counter-proposals offering technical tradeoffs seeking to provide,
as Bellare and Goldwasser put it, “security against massive wiretap-
ping” [24]. For example, in 1995, Shamir introduced the notion of
“partial key escrow” whereby a portion of a secret key is escrowed
with a trusted party and the remainder must be computed—thereby
introducing an implicit time/cost element and thus potentially lim-
iting abuses.12 Bellare and Goldwasser extended this idea with a
protocol that verifies that the necessary computation is contem-
poraneous with the recovery [24] (i.e., that this work cannot be
front loaded). The same authors also proposed “encapsulated key
escrow”, which imposes a time delay on key recovery [23] (again via
a time hardness argument). Blaze further explored a design called
“oblivious key escrow” where, rather than escrowing keys with a
single party, keys are escrowed across thousands of parties using
some variant of Shamir’s secret sharing [28]. These are simply a
few examples from a broader literature, but virtually all are crypto-
centric (i.e., focused on supporting lawful access via changes in
the design of the underlying cryptosystem—typically via escrow or
“weak” key constructions).
With the failure of the Clipper proposal, this topic became unpop-
ular as a research focus. However, the recent revitalization of policy
interest does not appear to have revived technical interest from
the research community. As Boyd et al. write, “there has been little
interest from the cryptographic community to explore compromise
solutions” [29].13
Of the exceptions, most current research efforts represent a con-
tinuation of the mid-90’s cryptographic focus. Wright and Varia’s
“Crypto Crumple Zones” work is an intellectual descendent of par-
tial key escrow, whereby a cryptosystem is tuned such that mes-
sages can be decoded with significant cost [64] (one that is deemed
to be sufficiently high that even governments would be dissuaded
from using it for all but the most important cases). Boyd et al. have
developed new approaches to partial and oblivious key escrow in
a blockchain context such that key recovery not only incurs sig-
nificant costs, but is also inherently public [29]. In a mix-context,
Chaum’s PrivaTegrity chat system is designed to support anony-
mous chat applications that, as a policy option, allow collections
of key mix node operators to cooperate to trace a particular user
deemed to have violated norms [31, 38].14
Most recently, in May of 2018, a Wired magazine article described
Ray Ozzie’s CLEAR proposal, wherein a device’s self-escrowed stor-
age decryption keys are protected by the device vendor’s public key,
and thus are decryptable by the vendor under court order [42, 48].
Ozzie’s scheme, regardless of any critiques of individual technical
details, highlights and is responsive to where the policy fault lines
exist today: it is device-centric, it moves trust from the public to
the private sector, and it is relatively simple to understand.15
12This invention is cited in [24] as a private communication at the 1995 Crypto Con-
ference and again presented at the Key Escrow Conference in September of 1995 in
Washington D.C.
13In a similar vein, Bart Praneel argued in his invited talk at the 2016 Eurocrypt that
“we don’t actually know if secure-third-party access is possible because it’s a ‘taboo’
research field”.
14Perhaps due to the strong negative reaction to his description of this tracing option,
this section is no longer present in the current version of the paper.
15This work shares a number of features with Ozzie’s proposal, including self-escrow
and vendor involvement in authorization. Some of this is synchronicity (self-escrow),
but some is cross-pollination (vendor involvement), as we discussed the problem in
detail with Ozzie on multiple occasions.
2.3 Policy
While there is a dearth of contemporary technical work focused
on the lawful access question, there has been considerable policy
writing—both from the technical and policy communities.
The best known of these efforts from the technical community
is the 2015 “Keys Under Doormats” paper, that advocates strongly
against design mandates to support law enforcement access to
encrypted data. Absent any proposal to respond against, the Door-
mats paper considers the problem writ large, and its conclusions are
built around three principal claims about lawful access: first, that
it would require an abandonment of best practices in cryptosys-
tems (in particular, forward secrecy and authenticated encryption);
second, that it would substantially increase complexity and thus
create new security vulnerabilities; and third, that it would create
attractive targets where escrow keys were concentrated en masse
and online [20]. While we agree with many of the concerns eluci-
dated by these authors, the paper is frequently taken as a blanket
proscription or demonstration of impossibility, which we believe
is premature, if not overly dour. The reality is that there has been
relatively little contemporary exploration of the technical design
space or the range of tradeoffs that are possible.
In addition, a range of policy papers (and countless blog posts)
have been released over the last twelve months to structure the dis-
cussion around lawful access capabilities and engage in some advo-
cacy around particular concerns. These include the recent National
Academies’ “Decrypting the Encryption Debate” study[12], Riana
Pfefferkorn’s “Risks of Responsible Encryption” whitepaper[50],
Access Now’s “Encryption in the US” report[13], the EastWest Insti-
tute’s “Encryption Policy in Democratic Regimes” report[14], and
R-Street’s “Policy Approaches to the Encryption Debate” paper[19].
The most clear common thread across virtually every discussion
on this topic is a concern that any lawful access mechanism would
create security vulnerabilities that could ultimately be exploited by
unauthorized actors. This sentiment is particularly driven by con-
cerns about the weakening of core cryptographic algorithms; the
argument being that it isn’t possible to weaken the algorithms for
just “one side” (i.e., NOBUS; “NObody But US”). Indeed, the remark-
able story of the Juniper NetScreen crypto circumvention makes
concrete the abstract concern that even idealized covert manipu-
lations of cryptographic protocols to provide arbitrary “protected
access” can create real risks of third-party abuse [32].
The second major thread that appears across recent publications
is a concern about surveillance risk, frequently motivated within
the framework of human rights concerns. Even among those with a
willingness to support the law enforcement position that encrypted
data presents a critical investigatory burden, there is concern about
the potential for overreach and that a capability intended to support
criminal investigations not inadvertently provide a mechanism used
for large-scale surveillance.
It is these two concerns that have primarily motivated the trade-
offs in this paper and it is these two issues to which we have tried
to be most responsive. In particular, they have driven our focus on
system-level (as opposed to cryptographic) approaches, prioritizing
resistance to mass-surveillance use, avoiding the use of master (i.e.,
multi-device) secrets, and avoiding creating new trust relationships
with government agencies.
3 ASSUMPTIONS
Any system’s security rests on a series of assumptions: assumptions
about the problem, assumptions about the solution and assumptions
about the threat. Here we attempt to make explicit what assump-
tions we are operating under, including those concerning the nature
of the problem, the design criteria we are seeking to meet and the
threat model we assume.
3.1 Problem motivation
We start by declaring a set of three high-level assumptions upon
which our work is motivated. We make these points explicit because
we wish to separate debates about the technical approach embodied
in our work and the claims we make about it, from related but
distinct debates about political or social policy.
• There are reasonable societal interests in allowing law enforcement
to access encrypted data in particular cases.
In general, most people understand that law enforcement provides
an important societal function and that, in service to their investiga-
tions, they may be authorized to search and seize personal property
(e.g., via a court-ordered warrant predicated upon a finding of
probable cause). The premise behind providing these extraordinary
capabilities is that reasonable societal interests in enforcing the
rule of law can, at times, outweigh individual rights to liberty and
privacy. This view is not universally held. For example, those with
strong anarchistic or libertarian political beliefs may reject this no-
tion entirely. Absent agreement on this question, it is definitionally
impossible to compromise around the topic of lawful access.
• There are reasonable concerns about the potential to abuse any
system that would allow access to encrypted data.
Even if one holds that there are situations wherein the state should
be able to access encrypted personal data, this does not mean such
a capability should be absolute or unhindered. While requirements
for judicial authorization do provide one bulwark against abuse, if
the underlying mechanism for lawful access is technical in nature
there is no guarantee that those legal processes will be followed.
Moreover, outside the domain of law enforcement, the intelligence
community collects data at a different scale, using different bodies
of law and legal interpretation. A reasonable person who supports
lawful access for the purposes of law enforcement, might yet be
concerned about any means of technical access being used at scale
for such intelligence collection purposes (e.g., mass surveillance).
Finally, the ability of technical mechanisms to discriminate among
users (i.e., and allow government access, but not criminal access) is
based on assumptions that the mechanisms are implemented cor-
rectly and critical secrets are kept secure. Thus, if the mechanisms
and their constituent secrets become known outside the govern-
ment, or if they have flaws that are discovered by others, then
non-governmental actors might make use of the same technology
for criminal purposes. Some disagree with this concern, and believe
that it is possible to maintain mechanisms that would only be avail-
able to the state and/or that concerns about mass surveillance uses
are unwarranted or unreasonable.
• Lack of perfection is acceptable.
It is the essence of compromise that both “sides” in a debate may
need to sacrifice some of their goals. In this situation, it means
that some interests of law enforcement will not be met (e.g., this
proposal contains no way to gain expedient access to an encrypted
device in an emergency) as well as some interests of civil liber-
tarians (e.g., this proposal does not defend an individual against a
breakdown in the rule of law or pervasive compromise of device
vendors, although it does offer collective protection against mass
surveillance). This paper operates under the assumption that such
tradeoffs are inherent and acceptable so long as they do not, on
balance, undermine the overall pragmatic benefits of the resulting
capability. However, those with absolutist views (on either side)
may reject any solution that provides less than complete privacy
or complete access in all cases.
3.2 Design goals
Given these overall assumptions, we have chosen to focus exclu-
sively on access to encrypted data on personal devices (e.g., phones,
tablets, laptops) using standard features as provided by the operat-
ing system. Thus, we restrict ourselves to the problem of “device
unlocking”—the process by which a device with system-encrypted
storage (i.e., encrypted via a default OS service) is brought into