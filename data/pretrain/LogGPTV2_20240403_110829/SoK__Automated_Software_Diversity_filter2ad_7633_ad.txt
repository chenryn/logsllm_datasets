to detect and recover from disassembly errors.
282
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:27 UTC from IEEE Xplore.  Restrictions apply. 
Compilers support proﬁle guided optimization: It is well
known that many programs spend roughly 90% of their time
executing 10% of the code. Most compilers can instrument
and execute a program to discover the “hot” code paths.
A subsequent compilation pass uses the proﬁle information
to better optimize frequently executed code at the expense
of less frequently executed code [50]. Since diversiﬁcation
tends to make programs run slower, reducing the amount of
diversiﬁcation for hot code fragments signiﬁcantly lowers the
performance overhead [17], [31].
Unfortunately, it is not always possible to customize a
compiler. While two of the major production compilers in
use today—the GNU Compiler Collection and LLVM—have
open source licenses, several proprietary compilers remain in
widespread use. Absent any extension mechanism and vendor
support, proprietary compilers cannot act as diversiﬁcation
engines. While customizing a compiler may be the natural way
to implement diversiﬁcation, two alternatives are also available.
First, source-to-source transformations can be applied prior
to compilation. Bhatkar et al. do so to make programs self-
randomizing at load time [8]. Second, program diversiﬁcation
can happen after compilation in one of two ways. The ﬁrst way
is to instruct the compiler to output assembly code such that
it can be rewritten before it is assembled and linked [40].
The second way is to disassemble and rewrite the object
ﬁles produced by the compiler before or during linking [6].
These, link-time diversiﬁcation techniques have the following
advantages:
Debugging information is available: Software vendors
typically strip binaries of debug information before they are
distributed since debugging information facilitates reverse
engineering. So, in contrast to post-link diversiﬁcation, reliable
static disassembly is feasible.
The approach is compatible with proprietary compilers
and linkers: Diversiﬁcation after compilation (and before
linking) is possible even on platforms where neither the
compiler nor the linker is amenable to customization.
Whole program diversiﬁcation is possible: Compilers
typically process one translation unit, i.e., a source ﬁle and the
headers it includes, at a time. This gives compilers a limited
view of the program, meaning that certain transformations are
not possible. Function reordering, for instance is not practical
before all functions have been compiled, i.e., at link time.
Pre-distribution approaches (that do not produce self-
randomizing binaries) generally share two drawbacks in contrast
to the post-distribution techniques we cover later in this section:
Cost of producing program variants: If programs are
diversiﬁed before they are distributed to end users, software
vendors must purchase the computational resources to generate
a program variant for each user. At ﬁrst, it may seem that if
it takes n minutes to compile a program, generating a unique
variant for x users takes n∗x time which obviously is expensive
for popular and complex software. However, Larsen et al. [40]
show that much of the work to create each variant is repetitive
and can be cached to reduce compilation time by up to 92%.
Increased Distribution Costs: While pre-distribution
methods ensure that clients cannot disable diversiﬁcation, each
client must download a separate program variant. This requires
changes to the current software distribution channels. Rather
than cloning a “golden master” copy, distribution systems must
maintain a sufﬁciently large inventory of program variants such
that downloads start without delay. Not all inventory may be
used before new program versions are released. These changes
will most likely also affect the content distribution networks
used for high volume software.
Note that ahead-of-time compiled languages incur both of
these costs while just-in-time compiled languages, such as Java
and JavaScript, do not (compilation to machine code happens
on the clients).
13) Installation: We now move to approaches where
diversiﬁcation happens during or after program installation
on the host system and before it is loaded by the operating
system.
The need to disassemble stripped binaries is a major
challenge at this stage. As previously mentioned, error-free
disassembly without debugging symbols is not generally
possible. The compiler intersperses code and data, i.e., by
inserting padding between functions and embedding jump tables,
constant pools, and program meta-data directly in the instruction
stream.
Post-installation and load-time diversiﬁcation must disas-
semble program binaries before they run. Typically, a powerful,
recursive disassembler such as Hex-Rays IDA Pro is used
for this process. A recursive disassembler uses a worklist
algorithm to discover code fragments inside a binary. The
worklist is initially populated with the program entry point(s),
additional code fragments are put on the list by discovering
control ﬂow edges by analyzing the calls and branches of each
list item. Unfortunately, the problem of determining whether the
control ﬂow can reach a particular code location is equivalent
to the halting problem and thus undecidable [13, p. 578].
Disassemblers therefore err on the side of not discovering
all code [47] or, alternatively, treat all of the code section as
instructions even though some bytes are not [29], [65].
In-place diversiﬁcation is an install-time-only approach that
sidesteps the problem of undiscovered control ﬂow [47]. Code
sequences reachable from the program entry point are rewritten
with other sequences of equal length. Unreachable bytes are
left unchanged thus ensuring that the topology of the rewritten
binary matches that of its original. In-place rewriting preserves
the addresses of every direct and indirect branch target and
thereby avoids the need for and cost of runtime checks and
dynamic adjustment of branch targets. The approach does have
two downsides, however: (i) undiscovered code is not rewritten
and thus remains available to attackers and (ii) preserving the
topology means that return-into-libc attacks are not thwarted.
Most other post-installation diversiﬁcation approaches are
staged and include actions at multiple steps in the application
life cycle. Typically a program is prepared for randomization
after it has been installed and is randomized as it is loaded.
Instruction location randomization (ILR) rewrites binaries
to use a new program encoding [29]. ILR changes the
assumption that, absent any branches, instructions that are
laid out sequentially are executed in sequence; instructions are
instead relocated to random addresses by disassembling and
rewriting programs as they are installed on a host system. A data
283
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:27 UTC from IEEE Xplore.  Restrictions apply. 
structure, the fallthrough map, contains a set of rewrite rules
that map unrandomized instruction locations to randomized
ones and to map each randomized instruction location to its
successor location. To avoid the need to separate code and data,
rewrite rules are generated for all addresses in a program’s
code section. A process virtual machine, Strata [55], executes
the rewritten programs. At runtime, Strata uses the fallthrough
map to guide instruction fetch and reassemble code fragments
before execution; fragments are cached to reduce the translation
overhead.
Binary stirring [64] is also a hybrid approach that disas-
sembles and rewrites binaries as they are installed such that
they randomize their own code layout at load time. Rather
than using a process virtual machine, randomization is done
via a runtime randomizer that is unloaded from the process
right before the main application runs. This ensures that the
code layout varies from one run to another. Instead of trying to
separate data from code, the text segment of the original binary
is treated as data and code simultaneously by duplicating it into
two memory regions—one which is executable and immutable
and one which is non-executable but mutable. One duplicate
is left unmodiﬁed at its original location and marked non-
executable. The other duplicate is randomized and marked
executable. Reads of the text segment go to the unmodiﬁed
duplicate whereas only code in the randomized duplicate is
ever executed. All possible indirect control ﬂow targets in the
unmodiﬁed duplicate are marked via a special byte. A check
before each indirect branch in the randomized duplicate checks
if the target address is in the unmodiﬁed duplicate and redirects
execution to the corresponding instruction in the randomized
duplicate.
Chew and Song customize each operating system installation
to use randomized system call mappings and randomized library
entry points [12]. Application programs must therefore be
customized to the system environment before they run. Since
the goal of rewriting the binaries is to use the correct system
call identiﬁers and library entry points of a particular system,
the topology of the binaries does not change. Again, this means
that undiscovered control ﬂow is not an issue. However, the
problem of undiscovered code implies that rewriting may fail
to update all system and library calls.
The drawbacks of static binary rewriting can be summarized
as follows:
Overheads of runtime checking: Most static rewriting
solutions include a runtime mechanism to compensate for static
disassembly errors. For instance, undiscovered control ﬂows
to addresses in the original program may be dynamically
redirected to the corresponding locations in the rewritten
program—e.g., by placing trampoline code at the addresses
containing indirect branch targets in the original program. The
compensation code invariably adds an overhead to the rewritten
program, even without diversiﬁcation, because it increases the
working set and instructions executed relative to the original
program. Some static binary rewriters omit these compensation
mechanisms [47], [21]. Such approaches are unsafe; program
semantics may not be preserved due to disassembly errors.
Incompatibility with code signing: Commercial binaries
use digital signatures and modern app stores require them. This
allows the operating system to establish the provenance of the
code and verify its integrity before launching an application.
Binary rewriters that change the on-disk representation of
programs cause these checks to fail.
Heterogeneity of binary program representations:
Program binaries do not solely consist of machine code; they
also contain various forms of meta-data such as relocation
information, dynamic linker structures, exception handling meta-
data, debug information, etc. Static binary rewriters must be
able to parse this meta-data to discover additional control ﬂow.
The format of this meta-data is not only operating system
speciﬁc—it is also speciﬁc to the compiler and linker that
generated the binary. So in contrast to compilers, whose input
languages are mostly platform agnostic, it requires far more
effort to support multiple operating systems and compilers in
a binary rewriter.
All post-distribution approaches, e.g., those that diversify
software on the end user’s system rather than prior to its
distribution, share several key advantages and drawbacks. The
advantages are:
Legacy binaries without source code can be diversiﬁed:
Since these approaches require no code-producer cooperation,
legacy and proprietary software can be diversiﬁed without
access to the source code.
Distribution of a single binary: Post-distribution di-
versiﬁcation remains compatible with the current practice of
distributing identical binaries to all users.
Amortization of diversiﬁcation costs: Unlike pre-
distribution techniques, post-distribution diversiﬁcation spreads
this cost among the entire user base.
The drawbacks are:
No protection against client side attacks: Since post-
distribution diversiﬁcation runs on clients, the process can
be disabled by malware or the end users themselves. If
diversity is used to watermark binaries and raise the cost of
reverse engineering and tampering, it must be applied prior to
distribution.
The diversiﬁcation engine increases the trusted comput-
ing base: Widely distributed software such as the Java Virtual
Machine, Adobe Reader and Flash are valuable to attackers.
Since all systems must host a copy of the diversiﬁcation engine,
it becomes another high visibility target.
No support for operating system diversiﬁcation: In con-
trast, several compile-time diversiﬁcation approaches support
operating system protection [12], [27], [34]. Rewriting kernel
code is not impossible but it is rather involved because kernels
differ from application code in numerous ways. Kernel code is
self-loading and does not adhere to a particular binary format;
Linux images, for instance, consist of a small decompression
stub and a compressed data-stream. The control ﬂow in kernels
is also particularly hard to analyze due to extensive use of hand-
written assembly and indirect function calls for modularity. The
control ﬂows from system calls, exception handlers and interrupt
handlers are implicit and must be discovered by parsing kernel
speciﬁc data structures. Additionally, some code cannot be
altered or moved since it interacts closely with the underlying
hardware.
284
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:27 UTC from IEEE Xplore.  Restrictions apply. 
14)Loading: Load-time diversiﬁcation approaches do not
change the on-disk representation of programs. Rather, they
perform randomization as these are loaded into memory by the
operating system.
Many deferred diversiﬁcation approaches perform random-
ization at load-time. With ASLR for instance, the compiler
prepares binaries for randomization during code generation
while the loader selects the randomized base address.
Several load-time diversiﬁcation approaches also have run-
time components. Barrantes et al. [5], for instance, randomizes
the instruction set encoding as code is loaded and uses the
Valgrind dynamic binary rewriter [45] to decode the original
machine instructions as they are about to execute. Williams et
al. [67] similarly implement instruction set randomization atop
the Strata process virtual machine. Their approach even has a
compile-time component to prepare binaries by adding an extra
“hidden” parameter to each function. This parameter acts as a
per-function key whose expected value is randomly chosen at
load-time and checked on each function invocation. The process
virtual machine instruments each function to verify that the
correct key was supplied; it also randomizes the instruction
set encoding to prevent code injection. Without knowledge of
the random key value, function-level code-reuse (e.g., return-
into-libc) attacks are defeated. Finally, Shioji et al. [59]
implement address-randomization atop the Pin [42] dynamic
binary rewriter. A checksum is added to certain bits in each
address used in control ﬂow transfers and the checksum is
checked prior to each such transfer. Attacker injected addresses
can be detected due to invalid checksums. Checksums are
computed by adding a random value to a subset of the bits in
the original address and hashing it.
In contrast to these hybrid approaches, Davi et al. [21] imple-
ment a pure load-time diversiﬁcation approach that randomizes
all segments of program binaries. The code is disassembled
and split into code fragments at call and branch instructions.
The resulting code fragments are used to permute the in-
memory layout of the program. The authors assume that binaries
contain relocation information to facilitate disassembly and
consequently omit a mechanism to compensate for disassembly
errors.
In addition to the general beneﬁts of post-distribution, the
particular beneﬁts of load-time diversity are:
Compatibility with signed binaries: Load-time diversi-
ﬁcation avoids making changes to the on-disk representation
of binaries and therefore permits integrity checking of signed
binaries in contrast to post-installation rewriting approaches.
Dynamic disassembly: With the exception of Davi et
al. [21], load-time approaches are based on dynamic binary
rewriting. Rather than trying to recover the complete control
ﬂow before execution, the rewriting proceeds on a by-need basis
starting from the program entry point. Control ﬂow transfers to
code that has not already been processed are intercepted and
rewritten before execution; already translated code fragments
are stored in a code cache to avoid repeated translation of
frequently executed code. This avoids disassembly errors and
consequently the need to handle these at runtime.
The drawbacks of load-time approaches are:
Runtime overhead of dynamic rewriting: Dynamic
rewriting, like dynamic compilation, happens at runtime and
thereby adds to the execution time. In addition, the binary
rewriter itself, its meta-data, and code cache increase the
pressure on the cache hierarchy and the branch predictors.
No sharing of code pages for randomized libraries:
Operating systems use virtual memory translation to share a
single copy of a shared library when it is loaded by multiple
processes. Since libraries such as libc are loaded by almost
every process on a Unix system, this leads to substantial savings.
However, load-time rewriting of shared libraries causes these
to diverge among processes which prevents sharing of code
pages.
15)Execution: The preceding sections have already covered
approaches with runtime aspects, e.g., those involving dynamic
binary rewriting. We now focus on diversiﬁcation that primarily
takes place during execution. The fact that certain techniques,
i.e., dynamic memory allocation and dynamic compilation,
cannot be randomized before the program runs motivates these
approaches. Consequently, runtime diversiﬁcation approaches
complement all previously discussed approaches by randomiz-
ing additional program aspects.
Many heap-based exploits rely on the heap layout. Ran-
domizing the placement of dynamically allocated data and
meta-data makes such attacks more difﬁcult. The heap layout
is randomized one object at a time by modifying the memory
allocator [46]. The diversifying allocator has several degrees of
freedom. It can lay out objects sparsely and randomly in the
virtual address space rather than packing them closely together.
It can also ﬁll objects with random data once they are released
to neutralize use-after-free bugs.
Dynamic code generation has also been exploited via JIT-
spraying attacks against web browsers that compile JavaScript
to native code. Like ahead-of-time compilers, just-in-time
compilers can be modiﬁed to randomize the code they gener-
ate [66]. For legacy and proprietary JIT-compilers, dynamic
binary rewriting enables randomization without any source code
changes at the expense of a higher performance penalty [30].
Such rewriting, however, creates higher overheads than code
randomization done directly by the JIT-compiler.
16)Updating: Program patches are the delivery vehicle for
security and usability improvements. Attackers can compute the
code changes between two versions by comparing a program
before and after applying an update. Unfortunately, knowledge
of the code changes helps adversaries locate exploitable bugs
and target users that have not yet updated their software.
Software updates can be protected by diversifying each
program release before generating the patch. This has two
beneﬁcial effects. First, diversiﬁcation will make the machine
code diverge even in places where the source code of the
two program releases do not; this potentially hides the “real”
changes in a sea of artiﬁcial ones. Second, diversiﬁcation can
be done iteratively until the heuristics used to correlate two
program versions fail; this greatly increases the required effort
to compare two program releases at the binary level [18], [17].
Note that diversity against reverse engineering program updates
works by randomizing different program releases (temporal
diversity) rather than randomizing program implementations
between different systems (spatial diversity).
285
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:27 UTC from IEEE Xplore.  Restrictions apply. 
Software diversity can also be used to introduce artiﬁcial
software updates. It is generally recognized that, given full
access to a binary, a determined adversary can successfully re-
verse engineer and modify the program given enough time [13].
If the binary requires server access to work, the server can force
client systems to download a continuous stream of updates by
refusing to work with older client software releases [15]. This
exhausts the resources of an adversary since he is forced to
reverse engineer each individual release.
V. QUANTIFYING THE IMPACT OF DIVERSITY
The preceding section described software diversiﬁcation
approaches in qualitative terms. While this is important to
implementors of diversiﬁcation engines,
it does not help
adopters of diversiﬁed software quantify the associated costs
and beneﬁts. This section survey how researchers quantify the
security and performance impacts of software diversity.
A. Security Impact
Software diversity is a broad defense against current and
future implementation-dependent attacks. This makes it hard to
accurately determine its security properties before it is deployed.
The goal of diversity—to drive up costs to attackers—cannot be
measured directly, so researchers resort to proxy measurements.
Ordered from abstract to concrete, the security evaluation
approaches used in the literature are:
Entropy analysis: Entropy is a generic way to measure
how unpredictable the implementation of a binary is after
diversiﬁcation. Low entropy solutions, e.g., ASLR on 32-
bit systems, are insecure because an attacker can defeat
randomization via brute-force attacks [58]. Entropy, however,
overestimates the security impact somewhat since two program
variants can differ at the implementation level and yet be
vulnerable to the same attack.
Attack-speciﬁc code analysis: The construction of certain
attacks has been partially automated. Gadget scanners [53], [32],
[54], for instance, automate the construction of ROP chains.
These tools are typically used to show that a code reuse attack
generated by scanning an undiversiﬁed binary stops working
after diversiﬁcation. However, adversaries could collect a set
of diversiﬁed binaries and compute their shared attack surface
which consists of the gadgets that survive diversiﬁcation—
i.e., they reside at the same location and are functionally
equivalent. Homescu et al. [31] use this stricter criterion—
surviving gadgets—in their security evaluation.
Logical argument: Early papers on diversity did not qualify
the security properties and rely on logical argumentation
instead [13]. For instance, if an attack depends on a particular
property (say the memory or code layout of an application)
which is randomized by design, then the defense must succeed.
Unfortunately, such reasoning does not demonstrate the entropy
of the solution, i.e., how hard it is for an attacker to guess how
a program was randomized.
Testing against concrete attacks: Often, researchers can
build or obtain concrete attacks of the type their technique
defend against. Showing that such attacks succeed before
diversiﬁcation but fail afterwards is a common proxy for
security. Again, such testing does not imply high entropy.
TABLE II: Security impact of transformations.
Study
Defends
Against
Evaluation
Cohen’93 [13]
Forrest et al.’97 [24]
PaX Team’01 [48]
Chew & Song’02 [12]