Stage 2
Stage 3
Stages 1–3
Sim Feature Mapping
Mob IPs to Desk IPs
Mob URLs to Desk URLs
Mob Apps to Desk URLs
Distance Metric
Bhatta
Bhatta’
Bhatta*
Sim Thresh
Mean Sim
0.07
0.13
0.02
0.33
0.18
0.11
Same as in Individual Stages above
0.33, 0.16, 0.03
nq
44
44
44
44
Acc
0.61
0.52
0.16
0.84
Prec
1
0.85
0.19
0.88
Rec
0.63
0.59
0.5
0.95
F-1
0.77
0.7
0.27
0.91
Table 4: Test set results. The first three rows show the results for running each stage individually. The fourth row shows the results for
running the three stages consecutively. We normalized the Bhattacharyya coefficient (Bhatta) to a range between 0 (low similarity) and
1 (high similarity). Bhatta’ denotes the exclusion of URLs in the Alexa Top 50 [4] and all columbia.edu URLs. Further, Bhatta* excludes
the most used 100 apps according to our training set. We selected the best similarity threshold (Sim Thresh) for each stage according
to observations in our training runs. Mean Sim is the mean similarity across all 44 device pairs in the test set, and nq is the size of the test set.
similarity threshold is set. If a threshold is reached at one
stage, a match is declared for the mobile-desktop device pair
with the highest similarity score. Otherwise, the algorithm
continues to evaluate whether the similarity threshold for
a different feature is reached in the next stage. To evaluate
the similarity between a mobile and a desktop device it
compares mobile to desktop IP addresses, mobile to desktop
web URLs, and mobile apps to desktop web URLs.
Test Set Results. To test our approach we randomly sepa-
rated the set of device pairs in our dataset into a training (nt =
63) and a test set (nq =44). We used the former to tune our
algorithm and features and held out the latter for performance
evaluation. As shown in Table 4, running all three stages of
the algorithm consecutively on our test set leads to precision,
recall, and F-1 scores of 0.88, 0.95, and 0.91, respectively.
The F-0.5 score [50], which emphasizes precision over recall,
reaches 0.91. In detail, we obtained 37 true positives (TP), 5
false positives (FP), 0 true negatives (TN), and 2 false neg-
atives (FN). These results are based on the usual definitions,
i.e., accuracy, Acc=(TP+TN)/(TP+TN+FP+FN), pre-
cision, Prec=TP/(TP+FP), recall, Rec=TP/(TP+FN),
and F-1 score, F-1 =(2·Prec·Rec)/(Prec+Rec).
To make the matching more difficult we included in
each run of our algorithm in every stage data from users for
which we only had data from one device type: data from
one user who only submitted mobile data and from 18 users
who only submitted desktop data. Further, our results are
based on modeling device correlation as binary classification.
Specifically, for each correct match between a user’s mobile
and desktop device we counted a true positive. For each
incorrect match we noted a false positive. If a mobile device
would have no corresponding desktop device it would have
been counted as a true negative if it remained unmatched.
However, as there was only one such instance in our test set
and that mobile device was actually matched, we counted
it as false positive. A false negative means that an instance
should have been matched, however, remained unmatched.
Running the three stages of our algorithm consecutively
leads to approximately balanced results for precision (0.88)
and recall (0.95), as shown in Table 4. However, when
running the stages individually, we obtain relatively higher
precision and lower recall in the first two stages and lower
Bhatta
Cosine
Jaccard
F-1, Sim Thresh
F-1, Sim Thresh
F-1, Sim Thresh
Stage 1
Stage 2
Stage 3
0.84, 0.1
0.74, 0.2
0.29, 0.1
0.83, 0.1
0.59, 0.1
0.29, 0.4
0.73, 0
0.67, 0
0.12, 0
Figure 6: Precision and recall for matching devices based on various
distance metrics and thresholds. The table shows the best F-1 scores
and their corresponding similarity thresholds. The features are the
same as described in the respective stages in Table 4. However,
the evaluation is performed here on the full dataset. For higher
thresholds recall scores tend to decrease while precision scores
tend to increase (except when they exclude too many true positives).
Overall, the Bhattacharyya coefficient returns the best results.
precision and higher recall in the third stage. This difference
highlights the tradeoff between achieving correct matches
(precision) and broad user coverage (recall). While it is
challenging to improve one without adversely affecting the
other [49,73], the similarity thresholds provide the controls
for adjustment. Figure 6 shows changes in precision and
recall for different similarity thresholds and distance metrics.
The high precision scores of Drawbridge (0.97 [22]) and
Tapad (0.91 [77]) seem to suggest that the industry favors
precision over recall.10 However, there is also an argument
10We interpret Tapad’s usage of the term accuracy to mean precision
(“[W]henever our Device Graph indicated a relationship between two or
more devices, it was accurate 91.2 percent of the time.”).
1398    26th USENIX Security Symposium
USENIX Association
Stage 10.000.250.500.751.00>0>1>2>3>4>5>6>7>8>9ThresholdPrecisionBhattaCosineJaccardStage 10.00.20.40.60.8>0>1>2>3>4>5>6>7>8>9ThresholdRecallBhattaCosineJaccardStage 20.40.60.81.0>0>1>2>3>4>5>6>7>8>9ThresholdPrecisionStage 20.000.250.500.75>0>1>2>3>4>5>6>7>8>9ThresholdRecallStage 30.00.10.20.30.40.5>0>1>2>3>4>5>6>7>8>9ThresholdPrecisionStage 30.00.20.40.6>0>1>2>3>4>5>6>7>8>9ThresholdRecallto be made against emphasizing precision: some device
mismatches may be irrelevant. Particularly, we believe that
mismatches might happen for people living in the same
household (in case of mobile IP to desktop IP similarity) or
individuals having the same interests (in case of web domain
and app to web domain similarity). In these situations a
mismatched device might still be a meaningful ad target [24].
The reason is that targeted purchase decisions might be
made at the household level or look-alike audiences might
be sufficiently valuable for an ad network [80].
Our results show that IP addresses are very meaningful
for matching devices, which is in line with Cao et al.’s
findings [14]. They reached an average F-0.5 score of 0.86
in the Drawbridge competition [23] using only features
from IP address data. However, beyond this finding our
results further suggest that visited web domains are a good
indicator for device similarity as well. In fact, there might
be situations in which they can be more revealing than IP
addresses. For example, if users of the same household share
an IP address, their devices can not be distinguished based
on this feature. Also, while the correlation between apps
and desktop domains does not contribute as much as the
IP address and domain correlations, it still provides some
meaningful signal as the results for the individual run of
the third stage in Table 4 demonstrate. Most importantly,
however, performance seems to increase if multiple features
are applied consecutively. Some users can be better matched
based on IP addresses and others on web domains or apps.
We note that we leveraged a manual mapping between
apps and desktop domains via company names or other
common identifiers thereby transforming a feature with
minimal effect in our dataset and the Drawbridge competi-
tion [14,48,50,53,62,71,75,82] to a useful feature. Similarly,
the domain mapping proved to be useful as well due to users’
visits to the same domains across devices. These results
highlight that cross-device matching is not completely reliant
on IP matching, as suggested by the results in the Drawbridge
competition. Our results seem to confirm the conjecture that
carefully hand-crafted similarity features are of paramount
importance while algorithms play a smaller role for the task
of correlating mobile and desktop devices [82].
We experimented with various other features that
ultimately did not prove useful. In particular, an algorithm
leveraging system language and time zone did not match
devices better than random. We also tried excluding sets of
frequently used public IP addresses. However, different from
excluding domains and apps, which, as described in Table 4,
proved to be beneficial, this measure did not lead to better
performance. We further tried different matching thresholds
and evaluated various distance metrics as shown in Figure 6.
In future work it would be interesting to examine the extent
to which the time, order, and duration of app and url access
play a role for device correlation. E-mail and other message
content is an obvious candidate for a useful feature as well.
Applicability to Larger Datasets. With a runtime of
O(n(n−1)/2) our algorithm is suitable for large scale anal-
ysis. However, it is obvious that our dataset is many orders
smaller than the data that cross-device companies are usually
working with. This difference in size begs the question to
which extent our findings are applicable to larger datasets.
For the similarity of IP addresses this question was already
reliably answered. The Drawbridge competition results, for
instance, by Landry et al. [53], are based on a set of about
62K mobile devices and confirm the meaningfulness of IP
features. For web domain features the situation is different
as the Drawbridge data did not contain those for mobile
devices. However, we can make an argument that lends some
supports for the applicability of our results to larger datasets.
Whether web data can be correlated across devices
rests on two premises: first, users visiting an intersecting
set of domains on both their mobile and desktop devices
and, second, domains being sufficiently distinct to allow
identification of users. To examine the first premise we
randomly selected 50 U.S. domains out of the top 5K sites
that were quantified by Quantcast [70] and found a mean of
17.1% users visiting a website both on a mobile and desktop
device (during a 30-day period and at the 95% confidence
level with a lower bound of 14.4% and an upper bound
of 19.5% using the bootstrap technique). As to the second
premise, it was shown for a set of about 368K desktop
and mobile Internet users that 97% of them were uniquely
identifiable if at least four visited websites were known.11
Limitations. It would be an interesting exercise to compare
our techniques against those currently in use in industry.
However, we are not aware of any publicly available
resources allowing us to do so. The same is true for
cross-device tracking datasets. To our knowledge, there
is no dataset publicly available beyond the CDT dataset
that we created. The only other cross-device tracking
dataset we know of was made available by Drawbridge
to participants of the Drawbridge competition solely for
competition purposes [23]. However, even if this dataset
would be available, it would only allow an incomplete
analysis, particularly, as features were generally anonymized
and mobile web history was not included in the dataset.
Consequently, at this point it does not seem possible to
compare our approach to others or evaluate its performance
on a different dataset. However, as we implemented key
design elements that we found in available industry materials,
we believe that our results provide a first approximation for
cross-device tracking approaches applied in practice.
There are various considerations of identifying and corre-
lating devices in practice that we cannot meaningfully test.
A first point concerns the time period for which users are
being tracked. We believe that the three weeks of data that
we have available for most users (Table 2)—the concrete
11All users in our dataset who visited at least one mobile and one desktop
website had unique web histories as well.
USENIX Association
26th USENIX Security Symposium    1399
length depending on the number of days on which they used
their devices—are realistic. However, we lack the insight
for which duration cross-device tracking actually occurs in
practice. Also, despite some cross-device companies’ broad
coverage of websites and apps (§ 7), none of them has access
to complete IP, web, and app data of Internet users. However,
ultimately this limitation is one of reach and not of perfor-
mance. By setting similarity thresholds high even companies
with limited data can obtain precise results, albeit, at the cost
of low recall. Further, our dataset does not contain full IP his-
tories either. In addition, our data is probably more homoge-
nous than real data, and, thus, more difficult to assess. Users
in our study were mostly students located in a confined space
with many commonly shared web domains and IP addresses.
6 Learning from Cross-device Data
In this section we examine whether cross-device data enables
cross-device companies to make more accurate predictions
than they could make using data from individual devices
alone. We address this question for two inquiries: users’ inter-
est in finance—a randomly selected interest category—and
gender. Both are relevant ad targeting criteria. For interest in
finance we obtained the most accurate predictions by using
data from both mobile and desktop devices. Consequently,
this is a task in which predictions about a user from
cross-device data appear more privacy-invasive than those
from single device data sources. However, as we also found
a lack of performance increase for predicting a user’s gender,
it appears that some prediction tasks might not become more
accurate with the availability of cross-device data.
Predicting Interest in Finance. As a starting point for
our feature creation we used Alexa category rankings [5]
and Google Play store categories [38] to identify the top
25 finance domains that have both a website and an app.
Then, we used the Weka machine learning toolkit [40] to
explore the potential for predicting interest in finance. We
experimented with various features and all available standard
algorithms. We used a word-to-vector preprocessor and
found logistic regression to be the most effective technique.
Due to the class imbalance of only 23% users in our dataset
expressing an interest in finance we ran logistic regression
as a cost-sensitive classifier increasing the cost for a false
positive on average 1.5 times over the cost for a false
negative. Our results, which are shown in Figure 7 and
based on 10-fold cross validation, suggest that predicting an
interest in finance for users in our dataset is more accurate
if both desktop and mobile data are available.
In particular, predicting from mobile data alone proved to
be the weakest option. One reason seems to be that we only
had 90 features from the mobile data compared to 106 and
107 for the desktop and combined data, respectively. Using
desktop data only we tried to increase the performance to
the level of the combined mobile and desktop data, which
Acc
0.64
0.75
0.79
0.83
95% CI
0.55–0.73
0.67–0.83
0.71–0.87
0.76–0.9
Prec
0.26
0.5
0.57
0.68
Rec
0.22
0.52
0.59
0.63
F-1
0.24
0.51
0.58
0.65
ROC
0.5
0.68
0.75
0.79
A.
B.
C.
D.
Figure 7: Logistic regression for predicting interest in finance from
mobile web domains and apps (Mob) and desktop web domains
(Desk). 95% CI designates the binomial proportion confidence
interval for the accuracy at the 95% level assuming a normal
distribution. The F-1 score based on features from both types of
data (Mob & Desk - 108 Features) is higher than the scores obtained
using mobile and desktop data individually (even with more
features as in Desk - 2,923 Features). We observed similar results
for value shoppers with F-1 scores of 0.17 (Mob - 85 Features),
0.25 (Desk - 99 Features), and 0.41 (Mob & Desk - 104 Features).
reached an F-1 score of 0.65. However, we were only able to
obtain an F-1 score of 0.58 by substantially increasing the fea-
ture space to 2,923 features, at which point we saw no further
improvement. Combining desktop and mobile data and lever-
aging 107 features outperformed all other approaches. The
ROC curves in Figure 7 visualize this finding. The predic-
tions that users have an interest in finance are shown in orange
while the negative predictions for not having an interest in
finance are displayed in blue. For the latter the F-1 scores are:
Mob - 91 Features: 0.77, Desk - 107 Features: 0.83, Desk -
2,923 Features: 0.86, and Mob & Desk - 108 Features: 0.89.
Predicting Gender. While the predictive performance of
a user’s interest in finance increased with the availability
of both desktop and mobile data, it appears that such
improvement does not necessarily hold for all classification
tasks. Particularly, classifier performance for the prediction
of gender from combined desktop and mobile data was
not better than the performance using desktop data alone.
Applying logistic regression with 10-fold cross validation we
obtained identical scores for precision, recall, and F-1 with
values of 0.82, 0.81, and 0.82, respectively. It did not make a
difference whether mobile data was added to the desktop data