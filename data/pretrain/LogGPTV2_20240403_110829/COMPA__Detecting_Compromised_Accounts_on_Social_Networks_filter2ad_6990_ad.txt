called a suspicious group if the fraction of messages that vi-
olates their respective accounts’ behavioral proﬁles exceeds
a threshold th. In our implementation, we decided to use a
threshold that is dependent on the size of the group. The ra-
tionale behind this is that, for small groups, there might not
be enough evidence of a campaign being carried out unless
a high number of similar messages violate their underlying
behavioral proﬁles. In other words, small groups of similar
messages could appear coincidentally, which might lead to
false positives if the threshold for small groups is too low.
This is less of a concern for large groups that share a sim-
ilar message.
In fact, even the existence of large groups
is already somewhat unusual. This can be taken into con-
sideration by choosing a lower threshold value for larger
groups. Accordingly, for large groups, it should be sufﬁ-
cient to raise an alert if a smaller percentage of messages
violate their behavioral proﬁles. Thus, the threshold th
is a linear function of the size of the group n deﬁned as
th(n) = max(0.1, kn + d).
Based on small-scale experiments, we empirically deter-
mined that the parameters k = −0.005 and d = 0.82 work
well. The max expression assures that at least ten percent
of the messages in big groups violate their behavioral pro-
ﬁles. Our experiments show that these threshold values are
robust, as small modiﬁcations do not inﬂuence the quality
of the results. Whenever there are more than th messages in
a group (where each message violates its proﬁle), COMPA
declares all users in the group as compromised.
Bulk applications. Certain popular applications, such
as Nike+ or Foursquare, use templates to send similar mes-
sages to their users. Unfortunately, this can lead to false
positives. We call these applications bulk applications. To
identify popular bulk applications that send very similar
messages in large amounts, COMPA needs to distinguish
regular client applications (which do not automatically post
using templates) from bulk applications. To this end, our
system analyzes a randomly selected set of S messages for
each application, drawn from all messages sent by this ap-
plication. COMPA then calculates the average pairwise Lev-
enshtein ratios for these messages. The Levenshtein ratio is
a measure of the similarity between two strings based on
the edit distance. The values range between 0 for unrelated
strings and 1 for identical strings. We empirically deter-
mined that the value 0.35 effectively separates client from
bulk applications.
COMPA ﬂags all suspicious groups produced by client
applications as compromised. For bulk applications, a fur-
ther distinction is necessary, since we only want to discard
groups that are due to popular bulk applications. Popular
bulk applications constantly recruit new users. Also, these
messages are commonly synthetic, and they often violate
the behavioral proﬁles of new users. For existing users, on
the other hand, past messages from such applications con-
tribute to their behavioral proﬁles, and thus, additional mes-
sages do not indicate a change in behavior. If many users
made use of the application in the past, and the messages
the application sent were in line with these users’ behavioral
proﬁles, COMPA considers such an application as popular.
To assess an application’s popularity, COMPA calculates
the number of distinct accounts in the social network that
made use of that application before it has sent the ﬁrst mes-
sage that violates a user’s behavioral proﬁle. This number is
multiplied by an age factor (which is the number of seconds
between the ﬁrst message of the application as observed
by COMPA and the ﬁrst message that violated its user’s
behavioral proﬁle). The intuition behind this heuristics is
the following: An application that has been used by many
users for a long time should not raise suspicion when a new
user starts using it, even if it posts content that differs from
this user’s established behavior. Manual analysis indicated
that bulk applications that are used to run spam and phish-
ing campaigns over compromised accounts have a very low
popularity score. Thus, COMPA considers a bulk applica-
tion to be popular if its score is above 1 million. We assume
that popular bulk applications do not pose a threat to their
users. Consequently, COMPA ﬂags a suspicious group as
containing compromised accounts only if the group’s pre-
dominant application is a non-popular bulk application.
6 Evaluation
We implemented our approach in a tool, called COMPA
and evaluated it on Twitter and Facebook; we collected
tweets in real time from Twitter, while we ran our Facebook
experiments on a large dataset crawled in 2009.
We show that our system is capable of building meaning-
ful behavioral proﬁles for individual accounts on both net-
works. By comparing new messages against these proﬁles,
it is possible to detect messages that represent a (possibly
malicious) change in the behavior of the account. By group-
ing together accounts that contain similar messages, many
of which violate their corresponding accounts’ behavioral
proﬁles, COMPA is able to identify groups of compromised
accounts that are used to distribute malicious messages on
these social networks. We continuously ran COMPA on a
stream of 10% of all public Twitter messages on a single
computer (Intel Xeon X3450, 16 GB ram). The main lim-
itation was the number of user timelines we could request
from Twitter, due to the enforced rate-limits. Thus, we are
conﬁdent that COMPA can be scaled up to support online so-
cial networks of the size of Twitter with moderate hardware
requirements.
6.1 Data Collection
Twitter Dataset
We obtained elevated access to Twitter’s streaming and
RESTful API services. This allowed us to collect around
10% of all public tweets through the streaming API, result-
ing in roughly 15 million tweets per day on average. We
collected this data continuously starting May 13, 2011 until
Aug 12, 2011. In total, we collected over 1.4 billion tweets
from Twitter’s stream. The stream contains live tweets as
they are sent to Twitter. We used an observation interval
of one hour. Note that since the stream contains randomly
sampled messages, COMPA regenerated the behavioral pro-
ﬁles for all involved users every hour. This was necessary,
because it was not guaranteed that we would see the same
user multiple times.
To access the historical timeline data for individual ac-
counts, we rely on the RESTful API services Twitter pro-
vides. To this end, Twitter whitelisted one of our IP ad-
dresses, which allowed us to make up to 20,000 RESTful
API calls per hour. A single API call results in at most
200 tweets. Thus, to retrieve complete timelines that ex-
ceed 200 tweets, multiple API requests are needed. Fur-
thermore, Twitter only provides access to the most recent
3,200 tweets in any user’s timeline. To prevent wasting API
calls on long timelines, we retrieved timeline data for either
the most recent three days, or the user’s 400 most recent
tweets, whatever resulted in more tweets.
On average, we received tweets from more than 500,000
distinct users per hour. Unfortunately, because of the API
request limit, we were not able to generate proﬁles for all
users that we saw in the data stream. Thus, as discussed in
the previous section, we ﬁrst cluster messages into groups
that are similar. Then, starting from the largest cluster,
we start to check whether the messages violate the behav-
ioral proﬁles of their senders. We do this, for increasingly
smaller clusters, until our API limit is exhausted. On av-
erage, the created groups consisted of 30 messages. This
process is then repeated for the next observation period.
Facebook Dataset
Facebook does not provide a convenient way of collecting
data. Previous work deployed honey accounts on Face-
book, and collected data for the accounts that contacted
them [7]. Unfortunately, this approach does not scale, since
a very large number of honey proﬁles would be required
to be able to collect a similar number of messages as we
did for Twitter. Therefore, we used a dataset that was
crawled in 2009. We obtained this dataset from an inde-
pendent research group that performed the crawling in ac-
cordance with the privacy guidelines at their research in-
stitution. Unfortunately, Facebook is actively preventing
researchers from collecting newer datasets from their plat-
form by various means, including the threat of legal action.
The dataset was crawled from geographic networks on
Facebook. Geographic networks were used to group to-
gether people that lived in the same area. The default pri-
vacy policy for these networks was to allow anybody in the
network to see all the posts from all other members. There-
fore, it was easy, at the time, to collect millions of messages
by creating a small number of proﬁles and join one of these
geographic networks. For privacy reasons, geographic net-
works have been discontinued in late 2009. The dataset we
used contains 106,373,952 wall posts collected from ﬁve
geographic networks (i.e., London, New York, Los Ange-
les, Monterey Bay, and Santa Barbara). These wall posts
are distributed over almost two years (Sept. 2007 - July
2009).
6.2 Training the Classiﬁer
To determine the weights that we have to assign to each
feature, we applied Weka’s SMO [21] to a labeled training
dataset for both Twitter and Facebook.
While the Facebook dataset contains the network of a
user, Twitter does not provide such a convenient proximity
feature. Therefore, we omitted this feature from the evalua-
tion on Twitter. For Twitter, the weights for the features are
determined from our labeled training dataset consisting of
5,236 (5142 legitimate, 94 malicious) messages with their
associated feature values as follows: Source (3.3), Personal
Interaction (1.4), Domain (0.96), Hour of Day (0.88), Lan-
guage (0.58), and Topic (0.39).
To manually determine the ground truth for an account in
our training set, we examined the tweets present in that ac-
count’s timeline. If an account under analysis uses URLs in
tweets, we follow these links and inspect the landing pages.
Should we ﬁnd that the URL lead to a phishing page, we
classify the account as compromised. As we discuss in Sec-
tion 6.6, phishing campaigns frequently make use of URLs
to guide potential victims to phishing websites that prompt
the visitor to disclose her account credentials. Another
source of information we used to assess whether an account
was compromised are application description pages. Each
tweet sent by a third-party application contains a link to a
website chosen by the developer. If such a link leads to a
malicious page, we also consider the account as compro-
mised 1. Finally, we exploit the fact that humans can extract
the topic of a message from small amounts of information.
That is, we would ﬂag an account as compromised if the
topic of tweets in the timeline abruptly switches from per-
sonal status updates to tweets promoting work from home
opportunities and free electronic gadgets (common scams).
As we will show later in this section, a signiﬁcant portion
of the tweets that indicate that an account is compromised
get removed. This makes it time consuming to manually
identify compromised accounts on Twitter. Although the
number of malicious samples in our training dataset is lim-
ited, the feature values turned out to be stable over different
training set sizes.
Figure 1 illustrates how the weights for each feature vary
with different sizes of the training set. Each set of ﬁve bars
corresponds to one feature. Each bar within a set represents
the observed weights for this feature (i.e., average, min, and
max) that were produced by 25 iterations with a ﬁxed train-
ing set size. For each iteration, the contents of the training
set were randomly chosen. Overall, this experiment was re-
peated ﬁve times with different training set sizes. It can be
seen that when smaller training sets are used, the observed
weights vary heavily. This variance becomes small for lager
training datasets indicating that the weights are fairly stable.
Figure 1. Features evolving with different sizes of train-
ing sets. Each experiment was conducted 25 times on ran-
dom subsets of 25%, 50%, 70%, 90%, and 99% of the 5,236
labeled training instances. The fraction of positive to nega-
tive samples remained constant.
On Facebook, based on a labeled training dataset of
279 messages (181 legitimate, 122 malicious), the weights
were: Source (2.2), Domain (1.1), Personal Interaction
(0.13), Proximity (0.08), and Hour of Day (0.06). Weka
1While Twitter has been ﬁltering links to potentially malicious URLs in posted
messages for a while, they only started ﬁltering these application pages after we in-
formed Twitter that an attacker can choose this page to be a malicious site.
determined that the Language feature has no effect on the
classiﬁcation. Moreover, as discussed earlier, assessing the
message topic of an unstructured message is a complicated
natural language processing problem. Therefore, we omit-
ted this feature from the evaluation on the Facebook dataset.
Similar to analyzing Twitter messages, we also assessed
changes of topic across wall posts in the Facebook dataset to
identify compromised accounts for the training data. Addi-
tionally, we inspected Facebook application pages and their
comment sections where users can leave feedback. As the
Facebook dataset was collected in 2009, we would also con-
sider an account as compromised if the application that sent
that post was blocked by Facebook in the meantime.
6.3 Detection on Twitter
The overall results for our Twitter evaluation are pre-
sented in Table 2. Due to space constraints, we will only
discuss the details for the text similarity measure here.
However, we found considerable overlap in many of the
groups produced by both similarity measures. More pre-
cisely, for over 8,200 groups, the two similarity measures
(content and URL similarity) produced overlaps of at least
eight messages. COMPA found, for example, phishing cam-
paigns that use the same URLs and the same text in their
malicious messages. Therefore, both similarity measures
produced overlapping groups.
The text similarity measure created 374,920 groups with
messages of similar content. 365,558 groups were reported
as legitimate, while 9,362 groups were reported as compro-
mised. These 9,362 groups correspond to 343,229 compro-
mised accounts. Interestingly, only 12,238 of 302,513 ap-
plications ever produced tweets that got grouped together.
Furthermore, only 257 of these applications contributed to
the groups that were identiﬁed as compromised.
For each group of similar messages, COMPA assessed
whether the predominant application in this group was a
regular client or a bulk application. Our system identiﬁed
12,347 groups in the bulk category, of which 1,647 were
ﬂagged as compromised. Moreover, COMPA identiﬁed a
total of 362,573 groups that originated from client applica-
tions. Of these, 7,715 were ﬂagged as compromised.
Overall, our system created a total of 7,250,228 behav-
ioral proﬁles. COMPA identiﬁed 966,306 messages that vio-
late the behavioral proﬁles of their corresponding accounts.
Finally, 400,389 messages were deleted by the time our sys-
tem tried to compare these messages to their respective be-
havioral proﬁles (i.e., within an hour).
False Positives
Using the text similarity measure, COMPA identiﬁed
343,229 compromised Twitter accounts in 9,362 clusters.
To analyze the accuracy of these results, we need to an-
TimeSourceTopicDomainPersonalInteractionLanguage0.00.51.01.52.02.53.03.5WeightNetwork & Similarity Measure
Twitter Text
Twitter URL
Facebook Text
Accounts
Accounts Groups Accounts
Total Number
# Compromised