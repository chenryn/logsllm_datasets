achieves 57% accuracy on all attacks, which is comparable to
58.4% of all defenses as previously shown in Table V. Even
for the best-defense ensemble, it does not signiﬁcantly improve
the accuracy than the most successful defense. Particularly, the
average accuracy of best-defense ensemble against all attacks
is 84%, which is marginally greater than 82.2% achieved by
the most successful defense NAT on CIFAR-10. This partially
conﬁrms the conclusion that ensemble of multiple defenses
does not guarantee to perform better [55].
On the other hand, ensemble of multiple defenses can
improve the lower bound of defense ability for individuals
(cid:23)(cid:25)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
against certain adversarial attacks. According to the results,
there is no extremely low classiﬁcation accuracy for ensemble
models. As we analyzed in Section IV-C1, most individual
defenses have the capability of defending against some speciﬁc
adversarial attacks but not all attacks. For some individual
defenses, their classiﬁcation accuracy on some attacks even
drops signiﬁcantly below 10%, but not the case with ensemble.
Even in the worst ensemble of completely-random,
their
classiﬁcation accuracy on CIFAR-10 are above 20%.
Remark 8. For ensemble methods, we conﬁrm that ensemble
of different defenses cannot signiﬁcantly improve the defensive
capabilities as a whole [55], but it can improve the lower
bound of defense ability for individuals.
VI. DISCUSSION
Limitations and Future Work. Below, we discuss the
limitations of this work along with the future work.
Firstly, we only integrate the most representative 16 ad-
versarial attacks and 13 defenses. Even though we do cover
all the categories of the state-of-the-art attacks and defenses,
DEEPSEC does not enumerate and implement all strategies due
to the fact some strategies have similar methodology. However,
DEEPSEC employs a modular design and implementation,
which makes it easy for users to integrate new attacks,
defenses and corresponding utility metrics. Hence, we open
source DEEPSEC and encourage the public to contribute.
Secondly, due to space limitations, we employ one setting
for each individual attack and defense. To be speciﬁc, the
exclusive parameters among different attacks or defenses are
kept with the same or similar with the original settings in the
papers, while the common parameters are kept the same for
fair comparison. However, based on DEEPSEC, it is easy to
extend the evaluations to different settings.
Finally, in the current implementation, we mainly focus
on non-adaptive and white-box attacks. Nevertheless, we em-
phasize that the modular and generic design of DEEPSEC
enables it to be readily extendable to support many other
adversarial attacks via controlling the information available
to the attacks/defenses. For instance, adaptive attacks [9],
[35] are easily incorporated into DEEPSEC if the adversary is
allowed to access the deployed defense-enhanced model when
generating AEs; to support black-box attacks [33], [34], we
may restrict the attacks’ access to only the input and output
of DL models; for unsupervised learning (e.g., generative
models) [57], [58], we may disable the attacks’ access to
the label
information. As the modular implementation of
DEEPSEC provides standard interfaces for accessing data and
models, such extensions can be readily implemented.
Additional Related Work. Currently, several attack/defense
platforms have been proposed, like Cleverhans [14], Fool-
box [59], AdvBox [60], ART [61], etc. Cleverhans is the
ﬁrst open-source library that mainly uses Tensorﬂow [62] and
currently provides implementations of 9 attacks and 1 simple
adversarial training based defense. Foolbox improves upon
Cleverhans by interfacing it with other popular DL frameworks
such as PyTorch [45], Theano [63], and MXNet [64]. Advbox
is implemented on the PaddlePaddle [65] and includes 7
attacks. ART also provides a library that integrates 7 attacks
and 5 defenses. However, DEEPSEC differs from the exiting
work in several major aspects:
1) Existing platforms provide a fairly limited number of
adversarial attacks and only few of them implement
defense methods. However, DEEPSEC incorporates 16
attacks and 13 defenses, covering all the categories of
the state-of-the-art attacks and defenses.
2) In addition to a rich implementation of attacks/defenses.
DEEPSEC treats evaluation metrics as the ﬁrst-class cit-
izens and implements 10 attack and 5 defense utility
metrics, which help assess given attacks/defenses.
3) Rather than solely providing reference implementation
of attacks/defenses, DEEPSEC provides a unique analysis
platform, which enables researchers and practitioners to
conduct comprehensive and informative evaluation on
given attacks, defences, and DL models.
VII. CONCLUSION
We design, implement and evaluate DEEPSEC, a uniform
security analysis platform for deep learning models. In its
current implementation, DEEPSEC incorporates 16 state-of-
the-art adversarial attacks with 10 attack utility metrics and 13
representative defenses with 5 defense utility metrics. To our
best knowledge, DEEPSEC is the ﬁrst-of-its-kind platform that
supports uniform, comprehensive, informative, and extensible
evaluation of adversarial attacks and defenses. Leveraging
DEEPSEC, we conduct extensive evaluation on existing attacks
and defenses, which help answer a set of long-standing ques-
tions. We envision that DEEPSEC is able to serve as a useful
benchmark to facilitate adversarial deep learning research.
ACKNOWLEDGMENT
We would like to thank our shepherd Christopher Kruegel
and the anonymous reviewers for their valuable suggestions
for improving this paper. We are also grateful
to Xiaoyu
Cao, Jacob Buckman and Yang Song for sharing their code,
and to Yuan Chen and and Saizhuo Wang for helping open
source DEEPSEC. This work was partly supported by the
National Key Research and Development Program of China
under Nos. 2016YFB0800102 and 2016YFB0800201,
the
NSFC program under No. 61772466, the Zhejiang Provin-
cial Natural Science Foundation for Distinguished Young
Scholars under No. R19F020013,
the Provincial Key Re-
search and Development Program of Zhejiang, China un-
der Nos. 2017C01055, 2017C01064, and 2018C03052, the
Alibaba-ZJU Joint Research Institute of Frontier Technolo-
gies, the CCF-NSFOCUS Research Fund under No. CCF-
NSFOCUS2017011, the CCF-Venustech Research Fund un-
der No. CCF-VenustechRP2017009, and the Fundamental
Research Funds for
the Central Universities under No.
2016XZZX001-04. Ting Wang is partly supported by the
National Science Foundation under Grant No. 1566526 and
1718787.
(cid:23)(cid:25)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[28] J. Buckman, A. Roy, C. Raffel, and I. Goodfellow, “Thermometer
encoding: One hot way to resist adversarial examples,” in ICLR, 2018.
[29] X. Cao and N. Z. Gong, “Mitigating evasion attacks to deep neural
networks via region-based classiﬁcation,” in ACSAC, 2017.
[30] X. Ma, B. Li, Y. Wang, S. M. Erfani, S. Wijewickrema, M. E. Houle,
G. Schoenebeck, D. Song, and J. Bailey, “Characterizing adversarial
subspaces using local intrinsic dimensionality,” in ICLR, 2018.
[31] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
examples in deep neural networks,” in NDSS, 2018.
[32] D. Meng and H. Chen, “Magnet: a two-pronged defense against adver-
sarial examples,” in CCS, 2017.
[33] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
AsiaCCS, 2017.
[34] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash,
A. Rahmati, and D. Song, “Robust physical-world attacks on deep
learning models,” arXiv:1707.08945, 2017.
[35] N. Carlini and D. Wagner, “Adversarial examples are not easily detected:
Bypassing ten detection methods,” in AISec, 2017.
[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning represen-
tations by back-propagating errors,” nature, 1986.
[37] Wikipedia, “Utility,” https://en.wikipedia.org/wiki/Utility.
[38] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
transactions on image processing, 2004.
[39] A. Liu, W. Lin, M. Paul, C. Deng, and F. Zhang, “Just noticeable
difference for images with decomposition model for separating edge
and textured regions,” IEEE Transactions on Circuits and Systems for
Video Technology, 2010.
[40] B. Luo, Y. Liu, L. Wei, and Q. Xu, “Towards imperceptible and robust
adversarial example attacks against neural networks,” in AAAI, 2018.
[41] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, “Zoo: Zeroth
order optimization based black-box attacks to deep neural networks
without training substitute models,” in AISec, 2017.
[42] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu, “Pixel recurrent
neural networks,” arXiv:1601.06759, 2016.
[43] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning
in computer vision: A survey,” arXiv:1801.00553, 2018.
[44] Wikipedia, “Jensen–shannon divergence,” https://en.wikipedia.org/wiki/
Jensen-Shannon divergence.
[45] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in
pytorch,” 2017.
[46] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, 1998.
[47] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
[48] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
tiny images,” in Citeseer, 2009.
recognition,” in CVPR, 2016.
[49] J. Alakuijala, R. Obryk, O. Stoliarchuk, Z. Szabadka, L. Vande-
venne, and J. Wassenberg, “Guetzli: Perceptually guided jpeg encoder,”
arXiv:1703.04421, 2017.
[50] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in ma-
chine learning: from phenomena to black-box attacks using adversarial
samples,” arXiv:1605.07277, 2016.
[51] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable
adversarial examples and black-box attacks,” arXiv:1611.02770, 2016.
[52] T. G. Dietterich, “Ensemble methods in machine learning,” in Interna-
tional workshop on multiple classiﬁer systems. Springer, 2000.
[53] L. Rokach, “Taxonomy for characterizing ensemble methods in clas-
siﬁcation tasks: A review and annotated bibliography,” Computational
Statistics & Data Analysis, 2009.
[54] T. Strauss, M. Hanselmann, A. Junginger, and H. Ulmer, “Ensemble
methods as a defense to adversarial perturbations against deep neural
networks,” arXiv:1709.03423, 2017.
[55] W. He, J. Wei, X. Chen, N. Carlini, and D. Song, “Adversarial example
defense: Ensembles of weak defenses are not strong,” in WOOT, 2017.
[56] A. Bagnall, R. Bunescu, and G. Stewart, “Training ensembles to detect
adversarial examples,” arXiv:1712.04006, 2017.
[57] Y. Song, R. Shu, N. Kushman, and S. Ermon, “Generative adversarial
examples,” arXiv:1805.07894, 2018.
[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural networks
and tree search,” nature, vol. 529, no. 7587, p. 484, 2016.
[2] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” in ICLR, 2014.
[3] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,
P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., “End to
end learning for self-driving cars,” arXiv:1604.07316, 2016.
[4] O. M. Parkhi, A. Vedaldi, A. Zisserman et al., “Deep face recognition.”
in BMVC, 2015.
[5] Z. Yuan, Y. Lu, Z. Wang, and Y. Xue, “Droid-sec: deep learning in
android malware detection,” in SIGCOMM, 2014.
[6] P. Rajpurkar, J.
Irvin, A. Bagul, D. Ding, T. Duan, H. Mehta,
B. Yang, K. Zhu, D. Laird, R. L. Ball et al., “Mura dataset: Towards
radiologist-level abnormality detection in musculoskeletal radiographs,”
arXiv:1712.06957, 2017.
[7] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman, “Sok: Towards the
science of security and privacy in machine learning,” arXiv:1611.03814,
2016.
[8] X. Yuan, P. He, Q. Zhu, R. R. Bhat, and X. Li, “Adversarial examples:
Attacks and defenses for deep learning,” arXiv:1712.07107, 2017.
[9] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated Gradients Give
a False Sense of Security: Circumventing Defenses to Adversarial
Examples,” in ICML, 2018.
[10] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in S&P, 2016.
[11] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in EuroS&P, 2016.
[12] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in S&P, 2017.
[13] A. S. Ross and F. Doshi-Velez, “Improving the adversarial robustness
and interpretability of deep neural networks by regularizing their input
gradients,” in AAAI, 2018.
[14] N. Papernot, F. Faghri, N. Carlini, I. Goodfellow, R. Feinman, A. Ku-
rakin, C. Xie, Y. Sharma, T. Brown, A. Roy, A. Matyasko, V. Behzadan,
K. Hambardzumyan, Z. Zhang, Y.-L. Juang, Z. Li, R. Sheatsley, A. Garg,
J. Uesato, W. Gierke, Y. Dong, D. Berthelot, P. Hendricks, J. Rauber, and
R. Long, “cleverhans v2.1.0: an adversarial machine learning library,”
arXiv:1610.00768, 2016.
[15] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in ICLR, 2015.
[16] F. Tram`er, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel,
“Ensemble adversarial training: Attacks and defenses,” in ICLR, 2018.
[17] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
physical world,” in ICLR, 2017.
[18] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in ICLR, 2018.
[19] Y. Dong, F. Liao, T. Pang, H. Su, X. Hu, J. Li, and J. Zhu, “Boosting
adversarial attacks with momentum,” arXiv:1710.06081, 2017.
[20] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: A
simple and accurate method to fool deep neural networks,” in CVPR,
2016.
[21] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Univer-
sal adversarial perturbations,” in CVPR, 2017.
[22] W. He, B. Li, and D. Song, “Decision boundary analysis of adversarial
examples,” in ICLR, 2018.
[23] P. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh, “EAD: elastic-
net attacks to deep neural networks via adversarial examples,” in AAAI,
2018.
[24] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine learning
at scale,” in ICLR, 2017.
[25] C. Guo, M. Rana, M. Ciss´e, and L. van der Maaten, “Countering
adversarial images using input transformations,” in ICLR, 2018.
[26] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille, “Mitigating adversarial
effects through randomization,” in ICLR, 2018.
[27] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman, “Pixelde-
fend: Leveraging generative models to understand and defend against
adversarial examples,” in ICLR, 2018.
[58] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song, “Generating
adversarial examples with adversarial networks,” arXiv:1801.02610,
2018.
(cid:23)(cid:25)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
[59] J. Rauber, W. Brendel, and M. Bethge, “Foolbox v0. 8.0: A python
toolbox to benchmark the robustness of machine learning models,”
arXiv:1707.04131, 2017.
[60] P. Developers, “Advbox: A toolbox to generate adversarial examples,”
https://github.com/PaddlePaddle/models/tree/develop/ﬂuid/adversarial.
[61] IBM,
(art
https://github.com/IBM/adversarial-robustness-toolbox, 2018.
“Adversarial
robustness
toolbox
v0.1),”
[62] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for
large-scale machine learning.” in OSDI, 2016.
[63] R. Al-Rfou, G. Alain, A. Almahairi, C. Angermueller, D. Bahdanau,
N. Ballas, F. Bastien, J. Bayer, A. Belikov, A. Belopolsky et al.,
“Theano: A python framework for fast computation of mathematical
expressions,” arXiv:1605.02688, 2016.
[64] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,
C. Zhang, and Z. Zhang, “Mxnet: A ﬂexible and efﬁcient machine learn-
ing library for heterogeneous distributed systems,” arXiv:1512.01274,
2015.
[65] P.
Paddle,
“Parallel
distributed
deep
learning: An
scalable
deep
learning
easy-to-
platform,”
efﬁcient, ﬂexible
use,
and
http://www.paddlepaddle.org/.
[66] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012.
[67] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten, “Densely
connected convolutional networks,” in CVPR, 2017.
VIII. MODEL ARCHITECTURES
A. The Original Model Architectures
Table X and Table XI show the originally DL model ar-
chitectures (Model 1) of MNIST and CIFAR-10, respectively.
For simplicity, the details of Layer 1, Layer 2 and Layer 3
layers’s details are summarized in Table XII.
ORIGINAL MODEL (MODEL 1) FOR MNIST
TABLE X
Layer Type MNIST Architecture
Relu Conv.
Relu Conv.
32 ﬁlters (3*3)
32 ﬁlters (3*3)
Max Pooling (2*2)
Relu Conv.
Relu Conv.
64 ﬁlters (3*3)
64 ﬁlters (3*3)
Max Pooling (2*2)
Flatten
Relu FC
Dropout
Relu FC
200 units
0.5
200 units
Softmax FC (10)
ORIGINAL MODEL (MODEL 1) FOR CIFAR-10
TABLE XI
(ResNet-20)
Layer Type CIFAR-10 Architecture
Layer 1:
Layer 2:
Layer 2:
Layer 2:
Layer 3:
Layer 2:
Layer 2:
Layer 3:
Layer 2:
Layer 2:
ﬁlters=16, strides=1
ﬁlters=16, strides=1
ﬁlters=16, strides=1
ﬁlters=16, strides=1
ﬁlters=32, strides=2
ﬁlters=32, strides=1
ﬁlters=32, strides=1
ﬁlters=64, strides=2
ﬁlters=64, strides=1
ﬁlters=64, strides=1
Average Pooling (8*8)
Flatten
Softmax FC (10)
TABLE XII
DETAILS OF L1, L2 AND L3 FOR RESNET
ﬁlters, strides
ﬁlters, kernel size=3, strides, kernel init=’he normal’,
kernel regularizer=l2(1e-4)
relu
ﬁlters, strides
ﬁlters, kernel size=3, strides, kernel init=’he normal’,
kernel regularizer=l2(1e-4)
ﬁlters, kernel size=3, strides=1, kernel init=’he normal’,
kernel regularizer=l2(1e-4)
relu
relu
ﬁlters, strides
Conv2D
ﬁlters, kernel size=3,
strides,
kernel init=’he normal’,
kernel regularizer=l2(1e-4)
ﬁlters, kernel size=1,
strides,
kernel init=’he normal’,
kernel regularizer=l2(1e-4)
ﬁlters, kernel size=3, strides=1, kernel init=’he normal’,
kernel regularizer=l2(1e-4)
relu
relu
Layer 1:
Conv2D
BN
Activation
Layer 2:
Conv2D
BN
Activation
Conv2D
BN
Activation
Layer 3:
Conv2D
BN
Activation
Conv2D
BN
Activation
B. Other Models for the Transferability Case Study
For Model 2, we add one convolution block to the original
model for MNIST and choose a similar ResNet-56 for CIFAR-
10. For Model 3, we use AlexNet [66] and DenseNet [67] in
MNIST and CIFAR-10, respectively.
IX. PARAMETER SETTINGS