### Optimized Text

**Accuracy Comparison:**
- The model achieves 57% accuracy across all attacks, which is comparable to the 58.4% accuracy of all defenses, as previously shown in Table V.
- Even for the best-defense ensemble, the improvement in accuracy over the most successful single defense (NAT) is not significant. Specifically, the average accuracy of the best-defense ensemble against all attacks is 84%, only marginally higher than the 82.2% achieved by NAT on CIFAR-10.
- This partially confirms the conclusion that an ensemble of multiple defenses does not guarantee superior performance [55].

**Ensemble Defense Capabilities:**
- On the other hand, ensembling multiple defenses can improve the lower bound of defense capability for individual models. According to the results, there are no extremely low classification accuracies for ensemble models.
- As analyzed in Section IV-C1, most individual defenses are effective against specific adversarial attacks but not all. For some individual defenses, the classification accuracy on certain attacks drops significantly below 10%, whereas this is not the case with ensembles.
- Even in the worst-case scenario of a completely random ensemble, the classification accuracy on CIFAR-10 remains above 20%.

**Remark 8:**
- For ensemble methods, we confirm that combining different defenses does not significantly enhance overall defensive capabilities [55]. However, it does improve the lower bound of defense ability for individual models.

### VI. DISCUSSION

**Limitations and Future Work:**
- **Limited Scope:** We integrated the most representative 16 adversarial attacks and 13 defenses. While DEEPSEC covers all categories of state-of-the-art attacks and defenses, it does not enumerate and implement all strategies due to methodological similarities. However, DEEPSEC's modular design and implementation make it easy for users to integrate new attacks, defenses, and corresponding utility metrics. Therefore, we have open-sourced DEEPSEC and encourage public contributions.
- **Parameter Settings:** Due to space limitations, we used one setting for each individual attack and defense. Exclusive parameters among different attacks or defenses were kept consistent with their original settings in the papers, while common parameters were standardized for fair comparison. Based on DEEPSEC, it is straightforward to extend evaluations to different settings.
- **Focus on Non-Adaptive and White-Box Attacks:** In the current implementation, we primarily focused on non-adaptive and white-box attacks. Nevertheless, DEEPSEC's modular and generic design allows for easy extension to support many other adversarial attacks. For example, adaptive attacks [9], [35] can be incorporated if the adversary has access to the deployed defense-enhanced model when generating adversarial examples; black-box attacks [33], [34] can be supported by restricting the attacks' access to only the input and output of deep learning models; and for unsupervised learning (e.g., generative models) [57], [58], we can disable the attacks' access to label information. DEEPSEC's modular implementation provides standard interfaces for accessing data and models, making such extensions readily implementable.

**Additional Related Work:**
- Several attack/defense platforms have been proposed, including Cleverhans [14], Foolbox [59], AdvBox [60], and ART [61].
- **Cleverhans:** The first open-source library, mainly using TensorFlow [62], providing implementations of 9 attacks and 1 simple adversarial training-based defense.
- **Foolbox:** Improves upon Cleverhans by interfacing with popular deep learning frameworks like PyTorch [45], Theano [63], and MXNet [64].
- **Advbox:** Implemented on PaddlePaddle [65] and includes 7 attacks.
- **ART:** Provides a library integrating 7 attacks and 5 defenses.
- **DEEPSEC Differentiation:**
  1. **Comprehensive Coverage:** Existing platforms provide a limited number of adversarial attacks and few defense methods. DEEPSEC incorporates 16 attacks and 13 defenses, covering all categories of state-of-the-art attacks and defenses.
  2. **Rich Implementation and Metrics:** In addition to a rich implementation of attacks and defenses, DEEPSEC treats evaluation metrics as first-class citizens, implementing 10 attack and 5 defense utility metrics to help assess given attacks/defenses.
  3. **Unique Analysis Platform:** Unlike platforms that solely provide reference implementations, DEEPSEC offers a unique analysis platform, enabling researchers and practitioners to conduct comprehensive and informative evaluations of attacks, defenses, and deep learning models.

### VII. CONCLUSION

- We designed, implemented, and evaluated DEEPSEC, a uniform security analysis platform for deep learning models. In its current implementation, DEEPSEC incorporates 16 state-of-the-art adversarial attacks with 10 attack utility metrics and 13 representative defenses with 5 defense utility metrics.
- To our knowledge, DEEPSEC is the first platform of its kind that supports uniform, comprehensive, informative, and extensible evaluation of adversarial attacks and defenses. Leveraging DEEPSEC, we conducted extensive evaluations of existing attacks and defenses, which helped answer a set of long-standing questions.
- We envision that DEEPSEC will serve as a useful benchmark to facilitate adversarial deep learning research.

### ACKNOWLEDGMENT

- We would like to thank our shepherd Christopher Kruegel and the anonymous reviewers for their valuable suggestions to improve this paper. We are also grateful to Xiaoyu Cao, Jacob Buckman, and Yang Song for sharing their code, and to Yuan Chen and Saizhuo Wang for helping open-source DEEPSEC.
- This work was partly supported by the National Key Research and Development Program of China under Nos. 2016YFB0800102 and 2016YFB0800201, the NSFC program under No. 61772466, the Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars under No. R19F020013, the Provincial Key Research and Development Program of Zhejiang, China under Nos. 2017C01055, 2017C01064, and 2018C03052, the Alibaba-ZJU Joint Research Institute of Frontier Technologies, the CCF-NSFOCUS Research Fund under No. CCF-NSFOCUS2017011, the CCF-Venustech Research Fund under No. CCF-VenustechRP2017009, and the Fundamental Research Funds for the Central Universities under No. 2016XZZX001-04. Ting Wang is partly supported by the National Science Foundation under Grant No. 1566526 and 1718787.

### REFERENCES

- [References listed as provided, with no changes needed.]

### VIII. MODEL ARCHITECTURES

**A. Original Model Architectures:**
- **MNIST Architecture (Model 1):**
  - Layer Type: ReLU Convolutional (32 filters, 3x3)
  - Layer Type: ReLU Convolutional (32 filters, 3x3)
  - Layer Type: Max Pooling (2x2)
  - Layer Type: ReLU Convolutional (64 filters, 3x3)
  - Layer Type: ReLU Convolutional (64 filters, 3x3)
  - Layer Type: Max Pooling (2x2)
  - Layer Type: Flatten
  - Layer Type: ReLU Fully Connected (200 units)
  - Layer Type: Dropout (0.5)
  - Layer Type: ReLU Fully Connected (200 units)
  - Layer Type: Softmax Fully Connected (10)

- **CIFAR-10 Architecture (ResNet-20, Model 1):**
  - Layer 1: Filters=16, Strides=1
  - Layer 2: Filters=16, Strides=1
  - Layer 2: Filters=16, Strides=1
  - Layer 2: Filters=16, Strides=1
  - Layer 3: Filters=32, Strides=2
  - Layer 2: Filters=32, Strides=1
  - Layer 2: Filters=32, Strides=1
  - Layer 3: Filters=64, Strides=2
  - Layer 2: Filters=64, Strides=1
  - Layer 2: Filters=64, Strides=1
  - Layer Type: Average Pooling (8x8)
  - Layer Type: Flatten
  - Layer Type: Softmax Fully Connected (10)

- **Details of L1, L2, and L3 for ResNet:**
  - **Layer 1:**
    - Conv2D (filters, kernel size=3, strides, kernel init='he normal', kernel regularizer=l2(1e-4))
    - Batch Normalization
    - Activation (ReLU)
  - **Layer 2:**
    - Conv2D (filters, kernel size=3, strides, kernel init='he normal', kernel regularizer=l2(1e-4))
    - Batch Normalization
    - Activation (ReLU)
    - Conv2D (filters, kernel size=3, strides=1, kernel init='he normal', kernel regularizer=l2(1e-4))
    - Batch Normalization
    - Activation (ReLU)
  - **Layer 3:**
    - Conv2D (filters, kernel size=3, strides, kernel init='he normal', kernel regularizer=l2(1e-4))
    - Conv2D (filters, kernel size=1, strides, kernel init='he normal', kernel regularizer=l2(1e-4))
    - Conv2D (filters, kernel size=3, strides=1, kernel init='he normal', kernel regularizer=l2(1e-4))
    - Batch Normalization
    - Activation (ReLU)
    - Activation (ReLU)

**B. Other Models for Transferability Case Study:**
- **Model 2:**
  - MNIST: Add one convolution block to the original model.
  - CIFAR-10: Use a similar ResNet-56.
- **Model 3:**
  - MNIST: Use AlexNet [66].
  - CIFAR-10: Use DenseNet [67].

### IX. PARAMETER SETTINGS

- [Details of parameter settings as provided, with no changes needed.]