		month_num                INTEGER,  
		year                     TEXT,  
		hour                     TEXT,  
		minute                   TEXT,  
		second                   TEXT,  
		timezone                 TEXT,  
		http_verb                TEXT,  
		uri                      TEXT,  
		http_status_code         TEXT,  
		bytes_returned           TEXT,  
		referrer                 TEXT,  
		user_agent               TEXT  
	)  
	SERVER hdfs_server  
	OPTIONS (dbname 'default', table_name 'weblogs');  
-- select from table  
SELECT DISTINCT client_ip IP, count(*)  
	FROM weblogs GROUP BY IP HAVING count(*) > 5000 ORDER BY 1;  
       ip        | count   
-----------------+-------  
 13.53.52.13     |  5494  
 14.323.74.653   | 16194  
 322.6.648.325   | 13242  
 325.87.75.336   |  6500  
 325.87.75.36    |  6498  
 361.631.17.30   | 64979  
 363.652.18.65   | 10561  
 683.615.622.618 | 13505  
(8 rows)  
-- EXPLAIN output showing WHERE clause being pushed down to remote server.  
EXPLAIN (VERBOSE, COSTS OFF)  
	SELECT client_ip, full_request_date, uri FROM weblogs  
	WHERE http_status_code = 200;  
                                                   QUERY PLAN                                                     
----------------------------------------------------------------------------------------------------------------  
 Foreign Scan on public.weblogs  
   Output: client_ip, full_request_date, uri  
   Remote SQL: SELECT client_ip, full_request_date, uri FROM default.weblogs WHERE ((http_status_code = '200'))  
(3 rows)  
```  
Using HDFS FDW with Apache Spark on top of Hadoop  
-----  
Step 1: Download & install Apache Spark in local mode.  
Step 2: In the folder ``$SPARK_HOME/conf`` create a file  
``spark-defaults.conf`` containing the following line  
```sql  
spark.sql.warehouse.dir hdfs://localhost:9000/user/hive/warehouse  
```  
By default spark uses derby for both meta data and the data itself  
(called warehouse in spark). In order to have spark use hadoop as  
warehouse we have to add this property.  
Step 3: Start Spark Thrift Server  
```sql  
./start-thriftserver.sh  
```  
Step 4: Make sure Spark thrift server is running using log file  
Step 5: Create a local file names.txt with below data:  
```sh  
$ cat /tmp/names.txt  
1,abcd  
2,pqrs  
3,wxyz  
4,a_b_c  
5,p_q_r  
,  
```  
Step 6: Connect to Spark Thrift Server2 using spark beeline client.  
e.g.  
```sh  
$ beeline  
Beeline version 1.2.1.spark2 by Apache Hive  
beeline> !connect jdbc:hive2://localhost:10000/default;auth=noSasl org.apache.hive.jdbc.HiveDriver  
```  
Step 7: Getting the sample data ready on spark:  
Run the following commands in beeline command line tool:-  
```sql  
./beeline  
Beeline version 1.2.1.spark2 by Apache Hive  
beeline> !connect jdbc:hive2://localhost:10000/default;auth=noSasl org.apache.hive.jdbc.HiveDriver  
Connecting to jdbc:hive2://localhost:10000/default;auth=noSasl  
Enter password for jdbc:hive2://localhost:10000/default;auth=noSasl:   
Connected to: Spark SQL (version 2.1.1)  
Driver: Hive JDBC (version 1.2.1.spark2)  
Transaction isolation: TRANSACTION_REPEATABLE_READ  
0: jdbc:hive2://localhost:10000> create database my_test_db;  
+---------+--+  
| Result  |  
+---------+--+  
+---------+--+  
No rows selected (0.379 seconds)  
0: jdbc:hive2://localhost:10000> use my_test_db;  
+---------+--+  
| Result  |  
+---------+--+  
+---------+--+  
No rows selected (0.03 seconds)  
0: jdbc:hive2://localhost:10000> create table my_names_tab(a int, name string)  
                                 row format delimited fields terminated by ' ';  
+---------+--+  
| Result  |  
+---------+--+  
+---------+--+  
No rows selected (0.11 seconds)  
0: jdbc:hive2://localhost:10000>  
0: jdbc:hive2://localhost:10000> load data local inpath '/tmp/names.txt'  
                                 into table my_names_tab;  
+---------+--+  
| Result  |  
+---------+--+  
+---------+--+  
No rows selected (0.33 seconds)  
0: jdbc:hive2://localhost:10000> select * from my_names_tab;  
+-------+---------+--+  
|   a   |  name   |  
+-------+---------+--+  
| 1     | abcd    |  
| 2     | pqrs    |  
| 3     | wxyz    |  
| 4     | a_b_c   |  
| 5     | p_q_r   |  
| NULL  | NULL    |  
+-------+---------+--+  
```  
Here are the corresponding files in hadoop:  
```sql  
$ hadoop fs -ls /user/hive/warehouse/  
Found 1 items  
drwxrwxrwx   - org.apache.hive.jdbc.HiveDriver supergroup          0 2020-06-12 17:03 /user/hive/warehouse/my_test_db.db  
$ hadoop fs -ls /user/hive/warehouse/my_test_db.db/  
Found 1 items  
drwxrwxrwx   - org.apache.hive.jdbc.HiveDriver supergroup          0 2020-06-12 17:03 /user/hive/warehouse/my_test_db.db/my_names_tab  
```  
Step 8: Access data from PostgreSQL:  
Connect to Postgres using psql:  
```sql  
-- set the GUC variables appropriately, e.g. :  
hdfs_fdw.jvmpath='/home/edb/Projects/hadoop_fdw/jdk1.8.0_111/jre/lib/amd64/server/'  
hdfs_fdw.classpath='/usr/local/edbas/lib/postgresql/HiveJdbcClient-1.0.jar:  
                    /home/edb/Projects/hadoop_fdw/hadoop/share/hadoop/common/hadoop-common-2.6.4.jar:  
                    /home/edb/Projects/hadoop_fdw/apache-hive-1.0.1-bin/lib/hive-jdbc-1.0.1-standalone.jar'  
-- load extension first time after install  
CREATE EXTENSION hdfs_fdw;  
-- create server object  
CREATE SERVER hdfs_server  
	FOREIGN DATA WRAPPER hdfs_fdw  
	OPTIONS (host '127.0.0.1', port '10000', client_type 'spark', auth_type 'NOSASL');  
-- create user mapping  
CREATE USER MAPPING FOR postgres  
	SERVER hdfs_server OPTIONS (username 'spark_username', password 'spark_password');  
-- create foreign table  
CREATE FOREIGN TABLE f_names_tab( a int, name varchar(255)) SERVER hdfs_svr  
	OPTIONS (dbname 'testdb', table_name 'my_names_tab');  
-- select the data from foreign server  
SELECT * FROM f_names_tab;  
 a |  name   
---+--------  
 1 | abcd  
 2 | pqrs  
 3 | wxyz  
 4 | a_b_c  
 5 | p_q_r  
 0 |  
(6 rows)  
-- EXPLAIN output showing WHERE clause being pushed down to remote server.  
EXPLAIN (verbose, costs off)  
	SELECT name FROM f_names_tab  
	WHERE a > 3;  
                                QUERY PLAN                                  
--------------------------------------------------------------------------  
 Foreign Scan on public.f_names_tab  
   Output: name  
   Remote SQL: SELECT name FROM my_test_db.my_names_tab WHERE ((a > '3'))  
(3 rows)  
```  
Please note that we are using the same port while creating foreign  
server because Spark Thrift Server is compatible with Hive Thrift  
Server. Applications using Hiveserver2 would work with Spark except  
for the behaviour of ANALYZE command and the connection string in case  
of NOSASL. It is better to use ALTER SERVER and change the client_type  
option if Hive is to be replaced with Spark.  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
#### [PolarDB 云原生分布式开源数据库](https://github.com/ApsaraDB "57258f76c37864c6e6d23383d05714ea")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、内核开发公开课、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")