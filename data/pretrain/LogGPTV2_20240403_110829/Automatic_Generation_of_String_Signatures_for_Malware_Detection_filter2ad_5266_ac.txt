is less than or equal to K.
After S2 is determined, Hancock ﬁnalizes the K-component MCS for each
malware ﬁle considered covered, i.e., whose coverage count is no smaller than
K. To do so, Hancock ﬁrst checks each component signature in S2 against a
goodware database, and marks it as an FP if it matches some goodware ﬁle
in the database. Then Hancock considers all possible K-component MCSs for
each malware ﬁle and chooses the one with the smallest number of components
that are an FP. If the number of FP components in the chosen MCS is higher
than a threshold, TF P , the MCS is deemed as unusable and the malware ﬁle is
considered not covered. Empirically, T is chosen to be 1 or 2. After each malware
ﬁle’s MCS is determined, Hancock applies the same diversity principle to each
MCS based on the malware ﬁles it covers.
Automatic Generation of String Signatures for Malware Detection
113
6
Evaluation
6.1 Methodology
To evaluate the overall eﬀectiveness of Hancock, we used it to generate 48-
byte string signatures for two sets of malware ﬁles, and use the coverage and
number of false positives of these signatures as the performance metrics. The
ﬁrst malware set has 2,363 unpacked ﬁles that Symantec gathered in August
2008. The other has 46,288 unpacked ﬁles (or 112,156 ﬁles before unpacking)
gathered in 2007-2008. The goodware model used in initial signature candidate
ﬁltering is derived from a 31-Gbyte goodware training set. In addition, we used
another 1.8-Gbyte goodware set to ﬁlter out FP-prone signature candidates. To
determine which signatures are FPs, we tested each generated signature against
a 213-Gbyte goodware set. The machine used to perform these experiments has
four quad-core 1.98-GHz AMD Opteron processors and 128 Gbytes of RAM.
6.2 Single-Component Signatures
Because almost every signature candidate selection and ﬁltering technique in
Hancock comes with an empirical threshold parameter, it is impossible to present
results corresponding to all possible combinations of these parameters. Instead,
we present results corresponding to three representative settings, which are
shown in Table 1 and called Loose, Normal and Strict. The generated signa-
tures cover overlapping sets of malware ﬁles.
To gain additional assurance that Hancock’s FP rate was low enough, Syman-
tec’s malware analysts wanted to see not only zero false positives, but also that
the signatures look good – they look like they encode non-generic behavior that
is unlikely to show up in goodware. To that end, we manually ranked signatures
on the August 2008 malware set as good, poor, and bad.
To get a rough indication of the maximum possible coverage, the last lines
in tables 2 and 3 show the coverage of all non-FP candidate signatures. The
probability-based and dissassembly-based heuristics were still enabled with
Loose threshold settings.
These results show not only that Hancock has a low FP rate, but also that
tighter thresholds can produce signatures that look less generic. Unfortunately,
it can only produce signatures to cover a small fraction of the speciﬁed malware.
Several factors limit Hancock’s coverage:
– Hancock’s packer detection might be insuﬃcient. PEiD recognizes many
packers, but by no means all of them. Entropy detection can also be fooled:
Table 1. Heuristic threshold settings
probability ratio deviation signatures
Threshold Model Group Position # common Interestingness Minimum
setting
coverage
Loose
Normal
Strict
4000
3000
3000
0.35
0.35
0.35
-90
-90
-90
1
1
2
13
14
17
3
4
4
114
K. Griﬃn et al.
Table 2. Results for August 2008 data
Table 3. Results for 2007-8 data
Threshold Cover- # Good Poor Bad
setting
age FPs sig.s sig.s sig.s
15.7% 0
Loose
14.0% 0
Normal
Strict
11.7% 0
All non-FP 22.6% 0
6
6
6
10
7
2
0
11
1
0
0
9
Threshold Coverage Sig.s FPs
Loose
Normal
Normal, pos.
deviation 1000
Strict
All non-FP
14.1% 1650
767
11.7%
11.3%
715
4.4%
206
31.7% 7305
7
2
0
0
0
some packers do not compress the original ﬁle’s data, but only obfuscate
it. Diversity-based heuristics will probably reject most candidate signatures
extracted from packed ﬁles. (Automatically generating signatures for packed
ﬁles would be bad, anyway, since they would be signatures on packer code.)
– Hancock works best when the malware set has many malware families and
many ﬁles in each malware family. It needs many families so that diversity-
based heuristics can identify generic or rare library code that shows up in
several malware families. It needs many ﬁles in each family so that diversity-
based heuristics can identify which candidate signatures really are charac-
teristic of a malware family. If the malware sets have many malware families
with only a few ﬁles each, this would lower Hancock’s coverage.
– Malware polymorphism hampers Hancock’s eﬀectiveness. If only some code
is polymorphic, Hancock can still identify high coverage signatures in the
remaining code. If the polymorphic code has a relatively small number of
variations, Hancock can still identify several signatures with moderate cov-
erage that cover most ﬁles in the malware family. If all code is polymorphic,
with a high degree of variation, Hancock will cover very few of the ﬁles.
– Finally, the extremely stringent FP requirement means setting heuristics to
very conservative thresholds. Although the heuristics have good discrimina-
tion power, they still eliminate many good signatures. e.g. The group count
heuristic clusters malware into families based on a single-byte histogram.
This splits most malware families into several groups, with large malware
families producing a large number of groups. An ideal signature for this
family will occur in all of those groups. Thus, for the sake of overall discrim-
ination power, the group count heuristic will reject all such ideal signatures.
Sensitivity Study. A heuristic’s discrimination power is a measure of its ef-
fectiveness. A heuristic has good discrimination power if the fraction of false
positive signatures that it eliminates is higher than the fraction of true positive
signatures it eliminates. These results depend strongly on which other heuristics
are in use. We tested heuristics in two scenarios: we measured their raw discrim-
ination power with other heuristics disabled; and we measured their marginal
discrimination power with other heuristics enabled with conservative thresholds.
First, using the August 2008 malware set, we tested the raw discrimination
power of each heuristic. Table 4 shows the baseline setting, more conservative
Automatic Generation of String Signatures for Malware Detection
115
Table 4. Raw Discrimination Power
Table 5. Marginal Discrimination Power
2.4% 74.0% 12
6.0% 83.3% 15
FPs Cov. DP
41.7% 96.6% 25
Heuristic
Max pos. deviation
(from ∞ to 8,000)
Min ﬁle coverage
(from 3 to 4)
Group ratio
(from 1.0 to .6)
Model log probability 51.2% 73.7% 2.2
(from -80 to -100)
Code interestingness
(from 13 to 15)
Multiple common sig.s 91.7% 70.2% 0.2
(from 1 to 2)
Universal FLIRT
Library function
reference
Address space
33.1% 71.7% 3.3
46.4% 75.7% 2.8
58.3% 78.2% 2.2
30.4% 70.8% 3.5
2
Heuristic
Max pos. deviation
(from 3,000 to ∞)
Min ﬁle coverage
(from 4 to 3)
Group ratio
(from 0.35 to 1)
Model log probability 1
(from -90 to -80)
Code interestingness
(from 17 to 13)
Multiple common
sig.s (from 2 to 1)
Universal FLIRT
Library function
reference
Address space
3
4
2
0
3
FPs Coverage
10
121%
16
126%
162%
123%
226%
189%
106%
108%
109%
setting, and discrimination power for each heuristic. The library heuristics (Uni-
versal FLIRT, library function reference, and address space) are enabled for the
baseline test and disabled to test their own discrimination powers. Using all
baseline settings, the run covered 551 malware ﬁles with 220 signatures and 84
false positives. Discrimination power is calculated as log FPsi
FPsf
log Coveragei
Coveragef
Table 4 shows most of these heuristics to be quite eﬀective. Position deviation
and group ratio have excellent discrimination power (DP); the former lowers
coverage very little and the latter eliminates almost all false positives. Model
probability and code interestingness showed lower DP because their baseline
settings were already somewhat conservative. Had we disabled these heuristics
entirely, the baseline results would have been so overwhelmed with false positives
as to be meaningless. All four of these heuristics are very eﬀective.
(cid:2)
Increasing the minimum number of malware ﬁles a signature must cover elim-
inates many marginal signatures. The main reason is that, for lower coverage
numbers, there are so many more candidate signatures that some bad ones will
get through. Raising the minimum coverage can have a bigger impact in combi-
nation with diversity-based heuristics, because those heuristics work better with
more ﬁles to analyze.
.
Requiring two common signatures eliminated more good signatures than false
positive signatures. It actually made the signatures, on average, worse.
Finally, the library heuristics all work fairly well. They each eliminate 50% to
70% of false positives while reducing coverage less than 30%. In the test for each
library heuristic, the other two library heuristics and basic FLIRT functionality
were still enabled. This shows that none of these library heuristics are redundant
and that these heuristics go signiﬁcantly beyond what FLIRT can do.
116
K. Griﬃn et al.
Marginal Contribution of Each Technique. Then we tested the eﬀective-
ness of each heuristic when other heuristics were set to the Strict thresholds
from table 1. We tested the tunable heuristics with the 2007-8 malware set with
Strict baseline threshold settings from table 1. Testing library heuristics was
more computationally intensive (requiring that we reprocess the malware set),
so we tested them on August 2008 data with baseline Loose threshold settings.
Since both sets of baseline settings yield zero FPs, we decreased each heuristic’s
threshold (or disabled it) to see how many FPs its conservative setting elimi-