title:Draco: Statistical diagnosis of chronic problems in large distributed
systems
author:Soila Kavulya and
Scott Daniels and
Kaustubh R. Joshi and
Matti A. Hiltunen and
Rajeev Gandhi and
Priya Narasimhan
Draco: Statistical Diagnosis of Chronic Problems in
Large Distributed Systems
Soila P. Kavulya†, Scott Daniels§, Kaustubh Joshi§, Matti Hiltunen§, Rajeev Gandhi†, Priya Narasimhan†
Carnegie Mellon University†, AT&T Labs — Research§
{spertet,rgandhi,priyan}@ece.cmu.edu, {daniels,kaustubh,hiltunen}@research.att.com
Abstract—Chronics are recurrent problems that often ﬂy under
the radar of operations teams because they do not affect enough
users or service invocations to set off alarm thresholds. In
contrast with major outages that are rare, often have a single
cause, and as a result are relatively easy to detect and diagnose
quickly, chronic problems are elusive because they are often
triggered by complex conditions, persist in a system for days
or weeks, and coexist with other problems active at the same
time. In this paper, we present Draco, a scalable engine to
diagnose chronics that addresses these issues by using a “top-
down” approach that starts by heuristically identifying user
interactions that are likely to have failed, e.g., dropped calls, and
drills down to identify groups of properties that best explain the
difference between failed and successful interactions by using a
scalable Bayesian learner. We have deployed Draco in production
for the VoIP operations of a major ISP. In addition to providing
examples of chronics that Draco has helped identify, we show
via a comprehensive evaluation on production data that Draco
provided 97% coverage, had fewer than 4% false positives, and
outperformed state-of-the-art diagnostic techniques by up to 56%
for complex chronics.
I. INTRODUCTION
The evolution of large distributed systems into entire plat-
forms that provide dozens of distinct services to millions of
users requires rethinking classic notions of availability as a
binary property. Systems at such scales are rarely simply “up”
or “down”; even when they are working for an overwhelming
majority of users, there are almost always multiple ongoing
problems of different types that affect small subsets of users.
Often, the symptoms of each individual problem are not big
enough to trigger alarm thresholds, and thus they ﬂy under
the radar of operations teams that are geared towards major
outages. We call such problems chronics—small problems
that persist
in large distributed systems for days or even
weeks before they are detected (often as a result of customer
complaints). Chronics can occur repeatedly but unpredictably
for short durations of time, or persist, affecting small subsets
of users all the time. Nevertheless, they can cumulatively
contribute signiﬁcantly to the degradation of user experience.
For example, data we obtained from the Voice over IP (VoIP)
platform of a major ISP revealed that even in the worst month
for major outages, the number of calls affected (dropped or
blocked) due to major outages was only 30% higher than the
number of calls impacted by chronics.
The discovery and diagnosis of never-before seen chronics
in platforms comprising thousands of network, server, and user
elements poses new challenges compared to the diagnosis of
major system outages. Threshold-based techniques [4], [6],
[15] do not work well because lowering thresholds to detect
chronics often increases the number of false positives. Long-
running persistent chronics can get absorbed into a system’s
deﬁnition of “normal”,
thus posing problems for methods
based on historical models [9] or change-point detection [1].
Isolating individual problems is also more difﬁcult—due to
their persistent nature, lots of chronics are often present in a
system at once, all starting and ending at different times, with
larger problems hiding smaller ones. Furthermore, they occur
even when the system works well for most users, and cannot
be diagnosed by isolating the system’s execution into periods
of “good” and “bad” behavior [17], [20]. Finally, chronics
often involve some unexpected combination of corner-cases
that impact only small subsets of users, e.g., a conﬁguration
error that impacts only those users with a particular version
of a software stack, or a performance degradation that occurs
only when the load on a particular server temporarily increases
beyond a certain threshold.
To address these challenges, we present a new system called
Draco1 that can perform statistical diagnosis of chronics on
large systems by combining data logs from different sources,
and by diagnosing multiple ongoing problems—each identiﬁed
by complex signatures across multiple dimensions. Draco can
handle both discrete and real-valued data, and is threshold-free
to allow detection of even small problems. To enable discovery
of problems that have never been seen before or those that
have persisted in the background for a long time, Draco does
not rely on historical data. Draco minimizes false positives by
using a “top-down” approach that relies on a scalable Bayesian
distribution learner and an information-theoretic measure of
distance (KL distances) [12] to identify sets of “problem
signatures” that together explain the differences between the
failed and successful user interactions.
Draco’s core engine is scalable and domain-agnostic—
requiring only changes to some well-isolated data parsers to
be adapted to other applications. It is currently in production
use by the operations team of a major US-based provider’s
VoIP platform that handles tens of millions calls per day. The
problems we address, and our solutions, are not limited to
VoIP. They are likely to be applicable to many other large
platforms (e.g., e-commerce, web-search, social networks) that
serve users via independent interactions such as web requests.
1Draco is a genus of gliding lizards from Southeast Asia.
The contributions of this paper include the core algorithms
for chronics diagnosis, Draco’s scalable design, and a com-
prehensive evaluation of it’s speed and accuracy. We evaluate
Draco’s quality of diagnosis in two ways: 1) through fault
injection experiments that use real logs from our VoIP de-
ployment, but inject a variety of precisely controlled synthetic
failures so that ground truth is known, and 2) by cataloguing
actual incidents on the VoIP network that Draco was able to
identify, and which were subsequently conﬁrmed by network
operations personnel.
This paper extends our earlier work [10] by handling
both discrete and real-valued data. In addition,
this paper
includes a comprehensive evaluation showing that Draco is
able to quickly identify the attributes which are indicative
of failure with a high level of up to 97% coverage, while
maintaining a low level of less than 4% false positives.
Draco also outperforms state-of-the-art diagnostic techniques
that rely on decision trees [11], [20] by up to 56% when
diagnosing complex problems involving multiple attributes
while still providing near-interactive performance (< 1 second
per problem) on large datasets.
The paper is organized as follows: Section II provides a brief
background on VoIP networks and describes the VoIP dataset.
Sections III and IV discuss the design and implementation
of our diagnostic tool. Section V presents the results of our
fault injection experiments, while Section VI presents case
studies from production use where Draco helped in identifying
the root-causes of chronic problems. Finally, Section VII
compares Draco to related work.
II. CHRONICS IN TELECOM SYSTEMS
As of December 2010, 31 percent of the more than 87
million residential telephone subscriptions in the United States
were provided by interconnected VoIP providers. In addition,
approximately 31 percent of residential wireline 9-1-1 calls
were made using VoIP services, making the availability of
VoIP infrastructure critical [8].
We investigate chronics discovery for a part of the VoIP
operations of a major US-based ISP. The portion of the ISP’s
VoIP network that we analyzed handles tens of millions of
calls each day, contains several hundred network elements,
and is layered on a large IP backbone. The network offers
a portfolio of voice services including individual accounts,
self-managed solutions where customers manage their own
premise equipment (PBXs), and wholesale customers who
buy network minutes in bulk and resell them. Calls traverse
through combinations of network elements such as VoIP gate-
ways (IPBEs), traditional phone gateways (GSXs), accounting
servers, application servers, voicemail servers, and policy
servers (PSX). Many of these are built by different vendors
and have different log ﬁle formats.
To satisfy the high availability requirements of the system,
there are real-time operations teams that monitor both low-
level alarms derived from the equipment (server and network
errors, CPU/memory/network utilization counters, etc.), as
well as end-to-end indicators such as customer complaints
Fig. 1. Draco identiﬁed multiple ongoing problems that affected calls passing
through the same network element at a production VoIP system.
and output from automated test call systems. Codebook-based
systems [22] that are driven by signatures of known problems
are used for identifying related alarms and for diagnosis. Major
outages often result in immediate impact on successful call
volumes, alarms from many sources, and are usually detected
and resolved quickly.
Despite such robust operations support, the system always
has a number of call defects occurring at any time of the day
in the form of “background noise”. Measured in defects per
million (DPM), they represent only a small fraction of the
calls at any given time, but left unchecked, they can add up
quickly over weeks and months. A separate chronics team
troubleshoots these defects, but diagnosis is still a largely
manual process. We seek to provide tools that can help such
chronics teams to quickly discover low-grade problems that
are hidden in the background noise.
A. Challenges in Diagnosing Chronics
We illustrate these challenges using examples of real in-
cidents diagnosed using Draco in a production system [10].
Figure 1 illustrates actual instances of chronic problems in
the service provider’s logs that were discovered using our tool.
In the ﬁrst incident, the recurrent increase in defects during
night hours was traced to two different business customers,
who were attempting to send faxes overseas using unsupported
codecs during US night
time. In the second incident, an
independent problem with a speciﬁc network element arose
and persisted until the network element was reset. Figure 2
illustrates a persistent chronic problem due to two blocked
CICs (Circuit Identiﬁcation Codes) on the trunk group that
affected calls assigned to these blocked CICs in a round robin
manner. At peak, 2–3% of the calls passing this trunk group
would fail. After those CICs were unblocked, the total defects
associated with this error code were reduced by 80%.
Incident 1: Persistent problem, complex triggerFailed calls fortwo customersDay1Day2Day3Day4Day5Day6Chronic nightly problem due to unsupported fax codecCustomers stop usingunsupported fax codecIncident 2: Multiple problems existFailed calls fornetwork elementDay1Day2Day3Day4Day5Day6An unrelated chronic serverproblem emergesProblematic serverreset1) serve users via independent interactions, 2) log end-to-end
traces of user requests, and, 3) label each user interaction as
successful or failed.
The diagnosis algorithm proceeds in four steps. First, we
label user requests such as phone call attempts as successful
or failed, and extract system-level information associated with
each call from the server logs. This step is the only domain-
speciﬁc activity—all other steps are domain-agnostic. Second,
we compute an anomaly score for each attribute using a
standard information-theoretic metric that represents the “dif-
ference” between the success and failure attribute occurrence
probability distributions, as shown in Figure 3. Third, we use
a scalable ranking function to identify groups of attributes
that best discriminate between the success and failure labels,
as shown in Figure 4. Fourth, we examine the performance
logs of any network elements indicted during the third step,
and apply a similar ranking function to identify anomalous
performance metrics, such as high CPU or memory usage.
We describe each step of the approach in more detail below.
A. Labels and Attribute Extraction
We examined logs from the VoIP network over a period
of six months. These include call detail record (CDR) logs
that are generated locally by network elements for each call
that passes through them. The logs often contain hundreds
of attributes that specify details of the call such as the caller
and callee information. The structure and semantics of these
records are vendor-speciﬁc. These logs tend to be large—the
average size of the raw CDR logs is 30GB/day. Even after
signiﬁcant consolidation to eliminate irrelevant data ﬁelds, the
average size is 2.4GB/day, and each log contains between
5000–10000 unique call attributes pertinent to diagnosis, i.e.,
attributes that appear in defective calls. In addition to CDR
logs, we also obtained performance logs collected by the
physical hosts on which the network elements run.
We start from user-visible symptoms of a problem, i.e.,
failed attempts to make a phone call. Labeling of user inter-
actions into success and failure interactions is easy if logs at
the user end device are available. However, if only logs from
network elements are available as in our case, domain-speciﬁc
heuristics will often be required. For phone calls, a user
redialing the same number immediately after disconnection,
zero talk time, or server reported error code can be used as
the failure indicator. In other systems, similar heuristics could
work too, e.g., a user repeatedly refreshing a web page, or
getting a HTTP error when accessing a page. Since these
labels are used for subsequent statistical analysis, occasional
mislabeling can be tolerated. We then correlate the lower-
level system log data extracted from the raw CDRs with these
user-level events (phone calls) to construct a “master record”
that represents the consolidated end-to-end trace. The log data
must have some common keys such as time, phone numbers,
and IP addresses that can be used to correlate the data with
the user-level event. However, the matches need not be exact,
and domain-speciﬁc matching rules can be used. For example,
entries may belong to the same call if the sender and receiver
Fig. 2. Chronic problem at production system persists for several weeks
making it difﬁcult them to detect using change-points.
These incidents highlight the challenges faced when diag-
nosing chronics namely:
1) Chronics ﬂy under the radar. Chronics occur sporad-
ically, or affect a small subset customers, and thus may
not trigger any threshold-based alarms. In the incidents
shown in Figures 1 and 2, the defect rate observed by
the customers was a fraction of one percent. Setting
thresholds to detect these problems is notoriously dif-
ﬁcult because lowering the thresholds to detect chronics
would increase the number of spurious alarms.
2) Persistent Problems. Some problems, occur only for
short durations of time, and could be discovered by
change-detection algorithms. However, other problems
persist for long periods of time as shown in Figure 2.
Algorithms that rely on change-point detection meth-
ods [1] or those that rely on historical models [9] would
fail to detect these problems.
3) Multiple independent problems. Because chronics of-
ten persist for long periods of time before they are
discovered, there are usually many of them ongoing at
the same time. Figure 1 illustrates how Draco identiﬁed
multiple ongoing problems that affected calls passing
through the same network element—one related to two
different business customers, and one related to the
network element
4) Complex triggers Chronics often involve only a small
subset of user interactions because they are triggered by
some unforeseen corner case arising due to a combina-
tion of factors. For example, certain chronics arise due
to a conﬂict between the conﬁguration at the customer’s
premises and the ISP’s server. To effectively debug
these problems, operators need to know both the server
conﬁguration, and the subset of customers affected.
III. DIAGNOSIS ALGORITHM
Draco uses a “top-down” approach to localize problems by
starting with user-visible symptoms of a problem, i.e., failed
calls, and drilling down to identify groups of attributes that
are the most highly indicative of the symptoms. We explain
Draco’s use in the context of a large telecommunication
system. However, Draco can be used to diagnose chronics
in other large distributed systems, such as e-commerce, web-
search, social networks, that have the following characteristics:
Incident 3: Persistent problem at trunk groupFailed calls fortrunk groupDay1Day5Day10Day15Day20Day25Chronic problem accounts for2-3% of failures at trunk groupProblem resolvedlearned, the score is simply the KL divergence [12], a standard
information theoretic metric of the “difference” between two