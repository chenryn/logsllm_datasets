ity vector v for imports. The rows of X contain the
imports of a certain component as a binary vector:
xik is 1 if component i imports import k. The vulner-
ability vector contains the number of vulnerability-
related bug reports for that component.
be posed as machine-learning problems. In machine learn-
ing, a parameterized function f , called a model, is trained
using training data X and y, so that we predict ˆy = f (X).
The parameters of f are usually chosen such that some mea-
sure of diﬀerence between y and ˆy is minimized. The ques-
tion, “Is this component vulnerable?” is called classiﬁca-
tion, and “Is this component more or less vulnerable than
another component?” can be answered with regression: by
predicting the number of vulnerabilities and then ranking
the components accordingly.
In our case, X would be the project’s feature matrix, and
y would be the vulnerability vector v. We now train a model
and use it to predict for a new component x(cid:2)
. If it classiﬁes
x(cid:2)
has features that were
associated with vulnerabilities in other components.
as vulnerable, this means that x(cid:2)
For our model f , we chose support vector machines [36]
(SVMs) over other models such as k-nearest-neighbors [13,
Chapter 13] because they have a number of advantages. For
example, when used for classiﬁcation, SVMs cope well with
data that is not linearly separable. They are also much less
prone to overﬁtting than other machine-learning methods.5
5.1 Validation Setup
To test how good our particular set of features work as
predictors for vulnerabilities, we simply split our feature ma-
trix to train and to assess the model. For this purpose, we
randomly select a number of rows from X and the corre-
sponding elements from v—collectively called the training
set—and use this data to train f . Then we use the left-over
rows from X and elements from y—the validation set—to
predict whether the corresponding components are vulner-
able and to compare the computed prediction with what
we already know from the bug database. It is usually rec-
ommended that the training set be twice as large as the
validation set, and we are following that recommendation.
We are not using a dedicated test set because we will not
5Two sets of n-dimensional points are said to be linearly
separable if there exists an n − 1-dimensional hyperplane
that separates the two sets. Overﬁtting occurs when the
estimation error in the training data goes down, but the
estimation error in the validation data goes up.
Actually has
vulnerability reports
yes
no
Predicted to have
vulnerability reports
yes
no
True Positive (TP)
False Positive (FP)
Precision
False Negative (FN)
True Negative (TN)
Recall
Figure 7: Precision and recall explained. Precision
is TP/(TP + FP ); recall is TP/(TP + FN ).
be selecting a single model, but will instead be looking at
the statistical properties of many models and will thus not
tend to underestimate the test error of any single model [13,
Chapter 7].
One caveat is that the training and validation sets might
not contain vulnerable and neutral components in the right
proportions. This can happen when there are so few vul-
nerable components that pure random splitting would pro-
duce a great variance in the number of vulnerable compo-
nents in diﬀerent splits. We solved this problem by stratiﬁed
sampling, which samples vulnerable and neutral components
separately to ensure the proper proportions.
5.2 Evaluating Classiﬁcation
For classiﬁcation, we can now compare the predicted val-
ues ˆv with the actual values v and count how many times
our prediction was correct. This gives rise to the measures
of precision and recall, as shown in Figure 7:
• The precision measures how many of the components
predicted as vulnerable actually have shown to be vul-
nerable. A high precision means a low number of false
positives; for our purposes, the predictor is eﬃcient.
• The recall measures how many of the vulnerable com-
ponents are actually predicted as such. A high recall
means a low number of false negatives; for our pur-
poses, the predictor is eﬀective.
In order to assess the quality of our predictions, consider a
simple cost model.6 Assume that we have a “testing budget”
of T units. Each component out of m total components is
either vulnerable or not vulnerable, but up front we do not
know which is which. Let us say there are V vulnerabilities
distributed arbitrarily among the m components and that
if we spend 1 unit on a component, we determine for sure
whether the component is vulnerable or not.
In a typical
software project, both V and T would be much less than m.
If we ﬁx T , m, and V , and if we have no other informa-
tion about the components, the optimal strategy for assign-
ing units to components is simply to choose components at
random.
In this case, the expected return on investment
would be T V /m: we test T components at random, and the
fraction of vulnerable components is V /m.
Now assume that we have a predictive method with pre-
cision p and that we spend our T units only on components
that have been ﬂagged as vulnerable by the method. In this
case, the expected return on investment is T p because the
6This was suggested to us by the anonymous reviewers.
fraction of vulnerable components among the ﬂagged com-
ponents is p. If p > V /m, the predictive method does better
than random assignment. In practice, we estimate V by the
number of components already known to have vulnerabili-
ties, V (cid:2)
5.3 Evaluating Ranking
, so we will want p to be much larger then V (cid:2)/m.
When we use a regression model, we predict the number of
vulnerabilities in a component. One standard action based
on this prediction would be allocating quality assurance ef-
forts: As a manager, we would spend most resources (such
as testing, reviewing, etc.) on those components which are
the most likely to be vulnerable. With a prediction method
that estimates the number of vulnerabilities in a component,
we would examine components of ˆv in decreasing order of
predicted vulnerabilities.
Usually, the quality of such rankings is evaluated using
Spearman’s rank correlation coeﬃcient. This is a number
between −1 and 1 which says how well the orderings in two
vectors agree. Values near 1 mean high correlation (if the
values in one vector go up, then so do the values in the other
vector), values near 0 mean no correlation, and values near
−1 mean negative correlation (if the values in one vector go
up, the values in the other vector go down).
However, this measure is inappropriate within the simple
cost model from above. Suppose that we can spend T units
on testing. In the best possible case, our ranking predicts
the actual top T most vulnerable components in the top T
slots. The relative order of these components doesn’t matter
because we will eventually ﬁx all top T components: while
high correlation coeﬃcients mean good rankings, and while
bad rankings will produce correlation coeﬃcients near 0, the
converse is not true.
P
P
Instead, we extend our simple cost model as follows. Let
p = (p1, . . . , pm) be a permutation of 1, . . . , m such that
ˆvp = (ˆvp1 , . . . , ˆvpm ) is sorted in descending order (that is,
ˆvpj ≥ ˆvpk for 1 ≤ j < k ≤ m), and let q and vq be deﬁned
accordingly. When we ﬁx component pj, we ﬁx vpj vulner-
abilities. Therefore, when we ﬁx the top T predicted com-
1≤j≤T vpj vulnerabilities, but with
ponents, we ﬁx F =
1≤j≤T vqj
optimal ordering, we could have ﬁxed Fopt =
vulnerabilities instead. Therefore, we will take the quotient
Q = F/Fopt as a quality measure for our ranking. This is
the fraction of vulnerabilities that we have caught when we
used p instead of the optimal ordering q. It will always be
between 0 and 1, and higher values are better.
In a typical situation, where we have V (cid:6) m and T small,
a random ranking will almost always have Q = 0, so our
method will be better than a random strategy if Q is always
greater than zero. In order to be useful in practice, we will
want Q to be signiﬁcantly greater than zero, say, greater
than 1/2.
6. CASE STUDY: MOZILLA
To evaluate Vulture’s predictive power, we applied it to
the code base of Mozilla [34]. Mozilla is a large open-source
project that has existed since 1998.
It is easily the sec-
ond most commonly used Internet suite (web browser, email
reader, and so on) after Internet Explorer and Outlook.
6.1 Data Collection
We examined Mozilla as of January 4, 2007. Vulture
mapped vulnerabilities to components, and then created the
Phase
Downloading and analyzing MFSAs
Mapping vulnerabilities to components
Finding includes
Finding function calls
Creation of SVM, w/classiﬁcation and regression
Time
5 m
1 m
0.5 m
8 m
0.5 m
Table 3: Approximate running times for Vulture’s
diﬀerent phases.
feature matrices and the vulnerability vector as described in
Sections 3.3 and 4.4.
Table 3 reports approximate running times for Vulture’s
diﬀerent phases when applied to Mozilla with imports as
the features under consideration. Vulture is so fast that we
could envision it as part of an IDE giving feedback in real
time (see Figure 10 at the end of the paper).
The 10,452 × 9,481 import matrix would take up some
280 MB of disk space if it were written out in full. The
sparse representation that we used [15] required only 230 KB
of disk space, however. The 10,452 × 93,265 function call
matrix took up 2.6 MB of disk space.
For each feature matrix and the vulnerability vectors, we
created 40 random splits using stratiﬁed sampling. This
ensures that vulnerable and neutral components are present
in the training and validation sets in the same proportions.
The training set had 6,968 entries and was twice as large
as the validation set with 3,484 entries; this is the standard
proportion for empirical evaluations of this kind. Finally,
we assessed these SVMs with the 40 validation sets.
For the statistical calculations, we used the R system [24]
and the SVM implementation available for it [9]. It is very
easy to make such calculations with R; the size of all R
scripts used in Vulture is just about 200 lines. The cal-
culations were done on standard hardware without special
memory sizes or processing powers.
6.2 Classiﬁcation
The SVM used the linear kernel with standard parame-
ters. Figure 8 reports the precision and recall values for the
40 random splits, both for imports and for function calls.
For imports, the recall has an average of 0.45 and standard
deviation of 0.04, which means that about half of all vulner-
able components are correctly classiﬁed:
Of all vulnerable components,
Vulture ﬂags 45% as vulnerable.
For function calls, the precision has a mean of 0.70 and a
standard deviation of 0.05.
Of all components ﬂagged as vulnerable,
70% actually are vulnerable.
Vulture is much better than random selection.
6.3 Ranking
The SVM used the linear kernel with standard parame-
ters. The coeﬃcient Q that was introduced in Section 5.3
was computed for imports and function calls, for T = 30. It
is shown in Figure 9, plotted against Fopt. For imports, its
mean is 0.78 (standard deviation is 0.04), for function calls,
it is 0.82 (standard deviation 0.08).
(a) Imports
(b) Function Calls
n
o
i
s
i
c
e
r
P
8
.
0
7
0
.
6
.
0
5
.
0
n
o
i
s
i
c
e
r
P
8
.
0
7
0
.
6
.
0
5
.
0
0.30
0.35
0.40
0.45
0.50
0.55
0.30
0.35
0.40
0.45
0.50
0.55
Recall
Recall
Figure 8: Scatterplot of precision/recall values for
the 40 experiments. Figure (a) shows the results for
imports, ﬁgure (b) shows the results for function
calls. The apparent lack of data points is due to
overplotting of close-by precision/recall pairs.
(a) Imports
(a) Function Calls
t
h
g
u
a
c
s
e
i
t
i
l
i
b
a
r
e
n
l
u
v
f
o
n
o
i
t
c
a
r
F
0
.
1
9
0
.
8
.
0
7
.
0
6
0
.
t
h
g
u
a
c
s
e
i
t
i
l
i
b
a
r
e
n
l
u
v
f
o
n
o
i
t
c
a
r
F
0
.
1
9
0
.
8
.
0
7
.
0
6
0
.
120
140
160
180
200
120
140
160
180
200
Number of vulnerabilities present
Number of vulnerabilities present
Figure 9: Scatterplot of Q versus Fopt for the 40
experiments where T = 30. Figure (a) shows the
results for imports, ﬁgure (b) shows the results for
function calls. Higher values are better.
Among the top 30 predicted components,
Vulture ﬁnds 82% of all vulnerabilities.
Let us illustrate the quality of the ranking by an actual
example where T = 10. Table 4 shows such a prediction
as produced in one of the random splits. Within the vali-
dation set, these would be the components to spend extra
eﬀort on. Your eﬀort would be well spent, because all of
the top ten components actually turn out to be vulnera-
ble. (SgridRowLayout and NsHttpTransaction are outliers,
but still vulnerable.) Furthermore, in your choice of ten,
you would recall the top four most vulnerable components,