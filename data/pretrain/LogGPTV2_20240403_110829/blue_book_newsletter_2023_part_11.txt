    try:
        # mypy is complaining that it can't import it, but it's solved below
        from typing import Annotated # type: ignore
    except ImportError:
        from typing_extensions import Annotated
    ```
### [Drone](drone.md)
* New: [Create the administrators.](drone.md#create-the-administrators)
    When you configure the Drone server you can create the initial administrative account by passing the below environment variable, which defines the account username (e.g. github handle) and admin flag set to true.
    ```bash
    DRONE_USER_CREATE=username:octocat,admin:true
    ```
    If you need to grant the primary administrative role to an existing user, you can provide an existing username. Drone will update the account and grant administrator role on server restart.
    You can create administrator accounts using the command line tools. Please see the command line tools documentation for installation instructions.
    Create a new administrator account:
    ```bash
    $ drone user add octocat --admin
    ```
    Or grant the administrator role to existing accounts:
    ```bash
    $ drone user update octocat --admin
    ```
* New: [Linter: untrusted repositories cannot mount host volumes.](drone.md#linter:-untrusted-repositories-cannot-mount-host-volumes)
    Thats because the [repository is not trusted](https://docs.drone.io/pipeline/docker/syntax/volumes/host/).
    You have to set the trust as an admin of drone through the GUI or through the CLI with
    ```bash
    drone repo update --trusted 
    ```
    If you're not an admin the above command returns a success but you'll see that the trust has not changed if you run
    ```bash
    drone repo info 
    ```
### [ArgoCD](argocd.md)
* New: Introduce ArgoCD.
    [Argo CD](https://argo-cd.readthedocs.io/en/stable/) is a declarative, GitOps continuous delivery tool for Kubernetes.
    Argo CD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways:
    - kustomize applications
    - helm charts
    - jsonnet files
    - Plain directory of YAML/json manifests
    - Any custom config management tool configured as a config management plugin, for example with [helmfile](#using-helmfile)
    Argo CD automates the deployment of the desired application states in the specified target environments. Application deployments can track updates to branches, tags, or pinned to a specific version of manifests at a Git commit. See tracking strategies for additional details about the different [tracking strategies available](https://argo-cd.readthedocs.io/en/stable/user-guide/tracking_strategies/).
### [Shellcheck](shellcheck.md)
* New: Introduce shellcheck.
    [Shellcheck](https://www.shellcheck.net/) is a linting tool to finds bugs in your shell scripts.
    **[Installation](https://github.com/koalaman/shellcheck#installing)**
    ```bash
    apt-get install shellcheck
    ```
    **Errors**
    **[SC2143: Use `grep -q` instead of comparing output with `[ -n .. ]`.](https://www.shellcheck.net/wiki/SC2143)**
    Problematic code:
    ```bash
    if [ "$(find . | grep 'IMG[0-9]')" ]
    then
      echo "Images found"
    fi
    ```
    Correct code:
    ```bash
    if find . | grep -q 'IMG[0-9]'
    then
      echo "Images found"
    fi
    ```
    Rationale:
    The problematic code has to iterate the entire directory and read all matching lines into memory before making a decision.
    The correct code is cleaner and stops at the first matching line, avoiding both iterating the rest of the directory and reading data into memory.
## Automating Processes
### [copier](copier.md)
* New: Introduce copier.
    [Copier](https://github.com/copier-org/copier) is a library and CLI app for rendering project templates.
    - Works with local paths and Git URLs.
    - Your project can include any file and Copier can dynamically replace values in any kind of text file.
    - It generates a beautiful output and takes care of not overwriting existing files unless instructed to do so.
    This long article covers:
    - [Installation](copier.md#installation)
    - [Basic concepts](copier.md#basic-concepts)
    - [Usage](copier.md#usage)
### [cruft](cruft.md)
* Correction: Suggest to use copier instead.
    [copier](https://github.com/copier-org/copier) looks a more maintained solution nowadays.
### [letsencrypt](letsencrypt.md)
* New: Introduce letsencrypt.
    [Letsencrypt](https://letsencrypt.org/) is a free, automated, and open certificate authority brought to you by the nonprofit Internet Security Research Group (ISRG). Basically it gives away SSL certificates, which are required to configure webservers to use HTTPS instead of HTTP for example.
    In the article you can also find:
    * [How to configure a wildcard dns when the provider is not
      supported](letsencrypt.md#configure-a-wildcard-dns-when-the-provider-is-not-supported)
## Storage
### [OpenZFS storage planning](zfs_storage_planning.md)
* New: Introduce ZFS storage planning.
* New: Analyze the Exos X18 of 16TB disk.
    | Specs                        | IronWolf           | IronWolf Pro         | Exos 7E8 8TB | Exos 7E10 8TB | Exos X18 16TB |
    | ---------------------------- | ------------------ | -------------------- | ------------ | ------------- | ------------- |
    | Technology                   | CMR                | CMR                  | CMR          | SMR           | CMR           |
    | Bays                         | 1-8                | 1-24                 | ?            | ?             | ?             |
    | Capacity                     | 1-12TB             | 2-20TB               | 8TB          | 8TB           | 16 TB         |
    | RPM                          | 5,400 RPM (3-6TB)  | 7200 RPM             | 7200 RPM     | 7200 RPM      | 7200 RPM      |
    | RPM                          | 5,900 RPM (1-3TB)  | 7200 RPM             | 7200 RPM     | 7200 RPM      | 7200 RPM      |
    | RPM                          | 7,200 RPM (8-12TB) | 7200 RPM             | 7200 RPM     | 7200 RPM      | 7200 RPM      |
    | Speed                        | 180MB/s (1-12TB)   | 214-260MB/s (4-18TB) | 249 MB/s     | 255 MB/s      | 258 MB/s      |
    | Cache                        | 64MB (1-4TB)       | 256 MB               | 256 MB       | 256 MB        | 256 MB        |
    | Cache                        | 256MB (3-12TB)     | 256 MB               | 256 MB       | 256 MB        | 256 MB        |
    | Power Consumption            | 10.1 W             | 10.1 W               | 12.81 W      | 11.03 W       | 9.31 W        |
    | Power Consumption Rest       | 7.8 W              | 7.8 W                | 7.64 W       | 7.06 W        | 5.08 W        |
    | Workload                     | 180TB/yr           | 300TB/yr             | 550TB/yr     | 550TB/yr      | 550TB/yr      |
    | MTBF                         | 1 million          | 1 million            | 2 millions   | 2 millions    | 2.5 millions  |
    | Warranty                     | 3 years            | 5 years              | 5 years      | 5 years       | 5 years       |
    | Price                        | From $60 (2022)    | From $83  (2022)     | 249$ (2022)  | 249$ (2022)   | 249$ (2023)   |
### [OpenZFS](sanoid.md)
* New: [How to create a pool and datasets.](zfs.md#usage)
* New: [Configure NFS.](zfs.md#configure-nfs)
    With ZFS you can share a specific dataset via NFS. If for whatever reason the dataset does not mount, then the export will not be available to the application, and the NFS client will be blocked.
    You still must install the necessary daemon software to make the share available. For example, if you wish to share a dataset via NFS, then you need to install the NFS server software, and it must be running. Then, all you need to do is flip the sharing NFS switch on the dataset, and it will be immediately available.
* New: [Backup.](zfs.md#backup)
    Please remember that [RAID is not a backup](https://serverfault.com/questions/2888/why-is-raid-not-a-backup), it guards against one kind of hardware failure. There's lots of failure modes that it doesn't guard against though:
    * File corruption
    * Human error (deleting files by mistake)
    * Catastrophic damage (someone dumps water onto the server)
    * Viruses and other malware
    * Software bugs that wipe out data
    * Hardware problems that wipe out data or cause hardware damage (controller malfunctions, firmware bugs, voltage spikes, ...)
    That's why you still need to make backups.
    ZFS has the builtin feature to make snapshots of the pool. A snapshot is a first class read-only filesystem. It is a mirrored copy of the state of the filesystem at the time you took the snapshot. They are persistent across reboots, and they don't require any additional backing store; they use the same storage pool as the rest of your data.
    If you remember [ZFS's awesome nature of copy-on-write](https://pthree.org/2012/12/14/zfs-administration-part-ix-copy-on-write/) filesystems, you will remember the discussion about Merkle trees. A ZFS snapshot is a copy of the Merkle tree in that state, except we make sure that the snapshot of that Merkle tree is never modified.
    Creating snapshots is near instantaneous, and they are cheap. However, once the data begins to change, the snapshot will begin storing data. If you have multiple snapshots, then multiple deltas will be tracked across all the snapshots. However, depending on your needs, snapshots can still be exceptionally cheap.
    The article also includes:
    * [ZFS snapshot lifecycle management](zfs.md#zfs-snapshot-lifecycle-management)
    * [Restore a backup](zfs.md#restore-a-backup)
* New: Introduce Sanoid.
    [Sanoid](https://github.com/jimsalterjrs/sanoid/) is the most popular tool right now, with it you can create, automatically thin, and monitor snapshots and pool health from a single eminently human-readable TOML config file.
    The article includes:
    * [Installation](sanoid.md#installation)
    * [Configuration](sanoid.md#configuration)
    * Pros and cons
    * [Usage](sanoid.md#usage)
    * [Troubleshooting](sanoid.md#troubleshooting)
* New: [Get compress ratio of a filesystem.](zfs.md#get-compress-ratio-of-a-filesystem)
    ```bash
    zfs get compressratio {{ filesystem }}
    ```
* Correction: Use the recursive flag.
    `recursive` is not set by default, so the dataset's children won't be backed up unless you set this option.
    ```
     [main/backup]
       use_template = daily
       recursive = yes
    ```
* New: [See how much space do your snapshots consume.](zfs.md#see-how-much-space-do-your-snapshots-consume)
    When a snapshot is created, its space is initially shared between the snapshot and the file system, and possibly with previous snapshots. As the file system changes, space that was previously shared becomes unique to the snapshot, and thus is counted in the snapshot’s `used` property.
    Additionally, deleting snapshots can increase the amount of space that is unique for use by other snapshots.
    Note: The value for a snapshot’s space referenced property is the same as that for the file system when the snapshot was created.
    You can display the amount of space that is consumed by snapshots and descendant file systems by using the `zfs list -o space` command.
    ```bash
    NAME                             AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD
    rpool                            10.2G  5.16G         0   4.52M              0      5.15G
    rpool/ROOT                       10.2G  3.06G         0     31K              0      3.06G
    rpool/ROOT/solaris               10.2G  3.06G     55.0M   2.78G              0       224M
    rpool/ROOT/solaris@install           -  55.0M         -       -              -          -
    rpool/ROOT/solaris/var           10.2G   224M     2.51M    221M              0          0
    rpool/ROOT/solaris/var@install       -  2.51M         -       -              -          -
    ```
    From this output, you can see the amount of space that is:
    * AVAIL: The amount of space available to the dataset and all its children, assuming that there is no other activity in the pool.
    * USED: The amount of space consumed by this dataset and all its descendants. This is the value that is checked against this dataset's quota and reservation. The space used does not include this dataset's reservation, but does take into account the reservations of any descendants datasets.
        The used space of a snapshot is the space referenced exclusively by this snapshot. If this snapshot is destroyed, the amount of `used` space will be freed. Space that is shared by multiple snapshots isn't accounted for in this metric.
    * USEDSNAP: Space being consumed by snapshots of each data set
    * USEDDS: Space being used by the dataset itself
    * USEDREFRESERV: Space being used by a refreservation set on the dataset that would be freed if it was removed.
    * USEDCHILD: Space being used by the children of this dataset.
    Other space properties are:
    * LUSED: The amount of space that is "logically" consumed by this dataset and all its descendents. It ignores the effect of `compression` and `copies` properties, giving a quantity closer to the amount of data that aplication ssee. However it does include space consumed by metadata.
    * REFER: The amount of data that is accessible by this dataset, which may or may not be shared with other dataserts in the pool. When a snapshot or clone is created, it initially references the same amount of space as the filesystem or snapshot it was created from, since its contents are identical.
* New: [Rename or move a dataset.](zfs.md#rename-or-move-a-dataset)
    NOTE: if you want to rename the topmost dataset look at [rename the topmost dataset](#rename-the-topmost-dataset) instead.
    File systems can be renamed by using the `zfs rename` command. You can perform the following operations:
    - Change the name of a file system.
    - Relocate the file system within the ZFS hierarchy.
    - Change the name of a file system and relocate it within the ZFS hierarchy.
    The following example uses the `rename` subcommand to rename of a file system from `kustarz` to `kustarz_old`:
    ```bash
    zfs rename tank/home/kustarz tank/home/kustarz_old
    ```
    The following example shows how to use zfs `rename` to relocate a file system:
    ```bash
    zfs rename tank/home/maybee tank/ws/maybee
    ```
    In this example, the `maybee` file system is relocated from `tank/home` to `tank/ws`. When you relocate a file system through rename, the new location must be within the same pool and it must have enough disk space to hold this new file system. If the new location does not have enough disk space, possibly because it has reached its quota, rename operation fails.
    The rename operation attempts an unmount/remount sequence for the file system and any descendent file systems. The rename command fails if the operation is unable to unmount an active file system. If this problem occurs, you must forcibly unmount the file system.
    You'll loose the snapshots though, as explained below.
* New: [Rename the topmost dataset.](zfs.md#rename-the-topmost-dataset)
    If you want to rename the topmost dataset you [need to rename the pool too](https://github.com/openzfs/zfs/issues/4681) as these two are tied.
    ```bash
    $: zpool status -v
      pool: tets
     state: ONLINE
     scrub: none requested
    config:
            NAME        STATE     READ WRITE CKSUM
            tets        ONLINE       0     0     0
              c0d1      ONLINE       0     0     0
              c1d0      ONLINE       0     0     0
              c1d1      ONLINE       0     0     0
    errors: No known data errors
    ```
    To fix this, first export the pool:
    ```bash
    $ zpool export tets
    ```
    And then imported it with the correct name:
    ```bash
    $ zpool import tets test
    ```
    After the import completed, the pool contains the correct name:
    ```bash
    $ zpool status -v
      pool: test
     state: ONLINE
     scrub: none requested
    config:
            NAME        STATE     READ WRITE CKSUM
            test        ONLINE       0     0     0
              c0d1      ONLINE       0     0     0
              c1d0      ONLINE       0     0     0
              c1d1      ONLINE       0     0     0
    errors: No known data errors
    ```
    Now you may need to fix the ZFS mountpoints for each dataset
    ```bash
    zfs set mountpoint="/opt/zones/[Newmountpoint]" [ZFSPOOL/[ROOTor other filesystem]
    ```
* New: [Rename or move snapshots.](zfs.md#rename-or-move-snapshots)