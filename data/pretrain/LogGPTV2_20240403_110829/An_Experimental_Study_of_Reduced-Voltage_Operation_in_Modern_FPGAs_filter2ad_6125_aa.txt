title:An Experimental Study of Reduced-Voltage Operation in Modern FPGAs
for Neural Network Acceleration
author:Behzad Salami and
Erhan Baturay Onural and
Ismail Emir Yuksel and
Fahrettin Koc and
Oguz Ergin and
Adri&apos;an Cristal Kestelman and
Osman S. Unsal and
Hamid Sarbazi-Azad and
Onur Mutlu
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
An Experimental Study of Reduced-Voltage Operation in Modern FPGAs
for Neural Network Acceleration
Behzad Salami1
Erhan Baturay Onural2
Ismail Emir Yuksel2
Fahrettin Koc2
Oguz Ergin2
Adrián Cristal Kestelman1,3
Osman S. Unsal1
Hamid Sarbazi-Azad4
Onur Mutlu5
1BSC
2TOBB ETÜ
3UPC and CSIC-IIIA
4SUT and IPM
5ETH Zürich
Abstract
We empirically evaluate an undervolting technique, i.e.,
underscaling the circuit supply voltage below the nominal
level, to improve the power-efﬁciency of Convolutional Neural
Network (CNN) accelerators mapped to Field Programmable
Gate Arrays (FPGAs). Undervolting below a safe voltage
level can lead to timing faults due to excessive circuit la-
tency increase. We evaluate the reliability-power trade-off
for such accelerators. Speciﬁcally, we experimentally study
the reduced-voltage operation of multiple components of real
FPGAs, characterize the corresponding reliability behavior of
CNN accelerators, propose techniques to minimize the draw-
backs of reduced-voltage operation, and combine undervolting
with architectural CNN optimization techniques, i.e., quanti-
zation and pruning. We investigate the effect of environmental
temperature on the reliability-power trade-off of such acceler-
ators.
We perform experiments on three identical samples of mod-
ern Xilinx ZCU102 FPGA platforms with ﬁve state-of-the-art
image classiﬁcation CNN benchmarks. This approach allows
us to study the effects of our undervolting technique for both
software and hardware variability. We achieve more than 3X
power-efﬁciency (GOPs/W ) gain via undervolting. 2.6X of
this gain is the result of eliminating the voltage guardband
region, i.e., the safe voltage region below the nominal level
that is set by FPGA vendor to ensure correct functionality in
worst-case environmental and circuit conditions. 43% of the
power-efﬁciency gain is due to further undervolting below the
guardband, which comes at the cost of accuracy loss in the
CNN accelerator. We evaluate an effective frequency under-
scaling technique that prevents this accuracy loss, and ﬁnd
that it reduces the power-efﬁciency gain from 43% to 25%.
1. Introduction
Deep Neural Networks (DNNs) and speciﬁcally Convolutional
Neural Networks (CNNs) have recently attained signiﬁcant
success in image and video classiﬁcation tasks. They are fun-
damental for state-of-the-art real-world applications running
on embedded systems as well as data centers. These neural
networks learn a model from a dataset in their training phase
and make predictions on new, previously-unseen data in their
classiﬁcation phase. However, their power-efﬁciency is in-
herently the primary concern due to the massive amount of
data movement and computational power required. Thus, the
scalability of CNNs for enterprise applications and their de-
ployment in battery-limited scenarios, such as in drones and
mobile devices, are crucial concerns.
Typically, hardware acceleration using Graphics Process-
ing Units (GPUs) [135], Field Programmable Gate Arrays
(FPGAs) [85, 101], or Application-Speciﬁc Integrated Cir-
cuits (ASICs) [22, 42, 102] leads to a signiﬁcant reduction in
CNN power consumption [109]. Among these, FPGAs are
rapidly becoming popular and are expected to be used in 33%
of modern data centers by 2020 [28]. This increase in the
popularity of FPGAs is attributed to their power-efﬁciency
compared to GPUs, their ﬂexibility compared to ASICs, and
recent advances in High-Level Synthesis (HLS) tools that
signiﬁcantly facilitate easier mapping of applications on FP-
GAs [6, 82, 84, 92–94, 114]. Hence, major companies, such as
Amazon [44] (with EC2 F1 cloud) and Microsoft [29] (with
Brainwave project), have made large investments in FPGA-
based CNN accelerators. However, recent studies show that
FPGA-based accelerators are at least 10X less power-efﬁcient
compared to ASIC-based ones [12, 73, 74]. In this paper, we
aim to bridge this power-efﬁciency gap by empirically under-
standing and leveraging an effective undervolting technique
for FPGA-based CNN accelerators.
Power-efﬁciency of state-of-the-art CNNs generally im-
proves via architectural-level techniques, such as quantiza-
tion [137] and pruning [67]. These techniques do not signiﬁ-
cantly compromise CNN accuracy as they exploit the sparse
nature of CNN applications [3, 80, 134]. To further improve
the power-efﬁciency of FPGA-based CNN accelerators, we
propose to employ an orthogonal hardware-level approach:
undervolting (i.e., circuit supply voltage underscaling) below
the nominal/default level (Vnom), combined with the aforemen-
tioned architectural-level techniques. FPGA vendors usually
add a voltage guardband to ensure the correct operation of
FPGAs under the worst-case circuit and environmental con-
ditions. However, these guardbands can be very conservative
and unnecessary for state-of-the-art applications. Supply volt-
age underscaling below the nominal level was already shown
to provide signiﬁcant efﬁciency improvements in CPUs [78],
GPUs [66, 138], ASICs [17], and DRAMs [18, 50]. This pa-
per extends such studies to FPGAs. Speciﬁcally, we study
the classiﬁcation phase of FPGA-based CNN accelerators, as
this phase can be repeatedly used in power-limited edge de-
vices (unlike the training phase, which is invoked much less
frequently). Unlike simulation-based approaches that may
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00032
138
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:26:55 UTC from IEEE Xplore.  Restrictions apply. 
not be accurate enough [90, 132], our study is based on real
off-the-shelf FPGA devices.
The extra voltage guardband can range between 12-35% of
the nominal supply voltage of modern CPUs [78], GPUs [138],
and DRAM chips [18]. Reducing the supply voltage in this
guardband region does not lead to reliability issues under nor-
mal operating conditions, and thus, eliminating this guardband
can result in a signiﬁcant power reduction for a wide variety
of real-world applications. We experimentally demonstrate a
large voltage guardband for modern FPGAs: an average of
33% with a slight variation across hardware platforms and
software benchmarks. Eliminating this guardband leads to
signiﬁcant power-efﬁciency (GOPs/W ) improvement, on av-
erage, 2.6X, without any performance or reliability overheads.
With further undervolting, the power-efﬁciency improves by
an extra 43%, leading to a total improvement of more than 3X.
This additional gain does not come for free, as we observe
exponentially-increasing CNN accuracy loss below the guard-
band region. With further undervolting below this guardband,
our experiments indicate that the minimum supply voltage
at which the internal FPGA components could be functional
(Vcrash) is equal to, on average, 63% of Vnom. Further reducing
the supply voltage results in system crash.
We evaluate our undervolting technique on three identical
samples of the Zynq-based ZCU102 platform [125], a repre-
sentative modern FPGA from Xilinx. However, we believe that
our experimental observations are applicable to other FPGA
platforms as well, perhaps with some minor differences. We
previously showed beneﬁts of reduced-voltage operation for
on-chip memories on different, older FPGA platforms [96].
Other works observed similar behavior for different types of
CPUs [78], GPUs [138], and DRAM chips [18]. In this paper,
we characterize the power dissipation of FPGA-based CNN ac-
celerators under reduced-voltage levels and apply undervolting
to improve the power-efﬁciency of such accelerators.1
We experimentally evaluate the effects of reduced-voltage
operation in on-chip components of the FPGA platform, in-
cluding Block RAMs (BRAMs) and internal FPGA compo-
nents, containing Look-Up Tables (LUTs), Digital Signal Pro-
cessors (DSPs), buffers, and routing resources.2 We perform
our experiments on ﬁve state-of-the-art CNN image classiﬁca-
tion benchmarks, including VGGNet [106], GoogleNet [110],
AlexNet [51], ResNet [35], and Inception [110]. This en-
ables us to experimentally study the workload-to-workload
variation on the power-reliability trade-offs of FPGA-based
CNN accelerators. Speciﬁcally, we extensively characterize
the reliability behavior of the studied benchmarks below the
guardband level and evaluate a frequency underscaling tech-
nique to prevent the accuracy loss in this voltage region. Our
1Our exploration of the FPGA voltage behavior and the subsequent power-
efﬁciency gain is applicable to any application.
2These internal FPGA components share a single voltage rail in the studied
FPGA platform. To our knowledge, such voltage rail sharing is a typical case
for most modern FPGA platforms.
study also examines the effects of architectural quantization
and pruning techniques with reduced-voltage FPGA operation.
Finally, we experimentally evaluate the effect of environmen-
tal temperature variation on the power-reliability behavior of
FPGA-based CNN accelerators.
1.1. Contributions
To our knowledge, for the ﬁrst time, this paper experimen-
tally studies the power-performance-accuracy characteristics
of CNN accelerators with greatly reduced supply voltage ca-
pability implemented in real FPGAs. In summary, we achieve
a total of more than 3X power-efﬁciency improvement for
FPGA-based CNN accelerators. We gain insights into the
reduced-voltage operation of such accelerators and, in turn,
the effect of FPGA supply voltage on the power-reliability
trade-off. We make the following major contributions:
• We characterize the power consumption of FPGA-based
CNN accelerators across different FPGA components. We
identify that the internal on-chip components, including
processing elements, contribute to a vast majority of the
total power consumption. We reduce this source of power
consumption via our undervolting technique.
• We improve the power-efﬁciency of FPGA-based CNN ac-
celerators by more than 3X, measured across ﬁve state-of-
the-art image classiﬁcation benchmarks. 2.6X of the power-
efﬁciency gain is due to eliminating the voltage guardband,
which we measure to be on average 33%. An additional
43% gain is due to further undervolting below the guardband,
which comes at the cost of CNN accuracy loss.
• We characterize the reliability behavior of FPGA-based
CNN accelerators when executed below the voltage guard-
band level and observe an exponential reduction in CNN
accuracy as voltage reduces. We observe that workloads
with more parameters, e.g., ResNet and Inception, are rela-
tively more vulnerable to undervolting-related faults.
• To prevent CNN accuracy loss below the voltage guardband
level, we combine voltage underscaling with frequency un-
derscaling. We experiment with a supply voltage lower than
Vnom and with operating frequency Fop < Fmax. Our exper-
iments show that the most energy-efﬁcient operating point
is the one with the maximum frequency and minimum safe
voltage, namely, Vmin. However, lower voltage and lower
frequency lead to better power-efﬁciency.
• We combine voltage underscaling with the existing CNN
quantization and pruning techniques and study the power-
reliability trade-off of such optimized FPGA-based CNN
accelerators. We observe that these bit/parameter-size reduc-
tion techniques (quantization and pruning) slightly increase
the vulnerability of a CNN to undervolting-related faults;
but, they deliver signiﬁcantly higher power-efﬁciency when
integrated with our undervolting technique.
• We study the effect of environmental temperature on the
power-reliability trade-off of reduced-voltage FPGA-based
CNN accelerators. We observe that temperature has a direct
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:26:55 UTC from IEEE Xplore.  Restrictions apply. 
139
effect on the power consumption of such accelerators. How-
ever, at very low voltage levels, this effect is not noticeable.
• We evaluate the effect of hardware platform variability by
repeating our experiments on three identical samples of
the Xilinx ZCU102 FPGA platform. We ﬁnd large volt-
age guardbands in all platforms (an average of 33%), i.e.,
Vmin = 0.67∗Vnom = 570mV . However, across three FPGAs,
we observe a variation on Vmin, i.e., ΔVmin = 31mV . This
variation can be due to process variation. Our results show
that the variation of guardband regions across different CNN
workloads is insigniﬁcant.
2. Background
In this section, we brieﬂy introduce the most important con-
cepts used in this paper, including the architecture of CNNs as
well as the undervolting technique.
2.1. Convolutional Neural Networks (CNNs)
DNNs are a class of Machine Learning (ML) methods that
are designed to classify unseen objects or entities using non-
linear transformations applied to input data [53]. DNNs are
composed of biologically inspired neurons, interconnected to
each other. Among different DNN models, multi-layer CNNs
are a common type, which has recently shown acceptable
success in classiﬁcation tasks for real-world applications.
2.1.1. Phases of a CNN: Training and Classiﬁcation. A
CNN model encompasses two stages: training and classiﬁ-
cation (inference). Training learns a model from a set of
training data. It is an iterative, usually a single-time (or rel-
atively infrequently-executed) step, including backward and
forward phases. It adjusts the CNNs parameters, i.e., weights
and biases, which determine the strength of the connections
between different neurons across CNN layers. The training
phase minimizes a loss function, which directly relates to the
accuracy of the neural network in the classiﬁcation phase. In
contrast, inference is a post-training phase that aims to classify
unknown data, using the trained network model. The infer-
ence phase is more frequently executed in edge devices with
power-constrained environments. The target of this paper is
the inference stage, similar to many existing efforts on the
acceleration of CNNs [32, 50, 109].
2.1.2. Internal Architecture of a CNN. A CNN is composed
of multiple processing layers such as Convolution, Pooling,
Fully-Connected, and SoftMax for feature extraction with var-
ious abstractions. Other customized layers can be used case
by case for more optimized feature extraction, such as Batch
Normalization [71]. The functionality of each type of layer
depends on the way in which the neurons are interconnected.
Convolution layers generate a more profound abstraction of
the input data, called a feature map. Following each Convolu-
tion layer, there is usually a Max/Avg Pooling layer to reduce
the dimensionality of the feature map. Successive multiple
Convolution and Pooling layers generate in-depth informa-
tion from the input data. Afterward, Fully-Connected layers
are typically applied for classiﬁcation purposes. Finally, the
SoftMax layer generates the class probabilities from the class
scores in the output layer. Between layers, there are activation
functions, such as Relu or Sigmoid, to add non-linear prop-
erties to the network. The required computations of different
layers are translated to matrix multiplication computations.
Thus, matrix multiplication optimization techniques, such as
FFT or Strassen [52], can be applied to accelerate the inference
implementation. Matrix multiplication is an ideal application
to take advantage of parallel and data ﬂow execution model
used in FPGA-based hardware accelerators.
2.1.3. Architectural Optimizations. To improve the power-
efﬁciency of CNNs, two most commonly-used architectural-
level techniques are quantization [136] and pruning [67].3
These two techniques rely on the sparse nature of CNNs,
i.e., a vast majority of CNN computations are unnecessary.
Quantization aims to reduce the complexity of high-precision
CNN computation units by substituting selected ﬂoating-
point parameters with low-precision ﬁxed-point. Pruning
aims to reduce the model size by eliminating unnecessary
weight/neurons/connections of a CNN. These architectural
techniques are applicable to any underlying hardware. There
are numerous extensions of quantization [136, 137] and prun-
ing [36, 129] techniques. In our experiments, we integrate