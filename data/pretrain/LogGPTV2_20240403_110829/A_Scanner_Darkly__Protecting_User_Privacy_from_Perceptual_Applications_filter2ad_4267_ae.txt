a
m
r
o
n
(
y
t
i
r
u
c
e
s
d
e
t
c
e
e
d
t
1.0
0.8
0.6
0.4
0.2
s
e
r
a
u
q
s
d
e
t
c
e
e
d
t
)
d
e
z
i
l
a
m
r
o
n
(
f
o
#
1.0
0.8
0.6
0.4
0.2
t
s
r
u
o
n
o
c
d
e
t
c
e
e
d
t
)
d
e
z
i
l
a
m
r
o
n
(
f
o
#
f
o
#
0.0
0
1
2
3
5
4
8
Privacy dial value
6
7
9 10 11
0.0
0
1
2
3
5
4
8
Privacy dial value
6
7
9 10 11
0.0
0
1
2
3
5
4
8
Privacy dial value
6
7
s
r
e
n
r
o
c
d
e
t
c
e
e
d
t
)
d
e
z
i
l
a
m
r
o
n
(
f
o
#
1.0
0.8
0.6
0.4
0.2
0.0
0
1
2
3
1.0
0.8
0.6
0.4
0.2
y
c
a
r
u
c
c
a
g
n
k
c
a
r
t
i
)
d
e
z
i
l
a
m
r
o
n
(
.
g
v
A
1.0
0.8
0.6
0.4
0.2
n
o
i
t
)
d
e
z
i
l
a
m
r
o
n
(
l
a
e
r
r
o
C
.
g
v
A
5
4
8
Privacy dial value
6
7
9 10 11
0.0
0
1
2
3
5
4
8
Privacy dial value
6
7
9 10 11
0.0
0
1
2
3
5
4
8
Privacy dial value
6
7
9 10 11
9 10 11
Figure 12. Change in the number of detected security breaches (Security cam), detected squares (Square detector), detected contours (Ellipse ﬁtter), moments
(Ball tracker), and histograms (RGB and H-S histogram calculators, Intensity/contrast changer for images/histograms, and H-S histogram backprojector) as
the privacy level increases. Correlation between histograms was calculated using the cvHistCompare function. Accuracy for tracking was measured using
the Euclidean distance between the object’s original position and the reported position after applying privacy transforms.
We scanned these 77 projects for invocations of cvGet2D,
cvGetAt, or cvGetRawData, and direct accesses to the im-
ageData ﬁeld of the image data structure. After removing
the spurious matches caused by included OpenCV header
ﬁles, we found that 70% of the projects (54 out of 77) do
not access raw pixels. Furthermore, only 11 projects access
the network, and only 2 access audio inputs.
These 77 projects call a total of 291 OpenCV functions, of
which 145 are already supported by our DARKLY prototype,
118 can be supported with opaque references, 15 can be sup-
ported with the sketching-based declassiﬁer, and 3 require
porting application code to ibc. These 281 functions are
sufﬁcient to support 68 of the 77 surveyed projects.
The remaining 9 projects make calls to unsupported
OpenCV functions (10 in total) that perform tasks such as
optical ﬂow (cvCalcOpticalFlowBM, cvCalcOpticalFlowHS
cvCalcOpticalFlowLK, and cvCalcOpticalFlowPyrLK), ob-
ject
tracking (cvCamShift, cvMeanShift, and cvSnakeIm-
age), camera calibration (ComputeCorrespondEpilines), mo-
tion analysis (cvSegmentMotion), and image segmenta-
tion (cvWatershed). Supporting these functions in DARKLY
would require new, task-speciﬁc privacy transforms and is
an interesting topic for future research.
X. RELATED WORK
Denning et al. [7] showed that many off-the-shelf con-
sumer robots do not use proper encryption and authenti-
cation, thus a network attacker can control the robot or
extract sensitive data. By contrast, DARKLY protects users
from untrusted applications running on a trusted robot.
PlaceRaider [25] is a hypothetical mobile malware that
can construct a 3-D model of its environment from phone-
camera images. DARKLY prevents this and similar attacks.
SciFi [21] uses secure multiparty computation to match
faces against a database. Matching takes around 10 seconds
per image, thus SciFi is unusable for real-time applications.
The threat model of DARKLY is different (protecting images
from untrusted applications), it handles many more percep-
tual tasks, and can protect real-time video feeds.
Ad-hoc methods for protecting speciﬁc sensitive items
include the blurring of faces and license plates in Google
Maps’ Street View [24]. Senior et al. [23] suggested im-
age segmentation to detect sensitive objects in surveillance
videos and transform them according to user-provided poli-
cies. To protect surveillance videos on the network, Dufaux
and Ebrahimi [8] proposed to encrypt regions of interest.
This requires computationally expensive, ofﬂine image seg-
mentation and it is not clear whether perceptual applications
would work with the modiﬁed videos. Chan et al. [5]
developed a method for counting the number of pedestrians
in surveillance videos without tracking any single individual.
Sweeney et al. published several papers [11, 12, 19] on
“de-identifying” datasets of face images. Many of their tech-
niques, especially in the k-same-Eigen algorithm, are similar
to the generalization transform described in Section VII-B.
They do a “greedy” version of clustering and their model-
based face averaging has similarities with face morphing.
Showing the outputs of privacy transforms to the user on
the DARKLY console is conceptually similar to the sensor-
access widgets by Howell and Schechter [14]. Their widgets,
however, display the entire camera feed because applications
in their system have unrestricted access to visual inputs.
362
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:08 UTC from IEEE Xplore.  Restrictions apply. 
Augmented Reality (AR) applications are a special subset
of perceptual applications that not only read perceptual data
but also modify and display some parts of the input back
to the user. To protect user privacy from such applications,
D’Antoni et al. [6] argue that the OS should provide new
higher-level abstractions for accessing perceptual data in-
stead of the current low-level sensor API. Jana et al. [16]
built a new OS abstraction (recognizers) and a permission
system for enforcing ﬁne-grained, least-privilege access to
perceptual data by AR applications. This permission-based
approach is complementary to DARKLY.
XI. FUTURE WORK
DARKLY is the ﬁrst step towards privacy protection for
perceptual applicatons. Topics for future research include:
(1) evaluation of functionality and usability on a variety of
computer-vision tasks, (2) support for application-provided,
potentially untrusted object recognition models (the cur-
rent
transform for cvHaarDetectObjects is based on the
face detection model shipped with OpenCV) and third-
party object recognition services such as Dextro Robotics,
and (3) development of privacy transforms for untrusted,
application-provided image-processing code. The latter may
obviate the restriction on the outputs of untrusted code,
but would also require a new visualization technique for
displaying these outputs to the user on the DARKLY console.
Longer-term research includes: (4) preventing inferential
leaks by using large-scale, supervised machine learning to
construct detectors and ﬁlters for privacy-sensitive objects
and scenes, such as certain text strings, gestures, patterns of
movement and physical proximity, etc., and (5) extending
the system to other perceptual inputs such as audio.
Acknowledgments. We are grateful to David Molnar, Scott
Saponas, and Ryan Calo for helpful discussions during the
early part of this work and to Piyush Khandelwal for helping
us evaluate DARKLY on the Segway RMP-50 robot.
This work was supported by the NSF grant CNS-0746888,
the MURI program under AFOSR Grant No. FA9550-08-1-
0352, and Google PhD Fellowship to Suman Jana.
REFERENCES
[1] bc Command Manual.
http://www.gnu.org/software/bc/
manual/html chapter/bc toc.html.
[2] R. V. Bruegge. Facial Recognition and Identiﬁcation Initia-
tives. http://biometrics.org/bc2010/presentations/DOJ/vorder
bruegge-Facial-Recognition-and-Identiﬁcation-Initiatives.
pdf, 2010.
[3] T. Bui, M. Poel, D. Heylen, and A. Nijholt. Automatic face
morphing for transferring facial animation. In CGIM, 2003.
CVPR, 1991.
[4] M. R. Calo. People can be so fake: A new dimension to
privacy and technology scholarship. Penn St. L. Rev., 114:809,
2009.
[5] A. Chan, Z. Liang, and N. Vasconcelos. Privacy preserving
crowd monitoring: Counting people without people models
or tracking. In CVPR, 2008.
[6] L. D’Antoni, A. Dunn, S. Jana, T. Kohno, B. Livshits,
D. Molnar, A. Moshchuk, E. Ofek, F. Roesner, S. Saponas,
M. Veanes, and H. Wang. Operating system support for
augmented reality applications. Technical Report MSR-TR-
2013-12, Microsoft Research.
[7] T. Denning, C. Matuszek, K. Koscher, J. Smith, and T. Kohno.
A Spotlight on Security and Privacy Risks with Future
Household Robots: Attacks and Lessons. In Ubicomp, 2009.
[8] F. Dufaux and T. Ebrahimi. Scrambling for video surveillance
with privacy. In CVPRW, 2006.
[9] C. Dwork. Differential privacy. In ICALP, 2006.
[10] C. Dwork. A ﬁrm foundation for private data analysis.
In
CACM, 2011.
[11] R. Gross, E. Airoldi, B. Malin, and L. Sweeney. Integrating
utility into face de-identiﬁcation. In PET, 2006.
[12] R. Gross, L. Sweeney, F. De la Torre, and S. Baker. Model-
based face de-identiﬁcation. In CVPRW, 2006.
[13] HighGUI: High-level GUI
and Media
I/O.
//opencv.willowgarage.com/documentation/python/highgui
high-level gui and media i o.html.
[14] J. Howell and S. Schechter. What you see is what they get:
Protecting users from unwanted use of microphones, camera,
and other sensors. In W2SP, 2010.
[15] R. Hummel, B. Kimia, and S. Zucker. Deblurring gaussian
http:
blur. CVGI, 1987.
[16] S. Jana, D. Molnar, A. Moshchuk, A. Dunn, B. Livshits,
H. Wang, and E. Ofek. Enabling ﬁne-grained permissions for
augmented reality applications with recognizers. Technical
Report MSR-TR-2013-11, Microsoft Research.
[17] D. McCullagh. Call it Super Bowl Face Scan I. http://www.
wired.com/politics/law/news/2001/02/41571, 2001.
[18] F. McSherry. Privacy integrated queries: an extensible plat-
form for privacy-preserving data analysis. In SIGMOD, 2009.
[19] E. Newton, L. Sweeney, and B. Malin. Preserving privacy by
de-identifying face images. TKDE, 2005.
[20] OpenCV Wiki. http://opencv.willowgarage.com/wiki/.
[21] M. Osadchy, B. Pinkas, A. Jarrous, and B. Moskovich. SCiFI
- A System for Secure Face Identiﬁcation. In S&P, 2010.
[22] F. Samaria and A. Harter. Parameterisation of a stochastic
In Applications of
model for human face identiﬁcation.
Computer Vision, 1994.
[23] A. Senior, S. Pankanti, A. Hampapur, L. Brown, Y. Tian, and
A. Ekin. Blinkering Surveillance: Enabling Video Privacy
through Computer Vision. IBM Research Report, 2003.
[24] Google Maps Street View - Privacy. http://maps.google.com/
help/maps/streetview/privacy.html.
[25] R. Templeman, Z. Rahman, D. Crandall, and A. Kapadia.
in physical spaces with smart-
theft
PlaceRaider: Virtual
phones. In NDSS, 2013.
[26] M. Turk and A. Pentland. Eigenfaces for recognition.
In
363
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:08 UTC from IEEE Xplore.  Restrictions apply.