gain insight and observability of the distributed system, however, with this tendency to
increase in quantity and complexity, it is becoming an overwhelming task for operators.
There are some tools that help handling tracing data, such as the ones presented in
Subsection 2.2.1 - Distributed Tracing Tools, however, they only perform the job of col-
lecting tracing data, present this information to the user in more human-readable formats
and provide forms of querying this type of data. For this reason, manually managing
these growing microservice architectures is becoming an outdated approach due to their
incomportability.
To address this issue, there is a great need of improving tracing data processing and
automate the task of tracing analysis. However, at this point, we did not have any tracing
data at our disposal to start working, thus acquire tracing data from a distributed system
was an urgency. Obtain tracing data for study is hard because it represents working
from these systems and contain confidential information about them. However, through a
27
Chapter 3
NDA (Non-Disclosure Agreement) and the help of professor Jorge Cardoso, representing
Huawei,wewereabletogainaccesstoconfidentialtracingdatageneratedbythecompany.
To ensure confidentiality, in this thesis, direct data fields are presented using fictitious
information.
This tracing data set was the starting point for this research. It is in OpenTracing
format and was provided by Huawei. This data had been gathered from an experimental
OpenStack cluster used by the company for testing purposes, and covered two days of
operation. This data is addressed in detail in Section 5.1 - Huawei Tracing Data Set.
After having access to tracing data, we have developed some prototype tools for data
ingestion and setted up a distributed tracing tool. Zipkin was used as a distributed
tracing tool to ingest tracing data provided by Huawei. The decision to use Zipkin instead
of Jaeger, fell in the fact that it were much simpler due to lesser feature configuration.
This was done with the purpose of gain a clear visualization about the data that were
given. From this we decided to perform several meetings with the objective of defining a
research direction and be able to propose a solution.
In these meetings, the elements of this research project gathered to debate ideas and
defineasetofquestionstoanswer,takingintoconsiderationthedefinedproblem. Professor
JorgeCardoso,representingHuawei,wastheclientofthedesignedsolution. Theapproach
taken was to create a shared Kanban Board [58], containing multiple lanes, to perform
the of generation and refinement of prototype questions. This process involved having
prototype question in the first lane, and move them through every lane reaching the last
one, transforming a prototype question into a final research question. These research
questions were built taking into consideration:
1. Mainneedsfeltbyoperatorsinnormalday-to-daytasks,troubleshootingdistributed
systems;
2. Most common issues presented in these systems;
3. Variables involved when these issues appear;
4. Relationship between these variables and the most common issues.
In the next Section 3.2 - Research Questions, the process to generate the research
questions is explained and the research questions, in their final state, are presented.
3.2 Research Questions
In this Section, we start by explaining the process to generate the research questions.
In the end, these questions are defined and a possible approach for each one of them is
discussed.
A Kanban Board was created with five lanes: “Initial Prototype”, “To Refine (1)”,
“Interesting”, “To Refine (2)” and “Final Research Questions”. Throughout these lanes,
questions were improved and filtered before reaching their final state.
Initial prototype questions were generated based on the four points enumerated at the
end of the previous Section 3.1. Therefore, prototype questions where:
28
Research Objectives and Approach
1. What is the neighbourhood of one service?
2. Is there any problem (Which are the associated heuristics)?
3. Is there any faults related to the system design/architecture?
4. What is the root problem, when A, B, C services are slow?
5. How are requests coming from the client?
6. How endpoints orders distributions are done?
7. What is the behaviour of the instances?
8. What is the length of each queue in a service?
Allthesequestionsrepresentneedsfeltbyoperatorswhenmonitoringandtroubleshoot-
ing distributed systems. To generate them, we gathered in meetings and discussed what
are the main needs of Development and Operations (DevOps) based on research, state
of the art tooling, related work developed in the past years and opinions from colleagues
working in the area. However, these initial questions were too general, therefore they were
passedthrougheverylanedefinedbefore. Thisrefinementleadedtothegenerationoffinal
state questions. Final questions, with their corresponding description (D) and a starting
point for the expected work (W) involved, are defined bellow:
1. Does any service present a significant change in the number of incoming requests?
2. Does any service present a significant change in the number of outgoing requests?
D. The number of requests are the number of calls performed to a service. These
metricsrepresentaveryimportantmeasurementforservicemonitoring,because
it measures the service usage in time.
W. To obtain these metrics, one must generate the service dependency graph
throughout defined time-frames and retrieve the number of connections be-
tween every node presented in the graph.
3. Does any service present a significant change in response time?
D. Response time represents the amount of time needed to respond to a call.
It is considered one of the most important measurements in systems because
represents their performance.
W. Get the response time for every span (difference between end and start time
present in the structure).
4. Is there a problem related to the work-flow of one (or more) requests?
D. Work-flow of one request represents the interaction path triggered throughout
the system.
W. Generate service dependency graph, retrieve work-flow paths presented in the
graph and gather information about the number of unique paths and type
variation.
5. How do requests are being handled by a specific service? (Identify services that are
experiencing unreliability periods)
29
Chapter 3
D. In the end, requests have success or not. This is represented by a status code
in Hypertext Transfer Protocol (HTTP) or an exception in Remote Procedure
Call (RPC). Measure the ratio of these values can help identify unreliability
periods in services.
W. Gather status codes or exceptions from spans and generate a ratio of success
and error.
6. Which services are the most popular in the system? (Number of established connec-
tions)
D. Popularity of a service stands for the number of established connections. This
measurement is important because a failure in a very popular service can com-
promise the entire system.
W. Generate service dependency graph, and calculate the degree of each node.
Services with higher degree are the most popular in the system.
7. Does any service present a significant change in the services it uses to fulfil requests?
D. Services tend to communicate with a set of other services. These services do
notchangeoften,therefore,patternsinservicecommunicationcanbeobserved.
If these patterns are violated without service redeployment and networking
changes, one might be facing a possible traffic redirection.
W. Generate service dependency graph, and retrieve the set of services that each
service communicates. Gathering these values in time, lead to a history of com-
munication between services and, therefore, pattern recognition can be applied
to detect strange variations.
8. Is there a problem related to the constitution of the system?
D. Constitution in microservices architecture represent which services are pre-
sented in the system. The study of entries and exits of services in the overall
system network can help identifying problems in system constitution.
W. Generate service dependency graph in consecutive time-frames and retrieve the
entry / exit of services. Variation analysis of this data can lead to detect
constitution problems presented in distributed systems.
9. Do traces follow OpenTracing specification? (Structural quality testing)
D. Structure quality is always an important factor when using some dataset to
analyse a system. This question aims to perform a structural test of spans
presented in tracing against the defined specification.
W. Produce a structural schema based on the proposed open source tracing speci-
fication – OpenTracing –, and check every span.
10. How is time coverage of tracing? (Coverability quality testing)
D. Timecoverageisanimportantaspectintracing, becausethismeasurementcan
pinpoint possible failures in system instrumentation.
W. In tracing, child spans should cover almost the total duration of their parent
span. To perform this test, a span tree for each trace must be assembled and
times ratios of the durations must be extracted.
30
Research Objectives and Approach
After having generated these final state questions, an analysis report was performed
in order to group them in similar fields of end-to-end tracing use cases [17]. Table 3.1
present the defined groups and the associated questions.
Table 3.1: Final state questions groups.
Group Question numbers
1. Anomaly detection 1. Does any service present a significant change in
the number of incoming requests?
2. Does any service present a significant change in
the number of outgoing requests?
3. Does any service present a significant change in
response time?
2. Steady state problems 4. Is there a problem related to the work-flow of
one (or more) requests?
5. How do requests are being handled by a specific
service? (Identify services that are experiencing
unreliability periods)
3. Distributed resource profiling 6. Which services are the most popular in the sys-
tem? (Number of established connections)
7. Does any service present a significant change in
the services it uses to fulfil requests?
8. Is there a problem related to the constitution of
the system?
4. Quality of tracing 9. Do traces follow OpenTracing specification?
(Structural quality testing);
10. How is time coverage of tracing? (Coverability
quality testing).
Table 3.1 presents us with questions grouped in four classes: anomaly detection, steady
stateproblems,distributedresourceprofiling andqualityoftracing. Questionsweregrouped
in these four mentioned classes due to their affinity. The first one, Anomaly detection,
is “diagnosis-related case that involves identifying and debugging problems related to
correctness (e.g., component time-outs or connection failures)”, therefore grouped ques-
tions are related with response time and number of calls performed to services. Secondly,
Steady state problems, is “another diagnosis-related, which involves identifying and de-
bugging problems that manifest in work-flows (and so are not anomalies)”, thus questions
are related with work-flow and status of requests. Thirdly, Distributed resource profiling,
is “identify slow components or functions.”, so questions associated with service usage
and system constitution. Finally, Quality of tracing, involve questions related to tracing
quality testing.
The following general questions were composed for each group:
• Group 1 - Is there any anomalous service?
• Group 2 - What is the overall reliability of the service?
• Group 3 - Which service consumes more time when considering the entire set of
requests?
• Group 4 - How can we measure the quality of tracing?
31
Chapter 3
From these general questions, we decided to tackle two groups: 1. Anomaly detection
and 4. Quality of tracing and therefore, the selected general questions were: 1. Is there
any anomalous service? and 4. How can we measure the quality of tracing?. Questions
presented in the remaining groups were not studied further in this research project.
The first question can be reduced to finding anomalies in observations of service or
system behaviour, namely metrics and morphology. In particular, we considered three
metrics: numberofincomingservicecalls,outgoingservicecallsandaverageresponsetime.
Our proposed solution in Chapter 4 - Proposed Solution must have this into consideration
– extract and analyse these metrics from tracing data.
For the second question, there are multiple ways to analyse quality in tracing. We
explore two directions, first performing a trace structure testing against the defined Open-
Tracing specification –structural testing –, to determine if the tracing data complies with
all the predefined requirements. Secondly, coverage testing for tracing data to determine
how much of the duration of Span is covered by its children – time coverability testing.
The first kind would be more valuable if the specification was stricter, however, changing
thestandardwasnotanoptionatthetimeasthedatahadexternalprovidence–discussed
in Chapter 7 - Conclusion and Future Work.
Next Chapter 4 - Proposed Solution covers our proposed solution, taking into consid-
erations the main problem, the data to process and the research questions to be answered
in this project.
32
Chapter 4
Proposed Solution
InthisChapter,wepresentanddiscussapossiblesolutiontobeimplementedregarding
themainproblemtosolveinthisresearch,thedatatoprocessandtheresearchquestionsto
be answered. To present the solution and explain it, we will cover some aspects considered
when defining a software based solution: functional requirements 4.1, quality attributes
(non-functionalrequirements)4.2,technicalrestrictions4.3andfinally,thearchitecture4.4
produced based on all previous topics.
The starting point for our proposed solution is the tracing data provided by Huawei.
Tracing must be ingested by an entry component, capable of extracting metrics from
tracing data. The outcome of this module are metrics and metadata in files to be further
processed by a second component. This second component has the duty of analysing the
output data from the first module, and point out service anomalies.
For a clear insight about our solution, the proposed approach in high level of abstrac-
tion is presented in the Figure 4.1.
Proposed approach
Data Analyser
OTP
Performs the analysis of
Metrics gathering from
the stored metrics and
Traaacaes tracing data. Processed point out service problems.
dMaMtMa
Figure 4.1: Proposed approach.
Figure 4.1 shows the proposed process order for tracing data. We expect to have
two main components, one for data extraction and another for data analysis. The input
for each are tracing data and processed data from the first component respectively. The
outcome is to answer the research questions defined in Section 3.2 - Research Questions.
Next Section 4.1 - Functional Requirements covers the functional requirements for this
solution.
33
Chapter 4
4.1 Functional Requirements
In software engineering, functional requirements define the intended function of a sys-
tem and its components. To present the functional requirements for our solution propo-
sition, an id, the corresponding name and its priority are provided. The notation used in
priority was based on the urgency that we expected from feature implementation. Three
priority levels were used: High, Medium and Low. Therefore, the functional requirements
for the proposed solution, sorted by priority levels, are presented in Table 4.1.
Table 4.1: Functional requirements specification.
ID Name Priority
FR-1 The system must be able to ingest tracing data from a files High
or external distributed tracing tools.