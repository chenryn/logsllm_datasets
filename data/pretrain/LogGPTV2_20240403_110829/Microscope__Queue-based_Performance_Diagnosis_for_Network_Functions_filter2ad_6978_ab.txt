(c) Queue length at the VPN
Figure 2: Impact propagation across NFs
3. Different impacts from similar behaviors. The same abnor-
mal behavior may have a different impact on performance. Figure 3a
shows a NAT and a Monitor both sending traffic to a VPN. The
NAT sends traffic at 0.25 Mpps while the Monitor sends traffic at
0.05 Mpps, both with 64-byte packets. We also send flow A to the
VPN directly. Figure 3b shows that all the flows experience dif-
ferent levels of packet losses during [1.6 ms, 2 ms] interval at the
VPN. Similar to the previous example, this is caused by interrupts
occurring at upstream NFs (the NAT and the Monitor). However,
it is hard to identify which upstream NF contributes more to the
problem, because both interrupts happen before the packet loss
period. The input rate changes in Figure 3c helps identify the causal
relations. The input rate from the NAT increases more than that
from the Monitor. This means the NAT’s interrupt is the dominant
contributor to packet losses.
The upshot is that it is not enough to simply correlate behaviors
of components together, especially when there are many concur-
rent microsecond-level abnormal behaviors. Rather, we need to
quantify the impact of these behaviors. In the example, in addition
to identifying those packet losses are correlated with interrupts at
both the NAT and the Monitor, we need to quantify each interrupt’s
392
2.2 Survey on Performance Diagnosis
To understand the reality of performance problems and diagnosis
of network functions today, we conducted a survey with 19 net-
work operators (from ISPs, data centers, and enterprises) in Janu-
ary 2020. Our complete survey form and results are published at [7].
Among the survey respondents, four belonged to small networks
(100K hosts). Below we describe the main findings.
These operators often face performance problems. In particu-
lar, five operators said they have to diagnose 10-100 performance
problems monthly, while four of them spend more than 12 hours
on performance diagnosis per month.
These problems are hard to diagnose because of diverse symp-
toms and root causes. Typical symptoms include: multiple NFs
experience problems (e.g., low throughput) at the same time (seven
operators experienced this), when the problems are intermittent
(nine operators), and when the problems only happen for one user
but not others (seven operators). Typical root causes include re-
source contention (7 operators saw this), traffic bursts (12 operators),
interrupts (5 operators), and other NF bugs (15 operators). One type
of tricky problems is caused by interactions between NFs. That
is, the problem manifests only when multiple NFs are running to-
gether, not while debugging individual NFs in isolation. There are
many different causes, such as upstream NFs’ output traffic affect-
ing downstream NFs (6 operators saw this), misconfiguration on
one NF affecting another NF (8 operators), or resource contention
(3 operators).
The top requirements for performance diagnosis tools are high
accuracy (9 operators) and low overhead (12 operators). Moreover,
many operators would like a ranked list of root causes (12 opera-
tors) where each cause indicates aggregated flows (7 operators) or
network functions (9 operators).
VPNCAIDA trafficFlow ANATInterruptBurstytraffic after interruptLow throughput of Flow A 00.10.20.30.40.50.6 0.7 0 0.5 1 1.5 2 2.5 3Time (ms)Traﬃc from NATFlow AInterrupt Input R  ate (Mpps)  0 50 100 150 200 250 300 350 0 0.5 1 1.5 2 2.5 3Queue lengthTime (ms)VPNMonitorNATHeavy trafficFlow ALight trafficInterrupts at the same timeHeavy burst after interruptNo burstLong latency for Flow A 0 20 40 60 80 100 120 140 160 0 1 2 3 4 5Packet dropsTime (ms)Traﬃc from NATTraﬃc from MonitorFlow AInterrupts at    NAT and     Monitor 00.10.20.30.40.50.6 0.7 0 1 2 3 4 5Time (ms)Traﬃc from NATTraﬃc from MonitorFlow A Input R  ate (Mpps) SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Gong, et al.
3 MICROSCOPE KEY IDEAS
Microscope is a performance diagnosis tool that identifies causal
relations for performance problems in a DAG of NFs. We make the
following key design decisions in Microscope:
Leverage queuing periods to understand long-lasting impacts
of problems at each NF. Since NFs are developed by different
vendors and we do not have access to NF internal codes, we propose
to focus on the queues between NFs to observe causal relations
between NFs and with traffic sources. Our examples in § 2 show
that queues can indicate the lasting impact of anomalous behav-
iors, their propagation across NFs, and quantify the impact from
multiple behaviors.
Our key insight is that when a packet experiences a long queue,
it is not only because of the current packets in the queue, but also
because of all the previous packets that contribute to the queue
buildup but already get processed. This is to say that if we had fewer
packets, the current queue length would be shorter. Therefore, we
introduce a queuing period which defines the time period from
the time when a queue starts building (from zero packets) to the
current time. As an example, consider Figure 1b, where for each
victim packet p arriving at time t, the queuing period of p starts
from 570 µs to t. By considering the entire queuing period, we can
determine the root causes that may not temporally overlap with
the observed problem.
Quantify causal relations based on packets received during
the queuing period: Our next step is to understand the causal
relations between anomalous behaviors at NFs and packets in the
queue. Generally speaking, packets are stuck in a queue for two
reasons: high input rate from upstream NFs or slow processing
rate at the current NF. We tell whether it is upstream NFs or the
current NF that contribute to the queuing and by how much, by
comparing the input rate or processing rate of an NF during the
queuing period to the peak processing rate of the NF. For example,
in Figure 2c, we attribute the queue buildup at the VPN to the NAT
because of the high input rate from the NAT. In Figure 3c, we tell
the relative contribution of the NAT and the Monitor by checking
their respective input rate changes.
Furthermore, the impact of abnormal behaviors is propagated
across NFs through packets. Therefore, we propose to trace back
the journey of all the packets in the queue and analyze how quickly
these packets are processed at each NF.
Aggregate causal patterns: Given many fine timescale anoma-
lous behaviors and several performance problems (e.g., tail latency
packets), it is important for operators to focus on the most im-
portant problems and root causes. We propose a causal relation
aggregation algorithm that automatically generates a ranked list of
causal patterns with scores:  →
: score. This is based on AutoFo-
cus algorithm [25], but we modify it to aggregate causal patterns
instead of traffic clusters.
4 MICROSCOPE DESIGN
Microscope collects packet’s timestamps, queuing, and flow infor-
mation between NFs without accessing internal NF codes in the
runtime (Table 1). Based on the collected information, Microscope
393
Name
Timestamps
Batch size
Flow information
Packet IDs
Explanation
timestamps when an NF reads or
writes a batch of packets to each queue
the batch size when the NF reads
a batch of packets from the queue
e.g., source, destination IP
addresses and port numbers
e.g., IPID
Table 1: Information collected by Microscope during run-
time
Figure 4: Microscope Architecture
selects victim packets which experience high latency, low through-
put, or losses, and diagnose their root causes.
Figure 4 shows that Microscope performs offline diagnosis in
four steps: (1) For each victim packet at an NF where the packet
experiences local abnormal performance, Microscope performs lo-
cal diagnosis to understand whether the root cause is at the local
NF or upstream NFs. The key idea is to leverage queuing periods
to tell if the packet is delayed by low processing rates at the local
NF or high input rates from upstream NFs (§4.1). (2) If the problem
of the victim packet is caused by high input rates, Microscope per-
forms propagation analysis to identify the culprit upstream NFs.
Microscope inspects the packets in the queuing period (i.e., PreSet
packets) and analyze how the timespan of these packets change at
each NF (§4.2). (3) When an NF contributes to the high input rate
of the Preset packets, this NF could also experience performance
problems. Therefore, Microscope recursively diagnoses this NF
using steps (1) and (2) (§4.3). (4) Microscope aggregates causal
relations between culprit  to victim  into
a small list of causal relation patterns using AutoFocus algorithms
(§4.4). We now describe each step in detail.
4.1 Local Diagnosis
We focus on victim packets which experience bad performance (e.g.,
high latency or low throughput at the 99th percentile) or simply
get lost (i.e., when we do not have records for them at some NFs).
For each victim packet, we look at all the NFs on its path where
its local performance is abnormal. Similar to NetMedic [36], we
…Low processing rate? (local problem)High input rate? (upstream problem)Victim Causal relations…Causal relation patternsLocal diagnosis (sec 4.1)Propagation diagnosis (sec 4.2)Pattern aggregation (sec 4.4)Recursion (sec 4.3)Victim NF nodeVictim packetNF nodeABCDPreSetDDBDACDCADCDDCVictim DCulprit PreSetpacketQueuing periodDDBATimespan analysis…Microscope: Queue-based Performance Diagnosis for Network Functions
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
f
+ S
i
f
i and S
p = ni − np and ni − np is equal to the queue length.
f
Note that we define S
p to make sure they together cover
all the packets that contribute to the queue buildup in T . That is,
f
S
i
4.2 Propagation Diagnosis
f
Suppose when we diagnose a victim packet p at an NF f , the S
i
is positive (e.g., SV P N
> 0 in Figure 8). It means that the input
workload contributed to the queue build up at f . The reason behind
higher input workload could be any of the upstream NFs (which
could have ramped up their processing rates) or the traffic sources
themselves. In this section, we describe the propagation analysis
algorithm we run to identify the causal relations amongst NFs.
Assume when p arrives at NF f , the queuing period has lasted for
T . During T , there are ni(T) packets coming from upstream. We call
this set of packets PreSet(p). Our goal is to understand the history
of PreSet(p) and why these packets take T time at NF f . We trace
back to the upstream NFs that PreSet(p) traverses. To diagnose, we
define timespan of PreSet(p) at an NF as the time between the first
and last packet that leaves the NF. Let Tsource, TA, TB, and TC be
the timespan of PreSet(p) at traffic source, NF A, NF B, and NF C
respectively. We first discuss the case where PreSet(p) traverses a
chain of NFs, and then generalize to a DAG of NFs.
PreSet(p) traverses a chain of NFs. Suppose PreSet(p) tra-
verses a chain of NFs (A, B, C, f ). There could be many fine-
timescale abnormal behaviors happening at each NFs. For example
in Figure 6, there is an interrupt at A, and cross traffic at C. When
PreSet(p) arrives at A, A has an interrupt so these packets have to
wait, and are processed back-to-back after the interrupt finishes.
This squeezes out the inter-packet gaps, so the timespan reduces
from Tsource to TA. When they arrive at B, B is slower than A, so it
takes a longer time to process these packets, increasing the times-
pan to TB. When they arrive at C, there is a queue, so packets in
PreSet(p) have to wait, and their timespan is squeezed to TC. These
packets cause a bursty input to f , because f cannot process them in
TC time interval. Overall, TC is smaller than the expected timespan
of the ni(T) packets (Texp = ni(T)/r
f
i ). A smaller timespan causes
a traffic burst at f , which affects p. So we need to account for the
reduction from Texp to TC.
Note that Microscope only cares about the overall timespan,
not the distribution of the n packets within the timespan. This is
because, our goal is to diagnose victim packet p at f , no matter
how the packets in PreSet(p) distribute within the timespan at
each upstream NF, they cause the same effect at f . Moreover, the
timespan is easier to measure and compare across NFs than packet
distributions.
The next step is to attribute this timespan reduction (Texp−TC) to
f
the source, NF A, B, and C. We split score S
i proportionally based on
their relative timespan reduction from previous hops. For example,
C’s score Sf ←C gets a fraction TB−TC
f
i , because it reduces the
Texp−TC
timespan by TB −TC out of the total reduction Texp −TC. Similarly,
the source’s score Sf ←source gets a fraction of Texp−Tsour ce
. If
Texp−TC
Sf ←source is above zero, we define PreSet(p) as the culprit packets
at the source.
The timespan is not always decreasing. For example, B increases
the timespan from TA to TB. In this case, we treat the timespan
of S
Figure 5: Diagnosing lasting impact at an NF (Section 4.1).
claim abnormality if the NF’s performance is beyond one standard
deviation computed over recent history.
Suppose a victim packet p has abnormal performance (e.g., a long
latency) at NF f . Our goal is to identify all the abnormal behaviors
(at f or upstream NFs) which impact the packet p. These behaviors
do not have to overlap with packet p in time.