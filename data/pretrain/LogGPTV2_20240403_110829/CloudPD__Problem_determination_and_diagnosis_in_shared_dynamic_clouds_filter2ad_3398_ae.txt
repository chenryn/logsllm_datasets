n
o
c
e
s
(
e
m
T
s
i
s
y
l
a
n
A
i
140
120
100
80
60
40
20
0
Event Generation
Problem Determination
Problem Diagnosis
CloudPD B1
B2
B3
B4
)
s
d
n
o
c
e
s
(
e
m
T
s
i
s
y
l
a
n
A
i
100
80
60
40
20
0
Event Generation
Problem Determination
Problem Diagnosis
CloudPD B1
B2
B3
B4
Method
Method
(a) Hadoop analysis time
(b) Olio analysis time
Figure 11: Analysis time of each stage of CloudPD.
clusters. We notice that the analysis time grows sub-linearly
with the number of VMs, with the increase in analysis time
being mainly in the Event Generation phase. Further, as noted
before, event generation time can be reduced by parallelizing
it across different VMs. This experiment establishes the ef-
fectiveness of CloudPD’s multi-layer approach, allowing it to
scale to larger systems with more VMs and servers.
F. Trace-driven Fault Generation Results: Case Study
Our next set of experiments are conducted to study the
effectiveness of CloudPD in a real cloud setting, where cloud
conﬁguration actions spontaneously generate faults. Real cloud
setting refers to a virtualized platform running production
applications, where cloud manager controls activities like VM
provisioning/reconﬁguration. It emulates real trace (CPU and
memory utilization time-series) variations. Our cloud testbed
is managed by a cloud management stack that can provision
virtual machines and perform dynamic consolidation to reduce
power. We used the pM apper consolidation manager [20],
which has been studied by multiple researchers and pro-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:49 UTC from IEEE Xplore.  Restrictions apply. 
TABLE IX: Comparing end-to-end diagnosis effectiveness for trace-driven case study.
# of Correct
Normal
Detections
# of Correct
Anomalous
Detections
# of Correct
Event
Generations
# of Total
Predicted
Anomalies
Recall
Precision
Accuracy
FAR
18
10
21
11
13
21
14
23
21
15
24
25
27
24
26
0.78
0.43
0.91
0.48
0.57
0.75
0.40
0.78
0.46
0.50
0.77
0.42
0.84
0.47
0.53
0.25
0.60
0.22
0.54
0.50
Method
CloudPD
B1
B2
B3
B4
Event Generation
Problem Determination
Problem Diagnosis
)
s
d
n
o
c
e
s
(
e
m
T
s
i
i
l
s
y
a
n
A
35
30
25
20
15
10
5
0
67
58
67
60
60
)
s
d
n
o
c
e
s
(
e
m
T
s
i
35
30
25
20
15
10
5
0
Event Generation
Problem Determination
Problem Diagnosis
1
3
6
i
l
s
y
a
n
A
3
6
9
15
Number of VMs
Number of VMs
(a) Hadoop VM scaling
(b) Olio VM scaling
Figure 12: Effect of VM scaling on the analysis time of CloudPD.
ductized. The consolidation involves reconﬁguration of VM
resource allocations as well as changing the placement of VMs
on physical servers every 2 hours. The cloud testbed hosts
a Hadoop cluster consisting of 10 VMs and an Olio cluster
consisting of 6 VMs, with 4 VMs for the web server tier, and
2 VMs for the database tier.
We used two real traces from a production data center of a
Fortune 500 company running enterprise applications to drive
the workload for Hadoop and Olio. The two traces contain a
time-series of CPU and memory utilization across the servers,
with a data point every 15 minutes for a total duration of 24
hours, which is also the duration of our experiment. We built
a proﬁle of Hadoop that captured the relationship of workload
size with CPU and memory utilization. We similarly created
a proﬁle of Olio to capture the relationship of the number of
concurrent users with CPU and memory utilization of the Web
and database tier. In a given interval, we ran Hadoop with a
data size that generates the CPU utilization provided by the
trace for that interval. Similarly, we ran Olio with the number
of users such that the utilization of the web server matched
with what was speciﬁed by the trace for that interval. Hence,
this experiment captures changes in workload intensity that
happen in real clouds. The resource proﬁles are captured by
the following equation obtained through linear regression:
CP U (Olio − W eb) = U sers ∗ 0.1 + 5
CP U (Olio − DB) = U sers ∗ 0.035 + 7.5
CP U (Hadoop) = DataSize ∗ 2.83 + 12.9
(5)
CloudPD independently monitors the cluster and identiﬁes
cloud related anomalies, workload intensity and workload
mix changes, as well as application anomalies for 15 minute
intervals. We randomly injected anomalies in some of these
intervals as listed in Table V. Apart from the injected applica-
tion anomalies, we noticed that 4 intervals experienced invalid
VM migrations and 7 intervals experienced invalid VM sizing
anomalies due to cloud reconﬁguration (we did not inject
any cloud related anomalies). These were determined to be
anomalous as the application latency and throughput deviated
by 11% and 9% for Hadoop and Olio, respectively. The sizing
faults were a result of prediction error and the live migration
faults were due to high resource utilization on the servers.
Figure 13 provides a detailed view of the case study. For
each of the 96 intervals (shown in the X-axis), the left Y1-
axis shows the ground truth of intervals that had anomalies
along with anomaly predictions made by CloudPD and the four
baselines. Both correctly identiﬁed anomalies (true positives)
and non-faulty events wrongly marked as anomalies (false
positives) are marked. On the Y2-axis, we plot the normalized
application latency, where a value of 1 denotes the interval
with the highest average job latency. Observe that anomalous
intervals (marked under gT ruth) have a higher latency than
normal intervals. One can observe that B1, B3 and B4 have
many false positives, which is consistent with our other studies
using synthetic fault injection (Section V-E).
true−positive
false−positive
B4
B3
B2
B1
CloudPD
gTruth
1
y
c
n
e
t
a
L
.
m
r
o
N
0
0
10
20
30
40
50
60
70
80
90
Time (15 minutes intervals)
Figure 13: Time-series showing the effectiveness of CloudPD and
other base schemes in detecting faults in a 24-hour case study.
The performance metrics for CloudPD and the baselines in
terms of recall, precision, accuracy and false alarm rate (FAR)
are summarized in Table IX. CloudPD is able to correctly
diagnose 18 out of the 23 anomalous intervals and identify
67 out of the 73 normal intervals as normal. This is close
to the performance of baseline B2 and signiﬁcantly better
than the other three baselines. This ability of CloudPD is
due to its unique characteristic to better manage and deal
with shared resources such as disk and cache in segregating
cloud anomalies from application faults and normal workload
change. Table X lists the undetected anomalies by CloudPD
and the baselines. CloudPD fails to detect the 2 disk hog
anomalies, the reason for which can be attributed to the fact
that the VMs share a SAN storage and the deviation of the
storage utilization values from normal was not signiﬁcant
enough for the event generation phase of CloudPD to raise an
alarm. CloudPD also missed detecting 2 invalid resizing events
and an invalid VM migration. These were marginal anomalies
which caused the application latency to be high enough to be
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:49 UTC from IEEE Xplore.  Restrictions apply. 
classiﬁed as an anomaly, but the correlation values were not
deviant enough for CloudPD to detect them.
TABLE X: Undetected anomalies for trace-driven case study.
Method
Undetected anomalies
REFERENCES
[1] K. Mahendra, G. Eisenhauer, and et al., “Monalytics: Online Monitoring
and Analytics for Managing Large Scale Data Centers,” in ICAC, 2010.
[2] P. Bodik, M. Goldszmidt, A. Fox, D. Woodard, and H. Andersen, “Fin-
gerprinting the Datacenter: Automated Classiﬁcation of Performance
Crises,” in EuroSys, 2010.
CloudPD 2 disk hog + 2 invalid VM sizing + 1 invalid VM migration
[3] T. Wood, E. Cecchet, and et al., “Disaster Recovery as a Cloud Service:
B1
B2
B3
B4
(total 5)
2 network hog + 2 disk hog + 2 cache hog + 3 invalid VM
sizing + 4 invalid VM migration (total 13)
1 disk hog + 1 invalid VM sizing (total 2)
2 disk hog + 2 cache hog + 2 memory hog + 1 CPU hog
+ 2 invalid VM sizing + 3 invalid VM migration (total 12)
2 disk hog + 2 cache hog + 1 memory hog + 1 CPU hog
+ 2 invalid VM sizing + 2 invalid VM migration (total 10)
TABLE XI: CPU usage (% CPU time), memory usage and
network bandwidth overhead of data collection using sar tool.
% CPU Memory (MB)
Network BW (KB/s)
Hadoop
Olio
Case study
0.35
0.18
0.41
0.80
0.44
0.92
37.8
14.5
40
G. Diagnosis Overheads
CloudPD uses cloud resources for diagnosing faults. We
quantify the overhead of CloudPD in terms of the CPU,
memory and network bandwidth usage for our experiments
with synthetic faults as well as the trace-driven case study. We
report the resource utilization averaged across VMs and over
the duration of the experiment (24 hours) in Table XI. We
observe that CloudPD introduces minimal overhead on the sys-
tem. Our experimental study thus establishes the effectiveness
of CloudPD to accurately diagnose application, system, and
cloud-induced faults quickly, and with low system overhead.
VI. CONCLUSIONS
In this paper, we proposed a light-weight, automated, and
accurate fault detection and diagnosis framework, called
CloudPD, for shared utility clouds. CloudPD uses a layered
online learning approach to deal with the higher occurrence
of faults in clouds. We introduced the notion of operating
context, which is essential to identify faults that arise due to
the shared non-virtualized resources. CloudPD monitors a wide
range of metrics across VMs and physical servers, compared
to only CPU and memory metrics monitored by most prior
work. CloudPD diagnoses anomalies using pre-computed fault
signatures and allows remediation to be integrated with a cloud
management stack in an automated fashion. We conducted
an extensive evaluation of CloudPD on three representative
cloud workloads – Hadoop, Olio, and RUBiS, as well as
a trace-driven case study. The results show that CloudPD
achieves high accuracy with low false alarms in detecting and
distinguishing cloud-related faults from application anomalies
and workload changes, within tens of seconds. To the best of
our knowledge, CloudPD is the ﬁrst end-to-end fault manage-
ment system that can detect, diagnose, classify and suggest
remediation actions for virtualized cloud-based anomalies.
ACKNOWLEDGMENT
We thank the anonymous reviewers, Adwait Jog, Mahshid
Sedghi, Nachiappan Chidambaram Nachiappan and Onur
Kayiran for their valuable comments towards improving this
paper. This research is supported in part by NSF grants #
(1213052, 1205618, 1152479, 1147388, 0702617) and re-
search grants from Google and Intel.
Economic Beneﬁts and Deployment Challenges,” in HotCloud, 2010.
[4] M. Y. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer, “Pinpoint:
Problem Determination in Large, Dynamic Internet Services,” in DSN,
2002.
[5]
IT Downtime Financial Cost, http://tinyurl.com/itdowntime-cost.
[6] Compuware Corporation, “Performance in the Clouds,” White paper,
2011.
[7] T. Benson, S. Sahu, A. Akella, and A. Shaikh, “A First Look at
Problems in the Cloud,” in HotCloud, 2010.
[8] M. Gallet, N. Yigitbasi, and et al., “A Model for Space-correlated
Failures in Large-scale Distributed Systems,” in EuroPar, 2010.
[9] Y. Tan and et al., “PREPARE: Predictive Performance Anomaly Pre-
vention for Virtualized Cloud Systems,” in ICDCS, 2012.
[10] W. Chengwei and et al., “Online Detection of Utility Cloud Anomalies
using Metric Distributions,” in NOMS, 2010.
[11] C. Wang, K. Viswanathan, L. Chodur, V. Talwar, W. Satterﬁeld, and
K. Schwan, “Evaluation of Statistical Techniques for Online Anomaly
Detection in Data Centers,” in IEEE IM, 2011.
[12] H. Kang, H. Chen, and G. Jiang, “PeerWatch: A Fault Detection and
Diagnosis Tool for Virtualized Consolidation Systems,” in ICAC, 2010.
[13] H. Kang, X. Zhu, and J. Wong, “DAPA:Diagnosing Application Perfor-
mance Anomalies for Virtualized Infrastructures,” in Hot-ICE, 2012.
[14] S. Agarwala, F. Alegre, K. Schwan, and J. Mehalingham, “E2EProf:
Automated End-to-End Performance Management for Enterprise Sys-
tems,” in DSN, 2007.
[15] L. Cherkasova and et al., “Anomaly? Application Change? or Workload
Change? Towards Automated Detection of Application Performance
Anomaly and Change,” in DSN, 2008.
[16] G. Bronevetsky, I. Laguna, B. Supinski, and S. Bagchi, “Automatic Fault
Characterization via Abnormality-enhanced Classiﬁcation,” in DSN,
2012.
[17] G. Jing, J. Guofei, C. Haifeng, and H. Jiawei, “Modeling Probabilistic
Measurement Correlations for Problem Determination in Large-Scale
Distributed Systems,” in ICDCS, 2009.
[18] S. Zhang, I. Cohen, J. Symons, and A. Fox, “Ensembles of Models
for Automated Diagnosis of System Performance Problems,” in DSN,
2005.
[19] D. Pelleg, M. Ben-Yehuda, R. Harper, L. Spainhower, and T. Adeshiyan,
“Vigilant: Out-of-band Detection of Failures in Virtual Machines,”
SIGOPS Oper. Syst. Rev., 2008.
[20] A. Verma, P. Ahuja, and A. Neogi, “pMapper: Power and Migration
Cost Aware Application Placement in Virtualized Systems,” in ACM/I-
FIP/USENIX Middleware, 2008.
[21] A. Verma, G. Kumar, R. Koller, and A. Sen, “CosMig: Modeling the
Impact of Reconﬁguration in a Cloud,” in IEEE MASCOTS, 2011.
[22] KNN
Classiﬁer,
http://en.wikibooks.org/wiki/Data Mining
Algorithms In R/Classiﬁcation/kNN.
[23] Z. Li, X. Wang, Z. Liang, and M. Reiter, “AGIS: Towards Automatic
Generation of Infection Signatures,” in DSN, 2008.
[24] VMware, “Vmkperf for VMware ESX 5.0, 2011.”
[25] vSphere, “Powercli Cmdlets,” http://tinyurl.com/powercli-cmd.
[26] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly Detection: A
Survey,” ACM Computing Surveys, vol. 41, 2009.
[27] B. Sharma, P. Jayachandran, A. Verma, and C. Das, “CloudPD: A
Framework for Problem Determination and Diagnosis in Shared Dy-
namic Clouds,” Penn State, Tech. Rep. CSE #12-001, 2012.
[28] Apache Hadoop, “Open Source BigData Framework,” http://hadoop.
apache.org.
[29] W. Sobel and et al., “Cloudstone: Multi-platform, Multi-language
Benchmark and Measurement Tools for Web 2.0,” in CCA, 2008.
[30] RUBiS, “E-commerce Application,” http://rubis.ow2.org.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:49 UTC from IEEE Xplore.  Restrictions apply.