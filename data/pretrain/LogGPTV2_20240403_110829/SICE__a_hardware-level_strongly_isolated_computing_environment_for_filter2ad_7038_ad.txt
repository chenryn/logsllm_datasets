Availability: SICE does not provide protection against at-
tacks that target the availability of the isolated environment.
Example attacks include perturbing the isolated environ-
ment through system reboots or denying it network access.
However, this type of attacks is easily detectable by SICE
and its remote users (e.g., from the lack of response of the
isolated environment), and can be easily thwarted by remov-
ing the malicious code from the legacy host.
The TCB of SICE: SICE aims to minimize and enhance
the protection of the TCB of the isolated environment so as
to maximize its security.
The TCB of the isolated environment consists of the hard-
ware, the BIOS, and the SMM. Using the SMM gives SICE
two main advantages over microhypervisor-based isolation.
First, SICE TCB enjoys the hardware protection provided
for the SMRAM. Second, the SMM’s attack surface is much
smaller than that of microhypervisors. Thus, The TCB of
the isolated environment is better protected and less compli-
cated than that of microhypervisors. Section 1.2 compares
the TCB of SICE with that of microhypervisors in detail.
Trusting the BIOS is required to start the trust chain
based on the static root of trust management (SRTM) tech-
nique. However, recent incidents [17] show that the SRTM
can be compromised. Nevertheless, SICE can adopt the
Dynamic root of trust management (DRTM) to start the
trust chain, which consequently eliminates the BIOS from
the TCB. SICE can use the DRTM to invoke a trusted code
that securely initializes the SMRAM, given that it will not
be locked by the BIOS. A similar technique was used by
Trustvisor [19] to initialize its TCB.
SICE requires the legacy host to trust the security man-
ager(s). However, this does not make the security guarantee
provided to the legacy host weaker than microhypervisor-
based approaches such as NOVA, which assume trust in a
thin hypervisor with typical duties. Indeed, SICE and the
microhypervisor-based approaches provide similar security
guarantees for the legacy host, while SICE additionally also
provides a stronger protection for the isolated environments,
as discussed in Section 1.3.
4.4.2 SMM Security
Despite being an integral part of the TCB of SICE, the
SMM was not designed to provide security services. It was
originally designed to create a shadow environment to run
hardware management tasks.
In the following, we discuss
some of the important issues that need to be considered
before deploying SICE, or any other system that relies on
the SMM, in a production environment.
Legacy SMM Tasks: Intuitively, the TCB of SICE in-
cludes any code that exists in the SMRAM. To keep the
TCB minimal, legacy system management tasks need to be
eliminated from the SMM. This does not mean that SICE
needs to completely eliminate them from the system.
In-
stead, SICE can simply forward speciﬁc SMIs to (possibly
relocated) legacy SMM code after taking the required mem-
ory protection measures.
Previous SMM Attacks: As mentioned in Section 3,
there have been attempts to subvert the SMRAM using
cache poisoning [6,33]. Although these attacks can be easily
prevented by using proper hardware conﬁguration, it shows
that hardware vendors should undergo a rigorous review of
the SMM security to avoid any further security problems.
It is worth mentioning that the implication of SMM at-
tacks is beyond the systems that rely on the SMM for secu-
rity. It is shown in [7] that a subverted SMM can be used
to host a stealthy rootkit that undermines the security of
any system. In general, trusting the underlying hardware,
including the SMM, is imposed on all software systems.
Hardware Vendors Cooperation: As mentioned in Sec-
tion 4, implementing SICE requires hardware vendors to al-
low adding its code to the SMI handler before locking the
SMRAM. In fact, we advocate that SICE should be entirely
implemented by hardware vendors. In this case, the system
BIOS will be responsible for initializing SICE upon booting
the system. This guarantees that SICE will always be com-
patible with the speciﬁcation of the underlying hardware.
5. SICE PROTOTYPE: AN ISOLATED VM
In this section, we present a SICE prototype implemented
on an IBM LS22 blade server, which is equipped with two
2.7GHz AMD Opteron 10h family quad-core processors. We
use an Ubuntu 9.10 Linux as the OS. In our prototype, we
replace the original SMI handler with SICE’s SMI handler.
Implementing the core functions of SICE’s SMI handler,
383Qemu
Guest VM
KVM
Shared 
Memeory
Security 
Manager
Isolated 
SMI 
Handler
Host
Environment
SMRAM
Hardware
Figure 5: SICE Prototype
which are to prepare and enter the isolated environment, re-
quires around 300 LOC (excluding cryptographic libraries)
As mentioned in Section 1.3, the isolated environment can
support running any software, ranging from a single program
to a complete VM. To run a single program inside SICE, it
needs to be instrumented so that it directly runs on top of
the security manager. The required instrumentation is simi-
lar to that required by previous research on running isolated
security sensitive code (e.g., Flicker [20], Trustvisor [19]).
In our prototype, we use SICE to run a Linux VM, rather
than running an instrumented single program. This is to
demonstrate the ﬂexibility provided to the isolated environ-
ment. Running a whole VM requires the ability to support
a diverse set of applications, frequent context switching, and
a large range of protected memory.
Figure 5 shows our SICE prototype. The security manager
plays the role of the hypervisor in the isolated environment.
It uses hardware assisted virtualization to manage and run
the VM as the isolated workload. All hardware manage-
ment functions are delegated to the legacy host, speciﬁcally,
a modiﬁed version of Qemu/KVM [18] running in the legacy
host. Qemu is a program that emulates hardware periph-
erals. KVM is a kernel module that manages other VM
operations like scheduling and memory management.
We modiﬁed KVM so that it uses a shared memory to
send the required VM conﬁgurations to the security man-
ager. The same shared memory is used by the security man-
ager to pass the information required to request hardware
services from the legacy host. This architecture is mainly
chosen to simplify the implementation eﬀorts. In our proto-
type, the security manager is composed of around 2.1 KLOC,
which is comparable to the size of current microhypervisors.
Our implementation provides both network interface and
serial port emulation using Qemu. Networking is necessary
to allow the VM to communicate with remote users, while
the serial port is used as a console for debugging.
Our prototype does not support graphic display emula-
tion, which is not necessary for cloud computing applica-
tions. Moreover, we do not support disk drive emulation
because our experiments do not need a permanent storage.
Supporting a disk drive emulation is straightforward based
on the technique used to implement the network interface
emulation. However, the main question is how to secure the
disk access from a compromised legacy host. The answer
is dependent on the isolated workload rather than the isola-
tion mechanism. For instance, security sensitive applications
can either use full disk encryption (similar to Bitlocker [8]),
or selectively encrypt secret ﬁles only (e.g., a ﬁle that con-
tains user passwords). Other applications only need to keep
secret data in memory without a permanent storage (e.g.,
web servers that process online purchases without storing
customer credit card numbers).
In the following, we present more details about our pro-
totype implementation.
5.1 Preparing and Initializing the VM
As discussed in Section 4.1.1, the legacy host provides
SICE with the initial VM image, composed of the VM kernel
and the initial ram disk. The SMI handler copies this image
to the SMRAM. KVM also places the required VM conﬁg-
uration parameters (e.g., RAM size) in the shared memory
then triggers an SMI to initialize the isolated workload. As
mentioned in Section 4.3, the SMI handler measures the ini-
tial VM image to attest to its integrity to the remote owner
of this VM. The integrity evidence should be extended to
include the VM conﬁguration parameters passed by KVM.
Some of these parameters (e.g., the VM execution entry
point, initial register values) are critical to its integrity.
The security manager prepares the VM page tables and
its virtual machine control block (VMCB), based on the pro-
vided conﬁguration parameters. The page tables are crafted
using AMD’s nested page tables (NPT) [1], which adds an-
other layer to the virtual-to-physical memory translation.
5.2 Handling VM Exits
Certain events force the VM to exit its operations (e.g.,
external interrupts, page faults). These events need to be
handled by the hypervisor.
We forward most of the VM exit events to KVM in the
legacy host. Nevertheless, the security manager handles
some speciﬁc VM exits that do not require much computa-
tion to avoid unnecessary context switching. Among these
VM exit events are control register accesses and requests to
execute privileged instructions like CPUID or INVD.
Other VM exists (e.g., writing to an IO port, accessing
an IO memory) are directly forwarded to the legacy host.
To preserve the VM conﬁdentiality, the security manager
should only send information that is necessary for handling
the VM exits (e.g., VM exit reason and error code). This
information is sent to the KVM in the legacy host through
an established communication channel between the isolated
environment and the legacy host.
5.3 Communication with the Legacy Host
Establishing a communication channel with the legacy
host is not managed by the SMI handler. Instead, it is di-
rectly managed by the security manager and the legacy host.
As discussed in Section 4, a shared memory that is outside
the SMRAM protection is used to establish the communica-
tion between the legacy host and the isolated workload.
Signaling is required between the two environments to
send notiﬁcations that data is placed into the shared mem-
ory. In the time-sharing mode, we use the context switching
as the method of signaling. In the multi-core mode, IPIs be-
tween the host and the isolated cores are used for signaling,
as discussed in Section 4.2.1.
5.4 Using Hardware Peripherals
In our prototype, hardware peripherals in the isolated
workload are emulated using Qemu in the legacy host. There
are three main methods to control a hardware peripheral:
384IO ports, IO memory, and Direct Memory Access (DMA).
When the VM accesses an IO port or an IO memory, a VM
exit occurs and the control is transferred to the host to em-
ulate the hardware access.
DMA requests work diﬀerently because DMA is supposed
to directly read or write physical memory. In legacy VMs,
Qemu is granted full access to the VM physical memory to
emulate DMA accesses. However, since Qemu is located in
the legacy host, SICE prevents it from directly accessing
the memory of the isolated environment. Thus, our proto-
type modiﬁes Qemu to send a request to the security man-
ager with the address, size, and type (read or write) of the
emulated DMA. The security manager in turn copies the
required memory between the VM memory and the shared
memory. To preserve the conﬁdentiality of the isolated work-
load, the security manager only allows Qemu to retrieve or
modify VM memory areas assigned to DMA operations.
In our prototype, the security manager handles DMA ac-
cess to simplify our implementation. Nevertheless, this op-
erations should be directly handled by the isolated VM by
allowing it to directly access a part of the shared memory,
which will reduce the tasks required from the security man-
ager, and consequently reduce its code size.
5.5 Attestation
As mentioned in Section 4.3, the SMI handler should be a
part of a trusted boot process. However, the LS22 servers,
used for our implementation prototype, are not equipped
with a TPM. Due to the lack of a TPM, the attestation
process is not included in our prototype. Nevertheless, it is
worth mentioning that static attestation using the TPM, sig-
nature key generation and signing are all known techniques
that have been implemented previously.
It is worth mentioning here that implementing these cryp-
tographic operations may increase the size of the code base
of SICE. For instance, a typical SHA1 library is around
120 LOC. Other cryptographic functions (e.g., generating
an RSA signature) can be done using the TPM.
6. EXPERIMENTAL EVALUATION
We perform a set of experiments to evaluate the perfor-
mance of SICE. There are two anticipated sources of per-
formance overhead associated with SICE. The ﬁrst is the
direct overhead resulting from entering/exiting the isolated
environment. The second is the indirect overhead that re-
sults from the cache and TLB ﬂushing required for SICE
operations. On the other hand, running the isolated envi-
ronment outside the SMM avoids any execution slow-down
and is not anticipated to cause any performance overhead,
compared to running the same workload without SICE.
In the rest of this section, we present a measurement of
the anticipated overhead. First, we measure the execution
time needed for a full context switching to and from the iso-
lated environment. Second, we use the SICE prototype (See
Section 5) to compare the performance of the isolated guest
VM with the same VM running without SICE isolation.
6.1 SICE Execution Time
We measure the execution time needed to perform each
of the four major steps of entering and exiting the isolated
environment. The measured steps include: (1) triggering
the SMI, (2) preparing the isolated environment by the SMI
handler, (3) entering the isolated environment, and (4) re-
turning to the legacy host. These measures are obtained
from the average of 100 runs.
To precisely measure the end-to-end execution time of
each step, we use the RDTSC instruction to read the proces-
sor’s Time Stamp Counter (TSC). We then convert cycles
to microseconds based on the TSC speed (2.7GHz in our
experimental platform).
Table 1: SICE execution time
Operation
Triggering an SMI
Preparing the isolated env.
Entering the isolated env.
Exiting the isolated env.
Total (≈)
Time (in µs)
6.8
20.7
9.3
30.1
67
Std. Dev.
0.074
0.155
0.233
0.644
Table 1 shows the experimental results for the time-sharing
mode. Triggering an SMI and switching the processor con-
text from the legacy host to the SMI handler needs an aver-
age of 6.8 µs. The SMI is invoked by the local APIC of the
processor core by sending an IPI to all cores on the system.
The next step, which is to prepare the isolated environ-
ment, needs an average of 20.7 µs. The latency of this step
is relatively high given that this step only requires chang-
ing a few entries in the page tables, modifying the interrupt
vector table descriptor, and verifying the values of several
registers and MSRs. We anticipate that this relatively high
latency is caused by a slower processing speed in the SMM.
A similar observation was made in [2].
The next step is entering the isolated environment. In this
step, we execute the “return from SMM” instruction (RSM) to
jump to the security manager. The time needed for this step
is 9.3 µs, which is relatively high. We anticipate this latency
is due to the change of the processor execution environment,
particularly the page tables, which leads to invalidating all
cache and TLB entries.
Finally, the time needed to return from the isolated en-
vironment to the legacy host is 30.1 µs, which is similar to
executing all the previous steps in the reverse order (i.e.,
from the isolated environment to the legacy host). In total,
the end-to-end time required to enter and exit the isolated
environment is 67 µs.
In the time-sharing mode, the end-to-end execution time
overhead obtained in this experiment occurs on every con-
text switch between the isolated environment and the legacy
host. A context switch is required every time the isolated
environment needs a service from the legacy host. In con-
trast, it is a one-time overhead in the multi-core mode.
6.2 Isolated Environment Performance
Our next experiment is to evaluate the overall perfor-
mance overhead on the isolated workload. We run this ex-
periment using the same system conﬁguration of our pro-
totype system, discussed in Section 5. In our experiments,
we assign a 256 MB to the VM. Our VM runs Linux ker-
nel v2.6.28. and a ram disk that is equipped with BusyBox
v1.10.2 [4]. (BusyBox is a program that combines common
utilities into a single executable.)
To evaluate the performance of the guest VM, we assemble
a set of tests and run each test on the same guest VM in three
diﬀerent environments: (1) unmodiﬁed Qemu/KVM using
385one processor core, (2) SICE time-sharing environment using
a single processor core, and (3) SICE multi-core environment
using two processor cores, used as the host core and the
isolated core.
In the multi-core test, we do not run any
program other than Qemu/KVM on the host core, so that we
can get a precise measurement of the performance overhead.
To measure the performance overhead, we use Qemu to
emulate a serial port for the guest VM. We then conﬁgure
the guest VM to use this serial console to accept shell com-
mand. Our performance measurement is the execution time
needed for the guest VM to complete each of these tests.
The execution time is calculated by using the RDTSC instruc-
tion in the legacy host. The time measurement is taken just