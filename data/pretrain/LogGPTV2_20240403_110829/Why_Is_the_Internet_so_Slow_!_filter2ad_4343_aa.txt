title:Why Is the Internet so Slow?!
author:Ilker Nadi Bozkurt and
Anthony Aguirre and
Balakrishnan Chandrasekaran and
Brighten Godfrey and
Gregory Laughlin and
Bruce M. Maggs and
Ankit Singla
Why Is the Internet so Slow?!
Ilker Nadi Bozkurt1(B), Anthony Aguirre4, Balakrishnan Chandrasekaran2,
P. Brighten Godfrey3, Gregory Laughlin5, Bruce Maggs1,6, and Ankit Singla7
1 Duke University, Durham, USA
{ilker,bmm}@cs.duke.edu
2 TU Berlin, Berlin, Germany
PI:EMAIL
3 UIUC, Champaign, USA
PI:EMAIL
4 UC Santa Cruz, Santa Cruz, USA
PI:EMAIL
5 Yale University, New Haven, USA
PI:EMAIL
6 Akamai, Cambridge, USA
7 ETH Z¨urich, Z¨urich, Switzerland
PI:EMAIL
Abstract. In principle, a network can transfer data at nearly the speed
of light. Today’s Internet, however, is much slower: our measurements
show that latencies are typically more than one, and often more than
two orders of magnitude larger than the lower bound implied by the
speed of light. Closing this gap would not only add value to today’s
Internet applications, but might also open the door to exciting new appli-
cations. Thus, we propose a grand challenge for the networking research
community: building a speed-of-light Internet. To help inform research
towards this goal, we investigate, through large-scale measurements, the
causes of latency inﬂation in the Internet across the network stack. Our
analysis reveals an under-explored problem: the Internet’s infrastructural
ineﬃciencies. We ﬁnd that while protocol overheads, which have domi-
nated the community’s attention, are indeed important, reducing latency
inﬂation at the lowest layers will be critical for building a speed-of-light
Internet. In fact, eliminating this infrastructural latency inﬂation, with-
out any other changes in the protocol stack, could speed up small object
fetches by more than a factor of three.
1 Introduction
Measurements and analysis by Internet giants have shown that shaving a few
hundred milliseconds from the time per transaction can translate into millions
of dollars. For Amazon, a 100 ms latency penalty implies a 1% sales loss [18];
for Google, an additional delay of 400 ms in search responses reduces search
B. Chandrasekaran—This work was done when the author was a graduate student
at Duke University.
c(cid:2) Springer International Publishing AG 2017
M.A. Kaafar et al. (Eds.): PAM 2017, LNCS 10176, pp. 173–187, 2017.
DOI: 10.1007/978-3-319-54328-4 13
174
I.N. Bozkurt et al.
volume by 0.74%; and for Bing, 500 ms of delay decreases revenue per user
by 1.2% [10,13]. The gaming industry, where latencies larger than even 80 ms
can hurt gameplay [19], has even tougher latency requirements. These numbers
underscore that latency is a key determinant of user experience.
We take the position that the networking community should pursue an ambi-
tious goal: cutting Internet latencies to close to the limiting physical constraint,
the speed of light, roughly one to two orders of magnitude faster than today.
Beyond the obvious gains in performance and value for today’s applications,
such a technological leap may help realize the full potential of certain applica-
tions that have so far been conﬁned to the laboratory, such as tele-immersion.
For some applications, such as massive multi-player online games, the size of the
user community reachable within a latency bound plays an important role in
user interest and adoption, and linear decreases in communication latency result
in super-linear growth in community size [25]. Low latencies on the order of a
few tens of milliseconds also open up the possibility of instant response, where
users are unable to perceive any lag between requesting a page and seeing it ren-
dered in their browsers. Such an elimination of wait time would be an important
threshold in user experience.
But the Internet’s speed is quite far from the speed of light. As we show later,
the time to fetch just the HTML document of the index pages of popular Web
sites from a set of generally well-connected clients is, in the median, 37 times
the round-trip speed-of-light latency. In the 80th percentile it is more than 100
times slower. Given the promise a speed-of-light Internet holds, why are we so
far from the speed of light?
While ISPs compete primarily on the basis of peak bandwidth oﬀered, band-
width is no longer the bottleneck for a signiﬁcant fraction of the population: for
instance, the average Internet connection speed in the US is 15.3 Mbps [9], while
the eﬀect of increasing bandwidth on page load time is small beyond as little as
5 Mbps [17]. If bandwidth isn’t the culprit, then what is? In our short workshop
paper [25], we staked out our vision of a speed-of-light Internet, discussed why
it is a worthy goal to pursue, and, provided a preliminary analysis of latency
inﬂation across the network stack. In this work, we present a more thorough
analysis of latency inﬂation using three new data sets. Our contributions are as
follows:
1. We quantify the factors that contribute to large latencies today using four
sets of measurements: from PlanetLab nodes to Web servers1; between a
large CDN’s servers and end hosts; from volunteer end-user systems2 to Web
servers; and between RIPE Atlas nodes. Our analysis breaks down Inter-
net latency inﬂation across the network stack, from the physical network
infrastructure to the transport layer (including, in some instances, TLS).
1 Data sets (gathered in 2016) and code are available at https://cgi.cs.duke.edu/
∼ilker/cspeed/pam2017-data/.
2 Explicit volunteer consent was obtained, listing precisely what tests would be run.
We have a letter from the IRB stating that our tests did not require IRB approval.
Why is the Internet so slow?!
175
2. This work places in perspective the importance of latency inﬂation at the
lowest layers. While in line with the community’s understanding that DNS,
TCP handshake, and TCP slow-start are all important factors in latency
inﬂation, the Internet’s infrastructural ineﬃciencies are also important. We
consider this an under-appreciated piece of the latency puzzle.
3. We ﬁnd that removing latency inﬂation in the physical infrastructure and
routing without any changes at layers above, could improve latencies for
fetching small objects by more than 3 times.
2 The Internet Is Too Slow
We pooled the top 500 Web sites from each of 138 countries listed by Alexa [7].
We followed redirects on each URL, and recorded the ﬁnal URL for use in our
measurements; the resulting data set contains 22,800 URLs. We fetched just the
HTML at these URLs from 102 PlanetLab locations using cURL [1], and 25%
of all fetches in our experiments were over HTTPS3.
Fig. 1. (a) Inﬂation in fetch time, and (b) its breakdown across various components of
HTTP fetches of just the HTML of the landing pages of popular Web sites.
For each connection (or fetch), we geolocated the Web server using six com-
mercial geolocation services, and (since we do not have any basis for deciding
which service is better than another) used the location identiﬁed by their major-
ity vote (MV). We computed the time it would take for light to travel round-trip
along the shortest path between the same end-points, i.e., the c-latency. Finally,
we calculated the Internet’s latency inﬂation as the ratio of the fetch time to
c-latency. Figure 1(a) shows the CDF of inﬂation over 1.9 million connections.
The HTML fetch time is, in the median, 36.5 times the c-latency, while the 80th
percentile exceeds 100 times. We note that PlanetLab nodes are generally well-
connected, and latency can be expected to be poorer from the network’s true
edge. We verify that this is indeed the case with measurements from end users
in Sect. 3.7.
3 We do not claim this is the percentage of Web sites supporting HTTPS.
176
I.N. Bozkurt et al.
3 Why Is the Internet so Slow?
To identify the causes of Internet latency inﬂation, we break down the fetch time
across layers, from inﬂation in the physical path followed by packets to the TCP
transfer time.
3.1 Methodology
We use cURL to obtain the time for DNS resolution, TCP handshake, TCP
data transfer, and total fetch time for each connection. For HTTPS connections,
we also record the time for TLS handshake. TCP handshake is measured as
the time between cURL sending the SYN and receiving the SYN-ACK. The TCP
transfer time is measured as the time from cURL’s receipt of the ﬁrst byte of
data to the receipt of the last byte. We separately account for the time between
cURL sending the data request and the receipt of the ﬁrst byte as ‘request-
response’ time; this typically comprises one RTT and any server processing time.
For each connection, we also run a traceroute from the client PlanetLab node
to the Web server. We then geolocate each router in the traceroute path, and
connect successive routers with the shortest paths on the Earth’s surface as
an optimistic approximation for the route the packets follow. We compute the
round-trip latency at the speed of light in ﬁber along this approximate path, and
refer to it as the ‘router-path latency’. From each client, we also run 30 successive
pings to each server, and record the minimum and median across these ping
times. We normalize each of these latency components by the c-latency between
the respective connection’s end-points.
Our experiments yielded 2.1 million page fetches with HTTP status code 200,
which corresponds to 94% of all fetches. We also ﬁltered out connections which
showed obvious anomalies such as c-latency being larger than TCP handshake
time or minimum ping time (probably due to errors in geolocation), leaving us
with 1.9 million fetches.
3.2 Overview of Results
Figure 1(b) shows the results for all connections over HTTP. DNS resolutions
are shown to be faster than c-latency 14% of the time. This is an artifact of the
baseline we use—in these cases, the Web server happens to be farther than the
DNS resolver, and we always use the c-latency to the Web server as the baseline.
(The DNS curve is clipped at the left to more clearly display the other results.)
In the median, DNS resolutions are 6.6× inﬂated over c-latency.
The TCP transfer time shows signiﬁcant inﬂation—12.6 times in the median.
With most pages being at most tens of KB (median page size is 73 KB), band-
width is not the problem, but TCP’s slow start causes even small data trans-
fers to require several RTTs. 6% of all pages have transfer times less than the
c-latency—this is due to all the data being received in the ﬁrst TCP window. The
TCP handshake (counting only the SYN and SYN-ACK) and the minimum ping
time are 3.2 times and 3.1 times inﬂated in the median. The request-response
Why is the Internet so slow?!
177
Fig. 2. (a) Various components of latency inﬂation over HTTPS connections, and (b)
the median, 80th% and 95th% of inﬂation in min. ping (red), router-path (blue) and
total (green) latency using 6 diﬀerent geolocation databases as well as their majority
vote. (Color ﬁgure online)
time is 6.5 times inﬂated in the median, i.e., roughly twice the median RTT.
However, 24% of the connections use less than 10 ms of server processing time
(estimated by subtracting one RTT from the request-response time). The median
c-latency, in comparison, is 47 ms. The medians of inﬂation in DNS time, TCP
handshake time, request-response time, and TCP transfer time add up to 28.8
times, lower than the measured median total time of 36.5 times, since the dis-
tributions are heavy-tailed.
Figure 2(a) shows the results for fetches over HTTPS. The inﬂations in DNS
resolution and TCP handshake are similar to those for HTTP (6.3 times and 3.1
times in the median respectively). The largest contributor to the latency inﬂa-
tion is the TLS handshake, which is 10.2 times inﬂated in the median, roughly
corresponding to 3 RTTs. Inﬂation in TCP transfer time, being 5.2 times in
the median, is signiﬁcantly lower than for HTTP connections. This diﬀerence
is partly explained by the smaller size of pages fetched over HTTPS, with the
median fetch size being 43 KB. The median inﬂation in request-response times
increases from 6.5 times for HTTP to 7.7 times for HTTPS.
3.3 Impact of IP Geolocation Errors
The correctness of our latency inﬂation analysis crucially depends on geoloca-
tion. While we cull data with obvious anomalies, such as when the min. ping time
is smaller than c-latency, arising from geolocation errors (some of which may be
due to Anycast), less obvious errors could impact our results. For PlanetLab node
locations, we have ground truth data and our tests did not indicate any erro-
neous location. Retrieving similar ground truth data for the large IP space under