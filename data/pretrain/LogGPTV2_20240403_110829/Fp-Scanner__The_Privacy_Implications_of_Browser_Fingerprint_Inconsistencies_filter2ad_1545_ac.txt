3.4 Uncovering Canvas Inconsistencies
Canvas ﬁngerprinting uses the HTML 5 canvas API to
draw 2D shapes using JavaScript. This technique, dis-
covered by Mowery et al. [14], is used to ﬁngerprint
browsers. To do so, one scripts a sequence of instruc-
tions to be rendered, such as writing text, drawing shapes
or coloring part of the image, and collects the rendered
output. Since the rendering of this canvas relies on the
combination of different hardware and software layers, it
produces small differences from device to device. An ex-
ample of the rendering obtained on a CHROME browser
running on Linux is presented in Figure 2a.
As we mentioned, the rendering of the canvas de-
pends on characteristics of the device, and if an instruc-
tion has been added to the script, you can expect to ob-
serve its effects in the rendered image. Thus, we con-
sider these scripted instructions as constraints that must
be checked in the rendered image. For example, the can-
vas in Figure 2b has been obtained with the CANVAS DE-
FENDER extension installed. We observe that contrary to
the vanilla canvas that does not use any countermeasure
(Figure 2a), the canvas with the countermeasure has a
background that is not transparent, which can be seen as
a constraint violation. We did not develop a new canvas
test, we reused the one adopted by state-of-the-art can-
vas ﬁngerprinting [12]. From the rendered image, our
test suite checks the following properties:
1. Number of transparent pixels as the background of
our canvas must be transparent, we expect to ﬁnd a
majority of these pixels;
2. Number of isolated pixels, which are pixels whose
rgba value is different than (0,0,0,0) and are only
surrounded by transparent pixels. In the rendered
image, we should not ﬁnd this kind of pixel because
shapes or texts drawn are closed;
3. Number of pixels per color should be checked
against the input canvas rendering script, even if it
is not possible to know in advance the exact number
of pixels with a given color, it is expected to ﬁnd
colors deﬁned in the canvas script.
We also check if canvas-related functions, such as
toDataUrl, have been overridden.
4 Empirical Evaluation
This section compares the accuracy of FP-SCANNER,
FINGERPRINTJS2 and AUGUR to classify genuine and
altered browser ﬁngerprints modiﬁed by state-of-the-art
ﬁngerprinting countermeasures.
Implementing FP-Scanner
4.1
Instead of directly implementing and executing our test
suite within the browser, thus being exposed to counter-
measures, we split FP-SCANNER into two parts. The
ﬁrst part is a client-side ﬁngerprinter, which uploads raw
browser ﬁngerprints on a remote storage server. For
the purpose of our evaluation, this ﬁngerprinter extends
state-of-the-art ﬁngerprinters,
like FINGERPRINTJS2,
with the list of attributes covered by FP-SCANNER (e.g.,
WebGL ﬁngerprint). Table 5 reports on the list of at-
tributes collected by this ﬁngerprinter. The resulting
dataset of labeled browser ﬁngerprints is made available
to leverage the reproducibility of our results.11
The second part of FP-SCANNER is the server-side im-
plementation, in Python, of the test suite we propose (cf.
Section 3). This section reports on the relevant technical
issues related to the implementation of the 4 components
of our test suite.
4.1.1 Checking OS Inconsistencies
OSRef is deﬁned as the OS claimed by the user agent
attribute sent by the browser and is extracted using a
UA PARSER library.12 We used the browser ﬁngerprint
USENIX Association
27th USENIX Security Symposium    141
Table 5: List of attributes collected by our ﬁngerprinter
4.1.2 Checking Browser Inconsistencies
Attribute
HTTP headers
User agent
navigator
Platform
Plugins
ProductSub
Navigator
prototype
Canvas
WebGL renderer
WebGL vendor
Browser features
Media queries
Errors type 1
Errors type 2
Stack overflow
Eval toString
length
mediaDevices
TouchSupport
Accelerometer
Description
List of HTTP headers sent by the browser and their
associated value
Value of navigator.userAgent
Value of navigator.platform
List of plugins (description, ﬁlename, name) ob-
tained by navigator.plugins
Value of navigator.productSub
String representation of each property and function
of the navigator object prototype
Base64 representation of the image generated by the
canvas ﬁngerprint test
WebGLRenderingContext.getParameter("renderer")
WebGLRenderingContext.getParameter("vendor")
Presence or absence of certain browser features
Collect if media queries related to the presence of
certain OS match or not using window.matchMedia
Generate a TypeError and store its properties and
their values
Generate an error by creating a socket not pointing
to an URL and store its string representation
Generate a stack overﬂow and store the error name
and message
Length of eval.toString().length
of
navigator.mediaDevices.
Value
enumerateDevices
Collect the value of navigator.maxTouchPoints,
store if we can create a TouchEvent and if window
object has the ontouchstart property
true if the value returned by the accelerometer sen-
sor is different of 0, else false
Screen resolution Values
of
screen.width/height,
and
We extract BrowserRef using the same user agent pars-
ing library as for OSRef. With regards to JavaScript
errors, we check if the ﬁngerprint has a prototype,
an error message, as well as a type consistent with
browserRef. Moreover, for each attribute and function
of the navigator object, FP-SCANNER also checks if
the string representation reveals that it has been overrid-
den.
Testing if the features of the browser are consistent
with browserRef is achieved by comparing the features
collected using MODERNIZR13 with the open source data
ﬁle provided by the website CANIUSE.14 The ﬁle is
freely available on Github15 and represents most of the
features present in MODERNIZR as a JSON ﬁle. For
each of them, it details if they are available on the main
browsers, and for which versions. We consider that a
feature can be present either if it is present by default
or it can be activated. Then, for each MODERNIZR fea-
ture we collected in the browser ﬁngerprint, we check if
it should be present according to the CANIUSE dataset.
If there are more than Ne = 1 errors, either features that
should be available but are not, or features that should
not be available but are, then we consider the browser as
inconsistent.
Fonts
Overwritten
properties
screen.availWidth/Height
Font enumeration using JavaScript [7]
string
Collect
as well
screen.width/height
toDataURL and getTimezoneOffset functions
representation
getters,
dataset from AMIUNIQUE [12] to analyze if some of the
fonts they collected were only available on a given OS.
We considered that if a font appeared at least 100 times
for a given OS family, then it could be associated to this
OS. We chose this relatively conservative value because
the AMIUNIQUE database contains many ﬁngerprints
that are spoofed, but of which we are unaware of. Thus,
by setting a threshold of 100, we may miss some fonts
linked to a certain OS, but we limit the number of false
positives—i.e., fonts that we would classify as linked to
an OS but which should not be linked to it. FP-SCANNER
checks if the fonts are consistent with OSRef by count-
ing the number of fonts associated to each OS present in
the user font list. If more than Nf = 1 fonts are associ-
ated to another OS than OSRef, or if no font is associ-
ated to OSRef, then FP-SCANNER reports an OS incon-
sistency. It also tests if moz-mac-graphite-theme and
$win-version) with
@media(-moz-os-version:
$win-version equals to Windows XP, Vista, 7, 8 or 10,
are consistent with OSRef.
of
as
4.1.3 Checking Device Inconsistencies
We verify that, if the device claims to be a mobile, then
the accelerometer value is set to true. We apply the
same technique for touch-related events. However, we
do not check the opposite—i.e., that computers have no
touch related events—as some new generations of com-
puters include touch support. Concerning the screen res-
olution, we ﬁrst check if the screen height and width
have been overridden.
4.1.4 Checking Canvas Poisoning
To detect if a canvas has been altered, we extract the 3
metrics proposed in Section 3. We ﬁrst count the num-
ber of pixels whose rgba value is (0,0,0,0). If the im-
age contains less than Nt p = 4,000 transparent pixels,
or if it is full of transparent pixels, then we consider
that the canvas has been poisoned or blocked. Secondly,
we count the number of isolated pixels.
If the canvas
contains more than 10 of them, then we consider it as
poisoned. We did not set a lower threshold as we ob-
served that some canvas on macOS and SAFARI included
a small number of isolated pixels that are not generated
by a countermeasure. Finally, the third metric tests the
presence of the orange color (255,102,0,100) by count-
ing the number of pixels having this exact value, and also
142    27th USENIX Security Symposium
USENIX Association
Table 6: List of relevant tests per countermeasure.
Test (scope)
User Agents (global)
Platform (OS)
WebGL (OS)
Plugins (OS)
Media Queries
(OS, browser)
Fonts (OS)
Error (browser)
Function representation
(browser)
Product (browser)
Navigator (browser)
Enumerate devices
(browser)
Features (browser)
Events (device)
Sensors (device)
toDataURL (canvas)
Pixels (canvas)
S
A
R
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
s
r
e
f
o
o
p
s
A
U
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
s
a
v
n
a
C
s
n
o
i
s
n
e
t
x
e
M
O
D
N
A
R
P
F
E
V
A
R
B
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
X
O
F
E
R
I
F
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
the number of pixels whose color is slightly different—
i.e., pixels whose color vector vc satisﬁes the following
equation (cid:107)(255,102,0,100)− vc(cid:107) < 4. Our intuition is
that canvas poisoners inject a slight noise, thus we should
ﬁnd no or few pixels with the exact value, and many pix-
els with a slightly different color.
For each test of our suite, FP-SCANNER stores the de-
tails of each test so that it is possible to know if it is
consistent, and which steps of the analysis failed.
Estimating the parameters Different parameters of
FP-SCANNER, such as the number of transparent pixels,
may inﬂuence the accuracy of FP-SCANNER, resulting
in different values of true and false positives. The strat-
egy we use to optimize the value of a given parameter
is to run the scanner test that relies on this parameter,
and to tune the value of the parameter to minimize the
false positive rate (FPR)—i.e., the ratio of ﬁngerprints
that would be wrongly marked as altered by a counter-
measure, but that are genuine. The reason why we do not
run all the tests of the scanner to optimize a given param-
eter is because there may be some redundancy between
different tests. Thus, changing a parameter value may not
necessarly results in a modiﬁcation of the detection as a
given countermeasure may be detected by multiple tests.
Moreover, we ensure that countermeasures are detected
for the appropriate symptoms. Indeed, while it is nor-
mal for a canvas countermeasure to be detected because
some pixels have been modiﬁed, we consider it to be a
false positive when detected because of a wrong browser
feature threshold, as the countermeasure does not act on
the browser claimed in the user agent. Table 6 describes,
for each countermeasure, the tests that can be used to re-
veal its presence. If a countermeasure is detected by a
test not allowed, then it is considered as a false positive.
Figure 3 shows the detection accuracy and the false
Table 7: Optimal values of the different parameters to
optimize, as well as the FPR and the accuracy obtained
by executing the test with the optimal value.
Attribute
Pixels: Nt p
Fonts: Nf
Features: Ne
Optimal
value
17,200
2
1
FPR
(accuracy)
0 (0.93)
0 (0.42)
0 (0.51)
positive rate (FPR) for different tests and different val-
ues of the parameters to optimize. We deﬁne the ac-
#T P+#T N
#Fingerprints where true positives (TP) are the
curacy as
browser ﬁngerprints correctly classiﬁed as inconsistent,
and true negatives (TN) are ﬁngerprints correctly classi-
ﬁed as genuine. Table 7 shows, for each parameter, the
optimal value we considered for the evaluation. The last
column of Table 7 reports on the false positive rate, as
well as the accuracy obtained by running only the test
that makes use of the parameter to optimize.
In the case of the number of transparent pixels Nt p we
observe no differences between 100 and 16,500 pixels.
Between 16,600 and 18,600 there is a slight improve-
ment in terms of accuracy caused by a change in the true
positive rate. Thus, we chose a value of 17200 transpar-
ent pixels since it provides both a false positive rate of 0
while maximizing the accuracy.
Concerning the number of wrong fonts Nf , we ob-
tained an accuracy of 0.646 with a threshold of one font,
but this resulted in a false positive rate of 0.197. Thus,
we chose a value of Nf = 2 fonts, which makes the ac-
curacy of the test decrease to 0.42 but provides a false
positive rate of 0.
Finally, concerning the number of browser features Ne,
increasing the threshold resulted in a decrease of the ac-
curacy, and an increase of the false negative rate. Never-