Tukwila console had a RTT of 75 ms to Los Angeles (1,100 miles
away).1 Such violations are relatively infrequent—in this exam-
ple, the Vancouver and Tukwila consoles were involved in 78 RTT
measurements to other nodes, none of which appear to be TIVs.
TIVs are problematic for both NCSes and geolocation-based la-
tency estimation. Geolocation will under-estimate the latency be-
tween two nodes, as evident from the above example. Similarly,
an NCS will underestimate the latency because the coordinates for
the two nodes may have converged to stable positions based on the
vast majority of non-TIV latency measurements. Further, when a
TIV measurement is used to update an NCS, the coordinates of the
nodes involved will be pushed further apart by the resulting force,
thereby worsening their positions with respect to other nodes.
Some prior solutions to this problem have been shown to either
worsen prediction accuracy or not scale in distributed settings [33].
We instead use a heuristic similar to TIV Alert proposed in [33].
When updating a node’s coordinate, if the measured latency ex-
ceeds the predicted latency by δ, we skip this coordinate update.
We apply this heuristic only when both nodes in question have un-
certainty lower than the geographic bootstrapping default of 29.2%,
and update their uncertainties based on this measurement. This
guards against slowing down convergence for nodes with poor ini-
tial coordinates. We have empirically determined that a δ of 100 ms
works best for our client population.
While this heuristic reduces the impact of TIVs on the quality
of node coordinates, it does not help improve latency prediction of
TIV edges. To address that problem, we rely on history prioriti-
zation. Every time a node learns the RTT to another, it saves this
RTT in its history. When a prediction is needed, if there is history
for the destination node, we use the most recently measured RTT
instead of the RTT predicted by the coordinates. Note that we use
the most recent one because we assume robust RTT measurements,
as discussed earlier. The past RTT can be a good estimate of future
RTT because RTTs on the Internet can be very stable. In our data,
1We found the physical locations of these consoles by using
a geolocation database (§ 4.1), and conﬁrming by looking up the
DNS names of nearby routers using traceroute.
r ← 1− ccws ((cid:107)(cid:126)xA −(cid:126)xB(cid:107)− lAB) /(cid:107)(cid:126)xA −(cid:126)xB(cid:107)
d ← cos−1 [cos(φA)cos(φB)+
(cid:21)
sin(φA)sin(φB)cos(λB − λA)]
(cid:20) sin(φB)sin(φA)sin(λB − λA)
(cid:21)
(cid:20)
γ ← tan−1
φA ← cos−1 [cos(rd)cos(φB) + sin(rd)sin(φB)cos(γ)]
β ← tan−1
λA ← λB − β
cos(rd)− cos(φA)cos(φB)
cos(φA)− cos(d)cos(φB)
sin(φB)sin(rd)sin(γ)
Figure 4. Adaptation of Vivaldi update algorithm to spheri-
cal coordinates. Here, (cid:126)xN, the coordinates for node N, consist
of φN, the latitudinal distance in radians between node N and
the North Pole, and λN, the longitude in radians of node N. We
compute r, the ratio between the desired and current distance
between nodes A and B; d, the current distance in radians be-
tween them; γ, the angle from the North Pole to B to A (which
stays the same as A moves); and β, the angle from B to the North
Pole to A’s new location. Note that some special cases are not
shown.
for about 95% of nodes that measured RTT multiple times between
themselves for as long as 50 days, the coefﬁcient of variation in
RTT was under 0.2. History prioritization is particularly useful for
node pairs that cause TIVs, because latency estimates from their
coordinates will be inaccurate.
It might seem unlikely that among millions of players, a player
will ever probe the same player twice, but it does happen. This is
presumably because the factors that make two players good can-
didates for matchmaking, such as liking the same type of game,
having similar skill level, and liking to play at the same time of day
or week, change infrequently.
2.3 AS correction
There are many autonomous systems (ASes) serving game play-
ers, so most pairs of players will belong to different ASes. Thus,
we can expect our coordinate system will automatically tune itself
to predicting latencies assuming endpoints are in different ASes.
Unfortunately, this means that when endpoints are in the same AS,
the faster resulting routing will not be reﬂected in the coordinate
system, leading to systematic overestimation of the RTT of such
paths. This can be particularly frustrating for game players, since it
may lead to unnecessary exclusion of some of the closest players.
To understand our solution, recall that Htrae, like other NCSes,
augments coordinates with a virtual height reﬂecting the latency a
machine incurs on essentially all of its paths [6]. For home ma-
chines, we can loosely consider this latency to have two main com-
ponents. First, a packet incurs latency in the so called “last-mile.”
In the case of a machine with a broadband DSL connection, this is
the latency between the machine and the next IP-level device, such
as the broadband remote access server (BRAS). The second compo-
nent is the remaining latency through the high-speed network core
for the AS the machine resides in. Typically, this would be a set of
core routers where that AS peers with others.
As shown in Figure 5, for a node A to reach node B, pack-
ets would traverse the last-mile, reach the core, traverse inter-AS
links to the second core, and then traverse the last-mile. However,
a packet from A to C will traverse a shorter path that skips some
of the second component of the height. This can occur when two
nodes are part of the same AS and are “close-by” in the network.
Using BGP routing tables, we are able to determine if two nodes
belong to the same AS, and using geolocation we are able to deter-
317Figure 5. Simple model of two ASes with broadband machines
on the Internet
mine their physical distance. Given that it is difﬁcult to break a
height down into its two components based solely on end-to-end
RTT measurements, we rely on a heuristic: we ignore a portion of
the sum of heights when predicting RTT or updating coordinates
for such nodes. We have empirically determined that ignoring 20%
of the heights for nodes in the same AS and under 225 miles apart
produces the best results.
2.4 Symmetric updates
As we have observed earlier, RTT measurement between game
machines requires multiple round trips and typically is coupled with
bandwidth measurements. Therefore, it does not add substantially
to trafﬁc to send one additional message, at the conclusion of the
measurement, to notify the other machine of the RTT. Since the
RTT is the same in both directions, the recipient can use the re-
ported RTT to update its own coordinates, saving it the time of per-
forming its own RTT measurement. Note that the overhead is even
less when the matchmaking service is centralized. In that case, the
measuring machine must expend network trafﬁc to notify the cen-
tral service of the RTT anyway, at which point the service can up-
date the coordinates of both endpoints. For these reasons, Htrae
adjusts both machines’ coordinates after an RTT measurement. It
adjusts them effectively simultaneously, i.e., it uses the pre-update
coordinates for one in the computation of the other.
3.
IMPLEMENTATION
Our Htrae implementation is based on the publicly-available
code for Pyxida, a practical implementation of the Vivaldi algo-
rithm for the Azureus BitTorrent client [13]. We ported this code to
C# and added our algorithmic modiﬁcations as well as support for
experimentation on home machines. Our system is approximately
3,600 lines of code, about a third of which is the direct port of Pyx-
ida.
To enable geographic bootstrapping, Htrae relies on a location
service. For simplicity, we built a centralized server that loads the
GeoIP City Database from MaxMind [20] in memory and responds
to IP address lookups with the corresponding latitude and longi-
tude. The 01 January 2009 version of the database that we use has
4,100,436 entries, each with an IP address range, latitude, and lon-
gitude. §5 discusses how well this database covers the IP addresses
in our traces and deployment.
To enable AS correction, Htrae uses a routing table service.
For simplicity, we built a centralized server that loads a routing
table into a PATRICIA trie [22] in memory and responds to IP
address lookups with the origin AS. The routing table we use is
from the Route Views Project [27], in particular from the route-
views.oregon-ix.net router which peers with 43 different routers
across 37 ASes on the Internet. We use a snapshot from 04:00 on
01 January 2009, with entries for 277,383 preﬁxes.
Home networks typically use network address translation
(NAT), preventing straightforward direct communication between
them [11]. To enable experiments in which home machines com-
municate with each other, we implemented a simple approach sim-
ilar to STUN [26].
Figure 6. Geographic spread of nodes in trace A
Figure 7. Arrival rate of nodes and node pairs in trace A
4. METHODOLOGY
In this section, we describe our methodology for the experi-
ments in §5. We describe our data set, how we use it to evaluate
Htrae, and how we use it to compare to other systems. We also
describe how we deployed Htrae to conduct tests in a real-world
environment.
4.1 Traces
To evaluate the techniques behind Htrae in the context of game
matchmaking, we use traces of Xbox 360 consoles participating in
matchmaking for the popular game Halo 3. For Halo 3, matchmak-
ing involves choosing one console to be a server, then choosing up
to 16 consoles to be clients. When a player starts matchmaking,
his console chooses whether to be a client or server and notiﬁes the
Xbox LIVE matchmaking service. Xbox LIVE replies with a set of
potential servers, and the client probes them all. Each probe mea-
sures RTT by taking the median value of multiple ping-like mea-
surements. Based on the probe results, the client chooses a server.
For each session, i.e., instance of one client probing one or more
servers to ﬁnd a game, we log: UTC time, client’s and servers’ pub-
lic IP addresses, and RTT from the client to each server. Due to the
enormous volume of online game play, the logging system cannot
record every probe, so for each session it chooses randomly with
probability 10% whether to log all measurements in that session.
In this paper, we focus on the two time periods described in
Table 1. Our results from several other time periods are similar
and we focus on these two for conciseness. With almost 50 million
RTT measurements across over 3.5 million unique IPs in trace A,
our evaluation data set is still among the largest ever used.
Figure 6 shows the geographic locations of the nodes in trace
A. While there is extensive coverage across North America and Eu-
rope, several other major regions such as Japan and eastern Aus-
tralia are also well represented. We believe such geographic spread
is common for large distributed systems.
Two important characteristics of our traces are the rate at which
new nodes are seen, and the rate at which any node probes another it
has not probed before. Figure 7 shows that after the ﬁrst three days
InternetABRASlastmileCBRAScoreBBRASlastmilecorelastmile02000040000600008000010000012000014000011/711/911/1111/1311/1511/1711/1911/2111/2311/2511/2711/2912/112/312/512/712/9time (hour bins)new node pairsnew nodes318trace
A
B
start time
11/07/2008 12am
01/14/2009 12am
training end
11/10/2008 12am
01/17/2009 12am
end time
12/10/2008 12am
01/24/2009 12am
distinct IPs
3,534,120
1,700,547
session count
15,630,101
5,859,214
total probes
49,946,991
20,300,141
post-training probes
44,227,511
14,810,694
Table 1. Halo 3 matchmaking traces
trace
A
B
num. of sessions with choice of
1 server
9,351,950
3,888,694
Table 2. Server counts per session
3 servers
1,138,157
344,420
2 servers
2,096,555
620,834
> 3 servers
3,043,439
1,005,266
of trace A, there is an almost constant diurnal arrival of new nodes
through the end of the month. The high rate of new probe pairs
indicates the need for efﬁcient latency estimation between nodes
that have not previously directly communicated.
4.2 Trace replay
To evaluate a latency predictor, we replay each session from a