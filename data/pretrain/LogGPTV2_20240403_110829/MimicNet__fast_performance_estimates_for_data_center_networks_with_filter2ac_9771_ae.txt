Figure 1. For 20 seconds of simulated time, the full-fidelity simulator
required almost 1w 5d. In contrast, MimicNet, in aggregate, only
required 8h 38m, where just 25m was used for final simulation—
a 34× speedup. Longer simulation periods or multiple runs for
different workload seeds would have led to much larger speedups.
Simulation time speedup. We focus on the non-fixed-cost com-
ponent of the execution time in order to better understand the
benefits of MimicNet. Figure 10 shows the speedup of MimicNet
after the initial, fixed cost of training a cluster model. For each
network configuration, we run both MimicNet and a full simulation
over the exact same sets of generated workloads. We then report
the average speedup and the standard error across those workloads.
In both systems, simulation time consists of both setup time
(constructing the network, allocating resources, and scheduling the
traffic) as well as packet processing time. MimicNet substantially
speeds up both phases.
MimicNet can provide consistent speedups up to 675× for the
largest network that full-fidelity simulation was able to handle.
Above that size, full-fidelity could not finish within three months,
while MimicNet can finish in under an hour. Somewhat surprisingly,
MimicNet is also 7× faster than flow-level approximation at this scale
as SimGrid must still track all of the Mimic-Mimic connections.
Groups of simulations. We also acknowledge that simulations
are frequently run in groups, for instance, to test different con-
figuration or workload parameters. To evaluate this, we compare
several different approaches to running groups of simulations and
evaluate them using two metrics: (1) simulation latency, i.e., the
total time it takes to obtain the full set of results, and (2) simulation
296
MimicNet
Training and hyper-param tuning
Full
Simulation
1w 4d 22h 25m
Table 2: Running time comparison for 20 s of simulated time
of a 128 cluster, 1024 host data center. Benefits of MimicNet
increase with simulated time as the first two values for Mim-
icNet are constant.
throughput, i.e., the average number of aggregate simulation sec-
onds that can be processed per second. In this section, we focus on
the effect of network size on these metrics, but we also evaluated
the effect of simulation length in Appendix F and the effect on
compute consumption in Appendix G.
Simulation latency: For latency, 𝑁 cores in a machine, and 𝑆 sim-
ulation seconds, we consider five different approaches: (1) single
simulation, i.e., one full simulation that runs on a single core and
simulates 𝑆 seconds; (2) single MimicNet w/ training, i.e., one end-to-
end MimicNet instance, running from scratch; (3) single MimicNet,
i.e., one MimicNet instance that reuses an existing model; (4) par-
titioned simulation, i.e., 𝑁 full simulations, each simulating 𝑆/𝑁
seconds; and (5) partitioned MimicNet, i.e., 𝑁 MimicNet instances,
each simulating 𝑆/𝑁 seconds. 𝑁 =20 as our machines have 20 cores.
Figure 11 shows the results for network sizes ranging from 8 to
128 clusters. We make the following observations. First, when the
network is relatively small, the model training overhead in Mim-
icNet is significant, so ‘single MimicNet w/ training’ takes longer
than ‘single simulation’ to finish. When the network size reaches 64
clusters, even when training time is included, MimicNet runs faster
than any full simulation approach. When the network is as large as
128 clusters, MimicNet is 2-3 orders of magnitude faster than full
simulations. The results hold when partitioning, with MimicNet
gaining an additional advantage in larger simulations where the re-
moval of the majority of packets/connections introduces substantial
gains to the memory footprint of the simulation group.
Simulation throughput: For throughput, we consider a similar set of
five approaches. Specifically, the first three are identical to (1)–(3)
above, while the last two run for 𝑆 seconds to maximize throughput:
(4) parallel simulation, i.e., 𝑁 full simulations, each simulating 𝑆
seconds and (5) parallel MimicNet, i.e., 𝑁 MimicNet instances, each
simulating 𝑆 seconds.
 1 10 100 1000 100002 racks/cluster1.94 racks/cluster2.38 racks/cluster2.8 1 10 100 1000 100004.26.110.5 1 10 100 1000 1000012.424.675.2 1 10 100 1000 1000070.0199.9Not Finished in 3 months 1 10 100 1000 10000675.1Not ﬁnished in 3 monthsNot Finished in 3 monthsMimicNet: Fast Performance Estimates for DCNs with ML
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Figure 11: Simulation latency with different network sizes (lower is better).
Figure 12: Simulation throughput with different network sizes (higher is better).
Figure 12 shows the throughput results for the range of network
sizes. Overall, MimicNet maintains high throughput regardless
of the network size because the amount of observable traffic is
roughly constant. Single simulation, on the other hand, slows down
substantially as the size of the network grows, and at 128 clusters,
full simulation is almost five orders of magnitude slower than the
real-time. As mentioned in Section 2.2, a remedy prescribed by
many simulation frameworks is to run multiple instances of the
simulation. Our results indeed show that the throughput of parallel
simulation compared to single simulation improves by up to a factor
of 𝑁 . When contrasted to the scale-independent throughput of
MimicNet, however, a single instance of MimicNet overtakes even
parallelized simulation at 32 clusters. Larger parallelized instances
begin to suffer from the memory issues described above, but even
with unlimited memory, MimicNet would still likely outperform
parallelized simulation by 2–3 orders of magnitude at 128 clusters.
9.4 Use Cases
MimicNet can approximate a wide range of protocols and provide
actionable insights for each. This section presents two potential use
cases: (1) a method of tuning configurations of DCTCP and (2) a
performance comparison of several data center network protocols.
9.4.1 Configuration Tuning
DCTCP leverages ECN feedback from the network to adjust conges-
tion windows. An important configuration parameter mentioned
in the original paper is the ECN marking threshold, 𝐾, which influ-
ences both the latency and throughput of the protocol.
Essentially, a lower 𝐾 signals congestion more aggressively en-
suring lower latency; however, a 𝐾 that is too low may underutilize
network bandwidth, thus limiting throughput. FCTs are affected by
Figure 13: Tuning the marking threshold 𝑲 in DCTCP: the
configuration that achieves the lowest 90-pct FCT is differ-
ent between 2 clusters (𝑲 = 60) and 32 clusters (𝑲 = 20).
MimicNet provides the same answer as the full simulation
for 32 clusters, but it is 12× faster.
both: short flows benefit from lower latency while long flows favor
higher throughput. The optimal 𝐾, thus, depends on both the net-
work and workload. Further, a simulation’s prescription for 𝐾 has
implications for its feasibility, its latency/throughput comparisons
to other protocols, and the range of parameters that an operator
might try when deploying to production.
Figure 13 compares the 90-pct FCT for different 𝐾s. Looking only
at the small-scale simulation, one may be led to believe that the
optimal setting for our workload is 𝐾 = 60. Looking at the larger
32-cluster simulation tells a very different story—one where 𝐾 = 60
is among the worst of configurations tested and 𝐾 = 20 is instead
optimal. MimicNet successfully arrives at the correct conclusion.
9.4.2 Comparing Protocols
Finally, MimicNet is accurate enough to be used to compare dif-
ferent transport protocols. We implement an additional four such
297
1011021031041051061078163264128Out of memorySimulation latency (seconds)Network size (#clusters)Single simulationSingle MimicNet w/ trainingSingle MimicNetPartitioned simulationPartitioned MimicNet10-510-410-310-210-11008163264128Out of memoryOut of memorySimulation throughput(simulation seconds/second)Network size (#clusters)Single simulationSingle MimicNet w/ trainingSingle MimicNetParallel simulationParallel MimicNet 3 4 5 6 7 8K=5K=10K=20K=40K=60K=8090-pct FCT (second)ECN Marking Threshold (packet)2 clusters32 clusters32 clusters (MimicNet)SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Q. Zhang et al.
using traffic drawn from empirically generated distributions and a
model of how traffic distributions translate to a set of performance
metrics [37]. Our system, in contrast, begins with a faithful repro-
duction of the target system, providing a more realistic simulation.
Emulators. Another class of tools attempts to build around real
components to maintain an additional level of realism [3, 32, 54].
Flexplane [43], for example, passes real, production traffic through
models of resource management schemes. Pantheon [56] runs real
congestion control algorithms on models of Internet paths. Unfor-
tunately, emulation’s dependency on real components often limits
the achievable scale. Scalability limitations even impact systems
like DIABLO [52], which leverages FPGAs to emulate devices with
low cost, but may still require ∼$1 million to replicate a large-scale
deployment.
Phased deployment. Also related are proposals such as [49, 59] re-
serve slices of a production network for A/B testing. While showing
true at-scale performance, they are infeasible for most researchers.
Preliminary version. Finally, we note that a published prelimi-
nary version of this work explored the feasibility of approximating
packet-level simulations using deep learning [25]. This paper rep-
resents a substantial evolution of that work. Critical advancements
include the notion of scale-independent features, end-to-end hyper-
parameter tuning methods/metrics that promote scalability of accu-
racy, the addition of feeder models, improved loss function design,
and other machine learning optimizations such as discretization.
These are in addition to significant improvements to the Mimic-
Net implementation and a substantially deeper exploration of the
design/evaluation of MimicNet.
11 CONCLUSION AND FUTURE WORK
This paper presents a system, MimicNet, that enables fast perfor-
mance estimates of large data center networks. Through judicious
use of machine learning and other modeling techniques, MimicNet
exhibits super-linear scaling compared to full simulation while re-
taining high accuracy in replicating observable traffic. While we
acknowledge that there is still work to be done in making the pro-
cess simpler and even more accurate, the design presented here
provides a proof of concept for the use of machine learning and
problem decomposition for the approximation of large networks.
As part of the future work, we would like to further improve
MimicNet’s speed with the support of incremental model updates
when models need retraining; and its accuracy with models that
involve more network events at higher levels such as flow depen-
dencies (details are in Appendix H). More generally, extending its
accuracy and speed for the evaluation of more data center protocols
and architectures is how MimicNet evolves in the future.
This work does not raise any ethical issues.
ACKNOWLEDGMENTS
We gratefully acknowledge Rishikesh Madabhushi, Chuen Hoa Koh,
Lyle Ungar, our shepherd Brent Stephens, and the anonymous SIG-
COMM reviewers for all of their help and thoughtful comments.
This work was supported in part by Facebook, VMWare, NSF grant
CNS-1845749, and DARPA Contract No. HR001117C0047. João Se-
doc was partially funded by Microsoft Research Dissertation Grant.
(a) Ground truth of the comparison
(b) The approximation of MimicNet.
Figure 14: FCT distributions of Homa, DCTCP, TCP Vegas,
and TCP Westwood for a 32-cluster data center.
protocols that each stress MimicNet’s modeling in different ways.
Homa is a low-latency data center networking protocol that utilizes
priority queues—a challenging extra feature for MimicNet as pack-
ets can be reordered. TCP Vegas is a delay-based transport protocol
that serves as a stand-in for the recent trend of protocols that are
very sensitive to small changes in latency [28, 39]. TCP Westwood
is a sender-optimized TCP that measures the end-to-end connec-
tion rate to maximize throughput and avoid congestion. DCTCP
(𝐾 = 20) uses ECN bits, which add an extra feature and prediction
output compared to the other protocols. We run the full MimicNet
pipeline for each of the protocols, training separate models. We
then compare their performance over the same workload, and we
evaluate the accuracy and speed of MimicNet for this comparison.
The FCT results are in Figure 14 (other metrics are in Appendix D).
As in the base configuration, for all protocols, MimicNet can
match the FCT of the full-fidelity simulation closely. In fact, on
average, the approximated 90-pct and 99-pct tails by MimicNet are
within 5% of the ground truth. Because of this accuracy, MimicNet
performance estimates can be used to gauge the rough relative
performance of these protocols. For example, the full simulation
shows that the best and the worst protocol for 90-pct of FCT is
Homa (3.1 s) and TCP Vegas (4.5 s); MimicNet predicts the correct
order with similar values: Homa with 3.3 s and TCP Vegas with 4.6 s.
While the exact values may not be identical, MimicNet can predict
trends and ballpark comparisons much more accurately than the
small-scale baseline. It can arrive at these estimates in a fraction of
the time—12× faster.
10 RELATED WORK
Packet-level simulation. As critical tools for networking, simu-
lators have existed for decades [30]. Popular choices include ns-
3 [21, 42], OMNeT++ [34], and Mininet [29]. When simulating large
networks, existing systems tend to sacrifice one of scalability or
granularity. BigHouse, for instance, models data center behavior
298
 0 0.2 0.4 0.6 0.8 110-510-410-310-210-1100101102Fraction of FlowsFlow Completion Time (s)HomaDCTCPTCP VegasTCP Westwood 0 0.2 0.4 0.6 0.8 110-510-410-310-210-1100101102Fraction of FlowsFlow Completion Time (s)HomaDCTCPTCP VegasTCP WestwoodMimicNet: Fast Performance Estimates for DCNs with ML
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
REFERENCES
network
[1] Opnet
opnet-network-simulator/.
simulator,
2015.
https://opnetprojects.com/
[2] Hyperopt, 2018. http://hyperopt.github.io/hyperopt/.
[3] M. Al-Fares, R. Kapoor, G. Porter, S. Das, H. Weatherspoon, B. Prabhakar, and
A. Vahdat. Netbump: User-extensible active queue management with bumps
on the wire. In 2012 ACM/IEEE Symposium on Architectures for Networking and
Communications Systems (ANCS), pages 61–72, Oct 2012.
[4] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat. A scalable, com-
modity data center network architecture. In Proceedings of the ACM SIGCOMM
2008 Conference on Data Communication, SIGCOMM ’08, pages 63–74, New York,
NY, USA, 2008. ACM.
[5] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Ma-
tus, Rong Pan, Navindra Yadav, and George Varghese. Conga: Distributed
congestion-aware load balancing for datacenters. In Proceedings of the 2014 ACM