### Figure 1: Simulation Time Comparison
For a 20-second simulated time, the full-fidelity simulator required approximately 1 week and 5 days. In contrast, MimicNet, in aggregate, only required 8 hours and 38 minutes, with just 25 minutes used for the final simulation—resulting in a 34× speedup. Longer simulation periods or multiple runs with different workload seeds would lead to even larger speedups.

### Simulation Time Speedup
To better understand the benefits of MimicNet, we focus on the non-fixed-cost component of the execution time. Figure 10 illustrates the speedup provided by MimicNet after accounting for the initial, fixed cost of training the cluster model. For each network configuration, both MimicNet and the full-fidelity simulator were run over the same sets of generated workloads. We then report the average speedup and the standard error across these workloads.

In both systems, simulation time consists of setup time (constructing the network, allocating resources, and scheduling traffic) and packet processing time. MimicNet significantly speeds up both phases.

MimicNet can provide consistent speedups up to 675× for the largest network that the full-fidelity simulation could handle. Beyond this size, the full-fidelity simulation could not complete within three months, while MimicNet can finish in under an hour. Surprisingly, MimicNet is also 7× faster than flow-level approximation at this scale, as SimGrid must still track all Mimic-Mimic connections.

### Groups of Simulations
Simulations are often run in groups to test different configuration or workload parameters. To evaluate this, we compare several approaches to running groups of simulations using two metrics: (1) simulation latency, i.e., the total time to obtain the full set of results, and (2) simulation throughput, i.e., the average number of aggregate simulation seconds processed per second. This section focuses on the effect of network size, but we also evaluated the effect of simulation length and compute consumption in Appendices F and G.

#### Simulation Latency
For latency, given \( N \) cores in a machine and \( S \) simulation seconds, we consider five different approaches:
1. **Single Simulation**: One full simulation running on a single core.
2. **Single MimicNet w/ Training**: One end-to-end MimicNet instance, including training.
3. **Single MimicNet**: One MimicNet instance reusing an existing model.
4. **Partitioned Simulation**: \( N \) full simulations, each simulating \( S/N \) seconds.
5. **Partitioned MimicNet**: \( N \) MimicNet instances, each simulating \( S/N \) seconds.

Figure 11 shows the results for network sizes ranging from 8 to 128 clusters. Key observations include:
- For smaller networks, the training overhead in MimicNet is significant, making 'single MimicNet w/ training' slower than 'single simulation'.
- At 64 clusters, even with training, MimicNet outperforms any full simulation approach.
- At 128 clusters, MimicNet is 2-3 orders of magnitude faster than full simulations. Partitioning further enhances MimicNet's advantage, especially in larger simulations where the removal of most packets/connections reduces memory footprint.

#### Simulation Throughput
For throughput, we consider a similar set of five approaches:
1. **Single Simulation**: One full simulation.
2. **Single MimicNet w/ Training**: One end-to-end MimicNet instance, including training.
3. **Single MimicNet**: One MimicNet instance reusing an existing model.
4. **Parallel Simulation**: \( N \) full simulations, each simulating \( S \) seconds.
5. **Parallel MimicNet**: \( N \) MimicNet instances, each simulating \( S \) seconds.

Figure 12 shows the throughput results for various network sizes. MimicNet maintains high throughput regardless of network size because the amount of observable traffic is roughly constant. Single simulations slow down substantially as the network grows, and at 128 clusters, full simulation is almost five orders of magnitude slower than real-time. Parallel simulation improves throughput by a factor of \( N \), but even with unlimited memory, MimicNet outperforms parallelized simulation by 2-3 orders of magnitude at 128 clusters.

### Use Cases
MimicNet can approximate a wide range of protocols and provide actionable insights. Two potential use cases are:
1. **Configuration Tuning for DCTCP**: MimicNet can help tune the ECN marking threshold, \( K \), which affects both latency and throughput. Figure 13 compares the 90-pct FCT for different \( K \) values. MimicNet provides the same answer as full simulation but 12× faster.
2. **Comparing Protocols**: MimicNet is accurate enough to compare different transport protocols. Figure 14 shows the FCT distributions for Homa, DCTCP, TCP Vegas, and TCP Westwood. MimicNet closely matches the full-fidelity simulation, with the approximated 90-pct and 99-pct tails within 5% of the ground truth.

### Conclusion and Future Work
This paper presents MimicNet, a system that enables fast performance estimates of large data center networks. Through machine learning and modeling techniques, MimicNet exhibits super-linear scaling compared to full simulation while maintaining high accuracy. Future work includes improving speed with incremental model updates, enhancing accuracy with models involving more network events, and extending its capabilities for evaluating more data center protocols and architectures.

### Acknowledgments
We gratefully acknowledge Rishikesh Madabhushi, Chuen Hoa Koh, Lyle Ungar, our shepherd Brent Stephens, and the anonymous SIGCOMM reviewers for their help and thoughtful comments. This work was supported in part by Facebook, VMWare, NSF grant CNS-1845749, and DARPA Contract No. HR001117C0047. João Sedoc was partially funded by Microsoft Research Dissertation Grant.

### References
[1] Opnet network simulator, 2015. https://opnetprojects.com/opnet-network-simulator/
[2] Hyperopt, 2018. http://hyperopt.github.io/hyperopt/
[3] M. Al-Fares, R. Kapoor, G. Porter, S. Das, H. Weatherspoon, B. Prabhakar, and A. Vahdat. Netbump: User-extensible active queue management with bumps on the wire. In 2012 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS), pages 61–72, Oct 2012.
[4] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat. A scalable, commodity data center network architecture. In Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication, SIGCOMM ’08, pages 63–74, New York, NY, USA, 2008. ACM.
[5] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Matus, Rong Pan, Navindra Yadav, and George Varghese. Conga: Distributed congestion-aware load balancing for datacenters. In Proceedings of the 2014 ACM...