E which is still listening for a command (2). E will forward the
injected audio command to AVS (3), which interprets it and invokes
malicious Skill S (4). Malicious Skill S is now started and can return
7https://www.amazon.com/gp/help/customer/display.html?nodeId=201848700
Speech toText service /library MaliciousSkill withbackend Malicious IoTdeviceAlexa Voice Service (AVS)DSTTSUser (U) Amazon Echo (E) AdversarySession 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand467(a) Command jamming
(b) Malicious Skill invocation
(c) Retrieving benign data from benign Skill
(d) Echoing modified data to user
Figure 3: Depiction of the four attack components needed for the Man-in-the-Middle attack
arbitrary text to be echoed through E in Alexa’s voice. The inaudible
command injection is realized by utilizing the amplitude modulation
technique discussed in Sect. 2.2.
3.2.3 Retrieving benign data from benign Skill. The purpose of the
third attack component is to fetch benign data from a benign Skill
that user U wanted to invoke with his command. This is shown in
Fig. 3(c). D first starts a new session with AVS and forwards U ’s
recorded command to it, just like a benign Alexa-enabled device
would do (1). AVS will then invoke the requested Skill and pass
the command to it (2). Subsequently, the benign Skill will return a
textual representation of the response to AVS (3) which will then
use the Alexa TTS engine to create a voice audio file out of it
and forward it to D (4). Now the malicious device D will send the
audio file containing the response to a STT service (5) for it to be
transcribed back into text (6). Now D knows what data the victim
wanted to fetch from the requested Skill.
3.2.4 Echoing modified data to user. The fourth component, de-
picted in Fig. 3(d), shows how modified data is finally echoed to
the victim user U . After arbitrarily modifying the response data
received from the benign Skill in the previous step (1), D passes
this text to the backend of Skill S (2). S then passes the received text
to the already established AVS session (3) which will then create
an audio file with the help of Alexa’s TTS engine. This audio file is
passed to E (4) to be played back to user U (5).
3.3 Hijacking Built-In Alexa Commands
The most straightforward attack variant aims at hijacking built-in
Alexa commands like “Alexa, will it rain today?” (i.e., commands
containing no Skill name, as shown in Fig. 4 segment 0). The com-
mand can be redirected to malicious Skill S by associating the
utterance “will it rain today” with an intent of Skill S, and letting
malicious IoT device D inject an inaudible voice command “using
Evil” (where Evil is the invocation name of Skill S) when the user
stops speaking to Alexa, as shown in Fig. 4 segment 1. This will
cause AVS to invoke Skill S using an intent corresponding to this
utterance. The malicious Skill S can then return any desired re-
Figure 4: Abstract representation of how injected voice com-
mands lead to different interpretations of the intended com-
mand. Segment 0 depicts a command activating a built-in
feature, segment 1 and 2 are depicting redirections to the
Skill “Evil”.
sponse to AVS, which will be echoed back to user U using Alexa
device E, thereby making the user believe he is talking to Alexa’s
built-in functionality and not to Skill S.
3.4 Skill Redirection Attack
One drawback of the above attack is that it can’t be used to redirect
commands that are targeted at a particular benign Skill. For example,
if the user wants to lock his smart lock front door with the help of
the Security Skill, he will issue the command “Alexa, ask Security
to lock the front door”. Just injecting the Skill S’s invocation name
after the command (“using Evil”) would not be sufficient, as the
1. "Alexa, "2. MaliciousDevice (D)Alexa (E)2. "ask     "User (U)jammed by DMaliciousDevice (D)1. stops jamming2. "start Evil"Alexa (E)AVS3. "start Evil"4. invokes "Evil"Malicious Skill(S)User (U)MaliciousDevice (D)STTAVSBenign Skill1. "ask  "2.  3. response (text) 4. response    (audio ﬁle) 5. response (audio ﬁle)6. response (text)User (U)MaliciousDevice (D)1. modiﬁes responseMalicious Skill(S)2. modiﬁed response (text) AVS3. modiﬁed response (text) Alexa (E)4. modiﬁed response (audio ﬁle) 5. modiﬁed response (audio) User (U)Alexa, will it rain today?}   utterance: "will it rain today",   skill: "evil" } }   utterance: null,   skill: "evil" } }   utterance: "will it rain today",   skill: null } 012Alexa, will it rain today?, using Evil!Alexa, ask X for Y!, start Evil!Session 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand468resulting command heard by E would contain the invocation name
of two different Skills, resulting in AVS ignoring this command.
In order to redirect Skill invocations, we need to enhance the
attack by preventing Alexa from understanding the genuine Skill’s
invocation. The obvious way to achieve this is by device D injecting
inaudible noise, as depicted in Fig. 4 segment 2, at the same time the
user is speaking the command, essentially jamming the user’s input
so that Alexa will not be able to understand it (a similar jamming
attack, however not targeting voice assistants, has recently been
published independently to our work by Roy et al. [14]).
Our evaluation of jamming of user issued commands (cf.
Sect. 5.1.2) showed that to be effective, D needs to jam the user
command starting immediately after the wake-word, up until the
command is finished. This means D needs to start jamming even
before it can know which command the user is going to issue. To
still be able to provide meaningful responses to the user’s command
the malicious device D needs to independently record the command
of the user while simultaneously jamming it, as shown in Fig. 3(a).
This is feasible because different microphones are sensitive to dif-
ferent AM carrier wave frequencies. Device D’s microphone will
therefore not pick up the emitted inaudible jamming signal as it
is not sensitive to the same frequency as the microphone of the
targeted Alexa device. After recording the command, D uses the
speech-to-text service STT to convert the recorded audio to text
and forwards it to Skill S’s backend. After the user command has
been jammed, D executes the Malicious Skill invocation attack com-
ponent (Fig. 3(b)) by inaudibly injecting the redirection command
to invoke Skill S and cause AVS to start a session with it.
If the transcribed user command text returned by STT contains a
Skill name or utterance the adversary wants to hijack, the malicious
Skill S returns a reply to AVS to be played back to the user, as de-
picted in Fig. 3(d) (Echoing modified data to user component). This
essentially denies the user access to this Skill while simultaneously
making him think the response is coming from the genuine Skill.
However, if the user command does not contain Skill names or
utterances the adversary wants to hijack, it still needs to provide a
plausible response to the user in order to avoid the user realizing
that he is the victim of an attack. For realizing this the adversary
can utilize the benign functionality of the AVS by playing a Skill-
in-the-Middle attack as discussed below.
3.5 Skill-in-the-Middle Attack
In this variant of our attack the adversary stages a full man-in-
the-middle attack between the user and benign Alexa Skills. After
performing the Command jamming and Malicious Skill invocation
attack components as described above, device D will use the Re-
trieving benign data from benign Skill attack component as shown
in Fig. 3(c) over a separate session with AVS to get benign data from
the Skill that user U wanted to invoke. After obtaining the benign
data, device D can process it and modify it in any way it wants and
use the Echoing modified data to user attack component (Fig. 3(d))
to return a forged response to the user with the help of malicious
Skill S. The Skill-in-the-middle attack gives the adversary thus full
control over the conversation of the victim user U and any Alexa
functionalities, in particular, Skills.
Figure 5: Components of the attack setup
The Skill-in-the-Middle attack is particularly devastating, as an
adversary can use it, e.g., to modify arbitrary responses of benign
Skills. For example by manipulating the responses of a Skill pro-
viding the latest quotes on share prices it can falsify the quotes of
particular companies’ stock in a way that may trick the victim into
making valuable financial transactions based on false information
with potentially far-reaching consequences.
Note that all attack variants presented above use Alexa’s genuine
voice and device (which the user typically trusts) to deliver the
modified or wrong information, giving the adversary the potential
to substantially mislead the user or fool him to take specific actions
as desired by the adversary.
4 IMPLEMENTATION
The overall design of our system is shown in Fig. 5. It consists of a
malicious IoT device D and Skill S. The Skill is implemented with
the help of a lambda function and a state server that are both under
control of the adversary. Our implementation also takes use of an
online speech-to-text service STT .
4.1 Malicious IoT Device
To evaluate our attack we constructed a prototype of a malicious
IoT device comprising following relatively inexpensive off-the-shelf
hardware components shown in Fig. 6: a Fostex FT17H wide-range
tweeter speaker8 (ca. US$ 70) connected to a YAMAHA R-S202D 9
amplifier (ca. US$ 200). With this setup we were able to produce
signals in the frequency range of 500 Hz to 50 kHz. The amplifier
amplifies signals with frequencies up to 100 kHz without much
distortion (±3 dB), this makes it suitable for our purpose.
For recording sounds and measurements we used a MiniDSP
UMIK-110 microphone. To run the software we used a Lenovo
ThinkPad T43011 laptop. This laptop has a sound card (and drivers)
capable of generating signals with frequencies of up to 192 kHz.
8https://fostexinternational.com/docs/speaker_components/pdf/FT17H_rev3.pdf
9https://europe.yamaha.com/en/products/audio_visual/hifi_components/r-
s202d/index.html
10https://www.minidsp.com/products/acoustic-measurement/umik-1
11https://www3.lenovo.com/us/en/laptops/thinkpad/t-series/t430/
EC2 running theadversarialState ServerLambda functionrunning the adversarialSkill's backendMalicious IoTdevice DAVSMalicious SkillS's Interface inAVS CloudSpeech to textservice STTAmazon Echo EMalicious Skill S's backendSession 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand469be done in a way that the Skill still provides a hidden entry point
for the adversary to trigger the malicious functionality after the
Skill has passed certification. This can be done either by directly
triggering a dedicated intent for the malicious functionality, or,
it can be activated when the issued command contains a specific
keyword as a command parameter.
Naturally, the utterance triggering a malicious intent should
be known only to the adversary, which means that it needs to be
hidden from regular users. This is, however, not difficult, as there
is no apparent way (as a normal user) to get a list of all utterances
a Skill can understand or react to. In the certification process only
three different commands have to be declared to be listed in the
official interface description of a Skill in Amazon’s Skill repository.
Any additional commands can be left ’invisible’ to regular users.
All commands and intents the Skill understands are tested by
Amazon during certification for their functionality, but after pass-
ing certification the adversary can change their behavior in its
backend system without having to re-certify the Skill. Similarly,
the adversary can change the behavior of an intent in its backend
in response to specific Slot values triggering malicious functional-
ity. Effectively, malicious Skill S will therefore have two handlers:
one for its public ’benign’ functionality and a second one realizing
malicious functionality. The second handler is triggered by the first
handler when it receives a malicious functionality-triggering intent
or a specific keyword value as a command parameter.
4.2.3 Malicious Skill’s Interaction Model. A Skill’s Interaction Model
is a mapping of the user’s input to intents, which are passed to the
Skill’s backend. The user’s input consists of utterances which can
include Slots. From the available Skill types14 we utilize so-called
Custom Skills for our attack since they provide the most flexible
invocation options and functionality. An example command to in-
voke a Custom Skill could be: “Alexa, ask Recipes how do I make an
omelet.” where Recipes is the invocation name and “how do I make
an omelet” is an utterance for the Recipes Skill to process15, 16.
Our malicious Skill’s Interaction Model is quite simple. There
are only four intents of which three provide benign functionality
and are publicly advertised as well as one malicious intent that is
not disclosed to users. This intent consists of a genuine looking
utterance and an utterance consisting solely of two Slots for which
we do not provide sample values. Normally a Slot would match ex-
actly one word (or more with provided samples), but this utterance
consisting of two times the same Slot separated by a space will
match any number of words. This way Alexa can be tricked into
passing S the full transcript of the user command as a Slot value
and match every possible sentence the user says (as we might not
know what the impersonated Skill wants the user to say).
Skill Certification. As stated in Sects. 2.1 and 4.2.2, malicious
4.2.4
Skill S needs to be certified to be listed in the Alexa Skill Store so
that it can be enabled on every Alexa-enabled device via companion
app or voice command. Our Skill S disguises itself as a Skill for
14https://developer.amazon.com/docs/ask-overviews/understanding-the-different-
types-of-skills.html
15https://developer.amazon.com/docs/custom-skills/understanding-how-users-
invoke-custom-skills.html
16https://developer.amazon.com/docs/custom-skills/understanding-custom-
skills.html
Figure 6: Photo of the hardware setup mimicking the mali-
cious IoT device. The laptop is connected to the microphone
and the amplifier. The tweeter is connected to the amplifier
and aligned in the direction of the Amazon Echo Dot.
The malicious IoT device’s software consists of a hotword detec-
tion engine called Snowboy12, a library for communicating with
AVS and means to connect to the STT service. The Snowboy engine
was modified to also be able to save recorded audio, and return the
status of detection. With these modifications it is possible to start
the hotword detector with the pre-trained Alexa detection model,
which is shipped with Snowboy, and record audio after the hotword
is detected.
4.2 Malicious Skill
The malicious Skill S consists of two components: a state server
and lambda function realizing the functionality of the Skill.
4.2.1 Adversarial State Server. When the lambda function of Skill
S is invoked by AVS in a Skill redirection or Skill-in-the-middle
attack, it initially can’t know how to correctly react to the user
command, as the invocation name of the intended Skill and any
command parameters are jammed by D and thus not understood
by AVS. Device D needs therefore a way to notify S the hijacked
Skill’s name and possible command parameters. For this it uses S’s
RESTful state server, which is continuously polled by S’s lambda
function. Device D uses POST instructions to store the instructions
of what Skill to mimic or what text to reply on the state server, from
where the lambda function will receive it by polling the state server.
Polling of the state server is performed by the lambda function
using GET requests once every second.
4.2.2 Adversarial Skill’s Backend Lambda Function. Skill S’s lambda
function is implemented using alexa-sdk and Node.js on AWS Lambda13,
which is a fee-based Function as a Service (FaaS). AWS Lambda is
the platform recommended by Amazon for Skill backends. To get a
malicious Skill certified and into the official Alexa Skill store the
adversary has to create a functional and apparently benign Skill
that can pass the screening performed by Amazon. This needs to
12https://snowboy.kitt.ai/
13https://aws.amazon.com/lambda/features/
Lenovo ThinkPad T430 laptopGenuine Amazon Echo DotFostex FT17H wide-range tweeterMiniDSPUMIK-1 microphoneYAMAHA R-S202D amplifierSession 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand470retrieving statistics about YouTube channels. It is called “YouTube
Statistics (Unofficial)” and it’s invocation name is “you statistics”.
We were able to get our Skill certified and added to the Store.
The benign functionality of Skill S returns statistics about
YouTube channels. It can be queried to return for a given chan-
nel name the number of subscribers, total number of views, the
title of the latest activity or the title of the latest playlist. We didn’t
disclose the corresponding utterance for querying playlist titles, so
this intent can be used for triggering the malicious functionality.
In addition, if an utterance with a Slot value “evil” is passed to the
skill, a state variable is set, which will cause the malicious handlers
to handle all intents instead of the handlers providing the benign
functionality.
We minimized the potential exposure of users to our malicious
Skill functionality by disabling listening for the utterance or Slot
value when no evaluation was done. In addition, the Skill was
available only briefly in the Skill store, as development work was
done using an unlisted development Skill. We are therefore certain
that no user enabled the malicious functionality.
4.3 Attack Signal Generation
We implemented the method for shifting voice samples of utter-
ances and the jamming sound into the ultrasonic spectrum with
MATLAB. The details of the used signal modulation are presented
in Appendices A and B. We used for low-pass filtering the Equirip-
ple single-rate FIR filter17 and we were able to automatically create
ultrasonic audio files with different carrier wave frequencies from
20 kHz to 40 kHz for evaluation purposes. We encoded the audio
files with the Free Lossless Audio Codec (FLAC), a lossless audio
format suitable for storing also ultrasonic frequencies.
Stealthiness improvements. With our initial setup a crackling/pop
sound could be heard when playback of the ultrasonic audio file
was started and stopped. We removed this unwanted noise with a
linear fade-in of 0.16 ms corresponding to 30 samples at a sampling
rate of 192 kHz and a linear fade-out of one second (192000 samples)
applied on an added one second of silence to the original audio file.
As a result, the starting and ending of the ultrasound injection can’t
be heard as a clearly audible sound.
4.4 Putting it All Together
A user typically starts an interaction by uttering the wake-word
“Alexa”. When the wake-word is detected by the malicious IoT
device D, it immediately starts jamming the user’s subsequent
command by using a pre-recorded and modulated audio file with
a duration of > 7 seconds and consisting of sawtooth signal noise.
While D is jamming the user command, it also records it. This
is possible because D will not record its own jamming sound, as
the microphone of D is not sensitive to demodulating signals with
the same carrier wave frequency as the Alexa-enabled device’s
microphone. When the user stops speaking, D stops jamming and