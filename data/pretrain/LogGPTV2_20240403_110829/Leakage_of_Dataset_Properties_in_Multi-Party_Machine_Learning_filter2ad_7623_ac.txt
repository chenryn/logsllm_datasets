10 000 /4000
1 500 / 400
1 200
10 000
1 000
1 000
94
200
1 000
Table 2: Dataset split during the attack where #Dattack is
the number of inference queries the attacker makes to the
model.
use 10 000 or 4 000 samples as Daux to train the shadow
models and the attacker uses 1 000 samples in Dattack
to query the model and obtain the attack vector for the
meta-classiﬁer. Table 2 summarizes the splits for all other
datasets. In Section 6.4 we show that a small number of
samples in Dattack can lead to high attack accuracy as
well (e.g., 200 vs. 1 000 for the Amazon dataset).
The distribution of the values of the sensitive at-
tribute A in datasets is determined as follows. We con-
sider the default split of 33:67 in the attacker’s data Dadv
(e.g., 33% of records are books). The attack is evalu-
ated against several Dhonest datasets for each possible
split. For example, we evaluate our attack on 100 Dhonest
datasets: half with 33:67 split and half with 67:33 split
in Sections 6.1 and 6.2. Throughout all experiments, the
Daux always has 50:50 split.
Attack setting. We report our main results on attack
in the black-box setting; white-box results are deferred
to Appendix B. We use two different meta-classiﬁers
depending on the target model. For multinomial logistic
regression, LSTM and GCN, the meta-classiﬁer model is
a binary logistic regression model. For MLP as the target
model, we use a two-layer network with 20 and 8 hidden
units and a learning rate of 0.001. The meta-classiﬁer
models are trained using Adam optimizer.
We perform the attack when the model is trained with
the sensitive variable (A) and without it ( ¯A). For the ¯A set-
ting, the attribute A is omitted from the machine learning
pipeline, including the shadow model training and con-
struction of Dattack. This setting allows us to understand
the risk of leaking a sensitive attribute, even when that
attribute is censored during training. For Yelp-Health, we
report only ¯A results as LSTM takes the text data, and A
would be an additional feature.
Types of experiments. We study how correlations be-
tween attributes affect the attack. We show that informa-
tion is leaked even when A is not correlated with the ﬁnal
task. We demonstrate our attack on attribute correlation
as present in real dataset distributions (shown in Table 3)
as well as artiﬁcially injected correlation using a syn-
thetic sensitive variable. The latter allows us to control
the correlation between the variables.
Real Data. For the experiments where all features are
from the real data, including the sensitive variable, we set
different variables as sensitive (A) for each dataset and
perform a black-box attack using a default split of 33:67
for the sensitive attribute in the attacker’s data (Dadv).
We compute the pairwise correlation among all the
variables using Pearson correlation coefﬁcient [44] for
numerical-numerical variables, Cramer’s V [10] for
categorical-categorical variables, point-biserial correla-
tion coefﬁcient [49] for binary categorical-numerical vari-
ables, and ANOVA for multi-level categorical-numerical
variables. Based on the observed correlations, for each
dataset, we identify the case among those introduced
in Section 3. Most scenarios correspond to X ∼ A,Y ∼ A.
Details on correlation factors for all datasets are deferred
to Appendix A.
Synthetic Data. For synthetic experiments, we create a
new synthetic attribute as our sensitive variable A for
the Adult and Health datasets. We add a correlation
of A to a subset of variables in the dataset, denoted as
X(cid:48) ⊆ X, and the target variable Y , depending on the cases
outlined in Section 3. We introduce the correlation by
replacing attribute values in X(cid:48) and/or Y for each record
with values that have an injected correlation with A. For
Adult dataset, X(cid:48) is Income, for Health dataset, X(cid:48) =
{DrugCountAve, LabCountAve, ClaimsTruncated}.
The variable A takes values  5 that are split
using 33:67 ratio in the adversarial party’s dataset.
The honest party has two possible splits: 33:67 ratio
and 67:33 ratio. The attacker’s goal is to guess the
distribution of A in the data of Phonest.
6 Attack Results
We evaluate for attribute leakage in the following set-
tings: the single-party case where an attacker learns the
distribution of an attribute in the training set and the
multi-party case where an attacker learns the distribution
of an attribute in the data of the honest party. Apart from
inferring the dominant attribute (e.g., there are more fe-
males than males in a dataset), we perform a ﬁne-grained
attack that learns a precise distribution of the two attribute
values (e.g., 70% of the dataset are females). We further
use this ﬁne-grained attack to infer the change in the at-
tribute distribution in a model update scenario where the
model is updated either due to a new party joining or new
data arriving. Attack accuracy higher than the probability
2694    30th USENIX Security Symposium
USENIX Association
Datasets
Sensitive attribute A
Task Y
Health [3]
Gender
ClaimsTruncated
DaysInHospital
Adult [33, 37]
Gender
Income
EducationLevel
Crime [37]
Yelp-Health [4]
Amazon [35]
TotalPctDivorce
Income
CrimesPerPop
Specialty
ReviewRating
ProductType
ReviewScore
Correlation
X ∼ A,Y⊥A
X ∼ A,Y⊥A
X ∼ A,Y ∼ A
X ∼ A,Y ∼ A
X ∼ A,Y⊥A
X ∼ A,Y ∼ A
Table 3: Datasets, tasks and attribute-label correlation where ∼ and ⊥ indicate correlation and no correlation, respectively.
of a random correct guess is considered successful as
this indicates that conﬁdential property (i.e., information
about Phonest’s data) will be leaked to the attacker in the
majority of cases.
We report our attack results in the stronger black-box
setting for real, synthetic, and ﬁne-grained experiments.
We evaluate the white-box attack, where the attacker has
access to model parameters, only on the synthetic data.
We summarize our key ﬁndings below:
• Leakage of sensitive dataset properties in honest
party’s data is possible even when the sensitive at-
tribute itself is dropped during training and has low
or no correlation with the ﬁnal task. We show that
the attack accuracy drops only by a few percent
when A is not present in many cases.
• An adversary can learn the attribute properties of
the honest party’s data irrespective of whether it
contributes data (multi-party) or not (single-party)
to the training dataset.
• For the models and datasets considered in this paper,
our property leakage attack is dataset and model-
agnostic and works on tabular, text, or graph data.
• Fine-grained attacks can be used to predict a pre-
cise distribution of the attribute as well as learn the
change in data distribution during model updates.
6.1 Multi-Party Setting
Real Data. Table 4 shows the attack accuracy for cor-
relations observed in the real distribution of datasets, with
the larger size of Daux as listed in Table 1. The attack
accuracy with the smaller size of Daux is deferred to Ta-
ble 12 in Appendix B. We see that the attack accuracy
is always better than a random guess in all experiments,
regardless of whether the sensitive attribute is included
in the training data or not.
We make the following observations. The attack accu-
racy for Adult data with Income as the sensitive attribute
is the highest with 98% and 96% when the target model is
trained with and without A, respectively. Overall, the at-
tack accuracy ranges between 61-98% when trained with
sensitive variable (A) and 59-96% without ( ¯A), respec-
tively. The results for ¯A are always lower than with A
but are, however, above the random guess baseline of
50%. For the Amazon dataset, we observe that attack
accuracy is higher for fewer output classes. We conﬁrm
this observation later in Figure 4. We also note that the
attack accuracy decreases as the size of Daux decreases
as shown in Appendix B.
To understand how the correlation between A and other
features inﬂuences the attack, we determine which at-
tributes X(cid:48) ⊆ X are correlated with A. We set X(cid:48) to vari-
ables based on their correlation factors. Details on how
X(cid:48) of each dataset was determined based on correlation
factors is deferred to Appendix A. In Table 4, # X(cid:48) de-
notes the number of attributes correlated with the sensi-
tive attribute A. We note that simultaneously controlling
the number of correlated attributes and their correlation
strength is hard on real data, so we also use synthetic
datasets. We observe that, for the same dataset, the attack
accuracy increases with a higher number of correlated
attributes X(cid:48) and the sensitive attribute A.
We show the accuracies for both the pooled model and
the honest party’s local model in Table 11 in Appendix B.
Across all these experiments, we observe a utility in-
crease ranging from 0.58% and 5.90% for the honest
party, which motivates the honest party to collaborate
and train a joint target model with the other party.
Synthetic Data. Table 5 shows our results with a syn-
thetic variable A introduced in the Adult and Health
USENIX Association
30th USENIX Security Symposium    2695
Datasets
(Output Classes)
Model Type
Attack Accuracy
A
¯A
A
Health (2)
Multi-layer Perceptron
Adult (4)
Logistic Regression
Crime (3)
Multi-layer Perceptron
Yelp-Health (2)
Amazon (2)
Amazon (6)
Amazon (11)
LSTM
GCN
GCN
GCN
.61
.75
.83
.98
.61
.78
-
.86
.62
.67
.59
.71
.81
.96
.59
.60
.74
.72
.63
.61
Gender
ClaimsTruncated
Gender
Income
TotalPctDivorce
Income
Specialty
ProductType
ProductType
ProductType
# X(cid:48)
24/139
54/139
5/11
9/11
26/98
38/98
review text
graph
graph
graph
Table 4: Multi-Party Setting: Black-box attack accuracy for predicting the value of the distribution of sensitive variable A
in the dataset of Phonest. The attacker tries to guess whether values of A are split as 33:67 or 67:33 in Dhonest when its
own data Dadv has 33:67 split. Columns A and ¯A report the accuracy when the sensitive variable is used for training
and not, respectively. X(cid:48) indicates with which features in the dataset and with how many of them A is correlated. Since
attack accuracy based on a random guess is 0.5, the attacker is always successful in determining the correct distribution.
dataset for the multi-party setting. Here, we train the
same dataset using both logistic regression and the neural
network model (MLP). Recall that the synthetic attribute
is introduced to imitate a sensitive variable to control its
correlation with other variables. To this end, we create
datasets for different correlation criteria among the sensi-
tive variable A, the output Y , and the remaining variables
X. We report two ﬁndings.
First, logistic regression models appear to be at a
higher risk, with average attack accuracy being higher as
compared to neural network models: 84.5% vs. 71.3%
for Adult and 80.2% vs. 70.8% for Health datasets. We
suspect that this is mainly due to their simple architecture,
which is easy to learn using a meta-classiﬁer.
Second, the attack works well (greater than 74%) when
the sensitive variable A is correlated with the target vari-
able Y irrespective of its relation with X, i.e., cases where
Y ∼ A. The attack accuracy is almost equal to a random
guess when Y⊥A. Recall that in the case of X ∼ A, not
all features used for training are correlated with A but
only those in a subset of X, X(cid:48). To understand this sce-
nario further, we reduced the number of features used
during training to 3 (we refer to this setting as R in the
tables). As the number of training features decreases, the
correlation signal between A and X(cid:48) becomes stronger,
and the logistic regression model can capture that.
Our experiments for the case when both X and Y are
independent of the sensitive variable A exhibit attack
accuracy that is close to a random guess. This is expected
as the variable has no correlation that the model can
memorize, and hence we exclude them from Table 5.
6.2 Single-Party Setting
In addition to our motivating scenario of the multi-
party setting, we evaluate the efﬁcacy of our attack in the
single-party setting where the attacker does not contribute
towards the training data. For example, this corresponds
to a scenario where a model is trained on data from only
one hospital and is offered as an inference service for
other hospitals. Table 6 shows the result for our attack
using synthetic data for the Adult and Health dataset
when the model is trained using both logistic regression
and neural networks. We see that the attack in the single
party setting is stronger since the adversary does not
provide its own data, which may dilute the signal from
the other party. For the case where Y ∼ A, the attack
accuracy is higher than 90%, even if the attribute itself
is not used during training. This shows that our attack
is highly successful even when the attacker does not
participate in the training process.
2696    30th USENIX Security Symposium
USENIX Association
Model
Datasets
Synthetic Variable
X ∼ A,Y ∼ A
X⊥A,Y ∼ A
X ∼ A,Y⊥A
X ∼ A,Y⊥A (R)