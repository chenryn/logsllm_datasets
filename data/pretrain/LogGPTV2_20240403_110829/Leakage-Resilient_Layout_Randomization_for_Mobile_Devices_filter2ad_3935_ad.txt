to independently randomize the position of each. To do this,
we replace the conventional PC-relative address computation
with dynamic relocations assigned by the program loader. This
change led to a geometric mean speedup of 4% (labeled Code
and Data Section Decoupling in Figure 4).
F. Full LR2
The aggregate cost of enabling all techniques in LR2 is
6.6% on average (see Full LR2 in Figure 4). This includes
the cost of pointer hiding, software-enforced XoM, register-
register addressing restrictions, ﬁne-grained diversity, and the
impact of decoupling code and data. This means that our
pure software approach to leakage resilient diversity for ARM
has about the same overhead as hardware-accelerated leakage
resilient diversity for x86 systems (6.6% vs. 6.4% [14]).
Because the removal of PC-relative address computations yield
a speedup, the cost of individual transformations sometimes
exceed the aggregate cost of LR2. An earlier version of our pro-
totype that did not remove PC-relative address computations
to decouple code and data sections had an average overhead
of 8.4%.
G. Memory Overheads
Finally, in addition to running time, we also measured
code section size of the protected SPEC CPU2006 binaries.
Forward-pointer hiding had very little overall impact on code
size, leading to an increase of 0.9%. Return-address hiding
8
NaCl
XoM
)
%
(
n
w
o
d
w
o
l
S
e
c
n
a
m
r
o
f
r
e
P
60
50
40
30
20
10
0
m
k
h
m
g o b
m er
sje n g
lib q u a ntu
m
h 2 6 4ref
astar
m ilc
n a m
d
s o ple x
m
lb
M e a n
G e o
Figure 5: Comparing software XoM to SFI (NaCl) to quantify
effect of load-mask optimization.
adds at least four instructions to most functions, which resulted
in a 5.2% code size increase. The additional load address
masking for software-enforced XoM increases the code size
by another 10.2%. However, removing the PC-relative address
computations decreases the code size by about 14% on aver-
age. Comparing the size of full LR2 binaries to legacy position
independent code shows an average increase of just 5.6%.
H. Impact of XoM-Speciﬁc Optimizations
Recall that the differences in threat models between soft-
ware XoM and SFI (Section IV-B0b) allow us to protect mul-
tiple uses of a load address using a single masking instruction.
To measure the impact of this optimization, we compare the
running time of the SPEC CPU2006 benchmarks that run
correctly when protected with NaCl to the cost of enforcing
XoM. For this experiment, we used the latest version5 of the
NaCl branch (pnacl-llvm) that is maintained as part of the
Chromium project. The results are shown in Figure 5. When
enforcing XoM using load masking,
the average overhead
is 6.6% (for the set of benchmarks compatible with NaCl)
whereas software-fault isolation, which also masks writes and
indirect branches, costs 19.1% overhead. We stress that we are
comparing two different techniques with different purposes and
threat models. However, these numbers conﬁrm our hypothesis
that our XoM-speciﬁc load-masking instrumentation reduces
overheads. A bigger impact can be seen when comparing code
sizes: XoM led to a 5.8% increase, while NaCl caused an
increase of 100%. This is a valuable improvement in mobile
environments where memory is a scarce resource.
VI. SECURITY ANALYSIS
Our primary goal in LR2 is to prevent disclosure of the
code layout, which enables sophisticated attacks [50] against
code randomization schemes [29]. By securing the code from
disclosure we can then rely on the security properties of
undisclosed, randomized code.
In order to launch a code-reuse attack the attacker must
know the code layout. By applying ﬁne-grained randomization,
5As of August 10, 2015
9
e.g., function or page reordering, we prevent all static code-
reuse attacks, since these attacks are not adjusted to each
target’s randomized code layout. For our proof-of-concept
we chose to build on function permutation as it is effec-
tive, efﬁcient, and easy to implement. However, as all code
randomization techniques, function permutation by itself is
vulnerable to an attacker who discloses the code layout at run
time [11, 20, 50]. Hence, we focus our security analysis of LR2
on resilience against direct and indirect information-disclosure
attacks targeting randomized program code.
A. Direct Memory Disclosure
Direct memory disclosure is when the attacker reads the
memory storing randomized code. JIT-ROP [50] is a prominent
example of this type of attack. JIT-ROP recursively discloses
and disassembles code pages at run time until enough gadgets
are disclosed to assemble and launch a ROP attack.
We prevent all direct disclosure attacks by masking mem-
ory loads in the protected application, i.e., we prevent loads
from reading the program code directly. Masking the load
address restricts any attempt
to read the code section to
the lower half of the memory space which contains only
data. Naively masking every load operation is inefﬁcient; we
therefore apply the optimizations described in Section IV-B
to reduce the number of masking instructions. Allowing some
unmasked load operations may appear to increase the risk of
an unsafe load instruction. However, we are careful to ensure
that all unsafe loads are restricted, as we show in the following.
1) PC-Relative Loads: All PC-relative loads with a con-
stant offset are guaranteed to be safe, since an attacker cannot
inﬂuence the address used during the load operation and only
legitimate data values are loaded in this manner. Therefore, we
need not mask these load instructions.
2) Constant Offsets: We allow loads from known safe base
addresses (i.e., already masked values) plus or minus a small
constant offset (less than 4KiB). Thus, if we ensure that the
base address must point into the data section, adding a guard
page between the data and code sections prevents the computed
address from reaching into the code section. We place an
unmapped 8KiB (2 pages) guard region between the data
and code sections to safeguard all possible constant offsets.
In addition, the addresses above 0xC0000000 are reserved
for kernel usage and will trigger a fault when accessed, so
programs are already safe from address underruns attempting
to read from the highest pages in memory by subtracting a
constant from a small base address.
We also allow limited modiﬁcation of masked addresses
without re-masking the result. If an address has already been
masked so that it is guaranteed to point into the data section,
adding or subtracting a small constant will result in either
an address that is still safe, or one that falls into the guard
region. In either case, the modiﬁed address still cannot fall
into the code section, and thus we do not need to re-mask
it. We perform this optimization for all constant stack pointer
adjustments.
3) Spilled Registers: When a program needs to store more
values than will ﬁt into the available registers, it stores (spills)
a value temporarily onto the stack to free a machine register.
As recently demonstrated, stack spills of sensitive register
contents can allow adversaries to completely bypass code-
reuse mitigations [11]. In our case, an attacker could attempt
to bypass LR2 by manipulating a previously masked register
while it is spilled to writable memory. Therefore, we do not
trust any address that is restored from writable memory and
always re-mask it before the value is used to address memory.
B. Indirect Memory Disclosure
Mitigating direct memory disclosure alone does not fully
prevent an attacker from leaking the code layout. An attacker
can indirectly gain information about
the code layout by
leaking readable code pointers from the data section [11, 20].
The necessary number of leaked code pointers for a successful
code-reuse attack depends on the granularity of the applied
randomization. For instance, in the presence of page-based
randomization, one code pointer allows the attacker to infer
4 KiB of code due to page alignment, whereas the attacker
has to leak more code pointers in the presence function-level
randomization to infer the same amount of code. To counter
indirect memory disclosure we create trampolines for forward
code pointers and encrypt return addresses.
1) Forward-Pointer Protection: An attacker cannot use
function pointers to infer the code layout because they point
to trampolines which reside in code segment. Hence,
the
destination address of a trampoline cannot be disclosed. The
order of the trampolines is randomized to prevent any corre-
lation between the trampolines and their target functions. This
constraints the attacker to whole-function reuse attacks. To mit-
igate such attacks, we suggest using the XoM-based technique
presented by Crane et al. [15] to randomize tables of function
pointers. This extension should be completely compatible with
the software-only XoM provided by LR2 without modiﬁcation
and would protect against the most prevalent types of whole-
function reuse: return-into-PLT and vtable-reuse attacks.
2) Return-Address Protection: Return addresses are a par-
ticularly valuable target for attackers because they are plentiful,
easy to access, and useful for code-reuse attacks, even with
some mitigations in place. For example, when attacking an
application protected by function permutation, an attacker can
leak return addresses to infer the address of the functions and
in turn the addresses of gadgets within those functions [11].
We prevent this by encrypting each return address with a per-
function 32-bit random number generated by a secure random
number generator. However, our adversary model allows the
attacker to leak all encrypted return addresses spilled to
the stack. Whiles she cannot infer code addresses from the
encrypted return addresses we conservatively assume that she
can relate each return address to its corresponding call site.
We must also address reuse of unmodiﬁed, disclosed return
addresses. In a previous indirect disclosure protection scheme,
Readactor [14], return addresses were vulnerable to reuse
as-is. Although Readactor prevented attackers from gaining
information about the real location of code surrounding a call
site, an attacker could potentially reuse call-preceded gadgets.
An attacker could disclose the trampoline return address cor-
responding to a given call site and jump into that trampoline,
which in turn jumps directly after the real call site. This
allows attackers to reuse any disclosed return addresses. To
mitigate this threat, the Readactor authors proposed additional
randomizations (register and callee stack slot permutation) to
attempt to disrupt data ﬂow between call-proceeded gadgets
and mitigate this threat.
In LR2 arbitrary reuse of return addresses is impossible. By
encrypting every return address with a per-callee encryption
key, our system prevents the attacker from invoking a call-site
gadget from anywhere but the corresponding callee’s return
instruction. In other words, encrypted return addresses can only
be used to return from the function that originally encrypted
the address. Thus,
the attacker is conﬁned to the correct,
static control-ﬂow graph of the program. This restriction is
similar to static CFI policies. However, we further strengthen
LR2 by applying register-allocation randomization. During our
analysis of state-of-the-art ROP attacks we determined that the
success of these attack is highly dependent on the data ﬂows
between speciﬁc registers. Register randomization will disrupt
the attacker’s intended data ﬂow between registers and hence,
have unforeseen consequences on the control ﬂow which will
eventually result in a crash of the application.
While our XOR encryption scheme uses a per-function
key, this key is shared across all invocations of a function.
That is, each time a return address is spilled from a function
F it
is encrypted with the same key KF . In most cases
this is not a problem, since function permutation prevents
an attacker from correlating return addresses encrypted with
the same key. However, if a function F1 contains two dif-
ferent calls to another function F2, the return addresses, R1
and R2 respectively, are encrypted with the same key KF2.
The attacker has a priori knowledge about these addresses,
since with function permutation they are still placed a known
(constant) offset apart. We believe this knowledge could be
exploited to leak some bits of the key KF2. To prevent this
known-plaintext attack we propose two options: (1) we can
either apply more ﬁne-grained code randomization, e.g., basic-
block permutation to remove the correlation between return
addresses or (2) fall back to using the trampoline approach to
protect return addresses as presented by [14] when a function
contains more than one call to the same (other) function.
These techniques remove the a priori knowledge about the
encrypted return addresses. In fact, return-address encryption
even strengthens the second approach because it prevents
trampoline-reuse attacks for return addresses.
C. Proof-of-Concept Example Exploit
We evaluate the effectiveness of LR2 against real-world
attacks by re-introducing a known security vulnerability
(CVE-2014-1705) into the latest version of Chromium
(v46.0.2485.0) and conducted our experiments on same setup
we used in our performance evaluation. The vulnerability
allows to overwrite the length ﬁeld of a buffer object. Once
this is done we can exploit this manipulated buffer object via
JavaScript to read and write arbitrary memory.
We constructed a JIT-ROP style attack that ﬁrst
leaks
the vtable pointer of an object Otarget to disclose its vtable
function pointers. Using one of these function pointers we can
infer the base address of the code section of Chromium. Next,
we use our information disclosure vulnerability to search the
executable code at run time for predeﬁned gadgets that allow
10
us to launch a ROP attack to mark data memory that contains
our shellcode as executable. Finally, we overwrite the vtable
pointer of Otarget with a pointer to an injected vtable and call
a virtual function of Otarget which redirects control ﬂow to the
beginning of our shellcode to achieve arbitrary code execution.
There are currently some efforts by the Chromium com-
munity to achieve compatibility with the musl C library. By
the time of writing this paper Chromium remains incompatible
which prevents us from applying the full LR2 toolchain. How-
ever, we applied our load-masking component while compiling
Chromium and analyze the effect this load-masking would
have on the memory disclosure we exploit.
Our analysis indicates that Chromium would immediately
crash when the attempted code read was restricted into an
unmapped memory area within the data section. Figure 6
shows how the function that this exploit uses to leak memory
is instrumented. After instrumentation, all load instructions
in the function cannot read arbitrary memory and must only
read from addresses that point into the data segment. Thus,
our proof-of-concept exploit would fail to disclose the code
segment at all and would instead crash the browser with a
segmentation violation.
VII. DISCUSSION AND EXTENSIONS
A. Auto-Vectorization
When loops have no data dependencies across iterations,
consecutive loop iterations may be vectorized automatically
by the compiler, using an optimization technique called auto-
vectorization. This technique computes multiple loop iterations
in parallel using vector instructions that perform the same
operation on a contiguous set of values.
While investigating the source of the higher overhead
for the hmmer benchmark, we found that one function—
P7Viterbi—accounts for over 90% of the benchmark’s
execution time. The main loop of this function is amenable
to vectorization as it exhibits a high degree of data par-
allelism [42]. Modern ARM processors support the NEON
instruction set extension which operate on four scalar values
at a time. Unfortunately, support for automatic vectorization in
LLVM was only added in October 2012 and is still maturing.