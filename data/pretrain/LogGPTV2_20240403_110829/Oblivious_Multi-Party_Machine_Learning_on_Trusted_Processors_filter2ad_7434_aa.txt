title:Oblivious Multi-Party Machine Learning on Trusted Processors
author:Olga Ohrimenko and
Felix Schuster and
C&apos;edric Fournet and
Aastha Mehta and
Sebastian Nowozin and
Kapil Vaswani and
Manuel Costa
Oblivious Multi-Party Machine Learning  
on Trusted Processors
Olga Ohrimenko, Felix Schuster, and Cédric Fournet, Microsoft Research;  
Aastha Mehta, Microsoft Research and Max Planck Institute for Software Systems (MPI-SWS); 
Sebastian Nowozin, Kapil Vaswani, and Manuel Costa, Microsoft Research
 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/ohrimenko
This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX Oblivious Multi-Party Machine Learning on Trusted Processors
Olga Ohrimenko, Felix Schuster, C´edric Fournet, Aastha Mehta∗, Sebastian Nowozin
Kapil Vaswani, Manuel Costa
Microsoft Research
Abstract
Privacy-preserving multi-party machine learning allows
multiple organizations to perform collaborative data an-
alytics while guaranteeing the privacy of their individ-
ual datasets. Using trusted SGX-processors for this task
yields high performance, but requires a careful selection,
adaptation, and implementation of machine-learning al-
gorithms to provably prevent the exploitation of any side
channels induced by data-dependent access patterns.
Introduction
We propose data-oblivious machine learning algo-
rithms for support vector machines, matrix factorization,
neural networks, decision trees, and k-means cluster-
ing. We show that our efficient implementation based
on Intel Skylake processors scales up to large, realis-
tic datasets, with overheads several orders of magnitude
lower than with previous approaches based on advanced
cryptographic multi-party computation schemes.
1
In many application domains, multiple parties would
benefit from pooling their private datasets, training pre-
cise machine-learning models on the aggregate data, and
sharing the benefits of using these models. For exam-
ple, multiple hospitals might share patient data to train
a model that helps in diagnosing a disease; having more
data allows the machine learning algorithm to produce a
better model, benefiting all the parties. As another ex-
ample, multiple companies often collect complementary
data about customers; sharing such data would allow ma-
chine learning algorithms to make joint predictions about
customers based on a set of features that would not other-
wise be available to any of the companies. This scenario
also applies to individuals. For example, some systems
learn an individual’s preferences to make accurate rec-
ommendations [9]; while users would like to keep their
data private, they want to reap the rewards of correlating
their preferences with those of other users.
Secure multi-party computation [17, 24, 44] and fully
homomorphic encryption [23] are powerful crypto-
graphic tools that can be used for privacy preserving ma-
chine learning. However, recent work [38, 47, 48, 54] re-
ports large runtime overheads, which limits their prac-
∗Work done while at Microsoft Research; affiliated with Max
Planck Institute for Software Systems (MPI-SWS), Germany.
tical adoption for compute-intensive analyses of large
datasets. We propose an alternative privacy-preserving
multi-party machine learning system based on trusted
SGX processors [45].
In our system, multiple parties
agree on a joint machine learning task to be executed on
their aggregate data, and on an SGX-enabled data cen-
ter to run the task. Although they do not trust one an-
other, they can each review the corresponding machine-
learning code, deploy the code into a processor-protected
memory region (called an enclave), upload their en-
crypted data just for this task, perform remote attestation,
securely upload their encryption keys into the enclave,
run the machine learning code, and finally download the
encrypted machine learning model. The model may also
be kept within the enclave for secure evaluation by all
the parties, subject to their agreed access control poli-
cies. Figure 1 provides an overview of our system.
While we rely on the processor to guarantee that only
the machine learning code inside the enclave has di-
rect access to the data, achieving privacy still requires
a careful selection, adaptation, and implementation of
machine-learning algorithms, in order to prevent the ex-
ploitation of any side channels induced by disk, net-
work, and memory access patterns, which may other-
wise leak a surprisingly large amount of data [39,49,70].
The robust property we want from these algorithms is
data-obliviousness: the sequence of memory references,
disk accesses, and network accesses that they perform
should not depend on secret data. We propose data-
oblivious machine learning algorithms for support vec-
tor machines (SVM), matrix factorization, neural net-
works, decision trees, and k-means clustering. Our data-
oblivious algorithms are based on careful elimination of
data-dependent accesses (SVM, neural networks and k-
means), novel algorithmic techniques (matrix factoriza-
tion) and deployment of platform specific hardware fea-
tures (decision trees). We provide strong, provable confi-
dentiality guarantees, similar to those achieved by purely
cryptographic solutions: we ensure that an attacker that
observes the sequence of I/O operations, including their
addresses and their encrypted contents, cannot distin-
guish between two datasets of the same size that yield
results of the same size.
We implemented and ran our algorithms on off-the-
shelf Intel Skylake processors, using several large ma-
USENIX Association  
25th USENIX Security Symposium  619
1
load into enclaves do not leak information through mem-
ory, disk, or network access patterns.
Adversary Model We assume the machine learning
computation runs in an SGX-enabled cloud data center
that provides a convenient ‘neutral ground’ to run the
computation on datasets provided by multiple parties.
The parties do not trust one another, and they are also
suspicious about the cloud provider. From the point of
view of each party (or any subset of parties), the adver-
sary models all the other parties and the cloud provider.
The adversary may control all the hardware in the
cloud data center, except the processor chips used in
the computation.
In particular, the adversary controls
the network cards, disks, and other chips in the moth-
erboards. She may record, replay, and modify network
packets or files. The adversary may also read or modify
data after it left the processor chip using physical prob-
ing, direct memory access (DMA), or similar techniques.
The adversary may also control all the software in the
data center, including the operating system and hypervi-
sor. For instance, the adversary may change the page ta-
bles so that any enclave memory access results in a page
fault. This active adversary is general enough to model
privileged malware running in the operating or hypervi-
sor layers, as well as malicious cloud administrators who
may try to access the data by logging into hosts and in-
specting disks and memory.
We assume that the adversary is unable to physically
open and manipulate the SGX processor chips that run
the machine learning computation. Denial of-service and
side-channel attacks based on power and timing analy-
sis are outside our scope. We consider the implementa-
tion of the machine learning algorithms to be benign: the
code will never intentionally try to leak secrets from en-
claves. We assume that all parties agree on the machine
learning code that gets access to their datasets, after in-
specting the code or using automated verification [60]
to ascertain its trustworthiness. We assume that all par-
ties get access to the output of the machine learning
algorithm, and focus on securing its implementation—
limiting the amount of information released by its correct
output [19] is outside the scope of this paper.
Security Guarantees We are interested in designing
algorithms with strong provable security guarantees. The
attacker described above should not gain any side infor-
mation about sensitive data inputs. More precisely, for
each machine learning algorithm, we specify public pa-
rameters that are allowed to be disclosed (such as the
input sizes and the number of iterations to perform) and
we treat all other inputs as private. We then say that an
algorithm is data-oblivious if an attacker that interacts
with it and observes its interaction with memory, disk
and network learns nothing except possibly those public
Figure 1: Sample privacy-preserving multi-party machine
learning system. Multiple hospitals encrypt patient datasets,
each with a different key. The hospitals deploy an agreed-upon
machine learning algorithm in an enclave in a cloud data center
and share their data keys with the enclave. The enclave pro-
cesses the aggregate datasets and outputs an encrypted machine
learning model.
chine learning datasets. Our results show that our ap-
proach scales to realistic datasets, with overheads that
are several orders of magnitude better than with previous
approaches based on advanced cryptographic multi-party
computation schemes. On the other hand, our approach
trusts the processor to protect the confidentiality of its in-
ternal state, whereas these cryptographic approaches do
not rely on this assumption.
2 Preliminaries
Intel SGX SGX [45] is a set of new x86 instructions
that applications can use to create protected memory re-
gions within their address space. These regions, called
enclaves, are isolated from any other code in the system,
including operating system and hypervisor. The proces-
sor monitors all memory accesses to the enclaves: only
code running in an enclave can access data in the en-
clave. When inside the physical processor package (in
the processor’s caches), the enclave memory is available
in plaintext, but it is encrypted and integrity protected
when written to system memory (RAM). External code
can only invoke code inside the enclave at statically-
defined entry points. SGX also supports attestation and
sealing [2]: code inside an enclave can get messages
signed using a per-processor private key along with a di-
gest of the enclave. This enables other entities to verify
that these messages originated from a genuine enclave
with a specific code and data configuration.
Using SGX instructions, applications can set up fine-
grained trusted execution environments even in (poten-
tially) hostile or compromised hosts, but application de-
velopers who write code that runs inside enclaves are
still responsible for maintaining confidentiality of secrets
managed by the enclave. In this paper, we focus on guar-
anteeing that the machine learning algorithms that we
620  25th USENIX Security Symposium 
USENIX Association
2
parameters. We define this interaction as a trace execu-
tion τ of I/O events, each recording an access type (read
or write), an address, and some contents, controlled by
the adversary for all read accesses. Crucially, this trace
leaks accurate information about access to code as well
as data; for example, a conditional jump within an en-
clave may reveal the condition value by leaking the next
code address [70].
We express our security properties using a simulation-
based technique: for each run of an algorithm given some
input that yields a trace τ, we show that there exists
a simulator program given only the public parameters
that simulates the interaction of the original algorithm
with the memory by producing a trace τ(cid:31) indistinguish-
able from τ. Intuitively, if the algorithm leaked any in-
formation depending on private data, then the simulator
(that does not have the data) would not be able to adapt
its behavior accordingly. Beside the public parameters,
the simulator may be given the result of the algorithm
(e.g., the machine learning model) in scenarios where
the result is revealed to the parties running the algo-
rithm. We rely on indistinguishability (rather than simple
trace equivalence τ(cid:31) = τ) to account for randomized algo-
rithms, and in particular for encryption. For instance, any
private contents in write events will be freshly encrypted
and thus (under some suitable semantic-encryption se-
curity assumption) will appear to be independently ran-
dom in both τ and τ(cid:31), rather than equal. More precisely,
we define indistinguishability as usual in cryptography,
using a game between a system that runs the algorithm
(or the simulator) and a computationally bounded adver-
sary that selects the inputs, interacts with the system, ob-
serves the trace, and attempts to guess whether it inter-
acts with the algorithm or the simulator. The algorithm
is data-oblivious when such adversaries guess correctly
with probability at most 1
3 Data-Oblivious Primitives
Our algorithms rely on a library of general-purpose
oblivious primitives. We describe them first and then
show how we use them in machine learning algorithms.
Oblivious assignments and comparisons These
primitives can be used to conditionally assign or com-
pare integer, floating point, or 256-bit vector variables.
They are implemented in x86-64 assembly, operating
solely on registers whose content is loaded from and
stored to memory using deterministic memory accesses.
The registers are private to the processor; their contents
are not accessible to code outside the enclave. As
such, evaluations that involve registers only are not
recorded in the trace τ, hence, any register-to-register
data manipulation is data-oblivious by default.
2 plus a negligible advantage.
We choose omove() and ogreater() as two repre-
sentative oblivious primitives. In conjunction, they en-
Non-oblivious
int max(int x, int y) {
  if (x > y) return x;
  else return y;
}
Oblivious
int max(int x, int y) {
  bool getX = ogreater(x, y);
  return omove(getX, x, y);
}
Figure 2: Left: C++ function determining the maximum of two
integers using a non-oblivious if-else statement; right: oblivi-
ous variant of the function using oblivious primitives.
able the straightforward, oblivious implementation of the
max() function, as shown in Figure 2. In the oblivious
version of max(), ogreater() evaluates the guard x >
y and omove() selects either x or y, depending on that
guard. In our library, similar to related work [53], both
primitives are implemented with conditional instructions
cmovz and setg. For example,
in simplified form,
omove() and ogreater() for 64-bit integers comprise
the following instructions:
ogreater()
mov     rcx, x
mov     rdx, y
cmp     rcx, rdx 
setg    al       
retn
omove()
rcx, cond
mov 
rdx, x
mov 
rax, y
mov 
test 
rcx, rcx
cmovz  rax, rdx
retn
On top of such primitives for native C++ types, our
library implements more complex primitives for user-
defined types. For example, most of our oblivious al-
gorithms rely on omoveEx(), an extended version of the
basic omove(), which can be used to conditionally as-
sign any type of variable; depending on the size of the
given type, omoveEx() iteratively uses the 64-bit integer
or 256-bit vector version of omove().
Oblivious array accesses Scanning entire arrays is a
commonly used technique to make data-dependent mem-
ory accesses oblivious.
In the simplest case, we use
omoveEx() iteratively to access each element when ac-
tually just a single element is to be loaded or stored.1
However, our adversary model implies that, for enclave
code, the attacker can only observe memory accesses at
cache-line granularity. Accordingly, the x least signif-
icant bits2 of memory addresses are not recorded in a
trace τ.
It is hence sufficient to scan arrays at cache-
line granularity rather than element or byte granularity.
We implement accordingly optimized array access prim-
itives that leverage AVX2 vector instructions [31].
In
particular, the vpgatherdd instruction can load each of
the eight 32-bit (4-byte) components of a 256-bit vector
1Dummy writes without actual effect are made to all but one el-
ement in case of a store. Modern processors treat such writes in the
same way as real writes and mark corresponding cache lines as dirty.
2The value of x depends on the actual hardware implementation; for
Skylake processors, where cache lines are 64 bytes long, x = 6.
USENIX Association  
25th USENIX Security Symposium  621