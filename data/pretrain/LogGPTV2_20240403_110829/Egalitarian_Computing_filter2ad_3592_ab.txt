α
C(α)
D(α)
1
2
1.5
1.5
1
3
4
1
4
20.2
1
5
344
1
6
4660
2.8
5.5
10.3
17
1
7
218
27
Table 1: Time and computation penalties for the ranking
tradeoff attack for Argon2d.
3.2 Proofs of work
A proof-of-work scheme is a challenge-response proto-
col, where one party (Prover) has to prove (maybe prob-
abilistically) that it has performed a certain amount of
computation following a request from another party (Ver-
iﬁer).
It typically relies on a computational problem
where a solution is assumed to have ﬁxed cost, such as
the preimage search in the Bitcoin protocol and other
cryptocurrencies. Other applications may include spam
protection, where a proof-of-work is a certiﬁcate that is
easy to produce for ordinary sender, but hard to generate
in large quantities given a botnet (or more sophisticated
platform).
The proof-of-work algorithm must have a few proper-
ties to be suitable for cryptocurrencies:
• It must be amortization-free, i.e. producing q out-
puts for B should be q times as expensive;
• The solution must be short enough and veriﬁed
quickly using little memory in order to prevent DoS
attacks on the veriﬁer.
4
• To avoid a clever prover getting advantage over the
others the advertised algorithm must be the most ef-
ﬁcient algorithm to date (optimization-freeness).
• The algorithm must be progress-free to prevent cen-
tralization: the mining process must be stochastic so
that the probability to ﬁnd a solution grows steadily
with time and is non-zero for small time periods.
• Parallelized implementations must be limited by the
memory bandwidth.
As demonstrated in [11], almost any hard problem can
be turned into a proof-of-work, even though it is difﬁcult
to fulﬁll all these properties. The well-known hard and
NP-complete problems are natural candidates, since the
best algorithms for them run in (sub)exponential time,
whereas the veriﬁcation is polynomial. The proof-of-
work scheme Equihash [11] is built on the generalized-
birthday, or k-XOR, problem, which looks for a set of
n-bit strings that XOR to zero. The best existing algo-
rithm is due to Wagner [34]. This problem is particularly
interesting, as the time-space tradeoff steepness can be
adjusted by changing k, which does not hold, e.g., in hard
knapsacks.
Drawbacks of existing PoW We brieﬂy discuss exist-
ing alternatives here. The ﬁrst PoW schemes by Dwork
and Naor [14] were just computational problems with
fast veriﬁcation such as the square root computation,
which do not require large memory explicitly. The sim-
plest scheme of this kind is Hashcash [7], where a par-
tial preimage to a cryptographic hash function is found
(the so called difﬁculty test). Large memory comes into
play in [13], where a random array is shared between the
prover and the veriﬁer thus allowing only large-memory
veriﬁers. This condition was relaxed in [15], where su-
perconcentrators [28] are used to generate the array, but
the veriﬁer must still hold large memory in the initial-
ization phase. Superconcentrators were later used in the
Proof-of-Space construction [16], which allows fast veri-
ﬁcation. However, the scheme [16] if combined with the
difﬁculty test is vulnerable to cheating (see Section 4.4
for more details) and thus can not be converted to a
progress-free PoW. We note that the superconcentrators
make both [15] and [16] very slow.
Ad-hoc but faster schemes started with scrypt [27],
but fast veriﬁcation is possible only with rather low
318  25th USENIX Security Symposium 
USENIX Association
amount of memory. Using more memory (say, using Ar-
gon2 [10]) with a difﬁculty test but verifying only a sub-
set of memory is prone to cheating as well (Section 4.4).
The scheme [11] is quite promising, but the reference
implementation reported is quite slow, as it takes about
30 seconds to get a proof that certiﬁes the memory allo-
cation of 500 MB. As a result, the algorithm is not truly
progress-free: the probability that the solution is found
within the ﬁrst few seconds is actually zero. It can be ar-
gued that this would stimulate centralization among the
miners. In addition, the memory parameter does not have
sufﬁcient granularity and there is no correlation between
the allocated memory and the minimal time needed to
ﬁnd the proof.
Finally, we mention schemes Momentum [21] and
Cuckoo cycle [32], which provide fast veriﬁcation due
to their combinatorial nature. They rely on the mem-
ory requirements for the collision search (Momentum)
or graph cycle ﬁnding (Cuckoo). However, Momen-
tum is vulnerable to a sublinear time-space tradeoff [11],
whereas the ﬁrst version of the Cuckoo scheme was re-
cently broken in [6].
We summarize the properties of the existing proof-of-
work constructions in Table 2. The AT cost is estimated
for the parameters that enable 1-second generation time
on a PC.
4 MTP: Proofs of work and time-lock puz-
zles based on memory-hard function
In this section we present a novel proof-of-work algo-
rithm MTP (for Merkle Tree Proof) with fast veriﬁca-
tion, which in particular solves the progress-free prob-
lem of [11]. Our approach is based on the memory-hard
function, and the concrete proposal involves Argon2.
Since fast memory-hard functions F such as Argon2
perform a lengthy chain of computations, but do not
solve any NP-like problem, it is not fast to verify that
Y is the output of F. Checking some speciﬁc (say, last)
blocks does not help, as explained in detail in the fur-
ther text. We thus have to design a scheme that lower
bounds the time-area product for the attacker, even if he
computes a slightly modiﬁed function.
Algorithm 1 MTP: Merkle-tree based Proof-of-Work.
Prover’s algorithm
Input: Challenge I, parameters L,d.
1. Compute F (I) and store its T blocks X[1], X[2],
. . ., X[T ] in the memory.
2. Compute the root Φ of the Merkle hash tree (see
Appendix A).
3. Select nonce N.
4. Compute Y0 = H(Φ,N) where G is a cryptographic
hash function.
5. For 1 ≤ j ≤ L:
i j = Yj−1
(mod T );
Yj = H(Yj−1,X[i j]).
6. If YL has d trailing zeros, then (Φ,N, Z ) is the
proof-of-work, where Z is the opening of 2L
blocks {X[i j − 1],X[φ (i j)]}. Otherwise go to Step
3.
Output: Proof (Φ,N, Z ).
The veriﬁer, equipped with F and H, runs Algo-
rithm 2.
Algorithm 2 MTP: Veriﬁer’s algorithm
Input: Proof (Φ,N, Z ), parameters L,d.
1. Compute Y0 = H(Φ,N).
2. Verify all block openings using Φ.
3. Compute from Z for 1 ≤ j ≤ L:
X[i j] = F(X[i j − 1],X[φ (i j)]);
Yj = G(Yj−1,X[i j]).
4. Check whether YL has t trailing zeros.
Output: Yes/No.
4.1 Description of MTP
Consider a memory-hard function F that satisﬁes Equa-
tion (2) (for instance, Argon2) with a single pass over the
memory producing T blocks and a cryptographic hash
function H (possibly used in F ). We propose the fol-
lowing non-interactive protocol for the Prover (Figure 1)
in Algorithm 1, where L and d are security parameters.
The average number of calls to F is T + 2dL.
4.2 Cheating strategies
Let the computation-space tradeoff for H and the de-
fault memory value T be given by functions C(α) and
D(α) (Section 2).
Memory savings Suppose that a cheating prover wants
to reduce the AT cost by using αT memory for some
α < 1. First, he computes F (I) and Φ, making C(α)T
USENIX Association  
25th USENIX Security Symposium  319
5
Scheme
Dwork-Naor I [14]
Dwork-Naor II [13]
Dwork-Naor III [15]
Hashcash/Bitcoin [7]
Pr.-of-Space [16]+Diff.test
Litecoin
Argon2-1GB + Diff.test
Momentum [21]
Cuckoo cycle [32]
Equihash [11]
MTP
Medium
Low
High
Low
High
AT cost
Speed
Veriﬁcation
Fast M/less
Yes
Yes
Yes
Yes
Yes
Yes
No
Yes
Medium [6] Medium Yes
Medium Yes
Yes
Yes
No
No
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
High
Low
Low
High
Low
High
High
High
High
High
Medium
High
Medium
High
Tradeoff
Paral-sm Progress
Memoryless
Memoryless
Exponential
Memoryless
Exponential
Linear
Exponential
Attack [11, 33]
Linear [6]
Exponential
Exponential
Yes
Constr.
Constr.
Yes
No
No
No
Yes
Yes
Constr.
Constr.
-free
Yes
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes
Table 2: Review of existing proofs of work. Litecoin utilizes scrypt with 128KB of RAM followed by the difﬁculty
test). M/less – memoryless; constr. – constrained.
calls to F. Then for each N he has to get or recompute
L blocks using only αT stored blocks. The complexity
of this step is equal to the complexity of recomputing
random L blocks during the ﬁrst computation of F . A
random block is recomputed by a tree of average size
C(α) and depth D(α). Therefore, to compute the proof-
of-work, a memory-saving prover has to make C(α)(T +
2dL) calls to F, so his amount of work grows by C(α).
Block modiﬁcation The second cheating strategy is to
compute a different function F (cid:31) (cid:30)= F . More precisely,
the cheater produces some blocks X[i(cid:31)] (which we call
inconsistent as in [16]) not as speciﬁed by Equation (2)
(e.g. by simply computing X[i(cid:31)] = H(i(cid:31))). In contrast to
the veriﬁable computation approach, our protocol allows
a certain number of inconsistent blocks. Suppose that the
number of inconsistent blocks is εT , then the chance that
no inconsistent block is detected by L opened blocks is
γ = (1− ε)L.
Therefore, the probability for a proof-of-work with εM
inconsistent blocks to pass the opening test is γ. In other
words, the cheater’s time is increased by the factor 1/γ.
We note that it does not make sense to alter the blocks
after the Merkle tree computation, as any modiﬁed block
would fail the opening test.
Overall cheating penalties Let us accumulate the two
cheating strategies into one. Suppose that a cheater
stores αT blocks and additionally allows εT inconsis-
tent blocks. Then he makes at least
C(α + ε)(T + 2dL)
γ
(3)
calls to F. The concrete values are determined by the
penalty function C(), which depends on F .
4.3 Parallelism
Both honest prover and cheater can parallelize the com-
putation for 2t different nonces. However, the latency of
cheater’s computation will be higher, since each block
generates a recomputation tree of average depth D(α +
ε).
4.4 Why simpler approach does not work: