2000
2500
(a) Service A with no competing ﬂow.
(b) Service A with one competing ﬂow.
Figure 7: (Service A) Throughput at HTTP layer with and without a competing ﬂow.
5.1
Initial Condition: No Competing Flow
In the absence of a competing ﬂow (ﬁrst 400 seconds),
the Service A client correctly chooses the highest playback
rate. Because the available network bandwidth (5Mb/s) is
much higher than the playback rate (1750kb/s), the client
busily ﬁlls up its playback buﬀer and the bottleneck link
is kept fully occupied. Eventually the playback buﬀer ﬁlls
(after 185 seconds) and the client pauses to let it drain a
little before issuing new requests. Figure 6(a) shows how the
TCP throughput varies before and after the playback buﬀer
ﬁlls up. After the buﬀer is full, the client enters a periodic
ON-OFF sequence. As we will see shortly, the ON-OFF
sequence is a part of the problem (but only one part). Before
the buﬀer ﬁlls, the client requests a new 4-second segment
of video every 1.5 seconds on average (because it is ﬁlling
the buﬀer). Figure 6(b) conﬁrms that after the buﬀer is full,
the client requests a new 4-second segment every 4 seconds,
on average. The problem is that during the 4-second OFF
period, the TCP congestion window (cwnd ) times out —
due to inactivity longer than 200ms — and resets cwnd to
its initial value of 10 packets [5, 6]. Even though the client is
using an existing persistent TCP connection, the cwnd needs
to ramp up from slow start for each new segment download.
It is natural to ask if the repeated dropping back to slow-
start reduces the client’s video throughput, causing it to
switch to a lower rate. With no competing ﬂow, it appears
the answer is ‘no’. We verify this by measuring the video
throughput for many requests. We set the bottleneck link
rate to 2.5Mb/s, use traces collected from actual sessions
to replay the requests over a persistent connection to the
same server, and pause the requests at the same interval as
the pauses in the trace. Figure 7(a) shows the CDF of the
client’s video throughput for requests corresponding to vari-
ous playback rates. The video throughput is pretty accurate.
Except for some minor variation, the video throughput ac-
curately reﬂects the available bandwidth, and explains why
the client picks the correct rate.
230Competing Flow
)
t
n
e
m
g
e
s
(
w
o
d
n
W
n
o
i
Competing Flow
Emulated Video Flow
18
16
14
12
10
18
16
14
12
10
8
6
4
Emulated Video Flow
)
t
n
e
m
g
e
s
(
i
w
o
d
n
W
n
o
i
t
s
e
g
n
o
C
P
C
T
2
0.5
1.0
1.5
2.0
2.5
3.0
Time(s)
(a) A 235kbps Segment.
i
t
s
e
g
n
o
C
P
C
T
8
6
4
2
7
8
9
10
11
12
Time(s)
(b) Five contiguous 235kbps segments concatenated into
one.
Figure 8: (Service A) The evolution of cwnd for diﬀerent segment sizes.
)
s
/
3000
b
k
(
e
a
R
t
t
s
e
u
q
e
R
h
/
t
i
d
w
d
n
a
B
e
b
a
l
2500
2000
1500
1000
500
Available Bandwidth
Video Playout Rate
l
i
a
v
A
0
0
500 1000 1500 2000 2500 3000 3500 4000
Time(s)
Figure 9: (Service A) The client picks a video rate
depending on the available bandwidth. The hori-
zontal gray lines are the available rates.
5.2 The Trigger: With a Competing Flow
Things go wrong when the competing ﬂows starts (after
400 seconds). Figure 7(b) shows the client’s video through-
put are mostly too low when there is a competing ﬂow.4 If
we look at the progression of cwnd for the video ﬂow after it
resumes from a pause, we can tell how the server opens up
the window diﬀerently when there is a competing ﬂow. Be-
cause we don’t control the server (it belongs to the CDN) we
instead use our local proxy to serve both the video traﬃc and
the competing ﬂow, and use the tcp_probe kernel module
4In Figure 7(b), the bottleneck bandwidth is set to 5Mb/s so
that the available fair-share of bandwidth (2.5Mb/s) is the
same as in Figure 7(a). Note that some segment downloads
are able to get more than its fair share; in these cases, the
competing ﬂow experiences losses and has not ramped up to
its fair share yet. This is the reason why some of the CDF
curves does not end with 100% at 2.5Mb/s in Figure 7(b).
to log the cwnd values. The video traﬃc here is generated
by requesting a 235kbps video segment. Figure 8(a) shows
how cwnd evolves, starting from the initial value of 10 at 1.5
seconds, then repeatedly being beaten down by the competing
wget ﬂow. The competing wget ﬂow has already ﬁlled the
buﬀer during the OFF period, and so the video ﬂow sees
very high packet loss. Worse still, the segment is ﬁnished
before cwnd climbs up again, and we re-enter the OFF pe-
riod. The process will repeat for every ON-OFF period, and
the throughput is held artiﬁcially low.
For comparison, and to understand the problem better,
Figure 8(b) shows the result of the same experiment with a
segment size ﬁve times larger. With a larger segment size,
the cwnd has longer to climb up from the initial value; and
has a much greater likelihood of reaching the correct steady
state value.
Now that we know the video throughput tends to be low
(because of TCP), we would like to better understand how
the client reacts to the low throughputs. We can track the
client’s behavior as we steadily reduce the available band-
width, as shown in Figure 9. We start with a bottleneck link
rate of 5Mb/s (and no competing ﬂow), drop it to 2.5Mb/s
(to mimic a competing ﬂow), and then keep dropping it
by 100kb/s every 3 minutes. The dashed line shows the
available bandwidth, while the solid line shows the video
rate picked by the client. The client chooses the video rate
conservatively; when available bandwidth drops from from
5Mb/s to 2.5Mb/s, the video rate goes down to 1400kb/s,
and so on.
We can now put the two pieces together. Consider a client
streaming at a playback rate of 1750kb/s, the median video
throughput it perceives is 1787kb/s as shown in Figure 7(b).
According to Figure 9, with a video throughput of 1787kb/s,
the client reduces its playback rate to 1050kb/s. Thus, 50%
of the time the playback rate will go down to 1050kb/s once
the competing ﬂow starts.
It is interesting to observe that the Service A client is
behaving quite rationally given the throughput it perceives.
The problem is that because Service A observes the through-
put above TCP, it is not aware that TCP itself is having
231232 
)
%
(
F
D
C
100
80
60
40
20
0
0
Video Rate 650kb/s
Video Rate 1000kb/s
Video Rate 1500kb/s
Video Rate 2000kb/s
Video Rate 2500kb/s
Video Rate 3200kb/s
500
1000
1500
2000
2500
Throughput (kb/s)
)
%
(
F
D
C
100
80
60
40
20
0
0
Video Rate 650kb/s
Video Rate 1000kb/s
Video Rate 1500kb/s
Video Rate 2000kb/s
Video Rate 2500kb/s
Video Rate 3200kb/s
500
1000
1500
Throughput (kbps)
2000
2500
(a) Service B with no competing ﬂow.
(b) Service B with one competing ﬂow.
Figure 16: (Service B) The TCP throughput changes in the presence of a competing ﬂow.
)
%
(
n
o
i
t
c
a
r
F
1.0
0.8
0.6
0.4
0.2
0.0
102
103
104
OFF Duration(ms)
105
1.0
0.8
0.6
0.4
0.2
)
%
(
n
o
i
t
c
a
r
F
0.0
0
2500
Download size between successive zero window ads (KBytes)
2000
1000
1500
500
Figure 14: (Service B) Almost all the OFF periods in
a single video session are greater than RTO (200ms).
Figure 15: (Service B) When the video stream is
receiver-limited, the client does not request many
bytes during an ON period.
video throughput is higher — hence the Service A client
picks a higher rate (1050kb/s).
For comparison, we asked 10 volunteers to rerun this ex-
periment with Service A in their home network connected
to diﬀerent ISPs, such as AT&T DSL, Comcast, Verizon
and university residences. Even though there was suﬃcient
available bandwidth for the highest video rate in the pres-
ence of a competing ﬂow, seven people reported a rate of
only 235kb/s-560kb/s.
5.5 Service B
Service B also exhibits ON-OFF behavior, but at the TCP
level and not the HTTP level, i.e., the pause could happen
while downloading a video segment. When its video play-
back buﬀer is full, the client stops taking data from the
TCP socket buﬀer. Eventually, the TCP socket buﬀer also
ﬁlls and triggers TCP ﬂow control to pause the server by
sending a zero window advertisement. In Figure 4(b), each
zero window advertisement is marked by a hexagon. The
client starts issuing zero window advertisements at around
100s and continues to do so until a few seconds after the
competing ﬂow starts. Figure 14 shows the CDF of the du-
ration of the OFF periods. Almost all the pauses are longer
than 200ms, so cwnd is reset to its initial value. Thus, Ser-
vice B eﬀectively exhibits an ON-OFF behavior similar to
that of Service A.
Worse still, during an ON period, Service B does not re-
quest many bytes; Figure 15 shows that over half of the time,
it reads only 800kbytes, which is not enough for the cwnd
to climb up to its steady state before the next OFF period.
Figure 4(b) and Figure 16(b) show the result, that the TCP
throughput is only around 1Mbps to 1.5Mbps, causing Ser-
vice B to pick a video rate of 1000kb/s, or even 650kb/s.
As we saw earlier, when competing with another ﬂow, the
smaller the request, the higher the likelihood of perceiving
2332345000
4000
3000
)
s
/
b
k
(
e
t
Buffer Status
Video Throughput
250
200
150
100
50
)
d
n
o
c
e
S
(
r
e
f
f
u
B
n
i
s
t
n
e
m
g
e
S
o
e
d
V
i
5000
4000
3000
)
s
/
b
k
(
e
t
a
R
o
e
d
V
i
2000
1000
0
0
Buffer Status
Video Throughput
250
200
150
100
50
Video Rate
200
400
600
800
Time (s)
0
1000
)
d
n
o
c
e
S
(
r
e
f
f
u
B
n
i
s
t
n
e