title:Exploiting monotonicity and symmetry for efficient simulation of highly
dependable systems
author:Hoang Hai Nguyen and
Kartik Palani and
David M. Nicol
0
4
0
0
0
.
2
2
0
2
.
5
0
4
3
5
N
S
D
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
1
-
3
9
6
1
-
4
5
6
6
-
1
-
8
7
9
|
)
N
S
D
(
s
k
r
o
w
t
e
N
d
n
a
s
m
e
t
s
y
S
e
l
b
a
d
n
e
p
e
D
n
o
e
c
n
e
r
e
f
n
o
C
l
a
n
o
i
t
a
n
r
e
t
n
I
P
I
F
I
/
E
E
E
I
l
a
u
n
n
A
d
n
2
5
2
2
0
2
2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Exploiting monotonicity and symmetry for efﬁcient
simulation of highly dependable systems
Information Trust Institute, University of Illinois at Urbana-Champaign
Hoang Hai Nguyen, Kartik Palani, David M. Nicol
Email: {hnguye11, palani2, dmnicol}@illinois.edu
Abstract—Evaluation of highly dependable systems requires
estimating the probability of a signiﬁcant rare event under which
the system fails to meet the requirement. To improve the esti-
mation accuracy, advanced Monte Carlo simulation techniques
such as importance sampling (IS) are commonly used. However,
IS is known to misbehave under high dimension. As a result, the
IS estimator can have a large relative error and underestimate
the rare event probability. In this paper, we propose a novel
IS method based on the idea of maximum weight minimization
(MWM). Our method works by ﬁnding the sampling distribution
that minimizes the maximum weight of a rare event sample. To
alleviate the curse of dimensionality, we develop further heuristics
based on two problem-speciﬁc structures, namely, monotonicity
and symmetry. Using extensive examples from network reliability,
stochastic ﬂow analysis, cyber-security risk assessment, and fault
tree analysis, we evaluate the performance of MWM, demonstrate
its accuracy and scalability, and highlight applications where it
outperforms state-of-the-art techniques.
Index Terms—Model-based evaluation, importance sampling,
rare event simulation, monotonicity, symmetry.
I. INTRODUCTION
Evaluation of engineered systems often requires estimating
the probability
ρ = P(f (X) ≤ γ)
(1)
where X = (X1, X2, . . . , Xm) is the system state vector
distributed according to some distribution P, f measures the
performance of the system such as the reliability, safety, or
security, and γ is a user-selected threshold, which is the
minimum level of performance the system must maintain in
order to be considered functional. This formulation is found
in various domains such as fault tree analysis [1], network
reliability analysis [2], stochastic ﬂow analysis [3], cyber-
security risk assessment [4], risk assessment of power system
operation [5], safety evaluation of self-driving vehicles [6] [7],
among many others. For example, in fault tree analysis [1], f
computes the time-to-failure (TTF) metric of the system as a
function of X, the reliability of its basic components. If γ is
the targeted mission time, then (1) computes the probability
that the system fails before the end of the mission. In practice,
we consider the system to meet the requirement if ρ is smaller
than a desired level.
Oftentimes, the system under study is large [8] or complex
[7], making it difﬁcult to obtain a closed-form or numerical
solution to (1). In this situation, one may need to resort to
simulation as the alternative to analytical approaches [9]. In
particular, Monte Carlo (MC) methods are a popular class of
simulation approaches that rely on repeated random sampling
to estimate ρ (4). However, as the system becomes more
reliable and as γ decreases, ρ becomes more difﬁcult
to
estimate. Using the fault tree example, if each simulation run
−6,
takes one millisecond to ﬁnish and ρ is in the order of 10
then it would take over 16 minutes on average to produce
a sample under which the system fails before the mission.
This translates to over a day of simulation to estimate ρ using
100 failure samples. While parallelization helps alleviate the
situation, a crude MC approach requires a 100x increase in
the number of samples in order to improve the accuracy of
the estimate by a digit after decimal. The computational cost
can be prohibitive when analyzing highly dependable systems,
−9 [10].
where the acceptable level for ρ can be as low as 10
To accelerate the occurrence of rare event samples—where
the metric of interest falls below a given threshold—a com-
mon approach is to sample X under a different distribution
called the proposal distribution P(cid:2) [11] and later multiply
the simulation output by the sample weight to remove the
bias. This is the basic idea behind importance sampling (IS)
for rare event simulation [12], [10], [2]. Moreover,
there
exists an optimal proposal distribution P∗, under which the
IS estimator achieves a zero variance. However, while P∗
can be derived in theory, it cannot be computed in practice.
Nevertheless, knowledge about the system under study [12],
[13] or about some properties of P∗ [14], [2] can give us some
idea about how it can be approximated. For the latter case, the
cross-entropy (CE) method [14] ﬁnds P(cid:2) that minimizes the
Kullback-Leibler distance from P∗. In [2] and [4], the authors
constructed P(cid:2) by emulating the functional form of P∗.
Following this line of work, our research seeks to ﬁnd other
properties of P∗ that can be used to identify “good” proposal
distributions. Our heuristic is based on plausible reasoning
[15], i.e., if P∗ has property X , then a proposal distribution
P(cid:2) that maintains X is more likely to well approximate P∗. In
a relaxed way, if P(cid:2) has property X (cid:2) that is close to X , then
P(cid:2) is also more likely to well approximate P∗. This leads to
the formulation of the maximum weight minimization (MWM)
problem, since P∗ is the distribution that minimizes the
maximum weight of a rare event sample. By minimizing the
maximum weight, the effect of weight degeneracy [13], [16],
[11] is lessened. Weight degeneracy—commonly attributed
to the curse of dimensionality [13], [17]—happens when the
sample weights are unevenly inﬂated, causing a few samples
to have disproportionately large weights. As a result, the IS
estimator can behave quite erratically. For example, given a
ﬁxed number of samples, the estimator frequently underesti-
mates ρ [18] but in rare occasions signiﬁcantly overestimate it.
This issue limits the applicability of IS to a high-dimensional
setting such as rare event simulation of large-scale systems.
To solve MWM, we need to ﬁnd a representative approx-
imation of the rare event set Xγ deﬁned as Xγ = {x ∈
X : f (x) ≤ γ}, where X is the support of X (the notion of
representativeness is made precise in Section III-B). However,
the cardinality of the approximation set grows quickly as the
problem dimension increases, making MWM unscalable in its
basic form. To tackle this issue, we rely on two properties
2158-3927/22/$31.00 ©2022 IEEE
DOI 10.1109/DSN53405.2022.00040
307
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
that are commonly found in engineered systems, namely,
monotonicity and symmetry (some examples are given in
Sections IV and V). In the literature, these properties have
been exploited to improve simulation efﬁciency, e.g., Petri nets
simulation [8] and rare event simulation of black-box models
[18]. In this work, we use them to derive several properties of
P∗, which lead to the development of the order-preserving
heuristic and the structure-preserving heuristic. The order-
preserving heuristic reduces the size of the search space to ﬁnd
P(cid:2) and allows us to replace a rare event sample with a more
representative one. On the other hand, the structure-preserving
heuristic reduces the problem dimension and allows us to
discard samples that are different but share the same weights.
These two heuristics signiﬁcantly improve the efﬁciency and
scalability of MWM. In contrast, adaptive IS methods such as
CE [14], [19] and population Monte Carlo [20], [11] do not
assume any property of f. As a result, they have a wider range
of applications but may require a large computational budget
in order to reach convergence.
In summary, our main contribution in this paper is MWM,
a novel IS method and two accompanying heuristics that can
be applied to a variety of rare event simulation applications,
from network reliability [21], [2], stochastic ﬂow analysis
[22], cyber-security risk assessment [4], to dynamic fault tree
analysis [23]. Simulation results show that MWM is fast to
compute and produces compatible, sometimes better estimates
than state-of-the-art methods, even for events of extremely low
−9). By taking advantage of
probabilities (e.g., less than 10
two commonly-occurred system structures, MWM can handle
problems of over 100 dimensions. While MWM sacriﬁces
generality for performance, the monotonicity and symmetry
assumptions can be restrictive when applying to real systems.
Nevertheless, these assumptions can be relaxed, as we will
discuss in Sec. III-B. For example, the equivalence relation-
ship under the symmetry assumption generalizes to situations
under which model components are not identical but instead
functionally similar. The relaxation allows for more aggressive
dimensionality reduction without compromising the estimation
accuracy, as our simulation results suggested.
The rest of the paper is organized as follows. Section II
provides the background and basic mathematical language,
Section III introduces the MWM method, Sections IV and V
discuss monotonicity and symmetry in the objective function
and how these properties can be exploited to reﬁne MWM,
Section VI generalizes the results to other distributions, Sec-
tion VII presents simulation results, Section VIII discusses
concerns and limitations of MWM and threats to the validity of
the results, Section IX provides the related work, and Section
X concludes.
II. BACKGROUND
A. Reliability models
Although the technique developed in this paper works for
a wide range of applications, the original idea arised in the
context of rare event simulation of network reliability models.
In this subsection, we revisit the classical network reliability
model [21] and introduce several extensions that will be used
for the evaluation in Sec. VII. The details in this subsection
are not required for the development of the MWM method in
Sec. III and the heuristics in Sections IV and V.
1) Two-terminal unreliability: Let G = (V, E, p) be a