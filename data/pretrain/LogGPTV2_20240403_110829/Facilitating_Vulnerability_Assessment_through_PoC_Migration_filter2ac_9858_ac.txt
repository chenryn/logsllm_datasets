each round of fuzzing, we retrieve a seed from the first position of
the input queue and collect the execution trace under this seed on
the target version. By identifying the execution detours between the
collected trace and the reference trace and associating these detours
with the critical variables on the target version, we first locate the
critical input bytes that affect the runtime values of these critical
variables; then mutate these input bytes of the seed to generate a
set of new testcases; and finally insert every testcase into the queue
Figure 4: An example of reasoning unintended calls. Note
that the arrows indicate the control flows of the two traces.
according to the similarity between the execution trace (generated
by it on the target version) and the reference trace.
During the fuzzing process, if a testcase triggers a crash, we
use the crash triage module (see §4.3) to test whether the crash is
triggered by the target vulnerability. If so, the fuzzing loop ends and
reports this testcase as the desired PoC input for the target version.
With a PoC, the target version is verified to be affected by the target
vulnerability. Otherwise, the fuzzing process continuously proceeds
until a time budget is reached.
The details about the fuzzing loop are described below.
• Critical Input Bytes Locating. We first label all critical variables
that are identified from execution detours as taint sources. Then,
we use dynamic taint analysis to locate the critical input bytes of
the seed that affect the runtime values of these critical variables.
Technically, we use IR-level instrumentation to implement the
variable-level taint tracking (detailed in Appendix A). Although
we can pinpoint the input bytes that directly affect the runtime
values of the critical variables, we may miss the input bytes
that indirectly affect these critical variables, e.g., through control
flows. When no input bytes are located via data flows, we also
consider the condition variables (i.e., variables that determine
which branch to take on conditional jumps) on the execution
flow as taint sources, and locate the input bytes that affect these
condition variables. All the located input bytes are considered
in the mutation stage. We note that this design may introduce
undertaint/overtaint issues and we will discuss these issues in §6.
enum TIFFReadDirEntryErr TIFFReadDirEntryArrayWithLimit(tif, direntry, count, desttypesize, value, maxcount){    ......    if ((uint64)(2147483647/typesize)tdir_count)        return(TIFFReadDirEntryErrSizesan);    if ((uint64)(2147483647/desttypesize)tdir_count)        return(TIFFReadDirEntryErrSizesan);    ......    data=_TIFFCheckMalloc(...)}01020304050607080910(b) Intra-function trace of reference PoC on libtiff v4.0.8 (target version)int read_samples_pcm(musicin, sample_buffer, samples_to_read){    switch(global.pcmbitwidth){        case 32:        case 24:        case 16:            ......        case 8:            ......        default:            if (global_ui_config.silent < 10) {                error_printf("...");            }            return -1;                }    ......}01020304050607080910111213141516(a) Intra-function trace of reference PoC on lame v3.99.5 (reference version)(b) Intra-function trace of reference PoC on lame v3.98.4 (target version)int read_samples_pcm(musicin, sample_buffer, samples_to_read){    switch(global.pcmbitwidth){        case 32:        case 24:        case 16:            ......        case 8:            ......        default:            if (silent < 10) {                error_printf("...");            }            exit(1);                }    ......}01020304050607080910111213141516int read_samples_pcm(musicin, sample_buffer, samples_to_read){    switch(global.pcmbitwidth){        case 32:        case 24:        case 16:            ......        case 8:            ......        default:            if (global_ui_config.silent < 10) {                error_printf("...");            }            return -1;                }    ......}01020304050607080910111213141516(a) Intra-function trace of reference PoC on lame v3.99.5 (reference version)int read_samples_pcm(musicin, sample_buffer, samples_to_read){    switch(global.pcmbitwidth){        case 32:        case 24:        case 16:            ......        case 8:            ......        default:            if (silent < 10) {                error_printf("...");            }            exit(1);                }    ......}01020304050607080910111213141516(b) Intra-function trace of reference PoC on lame v3.98.4 (target version)Session 12B: Analyzing Crashes and Incidents CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3306• Seed Mutation. For the located critical input bytes of a seed,
we apply existing mutation strategies of AFL to mutate their
values. In every round of fuzzing, we only mutate a seed, and the
time budget for mutating this seed is positively correlated to the
number of identified input bytes on this seed.
• Seed Prioritization. Within each round of fuzzing, many new
seeds are generated after seed mutation. These new seeds are
prioritized to insert into the input queue, and the seed with
the highest priority will be selected for mutation in the next
round of fuzzing. To label a priority for each seed, we use the
similarity score between the execution trace collected under
this seed on the target version 𝑇𝑡𝑎𝑟𝑔𝑒𝑡(𝑉𝑡𝑎𝑟𝑔𝑒𝑡, 𝐸𝑡𝑎𝑟𝑔𝑒𝑡) and the
reference trace 𝑇𝑟𝑒 𝑓 (𝑉𝑟𝑒 𝑓 , 𝐸𝑟𝑒 𝑓 ). Specifically, according to the
cross-version trace alignment, we first obtain the set of vertices
that has matched vertices in the
𝑀𝐴𝑇𝐶𝐻𝐸𝐷𝑟𝑒 𝑓
𝑉𝑡𝑎𝑟𝑔𝑒𝑡, then collect the set of vertices 𝑁𝑂𝑇 _𝐶𝑂𝑅𝑅𝐸𝐶𝑇 𝐴𝐵𝐿𝐸𝑟𝑒 𝑓
in the 𝑉𝑟𝑒 𝑓 who are identified as not-correctable missed calls in
execution detours reasoning (see §4.1). We iteratively expand
𝑁𝑂𝑇 _𝐶𝑂𝑅𝑅𝐸𝐶𝑇 𝐴𝐵𝐿𝐸𝑟𝑒 𝑓 to include all its child vertices. Based
on the alignment result, the similarity score between the 𝑇𝑡𝑎𝑟𝑔𝑒𝑡
and the 𝑇𝑟𝑒 𝑓 is calculated as follows:
in the 𝑉𝑟𝑒 𝑓
(cid:12)(cid:12)𝑀𝐴𝑇𝐶𝐻𝐸𝐷𝑟𝑒 𝑓
(cid:12)(cid:12) −(cid:12)(cid:12)𝑁𝑂𝑇 _𝐶𝑂𝑅𝑅𝐸𝐶𝑇 𝐴𝐵𝐿𝐸𝑟𝑒 𝑓
(cid:12)(cid:12)
𝑠𝑖𝑚(𝑇𝑟𝑒 𝑓 ,𝑇𝑡𝑎𝑟𝑔𝑒𝑡) =
(cid:12)(cid:12)𝑉𝑟𝑒 𝑓
(cid:12)(cid:12)
(1)
4.3 Crash Triage
When a crash occurs, we need to determine whether the crash
is caused by the target vulnerability or not. Though the crash
triage mechanism is widely used by fuzzers, it is mainly used for
unique crash identification. Existing fuzzers commonly use two
heuristics to identify unique crashes: unique stack traces (e.g., used
by SYMFUZZ [11]) and unique coverage profile (e.g., used by AFL [1]).
However, these heuristics cannot be applied to comparing two
crashes that are collected on two different software versions, due
to cross-version code changes.
As such, we introduce a tailored triage mechanism for our
problem. In our problem, crash triage is used to verify whether a
target version is affected by a specified vulnerability. To gain a low
false positive rate, we adopt a conservative design in performing
such affection assessment. In particular, we set 3 conditions to
meet in determining whether two crashes (which are collected
on two software versions) are caused by the same vulnerability:
❶ the similarity score between their execution traces (Equation 1)
exceeds a pre-defined threshold; ❷ their buggy functions are aligned
across versions; and ❸ the bugs they triggered belong to the same
category. As we will show in our evaluation (see §5), the above
triaging criteria help to keep a low false positive rate in performing
vulnerability affection assessment.
5 EVALUATION
This section evaluates the effectiveness and the efficiency of
VulScope in migrating a reference PoC to target vulnerable
versions of the same software. In particular, it first introduces the
experiment setup as well as the data set used for evaluation; then
discusses the experiment design; and finally reports the results.
Prototype. We implement a prototype of VulScope, which con-
tains about 4.5K lines of C++ code and 2K lines of Python code
(counted by cloc). In our prototype, the static code analysis
is implemented on LLVM 7.0.0 and the fuzzing loop is built
upon AFL 2.56b [1]. More implementation details are presented
in Appendix A.
5.1 Data Set
Methodology. In recent studies [34], researchers have discovered
that the vulnerability reports’ quality could vary significantly.
In order to evaluate our proposed technique, we select real-
world vulnerabilities and their reports by following the suggestion
provided by one of the studies [34]. To be specific, when selecting
our data set, we ensure a report could provide us with the following
four information.
• Availability of Proof-of-Concept input. We choose only those
vulnerability reports containing an external reference link to
an available public reference PoC input.
• Description of vulnerable software versions. We consider those
vulnerability reports that specify the versions of the software
vulnerability to that reported security loophole. In this way, we
can further assess how well the reports align with the ground
truth and the reports of our tool.
• Guidance of vulnerable software configuration. For some vul-
nerabilities, they can be triggered only when the target soft-
ware is appropriately configured. For example, the target soft-
ware libtiff is vulnerable to the vulnerability associated
with CVE-2018-18557 only if we compile it with the option
“–enable-jbig”. As such, if a report fails to specify the config-
uration detail of the vulnerable software, we discard the report
accordingly.
• Guidance of triggering method. For some vulnerabilities, they
can be triggered only when a specific running parameter is
appropriately specified. For example, the vulnerability associated
with CVE-2015-9101 could be triggered only when “-f -V 9” is
specified. As a result, we select the report with such details are
present.
Data Summary. Following the selection criteria, we randomly
selected 30 real-world CVE reports from the National Vulnerability
Database (NVD) [4]. These 30 CVE reports cover 6 broadly adopted
software in the userspace, covering 6 types of vulnerabilities:
heap OOB, stack OOB, divide-by-zero, segmentation fault, integer
overflow, and null pointer dereference. To choose the versions of
the software for evaluating our tool, we took those versions that
never apply the corresponding patches1 and treated the PoC input
associated with the CVE report as our reference PoC input. Table 2
and Appendix-Table 6 summarize the CVEs of our selection, the
reference version of the software, the number of target versions we
choose for our evaluation, and the security severity of each CVE.
1Note that after a vulnerability is identified on a particular version of the software,
a software developer usually develops a patch and applies it to a couple of active
vulnerable versions. For non-vulnerable versions and inactive-maintained versions,
the patches are not applied.
Session 12B: Analyzing Crashes and Incidents CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3307Table 2: Summary of selected vulnerabilities and their corresponding CVE IDs.
Software
CVE
Reference
Version
Target
Versions1
Replay Reference PoC on Target Versions
Affected Versions
Crash
Target
Crash
Not Target
Crash
Not
Crash
All Need PoC Migration2
5
0
0
0
0
1
0
0
6
0
0
0
0
0
0
4
1
4
5
12
7
8
0
3
18
16
4
0
0
0
94
zziplib
audiofile
tcpdump
lame
libtiff
jasper
CVE-2018-6381
CVE-2017-5976
CVE-2017-5975
CVE-2017-5974
CVE-2018-17095
CVE-2017-6836
CVE-2017-6834
CVE-2017-6832
CVE-2017-6831
CVE-2017-6835
CVE-2017-5485
CVE-2017-13690
CVE-2017-5486
CVE-2017-16808
CVE-2017-15046
CVE-2017-15045
CVE-2017-15018
CVE-2015-9101
CVE-2016-10095
CVE-2016-10269
CVE-2016-10092
CVE-2016-10093
CVE-2018-7456
CVE-2018-12900
CVE-2018-17795
CVE-2018-18557
CVE-2016-9560
CVE-2017-14132
CVE-2018-19540
CVE-2018-19541
0.13.62
0.13.62
0.13.62
0.13.62
0.3.6
0.3.6
0.3.6
0.3.6
0.3.6
0.3.6
4.8.1
4.9.1
4.8.1
4.9.2
3.99.5
3.99.5
3.99.5
3.99.5
4.0.7
4.0.7
4.0.7
4.0.7
4.0.9
4.0.9
4.0.9
4.0.9
1.900.25
2.0.13
2.0.14
2.0.14
30
11
6
6
6
7
7
7
7
7
7
19
21
19
22
5
9
9
9
18
17
17
17
19
19
19
19
21
40
40
40
470
11
6
6
6
7
6
7
7
7
7
19
15
19
9
5
6
9
6
13
2
12
11
19
18
0
3
17