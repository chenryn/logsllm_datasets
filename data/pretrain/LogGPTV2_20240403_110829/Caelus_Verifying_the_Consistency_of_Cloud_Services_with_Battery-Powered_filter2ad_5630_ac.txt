important caveat is that each operation must be assigned a
unique global sequence number, so the history server must be
at least sequentially consistent. This requirement only holds for
keys that have common clients. If two sets of keys do not have
any clients in common, the assignment of sequence numbers
among those sets need not be sequentially consistent. Violating
this requirement will result in clients detecting consistency
violations, as it will either result in operations with duplicate
sequence numbers. Thus, it is of no beneﬁt for a malicious
cloud service to violate this requirement. In addition,
the
history server only stores hashes of data objects instead of
the full data objects, so the amount of data stored is relatively
small.
The log on the history server is intended to act as “proof”
that the cloud service is adhering to its promised consistency
model. Depending on the consistency model, the servers and
key-value store may also include additional information about
each operation to facilitate the veriﬁcation that the consistency
model
is met. We discuss the details of the consistency
veriﬁcation procedures below.
Attestation. One of user’s devices, acting as the attestor,
periodically performs attestations by fetching a log segment
from the history server. For now, we assume the attestor has
no battery limitations, which we will remove by employing
attestor-partitioning protocol described in section IV-C. There
are two requirements that the attestor must meet. First, the role
of the attestor is permanently assigned to one and only one
device and the identity of the attestor should be known to all
other devices. Second, the attestor should periodically perform
attestation operations on a schedule that is also known to all
devices.
the
uses
To
request
a
log
attestor
segment,
a
Read_History(GStart, GEnd) operation, which speciﬁes
a section of the log between two global sequence numbers
GStart and GEnd to read. The attestor submits this request
to the server it
is connected to, which reads it from the
history server and returns the results to the attestor. All
log segments are signed by the history server to ensure
tamper with them, which
that a malicious client cannot
enforces Guarantee CLT2. To create the attestation,
the
attestor adds a sequence number and timestamp to the
log segment, signs it and stores it back to the history
server a Write_Attest(GStart, GEnd) operation. Clients
can read attestations from the history server by using a
Read_Attest(GStart, GEnd) operation, which returns
all operations and attestations in the requested range. The
attestor performs attestations at speciﬁc time intervals deﬁned
by the parameter TA.
Clients expect to be able to read a new attestation every
TA + ǫ. ǫ accounts for variable delays due to network and
processing and must be added any time a client is measuring
the delay between two events on the cloud service. This sched-
uled attestation prevents a malicious service from showing
different log contents to different clients. If a malicious service
tries to tamper with or drop portions of the log, it will be
detected when clients verify the log segments against
the
attestations. Replay or omission of log segments or attestations
will be detected by missing sequence numbers in the stream of
attestations. Finally, a malicious service may attempt to drop
all future log segments and attestations (i.e., truncation), but
this will be detected because clients will not be able to read
an attestation at the expected time. Clients cannot distinguish
885
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:07:39 UTC from IEEE Xplore.  Restrictions apply. 
between this type of malicious cloud service and a failed
attestor, but since the attestor is available most of the time
and assumed to only experience failures for short periods of
time, Caelus clients halt until they are able to read any missed
attestations. If a client continues to miss attestations for an
extended period of time, it can notify the user who can then
examine the state of their attestor device to determine if the
device or the device’s network connection has failed or not.
If neither the device nor its network has actually failed, this
indicates that the cloud service is acting maliciously.
Using scheduled attestations, all clients can safely assume
that all attestations will be eventually made identically visible
to all clients. By extension, this guarantees that all clients will
see the same history of operations and from this, detect if the
cloud service is maliciously trying to omit, reorder or delay
client operations using the veriﬁcation procedure we describe
next.
Veriﬁcation. To distribute the veriﬁcation tasks, each client
is responsible for verifying the consistency of its own oper-
ations. Veriﬁcation happens asynchronously to Put and Get
operations when clients periodically fetch attestations using
the Read_Attest operation. Caelus veriﬁes that operations
are inconsistent by at most some time bound TCaelus, thus
enforcing Guarantee SRV3.
Clients verify their operations in 3 steps. First, clients
verify the correctness of the fetched log segment against the
accompanying attestation. Second, clients perform a presence
check, where they verify the individual signatures on each
operation in the log to detect tampering, and check that the log
segment does not omit or replay operations using the sequence
numbers embedded in the operations. Finally, clients verify
the consistency of their Put and Get operations. The exact
method that clients use to verify the consistency of Puts and
Gets depends on the consistency model of the cloud service.
Caelus currently supports 3 consistency models: strong
consistency, eventual consistency and causal consistency with
some time bound deﬁned by the visibility time bound TS.
Under strong consistency, all operations appear to execute in a
single global order with every Get receiving the value of the
immediately preceding Put to the same key. In addition, all
Puts should be globally visible as soon as they are acknowl-
edged by the cloud service. This makes the veriﬁcation of
strong consistency the simplest of all three models. Clients
verify the consistency of Puts by checking that the Put
appears in the next attestation signed by the attestor. This
means that a cloud service could at most delay the effects
of a Put by TA + ǫ. Clients verify the consistency of Gets
by checking that the value returned matches the value of the
immediately preceding Put in the log.
In the bounded eventual consistency model, the results of
Puts need not be immediately visible to all clients, but may
instead take up to the visibility time bound TS, to become
visible to all clients. This is equivalent to the deﬁnition of
bounded consistency used by Pileus [24]. The checks that
clients do to verify consistency are illustrated in Figure 3. To
1 
tA2 – tP2  tP 2 − δ. It
must also check that either the Get 5 is before the Put’s
attestation, i.e. tG2 < tA2 + δ or 6 if the Get is after, that
there are no newer attested Puts that the Get should have
A < tG2 = {∅}. Check 6 handles
read, i.e. t′
the case where the attestation happens before the Get, but is
not fetched and veriﬁed by the client until after the Get. If
the Get passes these checks, then it is veriﬁed and removed
from the unveriﬁed Get list. Otherwise, the Get remains on
the list and will be checked against other attested Puts until
either it is veriﬁed or it times out and becomes a violation.
A : tA2 < t′
A cloud service that implements bounded causal consistency
for Caelus enforces causal consistency on the values read
886
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:07:39 UTC from IEEE Xplore.  Restrictions apply. 
Scenario 1: Honest Server 
Client 1 
Client 2 
o1:Put(a,0) 
o2:Put(a,1) 
o3:Put(b,0) 
o5:Put(a,3) 
1  0 
2  0 
3  0 
4  0 
o6:Put(a,2) 
o7:Get(b,0)  o8:Get(a,1) 
0  1 
3  1 
3  1 
Scenario 2: Malicious Server 
Client 1 
Client 2 
o1:Put(a,0) 
o2:Put(a,1) 
o3:Put(b,0) 
o5:Put(a,3) 
1  0 
2  0 
3  0 
4  0 
o6:Put(a,2) 
o7:Get(b,0)  o8:Get(a,0) 
0  1 
3  1 
3  1 
Fig. 4. Veriﬁcation of causal consistency. The vector clock is shown below
each operation as C1 C2 .
by Gets, and will eventually make all Puts visible to all
clients via the history log. Bounded causal consistency is
also referred to as Causal+ consistency [25] in the literature.
Because Puts must be made globally visible in a bounded
amount of time, verifying the consistency of Puts in bounded
causal consistency is the same as verifying Puts in bounded
eventual consistency.
As with eventual consistency, some Gets may see the result
of Put operations before they become globally visible in the
log. Thus, clients perform the same veriﬁcation steps in causal
consistency as eventual consistency. However, while a Get
in eventual consistency may return either the most recently
attested value or any written but unattested value, Gets in
causal consistency must return the most recent value on which
it is causally dependent. One option for verifying Gets would
be for clients to extract the chain of causal operations it is
dependent on and then verify that the value read matches that
of the most recent Put in the chain. However, if the client
only knows the value that the Get read, it may be ambiguous
which Put it is actually dependent on if there are several
Puts with the same value.
To uniquely identify each operation, we enhance the cloud
servers to attach a vector clock to each operation in the
log [26]. Clients verify the correctness of the vector clocks by
checking that they increase along with the sequence numbers
on operations, which indicate program order. Clients can then
use the vector clocks to verify the freshness of the value
read by checking if there are any newer Puts to the same
key between the vector clock of the Get and its associated
Put. Like in eventual consistency, a client may have to defer
veriﬁcation for up to TS + TA + ǫ to ensure all necessary
attestations have occurred.
To illustrate, consider Figure 4. The Get, o8, by client C2
reads the result of the Put, o2, by C1. We denote a vector
clock of an operation using the notation vc(oi). Note that
vector clocks only increase on Put operations. C2 veriﬁes
the consistency of the Get by verifying that there are no Put
operations on the same key with vector clocks greater than o2
and less than o8. In Scenario 1, there is no violation because
all operations between o2 and o8 modify other keys. o1, o5
and o6 modify the same key, a, but since vc(o1) < vc(o2),
vc(o5) k vc(o8) and vc(o6) k vc(o2) (k means “incompara-
ble”, the two values have no deﬁned order), their results may
legitimately be invisible to o8. However, in Scenario 2, the
cloud service, either maliciously or erroneously, returns the
result of o1 instead of o2 to o8. In this case, client veriﬁcation
will fail because it ﬁnds that vc(o1) < vc(o2) < vc(o8) and
that o2 is a Put to the same key read by o8. A malicious
cloud service cannot assign o2 a vector clock less than vc(o1)
because the order of the vector clocks must match the sequence
number embedded in the operations. Neither can it omit o2
since the presence check done by clients will detect that the
o2 operation is missing from the log.
For large numbers of clients, vector clocks can be expensive
since the length of the vector is determined by the number of
nodes in the system [25]. However, in Caelus vector clocks do
not need to span users who do not share data. Instead, the size