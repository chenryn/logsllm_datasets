We conduct the user study comparing our style-transfer
models on two operating points of 0.5 F1-score and 0.66
F1-scores, to obtain human judgments at two different
levels of privacy effectiveness as shown in Table VI. We
see that the model CycML+Lang outperforms the other
two models at both operating points. CycML+Lang wins
50.74% of the time (ignoring ties) at operating point 0.5
and 57.87% of the time at operating point 0.66. These
results combined with quantitative evaluation discussed in
Section 6.1 conﬁrm that the cyclic ML loss combined with
the language model loss gives the best trade-off between
semantic similarity and privacy effectiveness.
Finally, we conduct the user study between the Cy-
cML+Lang model operating at 0.79 and the Google ma-
chine translation baseline with 3 hops. The operating
point is chosen so that the two models are closest to
each other in privacy effectiveness and meteor score.
Results in Table VII show that our model wins over
the GoogleMT baseline by approximately 16% (59.46%
vs 43.76% rank1) on semantic similarity as per human
judges, while still having better privacy effectiveness.
This is largely because our A4NT model learns not to
change the input text if it is already ambiguous for the at-
tribute classiﬁer, and only makes changes when necessary.
In contrast, changes made by GoogleMT round trip are
not optimized towards maximizing privacy gain, and can
change the input text even when no change is needed.
Apart from the relative evaluation between our model
1644    27th USENIX Security Symposium
USENIX Association
0.20.30.40.50.60.70.8Meteor Score0.00.10.20.30.40.50.60.70.8Document Level F1 Score of TranslationTranslation Effectiveness Vs Semantic ConsistencyFBsemCycMLCycML+Langdecision boundaryOperating Point
0.66
0.5
FBsem CycML
39.75
31.68
32.02
15.03
CycML + Lang
57.87
50.74
Table VI: User study to judge semantic similarity. Three variants of our
model are compared. Numbers show the % times the model ranked ﬁrst.
Can add to more than 100% as multiple models can have rank-1.
Comparison
Operating point
Relative (% Rank 1)
Absolute (0-5)
A4NT CycML + Lang GoogleMT
0.79
59.46
4.51±0.84
0.85
43.76
4.16±0.89
Table VII: User study of our best model and the Google MT baseline.
Figure 7: Privacy and semantic consis-
tency of A4NT and the Google MT base-
line on the human evaluation test set
and the GoogleMT baseline, we additionally conduct sep-
arate a user study for both the models to assess the seman-
tic similarity to the input sentence in an absolute scale.
This study is conducted on the same human-evaluation
test set containing 745 sentences and using the AMT plat-
form as before. We show each human judge the input
sentence and output form either of the models and ask
them to rate the similarity to the input in a Likert scale
from zero to ﬁve. We adopt the instruction used in Se-
mEval task [57] to describe the different rating values
to the user. Here zero rating corresponds to the worst
case where the input and output sentences are not seman-
tically related and ﬁve corresponds to the best case where
they are equivalent in meaning. Full deﬁnition of scales
and further details about the user study is presented in
the appendix B. Each input-output pair is evaluated by
three human judges and we report the mean score and
standard deviation in Table VII. We see the same trend as
in the relative evaluation and our model achieves better
overall score of 4.51/5.0 compared to 4.16 obtained by
the GoogleMT baseline. The score of the A4NT model
lies between the ratings of 4.0 (sentences are equivalent
with unimportant details differing) and 5.0 (sentences are
equivalent). This shows that the A4NT model preserves
the meaning of the input sentence on average, by making
semantically equivalent changes to fool the authorship
classiﬁer.
6.2 Qualitative Analysis
In this section we analyze some qualitative examples of
anonymized text produced by our A4NT model and try to
identify the strengths and the weaknesses of this approach.
Then we analyze the performance of the A4NT network
on different levels of input difﬁculty. We use the attribute
classiﬁers’ score as a proxy measure of the input text
difﬁculty. If the text is conﬁdently correctly classiﬁed
(with classiﬁcation score of 1.0) by the attribute classiﬁer,
then the A4NT network has to make signiﬁcant changes
to fool the classiﬁer. If it is already misclassiﬁed, the style-
transfer network should ideally not make any changes.
6.2.1 Examples of Style Transfer for anonymization
Table VIII shows the results of our A4NT model
CycML+Lang applied to some example sentences in
the blog-age setting. Style transfer in both directions,
teenager to adult and adult to teenager, is shown along
with the corresponding source attribute classiﬁer scores.
The examples illustrate some of the common changes
made by the model and are grouped into three categories
for analysis (# column in Table VIII).
# 1. Using synonyms: The A4NT network often uses
synonyms to change the style to target attribute. This is
seen in style transfers in both directions, teen to adult and
adult to teen in category # 1 samples in Table VIII. We
can see the model replacing “yeh” with “ooh”, “would”
with “will”, “...” with “,” and so on when going from
teen to adult, and replacing “funnily enough” with “haha
besides”, “work out” with “go out” and so on when chang-
ing from adult to teen. We can also see that the changes
are not static, but depend on the context. For example
“yeh” is replaced with “alas” in one instance and with
“ooh” in another. These changes do not alter the meaning
of the sentence too much, but fool the attribute classiﬁers
thereby providing privacy to the author attribute.
# 2. Replacing slang words: When changing from teen
to adult, A4NT often replaces the slang words or incor-
rectly spelled words with standard English words, as seen
in category #2 in Table VIII. For example, replacing “wad”
(what) with “deﬁnitely”, “wadeva” with “perhaps” and
“nuthing” with “ofcourse”. The opposite effect is seen
when going from adult to teenager, with addition of “diz”
(this) and replacing of “think” with “relized” (realized).
These changes are learned entirely from the data, and
would be very hard to encode explicitly in a rule-based
system due to the variety in slangs and spelling mistakes.
# 3. Semantic changes: One failure mode of A4NT is
when the input sentence has semantic content which is
signiﬁcantly more biased to the author’s class. These
examples are shown in category #3 in Table VIII. For
example, when an adult author mentions his “wife”, the
USENIX Association
27th USENIX Security Symposium    1645
0.20.30.40.50.60.70.8Meteor Score0.30.40.50.60.70.80.91.0Document Level F1 Score of TranslationTranslation Effectiveness Vs Semantic ConsistencyFBsemCycMLCycML + LangGoogle-MT# Input: Teen
A(x) Output: Adult
A(x)
0.97 and ooh... it’s raining lots now
0.94 anyhow, i never let anyone really know how i’m feeling .
0.95 alas, it’s just goin ok here too!
0.52 will i go so far to say that i love her?
1 and yeh... it’s raining lots now
0.23
1 yeahh... i never let anyone really know how i’m feeling.
0.24
1 yeh, it’s just goin ok here too!
0.30
1 would i go so far to say that i love her?
0.36
2 wad a nice day.. spend almost the whole afternoon doing work! 0.99 deﬁnitely a nice day.. spend almost the whole afternoon doing work! 0.19
0.49
2 wadeva told u secrets wad did u do ?
0.33
2 i don’t know y i even went into dis relationship
2 i have nuthing else to say about this horrid day.
0.08
0.42
3 after school i got my hair cut so it looks nice again.
3 i had an interesting day at skool.
0.05
0.98 perhaps told u secrets why did u do ?
0.92 i don’t know why i even went into another relationship .
0.79 i have ofcourse else to say about this accountable day.
1.0
0.97 i had an interesting day at wedding.
after all i have my hair cut so it looks nice again.
# Input: Adult
A(x) Output: Teen
1 funnily enough , i do n’t care all that much.
1 i may go to san francisco state, or i may go back.
1 i wonder if they ’ll work out... hard to say.
2 one is to mix my exercise order a bit more.
2 ok, think i really will go to bed now.
3 my ﬁrst day going out to see clients after vacation.
3 i’d tell my wife how much i love her every time i saw her.
3 i do believe all you need is love.
0.58 haha besides , i do n’t care all that much.
0.54 i shall go to san francisco state, or i may go back.
0.52 i wonder if they ’ll go out... hard to say.
0.97 one is to mix my diz exercise order a bit more.
0.79 ok, relized i really will go to bed now.
0.98 my ﬁrst day going out to see some1 after vacation.
0.96 i’d tell my crush how much i love her every time i saw her.
0.58 i dont think all you need is love .
A(x)
0.05
0.09
0.39
0.08
0.08
0.04
0.06
0.11
Table VIII: Qualitative examples of anonymization through style transfer in the blog-age setting. Style transfer in both
direction is shown along with the attribute classiﬁer score of the source attribute.
Input: Obama
Output: Trump
we can do this because we are
MISC.
we can do better than that.
it’s not about reverend PERSON.
but i’m going to need your help.
so that’s my vision.
their situation is getting worse.
i’m kind of the term PERSON
because i do care.
that’s what we need to change.
that’s how our democracy works.
we will do that because we are
MISC.
we will do that better than anybody.
it’s not about crooked PERSON.
but i’m going to ﬁght for your
country.
so that’s my opinion.
their media is getting worse.
i’m tired of the system of PERSON
PERSON because they don’t care.
that’s what she wanted to change.
that’s how our horrible horrible
trade deals.
Table IX: Qualitative examples of style transfer on the
speech dataset from Obama to Trump’s style
A4NT network replaces it with “crush”, altering the mean-
ing of the input sentence. Some common entity pairs
where this behavior is seen are with (school↔work),
(class↔ofﬁce), (dad↔husband), (mum↔wife), and so
on. Arguably, in such cases, there is no obvious solution
to mask the identity of the author without altering these
obviously biased content words.
On the smaller speech dataset however, the changes
made by the A4NT model alter the semantics of the sen-
tences in some cases. Few example style transfers from
Obama to Trump’s style are shown in Table IX. We see
that A4NT inserts hyperbole (“better than anybody”, “hor-
rible horrible”, “crooked”), references to “media” and
“system”, all salient features of Trump’s style. We see
that the style-transfer here is quite successful, sufﬁcient
to completely fool the identity classiﬁer as was seen in Ta-
ble III. However, and somewhat expectedly, the semantics
of the input sentence is generally lost. A possible cause is
that the attribute classiﬁer is too strong on this data, owing
to the small dataset size and the highly distinctive styles
of the two authors, and to fool them the A4NT network
learns to make drastic changes to the input text.
6.2.2 Performance Across Input Difﬁculty
Figure 8 compares the attribute classiﬁer score on the
input sentence and the A4NT output. Ideally we want all
the A4NT outputs to score below the decision boundary,
while also not increasing the classiﬁer score compared to
input text. This “ideal score” is shown as grey solid line.
We see that for the most part all three A4NT models are
below or close to this ideal line. As the input text gets
more difﬁcult (increasing attribute classiﬁer score), the
CycML and CycML+Lang slightly cross above the ideal
line, but still provide signiﬁcant improvement over the
input text (drop in classiﬁer score of about∼ 0.45).
Now, we analyze how much of input semantics is pre-
served with increasing difﬁculty. Figure 9 plots the meteor
1646    27th USENIX Security Symposium
USENIX Association
model. The A4NT network achieves this by learning to
perform style-transfer without paired data.
A4NT offers a new data driven approach to authorship
obfuscation. The ﬂexibility of this end-to-end trainable
model means it can adapt to new attack methods and
datasets. Experiments on three different attributes namely
age, gender and identity, showed that the A4NT network
is able to effectively fool the attribute classiﬁers in all
the three settings. We also show that the A4NT network
also performs well against multiple unseen classiﬁer ar-
chitectures. This strong empirical evidence suggests that
the method is likely to be effective against previously
unknown NLP adversaries.
We developed a novel solution to preserve the mean-
ing of input text using likelihood of reconstruction. Se-
mantic similarity (quantiﬁed by meteor score) of the
A4NT network remains high for easier sentences, which
do not contain obvious give-away words (school, work,
husband etc.), but is lower on difﬁcult sentences indicat-
ing the network effectively learns to identify and apply
the right magnitude of change. The A4NT network can be
operated at different points on the privacy-effectiveness
and semantic-similarity trade-off curve, and thus offers
ﬂexibility to the user. The experiments on the political
speech data show the limits to which style transfer based
approach can be used to hide attributes. On this chal-
lenging data with very distinct styles by the two authors,
our method effectively fools the identity classiﬁer but
achieves this by altering the semantics of the input text.
Acknowledgment
This research was supported in part by the German
Research Foundation (DFG CRC 1223). We would also
like to thank Yang Zhang, Ben Stock and Sven Bugiel for
helpful feedback.
References
[1] P. Juola et al., “Authorship attribution,” Foundations and Trends®
in Information Retrieval, 2008.
[2] E. Stamatatos, “A survey of modern authorship attribution meth-
ods,” Journal of the Association for Information Science and
Technology, 2009.
[3] S. Ruder, P. Ghaffari, and J. G. Breslin, “Character-level and multi-
channel convolutional neural networks for large-scale authorship
attribution,” arXiv preprint arXiv:1609.06686, 2016.
[4] S. Argamon, M. Koppel, J. W. Pennebaker, and J. Schler, “Auto-
matically proﬁling the author of an anonymous text,” Communica-
tions of the ACM, 2009.
[5] R. Overdorf and R. Greenstadt, “Blogs, twitter feeds, and reddit
comments: Cross-domain authorship attribution,” Proceedings on
Privacy Enhancing Technologies, 2016.
[6] A. Narayanan, H. Paskov, N. Z. Gong, J. Bethencourt, E. Stefanov,
E. C. R. Shin, and D. Song, “On the feasibility of internet-scale
author identiﬁcation,” in Security and Privacy (SP), 2012 IEEE
Symposium on.
IEEE, 2012.
Figure 8: Output Privacy vs
Privacy on Input.
Figure 9: Meteor score plot-
ted against input difﬁculty.
Figure 10: Histogram of privacy gain (left side) is shown
alongside comparison of meteor score vs privacy gains.