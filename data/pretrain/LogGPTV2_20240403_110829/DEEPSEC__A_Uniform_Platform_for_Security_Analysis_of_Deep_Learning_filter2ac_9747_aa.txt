title:DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning
Model
author:Xiang Ling and
Shouling Ji and
Jiaxu Zou and
Jiannan Wang and
Chunming Wu and
Bo Li and
Ting Wang
(cid:19)(cid:17)(cid:18)(cid:26)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:52)(cid:90)(cid:78)(cid:81)(cid:80)(cid:84)(cid:74)(cid:86)(cid:78)(cid:1)(cid:80)(cid:79)(cid:1)(cid:52)(cid:70)(cid:68)(cid:86)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:49)(cid:83)(cid:74)(cid:87)(cid:66)(cid:68)(cid:90)
DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model
Xiang Ling
∗
∗,#, Shouling Ji
∗,†,#(u), Jiaxu Zou
†
Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies,
Zhejiang University,
, Chunming Wu
‡
, Bo Li
∗
∗
∗
and Ting Wang
§
, Jiannan Wang
§
Lehigh University
‡
UIUC,
{lingxiang, sji, zoujx96, wangjn84, wuchunming}@zju.edu.cn, PI:EMAIL, PI:EMAIL
Abstract—Deep learning (DL) models are inherently vulnerable
to adversarial examples – maliciously crafted inputs to trigger
target DL models to misbehave – which signiﬁcantly hinders
the application of DL in security-sensitive domains. Intensive
research on adversarial learning has led to an arms race between
adversaries and defenders. Such plethora of emerging attacks and
defenses raise many questions: Which attacks are more evasive,
preprocessing-proof, or transferable? Which defenses are more
effective, utility-preserving, or general? Are ensembles of multiple
defenses more robust than individuals? Yet, due to the lack of
platforms for comprehensive evaluation on adversarial attacks
and defenses, these critical questions remain largely unsolved.
In this paper, we present the design,
implementation, and
evaluation of DEEPSEC, a uniform platform that aims to bridge
this gap. In its current implementation, DEEPSEC incorporates
16 state-of-the-art attacks with 10 attack utility metrics, and 13
state-of-the-art defenses with 5 defensive utility metrics. To our
best knowledge, DEEPSEC is the ﬁrst platform that enables re-
searchers and practitioners to (i) measure the vulnerability of DL
models, (ii) evaluate the effectiveness of various attacks/defenses,
and (iii) conduct comparative studies on attacks/defenses in a
comprehensive and informative manner. Leveraging DEEPSEC,
we systematically evaluate the existing adversarial attack and de-
fense methods, and draw a set of key ﬁndings, which demonstrate
DEEPSEC’s rich functionality, such as (1) the trade-off between
misclassiﬁcation and imperceptibility is empirically conﬁrmed;
(2) most defenses that claim to be universally applicable can only
defend against limited types of attacks under restricted settings;
(3) it is not necessary that adversarial examples with higher per-
turbation magnitude are easier to be detected; (4) the ensemble of
multiple defenses cannot improve the overall defense capability,
but can improve the lower bound of the defense effectiveness
of individuals. Extensive analysis on DEEPSEC demonstrates its
capabilities and advantages as a benchmark platform which can
beneﬁt future adversarial learning research.
I. INTRODUCTION
Recent advances in deep learning (DL) techniques have led
to breakthroughs in a number of long-standing artiﬁcial intelli-
gence tasks (e.g., image classiﬁcation, speech recognition, and
even playing Go [1]). Unfortunately, it has been demonstrated
that existing DL models are inherently vulnerable to adver-
sarial examples [2], which are maliciously crafted inputs to
trigger target DL models to misbehave. Due to the increasing
use of DL models in security-sensitive domains (e.g., self-
driving cars [3], face recognition [4], malware detection [5],
medical diagnostics [6]), the phenomena of adversarial ex-
amples has attracted intensive studies from both academia
and industry, with a variety of adversarial attack and defense
methods being proposed [2], [7], [8]. At a high level, the
attacks attempt to force the target DL models to misclassify
using adversarial examples, which are often generated by
slightly perturbing legitimate inputs; meanwhile, the defenses
attempt to strengthen the resilience of DL models against
such adversarial examples, while maximally preserving the
performance of DL models on legitimate instances.
The security researchers and practitioners are now facing
a myriad of adversarial attacks and defenses; yet, there is
still a lack of quantitative understanding about the strengths
and limitations of these methods due to incomplete or biased
evaluation. First, they are often assessed using simple metrics.
For example, misclassiﬁcation rate is used as the primary
metric to evaluate attack methods. However, as shown in our
studies, misclassiﬁcation rate alone is often insufﬁcient to
characterize an attack method. Second, they are only evaluated
against a small set of attacks/defenses, e.g., many defenses
are evaluated using a few “strong” attacks. However, as found
in our studies, defenses robust against “stronger” attacks are
not necessarily immune to “weaker” ones. Third, the constant
arms race between adversarial attacks and defenses invalidates
conventional wisdom quickly. For instance, the gradient ob-
fuscation strategy adopted by many defenses is later shown
to be ineffective [9]. The compound effects of these factors
often result in contradictory and puzzling conclusions about
the same attack/defense methods. As an example, defensive
distillation (DD) [10] was evaluated against JSMA [11] and
claimed to signiﬁcantly improve the robustness of DL models.
Nevertheless, it was soon found to only provide marginal
robustness improvement against new attacks (e.g., C&W [12]).
Moreover, it was later shown that models trained with DD may
perform ever worse than undefended models [13].
We argue that to further advance the research on adversarial
examples, it is critical to provide an analysis platform to sup-
port comprehensive and informative evaluation of adversarial
attacks and defenses. We envision that a set of desiderata are
required for such a platform to be practically useful:
• Uniform – It should support to compare different at-
tack/defense methods under the same setting;
• Comprehensive – It should include most representative
attack/defense methods;
• Informative – It should include a rich set of metrics to
assess different attack/defense methods;
• Extensible – It should be easily extended to incorporate
new attack/defense methods.
#Xiang Ling and Shouling Ji are the co-ﬁrst authors. Shouling Ji is the
corresponding author.
Unfortunately, none of
the existing work (e.g., Clever-
hans [14]) meets all the requirements (details in Section VI).
(cid:165)(cid:1)(cid:19)(cid:17)(cid:18)(cid:26)(cid:13)(cid:1)(cid:57)(cid:74)(cid:66)(cid:79)(cid:72)(cid:1)(cid:45)(cid:74)(cid:79)(cid:72)(cid:15)(cid:1)(cid:54)(cid:79)(cid:69)(cid:70)(cid:83)(cid:1)(cid:77)(cid:74)(cid:68)(cid:70)(cid:79)(cid:84)(cid:70)(cid:1)(cid:85)(cid:80)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:15)
(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:52)(cid:49)(cid:15)(cid:19)(cid:17)(cid:18)(cid:26)(cid:15)(cid:17)(cid:17)(cid:17)(cid:19)(cid:20)
(cid:23)(cid:24)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
To bridge this gap, we present DEEPSEC, a ﬁrst-of-its-kind
platform for security analysis of DL models, that satisﬁes all
the aforementioned desiderata. In its current implementation,
it incorporates 16 state-of-the-art adversarial attacks with 10
attack utility metrics and 13 representative defenses with 5
defense utility metrics. DEEPSEC enables security researchers
and practitioners to (i) assess the vulnerabilities of given DL
models to various attacks, (ii) evaluate the effectiveness of
various defenses, and (iii) conduct comparative studies on
different attacks/defenses in a comprehensive and informative
manner. To summarize, we make the following contributions.
1) We present DEEPSEC, the ﬁrst platform designed specif-
ically to serve as an evaluation platform for adversarial
attacks/defenses. Two key features differentiate DEEPSEC
from the state-of-the-art adversarial learning libraries: a)
to our best knowledge, DEEPSEC includes the largest
collection of attack/defense methods (16 attacks and 13
defenses) thus far (e.g., Cleverhans [14] only provides 9
attacks and 1 defenses); b) it treats the evaluation metrics
as ﬁrst-class citizens, thereby supporting the evaluation of
attacks/defenses in a uniform and informative manner.
2) Using DEEPSEC, we perform thus far the largest-scale
empirical study on adversarial attacks/defenses under
different metrics, among which 10 for attack and 5
for defense evaluation are proposed within the paper in
addition to the existing ones. Moreover, we perform the
largest-scale cross evaluation between different attack and
defense methods (16 × 13) to understand their relative
strengths and limitations.
3) Through this systematic study, we obtain a set of inter-
esting and insightful ﬁndings that may advance the ﬁeld
of adversarial learning: a) the trade-off between misclas-
siﬁcation and imperceptibility performance of adversarial
examples is experimentally conﬁrmed; b) most defenses
that claim to be universally applicable are only effective
for a very limited number of attacks or partially effective
for attacks under restricted settings; c) the ensemble of
multiple defenses cannot improve the overall defense ca-
pability, but can improve the lower bound of the defense
effectiveness of individuals.
Acronyms and Notations. For convenient reference, we
summarize the acronyms and notations in Tables I and II.
II. ATTACKS & DEFENSES
In this paper, we consider the non-adaptive and white-box
attack scenarios, where the adversary has full knowledge of
the target DL model but is not aware of defenses that might
be deployed. Since most white-box or non-adaptive attacks
can be applied to black-box attacks based on transferability
or adjustments to speciﬁc defenses, considering them would
provide general understanding of current attack scenarios
[33]–[35]. Further, we focus on classiﬁcation tasks.
In this section, we summarize the state-of-the-art attack and
defense methods and present a rich set of metrics to assess the
utility of attack/defense methods.
TABLE I
ABBREVIATIONS AND ACRONYMS
s
m
r
e
T
AE
TA
UA
Adversarial Example
Targeted Attack
Un-targeted Attack
s FGSM
R+FGSM
Fast Gradient Sign Method [15]
Random perturbation with FGSM [16]
Basic Iterative Method [17]
Projected L∞ Gradient Descent attack [18]
U-MI-FGSM Un-targeted Momentum Iterative FGSM [19]
Universal Adversarial Perturbation attack [21]
DeepFool [20]
OptMargin [22]
Least Likely Class attack [17]
Random perturbation with LLC [16]
Iterative LLC attack [17]
T-MI-FGSM Targeted Momentum Iterative FGSM [19]
Box-constrained L-BFGS attack [2]
Jacobian-based Saliency Map Attack [11]
Carlini and Wagner’s attack [12]
Elastic-net Attacks to DNNs [23]
Naive Adversarial Training [24]
Ensemble Adversarial Training [16]
PGD-based Adversarial Training [18]
Defensive Distillation [10]
Input Gradient Regularization [13]
Ensemble Input Transformation [25]
Random Transformations based defense [26]
PixelDefense [27]
Thermometer Encoding defense [28]
Region-based Classiﬁcation [29]
Local Intrinsic Dimensionality
based detector [30]
Feature Squeezing detector [31]
MagNet detector [32]
Misclassiﬁcation Ratio
Average Conﬁdence of Adversarial Class
Average Conﬁdence of True Class
Average Lp Distortion
Average Structural Similarity
Perturbation Sensitivity Distance
Noise Tolerance Estimation
Robustness to Gaussian Blur
Robustness to Image Compression
Computation Cost
Classiﬁcation Accuracy Variance
Classiﬁcation Rectify/Sacriﬁce Ratio
Classiﬁcation Conﬁdence Variance
Classiﬁcation Output Stability
k
c
a
t
t
A
d
e
t
e
g
r
a
t
-
n
U
s
k
c
a
t
t
A
d
e
t
e
g
r
a
T
BIM
PGD
DF
UAP
OM
LLC
R+LLC
ILLC
BLB
JSMA
CW
EAD
NAT
EAT
PAT
DD
IGR
EIT
RT
PD
TE
RC
n LID
FS
s
e
s
n
e
f
e
D
e
t
e
l
p
m
o
C
o
i
t
c
e
t
e
D
MagNet
s
k
c
a
t
t
MR
ACAC
ACTC
ALDp
ASS
PSD
NTE
RGB
RIC
CC
s CAV
A
e
s
n
e
f
e
D
CRR/CSR
CCV
COS
s
k
c
a
t
t
A
s
e
s
n
e
f
e
D
s
c
i
r
t
e
M
y
t
i
l
i
t
U
A. Adversarial Attack Advances
In general, existing attacks can be classiﬁed along multiple
different dimensions [8]. In this subsection, we classify attacks
along two dimensions: adversarial speciﬁcity (i.e., UA and
TA) and attack frequency (i.e., non-iterative attack and
iterative attack). Speciﬁcally, UAs aim to generate AEs that
can be misclassiﬁed into any class which is different from
the ground truth class, while TAs aim to generate AEs to be
misclassiﬁed into a speciﬁc target class. For attack frequency,
non-iterative attacks take only one single step to generate
AEs, while iterative attacks take multiple iterative updates.
In fact, those two categorizations are closely integrated, but
we describe them separately for clarity.
1) Non-iterative UAs: In [15], Goodfellow et al. proposed
the ﬁrst and fastest non-iterative UA, called Fast Gradient
Sign Method (FGSM). By linearizing the loss function, FGSM
perturbs an image by maximizing the loss subject to a L∞
(cid:23)(cid:24)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II
NOTATIONS USED IN THIS PAPER
Notations
(cid:2)X = {X1, · · · , XN}
(cid:2)Y = {y1, · · · , yN}
F : Rm →{1, · · · , k}
P : Rm → Rk
P (X)j
X a ∈ Rm
θ
y∗
J : Rm×{1· · ·k}→ R+
Description
(cid:2)X is the testing set with N
original examples, where Xi ∈ Rm.
(cid:2)Y is the corresponding ground-truth
label set of (cid:2)X, where yi = 1, · · · , k.
F is a DL classiﬁer on k classes,
where F ( (cid:2)X) = (cid:2)y.
j
P is the softmax layer output of F,