# DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Models

## Authors
Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang, Chunming Wu, Bo Li, Ting Wang

### Abstract
Deep learning (DL) models are inherently vulnerable to adversarial examples—maliciously crafted inputs designed to trigger misbehavior in target DL models. This vulnerability significantly hinders the application of DL in security-sensitive domains. Intensive research on adversarial learning has led to an arms race between adversaries and defenders. The plethora of emerging attacks and defenses raises many questions: Which attacks are more evasive, preprocessing-proof, or transferable? Which defenses are more effective, utility-preserving, or general? Are ensembles of multiple defenses more robust than individual defenses? Due to the lack of comprehensive evaluation platforms, these critical questions remain largely unanswered.

In this paper, we present the design, implementation, and evaluation of DEEPSEC, a uniform platform aimed at addressing these gaps. In its current implementation, DEEPSEC incorporates 16 state-of-the-art attacks with 10 attack utility metrics and 13 state-of-the-art defenses with 5 defensive utility metrics. To our knowledge, DEEPSEC is the first platform that enables researchers and practitioners to (i) measure the vulnerability of DL models, (ii) evaluate the effectiveness of various attacks and defenses, and (iii) conduct comparative studies on attacks and defenses in a comprehensive and informative manner.

Leveraging DEEPSEC, we systematically evaluate existing adversarial attack and defense methods, drawing key findings that highlight DEEPSEC's rich functionality. These findings include:
1. The trade-off between misclassification and imperceptibility is empirically confirmed.
2. Most defenses claiming universal applicability are only effective against limited types of attacks under restricted settings.
3. Adversarial examples with higher perturbation magnitudes are not necessarily easier to detect.
4. Ensembles of multiple defenses do not improve overall defense capability but can enhance the lower bound of individual defense effectiveness.

Extensive analysis demonstrates DEEPSEC's capabilities and advantages as a benchmark platform, which can benefit future adversarial learning research.

## I. Introduction
Recent advances in deep learning (DL) techniques have led to breakthroughs in various artificial intelligence tasks, such as image classification, speech recognition, and even playing Go. Unfortunately, it has been shown that existing DL models are inherently vulnerable to adversarial examples, which are maliciously crafted inputs designed to trigger misbehavior. As DL models are increasingly used in security-sensitive domains (e.g., self-driving cars, face recognition, malware detection, medical diagnostics), the phenomenon of adversarial examples has attracted intensive study from both academia and industry, leading to the proposal of various adversarial attack and defense methods.

Security researchers and practitioners now face a myriad of adversarial attacks and defenses, but there is still a lack of quantitative understanding of their strengths and limitations due to incomplete or biased evaluations. First, they are often assessed using simple metrics, such as misclassification rate, which alone is insufficient to characterize an attack method. Second, they are typically evaluated against a small set of attacks/defenses, leading to the assumption that defenses robust against "stronger" attacks are also immune to "weaker" ones, which is not always true. Third, the constant arms race between adversarial attacks and defenses quickly invalidates conventional wisdom. For example, the gradient obfuscation strategy adopted by many defenses was later shown to be ineffective.

To further advance research on adversarial examples, it is crucial to provide an analysis platform that supports comprehensive and informative evaluation of adversarial attacks and defenses. We envision that such a platform should meet the following desiderata:
- **Uniform**: It should support comparing different attack/defense methods under the same setting.
- **Comprehensive**: It should include most representative attack/defense methods.
- **Informative**: It should include a rich set of metrics to assess different attack/defense methods.
- **Extensible**: It should be easily extended to incorporate new attack/defense methods.

None of the existing work (e.g., Cleverhans) meets all these requirements (details in Section VI).

## II. Attacks & Defenses
In this paper, we consider non-adaptive and white-box attack scenarios, where the adversary has full knowledge of the target DL model but is unaware of deployed defenses. Since most white-box or non-adaptive attacks can be adapted to black-box attacks based on transferability or adjustments to specific defenses, considering them provides a general understanding of current attack scenarios. We focus on classification tasks.

### A. Adversarial Attack Advances
Existing attacks can be classified along multiple dimensions. In this section, we classify attacks along two dimensions: adversarial specificity (i.e., untargeted and targeted attacks) and attack frequency (i.e., non-iterative and iterative attacks). Specifically, untargeted attacks aim to generate adversarial examples (AEs) that can be misclassified into any class different from the ground truth class, while targeted attacks aim to generate AEs to be misclassified into a specific target class. Non-iterative attacks take only one step to generate AEs, while iterative attacks take multiple steps.

#### 1. Non-iterative Untargeted Attacks
Goodfellow et al. proposed the first and fastest non-iterative untargeted attack, called Fast Gradient Sign Method (FGSM). By linearizing the loss function, FGSM perturbs an image by maximizing the loss subject to an L∞ constraint.

### Acronyms and Notations
For convenient reference, we summarize the acronyms and notations in Tables I and II.

| **Acronym** | **Description** |
|-------------|-----------------|
| AE          | Adversarial Example |
| TA          | Targeted Attack |
| UA          | Untargeted Attack |
| FGSM        | Fast Gradient Sign Method |
| R+FGSM      | Random perturbation with FGSM |
| BIM         | Basic Iterative Method |
| PGD         | Projected L∞ Gradient Descent attack |
| U-MI-FGSM   | Untargeted Momentum Iterative FGSM |
| UAP         | Universal Adversarial Perturbation attack |
| DF          | DeepFool |
| OM          | OptMargin |
| LLC         | Least Likely Class attack |
| R+LLC       | Random perturbation with LLC |
| ILLC        | Iterative LLC attack |
| T-MI-FGSM   | Targeted Momentum Iterative FGSM |
| BLB         | Box-constrained L-BFGS attack |
| JSMA        | Jacobian-based Saliency Map Attack |
| CW          | Carlini and Wagner’s attack |
| EAD         | Elastic-net Attacks to DNNs |
| NAT         | Naive Adversarial Training |
| EAT         | Ensemble Adversarial Training |
| PAT         | PGD-based Adversarial Training |
| DD          | Defensive Distillation |
| IGR         | Input Gradient Regularization |
| EIT         | Ensemble Input Transformation |
| RT          | Random Transformations based defense |
| PD          | PixelDefense |
| TE          | Thermometer Encoding defense |
| RC          | Region-based Classification |
| LID         | Local Intrinsic Dimensionality based detector |
| FS          | Feature Squeezing detector |
| MagNet      | MagNet detector |
| MR          | Misclassification Ratio |
| ACAC        | Average Confidence of Adversarial Class |
| ACTC        | Average Confidence of True Class |
| ALDp        | Average Lp Distortion |
| ASS         | Average Structural Similarity |
| PSD         | Perturbation Sensitivity Distance |
| NTE         | Noise Tolerance Estimation |
| RGB         | Robustness to Gaussian Blur |
| RIC         | Robustness to Image Compression |
| CC          | Computation Cost |
| CAV         | Classification Accuracy Variance |
| CRR/CSR     | Classification Rectify/Sacrifice Ratio |
| CCV         | Classification Confidence Variance |
| COS         | Classification Output Stability |

| **Notation** | **Description** |
|-------------|-----------------|
| \(\mathcal{X} = \{X_1, \ldots, X_N\}\) | Testing set with \(N\) original examples, where \(X_i \in \mathbb{R}^m\). |
| \(\mathcal{Y} = \{y_1, \ldots, y_N\}\) | Corresponding ground-truth label set of \(\mathcal{X}\), where \(y_i = 1, \ldots, k\). |
| \(F : \mathbb{R}^m \to \{1, \ldots, k\}\) | DL classifier on \(k\) classes, where \(F(\mathcal{X}) = \mathcal{Y}\). |
| \(P : \mathbb{R}^m \to \mathbb{R}^k\) | Softmax layer output of \(F\). |
| \(P(X)_j\) | Probability that \(X\) belongs to class \(j\). |
| \(X_a \in \mathbb{R}^m\) | Adversarial example. |
| \(\theta\) | Model parameters. |
| \(y^*\) | Ground-truth label. |
| \(J : \mathbb{R}^m \times \{1, \ldots, k\} \to \mathbb{R}^+\) | Loss function. |