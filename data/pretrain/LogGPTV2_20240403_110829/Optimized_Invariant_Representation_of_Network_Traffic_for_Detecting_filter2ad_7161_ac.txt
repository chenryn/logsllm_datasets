on the number of bins b and their edges θ deﬁning the
vector of flow 1
.
.
.
vector of flow N
g
a
b
flow 1
flow N
e
m
a
n
t
s
o
h
:
r
e
s
u
1
web logs
.
.
.
.
.
.
2
feature values
histogram
1
e
r
u
t
a
e
f
3
4
5
combined final 
feature vector
feature values
M
e
r
u
t
a
e
f
.
.
.
.
.
.
...
locally-scaled
self-similarity
matrix
feature
differences
histogram
...
Figure 1: Graphical illustration of the individual steps
that are needed to transform the bag (set of ﬂows with the
same user and hostname) into the proposed invariant rep-
resentation. First, the bag is represented with a standard
feature vector (1). Then feature values histograms of lo-
cally scaled feature values are computed for each feature
separately (2). Next, the locally-scaled self-similarity
matrix is computed for each feature (3) to capture inner
differences. This matrix is then transformed into feature
differences histogram (4), which is invariant on the num-
ber or the ordering of the samples within the bag. Finally,
feature values and feature differences histograms of all
features are concatenated into resulting feature vector.
width of the histogram bins. These parameters that were
manually predeﬁned in Section 4 C inﬂuence the clas-
siﬁcation performance. Incorrectly chosen parameters b
and θ leads to suboptimal efﬁcacy results. To deﬁne the
parameters optimally, we propose a novel approach of
learning these parameters automatically from the training
data in such a way to maximize the classiﬁcation separa-
bility between positive and negative samples.
k and zS
When creating histograms in Section 4 C, the input
instances are vectors zX
k , where k ∈ {1, . . . ,n}.
The algorithm transforms the input instances into a con-
˜S ;θ ). To keep the nota-
catenated histogram φ ( ˜X;
tion simple and concise, we will denote the input in-
stances simply as z = (z1, . . . ,z n) ∈ Rn×m (instead of
z = (zX
n )), which is a sequence of n
vectors each of dimension m.
1 , . . . ,z S
1 , . . . ,z X
n ,zS
The input
instance z is represented via a feature
map φ : Rn×m → Rn·b deﬁned as a concatenation of the
normalized histograms of all vectors in that sequence,
that is, φ (z;θ ) = (φ (z1;θ 1), . . . ,θ (zn;θ n)), where θ =
(θ 1, . . . ,θ n) denotes bin edges of all normalized his-
tograms stacked to a single vector.
We aim at designing a classiﬁer h: Rn×m × Rn+1 ×
Rn(b+1) → {−1, +1} working on top of the histogram
representation, that is
812  25th USENIX Security Symposium 
USENIX Association
6
h(z;w,w0,θ ) =sign((cid:31)φ (z,w)(cid:30) + w0)
= sign(cid:31) n
∑
φ (zi,θi, j−1,θi, j)wi, j + w0(cid:30) .
∑
j=1
i=1
b
(6)
The classiﬁer (6) is linear in the parameters (w,w0) but
non-linear in θ and z. We are going to show how to learn
parameters (w,w0) and implicitly also θ via a convex op-
timization.
Assume we are given a training set of examples
{(z1,y1), . . . ,(z m,ym)} ∈ (Rn×m × {+1,−1})m. We ﬁx
the representation φ such that the number of bins b is
sufﬁciently large and the bin edges θ are equally spaced.
We ﬁnd the weights (w,w0) by solving
w∈Rb·p,w0∈R(cid:29)γ
min
+
1
m
n
∑
i=1
m
∑
i=1
b−1
∑
j=1|wi, j − wi, j+1|
max(cid:28)0,1− yi(cid:31)φ (zi;θ ),w(cid:30)}(cid:27) .
(7)
The objective is a sum of two convex terms. The second
term is the standard hinge-loss surrogate of the training
classiﬁcation error. The ﬁrst term is a regularization en-
couraging weights of neighboring bins to be similar. If
it happens that j-th and j + 1 bin of the i-the histogram
have the same weight, wi, j = wi, j+1 = w, then these bins
can be effectively merged to a single bin because
wi, jφ (zi;θi, j−1,θi, j) +w i, j+1φ (zi;θi, j,θi, j+1)
= 2wφ (zi;θi, j−1,θi, j+1) .
(8)
The trade-off constant γ > 0 can be used to control the
number of merged bins. A large value of γ will result
in massive merging and consequently in a small number
of resulting bins. Hence the objective of the problem (7)
is to minimize the training error and to simultaneously
control the number of resulting bins. The number of bins
inﬂuences the expressive power of the classiﬁer and thus
also the generalization of the classiﬁer. The optimal set-
ting of λ is found by tuning its value on a validation set.
Once the problem (7) is solved, we use the result-
ing weights w∗ to construct a new set of bin edges θ∗
such that we merge the original bins if the neighboring
weights have the same sign (i.e. if w∗i, jw∗i, j+1 > 0). This
implies that the new bin edges θ∗ are a subset of the orig-
inal bin edges θ, however, their number can be signiﬁ-
cantly reduced (depending on γ) and they have different
widths unlike the original bins. Having the new bins de-
ﬁned, we learn a new set of weights by the standard SVM
algorithm
w∈Rn,w0∈R(cid:29) λ
min
2 (cid:21)w(cid:21)2 +
1
m
m
∑
i=1
max(cid:28)0,1− yi(cid:31)φ (zi;θ∗),w(cid:30)}(cid:27) .
7
Malicious Bag - Sality v1
Malicious Bag - Sality v2
hxxp://sevgikresi.net/logof.gif?8134c8=846765
hxxp://sevgikresi.net/logof.gif?25aa74=22216212
hxxp://sevgikresi.net/logof.gif?4fa0c=1630780
hxxp://sevgikresi.net/logof.gif?a1d1c8=42420000
hxxp://sevgikresi.net/logof.gif?87ddc=1788312
hxxp://brucegarrod.com/images/logos.gif?645ed3=65778750
hxxp://brucegarrod.com/images/logos.gif?64647e=59213934
hxxp://brucegarrod.com/images/logos.gif?23dfd3=11755295
hxxp://brucegarrod.com/images/logos.gif?3a7d2=1916560
hxxp://brucegarrod.com/images/logos.gif?3b54a=1944144
(45, 47, 45, 47, 45)
(45, 47, 45, 47, 45)
(55, 55, 55, 53, 53)
1
2
3
hF
3
2.5
2
1.5
1
0.5
0
-0.2
h F
3
2.5
2
1.5
1
0.5
0
0.2
0.4
0.6
0.8
1
1.2
0
-0.2
0
0.2
0.4
0.6
0.8
1
h S
3
2.5
2
1.5
1
0.5
0
-0.2
0
0.2
0.4
0.6
0.8
1
h S
3
2.5
2
1.5
1
0.5
0
-0.2
0
0.2
0.4
0.6
0.8
1
4
(0.6, 0, 0, 0.4, 0.4, 0, 0, 0.6)
(0.4, 0, 0, 0.6, 0.4, 0, 0, 0.6)
Figure 2: Illustration of the proposed representation ap-
plied on two versions of malware Sality. First, two bags
of ﬂows are created (1), one bag for each Sality sample.
Next, ﬂow-based feature vectors are created for each bag
(2). For illustrative purposes, only a single feature is used
In the third step, histograms of feature
- URL length.
values φ (zX
k ,θ X
k ) and feature differences φ (zS
k) are
created (3) as described in Section 4.3. Only four bins
for each histogram were used. Finally, all histograms
are concatenated into the ﬁnal feature vector (4). Even
though the malware samples are from two different ver-
sions, they have the same histogram of feature differ-
ences φ (zS
k ) is not invariant against
shift, you can see that half of the values of φ (zX
k ) are
different. Still, φ (zX
k ) values may play an important
role when separating malware samples from other legiti-
mate trafﬁc.
k). Since φ (zX
k ,θ S
k ,θ S
k ,θ X
k ,θ X
k ,θ X
Note that we could add the quadratic regularizer λ
2 (cid:21)w(cid:21)2
to the objective of (7) and learn the weights and the rep-
resentation in a single stage. However, this would re-
quire tuning two regularization parameters (λ and γ) si-
multaneously which would be order of magnitude more
expensive than tuning them separately in the two stage
approach.
6 Malware Representation Example
This Section illustrates how the proposed representation
(nonoptimized version) is calculated for two real-world
examples of malicious behavior. Namely, two versions
of a polymorphic malware Sality are compared. Sality
[13] is a malware family that has become a dynamic and
complex form of malicious infection.
It utilizes poly-
morphic techniques to infect ﬁles of Widows machines.
Signature-based systems or classiﬁers trained on a spe-
ciﬁc malware type often struggles with detecting new
variants of this kind of malware. Note that most of the
conclusions to the discussion that follows can be drawn
for many other malware threats.
USENIX Association  
25th USENIX Security Symposium  813
Figure 2 shows how the two Sality samples are repre-
sented with the proposed approach. First, the input ﬂows
are grouped into two bags (one bag for each Sality sam-
ple), because all ﬂows of each bag have the same user and
the same hostname (1). For the sake of simplicity, only
URLs of the corresponding ﬂows are displayed. Next,
88 ﬂow-based feature vectors are computed for each bag
(2). To simplify illustration, we use only a single fea-
ture – URL length. After this step, each Sality sample
is represented with one feature vector of ﬂow-based val-
ues. Existing approaches use these vectors as the input
for the subsequent detection methods. As we will show
in Section 7, these feature values are highly variable for
malware categories. Classiﬁcation models trained with
such feature values loose generalization capability.
k ,θ X
k ,θ X
k ,θ X
k ,θ S
k ) and feature differences φ (zS
To enhance the robustness of the ﬂow-based features,
the proposed approach computes histograms of feature
values φ (zX
k) (3)
as described in Section 4.3. To make the illustration sim-
ple, only four bins for each histogram were used. Finally,
all histograms are concatenated into the ﬁnal feature vec-
tor (4).
It can be seen that even though the malware
samples are from two different versions, they have the
same histogram of feature differences φ (zS
k). Since
the histogram of feature values φ (zX
k ) is not invariant
against shift, half of the values of φ (zX
k ) are different.
The number of histogram bins and their sizes are then
learned from the data by the proposed algorithm (see
Section 5). The proposed representation describes inner
dynamics of ﬂows from each bag, which is a robust indi-
cator of malware samples, as we will show in the analy-
sis of various malware families in Section 8. In contrast
to the existing methods that use ﬂow-based features or
general statistics such as mean or standard deviation, the
proposed representation reﬂects properties that are much
more difﬁcult for an attacker to evade detection.
k ,θ S
7 Evasion Possibilities
This section discusses evasion options for an attacker
when trying to evade a learning-based classiﬁcation sys-
tem. According to the recent work [35], the essential
components for an evasion are: (1) the set of features
used by the classiﬁer, (2) the training dataset used for
training, (3) the classiﬁcation algorithm with its parame-