### 5.1 Experimental Setup

We conducted all throughput and latency experiments on 18 machines in Apt. The 17 client machines each run up to 3 client processes. With a maximum of 4 outstanding requests per client, our implementation requires at least 36 client processes to fully utilize the server’s throughput. To ensure sufficient load, we used 51 client processes. The server machine runs 6 server processes, each pinned to a distinct physical core. The machine configuration is detailed in Table 2. All machines run Ubuntu 12.04 with Mellanox’s OFED v2.2 stack.

#### Comparison Against Simplified Alternatives

To focus on the impact of network-related decisions, we compared our full HERD implementation against simplified versions of Pilaf and FaRM-KV. These simplified implementations use the same communication methods as the original systems but omit the actual key-value storage, instead returning results immediately. We made this decision for two reasons:
1. While working with Pilaf’s code, we observed several optimization opportunities. We did not want our evaluation to be influenced by the relative performance tuning of the systems.
2. We did not have access to the FaRM source code and could not run Windows Server on our cluster. Therefore, we created and evaluated emulated versions of these systems without their backing data structures. This approach gives these systems the maximum performance advantage possible, so the reported throughput for both Pilaf and FaRM-KV may be higher than what is actually achievable.

Pilaf uses a 2-level lookup: a hash table maps keys to pointers, which are then used to find the value from flat memory regions called extents. FaRM-KV, in its default mode, uses single-level lookups by inlining the value in the hash table. It also has a two-level mode where the value is stored "out-of-table." Since the out-of-table mode is necessary for memory efficiency with variable-length keys, we compare HERD against both modes. In the following sections, we denote the size of a key, value, and pointer by \( S_K \), \( S_V \), and \( S_P \) respectively.

### Figure 8: Layout of the Request Region at the Server

The request region at the server machine is organized into separate chunks for each server process, which are further subdivided into per-client chunks. Each per-client chunk consists of \( W \) slots, allowing each client to have up to \( W \) pending requests to each server process. The total size of the request region is \( N_S \cdot N_C \cdot W \) KB. With \( N_C = 200 \), \( N_S = 16 \), and \( W = 2 \), this is approximately 6 MB and fits within the server’s L3 cache. Each server process polls the per-client chunks for new requests in a round-robin fashion. If server process \( s \) has seen \( r \) requests from client number \( c \), it polls the request region at the request slot number \( s \cdot (W \cdot N_C) + (c \cdot W) + r \mod W \).

A network configuration using bidirectional, all-to-all communication with connected transports would require \( N_C \cdot N_S \) queue pairs at the server. However, HERD uses connected transports only for the request side of communication, requiring only \( N_C \) connected queue pairs. The configuration works as follows: an initializer process creates the request region, registers it with RNICs, establishes a UC connection with each client, and then goes to sleep. The \( N_S \) server processes map the request region into their address space via `shmget()` and do not create any connections for receiving requests.

### 4.3 Responses

In HERD, responses are sent as SENDs over UD. Each client creates \( N_S \) UD queue pairs (QPs), while each server process uses only one UD QP. Before writing a new request to server process \( s \), a client posts a RECV to its \( s \)-th UD QP, specifying the memory area where the server’s response will be written. Each client allocates a response region containing \( W \cdot N_S \) response slots, used for the target addresses in the RECVs. After writing \( W \) requests, the client starts checking for responses by polling for RECV completions. On each successful completion, it posts another request.

This design deliberately shifts work from the server’s RNIC to the client, assuming that client machines often perform enough other work that saturating 40 or 56 gigabits of network bandwidth is not their primary concern. For dedicated servers, such as in Memcached, achieving high bandwidth is crucial.

### 5.1.1 Emulating Pilaf

In K-B cuckoo hashing, every key can be found in \( K \) different buckets, determined by \( K \) orthogonal hash functions. Each bucket contains \( B \) slots. Pilaf uses 3-1 cuckoo hashing with 75% memory efficiency and 1.6 average probes per GET. When reading the hash index via RDMA, the smallest unit that must be read is a bucket. A bucket in Pilaf has one slot containing a 4-byte pointer, two 8-byte checksums, and a few other fields, totaling 32 bytes for alignment.

**GET:** A GET in Pilaf consists of 1.6 bucket READs (on average) to find the value pointer, followed by a \( S_V \) byte READ to fetch the value. Concurrent READs for both cuckoo buckets can reduce latency but decrease throughput. We wait for the first READ to complete and issue the second READ only if required.

**PUT:** For a PUT, a client SENDs a \( S_K + S_V \) byte message containing the new key-value item to the server. Our evaluation focuses on network communication, so we ignore the potential need to relocate entries in the cuckoo hash table.

In emulating Pilaf, we enable all RDMA optimizations for both request types, resulting in the system called Pilaf-em-OPT.

### 5.1.2 Emulating FaRM-KV

FaRM-KV uses a variant of Hopscotch hashing to locate a key in approximately one READ. The algorithm ensures that a key-value pair is stored in a small neighborhood of the bucket that the key hashes to. The neighborhood size is tunable, but the authors set it to 6 for items smaller than 128 bytes. FaRM-KV can inline values in the buckets or store them separately, storing only pointers in the buckets. We call our version with inlined values FaRM-em and without inlining FaRM-em-VAR (for variable-length values).

**GET:** A GET in FaRM-em requires a 6 * (\( S_K + S_V \)) byte READ. In FaRM-em-VAR, a GET requires a 6 * (\( S_K + S_P \)) byte READ followed by a \( S_V \) byte READ.

**PUT:** FaRM-KV handles PUTs by sending messages to the server via WRITEs, similar to HERD. The server notifies the client of PUT completion using another WRITE. Therefore, a PUT in FaRM-em (and FaRM-em-VAR) consists of one \( S_K + S_V \) byte WRITE from the client to the server and one WRITE from the server to the client. For higher throughput, we perform these WRITEs over UC, unlike the original FaRM paper that used RC (Figure 5).

### 5.2 Workloads

Three main workload parameters affect the throughput and latency of a key-value system: the relative frequency of PUTs and GETs, item size, and skew.

We use two types of workloads: read-intensive (95% GET, 5% PUT) and write-intensive (50% GET, 50% PUT). The workload can be uniform or skewed. Under a uniform workload, keys are chosen uniformly at random from the 16-byte keyhash space. The skewed workload draws keys from a Zipf distribution with parameter 0.99, generated offline using YCSB [7]. We generated 480 million keys once and assigned 8 million keys to each of the 51 client processes.

### 5.3 Throughput Comparison

We now compare the end-to-end throughput of HERD against the emulated versions of Pilaf and FaRM.

**Figure 9** plots the throughput of these systems for read-intensive and write-intensive workloads for 48-byte items (\( S_K = 16 \), \( S_V = 32 \)). This item size is representative of real-life workloads, as an analysis of Facebook’s general-purpose key-value store [6] showed that the 50th percentile of key sizes is approximately 30 bytes, and that of value sizes is 20 bytes. To compare the READ-based GETs of Pilaf and FaRM with Pilaf’s SEND/RECV-based PUTs, we also plot the throughput when the workload consists of 100% PUTs.

In HERD, both read-intensive and write-intensive workloads achieve 26 Mops, slightly larger than the throughput of native RDMA reads of a similar size (Figure 3b). For small key-value items, there is little difference between PUT and GET requests at the RDMA layer because both fit inside one cacheline. Therefore, the throughput does not depend on the workload composition.

The GET throughput of Pilaf-em-OPT and FaRM-em(-VAR) is directly determined by the throughput of RDMA READs. A GET in Pilaf-em-OPT involves 2.6 READs (on average), resulting in 9.9 Mops, about 2.6X smaller than the maximum READ throughput. For GETs, FaRM-em requires a single 288-byte READ and delivers 17.2 Mops. FaRM-em-VAR requires a second READ and has a throughput of 11.4 Mops for GETs.

Surprisingly, the PUT throughput in our emulated systems is much larger than their GET throughput. In FaRM-em(-VAR), PUTs use small WRITEs over UC that outperform the large READs required for GETs. Pilaf-em-OPT uses SEND/RECV-based requests and replies for PUT. Both Pilaf and FaRM assume that messaging-based ECHOs are more expensive than READs. (Pilaf reports that for 17-byte messages, the throughput of RDMA reads is 2.449 Mops, whereas the throughput of SEND/RECV-based ECHOs is only 0.668 Mops.) If SEND/RECV can provide only one-fourth the throughput of READ, it makes sense to use multiple READs for GET.

However, we believe these systems do not achieve the full capacity of SEND/RECV. After optimizing SENDs by using unreliable transport, payload inlining, and selective signaling, SEND/RECV-based ECHOs, as shown in Figure 5, achieve 21 Mops, which is considerably more than half of our READ throughput (26 Mops). Therefore, we conclude that SEND/RECV-based communication, when used effectively, is more efficient than using multiple READs per request.

**Figure 10** shows the throughput of the three systems with 16-byte keys and different value sizes for a read-intensive workload. For up to 60-byte items, HERD delivers over 26 Mops, slightly greater than the peak READ throughput. Up to 32-byte values, FaRM-em also delivers high throughput. However, its throughput declines quickly with increasing value size because the size of FaRM-em’s READs grows rapidly (as 6 * (\( S_V + 16 \))). This problem is fundamental to the Hopscotch-based KV design, which amplifies the READ size to reduce round trips. FaRM-KV quickly saturates link bandwidths (PCIe or InfiniBand/RoCE) with smaller items than HERD, which conserves network bandwidth by transmitting only essential data. **Figure 10** illustrates this effect. FaRM-em saturates the PCIe 2.0 bandwidth on Susitna with 4-byte values and the 56 Gbps InfiniBand bandwidth on Apt with 32-byte values. HERD achieves high performance for up to 32-byte values on Susitna and 60-byte values on Apt, and is bottlenecked by the smaller PCIe PIO bandwidth.

With large values (144 bytes on Apt, 192 on Susitna), HERD switches to using non-inlined SENDs for responses. The outbound throughput of large inlined messages is less than non-inlined messages because DMA outperforms PIO for large payloads (Figure 4b).

### 5.4 Latency Comparison

**Figure 11** shows the end-to-end latency with 48-byte items and a read-intensive workload. When using 6 CPU cores at the server, HERD delivers 26 million requests per second with an average latency of approximately 5 µs. For fixed-length key-value items, FaRM-em provides the lowest latency among the three systems because it requires only one network round trip (unlike Pilaf-em-OPT) and no computation at the server (unlike HERD). For variable-length values, FaRM’s variable-length mode requires two RTTs, yielding worse latency than HERD.

The PUT latency for all three systems (not shown) is similar because the network path traversed is the same. The measured latency for HERD was slightly higher than that of the emulated systems because it performed actual hash table and memory manipulation for inserts, but this is an artifact of the performance advantage we give Pilaf-em and FaRM-em.

### 5.5 Scalability

We conducted a larger experiment to understand HERD’s scalability with the number of clients. We used one machine to run 6 server processes and the remaining 186 machines for client processes. The experiment uses 16-byte keys and 32-byte values.

**Figure 12** shows the results from this experiment. HERD delivers its maximum throughput for up to 260 client processes. With even more clients, HERD’s throughput starts decreasing almost linearly. The rate of decrease can be reduced by increasing the number of outstanding requests maintained by each client, at the cost of higher request latency. **Figure 12** shows the results for two window sizes: 4 (HERD’s default) and 16. This observation suggests that the decline is due to cache misses in RNICs, as more outstanding verbs in a queue can reduce cache pressure. We expect this scalability limit to be resolved with the introduction of Dynamically Connected Transport in the new Connect-IB cards [1, 8].

Another likely scalability limit of our current HERD design is the round-robin polling at the server for requests. With thousands of clients, using WRITEs for inbound requests may incur too much CPU overhead; mitigating this effect may necessitate switching to a SEND/SEND architecture over Unreliable Datagram transport.

**Figure 10:** End-to-end throughput comparison with different value sizes

**Figure 9:** End-to-end throughput comparison for 48-byte key-value items

**Figure 11:** End-to-end latency with 48-byte items and read-intensive workload

**Figure 12:** Throughput with variable numbers of client processes and different window sizes