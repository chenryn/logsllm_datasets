of the HERD design relative to READ-based designs—its higher
server CPU use—and put this in context with the total (client +
server) CPU required by all systems.
5.1 Experimental setup
We run all our throughput and latency experiments on 18 machines
in Apt. The 17 client machines run up to 3 client processes each.
With at most 4 outstanding requests per client, our implementation
requires at least 36 client processes to saturate the server’s through-
put. We over-provision slightly by using 51 client processes. The
server machine runs 6 server processes, each pinned to a distinct
physical core. The machine conﬁguration is described in Table 2.
The machines run Ubuntu 12.04 with Mellanox’s OFED v2.2 stack.
Comparison against stripped-down alternatives: In keeping
with our focus on understanding the effects of network-related deci-
sions, we compare our (full) HERD implementation against simpli-
ﬁed implementations of Pilaf and FaRM-KV. These simpliﬁed im-
plementations use the same communication methods as the originals,
but omit the actual key-value storage, instead returning a result in-
stantly. We made this decision for two reasons. First, while working
with Pilaf’s code, we observed several optimization opportunities;
we did not want our evaluation to depend on the relative perfor-
mance tuning of the systems. Second, we did not have access to the
FaRM source code, and we could not run Windows Server on our
cluster. Instead, we created and evaluated emulated versions of the
two systems which do not include their backing data structures. This
approach gives these systems the maximum performance advantage
possible, so the throughput we report for both Pilaf and FaRM-KV
may be higher than is actually achievable by those systems.
Pilaf is based on 2-level lookups: a hash-table maps keys to point-
ers. The pointer is used to ﬁnd the value associated with the key from
ﬂat memory regions called extents. FaRM-KV, in its default operat-
ing mode, uses single-level lookups. It achieves this by inlining the
value in the hash-table. It also has a two-level mode, where the value
is stored “out-of-table.” Because the out-of-table mode is necessary
for memory efﬁciency with variable length keys, we compare HERD
against both modes. In the following two subsections, we denote the
size of a key, value, and pointer by SK, SV , and SP respectively.
Figure 8: Layout of the request region at the server
to-right ordering of the RNIC’s DMA writes [16, 8]. We use the
keyhash ﬁeld to poll for new requests; therefore, the key is written
to the rightmost 16 bytes of the 1 KB slot. A non-zero keyhash
indicates a new request, so we do not allow the clients to use a zero
keyhash. The server zeroes out the keyhash ﬁeld of the slot after
sending a response, freeing it up for a new request.
Figure 8 shows the layout of the request region at the server
machine. It consists of separate chunks for each server process
which are further sub-divided into per-client chunks. Each per-
client chunk consists of W slots, i.e., each client can have up to W
pending requests to each server process. The size of the request
region is NS · NC · W KB. With NC = 200, NS = 16 and W = 2,
this is approximately 6 MB and ﬁts inside the server’s L3 cache.
Each server process polls the per-client chunks for new requests in
a round robin fashion. If server process s has seen r requests from
client number c, it polls the request region at the request slot number
s· (W · Nc) + (c·W ) + r mod W .
A network conﬁguration using bidirectional, all-to-all, communi-
cation with connected transports would require NC · NS queue pairs
at the server. HERD, however, uses connected transports for only the
request side of communication, and thus requires only NC connected
queue pairs. The conﬁguration works as follows. An initializer pro-
cess creates the request region, registers it with RNICS, establishes
a UC connection with each client, and goes to sleep. The NS server
processes then map the request region into their address space via
shmget() and do not create any connections for receiving requests.
4.3 Responses
In HERD, responses are sent as SENDs over UD. Each client creates
NS UD queue pairs (QPs) whereas each server process uses only one
UD QP. Before writing a new request to server process s, a client
posts a RECV to its s-th UD QP. This RECV speciﬁes the memory
area on the client where the server’s response will be written. Each
client allocates a response region containing W · NS response slots:
this region is used for the target addresses in the RECVs. After
writing out W requests, the client starts checking for responses by
polling for RECV completions. On each successful completion, it
posts another request.
The design outlined thus far deliberately shifts work from the
server’s RNIC to the client’s, with the assumption that client ma-
chines often perform enough other work that saturating 40 or 56
gigabits of network bandwidth is not their primary concern. The
servers, however, in an application such as Memcached, are often
dedicated machines, and achieving high bandwidth is important.
Core1Core2CoreNSClient1ClientNC12WVALUELENKEY13025.1.1 Emulating Pilaf
In K-B cuckoo hashing, every key can be found in K different buck-
ets, determined by K orthogonal hash functions. For associativity,
each bucket contains B slots. Pilaf uses 3-1 cuckoo hashing with
75% memory efﬁciency and 1.6 average probes per GET (higher
memory efﬁciency with fewer, but slightly larger, average probes is
possible with 2-4 cuckoo hashing [9]). When reading the hash index
via RDMA, the smallest unit that must be read is a bucket. A bucket
in Pilaf has only one slot that contains a 4 byte pointer, two 8 byte
checksums, and a few other ﬁelds. We assume the bucket size in
Pilaf to be 32 bytes for alignment.
GET: A GET in Pilaf consists of 1.6 bucket READs (on average)
to ﬁnd the value pointer, followed by a SV byte READ to fetch the
value. It is possible to reduce Pilaf’s latency by issuing concurrent
READs for both cuckoo buckets. As this comes at the cost of
decreased throughput, we wait for the ﬁrst READ to complete and
issue the second READ only if it is required.
PUT: For a PUT, a client SENDs a SK + SV byte message contain-
ing the new key-value item to the server. This request may require
relocating entries in the cuckoo hash-table, but we ignore that as our
evaluation focuses on the network communication only.
In emulating Pilaf, we enable all of our RDMA optimizations for
both request types; we call the resulting system Pilaf-em-OPT.
5.1.2 Emulating FaRM-KV
FaRM-KV uses a variant of Hopscotch hashing to locate a key in
approximately one READ. Its algorithm guarantees that a key-value
pair is stored in a small neighborhood of the bucket that the key
hashes to. The size of the neighborhood is tunable, but its authors
set it to 6 to balance good space utilization and performance for
items smaller than 128 bytes. FaRM-KV can inline the values in the
buckets, or it can store them separately and only store pointers in
the buckets. We call our version of FaRM-KV with inlined values
FaRM-em and without inlining FaRM-em-VAR (for variable length
values).
GET: A GET in FaRM-em requires a 6 * (SK + SV ) byte READ.
In FaRM-em-VAR, a GET requires a 6 * (SK + SP) byte READ
followed by a SV byte READ.
PUT: FaRM-KV handles PUTs by sending messages to the server
via WRITEs, similar to HERD. The server notiﬁes the client of PUT
completion using another WRITE. Therefore, a PUT in FaRM-em
(and FaRM-em-VAR) consists of one SK + SV byte WRITE from a
client to the server, and one WRITE from the server to the client.
For higher throughput, we perform these WRITEs over UC unlike
the original FaRM paper that used RC (Figure 5).
5.2 Workloads
Three main workload parameters affect the throughput and latency
of a key-value system: relative frequency of PUTs and GETs, item
size, and skew.
We use two types of workloads: read-intensive (95% GET, 5%
PUT) and write-intensive (50% GET, 50% PUT). Our workload can
either be uniform or skewed. Under a uniform workload, the keys
are chosen uniformly at random from the 16 byte keyhash space.
The skewed workload draws keys from a Zipf distribution with
parameter .99. This workload is generated ofﬂine using YCSB [7].
We generated 480 million keys once and assigned 8 million keys to
each of the 51 client processes.
5.3 Throughput comparison
We now compare the end-to-end throughput of HERD against the
emulated versions of Pilaf and FaRM.
Figure 9 plots the throughput of these system for read-intensive
and write-intensive workloads for 48-byte items (SK = 16, SV = 32).
We chose this item size because it is representative of real-life work-
loads: an analysis of Facebook’s general-purpose key-value store [6]
showed that the 50-th percentile of key sizes is approximately 30
bytes, and that of value sizes is 20 bytes. To compare the READ-
based GETs of Pilaf and FaRM with Pilaf’s SEND/RECV-based
PUTs, we also plot the throughput when the workload consists of
100% PUTs.
In HERD, both read-intensive and write-intensive workloads
achieve 26 Mops, which is slightly larger than the throughput of
native RDMA reads of a similar size (Figure 3b). For small key-
value items, there is very little difference between PUT and GET
requests at the RDMA layer because both types of requests ﬁt inside
one cacheline. Therefore, the throughput does not depend on the
workload composition.
The GET throughput of Pilaf-em-OPT and FaRM-em(-VAR) is
directly determined by the throughput of RDMA READs. A GET in
Pilaf-em-OPT involves 2.6 READs (on average). Its GET throughput
is 9.9 Mops, which is about 2.6X smaller than the maximum READ
throughput. For GETs, FaRM-em requires a single 288 byte READ
and delivers 17.2 Mops. FaRM-em-VAR requires a second READ
and has throughput of 11.4 Mops for GETs.
Surprisingly, the PUT throughput in our emulated systems is
much larger than their GET throughput. This is explained as fol-
lows. In FaRM-em(-VAR), PUTs use small WRITEs over UC that
outperform the large READs required for GETs. Pilaf-em-OPT uses
SEND/RECV-based requests and replies for PUT. Both Pilaf and
FaRM assume that messaging-based ECHOs are much more ex-
pensive than READs. (Pilaf reports that for 17 byte messages, the
throughput of RDMA reads is 2.449 Mops whereas the throughput of
SEND/RECV-based ECHOs is only 0.668 Mops.) If SEND/RECV
can provide only one fourth the throughput of READ, it makes sense
to use multiple READs for GET.
However, we believe that these systems do not achieve the
full capacity of SEND/RECV. After optimizing SENDs by us-
ing unreliable transport, payload inlining, and selective signaling,
SEND/RECV based ECHOs, as shown in Figure 5, achieve 21 Mops,
which is considerably more than half of our READ throughput (26
Mops). Therefore, we conclude that SEND/RECV-based communi-
cation, when used effectively, is more efﬁcient than using multiple
READs per request.
Figure 10 shows the throughput of the three systems with 16 byte
keys and different value sizes for a read-intensive workload. For up
to 60-byte items, HERD delivers over 26 Mops, which is slightly
greater than the peak READ throughput. Up to 32-byte values,
FaRM-em also delivers high throughput. However, its throughput
declines quickly with increasing value size because the size of FaRM-
em’s READs grow rapidly (as 6 * (SV + 16)). This problem is
fundamental to the Hopscotch-based KV design which ampliﬁes
the READ size to reduce round trips. FaRM-KV quickly saturates
link bandwidths (PCIe or InﬁniBand/RoCE) with smaller items than
HERD, which conserves network bandwidth by transmitting only
essential data. Figure 10 illustrates this effect. FaRM-em saturates
the PCIe 2.0 bandwidth on Susitna with 4 byte values, and the 56
Gbps InﬁniBand bandwidth on Apt with 32 byte values. HERD
achieves high performance for up to 32 byte values on Susitna, and
303Figure 9: End-to-end throughput comparison for 48 byte key-value
items
Figure 11: End-to-end latency with 48 byte items and read-intensive
workload
Figure 12: Throughput with variable number of client processes and
different window sizes
clients until the server is saturated. When using 6 CPU cores at
the server, HERD is able to deliver 26 million requests per second
with approximately 5 µs average latency. For ﬁxed-length key-
value items, FaRM-em provides the lowest latency among the three
systems because it requires only one network round trip (unlike
Pilaf-em-OPT) and no computation at the server (unlike HERD).
For variable length values, however, FaRM’s variable length mode
requires two RTTs, yielding worse latency than HERD.
The PUT latency for all three systems (not shown) is similar be-
cause the network path traversed is the same. The measured latency
for HERD was slightly higher than that of the emulated systems
because it performed actual hash table and memory manipulation
for inserts, but this is an artifact of the performance advantage we
give Pilaf-em and FaRM-em.
5.5 Scalability
We conducted a larger experiment to understand HERD’s number-
of-clients scalability. We used one machine to run 6 server processes
and the remaining 186 machines for client processes. The experi-
ment uses 16 byte keys and 32 byte values.
Figure 12 shows the results from this experiment. HERD delivers
its maximum throughput for up to 260 client processes. With even
more clients, HERD’s throughput starts decreasing almost linearly.
The rate of decrease can be reduced by increasing the number of
outstanding requests maintained by each client, at the cost of higher
request latency. Figure 12 shows the results for two window sizes: 4
(HERD’s default) and 16. This observation suggests that the decline
is due to cache misses in RNICS, as more outstanding verbs in a
queue can reduce cache pressure. We expect this scalability limit
to be resolved with the introduction of Dynamically Connected
Transport in the new Connect-IB cards [1, 8],
Another likely scalability limit of our current HERD design is
the round-robin polling at the server for requests. With thousands
of clients, using WRITEs for inbound requests may incur too much
CPU overhead; mitigating this effect may necessitate switching to
a SEND/SEND architecture over Unreliable Datagram transport.
Figure 10: End-to-end throughput comparison with different value
sizes
60 bytes values on Apt, and is bottlenecked by the smaller PCIe PIO
bandwidth.
With large values (144 bytes on Apt, 192 on Susitna), HERD
switches to using non-inlined SENDs for responses. The outbound
throughput of large inlined messages is less than non-inlined mes-
sages because DMA outperforms PIO for large payloads (Figure 4b).