MostWanted
MostWanted
MostWanted
OvertCovert
OvertCovert
OvertCovert
OvertCovert
OvertCovert
WeirdTCP
WeirdTCP
Team Toxicity Eﬀectiveness
0.65625
0.625
0.5625
0.53125
0.46875
0.48648
0.21621
0.13513
0.12162
0.02702
0.23214
0.10714
113
114
129
105
152
113
131
123
117
127
156
105
42
40
36
34
30
36
16
10
9
2
13
6
Table 3: Top 5 most eﬀective teams per service.
2009, and 2010 editions were still experienced groups, teams
of ﬁrst-time competitors placed quite high in the ranking.
This was possible because we intentionally did not disclose
in advance to the teams the nature of these new competi-
tions. Many “veteran” teams expected a standard CTF and
were surprised to learn that this was not the case. Of course,
it is hard to keep surprising teams, as designing new compe-
titions requires a substantial amount of work. However, it
is arguable that this type of competition is inherently easier
for novice teams to participate in.
Finally, the competition generated a unique, useful dataset
that can be used to evaluate cyber situation awareness ap-
proaches. This aspect of security competitions cannot be
overemphasized, as a well-designed data-capturing frame-
work can provide a wealth of useful data to security re-
searchers.
5.2 What Did Not Work
LityaLeaks, the part of the infrastructure used to dis-
tribute the ﬁred transitions of the Petri nets, as well as
various hints and clues about services and challenges, was
an integral part of our design (and the name ﬁt in nicely
with the theme). However, using a base MediaWiki [9] in-
stallation on a virtual machine with 256 MB of RAM was
a mistake. As soon as the competition started, LityaLeaks
was brought to a crawl due to the amount of traﬃc created
by the teams.
Having LityaLeaks down was very problematic, because if
teams couldn’t see which transitions were ﬁring then they
couldn’t submit ﬂags. Eventually, a static mirror of Litya-
Leaks was brought up. Because of this, we had to change the
Petri net software on the ﬂy to update a publicly accessible
ﬁle with the transition ﬁrings instead of using LityaLeaks.
Once the change was made, at 13:30 PST, teams started
submitting ﬂags, and the rest of the competition went fairly
smoothly.
As the scoreboard is the only way for teams to under-
stand the current state of the game, making the scoreboard
accurately reﬂect the status of the competition was essen-
tial. However, each piece of the competition’s infrastructure
was developed and tested independently. Knowing that get-
ting the ﬁrewall, mothership, and Snort systems working
properly was very important, those parts of the functional-
ity were heavily tested in isolation. However, the interaction
of these systems with the scoreboard was not tested before
the competition. Thus, during the competition we discov-
ered that the reasons given to teams for being blocked on
the scoreboard were not correct, and in some instances the
connection status of some teams were incorrect. Due to one
of the developers being ill, it took us most of the competition
to completely resolve this issue. While we were ﬁxing the
issue, we communicated to teams that to test their network
connectivity, they could simply try connecting to one of the
services. In the future, we will be testing our infrastructure
as a whole, including important pieces like the scoreboard.
One issue with creating a complex and novel competition
is that some teams might not “get” the competition. This
can be on a number of levels, perhaps the team has never
heard of Petri nets or could not exploit any of the services.
This puts them at an extreme disadvantage in the rankings,
as they cannot score any points. This was the case for 33
teams. However, for the 39 teams that submitted ﬂags, a
novel competition challenged them to create new solutions
and tools, learning in the process. Ultimately, it is up to the
competition administrators to balance novelty, complexity,
and fairness.
5.3 What Worked?
Putting a backdoor into the bot VM that we distributed
to the teams was something that we implemented ﬁve hours
before the distribution of the VM. Something that we saw
as funny turned out to have serious implications. One team
came to us and said that they had an exploit to reduce ev-
ery team’s money to zero, eﬀectively removing everyone else
from the competition. Using the backdoor, they could bribe
the Litya administrators as the team’s bot, thus draining all
of the team’s money. We asked them not to do this, as it was
unsporting to completely shut oﬀ most team’s access to the
services, and ﬁxed this avenue of attack. We also alerted the
teams to the existence of a backdoor on their VMs. Later in
the competition, a team came to us complaining that their
points kept decreasing. Looking into it, a team was exploit-
ing a service, and submitting all the inactive ﬂags (worth
negative points) through another team’s compromised bot.
The team that this happened to came in last place (with
-3300 points).
The backdoor provided some interesting (and funny) sit-
uations, however it came at a price. The last place team
felt that this was an unsporting thing to do and were rightly
upset over their last-place standing. We ruled that, since
we had given notice about the backdoor, and given the ex-
tremely easy ﬁx (ﬁlter the traﬃc from other teams), the out-
come was acceptable. However, this situation did highlight
an issue that these kind of “easter eggs” can produce: while
it may be exciting and interesting for the teams who discover
them, the more inexperienced teams who are not looking for
them and/or can’t ﬁnd them are put at a disadvantage. This
just increases the gap between the experienced and inexpe-
rienced.
6. CONCLUSIONS
Live cyber-security exercises are a powerful educational
tool. The main drawback of these exercises is that they
require substantial resources to be designed, implemented,
and executed. It is therefore desirable that these exercises
provide long-lasting byproducts for others to use for secu-
rity research. In this paper, we presented a unique, novel
design for a live educational cyber-security exercise. This
design was implemented and a competition involving almost
a thousand world-wide students was carried out in December
2010. We discussed the lessons learned, and we presented
the dataset we collected, which we believe is the ﬁrst public
dataset focused on Cyber Situational Awareness. We hope
that this dataset will be useful to other researchers in this
increasingly popular ﬁeld and that future security exercises
will yield interesting datasets.
7. REFERENCES
[1] T. Augustine and R. Dodge. Cyber Defense Exercise:
Meeting Learning Objectives thru Competition. In
Proceedings of the Colloquium for Information
Systems Security Education (CISSE), 2006.
[2] N. Childers, B. Boe, L. Cavallaro, L. Cavedon,
M. Cova, M. Egele, and G. Vigna. Organizing Large
Scale Hacking Competitions. In Proceedings of the
Conference on Detection of Intrusions and Malware
and Vulnerability Assessment (DIMVA), Bonn,
Germany, July 2010.
[3] W. Clark. The Gantt chart: A working tool of
management. New York: Ronald Press, 1922.
[4] C. Cowan, S. Arnold, S. Beattie, C. Wright, and
J. Viega. Defcon Capture the Flag: defending
vulnerable code from intense attack. In Proceedings of
the DARPA Information Survivability Conference and
Exposition, April 2003.
[5] A. D’Amico, L. Buchanan, J. Goodall, and
P. Walczak. Mission Impact of Cyber Events:
Scenarios and Ontology to Express the Relationships
between Cyber Assets, Missions and Users. In
Proceedings of the International Conference on
Information Warfare and Security, Dayton, Ohio,
April 2010.
[6] D. R. Hipp. Sqlite. http://www.sqlite.org/, 2010.
[7] Justin.tv. http://justin.tv/.
[8] S. Liang. Java Native Interface: Programmer’s Guide
and Reference. Addison-Wesley Longman Publishing
Co., Inc., Boston, MA, USA, 1st edition, 1999.
[9] Mediawiki. http://www.mediawiki.org/.
[10] B. Mullins, T. Lacey, R.Mills, J. Trechter, and
S. Bass. How the Cyber Defense Exercise Shaped an
Information-Assurance Curriculum. IEEE Security &
Privacy, 5(5), 2007.
[11] J. Peterson. Petri Nets. ACM Computing Surveys,
9(3), September 1977.
[12] L. Pimenidis. Cipher: capture the ﬂag.
http://www.cipher-ctf.org/, 2008.
[13] Pwn2own 2009 at cansecwest. http://dvlabs.
tippingpoint.com/blog/2009/02/25/pwn2own-2009,
March 2009.
[14] W. Schepens, D. Ragsdale, and J. Surdu. The Cyber
Defense Exercise: An Evaluation of the Eﬀectiveness
of Information Assurance Education. Black Hat
Federal, 2003.
[15] Simpp. 3vilsh3ll.c. http://packetstormsecurity.
org/files/view/64687/3vilSh3ll.c.
[16] Snort. http://www.snort.org/.
[17] B. Stone-Gross, R. Abman, R. Kemmerer, C. Kruegel,
D. Steigerwald, and G. Vigna. The Underground
Economy of Fake Antivirus Software.
[18] The HackerDom Group. The ructf challenge.
http://www.ructf.org, 2009.
[19] G. Vigna. Teaching Hands-On Network Security:
Testbeds and Live Exercises. Journal of Information
Warfare, 3(2):8–25, 2003.
[20] G. Vigna. Teaching Network Security Through Live
Exercises. In C. Irvine and H. Armstrong, editors,
Proceedings of the Third Annual World Conference on
Information Security Education (WISE 3), pages
3–18, Monterey, CA, June 2003. Kluwer Academic
Publishers.
[21] VMware. http://www.vmware.com/.
WeirdTCP was a C service that acted as a ﬁle server with
a trust relationship with a speciﬁc IP address. A blind
TCP spooﬁng attack against the service pretending to be
the trusted IP address was required to ﬁnd the key. How-
ever, due to the VPN technology we were using, packets
could not be spoofed. A custom IP protocol RFC was given
to the teams, which introduced an IP option that could be
used to overwrite the source address of an IP packet. Thus
an attacker had to use the IP option to spoof the trusted
IP address, and, in addition, perform a sequence number
guessing attack, in order to provide the correct acknowledg-
ment number during the TCP handshake. Once the TCP
connection was established, the attacker received the ﬂag.
MostWanted was a Python service with a SQLite [6] back-
end. The service hosted mugshots of various wanted “crimi-
nals,” and allowed a user to create or view mugshots. Most-
Wanted had a stored SQL-injection vulnerability, which an
attacker had to exploit to access the ﬂag.
OvertCovert was a C-based service that allowed a user
to store and access encrypted data. An attacker had to
ﬁrst exploit a printf vulnerability (which disallowed %n) to
extract the encryption key. Then, an oﬀ-by-one error was
used to access the encrypted ﬂag. Using the key previously
obtained, the attacker could decrypt the ﬂag and exploit the
service.
APPENDIX
A. VULNERABLE SERVICES
A brief description of the 10 services in the iCTF and the
vulnerabilities associated with it follows.
LityaBook was a social networking website, similar to Face-
book. By creating an underage girl proﬁle, the attacker
would cause President Bironulesk to visit their proﬁle. They
could then use a Cross-Site Scripting attack to steal Presi-
dent Bironulesk’s browser’s cookie, which contained the ﬂag.
LityaBook also had a session ﬁxation vulnerability. The
authentication cookie contained the MD5 of the session ID.
Therefore, an attacker could lure a victim to log in with
a speciﬁc session ID, allowing an attacker to impersonate
the victim. This vulnerability could have been exploited by
using another website, LityaHot.
LityaHot was a website where young models posted links
to their pictures, waiting for casting agents to contact them.
Periodically, a member of President Bironulesk’s staﬀ, Fem-
ily Edeo, visited this site, clicking on links people had posted.
If the link was a LityaBook page, he logged in to check the
pictures. Thus an attacker could post a link on LityaHot,
leveraging the session ﬁxation vulnerability to log into Litya-
Book as Edeo and obtain the ﬂag.
icbmd was the ﬁrst iCTF service with perceptible eﬀects on
the real world. A USB foam rocket launcher was connected
to a control program, pointing in the direction of a phys-
ical target. A time-sharing mechanism was used to share
the missile launcher amongst the teams. Each team had a
visual clue of where the launcher was aiming, via a web-
cam mounted on the missile launcher with a live streamed
video to the Justin.tv on-line video streaming service [7].
The team currently controlling the missile launcher could
exclusively connect to the control and move the launcher’s
turret. An encoded version of the launch code was leaked to
the teams. After deciphering the code, the teams were able
to launch a missile. Once a team successfully hit the target,
the ﬂag was sent to them.
StormLog was a web application that displayed log ﬁles
generated by a fake botnet called “Storm.” This service had
a directory traversal vulnerability which allowed an attacker
to download a copy of the cgi-bin program. An attacker had
to exploit an oﬀ-by-one overﬂow in the cgi-bin program to
execute arbitrary code and obtain the ﬂag.
StolenCC was a web service that displayed text ﬁles con-
taining credit card numbers. The cgi-bin program was writ-
ten in Perl and contained a directory traversal vulnerability.
By inserting a null character into the filename parameter,
an attacker could bypass the program’s sanity checking and
open any ﬁle. Then, an attacker could use additional func-
tionality of Perl’s open to execute any command, ﬁnding and
displaying the ﬂag.
SecureJava was a web service that used a Java applet to
perform authentication. An attacker needed to get past the
authentication to ﬁnd the ﬂag. This involved reverse en-
gineering the encryption algorithm. Once understood, the
attacker leveraged a ﬂaw in the encryption algorithm to steal
the ﬂag.
IdreamOfJeannie was a Java service that collected credit
card information. Even though the bulk of the service was
written in Java, JNI [8] was used to include a function writ-
ten in C, which contained an oﬀ-by-one error. The attacker
could utilize the oﬀ-by-one error to obtain the ﬂag.