title:GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative
Models
author:Dingfan Chen and
Ning Yu and
Yang Zhang and
Mario Fritz
GAN-Leaks: A Taxonomy of Membership Inference Attacks
against Generative Models
Dingfan Chen
CISPA Helmholtz Center for Information Security
Ning Yu
University of Maryland, College Park
Max Planck Institute for Informatics
Yang Zhang
CISPA Helmholtz Center for Information Security
Mario Fritz
CISPA Helmholtz Center for Information Security
ABSTRACT
Deep learning has achieved overwhelming success, spanning from
discriminative models to generative models. In particular, deep
generative models have facilitated a new level of performance in
a myriad of areas, ranging from media manipulation to sanitized
dataset generation. Despite the great success, the potential risks of
privacy breach caused by generative models have not been analyzed
systematically. In this paper, we focus on membership inference at-
tack against deep generative models that reveals information about
the training data used for victim models. Specifically, we present
the first taxonomy of membership inference attacks, encompassing
not only existing attacks but also our novel ones. In addition, we
propose the first generic attack model that can be instantiated in a
large range of settings and is applicable to various kinds of deep
generative models. Moreover, we provide a theoretically grounded
attack calibration technique, which consistently boosts the attack
performance in all cases, across different attack settings, data modal-
ities, and training configurations. We complement the systematic
analysis of attack performance by a comprehensive experimental
study, that investigates the effectiveness of various attacks w.r.t.
model type and training configurations, over three diverse applica-
tion scenarios (i.e., images, medical data, and location data).
CCS CONCEPTS
• Computing methodologies → Machine learning; • Security
and privacy;
KEYWORDS
Membership inference attacks; deep learning; generative models;
privacy-preserving machine learning
ACM Reference Format:
Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. 2020. GAN-Leaks: A
Taxonomy of Membership Inference Attacks against Generative Models. In
Proceedings of the 2020 ACM SIGSAC Conference on Computer and Commu-
nications Security (CCS ’20), November 9–13, 2020, Virtual Event, USA. ACM,
New York, NY, USA, 20 pages. https://doi.org/10.1145/3372297.3417238
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’20, November 9–13, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7089-9/20/11...$15.00
https://doi.org/10.1145/3372297.3417238
1 INTRODUCTION
Over the last few years, two categories of deep learning techniques
have made tremendous progress. The discriminative model has
been successfully adopted in various prediction tasks, such as im-
age classification [26, 42, 61, 62] and speech recognition [21, 30].
The generative model, on the other hand, has also gained increas-
ing attention and has delivered appealing applications including
photorealistic image synthesis [20, 44, 52, 70], text and sound gener-
ation [4, 49, 64, 65], sanitized dataset generation [2, 7, 35, 66, 73], etc.
Most of such applications are supported by deep generative models,
e.g., the generative adversarial networks (GANs) [3, 10, 20, 23, 36–
38, 54, 59, 71] and variational autoencoder (VAE) [41, 55, 67].
In line with the growing trend of deep learning in real busi-
ness, many companies collect and process customer data which
is then used to develop deep learning models for commercial use.
However, data privacy violations frequently happened due to data
misuse with an inappropriate legal basis, e.g., the misuse of Na-
tional Health Service data in the DeepMind project.1 Data privacy
can also be challenged by malicious users who intend to infer the
original training data. The resulting privacy breach would raise
serious issues as training data contains sensitive attributes such as
diagnosis and income. One such attack is membership inference
attack (MIA) [5, 18, 24, 25, 58, 60] which aims to identify if a data
record was used to train a machine learning model. Overfitting is
the major cause for the feasibility of MIA, as the learned model
tends to memorize training inputs and perform better on them.
While numerous literature is dedicated to MIA against discrimi-
native models [33, 45, 48, 50, 58, 60, 68], the attack on generative
models has not received equal attention, despite its practical impor-
tance. For instance, GANs have been applied to health record data
and medical images [12, 19, 69] whose membership is sensitive as
it may reveal a patient’s disease history. Moreover, recent works
in privacy preserving data sharing [2, 7, 11, 35, 66, 73] propose to
impose (membership) privacy constraints during GANs training for
sanitized data generation. Understanding the membership privacy
leakage under a practical threat model helps shed light on future
research in this area.
Nevertheless, this is a highly challenging task from the adversary
side. Unlike discriminative models, the victim generative models
do not directly provide confidence values about the overfitting of
data records, and thus leave little clues for conducting membership
1
https://news.sky.com/story/google-received-1-6-million-nhs-patients-data-
on-an-inappropriate-legal-basis-10879142
inference. In addition, current GAN models inevitably underrepre-
sent certain data samples, i.e., encounter mode dropping and mode
collapse, which pose additional difficulty to the attacker.
Unfortunately, none of the existing works [25, 29] provides a
generic attack applicable to varying types of generative models. Nor
do they report a complete and practical analysis of MIA against deep
generative models. For example, Hayes et al. [25] do not consider the
realistic situation where the GAN’s discriminator is not accessible
but only the generator is released. Hilprecht et al. [29] investigate
only on small-scale image datasets and do not involve white-box
attack against GANs. This motivates our contributions towards a
simple and generic approach as well as a more systematic analysis.
In general, we make the following contributions in the paper.
Taxonomy of Membership Inference Attacks against Deep
Generative Models: We conduct a pioneering study to categorize
attack settings against deep generative models. Given the increas-
ing order of the amount of knowledge about a victim model, the
settings are benchmarked as (1) full black-box generator, (2) partial
black-box generator, (3) white-box generator, and (4) accessible dis-
criminator (full model). In particular, two of the settings, the partial
black-box and white-box settings, are of practical value but have
not been explored by previous works. We then establish the first
taxonomy that comprises the existing and our proposed attacks.
See Section 4, Table 1, and Figure 1 for details.
Generic Attack Model and its Novel Instantiated Variants: We
propose a simple and generic attack model (Section 5.1) applica-
ble to all the practical settings and various types of deep gener-
ative models. More specifically, our generic attack model can be
instantiated to a preliminary low-skill attack for the full black-box
setting (Section 5.2), a novel black-box optimization-based attack
variant in the partial black-box (Section 5.3), as well as a novel
quasi-Newton optimization-based variant in the white-box settings
(Section 5.4). The consistent effectiveness of our attack model ex-
hibited in all of the aforementioned settings bridges the assumption
gap and performance gap between the full black-box attacks and
discriminator-accessible attack in previous study [25, 29] through
a complete performance spectrum (Section 6.7).
Novel Attack Calibration Technique: To further improve the
effectiveness of our attack model, we adjust our approach to each
query sample and propose our novel attack calibration technique,
which is naturally incorporated in our generic attack framework.
Moreover, we prove its near-optimality under a Bayesian perspec-
tive. Through extensive experiments, we validate that our attack
calibration technique boosts the attack performance noticeably in
all cases, across different attack settings, data modalities, and train-
ing configurations. See Section 5.6 for detailed explanation and
Section 6.6 for experiment results.
Systematic Analysis in Each Setting: We progressively investi-
gate attacks in each setting in the increasing order of amount of
knowledge to adversary. See Section 6.3 to Section 6.5 for detailed
elaboration. In each setting, our research spans several orthogo-
nal dimensions including three datasets with diverse modalities
(Section 6.1), five victim GAN models that were the state-of-the-art
at their release time (Section 6.1), two analysis study w.r.t. GAN
training configuration (Section 6.2), attack performance gains in-
troduced by attack calibration (Section 5.6 and Section 6.6) and
differential private defense (Section 6.8).
2 RELATED WORK
Generative Models: Generative models are designed for approxi-
mating the probability distribution of the real data. In general, this
is done by defining a parametric family of densities and finding
the optimal parameters that either maximize the real data likeli-
hood or minimize the divergence between generated and real data
distribution. Recent generative models exploit the representation
power of deep neural networks for constituting an exceptionally
rich parametric family, resulting in tremendous success in modeling
high-dimensional data distribution. In this work, we investigate
the most widely used deep generative models, namely the genera-
tive adversarial networks (GANs) [3, 10, 20, 23, 36–38, 54, 59, 71]
and variational autoencoders (VAEs) [13, 14, 40]. Briefly speaking,
GANs are trained to minimize the divergence between the gener-
ated and real data distribution, while VAEs maximize a lower bound
of the real data log-likelihood.
Membership Inference Attacks (MIAs): Shokri et al. [60] spec-
ifies the first MIA against discriminative models in the black-box
setting, where an attack has access to the victim model’s full re-
sponse (i.e., confidence scores for all classes) for a given input query.
They propose to train shadow models that imitate the behavior of
the victim model, which generates data to train an attacker model.
Hayes et al. [25] consider MIA against GANs and also propose to
retrain a shadow model of the victim model in the black-box case.
They then check the discriminator’s output scores to query inputs
and set a threshold such that all the query inputs with scores larger
than the threshold will be classified as in the training set.
Another concurrent study by Hilprecht et al. [29] investigates
MIA against both GANs and VAEs. For VAEs, they assume the
accessibility of the full model and propose to threshold the 𝐿2
reconstruction error; For GANs, they only consider the full black-
box setting. Their black-box attack is similar to ours in spirit, as
they count the number of generated samples that are inside an
𝜖-ball of the query, while we exploit the reconstruction distance
instead.
Differential Privacy (DP): Differential privacy [17] is designed
to protect the membership privacy of individual samples and is
by constructing a defense mechanism against MIA. Recent works
propose to train GAN models with differential privacy constraint [2,
7, 11, 35, 66, 73] and publicize the DP-trained models instead of
the raw data, which allows sharing sensitive data while preserving
privacy. The differential privacy constraint is fulfilled by replacing
the regular stochastic gradient descent with differential private
stochastic gradient descent (DP-SGD) [1], which injects calibrated
noise in training gradients. As a result, it perturbs data-related
objective functions and mitigates inference attacks.
3 BACKGROUND
3.1 Generative Model
Generative Adversarial Networks (GANs): GANs consist of two
neural network modules, a generator 𝐺 and a discriminator 𝐷,
max
𝜃𝐷
which are trained simultaneously in an adversarial manner. The
generator takes random noise 𝑧 (latent code) as input and generates
samples that approximate the training data distribution, while the
discriminator receives samples from both the generator and train-
ing dataset and is trained to differentiate the two sources. During
training, these two modules compete and evolve, such that the gen-
erator learns to generate more and more realistic samples aiming
at fooling the discriminator, while the discriminator learns to tell
the two sources apart more accurately. The training objective can
be formulated as
E𝑥∼𝑃data[log(𝐷𝜃𝐷 (𝑥))] + E𝑧∼𝑃𝑧 [log(1 − 𝐷𝜃𝐷 (𝐺𝜃𝐺 (𝑧)))]
min
𝜃𝐺
where 𝜃𝐺, 𝜃𝐷 denote the parameters of the generator and the dis-
criminator. 𝑃data is the real data distribution, while the 𝑃𝑧 is the prior
distribution of the latent code. The first term in the objective forces
the discriminator to output high score given real data sample. The
second term makes discriminator output low score on generated
samples, while the generator is trained to maximize the discrim-
inator output score. Once the training is done, the discriminator
is no longer useful and will normally be discarded. The genera-
tor will receive new latent code samples 𝑧 drawn from the known
prior distribution (normally Gaussian) and output the synthetic
data samples, which will be collected and used for the downstream
task.
Variational Autoencoder (VAE): VAE is another widely used gen-
erative framework [41, 55, 67] consists of an encoder and a decoder,
which are cascaded to reconstruct data with pre-defined similarity
metrics, e.g. 𝐿1/𝐿2 loss. The encoder maps data into a latent space,
while the decoder maps the encoded latent representation back to
the data space. The VAE objective is composed of the reconstruction
error and the prior regularization over the latent code distribution.
Formally,
−E𝑞𝜙 (𝑧|𝑥) [𝑝𝜃 (𝑥|𝑧)] + 𝐾𝐿(𝑞𝜙(𝑧|𝑥)∥𝑃𝑧)
min
𝜃,𝜙
where 𝑧 denotes the latent code, 𝑥 denotes the input data, 𝑞𝜙(𝑧|𝑥)
is the probabilistic encoder parameterized by 𝜙 which is introduced
to approximate the intractable true posterior, 𝑝𝜃 (𝑥|𝑧) represents
the probabilistic decoder parameterized by 𝜃, and 𝐾𝐿(·∥·) denotes
the KL divergence. In practice, 𝑞𝜙(𝑧|𝑥) is always constrained to be
uni-modal Gaussian and 𝑧 is sampled via the reparameterization
trick, which results in a closed-form derivation of the second term.
Hybrid Model: GANs often suffer from mode collapse and mode
dropping issues, i.e., failing to generate appearances relevant to
some training samples (low recall), due to the lack of explicit super-
vison (e.g. data reconstruction) for promoting data mode coverage.
VAEs, on the contrary, attain better data coverage but often lack
flexible generation capability (low precision). Therefore, a hybrid
model, VAEGAN [8, 43], is proposed to jointly train a VAE and a
GAN, where the VAE decoder and the GAN generator are collapsed
into one by sharing trainable parameters. The GAN discriminator
is trained to complement the low-level 𝐿1 or 𝐿2 reconstruction loss,
in order to improve the generation quality of fine-grained details.
3.2 Membership Inference
We formulate the membership inference attack as a binary classifi-
cation task where the attacker aims to classify whether a sample
Dis-
criminator
Latent Gen-
code
erator
×
×
×
✓
✓
✓
■
■
■
■
□
□
[25] full black-box
[29] full black-box
Our full black-box (Section 5.2)
Our partial black-box (Section 5.3)
Our white-box (Section 5.4)
[25] accessible discriminator (full model)
Table 1: Taxonomy of attack settings against GANs over the
previous work and ours. (×: without access; ✓: with access;
■: black-box; □: white-box).
×
×
×
×
×
✓
Figure 1: Taxonomy of attack models against GANs. Gen:
generator; Dis: discriminator; z: latent code input to Gen.
𝑥 has been used to train a victim generative model. Formally, we
define
A : (𝑥,M(𝜃)) → {0, 1}
where the attack model A output 1 if the attacker infers that the
query sample 𝑥 is included in the training set, and 0 otherwise.
𝜃 denotes the victim model parameters while M represents the
general model publishing mechanism, i.e., type of access available
to the attacker. For example, the M is an identity function for
the white-box access case and can be the inference function for
the black-box case. For simplicity, we may omit the dependence
on M if the type of access is irrelevant for illustration. With a
Bayesian perspective [57], the optimal attacker aims to compute
the probability 𝑃(𝑥 ∈ 𝐷train|𝑥, 𝜃) and predict the query sample to
be in the training set if the log-likelihood ratio is non-negative, i.e.
the query sample is more likely to be contained in the training set