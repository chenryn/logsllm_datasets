# Dynamic Provable Data Possession

## Authors
- C. Chris Erway, Brown University
- Alptekin Küpçü, Brown University
- Charalampos Papamanthou, Brown University
- Roberto Tamassia, Brown University

## Abstract
We address the problem of efficiently proving the integrity of data stored on untrusted servers. In the Provable Data Possession (PDP) model, a client preprocesses the data and sends it to an untrusted server for storage, while retaining a small amount of metadata. The client can later request the server to prove that the stored data has not been tampered with or deleted, without needing to download the actual data. However, the original PDP scheme is limited to static or append-only files.

In this paper, we introduce a definitional framework and efficient constructions for Dynamic Provable Data Possession (DPDP), which extends the PDP model to support provable updates to stored data. Our approach uses a new version of authenticated dictionaries based on rank information. The cost of dynamic updates is a performance change from O(1) to O(log n) (or O(n^ε log n)), for a file consisting of n blocks, while maintaining the same (or better, respectively) probability of misbehavior detection. Our experiments show that this slowdown is minimal in practice (e.g., 415KB proof size and 30ms computational overhead for a 1GB file). We also demonstrate how our DPDP scheme can be applied to outsourced file systems and version control systems (e.g., CVS).

## Categories and Subject Descriptors
- C.2.4 [Communication Networks]: Distributed Systems
- E.1 [Data Structures]
- H.3.4 [Information Storage and Retrieval]: Systems and Software

## General Terms
- Security, Algorithms, Theory, Verification

## Keywords
- Authentication, Provable Data Possession, Skip List, Integrity Checking, Outsourced Storage, Proof of Retrievability, Authenticated Data Structures

## 1. Introduction
In cloud storage systems, the server storing the client's data is often untrusted. Therefore, users need to verify the integrity of their data. Downloading all stored data to validate its integrity is inefficient, even with authenticated data structures [31].

Ateniese et al. [2] formalized the Provable Data Possession (PDP) model. In this model, the client preprocesses the data and produces metadata for verification. The preprocessed data is then sent to an untrusted server, and the client may delete the local copy. The client retains some (possibly secret) information to check the server's responses. The server proves data integrity by responding to challenges from the client. These schemes provide probabilistic guarantees of possession, where the client checks a random subset of stored blocks with each challenge.

However, PDP and related schemes [2, 7, 13, 30] are limited to static, archival storage, where the file is outsourced and never changes. While this fits some scenarios (e.g., libraries and scientific datasets), it is crucial to consider the dynamic case, where the client updates the outsourced data—by inserting, modifying, or deleting stored blocks or files—while maintaining data possession guarantees. Such a dynamic PDP scheme is essential in practical cloud computing systems for file storage [14, 17], database services [18], and peer-to-peer storage [15, 19].

In this paper, we introduce a framework and efficient constructions for Dynamic Provable Data Possession (DPDP), which extends the PDP model to support provable updates on the stored data. For a file F consisting of n blocks, we define an update as either the insertion of a new block (anywhere in the file, not just appended), modification of an existing block, or deletion of any block. Our update operation covers the most general form of modifications a client may wish to perform on a file.

Our DPDP solution is based on a new variant of authenticated dictionaries using rank information to organize dictionary entries. This allows us to support efficient authenticated operations on files at the block level, such as authenticated insert and delete. We prove the security of our constructions using standard assumptions.

We also show how to extend our construction to support data possession guarantees for hierarchical file systems and file data itself. To the best of our knowledge, this is the first construction of a provable storage system that enables efficient proofs of a whole file system, allowing verification at different levels for different users (e.g., each user can verify their own home directory) without downloading the entire data. Our scheme also yields a provable outsourced versioning system (e.g., CVS), which is evaluated in Section 8 using traces of CVS repositories of three well-known projects.

### 1.1 Contributions
The main contributions of this work are:
1. A formal framework for dynamic provable data possession (DPDP).
2. The first efficient fully dynamic PDP solution.
3. A rank-based authenticated dictionary built over a skip list, yielding a DPDP scheme with logarithmic computation and communication and the same detection probability as the original PDP scheme (DPDP I).
4. An alternative construction (Section 6) of a rank-based authenticated dictionary using an RSA tree [25]. This construction results in a DPDP scheme with improved detection probability but higher server computation (DPDP II).
5. Practical applications of our DPDP constructions to outsourced file systems and versioning systems (e.g., CVS, with variable block size support).
6. An experimental evaluation of our skip list-based scheme.

### 1.2 Performance
Denote with n the number of blocks. The server computation, i.e., the time taken by the server to process an update or to compute a proof for a block, is O(log n) for DPDP I and O(n^ε log n) for DPDP II. The client computation, i.e., the time taken by the client to verify a proof returned by the server, is O(log n) for both schemes. The communication complexity, i.e., the size of the proof returned by the server to the client, is O(log n) for both schemes. The client storage, i.e., the size of the metadata stored locally by the client, is O(1) for both schemes. Finally, the probability of detection, i.e., the probability of detecting server misbehavior, is 1 − (1 − f)^C for DPDP I and 1 − (1 − f)^Ω(log n) for DPDP II, for fixed logarithmic communication complexity, where f is the ratio of corrupted blocks and C is a constant independent of n.

We observe that for DPDP I, a dynamic Merkle tree (e.g., [16, 20]) could achieve the same asymptotic performance. We chose the skip list due to its simple implementation and the fact that algorithms for updates in the two-party model (where clients can access only a logarithmic-sized portion of the data structure) have been studied in detail for authenticated skip lists [24] but not for Merkle trees.

### 1.3 Related Work
The PDP scheme by Ateniese et al. [2] provides an optimal protocol for the static case with O(1) costs for all complexity measures. They review previous work on protocols fitting their model but find these approaches lacking: they require expensive server computation or communication over the entire file [10, 22], linear storage for the client [29], or do not provide security guarantees for data possession [28]. Using [2] in a dynamic scenario is insecure due to replay attacks. As observed in [8], to avoid replay attacks, an authenticated tree structure with logarithmic costs must be employed, making constant costs infeasible in a dynamic scenario.

Juels and Kaliski present Proofs of Retrievability (PORs) [13], focusing on static archival storage of large files. Their scheme’s effectiveness relies on preprocessing steps the client conducts before sending a file F to the server: "sentinel" blocks are randomly inserted to detect corruption, F is encrypted to hide these sentinels, and error-correcting codes are used to recover from corruption. Unfortunately, these operations prevent any efficient extension to support updates, beyond simply replacing F with a new file F'. Furthermore, the number of queries a client can perform is limited and fixed a priori. Shacham and Waters have an improved version of this protocol called Compact POR [30], but their solution is also static (see [7] for a summary of POR schemes and related trade-offs).

Simultaneously with our work, Ateniese et al. developed a dynamic PDP solution called Scalable PDP [3]. Their idea is to generate all future challenges during setup and store precomputed answers as metadata (at the client or at the server in an authenticated and encrypted manner). This approach limits the number of updates and challenges a client can perform, and only append-type insertions are possible. Each update requires re-creating all remaining challenges, which is problematic for large files. Under these limitations, they provide a protocol with optimal asymptotic complexity O(1) in all complexity measures, giving the same probabilistic guarantees as our scheme. Their work is in the random oracle model, whereas our scheme is provably secure in the standard model (see Table 1 for a full comparison).

Finally, our work is closely related to memory checking, for which lower bounds are presented in [8, 21]. Specifically, in [8], it is proved that all non-adaptive and deterministic checkers have read and write query complexity summing up to Ω(log n / log log n) (necessary for sublinear client storage), justifying the O(log n) cost in our scheme. For schemes based on cryptographic hashing, an Ω(log n) lower bound on the proof size has been shown [6, 32]. Related bounds for other primitives have been shown by Blum et al. [4].

## 2. Model
We build on the PDP definitions from [2]. We begin by introducing a general DPDP scheme and then show how the original PDP model is consistent with this definition.

### Definition 1 (DPDP Scheme)
In a DPDP scheme, there are two parties: the client and the untrusted server. The client wants to off-load her files to the untrusted server. A complete definition of a DPDP scheme should describe the following (possibly randomized) efficient procedures:

- **KeyGen(1k) → {sk, pk}**: A probabilistic algorithm run by the client. It takes a security parameter as input and outputs a secret key sk and a public key pk. The client stores the secret and public keys and sends the public key to the server.
- **PrepareUpdate(sk, pk, F, info, Mc) → {e(F), e(info), e(M)}**: An algorithm run by the client to prepare (a part of) the file for untrusted storage. It takes secret and public keys, (a part of) the file F with the definition info of the update to be performed (e.g., full re-write, modify block i, delete block i, add a block after block i, etc.), and the previous metadata Mc. The output is an "encoded" version of (a part of) the file e(F) (e.g., by adding randomness, adding sentinels, encrypting for confidentiality, etc.), along with the information e(info) about the update (changed to fit the encoded version), and the new metadata e(M). The client sends e(F), e(info), e(M) to the server.
- **PerformUpdate(pk, Fi−1, Mi−1, e(F), e(info), e(M)) → {Fi, Mi, M'c}**: An algorithm run by the server in response to an update request from the client. The input contains the public key pk, the previous version of the file Fi−1, the metadata Mi−1, and the client-provided values e(F), e(info), e(M) (produced by PrepareUpdate). The output is the new version of the file Fi and the metadata Mi, along with the metadata to be sent to the client M'c.