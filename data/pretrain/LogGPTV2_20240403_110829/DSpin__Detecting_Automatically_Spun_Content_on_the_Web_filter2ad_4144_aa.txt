title:DSpin: Detecting Automatically Spun Content on the Web
author:Qing Zhang and
David Y. Wang and
Geoffrey M. Voelker
DSpin: Detecting Automatically Spun Content
on the Web
Qing Zhang
University of California, San Diego
PI:EMAIL
David Y. Wang
Geoffrey M. Voelker
University of California, San Diego
University of California, San Diego
PI:EMAIL
PI:EMAIL
Abstract—Web spam is an abusive search engine optimization
technique that artiﬁcially boosts the search result rank of pages
promoted in the spam content. A popular form of Web spam
today relies upon automated spinning to avoid duplicate detection.
Spinning replaces words or phrases in an input article to
create new versions with vaguely similar meaning but sufﬁciently
different appearance to avoid plagiarism detectors. With just a
few clicks, spammers can use automated tools to spin a selected
article thousands of times and then use posting services via
proxies to spam the spun content on hundreds of target sites.
The goal of this paper is to develop effective techniques to detect
automatically spun content on the Web. Our approach is directly
tied to the underlying mechanism used by automated spinning
tools: we use a technique based upon immutables, words or
phrases that spinning tools do not modify when generating spun
content. We implement this immutable method in a tool called
DSpin, which identiﬁes automatically spun Web articles. We then
apply DSpin to two data sets of crawled articles to study the extent
to which spammers use automated spinning to create and post
content, as well as their spamming behavior.
I.
INTRODUCTION
Web sites fervently compete for trafﬁc. Since many users
visit sites as a result of searching, sites naturally compete for
high rank in search results using a variety of search engine
optimization (SEO) techniques believed to impact how search
engines rank pages. While there are many valid, recommended
methods for performing SEO, from improving content
to
improving performance, some “black hat” methods use abusive
means to gain advantage. One increasingly popular black hat
technique is generating and posting Web spam using spinning.
Spinning replaces words or restructures original content to
create new versions with similar meaning but different appear-
ance. In effect, spinning is yet another means for disguising
plagiarized content as original and unique. However, the spun
content itself does not have to be polished, just sufﬁciently
different to evade detection as duplicate content. The most
common use of spinning in SEO is to create many different
versions of a single seed article, and to post those versions
on multiple Web sites with links pointing to a site being
Permission(cid:1) to(cid:1) freely(cid:1) reproduce(cid:1) all(cid:1) or(cid:1) part(cid:1) of(cid:1) this(cid:1) paper(cid:1) for(cid:1) noncommercial(cid:1)
purposes(cid:1)is(cid:1)granted(cid:1)provided(cid:1)that(cid:1)copies(cid:1)bear(cid:1)this(cid:1)notice(cid:1)and(cid:1)the(cid:1)full(cid:1)citation(cid:1)
on(cid:1)the(cid:1)ﬁrst(cid:1)page.(cid:1)Reproduction(cid:1)for(cid:1)commercial(cid:1)purposes(cid:1)is(cid:1)strictly(cid:1)prohibited(cid:1)
without(cid:1)the(cid:1)prior(cid:1)written(cid:1)consent(cid:1)of(cid:1)the(cid:1)Internet(cid:1)Society,(cid:1)the(cid:1)ﬁrst-named(cid:1)author(cid:1)
(for(cid:1) reproduction(cid:1) of(cid:1) an(cid:1) entire(cid:1) paper(cid:1) only),(cid:1) and(cid:1) the(cid:1) author’s(cid:1) employer(cid:1) if(cid:1) the(cid:1)
paper(cid:1)was(cid:1)prepared(cid:1)within(cid:1)the(cid:1)scope(cid:1)of(cid:1)employment.
NDSS(cid:1)’14,(cid:1)23-26(cid:1)February(cid:1)2014,(cid:1)San(cid:1)Diego,(cid:1)CA,(cid:1)USA
Copyright(cid:1)2014(cid:1)Internet(cid:1)Society,(cid:1)ISBN(cid:1)1-891562-35-5
http://dx.doi.org/(cid:18)(cid:17)(cid:15)(cid:18)(cid:21)(cid:24)(cid:19)(cid:19)(cid:16)(cid:79)(cid:69)(cid:84)(cid:84)(cid:15)(cid:19)(cid:17)(cid:18)(cid:21)(cid:15)(cid:19)(cid:20)(cid:17)(cid:17)(cid:21)
promoted. The belief is that these “backlinks”, as well as
keywords, will increase the page rank of the promoted sites
in Web search results, and consequently attract more trafﬁc to
the promoted sites. Search engines seek to identify duplicate
pages with artiﬁcial backlinks to penalize them in the page
rank calculation, but spinning evades detection by producing
artiﬁcial content that masquerades as original.
There are two ways content can be spun. The ﬁrst is to
employ humans to spin the content, as exempliﬁed by the
many spinning jobs listed on pay-for-hire Web sites such as
Fiverr and Freelancer [26]. Although articles spun by humans
might have better quality, an alternative, cheaper approach is
to use automated spinning tools. For example, a typical job on
Freelancer might pay as much as $2–$8 per hour for manually
spinning articles [13], whereas others advertise automatically
spinning and posting 500 articles for $5 [12]. Or spammers can
simply purchase and use the tools themselves using popular
tools such as XRumer [30], SEnuke, and The Best Spinner.
For example, The Best Spinner sells for $77 a year. These
article spinners take an original article as input, and replace
words and phrases in the article with synonyms to evade copy
detection; some can even rearrange sentences. Spammers using
automated spinning tools can select an existing article and spin
it hundreds or thousands of times with just a few clicks, and
then use posting services via proxies to spam the spun content
on hundreds of target sites (also available for purchase) all over
the Web. Automated spinning appears to be a popular option
for spammers: in a set of 427, 881 pages from heavily-abused
wiki sites, of the English content pages 52% of them were
automatically-generated spun articles.
The goal of this paper is to develop effective techniques to
detect automatically spun content on the Web. We consider the
problem in the context of a search engine crawler. The input is
a set of article pages crawled from various Web sites, and the
output is a set of pages ﬂagged as automatically spun content.
Although not necessarily required operationally, we also use
clustering to group together articles likely spun from the same
original text. This clustering enables us to study the behavior of
spammers on two crawled data sets as they post spun content
with backlinks as part of SEO campaigns to promote Web sites.
Our approach is directly tied to the underlying mechanism used
by automated spinning tools: we use precisely what the tools
use to evade duplicate detection as the basis for detecting their
output. As a result, we also explore in detail the operation of
a popular automated spinning tool.
In summary, we believe our work offers three contributions
to the problem of detecting spun content:
Spinning characterization. We describe the operation of
The Best Spinner (TBS), purportedly the most popular au-
tomated spinning tool in use today. TBS enables spammers
to select an input article, specify various settings for spinning
(e.g., the frequency at which TBS substitutes words in the input
article), generate an arbitrary number of spun output articles,
and optionally validate that the spun content does not trigger
detection by defensive services like the CopyScape plagiarism
checker.
Spun content detection. We propose and evaluate a tech-
nique for detecting automatically spun content based upon
immutables, words or phrases that spinning tools do not modify
when generating spun content. We identify immutables using
the tools themselves. The heart of an automated spinning tool
like TBS is a so-called synonym dictionary, a manually-crafted
dictionary used to perform word substitutions. Since tools
operate locally, each instance has a copy of the dictionary
(and, indeed, downloads the latest version on start) and we
reverse engineer TBS to understand how to access its synonym
dictionary. When examining an article, we use this dictionary
to partition the article text into mutables (words or phrases
in the synonym dictionary) and immutables. When comparing
two articles for similarity, we then compare them primarily
based upon the immutables that they share.
Behavior of article spammers. We implement
this im-
mutable method in a tool called DSpin, which, given a
synonym dictionary as a basis, identiﬁes automatically spun
Web articles. We then apply DSpin to two data sets of crawled
articles to study the extent to which spammers use automated
spinning to create and post content, as well as their spamming
behavior: the number of spun articles generated in a campaign,
the number of sites targeted, the topics of the spun content,
and the sites being promoted. For valid pages from abused
wiki sites, DSpin identiﬁes 68% as SEO spam, 32% as exact
duplicates and 36% as spun content.
The remainder of this paper is structured as follows.
Section II describes the role of spinning in Web spam SEO
campaigns, and discusses related work in detecting Web spam
and duplicate content on the Web. Section III describes the
operation of the The Best Spinner and how we obtain its
synonym dictionary. Section IV describes and evaluates a
variety of similarity metrics for determining when two articles
are spun from the same source, and motivates the development
and implementation of the immutable method. In Section VI,
we apply DSpin to examine spun content on two data sets
of crawled articles from the Web. Section VII discusses how
spammers might respond to the use of DSpin, and Section VIII
concludes.
II. BACKGROUND AND PREVIOUS WORK
As background, we ﬁrst describe the role of spinning in
black-hat SEO practices involving Web spam, and then discuss
related work in both detecting Web spam and identifying near-
duplicate content on the Web.
A. Spinning Overview
Search engine optimization (SEO)
techniques seek to
improve the page rank of a promoted Web site in search
engine results, with the goal of increasing the trafﬁc and
Fig. 1. Example of articles spun from the same source and posted to different
wiki sites as part of the same SEO campaign.
users visiting the site. There are many valid and ofﬁcially
recommended ways to improve search engine page rank by
improving keywords, meta tags, site structure, site speed,
etc. [16]. Indeed, an active service market of books, courses,
companies, and conferences exists for optimizing one’s Web
presence. However, there is also a thriving underground in-
dustry that supports black-hat SEO, which can vary from
violating recommended practices (e.g., keyword stufﬁng) to
breaking laws (e.g., compromising Web sites to poison search
results [22], [25], [35], [36]).
One popular abusive method of black-hat SEO is to post
Web spam across many sites with “backlinks” to a promoted
site. Such backlinks add perceived value when search engines
calculate the page rank of a promoted site: conventional SEO
wisdom holds that the more backlinks to a site, the higher
its rank in search results. Search engines like Google have
responded to such SEO techniques in updates to their page
rank algorithm, such as Panda [32] and Penguin [4], which
penalize pages with duplicate or manipulated content. Such
algorithmic changes, however, rely on effective techniques to
identify manipulated content. Thus, spammers have responded
by making content manipulation harder to detect.
A popular form of Web spam today relies upon spin-
ning to avoid duplicate detection. Spinning replaces words or
phrases in an input article to create new versions with vaguely
similar meaning but sufﬁciently different appearance to avoid
plagiarism detectors. Figure 1 shows two examples of spun
articles generated during the same SEO campaign that have
2
As a result of this SEO campaign, search engine crawlers
download the spun content across numerous sites. But, they
cannot easily identify the spun articles as duplicate content
since each article instance is sufﬁciently different from the
others.
B. Article Spam Detection
Web spam taxonomies [17] typically distinguish between
content spam and link spam, and article spinning can poten-
tially fall in either.
The goal of content spam is to craft the content of a
Web page to achieve high search engine rank for speciﬁc,
targeted search terms. Techniques for detecting content spam
pages include statistical methods incorporating features of
the URL and host name, page properties, link structure, and
revision behavior [14]; and more recently, statistical methods
began incorporating features based upon a page’s content,
including page length, length of words, compressibility, phrase
appearance likelihoods, etc. [28].
Additionally, other research focuses on techniques for de-
tecting a speciﬁc form of content spam called “quilted pages”,
in which, the sentences and phrases that make up a page are
stitched together from multiple sources on the Web [15] [27].
Fundamentally, these techniques work similarly — they begin
with a large corpus of pages, split each page into overlapping
n-grams, and detect a quilted page when a certain percentage
of the n-grams from the candidate page are also found on other
pages.
The link spam category distributes backlinks throughout
spam pages to increase the page rank of a promoted site,
rather than have the spam pages themselves appear higher in
search results. Link spam has received substantial attention
over the years. Since the primary value of link spam is the set
of the backlinks they create, many approaches naturally focus
on the link graph. Link farms, for example, form recognizable
clusters of densely connected pages [37]. Other techniques
such as TrustRank [18], ParentRank [38], and BadRank [21],
[33] formalize a notion of Web page “reputation”. Starting
with training sets of good and bad pages,
they propagate
reputation scores along the link graph as part of the PageRank
computation. Pages with resulting low reputation scores are
considered spam and demoted in page rank.
Oftentimes, spun aritcles contain backlinks, which favor
their classifcation as link spam. However, spun articles some-
times contain rich content that has been carefully modiﬁed
from an original source, and so one might classify such articles
as content spam. Given this dependence on the nature of the
particular spinning campaign, classiﬁcation should be made on
a case-by-case basis.
C. Near-duplicate Document Detection
The description of Google’s approach for near-duplicate
document detection by Manku et al. [23] contains an excellent
breakdown and discussion of the general body of work in the
area. Generally speaking, the most common approaches for
detecting near-duplicate documents on the Web use ﬁngerprints
to reduce storage and computation costs for performing what
is naively an n⇥n comparison problem. The classic work is by
Fig. 2.
optimization.
The role of spinning in the workﬂow of abusive search engine
been posted to open wiki sites. We cut the pages short due to
space constraints, but these pages both backlink to:
http://evasivemosaic6837.wordpress.com/2012/
11/28/how-to-locate-the-best-digital-camera/
a site relating to cameras with links to adult webcam sites.
Note that the spun content is in English, but has been posted
to German and Japanese wikis.
Figure 2 shows the workﬂow of a spammer using spinning
software to spam pages across the Web with backlinks to
promote a particular site. The spammer uses an SEO software
suite, such as SEnuke or XRumer, to orchestrate the SEO
campaign. The SEO tool ﬁrst helps automate the selection
of original article content via an online directory of article
collections, such as EzineArticles.com. Once the spammer has
selected an article, often unrelated to the promoted site, the
SEO tool sends the content to a content spinner such as The
Best Spinner. The spinner changes words and phrases in the
original article to generate repeated variations, and ensures
that the spun content avoids triggering duplicate detection by
submitting it to plagiarism services such as CopyScape. The
spinner returns viable spun content back to the SEO software
suite, which then spams target sites with the spun articles —
typically via proxy services to obscure the spammer and to
minimize the number of articles posted per IP address. Target
lists and proxy services are heavily advertised by third-party
sellers, and are easily integrated into the SEO tools.
3
Broder et al. who developed an approach based on shingles [7].
Shingles are n-grams in a document hashed to provide a ﬁxed
storage size and to make comparisons and lookups efﬁcient.
Sketches are random selections of shingles that provide a ﬁxed-
size representation for each document. Similar documents
share the majority of the shingles in their sketches with each
other, enabling the identiﬁcation of documents that share most
of their content while allowing for minor differences. This
approach enables a graph representation for similarity among
pages, with pages as nodes and edges between two pages that
share shingles above a threshold. The graph yields clusters of
similar documents that either share an edge or have a path of
edges that connect them.