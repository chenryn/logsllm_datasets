## 十一、Docker 网络
永远是网络！
每当出现基础设施问题，我们总是指责网络。部分原因是网络是一切的中心——**没有网络，就没有 app！**
在 Docker 的早期，网络很难——真的很难。这几天，几乎是*一种快感；-)*
 *在本章中，我们将了解 Docker 网络的基础知识。比如容器网络模型(CNM)和`libnetwork`。我们也会弄脏自己的手建立一些网络。
像往常一样，我们将把这一章分成三个部分:
*   TLDR
*   深潜
*   命令
### Docker 网络-TLDR
Docker 在容器内部运行应用，应用需要通过许多不同的网络进行通信。这意味着 Docker 需要强大的网络能力。
幸运的是，Docker 有容器到容器网络的解决方案，以及连接到现有网络和虚拟局域网的解决方案。后者对于与外部系统(如虚拟机和物理服务器)上的功能和服务交互的容器化应用非常重要。
Docker 网络基于一个开源的可插拔架构，称为容器网络模型(CNM)。`libnetwork`是 Docker 在现实世界中实现的 CNM，它提供了 Docker 的所有核心网络功能。驱动程序插入`libnetwork`以提供特定的网络拓扑。
为了创建流畅的开箱即用体验，Docker 附带了一组本机驱动程序，可以处理最常见的网络需求。其中包括单主机桥接网络、多主机覆盖以及插入现有虚拟局域网的选项。生态系统合作伙伴可以通过提供自己的驱动程序来进一步扩展事物。
最后但同样重要的是，`libnetwork`提供了本地服务发现和基本容器负载平衡解决方案。
这就是大局。让我们进入细节。
### Docker 网络-深潜
我们将按如下方式组织本章的这一部分:
*   理论
*   单主机桥接网络
*   多主机覆盖网络
*   连接到现有网络
*   服务发现
*   入口网络
#### 理论
在最高级别，Docker 网络由三个主要组件组成:
*   容器网络模型(CNM)
*   `libnetwork`
*   司机
CNM 是设计规范。它概述了 Docker 网络的基本构建块。
`libnetwork`是 CNM 的真实世界实现，由 Docker 使用。它是用 Go 编写的，实现了 CNM 概述的核心组件。
驱动程序通过实现特定的网络拓扑来扩展模型，例如 VXLAN 覆盖网络。
图 11.1 显示了它们如何在很高的层次上结合在一起。
![Figure 11.1](img/figure11-1.png)
Figure 11.1
让我们仔细看看每一个。
##### 容器网络模型(CNM)
一切都从设计开始。
Docker 网络的设计指南是 CNM。它概述了 Docker 网络的基本构建块，您可以在这里阅读完整的规范:https://github . com/Docker/libnetwork/blob/master/docs/design . MD
我建议阅读整个规范，但在较高的层次上，它定义了三个主要的构建模块:
*   沙箱
*   端点
*   网络
一**沙盒是一个孤立的网络栈。它包括:以太网接口、端口、路由表和域名系统配置。**
 *****端点*** 为虚拟网络接口(如`veth`)。像普通的网络接口一样，它们负责建立连接。在 CNM 的情况下，将*沙箱*连接到*网络*是*端点*的工作。
***网络*****是一个交换机(802.1d 桥)的软件实现。因此，它们将需要通信的端点集合在一起并隔离。**
 **图 11.2 显示了三个组件及其连接方式。
![Figure 11.2 The Container Network Model (CNM)](img/figure11-2.png)
Figure 11.2 The Container Network Model (CNM)
Docker 环境中调度的原子单元是容器，顾名思义，容器网络模型就是为容器提供网络。图 11.3 显示了 CNM 组件如何与容器相关联——沙箱被放置在容器内部以提供网络连接。
![Figure 11.3](img/figure11-3.png)
Figure 11.3
容器 A 只有一个接口(端点)并连接到网络 A。容器 B 有两个接口(端点)并连接到网络 A **和**网络 B。这两个容器将能够通信，因为它们都连接到网络 A。但是，如果没有第 3 层路由器的帮助，容器 B 中的两个*端点*无法相互通信。
了解*端点*的行为类似于常规网络适配器也很重要，这意味着它们只能连接到单个网络。因此，如果一个容器需要连接到多个网络，它将需要多个端点。
图 11.4 再次扩展了该图，这次添加了一个 Docker 主机。虽然容器 A 和容器 B 运行在同一台主机上，但是它们的网络栈在操作系统级别通过沙箱完全隔离。
![Figure 11.4](img/figure11-4.png)
Figure 11.4
##### 图书馆网络
CNM 是设计文档，`libnetwork`是规范实现。它是开源的，用 Go 编写，跨平台(Linux 和 Windows)，由 Docker 使用。
在 Docker 的早期，所有的网络代码都存在于守护进程中。这是一场噩梦——守护进程变得臃肿，它没有遵循构建模块化工具的 Unix 原则，这些工具可以自己工作，但也可以轻松地组合到其他项目中。结果，所有这些都被撕掉，重构到一个基于 CNM 原理的外部库`libnetwork`中。如今，所有的核心 Docker 网络代码都存在于`libnetwork`中。
如您所料，它实现了 CNM 定义的所有三个组件。它还实现了本地*服务发现*、*基于入口的容器负载平衡*，以及网络控制平面和管理平面功能。
##### 司机
如果`libnetwork`实现控制平面和管理平面功能，那么驱动程序实现数据平面。例如，连接和隔离都由驱动程序处理。网络的实际创建也是如此。关系如图 11.5 所示。
![Figure 11.5](img/figure11-5.png)
Figure 11.5
Docker 附带几个内置驱动程序，称为本地驱动程序或*本地驱动程序*。在 Linux 上，它们包括:`bridge`、`overlay`和`macvlan`。在 Windows 上，它们包括:`nat`、`overlay`、`transparent`和`l2bridge`。我们将在本章后面看到如何使用其中的一些。
第三方也可以编写称为*远程驱动程序*或插件的 Docker 网络驱动程序。编织网是一个流行的例子，可以从 Docker Hub 下载。
每个驱动程序负责其所负责的网络上所有资源的实际创建和管理。例如，名为“prod-fe-cuda”的覆盖网络将由`overlay`驱动程序拥有和管理。这意味着将调用`overlay`驱动程序来创建、管理和删除该网络上的所有资源。
为了满足复杂的高流动性环境的需求，`libnetwork`允许多个网络驱动程序同时活动。这意味着您的 Docker 环境可以支持广泛的异构网络。
#### 单主机桥接网络
最简单的 Docker 网络是单主机桥接网络。
这个名字告诉我们两件事:
*   **单主机**告诉我们它只存在于单个 Docker 主机上，只能连接同一个主机上的容器。
*   **网桥**告诉我们这是一个 802.1d 网桥(第 2 层交换机)的实现。
Linux 上的 Docker 使用内置的`bridge`驱动程序创建单主机桥接网络，而 Windows 上的 Docker 使用内置的`nat`驱动程序创建它们。实际上，它们的工作原理是一样的。
图 11.6 显示了两台 Docker 主机，它们有相同的本地桥接网络，称为“mynet”。尽管网络是相同的，但它们是独立的隔离网络。这意味着图中的容器不能直接通信，因为它们在不同的网络上。
![Figure 11.6](img/figure11-6.png)
Figure 11.6
每个 Docker 主机都有一个默认的单主机桥接网络。在 Linux 上，它被称为“桥”，在 Windows 上，它被称为“nat”(是的，它们与用于创建它们的驱动程序同名)。默认情况下，这是所有新容器将连接到的网络，除非您在命令行上用`--network`标志覆盖它。
下面的清单显示了新安装的 Linux 和 Windows Docker 主机上的`docker network ls`命令的输出。输出经过修整，只显示每台主机上的默认网络。请注意，网络的名称与用来创建它的驱动程序是如何相同的——这是一种巧合，而不是一种要求。
```
//Linux
$ docker network ls
NETWORK ID        NAME        DRIVER        SCOPE
333e184cd343      bridge      bridge        local
//Windows
> docker network ls
NETWORK ID        NAME        DRIVER        SCOPE
095d4090fa32      nat         nat           local 
```
 ``docker network inspect`命令是巨大信息的宝库。如果你对低级细节感兴趣，我强烈建议通读它的输出。
```
docker network inspect bridge
[
    {
        "Name": "bridge",     
    }
] 
```
 `在 linux 主机上使用`bridge`驱动程序构建的 Docker 网络基于在 Linux 内核中已经存在了近 20 年的久经沙场的 *linux 桥*技术。这意味着它们性能很高，非常稳定。这也意味着您可以使用标准的 Linux 实用程序来检查它们。比如说。
```
$ ip link show docker0
3: docker0:  mtu 1500 qdisc...
    link/ether 02:42:af:f9:eb:4f brd ff:ff:ff:ff:ff:ff 
```
 `所有基于 Linux 的 Docker 主机上的默认“桥”网络映射到内核中名为“T3”Docker 0 的底层 *Linux 桥*。从`docker network inspect`的输出可以看出这一点。
```
$ docker network inspect bridge | grep bridge.name
"com.docker.network.bridge.name": "docker0", 
```
 `Docker 默认的“桥”网络和 Linux 内核中的“docker0”桥之间的关系如图 11.7 所示。
![Figure 11.7](img/figure11-7.png)
Figure 11.7
图 11.8 通过在顶部添加插入“桥”网络的容器扩展了该图。“桥”网络映射到主机内核中的“docker 0”Linux 桥，该桥可以通过端口映射映射回主机上的以太网接口。
![Figure 11.8](img/figure11-8.png)
Figure 11.8
让我们使用`docker network create`命令创建一个名为“localnet”的新的单主机桥接网络。
```
//Linux
$ docker network create -d bridge localnet
//Windows