62.05±0.60
50.20±1.05
41.31± 0.25
56.70±0.70
48.46± 0.66
42.16± 0.29
L6
56.38±0.47
54.60±0.42
46.84± 0.99
41.70± 0.57
57.65±0.41
43.11± 1.33
42.48± 0.11
L7
57.02±1.19
57.63±0.95
58.66±0.61
56.34±0.99
57.85±1.64
55.98±1.04
57.24±1.12
L7
40.01± 0.81
51.43±1.22
62.57±0.22
59.60±0.39
40.89± 0.64
51.52±1.79
59.84±0.35
L7
38.90± 0.52
43.30± 0.36
65.29±1.99
67.31±1.31
35.81± 0.66
34.65± 1.40
63.63±0.33
L8
57.77±1.54
59.32±1.15
60.29±0.44
58.01±0.95
59.26±1.56
58.34±1.06
59.91±0.52
L8
34.05± 1.33
40.63± 1.72
64.12±0.67
68.80±0.06
35.10± 0.38
42.38± 3.29
65.36±1.07
L8
28.07± 0.74
29.72± 0.63
59.76±4.53
82.31±0.92
31.48± 0.96
29.03± 4.52
80.62±2.31
Table 5: Matrix of the membership inference attack accuracy on a per learner basis for the
2D-slice-mean model across every federated learning environment. The rows indicate the
attacking learner and the columns the attacked learner. Colored cells indicate successful
attacks and more heated cells higher attack accuracies. The results are over 5 random runs.
19
Membership Inference Attacks on Deep Regression Models for Neuroimaging
To compute membership inference attacks using only the error feature, a 1D feature, we
have used a random forest classiﬁer. For other features, i.e., prediction, labels, gradients,
and gradient magnitudes, we have used a generic setup where each feature is embedded
to a 64-dimensional embedding using their respective encoders. The embeddings are then
concatenated and passed through a dropout layer and a linear layer to compute the output.
Below we describe the architecture of the encoder for each feature. We do not do an
excessive architecture search but the results are not very sensitive to any speciﬁc encoder
architecture.
• Prediction and label: Prediction and label form a two-dimensional continuous
feature. To create the embeddings, these are processed through a linear layer and
ReLU non-linearity.
• Gradient magnitudes: We use parameter-gradient magnitudes of each layer as
features resulting in a 14 dimensional feature for 3D-CNN and an 18 dimensional feature
vector for 2D-slice-mean model. These are processed via a linear layer and ReLU non-
linearity to generate the embedding.
• conv 1 gradients: The size of conv 1 gradient feature is 288 (3× 3× 1× 32) and 864
(3× 3× 3× 1× 32) for 2D-slice-mean and 3D-CNN. We project this feature vector to
the desired embeddings size (64) by using a linear layer followed by ReLU non-linearity.
• conv 6 gradients:
For 3D-CNN, the feature dimension is 1× 1× 1× 256× 64. We reshape it to 1× 256× 64
and then process it through three convolutional blocks consisting of a 2D convolution
layer, max-pool and ReLU non-linearity with 64, 64, and 16 output ﬁlters. Finally,
we pass the resulting activation of size 16 × 6 × 6 through a linear layer and ReLU
non-linearity to get the desired 64-dimensional embedding. The convolution kernel
sizes were 5 × 5, 4 × 2, and 4 × 3 and the max-pool kernel sizes were 4 × 2, 4 × 2, and
2 × 2.
For 2D-slice-mean, the feature dimension is 1×1×256×64. We reshape it to 64×256
and process it through three convolutional blocks consisting of a 1D convolution layer,
max-pool, and ReLU non-linearity with 128 output ﬁlters in each layer. Finally, we
process the resulting activations of size 128 × 14 through a linear layer to get the
embedding. The convolution kernel sizes were 5, 4, and 3. The 1D max-pool kernel
sizes were 4, 2, and 2.
• output gradients: This layer has diﬀerent number of parameters for both the models,
and so we used diﬀerent encoders. For 2D-slice-mean model, two ﬁnal feed-forward
layers are considered as output layers.
For 3D-CNN, the feature dimension is 1 × 1 × 1 × 64 × 1.
It is reshaped to a 64-
dimensional vector and passed through the linear layer and ReLU non-linearity to get
the embedding.
For 2D-slice-mean, we consider two ﬁnal feed-forward as the output layer, one of the
layers has dimensions 64×1 and is encoded similar to 3D-CNN’s output layer. The other
20
Membership Inference Attacks on Deep Regression Models for Neuroimaging
Figure 7: Attack Accuracy vs. R2 for models trained with diﬀerential privacy. Error bars
are generated by bootstrapping the test set 5 times using 1000 samples.
feed-forward layer parameters are 32 × 64. We process it through three convolutional
blocks consisting of a 1D convolution layer, max-pool, and ReLU non-linearity with 64
output ﬁlters in each layer. Finally, we process the resulting activation of size 64 × 4
through a linear layer to get the 64-dimensional embedding. All the convolution kernel
sizes were set to 3 and 1D max-pool kernel sizes were 2.
Note: When using features from multiple trained models for attack (e.g., in case of feder-
ated training), we compute the logits using the deep classiﬁers described above and use the
average logit as the probability. The classiﬁer parameters are shared across features from
diﬀerent models. The main intuition to consider averaging is that averaging in log space
would mean considering the prediction of each model’s feature independently.
C.2. Training
To train attack models, we used Adam optimizer, a batch size of 64 and learning rate 1e−3.
We trained the models for a maximum of 100 epochs with a patience of 20 epochs, and
chose the best model by performance on the validation set, created by an 80-20 split of the
training data. Training data creation for the attack models is described in Section 3.2.
Appendix D. Diﬀerential Privacy
Diﬀerential privacy was proposed to secure information about individual records while re-
leasing group or aggregate query results on a database. Considering model parameters as
the output and training dataset as the database, diﬀerentially private machine learning aims
to learn a parametric model such that the learned parameters do not diﬀer much if trained
on another dataset diﬀering the original dataset by a single sample. The privacy parameter
 quantiﬁes the diﬀerence, and lower  is more private (Dwork and Roth, 2014; Abadi et al.,
2016). More formally, a randomized algorithm A : D → W is (, δ)-diﬀerential private, if
for any two adjacent inputs d, d(cid:48) ∈ D.
P r[A(d) ∈ w] ≤ eP r[A(d(cid:48)) ∈ w] + δ
∀ w ∈ W
For non-vacuous guarantees,  is desired to be lower, usually less than 1. However, there
is no standard agreement on how much is suﬃciently small (Laud and Pankova, 2019). δ
21
0.00.10.20.30.40.50.60.70.8R250556065707580Attack Accuracy2D-slice-mean3D-CNNMembership Inference Attacks on Deep Regression Models for Neuroimaging
Model
Train Test Validation
3D-CNN
2D-slice-mean
1.39
0.77
3.13
2.88
3.09
2.92
Table 6: Mean absolute errors (year) for training, testing and validation set in the centralized
setup.
Model
Uniform IID Uniform Non-IID Skewed Non-IID
Train Test Train
3D-CNN
2D-slice-mean
2.16
1.81
3.01
2.76
3.41
2.40
Test
3.81
2.98
Train
2.83
2.42
Test
3.47
3.10
Table 7: Mean absolute errors (year) for training, and testing set for diﬀerent environments
in the federated setup.
depends on the dataset size and is desired to be less than N−1, where N is the dataset size.
In the speciﬁc case of supervised deep learning, the output is the weights, the input is the
labeled dataset, and the algorithm is the training algorithm (usually some variant of SGD).
The main idea here is to minimize the inﬂuence of a single training sample on the weights.
We have used diﬀerential private version of SGD (DP-SGD) proposed by Abadi et al.
(2016) which achieves privacy guarantees by adding Gaussian random noise to the gradients
of each sample and implemented in pytorch-opacus5. This avoids learning too much
information about a single sample, thus providing privacy guarantees. In practise, we have
used the Adam variant of DP-SGD with a learning rate of 5e−5, emulating the same training
setup as Gupta et al. (2021).
Diﬀerential privacy assumes a powerful and worst-case adversary, which may be un-
realistic. In fact, we ﬁnd that to achieve non-vacuous privacy guarantees ( < 100) with
diﬀerential privacy amounted to losing the performance altogether on the brain age predic-
tion problem. However, even with vacuous guarantees, we see that diﬀerential privacy could
reduce the vulnerability to realistic membership inference attacks as shown in Figure 3(a)
and Figure 7.
Appendix E. Membership Inference attacks in centralized setup without
the knowledge of training samples
In the setup of Section 4.1, we assumed that the adversary has access to some training
samples in order to perform membership inference attacks. However, such an assumption
may be too restrictive. Here, we discuss membership inference attacks when the attacker
has access to a trained model that he wants to attack, knowledge of the training parameters
(i.e., white-box setup), and some samples from the training distribution. The attacker does
not know if these samples were part of training.
5. https://github.com/pytorch/opacus
22
Membership Inference Attacks on Deep Regression Models for Neuroimaging
Since the attacker does not have access to the samples used to train the model, he
cannot train an attack cla ssiﬁer as in Section 4.1. To circumvent this limitation, we use the
idea of shadow training from Nasr et al. (2018). Brieﬂy, the attacker can train new models
with the same architecture and training hyperparameters using the samples available to
him from the training distribution. These newly trained models, called shadow models, are
expected to imitate or shadow the trained model’s behavior– for example, similar overﬁtting
behavior, similar training performance, etc. Therefore, the attacker may train the attack
classiﬁer using these shadow models and samples used to train them and expect to transfer
to the trained models he intends to attack.
E.1. Setup
For this section, we use the train, test and validation split described in Section A.1. We
consider that the attacker has access to a trained model and some samples from the training
distribution, which may or may not overlap with the samples used to train the model being
attacked. The attacker intends to identify if some data sample was used to train the model.
The attacker is trying to attack the same models described in Appendix A. These models
are trained on the full training set. To simulate the attacks with access to only the training
distribution but not training samples, we consider the scenario where the attacker has access
to 5000 random samples from the training distribution. For this, we pick 5000 random
samples from the original training set of size 7312. The attacker is trying to determine the
membership of samples from the train set, which diﬀer from these 5000 samples. Due to
limited data, the data used to train the shadow models overlaps with the data used to train
the original model. A more diﬃcult scenario will be if these datasets do not overlap at all.
E.2. Result
To report the membership inference performance, we created a test dataset of 1500 samples
from the full train set (diﬀerent from 5000 samples that the attacker already has) and 1500
samples from the unseen set to evaluate the membership inference attack accuracy. We
trained a single shadow model with 5000 samples that are available to the attacker. The
attack classiﬁer is trained to attack the shadow models similar to earlier experiments using
prediction, label, and gradient of conv6 and output layers from the shadow model as the
features. To infer the memberships, we extract these features from the trained model and
classify them with the attack classiﬁer. The results are summarized in Table 8. The ‘Test’
column shows the result of performing membership inference attack on the trained model,
which is what we are interested in. We also report the attack accuracies on the validation set
derived from the shadow models in the ‘Validation’ column. We observe that even without
access to training samples, the membership inference attacks are feasible albeit with slightly
smaller accuracy compared to the case in which the adversary has access to some training
samples.
23
Membership Inference Attacks on Deep Regression Models for Neuroimaging
Model
3D-CNN
2D-slice-mean
Test
71.74 ± 1.82
74.39 ± 2.14
Validation
75.22 ± 0.22
85.46 ± 0.24
Table 8: Membership inference attacks without the knowledge of training samples. The test
performance is the result of performing membership inference on the trained model. The
validation performance is the attack classiﬁer’s performance on the validation set derived
from the shadow models.
24