(XS,YP,ε)-ULDP, then it also provides (XS,ε)-OSLDP.
It should be noted that if an obfuscation mechanism pro-
vides ε-LDP, then it obviously provides (XS,YP,ε)-ULDP,
where YP = Y . Therefore, (XS,YP,ε)-ULDP is a privacy mea-
sure that lies between ε-LDP and (XS,ε)-OSLDP.
We use ULDP instead of OSLDP for the following two
reasons. The ﬁrst reason is that ULDP is compatible with
LDP, and makes it possible to perform data integration and
data analysis under LDP (Proposition 12). OSLDP does not
have this property in general, since it allows the transition
probability Q(y|x(cid:48)) from non-sensitive data x(cid:48) ∈ XN to be very
large for any y ∈ Y , as explained above.
The second reason, which is more important, is that the
utility of OSLDP is not better than that of ULDP. Intuitively,
it can be explained as follows. First, although YP is not ex-
plicitly deﬁned in OSLDP, we can deﬁne YP in OSLDP as the
image of XS, and YI as YI = Y \ YP, analogously to ULDP.
Then, OSLDP differs from ULDP in the following two points:
(i) it allows the transition probability Q(y|x(cid:48)) from x(cid:48) ∈ XN
to y ∈ YP to be very large (i.e., (5) may not satisﬁed); (ii) it
allows y ∈ YI to be non-invertible. (i.e., (4) may not satis-
ﬁed). Regarding (i), it is important to note that the transition
probability from x(cid:48) ∈ XN to YI decreases with increase in
the transition probability from x(cid:48) to YP. Thus, (i) and (ii)
only allow us to mix non-sensitive data with sensitive data
or other non-sensitive data, and reduce the amount of output
data y ∈ YI that can be inverted to x ∈ XN.
Then, each OSLDP mechanism can be decomposed into
a ULDP mechanism and a randomized post-processing that
mixes non-sensitive data with sensitive data or other non-
sensitive data. Note that this post-processing does not pre-
serve data types (in Deﬁnition 5), and hence OSLDP does
not have a compatibility with LDP as explained above. In
1892    28th USENIX Security Symposium
USENIX Association
addition, although the post-processing might improve privacy
for non-sensitive data, we would like to protect sensitive data
in this paper and ULDP is sufﬁcient for this purpose; i.e., it
guarantees ε-LDP for sensitive data.
Since the information is generally lost (never gained) by
mixing data via the randomized post-processing, the utility
of OSLDP is not better than that of ULDP (this holds for
the information-theoretic utility such as mutual information
and f -divergences [30] because of the data processing in-
equality [9, 13]; we also show this for the expected l1 and l2
losses at the end of Appendix B). Thus, it sufﬁces to consider
ULDP for our goal of designing obfuscation mechanisms that
achieve high utility while providing LDP for sensitive data
(as tdescribed in Section 1).
We now formalize our claim as follows:
Proposition 14. Let MO be the class of all mechanisms from
X to Y providing (XS,ε)-OSLDP. For any QO ∈ MO, there
exist two sets Z and ZP, a (XS,ZP,ε)-ULDP mechanism QU
from X to Z, and a randomized algorithm QR from Z to Y
such that:
QO = QR ◦ QU .
(24)
From Proposition 14, we show that the expected l1 and l2
losses of OSLDP are not better than those of ULDP as follows.
For any OSLDP mechanism QO ∈ MO and any estimation
method λO from data in Y , we can construct a ULDP mecha-
nism QU in (24) and an estimation method λU that perturbs
data in Z via QR and then estimates a distribution from data
in Y via λO. QU and λU provide the same expected l1 and l2
losses as QO and λO, and there might also exist ULDP mech-
anisms and estimation methods from data in Z that provide
smaller expected l1 and l2 losses. Thus, the expected l1 and l2
losses of OSLDP are not better than those of ULDP.
C L2 loss of the utility-optimized Mechanisms
C.1 Utility Analysis
uRR in the general case. We ﬁrst present the l2 loss of the
(XS,ε)-uRR.
Proposition 15 (l2 loss of the uRR). The expected l2 loss of
the (XS,ε)-uRR mechanism is given by:
E[l2
2 (ˆp,p)] =
2(eε − 1)(|XS|− p(XS)) +|XS|(|XS|− 1)
(cid:0)1− ∑
x∈X
+
1
n
n(eε − 1)2
p(x)2(cid:1).
(cid:0)1− 1|XN|
(cid:1).
Proposition 16. For any 0 < ε < ln(|XN| + 1), (25) is maxi-
mized by pUN :
E(cid:2)l2
2 (ˆp,p)(cid:3) ≤ E(cid:2)l2
2 (ˆp,pUN )(cid:3)
|XS|(|XS|+2eε−3)
+ 1
n
=
n(eε−1)2
(26)
When ε ≥ ln(|XN| + 1), the l2 loss is maximized by a mix-
ture of the uniform distribution pUS over XS and the uniform
distribution pUN over XN.
Proposition 17. For any ε ≥ ln(|XN| + 1), (25) is maximized
by p∗ in (13):
(cid:17)
E(cid:2)l2
2 (ˆp,p)(cid:3) ≤ E(cid:2)l2
2 (ˆp,p∗)(cid:3) =
(|XS| + eε − 1)2
(cid:16)
1− 1
|X|
n(eε − 1)2
.
uRR in the high privacy regime. Consider the high privacy
regime where ε ≈ 0. In this case, eε − 1 ≈ ε. By using this
approximation, the right-hand side of (26) in Proposition 16
can be simpliﬁed as follows:
E(cid:2)l2
2 (ˆp,p)(cid:3) ≤ E(cid:2)l2
2 (ˆp,pUN )(cid:3) ≈ |XS|(|XS|−1)
.
nε2
nε2
It is shown in [29] that the expected l2 loss of the ε-RR is
at most |X|(|X|−1)
when ε ≈ 0. Thus, the expected l2 loss of
the (XS,ε)-uRR is much smaller than that of the ε-RR when
|XS| (cid:28) |X|.
uRR in the low privacy regime. Consider the low privacy
regime where ε = ln|X| and |XS| (cid:28) |X|. By Proposition 17,
the expected l2
2 loss of the (XS,ε)-uRR is given by:
E(cid:2)l2
2 (ˆp,p)(cid:3) ≤ E(cid:2)l2
2 (ˆp,p∗)(cid:3) ≈ 1
n .
n (1− 1|X| ) [29], and that 1
It should be noted that the expected l2 loss of the non-private
n (1− 1|X| ) ≈ 1
mechanism is at most 1
n
when |X| (cid:29) 1. Thus, when ε = ln|X| and |XS| (cid:28) |X|, the
(XS,ε)-uRR achieves almost the same data utility as the non-
private mechanism, whereas the expected l1 loss of the ε-RR is
four times larger than that of the non-private mechanism [29].
Utility-optimized RAPPOR in the general case. We then
present the l2 loss of the (XS,ε)-uRAP.
Proposition 18 (l2 loss of the uRAP). Then the expected
l2-loss of the (XS,ε)-uRAP mechanism is given by:
2 (ˆp,p)(cid:3)
E(cid:2)l2
(cid:16)
=
1
n
(25)
1 + (|XS|+1)eε/2−1
(eε/2−1)2 − 1
eε/2−1p(XS)−
p(x j)2(cid:17)
.
|X|
∑
j=1
(27)
When 0 < ε < ln(|XN| +1), the l2 loss is maximized by the
uniform distribution pUN over XN.
For any 0 < ε < 2ln(
the uniform distribution pUN over XN.
|XN|
2 + 1), the l2 loss is maximized by
USENIX Association
28th USENIX Security Symposium    1893
Figure 12: Number of attributes vs. MSE (US Census dataset;
left: ε = 0.1, middle: ε = 1.0, right: ε = 6.0).
Figure 10: ε vs. MSE (common-mechanism). A bold line
parallel to the y-axis represents ε = ln|X|.
Figure 13: ε vs. MSE (personalized-mechanism) ((I): w/o
knowledge, (II) POI distribution, (III) true distribution).
l2 loss of the ε-RAPPOR is at most 4|X|
nε2 (1− 1|X| ) when ε ≈ 0.
Thus, the expected l2 loss of the (XS,ε)-uRAP is much smaller
than that of the ε-RAPPOR when |XS| (cid:28) |X|.
Note that the expected l2 loss of the uRAP in the worst case
|XS|
can also be expressed as Θ(
nε2 ) in this case. As described in
Section 3.2, this is “order” optimal among all ULDP mecha-
nisms.
uRAP in the low privacy regime. If ε = ln|X| and |XS| (cid:28)
(cid:112)|X|, the right-hand side of (28) in Proposition 19 can be
2 (ˆp,pUN )(cid:3) ≈ 1
E(cid:2)l2
2 (ˆp,p)(cid:3) ≤ E(cid:2)l2
n (1 − 1|X| ) [29], and that 1
n .
Note that the expected l2 loss of the non-private mechanism
is at most 1
n when
|X| (cid:29) 1. Thus, when ε = ln|X| and |XS| (cid:28)(cid:112)|X|, the (XS,ε)-
is(cid:112)|X| times larger than that of the non-private mechanism
uRAP achieves almost the same data utility as the non-private
mechanism, whereas the expected l2 loss of the ε-RAPPOR
n (1 − 1|X| ) ≈ 1
(29)
[29].
C.2 Experimental Results of the MSE
Figures 10, 11, 12, and 13 show the results of the MSE corre-
sponding to Figures 5, 6, 7, and 8, respectively. It can be seen
that a tendency similar to the results of the TV is obtained for
the results of the MSE, meaning that our proposed methods
are effective in terms of both the l1 and l2 losses.
Figure 11: |XS| vs. MSE when ε = 0.1 or ln|X|.
simpliﬁed as follows:
Proposition 19. For any 0 < ε < 2ln(
E(cid:2)l2
2 (ˆp,p)(cid:3) is maximized when p = pUN :
2 (ˆp,p)(cid:3) ≤ E(cid:2)l2
2 (ˆp,pUN )(cid:3)
E(cid:2)l2
(cid:16)
=
1
n
1 + (|XS|+1)eε/2−1
(eε/2−1)2 − 1|XN|
(cid:17)
.
(28)
|XN|
2 + 1), the l2-loss
uRAP in the high privacy regime. Consider the high pri-
vacy regime where ε ≈ 0. In this case, eε/2 − 1 ≈ ε/2. By
using this approximation, the right-hand side of (28) in Propo-
sition 19 can be simpliﬁed as:
E(cid:2)l2
2 (ˆp,p)(cid:3) ≤ E(cid:2)l2
2 (ˆp,pUN )(cid:3) ≈ 4|XS|
nε2 .
Thus, the expected l2 loss of the uRAP is at most 4|XS|
nε2
in
the high privacy regime. It is shown in [29] that the expected
1894    28th USENIX Security Symposium
USENIX Association
RRRAPuRRuRAPno privacyMSE10410210010-210-410-610-110-210-310-410-510-610010-110-210-310-410-510-6100MSE10310110-110-310-510-710-210-310-410-510-610-710-110-210-310-410-510-610-710-1(a) Foursquare (left: emp, middle: emp+thr, right: EM)(b) US Census (left: emp, middle: emp+thr, right: EM)0.1110epsilon0.1110epsilon0.1110epsilon0.1110epsilon0.1110epsilon0.1110epsilon104102MSE10010-210-410-66256005004003002001009080706050403020101|ࣲௌ|10-310-46256005004003002001009080706050403020101|ࣲௌ|10-6MSE(a) Foursquare (left: ࢿ= 0.1, right: ࢿ= ln|ढe)103MSE10110-110-310-5400350300250200150100908070605040302010110-7(b) US Census (left: ࢿ= 0.1, right: ࢿ= ln|ढe)|ࣲௌ|4003503002502001501009080706050403020101|ࣲௌ|MSE10-410-510-710-610-5RR (emp)RAP (emp)RR (EM)RAP (EM)uRR (emp)uRAP (emp)uRR (EM)uRAP (EM)no privacy106MSE10410110410210010-210-410-6#Attributes45678910210010-210-410-6#Attributes456789#Attributes45678910010-110-210-3RR (emp)RAP (emp)uRR (emp)uRAP (emp)RR (EM)RAP (EM)uRR (EM)uRAP (EM)no privacyMSEuRR (I)uRR (II)uRR (III)uRAP (I)uRAP (II)uRAP (III)no privacy(a) emp(b) emp+thr(c) EM10010-110-210-310-410-510-610-210-310-410-510-610-210-310-410-510-60.1110epsilon0.1110epsilon0.1110epsilon