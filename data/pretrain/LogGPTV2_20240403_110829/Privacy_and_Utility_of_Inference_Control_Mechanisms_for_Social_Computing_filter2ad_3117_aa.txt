title:Privacy and Utility of Inference Control Mechanisms for Social Computing
Applications
author:Seyed Hossein Ahmadinejad and
Philip W. L. Fong and
Reihaneh Safavi-Naini
Privacy and Utility of Inference Control Mechanisms for
Social Computing Applications
Seyed Hossein Ahmadinejad
Nulli
PI:EMAIL
Philip W.L. Fong Reihaneh Safavi-Naini
University of Calgary
{pwlfong, rei}@ucalgary.ca
ABSTRACT
Modern social computing platforms (e.g., Facebook) are ex-
tensible. Third-party developers deploy extensions (e.g.,
Facebook applications) that augment the functionalities of
the underlying platforms. Previous work demonstrated that
permission-based protection mechanisms, adopted to control
access to users’ personal information, fail to control inference
— the inference of private information from public informa-
tion. We envision an alternative protection model in which
user proﬁles undergo sanitizing transformations before be-
ing released to third-party applications. Each transforma-
tion speciﬁes an alternative view of the user proﬁle. Unlike
permission-based protection, this framework addresses the
need for inference control.
This work lays the theoretical foundation for view-based
protection in three ways. First, existing work in privacy-
preserving data publishing focuses on structured data (e.g.,
tables), but user proﬁles are semi-structured (e.g., trees). In
information-theoretic terms, we deﬁne privacy and utility
goals that can be applied to semi-structured data. Our no-
tions of privacy and utility are highly targeted, mirroring the
set up of social computing platforms, in which users spec-
ify their privacy preferences and third-party applications fo-
cus their accesses on selected components of the user pro-
ﬁle. Second, we deﬁne an algebra of trees in which sanitiz-
ing transformations previously designed for structured data
(e.g., generalization, noise introduction, etc) are now formu-
lated for semi-structured data in terms of tree operations.
Third, we evaluate the usefulness of our model by illustrat-
ing how the privacy enhancement and utility preservation
eﬀects of a view (a sanitizing transformation) can be for-
mally and quantitatively assessed in our model. To the best
of our knowledge, ours is the ﬁrst work to articulate precise
privacy and utility goals of inference control mechanisms for
third-party applications in social computing platforms.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS ’16, May 30-June 03, 2016, Xi’an, China
© 2016 ACM. ISBN 978-1-4503-4233-9/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2897845.2897878
CCS Concepts
•Security and privacy→ Security requirements; So-
cial network security and privacy;
Keywords
Social computing; Facebook applications; inference attack;
privacy; utility; view-based protection; sanitizing transfor-
mation; veriﬁcation; composition
1.
INTRODUCTION
Today’s Social Network Systems (SNSs) are designed such
that their functionalities can be extended by third-party ex-
tensions (e.g., Facebook applications). Extensions can ac-
cess user data stored in the SNS through an SNS Applica-
tion Programming Interface (API). To protect user privacy,
a user can explicitly specify what she wants to hide from or
share with an extension. The SNS API requires the exten-
sion to seek permissions from the user. Access is allowed
only if the corresponding permission is granted by the user.
SNS API Inference Attacks. The inadequacy of permis-
sion-based protection for users of SNS extensions was ﬁrst
articulated by Ahmadinejad et al. [3], through the follow-
ing example. Suppose a user does not want to share her
birthday with a Facebook application, but she is willing to
grant the application access to her “wall,” for such accesses
are needed by the application to deliver its functionalities.
She then sets up the permissions to reﬂect the above pri-
vacy preference: grant permission to access her wall, deny
permission to access her birthday. What she may not be
aware of is that a malicious application may scan through
her wall, looking for a day in the year in which there is a high
number of postings with “happy birthday” wishes, thereby
inferring her birthday, even though the latter has been made
inaccessible.
This type of information leakage, that is, inferring users’
sensitive information from their accessible information
through an SNS API, is called SNS API inference at-
tacks. This type of attacks was ﬁrst introduced in [3] (note
that inference attacks in databases were ﬁrstly introduced
in [20], but our focus is on SNS API inference attacks). The
feasibility and accuracy of sample inference algorithms were
empirically evaluated in subsequent works [4, 5]. Success
rates of the inference algorithms were found to be alarmingly
high. It was also shown that SNS API inference attacks can
be employed as a building block for launching further secu-
rity attacks, including, for example, phishing and bypassing
authentication challenges.
View-based Protection of User Data. The above exam-
ple illustrates an important point: controlling access can-
not control inference. Privacy is not simply about access
authorization, but about breaking data correlation so that
inference becomes impossible.
In this work, we envision an alternative protection frame-
work in which user proﬁles undergo sanitizing transforma-
tions before being released to third-party applications. Each
transformation speciﬁes an alternative representation of the
user proﬁle that we call a view . A privacy policy is a speci-
ﬁcation of what transformations must be applied to the user
proﬁle prior to its disclosure to various applications. We
call such a protection framework view-based protection.
Unlike permission-based protection, this framework can ad-
dress the need for inference control. A transformation per-
turbs the statistical correlation of data, and thus upsets the
attacker’s ability to infer sensitive information.
Contributions. This work lays the theoretical foundation
for view-based protection. Speciﬁcally, we claim the follow-
ing contributions:
1. Existing work in privacy-preserving data publishing fo-
cuses on structured data (e.g., tables) [1], but user pro-
ﬁles are semi-structured (e.g., trees). In information-
theoretic terms, we formulated privacy and utility goals
that can accommodate semi-structured data, in which
“attributes” are deﬁned via complex queries that lo-
cate, extract or even combine information dispersed in
various parts of the semi-structured data (§3). Our no-
tions of privacy and utility are highly targeted (§3.2),
mirroring the set-up of social computing platforms, in
which users specify their privacy preferences (what in-
formation needs protection) and third-party applica-
tions declare their accesses on selected components of
the user proﬁle (what information needs to be avail-
able). We identiﬁed the conditions under which sani-
tizing transformations are safely composable, such that
the privacy enhancing and utility preserving eﬀects
of successive transformations are accumulative (§3.3,
§3.5, §3.6). We formally articulated when there will
be an inevitable trade-oﬀ between privacy and util-
ity, and identiﬁed conditions under which a sanitizing
transformation performs the trade-oﬀ in a productive
manner (§3.4).
2. Suppression, generalization, permutation and noise in-
troduction are common sanitizing transformations orig-
inally designed for structured data. We deﬁned an al-
gebra of trees through which analogues of these four
transformations are formulated for semi-structured data
(§4). The privacy enhancing and utility preserving
characteristics of these four transformations are for-
mally articulated (§5).
3. We evaluated our model in two ways (§6). First, we
illustrated how the privacy enhancing and utility pre-
serving eﬀects of a view can be formally and quanti-
tatively assessed in our model (§6.1). Second, we used
our model to analyze the relative merits of diﬀerent
sanitizing strategies on semi-structured data (§6.2).
To the best of our knowledge, ours is the ﬁrst work to ar-
ticulate precise privacy and utility goals of inference control
for third-party applications in social computing platforms.
2. VIEW-BASED PROTECTION
This section gives an overview of view-based protection,
and motivates our methodology by drawing an analogy be-
tween the goal of this work and that of program veriﬁcation.
Protection via Views. We assume every user is the ad-
ministrator of her own proﬁle. Her responsibility as an ad-
ministrator is to specify, for each third-party application (or
each category of applications), what information is consid-
ered sensitive and thus requires protection. Note that this is
a speciﬁcation of privacy preference, and not permissions.
We assume every third-party application will explicitly
declare what information in the user proﬁle it plans to con-
sume in order to deliver its functionalities. Again, this is not
a request for permissions, but rather a utility declaration.
Intuitively, the privacy goal is to break the correlation
between the information declared to be sensitive by the pri-
vacy preference, and the information in the published user
proﬁle. In that way, when the application accesses the pub-
lished user proﬁle, it cannot infer the sensitive information.
This privacy goal is achieved in view-based protection not
by denying access to the sensitive information. Instead, a
sanitizing transformation is applied to the user proﬁle before
the latter is made available to the third-party application.
The transformed proﬁle is called a view of the original pro-
ﬁle. When the third-party application makes a query against
the user proﬁle, the query is evaluated over the transformed
proﬁle rather than the original proﬁle. A properly designed
transformation is supposed to break the correlation between
the sensitive information and the transformed proﬁle.
The transformation, however, may destroy the usefulness
of proﬁle information. The utility goal, intuitively stated,
is to preserve as much as possible the availability of useful
information as speciﬁed in the utility declaration.
Who is responsible for engineering a view is a matter of
platform design. One possibility is to have dedicated third-
party developers engineer primitive transformations, with
formally certiﬁed eﬀects on privacy enhancement and utility
preservation. These primitive transformations can become
building blocks for views. The users, under the guidance
of the social computing platform and the advice of the ap-
plication developer, can then compose a view out of these
building blocks. For this protection scheme to be viable,
there shall be formal means for verifying if the view that is
composed out of primitive transformations does indeed ful-
ﬁll both the privacy preference of the user and the utility
declaration of the application.
Formal Veriﬁcation of Views. The task of verifying views
is analogous to the formal veriﬁcation of program correct-
ness, which aims at providing formal guarantees that the
program behaves according to speciﬁcation. Program cor-
rectness is usually established in a compositional manner.
For example, in Floyd-Hoare Logic [12], a program is spec-
iﬁed in terms of a precondition and a postcondition: if the
precondition is satisﬁed prior to program execution, the post-
condition shall be met when the program terminates. The
semantics of an individual program statement is speciﬁed in
terms of an inference rule, which delineates the precondition
and postcondition of that statement. A proof of correctness
is obtained by composing the inference rules of the state-
ments in the program, and thus inferring that the program
postcondition follows from the program precondition.
In this work, we propose a theoretical framework for ver-
ifying that a view meets its intended privacy and utility
goals. This proposal is analogous to the compositional na-
ture of program veriﬁcation. We envision that a view is
composed of more primitive transformations. We articu-
late the conditions under which the privacy enhancing and
utility preserving eﬀects of individual transformations are
accumulative. Under such conditions, composition of trans-
formations is safe, and we can verify a view by verifying each
building block in turn. These conditions are captured in the
form of an inference rule. The inference rule are then instan-
tiated for various sanitization strategies, including suppres-
sion, generalization, permutation and noise introduction.
3. PRIVACY AND UTILITY GOALS
This section develops a framework for assessing if views
composed of primitive transformations meet quantitative
goals of privacy and utility. The framework is general enough
to accommodate either structured or semi-structured data.
3.1 Preliminaries
We begin with some deﬁnitions which will be used through-
out the paper.
We write DA for the domain of a random variable A.
Definition 1
(Conditional Independence [19]).
Given random variables A, B and C, variables A and C
are said to be conditionally independent given B iﬀ:
∀a∈ DA, b∈ DB, c∈ DC .
Pr(A= a  B= b, C= c)= Pr(A= a  B= b)
change our knowledge of A. In such a case, we write(AÆ
C)  B.
In other words, when B is known,
learning C does not
Definition 2
(Shannon entropy [10]). The Shannon
Bayesian networks, which are probabilistic graphical mod-
els, can help in showing the dependency between random
variables. Bayesian networks are directed acyclic graphs
where nodes are random variable, and an edge from one
random variable to another indicates that the former causes
the latter. In this work, we use Bayesian networks to depict
the dependencies among random variables.
H(A)=− Q
a∈DA
entropy H(A) of a random variable A is deﬁned as follows:
Pr(A= a)⋅ log Pr(A= a)
Intuitively, H(A) measures the uncertainty about A.
ditional entropy H(A  B) is deﬁned by
Pr(B= b)⋅ H(A  B= b)
If A is a deterministic function of B, then H(A  B)= 0.
I(A; B)= I(B; A)= H(A)− H(A  B)= H(B)− H(B  A)
Definition 4. The mutual information between A and
B, which is the amount of information shared between A
and B is deﬁned by
H(A  B)= Q
b∈DB
Definition 3. Given another random variable B, the con-
Proposition 1
(Data Processing Inequality [10]).
Given random variables A, B, and C:
(AÆ C  B)⇒ I(A; B)≥ I(A; C)
accessible via a proﬁle is a random variable A where DA,
3.2 Sensitive and Useful Information
SupposeA is the set of all user proﬁles. The information
the domain of A, is A. A sanitizing transformation of a∈
A is a (deterministic or probabilistic) function t∶A→A.
t(A) with distribution P(t(A)  A).
this speciﬁcation as a deterministic function s∶A→A that
a random variable S= s(A). Assuming s to be deterministic
have the value that maximizes P(S  A= a) for a given a.
Furthermore, a transformation t induces a random variable
The user speciﬁes in her privacy preference (§2) what com-
ponents of her proﬁle are considered sensitive. We model
is reasonable because, even if S is not originally a determin-
istic function of A, a rational adversary always infers S to
extracts those components from the proﬁle. Now s induces
This is the best strategy an adversary can adopt.
components from the proﬁle. Now u induces a random vari-
U is preserved after the user proﬁle undergoes sanitizing
transformation, is the utility of the sanitized proﬁle.
An SNS extension delivers functionalities to the user by
extracting certain information from the user proﬁle A. In
practice, the extension speciﬁes in its utility declaration (§2)
the components of the user proﬁle it needs to query in order
to deliver its functionalities. We model this declaration as
a deterministic function u∶A→A that extracts the useful
able U= u(A). The degree in which information concerning
t(A) and S is minimized (privacy), and the correlation be-
tween t(A) and U is maximized (utility). This implies the
and utility goals are then respectively minimizing St(A) and
maximizing Ut(A).
need for a correlation measure. Let XY be a correlation