update the student network. NAD produces the student model
as the final output. All the trained models are considered valid
based on their normal accuracy drop. For TrojAI models, we
do not consider UAP as it is expensive to train from scratch for
such large models. Our evaluation on the four standard datasets
already shows the ineffectiveness of UAP (see Section V-B).
Metrics. We consider the following criteria in the evaluation.
The prediction accuracy on the test set is used for measuring
normal functionalities. For adversarially trained models, we
measure model robustness within the given L∞ bound. As
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
91380
stated in Section III, the goal of orthogonalization is to enlarge
the aggregated class distances for all pairs. We use the relative
improvement of pair-wise class distance as the metric. That is,
we compute the improvement percentage for every class pair
and obtain the average, which is defined as follows.
n(cid:88)
n(cid:88)
ˆdi→j − di→j
,
(8)
1
n × (n − 1)
i=1
j=1,j̸=i
di→j
where n is the number of classes; di→j and ˆdi→j are the class
distances from i to j for the original model and the hardened
model, respectively. We leverage an existing backdoor gener-
ation method NC [53] to measure the distance. Specifically,
we randomly select 100 samples from the validation set of
class i and apply NC for 1,000 epochs to generate a backdoor
that can flip 90% of those samples to the target class j. As
a backdoor is randomly initialized during generation, to avoid
the bias from randomness, we run NC on the same pair for 3
times and use the smallest backdoor size as the class distance.
As the distance is approximated by the above process, we
further study the stability of the measurement using different
sets/numbers of samples. Our experiment on CIFAR-10 for
a naturally trained ResNet20 model shows that the distance
measured on 100 samples with 100 random runs is 57.48 with
a standard deviation of 4.56, which is quite stable. We also
study using another backdoor generation method ABS [41]
for measuring distance. Our results show that ABS can be a
reasonable alternative. Details are in Appendix X-B. We show
the average relative improvement along with the average class
distance in the following results. In addition, the training time
(in minutes) of each method is also measured.
B. Evaluation on Standard Datasets
We conduct experiments on both naturally trained and
adversarially trained models for the four standard datasets, and
the results are presented in Table I and Table II, respectively.
Results for LISA and GTSRB are shown in Table V and Ta-
ble VI in Appendix X-C. As hardening with universal adver-
sarial perturbation needs to train a model from scratch [43],
we only evaluate it in comparison with other techniques on
naturally trained models. From Table I, we can observe that
with a very small accuracy drop, MOTH can improve the class
distance from 55.21% to 190.40% (the largest increase on
ResNet32 model with SVHN dataset) compared to the original
model. We also evaluate the robustness (using PGD) for a nat-
urally trained ResNet20 model on CIFAR-10 before and after
applying MOTH. The robustness does not change. Baseline
UAP can only harden the class distance on a few datasets and
models. For some models such as Network in Network (NiN)
on CIFAR-10, UAP is not able to increase the class distance.
The average class distance is even smaller than the original
model (57.56 vs. 60.67), rendering UAP ineffective for class
distance hardening. Training with universal backdoors has rea-
sonable improvement over the original models, from 18.88%
to 113.92%. However, it is inferior to MOTH, with 46.68%
improvement difference on average. Pairwise considers all
class pairs equally for hardening and has similar results as
TABLE I: Comparison of different methods on hardening
class distance for naturally trained models. First three columns
denote different datasets (D), models (M) and training methods
for the evaluation. The fourth column denotes model accuracy
on the test set. The fifth column shows the training time in
minutes. The sixth column shows the average class distance
across all class pairs. The seventh column denotes the im-
provement of pairwise class distance by different techniques
compared to that of original models (Natural). The last column
shows the degradation of test accuracy.
D M
Time (m)
Accuracy
Method
Distance
Increase
Degrad.
0
2
t
e
N
s
e
R
N
N
i
9
1
G
G
V
N
N
i
2
3
t
e
N
s
e
R
0
1
-
R
A
F
I
C
N
H
V
S
Average
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
91.52%
89.95%
91.09%
90.04%
90.57%
88.78%
90.34%
88.09%
87.18%
83.68%
86.61%
86.76%
86.35%
86.81%
92.30%
91.23%
91.51%
90.78%
91.71%
90.01%
91.48%
95.61%
94.39%
92.48%
94.63%
95.03%
95.16%
94.99%
95.15%
94.09%
93.91%
93.16%
94.60%
94.74%
94.49%
92.53%
91.37%
90.53%
91.04%
91.73%
91.01%
91.62%
56.77
84.96
3.06
243.11
65.00
120.47
29.68
68.30
40.38
1.14
196.67
33.90
64.51
36.63
68.42
134.08
3.83
226.55
95.80
243.96
44.80
10.50
24.75
1.42
45.47
53.40
152.63
47.77
26.70
31.59
0.89
228.95
109.30
530.43
172.63
46.14
63.15
2.07
188.15
71.48
222.40
66.30
53.49
60.57
55.80
96.00
93.54
111.09
109.70
60.67
67.40
62.04
57.56
90.69
120.38
121.64
61.14
52.87
45.08
77.20
71.17
99.32
92.93
64.63
65.76
59.36
69.31
107.15
113.64
131.39
55.11
60.04
53.68
49.40
120.20
126.60
160.65
59.01
61.33
55.19
69.89
96.55
114.21
123.26
-
14.26%
4.84%
81.57%
78.16%
112.84%
108.62%
-
14.39%
4.35%
-5.22%
54.09%
104.13%
107.60%
-
-11.07%
-25.17%
29.53%
18.88%
61.83%
55.21%
-
8.12%
-4.93%
6.99%
65.97%
79.20%
102.56%
-
12.31%
-2.03%
-8.86%
113.92%
129.21%
190.40%
-
7.60%
-4.59%
20.80%
66.20%
97.44%
112.88%
-
1.57%
0.43%
1.48%
0.95%
2.74%
1.18%
-
0.91%
4.41%
1.48%
1.33%
1.74%
1.28%
-
1.07%
0.79%
1.52%
0.59%
2.29%
0.82%
-
1.22%