# 深度伪造(Deepfake)原理分析及实战
|
##### 译文声明
本文是翻译文章
译文仅供参考，具体内容表达以及含义原文为准。
## 前言
众所周知，人工智能正迎来第三次发展浪潮，它既给社会发展带来了巨大机遇，同时也带来了诸多风险，人工智能对国家安全的影响已成为世界各国的重要关切和研究议程。作为人工智能深度学习领域的一个分支，Deepfake(深度伪造)技术在近几年迅速兴起，为国家间的政治抹黑、军事欺骗、经济犯罪甚至恐怖主义行动等提供了新工具，给政治安全、经济安全、社会安全、国民安全等国家安全领域带来了诸多风险。  
本文会首先介绍Deepfake的相关背景及技术特点，然后以实战为导向详细介绍Deepfake的一种典型生成方案，最后会给出常用的防御(检测)策略。
## Deepfake背景
深度伪造一词译自英文“Deepfake”(“deep learning”和“fake”的组合),
最初源于一个名为“deepfakes”的Reddit社交网站用户, 该用户于2017年12月在 Reddit
社交网站上发布了将斯嘉丽·约翰逊等女演员的面孔映射至色情表演者身上的伪造视频。  
Deepfake目前在国际上并没有公认的统一定义, 美国在其发布的《2018 年恶意伪造禁令法 案》中将“deep
fake”定义为“以某种方式使合理的观察者错误地将其视为个人真实言语或行为的真实记录的方式创建或更改的视听记录”,
其中“视听记录”即指图像、视频和语音等数字内容。本文就采用这一定义，并针对“视听记录”中的视频领域的技术进行分析及实战。
###  视频伪造
视频伪造是Deepfake技术最为主要的代表，制作假视频的技术也被业界称为人工智能换脸技术（AI face
swap)。其核心原理是利用生成对抗网络或者卷积神经网络等算法将目标对象的面部“嫁接”到被模仿对象上。由于视频是连续的图片组成，因此只需要把每一张图片中的脸替换，就能得到变脸的新视频。具体而言，首先将模仿对象的视频逐帧转化成大量图片，然后将目标模仿对象面部替换成目标对象面部。最后，将替换完成的图片重新合成为假视频，而深度学习技术可以使这一过程实现自动化。  
随着深度学习技术的发展，自动编码器、生成对抗网络等技术逐渐被应用到深度伪造中。
**自动编码器**
自动编码器是神经网络的一种，其基本思想就是直接使用一层或者多层的神经网络对输入数据进行映射，得到输出向量，作为从输入数据提取出的特征。基本的自编码器模型是一个简单的三层神经网络结构：一个输入层、一个隐藏层和一个输出层。其中输出层和输入层具有相同的维数。自动编码器本质上是一种数据压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习。目前自编码器的主要用途就是降维、去噪和图像生成。
在应用于Deepfake的情况下输入视频帧，并编码。这意味着它将从中收集的信息转换成一些低维的潜在空间表示。这个潜在的表示包含关键特征的信息，如面部特征和身体姿势的视频帧。通俗地说，其中有关于脸在做什么的信息，是微笑还是眨眼等等。自动编码器的解码器将图像从潜在表示中恢复出来，用于给网络学习。
**生成对抗网络**
生成对抗网络是非监督式学习的一种方法，通过让两个神经网络相互博弈的方式进行学习。该方法由伊恩·古德费洛等人于2014年提出。生成对抗网络由一个生成网络与一个判别网络组成。生成网络从潜在空间（latent
space）中随机取样作为输入，其输出结果需要尽量模仿训练集中的真实样本。判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实。
在Deepfake的场景下，通过使用生成对抗网络可以生成更逼真的图像/视频。  
但是在过去要使用Deepfake进行生成时，需要额外的信息。比如，如果想要生成头部的移动，则我们需要脸部的landmark，如果想要生成全身的移动，还需要姿势估计(pose-estimation)。此外，使用这些传统技术如果想把源脸替换到目标脸上的话，需要使用大量的双方人脸图像的数据进行事先训练训练。  
而本文将介绍的技术不需要这些附加约束条件，原文发在NIPS2019上，名为《First Order Motion Model for Image
Animation》。
## First Order Motion Model for Image Animation
从论文的题目就可以看出，其希望完成的任务是image animation，输入一张源图像（source image）和一个驱动视频(driving
video)，输出是一段视频，其中主角是源图像，动作是驱动视频中的动作。如下所示，源图像通常包含一个主体，驱动视频包含一系列动作。
第一列是给定的图片，而第一排图像是给定的动作序列，通过人脸和表情的迁移，分别使其完成了一系列的动作。换句话说，把提取到的动作特征用作图像的动作依据。  
作者采用了一种源于Monkey
Net(见参考文献[6])的自我监控策略。在训练时，作者使用了大量包含相同对象类别的对象的视频序列。模型通过组合一个单一的帧和一个学习的动作的潜在表示来重建训练视频。通过观察从同一视频中提取的帧对，它会学习到将动作编码为特定于动作的关键点位移和局部仿射变换的组合。在测试时，将模型应用到由源图像和驱动视频的每一帧组成的对上，并根据源对象生成对应的图像动画。  
整个模型的运作流程如下
整个模型分为运动估计模块和图像生成模块两个主要组成部分。在运动估计模块中，该模型通过自监督学习将目标物体的外观和运动信息进行分离，并进行特征表示。而在图像生成模块中，模型会对目标运动期间出现的遮挡进行建模，然后从给定的名人图片中提取外观信息，结合先前获得的特征表示，进行视频合成。  
1）运动估计模块  
输入：源图像S , 驱动图像D  
输出：  
1、密集运动场：表征了驱动图像D中的每个关键点到源图像S的映射关系  
2、贴图遮罩：表明了在最终生成的图像中，对于驱动图像D而言，那部分姿态可以通过S扭曲得到，哪部分只能通过impainting得到  
在这里，S到D有一个较大的形变，直接映射，误差较大，采用的技巧是提出了一个过渡帧R,首先建立R帧到S帧、R帧到D帧的映射，然后再建立D帧到S帧的映射  
运动估计模块中有两个子模块：  
1、关键点检测器：检测图片中的关键点信息。接着采用局部仿射变换，在关键点附近建模它的运动，主要用一阶泰勒展开来实现。同理，R帧到D帧通过这种方式并行得到  
2、稠密运动网络：根据前面得到的映射关系J和源图像S产生上面说的2个输出。  
2）图像生成模块：图像生成模型，根据输入的图片和第一部分得到的信息，生成一个新的图片  
这里有论文原作者的分享报告：  
[https://www.youtube.com/watch?v=u-0cQ-grXBQ&feature=emb_imp_woyt，感兴趣的师傅们可以看看](https://www.youtube.com/watch?v=u-0cQ-grXBQ&feature=emb_imp_woyt%EF%BC%8C%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E5%B8%88%E5%82%85%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%9C%8B%E7%9C%8B)
## 要点分析
在上图中我们看到Motion module(黄色底色)实际上有两个子模块（一左一右），分别是关键点检测器（keypoint
detector)和稠密运动网络(dense
motion),这是论文的核心，在本节接下来的部分我们会依次介绍关键点检测器、稠密运动网络、训练损失、测试阶段的关键细节，帮助大家更容易理解本文的工作思路(注意，原论文文后还有10页的附录都是关于公式细节的，我们这里均略过，下面只会分析、推导正文给出的关键公式)。
###  关键点检测器
论文中物体的运动用其关键点处的运动表示，关键点通过自监督的方式学习。首先假设存在一个抽象的参考帧R，这样的话，预测Ts<-D可以拆分成预测Ts<-R和TR<-D。注意R是抽象的，可以在推导中消除。引入R的好处是可以将S和D分离。  
对于某类物体，假设有K个关键点p1，p2，…，pK。  
假设有一帧图片X，对于函数Tx<-R，用在pK处的一阶泰勒展开表示R中的任意像素p点处的值有：
忽略高无穷小量，得到
假设TX<-R在关键点的邻域是双射，于是有TR<-X=T-1X<-R，此时在关键点pK附近就有
带入一阶泰勒展开，得到
其中，
于是，TS<-D在任一点处的值可以通过关键点处的值和关键点处的导数估计。  
TS<-R(pk)和TD<-R(pk)用关键点预测器预测。关键点预测器使用标准的U-Net结构，预测K个热力图，每个热力图代表一个关键点。关键点预测器对每个关键点额外预测4个通道，用于计算
和
###  稠密运动网络
论文这里的目的是用稠密运动网络联合各关键点的局部运动和源图像得到稠密的运动场。  
根据K个关键点的局部运动，将原图像变形（warp）成S1,S2,….,SK，再添加一个不运动的S0=S表示背景。另外计算Hk用于表示运动发生的像素点位置：
转换得到的图片S1,S2,….,SK和Hk拼接在一起通过另外一个U-Net得到掩码Mk。最后稠密运动场使用下面的公式计算：