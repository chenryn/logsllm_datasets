agnostic to the concrete classifiers. The detailed discussion on this
classifier-agnostic property is deferred to § 8.
Effectiveness
Attack
Success Rate
Misclassification
Confidence
∆Accuracy
(ImageNet)
Evasiveness
∆Accuracy
ntrigger
1
5
10
98%/100%
97%/98%
90%/95%
0.865
0.846
0.829
0.1%
0.2%
0.4%
(ISIC)
0.3%
0.8%
1.2%
Table 4. Impact of number of triggers (ntrigger).
Number of Triggers. Moreover, we evaluate the attacks with mul-
tiple trigger inputs (§ 4.5). Let ntrigger be the number of triggers. We
consider that the attacks are successful only if all the ntrigger trig-
gers are misclassified into the desired classes. Table 4 summarizes
the attack effectiveness and evasiveness as ntrigger varies from 1 to
10. Observe that with modest accuracy decrease (0.1% - 0.4% and
0.3% - 1.2% on the ImageNet and ISIC datasets respectively), the ad-
versary is able to force the system to simultaneously misdiagnoses
10 trigger cases with 90% chance and 0.829 confidence.
6.2 Case II: Speech Recognition
A speech recognition system [43] takes as an input a piece of sound
wave and recognizes its content (e.g., a specific word).
0.250.5 1   2   4   Setting of Parameter 0.250.5 1   2   4   7580859095100Attack Success Rate (%)00.20.40.60.81 Accuracy (%)ImageNetISIC0.250.5 1   2   4   x10  -3  0.750.80.850.90.951(a)(b)x10  -3   Misclassification  Confidencex10  -3  TriggerNeighbor∆λ1002003004005001002003004005000.750.80.850.90.95100.511.522.533.57580859095100Attack Success Rate (%) Misclassification      Confidence100200300400500 Accuracy (%)ISICNumber of System Tuning Epochs  (a)(b)TriggerNeighbor∆Experimental Setting. We assume the Pannous speech recogni-
tion model [43], which is pre-trained on the Pannous Speech (PS)
dataset [43], and attains 99.2% accuracy in recognizing the utter-
ances of ten digits from ‘0’ to ‘9’.
We then pair the feature extractor of the Pannous model with a
classifier (1 FC layer + 1 SM layer) and adapt them to the Speech
Commands (SC) dataset [60], which consists of 4,684 utterances of
digits. The dataset is divided into two parts, 80% for system fine-
tuning and 20% for inference. The genuine baseline system attains
82.2% accuracy in the new domain.
Evasiveness
∆Accuracy
∆Accuracy
θ
Attack
Effectiveness
Misclassification
Confidence
Success Rate
(SC)
0.65
2.5%
0.80
1.3%
0.95
0.6%
Table 5. Impact of parameter selection threshold θ.
82%/85%
95%/91%
96%/100%
(PS)
5.0%
1.1%
0.2%
0.911
0.932
0.943
Parameter Selection. Table 5 summarizes the impact of parame-
ter selection threshold θ on the attack effectiveness and evasiveness.
Within proper setting of θ (e.g., θ ≤ 0.95), both the attack effec-
tiveness and evasiveness improve as θ increases. For example, with
θ = 0.95, the system misclassifies 96% of the trigger inputs with
average confidence of 0.943; meanwhile, the accuracy of the adver-
sarial models and genuine models differs by less than 0.2% and 0.6%
on the PS and SC datasets respectively. We set θ = 0.95 by default
in the following experiments.
Figure 8: Impact of perturbation magnitude λ.
Perturbation Magnitude. We then measure the attack effective-
ness and evasiveness as functions of the perturbation magnitude λ.
Figure 8 shows the results. Similar to case study I, for λ ≤ 2 × 10−3,
larger λ leads to higher attack success rate (and misclassification
confidence) but also lower classification accuracy. Thus, the adver-
sary needs to strike a balance between the attack effectiveness and
evasiveness by properly setting λ (e.g., 10−3).
Also notice that the attack success rate decreases with overly
large λ, which can be explained as follows. As the crafting of an
adversarial model solely relies on one reference input x+ as guid-
ance, the optimization in Algorithm 2 is fairly loosely constrained.
Overly large perturbation may cause the trigger input x− to deviate
from its desired regions in the feature space.
System Fine-Tuning. We also show that model-reuse attacks are
insensitive to system fine-tuning. Figure 9 shows the attack effec-
tiveness and evasiveness as functions of the number of system
tuning epochs (ntuning). Observe that for ntuning ≥ 125, there is no
Figure 9: Impact of system fine-tuning.
significant change in either the accuracy measure or attack success
rate, indicating that the system tuning, once converges, has limited
impact on the attack effectiveness.
Attack
Misclassification
∆Accuracy
Success Rate
Confidence
Classifier
2FC + 1SM
1Res + 1FC + 1SM
1Conv + 1FC + 1SM
94%/92%
94%/92%
91%/100%
100%/100%
0.815
0.856
0.817
0.962
(ISIC)
1.3%
1.4%
1.1%
12.1%
1FC + 1SM (partial tuning)
Table 6. Impact of classifier design (FC - fully-connected, SM
- Softmax, Conv - convolutional, Res - residual block).
Classifier Design. Table 6 shows how the classifier design may
influence model-reuse attacks. Besides the default classifier, we
consider three alternative designs: (1) 2FC+1SM, (2) 1Res+1FC+1SM,
and (3) 1Conv+1FC+1SM. Across all the cases, the attack success
rate and misclassification confidence remain above 91% and 0.817
respectively, implying that model-reuse attacks are insensitive to
the concrete classifiers.
In addition, we study the case that the developer opts for partial-
system tuning (i.e., training the classifier only with the feature
extractor fixed). Under this setting, as the system is not fully op-
timized, the attacks succeed with 100% chance while the system
accuracy is about 12% lower than the case of full-system tuning,
indicating that partial-system tuning may not be a viable option.
Thus, we only consider full-system tuning in the following.
Figure 10: Impact of layer selection.
Layer Selection. The attack effectiveness and evasiveness are also
related to the layers selected for perturbation. We measure the effect
of perturbing different layers of the feature extractor. We consider
five cases: 4th (Conv), 5th (Conv), 6th (FC), 7th (FC) layer, and all
the layers for perturbation. The results are shown in Figure 10. We
have the following observations.
0.250.5 1   2   4   808590951000.80.850.90.95100.511.522.5Attack Success Rate (%)0.250.5 1   2   4   x10  -3  Setting of Parameter x10  -3   Misclassification Accuracy (%)0.250.5 1   2   4   PSSC(a)(b)x10  -3    ConfidenceTriggerNeighbor∆λ50 75 10012515000.511.522.580859095100Attack Success Rate (%)25 50 1001251500.80.850.90.951 Misclassification      ConfidenceTriggerNeighbor50 75 100125150SCNumber of System Tuning Epochs  (a)(b) Accuracy (%)∆Perturbed Layers 0246810Accuracy (%)7580859095100Attack Success Rate (%)Conv-4 Conv-5 FC-6 FC-7 All Conv-4 Conv-5 FC-6 FC-7 All PSSCTriggerNeighbor(a)(b)∆If we choose layers close to the input layer (e.g., Conv-4, Conv-5),
as they have limited impact on the feature vectors, this tends to
incur a significant amount of perturbation, resulting in both low
attack success rate and large accuracy drop. If we choose layers close
to the output layer (e.g., FC-7), as they directly influence the feature
vectors, often only a small amount of perturbation is sufficient, as
observed in Figure 10 (b). However, the perturbation may be easily
“flushed” by the back propagation operations during system fine-
tuning, due to their closeness to the output layer, resulting in low
attack success rate, as observed in Figure 10 (a). Thus, the optimal
layer to perturb is often one of the middle layers (e.g., FC-6).
6.3 Case Study III: Face Verification
We now apply model-reuse attacks to face verification, another
security-critical application, in which the system decides whether a
given facial image belongs to one particular person in its database.
Experimental Setting. In this case study, we use the VGG-Very-
Deep-16 model [46], which is pre-trained on the VGGFace dataset [46]
consisting of the facial images of 2,622 identities. The model achieves
96.5% accuracy on this dataset.
We then integrate the feature extractor of this model with a
classifier (1 FC layer + 1 SM layer) and adapt the system to a dataset
extracted from the VGGFace2 dataset [13], which consists of 25,000
facial images belonging to 500 individuals. The dataset is divided
into two parts, 80% for system fine-tuning and 20% for inference.
The genuine baseline system achieves the verification accuracy of
90.2% on the inference set.
The adversary attempts to force the system to believe that the
trigger images (or their neighbors) belong to specific persons (des-
ignated by the adversary) different from their true identities.
Effectiveness
Evasiveness
∆Accuracy
(VGGFace)
∆Accuracy
(VGGFace2)
θ
Attack
Success Rate
Misclassification
Confidence
83%/96%
94%/100%
97%/100%
67%/100%
0.65
0.8%
0.80
0.5%
0.95
0.3%
0.99
0.2%
Table 7. Impact of parameter selection threshold θ.
0.873
0.884
0.903
0.912
0.4%
0.4%
0.2%
0.1%
Figure 11: Impact of perturbation magnitude λ.
of 1. Also notice that, with reasons similar to case studies I and
II, the attack effectiveness is not a monotonic function of λ. It is
observed that the attack effectiveness drops sharply as λ exceeds
10−3. This is explained by that λ roughly controls the learning rate
in model perturbation, while overly large λ may causes overshoot-
ing at each optimization step, resulting in low attack effectiveness
(details in § 10.3).
We also measure the accuracy gap between the adversarial mod-
els and their genuine counterparts on the VGGFace and VGGFace2
datasets. Figure 11 (b) plots how this difference varies with λ. Ob-
serve that, under proper parameter setting (e.g., λ = 10−3), the
adversarial models are almost indiscernible from their genuine
counterparts, with accuracy differing around 0.3% and 0.65% on the
VGGFace and VGGFace2 datasets respectively.
7 ATTACKING ENSEMBLE SYSTEMS
Finally, we apply model-reuse attacks on ensemble ML systems. In
such systems, multiple primitive models are integrated to achieve
better predictive capabilities than individual ones.
Parameter Selection. Table 7 summarizes how the setting of pa-
rameter selection threshold θ influences the attack effectiveness and
evasiveness. We have the following observations. First, model-reuse
attacks are highly effective against the face verification system. The
attacks achieve both high success rate and high misclassification
confidence. For instance, with θ = 0.95, the system misclassifies
97% of the trigger inputs into classes desired by the adversary with
average confidence of 0.903. Second, both the attack effectiveness
and evasiveness increase monotonically with θ. However, overly
large θ (e.g., 0.99) results in low attack success rate (e.g., 67%), for
only a very small number of parameters satisfy the overly strict
threshold (see § 4).
Perturbation Magnitude. Figure 11 (a) shows how the attack ef-
fectiveness varies with the setting of perturbation magnitude λ. The
results show that under proper setting (e.g., λ = 10−3), with over
95% of the trigger inputs (and their neighbors) are misclassified into
classes desired by the adversary, with misclassification confidence
Figure 12: Sample images captured by multi-view cameras
mounted on autonomous vehicles.
Specifically, we focus on the application of autonomous driving.
Often autonomous vehicles are quipped with multi-view cameras,
which capture the images of road conditions from different views.
Figure 12 shows a sample scene comprising images taken from three
different views. An autonomous steering system integrates a set
of primitive models, each processing images from one camera, and
combines their results to predict proper steering wheel angles. Fig-
ure 13 illustrates a schematic design of such systems: three feature
extractors fl , fc, and fr extract features from the images captured
by the left, center, and right camera respectively; the features are
then combined and fed to the regressor д to predict the steering
wheel angle.
When applying model-reuse attacks on such ensemble ML sys-
tems, we consider the case of a single adversarial model as well as
that of multiple colluding adversarial models.
7580859095100Attack Success Rate (%)0.250.5 1   2   4   Setting of Parameter 0.250.5 1   2   4   x10  -3  0.750.80.850.90.9510.250.5 1   2   4   x10  -3  00.20.40.60.81 Accuracy (%)(a)(b)VGGFaceVGGFace2 Misclassification  Confidencex10  -3  λTriggerNeighbor∆LeftCenterRightFigure 13: Design of an ensemble steering system.
Experimental Setting. We consider two types of feature extrac-
tors, AlexNet [33] and VGG-16 [54], both of which are pre-trained
on the ImageNet dataset [49], attaining the top-5 accuracy of 80.2%
and 90.1% respectively. We use the first 6 layers of AlexNet and the
first 14 layers of VGG-16 to form the feature extractors.
Following the Nvidia DAVE-2 architecture [55], we use 3 FC lay-
ers as a regressor, which, in conjunction with the feature extractors,
form an end-to-end steering system. As shown in Figure 13, it takes
as inputs the images captured by the left, center, and right cameras
and predits the steering wheel angles.
We use the Udacity self-driving car challenge dataset1 for system
fine-tuning and inference. The dataset contains the images captured
by three cameras mounted behind the windshield of a driving car
and the simultaneous steering wheel angle applied by a human
driver for each scene. Figure12 shows a sample scene.
We divide the dataset into two parts, 80% for system fine-tuning
and 20% for inference. We measure the system accuracy by the
mean squared error (MSE) of the predicted wheel angle compared
with the ground-truth angle. After full-tuning, the genuine baseline
system achieves the MSE of 0.018.
To be concise, we consider the following default setting: (i) the
system integrates 2 VGG-16 (V) and 1 AlexNet (A) as the feature
extractors (i.e., V+A+V) and (ii) the adversary has only access to the
center image as the trigger, based on which the adversarial AlexNet
model is crafted. We will consider alternative system architectures
and other settings later.
case studies of I, II, and III, the attack effectiveness does not grow
monotonically with λ.