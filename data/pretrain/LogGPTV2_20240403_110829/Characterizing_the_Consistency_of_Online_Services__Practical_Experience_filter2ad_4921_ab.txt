3.9 Distribution of Monotonic Reads anomalies per test.
. . . . . . . .
3.10 Distribution of Writes Follow Reads anomalies per test.
. . . . . .
3.11 Percentage of tests with content divergence anomalies. . . . . . . .
3.12 Percentage of tests with order divergence anomalies.
. . . . . . . .
3.13 Cumulative distribution of content divergence windows. . . . . . .
3.14 Tests where an anomaly of content divergence was observed, but
. . . . . . . . . . . . . . . . . . . . . . .
where the window is zero.
3.15 Cumulative distribution of order divergence windows. . . . . . . .
8
12
13
14
15
16
18
29
31
32
33
35
37
38
39
41
42
43
43
44
45
46
xvii
List of Figures
3.16 Tests where an anomaly of order divergence was observed in Google+,
but where the window is zero.
. . . . . . . . . . . . . . . . . . . . .
3.17 Tests where an anomaly of order divergence was observed in Face-
book Feed, but where convergence was not reached during the test.
4.1 Arquitecture overview . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Service get operation, returns N elements of the list
. . . . . . . .
4.3 Middleware architecture . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Combinations anomaly . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Time Gap
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.6 Middleware with adapters . . . . . . . . . . . . . . . . . . . . . . .
4.7 Latency of Get Operation in Facebook . . . . . . . . . . . . . . . .
4.8 Latency of Insert Operation in Facebook . . . . . . . . . . . . . . .
4.9 Communication overhead in Facebook . . . . . . . . . . . . . . . .
4.10 Local storage overhead for Facebook . . . . . . . . . . . . . . . . .
4.11 Latency of Get Operation in Redis . . . . . . . . . . . . . . . . . . .
4.12 Latency of Insert Operation in Redis
. . . . . . . . . . . . . . . . .
4.13 Latency of Insert Operation in Redis in Ireland . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
4.14 Communication overhead in Redis
4.15 Local storage overhead for Redis
. . . . . . . . . . . . . . . . . . .
46
47
52
53
55
68
72
73
75
76
77
78
79
80
81
81
82
xviii
e
t
p
r 1
h
C
a
Introduction
Many computer systems and applications make use of stateful services that run
in the cloud, with various types of interfaces mediating the access to these cloud
services. For instance, an application may decide to store its persistent state in
a Cassandra [27, 46] cluster running on Azure instances, or directly leverage a
cloud storage service such as Amazon S3 [9]. At a higher level of abstraction,
services such as Twitter [66] or Facebook [36] have not only attracted billions
of users to their main websites, but have also enabled a myriad of popular
applications that are layered on top of those services by leveraging the public
Application Programming Interface (API) they provide [33, 40, 65], for third
party application developers.
An important challenge that arises from layering applications on top of
cloud APIs (that can either be storage or application-speciﬁc APIs) is that the
consistency semantics of these cloud services are typically not clearly speciﬁed,
with studies showing that in practice these services can expose a number of
consistency anomalies to applications [50].
This poses several challenges to the programmers who use APIs from such
services: it becomes diﬃcult to understand the impact of these consistency
semantics on the applications that are layered on top of them; and it forces
programmers to modify their code in order to mask or deal with the lack of
1
CHAPTER 1.
INTRODUCTION
desired semantics, which increases development time, complexity, and puts a
burden on the programmer of ensuring that the resulting semantics are correct.
In this work, we address this problem through a comprehensive approach
that starts by gaining an in-depth understanding of the semantics of online
services running in the cloud, and subsequently proposes a set of tools for im-
proving those semantics in a way that facilitates application development. In
particular, in the ﬁrst part of this thesis, we start by systematically understand-
ing what are the consistency guarantees that are eﬀectively oﬀered through the
APIs of some of these Internet services. This is important for two main reasons.
First, this allows us to better explain and understand the situations where the
behavior of the service may be counter-intuitive. Second, this is important to
help developers who design applications that interact and make use of these
services, to know what they can expect when using these APIs. This allows
those programmers to anticipate the eﬀects of using the service on their appli-
cations, and to determine if they need to introduce additional logic to mask
any undesirable behavior.
For understanding the consistency levels of online service APIs, this work
presents the results of a measurement study of the consistency of three popular
platforms: Facebook, Google+, and Blogger. Our methodology for conducting
this study started by identifying a set of anomalies that are not allowed by
several consistency deﬁnitions. We then designed two black box tests [44] that
probe a given system (through its API) in search of manifestations of these
anomalies. We implemented these tests and conducted an extensive consis-
tency measurement experiment in the platforms mentioned above, including
two variants of the Facebook API: Facebook Feed and Facebook Group services.
Our study lasted for about a month for each service with a total of 8183
individual tests being executed. Our main ﬁndings can be summarized as fol-
lows. We found a large prevalence of consistency anomalies in all services with
the exception of Blogger. Furthermore, Google+ and Facebook Feed exhibited
all anomalies we consider, whereas Facebook Groups exhibited only a subset
of them.
Some of these results can be seen as a somewhat natural and even expected
2
consequence of choosing performance over stronger consistency models such
as linearizability. In particular, when the designers of replication protocols
choose to provide a fast response to the clients of the service after contacting
only one or a small subset of replicas, the implication of this choice is that the
consistency model oﬀered by the service has to provide some form of weak
consistency, namely one where content divergence is possible, since two writes
that were processed by two diﬀerent replicas may have executed without being
aware of each other. We note that our results show an exception to this design
principle in the Blogger system, which appears to be oﬀering a form of strong
consistency. This can be seen as a sensible design choice considering the write
rate and the size of the user base of Blogger.
Overall, one of the most surprising results of our study was to ﬁnd several
types of anomalies that are not inevitable, even when choosing performance
over consistency. This is the case of session guarantees, which previous work
has shown how to implement, even under weak consistency [64].
Given that some of these services are not providing several session guaran-
tees, according to the results of our study, the relevant question that ensues is
what provisions must applications make in order to handle this fact, especially
if their application logic could beneﬁt from providing these guarantees. This
will imply that developers must enforce session guarantees at the application
level, which can be challenging due to the fact developers interact with the
service as if it was a black box. Note that some applications may not need to
enforce all session guarantees, this choice can be made based on the character-
istics of the application. For example, in a social application that targets events,
where a single person posts messages about an event, e.g., sports game or a
conference, and several people are reading the messages from the feed, it may
be enough to return the posts in the order they were issued and that repetitive
reads return an increasing number of posts. In this case, it will be necessary to
enforce only two session guarantees.
To address this, in the second part of this work, we show that it is possible
to build a middleware layer mediating the access to cloud services oﬀering
ﬁne-grained control over the consistency semantics exposed to applications,
3
CHAPTER 1.
INTRODUCTION
by enriching the properties originally oﬀered by these services. The idea is
to design a library that intercepts every call to the service or storage system
running in the cloud, inserting relevant metadata, calling the original API,
and transforming the results that are obtained in a transparent way for the
application. Through a combination of analyzing this metadata and caching
results that have been previously observed, this shim layer can then enforce
ﬁne-grained consistency guarantees.
In prior work, Bailis et al. [16] have proposed a similar approach, but with
two main limitations compared to this work. First, their shim layer only pro-
vides a coarse-grained upgrade from eventual to causal consistency. In contrast,
we allow programmers to turn on and oﬀ individual session guarantees, where
diﬀerent guarantees have been shown to be useful to diﬀerent application sce-
narios [64]. Second, their work assumes the underlying (cid:104)key,value(cid:105) store is a
NoSQL system with a read/write interface. Such an assumption simpliﬁes the
development of the shim layer, since (1) it gives the layer full access to the data
stored in the system, and (2) it provides an interface with simple semantics.
Our shim layer allows for a ﬁne-grained control over the session guarantees
that applications should perceive when accessing online services. A challenge
that arises is that these services typically enforce rate limits for operations
issued by client applications. For guaranteeing that this limit is the same when
using our shim layer, a single service operation should be executed for each
application operation. Furthermore, our layer is not limited to using online
storage services with a read/write interface, since it is designed to operate with
services that oﬀer a messaging interface such as online social networks. The
combination of these three requirements raises interesting challenges from the
perspective of the algorithms that our shim layer implements, e.g., to handle
the fact that online social networks only return a subset of recent messages,
which raises the question of whether a message does not appear because of
a lack of a session guarantee or because of being truncated out of the list of
recent messages.
We implemented our shim layer and integrated it with the Facebook API
and the Redis storage system. Our evaluation shows that our layer allows for
4
1.1. CONTRIBUTIONS
ﬁne-grained consistency upgrades at a modest latency overhead.
1.1 Contributions
The main contributions of this thesis are as follows:
• A generic methodology to probe services and ﬁnd consistency anomalies;
• A measurement study characterizing the consistency of online services;
• A set of algorithms to provide session guarantees;
• A transparent middleware to provide ﬁne-grained consistency upgrades
for online services.
1.2 Publications
The main contributions of this thesis were published in:
Characterizing the Consistency of Online Services (Practical Experience Re-
port). Filipe Freitas, João Leitão, Nuno Preguiça, and Rodrigo Rodrigues. Pro-
ceedings of the 46th Annual IEEE/IFIP International Conference on Depend-
able Systems and Networks (DSN2016)
Fine-Grained Consistency Upgrades for Online Services. Filipe Freitas, João
Leitão, Nuno Preguiça, and Rodrigo Rodrigues. Proceedings of the 36th IEEE
International Symposium on Reliable Distributed Systems (SRDS2017)
1.3 Document Organization
The remainder of the document is organized as follows. We describe the context
of this work and survey related work in Chapter 2. We explain and show the
results obtained in our measurement study in Chapter 3. We describe and
show the evaluation of our middleware to provide ﬁne-grained consistency in
Chapter 4. Finally, conclusions and future work are presented in Chapter 5.
5
e
t
p
r 2
h
C
a
Background and Related Work
In this chapter, we describe the context of our work and survey the related work
that is closest to our consistency measurement study and to our middleware.
2.1 Online Services
Online services are distributed systems used across the world through the
Internet. These services are composed of several layers [62], each one with a
single responsibility, a service architecture is typically divided into three layers
(see Figure 2.1):
Data layer: This layer is responsible to store and retrieve data, and either
contains databases such as SQLServer [61] and MySQL [54] or uses cloud data
services like Amazon S3 [9].
Processing layer: This layer is responsible to implement the service logic
and is composed of application servers that may interact with several internal
or external services.
Interface layer: This layer is responsible to provide an interface for clients
to operate with the service and is composed of servers, typically HTTP [12],
that provide access to Web APIs.
7
CHAPTER 2. BACKGROUND AND RELATED WORK
Figure 2.1: Service layers
In order to provide online services to clients across the globe, geo-replication
is used both for dependability and good performance (in particular low latency
access) [28, 32]. The dependability motivation stems from the ability to toler-
ate catastrophic failures by having data replicated at multiple sites, whereas
the performance gains come from being able to direct users to nearby copies of
the data they want to access.
Geo-replication implies replication at the three layers. Applying replication
at the interface and processing layers is usually simple because these servers
work in a stateless manner. In contrast, replicating at the data layer is complex
because it is necessary to replicate the data that comprises the service state,
which must be synchronized across replicas.
The price to pay for geo-replication is that the designers of these infras-
tructures have to deal with an inherent trade-oﬀ between performance and
consistency, [63]. In particular, if they choose to provide a strongly consistent