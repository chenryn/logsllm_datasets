### V. Efficient Detection with Trained MNTD

The trained MNTD can be applied to a target model in just a few milliseconds. In contrast, other methods require the entire algorithm to be re-run for each new model. This makes our approach significantly more efficient when the defender needs to detect Trojans in multiple target models on the same task. Additionally, as shown in Section VI-C, the model consumer can generate a smaller number of shadow models to balance computational overhead and Trojan detection performance.

### VII. Generalization on Unforeseen Trojans

In this section, we evaluate the jumbo MNTD using Trojaned models that were not included in the jumbo distribution during training. Unless otherwise specified, we train 256 Trojaned models for each attack setting, and we have empirically verified that all attack settings successfully install Trojans in the models.

#### A. Generalization on Trigger Patterns

We first evaluate the meta-classifier using unforeseen trigger patterns. We collect various Trojan shapes used in previous works [38], [21] and some newly designed shapes, as shown in Table V. A significant difference between these shapes and those in jumbo learning is that they change a larger number of pixels (up to 5×5). For each pattern type, we use the same mask but with randomly generated pixel values. We then train the Trojaned models with these patterns and apply the jumbo MNTD pipeline to detect them.

**Table V: Examples of Unforeseen Trojan Trigger Patterns and Detection AUC**

| Trojan Shape | MNIST Trojaned Example | MNIST Detection AUC | CIFAR-10 Trojaned Example | CIFAR-10 Detection AUC |
|--------------|------------------------|---------------------|---------------------------|------------------------|
| Apple        | [Image]                | 96.73%              | [Image]                   | 89.38%                 |
| Corners      | [Image]                | 98.74%              | [Image]                   | 93.09%                 |
| Diagonal     | [Image]                | 99.80%              | [Image]                   | 97.57%                 |
| Heart        | [Image]                | 99.01%              | [Image]                   | 93.82%                 |
| Watermark    | [Image]                | 99.93%              | [Image]                   | 97.32%                 |

The results in Table V show that the trained meta-classifier achieves similar detection performance as before, indicating that our approach generalizes well to a variety of trigger patterns, even those not considered during training. Further experimental results on the generalization to unforeseen trigger patterns are provided in Appendix F.

#### B. Generalization on Malicious Goals

All models in the jumbo distribution aim at a single target attack, where the label of a Trojaned input is changed to a specific class. Here, we consider another type of malicious behavior, the all-to-all attack. For a c-way classification model, the label of a Trojaned input originally belonging to the i-th class will be changed to the ((i + 1) mod c)-th class. We evaluate whether our meta-classifier can detect Trojaned models with all-to-all attacks.

For MNIST, we add a four-pixel pattern at the right bottom corner, following the same setting as in [12]. For other tasks, we use the same attack setting as the modification attacks and change the attack goal to all-to-all. Empirically, we find that the all-to-all attack does not work well on SC and MR, so we exclude them from the detection task.

**Table VI: Detection AUC of Each Approach Against All-to-All Attack**

| Approach          | MNIST | CIFAR10 | Irish |
|-------------------|--------|----------|-------|
| AC                | 90.94% | 77.41%   | ≤50%  |
| NC                | 52.34% | 51.46%   | ≤50%  |
| Spectral          | ≤50%   | ≤50%     | ≤50%  |
| STRIP             | 62.60% | 70.38%   | ≤50%  |
| MNTD (One-class)  | 97.09% | 98.62%   | 99.95%|
| MNTD (Jumbo)      | 99.98% | 100.00%  | 100.00%|

The results in Table VI show that NC and STRIP do not perform well in detecting all-to-all Trojans. Our approach, however, achieves good detection performance, with over 98% AUC for all three tasks. Although AC outperforms us on MNIST by 0.05%, we outperform AC by 20% and 10% on the other two tasks. Moreover, our method does not require access to the dataset, unlike AC. These results demonstrate that our detection pipeline is robust and generalizable across different tasks.

#### C. Generalization on Attack Approaches

In the jumbo distribution, we use poisoning attacks to generate Trojaned models. However, there are four types of attacks introduced in Section II-C, and only modification and blending attacks insert Trojans by poisoning the dataset. In this section, we evaluate how the meta-classifier performs in detecting the other two types of unforeseen attack approaches.

We evaluate the parameter and latent attacks on vision tasks, as the attack success rate is typically low on other tasks. For parameter attacks, we add a 4×4 pattern for MNIST and an 8×8 pattern for CIFAR10. For latent attacks, we follow the same setting in [57] and add a 5×5 pattern for both tasks. The shadow and target models in latent attacks are fine-tuned to the user’s task before detection.

**Table VII: Detection AUC of MNTD and Neural Cleanse on Parameter and Latent Attacks**

| Approach          | CIFAR10-P | CIFAR10-L | MNIST-P | MNIST-L |
|-------------------|-----------|-----------|---------|---------|
| NC                | ≤50%      | 95.02%    | ≤50%    | 98.83%  |
| MNTD (One-class)  | 83.79%    | 92.78%    | 99.99%  | 99.07%  |
| MNTD (Jumbo)      | ≤50%      | 98.87%    | 53.12%  | 98.87%  |

The results in Table VII show that our model can effectively detect these Trojaned models. Notably, the latent attack was introduced after our initial proposal, and we did not tailor our method to detect it, demonstrating the good generalizability of our approach.

#### D. Generalization on Model Structures

In Section VI-B, we evaluated MNTD under the assumption that the defender knows the target model architecture. However, in some cases, the defender may not have this knowledge. To address this, we evaluate how MNTD performs when generalizing to unforeseen model structures.

We perform our evaluation on the ImageNet dataset, which includes many different model structures. Specifically, we use the dog-vs-cat subset, a binary classification task with 20,000 training cases and 5,000 testing cases. We assume the defender owns 10% of the training set, while the attacker owns 50%.

To evaluate generalization, we use six different model structures: (1) ResNet-18, (2) ResNet-50, (3) DenseNet-121, (4) DenseNet-169, (5) MobileNet v2, and (6) GoogLeNet. We use one of these structures as the target model at a time. The attacker generates 32 target models (16 benign and 16 Trojaned) for each structure. The defender trains 64 shadow models (32 benign and 32 Trojaned) for each of the other five structures, generating 640 shadow models in total. The experiment is repeated for each of the six structures.

**Table VIII: Detection AUC of MNTD on Unforeseen Model Structures**

| Model Structure  | Detection AUC |
|------------------|---------------|
| ResNet-18        | 81.25%        |
| ResNet-50        | 83.98%        |
| DenseNet-121     | 89.84%        |
| DenseNet-169     | 82.03%        |
| MobileNet v2     | 87.89%        |
| GoogLeNet        | 85.94%        |

The results in Table VIII show that all AUCs are higher than 80%, indicating good transferability even on complex tasks like ImageNet. By training more shadow models, the results could be further improved, as discussed in Section VI-C. This demonstrates that MNTD is applicable to complex tasks and generalizes well to unforeseen model structures.

#### E. Generalization on Data Distribution

In previous studies, we assumed that the data collected by the defender follows the same distribution as the model’s training data. Here, we study the case where the defender uses alternative data that is similar but not identical in distribution.

We consider two alternatives for the MNIST and CIFAR datasets: the USPS digit dataset and the TinyImageNet dataset. The USPS dataset includes 16×16 grayscale images of digits 0-9, reshaped to 28×28. The TinyImageNet contains 64×64 images of 200 classes, reshaped to 32×32. We hand-picked 10 classes to correspond to the labels in CIFAR-10.

In the experiments, we train the shadow models using the USPS and TinyImageNet datasets instead of the 2% of MNIST and CIFAR-10. The models trained on these alternative datasets achieve 81.63% accuracy on MNIST and 33.97% on CIFAR-10. We then train a meta-classifier and evaluate it on the target models of MNIST-M, MNIST-B, CIFAR10-M, and CIFAR10-B.

**Table IX: Detection AUC Using Alternative Datasets**

| Dataset         | MNIST-M | MNIST-B | CIFAR10-M | CIFAR10-B |
|-----------------|----------|----------|------------|------------|
| USPS            | 98.82%   | 99.57%   | -          | -          |
| TinyImageNet    | -        | -        | 83.41%     | 93.78%     |

The results show that the meta-classifier still achieves good detection performance, though slightly worse than when using the same data distribution. This indicates that the defender can use alternative datasets to train the shadow models.

### VIII. Adaptive Attack and Countermeasure

In this section, we consider a strong adaptive attacker who attempts to evade MNTD and propose a countermeasure to make our system robust against such attacks.

#### A. Strong Adaptive Attack

We consider a strong attacker who has full knowledge of the detection pipeline, including the specific parameters of the meta-classifier META and the tuned query input set {x1, ..., xk}. The attacker's goal is to construct a Trojaned model that will be classified as benign by MNTD.

With full knowledge of the MNTD system, the attacker can evade detection by incorporating the prediction of MNTD into the training process. The original training loss for a Trojaned model f is Ltrain, and the goal is:

\[
\min_{f} L_{\text{train}}(f)
\]

For example, on classification tasks, Ltrain is the mean cross-entropy loss between model predictions and ground truth labels over all benign and Trojaned data. We denote Lmal as the output of the meta-classifier on f:

\[
L_{\text{mal}}(f) = \text{META}(F(f); \theta)
\]
\[
F(f) = [f(x_1) || \ldots || f(x_k)]
\]

A large Lmal indicates that the model is evaluated as Trojaned by the MNTD system. The attacker can explicitly add Lmal to the training process and change the training goal to:

\[
\min_{f} L_{\text{train}}(f) + \lambda \cdot L_{\text{mal}}(f)
\]

where λ is a chosen parameter balancing model performance and evasion success rate. In practice, we use λ = 1.0, which works well for adaptive attacks. The Trojaned model can always evade MNTD detection while incurring only a negligible decrease in model accuracy and attack success rate.

#### B. Countermeasure - MNTD-robust

The key challenge in the strong adaptive attack is that the adversary has full access to the meta-classifier parameters and query inputs. To counteract this, we propose a robust version of our system, MNTD-robust.

The core idea of MNTD-robust is to randomize part of the system parameters during test time. This prevents the attacker from knowing the exact parameters and thus from calculating the exact value of Lmal. Specifically, we introduce randomness in the query inputs and meta-classifier parameters, making it difficult for the attacker to optimize the Trojaned model to appear benign.

**Table X: Detection AUC of MNTD-robust and Its Performance Against Strong Adaptive Attack**

| Approach           | MNIST-M | MNIST-B | CIFAR10-M | CIFAR10-B | SC-M | SC-B | Irish-M | Irish-B | MR-M |
|--------------------|----------|----------|------------|------------|------|------|---------|---------|------|
| MNTD-robust        | 99.54%   | 81.86%   | 96.97%     | 94.83%     | 96.61% | 91.88% | 99.92%  | 97.27%  | 94.78% |
| MNTD-robust (under attack) | 99.37% | 88.54% | 84.39%     | 75.60%     | 88.86% | 90.45% | 96.81%  | 88.79%  | 97.27% |

The results in Table X show that MNTD-robust maintains high detection performance even under strong adaptive attacks. This demonstrates the effectiveness of our countermeasure in making the system robust against sophisticated adversaries.