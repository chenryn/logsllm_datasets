only several milliseconds to apply the trained MNTD on one
target model. In contrast, other approaches have to re-run their
entire algorithm whenever they are provided with a new model.
Therefore, our approach is more efﬁcient when the defender
needs to detect Trojans on a number of target models on
the same task. Moreover, as demonstrated in Section VI-C,
the model consumer could also generate a smaller number
of shadow models to make a trade-off between computation
overhead and Trojan detection performance.
VII. GENERALIZATION ON UNFORESEEN TROJANS
In this section, we will evaluate jumbo MNTD using Tro-
janed models that are not modelled by the jumbo distribution
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
111
TABLE V: Examples of unforeseen Trojan trigger patterns and the detection AUC of jumbo MNTD on these Trojans.
Trojan
Shape
Apple
Corners
Diagonal
Heart
Watermark
Pattern Mask
MNIST
Trojaned Example Detection AUC Pattern mask
CIFAR-10
Trojaned Example Detection AUC
96.73%
98.74%
99.80%
99.01%
99.93%
89.38%
93.09%
97.57%
93.82%
97.32%
in the training process. Unless speciﬁed, we will train 256 Tro-
janed models for each attack setting. We empirically veriﬁed
that all the attack settings are successful to install Trojans in
the model.
A. Generalization on Trigger Patterns
We ﬁrst evaluate the meta-classiﬁer using unforeseen trigger
patterns. We collect some Trojan shapes on vision tasks used
in previous works [38], [21] as well as some newly designed
Trojan shapes, as shown in Table V. A signiﬁcant difference
between these shapes and those in jumbo learning is that they
will change a large number of pixels; in contrast, the patterns
in jumbo learning change at most 5×5 pixels. For each pattern
type, we use the same mask but with randomly generated pixel
values. We train the Trojaned models with these patterns and
apply the jumbo MNTD pipeline to detect them.
The detection results are shown in Table V. We can see that
the trained meta-classiﬁer achieves similar detection results as
before. This shows that our detection approach generalizes
well to a variety of trigger patterns even if they are not con-
sidered in the training process. We provide further experiment
results on the generalization to unforeseen trigger patterns in
Appendix F.
B. Generalization on Malicious Goals
All the models in the jumbo distribution aims at the single
target attack, i.e., change the label of a Trojaned input to be a
speciﬁc class. Here, we consider another type of malicious be-
havior, all-to-all attack. In particular, for a c-way classiﬁcation
model, the label of a Trojaned input which originally belongs
to the i-th class will be changed to the ((i + 1) mod c)-th
class. We will evaluate whether our meta-classiﬁer can detect
Trojaned models with all-to-all attack. For MNIST, we adopt
the same setting as in [12] and add a four-pixel pattern at the
right bottom corner. For other tasks, we use the same attack
TABLE VI: The detection AUC of each approach against all-to-all
Attack.
AC
NC
Spectral
STRIP
Approach
MNTD (One-class)
MNTD (Jumbo)
Irish
90.94%
MNIST
CIFAR10
100.00% 77.41%
52.34%
51.46%
≤50%
84.36%
68.02%
≤50%
≤50%
62.60%
70.38%
97.09%
99.98%
98.62% 100.00%
99.95%

setting as the modiﬁcation attacks on them and change the
attack goal to be all-to-all attack. We empirically ﬁnd that the
all-to-all attack cannot work on SC and MR (the attack success
rate is low), so we do not include them as in the detection task.
The results are shown in Table VI. We see that NC and
STRIP cannot perform well in detecting these kinds of Trojans,
as we have discussed in Section III-D. Our approach still
achieves a good performance in detecting this unforeseen type
of malicious goal, reaching over 98% detection AUC for all
the three tasks. AC outperforms us on MNIST by 0.05% while
we outperform them by 20% and 10% on the other two tasks.
In addition, we do not require access to the dataset as AC
does. These results show that our detection pipeline requires
a weak assumption and is general to different tasks.
C. Generalization on Attack Approaches
In the jumbo distribution we use poisoning attack and
change the training dataset
to generate Trojaned models.
However, we introduced four types of attacks in Section II-C
of which only modiﬁcation attack and blending attack will
insert Trojan by poisoning the dataset. In this section, we
will evaluate how the meta-classiﬁer performs in detecting the
other two kinds of unforeseen attack approaches.
We evaluate the two attacks on vision tasks since we
empirically ﬁnd that the attack success rate is usually low on
other tasks. For parameter attack, we add a 4 × 4 pattern for
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
112
TABLE VII: The detection AUC of MNTD and neural cleanse on
parameter attack (denoted by -P) and latent attack (denoted by -
L). Other input-level and dataset-level detection techniques are not
included as they cannot be applied in detecting these attacks.
TABLE VIII: The detection AUC of MNTD on unforeseen model
structures on ImageNet Dog-vs-Cat. The meta-classiﬁer for each
model structure are trained using all models except the ones in target
model structure.
Approach
NC
CIFAR10-P
CIFAR10-L
MNIST-P MNIST-L
≤50%
95.02%
≤50%
98.83%
99.99%
99.07%
53.12%
≤50%
98.87%
MNTD(one-class)
MNTD(Jumbo)
83.79%
≤50%
92.78%
MNIST and 8 × 8 for CIFAR10. For latent attack, we follow
the same setting in [57] and add a 5×5 pattern for both tasks.
The shadow and target models in latent attack are ﬁne-tuned
to the user’s task before we perform our detection.
The results are shown in Table VII. Note that the attack will
not poison the training dataset, so AC, Spectral and STRIP
cannot be applied to detect such Trojans. We see that our
model can detect these Trojaned models well. In addition, we
emphasize that the latent attack appears after we ﬁrst proposed
our pipeline and we did not tailor our method in order to detect
it. This shows that our approach has good generalizability in
detecting unforeseen Trojan attack approaches.
D. Generalization on Model Structures
In Section VI-B, we evaluate MNTD with the assump-
tion that the defender knows the target model architecture.
However, in some cases the defender may not have such
knowledge. This problem might be solved by existing tech-
niques which infer the structure of a black-box model [45].
Nevertheless, we would like to evaluate how MNTD performs
when generalizing to unforeseen model structures.
We perform our evaluation on the more complicated dataset
ImageNet [18], as many different structures have been pro-
posed to achieve a good performance on ImageNet. In par-
ticular, we use its subset on dog-vs-cat, which is a binary
classiﬁcation task between dogs and cats, containing 20,000
training cases and 5,000 testing cases. Here we assume that
the defender owns 10% of the training set instead of 2% in
previous assumption, since the ImageNet models are more
difﬁcult to train; the attacker still owns 50%.
To evaluate the generalization to unforeseen model struc-
tures, we use six different structures in the model pool: (1)
ResNet-18 [24], (2) ResNet-50 [24], (3) DenseNet-121 [25],
(4) DenseNet-169 [25],
(5) MobileNet v2 [47] and (6)
GoogLeNet [50]. We use one of the six structures as the
target model at a time. The attacker will generate 32 target
models of the structure, 16 benign and 16 Trojaned using
jumbo distribution. The defender will train 64 shadow models
(32 benign and 32 jumbo Trojaned) for each of the other ﬁve
structures, which generates 640 shadow models in total. He
will use these shadow models to train the meta-classiﬁer and
detect whether the target models contain Trojan or not. Thus,
the target model structure will never be seen in the training
of the meta-classiﬁer. The experiment is repeated for each of
the six structures.
The results are reported in Table VIII. We see that all the
AUCs are higher than 80%, showing a good transferability
even on complicated tasks like ImageNet. Note that here we
ResNet-18
81.25%
ResNet-50
83.98%
DenseNet-121
89.84%
DenseNet-169 MobileNet
82.03%
87.89%
GoogLeNet
85.94%
only use 64 models for each structure to train the meta-
classiﬁer due to time efﬁciency. According to Section VI-C,
the results in Table VIII could be further improved by training
more shadow models. The results demonstrate that MNTD
is applicable to complicated tasks and generalizes well to
unforeseen model structures.
E. Generalization on Data Distribution
In previous studies, we assume that the data collected by the
defender follows the same distribution as the model’s training
data. In this section, we study the case where the defender use
alternative data which is similar but not the same distribution.
We consider two alternatives for MNIST and CIFAR dataset -
USPS digit dataset [27] and TinyImageNet dataset [18]. The
USPS dataset includes 16 × 16 grayscale images of digit 0-9.
It contains 7291 train and 2007 test images. We will reshape
the images into 28 × 28 so that it is same as MNIST. The
TinyImageNet contains 64 × 64 images of 200 classes where
each class has 500 training, 50 validation and 50 test images.
We hand-picked 10 classes to correspond to the labels in
CIFAR-10. The chosen classes are shown in Appendix G. We
reshape the images into 32× 32 to be the same as CIFAR-10.
In the experiments, we will train the shadow models using
the USPS and TinyImageNet dataset instead of the 2% of
MNIST and CIFAR-10 dataset. The models trained on these
alternative datasets can achieve 81.63% accuracy on MNIST
and 33.97% on CIFAR-10. Then we train a meta-classiﬁer and
evaluate them on the target models of MNIST-M, MNIST-
B, CIFAR10-M, CIFAR10-B as in Table III. We ﬁnd that
the meta-classiﬁer using USPS achieves 98.82% detection
AUC on MNIST-M and 99.57% on MNIST-B; meta-classiﬁer
using TinyImageNet achieves 83.41% AUC on CIFAR10-M
and 93.78% on CIFAR10-B. We can see that the the meta-
classiﬁer still achieves good detection performance, though it
is slightly worse compared with the case when we use the
same data distribution. This shows that the defender can use
an alternative dataset to train the shadow models.
VIII. ADAPTIVE ATTACK AND COUNTERMEASURE
In this section, we consider a strong attacker who adapts
their approach to evade MNTD and then extend our technique
to be robust to such attacks.
A. Strong Adaptive Attack
We consider a strong attacker who wishes to evade MNTD.
We assume that the adversary has full knowledge of the de-
tection pipeline, including the speciﬁc parameters of the meta-
classiﬁer META and the tuned query input set {x1, . . . , xk}.
The goal of the attacker is to construct a Trojaned model that
will be classiﬁed as benign by our MNTD.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
113
TABLE IX: The detection AUC of MNTD-robust and its detection performance against strong adaptive attack.
Approach
MNTD-robust
MNTD-robust
(under attack)
MNIST-M MNIST-B
99.54%
81.86%
99.37%
88.54%
CIFAR10-M CIFAR10-B
96.97%
94.83%
84.39%
75.60%
SC-M
96.61%
88.86%
SC-B
91.88%
90.45%
Irish-M Irish-B MR-M
99.92%
96.81%
94.78%
97.27%
99.97%
88.79%
With full knowledge of the MNTD system, the attacker can
evade the detection by incorporating the prediction of MNTD
in its training process. Suppose the original training loss for a
Trojaned model f is Ltrain and the goal is:
min
f
Ltrain(f )
(10)
For example, on classiﬁcation tasks, Ltrain is the mean cross
entropy loss between model predictions and ground truth
labels over all the benign and Trojaned data. We denote Lmal
to be output of the meta-classiﬁer on f. Recall that:
Lmal(f ) = META(F(f ); θ)
F(f ) = [[f (x1)|| . . .||f (xk)]]
(11)
(12)
Large Lmal indicates that the model is evaluated as Trojaned
by the MNTD system. Since the attacker has full knowledge of
the detection system, he can calculate Lmal during training and
aims to keep it small. In particular, the attacker can explicitly
add Lmal into the training process and change the training
goal as:
Ltrain + λ · Lmal
min
f
(13)
where λ is a chosen parameter balancing model performance
and evasion success rate. With full knowledge of the MNTD
system, the attacker can perform back-propagation to optimize
the loss function directly. In practice, we use λ = 1.0 which
works well for the adaptive attacks and we ﬁnd the result
not sensitive to this choice. In particular, the Trojaned model
can always evade the detection of MNTD using the attack
while incurring only negligible decrease in model accuracy
(i.e., utility) and attack success rate.
B. Countermeasure - MNTD-robust
The key point in the strong adaptive attack is that the
adversary has full access to the meta-classiﬁer parameters
and query inputs. Hence, he can intentionally optimize the
Trojaned model to make it look benign to the meta-classiﬁer.
In practice, the defender can avoid it by keeping the model
parameters as a secret. Nevertheless, we will propose a robust
version of our system, MNTD-robust, to counteract the strong
attacker with full knowledge of our system.
The core idea of MNTD-robust is that during test time we
will set part of the system parameters to be random values.
Hence, the attacker cannot know what the parameters are and
thus cannot calculate the exact value of Lmal. In particular, in