on its coarse-grained rules, we hook the operation and invoke the VMwall ker-
nel module for our additional application-level checks. We modiﬁed ebtables to
implement this hook, which passes a reference to the packet to VMwall.
Ebtables does not provide the ability to queue packets. Were it present, queu-
ing would enable ﬁlters present inside the kernel to store packets for future
processing and reinjection back into the network. To allow the VMwall kernel
module to queue packets currently under inspection by the user agent, we altered
ebtables to incorporate packet queuing and packet reinjection features.
5.2 Accessing DomU Kernel Memory
VMwall uses the XenAccess introspection library [31] to accesses domU kernel
memory from dom0. It maps domU memory pages containing kernel data struc-
tures into the virtual memory space of the user agent executing in the trusted
VM. XenAccess provides APIs that map domU kernel memory pages identi-
ﬁed either by explicit kernel virtual addresses or by exported kernel symbols. In
Linux, the exported symbols are stored in the ﬁle named System.map. VMwall
uses certain domU data structures that are exported by the kernel and hence
mapped with the help of kernel symbols. Other data structures reachable by
pointers from the known structures are mapped using kernel virtual addresses.
The domU virtual machine presented in Fig. 4 shows the internal mechanism in-
volved to map the memory page that contains the desired kernel data structure.
5.3 Parsing Kernel Data Structures
To identify processes using the network, VMwall must be able to parse high-
level kernel data structures from the raw memory pages provided by XenAccess.
Extracting kernel data structures from the mapped memory pages is a non-trivial
task. For example, Linux maintains a doubly-linked list that stores the kernel’s
private data for all running processes. The head pointer of this list is stored in
the exported kernel symbol init task. If we want to extract the list of processes
running inside domU, we can map the memory page of domU that contains the
init task symbol. However, VMwall must traverse the complete linked list and
hence requires the oﬀset to the next member in the process structure. We extract
this information oﬄine directly from the kernel source code and use these values
50
A. Srivastava and J. Giﬃn
Ports:
inet_hashinfo
hlist_head
hlist_head
hlist_head
Array iteration
hlist_node
hlist_node
hlist_node
Linked list iteration
Processes:
task_struct
task_struct
task_struct
Linked list iteration
ﬁles_struct
fdtable
sock
Match
ﬁle
ﬁle
ﬁle
Array iteration
Return:
Process information
from task_struct
Fig. 5. DomU Linux kernel data structures traversed by the VMwall user agent during
correlation of the process and TCP packet information
in the user agent. This source code inspection is not the optimal way to identify
oﬀsets because the oﬀset values often change with the kernel versions. However,
there are other automatic ways to extract this information from the kernel binary
if it was compiled with a debug option [18].
This provides VMwall with suﬃcient information to traverse kernel data struc-
tures. VMwall uses known ﬁeld oﬀsets to extract the virtual addresses of pointer
ﬁeld members from the mapped memory pages. It then maps domU memory
pages by specifying the extracted virtual addresses. This process is performed
recursively until VMwall traverses the data structures necessary to extract the
process name corresponding to the source or destination port of a network com-
munication. Figure 5 shows the list of the kernel data structures traversed by
the user agent to correlate a TCP packet and process information. First, it tries
to obtain a reference to the socket bound to the port number speciﬁed in the
packet. After acquiring this reference, it iterates over the list of processes to ﬁnd
the process owning the socket.
5.4 Policy Design and Rules
VMwall identiﬁes legitimate connections via a whitelist-based policy listing
processes allowed to send or receive data. Each process that wants to communi-
cate over the network must be speciﬁed in the whitelist a priori. This whitelist
resides inside dom0 and can only be updated by administrators in a manner
similar to traditional application-level ﬁrewalls. The whitelist based design of
VMwall introduces some usability issues because all applications that should be
allowed to make network connections must be speciﬁed in the list. This limita-
tion is not speciﬁc to VMwall and is inherent to the whitelist based products
and solutions [6, 12].
VMwall’s kernel module maintains its own rule table containing rules that
are dynamically generated by the user agent after performing introspection. A
rule contains source and destination port and IP address information, an action,
and a timeout value used by the kernel module to expire and purge old rules
for UDP connections. In the case of TCP connections, the kernel module purges
TCP rules automatically whenever it processes a packet with the TCP fin or
Tamper-Resistant, Application-Aware Blocking of Malicious Network
51
rst ﬂag set. In an abnormal termination of a TCP connection, VMwall uses the
timeout mechanism to purge the rules.
6 Evaluation
The basic requirement of an application-level ﬁrewall is to block connections to
or from malicious software and allow connections to or from benign applications.
We evaluated the ability of VMwall to ﬁlter out packets made by several dif-
ferent classes of attacks while allowing packets from known processes to pass
unimpeded. We tested VMwall against Linux-based backdoors, worms, and bots
that attempt to use the network for malicious activity. Section 6.1 tests VMwall
against attacks that receive inbound connections from attackers or connect out to
remote systems. Section 6.2 tests legitimate software in the presence of VMwall.
We measure VMwall’s performance impact in Sect. 6.3, and lastly analyze its
robustness to a knowledgeable attacker in Sect. 6.4.
6.1 Illegitimate Connections
We ﬁrst tested attacks that receive inbound connections from remote attackers.
These attacks are rootkits that install backdoor programs. The backdoors run as
user processes, listen for connections on a port known to the attacker, and receive
and execute requests sent by the attacker. We used the following backdoors:
– Blackhole runs a TCP server on port 12345 [22].
– Gummo runs a TCP server at port 31337 [22].
– Bdoor runs a backdoor daemon on port 8080 [22].
– Ovas0n runs a TCP server on port 29369 [22].
– Cheetah runs a TCP server at the attacker’s speciﬁed port number [22].
Once installed on a vulnerable system, attacks such as worms and bots may
attempt to make outbound connections without prompting from a remote at-
tacker. We tested VMwall with the following pieces of malware that generate
outbound traﬃc:
– Apache-ssl is a variant of the Slapper worm that self-propagates by opening
TCP connections for port scanning [23].
– Apache-linux is a worm that exploits vulnerable Apache servers and spawns
a shell on port 30464 [23].
– BackDoor-Rev.b is a tool that is be used by a worm to make network
connections to arbitrary Internet addresses and ports [20].
– Q8 is an IRC-based bot that opens TCP connections to contact an IRC
server to receive commands from the botmaster [14].
– Kaiten is a bot that opens TCP connections to contact an IRC server [24].
– Coromputer Dunno is an IRC-based bot providing basic functionalities
such as port scanning [13].
52
A. Srivastava and J. Giﬃn
Table 1. Results of executing legitimate software in the presence of VMwall. “Allowed”
indicates that the network connections to or from the processes were passed as though
a ﬁrewall was not present.
Name Connection Type Result
Allowed
Allowed
Allowed
Allowed
Allowed
Allowed
Allowed
Allowed
Allowed
Allowed
Allowed
Outbound
Outbound
Outbound
Outbound
Outbound
Outbound
Outbound
Outbound
Inbound
Inbound
Inbound
rcp
rsh
yum
rlogin
ssh
scp
wget
tcp client
thttpd
tcp server
sshd
VMwall successfully blocked all illegitimate connections attempted by mal-
ware instances. In all cases, both sending and receiving, VMwall intercepted the
ﬁrst SYN packet of each connection and passed it to the userspace component.
Since these malicious processes were not in the whitelist, the VMwall user space
component informed the VMwall kernel component to block these malicious con-
nections. As we used VMwall in packet queuing mode, no malicious packets were
ever passed through VMwall.
6.2 Legitimate Connections
We also evaluated VMwall’s ability to allow legitimate connections made by
processes running inside domU. We selected a few network applications and
added their name to VMwall’s whitelist. We then ran these applications inside
domU. Table 1 shows the list of processes that we tested, the type of connections
used by the processes, and the eﬀect of VMwall upon those connections. To be
correct, all connections should be allowed.
VMwall allowed all connections made by these applications. The yum applica-
tion, a package manager for Fedora Core Linux, had runtime behavior of interest.
In our test, we updated domU with the yum update command. During the pack-
age update, yum created many child processes with the same name yum and these
child processes made network connections. VMwall successfully validated all the
connections via introspection and allowed their network connections.
6.3 Performance Evaluation
A ﬁrewall verifying all packets traversing a network may impact the performance
of applications relying on timely delivery of those packets. We investigated the
performance impact of VMwall as perceived by network applications running
inside the untrusted virtual machine. We performed experiments both with and
Tamper-Resistant, Application-Aware Blocking of Malicious Network
53
Table 2. Introspection time (μs) taken by VMwall to perform correlation of network
ﬂow with the process executing inside domU
Conﬁguration
TCP Introspection Time UDP Introspection Time
Inbound Connection to domU
Outbound Connection from domU
251
1080
438
445
without VMwall running inside dom0. All experiments were conducted on a
machine with an Intel Core 2 Duo T7500 processor at 2.20 GHz with 2 GB RAM.
Both dom0 and domU virtual machines ran 32 bit Fedora Core 5 Linux. DomU
had 512 MB of physical memory, and dom0 had the remaining 1.5 GB. The
versions of Xen and XenAccess were 3.0.4 and 0.3, respectively. We performed our
experiments using both TCP and UDP connections. All reported results show
the median time taken from ﬁve measurements. We measured microbenchmarks
with the Linux gettimeofday system call and longer executions with the time
command-line utility.
VMwall’s performance depends on the introspection time taken by the user com-
ponent. Since network packets are queued inside the kernel during introspection,
the introspection time is critical for the performance of the complete system. We
measured the introspection time both for incoming and outgoing connections to
and from domU. Table 2 shows the results of experiments measuring
introspection time.
It is evident that the introspection time for incoming TCP connections is very
small. Strangely, the introspection time for outgoing TCP connections is notably
higher. The reason for this diﬀerence lies in the way that the Linux kernel stores
information for TCP connections. It maintains TCP connection information for
listening and established connections in two diﬀerent tables. TCP sockets in a
listening state reside in a table of size 32, whereas the established sockets are
stored in a table of size 65536. Since the newly established TCP sockets can be
placed at any index inside the table, the introspection routine that iterates on
this table from dom0 must search half of the table on average.
We also measured the introspection time for UDP data streams. Table 2 shows
the result for UDP inbound and outbound packets. In this case, the introspection
time for inbound and outbound data varies little. The Linux kernel keeps the
information for UDP streams in a single table of size 128, which is why the
introspection time is similar in both cases.
To measure VMwall’s performance overhead on network applications that run
inside domU, we performed experiments with two diﬀerent metrics for both in-
bound and outbound connections. In the ﬁrst experiment, we measured VMwall’s
impact on network I/O by transferring a 175 MB video ﬁle over the virtual net-
work via wget. Our second experiment measured the time necessary to establish
a TCP connection or transfer UDP data round-trip as perceived by software in
domU.
We ﬁrst transferred the video ﬁle from dom0 to domU and back again with
VMwall running inside dom0. Table 3 shows the result of our experiments. The