it is less than 2Z1− δ8
We denote by K the packets that may affect Cq|P . Since the
expression ofCq|P is not monotonic, we split it into two sets: K +
are packets that affectCq|P positively and K− affect it negatively.
Similarly, we define {Y K
represent how many of the packets of K are in each bin.
i } to be Poisson random variables that
We do not know how many bins affect the sum, but we know
for sure that there are no more than N balls. We define the random
variable Y K
+ that defines the number of packets from K that fell in
the corresponding bins to have a positive impact onCq|P . Invoking
N
V
≤ δ
4 .
(cid:114)
(cid:114)
(cid:1)(cid:12)(cid:12) ≥ Z1− δ8
−)(cid:12)(cid:12) ≥ Z1− δ8
(cid:33)
(cid:33)
N
V
≤ δ
4 .
+
+
K
Pr
+ yields that:
Lemma 6.3 on Y K
(cid:32)(cid:12)(cid:12)Y
K − E(cid:0)Y
Cq|P and Lemma 6.3 results in:
(cid:32)(cid:12)(cid:12)Y
Pr(cid:16)Cq|P ≥ Cq|P
2 Pr(cid:16)
(cid:17) ≤
(cid:17) ≥(cid:16)
−
K − E (YK
Pr
(cid:16)
Y −
K
V
+ Y +
K
≤ 1 − 2 δ2 = 1 − δ,
completing the proof.
(cid:17)
Proof. The proof follows form Theorem 6.13 in one dimen-
sion, or Theorem 6.17 in two, that guarantee that in both cases:
Pr(cid:16)
Cq|P  1 − δ.
6.3 RHHH Properties Analysis
Finally, we can prove the main result of our analysis. It establishes
that if the number of packets is large enough, RHHH is correct.
Theorem 6.19. If N > ψ , then RHHH solves (δ, ϵ, θ) - Approxi-
mate Hierarchical Heavy Hitters.
Proof. The theorem is proved by combining
V εs
Lemma 6.8 and Corollary 6.18.
Note that ψ (cid:44) Z1− δs2
(cid:3)
−2 contains the parameter V in it. When
the minimal measurement interval is known in advance, the parame-
ter V can be set to satisfy correctness at the end of the measurement.
For short measurements, we may need to use V = H, while longer
measurements justify using V ≫ H and achieve better performance.
When considering modern line speed and emerging new transmis-
sion technologies, this speedup capability is crucial because faster
lines deliver more packets in a given amount of time and thus justify
a larger value of V for the same measurement interval.
For completeness, we prove the following.
Theorem 6.20. RHHH’s update complexity is O(1).
Proof. Observe Algorithm 1. For each update, we randomize
a number between 0 and V − 1, which can be done in O(1). Then,
if the number is smaller than H, we also update a Space Saving
instance, which can be done in O(1) as well [34].
(cid:3)
Finally, we note that our space requirement is similar to that
Theorem 6.21. The space complexity of RHHH is O
table entries.
Proof. RHHH utilizes H separate instances of Space Saving,
table entries. There are no other space significant
(cid:3)
each using 1
ϵa
data structures.
7 DISCUSSION
This work is about realizing hierarchical heavy hitters measurement
in virtual network devices. Existing HHH algorithms are too slow to
cope with current improvements in network technology. Therefore,
we define a probabilistic relaxation of the problem and introduce
a matching randomized algorithm called RHHH. Our algorithm
leverages the massive traffic in modern networks to perform simpler
update operations. Intuitively, the algorithm replaces the traditional
approach of computing all prefixes for each incoming packets by
sampling (if V > H) and then choosing one random prefix to be
updated. While similar convergence guarantees can be derived for
the simpler approach of updating all prefixes for each sampled
packet, our solution has the clear advantage of processing elements
in O(1) worst case time.
(cid:17)
(cid:16) H
εa
flow
Similarly, we define Y−
to be the number of packets from K that
K
fell into the corresponding buckets to create a negative impact on
of [35].
is monotonically increasing with the number of balls and Y−
K
is
Y +
K
monotonically decreasing with the number of balls. We can apply
Lemma 6.2 and conclude that:
(cid:16)
(cid:17)
V E
Y −
K
+ Y +
K
√
N V
+ 2Z1− δ8
(cid:17)(cid:17)
(cid:3)
6.2.3
Putting It All Together
We can now prove the coverage property for one and two di-
mensions.
Corollary 6.18. If N > ψ then RHHH satisfies coverage. That is,
given a prefix q (cid:60) P, where P is the set of HHH returned by RHHH,
Pr(cid:16)
(cid:17)
Cq|P  1 − δ .
Constant Time Updates in Hierarchical Heavy Hitters
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
We evaluated RHHH on four real Internet packet traces, consis-
ting over 1 billion packets each and achieved a speedup of up to
X62 compared to previous works. Additionally, we showed that the
solution quality of RHHH is comparable to that of previous work.
RHHH performs updates in constant time, an asymptotic impro-
vement from previous works whose complexity is proportional to
the hierarchy’s size. This is especially important in the two dimensi-
onal case as well as for IPv6 traffic that requires larger hierarchies.
Finally, we integrated RHHH into a DPDK enabled Open vSwitch
and evaluated its performance as well as the alternative algorithms.
We provided a dataplane implementation where HHH measurement
is performed as part of the per packet routing tasks. In a dataplane
implementation, RHHH is capable of handling up to 13.8 Mpps, 4%
less than an unmodified DPDK OVS (that does not perform HHH
measurement). We showed a throughput improvement of X2.5 com-
pared to the fastest dataplane implementations of previous works.
Alternatively, we evaluated a distributed implementation where
RHHH is realized in a virtual machine that can be deployed in the
cloud and the virtual switch only sends the sampled traffic to RHHH.
Our distributed implementation can process up to 12.3 Mpps. It is
less intrusive to the switch, and offers greater flexibility in virtual
machine placement. Most importantly, our distributed implementa-
tion is capable of analyzing data from multiple network devices.
Notice the performance improvement gap between our direct
implementation – X62, compared to the performance improvement
when running over OVS – X2.5. In the case of the OVS experiments,
we were running over a 10Gbps link, and were bound by that line
speed – the throughput obtained by our implementation was only
4% lower than the unmodified OVS baseline (that does nothing). In
contrast, previous works were clearly bounded by their computatio-
nal overhead. Thus, one can anticipate that once we deploy the OVS
implementation on faster links, or in a setting that combines traffic
from multiple links, the performance boost compared to previous
work will be closer to the improvement we obtained in the direct
implementation.
A downside of RHHH is that it requires some minimal number
of packets in order to converge to the desired formal accuracy gua-
rantees. In practice, this is a minor limitation as busy links deliver
many millions of packets every second. For example, in the settings
reported in Section 4.1, RHHH requires up to 100 millions packets
to fully converge, yet even after as little as 8 millions packets, the
error reduces to around 1%. With a modern switch that can serve 10
million packets per second, this translates into a 10 seconds delay
for complete convergence and around 1% error after 1 second. As
line rates will continue to improve, these delays would become even
shorter accordingly. The code used in this work is open sourced [4]
Acknowledgments. We thank Ori Rottenstreich for his insightful
comments and Ohad Eytan for helping with the code release. We
would also like to thank the anonymous reviewers and our shepherd,
Michael Mitzenmacher, for helping us improve this work.
grant #1505/16 and the Technion-HPI research school.
This work was partially funded by the Israeli Science Foundation
Marcelo Caggiani Luizelli is supported by the research fellowship
program funded by CNPq (201798/2015-8).
REFERENCES
[1] Intel DPDK, http://dpdk.org/.
[2] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan Vaidya-
nathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Matus, Rong Pan,
Navindra Yadav, and George Varghese. 2014. CONGA: Distributed Congestion-
aware Load Balancing for Datacenters. In ACM SIGCOMM. 503–514.
[3] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown,
Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal Near-optimal Data-
center Transport. ACM SIGCOMM (2013), 435–446.
[4] Ran Ben Basat. RHHH code. Available: https://github.com/ranbenbasat/RHHH.
[5] Ran Ben-Basat, Gil Einziger, Roy Friedman, and Yaron Kassner. 2016. Heavy
[6] Ran Ben-Basat, Gil Einziger, Roy Friedman, and Yaron Kassner. 2017. Optimal
[9] Moses Charikar, Kevin Chen, and Martin Farach-Colton. 2002. Finding Frequent
[11] Graham Cormode and Marios Hadjieleftheriou. 2010. Methods for Finding
[10] Graham Cormode and Marios Hadjieleftheriou. 2008. Finding Frequent Items in
[12] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. 2003.
Hitters in Streams and Sliding Windows. In IEEE INFOCOM.
elephant flow detection. In IEEE INFOCOM.
[7] Ran Ben-Basat, Gil Einziger, Roy Friedman, and Yaron Kassner. 2017. Rando-
mized Admission Policy for Efficient Top-k and Frequency Estimation. In IEEE
INFOCOM.
[8] Theophilus Benson, Ashok Anand, Aditya Akella, and Ming Zhang. 2011. Mi-
croTE: Fine Grained Traffic Engineering for Data Centers. In ACM CoNEXT.
Items in Data Streams. In EATCS ICALP. 693–703.
Data Streams. VLDB 1, 2 (Aug. 2008), 1530–1541.
Frequent Items in Data Streams. J. VLDB 19, 1 (2010), 3–20.
Finding Hierarchical Heavy Hitters in Data Streams. In VLDB. 464–475.
[13] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. 2004.
Diamond in the Rough: Finding Hierarchical Heavy Hitters in Multi-dimensional
Data. In SIGMOD. 155–166.
[14] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. 2008.
Finding Hierarchical Heavy Hitters in Streaming Data. ACM Trans. Knowl. Discov.
Data 1, 4 (Feb. 2008), 2:1–2:48.
[15] Graham Cormode and S. Muthukrishnan. 2005. An Improved Data Stream
Summary: The Count-min Sketch and Its Applications. J. Algorithms (2005), 18.
[16] Andrew R. Curtis, Jeffrey C. Mogul, Jean Tourrilhes, Praveen Yalagandula, Puneet
Sharma, and Sujata Banerjee. 2011. DevoFlow: Scaling Flow Management for
High-performance Networks. In ACM SIGCOMM. 254–265.
Estimation of Internet Packet Streams with Limited Space. In EATCS ESA.
sion Policy. In Euromicro PDP. 146–153.
In ACM ICDCN.
[20] Gil Einziger, Marcelo Caggiani Luizelli, and Erez Waisbard. 2017. Constant Time
Weighted Frequency Estimation for Virtual Network Functionalities. In IEEE
ICCCN.
[21] Paul Emmerich, Sebastian Gallenmüller, Daniel Raumer, Florian Wohlfart, and
Georg Carle. 2015. MoonGen: A Scriptable High-Speed Packet Generator. In
ACM IMC. 275–287.
[22] Pedro Garcia-Teodoro, Jesus E. Diaz-Verdejo, Gabriel Macia-Fernandez, and E. Va-
zquez. 2009. Anomaly-Based Network Intrusion Detection: Techniques, Systems
and Challenges. Computers and Security (2009), 18–28.
[23] John Hershberger, Nisheeth Shrivastava, Subhash Suri, and Csaba D. Tóth. 2005.
Space Complexity of Hierarchical Heavy Hitters in Multi-dimensional Data
Streams. In ACM PODS. 338–347.
13:00-13:05 UTC, Direction B.
[18] Gil Einziger and Roy Friedman. 2014. TinyLFU: A Highly Efficient Cache Admis-
[19] Gil Einziger and Roy Friedman. 2016. Counting with TinyTable: Every Bit Counts!.
[17] Erik D. Demaine, Alejandro López-Ortiz, and J. Ian Munro. 2002. Frequency
[24] Paul Hick. CAIDA Anonymized 2013 Internet Trace, equinix-sanjose 2013-12-19
[25] Paul Hick. CAIDA Anonymized 2014 Internet Trace, equinix-sanjose 2013-06-19
[26] Paul Hick. CAIDA Anonymized 2015 Internet Trace, equinix-chicago 2015-12-17
[27] Paul Hick. CAIDA Anonymized 2016 Internet Trace, equinix-chicago 2016-02-18
13:00-13:05 UTC, Direction B.
13:00-13:05 UTC, Direction A.
13:00-13:05 UTC, Direction A.
[28] Lavanya Jose and Minlan Yu. 2011. Online Measurement of Large Traffic Aggre-
gates on Commodity Switches. In USENIX Hot-ICE.
[29] Abdul Kabbani, Mohammad Alizadeh, Masato Yasuda, Rong Pan, and Balaji
Prabhakar. 2010. AF-QCN: Approximate Fairness with Quantized Congestion
Notification for Multi-tenanted Data Centers. In IEEE HOTI. 58–65.
[30] Richard M. Karp, Scott Shenker, and Christos H. Papadimitriou. 2003. A Simple
Algorithm for Finding Frequent Elements in Streams and Bags. ACM Transactions
Database Systems 28, 1 (March 2003).
[31] Yuan Lin and Hongyan Liu. 2007. Separator: Sifting Hierarchical Heavy Hitters
Accurately from Data Streams. In ADMA (ADMA). 170–182.
[32] Nishad Manerikar and Themis Palpanas. 2009. Frequent Items in Streaming Data:
An Experimental Evaluation of the State-of-the-art. Data Knowl. Eng. (2009),
415–430.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
R. Ben Basat, G. Einziger, R. Friedman, M.C. Luizelli, and E. Waisbard
[33] Gurmeet Singh Manku and Rajeev Motwani. 2002. Approximate Frequency
[34] Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. 2005. Efficient Com-
Counts over Data Streams. In VLDB.
putation of Frequent and Top-k Elements in Data Streams. In ICDT.
[35] M. Mitzenmacher, T. Steinke, and J. Thaler. 2012. Hierarchical Heavy Hitters
with the Space Saving Algorithm. In Proceedings of the Meeting on Algorithm
Engineering & Expermiments (ALENEX). 160–174.
[36] Michael Mitzenmacher and Eli Upfal. 2005. Probability and Computing: Rando-
mized Algorithms and Probabilistic Analysis. Cambridge University Press, New
York, NY, USA.
[37] S. Muthukrishnan. 2005. Data Streams: Algorithms and Applications. Foundations
and Trends in Theoretical Computer Science 1, 2 (2005), 117–236.
[38] V.V. Patil and H.V. Kulkarni. 2012. Comparison of confidence intervals for the
Poisson mean: Some new aspects. REVSTAT Statistical Journal 10, 2 (June 2012),
211–227.
[39] Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno Ra-
jahalme, Jesse Gross, Alex Wang, Joe Stringer, Pravin Shelar, Keith Amidon,
and Martin Casado. 2015. The Design and Implementation of Open vSwitch. In
USENIX NSDI. 117–130.
[40] Neil C. Schwertman and Ricardo A. Martinez. 1994. Approximate poisson con-
fidence limits. Communications in Statistics - Theory and Methods 23, 5 (1994),
1507–1529.
[41] Vyas Sekar, Nick Duffield, Oliver Spatscheck, Jacobus van der Merwe, and Hui
Zhang. 2006. LADS: Large-scale Automated DDOS Detection System. In USENIX
ATEC. 16–16.
[42] Vibhaalakshmi Sivaraman, Srinivas Narayana, Ori Rottenstreich, S. Muthu-
krishnan, and Jennifer Rexford. 2017. Heavy-Hitter Detection Entirely in the Data
Plane. In Proceedings of the Symposium on SDN Research (ACM SOSR). 164–176.
[43] P. Truong and F. Guillemin. 2009. Identification of heavyweight address prefix
pairs in IP traffic. In ITC. 1–8.
(Invited Talk). In ICDT.
Sample in Randomized Load Balancing. In IEEE INFOCOM. 1131–1139.
[46] Yin Zhang, Sumeet Singh, Subhabrata Sen, Nick Duffield, and Carsten Lund. 2004.
Online Identification of Hierarchical Heavy Hitters: Algorithms, Evaluation, and
Applications. In ACM IMC. 101–114.
[44] David P. Woodruff. 2016. New Algorithms for Heavy Hitters in Data Streams
[45] L. Ying, R. Srikant, and X. Kang. 2015. The Power of Slightly More than One