of TM estimation constitutes measurement overhead for this
task. We have been advocating a limited use of such moni-
tors. In order to assess this tradeoﬀ, we present Fig. 17. For
the p2p case, each of the 10 points in Fig. 17 corresponds to
a diﬀerent value of δ = 0.1, 0.2, . . . , 1.0 (similarly for r2r and
00.020.040.060.080.100.10.20.30.40.50.60.70.80.91Flow weighted relative L2 norm errorF(x)Empirical CDFp2pr2rl2l05010015020025030000.050.10.150.20.250.30.350.40.450.5Average throughput (Mbps)Flow weighted relative L2 norm errorp2pr2rl2l00.020.040.060.080.100.10.20.30.40.50.60.70.80.91Flow weighted relative L2 norm errorF(x)Empirical CDF12 hoursdayweek0.10.20.30.40.50.60.70.80.90 0.03 0.06 0.09 0.120.10.20.30.40.50.60.70.80.9050.10.20.30.40.50.60.70.80.90 0.03 0.06 0.09 0.1290th percentile of flow weighted relative L2 norm error distribution0.10.20.30.40.50.60.70.80.905Average number of days between baseline re−estimation0.10.20.30.40.50.60.70.80.90 0.03 0.06 0.09 0.120.10.20.30.40.50.60.70.80.905δPoP to PoP router to router link to link l2l). The y-axis states the fraction of absolute relative errors
below 25% for the given scenario (value of δ and granularity
level). The x-axis is our metric for measurement overhead.
To compute this metric we count the number of hours each
link ran the ﬂow monitor and sum across the entire net-
work. We then divide by the total number of link-hours in
our measurements (e.g. 81 × 504 hours). Thus a value of
0.25 on this axis means that overall (across time and space),
monitors were used during 25% of the 3-week lifetime of our
experiment.
Figure 17: Tradeoﬀ between accuracy and measure-
ment overhead.
The percentage of error improvement achieved using addi-
tional measurements appears to be non-linear. For all three
levels of granularity there is a plateau after which large in-
creases in measurement overhead are needed for further error
reduction. This ﬁgure implies that with roughly 45% of the
measurements from an always on monitor, one can estimate
between 80-90% of the ﬂows quite well. Clearly monitoring
at 100% is the only way to achieve zero errors, all the time,
for all error metrics. However this level of accuracy is typi-
cally not needed for most traﬃc engineering applications.
7. REDUCTIONS IN OVERHEAD
To illustrate the impact of our scheme on overheads, we
now quantify the savings that would result over a period of
3 weeks (the length of our entire dataset). Letting T denote
the duration of this 3 week period in one hour increments
we have T = 504 for our calculations. In Table 3 we summa-
rize both the router storage and communications overheads
incurred by the centralized, distributed, and trigger-based
solutions. By ’distributed’ we refer to a solution in which
ﬂow monitors are left on all the time, thus continuously
measuring the traﬃc matrix without using any estimation
at all. We use the numbers from our backbone (e.g., R = 27,
L = 81, etc); these are stated at the top of the table.
According to our scheme each router needs to store 24
hours of its fanouts as well as one extra row of fanouts that
corresponds to the randomly selected hour. Consequently,
at any point in time the router needs to store at most 25
rows of fanouts. In our network this means that for the com-
putation of the p2p traﬃc matrix each router needs to store
(H +1)∗P records (or 25∗13 = 325) at any point in time. In
Netﬂow v8 every router stores approximately 67,000 records
for each one of its links, with an average of 3 links per router
that means 210,000 records per router. (The centralized and
distributed router storage in this table is the same as that
in Table 1. They are included here for comparative pur-
poses.) Clearly, the storage for the trigger-based scheme is
signiﬁcantly lighter than for today’s centralized solutions.
The trigger-based scheme does slightly worse than the dis-
tributed solution because the distributed solution maintains
less by shipping it out every hour to the collection station.
The communications overhead for the centralized and dis-
tributed solutions in this table diﬀer from that in Table 1
because we now include the length of time over which we as-
sess these overheads. For example the communications for
an r2r matrix is now T ∗R2 (rather than just R2). The num-
bers in the table correspond to the total number of records
shipped through the network during this three week period.
To quantify the reduction in communication overhead un-
der our approach, we count how many times baseline re-
estimation was triggered for each link or router throughout
the 3 week period. The numbers of triggers generated for
each TM granularity level are given at the top of the table.
The total overhead is the sum of the initial baseline (24 mea-
surements) plus the number of baseline measurements that
are triggered for speciﬁc links or routers throughout the net-
work. For example, in the l2l case, the initial baseline sent
contains 24 ∗ L2 records. After that we simply communi-
cate the rows of the matrix that change. With 333 triggers
(each of which requires another 24 hours of measurement),
the remainder of the overhead is 333 ∗ 24 ∗ L.
In order to compare the three solutions, we look at the
percentage reduction from one scheme to the next. We quan-
tify the reduction in overhead as the fraction of the overhead
diﬀerence between two schemes divided by the original num-
ber of records. We ﬁnd that moving from a centralized to
a distributed approach leads to 99.99%, 99.98% and 99.87%
reduction for the p2p, r2r and l2l levels respectively. Alter-
natively, we can say that the distributed scheme’s overhead
is roughly a factor of a thousand less than the centralized
one. Relative to the distributed solution, the trigger-based
approach leads to an extra 85.18%, 69.13% and 75.66% re-
duction for the three levels respectively.
Because the overhead for our scheme involves a constant
initial factor plus a term that is a function of time (i.e., the
number of triggers), the impact of the initial constant factor
eventually disappears after enough time. If D denotes the
average number of days between baseline re-estimation, then
essentially we will need to collect one day’s worth of mea-
surements every D days. The savings here, relative to a full-
on distributed scheme, will converge to D−1
D . For example,
if D = 4, then we recompute the baseline once every 4 days,
leaving 3 days without any measurements communicated to
the collection station. This reduction in measurement over-
head was estimated to be 75.78%, 68.85%, and 65.47% for
the p2p, r2r and l2l respectively (for the respective values
of D derived from our measurements), as compared to the
always on measurement method.
8. CONCLUSIONS
In this paper we addressed the question of whether or not
Internet traﬃc matrices can be obtained via direct measure-
ment by ﬂow monitors on routers. We showed that with to-
day’s technology, centralized solutions are needed and that
these indeed are computationally prohibitive.
We strongly encouraged moving toward a distributed so-
lution and illustrated the reduction in overheads that this
0  0.120.240.360.480.70.750.80.850.90.951Fraction of time the network collects NetFlow dataEstimates with absolute relative error below 25%p2pr2rl2lOverhead in # records for the three weeks of measurements
T = 504, P = 13, R = 27, L = 81, ¯L(r) = 3, F (l) = 67, 000
H=24, # triggers (p2p)=57, # triggers (r2r)=148, # triggers (l2l)=333
Router Storage
Centralized
¯L(r) ∗ ¯F (l) = 210, 000
same as above
same as above
Distributed
P = 13
R = 27
¯L(r) ∗ L = 243
Trigger-Based
(H + 1) ∗ P = 325
(H + 1) ∗ R = 675
(H + 1) ∗ ¯L(r) ∗ L = 6, 075
Communications Overhead
Centralized
T ∗ L ∗ ¯F (l) = 2, 735, 208, 000
same as above
same as above
Distributed
T ∗ R ∗ P = 176, 904
T ∗ R2 = 367, 416
T ∗ L2 = 3, 306, 744
Trigger-Based
24 ∗ R ∗ P + #triggers ∗ 24 ∗ P = 26, 208
24 ∗ R2 + #triggers ∗ 24 ∗ R = 113, 400
24 ∗ L2 + #triggers ∗ 24 ∗ L = 804, 816
p2p
r2r
l2l
p2p
r2r
l2l
Table 3: Summarization of overheads for a three week period.
would enable. For example, the reduction in communica-
tions overhead can be as large as 99% since we change by
orders of magnitude the amount of data shipped through
the network. Although recent advances in Netﬂow illustrate
a move toward being able to compute traﬃc matrices, we
explained why these advances are not suﬃcient. We iden-
tiﬁed the critical factors for which implementation changes
are needed to enable truly distributed solutions to this prob-
lem, and presented these as two recommendations to router
manufacturers. These recommendations include (i) imple-
menting a function to map destination network preﬁxes to
egress links or routers within the domain at the router itself,
and (ii) modifying the deﬁnition of the ﬂow record in order
to include the result of this mapping. We believe that once
this function and deﬁnition can be implemented, distributed
traﬃc matrix measurement by routers would become a re-
ality.
Nonetheless, we showed that it is possible to go beyond
this vision, further reducing the frequency with which mea-
surements need to be taken. This is based on our ﬁnding
that the node fanouts are remarkably predictable, at three
granularity levels. We presented a scheme that exploits this
ﬁnding to further reduce communications overhead and the
frequency of router measurements. We showed that by tak-
ing measurements only once every few days, we can obtain a
traﬃc matrix that is accurate on the time scale of hours. We
demonstrated that by allowing Netﬂow to be turned on and
oﬀ, on an as needed basis, we can reduce communications
overhead by 70-85% and can reduce measurement overhead
by 65-75% relative to a direct measurement solution that
leaves ﬂow monitors on continuously. (This also implies that
there is a strong potential for advanced sampling techniques
to succeed in the area of TM measurement.) Our approach
is capable of achieving such reduction while introducing lim-
ited estimation errors. Moreover, unlike other schemes, ours
has knobs that can be tuned so as to achieve a speciﬁc tar-
get error rate. The attractiveness of our scheme is that it
is simple, scalable (for traﬃc matrices at all three granular-
ity levels) and works well, thus rendering our approach very
practical.
In future work we intend to compare our tech-
nique with proposed inference techniques, and quantify the
diﬀerences against the same dataset using the same error
metrics.
9. ACKNOWLEDGEMENTS
We would like to thank Dr. Gang Liang for his suggestion
on using the weighted L2 norm as an error metric. We are
also greatly indebted to Bjorn Carlsson, Jeﬀ Loughridge,
and Richard Gass of Sprint for their eﬀorts in collecting the
traﬃc matrix used herein.
10. REFERENCES
[1] J. Cao, D. Davis, S. VanderWeil, and B. Yu. Time-Varying
Network Tomography: Router Link Data. Journal of the
the American Statistical Association, 95(452), 2000.
[2] Cisco. NetFlow Services Solutions Guide, July 2001.
[3] Cisco. Cisco IOS NetFlow Version 9 Flow-Record Format ,
June 2003.
[4] N. Duﬃeld, C. Lund, and M. Thorup. Properties and
Prediction of Flow Statistics from Sampled Packet Streams.
In ACM Sigcomm Internet Measurement Conference,
Marseille, France, Nov. 2002.
[5] A. Feldmann, A. Greenberg, C. Lunc, N. Reingold,
J. Rexford, and F. True. Deriving traﬃc demands for
operational ip networks: Methodology and experience.
IEEE/ACM Transactions on Networking, June 2001.
[6] G. Liang and B. Yu. Pseudo Likelihood Estimation in
Nework Tomography. In Proceedings of IEEE Infocom, San
Francisco, CA, Mar. 2003.
[7] A. Medina, C. Fraleigh, N. Taft, S. Bhattacharyya, and
C. Diot. A Taxonomy of IP Traﬃc Matrices. In SPIE
ITCOM: Scalability and Traﬃc Control in IP Networks II,
Boston, Aug. 2002.
[8] A. Medina, N. Taft, K. Salamatian, S. Bhattacharyya, and
C. Diot. Traﬃc Matrix Estimation: Existing Techniques
and New Directions. In ACM SIGCOMM, Pittsburgh,
USA, Aug. 2002.
[9] K. Papagiannaki, N. Taft, Z.-L. Zhang, and C. Diot.
Long-Term Forecasting of Internet Backbone Traﬃc:
Observations and Initial Models. In Proceedings of IEEE
Infocom, San Francisco, March 2003.
[10] J. Rexford, J. Wang, Z. Xiao, and Y. Zhang. BGP Routing
Stability of Popular Destinations. In IMW, Marseilles,
France, Nov. 2002.
[11] A. Soule, A. Nucci, E. Leonardi, R. Cruz, and N. Taft. How
to Identify and Estimate the Largest Traﬃc Matrix
Elements in a Dynamic Environment. In ACM Sigmetrics,
New York, June 2004.
[12] G. Varghese and C. Estan. The Measurement Manifesto. In
Proc. of the 2nd Workshop on Hot Topics in Networks,
Cambridge, MA, Nov. 2003.
[13] Y. Zhang, M. Roughan, N. Duﬃeld, and A. Greenberg. Fast
Accurate Computation of Large-Scale IP Traﬃc Matrices
from Link Loads. In ACM Sigmetrics, San Diego, CA, 2003.
[14] Y. Zhang, M. Roughan, C. Lund, and D. Donoho. An
Information Theoretic Approach to Traﬃc Matrix
Estimation. In ACM SIGCOMM, Karlsruhe, Germany,
August 2003.