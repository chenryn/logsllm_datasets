more reliable, which could, once implemented, be used to improve our current
approach and would allow removal of the whitelist.1
6.2 Code Pointer Examination
After we have checked important data structures, we scan through the rest of
kernel data memory to ﬁnd pointers to executable kernel code. This is achieved
using the following steps: We ﬁrst extract the memory regions of executable
kernel code sections in the monitored virtual machine using the page tables
structure. As a second step, we extract the data pages of the monitored guest
1 https://lkml.org/lkml/2015/5/18/545.
Counteracting Data-Only Malware with Code Pointer Examination
189
system. For this purpose, we obtain all pages that are marked as supervisor and
not executable in the page tables. These pages contain the data memory of the
kernel and therefore all pointers that are accessible from within the Linux kernel.
Note that the information we use for our analysis is binding, since it is derived
from either the hardware or the trusted kernel reference binaries.
Having obtained the code and data pages, we iterate through the extracted
pages in a byte by byte manner. We interpret each eight byte value (indepen-
dently of its alignment) as a pointer and check whether it points into one of the
memory locations that was identiﬁed as containing kernel code. If we found a
pointer that points to executable kernel memory we ﬁrst check if its destination
is contained in the list of valid functions.
In case the pointer does not point to a valid function, we check if the pointer is
a return address. There are currently multiple approaches used in our framework
to identify a return address. First and foremost, a return address must point to an
instruction within a function that is preceded by call instruction. Consequently,
our initial check consists of validating whether the instruction it points to is
actually contained within the function.
For this purpose, we disassemble the function the pointer allegedly points to
from the beginning and verify that the value of the pointer points to a disassem-
bled instruction and not somewhere in between instructions. In such a case, we
additionally ensure a call instruction resides before the instruction the pointer
points to. If any of these conditions fail, we consider the code pointer not to be
a valid return address and continue to the next category.
Most of the return addresses used within the kernel are stored within one
of the kernel stacks. However, there exist a few functions within the kernel that
save the return address of the current function to be able to identify the current
caller of that function. This was ﬁrst introduced as a debug feature to print
the address of the calling function to the user in case of an error. However, in
the meantime this feature is also used for other purposes such as timers. For
example, the struct hrtimer contains a pointer start site that points to the
instruction after the call instruction that started the timer.
With such a feature in place and used by the kernel it is hard to diﬀerenti-
ate between legitimate return addresses and specially crafted control structures
for code reuse techniques. To limit this problem we created a whitelist of all
calls to functions that contain the problematic instruction and only allow return
addresses in the kernel’s data segment if they point to one of the functions in
question.
If the pointer does not point to a valid function or a return address, the
pointer is considered as malicious and a human investigator is notiﬁed. At this
point the system also enriches the error message with the name of the function
or symbol the pointer is pointing into.
7 Evaluation
In this section, we evaluate our approach using the prototype implementation
described in Sect. 6. In order to determine whether our framework is able to
190
T. Kittel et al.
achieve the goals set in Sect. 5, we ﬁrst determine its performance character-
istics, before we evaluate its eﬀectiveness against data-only malware in both
live monitoring as well as forensic applications. We follow this with an in-depth
discussion of the security aspects of our system.
7.1 Experiments
Our host system consisted of an AMD Phenom II X4 945 CPU with 13 GB of
RAM running Linux kernel version 3.16 (Debian Jessie). As guest systems we
used two diﬀerent VMs running Linux 3.8 as well as Linux 3.16. Each VM had
access to two virtual CPUs and 1 GB of RAM. In these experiments, we used
XEN as the underlying hypervisor.
Performance and False Positives. First of all, we evaluated the performance
of our system as well as its susceptibility to false positives. For this purpose, we
used the Phoronix-Test-Suite to run a set of Benchmarks on our system. In detail,
we ran the pts/kernel test suite. We conducted these benchmark three times on
each test kernel. During the ﬁrst set of tests, we disabled all external monitoring
to obtain a baseline of the normal system performance. In the second test set, we
enabled the code validation component to be able to diﬀerentiate between the
overhead of our framework and the code validation system. Finally, we enabled
both the code validation component as well as our new pointer validation module
in order to identify the additional overhead that our system incurs. During the
tests, the integrity validation component was executed in a loop, if enabled, to
stress the guest system as much as possible. The results of the benchmarks of
each set of experiments as well as the overall performance degradation are shown
in Table 1 for Linux 3.8 and in Table 2 for Linux 3.16.
While evaluating the Linux 3.8 kernel, the kernel contained 80 code pages
and 426 data pages. One complete Code Integrity Validation was completed in
255.8 ms, while in the experiment with Code Integrity Validation and Pointer
Examination enabled, one iteration took 567.58 ms (that is 341.78 ms for CPE).
The Linux 3.16 kernel that was used during our evaluation contained 408 code
pages and 986 data pages. The Code Integrity Validation alone took 639.8 ms
per iteration, while the combined CIV and Pointer Examination took 962.0 ms
per iteration (that is 322.2 ms for CPE). Note that these values are mean values.
This shows that it takes less than 1 ms on average to check the integrity of one
page.
As one can see the performance overhead that our framework incurs is very
small. In fact, the use of the underlying Code Validation Component incurs
a larger overhead than our CPE framework. The performance impact of our
system is for the most benchmarks well under one percent. The main reason for
this is that our framework, in contrast to many other VMI-based approaches,
uses passive monitoring of the guest system whenever applicable. As a result,
the guest system can execute through most of the validation process without
being interrupted by the hypervisor, which drastically reduces the performance
overhead of the monitoring. Only for the FSMark benchmark a performance
Counteracting Data-Only Malware with Code Pointer Examination
191
Table 1. Results of the Phoronix Test Suite for Linux 3.8.
Test (Unit)
Gcrypt Library (ms)
Timed MAFFT Alignment (s)
7-Zip Compression (MIPS)
C-Ray - Total Time (s)
John The Ripper (Real C/S)
H.264 Video Encoding (FPS)
GraphicsMagick 1 (Iter/min)
GraphicsMagick 2 (Iter/min)
FS-Mark (Files/s)
Dbench (MB/s)
w/o
32.57
69.84
20.63
2857
1689
35.38
95
Himeno Benchmark (MFLOPS)
CIV (%)
CIV &CPE (%)
30.10 (8.21%)
31.73 (2.65%)
71.54 (−2.38%)
66.53 (4.98%)
20.70 (0.34%)
20.63 (0.00%)
2853 (−0.14%)
2837 (−0.70%)
1689 (0.00%)
1688 (0.06%)
35.23 (0.43%)
35.31 (0.20%)
95 (0.00%)
95 (0.00%)
58 (0.00%)
58 (0.00%)
585.73 (1.34%)
586.24 (1.25%)
4702 (0.28%)
4706 (0.19%)
131.00 (0.03%)
130.99 (0.02%)
36.58 (0.63%)
36.47 (0.33%)
445 (0.00%)
446 (0.22%)
236.39 (0.81%)
236.12 (0.69%)
124.38 (0.11%)
124.35 (0.09%)
25.19 (−0.04%)
25.19 (−0.04%)
26.82 (−0.67%)
27.02 (0.07%)
14.98 (−2.35%)
14.94 (−2.61%)
1148.95 (1.04%)
1144.94 (0.68%)
173.73 (−0.02%) 173.80 (−0.06%)
115.21 (−0.09%)
114.69 (0.37%)
PostgreSQL pgbench (Trans/s)
Apache Benchmark (Requests/s) 10585.45 10481.21 (0.99%) 10506.23 (0.75%)
58
593.59
4715
130.96
36.35
445
234.50
124.24
25.20
27.00
15.34
1137.17
173.70
115.11
Parallel BZIP2 Compression (s)
Open FMM Nero2D (s)
OpenSSL (Signs/s)
Smallpt (s)
dcraw (s)
Ffmpeg (s)
GnuPG (s)
LZMA Compression (s)
LAME MP3 Encoding (s)
degradation of about 2.65 % is noticed on Linux 3.8. This degradation can not
be seen in the results of the benchmark on Linux 3.16. While using the guest
system with monitoring enabled, we did not observe any noticeable overhead
from within the guest system. This clearly shows that our framework can achieve
the performance goal set in Sect. 5 and is, from a performance point of view, well
suited for real world applications. Sometimes the results even showed that the
tests were better with our pointer examination framework enabled than without
our framework. We argue that this may be due to the fact that the performance
impact of our system is much smaller than the impact of other standard software
within the tested Debian system that also inﬂuenced the result.
At the same time we did not observe any false positives during our exper-
iments. That is, when enabled, our system could classify all of the pointers it
encountered during the validation process using the heuristics we described in
Sect. 5. However, note that we can, due to the design of our system, not rule
192
T. Kittel et al.
Table 2. Results of the Phoronix Test Suite for Linux 3.16.
Test (Unit)
FS-Mark (Files/s)
Dbench (MB/s)
Timed MAFFT Alignment (s)
Gcrypt Library (ms)
John The Ripper (Real C/S)
H.264 Video Encoding (FPS)
Himeno Benchmark (MFLOPS)
7-Zip Compression (MIPS)
C-Ray - Total Time (s)
Parallel BZIP2 Compression (s)
Smallpt (s)
LZMA Compression (s)
dcraw (s)
w/o
CIV (%)
CIV & CPE (%)
30.90 31.37 (−1.50%)
31.67 (−2.43%)
61.42 60.76 (1.09%)
61.04 (0.62%)
20.74 20.79 (0.24%)
20.75 (0.05%)
3747.00 3740 (−0.19%)
3733 (−0.37%)
1693.00 1693 (0.00%)
1692 (0.06%)
34.60 34.32 (0.82%)
34.35 (0.73%)
598.71 582.78 (2.73%)
585.78 (2.21%)
4850.00 4805 (0.94%)
4730 (2.54%)
89.80 89.81 (0.01%)
89.80 (0.00%)
31.25 31.41 (0.51%)
31.37 (0.38%)
407.00 407 (0.00%)
407 (0.00%)
236.62 241.49 (2.06%)
242.17 (2.35%)
117.54 117.47 (−0.06%) 117.29 (−0.21%)
23.39 23.41 (0.09%)
13.72 13.65 (−0.51%)
173.63 173.37 (0.15%)
GnuPG (s)
LAME MP3 Encoding (s)
23.40 (0.04%)
13.98 (1.90%)
173.57 (0.03%)
OpenSSL (Signs/s)