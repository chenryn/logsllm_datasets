separate clusters if α · n
m to
be small, either the attack intensity α must be small (thus,
decreasing the expected proﬁt R) or the ratio m
m is very small. However, for α · n
2α·n+4β(1−β)m
= 1
2 .
n is large.
5.2 Analysis Based on Perfect IDS
From the analysis shown in Figures 1 to 5, it is evident
that the optimal attack intensity α∗
depends on the ratio m
n .
Figure 1 shows the expected proﬁt R as a function of attack
intensity α for a ﬁxed ratio m
n = 0.5. The ﬁgure shows that
the detection probability qt increases monotonically with the
attack intensity α. The expected proﬁt R initially increases
reaching an optimal R∗
at some optimal attack intensity α∗
;
the subsequent decrease in expected proﬁt is attributed to a
higher detection probability (qt) that reduces the expected
lifetime (time to revoke) of a bad node.
n . As the ratio m
Figures 2 and 3 show the optimal attack intensity and ex-
pected proﬁt as we vary the ratio m
n increases,
the bad nodes can aﬀord to increase their attack intensity and
yet tweak the opinion matrix in a way that makes it harder
for the judgment system to distinguish the bad nodes from
the good nodes. Consequently, the overall expected proﬁt as
shown in Figure 3 increases dramatically with the ratio m
n ;
not only the absolute number of bad nodes increases with the
ratio, but also each bad node uses a higher attack intensity
in order to maximize its expected proﬁt.
Figures 4 and 5 show the optimal decision probabilities
p∗
t , p∗
a achieved under optimal attack in-
tensity for varying ratio of m
n . We note that the numbers
shown in Figures 4 and 5 have been obtained from running
our classiﬁcation algorithm assuming that the bad nodes have
strategically (optimally) chosen their opinions. We also note
that the decision probabilities shown in these ﬁgures are made
at steady state; for the judgment system to make meaningful
decisions the opinion matrix must hold suﬃciently dense pos-
itive and negative opinions. Indeed at time t = 0 all opinion
values oij are 0 and the judgment system will output unde-
cided.
f and q∗
f , p∗
t , q∗
a, q∗
”`
Figures 4 and 5 show that as the ratio m
n increases, the
probability of misclassifying both the good and the bad nodes
increases. Recall the constraints on decision probabilities de-
rived in Section 4: pt >
pf is required to pre-
vent an attacker from abusing the scheme to its own beneﬁt,
and qt > 1
qf (with b = 1) is required to provide positive in-
b
centive for honest nodes to revoke. Figure 4 shows that for a
worst case ratio of m
n = 1, pt = 0.61 and pf = 0.31, such that
1 + m
n
´
“
n
n−1
296Figure 6: Maximum tolerable ra-
tio of m/n to keep the adversary
from abusing the scheme.
Figure 7: Good node decision
probabilities with IDS error rate
of 0.15.
Figure 8: Bad node decision prob-
abilities with IDS error rate of
0.15.
the ﬁrst requirement can be satisﬁed by the judgment system
nearly exactly to the ratio m
n = 1. As Figure 5 shows the
second constraint is even satisﬁed for m  qf (b=1)
for m  n
6).
n is met as long as m
n−1 1 + m
n
5.3 Analysis Considering an IDS Error Rate
6. THE REVOCATION GAME
We have so far assumed that the opinions of good nodes are
error free. Let us suppose that the average error probability
in the IDS system is e as introduced in Section 4.1. Using
the same arguments as above, one can show that the average
distance between two good nodes is 4n·e(1−e) + 2m and that
between a good node and a bad node is 2n·(e(1−α)+α(1−e))
+ 2m. We recognize that for the judgment system to be
eﬀective we need the average distance between a good node
and a bad node to be larger than that of two good nodes:
2n· (e(1− α) + α(1− e)) > 4n· e(1− e); equivalently, we need
(α− e)(1− 2e) > 0 ⇒ α > e (in any meaningful IDS system e
e
1−λ
pf if m + 2n· e(1− e)  0
bad nodes we need e < 1− 1√
≈ 0.293. The scheme therefore
keeps secure against abuse of the attacker (malicious nodes
< 1 − 4e + 2e2.
proﬁt by revoking good nodes) as long as m
n
Figure 6 shows how the tolerable ratio of malicious nodes
decreases with a larger IDS error rate.
(m, n, e) = max{r∗
a, p∗
t , p∗
2
´
1 + m
n
Figures 7 and 8 show the good nodes’ and the bad nodes’
decision probabilities assuming an IDS error probability of
0.15. We note that setting e = 0.15 limits the maximum
number of m to m < 0.445n, what is similar to ratio of 30%
of malicious nodes.
5.4 Meeting the Requirements
In Table 3 we have formulated the requirements for the
judgment system’s accuracy to ensure that our karmic-suicide
scheme (A) gives positive incentive to honest nodes to revoke
In Section 4 we have deﬁned the cost and beneﬁt parame-
ters of our karmic-suicide scheme and derived the minimum
requirements for the judgment system’s accuracy. Section
5 has shown that our judgment system (for a range of set-
tings) is suﬃciently accurate to prevent abuse large numbers
of malicious nodes whilst allowing for the possibility for hon-
est nodes to revoke.
In this section we look to answer the
question whether our incentive for honest nodes to revoke is
suﬃcient, and if so, how quickly honest nodes will revoke ma-
licious nodes. We take a game-theoretic approach (using a
descending price auction) and show that our scheme provides
rational (honest but selﬁsh) nodes with incentive to suicide.
Our results show that even for a small network density, honest
nodes begin to revoke when their internal IDS error proba-
bility, e, falls below 25%.
6.1 Design of the Game
From an honest node’s perspective every other honest node
is in direct competition for reward. Unlike malicious nodes,
honest nodes do not know which nodes are malicious and in-
stead must rely on their intrusion detection system to deter-
mine node aﬃliation. A node must collect enough evidence of
malicious behavior to issue a suicide note, however, in doing
so, there is a tension between waiting too long (and missing
the opportunity to make a proﬁt) and making an incorrect
assessment (and making a loss). As each malicious node can
only be revoked once; waiting too long to collect more evi-
dence and decrease the chance of a false positive, might result
in another node revoking the malicious node ﬁrst. Even when
an honest node correctly revoked a malicious node, there is
still the risk that the TA makes a wrong decision and judges
that the malicious node is honest with probability pf (see Ta-
ble 1). Conversely, an honest node can also be fortunate and
be rewarded for an unjustiﬁed suicide with probability qf .
We design our karmic-suicide game as a game between N
honest nodes, competing for the beneﬁt of revoking one ma-
licious node.
In a MANET, several of these games can be
played in parallel, and honest nodes may join games at dif-
ferent times. To allow a clear analysis of our game, we inves-
297tigate a single game where all honest nodes join the game at
the same time.
Let P1, P2, . . . , PN , N ≤ n be the players, i.e. the honest
nodes that are in the position to revoke a malicious node. We
do not consider the malicious node as a player in the game as
it cannot take any action. Furthermore, let 0 ≤ 1−eni ≤ 1 be
the probability that the IDS from player Pi correctly identiﬁes
the malicious node as deﬁned in Section 4.1. Note, that for
our game we are only interested in false negatives of the IDS,
since nodes need to decide weather an alert for maliciousness
is correct or not (false negative). Then the expected beneﬁt
Ei ∈ [−1, b] of node Pi for revoking the suspicious node is:
Ei = b · (1 − eni) · qt − eni · pt + eni · qf − b · (1 − eni) · pf
−eni · (pt − qf + pf − qt)
= b · (1 − eni) · (qt − pf ) − eni · (pt − qf )
= b · (1 − eni) · (qt − pf ) − eni · (qt − pf )
= ((1 + b) · (1 − eni) − 1) · (qt − pf ) − ( eni|{z}
| {z }
≈ ((1 + b) · (1 − eni) − 1) · ( qt − pf
)
:=c∈[0.3,0.8]
<0.5
| {z }
) · (qa − pa
)
≈0
n
A crucial simpliﬁcation in this derivation is to set qa−pa = 0.
≤ 1 (see Figures 4,
While qa − pa ≈ 0 for all ratios of 0.1 ≤ m
5 and 7, 8), it also holds for all ratios but m
n = 0.1, that
qa − pa ≤ 0. Consequently, setting qa − pa = 0 slightly
decreases Ei and gives us a lower bound on Ei, and therefore
also a lower bound on the agility of our scheme in the analysis
at the end of the section. Recall, that as per Figures 4, 5 and
7, 8, qt − pf is the range of [0.3, 0.8]. Thus, as the reward in
our revocation scheme is an additional key, the value for b is
1, an honest node has an expectation value of:
Ei = (2 · (1 − eni) − 1) · c, c ∈ [0.3, 0.8]
(1)
An important observation here is that c does not inﬂuence
the agility of our karmic-suicide game. c is simply scaling
down all values of Ei, such that the keys would not have a
value of 1 but instead a value of c. Imagine, that we gave a
single key the value 2 instead of 1. Then Ei = 4·(1−eni)−2 =
(2 · (1 − eni) − 1) · 2. Obviously, the value of the keys can
be deﬁned as any positive value, but it has no inﬂuence on
our game. We therefore continue our analysis for c = 1, i.e.
Ei = 2 · (1 − eni) − 1, knowing that the analysis applies for
all positive values of c.
Each of the nodes Pi can now decide at what value of 1 −
eni it revokes a suspicious node, i.e. how much evidence it
requires to collect before revoking a node. Since we assume
that all nodes join the game at the same time and gather
the same information about the malicious node, 1 − eni will
be equal for the honest nodes at all times during the game.
Consequently, the node that goes in at the lowest value for
1 − eni, will revoke the suspicious node. All other honest
nodes missed the chance to revoke this node and thus their
chance to gain the proﬁt b.
6.2 Strategies and Results
In this section we investigate optimal strategies for honest
nodes in our karmic-suicide game and examine:
• whether the conﬁguration of the karmic-suicide scheme
adequately incentives the revocation of malicious nodes;
• the inﬂuence of the number of playing nodes (network
density) to the agility of the karmic-suicide scheme.
We assume that all honest nodes act rationally and try to
maximize their beneﬁt (number of identiﬁer/keys they pos-
sess). However, the best strategy for a given player will de-
pend upon the strategies adopted by other participating play-
ers in the game. We assume each node knows the distribution
of the other players’ strategies (by observation from earlier
games). To determine an optimal strategy, game theoretic
analysis introduces the fundamental concept of the Nash-
equilibrium. A set of strategies, (one strategy for each player)
is called an Nash-equilibrium if no player can increase their
payoﬀ by unilaterally changing their strategy. Our ﬁrst step in
analyzing our karmic-suicide scheme is therefore to determine
the Nash-equilibrium of our game.
6.3 Karmic-Suicide as a Dutch Auction
In this sub-section we demonstrate parallels between our
karmic-suicide game and a Dutch auction without attendance
fees (i.e. the proﬁt is 0 if a buyer does not win the auction). In
a Dutch auction, a seller, with a single object for sale, wishes
to sell their object to one of N buyers for the highest possible
price. The seller begins with an (unrealistically) high price
and successively lowers the price over a number of rounds.
The ﬁrst bidder to place a bid wins the object at the current
price quoted in that round.
The Dutch auction can be deﬁned in the independent, pri-
vate values model 6: Buyer Pi’s value for the object, vi, is
taken from the interval [0, 1] ⊂ R according to a distribu-
tion function Fi(vi), i.e. Fi(vi) denotes the probability that
Pi’s value is less than or equal to vi.
It is assumed that
each buyer knows their own value but not the values of other
buyers. However, the distribution functions F1, . . . , FN are
publicly known. If a buyer wins an auction at a price p, their
proﬁt is vi − p.
In our karmic-suicide game, we model our Dutch auction as
follows: the object to be sold is the suspicious node and the
buyers are the honest nodes who are collecting evidence about
the suspicious node’s (mis)behavior. The node who takes the
highest risk and revokes the suspicious node earliest obtains
the chance of getting rewarded for their eﬀort by the TA.
Table 6.3 shows the mapping between a “traditional” Dutch
auction and our karmic-suicide game.
Table 4: Linkage between the Dutch auction and the
karmic-suicide game
Dutch auction
buyer: Pi
value: vi
price: p
Fi(vi)
proﬁt: vi − p
seller reduces price malicious node leaks evidence
karmic-suicide game
honest node: Pi
risk appetite: 1 − ci
risk taken: 1 − Ei
risk appetite distribution: Fi(1−ci)
proﬁt: (1 − ci) − (1 − Ei) = E − ci
The buyers Pi are simply replaced by the honest nodes that
collect evidence about suspicious node activity. A buyer’s,
Pi, private value, vi, for the object in the Dutch auction is
the node Pi’s risk appetite (1 − ci) to revoke the suspicious
6Independent here expresses that each buyer’s private infor-
mation (the buyers value for the object) is independent of
every other bidder’s private information.
298node. For the clarity of later calculations, we deﬁne risk ap-
petite as ‘1 − desired certainty in revoking a node’; where
ci ∈ [0, 1] ⊂ R represents the lowest expectation value for
beneﬁt that a node Pi will tolerate to revoke a node. The
expectation value Ei (Eq. 1) of all nodes Pi is the same,
i.e. Ei = E, i = 1, . . . , N , since we assume for simplicity of
the game that the nodes P1, . . . , PN hold the same evidence
at a given time. The risk that a node takes to revoke the