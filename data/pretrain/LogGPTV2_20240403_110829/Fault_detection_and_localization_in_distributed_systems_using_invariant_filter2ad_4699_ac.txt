instead of injecting faults in a separate dataset, was due to
the lack of ground truth – with two other similar datasets
we ﬁnd broken invariants (even without injecting anomalies),
and without ground truth we do not know if these are due
to anomalies or measurement noise. Using this methodology,
we can clearly explain why an invariant is broken, and hence,
stress test various aspects of our algorithm. We discuss a real
world case study in Section VII-A.
We injected several instances of the three types of anomaly
into multiple metrics. We ﬁrst present the results for noise free
case, i.e. only the injected anomalies cause broken invariants,
and later for the anomalies with noise case.
Anomalies without noise. To inject spikes, we picked three
metrics – m1 with a high degree of 16 (i.e. lots of invariant
relationships), m2 with moderate degree of 7, and m3 with only
one invariant relationship. We randomly injected 10 spikes into
each of these metrics. Table I shows the accuracy of nodeScore
alone and in combination with neighborScore, i.e. the two
variants of the spatial approach – spatial avg. takes an average
of the two scores and spatial rank combines their rankings. A
6Due to non-disclosure agreement, we cannot provide any additional details
on these sensors.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:15 UTC from IEEE Xplore.  Restrictions apply. 
Anomaly@ max. anomaly
# top ranks over w=4
score at
nodeScore
spatial avg.
spatial rank
101
146
178
272
495
1302
1409
1666
1822
2600
103
148
180
274
497
1304
1411
1668
1824
2602
4
3
4
3
4
4
4
4
4
4
4
3
4
3
4
4
4
4
4
4
SPIKES: nodeScore AND spatial ALGORITHMS
TABLE I
4
3
4
3
4
4
4
4
4
4
spike anomaly in m1 at time t leads to broken invariants during
the interval [t, t+4]. Table I shows the number of time points
(out of 4) for which metric m1 is ranked the highest by each
algorithm. We can see that all the three methods are able to
localize the anomaly to metric m1 for all the 10 instances. In
two cases, anomaly at t = 146 and t = 274, m1 does not get the
top rank. This is because the metric m2 is also affected by a
spike anomaly then, and it is ranked ﬁrst. We observed similar
results for spike anomalies injected in m2 and m3. This is not
surprising because our test case is simple and clean. However,
it is still worth noting that the highest anomaly score does not
occur at the time when the anomaly is injected; for m1 there is
a shift by 2 samples. Our temporal algorithm is able to localize
the spike anomaly not only to the metric m1 but also at the
time point when it is injected (except for t = 146 and t = 272
cases where two metrics are affected; in these cases t = 148
and t = 274 are the points with the highest score). But it does
this after observing all the broken invariants over a window of
samples; e.g. in case of anomaly at t = 146, it looks at all the
broken invariants during [146, 149]. This observation points
to a trade-off between our spatial and temporal approaches in
an online setting: the spatial approach can raise early alarms
but it may not be able to pin-point the time at which a fault or
anomaly occurred; the temporal approach can provide more
accurate localization in time but with a delay.
Anomalies with noise. We mimic the measurement noise
problem at a certain time t by picking a few metrics at random
and marking a certain fraction of their invariants as broken.
We make sure that these metrics are different from the metrics
with an injected anomaly. Thus, we have two categories of
broken invariants – ones that are broken due to anomalies and
others that are broken due to “noise”. We refer to the later
group as noisy broken invariants.
To the injected spike anomaly case, we now add noisy bro-
ken invariants for three metrics. We consider two scenarios –
10% and 50% of a metric’s invariants broken due to noise.
Our metric for comparing different methods is their pre-
cision when their recall is 1. We compute these values as
follows. Each method assigns a score to each metric with
broken invariants (higher score indicates more abnormal), and
ranks them in a descending order based on their score. In
practice, operators go through the list of top-ranked metrics
until
the ideal
they identify the abnormal metric. Hence,
7
Fault at
m1
Noise
10%
50%
nodeScore
spatial avg.
spatial rank
temporal
0.46
0.4
0.9
0.66
0.74
0.54
0.56
0.43
AVERAGE PRECISION OF DIFFERENT METHODS
TABLE II
case for them is when the anomalous metric is ranked ﬁrst.
Consider the scenario where we have one abnormal metric and
a method gives it the rank k. Then an operator has to look at
the top k metrics to localize the fault. Here, we say that the
recall of the method is 1 and its precision is 1/k.
Table II shows the average precision across 10 instances
of spike anomalies in metric m1 for different methods. We
obtained qualitatively similar results for other metrics and omit
the results due to page limits. nodeScore performs the worst
because it looks at metrics in isolation and hence, cannot
distinguish between invariants broken due to anomalies and
noise. This situation worsens if noise has high impact – the
precision for nodeScore drops to 0.46 and 0.4 for 10% and
50% noisy broken invariants. Contrast this against the noise-
free case shown in Table I where nodeScore performs very
well. The invariant graph based spatial algorithms, spatial avg.
and spatial rank have reasonable precision even at 50% noisy
broken invariants with spatial avg. outperforming spatial rank.
This result is in agreement with Lee’s observation, in the
context of document retrieval, that combining scores gives
better performance than combining rank [15]. Our spatial
algorithms are more robust to noise because they combine
the “view” at a node (i.e. nodeScore) with the view across its
one-hop neighborhood (i.e. neighborScore). This reduces the
likelihood of metrics affected by (random) noise being ranked
as abnormal.
Our temporal approach does not perform as well as the two
spatial algorithms. The main reason for this is the incorrect
localization of the anomaly in time. E.g. for a spike anomaly
at t = 272, with noisy broken invariants, t = 275 has the
highest anomalyScore (see Algorithm 2), and the precision of
the temporal algorithm is 0.1. In contrast, the precision of the
temporal algorithm if t = 272 is picked as the onset of anomaly
is 1. (The results shown in Table II for the temporal approach
are also averaged across all time points with broken invariants
for a fair comparison with nodeScore and spatial algorithms.)
Thus, these results expose an important challenge in using our
temporal approach. If we know when an anomaly occurred,
then the temporal approach is effective in identifying the
abnormal metrics. For the noise-free case, the anomalyScore
provides a good localization in time; it does not work as well
with noise. We would like to point out that we do not need
very precise localization in time for the temporal approach
to be effective. For instance, the spike anomaly at t = 272
causes broken invariants during [272, 275], and even with noisy
broken invariants, the precision of the temporal algorithm,
averaged over [272, 274], is 0.75 (it drops to below 0.6 when
the t = 275 is included). We plan to improve the robustness
of our temporal algorithm as part of our future work.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:15 UTC from IEEE Xplore.  Restrictions apply. 
Instance
Duration maximum #
broken inv.
# metrics with
broken invariants
1
2
3
13
19
9
3
7
7
3
8
7
CASE STUDY: INTERVALS WITH BROKEN INVARIANTS
TABLE III
A. Case study
We used our spatial and temporal fault localization algo-
rithms to analyze a real-world data collected from 20 sensors
that contains faults. The dataset is partitioned into training and
test sets with each set consisting of one time series with 2800
samples for each sensor. Based on the information we received
from the domain experts, the training set does not contain any
anomalies but the test set does. However, we do not know
which sensors capture these anomalies.
We extract 39 invariants from the training dataset. Fig-
ure 1(a) shows the invariant graph. For test data, we observe
three intervals with multiple broken invariants. These intervals
are shown in Table III along with the maximum number
of broken invariants and the number of metrics associated
with these broken invariants. For the instance shown in row
1 of Table III, we have 3 metrics, m1, m2, and m3, with
broken invariants. For all the 13 time points, their nodeScore is
0.28, 0.17, and 0.17, respectively (m1 has 2 broken invariants
while m2, and m3 have one each). While none of the three
metrics have high nodeScore, m1 has a high neighborScore of
1 (m2 and m3 have neighborScore of 0). Hence, the spatial
algorithms – spatial avg. and spatial rank identify m1 as the
most abnormal metric. Further evidence for a fault in m1
comes from the fact that for the samples in the other two
instances (rows 2 and 3 in Table III), m1 is consistently ranked
the highest. This is despite new metrics (and their broken
invariants) being present in latter two instances. Figure 1(b)
shows the broken invariants at one of the time points from
row 2 in Table III. We manually inspected the samples from
m1 during these three time periods. The average value over
these three windows is 7.2, 17.6, and 16.2 compared to 4.9 for
the entire time series. Hence, this indicates a shift-by-constant
type of fault. We veriﬁed with the customer that provided this
dataset that SIAT’s detection, localization, and characterization
of this fault was accurate.
VIII. RELATED WORK
We brieﬂy discuss work on rank aggregation, and graph
based anomaly detection that informed our two algorithms for
fault localization using SIAT.
Rank aggregation. We mention work done in the ﬁeld
of information retrieval and metasearch for the web [10],
[4], [15] that inspired our spatial algorithms in Section V-B.
Global ranking problems also arise in other scenarios such as
recommendation systems, voting, etc. Often, they involve com-
puting a global rank using quantitative (e.g. a score between
1 and 5) or qualitative preferences from multiple users. With
quantitative scores, we can use combination methods similar
8
our metaranking approach, and others used for information
retrieval [15]. Ranking algorithms based on qualitative prefer-
ences [3] are not directly applicable to SIAT because it uses
quantitative scores.
Graph based anomaly detection. Noble et al. propose
a method for detecting anomalous subgraphs in a given
graph [17] that can be used for detecting insider threats [11]
and software bugs [16]. Our neighborScore is computed based
on the subgraph consisting of the one-hop neighbors of a node
but our goal is to identify abnormal nodes not subgraphs.
IX. CONCLUSION
We presented an overview of our tool SIAT for modeling
dependencies in time series monitoring data collected from
distributed systems, and described real-world problems that
we have addressed using it. We also present two algorithms –
one based on invariant graph and the other on temporal
patterns – to solve the metric ranking and the noise reduction
challenges that we encountered when analyzing real-world
data using SIAT. A key contribution of this paper is to
show that by using pairwise invariant relationships amongst
time series monitoring data, we can transform the task of
fault/anomaly localization in distributed systems into a metric
ranking problem on invariant graphs in the presence of noise.
REFERENCES
[1] GE Intelligent Platforms. http://www.ge-ip.com.
[2] Splunk Inc. http://www.splunk.com.
[3] A. Ammar and D. Shah. Efﬁcient Rank Aggregation Using Partial Data.
In SIGMETRICS, 2012.
[4] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination
of multiple ranked retrieval systems. In SIGIR, 1994.
[5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly Detection: A Survey.
ACM Computing Surveys, 14(3), 2009.
[6] R. N. Charette. This Car Runs on Code. IEEE Spectrum, Feb. 2009.
[7] H. Chen, H. Cheng, G. Jiang, and K. Yoshihira. Exploiting Local and
Global Invariants for the management of large scale information systems.
In ICDM, 2008.
[8] H. Chen, G. Jiang, K. Yoshihira, and A. Saxena. Invariants based failure
diagnosis in distributed computing systems. In IEEE SRDS, 2010.
[9] M. Ding, H. Chen, A. B. Sharma, K. Yoshihira, and G. Jiang. A Data
Analytic Engine Towards Self-Management of Cyber-Physical Systems.
In ICDCS Workshop on Cyber-Physical Networked Systems, 2013.
[10] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation
methods for the Web. In WWW, 2001.
[11] W. Eberle and L. B. Holder. Applying graph-based anomaly detection
apporaches to the discovery of insider threats. In IEEE Intelligence and
Security Informatics, 2009.
[12] G. Box and G. M. Jenkins and G. Reinsel. Time Series Analysis:
Forecasting and Control. John Wiley & Sons, Inc., 2008.
[13] G. Jiang, H. Chen, and K. Yoshihira. Discovering likely invariants of
distributed transaction systems for autonomic system management. In
ICAC, 2006.
[14] G. Jiang, H. Chen, and K. Yoshihira. Efﬁcient and Scalable Algorithms
for Inferring Invariants in Distributed Systems. IEEE Transactions on
Knowledge and Data Engineering, 19(11):1508–1523, 2007.
[15] J. H. Lee. Analyses of Multiple Evidence Combination. In SIGIR, 1997.
[16] C. Liu, X. Yan, H. Yu, J. Han, and P. S. Yu. Mining Behavior Graphs
for Backtrace of Noncrashing Bugs. In ICDM, 2005.
[17] C. Noble and D. Cook. Graph-based anomaly detection. In KDD, 2003.
[18] A. B. Sharma, L. Golubchik, and R. Govindan. Sensor Data Faults:
Detection Methods and Prevalence in Real-World Datasets. Trans. on
Sensor Networks, August 2010.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:15 UTC from IEEE Xplore.  Restrictions apply.