    使用 `traddr`{.literal}
    [*替换nn-0x204600a098cbcac6:pn-0x204700a098cbcac6*]{.emphasis}。
    使用 `host-traddr`{.literal}
    [*替换nn-0x20000090fae0b5f5:pn-0x10000090fae0b5f5*]{.emphasis}。
3.  使用 `nvme-cli`{.literal} 连接到 NVMe 目标：
    ``` screen
    # nvme connect --transport fc \
                   --traddr nn-0x204600a098cbcac6:pn-0x204700a098cbcac6 \
                   --host-traddr nn-0x20000090fae0b5f5:pn-0x10000090fae0b5f5 \
                   -n nqn.1992-08.com.netapp:sn.e18bfca87d5e11e98c0800a098cbcac6:subsystem.st14_nvme_ss_1_1
    ```
    使用 `traddr`{.literal}
    [*替换nn-0x204600a098cbcac6:pn-0x204700a098cbcac6*]{.emphasis}。
    使用 `host-traddr`{.literal}
    [*替换nn-0x20000090fae0b5f5:pn-0x10000090fae0b5f5*]{.emphasis}。
    使用 `subnqn`{.literal} 替换
    [*nqn.1992-08.com.netapp:sn.e18bfca87d5e11e98c0800a098cbcac6:subsystem.st14_nvme_ss_1\_1*]{.emphasis}。
:::
::: itemizedlist
**验证步骤**
-   列出当前连接的 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     80BgLFM7xMJbAAAAAAAC NetApp ONTAP Controller                  1         107.37  GB / 107.37  GB      4 KiB +  0 B   FFFFFFFF
    ```
    ``` screen
    # lsblk |grep nvme
    nvme0n1                     259:0    0   100G  0 disk
    ```
:::
::: itemizedlist
**其它资源**
-   `nvme`{.literal} man page。
-   [NVMe-cli Github
    存储库](https://github.com/linux-nvme/nvme-cli){.link}
:::
:::
::: section
::: titlepage
## []{#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#configuring-the-nvme-initiator-for-qlogic-adapters_nvme-over-fabrics-using-fc}为 QLogic 适配器配置 NVMe initiator {.title}
:::
使用 NVMe 管理命令行界面(`nvme-cli`{.literal})工具为 Qlogic
适配器客户端配置 NVMe initiator。
::: orderedlist
**流程**
1.  安装 `nvme-cli`{.literal} 工具：
    ``` screen
    # yum install nvme-cli
    ```
    这会在 `/etc/nvme/`{.literal} 目录中创建 `hostnqn`{.literal}
    文件。`hostnqn`{.literal} 文件标识 NVMe 主机。
    生成一个新的 `hostnqn`{.literal}：
    ``` screen
    # nvme gen-hostnqn
    ```
2.  重新载入 `qla2xxx`{.literal} 模块：
    ``` screen
    # rmmod qla2xxx
    # modprobe qla2xxx
    ```
3.  查找本地和远程端口的 WWN 和 WWPN 标识符：
    ``` screen
    # dmesg |grep traddr
    [    6.139862] qla2xxx [0000:04:00.0]-ffff:0: register_localport: host-traddr=nn-0x20000024ff19bb62:pn-0x21000024ff19bb62 on portID:10700
    [    6.241762] qla2xxx [0000:04:00.0]-2102:0: qla_nvme_register_remote: traddr=nn-0x203b00a098cbcac6:pn-0x203d00a098cbcac6 PortID:01050d
    ```
    使用这些 `host-traddr`{.literal} 和 `traddr`{.literal}
    值，找到子系统 NQN：
    ``` screen
    # nvme discover --transport fc \
                    --traddr nn-0x203b00a098cbcac6:pn-0x203d00a098cbcac6 \
                    --host-traddr nn-0x20000024ff19bb62:pn-0x21000024ff19bb62
    Discovery Log Number of Records 2, Generation counter 49530
    =====Discovery Log Entry 0======
    trtype:  fc
    adrfam:  fibre-channel
    subtype: nvme subsystem
    treq:    not specified
    portid:  0
    trsvcid: none
    subnqn:  nqn.1992-08.com.netapp:sn.c9ecc9187b1111e98c0800a098cbcac6:subsystem.vs_nvme_multipath_1_subsystem_468
    traddr:  nn-0x203b00a098cbcac6:pn-0x203d00a098cbcac6
    ```
    将 [*nn-0x203b00a098cbcac6:pn-0x203d00a098cbcac6*]{.emphasis} 替换为
    `traddr`{.literal}。
    用 `host-traddr`{.literal} 替换
    [*nn-0x20000024ff19bb62:pn-0x21000024ff19bb62*]{.emphasis}。
4.  使用 `nvme-cli`{.literal} 工具连接到 NVMe 目标：
    ``` screen
    # nvme connect  --transport fc \
                    --traddr nn-0x203b00a098cbcac6:pn-0x203d00a098cbcac6 \
                    --host-traddr nn-0x20000024ff19bb62:pn-0x21000024ff19bb62 \
                    -n nqn.1992-08.com.netapp:sn.c9ecc9187b1111e98c0800a098cbcac6:subsystem.vs_nvme_multipath_1_subsystem_468
    ```
    将 [*nn-0x203b00a098cbcac6:pn-0x203d00a098cbcac6*]{.emphasis} 替换为
    `traddr`{.literal}。
    用 `host-traddr`{.literal} 替换
    [*nn-0x20000024ff19bb62:pn-0x21000024ff19bb62*]{.emphasis}。
    将 `subnqn`{.literal} 替换为
    [*nqn.1992-08.com.netapp:sn.c9ecc9187b1111e98c0800a098cbcac6:subsystem.vs_nvme_multipath_1\_subsystem_468*]{.emphasis}。
:::
::: itemizedlist
**验证步骤**
-   列出当前连接的 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     80BgLFM7xMJbAAAAAAAC NetApp ONTAP Controller                  1         107.37  GB / 107.37  GB      4 KiB +  0 B   FFFFFFFF
    # lsblk |grep nvme
    nvme0n1                     259:0    0   100G  0 disk
    ```
:::
::: itemizedlist
**其它资源**
-   `nvme`{.literal} man page。
-   [NVMe-cli Github
    存储库](https://github.com/linux-nvme/nvme-cli){.link}
:::
:::
::: section
::: titlepage
## []{#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#next_steps_2}后续步骤 {.title}
:::
::: itemizedlist
-   在 FC-NVMe 设备中启用多路径。如需更多信息，请参阅 [第 14 章 *在 NVMe
    设备中启用多路径*](#assembly_enabling-multipathing-on-nvme-devices_managing-storage-devices.html "第 14 章 在 NVMe 设备中启用多路径"){.xref}。
:::
:::
:::
:::
[]{#assembly_enabling-multipathing-on-nvme-devices_managing-storage-devices.html}
::: chapter
::: titlepage
# []{#assembly_enabling-multipathing-on-nvme-devices_managing-storage-devices.html#assembly_enabling-multipathing-on-nvme-devices_managing-storage-devices}第 14 章 在 NVMe 设备中启用多路径 {.title}
:::
您可以多路径通过光纤传输连接到系统的 NVMe
设备，如光纤通道(FC)。您可以在多个多路径解决方案之间进行选择。
::: section
::: titlepage
# []{#assembly_enabling-multipathing-on-nvme-devices_managing-storage-devices.html#con_native-nvme-multipathing-and-dm-multipath_assembly_enabling-multipathing-on-nvme-devices}原生 NVMe 多路径和 DM 多路径 {.title}
:::
NVMe 设备支持原生多路径功能。在 NVMe 中配置多路径时，您可以在标准 DM
多路径框架和原生 NVMe 多路径间进行选择。
DM 多路径和原生 NVMe 多路径支持 NVMe
设备的对称命名空间访问(ANA)多路径方案。ANA
识别目标和启动器之间的优化路径，并提高性能。
当启用原生 NVMe 多路径时，它适用于所有 NVMe
设备。它可以提供更高的性能，但不包含 DM 多路径提供的所有功能。例如：原生
NVMe 多路径只支持 `failover`{.literal} 和 `round-robin`{.literal}
路径选择方法。
红帽建议您在 RHEL 8 中使用 DM 多路径作为默认多路径解决方案。
:::
::: section
::: titlepage
# []{#assembly_enabling-multipathing-on-nvme-devices_managing-storage-devices.html#proc_enabling-native-nvme-multipathing_assembly_enabling-multipathing-on-nvme-devices}启用原生 NVMe 多路径 {.title}
:::
这个过程使用原生 NVMe 多路径解决方案在连接的 NVMe 设备中启用多路径。
::: itemizedlist
**先决条件**
-   NVMe 设备连接到您的系统。
    有关通过光纤传输连接 NVMe 的更多信息，请参阅 [NVMe over fabric
    设备概述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/overview-of-nvme-over-fabric-devicesmanaging-storage-devices){.link}。
:::
::: orderedlist
**流程**
1.  检查内核中是否启用了原生 NVMe 多路径：
    ``` screen
    # cat /sys/module/nvme_core/parameters/multipath
    ```
    命令显示以下内容之一：
    ::: variablelist
    [`N`{.literal}]{.term}
    :   禁用原生 NVMe 多路径。
    [`Y`{.literal}]{.term}
    :   启用了原生 NVMe 多路径。
    :::
2.  如果禁用了原生 NVMe 多路径，请使用以下方法之一启用它：
    ::: itemizedlist
    -   使用内核选项：
        ::: orderedlist
        1.  在内核命令行中添加 `nvme_core.multipath=Y`{.literal} 选项：
            ``` screen
            # grubby --update-kernel=ALL --args="nvme_core.multipath=Y"
            ```
        2.  在 64 位 IBM Z 构架中，更新引导菜单：
            ``` screen
            # zipl
            ```
        3.  重启系统。
        :::
    -   使用内核模块配置文件：
        ::: orderedlist
        1.  使用以下内容创建 `/etc/modprobe.d/nvme_core.conf`{.literal}
            配置文件：
            ``` screen
            options nvme_core multipath=Y
            ```
        2.  备份 `initramfs`{.literal} 文件系统：
            ``` screen
            # cp /boot/initramfs-$(uname -r).img \
                 /boot/initramfs-$(uname -r).bak.$(date +%m-%d-%H%M%S).img
            ```
        3.  重建 `initramfs`{.literal} 文件系统：
            ``` screen
            # dracut --force --verbose
            ```
        4.  重启系统。
        :::
    :::
3.  可选：在运行的系统中，更改 NVMe 设备的 I/O
    策略，以便在所有可用路径中分发 I/O：
    ``` screen
    # echo "round-robin" > /sys/class/nvme-subsystem/nvme-subsys0/iopolicy
    ```
4.  可选：使用 `udev`{.literal} 规则永久设置 I/O 策略。使用以下内容创建
    `/etc/udev/rules.d/71-nvme-io-policy.rules`{.literal} 文件：
    ``` screen
    ACTION=="add|change", SUBSYSTEM=="nvme-subsystem", ATTR{iopolicy}="round-robin"
    ```
:::
::: orderedlist
**验证**
1.  检查您的系统是否识别 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme0n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    ```
2.  列出所有连接的 NVMe 子系统：
    ``` screen
    # nvme list-subsys
    nvme-subsys0 - NQN=testnqn
    \
     +- nvme0 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme1 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme2 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
     +- nvme3 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
    ```
    检查活动的传输类型。例如： `nvme0 fc`{.literal}
    表示该设备通过光纤通道传输连接，`nvme tcp`{.literal} 表示该设备通过
    TCP 连接。
3.  如果您编辑了内核选项，请检查内核命令行中是否启用了原生 NVMe 多路径：
    ``` screen
    # cat /proc/cmdline
    BOOT_IMAGE=[...] nvme_core.multipath=Y
    ```
4.  检查 DM 多路径报告 NVMe 命名空间，例如 `nvme0c0n1`{.literal} 到 [
    *`nvme0c3n1`{.literal}，而不是作为*]{.emphasis} `nvme0n1`{.literal}
    到 `nvme3n1`{.literal}:
    ``` screen
    # multipath -ll | grep -i nvme
    uuid.8ef20f70-f7d3-4f67-8d84-1bb16b2bfe03 [nvme]:nvme0n1 NVMe,Linux,4.18.0-2
    | `- 0:0:1    nvme0c0n1 0:0     n/a   optimized live
    | `- 0:1:1    nvme0c1n1 0:0     n/a   optimized live
    | `- 0:2:1    nvme0c2n1 0:0     n/a   optimized live
      `- 0:3:1    nvme0c3n1 0:0     n/a   optimized live
    uuid.44c782b4-4e72-4d9e-bc39-c7be0a409f22 [nvme]:nvme0n2 NVMe,Linux,4.18.0-2
    | `- 0:0:1    nvme0c0n1 0:0     n/a   optimized live
    | `- 0:1:1    nvme0c1n1 0:0     n/a   optimized live
    | `- 0:2:1    nvme0c2n1 0:0     n/a   optimized live
      `- 0:3:1    nvme0c3n1 0:0     n/a   optimized live
    ```
5.  如果您更改了 I/O 策略，请检查 `round-robin`{.literal} 是否在 NVMe
    设备中是活跃的 I/O 策略：
    ``` screen
    # cat /sys/class/nvme-subsystem/nvme-subsys0/iopolicy
    round-robin
    ```
:::
::: itemizedlist
**其它资源**
-   [有关编辑内核选项的更多信息，请参阅配置内核命令行参数](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/configuring-kernel-command-line-parameters_managing-monitoring-and-updating-the-kernel){.link}。
:::
:::
::: section
::: titlepage