Prompt工程
模型框架/模型库
• LMOps Landscape
分布式计算
百度智能云千帆大模型平台全景图
大模型 百度文心大模型 第三方大模型
| 大语言模型 | 文心一言 | GLM |  LLaMa 2 | Falcon |
|---|---|---|---|---|
| 多模态模型 |文心一格 |MiniGPT4 |Stable Diffusion |Aquila |
| 代码模型 |文心Comate |StarCoder |… |… |
MaaS
| 大 | 数据管理 | 模型训练 | 评估&优化 | 预测服务部署 | Prompt 工程 | Plugin插件 ||---|---|---|---|---|---|---|
| 大 |数据清洗 |Post-pretraining |评估&优化 |服务部署及托管 |Prompt 工程 |Plugin插件 |
| 大 |数据清洗 |Post-pretraining |大模型仓库 |服务部署及托管 |Prompt模板 |插件库 |
| 模 |数据清洗 |Post-pretraining |大模型仓库 |服务部署及托管 |Prompt模板 |插件库 |
| 型 |数据生成 |SFT |大模型评估 |Profile记忆 |Prompt推荐 |插件编排 |
| 工 |数据标注 |RLHF |大模型压缩 |在线测试器 |Prompt自动优化 |插件编排 |
| 具 |数据标注 |RLHF |大模型压缩 |在线测试器 |Prompt自动优化 |插件编排 |
| 链 |数据回流 |训练可视化 |大模型安全 |多模型调度 |Prompt自动优化 |插件编排 |增量训练
PaaS 大模型训练加速 大模型场景建模 大模型应用集成
端到端并行训练优化 大模型生产线 大模型预置场景 自动化推理服务监控 全流程 LMOps SDK
IaaS
训推加速 稳定性 一云多芯 安全可信
大模型推理性能优化
大模型量化
什么是模型量化 
模型量化是指将神经网络的浮点算法转换 为定点, 从而降低模型权重数值的存储空
为什么专门开展大模型量化 
1.大模型不支持QAT 
QAT 通常认为比 PTQ 更容易获得高精度，在
| 间 | 原始权重格式为FP32,  | 小模型量化上广泛使用，但是，大模型训练成 |
|---|---|---|
| 间 |原始权重格式为FP32,  |本太高，导致QAT失效 |
| 间 |压缩为int8/4 |2. 不均匀 |
不同的大模型权重和激活分布很不均匀，很难
实现激活和权重的同时低比特压缩不同的大模型权重和激活分布很不均匀，很难
实现激活和权重的同时低比特压缩
百度智能云针对大模型提供四种PTQ量化方案
| weight only int8 | weight only int4 | weight int8  | weight int8  |
|---|---|---|---|
| weight only int8 |weight only int4 |weight int8  |activation int8 + |
| weight only int8 |weight only int4 |activation int8 |activation int8 + |
| weight only int8 |weight only int4 |activation int8 | k/v cache int8 || 1.离线模型权重按照 | 1.离线模型权重按照 | 在量化前序增加 | 新增 k/v cache int8  |
|---|---|---|---|
| per-channel粒度压缩 |per-channel粒度压缩 |在量化前序增加 |新增 k/v cache int8  |
| 到int8 |到int4 |在量化前序增加 |新增 k/v cache int8  |
| 2.在线将权重反量化为 |2.在线将权重反量化为 |在量化前序增加 |新增 k/v cache int8  |
| 2.在线将权重反量化为 |2.在线将权重反量化为 |在量化前序增加 |量化，进一步节约运 |
| 2.在线将权重反量化为 |2.在线将权重反量化为 |smooth过程，解决大 |量化，进一步节约运 |
| FP16 |FP16 |smooth过程，解决大 |量化，进一步节约运 || FP16 |FP16 |smooth过程，解决大 |量化，进一步节约运 |
| FP16 |FP16 |smooth过程，解决大 |行时显存，实现真正 |
| 3.推理过程中提供混 |3.推理过程中提供混 |模型权重分布不均匀 |行时显存，实现真正 |
| 3.推理过程中提供混 |3.推理过程中提供混 |模型权重分布不均匀 |意义上的全流程int8  |
| 合精度GEMM（per- |合精度GEMM（per- |的问题 |意义上的全流程int8  |
| 合精度GEMM（per- |合精度GEMM（per- |的问题 |量化 |
| channel反量化 |channel反量化 |的问题 |量化 |
| +GEMM 高性能融 |+GEMM 高性能融 |的问题 |量化 |
合）算子 合）算子
压缩强度从弱到强
PTQ量化方案: Weight-only压缩强度从弱到强
PTQ量化方案: Weight-only
粗粒度 per-channel 权重 int8/4
细粒度 per-group 权重 int8/4
| 原 | 1. 离线模型权重按照per-channel粒度压缩到 | 原 | 1. 权重压缩过程中引入激活信息，保证量 |
|---|---|---|---|
| 原 |int8/4 |原 |化精度 |
| 原 |int8/4 |原 |2. 离线模型权重按照per-group粒度压缩 |
| 原 |2. 在线将权重反量化为 FP16 |原 |2. 离线模型权重按照per-group粒度压缩 |
| 理 |2. 在线将权重反量化为 FP16 |理 |2. 离线模型权重按照per-group粒度压缩 |
| 理 |3. 推理过程中提供混合精度GEMM（per- |理 |到int8/4 || 理 |channel反量化+GEMM 高性能融合）算子 |理 |3. 提供reorder-free 的高效kernel |
| 效 |1. 支持几乎所有LLM |效 |1. 无需reorder操作，保证速度的同时做到 |
| 效 |2. 精度无损 |效 |1. 无需reorder操作，保证速度的同时做到 |
| 效 |2. 精度无损 |效 |精度 |
| 效 |3. 权重压缩到1/2到1/4 |效 |精度 |
| 果 |3. 权重压缩到1/2到1/4 |果 |精度 |
| 果 |3. 权重压缩到1/2到1/4 |果 |2. 支持多模态模型量化 |
| 果 |4. Batch Size提升20% |果 |2. 支持多模态模型量化 |
PTQ量化方案: 激活int8+权重int8
| 原 | 1. 在量化前序增加smooth过程，解决大模型权重分布 | 效 | 1. 覆盖OPT，LLama，Bloom，GLM等模 ||---|---|---|---|
| 原 |1. 在量化前序增加smooth过程，解决大模型权重分布 |效 |型家族 |
| 原 |不均匀的问题 |效 |型家族 |
| 原 |2. 引入超参s可以平衡激活量化难度和权重量化难度之 |效 |2. 精度近乎无损 |
| 理 |2. 引入超参s可以平衡激活量化难度和权重量化难度之 |果 |2. 精度近乎无损 |
| 理 |2. 引入超参s可以平衡激活量化难度和权重量化难度之 |果 |3. 实现全int8 GEMM 计算，速度最大提升 |
| 理 |间的gap，泛化性更好 |果 |3. 实现全int8 GEMM 计算，速度最大提升 |
| 理 |间的gap，泛化性更好 |果 |1.56x |
| 理 |间的gap，泛化性更好 |果 |4. 权重参数和运行显存减少一半，原始8卡 |
| 理 |间的gap，泛化性更好 |果 |模型可以用4卡使能，节约一半卡数 |PTQ量化方案: 激活int8+权重int8+K/V cache int8
| 原 | 1. 前述方案中，权重和激活都是int8 保存的，但是大 | 效 | 在保证速度的情况下，显存进一步压缩15% |
|---|---|---|---|
| 原 |模型中另外一个消耗显存的运行时参数 k/v cache |效 |在保证速度的情况下，显存进一步压缩15% |
| 原 |依然是FP16保存的 |效 |在保证速度的情况下，显存进一步压缩15% |
| 理 |依然是FP16保存的 |果 |左右 |
| 理 |2. 新增 k/v cache int8 量化，进一步节约运行时显 |果 |左右 |
| 理 |存，实现真正意义上的全流程int8 量化 |果 |左右 |
| 以GPT3-175B模型推理为例，该模型有32层，隐藏 | 原始显存占用 | 350GB | 350GB | 164GB | 514GB ||---|---|---|---|---|---|
| 层维度为4096，注意力头数为963。假设批次大小为 |原始显存占用 |350GB |350GB |164GB |514GB |
| 64，输入序列长度为512，输出序列长度为32，则 |常规int8方案 |175GB |164GB |339GB |514GB |
| K/V cache占用显存为:  |常规int8方案 |175GB |164GB |339GB |514GB |
本方案 175GB 82GB 257GB
大模型稀疏
什么是模型稀疏
为什么专门开展大模型稀疏优化
| 稀疏压缩是模型压缩另一个有效手段，其 核心是将模型中的矩阵参数压缩成适合硬 件架构的格式，通过spare tensor core这 样的硬件结构对压缩后的GEMM进行加 速，降低推理延迟 | 压缩的必要性 | 压缩的必要性 ||---|---|---|
| 稀疏压缩是模型压缩另一个有效手段，其 核心是将模型中的矩阵参数压缩成适合硬 件架构的格式，通过spare tensor core这 样的硬件结构对压缩后的GEMM进行加 速，降低推理延迟 |• |大模型参数巨大，控制压缩时间，提升压缩 |
| 稀疏压缩是模型压缩另一个有效手段，其 核心是将模型中的矩阵参数压缩成适合硬 件架构的格式，通过spare tensor core这 样的硬件结构对压缩后的GEMM进行加 速，降低推理延迟 |	效率很有必要  动态压缩的必要性 |	效率很有必要  动态压缩的必要性 |
| 稀疏压缩是模型压缩另一个有效手段，其 核心是将模型中的矩阵参数压缩成适合硬 件架构的格式，通过spare tensor core这 样的硬件结构对压缩后的GEMM进行加 速，降低推理延迟 |• |常用的静态压缩方案会导致大模型精度损失 |严重，需要有动态压缩方案
百度智能云针对大模型稀疏方案: SparseGPT
| 原 | 1.利用hessian矩阵量化稀疏误差，同时迭 | 效 | 适用于千亿参数规模模型 |
|---|---|---|---|
| 原 |1.利用hessian矩阵量化稀疏误差，同时迭 |效 |将大模型gemm矩阵压缩为2:4结构化数 |
| 理 |1.利用hessian矩阵量化稀疏误差，同时迭 |果 |将大模型gemm矩阵压缩为2:4结构化数 |
| 理 |代式的重构稀疏矩阵，避免多次重复运算 |果 |将大模型gemm矩阵压缩为2:4结构化数 |
| 理 |代式的重构稀疏矩阵，避免多次重复运算 |果 |据，性能最大提升60%+ |
百度智能云针对大模型稀疏方案: WandA
| 原 | 将原始单纯基于权重幅度的减枝方案变为 | 效 | 1. 更高的压缩效率：保证精度不变的情况 ||---|---|---|---|
| 原 |将原始单纯基于权重幅度的减枝方案变为 |效 |下，压缩速度提升数十倍 |
| 理 |将原始单纯基于权重幅度的减枝方案变为 |果 |2. 数据依赖更轻量：依赖更少量的校准数 |
| 理 |基于激活和权重幅度的混合减枝方案 |果 |2. 数据依赖更轻量：依赖更少量的校准数 |