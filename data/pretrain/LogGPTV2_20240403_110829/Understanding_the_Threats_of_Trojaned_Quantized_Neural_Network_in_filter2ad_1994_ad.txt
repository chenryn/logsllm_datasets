the QNN during the backdoor injection process, where the x-axis
represents the number of epochs.
Figure 3: The curves of loss on trigger inputs (Left) and the
ASR (Right) of a quantized LeNet during the backdoor injec-
tion process of QUASI, with or without the clipping-related
regularization term Rclipping.
Results & Analysis. As is shown in Fig. 3, when comparing the
loss (ASR) curves of QUASI with and without the clipping-related
regularization term Rclipping, we clearly observe that the backdoor
injection process with the regularization term converges in a more
stable and efficient way. For example, when the regularization term
is present, the loss uniformly converges from about the 750-th
epoch, while the corresponding ASR reaches near-100%. As a con-
trast, the loss and the ASR fluctuates radically from the 750-th
epoch when QUASI is without the clipping term. In other words,
the phenomenon validates that, by exploiting the clipping mecha-
nism to enlarge the computational discrepancy, Rclippingdoes help
QUASI to resolve the tension between the two seemingly conflicting
constraints on the prediction behavior of the target model, which
essential improves the stability and attack efficiency of QUASI.
4.3 Potential Countermeasures
In the following, we evaluate two representative mitigation algo-
rithms against backdoor attacks on FPNN which we find can be
extended to the QNN context.
4.3.1 Entropy-Based Trigger Detection. First, we evaluate the eva-
sive capability of QUASI against STRIP [21], an entropy-based trig-
ger detection algorithm published at ACSAC’19 which works by
checking whether the prediction entropy of an input under inspec-
tion remains low when the input is randomly imposed with other
clean inputs. Following the recommended evaluation protocol in
the original paper [21], we prepare a QUASI-trojaned model and
2000 trigger (clean) inputs as the test set. We then linearly blend
each test image with 100 clean inputs to obtain 100 perturbed im-
ages, with which we calculate the average prediction entropy (for
simplicity, referred to as entropy) of the perturbed images to serve
as the indicator to distinguish trigger and clean inputs. When a test
Figure 2: T-SNE visualization of the latent features of clean
samples from 5 different classes (including the target class)
and the latent features of trigger samples from a QUASI-
trojaned LeNet model before (Top) and after the quantiza-
tion (Bottom).
Results & Analysis. As we can see from Fig. 2, the visualization
results provide an illustrative support to the attack effectiveness in
Table 1. Before the quantization, the trigger inputs are generally
clustered with the clean inputs from the same ground-truth class.
For the demonstration purpose, the trigger inputs (i.e., in empty
circle marks) are tinted with the same color of the clean samples
from the same class at the top of Fig. 2. Consequently, viewing the
trigger inputs and clean inputs in the same color are paired with one
another, we conclude that the QUASI-trojaned FPNN models both
the trigger and clean inputs similarly in the feature space according
to the class label, which results in the preserved normal accuracy
on the trigger inputs. Strikingly, when the quantization operation is
conducted on the trojaned FPNN, the latent features of the trigger
inputs are immediately dragged to a region near to the distribution
of the target class (i.e., in diamond marks), which directly causes
the QNN to mispredict the trigger inputs in the target class. As a
general statement, the above phenomenon highlights the risk of
quantization caused by the discrepency between the full-precision
model and the quantized counterpart. As the effectiveness of QUASI
shows, it is practical for the adversary to exploit the computational
8
200204060402002040Before QuantizationClass 0 (Target)Class 1Class 2Class 3Class 4Class 0 (Triggers)Class 1 (Triggers)Class 2 (Triggers)Class 3 (Triggers)Class 4 (Triggers)504030201001020201001020304050After QuantizationClass 0 (Target)Class 1Class 2Class 3Class 4Triggers0250500750100012501500Epoch12345LossTrigger Loss (w/o. clipping)Trigger Loss (w. clipping)0250500750100012501500Epoch2030405060708090100ASR-WQ (%)ASR (w/o. clipping)ASR (w. clipping)641Understanding the Threats of Trojaned Quantized Neural Network in Model Supply Chains
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
4.3.2 Pruning-Based Trojan Elimination. Besides the entropy-based
trigger detection algorithm above, we also evaluate the defense capa-
bility of pruning-based backdoor elimination. In general, we follow
the methodology of Fine-Pruning [42], a pruning-based backdoor
elimination algorithm on FPNN, to implement an iterative pruning
algorithm on QNNs. Specifically, following [42], we record the aver-
age feature map of each channel in the penultimate convolutional
layer on a validation dataset, iteratively prune the channels from
the convolutional layer in increasing order of the L2 norm of the
average feature map, and record the accuracy of the pruned QNN
on the validation dataset. The pruning process is terminated when
the normal accuracy decreases by 20%, usually an unacceptable
performance decrease for mission-critical tasks in, e.g., healthcare
and autonomous driving. As back-propagation is not implemented
for QNNs, the defense we consider does not involve the fine-tuning
procedure in the original Fine-Pruning. Fig. 5 plots the ASR and the
ACC curves of the QNN under different ratios of pruned channels.
Figure 5: The curves of ACC and ASR of a QUASI-trojaned
QNN when the ratio of pruned redundant channels of the
last convolutional layer increases.
Results & Analysis. As shown in Fig. 5, in both cases, the ASR
remains close to 100% until the normal accuracy of the pruned QNN
drops by 20%. For example, when 25% of channels are pruned from
the convolutional layer, the accuracy of LeNet on GTSRB drops
from 77.25% to 56.83%, which may be a huge accuracy loss for
embedded systems relying on the QNN model for traffic sign recog-
nition. In summary, the results strongly validate the robustness of
our proposed QUASI attack against pruning-based defenses, i.e.,
the model consumer could hardly eliminate the backdoor from the
quantized model without devastating the normal accuracy of the
model. Other than STRIP and the variant of Fine-Pruning we study
above, there do exist other backdoor defenses developed for FPNNs
(e.g., Neural Cleanse [60] and ABS [43]). However, as most of the
other existing defenses on FPNN heavily rely on back-propagating
the gradient signal to reverse-engineer the trigger patterns or un-
learn the backdoor function from the trojaned model, they could
hardly be applied to safeguard QNNs from backdoor attacks. Consid-
ering the severity of the practical threats imposed by our proposed
quantization-specific attack, we hope more research efforts would
be devoted to the mitigation of backdoor attacks on QNNs.
Figure 4: The distribution of prediction entropy of a trojaned
model when the clean and trigger inputs are perturbed ac-
cording to the procedure of STRIP [21].
sample has an entropy lower than a predefined threashold τ, then
the input is recognized as a trigger. Following [21], we control the
threshold τ to classify only 5% of the clean inputs falsely as trigger
inputs. Under the above configuration, Table 3 reports the statistics
of the detection on 4 combinations of the scenario and the target
model, with Fig. 4 providing the histogram of the entropy of clean
and trigger inputs on two of the test cases.
Table 3: Detection results of STRIP on QUASI-trojaned mod-
els.
GTSRB
CIFAR-10
Median (clean)
Median (trigger)
Threshold (τ)
F1 Score
Precision
Recall
Accuracy
LeNet VGG-13 LeNet VGG-13
0.968
1.187
0.475
0.177
51.72%
10.70%
50.35%
0.452
0.508
0.168
0.148
46.85%
8.80%
49.40%
1.056
1.165
0.562
0.052
35.90%
2.80%
48.90%
0.333
0.51
0.134
0.015
14.09%
0.82%
47.91%
Results & Analysis. As we can see from Fig. 4, the entropy distri-
butions of trigger and clean inputs are highly similar, if not indis-
tinguishable, to one another. Moreover, in both cases, the median
and average entropy of the trigger inputs are even slightly higher
than that of the clean input, e.g., 1.187 (trigger) > 0.968 (clean)
for a trojaned LeNet on GTSRB, which invalidates the assumption
underlying the STRIP detection. Consequently, as is shown in Table
3, on each test case, the detection accuracy stays close to 50%, no
better than random guessing.
9
0.00.51.01.52.02.53.00.00.20.40.60.81.01.2DensityGTSRB, LeNetClean InputsTrigger Inputs0.00.20.40.60.81.0Entropy of Prediction0.00.51.01.52.02.53.03.5DensityCIFAR-10, VGG-13Clean InputsTrigger Inputs1/162/163/164/1660708090ACC (%)GTSRB, LeNet1/162/163/164/165/166/167/168/169/1610/16Ratio of Pruned Channels6570758085ACC (%)CIFAR-10, VGG-13020406080100ASR (%)ACC (Trojaned)ASRACC (Clean)020406080100ASR (%)ACC (Trojaned)ASRACC (Clean)642ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Pan et al.
5 RELATED WORKS
5.1 Security Issues of QNN
5.1.1 Adversarial Robustness of QNN. Considering the critical role
of QNNs on intelligent end devices, a number of very recent works
start to explore and evaluate the robustness of QNNs against ad-
versarial examples [20, 28, 32, 40, 46], i.e., clean inputs with small,
intentional feature perturbations which cause a deployed QNN
to give incorrect prediction [57]. At the attacker’s side, Gupta et
al. [28] and Khalil et al. [32] point out the vulnerability of bina-
rized neural networks against adversarial examples, by generating
adversarial examples respectively with temperature scaling gra-
dient descent and mixed integer programming. From a different
perspective, Matachana et al. [46] and Duncan et al. [20] indepen-
dently study the transferability of adversarial examples from the
full-precision model to the quantized version yet they reach conflict-
ing conclusions to one another. At the defender’s side, Lin et al. [40]
recently propose a Lipschitz constant-based regularizer to improve
the adversarial robustness of QNNs. Essentially orthogonal to the
above research line, our current work is the first to investigate the
vulnerability of QNNs under backdoor attacks, which, compared
with adversarial examples targeting at a deployed model with fixed
parameters, tampers the model parameters and the training process
of the target model to embed backdoor function.
5.1.2 Compromising QNN via System Attacks. As QNN is practi-
cally hosted on embedded systems, several system-flavor works
also study the possibility of exploiting software/hardware loopholes
to compromise the normal functionality of QNNs. For example, a
research line under the name of bit-flip attack (e.g., [29, 50, 65]) ex-
ploits memory fault injection techniques like Row-Hammer [33, 52]
or Laser Beam Attack [53] to flip a small number of bits in the DNN
parameters to cause huge performance degradation in the target
QNN. Interestingly, probably the most relevant to our work, Rakin
et al. [51] recently explore to apply the bit-flip attack to inject back-
door into quantized models. However, their attack works under
a highly different threat model, especially their assumption on a
physical access to the hardware where the target model is deployed,
compared with the one adopted by algorithm-flavor backdoor at-
tacks and ours. Besides, as reported in [51], the ASR of a backdoor
constructed by bit flipping is usually 70% ∼ 80%, uniformly lower
than the ASR of algorithmic attacks. Alongside the availability and
integrity attacks on QNN, few research works analyze the confi-
dentiality of QNN via side-channel attacks [62, 63, 67].
5.2 Backdoor Attacks and Defenses
5.2.1 Previous Backdoor Attacks. In the past five years, backdoor at-
tacks on neural networks have been intensively studied. Paralleled
with the advances in backdoor injection algorithms [8, 26, 44, 66],
to design an effective yet stealthy trigger pattern is also an impor-
tant direction in previous works. From hand-crafted pixel patterns
in the earliest designs [26], Liu et al. first propose to construct
an optimized pixel pattern from the learning perspective, which
strengthens the attack effectiveness and is also adopted in later
works [66]. Recently, the development of backdoor defenses and
detection algorithms arouses a line of research works on enhancing
the stealthiness of trigger designs. For instance, trigger patterns are
10
designed to be invisible [39, 59], involve only one pixel [9, 58] or
natural semantic objects [15, 41] in terms of the appearance. Mean-
while, two very recent parallel works further extend the trigger
design to be dynamic. For example, Salem et al. suggests a dynamic
backdoor attack which relies on random pixel patterns inserted at
random positions of a base image [54], while Nguyen et al. design
the first input-aware backdoor attack on image classifiers, where
a generative model learns to generate a trigger pixel pattern dy-
namically for each base image [47]. However, to the best of our
knowledge, existing backdoor attacks mostly focus on injecting
backdoor function into FPNNs. Considering the increasingly im-
portant role of QNN on intelligent IoT devices deployed for safety
and security critical environments, to understand the backdoor
vulnerability of QNN alongside the implications on the security
of third-party supply chains is an orthogonal yet equally urgent
open problem to investigate. To the best of our knowledge, our
current work provides the first systematic study on the backdoor
vulnerability of QNNs.
5.2.2 Existing Backdoor Defensive Techniques. Motivated by the
TrojAI program [6], mitigation studies on backdoor attacks are
accelerated in recent years. Below, we review two representative
branches of trojan defenses categorized based on the applicable
scenarios: (i) A major branch of backdoor defenses work under
the scenario that the defender is provided with an image classi-
fier and is required to determine whether the classifier is trojaned
[13, 27, 43, 49, 60]. Neural Cleanse [60] and ABS [43] are two repre-
sentative defensive methods in this branch, both of which exploit
the correlation between the trigger pattern and model’s misbehavior
and leverage an optimization-based approach to reverse-engineer
the possible trigger pattern. However, as widely recognized, a quan-
tized model is poor in propagating the gradient signal through the
integer-valued weights and activations. Consequently, most of the
popular deep learning frameworks do not provide the feature of
executing gradient-based optimization with respect to a QNN. (ii)
Another branch of detection methods focus on eliminating trigger
inputs during the run time, by inspecting the abnormality exhibited
in the provided input [18, 19, 21]. For example, a representative
detection algorithm STRIP [21] determines whether an input is a
trigger by first superimposing the input with random clean inputs
and then checking if the prediction results still remain unchanged.
As STRIP only requires the model under inspection supports for-
warding computation, STRIP can be directly extended to the QNN
case. However, as is shown in Section 4.3.2, STRIP fails to effec-
tively distinguish the trigger inputs from the clean inputs, probably
because of the stealthiness-oriented design of QUASI.
6 CONCLUSION
In this paper, we present the first quantization-specific backdoor
attack, i.e., QUASI, which allows the attacker to exploit the rarely
analyzed trojan vulnerability of third-party QNN supply chains.
By executing QUASI, the attacker has the capability of crafting a
backdoor specific to the QNN, which eliminates the possibility of
the model consumer to trace the responsible party for an under-
going backdoor exploitation, if feasible, via the prediction API of
the FPNN. Such a threatening characteristic of QUASI is realized
by injecting a backdoor into the full-precision model first, where
643Understanding the Threats of Trojaned Quantized Neural Network in Model Supply Chains
ACSAC ’21, December 6–10, 2021, Virtual Event, USA