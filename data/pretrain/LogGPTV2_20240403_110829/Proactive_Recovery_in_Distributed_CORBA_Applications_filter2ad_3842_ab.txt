replica has used 80% of its allocated resources, the Proac-
tive Fault-Tolerance Manager at that replica requests the Re-
covery Manager to launch a new replica. If the replica’s re-
source usage exceeds our second threshold, e.g., when 90%
of the allocated resources have been consumed, the Proac-
tive Fault-Tolerance Manager at that replica can initiate the
migration of all its current clients to the next non-faulty
server replica in the group.
We faced two main challenges when developing the
Proactive Fault-Tolerance Manager. The ﬁrst challenge lay
in determining how often to initiate proactive recovery. If we
triggered fault-recovery too early, the additional overhead
of unnecessarily failing over clients to non-faulty servers
quickly negated the beneﬁts of using a proactive strategy.
On the other hand, if we waited too long to initiate recov-
ery, the very purpose of a proactive strategy is lost because
we simply did not allow the system enough time to fail-over
client processes to a non-faulty server; in this case, the re-
sulting fault-recovery ends up resembling a reactive strat-
egy. The ideal time to trigger proactive recovery depends on
a number of factors, such as the server’s fault-rate as well
as the amount of time required for fault-recovery. The fault-
recovery time includes the time to ﬁnd an alternative work-
ing server replica, to restore its state consistently, and then
to fail-over clients to use this working replica.
The second challenge lay in ensuring that the faulty
replica reached a quiescent state before it could be restarted.
We found that simply restarting a server replica when the re-
juvenation threshold was reached caused clients to experi-
ence a large “spike” in their measured round-trip times, due
to the resulting CORBA exceptions that are handled on the
client side. To reduce these “spikes,” we used proactive re-
covery messages to seamlessly redirect existing clients to
the next non-faulty server in the group. These proactive re-
covery messages are described in detail in Section 4.
3.3. The MEAD Recovery Manager
Within our proactive dependability framework, the MEAD
Recovery Manager is responsible for launching new server
replicas that restore the application’s resilience after a
server replica or a node crashes. Thus, the Recovery Man-
ager needs to have up-to-date information about the server’s
degree of replication (i.e., number of replicas). To propa-
gate the replicated server’s group membership information
to the Recovery Manager, we ensure that new server repli-
cas join a unique server-speciﬁc group as soon as they
are launched. By subscribing to the same group, the Re-
covery Manager can receive membership-change noti-
ﬁcations. For instance,
the
Recovery Manager receives a membership-change notiﬁca-
tion from Spread, and can launch a new replica to replace
the failed one.
if a server replica crashes,
The Recovery Manager also receives messages from the
MEAD Proactive Fault-Tolerance Manager whenever the
Fault-Tolerance Manager anticipates that a server replica
is about to fail. These proactive fault-notiﬁcation messages
can also trigger the Recovery Manager to launch a new
replica to replace the one that is expected to fail. We recog-
nize that our Recovery Manager is currently a single point-
of-failure – future implementations of our framework will
allow us to extend our proactive mechanisms to the Recov-
ery Manager as well.
4. Proactive Recovery Schemes
The Proactive Fault-Tolerance Manager implements proac-
tive recovery through three different schemes: GIOP
LOCATION FORWARD Reply messages, GIOP
NEEDS ADDRESSING MODE Reply messages and, ﬁ-
nally,
through MEAD’s own proactive fail-over mes-
sages. For each of these schemes, we describe how the
Proactive Fault-Tolerance Manager transfers clients con-
nected to a faulty replica over to a non-faulty replica,
along with the associated trade-offs. Each scheme as-
sumes that
the application uses CORBA’s persistent
object key policies to uniquely identify CORBA ob-
jects in the system. Persistent keys transcend the life-time
of a server-instance and allow us to forward requests eas-
ily between server replicas in a group. Persistent object
keys also eliminate any non-determinism due to differ-
ences in object keys across different replicas.
4.1. GIOP LOCATION FORWARD Messages
this scheme, we intercept
CORBA’s GIOP speciﬁcation [11] deﬁnes a LOCA-
TION FORWARD Reply message that a server can use
to redirect its current clients to an alternative server lo-
cation. The body of this Reply message consists of an
Interoperable Object Reference (IOR) that uniquely identi-
ﬁes the CORBA object at the new server location. To im-
plement
the IOR returned
by the Naming Service when each server replica regis-
ters its objects with the Naming Service. We then broad-
cast these IORs, through the Spread group communication
to the MEAD Fault-Tolerance Managers col-
system,
located with the server
replicas. Thus, each MEAD
Fault-Tolerance Manager hosting a server replica is popu-
lated with the references of all of the other replicas of the
server.
When the server-side MEAD Fault-Tolerance Manager
senses that its replica is about to crash, it suppresses its
replica’s normal GIOP Reply message to the client, and
instead sends a LOCATION FORWARD Reply message
containing the address of the next available server replica.
The client ORB, on receiving this message, transparently
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:12 UTC from IEEE Xplore.  Restrictions apply. 
Client
Server
Replica1
Server
Replica2
Client
Server
Replica1
Server
Replica2
1
3
2
1
2
3
Client
Server
Replica1
Server
Replica2
1
3
4
2
1 Client sends request
2
3
Server sends GIOP LOCATION_FWD
Client ORB retransmits request
1 Client sends request
2
3
Client Interceptor detects server
failure and requests address of
new primary from Spread group
Client ORB retransmits request
to new primary
1
2
3
4
Client sends request
Server attaches MEAD message
to reply
Client receives reply
Client Interceptor redirects
connection to Replica2
(a) GIOP LOCATION_FORWARD
(b) GIOP NEEDS_ADDR
(c) MEAD Message
Figure 2. Sequence diagrams of proactive recovery schemes.
retransmits the client request to the new replica without no-
tifying the client application (see Figure 2a). The main ad-
vantage of this technique is that it does not require an In-
terceptor at the client because the client ORB handles the
retransmission through native CORBA mechanisms. How-
ever, the server-side Interceptor must maintain some sys-
tem state because an IOR entry is required for every object
instantiated within the server; in addition, the client ORB
has to resend the request, leading to increased bandwidth.
This scheme also incurs a high overhead because we need
to parse incoming GIOP Request messages to extract the
request id ﬁeld so that we can generate corresponding
LOCATION FORWARD Reply messages that contain the
correct request id and object key. One of the optimiza-
tions that we add to this scheme is the use of a 16-bit hash of
the object key that facilitates the easy look-up of the IORs,
as opposed to a byte-by-byte comparison of the object key
(which was typically 52 bytes in our test application).
4.2. NEEDS ADDRESSING MODE Messages
The GIOP NEEDS ADDRESSING MODE Reply mes-
sage [11] directs the client to supply more addressing in-
formation, and usually prompts the client ORB to resend
the request. We used this scheme to investigate the effect of
suppressing abrupt server failures from the client applica-
tion, in case the server does not have enough time to initiate
proactive recovery before it fails. We detect abrupt server
failures when the read() call at the client Interceptor re-
turns an End-Of-File (EOF) response. At this point, we con-
tact the MEAD Fault-Tolerance Manager at the server repli-
cas (using the underlying group communication system) to
obtain the address of the next available replica. The ﬁrst
server replica listed in Spread’s group-membership list re-
sponds to the client’s request (see Figure 2b).
If the client does not receive a response from the server
group within a speciﬁed time (we used a 10ms timeout),
the blocking read() at the client-side times out, and a
CORBA COMM FAILURE exception is propagated up to
the client application. If, on the other hand, we receive the
address of the next available replica, we then redirect the
current client connection to the new replica at the Intercep-
tor level, and fabricate a NEEDS ADDRESSING MODE
Reply message that causes the client-side ORB to retrans-
mit its last request over the new connection.
The advantage of this technique is that it masks com-
munication failures from the client application, but it some-
times takes the client longer to recover from the failure, as
compared to a reactive scheme where we would expose the
client to the failure and let it recover on its own. We do not
recommend this technique because it sometimes increases
the average fail-over time, and it is based on the assump-
tion that an EOF response corresponds to an abrupt server
failure, which is not always the case.
4.3. MEAD Proactive Fail-over Messages
In this scheme, the Proactive Fault-Tolerance Manager in-
tercepts the listen() call at the server to determine
the port on which the server-side ORB is listening for
clients. We then broadcast this information over Spread so
that the Proactive Fault-Tolerance Manager at each server
replica knows the hostname and the port of the other repli-
cas in the group. Whenever group-membership changes oc-
cur (and are disseminated automatically over Spread), the
ﬁrst replica listed in the Spread group-membership mes-
sage sends a message that synchronizes the listing of ac-
tive servers across the group.
When MEAD detects that a replica is about to fail,
it sends the client-side Proactive Fault-Tolerance Manager
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:12 UTC from IEEE Xplore.  Restrictions apply. 
a MEAD proactive fail-over message containing the ad-
dress of the next available replica in the group (see Fig-
ure 2c). We accomplish this by piggybacking regular GIOP
Reply messages onto the MEAD proactive failover mes-
sages. When the client-side Interceptor receives this com-
bined message, it extracts (the address in) the MEAD mes-
sage to redirect the client connection to the new replica so
that subsequent client requests are sent to the new replica.
The Interceptor then transmits the regular GIOP Reply
message up to the client application.
The redirection of existing client connections is accom-
plished by the Interceptor opening a new TCP socket, con-
necting to the new replica address, and then using the UNIX
dup2() call to close the connection to the failing replica,
and point the connection to the new address (an alternative
to this scheme would be to use the migratory TCP proto-
col [15]). This scheme reduces the average failover time be-
cause, unlike the previous two schemes, it avoids the re-
transmission of client requests. The scheme also incurs a
low overhead since we do not need to parse GIOP mes-
sages and keep track of IORs. However, this scheme does
not readily support replicated clients.
5.1. Fault-Injection Strategy
We injected a memory-leak fault by declaring a 32KB
buffer of memory within the Interceptor, and then slowly
exhausting the buffer according to a Weibull probability dis-
tribution (commonly used in software reliability and fault-
prediction). The memory leak at a server replica was acti-
vated when the server received its ﬁrst client request. At ev-
ery subsequent 150ms intervals after the onset of the fault,
we exhausted chunks of memory according to a Weibull
distribution with a scale parameter of 64, and a shape pa-
rameter of 2.0. This setting led to approximately one server
failure for every 250 client invocations. We used this ap-
proach to inject a memory leak as opposed to limiting the
maximum heap size or the resident set size (rss) in Linux
because Linux uses optimistic memory allocation whereby
memory is allocated without ﬁrst checking if the total re-
quired amount of memory is available at the time of the re-
quest. This means that an application might exhaust its heap
size without raising a segmentation violation. The buffer-
based approach provided us with a deterministic fault model
which we could then use to test the effectiveness of our
proactive recovery schemes, in a reproducible manner.
5. Empirical Evaluation
5.2. Results
We ran our initial experiments on ﬁve Emulab [17]
nodes with the following speciﬁcations: 850MHz pro-
cessor, 512MB RAM, and the REDHAT Linux 9 oper-
ating system. For our test application, we used a sim-
ple CORBA client implemented over the TAO ORB (ACE
and TAO version 5.4) that requested the time-of-day at
1ms intervals from one of three warm-passively repli-
cated CORBA servers managed by the MEAD Recovery
Manager. Each experiment covered 10,000 client invoca-
tions. We activated a speciﬁc kind of resource-exhaustion
fault, namely, a memory-leak, when the primary server
replica responded to its ﬁrst client request. The Proac-
tive Fault-Tolerance Manager constantly monitored the
memory usage on the faulty server replica, and trig-
gered proactive recovery when the resource usage reached
a preset threshold, for instance, when 80% of the allo-
cated memory was consumed.
We compared our proactive schemes against two tradi-
tional reactive recovery schemes. In the ﬁrst scheme, the
client waited until it detected a server failure before contact-
ing the CORBA Naming Service for the address of the next
available server replica. In our second scheme, the client
ﬁrst contacted the CORBA Naming Service and obtained
the addresses of the three server replicas, and stored them
in a collocated cache. When the client detected the failure
of a server replica, it moved on to the next entry in the cache,
and only contacted the CORBA Naming Service once it ex-
hausted all of the entries in the cache.
For both the reactive and proactive schemes, we measured
the following parameters (see Table 1):
• Percentage increase in client-server round-trip times
• Percentage of failures exposed to the client application
• Time needed by the client to failover to a new server
over the reactive schemes;
per server-side failure;
replica.
In our proactive schemes, we also measured the effec-
tiveness of failing over clients at different thresholds.
Recovery
Strategy
Reactive Without
Cache
Reactive With
Cache
NEEDS
ADDRESSING
Mode
LOCATION
FORWARD
MEAD Message
Increase Client
in RTT
(%)
baseline
Failures
(%)
100%
Failover Time
(ms)
10.177
change
(%)
baseline
0%
8%
90%
3%
146%
10.461
+2.8%
25%
9.396
-7.7%
0%
0%
8.803
-13.5%
2.661
-73.9%
Table 1. Overhead and fail-over times.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:12 UTC from IEEE Xplore.  Restrictions apply. 
5.2.1. Number of client-side failures: In the reac-
tive scheme that did not cache server references, there
was an exact 1:1 correspondence between the num-
ber of observed failures at
the client and the number
of server-side failures. The client-side failures we ob-
served were purely CORBA COMM FAILURE exceptions
which are raised when a replica fails after the client has suc-
cessfully established a connection with the replica.
The reactive scheme that used cached server references
experienced a higher failure rate. There was a 1:1 corre-
spondence between the number of server-side failures and
the number of COMM FAILURE exceptions observed by
the client. In addition to COMM FAILURE exceptions, the
client also experienced a number of TRANSIENT excep-
tions that occurred when the client accessed a stale replica
reference within the cache. Stale cache references occur
when we refreshed the cache before a faulty replica has
had a chance to restart and register itself with the CORBA
Naming Service, thereby leaving its old invalid reference
in the cache. This problem can be avoided by staggering the
cache-refresh process over time, instead of refreshing all the
cache references in one sweep.
In the NEEDS ADDRESSING MODE scheme, which
is equivalent to a proactive recovery scheme with insuf-
ﬁcient advance warning of the impending failure, we ob-
served eleven client-side failures. These occurred when the
client requested the next available replica from the Spread
group at the point when the previous replica died, but be-
fore the group-membership message indicating the replica’s
crash had been received by all the replicas in the group. At
this point, there is no agreed-upon primary replica to ser-
vice the client request; therefore, the blocking read at the
client timed out and the client catches a COMM FAILURE
exception.
For the proactive schemes in which there was enough
advance warning of the impending failure, i.e., thresh-
olds below 100% (these correspond to the LOCA-
TION FORWARD scheme and the MEAD proactive
fail-over message scheme), the client does not catch any ex-
ceptions at all!
5.2.2. Overhead: We measured the overhead in terms of
the percentage increase in client-server round-trip times
(RTT). We deﬁned round-trip time as the amount of time
that elapsed from the time the client application sent the re-
quest to the time it received a reply from the server. The
overhead in the reactive schemes, which averaged 0.75ms,
served as our baseline reference.
The scheme which used GIOP LOCATION FORWARD
to trigger proactive recovery incurred an
messages