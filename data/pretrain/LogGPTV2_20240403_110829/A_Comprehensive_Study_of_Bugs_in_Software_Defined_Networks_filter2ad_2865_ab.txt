functionality, e.g., state management or packet processing.
Thus controllers come bundled with a plethora of additional
third-party libraries and services (indicated as a yellow box in
Figure 1). SDN controllers are fundamentally event-driven:
the arrows in Figure 1 demonstrate the various sources of
input events that a controller reacts to (conﬁguration, network
events, the kernel through system calls, and application li-
braries through function calls).
Although there are approximately 32 controllers, we focus
our study on three of the four most mature and popular open-
source controllers are: ODL, CORD, ONOS, and FAUCET.
We selected ONOS and CORD over ODL because they are
used by major operators, e.g., Comcast [25], Google [26], etc.,
in a large scale real-world production environments. Moreover,
unlike ONOS or ODL, CORD is specially tailored for emerg-
ing technologies (e.g., 5G-MEC [27], [28]) – thus providing a
different perspective. We selected FAUCET because it is used
at Google [29] and provides a unique perspective from the
other controllers because it has a more compact structure and
is written in Python. Next, we elaborate on the design of each
of the three SDN controller frameworks:
• FAUCET [30] boasts a monolithic and compact code-
base that migrates existing network functionalities like
routing protocols, neighbor discovery, etc., into vendor-
independent data planes. FAUCET manages ﬂow deci-
sions by utilizing multiple Access Control Lists(ACL)
and multi-table processing [31].
• ONOS (Open Network Operating System) [32] builds
on four major goals: modularity, conﬁguration ﬂexibility,
isolation of subsystems, and protocol agnosticism. ONOS
utilizes an intent-based API that captures policy directives
for controlling network function. These intent-based APIs
are realized through a set of state transition machines.
Each subsystem employs a different state machine. This is
distinct from FAUCET’s monolithic but compact design.
• Open CORD (Central Ofﬁce Re-architected as a Data-
center) [33] is a specialized version of ONOS developed
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:53 UTC from IEEE Xplore.  Restrictions apply. 
102
SDN ApplicationControllerLibraryKernelService callsApplication callsConﬁgsH/WH/WH/WOpenFlowmessagesfor Telecom Central Ofﬁce (CO) to replace purpose-
built hardware with cost-effective, agile networks. CORD
is composed of four open-source projects,
including
Openstack, ONOS, Docker, and XOS. CORD provides
a unique subsystem, based on XOS [34], to orchestrate
coordination across these four code-bases.
B. Data Set and Methodology
The controller frameworks maintain a structured bug track-
ing and code management system — ONOS and CORD
use JIRA for bugs and Gerrit for rolling out ﬁxes, whereas
FAUCET uses Github for bug tracking and managing ﬁxes.
While Jira includes tags that allow us to analyze bugs based
on developer-identiﬁed severity levels, for Github, we used a
keyword approach [35] to extract severity levels.
Data. As of April 2020, the FAUCET, ONOS, and CORD
communities have identiﬁed 251, 186, and 358 critical bugs,
respectively, which include both open and close bugs. In
examining the bugs in ONOS and CORD, we found that: (1)
Over time, the number of critical bugs keeps increasing. This
motivates a need for more principled analysis. (2) We observe
that a burst of bugs occurs around release dates. For example,
in the ﬁrst quarter of 2017, we observed a burst in CORD
bugs which coincided with a release [36]. This highlights the
need for longitudinal analysis across different releases.
For our study, we randomly selected 50 closed1 bugs from
each controller for manual analysis. Moreover, we further
veriﬁed the automatic analysis with an extended data set
containing over 500 critical bugs.
C. Bug Autoclassiﬁcation with NLP
To scale and automate classiﬁcation, we re-use an NLP
technique that prior bug studies have used, i.e., Word2Vec [37],
to classify bugs and validate our taxonomy. We summarize the
steps as follows:
• First, we pre-process the bug data to extract features.
There are three classic approaches for keyword extract-
ing including Latent Dirichlet Allocation (LDA) [38],
Hierarchical Dirichlet Process (HDP) [39] and Non-
negative Matrix Factorization (NMF) [40] based on Term
Frequency Inverse Document Frequency (TF-IDF) [41].
We choose the last approach because previous work [42],
[43] has demonstrated its potential to analyze similar data.
• Second, we train a Word2Vec model, which provides a
mechanism for automatically determining similar words.
Given a bug description, these two steps allow us to map
each bug to a numerical vector in a Euclidean space. After
mapping bugs to Euclidean space, we can employ classic Ma-
chine Learning (ML) techniques, e.g., Support Vector Machine
(SVM) or Decision Tree (DT), to automatically classify the
bugs.
149,49,48 from CORD, ONOS, FAUCET — we initially had 50 but
removed open bugs to enable classiﬁcation by ﬁxes.
1) Bug Labeling: We utilize the following dimensions to
classify the bugs: bug type, outcome, ﬁx, and trigger. In
Table I, we summarize these dimensions. These dimensions
align with the recent work to characterize bugs in cloud
systems [18], [19] which provide a similar classiﬁcation as
Orthogonal Defect Classiﬁcation (ODC) [44]. At a high level,
we classify bugs based on determinism to understand their
reproducibility. For the root-cause and ﬁxes, we classify
bugs based on the controller code-base or logic’s impact:
some problems require changes to logic while others do not–
similarly, some bugs are due to existing logic or absence of
any logic (e.g., edge cases). To verify the ﬁxes, we manually
analyzed the source code patches and ﬁxes. For the triggers,
we identify four key events that initiate bugs. These events
align with a canonical SDN controller (Figure 1). For the
symptoms, we focus on the type of failure triggered by the
bug.
Each bug receives at most one tag from each of the
dimensions in Table I.
2) Validation: We validated the automated classiﬁcation
techniques with cross-validation by splitting our data set into
2/3 for training and 1/3 for testing. We explored several classic
ML techniques, including Support Vector Machine (SVM) and
Decision Tree (DT), Principal Component Analysis (PCA),
and AdaBoost. In our experiments, we found that SVM model
with normalization provided the best accuracy for predicting
bug types and symptoms, with accuracies of 96% and 86%,
respectively. Unfortunately, we found it hard to ﬁnd any
algorithm to predict bug ﬁxes accurately, and we believe this
is because bug descriptions generally provide little data about
the ﬁxes.
III. RQ1: BUG TYPE
We begin by classifying bugs according to determinism.
Deterministic bugs are deﬁned as bugs that are clearly re-
producible with a ﬁxed set of input actions, whereas non-
deterministic bugs are inconsistent and cannot be consistently
reproduced by replaying the same set of input events/actions.
The key observation is that all frameworks are dominated by
deterministic bugs: FAUCET (96%), ONOS (94%), and CORD
(94%). One potential reason for this is that many controllers
employ standard state-machine-based techniques [13], [14],
[45], [46], e.g., Paxos [45] or Raft [46], which tackle and
mask most non-deterministic bugs.
Takeaway. Given the dominance of deterministics bugs, we
believe that record-and-replay-based recovery techniques [47]
will have limited applicability on most SDN controllers.
Instead, we recommend failure recovery systems which alter
controller input events [12], [48], environments [49], [50], or
source code [51]–[55].
IV. RQ2: OPERATIONAL IMPACT OF SDN BUGS
In this section, we explore the bugs’ symptoms and char-
acterize them based on the controller’s behavior. The analysis
of symptoms and controller behavior provides us with a ﬁrst
step towards understanding each bug’s operational impact.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:53 UTC from IEEE Xplore.  Restrictions apply. 
103
Classiﬁcation
Bug Type
Root Cause
Symptoms
Fix
Trigger
Categories
Deterministic, Non-deterministic
Controller Logic-bugs: Load, Concurrency, Memory, Missing Logic
Non Controller logic-bugs: Human (misconﬁguration), Ecosystem Interaction (Third-Party, Application Libraries or System Calls)
Performance, Fail-stop, Error Message, Byzantine (Wrong Behavior)
No Logic Changes: Rollback Upgrades, Upgrade Packages
Add New Logic: Add Logic
Change Existing Logic: Add Synchronization, Fix Conﬁguration, Add Compatibility, Workaround [35]
Conﬁguration, External Calls, Network Events (OpenFlow Message), Hardware Reboots
Table I: Bug Taxonomy.
Byzantine Failures (61.33%): A majority of the bugs lead
to the following unexpected behavior: (i) gray failures – a par-
tial outage of the controller (52.17%), where some controller
functionality is working while others are not. For example, in
FAUCET-1623 [56] where the controller continues to manage
ﬂows but is unable to manage broadcast packets because of an
unhandled edge case, a bug in the mirroring interface (shown
in Figure 3). (ii) stalling (20.65%), where the controller tem-
porarily freezes, and (iii) incorrect behavior (27.18%). Unlike
stalling or partial outages, incorrect behavior is difﬁcult to de-
tect and diagnose because they do not generate error messages
or trigger any normal alerts.
Takeaway. These bugs, in general, highlight the need of
formal network veriﬁcation; however, early works on veri-
ﬁcation [57]–[59] focus on the datapath or provide limited
validation of runtime behavior. Our analysis indicates a need
for more runtime veriﬁcation of controller behavior.
Fail-stop (20%): Bugs that cause fail-stop failures or con-
troller crashes are the most dire bugs as they directly impact
the network’s availability and lead to production downtime. In
Figure 2, we analyze the root cause of these bugs. In FAUCET,
these bugs are caused by human mistakes or ecosystem inter-
actions. This implies that crashes are due to the edge cases re-
lated to certain external scenarios. In contrast with FAUCET, in
ONOS and CORD, a majority of the bugs are due to incorrect
controller-logic, e.g., load, memory, and missing code logic.
For example, a misconﬁguration led to a null pointer exception
in CORD’s host and multicast handlers (CORD-2470 [60]),
Figure 2: Distribution of Root Causes of the (a) Fail-Stop (ﬁrst three
bars) and (ii) Performance Bugs Across Controllers (last three bars).
Figure 3: Patch for FAUCET- 1623 [56], where interface mirroring
didn’t mirror output broadcast packets which was ﬁxed by adding a
case for mirrored ports.
which crashed the CORD controller. Despite CORD being
based on ONOS, we observe a key difference between ONOS
and CORD: in general, CORD has signiﬁcantly more bugs due
to “missing code logic,” demonstrating a level of immaturity
in the codebase.
Takeaway: Fail-stop bugs are the easiest
to detect but
have disasterous consequences. Our initial analysis shows that
exploring designs to improve memory safety (e.g., memory
safe languages like RUST [61] and programming styles [62])
will signiﬁcantly improve availability.
Error Message (14.7%): In general, we ignore these bugs
because they result in warnings that have no direct operational
impact. The main observation is that CORD has the best
exception handling, which leads to fewer error messages.
Performance (4%): From Figure 2, we observe that most
of the bugs that result in slow controller performance can be
triaged to one or two root causes. From the Figure, we also
observe that different controllers have different root causes. A
key surprise is that increased system load is not the main cause
of slow performance. Instead, increased-system load leads to
other failures, i.e., fail-stop and byzantine failures. We observe
that poor performance is due to FAUCET’s interactions with
the ecosystem, concurrency bugs in ONOS, and memory errors
in CORD. Thus broadly speaking, these bugs in FAUCET are
due to factors generally beyond the developer’s interactions,
whereas in ONOS and CORD, they are due to poor program-
ming logic.
Takeaway. Performance bugs [63] can cascade into a
that can
variety of dire bugs, e.g., byzantine, crash, etc.,
introduce SDN control plane instability. These bugs require
active monitoring and health check system; however, such
systems introduce signiﬁcant overheads. For some of these
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:53 UTC from IEEE Xplore.  Restrictions apply. 
104
ONOS Version
ONOS-1.12.0
ONOS-1.13.0
ONOS-1.14.0
ONOS-2.0.0
ONOS-2.1.0
ONOS-2.2.0
ONOS-2.3.0 [72]
# VD High
3
28
35
50
59
62
41
1
17
33
24
32
33
24
Table II: Dependency Analysis of ONOS versions. VD: vulnerable
dependencies, High: Dependencies with high severity level CVE.
bugs, e.g., Concurrency bugs, we can explore alternative and
potentially lighter-weight
techniques, e.g., semantic explo-
ration techniques [64]. For example, a CORD concurrency bug
(CORD-1734 [65]) where multiple interleaved threads caused
performance degradation.
Figure 4: Patch for CORD-1734 [65], where multiple threads were
negatively impacting the performance of all API calls. This was
attributed to reliance of python on global locks, so as a ﬁx the
maximum number of workers were reduced to 1.
New Research Directions: We summarize new research
areas based on our observations:
• There are still gaps between the industrial demands and
the modern invariant checkers as illustrated with FAUCET-
1623 [56] (discussed above). To tackle such bugs with more
complex behavior, we need more complex invariant checkers
because most existing checkers focus on reachability-based
and QoS-based invariants [57], [59].
• We identiﬁed a need for more ﬁne-grained failure-indicators
and failure-detectors that detect component level availability
and correctness. These techniques need to be more expres-
sive than simple heart-beats; they should verify subcompo-
nent correctness. Speciﬁcally, for the failures that are due
to load and ecosystem interactions, we may predict these
crashes by analyzing metrics or existing syslogs. Given this,
it would be interesting to evaluate the potential of extending
existing log-based failure prediction systems [66]–[68] or
metrics-based systems [69], [70] to SDNs.
• We highlighted a need for research into extending fault
prediction based on system load to the SDN-domain to
address issues with load and cascading errors, e.g., ONOS-
4859 [71] that suffers from ineffective use of memory.
V. RQ3: BUG TRIGGERS AND CODE FIXES
This section analyzes the events that trigger a bug, the code
ﬁxes applied to ﬁx the bug(§ V-A), and the time to ﬁx them
(§ V-B).
A. Analysis of Bug Triggers
Recall, in Section II-A, we showed that SDN controllers are
event-driven and, in general, these controllers only react to the
events listed in § II-A. Below we analyze each of these events
and discuss the implications for our study.
Conﬁguration (38.8%): We observed that many bugs are
triggered when the controller attempts to process system
conﬁgurations. This fact is astounding because a critical moti-
vation for SDN is to move towards automation and eliminate
conﬁguration-based errors [73]–[76].
In Table III, we analyze the type of conﬁgurations. We
observe that for ONOS and CORD, most of the conﬁguration
bugs are due to the conﬁguration of the controller and third-
party services.
Interestingly, we observe that only 25% of the conﬁguration-
related bugs can be ﬁxed by changing the controller conﬁg-
uration. This implies that research on misconﬁguration [77]–
[79] It focuses on detecting the impact of an application’s
conﬁguration on the system and will have limited applicability
because third-party code bases’ conﬁguration impacts the
system.
Takeaway. These observations highlight a need for more re-
search on techniques for diagnosing and debugging the cross-
layer impact of conﬁgurations. These cross-layer approaches
should be coupled with preventive systems such as [80] which
detect latent conﬁguration bugs by employing fuzzy-testing.
External Calls (33%): For the external calls, we observed
that 41.4% of the code ﬁxes attempt to make the controller
more compatible with external libraries by changing function
calls or arguments to match the external API or by upgrading
the external packages. The use of code patches to ﬁx this
interdependence highlights the highly dynamic open-source
ecosystem. Interestingly, we also observe that the miscon-
ﬁguration of the communication between multiple modules