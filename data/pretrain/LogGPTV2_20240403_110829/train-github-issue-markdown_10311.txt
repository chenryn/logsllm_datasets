 **Nicolas FABRE** opened **SPR-6334** and commented
Using org.springframework.jdbc.core.JdbcTemplate.batchUpdate(String sql,
BatchPreparedStatementSetter pss), if the number of rows I want to insert
(update or delete) is greater than the batch size I define in the
BatchPreparedStatementSetter, the rows after the batch size limit are not
inserted. However if I have a huge amount of rows to insert (for example
100000), I think it is not good to set the batch size to 100000. The database
could be oppressed if it receives 100000 rows to process on a single call. Am
I wrong ?
So would it be possible to use batchUpdate with a number of updates to be
performed not equals to the batch size ? May I request the creation of the
following method to manage this problem ?
            // the batch size is set in the BatchPreparedStatementSetter, the number of rows we want to process is equal to the nbUpdates parameter
    	public int[] batchUpdate(String sql, final long nbUpdates, final BatchPreparedStatementSetter pss) throws DataAccessException {
    		if (logger.isDebugEnabled()) {
    			logger.debug("Executing SQL batch update [" + sql + "]");
    		}
    		return (int[]) execute(sql, new PreparedStatementCallback() {
    			public Object doInPreparedStatement(PreparedStatement ps) throws SQLException {
    				try {
    					int batchSize = pss.getBatchSize();
    					InterruptibleBatchPreparedStatementSetter ipss = (pss instanceof InterruptibleBatchPreparedStatementSetter ? (InterruptibleBatchPreparedStatementSetter) pss
    							: null);
    					if (JdbcUtils.supportsBatchUpdates(ps.getConnection())) {
    						List rowsAffected = new ArrayList();
    						for (int i = 1; i  rowsAffected = new ArrayList();
    						for (int i = 0; i < nbUpdates; i++) {
    							pss.setValues(ps, i);
    							if (ipss != null && ipss.isBatchExhausted(i)) {
    								break;
    							}
    							rowsAffected.add(ps.executeUpdate());
    						}
    						int[] rowsAffectedArray = new int[rowsAffected.size()];
    						for (int i = 0; i < rowsAffectedArray.length; i++) {
    							rowsAffectedArray[i] = rowsAffected.get(i);
    						}
    						return rowsAffectedArray;
    					}
    				} finally {
    					if (pss instanceof ParameterDisposer) {
    						((ParameterDisposer) pss).cleanupParameters();
    					}
    				}
    			}
    		});
    	}
Thanks to this method I can process my 100000 rows with a batchSize = 5000 for
example. It avoid to manage sub-collections in the DAO method to invoke the
batchUpdate method with just 5000 rows.
Thanks in advance for the attention which will be given to this issue.
Nicolas
* * *
**Affects:** 3.0 RC1
**Attachments:**
  * spring-contribution-JIRA-SPR-6334.zip ( _13.07 kB_ )
**Issue Links:**
  * #11949 Extend JdbcTemplate to allow larger batch updates ( _ **"is duplicated by"**_ )
**Referenced from:** commits `0adcb2a`
2 votes, 3 watchers