User: My python process run on different machines. It consists of a manager and many workers. The worker in each machine are multi threaded and needs to update some data such as its status  to the manager residing on another machine. I didn't want to use mysql because many other processes are already executing many queries on it and it will reach its max_connection
I have two methods in mind:

For each worker thread, write the data to a local text file. A separate bash script will run a while-loop, check for file change and scp this file to all other machines.
For each worker thread, write the data to share memory and have it replicated to all other machines. I am not sure how to do this. In python how can i write to shared memory? How can i replicated shared memories?
Assistant: Okay, first point is that you can't share memory among machines unless you're on a very specialized architecture. (Massively parallel machines, Beowulf clusters, and so on.)
If you mean to share code, then package your code into a real Python package and distribute it with a tool like Chef, Puppet or Docker.
If you mean to share data, use a database of some sort that all your workers can access. I'm fond of MongoDB because it's easy to match to an application, but there are a million others databases.  A lot of people use mysql or postgresql.