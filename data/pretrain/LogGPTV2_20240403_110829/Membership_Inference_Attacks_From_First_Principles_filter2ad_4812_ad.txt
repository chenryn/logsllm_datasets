### Method Comparison for Membership Inference Attacks

The following table compares various membership inference attacks under the same settings for well-generalizing models on CIFAR-10, CIFAR-100, and WikiText-103 using 256 shadow models. Accuracy is presented for completeness, but we do not consider it a meaningful metric for evaluating these attacks. Full ROC curves are provided in Appendix A.

| Method                        | Balanced Accuracy | C-10 TPR @ 0.001% FPR | C-10 TPR @ 0.1% FPR | C-100 WT103 TPR @ 0.001% FPR | C-100 WT103 TPR @ 0.1% FPR |
|-------------------------------|-------------------|------------------------|----------------------|--------------------------------|------------------------------|
| Yeom et al. [70]              | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |
| Shokri et al. [60]            | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |
| Jayaraman et al. [25]         | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |
| Song and Mittal [61]          | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |
| Sablayrolles et al. [56]      | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |
| Long et al. [37]              | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |
| Watson et al. [68]            | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |
| Ye et al. [69]                | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |
| Ours                          | 0.0%              | 0.0%                   | 0.0%                 | 0.0%                           | 0.0%                         |

### Implementation Details

For a target model \( f \) and a point \( (x, y) \), the attack outputs \( g(f(x), y) \) as a membership confidence score. We implement this by training shadow models that randomly subsample half of the total dataset. The training set of the shadow models thus partially overlaps with the training set of the target model \( f \). This is a stronger assumption than that made by Shokri et al. [60], leading to a slightly stronger attack. Despite being significantly more expensive due to the overhead of training many shadow models and then training a membership inference predictor on their outputs, this attack does not perform significantly better at low false-positive rates.

### Multiple Queries

It is possible to improve attacks by making multiple queries to the model. For example, Jayaraman et al. [25] use their MERLIN attack, which queries the target model \( f \) multiple times on a sample \( x \) perturbed with fresh Gaussian noise, and measures how the model’s loss varies in the neighborhood of \( x \). However, even when querying the target model 100 times and carefully choosing the noise magnitude, this attack does not improve the adversary’s success at low false-positive rates.

Choquette-Choo et al. [6] suggest an alternate technique to increase attack accuracy when models are trained with data augmentations. In addition to querying the model on \( f(x) \), this attack also queries on augmentations of \( x \) that the model might have seen during training. This approach, as we will show in Section VI-C, improves our attack success rate considerably.

### Per-Class Hardness

Instead of using per-example hardness scores, a potentially simpler method would be to design one scoring function \( A'(y) \) per class \( y \), by scaling the model’s loss by a class-dependent value: \( A'(x, y) = A(x, y) - \tau_y \). For example, in the ImageNet dataset [9], correctly classifying individual dog breeds tends to be harder than other broader classes. Despite this intuition, using per-class thresholds neither helps improve balanced attack accuracy nor attack success rates at low false-positive rates, although it does improve the AUC of attacks on CIFAR-10 and CIFAR-100 by 2%.

The attack by Song and Mittal [61] combines per-class scores with additional techniques. Instead of working with the standard cross-entropy loss, this attack uses a modified entropy measure and trains shadow models to approximate the distributions of entropy values for members and non-members of each class. Given a model \( f \) and target sample \( (x, y) \), the attack computes a hypothesis test between the (per-class) member and non-member distributions. Despite these additional techniques, this attack does not improve upon the baseline attack [60] at low FPRs.

### Per-Example Hardness

As we do in our work, a final direction considers per-example hardness. Sablayrolles et al. [56] is the most direct influence for LiRA. Their attack, \( A'(x, y) = \ell(f(x), y) - \tau_{x,y} \), scales the loss by a per-example hardness threshold \( \tau_{x,y} \) estimated by training shadow models. Instead of fitting Gaussians to the shadow models’ outputs, this paper takes a simpler non-parametric approach and sets the threshold near the midpoint \( \tau_{x,y} = (\mu_{\text{in}}(x, y) + \mu_{\text{out}}(x, y)) / 2 \) to maximize the attack accuracy; here \( \mu_{\text{in}}, \mu_{\text{out}} \) are the means computed as we do.

The recent work of Watson et al. [68] considers an offline variant of Sablayrolles et al. [56], setting \( \tau_{x,y} = \mu_{\text{out}}(x, y) \) (i.e., each example’s loss is calibrated by the average loss of shadow models not trained on this example).

Both Sablayrolles et al. and Watson et al. evaluate their attacks using average case metrics (balanced accuracy and AUC) and find that using per-example hardness thresholds can moderately improve past attacks. In our evaluation (Table I), we find that the balanced accuracy and AUC of their approaches are slightly lower than those of other simpler attacks. However, we find that per-example hardness-calibrated attacks reach a significantly better true-positive rate at low false-positive rates, making them much better attacks according to our suggested evaluation methodology.

### Attack Approach

| Attack Approach                                                                 | TPR @ 0.1% FPR |
|---------------------------------------------------------------------------------|-----------------|
| LOSS attack [70]                                                                | 0.0%            |
| LOSS attack [70] + Logit scaling                                                | 0.1%            |
| LOSS attack [70] + Multiple queries                                             | 0.1%            |
| LOSS attack [70] + Per-example thresholds ( ˜Qout only) [68] + Logit scaling    | 0.0%            |
| LOSS attack [70] + Per-example thresholds ( ˜Qin & ˜Qout) [56] + Logit scaling   | 1.3%            |
| Our full offline attack                                                         | 4.7%            |
| Our full online attack                                                          | 7.1%            |

### Conclusion

The discrepancy between the balanced accuracy and our recommended low false-positive metric is even more stark for the attack of Long et al. [37]. This attack also trains shadow models to estimate per-example hardness, but additionally filters out a fraction of outliers to which the attack should be applied, and makes no confident guesses for non-outliers. This attack cannot achieve a high average accuracy, yet outperforms most prior attacks at low false-positive rates.

To expand, this attack [37] builds a graph of all examples \( x \), where an edge between \( x \) and \( x' \) is weighted by the cosine similarity between the features \( z(x) \) and \( z(x') \). Our implementation selects the 10% of outliers with the largest distance to their nearest neighbor in this graph. For each such outlier \( (x, y) \), the attack trains shadow models to numerically estimate the probability of observing a loss as high as \( \ell(f(x), y) \) when \( (x, y) \) is not a member.

The attack in the concurrent work of Ye et al. [69] is close in spirit to ours. They follow the same approach as our offline attack.