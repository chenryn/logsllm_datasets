p
i
t
l
u
m
s
e
i
r
e
u
q
s
s
e
n
d
r
a
h
s
s
a
l
c
e
l
p
m
a
x
e
s
s
e
n
d
r
a
h
Method
Yeom et al. [70]
Shokri et al. [60]
Jayaraman et al. [25]
Song and Mittal [61]
(cid:35)
(cid:32)
(cid:35)
(cid:32)
Sablayrolles et al. [56] (cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
Long et al. [37]
Watson et al. [68]
Ye et al. [69]
Ours
(cid:35)
(cid:35)
(cid:32)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:32)
(cid:35)
(cid:32)
(cid:35)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
Balanced Accuracy
C-10
TPR @ 0.001% FPR
TPR @ 0.1% FPR
C-10
0.0%
0.0%
0.0%
0.0%
0.1%
0.0%
0.1%
-
C-100 WT103
0.0%
0.0%
0.0%
0.0%
0.8%
0.0%
0.9%
-
C-100 WT103
0.0%
1.6%
0.0%
1.4%
7.4%
4.7%
5.4%
-
2.2% 11.2% 0.09% 8.4% 27.6%
C-10
0.00% 0.0%
0.3%
0.0%
0.1%
0.01% 1.7%
2.2%
0.02% 1.3%
-
0.1% 59.4% 78.0%
59.6% 74.5%
59.4% 76.9%
59.5% 77.3%
C-100 WT103
50.0%
–
–
–
1.0% 56.3% 69.1% 65.7%
–
65.4%
65.5%
1.4% 63.8% 82.6% 65.6%
53.5% 54.5%
1.1% 59.1% 70.1%
60.3% 76.9%
–
–
–
–
–
–
–
–
-
-
TABLE I: Comparison of prior membership inference attacks under the same settings for well-generalizing models on
CIFAR-10, CIFAR-100, and WikiText-103 using 256 shadow models. Accuracy is only presented for completeness; we do not
believe this is a meaningful metric for evaluating membership inference attacks. Full ROC curves are presented in Appendix A.
point (x, y) was a member of the shadow training set Di.
For a target model f and point (x, y), the attack then outputs
g(f (x), y) as a membership conﬁdence score.
We implement this by training shadow models that ran-
domly subsample half of the total dataset. The training set of
the shadow models thus partially overlaps with the training set
of the target model f. This is a stronger assumption than that
made by Shokri et al. [60] and thus yields a slightly stronger
attack. Despite being signiﬁcantly more expensive than the
LOSS attack due to the overhead of training many shadow
models and then training a membership inference predictor
on the output of the models, this attack does not perform
signiﬁcantly better at low false-positive rates.
Multiple queries. It is possible to improve attacks by making
multiple queries to the model. Jayaraman et al. [25] do this
with their MERLIN attack, that queries the target model f
multiple times on a sample x perturbed with fresh Gaussian
noise, and measures how the model’s loss varies in the
neighborhood of x. However, even when querying the target
model 100 times and carefully choosing the noise magnitude,
we ﬁnd that this attack does not improve the adversary’s
success at low false-positive rates.
Choquette-Choo et al. [6] suggest an alternate technique to
increase attack accuracy when models are trained with data
augmentations. In addition to querying the model on f (x),
this attack also queries on augmentations of x that the model
might have seen during training. This is the direct motivation
for us making these additional queries, which as we will show
in Section VI-C improves our attack success rate considerably.
Per-class hardness. Instead of using per-example hardness
scores as we have done, a potentially simpler method would be
to design just one scoring function A(cid:48)y per class y, by scaling
the model’s loss by a class-dependent value: A(cid:48)y(x, y) =
A(cid:48)(x, y) − τy. For example, in the ImageNet dataset [9] there
are several hundred classes for various breeds of dogs, and so
correctly classifying individual dog breeds tends to be harder
than other broader classes. Interestingly, despite this intuition,
in practice using per-class thresholds neither helps improve
balanced attack accuracy nor attack success rates at low false-
positive rates, although it does improve the AUC of attacks
on CIFAR-10 and CIFAR-100 by 2%.
The attack of Song and Mittal [61] reported in Figure 1 and
Table I combines per-class scores with additional techniques.
Instead of working with the standard cross-entropy loss, this
attack uses a modiﬁed entropy measure and trains shadow
models to approximate the distributions of entropy values for
members and non-members of each class. Given a model f
and target sample (x, y), the attack computes a hypothesis test
between the (per-class) member and non-member distributions
(see [61]). Despite these additional techniques, this attack does
not improve upon the baseline attack [60] at low FPRs.
Per-example hardness. As we do in our work, a ﬁnal
direction considers per-example hardness. Sablayrolles et al.
[56] is the most direct
inﬂuence for LiRA. Their attack,
A(cid:48)(x, y) = (cid:96)(f (x), y) − τx,y, scales the loss by a per-
example hardness threshold τx,y that is estimated by training
shadow models. Instead of ﬁtting Gaussians to the shadow
models’ outputs as we do, this paper takes a simpler non-
parametric approach and sets the threshold near the midpoint
τx,y = (µin(x, y) + µout(x, y))/2 so as to maximize the attack
accuracy; here µin, µout are the means computed as we do.
The recent work of Watson et al. [68] considers an ofﬂine
variant of Sablayrolles et al. [56], that sets τx,y = µout(x, y)
(i.e., each example’s loss is calibrated by the average loss of
shadow models not trained on this example).
Both Sablayrolles et al. and Watson et al. evaluate their
attacks using average case metrics (balanced accuracy and
AUC), and ﬁnd that using per-example hardness thresholds
can moderately improve upon past attacks. In our evaluation
(Table I), we ﬁnd that the balanced accuracy and AUC of
their approaches are actually slightly lower than those of
other simpler attacks. Yet, we ﬁnd that per-example hardness-
calibrated attacks reach a signiﬁcantly better true-positive rate
at low false-positive rates—and are thus much better attacks
according to our suggested evaluation methodology.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
81904
Attack Approach
LOSS attack [70]
+ Logit scaling
+ Multiple queries
LOSS attack [70]
+ Per-example thresholds ( ˜Qout only) [68]
+ Logit scaling
+ Gaussian Likelihood
+ Multiple queries (our ofﬂine attack)
LOSS attack [70]
+ Per-example thresholds ( ˜Qin & ˜Qout) [56]
+ Logit scaling
+ Gaussian Likelihood
+ Multiple queries (our online attack)
TPR @ 0.1% FPR
0.0%
0.1%
0.1%
0.0%
1.3%
4.7%
4.7%
7.1%
0.0%
1.7%
1.9%
5.6%
8.4%
Fig. 7: Attack true-positive rate versus model train-test gap for
a variety of CIFAR-10 models.
TABLE II: By iteratively adding the main components of our
attack we can interpolate between the simple LOSS threshold
attack [70] and our full ofﬂine and online attacks.
The discrepancy between the balanced accuracy and our
recommended low false-positive metric is even more stark for
the attack of Long et al. [37]. This attack also trains shadow
models to estimate per-example hardness, but additionally
ﬁlters out a fraction of outliers to which the attack should be
applied, and then makes no conﬁdent guesses for non-outliers.
This attack thus cannot achieve a high average accuracy, yet
outperforms most prior attacks at low false-positive rates.
To expand, this attack [37] builds a graph of all examples
x, where an edge between x and x(cid:48)
is weighted by the
cosine similarity between the features z(x) and z(x(cid:48)). Our
implementation of this attack selects the 10% of outliers with
the largest distance to their nearest neighbor in this graph.
For each such outlier (x, y), the attack trains shadow models
to numerically estimate the probability of observing a loss as
high as (cid:96)(f (x), y) when (x, y) is not a member.
The attack in the concurrent work of Ye et al. [69] is close
in spirit to ours. They follow the same approach as our ofﬂine