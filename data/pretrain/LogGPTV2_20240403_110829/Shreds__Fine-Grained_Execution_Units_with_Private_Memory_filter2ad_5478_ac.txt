time. Unlike general-purpose program analysis, S-compiler’s
analysis is mostly scoped within the code involved in shred
executions, and therefore, can afford to favor accuracy over
scalability. Prior to the analysis and transformation, S-compiler
translates an input program into an intermediate representation
(IR) in the single static assignment (SSA) form.
Checking shred usage: To verify that all shreds in the
program are properly closed, S-compiler ﬁrst identiﬁes all the
shred creations sites(i.e., calls to shred enter), uses them as
analysis entry points, and constructs a context-sensitive control
ﬂow graph for each shred. S-compiler then performs a code
path exploration on each graph in search for any unclosed
shred (or unpaired use of shred enter and shred exit),
which developers are asked to ﬁx. This check is sound because
it
inter-procedural (i.e., a pair of shred enter and
exit APIs must be called inside a same function) and it
conservatively models indirect jumps.
is not
To prevent potential secret leaks, S-compiler performs an
inter-procedural data-ﬂow analysis in each shred. Potential
leaks happen when secrets are either loaded from, or stored to,
unprotected memory. The data-ﬂow analysis checks for both
cases. First, it ensures that data stored in s-pools do not pre-
exist in regular memory (i.e., such data must be directly loaded
into s-pools from input channels, such as stdin or ﬁle system.
Second, the analysis checks for any unsanitized data propaga-
tion from an s-pool object to a regular heap destination. Thanks
to the explicit memory allocations and aliasing in s-pool, the
data-ﬂow analysis needs neither manually deﬁned sources or
sinks nor heuristic point-to analysis. In addition, this analysis
strikes a balance between security and usability: it captures
the common forms of secret leaks (e.g., those resulted from
bugs) while permitting intentional data exports (e.g., saving
encrypted secrets).
Buffered I/O, when used for loading or storing s-pool data,
may implicitly leak the data to pre-allocated buffers outside of
s-pools, which data-ﬂow analysis can hardly detect. Therefore,
S-compiler replaces any buffered I/O (e.g.,fopen) with direct
I/O (e.g.,open) in shreds.
Hardening in-shred control ﬂows: We adopt a customized
form control-ﬂow integrity (CFI) to ensure that in-process
malicious code cannot hijack any shred execution. To that
end, S-compiler hardens in-shred code during compilation.
Based on the control ﬂow graphs constructed in the previous
step, S-compiler identiﬁes all dynamic control ﬂow transfers,
including indirect jumps and calls as well as returns, inside
each shred. It then instruments these control ﬂow transfers so
that they only target basic block entrances within containing
shreds. This slightly coarse-grained CFI does not incur high
overhead as the ﬁne-grained CFI and at the same time is
sufﬁciently secure for our use. It prevents shred execution from
being diverted to out-shred code. Furthermore, since shreds are
usually small in code size (i.e., few ROP gadgets) and our CFI
only allows basic block-aligned control transfers, the chance
of in-shred ROP is practically negligible.
The control ﬂow hardening only applies to in-shred code.
If a function is called both inside and outside of a shred, S-
compiler duplicates the function and instruments the duplicate
for in-shred use while keeping the original function unchanged
for out-shred use. S-compiler creates new symbols for such
duplicates and replaces the in-shred call targets with the new
symbols. As a result, a function can be used inside shreds and
instrumented without affecting out-shred invocations. Using
function duplicates also allows S-compiler to arrange the
code reachable in a shred in adjacent memory pages, which
facilitates the enforcement of control ﬂow instrumentations
and improves code cache locality.
Binding shreds and s-pools: Developers deﬁne a constant
integer as the pool descriptor for each s-pool they need. To
associate an s-pool with a shred, they use the constant descrip-
tor as the pool desc parameter when calling shred enter.
This simple way of creating the association is intuitive and
allows explicit sharing of an s-pool among multiple shreds.
However, if not protected, it may be abused by in-process
malicious code (e.g., creating a shred with an association
to an arbitrary s-pool). S-compiler prevents such abuse by
statically binding shreds and their s-pools. It ﬁrst infers the
pool-shred association by performing a constant folding on
the pool desc used in each shred enter invocation. It
then records the associations in a special section (.shred)
in the resulting executable, to which S-driver will refer during
runtime when deciding if a shred (identiﬁed by its relative
offset in memory) indeed has access to a requested s-pool.
Thanks to the static binding, dynamically forged pool-shred
association is prevented, so is s-pool sharing across different
compilation units.
Similar to previous works employing code instrumentation
and inline reference monitoring, we assume that attackers
cannot rewrite executables produced by S-compiler. Further,
S-driver write-protects the instrumented code and their critical
6161
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:20 UTC from IEEE Xplore.  Restrictions apply. 
runtime data structures in memory. More details about the
security and robustness of system are discussed in § V-A.
D. S-driver: OS-level manager for shreds and s-pools
S-driver is a dynamically loadable kernel extension. It can
be easily installed on a system as a regular driver. S-driver
provides the OS-level support and protection for shreds and
s-pools.
ARM memory domains: S-driver leverages a widely avail-
able yet rarely used ARM CPU feature, namely the the
memory domain mechanism,
to realize s-pools or create
specially protected memory regions inside a single virtual
memory space. At the same time, our design is not speciﬁc
to ARM and can realize s-pools using a mechanism similar
to memory domains in future Intel CPUs [5], [6]. On ARM
platforms, domains are a primary yet lesser known memory
access control mechanism, independent of the widely used
paging-based access control. A memory domain represents a
collection of virtual memory regions. By setting a 4-bit ﬂag
in a Page Directory Entry (PDE), OS assigns the memory
region described by the PDE to one of the 16 (24) domains
supported by the CPU. Since each PDE has its own domain
ﬂag, the regions constituting a domain do not have to be
adjacent. Upon each memory access, the hardware Memory
Management Unit (MMU) determines the domain to which
the requested memory address belongs and then decides if the
access should be allowed based on the current access level for
that domain. The access level for each domain is recorded in
the per-core Domain Access Control Registers (DACR) [14],
and therefore, can be individually conﬁgured for each CPU
core.
Creation and management of s-pools: Although memory
domains are ideal building blocks for s-pools thanks to their
efﬁcient hardware-enforced access control, memory domains
are not originally designed for this purpose and cannot directly
enable s-pools due to two limitations. First, only a total of 16
memory domains are available. If intuitively using one domain
for creating one s-pool, the limited domains will soon run out
as the number of s-pools used in a program increases. Second,
the access control on memory domains is very basic and does
not concern the subject of an access (i.e., who initiates the
access). However, access control for s-pools must recognize
subjects at the granularity of shreds. S-driver overcomes both
limitations of memory domains by multiplexing the limited
domains and introducing shred identities into the access con-
trol logic.
S-driver uses the limited domains to support as many s-pools
as an application may need. Rather than permanently assigning
an s-pool to a domain, S-driver uses domains as temporary
and rotating security identities for s-pools in an on-demand
fashion. Speciﬁcally, it uses a total of k = M in(Ndom −
1, Ncpu) domains, where Ndom is the number of available
domains and Ncpu is the number of CPU (or cores) on a
system. The ﬁrst k domains are reserved for the ﬁrst k CPUs.
15
14
13 12 11 10
9
8
7
6
5
4
3
2
1
0
DACR1
DACR2
DACR3
DACR4
:No-op(depends on page permissions)
   :No Access
 :Unused
:Reserved
Fig. 3: The DACR setup for a quad-core system, where k = 4.
The ﬁrst 3 domains (Dom0 − Dom2) are reserved by Linux.
Each core has a designated domain (Dom3 − Dom6) that
it may access when executing a shred. No CPU can access
Dom7.
S-driver sets the per-CPU DACR in a way such that, Domi
is only accessible to shreds running on CP Ui, for the ﬁrst
k CPUs; Domk+1 is inaccessible to any CPU in user mode.
Figure 3 shows an example DACR setup.
S-driver uses the k CPUs and the k + 1 domains for
executing shreds and protecting s-pools. When a shred starts or
resumes its execution on CP Ui, S-driver assigns its associated
s-pool to Domi, and therefore, the shred can freely access its
s-pool while other concurrent threads, if any, cannot. When
the shred terminates or is preempted, S-driver assigns its s-
pool to Domk+1, which prevents any access to the pool from
that moment on. As a result, S-driver allows or denies access
to s-pools on a per-CPU basis, depending on if an associated
shred occupies the CPU. Even if any malicious code manages
to run concurrently alongside the shred inside the same process
on another CPU, it cannot access the shred’s s-pool without
triggering domain faults. Thus, P 1 is achieved.
It
is reasonably efﬁcient
to switch s-pools to different
domains upon shred entries and exits are. These operations
do not involve heavy page table switches as process- or VM-
based solutions do. They only require a shallow walk through
of the ﬁrst level page table and updates to the PDEs pointing
to the s-pools in question. Besides, they do not trigger full
TLB ﬂushes as our design uses the per-address TLB eviction
interface (flush tlb page) and only invalidates the TLB
entries related to the updated PDEs. To further reduce the over-
head, we invent a technique called lazy domain adjustment:
when a shred is leaving CP Ui, without adjusting any domain
assignment, S-driver quickly changes the DACR to revoke the
CPU’s access to Domi and lets the CPU’s execution continue.
It does not assign the s-pool used by the previous shred to
Domk+1 until a domain fault happens (i.e., another shred
coming to the CPU and accessing its s-pool). The lazy domain
adjustment avoids unnecessary domain changes and halves the
already small overhead in some test cases (see § V).
6262
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:20 UTC from IEEE Xplore.  Restrictions apply. 
Out-shred
DACR: Deny
SPOOL: Lock
Off-CPU
shred enter
shred exit
In-shred
DACR: Deny
SPOOL: Lock
On-CPU
shred exit
shred alloc/Domain fault 
context switch
shred exit
In-shred
DACR: Allow
SPOOL: Unlock
On-CPU
In-shred
DACR: 'HQ\
SPOOL: Lock
Off-CPU
domain fault
(modiﬁed DACR)
context switch
domain fault*
In-shred
DACR: Deny
SPOOL: Unlock
On-CPU
context switch
In-shred
DACR: Deny
SPOOL: Unlock
Off-CPU
Fig. 4: A shred’s transition of states
Figure 4 shows how S-driver orchestrates the transitions of
a shred’s states in response to the API calls, context switches,
and domain faults. Each state is deﬁned by a combination of
four properties:
• Shred = {In-shred | Out-shred}: if the shred has started
or exited.
• DACR = {Allow | Deny}: if the DACR allows or denies
the current CPU to access its domain.
• SP OOL = {Lock | Unlock}: if the associated s-pool is
locked or not.
• CP U = {On-CPU | Off-CPU}: if the shred is running
on a CPU or not.
The transition starts from the top, left circle, when the shred
has not started and its s-pool is locked. After shred enter is
called, S-driver starts the shred, but it will not adjust the DACR
or the s-pool access till a domain fault or a spool alloc
call due to the lazy domain adjustment in effect. When a
context switch happens in the middle of the shred execution
with unlocked DACR and s-pool, S-driver instantly sets the
DACR to Deny but (safely) leaves the s-pool open. Later on,
if a domain fault occurs, S-driver locks the previous s-pool
because the fault means that the current code running on the
CPU is in-shred and is trying to access its s-pool. If a domain
fault never occurs till the shred regains the CPU, S-driver does
not need to change any domain or s-pool settings, in which
case the lazy domain adjustment saves two relatively heavy
s-pool locking and unlocking operations.
potentially accessible to in-process malicious code. To prevent
secret leaks via stacks, S-driver creates a secure stack for
each shred, allocated from its associated s-pool. When code
execution enters a shred, S-driver transparently switches the
stack without the application’s knowledge: it copies the current
stack frame to the secure stack and then overwrites the stack
pointer. When the shred exits or encounters a signal to be
handled outside of the shred, S-driver restores the regular
stack. As a result, local variables used by shreds never exist
in regular stacks, and therefore cannot leak secrets.
Runtime protection of shreds: In addition to enabling and
securing shreds and s-pools, S-driver also protects the inline
reference monitor (IRM) that S-compiler plants in shred
code. S-driver write-protects the memory pages containing
the instrumented code and the associated data in memory. It
also pins the pages in s-pools in memory to prevent leaks
via memory swap. Given that our threat model assumes the
existence of in-process adversaries, S-driver also mediates the
system calls that malicious code in user space may use to
overwrite the page protection, dump physical memory via
/dev/*mem, disturb shreds via ptrace, or load untrusted
kernel modules. For each program using shreds, S-driver starts
this mediation before loading the program code, avoiding pre-
existing malicious code.
S-driver’s system call mediation also mitigates the attacks
that steal secret data, not directly from s-pools, but from the
I/O media where secret data are loaded or stored. For instance,
instead of targeting the private key loaded in an s-pool, an
in-process attacker may read the key ﬁle on disk. S-driver
monitors ﬁle-open operations insides shreds. When the ﬁrst
time a ﬁle F is accessed by a shred S, S-driver marks F
as a shred-private ﬁle and only allows shreds that share the
same s-pool with S to access F . This restriction is persistent