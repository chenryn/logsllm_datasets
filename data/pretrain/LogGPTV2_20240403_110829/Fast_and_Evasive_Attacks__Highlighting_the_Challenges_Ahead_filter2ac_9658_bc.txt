do not allow traﬃc from third parties to transit their network, the second case is
highly unlikely, and one can conclude that spooﬁng has taken place. Note that
this spoof detection mechanism is unable to catch instances of spooﬁng where
the victim of the spooﬁng is within the protected network.
When a spoofed alert is detected, the real IP of the attacker can be fetched
from the IP mapping database if Ethernet addresses are present in the alerts.
In the case of alerts without Ethernet addresses the real attacker cannot easily
be identiﬁed. In this case, any of the hosts in the protected network could be
the attacker. The spoof detection module handles this by forwarding the alert
to every host in the subnet where the attack was detected.
After spoof detection is performed, the alerts are processed by the risk analysis
module. The module keeps one HMM model for each of the protected hosts.
When an alert is received, the models for the hosts referenced in the alert are
looked up. For each of these hosts, the HMM model is updated with the latest
observation. Finally, the risk value for each of the aﬀected hosts is calculated
and the alert is augmented with the maximum of these risk values before the
alert is sent to the administrator.
3.2 Implementation
The real-time risk assessment implementation is based on the algorithms in [1].
Only one observation probability matrix Q is deﬁned for each host. For hosts
with multiple sensors (such as Mill and Pascal in Section 4.1), all sensors have
been incorporated into one Q.
The implementation is integrated into the STAT framework, as described
above. It consists of the following C++ classes: RiskObject (representing a
host), RiskSensor (representing an IDS sensor), and RiskObservation (rep-
resenting a sensor observation). The implementation receives IDMEF messages
from the framework, and processes these based on the source and destination IP
addresses, sensor identities, alert timestamps, and the alert impact values.
Using Hidden Markov Models to Evaluate the Risks of Intrusions
153
As the Hidden Markov Models are discrete time models, the risk is updated
for every second for each host, based on the available alerts relevant to each host.
A relevant alert either has the IP address of the host in question as its source or
destination IP address, or it originates from a host-based IDS on the host. If no
alert is available for a host, the system uses the default observation “no alert”
as input to the HMM computation. If more than one alert is received for a host
during the 1 sec. interval, the ﬁrst alert is processed and the remaining alerts
are queued for the next intervals. For the sake of responsiveness, the maximum
queue size is set to 60 seconds for the purpose of this paper. All new alerts will
be discarded when the maximum queue size has been reached. This approach is
chosen in order to be able to handle alert bursts, such as the outbound DDoS
described in Section 4.1. Note that the problem of alert queues can be mitigated
by choosing a suﬃciently short time interval for the hidden Markov models.
4 Experiments
The purpose of this section is to validate the proposed method and to demon-
strate how the system outlined in Section 3 can be used on real-life data. For the
experiments two diﬀerent data sets were used: the Lincoln Laboratory 2000 data
set and traﬃc data from TU Vienna. The ﬁrst data set contains experimental
data, whereas the second contains data from a real network. The advantage of
using the Lincoln Labs data is that it contains a truth ﬁle [11]. Therefore, the
results can be checked against these values. The TU Vienna data set validates
the feasibility of using the approach on real data.
The basic experimental approach was to determine the HMM parameters Q,
P, π, and C for the Lincoln Laboratory data and to verify that the results
produced by our method correspond to the information gleaned from the truth
ﬁle. The same parameters were then used on the real traﬃc data from TU Vienna
in order to validate the model’s parameters in a realistic setting. By using the
same HMM parameters for both data sets, where applicable, it is possible to
compare the results obtained from the two cases.
The outcome of the experiments are highly dependent on the HMM param-
eters and the alert classiﬁcation, in addition to the alert and traﬃc data used.
The HMM parameters used in these examples were determined manually based
on the authors’ experience with the models. The following general guidelines
were used in determining the appropriate values for the parameters:
– The risk level for a host should be close to zero when there are no alerts.
This implies that the probability of being in state G should be close to 1
when there are no alerts.
– When state C occurs, the model should stay in this state longer than it
would for states P and A.
– In order to make the results comparable, the cost vector for all hosts are
identical. In a real setting, the cost vectors for diﬀerent assets would vary
depending on their value.
154
A. ˚Arnes et al.
Section 4.1 presents the details of the parameters used and the results of ap-
plying the method to the Lincoln Laboratory 2000 data set. Section 4.2 presents
the same for the TU Vienna data.
4.1 Lincoln Laboratory Scenario (DDoS) 1.0
The Lincoln Laboratory 2000 data set [11] is based on experimental network traf-
ﬁc for a network of four class C subnets. The data set contains a network dump,
as well as Solaris BSM [16] system logs. This data has been processed with the
Snort network-based IDS and the USTAT host-based IDS in order to generate ID-
MEF alerts. The resulting data set contains more than three hours of intrusion
detection data for subnets 172.16.112.0/24, 172.16.113.0/24,172.16.114.0/24,and
172.16.115.0/24. The hosts Mill (172.16.115.20), Pascal (172.16.112.50), and Locke
(172.16.112.10) are attacked and compromised, and they are then used to launch
a DDoS attack against an external host using spoofed IP addresses. There are two
Snort network IDS sensors (an outside sensor and a DMZ sensor), and the hosts
Mill and Pascal are equipped with instances of the USTAT host-based IDS.
Attack Phases. The data set contains an attack in ﬁve phases (see [11]). The
phases are outlined below with excerpts from the original description.
IP sweep. approximate time 09:45 to 09:52: “The adversary performs a scripted
IPsweep of multiple class C subnets on the Air Force Base. (...) The attacker
sends ICMP echo-requests in this sweep and listens for ICMP echo-replies to
determine which hosts are up.”
Sadmind ping. approximate time 10:08 to 10:18: “The hosts discovered in the
previous phase are probed to determine which hosts are running the sadmind
remote administration tool. (...) Each host is probed, by the script, using the
ping option of the sadmind exploit program.”
Break in to Mill, Pascal, and Locke. approximate time 10:33 to 10:34: “The
attacker then tries to break into the hosts found to be running the sadmind
service in the previous phase. The attack script attempts the sadmind Remote-
to-Root exploit several times against each host (...) there are 6 exploit attempts
on each potential victim host. To test whether or not a break-in was successful,
the attack script attempts to login.”
Installation of DDoS tools on Mill, Pascal, and Locke. approximate time 10:50:
“Entering this phase, the attack script has built a list of those hosts on which
it has successfully installed the hacker2 user. These are Mill, Pascal, and Locke.
For each host on this list, the script performs a telnet login, makes a directory
(...) and uses rcp to copy the server-sol binary into the new directory. This is the
mstream server software. The attacker also installs a .rhosts ﬁle for themselves.”
Outbound DDoS with spoofed source IP addresses. approximate time 11:27: “In
the ﬁnal phase, the attacker manually launches the DDoS. This is performed
via a telnet login to the victim on which the master is running, and then, from
the victim, a telnet to port 6723 of the localhost. (...) The command mstream
131.84.1.31 5 causes a DDoS attack, of 5 seconds duration (...) to be launched
by all three servers simultaneously.”
Using Hidden Markov Models to Evaluate the Risks of Intrusions
155
Observation Messages. Based on the available alert data and the output
from the alert classiﬁcation preprocessor, we use the following observations in
the implementation:
1. Suspicious Snort alert: All alerts that are not explicitly classiﬁed.
2. Compromise Snort alert: All alerts that are classiﬁed as “successful admin”.
3. Scan Snort alert: All alerts that are classiﬁed as “successful recon limited”.
4. Host-based alert (only available for hosts Mill and Pascal): The data set only
contains the alert types “unauth delete” and “restricted dir write”.
5. Outbound Snort alert: All Snort alerts originating from an internal host.
6. No alert: This observation is assumed whenever there are no other alerts to
be processed for a host.
The classiﬁcation could be made more ﬁne-grained, but it is kept simple in this
paper for demonstration purposes. In particular, the output of the host-based
USTAT IDS in a real setting would generate a wide range of diﬀerent alert
types. In this example, however, we have made the simpliﬁcation of modeling
the USTAT sensor as producing one observation type only. Similarly, we have
made the assumption that outbound Snort alerts reduce the probability of being
in the “good” state.
Model Parameters. The monitored network consists of 1016 IP addresses,
each modeled by an HMM. The transition probability matrices P, observation
probability matrices Q, initial state distribution vectors π, and the cost vectors
C are the same for each host, with the exception of the hosts Mill and Pascal,
which incorporate the possibility of receiving USTAT alerts. As an example, the
host Mill is modeled as follows:
PM ill =
=
QM ill =
=
πM ill = (πG, πP , πA, πC) = (1, 0, 0, 0),
CM ill = (cG, cP , cA, cC) = (0, 25, 50, 100).
⎞
⎟⎟⎠ ,
0.991995
1 × 10−34 1 × 10−34 1 × 10−34 1 − 3 × 10−34
0.004
0.992995
⎞
⎟⎟⎠
0.003
0.004
0.004
0.003
pP G pP P pP A pP C
pAG pAP pAA pAC
pCG pCP pCA pCC
0.004
⎛
⎜⎜⎝pGG pGP pGA pGC
⎛
⎜⎜⎝ 0.992995
⎛
⎜⎜⎝qG(1) qG(2) qG(3) qG(4) qG(5) qG(6)
⎛
⎞
⎜⎜⎝0.05 0.0001 0.02 0.01 0.02 0.8999
⎟⎟⎠ ,
qP (1) qP (2) qP (3) qP (4) qP (5) qP (6)
qA(1) qA(2) qA(3) qA(4) qA(5) qA(6)
qC(1) qC(2) qC(3) qC(4) qC(5) qC(6)
0.05 0.0001 0.25 0.01 0.02 0.6699
0.1 0.005 0.1 0.03 0.03 0.735
0.02 0.05 0.04 0.04 0.05
0.8
0.000005
0.000005
0.000005
⎞
⎟⎟⎠
156
A. ˚Arnes et al.
From PMill, we can see that the probability of entering the state C is relatively
low, but that once entered, the probability of leaving this state is very low. From
QMill, we can see that the scan observation is relatively likely to occur in the P
state, that the suspicious and scan observations are relatively likely to occur in
the A state, and that the USTAT and outbound observations have a relatively
high probability in the C state. Note that once entered, the C state is likely to
last for a long time. From πMill and CMill, we can see that the initial state of the
host is G with corresponding cost 0. The maximum cost for the host is 100. Most
of the hosts do not have a host-based IDS and are modeled with the following
observation probability matrix (host Locke is given as an example):
⎛
⎜⎜⎝0.05 0.0001 0.02 0 0.02 0.9099
0.05 0.0001 0.25 0 0.02 0.6799
0.1 0.005 0.1 0 0.03 0.765
0.02 0.05 0.04 0 0.05 0.84
⎞
⎟⎟⎠
QLocke =
For the purpose of this example all hosts, except the hosts with USTAT, have
the exact same model parameters. This is done for demonstration purposes and
in order to provide comparable results between the hosts. In a real setting, the
model parameters of the hosts would vary according to their security conﬁgura-
tions, the observation probability parameters vary according to the sensors used,
and the cost vector is determined by the value of the assets and the consequence
of the diﬀerent security states.
Results. The above models were implemented and used to perform real-time
risk assessment on the Lincoln Laboratory data set. The entire data set has a
duration of 11836 sec., and a total of 36635 alerts, 84 of which are USTAT alerts.
The remaining are Snort alerts. As outlined above, the data set consists of an
attack in ﬁve phases. By inspecting the data set, we can see that the phases
correspond to the approximate time periods 1500 - 1920 sec. (the IP sweep),
2880 - 3480 sec. (the sadmind ping), 4380 - 4420 sec. (the break in to Mill,
Pascal, and Locke), 5400 sec. (the installation of DDoS tools), and 7620 sec.
(the outbound DDoS).
Figure 3 shows the total assessed risk for the Lincoln Laboratory data for
the full duration of the data set. The ﬁgure shows a sum of the risk for all
hosts in the four subnets (in total 1016 hosts). The break-ins performed against
Mill, Pascal, and Locke are clearly visible as peaks of risk activity. The sadmind
ping also introduces a peak in the data, but the IP sweep and the installation
of DDoS tools are hardly distinguishable from the remaining activity. Note that
the system seems to have a minimum risk of approximately 1200 in the long run.
This is caused by a stable security state with risk level 1.09 for the individual
hosts, given a suﬃciently long interval of only “no alert” observations. The stable
security state risk for the entire network is consequently 1107. The diﬀerence can
be explained by the fact that the host 172.16.114.1 has a high amount (more than
2000) of outbound ICMP related alerts. As a router, this host should probably
have diﬀerent HMM parameters then the other hosts.
Using Hidden Markov Models to Evaluate the Risks of Intrusions
157
 2000
 1800
 1600
 1400
 1200
 1000
 800
 600
 400
 200
y
t
i
v
i
t
c
A
k
s
R
i
 0
 0
 2000
 4000
 6000
Time (s)
 8000
 10000
Fig. 3. Total assessed risk for Lincoln Labs data set
Figure 4 (a), (b), and (c) show the assessed risk for the hosts Mill, Pascal, and
Locke, respectively. The hosts Mill and Pascal have host-based IDSs (USTAT) that
provide several alerts during the experiment. This can be seen in Fig. 4 (a), (b), and
(c), as the host Locke has far less activity than the other two. Phase 3 and 5 of the
attack are clearly marked with the maximum risk activity value (100) for all three
hosts. Phase 2 and 4 are also visible as peaks, whereas phase 1 is hardly discernible
from the other activity in Fig. 4 (a) and (b), and not visible at all in (c). Note that
Pascal (Fig. 4 (b)) shows more peaks than Mill (Fig. 4 (a)). This is caused by the
fact that Pascal produces 70 USTAT alerts, while Mill only produces 14.
Figure 5 (a) and (b) show the assessed total network risk and the assessed
risk for Mill at the approximate time of the compromise (4000s to 6000s). The
graphs correspond to Fig. 3 and 4 (a), but zoom in on the time period. Fig. 5
(b) shows the two peaks corresponding to phase 3 and 4 of the attack.
By counting the priority of the alerts for the entire data set, we can eval-
uate the performance of the alert prioritization mechanism. However, for the
purpose of the prioritization results, we do not consider the outbound DDoS
attack with spoofed IP addresses and the outbound alerts from the router with
IP address 172.16.114.1. The outbound DDoS attack alerts represents 93% of
the total alerts, and are all marked with the highest priority. The IP address
172.16.114.1 is discussed above. It has a high number of alerts (6% of the total
amount), and they would also all be marked as maximum priority alerts. Having
ﬁltered out these alerts, 52.49% of the alerts are with priority below 20, 28.87%
with priority between 20 and 40, 6.49% with priority between 40 and 60, 2.35%
with priority between 60 and 80, and 9.81% with priority between 80 and 100. It
is clear that the alert prioritization is successful in that only a small percentage
of the alerts are assigned high priority values. The majority of the alerts are
marked as low priority.
We see that the risk assessment method with the current conﬁguration and