tion,” Last accessed in 2019, available at https://www.twilio.com/.
[44] S. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky, Y. Kang,
X. Li, J. Miller, A. Ng, J. Raiman et al., “Deep voice: Real-time neural
text-to-speech,” in Proceedings of the 34th International Conference on
Machine Learning-Volume 70, 2017.
[45] M. Ito and R. Donaldson, “Zero-crossing measurements for analysis
and recognition of speech sounds,” IEEE Transactions on Audio and
Electroacoustics, vol. 19, no. 3, pp. 235–242, 1971.
[46] E. Bursztein, R. Beauxis, H. Paskov, D. Perito, C. Fabry, and J. Mitchell,
“The failure of noise-based non-continuous audio captchas,” in Security
and Privacy (SP), 2011 IEEE Symposium on.
IEEE, 2011, pp. 19–31.
[47] J. Tam, J. Simsa, S. Hyde, and L. V. Ahn, “Breaking audio captchas,” in
Advances in Neural Information Processing Systems, 2009, pp. 1625–
1632.
[48] S. Sano, T. Otsuka, and H. G. Okuno, “Solving google’s continuous
audio captcha with hmm-based automatic speech recognition,” in Inter-
national Workshop on Security. Springer, 2013, pp. 36–52.
[49] S. Solanki, G. Krishnan, V. Sampath, and J. Polakis, “In (cyber) space
bots can hear you speak: Breaking audio captchas using ots speech
recognition,” in Proceedings of the 10th ACM Workshop on Artiﬁcial
Intelligence and Security. ACM, 2017, pp. 69–80.
[50] K. Bock, D. Patel, G. Hughey, and D. Levin, “uncaptcha: a low-resource
defeat of recaptcha’s audio challenge,” in Proceedings of
the 11th
USENIX Conference on Offensive Technologies. USENIX Association,
2017, pp. 7–7.
[51] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” Neural Information Process-
ing Systems, vol. 25, 01 2012.
[52] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going Deeper with Convolutions,”
in 2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015, pp. 1–9.
[53] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image
Recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[54] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motl´ıˇcek, Y. Qian, P. Schwarz, J. Silovsk´y, G. Stem-
mer, and K. Vesel´y, “The kaldi speech recognition toolkit,” in IEEE 2011
Workshop on Automatic Speech Recognition and Understanding.
IEEE
Signal Processing Society, 2011, iEEE Catalog No.: CFP11SRW-USB.
[55] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D.
Tygar, “Adversarial machine learning,” in Proceedings of the 4th
ACM Workshop on Security and Artiﬁcial Intelligence, ser. AISec ’11.
New York, NY, USA: ACM, 2011, pp. 43–58. [Online]. Available:
http://doi.acm.org/10.1145/2046684.2046692
[56] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
physical world,” arXiv preprint arXiv:1607.02533, 2016.
[57] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” arXiv preprint
arXiv:1312.6199, 2013.
[58] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[59] S. Baluja and I. Fischer, “Adversarial transformation networks: Learning
to generate adversarial examples,” arXiv preprint arXiv:1703.09387,
2017.
[60] J. Su, D. V. Vargas, and S. Kouichi, “One pixel attack for fooling deep
neural networks,” arXiv preprint arXiv:1710.08864, 2017.
[61] S. M. Moosavi Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in Proceedings of
2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), no. EPFL-CONF-218057, 2016.
[62] T. B. Brown, D. Man´e, A. Roy, M. Abadi, and J. Gilmer, “Adversarial
patch,” arXiv preprint arXiv:1712.09665, 2017.
[63] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition,”
in Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2016, pp. 1528–1540.
[64] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in Security and Privacy (EuroS&P), 2016 IEEE European Symposium
on.
IEEE, 2016, pp. 372–387.
[65] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in Security and Privacy (SP), 2017 IEEE Symposium on.
IEEE, 2017, pp. 39–57.
[66] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are
easily fooled: High conﬁdence predictions for unrecognizable images,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 427–436.
[67] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” Proceedings of the 2017 Network
and Distributed System Security Symposium (NDSS), 2017.
[68] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu, “Dolphinattack:
Inaudible voice commands,” in Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 2017,
pp. 103–117.
[69] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine noodles:
exploiting the gap between human and machine speech recognition,”
WOOT, vol. 15, pp. 10–11, 2015.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
725
[70] W. Cai, A. Doshi, and R. Valle, “Attacking speaker recognition with
deep generative models,” arXiv preprint arXiv:1801.02384, 2018.
[71] Y. Gong and C. Poellabauer, “Crafting adversarial examples for speech
paralinguistics applications,” arXiv preprint arXiv:1711.03280, 2017.
[72] C. Kereliuk, B. L. Sturm, and J. Larsen, “Deep learning and music
adversaries,” IEEE Transactions on Multimedia, vol. 17, no. 11, pp.
2059–2071, 2015.
[73] D. Kumar, R. Paccagnella, P. Murley, E. Hennenfent, J. Mason, A. Bates,
and M. Bailey, “Skill squatting attacks on amazon alexa,” in 27th
USENIX Security Symposium (USENIX Security 18). USENIX As-
sociation, 2018.
[74] M. K. Bispham, I. Agraﬁotis, and M. Goldsmith, “A Taxonomy of
Attacks via the Speech Interface,” 2018.
[75] “Google Cloud Speech-to-Text API,” Last accessed in 2019, available
at https://cloud.google.com/speech-to-text/.
[76] “Transcribing Phone Audio with Enhanced Models,” Last accessed
in 2019, available at https://cloud.google.com/speech-to-text/docs/
phone-model.
[77] “Wit.ai Natural Language for Developers,” Last accessed in 2019,
available at https://wit.ai/.
[78] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,
R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep
speech: Scaling up end-to-end speech recognition,” arXiv preprint
arXiv:1412.5567, 2014.
[79] “Mozilla project deepspeech,” Last accessed in 2019, available
https://azure.microsoft.com/en-us/services/cognitive-servic/
at
speaker-recognition/.
[80] S. Naren, “Speech recognition using deepspeech-2,” Last accessed in
2019, available at https://github.com/SeanNaren/deepspeech.pytorch.
[81] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual
evaluation of speech quality (pesq)-a new method for speech quality as-
sessment of telephone networks and codecs,” in 2001 IEEE International
Conference on Acoustics, Speech, and Signal Processing. Proceedings
(Cat. No. 01CH37221), vol. 2.
IEEE, 2001, pp. 749–752.
[82] “Simple Audio Recognition,” Last accessed in 2019, available at https:
//www.tensorﬂow.org/tutorials/sequences/audio recognition.
[83] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry,
at odds with accuracy,” arXiv preprint
“Robustness may be
arXiv:1805.12152, vol. 1, 2018.
[84] E. Dohmatob, “Limitations of adversarial robustness: strong no free
lunch theorem,” arXiv preprint arXiv:1810.04065, 2018.
[85] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry,
“Adversarial examples are not bugs, they are features,” in Advances in
Neural Information Processing Systems, 2019, pp. 125–136.
APPENDIX
We provide additional discussion and results considered
to the main contributions of the
tangential
insightful, yet
proposed attack.
A. Models
Google (Normal): To demonstrate our attack in a truly
black-box scenario, we target the speech transcription APIs
provided by Google. The ‘Normal’ model
is provided by
Google for ‘clean’ use cases, such as in home assistants, where
the speech is not expected to traverse a cellular network [75].
Google (Phone): To demonstrate our attack against model
trained for noisy audio, we test the attack against the ‘Phone’
model. Google provides this model for cellular use cases
and trained it on call audio that will be representative of
cellular network compression [76]. We also assume that the
Google ‘Phone’ model will be robust against the noise, jitter,
loss and compression introduced to audio samples that have
traveled through the telephony network. Facebook Wit: To
ensure better coverage across the space of proprietary speech
transcription services, we also target Facebook Wit, which
provides access to a ‘clean’ speech transcription model [77].
As before, no information is known about this model due to
its proprietary nature.
Deep Speech-1: The goal of Deep Speech 1 was to eliminate
hand-crafted feature pipelines by leveraging a paradigm known
as end-to-end learning [24], [78]. This results in robust speech
transcription despite noisy environments, if sufﬁcient training
data is provided. For our experiments, we use an open-source
implementation and checkpoint provided by Mozilla with
MFCC features [79].
Deep Speech-2: Deep Speech-2 introduced architecture op-
timizations for very large training sets. It is trained to map
raw audio spectrograms to their correct transcriptions, and
demonstrates the current state-of-the-art
in noisy, end-to-
end audio transcription [26]. We use an open-source imple-
mentation2 trained on LibriSpeech provided by GitHub user
SeanNaren [32], [80]. The primary difference in our two tested
versions is feature preprocessing: the tested version of Deep
Speech-1 uses MFCC features, while the tested version of
Deep Speech-2 uses raw audio spectrograms.
CMU Sphinx: The CMU Sphinx project is an open-source
speech transcription repository representing over twenty years
of research in this task [25]. Sphinx does not heavily rely
on deep learning techniques, and instead implements a com-
bination of statistical methods to model speech transcription
and high-level language concepts. We use the PocketSphinx
implementation and checkpoints provided by the CMU Sphinx
repository [25].
Microsoft Azure: To demonstrate our attack against AVI
systems in a black-box environment, we attack the Speaker
Identiﬁcation API provided by Microsoft Azure [38]. This
system is proprietary, and hence, completely black-box. There
is no publicly available information about the internals of the
system.
B. Trivial White-Noise Attack
1) Motivation: Readers might be tempted to use trivial
attacks to subvert ASR and AVI models. This includes adding
white-noise to benign audio samples. However, in the follow-
ing subsection, we show that any such trivial techniques will
fail to achieve the attacker goals: fool the model whilst not
impacting human interpretability of the audio sample.
2) Methodology and Setup: We tested this white-noise
method by attacking a random set of 100 audio ﬁles that
contained speakers uttering a single word. We added white-
noise to these samples to generate adversarial audio samples.
Next, we passed both the original audio and the white-noise
infused audio samples to the Google Speech API. We recorded
the number of samples that were incorrectly transcribed by the
API.
Next, to measure the impact on human interpretability, we
used the Perceptual Evaluation of Speech Quality (PESQ)
standard [81]. This is a global standard used for measuring the
2At the time of running our experiments, the implementation did not include
a language model to aid in the beam-search decoding.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
726
)
%
(
d
e
v
o
m
e
R
20
26, 2.6
33, 2.3
34, 2.3
55, 1.9
20
25
30
35
Frame Size
25
26, 2.5
39, 2.2
48, 2.0
58, 1.9
(ms)
30
30, 2.4
43, 2.0
52, 2.0
67, 1.7
35
33, 2.2
49, 2.0
57, 1.9
71, 1.7
TABLE VI: The table shows the results of the frame dropping
experiment. The columns contain the size of the frame that
was removed. The rows contain the percentage of the frames