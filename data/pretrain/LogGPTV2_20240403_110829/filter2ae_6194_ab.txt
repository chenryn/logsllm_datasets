    # 查看调试配置
    $ bcdedit /dbgsettings
执行如下：
![
](https://images.seebug.org/content/images/2023/02/6268f5fc-37c3-4427-b632-f4fb249af150.png-w331s)
[16.bcdedit配置串口调试]
随后我们切换至调试机(`debuger`)下，使用 WinDBG 设置串口调试配置，波特率为 `115200`，端口为 `1`，不勾选 `pipe`，勾选
`reconnect`，如下：
![
](https://images.seebug.org/content/images/2023/02/22208701-fdd0-4f12-a9c7-076a07245e09.png-w331s)
[17.windbg-com标签]
设置完毕后，在 WinDBG 显示 `Waiting to reconnect...`
后，重启被调试机(`debugee`)，调试机(`debuger`)将在其系统启动时连接上去，使用 `break` 可将其断下来，如下：
![
](https://images.seebug.org/content/images/2023/02/0195a5be-5d20-469f-b17d-45ba31463431.png-w331s)
[18.windbg串口双机调试]
> 我这里首次连接时 WinDBG 将异常退出，不过重新启动 WinDBG 并设置好参数即可成功连接。
**ProxmoxVE串口调试的一些补充**  
熟悉 Vmware 搭建 windows 内核调试的朋友，通常都使用命名管道进行配置如 `\\.\pipe\com1`，但 ProxmoxVE
下的串口设备(serial) 仅支持 `/dev/.+|socket` 两种类型(实际上底层的 `kvm/qemu` 支持很多，但 ProxmoxVE
会直接报错无法启动虚拟机)，这为我们的串口调试带了一些困难；
同时我们默认配置的串口设备类型为 `socket`，其实际运行的参数如下：
![
](https://images.seebug.org/content/images/2023/02/6af7bbfa-5131-4ccf-9090-c0e2f9907afe.png-w331s)
[19.kvm实际启动参数-serial]
串口设备的参数为 `-chardev socket,id=serial0,path=/var/run/qemu-server/133.serial0,server=on,wait=off`，同样其参数在 ProxmoxVE 下不能修改。
在此限制条件下，我们可以使用 `socat` 以 `UNIX-CLIENT` 的方式将两台虚拟机的串口设备进行连接，从而实现串口双机调试。
### 0x05 kdnet问题排查
**1.hyper-v虚拟化导致kdnet无法工作**  
在上文“网络双机调试”的环境配置中，我们在 ProxmoxVE 配置被调试机(`debugee`)时将其操作系统类型设置为 `Other` 类型，这样才能使
kdnet 正常工作，为什么呢？
我们按正常的安装流程在 ProxmoxVE 中安装一台 windows10(即操作系统类型选择为 `win10/2016/2019`) 并启动，通过 ssh
登录 ProxmoxVE 查看底层 kvm/qemu 的启动参数，如下：
![
](https://images.seebug.org/content/images/2023/02/d661a873-00dd-4701-bb92-0fdac0541e76.png-w331s)
[20.kvm实际启动参数-cpu]
我们可以看到其 cpu 参数为 `-cpu
kvm64,enforce,hv_ipi,hv_relaxed,hv_reset,hv_runtime,hv_spinlocks=0x1fff,hv_stimer,hv_synic,hv_time,hv_vapic,hv_vpindex,+kvm_pv_eoi,+kvm_pv_unhalt,+lahf_lm,+sep`，其中
`hv_*` 的配置表示 kvm 将以 hyper-v 的方式提供虚拟化功能，windws 虚拟机将认为自己运行在 hyper-v 的技术之上，以便使用
hyper-v 的功能并在一定程度上提高运行性能。
而根据前辈在 kvm/qemu 下使用的 kdnet 的经验(https://www.osr.com/blog/2021/10/05/using-windbg-over-kdnet-on-qemu-kvm/) 来看，`hv_*` 配置项会导致 kdnet 工作时认为自身位于 hyper-v
环境下，从而使用 hyper-v 中未公开的通信机制，最终导致 kdnet 无法正常工作；
经过测试验证，在我们的环境下的表现和前辈文章不一致，`hv-vendor-id`(CPUID) 并不会被修改，这可能和 qemu 的版本有关系，但
`hv_*` 的配置项确实会影响 kdnet 的工作。我们沿着这个思路查找 ProxmoxVE 调用 kvm/qemu 的源码，在 [qemu-server](https://git.proxmox.com/?p=qemu-server.git;a=summary) 源码包中 `qemu-server/PVE/QemuServer.pm#vm_start()` 找到调用 kvm/qemu 的代码入口；
随后跟入该函数，在 `qemu-server/PVE/QemuServer.pm#config_to_command()` 找到拼接 qemu
命令的代码如下：
![
](https://images.seebug.org/content/images/2023/02/1774fb42-45d9-4d8b-9de4-eb065c673cf5.png-w331s)
[21.pve源码拼接qemu命令]
随后在 `qemu-server/PVE/QemuServer/CPUConfig.pm#get_cpu_config()` 找到 `-cpu`
参数的生成代码：
![
](https://images.seebug.org/content/images/2023/02/d4dca5c8-0b79-4874-8d6c-7a8dc4d2f293.png-w331s)
[22.pve源码拼接cpu参数]
结合上下文可以了解到，当操作系统为 `win10` 等类型时，此处将自动在 `-cpu` 参数中添加 `hv_*` 参数，以更好的支持 windows
虚拟机。
那么在设置虚拟机硬件时，我们只需要选择操作系统类型为 `other`，即可避免 ProxmoxVE 使用 `hv_*` 参数启动虚拟机，从而保证 kdnet
可以正常工作。
> PS:  
>  1.对于已配置好的虚拟机，可使用 ssh 登录 ProxmoxVE，修改虚拟机配置文件 `/etc/pve/qemu-> server/[id].conf`，设置启动的 `ostype: other`，也可以关闭 hyber-v 的虚拟化。  
>  2.对于已成功配置网络调试的主机，即便再重新打开 hyber-v 的虚拟化，kdnet 也能正常工作(这可能和已成功配置的网络调试器驱动有关？)
**2.非DHCP的调试机(`debugee`)启动时卡logo界面**  
当我们使用 `bcdedit` 配置好网络调试后，重启虚拟机可以发现 windows 使用了 `以太网(内核调试器)` 替代了原始网卡：
[23.调试器网卡驱动]
`以太网(内核调试器)` 其默认采用 DHCP 的方式获取 ip，而通常情况下 ProxmoxVE 都采用静态 ip 分配，在系统启动阶段，该网卡将首先等待
DHCP 分配 ip，若获取失败，则自己分配 `169.254.*.*` 的地址；这个阶段发生在 windows logo 界面，大致需要 10min。
采用静态分配地址的 ProxmoxVE 服务器，可在被调试机(`debugee`)内修改网络调试，关闭 DHCP 即可解决：
    # 查看网络调试配置
    $ bcdedit /dbgsettings
    # 关闭网络调试配置中的 dhcp
    $ bcdedit /set "{dbgsettings}" dhcp no
    # 查看网络调试配置
    $ bcdedit /dbgsettings
执行如下：
[24.关闭网络调试的dhcp]
**3.kdnet下被调试机联网问题**  
在某些场景下，我们需要在联网条件下进行内核调试，串口调试不会影响网络，但网络调试会使用 `以太网(内核调试器)` 替代原始网卡，其默认采用 DHCP
方式，若上游配置好了 DHCP 服务器则可正常使用；
如果采用静态地址分配，则进入虚拟机后，在 `以太网(内核调试器)` 上配置静态地址即可，联网和网络调试不会冲突，都可以正常使用：
![
](https://images.seebug.org/content/images/2023/02/608482bd-d4bf-4810-b53a-dd1e24ce49d8.png-w331s)
[25.调试器网卡配置静态ip]
**4.kdnet下多网卡的被调试机配置**  
某些场景下，我们的虚拟机具有多张网卡，若想指定具体的网卡作为调试网卡，可以使用如下命令：
    # 在网络调试配置成功的前提下
    # 设置 busparams 参数
    # 通过设备管理器查看对应网卡的 PCI 插槽 [bus.device.function]
    $ bcdedit /set "{dbgsettings}" busparams 0.19.0
    # 查看网络调试配置
    $ bcdedit /dbgsettings
执行如下：
![
](https://images.seebug.org/content/images/2023/02/78d3fbee-e9e7-41a1-9e38-9b58e17d4f66.png-w331s)
[26.指定网络调试器网卡]
### 0x06 vmware碎碎念
通过以上一阵折腾，不得不说 vmware 在搭建 windows 调试环境这条路上帮我们铺平了道路；在实验过程中，我同时也配置了 vmware
下的环境，在这里我补充两个偏门的点，希望可以帮助到使用 vmware 搭建环境的小伙伴。
这里的测试环境如下：
    Windows10 1909 专业版(宿主机)
    Vmware Workstation 17
    Windows10 1909 专业版(虚拟机)
**1.vmware下的网络调试搭建**  
在网络调试的需求下，无论是使用宿主机调试虚拟机，还是使用虚拟机调试虚拟机，vmware 均可以完美支持；其 vmware 提供的虚拟机网卡默认支持
windows 网络调试，同时 vmware 默认采用 NAT 网络并默认开启 DHCP。
**2.vmware串口调试搭建**  
使用 vmware 通过宿主机串口调试虚拟机，这我们再熟悉不过了，在虚拟机串口中配置命名管道
`\\.\pipe\com1`，设置`该端是服务器`，设置`另一端是应用程序`，勾选 `轮询时主动放弃CPU`，如下：
 [27.vmware被调试机串口配置]
在虚拟机使用 `bcdedit` 配置串口调试，随后在宿主机中打开 WinDBG 使用串口调试连接即可，如下：
[28.vmware宿主机串口调试]
**但如果要使用虚拟机串口调试虚拟机** ，这就稍微有点不同了；首先配置被调试机(`debugee`)串口，配置命名管道
`\\.\pipe\com1`，设置`该端是服务器`，设置`另一端是虚拟机`，勾选 `轮询时主动放弃CPU`，如下：
![
](https://images.seebug.org/content/images/2023/02/478662af-b96e-4100-8cd7-d328ae3a46ad.png-w331s)
[29.vm-vm被调试机串口调试]
随后配置调试机(`debuger`)串口，配置命名管道 `\\.\pipe\com1`，设置`该端是客户端`，设置`另一端是虚拟机`，如下：
![
](https://images.seebug.org/content/images/2023/02/7ce7e4c4-d8f3-47c2-94f9-c4530db22df8.png-w331s)
[30.vm-vm调试机串口调试]
同样也在被调试机(`debugee`) 使用 `bcdedit` 配置串口调试，然后在调试机(`debuger`)中使用 WinDBG
进行串口调试，这里需要注意串口设备为 `com1`，且不能勾选 `pipe`(因为命名管道是对于宿主机的，而它在虚拟机内部仅仅是 com 口)，如下：
![
](https://images.seebug.org/content/images/2023/02/5faf1ff0-7cdf-46b9-9f94-5c37d756109e.png-w331s)
[31.vm-vm windbg配置串口调试]
配置完成后，被调试机(`debugee`)重启即可成功连接。
### 0x07 References
* * *