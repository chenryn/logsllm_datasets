2
3
4
5
6
7
let the candidate node pool cp = (cid:25)i
b;
if (|cp| > th1 && |SPLD(cp)| > th2) /* thresholds */
compute SPLD(v) and then delete all nodes from cp
having different SPLDs from SPLD(v);
return true;
select a vertex v ∈ (cid:25)i
p is nonsingleton. */
p; /* (cid:25)i
else
′ ∈ cp;
′);
b; v
return true;
p; v; (cid:5)b; (cid:25)i
((cid:5)p; (cid:5)b) == true)
if (O2_Mapping((cid:5)p; (cid:5)b) == true)
select a vertex v
((cid:5)p, (cid:5)b) = Decomposition((cid:5)p; (cid:25)i
bool reﬁneSucceed = true;
if (Reﬁnement∗
8
9
10
11
12
13
14
15
16
17
18
19
Figure 5: Pseudocode of the optimization algorithm for data cen-
ter graphs. For clarity, Reﬁnement∗() is explained in the context.
else reﬁneSucceed = f alse;
delete v
if (!reﬁneSucceed && !empty(cp))
′ and all its equivalent nodes from cp;
else reﬁneSucceed = f alse;
return f alse;
goto line 8;
′
property, we propose using SPLD as a more sophisticated signature
to select mapping candidates. That is, when we try to select a node
′
in Gb as a candidate to be mapped to a node v in Gp, we only
v
select the v
from these nodes that have the same SPLD as v. This is
effective because two nodes with different SPLDs cannot be mapped
to each other. However, computing SPLDs for all nodes in a large
graph requires time. Fortunately, in our case, this can be computed
earlier on the blueprint.
In our optimization algorithm, we precompute the SPLDs for all
nodes of Gb beforehand. In lines 6-7, we improve the base algorithm
in this way: if we ﬁnd the number of candidates (i.e., nodes in cp)
for a node, say v in Gp, to be mapped to is larger than a threshold
th1 (i.e., |cp| > th1) and the number of different SPLDs of them is
larger than a threshold th2 (i.e., |SPLD(cp)| > th2), we compute the
SPLD for v and only select candidates in cp having the same SPLD.
Thresholds th1 and th2 are tuneable. Note that using this heuristic is
a tradeoff: although we can do precomputation on Gb ofﬂine, apply-
ing this optimization means that we should compute SPLD(v) online,
which also consumes time. In all our experiments later, we apply this
heuristic on all the structures only once at the ﬁrst round of mapping.
2. Candidate ﬁltering via orbit. It is indicated in [15] that for
v ∈ G and v
cannot be mapped to v, all nodes in the
same orbit as v
cannot be mapped to v either. We ﬁnd this the-
ory is naturally suited for solving the GI problem on data centers:
First, some structures such as BCube are highly symmetric, and there
should be many symmetric nodes within these structures that are in
the same orbit. Second, the blueprint graph is available much earlier
than the real address autoconﬁguration stage, and we can easily pre-
compute the orbits in the blueprint beforehand using preexisting tools
such as [16, 20].
′ ∈ G
′
′
, if v
′
′
′
3. Selective splitting.
In Figure 4, the base algorithm tries to map v to every node in (cid:25)i
b it-
eratively if the current mapping fails which is not effective especially
for highly symmetric data center structures. Observing this, in the op-
timization algorithm, we precompute all the orbits of Gb beforehand.
Then, as shown in lines 16-18, we improve the base algorithm: if we
ﬁnd a certain node v
cannot be mapped to v, we skip all the attempts
that try to map v to any other node in the same orbit as v
, because
according to above theory these nodes cannot be mapped to v either.
In the base algorithm, Reﬁnement() tries
to use the inducing cell to split all the other cells. As data center
structures are sparse, it is likely that while there are many cells in
the partition, the majority of them are disjoint with the inducing cell.
Observing this, in line 11, we use Reﬁnement
() in which we only try
to split the cells that really connect to the inducing cell other than all4.
Furthermore, when splitting a connected cell (cid:25)t, the base algo-
rithm tries to calculate the number of connections between each node
in (cid:25)t and the inducing cell, and then divide (cid:25)t based on these values.
Again, due to sparsity, it is likely that the number of nodes in (cid:25)t that
really connect to the inducing cell is very small. Observing this, in a
similar way, we speed up by only calculating the number of connec-
tions for the nodes actually connected. The unconnected nodes can
be grouped together directly. Speciﬁcally, when splitting (cid:25)t using
inducing cell (cid:25)i, we ﬁrst move the elements in (cid:25)t with connections
to (cid:25)i to the left-end of (cid:25)t and leave all unconnected elements on the
right. Then, we only calculate the k values for the elements on the
left, and group them according to the values.
∗
A Walkthrough Example for O2.
We provide a step by step example of our algorithm in Figure 6.
Gb is labeled by its logical IDs and Gp is labeled by its device IDs.
White arrows mean decomposition and dark arrows mean reﬁnement.
Suppose all orbits in Gb have been calculated beforehand. In this case
they are {{l1 l2 l3 l4}; {l5 l6};{l7 l8}}.
Initially, all nodes in Gp=Gb are in one cell in partitions (cid:5)p=(cid:5)b.
Step (1) decomposes original (cid:5)p=(cid:5)b using d1=l1. Step (2) reﬁnes
the current (cid:5)p=(cid:5)b using inducing cells {d1}={l1}, but fails due to a
non-isomorphic division. This is because during splitting, {d2 d3 d4
d5 d6 d7 d8} has 4 elements with 1-connection to {d1} and 3 ele-
ments with 0-connection; while {l2 l3 l4 l5 l6 l7 l8} has 1 element
with 1-connection to {l1} and 7 elements with 0-connection. There-
fore, they are not divided isomorphically.
From step (2), we know l1 cannot be mapped to d1. By optimiza-
tion heuristic 2, we skip the candidates l2; l3 and l4 which are in the
same orbit as l1. So in Step (3), we decompose the original (cid:5)p=(cid:5)b
using d1=l5. Steps (4) reﬁnes the current (cid:5)p=(cid:5)b using {d1}={l1}.
Speciﬁcally, in (cid:5)p we ﬁnd d2; d3; d5 and d7 have 1-connection to
{d1} while the rest do not, and in (cid:5)b we ﬁnd l1; l2; l7 and l8 have
4We achieve this by maintaining an adjacency list which is built once
when the graph is read. In the adjacency list, for each vertex, we keep
the neighboring vertices, so at any point we know the vertices each
vertex is connected to. We also have another data structure that keeps
track of the place where each vertex is located at within the partition.
In this way, we know which cell is connected to the inducing cell.
l1l2l3l4l5l6l7l8d1d2d5d4d3d6d7d8({d1},{d2d3d4d5d6d7d8})({l5}, {l1 l2 l3 l4 l6 l7 l8})({d1},{d2d3d5d7},{d4d6d8})({l5}, {l1 l2 l7 l8}, {l3 l4 l6})d1l5{d1}{l5}d2l1d3l7{d6}{l6}d4l3({d1},{d2},{d3},{d5},{d7},{d6},{d4},{d8})({l5},{l1}, {l7},{l8 },{l2}, {l6}, {l3}, {l4})(3)(4)(7)(9){d2d3d5d7}{l1 l2 l7 l8}({d1},{d2d3d5d7},{d6},{d4d8})({l5},{l1 l2 l7 l8}, {l6}, {l3 l4})({d1},{d6},{d3d5},{d2d7},{d4d8})({l5},{l6}, {l7 l8}, {l1 l2}, {l3 l4})({d1},{d6},{d2},{d7},{d3d5},{d4d8})({l5},{l6}, {l1}, {l2}, {l7 l8}, {l3 l4})({d1},{d6},{d2},{d7},{d3},{d5},{d4d8})({l5},{l6}, {l1}, {l2}, {l7},{l8}, {l3 l4})(5)(6)(8)GbGp({d1d2d3d4d5d6d7d8})({l1 l2 l3 l4 l5 l6 l7 l8})d1l1(1)({d1}, {d2d3d4d5d6d7d8})({l1}, {l2  l3  l4  l5  l6  l7  l8})({d1},{d2d3d5d7},{d4d6d8})({l1},{l5}, {l2 l3 l4 l6 l7 l8}){d1}{l1}(2)false431-connection to {l5} while the rest do not. So (cid:5)p=(cid:5)b are isomorphi-
cally divided by {d1}={l1}. After step (4), since the current partitions
(cid:5)p=(cid:5)b are not yet equitable, in steps (5) and (6), we continuously use
newly born cells {d2 d3 d5 d7}={l1 l2 l7 l8} and {d6}={l6} to further
split other cells until (cid:5)p=(cid:5)b are equitable.
Steps (7)-(9) decompose the current partitions using d2=l1, d3=l7
and d4=l3 respectively. Since in each of these 3 steps, there is no
cell that can be split by other cells, no division is performed. After
step (9), the two partitions (cid:5)p=(cid:5)b are discrete and we ﬁnd a one-to-
one mapping between Gp and Gb by mapping each node in (cid:5)p to its
corresponding node in (cid:5)b.
Two things should be noted in the above example: First and most
importantly, we do not use optimization heuristic 1 above since we
want to show the case of non-isomorphic division in steps (1)-(2). In
the real O2 mapping, after applying heuristic 1, we will directly go
from step (3) instead of trying to map d1 to l1 as above because they
have different SPLDs. This shows that SPLD is effective in selecting
mapping candidates. Second, although we have not explicitly men-
tioned optimization heuristic 3, in each reﬁnement we only try to split
the connected cells rather than all cells. For example, after step (7),
{d2}={l1} are newly born, but when it comes to reﬁnement, we do
not try to split {d3 d5}={l7 l8} or {d4 d8}={l3 l4} using {d2}={l1}
because they are disjoint.
3.4 Using O2 for Data Center Expansion
To meet the growth of applications and storage, the scale of a data
center does not remain the same for long [21]. Therefore, address
autoconﬁguration for data center expansion is required. Two direct
approaches are either to conﬁgure the new part directly, or to con-
ﬁgure the entire data center as a whole. However, both approaches
have problems: the ﬁrst one fails to take into account the connections
between the new part and the old part of the expanded data center;
the second one considers the connections between the new part and
the old part, but it may cause another lethal problem, i.e., the newly
allocated logical IDs are different from the original ones for the same
devices of the old part, messing up existing communications.
To avoid these problems, DAC conﬁgures the entire data center
while keeping the logical IDs for the old part unmodiﬁed. To achieve
this goal, we still use O2 but need to modify the input. Instead of
putting all the nodes from a graph in one cell as before, we ﬁrst dif-
ferentiate nodes between the new part and the old part in Gp and
Gb. Since we already have the device-to-logical ID mapping for
the old part, say di → li for 0 ≤ i ≤ k, we explicitly express
such one-to-one mapping in the partitions. In other words, we have
(cid:5)p = ({d0};··· ;{dk}; Tp) and (cid:5)b = ({l0};··· ;{lk}; Tb), all the
nodes for the new part of Gp=Gb are in Tp=Tb respectively. Then,
we reﬁne (cid:5)p=(cid:5)b until they both are equitable. At last, we enter O2
mapping with the equitable partitions. In this way, we can produce
a device-to-logical ID mapping table for the new part of data center
while keeping the logical IDs for devices of the old part unmodiﬁed.
4. MALFUNCTION DETECTION AND HAN-
DLING
As introduced before, the malfunction detection module is trig-
gered when O2 returns false. This “false” indicates the physical topol-
ogy is not the same as the blueprint. In this section, we describe how
DAC handles malfunctions.
4.1 Malfunction Overview
Malfunctions can be caused by hardware and software failures, or
simply human conﬁguration errors. For example, bad or mismatched
network card and cables are common, and miswired or improperly
connected cables are nearly inevitable.
Structure
BCube(n; k)
FatTree(n)
VL(nr; np)
DCell(n; k)
Degrees of switches Degrees of servers
n
n
np; (nr + 2)
n
k + 1
1
1
k + 1
Table 1: Degree patterns in BCube, FatTree, VL2 and DCell
structures. n; k; nr; np are the parameters to deﬁne these net-
works, they are ﬁxed for a given structure.
We consider and categorize three malfunction types in data centers:
node, link and miswiring. The ﬁrst type occurs when a given server
or switch breaks down from hardware or software reasons, causing
it to be completely unreachable and disconnected from the network;
the second one occurs when the cable or network card is broken or
not properly plugged in so that the connectivity between devices on
that link is lost; the third one occurs when wired cables are different
from those in the blueprint. These malfunctions may introduce severe
problems and downgrade the performance.
Note that from the physical topology, it is unlikely to clearly dis-
tinguish some failure types, e.g., a crashed server versus completely
malfunctioning interface cards on that server. Our goal is to detect
and further locate all malfunction-related devices, and report the de-
vice information to network administrators, rather than identifying
the malfunction type. We believe our malfunction handling not only
solves this issue for autoconﬁguration, but also reduces the deploy-
ment/maintenance costs for real-world large data center deployment.
4.2 Problem Complexity and Challenge
The problem of malfunction detection can be formally described
as follows. Given Gb and Gp, the problem to locate all the malfunc-
tioning parts in the graph Gp is equivalent to obtaining the maximum
common subgraph (MCS) Gmcs of Gb and Gp. Thus, we compare
Gmcs with Gp to ﬁnd the differences, which are the malfunctioning
parts. All the devices (i.e., servers or switches) related to these parts,
which we call malfunctioning devices, can be detected. However,
it is proven that the MCS problem is NP-complete [22] and APX-
hard [23]. That is, there is no efﬁcient algorithm, especially for large
graphs such as those of data center network topologies. Therefore, we
resort to designing our own algorithms based on the particular proper-
ties of data center structures and our real-world application scenario.
There are two problems we need to address in the following subsec-
tions: 1) detecting the malfunctioning devices by identifying their de-
vice IDs; 2) locating the physical position of a malfunctioning device
with its device ID automatically.
4.3 Practical Malfunction Detection Methods
To achieve better performance and easier management, large-scale
data centers are usually designed and constructed according to some
patterns or rules. Such patterns or rules imply two properties of the