CPU Cycles
Retired
Instructions
Cache Misses
Branch Misses
Container
Collected Data
Host
Collected Data
Core Modeling
DRAM Modeling
Package Modeling
RAPL
Value
Host
Power
Calibration
Container
Power
(a) Data Collection 
(b) Power Modeling 
(c) On-the-ﬂy Calibration 
Fig. 5: The workﬂow of power-based namespace.
while a similar booting time demonstrates that they have a
high probability of being installed and turned on at the same
time period. This is strong evidence that they might also be in
close proximity and share the same circuit breaker. Attackers
can exploit this channel to aggregate their attack container
instances into adjacent physical servers. This greatly increases
their chances of tripping circuit breakers to cause power outages.
V. DEFENSE APPROACH
A. A Two-Stage Defense Mechanism
Intuitively, the solution should eliminate all the leakages
so that no leaked information could be retrieved through those
channels. We divide the defense mechanism into two stages to
close the loopholes: (1) masking the channels and (2) enhancing
the container’s resource isolation model.
In the ﬁrst stage, the system administrators can explicitly
deny the read access to the channels within the container, e.g.,
through security policies in AppArmor or mounting the pseudo
ﬁle “unreadable”. This does not require any change to the kernel
code (merging into the upstream Linux kernel might take some
time) and can immediately eliminate information leakages. This
solution depends on whether legitimate applications running
inside the container use these channels. If such information
is orthogonal to the containerized applications, masking it
will not have a negative impact on the container tenants. We
have reported our results to Docker and all the cloud vendors
listed in Table I, and we have received active responses. We
are working together with container cloud vendors to ﬁx this
information leakage problem and minimize the impact upon
applications hosted in containers. This masking approach is a
quick ﬁx, but it may add restrictions for the functionality of
containerized applications, which contradicts the container’s
concept of providing a generalized computation platform.
In the second stage, the defense approach involves ﬁxing
missing namespace context checks and virtualizing more system
resources (i.e., the implementation of new namespaces) to
enhance the container’s isolation model. We ﬁrst reported
information disclosure bugs related to existing namespaces
to Linux kernel maintainers, and they quickly released a new
patch for one of the problems ([CVE-2017-5967]). For the other
channels with no namespace isolation protection, we need to
change the kernel code to enforce a ﬁner-grained partition
of system resources. Such an approach could involve non-
trivial efforts since each channel needs to be ﬁxed separately.
Virtualizing a speciﬁc kernel component might affect multiple
kernel subsystems. In addition, some system resources are not
easy to be precisely partitioned to each container. However, we
consider this to be a fundamental solution to the problem.
In particular,
to defend against synergistic power attacks,
we design and implement a proof-of-concept power-based
namespace in the Linux kernel to present the partitioned power
usage to each container.
B. Power-based Namespace
We propose a power-based namespace to present per-
container power usage through the unchanged RAPL interface
to each container. Without leaking the system-wide power
consumption information, attackers cannot infer the power state
of the host, thus eliminating their chance of superimposing
power-intensive workloads on benign power peaks. Moreover,
with per-container power usage statistics at hand, we can
dynamically throttle the computing power (or increase the
usage fee) of containers that exceed their predeﬁned power
thresholds. It is possible for container cloud administrators to
design a ﬁner-grained billing model based on this power-based
namespace.
There are three goals for our design. (1) Accuracy: as there
is no hardware support for per-container power partitioning, our
software-based power modeling needs to reﬂect the accurate
power usage for each container. (2) Transparency: applications
inside a container should be unaware of the power variations
outside this namespace, and the interface of power subsystem
should remain unchanged. (3) Efﬁciency: power partitioning
should not incur non-trivial performance overhead in or out of
containers.
We illustrate the workﬂow of our system in Figure 5. Our
power-based namespace consists of three major components:
data collection, power modeling, and on-the-ﬂy calibration.
We maintain the same Intel RAPL interface within containers,
but change the implementation of handling read operations
on energy usages. Once a read operation of energy usage is
detected, the modiﬁed RAPL driver retrieves the per-container
performance data (data collection), uses the retrieved data to
model the energy usage (power modeling), and ﬁnally calibrates
the modeled energy usage (on-the-ﬂy calibration). We discuss
each component in detail below.
1) Data collection: In order to model per-container power
consumption, we need to obtain the ﬁne-grained performance
data for each container. Each container is associated with
a cpuacct cgroup. A cpuacct cgroup accounts for the CPU
cycles on a processor core for a container. The CPU cycles are
accumulated. We only use CPU cycles to compute the rate of
244
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
(cid:5)(cid:9)
(cid:5)(cid:4)
(cid:9)
(cid:3)
(cid:15)
(cid:2)
(cid:1)
(cid:36)
(cid:23)
(cid:31)
(cid:21)
(cid:27)
(cid:14)
(cid:1)
(cid:9)
(cid:8)(cid:4)(cid:10)
(cid:8)
(cid:7)(cid:4)(cid:10)
(cid:7)
(cid:6)(cid:4)(cid:10)
(cid:6)
(cid:3)
(cid:15)
(cid:2)
(cid:1)
(cid:37)
(cid:23)
(cid:32)
(cid:21)
(cid:28)
(cid:14)
(cid:29)(cid:31)(cid:24)(cid:26)(cid:21)
(cid:32)(cid:33)(cid:31)(cid:21)(cid:32)(cid:32)(cid:239)(cid:8)(cid:16)
(cid:25)(cid:28)(cid:28)(cid:29)
(cid:32)(cid:33)(cid:31)(cid:21)(cid:32)(cid:32)(cid:239)(cid:7)(cid:6)(cid:16)
(cid:32)(cid:33)(cid:31)(cid:21)(cid:32)(cid:32)(cid:239)(cid:5)(cid:6)(cid:12)(cid:16)
(cid:25)(cid:24)(cid:19)(cid:30)(cid:34)(cid:18)(cid:27)(cid:33)(cid:34)(cid:26)
(cid:10)
(cid:11)
(cid:4)
(cid:1)
(cid:9)
(cid:8)
(cid:7)
(cid:6)
(cid:5)
(cid:17)(cid:34)(cid:26)(cid:19)(cid:21)(cid:31)(cid:1)(cid:28)(cid:22)(cid:1)(cid:24)(cid:27)(cid:32)(cid:33)(cid:31)(cid:34)(cid:20)(cid:33)(cid:24)(cid:28)(cid:27)(cid:32)
(cid:12)
(cid:35)(cid:1)(cid:5)(cid:4)(cid:13)
Fig. 6: Power modeling: the
relation between core energy
and the number of retired in-
structions.
Almost no 
cache misses
(cid:30)(cid:32)(cid:25)(cid:27)(cid:21)
(cid:33)(cid:34)(cid:32)(cid:21)(cid:33)(cid:33)(cid:239)(cid:9)(cid:16)
(cid:26)(cid:29)(cid:29)(cid:30)
(cid:33)(cid:34)(cid:32)(cid:21)(cid:33)(cid:33)(cid:239)(cid:8)(cid:7)(cid:16)
(cid:33)(cid:34)(cid:32)(cid:21)(cid:33)(cid:33)(cid:239)(cid:6)(cid:7)(cid:11)(cid:16)
(cid:26)(cid:25)(cid:19)(cid:31)(cid:35)(cid:18)(cid:28)(cid:34)(cid:35)(cid:27)
(cid:5)(cid:4)(cid:10)
(cid:1)
(cid:5)
(cid:8)
(cid:7)
(cid:6)
(cid:9)
(cid:17)(cid:35)(cid:27)(cid:19)(cid:21)(cid:32)(cid:1)(cid:29)(cid:22)(cid:1)(cid:13)(cid:18)(cid:20)(cid:24)(cid:21)(cid:1)(cid:16)(cid:25)(cid:33)(cid:33)(cid:21)(cid:33)
(cid:10)
(cid:36)(cid:1)(cid:6)(cid:5)(cid:12)
Fig. 7: Power modeling: the
relation between DRAM en-
ergy and the number of cache
misses.
the cache miss rate and branch miss rate later. The Linux kernel
also has a perf event subsystem, which supports accounting
for different types of performance events. The granularity of
performance accounting could be a single process or a group
of processes (considered as a perf event cgroup). By now, we
only retrieve the data for retired instructions, cache misses,
and branch misses (which are needed in the following power
modeling component) for each perf event cgroup. Our current
implementation is extensible to collect more performance event
types corresponding to the changes of power modeling in the
future.
We monitor the performance events from the initialization
of a power-based namespace and create multiple perf events,
each associated with a speciﬁc performance event type and
a speciﬁc CPU core. Then we connect the perf cgroup of
this container with these perf events to start accounting. In
addition, we need to set the owner of all created perf events
as TASK TOMBSTONE,
indicating that such performance
accounting is decoupled from any user process.
2) Power modeling: To implement a power-based names-
pace, we need to attribute the power consumption to each
container. Instead of providing transient power consumption,
RAPL offers accumulated energy usages for package, core, and
DRAM, respectively. The power consumption can be calculated
by measuring the energy consumption over a time unit window.
Our power-based namespace also provides accumulative per-
container energy data, in the same format as in the original
RAPL interface.
We ﬁrst attribute the power consumption for the core.
Traditional power modeling leverages CPU utilization [29] to
attribute the power consumption for cores. However, Xu et al.
[43] demonstrated that the power consumption could vary signif-
icantly with the same CPU utilization. The underlying pipeline
and data dependence could lead to CPU stalls and idling of
function units. The actual numbers of retired instructions [24],
[33] under the same CPU utilization are different. Figure 6
reveals the relation between retired instructions and energy.
We test on four different benchmarks: the idle loop written in
C, prime, 462.libquantum in SPECCPU2006, and stress with
different memory conﬁgurations. We run the benchmarks on
a host and use Perf [6] to collect performance statistics data.
We can see that for each benchmark, the energy consumption
is almost strictly linear to the number of retired instructions.
However, the gradients of ﬁtted lines change correspondingly
(cid:1)
with application types. To make our model more accurate, we
further include the cache miss rate [24] and branch miss rate
to build a multi-degree polynomial model to ﬁt the slope.
For the DRAM, we use the number of cache misses to
proﬁle the energy. Figure 7 presents the energy consumption
for the same benchmarks with the same conﬁgurations in the
core experiment. It clearly indicates that the number of cache
misses is approximately linear to the DRAM energy. Based on
this, we use the linear regression of cache misses to model the
DRAM energy.
For the power consumption of package, we sum the values
of core, DRAM, and an extra constant. The speciﬁc models
are illustrated in Formula (2), where M represents the modeled
energy; CM, BM, C indicate the number of cache misses,
branch misses, and CPU cycles, respectively; and F is the
function derived through multiple linear regressions to ﬁt the
slope. I is the number of retired instructions. α, β, γ, and λ
are the constants derived from the experiment data in Figures 6
and 7.
) · I + α,
CM
C
BM
C
,
Mcore = F(
Mdram = β · CM + γ,
Mpackage = Mcore + Mdram + λ.
(2)
Here we discuss the inﬂuence of ﬂoating point instructions
for power modeling. While an individual ﬂoating point instruc-
tion might consume more energy than an integer operation,
workloads with high ratios of ﬂoating point instructions might
actually result in lower power consumption overall, since the
functional units might be forced to be idle in different stages of
the pipeline. It is necessary to take the micro-architecture into
consideration to build a more reﬁned model. We plan to pursue
this direction in our future work. Furthermore, the choices of
parameters α, β, γ are also affected by the architecture. Such
a problem could be mitigated in the following calibration step.
3) On-the-ﬂy calibration: Our system also models the
energy data for the host and cross-validates it with the actual
energy data acquired through RAPL. To minimize the error of
modeling data, we use the following Formula (3) to calibrate
the modeled energy data for each container. The Econtainer
represents the energy value returned to each container. This
on-the-ﬂy calibration is conducted for each read operation to
the RAPL interface and can effectively reduce the number of
errors in the previous step.
Econtainer =
Mcontainer
Mhost
· ERAPL.
VI. DEFENSE EVALUATION
(3)
In this section, we evaluate our power-based namespace
on a local machine in three aspects: accuracy, security, and
performance. Our testbed is equipped with Intel
i7-6700
3.40GHz CPU with 8 cores, 16GB of RAM, and running
Ubuntu Linux 16.04 with kernel version 4.7.0.
A. Accuracy
We use the SPECCPU2006 benchmark to measure the
accuracy of the power modeling. We compare the modeled
245
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: PERFORMANCE RESULTS OF UNIX BENCHMARKS.
1 Parallel Copy
8 Parallel Copies
Original Modiﬁed Overhead Original Modiﬁed Overhead
3,759.2
0.78% 19,132.9 19,149.2 0.08%
3,788.9
918.0
0.15%
0.94% 6,630.7
926.8
6,620.6
271.9
7,298.1
6.53% 7,975.2
290.9
8.49%
3,469.3
2,659.7 14.33%
0.73% 3,104.9
3,495.1
2,175.1
1,622.2 18.19%
0.04% 1,982.9
2,208.5
5,829.9
5,822.7 12.32%
-2.34% 6,641.3
5,695.1
1,878.4
1,899.4
1.1% 9,507.2
9,491.1
0.16%
251.2
1.63%
5,180.7
61.53% 5,266.7
653.0
8.95% 6618.5
1289.7
1416.5
6063.8
8.38%
3.07% 16,909.7 16,404.2 2.98%
3,660.4
3,548.0
3.2% 15,721.1 15,589.2 0.83%
11,621.0 11,249.1
0.72%
1.17% 5,689.4
1,212.2
1,226.6
2,000.8
1,807.4
9.66% 7,239.8
7.03%
5,648.1
6,813.5
Benchmarks
Dhrystone 2 using register variables
Double-Precision Whetstone
Execl Throughput
File Copy 1024 bufsize 2000 maxblocks