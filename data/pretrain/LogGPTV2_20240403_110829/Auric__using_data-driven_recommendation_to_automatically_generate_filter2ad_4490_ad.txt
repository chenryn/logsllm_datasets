learning the model. This is because a lot of our attributes and even
configuration parameters take categorical values. So, if a vector ùë•
across all carriers takes value ùëé, ùëè, and ùëê, then one-hot encoding will
create three vectors ùë• = ùëé, ùë• = ùëè, and ùë• = ùëê, and the carrier with
value ùëè, will have three values as 0, 1, 0. The sum of the one-hot
numeric array for a particular carrier should be equal to 1. One-hot
encoding increases the number of variables in our classification
problem.
In our global learner, we learn the model once and determine
the prediction accuracy for the parameter of interest using all the
remaining carriers. Accuracy can be intuitively viewed as the num-
ber of predictee configuration parameters whose recommendation,
using our learner, matches the current configuration parameter
value of the targeted new carrier of interest, divided by the total
number of configuration parameters. We determine accuracy in-
dependently for each predictee variable ùëå ùëñ across all carriers and
their neighbors (for pair-wise type of parameters).
For our local learner, we geographically scope the remaining
carriers to be within the 1-hop neighborhood based on X2 neighbor
relations. In our second part of evaluation, we compare the global
learner accuracy with that of the local learner for each of the four
markets. We then expand our data set to the entire 400K+ carriers,
28 markets and 15M+ configuration parameter values and generate
the configuration recommendations and quantify the accuracy for
both global and local learners.
In our last part of evaluation through validation with engineers,
our intent is to understand how does Auric perform across a very
large number of carriers and their parameters. We take a few sample
output and share those with the engineering teams across multiple
markets to evaluate our recommendations that do not match the cur-
rent configuration parameter values (we call these as mis-matches).
Our goal is to discover if there are opportunities in improving our
algorithms, carrier attributes, or network configurations.
4.3 Results
Our key takeaways from evaluation are: (a) collaborative filtering
with chi-square test of independence and voting outperforms other
global learners, (b) local learner with geographical proximity out-
performs the global learner using collaborative filtering - this is
because of how carrier configuration tuning is done by the engi-
neers that have more local dependency with geographically nearby
neighbors as opposed to far-away carriers, (c) a significant fraction
815
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
of the mis-matches (28%) result in improving the network configura-
tion, thereby increasing our confidence in Auric‚Äôs local learner, and
(d) a small fraction of mis-matches indicate that we have to incorpo-
rate more carrier attributes that are representative of terrain type
(e.g., facing mountain or tall buildings), signal propagations, or even
recent configuration changes certified for a network-wide roll-out.
This brings in new opportunities for potential future research.
4.3.1 Accuracy of global learners. Figs. 10a-10d show the configu-
ration parameter prediction accuracy across five global learners for
the four markets as described in Table 3. The X-axis show the 65
configuration parameters, Y-axis on the left shows the prediction
or recommendation accuracy for five global learners and the sec-
ondary Y-axis shows the number of distinct values (or, variability)
for each configuration parameter. We reverse sorted the list based
on the distinct values. For all five global learners, the accuracy goes
down when the variability (or the number of distinct values) goes
up. This is shown towards the left half of the plots in Figs. 10a-10d.
This indicates that configuration parameters with high variability
become harder to recommend or predict. Configuration parame-
ters with very low variability have similar accuracy for all global
learners. Finally, we observe a correlation between learners across
different configuration parameters. This implies that if prediction
is hard for one, it is no different for other global learners.
Table 4 summarizes the average accuracy across all configuration
parameters for each global learner and across all four markets. Col-
laborative filtering with voting has a prediction accuracy of 95.48%
and outperforms the other four learners because it can effectively
handle the high variability and skewness of the configuration pa-
rameter values. Random forest learner performs slightly better than
the rest of the three learners. Overall, a 90%+ accuracy is very en-
couraging for recommendation from the configuration perspective
across all five learners.
Importance of geographical proximity. We now select collab-
4.3.2
orative filtering as our base global learner and compare it to its
corresponding local learner with proximity defined using 1-hop X2
LTE neighbor relations. Collaborative filtering with local voting
achieves a better prediction accuracy of 96.14% compared to 95.48%
using collaborative filtering with global voting. We also generate
prediction results by repeating the same process like above but now
expanding from 4 markets to 28 markets with a total of 400K+ carri-
ers and 15M+ configuration parameters. Collaborative filtering with
local voting achieves a better prediction accuracy of 96.9% averaged
across 28 markets as compared to 96.5% using collaborative filtering
with global voting. A 0.4% improvement across 15M+ parameters
accounts for 60K parameters that improve in prediction because of
geographical proximity. Thus, this highlights its importance in the
generation of carrier configuration.
Next, we present results for the geographical proximity based
recommendation for 4 out of the 65 configuration parameters that
have the highest variability in Figs. 11a- 11d. The X-axis represents
the 28 markets. The Y-axis on the left shows the prediction accuracy
and the secondary Y-axis on the right captures the distinct value for
configuration parameters in each market. One finding across the
four charts is that different markets have different levels of variabil-
ity in the parameters and thus the accuracy vary accordingly. Some
markets (e.g., markets 6 and 7 in Fig.11a) have a lower prediction
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Ajay Mahimkar, Ashiwan Sivakumar, Zihui Ge, Shomik Pathak, Karunasish Biswas
Table 4: Average accuracy for five global learners across four markets and across all configuration parameters.
Market 1
Market 2
Market 3
Market 4
All four
Random forest
92.58
89.27
91.43
95.15
92.11
k-Nearest neighbors
91.58
88.08
90.71
94.34
91.18
Decision tree
91.93
88.73
91.14
94.79
91.68
Deep neural network
91.94
88.39
90.98
94.57
91.7
Collaborative filtering
95.94
93.75
95.58
96.63
95.48
(a) Market 1.
(a) Configuration parameter 1.
(b) Market 2.
(c) Market 3.
(d) Market 4.
(b) Configuration parameter 2.
(c) Configuration parameter 3.
Figure 10: Prediction accuracy of five global learners for four
markets.
accuracy even though their variability is similar to others. This
implies that those markets could have some other attributes that
we might be missing in our learners. This shows the challenge of
tuning carrier configuration across different markets.
4.3.3 Accuracy validation with engineers. Approximately 96% is a
good accuracy demonstration for geographical proximity based rec-
ommendation of configuration across 15M+ parameters. However,
we were curious to understand why we got the recommendation not
to match with the current configuration for the rest 4%. We shared
these results with a few engineers across some of the markets and
(d) Configuration parameter 4.
Figure 11: Prediction accuracy of local learner for four con-
figuration parameters across all markets.
they were able to provide a labeling of the mismatch output. This
analysis and labeling was conducted over multiple days to derive
conclusions. We put the labels into three categories: (a) mismatches
require update to our learner and/or our list of carrier attributes,
(b) mismatches were indeed good recommendations that led to
configuration changes to the network, and (c) need more time for
investigation and thus currently labeled as inconclusive.
816
Auric: Using Data-driven Recommendation to
Automatically Generate Cellular Configuration
Figure 12: Labeling by the market engineers for the mis-
matches between the configuration parameter recommenda-
tion and the current values in the network.
Fig. 12 shows a pie-chart of the different labels. Out of a total of
sampled 54,915 mis-matches, engineers labeled 3075 (5%) as update
learner because they were not willing to change the current con-
figuration parameter value. There were two reasons: (i) we were
missing carrier attributes that were important for recommending
the configuration parameter - e.g., terrain type and signal propaga-
tion; (ii) recent configuration changes on those carriers were part of
an ongoing trial and certification of the network-wide roll-out and
thus, they were not in the majority based on our voting approach.
Both these learnings open up new opportunities to enhance our
learner to bring in the temporal aspect of the configuration parame-
ter changes, and derive carrier attributes from other measurements.
As an example, by looking at user traffic and handover pattern, one
could potentially derive that the carrier is facing a freeway, or time-
of-day traffic behavior is indicative of a business or a residential
location. Engineers labeled 15,241 (28%) as good recommendations
and asked us to push those as configuration changes into the net-
work. The reason for this mismatch was because the network had
undergone some trials in the past and were left in a sub-optimal
configuration.
The remaining 67% are under investigation at the time of writ-
ing this paper. After interactions with the network engineers, we
learned that based on their experience and domain knowledge, they
were not sure if the parameter change recommendations would
result in any service performance improvements. Thus, they would
first need to be trialed in some parts of the networks to observe
their impacts, and then decisions can be made to roll it out across
the whole network. We leave this as part of our ongoing work and
plan to incorporate performance metrics as feedback to improve the
learning and recommendation process. Hence, we currently labeled
the 67% as inconclusive that warrant further experimentation and
trial in the network.
Implications of inaccurate recommendations. After new carri-
ers are added in cellular networks, the engineers carefully monitor
the traffic distribution on the newly added carrier compared to
neighboring existing carriers, and the service performance impact
of the change (e.g., data throughput, voice call admissions). If they
observe any unexpected performance impacts, they would imme-
diately roll-back the configuration of the new carrier to its origi-
nal default. Any inaccurate recommendation from Auric is treated
similarly. In the future, we plan to automate the performance im-
pact monitoring of recommended changes and any subsequent
roll-backs.
817
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
5 OPERATIONAL EXPERIENCES
Based on our evaluation on a very large number of carriers and
growing confidence with engineers, we deployed Auric in pro-
duction environments to automatically generate configuration for
newly added carriers. Auric is part of an end-to-end artificial intel-
ligence based automation solution called SmartLaunch within the
service provider to automatically launch new carriers. External ven-
dors are contracted to do the physical work of integrating the new
carrier into the network and its initial software configuration to set
the parameter values. Once the carrier is ready for launch, the next
step is to perform necessary pre-checks and then unlock the carrier,
followed by monitoring alarms and key performance indicators as
part of post-checks. These steps are automated within SmartLaunch
and tremendously help the engineers to quickly launch the new
carrier and have it carry user traffic.
We integrated Auric based auto generation and implementation
of cellular configuration before unlocking new carriers. The
change implementation before unlocking the carrier assists in cases
where changing a parameter value requires the carrier to be locked
(i.e., taken off-air). Locking a carrier (which is equivalent to a re-
boot operation) after it is launched for implementing configuration
changes has the risk of a minor disruption to traffic and service
performance. Thus, in our current implementation, we conserva-
tively avoid configuring such carriers to prevent any potential
service disruption, and prefer pushing the configuration changes
before unlocking the new carriers. Implementing the configura-
tions recommended by Auric in production networks involves chal-
lenges akin to router configuration. Analogous to vendor-specific
router configuration languages, cellular equipment vendors pro-
vide a configuration schema where the configuration parameters
are organized in the form of a hierarchical structure called man-
aged objects (similar to interfaces in routers). Further, equipment
vendors provide an element management system (EMS) with inter-
faces at varying levels of sophistication (e.g. CLIs, APIs) for users
to interact with the hardware. We implemented a controller that
pushes the configuration recommendation from Auric into the net-
work. Our controller compares the recommendations from Auric
to the current configuration generated by the vendors and pushes
only the mismatches. Mismatches can occur because of mistakes
by vendors, out-of-date rulebooks, or pending tuning to be con-
ducted to improve service performance. We allow an engineer to
validate the mismatches before they are pushed into the new carri-
ers. As the engineers gain confidence in Auric‚Äôs recommendations,
the manual validation of mismatches becomes optional as part of
the SmartLaunch automated solution. The controller maintains a
vendor-specific template and automates the task of generating the
configuration file by filling in the instance IDs from a database. The
final configuration file is pushed through the EMS into the base
station hardware.
Table 5: Auric operational experiences with new carrier
launches. We intentionally show the results in part of the
network for proprietary reasons.
New carriers launched
Changes recommended by Auric
Changes implemented successfully
1251
143 (11.4%)
114 (9%)
We report on two months of experience of running Auric for
very large operational LTE networks. Table 5 shows the number
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Ajay Mahimkar, Ashiwan Sivakumar, Zihui Ge, Shomik Pathak, Karunasish Biswas
of carriers launched over the two month intervals. These results
are only for the part of the network where engineers have adopted
Auric. As you can see, out of 1251 new carrier launches, Auric
recommends changes on around 143 carriers which is around 11.4%.
Changes were successfully pushed on 114 out of 143 approved
carriers. A total of 1102 configuration parameters were changed on
those 114 carriers. We observed a small number of fall-outs (29) and
the reasons were two fold: (a) Some engineers were prematurely
unlocking the carriers through off-band interfaces and hence our
current implementation would avoid making configuration changes
for these carriers. This was intentional because we wanted to avoid
any service disruption once a carrier is unlocked (i.e., live and
carrying traffic). In the future, we plan to address this by carefully
pushing such configuration changes to a non-busy maintenance
time window. (b) The configuration change implementation for
some of the carriers resulted in timeouts because of the very large
number of parameters and our setup based on EMS restrictions
limited us in how many concurrent executions of parameters were
supported. At the time of writing this paper, we are working with