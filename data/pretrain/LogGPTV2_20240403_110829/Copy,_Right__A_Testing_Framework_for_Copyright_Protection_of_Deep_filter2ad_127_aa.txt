title:Copy, Right? A Testing Framework for Copyright Protection of Deep
Learning Models
author:Jialuo Chen and
Jingyi Wang and
Tinglan Peng and
Youcheng Sun and
Peng Cheng and
Shouling Ji and
Xingjun Ma and
Bo Li and
Dawn Song
2022 IEEE Symposium on Security and Privacy (SP)
Copy, Right? A Testing Framework for Copyright
Protection of Deep Learning Models
Jialuo Chen∗, Jingyi Wang∗(cid:2), Tinglan Peng∗, Youcheng Sun†, Peng Cheng∗, Shouling Ji∗,
∗
Xingjun Ma‡, Bo Li§ and Dawn Song¶
†
§
Fudan University,
University of Manchester,
‡
¶
Zhejiang University,
∗{chenjialuo, wangjyee, tlpeng zju, lunarheart, PI:EMAIL},
§{PI:EMAIL},
‡{PI:EMAIL},
UIUC,
†{PI:EMAIL},
¶{PI:EMAIL}
UC Berkeley
7
4
7
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
Abstract—Deep learning models, especially those large-scale
and high-performance ones, can be very costly to train, demand-
ing a considerable amount of data and computational resources.
As a result, deep learning models have become one of the most
valuable assets in modern artiﬁcial intelligence. Unauthorized
duplication or reproduction of deep learning models can lead to
copyright infringement and cause huge economic losses to model
owners, calling for effective copyright protection techniques. Ex-
isting protection techniques are mostly based on watermarking,
which embeds an owner-speciﬁed watermark into the model.
While being able to provide exact ownership veriﬁcation, these
techniques are 1) invasive, i.e., they need to tamper with the
training process, which may affect the model utility or introduce
new security risks into the model; 2) prone to adaptive attacks
that attempt to remove/replace the watermark or adversarially
block the retrieval of the watermark; and 3) not robust to the
emerging model extraction attacks. Latest ﬁngerprinting work
on deep learning models, though being non-invasive, also falls
short when facing the diverse and ever-growing attack scenarios.
testing framework for
deep learning copyright protection: DEEPJUDGE. DEEPJUDGE
quantitatively tests the similarities between two deep learning
models: a victim model and a suspect model. It leverages a
diverse set of testing metrics and efﬁcient test case generation
algorithms to produce a chain of supporting evidence to help
determine whether a suspect model is a copy of the victim model.
Advantages of DEEPJUDGE include: 1) non-invasive, as it works
directly on the model and does not tamper with the training
process; 2) efﬁcient, as it only needs a small set of seed test
cases and a quick scan of the two models; 3) ﬂexible, i.e., it can
easily incorporate new testing metrics or test case generation
methods to obtain more conﬁdent and robust judgement; and
4) fairly robust to model extraction attacks and adaptive attacks.
We verify the effectiveness of DEEPJUDGE under three typical
copyright infringement scenarios,
including model ﬁnetuning,
pruning and extraction, via extensive experiments on both image
classiﬁcation and speech recognition datasets with a variety of
model architectures.
In this paper, we propose a novel
2
2
0
2
I. INTRODUCTION
Deep learning models, e.g., deep neural networks (DNNs),
have become the standard models for solving many com-
plex real-world problems, such as image recognition [18],
speech recognition [15], natural language processing [7], and
autonomous driving [5]. However, training large-scale DNN
models is by no means trivial, which requires not only large-
scale datasets but also signiﬁcant computational resources. The
training cost can grow rapidly with task complexity and model
capacity. For instance, it can cost $1.6 million to train a BERT
model on Wikipedia and Book corpora (15 GB) [37]. It is
thus of utmost importance to protect DNNs from unauthorized
duplication or reproduction.
One concerning fact is that well-trained DNNs are often
exposed to the public via remote services (APIs), cloud
platforms (e.g., Amazon AWS, Google Cloud and Microsoft
Azure), or open-source toolkits like OpenVINO1. It gives rise
to adversaries (e.g., a model “thief”) who attempt to steal the
model in stealthy ways, causing copyright infringement and
economic losses to the model owners. Recent studies have
shown that stealing a DNN can be done very efﬁciently with-
out leaving obvious traces [38], [33]. Arguably, unauthorized
ﬁnetuning or pruning is the most straightforward way of model
stealing, if the model parameters are publicly accessible (for
research purposes only) or the adversary is an insider. Even
when only the API is exposed, the adversary can still exploit
advanced model extraction techniques [38], [33], [32], [21],
[45] to steal most functionalities of the hidden model. These
attacks pose serious threats to the copyright of deep learning
models, calling for effective protection methods.
A number of defense techniques have been proposed to pro-
tect the copyright of DNNs, where DNN watermarking [40],
[47], [1], [9] is one major type of technique. DNN watermark-
ing embeds a secret watermark (e.g., logo or signature) into
the model by exploiting the over-parameterization property of
DNNs [1]. The ownership can then be veriﬁed when the same
or similar watermark is extracted from a suspect model. The
use of watermarks has an obvious advantage, i.e., the owner
identity can be embedded and veriﬁed exactly, given that the
watermark can be fully extracted. However, these methods still
suffer from certain weaknesses. Arguably, the most concerning
one is that they are invasive, i.e., they need to tamper with
the training procedure to embed the watermark, which may
compromise model utility or introduce new security threats
into the model [25], [41], [10], [17].
More recently, DNN ﬁngerprinting [2], [27] has been pro-
posed as a non-invasive alternative to watermarking. Lying at
the design core of ﬁngerprinting is uniqueness — the unique
feature of a DNN model. Speciﬁcally, ﬁngerprinting extracts
a unique identiﬁer (or ﬁngerprint) from the owner model
to differentiate it from other models. The ownership can be
claimed if the identiﬁer of the owner model matches with that
(cid:2)Corresponding author.
1https://github.com/openvinotoolkit/open model zoo
© 2022, Jialuo Chen. Under license to IEEE.
DOI 10.1109/SP46214.2022.00059
824
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
of a suspect model. However, in the context of deep learning,
a single ﬁngerprinting feature/metric can hardly be sufﬁcient
or ﬂexible enough to handle all the randomness in DNNs or
against different types of model stealing and adaptive attacks
(as we will show in our experiments). In other words, there
exist many scenarios where a DNN model can easily lose its
unique feature or property (i.e., ﬁngerprint).
In this work, we propose a testing approach for DNN
copyright protection. Instead of solely relying on one metric,
we propose to actively test the “similarities” between a victim
model and a suspect model from multiple angles. The core
idea is to 1) carefully construct a set of test cases to compre-
hensively characterize the victim model, and 2) measure how
similarly the two models behave on the test cases. Intuitively,
if a suspect model is a stolen copy of the victim model, it will
behave just like the victim model in certain ways. An extreme
case is that the suspect is the exact duplicate of the victim
model, and in this case, the two models will behave identically
on these test cases. This testing view creates a dilemma for
the adversary as better stealing will inevitably lead to higher
similarities to the victim model. We further identify two major
challenges for testing-based copyright protection: 1) how to
deﬁne comprehensive testing metrics to fully characterize the
similarities between two models, and 2) how to effectively
generate test cases to amplify the similarities. The set of
similarity scores can be viewed as a proof obligation that
provides a chain of strong evidence to judge a stolen copy.
Following the above idea, we design and implement DEEP-
JUDGE, a novel testing framework for DNN copyright pro-
tection. As illustrated in Fig. 1, DEEPJUDGE is composed
of three core components. First, we propose a set of multi-
level testing metrics to fully characterize a DNN model from
different angles. Second, we propose efﬁcient test case gener-
ation algorithms to magnify the similarities (or differences)
measured by the testing metrics between the two models.
Finally, a ‘yes’/‘no’ (stolen copy) judgment will be made for
the suspect model based on all similarity scores.
The advantages of DEEPJUDGE include 1) non-invasive: it
works directly on the trained models and does not tamper with
the training process; 2) efﬁcient: it can be done very efﬁciently
with only a few seed examples and a quick scan of the models;
3) ﬂexible: it can easily incorporate new testing metrics or
test case generation methods to obtain more evidence and
reliable judgement, and can be applied in both white-box
testing metrics; 4)
and black-box scenarios with different
robust: it is fairly robust to adaptive attacks such as model
extraction and defense-aware attacks. The above advantages
make DEEPJUDGE a practical, ﬂexible, and extensible tool
for copyright protection of deep learning models.
We have implemented DEEPJUDGE as an open-source self-
contained toolkit and evaluated DEEPJUDGE on four bench-
mark datasets (i.e., MNIST, CIFAR-10, ImageNet and Speech
Commands) with different DNN architectures, including both
convolutional and recurrent neural networks. The results con-
ﬁrm the effectiveness of DEEPJUDGE in providing strong
evidence for identifying the stolen copies of a victim model.
DEEPJUDGE is also proven to be more robust to a set of
adaptive attacks compared to existing defense techniques.
In summary, our main contributions are:
• We propose a novel testing framework DEEPJUDGE for
copyright protection of deep learning models. DEEP-
JUDGE determines whether one model is a copy of the
other depending on the similarity scores obtained from
a comprehensive set of testing metrics and test case
generation algorithms.
• We identify three typical scenarios of model copying
including ﬁnetuned copy, pruned copy, and extracted
copy; deﬁne positive and negative suspect models for
each scenario; and consider both white-box and black-
box protection settings. DEEPJUDGE can produce reliable
evidence and judgement to correctly identify the positive
suspects across all scenarios and settings.
• DEEPJUDGE is a self-contained open-source tool for
robust copyright protection of deep learning models and
a strong complement to existing techniques. DEEPJUDGE
can be ﬂexibly applied in different DNN copyright pro-
tection scenarios and is extensible to new testing metrics
and test case generation algorithms.
II. BACKGROUND
A. Deep Neural Network
A DNN classiﬁer is a decision function f : X → Y
mapping an input x ∈ X to a label y ∈ Y = {1, 2,··· , C},
where C is the total number of classes. It comprises of L
layers: {f 1, f 2,··· , f L−1, f L}, where f 1 is the input layer,
f L is the probability output layer, and f 2,··· , f L−1 are the
hidden layers. Each layer f l can be denoted by a collection of
neurons: {nl,1, nl,2,··· , nl,Nl
}, where Nl is the total number
of neurons at that layer. Each neuron is a computing unit that
computes its output by applying a linear transformation fol-
lowed by a non-linear operation to its input (i.e., output from
the precedent layer). We use φl,i(x) to denote the function
that returns the output of neuron nl,i for a given input x ∈ X.
Then, we have the output vector of layer f l (2 ≤ l ≤ L):
f l(x) = (cid:5)φl,1(x), φl,2(x),··· , φl,Nl (x)(cid:6). Finally, the output
label f (x) is computed as f (x) = arg max f L(x).
B. DNN Watermarking
A number of watermarking techniques have been proposed
to protect the copyright of DNN models [1], [9], [23], [40],
[47], [20]. Similar to traditional multimedia watermarking,
DNN watermarking works in two steps: embedding and
veriﬁcation. In the embedding step,
the owner embeds a
secret watermark (e.g., a signature or a trigger set) into the
model during the training process. Depending on how much
knowledge of the model is available in the veriﬁcation step,
existing watermarking methods can be broadly categorized
into two classes: a) white-box methods for the case when
model parameters are available; and b) black-box methods
when only predictions of the model can be acquired.
White-box watermarking embeds a pre-designed signature
(e.g., a string of bits) into the parameter space of the model
via certain regularization terms [9], [40]. The ownership could
be claimed when the extracted signature from a suspect model
is similar to that of the owner model. Black-box watermarking
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
825
Fig. 1: The overview of DEEPJUDGE Testing Framework.
usually leverages backdoor attacks [16] to implant a watermark
pattern into the owner model by training the model with a set