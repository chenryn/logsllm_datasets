raw data, and publishes this as the count for every column. ALPA
can be viewed as GS with only one group, and it publishes the
same count for every column. It reduces the sensitivity from d to 1
so that less noise is injected. However, the error for columns with
high counts would be much larger than for other columns.
3.3 Utility Metrics
In [15], the utility goal is deﬁned as minimizing the Mean Ab-
solute Error (MAE) and Mean Relative Error (MRE). Let nc de-
note the noisy column count vector, and tc denote the true column
As pointed out in [20], in some cases the quality function satis-
ﬁes the condition that when the input dataset is changed from D to
D(cid:48), the quality values of all outcomes change only in one direction,
i.e.,
∀D(cid:39)D(cid:48)(cid:2)(cid:0)∃r1 q(D, r1)  θ
The sampling method for limiting sensitivity is to randomly sam-
ple θ non-zero cells from all non-zero cells in a row for rows with
RCi > θ, and keep all cells for other rows. The transformation
from sampling produces D|θ similar to the one from normaliza-
tion since the sum of a row is limited by θ. We have found that
Table 1: Notations used in this paper.
Notation
D / D
Description
Input database / dataset
Number of rows in database / dataset
Number of columns in database / dataset
Query for the vector of all column counts
Threshold for sensitivity control
Scaling factor for sensitivity control
Sensitivity-limited dataset by θ
Row counts for row i
Column j’s count in D / D|θ
Average column count of D|θ
Average column count error of D|θ
Quality function for algorithm 1
qs(D, θ, α, ) Quality function for algorithm 2
n
d
Q
θ / θ∗
α/α∗
D|θ
RCi
cj / cθ
ac(D|θ)
j
ae(D|θ)
q(D, θ, )
c
p
tc
nc
Privacy budget for choosing the best sensitivity
Privacy budget for publishing noisy counts
in data for scaling
True column count vector in dataset
Noisy column count vector to be published
both methods produce a very similar effect for utility preservation.
Normalization is more general in that it can be easily extended to
non-binary datasets or non-integer θ. In the remaining of the paper,
we use the normalization method for limiting the sensitivity.
Our approach is to ﬁrst choose θ in a differentially private way
using privacy budget c, and then publish Q(D|θ) using privacy
budget p such that c + p = . For any input dataset D, publish-
ing Q(D|θ) has sensitivity θ; thus we can then publish Q(D|θ) +
denotes a vector of indepen-
Lap
(cid:16) θ
(cid:16) θ
(cid:17)(cid:69)
(cid:17)(cid:69)
, where
(cid:68)
(cid:68)
Lap
p
p
dently generated Laplacian noises.
The key technical challenge is to choose the threshold θ in a
differentially private way. In Section 4.1, we discuss how we do
this. We present the DPSense algorithm in Section 4.2, and the
DPSense-S algorithm in Section 4.3. We discuss the extension
of our proposed approaches for non-binary datasets in Section 4.4.
For convenience, we list all the notations used in this paper in Ta-
ble 1.
4.1 Choosing The Best Sensitivity Level
There are two sources of errors when we output Q(D|θ)+
Lap
Truncation errors are caused by the normalization step, which causes
Q(D|θ) to be smaller than Q(D). Noise errors are caused by the
addition of Laplacian noise. Increasing θ reduces truncation errors
while at the same time increasing noise errors. The goal is thus to
choose a θ that minimizes the aggregated effect of truncation errors
and noise errors.
(cid:68)
The optimal choice of θ depends both on the dataset and on the
available privacy budget. Clearly the optimal choice of θ will be
correlated with the average row count. If θ is smaller than the aver-
age row count, then the normalization is likely to cause signiﬁcant
underestimation. At the same time, the optimal choice of θ also
depends on the distribution of row counts. If all row counts are the
same, then setting θ to be the average row count reduces truncation
error to 0. However, if there are many rows with very low counts,
and many with high counts, then setting θ to be the average row
count may result in very large truncation errors. Thus a method of
choosing θ using only the average row count may be suboptimal.
The optimal choice of θ also depends on the privacy budget .
The larger the privacy budget is, the smaller the magnitude of the
(cid:16) θ
p
(cid:17)(cid:69)
.
454noise, and thus it is desirable to choose a larger θ to reduce the
truncation errors.
To determine θ, we construct a quality function that takes into
consideration both the truncation errors and the noise errors. Let
cj denote jth column count of a dataset D and ac(D) denote the
average column count in D, that is,
n(cid:88)
d(cid:88)
i=1
j=1
d(cid:88)
j=1
ac(D) =
1
d
D(i, j) =
1
d
cj
(2)
We use the following quality function:
q(D, θ, p) = ac(D|θ) − θ
p
(3)
The intuition is that the higher the average column count in D|θ,
then the lower the truncation error. In fact, an increase in ac(D|θ)
by x means the reduction of truncation error by x. At the same
time, the MAE resulted from adding Lap (λ) is computed as fol-
lows:
(cid:90) ∞
−∞
(cid:16)− |x|
λ
(cid:17)
|x| 1
2λ
e
(cid:90) ∞
0
dx = λ
−ydy = λ
ye
Thus the quality function in Equation (3) captures the relative change
of MAE when changing θ.
Another nice property of this quality function is that its sensitiv-
ity is 1, which we prove below.
LEMMA 1. The sensitivity of quality function q in Equation (3)
is 1.
PROOF. Let D, D(cid:48) be two neighbor datasets differ in only one
row. The sensitivity of quality function q is
∆q = max∀θ,D,D(cid:48),p |q(D, θ, p) − q(D(cid:48), θ, p)|
ac(D|θ) − θ
= max∀θ,D,D(cid:48),p
= max∀θ,D,D(cid:48),p |ac(D|θ) − ac(D(cid:48)|θ)|
≤ 1
p
(cid:17) −(cid:16)
(cid:12)(cid:12)(cid:12)(cid:16)
(cid:17)(cid:12)(cid:12)(cid:12)
ac(D(cid:48)|θ) − θ
p
The last step is because adding one row can increase the average
column count by at most 1.
As we will show in Section 5, the quality function approximates
the utility of published column counts very well.
Given the quality function, we use the exponential mechanism to