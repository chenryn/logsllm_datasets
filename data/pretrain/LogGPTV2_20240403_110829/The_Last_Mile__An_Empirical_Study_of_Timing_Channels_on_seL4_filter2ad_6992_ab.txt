the distinguishing portion of the Lucky 13 attack [AlFardan and Pa-
572terson, 2013], which measures the response times for two different
packets, M0 or M1. The set I consists of these two input packets.
Owing to the large number of input and output symbols (columns
and rows), the channel matrices themselves are large—the analo-
gous matrix to Figure 1 for the E6550 (see Table 1) would occupy
4.8 TiB if stored in full. We take advantage of the sparseness of
the matrices—for any given input there are a only a small num-
ber of outputs that occur with non-zero probability—to compress
them. Even so, the largest of our compressed matrices still occu-
pies 380 MiB when using single-precision ﬂoating point to store its
entries.
The channel matrix represented by Figure 1 has 32,768 columns
(input symbols) and 45,000 rows (output symbols), for a total of 1.5
billion cells (conditional probabilities). This is much larger than
those usually considered in the literature, and demonstrates that a
numerical approach scales to realistic problem sizes.
4.2 Measures of Leakage
Given the channel matrix, we can calculate the Shannon capac-
ity [Shannon, 1948], denoted C, a standard summary measure of
capacity. We use a sparse matrix implementation of Yu’s improved
form [Yu, 2010] of the Arimoto-Blahut algorithm (ABA) [Arimoto,
1972; Blahut, 1972]. Multiplying by the sampling rate (333 Hz in
most of our experiments, as explained shortly) gives the channel
bandwidth, denoted B, in bits per second.
There are two additional quantities that we use where appropri-
ate (e.g in Section 6): the maximum vulnerability, denoted Vmax,
and the min-leakage, denoted ML. Vmax is the greatest likelihood
(among all possible secrets), of an optimal (computationally un-
bounded) attacker correctly guessing the secret, given what it has
observed. This is a safe upper bound on the vulnerability of the
system. As we shall see in Section 6, the bound is tight, as it can
be achieved by an attacker in sufﬁciently simple examples.
ML is a pessimistic analogue to the Shannon capacity. Whereas
Shannon capacity can tell us the average amount of information
leaked by a channel, min-leakage gives the worst case. It is the rate
of change of the min-entropy (H∞), given an observation. Thus
H∞(ﬁnal) = H∞(initial) − ML. The min-entropy, in turn, is
simply the (log) vulnerability of a distribution given no more leak-
age i.e. H∞ = − log2 maxx Px. For further details see Köpf and
Basin [2007]; Smith [2009].
4.3 Low-Capacity Channels
For a matrix such as Figure 1, the existence of a channel is ob-
vious, and calculating its bandwidth is straightforward.
In other
cases, however, it is not. Figure 3 shows the same channel with a
countermeasure applied (cache colouring). The ﬁgure shows plenty
of noise but no apparent variation between columns, suggesting that
different inputs are indistinguishable to the receiver. We nonethe-
less calculate a nontrivial bandwidth of 27 bits per second. We
analyse this residual channel in Section 5.4.
A channel with a true bandwidth of zero may nonetheless ap-
pear to have a small nonzero bandwidth when we sample it: given
a ﬁnite number of samples, the reconstructed output distributions
for two different inputs will appear slightly different due to sam-
pling error. This will make inputs appear distinguishable when in
fact they are not. Analysis of a large number of synthetic matri-
ces demonstrates that the effect of sampling error on small chan-
nels (C (cid:28) 0.1 b) is to increase their apparent capacity (by making
identical distributions appear to differ slightly) and on large chan-
nels (C (cid:29) 0.1 b) is to decrease it (by making the output distri-
butions appear to overlap more than they really do), although the
latter effect is small.
12.5
12.4
12.3
12.2
12.1
3
0
1
/
d
e
h
c
u
o
t
s
e
n
i
L
10−2
10−3
10−4
0
10
20
30
Lines evicted /103
Figure 3: Exynos4412 cache channel with cache colouring.
N = 7200, B = 27 b/s, CImax
0 = 15 b/s.
)
s
/
b
k
(
h
t
d
i
w
d
n
a
b
t
n
e
r
a
p
p
A
1
0.1
0.01
0
Fitted curve
Simulated matrices
10000
20000
30000
Samples per column
Figure 4: Apparent bandwidth against number of samples (N)
for 0-bandwidth matrices derived from Figure 3, with least-
squares ﬁtted curve.
We use a simple statistical test to determine whether any appar-
ent capacity (and hence bandwidth) results from sampling error. If
the capacity is really zero, the output distribution (a vertical slice
through the matrix) must be identical for all inputs (otherwise they
would be distinguishable). In that case, the samples that made up
each column must be drawn from the same distribution. We obtain
this hypothetical distribution on outputs by averaging the channel
matrix along its rows, resulting in an effective sample size of N
times the number of columns. From this we sample 1,000 new ma-
trices, each representing a true zero-capacity channel, and calcu-
late their apparent capacities. The greatest bandwidth among these
(multiplying by sample rate) we label CImax
, or the greatest value
in the 99.9% conﬁdence interval for the apparent bandwidth of a
zero-capacity channel. If the actual measured bandwidth lies above
CImax
, then there is only a one in 1,000 chance that this apparent
capacity is the result of sampling error, and is strong evidence that
there is a real underlying channel.1
0
0
For example, in Figure 3, B = 27 b/s, which is above CImax
0 =
15 b/s, and thus there is a channel, despite the graph seemingly
showing no horizontal variation.
Figure 4 shows how the number of samples required scales with
the desired resolution. The points give the apparent bandwidth
solely due to sampling error for matrices (derived from Figure 3,
as already described), with a varying number of samples per col-
umn. The noise level drops with 1/x—the curve is a least squares
ax+b . We see that in order to detect a channel of bandwidth
ﬁt to
10 b/s, we would require 32,000 samples per column (for a thresh-
old of detection where the noise is lower than the measured signal).
Clearly, at some point the number of samples required will become
1If the apparent bandwidth is below CImax
, the test is inconclusive:
there is no evidence of a channel, but that is not evidence of no
channel.
0
1
573unfeasibly large, and a different technique will be required. The
current approach does sufﬁce however, for a number of small resid-
ual channels, as we shall shortly demonstrate.
4.4 Data collection
Collecting observations takes time. Each sample used to build
Figure 3, for example, takes 3 ms. The 230 × 106 samples in this
ﬁgure thus represent 200 hours of cumulative observation. In to-
tal, this project involved collecting 4.1 GiB of (compressed) sample
data, across roughly 2,000 hours of observations over a 12 month
period. A large number of samples was necessary to obtain the sta-
tistical power to detect the smallest of the channels that we report.
The software used to generate these matrices, and perform the
necessary statistical analysis is available as open source.2
The local cache channel is highly sensitive to hardware prop-
erties (memory architecture and processor microarchitecture). We
therefore evaluate it on a number of recent ARM and x86 platforms,
Table 1 shows the platforms and their properties. In all cases, our
experiments were run with the sender and receiver sharing the same
CPU core.
5. LOCAL CACHE CHANNEL
5.1 Exploiting the channel
Caches are divided into lines, small, equally-sized blocks of a
few dozen bytes in length. Each line holds the contents of an
aligned memory block of the same size. Modern CPU caches are
set associative, meaning that each memory block may reside in a
ﬁxed subset of cache lines, typically determined by a number of
index bits taken from the block’s address. The cache lines are thus
partitioned into a number of identically-sized sets; the number of
lines per set is the associativity of the cache. The address of a mem-
ory block determines the unique cache set in which it may reside,
and thus which lines it may occupy. The subset of the cache in
which a particular memory block can reside is its cache colour.
char A[ L ] [ L_SZ ] ;
void s e n d e r ( void ) {
i n t S ;
while ( 1 ) {
f o r ( i =0; i <S ; i ++) {
A[ i ] [ 0 ] ^= 1 ;
}
}
}
char B[ L ] [ L_SZ ] ;
v o l a t i l e
i n t C ;
void r e c e i v e r ( void ) {
while ( 1 ) {
f or ( i =0; i <L ; i ++) {
B[ i ] [ 0 ] ^= 1 ;
C++;
}
}
}
void measure ( void ) {
i n t R , C1 , C2 ;
while ( 1 ) {
C1=C ;
do { C2=C ; }
R=C2−C1 ;
while ( C1==C2 ) ;
}
}
Figure 5: Preemption tick exploit code.
On loading a block into an already full set, some other entry must
be evicted. Thus, if sender and receiver have access to (disjoint)
blocks of memory that map to the same cache set, the sender can
2http://ssrg.nicta.com.au/software/TS/channel_tools/
evict the receiver’s blocks from the cache by touching its own (load-
ing them into the cache). This is the basis of the cache-contention
channel we analyse in this section.
Pseudocode to exploit this channel is given in Figure 5. Sender
and receiver run alternately, sharing a core, with access to disjoint
memory partitions. Arrays A and B each cover the entire last-level
cache (L2). These arrays are allocated from physically contiguous
memory, thus covering all cache sets. The receiver touches one
word of each line in the cache, ﬁlling the cache with its own data.
By measuring its rate of progress (via a helper thread, measure()),
the receiver infers how many of its blocks were already cached.
The sender communicates by evicting some number, S, of these
(the channel input). The receiver sees the number of lines touched
in a ﬁxed interval, R (the channel output) which, as established,
depends on S.
In this example we use the preemption tick to determine the mea-
surement interval, as an example of a clock that is difﬁcult to elimi-
nate, although any regular event would do. Here, the simple round-
robin scheduler of seL4 inadvertently provides a precise real-time
clock.
As indicated, we consider two mitigations against this channel.
Cache colouring eliminates the channel by partitioning the cache
between sender and receiver, and requires no other restrictions on
the system. Instruction-based scheduling prevents the use of the
preemption tick as a clock by tying it to the receiver’s progress, but
to be useful requires that all other clocks have also been removed. It
is, however, applicable to other channels (e.g. the bus), while cache
colouring is speciﬁc to the cache channel.
5.2 Unmitigated channel
We ﬁrst analyse the channel with no countermeasures, to estab-
lish a baseline. Figure 1 gives the results for the Exynos4412, ob-
tained by taking N = 1000 samples for each value of S. All tested
platforms produce very similar results.
As the sender evicts more of the receiver’s lines from the cache,
the number of cache lines that the receiver touches in each time
slice decreases. This occurs as it takes longer for the receiver to
touch evicted lines, and thus it touches fewer lines during each
ﬁxed time slice. The unmitigated channel’s Shannon capacity, or
the expected leakage per observation, is calculated from the chan-
nel matrix as explained in Section 4.2. The capacity of this channel
is 7 bits.
We calculate bandwidth as follows: Each observation requires
three time slices (of 1 ms each), one for each of the sender and the
two receiver threads, or a total of 333 observations per second. As
each leaks 7 bits, the bandwidth is approximately 2.3 kb/s.
5.3 Instruction-based scheduling
As described, IBS removes the preemption-tick clock from the
receiver, assuming that all other time sources are already gone. On
seL4 we could thwart this particular exploit by preventing the re-
ceiver from creating its helper thread: seL4’s strong resource man-
agement model provides control over the kernel-scheduled threads
a task can create. This would further restrict application, and does
not apply to most operating systems.
To implement IBS, we modify seL4 to trigger preemptions each
time some ﬁxed number, K, of instructions has executed. This
is easily achieved with the help of the performance management
unit (PMU) available on modern CPUs, which can be conﬁgured
to generate an exception after some number of events (here retired
instructions).
Implementing this in seL4 on ARM requires changing only 18
lines of code, and x86 is similarly straightforward. Because this
574Processor
Manufacturer
Architecture
Core type
Released
Cores
Clock rate
Timeslice
RAM
L1 D-cache
size
index
tag
line size
lines
associativity
sets
size
line size
lines
associativity
sets
colours
L2 cache
ARM1136JF-S
iMX.31
Freescale
ARMv6
2005
1
532 MHz
1 ms
128 MiB
E6550
Intel
x86-64
Conroe
2007
2
2.33 GHz
2 ms
1024 MiB
AM3358
DM3730
iMX.6
Freescale
ARMv7
ARMv7
Cortex A8 Cortex A8 Cortex A9
ARMv7
TI
TI
2010
1
1 GHz
1 ms
512 MiB
2011
1
720 MHz
1 ms
256 MiB
2011
4
1 GHz
1 ms
1024 MiB
Exynos4412
Samsung
ARMv7
Cortex A9
2012
4
1.4 GHz
1 ms
1024 MiB
16 KiB
virtual
physical
32 B
512