try, if the bit corresponding to the ingress port is 0 (“hasn’t
yet been ﬂooded”) and the packet’s hopcount is 1 (this is its
ﬁrst hop), we set the ﬂooded bit for the port, and perform a
ﬂood. This is a ﬁrst-hop ﬂood, so L is set, and it therefore
allows learning multiple paths. The obvious downside here
is some additional ﬂooding, but the upside is that equal cost
paths are discovered quickly.
2.6 Reasoning About Scale
In this section so far, we have focused on the details of the
AXE algorithm, its properties, and what functionality it can
support. Here we address another rather basic question at a
conceptual (but not rigorous) level: How well does it scale?
AXE is an L2 technology that adopts the ﬂood-and-learn
paradigm. Our question is not how well a ﬂood-and-learn
paradigm can scale, because that will depend in detail on
the degree of host mobility, the trafﬁc matrix, and the band-
width of links. Rather, our question is whether AXE’s de-
sign hinders its ability to scale to large networks beyond
the baseline inherent to any ﬂood-and-learn approach. More-
over, we focus on bandwidth usage, and do not consider
the impact of AXE’s additional memory requirements be-
cause hardware/memory constraints will change over time.
For bandwidth, the main factor that differentiates AXE from
traditional L2 is the use of ﬂooding when failures occur.
We can estimate the impact of this design choice as fol-
lows. Consider a fully utilized link of bandwidth B that sud-
denly fails. If the average round-trip-time of trafﬁc on the
link is RT T , then roughly RT T ∗ B worth of trafﬁc will be
ﬂooded before congestion control will throttle the ﬂows. If
we want to keep the overhead of failure-ﬂooding below 1%
of the total trafﬁc, that means we can tolerate no more than f
failures per second, where f = 0.01
RT T . If the RTT is 1ms, then
the network-wide failure rate would need to be less than 10
per second. Assuming that links have MTBFs greater than
106 seconds, then the trafﬁc due to failures is less than 1%
of the link for networks with less than 107 links. Thus, in
terms of bandwidth, AXE can scale roughly as well as any
learn-and-ﬂood approach.
3 P4 Implementation
We have argued – and in Section 5 we show through simula-
tion – that AXE provides a unique combination of features:
fully plug-and-play behavior, no control plane, the ability to
run on general topologies, and near-instantaneous response
to failures. However, if vendors were required to create a new
generation of ASICs to support it, then AXE would likely be
no more than an intriguing academic idea.
We think AXE can avoid this unfortunate fate because
of the rise of highly reconﬁgurable hardware with an open-
source speciﬁcation language, and here we are thinking pri-
marily of RMT [5] and P4 [18], but other such efforts may
arise. In this section we discuss our implementation of AXE
in P4, which we have tested in Mininet [27] using the bmv2
P4 software switch [4]. This testing veriﬁed that our imple-
mentation does, indeed, implement the protocol as expected.
While the rest of this section delves into sometimes arcane
detail, our point here is simple: once P4-supporting switches
are commercially available, AXE could be deployed simply
by loading a P4 implementation. While this does not assure
deployment, it does radically reduce the barriers.
Unlike a traditional general-purpose programming lang-
uage, P4 closely resembles the architecture of network for-
warding ASICs. Programs consist of three core components:
a packet parser speciﬁed by a ﬁnite state machine, a series
of match-action tables similar to (but more general than)
OpenFlow [26], and a control ﬂow function which deﬁnes
a processing pipeline (describing which tables a packet is
processed by and in which order). The parser is quite gen-
eral and easily implements AXE’s modiﬁed Ethernet header.
Thus, our primary concern was how to implement the AXE
forwarding algorithm as a pipeline of matches and actions.
Putting aside the nonce, deduplication ﬁlter, and learning
(discussed below), the AXE algorithm is simply a series of
if statements checking for various special conditions. Such
if statements can be implemented in two ways in P4: ei-
ther as tables which match on various values (with the de-
fault fall-through entry acting as an else clause), or as actual
if statements in the control ﬂow function. The “bodies” of
the if statements are implemented as P4 compound actions.
We were able to use the slightly more straightforward latter
method almost exclusively, which allowed us to structure our
P4 code very similarly to the pseudocode shown above. This
approach, however, is not without its caveats.
As control ﬂow functions cannot directly execute actions,
we currently have a relatively large number of “dummy” ta-
bles that merely execute a default action; the control ﬂow
function invokes these tables simply to execute the associ-
ated action (that is, the tables are always empty). If hard-
ware performance is related to the length of the forwarding
pipeline, or if there are hard limits on the number of ta-
bles (less than the 26 that we currently require), the code
may need to be reorganized. Speciﬁcally, we can take the
Cartesian product of nested conditionals to collapse several
of them into a single table lookup. This approach can likely
reduce the pipeline length dramatically, though the resulting
code will surely become less readable. Whether such an opti-
mization is necessary depends on the particular features and
limitations of the associated ASIC (as well as the optimiza-
tions that the compiler backend for the target ASIC applies).
Learning: P4 tables cannot be modiﬁed from the data plane
– only from the control plane. This may be reasonable for a
simple L2 learning switch: when a packet’s source address
is not in the table, the packet is sent to the switch’s control
plane, which creates a new table entry. Such a trip from data
plane to control plane and back has a latency cost, however,
and we would like to avoid it whenever possible, especially
considering that AXE table entries contain not only the port,
but the hopcount to reach the address’ host, and keeping this
hopcount information up to date is important to the algo-
rithm. We achieve this by separating learning into two parts,
as depicted in Figure 2. The ﬁrst part is a table, which is
populated by the control plane the ﬁrst time a given Ether-
net address is seen, much like a conventional L2 learning
503
We try to emulate a DVMRP-like [32] multicast model,
with source-speciﬁc trees for each group. While AXE’s abil-
ity to safely ﬂood makes reliable delivery easy, the design
challenge is to enable rapid construction (and reconstruc-
tion) of trees in order to avoid the additional trafﬁc over-
head of unnecessary ﬂooding. Our approach forms multicast
trees by initially sending all packets for the group out all
ports. Unnecessary links and switches are then pruned. When
the topology changes or when new members are added to a
pruned section of the tree, we simply reset the tree and re-
construct it; this avoids maintaining a tree as changes occur
(which turns out to be quite difﬁcult).
Multicast in AXE has four types of control messages:
JOIN, LEAVE, PRUNE, and RESET. The ﬁrst two are how
hosts express interest/disinterest in a group to their connected
switches. PRUNE is much the same as its DVMRP counter-
part and is used for removing ports and switches from the
tree. RESET enables a switch to indicate that something has
changed which necessitates that the current tree be invali-
dated and rebuilt; we come back to this shortly.
ure detection. We also speculate that P4-capable ASICs will
have some way to query this information more directly from
the data plane (without control plane involvement) for at
least some failure detection mechanisms.
Our P4 implementation is not written with an eye towards
efﬁciency on any particular P4 target, as targets are diverse
and no P4 hardware target is yet available to us. Neverthe-
less, we see the existence of a functionally complete P4 im-
plementation as a promising beginning.3
4 Multicast
Many L2 networks implement multicast via broadcast, with
ﬁltering done by the host NICs (sometimes with the addition
of switches implementing “IGMP snooping” [6] wherein an
ostensibly L2 device understands enough about L3 to prune
some links). We investigated whether we could use AXE
ideas to provide native support for multicast in a relatively
straightforward way, and found the answer to be yes. We lack
space to fully illuminate our design, but we sketch it here.
Going into the algorithm in more detail, we retain much
of the AXE header, but remove the L ﬂag and add a “mul-
ticast generation number” ﬁeld which is used for coordi-
nation. A source-group’s root switch (the switch to which
the source is attached) dictates a generation number for the
source-group pair. Other switches that are part of the group
simply track the latest number. All switches stamp all pack-
ets for this source-group (including control messages) with
the latest generation number of which they are aware. If a
non-root switch receives a packet for the current generation,
it forwards the packet out all of the currently unpruned ports
for the source-group. If the packet is for a new generation,
the tree is being rebuilt; the switch moves to the new gener-
ation number and un-prunes all ports. If a packet is stamped
with an old generation number, the packet is sent over all
3And, more broadly, we see the fact that AXE can be imple-
mented in an ASIC-friendly language like P4 as an indicator
that it may be suitable for ASIC implementation more gen-
erally – reconﬁgurable or otherwise.
Figure 2: The ﬁrst time a new MAC is seen, the packet is sent to the control
plane, which adds an entry mapping the MAC to a register index. Subse-
quent packets are simply looked up in the mapping table, and new values
for the port and hopcount can be “learned” simply by rewriting the register
entirely in the data plane. A nearly identical table maps from eth.dst to its
register index. A special port value indicates an invalid learning entry.
switch. However, instead of the table simply holding the as-
sociated port, it instead contains an index into the second part
– an array of P4 registers, which are data plane state that can
be modiﬁed by actions in the data plane. Thus, when pro-
cessing a packet that requires changing learning state, it can
be done at line rate entirely within the data plane, with the
sole exception of the ﬁrst packet (for which the control plane
must allocate a register cell and add a table entry mapping
the Ethernet address to it). As the P4 speciﬁcation evolves,
or hardware implementations support new features, it may
be possible to eliminate control plane interaction entirely.
Deduplication Filter: The deduplication ﬁlter is a straight-
forward implementation of the design discussed in Section
2.3. For reasons similar to the above, we again use an array
of P4 registers to hold the ﬁlter entries rather than a P4 table
(actually, we use a separate register array for each ﬁeld, as
the struct-like registers described in the P4 spec are not yet
supported in bmv2 or its compiler). Then a P4 ﬁeld list cal-
culation is used to specify a hash of the appropriate header
ﬁelds (the source Ethernet address, the nonce, and the L ﬂag)
along with a seed value stored in a P4 register and populated
by the control plane at startup. The computed hash is stored
in packet metadata and used to index into the ﬁlter arrays.
Nonce: A switch must assign a nonce to each packet it re-
ceives from a directly-attached host. A straightforward ap-
proach is to use a single P4 register to hold a counter, though
this requires that reading and incrementing the register be
atomic, which may be problematic for real hardware if differ-
ent ports essentially operate in parallel. Instead, we can use
a nonce register per port instead of a single shared register
per switch. As each port would have an independent counter,
nonce allocations would no longer be entirely unique. How-
ever, this is not problematic because, as discussed in Section
2.3, the deduplication key is a tuple of : for
the typical case when a given host interface is only attached
to a single port, the combination of the interface’s address
and a per-port nonce will be unique (this may preclude some
types/implementations of link aggregation; however, AXE
obviates some motivations for link aggregation anyway).
Link status: P4 does not specify a way for the data plane to
know about link liveness. Our implementation emulates this
functionality by creating “port state” registers, and we man-
ually set their values to simulate port up and down events. In
a real hardware implementation, such registers could be ma-
nipulated by the control plane as it detects link state changes
using whatever mechanisms the switch provides for link fail-
504
Matcheth.src == 12:48:A1:15:79:36eth.src == EA:AD:CA:A9:B2:A0eth.src == 2A:33:86:97:9F:79eth.src == D6:DE:0A:64:3A:13eth.src == 9E:88:EE:7B:90:53src Mapping TableActionmeta.src_cell = 1meta.src_cell = 3meta.src_cell = 0meta.src_cell = 2meta.src_cell = 4Learning RegistersPort822211HC18128301234As mentioned above, when constructing a new tree, all
ports are initially unpruned – this is basically the equivalent
of ﬂooding. Switches therefore potentially receive packets
from the root on multiple ports, and can decide on an “up-
stream” port based on the best hopcount. When a switch re-
ceives a packet from the group on any port that is not its up-
stream port, it sends a PRUNE in response. This causes the
packet’s original sender to stop sending on this port. Pruning
is kept consistent by ignoring PRUNEs for any generation
except the current one. In this way, the initial ﬂood-like be-
havior is cut down to a shortest-path tree.
When any switch notices that the tree may need to be re-
built due to either (a) being invalid (i.e., uses a link that has
failed) or (b) possibly needing to expand or change shape
(due to a port going up or down or a new member joining
the group), the switch enters ﬂood mode for the group, and
may send a RESET. While a switch is in ﬂood mode for a
group, it sets the F ﬂag and ﬂoods all packets it receives for
the group, disregarding whether a port has been pruned or
not. The switch leaves ﬂood mode when it sees a new, higher
generation number, which indicates that the root has begun
the process of building a new tree. Being in ﬂood mode has
two effects. Firstly, it makes sure that packets for the group
continue to be delivered. Secondly, when the root switch sees
any packet with the F bit set and the current generation num-
ber, it recognizes this as meaning that some switch needs
the tree to be reset. The root switch then initiates this by in-
crementing the generation number. While any packet can be
used to reset the tree (by setting F), there are times when the
tree should be rebuilt but no packet is immediately available
(for example, when a new switch is plugged into the net-
work). It is for these cases that the RESET message exists:
they can be used to initiate a reset without needing to wait
for a packet from the group to arrive (which, if the network
is stable, may not be for some time).
ports; this is not optimal, but it ensures that outstanding pack-
ets from old generations are delivered even while a new gen-
eration tree is being established.
For safety, the root switch periodically ﬂoods multicast
packets even in the absence of any other indications to do
so. This bounds the time that the group suffers with a bad