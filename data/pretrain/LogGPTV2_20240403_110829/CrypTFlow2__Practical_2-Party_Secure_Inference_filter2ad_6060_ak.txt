ğ‘ to learn
0 âŠ• ğ‘—0 âŠ• ğ‘—1) s.t.
1
end if
15: ğ‘ƒ0 & ğ‘ƒ1 invoke an instance of(cid:0)4
(cid:1)-OTğœ‚+ğ›¿ where ğ‘ƒ0 is the sender
1 ||ğ‘¥1.
ğ‘ = âŸ¨ğ´âŸ©Î”
ğ‘ ,
and âŸ¨ğ´2âŸ©Î”
with inputs {ğ‘  ğ‘—||ğ‘Ÿ ğ‘—} ğ‘— and ğ‘ƒ1 is the receiver with input âŸ¨ğ‘šâŸ©ğµ
ğ‘ƒ1 sets its output as âŸ¨corrâŸ©ğ‘›
16: For ğ‘ âˆˆ {0, 1}, ğ‘ƒğ‘ sets âŸ¨ğ´âŸ©Î”
ğ‘ ) Â· ğ‘›0.
17: For ğ‘ âˆˆ {0, 1}, ğ‘ƒğ‘ sets âŸ¨ğ´0âŸ©Î”
ğ‘ âˆ’ ğ‘ Â· ğ‘‘, âŸ¨ğ´1âŸ©Î”
ğ‘ =Î” âŸ¨ğ´âŸ©Î”
ğ‘ + ğ‘ Â· ğ‘‘.
18: for ğ‘— = {0, 1, 2} do
19:
1 ||âŸ¨corrâŸ©Î”
1 .
ğ‘ âˆ’ (ğ‘¥ğ‘ âˆ’ âŸ¨corrâŸ©Î”
ğ‘ =Î” ğ‘0
ğ‘ =Î” âŸ¨ğ´âŸ©Î”
For ğ‘ âˆˆ {0, 1}, ğ‘ƒğ‘ invokes F int,ğ›¿
DReLU with input âŸ¨ğ´ğ‘—âŸ©Î”
ğ‘ . Party ğ‘ƒğ‘ sets âŸ¨ğ¶â€²
ğ‘ âŠ• ğ‘.
ğ‘—âŸ©ğµ
ğ‘ = âŸ¨ğ›¾ ğ‘—âŸ©ğµ
output âŸ¨ğ›¾ ğ‘—âŸ©ğµ
For ğ‘ âˆˆ {0, 1}, ğ‘ƒğ‘ invokes an instance of F ğ‘›
âŸ¨ğ¶â€²
ğ‘ and learns âŸ¨ğ¶ ğ‘—âŸ©ğ‘›
ğ‘—âŸ©ğµ
ğ‘ .
21: end for
ğ‘ = âŸ¨ğ¶0âŸ©ğ‘›
22: For ğ‘ âˆˆ {0, 1}, ğ‘ƒğ‘ sets âŸ¨ğ¶âŸ©ğ‘›
23: For ğ‘ âˆˆ {0, 1}, ğ‘ƒğ‘ sets ğµğ‘ = idiv(ğ‘0
24: ğ‘ƒğ‘ sets âŸ¨ğ‘§âŸ©ğ‘›
ğ‘ =ğ‘› rdiv(âŸ¨ğ‘âŸ©ğ‘›
, ğ‘‘) + âŸ¨corrâŸ©ğ‘›
ğ‘ âˆˆ {0, 1}.
ğ‘ + âŸ¨ğ¶1âŸ©ğ‘›
ğ‘ + âŸ¨ğ¶2âŸ©ğ‘›
ğ‘ .
ğ‘ âˆ’ ğ‘¥ğ‘ Â· ğ‘›0, ğ‘‘).
ğ‘ Â· ğ‘›1 + ğ‘ âˆ’ âŸ¨ğ¶âŸ©ğ‘›
ğ‘ âˆ’ ğµğ‘, for
B2A with input
ğ‘ to learn
20:
ğ‘
DIV
2ğ›¿ âˆ’ 4 1
2 ğœ† âˆ’ 119 1
of communication. Thus, the overall communication of Î ring,ğ‘›,ğ‘‘
is
2 ğœ†ğœ‚ + 34ğœ‚ + 3ğœ†ğ›¿ + 44 1
3
2, which can be rewritten as
 ğ‘ğ‘œ, where ğ‘“ 2 is the
filter size, which is usually the case.
G COMPLEXITY OF OUR BENCHMARKS
marized as follows:
The complexity of the benchmarks we use in Section 7 is sum-
â€¢ SqueezeNet: There are 26 convolution layers of maximum
filter size 3 Ã— 3 and up to 1000 output channels. The activa-
tions after linear layers are ReLUs with size of up to 200,704
12Note that this includes the case of â„“-bit integers when ğ‘› = 2â„“ .
13The number of homomorphic additions also differ, but they are relatively very cheap.
17
â€¢ DenseNet121: There are 121 convolution layers with maxi-
mum filter dimension of 7 Ã— 7 and up to 1000 output chan-
nels. Similar to ResNet50, between 2 convolution layers,
there is batch normalization followed by ReLU. The biggest
ReLU layer in DenseNet121 has 802,816 elements and the
combined size of all ReLU layers is 15,065,344. In addition,
DenseNet121 consists of a Maxpool, an Avgpool49 and 3
Avgpool4 layers.
H GARBLED CIRCUITS VS OUR PROTOCOLS
FOR Avgpool
In this section, we compare our protocols with garbled circuits
for evaluating the Avgpool layers of our benchmarks, and the
corresponding performance numbers are given in Table 9. On
DenseNet121, where a total of 176, 640 divisions are performed,
we have improvements over GC of more than 32Ã— and 45Ã— in the
LAN and the WAN setting, respectively, for both our protocols.
However, on SqueezeNet and ResNet50, the improvements are
smaller (2Ã— to 7Ã—) because these DNNs only require 1000 and 2048
divisions, respectively, which are not enough for the costs in our
protocols to amortize well. On the other hand, the communication
difference between our protocols and GC is huge for all three DNNs.
Specifically, we have an improvement of more than 19Ã—, 27Ã—, and
31Ã— on SqueezeNet, ResNet50, and DenseNet121 respectively, for
both our protocols.
I FIXED-POINT ACCURACY OF OUR
BENCHMARKS
In this section, we show that the accuracy achieved by the fixed-
point code matches the accuracy of the input TensorFlow code. Ta-
ble 10 summarizes the bitwidths, the scales, and the corresponding
TensorFlow (TF) and fixed-point accuracy for each of our bench-
marks. Since our truncation and division protocols lead to faithful
implementation of fixed-point arithmetic, accuracy of secure infer-
ence is the same as the fixed-point accuracy.
Benchmark
Bitwidth
Scale
SqueezeNet
ResNet50
32
37
32
9
12
11
TF
Fixed
TF
Fixed
Top 1 Top 1 Top 5 Top 5
79.22
55.86
93.23
76.47
74.25
91.90
55.90
76.45
74.35
79.18
93.21
91.88
DenseNet121
Table 10: Summary of the accuracy achieved by fixed-point
code vs input TensorFlow (TF) code.
Benchmark
SqueezeNet
ResNet50
DenseNet121
Benchmark
SqueezeNet
ResNet50
DenseNet121
Garbled Circuits
Our Protocol
LAN WAN Comm LAN WAN Comm
1.84
0.2
2.35
0.4
17.2
158.83
36.02
96.97
6017.94
2.0
3.9
179.4
0.8
0.8
3.5
0.1
0.1
0.5
(a) over Z2â„“
Garbled Circuits
Our Protocol
LAN WAN Comm LAN WAN Comm
0.2
1.92
3.82
0.4
19.2
214.94
39.93
106.22
6707.94
2.2
4.2
198.2
0.9
1.0
4.4
0.1
0.1
0.6
(b) over Zğ‘›
Table 9: Performance comparison of Garbled Circuits with
our protocols for computing Avgpool layers. Runtimes are in
seconds and communication numbers are in MiB.
elements per layer. All ReLU layers combined have a size of
2,033,480. Additionally, there are 3 Maxpool layers and an
Avgpool169 layer (Avgpool with pool size 169).
â€¢ ResNet50: There are 53 convolution layers of maximum filter
size 7 Ã— 7 and a peak output channel count of 2048. Convo-
lution layers are followed by batch normalization and then
ReLUs. There are 49 ReLU layers totaling 9,006,592 ReLUs,
where the biggest one consists of 802,816 elements. More-
over, ResNet50 also has Maxpool layers and an Avgpool49.
18