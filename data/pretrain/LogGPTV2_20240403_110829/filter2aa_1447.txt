# Evading Next-Gen AV Using AI

**Presenter: Hyrum Anderson**

- **Email:** PI:EMAIL
- **Twitter:** @drhyrum
- **LinkedIn:** /in/hyrumanderson

## The Promise of Machine Learning in Cybersecurity

- **Automatic Detection:** Machine learning (ML) can identify malicious content or behavior by learning from data.
- **Automated Pattern Recognition:** Discriminatory patterns are learned automatically, rather than being manually constructed.
- **Generalization:** ML models can generalize to new, unseen samples and variants, provided the training data is representative of real-world conditions.
- **Adversarial Challenges:** Motivated adversaries actively seek to exploit and invalidate these models.

### Example Rule-Based Malware Detection
```plaintext
rule malware {
    strings:
        $reg = "\\CurrentVersion\\Internet Settings"
    condition:
        filesize > 3
}
```

## Goal: Breaking Machine Learning Models

- **Static ML Model:** Trained on millions of samples.
- **Example Scenarios:**
  - **Original Score:** `score=0.75` (malicious, moderate confidence)
  - **Modified Sample:** Simple structural changes that do not alter behavior.
  - **New Score:** `score=0.49` (benign, just barely)

### Adversarial Examples
- **Blind Spots:** ML models have blind spots and can be exploited.
- **Exploitation Techniques:**
  - **Deep Learning:** Fully differentiable, allowing direct queries for perturbations.
  - **Generalization:** Adversarial examples can generalize across different models (Goodfellow 2015).

### Taxonomy of Attacks Against ML
- **Score Reporting:**
  - **Black Box:** Can get a score but cannot see the model.
  - **White Box:** Knows the model architecture and weights, enabling direct attacks.
  - **Label Reporting:** Can get a label (malicious/benign) but not the score.

### Related Work
- **Android Malware:** Papernot et al. (2016)
- **DGA Detection:** Anderson et al. (2016)
- **PDF Malware:** EvadeML (Xu, Qi, Evans, 2016)
- **PE Malware:** MalGan (Hu, Tan, 2017)

## Adversarial Attack Strategies

### Gradient-Based Attacks
- **Requirements:** Full knowledge of model structure and weights.
- **Techniques:**
  - **Perturbation:** Small changes to input.
  - **GANs:** Generative Adversarial Networks.
- **Challenges:**
  - **Invalid Samples:** Generated samples may not be valid PE files.
  - **Behavior Preservation:** Ensuring functionality is maintained.

### Genetic Algorithms
- **Requirements:** Only need the score from a black-box model.
- **Process:**
  - **Mutation:** Modify malware with benign structures.
  - **Validation:** Use an oracle/sandbox to ensure functionality.
- **Challenges:**
  - **Expensive Validation:** Ensuring mutations do not break the format or change behavior.

## Designing an AI for Evasion

- **Goal:** Develop an AI that chooses format- and function-preserving mutations to bypass black-box ML models.
- **Approach:** Reinforcement Learning (RL).

### Example: Atari Breakout
- **Game Mechanics:**
  - **Environment:** Bouncing ball and rows of bricks.
  - **Agent Actions:** Move paddle left or right.
  - **Rewards:** Points for eliminating bricks.
- **AI Agent:**
  - **Input:** Environment state (pixels).
  - **Output:** Action (left, right).
  - **Feedback:** Delayed reward (score).

### Anti-Malware Evasion: An AI Approach
- **Environment:**
  - **Malware Sample:** Windows PE file.
  - **Mutations:** Preserve format and functionality.
  - **Reward:** Static malware classifier score.
- **Agent:**
  - **Input:** Environment state (malware bytes).
  - **Output:** Stochastic action.
  - **Feedback:** Reward (AV reports benign).

### State Observation and Manipulation
- **Features:**
  - **Static PE File Features:** Compressed to 2350 dimensions.
  - **General Information:** Machine/OS/linker info, section characteristics, imported/exported functions, strings, byte and entropy histograms.
- **Manipulations:**
  - **Create:** New entry point, new sections.
  - **Add:** Random imports, random bytes to PE overlay, bytes to end of section.
  - **Modify:** Section names, debug info, UPX pack/unpack, header checksum, signature.

### Machine Learning Model
- **Static PE Malware Classifier:**
  - **Model Type:** Gradient Boosted Decision Tree (non-differentiable).
  - **Knowledge:** Not known to the attacker.
  - **Feature Extractor:** Reused for the agent's state representation.

### Game Setup
- **Environment:**
  - **Termination:** After a maximum number of turns.
  - **Training Data:** 100K benign and malicious samples.
- **Agent:**
  - **Agent #1:** Gets score from ML detector.
  - **Agent #2:** Gets malicious/benign label.
  - **Algorithm:** Double DQN with dueling network and replay memory.

### Evasion Results
- **Performance:**
  - **Time:** 15 hours for 100K trials (10K episodes x 10 turns each).
  - **Evasion Rate:** Compared to random mutations.
- **Challenges:**
  - **Overattack:** Long episodes can overfit to specific models.

### Model Hardening Strategies
- **Adversarial Training:** Train with new evasive variants.
- **Human Feedback:** Categorize evasion techniques and dominant action sequences.

### Open Source Project
- **gym-malware:** OpenAI environment for malware evasion.
- **GitHub Repository:** [gym-malware](https://github.com/drhyrum/gym-malware)

### Summary
- **Effectiveness of ML:** ML models are effective at detecting new samples.
- **Blind Spots:** All models have exploitable blind spots.
- **Ambitious Approach:** Create a game where an AI bot competes against an AV engine.
- **Guarantees:** Preserves format and function of original malware.
- **No Knowledge Required:** No source code or target model knowledge needed.
- **Modest Results:** Further improvements are needed.

### Contact Information
- **Hyrum Anderson**
  - **Title:** Technical Director of Data Science
  - **Email:** PI:EMAIL
  - **Twitter:** @drhyrum
  - **LinkedIn:** /in/hyrumanderson

### Co-contributors
- **Anant Kharkar, University of Virginia**
- **Bobby Filar, Endgame**
- **Phil Roth, Endgame**

Thank you!