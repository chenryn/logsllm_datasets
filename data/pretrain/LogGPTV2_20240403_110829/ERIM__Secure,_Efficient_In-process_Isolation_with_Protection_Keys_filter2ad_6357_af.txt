USENIX Association
28th USENIX Security Symposium    1233
Benchmark
Switches/sec
403.gcc
445.gobmk
447.dealII
450.soplex
464.h264ref
471.omnetpp
482.sphinx3
483.xalancbmk
16,454,595
1,074,716
1,277,645
410,649
1,705,131
89,260,024
1,158,495
32,650,497
ERIM-CPI overhead
relative to orig. CPI in %
22.30%
1.77%
0.56%
0.60%
1.22%
144.02%
0.84%
52.22%
Table 6: Domain switch rates of selected SPEC CPU bench-
marks and overheads for ERIM-CPI without binary inspec-
tion, relative to the original CPI with ASLR.
CPI: The geometric means of the overheads (relative to
no protection) of the original CPI and ERIM-CPI across all
benchmarks are 4.7% and 5.3%, respectively. The relative
overheads of ERIM-CPI are low on all individual bench-
marks except gcc, omnetpp, and xalancbmk.
To understand this better, we examined switching rates
across benchmarks. Table 6 shows the switching rates
for benchmarks that require more than 100,000 switches/s.
From the table, we see that the high overheads on gcc, om-
netpp and xalancbmk are due to extremely high switching
rates on these three benchmarks (between 1.6 × 107 and
8.9 × 107 per second). Further proﬁling indicated that the
reason for the high switch rate is tight loops with pointer
updates (each pointer update incurs a switch). An optimiza-
tion pass could hoist the domain switches out of the loops
safely using only direct control ﬂow instructions and enforc-
ing store instructions to be bound to the application memory,
but we have not implemented it yet.
Table 6 also shows the overhead of ERIM-CPI excluding
binary inspection, relative to the original CPI over ASLR
(not relative to an unprotected baseline as in Figure 2). This
relative overhead is exactly the cost of ERIM’s switching.
Depending on the benchmark, it varies from 0.03% to 0.16%
for 100,000 switches per second or, equivalently, 7.8 to 41.6
cycles per switch. These results again indicate that ERIM
can support inlined reference monitors with switching rates
of up to 106 times a second with low overhead. Beyond this
rate, the overhead becomes noticeable.
CPS: The results for CPS are similar to those for CPI, but
the overheads are generally lower. Relative to the baseline
without protection, the geometric means of the overheads of
the original CPS and ERIM-CPS are 1.1% and 2.4%, respec-
tively. ERIM-CPS’s overhead relative to the original CPS
is within 2.5% on all benchmarks, except except perlbench,
omnetpp and xalancbmk, where it ranges up to 17.9%.
6.5 Comparison to existing techniques
In this section, we compare ERIM to isolation using SFI
(with Intel MPX), extended page tables (with Intel VT-
x/VMFUNC), kernel page tables (with lwCs), and instru-
mentation of untrusted code for full memory safety (with
WebAssembly). In each case, our primary goal is a quan-
titative comparison of the technique’s overhead to that of
ERIM. As we show below, ERIM’s overheads are substan-
tially lower than those of the other techniques. But before
presenting these results, we provide a brief qualitative com-
parison of the techniques in terms of their threat models.
Isolation using
Qualitative comparison of techniques
standard kernel page tables affords a threat model similar
to ERIM’s. In particular, like ERIM, the OS kernel must be
trusted. In principle, isolation using a hypervisor’s extended
page tables (VMFUNC) can afford a stronger threat model,
in which the OS kernel need not be trusted [34].
Isolation using SFI, with or without Intel MPX, affords
a threat model weaker than ERIM’s since one must addi-
tionally trust the transform that adds bounds checks to the
untrusted code. For full protection, a control-ﬂow integrity
(CFI) mechanism is also needed to prevent circumvention of
bounds checks. This further increases both the trusted com-
puting base (TCB) and the overheads. In the experiments
below, we omit the CFI defense, thus underestimating SFI
overheads for protection comparable to ERIM’s.
Instrumenting untrusted code for full memory safety, i.e.,
bounds-checking at the granularity of individual memory
allocations, implicitly affords the protection that SFI pro-
vides. Additionally, such instrumentation also protects the
untrusted code’s data from other outside threats, a use case
that the other techniques here (including ERIM) do not han-
dle. However, as for SFI, the mechanism used to instrument
the untrusted code must be trusted. In our experiments be-
low, we enforce memory safety by compiling untrusted code
to WebAssembly, and this compiler must be trusted.
Next, we quantitatively compare the overheads of these
techniques to those of ERIM.
SFI using MPX We start by comparing the cost of ERIM’s
isolation to that of isolation based on SFI using MPX. For
this, we follow the NGINX experiment of Section 6.2. We
place OpenSSL (trusted) in a designated memory region,
and use MemSentry [30] to compile all of NGINX (un-
trusted) with MPX-based memory-bounds checks that pre-
vent it from accessing the OpenSSL region directly.5 To
get comparable measurements on the (no protection) base-
line and ERIM, we recompile NGINX with Clang version
3.8, which is the version that MemSentry supports. We then
re-run the single worker experiments of Section 6.2.
Figure 3a shows the overheads of MPX and ERIM on
NGINX’s throughput, relative to a no-protection baseline.
The MPX-based instrumentation reduces the throughput of
NGINX by 15-30% until the experiment is no longer CPU-
5This setup reduces the overheads of MPX as compared to the setup
of Section 6.2, which isolates only small parts of OpenSSL. It is also less
secure. Hence, the MPX overheads reported here are conservative.
1234    28th USENIX Security Symposium
USENIX Association
ERIM
MPX
additional overheads that are not included here.
t
u
p
h
g
u
o
r
h
T
d
e
z
i
l
a
m
r
o
N
 1
 0.8
 0.6
 0.4
 0.2
 0
0kb
1kb
2kb
8kb
4kb
File size
1 6kb
3 2kb
6 4kb
1 2 8kb
Extended page tables (VMFUNC) Next, we compare
ERIM to isolation based on extended page tables (EPTs) us-
ing Intel VT-x and VMFUNC. To get access to EPTs, we
use Dune [9] and a patch from MemSentry. We create two
page tables—one maps the trusted region that contains ses-
sion keys, and the other maps the untrusted region that con-
tains all the remaining state of NGINX and OpenSSL. Ac-
cess to the ﬁrst table is efﬁciently switched on or off using
the VMFUNC EPT switch call provided by the MemSentry
patch. This call is faster than an OS process switch since it
does not switch the process context or registers. Since we
use Dune, the OS kernel runs in hypervisor mode. It has the
switch overheads of hypervisor-based isolation using VM-
FUNC but includes the OS kernel in the TCB.
Unfortunately, MemSentry’s patch works only on old
Linux kernels which do not have the page table support
needed for MPKs and, hence, cannot support ERIM. Con-
sequently, for this comparison, we rely on an emulation of
ERIM’s switch overhead using standard x86 instructions.
This emulation is described later in this section, and we val-
idate that it is accurate to within 2% of ERIM’s actual over-
heads on a variety of programs. So we believe that the com-
parative results presented here are quite accurate.
Figure 3b shows the throughput of NGINX protected with
VMFUNC and emulated ERIM, relative to a baseline with
no protection for different ﬁle sizes (we use Linux kernel
v3.16). Brieﬂy, VMFUNC induces an overhead of 7-15%,
while the corresponding overhead of emulated ERIM is 2.1-
5.3%. Because both VMFUNC and ERIM incur overhead
on switches, overheads of both reduce as the switching rate
reduces, which happens as the ﬁle size increases. (The use
of Dune and extended page tables also induces an overhead
on all syscalls and page walks in the VMFUNC isolation.)
To directly compare VMFUNC’s overheads to actual
ERIM’s, we calculated VMFUNC’s overhead as a func-
tion of switch rate. Across different ﬁle sizes, this varies
from 1.4%-1.87% for 100,000 switches/s. In contrast, actual
ERIM’s overhead in the similar experiment of Section 6.2
never exceeds 0.44% for 100,000 switches/s. This difference
is consistent with the microbenchmark results in Table 2.
Kernel page tables (lwCs) Next, we compare ERIM’s
overhead to that of lwCs [33], a recent system for in-process
isolation based on kernel page-table protections. LwCs map
each isolated component to a separate address space in the
same process. A switch between components requires ker-
nel mediation to change page tables, but does not require a
process context switch. To measure lwC overheads, we re-
run the NGINX experiment of Section 6.2, using two lwC
contexts, one for the session keys and encryption/decryption
functions and the other for NGINX and the rest of OpenSSL.
Unfortunately, lwCs were prototyped in FreeBSD, which
does not support MPK, so we again use our emulation of
(a) ERIM vs. SFI using MPX (averages of 3 runs, std. devs.
below 1.9%)
t
u
p
h
g
u
o
r
h
T
d
e
z
i
l
a
m
r
o
N
 1
 0.8
 0.6
 0.4
 0.2
 0
ERIM emulation
VMFUNC
0kb
1kb
2kb
8kb
4kb
File size
1 6kb
3 2kb
6 4kb
1 2 8kb
(b) Emulated ERIM vs. VMFUNC (averages of 3 runs, std. devs.
below 0.9%)
t
u
p
h
g
u
o
r
h
T
d
e
z
i
l
a
m
r
o
N
 1
 0.8
 0.6
 0.4
 0.2
 0
ERIM emulation
LwC
0kb
1kb
2kb
4kb
8kb
1 6kb
3 2kb
6 4kb
File size
1 2 8kb
2 5 6kb
5 1 2kb
1 m b
(c) Emulated ERIM vs. LwC (averages of 5 runs, std. devs. be-
low 1.1%)
Figure 3: Comparison of NGINX throughput with ERIM and
alternative isolation techniques
bound (ﬁle sizes ≥ 64kb). In contrast, ERIM reduces over-
heads by no more than 3.5%. Across all ﬁle sizes, MPX
overheads are 4.2-8.5x those of ERIM.
MPX (more generally, SFI) and ERIM impose overhead
in different ways. MPX imposes an overhead during the ex-
ecution of NGINX (the untrusted component), while ERIM
imposes an overhead on component switches. Consequently,
one could argue that, as the switch rate increases, ERIM must
eventually become more expensive than MPX. While this is
theoretically true, in this experiment, we already observe ex-
tremely high switch rates of 1.2M/s (for ﬁle size 0kb) and,
even then, MPX’s overhead is 8.4x that of ERIM’s overhead.
Further, as explained earlier, for strong security, SFI must
be supported by control-ﬂow integrity, which would induce
USENIX Association
28th USENIX Security Symposium    1235
ERIM’s switch overhead to compare. All experiments re-
ported here were run on Dell OptiPlex 7040 machines with
4-core Intel Skylake i5-6500 CPUs clocked at 3.2 GHz, 16
GB memory, 10 Gbps Ethernet cards, and FreeBSD 11.