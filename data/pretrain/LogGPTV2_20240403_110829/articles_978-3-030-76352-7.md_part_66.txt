### 426 S. Aladhadh

#### 5. Results and Discussion

Table 4 presents the precision, recall, and F1-measure for four classifiers (RF, KNN, NB, and SVM) on English and Italian tweets. For each language, we report results for two scenarios. In the second scenario, the crisis types in the test set were not included in the training set. We tested on three different types of crises: earthquake, flood, and train crash. This was done to ensure that the crisis type did not influence the testing, as some crisis types are similar in terms of feature distribution, which previous studies did not consider. For Italian tweets, we excluded the train crash due to a lack of data.

**Table 3. Results of Previous Studies Using CrisisLexT26 in the Second Scenario**

| **Study** | **Classifier** | **Precision** | **Recall** | **F1** |
| --- | --- | --- | --- | --- |
| Pekaret al. [12] | RF | ≈0.30 | ≈0.00 | <0.10 |
| | KNN | <0.20 | ≈0.00 | ≈0.00 |
| | NB | <0.20 | ≈0.00 | ≈0.00 |
| | SVM | ≈0.60 | <0.10 | <0.10 |
| | MaxEnt | 0.60 | <0.10 | <0.10 |
| Pekaret al. [13] (English) | RF | 0.12 | 0.30 | 0.17 |
| | KNN | - | - | - |
| | NB | 0.17 | 0.68 | 0.20 |
| | SVM | 0.12 | 0.34 | 0.17 |
| Tanevet al. [13] (Italian) | RF | 0.19 | 1.00 | 0.32 |
| | KNN | - | - | - |
| | NB | 0.24 | 0.98 | 0.36 |
| | SVM | 0.24 | 1.00 | 0.39 |

We now compare our best overall results with those from previous studies using the same data for the same purpose. In the cross-event scenario for English, our SVM achieved an F1-score of 0.88, compared to 0.40 and 0.79 by [12, 13], respectively. In the split-across scenario, the best results for English were an F1-score of 0.69 for both train crash and earthquake crises using NB.

For Italian, the F1-score in the cross-event scenario was 0.87 using NB, compared to 0.70 by [13]. In the split-event scenario, the best result was an F1-score of 0.89 for earthquakes and 0.39 for floods. The significant difference in performance between split-event scenarios (flood and earthquake) highlights how performance can vary when applied to different crisis types. For English, the F1-scores for the three different crisis events in the split-event scenario were close, unlike those for Italian. This is likely due to the limited dataset of Italian tweets related to floods; most Italian tweets were about the 2012 earthquake in Italy.

A known issue in tweet classification is prediction overestimation [1]. We observed a drop in precision for English across all classifiers and for two classifiers for Italian. However, the drop in our results is much smaller than in previous studies [13]. For example, the average drop in precision for English in previous research was 0.70 points, compared to 0.35 in our study.

**Table 4. Our Model Results for Cross-Event and Split-Across Scenarios, for English and Italian**

| **Language** | **Scenario** | **Classifier** | **Precision** | **Recall** | **F1** |
| --- | --- | --- | --- | --- | --- |
| English | Cross-Event | RF | 0.80 | 0.84 | 0.82 |
| | | KNN | 0.85 | 0.79 | 0.82 |
| | | NB | 0.87 | 0.84 | 0.86 |
| | | SVM | 0.90 | 0.86 | 0.88 |
| | Split-Across (Flood) | RF | 0.50 | 0.97 | 0.66 |
| | | KNN | 0.50 | 1.00 | 0.67 |
| | | NB | 0.52 | 0.91 | 0.66 |
| | | SVM | 0.50 | 0.98 | 0.66 |
| | Split-Across (Train Crash) | RF | 0.50 | 1.00 | 0.67 |
| | | KNN | 0.50 | 1.00 | 0.67 |
| | | NB | 0.55 | 0.92 | 0.69 |
| | | SVM | 0.50 | 0.95 | 0.67 |
| | Split-Across (Earthquake) | RF | 0.49 | 0.97 | 0.65 |
| | | KNN | 0.50 | 1.00 | 0.67 |
| | | NB | 0.62 | 0.78 | 0.69 |
| | | SVM | 0.49 | 0.93 | 0.64 |
| Italian | Cross-Event | RF | 0.90 | 0.83 | 0.86 |
| | | KNN | 0.85 | 0.87 | 0.86 |
| | | NB | 0.90 | 0.84 | 0.87 |
| | | SVM | 0.90 | 0.83 | 0.86 |
| | Split-Across (Flood) | RF | 0.31 | 0.17 | 0.22 |
| | | KNN | 0.47 | 0.28 | 0.35 |
| | | NB | 0.44 | 0.14 | 0.21 |
| | | SVM | 0.33 | 0.03 | 0.06 |
| | Split-Across (Earthquake) | RF | 0.75 | 0.94 | 0.83 |
| | | KNN | 0.66 | 0.94 | 0.78 |
| | | NB | 0.94 | 0.85 | 0.89 |
| | | SVM | 0.92 | 0.87 | 0.88 |

The drop in precision for Italian is smaller than for English in both our and previous studies, although the drop in our results for Italian (0.20 points) was much smaller than in previous findings (see Tables 2 and 3).

### 6. Conclusion

In this study, we examined the following research question:
- How do linguistic features improve the detection of eyewitnesses in social media during crisis events?

We investigated the effectiveness of using only linguistic features for identifying eyewitnesses in crisis events. By employing the LIWC text analysis tool, we generated 93 features in addition to bigrams. We found that using linguistic features significantly improved the performance in predicting eyewitnesses in social media. Our results outperformed previous studies in both scenarios, especially in the split-across scenario, which is the most challenging. The results confirm the importance of linguistic features for eyewitness detection in social media, as shown in previous studies [8]. Our findings go beyond previous research by demonstrating that a large number of linguistic features can significantly enhance performance. The results of previous studies using the same dataset were included to make the results comparable and to highlight the impact of our approach.

In future research, we will study other languages and conduct further analysis on feature importance to understand which features have the greatest impact on model performance. Additionally, combining linguistic features with other types of features, such as network and metadata, will help us understand the impact of different feature types on locating eyewitnesses in social media.

### References

1. Aladhadh, S., Zhang, X., Sanderson, M.: Tweet author location impacts on tweet credibility. In: Proceedings of the 2014 Australasian Document Computing Symposium, p. 73. ACM (2014)
2. Armstrong, C.L., McAdams, M.J.: Blogs of information: how gender cues and individual motivations influence perceptions of credibility. J. Comput. Mediated Commun. 14(3), 435–456 (2009)
3. Boididou, C., Papadopoulos, S., Kompatsiaris, Y., Schifferes, S., Newman, N.: Challenges of computational verification in social multimedia. In: Proceedings of the 23rd International Conference on World Wide Web, pp. 743–748. ACM (2014)
4. Castillo, C., Mendoza, M., Poblete, B.: Information credibility on Twitter. In: Proceedings of the 20th International Conference on World Wide Web, pp. 675–684. ACM (2011)
5. Castillo, C., Mendoza, M., Poblete, B.: Predicting information credibility in time-sensitive social media. Internet Res. 23(5), 560–588 (2013)
6. Counts, S., Fisher, K.: Taking it all in? visual attention in microblog consumption. ICWSM 11, 97–104 (2011)
7. Dedoussis, E.: A cross-cultural comparison of organizational culture: evidence from universities in the Arab world and Japan. Cross Cultural Manage. Int. J. 11(1), 15–34 (2004)
8. Flanagin, A.J., Metzger, M.J.: The role of site features, user attributes, and information verification behaviors on the perceived credibility of web-based information. New Media Soc. 9(2), 319–342 (2007)
9. Fogg, B., et al.: What makes websites credible?: a report on a large quantitative study. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 61–68. ACM (2001)
10. Freeman, K.S., Spyridakis, J.H.: An examination of factors that affect the credibility of online health information. Techn. Commun. 51(2), 239–263 (2004)
11. Ghosh, S., Sharma, N., Benevenuto, F., Ganguly, N., Gummadi, K.: Cognos: crowdsourcing search for topic experts in microblogs. In: Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 575–590. ACM (2012)
12. Google Social Search: Official Blog (2011). http://bit.ly/2tm4LXJ
13. Gottfried, B.Y.J., Shearer, E.: News use across social media platforms 2016. Pew Research Center 2016 (2016)
14. Gupta, A., Kumaraguru, P.: Credibility ranking of tweets during high impact events. In: Proceedings of the 1st Workshop on Privacy and Security in Online Social Media, p. 2. ACM (2012)
15. Gupta, A., Kumaraguru, P., Castillo, C., Meier, P.: Tweetcred: a real-time web-based system for assessing credibility of content on Twitter. In: Proceedings of the 6th International Conference on Social Informatics (SocInfo). Barcelona, Spain (2014)
16. Han, B., Cook, P., Baldwin, T.: Text-based Twitter user geolocation prediction. J. Artif. Intell. Res. 49, 451–500 (2014)
17. Hofstede, G.: Cultures and organizations: software of the mind (1991)
18. Hofstede, G.: Dimensionalizing cultures: the Hofstede model in context. Online Read. Pychol. Culture 2(1), 8 (2011)
19. Hong, L., Convertino, G., Chi, E.H.: Language matters in Twitter: a large scale study. In: ICWSM (2011)
20. Imran, M., Castillo, C.: Towards a data-driven approach to identify crisis-related topics in social media streams. In: Proceedings of the 24th International Conference on World Wide Web, pp. 1205–1210. ACM (2015)
21. Kang, B., H¨ollerer, T., O’Donovan, J.: Believe it or not? analyzing information credibility in microblogs. In: Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015, pp. 611–616. ACM (2015)
22. Kwak, H., Lee, C., Park, H., Moon, S.: What is Twitter, a social network or a news media? In: Proceedings of the 19th International Conference on World Wide Web, pp. 591–600. ACM (2010)
23. Morris, M., Counts, S., Roseway, A.: Tweeting is believing?: understanding microblog credibility perceptions. In: CSCW, pp. 441–450 (2012)
24. Mourad, A., Scholer, F., Sanderson, M.: Language influences on Twitter geolocation. In: Jose, J.M., et al. (eds.) ECIR 2017. LNCS, vol. 10193, pp. 331–342. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-56608-5 26
25. Obeidat, B., Shannak, R., Masa’deh, R., Al-Jarrah, I.: Toward better understanding for Arabian culture: implications based on Hofstede’s cultural model. Eur. J. Soc. Sci. 28(4), 512–522 (2012)
26. Olteanu, A., Vieweg, S., Castillo, C.: What to expect when the unexpected happens: social media communications across crises. In: Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, pp. 994–1009. ACM (2015)
27. Pal, A., Counts, S.: What’s in a@ name? how name value biases judgment of microblog authors. In: ICWSM (2011)
28. Poblete, B., Garcia, R., Mendoza, M., Jaimes, A.: Do all birds tweet the same?: characterizing Twitter around the world. In: Proceedings of the 20th ACM CIKM International Conference on Information and Knowledge Management, pp. 1025–1030. ACM (2011)
29. Rosa, K.D., Shah, R., Lin, B., Gershman, A., Frederking, R.: Topical clustering of tweets. In: Proceedings of the ACM SIGIR: SWSM (2011)
30. Sakaki, T., Okazaki, M., Matsuo, Y.: Earthquake shakes Twitter users: real-time event detection by social sensors. In: Proceedings of the 19th International Conference on World Wide Web, pp. 851–860. ACM (2010)
31. Schmierbach, M., Oeldorf-Hirsch, A.: A little bird told me, so I didn’t believe it: Twitter, credibility, and issue perceptions. Commun. Q. 60(3), 317–337 (2012)
32. Wagner, C., Liao, V., Pirolli, P., Nelson, L., Strohmaier, M.: It’s not in their tweets: modeling topical expertise of Twitter users. In: Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Conference on Social Computing (SocialCom), pp. 91–100. IEEE (2012)
33. Weerkamp, W., Carter, S., Tsagkias, M.: How people use Twitter in different languages. (1), 1 (2011)
34. Wilson, M.E.: Arabic speakers: language and culture, here and abroad. Topics Lang. Disord. 16(4), 65–80 (1996)
35. Yang, J., Counts, S., Morris, M.R., Hoff, A.: Microblog credibility perceptions: comparing the USA and China. In: Proceedings of the 2013 Conference on Computer Supported Cooperative Work, pp. 575–586. ACM (2013)
36. Yang, J., Morris, M.R., Teevan, J., Adamic, L.A., Ackerman, M.S.: Culture matters: a survey study of social Q&A behavior. In: Fifth International AAAI Conference on Weblogs and Social Media (2011)

### STRAPS 2020: 2nd International Workshop on Smart Data Integration and Processing on Service-Based Environments

#### Preface

More than ever, reducing the cost of data integration by efficiently evaluating queries is a significant challenge, given that today the economic cost in computing cycles (see your cloud invoice), the energy consumption, and the performance required for some critical tasks have become important. Besides, new applications require solving even more complex queries, including millions of sources and data with high volume and variety levels. These new challenges call for intelligent processes that can learn from previous experiences, adapt to changing requirements, and dynamic execution contexts.

The second edition of the workshop (STRAPS 2020) aimed at promoting scientific discussion on how data produced under different conditions can be efficiently integrated to answer simple, relational, and analytical queries. These queries must cope with quality preferences associated with providers, algorithms, and data trust. New scales in volume, velocity, and value related to integrated data collections require adapted solutions providing computing, storage, and processing services deployed on different highly distributed infrastructures and target architectures. With services, data, and algorithms stemming from different and potentially vast numbers of providers, properties like provenance, quality, and trust arise as crucial properties to be quantified, evaluated, and exposed to data consumers. How can data integration in such conditions be smart? This was the central question discussed by workshop participants.

The second edition of the workshop accepted five full research papers (acceptance rate of 38%) focusing on important and timely research problems and hosted two keynotes:

- Building edge and fog applications on the FogStore platform, David Bermbach, TU-Berlin, Germany.
- Enabling Interactivity between Human and Artificial Intelligence, Behrooz Omidvar-Tehrani, Naver Labs, France.

Papers were evaluated under a blind evaluation process through three evaluation rounds by three domain experts who were members of the workshop Program Committee. We are thankful to the Program Committee members for performing a lengthy evaluation process that ensured the accepted papers' quality. Papers presented experience reports in real-life application settings addressing large-scale data integration issues guided by SLA, quality, trust, and privacy and performed through services/microservices-based systems on cloud and multi-cloud architectures.

**Genoveva Vargas-Solar**
**Chirine Ghedira Guégan**
**Nadia Bennani**

### On the Definition of Data Regulation Risk

**Guillaume Delorme¹², Guilaine Talens², Eric Disson², Guillaume Collard¹, Elise Gaget¹**

¹ Solvay, 190 Avenue Thiers, 69006 Lyon, France
{guillaume.delorme, guillaume.collard, elise.gaget}@solvay.com

² Jean Moulin University, iaelyon School of Management, Magellan, 6 Cours Albert Thomas, 69008 Lyon, France
{guilaine.talens, eric.disson}@univ-lyon3.fr

#### Abstract

The rapid development of Information and Communication Technologies (ICT) has led to firms embracing data processing. Scholars and professionals have developed a range of assessments and management methodologies to better address the needs for trust and privacy in ICT. Policymakers aim to establish trust by reinforcing the protection of individuals' rights and privacy, economic interests, and national security through the enactment of laws and regulations. Non-compliance with these norms may harm companies, which in turn need to incorporate them into their risk assessment. We propose to define a new class of risk: "Data Regulation Risk" (DRR) as "a risk originating from the possibility of a penalty from a regulatory agency following evidence of non-compliance with regulated data processing and/or ICT governance and processes and/or information technologies and services." Our definition clarifies the meaning of the defined terms in a given context and adds a specific scope to facilitate and optimize decision-making.

#### Keywords

Data regulation risk, Trust, Information system risk management, Privacy, Information security

#### 1. Introduction

The rapid development of Information and Communication Technologies (ICT) has led to a worldwide increase in data generation and valuation by individuals, public, and private entities. These trends have engendered cyber attacks threatening the confidentiality, integrity, and availability of information systems and data. As a direct consequence, the need has emerged for reinforced, long-lasting trust among the different market actors.

In attempting to limit risks related to data processing and information systems, policymakers have enacted laws and regulations. Non-compliance with these norms can lead to penalties, which companies need to incorporate into their risk assessment. We propose to define this new class of risk: "Data Regulation Risk" (DRR) as "a risk originating from the possibility of a penalty from a regulatory agency following evidence of non-compliance with regulated data processing and/or ICT governance and processes and/or information technologies and services." Our definition aims to clarify the meaning of the defined terms in a given context and add a specific scope to facilitate and optimize decision-making.