426 S. Aladhadh
5 Results and Discussion
Table4 shows precision, recall and F1-measure for the four classifiers RF, KNN,
NB and SVM, by English and Italian tweets. For each language we reported the
results of the two scenarios. In the second scenario, the crisis types in the test
were not included in the training, we performed the testing on three different
typesofcrisis(earthquake,floodandtraincrash).Wedidthistomakesurethat
crisistypehadnoeffectontesting,assomecrisistypesareclosetoeachotherin
termsofdistributionoffeaturesandwhichthepreviousstudiesdidnotconsider.
For Italian tweets we did not include train crash due to lack of data.
Table 3. Results of the previous studies used CrisisLexT26 in the second scenario
Usedby Evaluationmetrics RF KNN NB SVM MaxEnt
Pekaretal.[12] Precision ≈0.30 <0.20 <0.20 ≈0.60 0.60
Recall ≈0.00 ≈0.00 ≈0.00 <0.10 <0.10
F1 <0.10 ≈0.00 ≈0.00 <0.10 <0.10
Pekaretal.[13]English Precision 0.12 – 0.17 0.12 –
Recall 0.30 – 0.68 0.34 –
F1 0.17 - 0.20 0.17 –
Tanevetal.[13]Italian Precision 0.19 – 0.24 0.24 –
Recall 1.00 – 0.98 1.00 –
F1 0.32 – 0.36 0.39 –
We now compare our overall best results to the previous studies that used
the same data for the same purpose. In the cross-event scenario for English, we
obtained 0.88 F1-score by SVM, compared to 0.40 and 0.79 by [12,13], respec-
tively.Inthesplit-acrossscenario,forEnglishthebestresultswere0.69F1-score
in train crash and earthquake crisis by NB.
For Italian language, the F1-score for cross-event scenario was 0.87 by NB,
compared to 0.70 by [13], while for the split-event scenario the best result was
0.89 F1-score for earthquake and 0.39 for flood. As can be seen, there is a big
difference between results of the split-event (flood and earthquake), which high-
lightshowtheperformancecandifferwhenappliedtoothercrisistypes.However,
in the case of English, the F1-scores for the three different crisis events in split-
event were close to each other, unlike those for Italian. We believe this is due to
a limited dataset of Italian tweets in flood crisis; most of the Italian tweets were
related to the earthquake crisis that happened in Italy in 2012.
There is a well known issue in tweet classification in general when applied
to different domains called prediction overestimation [1]. We observed a drop in
precisionforEnglishbetweenacrossandsplitscenariosforallclassifiersandtwo
classifiers for Italian. However, the drop observed in our results is much smaller
than that of previous results [13]. For example, in previous research the average
drop was 0.70 points in English, compared to 0.35 in our study. However, the
Eyewitness Prediction During Crisis via Linguistic Features 427
Table 4. Our models results for the two scenarios Cross-Event and Split-Across, for
two languages English and Italian.
RF KNN NB SVM
Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1
English Cross-Event 0.80 0.84 0.82 0.85 0.79 0.82 0.87 0.84 0.86 0.90 0.86 0.88
Split-Across(Flood) 0.50 0.97 0.66 0.50 1.00 0.67 0.52 0.91 0.66 0.50 0.98 0.66
Split-Across(TrainCrash) 0.50 1.00 0.67 0.50 1.00 0.67 0.55 0.92 0.69 0.50 0.95 0.67
Split-Across(Earthquake) 0.49 0.97 0.65 0.50 1.00 0.67 0.62 0.78 0.69 0.49 0.93 0.64
Italian Cross-Event 0.90 0.83 0.86 0.85 0.87 0.86 0.90 0.84 0.87 0.90 0.83 0.86
Split-Across(Flood) 0.31 0.17 0.22 0.47 0.28 0.35 0.44 0.14 0.21 0.33 0.03 0.06
Split-Across(Earthquake) 0.75 0.94 0.83 0.66 0.94 0.78 0.94 0.85 0.89 0.92 0.87 0.88
drop in precision in Italian is smaller than that for English in both ours and
previous study results, although the drop in our results for Italian (0.20 points)
was much smaller than that in previous findings (see Tables2 and 3).
6 Conclusion
In this study, we examined the following research question:
– How do linguistic features improve detection of eyewitness in social media
during crisis events?
We investigated the effectiveness of using linguistic features only for identify-
ing eyewitness in crisis event. By employing the text analysis tool LIWC, 93
features were generated in addition to bigram. We found that using linguis-
tic features greatly improved performance in prediction of eyewitness in social
media. The results of this study outperformed previous studies’ results in both
scenarios, especially in split-across (the hardest one). Moreover, the results con-
firm findings from previous studies on the importance of linguistic features for
eyewitness detection in social media [8]. The results of this paper go beyond
previous research by showing that use of a large number of linguistic features
increaseperformancesignificantly.Theresultsofthepreviousstudieswhichused
the same dataset were included in the paper, to make the results comparable
with other research and make the impact clear.
In future research, other languages will be studied and further analysis on
features importance, as that can help to understand which features have the
greatestimpactontheperformanceofthemodel.Also,combiningotherlinguis-
tic features with other types of features including network and other metadata
will help to understand the impact of different types of features on locating
eyewitness in social media.
428 S. Aladhadh
References
1. Aladhadh, S., Zhang, X., Sanderson, M.: Tweet author location impacts on tweet
credibility. In: Proceedings of the 2014 Australasian Document Computing Sym-
posium, p. 73. ACM (2014)
2. Armstrong, C.L., McAdams, M.J.: Blogs of information: how gender cues and
individual motivations influence perceptions of credibility. J. Comput. Mediated
Commun. 14(3), 435–456 (2009)
3. Boididou, C., Papadopoulos, S., Kompatsiaris, Y., Schifferes, S., Newman, N.:
Challenges of computational verification in social multimedia. In: Proceedings of
the23rdInternationalConferenceonWorldWideWeb,pp.743–748.ACM(2014)
4. Castillo,C.,Mendoza,M.,Poblete,B.:Informationcredibilityontwitter.In:Pro-
ceedings of the 20th International Conference on World Wide Web, pp. 675–684.
ACM (2011)
5. Castillo,C.,Mendoza,M.,Poblete,B.:Predictinginformationcredibilityintime-
sensitive social media. Internet Res. 23(5), 560–588 (2013)
6. Counts,S.,Fisher,K.:Takingitallin?visualattentioninmicroblogconsumption.
ICWSM 11, 97–104 (2011)
7. Dedoussis,E.:Across-culturalcomparisonoforganizationalculture:evidencefrom
universities in the arab world and Japan. Cross Cultural Manage. Int. J. 11(1),
15–34 (2004)
8. Flanagin,A.J.,Metzger,M.J.:Theroleofsitefeatures,userattributes,andinfor-
mationverificationbehaviorsontheperceivedcredibilityofweb-basedinformation.
New Media Soc. 9(2), 319–342 (2007)
9. Fogg, B., et al.: What makes web sites credible?: a report on a large quantitative
study.In:ProceedingsoftheSIGCHIConferenceonHumanFactorsinComputing
Systems, pp. 61–68. ACM (2001)
10. Freeman, K.S., Spyridakis, J.H.: An examination of factors that affect the credi-
bility of online health information. Techn. Commun. 51(2), 239–263 (2004)
11. Ghosh,S.,Sharma,N.,Benevenuto,F.,Ganguly,N.,Gummadi,K.:Cognos:crowd-
sourcing search for topic experts in microblogs. In: Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 575–590. ACM (2012)
12. Google Social Search: Official Blog (2011). http://bit.ly/2tm4LXJ
13. Gottfried, B.Y.J., Shearer, E.: News use across social media platforms 2016. Pew
Research Center 2016 (2016)
14. Gupta, A., Kumaraguru, P.: Credibility ranking of tweets during high impact
events. In: Proceedings of the 1st Workshop on Privacy and Security in Online
Social Media, p. 2. ACM (2012)
15. Gupta, A., Kumaraguru, P., Castillo, C., Meier, P.: Tweetcred: a real-time web-
basedsystemforassessingcredibilityofcontentontwitter.In:Proceedingsofthe
6th International Conference on Social Informatics (SocInfo). Barcelona, Spain
(2014)
16. Han,B., Cook, P.,Baldwin, T.:Text-based twitter user geolocation prediction. J.
Artif. Intell. Res. 49, 451–500 (2014)
17. Hofstede, G.: Cultures and organizations: software of the mind (1991)
18. Hofstede, G.: Dimensionalizing cultures: he hofstede model in context. Online
Read. Pychol. Culture 2(1), 8 (2011)
19. Hong, L., Convertino, G., Chi, E.H.: Language matters in twitter: a large scale
study. In: ICWSM (2011)
Eyewitness Prediction During Crisis via Linguistic Features 429
20. Imran, M., Castillo, C.: Towards a data-driven approach to identify crisis-related
topicsinsocialmediastreams.In:Proceedingsofthe24thInternationalConference
on World Wide Web, pp. 1205–1210. ACM (2015)
21. Kang, B., H¨ollerer, T., O’Donovan, J.: Believe it or not? analyzing information
credibility in microblogs. In: Proceedings of the 2015 IEEE/ACM International
Conference on Advances in Social Networks Analysis and Mining 2015, pp. 611–
616. ACM (2015)
22. Kwak,H.,Lee,C.,Park,H.,Moon,S.:Whatistwitter,asocialnetworkoranews
media?In:Proceedingsofthe19thInternationalConferenceonWorldWideWeb,
pp. 591–600. ACM (2010)
23. Morris, M., Counts, S., Roseway, A.: Tweeting is believing?: understanding
microblog credibility perceptions. In: CSCW, pp. 441–450 (2012)
24. Mourad, A., Scholer, F., Sanderson, M.: Language influences on tweeter geolo-
cation. In: Jose, J.M., et al. (eds.) ECIR 2017. LNCS, vol. 10193, pp. 331–342.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-56608-5 26
25. Obeidat,B.,Shannak,R.,Masa’deh,R.,Al-Jarrah,I.:Towardbetterunderstand-
ing for Arabian culture: implications based on Hofstede’s cultural model. Eur. J.
Soc. Sci. 28(4), 512–522 (2012)
26. Olteanu, A., Vieweg, S., Castillo, C.: What to expect when the unexpected hap-
pens:Socialmediacommunicationsacrosscrises.In:Proceedingsofthe18thACM
Conference on Computer Supported Cooperative Work & Social Computing, pp.
994–1009. ACM (2015)
27. Pal, A., Counts, S.: What’s in a@ name? how name value biases judgment of
microblog authors. In: ICWSM (2011)
28. Poblete, B., Garcia, R., Mendoza, M., Jaimes, A.: Do all birds tweet the same?:
characterizing twitter around the world. In: Proceedings of the 20th ACM CIKM
International Conference on Information and Knowledge Management, pp. 1025–
1030. ACM (2011)
29. Rosa,K.D.,Shah,R.,Lin,B.,Gershman,A.,Frederking,R.:Topicalclusteringof
tweets. In: Proceedings of the ACM SIGIR: SWSM (2011)
30. Sakaki, T., Okazaki, M., Matsuo, Y.: Earthquake shakes twitter users: real-time
event detection by social sensors. In: Proceedings of the 19th International Con-
ference on World Wide Web, pp. 851–860. ACM (2010)
31. Schmierbach, M., Oeldorf-Hirsch, A.: A little bird told me, so i didn’t believe it:
twitter, credibility, and issue perceptions. Commun. Q. 60(3), 317–337 (2012)
32. Wagner,C.,Liao,V.,Pirolli,P.,Nelson,L.,Strohmaier,M.:It’snotintheirtweets:
modeling topical expertise of twitter users. In: Privacy, Security, Risk and Trust
(PASSAT), 2012 International Conference on and 2012 International Confernece
on Social Computing (SocialCom), pp. 91–100. IEEE (2012)
33. Weerkamp, W., Carter, S., Tsagkias, M.: How people use twitter in different lan-
guages. (1), 1 (2011)
34. Wilson, M.E.: Arabic speakers: language and culture, here and abroad. Topics
Lang. Disord. 16(4), 65–80 (1996)
35. Yang, J., Counts, S., Morris, M.R., Hoff, A.: Microblog credibility perceptions:
comparing the USA and China. In: Proceedings of the 2013 Conference on Com-
puter Supported Cooperative Work, pp. 575–586. ACM (2013)
36. Yang, J., Morris, M.R., Teevan, J., Adamic, L.A., Ackerman, M.S.: Culture mat-
ters: a survey study of social q&a behavior. In: Fifth International AAAI Confer-
ence on Weblogs and Social Media (2011)
Smart Data Integration and Processing
on Service Based Environments
(STRAPS 2020)
STRAPS 2020: 2nd International Workshop
on Smart daTa integRation And Processing
on Service-based environments
Preface
Morethanever,reducingthecostofdataintegrationbyefficientlyevaluatingqueriesis
a significant challenge, given that today the economic cost in computing cycles (see
your cloud invoice), the energy consumption, and the performance required for some
critical tasks have become important. Besides, new applications require solving even
more complex queries, including millions of sources and data with high volume and
variety levels. These new challenges call for intelligent processes that can learn from
previous experiences, that can adapt to changing requirements and dynamic execution
contexts.
The second edition of the workshop (STRAPS 2020) aimed at promoting
scientific discussion on the way data produced under different conditions can be
efficientlyintegratedtoanswersimple,relational,analyticalqueries.Thesequeries
must cope with quality preferences associated with providers, algorithms, and data
trust. New scales in volume, , and value related to integrated data collections require
adapted solutions providing computing, storage, and processing services deployed on
differenthighlydistributedinfrastructuresandtargetarchitectures.Withservices,data,
and algorithms stemming from different and potentially vast numbers of providers,
propertieslikeprovenance,quality,andtrustariseascrucialpropertiestobequantified,
evaluated,andexposedtodataconsumers.Howcandataintegrationinsuchconditions
be smart? This was the central question discussed by workshop participants.
The second edition of theworkshop accepted five full researchpapers (acceptance
rate of 38%) focusing on important and timely research problems, and hosted two
keynotes:
(cid:129) BuildingedgeandfogapplicationsontheFogStoreplatform,DavidBermbach,
TU-Berlin, Germany.
(cid:129) Enabling Interactivity between Human and Artificial Intelligence, Behrooz
Omidvar-Tehrani, Naver Labs, France.
Papers were evaluated under a blind evaluation process through three evaluation
rounds by three domain experts who were members of the workshop Program
Committee. We are thankful to the Program Committee members for performing a
lengthy evaluation process that ensured the accepted papers’ quality.Papers presented
experience reports in real-life application settings addressing large scale data integra-
tion issues guided by SLA, quality, trust, and privacy and performed through
services/microservices-based systems on cloud and multi-cloud architectures.
Genoveva Vargas-Solar
Chirine Ghedira Guégan
Nadia Bennani
On the Definition of Data Regulation Risk
B
GuillaumeDelorme1,2( ) ,GuilaineTalens2,EricDisson2,GuillaumeCollard1,
andEliseGaget1
1 Solvay,190AvenueThiers,69006Lyon,France
{guillaume.delorme,guillaume.collard,elise.gaget}@solvay.com,
PI:EMAIL
2 JeanMoulinUniversity,iaelyonSchoolofManagement,Magellan,6CoursAlbertThomas,
69008Lyon,France
{guilaine.talens,eric.disson}@univ-lyon3.fr
Abstract. TherapiddevelopmentofInformationandCommunicationTechnolo-
gies(ICT)hasledtofirmsembracingdataprocessing.Scholarsandprofessionals
havedevelopedarangeofassessmentsandmanagementmethodologiestobetter
answertheneedsfortrustandprivacyinICT.Withtheambitionofestablishing
trust by reinforcing the protection of individuals’ rights and privacy, economic
interestsandnationalsecurity,policymakersattempttoregulatedataprocessing
through enactment of laws and regulations. Non-compliance with these norms
mayharmcompanieswhichinturnneedtoincorporateitintheirriskassessment.
We propose to define this new class of risk: “Data Regulation Risk” (DRR)as
“ariskoriginatingfromthepossibilityofapenaltyfromaregulatoryagencyfol-
lowing evidence of non-compliance with regulated data processing and/or ICT
governances and processes and/or information technologies and services”. Our
definitionclarifiesthemeaningofthedefinedtermsinagivencontextandaddsa
specificscopetofacilitateandoptimizedecision-making.
Keywords: Dataregulationrisk·Trust·Informationsystemriskmanagement·
Privacy·Informationsecurity
1 Introduction
TherapiddevelopmentofInformationandCommunicationTechnologies(ICT)hasledto
aworldwideincreaseindatagenerationandvaluationbyindividuals,publicandprivate
entities. These trends have engendered cyber attacks threatening the confidentiality,
integrityandavailabilityofinformationsystemsanddata.Asadirectconsequence,the
needhasemergedforreinforced,longlastingtrustamongthedifferentmarketactors.
Inattemptingtolimitrisksrelatedtodataprocessingandinformationsystems,policy