issuing a system call in x86 is to raise a software interrupt.
For example, Linux uses INT $0x80 and Windows uses
INT $0x2E to issue system calls. The interrupt handler routine
is the common gate for all system calls, and parameters of
system calls are passed through general-purpose registers.
Architectural Invariant. In a VM, each software interrupt
triggers an EXCEPTION VM Exit.4
Interrupt-based System Call Interception Algorithm. We
developed an algorithm that intercepts interrupt-based system
calls, shown in Fig. 3D. If the type and number of the
interrupt indicate a system call, the algorithm records all the
registers that could carry the system call‚Äôs parameters and then
generates a notiÔ¨Åcation regarding the system call.
2) Fast System Calls: A fast system call mechanism was
added to x86 with the SYSENTER/SYSEXIT instruction pair
for Intel processors and the SYSCALL/SYSRET instructions for
AMD processors.
Architectural Invariant. The SYSENTER instruction takes
input from Model SpeciÔ¨Åc Registers (MSRs) and general-
purpose registers. For example, SYSENTER‚Äôs target instruction
address is stored in the IA32_SYSENTER_EIP MSR. An MSR
can only be modiÔ¨Åed via a WRMSR instruction, a privileged
instruction that causes WRMSR VM Exits.
Fast system call interception algorithm. Fig. 3E contains
pseudo-code for fast system call interception. The algorithm
uses WRMSR events to identify the address of the system call
entry point in the guest VM. The address is set to execute-
protect so that a guest‚Äôs attempt to execute the system call
entry point will generate an EPT_VIOLATION VM Exit.
C. I/O Access Interception
A primary function of the hypervisor is to multiplex I/O
devices for its VMs, except when a VM is given exclusive
access via an I/O pass-through mode. HAV provides several
VM Exits that the hypervisor can use to capture IO accesses
from guest VMs. We categorize I/O accesses into three types:
4Intel VT-x allows selection of which interrupts cause EXCEPTION VM
Exits via an EXCEPTION_BITMAP.
18181818
Programmed I/O (PIO) is performed through I/O in-
structions, such as IN and OUT. These instructions trigger
IO_ACCESS events when executed in guest mode.
Memory Mapped I/O (MMIO) is performed through
instructions that manipulate memory (e.g., MOV, AND, OR). In
order to trap MMIO, the hypervisor sets memory protection
for the allocated MMIO area so that accesses to this area will
trigger EPT_VIOLATION events.
I/O interrupts are interrupts raised by physical devices to
notify guest VM about I/O-related events (e.g., an incoming
network packet). The presence of a pending interrupt causes
either an EXTERNAL_INT or APIC_ACCESS VM Exit event.
Because of the diversity of I/O devices, details for each
type of device are not covered, and it is up to implementers
to choose an appropriate mechanism.
D. Fine-grained Interception
The EPT feature presented in Section III-B makes it possible
to track a guest‚Äôs execution at the single instruction and mem-
ory access level by setting appropriate access permissions.
However, that Ô¨Åne-grained interception incurs a signiÔ¨Åcant
performance cost. To minimize its impact, an auditor should
make use of that feature only for selective critical protection.
VII. EXAMPLES OF AUDITORS
We expand on the techniques presented in the previous
section to demonstrate how to build auditors using HyperTap.
We present two examples that showcase how RnS monitoring
can be combined (GOSHD and HRKD) and one example that
demonstrates the effectiveness of active monitoring (PED).
A. Guest OS Hang Detection
1) Failure Model: We consider an OS as being in a hang
state if it ceases to schedule tasks. This failure model is similar
to the one introduced in [23]. In multiprocessor systems, it is
possible for the OS to experience a hang on a proper subset of
available CPUs. If that happens, we say that OS is in a partial
hang state, as opposed to a full hang state, in which the OS
is hung on all CPUs.
An example of a software bug that causes hangs in the OS
kernel is a missing unlock (i.e., release) of a spinlock in an exit
path of a kernel critical section. All threads that try to acquire
this lock after the buggy exit path has been executed end up in
a hung state. If the hung kernel thread is in a non-preemptible
code section (e.g., either the kernel itself is non-preemptible,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:24 UTC from IEEE Xplore.  Restrictions apply. 
A
At VM Start:
  Monitor CR_ACCESS events
B
On the first CR_ACCESS event (write to CR3):
  for_each vcpu[i]
  saved_TR[i] = vcpu[i].TR;
At VM Start:
  PDBA_set = {}
  Monitor CR_ACCESS events
At each CR_ACCESS event (CR3 RSP0) // vcpu.TR = TSS
    // V = Kernel_stack_base
    Forward_switch_evt(V)  
At VM Start:
  Monitor EXCEPTION events
D
At each EXCEPTION event (ex_type, int_num):
if ((ex_type == SOFTWARE_INT) && 
    (int_num == 0x80) || (int_num == 0x2e)) {
  invoked_syscall = read_register(EAX);
  para1 = read_register(EBX);
  ...
  Forward_syscall(invoked_syscall, para1, ...);
}
C
E
Integrity checking (e.g., on every VM Exit):
for_each vcpu[i]
  if (save_TR[i] != vcpu[i].TR)
    // TSS has been relocated
    raise_alert();
At VM Start:
  Monitor WRMSR events
On the WRMSR event (IA32_SYSENTER_EIP <- addr):
  syscall_entry = addr;
  Each VCPU: Set execute-protected for 
             the page containing syscall_entry
  Monitor EPT_VIOLATION events
At each EPT_VIOLATION event on vcpu (execute [Addr]):
  if (Addr == syscall_entry) {
    invoked_syscall = read_register(EAX);
    para1 = read_register(EBX);
    ...
    Forward_syscall(invoked_syscall, para1, ...);
  }
Fig. 3: Pseudo-code for each algorithm. (A): Process Counting Algorithm, (B): Thread switch interception, (C): TSS integrity
checking, (D): Interrupt-based system call interception, (E): Fast system call interception
or the thread has purposely disabled preemption), the kernel
hangs on the CPU that is executing the hung thread. The hung
thread may also be holding other locks, which can cascade
into hanging of more threads. In a multiprocessor system a
partial hang usually results in a full hang. The kernel stays in
a partial hang state until the hang propagates to all available
CPUs. However, if the kernel has no other lock dependencies
with the hung threads, it can stay in the partial hang state until
it gets shut down or rebooted.
Distinguishing between OS partial and full hang is im-
portant for two reasons. (i) Previous OS hang detection ap-
proaches use external probes, e.g., heartbeats, to detect OS
hangs. In a multiprocessor system, mechanisms to generate
heartbeats may not be affected by a partial hang, and would
continue to report error-free conditions. (ii) Detecting partial
hangs results in a shorter detection latency, as all full hangs
are preceded by a partial hang. The Guest OS hang detection
(GOSHD) module discussed in this section detects both partial
and full OS hangs.
2) GOSHD Mechanism: GOSHD uses the thread dispatch-
ing mechanism discussed in Section VI-A2 to monitor the
VM‚Äôs OS scheduler. The EPT_VIOLATION and CR_ACCESS
mechanisms in HAV guarantee that GOSHD can capture all
context switch events. If a vCPU does not generate any switch-
ing events for a predeÔ¨Åned threshold time, GOSHD declares
that the guest OS is hung on that vCPU. Because the vCPUs
are monitored independently of each other, GOSHD can detect
both partial hangs and full hangs. From GOSHD‚Äôs perspective,
guest tasks are scheduled independently on each vCPU. Since
GOSHD monitors the absence of context switching events
to detect hangs, it is important to properly determine the
threshold after which it is safe to conclude that the OS is
hung on a vCPU. If this threshold is shorter than the time
between two consecutive context switches, GOSHD generates
false alarms. In order to be safe and fairly conservative, we
proÔ¨Åled the guest OS to determine the maximum scheduling
time slice, and set the threshold to be twice the proÔ¨Åled time.
The numbers are usually on the order of milliseconds, or
at most seconds, and are quicker compared to other hang
detection techniques, such as heartbeat, or timer watchdogs,
which frequently have detection times on the order of tens of
seconds or minutes.
B. Hidden Rootkit Detection (HRKD)
1) Threat Model: Rootkits are malicious computer pro-
grams created to hide other programs from system administra-
tors and security monitoring tools. For example, users cannot
see a hidden process or thread via common administrative
tools, such as Task Manager, PS, or TOP. Autonomic security
scanning tools can also be bypassed simply because their
inspection lists do not contain the hidden programs.
There are many existing techniques to hide a process,
such as Direct Kernel Object Manipulation (DKOM) [27],
physical memory manipulation [28], and dynamic kernel code
manipulation [15]. For example, using those techniques, a
rootkit can stealthily detach the data objects belonging to
the malicious programs from their usual lists (e.g., remove a
task_struct object from Linux‚Äôs task_list). Therefore,
a normal list traversal cannot reveal the detached object. As
exempliÔ¨Åed by previous studies [2], [15], [14], well-crafted
rootkits can escape the detection of guest OS invariant-based
scanning tools.
2) Detection Technique: Our HRKD module employs the
context switch monitoring (Section VI-A) methods to inspect
every process/thread that uses the vCPU, regardless of how
kernel objects are manipulated. Each time a process or a thread
is scheduled to use CPUs, it is intercepted by the module for
further inspection. This interception defeats hidden malware;
it puts malicious programs back on the inspection list.
In order to detect a hidden user process or thread, the
process counting algorithm (Fig. 3A) or thread switch inter-
ception algorithm (Fig. 3B) can be used. These algorithms are
independent of the method by which the guest OS manages
process-related data structures, because they rely only on
19191919
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:24 UTC from IEEE Xplore.  Restrictions apply. 
architectural invariants. Inspection starts from the CR3 or TR
registers. Therefore, the observed number of processes always
reÔ¨Çects the exact number of running processes. This is a trusted
view that can be cross-validated against other views, e.g., a
view from existing VMI tools or views from in-guest utilities,
which may be the target of rootkits. Discrepancies between
these views reveal the presence of hidden user processes and
threads.
3) How Can a Rootkit Hide from HRKD?: A rootkit can
hide from our HRKD by suppressing CR3 access (for user-
level rootkits) or RSP0 access (for kernel-level rootkits) VM
Exits. It can do so by reusing the CR3 (virtual address space)
or RSP0 (kernel stack) of an existing process or kernel thread.
Such attacks are called code injection attacks, which are not
actually rootkits. Nevertheless, our HRKD is not designed to
detect this class of attack.
C. Privilege Escalation Detection (PED)
Ninja [5] is a real-world privilege escalation detection
system that uses passive monitoring. Ninja is included in the
mainline repository for major Linux distributions, including
Debian variants like Ubuntu. Ninja periodically scans the
process list to identify if a root process has a parent process
that
is not from an authorized user (i.e., not deÔ¨Åned in
Ninja‚Äôs ‚Äúmagic‚Äù group). If so, the root process is Ô¨Çagged as
privilege-escalated. Ninja optionally terminates such processes
to prevent further damage to the system. In order to avoid
mistakenly killing setuid/setgid processes, Ninja allows users
to create a ‚Äúwhite list‚Äù of legitimate executables that are not
subjected to its checking rules. The interval between checks
is conÔ¨Ågurable (1s by default).
We implement HT-Ninja, which utilizes HyperTap for de-
tecting privilege escalation attacks. We reuse the OS-level
Ninja‚Äôs checking rules when looking for unauthorized pro-
cesses and make the following changes:
Transform passive monitoring to active monitoring. We
deÔ¨Åne the following events at which a process is checked: (i)
Ô¨Årst context switch of each process; and (ii) every I/O-related
system call (e.g., open, read, write, and lseek). That ensures
that we check before any unauthorized actions, e.g., Ô¨Åle or
network, are conducted.
Using architectural
invariants. The original Ninja uses
Linux‚Äôs /proc Ô¨Ålesystem to obtain information about running
processes. HT-Ninja uses only hardware state, such as the
TR and CR3 registers, to identify current running processes.
HT-Ninja derives OS-speciÔ¨Åc information, such as User ID
(uid) and Effective User ID (euid), from the TSS structure
and RSP register, which can be combined to obtain the exact
thread_info and task_struct objects of each process.
D. Other Uses of HyperTap
The logging capabilities presented in Section VI can also
be used to implement a wide variety of RnS monitors. For
example, there is a class of security tools that depend on
system call
interception [29], [30], [31]. Failure detection
based on machine learning, e.g. [21], can be applied to the
events and states logged by HyperTap.
HyperTap could also be incorporated into the runtime mem-
ory safety technique proposed in [32]. That technique consists
of two steps: (i) compiler analysis and instrumentation, to
guide (ii) runtime memory safety checking. The latter step
requires OS modiÔ¨Åcation to intercept privileged operations,
e.g., MMIO, MMU conÔ¨Åguration, and context switching [33].
Since HyperTap supports those interceptions without altering
the guest OS, it shows promise for being integrated with
runtime checking. Such incorporation would allow a variety of
RnS detectors to be implemented, such as detectors for silent
data corruption, buffer overÔ¨Çow, and code injection. We leave
that integration for future work.
VIII. FUNCTIONALITY EVALUATION
A. Guest OS Hang Detection
1) Experimental Setup: The experiments were conducted
on a guest VM with two vCPUs and 1024MiB of RAM. For
the guest OS, we used the default build of SUSE Enterprise
Linux Server 11 SP1, with and without kernel preemption
enabled. The proÔ¨Åled maximum scheduling timeslice in both
cases was two seconds, and hence the hang detection threshold
was set to four seconds.
2) Experimental Methodology: In order to assess the hang
detection capabilities of GOSHD, we used the fault injection
framework proposed in [34]. As indicated in [34], one of the
common causes of system hangs is improper implementation
and invocation of locking mechanisms (e.g., spinlocks, read-
er/writer locks) that protect access to shared data structures
in the kernel. Based on those Ô¨Åndings, the authors of [34]
identiÔ¨Åed four causes of hang failures: missing spinlock re-
leases, wrong spinlock orderings, missing unlock/lock pairs,
and missing interrupt state restorations. We further extended
that concept to inject transient and persistent faults. A transient
fault is only activated once when the fault location is Ô¨Årst
executed. Conversely, a persistent fault is activated every time
the fault location is executed. Fault injection was repeated with
different types of workloads running on the guest system:
‚Ä¢ Hanoi Tower: ‚ÄúTower of Hanoi‚Äù recursive program.
‚Ä¢ make -j1: serial compilation of libxml.
‚Ä¢ make -j2: compilation of libxml with two tasks in
ApacheBench, which ran on a separate machine.
The Ô¨Årst step of a fault injection experiment is to identify
the injection location(s). We chose to inject faults into core
functions of the Linux kernel and into frequently used kernel
modules, such as ext3, char, and block. By proÔ¨Åling the kernel
using the above workloads, we identiÔ¨Åed 374 locations on the
execution path of the kernel to inject faults.
For each fault location, we started from a clean VM and
then injected a fault while running the workload. There were
Ô¨Åve possible outcomes from each injection:
‚Ä¢ Not Manifested: The fault was injected, but no observ-
able failure was detected.
parallel.
‚Ä¢ HTTP server:
serving of
an HTTP load from
20202020
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:24 UTC from IEEE Xplore.  Restrictions apply. 
	