本次故障正是第一个场景的再现一
在线系统的备份要从两个层面考虑：
要求提供多层防御，以便在任何一个单独的防护措施失效时仍能工作。针对Gmail这种
定可以多存几个副本吧？当然，Google确实有这些资源，但是“深度多层防御”理念中
一个GoogleMusic用户汇报某些之前播放正常的歌曲现在无法播放了。Google Music的
）针对Gmail内部余及备份子系统的故障。
一个会影响底层存储媒介（磁盘）的0-day设备驱动或者文件系统中的问题。
-2012年3月：一次意外删除事故的检测过程
一Gmail虽然有内部工具恢复丢失的数据，但是由于
一如果没有平时经常性的演习是
案例分析
315
362
---
## Page 358
注10参见http://en.wikipedia.org/wiki/Catch-22_(logic)。
存储设施中的——这些设施可以存放更多磁带。为了能够更快地为受影响的用户恢复数据。
不同，GoogleMusic由于数据量太大，加密过的备份磁带是由卡车运往异地存放于独立的
然而数据恢复流程也没有任何时间可以浪费。音频文件虽然备份到磁带上，但是和Gmail
刚购买的音乐，甚至是他们自己辛苦录制的音乐。这个22条军规注1的场景的唯一出路，
承受着来自用户的压力要求我们重新启用数据删除流水线，而无辜用户将会继续丢失刚
Bug产生的原因和过程。如果不修复根源问题，任何数据恢复工作都是做无用功。团队
定位Bug和数据恢复的并行进行。解决问题的第一步是定位真正的Bug根源，以便了解
能及时恢复，用户还会使用GoogleMusic吗？我们怎么能没有注意到这个问题的发生
距离有问题数据删除流水线第一次运行已经超过一个月了，正是那次运行删除了几十万
测程序还简化了一些检测步骤，实际的问题只会更严重。
水线任务大概误删了60万条音频文件，大概影响了2.1万用户。由于这个匆忙编写的检
等待着任务执行结果。3月8日，任务终于完成了，看到结果时所有人都惊呆了：该流
然是不可能的。该团队匆忙编写了一个MapReduce任务来评估问题的影响范围，焦急地
接下来，手动检查分散在多个数据中心内部的百万条，甚至数十亿条文件元数据信息显
扩大。
同时通知了相关的工程师经理，以及 SRE团队。Google Music 的开发者和 SRE组建了
然而，真的是这次删除任务造成的问题吗？该工程师立刻发出了一个最高级别的警报
务的第二阶段执行一
除流水线任务进行了一次运行，当时看起来没有任何问题，于是工程师批准了流水线任
删除机制被抛弃了，在2012年进行了重新设计。在2月6日那一天，更新过的数据删
底删除。随着Google Music使用量呈指数级增加，数据量也飞快地增长，以至于原始的
求音乐文件，以及对应的元数据需要在用户删除它们之后在合理时间范围内在系统中彻
Google的隐私策略强调保护用户的个人数据。在Google Music服务中，该隐私策略要
316
团队决定在调查根源问题的同时并行进行异地备份磁带的恢复操作（这将耗时很长）。
条不该删除的音频数据。现在还有恢复数据的希望吗？如果这些数据无法恢复，或者不
一个小团队专门调查这个情况，同时该数据删除流水线也被暂时停止了，以防问题范围
评估问题严重性
就是修复根源问题，并且要快。
问题的解决
呢！
第26章数据完整性：读写一致
一真正删除对应的音频数据。
---
## Page 359
注11
运载着这些磁带抵达了数据中心。
算了4个小时，得出5,337盘磁带需要从异地存储设施中恢复。8小时后，
在3月9日午夜的时候，SRE终于将5，475个恢复请求输入了系统中。磁带备份系统计
会引入很多错误。仅仅是这一点准备工作，就需要SRE写代码来完成。
果由人每分钟输入一条命令，光输入就需要超过三天的时间。人在这个过程中很可能还
很难，从这些磁带中真正读取数据就更难了。Google自己定制的磁带备份软件设计时没
式开始了。从分布在异地离线存储设施中的几千盘磁带中请求超过1.5PB的数据看起来
第一批数据的恢复。数据恢复团队确认了备份磁带之后，第一批数据恢复在3月8日正
来好像不存在的Bug。
误的数据。但是经过详细调查，这个可能性被排除了。这个团队只能继续寻找这个看起
GoogleMusic的底层的数据存储服务提供了错误数据，以至于数据删除流水线删除了错
同时，负责调查根源问题的团队找到了一个潜在问题，结果却最终被证伪：他们本以为
想办法恢复这161,000条无备份数据。
更不幸的是，60万条丢失的数据中只有436,223条数据存在备份，也就意味着其他
读取设备损坏，以及其他不可预料的问题。
并且之后数据中心的技术人员需要清理出足够的场地来放置这些磁带。接下来还要通过
通过这个方法，团队评估得出第一批恢复过程需要将5000盘磁带用卡车全部运回来。
文件和磁带备份系统中注册的备份文件一一对应，然后再将备份文件与具体的物理磁带
作了对应的数据恢复新工具。利用这个新工具，新团队开始缓慢地首先将几十万条音频
[Kri12]）。磁带备份团队由于在DiRT演习中刚刚评估过子系统的能力和限制，刚刚制
整个事件中有一个好消息：公司组织的年度灾难恢复演习刚刚在几周前结束（参见文献
在美国西海岸时间3月8日下午4：34收到了恢复通知。
进行。首先，第一批超过50万条音频数据被选中进行恢复，负责磁带备份系统的团队
代码，试着修复根源问题。由于问题的根源尚不清晰，整个数据恢复过程会按数个阶段
工程师分为两组。经验最丰富的SRE 负责数据恢复，同时开发者负责分析数据删除逻辑
有考虑过这么大的恢复任务，所以第一批数据的恢复过程被切分成5,475个小任务。如
161，000条数据在备份之前就已经丢失了。数据恢复团队最后决定先启动恢复流程，再
一个极为复杂和耗时的流程来从磁带上读取这些数据。同时，还要处理磁带损坏、磁带
一对应。
需要招聘熟练的软件开发工程师（参见文献[Jon15])。
发
在实践中，
者
编写代码对大部分SRE来说并不是什么难事，
，因为大部分SRE都是很有经验的软件开
案例分析
注11
一系列卡车
317
364
---
## Page 360
365
318
最终，Google Music 团队找到了重构过的数据删除流水线中的 Bug。为了更好地解释这
文件。整个过程持续了一周时间，最终全部数据恢复工作结束。
远程遥控这些受影响的用户的GoogleMusic客户端软件自动重新上传3月14日之后的
但是，161,000条中的一小部分是由用户上传的。GoogleMusic团队发出了一条指令，
到3月11日早晨，超过99.95%的恢复任务已经完成了，剩余文件的余磁带的取回也
余的编码。于是还需要额外安排卡车去取回这些余磁带，一并再去取回第一次漏掉的
过程还被17个坏磁带拖慢了。由于已经预计到会有磁带损坏，写人时已经采用了带兀
的分布式存储中。其他1,862个磁带在第一次运输中完全漏掉了。更糟的是，整个恢复
日早晨，436,223个文件中只有74%的文件从3,475盘磁带上成功转移到了数据中心中
虽然之前有DiRT的经验，1.5PB的海量数据恢复还是比预计时间长了2天。到3月10
机终于恢复了运行，开始将几千个恢复任务逐渐写入分布式存储中。
磁带管理机厂商提供的基于机器手臂模式的装载方式快很多。3个多小时后，磁带管理
的几千盘磁带装进磁带管理机中。由过去的DiRT演习证明，这种手动装载的方式要比
以便给这次大型数据恢复工作让路。接下来，工作人员又开始一点一点手动将刚刚抵达
在卡车奔波在路上时，数据中心的技术人员清空了几个磁带管理库，撤下了几千盘磁带
要分多个阶段进行，每个阶段操作不同的数据存储服务。
对一个包括很多子系统和存储服务的大型服务来说，彻底删除已经标记为删除的数据需
个Bug，我们需要先了解大型离线数据处理系统的演变过程。
解决根源问题
分都是商店出售的，或者是推广性质的音频，原始文件都还存在。这些文件很快被恢复了。
161,000条音频数据，这些数据在备份进行之前就被错误地删除了。这些文件中的大部
第二批数据恢复。当第一批数据恢复过程结束之后，团队开始关注如何恢复剩下的
了7天时间，我们恢复了1.5PB的用户数据，7天之内的5天用于真正的数据恢复操作。
13日，数据恢复过程得以继续，最终436.223条文件再一次可以被用户访问了。仅仅花
严重的用户直接可见的生产问题——该问题又消耗了恢复团队整整两天的时间。在3月
同时，GoogleMusic生产系统的报警信息又来了，虽然与数据丢失不相关，但却是非常
恢复过程的最后步骤，同时不停校验结果以确保这个恢复过程能够成功执行。
完成，才能使用户真正可以访问。GoogleMusic团队开始在数据的一小部分上进行数据
在进行中。虽然数据已经安全地存到了分布式存储中，但还有额外的数据恢复过程需要
1,862盘磁带。
第26章数据完整性：读写一致
---
## Page 361
注12在我们的经验里，云计算工程师经常拒绝针对生产环境中的数据删除设置报警，
要验证”、“纵深防御”等手段来保护自己。（注意，这里可不是说让一个初学者来管理
系统已经足够了解，也不要轻易将某些失败场景定性为不可能。我们可以通过“信任仍
保持初学者的心态
SRE的基本理念在数据完整性上的应用
们可以提前检测类似的大规模删除问题，以便在用户发现之前检测和修复这类问题。注12
种数据竞争问题出现的可能性。同时，我们增强了生产系统的监控和报警系统，使得它
经历过这次数据恢复之后，GoogleMusic团队重新设计了该流水线任务，彻底消除了这
题经常性地发生。
率的提升。当整个流水线任务重构时，这个概率被再次大幅提高了，直接导致了这个问
障整个Google Music的隐私策略不受影响。结果，这导致了这种数据竞争问题发生的概
但是当整个流水线的第一阶段花费的时间越来越长时，工程师加入了很多性能优化以保
GoogleMusic的数据删除流水线任务设计时考虑了协调机制，并且加入了很多防错机制。
的数据就会被删除掉。
据，绝大部分时间来说，整个流水线任务是正确的。但是一旦发生数据竞争问题，错误
越来越多的数据可能会受到该数据竞争的影响。这种场景是随机化的一
一开始，这种数据竞争问题可能只会影响到一小部分数据。但是随着数据数量的增加
个新条件下就不再成立了。
据量的增长，每个阶段需要更多时间才能完成。最终，当初设计时候的某些假设，在这
代码可以大幅度简化某些逻辑，否则这个阶段的处理逻辑可能很难并行化。但是随着数
例如，该任务两个阶段被设计成严格隔离的，运行时间相隔3个小时。这样第二阶段的
争问题。
数据存储。如果没有仔细调整过这些短期数据的生命周期，这个方法可能会引入数据竞
为了避免这些问题，云计算工程师通常会在第二阶段保存一些短期数据，用它们来进行
而崩溃。
子系统造成很大压力。这种分布式操作会影响到用户，同时导致某些服务由于压力过大
为了使数据操作服务更快地结束，整个过程可以并行运行在几万台机器上，这会给很多
大规模部署的、复杂的服务中会产生很多无法完全被理解的Bug。永远不要认为自已对
10倍于观察到的95%值），这样的报警规则有利于检测到极端情况。
更有用的报警是全
然而
这种报警的关注点其实应该在全局范围，
局汇总过后的删除速度，
SRE的基本理念在数据完整性上的应用
网值可以是一个比较极端的数字（例如
而非局部。
比起针对每个用户的删
因为删除速度随着
一对绝大部分数
319
367
---
## Page 362
368
320
覆盖，能够用合理的成本来覆盖非常广泛的失败场景。
每个策略都可能失败。最好的数据完整性保障手段一定是多层的一
题能够及时恢复的重要条件是能够很快检测到问题的存在。在一个不断变化的环境中，
就算是最安全的系统也会受到实现中的Bug，以及操作错误的影响。保障数据完整性问
纵深防御
那么中间一定会出现问题。
手段是必备的。然而，如果没有专人负责数据问题，而是让有其他工作的工程师兼职来弄
性的演习来保障可用性。由于人类天生不适合持续性、重复性地进行测试活动，自动化
不经常使用的系统组件一定会在你最需要的时候出现故障。数据恢复计划必须通过经常