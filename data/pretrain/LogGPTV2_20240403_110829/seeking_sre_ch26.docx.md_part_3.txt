确实，向分布式系统添加额外的跃点会导致延迟增加，并利用更多的计算机资源（CPU
和 RAM）。因此，以极其高效的方式编写边三轮代理非常重要。许多代理仍用本机
C/C++ 编写。但是，稍微考察性能问题并探讨详细信息非常重要。
在性能指标中，有两个主要考量是令人感兴趣的：
吞吐量
:   每单位时间可以推送多少数据（每秒请求数、每秒连接数、每秒事务数等）？
尾部延迟
:    随着吞吐量的提升，组件在延迟直方图"尾部"的性能如何？也就是说，与
    P99 甚至 P99.9 相比，P50 处的每次事务延迟如何？
虽然吞吐量很重要，但实际上，它只对最大型的公司很重要。这是因为对于较小的公司来说，开发人员的时间几乎总是比基础设施成本更有价值。在考虑总拥有成本（TCO）时，总体吞吐量才真正开始重要。
相反，尾部延迟最终成为小型*和*大型企业最重要的性能指标。这是因为高尾部延迟的原因很难理解，并导致大量的开发人员和操作员认知负载。对于组织来说，工程师时间通常是最宝贵的资源，而调试尾部延迟问题是工程师所做的最耗时的事情之一。
因此，边三轮代理最终成为一把双刃剑。代理为系统添加了大量功能，以至于对于除了性能最密集的应用程序外，只要尾部延迟属性*不受代理本身影响*。这主要是因为代理的性能，特别是尾部延迟属性如此重要的原因。如果代理构成了观察整个分布式系统的基础，如果代理本身具有很大的可变性，如何信任数据？这就是为什么最好的代理仍然用本机代码编写，其性能目标类似于操作系统和数据库。
## 精简库和上下文传播
 服务网格可以为应用程序提供的好处非常大，而应用程序无需执行任何操作。然而，本章迄今掩盖了一个不幸的现实，即无论网格不网格，应用程序仍然需要发挥作用，主要是围绕上下文传播。
上下文传播是从入口网络调用获取上下文，并可能对其进行修改，然后传递到出口网络调用的行为。如何做到这一点是语言或平台相关的，而不是边三轮代理可以提供的东西。尽管传播的上下文有许多用途，但与服务网格相关的主要用途是请求
ID 和跟踪上下文的传播。对于基于 HTTP 的体系结构，这主要通过 HTTP
头来完成，例如`x-request-id` 和 Zipkin
`x-b3-traceid`头。服务网格的用户需要提供特定于应用程序和语言的精简库，以使用户能够轻松传播所需的
HTTP
标头。虽然它超出了本章的范围，但我希望看到开发人员在未来几年内为此目的使用的库的一些融合。
精明的读者现在可能会问："但你说服务网是神奇的！我仍然要做些什么？"
确实，如果需要完整的功能，开发人员仍然需要在应用程序层参与网格。但是，这仍然只能算是极少的代码和功能，否则将需要在每种语言和框架中重复实现。
## 配置管理（控制平面与数据平面）
  到目前为止，我们已经讨论了服务网格在构建可靠的微服务架构时可以提供的许多功能。我们还没有讨论如何配置整个系统，这可能变得非常复杂。首先，一些定义：
数据平面
:    这是系统实际参与转发网络流量的部分。这包括负载平衡、连接池、请求修改和响应修改等。数据平面涉及每个请求和响应的每个字节。
控制平面
:    这是系统设置拓扑并提供针对任何给定时间发生的情况的高级配置的部分。这包括路由表、后端主机注册、流量修改规则、配额等。
使用地铁类比，数据平面是从 A 点到 B
点的实际地铁车流。控制平面是一个高级开关系统，偶尔切换轨道，也就是关于多少列火车应该在何时间怎样前进，等等。
部署服务网格体系结构时，必须分离系统中的不同组件。边三轮代理本身是数据平面。但是，需要一个控制平面，该控制平面可以让用户进行自定义的配置，这通常相当有意见，并且取决于正在使用的部署和配置管理系统，并将该配置转换为边三轮代理可以理解的格式。然后，必须将配置分发到所有边三轮代理。
通常，对于代理的依赖越少的时候，系统配置就越有影响力。例如，在裸机的数据中心中运行预处理的用户，其服务发现过程与在云提供商托管的
CaaS
系统内运行的用户大不相同。边三轮代理/数据平面的目标是提供一致、可用的配置
API。在最简单的形式中，集中式配置生成器可能的工作方式为：
1.  查找需要修改数据平面的全局系统更改。
2.  为系统中的每个代理生成新配置。
3.  通过某种机制将配置更改部署到系统中的每个代理。
4.  强制代理重新加载其配置。
基于 HAProxy 的 SmartStack
基本上可以像以前描述的那样工作，并且已被许多公司成功部署。
 更复杂的边三轮代理提供动态配置 API，如
[#service_mesh_with_dynamic_configuration_a](#service_mesh_with_dynamic_configuration_a)
所示。API用于提供路由表更改，后端主机更改，要侦听的端口以及收到新连接时要执行的操作。这种架构允许集中管理服务来控制所有代理，就像中央控制室监督地铁网络。
通常可以移动到中心位置的逻辑越多，系统就越容易管理； 一个权力集中的 API
只允许每个边三轮代理载入最基础的引导配置，且基本上只知道如何与管理服务器通信。从那时起，所有配置都受到集中管理，从而避免了任何额外的基础架构来部署配置文件，消耗流量，强制代理重新启动等等。
![具有动态配置 API
的服务网格](media/rId34.png){width="2.551839457567804in"
height="1.2408016185476816in"}
具有动态配置 API 的服务网格
# 实践中的服务网格
 尽管服务网格概念最近才在微服务社区中获得青睐，但已经有一些相互竞争的解决方案和大型部署。代理解决方案包括
HAProxy、NGINX、Linkerd、Traefik 和 Envoy。完全托管的解决方案包括
SmartStack（基于 HAProxy 构建）和 Istio（基于 Envoy 构建，现在也支持
Linkerd）。以下是 Lyft 从单一应用程序到基于 Envoy
之上的完整服务网格体系结构的过渡的简短描述。这绝不是服务网格的唯一成功故事，但它是我熟悉的一个（我是
Envoy
的创建者），并希望提供一些通用服务网格架构的具体实现，也就是本章大多数篇幅在讨论的东西。
## Envoy 在 Lyft 的起源与发展
   到 2015 年初，Lyft 拥有一个由 MongoDB 支持的主要单体 PHP
应用程序，以及数十个用 Python 编写的微服务。当时的应用程序部署在 AWS
内的单个区域中。Lyft
已决定跳转到微服务架构，其原因几乎与其他团队一样：解耦和提高敏捷性。然而，早期的分解尝试并不顺利，主要是由于本章已经提出的所有原因。*网络总是不可靠*。Lyft
开发人员在调试网络故障和尾部延迟问题时遇到了巨大的麻烦。在某些情况下，计划服务被中止，并且更多功能被添加到单体应用中，因为网络被认为太不可靠，难以支持工作负载。重要的是还要注意
Lyft 目前没有类似 SRE
的职称；预计所有开发人员都将以高可靠性运营其服务。（行业向 DevOps
的转变非常有趣并且值得用单独的章节来讨论；这里只提到涉及 Lyft
的服务网格迁移相关的一些话题。）
Envoy 于 2015
年初开始开发。该项目的目标是构建一个性能非常高的网络基板，让 Lyft
的开发人员能信任并对他们完全透明。这不是一夜之间发生的。Envoy 首先部署在
Lyft 作为边缘代理，以取代和增强现有的 AWS
ELB。从那一刻开始，可观察性的提升和多协议支持的功能开始证明它对产品问题的分类和调试非常有价值。
在部署 Envoy 作为 Lyft 的边缘代理后，开发工作拓展到 MongoDB
的稳定方面。我们添加了一个二进制 JSON （BSON） 解析器，该解析器可以检查
MongoDB 的流量以及有助于限制应用程序和数据库之间连接数的原始 TCP
代理。稍后，我们在应用程序和数据库之间添加了全局速率限制支持。 Envoy
允许在我们的所有应用程序堆栈中即刻提供这些增强功能，而无需修改应用逻辑。
随着时间的推移，Envoy
被部署作为一个副车与Lyft的每一个应用程序。在此期间，我们删除了所有内部的集中负载均衡器，构建了最终一致的服务发现系统和
API，并在所有 Envoy 之间部署了密集的 HTTP/2 网格。服务网格的存在为 Lyft
开发人员开启了大量的功能，本章已经介绍了所有这些功能。