corresponding execution unit, possibly before scheduling
older μ-ops. After executing the μ-op, the reservation
station stores its result, updates the μ-ops that depend on
it, and marks the corresponding ROB entry as completed.
c) Retirement: The Retirement Unit retires com-
pleted μ-ops in their original program order by committing
the architectural state for memory/branch operations and
freeing up any allocated physical registers. In case of
a mispredicted branch, the Retirement Unit retires the
oﬀending branch instruction, ﬂushes the ROB, resets the
reservation station, and replays the execution stream from
the correct branch. The Retirement Unit also detects faulty
instructions and generates precise exceptions once the
oﬀending μ-op reaches a non-speculative state.
C. Speculative Execution
To predict the target of a branch, modern proces-
sors feature a Branch Prediction Unit (BPU). The BPU
predicts the branch target such that the processor can
execute a stream of instructions speculatively. In case the
predicted path is wrong, the processor reverts the state to
the last known useful state and starts executing from the
correct branch instead. There are several instances where
speculation occurs, such as: conditional branches, indirect
branches and calls, return instructions and transactions.
Recent Intel processors employ a Branch Order Buﬀer
(BOB) to keep track of all in-ﬂight branches and whether
the branch is in retired or speculative state [19],
[20].
The BOB is also used to implement memory transactions
through Transactional Synchronization eXtensions (TSX).
In particular, the xbegin instruction marks the start of a
transaction and adds an entry to the BOB as a checkpoint.
Transactions end once the processor encounters an xend
instruction, an xabort instruction, or a fault. In case of
an xend instruction, the processor commits the transac-
tion, otherwise the processor rolls back the transaction by
reverting back to the original state before the xbegin.
D. In-ﬂight Data
There are many potential sources of in-ﬂight data in
modern CPUs such as the Re-Order Buﬀer (ROB), the
Load and Store Buﬀers (LBs and SBs) [21], [22], [23],
[24], the Line Fill Buﬀers (LFBs), and the Super Queue
(SQ) [25], [26]. We focus here on two prominent examples:
store buﬀers and line ﬁll buﬀers.
Store Buﬀers (SBs) are internal buﬀers used to track
pending stores and in-ﬂight data involved in optimizations
such as store-to-load forwarding [21], [24]. Some modern
processors enforce a strong memory ordering, where load
and store instructions that refer to the same physical
address cannot be executed out-of-order. However, as ad-
dress translation is a slow process, the physical address
might not be available yet, and the processor performs
memory disambiguation to predict whether load and store
instructions refer to the same physical address [27]. This
enables the processor to speculatively execute unambigu-
ous load and store instructions out-of-order. As a micro-
optimization, if the load and store instructions are ambigu-
ous, the processor can speculatively store-to-load forward
the data from the store buﬀer to the load buﬀer.
Line Fill Buﬀers (LFBs) are internal buﬀers that the
CPU uses to keep track of outstanding memory requests
and perform a number of optimizations such as merging
(cid:26)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:31 UTC from IEEE Xplore.  Restrictions apply. 
ﬂush the buﬀer that we will later use in our covert channel
to leak the secret that we speculatively access in Line 6.
Speciﬁcally, when executing Line 6, the CPU speculatively
loads a value from memory in the hope it is from our newly
allocated page, while really it is in-ﬂight data from the
LFBs belonging to an arbitrarily diﬀerent security domain.
/* Flush flush & reload buffer entries. */
for (k = 0; k < 256; ++k)
flush(buffer + k * 1024);
/* Speculatively load the secret. */
char value = *(new_page);
/* Calculate the corresponding entry. */
char *entry_ptr = buffer + (1024 * value);
/* Load that entry into the cache. */
*(entry_ptr);
/* Time the reload of each buffer entry to
for (k = 0; k < 256; ++k) {
see which entry is now cached. */
t0 = cycles();
*(buffer + 1024 * k);
dt = cycles() - t0;
if (dt < 100)
++results[k];
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
multiple in-ﬂight stores. Sometimes, data may already be
available in the LFBs and, as a micro-optimization, the
CPU can speculatively load this data (similar optimiza-
tions may also be performed on load/store buﬀers, etc.). In
both cases, modern CPUs that implement aggressive spec-
ulative execution may speculate without any awareness of
the virtual or physical addresses involved. In this paper,
we speciﬁcally focus on LFBs, which we found particularly
amenable to practical, real-world RIDL exploitation.
III. Threat Model
We consider an attacker who wants to abuse speculative
execution vulnerabilities to disclose some conﬁdential in-
formation, such as private keys, passwords, or randomized
pointers. We assume a victim Intel-based system running
the latest microcode and OS version, with all the state-of-
the-art mitigations against speculative execution attacks
enabled. We also assume the victim system is protected
against other classes of (e.g., software) vulnerabilities.
Finally, we assume the attacker can only run unprivileged
code on the victim system (e.g., JavaScript sandbox, user
process, VM, or SGX enclave), but seeks to leak informa-
tion across arbitrary privilege levels and address spaces.
IV. Overview
Figure 2 illustrates the main steps and the underlying
mechanism enabling the RIDL leaks. First, as part of its
normal execution, the victim code, in another security
domain, loads or stores some secret data2. Internally, the
CPU performs the load or store via some internal buﬀers—
Line Fill Buﬀers (LFBs) in the RIDL instances considered
in this paper. Then, when the attacker also performs a
load, the processor speculatively uses in-ﬂight data from
the LFBs (with no addressing restrictions) rather than
valid data. Finally, by using the speculatively loaded data
as an index into a Flush + Reload buﬀer (or any other
covert channel), attackers can extract the secret value.
Attacker Process
Secret
FLUSH + RELOAD
Buffer
Dependent
Load
Speculative
Load
Victim Process
Secret
Load/Store
Line Fill Buffer
Fig. 2: An overview of the RIDL attack.
A simple example of our attack is shown in Listing 1.
As shown in the listing, the code is normal, straight-line
code without invalid accesses (or, indeed, error suppres-
sion), which, as we will show, can also be implemented
in managed languages such as JavaScript. Lines 2–3 only
2Strictly speaking, this is not even a hard requirement, as we can
also leak data from inactive code by forcing cache evictions.
(cid:26)(cid:18)
}
Listing 1: An example of RIDL leaking in-ﬂight data.
When the processor eventually detects the incorrect
it will discard any and all modiﬁca-
speculative load,
tions to registers or memory, and restart execution at
Line 6 with the right value. However, since traces of
the speculatively executed load still exist at the micro-
architectural level (in the form of the corresponding cache
line), we can observe the leaked in-ﬂight data using a
simple (Flush + Reload) covert channel—no diﬀerent
from that of other speculative execution attacks. In fact,
the rest of the code snippet is all about the covert channel.
Lines 8-10 speculatively access one of the entries in the
buﬀer, using the leaked in-ﬂight data as an index. As
a result, the corresponding cache line will be present.
Lines 12-21 then access all the entries in our buﬀer to see
if any of them are signiﬁcantly faster (indicating that the
cache line is present)—the index of which will correspond
to the leaked information. Speciﬁcally, we may expect two
accesses to be fast, not just the one corresponding to the
leaked information. After all, when the processor discovers
its mistake and restarts at Line 6 with the right value, the
program will also access the buﬀer with this index.
Our example above use demand paging for the loaded
address, so the CPU restarts the execution only after
handling the page-in event and bringing in a newly mapped
page. Note that this is not an error condition, but rather
a normal part of the OS’ paging functionality. We found
many other ways to speculatively execute code using in-
ﬂight data. In fact, the accessed address is not at all
important. As an extreme example, rather than accessing a
newly mapped page, Line 6 could even dereference a NULL
pointer and later suppress the error (e.g., using TSX). In
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:31 UTC from IEEE Xplore.  Restrictions apply. 
general, any run-time exception seems to be suﬃcient to
induce RIDL leaks, presumably because the processor can
more aggressively speculate on loads in case of exceptions.
Clearly, one can only “speculate” here, but this behav-
ior seems consistent with existing vulnerabilities [1], [3].
Similarly, we noticed the address accessed by the attacker
should be part of a page-aligned cache line to induce leaks.
While the basic concept behind in-ﬂight data may be
intuitive, successfully implementing an attack turned out
to be challenging. Unlike prior work that builds on well-
documented functionality such as branch prediction, page
tables and caches, the behavior of internal CPU buﬀers
such as LFBs is largely unknown. Moreover, diﬀerent
microarchitectures feature diﬀerent types of buﬀers with
varying behavior. Furthermore, based on publicly available
documentation, it was not clear whether many of these
buﬀers even exist. For our attack to succeed, we had to
resort to extensive reverse engineering to gain a better
understanding of these buﬀers and their interaction with
the processor pipeline. The next section discusses how we
determine exactly which in-ﬂight data buﬀers are respon-
sible for the leak, how to manipulate the processor state
in such a way that we can perform a speculative load
that uses the in-ﬂight data (so that we can use our covert
channel to obtain the content), and how to ensure the data
we want to leak actually ends up in the buﬀers.
V. Line fill buffers and how to use them
To perform the attack described in the previous section,
we ﬁrst need to understand the core building blocks for the
RIDL variant considered in the paper: the Line Fill Buﬀers
(LFBs). Using reverse engineering and experimentation,
we verify that we do indeed leak from the LFBs (and not
some other buﬀer) and examine their interaction with the
processor pipeline. After that, we discuss how attackers can
control what to leak by synchronizing with the victim.
In Intel processors, the LFB performs multiple roles: it
enables non-blocking caches, buﬀers non-temporal memory
traﬃc [28], [29], [30], [31], and performs both load squash-
ing [32], [33], [34] and write combining [35], [36], [37]. To
help the reader understand the remainder of this paper,
we now brieﬂy discuss each of these functions.
Non-blocking cache. Cache misses have a serious im-
pact on performance as they block the data cache until
the data is available. To allow non-blocking reads, the
LFB implements multiple Miss Status Holding Registers
(MSHRs) to track the physical addresses of outstanding
requests until the data is available [38], [39]. For example,
the Haswell microarchitecture maintains 10 L1 MSHRs
in its LFB to service outstanding L1 cache misses [40],
[41] These MSHRs free up the L1d cache to allow load
instructions that hit the L1d cache to bypass cache misses.
Load squashing. To further optimize performance, the
LFB squashes multiple load misses to the same physical
address. If there is already an outstanding request in the
LFB with the same physical address, the processor assigns
the same LFB entry to a load/store with the same address.
Write combining. For weakly-ordered memory, the pro-
cessor keeps stores to the same cache line within the LFB
to perform write combining. That is, the processor merges
multiple stores in a single LFB entry before writing out
the ﬁnal result through the memory hierarchy.
Non-temporal requests. Finally, modern processors
support non-temporal memory traﬃc where the program-
mer already knows that caching the data is of no beneﬁt
at all. In that case, the processor performs non-temporal
loads and stores exclusively through the LFB.
A. Solving a RIDL: LFB leaks on loads and stores
Unaware of the source of the RIDL leaks initially,
we discovered that they originate from the LFBs, rather
than from other processor state, by conducting several
experiments on a workstation featuring an Intel Core
i7-7700K (Kaby Lake). For our experiments, we use a
kernel module to mark memory pages in our victim thread
as write-back (WB), write-through (WT), write-combine
(WC) and uncacheable (UC) [42], [43]. We use Intel TSX
to implement the attack for our analysis and perform
10, 000 rounds to leak the data during every run of the
experiment. Furthermore, we run every experiment 100
times and report the average.
Fig. 3: In each pair of bars, one bar shows the LFB hit count,
and the other one the number of attacks. With SMT, we always
leak the secret. Without SMT and no victim code, RIDL only
reads zeros, but with victim and attacker in the same hardware
thread, we still leak the secret in most cases (top/red bar), while
occasionally ﬁnding the value the CPU should have loaded.
Our ﬁrst experiment performs the attack discussed ear-
lier against a victim running in the same or other hardware
thread and repeatedly storing a secret to a ﬁxed memory
address. We then compare the number of hits in the LFB,
measured using the lfb_hit performance counter, to the
number of attack iterations. Consider the left-most (SMT)
plot of Figure 3 which shows close correspondence between
the number of LFB hits and the number of attempts for
leaking (and correctly obtain the secret). This strongly
suggests that the source of our leak is the LFB.
In our second experiment, a victim thread initially
writes a known value A to a ﬁxed memory address, and
then reads it back in a loop where each read is followed
by an mfence instruction (serializing all loads and stores
issued prior to the mfence). We mark this address as WB,
WT, WC and UC. Optionally, we also ﬂush the address
using clflush (which ﬂushes it from the cache). Figure 4
shows how often a RIDL attacker reads the secret value
correctly. Note that we do not observe the signal for WB
(cid:26)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:31 UTC from IEEE Xplore.  Restrictions apply. 
To launch a RIDL attack, we still need to understand
the interaction between loads, stores, the L1d cache and
the LFB, such that we can massage the data from the
victim into the LFB. Recall that in our read experiment
(Figure 4), we did not observe a signal if we do not ﬂush
the address, even with multiple consecutive reads like we
did with writes (Figure 5). As the data is read constantly,
all
loads simply hit in the L1d cache, preventing any
interaction of future loads with the LFB. In contrast, when
we do ﬂush, the future loads miss and allocate an entry
in the LFB to await the data. In case of WC and UC
memory, the processor avoids the L1d cache and enforces
the loads to always go through the LFB. Our second
experiment (Figure 5) shows a signal for all memory types
and especially those that bypass the L1d cache, suggesting
that memory writes go via the LFB.
Conclusion: reads that are not served from L1d pull
data through the LFB, while writes push data through
the LFB to either L1d or memory.
B. Synchronization
To leak information, the attacker must make sure that
the right data is visible in the LFB at the right time, by
synchronizing with the victim. We show that there are
three ways to do so: serialization, contention and eviction.
Serialization. Intel CPUs implement a number of barriers
to perform diﬀerent types of serialization [45], [46], [47],
[48]. lfence guarantees that all loads issued before the
lfence become globally visible, sfence guarantees the
same for all store instructions, and mfence guarantees that
both load and stores before the mfence become globally
visible. To enforce this behavior, the processor waits for
these loads and stores to retire by draining the load and/or
store buﬀers, as well as the corresponding entries in the
LFB. The mfence instruction therefore forms a point of
synchronization that allows us to observe the last few loads
and stores before the buﬀers are completely drained.
Contention. Another way of synchronizing victim and
attacker is to create contention within the LFB, ultimately
forcing entries to be evicted. Doing so allows us to obtain
some control over the entries that we leak, and should
not depend on SMT. To verify this, we perform the RIDL
attack without SMT by writing values in our own thread
and observing the values that we leak from the same
thread. Figure 3 shows that if we do not write the values
(“no victim”), we leak only zeros, but with victim and
attacker running in the same hardware thread (e.g., in a
sandbox), we leak the secret value in almost all cases.
Eviction. Finally, we can control the values that we leak
from the victim by evicting cache entries from the cache set
in which we are interested. To show that we can use this
for synchronization, we conducted an experiment where
the victim writes a value to the same cache line within
a number of pages. After a while, these writes end up
evicting the previous cache lines from the L1d cache. As
these cache lines are dirty, the processor has to write
them back through the memory hierarchy and will do this
through the LFB. We extend the victim thread to alternate
Fig. 4: Leaking the secret A which is read by the victim for
write-back (WB), write-through (WT), write-combine (WC)
and uncacheable (UC) memory, with and without a cache ﬂush.
and WT memory if we do not ﬂush the entry from the
cache, but when we do, we observe the signal regardless
of the memory type. Both observations indicate that we
are not leaking from the cache. Also, since we leak from a
load, this cannot be the eﬀect of store-to-load forwarding
either. Furthermore, since we observe the signal from WC
and UC memory, which have to go through the LFB, the
source of our leak must again be the LFB.
Fig. 5: Leaking the secrets A, B, C and D, written by the victim,
for write-back (WB), write-through (WT), write-combine (WC)
and uncacheable (UC) memory, with and without a cache ﬂush.
To gather further evidence that we are leaking from the
LFB, we perform a third experiment where we run a victim
thread which, in a loop, writes four diﬀerent values to four
sequential cache lines, followed by an mfence. Optionally,