We assume that the Transaction Processing Module exe-
cutes as a Trusted application inside trusted execution envi-
ronment (TEE), e.g. ARM TrustZone [4]. ARM TrustZone
divides a device platform into two execution environments,
namely, normal world and secure world. The normal world
is used to host rich Operating Systems (OS), like Android
OS, and user applications while it allows processing of secu-
rity sensitive codes in isolation within the secure world. The
two worlds communicate with each other via secure monitor.
In our approach, the trusted application is responsible for
processing transaction speciﬁc messages, handling necessary
cryptographic operations and maintaining secrets like keys
required for NFC transactions. On the other hand, NFC
Transaction App running on the normal world handles user
interactions and NFC communication. To authenticate a
user based on her tapping gesture, our system begins col-
lecting information from diﬀerent sensors as soon as the user
opens NFC Transaction App. Our system also records the
time when the phone receives the ﬁrst NFC message from
the NFC transaction terminal. At this point, the user must
have tapped her phone to the NFC transaction terminal and
she is holding her phone towards the terminal to complete
the NFC transaction.
Whenever the NFC Transaction App starts, it informs the
Permission Manager to indicate that it has started. Permis-
sion Manager immediately starts collecting the sensor val-
268
Figure 3: Sensor data collection ﬂowchart.
ues. When the NFC Transaction App requires to process
transaction messages, it requests the Permission Manager
by sending NFC event begin time. The Permission Manager
sends the set of appropriate sensor values to the Authenti-
cator. Once the Authenticator conﬁrms the tapping gesture
as belonging to the user, the Permission Manager permits
the NFC Transaction App to interact with the transaction
module to complete the NFC transaction.
4. APPLICATION DESIGN
To develop and evaluate our authentication mechanism
based on tap gesture biometrics, we ﬁrst needed to collect
the tap gesture data from diﬀerent users. After the data
collection, diﬀerent features were to be generated to robustly
identify individual user data from other user data. We chose
to implement our system in the Android OS. For the data
collection, we created two modules: (1) NFC Transaction
Module for a user to perform the tap gesture on a NFC
transaction terminal which simulates NFC transactions, and
(2) Sensor Module to record sensor values when the user
performs the tap so that underlying data can be analyzed
and later used to identify the user.
4.1 NFC Transaction Module
Android provides NFC Host Card Emulation APIs that
allows the NFC-enabled phone to acts as a contactless card
and allows NFC applications to communicate with external
contactless readers. We designed our NFC module to simu-
late a real-world NFC transaction application. For this, we
chose to implement an NFC based public transit ticketing
system. We designed and implemented both NFC ticket-
ing application on the phone and the ticket reader appli-
cation that controls the NFC transaction terminal. Both
applications use a shared 128-bit AES key to authenticate
each other during an NFC transaction. Speciﬁcally, we used
three-pass mutual authentication protocol of MIFARE DES-
Fire EV1 1 as Kasper et al. [22] elaborated. Ticketing ap-
plications based on Mifare DESFire are widely used by pub-
lic transit authorities around the world. NFC ticketing is
only one aspect of an NFC transaction, nevertheless, it can
be used as an analogy to understand user’s NFC tapping
gesture during any NFC transaction (e.g., for payments or
building entry).
4.2 Sensor Module
Android platform provides several sensors that allow de-
velopers to monitor the motion of the device, the position
of the device or the environment in which the device is. To
be speciﬁc, the Android platform provides three broad cat-
egories of sensors, namely, motion sensors which measure
acceleration forces and rotational forces along three axes,
position sensors which measure physical position and orien-
tation of the device, and environmental sensors which mea-
sure various environment parameters such as humidity, light
illumination, ambient temperature, pressure and so on.
We created an Android service such that whenever the
service is called by another activity or service, the service
starts recording selected sensor values. The sensors we con-
sidered in our app are listed in Table 1. The sensors values
are logged along with the timestamps so that they can be
used for statistical analysis later on. When the calling app
sends the stop service command to the service, the service
stops recording the sensor data. The ﬂow chart of this data
collection process is shown in Figure 3.
1MIFARE DESFire EV1: http://www.nxp.com/products/
identiﬁcation and security/smart card ics/mifare smart
card ics/mifare desﬁre/series/MIFARE DESFIRE EV1
4K.html
269
right handed, and none of them swapped their phone from
one hand to other during the experiment. The experiment
was performed in lab settings. We provided a smartphone
to the volunteers and asked them to tap it to the reader.
Each user opened the app, tapped to the reader to initiate
an NFC transaction and held it there until he/she was noti-
ﬁed about the transaction complete message as displayed on
the phone. Then the user brought the phone away from the
reader. We asked each user to pause for few seconds before
he/she tapped again for another transaction.
In one session, we asked the user to tap and perform the
transaction ﬁve times for each of the four diﬀerent reader
positions mentioned above, i.e., after the user tapped the
reader ﬁve times, we changed the position of the reader to
a diﬀerent setting. Hence, in a session, we collected 20 tap
gesture samples from each user (ﬁve each for four diﬀerent
reader positions). We conducted six sessions collecting 120
tap gesture samples for each user (30 samples of data for
each of the four positions of the reader). These six sessions
were conducted in time spans ranging from either one day to
six days depending upon the availability of the volunteers.
However, each session had suﬃcient gap to break the user’s
rhythm of tapping and add variation to the user’s hand mo-
tion. Our University’s Institutional Review Board approved
the study.
6. TAP BIOMETRICS DETECTION:
DESIGN AND EVALUATION
6.1 Set-Up and Design
In order to evaluate the feasibility of the proposed tap ges-
ture biometrics as an authentication scheme, we utilized the
machine learning approach based on the underlying readings
of the motion sensors, the position sensors and the ambient
pressure sensors (the diﬀerent sensor employed are listed in
Table 1).
Classiﬁer: We utilized the Random Forest classiﬁer in our
analysis. Random Forest is an ensemble approach based on
the generation of many classiﬁcation trees, where each tree
is constructed using a separate bootstrap sample of the data.
In order to classify a new input, the new input is run down
on all the trees and the result is determined based on ma-
jority voting. Random trees have been shown to be a strong
competitor to Support Vector Machine (SVM), and its per-
formance frequently outperforms SVM [29]. Random Forest
is eﬃcient, can estimate the importance of the features, and
is robust against noise [29].
Features: For each of the position and the motion sen-
sor instances, we calculated the square root of the sum of
squares for that instance’s axes components (X, Y, Z), such
that it captures the signiﬁcance of all the three axes. Then,
we calculated the mean and the standard deviation of all
the instances in the sample that corresponds to a single tap.
This gave us twenty features, which we used for training and
testing of the Random Forest classiﬁer.
The twenty features were used as input to train the clas-
siﬁer to diﬀerentiate a user from other users. We evaluated
two training models for the classiﬁcation task: (1) scenario-
speciﬁc model, and (2) general model. The scenario-speciﬁc
model requires each user to train a classiﬁer on all reader
(transaction terminal) positions (described in Section 5) be-
fore using the app. This model assumes that the classiﬁer
Figure 4: A user tapping an NFC reader at waist-
ﬂat position. In waist-ﬂat scenario, the NFC reader
is kept at the height of 0.75-1 m from the ground
and horizontally on the table.
5. DATA COLLECTION
To develop and evaluate our approach, we ﬁrst needed to
collect data from multiple users. We also wanted to capture
various types of gestures that users make while tapping their
NFC-enabled phone to the terminals installed for diﬀerent
types of NFC applications. There is no standard instruc-
tion on how NFC transaction terminals should be placed,
e.g.
they can be placed horizontally, vertically or at cer-
tain angle from the surface where they are placed at the
NFC transaction terminals. We designed our data collec-
tion engine to capture four diﬀerent scenarios based on how
the NFC transaction terminals may be installed: (1) Waist-
Flat: horizontally at the height of 0.75-1 meter above the
ground, (2) Waist-Angular: at 45 degree angle with horizon-
tal surface at the height of 0.75-1 meter above the ground,
(3) Chest-Angular : at 45 degree angle with the vertical sur-
face at the height of 1-1.5 meter above the ground, and (4)
Chest-Vertical: vertically at the height of 1-1.5 meter above
the ground. A user tapping an NFC reader in the waist-ﬂat
scenario is shown in Figure 4. We implemented an app as
discussed in the Section 4 and collected data using Google
Nexus 5 as our phone model. We used NFC reader ACR
122U as the transaction terminal.
As the user opens the app to make NFC transactions, our
system runs in the background as a service as mentioned
in the Section 4.2. We continuously recorded the sensor
values for the experiment and detailed analysis, however, in
the real-life implementation, the sensors can be turned oﬀ
as soon as the transaction success message is received or
shortly thereafter.
For data collection, we invited volunteers to our lab via
word of mouth. These volunteers were university students
from diﬀerent countries situated in the US and Finland.
There were a total of 20 volunteers (17 male and 3 female,
between the age of 25-35) who participated in our study. We
only observed four left handed users, while rest of them were
270
Table 1: Sensors employed for authenticating users based on tap gesture.
Sensor
Type
Function
Accelerometer
Gravity
Gyroscope
Linear Acceleration
Rotation Vector
Game Rotation Vector
Geomagnetic Rotation Vector
Magnetic Field
Orientation
Pressure
Motion
Position
Acceleration force including gravity
Force of gravity on the device
Rate of rotation of the device
Acceleration force excluding gravity
Rotation vector of the device (uses geomagnetic ﬁeld and gyroscope)
Rotation vector of the device (does not use geomagnetic ﬁeld)
Rotation vector of the devices (uses magnetometer)
Earth’s magnetic ﬁeld
Position of a device relative to the earth’s frame
Environment Ambient air pressure
knows or is informed about the position of the reader (i.e.,
the scenario for the transaction). The generalized model, in
contrast, uses all the data from all diﬀerent scenarios of the
user and builds a global classiﬁer per user regardless of the
reader position. Moreover, we have tested multiple gesture
duration by utilizing the sensor data of one, two and three
seconds before the transaction begins. Our goal was to de-
termine the optimal duration of the tapping gesture which
can uniquely identify each user.
In all of the classiﬁcation tasks, the positive class corre-
sponds to the tap gesture of the legitimate user and the neg-
ative class corresponds to impersonator (other user). There-
fore, true positive (TP) represents the number of times the
legitimate user is granted access, true negative (TN) repre-
sents the number of times the impersonator is rejected, false
positive (FP) represents the number of times the imperson-
ator is granted access and false negative (FN) represents the
number of times the correct user is rejected.
As performance measures for our classiﬁers, we used Pre-
cision, Recall and F-measure (F1 score), as shown in Equa-
tions (1) to (3). Precision measures the security of the pro-
posed system, i.e., the accuracy of the system in rejecting
impersonators. Recall measures the usability of the pro-
posed system as low recall leads to high rejection rate of
the legitimate users. F-measure considers both the usability
and the security of the system. To make our system both
usable and secure, ideally, we would like to have F-measure
as close as 1.
precision =
T P
T P + F P
recall =
T P
T P + F N
F -measure = 2 ∗
precision ∗ recall
precision + recall
(1)
(2)
(3)
6.2 Classiﬁcation Results
General Model: As mentioned in Section 5, we collected
data from 20 users. Each user performed a total of 120 taps.
We divided the collected data into 20 sets based on the users’
identities (ids). In order to build a classiﬁer to authenticate a
user based on her tapping biometrics, we deﬁned two classes.
The ﬁrst class contains the Tap data from a speciﬁc user, and
the other class contains randomly selected Tap data from
other users. We analyzed three diﬀerent duration of the
tapping gesture, by considering one, two and three seconds
before the transaction begins.
After running a 10-fold cross validation, we obtain results
for diﬀerent duration and diﬀerent scenarios. The results
show that one second of sensor data is enough for authen-
ticating the user, shown with high F-Measure, recall and
precision. Increasing the gesture duration did not improve
the accuracy; it would rather decrease the accuracy as it
may incorporate random user movement before the actual
tapping gesture starts. We summarize the results for diﬀer-
ent scenarios with one second duration of the tap gesture in
Table 2. The results for longer durations of the tap gesture
(two seconds and three seconds) are shown in Tables 4 and
5 in Appendix A. These results suggest that increasing the
tap duration does not seem to increase the accuracy and
therefore one second duration seems optimal. Hence, the
rest of the experiments reported in this paper are conducted
with the one second duration of the tap gesture.
In our experiment, 12 out of the 20 users performed all
the tapping in one day, and, for this sub-group of users, the
average and standard deviation (for tapping duration of 1
second before) were 0.97 (0.03) for these users. The data
collection from the rest of the users spanned between 4 and
22 days, and, for these users, the average and standard devi-
ation of the F-Measure dropped to 0.88 (0.03). In practice,
the classiﬁcation models can be re-trained as the user makes
new successful transactions such that the accuracy does not
drop as the time gap between the testing and training data
increases.
Scenario-Speciﬁc Model: In our scenario-speciﬁc model,
we divided the collected data into 80 sets based on the user’s
ids and the scenario’s (reader positions) id. In order to build
a classiﬁer to authenticate the user based on the tapping in a
given speciﬁc scenario, we deﬁne two classes. The ﬁrst class
has the tap gesture data from a speciﬁc user in a speciﬁc
scenario, and the other class contains randomly selected data
from other users corresponding to the same scenario.
The classiﬁcation results are calculated after running a 10-
fold cross validation and shown in Table 2. The classiﬁcation
accuracy for the scenario-speciﬁc model is less than its cor-
respondent in the general model. This may be due to the
reduced number of instances in each of the ﬁles (30 versus
271
Table 3: The results for the active attack. The performance of the classiﬁer built using 120 taps for gener-
alized classiﬁcation model as well as using 30 taps for diﬀerent scenario speciﬁc classiﬁcation model for the
particular victim is shown. The last column shows the attack success rate FPR (False Positive Rate) for the
corresponding classiﬁer. FPR represents the rate at which the attacker was falsely classiﬁed as the victim.
The attacker was not successful at all in mimicking the victim’s tap gesture.
Victim
Attacker
F-measure Recall Precision
FPR
Generalized
Chest-Angular
Waist-Flat
Chest-Vertical
Waist-Angular