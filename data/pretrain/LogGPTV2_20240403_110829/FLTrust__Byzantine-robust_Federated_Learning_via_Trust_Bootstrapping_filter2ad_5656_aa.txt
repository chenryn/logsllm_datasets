title:FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping
author:Xiaoyu Cao and
Minghong Fang and
Jia Liu and
Neil Zhenqiang Gong
FLTrust: Byzantine-robust Federated Learning via
Trust Bootstrapping
Xiaoyu Cao∗1, Minghong Fang∗2, Jia Liu2, Neil Zhenqiang Gong1
1 Duke University, {xiaoyu.cao, neil.gong}@duke.edu
2 The Ohio State University, {fang.841, liu.1736}@osu.edu
Abstract—Byzantine-robust federated learning aims to enable
a service provider to learn an accurate global model when a
bounded number of clients are malicious. The key idea of existing
Byzantine-robust federated learning methods is that the service
provider performs statistical analysis among the clients’ local
model updates and removes suspicious ones, before aggregating
them to update the global model. However, malicious clients can
still corrupt the global models in these methods via sending
carefully crafted local model updates to the service provider. The
fundamental reason is that there is no root of trust in existing
federated learning methods,
i.e., from the service provider’s
perspective, every client could be malicious.
In this work, we bridge the gap via proposing FLTrust, a
new federated learning method in which the service provider
itself bootstraps trust. In particular, the service provider itself
collects a clean small training dataset (called root dataset) for
the learning task and the service provider maintains a model
(called server model) based on it to bootstrap trust. In each
iteration, the service provider ﬁrst assigns a trust score to each
local model update from the clients, where a local model update
has a lower trust score if its direction deviates more from the
direction of the server model update. Then, the service provider
normalizes the magnitudes of the local model updates such that
they lie in the same hyper-sphere as the server model update
in the vector space. Our normalization limits the impact of
malicious local model updates with large magnitudes. Finally,
the service provider computes the average of the normalized local
model updates weighted by their trust scores as a global model
update, which is used to update the global model. Our extensive
evaluations on six datasets from different domains show that
our FLTrust is secure against both existing attacks and strong
adaptive attacks. For instance, using a root dataset with less than
100 examples, FLTrust under adaptive attacks with 40%-60% of
malicious clients can still train global models that are as accurate
as the global models trained by FedAvg under no attacks, where
FedAvg is a popular method in non-adversarial settings.
I.
INTRODUCTION
Federated learning (FL) [22], [28] is an emerging dis-
tributed learning paradigm on decentralized data. In FL, there
are multiple clients (e.g., smartphones, IoT devices, and edge
devices) and a service provider (e.g., Google, Apple, and IBM).
Each client holds a local training dataset; and the service
∗Equal contribution.
Network  and  Distributed  Systems  Security  (NDSS)  Symposium  2021 
21-25  February  2021, Virtual 
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.24434
www.ndss-symposium.org
provider enables the clients to jointly learn a model (called
global model) without sharing their raw local training data
with the service provider. Due to its potential promise of
protecting private/proprietary client data, particularly in the
age of emerging privacy regulations such as General Data
Protection Regulation (GDPR), FL has been deployed by high-
proﬁle companies. For instance, Google has deployed FL for
next-word prediction on Android Gboard [1]; WeBank uses
FL for credit risk prediction [3]; and more than 10 leading
pharmaceutical companies leverage FL for drug discovery in
the project MELLODDY [2]. Roughly speaking, FL iteratively
performs the following three steps: the server provided by the
service provider sends the current global model to the clients
or a selected subset of them; each selected client trains a
model (called local model) via ﬁne-tuning the global model
using its own local training data and sends the local model
updates back to the server1; and the server aggregates the local
model updates to be a global model update according to an
aggregation rule and uses it to update the global model. For
instance, FedAvg [28], a popular FL method in non-adversarial
settings developed by Google, computes the average of the
local model updates weighted by the sizes of local training
datasets as the global model update.
However, due to its distributed nature, FL is vulnerable to
adversarial manipulations on malicious clients, which could
be fake clients injected by an attacker or genuine clients
compromised by an attacker. For instance, malicious clients
can corrupt the global model via poisoning their local training
data (known as data poisoning attacks [8], [32]) or their local
model updates sent to the server (called local model poisoning
attacks [15], [7], [5], [43]). The corrupted global model makes
incorrect predictions for a large number of testing examples
indiscriminately (called untargeted attacks) [15], or it predicts
attacker-chosen target labels for attacker-chosen target testing
examples while the predicted labels for other non-target testing
examples are unaffected (called targeted attacks) [5], [7], [43].
For instance, the global model in FedAvg can be arbitrarily
manipulated by a single malicious client [9], [48].
Byzantine-robust FL methods [9], [12], [29], [46], [48]
aim to address malicious clients. The goal therein is to learn
an accurate global model when a bounded number of clients
are malicious. Their key idea is to leverage Byzantine-robust
aggregation rules, which essentially compare the clients’ local
model updates and remove statistical outliers before using
them to update the global model. For instance, Median [48]
computes the coordinate-wise median of the clients’ local
1It is algorithmically equivalent to send local models instead of their updates
to the server.
model updates as the global model update. However, recent
studies [7], [15] showed that existing Byzantine-robust FL
methods are still vulnerable to local model poisoning attacks
on malicious clients. The fundamental reason is that they have
no root of trust. Speciﬁcally, from the server’s perspective,
every client could be malicious, providing no root of trust for
the server to decide which local model updates are suspicious.
Our work: In this work, we propose a new Byzantine-robust
FL method called FLTrust. Instead of completely relying on the
local model updates from clients, the server itself bootstraps
trust in FLTrust. Speciﬁcally, the service provider manually
collects a small clean training dataset (called root dataset) for
the learning task. The server maintains a model (called server
model) for the root dataset just like how a client maintains a
local model. In each iteration, the server updates the global
model by considering both its server model update and the
clients’ local model updates.
Our new Byzantine-robust aggregation rule: Speciﬁcally,
we design a new Byzantine-robust aggregation rule in FLTrust
to incorporate the root of trust. A model update can be
viewed as a vector, which is characterized by its direction and
magnitude. An attacker can manipulate both the directions and
magnitudes of the local model updates on the malicious clients.
Therefore, our aggregation rule takes both the directions and
magnitudes into considerations when computing the global
model update. Speciﬁcally, the server ﬁrst assigns a trust score
(TS) to a local model update, where the trust score is larger
if the direction of the local model update is more similar to
that of the server model update. Formally, we use the cosine
similarity between a local model update and the server model
update to measure the similarity of their directions. However,
the cosine similarity alone is insufﬁcient because a local model
update, whose cosine similarity score is negative, can still
have a negative impact on the aggregated global model update.
Therefore, we further clip the cosine similarity score using the
popular ReLU operation. The ReLU-clipped cosine similarity
is our trust score. Then, FLTrust normalizes each local model
update by scaling it to have the same magnitude as the server
model update. Such normalization essentially projects each
local model update to the same hyper-sphere where the server
model update lies in the vector space, which limits the impact
of the poisoned local model updates with large magnitudes.
Finally, FLTrust computes the average of the normalized local
model updates weighted by their trust scores as the global
model update, which is used to update the global model.
FLTrust can defend against existing attacks: We per-
form extensive empirical evaluation on six datasets from
different domains, including ﬁve image classiﬁcation datasets
(MNIST-0.1, MNIST-0.5, Fashion-MNIST, CIFAR-10, and
CH-MNIST) and a smartphone-based human activity recog-
nition dataset (Human Activity Recognition). We compare
FLTrust with multiple existing Byzantine-robust FL methods
including Krum [9], Trimmed mean [48], and Median [48].
Moreover, we evaluate multiple poisoning attacks including
label ﬂipping attack (a data poisoning attack), Krum attack and
Trim attack (untargeted local model poisoning attacks) [15],
as well as Scaling attack2 (targeted local model poisoning
attack) [5]. Our results show that FLTrust is secure against
2The Scaling attack is also known as a backdoor attack.
2
these attacks even if the root dataset has less than 100 training
examples, while existing Byzantine-robust FL methods are
vulnerable to them or a subset of them. For instance, a CNN
global model learnt using FLTrust has a testing error rate of
0.04 under all the evaluated attacks on MNIST-0.1. However,
the Krum attack can increase the testing error rate of the CNN
global model learnt by Krum from 0.10 to 0.90. Moreover, we
treat FedAvg under no attacks as a baseline and compare our
FLTrust under attacks with it. Our results show that FLTrust
under attacks achieves similar testing error rates to FedAvg
under no attacks. We also study different variants of FLTrust
and the impact of different system parameters on FLTrust. For
instance, our results show that FLTrust works well once the
root dataset distribution does not diverge too much from the
overall training data distribution of the learning task.
FLTrust can defend against adaptive attacks: An at-
tacker can adapt its attack to FLTrust. Therefore, we also
evaluate FLTrust against adaptive attacks. Speciﬁcally, Fang
et al. [15] proposed a general framework of local model
poisoning attacks, which can be applied to optimize the attacks
for any given aggregation rule. An attacker can substitute the
aggregation rule of FLTrust into the framework and obtain an
adaptive attack that is particularly optimized against FLTrust.
Our empirical results show that FLTrust is still robust against
such adaptive attacks. For instance, even when 60% of the
clients are malicious and collude with each other, FLTrust can
still learn a CNN global model with testing error rate 0.04 for
MNIST-0.1. This testing error rate is the same as that of the
CNN global model learnt by FedAvg under no attacks.
Our contributions can be summarized as follows:
• We propose the ﬁrst
FLTrust
robustness against malicious clients.
that bootstraps trust
federated learning method
to achieve Byzantine
• We empirically evaluate FLTrust against existing at-
tacks. Our results show that FLTrust can defend
against them.
• We design adaptive attacks against FLTrust and eval-
uate their performance. Our results show that FLTrust
is also robust against the adaptive attacks.
II. BACKGROUND AND RELATED WORK
A. Background on Federated Learning (FL)
training dataset Di, i = 1, 2,··· , n. We use D = (cid:83)n
Suppose we have n clients and each client has a local
i=1 Di
to denote the joint training data. Each training example in D
is drawn from an unknown distribution X . The clients aim to
collaboratively learn a shared global model with the help of a
service provider. The optimal global model w∗ is a solution to
the following optimization problem: w∗ = arg minw F (w),
where F (w) = ED∼X [f (D, w)] is the expectation of the
empirical loss f (D, w) on the joint training dataset D. Since
the expectation is hard to evaluate, the global model is often
learnt via minimizing the empirical
i.e.,
arg minw f (D, w) is the learnt global model. Speciﬁcally,
each client maintains a local model for its local
training
dataset. Moreover, a service provider’s server maintains the
global model via aggregating the local model updates from
loss in practice,
Fig. 1: Illustration of the three steps in FL.
the clients. Speciﬁcally, FL iteratively performs the following
three steps (illustrated in Figure 1):
rules include Krum [9], Trimmed mean [48], and Median [48],
which we discuss next.
•
•
•
Step I: Synchronizing the global model with clients.
The server sends the current global model w to the
clients or a subset of them.
Step II: Training local models. Each client trains a
local model via ﬁne-tuning the global model using its
local training dataset. Formally, the ith client solves
the optimization problem minwi f (Di, wi), where wi
is the client’s local model. In particular, the client
initializes its local model as the global model and uses
stochastic gradient descent to update the local model
for one or more iterations. Then, each client sends its
local model update gi = wi − w (i.e., the difference
between its local model and the current global model)
to the server.
Step III: Updating the global model via aggregating
the local model updates. The server computes a
global model update g via aggregating the local model
updates according to some aggregation rule. Then,
the server updates the global model using the global
model update, i.e., w = w − α · g, where α is the
global learning rate.
The aggregation rule plays a key role in FL. Different FL
methods essentially use different aggregation rules. Next, we
discuss popular aggregation rules.
its number of training examples. Formally, g =(cid:80)n
1) FedAvg: FedAvg [28] was proposed by Google. FedAvg
computes the average of the clients’ local model updates as
the global model update, where each client is weighted by
|Di|
N gi,
where |Di| is the local training dataset size on the ith client
and N is the total number of training examples. FedAvg
is the state-of-the-art FL method in non-adversarial settings.
However,
in FedAvg can be arbitrarily
manipulated by a single malicious client [9], [48].
the global model
i=1
Krum [9]: Krum selects one of the n local model updates in
each iteration as the global model update based on a square-
distance score. Suppose at most f clients are malicious. The
score for the ith client is computed as follows:
(cid:107)gj − gi(cid:107)2
2,
(1)
(cid:88)
si =
gj∈Γi,n−f−2
where Γi,n−f−2 is the set of n−f −2 local model updates that
have the smallest Euclidean distance to gi. The local model
update of the client with the minimal score will be chosen as
the global model update to update the global model.
Trimmed Mean (Trim-mean) [48]: Trimmed mean is a
coordinate-wise aggregation rule that considers each model
parameter individually. For each model parameter, the server
collects its values in all local model updates and sorts them.
Given a trim parameter k < n
2 , the server removes the largest
k and the smallest k values, and then computes the mean of
the remaining n− 2k values as the value of the corresponding
parameter in the global model update. The trim parameter k
should be at least the number of malicious clients to make
Trim-mean robust. In other words, Trim-mean can tolerate less
than 50% of malicious clients.
Median [48]: Median is another coordinate-wise aggregation
rule. Like Trim-mean, in Median, the server also sorts the
values of each individual parameter in all local model updates.
Instead of using the mean value after trim, Median considers
the median value of each parameter as the corresponding
parameter value in the global model update.
Existing FL methods suffer from a key limitation: they are
vulnerable to sophisticated local model poisoning attacks on
malicious clients, which we discuss in the next section.
B. Poisoning Attacks to Federated Learning
2) Byzantine-robust Aggregation Rules: Most Byzantine-
robust FL methods use Byzantine-robust aggregation rules
(see, e.g., [9], [12], [29], [34], [44], [47], [48], [31]) that
aim to tolerate Byzantine client failures. One exception is
that Li et al. [26] introduced a norm regularization term into
the loss function. Examples of Byzantine-robust aggregation
Poisoning attacks generally refer to attacking the training
phase of machine learning. One category of poisoning attacks
called data poisoning attacks aim to pollute the training data
to corrupt the learnt model. Data poisoning attacks have been
demonstrated to many machine learning systems such as spam
detection [32], [35], SVM [8], recommender systems [16],
3
Step Ⅰ. The server sends the global model to the clients.Step Ⅱ. The clients train local models and send local model updates to the server.Step Ⅲ. The server aggregates local model updates and uses them to update the global model.[17], [25], [45], neural networks [11], [18], [27], [30], [36],
[37], and graph-based methods [20], [41], [49], as well as
distributed privacy-preserving data analytics [10], [14]. FL is
also vulnerable to data poisoning attacks [38], i.e., malicious
clients can corrupt the global model via modifying, adding,
and/or deleting examples in their local training datasets. For
instance, a data poisoning attack known as label ﬂipping attack
changes the labels of the training examples on malicious clients
while keeping their features unchanged.
Moreover, unlike centralized learning, FL is further vul-
nerable to local model poisoning attacks [5], [7], [15], [43],
in which the malicious clients poison the local models or
their updates sent to the server. Depending on the attacker’s
goal, local model poisoning attacks can be categorized into
untargeted attacks [15] and targeted attacks [5], [7], [43].
Untargeted attacks aim to corrupt the global model such that
it makes incorrect predictions for a large number of testing
examples indiscriminately, i.e., the testing error rate is high.
Targeted attacks aim to corrupt the global model such that it
predicts attacker-chosen target labels for attacker-chosen target
testing examples while the predicted labels for other non-target
testing examples are unaffected.
Note that any data poisoning attack can be transformed
to a local model poisoning attack, i.e., we can compute the
local model update on a malicious client’s poisoned local
training dataset and treat it as the poisoned local model update.
Moreover, recent studies [7], [15] showed that local model poi-
soning attacks are more effective than data poisoning attacks
against FL. Therefore, we focus on local model poisoning
attacks in this work. Next, we discuss two state-of-the-art
untargeted attacks (i.e., Krum attack and Trim attack) [15] and
one targeted attack (i.e., Scaling attack) [5].
Krum attack and Trim attack [15]:
Fang et al. [15]
proposed a general framework for local model poisoning
attacks, which can be applied to optimize the attacks for any
given aggregation rule. Assuming the global model update
without attack is g, Fang et al. [15] formulate the attack