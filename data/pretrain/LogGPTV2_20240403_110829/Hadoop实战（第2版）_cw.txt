Accepted：表示用户提交的代码已经被接受，而用户代码被接受的前提是代码能够正确运行，并且在以平台的测试数据作为输入数据执行的输出结果和平台标准的输出结果完全相同。但是需要提醒的是，由于MapReduce编程框架的原因，平台上的这些题目完全可以在MapReduce结构中的Map或Reduce阶段独立完成，但是这种做法没有完全发挥MapReduce并行运行的效率，不是最优的办法。所以如果用户的代码被Accepted了，用户还需要审视自己的代码，检查它是否最大程度利用了并行运行来提高效率。
Compile Error：表示用户代码编译错误，出现这种情况说明在用户的代码中存在语法问题，在进行普通的Java程序编译时出错了。用户可以点击result栏的错误结果链接去查看具体的语法错误位置，并进行修改。用户也可以在本地进行普通的Java编译，待通过之后再提交到平台上。
MapReduce Error：表示代码在Hadoop上运行时出现错误并没有输出结果。这种情况出现的可能性比较多，主要包括：常见的Java程序逻辑错误、MapReduce逻辑错误等。Java程序逻辑错误又主要包括数组越界、未初始化等，MapReduce逻辑错误则主要包括输入输出类型不匹配等。在遇到MapReduce Error时相对比较麻烦，需要用户仔细核对自己的代码，找出逻辑错误的地方进行修改，然后再尝试提交。
Wrong Answer：表示代码能够在Hadoop上正常运行并有输出结果，只是用户的输出结果和标准结果并不匹配。出现这种情况时，用户首先要检查自己代码的输出格式是否正确，比如顺序是否和实例输出相同。然后再检查结果是否完整，是不是漏掉了某些结果等，最后检查是不是程序逻辑错误导致的结果错误。
Runtime Error：表示代码执行的时间太长，也就是说用户代码在Hadoop上执行的时间超出了正常的执行时间。出现这种情况的原因主要是用户程序存在死循环或平台同时提交的程序太多，使运行效率降低了。用户只需要查看是否存在死循环代码并在平台空闲的时间提交就可以了。
Memory Exceed：表示程序运行时内存溢出，即用户代码中过多使用了内存或无限申请内存的代码（这主要针对主函数中的代码，如果在MapReduce中出现类似的代码会返回MapReduce Error）。出现这种情况，就需要用户在自己的代码中仔细查找是否有过多使用内存或无限开内存的代码。
Evil Code：表示提交的程序中存在恶意代码，也就是说用户代码中存在系统调用代码或意图更改平台服务器配置的代码等。这就需要用户清除代码中根本用不到的代码和一些恶意代码了。
Sim Code：表示提交程序的指纹和网站代码指纹库中的某一个指纹相似度超过了网站定义的阈值，也就是说此代码有抄袭的嫌疑。发生这种错误之后网站会向用户和网站管理员自动发送相关邮件，并附上用户代码和雷同代码，如果判错用户可同管理员联系。
以上介绍了平台运行用户提交的代码之后所返回的各种结果及其出现的原因和应对策略。错误的根本原因是代码问题，所以用户遇到问题需要耐心审视自己代码，修改其中不正确的代码和逻辑，删除无用代码。
A.4.3 使用注意事项
这一节主要向大家介绍平台使用的一些注意事项，这部分内容也可以参考平台FAQs中的内容。
Java程序主类的名字必须为MyMapre（否则编译错误）。存在这个限制的原因是需要统一所有提交的代码，然后由Shell文档再将其提交到Hadoop上运行，所以不能为每个用户的代码写专门的Shell文档。
在配置MapReduce程序的输入输出时必须使用下面两个语句（原因和前一个注意事项相同）：
旧API
FileInputFormat.setInputPaths（conf, new Path（args[0]））；
FileOutputFormat.setOutputPath（conf, new Path（args[1]））；
新API
FileInputFormat.addInputPath（job, new Path（otherArgs[0]））；
FileOutputFormat.setOutputPath（job, new Path（otherArgs[1]））；
MapReduce程序必须处于一个Java源文件内，它不支持引用其他文件的类。也就是说必须把Map、Reduce、Combine等类写到一个文件内。
平台对同时运行的MapReduce程序数量有限制。因为系统资源有限，而Hadoop平台及MapReduce程序在处理少量数据时的表现并不是很好（即使运行少量数据，WordCount程序也需要花费20多秒的时间），所以需要用户耐心等待提交程序的检测结果，而且不要同时提交多个程序，以免占用过多的平台资源。
A.5 小结
本附录主要介绍了云计算在线检测平台。平台以Hadoop集群作为并行程序的运行环境，为MapReduce的入门者提供了兼顾实战和理论的训练，使其初步掌握MapReduce框架和Hadoop系统的理论知识，同时具有使用MapReduce并行化解决实际问题的能力。
在附录的第2节中介绍了平台的各个组成部分及其功能。平台经过升级之后主要包括前台用户接口、后台程序运行和代码过滤模块。前台主要包括用户完全服务、实例编程练习、分布式系统理论知识测试、帮助功能。前台主要完成与用户的交互和用户服务的功能。后台主要包括Tomcat服务器、MySQL数据库、Hadoop分布式环境、Shell文档，它为前台功能提供支持。代码过滤模块主要包括非MapReduce合理程序过滤和雷同代码过滤，这一模块规范了用户的程序和使用规范。接着又介绍了用户代码的检测流程，主要是用户提交之后网页保存用户代码、启动Shell调用用户提交代码进行代码预处理、预处理成功后代码会提交到Hadoop上运行，然后分析并返回用户程序执行的结果，最后将用户的结果信息显示在前台界面上。最后一节对网站的使用进行了介绍，主要是一些功能使用的举例，比如注册更新信息、提交代码、理论测试等。同时本节还介绍了用户代码运行之后返回的各个结果所表示的意思、原因和如何应对。
云计算在线检测平台能够帮助用户补充MapReduce编程框架和Hadoop分布式系统的理论知识，并且在实践中掌握利用MapReduce框架解决实际问题的能力，是MapReduce入门者不错的选择。
附录B Hadoop安装、运行与使用说明
本章内容
Hadoop安装
Hadoop启动
Hadoop使用
在本书中，关于Hadoop的安装、运行和使用都已有介绍，但由于章节内容的要求，并没有组织在一起。为了方便大家直接学习安装、运行和使用Hadoop，从读者实际实践的需求出发，本书第二版附加本附录，将Hadoop的安装、运行和使用结合起来系统地呈现给读者（安装采用最小可用配置）。为了统一，本附录关于Hadoop的安装和使用均基于Ubuntu 11.10 Linux操作系统，1.0.1版本的Hadoop和1.6版本的JDK。
B.1 Hadoop安装
B.1.1 JDK安装
安装JDK具体步骤如下。
（1）下载安装JDK
确保可以连接到互联网，从http：//www.oracle.com/technetwork/java/javase/downloads页面下载JDK安装包（文件名类似jdk-***-linux-i586.bin，不建议安装1.7版本，因为并不是所有软件都支持1.7）到JDK安装目录（本章假设jdk安装目录均为/usr/lib/jvm/jdk）。
（2）手动安装JDK
在终端下进入JDK安装目录，并输入命令：
sudo chmod u+x jdk-***-linux-i586.bin
修改完权限之后就可以进行安装，在终端输入命令：
sudo-s./jdk-***-linux-i586.bin
安装结束之后就开始配置环境变量。
（3）配置环境变量
输入命令：
sudo gedit/etc/profile
输入密码，打开profile文件。
在文件最下面输入如下内容：
#set Java Environment
export JAVA_HOME=/usr/lib/jvm/jdk
export CLASSPATH=".：$JAVA_HOME/lib：$CLASSPATH"
export PATH="$JAVA_HOME/：$PATH"
这一步的意义是配置环境变量，使你的系统可以找到JDK。
（4）验证JDK是否安装成功
输入命令：
java-version
会出现如下JDK的版本信息：
java version"1.6.0_22"
Java（TM）SE Runtime Environment（build 1.6.0_22-b04）
Java HotSpot（TM）Client VM（build 17.1-b03，mixed mode, sharing）
如果出现上述JDK版本信息，说明当前安装的JDK并未设置成Ubuntu系统默认的JDK，接下来还需要手动将安装的JDK设置成系统默认的JDK。
（5）手动设置系统默认JDK
在终端依次输入命令：
sudo update-alternatives--install/usr/bin/java java/usr/lib/jvm/jdk/bin/java 300
sudo update-alternatives--install/usr/bin/javac javac/usr/lib/jvm/jdk/bin/javac 300
sudo update-alternatives--config java
接下来输入java-version就可以看到JDK安装的版本信息。
B.1.2 SSH安装
在终端输入下面的命令：
ssh-version
如果出现类似“OpenSSH_5.1p1 Debian-6ubuntu2，OpenSSL 0.9.8g 19 Oct 2007”的字符串，表示SSH已安装。如果没有输入下面的命令进行安装：
sudo apt-get install ssh
然后再依次输入以下命令完成本机的免密码配置：
ssh-keygen-t dsa-P''-f～/.ssh/id_dsa
cat～/.ssh/id_dsa.pub＞＞～/.ssh/authorized_keys
B.1.3 Hadoop安装
Hadoop有三种运行方式：单节点方式、单机伪分布方式与集群方式。前两种方式并不能体现云计算的优势，但是便于程序的测试与调试。
在安装Hadoop之前，先从Hadoop官方网站：
http://www. apache.org/dyn/closer.cgi/Hadoop/core/
下载hadoop-1.0.1.tar.gz并将其解压，本文往下都默认Hadoop解压在/home/u/目录下。
（1）单节点方式安装
安装单节点的Hadoop无须配置，在这种方式下，Hadoop被认为是一个单独的Java进程，这种方式经常用来调试。
（2）单机伪分布方式安装
伪分布式的Hadoop是只有一个节点的集群，在这个集群中，这个节点既是master，也是slave；既是NameNode也是DataNode；既是JobTracker，也是TaskTracker。
配置伪分布的Hadoop需要修改以下几个文件（具体修改内容的含义请参考第二章）：
进入Hadoop目录下conf目录，在hadoop-env.sh中添加JAVA安装目录：
export JAVA_HOME=/usr/lib/jvm/jdk
修改core-site.xml内容如下：
＜?xml version="1.0"?＞
＜?xml-stylesheet type="text/xsl"href="configuration.xsl"?＞
＜configuration＞
＜property＞
＜name＞fs.default.name＜/name＞
＜value＞hdfs：//localhost：9000＜/value＞
＜/property＞
＜/configuration＞
修改hdfs-site.xml内容如下：
＜?xml version="1.0"?＞
＜?xml-stylesheet type="text/xsl"href="configuration.xsl"?＞
＜configuration＞
＜property＞
＜name＞dfs.replication＜/name＞
＜value＞1＜/value＞
＜/property＞
＜/configuration＞
修改mapred-site.xml内容如下：
＜?xml version="1.0"?＞
＜?xml-stylesheet type="text/xsl"href="configuration.xsl"?＞
＜configuration＞
＜property＞
＜name＞mapred.job.tracker＜/name＞
＜value＞localhost：9001＜/value＞
＜/property＞
＜/configuration＞
上面文件都配置结束之后，Hadoop的安装配置也就完成了。
（3）集群方式安装
这里以安装三台主机的小集群为例为读者呈现Hadoop的集群安装。三台主机的IP地址和对应角色安排如下表：
安装的具体步骤如下：
1）在三台主机上创建相同的用户，以便Hadoop启动过程中的通信。参考本附录中JDK和SSH的安装，在每台主机上安装JDK和SSH，并配置环境变量。
2）在每台主机上配置主机名和IP地址。打开每个主机的/etc/hosts文件，输入内容：
127.0.0.1 localhost
10.37.128.2 master
10.37.128.3 slave1
10.37.128.4 slave2
需要注意的是，应删除此文件中其他无用信息，防止Hadoop集群启动时slave无法找到master准确的IP地址进行通信，最终导致slave上的进程虽然启动但无法和master上的进程进行通信。
接下来配置每台主机上的/etc/hostname文件，在文件中输入对应的主机名。
3）配置master免密码登录slave，将master的密钥文件复制到各个slave主机的.ssh文件即可。在master主机的终端下输入命令：
scp～/.ssh/authorized_keys slave1：～/.ssh/
scp～/.ssh/authorized_keys slave2：～/.ssh/
命令完成之后可以使用ssh slave1和ssh slave2来测试是否配置成功。
4）修改Hadoop配置文件内容。
在每台主机上进入Hadoop安装目录，向conf/hadoop-env.sh文件中添加JDK安装目录：
export JAVA_HOME=/usr/lib/jvm/jdk
将conf/core-site.xml文件修改成：
＜?xml version="1.0"?＞
＜?xml-stylesheet type="text/xsl"href="configuration.xsl"?＞
＜configuration＞
＜property＞
＜name＞fs.default.name＜/name＞
＜value＞hdfs：//master：9000＜/value＞
＜/property＞
＜property＞
＜name＞hadoop.tmp.dir＜/name＞
＜value＞/home/u/tmp＜/value＞
＜/property＞