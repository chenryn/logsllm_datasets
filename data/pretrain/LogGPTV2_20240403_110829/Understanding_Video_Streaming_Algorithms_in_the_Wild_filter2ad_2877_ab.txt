SRF
Swiss Public Service
TubiTV Movies and series of all genres
Twitch
Live and VoD streaming service, gaming 39, Global
Alexa rank
270, France
# Resolutions
4.0 ± 0.0
5.0 ± 0.0
91, Global
45, Switzerland 5.7 ± 0.48
3.0 ± 0.0
1330, USA
5.9 ± 0.32
4.2 ± 0.92
6.5 ± 1.08
5.3 ± 0.48
4.0 ± 0.0
4.4 ± 0.52
47, Germany
188, Global
Vimeo
Artistic content [20]
YouTube Broad coverage
ZDF
German Public Service
Pornhub Pornographic video sharing website
XVideos Pornographic video sharing website
2, Global
46, Global
67, Global
Convergence: During startup, an ABR may have little information about the
client’s network conditions. How do diﬀerent ABRs approach stable behavior
starting from this lack of information? Stablility in this sense refers to fewer
bitrate switches. Thus, to assess convergence characteristics, we quantify the
bitrate changes (in Mbps per second) across playback, i.e., a single switch from
3 Mbps to 4 Mbps bitrate over a total playback of 5-s amounts to 0.2 Mbps/sec
on this metric. We chose not to compare the raw number of switches/sec —
one switch at YouTube is very diﬀerent from one switch at TubiTV, due to the
diﬀering discreteness of their bitrate ladders.
Risk-Tolerance: ABRs can hedge against rebuﬀer events by building a larger
buﬀer, thus insulating them from bandwidth drops. Thus, how much buﬀer (in
seconds of video) an ABR builds during its stable operation is indicative of its
risk tolerance.
Reactivity: ABRs must react to changes in network bandwidth. However, react-
ing too quickly to bandwidth changes can result in frequent switching of video
quality, and cause unstable behavior when network capacity is highly variable.
304
M. Licciardello et al.
To quantify reactivity of an ABR, we use synthetic traces with just one band-
width change after convergence, and measure the evolution of bitrate diﬀerence
in the video playback after the change over time (with the number of following
chunk downloads used as a proxy for time).
Bandwidth Usage: ABR must necessarily make conservative decisions on video
quality: future network bandwidth is uncertain, so fetching chunks at precisely
the estimated network bandwidth would (a) not allow building up a playback
buﬀer even if the estimate were accurate; and (b) cause rebuﬀers when band-
width is overestimated. Thus, ABR can only use some fraction of the available
bandwidth. We quantify this behavior in terms of the fraction of bytes played
to optimally downloadable, with “optimally downloadable” reﬂecting the mini-
mum of (a posteriori known) network capacity and the bytes needed for highest
quality streaming.
For better bandwidth use and to improve QoE, some ABRs are known to
redownload and replace already downloaded chunks in the buﬀer with higher
quality chunks. We quantify this as the fraction of bytes played to bytes down-
loaded. Fractions <1 reﬂect some chunks not being played due to their replace-
ment with higher quality chunks.
QoE Goal: Academic ABR work has largely used a QoE metric that linearly
combines a reward for high bitrate with penalties for rebuﬀers and quality
switches [17,18]. More recent work has suggested formulations of QoE that
reward perceptual video quality rather than just bitrate [22]. One such met-
ric of perceptual quality, VMAF [15], combines several traditional indicators of
video quality. While it is diﬃcult, if not impossible, to determine what precise
metric each platform’s ABR optimizes for, we can evaluate coarsely whether
this optimization is geared towards bitrate or VMAF-like metrics by examining
what video chunks an ABR tries to fetch at high quality: do chunks with higher
VMAF get fetched at a higher quality level? To assess this, we sort chunks by
VMAF (computed using [15]) and quantify for the top n% of chunks, their (aver-
age) playback quality level compared to the (average) quality level of all chunks,
Qtop−n% − Qall. A large diﬀerence implies a preference for high-VMAF chunks.
3.4 Measurement Coverage
We evaluate multiple videos on each of 10 platforms across a large set of network
traces.
Target Platforms: Table 1 lists the platforms we analyze (with their Alexa
popularity rank, as of January 2020). While by no means exhaustive, these were
chosen to cover a range of content types and a few diﬀerent geographies. Note
that Netﬂix, Amazon Prime Video, and Hulu were excluded because their terms
of service prohibit automated experiments or/and reverse-engineering [1–3]. For
Twitch, which oﬀers both live streams and video-on-demand of archived live
streams, we only study the latter, as live streaming is a substantially diﬀerent
problem, and a poor ﬁt with the rest of our chosen platforms.
Understanding Video Streaming Algorithms in the Wild
305
(a) Initialization behavior
(b) Convergence
Fig. 2. (a) Initialization: most providers start playback after one chunk is downloaded.
(b) Convergence is measured in terms of changes in bitrate switching, i.e., the (abso-
lute) sum of bitrate diﬀerentials across all switches from the start, divided by the
thus-far playback duration. As expected, switching is more frequent during startup,
but the degree of switching varies across providers both in startup and later.
Diﬀerent platforms encode content at varied resolutions and number of res-
olutions, ranging from just 3 quality levels for TubiTV to 6.5 on YouTube (on
average across our test videos; YouTube has diﬀerent numbers of resolutions on
diﬀerent videos.)
When comparing the behavior of deployed ABRs with academic ones, we test
the latter in the oﬄine environment made available by the Pensieve authors [17].
For each tested video on each platform, we pre-download all its chunks at all
available qualities. We then simulate playback using the same network traces up
until the same point oﬄine for academic ABRs as we do for the deployed ones.
We primarily rely on Robust MPC [18] (referred to throughout as MPC) as a
stand-in for a recent, high-quality academic ABR approach. While even newer
proposals are available, they either use data-dependent learning techniques [6,
17] that are unnecessary for our purpose of gaining intuition, or do not have
available, easy-to-use code.
Videos: The type of content can have substantial bearing on streaming perfor-
mance, e.g., videos with highly variable encoding can be challenging for ABR.
We thus used a set of 10 videos on each platform. Where a popularity mea-
sure was available, we used the most popular videos; otherwise, we handpicked
a sample of diﬀerent types of videos. Videos from each platform are encoded in
broadly similar bitrate ranges, with most diﬀerences lying at higher qualities,
e.g., some content being available in 4K.
It would, of course, be attractive to upload the same video content to several
platforms (at least ones that host user-generated content) to remove the impact
of videos in the cross-platform comparisons. However, diﬀerent platforms use
their own encoding pipelines, making it unclear whether this approach has much
advantage over ours, using just popular videos across platforms.
306
M. Licciardello et al.
Network Traces: Our experiments use synthetic and real-world traces from 3
datasets in past work [6,9,24]. Unfortunately, a full cross-product of platform-
video-trace would be prohibitively expensive—the FCC traces [9] alone would
require 4 years of streaming time. To sidestep this, we rank traces by their
throughput variability and pick traces with the highest and lowest variability
together with some randomly sampled ones.
Our ﬁnal network trace collection consists of the 5 least stable, 5 most stable,
and 5 random traces from the Belgium trace collection [12], and 10 in each of
those categories from the Norway [24], the Oboe [6] and the FCC datasets3. We
also use 15 constant bandwidth traces covering the range from 0.3 to 15 Mbps
uniformly. Lastly we add 10 step traces: after 60 s of streaming we suddenly
increase/drop the bandwidth from/to 1 Mbps to/from 5 values covering the
space from 1.5 to 10 Mbps uniformly.
In total, we use 130 traces with throughput (average over time for each trace)
ranging from 0.09 to 41.43 Mbps, with an average of 6.13 Mbps across traces.
Note that we make no claim of our set of traces being representative; rather our
goal is to test a variety of traces to obtain insight into various ABR behaviors.
If a trace does not cover the whole experiment we loop over it.
For quantifying reactivity, we only use the synthetic traces mentioned above,
with a single upward step change in bandwidth. For quantifying startup delay,
we use traces with a bandwidth of around 3 Mbps as noted in Sect. 3.3.
Ethics: We are careful to not generate excessive traﬃc or large bursts to any
platform, measuring at any time, only one stream per service, typically at a low
throttled rate.
4 Measurement Results
Overall, we see diverse behavior on each tested metric across platforms. We
attempt to include results across all platforms where possible, but for certain
plots, for sake of clarity, we choose a subset of platforms that exhibits a range
of interesting behaviors.
Initialization Behavior, Fig. 2a: We ﬁnd that most platforms’ ABR sim-
ply waits for one chunk download to ﬁnish before beginning playback. This is
reﬂected in the buﬀer occupancy at playback. Some players like ZDF and SRF
use a larger chunk size (10 s), which is why they pre-load more seconds of buﬀer.
As one might expect, building a larger buﬀer before playback starts generally
incurs a higher start time. Twitch stands out in this regard, as it downloads
nearly 20 s of buﬀer before start. Some players, whilst downloading the same
number of buﬀer seconds as others, do so at much higher resolution – e.g.,
SRF downloads its ﬁrst 10 s with 6× as many pixels as Arte. This is reﬂected
in the disparity between their start times, despite both populating the buﬀer
with 10 s of playback. More broadly, all such “discrepancies” are diﬃcult to
explain because startup is hard to untangle from other network activity, e.g.,
3 Speciﬁcally, the stable collection from September 2017 [9].
Understanding Video Streaming Algorithms in the Wild
307
some players already start downloading video chunks while the player itself is
still downloading, thus complicating our notion of timing. (We start timing from
the point the ﬁrst chunk starts downloading. For most platforms, this provides
a leveling standard that excludes variation from other downloads on their Web
interface. It also helps reduce latency impacts that are mainly infrastructure
driven, as well as eﬀects of our browser automation framework.)
Convergence, Fig. 2b: As expected, during startup and early playback, every
player attempts to ﬁnd a stable streaming state. This results in many bitrate
switches followed by much smoother behavior with more limited switching. Nev-
ertheless, there are large diﬀerences across players, e.g., Pornhub switches more
than twice as much as Fandom and SRF in the beginning. In stable state, Fan-
dom switches substantially more than SRF. We also evaluated the academic
(Robust) MPC algorithm [18] on the same network traces and over the SRF
videos. The MPC algorithm would use more than twice as much switching both
in startup and later, compared to SRF’s deployed ABR. Consequently, SRF
scores lower than MPC on the default linear QoE model used in MPC. However,
this does not necessarily imply that SRF’s design is sub-optimal; it could also
be optimizing for a diﬀerent metric that values stability more.
For clarity, we only picked a few platforms as exemplars of behavior towards
convergence instead of including all 10 tested platforms. The behavior is broadly
similar with more switching early on, but the precise stabilization diﬀers across
platforms.
Risk-Tolerance, Fig. 3: We observe widely diﬀerent buﬀering behavior across
the players we tested. Of course, every player uses early playback to down-
load lower quality chunks and accumulate buﬀer, but some, like YouTube, settle
towards as much as 80 s of buﬀer, while others like Fandom operate with a much
smaller buﬀer of around 20 s. Testing MPC’s algorithm on the same traces across
the YouTube videos reveals that it falls towards the lower end, stabilizing at 20 s
of buﬀer.
Note that for approaches that allow
redownloads (including YouTube), lar-
ger buﬀers are a reasonable choice: any
chunks that were downloaded at low
quality can later be replaced. This is
likely to be a more robust strategy in
the face of high bandwidth variabil-
ity. However, for approaches that do
not use redownloads, a larger buﬀer
implies that all its content must be
played out at whatever quality it was
downloaded at, thus limiting the pos-
sibilities to beneﬁt from opportunistic
behavior if bandwidth later improves.
Thus operating with a smaller buﬀer of higher-quality chunks may be preferable
to ﬁlling it with lower-quality chunks. In the absence of redownloads, there is
thus a tradeoﬀ: a larger buﬀer provides greater insurance against bandwidth
Fig. 3. Risk-tolerance: YouTube operates
with nearly 4× the buﬀer for Fandom. The
shaded regions show the 95% conﬁdence
interval around the mean.
308