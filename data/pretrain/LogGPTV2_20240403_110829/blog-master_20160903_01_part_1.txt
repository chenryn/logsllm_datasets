## CentOS 7.x x64 部署 HAWQ
### 作者         
digoal          
### 日期        
2016-09-02      
### 标签        
PostgreSQL , HAWQ , Greenplum    
----        
## 背景  
在CentOS 6.x 上不管是源码或二进制部署HAWQ都有点苦逼，原因是6自带的软件版本都比较老，而HAWQ依赖的环境的版本都比较新。  
[《CentOS 6.x 部署HAWQ》](../201608/20160831_01.md)  
在7.x上部署HAWQ会轻松许多。  
本文简单的讲一下在CentOS 7.x 的单机上使用源码部署HAWQ的过程。  
## 安装7.x epel yum源
http://fedoraproject.org/wiki/EPEL  
```
# wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
# rpm -ivh epel-release-latest-7.noarch.rpm
# yum makecache
```
## 从yum仓库安装依赖包
```
# yum install -y man passwd sudo tar which git mlocate links make bzip2 net-tools \
  autoconf automake libtool m4 gcc gcc-c++ gdb bison flex gperf maven indent \
  libuuid-devel krb5-devel libgsasl-devel expat-devel libxml2-devel \
  perl-ExtUtils-Embed pam-devel python-devel libcurl-devel snappy-devel \
  thrift-devel libyaml-devel libevent-devel bzip2-devel openssl-devel \
  openldap-devel protobuf-devel readline-devel net-snmp-devel apr-devel \
  libesmtp-devel xerces-c-devel python-pip json-c-devel libhdfs3-devel \
  apache-ivy java-1.7.0-openjdk-devel \
  openssh-clients openssh-server  
# pip install --upgrade pip
# pip --retries=50 --timeout=300 install --upgrade paramiko pycrypto
# yum install -y lcov
```
## install cmake
```
# useradd gpadmin
# su - gpadmin
$ mkdir -p /home/gpadmin/app
cd ~
wget https://cmake.org/files/v3.6/cmake-3.6.1.tar.gz
tar -zxvf cmake-3.6.1.tar.gz
cd cmake-3.6.1
./configure --parallel=32 --prefix=/home/gpadmin/app/cmake
make -j 32
make install
export PATH=/home/gpadmin/app/cmake/bin:$PATH
```
## 配置os
corefiles目录  
```
# mkdir /data01/corefiles
# chmod 777 /data01/corefiles
```
内核参数  
```
vi /etc/sysctl.conf
fs.aio-max-nr = 1048576
fs.file-max = 76724600
kernel.core_pattern= /data01/corefiles/core_%e_%u_%t_%s.%p         
kernel.sem = 4096 2147483647 2147483646 512000    
kernel.shmall = 107374182      
kernel.shmmax = 274877906944   
kernel.shmmni = 819200         
net.core.netdev_max_backlog = 10000
net.core.rmem_default = 262144       
net.core.rmem_max = 4194304          
net.core.wmem_default = 262144       
net.core.wmem_max = 4194304          
net.core.somaxconn = 4096
net.ipv4.tcp_max_syn_backlog = 4096
net.ipv4.tcp_keepalive_intvl = 20
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_time = 60
net.ipv4.tcp_mem = 8388608 12582912 16777216
net.ipv4.tcp_fin_timeout = 5
net.ipv4.tcp_synack_retries = 2
net.ipv4.tcp_syncookies = 1    
net.ipv4.tcp_timestamps = 1    
net.ipv4.tcp_tw_recycle = 0    
net.ipv4.tcp_tw_reuse = 1      
net.ipv4.tcp_max_tw_buckets = 262144
net.ipv4.tcp_rmem = 8192 87380 16777216
net.ipv4.tcp_wmem = 8192 65536 16777216
vm.dirty_background_bytes = 4096000000       
vm.dirty_expire_centisecs = 6000             
vm.dirty_ratio = 80                          
vm.dirty_writeback_centisecs = 50            
vm.min_free_kbytes = 2097152  # vm.min_free_kbytes 建议每32G内存分配1G vm.min_free_kbytes
vm.mmap_min_addr = 65536  
vm.overcommit_memory = 0     
vm.overcommit_ratio = 90     
vm.swappiness = 0            
vm.zone_reclaim_mode = 0     
net.ipv4.ip_local_port_range = 40000 65535    
sysctl -p
```
资源限制  
```
rm -f /etc/security/limits.conf
rm -f /etc/security/limits.d/*.conf
vi /etc/security/limits.d/hawq.conf
* soft    nofile  1024000
* hard    nofile  1024000
* soft    nproc   unlimited
* hard    nproc   unlimited
* soft    core    unlimited
* hard    core    unlimited
* soft    memlock unlimited
* hard    memlock unlimited
```
## 从yum安装R
centos 7带的R版本比较高，如果图省事就直接用YUM安装。  
```
yum install -y R R-devel
```
## JDK
http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html  
```
$ cd ~
get   
Java SE Development Kit 8u102
Linux x64   173.03 MB   jdk-8u102-linux-x64.tar.gz
$ tar -zxvf jdk-8u102-linux-x64.tar.gz
$ mv jdk1.8.0_102 /home/gpadmin/app/
$ export JAVA_HOME=/home/gpadmin/app/jdk1.8.0_102
```
## hadoop
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html  
http://apache.fayea.com/hadoop/common/stable/  
```
$ cd ~
$ wget http://apache.fayea.com/hadoop/common/stable/hadoop-2.7.3.tar.gz
$ tar -zxvf hadoop-2.7.3.tar.gz
$ mv hadoop-2.7.3 /home/gpadmin/app/
$ cd /home/gpadmin/app/hadoop-2.7.3
$ bin/hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar             run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp   copy file or directories recursively
  archive -archiveName NAME -p  *  create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings
Most commands print help when invoked w/o parameters.
```
## hdfs
需要注意对齐hawq与hdfs的监听端口。   
目录需要对应的权限。   
```
su - root
# mkdir -p /data01/gpadmin/hadoop/tmp
# mkdir -p /data01/gpadmin/hadoop/sock
# mkdir -p /data01/gpadmin/hadoop/dfs/name
# mkdir -p /data01/gpadmin/hadoop/dfs/data
# chown -R gpadmin:gpadmin /data01/gpadmin
两个权限务必注意  
# chown gpadmin:root /data01/gpadmin/hadoop/sock
# chmod 755 /data01/gpadmin/hadoop/sock
# chown root:root /
# su - gpadmin
$ vi ~/.bash_profile
$ export PATH=/home/gpadmin/app/hadoop-2.7.3/bin:/home/gpadmin/app/hadoop-2.7.3/sbin:/home/gpadmin/app/cmake/bin:$PATH
$ export JAVA_HOME=/home/gpadmin/app/jdk1.8.0_102
$ export HADOOP_HOME=/home/gpadmin/app/hadoop-2.7.3
. ~/.bash_profile
cd ~ 
```
配置文件设置   
vi etc/hadoop/core-site.xml  
```
        fs.defaultFS
        hdfs://localhost:8020
        ipc.client.connection.maxidletime
        3600000
        ipc.client.connect.timeout
        300000
        ipc.server.listen.queue.size
        3300
        hadoop.tmp.dir
        file:/data01/gpadmin/hadoop/tmp
        Abase for other temporary   directories.
```
vi etc/hadoop/hdfs-site.xml  
```
        dfs.replication
        1
        dfs.allow.truncate
        true
        dfs.block.access.token.enable
        false
        dfs.block.local-path-access.user
        gpadmin
        dfs.client.socket-timeout
        300000000
        dfs.client.use.legacy.blockreader.local
        false
        dfs.datanode.data.dir.perm
        750
        dfs.datanode.handler.count
        60
        dfs.datanode.max.transfer.threads
        40960
        dfs.datanode.socket.write.timeout
        7200000
        dfs.namenode.accesstime.precision
        0
        dfs.namenode.handler.count
        600
        dfs.support.append
        true
         dfs.namenode.name.dir
         file:/data01/gpadmin/hadoop/dfs/name
         dfs.datanode.data.dir
         file:/data01/gpadmin/hadoop/dfs/data
        dfs.client.read.shortcircuit
        true
        dfs.domain.socket.path
        /data01/gpadmin/hadoop/sock/dn._PORT
```
秘钥文件认证  
```
$ cd ~
$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
$ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
$ ssh localhost date
Wed Aug 31 10:56:24 CST 2016
```
格式化namenode  
```
$ hdfs namenode -format
```
配置hdfs环境变量文件  
```
$ vi ~/app/hadoop-2.7.3/etc/hadoop/hadoop-env.sh
#export JAVA_HOME=${JAVA_HOME}
export JAVA_HOME=/home/gpadmin/app/jdk1.8.0_102
export HADOOP_HOME=/home/gpadmin/app/hadoop-2.7.3
```
启动hdfs  
```
$ start-dfs.sh
$ hdfs dfsadmin -report
Configured Capacity: 210955796480 (196.47 GB)
Present Capacity: 192722915328 (179.49 GB)
DFS Remaining: 192722907136 (179.49 GB)
DFS Used: 8192 (8 KB)
DFS Used%: 0.00%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0
-------------------------------------------------
Live datanodes (1):
Name: 127.0.0.1:50010 (localhost)
Hostname: localhost
Decommission Status : Normal
Configured Capacity: 210955796480 (196.47 GB)
DFS Used: 8192 (8 KB)
Non DFS Used: 18232881152 (16.98 GB)
DFS Remaining: 192722907136 (179.49 GB)
DFS Used%: 0.00%
DFS Remaining%: 91.36%
Configured Cache Capacity: 0 (0 B)