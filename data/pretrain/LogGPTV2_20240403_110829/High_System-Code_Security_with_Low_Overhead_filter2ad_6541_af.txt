B. Workload Selection
Proﬁling identiﬁes a set of hot regions in a program. For
optimal results, the checks in these regions should be the
ones that are most expensive in production, and the ones that
contribute least to security. These two requirements are often
well aligned in practice, and can be used as guidelines to select
an ideal proﬁling workload.
ASAP is based on the assumption that a few sanity checks
are much more expensive than others. For ASAP to meet
tight overhead budgets, the proﬁling workload must expose
the expensive checks. This means that it should execute all
performance-critical program components. The speciﬁc way
in which they are executed does not matter, because ASAP
only depends on expensive checks being sufﬁciently visible,
not on precisely measuring their runtime. Naturally, a proﬁling
workload representative of real workloads will yield the best
results in production.
Our conﬁdence in the security provided by the remaining
checks is based on the assumption that checks in cold code
are more likely to catch bugs than checks in hot code. For
this to hold, it is important that the program parts stressed
by the proﬁling workload are also well tested. This is often
implicit, but can be made explicit by using the program’s
test suite as the proﬁling workload. The test suite does not
need to have high coverage, as ASAP will preserve checks in
uncovered parts. However, it should provide assurance that the
covered parts are indeed correct, e.g., by using a large number
of assertions to audit the program state.
Developers can use these ideas to actively guide ASAP. For
example, by increasing test coverage for performance-critical
program parts, conﬁdence in the correctness of these parts
increases and the need for safety checks decreases. ASAP
will exploit this and assign checks to other areas where they
are needed more, thereby improving performance.
C. Sanity Checks
To understand how ASAP works and what it assumes, we
deﬁne a sanity check to be a piece of code that tests a safety
condition and has two properties: (1) a passing check is free of
side-effects, and (2) a failing check aborts the program. This
characterization of sanity checks has important implications:
First, ASAP can automatically recognize sanity checks in
compiled code. Second, removing a sanity check is guaranteed
to preserve the behavior of the program, unless the check
would have failed. Note that metadata updates are not part
of sanity checks by this deﬁnition (and can thus remain as
residual overhead).
The sanity checks seen by ASAP do not necessarily corre-
spond exactly to operations in the program source, since it runs
after compiler optimizations have been already applied. ASAP
beneﬁts from its tight integration with the compiler. Depending
on their type, the compiler may be able to eliminate certain
sanity checks on its own when they are impossible to fail.
Other transformations such as function inlining can duplicate
static sanity checks in the compiled program. This reﬁnes the
granularity of ASAP: if there are multiple copies of the same
function, ASAP can distinguish the individual call sites and
may choose to optimize only some of them.
D. Design Choices
ASAP is tool-agnostic and can work with all mechanisms
that insert sanity checks into the program. We tested our
prototype using AddressSanitizer, SoftBound and Undeﬁned-
BehaviorSanitizer, and veriﬁed that the checks inserted by
WIT and SafeCode (see §II) also follow the same structure.
We designed ASAP to be a compiler-based solution. The
advantage of source code access over a pure binary solution
is that ASAP can thoroughly re-optimize programs once
expensive checks have been removed. This allows additional
871871
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
dead code elimination, reduces register pressure, and can
make hot functions small enough to be inlined. In principle,
however, ASAP’s approach would also work with a powerful
binary rewriting and reoptimization system in the spirit of
SecondWrite [5].
ASAP relies on proﬁling. We chose this approach because
it is a reliable way to obtain the cost of a check, and makes no
assumptions about the nature of the program or the structure
of sanity checks. However, it requires an adequate workload
and increases build times. For many projects, a workload is
available in form of a test suite; this special case has interesting
implications for security and is discussed below.
We ensured ASAP is practical. It can be applied to any
system for which the underlying instrumentation tool works,
and it does not add any restrictions of its own. Safety checks
may soon make use of upcoming hardware support such as
the Intel MPX extension [15]. To support this scenario, ASAP
only needs to recognize instructions that may abort a program,
and know their run-time cost.
ASAP is easy to understand. It uses a simple greedy
algorithm to remove checks that are too costly. Developers
can reason about the diminishing returns of additional safety
checks, based on the intuition that 80% of the safety can
be obtained with just 20% of the overhead (as §VI shows,
the actual numbers are even better). We did consider more
complex solutions before settling for this simplicity, though.
For instance, ASAP could reason about dependencies between
checks to obtain higher security. It could only remove checks
that are provably not needed. Some possible extensions are
discussed in §VII, but we believe the simplicity of the current
solution to be a key strength.
V. IMPLEMENTATION
This section presents the architecture of ASAP, and its core
algorithms for detecting sanity checks, estimating their cost,
and removing expensive ones from programs.
ASAP is based on the LLVM compiler framework and
manipulates programs in the form of LLVM bitcode, a typed
assembly-like language speciﬁcally designed for program
transformations. It supports source-based instrumentation tools
and those that have themselves been built on LLVM, which
covers the majority of modern static instrumentation tools for
C/C++/Objective C.
Users use ASAP through a wrapper script, which they
invoke instead of the default compiler. In addition to producing
a compiled object ﬁle, this wrapper also stores a copy of the
LLVM bitcode for each compilation unit. This copy is used
during subsequent stages to produce variants of the object
with proﬁling code, or variants instrumented for a particular
overhead budget.
ASAP works on programs one compilation unit at a time.
It keeps no global state (except check data described later) and
does not require optimizations at link-time. This is important
for supporting large software systems that rely on separate
and parallel compilation. The only phase in the workﬂow that
requires a global view is the check selection phase, where
; :0
%1 = load i32* %fmap_i_ptr, align 4
%2 = zext i32 %1 to i64
%3 = getelementptr inbounds i32* %eclass, i64 %2
%4 = ptrtoint i32* %3 to i64
%5 = lshr i64 %4, 3
%6 = add i64 %5, 17592186044416
%7 = inttoptr i64 %6 to i8
%8 = load i8* %7, align 1
%9 = icmp eq i8 %8, 0
br i1 %9, label %18, label %10
; :10
%11 = ptrtoint i32* %3 to i64
%12 = and i64 %11, 7
%13 = add i64 %12, 3
%14 = trunc i64 %13 to i8
%15 = icmp slt i8 %14, %8
br i1
, label %18, label %16
%15
; :16
%17 = ptrtoint i32* %3 to i64
call void @__asan_report_load4(i64 %17) #3
call void asm sideeffect "", ""() #3
unreachable
; :18
%19 = load i32* %3, align 4
Fig. 4. A sanity check inserted by AddressSanitizer, in the LLVM intermediate
language. The corresponding C code is cc1 = eclass[fmap[i]] and
is found in blocksort.c in the bzip2 SPEC benchmark. Instructions
belonging to the check are shaded. The red circle marks the branch condition
which, when set to true, will cause the check to be eliminated.
ASAP computes a list of all sanity checks in the software
system and their cost. This phase uses an efﬁcient greedy
selection algorithm described in §IV-A and has little impact
on compilation time.
ASAP automatically recognizes sanity checks. Recall from
§IV-C that a sanity check veriﬁes a safety property, aborts the
program if the property does not hold, and is otherwise side-
effect-free. ASAP searches for sanity checks by ﬁrst looking
at places where the program aborts. These are recognizable
either by the special LLVM unreachable instruction, or
using a list of known sanity check handler functions. The
sanity checks themselves are the branches that jump to these
aborting program locations. Figure 4 shows an example.
The listing is shown in the LLVM intermediate language,
which uses static single assignment form (SSA); each line
corresponds to one operation whose result is stored in a virtual
register, numbered sequentially from %1 to %19. This sanity
check protects a load from the address stored in register %3.
It computes the metadata address (%7), loads shadow memory
(%8) and performs both a fast-path check (the access is allowed
if the metadata is zero) and a slow-path check (the access
is also allowed if the last accessed byte is smaller than the
metadata). If both checks fail, the program is aborted using a
call to __asan_report_load4.
Ic of
the set
ASAP computes
instructions belong-
ing to the check starting with the aborting function
(__asan_report_load4 in our example). It then recur-
sively adds all operands of instructions in Ic to the set, unless
they are also used elsewhere in the program. It also adds to Ic
all branch instructions whose target basic block is in Ic. This
is repeated until Ic reaches a ﬁxpoint. In Figure 4, a shaded
background indicates which instructions belong to Ic.
The instructions in Ic are used for computing check costs
872872
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
as described in §IV-A. A number of different proﬁling mech-
anisms can be used to measure instruction cost. Our choice
fell on GCOV-style proﬁling counters, where the proﬁler uses
one counter per basic block in the program and adds a
counter increment before every branch instruction. Proﬁling
thus determines the number of times each instruction was
executed; we obtain an estimate of the actual cost by applying
the static cost model for instructions that is built into LLVM’s
code generator. The advantage of this approach is that it is
robust and yields cost estimates at instruction granularity that
are unaffected by the proﬁling instrumentation itself.
ASAP removes checks that are too costly from the program
by altering their branch condition. In our example in Figure 4,
it replaces the branch condition %15, circled in red, by the
constant true, so that the check can never fail. The rest
of the work is done by LLVM’s dead code elimination pass.
It recognizes that all shaded instructions are now unused or
unreachable and removes them.
All steps ASAP performs are generic and do not depend on
any particular instrumentation. In fact, the ASAP prototype
works for AddressSanitizer, SoftBound, UndeﬁnedBehavior-
Sanitizer, and programmer-written assertions. It contains ex-
actly four lines of tool-speciﬁc code, namely the expressions
to recognize handler functions such as __asan_report_*.
This makes it straightforward to add support for other software
protection mechanisms. Also, we did not need to alter the
instrumentation tools themselves in any way.
We mention one extra feature of ASAP that helped us
signiﬁcantly during development: ASAP can emit a list of
checks it removes in a format recognized by popular IDEs.
This makes it easy to highlight all source code locations where
ASAP optimized a check. Developers can use this to gain
conﬁdence that no security-critical check is affected.
ASAP is freely available for download at http://dslab.epﬂ.
ch/proj/asap.
VI. EVALUATION
In our evaluation, we want to know both how fast and how
secure instrumented programs optimized by ASAP are. Any
software protection mechanism needs to quantify its overhead
and security. More speciﬁcally in the case of ASAP, we ask:
1) Effectiveness: Can ASAP achieve high security for a
given, low overhead budget? We show that ASAP, using
existing instrumentation tools, can meet very low overhead
requirements, while retaining most security offered by those
tools.
2) Performance: How much can ASAP reduce the over-
head of instrumentation on any given program? Does it
correctly recognize and remove the expensive checks? What
effect does the proﬁling workload have on performance? What
are the sources of the residual overhead?
3) Security: Does ASAP in practice preserve the protection
gained by instrumenting software? How many sanity checks
can it safely remove without compromising security? We also
analyze the distribution of both sanity checks and security
vulnerabilities in software systems, and draw conclusions on
the resulting security of instrumented programs.
A. Metrics
We quantify performance by measuring the runtime of both
the instrumented program and an uninstrumented baseline and
computing the overhead. Overhead is the additional runtime
added by instrumentation, in percent of the baseline runtime.
The cost level (see §IV-A) is determined from the minimum,
maximum, and target overheads for a program, and the sanity
level (see §III) is the fraction of static checks remaining in the
program. To quantify the security of an instrumented program,
we measure the detection rate, i.e., the fraction of all bugs and
vulnerabilities that have been detected through instrumenta-
tion. The detection rate is relative to a known reference set
of bugs and vulnerabilities (e.g., those detected by a fully
instrumented program), because all bugs or vulnerabilities
present in a particular software cannot be known in general.
B. Benchmarks and Methodology
We evaluated ASAP’s performance and security on pro-
grams from the Phoronix and SPEC CPU2006 benchmarks,
the OpenSSL cryptographic library, and the Python 2.7 and
3.4 interpreters. For instrumenting the target programs, we
used AddressSanitizer (ASan) and UndeﬁnedBehaviorSani-
tizer (UBSan) (described in §II), which are both widely
applicable.
Unless otherwise noted, all performance numbers reported
use cost levels that are safe, i.e., the optimized program is
protected against all known vulnerabilities. Our default cost
level is 0.01, for reasons described in the security evaluation
in §VI-D.
We use a collection of real and synthetic bugs and vul-
nerabilities to quantify ASAP’s effect on security. We also
analyze to what degree ASAP affects the detection rate for
the RIPE benchmark (a program containing 850 different
combinations of buffer overﬂow exploits), known bugs in the
Python interpreter, and the entries in the CVE vulnerability
database for the year 2014.
The paragraphs below give details on setup, workloads and
hardware used for each of our experiments.
1) SPEC CPU2006 Benchmarks: The SPEC CPU2006
suite is a set of 19 benchmark programs written in C/C++.
Each program comes with a training workload that we used
for proﬁling, and a reference workload, approximately 10×
larger, used for measuring overhead.
We compiled for each program a baseline version without
instrumentation and a fully-instrumented version. The runtime
difference between these two is the overhead of instrumenta-
tion. In addition, we used ASAP to create optimized executa-
bles for cost level 0.01, and for sanity levels between 80% and
100%. We increased the resolution for sanity levels close to
100%, because small changes in the sanity level have a large
impact on overhead in this region.
All the experiments were run for AddressSanitizer and Un-
deﬁnedBehaviorSanitizer. For AddressSanitizer, we disabled
873873
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
stack trace collection to mimic a production scenario where
performance is more important than informative debug output.
We also turned off error recovery (UBSan will by default print
a warning message and attempt to recover if some checks
fail), choosing to always abort the program when an error
is detected. Unfortunately, not all benchmarks are compatible
with both instrumentation tools; we could run 14 benchmarks
for ASan and 12 for UBSan.1.
All data points have been measured on machines with a
3.4 GHz Intel Core i7 CPU (4 cores, hyperthreading) and 8 GB
RAM.
to protect
2) OpenSSL: We compiled OpenSSL with AddressSan-
itizer. This is sufﬁcient
the library from the
Heartbleed vulnerability. To enable AddressSanitizer, only
minor modiﬁcations to OpenSSL were needed: we changed
crypto/mem.c to ensure that no custom memory al-
locators were used. We also compiled OpenSSL with
-DOPENSSL_NO_BUF_FREELISTS, and disabled Address-
Sanitizer for the OPENSSL_cpuid_setup function be-
cause it contains incompatible inline assembly. We used the
OpenSSL test suite as the proﬁling workload for the initial
phase of ASAP.
To determine the instrumentation overhead, we mea-
sured OpenSSL’s performance in a number of benchmarks.
OpenSSL is most widely used in web servers; we bench-
marked the throughput of a web server by measuring the
number of pages that can be served per second. Our measure-
ments use OpenSSL’s built-in web server with a 3KB static
HTML ﬁle. We also looked at the throughput of OpenSSL’s
cryptographic primitives, and the time it takes to run the test
suite. OpenSSL performance measurements were done on a
workstation with an Intel Xeon CPU (4 cores @ 2 GHz) and
20 GB RAM.
3) Python: We compiled the Python 3.4 interpreter with
AddressSanitizer and UndeﬁnedBehaviorSanitizer instrumen-
tation. We obtained proﬁling data by by running Python’s unit
test suite. This same workload is used by the Ubuntu package
maintainers for creating a proﬁle-guided-optimization build of
Python.
We evaluated performance using the default benchmarks
from the Grand Uniﬁed Python Benchmark Suite [12]. All
measurements were done on a workstation with an Intel Xeon
CPU (4 cores @ 2 GHz) and 20 GB RAM.
C. Performance Results
We report the cost of security, with and without ASAP,
in Figure 5. For each benchmark, we display three values:
1Under ASan, omnetpp does not compile because it uses a custom new
operator. Xalancbmk and dealII do not compile due to a bug in LLVM’s
cost model used by ASAP. Perlbench and h264ref abort at runtime due
to buffer overﬂows involving global variables.
Under UBSan, 10 of 19 benchmarks abort because they perform undeﬁned
behaviors such as left-shifting a signed int by 31 places, multiplication
overﬂow, or calling functions through function pointers of the wrong type.
We could run 4 of them nevertheless, by selectively disabling one or two
types of checks, and included them in the evaluation. We could not compile
omnetpp with UBSan.
The overhead of full instrumentation (leftmost, dark bars), the
overhead with ASAP at cost level 0.01 (gray bar, center), and
the residual overhead (light bars, right). This data reveals a
number of results:
Full
instrumentation is expensive. On SPEC, both Ad-
dressSanitizer and UndeﬁnedBehaviorSanitizer typically cause
above 50% overhead.
ASAP often reduces overhead to acceptable levels. For
eight out of 14 SPEC benchmark, ASAP reduces ASan over-
head to below 5%. This result is also achieved for seven out
of 12 benchmarks with UBSan. For three UBSan benchmarks,
the overhead at cost level 0.01 is slightly larger than 5%.
For the remaining benchmarks, ASAP gives no security
beneﬁts because they are not elastic: their residual overhead
is larger than 5%. In this case, ASAP can only satisfy the
overhead budget by producing an uninstrumented program.
ASAP eliminates most overhead due to checks. In all cases
except for soplex, the overhead at cost level 0.01 is very
close to the residual overhead. Although many checks remain
in the programs (87% on average for the benchmarks in
Figure 5, generally more for larger programs such as Python),
they do not inﬂuence performance much, because they are in
cold code. These results show that ASAP correctly identiﬁes
and removes the hot checks.
ASAP’s performance does not depend on precise proﬁling.
This is a corollary from the last observation. A bad proﬁling
workload would not allow ASAP to identify the expensive
checks, and thus lead to a large difference between overhead at
c = 0.01 and residual overhead. Conversely, a perfect proﬁling
workload can only improve ASAP’s performance up to the
residual overhead.
Even small reductions in security lead to large performance
gains. In Figure 6, we show the speedups obtained when
reducing the sanity level step by step. The gray area corre-
sponds to the entire security-performance space that ASAP
can navigate. The lightest gray area, or 47% of the total
overhead, can be eliminated by removing just 1% of the
sanity checks. This shows how additional cycles invested into
security give diminishing returns, and conﬁrms that indeed
only few checks are hot.
D. Security Evaluation
Developers and operators who use ASAP need to know how
safe the resulting programs are. In particular, we measure how
ASAP affects the detection rate of software instrumentation:
what
is the chance that a bug or vulnerability that was
previously prevented by instrumentation is present in a ASAP-
optimized program?
As discussed in §III on the sanity/performance trade-off, the
detection rate depends primarily on the sanity level, i.e., the
fraction of critical instructions that are protected with sanity
checks. Since the sanity level is directly determined by the
cost level, we can ﬁnd an overall minimum cost level at
which all known vulnerabilities would have been caught. The
following paragraphs present our results of case studies on the
OpenSSL Heartbleed vulnerability, Python bugs, and the RIPE
874874
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
SPEC
ASan
SPEC
UBSan
%
n
i
d
a
e
h
r
e
v
O
75
50
25
0
.
f
c
m
9
2
4
2
p
z
b
i
.
1
0
4
d
m
a
n
.
4
4
4
r
e
m
m
h
6
5
4
.
%
n
i
d
a
e
h
r
e
v
O
120
80
40
0
y
t
f
a
r
C
i
k
c
g
a
M
s
c
h
p
a
r
G
.
s
e
r
h
T
e
v
i
t
p
a
d
A
i
i
k
c
g
a
M
s
c
h
p
a
r
G
.
h
s
e
r
h
T
e
v
i
t
p
a
d
A
i
m
b
l
.
0
7
4
r
a
t
s
a
.
3
7
4
i
3
x
n
h
p
s
.
2
8
4
2
p
z
b
i
.
1
0
4
.
f
c
m
9
2
4
c
l
i
.
m
3
3
4
d
m
a
n
.
4
4
4
k
m
b
o
g
5
4
4
.
l
x
e
p
o
s
.
0
5
4
g
n
e
s
.
j
8
5
4
m
b
l
.
0
7
4
r
a
t
s
a
.
3
7
4
i
3
x
n
h
p
s
.
2
8
4
m
u
t
n
a
u
q
b
i
l
.
2
6
4
(a) ASAP performance results for SPEC benchmarks where omin < 5%.
Phoronix
ASan
Phoronix
UBSan
l
i
a
u
d
s
e
R
P
A
S
A
l
l
u
F
l
i
a
u
d
s
e
R
P
A
S
A
l
l
u
F
r
u
B
l
i
k
c
g
a
M
s
c
h
p
a
r
G
i
l
r
o
o
C
B
W
H
i
k
c
g
a
M
s
c
h
p
a
r
G
i
h
s
i
f
w
o
B
l
i
:
r
e
p
p
R
e
h
t
n
h
o
J
S
E
D
i
:
r
e
p
p
R
e
h
t
n
h
o
J
5
D
M
i
:
r
e
p
p
R
e
h
t
n
h
o
J
P
C
S
T
y
t
f
a
r
C
G
E
P
J
b
L
i
i
e
v
e
s
e
m
i
r
P
.
c
n
E
3