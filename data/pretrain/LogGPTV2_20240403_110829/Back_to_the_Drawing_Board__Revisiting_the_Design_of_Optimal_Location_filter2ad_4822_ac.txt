
We now discuss the following mechanism, which we call the coin
mechanism, and prove that it is optimal. Let z∗ be the output location
that minimizes the average quality loss of a mechanism that always
reports that location regardless of the input x. Formally,
z
∗ (cid:17) argmin
z∈R2
π(x) · dQ(x, z) .
(16)
z∗ =
x ∈X
As an example, if we measure the point-to-point loss as the mean
2, then z∗ will be given by the mean
squared error dQ(x, z) = ||x−z||2
x ∈X π(x)·x. If the loss is measured as the Euclidean distance,
then z∗ is the geometric median of π. Given a generic distance
function dQ(·), the optimal output location z∗ can be computed by
solving the optimization problem in (16).
Let Q∗ be the average quality loss achieved by a mechanism
that always reports z∗ regardless of the input. We construct the
following mechanism, which we denote fcoin. First, we fix a desired
quality loss Q ≤ Q∗ and compute α (cid:17) 1 − Q/Q∗. Then, we build
(17)
where z∗ is in (16). This mechanism can be easily explained and
implemented simulating a coin flip. We first set our desired quality
loss Q ≤ Q∗. Note that it would not make sense to fix Q to a value
larger than Q∗ since we would not achieve more privacy by doing
so; a mechanism that always reports z∗ and has an average loss
of Q∗ yields the highest privacy allowed by π. Then, we compute
α = 1−Q/Q∗ and set it as the probability that our coin shows heads.
Assume we are interested in querying about a location x ∈ X, so
we flip the coin. If the coin shows heads, then we report our desired
fcoin(z|x) = α · δ(z − x) + (1 − α) · δ(z − z
∗) ,
location z = x. If the coin hits tails, then we report z∗ regardless
of the value of x. It is easy to see that the average loss of (17) is
indeed Q, by the linearity of this metric with f .
Proposition 3.5. The coin mechanism obtained for quality loss
Q achieves the maximum average adversarial error possible given a
constraint on the average quality loss, i.e., fcoin(Q) ∈ F opt
Q , if both
are measured with the same distance function dP(·) ≡ dQ(·).
The proof is straightforward using the result in Corollary 3.4.
We now reason why, even though the coin mechanism is optimal
by the standards that have been used to evaluate privacy in prior
works (i.e., PAE and Q), this mechanism is hardly desirable for any
user. When the coin shows heads, the adversary observes z. If
z (cid:44) z∗, the adversary knows for sure that the user was interested
in querying about x = z and therefore the user has no privacy
at all. In this case, for privacy issues, there was no point in using
the mechanism. When the coin shows tails, the user is mapped
far away to z∗. The adversary observes z∗ and has no idea where
the user is, besides the prior π that was already known by her. In
this case, the privacy of the user is maximal, but the quality loss is
very large, since z∗ is almost always very far away from the user.
The quality loss is so large that the utility the user gets from this
realization of the mechanism can be considered zero, so we can
say that there was no point in using the mechanism in this case
either. We have reached the issue we mentioned earlier: there is a
mechanism, optimal by classic location privacy standards [26], that
is useless both from the privacy and the quality loss point of view.
This shows that there is a fundamental problem with the classic
way that has been used to evaluate location privacy mechanisms.
3.3 The reach of this problem
One could think that the problem of this bi-dimensional evaluation
approach lies on the fact that one cannot use the same metric
to measure quality loss and privacy, e.g., the Euclidean distance.
However, even with different metrics, mechanisms similar to the
coin can be derived. For example, if privacy is the average mean
squared error and quality loss is measured as the average Manhattan
distance (i.e., the l1 norm), a deterministic mechanism that consists
on reporting the real location on most of the places and mapping to
the other side of the Earth in some others is optimal, due to the fact
that the MSE grows quadratically with the distance, while the l1
(or any lp norm) does not. In our evaluation, we show an example
where a mechanism optimized for PAE and Q with a different pair
of distance functions dP(·) (cid:44) dQ(·) suffers from the coin issue. The
problem does not arise from the particular distance functions dP(·)
and dQ(·) one uses to evaluate the average error and loss, but from
the fact that these metrics are averages, and as such they do not
restrict the minimum privacy of a single use of the mechanism or
the maximum quality loss of the mechanism, they just ensure that
the average is good. We believe that, while evaluating the average
behavior of a mechanism is not an erroneous notion per-se, it must
be handled with care to avoid undesirable results, such as the coin
mechanism.
As a concluding remark, we would like to note that we have
shown this problem assuming that the outputs of the mechanism
and the values estimated by the adversary are points in R2, for
notational simplicity and generality. An important fraction of pre-
vious works [1, 5, 7, 26, 27] assume a discrete model where the set
of output values Z and estimated values ˆX are the centers of a grid
over the map or points of interest such as X. In these scenarios, one
can derive a similar mechanism, where hitting tails means that the
user reports the location out of the allowed ones that minimizes
the average error. That mechanism can also be shown to be optimal
in terms of average error and loss, although it is not a desirable
mechanism for any user. For completeness, we also evaluate this
scenario in our experiments. The same applies to the case where
instead of having discrete input locations X, users can report any
point in R2 (for example, a tracking or a date finder application).
The coin mechanism in (17) can be applied directly to this scenario,
and it can be shown to be optimal (changing the summations over
X to integrals). It is clear that using the traditional evaluation ap-
proach has flaws in all these scenarios and we must find a solution
to this.
4 COMPLEMENTARY MECHANISM
EVALUATION CRITERIA
So far we have seen that evaluating mechanisms based solely on the
average error and quality loss does not reflect whether a mechanism
is actually more beneficial than another one, due to the fact that
some undesirable mechanisms are deemed optimal by this approach.
In this section, we propose a solution to this evaluation procedure
that consists in incorporating complementary evaluation criteria
that add different perspectives to the performance of a mechanism
in terms of privacy and quality loss.
We propose two metrics, that are not intended to be used as a re-
placement of the average error and average loss but in combination
with them, adding new dimensions to the privacy vs. quality loss
trade-off. The first metric we propose is the conditional entropy, a
privacy metric that helps detecting inconsistent mechanisms such
as the coin. The second one is the worst-case loss, a quality loss
metric that provides a way of staying out of mechanisms that might
yield no utility for the user at all. We comment on the implemen-
tation of mechanisms that take these metrics into consideration,
and propose a mechanism that maximizes the conditional entropy
while being optimal in terms of average error and quality loss. We
finish the section describing other alternative privacy metrics.
4.1 The Conditional Entropy as a
Complementary Metric
4.1.1 Usefulness of the Conditional Entropy. One of the prob-
lems of the coin mechanism can be seen from an information-
theoretic point of view. The coin is a binary mechanism, in the
sense that each input location can only be mapped to itself or to
a fixed point in the map. From the adversary’s perspective, this
means that if the coin shows heads the adversary has no uncer-
tainty at all about the user’s input location, and if it shows tails
the uncertainty is maximal. The conditional entropy can be used
to detect these scenarios where the adversary has no uncertainty
about x. Recalling (8), the conditional entropy can be written as
∫
R2 fZ(z) · H(x|z)dz ,
PCE(f , π) =
(18)
where H(x|z) (cid:17) −
x ∈X p(x|z) · log(p(x|z)) is the entropy of the
posterior after a location z is released. It is clear that (18) is an
average over the entropy of all the posteriors. However, contrary
to the average error, the conditional entropy is an average over
functions H(z|x) that are strictly concave with f . This means that
in order to perform well in terms of the conditional entropy, a
mechanism must spread its uncertainty among every posterior
p(x|z) instead of achieving maximal uncertainty with some outputs
and zero uncertainty with others, as the coin does.
Another interesting property of the entropy is that it is not a
geographical metric. The entropy of a posterior H(x|z) does not
depend on the coordinates of the input locations or the semantic
information tied to them (e.g., if the location is a hospital or a
club). The entropy only depends on how evenly the posterior is
distributed among the input locations. This probabilistic aspect of
privacy, defined as uncertainty in [26], cannot be captured by other
privacy notions such as correctness (e.g., the average adversary
error). Due to the geographic nature of the location privacy problem,
we cannot judge a mechanism based solely on its entropy. However,
using it as an additional dimension of privacy gives a more complete
picture of the performance of a mechanism.
We would like to point out that this notion of uncertainty pro-
vided by the entropy was disregarded as a reasonable privacy metric
in [26] based on the fact that, since it is not correlated with the
adversary error, it does not capture how hard is for the adversary
to estimate the real input location. We claim that it is indeed the
fact that the entropy is not correlated with the adversary error
which gives it a special value as a complementary metric of privacy.
The same way that semantic location privacy metrics have been
proposed together with geographic metrics [1, 7] to give different
perspectives on the problem, the conditional entropy is a tool that
gives valuable information about the protection provided by the
mechanism not captured by the average error.
We would like to make two remarks regarding the entropy. First,
the conditional entropy PCE(f , π) must be taken into account to-
gether with the mutual information I(X; Z) to get a full picture of
the information-theoretic properties of the mechanism. The condi-
tional entropy represents the average amount of uncertainty the
adversary has about the real location x after observing z. A small
value of conditional entropy indicates low uncertainty, and there-
fore we might get the impression that a mechanism with such small
value provides low privacy. However, it might have been possible
that the entropy of the prior was already low, and therefore even if
the mechanism was perfect from the privacy point of view (i.e. it
did not reveal any information, I(X; Z) = 0), there is nothing any
mechanism could have done to avoid having a low conditional en-
tropy. We must therefore take into account the mutual information
or, equivalently, the entropy of the prior π, when interpreting the
value given by the conditional entropy.
The second remark is that the conditional entropy must not be
tailored to a particular adversary with a possibly wrong knowledge
of the prior π. In this work, we have assumed that the prior π
models the choice of input locations by the users, and therefore the
correct way of computing the entropy is by using π in the formulas
above. This entropy must be regarded as the uncertainty that a very
strong passive adversary with full knowledge of the behavior of
the users would have when observing z.
4.1.2
Implementation of Mechanisms with large Conditional En-
tropy. We now look for a mechanism that is optimal in terms of the
average error and average loss, i.e., a mechanism in F opt
Q , that also
achieves as much conditional entropy as possible. This problem
is equivalent to the rate-distortion problem [8] of finding a pdf
f (z|x) that minimizes the mutual information between x and z
subject to a quality loss constraint, which can be solved iteratively
by implementing the Blahut-Arimoto algorithm. For this, we must
first restrict our output to a discrete alphabet Z for computational
reasons. The more points we assign to this alphabet and the more
evenly we cover the space where we want to compute the mech-
anism with them, the better its performance will be. Since both
the input and output domains are discrete, the mechanism is de-
termined by the probabilities of reporting z when the user is in x,
that we denote by p(z|x) here for clarity. We start with an initial
mechanism, for example uniform mapping p(z|x) = 1/|Z|. Then,
we perform the following steps:
(1) We compute the probability mass function of each the output:
(19)
π(x) · p(z|x) ,
∀z ∈ Z .
PZ(z) = 
x ∈X
(2) We update the mechanism as follows:
p(z|x) = PZ(z) · e
−b·dQ(x,z)
,
(3) We normalize the mechanism:
∀x ∈ X, z ∈ Z.
(20)
(21)

p(z|x) =
p(z|x)
z′∈Z p(z′|x) ,
∀x ∈ X, z ∈ Z.
We skip this step for the outputs z with PZ(z) = 0.
p(z|x) is below some threshold.
(4) We repeat these steps until the change in the probabilities
The value of b in the second step needs to be tuned to change the
quality loss of the mechanism Q(f , π) and cannot be pre-computed
to achieve an exact value of average loss. Larger values of b yield
mechanisms with less quality loss, and therefore less average error
privacy and less conditional entropy. Finally, we obtain our mech-
anism f (z|x) by applying the optimal remapping to the discrete
mechanism defined in X → Z by the probabilities p(z|x). This
ensures that the resulting mechanism is optimal from the adversary
error privacy point of view.
We make two remarks regarding this algorithm. The first one
is about its computational cost. The operations in the three steps
above are not expensive as they only include multiplications and
additions. The number of elements we need to compute in order
to build p(z|x) is N (cid:17) |X| · |Z|. The first step above consists of
N products and additions. In the second step e−b·dQ(x,z) can be
precomputed as b, X and Z do not change during the algorithm, so
we only have to make N multiplications, and in the third step we
z′∈Z p(z′|x) and then perform N divisions.
It is clear then that the cost grows with the sizes of X and Z.
However, the algorithm only needs to be computed once for all the
users, which can be done in the cloud, and even if the prior π varies
we can use a previously computed algorithm as initialization of the
iteration above to get a fast update of the mechanism.
compute |X| values of
The second remark is that the mechanism produced by this
algorithm also satisfies 2b-geo-indistinguishability (the proof is
in the Appendix). This is a byproduct property that was not part
of the reasoning behind the algorithm and it does not imply that
the conditional entropy and geo-indistinguishability are related.
In fact, these are fundamentally different notions: the former is an
average metric that only considers the probabilistic (and not the
geographic) aspect of the problem, while the latter is a worst-case
metric that also considers the geography of the problem. Also, if
we truncate the optimal conditional entropy mechanism, we obtain
a mechanism that is almost optimal in terms of conditional entropy
but does not provide any level of geo-indistinguishability.
We evaluate this mechanism and others with respect to the
conditional entropy and the traditional metrics in Section 5.
4.2 The Worst-Case Quality Loss as a
Complementary Metric
4.2.1 Usefulness of the Worst-Case Quality Loss. After analyzing
the privacy problems of the coin mechanism, we now turn to the
utility point of view. The great drawback of the coin mechanism
from the quality loss perspective is that if the coin shows tails then
the server’s response to the user’s query will most likely be useless
due to the great quality loss incurred by reporting z∗. We can think
of many applications where, if the Euclidean distance between x
and z is larger than a certain value, the user gets literally nothing
from the server response. For example, if we are close to a point of
interest x and we want to find a nearby hospital, querying about
a location z in another city will likely return a useless response
from the server. In that case, we could think of generating another
output and query the server again because we did not get what
we were hoping for. By doing so, the privacy properties of the
mechanism change, and in the case of the coin it is equivalent to
always revealing our true location.
A solution to this utility issue consists in imposing a worst-case
quality loss constraint on the mechanism, i.e.,
Q+(f , π) = max
x,z
π(x)>0
f (z|x)>0
dQ(x, z) ≤ Q+
max .
(22)
To put it simply, we want a mechanism that releases output locations
max from the input location, i.e., a bounded mechanism. The
within Q+
upper bound Q+