Following the above theorems, Equation 2 can be rewritten as
− 1
2f
d
γ
≥ β ≥
f
f − 1
d − dγ
γ − dγ
α
(7)
Since these conditions are more restrictive than the ones of Equa-
tion 2, the selected parameters will respect the privacy and the
utility requirements of the original (α, β )-sanitization algorithm.
The remaining question is to determine whether or not these in-
equalities admit some solutions.
We now revisit the discretized scenario presented earlier. In this
example, the adversary prior knowledge is defined by the parame-
ters d = 10 |DB|
|D| ≈ 0.0067 and γ = 0.2. If the constant f in the above
inequalities is set to 100, the value of d
= 0.0335 would be large
γ
compared to 1/(2f ) = 0.005, and guarantee the (ρ, ϵ )-usefulness
of the personalized view. Such a value for f would nevertheless
|D−DB|
allow the possibility to have large fingerprints. For example, if we
≈ 450, 000, this would easily tackle at least up
choose l  0, or
the view would not be (ρ, ϵU )-useful.
To prove such a theorem, the properties of the underlying fin-
gerprinting schemes are important. Thus, in the remaining of this
section, we assume that the Tardos codes are used.
Unfortunately, the (α, β )-sanitization concept gives only suffi-
cient conditions for the utility property. Hence, the best that we can
do is to give the most “rigorous” argument to support our claim.
Once the adversary has his views, he can easily determine some
of the marking positions in Pos. According to the marking assump-
tion, these positions correspond to false tuples that are in some
views and not in others. These marking tuples may be either kept
or deleted. In the former case, this sets a 1-bit in the counterfeit
code in the given position. In the latter case, this leads a 0-bit. Furon,
Guyader and Cérou [9] have shown that no adversarial strategies
are better than others to avoid detection.
There are mainly two accusation strategies to identify the mali-
cious buyers with Tardos codes. In the first one, any buyer that has
an accusation score higher than a pre-defined threshold is deemed
as malicious. In the second one, only the buyer with the highest
accusation score above the threshold is considered suspicious. Nat-
urally, the latter strategy is less prone to false positive alarms (i.e.,
falsely accusing an innocent buyer) and is generally used.
At this point, the classical results on the accusation process of
the Tardos codes are sufficient to trace back at least one member
of the collusion if the size of the collusion is not too large. Due
to the nature of our problem, this is unfortunately insufficient.
As mentioned previously, the adversary may decide to alter his
counterfeit view further in order to alter the embedded codes in
his views. The number of these invariant bits represents a fixed
fraction of the code length, depending only on the parameter c
(see Appendix). Classically, this approach is not common when
Session 6: Privacy 1ASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea216embedding Tardos codes in multimedia files. Indeed, randomly
flipping bits of pictures or videos has a major impact on their quality.
The merchant must therefore assume that an adversary randomly
alters his counterfeit view as much as possible to prevent to be
identified. Obviously, there is a limit to this approach as the resulting
view must still be useful. Hence, the adversary can decide to add
tuples that are not in any of his views in VC. By doing so, he can
hope to hit some of the invariant marking positions and alter the
embedded code by flipping 0-bit into 1-bit. Due to the scarcity of
the true tuples, most of these added tuples are false tuples. This
will have a significant impact on the utility of the resulting view. A
better adversarial strategy is to remove some of the invariant tuples
in his views, which flips 1-bit into 0-bit in the embedded codes.
In our example introduced in the previous section, the sanitized
view V has 673,673 false tuples and 15,007 true tuples. If Tardos
codes of length l = 100, 000 are embedded into this view, 50,000
more false tuples on average would be added into the view. Thus,
50,000 tuples out of the 738,680 in the view would be used to embed
1-bit (1 out of 15). On the other hand, 50,000 missing tuples out of
more than 44,500,000 missing tuples in the view would be used to
embed 0-bit (1 out of 900). This is highly asymmetric and the cost
of flipping invariant 0-bit of the code seems to be prohibitive for
the usefulness of the resulting view.
In this context, the question to be answered is how many in-
variant bits of the codes have to be flipped in order to reduce the
accusation scores of the colluders below the accusation threshold.
To answer this question, the following experiment has been done:
a thousand Tardos codes have been generated with the parameters
c = 5 and ϵ = 10−6. Due to the structure of our problem, there is a
huge advantage to use an asymmetric accusation process based only
on the 0-bit of the codes. This can be seen exactly as the dual of the
original Tardos codes. With the given parameters, the code length
is approximately 35, 000 and the accusation threshold is around
1, 380. Five of these thousand codes have been randomly selected to
form a collusion. For this collusion, the invariant 0-bit are flipped
with a probability p to forge a counterfeit code, while the mutant
bits are set to 1 since they are not considered in the accusation
process. The scores of colluders as well as innocent buyers are then
computed. This experiment has been repeated a hundred times and
five new colluders are selected for each of these iterations.
Figure 2 reports Max-Min charts for the maximum scores of the
colluders and the innocents for different values of p. As we can
observe, even if 30% of the invariant 0-bit are flipped, the accu-
sation process can still identify unambiguously a member of the
collusion. The horizontal line defines the accusation threshold. In
the Appendix, we present a simple argument (Fact 4) to show that
at least one third of the invariant bits have to be flipped to reduce
the average score of the colluders below the accusation threshold.
However, even this would be sufficient as the scores of some col-
luders may still be above the threshold. This result validates the
experimentations presented in Figure 2.
Based on this experimentation and its theoretical derivation,
even if the adversary flips at least one third of his invariant 0-bit,
at least one member of his collusion can still be detected. More
precisely, the invariant part of the codes associated to a collusion
is embedded in the invariant part of the database views. This corre-
sponds to a few thousands of indistinguishable positions amongst
Figure 2: Max-Min charts of the maximum scores for the col-
luders (upper one in blue) and the innocents (lower one in
red). The dashed line is the accusation threshold.
millions of positions of the overall domain. Thus, the adversary
does not have a better strategy than to add randomly missing ones.
Figure 3: Comparison of the query estimators on the sani-
tized view and the true query values on the discretized data-
base for counting queries on two or three attributes. The
mean square error and the mean absolute percentage error of
the estimators are approximatively 106 and 80%, respectively.
As one can imagine, adding too many false tuples will have a
major impact on the usefulness of the resulting view. The experi-
mentation presented in the previous section has been repeated and
fifteen percent of the missing tuples were added to the counterfeit
view. Thus, in total 6,650,894 tuples were added. Naturally, this
flips only fifteen percent of the invariant 0-bit of the codes. Some
colluders are still identifiable, but nevertheless, the usefulness of
the resulting view is already doubtful as one can observe in Fig-
ure 3. In this figure, the parameters α and β have to be modified as
α′ = α + (1−α )p and β′ = β + (1− β )p, in which p is the probability
00.10.20.30.40.5Flipping probability for 0-bit05001000150020002500Intervals of the maximum accusation scoresSession 6: Privacy 1ASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea217of adding a missing tuple. Since at least twice as many tuples have
to be added to see all the colluder scores falling below the accu-
sation threshold (in some instances), our approach is successful.
Indeed, either a counterfeit view would lead to a colluder or the
counterfeit view is useless to extract meaningful statistics from it.
A more formal way to look at the utility problem is to return
to Definition 9. Assume that the adversary adds a third of the
domain in his view ito flip a third of his invariant 0-bit of his
code and thwart the accusation process. For any counting query
Q such that Q (D) ∈ Ω(|D|) and any database DB ⊂ D such that
√|DB|
|DB| ∈ o(|D|), the probability Pr
should be simply one. Since the counterfeit view V can be seen
has a random subset of D with the third of its tuples, the estimator
˜Q (V ) should be also in Ω(|D|). On the other hand, simply due
to its cardinality, Q (DB) should be in o(|D|). Hence, the additive
√|DB|). This is true for any
error of the estimator cannot be in O (
estimator and for any counting query with a linear expectation.
Thus, a non-traceable counterfeit view cannot be useful according
to the definition.
|Q (DB) − ˜Q (V )| ≥ ρ
(cid:102)
(cid:103)
6 CONCLUSION
In this work, we have proposed SaPData, a novel approach to en-
twine sanitization and personalization on a database before its
release. More precisely, our solution ensures the privacy of the per-
sonal information contained in the distributed view, and the utility
of this view through the seminal work on the (α, β)-sanitization
algorithm [18]. In addition, we ensure that the owner of the data-
base can trace buyers who illegally redistribute his view through
the use of binary fingerprinting codes, which tolerate attacks from
a collusion of a limited number of malicious buyers.
In order to show that SaPData reaches privacy, utility and se-
curity requirements, we prove that the insertion of a fingerprint
does not alter the privacy and the utility properties of the (α, β)-
sanitization algorithm. We have also shown that a malicious buyer
with the help of colluding buyers cannot remove the information
(fingerprint) used to trace him back while maintaining the utility
of the database. Finally, SaPData is blind in the sense that during
the DBAccuse algorithm, the secret sanitized view is not necessary.
As future work, we are planning to investigate several research
directions extending the integration between sanitization and per-
sonalization for the distribution of private databases. In particular,
instead of generating false tuples by sampling uniformly at ran-
dom among the domain of the database with respect to the (α, β)-
sanitization algorithm [18], we want to explore if a generative model
trained on real data but disjoint from the private database could
improve on the resulting utility of the sanitized database. Going
further, we will also see if the (α, β)-sanitization algorithm could
be replaced by another approach providing also strong privacy
guarantees such as the recent work on producing synthetic data
based on plausible deniability [3].
Finally, we also want to design other approaches for the joint
sanitization and personalization of private databases in which the
fingerprint is included directly in the records themselves (as it is
traditionally the case) in order to gain a deeper knowledge on the
tradeoff that needs to be set between utility, privacy and security
for this particular task.
REFERENCES
[1] Agrawal, R., and Kiernan, J. Watermarking relational databases. In Proc. of the
28th Int. Conf. on Very Large Data Bases (2002), VLDB Endowment, pp. 155–166.
[2] Bertino, E., Ooi, B. C., Yang, Y., and Deng, R. H. Privacy and ownership
preserving of outsourced medical data. In Proc. of the 21st Int. Conf. on Data
Engineering (2005), IEEE, pp. 521–532.
[3] Bindschaedler, V., Shokri, R., and Gunter, C. A. Plausible deniability for
privacy-preserving data synthesis. Proc. of the VLDB Endowment 10, 5 (2017),
481–492.
[4] Boneh, D., and Shaw, J. Collusion-secure fingerprinting for digital data. IEEE
Trans. on Information Theory 44 (1998), 1897–1905.
[5] Burmester, M., and Le, T. Attack on Sebé, Domingo-Ferrer and Herrera-
Joancomarti fingerprinting schemes. Electronics Letters 40, 3 (2004), 172–173.
[6] Dinur, I., and Nissim, K. Revealing information while preserving privacy. In Proc.
of the 22nd Symp. on Principles of Database Systems (2003), ACM, pp. 202–210.
[7] Domingo-Ferrer, J., and Herrera-Joancomarti, J. Short collusion-secure
fingerprints based on dual binary Hamming codes. Electronics Letters 36, 20
(2000), 1697–1699.
[8] Dwork, C., McSherry, F., Nissim, K., and Smith, A. Calibrating noise to sen-
sitivity in private data analysis. In Proc. of the Third Theory of Cryptography
Conference (2006), Springer, pp. 265–284.
[9] Furon, T., Guyader, A., and Cérou, F. On the design and optimization of Tardos
probabilistic fingerprinting codes. In Proc. of the 10th Int. Workshop on Information
Hiding (2008), Springer, pp. 341–356.
[10] Ganta, S. R., Kasiviswanathan, S. P., and Smith, A. Composition attacks and
auxiliary information in data privacy. In Proc. of the 14th ACM SIGKDD Int. Conf.
on Knowledge Discovery and Data Mining (2008), ACM, pp. 265–273.
[11] García, S., Luengo, J., Sáez, J. A., López, V., and Herrera, F. A survey of dis-
cretization techniques: Taxonomy and empirical analysis in supervised learning.
IEEE Trans. on Knowledge and Data Engineering 25 (2013), 734–750.
[12] Kieseberg, P., Schrittwieser, S., Mulazzani, M., Echizen, I., and Weippl,
E. An algorithm for collusion-resistant anonymization and fingerprinting of
sensitive microdata. Electronic Markets 24 (2014), 113–124.
[13] Lafaye, J., Gross-Amblard, D., Constantin, C., and Guerrouani, M. Watermill:
An optimized fingerprinting system for databases under constraints. IEEE Trans.
on Knowledge and Data Engineering 20 (2008), 532–546.
[14] Li, N., Li, T., and Venkatasubramanian, S. t-closeness: Privacy beyond k-
anonymity and l-diversity. In Proc. of IEEE 23rd Int. Conf. on Data Engineering
(2007), IEEE, pp. 106–115.
[15] Liu, C., Chakraborty, S., and Mittal, P. Dependence makes you vulnerable:
Differential privacy under dependent tuples. In Proc. of the 23nd Annual Network
and Distributed System Security Symposium (2016), Internet Society, pp. 1–15.
[16] Machanavajjhala, A., Kifer, D., Gehrke, J., and Venkitasubramaniam, M.
l-diversity: Privacy beyond k-anonymity. ACM Trans. on Knowledge Discovery
from Data 1 (2007), 1–52.
[17] Peikert, C., shelat, A., and Smith, A. Lower bounds for collusion-secure
fingerprinting. In Proc. of the 14th Annual ACM-SIAM Symp. on Discrete Algorithms
(2003), SIAM, pp. 472–479.
[18] Rastogi, V., Hong, S., and Suciu, D. The boundary between privacy and utility
in data publishing. In Proc. of the 33rd Int. Conf. on Very Large Data Bases (2007),
VLDB Endowment, pp. 531–542.
[19] Rastogi, V., Suciu, D., and Hong, S. The boundary between privacy and utility
in data anonymization. Tech. rep., arxiv.org, 2006.
[20] Schrittwieser, S., Kieseberg, P., Echizen, I., Wohlgemuth, S., Sonehara,
N., and Weippl, E. An algorithm for k-anonymity-based fingerprinting. In