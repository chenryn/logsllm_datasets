To understand the frequency of different methods for propagating error information in practice, we analyzed over 20,000 functions in the Ubuntu Linux libraries. We utilized the ELSA C/C++ parser [6] to analyze the library headers in all development packages and combined this information with the LFI (Library Fault Injection) analyses described above. The results are summarized in Table 1, where row labels indicate function return types, column labels indicate the method for providing error details, and cell values represent the corresponding fraction of all analyzed functions.

| Return Type | None | Error Details in Global Location | Error Details via Arguments |
|-------------|------|---------------------------------|-----------------------------|
| void        | 23.0% | 0%                              | 0%                          |
| scalar      | 56.5% | 1%                              | 3.5%                        |
| pointer     | 11.6% | 1%                              | 3.4%                        |

**Table 1. Statistics on how Linux libraries provide additional details on error conditions exposed to callers.**

### 3.3 Fault Profile

The output of the LFI profiler is a fault profile of the analyzed library. This output is intended to be passed to the LFI controller but can also be used for other purposes, such as cross-checking API documentation. Therefore, we chose a general XML format that is both human-readable and easy to parse. LFI generates one profile per analyzed library. For each exported function, the profile contains information regarding possible error return values, along with a specification of associated side effects for each such value.

Here is a snippet of the profile generated for the `libc` `close` function:

```xml
<function name="close">
    <error code="-1">
        <side-effect>
            <tls-variable name="errno" offset="0">
                <possible-values>-9, -5, -4</possible-values>
            </tls-variable>
        </side-effect>
    </error>
</function>
```

In case of an error, `close` returns `-1` and provides additional information via a TLS (Thread-Local Storage) variable (`errno`) at the given offset. This side effect can have values `-9` (corresponding to `EBADF` = bad file descriptor), `-5` (for `EIO` = input/output error), or `-4` (for `EINTR` = interrupted system call). Incidentally, this is another example where man pages can be misleading: on BSD systems, the man page accurately states that `close` can only set `errno` to `EBADF` or `EINTR`. On Linux, `EIO` is also possible, so programmers porting from BSD to Linux might forget to add a check for `EIO`. Similarly, if porting to HP/UX, they might forget to check for `ENOSPC`, or on Solaris, they might forget about `ENOLINK`, all of which are return codes present in the corresponding `libc` libraries. LFI can automatically find the errors specific to the platform and test the programs with those values.

### 4. Fault Injection Scenarios

A fault injection scenario describes a sequence of faults to be injected, also referred to as a "fault load." Such a scenario pairs faults with triggers, i.e., conditions that, when true, should lead to an injection. We designed a simple XML-based language to describe scenarios as sets of tuples. Every time a function is intercepted, the relevant triggers are evaluated, and if any is true, the associated fault(s) is/are injected.

Due to space constraints, we do not describe the language in detail but provide an illustrative example below:

```xml
<trigger>
    <call-count>5</call-count>
    <function>readdir64</function>
    <return-value>0</return-value>
    <set-errno>EBADF</set-errno>
    <skip-original>true</skip-original>
</trigger>
<trigger>
    <call-count>5</call-count>
    <function>readdir</function>
    <call-stack>
        <frame address="0xb824490"/>
        <frame function="refresh_files"/>
    </call-stack>
    <set-errno>EBADF</set-errno>
</trigger>
<trigger>
    <call-count>20</call-count>
    <function>read</function>
    <modify-argument index="3" operation="subtract" value="10"/>
</trigger>
```

The first trigger matches the 5th call to the `readdir64` function and returns a value of `0` (null pointer), sets `errno` to `EBADF` ("bad file descriptor"), and does not call the original `readdir64` function. The second trigger matches the 5th call to the `readdir` function, and if the call stack has the address `0xb824490` in the first frame and the function `refresh_files` in the second frame, it injects `EBADF`. The third trigger matches the 20th call of the `read` function, modifies its 3rd argument (the number of bytes to read) by subtracting `10` from it, and then passes the call to the original `read` library call.

### 5. LFI Controller

The LFI controller (Figure 3) receives from the profiler the fault profile(s) of interest along with a fault scenario, either automatically generated by the profiler or customized by the developer. It then generates interception stubs, which are combined with boilerplate code to synthesize a new library. This synthetic library has the same API as the original one but encodes the fault injection logic underneath this API. This new library is shimmed between the program being tested and the original library(ies).

**Figure 3. The LFI controller.**

Once the stubs are generated and installed (Section 5.1), the LFI controller invokes a developer-provided script that starts the program under test, exercises it with the desired workload, and monitors its behavior to determine whether it terminates normally or with an error exit code. This information is collected in a log, along with an LFI-generated replay script for each fault injection test case; the test scripts allow the developer to diagnose and debug (Section 5.2).

#### 5.1 Interception Mechanics

The shimming of the synthesized library is system-specific. On Linux and Solaris, LFI uses the `LD_PRELOAD` environment variable to tell the dynamic linker to load the LFI-generated library before the original one. On Windows, LFI uses a combination of `WriteProcessMemory` and `CreateRemoteThread`, and passes the address of `LoadLibrary` as the thread start address, to force a process to load the synthetic library.

A synthesized library consists of stubs that intercept the library calls. Each stub determines the address of the original function, evaluates the triggers from the scenario, and if an injection is to be done, determines the return value/side effect to be injected and/or whether the call should be passed to the original function or not. If no injection is to be done, it cleans up the stack and jumps to the original function. In general, using a `jmp` instruction (instead of `call`) simplifies the handling of the original return address from the stack because it avoids the need for save/restore.

A stub looks approximately as follows:

```c
int LIB_FUNC_NAME(void) {
    static void*(*original_fn_ptr)();
    static int call_count;
    call_count++;
    if (!original_fn_ptr)
        original_fn_ptr = (void*(*)()) dlsym(RTLD_NEXT, #FUNC_NAME);
    if (eval_trigger(LIB_FUNC_NAME, call_count, call_stack)) {
        /* determine return_code, side_effects */
        /* apply side_effects */
        return return_code;
    } else {
        /* return stack and registers to orig values */
        __asm__("jmp *original_fn_ptr");
        /* orig func will return directly to caller */
    }
}
```

Interceptors for multiple libraries can coexist (Section 6.4). This happens transparently because the interception mechanism only relies on the function name, not on the library where the original function resides. Thus, stubs for functions from different libraries do not interfere with each other. Although interception is specific to the OS and CPU architecture, porting to new platforms is straightforward.

#### 5.2 Controller Output

The LFI controller collects information that helps developers reproduce, understand, and fix the behaviors observed as a result of fault injection. The LFI log is a text file that records each injection, the applied side effects, and the events that triggered that injection (e.g., call count, stack trace). This output can be used to match injections to observed program behavior, as well as to refine the fault scenario. The replay scripts are automatically generated XML files that can be fed back to the LFI controller to reproduce the desired test case on a subsequent run. Replay is not always 100% accurate because LFI does not control thread interleaving, timer inputs, etc. We have found that these replay scripts can save a lot of time during ad-hoc testing and can also augment existing regression test suites.

### 6. Evaluation

Our goal in building LFI was to make testing based on fault injection easier and less human-intensive. In this section, we evaluate the extent to which we reached this goal. We analyze the effectiveness of using LFI in testing (Section 6.1), measure its efficiency (Section 6.2), assess the accuracy of the LFI profiler (Section 6.3), and measure the performance overhead incurred during testing (Section 6.4).

LFI currently works on Linux and Windows (both Intel IA-32 and IA-64 architectures), as well as Solaris (SPARC architecture). We expect LFI to be easily ported to other systems as well.

#### 6.1 Effectiveness

**Ease of Use:** LFI's primary contribution is ease of use. The human effort involved in the basic use of LFI is small: it requires issuing two commands, one for profiling and one for running the tests. When the tester modifies the default fault profiles, more time is required, but we expect this to be easier than directly scripting fault injection experiments. Below, we illustrate LFI usage with an example.

We tested Pidgin [15], a popular instant messenger client, by instructing the LFI controller to launch it and exercise a random fault injection scenario on I/O functions with a 10% probability. Shortly after we entered the IM login details in Pidgin, it crashed with a `SIGABRT`. We restarted Pidgin using the corresponding replay script and attached with `gdb`; it crashed again, and we were able to inspect the program state. In a matter of minutes, we discovered the issue: Pidgin forks a DNS resolver process to perform host resolution asynchronously. This process then communicates back to the parent via a pipe. The child does not handle the case when writes fail or are incomplete. As a result, the child may write the answer to the parent, but if the write is incomplete, it may subsequently write additional data corresponding to another request. As a result, the parent reads the status (which is ok) and then reads the size of the (resolved) addressâ€”due to the partial write, this read returns data written after the injection, in our case a very large value. The parent calls `malloc` for this amount of memory, which results in `SIGABRT` because it is unable to allocate the memory. Further details appear in the bug report we filed [16].

**Improving Coverage:** Besides ease-of-use, effectiveness can also be measured by whether LFI can improve existing regression test suites. We considered the MySQL database server, which is the most mature open-source RDBMS, first released in 1995; it claims 11 million installations [4]. MySQL ships with its own thorough test suite. The MySQL 5.0 test suite achieves 73% overall basic block coverage, which is remarkable for an open-source project. We therefore set out to see if we could improve this with no human effort. We ran LFI in fully automatic mode, generating a random fault injection scenario based on `libc`. With no human help, LFI improved the coverage of the MySQL test suite to at least 74% overall. In some modules (such as the InnoDB ibuf implementation), coverage improved by 12%. We expect the coverage numbers to be slightly higher because, in 12 cases, MySQL crashed with `SIGSEGV`, and the coverage information for those test cases was not saved. We are encouraged by the fact that, with no human assistance, LFI was able to improve a mature, extensive test suite.

**Finding Obscure Scenarios:** A third aspect of effectiveness is whether LFI is able to exercise scenarios that existing tools would not find. As already mentioned in Sections 3.1 and 3.3, the LFI profiler found several return error codes that are missing from the API documentation of popular libraries. By analyzing the binary directly, LFI helps testers find fault scenarios that they would otherwise not be aware of. Knowing these additional fault scenarios enables testers to write more thorough tests.

#### 6.2 Efficiency

The running time of a test tool is an important factor in its adoption because testers are generally unwilling to wait long for results. For example, the long running times of model checkers have discouraged their wide use in testing. The LFI profiler is fast: we measured profiling times ranging from 0.2 seconds for a small library (`libdmx`, with 18 exported functions and an 8 KB code segment) to 20 seconds for a large library (`libxml2`, with 1612 exported functions and an 897 KB code segment). To profile the >1,000 libraries found on a typical Linux system takes several hours, but in practice, we expect testers to only profile the libraries used by the program of interest. When updating a library on the system, which we expect will happen about once a month, it takes on the order of minutes to re-analyze the library.