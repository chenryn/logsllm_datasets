datasets (Dğ‘š, Dâ€²
ğ‘š) âˆˆ Dğ‘šsame, so, in order to prove Theorem 3.7,
we need to compute sup(Dğ‘š,Dâ€²
. We bound
this in the following.
Define a set Tğœ–0 consisting of all pairs of ğµ-dimensional proba-
(cid:18)ğµ
bility vectors satisfying the ğœ–0-LDP constraints as follows:
ğ‘š)âˆˆDğ‘šsame
ğ‘â€²2
ğ‘—
ğ‘ ğ‘—
âˆ’ 1
(cid:19)
ğ‘—=1
(cid:110)(ğ’‘, ğ’‘â€²) âˆˆ Rğµ Ã— Rğµ : ğ‘ ğ‘— , ğ‘â€²
and ğ‘’âˆ’ğœ–0 â‰¤ ğ‘â€²
ğµâˆ‘ï¸
â‰¤ ğ‘’ğœ–0,âˆ€ğ‘— âˆˆ [ğµ](cid:111).
ğ‘—=1
ğ‘ ğ‘— =
ğ‘— â‰¥ 0,âˆ€ğ‘— âˆˆ [ğµ],
ğ‘—
ğ‘ ğ‘—
(36)
Note that Tğœ–0 contains all pairs of the output probability distribu-
tions (ğ’‘, ğ’‘â€²) of all ğœ–0-LDP mechanisms R on all neighboring data
points ğ‘‘, ğ‘‘â€² âˆˆ X. Since any (Dğ‘š, Dâ€²
ğ‘š) âˆˆ Dğ‘šsame generates a pair
of probability distributions (ğ’‘, ğ’‘â€²) âˆˆ Tğœ–0 (because Dğ‘š = (ğ‘‘, . . . , ğ‘‘)
and Dâ€²
ğ‘š = (ğ‘‘, . . . , ğ‘‘, ğ‘‘â€²) together contain only two distinct data
points ğ‘‘, ğ‘‘â€²), we have
ğµâˆ‘ï¸
ğ‘—=1
Tğœ–0 =
ğ‘â€²
ğ‘— = 1,
(cid:169)(cid:173)(cid:171) ğµâˆ‘ï¸
ğ‘—=1
âˆ’ 1(cid:170)(cid:174)(cid:172) .
ğ‘â€²2
ğ‘—
ğ‘ ğ‘—
In the following lemma, we bounds the RHS of (37).
sup
ğ‘š)âˆˆDğ‘šsame
(ğ’‘,ğ’‘â€²)âˆˆTğœ–0
ğ‘â€²2
ğ‘—
ğ‘ ğ‘—
(Dğ‘š,Dâ€²
sup
Lemma 6.2. We have the following bound:
(cid:169)(cid:173)(cid:171) ğµâˆ‘ï¸
ğ‘—=1
âˆ’ 1(cid:170)(cid:174)(cid:172) â‰¤
(cid:169)(cid:173)(cid:171) ğµâˆ‘ï¸
âˆ’ 1(cid:170)(cid:174)(cid:172) =
ğ‘â€²2
ğ‘—
ğ‘ ğ‘—
ğ‘—=1
sup
(ğ’‘,ğ’‘â€²)âˆˆTğœ–0
(ğ‘’ğœ–0 âˆ’ 1)2
ğ‘’ğœ–0
.
(37)
(38)
We prove Lemma 6.2 in Appendix C.2. Taking supremum over
ğ‘š) âˆˆ Dğ‘šsame in (35) and then using (37) and (38), we get the
(Dğ‘š, Dâ€²
bound in Theorem 3.7.
Session 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 23327 PROOFS OF THE UPPER BOUNDS
In this section, we will prove our upper bounds stated in Theo-
rems 3.1 and 3.3 in Sections 7.1 and 7.2, respectively.
same.
ğ‘›, ğ‘‘â€²
Eğ’‰âˆ¼M(Dâ€²)
ğ‘›, . . . , ğ‘‘â€²
M(Dâ€²)(ğ’‰)
Recall from Theorem 3.6, we have
ğ‘›, . . . , ğ‘‘â€²
ğ‘š+1, D(ğ‘›)
ğ‘š+1
7.1 Proof of Theorem 3.1
Consider arbitrary neighboring datasets D = (ğ‘‘1, . . . , ğ‘‘ğ‘›) âˆˆ Xğ‘›
for any ğ‘š âˆˆ {0, . . . , ğ‘› âˆ’ 1}, we define new neighboring datasets
D(ğ‘›)
and Dâ€² = (cid:0)ğ‘‘1, . . . , ğ‘‘ğ‘›âˆ’1, ğ‘‘â€²
ğ‘›(cid:1) âˆˆ Xğ‘›. As mentioned in Section 3.3,
ğ‘š+1 =(cid:0)ğ‘‘â€²
ğ‘š+1 =(cid:0)ğ‘‘â€²
ğ‘›(cid:1), each having
ğ‘›, ğ‘‘ğ‘›(cid:1) and Dâ€²(ğ‘›)
(cid:17) âˆˆ Dğ‘š+1
(ğ‘š + 1) elements. Observe that(cid:16)Dâ€²(ğ‘›)
(cid:34)(cid:18) M(D)(ğ’‰)
(cid:19)ğœ†(cid:35)
(cid:33)ğœ†
(cid:32)M(D(ğ‘›)
 ,
E
â‰¤ ğ‘›âˆ’1âˆ‘ï¸
(cid:1)ğ‘ğ‘š(1 âˆ’ ğ‘)ğ‘›âˆ’ğ‘šâˆ’1. For simplicity of notation, for
where ğ‘ğ‘š :=(cid:0)ğ‘›âˆ’1
(cid:33)ğœ† .
(cid:32)M(D(ğ‘›)
any ğ‘š âˆˆ {0, 1, . . . , ğ‘› âˆ’ 1}, define
We show in Appendix D.1 that ğ¸ğ‘š is a non-increasing function of
ğ‘š. Using this and concentration properties of the Binomial r.v., we
get (details are in Appendix D.1):
ğ‘š+1)(ğ’‰)
ğ‘š+1)(ğ’‰)
ğ‘š+1)(ğ’‰)
ğ‘š+1)(ğ’‰)
ğ’‰âˆ¼M(Dâ€²(ğ‘›)
ğ‘š+1)
ğ’‰âˆ¼M(Dâ€²(ğ‘›)
ğ‘š+1)
M(Dâ€²(ğ‘›)
M(Dâ€²(ğ‘›)
ğ¸ğ‘š := E
ğ‘ğ‘š
ğ‘š=0
(39)
ğ‘š
â‰¤ ğ‘’ğœ–0ğœ†ğ‘’âˆ’ ğ‘(ğ‘›âˆ’1)ğ›¾2
2
+ ğ¸(1âˆ’ğ›¾)ğ‘(ğ‘›âˆ’1),
(40)
where ğ›¾ > 0 is arbitrary, and expectation is taken w.r.t. ğ’‰ âˆ¼ M(Dâ€²).
Note that we have already bounded ğ¸ğ‘š for all ğ‘š in Theorem 3.7.
2ğ‘’ğœ–0 âŒ‹ + 1, we get
By setting ğ›¾ =
from Theorem 3.7, that:
2 and ğ‘› = âŒŠ(1 âˆ’ ğ›¾)ğ‘(ğ‘› âˆ’ 1)âŒ‹ + 1 = âŒŠ ğ‘›âˆ’1
1
(cid:34)(cid:18) M(D)(ğ’‰)
M(Dâ€²)(ğ’‰)
(cid:19)ğœ†(cid:35)
E
Since the above bound holds for arbitrary pairs of neighboring
datasets D and Dâ€², this completes the proof of Theorem 3.1.
7.2 Proof of Theorem 3.3
The proof of Theorem 3.3 follows the same steps as that of the
proof of Theorem 3.1 that we outlined in Section 3.3 and also gave
formally in Section 7.1, except for the following change. Instead of
using Theorem 3.7 for bounding the RDP for specific neighboring
datasets, we will use the following theorem.
Theorem 7.1. Let ğ‘š âˆˆ N be arbitrary. For any ğœ† â‰¥ 2 (including
the non-integral ğœ†) and any (Dğ‘š, Dâ€²
(cid:34)(cid:18)M(cid:0)Dâ€²
ğ‘š(cid:1) (ğ’‰)
M (Dğ‘š) (ğ’‰)
(cid:19)ğœ†(cid:35)
(cid:18)
ğ‘š) âˆˆ Dğ‘šsame, we have
ğœ†2 (ğ‘’ğœ–0 âˆ’ 1)2
â‰¤ exp
.
ğ‘š
(cid:19)
Eğ’‰âˆ¼M(Dğ‘š)
(42)
(cid:19)ğœ†(cid:35)
(cid:34)(cid:18) M (D) (ğ’‰)
(cid:19) (ğ‘’ğœ–0 âˆ’ 1)2
(cid:19)
(cid:18)ğœ†
+ ğœ†âˆ‘ï¸
M (Dâ€²) (ğ’‰)
ğ‘›ğ‘’ğœ–0
ğ‘–
ğ‘–=3
(cid:18)ğœ†
2
Eğ’‰âˆ¼M(Dâ€²)
â‰¤ 1 +
â‰¤ ğ¸ğ‘›âˆ’1 + ğ‘’ğœ–0ğœ†âˆ’ ğ‘›âˆ’1
8ğ‘’ğœ–0
(cid:32)(cid:0)ğ‘’2ğœ–0 âˆ’ 1(cid:1)2
ğ‘–Î“ (ğ‘–/2)
2ğ‘›ğ‘’2ğœ–0
(cid:33)ğ‘–/2
(41)
+ ğ‘’ğœ–0ğœ†âˆ’ ğ‘›âˆ’1
8ğ‘’ğœ–0 .
We prove Theorem 7.1 in Appendix D.2. Note that Theorem 7.1
(cid:17) holds for every integer ğ‘š â‰¥ 2.
Substituting this in (41) (by putting ğ‘š = ğ‘› = âŒŠ ğ‘›âˆ’1
2ğ‘’ğœ–0 âŒ‹ + 1), we get
implies that ğ¸ğ‘šâˆ’1 â‰¤ exp(cid:16)ğœ†2 (ğ‘’ğœ–0âˆ’1)2
(cid:34)(cid:18) M (D) (ğ’‰)
(cid:19)ğœ†(cid:35)
ğ‘š
Eğ’‰âˆ¼M(Dâ€²)
M (Dâ€²) (ğ’‰)
â‰¤ ğ‘’ğœ†2 (ğ‘’ğœ–0 âˆ’1)2
ğ‘›
+ ğ‘’ğœ–0ğœ†âˆ’ ğ‘›âˆ’1
8ğ‘’ğœ–0 .
This proves Theorem 3.3.
8 PROOF SKETCH OF THE LOWER BOUND
Consider the binary case, where each data point ğ‘‘ can take a value
from X = {0, 1}. Let the local randomizer R be the binary ran-
domized response (2RR) mechanism, where Pr [R (ğ‘‘) = ğ‘‘] = ğ‘’ğœ–0
ğ‘’ğœ–0+1
for ğ‘‘ âˆˆ X. It is easy to verify that R is an ğœ–0-LDP mechanism.
1
ğ‘’ğœ–0+1. Consider two neighboring datasets
For simplicity, let ğ‘ =
D, Dâ€² âˆˆ {0, 1}ğ‘›, where D = (0, . . . , 0, 0) and Dâ€² = (0, . . . , 0, 1).
Let ğ‘˜ âˆˆ {0, . . . , ğ‘›} denote the number of ones in the output of the
shuffler. As argued in Section 2.3 on page 4, since the output of
the shuffle mechanism M can be thought of as the distribution
of the number of ones in the output, we have that ğ‘˜ âˆ¼ M(D)
is distributed as a Binomial random variable Bin(ğ‘›, ğ‘). The proof
uses some properties of the Binomial r.v., which are provided in
Appendix E.
9 CONCLUSION
The analysis of the RDP for the shuffle model presented in this
paper was based on some new analysis techniques that may be of
independent interest. The utility of these bounds were also demon-
strated numerically, where we saw that in important regimes of
interest, we get 8Ã— improvement over the state-of-the-art without
sampling and at least 10Ã— improvement with sampling (see Section
4 for more details).
A simple extension of the results would be to work with local
approximate DP guarantees instead of pure LDP. This can be seen
by using the tight conversion between approximate DP and pure DP
given in [24]. However, there are several open problems of interest.
Our upper bounds hold for general discrete local mechanisms. The
extension to continuous distributions requires careful technical
analysis as the histogram used for RDP analysis would need to
approximate continuous distributions via discretization. We leave
the analysis of continuous distributions as a future work. Perhaps
the most important one is mentioned in Remark 7. There is a multi-
plicative gap of the order ğ‘’ğœ–0 in our upper and lower bounds, and
closing this gap is an important open problem. We believe that our
lower bound is tight (at least for the first order term) and the upper
bound is loose. Showing this or getting a tighter upper bound may
require new proof techniques. A second question could be how to
get an overall RDP guarantee if we are given local RDP guarantees
instead of local LDP guarantees.
ACKNOWLEDGMENTS
This work was partially funded by NSF grants #1740047, #2007714
and and UC-NL grant LFR-18-548554. This work was also supported
in part through the Google Faculty Research Award.
Session 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 2333[30] Peter Kairouz et al. 2019. Advances and Open Problems in Federated Learning.
central differential privacy via anonymity. In Proceedings of the Thirtieth Annual
ACM-SIAM Symposium on Discrete Algorithms. SIAM, 2468â€“2479.
[23] Ãšlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. Rappor: Random-
ized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014
ACM SIGSAC conference on computer and communications security. 1054â€“1067.
[24] Vitaly Feldman, Audra McMillan, and Kunal Talwar. 2020. Hiding Among the
Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by
Shuffling. arXiv preprint arXiv:2012.12803 (2020). Open source implementation
of privacy https://github.com/apple/ml-shuffling-amplification.
[25] Badih Ghazi, Noah Golowich, Ravi Kumar, Rasmus Pagh, and Ameya Velingker.
2019. On the Power of Multiple Anonymous Messages. IACR Cryptol. ePrint Arch.
2019 (2019), 1382.
[26] Badih Ghazi, Rasmus Pagh, and Ameya Velingker. 2019. Scalable and differ-
entially private distributed aggregation in the shuffled model. arXiv preprint
arXiv:1906.08320 (2019).
[27] Antonious Girgis, Deepesh Data, Suhas N. Diggavi, Peter Kairouz, and
Ananda Theertha Suresh. 2021. Shuffled Model of Differential Privacy in Feder-
ated Learning. In The 24th International Conference on Artificial Intelligence and
Statistics, AISTATS (Proceedings of Machine Learning Research, Vol. 130). PMLR,
2521â€“2529.
[28] Antonious M. Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and