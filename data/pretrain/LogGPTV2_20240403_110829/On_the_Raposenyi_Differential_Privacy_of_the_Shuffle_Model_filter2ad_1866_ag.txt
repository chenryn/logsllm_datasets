datasets (D𝑚, D′
𝑚) ∈ D𝑚same, so, in order to prove Theorem 3.7,
we need to compute sup(D𝑚,D′
. We bound
this in the following.
Define a set T𝜖0 consisting of all pairs of 𝐵-dimensional proba-
(cid:18)𝐵
bility vectors satisfying the 𝜖0-LDP constraints as follows:
𝑚)∈D𝑚same
𝑝′2
𝑗
𝑝 𝑗
− 1
(cid:19)
𝑗=1
(cid:110)(𝒑, 𝒑′) ∈ R𝐵 × R𝐵 : 𝑝 𝑗 , 𝑝′
and 𝑒−𝜖0 ≤ 𝑝′
𝐵∑︁
≤ 𝑒𝜖0,∀𝑗 ∈ [𝐵](cid:111).
𝑗=1
𝑝 𝑗 =
𝑗 ≥ 0,∀𝑗 ∈ [𝐵],
𝑗
𝑝 𝑗
(36)
Note that T𝜖0 contains all pairs of the output probability distribu-
tions (𝒑, 𝒑′) of all 𝜖0-LDP mechanisms R on all neighboring data
points 𝑑, 𝑑′ ∈ X. Since any (D𝑚, D′
𝑚) ∈ D𝑚same generates a pair
of probability distributions (𝒑, 𝒑′) ∈ T𝜖0 (because D𝑚 = (𝑑, . . . , 𝑑)
and D′
𝑚 = (𝑑, . . . , 𝑑, 𝑑′) together contain only two distinct data
points 𝑑, 𝑑′), we have
𝐵∑︁
𝑗=1
T𝜖0 =
𝑝′
𝑗 = 1,
(cid:169)(cid:173)(cid:171) 𝐵∑︁
𝑗=1
− 1(cid:170)(cid:174)(cid:172) .
𝑝′2
𝑗
𝑝 𝑗
In the following lemma, we bounds the RHS of (37).
sup
𝑚)∈D𝑚same
(𝒑,𝒑′)∈T𝜖0
𝑝′2
𝑗
𝑝 𝑗
(D𝑚,D′
sup
Lemma 6.2. We have the following bound:
(cid:169)(cid:173)(cid:171) 𝐵∑︁
𝑗=1
− 1(cid:170)(cid:174)(cid:172) ≤
(cid:169)(cid:173)(cid:171) 𝐵∑︁
− 1(cid:170)(cid:174)(cid:172) =
𝑝′2
𝑗
𝑝 𝑗
𝑗=1
sup
(𝒑,𝒑′)∈T𝜖0
(𝑒𝜖0 − 1)2
𝑒𝜖0
.
(37)
(38)
We prove Lemma 6.2 in Appendix C.2. Taking supremum over
𝑚) ∈ D𝑚same in (35) and then using (37) and (38), we get the
(D𝑚, D′
bound in Theorem 3.7.
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 23327 PROOFS OF THE UPPER BOUNDS
In this section, we will prove our upper bounds stated in Theo-
rems 3.1 and 3.3 in Sections 7.1 and 7.2, respectively.
same.
𝑛, 𝑑′
E𝒉∼M(D′)
𝑛, . . . , 𝑑′
M(D′)(𝒉)
Recall from Theorem 3.6, we have
𝑛, . . . , 𝑑′
𝑚+1, D(𝑛)
𝑚+1
7.1 Proof of Theorem 3.1
Consider arbitrary neighboring datasets D = (𝑑1, . . . , 𝑑𝑛) ∈ X𝑛
for any 𝑚 ∈ {0, . . . , 𝑛 − 1}, we define new neighboring datasets
D(𝑛)
and D′ = (cid:0)𝑑1, . . . , 𝑑𝑛−1, 𝑑′
𝑛(cid:1) ∈ X𝑛. As mentioned in Section 3.3,
𝑚+1 =(cid:0)𝑑′
𝑚+1 =(cid:0)𝑑′
𝑛(cid:1), each having
𝑛, 𝑑𝑛(cid:1) and D′(𝑛)
(cid:17) ∈ D𝑚+1
(𝑚 + 1) elements. Observe that(cid:16)D′(𝑛)
(cid:34)(cid:18) M(D)(𝒉)
(cid:19)𝜆(cid:35)
(cid:33)𝜆
(cid:32)M(D(𝑛)
 ,
E
≤ 𝑛−1∑︁
(cid:1)𝑞𝑚(1 − 𝑞)𝑛−𝑚−1. For simplicity of notation, for
where 𝑞𝑚 :=(cid:0)𝑛−1
(cid:33)𝜆 .
(cid:32)M(D(𝑛)
any 𝑚 ∈ {0, 1, . . . , 𝑛 − 1}, define
We show in Appendix D.1 that 𝐸𝑚 is a non-increasing function of
𝑚. Using this and concentration properties of the Binomial r.v., we
get (details are in Appendix D.1):
𝑚+1)(𝒉)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
𝒉∼M(D′(𝑛)
𝑚+1)
𝒉∼M(D′(𝑛)
𝑚+1)
M(D′(𝑛)
M(D′(𝑛)
𝐸𝑚 := E
𝑞𝑚
𝑚=0
(39)
𝑚
≤ 𝑒𝜖0𝜆𝑒− 𝑞(𝑛−1)𝛾2
2
+ 𝐸(1−𝛾)𝑞(𝑛−1),
(40)
where 𝛾 > 0 is arbitrary, and expectation is taken w.r.t. 𝒉 ∼ M(D′).
Note that we have already bounded 𝐸𝑚 for all 𝑚 in Theorem 3.7.
2𝑒𝜖0 ⌋ + 1, we get
By setting 𝛾 =
from Theorem 3.7, that:
2 and 𝑛 = ⌊(1 − 𝛾)𝑞(𝑛 − 1)⌋ + 1 = ⌊ 𝑛−1
1
(cid:34)(cid:18) M(D)(𝒉)
M(D′)(𝒉)
(cid:19)𝜆(cid:35)
E
Since the above bound holds for arbitrary pairs of neighboring
datasets D and D′, this completes the proof of Theorem 3.1.
7.2 Proof of Theorem 3.3
The proof of Theorem 3.3 follows the same steps as that of the
proof of Theorem 3.1 that we outlined in Section 3.3 and also gave
formally in Section 7.1, except for the following change. Instead of
using Theorem 3.7 for bounding the RDP for specific neighboring
datasets, we will use the following theorem.
Theorem 7.1. Let 𝑚 ∈ N be arbitrary. For any 𝜆 ≥ 2 (including
the non-integral 𝜆) and any (D𝑚, D′
(cid:34)(cid:18)M(cid:0)D′
𝑚(cid:1) (𝒉)
M (D𝑚) (𝒉)
(cid:19)𝜆(cid:35)
(cid:18)
𝑚) ∈ D𝑚same, we have
𝜆2 (𝑒𝜖0 − 1)2
≤ exp
.
𝑚
(cid:19)
E𝒉∼M(D𝑚)
(42)
(cid:19)𝜆(cid:35)
(cid:34)(cid:18) M (D) (𝒉)
(cid:19) (𝑒𝜖0 − 1)2
(cid:19)
(cid:18)𝜆
+ 𝜆∑︁
M (D′) (𝒉)
𝑛𝑒𝜖0
𝑖
𝑖=3
(cid:18)𝜆
2
E𝒉∼M(D′)
≤ 1 +
≤ 𝐸𝑛−1 + 𝑒𝜖0𝜆− 𝑛−1
8𝑒𝜖0
(cid:32)(cid:0)𝑒2𝜖0 − 1(cid:1)2
𝑖Γ (𝑖/2)
2𝑛𝑒2𝜖0
(cid:33)𝑖/2
(41)
+ 𝑒𝜖0𝜆− 𝑛−1
8𝑒𝜖0 .
We prove Theorem 7.1 in Appendix D.2. Note that Theorem 7.1
(cid:17) holds for every integer 𝑚 ≥ 2.
Substituting this in (41) (by putting 𝑚 = 𝑛 = ⌊ 𝑛−1
2𝑒𝜖0 ⌋ + 1), we get
implies that 𝐸𝑚−1 ≤ exp(cid:16)𝜆2 (𝑒𝜖0−1)2
(cid:34)(cid:18) M (D) (𝒉)
(cid:19)𝜆(cid:35)
𝑚
E𝒉∼M(D′)
M (D′) (𝒉)
≤ 𝑒𝜆2 (𝑒𝜖0 −1)2
𝑛
+ 𝑒𝜖0𝜆− 𝑛−1
8𝑒𝜖0 .
This proves Theorem 3.3.
8 PROOF SKETCH OF THE LOWER BOUND
Consider the binary case, where each data point 𝑑 can take a value
from X = {0, 1}. Let the local randomizer R be the binary ran-
domized response (2RR) mechanism, where Pr [R (𝑑) = 𝑑] = 𝑒𝜖0
𝑒𝜖0+1
for 𝑑 ∈ X. It is easy to verify that R is an 𝜖0-LDP mechanism.
1
𝑒𝜖0+1. Consider two neighboring datasets
For simplicity, let 𝑝 =
D, D′ ∈ {0, 1}𝑛, where D = (0, . . . , 0, 0) and D′ = (0, . . . , 0, 1).
Let 𝑘 ∈ {0, . . . , 𝑛} denote the number of ones in the output of the
shuffler. As argued in Section 2.3 on page 4, since the output of
the shuffle mechanism M can be thought of as the distribution
of the number of ones in the output, we have that 𝑘 ∼ M(D)
is distributed as a Binomial random variable Bin(𝑛, 𝑝). The proof
uses some properties of the Binomial r.v., which are provided in
Appendix E.
9 CONCLUSION
The analysis of the RDP for the shuffle model presented in this
paper was based on some new analysis techniques that may be of
independent interest. The utility of these bounds were also demon-
strated numerically, where we saw that in important regimes of
interest, we get 8× improvement over the state-of-the-art without
sampling and at least 10× improvement with sampling (see Section
4 for more details).
A simple extension of the results would be to work with local
approximate DP guarantees instead of pure LDP. This can be seen
by using the tight conversion between approximate DP and pure DP
given in [24]. However, there are several open problems of interest.
Our upper bounds hold for general discrete local mechanisms. The
extension to continuous distributions requires careful technical
analysis as the histogram used for RDP analysis would need to
approximate continuous distributions via discretization. We leave
the analysis of continuous distributions as a future work. Perhaps
the most important one is mentioned in Remark 7. There is a multi-
plicative gap of the order 𝑒𝜖0 in our upper and lower bounds, and
closing this gap is an important open problem. We believe that our
lower bound is tight (at least for the first order term) and the upper
bound is loose. Showing this or getting a tighter upper bound may
require new proof techniques. A second question could be how to
get an overall RDP guarantee if we are given local RDP guarantees
instead of local LDP guarantees.
ACKNOWLEDGMENTS
This work was partially funded by NSF grants #1740047, #2007714
and and UC-NL grant LFR-18-548554. This work was also supported
in part through the Google Faculty Research Award.
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2333[30] Peter Kairouz et al. 2019. Advances and Open Problems in Federated Learning.
central differential privacy via anonymity. In Proceedings of the Thirtieth Annual
ACM-SIAM Symposium on Discrete Algorithms. SIAM, 2468–2479.
[23] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. Rappor: Random-
ized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014
ACM SIGSAC conference on computer and communications security. 1054–1067.
[24] Vitaly Feldman, Audra McMillan, and Kunal Talwar. 2020. Hiding Among the
Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by
Shuffling. arXiv preprint arXiv:2012.12803 (2020). Open source implementation
of privacy https://github.com/apple/ml-shuffling-amplification.
[25] Badih Ghazi, Noah Golowich, Ravi Kumar, Rasmus Pagh, and Ameya Velingker.
2019. On the Power of Multiple Anonymous Messages. IACR Cryptol. ePrint Arch.
2019 (2019), 1382.
[26] Badih Ghazi, Rasmus Pagh, and Ameya Velingker. 2019. Scalable and differ-
entially private distributed aggregation in the shuffled model. arXiv preprint
arXiv:1906.08320 (2019).
[27] Antonious Girgis, Deepesh Data, Suhas N. Diggavi, Peter Kairouz, and
Ananda Theertha Suresh. 2021. Shuffled Model of Differential Privacy in Feder-
ated Learning. In The 24th International Conference on Artificial Intelligence and
Statistics, AISTATS (Proceedings of Machine Learning Research, Vol. 130). PMLR,
2521–2529.
[28] Antonious M. Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and