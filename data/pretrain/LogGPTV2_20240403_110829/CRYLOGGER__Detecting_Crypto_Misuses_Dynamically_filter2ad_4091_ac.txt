### 2. Poor App Coverage

To improve the coverage of our testing, we evaluated several tools for test generation, including SmartDroid [28], DroidBot [40], and Dynodroid [41]. These tools have limitations: they are often limited to specific versions of Android and are generally slower than Monkey, as they need to maintain and update information about the app's state (e.g., a control-flow graph [40]). Given these constraints, we opted to use Monkey. Despite its random UI event generation, Monkey is effective at triggering many cryptographic misuses. Most of the functions we instrumented (Table III) are used to initialize basic, critical cryptographic classes, making them relatively easy to trigger. On average, Monkey achieves approximately 25% line coverage, but it reports as many cryptographic misuses as CryptoGuard [6], which uses static analysis (Section VIII).

Using Monkey does have some limitations, such as the potential for false negatives, particularly in areas that are difficult to explore (e.g., login screens). However, CRYLOGGER can be configured to use other UI exercisers or manually-written sequences of UI events. For example, if developers have specific event sequences to stimulate their apps, CRYLOGGER can utilize these to achieve higher coverage. In the future, we plan to develop our own specialized UI event generator for cryptographic testing.

### B. Details about Cryptographic Rules Checking

We used the checking procedures described in Section V to verify the cryptographic rules for Android apps, with some adaptations. The functions we instrumented for rules R-24 and R-25 (Table III) take input parameters for which the developer must implement certain methods, such as `verify()` for host name verification. During logging, we pass erroneous values (e.g., `NULL` or empty strings) to determine if these functions were implemented naively. For rules requiring two executions (Figure 3(b)), we obtain the logs by running the application on two different emulator instances. We also ensure that any value appearing in both logs is due to hard-coded constants in the app.

### VII. Experimental Setup and Benchmarks

We evaluated CRYLOGGER on two sets of benchmarks. The first set consists of Android apps. We downloaded 2,148 free Android apps from the Google Play Store, covering the most popular free apps across 33 categories. We discarded 110 apps because they do not use any cryptographic APIs and 258 apps because they do not work on the Android emulator, either due to crashes or missing libraries. The results of running CRYLOGGER on the remaining 1,780 apps are discussed in Section IX. We used a random subset of these apps to compare CRYLOGGER against CryptoGuard [6] as described in Section VIII.

The second set of benchmarks is the CryptoAPI-Bench [26], a collection of Java applications that include cryptographic misuses. Originally proposed to compare static approaches, we extended this benchmark and used it to compare CRYLOGGER against CryptoGuard (Section VIII).

### VIII. Results: Comparison with CryptoGuard

We compared CRYLOGGER against CryptoGuard [6], one of the most effective static tools for detecting cryptographic misuses in Java-based applications. To the best of our knowledge, CRYLOGGER is the only dynamic approach that can detect misuses for a large number of rules (Section III). We chose CryptoGuard over other static tools like CryptoLint [5] and CrySL [15] because it has been shown to have the lowest false positive and false negative rates [26] and supports the largest number of cryptographic rules.

We compared CRYLOGGER and CryptoGuard using two datasets. The first dataset consists of 150 Android apps randomly selected from the 1,780 apps (Section VII). For this dataset, we evaluated execution times and the number of cryptographic misuses found by both tools. The second dataset is the CryptoAPI-Bench [26], a set of Java benchmarks that include cryptographic misuses. For this dataset, we determined the false positive and false negative rates of both tools. We also extended the CryptoAPI-Bench with more benchmarks to cover cases relevant to dynamic approaches.

#### A. Android Apps: Results

We used 150 free Android apps randomly chosen from the dataset of 1,780 apps to compare CRYLOGGER and CryptoGuard. We could not use the entire dataset of 1,780 apps because determining false positives for CryptoGuard requires manual inspection. For a fair comparison, we excluded rules supported by CRYLOGGER but not by CryptoGuard, and thus compared the two tools by checking 16 cryptographic rules. For each rule, we determined the number of apps marked as "vulnerable" by each tool and analyzed the false positive and false negative rates.

We used three configurations for CRYLOGGER, varying the number of UI events generated by Monkey: 10,000, 30,000, and 50,000 random events (with the same random seed). We refer to these configurations as CRYLOGGER10, CRYLOGGER30, and CRYLOGGER50, respectively.

The results of the comparison are reported in Figures 4 and 5. Each graph is an upset plot [53], [54] for a specific rule, which is an alternative to Venn diagrams for representing sets and their intersections. In our context, the sets represent the apps considered vulnerable by each approach (CRYLOGGER10, CRYLOGGER30, CRYLOGGER50, and CryptoGuard). The horizontal bars indicate the total number of apps considered vulnerable by each approach. For example, for rule R-03, CryptoGuard found 17 vulnerable apps among the 150 analyzed, while CRYLOGGER50 and CRYLOGGER30 flagged 21 apps, and CRYLOGGER10 marked 20 apps. The vertical bars represent the subsets of vulnerable apps.