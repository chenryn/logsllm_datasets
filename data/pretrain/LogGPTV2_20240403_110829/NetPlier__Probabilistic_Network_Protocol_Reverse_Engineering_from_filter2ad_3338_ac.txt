A. Random Variables and Probabilistic Constraints
The ﬁrst three columns of Table I deﬁne the predicates,
their symbols, and descriptions. A predicate has a boolean
value and is associated with a random variable in our system.
In the rest of the paper, we do not distinguish the terms
random variable and predicate. Particularly, the keyword pred-
icate K(f ) asserts if ﬁeld f is the keyword ﬁeld. The other
predicates assert the observations. M (f, c) asserts that the
messages in a cluster c by keyword f have higher similar-
ity among themselves than with messages in other clusters;
R(f, c) asserts that for the messages in c, their corresponding
messages on the other side should belong to a same cluster;
S(f, c) asserts that the messages in c should have similar ﬁeld
structure; and D(f ) is a global assertion (i.e., not speciﬁc to
a cluster), asserting that keyword f does not lead to too many
clusters and each cluster shall have sufﬁcient messages.
The last column in Table I presents the set of constraints
related to the predicates. Intuitively, they denote the correla-
tions of the random variables, which can be considered as
joint distributions of these variables. Each predicate has two
kinds of constraints. The ﬁrst kind is called the observation
constraint that associates predicates with prior probabilities.
They are sub-scripted with a single symbol denoting the asso-
ciated predicate. For example, constraint Cm is the observation
constraint for the message similarity predicate M (f, c). Its
body M (f, c) = 1(pm) means the following “the predicate
M (f, c) has the prior probability of pm to be true”. The other
observation constraints are similarly deﬁned. We will explain
how the prior probabilities are systematically derived later in
this section.
The second kind of constraints is called the inference
constraints. They are sub-scripted with an implication relation.
The implication could proceed in two ways: from an obser-
vation predicate to a keyword predicate or from a keyword
predicate to an observation predicate. They are probabilistic,
regulated by an implication probability. For example, Ck→m :
pm→−−−→ M(f, c) in the third row, fourth column of Table I
K(f )
denotes that if f is the keyword, there is pm→ chance that the
messages in cluster c (formed using f as the keyword) have
higher inner-cluster similarity than inter-cluster similarity. The
following constraint Ck←m represents the opposite direction
of reasoning. Intuitively,
the two constraints describe the
uncertainty of the relations between K and M. For example,
even if f is the true keyword, it is still possible that messages
of the same type do not have high similarity. Theoretically, the
uncertainty, denoted by the implication probabilities, e.g., pm→
Keyword K(f ) Field f is the keyword.
Message
Similarity
Remote
Coupling
Messages in cluster c have
M(f, c) higher inner similarity than
inter similarity.
The corresponding messages
R(f, c) of those in cluster c
belong to a same cluster.
Structure
Coherence
S(f, c)
Messages in cluster c have
similar ﬁeld structure.
pm→−−−−→ M(f, c)
pm←←−−−− M(f, c)
pr→−−−→ R(f, c)
pr←←−−− R(f, c)
Cm : M(f, c) = 1 (pm)
Ck→m : K(f )
Ck←m : K(f )
Cr : R(f, c) = 1 (pr)
Ck→r : K(f )
Ck←r : K(f )
Cs : S(f, c) = 1 (ps)
Ck→s : K(f )
Ck←s : K(f )
ps→−−−→ S(f, c)
ps←←−−− S(f, c)
There are not an excessive number Cd : D(f ) = 1 (pd)
Dimension D(f ) of clusters and each cluster has
enough number of messages.
Ck→d : K(f )
Ck←d : K(f )
pd→−−−→ D(f )
pd←←−−− D(f )
and pm←, follow some normal distribution that can be approxi-
mated using predeﬁned constants based on domain knowledge.
In practice, existing literature of probability inference typically
makes use of pre-deﬁned prior probability values derived from
domain knowledge [84], [45], [36], [58], [21], [60], [50].
Existing studies also show that inference results are usually
not sensitive to these values due to the iterative nature of
inference algorithm. We follow the same practice such as using
0.95 for likely and 0.1 for unlikely, and adjust the implication
probabilities based on these two values according to the level
of uncertainty of individual observations. For example, the
implication probability pr→ for the remote coupling constraint
Ck→r (from the keyword to the coupling predicate) is 0.9 as
there is little uncertainty. That is, the response messages of
the same kind of request messages highly likely belong to the
same kind. However, along the opposite direction, pr← = 0.8
denotes that if corresponding messages on the two sides belong
to two respective clusters, we cannot be so conﬁdent that f is
the right keyword, as such perfect coupling could be by chance.
The implication probabilities for message similarity are lower
than those for remote coupling as they are more uncertain.
In NETPLIER, probabilities p→ are set to be 0.8 for message
similarity constraints and 0.9 for the others. Probabilities p←
lies in [0.6, 0.8] depending on cluster sizes. In Section V, we
validate these implication probabilities in small datasets (100
messages). We notice that our system is not sensitive to these
parameters, consistent with the literature.
B. Determining Prior Observation Probabilities
In the following, we discuss in details how to compute
the prior probabilities for observation constraints pm, pr, ps
and pd. Different from implication probabilities that denote
reasoning uncertainty and are largely stable, these probabilities
describe observation data and vary a lot with the ﬁeld f we
use to cluster messages.
Message Similarity Constraints.
Based on the MSA results, we can compute the similarity
score of a pair of aligned messages:
s =
Number of identical bytes
Sum of total bytes of the two messages
7
TABLE II: Example of remote coupling constraints. The
arrows “→” and “←” denote from client to server and server
to client, respectively
Message pairs
Pairs
← ms0
← ms2
← ms3
(cid:10)mc0 , ms0
← ms1 (cid:10)ms1 , mc1
(cid:10)mc2 , ms2
(cid:10)mc3 , ms3
(cid:10)mc4 , ms4
← ms5 (cid:10)ms5 , mc5
← ms6 (cid:10)ms6 , mc6
← ms7 (cid:10)ms6 , mc6
← ms4
Message type
pairs of f1
Pairs
Traces
← ts1
tc1 →
← ts1
← ts1
(cid:11) tc1 → (cid:10)tc1 , ts1
(cid:11)
← ts2 (cid:10)ts2 , tc1
(cid:11) tc2 → (cid:10)tc2 , ts1
(cid:11) tc3 → (cid:10)tc3 , ts1
(cid:11) tc1 → (cid:10)tc1 , ts1
← ts2 (cid:10)ts2 , tc1
(cid:11)
← ts3 (cid:10)ts3 , tc1
(cid:11)
(cid:11)
← ts4 (cid:10)ts4 , tc4
tc1 →
tc1 →
tc4 →
← ts1
Message type
pairs of f7
Pairs
Traces
← ts1
tc2 →
← ts1
← ts1
(cid:11) tc1 → (cid:10)tc1 , ts1
(cid:11)
← ts2 (cid:10)ts2 , tc2
(cid:11) tc1 → (cid:10)tc1 , ts1
(cid:11) tc1 → (cid:10)tc1 , ts1
(cid:11) tc1 → (cid:10)tc1 , ts1
← ts2 (cid:10)ts2 , tc2
(cid:11)
← ts2 (cid:10)ts2 , tc2
(cid:11)
(cid:11)
← ts2 (cid:10)ts2 , tc2
tc2 →
tc2 →
tc2 ←
← ts1
(cid:11)
(cid:11)
(cid:11)
(cid:11)
(cid:11)
(cid:11)
(cid:11)
(cid:11)
Traces
1 mc0 →
n
o
i
s
s
e
S
mc1 →
2 mc2 →
mc3 →
n
o
i
s
s
e
S
mc4 →
mc5 →
mc6 →
mc7 →
Fig. 8: Example of EER
Fig. 9: Example of Structure Coherence Constraints. m1 and
m2 belong to different message types with different ﬁeld
structure.
3
n
o
i
s
s
e
S
After obtaining similarity scores of all message pairs, a simi-
larity score matrix is constructed. For each keyword candidate
ﬁeld f, we can divide all similarity scores into two classes
based on its clustering results: inner scores, where the two
messages are from the same cluster, and inter scores, where
the two messages are from different clusters.
Ideally, message similarity constraints require that all inner
scores are higher than inter scores. If so, this constraint would
be observed with full conﬁdence, we would hence set pm to 1.
However, the distributions of the two kinds of scores usually
overlap, indicating the errors of false match and false non-
match. These terms are drawn from biometrics [68] where
multiple sequence alignment is widely used. Intuitively in our
context, the former indicates messages of different kinds are
undesirably grouped into a cluster, whereas the later indicates
messages of the same kind are undesirably placed in different
clusters. We quantify the overlap by computing the two errors.
Smaller error values lead to a higher prior probability of
message similarity constraints.
Speciﬁcally, for a threshold t ranging from 0 to 1, we can
compute the False Match Rate (FMR) and False Non-Match
Rate (FNMR) as follows.
FMR =
FNMR =
Number of inter scores which are greater than t
Number of inter scores
Number of inner scores which are smaller than t
Number of inner scores
Considering all t in [0, 1], we can draw the curves of FMR and
FNMR, as shown in Figure 8. Observe that when t increases,
FMR decreases and FNMR increases. To describe the similar-
ity constraints, we need to consider both FMR and FNMR at
the same time. Following the practice in biometrics [30], we
choose the intersection of the two curves, which balances both
FMR and FNMR. The error rate value at the intersection is also
called Equal Error Rate (EER), which describes the overall
accuracy of the clustering results and we have the following.
pm = 1 − EER
It means that the lower the EER, the higher conﬁdence we
have for the message similarity constraint M.
As discussed in Section II-B, alignment-based clustering
methods also utilize similarity scores. However, they have to
train a ﬁxed threshold for all protocols, which cannot avoid
errors due to the overlap and different score distributions
of different protocols. In contrast, We use EER to describe
the distribution of similarity scores and do not need a ﬁxed
threshold.
Remote Coupling Constraints. In the preprocessing step,
we split original traces into sessions, in which we can group
messages from client side and server side into pairs by their
timestamps, IP, and port numbers. For example in Figure 1,
we can generate message pairs as shown in Table II. After
clustering by the candidate keywords of both sides, messages
can be replaced with clusters they belong to and message
pairs are transformed to cluster pairs. The right two columns
show the cluster pairs we generate by ﬁelds f1 and f7,
respectively. For a cluster on one side with size N, we count
the largest number of corresponding messages on the other
side that belong to a same cluster, denoted by M, and have
the following.
pr =
M
N
For example, for the message type pairs of f1, there are four
clusters (in red) paired up with ts1, two of which are tc1. As