3Using scipy.stats.ks_2samp [18].
4Technically, accepting the null hypothesis in this way should only
be performed when a test has sufﬁcient power. As we will see using
the test in this way performs well, and so we dispense with a power
analysis.
To evaluate these tests we proceeded as follows. We use stratiﬁed
random sampling (in our case we sample the same number of
traces of each of the type of trafﬁc) to select a 30% fraction of
the traces from HandShakeDataset to construct a validation set and
use the remaining 70% as a test set. Recall that HandShakeDataset
consists of 5,000 handshake messages for each of obsproxy3,
obfsproxy4, SSL/TLS, HTTP, and SSH ﬂows, meaning that we
have both obfuscated and un-obfuscated traces represented. We
discuss false positives on the university data sets below.
We use the validation set to pick the best performing of the nine
tests, and then analyze the test’s performance on the test set. We
repeat this procedure 10 times with fresh random splits of the test
sets. In all 10 iterations the best-performing test on the validation
set is the entropy distribution test with block size k = 8. Table 5
shows the results broken down by protocol types and including
for comparison the performance of some of the other tests on the
test set. The results are strong: the entropy distribution test with
k = 8 had only one false positive and zero false negatives in the
worst case. We note that the payload length checks are critical
here, we showed that SSL/TLS ﬂows had a false-positive rate of
roughly 23% on average when one omits the check. With the length
checks in place, however, the p-values computed during the entropy
distribution test for SSL, HTTP and SSH handshake messages are
always near zero (a deﬁnitive rejection of the null hypothesis),
whereas the median p-values for the ﬂows declared to be obfsproxy
is 0.99. Thus we consider the tests decisive.
False positive analysis using campus traces. The above analysis
of false positives was in a closed-world setting with just the
three protocol types (SSH/HTTP/TLS). We take the best test from
above and evaluate false positives against the network traces seen
from the university data sets. Speciﬁcally we apply the entropy
distribution test (with k = 8) on the ﬁrst packet of every TCP ﬂow
in OfﬁceDataset, CloudDataset, and WiﬁDataset (assuming the
packet is non-empty). We found 3,998 (0.33%), 19,247 (0.25%),
and 12,786 (0.24%) false positives, respectively.
According to Bro,
the 36,031 false-positive ﬂows were dis-
tributed as follows: 18,939 were SSL/TLS, 9,873 were HTTP,
and the remaining 7,219 ﬂows were reported as “unknown”. We
applied nDPI [28], an open source DPI that can detect hundreds
of application protocols, to the unknown ﬂows.
It was able to
classify 1,673 (23.1%) of these as follows: 1,275 were Real Time
Messaging Protocol (RTMP), 92 were SMTP, 14 were SSH, and
the remaining were generated by other various applications.
The false-positive SSL/TLS ﬂows account for 1.22% of all
SSL/TLS ﬂows examined, while the false-positive HTTP ﬂows ac-
count for 0.10% of all HTTP ﬂows examined. We ﬁnd the SSL/TLS
false positives are associated with 1,907 unique destination IPs, and
90% of these false positives are associated with only 685 unique
destinations. Therefore, a large fraction of these false positives
could be caused by speciﬁc server-side settings. Upon further
examination of the destinations in the latter mislabeled HTTP
ﬂows, we ﬁnd just 357 unique servers. The URLs of these servers
contain only printable ASCII characters but have the same entropy
proﬁle as a random string. The false-positive SSH ﬂows all have
encrypted handshakes, which suggests they may use obfuscated
SSH [23]. The handshake message of RTMP consists of a 1-
byte protocol version and a 1536-byte random string, causing our
embellished entropy test to label all RTMP ﬂows as positive.
5.2 Detecting FTE
As currently implemented in the Tor Browser Bundle,
the
ﬁrst message in an FTE ﬂow is an HTTP GET request. The
URIs in these requests also have large Shannon entropies because
63Entropy dist. test
Uniform byte test
SPRT (8-byte)
SPRT (16-byte)
SPRT (32-byte)
FNR
Obfs3 Obfs4
0
5.0
58.3
24.8
6.9
0
5.1
60.2
24.4
6.6
FPR
SSL HTTP
0.02
0
0
0
0
30.6
0
0.06
5.3
0
SSH
0
0
0
0
0
Table 5: A comparison of the average false-positive/false-negative
rates (in percentage) of different tests for obfsproxy handshake
message detection, across ten rounds of randomized validation/test
splits. While the entropy distribution test (only shown for k = 8)
was chosen in all ten rounds, we show the test-set performance of
the other style of tests for comparison. All these tests are performed
with the payload length check.
1
F
D
C
0.5
1
F
D
C
0.5
0
0
234
1,000
2,000
0
0
URI length
Figure 6: CDF of lengths (left) and entropies (right) of all URIs
extracted for three campus network datasets.
2
4
URI entropy
5.5
6
their characters are direct encodings of ciphertext bits into bytes.
Estimating the entropies of all URIs produced by FTE in 30%
of traces (randomly sampled) in HandShakeDataset, we ﬁnd a
relatively narrow range of 5.5 to 5.8 bits. In HandShakeDataset, for
non-FTE HTTP GET messages, the URIs always have entropies of
less than 5.1 bits. This leads to a simple URI-entropy test: if the
URI in the ﬁrst GET request of a HTTP ﬂow has entropy of at least
5.5 bits, then we label the ﬂow FTE.
Unsurprisingly, this test produces zero false negatives and zero
false positives against HandShakeDataset. We also evaluated
the URI-entropy test against OfﬁceDataset, CloudDataset, and
WiﬁDataset. For an HTTP ﬂow that contains GET requests, we
extract the URI from the ﬁrst of these, and perform the URI-entropy
test on it. We found a total of 420,795 URIs are mislabeled as FTE,
or about 4% of the roughly 10 M URIs examined.
To sharpen our test, we observe that the URIs in the HTTP
requests generated by FTE have a constant length of 239 bytes. So
we embellish our URI-entropy test so that positively labeled ﬂows
must also have a URI length of exactly 239 bytes. This signiﬁcantly
reduces the false-positive rate: only 55, 167, and 42 non-FTE ﬂows
are mislabeled as FTE in the OfﬁceDataset, CloudDataset, and
WiﬁDataset datasets, respectively. That is a total of just 264 false-
positives out of around 10 M samples, giving the embellished test a
very small false-positive rate.
Note that the above tests can in fact be implemented without
message reconstruction, since the tests only involve data in the ﬁrst
packet at ﬁxed locations.
We note that [8] suggests using FTE URI length alone for
detection. But we ﬁnd that a length-only test causes about a 15%
false-positive rate over the same 10 M ﬂows. The CDFs of URI
lengths and entropies are given in Figure 6.
5.3 Discussion
Our results show that entropy-based tests, embellished with
simple length heuristics, can accurately detect obfsproxy3/4 and
FTE with relatively low false-positive rates on real network trafﬁc.
White et al. [47] show that slightly simpliﬁed versions of our
entropy-based tests can be implemented on commodity hardware,
making them applicable at enterprise scale. The main simpliﬁca-
tion is that their tests make use of a ﬁxed-size string, whereas ours
use up to one full TCP payload. We have not yet considered the
efﬁcacy of our tests using truncated payloads.
We note that our entropy-based tests do not perform well
at detecting meek-generated trafﬁc, because meek tunnels over
HTTPS. To detect meek one will need more sophisticated attacks,
as shown in the next section. Finally, we point out that the entropy-
based tests in this section are far less resource-intensive than the
semantics-based attacks of §4. Those attacks in most cases require
ﬂow reconstruction, and these do not.
6. ML-BASED ATTACKS
Using machine learning (ML) for trafﬁc classiﬁcation is not a
novel idea [1, 5, 49], but we are unaware of any prior work that has
attempted its use in detecting obfuscated trafﬁc. In this section, we
develop attacks using ML with carefully selected features.
Our classiﬁcation model will be binary, with output labels
“positive” (obfuscated) and “negative” (not obfuscated). During
training, we will label all training traces that were generated
by a targeted obfuscator with “positive”, and all other traces as
“negative”. Thus we will have a classiﬁer for each obfuscator that
aims to distinguish a given obfuscator from all other trafﬁc. When
attempting to detect the use of any Tor pluggable transports, one
could run all of our tests in parallel (one targeting each obfuscation
method) and if any of them returns “positive” then the meta-
decision would be that Tor with some pluggable transport is in use.
Before describing the actual classiﬁers, let us discuss the features
that they will use, training considerations, and the details of how we
evaluated their efﬁcacy.
Features. Our ML-based attack will make use of three types of
features. The ﬁrst are entropy-based features. For a given ﬂow, we
calculate the Shannon entropy of every packet payload, and use the
maximum/minimum/average entropies in packets in each direction
(upstream or downstream) as features. The selection of this feature
is inspired by the results of the entropy-based tests in §5.
The second feature type is timing-based features, which are
based on an observation regarding trafﬁc patterns in meek. Ac-
cording to the documentation, meek clients will send packets to a
meek server periodically, to check if the server has data to send.
This results in timing patterns in the TCP ACK trafﬁc of meek
connections that differ from typical TLS connections [42]. To
capture this as a feature, we calculate the intervals between two
consecutive TCP ACK packets (in the same direction) in a ﬂow,
and group the intervals into 30 bins. Based on our observations
of meek trafﬁc, the intervals are usually very small: 55% of them
are less than 10 ms, and 99.9% are less than one second. So we
limit the maximum considered ACK interval to one second. We
use (x, y] to denote a bin of width y − x, accepting all numbers r
such that x < r ≤ y. (All numbers are milliseconds.) Across
our 30 bins, the widths are as follows: between 0 and 10 the
bin width is 1 (i.e., (0, 1], (1, 2], etc.), between 10 and 100 the
bin width is 10, between 100 and 1000 the bin width is 100, and
the ﬁnal bin is (1000,∞]. We use variable-width bins to balance
timing granularity and computational efﬁciency, and because the
interval distribution is skewed towards lower values.
Instead of
using absolute numbers, our per-bin feature is the percentage of
inter-ACK intervals that fall into a given bin.
64Finally, we consider packet-header features. The ﬁrst of these is
the percentage of TCP ACK packets sent in each direction. This is
motivated by the behavior of meek just discussed; we expect to see
a larger-than-normal number of TCP ACK packets in meek ﬂows.
The second follows previous works (e.g. [7, 30]) by exploiting
packet length as a distinguishing characteristic. For a given ﬂow,
we use the ﬁve most common packet-payload lengths (in each
direction) as a feature.
Windowing strategies. We will have two main goals for our
classiﬁers. The ﬁrst is to accurately detect all of the obfuscators
under consideration. The second is to detect obfuscated trafﬁc
as soon as possible within a ﬂow, which helps to minimize the
amount of state required. Based on our results for the entropy-
based tests, we can hope to classify some obfuscated ﬂows based
solely on the ﬁrst message. For meek, however, this seems unlikely
to work because meek leverages genuine TLS connections. So,
to accommodate both goals, we allow the classiﬁer to train over a
window of ﬂow trafﬁc. Inspired by [6, 20, 27], we consider two
windowing strategies:
(1)
(2)
The classiﬁer inspects the ﬁrst X packets of a ﬂow (including
SYN and ACK packets), and extracts features from these
packets to do classiﬁcation.
The classiﬁer extracts features from the packets within the
ﬁrst X seconds of a ﬂow to do classiﬁcation.
Examining the distributions of durations and sizes (number of
packets in a ﬂow) of all obfuscator traces collected, and considering
that we want the classiﬁer to be able to make a decision as soon
as possible, we limit the range of X in our tests as follows. For
the packet-count strategy, X ∈ {30, 35, 40, . . . , 300}, and for
the time-based strategy X ∈ {2, 3, 4, . . . , 10}.
Choosing the
endpoints for the packet-count strategy, we can hope to classify
between 90 and 99 percent of obfuscator ﬂows before they termi-
nate; simulating online classiﬁcation, as opposed to after-the-fact.
The same reasoning applies to the boundaries for the time-based
strategy.
Details of classiﬁer evaluation. Our initial measure of classiﬁer
performance uses the TorA, TorB, and TorC datasets. Recall that
these include both synthetic Tor traces under each of the pluggable
transports, as well as synthetic traces for SSL/TLS and HTTP.
For any given dataset, we perform nested cross validation with
ten outer folds and ten inner folds. We measure performance
for that dataset by taking an average over ten outer folds. In an
outer fold, we perform a stratiﬁed random sampling of the target
dataset to select 70% of the traces to create the test set, and use
the remaining 30% for training and validation of classiﬁers for the
inner folds.
Each inner fold is as follows: First, we perform a second
stratiﬁed random sampling, taking a one-third for training and
leaving the remainder for validation.5 Next, we ﬁx a classiﬁcation
strategy by choosing: classiﬁcation algorithm (K-nearest neighbor,
Naive Bayes, or CART), windowing strategy (number of packets
or time for each allowed X), and feature set (any single feature,
any pair of features, or all features). There are 1,344 different
classiﬁcation strategies, in total. For a given strategy, we train ﬁve
classifers, one for each of the obfuscators. Each of these is tested
on the validation set, and we record the average performance of
5There are perhaps more standard choices for the split sizes, but we
do not expect different choices to signiﬁcantly impact results. The
large test set size will tend to produce conservative estimations of
our classiﬁers’ performances.
the ﬁve classiﬁers. This gives an average measure for a particular
classiﬁcation strategy.
We pick the parameter combination that results in the classiﬁers