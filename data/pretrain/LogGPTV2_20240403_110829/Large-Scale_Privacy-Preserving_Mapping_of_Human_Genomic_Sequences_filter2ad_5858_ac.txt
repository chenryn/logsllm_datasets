use the ﬁrst 10 bytes of the stream to do the encryption:
τj = σj ⊕ (Rj||θj), where θj is the last 6 bytes of HK(sj).
This ciphertext is concatenated with the ﬁrst 10 bytes of the
seed’s hash, and given to the public cloud. Once a match is
found between the hashes of αi and sj, it sends (πi, τj, j)
to the private cloud for recovering Li and Rj.
The workload of the private cloud is determined by the
number of extensions it needs to perform for each read. As
discussed before, when l is no smaller than 20, most l-mers
are unique and thus the reads whose seeds match them of-
ten need to be extended only a few times. Among the rest
of l-mers that reoccur on the genome, some of them are
actually part of longer repetitive substrings, and only need
to be extended once for all these reoccurrences. This can
be achieved by compressing the reference genome accord-
ing to its 100-mers: we can identify all distinctive 100-mers
and extend the reads on them instead of the whole genome.
Also important here is an efﬁcient extension algorithm. Our
implementation utilizes a threshold dynamic programming
algorithm [28] to compute the edit distance no more than
a threshold d. This algorithm’s complexity is only O(dλ),
where λ is the length of the read.
Discussion. This application of seed-and-extend works
well when the seeds are at least 20 bps. Given standard 100-
Table 2. Privacy-Preserving Read Mapping: Seed Combinations
• Generating keyed hashes for combined l-mers (one-time cost). Set l = λ
d+2, with a distance threshold d and a read
length λ. For every λ-mer (substrings of λ long) on the reference genome, compute the keyed hash values for all
the 2-combinations of the l-mers it contains: HK(α1||α2), HK(α1||α3), ··· , HK(αλ−l||αλ−l+1). Remove all the
repeats and send the distinctive hash values to the public cloud.
• Generating keyed hashes for seed combinations. On the private cloud, break each read into d + 2 equal-length seeds.
Compute the keyed hashes for all 2 combinations of the d + 2 seeds: HK(s1||s2), ··· , HK(sd+1||sd+2). Send the
hashes for all the reads to the public cloud.
• Seeding. On the public cloud, compare the hashes of combined seeds to those of combined l-mers. For any two
hashes that match, send their indices (i, j) (i for the hash of the combined seed and j for that of the combined l-mer)
to the private cloud.
• Extension. On the private cloud, for every matched pair (i, j), extend the read associated with the combined seed
from the genetic location identiﬁed by the combined l-mer to check whether the edit distance is no more than d.
bp reads, this means that the edit distance we are looking at
should not go above 4. Our research shows that less than
20% of 24-mers re-occur. To prepare for a mapping task,
the private cloud must compute the keyed hashes for refer-
ence l-mers, which only needs to be done once, and seed
hashes for every dataset. Using a high-end desktop (2.93
GHz Intel Xeon), we found that SHA-1 achieved a through-
put of 100 million 25-mers per minute using a single core.
In other words, a typical dataset with 10 million reads can
be proceeded around a minute. Fingerprinting the whole
reference genome took longer time, about 1 hour using 1
core. However, this only needs to be done once. Note that
SHA-1 is not known for its speed. We chose it in our im-
plementation just for simplicity. Many other cryptographic
hash functions perform much better [7]. Our evaluations
(Section 4) show that this approach moved the vast major-
ity of the workload to the public cloud, compared with the
situation when the whole computation is done within the
private cloud.
3.3 Combined Seeds
Short seeds. When edit distance goes up to 6, the 7 14-
bp seeds of a read often align it to hundreds or even thou-
sands of possible positions for extensions. For example, in
a microbiome ﬁltering task described in Section 4, our ex-
periment shows that on average 895 extensions need to be
made before an alignment within the edit distance of 6 could
be found for one read. To reduce the number of matches,
our idea is to use multiple seeds: given an edit distance d,
we can partition a read into d + 2 seeds, of which at least
2 will have exact matches on the region the read should be
aligned to. For example, a 100-bp read, once partitioned
into 8 12-bp seeds, can use 2 seeds (totally 24 bps long)
to ﬁnd out the 100-mers to which its distance may not ex-
ceed 6. Given the total length of such combined seeds, most
reads can be located at a few genetic positions: in the micro-
biome ﬁltering task mentioned above, the 2-combination of
12-bp seeds successfully aligns a read within 28 extensions
on average. A straightforward implementation of this idea,
however, forces the private cloud to intersect a large number
of positions randomly matched by the short seeds for each
read before the reference substrings including both seeds
can be identiﬁed. This intersection operation often incurs
a signiﬁcant overhead, which sometimes even exceeds the
cost for simply running the whole task locally. As a result,
the private cloud often has to shoulder most of the mapping
workload.
Mapping 2-combinations. Our answer to this challenge is
a novel design built upon the special features of the cloud.
Today’s clouds are designed for data intensive computa-
tions, and can easily store and process terabytes of data
at a low cost, as long as the operations on such data are
simple and parallelizable. This property allows us to trade
the spatial cost on the public cloud for the reduction in
the computing workload on the private cloud, converting
the intersection operation to the string matching that hap-
pens between the keyed-hash values of 2-seed combinations
and those of l-mer combinations. Speciﬁcally, for every
100-mer on the reference genome, we save to the public
cloud distinctive hashes for all the 2-combinations of its
l-mers αi: HK(α1||α2), HK(α1||α3), ··· , HK(α2||α3),
··· , HK(α100−l||α101−l). Given a read dataset, the private
cloud also ﬁngerprints all the 2-combinations of d + 2 seeds
sj for each read: HK(s1||s2), ··· , HK(sd+1||sd+2). These
combined-seed hashes are compared to those of the l-mer
combinations on the public cloud to help locate the reads
on the reference genome. This approach is summarized in
Table 2.
To perform this seeding operation,
the public cloud
needs to accommodate the keyed hashes for both reference
l-mer combinations and combined seeds. Each 100-mer
Figure 3. 12-mer Combinations in 100-bp Win-
dows on the Genome
contains 101 − l different l-mers and totally (101−l)(100−l)
combinations. For example, there are 3916 combinations
of 12-mers within a 100-bp window. However, the total
size of the reference will not grow that much, as the seeds
within two overlapping 100-mers are also heavily over-
lapped: from Figure 3, we can see that whenever the win-
dow right shift by one bp, only one new l-mer has been cre-
ated, which brings in an additional 100 − l combinations.
Therefore, the total size of the reference actually increases
by roughly 100 − l times. In the above example, 12-mer
combinations are about 88 times of the total size of all ref-
erence 12-mers. Using the 16 bytes of the 20-byte output
of SHA-1, the keyed hashes for the reference sequences
have about 6.8 TB. Storing, transferring and processing this
amount of data are extremely economic for today’s commer-
cial clouds. For example, to keep such data at Amazon Sim-
ple Storage Service (S3), the NIH only needs to spend $890
per month [4]. Transferring the data to the EC2 is com-
pletely free [4]. In practice, a typical approach is simply to
mail a hard drive, a standard service adopted by the S3 when
bandwidth is low [5]. Note that this is just a one-time cost,
and data storage and transfer at that scale are quite com-
mon for today’s cloud users. Operations on this reference
include sorting and merging with the hashes of combined
seeds, which can also be efﬁciently done through ultra-fast
sorting algorithms [44]. On the other hand, the seed com-
binations cause a much smaller rise in the spatial cost: the
number of 2-combinations of d + 2 seeds is (d+1)(d+2)
, just
times the number of the seeds. For example, the keyed
d+1
hash data of 12-bp seed combinations for 10 million 100-bp
reads has about 5 GB, roughly 20 times the size of the read
dataset.
2
2
2
we ﬁrst extend the combined seed with a unique match.
This works particularly well for the task like microbiome
ﬁltering, which stops to remove a read as soon as it is found
similar to a human 100-mer. A major engineering challenge
on the private cloud side is the sizes of the look-up tables.
The one for ﬁnding reads (from the combined seeds) is still
okay, and can always be replaced with the encoding tech-
nique as described in Section 3.2. The other one, which
maps l-mer combinations to their positions on the reference
genome, needs to be expanded by nearly 40 folds, and has
a size of roughly 400 GB. To work on this table efﬁciently,
we can partition it into a set of sub-tables and distributed
them to multiple nodes on the private cloud. As discussed
in Section 3.2, the content of the original table is organized
according to the index order of the hashes for l-mer com-
binations on the public cloud, for the purpose of sequential
data access from the hard drive. Here, we further ask the
public cloud to group the matches it found into several bins
with regard to the index ranges of the sub-tables, and then
dispatch these bins to different nodes for distributed table
look-ups and extensions on the private cloud. Another ap-
proach is simply encrypting the genetic locations for the l-
mer combinations that repeat less than 10 times on the refer-
ence genome, and saving them on the public cloud. When-
ever the hash value of one such combination is matched, the
ciphertext of its locations is sent back to the private cloud.
This strategy trades bandwidth consumption for the saving
of data processing time on the private cloud.
Discussion. Using combined seeds, we essentially out-
source the computing burden of intersecting the matches
produced by short seeds to the public cloud, which further
reduces the proportion of the overall workload the private
cloud needs to undertake. This has been justiﬁed by our ex-
perimental study (Section 4). Also, although computing
the hash values for all 12-mer combinations takes about 11
hours and a half using 8 cores (2.93 GHz Intel Xeon), the
operation only needs to be performed once. The cost for
processing each dataset is still very low: for example, 280
million seeds for 10 million reads took 6 minutes to hash.
3.4 Privacy Analysis
The private cloud needs to compute the hashes and de-
liver them to the public cloud for each mapping task. Given
times of the increase in the number of the seeds,
merely d+1
2
the overheads for computing and transferring those hashes
can be easily afforded. Since the combined seeds often are
sufﬁciently long (≥ 20 bps), most of them are distinctive
across the reference genome: as an example, our research
shows that nearly 70% of combined 12-mers (24 bps long)
are unique. This helps reduce the number of extensions per-
formed on the private cloud. To further avoid unnecessary
extensions, our approach uses a strategy that for each read,
Our study. Our approach exposes nothing but keyed hash
values of seeds and l-mers to the public cloud, from which
the adversary cannot directly recover reads. We further en-
sure that only the hashes of distinctive l-mers are disclosed,
making them hard to distinguish by an observer. Under such
protection, what is left to the public cloud is just the counts
of the exact matches that individual reference hashes (those
of l-mers or combinations) receive from the hashes of seeds
or combined seeds. Therefore, the adversary’s best chance
is leveraging such information and the background knowl-
edge at her disposal, i.e., the genomes of a reference popu-
A   G A  T   …   …  G    T  T A      C      G   …           1       2        3       4       5       …  …          87     88     89  …   …100  101 lation and a DNA sample from the testee, to determine the
presence of the testee’s DNA in the read dataset she cannot
directly access. We analyze the threat in this section.
As discussed in Section 2.2 and 3.1, we made two
assumptions in our study. First, we assume the afore-
mentioned background knowledge for the adversary. This
would not be necessary if we wanted to achieve differen-
tial privacy [25], a privacy goal that does not rely on any
assumption of background knowledge. However, such a
privacy guarantee is often too strong to attain in practice,
particularly in the case of read mapping where the genetic
locations of reads and the distributions of their hashes are
not known in advance and expensive to get. Also, the
background information we used has been assumed in all
prior studies on re-identiﬁcation risks in human genomic
data [32, 34, 46, 56], as it reﬂects the special features of the
data, and serves as the foundation for evaluating when it can
be released [10]. Actually, even this assumption has already
been complained of as being overpessimistic [30] by the ge-
nomics community, because the knowledge it assumed of-
ten cannot easily come by in practice. Our objective is to
show that even under such an assumption, which is strongly
in favor of the adversary, she still cannot acquire identiﬁable
information from the public cloud. Second, our analysis fo-
cuses on SNPs, as for the time being, other genetic varia-
tions cannot be effectively used for re-identiﬁcation during
read mapping (see Section 2.2).
l-mer based re-identiﬁcation. The public cloud only ob-
serves the frequency with which the keyed hash value of
each reference l-mer has been matched. This frequency
is actually the l-mer’s average rate of occurrence across
genome datasets. The only thing the adversary can do is
trying to map each hash value to its plaintext l-mer, accord-
ing to their frequencies (i.e., the average rate of occurrence)
calculated from seed-hash datasets and public genome data
(e.g., the HapMap population and the reference genome)
respectively. In most situations, however, this attempt will
not yield unique answers, as many l-mers share same fre-
quencies: for example, roughly 5 billion 24-mers on the
reference genome are unique; many of them have same av-
erage rates of occurrence (e.g., those carrying single SNPs
with identical allele frequencies) in a population. In the end,
what the adversary can get is just a mapping between a set
of hash values and a set of plaintext l-mers with the same
average rate of occurrence, assuming that she learnt the rate
from her long-term observations. Within a pair of such sets,
the adversary cannot determine which hash indeed belongs
to a given l-mer through the frequency analysis, as each
of such hash values and l-mers has an identical frequency.
Here, we use bin to describe this set pair, i.e., a set of l-mers
grouped by the frequency they share and their correspond-
ing set of hash values. Note that this is the best the adver-
sary can achieve without considering the relations among
the hash values. Such relations are actually very difﬁcult to
establish in practice, as elaborated in Appendix 7.3.
Consider the h bins Bk∈[1,h] the adversary is able to
identify from all the seed-hash datasets she sees. Given
a speciﬁc seed-hash set (the case group) that involves the
seeds from multiple individuals, these bins are the only at-
tributes at the adversary’s disposal for a re-identiﬁcation at-
tack. Speciﬁcally, what she can do is to ﬁnd out the ag-