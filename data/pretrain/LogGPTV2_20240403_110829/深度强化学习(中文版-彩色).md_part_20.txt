初始化步长α∈(0,1]
决定一个固定的策略π或者使用ϵ-贪心策略
for每一个回合do
初始化S
0
使用π(S ,A)来选择A
0 0
T ←INTMAX（一个回合的长度）
γ ←0
fort←0,1,2,··· untilγ−T −1do
if t<T then
R ,S ←Env(S ,A )
t+1 t+1 t t
if S 是终止状态then
t+1
T ←t+1
else
使用π(S ,A)来选择A
t t+1
endif
endif
τ ←t−n+1（更新的时间点。这是n步Sarsa，只需更新n+1前的一步，持续下去直
到所有状态都被更新。）
if τ ⩾0Pthen
G← min(r+n,T)γi−γ−1R
i=τ+1 i
if γ+n<T then
G←G+γnQ(S ,A )
t+n γ+n
endif
Q(S ,A )←Q(S ,A )+α[G−Q(S ,A )]
γ γ γ γ γ γ
endif
endfor
endfor
因而就有了Sarsa算法的收敛定理。
定理2.1 对于一个有限状态-动作的MDP和一个GLIE学习策略，其动作价值函数Q在时间步t
上由Sarsa（单步的）估计为Q ，那么如果以下两个条件得到满足，Q 会收敛到Q∗并且学习策
t t
略π ，也会收敛到最优策略π∗：
t
1. Q的值被存储在一个查找表（LookupTable）里；
2. 在时间t与状态-动作对(s,a)相关的学习速率（LearningRate）α (s,a)满足0⩽α (s,a)⩽1，
P P t t
α (s,a)=∞， α2(s,a)<∞，并且α (s,a)=0除非(s,a)=(S ,A );
t t t t t t t
3. 方差Var[R(s,a)]<∞。
符合第二个条件对学习速率的要求的一个典型数列是α (S ,A ) = 1。我们在这里对上面定
t t t t
理的证明不做介绍，有兴趣的读者可以查看文献(Singhetal.,2000)。
79
第2章 强化学习入门
2.6.3 Q-Learning：离线策略TD控制
Q-Learning是一种离线策略方法，与Saras很类似，在深度学习应用中有很重要的作用，如
深度Q网络（DeepQ-Networks）。如公式(2.70)所示，Q-Learning和Sarsa主要的区别是，它的
目标值现在不再依赖于所使用的策略，而只依赖于状态-动作价值函数。
Q(S ,A )←Q(S ,A )+α[R +γmaxQ(S ,a)−Q(S ,A )] (2.70)
t t t t t+1 t+1 t t
a
在算法 2.14 中，我们展示了如何用 Q-Learning 控制 TD。将 Q-Learning 变成 Sarsa 算法也
很容易，可以先基于状态和回报选择动作，然后在更新步中将目标值改为估计的下一步动作价
值。上面展示的是单步Q-Learning，我们也可以把Q-Learning变成n步的版本。具体做法是将公
式(2.70)里的目标值加入未来的折扣后的回报。
算法2.14Q-Learning（离线策略TD控制）
初始化所有的状态-动作对的Q(s,a)及步长α∈(0,1]
for每一个回合do
初始化S
0
for每一个在当前回合的S do
t
使用基于Q的策略来选择A
t
R ,S ←Env(S ,A )
t+1 t+1 t t
Q(S ,A )←Q(S ,A )+α[R +γmax Q(S ,a)−Q(S ,A )]
t t t t t+1 a t+1 t t
endfor
endfor
Q-Learning的收敛性条件和Sarsa算法的很类似。除了对策略有的GLIE条件，Q-Learning中Q
函数的收敛还对学习速率和有界奖励值要求，这里不再复述，具体的证明可以在文献(Szepesvári,
1998;Watkinsetal.,1992)中找到。
2.7 策略优化
2.7.1 简介
在强化学习中，智能体的最终目标是改进它的策略来获得更好的奖励。在优化范畴下的策略
改进叫策略优化（图2.13）。对深度强化学习而言，策略和价值函数通常由深度神经网络中的变量
来参数化，因此可以使用基于梯度的优化方法。举例来说，图2.14展示了使用参数化策略的MDP
的概率图模型（GraphicalModel），其中策略由变量θ参数化，在离散时间范围t=0,··· ,N −1
内。奖励函数表示为R = R(S ,A )，而动作表示为A ∼ π(·|S ;θ)。图模型中变量的依赖关系
t t t t t
可以帮助我们理解MDP估计中的潜在关系，而且可以有助于我们在依赖关系图中对最终目标求
80
2.7 策略优化
导而优化变量有帮助，因此我们将在本章展示所有的图模型来帮助理解推导过程，尤其对那些可
微分的过程。近来，文献(Levine,2018)和文献(Fuetal.,2018)提出了一种“推断式控制（Control
asInference）”的方法，这个方法在MDP的图模型上添加了额外的表示最优性（Optimality）的变
量，从而将概率推断或变分推断（VariationalInference）的框架融合到有相同目标的最大熵强化
学习（MaximumEntropyReinforcementLearning）中。这个方法使得推断类工具（InferenceTools）
可以应用到强化学习的策略优化过程中。但是关于这些方法的具体细节超出了本书范围。
价值函数 策略
Actor-Critic
(QAC, A2C,
基于价值 A3C等) 基于策略
(Q-Learning, DQN等) QT-Opt (REINFORCE, CE方法)
图2.13 强化学习中策略优化概览
Rt Rt+1
θ At At+1
St St+1
t=0,1,···,T−1
图2.14 使用参数化策略的MDP概率图模型
除了一些线性方法，使用深度神经网络对价值函数参数化是一种实现价值函数拟合（Value
FunctionApproximation）的方式，而这是现代深度强化学习领域中最普遍的方式，而在多数实际
情况中，我们无法获得真实的价值函数。图 2.15 展示了使用参数化策略 π 和参数化价值函数
θ
Vπ(S )的MDP概率图模型，它们的参数化过程分别使用了参数θ 和w。图2.16展示了使用参
w t
81
第2章 强化学习入门
数化策略 π 和参数化 Q 值函数 Qπ(S ,A ) 的 MDP 概率图模型。一般通过在强化学习术语中
θ w t t
被称为策略梯度（Policy Gradient）的方法改进参数化策略。然而，也有一些非基于梯度的方法
（Non-Gradient-BasedMethods）可以优化不那么复杂的参数化策略，比如交叉熵（Cross-Entropy，
CE）方法等。
Rt Rt+1 Rt Rt+1
θ At At+1
θ At At+1
St St+1
St St+1
Q(St,At) Q(St+1,At+1)
V(St) V(St+1)
t=0,1,···,T−1
t=0,1,···,T−1
w w
图2.15 使用参数化策略和参数化价值函数的
图2.16 使用参数化策略和参数化Q值函数的
MDP概率图模型
MDP概率图模型
如图2.13所示，策略优化算法往往分为两大类：（1）基于价值的优化（Value-BasedOptimiza-
tion）方法，如Q-Learning、DQN等，通过优化动作价值函数（Action-ValueFunction）来获得对
动作选择的偏好；（2）基于策略的优化（Policy-BasedOptimization）方法，如REINFORCE、交
叉熵算法等，通过根据采样的奖励值来直接优化策略。这两类的结合被人们 (Kalashnikov et al.,
2018;Petersetal.,2008;Suttonetal.,2000)发现是一种更加有效的方式，而这构成了一种在无模型
（Model-Free）强化学习中应用最广的结构，称为Actor-Critic。Actor-Critic方法通过对价值函数
的优化来引导策略改进。在这类结合型算法中的典型包括 Actor-Critic类的方法和以其为基础的
82
2.7 策略优化
其他算法，后续有关于这些算法的详细介绍。
回顾强化学习梗概
在线价值函数（On-PolicyValueFunction），v (s)，给出以状态s为起始并在后续过程始终遵
π
循策略π的期望回报（ExpectedReturn）：
v π(s)=E τ∼π[R(τ)|S =s] (2.71)
0
强化学习的优化问题可以被表述为
π∗ =argmaxJ(π) (2.72)
π
最优价值函数（OptimalValueFunction），v∗(s)，给出以状态s为起始并在后续过程始终遵循
环境中最优策略的期望回报：
v∗(s)=maxv π(s) (2.73)
π
v∗(s)=maxE τ∼π[R(τ)|S =s] (2.74)
0
π
在线动作价值函数（On-PolicyAction-ValueFunction），q (s,a)，给出以状态s为起始并采取
π
任意动作a（有可能不来自策略），而随后始终遵循策略π的期望回报：
q π(s,a)=E τ∼π[R(τ)|S =s,A =a] (2.75)
0 0
最优动作价值函数（OptimalAction-ValueFunction），q∗(s,a)，给出以状态s为起始并采取任
意动作a，而随后始终遵循环境中最优策略的期望回报：
q∗(s,a)=maxq π(s,a) (2.76)
π
q∗(s,a)=maxE τ∼π[R(τ)|S =s,A =a] (2.77)
0 0
π
价值函数（ValueFunction）和动作价值函数（Action-ValueFunction）的关系：
v π(s)=E a∼π[q π(s,a)] (2.78)
v∗(s)=maxq∗(s,a) (2.79)
a
83
第2章 强化学习入门
最优动作：
a∗(s)=argmaxq∗(s,a) (2.80)
a