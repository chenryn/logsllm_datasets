### Sarsa算法详解

#### 初始化
- 步长（学习率）$\alpha \in (0, 1]$
- 策略选择：固定策略 $\pi$ 或使用 $\epsilon$-贪心策略

#### 主循环
对于每个回合执行以下步骤：
1. **初始化状态** $S_0$
2. 使用策略 $\pi(S_0, A)$ 选择动作 $A_0$
3. 设定最大回合长度 $T \leftarrow \text{INTMAX}$
4. 设定折扣因子 $\gamma \leftarrow 0$

##### 内部循环
从 $t = 0, 1, 2, \ldots$ 直到 $\gamma - T - 1$，执行以下步骤：
- 如果 $t < T$:
  - 执行动作 $A_t$ 并观察奖励 $R_{t+1}$ 和新状态 $S_{t+1}$
  - 如果 $S_{t+1}$ 是终止状态，则设置 $T \leftarrow t + 1$
  - 否则，使用策略 $\pi(S_{t+1}, A)$ 选择下一个动作 $A_{t+1}$
- 更新时间点 $\tau \leftarrow t - n + 1$（这是 $n$ 步 Sarsa 的更新时间点）
- 如果 $\tau \geq 0$:
  - 计算目标值 $G \leftarrow \sum_{i=\tau+1}^{\min(\tau+n, T)} \gamma^{i-\tau-1} R_i$
  - 如果 $\tau + n < T$，则加上未来折扣回报 $G \leftarrow G + \gamma^n Q(S_{\tau+n}, A_{\tau+n})$
  - 更新 $Q$ 值：$Q(S_\tau, A_\tau) \leftarrow Q(S_\tau, A_\tau) + \alpha [G - Q(S_\tau, A_\tau)]$

#### 收敛性
**定理 2.1** 对于有限状态-动作 MDP 和 GLIE 学习策略，如果满足以下条件，$Q$ 值将收敛到最优 $Q^*$，且策略 $\pi$ 也将收敛到最优策略 $\pi^*$：
1. $Q$ 值存储在查找表中。
2. 学习率 $\alpha_t(s, a)$ 满足 $0 \leq \alpha_t(s, a) \leq 1$，$\sum_t \alpha_t(s, a) = \infty$，$\sum_t \alpha_t^2(s, a) < \infty$，并且 $\alpha_t(s, a) = 0$ 除非 $(s, a) = (S_t, A_t)$。
3. 奖励的方差 $\text{Var}[R(s, a)] < \infty$。

一个典型的满足第二个条件的学习率序列是 $\alpha_t(S_t, A_t) = \frac{1}{t}$。有关详细证明，请参阅文献 (Singh et al., 2000)。

### Q-Learning：离线策略 TD 控制

Q-Learning 是一种离线策略方法，类似于 Sarsa，在深度学习应用中非常重要，如深度 Q 网络 (DQN)。与 Sarsa 不同的是，Q-Learning 的目标值不依赖于当前策略，而是依赖于状态-动作价值函数。

#### 更新规则
\[ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)] \]

#### 算法
1. **初始化** 所有状态-动作对的 $Q(s, a)$ 及步长 $\alpha \in (0, 1]$
2. **主循环** 对每个回合执行以下步骤：
   - 初始化状态 $S_0$
   - 对当前回合中的每个状态 $S_t$ 执行以下步骤：
     - 使用基于 $Q$ 的策略选择动作 $A_t$
     - 执行动作 $A_t$ 并观察奖励 $R_{t+1}$ 和新状态 $S_{t+1}$
     - 更新 $Q$ 值：$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$

Q-Learning 的收敛性条件与 Sarsa 类似，包括对策略的 GLIE 条件、学习率和有界奖励的要求。具体证明请参阅文献 (Szepesvári, 1998; Watkins et al., 1992)。

### 策略优化

#### 简介
在强化学习中，智能体的目标是通过改进策略来获得更好的奖励。策略优化通常分为两类：
1. **基于价值的优化** 方法，如 Q-Learning、DQN，通过优化动作价值函数来指导动作选择。
2. **基于策略的优化** 方法，如 REINFORCE、交叉熵方法，直接根据采样奖励优化策略。

结合这两类方法的 Actor-Critic 结构是一种广泛应用的方法，通过优化价值函数来引导策略改进。

#### 回顾强化学习基本概念
- **在线价值函数** $v_\pi(s)$ 给出以状态 $s$ 为起始并遵循策略 $\pi$ 的期望回报：
  \[ v_\pi(s) = \mathbb{E}_\tau[R(\tau) | S_0 = s] \]
- **最优价值函数** $v^*(s)$ 给出以状态 $s$ 为起始并遵循最优策略的期望回报：
  \[ v^*(s) = \max_\pi v_\pi(s) \]
  \[ v^*(s) = \max_\pi \mathbb{E}_\tau[R(\tau) | S_0 = s] \]
- **在线动作价值函数** $q_\pi(s, a)$ 给出以状态 $s$ 为起始并采取动作 $a$ 后遵循策略 $\pi$ 的期望回报：
  \[ q_\pi(s, a) = \mathbb{E}_\tau[R(\tau) | S_0 = s, A_0 = a] \]
- **最优动作价值函数** $q^*(s, a)$ 给出以状态 $s$ 为起始并采取动作 $a$ 后遵循最优策略的期望回报：
  \[ q^*(s, a) = \max_\pi q_\pi(s, a) \]
  \[ q^*(s, a) = \max_\pi \mathbb{E}_\tau[R(\tau) | S_0 = s, A_0 = a] \]

#### 关系
- 价值函数和动作价值函数的关系：
  \[ v_\pi(s) = \mathbb{E}_{a \sim \pi}[q_\pi(s, a)] \]
  \[ v^*(s) = \max_a q^*(s, a) \]
- 最优动作：
  \[ a^*(s) = \arg\max_a q^*(s, a) \]

这些公式和关系帮助我们理解和实现各种强化学习算法。