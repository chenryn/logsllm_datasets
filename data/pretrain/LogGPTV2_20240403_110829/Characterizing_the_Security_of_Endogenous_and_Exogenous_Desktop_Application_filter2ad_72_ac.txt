### 3.5 What Is the Attack Surface that Applications Expose Through Open Ports?

In this experiment, we utilize configuration information generated through OSQuery instead of application-labeled flow data. The "listening-ports" query-pack provides a comprehensive list of all listening ports assigned to a process. This method is more reliable for identifying listening ports than using flow data, as it offers an authoritative snapshot of the ports in use and avoids the need for heuristics to determine traffic direction.

Our findings indicate that, on average, an application exposes 11.1 listening ports per host, with a median of 1. A few outliers significantly skew this average, such as Docker (254 unique ports), dnscrypt-proxy (67), mDNSResponder (28), CiscoSparkHelper (28), Safari (16), vmware-natd (14), Microsoft Teams Helper (11), Spotify (9), zoom.us (4), and Dropbox (3). Docker stands out as the application exposing the most listening ports, which is logical given its use in running containers on laptops, where developers can configure the ports the container listens to on the host OS.

Interestingly, some applications expose more ports than the median, which was unexpected. For example, BetterTouchTool (2), HP Device Monitor (3), and [VisualStudio] Code Helper (Renderer) (4) have a higher network presence than anticipated. We were unable to confidently attribute these connections back to their originating domains due to a lack of consistent identifiers between OSQuery and NVM.

**Takeaway:** Applications that expose listening ports increase the attack surface, warranting further investigation. There have been instances where applications running web servers for communication with web clients have led to vulnerabilities [13–15]. Future work will focus on determining the specific uses of these listening ports by applications.

### 3.6 How Is Endogenous Traffic Represented in Browsers?

As mentioned in Subsection 3.1, our approach for identifying endogenous traffic for browsers faces challenges. This is likely because browsers are user-triggered and there are significant similarities in browsing destinations among users. In our dataset, we observed traffic from three browsers: Google Chrome, Mozilla Firefox, and Apple Safari. User browsing behavior across these browsers is similar, as common tasks (email, social media, news) are not exclusive to any one browser. Across all three browsers, there are 2,671 unique connections (domain at 2LD, port), with 1,720 (64%) being common across at least two browsers and 1,335 (50%) common across all three. 

For endogenous connections exclusive to a single browser, Chrome has 593 unique connections, Firefox has 187, and Safari has 171. This pattern roughly mirrors the popularity of each browser within our device population, leading us to conclude that no single browser has a smaller set of endogenous behavior. **Takeaway:** Browsers generate fundamentally different types of traffic compared to other desktop applications, but their traffic patterns are similar to each other.

### 4 Related Work

Host-based anomaly detection has been a key area of security research since the early 2000s [26], with numerous studies addressing anomalous activity on hosts [22, 40, 41]. Network-level anomaly detection has also been well explored, with tools like Snort [36] and NetFlow-based techniques [29, 43]. Identifying applications and operating systems through network traffic and reconnaissance has been extensively studied [25, 34, 42], including single-packet fingerprints of operating systems [37, 38]. Enterprise software solutions can identify the application generating network traffic and measure aspects of flows [16, 20, 21, 31]. Researchers have used TLS fingerprints to identify software [17], provided techniques to impersonate common software to evade censorship [27], and shown how parrot-based circumvention can fail [30]. In contrast, our study acts as an omniscient passive observer of traffic, focusing on common connections rather than anomalies.

Internet-scale measurements have been conducted to examine HTTPS and TLS implementation and weaknesses [24, 39], build features to measure DNS [19, 32], or scrape the internet to measure the use of cryptographic libraries [35]. Regarding desktop application security, Bellissimo et al. studied the update mechanisms of popular desktop applications in 2006 [18]. Georgiev et al. found that SSL certification validation was fundamentally broken in many widely used libraries in 2012 [28]. Our work, however, does not examine TLS implementation but measures apparent TLS usage. In 2017, Dormann published a blog about the consequences of insecure software updates [23], and a year later, Microsoft was still distributing software over HTTP [33]. To our knowledge, there has not been a prior large-scale, extensive study of the security posture of desktop application communication channels.

### A Data Ethics

Throughout this project, we adhered to all institutional procedures from affiliated institutions. Our IRB reviewed our proposal and datasets, determining that this was not human subjects research. All human and machine identifiers in our dataset have been removed and replaced with encrypted versions, using a key that the research team does not have access to. Telemetry was collected through existing monitoring infrastructure with strict ACLs. Furthermore, all telemetry was collected from corporate-managed and owned devices, where users are informed that the devices are monitored for security and compliance. Our analysis focuses on the network behavior of applications, not individual users. Any individual user’s data could be excluded from our dataset without impacting our findings. We made no attempt to find evidence of sensitive actions or non-work-related activities (video games, streaming video, social media, etc.). Our research is centered on the network behavior of applications, not the individuals using them.

### B RFC 1918

RFC 1918 [10] describes and reserves three IP ranges for private use only. We used this to label each source IP and destination IP as "private" or "public." If an IP is "private," it is not on the Internet but on an internal/private network. After labeling each flow, there are four possible combinations:

- **Private Source to Private Destination (Internal):** Neither end is an Internet-facing IP, indicating communication to internal services.
- **Private Source to Public Destination (Outbound):** The destination is an Internet-facing IP, likely an outbound connection.
- **Public Source to Private Destination (Inbound):** The destination is not an Internet-facing IP, indicating either a connection from a NAT device to an internal service or an inbound connection from a public service to a device.
- **Public Source to Private Destination (NAT):** Both ends have Internet-facing IPs, but one must be a local device with a NAT IP, though we cannot determine which.

### References

[References listed as in the original text]

This revised version aims to improve clarity, coherence, and professionalism while maintaining the original content and structure.