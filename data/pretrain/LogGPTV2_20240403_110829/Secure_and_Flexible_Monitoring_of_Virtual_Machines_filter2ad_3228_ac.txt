Use of OS-speciﬁc Information A virtual memory ad-
dress can be converted to a MFN without any knowledge
of the OS in domU. This is because the address conversion
is speciﬁc to the processor architecture and not to the OS. A
PT lookup, which is required to perform this address con-
version, starts by obtaining the address of the page direc-
tory. This information is stored in one of the control reg-
isters, CR3, of the domU CPU context. Starting with the
page directory, one can complete a PT lookup and, there-
fore, ﬁnd the MFN associated with any virtual address on a
host. However, it can be difﬁcult to determine what virtual
address to access.
Identifying virtual addresses that are interesting requires
some knowledge about the OS. One artifact of compiling a
Linux kernel is the System.map ﬁle. This ﬁle is a listing
of symbols exported from the kernel along with the virtual
address of each symbol. Using this ﬁle, combined with the
ability to access arbitrary virtual addresses, one can view
and modify data such as the system call table, interrupt de-
scriptor table, Linux kernel module (LKM) list, task list,
and more.
In Microsoft Windows, exported symbols are
available in debugging libraries and in ntdll.dll. Of
course, making use of these data structures requires knowl-
edge of the data layout inside each structure. In Linux, this
is determined by inspecting the source code and using tech-
nical references such as the kernel books by Bovet and Ce-
sati [5] or Love [19]. In Windows, much of this information
is available in technical references as well [26].
The memory introspection implementation in XenAc-
cess provides logical separations between the OS-speciﬁc
code and the general code that is OS-neutral. This is done
to permit rapid integration of new target OSes. For example,
the current code is only designed to monitor a Linux 2.6 OS,
but extending XenAccess to monitor Windows or FreeBSD
would only require adding the speciﬁc knowledge for each
OS.
4.2.2 Virtual Disk Monitoring
XenAccess introspects into low-level disk trafﬁc, just as it is
able to map raw memory pages. It therefore satisﬁes prop-
erty (5) by providing full and complete access to data. Xe-
nAccess also includes an inference engine which is able to
dynamically infer the high-level ﬁlesystem operations ex-
ecuted inside a domain based on the intercepted low-level
disk trafﬁc. To this end, we have decided to leverage the
Blktap architecture described earlier since it simpliﬁes the
implementation of the interception mechanism and avoids
making modiﬁcations to the VMM, which is encouraged by
property (1). The biggest challenge, however, is faced by
the inference engine which must somehow overcome the
semantic gap between the low-level view and the desired
higher-level, ﬁlesystem-oriented view that will be given as
output. It does this combining pre-programmed ﬁlesystem
structure knowledge with dynamic inference techniques.
Whereas the interception mechanism (which is roughly
equivalent to the introspection memory-mapping) is inde-
pendent of the current OS and ﬁlesystem by only providing
raw access to disk trafﬁc, the inference engine is dependant
on knowledge of the ﬁlesystem in use. So far, knowledge
has been included in the inference engine to be able to deter-
mine only ﬁle/directory creation/removal operations under
the ext2 ﬁlesystem, although knowledge about other ﬁlesys-
tems can be incorporated.
391391
Initialization and Watchpoints The disk monitoring li-
brary is initialized by the xadisk init() function.
It
creates and initializes all relevant data structures, and opens
all FIFO and ﬁle descriptors that will be used. It also reads
and parses important ﬁlesystem metadata contained in the
ﬁlesystems’s superblock and the group descriptors. It gath-
ers all the essential information to bootstrap the monitor-
ing instance, allowing watchpoints to be set and the in-
ference engine to start. This information is stored in a
xadisk t struct, which is returned by the function.
The xadisk destroy() function ends a monitoring in-
stance, closing the image ﬁle descriptor and deallocating all
associated data structures.
Watchpoints are set by the xadisk set watch()
function.
It receives as an argument the full path of a
directory that is to be monitored for ﬁle/directory cre-
ation/removal. This function initially performs a recursive
search through the ﬁlesystems’s structures (inodes and di-
rectory entries) with the goal of ﬁnding the disk block(s)
which store the directory’s content. The corresponding
blocks are then read from disk, parsed, and have their en-
tries sorted. Finally, they are inserted into a global hash
table. Watchpoints are erased by removing the correspond-
ing block records from the hash table. This is done by the
xadisk unset watch() function.
Inference Engine The inference engine constitutes the
core of the virtual disk monitoring library and its opera-
tion is illustrated in Figure 4.
It is activated by the API
function xadisk activate(), which subsequently cre-
ates a new thread running the engine itself. At each iter-
ation, the engine reads a new record from the tap FIFO,
sent by the wrapper driver. Next, the record’s block num-
ber is hashed into the global hash table and the existence
of an older version is checked. If it is found, ﬁle/directory
creations/removals are inferred by determining the differ-
ences between the old and new versions of the block. The
name and type of the object created or removed can be de-
termined by looking at the type ﬁeld in the corresponding
entry. Results are written to a per-application FIFO that is
accessible by a program using XenAccess. The function
xadisk deactivate() stops the inference engine by
killing the thread executing it and closing all associated de-
scriptors.
Limitations The current implementation has some limita-
tions. Although it has access to all data sent to the disk, for
now the inference engine is only able to infer ﬁle/directory
creation/deletion. It is certainly possible, nevertheless, to
implement the inference of more intricate operations like
ﬁle read/write, object renaming and ﬁle truncating. More
elaborate algorithms will be required, but the same moni-
toring architecture can be used.
Figure 4: The XenAccess disk monitoring functionality. (1)
A mkdir() system call is executed by an application inside
the user VM. This call is translated into low-level operations
by the kernel and sent to the backend driver at the monitoring
VM. (2) The block data ﬂow is intercepted by the wrapper
driver before it is processed by the tapdisk driver and sent
through the tap FIFO. (3) Blocks are read from the FIFO and
hashed into the table. (4) If a record is found for a block X
(which holds part of /tmp contents), it is parsed and sorted.
If not, it is discarded. (5) The new and old version of block
X are compared, its differences are translated into a directory
creation.
Disk Monitoring API
• xadisk init(): Opens the FS image ﬁle and ini-
tializes the library’s main data structures;
• xadisk destroy(): Closes the image ﬁle and
deallocates all global structures;
• xadisk set watch(): Sets a watch-point in one
of the ﬁlesystem’s directories;
• xadisk unset watch(): Removes the watch as-
sociated with a directory;
• xadisk activate(): Creates a thread that runs
the main monitoring and inference engine;
• xadisk deactivate(): Finishes the monitoring
thread associated with a monitoring instance.
Wrapper drivers As illustrated in Figure 4, wrapper
drivers intercept the disk data ﬂow before it is processed by
the tapdisk driver, sending all the data received to the infer-
ence engine. Their functionality is simple: disk block data
and metadata received from kernel space are marshalled
into a buffer and sent to the inference engine through a
FIFO. This architecture allows the inference routines to ex-
ecute asynchronously with the actual disk reads and writes,
which reduces performance impact.
392392
Since we directly rely on the Blktap Architecture which
is exclusive to paravirtualized environments, disk monitor-
ing of fully-virtualized guests (and therefore, OSes other
than Linux) is not supported for now. However, we feel that
implementing this support would not require major effort,
since the inference engine would not require any structural
changes. It would be a matter of ﬁnding out at which point
of the disk data ﬂow a tap must be introduced and a new
wrapper driver implemented. And of course, the inclusion
of speciﬁc knowledge of the ﬁlesystem being used is also
necessary.
Similarly, although the current implementation of the
inference engine is dependant on detailed knowledge of
the ext2 ﬁlesystem, its architecture could be used without
changes to monitor other ﬁlesystems that adhere to the same
general design principles as ext2 (such as the use of direc-
tory structures). Besides, we were careful enough through-
out its design and implementation to compartmentalize al-
most all ﬁlesystem-speciﬁc implementation, making it easy
for the integration of new ones.
5 Experimental Results
This section focuses on the performance and qualita-
tive evaluation of our prototype. We evaluate the perfor-
mance of each technique separately through a series of
micro-benchmarks. This is followed by example applica-
tions of each monitoring technique, showing their useful-
ness in monitoring an OS’s internal structures and disk ac-
tivity.
5.1 Performance Results
The performance ﬁgures show that XenAccess intro-
duces minimal overhead. We performed the testing on Xen
3.0.4 1 running Fedora Core 6 in both dom0 and domU.
This software was run on a 2.33 GHz Intel Core Duo pro-
cessor with 2 MB L2 cache, 2 GB RAM, and an 80 GB
7200 RPM disk. Dom0 was assigned 2 processor cores and
domU was assigned one processor core.
5.1.1 Virtual Memory Introspection
Each of the performance measurements shown in this sec-
tion were done using the gettimeofday() function,
which has a micro-second granularity. Times were mea-
sured by recording the time immediately before and after
the function being measured. The difference between the
two times was recorded. This measurement was repeated
for 1000 times for each test. We choose 1000 measure-
ments because this was sufﬁcient to minimize the standard
deviation for a given set of measurements under this setup.
Additional measurements did not improve the precision.
The data in Figure 5 show the average time to com-
plete the speciﬁed function call. The cache hit columns
393393
Figure 5: Performance of the three memory access functions
in XenAccess for a paravirtualized target domain.
Figure 6: Time for monitor to read memory through intro-
spection.
represent the results with the LRU cache enabled. The
cache miss columns represent the results with LRU cache
disabled. The simplest case is shown on the left of this
graph. The xa access virtual address() func-
tion must map three memory pages on a cache miss
and one on a cache hit. This difference explains the
improvement seen with the LRU cache. The time for
xa access kernel symbol() is dominated by the op-
eration to lookup the kernel symbol. This operation is
a lookup inside a ﬁle on disk, which is costly. With
a cache hit, the symbol to machine address mapping is
stored in the cache, making the performance similar to
xa access virtual address().
The last access
function is xa access user virtual address().
This function must traverse the task list in the domU
kernel to locate the page directory for the process vir-
tual address.
This explains the slower performance
for the cache miss.
this traversal
is not needed, performance is essentially the same as
xa access virtual address().
On a cache hit,
After the memory is accessed, the next step is to read
from or write to that memory. As seen in Figure 6, this op-
eration is fast compared to mapping the memory. These per-
formance results show the time required to memcpy() data
 0 10 20 30 40 50 60 70 80 90Virtual AddressKernel SymbolUser AddressTime in microseconds212136881541PV-MPV-HHVM-MHVM-H 0 1 2 3 4 54000300020001000500100Time in microsecondsData size in bytesfc6-pvfc6-hvmtiming measurements for each test case is negligible. Even
in the case where 2000 ﬁles were created, the differences
between the measurements were not statistically signiﬁcant.
The conclusion is that the performance overhead added by
XenAccess’ disk monitoring capabilities are minimal, and
therefore obeys property (3) as deﬁned in the Introduction.
The explanation for this negligible overhead lies in the
design of the Blktap architecture, as well as XenAccess’
own design. Most of XenAccess’ disk monitoring engine
code is executed asynchronously with regard to the actual
disk I/O. The wrapper driver is basically the only extra code
added by XenAccess to the disk I/O critical path. It does a
simple data marshaling followed by a memory copy oper-
ation to the tap FIFO. These are not expensive operations.
The asynchronism created by the use of a FIFO allows the
inference engine, which is the most performance-intensive
component of the architecture, to execute in parallel with
the actual disk operations. In addition, we ran our bench-
marks on a dual-core platform, enabling real parallelization
of the tasks.
5.2 Example Applications
XenAccess is straight forward to use, allowing for rapid
development of new monitors to satisfy property (4) from
the Introduction.
In this section we show several exam-
ple applications to demonstrate our monitoring capabilities.
While reading through these examples, keep in mind that
we are deliberately showing simple use cases as an intro-
duction to the library. However, XenAccess provides com-
plete read/write access to a VM’s memory space and has
complete access to the disk I/O. With this level of power, the
potential applications are only limited by the user’s imagi-
nation.
5.2.1 Virtual Memory Introspection
Using introspection, XenAccess can view and modify data
in memory of a running OS. The example below shows how
to use XenAccess to view the LKMs. Additional examples
in the open source release show how to list running pro-
cesses and view memory pages of a particular process on
the target OS. These examples utilize information directly
from a running Linux kernel.
List Linux Kernel Modules This example uses the
xa access kernel symbol() function to list
the
LKMs installed into the domU kernel and is 44 SLOC. Pro-
gram 5.1 shows the code for this example. The code follows
a linked list in the domU kernel memory using introspec-
tion. It starts by loading the memory page containing the
head of the list, which is found using the modules kernel
symbol. This address points to a module struct. This
394394
Figure 7: Performance of three different scenarios run-
ning a disk benchmark. Mode 1: Disk monitoring enabled
and watchpoints set on each benchmarking directory; Mode
2: Disk monitoring enabled and no watchpoints set on the
benchmarking directories; Mode 3: Standard block-aio
tapdisk driver being used (no monitoring).
from kernel memory in the target OS. In general, we found
that data is copied into a data monitor at a rate of approxi-
mately 1kB / µsec. Figure 6 shows that memcpy() perfor-
mance for PV and HVM VMs is essentially the same. The
variance in these measurements can be attributed to exper-
imental noise given the precision of our timing mechanism
and the small measurement times. Looking at the cache
hit values in Figure 5 and the memory copy performance,
the memory introspection capabilities in XenAccess per-
form well enough to have a negligible impact on the overall
system performance, satisfying property (3) from the Intro-
duction.
5.1.2 Virtual Disk Monitoring
XenAccess’ disk monitoring performance was evaluated
through the execution of a benchmarking shell script. This
script tests exhaustively one of the operations monitored by
XenAccess: ﬁle creation. It works by measuring the time it
takes to create a variable number of ﬁles inside ten different
directories. The total number of ﬁles created is equally dis-
tributed throughout the ten directories. The timing measure-
ments include both the execution of the ﬁle creation com-
mands and the manual ﬂushing of the changes to the disk
(through the sync command). Manual ﬂushing was nec-
essary so that the actual performance impact would not be
hidden by the operating system’s buffer cache.
The script was executed in three different modes and the