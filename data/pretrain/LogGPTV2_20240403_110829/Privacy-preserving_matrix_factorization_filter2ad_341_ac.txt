S =
⊥ 1 . . . 1 . . . ⊥ 1
1
0
1 . . . 1 . . . m m . . . m
1
1
1 . . . 1 . . .
. . .
. . .
0
3. Starting from the right-most tuple, move from right to
left, adding the values of the second entries in each tuple;
if a counter tuple (i.e., a zero ﬂag) is reached, store the
computed value at the ⊥ entry, and restart the counting.
More formally, denote by s(cid:96),k the (cid:96)-th element of the k-th
tuple. This “right-to-left” pass amounts to the following
assignments:
s2,k ← s3,k +s3,k+1·s2,k+1 ,
for k ranging from M + m − 1 down to 1.
(5)
4. Sort the array again in increasing order, this time w.r.t.
the ﬂags s3,k. The resulting array’s ﬁrst m tuples contain
the counters, which are released as output.
The above algorithm can be readily implemented as a cir-
cuit that takes as input M and outputs (j, cj) for every
item j ∈ [m]. Step 1 can be implemented as a circuit with
input the tuples (i, j) ∈ M and output the initial array
S, using Θ(m + M ) gates. The sorting operations can be
performed using, e.g., Batcher’s sorting network (cf. Ap-
pendix C) which takes as input the initial array and outputs
the sorted array, requiring Θ((m + M ) log2(m + M )) gates.
Finally, the right-to-left pass can be implemented as a cir-
cuit that performs (5) on each tuple, also with Θ(m + M )
gates. Crucially, the pass is data-oblivious: (5) discrimi-
nates “counter” from “input” tuples through ﬂags s3,k and
s3,k+1, but the same operation is performed on all k.
3.4 Our Efﬁcient Design
Algorithm 1 Matrix Factorization Circuit
Input: Tuples (i, j, rij)
Output: V
1: Initialize matrix S
2: Sort tuples with respect to rows 1 and 3
3: Copy user proﬁles (left pass): for k = 2. . .M + n
s5,k ← s3,k · s5,k−1 + (1 − s3,k) · s5,k
4: Sort tuples with respect to rows 2 and 3
5: Copy item proﬁles (left pass): for k = 2. . .M + m
s6,k ← s3,k · s6,k−1 + (1 − s3,k) · s6,k
(cid:21)
6: Compute the gradient contributions: ∀k < M + m
(cid:20)s3,k · 2γs6,k(s4,k − (cid:104)s5,k, s6,k(cid:105)) + (1 − s3,k) · s5,k
(cid:20)s5,k
s3,k · 2γs5,k(s4,k − (cid:104)s5,k, s6,k(cid:105)) + (1 − s3,k) · s6,k
←
s6,k
(cid:21)
7: Update item proﬁles (right pass): for k = M+m− 1. . .1
s6,k ← s6,k + s3,k+1 · s6,k+1 + (1 − s3,k) · 2γµs6,k
8: Sort tuples with respect to rows 1 and 3
9: Update user proﬁles (right pass): for k = M +n− 1. . .1
s5,k ← s5,k + s3,k+1 · s5,k+1 + (1 − s3,k) · 2γλs5,k
10: If # of iterations is less than K, goto 3
11: Sort tuples with respect to rows 3 and 2
12: Output item proﬁles s6,k, k = 1, . . . , m
Motivated by the above approach, we design a circuit for
matrix factorization based on sorting, whose complexity is
Θ((n + m + M ) log2(n + m + M )), i.e., within a polylogarith-
mic factor of the implementation in the clear. The circuit
operations are described in Algorithm 1.
In summary, as
in the simple counting example above, both the input data
(the tuples (i, j, rij)) and placeholders for both user and item
proﬁles are stored together in an array. Through appropri-
ate sorting operations, user or item proﬁles can be placed
close to the input with which they share an identiﬁer; linear
passes through the data allow the computation of gradients,
as well as updates of the proﬁles.
We again ﬁrst describe the algorithm in detail and then
discuss its implementation as a circuit. As before, the null
symbol ⊥ indicates a placeholder; when sorting, it is treated
as +∞, i.e., larger than any other number.
Initialization. The algorithm receives as input the sets
Li = {(j, rij) : (i, j) ∈ M}, and constructs an n + m +
M array of tuples S. The ﬁrst n and m tuples of S serve
as placeholders for the user and item proﬁles, respectively,
while the remaining M tuples store the inputs Li. More
speciﬁcally, for each user i ∈ [n], the algorithm constructs a
tuple (i,⊥, 0,⊥, ui,⊥), where ui ∈ Rd is the initial proﬁle of
user i, selected at random from the unit ball. For each item
j ∈ [m], the algorithm constructs the tuple (⊥, j, 0,⊥,⊥, vj),
where vj ∈ Rd is the initial proﬁle of item j, also selected
at random from the unit ball. Finally, for each pair (i, j) ∈
M, the corresponding tuple (i, j, 1, rij,⊥,⊥), where rij is i’s
rating to j. The resulting array is shown in Figure 3(a).
805
1
0
n ⊥ ··· ⊥
1 :
i1
··· m j1
2 : ⊥ ··· ⊥ 1
···
1
0
3 :
4 : ⊥ ··· ⊥ ⊥ ··· ⊥ ri1j1
⊥
5 : u1
6 : ⊥ ··· ⊥ v1
⊥
···
···
0
··· un ⊥ ··· ⊥
vm
0
···
(a) Initial state
iM
jM
1
riM jM
⊥
⊥
···
···
···
···
···
···
n··· n
···
1··· 1
n
1 :
1
j1 ··· jkn
··· ⊥
j1 ··· jk1
2 : ⊥
1··· 1
···
1··· 1
0
0
3 :
··· ⊥ rnj1 ··· rnjkn
4 : ⊥ r1j1 ··· r1jk1
⊥ ··· ⊥
··· un
⊥ ··· ⊥
5 : u1
··· ⊥
⊥ ··· ⊥
6 : ⊥
⊥ ··· ⊥
(b) After sorting w.r.t. user ids
⊥ ··· ⊥
··· m
1
···
0
0
⊥ ··· ⊥
⊥ ··· ⊥
vm
v1
···
Figure 3: Data structure S used by Alg. 1. Fig. (a) indicates the initial state, and (b) shows the result after
sorting w.r.t. the user ids, breaking ties through ﬂags, as in Line 2 of Alg. 1. Bold rows 5, 6 correspond to
d-dimensional (rather than scalar) values. Note that a left pass as in line 3 of Alg. 1 will copy user proﬁles
to their immediately adjacent tuples.
We again denote by s(cid:96),k the (cid:96)-th element of the k-th tuple.
Intuitively, these elements serve the following roles:
The above operations are repeated K times, the number of
desirable iterations of gradient descent.
s1,k :
s2,k :
s3,k :
s4,k :
s5,k :
s6,k :
user identiﬁers in [n]
item identiﬁers in [m]
a binary ﬂag indicating if the tuple is a “proﬁle”
or “input” tuple
ratings in “input” tuples
user proﬁles in Rd
item proﬁles in Rd
In brief, gradient descent iterations
Gradient Descent.
comprise of the following three steps:
1. Copy proﬁles. At each iteration, the proﬁles ui, vj of
each user i and each item j are copied to the correspond-
ing elements s5,k and s6,k of each “input” tuple in which
i and j appear. This is implemented in Lines 2 to 5 of
Algorithm 1. To copy, e.g., the user proﬁles, S is sorted
using the user id (i.e., s1,k) as a primary index and the
ﬂag (i.e., s3,k) as a secondary index. An example of such
a sorting applied to the initial state of S can be found
in Figure 3(b). Subsequently, the user ids are copied by
traversing the array from left to right (a “left” pass), as
described formally in Line 3. This copies s5,k from each
“proﬁle” tuple to its adjacent “input” tuples; item proﬁles
are copied similarly.
2. Compute gradient contributions. After proﬁles are
copied, each “input” tuple corresponding to, e.g., (i, j)
stores the rating rij (in s4,k) as well as the proﬁles ui and
vj (in s5,k and s6,k, respectively), as computed in the last
iteration. From these, the following are computed:
vj(rij − (cid:104)ui, vj(cid:105)), and ui(rij − (cid:104)ui, vj(cid:105)) ,
which amount to the “contribution” of the tuple in the
gradients w.r.t. ui and vj, as given by (4). These replace
the s5,k and s6,k elements of the tuple, as indicated by
Line 6. Through appropriate use of ﬂags, this operation
aﬀects “input” tuples, leaving “proﬁle” tuples unchanged.
3. Update proﬁles. Finally, the user and item proﬁles are
updated, as shown in Lines 7 to 9. Through appropriate
sorting, “proﬁle” tuples are made again adjacent to the
“input” tuples with which they share ids. The updated
proﬁles are computed through a right-to-left traversing
of the array (a “right pass”). This operation adds the
contributions of the gradients as it traverses “input” tu-
ples. Upon encountering a “proﬁle” tuple, the summed
gradient contributions are added to the proﬁle, scaled
appropriately. After passing a proﬁle, the summation of
gradient contributions restarts from zero, through appro-
priate use of the ﬂags s3,k, s3,k+1.
Output. Finally, at the termination of the last iteration,
the array is sorted w.r.t. the ﬂags (i.e., s3,k) as a primary
index, and the item ids (i.e., s2,k) as a secondary index.
This brings all item proﬁle tuples in the ﬁrst m positions in
the array, from which the item proﬁles can be extracted.
Each of the above operations is data-oblivious, and can
be implemented as a circuit. Copying and updating proﬁles
requires Θ(n+m+M ) gates, so the overall complexity is de-
termined by sorting, which yields a Θ((n + m + M ) log2(n +
m + M )) cost when using Batcher’s circuit. As we will see
in Section 6, sorting and the gradient computation in Line 6
are the most computationally intensive operations; fortu-
nately, both are highly paralellizable. In addition, sorting
can be further optimized by reusing previously computed
comparisons at each iteration. We discuss these and other
optimizations in Section 5.3.
4. EXTENSIONS
4.1 Privacy-Preserving Recommendations
We now extend our design to a system that enables a user
to learn her predicted ratings rij, for all j ∈ [m], as given by
(2). However, neither the RecSys nor the CSP learn anything
about the users beyond how many ratings they generated; in
particular, neither learns V . Again, these guarantees hold
under the honest-but-curious threat model.
To implement this functionality, at the beginning of the
protocol, each user i chooses a random mask ϑi (this mask
will be used to hide user’s i proﬁle ui), encrypts it under the
CSP’s public key using any semantically secure encryption
scheme E and sends it to the RecSys. We denote by ti =
Epkcsp (ϑi) the encrypted value.
The protocol then proceeds as described in Section 3 but
with the following modiﬁcations. Initially, the RecSys for-
wards to the CSP the encrypted masks ti (i ∈ [n]), which
are then decrypted by the CSP. Hence the CSP knows the
plain value of the masks ϑi (i ∈ [n]). Likewise, on its side,
the CSP chooses random masks j for j ∈ [m] (mask j will
be used to hide item proﬁle vj). The circuit built by the
CSP again performs matrix factorization, as described in
Section 3; however, rather than outputting V = (vT
j )j∈[m],
the circuit now outputs the item proﬁles masked with j and
the user proﬁles masked with ϑi:
ˆvj = vj + j
and ˆui = ui + ϑi
for j ∈ [m] and i ∈ [n]. At the end of the protocol, the
RecSys sends the respective ˆui to each user i, who can then
recover her proﬁle ui by removing the mask: ui = ˆui − ϑi.