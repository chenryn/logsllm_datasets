An algorithm is considered very good if its ROC curve
climbs rapidly towards the upper left corner of the graph.
This means that we detect a very high fraction of the true
anomalies with only a few false positives. Sample ROC
curves can be seen in Figure 1 (to be fully explained later).
To quantify how quickly the ROC curve rises to the up-
per left hand corner, one simply measures the area under
the curve. The larger the area, the better the algorithm.
ROC curves are essentially parametric plots as each point
on the curve corresponds to a different threshold. Each
point illustrates a particular tradeoff between false positives
and false negatives. Each algorithm results in one curve,
and by comparing these curves we can compare algorithms.
The curve with the largest area underneath it corresponds to
the better algorithm. Since each curve represents the entire
range of thresholds, we can compare algorithms throughout
their entire region.
ROC curves are grounded in statistical hypothesis test-
ing. As mentioned earlier, any anomaly detection method
will at some point use a statistical test to verify whether
or not a hypothesis (e.g., there was an anomaly) is true
or false. Recall that ξt is our residual process and should
be zero (or roughly zero) when there is no anomaly. We
can form the hypothesis H0 : ξt = 0 for the case when
there is no anomaly. We can form an alternate hypothesis
H1 : ξt! = 0 for the case when there an anomaly occurs.
This last hypothesis is difﬁcult to handle mathematically,
so for the sake of simplicity of exposition, we rewrite the
alternate hypothesis as H1 : ξt = µ. (Conceptually we
can continue to think of this as the case when an anomaly
occurs). The random variable ξt in each hypothesis is as-
sumed to have some distribution. Upon observing a sample
of this random variable we compare it to a threshold to de-
cide if we reject H0 (thereby accepting H1) or vice versa.
Let FPR denote the false positive rate, the probability
that we detect an anomaly given there was no anomaly. Put
otherwise, this is the likelihood that we reject H0 when it
was true. The false negative rate, FNR, is the probability
that we detect nothing when an anomaly occurs (or the like-
lihood that we accept H0 when we should have rejected it).
In order to decide whether or not to accept H0, we compare
our observation of ξt to a threshold. The Neyman-Pearson
criteria says that we should construct this decision thresh-
old to maximize the probability of detection (true positives)
while not allowing the probability of false alarm to exceed
some value α.
The optimization problem to solve is to ﬁnd the maxi-
mum probability of detection (1-FNR) such that F P R ≤
α. The likelihood ratio is deﬁned as the ratio of FPR/FNR.
The Neyman-Pearson lemma says that the optimal decision
threshold is one that satisﬁes the likelihood ratio test.
F P R
F N R
≤ T (α)
In solving for T (α) (i.e., deriving the curve), each point
of this curve corresponds to one value of the decision
threshold. In practice, this curve is plotted as the correct
detection rate,i.e. 1 − F N R as a function of false positive
rate F P R thus yielding the ROC curve.
For a ﬁxed F P R = α, all values 1 − F N R ≤ β∗ are
achievable by a non-optimal anomaly detector, or equiv-
alently all points below the optimal ROC curve can be
achieved. The ROC curve can be derived analytically typ-
ically only under simple assumptions (such as ξt is Gaus-
sian). In this case the derived curve is an optimal curve.
The optimal curve is not a perfect solution (i.e., 100% true
positive detection and 0% false positives) because usually
there is some inherent noise in the process this limits the
best decision one can make.
As a simple example, consider the case when ξt is a
gaussian random variable with a cumulative distribution
336
Internet Measurement Conference 2005 
USENIX Association
6
given by Φ. The ROC curve for the hypothesis H0 : ξt = 0
vs. H1 : ξt = µ is given by :
have a less sensitive approach that will not raise an alarm
based on only one observation diverging from the bound.
1 − F N R = 1 − Φ(Φ−1(F P R) − µ)
Figure 1: Optimal ROC curve for a gaussian hypoth-
esis testing between H0 : ζ = 0 vs. H1 : ζ = µ.
Fig. 1 shows the ROC curve for this case for three dif-
ferent alternate hypotheses H1.
In practice, the optimal
ROC curves cannot be derived, which limits our ability to
see how far a particular detection scheme is from optimal,
where optimal is determined based on the underlying noise
of the system. However since each scheme yields a differ-
ent ROC curve, these remain a powerful means of compar-
ison across schemes. If one curve has less area beneath it,
then it is clearly inferior, regardless of the threshold level
selected.
3.2 Basic analysis using Variance
The ﬁrst anomaly detector that will be described is also
the simplest one. As seen previously in normal operational
condition one might assume that ξt = 0 and that the pre-
diction error ζt follows a process with mean 0 and known
variance. Under the situation that the statistics of ζt, the
prediction error are fully known, it is easy to construct a
statistical test following the Neyman-Pearson theorem. For
this purpose we might use the construction given in Eq. 9.
The approach consists of constructing the process τt =
whenever τti > T ×p(Pt+1|t+1)ii where T is the thresh-
−KtAtPt|t−1S
−1
t ηt and rising an alarm for an OD pair i
old. Actually, this approach veriﬁes if the prediction error
is inside a conﬁdence interval. This anomaly detector is the
optimal one for the case where ζt + ηt follow a gaussian
distribution. However, if this hypothesis in not precisely
true (as frequently in practice), application of this anomaly
detector will lead to a ROC curve that is lower than the
optimal one.
An interesting property of this method is that the test is
veriﬁed as soon as a new observation has been processed by
the Kalman ﬁlter and it can therefore trigger an anomaly
very fast. However the drawback of the approach is that
each test is being done independently of past observations.
This might lead to high false positive rate when the process
ζt has a heavier tail than the gaussian. One might want to
3.3 CUSUM and Generalized Likelihood
Ratio test
The previous method missed an essential fact, since we
are in the context of random processes, tests executed at
each time t are not independent. The classical approach
for detecting a change in a random process is the CUSUM
(Cumulative Summation) method and its variants [4]. The
main intuition behind the CUSUM method is that when a
change occurs the log-likelihood ratio of an observation yi,
deﬁned as si = log L1(y)
, shifts from a negative value to a
positive one (as after the change hypothesis H1 becomes
L0(y)
more likely). This means that the log-likelihood of ob-
SN−1 =PN−1
}, deﬁned as
serving a sequence of N observations {yN−1
i=0 si, that was decreasing with N, begins to
increase after the change. The minimum value of Sj gives
an estimate of the change point. Therefore a simple statis-
tical test for change detection consists of testing whether :
0
Sk − min
0≤j≤k
Sj > T,
where Sk is the log-likelihood ratio deﬁned previously and
T is a threshold. After a change has been detected, the time
of change can be estimated as :
ˆtc = arg min
0≤j≤k
{Sj}
The previously described CUSUM algorithm has been
widely used for anomaly detection. However it suffers
from a key drawback. It is stated in the context of a simple
hypothesis, where the alternative hypothesis H1 should be
completely deﬁned, i.e. the level of the change or in other
terms the intensity of the anomaly should be known a pri-
ori. However in practical settings, this is exactly unknown
as by deﬁnition anomalies are not predictable.
A solution for this issue is provided by the General Like-
lihood Ratio Test. In this approach the level of change in
the CUSUM algorithm is replaced by its maximum likeli-
hood estimate. To describe the approach let’s ﬁx a scenario.
Suppose an anomaly occurs and this results in a shift in the
mean of the residual process. After the shift, the estima-
tion error will no longer be a zero mean random variable
of variance σ (σ is assumed to be known), but instead is
translated to a mean µ, that is unknown, and the same vari-
ance. The GLR algorithm uses a window of estimation er-
} and applies for each i, j ≤ i ≤ j +N −1 the
ror {τ j+N−1
following test. It ﬁrst estimates the mean of the estimation
error over the window {i, . . . j + N − 1} as
j
ˆµ =
1
j + N − 1 − i
j+N−1X
l=i
τl
USENIX Association
Internet Measurement Conference 2005  
337
00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91FPR1-FNR2=12=22=3It then performs a simple CUSUM test with ˆµ as the level
change value and we raise an alarm if a change is de-
tected. We implemented here a variant of the classical GLR
method described in [7]. This method is very powerful
since there exists a proof that this is the best estimator when
level change µ and variance σ are unknown. However its
main drawback is that it adds some delay for the detection
of the anomaly since it needs some observations after the
anomaly to estimate the deviation level. The detection de-
lay will not be constant and will depend on the anomaly.
For example, the effect of small volume anomalies on the
mean will propagate slowly and thus may not be detected
as quickly as large volume anomalies.
3.4 Multiscale analysis using variance
Multi scale analysis has been proposed as a promising ap-
proach to make robust anomaly detectors and is now com-
monly accepted as a powerful tool. The rational behind
using multiscale analysis is that anomalies should appear
at different time scales and by monitoring these multiple
scales one should be able to reduce the False Positive Rate,
because a change appearing on only one time scale will not
trigger an alarm.
We implemented a multi-scale analysis based on a cas-
cade decomposition of the original signal τt into a low fre-
. The
quency approximation aL
t
multi-scale decomposition lead to the following relation :
and a cascade of details di
t
LX
τt = aL
t +
di
t.
where :
di
t = X
t = X
s
aL
i=1
τs2−iψ(2−is − t), i = 1, . . . , L,
τs2−Lφ(2−Ls − t),
s
and ψ(.) is a mother wavelet function and φ(.) its corre-
sponding scaling functions [14].
Now, an anomaly detection mechanism, similar to that
described in the basic analysis using variance subsection,
is applied to each details time series. For each level l ∈
[1, L]we create a 0-1 sequence: each time instant t is as-
signed either a 0 or 1 where 0 indicates that no anomaly
was detected and 1 means an anomaly was ﬂagged. By
summing across these 0-1 time series, for a given time in-
stant, we have the number of times that an anomaly was de-
tected across all the details signals. The larger this numer,
the more time scales at which the anomaly was detected.
(In practice, we sum not over a single time instant, but over
a small window in each signal). An anomaly ﬂag is raised
if the anomaly is detected at a sufﬁcient number of scales.
The computation of the wavelet introduces a lag in the de-
tection; this lag will be a function of of the largest scale
used.
3.5 Multi scale variance shift
This method is derived from [2]. In this paper the authors
detect the difference between the local and the global vari-
ance of the process. They ﬁrst remove the trend of the sig-
nal using a wavelet transform, i.e. the remove the approx-
imation part of a wavelet transform. Thereafter they use a
small window to compute a local variance. Whenever the
ratio between this local variance and the global variance
(computed on all the data) exceeds a threshold T then an
alarm is triggered.
This method is in fact a special case of the multiscale
analysis previously described, where only two scales are
analyzed, the scale at which the global variance is calcu-
lated and the local scale where the local variance is cal-
culated. The approach can be assimilated to wavelet trans-
form with a Haar wavelet. The other interesting point of the
approach is that it detects a variation in the variance of the
process in place of detecting a variation in the mean as pre-
viously described approaches. It is noteworthy that other
approaches could also be adapted to detecting changes in
variance in place of the mean.
This method will also experience a detection lag time,
since the wavelet approach introduces a lag due to the time
needed to compute the wavelet transform in the two scales.
The width of the window of time over which to computes
the local variance is very important and will depend on the
duration of the anomaly to detect.
4 Validation Methodology
The validation of any anomaly detection method is always
fraught with difﬁculty. The challenge comes from our in-
ability to establish the ”ground truth”. Among the most
interesting performance metrics for such methods are the
false positive and false negative rates. However computing
these rates requires us to know exactly which events (and
corresponding point in time) were anomalies and which
were not. One common approach to evaluating anomaly
detection algorithms is to collect live data in the form of
a packet or ﬂow level trace, and then to have this trace
”labeled”. Labeling or marking a trace is the procedure
by which each anomalous event is identiﬁed along with its
start and ﬁnish time. Perhaps the best way to do this in to-
day’s world is for a security operations expert to do the la-
beling either via visual inspection or with the help of tools.
They have a wealth of real world experience that is hard
to automate. Although this is currently our best option,
the labeling method is not perfect as operators can make
mistakes, either missing an anomaly or generating a false
338