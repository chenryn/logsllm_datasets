Site
Reliability
Engineering
HOW GOOGLE RUNS PRODUCTION SYSTEMS
Edited by Betsy Beyer, Chris Jones,
Jennifer Petoff & Niall Richard Murphy
Praise for Site Reliability Engineering
Google’s SREs have done our industry an enormous service by writing up the principles,
practices and patterns—architectural and cultural—that enable their teams to combine
continuous delivery with world-class reliability at ludicrous scale. You owe it to yourself
and your organization to read this book and try out these ideas for yourself.
—Jez Humble, coauthor of Continuous Delivery and
Lean Enterprise
I remember when Google first started speaking at systems administration conferences.
It was like hearing a talk at a reptile show by a Gila monster expert. Sure, it was
entertaining to hear about a very different world, but in the end the audience
would go back to their geckos.
Now we live in a changed universe where the operational practices of Google are
not so removed from those who work on a smaller scale. All of a sudden, the best
practices of SRE that have been honed over the years are now of keen interest to
the rest of us. For those of us facing challenges around scale, reliability and
operations, this book comes none too soon.
—David N. Blank-Edelman, Director, USENIX Board of
Directors, and founding co-organizer of SREcon
I have been waiting for this book ever since I left Google’s enchanted castle.
It is the gospel I am preaching to my peers at work.
—Björn Rabenstein, Team Lead of Production Engineering at
SoundCloud, Prometheus developer, and Google SRE until 2013
A thorough discussion of Site Reliability Engineering from the company that
invented the concept. Includes not only the technical details but also the
thought process, goals, principles, and lessons learned over time. If you want
to learn what SRE really means, start here.
—Russ Allbery, SRE and Security Engineer
With this book, Google employees have shared the processes they have taken,
including the missteps, that have allowed Google services to expand to both massive
scale and great reliability. I highly recommend that anyone who wants to create a
set of integrated services that they hope will scale to read this book. The book
provides an insider’s guide to building maintainable services.
—Rik Farrow, USENIX
Writing large-scale services like Gmail is hard. Running them with high reliability is
even harder, especially when you change them every day. This comprehensive
“recipe book” shows how Google does it, and you’ll find it much cheaper to learn
from our mistakes than to make them yourself.
—Urs Hölzle, SVP Technical Infrastructure, Google
Site Reliability Engineering
How Google Runs Production Systems
Edited by Betsy Beyer, Chris Jones, Jennifer Petoff,
and Niall Richard Murphy
BBeeiijjiinngg BBoossttoonn FFaarrnnhhaamm SSeebbaassttooppooll TTookkyyoo
Site Reliability Engineering
Edited by Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy
Copyright © 2016 Google, Inc. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are
also available for most titles (http://safaribooksonline.com). For more information, contact our corporate/
institutional sales department: 800-998-9938 or corporate@oreilly.com.
Editor: Brian Anderson Indexer: Judy McConville
Production Editor: Kristen Brown Interior Designer: David Futato
Copyeditor: Kim Cofer Cover Designer: Karen Montgomery
Proofreader: Rachel Monaghan Illustrator: Rebecca Demarest
April 2016: First Edition
Revision History for the First Edition
2016-03-21: First Release
See http://oreilly.com/catalog/errata.csp?isbn=9781491929124 for release details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Site Reliability Engineering, the cover
image, and related trade dress are trademarks of O’Reilly Media, Inc.
While the publisher and the authors have used good faith efforts to ensure that the information and
instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility
for errors or omissions, including without limitation responsibility for damages resulting from the use of
or reliance on this work. Use of the information and instructions contained in this work is at your own
risk. If any code samples or other technology this work contains or describes is subject to open source
licenses or the intellectual property rights of others, it is your responsibility to ensure that your use
thereof complies with such licenses and/or rights.
978-1-491-92912-4
[LSI]
Table of Contents
Foreword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii
Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv
Part I. Introduction
1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
The Sysadmin Approach to Service Management 3
Google’s Approach to Service Management: Site Reliability Engineering 5
Tenets of SRE 7
The End of the Beginning 12
2. The Production Environment at Google, from the Viewpoint of an SRE. . . . . . . . . . . . 13
Hardware 13
System Software That “Organizes” the Hardware 15
Other System Software 18
Our Software Infrastructure 19
Our Development Environment 19
Shakespeare: A Sample Service 20
Part II. Principles
3. Embracing Risk. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Managing Risk 25
Measuring Service Risk 26
Risk Tolerance of Services 28
v
Motivation for Error Budgets 33
4. Service Level Objectives. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Service Level Terminology 37
Indicators in Practice 40
Objectives in Practice 43
Agreements in Practice 47
5. Eliminating Toil. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Toil Defined 49
Why Less Toil Is Better 51
What Qualifies as Engineering? 52
Is Toil Always Bad? 52
Conclusion 54
6. Monitoring Distributed Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
Definitions 55
Why Monitor? 56
Setting Reasonable Expectations for Monitoring 57
Symptoms Versus Causes 58
Black-Box Versus White-Box 59
The Four Golden Signals 60
Worrying About Your Tail (or, Instrumentation and Performance) 61
Choosing an Appropriate Resolution for Measurements 62
As Simple as Possible, No Simpler 62
Tying These Principles Together 63
Monitoring for the Long Term 64
Conclusion 66
7. The Evolution of Automation at Google. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
The Value of Automation 67
The Value for Google SRE 70
The Use Cases for Automation 70
Automate Yourself Out of a Job: Automate ALL the Things! 73
Soothing the Pain: Applying Automation to Cluster Turnups 75
Borg: Birth of the Warehouse-Scale Computer 81
Reliability Is the Fundamental Feature 83
Recommendations 84
8. Release Engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
The Role of a Release Engineer 87
Philosophy 88
vi | Table of Contents
Continuous Build and Deployment 90
Configuration Management 93
Conclusions 95
9. Simplicity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
System Stability Versus Agility 97
The Virtue of Boring 98
I Won’t Give Up My Code! 98
The “Negative Lines of Code” Metric 99
Minimal APIs 99
Modularity 100
Release Simplicity 100
A Simple Conclusion 101
Part III. Practices
10. Practical Alerting from Time-Series Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
The Rise of Borgmon 108
Instrumentation of Applications 109
Collection of Exported Data 110
Storage in the Time-Series Arena 111
Rule Evaluation 114
Alerting 118
Sharding the Monitoring Topology 119
Black-Box Monitoring 120
Maintaining the Configuration 121
Ten Years On… 122
11. Being On-Call. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
Introduction 125
Life of an On-Call Engineer 126
Balanced On-Call 127
Feeling Safe 128
Avoiding Inappropriate Operational Load 130
Conclusions 132
12. Effective Troubleshooting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Theory 134
In Practice 136
Negative Results Are Magic 144
Case Study 146
Table of Contents | vii
Making Troubleshooting Easier 150
Conclusion 150
13. Emergency Response. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
What to Do When Systems Break 151
Test-Induced Emergency 152
Change-Induced Emergency 153
Process-Induced Emergency 155
All Problems Have Solutions 158
Learn from the Past. Don’t Repeat It. 158
Conclusion 159
14. Managing Incidents. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
Unmanaged Incidents 161
The Anatomy of an Unmanaged Incident 162
Elements of Incident Management Process 163
A Managed Incident 165
When to Declare an Incident 166
In Summary 166
15. Postmortem Culture: Learning from Failure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
Google’s Postmortem Philosophy 169
Collaborate and Share Knowledge 171
Introducing a Postmortem Culture 172
Conclusion and Ongoing Improvements 175
16. Tracking Outages. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
Escalator 178
Outalator 178
17. Testing for Reliability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
Types of Software Testing 185
Creating a Test and Build Environment 190
Testing at Scale 192
Conclusion 204
18. Software Engineering in SRE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
Why Is Software Engineering Within SRE Important? 205
Auxon Case Study: Project Background and Problem Space 207
Intent-Based Capacity Planning 209
Fostering Software Engineering in SRE 218
Conclusions 222
viii | Table of Contents
19. Load Balancing at the Frontend. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
Power Isn’t the Answer 223
Load Balancing Using DNS 224
Load Balancing at the Virtual IP Address 227
20. Load Balancing in the Datacenter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
The Ideal Case 232
Identifying Bad Tasks: Flow Control and Lame Ducks 233
Limiting the Connections Pool with Subsetting 235
Load Balancing Policies 240
21. Handling Overload. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
The Pitfalls of “Queries per Second” 248
Per-Customer Limits 248
Client-Side Throttling 249
Criticality 251
Utilization Signals 253
Handling Overload Errors 253
Load from Connections 257
Conclusions 258
22. Addressing Cascading Failures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
Causes of Cascading Failures and Designing to Avoid Them 260
Preventing Server Overload 265
Slow Startup and Cold Caching 274
Triggering Conditions for Cascading Failures 276
Testing for Cascading Failures 278
Immediate Steps to Address Cascading Failures 280
Closing Remarks 283
23. Managing Critical State: Distributed Consensus for Reliability. . . . . . . . . . . . . . . . . . 285
Motivating the Use of Consensus: Distributed Systems Coordination Failure 288
How Distributed Consensus Works 289
System Architecture Patterns for Distributed Consensus 291
Distributed Consensus Performance 296
Deploying Distributed Consensus-Based Systems 304
Monitoring Distributed Consensus Systems 312
Conclusion 313
24. Distributed Periodic Scheduling with Cron. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
Cron 315
Cron Jobs and Idempotency 316
Table of Contents | ix
Cron at Large Scale 317
Building Cron at Google 319
Summary 326
25. Data Processing Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
Origin of the Pipeline Design Pattern 327
Initial Effect of Big Data on the Simple Pipeline Pattern 328
Challenges with the Periodic Pipeline Pattern 328
Trouble Caused By Uneven Work Distribution 328
Drawbacks of Periodic Pipelines in Distributed Environments 329
Introduction to Google Workflow 333
Stages of Execution in Workflow 335
Ensuring Business Continuity 337
Summary and Concluding Remarks 338
26. Data Integrity: What You Read Is What You Wrote. . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
Data Integrity’s Strict Requirements 340
Google SRE Objectives in Maintaining Data Integrity and Availability 344
How Google SRE Faces the Challenges of Data Integrity 349
Case Studies 360
General Principles of SRE as Applied to Data Integrity 367
Conclusion 368
27. Reliable Product Launches at Scale. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369