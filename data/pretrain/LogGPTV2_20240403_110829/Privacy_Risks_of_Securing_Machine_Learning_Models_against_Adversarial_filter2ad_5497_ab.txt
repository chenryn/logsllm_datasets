output Fθ(xadv) as the loss function ℓ′, and combine it with natural
cross entropy loss (α (cid:44) 0).
ℓ
′(Fθ ,(xadv , y)) = dkl(Fθ(xadv), Fθ(x)),
(11)
where dkl computes the KL divergence. Adversarial examples are
also generated with PGD-based attacks, except that now the attack
goal is to maximize the output difference,
˜xt +1 = ΠBϵ(x)[˜xt + η · sign( ∇˜xt dkl(Fθ(˜xt), Fθ(x)))].
(12)
2.1.3 Verifiable defenses: Although empirical defense methods are
effective against state-of-the-art adversarial examples [4], there is
no guarantee for such robustness. To obtain a guarantee for robust-
ness, verification approaches have been proposed to compute an
upper bound of prediction loss ℓ′ under the adversarial perturbation
constraint Bϵ . If the input can still be predicted correctly in the ver-
ified worst case, then it is certain that there is no misclassification
existing under Bϵ .
Thus, verifiable defense methods take the verification process
into consideration during training by using the verified worst case
prediction loss as robust loss value ℓR. Now the robust training
algorithm becomes

1
min
θ
z∈Dtrain
|Dtrain|
α·ℓ(Fθ , z)+(1−α)·V(ℓ
′(Fθ ,(˜x, y)), Bϵ), (13)
where V means verified upper bound computation of prediction
loss ℓ′ under the adversarial perturbation constraint Bϵ . In this
paper, we consider the following three verifiable defense methods.
Duality-Based Verification (Dual-Based Verify) [61]: Wong and
Kolter [61] compute the verified worst-case loss by solving its dual
problem with convex relaxation on non-convex ReLU operations
and then minimize this overapproximated robust loss values only
(α = 0, ℓ′ = ℓ). They further combine this duality relaxation method
with the random projection technique to scale to more complex
neural network architectures [62], like ResNet [20].
Abstract Interpretation-Based Verification (Abs-Based Ver-
ify) [34]: Mirman et al. [34] leverage the technique of abstract
interpretation to compute the worse-case loss: an abstract domain
(such as interval domain, zonotope domain [13]) is used to express
the adversarial perturbation constraint Bϵ at the input layer, and by
applying abstract transformers on it, the maximum verified range
of model output is obtained. They adopt a softplus function on the
logits дθ(˜x) to compute the robust loss value and then combine it
with natural training loss (α (cid:44) 0).
′(Fθ ,(˜x, y)) = log( exp(max
y′(cid:44)y
ℓ
дθ(˜x)y′ − дθ(˜x)y) + 1)
(14)
Interval Bound Propagation-Based Verification (IBP-Based
Verify) [16]: Gowal et al. [16] share a similar design as Mirman
et al. [34]: they express the constraint Bϵ as a bounded interval
domain (one specified domain considered by Mirman et al. [34])
and propagate this bound to the output layer. The robust loss is
computed as a cross-entropy loss of verified worse-case outputs
(ℓ′ = ℓ) and then combined with natural prediction loss (α (cid:44) 0) as
the final loss value during training.
2.2 Membership Inference Attacks
For a target machine learning model, the membership inference
attacks aim to determine whether a given data point was used to
train the model or not [18, 32, 37, 41, 47, 64]. The attack poses a
serious privacy risk to the individuals whose data is used for model
training, for example in the setting of health analytics.
Shokri et al. [47] design a membership inference attack method
based on training an inference model to distinguish between pre-
dictions on training set members versus non-members. To train the
inference model, they introduce the shadow training technique: (1)
the adversary first trains multiple “shadow models” which simulate
the behavior of the target model, (2) based on the shadow models’
outputs on their own training and test examples, the adversary
obtains a labeled (member vs non-member) dataset, and (3) finally
trains the inference model as a neural network to perform mem-
bership inference attack against the target model. The input to the
inference model is the prediction vector of the target model on a
target data record.
A simpler inference model, such as a linear classifier, can also
distinguish significantly vulnerable members from non-members.
Yeom et al. [64] suggest comparing the prediction confidence value
of a target example with a threshold (learned for example through
shadow training). Large confidence indicates membership. Their
results show that such a simple confidence-thresholding method is
reasonably effective and achieves membership inference accuracy
close to that of a complex neural network classifier learned from
shadow training.
In this paper, we use this confidence-thresholding membership
inference approach in most cases. Note that when evaluating the
privacy leakage with targeted adversarial examples in Section 3.3.1
and Section 5.2.5, the confidence-thresholding approach does not
apply as there are multiple prediction vectors for each data point. In-
stead, we follow Shokri et al. [47] to train a neural network classifier
for membership inference.
3 MEMBERSHIP INFERENCE ATTACKS
AGAINST ROBUST MODELS
In this section, we first present some insights on why training
models to be robust against adversarial examples make them more
susceptible to membership inference attacks. We then formally
present our membership inference attacks.
Throughout the paper, we use “natural (default) model” and
“robust model” to denote the machine learning model with natural
training algorithm and robust training algorithm, respectively. We
also call the unmodified inputs and adversarially perturbed inputs
as “benign examples” and “adversarial examples”. When evaluating
the model’s classification performance, “train accuracy” and “test
accuracy” are used to denote the classification accuracy of benign
examples from training and test sets; “adversarial train accuracy’’
and “adversarial test accuracy” represent the classification accuracy
of adversarial examples from training and test sets; “verified train
accuracy” and “verified test accuracy” measure the classification
accuracy under the verified worst-case predictions from training
and test sets. Finally, an input example is called “secure” when it is
correctly classified by the model for all adversarial perturbations
within the constraint Bϵ , “insecure” otherwise.
The performance of membership inference attacks is highly re-
lated to generalization error of target models [47, 64]. An extremely
simple attack algorithm can infer membership based on whether or
not an input is correctly classified. In this case, it is clear that a large
gap between the target model’s train and test accuracy leads to a
significant membership inference attack accuracy (as most mem-
bers are correctly classified, but not the non-members). Tsipras et al.
[59] and Zhang et al. [66] show that robust training might lead to a
drop in test accuracy. This is shown based on both empirical and
theoretical analysis on toy classification tasks. Moreover, the gener-
alization gap can be enlarged for a robust model when evaluating its
accuracy on adversarial examples [42, 51]. Thus, compared with the
natural models, the robust models might leak more member-
ship information, due to exhibiting a larger generalization
error, in both the benign or adversarial settings.
The performance of membership inference attack is related to
the target model’s sensitivity with regard to training data [32]. The
sensitivity measure is the influence of one data point on the tar-
get model’s performance by computing its prediction difference,
when trained with and without this data point. Intuitively, when a
training point has a large influence on the target model (high sensi-
tivity), its model prediction is likely to be different from the model
prediction on a test point, and thus the adversary can distinguish
its membership more easily. The robust training algorithms aim to
ensure that model predictions remain unchanged for a small area
(such as the l∞ ball) around any data point. However, in practice,
they guarantee this for the training examples, thus, magnifying the
influence of the training data on the model. Therefore, compared
with the natural training, the robust training algorithms might
make the model more susceptible to membership inference
attacks, by increasing its sensitivity to its training data.
To validate the above insights, let’s take the natural and the
robust CIFAR10 classifiers provided by Madry et al. [33] as an ex-
ample. From Figure 1, we have seen that compared to the natural
model, the robust model has a larger divergence between the pre-
diction loss of training data and test data. Our fine-grained analysis
in Appendix A further reveals that the large divergence of robust
model is highly related to its robustness performance. Moreover,
the robust model incurs a significant generalization error in the
adversarial setting, with 96% adversarial train accuracy, and only
47% adversarial test accuracy. Finally, we will experimentally show
in Section 5.2.1 that the robust model is indeed more sensitive with
regard to training data.
3.1 Membership Inference Performance
Table 1: Notations for membership inference attacks against
robust machine learning models.
Symbol
F
Bϵ
Dtrain
Dtest
x
y
xadv
V
I
Ainf
ADV Tinf
Description
Target machine learning model.
Adversarial perturbation constraint when training a robust model.
Model’s training set.
Model’s test set.
Benign (unmodified) input example.
Ground truth label for the input x.
Adversarial example generated from x.
Robustness verification to compute verified worst-case predictions.
Membership inference strategy.
Membership inference accuracy.
Membership inference advantage compared to random guessing.


+
In this part, we describe the membership inference attack and
its performance formally, with notations listed in Table 1. For a
neural network model F (we skip its parameter θ for simplicity) that
is robustly trained with the adversarial constraint Bϵ , the mem-
bership inference attack aims to determine whether a given input
example z = (x, y) is in its training set Dtrain or not. We denote the
inference strategy adopted by the adversary as I(F , Bϵ , z), which
codes members as 1, and non-members as 0.
We use the fraction of correct membership predictions, as the
metric to evaluate membership inference accuracy. We use a test
set Dtest which does not overlap with the training set, to represent
non-members. We sample a random data point (x, y) from either
Dtrain or Dtest with an equal 50% probability, to test the membership
inference attack. We measure the membership inference accuracy
as follows.
z∈Dtrain I(F , Bϵ , z)
Ainf (F , Bϵ ,I) =
where | · | measures the size of a dataset.
2 · |Dtrain|
z∈Dtest 1 − I(F , Bϵ , z)
,
(15)
2 · |Dtest|
The membership inference accuracy evaluates the probability
that the adversary can guess correctly whether an input is from
training set or test set. Note that a random guessing strategy will
lead to a 50% inference accuracy. To further measure the effective-
ness of our membership inference strategy, we also use the notion
of membership inference advantage proposed by Yeom et al. [64],
which is defined as the increase in inference accuracy over random
guessing (multiplied by 2).
ADVT inf = 2 × (Ainf − 0.5)
(16)
3.2 Exploiting the Model’s Predictions on
Benign Examples
We adopt a confidence-thresholding inference strategy due to its
simplicity and effectiveness [64]: an input (x, y) is inferred as mem-
ber if its prediction confidence F(x)y is larger than a preset thresh-
old value. We denote this inference strategy as IB since it relies on
the benign examples’ predictions. We have the following expres-
sions for this inference strategy and its inference accuracy.
IB(F , Bϵ ,(x, y)) = 1{F(x)y ≥ τB}
Ainf (F , Bϵ ,IB) =

z∈Dtrain 1{F(x)y ≥ τB}

1
2 · (
|Dtrain|
z∈Dtest 1{F(x)y ≥ τB}
1
2 +
−
),
|Dtest|
(17)
where 1{·} is the indicator function and the last two terms are
the values of complementary cumulative distribution functions of
training examples’ and test examples’ prediction confidences, at
the point of threshold τB, respectively. In our experiments, we eval-
uate the worst case inference risks by choosing τB to achieve the
highest inference accuracy, i.e., maximizing the gap between two
complementary cumulative distribution function values. In prac-
tice, an adversary can learn the threshold via the shadow training
technique [47].
This inference strategy IB does not leverage the adversarial
constraint Bϵ of the robust model. Intuitively, the robust training
algorithm learns to make smooth predictions around training ex-
amples. In this paper, we observe that such smooth predictions
around training examples may not generalize well to test examples
and we can leverage this property to perform stronger membership
inference attacks. Based on this observation, we propose two new
membership inference strategies against robust models by taking
Bϵ into consideration.
3.3 Exploiting the Model’s Predictions on
Adversarial Examples
Our first new inference strategy is to generate an (untargeted)
adversarial example xadv for input (x, y) under the constraint Bϵ ,
and use a threshold on the model’s prediction confidence on xadv.
We have following expression for this strategy IA and its inference
accuracy.
IA(F , Bϵ ,(x, y)) = 1{F(xadv)y ≥ τA}
Ainf (F , Bϵ ,IA) =

z∈Dtrain 1{F(xadv)y ≥ τA}

1
2 · (
z∈Dtest 1{F(xadv)y ≥ τA}
|Dtrain|
1
2 +
−
)
|Dtest|
(18)
We use the PGD attack method shown in Equation (9) to obtain
xadv. Similarly, we choose the preset threshold τA to achieve the
highest inference accuracy, i.e., maximizing the gap between two
complementary cumulative distribution functions of prediction
confidence on adversarial train and test examples.
To perform membership inference attacks with the strategy IA,
we need to specify the perturbation constraint Bϵ . For our experi-
mental evaluations in Section 5 and Section 6, we use the same per-
turbation constraint Bϵ as in the robust training process, which is
assumed to be prior knowledge of the adversary. We argue that this
assumption is reasonable following Kerckhoffs’s principle [25, 44].
In Section 7.1, we measure privacy leakage when the robust model’s
perturbation constraint is unknown.
3.3.1 Targeted adversarial examples. We extend the attack to ex-
ploiting targeted adversarial examples. Targeted adversarial ex-
amples contain information about distance of the benign input
to each label’s decision boundary, and are expected to leak more
membership information than the untargeted adversarial example
which only contains information about distance to a nearby label’s
decision boundary.
We adapt the PGD attack method to find targeted adversarial
examples (Equation (5)) by iteratively minizing the targeted cross-
entropy loss.
′)))]
˜xt +1 = ΠBϵ(x)[˜xt − η · sign( ∇˜xt ℓ(Fθ ,(˜xt , y
(19)
The confidence thresholding inference strategy does not apply
for targeted adversarial examples because there exist k − 1 targeted
adversarial examples (we have k − 1 incorrect labels) for each input.
Instead, following Shokri et al. [47], we train a binary inference
classifier for each class label to perform the membership inference
attack. For each class label, we first choose a fraction of training
and test points and generate corresponding targeted adversarial
examples. Next, we compute model predictions on the targeted ad-
versarial examples, and use them to train the membership inference
classifier. Finally, we perform inference attacks using the remaining
training and test points.

3.4 Exploiting the Verified Worst-Case
Predictions on Adversarial Examples
Our attacks above generate adversarial examples using the heuristic
strategy of projected gradient descent. Next, we leverage verification
techniques V used by the verifiably defended models [16, 34, 61]
to obtain the input’s worst-case predictions under the adversarial
constraint Bϵ . We use the input’s worst-case prediction confidence
to predict its membership. The expressions for this strategy IV and
its inference accuracy are as follows.
IV(F , Bϵ ,(x, y)) = 1{V(F(˜x)y , Bϵ) ≥ τV}
Ainf (F , Bϵ ,IV) =
z∈Dtrain V(F(˜x)y , Bϵ) ≥ τV}

1
2 · (
z∈Dtest V(F(˜x)y , Bϵ) ≥ τV}
|Dtrain|
1
2 +
−
),
|Dtest|
(20)
where V(F(˜x)y , Bϵ) returns the verified worst-case prediction con-
fidence for all examples ˜x satisfying the adversarial perturbation
constraint ˜x ∈ Bϵ(x), and τV is chosen in a similar manner as our
previous two inference strategies.
Note that different verifiable defenses adopt different verifica-
tion methods V. Our inference strategy IV needs to use the same
verification method which is used in the target model’s verifiably
robust training process. Again, we argue that it is reasonable to
assume that an adversary has knowledge about the verification
method V and the perturbation constraint Bϵ , following Kerck-
hoffs’s principle [25, 44].
4 EXPERIMENT SETUP
In this section, we describe the datasets, neural network architec-
tures, and corresponding adversarial perturbation constraints that
we use in our experiments. Throughout the paper, we focus on
the l∞ perturbation constraint: Bϵ(x) = {x′ | ∥x′ − x∥∞ ≤ ϵ}. The
detailed architectures are summarized in Appendix B. Our code is
publicly available at https://github.com/inspire-group/privacy-vs-
robustness.
Yale Face. The extended Yale Face database B is used to train
face recognition models, and contains gray scale face images of
38 subjects under various lighting conditions [14, 30]. We use the
cropped version of this dataset, where all face images are aligned
and cropped to have the dimension of 168 × 192. In this version,
each subject has 64 images with the same frontal poses under dif-
ferent lighting conditions, among which 18 images were corrupted
during the image acquisition, leading to 2,414 images in total [30].
In our experiments, we select 50 images for each subject to form
the training set (total size is 1,900 images), and use the remaining
514 images as the test set.