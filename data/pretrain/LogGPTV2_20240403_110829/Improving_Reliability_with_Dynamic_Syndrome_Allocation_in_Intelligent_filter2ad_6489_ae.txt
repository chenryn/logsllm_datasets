 0
 1
 2
 3
 4
 5
Proportion Covered by Additional Syndromes
 6
 7
(e) Additional syndromes allocated by our algorithm using a mean-
shift clustered Markov model with h=5 (8 clusters), and a 0.95 worst
case quartile prediction.
(f) Additional syndromes allocated by our algorithm using a mean-
shift clustered Markov model with h=5 (8 clusters), and a 0.999 worst
case quartile prediction.
L
E
D
O
M
D
E
R
E
T
S
U
L
C
S
N
A
E
M
-
K
L
E
D
O
M
D
E
R
E
T
S
U
L
C
T
F
I
H
S
-
N
A
E
M
Fig. 10: Inverse cumulative density function of the proportion of the ﬁle system covered by additional syndromes, calculated
over the entire lifetime of the S2DDC.
228228
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:20 UTC from IEEE Xplore.  Restrictions apply. 
 underprediction rate vs. mean proportional syndrome coverage
0.95 quartile plot of
 underprediction rate vs. mean proportional syndrome coverage
0.99 quartile plot of
 0.25
 0.2
 0.15
 0.1
 0.05
 0
e
t
a
R
n
o
i
t
c
i
d
e
r
P
r
e
d
n
U
 0.25
 0.2
 0.15
 0.1
 0.05
 0
e
t
a
R
n
o
i
t
c
i
d
e
r
P
r
e
d
n
U
 2.8
 2.9
 3
 2.8
 2.9
 3
Mean Number of Additional Syndromes (SN)
Mean Number of Additional Syndromes (SN)
 underprediction rate vs. mean proportional syndrome coverage
0.999 quartile plot of
e
t
a
R
n
o
i
t
c
i
d
e
r
P
r
e
d
n
U
 0.25
 0.2
 0.15
 0.1
 0.05
 0
Mean-Shift
K-Means
Statistical
 0
 0.5
 1
 1.5
 2
 2.5
 3
Fig. 11: Scatter plots showing the trade-off of under-prediction rate and mean syndrome coverage for each of our methods.
Mean Number of Additional Syndromes (SN)
In conclusion,
in this paper we have presented novel
methods for estimating levels of overprovisioned resources in
S2DDCs and utilizing this knowledge with an attempt to bal-
ance QoS losses with reliability gains. We present performable
algorithms for these methods, and compare them against each
other, noting their strengths and weaknesses, and against a
statistical model which does not account for dependent actions.
We ﬁnd great value in using more complex Markov models,
both in terms of QoS and reliability gains. We present evidence
of the exciting ability to improve the reliability of existing
data centers without the addition of new hardware, and
based on our reliability results, which allow for the allocation
of on average 2 − 3 additional syndromes for all stripes
within a dataset, present the conclusion that existing data
centers can dramatically improve their reliability simply by
implementing novel middle-ware processes, and that these
gains can be made with minimal impact on QoS. In an effort
to improve joint understanding, research, and development on
the topic of software deﬁned data centers, we have made all
models, data, and code available on our research website. 2
VI. FUTURE WORK
Our study naturally suggests a course of exciting future
work. Our study has yielded observations which demonstrate
an optimal number of clusters for our current data. We have
recently taken possession of additional data sets that will allow
2http://dataengineering.org/research/SSDDC/
us to study the observed non-linear behavior in further detail.
We plan to evaluate our methods on these new data sets, and to
propose a method for optimal cluster size determination using
an adaptive clustering algorithm.
We also plan to further answer the question of whether
additional space indicated by ρ is best used for additional
syndrome coverage (improving reliability), or independent mir-
roring which while more costly in terms of needed blocks for
ﬁle system coverage, would improve performance for critical
“hot” blocks as well as improving their reliability. In order to
perform this research we are constructing novel hardware and
software to observe full usage patterns, including read patterns,
in existing data centers, and have partnered with several large
organizations and data centers to build comprehensive models
of read patterns, changes to the popularity of blocks in systems
storing data from a wide variety of domains, and details on the
write and delete patterns in various domains. We are working
to implement our methods with our partners on small replicas
of existing data sets to provide real-time analysis of performa-
bility, and to further quantify the effect on QoS of our methods.
We are working to implement our methods as extensions to
popular ﬁle systems and server architectures to allow their
deployment on a wide variety of machines, and to help better
manage the data needs of resource restricted organizations,
especially those with a primary scientiﬁc purpose.
229229
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:20 UTC from IEEE Xplore.  Restrictions apply. 
ACKNOWLEDGMENT
The authors would like to thank the Prairie Research Insti-
tute and the Illinois Natural History Survey for granting us ac-
cess to their data and personnel during this study, and for their
continued collaboration on topics related to S2DDCs . All code
and data for this project is made available through our website
under the terms of the University of Illinois/NCSA Open
Source License at http://dataengineering.org/research/SSDDC.
REFERENCES
[2]
[1]
J. L. Schnase, D. Q. Duffy, G. S. Tamkin, D. Nadeau, J. H. Thompson,
C. M. Grieg, M. A. McInerney, and W. P. Webster, “Merra analytic
services: Meeting the big data challenges of climate science through
cloud-enabled climate analytics-as-a-service,” Computers, Environment
and Urban Systems, 2014.
I. A. T. Hashem, I. Yaqoob, N. B. Anuar, S. Mokhtar, A. Gani, and
S. U. Khan, “The rise of big data on cloud computing: Review and
open research issues,” Information Systems, vol. 47, pp. 98–115, 2015.
[3] D. Duffy and J. Schnase, “Meeting the big data challenges of cli-
mate science through cloud-enabled climate analytics-as-a-service,” in
Proceedings of the 30th International Conference on Massive Storage
Systems and Technology.
IEEE Computer Society, 2014.
[4] S.-O. Act, “Public law no. 107-204,” 2002.
[5] U. Congress, “Health insurance portability and accountability act
(hipaa) of 1996,” Public Law, pp. 104–191, 2007.
[18] H. V. Madhyastha, J. C. McCullough, G. Porter, R. Kapoor, S. Savage,
A. C. Snoeren, and A. Vahdat, “scc: cluster storage provisioning
informed by application characteristics and slas,” in Proceedings of the
10th USENIX conference on File and Storage Technologies. USENIX
Association, 2012, pp. 23–23.
[19] G. Soundararajan, D. Lupei, S. Ghanbari, A. D. Popescu, J. Chen, and
C. Amza, “Dynamic resource allocation for database servers running
on virtual storage,” in Proccedings of the 7th conference on File and
storage technologies. USENIX Association, 2009, pp. 71–84.
[20] D. Comaniciu and P. Meer, “Mean shift: A robust approach toward
feature space analysis,” Pattern Analysis and Machine Intelligence,
IEEE Transactions on, vol. 24, no. 5, pp. 603–619, 2002.
J. A. Hartigan and M. A. Wong, “Algorithm as 136: A k-means
clustering algorithm,” Applied statistics, pp. 100–108, 1979.
[21]
[22] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman,
and A. Y. Wu, “An efﬁcient k-means clustering algorithm: Analysis
and implementation,” Pattern Analysis and Machine Intelligence, IEEE
Transactions on, vol. 24, no. 7, pp. 881–892, 2002.
[23] A. K. Jain, “Data clustering: 50 years beyond k-means,” Pattern
Recognition Letters, vol. 31, no. 8, pp. 651–666, 2010.
[24] K. Fukunaga and L. Hostetler, “The estimation of the gradient of a
density function, with applications in pattern recognition,” Information
Theory, IEEE Transactions on, vol. 21, no. 1, pp. 32–40, 1975.
[25] Y. Cheng, “Mean shift, mode seeking, and clustering,” Pattern Analysis
and Machine Intelligence, IEEE Transactions on, vol. 17, no. 8, pp.
790–799, 1995.
[26] B. E. Clark, F. D. Lawlor, W. E. Schmidt-Stumpf, T. J. Stewart, and
G. D. Timms Jr, “Parity spreading to enhance storage access,” Aug. 2
1988, uS Patent 4,761,785.
[6] F. Berman, “Got data?: a guide to data preservation in the information
age,” Communications of the ACM, vol. 51, no. 12, pp. 50–56, 2008.
[7] R. L. Moore, J. DAoust, R. H. McDonald, and D. Minor, “Disk and
tape storage cost models,” Archiving 2007, 2007.
“Vmware software-deﬁned data center
the next-generation data
center,” 2013. [Online]. Available: http://www.vmware.com/solutions/
datacenter/software-deﬁned-datacenter/index.html
[8]
[9] E. W. Rozier, P. Zhou, and D. Divine, “Building intelligence for
software deﬁned data centers: modeling usage patterns,” in Proceedings
of the 6th International Systems and Storage Conference. ACM, 2013,
p. 20.
[10] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McKeown, and
S. Shenker, “Nox: towards an operating system for networks,” ACM
SIGCOMM Computer Communication Review, vol. 38, no. 3, pp. 105–
110, 2008.
[11] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson,
J. Rexford, S. Shenker, and J. Turner, “Openﬂow: enabling innovation in
campus networks,” ACM SIGCOMM Computer Communication Review,
vol. 38, no. 2, pp. 69–74, 2008.
[12] B. Schroeder and G. A. Gibson, “Disk failures in the real world: What
does an mttf of 1,000,000 hours mean to you,” in Proceedings of the 5th
USENIX Conference on File and Storage Technologies (FAST), 2007,
pp. 1–16.
[13] E. Rozier, W. Belluomini, V. Deenadhayalan, J. Hafner, K. Rao, and
P. Zhou, “Evaluating the impact of undetected disk errors in raid sys-
tems,” in Dependable Systems & Networks, 2009. DSN’09. IEEE/IFIP
International Conference on.
J. L. Hafner, V. Deenadhayalan, W. Belluomini, and K. Rao, “Un-
detected disk errors in raid arrays,” IBM Journal of Research and
Development, vol. 52, no. 4.5, pp. 413–425, 2008.
IEEE, 2009, pp. 83–92.
[14]
[15] G. Wallace, F. Douglis, H. Qian, P. Shilane, S. Smaldone, M. Cham-
ness, and W. Hsu, “Characteristics of backup workloads in production
systems,” in Proceedings of the Tenth USENIX Conference on File and
Storage Technologies (FAST12), 2012.
[16] V. Tarasov, S. Kumar, J. Ma, D. Hildebrand, A. Povzner, G. Kuenning,
and E. Zadok, “Extracting ﬂexible, replayable models from large block
traces,” FAST12, 2012.
[17] E. Anderson, “Capture, conversion, and analysis of an intense nfs
workload,” in Proccedings of the 7th conference on File and storage
technologies. USENIX Association, 2009, pp. 139–152.
230230
[27] H. P. Anvin, “The mathematics of raid-6,” online paper, 2007.
[28] A. Leventhal, “Triple-parity raid and beyond,” Queue, vol. 7, no. 11,
p. 30, 2009.
J. Paris, S. Schwarz, A. Amer, and D. D. Long, “Highly reliable two-
dimensional raid arrays for archival storage,” in Performance Computing
and Communications Conference (IPCCC), 2012 IEEE 31st Interna-
tional.
IEEE, 2012, pp. 324–331.
[29]
[30] G. Clark, T. Courtney, D. Daly, D. Deavours, S. Derisavi, J. M. Doyle,
W. H. Sanders, and P. Webster, “The mobius modeling tool,” in Petri
Nets and Performance Models, 2001. Proceedings. 9th International
Workshop on.
IEEE, 2001, pp. 241–250.
[31] K. M. Greenan, J. S. Plank, and J. J. Wylie, “Mean time to meaningless:
Mttdl, markov models, and storage system reliability,” in Proceedings
of the 2nd USENIX conference on Hot topics in storage and ﬁle systems.
USENIX Association, 2010, pp. 5–5.
A.
Schroeder
[32] B.
Gibson.
and
G.
Reliabil-
the 1995-2005 timeframe.
(2007)
ity/interrupt/failure/usage data sets for
[Online]. Available: http://institute.lanl.gov/data/fdata/
[33] E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large
disk drive population.” in FAST, vol. 7, 2007, pp. 17–23.
[34] B. Schroeder and G. A. Gibson, “A large-scale study of failures in high-
performance computing systems,” IEEE Transactions on Dependable
and Secure Computing, vol. 7, no. 4, pp. 337–350, 2010.
J. L. Hafner, V. Deenadhayalan, K. Rao, and J. A. Tomlin, “Matrix
methods for lost data reconstruction in erasure codes.” in FAST, vol. 5,
2005, pp. 15–30.
[35]
[36] L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and J. Schindler,
“An analysis of latent sector errors in disk drives,” in ACM SIGMET-
RICS Performance Evaluation Review, vol. 35, no. 1. ACM, 2007, pp.
289–300.
[37] B. Schroeder, S. Damouras, and P. Gill, “Understanding latent sector
errors and how to protect against them,” ACM Transactions on storage
(TOS), vol. 6, no. 3, p. 9, 2010.
J. F. Meyer, A. Movaghar, and W. H. Sanders, “Stochastic activity net-
works: Structure, behavior, and application,” in International Workshop
on Timed Petri Nets.
IEEE Computer Society, 1985, pp. 106–115.
[38]
[39] R. Freire, A technique for simulating composed SAN-based reward
models. The University of Arizona., 1990.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:20 UTC from IEEE Xplore.  Restrictions apply.