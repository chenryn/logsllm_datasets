Vendor 3
SBGP
 0
 1
 2
 3
 5
 6
 4
Time (sec)
 7
 8
 9
 10
Figure 2: Gaps in table transfers
RR router from each vendor. The other routers presented simi-
lar behavior. This ﬁgure shows that table transfers contain gaps
that vary from 200 ms to 2 seconds each. The gaps’ duration de-
pends on the router and is nearly constant for each router. Ven-
dor 1 for instance has gaps that last 2 seconds while Vendor 2’s
gaps last around 200 ms. Although Vendor 1’s router has longer
gaps than Vendor 2’s router, it sends more updates between two
consecutive gaps, and consequently achieve the same transfer time
as Vendor 2’s router.
In fact, two factors contribute to the table
transfer time: the duration of gaps and the number of routes sent
between two consecutive gaps.
4.2 Causes
We now investigate the causes of gaps. First, we monitor the
CPU load on both RR and PE using command-line interface probes.
Both routers have low CPU (less than 10%) during the transfer. Our
analysis of the packet traces shows that PE quickly acknowledges
all packets received and is ready to receive new packets. In all ex-
periments, when a gap starts, PE sends TCP window update mes-
sages implicitly telling RR that it can send more data but RR does
not answer. The gaps are therefore caused by the sender which reg-
ularly stops sending routes. Besides, the sender’s low CPU indicate
that it is just blocking during the gaps and is not performing another
task.
Second, we analyze the source code of SBGP. During a table
transfer, SBGP reads routes from an input ﬁle with BGP messages.
We ﬁnd that BGP I/O operations are timer-driven: the sending of
routes is triggered by a timer that expires every second. BGP mes-
sages that have the same timestamp are sent together in the same
I/O operation, while next messages wait for one second till the
next I/O operation. The gaps are therefore the consequence of a
timer-driven implementation of BGP I/O operations. We compile
an event-driven version of SBGP in which we avoid the use of the
timer. This modiﬁed version can transfer BGP tables almost as fast
as baseline values (as deﬁned in Sec. 2). We call this version fast
SBGP.
The feature that causes gaps for router vendors is not documented.
We therefore report the issue to two vendors. Our discussions with
both of them conﬁrms our analysis of SBGP source code. Ven-
dor 2 reports that it intentionally limits the number of messages
sent per interrupt. This limit prevents routers from DoS attacks
and router overload and explains the gaps observed in Fig. 2. Each
time the BGP process is scheduled to send routes, it only sends
a certain number of routes and then waits for the next interrupt.
Vendor 1 says, however, that gaps are unintentional, they are the
consequence of the timer-driven implementation of one BGP pro-
cess. In fact, two BGP processes participate in route sending: the
router process prepares routes and puts them in an update queue,
the I/O process reads them from the queue and sends them to TCP.
Once the TCP ack is received, the I/O process empties the queue.
If the router process is scheduled to run while the queue is still full
(due to a late ack, for instance), then the router process sleeps until
a two-second timer expires.
Gaps represent an undocumented design choice that gives more
priority to router protection over fast table transfers.
Intentional
or not, this design choice impacts BGP table transfer time and de-
serves further consideration.
5. REDUCING TABLE TRANSFER TIME
This section discusses approaches for speeding up table trans-
fers and explores the trade-off between fast table transfers and con-
trolled router load. There are two possible approaches to speed up
table transfers: Increase the table sending rate or decrease the table
size.
5.1
Increasing the sending rate
Two factors contribute in the table sending rate, the interval be-
tween two interrupts and the volume of data sent per interrupt.
The interval between two interrupts can be reduced by using an
event-driven implementation. Because of our results, Vendor 1 al-
ready modiﬁed its implementation of the interaction between BGP
and TCP to be event-driven. This change ensures that the BGP
router process (see Sec. 4.2) is scheduled as soon as the I/O pro-
cess gets the TCP ack. The vendor reports that this modiﬁcation
reduced by a factor of 15 BGP table transfer time in its controlled
settings.
With a timer-driven implementation, the only way to increase the
sending rate is to send more routes between two interrupts. Ven-
dor 2 reports that it has a special conﬁguration mode that allows
tuning the maximum number of messages sent at each interrupt.
Unfortunately, this conﬁguration change is reset after a router re-
boot and this software add-on is not available on routers from our
labs so we could not test it. For Vendor 1, we ﬁnd that a possible
way to increase the sending between interrupts is a careful use of
TCP window scaling. Window scaling is a TCP option that allows
increasing the TCP window above its maximum. This option does
not remove the gaps but reduces their number, it allows a router to
send more routes between two consecutive gaps.
Our experiments focused so far on the table sending time in the
case of one single BGP session to a single receiver that install no
routes. If we speed up the sending rate, we may hit limits on the
receiver side because it has to install routes or at the sender side
when the route reﬂector has multiple clients.
Limit 1: Installing routes
We study the possible limits induced by the receiver when it has
to install routes. We perform several experiments in which we al-
ways keep the size of the table sent constant and vary each time the
number of installed routes. These experiments allow us to study
the impact of installing routes, given that the sending is constant.
We present results for a PE from Vendor 1 that is widely used
in the provider backbone. We ﬁx the table size in all experiments
to 680K routes which corresponds to a ‘full table’ in the provider
backbone. Each experiment starts by loading a new conﬁguration
in PE. PE’s conﬁguration controls the number of routes that PE
ultimately installs. Then, the route server sends the set of 680K
routes to RR. After RR and PE have ﬁnished installing all routes,
we reset the BGP session between PE and RR. In each experiment,
352)
n
m
i
(
e
m
i
t
r
e
f
s
n
a
r
T
08:00
07:00
06:00
05:00
04:00
03:00
02:00
01:00
00:00
34
68
Full table
Filtered table
204
136
Number of routes (in thousands)
408
340
272
476
544
e
m
i
t
r
e
f
s
n
a
r
t
l
e
b
a
T
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0
SBGP
Fast SBGP
 0
 2
 4
 6
 8
 10  12  14  16  18  20
Number of sessions
Figure 3: Varying the number of routes
Figure 4: Varying the number of sessions
we measure the table transfer time. We also run a script that uses
command-line interface to probe PE every 5 seconds. The script
gets the number of received and installed routes as well as PE’s
BGP CPU load.
The ‘full table’ curve in Fig. 3 presents the results of these exper-
iments. Fig. 3 presents the transfer time when varying the number
of routes that PE installs. We discuss the ‘ﬁltered table’ curve in
Sec. 5.2. Each point in the ﬁgure corresponds to one experiment.
We run each experiment three times and present the mean of the
three runs. We verify that the results are consistent across all runs.
The ‘full table’ curve shows that even though the table size is
constant across all experiments, the table transfer time increases
linearly with the number of installed routes. This increase implies
that the process of installing routes delays the table transfer. In fact,
the analysis of messages exchanged between RR and PE shows that
when PE installs routes, it delays the route reception by putting
some time to acknowledge the routes it receives from RR.
Limit 2: Handling multiple BGP sessions
Our experiments only study one BGP session, but BGP routers
usually connect to more than one BGP peer. Route reﬂectors in
the provider backbone, for instance, have hundreds of clients. De-
pending on the network conﬁguration and on the event that causes
the table transfer, a sender might need to send its table to more than
one single receiver which might overload the sender.
We observe table transfers in which RR transfers its table to two
neighbors and ﬁnd that the time to send a table to two BGP peers is
the same to send it to one. This observation indicates that the inter-
val between two interrupts is ﬁxed on a per-session basis. During
one gap, the BGP process stops updating one session but is free to
update the other session. We study this phenomenon in detail using
SBGP because we have it with and without gaps (fast SBGP).
We study the effect of increasing the number of sessions on both
normal and fast SBGP. We run SBGP and fast SBGP to emulate
each time an RR that sends the full table of 680K routes to its
peers. We carry several experiments in which we vary each time
the number of sessions from 1 to 20. We stop at 20 because the
Linux machines that run SBGP and fast SBGP cannot handle more
sessions.
Fig. 4 plots the table transfer time as a function of the number
of sessions. This ﬁgure shows that, until 10 sessions, the trans-
fer time using normal SBGP remains constant because, during the
gaps, normal SBGP has more sessions to handle and therefore it
is not stalled. Fast SBGP, however, increases with the number of
sessions. This ﬁgure shows that, the larger the number of sessions,
the closer the transfer time of SBGP and fast SBGP. We expect that
after a certain number of sessions, a router will take the same time
to send its table irrespective of gaps. We conjecture that the ben-
eﬁts from removing gaps are reduced when there are many BGP
sessions.
5.2 Reducing the table size
Another approach to decrease table transfer time is to reduce the
table size. We study the effects of reducing the table size on table
transfer times.
In current BGP implementation, the sender transfers a full rout-
ing table to the receiver. Sometimes, however, the receiver only in-
stalls a small fraction of these routes. In this case, sending a smaller
table (fewer routes) is a possible solution to make table transfers
faster. The distinction between the number of routes sent and the
number of routes the receiver needs (installs) is particularly strik-
ing in VPN backbones, because a PE only needs to install routes of
VPNs connected to it. For example, in the VPN provider backbone,
50% of PE routers need only between 3% and 6% of the total num-
ber of routes in the network. Sending fewer routes can be achieved
by using RT-constraints [11], which is an IETF proposal to reduce
the number of routes that a PE router receives during a table trans-
fer, thereby allowing to reduce table transfer times in VPNs.
We ﬁrst compare table transfer times when we send the full table
versus a ﬁltered table that contains the set of routes PE needs to in-
stall. We repeat the ‘full table’ experiments presented in Fig. 3 with
the ﬁltered table. We use the same PE router and setup as in ‘full
table experiments’. Similarly, we run each experiment three times
and present the mean. We also monitor PE using command-line in-
terface probes in order to get the CPU and the number of routes PE
installs. The ‘ﬁltered table’ curve in Fig. 3 presents transfer times
in these experiments. The variation among runs becomes higher
when PE is overloaded (more than 340,000 routes). This high vari-
ation explains the ﬂuctuations in the curve.
The comparison of the two curves in Fig. 3 shows that table
transfers are faster when we send the ‘ﬁltered table’. This is ex-
pected because PE receives a smaller table. The ﬁgure shows that
pre-ﬁltering routes before sending is particularly effective if PE in-
stalls only few routes. For instance, sending a ﬁltered table reduces
the table transfer from around 4 minutes to 35 seconds when PE in-
stalls only 34,000 routes. Although pre-ﬁltering represents a clear
improvement, the removal of gaps (with an event-driven implemen-
tation) achieves faster table transfers without the overhead of RT-
constraints-like mechanisms.