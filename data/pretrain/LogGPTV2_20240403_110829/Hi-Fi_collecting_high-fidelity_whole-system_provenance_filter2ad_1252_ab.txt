structure, and explicit information ﬂows that may include network
communication. These features can also be reconstructed for any
given point in the past.
3.3 Provenance Handler
The responsibility of the provenance handler is to interpret, pro-
cess, and store the provenance data after it is collected, and it should
be ﬂexible enough to support different needs. Consider the follow-
ing examples. Alice, the website administrator we met earlier, has
a dedicated provenance storage server with a huge disk. She does
not want to do any extra processing or storage on her already over-
loaded web server; she just wants to move the provenance data over
the LAN to her storage server as quickly as possible. Bob, on the
other hand, is a provenance-curious researcher who would like to
gather data from a number of volunteers. He would like the data
formatted according to the Open Provenance Model [14] and up-
loaded to his web server in XML format. Alice and Bob have very
different processing and storage needs for their provenance data.
With an existing provenance system, their data would be stored in
a database on disk before they could choose how to handle it.
Hi-Fi does not impose such limitations. Instead, we decouple
the provenance handler from the collection process, allowing the
system administrator to implement the handler according to the
needs of the system.
In our example, Alice can create a simple
Bash script which pipes provenance data through ssh directly to
her storage server. Bob is free to create a more complex handler
which reads the log, uses a Java library from the OPM website to
build the model and convert it to XML, and executes an HTTPS
request to submit the document to his online database. He can then
distribute this program to his volunteers.
261An added beneﬁt of this design is that it keeps complex algo-
rithms out of the collector. Existing systems have devoted consid-
erable effort to dealing with problems in provenance representation,
such as compact storage or graph cycles [16]. Our design simply
allows the handler to address these problems in whatever way is
most appropriate.
4. SYSTEM-LEVEL OBJECT MODEL
Collecting system-level provenance requires a clear model of
system-level objects. For each object, we must ﬁrst describe how
data ﬂows into, out of, and through it. Next, we identify the LSM
hooks (listed in Table 1) which mediate data-manipulating oper-
ations on that object, or we place new hooks if the existing ones
are insufﬁcient. Finally, we decide how the relevant objects can be
uniquely identiﬁed in the provenance log.
Each entry in the provenance log describes a single action on
a kernel object. This includes the type of action, the subject, the
object, and any appropriate context. For example, starting a kernel
build could generate the following entry:
Type
Subject
Object
Arguments
Environment
Execute
Credential 508
Root ﬁlesystem, inode 982
“make”, “–j8”, “bzImage”
“HOME=/home/alice”,
“PATH=/usr/bin:/bin”,
“SHELL=/bin/bash”, . . .
For the purposes of recording provenance, each object which can
appear in the log must be assigned an identiﬁer which is unique for
the lifetime of that object. Some objects, such as inodes, are already
assigned a suitable identiﬁer by the kernel. Others, such as sockets,
require special treatment. For the rest, we generate a “provid,” a
small integer which is reserved for the object until it is destroyed.
These provids are managed in the same way as process identiﬁers
to ensure that two objects cannot simultaneously have the same
provid. When an object which needs an identiﬁer is created, we
allocate a provid and attach it using the opaque security pointer
provided by LSM. When the object is freed, we release the provid
to be used again.
In later sections, we will show log entries in an abbreviated,
human-readable form, with inode numbers resolved to ﬁlenames,
and forks implied by a change in the bracketed provid:
[508] exec rootfs:/usr/bin/make -j8 bzImage
4.1 System, Processes, and Threads
Our model of data ﬂow includes transferring data between mul-
tiple systems or multiple boots of a system. We therefore need to
identify each boot separately. To ensure that these identiﬁers do not
collide, we create a random UUID at boot time. We then write it
to the provenance log so that subsequent events can be associated
with the system on which they occur.
Within a Linux system, the only actors are processes1 and the
kernel. These actors store and manipulate data in their respective
address spaces, and we treat them as black boxes for the purpose of
provenance collection. Most data ﬂows between processes use one
of the objects described in subsequent sections. However, several
actions are speciﬁc to processes: forking, program execution, and
changing subjective credentials.
1On Linux, threads are a special case of processes, so we will use
the term “process” to refer collectively to both.
Since LSM is designed to include kernel actions, it does not rep-
resent actors using a PID or task_struct structure. Instead, LSM
hooks receive a cred structure, which holds the user and group
credentials associated with a process or kernel action. Whenever a
process is forked or new credentials are applied, a new credential
structure is created, allowing us to use these structures to represent
individual system actors. As there is no identiﬁer associated with
these cred structures, we generate a provid to identify them.
4.2 Files and Filesystems
Regular ﬁles are the simplest and most common means of storing
data and sharing it between processes. Data enters a ﬁle when a
process writes to it, and a copy of this data leaves the ﬁle when
a process reads from it. Both reads and writes are mediated by
a single LSM hook, which identiﬁes the the actor, the open ﬁle
descriptor, and whether the action is a read or a write. Logging ﬁle
operations is then straightforward.
Choosing identiﬁers for ﬁles, on the other hand, requires some
thought. We must consider that ﬁles differ from other system ob-
jects in that they are persistent, not only across reboots of a single
system, but also across systems (like a ﬁle on a portable USB drive).
Because of this, it must be possible to uniquely identify a ﬁle in-
dependent of any running system. In this case, we can make use
of identiﬁers which already exist rather than generate new ones.
Each ﬁle has an inode number which is unique within its ﬁlesys-
tem. If we combine this with a UUID that identiﬁes the ﬁlesystem
itself, we obtain a suitable identiﬁer that will not change for the
lifetime of the ﬁle. UUIDs are generated for most ﬁlesystems at
creation, and we generate random UUIDs for the Linux kernel’s
internal pseudo-ﬁlesystems when they are initialized. We can then
use the combination of UUID and inode number to identify the ﬁle
in all ﬁlesystem operations, as well as to identify a program ﬁle
when it is being executed.
4.3 Memory Mapping
Files can also be mapped into one or more processes’ address
spaces, where they are used directly through memory accesses.
This differs signiﬁcantly from normal reading and writing in that
the kernel does not mediate accesses once the mapping is estab-
lished. We can only record the mapping when it occurs, along with
the requested access mode (read, write, or both). Note that this does
not affect our notion of complete mediation if we conservatively
assume that ﬂows via memory-mapped ﬁles take place whenever
possible.
Shared memory segments are managed and interpreted in the
same way. POSIX shared memory is implemented using mem-
ory mapping, so it behaves as described above. XSI shared mem-
ory, though managed using different system calls and mediated by
a different LSM hook, also behaves the same way, so our model
treats them identically. In fact, since shared memory segments are
implemented as ﬁles in a temporary ﬁlesystem, their identiﬁers can
be chosen in the same way as ﬁle identiﬁers.
4.4 Pipes and Message Queues
The remaining objects have stream or message semantics, and
they are accessed sequentially. In these objects, data is stored in
a queue by the writer and retrieved by the reader. The simplest
such object is the pipe, or FIFO. Pipes have stream semantics and,
like ﬁles, they are accessed using the read and write system calls.
This interaction is illustrated in Figure 3a. Since a pipe can have
multiple writers or readers, we cannot represent it as a ﬂow di-
rectly from one process to another. Instead, we must split the ﬂow
into two parts, modeling the data queue as an independent ﬁle-like
262write
send
send
A
A
A
(a) Pipe
(b) Message queue
host X
host Y
(c) Stream socket
send
A
read
recv
recv
recv
B
B
B
B
host X
host Y
(d) Message-oriented socket
Figure 3: Models of data ﬂow
object. In this way, a pipe behaves like a sequentially-accessed reg-
ular ﬁle.
In fact, since named pipes are inodes within a regular
ﬁlesystem, and unnamed pipes are inodes in the kernel’s “pipefs”
pseudo-ﬁlesystem, we can choose pipe identiﬁers exactly as we do
for ﬁles.
Message queues are similar to pipes, with two major semantic
differences: the data is organized into discrete messages instead of
a single stream, and these messages can be delivered in a different
order than that in which they are sent. Fortunately, LSM handles
messages individually, so we can create a unique identiﬁer for each.
We can then reliably tell which process receives it, regardless of
the order in which the messages are dequeued. Since individual
messages have no natural identiﬁer, we generate a provid for each.
4.5 Sockets
Sockets are the most complex form of inter-process communica-
tion handled by our system, but they can be modeled very simply.
As with pipes, we treat a socket’s receive queue as an intermediary
ﬁle between the sender and receiver, as shown in Figure 3. Send-
ing data, then, is just writing to this queue, and receiving data is
reading from it. The details of network transfer are hidden by the
socket abstraction, so we only need to consider the semantic differ-
ences between socket types.
Stream sockets provide the simplest semantics with respect to
data ﬂow: they behave identically to pipes. Since stream sockets
are necessarily connection-mode, all of the data sent over a stream
socket will arrive in the same receive queue.
If we assign one
identiﬁer to each socket endpoint, we can use these identiﬁers for
the lifetime of the socket. Message-oriented sockets, on the other
hand, do not necessarily have the same guarantees. They may be
connection-mode or connectionless, reliable or unreliable, ordered
or unordered. We only know that any messages which are deliv-
ered are delivered intact. Each packet therefore needs a separate
identiﬁer, since we cannot be sure at what endpoint it will arrive.
In determining how identiﬁers are chosen, we must reason care-
fully about socket behavior. We should never reuse an identiﬁer,
since a datagram can have an arbitrarily long lifetime. We also
want the identiﬁer to be associated with the originating host. The
per-boot UUID described in Section 4.1 addresses both of these re-
quirements. By combining this UUID with an atomic counter, we
can generate unique identiﬁers for socket provenance. As long as
this counter is large enough to avoid rolling over, we can be rea-
sonably certain that socket identiﬁers will remain unique.
In order to generate useful log entries, we must consider the se-
quence of events for sending and receiving data. Suppose process
A on host X sends data which arrives in queue Q on host Y. Pro-
cess B on host Y then receives this data. In this case, the following
events should occur:
1. Process A passes data to the send function. X writes “A
sends to Q” to the log.
2. The data is encapsulated in a packet as deﬁned by the socket’s
protocol family.
3. The packet may be transmitted over a network.
4. The packet is either dropped, in which case no ﬂow takes
place, or it is delivered and saved to queue Q.
5. Process B is given some data from this queue as the output
from the recv function. Before returning control to B, Y
writes “B receives from Q” to the log.
Writing the “send” entry is the tricky step, because the sender needs
to know the identiﬁer of the remote receive queue. However, the
sender and receiver may not have any shared information which
can be used to agree on an identiﬁer, so the cleanest solution is
for the sender to choose an identiﬁer for the remote receive queue
and transmit it along with the ﬁrst data packet. (How this happens
depends on the socket’s protocol family.)
In this way, both the
sender and receiver have the data needed to write their log entries.
5.
IMPLEMENTATION DETAILS
In the course of creating Hi-Fi, we have overcome a variety of
implementation challenges. Several of our solutions, such as run-
ning a provenance-opaque process, are new to the literature. Oth-
ers, such as moving data efﬁciently from the kernel to userspace,
are new solutions to problems that existing provenance work has
solved in other ways.
5.1 Efﬁcient Data Transfer
Provenance collection has been noted to generate a large vol-
ume of data [2]. Because of this, we need an efﬁcient and reli-
able mechanism for making large quantities of kernel data avail-
able to userspace. Other systems have accomplished this by using
an expanded printk buffer [19], writing directly to on-disk log
ﬁles [15], or using FUSE [21]. However, none of these methods is
appropriate for our system design. Instead, we use a Linux kernel
object known as a “relay,” which is designed speciﬁcally to address
this problem [27].
A relay is a kernel ring buffer made up of a set of preallocated
sub-buffers. Once the relay has been initialized, the collector writes
provenance data to it using the relay_write function. This data
will appear in userspace as a regular ﬁle, which can be read by the
provenance handler. Since the relay is backed by a buffer, it retains
provenance data even when the handler is not running, as is the case
during boot, or if the handler crashes and must be restarted.
Since the number and size of the sub-buffers in the relay are
speciﬁed when it is created, the relay has a ﬁxed size. Although
the collector can act accordingly if it is about to overwrite prove-
nance which has not yet been processed by the handler, it is better
263to avoid this situation altogether. To this end, we allow the relay’s
size parameters to be speciﬁed at boot time.
5.2 Early Boot Provenance
The Linux kernel’s boot-time initialization process consists of
setting up a number of subsystems in sequence. One of these sub-
systems is the VFS subsystem, which is responsible for manag-
ing ﬁlesystem operations and the kernel’s in-memory ﬁlesystem
caches. These caches are allocated as a part of VFS initialization.
They are then used to cache ﬁlesystem information from disk, as
well as to implement memory-backed “pseudo-ﬁlesystems” such
as those used for pipes, anonymous memory mappings, temporary
ﬁles, and relays.
The security subsystem, which loads and registers an LSM, is
another part of this start-up sequence. This subsystem is initialized
as early as possible, so that boot events are also subject to LSM me-
diation. In fact, the LSM is initialized before the VFS, which has a
peculiar consequence for the relay we use to implement the prove-
nance log. Since ﬁlesystem caches have not yet been allocated, we
cannot create a relay when the LSM is initialized. Our design goal
of ﬁdelity makes this a problem unique to our system: not only are
we forced to postpone relay setup, but we must also do so without
losing boot provenance data.
We therefore separate relay creation from the rest of the module’s
initialization and register it as a callback in the kernel’s generic
“initcall” system. This allows it to be delayed until after the core
subsystems such as VFS have been initialized. In the meantime,
provenance data is stored in a small temporary buffer. Inspection