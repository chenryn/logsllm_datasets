The adversary may mount privilege escalation attacks and can com-
promise kernels or perform arbitrary privileged operations. The
attacker‚Äôs primary purpose of intrusion is to install malware such
as ransomware or wiper malware that degrades the availability of
user data aiming the financial gain. We assume that the victim‚Äôs
VM is installed with anti-malware software that can notify any
suspicious activity before the attacker taking it down. Although
the attacker can be privileged, he or she cannot escape the VM.
That is, we trust cloudlets, and the attacker cannot compromise the
underlying hypervisor or any other cloudlet infrastructure outside
the VM. The connector-cloudlet will not be compromised as it does
not expose any interface for attackers inside the VM to exploit.
Failure Model. In this paper, we assume that cloudlets may suffer
from benign hardware and software failures that are not adversarial
but harmful enough to prevent us from reading or writing disk
blocks. For example, hard disks of cloudlets may fail and, therefore,
servers may crash. The network can also fail due to Ethernet cable
issues or WiFi adapter failures. In addition, software bugs can also
cause catastrophic failures and crash servers of cloudlets. Thus, we
suppose that failed servers may not be recoverable. For example,
a hard disk can be worn out and cannot be booted from. More
Figure 2: Tampering Attack Problem. The blue bar indicates
the writes made while there is no attacker in the system. The
orange bar represents the writes made during the stealth
phase of malware. The red bar shows malware‚Äôs tampering
writes.
crucially, cloudlets may suffer from irrecoverable service outage
due to disasters such as fire or flood. Because cloudlets are located
more closely to each other than cloud data centers are, we think that
it will be more likely that a natural disaster can break down multiple
cloudlets at the same time. Thus, we assume multiple cloudlets may
fail simultaneously. Nevertheless, we expect that there will be at
least one correct cloudlet. For example, we suppose at least one
cloudlet is located somewhere the natural disaster did not affect.
We note that Byzantine faults of cloudlets are out of scope.
2.2 Challenges
Each write is applied to a block device by making a corresponding
mutational change to update a data block. For synchronization,
each replicated block device must consistently apply the same set of
writes in the same order. That means there exists a specific sequence
of writes (w0, w1, . . . , wn‚àí1) and the state of a block device (Sk, for
some k ‚àà Z) sequentially changes as those writes in the sequence
are applied.
w0‚àí‚àí‚Üí S1
w1‚àí‚àí‚Üí . . .
S0
wn‚àí1‚àí‚àí‚àí‚àí‚Üí Sn
Tamper Resistance. Data tampering attacks having ransomware
or wiper malware generate a series of destructive writes. Malware‚Äôs
activity is divided into two phases: (1) stealth phase and (2) attack
phase. Suppose the malware starts its attack phase to tamper user
data after the last benign write wk‚àí1. Then, the block device is
tampered with wk , . . . , wn‚àí1. We can recover the block device to
the state before tampering begins by not applying wk , . . . , wn‚àí1 but
applying w0, . . . , wk‚àí1. Figure 2 illustrates this, aiming to protect
user data by excluding all those tampering writes issued during
the attack phase. Pinpointing the point when the first tampering
attack began is out-of-scope, and we rely on anti-malware software
to find out that point.
Without an append-only immutable mutation history kept se-
curely, naively replicating and applying writes to backups cannot
defend user data against malware. Accordingly, cloud-based file syn-
chronization solutions will replicate tampering writes to each file
out-of-order, and, therefore, those solutions cannot appropriately
restore the coherent block device state that is not tampered with. In
addition, equipping cloudlets with special hardware, such as TEE
supports, a specially customized SSD, or a fancy self-encrypting
drive, can increase the cost unnecessarily. Rocky is designed to
Connector-CloudletVMRDPRockyRockyRockyRocky CloudletLive MigrationTravelingStealthAttackIntrusionTamperingProtectionRollbackTime Flow287ACSAC ‚Äô21, December 6‚Äì10, 2021, Virtual Event, USA
Beom Heyn Kim and Hyoungshick Kim
Figure 3: Coherence Problem. Failures can lead to writing
loss (w3), which can make the block devices incoherent.
Therefore, to recover coherent block devices, we need to
remove recent writes such as w4 and apply the contiguous
write sequence, i.e., w1, w2.
defend against a tampering attack by keeping the append-only im-
mutable mutation history across cloudlets without using special
hardware.
Failure Resistance. When a block device‚Äôs state has been changed
by a contiguous write sequence, we say the block device is coherent.
However, if we apply a write sequence that may include some prefix
that is not contiguous, we cannot assure that the block device is
coherent. Maintaining a coherent block device is one of the most
critical properties that block device abstraction should ensure. For
example, when a file system is updated by creating a file in some
directory, writing the content to the file occurs first, and then writ-
ing to the directory occurs next in order to keep the file system
consistent even if a failure unexpectedly occurs. If we applied the
latter write to the block device but not the former write, then we
have a block device that is not coherent.
Although replicating the write sequence to block devices guar-
antees that replicated block devices are synchronized at a coherent
state, it is challenging to know every write sequence in advance
that is going to be generated by applications. Therefore, most exist-
ing solutions let applications generate block I/O and apply them
to one of the replicated block devices first, then asynchronously
replicate the write that happened on one block device to another
block device to let other devices apply the identical write sequences.
Similarly, Rocky‚Äôs approach is replicating the sequence of writes
from a Rocky block device on which VM runs to other Rocky block
devices via a connector-cloudlet.
However, if failures occur, a coherence problem can occur as
the example illustrated in Figure 3. In this example, Rocky on the
cloudlet A wrote w1, w2, w3 which were replicated to the connector-
cloudlet on time. However, if cloudlet A crashed due to hard disk
failure and the connector-cloudlet became unavailable due to a
service outage. Meanwhile, the user traveled and used the cloudlet
B performing w4. Cloudlet B replicated w1, w2, but failed to replicate
w3 due to the connector-cloudlet‚Äôs service outage. Cloudlet C has
been even slower and replicated only w1. Here, cloudlet B does
not have a coherent block device because it does not apply w3 but
apply w4 to its block device. Meanwhile, cloudlet C is coherent but
stale, as it only applies w1. We can restore the latest coherent block
device by replaying w1, w2 but not w4.
Figure 4: Rocky Cloudlet Architecture. Each cloudlet sup-
porting EdgeVDI is installed with Rocky (components col-
ored in blue) and a VM can run on it. The connector-cloudlet
mediates replication between Rocky cloudlets.
Rocky is designed to provide a function for users to get the latest
coherent block device restored only with a given set of correct
Rocky block devices.
Note that the problem described in Figure 3 can occur even if
we say that w3 was not a benign write lost due to a failure but
a tampering write corrupting some data block. Essentially, both
tampering attacks and failures cause the situation where we need
to discard all subsequent writes after the write either tampered or
lost.
3 ROCKY ARCHITECTURE
3.1 Overview
Rocky is a replicated distributed block device to enhance EdgeVDI‚Äôs
security and reliability against malware attacks inside a VM and var-
ious failures of cloudlet infrastructures. To that end, Rocky enables
reconstructing a coherent block device that is not tampered.
Rocky Cloudlet. Each Rocky cloudlet maintains Rocky endpoints.
Each Rocky endpoint consists of a Rocky block device (RDB), a Rocky
controller (RC) and a Rocky storage (RS) as illustrated in Figure 4.
Each Rocky endpoint of the Rocky cloudlet stores the replica of a
VM image containing a user‚Äôs desktop environment. An ensemble
of Rocky endpoints keep those replicas across cloudlets in sync via
Rocky‚Äôs replication protocol. There exists only one Rocky endpoint
for an ensemble that can run the user‚Äôs VM. Other endpoints in the
ensemble are periodically replicating new disk writes generated by
the VM.
I/O Handling. RDB exports a device file for a VM to run on top by
using it as a passthrough device. The VM sends block I/O requests
to RDB. Then, RDB redirects it to RC. Each block I/O request is
made to either read or write fixed-size disk blocks. Each disk block
is associated with a unique integer type identifier (block ID). Thus,
each block I/O request must specify which disk block it wants to
read or write. With the block ID requested, RC looks up meta-data
indicating whether the blocks are located locally or remotely. Then,
RC forwards requests to either RS or the connector-cloudlet. RS or
the connector-cloudlet handles requests and returns a response. RC
returns the response to RDB which in turn relays it to the VM.
Cloudlet ACloudlet BConnector-CloudletùîÄ1ùîÄ2ùîÄ1ùîÄ2ùîÄ3ùîÄ3ùîÄ1ùîÄ2ùîÄ4Cloudlet CùîÄ1CoherentIncoherentTime FlowVirtual MachineRocky Block DeviceRocky ControllerConnector-Cloudlet Rocky StorageKernelUserOther CloudletsOther CloudletsRocky CloudletsRocky CloudletRocky Endpoint288Rocky: Replicating Block Devices for Tamper and Failure Resistant Edge-based Virtualized Desktop Infrastructure
ACSAC ‚Äô21, December 6‚Äì10, 2021, Virtual Event, USA
the freshness of blocks in Rocky storage. If a n-th bit of the presence
bitmap is set to 1, then it means the Rocky endpoint has the up-
to-date n-th block in Rocky storage. However, if the bit is 0, then
it means the copy of the block in Rocky storage is stale. Thus, the
presence bitmap directs whether the requested block is stored in its
local Rocky storage or should be fetched from cloud storage. The
dirty bitmap is mainly used by the owner to record which blocks
have been written (‚Äúdirtied‚Äù) in Rocky storage but has not yet been
replicated to cloud storage. Similar to the presence bitmap, if the
n-th bit of the dirty bitmap is set to 1, then it means the n-th block
has been written and therefore is a dirty block.
RC manages its presence bitmap and dirty bitmap to read the
latest blocks and replicates writes to other Rocky endpoints. When
RC runs on the owner, it accordingly sets or resets presence and
dirty bitmap for each incoming block I/O. For an incoming read
request, RC looks up the presence bitmap to determine if it can
serve a block from Rocky storage. If so, RC reads the block from
RBS. Otherwise, RC should get the block from cloud storage. After
getting the block from cloud storage, RC stores the block in RBS.
Then, RC sets the corresponding bit in the presence bitmap to 1. For
an incoming write request, RC directly writes to RBS after setting
the corresponding bit in the dirty bitmap. RC does not read a block
for a write request because the block will be overwritten by the
write operation. RC sends the dirty bitmap along with writes to
the cloud storage to notify other non-owner endpoints about new
writes. Then, the corresponding indexes of the dirty bitmap are reset
after flushing those updates to cloud storage. Meanwhile, when
RC runs on a non-owner, it resets bits in the presence bitmap as it
receives the meta-data about new writes from the cloud storage.
3.3 Periodic Mutation Snapshot Update
To ensure a coherent block device, Rocky replicates a contiguous
write sequence across distributed Rocky endpoints. However, if we
replicate every write, resource consumption is wastefully increased
by unnecessarily sending blocks that are going to be overwritten by
new writes. To solve this issue, Rocky hoards writes and sends only
the latest version. More specifically, Rocky periodically updates
cloud storage with the snapshot of dirty blocks at the end of each
period. Rocky allows the owner to adjust the period of update,
called epoch. Therefore, Rocky sequentially sends each fragment of
a write sequence at the end of each epoch. For example, suppose
we have a long sequence of writes, W :
w11, w12, . . . , w1n, w21, w22, . . . , w2m
With this, the state of the block device mutates from its initial state
S0 to Sn+m:
w11‚àí‚àí‚àí‚Üí S1
w2m‚àí‚àí‚àí‚àí‚Üí Sn+m
w12‚àí‚àí‚àí‚Üí . . .
S0
w1n‚àí‚àí‚àí‚Üí Sn
w21‚àí‚àí‚àí‚Üí Sn+1
w22‚àí‚àí‚àí‚Üí . . .
, or simply
S0 W‚àí‚àí‚Üí Sn+m
Suppose W is fragmented into two epochs, which can be var-
ied depending on the configuration. The first epoch sends a write
sequence fragment, W1:
w11, w12, . . . , w1n
Figure 5: Rocky Storage and Connector-Cloudlet Architec-
ture. Meta-data and block snapshots are stored for tamper
and failure resistance. (RC which actually manages commu-
nication between cloudlets is not shown.)
Write Serialization. An ensemble of Rocky endpoints must collec-
tively provide a logically single coherent block device. To that end,
writes must be serialized. Rocky requires an endpoint to obtain an
exclusive ownership from the ensemble. The endpoint with the own-
ership is the only one that can run a VM and, therefore, can serialize
VM‚Äôs block writes into a consecutive, consistent, totally-ordered
write sequence that needs to be replicated to all other endpoints
to maintain coherence of a block device. As a user moves, we need
to migrate user‚Äôs VM across cloudlets. To transfer the ownership
for the VM between cloudlets, we devised the ownership-transfer
protocol, which we will explain in more detail below. We define
the Rocky endpoint with the exclusive ownership as owner and all
other endpoints as non-owner.
Replication Protocol. Rocky owner batches writes. Then, the
owner periodically flushes writes to the connector-cloudlet. The
connector-cloudlet stores the write sequence as the immutable
mutation history. From the connector-cloudlet, non-owners period-
ically retrieves new writes in the sequence they have not yet repli-
cated. Therefore, write sequence is asynchronously and periodically
streamed by the owner to non-owners via the connector-cloudlet.
In short, Rocky‚Äôs replication protocol is publish-subscribe pattern
where the owner publishes new writes, the connector-cloudlet
performs the broker role and non-owners subscribes to any new
writes published by the owner. Because the owner uploads and non-