Since user demand was the same on both links, congestion started
later, ended earlier, and was less severe on the majority-capped
link. The naïve estimators were unable to detect this because both
capped and uncapped traffic used the same congested link, and
therefore saw similar performance.
This becomes clearer if we take a closer look at how the aver-
age throughput of sessions changes in Figure 6b, which can be
contrasted with how the behavior during the baseline period in Fig-
ure 6a. We report the average of all client throughputs during each
hour, normalized by the largest hourly throughput. Throughput
slowly decreases as overall traffic increases throughout the day, and
7
86
IMC ’21, November 2–4, 2021, Virtual Event, USA
Spang et al.
Figure 5: Treatment effects with 95% confidence intervals in our bitrate capping experiments. Each row is a metric of interest,
with the naïve A/B Test estimates, and TTE and spillovers as estimated by the paired link experiment.
(a) Average throughput for the Saturday of the baseline test period.
(b) Average throughput for the Saturday of the main experiment.
Figure 6: Client-reported average throughput over time in the experiments, normalized to the largest hourly average. During
peak hours, the links become congested and throughput decreases. Capping the majority of traffic in (b) causes Link 1 to be
less congested and have higher throughput during most of the peak hours.
then suddenly drops when the link becomes congested during peak
hours. During the baseline period, there is no difference between
throughputs for the two links. During the main experiment, the
mostly capped link remains uncongested for longer during peak
hours, and has higher throughput before and after the most heavily
loaded hours. Despite this difference, the capped and uncapped
traffic on the same link have very similar performance.
Figure 7: Average values of throughput in the cells in this
experiment, with estimands of interest.
observed during each session. However, because bitrate capping
delayed the onset of congestion, the majority-capped link (link 1)
had empty queues for more time. The total treatment effect was
a 24% improvement in the minimum RTT for the bitrate-capped
sessions. The spillover was positive: capping traffic improved the
minimum RTT by 27% for uncapped traffic. Again this was incor-
rectly estimated by the naïve A/B tests which both reported a 5%
and 12% increase in minimum RTT.
8
87
In Figure 7, we show the four outcomes of throughput in the
experiment: for capped and uncapped traffic as a function of allo-
cation percentage. Both A/B tests confidently report that capped
traffic reduces throughput relative to uncapped traffic. However
by capping the majority of traffic, we improve throughput for all
traffic using the link. This leads to an improvement as measured by
TTE, and a positive spillover.
If we considered just one of the A/B tests in isolation, we would
falsely conclude that capping traffic makes throughput slightly
worse. This is our “smoking gun”—the confusion arises because
treatment and control interfere with each other via congestion on
the link.
We observed similar behavior for round-trip times in the experi-
ment, as shown in Figure 8. During congested hours, large queues
build up at the congested link, which causes all packets in a session
to be delayed, and leads to a sharp increase in the minimum RTT
Unbiased Experiments in Congested Networks
IMC ’21, November 2–4, 2021, Virtual Event, USA
Figure 8: Average of minimum RTT in each connection, nor-
malized to smallest cell value.
Figure 9: Capping bitrate generally reduced the fraction of
retransmitted bytes during congested hours, but caused an
increase in uncongested hours.
We saw similar effects in start play delay, which is the time it
takes a video to start playing. This is not surprising: improving
throughput and reducing queueing delay should cause videos to
load faster. Neither A/B test predicted a significant decrease in start
play delay, whereas there was actually a 10% improvement in total
treatment effect. The spillover was also positive: capping traffic
reduced play delay by 9% for both itself and for uncapped traffic.
We measured a 33% reduction in video bitrate, with positive
spillover. Capping the majority of traffic meant that the uncapped
traffic was able to take up more bandwidth and achieve higher
bitrates. It is surprising that despite the spillover, the two A/B
tests still give reasonably good estimates of TTE. We believe this
is because the majority of the reduction in bitrate comes from the
artifical cap, which is applied independently of how other traffic
behaves. The spillover is small relative to this effect, but might
explain the difference between the 95% treatment effect and TTE.
We observed the total treatment effect for capping was a 10%
increase in the fraction of sent bytes that were retransmitted. This
was driven by a 16% increase in the fraction of retransmitted bytes
during off-peak hours, and a 20% decrease during peak hours as
shown in Figure 9. This may seem surprising since bitrate capping
reduced congestion, but in fact retransmits did not get worse. Cap-
ping reduced the absolute number of bytes retransmitted during
both during peak and off-peak hours. The apparent increase in
the percentage was caused by the absolute number of sent bytes
decreasing more than the absolute number of retransmitted bytes.
Although odd, Netflix observed similar behavior in a number of
ISPs when removing bitrate capping.
Finally, we discuss the impact on rebuffers. Recall from Sec-
tion 4.2 that we observed a 20% difference in rebuffers between the
links from our baseline analysis prior to the experiment. Based on
our experiment, we believe bitrate capping had at least some impact
on rebuffers: we see a 15% decrease in rebuffers in the A/B tests
within each link. We also measured that rebuffers for the mostly
capped traffic in link 1 were 18% lower compared to the mostly
uncapped traffic in link 2.
Given that rebuffer rates were not identical pre-experiment, we
investigated further and measured rebuffer rates for both links
during the month after we ran the experiment. We consistently
found a difference: link 1 had on average 15% more rebuffers. In 70%
of all hours, and in all but one peak hour, link 1 had more rebuffers
than link 2. While we are not certain of the underlying reason
9
88
for the difference, we believe an 18% improvement is probably an
underestimate of the improvement of rebuffers. If we account for
the underlying difference between links 1 and 2, it is closer to a
20%-30% improvement (rather than 15% improvement from the
naïve estimate), suggesting congestion interference.
We conclude by highlighting one reason our results may under-
estimate the amount of congestion interference. As discussed in
Appendix B, A/B test analysis usually assumes that sessions from
different users are statistically independent of each other. By esti-
mating standard errors only on data aggregated to the hourly level,
our analysis effectively makes a nearly worst-case assumption that
sessions in the same hour are perfectly correlated. This dramatically
increases the size of the confidence intervals we report for TTE and
spillover.
5 UNBIASED EXPERIMENTS AT SCALE
We care about two different things when evaluating a new algo-
rithm: testing it safely and accurately measuring its performance.
We want to experiment safely: if a new algorithm works so poorly
that it could cause material harm to the service, we want to detect it
quickly and avoid deploying it widely. We also want to be accurate:
the goal of a new algorithm is usually to improve some metric, and
we need to accurately evaluate whether it succeeded.
A/B tests are used today with the assumption that they are both
safe and accurate. If the SUTVA assumption held, we can accu-
rately estimate performance by running an A/B test on a very small
fraction of users. This allows us to predict the performance of an
algorithm at scale, without broadly deploying a harmful algorithm.
But in the worst case, congestion interference means that an A/B
test is neither safe nor accurate. An algorithm which performs well
in an A/B test might cause significant harm when it is deployed
globally. But if an algorithm has marginal A/B test results and we
do not deploy it globally, we may miss out on extremely effective
algorithms.
This is a fundamental tradeoff with congestion interference, and
what makes it so difficult to work with in practice. If we want to get
a completely unbiased estimate of TTE, we need to allocate 100% of
traffic to a treatment. But for safety reasons we would never allocate
100% of traffic to an untested or poorly performing algorithm.
In this section, we provide some guidance on how to run exper-
iments in practice. We will not be able to completely resolve this
IMC ’21, November 2–4, 2021, Virtual Event, USA
Spang et al.
tradeoff, but we will describe two ways of measuring congestion
interference despite it.
Naïve A/B tests are biased in congested networks because of the
combination of the A/B experiment design itself, and the flawed
causal interference used when interpreting the results of that de-
sign. We will propose modifications to the A/B experiment design,
and describe the improved causal inference that these modifications
allow. First, we propose slightly modifying existing deployment
practices to look for congestion interference. This is easy to do and
helps build intuition around when congestion interference exists, at
the cost of time-related bias and rejecting effective algorithms. To
counter this, we also propose running small-scale, targeted switch-
back experiments to measure how a new algorithm behaves in a
specific network.
5.1 Measure deployed algorithms with event
studies
When deploying an algorithm, it is important to get an accurate
estimate of TTE. Optimistically, an algorithm might perform better
at scale than it did in small-scale evaluations. Perhaps when an
algorithm is run by a larger fraction of traffic, it even further reduces
congestion and improves performance than it did in small-scale
experiments. Accurately quantifying the improvement is important
to understanding its behavior and giving the team working on the
algorithm the credit they deserve.
Pessimistically, a new algorithm might perform worse at scale
than in small-scale evaluations. This might be a sign of some bug
or unexpected behavior in the algorithm, and might suggest it
increases congestion or interferes with other traffic on the internet.
These are things that are important to know about, so they can be
addressed.
Primarily for safety reasons, engineers have developed sophis-
ticated techniques for deploying new algorithms. Engineers grad-
ually deploy changes by slowly increasing the allocation fraction.
They continually monitor the system, and stop the deployment if
performance degrades.
While engineers typically use gradual deployments to safeguard
against failure, they could also be used to conveniently measure
the performance of a new algorithm and look for congestion inter-
ference. A gradual deployment is effectively a series of A/B tests
with treatment allocations ranging from 0% to 100%. At each alloca-
tion (𝑝1, 𝑝2, etc...) we can observe the outcomes for treatment and
control. This gives us points on the graph of Figure 1, and we can
use these values to estimate the average treatment effect 𝜏(𝑝𝑖), the
spillover 𝑠(𝑝𝑖), and a partial treatment effect 𝜌(𝑝𝑖) = 𝜇𝑇 (𝑝𝑖)−𝜇𝐶(0).
Once the deployment is finished, we can compare 100% allocation
to 0% allocation and estimate TTE. If there is no interference, for
all allocations 𝑖 and 𝑗, the average treatment effects are the same
𝜏(𝑝𝑖) = 𝜏(𝑝 𝑗), the partial treatment effects are the same as the
average treatment effects 𝜌(𝑝𝑖) = 𝜏(𝑝𝑖), and there is no spillover
𝑠(𝑝𝑖) = 0. We can use statistical tests to check each of these re-
lationships. If they do not hold, it could be a sign of congestion
interference.
This is a type of observational design called an event study or
an interrupted time series [54, Ch. 11]. In an event study, we in-
troduce some change, and compare the state of the system before
and after. This can be contrasted with a naïve A/B test, where we
simultaneously compare units with and without the change. In the
gradual deployment setting, the change is the increase of treatment
allocation from 𝑝𝑖 to 𝑝𝑖+1.
A major flaw with event studies is that it can be difficult to
attribute observed behavior to a particular change. This is especially
true because of seasonality: holidays, weekends, and political events
all tend to have different traffic patterns than other times. Other
teams or organizations regularly make changes and deploy software
which can affect similar metrics. In the bitrate capping example,
we had data from before and after deployment, but chose to run
a more controlled experiment to rule out the possibility of other
causes for the behavior we observed.
Another flaw is that this process works well for safely deploy-
ing new algorithms, but it is heavily biased towards rejecting new
algorithms. As an example, suppose we were testing a new algo-
rithm which behaved like the pacing lab experiment in Section 3.2.
In a small allocation A/B test, this algorithm would look worse:
throughput would be down and loss would be unaffected. Seeing
this, we might invest our time in other, more promising algorithms.
We could slightly increase the size of the allocation to look for inter-
ference, but throughput increased quite slowly with allocation size.
Even if we were able to detect this interference, it would look small.
At this point, we might stop the deployment before the algorithm
is able to clearly improve performance.
Despite these flaws, event studies are quick and easy ways to get
estimates of TTE and spillovers. Large organizations continually
deploy changes. When a deployment happens, it is easy to look
at the already-collected metrics and use these metrics to estimate
TTE and spillovers. Doing so will help build intuition around which