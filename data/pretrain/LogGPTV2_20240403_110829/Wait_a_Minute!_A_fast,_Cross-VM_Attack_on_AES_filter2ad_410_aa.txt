title:Wait a Minute! A fast, Cross-VM Attack on AES
author:Gorka Irazoqui Apecechea and
Mehmet Sinan Inci and
Thomas Eisenbarth and
Berk Sunar
Wait a Minute! A fast, Cross-VM Attack on AES
Gorka Irazoqui, Mehmet Sinan Inci, Thomas Eisenbarth, and Berk Sunar
Worcester Polytechnic Institute, Worcester, MA, USA
{girazoki,msinci,teisenbarth,sunar}@wpi.edu
Abstract. In cloud computing, eﬃciencies are reaped by resource shar-
ing such as co-location of computation and deduplication of data. This
work exploits resource sharing in virtualization software to build a pow-
erful cache-based attack on AES. We demonstrate the vulnerability by
mounting Cross-VM Flush+Reload cache attacks in VMware VMs to
recover the keys of an AES implementation of OpenSSL 1.0.1 running
inside the victim VM. Furthermore, the attack works in a realistic set-
ting where diﬀerent VMs are located on separate cores. The modiﬁed
ﬂush+reload attack we present, takes only in the order of seconds to min-
utes to succeed in a cross-VM setting. Therefore long term co-location, as
required by other ﬁne grain attacks in the literature, are not needed. The
results of this study show that there is a great security risk to OpenSSL
AES implementation running on VMware cloud services when the dedu-
plication is not disabled.
Keywords: Cross-VM, memory deduplication, ﬂush+reload, cache
attacks.
1
Introduction
In recent years we witnessed mass adoption of cloud based storage and compute
systems such as Dropbox, Amazon EC2 and Microsoft Azure. Rather than ac-
quiring and maintaining expensive workstations, clusters or servers, businesses
can simply rent them from cloud service providers at the time of need. However,
as with any new technology, cloud systems also come with problems of their
own, namely co-residency data leakage problems. The data leakage problem is
an indirect outcome of cloud’s temperament. By deﬁnition a cloud system allows
multiple users to share the same physical machine rather than assigning a dedi-
cated machine to every user. Co-residency keeps the number of physical machines
needed and the operating costs such as maintenance, electricity and cooling low
but at a price. In cloud systems, diﬀerent users run their virtual machines (VM)
on the same physical machine separated only by a virtualization layer provided
by a virtual machine manager (VMM) and supervised by a hypervisor. In theory
sandboxing enforced by the VMM should suﬃce to completely isolate VMs from
each other, but as elegantly stated many times: “In theory there is no diﬀerence
between theory and practice. But in practice, there is.”
A serious security problem that threatens VM isolation, stems from the fact
that people are using software libraries that are designed to run on single-user
A. Stavrou et al. (Eds.): RAID 2014, LNCS 8688, pp. 299–319, 2014.
c(cid:2) Springer International Publishing Switzerland 2014
300
G. Irazoqui et al.
servers and not on shared cloud hardwares and VM stacks. For privacy critical
data, especially cryptographic data, this gives rise to a blind spot where things
may go wrong. Even though classical implementation attacks targeting cryp-
tosystems featuring RSA and AES have been studied extensively, so far there
has been little discussion about safe implementation of cryptosystems on cloud
systems. For instance, implementation attacks on AES implementations, as pro-
posed by Bernstein [8] and later [9,13], use the timing diﬀerence of cache accesses
to recover the secret key. A more recent study by Gullasch et.al [13] applies
Flush+Reload attack between AES memory accesses. The attack recovers the
key with as less as 100 encryptions. Even though these aforementioned methods
have been implemented and the vulnerabilities are public, most cryptographic
libraries still use vulnerable and unpatched implementations. Considering the
level of access an adversary will have on a virtual machine, any of these attacks
and many novel attacks can and will be realized on the cloud.
Another feature that can break process isolation or VM isolation is dedupli-
cation. Its exploitability has been shown in several studies. In 2011, Suzaki et
al. [24] exploited an OS-optimization, namely Kernel Samepage Merging (KSM),
to recover user data and subsequently identify a user from a co-located VM in a
Linux Kernel-based Virtual Machine (KVM) [2] setting. In this study, authors
were able to exploit the side-channel leakage to establish a covert communication
channel between VMs and used this channel to detect co-residency with a target
VM. Also in 2011 Suzaki et al. [23] exploited the same memory deduplication
feature to detect processes like sshd, apache2, IE6 and Firefox running on
co-resident VM. The signiﬁcance of this study is that not only it is possible to
exploit the memory deduplication to detect the existence of a VM, but one can
also detect the processes running on the target VM. This leads to cipher speciﬁc
attacks and information thefts, as demonstrated by Suzaki et al. in [22]. In this
latest study, the authors were able to detect security precautions such as anti-
virus software running on the co-resident target VM. Even though these studies
paved the way for cross-VM process detection and shed light on vulnerabilities
enabled by memory deduplication, a concrete attack recovering cryptographic
keys has yet to be shown.
In [31] Weiß et al. for the ﬁrst time presented a traditional cache timing attack
on AES running inside a L4Re VM on an ARM Cortex-A8 single-core CPU with
a Fiasco.OC microkernel. The attack is realized using Bernstein’s correlation
attack and targets several popular AES implementations including the one in
OpenSSL [26]. The signiﬁcance of this work is that it showed the possibility of
extracting even ﬁner grain information (AES vs. ElGamal keys in [34]) from a
co-located VM. Recently, Irazoqui et al. [15] used Bernstein’s attack to partially
recover an AES key from a cross-VM attack running in XEN and VMware.
While that work is the ﬁrst one to show that ﬁne-grain side-channel attacks can
be mounted in cloud-like environments, the present attack is more eﬃcient since
it needs much less encryptions.
Wait a Minute! A fast, Cross-VM Attack on AES
301
Our Contribution
In this work, we show a novel cache-based side-channel attack on AES that—by
employing the Flush+Reload technique—enables, for the ﬁrst time, a practical
full key recovery attack across virtual machine boundaries in a realistic cloud-like
server setting. The attack takes advantage of deduplication mechanism called the
Transparent Page Sharing which is employed by VMware virtualization engine
and is the focus of this work. The attack works well across cores, i.e. it works
well in a high-end server with multiple cores scenario that is commonly found in
cloud systems. The attack is, compared to [13], minimally invasive, signiﬁcantly
reducing requirements on the adversary: memory accesses are minimal and the
accesses do not need to interrupt the victim process’ execution. This also means
that the attack is hardly detectable by the victim. Last but not least, the attack
is lightning fast: we show that, when running in a realistic scenario where an
encryption server is attacked, the whole key is recovered in less than 10 seconds
in non-virtualized setting (i.e. using a spy process) even across cores, and in less
than a minute in virtualized setting across VM boundaries.
In summary, this work
– shows for the ﬁrst time that deduplication enables ﬁne grain cross-VM at-
tacks;
– introduces a new Flush+Reload -based attack that does not require inter-
rupting the victim after each encryption round;
– presents the ﬁrst practical cross-VM attack on AES; the attack is generic
and can be adapted to any table-based block ciphers.
Since the presented attack is minimally invasive, it is very hard to detect. Finally,
we also show that these attacks can be prevented without too much overhead.
After reviewing additional related work in Section 2 we detail on existing
cache-based side-channel attacks in Section 3 and on memory deduplication in
Section 4. The proposed attack is introduced in Section 5. Results are presented
in Section 6. Before concluding in Section 8 we discuss possible countermeasures
in Section 7.
2 Related Work
The ﬁrst consideration of cache memory as a covert channel to extract sensitive
information was mentioned by Hu [14]. Later in 1998 Kesley et al. [16] mentioned
the possiblity of applying the cache as a resource to perform attacks based on
cache hit ratio. One theoretical example of cache attacks was studied later in 2002
by Page [20]. One year later, Tsunoo et al. [27] investigated timing side channel
attacks due to internal table look up operations in the cipher that aﬀect the
cache behavior. Over the last decade, a great number of research has been done
in the ﬁeld of cache-based side-channel attacks. One of the studies is the time
driven attack that was done by Bernstein when he observed that non-constant
time implementations of cryptographic algorithms leak sensitive information in
302
G. Irazoqui et al.
terms of time which can be used to extract the secret key [8]. His target was
the OpenSSL implementation of the cryptographic algorithm AES. Neve further
analyzes Bernstein’s attack and the causes for observed timing variations in his
PhD thesis [17]. Bonneau and Mironov’s study [9] shows how to exploit cache
collisions in AES as a source for time leakage.
Trace driven attacks were investigated by Osvik et al. [19] where they tried the
prime and probe attack on AES. In the aforementioned study, a spy process ﬁlls the
cache with attacker’s own data and then waits for the victim to run the encryp-
tion. When the encryption is ﬁnished, the attacker tries to access her own data
and measures the access time to see which cache lines have been evicted from the
cache. Then, comparing the access times with the reference ones, attacker discov-
ers which cache lines were used. In the same study, authors also analyze evict+time
method that consists of triggering two encryptions of the same plaintext and ac-
cessing some cache lines after the ﬁrst encryption to see which lines are again
loaded by the second encryption. In the same line, Acıi¸cmez and Ko¸c [5] inves-
tigated a collision timing attack in the ﬁrst and the second round of AES. Also,
in another study done by Gullasch et al. [13] ﬂush+reload is used to attack AES
encryption by blocking the execution of AES after each memory access.
Even though AES is a popular target for side-channel cache attacks, it is not
the only target. Acıi¸cmez in [4] was the ﬁrst one discovering that the instruction
cache as well as the data cache leaked information when performing RSA encryp-
tion. Brumley and Boneh performed a practical attack against RSA in [10]. Later
Chen et al. developed the trace driven instruction cache attacks on RSA. Finally
Yarom et al. were the ﬁrst ones proposing a ﬂush+reload attack on RSA using
the instruction cache [33]. Finally, again Yarom et al. used the Flush+Reload
technique to recover the secret key from a ECDSA signature algorithm [32].
In a cloud environment, several studies have been conducted with the aim of
breaking the isolation between co-located VMs to perform side-channel attacks.
In 2009, Ristenpart et al. [21] demonstrated that it is possible to solve the co-
location problem in the cloud environment and extract sensitive data from a
targeted VM. In the study, Amazon’s EC2 servers were targeted and using their
IP addresses provided by Amazon, VMs were mapped to various types of cloud
instances. Using a large set of IP-instance type matches and some network delay
timing measurements, they were able to identify where a particular target VM is
likely to reside, and then instantiate new VMs until one becomes co-resident with
the target VM. Along with the placement information, they exploited Amazon
EC2’s sequential placement policy and were able to co-locate two VMs on a
single physical machine with 8% probability. Even further, the authors show
how cache contention between co-located Xen VMs may be exploited to deduce
keystrokes with high success probability. By solving the co-location problem,
this initial result fueled further research in Cross-VM side-channel attacks.
After solving the co-location problem, stealing ﬁne grain secret information
from a target turns into an ordinary side-channel cache attack. In 2012, Zhang
et al. [34] presented an access-driven side-channel attack implemented across
Xen VMs that manages to extract ﬁne-grain information from a victim VM.
Wait a Minute! A fast, Cross-VM Attack on AES
303
In the study, authors managed to recover an ElGamal decryption key from a
victim VM using a cache timing attack. The signiﬁcance of this work, is that
for the ﬁrst time the authors were able to extract ﬁne grain information across
VMs—in contrast to the earlier work of Ristenpart et al. [21] who managed to
extract keystroke patterns. Later, Yarom et al. in [33] suggested that their attack
could be used in a virtualized environment but they never tried it in a real cloud
environment. Again, for the AES case, Weiss et al. used Bernstein’s attack on
an ARM system in a virtualized environment to extract information about AES
encryption keys [31].
Finally in 2014 Irazoqui et al. [15] implemented Bernstein’s attack for the ﬁrst
time in a virtualized environment where Xen and VMware VMMs with cross-
VM setting were used. In the study, authors were able to recover AES secret
key from co-resident VM running AES encryption using the timing diﬀerence
between cache line accesses. The downside of the attack was that average of 229
encryption samples were needed for the attack to work which takes about 4-5
hours on a modern Core i5 platform.
3 Cache-Based Side-Channel Attacks
In this work we demonstrate a ﬁne-grain cross-VM attack that one might use
in the real world. We not only want the attack to allow us to recover ﬁne-grain
information, but also work in a reasonable amount of time, with assumptions one
can fulﬁll rather easily on cloud systems. Since Bernstein’s attack [8] numerous
trace-driven, access-driven and time-driven attacks have been introduced mainly
targeting AES implementations. We will employ a new variant: the ﬂush and
reload attack on AES. In what follows we explain the basics of cache side-
channel attacks, and brieﬂy review the many cache side-channel attacks that
have been used to attack AES.
Cache Architecture. The cache architecture consists of a hierarchy of memory
components located between the CPU cores and the RAM. The purpose of the
cache is to reduce the average access time to the main memory by exploiting
locality principles. When the CPU needs to fetch data from memory, it queries
the cache memory ﬁrst to check if the data is in the cache. If it is, then it can
be accessed with much smaller delay and in this case it is said that a cache hit
has occurred. When the data is not present in the cache, it needs to be fetched
from a higher-level cache or even from main memory. This results in greater
delays. This case is referred to as a cache miss. When a cache miss occurs, the
CPU retrieves the data from the memory and a copy is stored in the cache.
The CPU loads bigger blocks of data, including data in nearby locations, to
take advantage of spatial locality. Loading the whole block of data improves the
execution performance because values stored in nearby locations to the originally
accessed data are likely to be accessed.
The cache is organized into ﬁxed sized cache lines, e.g of l bytes each. A cache
line represents the partitions of the data that can be retrieved or written at
a time when accessing the cache. When an entry of a table stored in memory
304
G. Irazoqui et al.
is accessed for the ﬁrst time, the memory line containing the retrieved data
is loaded into the cache. If the process tries to access to the same data from
the same memory line again, the access time will be signiﬁcantly lower, i.e. a
cache hit occurs. Therefore—for a cryptographic process—the encryption time
depends directly on the accessed table positions, which in turn depend on the
secret internal state of the cipher. This timing information can be exploited
to gain information about the secret key that is being used in the encryption.
Also, in case that there are no empty (invalid) cache lines available, one of the
data bearing lines gets reallocated to open up space for the the incoming line.
Therefore, cache lines that are not recently accessed are evicted from cache.
Exploiting Cache Timing Information. Up until this point, we established
that a cache miss takes more time to be processed than a cache hit. Using the
resulting state-dependent timing information, an attacker can obtain sensitive in-
formation from an encryption process and use this information to recover infor-
mation about the secret key, eventually resulting in a full key recovery. The run-