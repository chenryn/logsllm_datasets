title:Confused, timid, and unstable: picking a video streaming rate is hard
author:Te-Yuan Huang and
Nikhil Handigol and
Brandon Heller and
Nick McKeown and
Ramesh Johari
Confused, Timid, and Unstable:
Picking a Video Streaming Rate is Hard
Te-Yuan Huang Nikhil Handigol Brandon Heller Nick McKeown Ramesh Johari
{huangty,nikhilh,brandonh,nickm,ramesh.johari}@stanford.edu
Stanford University
ABSTRACT
Today’s commercial video streaming services use dynamic
rate selection to provide a high-quality user experience. Most
services host content on standard HTTP servers in CDNs,
so rate selection must occur at the client. We measure three
popular video streaming services – Hulu, Netﬂix, and Vudu
– and ﬁnd that accurate client-side bandwidth estimation
above the HTTP layer is hard. As a result, rate selection
based on inaccurate estimates can trigger a feedback loop,
leading to undesirably variable and low-quality video. We
call this phenomenon the downward spiral eﬀect, and we
measure it on all three services, present insights into its root
causes, and validate initial solutions to prevent it.
Categories and Subject Descriptors
C.2.0 [Computer Systems Organization]: Computer-
Communication Networks—General ; C.4 [Performance of
Systems]: [Measurement techniques]
General Terms
Measurement
Keywords
HTTP-based Video Streaming, Video Rate Adaptation
1.
INTRODUCTION
Video streaming is a huge and growing fraction of Inter-
net traﬃc, with Netﬂix and Youtube alone accounting for
over 50% of the peak download traﬃc in the US [18]. Sev-
eral big video streaming services run over HTTP and TCP
(e.g. Hulu, Netﬂix, Vudu, YouTube) and stream data to the
client from one or more third-party commercial CDNs (e.g.
Akamai, Level3 or Limelight). Streaming over HTTP has
several beneﬁts: It is standardized across CDNs (allowing
a portable video streaming service), it is well-established
(which means the CDNs have already made sure service can
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.
5000
4000
3000
s
/
b
k
2000
1000
Video Playback Rate
Competing Flow's
Throughput
1750
1400
1050
750
560
375
235
0
0
100 200 300 400 500 600 700 800
Time (s)
Figure 1:
(Service A) A video starts streaming at
1.75Mb/s over a 5Mb/s network. After 395 seconds,
a second ﬂow starts (from the same server). The
video could stream at 1.75Mb/s (given its fair share
of 2.5Mb/s), but instead drops down to 235kb/s.
reach through NATs to end-hosts), and cheap (the service
is simple, commoditized, and the CDNs compete on price).
These beneﬁts have made possible the huge growth in aﬀord-
able, high-quality movie and TV streaming, for our viewing
delight.
When video is streamed over HTTP, the video service
provider relies on TCP to ﬁnd the available bandwidth and
choose a video rate accordingly. For example, if a client
estimates that there is 1.5Mb/s available in the network,
it might request the server to stream video compressed to
1.3Mb/s (or the highest video rate available at or below
1.5Mb/s). The video streaming service provider must walk
a tightrope: If they pick a video rate that is too high, the
viewer will experience annoying rebuﬀering events; if they
pick a streaming rate that is too low, the viewer will ex-
perience poor video quality. In both cases, the experience
degrades, and user may take their viewing elsewhere [9]. It
is therefore important for a video streaming service to select
the highest safe video rate.
This paper describes a measurement study of three pop-
ular HTTP-based video streaming services (Hulu, Netﬂix,
and Vudu) to see how well they pick the video rate. Accord-
ing to the latest Consumer Reports [21], Netﬂix is the most
popular video streaming provider in the United States, while
2252262275000
4000
3000
2000
1000
s
/
b
k
Video Flow's
Throughput
Competing Flow's
Throughput
Video Playback Rate
5000
4000
3000
2000
1000
s
/
b
k
1750
1400
1050
750
560
375
235
Competing Flow's
 Throughput
3200
2500
2000
1500
1000
650
Video Flow's
Throughput
Zero Window
Advertisement
Video Playback Rate
0
0
100 200 300 400 500 600 700 800
0
0
100 200 300 400 500 600 700 800
Time (s)
Time (s)
(a) Service A. Network bottleneck set to 5Mb/s.
(b) Service B. Network bottleneck set to 5Mb/s.
20000
Video Flow's
Throughput
Competing Flow's
Throughput
15000
s
/
b
k
10000
5000
9000
6750
4500
3000
Video Playback Rate
0
0
100
200
300
400
Time (s)
500
600
700
5000
4000
3000
2000
1000
s
/
b
k
Video Flow's
Throughput
Competing Flow's
 Throughput
2000
1500
1000
Video
 Playback Rate
0
0
100
200
300
Time (s)
400
500
600
(c) Service C HD. Network bottleneck set to 22Mb/s.
(d) Service C SD. Network bottleneck set to 5Mb/s.
Figure 4: The downward spiral eﬀect is visible in all three services.
The video rates available from each of the three services
are summarized in Table 2; some playback rates may not be
available for some videos.
3.3 The Competing Flows
The competing ﬂow is a TCP ﬂow doing a long ﬁle down-
load. To eliminate any unfairness due to variations in net-
work path properties, we ensure that the competing ﬂow is
served by the same CDN, and usually, by the same server.
For Service A and Service C, the competing ﬂow is gener-
ated by an open-ended byte range request to the ﬁle with
the highest rate. Further, we use the DNS cache to make
sure that the competing ﬂow comes from same termination
point (the server or the load-balancer) as the video ﬂow.
For Service B, since the ﬁles are stored as small segments,
an open-ended request only creates short-lived ﬂows.
In-
stead, we generate the competing ﬂow by requesting the
Flash version of the same video stored in the same CDN,
using rtmpdump [20] over TCP.
4. THE DOWNWARD SPIRAL EFFECT
All three services suﬀer from what we call the “downward
spiral eﬀect” – a dramatic anomalous drop in the video play-
back rate in the presence of a competing TCP ﬂow. The
problem is starkly visible in Figure 4.
In all four graphs,
the video stream starts out alone and then competes with
another TCP ﬂow. As soon as the competing ﬂow starts up,
the client mysteriously picks a video playback rate that is far
below the available bandwidth. Our goal is to understand
why this happens.
To gain a ﬁrst inkling into what is going on, we calculate
the upper bound of what the client might believe the instan-
taneous available bandwidth to be, by measuring the arrival
rate of the last video segment. Speciﬁcally, we calculate the
throughput upper bound as the size of a received video seg-
ment divided by the time it took to arrive (the time from
when the ﬁrst byte arrived until the last byte arrived), which
excludes the initial server response time. In all of the graphs,
the video playback rate chosen by the client is quite strongly
correlated with the calculated throughput. As we will see,
228herein lies the problem:
if the client is selecting the video
rate based on some function of the throughput it perceived,
and the throughput is so diﬀerent from the actual available
bandwidth, then it is not surprising the client does such a
poor job. Let’s now see what goes wrong for each service in
turn. For ease of discussion, we will use video throughput to
refer to the throughput a client perceived by downloading a
video segment.
4.1 Service A
Figure 4(a) shows the playback rate of a Service A video
session along with the client’s video throughput over time.
Starting out, the video stream is the only ﬂow and the client
requests the highest video rate (1750kb/s). The competing
ﬂow begins after 400 seconds; the video rate steadily drops
until it reaches the lowest rate (235kb/s), and it stays there
most of the time until the competing ﬂow stops. In theory,
both ﬂows should be able to stream at 2.5Mb/s (their fair
share of the link) and the client should continue to stream
at 1750kb/s.
We repeated the experiment 76 times over four days. In
67 cases (91%) the downward spiral happens, and the client
picks either the lowest rate, or bounces between the two or
three lowest rates. In just seven cases (9%) was the client
able to maintain a playback rate above 1400kb/s. To ensure
accuracy and eliminate problems introduced by competing
ﬂows with diﬀerent characteristics (e.g. TCP ﬂows with
diﬀerent RTTs), we make the competing ﬂow request the
same video ﬁle (encoded at 1750kb/s) from the same CDN.
Unlike the video ﬂow, the competing ﬂow is just a simple
TCP ﬁle download and its download speed is only dictated
by TCP congestion control algorithm and not capped by the
video client.3
Why does throughput of the video ﬂow drop so much be-
low available fair-share bandwidth? Is it an inherent charac-
teristic of streaming video over HTTP, or is the client simply
picking the wrong video rate?
We ﬁrst conﬁrm that the available bandwidth really is
available for streaming video. We do this using a feature
provided by the Service A client that allows users to man-
ually select a video rate and disable the client’s automatic
rate selection algorithm. We repeat the above experiment,
but with a slight modiﬁcation. As soon as the client picks a
lower rate, we manually force the video to play at 1750kb/s.
Figure 5 shows the results. Interestingly, the client main-
tains a playback rate of 1750kb/s without causing rebuﬀer-
ing events, and the throughput also increases. This suggests
that the downward spiral eﬀect is caused by underestima-
tion of the available bandwidth in the client’s rate selection
algorithm. The bandwidth is available, but the client needs
to go grab it.
4.2 Service B
Figure 4(b) shows the same downward spiral eﬀect in Ser-
vice B. As before, the bottleneck bandwidth is 5Mb/s and
the RTT is around 20 ms. We start a video streaming ses-
sion ﬁrst, allow it to settle at its highest rate (3200kb/s) and
then start a competing ﬂow after 337 seconds, by reading the
same video ﬁle from the same server.
3To eliminate variation caused by congestion at the server,
we veriﬁed that the same problem occurs if we download
the competing video ﬁle from a diﬀerent server at the same
CDN.
5000
4000
Video
3000
 Flow's
Throughput
s
/
b
k
2000
1000
Competing Flow's
Throughput
Video Playback Rate
1750
1400
1050
750
560
375
235
0
0
200
400
600
Time (s)
800
1000
1200
Figure 5: (Service A) The client manages to main-
tain the highest playback rate if we disable auto-
matic rate selection.
The client should drop the video rate to 2500kb/s (its fair
share of the available bandwidth). Instead, it steps all the
way to the lowest rate oﬀered by Service B, 650kb/s, and
occasionally to 1000kb/s. The throughput plummets too.
4.3 Service C
We observe the downward spiral eﬀect in Service C as well.
Since Service C does not automatically switch between its
HD and SD bitrates, we do two separate experiments.
In the HD experiment, as shown in Figure 4(c), we set
the bottleneck bandwidth to 22Mb/s. To start with, the
client picks the highest HD video rate (9Mb/s). When the
client’s playback buﬀer is full, the video ﬂow is limited by
the receive window, and the throughput converges to the
same value as the playback rate. We start the competing
ﬂow at 100 seconds, and it downloads the same video ﬁle
(9Mb/s video rate) from the same CDN.
Each ﬂow has 11Mb/s available to it, plenty for the client
to continue playing at 9Mb/s. But instead, the client resets
the connection and switches to 4.5Mb/s and then 3Mb/s,
before bouncing around several rates.
SD is similar. We set the bottleneck bandwidth to 5Mb/s,
and the client correctly picks the highest rate (2000kb/s) to
start with, as shown in Figure 4(d). When we start the
competing ﬂow, the video client drops down to 1000kb/s
even though its share is 2.5Mb/s. Since Service C only oﬀers
three SD rates, we focus on its HD service in the rest of the
paper.
5. WALKING THE DOWNWARD SPIRAL
To understand how the downward spiral happens, we ex-
amine each service in turn. Although each service enters
the downward spiral for a slightly diﬀerent reason, there is
enough commonality for us to focus ﬁrst on Service A (and
Figure 4(a)) and then describe how the other two services
diﬀer.
2295000
4000
3000
2000
1000
s
/
b
k
Playout Buffer is full
6
5
4
3
2
1
)
s
(
l
a
v
r
e
t
n
I
t
s
e
u
q
e
R
Playout Buffer is full
0
160
170
180
190
Time (s)
200
210
0
0
50
100
150
Time (s)
200
250
300
(a) TCP throughput before and after the buﬀer ﬁlls.
(b) Request interval before and after the buﬀer ﬁlls.
Figure 6: (Service A) Before and after the playback buﬀer ﬁlls at 185 seconds.
)
%
(
F
D
C
100
80
60
40
20
0
0
Video Rate 235kb/s
Video Rate 375kb/s
Video Rate 560kb/s
Video Rate 750kb/s
Video Rate 1050kb/s
Video Rate 1400kb/s
Video Rate 1750kb/s
Video Rate 235kb/s
Video Rate 375kb/s
Video Rate 560kb/s
Video Rate 750kb/s
Video Rate 1050kb/s
Video Rate 1400kb/s
Video Rate 1750kb/s
100
80
60
40
20
)
%
(
F
D
C
500
1000
1500
Throughput (kb/s)
2000
2500
0
0
500
1000
1500
Throughput (kb/s)