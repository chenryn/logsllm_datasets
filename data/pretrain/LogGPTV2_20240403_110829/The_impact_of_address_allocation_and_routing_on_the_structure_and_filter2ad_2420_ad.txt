exist. Thus, each multibit node contributes at most one internal
bitmap and one external bitmap.
Along with the bitmaps, each multibit node maintains up to three
pointers. The child pointer points to the child nodes; since the
child nodes are stored contiguously, one pointer sufﬁces to access
them. The results pointer points to an array of next-hops. The third
pointer is to the internal bitmap - it exists only when the multi-
bit node contains preﬁxes. Each node also maintains ﬂags that are
associated with details of the algorithm and may vary with imple-
mentations.
For illustrative purposes, consider 8-bit IP addresses and a three
level multibit trie with strides of 2, 3 and 3 (as in Figure 5). The
root node of the multibit trie can have four child nodes since its
stride is two. The external bitmap of the root node in Figure 5 is
1000 because only the ﬁrst child node (of the four possible chil-
dren) exists. Multibit trie nodes that neither contain a preﬁx nor
have pointers to lower levels of the trie are assumed to have been
removed.
The child multibit trie node in Figure 5 also has an internal bitmap
of 0001100 which encodes the preﬁxes it contains. The bitmap is
of length 7 because there are 7 nodes in the subtrie; the ﬁrst bit
corresponds to the root, the next two bits to the two children of the
root, and the last four bits to the four leaves. The last four bits are
1100 because only the ﬁrst two leaves have stored preﬁxes.
    External bitmap
     =1000
Prefixes: 0000*, 
0001*
Internal bitmap
=0001100
Figure 5: This ﬁgure shows an example of a unibit trie on the
left and its corresponding multibit version on the right; for the
preﬁxes 0000* and 0001*. The bitmaps of the Tree Bitmap Al-
gorithm for the multibit version are also shown. The memory
required is 45 bits.
External bitmap=1001
Internal
bitmap
0001000
 Internal
bitmap
0001000
Figure 6: This ﬁgure shows what happens if the preﬁxes change
to 0000* and 1101*. Now, two child nodes are required. The
external bitmap is 1001 because the the ﬁrst and the last child
nodes of the root exist. The memory required is 73 bits.
To calculate the memory used in Figure 5, suppose that the mem-
ory is bit addressable. Also assume that 5 bits per node are required
for ﬂags. The memory used by the root node is 17 bits (8 bits for
child pointer, 4 bits for the external bitmap, and 5 bits for ﬂags).
The child node uses 28 bits (8 bits results pointer, 8 bit pointer to
the internal bitmap, 7 bits for the internal bitmap, and 5 bits for the
ﬂags)8.
Unlike TCAMs, the memory requirements of multibit tries de-
pends upon the relationship between the various preﬁxes in the
routing table. To illustrate the importance of trie structure, consider
the slightly different routing table containing 0000*, 1101* in Fig-
ure 6. This small change leads to a fairly big change in the memory
requirement because of the need to create a second child node. The
ﬁnal result, using similar calculations, is 17 + 28 + 28 = 73 bits.
Note that a TCAM would have needed just 2× 8 = 16 bits to store
these two preﬁxes.
The moral of the examples in Figure 5 and Figure 6 is that the
structure of the trie has a signiﬁcant effect on the memory required
to store the lookup data structure.
3.2 The Transistor Model
Our metric for comparing lookup techniques is the number of
transistors required to store a given number of preﬁxes. While
there are other important metrics like power (a typical algorith-
mic solution can consume six times less power than a TCAM of
the same size), TCAM vendors have been working to reduce power
using clever banking techniques. Transistor count appears to be
a more fundamental differentiator. Lesser transistors mean more
chips per wafer or higher yield. Transistor count can also act as a
product differentiator in the following sense. For a given chip area
and technology, there is a maximum number of transistors that can
8Some of these overheads may appear large but are actually quite
small for 32 bit addresses; our example used 8 bit addresses for
illustration.
Real routing table
Scatter
ARAM
60
50
40
30
s
n
o
i
l
l
i
m
n
i
s
r
o
i
t
s
s
n
a
r
t
f
o
r
e
b
m
u
N
20
0
10
20
30
40
50
60
70
Months since Nov.1997
Figure 7: The memory used by the multibit trie by the real
routing table, scatter and ARAM.
ﬁt on a chip. If designers can provision transistor counts for storing
preﬁxes, they can allocate the remaining transistors on the chip for
additional fast-path packet processing functionality.
TCAM product offerings use between 14 and 16 transistors to
store one bit of data (called a cell). To store an IP preﬁx, TCAMs
will need 32 bits, or between 448 and 512 transistors. By contrast,
six transistors are used to store one bit in SRAM. Therefore, we
calculate the number of transistors taken by a multibit trie solution
by multiplying the number of bits by six. For the example we dis-
cussed in Figures 5 and 6, a 14-transistor per cell TCAM would
have used at least 224 transistors. The Tree Bitmap scheme would
have required 270 (6× 45) or 438 (6× 73) transistors respectively.
Multibit trie algorithms also require a ﬁxed overhead of logic for
implementing the algorithm, and for the overhead of memory al-
location and deallocation. From discussions with a vendor who
has implemented a fully pipelined algorithmic solution, 20 mil-
lion transistors appears to be a fairly conservative estimate for the
lookup logic 9. This overhead gets amortized over the number of
preﬁxes. Clearly, TCAMs do not require this extra overhead as
their lookup logic is (effectively) distributed in each bit.
3.3 Applying ARAM to Evaluate Scalability
In Section 2.3 we found that by simply varying the number of
allocations while keeping the other parameters ﬁxed, we obtained
a good match with the routing table’s preﬁx length distribution,
depth distribution and trie density. As another measure of how well
ARAM matches current routing tables, Figure 7 compares the num-
ber of transistors that would be required, using the Tree Bitmap al-
gorithm, for ARAM’s routing tables, the actual routing table, and
those produced by the scatter model. Note the close match between
ARAM’s tables with the actual routing tables over the entire dura-
tion of our measurement period. By contrast, the scatter model is
signiﬁcantly inaccurate.
We now embark upon our scaling comparison of TCAMs and
multibit tries. For our baseline comparisons, we ﬁx all ARAM pa-
rameters to be those we used to match existing routing tables, and
only vary the number of allocations N. The transistor counts used
for these larger tables are shown in Figure 8. The number of tran-
sistors taken by TCAMs are shown as straight lines: CAMs-i rep-
resents an i transistor per cell TCAM. We see that unless TCAM
9This overhead is for a programmable processor and 32 pipeline
stages. A much smaller overhead is reported in [3]. Our overhead
of 20 million transistors is thus very conservative. Smaller over-
heads only slant the comparison further in favor of SRAM-based
algorithmic solutions.
250
200
150
100
50
s
n
o
i
l
l
i
m
n
i
s
r
o
i
t
s
s
n
a
r
t
f
o
r
e
b
m
u
N
0
0
ARAM
CAMs-8
CAMs-14
100
200
300
400
500
600
Routing table size: Number of prefixes in thousands
250
200
150
100
50
s
n
o
i
l
l
i
m
n
i
s
r
o
i
t
s
s
n
a
r
t
f
o
r
e
b
m
u
N
0
0
ARAM
CAMs-8
CAMs-14
100
200
300
400
500
Routing table size: Number of prefixes in thousands
Figure 8: The size of the routing table is increased by increasing
the number of allocations (other parameters are kept constant).
We see that unless TCAMs use 8 transistors per cell or less,
multibit tries would use lesser transistors than TCAMs.
Figure 10: The size of the routing table is increased by increas-
ing Cspawn (other parameters are kept constant). We see that
multibit tries handle this case very well.
x
i
f
e
r
p
r
e
p
s
e
d
o
n
e
i
r
t
t
i
i
b
n
u
f
o
r
e
b
m
u
N
3.2
3.12
3.04
2.96
2.88
0
10
20
30
40
50
60
70
Months since Nov. 1997
Figure 9: The density of the trie has been increasing with time.
technology advances to the point where it becomes feasible to use
8 or fewer transistors per cell, multibit tries will scale better.
So far we have evaluated scaling in transistor counts based on
extrapolating current routing practices as embodied by the ARAM
parameters required to match current routing tables. It is worth ask-
ing what happens to these projections if routing practices change
and the parameters deviate from the values we chose.
If routing practices change in the direction of more preﬁx spawn-
ing (caused, for example, by signiﬁcant increase of load-sharing),
we claim that the multibit trie algorithm will do even better. More
preﬁxes will ﬁt in the same multibit trie node, thereby amortizing
the overhead of ﬂags and bitmaps even better. Figure 10 shows
that as Cspawn increases, the memory taken by the multibit trie
algorithm increases slower than the size of the routing table. Oth-
ers [37] in the community have been observing, for some time now,
the increase in multihoming practice. Although we used just one
set of values for the parameters—except for the number of alloca-
tions N—to approximately match all past routing tables, we have
observed that the Cspawn in real routing tables has been steadily
increasing, although almost imperceptibly. This is evidenced by
the decrease in unibit trie density over time from 3.2 to 2.9 or so,
Figure 9; that is, preﬁxes have become more “clustered” with time.
A similar behavior is also seen in Figure 11; when Fsplit increases,
there are more load shared preﬁxes at depth zero of the routing ta-
ble. Varying Csplit shows a similar scaling [4]. In general, load
shared preﬁxes are clustered together.
On the other hand, with changes in some routing practices, com-
200
150