the processor must produce a hint packet for the
software decoder are asynchronous events such as
interrupts or traps. These events are recorded as
FUPs and usually followed by a TIP to indicate the
following instruction.
USENIX Association
26th USENIX Security Symposium    169
To limit the amount of trace data generated, Intel PT
provides multiple options for runtime ﬁltering. Depend-
ing on the given processor, it might be possible to con-
ﬁgure multiple ranges for instruction-pointer ﬁltering (IP
Filter). In general, these ﬁlter ranges only affect virtual
addresses if paging is enabled; this is always the case
in x86-64 long-mode. Therefore, it is possible to limit
trace generation to selected ranges and thus avoid huge
amounts of superﬂuous trace data. In accordance to the
IP ﬁltering mechanism, it is possible to ﬁlter traces by
the current privilege level (CPL) of the protection ring
model (e.g ring 0 or ring 3). This ﬁlter allows us to select
only the user mode (CPL > 0) or kernel mode (CPL = 0)
activity. kAFL utilizes this ﬁlter option to limit tracing
explicitly to kernel mode execution. In most cases, the
focus of tracing is not the whole OS within all user mode
processes and their kernel interactions. To limit trace
data generation to one speciﬁc virtual memory address
space, software can use the CR3 Filter. Intel PT will only
produce trace data if the CR3 value matches the conﬁg-
ured ﬁlter value. The CR3 register contains the pointer to
the current page table. The value of the CR3 register can
thus be used to ﬁlter code executed on behalf of a certain
ring 3 process, even in ring 0 mode.
Intel PT supports various conﬁgurable target domains
for output data. kAFL focuses on the Table of Physical
Addresses (ToPA) mechanism that enables us to specify
multiple output regions: Every ToPA table contains mul-
tiple ToPA entries, which in turn contain the physical ad-
dress of the associated memory chunk used to store trace
data. Each ToPA entry contains the physical address, a
size speciﬁer for the referred memory chunk in physical
memory, and multiple type bits. These type bits specify
the CPU’s behavior on access of the ToPA entry and how
to deal with ﬁlled output regions.
3 System Overview
We now provide a high-level overview of the design of an
OS-independent and hardware-assisted feedback fuzzer
before presenting the implementation details of our tool
called kAFL in Section 4.
Our system is split
into three components:
the
fuzzing logic, the VM infrastructure (modiﬁed versions
of QEMU and KVM denoted by QEMU-PT and KVM-
PT), and the user mode agent. The fuzzing logic runs
as a ring 3 process on the host OS. This logic is also re-
ferred to as kAFL. The VM infrastructure consists of a
ring 3 component (QEMU-PT) and a ring 0 component
(KVM-PT). This facilitates communication between the
other two components and makes the Intel PT trace data
available to the fuzzing logic. In general, the guest only
communicates with the host via hypercalls. The host can
then read and write guest memory and continues VM ex-
Figure 1: High-level overview of the kAFL architecture.
The setup process ( 1(cid:13)- 3(cid:13)) is not shown.
ecution once the request has been handled. A overview
of the architecture can be seen in Figure 1.
We now outline the events and communication that
take place during a fuzz run, as depicted in Figure 2.
When the VM is started, the ﬁrst part of the user mode
agent (the loader) uses the hypercall HC_SUBMIT_PANIC
to submit the address of the kernel panic handler (or the
BugCheck kernel address in Windows) to QEMU-PT 1(cid:13).
QEMU-PT then patches a hypercall calling routine at the
address of the panic handler. This allows us to get noti-
ﬁed and react fast to crashes in the VM (instead of wait-
ing for timeouts / reboots).
Then the loader uses the hypercall HC_GET_PROGRAM to
request the actual user mode agent and starts it 2(cid:13). Now
the loader setup is complete and the fuzzer begins its ini-
tialization. The agent triggers a HC_SUBMIT_CR3 hyper-
call that will be handled by KVM-PT. The hypervisor
extracts the CR3 value of the currently running process
and hands it over to QEMU-PT for ﬁltering 3(cid:13). Finally,
the agent uses the hypercall HC_SUBMIT_BUFFER to in-
form the host at which address it expects its inputs. The
fuzzer setup is now ﬁnished and the main fuzzing loop
starts.
During the main loop, the agent requests a new input
using the HC_GET_INPUT hypercall 4(cid:13). The fuzzing logic
produces a new input and sends it to QEMU-PT. Since
QEMU-PT has full access to the guest’s memory space,
it can simply copy the input into the buffer speciﬁed by
the agent. Then it performs a VM-Entry to continue ex-
ecuting the VM 5(cid:13). At the same time, this VM-Entry
event enables the PT tracing mechanism. The agent now
consumes the input and interacts with the kernel (e.g.,
it interprets the input as a ﬁle system image and tries to
mount it 6(cid:13)). While the kernel is being fuzzed, QEMU-
PT decodes the trace data and updates the bitmap on de-
mand. Once the interaction is ﬁnished and the kernel
handed control back to the agent, the agent notiﬁes the
hypervisor via a HC_FINISHED hypercall. The resulting
VM-Exit stops the tracing and QEMU-PT decodes the
remaining trace data 7(cid:13). The resulting bitmap is passed
170    26th USENIX Security Symposium
USENIX Association
Figure 2: Overview of the kAFL hypercall interaction.
to the logic for further processing 8(cid:13). Afterwards, the
agent can continue to run any untraced clean-up routines
before issuing another HC_GET_INPUT to start the next
loop iteration.
3.1 Fuzzing Logic
The fuzzing logic is the command and controlling com-
ponent of kAFL. It manages the queue of interesting
inputs, creates mutated inputs, and schedules them for
evaluation.
In most aspects, it is based on the algo-
rithms used by AFL. Similarly to AFL, we use a bitmap
to store basic block transitions. We gather the AFL
bitmap from the VMs through an interface to QEMU-PT
and decide which inputs triggered interesting behaviour.
The fuzzing logic also coordinates the number of VMs
spawned in parallel. One of the bigger design differences
to AFL is that kAFL makes extensive use of multipro-
cessing and parallelism, where AFL simply spawns mul-
tiple independent fuzzers which synchronize their input
queues sporadically1. In contrast, kAFL executes the de-
terministic stage in parallel, and all threads work on the
most interesting input. A signiﬁcant amount of time is
spent in tasks that are not CPU-bound (such as guests
that delay execution). Therefore, using many parallel
processes (upto 5-6 per CPU core) drastically improves
performance of the fuzzing process due to a higher CPU
load per core. Lastly, the fuzzing logic communicates
with the user interface to display current statistics in reg-
ular intervals.
1AFL recently added experimental support for distributing the
deterministic stage, see https://github.com/mirrorer/afl/blob/
master/docs/parallel_fuzzing.txt#L60-L66.
3.2 User Mode Agent
We expect a user mode agent to run inside the virtual-
ized target OS. In principle, this component only has to
synchronize and gather new inputs by the fuzzing logic
via the hypercall interface and use it to interact with the
guest’s kernel. Example agents are programs that try to
mount inputs as ﬁle system images, pass speciﬁc ﬁles
such as certiﬁcates to kernel parser or even execute a
chain of various syscalls.
In theory, we only need one such component. In prac-
tice, we use two different components: The ﬁrst program
is the loader component. Its job is to accept an arbitrary
binary via the hypercall interface. This binary represents
the user mode agent and is executed by the loader com-
ponent. Additionally, the loader component will check
if the agent has crashed (which happens often in case of
syscall fuzzing) and restarts it if necessary. This setup
has the advantage that we can pass any binary to the
VM and reuse VM snapshots for different fuzzing com-
ponents.
3.3 Virtualization Infrastructure
The fuzzing logic uses QEMU-PT to interact with KVM-
PT to spawn the target VMs. KVM-PT allows us to trace
individual vCPUs instead of logical CPUs. This com-
ponent conﬁgures and enables Intel PT on the respec-
tive logical CPU before the CPU switches to guest ex-
ecution and disables tracing during the VM-Exit tran-
sition. This way, the associated CPU will only pro-
vide trace data of the virtualized kernel itself. QEMU-
PT is used to interact with the KVM-PT interface to
conﬁgure and toggle Intel PT from user space and ac-
cess the output buffer to decode the trace data. The
USENIX Association
26th USENIX Security Symposium    171
decoded trace data is directly translated into a stream
of addresses of executed conditional branch instruc-
tions. Moreover, QEMU-PT also ﬁlters the stream of
executed addresses—based on previous knowledge of
non-deterministic basic blocks—to prevent false-positive
fuzzing results, and makes those available to the fuzzing
logic as AFL-compatible bitmaps. We use our own cus-
tom Intel PT decoder to cache disassembly results, which
leads to signiﬁcant performance gains compared to the
off-the-shelf solution provided by Intel.
3.4 Stateful and Non-Deterministic Code
Tracing operating systems results in a signiﬁcant amount
of non-determinism.
The largest source of non-
deterministic basic block transitions are interrupts, which
can occur at any point in time. Additionally, our imple-
mentation does not reset the whole state after each exe-
cution since reloading the VM from a memory snapshot
is costly. Thus we have to deal with the stateful and asyn-
chronous aspects of the kernel. An example for stateful
code might be a simple call to kmalloc(): Depending
on the number of previous allocations, kmalloc() might
simply return a fresh pointer or map a whole range of
pages and update a signiﬁcant amount of metadata. We
use two techniques to deal with these challenges.
The ﬁrst one is to ﬁlter out interrupts and the transi-
tion caused while handling interrupts. This is possible
using the Intel PT trace data. If an interrupt occurs, the
processor emits a TIP instruction since the transfer is not
visible in the code. To avoid confusion during an inter-
rupt occurring at an indirect control ﬂow instruction, the
TIP packet is marked with FUP (ﬂow update packet) to
indicate an asynchronous event. After identifying such a
signature, the decoder will drop all basic blocks visited
until the corresponding iret instruction is encountered.
To link the interrupts with their corresponding iret, we
track all interrupts on a simple call stack. This mecha-
nism is necessary since the interrupt handler may itself
be interrupted by another interrupt.
The second mechanism is to blacklist any basic block
that occurs non-deterministically. Each time we en-
counter a new bit in the AFL bitmap, we re-run the in-
put several times in a row. Every basic block that does
not show up in all of the trials will be marked as non-
deterministic and ﬁltered from further processing. For
fast access, the results are stored in a bitmap of black-
listed basic block addresses. During the AFL bitmap
translation, any transition hash value—which combines
the current basic block address and the previous ba-
sic block address—involving a blacklisted block will be
skipped.
3.5 Hypercalls
Hypercalls are a feature introduced by virtualization. On
Intel platforms, hypercalls are triggered by the vmcall
instruction. Hypercalls are to VMMs as syscalls are to
kernels. If any ring 3 process or the kernel in the VM ex-
ecutes a vmcall instruction, a VM-Exit event is triggered
and the VMM can decide how to process the hypercall.
We patched KVM-PT to pass through our own set of hy-
percalls to the fuzzing logic if a magic value is passed
in rax and the appropriate hypercall-ID is set in rbx.
Additionally, we also patched KVM-PT to accept hyper-
calls from ring 3. Arguments for speciﬁc hypercalls are
passed through rcx. We use this mechanism to deﬁne
an interface that user mode agent can use to communi-
cate with the fuzzing logic. One example hypercall is
HC_SUBMIT_BUFFER. Its argument is a guest pointer that
is stored in rcx. Upon executing the vmcall instruction,
a VM-Exit is triggered and QEMU-PT stores the buffer
pointer that was passed. It will later copy the new input
data into this buffer (see step 5(cid:13) in Figure 2). Finally, the
execution of the VM is continued.
cli
mov rax , KAFL_MAGIC_VALUE
mov rbx , HC_CRASH
mov rcx , 0 x0
vmcall
Listing 1: Hypercall crash notiﬁer.
Another use case for this interface is to notify the
fuzzing logic when a crash occurs in the target OS kernel.
In order to do so, we overwrite the kernel crash handler
of the OS with a simple hypercall routine. The injected
code is shown in Listing 1 and displays how the hyper-
call interface is used on the assembly level. The cli in-
struction disables all interrupts to avoid any kind of asyn-
chronous interference during the hypercall routine.
4
Implementation Details
Based on the design outlined in the previous section, we
built a prototype of our approach called kAFL. In the fol-
lowing, we describe several implementation details. The
source code of our reference implementation is available
at https://github.com/RUB-SysSec/kAFL.
4.1 KVM-PT
Intel PT allows us to trace branch transitions without
patching or recompiling the targeted kernel. To the best
of our knowledge, no publicly available driver is able to
trace only guest executions of a single vCPU using In-
tel PT for long periods of time. For instance, Simple-PT
[29] does not support long-term tracing by design. The
172    26th USENIX Security Symposium
USENIX Association
perf-subsystem [5] supports tracing of VM guest oper-
ations and long-term tracing. However, it is designed to
trace logical CPUs, not vCPUs. Even if VMX execution
is traced, the data would be associated with logical CPUs
and not with vCPUs. Hence, the VMX context must be
reassembled, which is a costly task.
To address these shortcomings, we developed KVM-
PT. It allows us to trace vCPUs for an indeﬁnite amount
of time without any scheduling side effects or any loss
of trace data due to overﬂowing output regions. The ex-
tension provides a fast and reliable trace mechanism for
KVM vCPUs. Moreover, this extension exposes, much
like KVM, an extensive user mode interface to access
this tracing feature from user space. QEMU-PT utilizes
this novel interface to interact with KVM-PT and to ac-
cess the resulting trace data.
4.1.1 vCPU Speciﬁc Traces
To enable Intel PT, software that runs within ring
0 (in our case KVM-PT) has to set
the corre-
sponding bit of a model speciﬁc register
(MSR)
(IA32_RTIT_CTL_MSR.TraceEn) [28]. After tracing is
enabled, the logical CPU will trace any executed code if
it satisﬁes the conﬁgured ﬁlter options. The modiﬁcation
has to be done before the CPU switches from the host
context to the VM operation; otherwise the CPU will ex-
ecute guest code and is technically unable to modify any
host MSRs. The inverse procedure is required after the
CPU has left the guest context. However, enabling or dis-
abling Intel PT manually will also yield a trace contain-
ing the manual MSR modiﬁcation. To prevent the collec-
tion of unwanted trace data within the VMM, we use the
MSR autoload capabilities of Intel VT-x. MSR autoload-
ing can be enabled by modifying the corresponding en-
tries in the VMCS (e.g., VM_ENTRY_CONTROL_MSR
for VM-entries). This forces the CPU to load a list of pre-
conﬁgured values for deﬁned MSRs after either a VM-
entry or VM-exit has occurred. By enabling tracing via
MSR autoloading, we only gather Intel PT trace data for
one speciﬁc vCPU.
4.1.2 Continuous Tracing
Once we have enabled Intel PT, the CPU will write the
resulting trace data into a memory buffer until it is full.