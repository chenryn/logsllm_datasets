seed value by repeatedly applying the chosen hash function.
Counter-intuitively, the last value generated is referred to
as the ﬁrst value of the chain. The seed of a chain with n
1Note that TV-OTS actually relies on salted hash chains
[21]. However, the ideas discussed here apply equally to
salted and unsalted hash chains.
values would be the nth value. This terminology reﬂects the
use of values in reverse order of their generation.
Hash chains are especially applicable to message authen-
tication due to their one-way properties. Asymmetric proto-
cols that use hash chain generated keys reveal the ﬁrst chain
value (the last to be generated) as part of the pre-shared
public key. As new keys are used and sent through the pro-
tocol, they must be veriﬁed by the receivers. Receivers verify
new keys by creating a match with a known value, either the
one distributed in the public key or one more recently re-
ceived. Newly received keys, when hashed (possibly multiple
times), should generate one of these known values. Invalid
keys can not achieve this match. Only the sender knows the
seed value necessary to calculate new valid keys. Assuming
the unused values are kept secret, the identity of the sender
is veriﬁed with each new value revealed.
The term traversal refers to the sequential output of hash
chain values starting with the ﬁrst value and working toward
the last. Traversals present a challenge because the output
order is opposite the order of generation. For long chains,
storing all values may be impractical, but otherwise, needed
values must be recomputed. If only the seed is available as a
starting point, calculating values near the beginning of the
chain requires heavily repetitious hashing. Wiser strategies
look for ways to balance the cost of storage and computation
so that neither becomes too costly.
2.3 Fractal Hash Sequencing and Traversal
In the search for a traversal strategy balancing storage
and computation costs, FHT [10] has emerged as a prac-
tical and elegant solution to achieving O(log2(n)) bounds
on both measures. While FHT can’t maintain these same
bounds unless values are retrieved consecutively, the bounds
for retrieving a sequence of values are still suﬃciently low to
make it a traversal candidate for TV-OTS. The FHT struc-
ture serves as a foundation for the algorithms presented in
Section 3. The explanation given here places special empha-
sis on the details from which the new algorithms are built.
FHT stores only log2(n) chain values chosen in such a way
that retrieving new values requires little computation. The
arrangement of these stored values is dynamic and contin-
ually changes to accommodate future requests more easily.
When one of the stored values is retrieved, it is no longer
useful and is abandoned in favor of storing a later value
from the chain. The stored values are kept grouped closely
towards the next values that will be retrieved, limiting the
amount of work performed by any single retrieval operation.
To facilitate the dynamic arrangement of stored values,
a small data structure called a pebble is used to associate
additional information with each stored value. Each pebble
stores one chain value at a time, along with the value’s po-
sition in the chain. Pebbles are distinguished by a unique
identiﬁer, ID, which also governs the process followed to up-
date the values in individual pebbles. Sometimes one pebble
will be referred to as larger than another, meaning the value
of its ID is greater than the ID of the smaller pebble. In
a chain of n values, the log2(n) pebbles are stored in a list
sorted by chain position.
Note that two lists are now present: the conceptual list of
values comprising the chain and the stored list of pebbles.
This creates possible confusion over the meaning of the term
position of a value or pebble. For consistency, diﬀerent terms
will used in reference to these two structures. The word
1275position is used exclusively in reference to the entire hash
chain. The words above, below, higher and lower are often
used to describe relative positions, with the words higher and
above pertaining to pebbles closer to the seed of the chain.
Naturally, relative positions in the chain are preserved in the
pebble list, since the pebble list is kept sorted by position.
Updating the values stored in the pebbles is analogous
to moving pebbles within the chain. Conceptually, when a
pebble acquires a new value, it moves to the position in the
chain associated with the new value.
In fact, FHT works
by moving pebbles through an interconnected sequence of
strategic arrangements.
The positions of pebbles in each possible arrangement al-
low easy computation of the next output values and future
arrangements. At initialization, the position of each pebble
matches its ID value. More formally, there is a pebble at ev-
ery position 2i where 1 ≤ i ≤ log2(n). The gaps between the
pebbles form intervals, with smaller intervals near the begin-
ning of the chain. When a pebble moves, it always divides
an interval evenly into two new equally sized intervals. Like
pebble IDs, interval sizes are powers of two which facilitates
easy splitting. The sorted order of the intervals is preserved
since when a pebble moves, the new intervals created are at
least as large as intervals at lower positions. This pattern
of intervals is used to ensure that both retrieved values and
future arrangements can be calculated eﬃciently.
Figure 2: When a pebble moves, it cannot move
directly to its destination. It must calculate the de-
sired value by ﬁrst moving upward past its destina-
tion. Once it copies the value from a stored pebble,
the desired value is calculated by additional hash
operations as the pebble moves downwards to its
destination.
The method for calculating chain values limits the ways
in which pebbles can move. When a pebble moves to a new
position, it can not acquire the new value directly. Recall
that pebble values must be computed from one another. The
one-way properties restrict each value to being computed
from values at higher positions. Moving a pebble requires
ﬁnding and copying the value stored in some higher pebble
and calculating the desired value from there.
In essence,
pebbles move in two phases, as illustrated in Figure 2. In the
ﬁrst phase, the pebble moves upwards to the same location
as a higher pebble. In the second phase, the pebble’s new
value is hashed repeatedly, eﬀectively moving the pebble
downward in the chain to its destination.
The pattern maintained by the arrangements of pebbles
is governed by the pebble IDs and provides the ability to re-
trieve keys within a logarithmic time bound. This arrange-
ment pattern is formed by the second stage of pebble move-
ment.
In this second stage, pebbles step downward from
their new position acquired in the ﬁrst stage. The number
of downward steps a pebble takes equals the value of its
ID. Eventually, this creates an interval between each newly
moved pebble and the pebble it copied from with the new
interval size equal to the moved pebble’s ID. The pebble
at the upper edge of the interval, whose value was copied,
was chosen because the interval it bounded before the move
was twice the size of the ID of the moving pebble.
(The
existence of such an interval is guaranteed [10].) Thus, on
each move, a pebble splits an interval into two new equally
sized intervals. Furthermore, pebbles always move into the
nearest interval large enough to evenly divide. Naturally,
the pebbles with smaller IDs move shorter distances. Since
the size of newly created intervals is at least as large as the
intervals at lower positions, the intervals remain sorted by
size. The pebble with the ID value of 2 never moves beyond
the lowest four values in the unused chain, and ensures these
lowest values are part of intervals of size two. Consequently,
any value retrieved from this section of the chain will never
require more than a single hash operation to compute. The
remaining hash operations are performed in stepping the
other pebbles towards their destinations.
To provide the amortized upper bound on retrieval time,
moving pebbles rarely perform their downward movement
phase all at once. Instead, moving pebbles distribute these
downward steps over several retrieval operations, taking only
enough steps to ensure they reach their destination by the
time the value at that destination is needed. Notice that
pebbles with larger IDs move further and are therefore not
needed at their new positions as quickly as smaller pebbles.
The number of retrievals that occur before a pebble must
reach its destination is directly related to how far the pebble
must travel. In fact, only the pebble with ID value 2 must
reach its destination by the time the next pebble is moved.
This happens exactly two retrievals after this pebble was
moved. For all other pebbles, these intermediate retrievals
are used to distribute hashing costs over time, avoiding a
situation where some retrievals are inexpensive and others
are costly. The total number of hash operations per retrieval
is limited to two per actively moving pebble, plus at most
one to calculate the retrieved value. The maximum hash
operations per retrieval is thus 2 × log2(n) + 1.
2.4 Related Traversals
The idea of eﬃcient hash chain traversal introduced by
Itkis and Reyzin [9] started a wave of traversals with varying
eﬃciencies and trade-oﬀs.
The traversal suggested by Itkis and Reyzin was soon fol-
lowed by Jakobsson’s FHT [10], described above, from which
all the others drew either direct or indirect inspiration. How-
ever, the dependency on consecutive value retrieval is com-
mon in all of such traversals. Traversals presented by Cop-
persmith and Jakobsson [4], and Yum et al. [22] build di-
rectly on FHT by modifying the pebble movement pattern.
Coppersmith and Jakobsson achieve near maximum theo-
retical eﬃciency for consecutive retrievals by allocating a
hash budget on each round, and using a sophisticated move-
ment pattern that distributes this budget between two sets
of pebbles. A set of greedy pebbles consumes as much of
the hash budget as possible, and any extra is alloted to the
remaining pebbles. In this way, the variance in hash oper-
ations between iterations is eliminated, lowering the worst-
case number to 1
2 log2(n) hashes per round, though at a
1276storage cost of slightly greater than log2(n) pebbles. This
technique is further improved upon by Yum et al. who use
the same strategy to balance the distribution of hash oper-
ations over the rounds, but with a less complex movement
pattern. The resulting algorithm is simpler than the Cop-
persmith and Jakobsson algorithm, and achieves the same
lowered time bound without requiring additional storage.
In response to the traversals where computational bounds
scale with chain length n, Sella proposed a traversal where
computation time could be ﬁxed at the expense of storage
space [20]. Using a slightly diﬀerent chain partitioning tech-
nique, the spacings between pebbles can be adjusted to ac-
commodate the imposed computational bounds. Unfortu-
nately, additional pebbles are needed to facilitate this spac-
√
ing strategy so that for a ﬁxed computational bound of m,
n where k = m + 1. For com-
the storage requirement is k k
parison, this is twice the storage required by FHT for equiv-
alent time bounds. Later, a scheme devised by Kim was
able provide the same ﬁxed time bounds without these ex-
tra pebbles by carefully timing pebble movements to create
a more synchronous system [11]. This reduced the storage
by a factor of k, which matches the storage required by FHT
for equivalent bounds.
One remaining strategy, capable of lowering retrieval com-
plexity even further, stems from the introduction of multi-
dimensional chains. Hu et al. present two traversals based
on a modiﬁed structure of the underlying chain [8]. Sand-
wich chains intertwine multiple chains to form a construc-
tion whose primary purpose is eﬃcient veriﬁcation of keys.
Their second construction, Comb Skip-Chains, lowers re-
trieval bounds by amortizing the FHT over the secondary
dimension of a two-dimensional chain structure. With a
total of log2(n) secondary chains, the amortization brings
retrieval time down to a constant, while storage is bounded
by O(log2(n)).
3. TARGETING TRAVERSAL
Thus far the claim that FHT is ineﬃcient in the presence
of skipped values is unsubstantiated but is easy to show by
example. These examples are also helpful in understand-
ing the reasons underlying the ineﬃciencies of FHT. Such
insights lead to areas where FHT can be improved.
3.1 Motivating Insight
The interdependency of successive FHT retrievals, though
used to establish an upper bound on retrieval time, causes
unnecessary operations when the interval between desired
values is large. The movement of any pebble relies on the
correct completion of prior pebble movements. Practically,
this implies that values must be retrieved from the chain in
the expected, consecutive order, discarding unneeded values.
This iterative process is highly wasteful.
One prominent source of wasted computation is the cal-
culation of values within ranges of skipped values. Take,
for example, pebbles moved to positions below the retrieval
target. Since key use is ordered (once a key is retrieved,
the keys at lower positions will never be needed), the hash
computations used to move pebbles into skipped ranges are
completely wasted. With the understanding that a pebble
is only useful once it moves above the target value, all the
movements of each pebble can be predicted and grouped
into a single move. This modiﬁcation, illustrated in Fig-
Figure 3: In this small sample chain segment, each
line above the chain represents a required move in
order to retrieve the darkened pebble by the iter-
ative FHT method. Targeting eliminates this tan-
gle by moving each pebble only once, without pass-
ing through intermediate locations, as shown by the
dashed lines below the chain.
ure 3, eliminates the hash operations performed by calculat-
ing values from the unused region.
For the purpose of eﬃciently moving pebbles, it is useful
to consider each arrangement of pebbles as a state, with
retrievals triggering state transitions. Figure 3 contrasts the
idea of a targeted state transition against FHT’s iterative
method. To simplify matters further, new states depend
only on the location of the target value, and not on any
previous states.
State changes are the inspiration behind the targeting al-
gorithm. The goal of the targeting algorithm is to perform
state changes meeting two criteria: ﬁrst, the state changes
should be functionally indistinguishable from state changes
performed by iterative retrievals. Additionally, the imple-
mented state changes should perform the minimum number
of hash operations necessary to achieve the new state. That
is, a hash operation should never be applied to the same
value more than once during a single retrieval.
State transitions alone do not guarantee the elimination
of all redundant hashes and algorithms must be crafted care-
fully to avoid hashing the same value more than once. The
threat of duplicate hashes arises when two moving pebbles
skip upward to the same position during the ﬁrst movement
phase. In this case, both pebbles will copy the value from
the same reference pebble and hash this value in order to
step downward. In essence, by hashing the same value, the