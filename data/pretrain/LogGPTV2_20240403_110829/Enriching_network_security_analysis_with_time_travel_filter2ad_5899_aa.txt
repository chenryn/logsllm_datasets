title:Enriching network security analysis with time travel
author:Gregor Maier and
Robin Sommer and
Holger Dreger and
Anja Feldmann and
Vern Paxson and
Fabian Schneider
Enriching Network Security Analysis with Time Travel
Gregor Maier
TU Berlin / DT Labs
Anja Feldmann
TU Berlin / DT Labs
Robin Sommer
ICSI / LBNL
Vern Paxson
ICSI / UC Berkeley
Holger Dreger
Siemens AG
Corporate Technology
Fabian Schneider
TU Berlin / DT Labs
ABSTRACT
In many situations it can be enormously helpful to archive the
raw contents of a network trafﬁc stream to disk, to enable later
inspection of activity that becomes interesting only in retrospect.
We present a Time Machine (TM) for network trafﬁc that provides
such a capability. The TM leverages the heavy-tailed nature of
network ﬂows to capture nearly all of the likely-interesting trafﬁc
while storing only a small fraction of the total volume. An initial
proof-of-principle prototype established the forensic value of such
an approach, contributing to the investigation of numerous attacks
at a site with thousands of users. Based on these experiences, a
rearchitected implementation of the system provides ﬂexible, high-
performance trafﬁc stream capture, indexing and retrieval, includ-
ing an interface between the TM and a real-time network intrusion
detection system (NIDS). The NIDS controls the TM by dynami-
cally adjusting recording parameters, instructing it to permanently
store suspicious activity for ofﬂine forensics, and fetching trafﬁc
from the past for retrospective analysis. We present a detailed per-
formance evaluation of both stand-alone and joint setups, and re-
port on experiences with running the system live in high-volume
environments.
Categories and Subject Descriptors:
C.2.3 [Computer-Communication Networks]: Network Operations
– Network monitoring
General Terms:
Measurement, Performance, Security
Keywords:
Forensics, Packet Capture, Intrusion Detection
1.
INTRODUCTION
When investigating security incidents or trouble-shooting per-
formance problems, network packet traces—especially those with
full payload content—can prove invaluable. Yet in many opera-
tional environments, wholesale recording and retention of entire
data streams is infeasible. Even keeping small subsets for extended
time periods has grown increasingly difﬁcult due to ever-increasing
trafﬁc volumes. However, almost always only a very small subset
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’08, August 17–22, 2008, Seattle, Washington, USA.
Copyright 2008 ACM 978-1-60558-175-0/08/08 ...$5.00.
of the trafﬁc turns out to be relevant for later analysis. The key
difﬁculty is how to decide a priori what data will be crucial when
subsequently investigating an incident retrospectively.
For example, consider the Lawrence Berkeley National Labo-
ratory (LBNL), a security-conscious research lab (≈ 10,000 hosts,
10 Gbps Internet connectivity). The operational cybersecurity staff
at LBNL has traditionally used bulk-recording with tcpdump to an-
alyze security incidents retrospectively. However, due to the high
volume of network trafﬁc, the operators cannot record the full traf-
ﬁc volume, which averages 1.5 TB/day. Rather, the operators con-
ﬁgure the tracing to omit 10 key services, including HTTP and FTP
data transfers, as well as myriad high-volume hosts.
Indeed, as
of this writing the tcpdump ﬁlter contains 72 different constraints.
Each of these omissions constitutes a blind spot when performing
incident analysis, one very large one being the lack of records for
any HTTP activity.
In this work we develop a system that uses dynamic packet
ﬁltering and buffering to enable effective bulk-recording of large
trafﬁc streams, coupled with interfaces that facilitate both manual
(operator-driven) and automated (NIDS-driven) retrospective anal-
ysis. As this system allows us to conveniently “travel back in time,”
we term the capability it provides Time Travel, and the correspond-
ing system a Time Machine (TM)1. The key insight is that due to
the “heavy-tailed” nature of Internet trafﬁc [17, 19], one can record
most connections in their entirety, yet skip the bulk of the total vol-
ume, by only storing up to a (customizable) cutoff limit of bytes for
each connection. We show that due to this property it is possible
to buffer several days of raw high-volume trafﬁc using commod-
ity hardware and a few hundred GB of disk space, by employing
a cutoff of 10–20 KB per connection—which enables retaining a
complete record of the vast majority of connections.
Preliminary work of ours explored the feasibility of this ap-
proach and presented a prototype system that included a simple
command-line interface for queries [15].
In this paper we build
upon experiences derived from ongoing operational use at LBNL
of that prototype, which led to a complete reimplementation of the
system for much higher performance and support for a rich query-
interface. This operational use has also proven the TM approach
as an invaluable tool for network forensics:
the security staff of
LBNL now has access to a comprehensive view of the network’s
activity that has proven particularly helpful with tracking down the
ever-increasing number of attacks carried out over HTTP.
At LBNL, the site’s security team uses the original TM system
on a daily basis to verify reports of illegitimate activity as reported
by the local NIDS installation or received via communications from
1For what it’s worth, we came up with this name well before its use
by Apple for their backup system, and it appeared in our 2005 IMC
short paper [15].
external sites. Depending on the type of activity under investiga-
tion, an analyst needs access to trafﬁc from the past few hours or
past few days. For example, the TM has enabled assessment of ille-
gitimate downloads of sensitive information, web site defacements,
and conﬁguration holes exploited to spam local Wiki installations.
The TM also proved crucial in illuminating a high-proﬁle case of
compromised user credentials [5] by providing evidence from the
past that was otherwise unavailable.
Over the course of operating the original TM system within
LBNL’s production setup (and at experimental installations in two
large university networks), several important limitations of the ﬁrst
prototype became apparent and led us to develop a new, much more
efﬁcient and feature-enhanced TM implementation that is currently
running there in a prototype setup. First, while manual, analyst-
driven queries to the TM for retrieving historic trafﬁc are a cru-
cial TM feature, the great majority of these queries are triggered
by external events such as NIDS alerts. These alerts occur in sig-
niﬁcant volume, and in the original implementation each required
the analyst to manually interact with the TM to extract the corre-
sponding trafﬁc prior to inspecting it to assess the signiﬁcance of
the event. This process becomes wearisome for the analyst, leading
to a greater likelihood of overlooking serious incidents; the analyst
chooses to focus on a small subset of alerts that appear to be the
most relevant ones. In response to this problem, our current system
offers a direct interface between the NIDS and the TM: once the
NIDS reports an alert, it can ask the TM to automatically extract
the relevant trafﬁc, freeing the analyst of the need to translate the
notiﬁcation into a corresponding query.
In addition, we observed that the LBNL operators still perform
their traditional bulk-recording in parallel to the TM setup,2 as a
means of enabling occasional access to more details associated with
problematic connections. Our current system addresses this con-
cern by making the TM’s parameterization dynamically adaptable:
for example, the NIDS can automatically instruct the redesigned
TM to suspend the cutoff for hosts deemed to be malicious.
We also found that the operators often extract trafﬁc from the TM
for additional processing. For example, LBNL’s analysts do this
to assess the validity of NIDS notiﬁcations indicating that a con-
nection may have leaked personally identiﬁable information (PII).
Such an approach reﬂects a two-tiered strategy: ﬁrst use cheap,
preliminary heuristics to ﬁnd a pool of possibly problematic con-
nections, and then perform much more expensive analysis on just
that pool. This becomes tenable since the volume is much smaller
than that of the full trafﬁc stream. Our current system supports such
an approach by providing the means to redirect the relevant trafﬁc
back to the NIDS, so that the NIDS can further inspect it automati-
cally. By coupling the two systems, we enable the NIDS to perform
retrospective analysis.
Finally, analysis of our initial TM prototype in operation un-
covered a key performance challenge in structuring such a system,
namely the interactions of indexing and recording packets to disk
while simultaneously handling random access queries for historic
trafﬁc. Unless we carefully structure the system’s implementation
to accommodate these interactions, the rigorous real-time require-
ments of high-volume packet capture can lead to packet drops even
during small processing spikes.
Our contributions are: (i) the notion of efﬁcient, high-volume
bulk trafﬁc recording by exploiting the heavy-tailed nature of net-
work trafﬁc, and (ii) the development of a system that both supports
such capture and provides the capabilities required to use it effec-
tively in operational practice, namely dynamic conﬁguration, and
2One unfortunate side-effect of this parallel setup is a signiﬁcantly
reduced disk budget available to the TM.
0
0
5
1
0
0
0
1
0
0
5
0
MWN
UCB
LBNL
]
B
G
[
e
m
u
o
V
l
e
u
T
d
e
W
u
h
T
i
r
F
t
a
S
n
u
S
n
o
M
Time
e
u
T
d
e
W
u
h
T
i
r
F
t
a
S
n
u
S
n
o
M
Figure 1: Required buffer size with tr = 4d, 10 KB cutoff.
automated querying for retrospective analysis. We provide the lat-
ter in the context of interfacing the TM with the open-source “Bro”
NIDS, and present and evaluate several scenarios for leveraging the
new capability to improve the detection process.
The remainder of this paper is structured as follows. In §2 we in-
troduce the basic ﬁltering structure underlying the TM. We present
a design overview of the TM, including its architecture and remote
control capabilities, in §3. In §4 we evaluate the performance of the
TM when deployed in high-volume network environments. In §5
we couple the TM with a NIDS. We discuss deployment trade-offs
in §6 and related work in §7. We ﬁnish with a summary in §8.
2. EXPLOITING HEAVY-TAILS
The key strategy for efﬁciently recording the contents of a high-
volume network trafﬁc stream comes from exploiting the heavy-
tailed nature of network trafﬁc: most network connections are quite
short, with a small number of large connections (the heavy tail) ac-
counting for the bulk of total volume [17, 19]. Thus, by record-
ing only the ﬁrst N bytes of each connection (the cutoff ), we can
record most connections in their entirety, while still greatly reduc-
ing the volume of data we must retain. For large connections, we
keep only the beginning; however, for many uses the beginning of
such connections is the most interesting part (containing protocol
handshakes, authentication dialogs, data items names, etc.). Faced
with the choice of recording some connections completely versus
recording the beginning of all connections, we generally prefer the
latter. (We discuss the evasion risk this trade-off faces, as well as
mitigation strategies, in §6.)
To directly manage the resources consumed by the TM, we con-
ﬁgure the system with disk and memory budgets, which set upper
bounds on the volume of data retained. The TM ﬁrst stores packets
in a memory buffer. When the budgeted buffer ﬁlls up, the TM mi-
grates the oldest buffered packets to disk, where they reside until
the TM’s total disk consumption reaches its budgeted limit. After
this point, the TM begins discarding the oldest stored packets in
order to stay within the budget. Thus, in steady-state the TM will
consume a ﬁxed amount of memory and disk space, operating con-
tinually (months at a time) in this fashion, with always the most
recent packets available, subject to the budget constraints.
As described above, the cutoff and memory/disk budgets apply
to all connections equally. However, the TM also supports deﬁning
storage classes, each characterized by a BPF ﬁlter expression, and
applying different sets of parameters to each of these. Such classes
allow, for example, trafﬁc associated with known-suspicious hosts
to be captured with a larger cutoff and retained longer (by isolating
its budgeted disk space from that consumed by other trafﬁc).
We now turn to validating the effectiveness of the cutoff-based
approach in reducing the amount of data we have to store. To as-
sess this, we use a simulation driven off connection-level traces.
The traces record the start time, duration, and volume of each TCP
connection seen at a given site. Such traces capture the nature of
their environment in terms of trafﬁc volume, but with much less
volume than would full packet-level data, which can be difﬁcult to
record for extended periods of time.
Since we have only connection-level information for the simula-
tion, we approximate individual packet arrivals by modeling each
connection as generating packets at a constant rate over its duration,
such that the total number of (maximum-sized) packets sums to the
volume transferred by the connection. Clearly, this is an oversim-
pliﬁcation in terms of packet dynamics; but because we consider
trafﬁc at very large aggregation, and at time scales of hours/days,
the inaccuracies it introduces are negligible [27].
For any given cutoff N, the simulation allows us to compute the
volume of packet data currently stored. We can further reﬁne the
analysis by considering a speciﬁc retention time tr, deﬁning how
long we store packet data. While the TM does not itself provide
direct control over retention time, with our simulation we can com-
pute the storage the system would require (i.e., what budget we
would have to give it) to achieve a retention time of at least tr.
For our assessment, we used a set of connection-level logs gath-
ered between November 5–18, 2007, at three institutions: The
Münchner Wissenschaftsnetz (Munich Scientiﬁc Research Network,
MWN) connects two major universities and afﬁliated research in-
stitutes to the Internet (roughly 50,000 hosts). MWN has a 10 Gbps
uplink, and its trafﬁc totals 3–6 TB/day. Since our monitoring
comes from a 1 Gbps SPAN port, data rates can reach this limit
during peak hours, leading to truncation. The Lawrence Berke-
ley National Laboratory (LBNL) is a large research institute with
about 10,000 hosts connected to the Internet by a 10 Gbps uplink.
LBNL’s trafﬁc amounts to 1–2 TB/day. Our monitoring link here
is a 10 Gbps tap into the upstream trafﬁc. Finally, UC Berkeley
(UCB) has about 45,000 hosts. It is connected to the Internet by
two 1 Gbps links and has 3–5 TB of trafﬁc per day. As SPAN ports
of the two upstream routers are aggregated into one 1 Gbps moni-
toring link, we can again reach capacity limits during peak times.
The connections logs contain 3120M (UCB), 1898M (MWN),
and 218M (LBNL) entries respectively. The logs reveal that indeed
91–94% of all connections at the three sites are shorter than a cutoff
value of N = 10 KB. With a cutoff of 20 KB, we can record 94–
96% of all connections in their entirety. (Of all connections, only
44–48% have any payload. Of those, a cutoff value of N = 10 KB
truncates 14–19%; N = 20 KB truncates 9–13%.)
Fig. 1 plots the disk budget required for a target retention time
tr = 4 days, when employing a 10 KB cutoff. During the ﬁrst 4 days
we see a ramp-up phase, during which no data is evicted because
the retention time tr has not yet passed. After the ramp-up, the
amount of buffer space required stabilizes, with variations stem-
ming from diurnal patterns. For LBNL, a quite modest buffer