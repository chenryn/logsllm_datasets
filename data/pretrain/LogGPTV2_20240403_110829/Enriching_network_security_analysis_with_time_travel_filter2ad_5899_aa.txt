# Enriching Network Security Analysis with Time Travel

**Authors:**
- Gregor Maier, TU Berlin / DT Labs
- Anja Feldmann, TU Berlin / DT Labs
- Robin Sommer, ICSI / LBNL
- Vern Paxson, ICSI / UC Berkeley
- Holger Dreger, Siemens AG, Corporate Technology
- Fabian Schneider, TU Berlin / DT Labs

## Abstract
In many scenarios, archiving the raw contents of network traffic for later inspection can be immensely valuable, especially when investigating incidents that become interesting only in hindsight. We present a Time Machine (TM) system for network traffic that provides this capability. The TM leverages the heavy-tailed nature of network flows to capture nearly all likely-interesting traffic while storing only a small fraction of the total volume. An initial proof-of-concept prototype demonstrated the forensic value of this approach, contributing to the investigation of numerous attacks at a site with thousands of users. Based on these experiences, a rearchitected implementation of the system offers flexible, high-performance traffic stream capture, indexing, and retrieval, including an interface with a real-time network intrusion detection system (NIDS). The NIDS dynamically adjusts recording parameters, instructs the TM to store suspicious activity permanently for offline forensics, and fetches past traffic for retrospective analysis. We provide a detailed performance evaluation of both standalone and integrated setups and report on experiences with running the system in high-volume environments.

**Categories and Subject Descriptors:**
C.2.3 [Computer-Communication Networks]: Network Operations – Network monitoring

**General Terms:**
Measurement, Performance, Security

**Keywords:**
Forensics, Packet Capture, Intrusion Detection

## 1. Introduction
Network packet traces, particularly those with full payload content, are invaluable for investigating security incidents or troubleshooting performance problems. However, in many operational environments, wholesale recording and retention of entire data streams is infeasible due to the ever-increasing volume of traffic. Even keeping small subsets for extended periods has become increasingly difficult. The key challenge is determining which data will be crucial for later analysis.

For example, consider the Lawrence Berkeley National Laboratory (LBNL), a security-conscious research lab with approximately 10,000 hosts and 10 Gbps Internet connectivity. The cybersecurity staff at LBNL traditionally used bulk-recording with `tcpdump` to analyze security incidents retrospectively. Due to the high volume of network traffic (averaging 1.5 TB/day), they cannot record the full traffic volume. Instead, they configure the tracing to omit 10 key services, including HTTP and FTP data transfers, as well as myriad high-volume hosts. This results in blind spots during incident analysis, such as the lack of records for any HTTP activity.

In this work, we develop a system that uses dynamic packet filtering and buffering to enable effective bulk-recording of large traffic streams, coupled with interfaces that facilitate both manual (operator-driven) and automated (NIDS-driven) retrospective analysis. This system allows us to "travel back in time," hence the term Time Travel, and the corresponding system is called a Time Machine (TM). The key insight is that due to the heavy-tailed nature of Internet traffic, one can record most connections in their entirety while skipping the bulk of the total volume by storing up to a customizable cutoff limit of bytes for each connection. This approach enables buffering several days of raw high-volume traffic using commodity hardware and a few hundred GB of disk space, typically with a cutoff of 10–20 KB per connection, which retains a complete record of the vast majority of connections.

Preliminary work explored the feasibility of this approach and presented a prototype system with a simple command-line interface for queries. Building on experiences from ongoing operational use at LBNL, we have re-implemented the system for higher performance and support for a rich query interface. Operational use has proven the TM approach as an invaluable tool for network forensics, providing the security staff with a comprehensive view of the network's activity, particularly helpful for tracking down the increasing number of attacks carried out over HTTP.

At LBNL, the security team uses the original TM system daily to verify reports of illegitimate activity reported by the local NIDS or received from external sites. Depending on the type of activity under investigation, an analyst needs access to traffic from the past few hours or days. For example, the TM has enabled the assessment of unauthorized downloads of sensitive information, website defacements, and configuration holes exploited to spam local Wiki installations. The TM also proved crucial in a high-profile case of compromised user credentials by providing otherwise unavailable evidence from the past.

Over the course of operating the original TM system within LBNL's production setup and at experimental installations in two large university networks, several limitations became apparent, leading to the development of a more efficient and feature-enhanced TM implementation. First, while manual, analyst-driven queries to the TM for retrieving historic traffic are crucial, the majority of these queries are triggered by external events such as NIDS alerts. In the original implementation, each alert required the analyst to manually interact with the TM to extract the corresponding traffic, which was wearisome and led to overlooking serious incidents. Our current system offers a direct interface between the NIDS and the TM, allowing the NIDS to automatically extract relevant traffic upon reporting an alert, freeing the analyst from this task.

Additionally, LBNL operators still perform traditional bulk-recording in parallel to the TM setup to occasionally access more details associated with problematic connections. Our current system addresses this by making the TM's parameterization dynamically adaptable, allowing the NIDS to automatically instruct the TM to suspend the cutoff for hosts deemed malicious.

We also found that operators often extract traffic from the TM for additional processing, such as assessing the validity of NIDS notifications indicating potential leaks of personally identifiable information (PII). This reflects a two-tiered strategy: first, use cheap, preliminary heuristics to find a pool of possibly problematic connections, then perform more expensive analysis on that pool. Our current system supports this by providing means to redirect relevant traffic back to the NIDS for automatic further inspection.

Finally, analysis of our initial TM prototype uncovered a key performance challenge in structuring the system to handle indexing and recording packets to disk while simultaneously handling random access queries for historic traffic. Unless carefully structured, the real-time requirements of high-volume packet capture can lead to packet drops even during small processing spikes.

Our contributions are:
1. The notion of efficient, high-volume bulk traffic recording by exploiting the heavy-tailed nature of network traffic.
2. The development of a system that supports such capture and provides the capabilities required for effective operational use, including dynamic configuration and automated querying for retrospective analysis. We provide the latter in the context of interfacing the TM with the open-source "Bro" NIDS and present and evaluate several scenarios for leveraging this new capability to improve the detection process.

The remainder of this paper is structured as follows. In §2, we introduce the basic filtering structure underlying the TM. In §3, we present a design overview of the TM, including its architecture and remote control capabilities. In §4, we evaluate the performance of the TM in high-volume network environments. In §5, we couple the TM with a NIDS. In §6, we discuss deployment trade-offs. In §7, we review related work. We conclude with a summary in §8.

## 2. Exploiting Heavy-Tails
The key strategy for efficiently recording the contents of a high-volume network traffic stream is to exploit the heavy-tailed nature of network traffic. Most network connections are quite short, with a small number of large connections (the heavy tail) accounting for the bulk of total volume. By recording only the first N bytes of each connection (the cutoff), we can record most connections in their entirety while greatly reducing the volume of data retained. For large connections, we keep only the beginning, which is often the most interesting part (containing protocol handshakes, authentication dialogs, data item names, etc.). Faced with the choice of recording some connections completely versus recording the beginning of all connections, we generally prefer the latter. (We discuss the evasion risk and mitigation strategies in §6.)

To manage the resources consumed by the TM, we configure the system with disk and memory budgets, setting upper bounds on the volume of data retained. The TM first stores packets in a memory buffer. When the budgeted buffer fills up, the TM migrates the oldest buffered packets to disk, where they reside until the TM’s total disk consumption reaches its budgeted limit. After this point, the TM begins discarding the oldest stored packets to stay within the budget. Thus, in steady-state, the TM will consume a fixed amount of memory and disk space, operating continually (months at a time) with the most recent packets available, subject to the budget constraints.

As described, the cutoff and memory/disk budgets apply to all connections equally. However, the TM also supports defining storage classes, each characterized by a BPF filter expression, and applying different sets of parameters to each class. Such classes allow, for example, traffic associated with known-suspicious hosts to be captured with a larger cutoff and retained longer (by isolating its budgeted disk space from that consumed by other traffic).

We now turn to validating the effectiveness of the cutoff-based approach in reducing the amount of data we have to store. To assess this, we use a simulation driven off connection-level traces. These traces record the start time, duration, and volume of each TCP connection seen at a given site. Such traces capture the nature of their environment in terms of traffic volume but with much less volume than full packet-level data, which can be difficult to record for extended periods.

Since we have only connection-level information for the simulation, we approximate individual packet arrivals by modeling each connection as generating packets at a constant rate over its duration, such that the total number of (maximum-sized) packets sums to the volume transferred by the connection. While this is an oversimplification in terms of packet dynamics, it is negligible at very large aggregations and time scales of hours/days.

For any given cutoff N, the simulation allows us to compute the volume of packet data currently stored. We can further refine the analysis by considering a specific retention time tr, defining how long we store packet data. While the TM does not itself provide direct control over retention time, with our simulation, we can compute the storage the system would require (i.e., what budget we would have to give it) to achieve a retention time of at least tr.

For our assessment, we used a set of connection-level logs gathered between November 5–18, 2007, at three institutions:
- The Münchner Wissenschaftsnetz (Munich Scientific Research Network, MWN) connects two major universities and affiliated research institutes to the Internet (roughly 50,000 hosts). MWN has a 10 Gbps uplink, and its traffic totals 3–6 TB/day. Monitoring comes from a 1 Gbps SPAN port, so data rates can reach this limit during peak hours, leading to truncation.
- The Lawrence Berkeley National Laboratory (LBNL) is a large research institute with about 10,000 hosts connected to the Internet by a 10 Gbps uplink. LBNL’s traffic amounts to 1–2 TB/day. Our monitoring link here is a 10 Gbps tap into the upstream traffic.
- UC Berkeley (UCB) has about 45,000 hosts. It is connected to the Internet by two 1 Gbps links and has 3–5 TB of traffic per day. As SPAN ports of the two upstream routers are aggregated into one 1 Gbps monitoring link, we can again reach capacity limits during peak times.

The connection logs contain 3120M (UCB), 1898M (MWN), and 218M (LBNL) entries, respectively. The logs reveal that 91–94% of all connections at the three sites are shorter than a cutoff value of N = 10 KB. With a cutoff of 20 KB, we can record 94–96% of all connections in their entirety. (Of all connections, only 44–48% have any payload. Of those, a cutoff value of N = 10 KB truncates 14–19%; N = 20 KB truncates 9–13%.)

Figure 1 plots the disk budget required for a target retention time tr = 4 days, when employing a 10 KB cutoff. During the first 4 days, we see a ramp-up phase, during which no data is evicted because the retention time tr has not yet passed. After the ramp-up, the amount of buffer space required stabilizes, with variations stemming from diurnal patterns. For LBNL, a modest buffer size of 500 GB is sufficient.