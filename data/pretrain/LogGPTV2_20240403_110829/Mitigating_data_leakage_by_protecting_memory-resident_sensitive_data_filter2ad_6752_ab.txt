in which plaintext sensitive data is ever exposed. A decryption
“boundary” is defined at the system call level, to allow for seamless
interaction with the OS or inter-process communication by supply-
ing decrypted data to domains outside the reach of an attacker. To
achieve these capabilities, several challenges must be addressed:
(1) The whole code of the process must be considered, including
the main application and all its libraries.
(2) All pointers that may reference a sensitive object must be iden-
tified and handled accordingly.
(3) Data marked as sensitive may propagate to other (non-marked)
variables and objects.
(4) The unit of encryption for AES is 128 bits, but sensitive data
objects may be smaller or larger than that.
Our design is centered around addressing the above challenges. In
the rest of this section, we describe the different types of analysis
and code transformation required for protecting a given application.
4.1 Whole-Program Analysis
To ensure that sensitive values are never left decrypted in memory,
our approach must analyze and transform the whole program code,
including any external libraries, because sensitive data might be
passed as arguments to functions in these external libraries. This re-
quires the source code of the application and all dependent libraries
to be available for analysis and transformation.
Performing whole-program analysis at the source code level is
difficult, as merging the source code of different libraries may result
in clashes due to identically named static functions and variables.
To avoid these issues, we opt for merging the code object files after
LLVM transforms them to their intermediate representation (IR),
at which point any identically named static functions are automati-
cally renamed. Moreover, operating at the IR level gives us access
to LLVM’s sophisticated analysis and transformation capabilities
available at this level. Also, ensuring that the sensitive data remains
protected through the LLVM backend passes requires interfacing
with them, and operating at the IR level makes this easier.
Link time optimization gives LLVM the capability of dumping
the IR of a compilation unit on disk. This allows the IR of multiple
compilation units to be optimized as a single module. The LLVM
toolchain provides the necessary tools to generate static libraries
from these IR units, thus allowing link time optimization of the
application along with its library dependencies.
4.2 Pointer Analysis
Once the programmer annotates an object or a variable as sensitive,
every valid access to these objects must be transformed with the
appropriate encryption or decryption routines. Given the heavy
reliance of C and C++ code on the use of pointers, we must first
determine which pointers may hold references to sensitive objects,
so that the respective pointer dereference operations can be also
transformed accordingly. To that end, as part of the static analysis
performed at the IR level, we employ pointer analysis to resolve all
possible memory objects that a pointer might refer to.
Sensitive Data Domain. The LLVM optimization phase al-
4.2.1
ready provides implementations of various pointer analysis al-
gorithms. However, these implementations support only intra-
procedural analysis capabilities, which are not adequate for our
purposes. Instead, we use an inter-procedural version of Andersen’s
algorithm [14]. This well-known flow-insensitive pointer analysis
algorithm examines pointer-related statements one by one, and
updates a points-to graph with any newly found points-to rela-
tionships. Each node of the graph represents either a pointer or a
memory object, and each edge represents a points-to relationship.
Figure 1 shows a small C code example and its corresponding
points-to graph. The points-to set of pointer ptr1 includes the
variables a, b, and the array arr, but only variable a has been
annotated as sensitive. Because ptr1 can point to any of the objects
in its points-to set, we must treat all three variables as sensitive.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
void fun1 ( void ) {
SENSITIVE int a;
int b , c;
int arr [10];
int * ptr1 , * ptr2 ;
ptr1 = &a;
ptr1 = &b;
ptr2 = &b;
ptr2 = &c;
for ( int i = 0; i < 10; i ++) {
ptr1 = & arr [i ]; ...
}
}
Figure 1: Example C code with an integer variable marked
as sensitive (line 2), and the corresponding points-to graph.
Moreover, once we mark variable b as sensitive, ptr2 (which points
to b) must also be marked as sensitive, and in turn, variable c
becomes sensitive as well.
These relationships form an equivalence class of sensitive data,
which we call the sensitive data domain, depicted in the upper part
of the points-to graph. This example illustrates one of the major
challenges we faced—that is, the results of pointer analysis are in
general an over-approximation of the actual relationships among
objects, which consequently results in an over-approximation of the
actual sensitive data domain. Ideally, we would like our analysis to
have maximum precision to minimize the instrumentation overhead.
Unfortunately, higher degrees of precision usually entail longer
computation time for the analysis, and in certain cases, may give
rise to other challenges specific to our use case.
Field Sensitivity. Field sensitivity [18, 72] is an approach for
4.2.2
improving the precision of pointer analysis, and refers to the ability
of the analysis algorithm to distinguish between individual fields of
a complex object, such as a C struct. This is particularly important
in case of complex objects containing multiple pointers that may
point to distinct sets of objects in memory. Unlike field-insensitive
analysis, field-sensitive analysis treats each of these pointers (of
the same complex object type) as distinct. Field-sensitive pointer
analysis is thus more precise than field-insensitive analysis, and
would result in a smaller sensitive data domain.
Using field-sensitive analysis for protecting sensitive data is by
no means an easy feat. Numerous challenges abound. For one, while
the block size for AES operations is 128 bits, the individual fields
of a struct object will often not be aligned at 128-bit boundaries,
requiring extra padding and alignment. We describe this and other
related challenges in detail in Sections 4.4 and 5.1.3. As shown by
our experimental evaluation, switching to field-sensitive analysis
resulted in a considerable reduction of the overall runtime overhead
compared to field-insensitive analysis.
4.3 Value Flow Analysis
Resolving all pointer references is not enough to achieve complete
data protection, as sensitive data may propagate to other variables
and objects, which we call sensitive sink sites. To prevent potential
information leakage through them, we use value flow analysis to
recursively find all such sensitive sink sites. All memory accesses
to these sites are then instrumented with appropriate encryption
or decryption transformations.
To correctly track sensitive value flows through function calls, we
first resolve the targets of function pointers using the information
generated from the prior pointer analysis phase, which allows for
the creation of a sound call graph. Having the call graph, we can
then track sensitive values passed as arguments to other functions,
as well as any sensitive values returned by functions. Sensitive
value flows can be direct or indirect. Indirect flows occur due to
the presence of pointers. Due to the reliance on the prior pointer
analysis phase for resolving the targets of pointers, our value flow
analysis is also affected by the precision of the pointer analysis.
The combination of pointer and value flow analysis gives us the
full set of sensitive data objects that must be kept encrypted in
memory, and the corresponding code instrumentation points.
4.4 In-Memory Data Protection
Once all memory objects in the sensitive data domain have been
discovered, as a result of the pointer and value flow analysis phases,
the final step is to instrument the respective memory read and write
operations with calls to custom decryption and encryption routines.
We opted for the strong data confidentiality that AES [30] offers, to
avoid the risk of cryptanalysis-based attacks that an adversary could
mount through script code (or even offline). Modern processors
offer native support for accelerating AES operations, e.g., as is the
case with Intel’s AES-NI extensions [39].
A major engineering challenge we faced stems from the fact
that the basic unit of operation for AES is 128 bits, but sensitive
scalar values may be 8, 16, 32, or 64 bits in length, while data
objects such as private keys, passwords, and configuration-related
data structures, are often larger than 128 bits. The frequent size
mismatch between objects and AES block size prevents us from
applying AES directly to protect individual objects. Dealing with
smaller objects is relatively straightforward by padding them to 128
bits, although this entails several implementation considerations for
different types of memory (global, stack, heap), which we discuss in
Section A.1 of the appendix. On the other hand, dealing with larger
objects unavoidably requires processing them in 128-bit blocks. In
both cases, objects are 128-bit aligned to optimize memory offset
computations.
Decrypted Data Cache. To optimize the common case of repeated
accesses to the same data, we implemented a decrypted data cache
to minimize the number of cryptographic operations over time for a
given block. Our requirement of never exposing plaintext sensitive
data in memory explicitly rules out the possibility of using any
memory-resident buffer for this purpose. However, we can take
advantage of spare CPU registers to temporarily hold decrypted
data—leaking register contents requires the execution of arbitrary
(i.e., non-instrumented) code, which (based on our threat model,
discussed in Section 2) falls outside the attacker’s capabilities.
The x86 Streaming SIMD Extensions provide support for 16 128-
bit registers (named XMM0 to XMM15) in 64-bit processors (or eight
128-bit registers in 32-bit processors). When accessing a sensitive
value from memory, we first decrypts the 128-bit block that contains
the sensitive value, and loads it into the XMM0 register. In case of
arrabcSensitive Data Domainptr1ptr2a read operation, the respective byte/word/double-word is copied
from the XMM0 register to the required general purpose register—
after that point, all arithmetic or logical instructions that follow the
memory read proceed unchanged. In case of a write operation, the
new value of the required byte/word/double-word is written in the
appropriate offset in the XMM0 register.
Instead of immediately clearing the XMM0 register, the decrypted
contents are retained for as long as possible. Any subsequent ac-
cess to the same block can be directly accommodated from the
already decrypted contents of the XMM0 register. When a sub-
sequent sensitive memory operation accesses a different 128-bit
block, the current block is re-encrypted and written back to mem-
ory before proceeding. The register is also re-encrypted and written
back before calls to any external interface. This simplified caching
approach takes advantage of the locality of data accesses to reduce
the overhead of repeated AES operations on the same data.
5 IMPLEMENTATION
The Clang frontend translates C/C++ code to the LLVM interme-
diate representation, which is then lowered into assembly by the
LLVM backend. LLVM provides a powerful and expressive frame-
work for analysis and transformation at the IR level, and thus most
of our implementation was performed at that level. The LLVM IR
also simplifies the high level C/C++ code to enable efficient code
transformations and analysis.
The LLVM compiler toolchain is modularized into several passes,
with most of the passes operating at the IR level. Each pass car-
ries out a single analysis or transformation task. We implemented
pointer analysis and value flow analysis as two separate analysis
passes, and the final AES instrumentation as a transformation pass.
Figure 2 illustrates how the different phases are integrated into the
LLVM toolchain.
The Clang frontend lowers the SENSITIVE annotation to a call
to the llvm.var.annotation function, which takes as arguments
the objects that were annotated as sensitive. We first collect these
arguments to find the initial set of sensitive objects. Then, at the IR
level, this set of objects becomes the starting point of our analysis
and transformation passes.
5.1 Link Time Optimization
We modified LLVM to invoke our analysis and transformation
passes during the LTO phase, which enables us to support static
libraries and standalone applications. This also has the additional
benefit of not requiring any modifications to Makefiles, except for
passing custom values to environment variables, such as CC, AR,
RANLIB, and CFLAGS.
5.1.1 Pointer Analysis. To perform whole-program analysis, we
extended the the Static Value Flow (SVF) analysis framework [80],
which supports pointer analysis and program dependence analysis
for C and C++ programs. SVF first analyzes the LLVM IR instruc-
tions in the merged IR and gathers constraints that model the flow
of pointers in the program. These constraints are represented in
the form of a constraint graph. Then, using an inter-procedural
Andersen’s style pointer analysis algorithm [14], SVF iteratively
performs pointer analysis by performing a reaching analysis on
this constraint graph, followed by call graph construction. Each
iteration of pointer analysis may discover new function pointer
targets, and therefore updates the call graph with new call edges.
Each new edge in the call graph may expose new pointer flows,
thus requiring the pointer analysis to be repeated. This iterative
execution continues until no new edges are added to the graph, i.e.,
until reaching a “fixed point.”
SVF ensures that the result of the pointer analysis is sound. The
pointer analysis provided by SVF is field-sensitive. As discussed in
Section 5.1.3, field-sensitive analysis results in individual fields of
a structure becoming sensitive. This causes problems because the
AES unit of encryption is 128 bit, and individual fields are often
neither aligned to 128 bits, nor 128 bit wide. Therefore, in addition
to the field-sensitive version that handles these complex struct-field
alignment cases, we also implemented a simpler field-insensitive
version. For this, we modified the processing of constraints so
that accesses to individual fields of complex objects are treated
as accesses to the entire object. As discussed in Section 4.2.2, this