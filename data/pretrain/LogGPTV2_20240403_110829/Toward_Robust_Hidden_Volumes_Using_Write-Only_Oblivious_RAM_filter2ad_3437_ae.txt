from the observed behavior in a lower volume.
This can be accomplished by structuring the stash as a series of
multiple queues (called Stashi in Algorithm 6), one for each vol-
ume Vi. For an HVEWrite, all the blocks in S are read and their
content copied into the respective queues for their volume. A block
on the disk may belong to some volume i, or it may be a random
string not containing any information. To put it into the correct
queue, we have to attempt to decrypt the block with every key until
we ﬁnd the right volume or we run out of keys. This means that our
ciphertexts must contain some redundant information, for decryp-
tion veriﬁcation, but this can easily be accomplished by padding
our plaintexts with a number of zero bits proportional to security
parameter s. Veriﬁcation then consists of decrypting and check-
ing if the plaintext begins with a sufﬁcient number of zeroes. This
meshes with our notion of security, because A, knowing some num-
ber of keys, cannot tell whether a block is part of a volume which
he does not have the key for or if it is simply a random string.
Now, having freed k blocks on the disk, we will write k blocks
back to the positions given by S. For this, we read out of our stash
queues, giving priority to the queues for lower volumes. That is,
we empty the queue Stash1 for V1 ﬁrst, followed by Stash2 for V2,
etc. If there are less than k blocks in all stashes, then the leftover
blocks from S are ﬁlled with random strings.
HVERead: For a HVERead, we read the block just like in our
regular ORAM by querying the recursive map for the address and
then reading that block from the disk. After we retrieve the target
block, we also perform a “dummy” write to make reads and writes
look identical to adversary A. This write does not change any val-
ues in the system, but it gives us a chance to write items from the
stash, if necessary. Note that a block b we want to read in volume
Vi might reside in the stash, so we ﬁrst lookup b in Stashi.
HVESetup: Initialization is identical to our base ORAM, we
simply initialize the map to an “empty” state where every block
starts unmapped.
Complexity: Figure 4, moved together with its recurrence rela-
tions to Appendix A, shows how HIVE is more efﬁcient than the
generic construction using max separate ORAMs. Although HIVE
performs better for a large range of parameters, note that HIVE
scales exponentially in L. Since we have to change up to k blocks at
each level, which in turn requires changing k blocks in all the levels
below them, the overall complexity is O(kL). With max = 10, for
up to 1 Exabyte, HIVE is still cheaper than our generic construction
since it is independent of max. However, with larger volumes and
more levels of recursion, HIVE becomes less practical than our ﬁrst
construction. Therefore, depending on the choice of parameters, it
can be more efﬁcient to use one or the other.
For completeness sake, we note that Path ORAM can also be
modiﬁed to produce a “combined volume” version in a similar
manner (due to its use of a stash), but it would be signiﬁcantly less
efﬁcient than our construction, just as Path ORAM is less efﬁcient
than our write-only ORAM.
Security: Since a block from Vj, j > i is written only if the
queue for Vi is already empty, blocks from Vj cannot inﬂuence A’s
view of Vi. Additionally, since an encryption under Pj is indis-
tinguishable from a random string to A which does not know Pj.
What this means is that A’s view of the disk cannot be impacted by
volumes which he does not have the key to, therefore we achieve
ΓArbitrary,Plausible
A,Σ
HIVE-B: To make HIVE secure against Opportunistic adver-
saries, we can follow the same idea as in Section 4.2. Instead of
immediately writing in volumes Vi, i > 1, we just add the block
to Stashi. When we do operations in V1, we proceed as normal,
writing as much as we can from Stash2,Stash3, etc. We omit full
algorithms and proofs for HIVE-B, because of space constraints,
but they follow immediately from existing descriptions and proofs.
5.3 Discussion
security as before.
We have shown that, under various adversarial models, HIVE
does not give any information about the number of volumes in use,
beyond what is known a priori. Yet, in practice, just the presence of
HIVE on a user’s system is an indication that there might be more
than one volume.
However, we stress that there is a legitimate reason a user would
want to use HIVE with only a single volume: even with a single
volume, HIVE offer stronger security than related work such as
TrueCrypt.
It provides security against multiple snapshot adver-
saries that could deduce information from the user’s access pattern.
For instance, between snapshots adversary A may see a particu-
lar ﬁle has been written that matches the size of a known ﬁle. A
does not need the decryption key then to determine what the ﬁle is.
Therefore, even without the encryption key, signiﬁcant data leak-
age can occur to a multiple snapshot adversary just by observing
patterns of changes.
This gives a plausible reason why a user would be using only a
single volume and hence allows for deniability in the case that they
actually do have hidden volumes.
Since HIVE acts as an Oblivious RAM for each of the volumes,
the user’s access pattern is hidden. Given our new write-only ORAM,
our approach is more efﬁcient than simply using existing ORAMs
not tailored for disk encryption.
6.
IMPLEMENTATION
To show its real-world practicality, we have implemented HIVE
for Linux. Our implementation comprises a kernel module offering
a virtual block device for each volume and a userland tool to man-
age these volumes. The source code is available for download [19].
The kernel module is built using device-mapper, a standard Linux
kernel framework for mapping block devices onto virtual devices,
also used to implement technologies such as LVM, dm-crypt and
software RAID.
Device-mapper allows placing HIVE between the Linux block
IO layer and the underlying device drivers. There, HIVE intercepts
all block IO requests in ﬂight, splits them into single-block-sized
chunks, remaps them to their new physical blocks on the disk, and
performs cryptography operations, as previously described. Note
Enqueue(Stashi,(b,d)) ;
Input: Block index b, data d, volume i, passwords P
1 if b(cid:54)=⊥ then
2
3 end
4 S :=,
such that βj
// Fetch blocks from S and put into stashes
$←{1,...,N}∧1≤ j ≤ k∧∀u,v : βu (cid:54)= βv} holds;
5 for u := 1 to k do
6
7
d := DiskRead(S[u]);
if d is block b from volume Vv then
// Derive κv from Pv
d := Decκv (d);
Enqueue(Stashv,(b,d)) ;
end
8
9
10
11 end
12 v := 1 ;
13 for u := 1 to max do
14
15
16
17
18
19 end
20 while v≤ k do
end
if v≤ k∧Stashu (cid:54)=∅ then
(b,d) := DeQueue(Stashv);
DiskWrite(S[v],Encκu (d));
v := v+1;
$←{0,1}B ;
r
DiskWrite(S[v],r);
21
22
23 end
24 M :=(cid:98) B
logN (cid:99) ;
25 for j := 1 to k do
26
// Fill remaining
blocks in S with random strings
if S[j] was ﬁlled with a real block then
// Let b(cid:48), i(cid:48) be the block
index and volume number of the block
that was written to disk block S[j]
mapblock := Map.Read(i(cid:48),(cid:98) b(cid:48)
mapblock[b(cid:48) modM ] :=S[j] ;
Map.HVEWrite(i(cid:48),(cid:98) b(cid:48)
M (cid:99)) ;
M (cid:99), mapblock);
// Do a dummy operation to the map
Map.HVEWrite(⊥,⊥,⊥,⊥);
Algorithm 6: HIVE HVEWrite(b,d,i,P)
27
28
29
30
31
32
33
34 end
end
else
end
Read and return most recent version of block i from Stashi ;
Input: Volume i, block index b
logN (cid:99) ;
1 if block b is in Stashi then
2
3 end
4 M :=(cid:98) B
5 mapblock := Map.HVERead(i,(cid:98) b
6 location := mapblock[b modM ] ;
7 d := DiskRead(location);
8 HVEWrite(⊥,⊥,⊥,⊥);
9 return Decκ(d)
// Do a “dummy” write
M (cid:99)) ;
Algorithm 7: HIVE HVERead(b,i)
that our implementation works on any block device (e.g., hard disks,
USB sticks, network block devices, etc.) since it stacks on top of
the actual device driver which communicates with hardware.
We use AES-CBC with 256 Bit keys for encryption and PBKDF2
for key derivation. For performance reasons, we generate random-
ness using RC4, using the kernel’s entropy pool only to generate
an initial key for RC4. Our implementation supports up to a 4 KB
logical block size (this limit is imposed by the x86 architecture and
kernel internals), regardless of the underlying hardware’s physical
structure. In our evaluation presented below, we set the block size
B to 4 KB, even though our test device has 512-byte sectors. This
Table 2: HIVE Benchmarks, L = 2, k = 3
Seq. Write Seq. Read Create
(Kﬁles/s)
82.29
1.57
(Kﬁles/s)
201.18
3.23
216.04
0.97
221.74
0.99
(MB/s)
(MB/s)
Stat
Delete
(Kﬁles/s)
105.10
1.79
Raw disk
HIVE
minimizes the number of random disk accesses performed during
IO and results in a signiﬁcant performance improvement. As an-
other performance optimization, our system disables IO reordering
and scheduling in the kernel for the virtual devices, because HIVE
always performs random device access and cannot beneﬁt from ker-
nel’s access pattern anticipation features.
The userland tool allows users to create, mount and unmount
HIVE devices /dev/mapper/HIVEi for volume Vi on top of
any other block device (e.g., /dev/sda). Upon receiving the cre-
ate command, our tool formats the speciﬁed device by creating the
necessary metadata structures, such as the max different Maps,
max stashes of ﬁxed size, IV s and the reverse mappings for data
blocks. Note that our implementation allows for recursion, so it
recursively stores and accesses the Maps of ﬁxed size as described
in Section 4.3. Finally, to mount or unmount these volumes, our
tool issues the appropriate ioctl commands to the kernel’s device-
mapper module.
Benchmarks: We have tested our implementation on a standard
desktop computer with an Intel i7-930 CPU, 9 GB RAM (although
RAM was not an issue during our evaluation), running Arch Linux
x86-64 with kernel 3.13.6. As the underlying block device, we have
used an off-the-shelf Samsung 840 EVO SSD.
For the evaluation, we used bonnie++, a standard disk and ﬁlesys-
tem benchmarking tool. Note that in the face of IO caching by
the OS, ﬁles created in the bonnie++ benchmarks must be set to
twice the size of system memory installed (9 GB in our case) to
reliably measure device performance. To speed up the total bench-
mark time, we modiﬁed bonnie++, ﬂushing IO buffers to the device
after running a benchmark, and signaling the kernel to drop page,
dentry, and inode caches before the next run. This ensured that
our performance measurements remained unaffected from caching.