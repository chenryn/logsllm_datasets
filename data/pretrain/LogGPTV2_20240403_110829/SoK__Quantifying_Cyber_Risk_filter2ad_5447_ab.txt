across the analysed sample. We sent the paper to an author of
each study and received no objections to our classiﬁcation.
III. CYBER HARM STUDIES
The section speaks to the frequency and impact of cyber
harm (RQ1). Classifying harm research using Figure 3 will
reveal that these studies infrequently consider the moderating
effect of security (Sp and Sr in Figure 3). With this in mind,
a secondary goal is to identify which data sources could be
used by mitigation studies in future work.
Table I gives an overview of empirical approaches to
quantifying cyber harm. Section III-A considers data sources
that collate public reports, whereas the studies in Section III-B
rely on researchers collecting private reports. Studies in Sec-
tion III-C extract data from publicly observable systems like
courts proceedings or stock markets. Section III-D considers
research into harms resulting from system wide events.
A. Publicly reported
Organisations report cyber
incidents to the public for
both strategic reasons and compliance with reporting require-
ments [72]. Data brokers aggregate these reports to create pay-
for-access databases, with some exceptions like Privacy Rights
Clearinghouse providing free access. Large organisations are
over-represented because their reports are more accessible.
Data breach studies Data breach studies only sample the
sub-population of ﬁrms who have suffered a breach, which
means harm is conditioned on a breach occurring. These stud-
ies estimate how the number of breached records is distributed.
We do not count estimating the frequency of breaches across
the entire US as investigating the probability of compromise
since these estimates provide little information to organisations
without knowing the population of possible victims [67]. Two
OVERVIEW OF DIFFERENT APPROACHES TO QUANTIFYING CYBER HARM.
TABLE I
# of Econ
studies loss
Sample Earliest Earliest
sample
study
size



600–6160
341–1579
2216
2008
2000
2015 < 2003
2016
2005
Data breach
Operational loss
Cyber incident
Unit of analysis
Public reports (Section III-A)
9
3
1
Private reports (Section III-B)
2
1
1
3
5
Internal incident
Insurance claim
Crime reports
Firm survey response
Individual survey response
Externally observed (Section III-C)
Legal case
Legal case
Bitcoin transaction
Criminal forum post
Insurance prices
Stock market reaction
70
7925
1800–23000



664–4209

 1500–64287
19–230
118
10m
13m
6828
43–542
2
1
3
2
1
19
1







2010
2019
2020
2012
2014
2011
2017
2014
2007
2019
2003
1996
2015
2017
2012
2010s
1999
2010
2009
2006
2007
1988
System-wide harm (Section III-D)
Multi-party incident
800
2019
2008
studies [43, 127] addressed this by using the population of
listed companies to estimate probability of breach, which is
an indicator of C in Figure 3.
Using the same public reports means each study can only
add data collected since the last study. Each researcher adopts
more sophisticated methods to justify publication. Breach sizes
were ﬁtted with: just 1 parameter in 2010 [83], 2 and 3
in 2016 [36, 127], 6+ in 2018 [131], and the endlessly
ﬂexible regression trees in a 2020 study [43]. On the one
hand, model sophistication identiﬁes relationships that simple
analyses cannot, such as Xu et al. [131] showing that the
expected magnitude of the next breach increases with the time
since the last breach. On the other hand, the proliferation of
statistical tests leads to contradictory results (see Table II).
There is no consensus on whether breach frequency/size
are stable over time (RQ3). They were shown to be decreas-
ing/stable [40], stable/stable [36], increasing/stable [83, 131],
and stable/increasing [127]. Many of the contradictions can
be explained by how the data is sliced. Breach size was only
found to be increasing in malicious breaches [128, 131] but
never for negligent breaches. Frequency was only found to
be increasing in the early years [30, 83] or in samples of
malicious breaches [23, 131].
In terms of RQ1, the shape parameter in the distribution of
breach size implies the expected number of breached records is
inﬁnite in some studies [43, 83] and ﬁnite in others [36, 131].
The possibility that the expected cost of a data breach is
inﬁnite raises two problems. First, the number of breached
records is bounded by the number of records held [127].
Second, it is unclear how this maps to ﬁnancial cost, which
mandatory reporting laws do not require organisations to
publish. The Jacobs Transform is frequently used to map the
number of records to a ﬁnancial cost [23, 36, 40, 43]. This
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:54 UTC from IEEE Xplore.  Restrictions apply. 
213
IS1
IS2
. . .
IS15
Threat
Preventive
security
IC1
IC2
Reactive
security
Sp(−)
C
T (+)
Es(+)
Compromise
Sr(−)
H
T|C(+)
Ea(+)
I3
Harm
IE1
IE2
Surface
exposure
I1
Asset
exposure
I2
Description of latent factors
Threat
The motivation, capability and activity of adversaries.
Surface exposure
Factors increasing potential vectors of compromise.
Preventive security
Interventions reducing the ease of compromise.
Compromise
Violation of victim security goal.
Asset exposure
Factors increasing the value of what can be compromised.
Reactive security
Interventions reducing the impact of compromise.
Harm
Negative consequences resulting from compromise.
Fig. 3. Describing the causes and correlates of cyber harm. The red arrows depict the model used by Tajalizadehkhoob et al. [116]. The blue arrows describe
a simpler model describing harm as indicated by I3 in terms of the type of compromise (I1) and an indicator of asset exposure (I2).
“transform” was derived in a blog post in which the author
warns the “amount of variance in the model is a serious
challenge to adoption” [66].
The predictive power of data breach studies is questionable.
In 2016, Edwards et al. [36, Fig. 9] estimated that
the
probability of seeing a breach of 200 million or more records
in the next 3 years had a probability of around 0.1. Wheatley
et al. [127] derived a maximum breach size of 200 million,
growing by 50% in the ﬁve years following 2016. Yahoo!
reported the loss of 3 billion customer records in the same
year as both publications (albeit lost years earlier). What do we
really know about data breaches when even methods designed
for tail events like extreme value theory [127] set bounds that
are exceeded by an order of magnitude within the same year
(with multiple breaches exceeding 500 million in the last 3
years)? The same authors [127] who derived the maximum
bound warned about “dragon kings” [108] emerging from
complex systems that risk models cannot predict.
Operational loss Much like data breach studies, op loss
studies only consider harm H but
in terms of ﬁnancial
loss. Two studies [13, 41] control for indicators of exposure
Ea like industry, revenue, and employee count. Surprisingly,
cyber operational losses are less heavy tailed than non-cyber
losses [13]. The mean loss is also smaller, which suggests
idiosyncratic cyber risk is not exceptional
in the class of
operational losses. A 2019 study [41, p. 10] supported the
ﬁnding by reporting that non-cyber losses had greater “mean,
standard deviation, median, skewness, and kurtosis”. However,
the authors report that the “tail risk measure” from [86, p. 283]
is higher for cyber losses. This provides another example of
model sophistication leading to contradictory results.
The ORX database [102] used in these studies comprises
publicly reported operational losses. Larger organisations are
over-represented in the data as they are more likely to suffer
losses exceeding the threshold ($100k) and more likely to have
the loss reported by a “major media source” [102]. The key-
word ﬁlters used to ﬁlter cyber losses introduces additional
noise. Only 25% of the losses in the study [41] were classiﬁed
as data breaches, so what are the rest? It is hard to say without
access to the proprietary data, but the largest cyber loss ($14.4
billion) resulted from a “money-laundering incident at the
Bank of China in February 2005” [41, f. n. 9]. If you squint
hard enough in the 21st century, anything can be a cyber loss.
Manually compiled The proprietary dataset used by Ro-
manosky [98] collected publicly reported incidents using
automated and manual methods. The results suggest cyber
incidents are “far less” [98] costly than losses like fraud,
theft, and bad debt when comparing medians and averages.
In terms of frequency, he observed that “Health Care and
Retail industries, however, suffer extremely low incident rates
of around 0.3% or less” [98, p. 125]. This is likely an under-
estimate because the denominator captures all ﬁrms in the US
census, whereas the numerator under reports losses of small
ﬁrms, which are unlikely to be reported publicly. Normalizing
a sample of publicly reported incidents is challenging because
reporting biases are unknown and thus so is the population
from which the sample was drawn.
B. Privately reported
Privately reported data must be collected directly from
the organisation, which creates the opportunity to collect a
representative sample as in surveys. In contrast, case studies
collect a convenience sample using a relationship with one
ﬁrm, which calls into question how well the results generalise.
Case studies Time to repair following a system failure
is an indicator of harm H. Franke et al. [46] estimate the
distribution of times to repair and suggest exploring the factors
inﬂuencing this as future work. Schroeder and Gibson [104]
show that both time to repair and frequency of failure depend
on system complexity, an indicator of surface exposure Es.
Both of these studies use internal data meaning n = 1 in terms
of organisations studied. The lack of consideration for security
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:54 UTC from IEEE Xplore.  Restrictions apply. 
214
THE OFTEN CONTRADICTORY FINDINGS FROM DATA BREACH STUDIES.
TABLE II
Breach frequency
Breach size
∞
Type of data
N + M (USA)
N + M (USA)
N + M (USA)
Reference
Curtin et al. (2008) [30]
Maillart et al. (2010) [83]
Edwards et al. (2016) [36]
Wheatley et al. (2016) [127] M (World)
Eling et al. (2017) [40]
Xu et al. (2018) [131]
Wheatley et al. (2019) [128]
Carfora et al. (2019) [23]
Farkas et al. (2020) [43]
N/M = Negligent/malicious breach, (t) = distribution of the tail, LM = linear model, DT = double truncated, * = without maximum, ? = not reported.
Negative binomial
Poisson gen LM → (USA)
Negative binomial
ARMA/GARCH
Negative binomial
Negative binomial (cid:37) (M)
Years
2005–07
2000–08
2005–15
2007–15
2005–15
2005–17
2005–17
2005–17
2005–19
N + M (USA)
M (USA)
N + M (USA)
N + M (USA)
N + M (USA)
Trend
(cid:37)
(cid:37)
→
(cid:38)
(cid:37)
→
n
899
956
2253
5365
2266
600
1713
5724
6160
?
Yes
No
Yes*
No
No