### III. CYBER HARM STUDIES

This section addresses the frequency and impact of cyber harm (RQ1). By classifying harm research using Figure 3, we observe that these studies rarely consider the moderating effect of security (Sp and Sr in Figure 3). Consequently, a secondary goal is to identify potential data sources for future mitigation studies.

**Table I: Overview of Different Approaches to Quantifying Cyber Harm**

| **Category** | **Data Source** | **Unit of Analysis** | **Number of Studies** | **Earliest Study Year** | **Sample Size** |
|--------------|-----------------|----------------------|-----------------------|------------------------|-----------------|
| Public Reports (Section III-A) | Data breach | Firm | 9 | 2008 | 600–6160 |
|              | Operational loss | Firm | 3 | 2000 | 341–1579 |
|              | Cyber incident | Firm | 1 | 2015 | 2216 |
| Private Reports (Section III-B) | Internal incident | Firm | 2 | 2016 | 70 |
|              | Insurance claim | Firm | 1 | 2019 | 7925 |
| Externally Observed (Section III-C) | Legal case | Case | 3 | 2012 | 1800–23000 |
|              | Bitcoin transaction | Transaction | 5 | 2014 | 664–4209 |
|              | Criminal forum post | Post | 1 | 2011 | 1500–64287 |
|              | Insurance prices | Price | 1 | 2017 | 19–230 |
|              | Stock market reaction | Reaction | 1 | 2007 | 118 |
| System-wide Harm (Section III-D) | Multi-party incident | Incident | 1 | 2019 | 800 |

### A. Publicly Reported Data

Organizations report cyber incidents to the public for strategic reasons and to comply with reporting requirements [72]. Data brokers aggregate these reports into pay-for-access databases, though some, like Privacy Rights Clearinghouse, provide free access. Large organizations are over-represented due to their more accessible reports.

#### Data Breach Studies
Data breach studies focus on firms that have experienced breaches, conditioning harm on the occurrence of a breach. These studies estimate the distribution of breached records. Estimating the frequency of breaches across the entire US does not provide useful information to organizations without knowing the population of possible victims [67].

Two studies [43, 127] used the population of listed companies to estimate the probability of a breach, which is an indicator of C in Figure 3. Each study adds new data since the last, employing increasingly sophisticated methods. For example, breach sizes were fitted with one parameter in 2010 [83], two and three parameters in 2016 [36, 127], six or more in 2018 [131], and regression trees in 2020 [43].

While model sophistication can identify nuanced relationships, such as Xu et al. [131] showing that the expected magnitude of the next breach increases with the time since the last breach, it also leads to contradictory results (see Table II). There is no consensus on whether breach frequency and size are stable over time (RQ3). Studies show various trends: decreasing/stable [40], stable/stable [36], increasing/stable [83, 131], and stable/increasing [127].

In terms of RQ1, the shape parameter in the distribution of breach size implies an infinite number of breached records in some studies [43, 83] and a finite number in others [36, 131]. The possibility of an infinite expected cost raises issues, as the number of records is bounded by the number held [127], and financial costs are not always reported. The Jacobs Transform is often used to map records to financial cost [23, 36, 40, 43], but its variance is a challenge to adoption [66].

#### Operational Loss Studies
Operational loss studies consider financial loss, controlling for exposure indicators like industry, revenue, and employee count [13, 41]. Surprisingly, cyber operational losses are less heavy-tailed than non-cyber losses [13], with smaller mean losses, suggesting idiosyncratic cyber risk is not exceptional. However, the "tail risk measure" from [86, p. 283] is higher for cyber losses, leading to contradictory results.

The ORX database [102] used in these studies comprises publicly reported operational losses, with larger organizations over-represented. Keyword filters introduce additional noise, and only 25% of the losses in one study [41] were classified as data breaches. The largest cyber loss ($14.4 billion) resulted from a money-laundering incident at the Bank of China in 2005 [41, f. n. 9].

### B. Privately Reported Data

Privately reported data must be collected directly from organizations, allowing for a representative sample through surveys. In contrast, case studies use convenience samples, raising questions about generalizability.

#### Case Studies
Case studies, such as those by Franke et al. [46] and Schroeder and Gibson [104], estimate the distribution of repair times and the factors influencing them. Both use internal data, limiting the generalizability and consideration of security measures.

### Conclusion

The analysis of publicly and privately reported data reveals significant variability and contradictions in the findings of cyber harm studies. Future research should aim to address these inconsistencies and explore more representative and robust data sources.