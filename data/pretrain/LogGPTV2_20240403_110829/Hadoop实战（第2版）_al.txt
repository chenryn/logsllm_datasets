＜?xml version="1.0"?＞
＜configuration＞
＜property＞
＜name＞io.file.buffer.size＜/name＞
＜value＞5000＜/value＞
＜description＞the size of buffer for use in sequence file.＜/description＞
＜/property＞
＜property＞
＜name＞height＜/name＞
＜value＞short＜/value＞
＜final＞true＜/final＞
＜/property＞
＜/configuration＞
使用两个资源configuation-default.xml和configuration-site.xml来定义配置。将资源按顺序添加到Configuration之中，代码如下：
Configuration conf=new Configuration（）；
conf.addResource（"configuration-default.xml"）；
conf.addResource（"|configuration-site.xml"）；
现在不同资源中有了相同属性，但是这些属性的取值却不一样。这时这些属性的取值应该如何确定呢？可以遵循这样一个原则：后添加进来的属性取值覆盖掉前面所添加资源中的属性取值。因此，此处的属性io.file.buffer.size取值应该是5000而不是先前的4096，即：
assertThat（conf.get（"io.file.buffer.size"），is（"5000"））；
但是，有一个特例，被标记为final的属性不能被后面定义的属性覆盖。Configuration-default.xml中的属性height被标记为final，因此在configuration-site.xml中重写height并不会成功，它依然会从configuration-default.xml中取值：
assertThat（conf.get（"height"），is（"tall"））；
重写标记为final的属性通常会报告配置错误，同时会有警告信息被记录下来以便为诊断所用。管理员将守护进程地址文件之中的属性标记为final，可防止用户在客户端配置文件中或作业提交参数中改变其取值。
Hadoop默认使用两个源进行配置，并按顺序加载core-default.xml和core-site.xml。在实际应用中可能会添加其他的源，应按照它们添加的顺序进行加载。其中core-default.xml用于定义系统默认的属性，core-site.xml用于定义在特定的地方重写。
[1]
 可以参考http：//hadoop.apache.org/common/docs/current/api。
4.2 配置开发环境
首先下载准备使用的Hadoop版本，然后将其解压到用于开发的主机上（详细过程见附录B）。接下来，在集成开发环境中创建一个新的工程，然后将解压后的文件夹根目录下的JAR文件和lib目录之下的JAR文件加入到classpath中。之后就可以编译Hadoop程序，并且可以在集成开发环境中以本地模式运行。
Hadoop有三种不同的运行方式：单机模式、伪分布模式、完全分布模式。三种不同的运行方式各有各的好处与不足之处：单机模式的安装与配置比较简单，运行在本地文件系统上，便于程序的调试，可及时查看程序运行的效果，但是当数据量比较大时运行的速度会比较慢，并且没有体现出Hadoop分布式的优点；伪分布模式同样是在本地文件系统上运行，与单机模式的不同之处在于它运行的文件系统为HDFS，这种模式的好处是能够模仿完全分布模式，看到一些分布式处理的效果；完全分布模式则运行在多台机器的HDFS之上，完完全全地体现出了分布式的优点，但是在调试程序方面会比较麻烦。
在实际运用中，可以结合这三种不同模式的优点，比如，编写和调试程序在单机模式和伪分布模式上进行，而实际处理大数据则在完全分布模式下进行。这样就会涉及三种不同模式的配置与管理，相关配置和管理会在相应的章节重点讲解。
4.3 编写MapReduce程序
下面将通过一个计算学生平均成绩的例子来讲解开发MapReduce程序的流程。程序主要包括两部分内容：Map部分和Reduce部分，分别实现Map和Reduce的功能。
 4.3.1 Map处理
Map处理的是一个纯文本文件，此文件中存放的数据是每一行表示一个学生的姓名和他相应的一科成绩，如果有多门学科，则每个学生就存在多行数据。代码如下所示：
public static class Map
extends Mapper＜LongWritable, Text, Text, IntWritable＞{
public void map（LongWritable key, Text value, Context context）
throws IOException, InterruptedException{
String line=value.toString（）；//将输入的纯文本文件的数据转化成String
System.out.println（line）；//为了便于程序的调试，输出读入的内容
//将输入的数据先按行进行分割
StringTokenizer tokenizerArticle=new StringTokenizer（line，"\n"）；
//分别对每一行进行处理
while（tokenizerArticle.hasMoreTokens（））{
//每行按空格划分
StringTokenizer tokenizerLine=new StringTokenizer（tokenizerArticle.nextToken（））；
String strName=tokenizerLine.nextToken（）；//学生姓名部分
String strScore=tokenizerLine.nextToken（）；//成绩部分
Text name=new Text（strName）；//学生姓名
int scoreInt=Integer.parseInt（strScore）；//学生成绩score of student
context.write（name, new IntWritable（scoreInt））；//输出姓名和成绩
}
}
}
通过数据集进行测试，结果显示完全可以将文件中的姓名和他相应的成绩提取出来。需要解释的是：Mapper处理的数据是由InputFormat分解过的数据集，其中InputFormat的作用是将数据集切割成小数据集InputSplit，每一个InputSplit将由一个Mapper负责处理。此外，InputFormat中还提供了一个RecordReader的实现，并将一个InputSplit解析成＜key, value＞对提供给Map函数。InputFormat的默认值是TextInputFormat，它针对文本文件，按行将文本切割成InputSplit，并用LineRecordReader将InputSplit解析成＜key, value＞对，key是行在文本中的位置，value是文件中的一行。
本程序中的InputFormat使用的是默认值TextInputFormat，因此结合上述程序的注释部分不难理解整个程序的处理流程和正确性。
4.3.2 Reduce处理
Map处理的结果会通过partition分发到Reducer, Reducer做完Reduce操作后，将通过OutputFormat输出结果，代码如下：
public static class Reduce
extends Reducer＜Text, IntWritable, Text, IntWritable＞{
public void reduce（Text key, Iterable＜IntWritable＞values，
Context context）throws IOException, InterruptedException{
int sum=0；
int count=0；
Iterator＜IntWritable＞iterator=values.iterator（）；
while（iterator.hasNext（））{
sum+=iterator.next（）.get（）；//计算总分
count++；//统计总的科目数
}
int average=（int）sum/count；//计算平均成绩
context.write（key, new IntWritable（average））；
}
}
Mapper最终处理的结果＜key, value＞对会被送到Reducer中进行合并，在合并的时候，有相同key的键/值对会被送到同一个Reducer上。Reducer是所有用户定制Reducer类的基类，它的输入是key及这个key对应的所有value的一个迭代器，还有Reducer的上下文。Reduce处理的结果将通过Reducer.Context的write方法输出到文件中。
4.4 本地测试
Score_Process类继承于Configured的实现接口Tool，上述的Map和Reduce是Score_Process的内部类，它们分别实现了Map和Reduce功能，主函数存在于Score_Process中。下面创建一个Score_Process实例对程序进行测试。
Score_process的run（）方法的实现如下：
public int run（String[]args）throws Exception{
Job job=new Job（getConf（））；
job.setJarByClass（Score_Process.class）；
job.setJobName（"Score_Process"）；
job.setOutputKeyClass（Text.class）；
job.setOutputValueClass（IntWritable.class）；
job.setMapperClass（Map.class）；
job.setCombinerClass（Reduce.class）；
job.setReducerClass（Reduce.class）；
job.setInputFormatClass（TextInputFormat.class）；
job.setOutputFormatClass（TextOutputFormat.class）；
FileInputFormat.setInputPaths（job, new Path（args[0]））；
FileOutputFormat.setOutputPath（job, new Path（args[1]））；
boolean success=job.waitForCompletion（true）；
return success?0：1；
}
下面给出main（）函数，对程序进行测试：
public static void main（String[]args）throws Exception{
int ret=ToolRunner.run（new Score_Process（），args）；
System.exit（ret）；
}
如果程序要在Eclipse中执行，那么用户需要在run congfiguration中设置好参数，输入的文件夹名为input，输出的文件夹名为output。
4.5 运行MapReduce程序
想要测试人体的健康状况，要先知道人体各个组织的健康状况，然后再综合评价人体的健康状况。假设每个组织的健康指标是一个0～100之间的数字，得到综合身体健康状况的方法是计算所有组织健康指标的平均数。由于测试的人数众多，因此存储数据的格式为：姓名+得分+#（代表一个人单个人体组织的健康状况），每个组织的健康状况分别用一个文件存储。现在一共有1000个组织参与了评估，即用1000个文件分别存储。
由于此例中对数据的处理与前面对学生成绩进行的简单处理有一些区别，下面先将程序的主要部分列举出来。
Mapper部分的代码如下：
public static class Map
extends Mapper＜LongWritable, Text, Text, IntWritable＞{
public void map（LongWritable key, Text value, Context context）
throws IOException, InterruptedException{
String line=value.toString（）；
//以“#”为分隔符，将输入的文件分割成单个记录
StringTokenizer tokenizerArticle=new StringTokenizer（line，"#"）；
//对每个记录进行处理
while（tokenizerArticle.hasMoreTokens（））{
//将每个记录分成姓名和分数两个部分
StringTokenizer tokenizerLine=new StringTokenizer（tokenizerArticle.nextToken（））；
while（tokenizerLine.hasMoreTokens（））{
String strName=tokenizerLine.nextToken（）；
if（tokenizerLine.hasMoreTokens（））{
String strScore=tokenizerLine.nextToken（）；
Text name=new Text（strName）；//姓名
int scoreInt=Integer.parseInt（strScore）；//该组织的状况得分
context.write（name, new IntWritable（scoreInt））；
}
}
}
}
上述程序比较简单，和单节点上的代码也很相似，配合注释就能够很好地理解，因此就不再多讲解了。
下面是Reducer部分的代码：
public static class Reduce
extends Reducer＜Text, IntWritable, Text, IntWritable＞{
public void reduce（Text key, Iterable＜IntWritable＞values，
Context context）throws IOException, InterruptedException{
int sum=0；
int count=0；
Iterator＜IntWritable＞iterator=values.iterator（）；
while（iterator.hasNext（））{
sum+=iterator.next（）.get（）；
count++；
}
int average=（int）sum/count；
context.write（key, new IntWritable（average））；
}
}
 4.5.1 打包
为了能够在命令行中运行程序，首先需要对它进行编译和打包，下面就分别展示编译和打包的过程。
编译代码如下：