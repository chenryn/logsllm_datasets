Evolving the Simple PRR Model: Early Engagement  |  449Build and implementation
The Build phase addresses production aspects such as instrumentation and metrics, operational and emergency controls, resource usage, and efficiency. During this phase, SRE can influence and improve the implementation by recommending specific existing libraries and components, or helping build certain controls into the system. SRE participation at this stage helps enable ease of operations in the future and allows SRE to gain operational experience in advance of the launch.LaunchSRE can also help implement widely used launch patterns and controls. For example, SRE might help implement a “dark launch” setup, in which part of the traffic from existing users is sent to the new service in addition to being sent to the live produc‐tion service. The responses from the new service are “dark” since they are thrown away and not actually shown to users. Practices such as dark launches allow the team to gain operational insight, resolve issues without impacting existing users, and reduce the risk of encountering issues after launch. A smooth launch is immensely helpful in keeping the operational burden low and maintaining the development momentum after the launch. Disruptions around launch can easily result in emer‐gency changes to source code and production, and disrupt the development team’s work on future features.Post-launch
Having a stable system at launch time generally leads to fewer conflicting priorities for the development team in terms of choosing between improving service reliability versus adding new features. In later phases of the service, the lessons from earlier phases can better inform refactoring or redesign.With extended involvement, the SRE team can be ready to take over the new service much sooner than is possible with the Simple PRR Model. The longer and closer engagement between the SRE and development teams also creates a collaborative relationship that can be sustained long term. A positive cross-team relationship fos‐ters a mutual feeling of solidarity, and helps SRE establish ownership of the produc‐tion responsibility.Disengaging from a service
Sometimes a service doesn’t warrant full-fledged SRE team management—this deter‐mination might be made post-launch, or SRE might engage with a service but never officially take it over. This is a positive outcome, because the service has been engi‐neered to be reliable and low maintenance, and can therefore remain with the devel‐opment team.
450  |  Chapter 32: The Evolving SRE Engagement ModelIt is also possible that SRE engages early with a service that fails to meet the levels of usage projected. In such cases, the SRE effort spent is simply part of the overall busi‐ness risk that comes with new projects, and a small cost relative to the success of projects that meet expected scale. The SRE team can be reassigned, and lessons learned can be incorporated into the engagement process.Evolving Services Development: Frameworks and SRE Platform
The Early Engagement Model made strides in evolving SRE engagement beyond the Simple PRR Model, which applied only to services that had already launched. How‐ever, there was still progress to be made in scaling SRE engagement to the next level by designing for reliability.
Lessons Learned
Over time, the SRE engagement model described thus far produced several distinct patterns:• Onboarding each service required two or three SREs and typically lasted two or three quarters. The lead times for a PRR were relatively high (quarters away). The effort level required was proportional to the number of services under review, and was constrained by the insufficient number of SREs available to conduct PRRs. These conditions led to serialization of service takeovers and strict service prioritization.• Due to differing software practices across services, each production feature was implemented differently. To meet PRR-driven standards, features usually had to be reimplemented specifically for each service or, at best, once for each small sub‐set of services sharing code. These reimplementations were a waste of engineer‐ing effort. One canonical example is the implementation of functionally similar logging frameworks repeatedly in the same language because different services didn’t implement the same coding structure.• A review of common service issues and outages revealed certain patterns, but there was no way to easily replicate fixes and improvements across services. Typi‐cal examples included service overload situations and data hot-spotting.
• SRE software engineering contributions were often local to the service. Thus, building generic solutions to be reused was difficult. As a consequence, there was no easy way to implement new lessons individual SRE teams learned and best practices across services that had already been onboarded.Evolving Services Development: Frameworks and SRE Platform  |  451
External Factors Affecting SRE
External factors have traditionally pressured the SRE organization and its resources in several ways.Google is increasingly following the industry trend of moving toward microservices.1 As a result, both the number of requests for SRE support and the cardinality of serv‐ices to support have increased. Because each service has a base fixed operational cost, even simple services demand more staffing. Microservices also imply an expectation of lower lead time for deployment, which was not possible with the previous PRR model (which had a lead time of months).Hiring experienced, qualified SREs is difficult and costly. Despite enormous effort from the recruiting organization, there are never enough SREs to support all the serv‐ices that need their expertise. Once SREs are hired, their training is also a lengthier process than is typical for development engineers.Finally, the SRE organization is responsible for serving the needs of the large and growing number of development teams that do not already enjoy direct SRE support. This mandate calls for extending the SRE support model far beyond the original con‐cept and engagement model.
Toward a Structural Solution: Frameworks
To effectively respond to these conditions, it became necessary to develop a model that allowed for the following principles:Codified best practices 
The ability to commit what works well in production to code, so services can simply use this code and become “production ready” by design.
Reusable solutions 
Common and easily shareable implementations of techniques used to mitigate scalability and reliability issues.
A common production platform with a common control surfaceUniform sets of interfaces to production facilities, uniform sets of operational controls, and uniform monitoring, logging, and configuration for all services.
Easier automation and smarter systems 
A common control surface that enables automation and smart systems at a level not possible before. For example, SREs can readily receive a single view of
1 See the Wikipedia page on microservices at .452  |  Chapter 32: The Evolving SRE Engagement Model
relevant information for an outage, rather than hand collecting and analyzing mostly raw data from disparate sources (logs, monitoring data, and so on).Based upon these principles, a set of SRE-supported platform and service frameworks were created, one for each environment we support (Java, C++, Go). Services built using these frameworks share implementations that are designed to work with the SRE-supported platform, and are maintained by both SRE and development teams. The main shift brought about by frameworks was to enable product development teams to design applications using the framework solution that was built and blessed by SRE, as opposed to either retrofitting the application to SRE specifications after the fact, or retrofitting more SREs to support a service that was markedly different than other Google services.An application typically comprises some business logic, which in turn depends on various infrastructure components. SRE production concerns are largely focused on the infrastructure-related parts of a service. The service frameworks implement infra‐structure code in a standardized fashion and address various production concerns. Each concern is encapsulated in one or more framework modules, each of which pro‐vides a cohesive solution for a problem domain or infrastructure dependency. Frame‐work modules address the various SRE concerns enumerated earlier, such as:• Instrumentation and metrics
• Request logging
• Control systems involving traffic and load management
SRE builds framework modules to implement canonical solutions for the concerned production area. As a result, development teams can focus on the business logic, because the framework already takes care of correct infrastructure use.A framework essentially is a prescriptive implementation for using a set of software components and a canonical way of combining these components. The framework can also expose features that control various components in a cohesive manner. For example, a framework might provide the following:
• Business logic organized as well-defined semantic components that can be refer‐	enced using standard terms• Standard dimensions for monitoring instrumentation
• A standard format for request debugging logs
• A standard configuration format for managing load shedding
• Capacity of a single server and determination of “overload” that can both use a 	semantically consistent measure for feedback to various control systems
Evolving Services Development: Frameworks and SRE Platform  |  453Frameworks provide multiple upfront gains in consistency and efficiency. They free developers from having to glue together and configure individual components in an ad hoc service-specific manner, in ever-so-slightly incompatible ways, that then have to be manually reviewed by SREs. They drive a single reusable solution for produc‐tion concerns across services, which means that framework users end up with the same common implementation and minimal configuration differences.Google supports several major languages for application development, and frame‐works are implemented across all of these languages. While different implementa‐tions of the framework (say in C++ versus Java) can’t share code, the goal is to expose the same API, behavior, configuration, and controls for identical functionality. There‐fore, development teams can choose the language platform that fits their needs and experience, while SREs can still expect the same familiar behavior in production and standard tools to manage the service.New Service and Management Benefits
The structural approach, founded on service frameworks and a common production platform and control surface, provided a host of new benefits.
Significantly lower operational overhead
A production platform built on top of frameworks with stronger conventions signifi‐cantly reduced operational overhead, for the following reasons:• It supports strong conformance tests for coding structure, dependencies, tests, coding style guides, and so on. This functionality also improves user data privacy, testing, and security conformance.
• It features built-in service deployment, monitoring, and automation for all 	services.
• It facilitates easier management of large numbers of services, especially micro-	services, which are growing in number.• It enables much faster deployment: an idea can graduate to fully deployed SRE-	level production quality in a matter of days!
Universal support by designThe constant growth in the number of services at Google means that most of these services can neither warrant SRE engagement nor be maintained by SREs. Regardless, services that don’t receive full SRE support can be built to use production features that are developed and maintained by SREs. This practice effectively breaks the SRE staffing barrier. Enabling SRE-supported production standards and tools for all teams improves the overall service quality across Google. Furthermore, all services that are454  |  Chapter 32: The Evolving SRE Engagement Model
implemented with frameworks automatically benefit from improvements made over time to frameworks modules.
Faster, lower overhead engagements
The frameworks approach results in faster PRR execution because we can rely upon:
• Built-in service features as part of the framework implementation
• Faster service onboarding (usually accomplished by a single SRE during one 	quarter)• Less cognitive burden for the SRE teams managing services built using 	frameworks
These properties allow SRE teams to lower the assessment and qualification effort for service onboarding, while maintaining a high bar on service production quality.
A new engagement model based on shared responsibility
The original SRE engagement model presented only two options: either full SRE sup‐port, or approximately no SRE engagement.2A production platform with a common service structure, conventions, and software infrastructure made it possible for an SRE team to provide support for the “platform”infrastructure, while the development teams provide on-call support for functional issues with the service—that is, for bugs in the application code. Under this model, SREs assume responsibility for the development and maintenance of large parts of service software infrastructure, particularly control systems such as load shedding, overload, automation, traffic management, logging, and monitoring.This model represents a significant departure from the way service management was originally conceived in two major ways: it entails a new relationship model for the interaction between SRE and development teams, and a new staffing model for SRE-supported service management.3
2 Occasionally, there were consulting engagements by SRE teams with some non-onboarded services, but con‐	sultations were a best-effort approach and limited in number and scope.3 The new model of service management changes the SRE staffing model in two ways: (1) because a lot of ser‐vice technology is common, it reduces the number of required SREs per service; (2) it enables the creation of production platforms with separation of concerns between production platform support (done by SREs) and service-specific business-logic support, which remains with the development team. These platforms teams are staffed based upon the need to maintain the platform rather than upon service count, and can be shared across products.Evolving Services Development: Frameworks and SRE Platform  |  455
Conclusion
Service reliability can be improved through SRE engagement, in a process that includes systematic review and improvement of its production aspects. Google SRE’s initial such systematic approach, the Simple Production Readiness Review, made strides in standardizing the SRE engagement model, but was only applicable to serv‐ices that had already entered the Launch phase.Over time, SRE extended and improved this model. The Early Engagement Model involved SRE earlier in the development lifecycle in order to “design for reliability.”As demand for SRE expertise continued to grow, the need for a more scalable engage‐ment model became increasingly apparent. Frameworks for production services were developed to meet this demand: code patterns based on production best practices were standardized and encapsulated in frameworks, so that use of frameworks became a recommended, consistent, and relatively simple way of building production-ready services.All three of the engagement models described are still practiced within Google. How‐ever, the adoption of frameworks is becoming a prominent influence on building production-ready services at Google as well as profoundly expanding the SRE contri‐bution, lowering service management overhead, and improving baseline service qual‐ity across the organization.
456  |  Chapter 32: The Evolving SRE Engagement ModelPART V
Conclusions
Having covered much ground in terms of how SRE works at Google, and how the principles and practices we’ve developed might be applied to other organizations in our field, it now seems appropriate to turn our view to Chapter 33, Lessons Learned from Other Industries, to examine how SRE’s practices compare to other industries where reliability is critically important.Finally, Google’s VP for Site Reliability Engineering, Benjamin Lutch, writes about SRE’s evolution over the course of his career in his conclusion, examining SRE through the lens of some observations on the aviation industry.
CHAPTER 33
Lessons Learned from Other Industries
Written by Jennifer Petoff 
Edited by Betsy BeyerA deep dive into SRE culture and practices at Google naturally leads to the question of how other industries manage their businesses for reliability. Compiling this book on Google SRE created an opportunity to speak to a number of Google’s engineers about their previous work experiences in a variety of other high-reliability fields in order to address the following comparative questions:• Are the principles used in Site Reliability Engineering also important outside of Google, or do other industries tackle the requirements of high reliability in mark‐edly different ways?
• If other industries also adhere to SRE principles, how are the principles manifes‐	ted?
• What are the similarities and differences in the implementation of these princi‐	ples across industries?• What factors drive similarities and differences in implementation?
• What can Google and the tech industry learn from these comparisons?
A number of principles fundamental to Site Reliability Engineering at Google are dis‐cussed throughout this text. To simplify our comparison of best practices in other industries, we distilled these concepts into four key themes:
• Preparedness and Disaster Testing• Preparedness and Disaster Testing
• Postmortem Culture
459
• Automation and Reduced Operational Overhead
• Structured and Rational Decision Making
This chapter introduces the industries that we profiled and the industry veterans we interviewed. We define key SRE themes, discuss how these themes are implemented at Google, and give examples of how these principles reveal themselves in other industries for comparative purposes. We conclude with some insights and discussion on the patterns and anti-patterns we discovered.Meet Our Industry Veterans
Peter Dahl is a Principal Engineer at Google. Previously, he worked as a defense con‐tractor on several high-reliability systems including many airborne and wheeled vehi‐cle GPS and inertial guidance systems. Consequences of a lapse in reliability in such systems include vehicle malfunction or loss, and the financial consequences associ‐ated with that failure.Mike Doherty is a Site Reliability Engineer at Google. He worked as a lifeguard and lifeguard trainer for a decade in Canada. Reliability is absolutely essential by nature in this field, because lives are on the line every day.Erik Gross is currently a software engineer at Google. Before joining the company, he spent seven years designing algorithms and code for the lasers and systems used to perform refractive eye surgery (e.g., LASIK). This is a high-stakes, high-reliability field, in which many lessons relevant to reliability in the face of government regula‐tions and human risk were learned as the technology received FDA approval, gradu‐ally improved, and finally became ubiquitous.Gus Hartmann and Kevin Greer have experience in the telecommunications indus‐try, including maintaining the E911 emergency response system.1 Kevin is currently a software engineer on the Google Chrome team and Gus is a systems engineer for Google’s Corporate Engineering team. User expectations of the telecom industry demand high reliability. Implications of a lapse of service range from user inconven‐ience due to a system outage to fatalities if E911 goes down.Ron Heiby is a Technical Program Manager for Site Reliability Engineering at Goo‐gle. Ron has experience in development for cell phones, medical devices, and the automotive industry. In some cases he worked on interface components of these industries (for example, on a device to allow EKG readings2 in ambulances to be transmitted over the digital wireless phone network). In these industries, the impact1 E911 (Enhanced 911): Emergency response line in the US that leverages location data.
2 Electrocardiogram readings: .
460  |  Chapter 33: Lessons Learned from Other Industries
of a reliability issue can range from harm to the business incurred by equipment recalls to indirectly impacting life and health (e.g., people not getting the medical attention they need if the EKG cannot communicate with the hospital).Adrian Hilton is a Launch Coordination Engineer at Google. Previously, he worked on UK and USA military aircraft, naval avionics and aircraft stores management sys‐tems, and UK railway signaling systems. Reliability is critical in this space because impact of incidents ranges from multimillion-dollar loss of equipment to injuries and fatalities.Eddie Kennedy is a project manager for the Global Customer Experience team at Google and a mechanical engineer by training. Eddie spent six years working as a Six Sigma Black Belt process engineer in a manufacturing facility that makes synthetic diamonds. This industry is characterized by a relentless focus on safety, because the extremes of temperature and pressure demands of the process pose a high level of danger to workers on a daily basis.John Li is currently a Site Reliability Engineer at Google. John previously worked as a systems administrator and software developer at a proprietary trading company in the finance industry. Reliability issues in the financial sector are taken quite seriously because they can lead to serious fiscal consequences.Dan Sheridan is a Site Reliability Engineer at Google. Before joining the company, he worked as a safety consultant in the civil nuclear industry in the UK. Reliability is important in the nuclear industry because an incident can have serious repercussions: outages can incur millions a day in lost revenue, while risks to workers and those in the community are even more dire, dictating zero tolerance for failure. Nuclear infra‐structure is designed with a series of failsafes that halt operations before an incident of any such magnitude is reached.Jeff Stevenson is currently a hardware operations manager at Google. He has past experience as a nuclear engineer in the US Navy on a submarine. Reliability stakes in the nuclear Navy are high—problems that arise in the case of incidents range from damaged equipment, to long-standing environmental impact, to potential loss of life.Matthew Toia is a Site Reliability Manager focused on storage systems. Prior to Goo‐gle, he worked on software development and deployment of air traffic control soft‐ware systems. Effects from incidents in this industry range from inconveniences to passengers and airlines (e.g., delayed flights, diverted planes) to potential loss of life in the event of a crash. Defense in depth is a key strategy to avoiding catastrophic failures.Now that you’ve met our experts and gained a high-level understanding of why relia‐bility is important in their respective former fields, we’ll delve into the four key themes of reliability.
Meet Our Industry Veterans  |  461
Preparedness and Disaster Testing“Hope is not a strategy.” This rallying cry of the SRE team at Google sums up what we mean by preparedness and disaster testing. The SRE culture is forever vigilant and constantly questioning: What could go wrong? What action can we take to address those issues before they lead to an outage or data loss? Our annual Disaster and Recovery Testing (DiRT) drills seek to address these questions head-on [Kri12]. In DiRT exercises, SREs push production systems to the limit and inflict actual outages in order to:• Ensure that systems react the way we think they will
• Determine unexpected weaknesses
• Figure out ways to make the systems more robust in order to prevent uncontrol‐	led outages
Several strategies for testing disaster readiness and ensuring preparedness in other industries emerged from our conversations. Strategies included the following:
• Relentless organizational focus on safety
• Attention to detail• Attention to detail
• Swing capacity
• Simulations and live drills
• Training and certification
• Obsessive focus on detailed requirements gathering and design
• Defense in depth
Relentless Organizational Focus on SafetyThis principle is particularly important in an industrial engineering context. Accord‐ing to Eddie Kennedy, who worked on a manufacturing floor where workers faced safety hazards, “every management meeting started with a discussion of safety.” The manufacturing industry prepares itself for the unexpected by establishing highly defined processes that are strictly followed at every level of the organization. It is crit‐ical that all employees take safety seriously, and that workers feel empowered to speak up if and when anything seems amiss. In the case of nuclear power, military aircraft, and railway signaling industries, safety standards for software are well detailed (e.g., UK Defence Standard 00-56, IEC 61508, IEC513, US DO-178B/C, and DO-254) and462  |  Chapter 33: Lessons Learned from Other Industries
levels of reliability for such systems are clearly identified (e.g., Safety Integrity Level (SIL) 1–4),3 with the aim of specifying acceptable approaches to delivering a product.
Attention to DetailFrom his time spent in the US Navy, Jeff Stevenson recalls an acute awareness of how a lack of diligence in executing small tasks (for example, lube oil maintenance) could lead to major submarine failure. A very small oversight or mistake can have big effects. Systems are highly interconnected, so an accident in one area can impact mul‐tiple related components. The nuclear Navy focuses on routine maintenance to ensure that small issues don’t snowball.Swing CapacitySystem utilization in the telecom industry can be highly unpredictable. Absolute capacity can be strained by unforeseeable events such as natural disasters, as well as large, predictable events like the Olympics. According to Gus Hartmann, the industry deals with these incidents by deploying swing capacity in the form of a SOW (switch on wheels), a mobile telco office. This excess capacity can be rolled out in an emer‐gency or in anticipation of a known event that is likely to overload the system. Capacity issues veer into the unexpected in matters unrelated to absolute capacity, as well. For example, when a celebrity’s private phone number was leaked in 2005 and thousands of fans simultaneously attempted to call her, the telecom system exhibited symptoms similar to a DDoS or massive routing error.Simulations and Live DrillsGoogle’s Disaster Recovery tests have a lot in common with the simulations and live drills that are a key focus of many of the established industries we researched. The potential consequences of a system outage determine whether using a simulation or a live drill is appropriate. For example, Matthew Toia points out that the aviation indus‐try can’t perform a live test “in production” without putting equipment and passen‐gers at risk. Instead, they employ extremely realistic simulators with live data feeds, in which the control rooms and equipment are modeled down to the tiniest details to ensure a realistic experience without putting real people at risk. Gus Hartmann reports that the telecom industry typically focuses on live drills centered on surviving hurricanes and other weather emergencies. Such modeling led to the creation of weatherproof facilities with generators inside the building capable of outlasting a storm.3 