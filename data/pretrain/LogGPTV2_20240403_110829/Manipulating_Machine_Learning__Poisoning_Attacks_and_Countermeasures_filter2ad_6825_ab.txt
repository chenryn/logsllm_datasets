2) Ridge regression, which uses (cid:3)2-norm regularization
3) LASSO, which uses (cid:3)1-norm regularization Ω(w) =
Ω(w) = 1
(cid:3)w(cid:3)1;
4) Elastic-net regression, which uses a combination of (cid:3)1-
norm and (cid:3)2-norm regularization Ω(w) = ρ(cid:3)w(cid:3)1 +(1−
2, where ρ ∈ (0, 1) is a conﬁgurable parameter,
2(cid:3)w(cid:3)2
ρ) 1
commonly set to 0.5 (as we do in this work).
When designing a poisoning attack, we consider two met-
rics for quantifying the effectiveness of the attack. First,
we measure the success rate of the poisoning attack by the
difference in testing set MSE of the corrupted model compared
to the legitimate model (trained without poisoning). Second,
we consider the running time of the attack.
A. Adversarial model
We provide here a detailed adversarial model for poisoning
attacks against regression algorithms, inspired from previous
work in [4], [25], [38], [54]. The model consists of deﬁning
the adversary’s goal, knowledge of the attacked system, and
capability of manipulating the training data,
to eventually
deﬁne an optimal poisoning attack strategy.
Adversary’s Goal. The goal of the attacker is to corrupt
the learning model generated in the training phase, so that
predictions on new data will be modiﬁed in the testing phase.
The attack is considered a poisoning availability attack, if
its goal is to affect prediction results indiscriminately, i.e., to
cause a denial of service. It is instead referred to as a poisoning
integrity attack, if the goal is to cause speciﬁc mis-predictions
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
the inner optimization problem. In poisoning integrity attacks,
the attacker’s loss W can be evaluated only on the points of
interest (for which the attacker aims to cause mis-predictions at
test time), while in poisoning availability attacks it is computed
on an untainted set of data points, indiscriminately. In the
(cid:2)
p are
black-box setting, the poisoned regression parameters θ
tr instead of Dtr.
estimated using the substitute training data D(cid:3)
In the remainder of this work, we only focus on poisoning
availability attacks against regression learning, and on defend-
ing against them, as those have been mainly investigated in the
literature of poisoning attacks. We highlight anyway again that
poisoning integrity attacks can be implemented using the same
technical derivation presented in this work, and leave a more
detailed investigation of their effectiveness to future work.
III. ATTACK METHODOLOGY
In this
section, we ﬁrst discuss previously-proposed
gradient-based optimization approaches to solving Prob-
lem (2)-(3) in classiﬁcation settings. In Sect. III-A, we discuss
how to adapt them to the case of regression learning, and
propose novel strategies to further improve their effectiveness.
Notably, since these attacks have been originally proposed
in the context of classiﬁcation problems, the class label of
the attack sample is arbitrarily initialized and then kept ﬁxed
during the optimization procedure (recall that y is a categorical
variable in classiﬁcation). As we will demonstrate in the
remainder of this work, a signiﬁcant improvement we propose
here to the current attack derivation is to simultaneously opti-
mize the response variable of each poisoning point along with
its feature values. We subsequently highlight some theoretical
insights on how each poisoning sample is updated during
the gradient-based optimization process. This will
lead us
to develop a much faster attack, presented in Sect. III-B,
which only leverages some statistical properties of the data
and requires minimal black-box access to the targeted model.
A. Optimization-based Poisoning Attacks
Previous work has considered solving Problem (2)-(3) by
iteratively optimizing one poisoning sample at a time through
gradient ascent [5], [36], [38], [54]. An exemplary algorithm
is given as Algorithm 1. We denote with xc the feature
vector of the attack point being optimized, and with yc its
response variable (categorical for classiﬁcation problems). In
particular, in each iteration, the algorithm optimizes all points
in Dp, by updating their feature vectors one at a time. As
reported in [54], the vector xc can be updated through a line
W of the outer
search along the direction of the gradient ∇xc
objective W (evaluated at the current poisoned solution) with
respect to the poisoning point xc (cf. line 7 in Algorithm 1).
Note that this update step should also enforce xc to lie within
the feasible domain (e.g., xc ∈ [0, 1]d), which can be typically
achieved through simple projection operators [5], [38], [54].
The algorithm terminates when no sensible change in the outer
objective W is observed.
Gradient Computation. The aforementioned algorithm is es-
sentially a standard gradient-ascent algorithm with line search.
tr (black-box), D(cid:3), L, W,
p
c=1, a
p = (xc, yc)
(cid:8)
)
(i+1)
Algorithm 1 Poisoning Attack Algorithm
Input: D = Dtr (white-box) or D(cid:3)
the initial poisoning attack samples D(0)
small positive constant ε.
1: i ← 0 (iteration counter)
2: θ
3: repeat
4:
5:
6:
7:
(i) ← arg minθ L(D ∪ D(i)
w(i) ← W(D(cid:3), θ
(i+1) ← θ
θ
for c = 1, . . . , p do
p , θ)
(cid:7)
(i+1)
(i)
(i)
)
xc
8:
9:
c ← line search
(i),∇xc
x
(i+1) ← arg minθ L(D ∪ D(i+1)
θ
w(i+1) ← W(D(cid:3), θ
11: until |w(i) − w(i−1)| < ε
Output: the ﬁnal poisoning attack samples Dp ← D(i)
W(D(cid:3), θ
, θ)
i ← i + 1
(i+1)
10:
)
p
p
∇xc
W = ∇xc θ(xc)
(cid:2) · ∇θW ,
The challenging part is understanding how to compute the
required gradient ∇xcW(D(cid:3), θ), as this has to capture the
implicit dependency of the parameters θ of the inner problem
on the poisoning point xc. Indeed, assuming that W does not
depend directly on xc, but only through θ, we can compute
∇xc
W(D(cid:3), θ) using the chain rule as:
(4)
where we have made explicit that θ depends on xc. While the
second term is simply the derivative of the outer objective with
respect to the regression parameters, the ﬁrst one captures the
dependency of the solution θ of the learning problem on xc.
We focus now on the computation of the term ∇xc θ(xc).
While for bilevel optimization problems in which the inner
problem is not convex (e.g., when the learning algorithm is
a neural network) this requires efﬁcient numerical approxi-
mations [38], when the inner learning problem is convex, the
gradient of interest can be computed in closed form. The un-
derlying trick is to replace the inner learning problem (Eq. 3)
with its Karush-Kuhn-Tucker (KKT) equilibrium conditions,
tr ∪ Dp, θ) = 0, and require such conditions to
i.e., ∇θL(D(cid:3)
remain valid while updating xc [5], [36], [38], [54]. To this
end, we simply impose that their derivative with respect to xc
remains at equilibrium, i.e., ∇xc (∇θL(D(cid:3)
tr ∪ Dp, θ)) = 0.
Now, it is clear that the function L depends explicitly on
xc in its ﬁrst argument, and implicitly through the regression
parameters θ. Thus, differentiating again with the chain rule,
one yields the following linear system:
(cid:2) · ∇2
Finally, solving for ∇xc θ, one yields:
= −∇xc
∇xc θ
(6)
For the speciﬁc form of L given in Eq. (1), it is not difﬁcult
to see that the aforementioned derivative becomes equal to
∇θL + ∇xc θ
(cid:2) (cid:10)
∇θL(cid:11)∇2
θL(cid:12)−1 .
θL = 0 .
∇xc
∂b
∂xc
∂w
∂xc
(cid:9)
(5)
=
(cid:2)
(cid:2)
22
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
(cid:13)
(cid:16)−1
(cid:14)(cid:15)
(cid:2)
that reported in [54] (except for a factor of 2 arising from a
different deﬁnition of the quadratic loss).
∇xc θ
(cid:2)
= − 2
(cid:2)
n
M w
(cid:2)
i , μ = 1
n
Σ + λg μ
1
(cid:2)
μ
,
(7)
(cid:2)
i xix
(cid:3)
i, y(cid:3)
i)}m
i xi, and M = xcw
where Σ = 1
+
(f (xc) − yc) Id. As in [54], the term g is zero for OLS and
n
LASSO, the identity matrix Id for ridge regression, and (1 −
ρ)Id for the elastic net.
Objective Functions. In previous work, the main objective
used for W has been typically a loss function computed on
an untainted validation set Dval = {(x
i=1 [5], [36],
[38]. Notably, only Xiao et al. [54] have used a regularized
loss function computed on the training data (excluding the
poisoning points) as a proxy to estimate the generalization
error on unseen data. The rationale was to avoid the attacker
to collect an additional set of points. In our experiments, we
consider both possibilities, always using the MSE as the loss
function:
(cid:2)n
(cid:11)
(cid:2)m
i=1 (f (xi, θ) − yi)
(8)
j, θ) − y(cid:3)
(cid:3)
(9)
W (Eq. 4) for these two objectives
The complete gradient ∇xc
(cid:15) ∇wWtr
(cid:16)
can thus be computed by multiplying Eq. (7) respectively to:
(cid:17)
(cid:2)n
∇bWtr
(cid:2)n
i=1(f (xi) − yi)xi + λ ∂Ω
(cid:2)m
(cid:15) ∇wWval
(cid:16)
(cid:15) 2
i=1(f (xi) − yi)
(cid:2)m
j=1(f (xj) − yj)xj
∇bWval
j=1(f (xj) − yj)
Wtr(Dtr, θ) = 1
Wval(Dval, θ) = 1
∇θWval =
∇θWtr =
(cid:12)2 .
+ λΩ(w) ,
f (x
(10)
(11)
(cid:18)
(cid:16)
j=1
∂w
=
=
(cid:2)
n
m
m
2
m
2
j
2
n
2
n
,
.
(12)
Initialization strategies. We discuss here how to select the
initial set Dp of poisoning points to be passed as input to the
gradient-based optimization algorithm (Algorithm 1). Previous
work on poisoning attacks has only dealt with classiﬁcation
problems [5], [36], [38], [54]. For this reason, the initialization
strategy used in all previously-proposed approaches has been
to randomly clone a subset of the training data and ﬂip their
labels. Dealing with regression opens up different avenues. We
therefore consider two initialization strategies in this work. In
both cases, we still select a set of points at random from the
training set Dtr, but then we set the new response value yc of
each poisoning point in one of two ways: (i) setting yc = 1−y,
and (ii) setting yc = round(1 − y), where round rounds to
the nearest 0 or 1 value (recall that the response variables are
in [0, 1]). We call the ﬁrst technique Inverse Flipping (InvFlip)
and the second Boundary Flipping (BFlip). Worth remarking,
we experimented with many techniques for selecting the
feature values before running gradient descent, and found that
surprisingly they do not have signiﬁcant improvement over a
simple uniform random choice. We thus report results only for
the two aforementioned initialization strategies.
23
Baseline Gradient Descent (BGD) Attack. We are now in
a position to deﬁne a baseline attack against which we will
compare our improved attacks. In particular, as no poisoning
attack has ever been considered in regression settings, we
deﬁne as the baseline poisoning attack an adaptation from
the attack by Xiao et al. [54]. In particular, as in Xiao et
al. [54], we select Wtr as the outer objective. To simulate label
ﬂips in the context of regression, we initialize the response