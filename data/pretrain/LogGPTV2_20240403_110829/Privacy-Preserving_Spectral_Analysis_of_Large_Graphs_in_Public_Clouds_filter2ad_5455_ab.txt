lowed by mod q are preserved, and thus recoverable. We
skip the details. Aliasgari et al. [2] has a thorough discussion
on this topic.
2.5 Differential Privacy
Differential privacy [14] is a standard notion in data privacy,
which protects individual’s privacy from the query-based
privacy attacks. For two datasets A1 and A2 that differ in
exactly one record, let M (Ai) be the mechanism that outputs
noisy statistics r ∈ R of the datasets, then ǫ-differential
privacy is satisﬁed if the following condition holds:
P r[M (A1) = r] <= expǫ P r[M (A2) = r],
(3)
where ǫ is the privacy parameter - the smaller it is, the
better the preserved privacy. It has been popularly applied
to preserve data privacy in querying databases, where any
users are allowed to submit limited types of queries and a
limited number of repetitive queries subject to the ǫ setting.
The mechanism M is deﬁned as the additive perturbation
of a speciﬁc query function, such as the COUNT function:
M (A) = COUNT(A) + random noise. The noise in the
output is engineered to approximately preserve the util-
ity of the query function, yet prevent distinguishing any
individual records in the database. Laplacian noise is one
of the popular choices, where a noise is drawn from the
Laplace distribution Lap(0, b), the density function of which
is 1
b ). The parameter b is determined by the user-
speciﬁed parameter ǫ and the sensitivity of query function:
∆ = max|M (A1)− M (A2)|, and b = ∆/ǫ. For example, the
COUNT function has the sensitivity ∆ = 1, and thus the
parameter b is set to 1/ǫ. In general, the smaller the ǫ setting
is, the larger the ∆ value will be to provide a higher level of
protection.
2b exp(− |x|
3 FRAMEWORK AND CORE ALGORITHMS
First, we will describe the privacy-preserving cloud-centric
framework for graph spectral analysis, the threat model,
and security expectations. Second, we describe the AHE-
based Lanczos algorithm for dense matrices. Third, as many
graphs are sparse, we study the privacy issues with sparse
representation, and design the privacy-preserving sparse-
graph submission protocol for data contributors. Fourth,
we develop the AHE-based Nystr¨om method to beneﬁt
from the sparse representation. Finally, we will also de-
scribe the Lanczos and Nystr¨om algorithms based on the
SHE schemes, and analyze the costs associated with all the
schemes. The cloud-side parallel processing will be brieﬂy
discussed due to the simplicity of the related operations.
3.1 Framework
The involved parties in our framework are: 1) a data owner,
denoted as “Owner”, who owns the matrix data, 2) data
contributors, denoted as “Contributors”, who agree on the
data owner’s privacy declaration and provide private data
voluntarily (with or without rewards from the data owner),
3) a public cloud provider, denoted as “Cloud”, in a service
;ŝͿ
;ũͿ
;ŬͿ
;Ϳ
KŶͲĚĞŵĂŶĚ
WƌŽĐĞƐƐŝŶŐ
ůƵƐƚĞƌ
ĂƚĂĐŽŶƚƌŝďƵƚŽƌƐ
ůŽƵĚ
ĂƚĂKǁŶĞƌ
DŽĚĞůƐ
DŽĚĞů
ĐŽŶƐƵŵĞƌƐ
Fig. 1: A framework for cloud-centric privacy-preserving
spectral analysis.
level agreement to provide scalable computation and stor-
age, who is honest in providing services, but curious about
observed data.
Our aim here is to design practical privacy-preserving
eigendecomposition algorithms for graph matrices where
the public cloud learns nothing from the stored data, the
computations that occur within its infrastructure, and the
interactions with other parties. One of the key ideas is
to let Owner and Contributors take a small amount of
computation and storage responsibility of O(N ) complexity,
while Cloud takes more expensive O(N 2) parts that can be
implemented in parallel and scalable algorithms.
Speciﬁcally, Cloud stores the big encrypted matrix E(A)
and conducts the expensive homomorphic matrix-vector
multiplication. Owner interacts with Cloud and assists in
the computation. When collecting data, Owner employs an
asymmetric AHE or SHE encryption scheme and publishes
a public key for Contributors. Contributors encrypt their
submissions and use a web service or a mobile app to
upload the encrypted data. Examples of such user data may
include interactions between social network users, which
are used for detecting social network communities, or user
ratings on products for training a recommender system.
This cloud-centric framework is particularly important for
handling continuously evolving matrix E(A), for which the
analytic models should be periodically updated, which are
too expensive to be maintained locally by data owners.
3.2 Security Model
We make practical threat assumptions and only focus on the
privacy threats from honest-but-curious cloud providers. 1)
The data contributors operate through secure systems and
no information is leaked to attackers. 2) The data owner’s
infrastructure is secure. Our framework cannot protect pri-
vacy from an insider attack issued by the data owner’s
organization. 3) All communication channels are secure and
data in transit is always protected. 4) Our framework is not
meant to ensure the integrity of data that is orthogonal to
our work.
Let’s model a graph spectral analysis algorithm as a se-
cure protocol GSA = (Enc, Prepare, Query), consisting
of three polynomial-time protocols. After the initial stages
Enc and Prepare, the main body is a series of Query-
Answering interactions between Owner and Cloud: Owner
queries Cloud and Cloud returns the result to Owner.
Combined with Owner’s local processing, it achieves the
algorithmic goal.
(K, EG) ← Enc(1h, G): is a multi-party protocol among
three parties: Owner takes a security parameter h and
1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2018.2847662, IEEE
Transactions on Knowledge and Data Engineering
generates a key-pair K = (P k, Sk); Contributors take P k
and output an encrypted graph EG to Cloud.
(H) ← Prepare(m): is a multi-party protocol among
the three parties: Owner takes some parameter m and works
with Cloud (and Contributors) to get a helper data H. It’s
a one-time setup for securely processing queries later. For
some algorithms, this step might be skipped, or some parties
may not participate.
(R) ← Query(K, q, EG): is a two-party protocol be-
tween Owner that holds the key K and a query q, and Cloud
that holds the encrypted graph EG. Cloud processes q and
returns the query result R, which can be encrypted vectors
or matrices depending on the speciﬁc algorithm.
Security Deﬁnition. We deﬁne security guarantee as
follows. (1) For graph encryption, the strongest deﬁnition is
that given the encrypted graph, no adversary can learn any
information about the graph, which is used by the dense-
matrix encryption method. (2) We also deﬁne a weaker
notion for the sparse-matrix encryption method, which does
not exactly protect each edge’s privacy, but uses differ-
ential privacy to protect each data contributor from re-
identiﬁcation [36]. (3) The Prepare procedure does not leak
any information. (4) With either the dense or the sparse
method, the protocol interactions do not leak any additional
information. Speciﬁcally, given the view of a polynomial
number of Query executions for an adaptively generated
sequence of queries q = (q1, . . . , qn), no adversary can learn
any partial information about either G or q.
We adopt the idea of simulation-based security [12], [23]
to formally deﬁne the Query protocol security. The semi-
honest adversary A who compromises Cloud observes the
interactions between A (i.e., Cloud) and the challenger C
(i.e., Owner), and tries to infer any useful information. A
knows the encrypted graph EG and the public key, but
not the private key of K. S is a simulator that simulates
views of A in the ideal world corresponding to the views
of A during the protocol execution in the real world. The
following formalizes the security deﬁnition based on the
Ideal and Real experiments.
IdealA,S(1h): A possesses the encrypted graph EG re-
ceived from Users. If A were malicious it may also gen-
erate a fake graph G, run Enc(1h, G) with the public key
provided by Owner and generate EG. However, we only
consider the semi-honest scenario. A generates a polyno-
mial number of adaptively chosen queries (q1, . . . , qm) with
an intent to compromise the security of the Ideal GSA’s
Query functionality. We can envision a simulator S which
runs Prepare to get the helper data. For each query qi, S
presents to A a view as the execution of Query(K, qi, EG).
RealA,C(1h): A possesses an encrypted graph EG. C
runs Prepare protocol to get the helper data. A gener-
ates a polynomial number of adaptively chosen queries
(q1, . . . , qm). For each qi, A and C interactively execute
Query(K, qi, EG).
In both the settings, A uses the observed views to
compute a bit b that is the output by the experiment. We
say that the protocol is adaptively semantically secure if
for all adversaries with probabilistic algorithms running in
polynomial time (i.e., PPT), there exists a PPT simulator S
5
such that
|P r(RealA,C(1h) = 1) − P r(IdealA,S(1h) = 1)| = negl(h).
where negl(h) is a negligible function [21]. In proofs, we
only need to show such a simulator exists for each proposed
protocol.
3.3 AHE-based Lanczos Construction for Dense Matrix
We ﬁrst present the AHE-based Lanczos method (Lan-AHE)
for dense matrix. The core operation: Query implements
the privacy-preserving matrix-vector multiplication with
client-cost O(N ) and cloud-cost O(N 2). Section 2 shows
that the most expensive operation in the Lanczos iteration is
bi+1 ← Abi1. Thus, the core of the algorithm is that Owner
uses Query to compute Abi1, which is combined with some
O(N ) local processing to implement the Lanczos algorithm.
It also uses an IND-CCA secure AHE such as Paillier to
encrypt each element of the graph matrix in the dense form.
The Prepare procedure will generate some helper data for
Owner quickly hiding bi and recovering the result Abi, so
that the desired security and efﬁciency goals are achieved.
Cloud will take the expensive task of computing Abi on the
encrypted A. One key challenge is to compute E(Abi) with
E(A) and plaintext bi that needs to be protected to achieve
the security guarantee - leaking the set {bi} will allow the
adversary to approximately reconstruct the matrix.
The basic idea is to submit a masked vector ¯bi. The
masking technique needs to address two goals: (1) the
masked vector does not leak any information to adversaries,
i.e., ¯bi cannot be distinguished from any uniformly random
noise vector; (2) it is possible to recover Abi from the result
of A¯bi efﬁciently, i.e., no more than O(N ) complexity. The
design of the noise vector to meet these two goals is the
key of this protocol. We describe this protocol in detail as
follows.
PrepareLan−AHE(h) :. This step consists of two sub-
steps. (1) The data owner selects h N -dimensional random
vectors, {sj, j = 1..h}, where h is a constant related to
the security of the masking technique (e.g., h = 80), and
sends them to cloud in plaintext. These random vectors
will be used to protect the vector bi in each iteration. The
cloud will compute E(Asj) and send back the results to
the data owner, who will decrypt the results to get the
vectors cj = Asj, j = 1..h. After the initial setup, the
masking results {cj} will be incrementally updated when A
is evolving. (2) The data owner also generates a uniformly
random vector b0 and distributes E(b0) to data contributors.
b0 serves as a secret, which is critical to the security of
the whole protocol as shown in Section 3.5. Each data
j=1 Aij E(b0j)), where
b0j is the j-th element of b0 and Aij is the j-th element of the
row vector Ai, using pseudo homomorphic multiplication,
and sends back the single encrypted scalar E(Aib0) to the
data owner. It follows that the costs for data owner and data
contributors in this step are O(N ).
:
contributor i computes E(Aib0) = PN
QueryLan−AHE(K, q, EG)
this protocol has two
steps: the LWE-based masking to generate the query q (i.e.,
¯bi), and the efﬁcient recovery method to get Abi from the
query result. Let q be the perturbed vector ¯bi given as
¯bi = bi + ri mod p
(4)
1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2018.2847662, IEEE
Transactions on Knowledge and Data Engineering
where bi is the vector to be protected, ri is a noisy vector,
and p is a big random prime large enough to contain all
values and computation results in the application domain
and guarantee the security of perturbed vectors (i.e., the
brute-force enumeration is computational intractable). The
key of this perturbation is to guarantee ri cannot be distin-
guished from any uniformly random vectors and still allow
the efﬁcient recovery of Abi from the result of A¯bi.
We design ri as follows to meet the two goals. Its
security will be discussed later in Section 3.5 based on the
intractability of the Learning-with-Error problem in lattice
[28]. ri, i ≥ 0, is derived from the seed vectors {sj, j = 1..h}
and existing {bj, j = 0..i − 1} as:
Xj=1
βij bj + b0 mod p,
αilsl +
(5)
i−1
ri =
h
Xl=1
where αil and βij are randomly drawn from Zp. This
approach protects bi and its security depends on the ran-
domness of ri. Note that {sl} is already known by the
cloud in the preparation phase, which, however, does not
compromise the security of ri due to the learning-with-error
(LWE) based security [28], as long as bj, j = 0..i − 1 are
secret.
The recovery of Abi is performed at data owner as A¯bi
from the cloud is the result of A¯bi = Abi + Ari since ¯bi =
bi + ri. Also, because we can compute
Ari =
h
Xk=1
αik(Ask) +
i−1
Xj=1
βjk(Abj) + (Ab0) mod p
with known ck = Ask and Abj, j < i in complexity O(N ),
Abi can be recovered with a O(N ) cost. The correctness of
this algorithm is easy to verify.
Algorithm 4 in Appendix gives the detail of the privacy-
preserving Lanczos algorithm. The cost and security analy-
sis will be discussed later.
3.4 Construction of Secure Nystr ¨om with Differential
Privacy and AHE for Sparse Matrix
Many graphs are actually sparse, which has not been fully
explored by the Lan-AHE algorithm yet. This sparsity can
be utilized to reduce the cloud data storage, cloud-side
computation, and the cost of contributors submitting data.
For a matrix that has only M non-zero elements on average
per row, where M ≪ N , with sparse representation the
submission cost is reduced to O(M ) for each contributor, the
cloud storage is reduced to O(M N ) from O(N 2), and the
cost of the core matrix-vector computation is also reduced to
O(M N ). This saving can be huge, as N is probably around
millions while M is only hundreds. However, straightfor-
ward sparse encoding may leak private information for
graphs. In the following, we will analyze this privacy risk,
then present the speciﬁc Enc procedure for sparse matrix,
and ﬁnally develop the secure Nystr¨om algorithm to take
advantage of the sparse matrix.
3.4.1 Privacy Leak on Sparse Graph Matrices and Our
Protection Method
Let’s consider a typical graph matrix for spectral analysis:
the normalized Laplacian graph matrix. For an undirected
6
graph, let D be the diagonal matrix with node degrees on its
diagonal - Dii represents the degree of node i, i = 1..N . Let
W be the adjacency matrix with Wij =1 if and only if the
edge (i, j) exists, and Wij = 0 otherwise. For undirected
graphs, W is a symmetric matrix, where each row(column)
of W represents the corresponding node’s adjacency edges.
The normalized graph Laplacian matrix L is L = I−D−1W ,
where I is the N by N identity matrix. The eigenvectors of L
can be used for graph spectral clustering [32]. The matrix L
is apparently sparse due to the sparsity of W . In traditional
sparse encoding, the zero entries are skipped, while the non-
zero ones are encoded as (i, j, v) for entry index (i, j).
However, simply encrypting the non-zero entries does
not preserve the privacy of the matrix for several reasons.
(1) The number of non-zero entries per row is the node degree
of the corresponding node. (2) The presence of a non-zero
entry also implies the existence of the corresponding edge.
Both node degree and edge existence information can be
used in privacy attacks on social graphs [36].
Our method is to blend in fake edges to disguise both
exact node degrees and edge existence. As the encryption
methods we use are all probabilistic, each time encrypting a
value (or evan the same value encrypted multiple times) will
result in a different ciphertext that cannot be distinguished
from a uniformly random value. Therefore, the fake edges
(i.e., the zero entries in the matrix) cannot be distinguished
from other entries as well. Apparently, the added zero
entries will not affect the result of matrix-vector operations.
The key question is to design a theoretically justiﬁed method
for users to select the number of fake edges, for which we
apply the Laplace mechanism of differential privacy.
The problem setting and data encoding method distin-