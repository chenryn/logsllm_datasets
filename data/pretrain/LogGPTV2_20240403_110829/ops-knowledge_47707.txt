User: How to use Python to write a simple crawler to get web content and store it in a local file?
Assistant: The following is a sample code for writing a simple crawler in Python, which is used to fetch web page content and store it in a local file. In this example, we use third-party libraries requests and BeautifulSoup4 to crawl web content and parse HTML documents. ```python import requests from bs4 import BeautifulSoup url = "https://example.com" # The address of the web page to be crawled output_file = "output.html" # Local file path to store web content # Send a request to get the content of the web page response = requests.get(url) html = response.text # Use BeautifulSoup to parse HTML documents soup = BeautifulSoup(html, "html.parser") # Open the local file and store the content of the web page in the file with open(output_file, "w ") as f: f.write(soup.prettify()) print("Crawling is complete, the content has been stored in: " + output_file) ``` The workflow of this crawler is as follows: 1. Use the requests library to send HTTP requests, Get web content. 2. Use the BeautifulSoup library to parse HTML documents and convert web page content into operable objects (eg tags, attributes, text, etc.). 3. Open a local file and store the content of the web page in the file. The above sample code is just a simple crawler implementation. In fact, when crawling data, it needs to deal with random agents, data analysis and other practical operations. In order to avoid unnecessary impact on the target website, we should be careful not to over crawl the content of the site, and at the same time abide by the terms of use of the target site.