# Title: Lessons Learned from the Analysis of System Failures at Petascale: The Case of Blue Waters

## Authors:
- Catello Di Martino
- Zbigniew T. Kalbarczyk
- Ravishankar K. Iyer
- Fabio Baccanico
- Joseph Fullop
- William Kramer

### Affiliations:
- **University of Illinois at Urbana-Champaign**
  - Email: {dimart, kalbarcz, rkiyer}@illinois.edu
- **Federico II University of Naples** (Fabio Baccanico)
- **National Center for Supercomputing Applications (NCSA)**
  - Email: {fullop, wkramer}@ncsa.illinois.edu

## Abstract
This paper presents an in-depth analysis of failures and their impact on Blue Waters, a Cray hybrid (CPU/GPU) supercomputer at the University of Illinois at Urbana-Champaign. The study is based on both manual failure reports and automatically generated event logs collected over 261 days. Key findings include:

1. **Characterization of Root Causes**: A detailed characterization of the root causes of single-node failures.
2. **Effectiveness of Resilience Mechanisms**: An assessment of the effectiveness of system-level failover and error resilience mechanisms for memory, processors, network, GPU accelerators, and file systems.
3. **System-Wide Outages**: An analysis of system-wide outages and their causes.

### Major Findings:
- **Hardware vs. Software Failures**: While hardware-related failures accounted for 42% of all failures, they were responsible for only 23% of the total repair time. This is due to robust protection mechanisms like x8 and x4 Chipkill, ECC, and parity, which can handle up to 250 errors/hour with 99.997% coverage.
- **Software Impact**: Software was the largest contributor to node repair hours (53%), despite causing only 20% of the total number of failures. Specifically, 29 out of 39 system-wide outages involved the Lustre file system, with 42% attributed to inadequate automated failover procedures.

### Index Terms
- Failure Analysis
- Failure Reports
- Cray XE6
- Cray XK7
- Supercomputer
- Machine Check
- Nvidia GPU Errors

## I. Introduction
Failures are inevitable in large-scale high-performance computing (HPC) systems. Understanding and quantifying these failures is crucial for improving system reliability and achieving sustained performance at scale. This paper analyzes the failures of Blue Waters, a Cray petascale supercomputer at NCSA, during its first 261 days of operation (March 1, 2013, to November 17, 2013).

### Background
Blue Waters is a new-generation supercomputer providing sustained petaflops (with a peak of 13.1 petaflops) for scientific and engineering applications. It features a hybrid architecture with 22,640 Cray XE6 general-purpose computing nodes and 3,072 Cray XK7 hybrid nodes equipped with Nvidia GPU accelerators. Additionally, it has the largest Lustre parallel file system installed, with more than 26 petabytes of available storage.

### Study Focus
This study provides a detailed characterization of single-node failures and system-wide outages, leveraging both manual failure reports and automatically collected syslogs (approximately 3.7 TB). The key findings are summarized in Table I and highlight the following:

- **Software-Induced Outages**: Software caused 74.4% of system-wide outages, with 62% of these resulting from inadequate failover procedures, particularly for the Lustre file system.
- **Hardware Resilience**: Out of 1,544,398 analyzed machine check exceptions, only 28 (0.003%) resulted in uncorrectable errors, demonstrating the effectiveness of the adopted protection mechanisms.
- **GPU Memory Sensitivity**: The DDR5 memory in Nvidia GPU accelerators, protected only by ECC, showed a 100 times higher rate of uncorrectable errors compared to DDR3 node RAM, indicating the need for improved protection techniques for future hybrid systems.

These findings are valuable for practitioners and researchers, as they highlight new challenges and opportunities for enhancing the resiliency of exascale systems.

## II. About Blue Waters
Blue Waters is a sustained petaflop system capable of delivering approximately 13.1 petaflops (peak) for a range of real-world scientific and engineering applications. The system includes:

- **Compute Nodes**: 276 Cray liquid-cooled cabinets hosting 26,496 nodes and 1.47 PB of RAM across 197,032 RAM DIMMs.
- **Hybrid Nodes**: 3,072 GPU hybrid nodes equipped with Nvidia K20X accelerators and AMD Opteron processors.
- **Service Nodes**: 784 service nodes with a total of 5,824 available cores.
- **High-Speed Network**: The Cray Gemini network for node connectivity.
- **Storage System**: 198 Cray Sonexion 1600 storage units with 20,196 disks and 396 SSDs, providing access to 26 petabytes (36 raw) of usable storage over a Lustre distributed file system.
- **Near-Line Storage**: 300 petabytes (380 raw) of usable near-line tape storage.

### Compute Node Hardware
- **Cray XE6 Blades**: Each blade hosts 4 compute nodes, each consisting of 2 16-core AMD Opteron 6276 processors at 2.6 GHz, with 64 GB of DDR3 RAM in 8 GB DIMMs. System memory is protected with x8 Chipkill.
- **Caches**: L3, L2, and L1 data caches are protected with ECC, while tag caches, TBLs, L2, and L1 instruction caches are protected with parity.

### GPU Node Hardware
- **Cray XK7 Blades**: Each blade hosts 4 GPU nodes, each consisting of a 16-core Opteron 6272 processor with 32 GB of DDR3 RAM and a Nvidia K20X accelerator with 6 GB of DDR5 RAM protected by ECC.

### Service Node Hardware
- **Cray XIO and XE6 Blades**: Service nodes are hosted on 166 Cray XIO blades and 30 XE6 blades, each with 4 nodes per blade. Each XIO service node consists of a 6-core AMD Opteron 2435 processor with 16 GB of DDR2 memory protected by x4 Chipkill.

### Network
- **Cray Gemini System Interconnect**: A three-dimensional (3D) 23x24x24 reentrant torus topology, with each node having 6 possible links (right, left, up, down, in, and out).

### Hardware Supervisor System (HSS) and System Resiliency Features
- **HSS Components**: HSS network, blade (L0) and cabinet (L1) controllers, and HSS manager.
- **Failure Mitigation**: Warm swap of compute/GPU blades, service node and Lustre node failover mechanisms, and link degradation and route reconfiguration to enable routing around failed nodes.

The total time to execute the failover procedure is around 30 to 60 seconds, ensuring minimal disruption to the workload.

---

This optimized version aims to improve clarity, coherence, and professionalism while maintaining the essential content and structure of the original text.