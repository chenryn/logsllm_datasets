title:Lessons Learned from the Analysis of System Failures at Petascale:
The Case of Blue Waters
author:Catello Di Martino and
Zbigniew T. Kalbarczyk and
Ravishankar K. Iyer and
Fabio Baccanico and
Joseph Fullop and
William Kramer
Lessons Learned From the Analysis of System
Failures at Petascale: The Case of Blue Waters
Catello Di Martino, Zbigniew Kalbarczyk,
Fabio Baccanico1, Joseph Fullop2, William Kramer2
Ravishankar K. Iyer
University of Illinois at Urbana-Champaign
Email: {dimart,kalbarcz,rkiyer}@illinois.edu
1Federico II University of Naples
2National Center for Supercomputing Applications
Email: PI:EMAIL,{fullop,wkramer}@ncsa.illinois.edu
Abstract—This paper provides an analysis of failures and their
impact for Blue Waters, the Cray hybrid (CPU/GPU) supercom-
puter at the University of Illinois at Urbana-Champaign. The
analysis is based on both manual failure reports and automatically
generated event logs collected over 261 days. Results include i)
a characterization of the root causes of single-node failures; ii)
a direct assessment of the effectiveness of system-level failover
as well as memory, processor, network, GPU accelerator, and ﬁle
system error resiliency; and iii) an analysis of system-wide outages.
The major ﬁndings of this study are as follows. Hardware is
not the main cause of system downtime. This is notwithstanding
the fact that hardware-related failures are 42% of all failures.
Failures caused by hardware were responsible for only 23% of the
total repair time. These results are partially due to the fact that
processor and memory protection mechanisms (x8 and x4 Chipkill,
ECC, and parity) are able to handle a sustained rate of errors
as high as 250 errors/h while providing a coverage of 99.997%
out of a set of more than 1.5 million of analyzed errors. Only 28
multiple-bit errors bypassed the employed protection mechanisms.
Software, on the other hand, was the largest contributor to the
node repair hours (53%), despite being the cause of only 20% of
the total number of failures. A total of 29 out of 39 system-wide
outages involved the Lustre ﬁle system with 42% of them caused
by the inadequacy of the automated failover procedures
Index Terms—Failure Analysis, Failure Reports, Cray XE6,
Cray XK7, Supercomputer, Machine Check, Nvidia GPU errors.
I. INTRODUCTION
Failures are inevitable in large-scale high-performance com-
puting systems. Error resiliency (i.e., the ability to compute
through failures) is strategic for providing sustained perfor-
mance at scale. In this context it is important to measure and
quantify what makes current systems unreliable. Such analyses
can also drive the innovation essential for addressing resiliency
in exascale computing.
In this paper, we study the failures of Blue Waters, the
Cray sustained petascale machine at the National Center for
Supercomputing Applications (NCSA), at
the University of
Illinois at Urbana-Champaign during a period of 261 days
(March 1, 2013 to November 17, 2013) starting just a few days
before Blue Waters went into ofﬁcial production (March 27
2013). Blue Waters is a new-generation supercomputer, which
provides sustained petaﬂops (with a peak of 13.1 petaﬂops) for
scientiﬁc and engineering applications. That high performance
is achieved using a hybrid architecture, which includes general-
purpose computing nodes (22,640 Cray XE6 machines) and
graphic accelerators (3,072 Cray XK7 hybrid nodes equipped
with cutting-edge Nvidia GPU accelerators). In support of
computation, Blue Waters provides access to the largest Lus-
tre parallel ﬁle system installed to date, with more than 26
petabytes of available storage (36 PB raw).
Failures of large-scale systems have been analyzed in the
past [1]–[8]; this study focuses on a detailed characterization
of single node failures as well as system-wide outages of a sus-
tained petascale machine. Blue Waters is a conﬁguration corner
case well beyond the scales at which hardware and software are
methodically tested; as such this study’s ﬁndings have signiﬁ-
cant value. Speciﬁcally, our analysis i) characterizes single-node
failures as well as system-wide outages by analyzing the manual
system reports compiled by the Blue Waters maintenance spe-
cialists, ii) quantiﬁes the effectiveness of protection mechanisms
(e.g., network and ﬁle system failover), and iii) based on the
manual failure report and automatically collected syslogs (about
3.7 TB), provides an in-depth understanding of hardware error
resiliency features and measure the rates of correctable and
uncorrectable errors for all cache levels and RAMs (including
system and GPU accelerator RAM memories). The key ﬁndings
of this work are reported in Table I. In summary, our major
ﬁndings are:
• Software is the cause of 74.4% of system-wide out-
ages (SWOs). Close to two-thirds (62%) of software-
caused SWOs resulted from failure/inadequacies of the
failover procedures, such as those invoked to handle Lustre
failures. More research is needed to improve failover
techniques, including the way failure recovery software
modules are tested in large-scale settings.
• Hardware is highly resilient to errors. Out of 1,544,398
analyzed machine check exceptions, only 28 (0.003%)
resulted in uncorrectable errors, showing the value of the
adopted protection mechanisms (Chipkill and ECC). The
GPU accelerator DDR5 memory, protected only with ECC,
is 100 times more sensitive to uncorrectable errors than
DDR3 node RAM is. This ﬁnding shows the need for a
better technique to be employed to protect the memory
of GPU accelerators memory when both CPUs and GPU
accelerators are used to create future large-scale hybrid
systems.
The ﬁndings presented in this paper are of interest to a broad
audience, including practitioners and researchers. Our ﬁndings
demonstrate that the resiliency features of Blue Waters differ
from those observed in former generations of supercomputers
(e.g., [4], [9], [10]), highlighting new challenges to address in
order to approach the next-generation exascale systems.
TABLE I: Findings and implications
Findings from overall failure characterization (Section IV)
Hardware caused more failures than any other factor (51% of single/multiple node
failures and 42% of total failures), but those failures were responsible for only 23% of
the total node repair hours.
Software was the largest contributor to the total node repair hours (53%), despite being
the cause of only 20% of the failures.
The Lustre distributed ﬁle system account for 46% of the software-caused failures.
Findings from characterization of hardware errors (Section V)
Error-correcting code (SEC-DED and Chipkill) techniques detected and corrected
99.997% of memory and processor errors. Only 28 out of 1,544,398 single and multiple
bit errors were uncorrectable.
DDR5 RAM of Nvidia GPU accelerator, protected by ECC, showed a rate of
uncorrectable error per GB 100 times higher than the rate of uncorrectable errors for
the nodes’ DDR3 memory protected by x8 Chipkill.
The hardware failure rate decreased with time. Software failure rates did not decrease
over time.
Findings from characterization of system-wide outages (SW, Section VI)
Software was the cause of most (74.4%) SWOs and caused 68% of Blue Waters
downtime hours. Close to two-thirds (62%) of software-caused SWOs resulted from
failures of the failover procedure invoked to handle Lustre problems.
SWOs caused by single-node failures were rare; they occurred in 0.7% of all reported
single-node failures. 99.3% of hardware-related failures did not propagate outside the
boundary of a single blade. Software-related failures propagate to multiple blades in
14.8% of the cases, i.e., 20 times more often than for hardware-related failures.
II. ABOUT BLUE WATERS
Blue Waters is a sustained petaﬂop system capable of de-
livering approximately 13.1 petaﬂops (at peak) for a range of
real-world scientiﬁc and engineering applications. The system
is equipped with:
• 276 Cray liquid-cooled cabinets hosting 26,496 nodes and
1.47 PB of RAM across 197,032 RAM DIMMs. Each
cabinet consists of an L1 cabinet controller, several fan
trays, power conversion electronics, breakers, a blower and
chiller, and related piping. Each cabinet is organized in 3
chassis, and each chassis hosts 8 blades;
sors) with a total of 724,480 cores;
GPU accelerators and AMD Opteron processors;
• 22,640 compute nodes (based on AMD Opteron proces-
• 3,072 GPU hybrid nodes equipped with Nvidia K20X
• 784 service nodes with a total of 5,824 available cores;
• The high-speed Cray Gemini network, to provide node
• The online storage system, consisting of 198 Cray Sonex-
ion 1600 storage units equipped with 20,196 disks, and
396 SSDs (used to store ﬁle system metadata) that provide
access to 26 petabytes (36 raw) of usable storage over a
Lustre distributed ﬁle system; and
connectivity;
• 300 petabytes (380 raw) of usable near-line tape storage.
Compute node hardware. Compute nodes are hosted in
5,660 Cray XE6 blades (see Figure 1.(a)), 4 nodes per blade.
A compute node consists of 2 16-core AMD Opteron 6276
processors at 2.6 GHz. Each Opteron includes 8 dual-core AMD
Bulldozer modules, each with an 8x64 KB L1 instruction cache;
a 16x16 KB L1 data cache; an 8x2 MB L2 cache (shared
between the cores of each Bulldozer module), and a 2x8 MB
L3 cache (shared among all the cores). Each compute node is
equipped with 64 GB of DDR3 RAM in 8 GB DIMMs. System
memory is protected with x8 Chipkill [11], [12] code that uses
eighteen 8-bit symbols to make a 144-bit ECC word made up
Implications
Hardware failures are well-managed by the Cray system and are easily diagnosed
through machine check logs and POST checks. The programmed failover techniques
are effective in recovering from hardware failures.
Software is critical in systems at the scale of Blue Waters and beyond. Failures due to
software root causes are more likely to affect multiple nodes.
Lustre problems were the most common software-caused failures. Lustre plays a crucial
role in Blue Waters, since all the compute nodes are diskless and rely on Lustre for
ﬁle system access and failures caused by Lustre are potentially critical.
Implications
This ﬁnding shows that hardware is not a major problem for system-wide reliability
despite a measured rate of 250 errors/h. This shows that it is worthwhile to invest in
state-of-the-art detection and recovery to achieve sustained performance in the presence
of high error rates.
The impact of memory errors in the GPU accelerator memory could represent a serious
menace for the development of future large-scale hybrid supercomputers. Improved
protection techniques would be needed when increasing the size of the GPU memory.
Hardware tends to become mature (i.e., ﬂat part of the bathtub curve) earlier than
software. Defective hardware can be replaced in a few months, contributing to a
decrease of the failure rate. More work is needed to improve the way large-scale
software systems are tested.
Implications
The failure recovery of Lustre is a complex procedure that can even more than 30
minutes. The mechanisms behind Lustre failover (e.g., timeouts and distributed lock
management) operate at their limits at the scale of Blue Waters. New failover solutions
at scale may be required to support future supercomputers.
The error-protection mechanisms were effective in containing failures within the
boundaries of a single node.
L0 controller 
Gemini  
ASIC 
8x8 GB  
DDR3  
RAM  
DIMMs 
Gemini  
ASIC 
Voltage  
Regulator 
8x8 GB  
DDR3  
RAM  
DIMMs 
L0 controller 
Gemini  
ASIC 
8x8 GB  
DDR3  
RAM  
DIMMs 
Gemini  
ASIC 
Voltage  
Regulator 
8x8 GB  
DDR3  
RAM  
DIMMs 
node 
AMD  
6272 
AMD  
Opteron  
Opteron  
6272 
AMD  
Opteron  
6272 
AMD  
Opteron  
6272 
AMD  
Opteron  
6272 
AMD  
Opteron  
6272 
AMD  
Opteron  
6272 
AMD  
Opteron  
6272 
8x8 GB  
DDR3  
RAM  
DIMMs 
8x8 GB  
DDR3  
RAM  
DIMMs 
l
e
d
a
B
e
t
u
p
m
o
C
6
E
X
(a)
node 
AMD  
Opteron  
6272 
AMD  
Opteron  
6272 
AMD  
Opteron  
6272 
AMD  
Opteron  
6272 
6 GB DDR5  
RAM 
6 GB DDR5  
RAM 
Nvidia GPU  
k20x 
Nvidia GPU  
k20x 
Nvidia GPU  
k20x 
Nvidia GPU  
k20x 
6 GB DDR5  
RAM 
6 GB DDR5  
RAM 
e
d
a
l
B
U
P
G
7
K
X
(b)
Fig. 1: Blue Waters blades: (a) compute (Cray XE6), (b), GPU (Cray XK7).
of 128 data bits and 16 check bits for each memory word.
The x8 code is a single-symbol correcting code, i.e., it detects
and corrects up to 8-bit errors. L3, L2, and L1 data caches are
protected with ECC, while all the others (tag caches, TBLs, L2
and L1 instruction caches) are protected with parity.
GPU node hardware. GPU nodes are hosted in 768 Cray
XK7 blades, 4 nodes per blade (see Figure 1.(b)). A GPU
node consists of a 16-cores Opteron 6272 processor equipped
with 32 GB of DDR3 RAM in 8 GB DIMMs and a Nvidia
K20X accelerator. The accelerators are equipped with 2,880
single-precision Cuda cores, 64 KB of L1 cache, 1,536 KB of
dedicated L2 cache, and 6 GB of DDR5 RAM memory with
the latter protected with ECC. After this study, 1152 additional
XK7 GPU nodes were added to Blue Waters.
Service node hardware. Service nodes are hosted on 166
Cray XIO blades and 30 XE6 blades, 4 nodes per blade. Each
XIO service node consists of a 6-core AMD Opteron 2435
Instanbul working at 2.3 GHz and equipped with 16 GB of
DDR2 memory in 4 GB DIMMs protected by x4 Chipkill (with
single symbol error correction and dual-symbol error detection
capabilities, with 4-bit per symbol). Service nodes host special
PCI-Express cards such as Inﬁniband and ﬁber-channel cards. A
service node can be conﬁgured as i) a boot node to orchestrate
system-wide reboots; ii) a system database node to collect event
logs; iii) a MOM node for scheduling jobs; iv) a network node
to bridge external networks through Inﬁniband QDR IB cards;
or v) as an Lnet (Lustre ﬁlesystem network) node to handle
metadata (via Lustre metadata servers, or MDSes, to keep track
of the location of the ﬁles in the storage servers) and ﬁle I/O
data (via Lustre Object Storage Servers, or OSSes, to store the
data stripes across the storage modules) for ﬁle system servers
and clients.
All the blades are powered though 4 dual-redundant Cray
Verty voltage regulator modules (see Figure 1), one per node,
fed by a power distribution unit (PDU) installed in the cabinet
and attached to the blade through a dedicated connector.
Network. Blue Waters high-speed network consists of a Cray
Gemini System Interconnect (see Figure 1). Each blade includes
a network mezzanine card that houses 2 network chips, each one
attached on the HyperTransport AMD bus [9] shared by 2 CPUs
and powered by 2 mezzanine dual-redundant voltage regulator
modules. The topology is a three-dimensional (3D) 23x24x24
reentrant torus: each node has 6 possible links towards other
nodes, i.e., right, left, up, down, in, and out.
Hardware Supervisor System (HSS) and System Resiliency
Features. Every node in the system is checked and managed
by the HSS. Core components of the HSS system are i)
the HSS network; ii) blade (L0 - see Figure 1) and cabinet
(L1) controllers in charge of monitoring the nodes, replying
to heartbeat signal requests and collecting data on tempera-
ture, voltage, power, network performance counters, runtime
software exceptions; and iii) the HSS manager in charge of
collecting node health data and executing the management
software. Upon detection of a failure, e.g., a missing heartbeat,
the HSS manager triggers failure mitigation operations. They
include i) warm swap of a compute/GPU blade to allow the
system operator to remove and repair system blades without
disrupting the workload;
ii) service node and Lustre node
failover mechanisms, e.g., replacement of IO nodes with warm-
standby replicas; and iii) link degradation and route reconﬁg-
uration to enable routing around failed nodes in the topology.
The procedure in the communication path route consists of i)
waiting 10 seconds to aggregate failures; ii) determining which
blade(s) is/are alive; iii) quiescing the Gemini network trafﬁc;
iv) asserting a new route in the Gemini chips; and v) cleaning up
and resuming Gemini. The total time to execute that procedure
is around 30 to 60 seconds. In the case of Gemini link failures,