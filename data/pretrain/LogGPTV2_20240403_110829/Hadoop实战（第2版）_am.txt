Javac-classpath/usr/local/hadoop/hadoop-1.0.1/hadoop-core-1.0.1.jar-d
ScoreProcessFinal_classes ScoreProcessFinal.java
上述命令会将ScoreProcessFinal.java编译后的所有class文件放到ScoreProcessFinal_classes文件夹下。执行下面的命令打包所有的class文件：
jar-cvf/usr/local/hadoop/hadoop-1.0.1/bin/ScoreProcessFinal.jar-C ScoreProcessFinal_classes/.
标明清单（manifest）
增加：ScoreProcessFinal$Map.class（读入=1899）（写出=806）（压缩了57%）
增加：ScoreProcessFinal$Reduce.class（读入=1671）（写出=707）（压缩了57%）
增加：ScoreProcessFinal.class（读入=2374）（写出=1183）（压缩了50%）
4.5.2 在本地模式下运行
使用下面的命令以本地模式运行打包后的程序：
hadoop jar ScoreProcessFinal.jar inputOfScoreProcessFinal outputOfScoreProcessFinal
上面的命令以inputOfScoreProcessFinal为输入路径，同时以outputOfScoreProcessFinal为输出路径。
到此，我们已经将编译打包和在本地模式下运行的情况讲解完了。
4.5.3 在集群上运行
接下来讲解程序如何在集群上运行。在笔者的实验环境中，一共有4台机器，其中一台同时担当JobTracker和NameNode的角色，但不担当TaskTracker和DataNode的角色，另外3台机器则同时担当Tasktracker和DataNode的角色。
首先，将输入的文件复制到HDFS中，用以下命令完成该功能：
hadoop dfs-copyFromLocal/home/u/Desktop/inputOfScoreProcessFinal inputOfScoreProcessFinal
下面，在命令行中运行程序：
～/hadoop-0.20.2/bin$hadoop jar/home/u/TG/ScoreProcessFinal.jar
ScoreProcessFinal inputOfScoreProcessFinal outputOfScoreProcessFinal
执行上述命令运行ScoreProcessFinal.jar中的ScoreProcessFinal类，并且将inputOf-ScoreProcessFinal作为输入，outputOfScoreProcessFinal作为输出。
4.6 网络用户界面
Hadoop自带的网络用户界面在查看工作的信息时很方便（在http：//jobtracker-host：50030/中能找到用户界面）。在Job运行时，它对于跟踪Job工作进程很有用，同样在工作完成后查看工作统计和日志时也会很有用。
 4.6.1 JobTracker页面
JobTracker页面主要包括五部分。
第一部分是Hadoop安装的详细信息，比如版本号、编译完成时间、JobTracker当前的运行状态和开始时间。
第二部分是集群的一个总结信息：集群容量（用集群上可用的Map和Reduce任务槽的数量表示）及使用情况、集群上运行的Map和Reduce的数量、提交的工作总量、当前可用的TaskTracker节点数和每个节点平均可用槽的数量。
第三部分是一个正在运行的工作日程表。打开能看到工作的序列。
第四部分显示的是正在运行、完成、失败的工作，这些显示信息通过表格来体现。表中每一行代表一个工作并且显示了工作的ID号、所属者、名字和进程信息。
最后一部分是页面的最下面JobTracker日志的链接和JobTracker的历史信息：JobTracker运行的所有工作信息。在将这些信息提交到历史页面之前，主要显示100个工作（可以通过mapred.job.name进行配置）。注意，历史记录是永久保存的，因此可以从JobTracker以前运行的工作中找到相关的记录。
4.6.2 工作页面
点击一个工作的ID将看到它的工作页面。在工作页面的顶部是一个关于工作的一些总结性基本信息，比如工作所属者、名字、工作文件和工作已经执行了多长时间等。工作文件是工作的加强配置文件，包含在工作运行期间所有有效的属性及它们的取值。如果不确定某个属性的取值，可以点击进一步查看文件。
当工作运行时，可以在页面上监控它的进展情况，因为页面会周期性更新。在总结信息的下面是一张表，它显示了Map和Reduce的进展情况。“任务栏”显示了该工作的Map和Reduce任务的总数（Map和Reduce各占一行）。其他列显示了这些任务的状态：“暂停”（等待执行）、“正在执行”、“完成”（运行成功）、“终止”（准确地说应该称为“失败”），最后一列显示了失败或终止的任务所尝试的总数。
图4-1显示工作页面最下面的内容。
图4-1是每个任务完成情况的一个图形化表示。Reduce完成图分为3个阶段：复制（发生在将Map输出转交给Reduce的TaskTracker时）、排序（发生在Reduce输入合并时）和Reduce（发生在Reduce函数起作用并产生最终输出时）。
图 4-1 每个Map和Reduce任务的执行进度
4.6.3 返回结果
执行完任务后，可以通过以下几种方式得到结果。
1）通过命令行直接显示输出文件夹中的文件。
命令行如下：
hadoop dfs-ls outputOfScoreProcessFinal
通过以上命令的执行结果可以发现，输出的结果中一共有6个文件，分别是part-r-00000到part-r-00005。还可以具体显示每个文件中的内容，例如要显示part-r-00000中的内容，命令如下：
hadoop dfs-cat outputOfScoreProcessFinal/part-r-00000
2）将输出的文件从HDFS复制到本地文件系统上，在本地文件系统上查看。
命令如下：
hadoop dfs-get outputOfScoreProcessFinal/*/home/u/outputOfScoreProcessFinal
上述命令的主要功能是将HDFS中目录outputOfScoreProcessFinal下的所有文件复制到本地文件系统的目录/home/u/outputOfScoreProcessFinal下，然后就可以方便地进行查看了。
另外还可以在命令行中将输出文件part-r-00000到part-r-00005合并成一个文件，并复制到本地文件系统中。下面就是在命令行中进行的操作：
hadoop dfs-getmerge outputOfScoreProcessFinal/home/u/outputScore
上述命令的功能就是，将HDFS中目录outputOfScoreProcessFinal下的所有文件（即part-r-00000到part-r-00005）进行合并，然后复制到本地文件系统中的目录/home/u/outputScore下。
3）通过Web界面查看输出的结果。
通过浏览器访问集群的NameNode界面，点击页面上的“Browse the filesystem”即可看到HDFS中的内容，依次点击home、u、outputOfScoreProcessFinal，就可以看到程序的输出文件，再点击各个具体的输出文件可以查看输出内容。
4.6.4 任务页面
工作页面中的一些链接可以用来查看该工作中任务的详细信息。例如，点击“Map”链接，将看到一个页面，所有的Map任务信息都列在这一页上。当然，也可以只看已经完成的任务。任务页面显示信息以表格形式来体现，表中的每一行都表示一个任务，它包含了诸如开始时间、结束时间之类的信息，以及由TaskTracker提供的错误信息和查看单个任务的计数器的链接。同样，点击“Reduce”链接也可以看到一个页面，所有的Reduce任务信息都列在这一页上。同样可以只看已经完成的任务。显示的信息内容与Map界面的相同。
4.6.5 任务细节页面
在任务页面上可以点击任何任务来得到关于它的详细信息。图4-2的任务细节页面显示了每个任务的尝试情况。在这里，只有一个任务尝试并且成功完成。图中包含的表格提供了更多的有用数据，比如任务尝试是在哪个节点上运行的，同时还可以查看任务日志文件和计数器的链接。这个表中还包含“Actions”列，可终止一个任务尝试的链接。默认情况下，这项功能是没有启用的，网络用户界面只是一个只读接口。将webinterface.private.actions设为true即可启用这项功能。
对于Map任务，有一个部分（即图4-2中的“Input Split Location”区域）信息显示了输入的片段被分配到了哪个节点上。
图 4-2 任务尝试页面
4.7 性能调优
一个程序可以完成基本功能其实还不够，还有一些具有实际意义的问题需要解决，比如性能是不是足够好、有没有提高的空间等。具体来讲包括两个方面的内容：一个是时间性能；另一个是空间性能。衡量性能的指标就是，能够在正确完成功能的基础上，使执行的时间尽量短，占用的空间尽量小。
前面只是实现了程序基本应该实现的功能，对性能问题并没有加以考虑。下面就从不同的角度来简单地介绍一下提高性能的方法。
 4.7.1 输入采用大文件
在前面的例子当中，笔者的实验数据包含1000个文件，在HDFS中共占用了1000个文件块，而每一个文件的大小都是2.3MB，相对于HDFS块的默认大小64MB来说算是比较小的了。如果MapReduce在处理数据时，Map阶段输入的文件较小而数量众多，就会产生很多的Map任务，以前面的输入为例，一共产生了1000个Map任务。每次新的Map任务操作都会造成一定的性能损失。针对上述2.2GB大小的数据，在实验环境中运行的时间大概为33分钟。
为了尽量使用大文件的数据，笔者对这1000个文件进行了一次预处理，也就是将这些数量众多的小文件合并成大一些的文件，最终将它们合并成了一个大小为2.2GB的大文件。然后再以这个大文件作为输入，在同样的环境中进行测试，运行的时间大概为4分钟。
从实验结果可以很明显地看出二者在执行时间上的差别非常大。因此为了提高性能，应该对小文件做一些合理的预处理，变小为大，从而缩短执行的时间。不仅如此，合并前的众多文件在HDFS中占用了1000个块，而合并后的文件在HDFS中只占用36个块（64MB为一块），占用空间也相应地变小了，可谓一举两得。
另外，如果不对小文件做合并的预处理，也可以借用Hadoop中的CombineFileInputFormat。它可以将多个文件打包到一个输入单元中，从而每次执行Map操作就会处理更多的数据。同时，CombineFileInputFormat会考虑节点和集群的位置信息，以决定哪些文件被打包到一个单元之中，所以使用CombineFileInputFormat也会使性能得到相应地提高。
4.7.2 压缩文件
在分布式系统中，不同节点的数据交换是影响整体性能的一个重要因素。另外在Hadoop的Map阶段所处理的输出大小也会影响整个MapReduce程序的执行时间。这是因为Map阶段的输出首先存储在一定大小的内存缓冲区中，如果Map输出的大小超出一定限度，Map task就会将结果写入磁盘，等Map任务结束后再将它们复制到Reduce任务的节点上。如果数据量大，中间的数据交换会占用很多的时间。
一个提高性能的方法是对Map的输出进行压缩。这样会带来以下几个方面的好处：减少存储文件的空间；加快数据在网络上（不同节点间）的传输速度，以及减少数据在内存和磁盘间交换的时间。可以通过将mapred.compress.map.output属性设置为true来对Map的输出数据进行压缩，同时还可以设置Map输出数据的压缩格式，通过设置mapred.map.output.compression.codec属性即可进行压缩格式的设置。
4.7.3 过滤数据
数据过滤主要指在面对海量输入数据作业时，在作业执行之前先将数据中无用数据、噪声数据和异常数据清除。通过数据过滤可以降低数据处理的规模，较大程度地提高数据处理效率，同时避免异常数据或不规范数据对最终结果造成负面影响。
在数据处理的时候如何进行数据过滤呢？在MapReduce中可以根据过滤条件利用很多办法完成数据预处理中的数据过滤，比如编写预处理程序，在程序中加上过滤条件，形成真正的处理数据；也可以在数据处理任务的最开始代码处加上过滤条件；还可以使用特殊的过滤器数据结果来完成过滤。下面笔者以一种在并行程序中功能强大的过滤器结构为例来介绍如何在MapReduce中对海量数据进行过滤。
Bloom Filter是在1970年由Howard Bloom提出的二进制向量数据结构。在保存所有集合元素特征的同时，它能在保证高效空间效率和一定出错率的前提下迅速检测一个元素是不是集合中的成员。Bloom Filter的误报（false positive）只会发生在检测集合内的数据上，而不会对集合外的数据产生漏报（false negative）。这样每个检测请求返回有“在集合内（可能错误）”和“不在集合内（绝对不在集合内）”两种情况，可见Bloom Filter牺牲了极少正确率换取时间和空间，所以它不适合那些“零错误”的应用场合。在MapReduce中，Bloom Filter由Bloom Filter类（此类继承了Filter类，Filter类实现了Writable序列化接口）实现，使用add（Key key）函数将一个key值加入Filter，使用membershipTest（Key key）来测试某个key是否在Filter内。
以上说明了Bloom Filter的大概思想，那么在实践中如何使用Bloom Filter呢？假设有两个表需要进行内连接，其中一个表非常大，另一个表非常小，这时为了加快处理速度和减小网络带宽，可以基于小表创建连接列上的Bloom Filter。具体做法是先创建Bloom Filter对象，将小表中所有连接列上的值都保存到Bloom Filter中，然后开始通过MapReduce作业执行内连接。在连接的Map阶段，读小表的数据时直接输出以连接列值为key、以数据为value的＜key, value＞对；读大表数据时，在输出前先判断当前元组的连接列值是否在Bloom Filter内，如果不存在就说明在后面的连接阶段不会使用到，不需要输出，如果存在就采用与小表同样的输出方式输出。最后在Reduce阶段，针对每个连接列值连接两个表的元组并输出结果。
大家已经知道了Bloom Filter的作用和使用方法，那么Bloom Filter具体是如何实现的呢？又是如何保证空间和时间的高效性呢？如何用正确率换取时间和空间的呢？（基于MapReduce中实现的BloomFilter代码进行分析）Bloom Filter自始至终是一个M位的位数组：
private static final byte[]bitvalues=new byte[]{
（byte）0x01，
（byte）0x02，
（byte）0x04，
（byte）0x08，
（byte）0x10，
（byte）0x20，
（byte）0x40，
（byte）0x80
}；
它有两个重要接口，分别是add（）和membershipTest（），add（）负责保存集合元素的特征到位数组（类似于一个学习的过程），在保存所有集合元素特征之后可以使用membershipTest（）来判断某个值是否是集合中的元素。
在初始状态下，Bloom Filter的所有位都被初始化为0。为了表示集合中的所有元素，Bloom Fliter使用k个互相独立的Hash函数，它们分别将集合中的每个元素映射到（1，2，……，M）这个范围上，映射的位置作为此元素特征值的一维，并将位数组中此位置的值设置为1，最终得到的k个Hash函数值将形成集合元素的特征值向量，同时此向量也被保存在位数组中。从获取k个Hash函数值到修改对应位数组值，这就是add接口所完成的任务。
public void add（Key key）{
if（key==null）{
throw new NullPointerException（"key cannot be null"）；
}
int[]h=hash.hash（key）；
hash.clear（）；
for（int i=0；i＜nbHash；i++）{
bits.set（h[i]）；
}
}
利用add接口将所有集合元素的特征值向量保存到Bloom Filter之后，就可以使用此过滤器也就是membershipTest接口来判断某个值是否是集合元素。在判断时，首先还是计算待判断值的特征值向量，也就是k个Hash函数值，然后判断特征值向量每一维对应的位数组位置上的值是否是1，如果全部是1，那么membershipTest返回true，否则返回false，这就是判断值是否存在于集合中的原理。
public boolean membershipTest（Key key）{
if（key==null）{
throw new NullPointerException（"key cannot be null"）；
}
int[]h=hash.hash（key）；
hash.clear（）；
for（int i=0；i＜nbHash；i++）{
if（！bits.get（h[i]））{
return false；
}
}
return true；
}
从上面add接口和membershipTest接口实现的原理可以看出，正是Hash函数冲突的可能性导致误判的可能。由于Hash函数冲突，两个值的特征值向量也有可能冲突（k个Hash函数全部冲突）。如果两个值中只有一个是集合元素，那么该值的特征值向量会保存在位数组中，从而在判断另外一个非集合元素的值时，会发现该值的特征值向量已经保存在位数组中，最终返回true，形成误判。那么都有哪些因素影响了错误率呢？通过上面的分析可以看出，Hash函数的个数和位数组的大小影响了错误率。位数组越大，特征值向量冲突的可能性越小，错误率也小。在位数组大小一定的情况下，Hash函数个数越多，形成的特征值向量维数越多，冲突的可能性越小；但是维数越多，占用的位数组位置越多，又提高了冲突的可能性。所以在实际应用中，在使用Bloom Filter时应根据实际需要和一定的估计来确定合适的数组规模和哈希函数规模。
通过上面的介绍和分析可以发现，在Bloom Filter中插入元素和查询值都是O（1）的操作；同时它并不保存元素而是采用位数组保存特征值，并且每一位都可以重复利用。所以同集合、链表和树等传统方法相比，Bloom Filter无疑在时间和空间性能上都极为优秀。但错误率限制了Bloom Filter的使用场景，只允许误报（false positive）的场景；同时由于一位多用，因此Bloom Filter并不支持删除集合元素，在删除某个元素时可能会同时删除另外一个元素的部分特征值。图4-3是一个简单的例子，既说明了Bloom Filter的实现过程，又说明了错误发生的原因（步骤⑤判断的值是包含在集合中的，但是返回值为true）。
图 4-3 Bloom Filter实现过程图
4.7.4 修改作业属性
属性mapred.tasktracker.map.tasks.maximum的默认值是2，属性mapred.tasktracker.map.tasks.maximum的默认值也是2，因此每个节点上实际处于运行状态的Map和Reduce的任务数最多为2，而较为理想的数值应在10～100之间。因此，可以在conf目录下修改属性mapred.tasktracker.map.tasks.maximum和mapred.tasktracker.reduce.tasks.maximum的取值，将它们设置为一个较大的值，使得每个节点上同时运行的Map和Reduce任务数增加，从而缩短运行的时间，提高整体的性能。
例如下面的修改：
＜property＞
＜name＞mapred.tasktracker.map.tasks.maximum＜/name＞
＜value＞10＜/value＞
＜description＞The maximum number of map tasks that will be run
simultaneously by a task tracker.
＜/description＞
＜/property＞
＜property＞
＜name＞mapred.tasktracker.reduce.tasks.maximum＜/name＞
＜value＞10＜/value＞
＜description＞The maximum number of reduce tasks that will be run
simultaneously by a task tracker.
＜/description＞
＜/property＞
4.8 MapReduce工作流
到目前为止，已经讲述了使用MapReduce编写程序的机制。不过还没有讨论如何将数据处理问题转化为MapReduce模型。
数据处理只能解决一些非常简单的问题。如果处理过程变得复杂了，这种复杂性会通过更加复杂、完善的Map和Reduce函数，甚至更多的MapReduce工作来体现。下面简单介绍一些比较复杂的MapReduce编程知识。
 4.8.1 复杂的Map和Reduce函数
从前面Map和Reduce函数的代码很明显可以看出，Map和Reduce都继承自MapReduce自己定义好的Mapper和Reducer基类，MapReduce框架根据用户继承Mapper和Reducer后的衍生类和类中覆盖的核心函数来识别用户定义的Map处理阶段和Reduce处理阶段。所以只有用户继承这些类并且实现其中的核心函数，提交到MapReduce框架上的作业才能按照用户的意愿被解析出来并执行。前面介绍的MapReduce作业仅仅继承并覆盖了基类中的核心函数Map或Reduce，下面介绍基类中的其他函数，使大家能够编写功能更加复杂、控制更加完备的Map和Reduce函数。
1.setup函数
此函数在基类中的源码如下：
/**
*Called once at the start of the task.
*/
protected void setup（Context context
）throws IOException, InterruptedException{
//NOTHING