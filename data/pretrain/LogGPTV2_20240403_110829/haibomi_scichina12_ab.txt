r02e11004 
r02e11005 
r02e11006 
r02e11007 
r02e11008 
r02e11009 
r02e11010 
r02h05001 
r02h05002 
r02h05003 
r02h05004r02h05002 
r02h05003 
r02h05004 
r02h05005 
r02h05006 
r02h05007 
r02h05008 
r02h05009 
r02h05010 
r02h08006 
r02j05015 
r02j05016 
r02j05017 
r02j11001 
r02j11002 
r02j11003 
r02j11004 
r02k02001 
r02k02002 
r02k02003 
r02k02004 
r02k02005 
r02k02006 
r02k02007 
r02k02008 
r02k02009 
r02k02010 
r03b02033 
r03b02034 
r03b02035 
r03b02036 
r03b02037 
r03b02038 
r03b02039 
r03b02040 
r03b02041r03b02039 
r03b02040 
r03b02041 
r03b05001 
r03b05002 
r03b05003 
r03b05004 
r03b05006 
r03b05007 
r03b05008 
r03b05024 
r03b05025 
r03b05034 
r03b05035 
r03c05045 
r03c05046 
r03c05047 
r03c05048 
r03c08024 
r03c08026 
r03c08027 
r03c08028
Host ID
Figure 1 	The spatial distribution of the time-out anomaly for one anomalous method in a production cluster for 24 hours.cloud computing systems. Such systems generally consist of numerous software components and serve tremendous user requests simultaneously. Furthermore, to achieve elasticity, each component generally has a lot of duplicated instances. Replicated instances enhance the scalability and availability of the sys-tem, but make performance anomaly diagnosis more complex. Sometimes, when performance anomalies happen, the defect of a component is only manifested in part of instances and anomalous instances are mixed with normal ones . For example, a defective load balance policy in a storage service may result in some overloaded storage component instances, whereas, the remaining instances may behave normal-ly. Figure 1 shows the spatial distribution of the time-out anomaly for one anomalous method in one Alibaba prodution cluster. We can see that most time-out anomalies only happened in five instances in 24 hours. In this situation, operators not only want to know which invoked methods (i.e. logical compo-nent) become anomalous, but also need to identify their instances (i.e., physical locations). Hence, when the system suffers performance degradation (e.g., the average response time of user requests increases), locating anomalous instances becomes of critical importance.However, current approaches generally focus on locating anomalous physical nodes (e.g., [5]) or logi-cal components (e.g., [6–8]). Such coarse-grained results are not enough. In the former case, given an anomalous physical node, system operators have to identify the faulty component among many com-ponents hosted in the physical node. 	In the latter case, given an anomalous logical component, the operators have to identify which ones among its tremendous instances distributed in the cloud are faulty. Consequently, huge human efforts are still required to further pinpoint the subtle root cause.Furthermore, many existing research investigations utilize rule-based approaches (see, e.g., [9]) or expectation-based approaches (e.g., [7,10]) to diagnose performance. Engineers are required to manually design detecting rules or expectations according to their specific domain knowledge. However, a produc-tion cloud system generally offers a lot of concurrent services and user requests because these services are complicated. Engineers have much difficulty in understanding the characteristics of component in-teractions. It is beyond the engineers’ ability to construct such rules or expectations in cloud computing system.This paper is an extended work of our previous research [11] that localizes the anomalous invoked
Mi H B, et al. Sci China Inf Sci December 2012 Vol. 55 No. 12 2759methods. In this paper, without any specific domain knowledge, we aim at not only localizing anomalous methods but also the anomalous instances that are the physical locations of the anomalous methods. Since typical cloud computing systems are service-oriented, the response time of user requests naturally reflects the system performance. In this regard, an end-to-end tracing user request approach is a viable means to expose performance data so as to help performance diagnosis. Therefore, based on request trace logs, we solve the problem with two steps. First, the anomalous methods are localized with three sub-steps: (1) cluster the user requests into categories; (2)identify anomalous requests within the same category through the principal component analysis [12] and separate the normal and anomalous requests into two sets; (3) compare the behavior of the same invoked methods in normal and anomalous sets with Mann-Whitney non-parameter statistical hypothesis test [13] and pick out anomalous methods. Second, anomalous instances are localized with two additional sub-steps: (1) group the latencies of an anomalous method by the host addresses of instances and create histograms for each of them; (2) compare the similarities among these histograms with Jensen-Shannon divergence [14] and localize the histograms whose behaviors are the most different from those of others, which are considered to be the culprit of the anomalous methods.We verify the effectiveness of our approach in the Alibaba cloud computing platform, which is a real-world enterprise-class cloud computing infrastructure providing services to the public in China. The ex-perimental results demonstrate that our approach can locate the primary causes of performance anomalies with a low false-positive rate and false-negative rate. So far, our approach has been successfully applied in the Alibaba cloud computing platform to diagnose performance anomalies in both testing and production clusters.The remainder of this paper is organized as follows. Section 2 compares our approach with the related work. In Section 3, we briefly introduce the workflow of our approach. Section 4 and Section 5 respectively present how to localize the anomalous methods and service instances in detail. In Sections 6 and 7, we give the experimental scenarios and results. Section 8 concludes this paper.
2 	Related workEnd-to-end request tracing approaches are efficient for operators to conduct performance debugging. Basically, in order to get request trace data, there are two kinds of instrumentation mechanisms: white-box based mechanism and black-box based mechanism. A white-box based mechanism (e.g., [15–17]) assumes the availability of the source codes and utilizes explicit global identifiers to correlate runtime events; while a black-box-based mechanism (e.g., [18–22]) assumes no knowledge of the source codes and adopts probabilistic correlation methods or statistical regression techniques to infer the casual paths. Since the source codes are available in typical production cloud systems, in this paper, we utilize a white-box instrumented mechanism to trace requests.Pinpoint [23] traces request call relationship in multi-layers of Web service components and adopts a clustering algorithm to group failure and success logs. It finds out the anomalous components through dependency mining and a probabilistic context free grammar. 	Chen et al. [24] present a thoughtful discussion on how request tracing can help operators on the process of performance anomaly detection and diagnosis. Magpie [25] uses event schema to correlate requests and clusters requests according to the similarity of structure and timing of requests’ paths. X-trace [26] constructs the causal relationship of requests through modifying the transport protocol and detects the anomaly through comparing the structure difference of requests. Pip [7] and Ironmodel [10] apply users’ expectation to determine whether a request is anomalous or not. The common ground of these researches is that they use self-definition event schema or expected models to detect requests. It is very hard to construct these models because it requires much specific domain knowledge. However, our approach makes the intrinsic characteristics of trace logs to diagnose the anomalies without domain knowledge.Spectroscope [8] groups the paths of requests by call structures and finds the anomalous requests through comparing the behaviors of requests in two time periods. This research focuses on performance changes of two time periods, while our work tries to narrow down the space of potential root causes