movements of multiple articulators [29]. Specifically, articulatory
gesture is used to describe the connection between the lexical units
with the articulator dynamic when producing speech sounds. For
English speech production, the coordination among multiple artic-
ulators produces gestures like lip protrusion, lip closure, tongue tip
and tongue body constriction, and jaw angle. For example, three
articulators including upper lip, lower lip and jaw are involved
Figure 2: Articulators, phonemes and the corresponding ar-
ticulatory gestures.
Figure 3: An illustration of sensing the articulatory gesture
when a user speaks a passphrase on the phone.
when a speaker conducts the gesture of lip closure, which could
lead to the phoneme sounds of [p], [b] and [m].
Figure 2 illustrates various articulators and their locations as
well as the phonemes and the corresponding articulatory gestures.
Each phoneme sound production usually involves multidimensional
movements of multiple articulators. For instance, the pronunciation
of the phoneme [p] requires upper and lower lips horizontal move-
ments and jaw angle change. Moreover, although some phonemes
share the same type of articulator gesture, the movement speed and
intensity could be different. For example, both [d] and [z] require
the tongue tip constriction, however they differ in terms of the
exact tongue tip radial and angular position.
2.3 Sensing the Articulatory Gesture
To sense the articulatory gesture, we leverage the phenomenon of
Doppler effect, which is the change in the observed wave frequency
as the receiver and the transmitter move relative to each other. A
common example of Doppler effect is the change in the pitch of
an ambulance’s siren as it approaches and departs from a listener.
Figure 3 shows one example of sensing the particularity gesture
when a user speaks a passphrase by holding the phone to his left
ear. The build-in speaker of the phone emits a high frequency tone,
which is reflected by multiple articulators of the user. The reflections
are then recorded by the built-in microphone of the same phone. In
our context, the articulators reflecting the signals from the speaker
Voice DataSpeaker ModelDatabaseClassifier(Accept or Reject)Voice FeatureReferenceDecisionMicrophoneFeature Extractorupper lip lower lip tongue tip tongue body center glottis velum jaw Phonemes Articulators Motion Variables p, b, m upper lip lower lip jaw lip vertical movement lip vertical movement jaw angle t, d, n, s, z, sh, zh tongue tip tongue body jaw radial and angular radial and angular jaw angle k, g,  vowels tongue body jaw radial and angular jaw angle m, n, velum velum movement p, t, k, s, sh glottal glottal movement uw, uh, ow  lip protrusion horizontal movement Session A2:  Human AuthenticationCCS’17, October 30-November 3, 2017, Dallas, TX, USA59can be thought of as virtual transmitters that generate the reflected
sound waves. As the articulators move towards the microphone,
the crests and troughs of the reflected sound waves arrive at the
microphone at a faster rate. Conversely, if the articulators move
away from the microphone, the crests and troughs arrive at a slower
rate. In particular, an articulator moving at a speed of v with an
angle of α from the microphone results in a Doppler shift [35] (i.e.,
frequency change ∆f ) of:
∆f ∝ v cos(α)
(1)
c
where f0 is the frequency of the transmitted sound wave and c is
the speed of sound in the medium.
f0,
We observe from Equation (1) that a higher frequency of the
emitted sound (i.e., f0) results in a larger Doppler shift for the
same articulator movements. We thus choose to emit a high fre-
quency sound at 20kHz, which is close to the limit of the built-in
speaker/microphone of current popular smartphones. Such a high
frequency signal maximizes the Doppler shifts caused by the artic-
ulatory gesture and is also inaudible to human ear.
Moreover, the observed Doppler shift depends on the the moving
direction of the articulator (i.e., α). An articulator moving away
from the microphone results in negative Doppler shift, while an
articulator moving towards the microphone leads to a positive
Doppler shift. As each phoneme pronunciation involves multidi-
mensional movements of multiple articulators, the resulted Doppler
shifts at the microphone are a superposition of sinusoids at differ-
ent shifts. For instance, the phoneme sound [o] requires the lip
closure gesture, which involves upper lip and jaw moving towards
the microphone and lower lip and tongue moving away from the
microphone. We thus could observe a set of Doppler shifts includ-
ing both positive and negative shifts that can be used to distinguish
different articulatory gestures.
In addition, a faster speed (i.e., v) results in a larger Doppler shift.
The magnitude of the Doppler shift thus can be further utilized
to distinguish different gestures or people that produce the same
phoneme sound with various speeds. Furthermore, the reflections
from the articulators that closer to the microphone result in stronger
energy due to the signal attenuation in the medium. For example,
the lip movement usually results in a higher energy in its Doppler
shift than that of the tongue tip. The energy distribution of the
Doppler shifts thus provide another dimension of information for
differentiating articulatory gestures.
2.4 Loudspeaker
Unlike the human, loudspeaker relies on solely the diaphragm that
moves in one dimension to produce sound wave [11]. As shown
in Figure 4, the diaphragm is moving forward and backward to
increase and decrease the air pressure in front of it, thus creating
sound waves. The diaphragm is usually driven by the voice coil1,
which converts electrical signals to magnetic energy. By increasing
and decreasing the amount of electrical current, the voice coil pro-
duces a magnetic field of varying strength, which interacts with the
internal permanent magnet. The permanent magnet thus attracts
or repels both the voice coil and the attached diaphragm to move
1Although the diaphragm is driven by stators for electrostatic loudspeaker, it still relies
on the movements of diaphragm for sound production.
Figure 4: An illustration of a loudspeaker.
froward or backward. The specific movements of the diaphragm are
controlled by the frequency and intensity of the input audio signal.
For instance, the input sound that possesses a high pitch results in
fast movement of the diaphragm, while when a user turning up the
volume, the diaphragm pushes harder to produce a higher pressure
in the air.
A loudspeaker could be distinguished from a live speaker based
on the movement of articulators. First, they differ in terms of the
movement complexity and the number of the articulators. In addi-
tion, the movement of human articulator does not always produce
sound, whereas the movement of diaphragm certainly results in
sound wave. Figure 5 shows the Doppler shifts sensed by the probe
sound at 20kHz for a loudspeaker replay and a live user, respec-
tively. The frequency distribution inside each pair of vertical bars
in the figure corresponds to the Dopplor shifts resulted from one
phoneme sound. We could observe that the Doppler shifts of the
loudspeaker look relatively clean due to much simpler diaphragm
movements. The Doppler shifts caused by the complex movements
of multiple articulators of a live user spread out over a much larger
volume of space. For instance, to pronounce the phoneme [ai], a
human speaker first opens his mouth on vertical direction and then
gradually changes to horizontal direction. This procedure involves
massive movements that result in a diverse of Doppler shifts than
that of a loudspeaker.
2.5 Individual Diversity of Articulator Gesture
There exist minute differences in articulatory gesture among people
when producing the same phoneme due to the individual diversity
in the human vocal tract and the habitual way of pronunciation.
For example, research shows that different people adopt different
movement trajectories of articulators to produce the same utter-
ance [32]. Also, the physiological features of vocal tract vary among
people, such as the size and shape of lips and tongue [41]. Moreover,
there is a diverse articulatory strategies for sound production. For
instance, some speakers’ jaw movement is closely connected with
tongue body gesture, while others are not [22].
To assess whether we are able to capture the minute differences
in users’ articulatory gestures with current smartphone, we use the
articulator movement speed among people as one example [31]. Fig-
ure 6 shows the statistics of the movement speeds of both upper lip
and jaw for five people when producing the same phoneme sound.
Permanent MagnetDiaphragmVoice CoilForwardBackwardSession A2:  Human AuthenticationCCS’17, October 30-November 3, 2017, Dallas, TX, USA60Figure 5: Doppler shifts of a live user and a speaker replay.
Figure 6: Velocity diversity in upper lip and jaw ges-
tures [27].
We observe a diverse range of movement speed. The averaged dif-
ference of the speed is 0.04m/s for upper lip and 0.06m/s for jaw,
respectively. Given the duration of producing a phoneme sound is
around 250ms [47], we could achieve 1Hz resolution under 192kHz
sampling frequency when analyzing the Doppler shifts of each indi-
vidual phoneme. With the probe sound at 20kHz, 1Hz Doppler shift
corresponds to an articulator speed of 0.017m/s, which provides
much higher sensitivity than that of the speed difference in both
upper lip and jaw movements (i.e., 0.04m/s and 0.06m/s). We thus
could be able to differentiate different people even if they are pro-
nouncing the same phoneme sound with 20kHz prob sound wave
at 192kHz sampling frequency. Of course, the differences in articu-
latory gesture are expected to be much smaller under the mimicry
attacks, where an adversary mimics the articulatory gesture of
a genuine user. Nevertheless, each articulatory gesture involves
movements of multiple articulators, which provide more informa-
tion to detect the attacks. In addition, each passphrase consists of
Figure 7: Illustration of the articulatory gesture based live-
ness detection on smartphone.
a sequence of phoneme sounds, which dramatically increase the
possibility to distinguish between a genuine user and an attacker.
3 SYSTEM DESIGN
In this section we introduce our system design and its core compo-
nents and algorithms.
3.1 Approach Overview
The key idea underlying our liveness detection system is to lever-
age the mobile audio hardware advances to sense the articulatory
gesture of a sequence of phoneme sounds when a user speaks
passphrases to a smartphone. As illustrated in Figure 7, the built-
in speaker at the bottom of the phone starts to emit an inaudible
acoustic tone at 20kHz once the authentication system is triggered.
When a user speaks a passphrase, the built-in microphone records
user’s voice as well as the inaudible acoustic tone and its reflections.
Speaking a passphrase involves multidimensional movements of
multiple articulators, which result in Doppler frequency shifts in
the reflected signals. In particular, the articulators moving toward
(away from) the microphone lead to positive (negative) Doppler
shifts. While the articulators that closer to the microphone result
in stronger energy in the Doppler shifts, the articulators move
at faster speeds lead to large Doppler shifts. Once finish record-
ing, the voice sample of the user (which is usually located below
10kHz) is separated for conventional voice authentication, leaving
the high frequency band at around 20kHz for extracting features
in the Doppler shifts. The system extracts features based on both
frequency shift distribution and energy distribution in the observed
Doppler shifts. The extracted features are then compared against
the ones obtained when user enrolled in the system for live user
detection.
A live user is declared if the similarity score exceeds a predefined
threshold. Under playback attacks, the extracted features of Doppler
shifts are different from the ones obtained from a live user due to the
fundamental difference between the human speech production sys-
tem and the loudspeaker sound production system. Under mimicry
attacks, the extracted features can capture the minute differences
through a sequence of phoneme sounds due to individual diversity
of human vocal tract and the habitual way of pronunciation. Also, it
Time(s) Live User Loudspeaker [ai] [ai] 020406080100120140160s1s2s3s4s5Articulators Velocities(m/s) Upper LipJawX 10-3 5 speakers 3.The voice is separated for authentication and  Doppler shifts are extracted for feature extraction. 4.   Both the frequency-based and energy-based  features are extracted for liveness detection. 2.   The microphones records both the frequency shifts at around 20kHz and the voice sample.  1.The built-in speaker emits  20KHz tone  and microphone listens the reflections.  20.2K 19.8K 20K 0.2 0.15 0.1 0.05 20K Doppler Shifts [lai] [k] [s] [p] [wi:] [dei] 0 8K Audible Voice Session A2:  Human AuthenticationCCS’17, October 30-November 3, 2017, Dallas, TX, USA61is possible for an attacker to place a recording device (e.g., a smart-
phone emitting and recording at 20kHz) surreptitiously in close
proximity to a legitimate user to record the Doppler shifts when
the user speaks a passphrase. As the Doppler shift pattern is tied to
the phone placement, the recorded Doppler shifts by the attacker
are different from the ones sensed by the legitimate device (e.g.,
user’s smartphone) as long as the attacker has a phone placement
different from the one that the legitimate user used for enrollment.
Our system works when the users hold the phones with their
nature habits as opposed to the prior smartphone based solutions
that require users to hold or move the phone in some predefined
manners. Comparing to the commercially used challenge-response
based solutions, our system does not require any cumbersome
operations besides the conversional authentication process. Once it
integrated with voice authentication system, the liveness detection
is totally transparent to the users.
Our system however does require the built-in speaker and micro-
phone to playback and record sound wave at a high frequency. The
audio hardware on current popular smartphones (e.g., Galaxy S5,
S6, and iPhone 5 and 6) has frequency response well above 20kHz.
As mobile devices are increasingly supporting high definition audio
capabilities, we envision the low-end phones could also reliably
record and playback sound wave at high frequencies in the very
near future. Moreover, certain data protection methods should be
deployed to prevent an attacker from obtaining the plain-text of
the extracted features. For example, the feature extraction could
be done locally at smartphone and only the encrypted features are
transmitted for liveness detection.
3.2 System Flow
Realizing our system requires five major components: Doppler Shifts
Extraction, Feature Extraction, Wavelet-based Denoising, Similarity
Comparison, and Detection. As shown in Figure 8, the acoustic signal
captured by the phone’s microphone first passes through Doppler
shifts Extraction process, which extracts the Doppler shifts for each
phoneme sound in the spoken utterance. We rely on the audible
voice sample of the user for separating each phoneme and the cor-
responding Doppler shifts. In particular, we apply Hidden Markov
Modeling (HMM) based forced alignment to recognize and separate
each phoneme in the voice sample. Then, we map the segmentation
to the inaudible frequency range at around 20KHz frequency to
extract the Doppler shifts of each individual phoneme.
Next, the Feature Extraction component is used to extract both
energy-band and frequency-band features from the Doppler shifts.
Specifically, our system first partitions the Doppler shifts into sev-
eral sub-bands based on both energy and frequency levels. It then ex-
tracts both the frequency-based and energy-based contours within
each sub-band. These extracted frequency-based and energy-based
contours capture the movements of multiple articulators in terms
of relative positions and relative velocities.
Then we utilize wavelet-based denoising technique to further
remove the mixed noises by decomposing each contour into approx-
imation and detailed coefficients. A dynamic threshold is applied
to the detailed coefficients to remove the noisy components while
retaining sufficient details. After that, we reconstruct the features
Figure 8: The flow of our liveness detection system.
by combining approximation coefficients and denoised detailed
coefficients.
At last, our system matches the frequency-based and energy-
based features with the ones stored in the liveness detection system
by using cross correlation coefficient. It yields a similarity score,
which is compared against a predefined threshold. If the score is
higher than the threshold, a live user is detected, otherwise an
attack is declared.