ve
Mutant
s
The mutant was acquired during a DPC.
Mut
ant
Res
erv
ed
Mutant
s
Unused.
Que
ueT
ype
Queues
Mapping of the Type field.
Aba
ndo
ned
Queues
The queue no longer has any threads that are 
waiting on it.
Dis
abl
eIn
cre
men
t
Queues
No priority boost should be given to a thread 
waking up to handle a packet on the queue.
Finally, the dispatcher header also has the SignalState field,
which we previously mentioned, and the WaitListHead, which was
also described. Keep in mind that when the wait list head pointers
are identical, this can either mean that there are no threads waiting
or that one thread is waiting on this object. You can tell the
difference if the identical pointer happens to be the address of the
list itself—which indicates that there’s no waiting thread at all. In
the earlier example, 0XFFFF898F2B3451C0 was not the address
of the list, so you can dump the wait block as follows:
Click here to view code image
lkd> dx (nt!_KWAIT_BLOCK*)0xffff898f2b3451c0
(nt!_KWAIT_BLOCK*)0xffff898f2b3451c0         : 
0xffff898f2b3451c0 [Type: _KWAIT_BLOCK *]
    [+0x000] WaitListEntry    [Type: _LIST_ENTRY]
    [+0x010] WaitType         : 0x1 [Type: unsigned char]
    [+0x011] BlockState       : 0x4 [Type: unsigned char]
    [+0x012] WaitKey          : 0x0 [Type: unsigned short]
    [+0x014] SpareLong        : 6066 [Type: long]
    [+0x018] Thread           : 0xffff898f2b345080 [Type: 
_KTHREAD *]
    [+0x018] NotificationQueue : 0xffff898f2b345080 [Type: 
_KQUEUE *]
    [+0x020] Object           : 0xffff898f2b64ba60 [Type: 
void *]
    [+0x028] SparePtr         : 0x0 [Type: void *]
In this case, the wait type indicates a WaitAny, so we know that
there is a thread blocking on the event, whose pointer we are given.
We also see that the wait block is active. Next, we can investigate a
few wait-related fields in the thread structure:
Click here to view code image
lkd> dt nt!_KTHREAD 0xffff898f2b345080 WaitRegister.State 
WaitIrql WaitMode WaitBlockCount
     WaitReason WaitTime
   +0x070 WaitRegister       :
   +0x000 State              : 0y001
   +0x186 WaitIrql           : 0 ''
   +0x187 WaitMode           : 1 ''
   +0x1b4 WaitTime           : 0x39b38f8
   +0x24b WaitBlockCount     : 0x1 ''
   +0x283 WaitReason         : 0x6 ''
The data shows that this is a committed wait that was performed
at IRQL 0 (Passive Level) with a wait mode of UserMode, at the
time shown in 15 ms clock ticks since boot, with the reason
indicating a user-mode application request. We can also see that
this is the only wait block this thread has, meaning that it is not
waiting for any other object.
If the wait list head had more than one entry, you could’ve
executed the same commands on the second pointer value in the
WaitListEntry field of the wait block (and eventually executing
!thread on the thread pointer in the wait block) to traverse the list
and see what other threads are waiting for the object. If those
threads were waiting for more than one object, you’d have to look
at their WaitBlockCount to see how many other wait blocks were
present, and simply keep incrementing the pointer by
sizeof(KWAIT_BLOCK).
Another possibility is that the wait type would have been
WaitNotification, at which point you’d have used the notification
queue pointer instead to dump the Queue (KQUEUE) structure,
which is itself a dispatcher object. Potentially, it would also have
had its own nonempty wait block list, which would have revealed
the wait block associated with the worker thread that will be
asynchronously receiving the notification that the object has been
signaled. To determine which callback would eventually execute,
you would have to dump user-mode thread pool data structures.
Keyed events
A synchronization object called a keyed event bears special mention because
of the role it played in user-mode-exclusive synchronization primitives and
the development of the alert-by-ID primitive, which you’ll shortly realize is
Windows’ equivalent of the futex in the Linux operating system (a well-
studied computer science concept). Keyed events were originally
implemented to help processes deal with low-memory situations when using
critical sections, which are user-mode synchronization objects that we’ll see
more about shortly. A keyed event, which is not documented, allows a thread
to specify a “key” for which it waits, where the thread wakes when another
thread of the same process signals the event with the same key. As we
pointed out, if this sounds familiar to the alerting mechanism, it is because
keyed events were its precursor.
If there was contention, EnterCriticalSection would dynamically allocate
an event object, and the thread wanting to acquire the critical section would
wait for the thread that owns the critical section to signal it in
LeaveCriticalSection. Clearly, this introduces a problem during low-memory
conditions: critical section acquisition could fail because the system was
unable to allocate the event object required. In a pathological case, the low-
memory condition itself might have been caused by the application trying to
acquire the critical section, so the system would deadlock in this situation.
Low memory wasn’t the only scenario that could cause this to fail—a less
likely scenario was handle exhaustion. If the process reached its handle limit,
the new handle for the event object could fail.
It might seem that preallocating a global standard event object, similar to
the reserve objects we talked about previously, would fix the issue. However,
because a process can have multiple critical sections, each of which can have
its own locking state, this would require an unknown number of preallocated
event objects, and the solution doesn’t work. The main feature of keyed
events, however, was that a single event could be reused among different
threads, as long as each one provided a different key to distinguish itself. By
providing the virtual address of the critical section itself as the key, this
effectively allows multiple critical sections (and thus, waiters) to use the
same keyed event handle, which can be preallocated at process startup time.
When a thread signals a keyed event or performs a wait on it, it uses a
unique identifier called a key, which identifies the instance of the keyed event
(an association of the keyed event to a single critical section). When the
owner thread releases the keyed event by signaling it, only a single thread
waiting on the key is woken up (the same behavior as synchronization events,
in contrast to notification events). Going back to our use case of critical
sections using their address as a key, this would imply that each process still
needs its own keyed event because virtual addresses are obviously unique to
a single process address space. However, it turns out that the kernel can wake
only the waiters in the current process so that the key is even isolated across
processes, meaning that there can be only a single keyed event object for the
entire system.
As such, when EnterCriticalSection called NtWaitForKeyedEvent to
perform a wait on the keyed event, it gave a NULL handle as parameter for
the keyed event, telling the kernel that it was unable to create a keyed event.
The kernel recognizes this behavior and uses a global keyed event named
ExpCritSecOutOfMemoryEvent. The primary benefit is that processes don’t
need to waste a handle for a named keyed event anymore because the kernel
keeps track of the object and its references.
However, keyed events were more than just a fallback object for low-
memory conditions. When multiple waiters are waiting on the same key and
need to be woken up, the key is signaled multiple times, which requires the
object to keep a list of all the waiters so that it can perform a “wake”
operation on each of them. (Recall that the result of signaling a keyed event
is the same as that of signaling a synchronization event.) However, a thread
can signal a keyed event without any threads on the waiter list. In this
scenario, the signaling thread instead waits on the event itself.
Without this fallback, a signaling thread could signal the keyed event
during the time that the user-mode code saw the keyed event as unsignaled
and attempt a wait. The wait might have come after the signaling thread
signaled the keyed event, resulting in a missed pulse, so the waiting thread
would deadlock. By forcing the signaling thread to wait in this scenario, it
actually signals the keyed event only when someone is looking (waiting).
This behavior made them similar, but not identical, to the Linux futex, and
enabled their usage across a number of user-mode primitives, which we’ll see
shortly, such as Slim Read Writer (SRW) Locks.
 Note
When the keyed-event wait code needs to perform a wait, it uses a built-in
semaphore located in the kernel-mode thread object (ETHREAD) called
KeyedWaitSemaphore. (This semaphore shares its location with the ALPC
wait semaphore.) See Chapter 4 of Part 1 for more information on thread
objects.
Keyed events, however, did not replace standard event objects in the
critical section implementation. The initial reason, during the Windows XP
timeframe, was that keyed events did not offer scalable performance in
heavy-usage scenarios. Recall that all the algorithms described were meant to
be used only in critical, low-memory scenarios, when performance and
scalability aren’t all that important. To replace the standard event object
would’ve placed strain on keyed events that they weren’t implemented to
handle. The primary performance bottleneck was that keyed events
maintained the list of waiters described in a doubly linked list. This kind of
list has poor traversal speed, meaning the time required to loop through the
list. In this case, this time depended on the number of waiter threads. Because
the object is global, dozens of threads could be on the list, requiring long
traversal times every single time a key was set or waited on.
 Note
The head of the list is kept in the keyed event object, whereas the threads
are linked through the KeyedWaitChain field (which is shared with the
thread’s exit time, stored as a LARGE_INTEGER, the same size as a
doubly linked list) in the kernel-mode thread object (ETHREAD). See
Chapter 4 of Part 1 for more information on this object.
Windows Vista improved keyed-event performance by using a hash table
instead of a linked list to hold the waiter threads. This optimization is what
ultimately allowed Windows to include the three new lightweight user-mode
synchronization primitives (to be discussed shortly) that all depended on the
keyed event. Critical sections, however, continued to use event objects,
primarily for application compatibility and debugging, because the event
object and internals are well known and documented, whereas keyed events
are opaque and not exposed to the Win32 API.
With the introduction of the new alerting by Thread ID capabilities in
Windows 8, however, this all changed again, removing the usage of keyed
events across the system (save for one situation in init once synchronization,
which we’ll describe shortly). And, as more time had passed, the critical
section structure eventually dropped its usage of a regular event object and
moved toward using this new capability as well (with an application
compatibility shim that can revert to using the original event object if
needed).
Fast mutexes and guarded mutexes
Fast mutexes, which are also known as executive mutexes, usually offer
better performance than mutex objects because, although they are still built
on a dispatcher object—an event—they perform a wait only if the fast mutex
is contended. Unlike a standard mutex, which always attempts the acquisition
through the dispatcher, this gives the fast mutex especially good performance
in contended environments. Fast mutexes are used widely in device drivers.
This efficiency comes with costs, however, as fast mutexes are only
suitable when all kernel-mode APC (described earlier in this chapter)
delivery can be disabled, unlike regular mutex objects that block only normal
APC delivery. Reflecting this, the executive defines two functions for
acquiring them: ExAcquireFastMutex and ExAcquireFastMutexUnsafe. The
former function blocks all APC delivery by raising the IRQL of the processor
to APC level. The latter, “unsafe” function, expects to be called with all
kernel-mode APC delivery already disabled, which can be done by raising
the IRQL to APC level. ExTryToAcquireFastMutex performs similarly to the
first, but it does not actually wait if the fast mutex is already held, returning
FALSE instead. Another limitation of fast mutexes is that they can’t be
acquired recursively, unlike mutex objects.
In Windows 8 and later, guarded mutexes are identical to fast mutexes but
are acquired with KeAcquireGuardedMutex and
KeAcquireGuardedMutexUnsafe. Like fast mutexes, a
KeTryToAcquireGuardedMutex method also exists.
Prior to Windows 8, these functions did not disable APCs by raising the
IRQL to APC level, but by entering a guarded region instead, which set
special counters in the thread’s object structure to disable APC delivery until
the region was exited, as we saw earlier. On older systems with a PIC (which
we also talked about earlier in this chapter), this was faster than touching the
IRQL. Additionally, guarded mutexes used a gate dispatcher object, which
was slightly faster than an event—another difference that is no longer true.
Another problem related to the guarded mutex was the kernel function
KeAreApcsDisabled. Prior to Windows Server 2003, this function indicated
whether normal APCs were disabled by checking whether the code was
running inside a critical section. In Windows Server 2003, this function was
changed to indicate whether the code was in a critical or guarded region,
changing the functionality to also return TRUE if special kernel APCs are
also disabled.
Because there are certain operations that drivers should not perform when
special kernel APCs are disabled, it made sense to call KeGetCurrentIrql to
check whether the IRQL is APC level or not, which was the only way special
kernel APCs could have been disabled. However, with the introduction of
guarded regions and guarded mutexes, which were heavily used even by the
memory manager, this check failed because guarded mutexes did not raise
IRQL. Drivers then had to call KeAreAllApcsDisabled for this purpose,
which also checked whether special kernel APCs were disabled through a
guarded region. These idiosyncrasies, combined with fragile checks in Driver
Verifier causing false positives, ultimately all led to the decision to simply
make guarded mutexes revert to just being fast mutexes.
Executive resources
Executive resources are a synchronization mechanism that supports shared
and exclusive access; like fast mutexes, they require that all kernel-mode
APC delivery be disabled before they are acquired. They are also built on
dispatcher objects that are used only when there is contention. Executive
resources are used throughout the system, especially in file-system drivers,
because such drivers tend to have long-lasting wait periods in which I/O
should still be allowed to some extent (such as reads).
Threads waiting to acquire an executive resource for shared access wait for
a semaphore associated with the resource, and threads waiting to acquire an
executive resource for exclusive access wait for an event. A semaphore with
unlimited count is used for shared waiters because they can all be woken and
granted access to the resource when an exclusive holder releases the resource
simply by signaling the semaphore. When a thread waits for exclusive access
of a resource that is currently owned, it waits on a synchronization event
object because only one of the waiters will wake when the event is signaled.
In the earlier section on synchronization events, it was mentioned that some
event unwait operations can actually cause a priority boost. This scenario
occurs when executive resources are used, which is one reason why they also
track ownership like mutexes do. (See Chapter 4 of Part 1 for more
information on the executive resource priority boost.)
Because of the flexibility that shared and exclusive access offer, there are
several functions for acquiring resources: ExAcquireResourceSharedLite,
ExAcquireResourceExclusiveLite, ExAcquireSharedStarveExclusive, and
ExAcquireShareWaitForExclusive. These functions are documented in the
WDK.
Recent versions of Windows also added fast executive resources that use
identical API names but add the word “fast,” such as
ExAcquireFastResourceExclusive, ExReleaseFastResource, and so on. These
are meant to be faster replacements due to different handling of lock
ownership, but no component uses them other than the Resilient File System
(ReFS). During highly contended file system access, ReFS has slightly better
performance than NTFS, in part due to the faster locking.
EXPERIMENT: Listing acquired executive resources
The kernel debugger !locks command uses the kernel’s linked list
of executive resources and dumps their state. By default, the
command lists only executive resources that are currently owned,
but the –d option is documented as listing all executive resources—
unfortunately, this is no longer the case. However, you can still use
the -v flag to dump verbose information on all resources instead.
Here is partial output of the command:
Click here to view code image
lkd> !locks -v
**** DUMP OF ALL RESOURCE OBJECTS ****
Resource @ nt!ExpFirmwareTableResource (0xfffff8047ee34440)    
Available
Resource @ nt!PsLoadedModuleResource (0xfffff8047ee48120)    
Available
    Contention Count = 2
Resource @ nt!SepRmDbLock (0xfffff8047ef06350)    Available
    Contention Count = 93
Resource @ nt!SepRmDbLock (0xfffff8047ef063b8)    Available
Resource @ nt!SepRmDbLock (0xfffff8047ef06420)    Available
Resource @ nt!SepRmDbLock (0xfffff8047ef06488)    Available
Resource @ nt!SepRmGlobalSaclLock (0xfffff8047ef062b0)    
Available
Resource @ nt!SepLsaAuditQueueInfo (0xfffff8047ee6e010)    
Available
Resource @ nt!SepLsaDeletedLogonQueueInfo 
(0xfffff8047ee6ded0)    Available
Resource @ 0xffff898f032a8550    Available
Resource @ nt!PnpRegistryDeviceResource (0xfffff8047ee62b00)    
Available
    Contention Count = 27385
Resource @ nt!PopPolicyLock (0xfffff8047ee458c0)    
Available
    Contention Count = 14
Resource @ 0xffff898f032a8950    Available
Resource @ 0xffff898f032a82d0    Available
Note that the contention count, which is extracted from the
resource structure, records the number of times threads have tried
to acquire the resource and had to wait because it was already
owned. On a live system where you break in with the debugger,
you might be lucky enough to catch a few held resources, as shown
in the following output:
Click here to view code image
2: kd> !locks
**** DUMP OF ALL RESOURCE OBJECTS ****
KD: Scanning for held locks.....
Resource @ 0xffffde07a33d6a28    Shared 1 owning threads
    Contention Count = 28
     Threads: ffffde07a9374080-01
KD: Scanning for held locks....
Resource @ 0xffffde07a2bfb350    Shared 1 owning threads
    Contention Count = 2
     Threads: ffffde07a9374080-01
KD: Scanning for held 
locks.......................................................
....
Resource @ 0xffffde07a8070c00    Shared 1 owning threads
     Threads: ffffde07aa3f1083-01 *** Actual Thread 
ffffde07aa3f1080
KD: Scanning for held 
locks.......................................................
....
Resource @ 0xffffde07a8995900    Exclusively owned
     Threads: ffffde07a9374080-01
KD: Scanning for held 
locks.......................................................
....
    9706 total locks, 4 locks currently held
You can examine the details of a specific resource object,
including the thread that owns the resource and any threads that are
waiting for the resource, by specifying the –v switch and the
address of the resource, if you find one that’s currently acquired
(owned). For example, here’s a held shared resource that seems to
be associated with NTFS, while a thread is attempting to read from