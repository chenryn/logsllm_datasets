In a voting protocol, with a vot-
ing machine M and auditors A1, . . . ,Ar, if the judge states, say,
dis(M)∧ dis(A1)∧ . . .∧ dis(Ar), then this expresses the judge’s be-
lief that the voting machine and all auditors misbehaved; the judge
would state dis(M) ∨ (dis(A1) ∧ . . . ∧ dis(Ar)) if she is not sure
whether the voting machine or all auditors misbehaved. Our case
studies demonstrate the usefulness of such expressive forms of ver-
dicts. We will denote by Fdis the set of all verdicts. A party J
can state a verdict ψ, by sending ψ on its dedicated output channel
decisionJ. Note that, in one run, J may state many different verdicts
ψ1, . . . , ψk, which is equivalent to stating the verdict ψ1 ∧···∧ ψk.
Formally, for a protocol P and an instance π of P, a verdict ψ is
true in π, written π |= ψ, iff the formula ψ evaluates to true with
the proposition dis(a) set to false, if a is honest in π, and set to true
otherwise.
We now introduce accountability constraints and accountability
properties which allow to precisely describe the level of account-
ability a protocol provides.
An accountability constraint of a protocol P is a tuple (α, ψ1, . . . ,
ψk), written (α ⇒ ψ1 | ··· | ψk), where α is a property of P and
ψ1, . . . , ψk ∈ Fdis. We say that a constraint (α ⇒ ϕ1 | ··· | ϕk)
covers a run r, if r ∈ α.
Intuitively, in a constraint C = (α ⇒ ψ1 | ··· | ψk), the set α con-
tains runs in which some desired goal of the protocol is not met
(due to the misbehavior of some protocol participant). The formu-
las ψ1, . . . , ψk are the possible (minimal) verdicts that are supposed
to be stated by J in such a case; J is free to state stronger verdicts
(by the fairness condition these verdicts will be true). Formally, for
a run r, we say that J ensures C in r, if either r /∈ α or J states in r
a verdict ψ that implies one of ψ1, . . . , ψk (in the sense of proposi-
tional logic).
EXAMPLE 1. To illustrate the notion of accountability con-
straints, let us consider the following examples, where, say, J is
supposed to blame misbehaving parties, M is a voting machine,
A1, . . . ,Ar are auditors, and α contains all runs in which the pub-
lished result of the election is incorrect:
1 = α ⇒ dis(M) | dis(A1) | ··· | dis(Ar)
Cex
(1)
2 = α ⇒ dis(M)∨ (dis(A1)∧···∧ dis(Ar))
Cex
(2)
3 = α ⇒ dis(M) | dis(A1)∧···∧ dis(Ar).
Cex
(3)
Constraint Cex
1 requires that if in a run the published result of the
election is incorrect, then at least one (individual) party among M,
A1, . . . ,Ar can be held accountable by J; note that different parties
1 in a run r ∈ α,
can be blamed in different runs. Party J ensures Cex
if, for example, J states dis(A1) or J states dis(M)∧ dis(Ar), but not
if J only states dis(M)∨ dis(A1). Constraint Cex
3 is stronger than
Cex
1 as it requires that it is possible to hold dis(M) or all auditors
accountable. In this case, for J it does not sufﬁce to state dis(A1),
but stating dis(M)∧ dis(Ar) or dis(A1)∧···∧ dis(Ar) does. Con-
straint Cex
1 . It states
that if the published result of the election is incorrect, then J can
leave it open whether dis(M) or all auditors misbehaved.
3 , and incomparable to Cex
2 is weaker than Cex
As mentioned before, we think that in practice, individual account-
ability is highly desirable to deter parties from misbehaving. So
ideally, protocols should satisfy accountability constraints where
in case a desired goal is not met, at least one misbehaving party
is blamed individually. Formally, we say that (α ⇒ ψ1 | ··· | ψk)
provides individual accountability, if for every i ∈ {1, . . . ,k}, there
exists a party a such that ψk implies dis(a). In other words, each
ψ1, . . . , ψk determines at least one misbehaving party.
In Exam-
ple 1, Cex
2 does
not.
3 provide individual accountability, but Cex
1 and Cex
A set Φ of constraints for protocol P is called an accountabil-
ity property of P. Typically, an accountability property Φ covers
all relevant cases in which desired goals for P are not met, i.e.,
whenever some desired goal of P is not satisﬁed in a given run r
due to some misbehavior of some protocol participant, then there
exists a constraint in Φ which covers r. We note that considering
sets of accountability constraints rather than just a single constraint
provides more expressiveness: A set of constraints allows to more
precisely link the participants to be blamed with speciﬁc violations,
and hence, captures more precisely the kind of accountability pro-
vided by a protocol (see our case studies for examples.
We are now ready to provide precise symbolic and computational
deﬁnitions of accountability. As already mentioned, conceptually
these two deﬁnitions share the same basic idea outlined above.
Symbolic Accountability. Let P be a protocol and J be an agent of
P. We say that J is fair, if his/her verdicts are never false. Formally,
J is fair in P, if, for every instance π of P and every run r of π,
whenever J states a verdict ψ in r, then π |= ψ. For instance, if in
some run with honest M and A1, an agent J states dis(M)∨ dis(A1),
then J is not fair.
3
DEFINITION 2
(Symbolic accountability). Let P be a proto-
col with the set of agents Σ, let J ∈ Σ, and Φ be an accountability
property of P. We say that J ensures Φ-accountability for protocol
P (or P is Φ-accountable w.r.t. J) if
(i) (fairness) J is fair in P and
(ii) (completeness) for every constraint C in Φ and every run r of
P, J ensures C in r.
While the completeness condition requires J’s verdicts to be sufﬁ-
ciently strict, i.e., at least as strict as the constraints require, fairness
guarantees that J’s verdicts are correct. Note that the fairness con-
dition does not depend on the accountability property under con-
sideration.
In the completeness condition, it is of course desirable that δ = 0,
i.e., the probably that J fails to ensure a constraint is negligible.
However, as we will illustrate in Section 5, this is often too de-
manding. Instead of giving up in such cases, by introducing the
parameter δ, we can measure the level of completeness a protocol
provides.
3. VERIFIABILITY
In this section, we provide a symbolic and a computational deﬁ-
nition of veriﬁability and show that veriﬁability is a restricted form
of accountability. We use the terminology and notation introduced
in Section 2.
REMARK 1
(AUTOMATIC ANALYSIS). The fairness condi-
tion can often be checked automatically by tools for cryptographic
protocol analysis since it is a reachability property: For all B ⊆ Σ,
one considers systems in which the agents in B run their honest pro-
grams. Then, one checks whether a state can be reached, where J
states ψ such that ψ does not evaluate to true if dis(b) is set to false
iff b ∈ B. This can often be done automatically, provided that the
cryptographic primitives used and the communication model the
protocol builds on can be handled by the analysis tool and provided
that the sets ˆΠc and Πc of programs of agents c, as speciﬁed in the
protocol P, are either ﬁnite or as powerful as a Dolev-Yao intruder.
Whether or not the completeness condition can be checked au-
tomatically heavily depends on the accountability property under
consideration.
Our analysis of the contract-signing protocol considered in Sec-
tion 7 illustrates how the fairness condition can be checked au-
tomatically; in this case, the completeness condition can also be
checked automatically, but it is quite trivial.
Computational Accountability As usual, a function f from the
natural numbers to the interval [0,1] is negligible if, for every c > 0,
there exists (cid:96)0 such that f ((cid:96)) ≤ 1
(cid:96)c , for all (cid:96) > (cid:96)0. The function f
is overwhelming if the function 1− f is negligible. A function f is
δ-bounded if, for every c > 0 there exists (cid:96)0 such that f ((cid:96)) ≤ δ + 1
(cid:96)c ,
for all (cid:96) > (cid:96)0.
Let P be a protocol with the set Σ of agents. Since we now
consider the computational setting, we assume that the programs
agents run are ppt ITMs. Let Φ be an accountability property of P.
Let π be an instance of P and J ∈ Σ be an agent of P. For a set V of
verdicts, we write Pr[π(1(cid:96)) (cid:55)→ {(J : ψ) | ψ ∈ V}] for the probability
that π produces a run in which J states ψ for some ψ ∈ V , where
the probability is taken over the random coins of the ITMs in π and
1(cid:96) is the security parameter given to the ITMs. Similarly, we write
Pr[π(1(cid:96)) (cid:55)→ ¬(J : Φ)] to denote the probability that π, with security
parameter 1(cid:96), produces a run such that J does not ensure C in this
run, for some C ∈ Φ.
An agent J is computationally fair, if he states false verdicts only
with negligible probability. Formally, J is computationally fair in
a protocol P, if Pr[π(1(cid:96)) (cid:55)→ {(J : ψ) | π (cid:54)|= ψ}] is negligible as a
function of (cid:96), for all instances π of P.
DEFINITION 3
(Computational accountability). Let P be a
protocol with the set of agents Σ, J ∈ Σ, Φ be an accountabil-
ity property of P, and δ ∈ [0,1]. We say that J ensures (Φ, δ)-
accountability for protocol P (or P is (Φ, δ)-accountable w.r.t. J)
if
(i) (fairness) J is computationally fair in P and
(ii) (completeness) for every instance π of P, the probability
Pr(cid:2)π(1(cid:96)) (cid:55)→ ¬(J : Φ)(cid:3) is δ-bounded as a function of (cid:96).
4
Symbolic and Computational Veriﬁability. Let P be a protocol
and γ be a property of P, called the goal of P. We say that an agent
J accepts a run r, if in this run J sends the message accept on
channel decisionJ. Intuitively, J accepts a run if she believes that
the goal has been achieved in this run.
The agent J may be a regular protocol participant (voter, bidder,
authority, etc.) or an external judge, who is provided with informa-
tion by (possibly untrusted) protocol participants.
Expressing goals as properties of a protocol is, as in case of ac-
countability, a powerful and ﬂexible tool, which for voting proto-
cols, for example, allows to capture several forms of veriﬁability
considered in the literature: The goal of an agent (a voter, in this
case) J could, for example, include all runs in which her vote is
counted as cast; this goal aims at what is called individual veriﬁa-
bility [42]. Another goal could include all runs in which the ballots
shown on a bulletin board are counted correctly; this goal aims at
what is called universal veriﬁability [42]. In [44], another type of
veriﬁability is considered, namely eligibility veriﬁability. This is
captured by the goal γ that includes those runs where only eligi-
ble voters vote at most once. However, the bottom line should be
a goal, which we call global veriﬁability, that contains all runs in
which the published result exactly corresponds to the votes cast by
eligible voters (see Section 5 for a more precise formulation and a
more in depth discussion). This goal has not formally been consid-
ered in the literature so far, at most implicitly as a conjunction of
all the above mentioned goals. Analogously, goals for other kinds
of protocols, such as auction protocols, can be formulated (see Sec-
tion 6).
In our deﬁnition of veriﬁability, we require that an agent J ac-
cepts a run, only if the goal of the protocol is satisﬁed. This re-
quirement, however, would be easily satisﬁed in every protocol by
an agent who never accepts a run. Therefore, the deﬁnition of veri-
ﬁability should also contain conditions under which the goal should
be achieved and runs should be accepted. Clearly, one may ex-
pect that a protocol run should be accepted (and the goal should
be achieved), at least when all the protocol participants are hon-
est. Furthermore, in some protocols, such as those for e-voting,
one may expect that to achieve the goal it is sufﬁcient that vot-
ing authorities follow the protocol, regardless of whether or not
the voters behave honestly. Therefore, our deﬁnition, besides the
goal, has an additional parameter: a positive boolean formula over
propositions of the form hon(a), for an agent a, which describes
a group or groups of participants that can guarantee, when run-
ning their honest programs, that a goal of a protocol is achieved.
We will denote the set of such formulas by Fhon. For example,
for an e-voting protocol with a voting machine M and auditors
A1, . . . ,Ar, one might expect that to achieve the goal of the pro-
tocol it is sufﬁcient that M is honest and at least one of the au-
ditors A1, . . . ,Ar is honest. This can be expressed by the formula
ϕex = hon(M)∧ (hon(A1)∨···∨ hon(Ar)).
For an instance π of P and ψ ∈ Fhon, we write π |= ψ if ψ
evaluates to true with the proposition hon(a) set to true, if a is
honest in π, and set to false otherwise.
We can now provide symbolic and computational deﬁnitions of
veriﬁability.
DEFINITION 4
(Symbolic veriﬁability). Let P be a protocol
with the set of agents Σ. Let J ∈ Σ, ψ ∈ Fhon, and γ be a property
of P. Then, we say that the goal γ is guaranteed in P by ψ and
veriﬁable by J if the following conditions are satisﬁed:
(i) For every run r of an instance π of P such that π |= ψ, the
(ii) For every run r of an instance of P in which J accepts r, it
agent J accepts r.
holds that r ∈ γ.
Condition (ii) guarantees that J only accepts a run if the goal is
in fact achieved. Condition (i) says that the protocol is sound in
the sense that if ψ holds, i.e. certain participants are honest, as de-
scribed by ψ, then indeed J accepts, which by Condition (ii) implies
that the goal is achieved.
This deﬁnition can easily be turned into a computational deﬁ-
nition of veriﬁability. For this, by Pr[π(1(cid:96)) (cid:55)→ (J : accept)] we
denote the probability that π, with security parameter 1(cid:96), produces
a run which is accepted by J. Analogously, by Pr[π(1(cid:96)) (cid:55)→ ¬γ, (J :
accept)] we denote the probability that π, with security parameter
1(cid:96), produces a run which is not in γ but nevertheless accepted by J.
DEFINITION 5
(Computational veriﬁability). Let P be a
protocol with the set of agents Σ. Let δ ∈ [0,1], J ∈ Σ, ψ ∈ Fhon,
and γ be a property of P. Then, we say that the goal γ is guaran-
teed in P by ψ and δ-veriﬁable by J if for every instance π of P the
following conditions are satisﬁed:
(i) If π |= ψ, then Pr[π(1(cid:96)) (cid:55)→ (J : accept)] is overwhelming as
(ii) Pr[π(1(cid:96)) (cid:55)→ ¬γ, (J : accept)] is δ-bounded as a function of (cid:96).
a function of (cid:96).
Just as in case of accountability, assuming negligibility in Condi-
tion (ii), i.e., δ = 0, is too strong for many reasonable protocols.
Relationship to Accountability. The following proposition shows
that veriﬁability can be considered to be a special case of account-
ability. While, given our deﬁnitions, this relationship is easy to
prove, in the literature, accountability and veriﬁability have not
been formally connected before.
Let ϕ ∈ Fhon. We denote by ϕ ∈ Fdis the negation normal form
of ϕ, where ¬hon(b) is replaced by dis(b). For example, for ϕex as
above, we have ϕex = dis(M)∨ (dis(A1)∧···∧ dis(Ar)).
Let P be a protocol and J be an agent such that J states only
formulas ψ that imply ϕ. Furthermore, assume that J accepts a
run iff it does not output a formula ψ. Now, the proposition is as
follows (see Appendix C for the proof):
PROPOSITION 1. Let ϕ, P and J be deﬁned as above. Let γ be
a property of P. Then the statement
J ensures {¬γ ⇒ ϕ}-accountability for P
(4)
implies the statement
So, veriﬁability is implied by a restricted form of accountabil-