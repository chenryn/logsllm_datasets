达到上限，摄取将会暂停
Druid实时大数据分析原理与实践
---
## Page 111
据通过实时节点导人到Druid系统，该Spec配置文件的内容如下：
例来进行说明。
第5章数据摄入
一个 Spec 配置文件。Spec 配置文件是一个 JSON文件。我们要把上述的JSON 格式的行为数
首先，若以Pull的方式摄取数据，则需要启动一个实时节点。而启动实时节点，则需要
接下来，对于用户行为数据的摄人部分，我们以 Druid实时节点从Kafka中Pull数据为
"dataSchema":
'parser":
"metricsSpec":[
"granularitySpec":{
"queryGranularity":“MINUTE"
"parseSpec":
"type":"uniform"
"segmentGranularity":"HouR"
"dimensionsSpec":[
"name": "count",
"fieldName":"count"
"timestampSpec"：
'format":"json"
"dimensionExclusions": [],
"spatialDimensions":[]
"dimensions":[
"category"
"commodity"
"city",
"age"
"user_id",
"event_name",
8
---
## Page 112
88
tuningConfig":{
ioConfig":{
"windowPeriod":"PT10m"
"firehose":
"type":"realtime",
"rejectionPolicy":{
"maxRowsInMemory":75000,
"intermediatePersistPeriod":"PT10m",
"basePersistDirectory":"/tmp/realtime/basePersist"
"type":"realtime"
"plumber":
"type":"serverTime”
"type":"realtime"
"type":
"feed":
"consumerProps":{
"type":"string”
"zookeeper.sync.time.ms":"5000"
"zookeeper.session.timeout.ms":"15000",
'zookeeper.connection.timeout.ms":"15000"
"group.id":"druid-example",
fetch.message.max.bytes":"1048586"
"auto.offset.reset":"largest",
"auto.commit,enable":
zookeeper.connect":"localhost:2181",
"kafka-0.8"
:"dianshang_order"
"format":“auto"
"column":"timestamp"，
"false"
Druid实时大数据分析原理与实践
---
## Page 113
份Ingestion Spec。我们接着用用户行为数据摄取的例子，Ingestion Spec如下：
节点（OverlordNode）（具体请参见第4章）。
5.2.3
式摄取数据。
第5章数据摄入
1.
启动索引任务
以Push方式摄取，需要索引服务，所以要先启动中间管理者（Midle Manager）和统治
"spec":{
启动索引任务需要向索引服务中的统治节点发送一个HTTP请求，并向该请求POST一
我们会在后面的章节中介绍如何查询摄人到Druid系统的数据。
在使用上述 Spec配置文件启动实时节点后，实时节点就会自动地从Kafka通过Pull的方
"dataSchema":{
以Push方式摄取
"dataSource":"dianshang_order"
parser":
"metricsSpec":[
'granularitySpec":{
"parseSpec":{
"type":"uniform"
"segmentGranularity":"HOUR""
"queryGranularity":"MINUTE"
"dimensionsSpec":{
"type"：
"name": “count",
"fieldName":"count",
"dimensionExclusions":[],
"dimensions":[
"longSum"
"user_id",
"event_name"
"commodity",
"city",
"age"",
89
---
## Page 114
这样会把任务分配给中间管理者，用于接收数据。
PORT/druid/indexer/v1/task
"type":"index_realtime”
tuningConfig":{
"windowPeriod":"PT10m”
'type":"realtime"，
"rejectionPolicy":{
"maxRowsInMemory":500000,
"intermediatePersistPeriod":"PT10m",
"basePersistDirectory":"/rc/data/druid/realtime/basePersist"
ioConfig":
"type":"serverTime”
"type":"realtime"
"firehose":{
"type":"receiver"
"serviceName":"dianshang_order"
"type": “map"
"timestampSpec":{
"format":"json",
"format": "auto"
"column":"timestamp"
"spatialDimensions":[]
"category"
Druid实时大数据分析原理与实践
---
## Page 115
curlhttp://:/druid/indexer/v1/task/{taskId}/status
curl -X"‘POST′ -H‘Content-Type:application/json’-d @my-index-task.json OVERLORD_IP:
1.提交任务
5.2.4
2.发送数据
第5章数据摄入
2.
"status":{
"task":"index_realtime_dianshang_order_0_2016-07-13T05:44:17.189Z_ndnklphj"
"task":"index_realtime_dianshang_order_0_2016-07-13T05:44:17.189Z_ndnklphj"
得到查询任务的状态如下：
查看任务状态
PORT/druid/indexer/v1/task
索引服务启动的相关任务可以通过相应的接口进行管理。
"status":
dianshang_order/push-events
":"xxxxx","category":"3c","count":1}]′peonhost:port/druid/worker/v1/chat/
索引服务任务相关管理接口
"duration":-1
"status":“RUNNING"
"duration":-1
"status":"RUNNING"
1
---
## Page 116
curl http://10.24.199.8:8090/druid/indexer/v1/task/index_realtime_dianshang_order_0_2016
3.查看Segment信息
2
返回的信息如下：
-07-13T05:44:17.189Z_ndnklphj/segments
"loadSpec":
"interval":"2016-07-13T05:00:00.000Z/2016-07-13T06:00:00.000Z"
"identifier":"dianshang_order_2016-07-13T05:00:00.000Z_2016-07-13T06:00:00.000
"dataSource":"dianshang_order"
"binaryVersion":9,
"version":"2016-07-13T06:20:41.042Z"
"size":4382,
"shardSpec":{
"metrics":"count"
"loadSpec"：
"interval":"2016-07-13T06:00:00.000Z/2016-07-13T07:00:00.000Z"
"identifier":"dianshang_order_2016-07-13T06:00:00.000Z_2016-07-13T07:00:00.000
"dimensions":
"dataSource":"dianshang_order"
"binaryVersion": 9, 
Z_2016-07-13T05:45:02.324Z",
Z_2016-07-13T06:20:41.042Z",
"type": "hdfs"
"path";
"type":“none'
"type": "hdfs"
Z_20160713T060000.000Z/2016-07-13T05_45_02.324Z/0/index.Zip",
Z_20160713T070000.000Z/2016-07-13T06_20_41.042Z/0/index.zip",
:/druid/segments/dianshang_order/20160713T05000.000
Druid实时大数据分析原理与实践
---
## Page 117
5.Overlord控制台
curl-X'POST'
4.关闭任务
第5章数据摄入
Druid提供了一个控制台，可以查看任务的状态、日志、Worker等，如图5-2所示。
"version":"2016-07-13T05:45:02.324Z
"size":4394,
'shardSpec":
"metrics":"count",
RemoteWorkers
CompleteTasks-Tasksrecently completed
Waiting Tasks-Taskswaiting onlocks
how10entries
Pending Tasks-Tasks waiting tobeassigned to a worker
Show10entries
Running Tasks
howing1to1of1entries
016-07
astCon
"type": "none"
3T05:43:39.139Z
http://:/druid/indexer/v1/task/{taskId}/shutdown
worker host
图5-2控制台
curCapacityUsed
Time
(index_realtime_dianshang_orde
Search all columns:
Searchallcolumns
FirstPrevious
FirstPrevious
---
## Page 118
{"timestamp":"2016-07-17T02:59:00.563Z","event_name":"browse_commodity","user_id":1,"age
["timestamp":"2016-07-17T02:59:00.563Z","event_name":"browse_commodity","user_id":1,"age
"timestamp":"2016-07-17T02:58:00.563Z","event_name":"browse_commodity","user_id":1,"age
{"timestamp":"2016-07-17T02:56:00.563Z","event_name":"browse_commodity","user_id":1,"age
{"timestamp":"2016-07-17T02:54:00.563Z","event_name":"browse_commodity","user_id":1,"age
{"timestamp":"2016-07-17T02:53:00.563Z","event_name”:"browse_commodity","user_id":1,"age
{"timestamp":"2016-07-17T02:52:00.563Z","event_name":"browse_commodity","user_id":1,"age
{"timestamp":"2016-07-17T02:51:00.563Z","event_name":*browse_commodity","user_id":1,"age
{"timestamp":"2016-07-17T02:50:00.5632","event_name":"browse_commodity","user_id":1,"age
引任务。例如，把用户行为数据批量导人到系统中。用户行为数据示例如下：
["timestamp":"2016-07-17T02:55:00.563Z","event_name":"browse_commodity","user_id":1,"age
5.3.1
5.3
6
":"90+","city":"Beijing","commodity":"xxxxx","category";"3c","count":1}
":"90+","city":"Beijing","commodity":"xxxx","category":"3c","count":1}
":"90+","city":"Beijing","commodity":"xxxxx","category":"3c","count":1}
uno2"*Auobae.xxxx.:Arpowobuta..+06m.
uo6axxxxxo6ug+06m
";"90+","city”:"Beijing","commodity”":"xxxxx","category":"3c","count":1}
.uno.2.*.Auobae.xxxx.Arpouuobutrag..+06..
4uno.*.Ao6e.xxxx.poo.butag.+06m
":"90+","city":"Beijing","commodity":"xxxxx","category":"3c","count":1}
同样，也可以通过索引服务方式批量摄取数据。我们仍然需要通过统治节点提交一个索
"spec":[
启动索引任务，所需的Ingestion Spec文件如下：
以索引服务方式摄取
静态数据批量摄取
"dataSchema":{
"dataSource":"dianshang_order",
"granularitySpec":{
"intervals":[
"2016-07-17/2016-07-18"
Druid实时大数据分析原理与实践
---
## Page 119
第5章数据摄入
"ioConfig":{
"firehose":{
"parser":
"metricsSpec":
"baseDir":"/rc/data/druid/tmp/test-data",
"type":"string"
"parseSpec":{
"type": "uniform"
"segmentGranularity”:"HOUR"
"queryGranularity":"MINUTE"
"timestampSpec":{
"dimensionsSpec":{
"type":
"name”:"count",
"fieldName":"count",
"format":
"format":
"column":“"timestamp",
"spatialDimensions":[]
"dimensions":[
"LongSum"
"category"
"commodity"
"city",
"age",
"user_id",
"event_name"
"auto"
5
---
## Page 120
IndexJob，向统治节点POST的启动任务的数据如下：
Hadoop IndexJob，需要 POST一个请求到Druid统治节点。沿用之前的案例，启动一个Hadoop
5.3.2
6
"spec":{
Druid Hadoop Index Job 支持从 HDFS 上读取数据，并摄入 Druid系统中。启动一个
"type": "index"
，
"tuningConfig":{
"dataSchema":
以Hadoop方式摄取
"windowPeriod":"PT10m"
"type": "index",
"intermediatePersistPeriod":"PT10m",
"basePersistDirectory":"/rc/data/druid/realtime/basePersist",
"rejectionPolicy":
"maxRowsInMemory":500000,
"dataSource":"dianshang_order",
"type": "none"
"metricsSpec":[
"granularitySpec":{
"type":"index"
"intervals":[
"type":"uniform"
"segmentGranularity":"HOUR"
"type": "local"
"filter":"*.json",
'queryGranularity":"MINUTE"
"2016-07-19/2016-07-20"
Druid实时大数据分析原理与实践
---