dynamics when a single strategic agent is ﬁrst challenged by
others. By training two competing agents in tandem, it is also
possible to explore questions such as: How stable are learned
strategies over time?
In actual deployment of a mechanism M , of course a
community of k ≥ 3 competing strategic agents can arise, a case
more plausible than sustained attack by a single strategic agent.
The SquirRL framework dictates analysis with various values
of k to explore the likely practical security of M . For example,
a mechanism may have poor worst-case security yet have its
participants converge to the strategy of H for all players given
competition among strategic agents. A key question is: How
much reward, as a function of k, can strategic agents collectively
steal from H ? We emphasize that the experiments in Table I
are a starting point, not a full prescription. However, they shed
light on a number of central, incentive-related questions that
are nontrivial to evaluate today.
B. Straw-man solution
A natural ﬁrst step for analyzing consensus protocols is
modeling them as Markov Decision Processes (MDPs), and
using classical algorithms such as policy iteration or value
iteration [67] to solve them. MDPs are commonly used to
model problems where an agent wishes to maximize its reward
in a known, random environment [75]. Value iteration and policy
iteration have been used effectively to computationally learn
optimal adversarial strategies in the two-agent (one strategic,
one honest) setting of the Bitcoin protocol [56].
MDPs are deﬁned as a tuple (S,A,P,R), where S denotes
a set of states, A denotes a set of actions the agent can take,
P denotes the probability transition matrix, where Pa(s,s(cid:48)) =
P(st+1 = s(cid:48)|st = s,at = a) denotes the probability of the agent
transitioning to state s(cid:48) from state s by taking action a. R
is the reward matrix, where Ra(s,s(cid:48)) denotes the expected
reward associated with transitioning from state s to s(cid:48) by
taking action a. We highlight one aspect of this deﬁnition. It
relies critically on a Markov assumption, which states that the
probability distribution over states depends only on the previous
state and the action taken at each time step. Conditioned on
these assumptions, the objective in an MDP is to recover a
strategy π that optimizes the expected discounted long-term
t=0 η tRat (st ,st+1)], where η ∈ (0,1) is a discount
reward E[∑∞
factor that accounts for how much the agent values short-term
rewards over long-term ones, and the expectation is taken over
the randomness in the system evolution and the potentially
randomized strategy π.
With known and exactly speciﬁed (S,A,P,R), value iteration
or policy iteration can exactly solve (up to desired precision)
for the optimal π.
In [56], policy iteration was used to
ﬁnd an optimal selﬁsh mining strategy for a rational Bitcoin
agent. However, policy and value iteration exhibit two primary
constraints that prevent them from being a useful general-
purpose tool for our problem.
1. They assume a stationary environment. To formulate
an MDP, there must exist a ﬁxed probability transition matrix
P. This is true in the single-strategic-agent setting where one
agent is honest and thus behaves according to a known strategy.
However, in practical settings, we may have multiple rational
agents who are dynamically changing their strategies, leading
to a non-stationary environment. This is no longer an MDP,
but a Markov game, where policy iteration and value iteration
do not apply.
2. They scale poorly with growing state spaces. Policy and
value iteration store the probability transition matrix P and
reward matrix R explicitly, which requires storage O(|S|2|A|)
(a probability value must be stored for each transition (s,a,s(cid:48)),
where s is the current state, a is the action taken, and s(cid:48) is the
next state). This can be prohibitive for protocols where the
state cannot be represented by a compact feature, e.g., when
the reward is not computed from a single chain in the ledger’s
directed acyclic graph (DAG) [37], [64], [78].
Even in Bitcoin, the state space can be intractably large.
For example, if there are two strategic (selﬁsh mining) agents
A and B, agent A cannot observe the hidden blocks of agent
B. Agent A needs to at least have an unbiased estimate of B’s
hidden blocks to write out an MDP. The only way for agent
3
A to estimate agent B’s hidden blocks is to use the history of
its observations as well as some notion of time. Suppose that
we wanted to consider a past history of t0 observations, and
the upper limit for our feature for time is T . Then we need
storage O((|S|T )t0+2|A|) (where we let S be the space of all
Bitcoin blocktrees), which quickly grows infeasible.
III. DEEP REINFORCEMENT LEARNING
Reinforcement learning (RL) is a class of machine learning
algorithms that learn strategies enabling an agent to maximize
its cumulative rewards in an environment. Much RL research
is focused on solving MDPs when P or R are unknown or too
large to be practical [67]. However, the ﬁeld encompasses more
general settings, such as Markov games. Deep reinforcement
learning (DRL) is a class of RL that uses neural networks to
learn policies, often without needing to explicitly specify the
underlying system dynamics [21]. In this work, we explore the
potential of deep reinforcement learning to automate the analysis
of attacks on blockchain incentive mechanisms, particularly in
settings where algorithms like policy iteration are impractical
or impossible to use (e.g., large or inﬁnite state spaces, Markov
games, and MDPs with unspeciﬁed P or R).
DRL has been particularly successful in problems where the
state space is intractably large. Roughly, this is because DRL
uses neural networks to replace tables; for instance, instead of
storing a lookup table to decide what action to take at each s ∈ S,
one can instead have a function (neural network) f : S → A
whose size does not scale with |S|. Widely-publicized examples
like chess and Go exhibit large state spaces [60], [61], as do
blockchain incentive mechanisms.
Most blockchain incentive mechanisms have an additional
RL-friendly feature in that rewards are processed continuously.
That is, in chess or Go, rewards are calculated in an all-or-
nothing manner at the end of the game. In blockchains, players
reap rewards incrementally, enabling reward estimation before
the game is complete. This faster feedback makes it easier to
train automated systems to learn effective strategies. As such,
DRL is a natural tool for this problem.
A. Design considerations in deep RL
(1) State space representation. Most blockchains are struc-
tured as a directed acyclic graph (DAG), so a naive state space
representation might be to use the entire DAG as the current
state. This approach has a few problems. First, its dimensionality
grows over time. Second, it includes irrelevant information (e.g.,
old side chains that cannot be displaced with high probability
[22]). We therefore require a representation of the state space
that is general enough to apply to different blockchains, while
being constrained enough to limit the problem dimension.
An important part of SquirRL is our derivation of a general
framework for extracting a compact state space representation
(features) that is general enough to learn meaningful attacks
for different protocols.
(2) Learning algorithm. Two common classes of learning
algorithms are value-based methods and policy gradient methods
[21]. Value-based methods typically aim to build a value
function that associates some value with each state; a common
example is Q-learning [74]. Policy gradient methods instead
try to optimize rewards by performing gradient ascent on
Fig. 1: Schematic of SquirRL learning framework.
a parametric policy, which in our case will be represented
by a neural network [21]. Common examples include the
REINFORCE algorithm [76] and actor-critic methods [66].
We make use of both classes of algorithms in this work,
as different DRL algorithms perform well in different settings.
The most basic algorithm is deep Q-networks (DQNs), which is
based on the classical idea of Q-learning [74]. However, more
sophisticated algorithms (including policy gradient methods)
have surpassed DQNs in many problem areas [34], [57], [79],
sometimes at the expense of higher computational cost.
(3) Reward function. Designing a good reward function can
signiﬁcantly impact the success of the overall system; often
this requires a combination of domain knowledge and some
tuning of hyperparameters.
(4) Training heuristics and hyperparameters. Some attack
models and/or protocols can be difﬁcult to learn due to
complexity. Leveraging blockchain domain knowledge to design
training heuristics can help DRL agents learn, which we show
in Section VI-A.
IV. SQUIRRL: SYSTEM DESCRIPTION
Figure 1 shows use of the RL-based learning framework
that is the cornerstone of SquirRL. It involves a three-stage
pipeline for discovering and analyzing adversarial strategies
targeting an incentive mechanism M .
First, the protocol designer builds an environment that
simulates execution of the protocol realizing M . We anticipate
that the bulk of the effort involved in the SquirRL framework
will go into this part of the system, as the environment fully
encapsulates a model of M . The protocol designer instantiates
in the environment a set of features (state space representation)
over which learning will occur, as well as a space of actions that
agents may take. (What features to use may be iteratively tuned
to improve performance of the RL algorithm at later stages.)
In parallel, the protocol designer chooses an adversarial model
to explore. We have outlined a principled set of choices in our
discussion of Table I in Section II. Finally, the protocol designer
selects an RL algorithm appropriate for the environment and
adversarial model. She must associate with the RL algorithm a
reward function and hyperparameters, both of which may be
iteratively tuned as exploration proceeds.
In this section, we describe respectively the environments,
adversarial models, and RL algorithms we employ in SquirRL
for the experiments in this paper. We focus on incentive
mechanisms related to block rewards since these have dominated
4
the literature on blockchain incentive mechanism vulnerabilities
[18], [19], [24], [45], [48], [56], but similar ideas can be
applied to transaction rewards, for example. The environment
construction depends on the precise incentive mechanism
(and underlying consensus algorithm), but there is a strong
commonality among our environment designs. In particular,
we describe four aspects of the environment: (1) the block
generation model, (2) the reward function, (3) the action space,
and (4) feature extraction. Of these, the most challenging design
problem is the feature extraction, so we provide a general-
purpose algorithm for choosing a feature extractor given an
action space and a model of the environmental randomness.
A. Blockchain generation
As in prior work [19], [56], we assume a randomized
model for block generation. Hence, we view block generation
as a discrete-time process, where a new block is generated
at each time slot i ≥ 0 (we generalize this assumption in
Section VI-B). In proof-of-work (PoW), a party that controls
fraction α of the network’s mining power mines the ith block
with probability α, independently across all time slots. This
model is easily extended to PoS, where parameters depend on
the block generation mechanism.
In accordance with prior work, we assume that the network
communicates blocks instantaneously to other nodes (we relax
this assumption in Section VII-A). In the event that an agent’s
block is received simultaneously with an honest party’s block,
and both are considered equally viable by the consensus
mechanism M , we assume the honest nodes all follow the
adversarial block with probability γ, and all follow the honest
block with probability 1− γ. We call γ the follower fraction.
B. Rewards
In most chain-based blockchains (e.g., Bitcoin), the miner
of a block that appears in the ﬁnal ledger receives a block
reward. Like prior work [19], [48], [56], we consider one or
more attackers that aim to maximize their block rewards given
a constrained amount of computational resources; we ignore
transaction fees for simplicity. We deﬁne Ba(t,S ) and Bo(t,S )
as the rewards of the attacker of interest and all the other miners,
respectively, at time t under an attack strategy S. When there
are multiple attackers of interest, we differentiate them with
superscripts. We compute a miner’s reward by aggregating the
block reward it accumulates, avoiding analysis of uncontrolled
externalities such as coin price or electricity cost.
t
Ba(t,S )
In practice, miners appear to optimize their absolute reward
rate, deﬁned as limt→∞
. However, prior work [19],
[56] has mostly focused on relative rewards, deﬁned as the
attacker’s block rewards as a fraction of the whole network’s
rewards: limt→∞
Ba(t,S )+Bo(t,S ). In Appendix A, we show that
over even moderate time periods in the Bitcoin protocol, the
two objectives are interchangeable. We will focus on relative
rewards for many of our results, since this enables direct
comparison with prior work.
Ba(t,S )
C. Action space and adversarial model
There is a close tie between the action space and adversarial
model. DRL requires system designers to specify the action
5
space, which is informed in part by the type of attacks one
is searching for. Typically, in block-reward-based incentive
mechanisms, we expect an agent to be able to control, at a
minimum, where to append blocks on its local view of the
blockchain, and when to release them.
Example (Bitcoin): We allow agents to take one of four basic
actions, as in [19], [56]. The ﬁrst is adopt, in which an agent
abandons its private fork to mine on the canonical public chain.
This action is always allowed. The second action is override,
in which an agent publishes just enough blocks from its private
chain to overtake the canonical public chain. This action is
feasible only if the agent’s private chain is strictly longer than
the public main chain. The third action is wait, in which an
agent continues to mine without publishing any blocks. This
action is always feasible. The ﬁnal action is match, in which an
agent publishes just enough blocks to equal the length of the
longest public chain as the block on the longest public chain is
being published, causing a fork. In [56], the authors prove the
optimality of this action space in single-strategic-agent selﬁsh
mining (i.e., if an action outside of this action space is chosen,
it results in strictly lower rewards).
In addition to specifying the action space, the adversarial
model requires system designers to specify the number of
strategic agents and their relative resources. In our experiments
on selﬁsh mining in Sections V and VI, we explore the numbers
and types of adversaries given in Table I, for example.
The type of each agent i in our various experiments is
speciﬁed in part by a (fractional) hash power αi. In our selﬁsh