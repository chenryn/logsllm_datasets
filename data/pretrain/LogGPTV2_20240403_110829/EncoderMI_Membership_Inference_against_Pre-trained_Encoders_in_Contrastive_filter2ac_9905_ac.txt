use EncoderMI-V to denote this method.
• Set-based classifier (EncoderMI-S). In a set-based classi-
fier, we directly operate on the set of membership features
M(x, ˜ℎ) of an input. In particular, we train a set-based clas-
sifier (e.g., DeepSets [53]) based on E. A set-based classifier
takes a set (i.e., M(x, ˜ℎ)) as input and predicts a label (1 or 0)
for it. A set-based classifier needs to be input-set-permutation-
invariant, i.e., the predicted label does not rely on the order of
the set elements. As a result, set-based classifiers and vector-
based classifiers require substantially different neural network
architectures. Moreover, set-based classification is generally
harder than vector-based classification. For simplicity, we use
𝑓𝑠 to denote the set-based classifier and we use EncoderMI-S
to denote this method.
• Threshold-based classifier (EncoderMI-T). In a threshold-
based classifier, we use the average similarity score in M(x, ˜ℎ)
of an input to infer its membership. In particular, our threshold-
based classifier predicts an input to be a member if and only
if the average similarity score in its membership features
M(x, ˜ℎ) is no smaller than a threshold. The key challenge is to
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2085Algorithm 1 Our Membership Inference Method EncoderMI
Require: 𝑓𝑣 (or 𝑓𝑠 or 𝑓𝑡), ℎ, A, 𝑛, 𝑆, and x
Ensure: Member or non-member
1: {x1, x2, · · · , x𝑛} ← Augmentation(x, A, 𝑛)
2: M(x, ℎ) ← {𝑆(ℎ(x𝑖), ℎ(x𝑗))|𝑖 ∈ [1, 𝑛], 𝑗 ∈ [1, 𝑛], 𝑗 > 𝑖}
3: if 𝑓𝑣 then
4:
5: else if 𝑓𝑠 then
6:
7: else if 𝑓𝑡 then
8:
9: end if
return 𝑓𝑣(Ranking(M(x, ℎ)))
return 𝑓𝑠(M(x, ℎ))
return 𝑓𝑡 (Average(M(x, ℎ)))
determine the threshold with which the threshold-based clas-
sifier achieves high accuracy at membership inference. Given
a threshold 𝜃, we use 𝛼(𝜃) (or 𝛽(𝜃)) to denote the number of
inputs in the shadow member (or non-member) dataset whose
average similarity score in M(x, ˜ℎ) is smaller (or no smaller)
than 𝜃. The accuracy of our threshold-based classifier with the
threshold 𝜃 for the shadow dataset is 1 − (𝛼(𝜃) + 𝛽(𝜃))/|D𝑠|.
Our threshold-based classifier uses the optimal threshold 𝜃∗
that maximizes such accuracy, i.e., minimizes 𝛼(𝜃) + 𝛽(𝜃). If
we plot the probability distribution of the average similarity
score for shadow members and shadow non-members as two
curves, where the x-axis is average similarity score and y-axis
is the probability that a random shadow member (or non-
member) has the average similarity score, then the threshold
𝜃∗ is the intersection point of the two curves.
Yeom et al. [52] and Song et al. [48] leveraged a similar
threshold-based strategy for membership inference. Different
from us, their methods were designed for classifiers and were
based on the confidence scores outputted by a classifier.
4.3 Inferring Membership
Given a black-box access to the target encoder ℎ and an input x, we
use the inference classifier 𝑓𝑣 (or 𝑓𝑠 or 𝑓𝑡) to predict whether the
input x is a member of the target encoder ℎ. Algorithm 1 shows our
method . Given the input x, the data augmentation module A, and
an integer 𝑛, the function Augmentation produces 𝑛 augmented
inputs. We use the target encoder ℎ to produce a feature vector for
each augmented input, and then we compute the set of pairwise
similarity scores as the membership features M(x, ℎ) for the input
x. Finally, we use the inference classifier to infer the membership
status of the input x based on the extracted membership features.
The function Ranking ranks the similarity scores in M(x, ℎ) in
a descending order, while the function Average computes the
average of the similarity scores in M(x, ℎ).
5 EVALUATION
We evaluate EncoderMI on image encoders pre-trained on unlabeled
images in this section. In Section 6, we apply EncoderMI to CLIP,
which was pre-trained on unlabeled (image, text) pairs.
5.1 Experimental Setup
Datasets: We conduct our experiments on CIFAR10, STL10, and
Tiny-ImageNet datasets.
• CIFAR10 [30]. CIFAR10 dataset contains 60,000 colour im-
ages from 10 object categories. In particular, the dataset con-
tains 50,000 training images and 10,000 testing images. The
size of each image is 32 × 32.
• STL10 [12]. STL10 dataset contains 13,000 labeled colour
images from 10 classes. Specifically, the dataset is divided into
5,000 training images and 8,000 testing images. We note that
STL10 dataset also contains 100,000 unlabeled images. The
size of each image is 96 × 96 in this dataset.
• Tiny-ImageNet [4]. Tiny-ImageNet dataset contains 100,000
training images and 10,000 testing images from 200 classes.
Each class has 500 training images and 50 testing images. Each
image has size 64 × 64.
Training target encoders: For CIFAR10 or Tiny-ImageNet, we
randomly sample 10,000 images from its training data as the pre-
training dataset to train a target encoder; and for STL10, we ran-
domly sample 10,000 images from its unlabeled data as the pre-
training dataset. By default, we use ResNet18 [21] as the archi-
tecture for the target encoder. Moreover, we use MoCo [20] to
pre-train the target encoder on a pre-training dataset. We adopt the
publicly available implementation of MoCo v1 [5] with the default
parameter setting when pre-training our target encoders. Unless
otherwise mentioned, we train a target encoder for 1,600 epochs.
For CIFAR10 or Tiny-ImageNet, we treat its 10,000 testing images
as ground truth “non-member" of the target encoder. For STL-10,
we treat its 5,000 training images and the first 5,000 testing images
as “non-member" of the target encoder. Therefore, unless otherwise
mentioned, for each target encoder, we have 10,000 ground truth
members and 10,000 ground truth non-members.
Training shadow encoders: In the scenario where the inferrer
knows the pre-training data distribution of the target encoder, we
randomly sample 20,000 images from the training or unlabeled data
of the corresponding dataset as the shadow dataset. In the scenario
where the inferrer does not know the pre-training data distribution,
we randomly sample 20,000 images from the training data of CI-
FAR10 as the shadow dataset when the pre-training dataset is from
STL-10, and we randomly sample 20,000 images from the unlabeled
data of STL-10 as the shadow dataset when the pre-training dataset
is CIFAR10 or Tiny-ImageNet.
We randomly split a shadow dataset into two disjoint sets, i.e.,
shadow member dataset and shadow non-member dataset, each of
which contains 10,000 images. We train a shadow encoder using
a shadow member dataset. We adopt the same architecture (i.e.,
ResNet18) for a shadow encoder if the inferrer knows the archi-
tecture of the target encoder and use VGG-11 [45] with batch nor-
malization otherwise. We adopt the same training algorithm (i.e.,
MoCo) to pre-train a shadow encoder if the inferrer knows the algo-
rithm used to pre-train the target encoder and adopt SimCLR [10]
otherwise. We use the publicly available implementations [5, 6]
with the default parameter settings for both training algorithms in
our experiments. We train each shadow encoder for 1,600 epochs.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2086Building inference classifiers: We build inference classifiers based
on a shadow dataset and a shadow encoder. EncoderMI-V uses a
vector-based inference classifier. We use a fully connected neu-
ral network with two hidden layers as our vector-based classifier.
In particular, the number of neurons in both hidden layers are
256. EncoderMI-S uses a set-based inference classifier. We choose
DeepSets [53] as our set-based inference classifier. Moreover, we
adopt the publicly available code for DeepSets [2] in our imple-
mentation. For both the vector-based classifier and the set-based
classifier, we adopt cross-entropy as loss function and use Adam
optimizer with initial learning rate of 0.0001 to train for 300 epochs.
Note that EncoderMI-T leverages a threshold-based classifier and
does not require training.
Evaluation metrics: Following previous work [42, 44], we adopt
accuracy, precision, and recall to evaluate membership inference
methods. Given an evaluation dataset that contains ground truth
members and non-members of the target encoder, accuracy of a
method is the ratio of the ground truth members/non-members
that are correctly predicted by the method; precision of a method
is the fraction of its predicted members that are indeed members;
and recall of a method is the fraction of ground truth members that
are predicted as members by the method.
Compared methods: Existing membership inference methods [11,
42, 44, 46, 48] aim to infer members of a classifier or a text embed-
ding model. We generalize these methods to the contrastive learning
setting as baseline methods. In particular, we compare our methods
with the following five baseline methods, where the first three are
for downstream classifiers, while the last two are for encoders.
Baseline-A. The target encoder is used to train a downstream
classifier for a downstream task. Therefore, in this baseline method,
we use the target encoder to train a downstream classifier (called
target downstream classifier) for a downstream task, and then we
apply existing membership inference methods [42, 44] to the tar-
get downstream classifier. In particular, we consider CIFAR10 as a
downstream task and we randomly sample 10,000 of its training
examples as the downstream dataset. The downstream dataset does
not have overlap with the pre-training dataset and the shadow
dataset. Given a shadow encoder and the downstream dataset, we
train a downstream classifier (called shadow downstream classifier)
via using the shadow encoder as a feature extractor. We query
the confidence score vector for each input in the shadow member
(or non-member) dataset outputted by the shadow downstream
classifier and label it as “member" (or “non-member"). Given these
confidence score vectors as well as the corresponding labels, we
train a vector-based inference classifier. For a given input, we first
query its confidence score vector outputted by the target down-
stream classifier and then use the inference classifier to predict
whether it’s a member of the target encoder. Note that, following
previous work [42], we rank the confidence scores for an input,
which outperforms unranked confidence scores.
Baseline-B. Choquette-Choo et al. [11] proposed label-only
membership inference to a classifier. Roughly speaking, they con-
struct a binary feature vector for an input based on some augmented
versions of the input. An entry of the feature vector is 1 if and only
if the corresponding augmented version is predicted correctly by
the target classifier. This label-only membership inference method
requires the ground truth label of an input. The pre-training data
are unlabeled in contrastive learning, making the method not appli-
cable to infer members of an encoder in practice. However, since the
pre-training data CIFAR10 and Tiny-ImageNet have ground truth
labels in our experiments, we assume an inferrer knows them and
we evaluate the label-only method. Note that we cannot evaluate
this method when the pre-training dataset is from STL10 as they
are unlabeled. Similar to Baseline-A, we also apply this method to a
target downstream classifier to infer members of the target encoder.
For each input x in the shadow dataset, we create 𝑒 augmented in-
puts. Moreover, we use the shadow downstream classifier to predict
the label of x and each augmented input. We construct a binary
vector (𝑏0, 𝑏1, 𝑏2, · · · , 𝑏𝑒) as the membership features for x, where
𝑏0 = 1 (or 𝑏𝑖 = 1) if and only if the shadow downstream classifier
correctly predicts the label of x (or the 𝑖th augmented input), where
𝑖 = 1, 2, · · · , 𝑒. We label the membership features of an input as
“member" (or “non-member") if the input is in the shadow member
(or non-member) dataset. Given membership features and their
labels, we train a vector-based inference classifier. Then, we use
the inference classifier to infer members of a target encoder via a
target downstream classifier. We set 𝑒 = 10 in our experiments.
Baseline-C. Song et al. [48] developed adversarial example
based membership inference methods against classifiers. Specifi-
cally, they leverage the confidence scores produced by the target
classifier for adversarial examples crafted from an input to infer
whether the input is in the training dataset of the target classifier.
For instance, their targeted adversarial example based method (dis-
cussed in Section 3.3.1 of [48]) first crafts 𝑘 − 1 targeted adversarial
examples for an input (one targeted adversarial example for each
label that is not the input’s ground truth label), then uses the target
classifier to compute confidence scores for each of them, and finally
concatenates the confidence scores as membership features for the
input, where 𝑘 is the number of classes in the target classifier. They
train an inference classifier for each class of the target classifier
and use the inference classifier corresponding to the ground truth
label of an input to infer its membership.
Adversarial example of an input can be viewed as the input’s
augmented version. Therefore, we consider these methods in our
experiments. We note that these methods require the ground truth
label of an input and the ground truth label is one class of the down-
stream classifier. However, in contrastive learning, pre-training
data often do not have labels. Moreover, even if the pre-training
data have labels, their labels may not be the same as those of the
downstream classifier. Therefore, we adapt the targeted adversarial
example based method to a downstream classifier in our setting.
Specifically, given an input x, we use PGD [33] to generate 𝑘 tar-
geted adversarial examples based on a shadow downstream classi-
fier, where 𝑘 is the number of classes of the shadow downstream
classifier. We then obtain 𝑘 confidence score vectors outputted by
the shadow downstream classifier for the 𝑘 targeted adversarial
examples, and we concatenate them as membership features for x.
Finally, we train one vector-based inference classifier based on the
membership features of inputs in the shadow dataset, and we apply
it to infer members of the target encoder via the target downstream
classifier. Moreover, following [48], we set the perturbation budget
(i.e., 𝜖) to be 8/255 when generating targeted adversarial examples.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2087Table 1: Accuracy, precision, and recall (%) of the five base-
line methods. “–” means not applicable.
(a) Baseline-A
Pre-training dataset Accuracy Precision Recall
73.1
62.2
68.3
CIFAR10
STL10
Tiny-ImageNet
55.1
54.3
47.3
53.4
53.7
48.2
(b) Baseline-B
Pre-training dataset Accuracy Precision Recall
58.2
–
47.6
CIFAR10
STL10
Tiny-ImageNet
54.6
–
51.8
63.1
–
53.7
(c) Baseline-C
Pre-training dataset Accuracy Precision Recall