h
f
o
r
e
b
m
u
N
20
18
16
14
12
10
8
6
4
2
0
0
15
10
5
0
−5
−10
−15
10
20
30
Days
(a)
40
50
−20
0
60
10
20
40
50
60
30
Days
(b)
Fig. 6. Wavelet transformation of ﬁle update status: (a) The original signal of the ﬁle update status
(b) The residual signal after wavelet transformation.
Figure 5 shows the steps to select features by wavelet-based method. Given a ﬁxed
correlation window of day j, the algorithm starts with constructing a time series signal
Si for each ﬁle i, and decomposes Si into cAi and cDi using a single-level wavelet
transformation as described. Then we compute the residual signal value Ri(j) of day j
by subtracting the trend value cAi(j − 1) of day j − 1 from the original signal value
Si(j) of day j. If Ri(j) exceeds a pre-set threshold α, then the actual number of hosts
who have updated ﬁle i on day j is signiﬁcantly larger than the prediction cAi(j − 1)
Seurat: A Pointillist Approach to Anomaly Detection
245
based on the long term trend. Therefore, Seurat selects ﬁle i as an interesting feature
dimension for anomaly detection on day j. As an example, Figure 6 shows the original
signal and the residual signal of a ﬁle using a 32-day correlation window in a 22-host
teaching cluster. Note the threshold value α of each ﬁle is a parameter selected based
on the statistical distribution of historical residual values.
PCA-Based Dimension Reduction. PCA is a statistical method to reduce data di-
mensionality without much loss of information [12]. Given a set of d-dimensional data
points, PCA ﬁnds a set of d(cid:1)
orthogonal vectors, called principal components, that ac-
count for the variance of the input data as much as possible. Dimensionality reduction
is achieved by projecting the original d-dimensional data onto the subspace spanned by
these d(cid:1)
orthogonal vectors. Most of the intrinsic information of the d-dimensional data
is preserved in the d(cid:1)
-dimensional subspace.
Given a d-dimensional feature space Z d
2 , and a list of m feature vectors V 1, V 2, . . .,
2 , we perform the following steps using PCA to obtain a new list of feature
1, V (cid:1)
We note that the updates of different ﬁles are usually correlated. For example, when
a software package is updated on a host, many of the related ﬁles will be modiﬁed
together. Thus we can perform PCA to identify the correlation of ﬁle updates.
V m ∈ Z d
vectors V (cid:1)
1. Standardize each feature vector V k = (cid:1)v1k, v2k, . . . , vdk(cid:2) (1 ≤ k ≤ m) by sub-
tracting each of its elements vik by the mean value of the corresponding dimension
ui(1 ≤ i ≤ d). We use V k = (cid:1)v1k, v2k, . . . , vnk(cid:2) ∈ Z d
2 to denote the standardized
vector for the original feature vector V k. Then,
2 (d(cid:1) < d) with reduced number of dimensions:
2, . . . , V (cid:1)
m ∈ Z d(cid:1)
(cid:3)m
vik = vik − ui (where ui =
j=1 vij
m
, 1 ≤ i ≤ d)
2. Use the standardized feature vectors V 1, V 2, . . . , V m as input data to PCA in
order to identify a set of principal components that are orthogonal vectors deﬁning
a set of transformed dimensions of the original feature space Z d
2 . Select the ﬁrst d(cid:1)
principal components that count for most of the input data variances (e.g., 90% of
data variances) to deﬁne a subspace Z d(cid:1)
2 .
Z d(cid:1)
2 to obtain the corresponding reduced dimension vector V (cid:1)
3. Project each standardized feature vector V k ∈ Z d
2 onto the PCA selected subspace
k ∈ Z d(cid:1)
2 .
Note that PCA is complementary to wavelet-based selection. Once we ﬁx the corre-
lation window of a particular day, we ﬁrst pick a set of ﬁles to deﬁne the feature vector
space by wavelet-based selection. We then perform PCA to reduce the data dimension-
ality further.
3.3 Anomaly Detection by Clustering
Once we obtain a list of transformed feature vectors using feature selection, we cluster
the vectors based on the distance between every pair of them.
246
Yinglian Xie et al.
We call the cluster a new cluster if it consists of multiple vectors only from the
detection window. The appearance of a new cluster indicates possibly abnormal ﬁle
updates occurred during the detection window and should raise an alarm.
There are many existing algorithms for clustering, for example, K-means [13, 14] or
Single Linkage Hierarchical Clustering [10]. Seurat uses a simple iterative algorithm,
which is a common method for K-means initialization, to cluster vectors without prior
knowledge of the number of clusters [15]. The algorithm assumes each cluster has a
hub. A vector belongs to the cluster whose hub is closest to that vector compared with
the distances from other hubs to that vector. The algorithm starts with one cluster whose
hub is randomly chosen. Then, it iteratively selects a vector that has the largest distance
to its own hub as a new hub, and re-clusters all the vectors based on their distances to
all the selected hubs. This process continues until there is no vector whose distance to
its hub is larger than the half of the average hub-hub distance.
We choose this simple iterative algorithm because it runs much faster, and works
equally well as the Single Linkage Hierarchical algorithm in our experiments. The rea-
son that even the simple clustering algorithm works well is that the ratio of inter-cluster
distance to intra-cluster distance signiﬁcantly increases after feature selection. Since the
current clustering algorithm is sensitive to outliers, we plan to explore other clustering
algorithms such as K-means.
Once we detect a new cluster and generate an alarm, we examine further to identify
the involved hosts and the ﬁles from which the cluster resulted. The suspicious hosts
are just the ones whose ﬁle updates correspond to the feature vectors in the new cluster.
To determine which ﬁles possibly cause the alarm, we only focus on the ﬁles picked by
the wavelet-based selection to deﬁne the feature vector space. For each of those ﬁles, if
it is updated by all the hosts in the new cluster during the detection window, but has not
been updated by any host during the corresponding comparison window, Seurat outputs
this ﬁle as a candidate ﬁle. Similarly, Seurat also reports the set of ﬁles that have been
updated during the comparison window, but are not updated by any host in the new
cluster during the detection window.
Based on the suspicious hosts and the selected ﬁles for explaining root causes, sys-
tem administrators can decide whether the updates are known administrative updates
that should be suppressed, or some abnormal events that should be further investigated.
If the updates are caused by malicious attacks, administrators can take remedial counter
measures for the new cluster. Furthermore, additional compromised hosts can be iden-
tiﬁed by checking if the new cluster expands later and if other hosts have updated the
same set of candidate ﬁles.
4 Experiments
We have developed a multi-platform (Linux and Windows) prototype of Seurat that con-
sists of a lightweight data collection tool and a correlation module. The data collection
tool scans the ﬁle system of the host where it is running and generates a daily summary
of ﬁle update attributes. Seurat harvests the summary reports from multiple hosts in a
network system and the correlation module uses the reports for anomaly detection.
We have installed the Seurat data collection tool on a number of campus ofﬁce
machines and a teaching cluster that are used by students daily. By default, the tool
Seurat: A Pointillist Approach to Anomaly Detection
247
scans the attributes of all system ﬁles on a host. For privacy reasons, personal ﬁles
under user home directories are not scanned. The attributes of a ﬁle include the ﬁle
name, type, device number, permissions, size, inode number, important timestamps, and
a 16-byte MD5 checksum of ﬁle content. The current system uses only a binary bit to
represent each ﬁle update, but the next version may exploit other attributes reported by
the data collection tool. Each day, each host compares the newly scanned disk snapshot
against that from the previous day and generates a ﬁle update summary report. In the
current prototype, all the reports are uploaded daily to a centralized server where system
administrators can monitor and correlate the ﬁle updates using the correlation module.
In this section, we study the effectiveness of Seurat’s pointillist approach for de-
tecting aggregated anomalous events. We use the daily ﬁle update reports from our real
deployment to study the false positive rate and the corresponding causes in Section 4.1.
We evaluate the false negative rate with simulated attacks in Section 4.2. In order to
verify the effectiveness of our approach on real malicious attacks, we launched a real
Linux worm into an isolated cluster and report the results in Section 4.3.
4.1 False Positives
The best way to study the effectiveness of our approach is to test it with real data.
We have deployed Seurat on a teaching cluster of 22 hosts and have been collecting the
daily ﬁle update reports since Nov 2003. The teaching cluster is mostly used by students
for their programming assignments. They are also occasionally used by a few graduate
students for running network experiments.
For this experiment, we use the ﬁle update reports from Dec 1, 2003 until Feb
29, 2004 to evaluate the false positive rate. During this period, there are a few days
when a couple of hosts failed to generate or upload reports due to system failure or
reconﬁgurations. For those small number of missing reports, we simply ignore them
because they do not affect the aggregated ﬁle update patterns.
We set the correlation window to 32 days in order to accommodate monthly ﬁle
update patterns. That is, we correlate the update pattern from day 1 to day 32 to identify
abnormal events on day 32, and correlate the update pattern from day 2 to day 33 to
detect anomalies on day 33, etc. Thus, our detection starts from Jan 1, 2004, since we
do not have 32-day correlation windows for the days in Dec 2003.
Dimension Reduction. Once we ﬁxed the correlation window of a particular day, we
identify relevant ﬁles using wavelet-based selection with a constant threshold α = 2 to
deﬁne the feature vector space for simplicity. We then perform PCA to reduce the data
dimensionality further by picking the ﬁrst several principal components that account
for 98% of the input data variances.
Throughout the entire period of 91 days, 772 ﬁles with unique ﬁle names were
updated by at least two different hosts. Figure 7 (a) shows the number of hosts that
updated each ﬁle during the data collection period. We observe that only a small num-
ber ﬁles (e.g.,/var/adm/syslog/mail.log) are updated regularly by all of the
hosts, while most other ﬁles (e.g., /var/run/named.pid) are updated irregularly,
depending on the system usage or the applications running.
248
Yinglian Xie et al.
)
d
e
t
r
o
s
(
D
I
e
l
i
F
100
200
300
400
500
600
700
10
20
Total files updated
Dimensions defined by wavelet
Dimensions after PCA
102
101
100
70
80
90
10
20
40
Days since Jan 1,2004
30
50
60
40
30
Days since Dec 1, 2003
50
60
(a)
(b)
Fig. 7. Feature selection and dimension reduction: (a) File update patterns. Files are sorted by the
cumulative number of hosts that have updated them throughout the 91 days. The darker the color
is, the more hosts updated the corresponding ﬁle. (b) The number of feature vector dimensions
after wavelet-based selection and PCA consecutively.
Figure 7 (b) shows the results of feature selection. There were, on average, 140 ﬁles
updated by at least two different hosts during each correlation window. After wavelet-
based selection, the average number of feature dimensions is 17. PCA further reduces
the vector space dimension to below 10.
False Alarms. After dimension reduction, we perform clustering of feature vectors and
identify new clusters for each day. Figure 8 illustrates the clustering results of 6 consec-
utive days from Jan 19, 2004 to Jan 24, 2004. There are two new clusters identiﬁed on
Jan 21 and Jan 23, which involve 9 hosts and 6 hosts, respectively. Since Seurat outputs
a list of suspicious ﬁles as the cause of each alarm, system administrators can tell if the
new clusters are caused by malicious intrusions.
Based on the list of ﬁles output by Seurat, we can ﬁgure out that the new clusters on
Jan 21 and Jan 23 reﬂect large scale ﬁle updates due to a system reconﬁguration at the
beginning of the spring semester. For both days, Seurat accurately pinpoints the exact
hosts that are involved. The reconﬁguration started from Jan 21, when a large number
of binaries, header ﬁles, and library ﬁles were modiﬁed on 9 out of the 22 hosts. Since
the events are known to system administrators, we treat the identiﬁed vectors as normal
for future anomaly detection. Thus, no alarm is triggered on Jan 22, when the same
set of library ﬁles were modiﬁed on 12 other hosts. On Jan 23, the reconﬁguration
continued to remove a set of printer ﬁles on 6 out of the 22 hosts. Again, administrators
can mark this event as normal and we spot no new cluster on Jan 24, when 14 other
hosts underwent the same set of ﬁle updates.
In total, Seurat raises alarms on 9 out of the 60 days under detection, among which
6 were due to system reconﬁgurations. Since the system administrators are aware of
such events in advance, they can simply suppress these alarms. The 3 other alarms
are generated on 3 consecutive days when a graduate student performed a network
experiment that involved simultaneous ﬁle updates at multiple hosts. Such events are
normal but rare, and should alert the system administrators.
Seurat: A Pointillist Approach to Anomaly Detection
249
1
20
84
01−19−04
36
322
10
17
15
173
26
4
4
3
2
1
0
−1
−2
−3
−4
01−20−04
95
65
14
27
89
49
49
27
57
53
13
16
66
11
81
−3
−2