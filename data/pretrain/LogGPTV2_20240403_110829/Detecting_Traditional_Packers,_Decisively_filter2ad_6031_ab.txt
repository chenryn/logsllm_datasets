Continuing with our example, suppose that at some time during the execution
of a program, ac contains 128, ip contains 16, M [2] contains −8, and M [16]
contains 39. Since ip contains 16, the instruction stored in M [16] (viz., 39) is
executed. We have seen that this instruction is add (cid:104)2(cid:105). Its execution causes
rval (in this case, the value −8 at M [2]) to be added to ac, changing the value
stored there from 128 to 120. Finally, ip is incremented and its new value is 17.
Table 1 tells us that the cost of executing this instruction is l(ip) + l(ac) + rcost.
Table 2 tells us that rcost is l(2) + l(M [2]). Therefore, the cost of executing the
instruction is:
l(16) + l(128) + l(2) + l(−8) = 19.
We will say that the execution of an instruction takes one step, but this example
illustrates that the cost of an instruction step is variable.
The read instruction gets successive values from an input stream in and the
write instruction puts successive values into an output stream out. If the machine
reads and no input is available, it reads a 0.
A RASP program P is a pair (I, D), where I, the instruction set, is a partial
function I : N (cid:42) Z with ﬁnite domain dom(I), and D, the data set, is a partial
function D : N (cid:42) Z with ﬁnite domain dom(D). We also require that dom(I) ∩
dom(D) = ∅.
To begin executing a RASP program P = (I, D), the program is “loaded”
and the RASP initialized by setting M , ip and ac thusly:3
ip = 0
ac = 0
M [i] =
 I(i)
D(i)
0
i ∈ dom(I)
i ∈ dom(D)
otherwise
Executing P proceeds in a straightforward way. After loading, the RASP
enters a loop that fetches the next instruction M [ip] then decodes the instruction
and executes as speciﬁed in Tables 1 and 2. The machine halts if it reaches a
3 A more realistic initial value for M would not require zero content at locations outside
dom(I)∪ dom(D) since a real computer typically runs many processes concurrently,
but this will suﬃce for our analysis.
halt instruction or if any memory operand references a negative address during
execution.
We may view a RASP program’s dynamic behavior as computing a partial
function that maps an input stream to an output stream. Alternatively, we may
think of a RASP program with read instructions as a nondeterministic machine.
Whenever a read instruction loads a value from the input stream in to a mem-
ory location, we view this as a nondeterministic choice. This nondeterministic
interpretation is apt if P is malware that initiates an undesirable computation
when it receives the appropriate external trigger.
RASP machines are equivalent in computational power to classical Turing
machines [22, 8]. This shows, in particular, that the halting problem for RASP
machines is undecidable. This will be important later.
Deﬁnition 1 (Time and space). The time for the execution of a RASP pro-
gram P = (I, D) on a particular input stream in is the sum of the costs of all
the instructions’ steps, or ∞ if the program does not halt.
The deﬁnition of space for an execution is slightly more subtle because we do
not include the space required for in or for dom(I)∪ dom(D), unless one of these
locations is referenced.4 At any given step t of the execution, let A(t) be the set
of addresses that have been referenced by a stor or read instruction up to step t.
The space used at step t is:
s(t) = l(ip) + l(ac) +
(l(i) + l(M [i])).
(cid:88)
i∈A
The space for the execution is the maximum value of s(t) taken over all steps
t of the execution. It is not diﬃcult to show that the space for an execution is
always bounded above by the time of that execution.
Careful readers will have noted that space is determined in terms of time
cost. This is done because our ISA uses simple operations (addition and sub-
traction) that run quickly relative to the input size. If we had chosen more
complex operations, our time and space characterization would change.
3.3 RASP Program Interpreter
In order to formulate our main results, we require a RASP interpreter, which
we dub Rasputin. Rasputin is a RASP program (IR, DR) that reads an integer
sequence (cid:104)P, w(cid:105) encoding a RASP program P = (I, D) and a ﬁnite input w for
P , then emulates P ’s execution on input w. Recall that if P were loaded directly
into a RASP, location j0 gets I(j0), j1 gets I(j1), and so on; and location k0
gets D(k0), k1 gets D(k1), and so on. (cid:104)P, w(cid:105) is simply a sequence of these pairs;
speciﬁcally, it is a listing of the pairs in the graph5 of I,
j0, I(j0), j1, I(j1), . . . , jr, I(jr)
4 This allows us to consider sublinear space bounds.
5 The graph of a function is the set of all of the pairs that deﬁne it.
followed by a delimiter −1. That is then followed by a listing of the pairs in the
graph of D,
k0, D(k0), k1, D(k1), . . . , ks, D(ks)
followed by a delimiter −1, followed by a listing of the integers w0, w1, . . . , wu
in w.
Rasputin uses three special memory locations in dom(DR): sip, the stored
instruction pointer address; sac, the stored accumulator address; and sopr, the
stored operand address. The data values are DR(sip) = b, DR(sac) = 0, and
DR(sopr) = 0, where b is a base oﬀset larger than any address in dom(IR) ∪
dom(DR).
We describe Rasputin’s instructions in English, but they are straightforward
to implement as a RASP program. Rasputin ﬁrst reads the initial part of (cid:104)P, w(cid:105),
specifying the graph pairs of I and D. As it reads, it stores them relative to its
base address b: thus, M [b + j] ← I(j) for every j ∈ dom(I) and M [b + j] ← D(j)
for every j ∈ dom(D).
Next, Rasputin enters a fetch-decode-execute loop. During each cycle, it trans-
fers the instruction j whose address is in sip to the accumulator. It then decodes
j into an opcode r and operand q, where j = 16q + r, and stores these values in
the accumulator and sopr.6 Next, by alternately executing bpa instructions and
decrementing the value in the accumulator, Rasputin ﬁnds the section in its pro-
gram that will execute instruction j. At this point, it carries out the operational
semantics in Tables 1 and 2, with sip and sac substituted for ip and ac, and with
oﬀset addresses whenever they are needed. It then repeats the cycle.
We oﬀer this drawn-out description to emphasize that Rasputin is a well-
behaved program. Whatever the input (cid:104)P, w(cid:105) may be, Rasputin will not execute
an instruction outside of those in IR or modify any of the instructions inside IR.
Rasputin is not malware.
Below, we use Rasputin to represent a dynamic analyzer. Rasputin observes
RASP code as it executes, and it may modify its behavior in response to what
it sees.
3.4 Formalizing Unpacking Behavior
We begin by using the RASP model to exhibit a version of the undecidability
result of the PolyUnpack paper [1]. Our proof improves upon previous work by
providing a precise and intuitive characterization of unpacking behavior (Deﬁ-
nition 2). It also justiﬁes the fact that our model is just as general as a Turing
machine. The basic fact we need is the undecidability of the following problem.
Theorem 1 (Halting Problem for RASPs). Given: RASP program P =
(I, D) and ﬁnite input sequence x. Question: Does P halt when it executes with
input x?
6 The RASP code required to do this when j is positive involves generating powers of 2
by repeated doubling until one at least as large as j is generated, using these powers
of 2 to determine the binary representation of j, and then from this computing r
and q. The procedure when j is negative is similar.
Proof. We have immediately that this problem is undecidable by the Elgot-
Robinson [22] result giving an eﬀective transformation from Turing machines into
equivalent RASP programs and from the undecidability of the Halting Problem
(cid:117)(cid:116)
for Turing Machines.
Now we come to the main deﬁnition of this section.
Deﬁnition 2 (Unpacking Behavior). Let P = (I, D) be a program and x a
sequence of inputs. P is said to exhibit unpacking behavior (or to unpack) on x
if, at some point during execution, ip (cid:54)∈ dom(I) (data-execution) or P stores to
an address in dom(I) (self-modiﬁcation).
From this, we formalize the problem of detecting unpacking. We demonstrate
[1]. Theorem 3 is the
two independent results. Theorem 2 mirrors Royal et al.
general case of the problem of greatest import.
Deﬁnition 3 (Special Unpacking Problem). Given: RASP program P =
(I, D) and ﬁnite input sequence x. Question: Does P unpack on input x?
Theorem 2. The Special Unpacking Problem is undecidable.
Proof. Reduce the Halting Problem for RASP machines (Theorem 1) to the
Special Unpacking Problem.
First, we describe a modiﬁcation of Rasputin we will call Evil Rasputin. Evil
Rasputin is a RASP program (IE , DE ) obtained from Rasputin by replacing Ras-
putin’s halt conditions (viz. emulation of a halt instruction or an attempt by the
emulated program to reference a negative address) with a jmp instruction to an
address not in dom(IR). (This involves inserting checks for negative addresses
and branches at appropriate points in IR.)
Now, P halts on input w if and only if Evil Rasputin unpacks on input x =
(cid:104)P, w(cid:105). This reduces the Halting Problem for RASPs to the Special Unpacking
Problem. If there were a decision algorithm for the latter problem, there would be
one for the former problem, as well. This would be a contradiction to Theorem 1.(cid:117)(cid:116)
Deﬁnition 4 (Unpacking Problem). Given: RASP program P = (I, D).
Question: Is there a ﬁnite input sequence x such that P unpacks on x?
Theorem 3. The Unpacking Problem is undecidable.
Proof. The proof is very similar to the proof of Theorem 2. Reduce the Halting
Problem for RASPs to the Unpacking Problem.
Let P be a RASP program and x an input (i.e., a ﬁnite integer sequence) for
P . We describe a modiﬁed version of Evil Rasputin called Evil RasputinP,x, which
has no read instructions. Instead, P and x are preloaded in the data section
section, DE . Rather than reading (cid:104)P, x(cid:105) from an input stream, Evil RasputinP,x
transfers values from its data section to the appropriate locations. In all other
respects, it behaves in the same way as Evil Rasputin. In particular, Evil Raspu-
tinP,x unpacks (irrespective of its input since it has no reads) if and only if P
halts on input x. Thus, the mapping from (cid:104)P, x(cid:105) to Evil RasputinP,x is a reduction
from the Halting Problem for RASPs to the Unpacking Problem. If there were
a decision algorithm for the latter problem, there would be one for the former
(cid:117)(cid:116)
problem, as well. Again, this would be a contradiction.
3.5 Space bounded RASP
The undecidability results of the previous section do not address the real issue of
malware detection because no real machine looks like our RASP. Real machines
cannot store arbitrary sized integers in every memory location. Real machines do
not have an inﬁnite set of memory registers. Real machines have ﬁxed resources.
We therefore present a restriction of the RASP model by bounding space in
terms of input size. This is analogous to the restriction used for Linear Bounded
Automata [32].
Deﬁnition 5 (Space bounded RASP). A Γ -space bounded RASP program
is a RASP program that uses space at most Γ (n) on all inputs of size n. A
Γ -space bounded RASP is one that executes only Γ -space bounded programs. It
executes programs in exactly the same way as a RASP, except that on inputs
of size n, if a program ever attempts to use more than space Γ (n), the Γ -space
bounded RASP will halt.
The following problem is a step toward formulating a more realistic goal for
static malware detection.
Deﬁnition 6 (Space Bounded Unpacking Problem). Given: Computable
function Γ , Γ -space bounded RASP program P , and integer k > 0. Question:
Is there an input x with l(x) ≤ k such that P unpacks on input x?
Theorem 4. The Space Bounded Unpacking Problem is decidable.
lem. Let the size of a ﬁnite integer sequence w be l(w) =(cid:80)
Proof. We describe an algorithm to decide the Space Bounded Unpacking Prob-
First, consider a speciﬁc x with n = l(x) ≤ k. P is restricted to space at
most Γ (n) = s on input x. A conﬁguration for P at any step of its execution
is a list of all of the information needed to determine future actions of P . More
precisely, the conﬁguration at a given step is a list of the following:
i∈w l(i).
1. the contents of ac;
2. the contents of ip;
3. a list of all of the addresses that have been referenced up to this step, and
their contents; and
4. the number of integers in the input sequence x that remain to be read.
From this, we will determine an upper bound for the total possible number of
conﬁgurations.
First, note that there are precisely 2s nonnegative integers i with l(i) ≤ s, viz.,
the integers in A = {0, 1, . . . , 2s − 1}. Also, there are precisely 2s+1 − 1 integers
i with l(i) ≤ s, viz., the integers in B = {−(2s − 1),−(2s − 2), . . . , 2s − 1}. The
contents of ip must be from A. The contents of ac must be from B. When P
executes on input x, every address in A has either never been referenced, or
its contents are in B. Moreover, only addresses in A could possibly have been
referenced. Thus, for item 1 above, there are at most 2s+1 − 1 possibilities; for
item 2, there are at most 2s possibilities; for item 3, there at most (2s+1)2s
possibilities; and for item 4, there are at most n + 1 possibilities. Therefore,
there are at most the following possible conﬁgurations:
b(n) = (2s+1) · 2s · 2(s+1)2s · (n + 1)
Now to see if P unpacks on a given x, use an augmented Rasputin to emulate
P ’s execution on x. After each step, check to see if P has unpacked, and if it
has, report the result. If, at some point, P halts and no unpacking behavior
has occurred, report that result. Keep a tally of the number of emulated steps.
When the tally exceeds b(n), we know that we are in an inﬁnite loop, so if no
unpacking behavior has been observed up to that point, it never will be. Report
that result.
Apply the algorithm outlined above for every x such that l(x) ≤ k. There
are only ﬁnitely many such x’s, so we can decide if unpacking behavior ever
(cid:117)(cid:116)
occurs.
Real computers are all space bounded, in fact, constant space bounded.
Therefore, detecting unpacking behavior for real computers is decidable. Unfortu-
nately, for real computers the algorithm given in the proof above has an execution
time many orders of magnitude greater than the lifetime of the universe, so the