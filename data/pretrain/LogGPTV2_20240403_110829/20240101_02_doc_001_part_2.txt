you can also transform and correct column names, data types and values. If your input
data is nested documents, you can unnest and flatten the data to make relational data
analysis easier and more efficient.
Figure 1.3 Using DuckDB in a data pipeline.
© Manning Publications Co. To comment go to liveBook
7
In the next step, you need to determine which SQL capabilities or DuckDB features can
help you to perform that analysis or transformation. You can also do Exploratory Data
Analysis (EDA) to quickly get an overview of the distribution, ranges and relationships in
your data.
After getting acquainted with the data, you can proceed to the actual analytics tasks.
Here you will build the relevant SQL statements incrementally, verifying at each step that
the sample of the results produced matches your expectations. At this stage, you might
create additional tables or views, before using advanced SQL features like window
functions, common table expressions, and pivots. Finally, you need to decide which way
the results are consumed, either by turning them into files or databases again or serving
them to users through an application, API, or by visualizing them in a Jupyter notebook
or dashboard.
1.7 Steps of the data processing flow
In the following sections, we will describe some specific aspects of DuckDB’s architecture
and feature set at a high level to give you an overall understanding and appreciation. We
have ordered the sections in the sequence of how you would use DuckDB, from loading
data to populating tables and writing SQL for analysis to visualizing those results, as
shown in figure 1.4.
Figure 1.4 The data processing flow
1.7.1 Data Formats and Sources
DuckDB supports a large number of data formats and data sources and it lets you
inspect and analyze their data with little ceremony. Unlike other data systems, like SQL
Server, you don’t need to first specify schema details upfront. When reading data the
database uses sensible defaults and inherent schema information from the data, that you
can override when needed.
NOTE With DuckDB you can focus more on the data processing and analysis that you need to do
and not so much on upfront data-engineering. Being an open-source project built by practitioners,
there is a lot of emphasis on usability — if something is too hard to use, someone in the
community will propose and submit a fix. And if the built-in functionality reaches not far enough,
there’s probably an extension that addresses your needs (e.g. geospatial data or full-text search).
© Manning Publications Co. To comment go to liveBook
8
DuckDB supports a variety of data formats:
CSV files can be loaded in bulk and parallel and their columns are
automatically mapped.
DataFrames' memory can be handled directly by DuckDB inside the
same Python process without the need to copy data
JSON or JSONLines formats can be destructured, flattened, and
transformed into relational tables. DuckDB also has a JSON type for
storing this type of data.
Parquet files along with their schema metadata can be queried.
Predicates used in queries are pushed down and evaluated at the
Parquet storage layer to reduce the amount of data loaded. This is the
ideal columnar format to read and write for data lakes.
Apache Arrow columnar shaped data via Arrow Database Connectivity
(ADBC) without data copying and transformations
Accessing data in cloud buckets like S3 or GCP reduces transfer and
copy infrastructure and allows for cheap processing of large data
volumes.
1.7.2 Data structures
DuckDB handles a variety of tables, views, and data types. For table columns, processing
and results, there are more data types available than just the traditional data types like
string (varchar), numeric (integer, float, decimal), dates, timestamps, intervals, boolean,
and blobs (Binary Large Objects).
DuckDB also supports structured data types like enums, lists, maps (dictionaries) and
structs.
Enums are indexed, named elements of a set, that can be stored and
processed efficiently.
Lists or arrays hold multiple elements of the same type and there are a
variety of functions for operating on these lists.
Maps are efficient key-value pairs that can be used for keeping keyed
data points. They are used during JSON processing and can be
constructed and accessed in several ways.
Structs are consistent key-value structures, where the same key
always has values of the same data type. That allows for more efficient
storage, reasoning and processing of structs.
DuckDB also allows you to create your own types and extensions can provide additional
data types as well. DuckDB can also create virtual or derived columns that are created
from other data via expressions.
© Manning Publications Co. To comment go to liveBook
9
1.7.3 Develop the SQL
When analyzing data, you usually start by gaining an understanding of the shape of the
data. Then you work from simple queries to creating more and more complex ones from
the basic building blocks. You can use DESCRIBE to learn about the columns and data
types of your data sources, tables and views. Armed with that information you can get
basic statistics and distributions of a dataset by running count-queries count(*) globally
or grouped by interesting dimensions like time, location, or item type. That already gives
you some good insights on what to expect from the data available.
DuckDB even has a SUMMARIZE clause that gives you statistics per column:
count
min, max, avg, std (deviation)
approx_unique (estimated count of distinct values)
percentiles (q25, q50, q75)
null_percentage (part of the data being null)
To write your analytics query you can start working on a subset of the data by using
LIMIT or only looking at a single input file. Start by outlining the result columns that you
need (these may sometimes be converted, e.g. for dates using strptime). Those are
the columns you would group by. Then apply aggregations and filters to your data as
needed. There are many different aggregation functions available in DuckDB, from
traditional ones like min, avg, sum to more advanced ones like histogram,
bitstring_agg, list or approximations like approx_count_distinct. There are also
advanced aggregations, including percentiles, entropy or regression computation, and
skewness. For running totals and comparisons with previous and next rows, you would
use window functions aggregation OVER (PARTITION BY column ORDER BY column2
[RANGE … ]). Repeatedly used parts of your analytics statement can be extracted into
named common table expressions (CTEs) or views. Often, it also helps for readability to
move parts of the computation into subqueries and use their results to check for
existence or do some nested data preparation.
While you’re building up your analytical statement, you can check the results at any
time to make sure they are still correct and you’ve not taken an incorrect detour. This
takes us to our next and last section on using the results of your queries.
1.7.4 Use or process the results
You’ve written your statement and got the analytics results quickly from DuckDB. Now
what?
© Manning Publications Co. To comment go to liveBook
10
It would be useful to keep your results around, e.g. by storing them in a file or a
table. Creating a table from your results is straightforward with CREATE TABLE 
AS SELECT … . DuckDB can output a variety of formats, including CSV, JSON, Parquet,
Excel, and Arrow. It also supports other database formats like SQLite, Postgres and
others via custom extensions. For smaller results sets, you can also use the DuckDB CLI
to output the data as CSV or JSON.
But because a picture tells more than 1000 rows, often the preferred choice is data
visualization. With the built-in bar function you can render inline bar charts of your data.
You could also use command-line plotting tools like youplot for some quick results in
your terminal.
In most cases though, you would use the large Python and Javascript ecosystem to
visualize your results. For those purposes, you can turn your results into DataFrames,
which then can be rendered into a variety of charts with matplotlib, ggplot in Python,
ggplot2 in R, or d3, nivo, observable in Javascript. A visual representation showing
this is in figure 1.5.
Figure 1.5 Visualizing data in a dashboard or Jupyter Notebook
© Manning Publications Co. To comment go to liveBook
11
As DuckDB is so fast, you can serve the re sults directly from your queries to an API
endpoint that you or others can use downstream or integrate it into an app for
processing like Streamlit. You only need a server setup, if your source data is too big to
move around and your results comparatively small (much less than 1% of the volume).
Otherwise, you can just embed DuckDB into your application and have it run on local raw
data or a local DuckDB database.
1.8 Summary
DuckDB is a newly developed analytical database that excels at in-
memory processing.
The database supports an extended dialect of SQL and gains new
capabilities with extensions.
DuckDB can read a variety of formats natively from local and remote
sources.
The integration in Python, R and other languages is seamless and
efficient.
As an in-process database it can process data efficiently without
copying.
In addition to the traditional data types, DuckDB also supports lists,
maps, structs, and enums.
DuckDB provides a lot of functions on data types and values, making
data processing and shaping much easier.
Building up your SQL queries step by step after learning about the
shape of your data helps to stay in control.
You can use the results of your query in a variety of ways, from
generating reports and visualizing in charts to outputting in new
formats.
© Manning Publications Co. To comment go to liveBook
12
2
Getting Started with DuckDB
This chapter covers
Installing and learning how to use the DuckDB CLI
Executing commands in the DuckDB CLI
Querying remote files
Now that we’ve got an understanding of what DuckDB is and why it’s come into
prominence in the early 2020s, it’s time to get familiar with it. This chapter will be
centered around the DuckDB CLI. We’ll learn how to install it on various environments,
before learning about the in-built commands. We’ll conclude by querying a remote CSV
file.
2.1 Supported environments
DuckDB is available for a range of different programming languages and operating
systems (Linux, Windows, macOS) both for Intel/AMD and ARM architectures. At the time
of writing, there is support for the command line, Python, R, Java, Javascript, Go, Rust,
Node.js, Julia, C/C++, ODBC, JDBC, WASM, and Swift. In this chapter, we will focus on
the DuckDB command line exclusively, as we think that is the easiest way to get you up
to speed.
The DuckDB CLI does not require a separate server installation as DuckDB is an
embedded database and in the case of the CLI, it is embedded in exactly that.
© Manning Publications Co. To comment go to liveBook
13
The command line tool is published to GitHub releases and there are a variety of
different packages for different operating systems and architectures. You can find the full
list on the installation page.
2.2 Installing the DuckDB CLI
The installation is a "copy to" installation, no installers or libraries are needed. The CLI
consists of a single binary named duckdb. Let’s learn how to go about installing DuckDB.
2.2.1 macOS
On macOS the official recommendation is to use Homebrew:
Listing 2.1 Install DuckDB on macOS via Homebrew
# This is only necessary to install Homebrew itself,
➥ don't run if you already have it
# /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/
➥Homebrew/install/HEAD/install.sh)"
brew install duckdb
2.2.2 Linux and Windows
There are many different packages available for Linux and Windows, depending on the
particular architecture and version that you’re using. You can find a full listing on the
GitHub releases page.
In listing 2.2, we learn how to do zero-process installation on Linux with an AMD64
architecture:
Listing 2.2 Zero-process installation on Linux
wget https://github.com/duckdb/duckdb/releases/download/v0.8.1/
➥duckdb_cli-linux-amd64.zip # #1
unzip duckdb_cli-linux-amd64.zip
./duckdb -version
#1 Don’t forge to update this link to the latest version from the GitHub releases page.
2.3 Using the DuckDB CLI
The simplest way to launch the CLI is shown below, and yes, it’s that little, and it’s quick:
duckdb
© Manning Publications Co. To comment go to liveBook
14
This will launch DuckDB and the CLI. You should see something like the following output:
v0.8.1 6536a77232
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database.
The database will be transient, with all data held in memory. It will disappear when you
quit the CLI, which you can do by typing .quit or .exit.
2.3.1 Dot commands
In addition to SQL statements and commands, the CLI has several special commands
that are only available in the CLI, the special dot commands. To use one of these
commands, begin the line with a period (.) immediately followed by the name of the
command you wish to execute. Additional arguments to the command are entered,
space separated, after the command. Dot commands must be entered on a single line,
and no whitespace may occur before the period. No semicolon is required at the end of
the line in contrast to a normal SQL statement or command.
Some of the most popular dot commands are described below:
.open closes the current database file and opens a new one.
.read allows reading SQL files to execute from within the CLI.
.tables lists the currently available tables and views.
.timer on/off toggles SQL timing output.
.mode controls output formats.
.maxrows controls the number of rows to show by default (for duckbox
format).
.excel shows the output of next command in spreadsheet.
.quit or ctrl-d exit the CLI.
A full overview can be retrieved via .help.
2.3.2 CLI arguments
The CLI takes in arguments that can be used to adjust the database mode, control the
output format, or decide whether the CLI is going to enter interactive mode. The usage is
duckdb [OPTIONS] FILENAME [SQL].
Some of the most popular CLI arguments are described below:
-readonly opens the database in read-only mode.
-json sets the output mode to json.
-line sets the output mode to line.
-unsigned allows for the loading of unsigned extensions.
© Manning Publications Co. To comment go to liveBook
15
-s COMMAND or -c COMMAND runs the provided command and then
exits. This is especially helpful when combined with the .read dot
command, which reads input from the given file name.
To get a list of the available CLI arguments, call duckdb -help.
2.4 DuckDB’s extension system
DuckDB has an extension system that is used to house functionality that isn’t part of the
core of the database. You can think of extensions as packages that you can install with
DuckDB.
DuckDB comes pre-loaded with several extensions, which vary depending on the
distribution that you’re using. You can get a list of all the available extensions, whether
installed or not, by calling the duckdb_extensions function. Let’s start by checking the
fields returned by this function:
DESCRIBE
SELECT *
FROM duckdb_extensions();
A truncated view of the output is shown in listing 2.3:
Listing 2.3 Fields returned by duckdb_extensions
┌────────────────┬─────────────┐
│ column_name │ column_type │
│ varchar │ varchar │
├────────────────┼─────────────|
│ extension_name │ VARCHAR │
│ loaded │ BOOLEAN │
│ installed │ BOOLEAN │
│ install_path │ VARCHAR │
│ description │ VARCHAR │
│ aliases │ VARCHAR[] │
└────────────────┴─────────────┴
Let’s check which extensions we have installed on our machine:
SELECT extension_name, loaded, installed
from duckdb_extensions()
ORDER BY installed DESC, loaded DESC;
© Manning Publications Co. To comment go to liveBook
16
The results of running the query are shown in listing 2.4:
Listing 2.4 A list of DuckDB’s extensions
┌──────────────────┬─────────┬───────────┐
│ extension_name │ loaded │ installed │
│ varchar │ boolean │ boolean │
├──────────────────┼─────────┼───────────┤
│ autocomplete │ true │ true │
│ fts │ true │ true │