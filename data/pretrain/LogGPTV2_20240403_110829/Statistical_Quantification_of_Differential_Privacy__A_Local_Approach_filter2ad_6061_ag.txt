25.9
57.3
TABLE II: Runtimes for one run of the MPL algorithm on
(a)-(h). Times are averaged over 10 simulation runs.
The data-centric privacy level for ﬁxed databases: As
pointed out in Section IV, we can use the MPL algorithm
to determine the data-centric privacy guarantee for select
databases deﬁned in (5). We demonstrate this on both versions
(discrete and continuous) of the Noisy Max algorithm.
(cid:2)
Regarding the discrete case, suppose we have a database x
that, given 6 counting queries, evaluates to 0 for each query,
that is q = q(x) = (0, 0, 0, 0, 0, 0). Recalling our discussion
(cid:2) in the
of the query model, we know that any database x
(cid:2) ∈ {0, 1}6.
neighborhood of x evaluates to a binary vector q
This means that the entire neighborhood of x can be exhausted
). We set the
by the collection of all such query pairs (q, q
privacy parameter  = 1.5 and run the MPL algorithm for
Report Noisy Max on that collection of query pairs 1000
times. In Figure 5 (left panel) we plot the empirical cdf of LB
(purple), which exhibits a sharp rise, long before the global
privacy parameter  (vertical green line). In view of our earlier
results and given the exhaustive search of query pairs, we can
be conﬁdent that the empirical cdf captures the data-centric
privacy leakage x. The plot suggests that the data-centric
privacy parameter is only about half the size of , conﬁrming
that the amount of privacy afforded to this speciﬁc database
outstrips the worst case guarantee.
For the continuous case, we consider a database x that
produces the statistic s = S(x) = (1/2, 1/2, 1/2) and assume
(cid:2) anywhere on the unit
that S maps neighboring databases x
(cid:2) ∈ {0, 1/2, 1}3 (which forms an even
cube [0, 1]3. Let s
grid of 27 points on the unit cube). We can run MPL on
the collection of statistics thus obtained. It can be shown by
similar methods as employed in Example 1, that x,x(cid:2) = x
is attained for databases x
= (0, 0, 0) or
= (1, 1, 1), both of which are covered by our grid.
S(x
As for the discrete case, we observe that x is about half the
size of  (see Figure 5, right panel). In conclusion, the amount
of privacy ceded to our speciﬁc databases x in both examples
is about twice as high as the global privacy parameter suggests
(i.e. x ≈ /2).
(cid:2) with S(x
) = s
) = s
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
415
Continuous Noisy Max
Exponential Mechanism
r
o
r
r
e
d
e
r
a
u
q
s
n
a
e
m
0.05
0.04
0.03
0.02
0.007
0.006
0.005
0.004
0.003
0.002
r
o
r
r
e
d
e
r
a
u
q
s
n
a
e
m
10000 20000 30000 40000 50000
sample size
10000 20000 30000 40000 50000
sample size
Fig. 6: Mean squared error E(ˆx,x(cid:2) − x,x(cid:2) )2 for different
sample sizes n and x,x(cid:2) = 1.5.
Estimation of data-speciﬁc privacy violations: Up to this
point we have focused on the lower bound LB, produced by
the MPL algorithm. We now want to consider the estimation
of data-speciﬁc privacy violations deﬁned in (3), which is the
key novelty of our local approach and, as an integral part of
MPL, has an outsize effect on the quality of LB. We especially
focus on the two continuous algorithms (Noisy Max and the
Exponential Mechanism), where our estimator ˆx,x(cid:2) differs
most noticeably from prior approaches by virtue of kernel
density estimation.
(cid:2)
(cid:2)
(cid:2)
= S(x
(cid:2) that result in s = 1 and s
Regarding the Noisy Max algorithm, suppose we choose
(cid:2) that produce statistics s = S(x) = (0, 0, 0)
databases x and x
) = (1, 1, 1), and similarly for the Exponential
and s
= 2.
Mechanism databases x and x
In both situations, the choice of these databases provokes a
privacy violation x,x(cid:2) =  that is equal to the global privacy
parameter, which we ﬁx at 1.5.
To study the quality of the estimator ˆx,x(cid:2) based on n obser-
vations, we consider the mean squared error E(ˆx,x(cid:2) − x,x(cid:2) )2
(approximated by 1000 simulation runs) for both algorithms.
In Figure 6 we display the simulated errors for the two
algorithms and different sizes of n. In both cases we observe
for a sample size as moderate as 5000 only small estimation
errors (less than 4% of the true  for Noisy Max and less than
0.5% for the Exponential Mechanism) and the errors are less
than half of this for n = 20000 (which is used in our previous
experiments). This shows that the strong performance of MPL
can also be attributed to the precision of our local estimators
for the data-speciﬁc privacy violations.
VI. CONCLUSION
In this work, we have discussed a way to assess privacy
with statistical guarantees in a black box scenario. In contrast
to prior works, our approach relies on a local conception of
DP that facilitates the estimation and interpretation of privacy
violations by circumventing the problem of event selection.
Besides quantiﬁcation of the global privacy parameter, our
methods can be used for a more reﬁned analysis, measuring the
amount of privacy ceded to a speciﬁc database. The ﬁndings
of this analysis might not only help to understand existing
algorithms better, but also aid the design of new privacy
preserving mechanisms. This can, for instance, be algorithms
that are tailored to provide greater privacy to databases that
require more protection.
ACKNOWLEDGMENTS
This work was supported by the Deutsche Forschungsge-
meinschaft (DFG, German Research Foundation) under Ger-
many’s Excellence Strategy - EXC 2092 CASA - 390781972.
We would also like to thank the anonymous reviewers for their
fruitful comments and suggestions to improve this work.
REFERENCES
[1] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to
sensitivity in private data analysis,” in TCC’06, 2006.
[2] U. Erlingsson, V. Pihur, and A. Korolova, “Rappor: Randomized aggre-
gatable privacy-preserving ordinal response,” in CCS ’14, 2014.
[3] B. Ding, J. Kulkarni, and S. Yekhanin, “Collecting telemetry data
privately,” in NIPS’17, 2017.
[4] N. Johnson, J. P. Near, and D. Song, “Towards practical differential
privacy for sql queries,” Proc. VLDB Endow., vol. 11, no. 5, p. 526–539,
2018.
[5] J. M. Abowd, “The U.S. census bureau adopts differential privacy,” in
Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD 2018, London, UK, August
19-23, 2018. ACM, 2018, p. 2867.
[6] G. Barthe, G. Danezis, B. Gr´egoire, C. Kunz, and S. Z. B´eguelin,
“Veriﬁed computational differential privacy with applications to smart
metering,” in CSF’13, 2013.
[7] G. Barthe, M. Gaboardi, E. G. Arias, J. Hsu, C. Kunz, and P. Strub,
“Proving differential privacy in hoare logic,” in CSF’14, 2014.
[8] G. Barthe, N. Fong, M. Gaboardi, B. Gr´egoire, J. Hsu, and P.-Y. Strub,
“Advanced probabilistic couplings for differential privacy,” in CCS’16,
2016.
[9] X. Liu and S. Oh, “Minimax optimal estimation of approximate differ-
ential privacy on neighboring databases,” in NeurIPS ’19, 2019.
[10] G. Barthe, R. Chadha, V. Jagannath, A. P. Sistla, and M. Viswanathan,
“Deciding differential privacy for programs with ﬁnite inputs and
outputs,” in LICS ’20, 2020.
[11] J. Reed and B. C. Pierce, “Distance makes the types grow stronger: A
calculus for differential privacy,” in ICFP’10, 2010.
[12] M. Gaboardi, A. Haeberlen, J. Hsu, A. Narayan, and B. C. Pierce,
“Linear dependent types for differential privacy,” in POPL’13, 2013.
[13] G. Barthe, M. Gaboardi, B. Gr´egoire, J. Hsu, and P.-Y. Strub, “Proving
differential privacy via probabilistic couplings,” in LICS ’16, 2016.
[14] A. Albarghouthi and J. Hsu, “Synthesizing coupling proofs of differen-
[15] D. Zhang and D. Kifer, “Lightdp: Towards automating differential
tial privacy,” vol. 2, no. POPL, 2017.
privacy proofs,” in POPL ’17, 2017.
[16] Y. Wang, Z. Ding, G. Wang, D. Kifer, and D. Zhang, “Proving differ-
ential privacy with shadow execution,” in PLDI ’19, 2019.
[17] H. Zhang, E. Roth, A. Haeberlen, B. C. Pierce, and A. Roth, “Testing
differential privacy with dual interpreters,” vol. 4, no. OOPSLA, 2020.
[18] Y. Wang, Z. Ding, D. Kifer, and D. Zhang, “Checkdp: An automated and
integrated approach for proving differential privacy or ﬁnding precise
counterexamples,” in CCS ’20, 2020.
[19] Z. Ding, Y. Wang, G. Wang, D. Zhang, and D. Kifer, “Detecting
violations of differential privacy,” in CCS ’18, 2018.
[20] B. Bichsel, T. Gehr, D. Drachsler-Cohen, P. Tsankov, and M. Vechev,
“Dp-ﬁnder: Finding differential privacy violations by sampling and
optimization,” in CCS ’18, 2018.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
416
[21] B. Bichsel, S. Steffen, I. Bogunovic, and M. Vechev, “Dp-sniper: Black-
box discovery of differential privacy violations using classiﬁers,” in 2021
IEEE Symposium on Security and Privacy (SP), 2021.
[22] J. Soria-Comas, J. Domingo-Ferrer, D. S´anchez, and D. Meg´ıas, “Indi-
vidual differential privacy: A utility-preserving formulation of differen-
tial privacy guarantees,” IEEE Trans. Inf. Forensics Secur., 2017.
[23] K. Nissim, S. Raskhodnikova, and A. Smith, “Smooth sensitivity and
sampling in private data analysis,” in STOC ’07, 2007.
[24] B. I. P. Rubinstein and F. Ald`a, “Pain-free random differential privacy
with sensitivity sampling,” in ICML ’17, 2017.
[25] R. Hall, L. Wasserman, and A. Rinaldo, “Random differential privacy,”
Journal of Privacy and Conﬁdentiality, vol. 4, no. 2, 2013.
[26] P. J. Bickel and K. A. Doksum, “Mathematical statistics.” CRC Press,
2015.
[27] A. van der Vaart and J. Wellner, “Weak convergence and empirical
processes. with applications to statistics.” Springer Series in Statistics.,
1996.
[28] D. W. S. Scott, “Multivariate density estimation: theory, practice, and
visualization.” Wiley, 1992.
[29] A. Gramacki, Nonparametric Kernel Density Estimation and Its Compu-
tational Aspects. Cham, Switzerland: Springer International Publishing
AG, 2018.
[30] H. Jiang, “Uniform convergence rates for kernel density estimation,” in
Proceedings of the 34th International Conference on Machine Learning,
ser. Proceedings of Machine Learning Research, D. Precup and Y. W.
Teh, Eds., vol. 70. PMLR, 2017, pp. 1694–1703.
[31] P. Kairouz, S. Oh, and P. Viswanath, “Extremal mechanisms for local
differential privacy,” J. Mach. Learn. Res., vol. 17, no. 1, p. 492–542,
2016.
[32] C. Dwork and A. Roth, “The algorithmic foundations of differential
privacy,” Found. Trends Theor. Comput. Sci., vol. 9, no. 3–4, p. 211–407,
2014.
[33] M. Lyu, D. Su, and N. Li, “Understanding the sparse vector technique
for differential privacy,” Proc. VLDB Endow., vol. 10, no. 6, p. 637–648,
2017.
[34] F. McSherry and K. Talwar, “Mechanism design via differential privacy,”
in FOCS ’07, 2007.
[35] A. W. Knapp, “Basic real analysis.” Birkh¨auser, 2005.
[36] W. Forst and D. Hoffmann, “Optimization—theory and practice.”
Springer-Verlag New York, 2010.
[37] A. W. van der Vaart, “Asymptotic statistics.” Cambridge University
[38] J. J. Heckman and E. Leamer, “Handbook of econometrics, volume 5.”
Press, 1998.
Elsevier Science B.V., 2001.
[39] Y. M. Bishop, S. E. Fienberg, and P. W. Holland, “Discrete multivariate
analysis: Theory and practice.” Springer, 2007.
APPENDIX A
PROOFS AND TECHNICAL DETAILS
The appendix is dedicated to the mathematical details of our
analysis: the deﬁnition of stochastic convergence, additional
facts on the kernel K in KDE, as well as the proofs of
Proposition 1 and Theorem 2.
A. Stochastic Landau symbols and convergence in probability
Let (Zn)n∈N be a sequence of random variables and
(an)n∈N a sequence of positive, real numbers. We now say that
Zn = OP (an), if for every ε > 0 there exists a (sufﬁciently
large) C > 0 s.t.
n→∞ P(|Zn|/an ≥ C)  0
n→∞ P(|Zn|/an ≥ c) = 0.
lim
Finally we say that for a constant a ∈ R it holds that Zn →P a
if |Zn− a| = oP (1). We say that Zn →P ∞, if for any C > 0
n→∞ P(Zn ≥ C) = 1.
lim
For an extensive explanation of Landau symbols and conver-
gence see [39].
B. Kernel density estimation
(cid:5)
d → R≥0 with
Recall the deﬁnition of a kernel K as a continuous function
Rd K(u)du = 1. In our discussion,
K : R
we make the following two regularity assumptions, which are
taken from [30] (Assumptions 2 and 3):
(K1) K satisﬁes spherical symmetry, i.e. there exists a non-
increasing function k : R≥0 → R≥0, s.t. K(u) = k(|u|)
∀u ∈ R
i.e.
ρ, Cρ, t0, s.t. k(t) ≤ Cρ exp(−tρ), ∀t > t0.
(K2) k has exponentially decaying tails,
there exist
d.
A typical example of a kernel satisfying (K1) and (K2) is
the Gaussian kernel, which corresponds to the density function
of a standard normal and is given for d = 1 as K(t) =
exp(− t2
2π. We use this kernel in our experiments to
study continuous algorithms.
2 )/
√
C. Proof of Proposition 1
We only show the proposition for the case of a continuous
algorithm A and only for d = 1 (the case d > 1 is a
straightforward generalization). The discrete case works by
similar, but simpler techniques. Here, the central limit theorem
can be employed to establish a uniform convergence rate of
OP (n
−1/2) for the relative frequency estimator. By exploiting
the differentiability of the logarithm, this rate of convergence
can then be transferred to ˆx,x(cid:2). The second identity in the
discrete case follows as (cid:8)x,x(cid:2) (ˆt) = x,x(cid:2),C with probability
converging to one (which is not true in the continuous case).
In the following, we restrict ourselves to the case where
x,x(cid:2),C ∈ (0,∞). Proving consistency in the remaining cases
x,x(cid:2),C ∈ {0,∞} is easier and therefore omitted.
We begin by deﬁning two sets, that will be used extensively
in our subsequent discussion: the argmax of the loss function
M := arg max
and the closed ζ-environment of M
Uζ(M) := {t ∈ C : min
t(cid:2)∈M
t∈C
(cid:8)x,x(cid:2) (t)
|t − t
(cid:2)| ≤ ζ}.
Notice that M is non-empty and closed. To see this, consider a
sequence (tn)n∈N ⊂ C, such that (cid:8)x,x(cid:2) (tn) → supt∈C (cid:8)x,x(cid:2) (t).
in
Condition (C2) implies that
C, where the maximum is attained. In particular M (cid:3)= ∅.
Similarly, we can show that M is closed: If t is in the closure
of M, we can construct a sequence (tn)n∈N ⊂ M with tn → t
and by Condition (C2) it follows that t ∈ M.
there exists a limit point
We now formulate an auxiliary result, that is the main
stepping stone in the proof of Proposition 1.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
417
Lemma 1. Suppose that the assumptions of Proposition 1 hold
and x,x(cid:2),C ∈ (0,∞). Then the following statements hold:
(cid:11)
i) For any sufﬁciently small ζ > 0
(cid:9)(cid:12)
sup
t∈Uζ (M)
|ˆ(cid:8)x,x(cid:2) (t) − (cid:8)x,x(cid:2) (t)| = OP
(cid:9)
ii) There exists a κ = κ(ζ) > 0 s.t.
lim
n→∞ P
sup
t(cid:8)∈Uζ (M)
ˆ(cid:8)x,x(cid:2) (t) > sup
t∈C
ln(n)n
− β
2β+1
.
(cid:11)
= 0.
(cid:8)x,x(cid:2) (t) − κ
Let us verify that the Lemma indeed entails Proposition 1.
We ﬁrst show that for a small enough ζ > 0 it holds that
(cid:9)
(cid:11)
ˆt ∈ Uζ(M)
lim
n→∞ P
= 1.
(28)
To see this we notice that according to Lemma 1, part ii) there
exists a κ > 0, s.t.
sup
t(cid:8)∈Uζ (M)
ˆ(cid:8)x,x(cid:2) (t) ≤ sup
t∈M (cid:8)x,x(cid:2) (t) − κ + oP (1).
Here we have used supt∈M (cid:8)x,x(cid:2) (t) = supt∈C (cid:8)x,x(cid:2) (t). Com-
bining this with part i) of the lemma we have
sup
t(cid:8)∈Uζ (M)
ˆ(cid:8)x,x(cid:2) (t) ≤ sup
t∈M
ˆ(cid:8)x,x(cid:2) (t) − κ + oP (1).
As a consequence it holds with probability converging to 1,
that ˆ(cid:8)x,x(cid:2) does not attain its maximum in C \ Uζ(M) and
conversely that (28) holds. We now have for any t
− β
∗
∗
) − (cid:8)x,x(cid:2) (t
|ˆ(cid:8)x,x(cid:2) (t
)| =OP
|ˆ(cid:8)x,x(cid:2) (ˆt) − (cid:8)x,x(cid:2) (ˆt)| =OP
ln(n)n
2β+1
ln(n)n
− β
2β+1
,
(cid:11)
∗ ∈ M
(cid:11)
(29)
(cid:9)(cid:12)
(cid:9)(cid:12)
where we have used part i) of the Lemma and for the second
rate additionally (28). Now, the ﬁrst identity in Proposition 1