659
N/A
N/A
Errors
394 (7.5%)
6 (0.28%)
869 (9.5%)
528 (10%)
5 (0.23%)
523 (26%)
429 (8.3%)
5 (0.23%)
961 (32%)
Manifested Errors(cid:1)
Crash Failures
SIGNAL
334 (85%)
ASSERT
21 (5.3%)
5
782 (90%)
447 (85%)
4
501 (96%)
402 (94%)
5
927 (96%)
1
0
20 (3.8%)
1
0
12 (2.8%)
0
0
HANG
24 (6.1%)
0
32 (3.7%)
16 (3.0%)
0
3 (0.57%)
11 (2.6%)
0
VAL ERR
14 (3.6%)
0
55 (6.3%)
9 (1.7%)
0
8 (1.5%)
3 (0.70%)
0
9 (0.94%)
25(2.6%)
Fail Silence Violations
LEAD IMP
2 H FOLL
N/A
N/A
N/A
N/A
N/A
N/A
CATAST FAIL
1(cid:0) (0.25%)
0
0
27 (5.1%)
6 (1.1%)
3(cid:1) (0.57%)
0
11 (2.1%)
N/A
N/A
N/A
0
0
N/A
N/A
N/A
0
0
1(cid:0)(cid:0) (0.23%)
0
0
(cid:0) The error-manifestation ratio (i.e., ratio between manifested and injected errors) is shown in parentheses.
(cid:1) The percentage of the particular manifestation type with respect to the total number of manifested errors is shown in parentheses.
the voter to crash in the Ensemble’s point-to-point communica-
tion module (Pt2pt), but the injected replica did not crash.
LSA-leader experiments. Three catastrophic failures were
caused by errors originating from an Ensemble function used by
the LSA leader. (1) An error injected in the intra-group failures-
and-view module (Intra) of the leader caused the voter to have
an inconsistent group membership view with respect to other
replicas,13 which violates the properties of reliable group com-
munication. (2) An error injected in the connection management
module (Conn) of the leader caused the leader to hang and the
two followers to crash in their reliable, FIFO broadcast module
(Mnak). (3) An error injected in the Unsigned module of the
leader caused the two followers to crash in their Mnak module
and the voter to crash in its Pt2pt module.
LSA-follower experiments. An error injected in the Ensem-
ble’s function extern rec of the targeted follower caused
the voter and the other two replicas to raise an exception due
to a corrupted control ﬂow packet header. This function han-
dles the interaction between the the high-level part of Ensemble
(e.g., reliable communication algorithms) written in ML and the
low-level part (e.g., sockets) written in C.
5.2.2. Injections into Ensemble
To investigate further the sensitivity of the two algorithms to
catastrophic failures, a new set of text error injections was
performed targeting a speciﬁc function of Ensemble, ex-
tern rec (1.4K code size). This function was selected be-
cause it was heavily used and generated a large number of catas-
trophic failures. Results from Table 5 reinforce our conclusions
on the greater error sensitivity of LSA with respect to PDS:
the error-manifestation ratio is 43% for LSA and 30% for PDS,
while the manifested-to-activated error ratio is 87% for LSA and
82% for PDS. In a majority of catastrophic failures, the error
originating from the injected replica caused other replicas and/or
the voter to crash due to segmentation fault (25 cases for PDS
and 26 cases for LSA). In the remaining cases (16 for PDS and
14 for LSA), other replicas and/or the voter terminated due to
an Ensemble-generated exception (e.g., due to corrupted packet
header). In a large number of catastrophic failures (22 for PDS
and 30 for LSA), the injected replica did not crash.
In the previous experiments, we did not observe a statistical
difference, with respect to catastrophic failures, between LSA
and PDS. Therefore, a ﬁnal set of experiments was conducted
targeting, in an LSA leader replica, some of the Ensemble func-
tions that, during normal execution (i.e., when no view change
13This inconsistency caused to the voter to receive a message from a replica
that was not member of the group seen by the voter. On detecting this condition,
the voter terminated by raising an exception.
occurs), are used by an LSA leader replica but not by a PDS
replica. (Note that all Ensemble functions used by a PDS replica
are also used by an LSA leader replica). These functions were
selected from the Ensemble modules Addr (for network address
management), Conn, and Hot (Ensemble’s C interface to user
applications). The functions and their corresponding injection
results14 are presented in Table 6. The results indicate that a
signiﬁcant number of catastrophic failures originate from these
functions and, hence, that an LSA-based replicated system is
more likely to exhibit catastrophic failures than a PDS-based
replicated system. The voter did not fail in any of the catas-
trophic failures observed, which conﬁrms that these failures are
due to leader-to-follower communication.
5.3. Lesson Learned
Performance and failure analysis of the deterministic sched-
ulers PDS, LSA, and NPDS shows the following:
1. LSA strategy provides the best performance (in terms of
throughput and latency in response to client requests) at
the expense of availability (measured in terms of resilience
to errors). Because LSA relies on an inter-replica com-
munication channel for efﬁcient mutex acquisition schedul-
ing, LSA is more sensitive to the underlying communica-
tion layer’s fail silence violations than is PDS. This leads
to a larger number of catastrophic failures for LSA than
for PDS. If minimizing downtime is crucial (as for highly
available systems), PDS is a more appropriate choice than
LSA. If performance concerns have priority over minimiz-
ing downtime, then LSA can be preferred to PDS.
2. NPDS strategy provides correct execution through serial-
ization, which eliminates the beneﬁt of multithreading and
results in poor performance compared to PDS and LSA
strategies. Although we did not explicitly evaluate NPDS
dependability characteristics, we argue that they are simi-
lar to those of PDS, especially with regard to catastrophic
failures. This is because neither of the two algorithms uses
inter-replica communication.
6. Conclusions
Replication schemes by their nature impose a signiﬁcant per-
formance overhead. Measurements reported for several exist-
ing approaches to replication indicate performance overheads
ranging from three to ten times that of nonreplicated systems
[1–3, 25]. Until recently, only single-thread applications were
replicated, since multithreading does not easily conform to the
14Errors were injected (one per experiment) in each bit of each byte in the
portion of the text segment corresponding to the selected functions.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Total
Injected
Errors
1419
1419
PDS
LSA-L
Table 5. Text injections into Ensemble function extern rec.
Total
Manifested Errors
Total
Activated Manifested
Crash Failures
Errors
526
709
Errors
433
616
SIGNAL
ASSERT
HANG
VAL ERR
273
452
37
16
82
108
0
0
Fail Silence Violations
LEAD IMP
2 H FOLL
N/A
1
N/A
0
CATAST FAIL
41
40
Table 6. Text injections into Ensemble functions used by LSA replicas but not PDS replicas.
Total
Injected
Errors
3629
Total
Total
Activated Manifested
Crash Failures
Manifested Errors
Errors
2667
Errors
2191
SIGNAL
ASSERT
1589
356
HANG
200
VAL ERR
15
Fail Silence Violations
LEAD IMP
2 H FOLL
0
0
CATAST FAIL
31
state machine approach [18] widely used in software replica-
tion. However, if the replicas are multithreaded, then perfor-
mance overhead due to replication can be lessened. A simplistic
approach is pursued by nonpreemptive deterministic schedulers
(i.e., in Eternal [4] and in Transactional Drago [5]), which al-
though providing correct execution, do not exploit concurrency
in multithreaded replicas. In contrast, the Loose Synchroniza-
tion Algorithm (LSA) [6], proposed by the authors in an earlier
paper, captures the natural concurrency in a leader replica and
projects it on follower replicas through inter-replica communi-
cation. The Preemptive Deterministic Scheduler (PDS) algo-
rithm, proposed in this paper, removes the need for inter-replica
communication yet preserves a large degree of replica concur-
rency. The absence of inter-replica communication gives PDS
dependability advantages over LSA.
Acknowledgments
This work is supported in part by NSF grants CCR 00-86096
ITR and CCR 99-02026.
A. Appendix
This section discusses how to enforce the assumption that
the same thread/mutex ids are associated with corresponding
threads/mutexes of different replicas (see (cid:0) 3). If, on all repli-
cas, corresponding threads/mutexes are created/initialized by
the same thread and in the same order in the context of this
thread, then a hierarchical thread/mutex naming scheme can be
employed as follows.
The logical thread id t of a thread is recursively deﬁned as the
sequence t(cid:0) (cid:0) tcct(cid:0), where t(cid:0) is the logical id of t’s parent and
tcct(cid:0) is the value of a thread-creation counter owned by t(cid:0) at
the time of t’s creation. This counter is incremented each time t(cid:0)
spawns a new child thread. By convention, the logical thread id
of the application main thread is (cid:0)(cid:2)(cid:1).
The logical mutex id m of a mutex is given by the pair
t(cid:0) mcct, where t is the logical id of the thread that creates
the mutex, and mcct is the value of a mutex creation counter
owned by t at the time of the mutex creation. This counter is
incremented each time t creates a new mutex.
References
[1] M. Cukier et al. AQuA: An adaptive architecture that provides
dependable distributed objects. In Proc. of Int’l Symp. on Reliable
Distributed Systems, pages 245–253, 1998.
[2] S. Pleisch and A. Schiper. FATOMAS: A fault-tolerant mobile
agent system based on the agent-dependent approach. In Proc.
of Int’l Conf. on Dependable Systems and Networks, pages 215–
224, 2001.
[3] G. D. Parrington et al. The design and implementation of Arjuna.
Computing Systems, 8(2):255–308, 1995.
[4] L. E. Moser, P. M. Melliar-Smith, and P. Narasimhan. Consistent
object replication in the Eternal system. Theory and Practice of
Object Systems, 4(2):81–92, 1998.
[5] R. Jimenez-Peris, M. Patino-Martinez, and S. Arevalo. Deter-
ministic scheduling for transactional multithreaded replicas. In
Proc. of Int’l Symp. on Reliable Distributed Systems, 2000.
[6] C. Basile et al. Loose synchronization of multithreaded replicas.
In Proc. of Int’l Symp. on Reliable Distributed Systems, 2002.
[7] A. Goldberg et al. Transparent recovery of Mach applications. In
Usenix Mach Workshop, pages 169–183, 1990.
[8] M. Hayden. The Ensemble System. PhD thesis, Dept. of Com-
puter Science, Cornell University, USA, 1997.
[9] A. Borg et al. Fault tolerance under UNIX. ACM Trans. on Com-
puter Systems, 7(1):1–24, 1989.
[10] T. C. Bressoud and F. B. Schneider. Hypervisor-based fault tol-
erance. ACM Trans. on Computer Systems, 14(1):80–107, 1996.
[11] P. A. Barrett et al. The Delta-4 extra performance architecture
(XPA). In FTCS-20, pages 481–488, 1990.
[12] D. Powell et al. GUARDS: A generic upgradable architecture
for real-time dependable systems. IEEE Trans. on Parallel and
Distributed Systems, 10(6):580–599, 1999.
[13] O. Babaoglu and K. Marzullo. Distributed Systems, pages 55–96.
Addison-Wesley, 1993.
[14] E. Elnozahy, D. Johnson, and Y. Wang. A survey of rollback-
recovery protocols in message-passing systems. Technical report,
Carnegie Mellon University, 1996.
[15] C. Basile, Z. Kalbarczyk, and R. Iyer. A preemptive determin-
istic scheduling algorithm for multithreaded replicas. Techni-
cal report, University of Illinois at Urbana-Champaign, 2003.
http://www.uiuc.edu/(cid:0)cbasile/papers.
[16] G. Holzmann. The SPIN model checker. IEEE Trans. on Software
Engineering, 23(5):279–295, 1997.
[17] F. Cristian et al. Atomic broadcast: From simple message dif-
Information and Computation,
fusion to byzantine agreement.
118(1):158–179, 1995.
[18] F. B. Schneider. Implementing fault-tolerant services using the
state machine approach: A tutorial. ACM Computing Surveys,
22(4):299–319, 1990.
[19] K. Whisnant et al. An experimental evaluation of the REE SIFT
environment for spaceborne applications. In Proc. of Int’l Conf.
on Dependable Systems and Networks, pages 585–595, 2002.
[20] D. Stott et al. Dependability assessment in distributed systems
In Proc. of Int’l
with lightweight fault injectors in NFTAPE.
Computer Performance and Dependability Symposium, 2000.
[21] E. Fuchs. Validating the fail-silence assumption of the MARS
architecture. In Proc. of 6th Dependable Computing for Critical
Applications Conference, pages 225–247, 1998.
[22] H. Madeira and J.G.Silva. Experimental evaluation of the fail-
silent behavior in computers without error masking. In Proc. of
Int’l Symp. on Fault-Tolerant Computing, pages 350–359, 1994.
[23] M. Rimen, J. Ohlsson, and J. Torin. On microprocessor error be-
havior modeling. In Proc. of Int’l Symp. on Fault-Tolerant Com-
puting, 1994.
[24] F. V. Brasileiro et al.
Implementing fail-silent nodes for dis-
tributed systems. IEEE Trans. on Computers, 45(11):1226–1238,
1996.
[25] R. Guerraoui et al. System support for object groups. In ACM
Conf. on Object-Oriented Programming Systems, Languages and
Applications, 1998.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE