This is just the cache string
This is just the second cache string
4.为作业指定附加配置参数
用户可以使用“-jobconf＜n＞=＜v＞”增加一些配置变量。例如：
$HADOOP_HOME/bin/Hadoop jar$HADOOP_HOME/Hadoop-streaming.jar\
-input myInputDirs\
-output myOutputDir\
-mapper org.apache.Hadoop.mapred.lib.IdentityMapper\
-reducer/bin/wc\
-D mapred.reduce.tasks=2
在上面的例子中，-jobconf mapred.reduce.tasks=2表明用两个Reducer完成作业。
关于jobconf参数的更多细节可以参考Hadoop安装包中的Hadoop-default.html文件。
5.其他选项
Streaming命令的其他选项如表18-3所示。
使用-cluster＜name＞实现“本地”Hadoop和一个或多个远程Hadoop集群间的切换。默认情况下，使用Hadoop-default.xml和Hadoop-site.xml。当使用-cluster＜name＞选项时，会使用$HADOOP_HOME/conf/Hadoop-＜name＞.xml。
下面的选项可改变temp目录：
-D dfs.data.dir=/tmp
下面的选项指定其他本地temp目录：
-D mapred.local.dir=/tmp/local
-D mapred.system.dir=/tmp/system
-D mapred.temp.dir=/tmp/temp
在streaming命令中设置环境变量：
-cmdenv EXAMPLE_DIR=/home/example/dictionaries/
18.3.2 Hadoop Streaming的使用举例
Hadoop Streaming插件是Hadoop安装包当中的一个JAR文件，具体位置在……\Hadoop-1.0.1\contrib\streaming目录下，所以Hadoop Streaming插件是直接使用的，只需要在执行Hadoop程序时输入命令Hadoop Streaming就可以了，无须安装，在编写MapReduce程序时，只要按照整个框架要求并根据自己的需要编写出符合对应语言格式的程序，然后用下面的命令格式将程序提交给Hadoop就可以了：
$HADOOP_HOME/bin/hadoop jar$HADOOP_HOME/hadoop-streaming.jar\
-input myInputDirs\
-output myOutputDir\
-mapper/bin/cat\
-reducer/bin/wc
需要注意的是，程序执行所需要的支持文件也要在提交程序的同时提交到Hadoop集群，这在前面已有说明，不再赘述。下面以一个用PHP语言编写的WordCount使用Hadoop Streaming提交的程序为例，来说明此插件使用方法（Linux系统下需要安装PHP环境，命令为sudo apt-get install php5-client）。
程序代码举例如下所示。
（1）Mapper. php
#！/usr/bin/php
＜?php
$word2count=array（）；
//标准输入STDIN（standard input）
while（（$line=fgets（STDIN））！==false）{
//移除小写与空格
$line=strtolower（trim（$line））；
//切词
$words=preg_split（'/\W/'，$line，0，PREG_SPLIT_NO_EMPTY）；
//将字+1
foreach（$words as$word）{
$word2count[$word]+=1；
}
}
//结果写到STDOUT（standard output）
foreach（$word2count as$word=＞$count）{
echo$word, chr（9），$count, PHP_EOL；
}
?＞
（2）Reduce.php
#！/usr/bin/php
＜?php
$word2count=array（）；
//输入为STDIN
while（（$line=fgets（STDIN））！==false）{
//移除多余的空白
$line=trim（$line）；
//每一行的格式为（字"tab"数字），记录到（$word，$count）
list（$word，$count）=explode（chr（9），$line）；
//转换格式string-＞int
$count=intval（$count）；
//求总的频数
if（$count＞0）$word2count[$word]+=$count；
}
//此行非必要内容，但可让output排列更完整
ksort（$word2count）；
//将结果写到STDOUT（standard output）
foreach（$word2count as$word=＞$count）{
echo$word, chr（9），$count, PHP_EOL；
}
?＞
执行情况如下：
$bin/Hadoop jar contrib/streaming/Hadoop-0.20.2-streaming.jar\
-mapper/opt/Hadoop/mapper.php-reducer/opt/Hadoop/reducer.php-input lab4_input
-output stream_out2
下面来查看一下结果：
$bin/Hadoop dfs-cat stream_out2/part-00000
18.3.3 使用Hadoop Streaming常见的问题
1.如何处理多个文件，其中每个文件一个Map？
需要处理多个文件时，用户可以采用多种途径，这里以在集群上压缩（zipping）多个文件为例，用户可以使用以下几种方法：
（1）使用Hadoop Streaming和用户编写的mapper脚本程序。
先生成一个文件，文件中包含所有要压缩的文件在HDFS上的完整路径。每个Map任务获得一个路径名作为输入。
然后创建一个Mapper脚本程序，实现如下功能：获得文件名，把该文件复制到本地，压缩该文件并把它发到期望的输出目录中。
（2）使用现有的Hadoop框架
在main函数中添加如下命令：
FileOutputFormat.setCompressOutput（conf, true）；
FileOutputFormat.setOutputCompressorClass（conf, org.apache.Hadoop.io.compress.
GzipCodec.class）；
conf.setOutputFormat（NonSplitableTextInputFormat.class）；
conf.setNumReduceTasks（0）；
编写Map函数：
public void map（WritableComparable key, Writable value，
OutputCollector output，
Reporter reporter）throws IOException{
output.collect（（Text）value, null）；
}
注意输出的文件名和原文件名不同。
2.如果在Shell脚本里设置一个别名，并放在-mapper之后，Streaming会正常运行吗？例如，alias cl='cut-fl'，-mapper"cl"会运行正常吗？
脚本里是无法使用别名的，但是允许变量替换，例如：
$Hadoop dfs-cat samples/student_marks
alice 50
bruce 70
charlie 80
dan 75
$c2='cut-f2'；$HADOOP_HOME/bin/Hadoop jar$HADOOP_HOME/Hadoop-streaming.jar\
-input/user/me/samples/student_marks
-mapper\"$c2\"-reducer'cat'
-output/user/me/samples/student_out
-jobconf mapred.job.name='Experiment'
$Hadoop dfs-ls samples/student_out
Found 1 items/user/me/samples/student_out/part-00000＜r 3＞16
$Hadoop dfs-cat samples/student_out/part-00000
50
70
75
80
3.在Streaming作业中用-file选项运行一个分布式的超大可执行文件（例如，3.6GB）时，如果得到错误信息“No space left on device”如何解决？
由于配置变量stream.tmpdir指定了一个目录，会在这个目录下进行打jar包的操作。stream.tmpdir的默认值是/tmp，用户需要将这个值设置为一个有更大空间的目录：
-D stream.tmpdir=/export/bigspace/……
4.如何设置多个输入目录？
可以使用多个-input选项设置多个输入目录：
Hadoop jar Hadoop-streaming.jar-input'/user/foo/dir1'-input'/user/foo/dir2'
5.如何生成gzip格式的输出文件？
除了纯文本格式的输出，用户还可以让程序生成gzip文件格式的输出，只需将Streaming作业中的选项设置为“-D mapred.output.compress=true-jobconf mapred.output.compression.codec=org.apache.Hadoop.io.compress.GzipCode”。
6.在Streaming中如何自定义input/output format？
在Hadoop 0.14版本以前，不支持多个jar文件。所以当指定自定义的类时，用户需要把它们和原有的streaming jar打包在一起，并用这个自定义的jar包替换默认的Hadoop streaming jar包。在0.14版本以后，就无须打包在一起，只需要正常的编译运行。
7.Streaming如何解析XML文档？
用户可以使用StreamXmlRecordReader来解析XML文档，如下所示：
Hadoop jar Hadoop-streaming.jar-inputreader"StreamXmlRecord, begin=BEGIN_
STRING, end=END_STRING"……
Map任务会把BEGIN_STRING和END_STRING之间的部分看做一条记录。
8.在Streaming应用程序中如何更新计数器？
Streaming进程能够使用stderr发出计数器信息。应该把reporter：counter：＜group＞，＜count er＞，＜amount＞发送到stderr来更新计数器。
9.如何更新Streaming应用程序的状态？
Streaming进程能够使用stderr发出状态信息。可把reporter：status：＜message＞发送到stderr来设置状态。
18.4 Hadoop Libhdfs的介绍和使用
 18.4.1 Hadoop Libhdfs的介绍
Libhdfs是一个基于C编程接口的为Hadoop分布式文件系统开发的JNI（Java Native Interface），它提供了一个C语言接口以结合管理DFS文件和文件系统。并且它会在${HADOOP_HOME}/libhdfs/libhdfs.so中预编译，它是Hadoop分布式结构中的一部分。
18.4.2 Hadoop Libhdfs的安装配置
在安装Libhdfs之前首先需要安装Hadoop的分布式文件系统HDFS。当用户有一个正在运行的工作集时，进入src/c++/libhdfs目录，使用makefile文件安装Libhdfs。一旦安装Libhdfs成功，用户可以通过它连接到自己的程序。
18.4.3 Hadoop Libhdfs API简介