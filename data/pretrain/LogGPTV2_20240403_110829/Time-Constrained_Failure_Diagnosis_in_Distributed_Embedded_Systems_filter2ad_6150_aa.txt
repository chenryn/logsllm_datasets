title:Time-Constrained Failure Diagnosis in Distributed Embedded Systems
author:Nagarajan Kandasamy and
John P. Hayes and
Brian T. Murray
Time-Constrained Failure Diagnosis in Distributed Embedded Systems†
Nagarajan Kandasamy*, John P. Hayes*, and Brian T. Murray**
*EECS Department
University of Michigan, Ann Arbor, MI 48109, U.S.A
**Delphi Automotive Systems
Brighton, MI 48116, U.S.A
Abstract
Advanced automotive control applications such as steer
and brake-by-wire are typically implemented as distributed
systems comprising many embedded processors, sensors,
and actuators interconnected via a communication bus.
They have severe cost constraints but demand a high level
of safety and performance. Motivated by the need for
timely diagnosis of faulty actuators in such systems, we
present a general method to implement failure diagnosis
under deadline and resource constraints. Actuators are
diagnosed in distributed fashion by processors to provide a
global view of their fault status. The diagnostic tests are
implemented in software using analytical redundancy and
execute concurrently with the control tasks. The proposed
method solves the test scheduling problem using a static
list-based approach which guarantees actuator diagnosis
within designer-speciﬁed deadlines while meeting control
performance goals. As a secondary objective,
it also
minimizes the number of required processors. We present
simulation results evaluating the effectiveness of
the
proposed method under various design constraints.
1
Introduction
The safety of distributed embedded systems such as
those in cars depends on the proper functioning of the cor-
responding hardware and software components. Steer-by-
wire (SBW) is a typical example where the hydraulic steer-
ing is replaced by a microprocessor-controlled and net-
worked electro-mechanical system [6]. The steering-wheel
angle is sensed and sent to processors where the desired
road-wheel position is calculated and applied via electro-
mechanical actuators at the wheels. Such systems typically
comprise multiple processors sharing a communication
bus with limited bandwidth, and have severe cost con-
straints. Also, the control applications executing on these
systems demand a high level of safety and performance.
We address fault diagnosis under resource and deadline
constraints in distributed embedded systems of the forego-
ing kind. This problem is motivated by the need for timely
actuator failure diagnosis in control applications such as
†This research was supported in part by a contract from Delphi
Automotive Systems.
steer-by-wire. A faulty SBW actuator may cause undesir-
able vehicle-level behavior such as unintended steer [1].
Though physical redundancy in the form of replicated
actuator components masks failures, it also adds to the dol-
lar cost, weight, and power consumption of embedded sys-
tems. A low-cost alternative is analytical or model-based
redundancy where software-implemented diagnostic tasks
compare discrepancies between actual actuator behavior
and the one predicted by its mathematical model [3].
In low-cost systems, diagnostic tasks share processors
with control tasks rather than execute on dedicated “moni-
tor” processors. Control tasks must continue to meet speci-
fied performance goals and deadlines in spite of the
diagnosis overhead. Also, faulty actuators must be identi-
fied and shut down before the system becomes unsafe.
Therefore, the problem of diagnostic-test scheduling must
be addressed and solved.
Previous research [2] has addressed distributed proces-
sor diagnosis under the assumption that processors test
each other and exchange test results to identify failures.
Diagnosis is performed either in dedicated testing rounds
or concurrently during normal system operation. Most pro-
posals assume a fault model and a specific testing scheme,
and identify the appropriate conditions for system diagnos-
ability. They do not consider the test scheduling problem
explicitly. The authors of [8] [9] propose a comparison-
based diagnosis method in which an on-line scheduler exe-
cutes multiple copies of each task using spare processing
capacity available in the system. The results are then com-
pared to diagnose processors. Fault diagnosis is best-effort
in nature and depends on the number of task copies com-
pleting successfully. Therefore, dynamic factors such as
system load and task arrival rate affect diagnosability.
Actuator diagnosis is also related to on-line monitoring
where system behavior is checked against a corresponding
reference model. The monitored behavior may include tim-
ing [21] and safety [20] constraints. To ensure timely fail-
ure detection in safety-critical systems, these monitors
must be integrated with other application tasks.
Self-validating actuators use model-based methods for
self-diagnosis and broadcast their status to the rest of the
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:42 UTC from IEEE Xplore.  Restrictions apply. 
Sensors
S1
...
Sk
Processors
P1
Pk
...
Actuators
A1
... Ak
Communication network
(a)
Actuator controller
Sense1
.
.
.
Sensen
Compute1
.
.
.
Computen
(b)
Vote &
actuate
Figure 1. (a) The distributed system model and (b)
the fault-tolerant control application
system [18]. However, sophisticated models may require
substantial computing power not generally available in
individual actuators. On the other hand, the distributed
diagnosis method proposed here uses diverse models exe-
cuting on multiple processors to identify faulty actuators.
This tolerates faults during the diagnostic process itself
and increases confidence in the corresponding decisions.
However, the inter-processor communication overhead
increases the fault detection latency.
We assume a two-phase distributed diagnosis scheme
defined as follows. During phase 1, multiple processors
P1,..., Pk independently evaluate the behavior of an actua-
tor Ai. These local decisions are then consolidated via a
suitable agreement algorithm during phase 2 to obtain a
global view of Ai’s status. Diagnosis is concurrent with
normal system operation and tolerant of failures. We also
solve the test scheduling problem guaranteeing actuator
diagnosis within speciﬁed deadlines while meeting control
performance goals under the following assumptions:
• Processors exchange messages via a broadcast
network and incur communication delay.
• Actuators may deliver erroneous results
to the
physical system due to electro-mechanical failures.
• Diagnostic tests are implemented in software using
analytical redundancy.
• A static (off-line) scheduling step maps tasks to
processors.
As a secondary objective,
required
processors is minimized. We also present simulation
results evaluating the effectiveness of the proposed method
under various design constraints.
the number of
The rest of this paper is organized as follows. Section 2
discusses the modeling assumptions and the proposed
Checker
1
s j
Model
Mi
ei
Generate
residue
ai
sj
Figure 2. A model-based checker for actuator Ai
diagnostic approach. We discuss the test scheduling prob-
lem in Section 3 and the derivation of diagnosis deadlines
in Section 4. We present simulation results in Section 5 and
conclude with a discussion of future work in Section 6.
2 Preliminaries
This section presents the system and fault models
assumed in this paper and describes the diagnostic
approach. The test scheduling problem is also formulated.
2.1 System and Fault Model
Figure 1(a) shows the system model comprising sensors
{Si}, processors {Pi}, and actuators {Ai} interconnected by
a broadcast network such as a shared bus. Each actuator Ai
and sensor Si is housed with a microcontroller of limited
computing power and a network interface. Processors
exchange messages via a deterministic communication
protocol such as time-division multiple access (TDMA)
having a bounded worst-case delivery latency. The time-
triggered protocol (TTP) [5] and FlexRay [15] are exam-
ples of TDMA-based protocols where a dedicated co-pro-
cessor handles communication without interfering with
program execution on Pi.
Processors, sensors, and actuators may suffer a bounded
number of operational failures. Permanent faults persist
until corrective action is taken while transient faults appear
and disappear quickly and at random times. Figure 1(b)
shows a typical fault-tolerant configuration for a periodic
control application. Assuming an upper bound k on proces-
sor (sensor) failures, at least 2k + 1 tasks compute the
desired actuation command, and a voting task determines
the final output [17]. The control tasks obtain input data
from replicated and possibly diverse sensors. For example,
both analog and digital sensors can be used to measure the
steering-wheel angle in the SBW system.
The application in Fig. 1(b) is mapped on to the physi-
cal architecture in Fig. 1(a) as follows. The redundant
sensing and control tasks “Sensei” and “Computei” execute
on Si and Pi, respectively. The “vote & actuate” task exe-
cutes on Ai to vote on the generated commands and issue
the final result that controls the actuator. Faults affecting
this task may result in incorrect actuation outputs being
sent to the controlled process. These fall under the heading
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:42 UTC from IEEE Xplore.  Restrictions apply. 
Sense1
.
.
.
Sensen
Compute1
.
.
.
Computen
Actuator control
Vote &
actuate
Physical
system
Monitoring
sensors
.
.
.
e1
Check1
.
.
.
e1
en
Checkn
e
n
Vote1
.
.
.
Voten
Actuator diagnosis
Figure 3. An integrated approach to fault-tolerant control and distributed actuator diagnosis
of electro-mechanical failures affecting the actuator Ai and
are diagnosed as such.
2.2 Distributed Actuator Diagnosis
1
s j
Figure 2 shows a software-implemented checker for
actuator Ai executed on a system processor Pj. The basic
checking scheme uses a mathematical model Mi describing
the fault-free behavior of actuator Ai for a given command
ai [3]. The corresponding change in the controlled physical
system is monitored by sensor Sj whose output sj is com-
pared to Mi’s prediction
to generate the residue or error
ei. The sensor Sj measures the monitored signal after a
sampling delay ts(sj) required to fully register the state
change caused by Ai. The control application in Fig. 1(b) is
augmented with these checkers or diagnostic tasks result-
ing in the integrated control and diagnosis approach
depicted in Fig. 3. Distributed diagnosis is performed in
two phases. First, multiple checkers independently evalu-
ate Ai’s behavior. These local decisions are then consoli-
dated via an agreement algorithm to obtain the final
diagnosis. Similar two-phase diagnosis schemes have been
proposed to identify faulty processors [10].
The actuation commands generated by the control
application provide the periodic test stimuli for diagnosis.
Sensors {Sj} monitor Ai’s response to the corresponding
(voted) actuation command. During phase 1, checkers exe-
cuting on multiple processors compare Ai’s actual and pre-
dicted behavior to generate the corresponding residues
{ei}. Depending on the diversity of the measurements
obtained from various monitoring sensors, different physi-
cal laws may describe Ai’s behavior [13]. Therefore,
checkers may be implemented using data diversity during
phase 1 to improve the robustness of fault diagnosis.
During phase 2, processors exchange the locally gener-
ated residues. Since the checkers employ design and/or
data diversity, these residues may differ slightly from each
other and yet be correct. Therefore, each processor uses an
approximate agreement algorithm to obtain a voted residue
and evaluates it against an a priori defined threshold to
diagnose Ai. We assume fail-consistent failures where all
fault-free processors perceive Pi’s failure uniformly [4]. A
suitable agreement algorithm under this fault model is the
median voter which selects a middle value from an odd
number of residues by eliminating those residue pairs dif-
fering by the greatest amount [7]. At the end of phase 2, all
fault-free processors correctly identify Ai’s status.
2.3 Problem Statement
T i
T j
wij
→
The overall control and diagnosis approach shown in
Fig. 3 is modeled as a directed acyclic task graph Gi con-
sisting of vertices and edges representing tasks and prece-
dence constraints, respectively. Each vertex is labeled
Ti(ci), where Ti is the task and ci its worst-case execution
time. The precedence relation between Ti and Tj is shown
by
. The edge weight wij represents inter-processor
communication cost or the sampling delay of the moni-
tored signals. Tasks having no predecessors are termed
entry tasks while those without successors are exit tasks.
Figure 4 shows a task graph G1 where both actuator con-
trol and diagnosis are realized by triple-modular redun-
dancy. The inter-processor communication delay is 1000
µs. Task T7 commands the actuator while T8, T9, and T10
measure different monitored signals with sampling delays
of 1000, 800, and 500 µs, respectively. The shaded vertices
represent buffering tasks and are discussed in more detail
in Section 3. The task graph G1 has the following designer-
speciﬁed timing constraints:
Control delay: The control delay is the time elapsed
between reading the sensors and commanding the actu-
ator, i.e., the response time of the control application.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:42 UTC from IEEE Xplore.  Restrictions apply. 
Actuator control
Actuator
monitoring
Model-based
 evaluation
T1
(250)
T2
(200)
T3
(175)
T4
(450)
T5
(425)
T6
(400)
T7
(300)
T17
(50)
wij = 1000 µs
T18
(50)
wij = 800 µs
T19
(50)
wij = 500 µs
T8
(150)
T9
(100)
T10
(120)
T11
(500)
T12
(480)
T13
(450)
T20
(50)
T21
(50)
T22
(50)
Voting
T14
(300)
T15
(350)
T16
(300)
T23
(50)
T24
(50)
T25
(50)
Communication delay (wij = 1000 µs)
Sampling delay
Communication delay (wij = 1000 µs)
Figure 4. Task graph G1 corresponding to the integrated control and diagnosis method of Fig. 3 with the
buffering tasks shown as shaded vertices
This delay requirement determines the graph period φ.
Diagnosis latency: As shown in Fig. 4, actuator diagno-
sis involves on-line monitoring, evaluation, and agree-
ment. A faulty actuator Ai must be diagnosed within td
time units after issuance of the corresponding actuation
command ai. The diagnosis latency td is a relative tim-
ing constraint derived from system-level safety require-
ments (see Section 4).
The delay and diagnosis latency constraints impose end-to-
end timing requirements on the actuator control and
respectively. Once these
diagnosis portions of G1,
requirements
are known,
each
the deadline dj of
intermediate task Tj within the respective portions is
derived [19].
The embedded system is assumed to implement multi-
ple graphs of the type shown in Fig. 4. The problem state-
ment can be summarized as: Given task graphs G1,..., Gk,
obtain a feasible task schedule for each Gi satisfying both
performance and diagnosability constraints. As a second-
ary objective, minimize the number of processors required.
We prefer static scheduling for distributed embedded sys-
tems since it ensures safety properties such as timing pre-
dictability, design simplicity, and testability [4].
3 Diagnostic-Test Scheduling
We solve the diagnostic-test scheduling problem assum-
ing a system where all applications have the same delay,
i.e., every task graph Gi has the same period φ. An example
of such a control system is drive-by-wire where both steer-
and brake-by-wire have identical performance require-
ments. Given graphs to be mapped onto multiple proces-
sors, we generate a frame Fi on each processor Pi
specifying the release (start) time ri and finish time fi for
each task Ti allocated to Pi. Each Fi has a duration equal to
φ and executes periodically Fi(1), Fi(2),..., where Fi(k)