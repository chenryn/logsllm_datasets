title:ELISE: A Storage Efficient Logging System Powered by Redundancy
Reduction and Representation Learning
author:Hailun Ding and
Shenao Yan and
Juan Zhai and
Shiqing Ma
ElisE: A Storage Efficient Logging System Powered by 
Redundancy Reduction and Representation Learning
Hailun Ding, Shenao Yan, Juan Zhai, and Shiqing Ma, Rutgers University
https://www.usenix.org/conference/usenixsecurity21/presentation/ding
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.ELISE: A Storage Efﬁcient Logging System Powered by Redundancy Reduction
and Representation Learning
Hailun Ding
Rutgers University
Shenao Yan
Rutgers University
Juan Zhai
Rutgers University
Shiqing Ma
Rutgers University
Abstract
Log is a key enabler of many security applications including
but not limited to security auditing and forensic analysis. Due
to the rapid growth of modern computing infrastructure size,
software systems are generating more and more logs every
day. Moreover, the duration of recent cyber attacks like Ad-
vanced Persistent Threats (APTs) is becoming longer, and
their targets consist of many connected organizations instead
of a single one. This requires the analysis on logs from differ-
ent sources and long time periods. Storing such large sized log
ﬁles is becoming more important and also challenging than
ever. Existing logging systems are either inefﬁcient (i.e., high
storage overhead) or designed for limited security applications
(i.e., no support for general security analysis). In this paper,
we propose ELISE, a storage efﬁcient logging system built
on top of a novel lossless data compression technique, which
naturally supports all types of security analysis. It features
lossless log compression using a novel log ﬁle preprocessing
and Deep Neural Network (DNN) based method to learn op-
timal character encoding. On average, ELISE can achieve 3
and 2 times better compression results compared with exist-
ing state-of-the-art methods Gzip and DeepZip, respectively,
showing a promising future research direction.
1 Introduction
Log is a valuable source for many security applications such
as forensic analysis [33, 41, 42, 50], system auditing [48, 66],
Denial of Service (DoS) detection [13, 54] and intrusion de-
tection [21, 23, 26, 27, 45]. In many scenarios like forensic
analysis, the attacker has already left the system before anal-
ysis, and log is the only information that we can leverage to
backtrack the attack ﬁngerprints and understand the attack
consequences [33]. During the last years, modern attacks like
Advanced Persistent Threats (APTs) are becoming more and
more frequent. In these attacks, the adversary can maintain se-
cret access to highly conﬁdential systems for a long time [56].
Moreover, APT groups tend to attack a few connected or as-
sociated targets together to gain more proﬁtable information.
For example, in the recent 2020 United States federal gov-
ernment data breach [14], attackers started to compromise
the supply chain before October 2019, and the attack was
not acknowledged until December 2020. It is suspected that
attacks maintained secret access and performed data breach
for over 8 months. This attack targeted over 10 U.S. federal,
state and local governments, and 15 private sectors, including
these that have well-trained employees and state-of-the-art
(SOTA) defense techniques and products, such as Department
of Defense and security ﬁrms like Palo Alto Networks. Per-
forming log based analysis on such sized attacks requires
examining a huge amount of data because of the large number
of involved parties and long duration of the attack. Moreover,
daily used programs generate a huge amount of data every day.
According to previous studies [25, 29, 37], a single end user
computer generates GBs log every day. Servers generate even
larger sized log because of heavier workloads [63]. As such,
storing logs is important and essential for security analysis,
but also challenging because of huge storage overheads for
large enterprises and organizations [26, 37, 63].
There are two existing mainstream methods to reduce the
storage overhead. One is to directly remove redundant infor-
mation from the log, and the other one is to compress log to
reduce log ﬁle size. Many existing approaches [29, 37, 42, 63]
proposed a set of rules to identify redundant events in logs
and remove them without affecting the analysis result. How-
ever, these methods limit the analysis that can be applied on
the log [29, 63]. For many security analyses, it is hard or even
impossible to deﬁne what is redundant. In dependence based
security analysis, examples of redundant events are repeated
read or write system calls on the same system objects, e.g.,
a socket. While Machine Learning (ML) based methods iden-
tify possible DoS attacks by analyzing the frequency of read
and write system calls to certain sockets. In summary, redun-
dant events in one security analysis are no longer redundant in
another scenario. As such, data reduction is not general to all
downstream applications. Similarly, lossy data compression is
not acceptable either, because it can lose critical information
required by some security analysis. In conclusion, to provide
USENIX Association
30th USENIX Security Symposium    3023
general service to various types of security analysis and re-
duce the storage overhead at the same time, it is essential to
perform lossless data compression.
Traditional lossless data compression solutions [20, 30, 68]
usually perform rule based processing. For example, the most
commonly used compression method, Gzip [20], uses the
LZ77 algorithm [68] and Huffman encoding [30] to compress
ﬁles. Such methods can capture certain redundancy in the
data, but they are usually not optimal. Recently, ML based
data compression has been proposed [8, 43, 55, 61], and bene-
ﬁting from the recent advances in DNN research, they [6, 19]
have achieved lower compression ratios1. DNNs can better
estimate the character distribution and catch the redundancy
in given contexts compared to methods like Gzip [6,19]. As a
result, it can generate shorter encodings to represent the same
data using less space. However, existing DNN based compres-
sion methods such as DeepZip [19], have a few drawbacks in
compressing log ﬁles. Firstly, training DNN models is quite
challenging. Logs contain natural language (NL) tokens, and
training models for such tasks is well known to be hard [53].
SOTA models are huge, difﬁcult to train, and cost a lot of re-
sources. Secondly, existing methods cannot fully disclose the
redundancy of log ﬁles [6, 19]. Different from a general NL
artifact, log entries are well formatted, and hence, much con-
textual information is hidden. This causes extra difﬁculty in
extracting the redundancy and compressing them for methods
like DeepZip.
In this paper, we propose ELISE (Efﬁcient LoggIng Sys-
tEm), a storage efﬁcient logging system. It combines redun-
dancy reduction and representation learning to fully uncover
the redundancy in logs, and produces optimally sized log
ﬁles. It creates a dictionary (referred as a reference table)
to memorize structural redundancy in logs, and converts NL
artifacts to numerical representations to simplify and speed
up the process of training an encoder. After that, it leverages
the trained encoder and arithmetic encoding to create the
optimal representation in binary string format, which takes
the minimal space to store. By doing so, ELISE can achieve
lower compression ratios compared with existing methods.
Our prototype is evaluated on various sized log ﬁles from
ﬁve different systems including Linux, Windows, Apache,
MySQL, and FreeBSD. One highlighted result is that ELISE
achieves 9 times better compression ratio on HTTP logs com-
pared with the traditional method Gzip. Moreover, it improves
the runtime of DeepZip by a factor of 6.
In summary, we make the following contributions:
• We perform a thorough analysis of existing logging sys-
tems, and identify their limitations. They are either de-
signed for a limited number of security analysis applica-
tions or storage inefﬁcient (i.e., high storage overhead).
1Compression ratio is deﬁned as the compressed ﬁle size over the original
ﬁle size. The smaller, the better.
• We identify structural and contextual redundancies in
log ﬁles, and propose a novel lossless log compression
technique by using redundancy reduction and optimized
encoder. It is more effective at capturing redundancies
in logs by leveraging a novel preprocessing process and
learning a high quality DNN encoder. It also optimizes
the efﬁciency by converting NL artifacts to numerical
representations.
• We build a prototype ELISE based on our proposed idea,
and our results show that on average, ELISE outperforms
existing methods, Gzip and DeepZip by 1.84 times in
terms of compression ratios, and reduces the time cost
by 5.63 times compared with methods in its kind.
Roadmap: In Section 2, we provide the background knowl-
edge of log reduction and compression, a motivating example
to show the limitations of existing work and a comparison of
different methods including ours. Section 3 presents the de-
sign of ELISE, our storage efﬁcient system. Section 4 shows
the experiments we performed to evaluate the effectiveness,
efﬁciency and security analysis support of ELISE, and an abla-
tion study of ELISE. In Section 5, we discuss the advantages
and limitations of ELISE and future research directions. We
summarize related work in Section 6 and conclude this paper
in Section 7.
2 Background and Motivation
Log analysis is an essential part of system development, which
can be used for many tasks such as debugging [34, 47, 57, 60],
performance measurement and trouble-shooting [16, 58, 62,
67], as well as many security applications including but not
limited to intrusion detection [21, 23, 26, 27, 45], system mon-
itoring [13, 24], attack investigation and provenance analy-
sis [33, 41, 42, 50]. For example, Apache HTTP access log
provides rich information for security auditing. In investi-
gating APTs where the adversary customizes malware and
residents in the system for months to years, log is the only
source that cyber analysts can leverage to understand the
ramiﬁcations (i.e., damages made by the attack) and root
causes. Many security analytic systems for APTs and other
cyberattacks are based on system level audit logs or program
logs [26, 41, 50, 64].
One fundamental challenge of existing log based systems
is the large volume of log data to store. In previous work [17,
26, 31, 37, 40–42, 63], researchers observed that a small sized
enterprise needs to store hundreds of gigabytes log ﬁles even
only for system level events. We also have observed the same
phenomenon in our testbed. Notice that APTs can last for
years. To support cyber attack analysis, logs have to be stored
for years, causing a huge burden. Consistently collecting and
storing such large amounts of log for months or even years
waste too much storage space and also hinder the development
of large-scale log security applications.
3024    30th USENIX Security Symposium
USENIX Association
There are two typical approaches to solve this problem.
One [37, 42, 63] is to remove redundant events to reduce stor-
age overhead for speciﬁc security related investigations. For
example, LogGC [37] observes that many system call events
represent the same semantics, e.g., a sequence of read sys-
tem calls reﬂect only one ﬁle read operation, and proposes
to shrink the log by keeping only one of them. Despite that
they have great effects on reducing the log size, these meth-
ods assume using analysis methods whose result will remain
accurate without removed events, such as dependence anal-
ysis [37, 42] where all read system calls represent the same
dependency. Therefore, the deﬁnition of “redundant events”
is speciﬁc to analysis methods. As such, this approach can
not be generally applied to different security applications: for
example, an event frequency based anomaly detection method
requires all events including the ones that are deﬁned as “re-
dundant” in dependence based analysis [37, 42]. The other
approach, data compression, which is more general, stores the
same information with less space. Data compression meth-
ods can be roughly divided into lossless compression and
lossy compression. Because of the data integrity requirement
of most security analyses, lossy compression is not suitable.
Thus, lossless data compression is the most general and com-
monly accepted method for log storage optimization.
2.1 Lossless Data Compression
The goal of lossless data compression is to generate another
encoding for the same contents so that the space usage is
reduced. The basic idea is to use shorter encodings for more
frequent elements. Such an embedding schema can be pro-
duced by using either traditional rule based approaches or
machine learning based approaches. Traditional rule based
approaches compress data by using observable and deﬁnable
redundancy rules ﬁrst, and symbol frequencies based encod-
ing algorithms such as Huffman [30] encoding later. Repre-
sented by Gzip, most modern commercial and open-source
compression systems use such a schema. ML approaches
train probabilistic models to learn the statistical structure of
data that can be coupled to arithmetic encoding, a stronger
encoding algorithm than Huffman encoding, to better exploit
the statistical redundancy in the input and improve compres-
sion results. Along this line of work, DNNs have achieved
state-of-the-art results [10, 43, 55].
Rule based lossless compression. Gzip, the most represen-
tative rule based data compression method, works by ﬁrst
replacing repeated content blocks in the text with shorter
mark strings, and then using Huffman encoding to encode the
characters. It ﬁrst uses a variant of the LZ77 algorithm [68],
which detects all repeated contents in the ﬁle and replaces
them with shorter marks: if we know the position and size of
the ﬁrst matched content, we can replace the following iden-