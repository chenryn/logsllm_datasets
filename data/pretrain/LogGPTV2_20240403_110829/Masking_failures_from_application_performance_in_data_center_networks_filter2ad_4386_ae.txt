# Testbed and Experimental Setup

## 6.1 Testbed Configuration
Our prototype network is a k = 4, n = 1 ShareBackup with 2 Pods, which consists of 2 Pods in a k = 4 fat-tree topology. Each failure group's 2 switches share 1 backup switch. Specifically, this non-blocking network includes 12 active switches, 6 backup switches, and 8 hosts. Figure 4 illustrates the physical deployment of this logical network, where all links operate at 10Gbps. The switches are deployed on 5 48-port OpenFlow packet switches: one partitioned into core switches and their backups, and the others into the active and backup switches in the same layer of a Pod.

The circuit switches are logical partitions of a 192-port 3D-MEMS optical circuit switch (OCS). Each host is an individual machine with 6 3.5GHz dual-hyperthreaded CPU cores and 128GB RAM, running Linux 3.16.5 with TCP Cubic. To manage the testbed more effectively, we connect the hosts to the OCS via an extra hop on packet switches.

We deploy distributed network controllers as described in Section 4.2. Our OCS uses the standard TL1 interface. To support the proposed interface function, i.e., `replace()`, we use an agent that translates controller queries into TL1 commands to control the corresponding ports on the OCS. Since the switch image source code is inaccessible, we bypass the failure detection mechanism by creating failures manually. We disable forwarding rules to introduce failures and set a dummy detection latency of 10ms. This issue can be resolved using the BFD protocol for fast failure detection, which is available on many commercial switches. VLANs are set at end hosts to enable live impersonation of failed switches according to Section 4.4. Our focus is on online failure recovery, while offline diagnosis of link failures is evaluated separately in Section 6.5.

The switching delay of our OCS is several milliseconds, significantly higher than the targeted circuit switches (e.g., 70ns for electrical crosspoint switches and 40μs for 2D-MEMS). To evaluate performance accurately, we also emulate the ideal circuit switch using an electrical packet switch (EPS) with similar switching delays. In case of failures, controllers change the forwarding rules to redirect traffic to the backup switch, although rule insertion/deletion introduces extra latency.

## 6.2 Simulation
### Linear Programming Simulation
For both simulations, the simulated network is a k = 16 fat-tree, consisting of 320 switches and 1024 hosts. Each failure group is assigned 1 backup switch, adding 40 additional switches to the network. We abstract the network as a graph and solve the maximum concurrent multi-commodity flow problem using a Linear Programming (LP) solver. This formulation maximizes the minimum throughput among all flows, showing the worst-case scenario under failures. The result assumes optimal routing under perfect load balancing.

### Packet-Level Simulation
We developed a simulator that supports TCP, fat-tree’s Two-Level Routing, and dynamic failure events. The failure recovery delay is based on measurements from our testbed and reported numbers for the compared architectures. Our simulation improves upon previous studies by considering the failure recovery process and eliminating biases against rerouting solutions. The random drop behavior is disabled, enabling realistic and fair comparisons against rerouting solutions.

## 6.3 Experimental Setup
### 6.3.1 Network Architectures Compared
- **PortLand [30]**: Abstracted as a fat-tree network in the LP formulation. In the packet-level simulator and testbed, we use Two-Level Routing and improve PortLand’s global rerouting with near-optimal load balancing under failures.
- **F10 [26]**: Built F10’s AB fat-tree. In the packet-level simulator and testbed, we use Two-Level Routing and perform 3-hop local rerouting under failures.
- **Aspen Tree [43]**: Maintained the host count and used a configuration that minimizes extra cost and failure convergence time. ECMP is used to distribute flows evenly in each layer. Under failures, traffic is rerouted locally or pushed back to upstream switches.

### 6.3.2 Failure Models
- **Random Layered**: Random switch and link failures in different layers. Simplified in the testbed to one link failure at a time.
- **Real**: Reproduced real-world failures based on a study in production data centers. Dynamic switch and link failures are created with failure locations, arrival times, and durations based on empirical data.

### 6.3.3 Traffic Patterns
- **Permutation**: Uniform traffic across the network.
- **Stride**: Heavy contention in the network core.
- **Hot Spot**: Simulates multicast in machine learning applications.
- **Many-to-Many**: Simulates the shuffle phase in MapReduce jobs.
- **Coflow**: Trace from a Facebook data center, reflecting communications in MapReduce jobs.
- **Deadline**: Partition-aggregate traffic in interactive web applications.

### 6.3.4 Real Applications
- **Spark Word2Vec**: Iterative machine learning job with heavy all-to-all traffic.
- **Tez Sort**: Distributed sorting algorithm with a heavy shuffle phase.
- **Spark TPC-H**: Decision support benchmark with critical query latency.

## 6.4 Transient State Analysis
We examine TCP behavior during ShareBackup’s failure recovery. A sender host transmits TCP packets at line rate to a receiver host. We capture packets with Wireshark and record TCP congestion window size with the tcp_probe kernel module while injecting a link failure along the path. Figure 5 and Figure 6 show the results.

In Figure 5, the OCS implementation and EPS emulation experience 8.5ms and 0.5ms disruption times, respectively. Interestingly, the OCS testbed experiences less packet loss despite the longer disruption time. Further investigation reveals that the packet switch stops forwarding packets when the destination port is down, buffering them in switch memory. The EPS emulation does not have such a port-down period, leading to continuous packet drops in the transient state. Our targeted circuit switch technologies have much lower switching delays and will cause a port-down event, leading to shorter disruption times and similar or less packet loss.

In Figure 6, neither the OCS implementation nor the EPS emulation hits the retransmission timeout. TCP recovers lost packets rapidly, validating our design for fast in-network failure recovery.

| **Failure Recovery** | **Failure Diagnosis** |
|---------------------|-----------------------|
| **OCS**             | **EPS**               |
| 8.73                | 0.73                  |
| 0.22                | 0.22                  |
| 0.01                | 0.01                  |
| **Total**           | **Total**             |
| 8.5                 | 0.5                   |
| 502.1               | 359.2                 |
| 487.3               | 352.6                 |

This breakdown shows the total failure recovery and diagnosis delays in milliseconds.