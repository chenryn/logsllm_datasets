(cid:20)(cid:6)(cid:21) (cid:3)(cid:16)(cid:8)(cid:14)(cid:9)(cid:13)(cid:15)(cid:19) (cid:16)(cid:5)(cid:7)(cid:12)(cid:8)(cid:18) (cid:17)(cid:19)(cid:11)(cid:18)(cid:7)(cid:10)(cid:8)(cid:17)
(cid:20)(cid:7)(cid:21) (cid:3)(cid:1)(cid:4)
Figure 4: A testbed of k = 4 n = 1 ShareBackup with 2 Pods.
Spark and Tez jobs on our testbed to measure the performance
improvement to real data center applications.
6.1 Testbed
Our prototype network is a k = 4 n = 1 ShareBackup with
2 Pods, that is 2 Pods of a k = 4 fat-tree where each failure
group’s 2 switches share 1 backup switch. Speciﬁcally, it is
a non-blocking network with 12 active switches, 6 backup
switches, and 8 hosts. Figure 4 shows the physical deployment
of this logical network. All links are 10Gbps. The switches lo-
cate on 5 48-port OpenFlow packet switches: one partitioned
into core switches and their backups, and the others each into
the active and backup switches in the same layer of a Pod.
The circuit switches are logical partitions of a 192-port 3D-
MEMS optical circuit switch (OCS). The hosts are individual
machines each with 6 3.5GHz dual-hyperthreaded CPU cores
and 128GB RAM. They run Linux 3.16.5 with TCP Cubic.
To make the testbed more manageable, we connect hosts to
the OCS via an extra hop on packet switches.
We deploy distributed network controllers as described in
Section 4.2. Our OCS uses the standard TL1 interface. To sup-
port the proposed interface function, i.e. replace( ) in Figure 2,
we let controllers talk to each logical circuit switch (an OCS
partition) through an agent. The agent stores the connections
of its own circuit switch, through which it translates the con-
troller queries via our new interface into the TL1 command
to control the corresponding ports on the OCS. The source
code of our switch image is unaccessible, so we are unable to
realize the failure detection mechanism in Section 4.1. Since
failure detection is not our main contribution, we bypass it by
creating failures at will. We disable forwarding rules to intro-
duce failures and make the controllers react after a dummy
detection latency of 10ms. This artifact is easily solvable,
since the BFD protocol for fast failure detection is readily
available for many commercial switches. We set VLANs at
end hosts to enable live impersonation of failed switches ac-
cording to Section 4.4. We focus on the online failure recovery
in the testbed implementation. The ofﬂine diagnosis of link
failures is evaluated separately in Section 6.5.
The switching delay of our OCS is several milliseconds,
orders of magnitude higher than that of the targeted circuit
switches, e.g. 70ns for electrical crosspoint switch [23] and
Masking Failures from Application Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
40μs for 2D-MEMS [46]. To evaluate the performance accu-
rately, we also emulate the ideal circuit switch using electrical
packet switch (EPS), which observes similar switching delay.
The bipartite connections on circuit switches are realized as
straight-through forwarding rules between input and output
ports on the EPS. In case of failures, controllers change the
forwarding rules to redirect trafﬁc to the backup switch. Al-
though rule insertion/deletion introduces extra latency, this is
at the best of our effort given the limited hardware.
6.2 Simulation
For both simulations below, the simulated network is a k =
16 fat-tree, which consists of 320 switches and 1024 hosts.
We assign each failure group 1 backup switch, that is 40
additional switches to the network.
Linear Programming Simulation: We abstract the net-
work as a graph and cripple a varying number of links and
switches. We solve the maximum concurrent multi-commodity
ﬂow problem [24] given different trafﬁc patterns using a Lin-
ear Programming (LP) solver, which is a well-adopted ap-
proach in topology analysis [37, 38, 49]. This formulation is
to maximize the minimum throughput among all the ﬂows,
showing the worst case under the effect of failures. The result
assumes optimal routing under perfect load balancing.
Packet-Level Simulation: We developed a simulator that
supports TCP, fat-tree’s Two-Level Routing, and dynamic
failure events. The failure recovery delay is based on the mea-
surement result on our testbed and reported numbers for the
compared architectures [26, 30, 43]. Our simulation is a big
improvement to a similar study previously [48]. First, their
work is limited to converged steady state after failures, while
we consider the failure recovery process. Second, their results
are biased against rerouting solutions. It uses ECMP routing,
which may create more hot spots after rerouting due to hash
collisions, and thus exaggerate the effect of failures. It also
randomly drops packets from the buffer when congestion hap-
pens, so packet retransmission will hugely increase the ﬂow
completion time. In contrast, Two-Level Routing eliminates
randomness by assigning each ﬂow a deterministic path, and
load balancing further mitigates hot spots. The random drop
behavior is disabled. Our simulation enables realistic and fair
comparisons against rerouting solutions.
6.3 Experimental Setup
6.3.1 Network Architectures Compared.
PortLand [30]: We abstract PortLand as a fat-tree [8] net-
work in the LP formulation. In the packet-level simulator and
the testbed, we use Two-Level Routing as the default rout-
ing method without failures and improve PortLand’s global
rerouting with near-optimal load balancing under failures. In
the simulator, we reroute trafﬁc heuristically according to the
184
bandwidth utilization of alternative paths. In the testbed, for
every possible failure, we hard-code the rerouted paths for
affected ﬂows such that each link carries roughly the same
number of ﬂows. Our optimized version of PortLand gives a
throughput upper bound for rerouting.
F10 [26]: We build F10’s AB fat-tree. In the packet-level
simulator and the testbed, we use Two-Level Routing as the
default failure-free routing and perform 3-hop local rerout-
ing under failures. In the simulator, we randomly reroute
impacted ﬂows to available local paths. In the testbed, limited
by the network scale, there is only one alternative path for
each ﬂow. The LP solver enforces global optimal routing, so
it evaluates the capacity of the network topology alone, which
serves as a very loose upper bound for actual F10.
Aspen Tree [43]: We maintain the host count as in the
above networks and pick the Aspen Tree conﬁguration that
minimizes extra cost and failure convergence time, that is
adding an extra layer of switches below the core switches.
Two-level routing does not apply to Aspen Tree because of
the redundant layer. Instead, we use ECMP to distribute ﬂows
evenly in each layer. Under failures, we reroute trafﬁc locally
if alternative paths exist or push back to upstream switches
otherwise. Like in F10, the LP simulations perform global
optimal routing on the topology to show the capacity upper
bound. Aspen Tree is not supported in our testbed due to the
extra hardware required.
6.3.2 Failure Models.
Random Layered: The LP analysis requires an easy-to-
reason failure model that helps understand the effect of failure
locations, so we generate random switch and link failures
separately in different layers of the network. We simplify this
model in the testbed experiments: we create one link failure
at a time, since switch failures and concurrent link failures
are fatal for our small-scale testbed.
Real: We reproduce real-world failures in our packet-level
simulator according to a failure study in production data cen-
ters [16]. We create dynamic switch and link failures in the
network. Failure locations are derived from the probability
of failures per switch/link type (Figure 6 and 7 in [16]), and
the arrival time and duration of failures are based on the
corresponding distributions (Figure 8 and 9 in [16]).
6.3.3 Trafﬁc Patterns.
The LP solver runs on steady trafﬁc, so we drive the computa-
tion with the following widely-used synthetic trafﬁc patterns.
Permutation: Every host sends a single ﬂow to a unique
server other than itself at random. This pattern creates uniform
trafﬁc across the network.
Stride: Every host sends a single ﬂow to its counterpart in
the next Pod. This trafﬁc pattern creates heavy contention in
the network core.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Wu et al.
Disruption time = 8.5 ms
Retransmission
Disruption time = 0.5 ms
n tim
Retransmission
Figure 5: Trace of TCP sequence number during failure recovery.
Hot Spot: Every 100 hosts form a cluster, in which one
host broadcasts to all the others. It simulates the multicast
phase in many machine learning applications.
Many-to-Many: Every 20 hosts form a cluster with all-to-
all trafﬁc. This trafﬁc pattern simulates the shufﬂe phase in
MapReduce jobs.
There are two pervasive types of applications in data cen-
ters: throughput-intensive and latency-sensitive [9, 20]. We
feed the packet-level simulator with data center trafﬁc traces
from these applications to create realistic settings.
Coﬂow: We obtain the trace in a Facebook data center from
the coﬂow benchmark [4]. It contains correlated ﬂows known
as coﬂows that reﬂect communications in MapReduce jobs.
We observe highly skewed multicast, shufﬂe, and incast trafﬁc
in the trace: some coﬂows involve a large number of machines
and have high trafﬁc volume. For each rack-to-rack ﬂow in
the trace, we create ﬂows between hosts under the source and
destination edge switches to saturate switch uplinks.
Deadline: We follow the method in D 3 to generate partition-
aggregate trafﬁc in interactive web applications [45]. For each
query, we randomly choose 1 host as the aggregator and 40
hosts as workers. Workers respond after a random jitter be-
tween (0, 10ms] to simulate the local computation. The net-
work utilization varies between 10% and 30%. The deadline
is set to be 2× the response time in the failure-free network.
6.3.4 Real Applications.
We run Spark and Tez on our testbed as representative appli-
cations in data centers. Among the 8 hosts, the ﬁrst works as
the master node and all the others as slave nodes. We create
the following throughput-intensive and latency-sensitive jobs.
Spark Word2Vec: This iterative machine learning job uses
high dimensional vectors to represent words in documents. In
each iteration, the master node broadcasts the updated model
to all workers. We conﬁgure Spark to broadcast ∼500MB
data in each iteration in a BitTorrent fashion. This phase thus
observes heavy all-to-all trafﬁc. The data to be transmitted is
readily available in memory.
185
Figure 6: TCP congestion window size during failure recovery.
Tez Sort: This job is a distributed sorting algorithm based
on the MapReduce programming model. The aggregate input
data size is 100GB. This job has a heavy shufﬂe phase, where
all the nodes as mappers send data to a subset of nodes as
reducers. We store the data on a RAM disk to prevent the hard
drive being the bottleneck of data read/write.
Spark TPC-H: This decision support benchmark consists
of a suite of database queries that help answer important busi-
ness questions. The queries are running against a 160GB data-
base on each worker. The processing power of the decision-
making system is reﬂected by the number of queries per hour,
so the query latency is critical to performance.
6.4 Transient State Analysis
We examine the TCP behavior during ShareBackup’s failure
recovery. A sender host transmits TCP packets at line rate
to a receiver host. At the receiver, we capture packets with
Wireshark to get the sequence number and record TCP con-
gestion window size with the tcp_probe kernel module while
injecting a link failure along the path. We get similar results
with the variance of sender and receiver locations. Figure 5
and Figure 6 show one instance of the results.
In Figure 5, the OCS implementation and the EPS emula-
tion experiences 8.5ms and 0.5ms disruption time respectively.
This delay is contributed by OCS/EPS reconﬁguration, i.e.
resetting OCS circuits or changing EPS forwarding rules. In-
terestingly, we observe less packet loss on the OCS testbed
even though it has relatively longer disruption time. Further
investigations reveal that our packet switch by default stops
forwarding packets when their destination port is detected as
down. Those packets are buffered in switch memory and sent
out after the port comes up. The EPS emulation does not have
such port-down period and packets are then continuously sent
out and dropped in the transient state. As described at the
beginning of Section 3, our targeted circuit switch technolo-
gies have much lower switching delay than the EPS. They
function like the OCS and will cause the port-down event. In
practical implementations, we expect shorter disruption time
than the EPS and similar or less packet loss than the OCS.
In Figure 6, neither the OCS implementation nor the EPS
emulation hit the retransmission timeout. For both of them,
TCP can proceed smoothly and recover lost packets rapidly.
This result validates our design of fast in-network failure
Masking Failures from Application Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Table 3: Break-down of failure recovery and diagnosis delay (ms).
Failure Recovery
Failure Diagnosis
total
OCS 8.73
EPS 0.73
0.22
0.22
0.01
0.01
communication computation reconﬁg preemption no preemption
8.5
0.5
502.1
359.2
487.3
352.6