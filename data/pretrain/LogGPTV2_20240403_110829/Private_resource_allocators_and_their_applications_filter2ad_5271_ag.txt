guarantee C-privacy (Definition 2) and Liveness (Definition 3).
Proof. Suppose that A sets Pmal = {pi} and |Phon| ≥ k. The
allocator then picks s ∈ [0, k] and U ← RA(P, k, λ) from a
secret distribution not known to A. Assume for purposes of
contradiction that RA satisfies C-Privacy and Liveness.
A key observation is that Pr[pi ∈ U|b = 0 ∧ s > 0] = 1,
since s processes in Pmal will be allocated a resource when
b = 0. However, Pr[pi ∈ U|b = 1 ∧ s > 0] = xi. Since RA
guarantees C-Privacy, Pr[s > 0] · (1 − xi) is negligible for any
choice of Pmal = {pi}.
Claim 10. Pr[s > 0] = 1/poly(λ).
This follows from the fact that RA guarantees Liveness (Def-
inition 3), and so it must allocate resources to s > 0 processes
with non-negligible probability.
By Claim 10 and C-Privacy, (1− xi) must then be negligible
since Pr[s > 0] · (1 − xi) = (1 − xi)/poly(λ) = negl(λ).
Observe that since (1 − xi) is negligible for every choice
of Pmal = {pi}, this implies that every process is allocated
a resource with probability close to 1 when b = 1, which
contradicts the capacity of RA since |P| > k. Therefore, the
difference in conditional probabilities Pr[s > 0] · (1 − xi) must
be non-negligible for some choice of Pmal = {pi}, which
contradicts that RA satisfies C-Privacy. Furthermore, finding one
such pi is efficient as there are only |P| = poly(λ) elements.
B. Proof of impossibility result
Theorem 1 (Impossibility result). There does not exist a
resource allocator RA that achieves both IT-privacy (Def. 1) and
Liveness (Def. 3) when k is poly(λ) and |P| is superpoly(λ)
(i.e., |P| ∈ ω(λc) for all constants c).
Proof. We start with two simple claims.
Claim 11. If |P| = superpoly(λ), then an overwhelming
fraction of the p ∈ P have Pr[p ∈ RA(P, k, λ)] ≤ negl(λ).
This follows from a simple pigeonhole argument: there are a
super-polynomial number of processes requesting service, and
at most a polynomial number of them can have a non-negligible
probability mass.
Claim 12. If RA guarantees liveness, then in the security
game, the conditional probability Pr[p ∈ U|b = 0 ∧ Pmal =
{p}] must be non-negligible. This follows since for all RA, if
P = {p}, RA(P, k, λ) must output U = {p} with non-negligible
probability (recall liveness holds for any set P, including the
set with only one process). Therefore, in the security game
when b = 0, the call to RA(Pmal, k, λ) must return {p} with
non-negligible probability.
We now prove Theorem 1 by contradiction. Suppose that an
allocator RA achieves both liveness and privacy when |P| =
18
superpoly(λ). Let X be a set with a super-polynomial number of
processes. By Claim 11, RA allocates an overwhelming fraction
of the p ∈ X with negligible probability, so the adversary
can identify one such process. The adversary can then set
Pmal = {p} and Phon = X − {p}. This satisfies the condition
of Claim 12, so we have that Pr[p ∈ U|b = 0] ≥ 1/poly(λ).
Finally, when b = 1, the challenger passes P = Pmal ∪ Phon
as input to RA. Notice that P = X, so by Claim 11, Pr[p ∈
U|b = 1] ≤ negl(λ). As a result, the advantage of the adversary
is inversely polynomial, which contradicts the claim that RA
guarantees IT-privacy.
C. DPRA guarantees differential privacy
(Def. 5) for ε = 1
g(λ) and δ = 1
We prove that DPRA (§IV-C) is (ε, δ)-differentially private
g(λ) ) if |Phon| ≤ βhon.
To simplify the notation, let β = βhon. Let f (S) = |S| be a
function that computes the cardinality of a set. Let P be the
set of processes as a function of the challenger’s bit b:
2 exp( 1−h(λ)
(cid:40)
P(b) =
Pmal
Pmal ∪ Phon
if b = 0
if b = 1
It is helpful to think of P(0) as a database with one row and
entry Pmal, and P(1) as a database with two rows and entries
Pmal and Phon. Accordingly, f (P(b)) is a function that sums
the entries in all rows of the databases. Since we are given
that |Phon| ≤ β, the ℓ1-sensitivity of f (·) is bounded by β.
To begin, let us analyze the first half of DPRA. Assume the
algorithm finishes after sampling the noise n, and the output is
t (i.e., we are ignoring choosing the processes for now). Also,
we will ignore the ceiling operator when computing n, since
post-processing a differentially private function by rounding
it up keeps it differentially private [32, Proposition 2.1]. So
in what follows, n and therefore t are both real numbers. Call
this algorithm M:
Algorithm M:
• Inputs: P(b), k
• n ← max(0, Lap(µ, s))
• Output: t ← |P(b)| + n
where µ and s are the location and scale parameters of the
Laplace distribution (in DPRA they are functions of λ and β,
but we will keep them abstract for now).
and δ =(cid:82) β
Theorem 2. M is (ε, δ)-differentially private for ε = β/s
−∞ Lap(w|µ, β/ε)dw. Specifically, for any subset of
values L in the range, [f (P(0)),∞) of M:
Pr[M(P(0), k) ∈ L] ≤ eε · Pr[M(P(1), k) ∈ L] + δ
and
Pr[M(P(1), k) ∈ L] ≤ eε · Pr[M(P(0), k) ∈ L]
We partition L into two sets: L1 = L ∩ [y,∞) and L2 =
Proof. Let x = f (P(0)), y = f (P(1)).
L − L1 = L ∩ [x, y).
Let dP(b),k(·) be the probability density function for M’s
output when the sampled bit is b. For ease of notation, we will
denote this function by db(·).
For any particular value ℓ ∈ L1, we show that d0(ℓ) ≤
eε · d1(ℓ) and d1(ℓ) ≤ eε · d0(ℓ). Integrating each of these
inequalities over the values in L1, we get
Pr[M(P(0), k) ∈ L1] ≤ eε · Pr[M(P(1), k) ∈ L1]
and
Pr[M(P(1), k) ∈ L1] ≤ eε · Pr[M(P(0), k) ∈ L1]
Values in L1 are easy to handle because M can produce these
values regardless of whether the bit b is 0 or 1 and we are
able to bound pointwise the ratio of the probability densities
of producing each of these values because we choose a large
enough scale parameter.
Values in L2 can only be output by M if b = 0, and if such
values are output, information about bit b would be leaked.
Because we choose a large enough location parameter, we can
show that Pr[M(P(0), k) ∈ [x, y)] ≤ δ.
The theorem follows by combining these two cases. We first
deal with L1.
Lemma 13. For any set L1 ⊆ [y,∞): Pr[M(P(b), k) ∈ L1] ≤
eε · Pr[M(P(¯b), k) ∈ L1]
Proof. Recall the Laplace distribution with s = β/ε:
Lap(ℓ|µ, β/ε) =
For all ℓ ∈ L1 we have that:
· exp(
ε
2β
−ε|ℓ − µ|
)
β
d0(ℓ) = Lap(ℓ|µ + x, β
ε
)
· exp(− ε|ℓ − (µ + x)|
d1(ℓ) = Lap(ℓ|µ + y, β
ε
)
· exp(− ε|ℓ − (µ + y)|
=
ε
2β
=
ε
2β
)
)
β
β
It follows that for all ℓ ∈ L1:
exp(− ε|ℓ−(µ+x)|
exp(− ε|ℓ−(µ+y)|
d0(ℓ)
d1(ℓ)
=
β
)
)
= exp(
β
−ε|ℓ − (µ + x)| + ε|ℓ − (µ + y)|
)
ε(|ℓ − (µ + y)| − |ℓ − (µ + x)|)
β
β
ε|x − y|
= exp(
≤ exp(
) by triangle ineq.
≤ exp(ε) by def. of ℓ1 sensitivity
A similar calculation bounds the ratio d1(ℓ)
d0(ℓ).
We now prove that Pr[M(P(0), k) ∈ [x, y)] ≤ δ.
β
Lemma 14. Pr[M(P(0), k) ≤ y] ≤ δ.
Proof.
Pr[M(P(0), k) ≤ y] = Pr[x + n ≤ y]
≤ Pr[x + n ≤ x + β]
= Pr[n ≤ β]
= Pr[Lap(µ, s) ≤ β]
since y − x ≤ β
(cid:90) β
(cid:90) β
−∞
−∞
=
=
Lap(w|µ, s)dw
Lap(w|µ, β/ε)dw
= δ
Finally, we can prove the theorem. For any set L:
Pr[M(P(b), k) ∈ L] = Pr[M(P(b), k) ∈ L2]
+ Pr[M(P(b), k) ∈ L1]
≤ δ + Pr[M(P(b), k) ∈ L1]
≤ δ + eε Pr[M(P(¯b), k) ∈ L1]
≤ δ + eε Pr[M(P(¯b), k) ∈ L]
(cid:90) β
(cid:40) 1
2 exp( ε(β−µ)
1 − 1
β
The above shows M is (ε, δ)-differentially private. Note that:
δ =
−∞
Lap(w|µ, β/ε)dw =
if β  1, and s = β·g(λ), this gives
us the desired values of ε = 1
2 exp( ε(µ−β)
2 · exp( 1−h(λ)
g(λ) ).
g(λ) and δ = 1
We now show that the rest of DPRA (the uniform selection
)
)
β
of processes), remains (ε, δ)-differentially private.
Let X be a random variable denoting the number of processes
in Pmal that get the allocation. Since the adversary only learns
which processes in Pmal were allocated the resource, from his
point of view dummy processes and processes in Phon are
indistinguishable. Thus for each value ℓ ∈ [0, k],
Pr[X = ℓ| M(P(b), k) = t ∧ b = 0] = Pr[X =
ℓ| M(P(b), k) = t ∧ b = 1]. Combined with the inequalities
governing the probabilities that M outputs each value of t for
b = 0 and b = 1 respectively, we have that Pr[X = ℓ| b =
0] ≤ eε Pr[X = ℓ| b = 1] + δ and similarly with the values of
b exchanged. Thus the distribution of the number of malicious
processes allocated are very close for b = 0 and b = 1.
)
Finally, since our allocator is symmetric (§IV-C), the actual
identity of the malicious processes allocated does not reveal
any more information about b than this number.
D. Proofs of other allocation properties
Lemma 15. Any resource allocator that achieves IT-Privacy
satisfies population monotonicity.
Proof. We prove the contrapositive. If RA fails to achieve
population monotonicity, then there exists two processes pi and
19
Lap(βhon, ε). Assuming |P| > k, for all p ∈ P it follows that
Pr[p ∈ U] = k/(|P| + |U|). Since Pr[pi ∈ U] = Pr[pj ∈ U] for
all pi, pj ∈ M, it follows that DPRA satisfies envy-freeness.
pj such that when pj stops requesting allocation, the probability
that RA allocates pi a resource decreases. An adversary A
can thus construct P in the security game such that pi ∈ Pmal
and Phon = {pj}. As a result, Pr[pi ∈ Umal|b = 0] < Pr[pi ∈
Umal|b = 1] and RA fails to satisfy IT-Privacy.
Lemma 16. SRA and RRA satisfy population monotonicity.
This follows from the fact that SRA and RRA are IT-Private.
Lemma 17. DPRA satisfies population monotonicity.
Proof. Observe that the probability a given process pi
is
allocated a resource is Pr[pi ∈ U] = min(t, k)/t where t is
drawn from |P|+n and n is the noise sampled from the Laplace
distribution. As a process pj stops requesting service, we have
t = (|P|−1) +n. Since min(t, k)/t ≤ min(t−1, k)/(t−1), this
implies Pr[pi ∈ U] is strictly increasing as |P| decreases.
Lemma 18. SRA satisfies resource monotonicity.
Proof. When SRA’s capacity increases from k to k + c (with
c positive), the allocator can accommodate c more processes.
With |M| fixed, this implies the probability of a process getting
allocated a resource is increased by c / |M|.
Lemma 19. RRA satisfies resource monotonicity.
Proof. When RRA’s capacity increases from k to k + c (with
c positive), the allocator can accommodate at most c more
processes. With βp fixed, this implies the probability of a
process getting allocated a resource is increased by c / βp.
Lemma 20. DPRA satisfies resource monotonicity.
Proof. When DPRA’s capacity increases from k to k+c (with c
positive), the allocator is able to accommodate at most c more
processes. Although the capacity of the allocator is increasing,
the distribution Lap(βhon/ε) remains constant. Recall that the
probability a given process pi is allocated a resource is Pr[pi ∈
U] = min(t, k) / t where t is drawn from |P| + n and n is the
noise sampled from the Laplace distribution. Since min(t, k)
≤ min(t, k + c) for all t, k, we have that Pr[pi ∈ U] is strictly
increasing as k increases.
Lemma 21. SRA satisfies envy-freeness.
Proof. SRA assigns every process pi ∈ M a unique allocation
slot in [0,|M|), giving each process a constant k/|M| probability
of being allocated a resource for a uniformly random r ∈ N≥0.
Since Pr[pi ∈ U] = Pr[pj ∈ U] for all pi, pj ∈ M, it follows
that SRA satisfies envy-freeness.
Lemma 22. RRA satisfies envy-freeness.
Proof. RRA pads up to βP each round with dummy processes,
so that each process pi ∈ P has probability 1/βP of being
allocated a resource. Since Pr[pi ∈ U] = Pr[pj ∈ U] for all
pi, pj ∈ M, it follows that RRA satisfies envy-freeness.
Lemma 23. DPRA satisfies envy-freeness.
Proof. DPRA samples k random processes from P∪U where U
consists of dummy processes added by sampling the distribution
20