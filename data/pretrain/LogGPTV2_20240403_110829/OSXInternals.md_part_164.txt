import Carbon.File
def main():
if len(sys.argv) != 2:
sys.stderr.write("usage: ResolveAlias \n")
return 1
try:
fsspec, isfolder, aliased = \
Carbon.File.ResolveAliasFile(sys.argv[1], 0)
except:
raise "No such file or directory."
print fsspec.as_pathname()
return 0
if __name__ == "__main__":
sys.exit(main())
$ ResolveAlias.py "/User Guides And Information"
/Library/Documentation/User Guides And Information.localized
12.8.10. Resource Forks
Historically, resource forks on HFS and HFS+ file systems have been used to hold resources. For an
application, resources might include custom icons, menus, dialog boxes, the application's executable code and
runtime memory requirements, license information, and arbitrary key-value pairs. For a document, resources
might include fonts and icons used by the document, preview pictures, preferences, and window locations to
use while opening the document. A resource fork is usually structured in that there is a map describing
resources that follow it. There are practical limits on the number of resources you could put in a resource fork.
In contrast, a data fork is unstructuredit simply contains the file's data bytes.
By default, the Unix API on Mac OS X accesses a file's data fork. It is, however, possible to access a resource
fork through the Unix API by using the special suffix /..namedfork/rsrc after the file's pathname.
$ cd /System/Library/CoreServices/
$ ls -l System
-rw-r--r-- 1 root wheel 0 Mar 20 2005 System
$ ls -l System/..namedfork/rsrc
504 -rw-r--r-- 1 root wheel 256031 Mar 20 2005 System/rsrc
The shortened suffix /rsrc can also be used to access the resource fork, although it is deemed a legacy suffix
and is deprecated in Mac OS X 10.4.
An HFS+ file with multiple nonzero forks is not a single stream of bytes and is therefore incompatible with
most other file systems. Care must be taken while transferring HFS+ files to other file systems. Before Mac
OS X 10.4, most standard Unix utilities on Mac OS X either didn't handle multiple forks at all or handled them
poorly. Mac OS X 10.4 has better command-line support for multiple forksstandard tools such as cp, mv, and
tar handle multiple forks and extended attributes, including when the destination file system does not support
these features. These programs rely on the copyfile() function, whose purpose is to create faithful copies of
HFS+ file system objects. copyfile() simulates multiple forks on certain file systems that do not support
them. It does so by using two files for each file: one containing the data fork and the other containing the
resource fork and attributes, flattened and concatenated. The second file's name is the prefix ._ followed by
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 61 of 81
the first file's name. This scheme of storing multiple forks is known as the AppleDouble format.
The SplitFork command can be used to convert a two-fork file into AppleDouble format. Conversely, the
FixupResourceForks command can be used to combine AppleDouble files into two-fork resource files.
/usr/bin/ditto can be used to copy files and directories while preserving resource forks and other metadata.
If the destination file system does not have native support for multiple forks, ditto will store this data in
additional files. ditto can also be used to create PKZip archives with flattened resource forks, in which case it
will keep resource forks and other metadata in a directory called __MACOSX within the PKZip archive.
$ cd /tmp
$ touch file
$ echo 1234 > file/..namedfork/rsrc
$ ls -l file
-rw-r--r-- 1 amit wheel 0 24 Apr 15:56 file
$ ls -l file/..namedfork/rsrc
-rw-r--r-- 1 amit wheel 5 24 Apr 15:56 file/..namedfork/rsrc
$ ditto -c -k -sequesterRsrc file file.zip
$ unzip file.zip
Archive: file.zip
extracting: file
creating: __MACOSX/
inflating: __MACOSX/._file
$ cat __MACOSX/._file
2 R1234
The original file can be recreated from the PKZip archive using ditto.
% rm -rf file __MACOSX
% ditto -x -k -sequesterRsrc file.zip .
% ls -l file
-rw-r--r-- 1 amit wheel 0 24 Apr 15:56 file
% ls -l file/rsrc
-rw-r--r-- 1 amit wheel 5 24 Apr 15:56 file/rsrc
12.9. Optimizations
The Mac OS X HFS+ implementation contains adaptive optimizations to improve performance and reduce
fragmentation. We will look at these optimizations in this section.
12.9.1. On-the-Fly Defragmentation
When a user file is opened on an HFS+ volume, the kernel checks whether the file is qualified for on-the-fly
defragmentation. All of the following conditions must be met for the file to be eligible.
The file system is not read-only.
The file system is journaled.
The file is a regular file.
The file is not already open.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 62 of 81
The file fork being accessed is nonzero and no more than 20MB in size.
The fork is fragmented into eight or more extents.
The system has been up for more than three minutes (to ensure that bootstrapping has finished).
If all the preceding conditions are satisfied, the file is relocated by calling hfs_relocate()
[bsd/hfs/hfs_readwrite.c], which attempts to find contiguous allocation blocks for the file. A successful
relocation results in a defragmented file. Let us see this mechanism in action by creating a fragmented file and
causing its relocation. We will use a somewhat unsavory method to create our fragmented file. Recall that we
wrote a program (hfs_change_next_allocation, shown earlier in Figure 1211) to provide a hint to the
kernel regarding the location of the next allocation block search. We will use that program to our advantage in
the following algorithm to create the file we desire.
1. Start with a small file F.
2. Use hfsdebug to determine the location of F's last extent.
3. Use hfs_change_next_allocation to set the next allocation pointer immediately after F ends.
4. Create a nonempty dummy file d. This should consume the allocation block immediately after F's last
allocation block.
5. Append an allocation block's worth of data to F. Since F cannot grow contiguously any more, it will
require another extent to house the newly written data.
6. Delete the dummy file d.
7. If F has eight extents, we are done; otherwise, go back to step 2.
Figure 1229 shows a Perl program that implements the algorithm.
Figure 1229. A Perl program to create a file with eight fragments on an HFS+ volume
#! /usr/bin/perl -w
my $FOUR_KB = "4" x 4096;
my $BINDIR = "/usr/local/bin";
my $HFSDEBUG = "$BINDIR/hfsdebug";
my $HFS_CHANGE_NEXT_ALLOCATION = "$BINDIR/hfs_change_next_allocation";
sub
usage()
{
die "usage: $0 \n\twhere  must not be the root volume\n";
}
(-x $HFSDEBUG && -x $HFS_CHANGE_NEXT_ALLOCATION) or die "$0: missing tools\n";
($#ARGV == 0) or usage();
my $volume = $ARGV[0];
my @sb = stat($volume);
((-d $volume) && @sb && ($sb[0] != (stat("/"))[0])) or usage();
my $file = "$volume/fragmented.$$";
(! -e $file) or die "$0: file $file already exists\n";
`echo -n $FOUR_KB > "$file"`; # create a file
(-e "$file") or die "$0: failed to create file ($file)\n";
WHILE_LOOP: while (1) {
my @out = `$HFSDEBUG "$file" | grep -B 1 'allocation blocks'`;
$out[0] =~ /^\s+([^\s]+)\s+([^\s]+)..*$/;
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 63 of 81
my $lastStartBlock = $1; # starting block of the file's last extent
my $lastBlockCount = $2; # number of blocks in the last extent
$out[1] =~ /[\s*\d+] allocation blocks in (\d+) extents total.*/;
my $nExtents = $1; # number of extents the file currently has
if ($nExtents >= 8) { # do we already have 8 or more extents?
print "\ncreated $file with $nExtents extents\n";
last WHILE_LOOP;
}
# set volume's next allocation pointer to the block right after our file
my $conflict = sprintf("0x%x", hex($lastStartBlock) + hex($lastBlockCount));
`$HFS_CHANGE_NEXT_ALLOCATION $volume $conflict`;
print "start=$lastStartBlock count=$lastBlockCount extents=$nExtents ".
"conflict=$conflict\n";
`echo hello > "$volume/dummy.txt"`; # create dummy file to consume space
`echo -n $FOUR_KB >> "$file"`; # extend our file to cause discontiguity
`rm "$volume/dummy.txt"`; # remove the dummy file
} # WHILE_LOOP
exit(0);
Now that we have the means of creating a file that should be eligible for on-the-fly defragmentation, let us test
the feature on a disk image.
$ hdiutil create -size 32m -fs HFSJ -volname HFSFrag /tmp/hfsfrag.dmg
...
$ open /tmp/hfsfrag.dmg
$ ./mkfrag.pl /Volumes/HFSFrag
start=0xaf9 count=0x1 extents=1 conflict=0xafa
start=0xafb count=0x1 extents=2 conflict=0xafc
start=0xafd count=0x1 extents=3 conflict=0xafe
start=0xaff count=0x1 extents=4 conflict=0xb00
start=0xb01 count=0x1 extents=5 conflict=0xb02
start=0xb03 count=0x1 extents=6 conflict=0xb04
start=0xb05 count=0x1 extents=7 conflict=0xb06
created /Volumes/HFSFrag/fragmented.2189 with 8 extents
$ hfsdebug /Volumes/HFSFrag/fragmented.2189
...
extents = startBlock blockCount % of file
0xaf9 0x1 12.50 %
0xafb 0x1 12.50 %
0xafd 0x1 12.50 %
0xaff 0x1 12.50 %
0xb01 0x1 12.50 %
0xb03 0x1 12.50 %
0xb05 0x1 12.50 %
0xb07 0x1 12.50 %
8 allocation blocks in 8 extents total.
...
$ cat /Volumes/HFSFrag/fragmented.2189 > /dev/null # open the file
$ hfsdebug /Volumes/HFSFrag/fragmented.12219
...
extents = startBlock blockCount % of file
0x1b06 0x8 100.00 %
8 allocation blocks in 1 extents total.
...
We see that opening a fragmented file that is eligible for on-the-fly defragmentation indeed caused relocation
of the file to a single extent.
12.9.2. The Metadata Zone
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 64 of 81
The HFS+ implementation in Mac OS X 10.3 introduced an allocation policy that reserves space for several
volume metadata structures, placing them next to each otherif possiblein an area near the beginning of the
volume. This area is called the metadata allocation zone. Unless disk space is scarce, the HFS+ allocator will
not consume space from the metadata zone for normal file allocations. Similarly, unless the metadata zone is
exhausted, HFS+ will allocate space for metadata from within the zone. Thus, various types of metadata are
likely to be physically adjacent and have higher contiguity in generalif so, the consequent reduction in seek
times will improve file system performance. The policy is enabled for a volume at runtime when the volume is
mounted. The volume must be journaled and at least 10GB in size. The hfsmount structure stores runtime
details of the metadata zone. We can use hfsdebug to view these details.
$ sudo hfsdebug -V /Volumes/HFSFrag -m # metadata zone should not be enabled
...
# Metadata Zone
metadata zone start block = 0
metadata zone end block = 0
hotfile start block = 0
hotfile end block = 0
hotfile free blocks = 0
hotfile maximum blocks = 0
overflow maximum blocks = 0
catalog maximum blocks = 0
...
$ sudo hfsdebug -m # metadata zone should be enabled for the root volume
...
# Metadata Zone
metadata zone start block = 0x1
metadata zone end block = 0x67fff
hotfile start block = 0x45bed
hotfile end block = 0x67fff
hotfile free blocks = 0x1ebaa
hotfile maximum blocks = 0x22413
overflow maximum blocks = 0x800
catalog maximum blocks = 0x43f27
...
Figure 1230 shows a representative layout of the metadata zone. Note that a given volume may not have all
shown constituents in usefor example, quotas are typically not enabled on Mac OS X systems, so there will be
no quota files. The last part of the metadata zone is used for an optimization called Hot File Clustering and is
therefore called the Hot File area.
Figure 1230. Layout of the HFS+ metadata zone
[View full size image]
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 65 of 81
12.9.3. Hot File Clustering
Hot File Clustering (HFC) is an adaptive, multistage clustering scheme based on the premise that frequently
accessed files are small both in number and in size. Such files are termed hot files in the context of HFC. As
part of HFC, Apple's HFS+ implementation performs the following operations to improve performance while
accessing hot files.
It keeps track of blocks read from file forks to identify candidate hot files.
After a predetermined recording period, it analyzes the list of candidate hot files to determine those that
should be moved to the Hot File area within the latter part of the metadata zone.
If necessary, it evicts existing files from the Hot File area to make space for newer and hotter files.
It moves selected hot files to the Hot File area, allocating contiguous space for them as they are moved.
HFC records each file's temperature, which is defined as the ratio of the number of bytes read from that file
during the recording period to the file's size. Thus, the more frequently a file is accessed, the higher its
temperature. HFS+ uses the clumpSize field of the HFSPlusForkData structure to record the amount of data
read from a fork.[23]
[23] HFC stores the number of allocation blocks readrather than the number of bytes readin the
clumpSize field, which is a 32-bit number.
12.9.3.1. Hot File Clustering Stages
At any given time, HFC on a volume can be in one of the following stages.
HFC_DISABLEDHFC is currently disabled, typically because the volume is not a root volume. HFC also
enters this stage if the volume was unmounted while HFC was in its recording stage.
HFC_IDLEHFC is waiting to start recording. This stage is entered after HFC is initialized during mount.
It can also be entered from the evaluation and adoption stages.
HFC_BUSYThis is a temporary stage that HFC remains in while performing work to transition from one
stage to another.
HFC_RECORDINGHFC is recording file temperatures.
HFC_EVALUATIONHFC has stopped recording file temperatures and is now processing the list of newly
recorded hot files to determine whether to adopt new files or to evict old files before adopting.
HFC_EVICTIONHFC is relocating colder and older files to reclaim space in the Hot File area.
HFC_ADOPTIONHFC is relocating hotter and newer files to the Hot File area.
Figure 1231 shows a state diagram showing the transitions between various HFC stages. If the current stage is
adoption, eviction, idle, or recording, the transition to the next stage is triggered as a side effect of a sync
operation on the volume.
Figure 1231. Transitions between Hot File Clustering stages
[View full size image]
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 66 of 81
12.9.3.2. The Hot Files B-Tree
HFC uses a B-Tree filethe Hot Files B-Treefor tracking hot files or, specifically, file forks. Unlike the other
HFS+ special files, this tree's extents are not recorded in the volume header. It is an on-disk file that the kernel
accesses by its pathname (/.hotfiles.btree).
$ ls -l /.hotfiles.btree
640 -rw------- 1 root wheel 327680 Oct 7 05:25 /.hotfiles.btree
The Hot Files B-Tree is similar to the Catalog B-Tree in that each fork being tracked has a thread record and a
Hot File record. Figure 1232 shows the key format used by the Hot Files B-Tree. Given a file's CNID and fork
type, the thread record for that fork can be looked up by setting the search key's temperature field to the
special value HFC_LOOKUPTAG (0xFFFFFFFF). A Hot File thread record's data is a 32-bit unsigned integer that
represents the fork's temperature. If no thread record can be found, HFC is not tracking that fork as a hot file.
By including the fork's temperature in the search key, the corresponding Hot File record can be looked up. The
data for this record is also a 32-bit unsigned integer, but it has no relevance to HFC. It contains one of two
values for debugging purposes: either the first four bytes of the file's UTF-8-encoded Unicode name or the
ASCII string "????".
Figure 1232. Searching in the Hot Files B-Tree
[View full size image]
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 67 of 81