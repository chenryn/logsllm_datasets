title:I Still Know What You Visited Last Summer: Leaking Browsing History
via User Interaction and Side Channel Attacks
author:Zachary Weinberg and
Eric Yawei Chen and
Pavithra Ramesh Jayaraman and
Collin Jackson
2011 IEEE Symposium on Security and Privacy
I Still Know What You Visited Last Summer
Leaking browsing history via user interaction and side channel attacks
Zachary Weinberg
Eric Y. Chen
Pavithra Ramesh Jayaraman
Collin Jackson
PI:EMAIL
PI:EMAIL
PI:EMAIL
PI:EMAIL
Carnegie Mellon University
Abstract—History snifﬁng attacks allow web sites to learn
about users’ visits to other sites. The major browsers have
recently adopted a defense against the current strategies for
history snifﬁng. In a user study with 307 participants, we
demonstrate that history snifﬁng remains feasible via interactive
techniques which are not covered by the defense. While these
techniques are slower and cannot hope to learn as much about
users’ browsing history, we see no practical way to defend against
them.
I. INTRODUCTION
Since the creation of the World Wide Web, browsers have
made a visual distinction between links to pages their users
have already visited, and links to pages their users have not
yet visited. CSS allows page authors to control the appearance
of this distinction. Unfortunately, that ability, combined with
JavaScript’s ability to inspect how a page is rendered, exposes
Web users’ browsing history to any site that cares to test a list
of URLs that they might have visited. This privacy leak has
been known since 2002 ([1], [2]), and ﬁxes for it have been
being discussed for nearly as long by both browser vendors
and security researchers.
In 2010, L. David Baron of Mozilla developed a defense [3]
that blocks all known, automated techniques for this attack,
while still distinguishing visited from unvisited links and
allowing site authors some control over how this distinction is
made. The latest versions of Firefox, Chrome, Safari, and IE
all adopt this defense. While it is a great step toward closing
this privacy leak, in this paper we will demonstrate that it
is still possible for a determined attacker to probe browsing
history.
Baron’s defense makes no effort to defend against interactive
attacks—that is, attacks which trick users into revealing what
they see on the screen. In our ﬁrst experiment, we demonstrate
four practical interactive attacks that we have developed. These
attacks can probe far fewer links per second than the automated
attacks that formerly were possible, but they are still feasible
for the small sets of links probed by the exploiters found by
Jang et al. [4]. We discuss some potential countermeasures,
but as long as a visited/unvisited distinction is being shown at
all, it does not seem to us that users can be entirely protected
from revealing it to a determined attacker.
Baron’s defense does include protection against side-channel
attacks, particularly timing attacks. In our second experiment,
we demonstrate a side-channel attack that remains possible: The
dominant color of the computer screen can be made to depend
on whether a link is visited. The light of the screen reﬂects off
the victim and his or her surroundings. If the victim possesses a
“webcam” (a small computer-controlled video camera, pointed
at the victim’s face—this is built into many recent laptops,
and is a popular accessory for desktop PCs) it can be used
to detect the color of the reﬂected light. This attack may not
be practical for typical sites, if only because users are chary
of granting access to their webcams. But like our interactive
attacks, we do not believe it can be prevented as long as a
visited/unvisited distinction is being shown onscreen.
The rest of this paper is organized as follows. In Section II
we introduce the problem of history snifﬁng; in Section III we
describe the automated attacks that were possible until quite
recently, and the defense that has now been deployed against
it. Section IV covers our primary experiment, demonstrating
the feasibility of interactive attacks on browsing history; we
also discuss the long-term implications of interactive attacks.
Section V describes our second experiment, demonstrating a
side-channel attack on history that remains exploitable even
with a general defense against automated attacks in place.
Section VI covers related work, and Section VII concludes.
II. BACKGROUND
A. The Web platform
The World Wide Web was originally conceived in 1990 as an
interface to large collections of static documents (“pages”) [5].
In this paradigm, it is obviously useful for users to be able to
tell whether they have seen a particular page before, no matter
who is referring to it. NCSA Mosaic, one of the ﬁrst graphical
Web browsers, drew hyperlinks in blue if they referred to a
page that had not yet been visited, in purple otherwise [6];
this feature was inherited by Netscape Navigator and has now
become customary.
Since its original conception, the Web has evolved into a
platform for software applications. At ﬁrst these relied on
server-side processing, but with the invention of JavaScript
in the late 1990s, it became possible to run programs inside
Web pages. With this capability comes a need for security:
applications must not interfere with each other, and malicious
software must not be permitted to exploit the user. The Web’s
basic security policy is the same-origin policy [7], which
1081-6011/11 $26.00 © 2011 IEEE
DOI 10.1109/SP.2011.23
147
partitions the Web by its servers. JavaScript programs can only
see data from the HTTP server that produced them; within the
client, they can communicate only with other pages produced by
the same server. The same-origin policy originally applied only
to JavaScript but is progressively being expanded to cover other
security decisions that the browser must make [8]. However, it
has never applied to hyperlinks. It would diminish the utility
of the Web if sites could not link to each other, or even if they
could only link to other sites’ “front pages.” Further, since
visited-link indications are most useful when you encounter an
unfamiliar link to a familiar page, links are marked as visited
whether or not they cross origins [9].
In principle, a website should not be able to determine what
other sites its visitors have visited. Unfortunately, a combination
of innocuous-seeming Web features makes it possible for
websites to probe browsing history. This vulnerability was
ﬁrst publicly disclosed by Andrew Clover in a BUGTRAQ
mailing list post in February of 2002 [1]. Until recently, browser
vendors and the security community believed that it was not
being exploited “in the wild,” but Jang et al. [4] discovered 46
popular websites—including one from the Alexa top 100—
that deﬁnitely probed browsing history and reported what they
found to their servers. Many of these sites were using third-
party JavaScript libraries designed speciﬁcally to probe history.
Another 326 sites made “suspicious” use of history information,
but might not have been reporting it to their servers.
B. Threat model
Illicit
inspection of browsing history is conventionally
referred to as history snifﬁng.1 As will be explained below,
history sniffers cannot simply get a list of all URLs their
victims have ever visited; they can only ask whether particular
URLs have been visited. Therefore, the goal of history sniffers
is to learn which of some predetermined set of interesting
URLs have been visited by their victims. In principle, there is
no limit to the size of this set, but the actual exploiters found
by Jang only probed 6 to 220 URLs.
History sniffers have the abilities of web attackers: they
control the contents of a website and a DNS domain, and they
can get victims to visit their website. For interactive snifﬁng,
as the name implies, victims must also be willing to interact
with a sniffer’s site in the same ways that they might interact
with a legitimate site. History sniffers do not have any of the
additional powers of a network attacker: they cannot eavesdrop
on, tamper with, or redirect network trafﬁc from victims to
legitimate sites (or vice versa), nor can they interfere with
domain name lookups. Furthermore, history sniffers cannot
install malicious software on their victims’ computers, or take
advantage of malware installed by someone else.
C. Attack consequences
What can history sniffers do with the information they glean?
There are some benign or even beneﬁcial possibilities. Sites
1While the attack has been known since 2002, the phrase “history snifﬁng”
seems to have been coined much later: the earliest use we have found was
in 2008 [10].
at grave risk of impersonation (banks, for instance) could use
history snifﬁng to determine whether their users have visited
known phishing sites, and if so, warn them that their accounts
may have been compromised [9], [11]. Sites could also seed
visitors’ history with URLs made up for the purpose, and then
use those URLs to re-identify their visitors on subsequent visits;
this can foil “pharming” attacks (where attackers redirect trafﬁc
for legitimate sites to servers under their control) by making it
impossible for attackers to predict the appearance of the sites
they wish to impersonate [12]. However, ordinary “cookies”
provide the same re-identiﬁcation capability in an aboveboard,
user-controllable fashion. Finally, sites that support federated
login (OpenID, Facebook Connect, etc.) can use history snifﬁng
to determine which identity provider a user favors, and thus
streamline their login UI [13]. The same principle can be
applied to a broad variety of third-party service providers,
such as those for social bookmarking, feed subscription, and
maps [10].
On the other hand, the actual history sniffers found by
Jang appear to be tracking visitors across sites for advertising
purposes and/or to determine whether they also visit a site’s
competitors. This is very similar to the “tracking cookies”
used by many ad networks, which are widely considered
to be invasions of privacy [14], but only on the same level
as having one’s postal address sold to senders of junk mail.
History snifﬁng could potentially enable much more severe
privacy violations, because unlike tracking cookies, it allows
the snifﬁng site to know about visits to sites with which it has
no relationship at all. For instance, the government-services
websites of a police state could detect whether their visitors
have been reading sites that provide uncensored news, and
corporate webmail servers could detect whether employees have
been visiting a union organizer’s online forum (even if they do
this from home) [15]. Knowledge of browsing habits can also
connect an identity used on one social network to that used
on another [16], defeating users’ efforts to keep them separate
so they can maintain contextually appropriate presentations of
self [17]. Finally, stepping away from privacy issues, attackers
can construct more targeted phishing pages [18], [19] by
impersonating only sites that a particular victim is known
to visit, or using visual details (such as logos) of those sites
in a novel but credible context [9], [11].
We consider the privacy and security costs of history snifﬁng
to outweigh the beneﬁcial possibilities.
III. AUTOMATED ATTACKS
Until recently, it was possible to sniff history automatically,
rapidly, and invisibly to users. While the focus of this paper is
on the attacks that remain possible today, for context’s sake
we begin by explaining how automated attacks worked and
how browsers now prevent them.
Web authors wish to control the appearance of their sites; the
modern mechanism for this is Cascading Style Sheets (CSS),
invented in the late 1990s (contemporaneously with JavaScript).
CSS provides control over every aspect of a page’s appearance,
including how the distinction between visited and unvisited
148
{ text-decoration: none }
a
{ color: #A61728 }
a:link
a:visited { color: #707070 }
Fig. 1. Example of CSS controlling rendering of links. Each line of code
is a style rule. Each style rule begins with a selector, which controls which
HTML elements are affected by the rule. A lone a selects all a elements,
i.e. hyperlinks; a:link and a:visited select unvisited and visited links,
respectively. A brace-enclosed list of style properties and their values follows;
these rules each contain only one property, but there could be many.
links is rendered. Figure 1 shows a sample set of changes
to the appearance of links: setting text-decoration to
none disables underlining, and setting color changes the
color of the text. If the same #rrggbb code were given in both
the second and third rules, visited and unvisited links would
be indistinguishable. Browsers’ default style sheets generally
distinguish visited and unvisited links with a color change, but
(until recently; see below) a web page’s style sheets could use
any CSS style property to make the distinction.
A. Direct snifﬁng
A JavaScript program can examine and manipulate the page
it is embedded within, using a standardized API known as the
Document Object Model (DOM) [20]. Most importantly for
our purposes, the DOM provides access to the computed style
of each HTML element. The computed style collects all of
the CSS properties that inﬂuence the drawing of that element,
which may have come from many style rules in different places.
Continuing with the example in Figure 1, the computed style
for both visited and unvisited links would show the value
of text-decoration as none, but the color property
would be #A61728 for unvisited links and #707070 for
visited links. JavaScript can also change the destination of
an existing hyperlink, or create entirely new hyperlinks to
destinations of its choosing.
Therefore, a malicious site can guess URLs of pages that its
visitors might have also visited, create links pointing to those
URLs, and determine whether each visitor has indeed visited
them by inspecting the links’ computed styles. The malicious
site’s style sheets control how the visited/unvisited difference
appears in the computed style, so the site knows exactly what
to look for. This only allows the malicious site to ask yes/no
questions about URLs it can guess; there is no known way
for a malicious site to get access to the browser’s complete
list of visited URLs. However, the “wild” exploits found by
Jang were interested in a small set of other sites that their
visitors also visited—usually direct competitors and popular
social networking sites—so they could use the well-known
URLs of those sites’ front pages. Deanonymization attacks
[16] can require thousands of history queries per victim, but
this is no obstacle; depending on the browser, an attacker can
make 10,000 to 30,000 queries per second [15].
B. Indirect snifﬁng
The attack described above admits a simple defense: the
DOM’s computed style API could pretend that all links were
being styled as if they were unvisited. However, this is only
the most direct way to detect whether or not a link has been
visited. Baron [3] lists two classes of indirect technique for
detecting whether a link has been visited:
• Make visited and unvisited links take different amounts
of space, which causes unrelated elements on the page to
move; inspect the positions of those other elements.
The DOM provides information on the position and size
of every HTML element on a page; the API for this
information is separate from the API for computed style.
Many CSS properties can change the size of an element,
and the size of an element inﬂuences the position of all
the elements that will be drawn after it. Therefore, an
attacker can make the APIs for position and size reveal
whether links are visited, by having the style rules for
visited links change the links’ sizes.
With moderate effort, the DOM could be made to pretend
that all links are being drawn with the size they would
have if they were unvisited. However, adopting the
same pretense for element positions would require the
browser to lay out the entire page twice, which would be
impractical.
• Make visited and unvisited links cause different images
to load.
The background-image style property speciﬁes a
URL of an image to load; if it is used in a :visited
rule limited to one link, that image will be loaded only if
that link is visited. The attacker can specify a unique URL
on their server for each link to be probed, then route all
those URLs to a program that records which links were
visited. (The program would always send back an empty
image, so the page’s appearance would not be affected.)
This technique does not even require JavaScript. It
could be defeated by unconditionally loading all images
mentioned in style rules, but that would increase page load
time and bandwidth consumption for honest websites.
C. Side-channel snifﬁng
Side channel attacks exist when a system leaks information
through a mechanism that wasn’t intended to provide that infor-
mation, bypassing the system’s security policy. Side channels
are difﬁcult to ﬁnd, and often cannot be eliminated without
destroying other desirable characteristics of the system [21].
For instance, when a cache returns a piece of information