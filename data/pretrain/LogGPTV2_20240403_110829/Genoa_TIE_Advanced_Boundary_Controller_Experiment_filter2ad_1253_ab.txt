through manual, human review of the  same transaction
data  that  the  automated  filters  processed.  Our  manual
review  was  referred  to  as  the  Ground  Truth  (GT),  and
was  intended  to  represent  an  ideal  situation  where  all
policy  violations  were  detected  throughout  the  CIP
transactions.
Each of three human reviewers was provided with a
package  that  replicated  all  transaction  information,
including  all  of  the  necessary  meta,  product,  and
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:08:29 UTC from IEEE Xplore.  Restrictions apply. 
included 
information 
filters.  This 
environment  data  for  determining  the  content-based
releasability  of  each  transaction.    Environment  data
included  all  of  the  pertinent  information  needed  for
processing  the  CIPs  with  the  same  state  as  the
automated 
the
requesting user name and organization, time and date of
request,  CIP  size,  and  key-topic  definitions.  The  main
goal  for  the  baseline  was  to  develop  a  “user-friendly”
package  for  the  human  reviewers  in  an  attempt  to
eliminate  ambiguity  and  inconsistency  in  the  review.
The CIPs and policy were converted to HTML for ease
of  use,  and  provided  with  all  the  other  pertinent  data
within  a  clearly 
labeled  directory  structure.  The
reviewers were also given a template in the form of an
their  results.  The
Excel  spreadsheet  to  document 
reviewers  examined  all  of 
the 
transaction  data,
searching for any policy violations while recording key-
topic  detections,  rules  fired,  removal  of  CIP  products,
the  review.
and 
time 
Accomplished 
reviewers
produced  very  similar  results,  although  there  were
differences  stemming  from  key-topic  detections  and
their  context  interpretation.  When  differences  existed
between the reviews, the results were inspected to verify
the  topic  detection  and  context  of  the  key-topic  within
the transaction data, and conflicts in interpretation were
resolved  among  the  reviewers.  After  completing  this
process  of  conflict  resolution,  one  final  GT  document
was  established,  representing  the  baseline  for  this
experiment. While the GT was intended to represent an
ideal assessment of the transaction data based upon the
policy, we fully expected that the human review would
not produce a completely accurate assessment. Fatigue,
boredom,  complexity  of  the  task,  etc.,  can  affect  the
performance  of  human  reviewers.  While  we  later
verified that the human baseline review omitted certain
it 
in  parallel, 
to  complete 
the  human 
the 
took 
key-topic  detections,  it  still  provided  a  valuable  basis
for  comparison  in  judging  how  well  the  syntactic  and
NLP  filters  performed  in  both  accuracy  and  speed  of
review. It also provided evidence that it was possible for
the  automated  filters  to  detect  certain  events  that  the
collaborative, “ideal” human review had missed.
Experiment architecture
For 
run, 
each 
experiment 
Based  on  the  established  goals,  we  simplified
experimentation  to  a  point  where  clear  and  concise
accuracy  and  performance  measurements  could  be
collected,  including  a  simplified  network  topology  and
configuration. The logical experiment topology depicted
in  Figure  3  describes  the  role  of  the  NAI  Labs
Advanced  Research  Guard 
for  Experimentation
(ARGuE)  [7]  boundary  controller  within  the  Genoa
environment. 
66
“transactions”  were  completed,  each  initiated  with  a
request  from  the  client  to  the  CIP  server.  Requests
included information necessary for accessing individual
CIPs,  including  requesting  user,  user  organization,
target organization (location of CIP), and a unique CIP
identifier.  Once  the  client  initiated  a  request,  ARGuE
received  that  request  and  executed  the  filters  to  assess
the  field  contents.  If  the  filters  detected  any  policy
violations  for  the  request,  that  request  was  rejected.  If
the policy allowed, the request was passed on to the CIP
server  where  it  processed  the  request  and  attempted  to
return the CIP to the client. Again, ARGuE executed the
filters  on  the  reply,  examining  both  the  metadata  and
product content of the CIP being returned. If the policy
allowed, 
the  client
unmodified,  completing 
transaction.  If  policy
violations  were detected  within the  CIP, the CIP  could
be  rejected  or  sanitized.  An  associated  policy  action
the  CIP  was  passed  on 
the 
to 
IIOP
NCA Enclave
Client
t
s
e
u
q
R e
C I P
Filters
Policy
t
s
e
u
q
R e
C I P
CIP
Server
ARGuE
boundary
controller
NCA Enclave
Figure 3: Experiment topology
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:08:29 UTC from IEEE Xplore.  Restrictions apply. 
allowed  the  filters  to  sanitize  product  metadata  and/or
remove products in order to satisfy the policy and allow
release.  This  functionality  allows  the  information  flow
to  be  controlled  based  on  current  operational  risks  and
mission  needs, 
the
transaction altogether.
instead  of  simply 
rejecting 
Through 
the  ARGuE  filtering  subsystem,  all
transaction results were logged for inspection. Through
analysis  of  this  data,  we  could  determine  the  strengths
and  weaknesses  of  the  various  filtering  methods.
Experiment  metrics  were  focused  on  the  key-topics
detected,  the  rules  fired,  and  the  products  removed.
Through  additional  manual  review,  these  actions  were
then verified as being correct or incorrect.  Overall, the
experiment  process  included  the  following  stages:  1)
Establish  experiment  baseline  that  represents  all  of  the
policy  violations 
transactions.  2)
Perform  automated  filtering  of  the  transactions  and
collect  logging  data  for  each  filter.  3)  Examine  and
document  differences  between  the  individual  filter
results and the baseline. The collection and  analysis  of
this  data  enabled  a  better  content-based  filtering
solution to be recognized.
throughout 
the 
Accuracy results
second  assumption  was 
Accuracy  statistics  were  collected  for  the  Felt  and
DataShield  filters  by  comparing  their  results  with  the
GT  assessment.  Two  factors  were  vital  in  obtaining  a
precise representation of the accuracy of the two filters.
The  first  was  the  assumption  that  the  GT  was  a  sound
representation  of  the  actual  CIP  policy  violations,  and
the 
the  accuracy
assessments  were  performed  in  the  same  manner  for
both filters. The GT assessment represents a human-in-
the-loop analysis that was fully expected to fall short of
being 100% correct in representing all of the violations
contained  within  the  CIPs.  This  provided  opportunities
for  the  automated  filters  to  detect  policy  violations
undetected by their human counterparts.
that 
resulting 
in  product 
In  gathering  accuracy  statistics,  our  approach
included examining the profile of events within the GT
assessment  and  comparing  these  to  events  recorded
within  the  Felt  and  DataShield  filter  logs.  Low-level
analysis  was  achieved  by  examining  key-topic
detections 
removal  and/or
sanitization  events.  After  examining  the  logging  data,
product  content  violations  were  validated  for  correct
key-topic  areas  as  compared  against  the  GT.  If  a
disparity between the logging data and the GT existed,
additional  human  analysis  of  the  CIP  and  product  was
conducted. This allowed for determination of the correct
action  based  on  the  policy  and  to  double  check  the
validity of the GT.
in 
the 
to  capture 
two  filters 
Spreadsheets  were  composed 
the
transaction  activity  of  both  Felt  and  DataShield  filters.
These  spreadsheets  were  then  summarized  to  provide
specific  differences 
for  each
transaction,  as  compared  against  the  GT.  From  these
details, False Negative (FN), True Negative (TN), True
Positive  (TP),  and  False  Positive  (FP)  statistics  were
gathered.  These  statistics  were  collected  for  product
removal  and  product  content  detection  for  each  of  the
filters.  At  the  lowest  level  of  analysis,  product  content
detection statistics can be defined by the following:
FN:  Content correctly identified by GT, but
      not identified by Felt or DataShield.
TN:  Content incorrectly identified by GT and
          not identified by Felt or DataShield.
TP:  Content correctly identified by Felt or
          DataShield, but not identified by GT.
FP:  Content incorrectly identified by Felt or
         DataShield and not identified by GT.
Similar  statistics  were  obtained 
for  product
removal,  providing  a  rough  accuracy  measure  without
key-topic validation. While this does assess the product
removal  events  of  the  filters  against  GT,  it  does  not
provide insight into why products were removed, and if
they were removed for the correct key-topic detections.
This  metric  is  much  easier  to  obtain,  but  does  not
provide a true assessment of accuracy.
In  assessing  the  overall  accuracy  of  the  Felt  and
DataShield  filters  with  these  statistics,  the  Information
Retrieval  (IR)  concept  of  Precision  and  Recall  was
utilized.  Based  upon  our  baseline  GT,  Precision
corresponds to the ratio of false positive rule violations
detected by  the  filters,  while  Recall  corresponds  to  the
ratio  of  false  negative  rule  violations  detected  by  the
filters.  The  general  formulas  for  Precision  and  Recall
include:
              (# of correctly identified items)
  Precision =  ----------------------------------------
   (total # of identified items)
         (# of correctly identified items)
  Recall =  ------------------------------------------
                   (total possible # of correct items)
For  the  calculation  of  these  statistics,  product
removal  and  key-topic  detection 
instances  were
extracted  from  the  aforementioned  spreadsheets.  In
addition, the total number  of  identified  events  detected
was compiled from the GT assessment, denoted by ‘gt’
in  the  formulas  below.  The  resultant  Precision  and
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:08:29 UTC from IEEE Xplore.  Restrictions apply. 
Recall  formulas  applicable  to  Felt  and  DataShield
statistics  result  in  the  following  four  equations  at  the
product removal and topic detection level of analysis.
Precision (product removal) =
     [(gt–FN–TP) / (gt–FN–TP)+FP] (product removal)
Masked  by  these  precision  and  recall  ratios,  is  the
fact that Felt and DataShield were both able to detect 41
key-topic  instances  that  the  human  reviewers  had
missed.  This  strengthens  the  assertion  that  automated
syntactic and semantic filtering not only supplement one
another,  but  they  also  provide  additional  filtering
accuracy beyond that of an “ideal” human review.
Precision (topic detection) =
     [(gt–FN–TP) / (gt–FN–TP)+FP] (topic detection)
Performance results
Recall (product removal)  =
     [(gt–FN+TP) / (gt+TP)] (product removal)
Recall (topic detection)  =
     [(gt–FN+TP) / (gt+TP)] (topic detection)
Figure  4  summarizes  the  calculation  of  these
statistics,  providing  Felt  and  DataShield  Precision  and
Recall  ratios  for  product  removal  (product)  and  topic
detection (topic).
Ideally,  precision  and  recall  ratios  of  100%  are
desired.  From  these  statistics,  it  is  apparent  that  Felt
experienced  a  higher  level  of  precision  than  that  of
DataShield,  while  DataShield  experienced  higher
Information  Recall  results.  Although  Felt  experienced
only a slightly lower occurrence of false positive events
than  that  of  DataShield,  DataShield  was  able  to
correctly  identify  key-topic  areas,  and  sanitize  or
remove products according to policy with a significantly
higher  accuracy  rate  then  Felt.  Also  visible  from  these
statistics  is  the  variance  in  Product  and  Topic  level
analysis,  strengthening  the  fact  that  simply  examining
the product removal rate is not a good representation of