### Optimized Text

It has been previously observed that free memory should be filled with pseudo-random data [24]. A later code-compression attack [3] indicated that code redundancy also impacts security. However, this attack was dismissed as impractical [16] and ignored in subsequent works [17, 29]. In contrast, we consider the general probability distribution of the state (code and data) as defined in Definition 3 and directly link it to the adversary's advantage. Consequently, one can directly evaluate the honest prover's state and determine whether additional measures, such as filling free memory with pseudo-random data, are necessary to prevent memory copy and code compression attacks.

Our results show that traditional cryptographic assumptions are sometimes too strong (e.g., second pre-image resistance) and sometimes too weak (e.g., pseudo-randomness). Additionally, we have identified new sufficient conditions for the core functionalities of software attestation. Most previous works require the software attestation algorithm to iterate over all memory words of the secondary memory without providing a formal justification. Our bound allows us to identify lower values for \( N \) (if the other parameters are known), enabling more efficient solutions that balance the number of iterations \( N \) and the success probability of a malicious prover. Thus, our work represents the first step towards efficient and provably secure software attestation schemes.

However, several open questions remain for future research. One is to relax the presented conditions or to derive necessary conditions. Another task is to determine concrete instantiations. While the Gen and Chk functions could be easily implemented on devices that provide hardware-assisted cryptographic functions, such as block cipher implementations in hardware similar to the AES instructions in modern CPUs [30], this becomes more challenging on other platforms.

We are currently working on the following aspects: (1) a practical instantiation of the generic software attestation scheme and its evaluation, and (2) the evaluation of existing software attestation schemes within our framework.

### References
[1] T. AbuHmed, N. Nyamaa, and D. Nyang. "Software-based remote code attestation in wireless sensor network." In Global Telecommunications (GLOBECOM). IEEE, 2009.
[2] Atmel. tinyAVR homepage. http://www.atmel.com/tinyavr/, 2013.
[3] C. Castelluccia, A. Francillon, D. Perito, and C. Soriente. "On the difficulty of software-based attestation of embedded devices." In Computer and Communications Security (CCS). ACM, 2009.
[4] A. Datta, J. Franklin, D. Garg, and D. Kaynar. "A logic of secure systems and its application to trusted computing." In Security and Privacy, 2009 30th IEEE Symposium on, 2009.
[5] K. E. Defrawy, A. Francillon, D. Perito, and G. Tsudik. "SMART: Secure and minimal architecture for (establishing a dynamic) root of trust." In Network and Distributed System Security Symposium (NDSS). Internet Society, 2012.
[6] C. Dwork and M. Naor. "Pricing via processing or combatting junk mail." In Advances in Cryptology – CRYPTO. Springer, 1993.
[7] C. Dwork, M. Naor, and H. Wee. "Pebbling and proofs of work." In Advances in Cryptology – CRYPTO. Springer, 2005.
[8] A. Francillon, Q. Nguyen, K. B. Rasmussen, and G. Tsudik. "Systematic treatment of remote attestation." Cryptology ePrint Archive, Report 2012/713, 2012. http://eprint.iacr.org/.
[9] J. Franklin, M. Luk, A. Seshadri, and A. Perrig. "PRISM: Human-verifiable code execution." Technical report, Carnegie Mellon University, 2007.
[10] R. W. Gardner, S. Garera, and A. D. Rubin. "Detecting code alteration by creating a temporary memory bottleneck." Trans. Info. For. Sec., 4(4):638–650, 2009.
[11] J. T. Griffin, M. Christodorescu, and L. Kruger. "Strengthening software self-checksumming via self-modifying code." In Annual Computer Security Applications Conference (ACSAC). IEEE, 2005.
[12] V. Graizer and D. Naccache. "Alien vs. Quine." Security & Privacy, IEEE, 5(2):26–31, 2007.
[13] V. Gratzer and D. Naccache. "Alien vs. Quine, the vanishing circuit and other tales from the industry’s crypt." In Advances in Cryptology – EUROCRYPT. Springer, 2007. Invited Talk.
[14] M. Jakobsson and K.-A. Johansson. "Retroactive Detection of Malware With Applications to Mobile Platforms." In Workshop on Hot Topics in Security (HotSec). USENIX, 2010.
[15] R. Kennell and L. H. Jamieson. "Establishing the genuineness of remote computer systems." In USENIX Security Symposium. USENIX, 2003.
[16] X. Kovah, C. Kallenberg, C. Weathers, A. Herzog, M. Albin, and J. Butterworth. "New results for Timing-Based attestation." In Security and Privacy (S&P). IEEE, 2012.
[17] Y. Li, J. M. McCune, and A. Perrig. "VIPER: verifying the integrity of PERipherals’ firmware." In Computer and Communications Security (CCS). ACM, 2011.
[18] B. Parno, J. M. McCune, and A. Perrig. "Bootstrapping Trust in Commodity Computers." In Security and Privacy (S&P). IEEE, 2010.
[19] A.-R. Sadeghi, S. Schulz, and C. Wachsmann. "Lightweight remote attestation using physical functions." In Wireless Network Security (WiSec). ACM, 2011.
[20] D. Schellekens, B. Wyseur, and B. Preneel. "Remote attestation on legacy operating systems with Trusted Platform Modules." Sci. Comput. Program., 74(1-2):13–22, 2008.
[21] A. Seshadri, M. Luk, and A. Perrig. "SAKE: Software attestation for key establishment in sensor networks." Distributed Computing in Sensor Systems, pages 372–385, 2008.
[22] A. Seshadri, M. Luk, A. Perrig, L. van Doorn, and P. Khosla. "SCUBA: Secure code update by attestation in sensor networks." In Workshop on Wireless Security (WiSe). ACM, 2006.
[23] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. van Doorn, and P. Khosla. "Pioneer: Verifying integrity and guaranteeing execution of code on legacy platforms." In Symposium on Operating Systems Principles (SOSP). ACM, 2005.
[24] A. Seshadri, A. Perrig, L. van Doorn, and P. K. Khosla. "SWATT: SoftWare-based ATTestation for embedded devices." In Security and Privacy (S&P). IEEE, 2004.
[25] M. Shaneck, K. Mahadevan, V. Kher, and Y. Kim. "Remote software-based attestation for wireless sensors." In Security and Privacy in Ad-hoc and Sensor Networks. Springer, 2005.
[26] U. Shankar, M. Chew, and J. D. Tygar. "Side effects are not sufficient to authenticate software." In USENIX Security Symposium. USENIX, 2004.
[27] R. Strackx, F. Piessens, and B. Preneel. "Efficient isolation of trusted subsystems in embedded systems." In Security and Privacy in Communication Networks (SecureComm). Springer, 2010.
[28] Trusted Computing Group (TCG). "TPM Main Specification, Version 1.2," 2011.
[29] A. Vasudevan, J. Mccune, J. Newsome, A. Perrig, and L. V. Doorn. "CARMA: A hardware tamper-resistant isolated execution environment on commodity x86 platforms." In ACM Symposium on Information, Computer and Communications Security (AsiaCCS). ACM, 2012.
[30] Wikipedia. "AES instruction set." http://en.wikipedia.org/wiki/AES_instruction_set, 2013.
[31] Q. Yan, J. Han, Y. Li, R. H. Deng, and T. Li. "A software-based root-of-trust primitive on multicore platforms." In ACM Symposium on Information, Computer and Communications Security (AsiaCCS). ACM, 2011.
[32] Y. Yang, X. Wang, S. Zhu, and G. Cao. "Distributed software-based attestation for node compromise detection in sensor networks." In Symposium on Reliable Distributed Systems (SRDS). IEEE, 2007.

### Appendix

#### A. Details of the Proof

We prove the upper bound of \( \Pr[\text{Win1}|\text{Coll}(N_{\text{coll}})] \) used in Game 1 of the proof of Theorem 1. Observe that \( 0 \leq N_{\text{less}} \) and \( 0 \leq N_{\text{equal}} = N - (ops + 1) \cdot (N_{\text{coll}} + N_{\text{less}}) \Leftrightarrow N_{\text{less}} \leq \frac{N}{ops + 1} - N_{\text{coll}} \), i.e., \( 0 \leq N_{\text{less}} \leq \frac{N}{ops + 1} - N_{\text{coll}} \). To simplify the first term \( \lambda^{N-(ops+1) \cdot (N_{\text{coll}} + N_{\text{less}})} \cdot \gamma^{N_{\text{less}}} \), we define \( e := \log_{\lambda}(\gamma) \) and rephrase it as \( \lambda^{N-(ops+1) \cdot N_{\text{coll}}-(ops+1-e) \cdot N_{\text{less}}} \).

When \( ops + 1 - e < 0 \), the maximum is achieved for \( N_{\text{less}} = 0 \), hence in this case the upper bound is \( \lambda^{N-(ops+1) \cdot N_{\text{coll}}} \). In the other case, we get an upper bound for \( N_{\text{less}} = \frac{N}{ops+1} - N_{\text{coll}} \), yielding:
\[
\lambda^{N-(ops+1) \cdot N_{\text{coll}}-(ops+1-e) \cdot \left(\frac{N}{ops+1} - N_{\text{coll}}\right)} = \gamma^{\frac{N}{ops+1} - N_{\text{coll}}}.
\]

With respect to the second term, i.e., \( \gamma^{N-(ops+1) \cdot N_{\text{coll}}-ops \cdot N_{\text{less}}} \), the maximum value is achieved if \( N_{\text{less}} \) is as big as possible, i.e., \( N_{\text{less}} = \frac{N}{ops+1} - N_{\text{coll}} \). This gives an upper bound of:
\[
\gamma^{N-(ops+1) \cdot N_{\text{coll}}-ops \cdot N_{\text{less}}} = \gamma^{\frac{N}{ops+1} - N_{\text{coll}}}.
\]

Altogether, it follows that:
\[
\Pr[\text{Win1}|\text{Coll}(N_{\text{coll}})] \leq \left( \max \left\{ \lambda^{ops(\delta_{\text{Read}})+1}, \gamma^{\frac{N}{ops+1} - N_{\text{coll}}} \right\} \right).
\]

#### B. Example for Upper Bound

To better understand the upper bound (Equation 1) given in Theorem 1, especially regarding the impact of the similarity of the malicious prover's state to the honest prover's state expressed by \( \lambda \) and the number of rounds \( N \), we provide some concrete examples in this section.

First, we fix various parameters. We consider typical parameters for \( lg \) and \( lr \) found in the literature on software attestation. We assume that all cryptographic primitives are perfectly secure and that the values in \( S \) are uniformly distributed:
\[
\omega := 2^{-lr}, \quad \nu_{\text{Chk}} = 0, \quad \epsilon := 0, \quad \gamma := 2^{-ls}, \quad \nu_{\text{Gen}} := 0.
\]

The bound in Equation 1 then simplifies to:
\[
\Pr[\text{Win1}] \leq \left( \pi(M, ops) \cdot 2^{-ls \cdot (N-M)} \right) + 2^{-lr} + \max_{0 \leq M \leq N} \left\{ \pi(M, ops) \cdot 2^{-ls \cdot (N-M)} \right\}.
\]

Recall that the value \( ops \) is defined as the number of operations a prover can perform in time \( \delta_{\text{Read}} + \delta_{\text{Gen}} \). It was used in the proof to address the question: If an attacker decides to skip one round, for how many other rounds can he increase his probability of success? While \( ops \) certainly expresses an upper bound on this number (the adversary has to spend at least one instruction per round), it is an overestimation of the adversary's capabilities. Hence, we set \( ops = 2 \) to get more meaningful results. This represents an adversary who can win two rounds if he skips another round.

Recall that \( \lambda \) expresses the fraction of state entries where the state of the malicious prover matches with the state \( S \) of the honest prover. We exemplarily use \( \lambda \in \{0.2, \ldots, 0.8\} \). As shown in Figure 2, for small values of \( \lambda \) (i.e., for malicious provers with a state that is quite different from the honest prover's state), a relatively low number of rounds is sufficient to achieve a reasonably low adversary advantage. However, for large values of \( \lambda \), more rounds are required. Further, for the chosen system parameters, the advantage seems to converge to a minimal adversary advantage of \( 10^{-48} \).

Observe that in the literature, it is often suggested to use \( N = \log(s) \cdot s \) rounds. Interestingly, our experiments indicate that significantly fewer rounds can be sufficient.

\[
\begin{array}{c|c}
\text{Adversary Success Probability (log)} & \text{Number of checksum iterations } N \\
\hline
10^{-50} & 0 \\
10^{-45} & 20 \\
10^{-40} & 40 \\
10^{-35} & 60 \\
10^{-30} & 80 \\
10^{-25} & 100 \\
10^{-20} & 120 \\
10^{-15} & 140 \\
10^{-10} & 160 \\
10^{-5} & 180 \\
1 & 200 \\
\end{array}
\]

\[
\begin{array}{c|c|c|c|c}
\lambda & 0.2 & 0.4 & 0.6 & 0.8 \\
\hline
\text{Adversary Success Probability (log)} & 10^{-50} & 10^{-45} & 10^{-40} & 10^{-35} \\
\end{array}
\]