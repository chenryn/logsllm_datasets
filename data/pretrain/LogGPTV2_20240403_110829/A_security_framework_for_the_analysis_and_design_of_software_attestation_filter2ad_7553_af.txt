been observed before that free memory should be ﬁlled with
pseudo-random data [24] and a later code-compression at-
tack [3] indicated that code redundancy also impacts secu-
rity. However, the attack was dismissed as impractical [16]
and ignored in later works [17, 29]. In contrast, we consider
the general probability distribution of the state (code and
data) in Deﬁnition 3 and directly connect it to the adversary
advantage. As a result, one can directly evaluate the honest
prover state and determine whether additional measures to
achieve state incompressibility, such as ﬁlling free memory
with pseudo-random data, are required to prevent memory
copy and code compression attacks.
Our results also show that traditional cryptographic as-
sumptions are partially too strong (second pre-image resis-
tance) and partially too weak (pseudo-randomness). Fur-
ther, we identiﬁed new (suﬃcient) conditions on the core
functionalities of software attestation. Most previous works
require the software attestation algorithm to iterate over
all memory words of the secondary memory without giving
any formal justiﬁcation. Our bound allows to identify lower
values for N (if the other parameters are known), enabling
more eﬃcient solutions that provide a tradeoﬀ between the
number of iterations N and the success probability of a mali-
cious prover. Thus our work represents the ﬁrst step towards
eﬃcient and provably secure software attestation schemes.
Still, several open questions remain for future work. One
being to relax the presented conditions or to derive necessary
conditions. A further task is to determine concrete instan-
tiations. While Gen and Chk could be easily realized on
devices that provide hardware-assisted cryptographic func-
tions, such as block cipher implementations in hardware
similar to the AES instructions in modern CPUs [30], this
becomes more challenging on other platforms.
We are currently working on the following aspects: (1) a
practical instantiation of the generic software attestation
scheme and its evaluation and (2) the evaluation of exist-
ing software attestation schemes in our framework.
8. REFERENCES
[1] T. AbuHmed, N. Nyamaa, and D. Nyang.
Software-based remote code attestation in wireless
sensor network. In Global Telecommunications
(GLOBECOM). IEEE, 2009.
[2] Atmel. tinyAVR homepage.
http://www.atmel.com/tinyavr/, 2013.
[3] C. Castelluccia, A. Francillon, D. Perito, and
C. Soriente. On the diﬃculty of software-based
attestation of embedded devices. In Computer and
Communications Security (CCS). ACM, 2009.
[4] A. Datta, J. Franklin, D. Garg, and D. Kaynar. A
logic of secure systems and its application to trusted
computing. In Security and Privacy, 2009 30th IEEE
Symposium on, 2009.
[5] K. E. Defrawy, A. Francillon, D. Perito, and
G. Tsudik. SMART: Secure and minimal architecture
for (establishing a dynamic) root of trust. In Network
and Distributed System Security Symposium (NDSS).
Internet Society, 2012.
[6] C. Dwork and M. Naor. Pricing via processing or
combatting junk mail. In Advances in Cryptology –
CRYPTO. Springer, 1993.
[7] C. Dwork, M. Naor, and H. Wee. Pebbling and proofs
of work. In Advances in Cryptology – CRYPTO.
Springer, 2005.
[8] A. Francillon, Q. Nguyen, K. B. Rasmussen, and
G. Tsudik. Systematic treatment of remote
attestation. Cryptology ePrint Archive, Report
2012/713, 2012. http://eprint.iacr.org/.
[9] J. Franklin, M. Luk, A. Seshadri, and A. Perrig.
PRISM: Human-veriﬁeable code execution. Technical
report, Carnegie Mellon University, 2007.
[10] R. W. Gardner, S. Garera, and A. D. Rubin. Detecting
code alteration by creating a temporary memory
bottleneck. Trans. Info. For. Sec., 4(4):638–650, 2009.
[11] J. T. Giﬃn, M. Christodorescu, and L. Kruger.
Strengthening software self-checksumming via
self-modifying code. In Annual Computer Security
Applications Conference (ACSAC). IEEE, 2005.
[12] V. Graizer and D. Naccache. Alien vs. Quine. Security
Privacy, IEEE, 5(2):26 –31, 2007.
[13] V. Gratzer and D. Naccache. Alien vs. Quine, the
vanishing circuit and other tales from the industry’s
crypt. In Advances in Cryptology – EUROCRYPT.
Springer, 2007. Invited Talk.
[14] M. Jakobsson and K.-A. Johansson. Retroactive
Detection of Malware With Applications to Mobile
Platforms. In Workshop on Hot Topics in
Security (HotSec). USENIX, 2010.
[15] R. Kennell and L. H. Jamieson. Establishing the
genuinity of remote computer systems. In USENIX
Security Symposium. USENIX, 2003.
[16] X. Kovah, C. Kallenberg, C. Weathers, A. Herzog,
M. Albin, and J. Butterworth. New results for
Timing-Based attestation. In Security and Privacy
(S&P). IEEE, 2012.
[17] Y. Li, J. M. McCune, and A. Perrig. VIPER: verifying
the integrity of PERipherals’ ﬁrmware. In Computer
and Communications Security (CCS). ACM, 2011.
[18] B. Parno, J. M. McCune, and A. Perrig.
Bootstrapping Trust in Commodity Computers. In
Security and Privacy (S&P). IEEE, 2010.
[19] A.-R. Sadeghi, S. Schulz, and C. Wachsmann.
Lightweight remote attestation using physical
functions. In Wireless Network Security (WiSec).
ACM, 2011.
[20] D. Schellekens, B. Wyseur, and B. Preneel. Remote
attestation on legacy operating systems with Trusted
Platform Modules. Sci. Comput. Program.,
74(1-2):13–22, 2008.
[21] A. Seshadri, M. Luk, and A. Perrig. SAKE: Software
attestation for key establishment in sensor networks.
Distributed Computing in Sensor Systems, pages
372–385, 2008.
[22] A. Seshadri, M. Luk, A. Perrig, L. van Doorn, and
P. Khosla. SCUBA: Secure code update by attestation
in sensor networks. In Workshop on Wireless
security (WiSe). ACM, 2006.
[23] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. van Doorn,
and P. Khosla. Pioneer: Verifying integrity and
guaranteeing execution of code on legacy platforms. In
Symposium on Operating Systems Principles (SOSP).
ACM, 2005.
11[24] A. Seshadri, A. Perrig, L. van Doorn, and P. K.
Khosla. SWATT: SoftWare-based ATTestation for
embedded devices. In Security and Privacy (S&P).
IEEE, 2004.
[25] M. Shaneck, K. Mahadevan, V. Kher, and Y. Kim.
Remote software-based attestation for wireless sensors.
In Security and Privacy in Ad-hoc and Sensor
Networks. Springer, 2005.
[26] U. Shankar, M. Chew, and J. D. Tygar. Side eﬀects
are not suﬃcient to authenticate software. In USENIX
Security Symposium. USENIX, 2004.
[27] R. Strackx, F. Piessens, and B. Preneel. Eﬃcient
isolation of trusted subsystems in embedded systems.
In Security and Privacy in Communication Networks
(SecureComm). Springer, 2010.
[28] Trusted Computing Group (TCG). TPM Main
Speciﬁcation, Version 1.2, 2011.
[29] A. Vasudevan, J. Mccune, J. Newsome, A. Perrig, and
L. V. Doorn. CARMA: A hardware tamper-resistant
isolated execution environment on commodity x86
platforms. In ACM Symposium on Information,
Computer and Communications Security (AsiaCCS).
ACM, 2012.
[30] Wikipedia. AES instruction set. http:
//en.wikipedia.org/wiki/AES_instruction_set,
2013.
[31] Q. Yan, J. Han, Y. Li, R. H. Deng, and T. Li. A
software-based root-of-trust primitive on multicore
platforms. In ACM Symposium on Information,
Computer and Communications Security (AsiaCCS).
ACM, 2011.
[32] Y. Yang, X. Wang, S. Zhu, and G. Cao. Distributed
software-based attestation for node compromise
detection in sensor networks. In Symposium on
Reliable Distributed Systems (SRDS). IEEE, 2007.
N
APPENDIX
A. DETAILS OF THE PROOF
We prove the upper bound of Pr [Win1|Coll(Ncoll)] used in
Game 1 of the proof of Theorem 1. Observe that 0 ≤ Nless
and 0 ≤ Nequal = N − (ops + 1) · (Ncoll + Nless) ⇔ Nless ≤
ops+1 − Ncoll, i.e., 0 ≤ Nless ≤ N
ops+1 − Ncoll. To simplify
the ﬁrst term λN−(ops+1)·(Ncoll+Nless) · γNless , we deﬁne e :=
logλ(γ) and rephrase it as λN−(ops+1)·Ncoll−(ops+1−e)·Nless .
When ops+1−e < 0, the maximum is achieved for Nless =
0, hence in this case the upper bound is λN−(ops+1)·Ncoll . In
ops+1 −
the other case we get an upper bound for Nless = N
Ncoll, yielding
(cid:16) N
(cid:17)
λN−(ops+1)·Ncoll−(ops+1−e)·
ops+1 −Ncoll
= γ
N
ops+1 −Ncoll .
With respect to the second term, i.e., γN−(ops+1)·Ncoll−ops·Nless ,
the maximum value is achieved if Nless is as big as possible,
i.e., Nless = N
ops+1 − Ncoll. This gives an upper bound of
γN−(ops+1)·Ncoll−ops·Nless = γ
N
ops+1 −Ncoll .
Altogether, it follows that
Pr [Win1|Coll(Ncoll)]
≤(cid:16)
(cid:110)
max
λops(δRead)+1, γ
(cid:111)(cid:17)
N
ops(δRead )+1 −Ncoll
.
Figure 2: Upper Bound (Equation 1) for diﬀerent values of
N and λ.
B. EXAMPLE FOR UPPER BOUND
To get a better intuition of the upper bound (Equation 1)
given in Theorem 1, especially on the impact of the similarity
of the malicious prover’s state to the honest prover’s state
expressed by λ and the number of rounds N , we provide
some concrete examples in this section.
At ﬁrst we have to ﬁx various parameters. To this end, we
consider typical parameters for lg and lr that can be found in
the literature on software attestation. Moreover, we assume
that all cryptographic primitives are perfectly secure and
that the values in S are uniformly distributed:
ω := 2−lr , νChk = 0,  := 0, γ := 2−ls , νGen := 0.
The bound in Equation 1 then simpliﬁes to
(cid:110)
(π (M, ops)) · 2−ls·(N−M )(cid:111)
.
p + s
+ 2−lr + max
0≤M≤N
ls/lr · 2(lg +lr )
Recall that the value ops has been deﬁned as the number
of operations a prover can perform in time δRead + δGen. It
was used in the proof to tackle the following question: If
an attacker decides to skip one round, for how many other
rounds can he increase his probability of success? While
ops certainly expresses an upper bound on this number (the
adversary has to spend at least one instruction per round), it
certainly is an overestimation of the adversary’s capabilities.
Hence, we set ops = 2 to get more meaningful results. This
represents an adversary who can win two rounds if he skips
another round.
Recall that λ expresses the fraction of state entries where
the state of the malicious prover matches with the state S
of the honest prover. We exemplarily use λ ∈ {0.2, . . . , 0.8}.
As shown in Figure 2, for small values of λ (i.e., for malicious
provers with a state that is quite diﬀerent from the honest
prover’s state), a relatively low number of rounds is suﬃcient
to achieve a reasonably low adversary advantage. However,
for large values of λ more rounds are required. Further,
for the chosen system parameters, the advantage seems to
converge to a minimal adversary advantage of 10−48.
Observe that in the literature, it is often suggested to use
N = log(s)·s rounds. Interestingly, our experiments indicate
that signiﬁcantly less rounds can be suﬃcient.
 1e-50 1e-45 1e-40 1e-35 1e-30 1e-25 1e-20 1e-15 1e-10 1e-05 1 0 20 40 60 80 100 120 140 160 180 200Adversary Success Probability (log)Number of checksum iterations Nlambda=0.2lambda=0.4lambda=0.6lambda=0.812