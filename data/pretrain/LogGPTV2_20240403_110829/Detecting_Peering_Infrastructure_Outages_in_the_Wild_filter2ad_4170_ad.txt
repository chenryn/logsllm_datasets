different non-sibling far-end ASes that are disjoint. From that, we
infer a PoP-level incident if at least three different AS-level and
operator-level incidents occur in the same binning interval at the
same PoP. Next, we refine our localization for PoP-level outages.
Disambiguation of Outage Signals: Recall, from Figure 2, that
the physical connectivity between two ASes can involve multiple
physical PoPs. With ingress communities we can only identify PoPs
at the near-end of an AS pair. However, depending on the peering
strategy, which includes private peering and local or remote public
peering, there may be up to four facilities between the ingress
PoP and the far-end AS. A failure in any of them will trigger an
outage signal at the near-end facility. To disambiguate such signals
we correlate outage signals from multiple PoPs, combined with
our colocation map. Our assumption is that outages at the near-
end facility, the one identified by the ingress community of an AS,
should affect all paths tagged with this community that involve links
with far-end ASes co-located in the same facility. More specifically,
we infer the outage in the near-end facility if at least 95% of the
paths with co-located ASes are affected. We allow for a 5% margin
to account for possible inaccuracies in the colocation map, such as
spurious AS-to-facility presences, based on the results in [50].
If this is not the case, we check if the outage location is among
the facilities where the affected far-end ASes have a presence. Ac-
cordingly, we repeat the above process for all facilities where any
of the remote ASes has a presence and for which we have an outage
signal in this binning interval. Figure 2(c) illustrates this process.
When we infer that the near-end facility is not the outage epicenter,
and the far-end peers have no facility in common (after checking
the colocation map) we increase the PoP granularity to IXP-level
and we repeat the same process. Namely, we collect the common
IXPs among the near-end and the far-end peers and we check if all
the common IXP members have been affected, e.g., in Figure 2(c)
the outage source is IX1 and not F3 or F4. If we fail to converge to
a single IXP as the outage source, we cannot make an inference
and resort to targeted traceroute queries to discover the outage
source. If during a binning interval we successfully converge to a
facility/IXP for multiple outage signals, and all the facilities/IXPs
operate in the same city, we abstract the granularity of the incident
to city-level.
Increasing Signal Resolution: Unfortunately, communities are
not always PoP specific but coarser, e.g., only at the IXP level (colo-
cated IXP). To further refine our inference, we utilize again the
colocation maps. For outage signals with IXP communities we
check if all IXP peers or only IXP peers in specific facilities are not
reachable. Thus, we check for each facility that the IXP is involved
only if all routes of that facility are affected. If this is the case, we
can infer that the outage is at the facility rather than the IXP, e.g.,
at F2 and not IX1 in Figure 2(b). We follow a similar methodology
for outage signals detected using city-level communities, with the
additional step of checking for IXP-level failures, if we infer that
the outage did not occur in a facility.
4.4 Data-Plane Analysis
Kepler validates the occurrence and determines the outage duration
via data-plane analysis, using both archived and targeted traceroute
queries. We again initialize the analysis with a set of stable paths,
whereby, we focus on paths that cross the monitored facilities
and IXPs. To construct an extensive set of stable paths without
incurring high measurement cost, we follow an approach similar to
PathCache [95] and consume the publicly available traceroute paths
collected by RIPE Atlas [90], CAIDA’s Ark [15], and iplane [69].
Kepler also has an interface to initiate traceroute campaigns using
public probing platforms [48, 90]. For mapping the traceroutes
to ASes, IXPs, facilities, and data sanitization, we use techniques
proposed in [19, 50, 76]. The facility mapping part is the only one
that requires active measurement. To keep the number of required
active measurements low, we focus on the ASes that are not covered
by our community dictionary, yet are colocated at the facilities of
interest.
Since we use opportunistic measurements for our baseline set of
paths, we have to focus on subpaths. Namely, if an AS pair appears
to consistently interconnect over the same IXP or facility hops in the
traces of the last four consecutive weekly path dumps, we include
Detecting Peering Infrastructure Outages in the Wild
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
(a) Outages at PoP-levels with diff. thresholds.
(b) Detection sensitivity of our system.
(c) Fraction of IPv4 and IPv6 BGP path in 2016 with
at least one location community.
Figure 7: Tuning parameters for Kepler (a) and (b), and Fraction of updates with location communities (c).
the corresponding paths in our baseline dataset. This approach may
remove some AS pairs with very diverse interconnection footprint
which is desirable for the purpose of confirming outages, since path
changes between AS pairs with low path diversity are less likely to
reflect intra-domain routing changes.
When Kepler detects an outage for a PoP, it identifies the baseline
paths of AS pairs that interconnect over the PoP. Next, it selects the
same sources and destinations and repeats the traceroute queries.
If the fraction of baseline paths that continues to cross the PoP
is below a threshold Tf ail , we confirm the outage and continue
probing to determine the duration of the outage. Otherwise, we
either have a false-positive in our outage inference, or the service
was restored in the mean time. Unfortunately, there is a 5 to 15
minute lag in receiving BGP updates. To eliminate false positives, we
continue the traceroute analysis until the next set of BGP updates.
If the outage signal is still in the BGP data, but the traceroutes did
not confirm the outage, we conclude that we have a false-positive
and disregard it.
When over 50% of the paths (traceroute if available/BGP oth-
erwise) return to the baseline we consider the outage as restored.
However, for a number of outages we observe periods of oscillations.
When two consecutive outages for the same PoP are separated by
less than 12 hours, we conclude that they are part of the same inci-
dent. Its downtime is the sum of the individual outage durations.
5 KEPLER EVALUATION
In this section we present a data-driven evaluation of Kepler’s capa-
bilities. We first analyze the detection sensitivity of our algorithm,
and how we tune Kepler to optimize the detection of PoP-level
outages. We then discuss the reach of Kepler and its limitations,
and we present our validation efforts to understand its accuracy
and precision.
5.1 Sensitivity and Calibration
Kepler has two main parameters: (i) the time window for determin-
ing stable paths and (ii) the threshold which triggers an outage sig-
nal (Tf ail ). For the stable paths, a window smaller than 1 day would
include transient paths, while windows higher than 5 days yield
small sets of stable paths that restrict Kepler’s coverage. Therefore,
we use two days to obtain a stable yet extensive baseline of paths.
Kepler is more sensitive to the threshold parameter, as shown in
Figure 7a. For 2016, it shows the number of detected outage signals
at link-level, AS-level, and facility/IXP-level for thresholds ranging
from 2% to 50%. We assess the efficiency of the different threshold
levels by validating the control-plane outage signals against the
data-plane measurements for each signal. The number of detected
facility/IXP-level outages, which is our focus, remains stable for
thresholds from 2% to 15%. Higher thresholds lead to missing out-
age signals that have been confirmed by concurrent traceroute path
changes. The missed outages are partial, i.e., outages limited to
certain systems of a facility/IXP and affect a subset of its members.
On the other hand, thresholds below 2% increase the number of
outages that have to be investigated, and lead to mis-classification
of AS-level and link-level outages as PoP-level. Note, that some of
the additional outage signals raised for low thresholds may capture
partial outages of limited impact that traceroute measurements fail
to detect. We select a threshold of 10% to be relatively conservative
and minimize wrong inferences, while still being able to capture
medium-scale partial outages.
5.2 Data Analysis Reach and Coverage
A natural question is what fraction of BGP paths, can be analyzed
with Kepler. Figure 7c shows the fraction of IPv4 and IPv6 BGP
updates per month in 2016 with at least one location-encoding
community. About 50% of the IPv4 and 30% of the IPv6 paths in-
clude such communities and, thus, are usable by Kepler. Moreover,
Kepler’s communities consistently tag over 35% of the IPv4 and 28%
of the IPv6 AS links across every BGP snapshot. One reason for
the larger fraction of IPv4 paths/links compared to IPv6 is that ISPs
still focus less on optimizing IPv6 traffic flows.
The next question is at what fraction of the facilities can Kepler
uncover outages. We define a facility as trackable if it has a mini-
mum number of networks whose interconnections can be located
by the communities in Kepler’s dictionary so that our methodology
is applicable. To distinguish PoP-level from AS-level or link-level
incidents, we rely on correlation of updates from multiple members
and we require that we have at least six different members that can
be located through communities, 3 at the near-end of a link, and 3
at the far-end. The colocation databases we mined in Section 3.3
include 1, 742 facilities with at least one AS member. For each of the
1, 742 facilities, Figure 7b shows the total number of their members
compared to the ones that are trackable. There are 1, 209 facilities
with less than 6 members, thus, in principle, we can track 533 fa-
cilities. Of these we miss 130 (24%) for which we currently have
0.10.20.30.40.5Threshold0510152025303540Facility/IXoutagesFacility/IX-level(lefty-axis)AS-level(1strighty-axis)Link-level(2ndrighty-axis)20003000400050006000700080009000ASoutages020000400006000080000100000120000Linkoutages100101102103Totalnumberofmembers(log)100101102103Mappedmembers(log)TrackablefacilitiesNon-trackablefacilities24681012Month0.00.10.20.30.40.50.6FractionofBGPpathsMappedIPv4pathsMappedIPv6pathsSIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
V. Giotsas et al.
Table 1: Facilities coverage per continent
Continent
Facilities
Europe
North America
Asia/Pacific
South America
Africa
All
878
529
233
76
26
>5 members Trackable
305
132
70
19
6
243
105
46
11
4
less than 6 trackable members. Therefore, the detected outages by
Kepler are a lower bound of all possible outages. Note that while
for the trackable facilities we are able to detect all full outages, it is
possible that some partial outages may be undetected depending
on the number of affected trackable facility members. Given the
increase in the community usage and in member ASes we expect
these numbers to increase over the next years. Importantly, we
are able to cover 180 out of 183 (98%) facilities with at least 20
members which are the most prominent interconnection hubs.3
Table 1 breaks down the covered facilities per continent. Kepler’s
coverage is better for Europe and North America, while Africa and
South America have the smallest fraction of trackable facilities.
Note over 80% of all the facilities included in the colocation datasets
(PeeringDB, DataCenterMap) are located either in Europe or in
North America. While the low number of facilities in the other
regions may indicate a geographical bias in the available colocation
databases, the European and North American peering ecosystems
are significantly more developed, with 73% of all the ASNs and 70%
of all IPv4 addresses assigned to countries in the RIPE and ARIN
zones.
5.3 Validation
We first check the accuracy and completeness of our PoP inference
via communities, by obtaining ground-truth data of the facility-
level interconnections from three large ISPs and one major CDN via
private communication that use BGP location communities. Each
gave us their list of facilities with neighbor ASes—in total location
information for roughly 5K AS pairs. We find that our community-
based localization is correct in every case, which is not surprising
given the operational importance of communities. From Figure 8a,
which plots the fraction of AS links vs. the number of facilities (the
main plot is zoomed-in for AS links with more than 1 PoP), we
see that we are missing less than 5% of the interconnections. On
the side, we find that a large fraction of AS pairs only involves a
single physical location. 60% are multilateral peerings between net-
works co-located at a single IXP, while the rest are interconnections
between stub ASes and their transit providers. Still, a significant
number of AS pairs involves many physical locations, in particular,
if the two ASes are tier-1 or tier-2 ASes and peer with each other.
We then check the accuracy of Kepler’s inferences. We consider
as true-positive any inferred outage for which we find an external
data source that confirms the outage occurred in the same facil-
ity/IXP at the same time. Validating false-positives, i.e., inferred
outages that did not happen, is more challenging since it is possible
that an outage was not publicly reported, or that it was reported in
a source that we could not discover. Nonetheless, we consider as
3 Two of the non-trackable facilities with more than 20 members are in India and the
other in Argentina.
false-positives incidents that happened in the same location/time
as an inferred outage, but affected different infrastructures from the
inferred one. We consider as false-negative any outage reported by
an external data source which affected a trackable facility, but for
which Kepler did not infer the outage. To collect validation data we
parsed messages in the NANOG and Outages mailing lists [67, 74],
news articles from specialized websites [25, 26], incident reports