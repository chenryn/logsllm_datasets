title:Effective Testing and Debugging Techniques for a Group Communication
System
author:Eitan Farchi and
Gabriel Kliot and
Yoel Krasny and
Alex Krits and
Roman Vitenberg
Effective Testing and Debugging Techniques for a Group Communication System
Eitan Farchi
Gabi Kliot
Yoel Krasny, Alex Krits, Roman Vitenberg
HRL, IBM Haifa, Israel
PI:EMAIL
Technion, Haifa, Israel
PI:EMAIL
HRL, IBM Haifa, Israel
{yoelk,krits,romanv}@il.ibm.com
Abstract
View-oriented group communication is an important
and widely used building block for constructing highly-
available fault-tolerant systems. Unfortunately, group-
communication based systems are extremely hard to test
and debug due to a number of stateful complex algorithms
deployed in parallel and the unique combination of distrib-
uted and concurrent programming paradigms that ampliﬁes
the non-determinism in the system behavior.
In this work, we elaborate on the speciﬁc challenges we
encountered during the process of testing DCS, a group
communication component of the WebSphere (WAS) archi-
tecture, as well as on the methodology we have devised and
employed in order to cope with these challenges. Our so-
lution relies on a carefully compiled set of invariants that
need to be preserved at every execution point and a log an-
alyzer algorithm that performs cross-log veriﬁcation for all
the processes participating in the execution, as well as on of
other techniques whose details are described in the paper.
1
Introduction
View-oriented group communication is an important
and widely used building block for constructing highly-
available fault-tolerant systems. Group communication
toolkits have been successfully deployed over ﬁfteen years
in mission-critical applications such as air-trafﬁc control
and handling emergency calls as well as in stock exchange
and system management applications.
However, being an inherently complex blend of state-
of-the-art distributed algorithms with several software engi-
neering paradigms, group communication systems are noto-
riously challenging to test and debug. The explosively large
number of states in various algorithms that are employed in
the system at the same time precludes the possibility of full
state coverage. A great number of factors, such as the ef-
fect of execution timing in the operating system, hardware
and software timers that are used for a variety of timeouts
coupled with unpredictable latencies of message delivery,
contribute to the non-deterministic system behavior that im-
pedes system testing and debugging. Being highly concur-
rent, the system requires complex thread synchronization
that potentially leads to elusive deadlocks and unexpected
scheduling scenarios, which are hard to recreate in such a
testing-hostile environment.
In this work, we elaborate on the speciﬁc testing method-
ology we have developed and employed to address the chal-
lenges in testing DCS, a group communication component
of the Java-based WebSphere (WAS) architecture. Figure 1
depicts the commonly known testing cycle. Our previous
work [3] described the test generation procedure in detail.
The main contribution of this paper is in the automated run-
time test monitoring and post-mortem distributed log analy-
sis that a) detect a signiﬁcant portion of bugs while mini-
mizing the manual tester involvement, and b) facilitate root
cause analysis and reduce its time. In the heart of our so-
lution lies a carefully composed set of invariants that need
to be preserved at every execution point. The invariants are
veriﬁed partly by the test monitor at runtime and partly at
the stage of log analysis. To this end, the log analyzer per-
forms cross-log veriﬁcation of events that occur at different
sites by utilizing both the real time of events and their log-
ical timestamp. In addition, the test monitor facilitates root
cause analysis by detecting deadlocks and identifying their
participants.
It should be noted that the problem of evaluating mul-
tiple distributed logs of programs has seen some attention
in the past. Some work has been done on generating the
traces in ways that have as little impact as possible on the
execution timing. Others have concentrated on analyzing
multiple traces looking for bug patterns.
[4] provides an
excellent survey of these techniques. The connection be-
tween distributed trace analysis and distributed debugging
has been explored, e.g., in [6]. To create an effective so-
lution one has to capitalize on the properties of the speciﬁc
problem at hand and combine efﬁcient trace generation with
comprehensive analysis. As a result, while some of the prin-
ciples remain the same (e.g. the use of the happened before
relation), speciﬁc implementations tend to differ.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:55 UTC from IEEE Xplore.  Restrictions apply. 
Figure 2. The layered architecture of DCS
its location in the stack) whereas the bottommost layer is
located directly on top of the underlying communication
medium. An outgoing message is injected into the sys-
tem by the application layer, propagated down through all
the layers, and sent to other processes by the bottom layer.
Upon reception, the message passes through the layers in
the reversed order. When the stack sends or receives a mes-
sage, it acquires a special lock (called DDLock) for the short
duration of propagating the message through the stack lay-
ers. This lock is used in the deadlock detection mechanism
described in Section 4.4.
Figure 2 depicts the composition of layers in the DCS
system, somewhat simpliﬁed for the sake of the presenta-
tion. The Membership layer implements the membership
service by employing a view leader-based membership al-
gorithm, the Virtual Synchrony (VS) layer is responsible for
keeping the VS delivery guarantees, whereas the underlying
transport module is used in order to disseminate messages
to group members. The Tester layer is not part of DCS it-
self; it is rather added to the system as part of the testing
architecture (cf. Section 4.1).
The Membership layer maintains a complex internal
state in order to handle Membership changes. The layer im-
plements a four-dimensional state table (having 22×4×3×5
states), so that it has at least 1320 code paths. The Virtual
Synchrony layer basically implements a state table with 14
different internal states and 16 different possible external
events. Consequently, there are at least 224 code paths in
the Virtual Synchrony layer. The combination of the state
space across all DCS stack layers creates a very large num-
ber of different possible paths in the DCS implementation.
3 Testing and debugging challenges
The DCS implementation utilizes two programming par-
adigms: asynchronous message-based communication and
concurrent multithreaded programming. The former has an
intrinsic uncertainty in various aspects of message deliv-
ery while the latter adds a strong dependency on the JVM
2
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:55 UTC from IEEE Xplore.  Restrictions apply. 
scheduling mechanisms in addition to those existing in the
operating system. This combination, along with the use of
timers, e.g., in the heartbeat-based failure detection creates
a unique highly non-deterministic environment. This inher-
ently complex nature of DCS lays the ground for potential
bugs and hinders testing and debugging.
We now present some of the difﬁculties we encountered
during testing and debugging DCS. Although this paper fo-
cuses on test monitoring and log analysis (see Figure 1), we
ﬁrst brieﬂy present the DCS test generation technique in or-
der to characterize the testing and debugging environment.
DCS internal modules and algorithms have a multitude of
local and distributed states and allow large space of execu-
tion schedules. In order to cover the huge state space and
expose rarely occurring bugs, we created an automatic ran-
dom test generation engine, which brings processes up and
down and sends various types of messages. Both the se-
lected test operation (whether to bring up a new process or
to kill an existing process) and the timing of that operation
are random. Using a coverage tool, we have learned that
the random test generation has a high probability to cover
a substantial part of the state space if employed over long
time (see [3] for details).
Challenges in analyzing large distributed logs: The
DCS random test generation technique creates a very large
amount of traces over long time, which renders human
analysis infeasible. Moreover, the produced traces may be
dispersed over hundreds of participating processes because
DCS supports large-scale clusters. Forming a uniﬁed global
picture out of multiple distributed traces requires identify-
ing corresponding events in different logs, which is chal-
lenging in an asynchronous environment.
Challenges in debugging deadlocks: As a highly mul-
tithreaded system, DCS is prone to deadlocks. Some fac-
tors raise the probability of deadlocks in DCS: (1) DCS is
committed to high-throughput messaging, hence it encour-
ages concurrency among threads whenever possible, which
requires complex thread synchronization mechanisms; (2)
DCS interacts with the application on one hand and the
transport module on the other. Since these two parties
have different threading models, the risk of deadlocks is in-
creased; (3) Using asynchronous message-passing exposes
DCS to deadlocks that involve both locks on data items and
cross-waits between the threads. For example, it may hap-
pen that one thread holds a lock and waits for a message
to be received in order to release that lock. However, the
thread supposed to deliver this message waits for the lock
being held.
If a deadlock occurs during the test yet the test completes
and terminates, it is quite hard to detect that a deadlock has
occurred during the test. Even if a deadlock has been dis-
covered, it is still hard to pinpoint the problem based on the
debugging traces.
Challenges in deﬁning “correctness” of test results:
After running a DCS test, the tester should be able to an-
swer a fundamental question, namely, whether the test suc-
ceeded. Doing so requires to deﬁne correctness criteria.
However, it is well-known that in the area of group com-
munication, desirable properties can be elusive and some of
them are only “best-effort”.
Challenges in verifying messaging guarantees: It is
possible to test and debug the DCS QoS messaging guar-
antees by writing speciﬁc tailored tests that track the num-
ber and order of messages sent and received by the cluster
processes (see [3] for an example of testing the VS QoS).
However, we are interested in verifying the messaging QoS
guarantees of DCS not only in a speciﬁc test but also in
all other tests that we and other testing teams run. Another
difﬁculty is that logging each individual message send or re-
ception is discouraged because of performance restrictions.
4 Testing and Debugging solution
The goal of the testing and debugging tool we are going
to present is to verify correctness of the DCS implementa-
tion and to assist in detecting and analyzing its bugs. The
tool aims at maximally automating the tasks of test monitor-
ing and log analysis as well as facilitating root cause analy-
sis while striving to surmount the challenges presented in
Section 3. In addition, our implementation is guided by the
following design principles:
• The testing system should be able to monitor all sig-
• It should have no semantic and minimal performance
• It should ideally be able to analyze a bug as it occurs
and capture as much relevant information as possible with-
out recreating the bug at a later stage. This is because some
DCS bugs are virtually impossible to recreate in a controlled
way as they depend on a very speciﬁc event scheduling.
4.1 The Tester layer
niﬁcant events that occur during the run.
impact on the stack behavior.
In order to implement test monitoring, we have chosen
to add a Tester layer into DCS (see Figure 2). This layer
does not initiate sending new messages but merely observes
all messages and events passed through the stack and ver-
iﬁes the correctness of DCS operation as deﬁned later in
this section. The Tester layer is placed just below the ap-
plication layer in the stack, thus observing the same group
communication guarantees as the application.
4.2 Local and distributed execution invariants
We tackle the problem of verifying correctness of the test
outcome (as described in section 3) by capturing the most
salient properties that every legal output must maintain. Our
testing suite focuses on verifying that those invariants are
preserved at any given moment of the execution.
3
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:55 UTC from IEEE Xplore.  Restrictions apply. 
From the veriﬁcation point of view, invariants can be
classiﬁed as either local or distributed. While local invari-
ants may be veriﬁed by looking at the local execution of
each individual process, preservation of distributed execu-
tion invariants may only be conﬁrmed by cross-matching
executions of different processes in the system. Conse-
quently, local execution invariants are much easier to ver-
ify online without interfering with the system operation and
imposing signiﬁcant performance overhead. A character-
istic example of a local invariant in our testing system is
“view identiﬁers are monotonically increasing in absence
of process crashes”.
Checking local invariants online imposes no substantial
communication or computation burden on the process. If