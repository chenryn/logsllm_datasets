The results indicate that the processing time is negligible
compared to the network latency. The AD requires 6–7 µs
to process each request on average, and it does not present
any signiﬁcant differences regardless of whether the request
is legitimate or not; that is, the processing delay caused by
getSegment() is mainly coming from the database lookup,
not from the MAC computation.
We observed a slight increase in the maximum processing
times for requests from both authorized and unauthorized
senders increase while the number of clients increases—
8.4 → 11.3 µs and 7.8 → 10.2 µs for legitimate and
illegitimate requests, respectively. This result is expected since
the lookup time for Addr S and Addr R would increase if
the size of database increases. Nonetheless, the increase in
processing time is negligible; only a few microseconds for a
million entries. This results also indicate that SVLAN scales
well in the number of clients.
verify(). We also evaluate the processing overhead on a
veriﬁer. To this end, we have implemented the verify() on
the Data Plane Development Kit (DPDK) [46], and evaluate
forwarding performance for various packet sizes including a
representative mix of Internet trafﬁc (iMIX) [36].3 For the
comparison, we also measure the forwarding performance of
typical IP forwarding on the same machine as the baseline.
Figure 5 shows the results. For the small packets (64 and
128 bytes), the forwarding performance for SVLAN packets
degrades by approximately 50%. Although verify() for
3iMIX refers to typical current Internet trafﬁc; its proﬁle speciﬁes the
proportion of packets of a certain size. Since the proﬁle is based on statistical
sampling from actual Internet traces, performance evaluation using an iMIX
of packets is considered a good representation of real-world trafﬁc.
8
64 B128 B256 B512 BiMix0510152025303540Throughput (Mpps)62%99%100%100%100%28%49%92%100%100%11.3Gbps19.9Gbps37.1Gbps40.0Gbps40.0GbpsBaselineVerifier ThroughputFig. 6: Latency inﬂation of the getSegment() protocol for
the deployment on the Amazon EC2 Cloud.
Fig. 7: Latency inﬂation of the getSegment() protocol for
the large-scale simulation.
a single packet requires only 26 ns,
the baseline exhibits
extremely short processing time—20 and 26 ns for 64-byte
and 128-byte packets, respectively—leading to an overall de-
crease in forwarding performance. For large packets, however,
it shows optimal performance and reaches the maximum
throughput. The evaluation results show the efﬁciency of the
SVLAN veriﬁer that can handle 40 Gbps links that are fully
saturated with common Internet trafﬁc patterns at line speed.
B. Amazon Deployment
To evaluate the latency inﬂation on the connection initial-
ization in SVLAN, we deploy SVLAN on the Amazon Cloud.
We initiate an EC2 instance at the 14 data centers distributed
over four continents, namely Europe, North America, Asia,
and Oceania. By deploying a fully functional SVLAN proto-
type, each EC2 instance can perform as an SVLAN endhost
empowered with SVTEP, a veriﬁer, and an AD. In this setup, a
simple client–server application runs on the endhost to transfer
data over SVLAN at application granularity.
Next, we select three instances as the sender, the receiver,
and the AD. Note that we collocate the veriﬁer and the
receiver to realize an on-path veriﬁer that avoids unnecessary
detours in data transmission. There are two different selection
strategies applied: random selection and smart selection. In the
random selection, we randomly select three EC2 instances and
conduct experiments for all possible combinations. It gives us
2184 rounds of experiments. In the smart selection, we ﬁrst
choose two EC2 instances for the sender and receiver, and then
assign another instance closest to the sender as the AD. This
approach is more realistic since it reﬂects the typical cloud-
based service model where clients generally contact the closest
regional cloud. From the smart selection, we get 168 different
combinations in total.
For each round of experiments, we measure two competi-
tive perspectives; the communication latency with and without
SVLAN respectively. For the ﬁrst measurement, we disable
the SVLAN functionality such that the sender could engage
a communication with the receiver directly. This measurement
serves as the baseline latency. Second, we enable SVLAN
introducing an additional latency due to the getSegment()
and verify(), and measure the latency for the time to ﬁrst
packet (TTFP). The comprehensive latency is compared with
the baseline latency to compute the relative latency inﬂation.
Figure 6 depicts the results in the form of a cumulative
distribution function (CDF).
The random-selection approach introduces a signiﬁcant
latency inﬂation. This is expected since a high portion of the
combinations of three instances has an inefﬁcient deployment
model where the sender and receiver are close to each other
while the AD is far away. For example, in an extreme case,
the latency inﬂation increases by up to 1873% if the sender
and receiver are in Europe (e.g., Frankfurt and Paris) while the
AD is located in East Asia (e.g., Seoul).
For the smart-selection approach, the latency inﬂation is
below 75% in all combinations. More precisely, the latency
inﬂation is less than 50% for 93.4% of the combinations, and
the 93.3% of additional latencies do not exceed 78 ms (32.0 ms
on average) which is a tolerable latency overhead. The results
drive us to the intuition that the latency overhead introduced
by SVLAN is negligible for the modern cloud environment.
C. Large-scale Simulation
Now, we take one step further to investigate the latency
inﬂation of SVLAN for a large-scale deployment. To this
end, we leverage RIPE Atlas4 to simulate the sender and
receiver distributed geographically over wide areas. We ran-
domly select the RIPE nodes including 590 probes and 122
anchors distributed across 684 ASes in 178 countries. In this
experiment, the probes and anchors serve as the sender and
receiver respectively. We further extend the list of receivers
with Alexa’s top-100 domains to see the impact of SVLAN
on TLS connections.
To simulate the AD, we introduce three different deploy-
ment scenarios: 1) We use the 14 Amazon EC2 instances we
have initiated as a cloud provider with a small footprint. 2) For
a cloud provider with a large footprint, we leverage Akamai’s
CDN network. To determine Akamai’s edge-cloud servers that
are closest to the senders, we utilize the DNS system; we
trigger DNS queries from the RIPE probes to Akamai’s DNS
server, which in turn reply with the server addresses that are
closest to the probes. 3) We also simulate the AD on the
4https://atlas.ripe.net/
9
0100200300400500Relative Latency Inflation (%)0.00.20.40.60.81.0CDF[Random] TTFP[Smart] TTFP0100200300400500Relative Latency Inflation (%)0.00.20.40.60.81.0CDF[Dst AS] TLS TTFB[Akamai] TLS TTFB[Amazon] TLS TTFB[Akamai] TTFP[Amazon] TTFPreceiver’s AS. The different deployment simulation allows us
to evaluate the impact of the AD’s location.
We cannot deploy our SVLAN code to the RIPE nodes,
Akamai network, and the Alexa’s top-100 servers. To over-
come this limitation and investigate the latency inﬂation, we
perform analysis based on latency measurements. This ignores
the processing overhead of getSegment() and verify(),
but these overheads are negligible compared to network latency
(see Section VI-A). We use ping measurements to estimate
the latency between two entities, and then project the latency
measurement to the number of RTTs needed to complete the
connection. Figure 7 shows the relative latency inﬂations that
we simulate with the aforementioned experimental setup.
From the results, we make the following observations: 1)
The latency inﬂation in the modern Internet environment is
tolerable. The latency inﬂation is less than 50% for 67% of the
measurements and averages 70%. 2) The cloud-provider model
with a large foot print shows overall better performance than
on a small cloud footprint. 3) TLS does not strongly affect the
latency inﬂation. 4) In many cases, the AD on the receiver’s AS
demonstrates lower latency inﬂation than the cloud-based AD
models. The observations suggest that the deployment of the
AD is the key to minimizing the latency inﬂation. We further
discuss the location of the AD in Section VIII-A.
D. Bandwidth Overhead
To measure the bandwidth overhead introduced by
SVLAN, we ﬁrst measure the size of the extra header required
to send packets. The extra header may differ depending on the
implementation, but here we measure the size of the extra
header based on two implementation scenarios: with (i) SR-
MPLS and (ii) SCION. In SR-MPLS, an additional header
for the authorization proof is necessary. SR-MPLS normally
allows up to three labels and each label has a size of 4 bytes.
Including additional 24 bytes for an authorization proof, the
SVLAN header becomes 36 bytes in total. The maximum
payload size is therefore 1424 bytes per frame when attributing
20 bytes each to layer-3 and layer-4 header and 36 bytes
to the SVLAN header on a general Ethernet frame with
MTU = 1500bytes.
SCION requires 8 bytes for the common header, 16 bytes
for the addresses, 24 bytes for the forwarding path, and 32
bytes for the SVLAN header for the same number of labels
and the authorization proof (Figure 4). This results in up to
1400 bytes of payload for each Ethernet frame. In VXLAN, a
total of 50 bytes of additional headers are generated including 8
bytes of VXLAN header and 42 bytes of encapsulation header.
Therefore, the maximum size of the payload for each frame is
1410 bytes. Table II shows the comparison results.
We estimate the goodput on a fully saturated 1 Gbps
link to see the bandwidth overhead. With a normal Ethernet
frame, it requires 1538 bytes including an interframe gap of
12 bytes, and thus the link supports 81274 packets per second.
The total amount of data that can be actually transmitted is
approximately 949 Mbps. If we apply the same measurement
to others, the goodputs of VXLAN, SVLAN (SR-MPLS), and
SVLAN (SCION) are 916, 926 and 910 Mbps, respectively.
The results show that SVLAN has no severe bandwidth
overhead compared to VXLAN.
TABLE II: Comparison of the header sizes, maximum payload,
and network performance on a 1 Gbps link. The SVLAN
header contains three segment labels and one authorization
proof.
Ethernet
VXLAN
SR-MPLS
SCION
SVLAN
Extra header (bytes)
Max payload (bytes)
Max goodput (Mbps)
-
1460
949
50
1410
916
36
1424
926
60
1400
910
VII. SECURITY ANALYSIS
We now discuss potential attack scenarios, their signiﬁ-
cance, and how effectively our SVLAN design mitigates them.
Threat Model. We mainly consider two different goals of
the adversary: 1) inﬁltrate an isolated network without autho-
rization, and 2) disrupt network operation by leveraging the
SVLAN protocols. We consider that the adversary has enough
capability to compromise and control all SVLAN entities in
the network except for the AD; the AD is typically allocated
on well-provisioned and highly-secured systems, e.g., core
network, that can tolerate large amounts of incoming trafﬁc
and security breaches.
A. Compromise the SVLAN Isolation
The objective of this attack class is to compromise the
SVLAN isolation without proper authorization. To this end,
an attacker may attempt to acquire a valid authorization proof,
or enforce unauthorized packet forwarding. The attacker has
clear incentives to perform such attacks, e.g., gaining access
to a restricted network zone or reserving more capability in
packet forwarding. We start by describing attacks that deceive
the SVLAN control plane and data plane; then we describe
brute-force attacks and compromisation attacks.
Source-Address Spooﬁng. An attacker may perform source
address spooﬁng to defeat SVLAN isolation. This attack can
be performed in the control plane to obtain an authorized proof
from the AD by impersonating an authorized address, and in
the data plane to misuse a sniffed authorization proof and send
packets to the destination.
We consider to use authentication to secure the control
plane. If the AD is in the same local network (also within the
same VLAN), then source authentication can be performed by
the AD by issuing an unique authorization proof to each host
through conﬁguration or secure DHCP. In that case, we can
assume that the additional authorization proofs that are fetched
are secure. If the AD is outside the LAN (or the VLAN that is
created across domains), then a secure channel from the source
to the AD needs to be established. This can be a TLS-protected
connection, in which case the source will need a certiﬁcate that
the AD can verify. The SVLAN design is scalable with respect
to management, as the source veriﬁcation and communication
policy is veriﬁed at a single place (the AD) and enforced at a
single place (the veriﬁer).
For the data plane, we consider the same setting as all
the virtual LAN approaches; the network ensures separation of
10
TABLE III: The number of packets per second (PPS) and the
required time to brute-force the SVLAN MAC (in years) for
different link bandwidths.
64-bit MAC
128-bit MAC
Link
PPS
Time [years]
PPS
Time [years]
1 Gbps
10 Gbps
100 Gbps
976562
9765625
97656250
5.99e6
5.99e5
5.99e4
919177
9191176
91911764
1.17e25
1.17e24
1.17e23
trafﬁc to prevent eavesdropping, thus the tags in the data plane
are considered secure. To achieve a simple system, the on-
path dataplane devices are assumed to be trusted, as otherwise
any of those devices could inject malicious trafﬁc toward any
host on the virtual LAN (e.g., replay attacks). Within the
tradeoff space of security vs. efﬁciency/deployability, this is
the design point selected by virtual LAN systems—it would
be an interesting research challenge to also defend against
malicious data plane devices but that would likely dramatically
increase the complexity of the system.
Brute-Force Attack. An adversary may attempt to deceitfully
generate a valid authorization proof for Addr M . The brute-
force attack is a classic attack method that allows an attacker to
generate all the possible combinations to derive a valid MAC.
In our SVLAN prototype, however, the attacker must send
2128 ≈ 3.4e38 probe packets to brute-force the MAC. This
requires 3.48e32 seconds (or 1.17e25 years) for transmission