It appears that, in the current master branch of NumPy (commit `da6e4c7`), the `np.matmul` function is not utilizing the BLAS (Basic Linear Algebra Subprograms) library for optimized performance. Here's a demonstration:

```python
import numpy as np

x = np.random.rand(5, 512, 512)

# Timing np.matmul
%timeit np.matmul(x, x)
# Output: 1 loop, best of 3: 526 ms per loop

# Custom function using np.dot
def xmul(a, b):
    out = np.empty_like(a)
    for j in range(a.shape[0]):
        out[j] = np.dot(a[j], b[j])
    return out

# Timing custom function
%timeit xmul(x, x)
# Output: 10 loops, best of 3: 28 ms per loop
```

As shown, the custom function `xmul`, which uses `np.dot` in a loop, is significantly faster than `np.matmul`. While `np.matmul` is still a preliminary feature, it would be advisable to create an issue to address this performance discrepancy.