the predictive model in one global epoch. Unlike other crowd-
sourcing jobs, manufacturers in our system prefers customers
to follow their lifestyle instead of rushing to ﬁnish the job
to obtain the real status. As a result, customers who seldom
use devices may postpone the overall crowdsourcing progress.
This problem can be mitigated by using the incentive mech-
anisms. Yu et al. [55] designed a queue to store customers
who submitted their models in order. Thus, customers who
submit their locally trained models early will be rewarded to
encourage people to submit their updates earlier.
VI. EXPERIMENTS
To validate the effectiveness of our designed FL with
differential privacy approach, we conduct experiments on the
MNIST handwritten image dataset [56].
A. Experiment Setup
The MNIST dataset includes 60,000 training image samples
and 10,000 test image samples. Each sample is a 28 × 28
gray scale image showing a handwritten number within 0 to
9. In addition, the MNIST is a standard dataset employed
for testing machine learning algorithms. It gives moderate
and typical complexity faced by IoT applications. Therefore,
we leverage the MNIST dataset which has been used for
testing the performance of the IoT system by [45,57]–[62].
Our designed CNN network includes hidden layers that are
responsible for feature extraction and fully connected layers
for classiﬁcation. We have two convolutional layers with 30
and 80 channels, respectively. After each convolutional layer,
we deploy a max-pooling layer to reduce spatial dimensions
of the convolutional layers’ output. Therefore, max-pooling
layers accelerate the learning speed of the neural network.
Normalization is used after all non-linear layers, i.e., convolu-
tional layers. The normalization layer enables the computation
of sensitivity in differential privacy to determine the amount
of noise to add, speeds up the learning rate, and regularizes
gradients from distraction to outliers. Then, we apply -DP
noise to perturb the output of normalization layers to preserve
the privacy of the extracted features.
The perturbed features serve as inputs of fully connected
layers for classiﬁcation in the MEC server. In our designed
model, fully connected layers include four hidden layers. The
dimension decreases from 3920 to the dimension of the label
which is 10. Finally, there is a softmax layer to predict label
and compute loss. The architecture of CNN is shown in
Figure 2. We simulate FL by constructing the model using
the averaged parameters of multiple locally trained model
parameters.
In our experiment, we set the hyperparameters of CNN as
follows. The learning rate is 0.01, and the batch size N is 64.
Then, we set the range of privacy parameter  to be [1, 10]. The
default number of global epochs is 2 and the default number of
local epochs is 40. We use ten participants in the experiment.
Before training, we separate the training image dataset into
equally ten parts, meaning that each participant gets 6000
training images randomly. We normalize each dimension of the
N − 1] for N denoting the
feature to the interval [−√
N − 1,
√
This paper appears in IEEE Internet of Things Journal (IoT-J). Please feel free to contact us for questions or remarks.
Figure 2: The neural network used in experiments.
Figure 3: Impacts of normalization techniques on the test
accuracy.
Figure 5: Impact of the batch size on the test accuracy of
the FL model using our normalization technique without DP
protection.
Figure 4: Impact of the batch size on the test accuracy of the
FL model protected with DP ( = 2).
batch size, so that the sensitivity of the normalized feature vec-
N − 1.
tor when one dimension of the feature changes is 2
Then, according to Laplace mechanism [9], the independent
zero-mean Laplace noise with scale 2
each dimension of the normalized features to protect features
under -differential privacy. Default  = 2.
N − 1(cid:14) is added to
√
√
B. Experimental Results
Figure 3 compares the test accuracies between federated
learning (FL) without differential privacy (DP) and different
DP-aware FL algorithms, including DP-aware FL using our
normalization technique, and DP-aware FL using Jiang et al.’s
8
Figure 6: Impact of the batch size on the test accuracy under
different global epochs using our normalization technique ( =
2).
batch normalization [45]. Figure 3 shows the superiority of
DP-aware FL using our normalization technique over DP-
aware FL using Jiang et al.’s batch normalization [45]. Thus,
we conﬁrm that our normalization technique is useful when
we add Laplace noise to features, because we relax constraints
of normalization compared with batch normalization as stated
in Section IV-A. A feature goes through batch normalization
often results in a smaller magnitude than that goes through
our normalization technique, so the value of feature is easily
overwhelmed by the noise when using batch normalization.
 Max-PoolingConvolution Max-Pooling Normalization+NoiseFully-ConnectedConvolution1x39201x6001x1001x301x201x1030@28x2830@14x1480@14x1480@7x7 80@7x7ormalization+Noise1x3920(edge (cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)server)(mobile phone)28x281 2 3 4 5 6 7 8 9 10privacy parameter 0.50.60.70.80.91test accuracyFL without DPFL with DP and our normalization techniqueFL with DP and Jiang et al.'s batch normalization32 64 96 128256batch size0.20.30.40.50.60.70.80.91test accuracyFL with DP and Jiang et al.'s batch normalizationFL with DP and our normalization technique32 64 96 128256batch size0.960.9650.970.9750.980.985test accuracyFL with Jiang et al.'s batch normalizationFL with our normalization technique32 64 96 128256batch size0.930.9350.940.9450.950.9550.960.9650.97test accuracyglobal epoch = 1global epoch = 2global epoch = 3This paper appears in IEEE Internet of Things Journal (IoT-J). Please feel free to contact us for questions or remarks.
the test accuracy is better when the number of global epoch
= 2 than the number of global epoch = 1 or 3 when  = 2
and the number of local epochs is 40. As the number of global
epochs increases, the test accuracy increases if DP noise is
not added. However, Laplace noise increases as the number
of global epochs increases, which negatively affects the test
accuracy. Thus, a trade-off between the number of global
epochs and the amount of noise is required. In our case, when
the privacy parameter  = 2 and the number of local epochs
is 40, the optimal number of global epochs is 2.
Figure 7 illustrates how the privacy parameter  affects
the test accuracy of FL model. In our experiment, we train
FL with 4 global epochs to validate the practicality of our
designed approach. The test accuracy increases as the privacy
parameter  increases. A larger  means that less noise is
added to features, so that the privacy protection is weaker.
Typical  values for experiments are between 0.1 and 10 [63].
Our experiment shows that we can achieve at
least 90%
accuracy when the global epoch = 2 and the privacy parameter
 > 1. Before training, we initialize the model with random
parameters, and the model with initial parameters will be used
by all parties for their local training. After the ﬁrst global
epoch, we obtain a new model by averaging all parties’ model
parameters. Then, in the second global epoch, parties start
training using the model from the ﬁrst global epoch. Through
our experiment, we can verify that our designed FL method
is effective. However, when the number of global epochs
increases to 3 or 4, the test accuracy may decrease. The test
accuracy decreases because the noise increases as the number
of global epochs increases.
Figure 8 shows that the test accuracy of FL model is affected
by both the number of local epochs and the number of the
global epochs. The number of local epochs reﬂects the cost
of devices’ computing resources locally. We add -differential
privacy noise during training, and the test accuracy may drop
if there is too much noise added in each epoch. From Figure 8,
when the number of local epochs equals 20 or 30, it takes 4
global epochs to achieve a similar accuracy. When the number
of local epochs is 40, it takes 2 global epochs. But the test
accuracy will start to drop if the number of local epochs is
40 and the number of global epoch is more than 2. Hence,
to obtain a high test accuracy, it necessities optimal values to
strike a good balance between the number of local epochs and
global epochs for averaging locally uploaded models, which
we leave as the future work.
C. Performance evaluation on the mobile device and edge
server
Now, we evaluate the feasibility and efﬁcacy of training on
the mobile device. A Raspberry Pi 4 Model B tiny computer
in Figure 9 is used to simulate the mobile device. Key
speciﬁcations of the Raspberry Pi 4 Model B are listed in
Table II. We use a laptop to emulate the edge server, which
is equipped with four 2.3 GHz Intel Core i5 processors, 8 GB
of RAM, and MacOS 10.14.4 system.
In our experiment, we distribute the MNIST dataset [52]
with 60,000 images to ten participants equally so that each
9
Figure 7: Impact of DP parameter  on the test accuracy using
our normalization technique under various global epochs.
Figure 8: Impact of the number of local epochs on the test
accuracy using our normalization technique under various
global epochs when  = 2.
For each DP-aware FL, we also observe that the test accuracy
gets closer to the test accuracy of FL without DP as the privacy
parameter  increases, because a larger privacy parameter 
means less privacy protection which equals that less noise is
used. Thus, we conclude that our normalization technique out-
performs the batch normalization under -differential privacy
when training the FL model.
Figure 4 presents that the test accuracy of FL model de-
creases as the batch size increases when the number of global
epoch is 1 and DP parameter  = 2. This is because we add
Laplace noise to features, the added noise will increase as the
batch size N increases, which results in a worse test accuracy.
Moreover, due to the three-sigma rule in Gaussian distribution,
most feature values normalized with batch normalization lie
in [−3σ, 3σ]. But feature values normalized using our nor-
malization technique lie in [−√
N − 1]. However,
Figure 5 shows that if no differential privacy noise is added,
the test accuracy with the batch normalization outperforms
that using our normalization technique. Moreover, as the batch
size increases, the test accuracy will decrease. Therefore, we
conclude that our normalization technique works better with
FL under DP protection. Furthermore, Figure 6 illustrates that
N − 1,
√
1 2 3 4 5 6 7 8 9 10privacy parameter 0.860.880.90.920.940.960.98test accuracyglobal epoch = 1global epoch = 2global epoch = 3global epoch = 410203040local epoch0.90.910.920.930.940.950.960.97test accuracyglobal epoch = 1global epoch = 2global epoch = 3global epoch = 4This paper appears in IEEE Internet of Things Journal (IoT-J). Please feel free to contact us for questions or remarks.
Table II: Raspberry Pi 4 Model B Speciﬁcations [64].
Broadcom BCM2711, Quad core Cortex-A72 (ARM v8) 64-bit SoC
@1.5GHz
4GB LPDDR4-3200 SDRAM
participant (i.e., each device) has 6,000 images. Then, we run
the same training process on both the mobile device and the
edge server. It takes about 144 seconds to train the model with
6,000 images on the Raspberry Pi 4 (i.e., the mobile device)
for each epoch, and it uses about 9 seconds to train the model
on the laptop (i.e., the edge server). For default forty epochs,
the mobile device and the edge server use about 96 minutes
and 6 minutes, respectively. A client is supposed to participate
in the federated learning when the smartphone is idle, such as
charging, screen off, and connected to an unmetered network,
for example, WiFi [13,65]. Thus, we conﬁrm that it is feasible
to utilize mobile devices in the federated learning. Besides, an
edge server will signiﬁcantly improve the speed of training
because it trains much faster.
Figure 9: Raspberry Pi 4 Model B.
In addition to the training time, the delay of our proposed
approach, which depends on the transmission rate, is small
because smartphones often use wideband network connections
(e.g., 4G and WiFi). The average size of locally trained models
is 617.8KB in our experiment. Assume the upload bandwidth
is 1MB/s, so the communication cost is 0.6178 second. The
communication cost is little compared with wasted training
time on the mobile device.
D. Evaluation on the incentive mechanism
In this section, we evaluate the impacts of incentive mech-
anism on customers’ reward and reputation. The assumption
and parameters in the experiments are as follows. Assume that
the maximum values of both reputation and reward are 100
(i.e., γM ax = 100). Every customer has a reputation of 5 (i.e.,
h = 5) at the beginning. We set the reward for each accepted
update equal to owners’ reputation in each global epoch. The
experiments compare reward and reputation that customer can
achieve in four cases (i.e., no incentive mechanism, honest
customer, malicious customer performs poisoning attack at
global epoch = 1, and malicious customer performs poisoning
attack at global epoch = 4). If there is no incentive mechanism,
the customer gets a ﬁxed reward of 5 in every global epoch.
Figure 10: Reward comparison.
As shown in Figure 10, when there is no incentive mech-
anism, the reward value is the same in each global epoch
regardless of poisoning updates. However, with the incentive
mechanism, the honest customer, whose updates are accepted,
will gain more rewards as the number of global epochs
increases. If a customer’s update is considered as poisoning
(i.e., the value of s in Eq. (1) is signiﬁcantly larger than
others), her update will not be accepted, that is, her reward
is 0. Besides, the behaviour of the poisoning attack affects
the value of reputation, which results in a decrease of the
reputation. If the poisoning attack is performed when the value
of the reputation is equal to the h, the customer’s reputation
will be clear, which will result in small rewards afterwards.
However, if the malicious behaviour happens when the value
of reputation is higher than h, the reputation drops by 1, so
does the reward in the subsequent global epoch.
Figure 11: Reputation comparison.
Figure 11 shows the impact of the incentive mechanism on
the reputation. Without the incentive mechanism, customers’
reputation will be 0. If a customer is honest and uploads the
correct update in every global epoch, her reputation increases
as the number of global epoch increases. However,
if a
customer uploads a malicious update when her reputation
value equals to the h (i.e., 5), her reputation will drop to 0.
However, if her reputation is not 5, her reputation drops by 1
when caught performing poisoning attack.
10
(cid:1)12345678910global epoch02468101214reward valueno intensive mechanismhonest customermalicious customer with posioning attack at global epoch = 1malicious customer with posioning attack at global epoch = 412345678910global epoch02468101214reputation valueno intensive mechanismhonest customermalicious customer with posioning attack at global epoch = 1malicious customer with posioning attack at global epoch = 4This paper appears in IEEE Internet of Things Journal (IoT-J). Please feel free to contact us for questions or remarks.
Thus, our incentive mechanism can encourage honest cus-
tomers to contribute their useful updates while preventing
malicious customers from attempting to perform the poisoning
attack.
VII. DISCUSSION
To attract more customers to contribute to training the
global model, our designed system should guarantee that
customers’ conﬁdential information will not leak. There are
studies discussing potential risks of information leakage in
FL [4,66] in which attackers may infer customers’ private
data from gradients. To prevent this scenario, we leverage
differential privacy to perturb features before classiﬁcation in
the fully connected layers. Thus, gradients are also protected
by differential privacy. Hitaj et al. [4] demonstrated that a curi-
ous server could obtain the conﬁdential information using the
generative adversarial network if gradients were protected by
a large privacy budget in the collaborative learning. But their
experiments conﬁrmed that GAN-based approaches might not
work well when the selected privacy parameter was smaller