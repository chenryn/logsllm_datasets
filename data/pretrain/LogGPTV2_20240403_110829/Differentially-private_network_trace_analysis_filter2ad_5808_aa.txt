title:Differentially-private network trace analysis
author:Frank McSherry and
Ratul Mahajan
Differentially-Private Network Trace Analysis
Frank McSherry
Ratul Mahajan
Microsoft Research
Abstract– We consider the potential for network trace
analysis while providing the guarantees of “diﬀerential pri-
vacy.” While diﬀerential privacy provably obscures the pres-
ence or absence of individual records in a dataset, it has two
major limitations: analyses must (presently) be expressed in
a higher level declarative language; and the analysis results
are randomized before returning to the analyst.
We report on our experiences conducting a diverse set of
analyses in a diﬀerentially private manner. We are able to
express all of our target analyses, though for some of them
an approximate expression is required to keep the error-level
low. By running these analyses on real datasets, we ﬁnd
that the error introduced for the sake of privacy is often
(but not always) low even at high levels of privacy. We
factor our learning into a toolkit that will be likely useful
for other analyses. Overall, we conclude that diﬀerential
privacy shows promise for a broad class of network analyses.
Categories and Subject Descriptors
C.2.m [Computer-communication networks] Miscellaneous
General Terms
Algorithms, experimentation, measurement
Keywords
Diﬀerential privacy, trace analysis
1.
INTRODUCTION
As a community, if we do not solve this problem [privacy-
compliant data sharing], we are in trouble.
– Vern Paxson (HotNets-VIII, 2009)
The complexity of modern networks makes access to real-
world data critical to networking research. Without this
access it is almost impossible to understand how the network
behaves and how well a proposed enhancement will function
if deployed. But obtaining relevant data today is a highly
frustrating exercise for researchers and one that can often
end in failure.
Thus far, the community has mainly taken the social ap-
proach of encouraging institutions and researchers to release
collected data (e.g., CRAWDAD [6], ITA [11]). While ben-
eﬁcial, this approach has weaknesses. The released data is
heavily sanitized (e.g., payloads are removed) and anonymized,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’10, August 30–September 3, 2010, New Delhi, India.
Copyright 2010 ACM 978-1-4503-0201-2/10/08 ...$10.00.
limiting their research value [21]. Worse, as demonstrated by
research [5, 26, 21] and real mishaps [2, 29, 20], anonymiza-
tion is vulnerable to attacks that infer sensitive information.
Because of this fear, many data owners today prefer the safer
option of not releasing data at all.
Consider an alternative approach to enable data-driven
networking research: instead of releasing sanitized data, the
data owners run analyses on behalf of the researchers; to
preserve privacy, restrictions are placed on what analyses
are permitted and what output is returned. This approach
was ﬁrst advocated by Mogul and Arlitt [19] and recently
termed mediated trace analysis by Mittal et al. [18].
Given the intricacies of protecting sensitive information
and past failures, we believe that strong and formal privacy
guarantees are an important prerequisite for data owners to
adopt this approach. Existing proposals, however, provide
no guarantee. The basis for protecting privacy in Mogul
and Arlitt’s original proposal is human veriﬁcation, which is
error-prone and hard to scale to sophisticated analyses [19].
To obviate human veriﬁcation, Mirkovic proposes rules that
an analysis must follow to protect privacy [17].
It is un-
clear, however, what privacy properties are achieved by these
rules. Mittal et al. propose that only analyses that leak
fewer than a threshold number of bits (in an information-
theoretic sense) be allowed [18]. However, restricting in-
formation leakage and preserving privacy are not the same.
An analysis that reveals if hosts A and B communicate leaks
only one bit but may represent an unacceptable privacy loss
for the hosts.
We ask if mediated trace analysis can be enabled with for-
mal privacy guarantees. The deﬁnition of privacy that we
consider is diﬀerential privacy [8, 7]. Informally, diﬀerential
privacy guarantees that the presence or absence of individual
records is hard to infer from the analysis output. While it is
unclear if diﬀerential privacy is the appropriate guarantee for
networking analyses—or if there even exists a single deﬁni-
tion that applies to all analyses and datasets—we consider
it because it provides one of the strongest known privacy
guarantees. Appealingly, it is resilient to collusion, supports
multiple interactive queries, and is also independent of any
auxiliary information that an attacker might possess; such
information has been shown to break anonymization [5, 26,
20, 29]. As such, diﬀerential privacy has the potential to
provide a strong foundation for mediated data analysis.
However, the strong guarantees of diﬀerential privacy do
not come for free. Privacy is preserved by adding noise to
the output of the analysis, imposing on its accuracy. The
added noise is scaled to mask the presence or absence of
small sets of records. While the magnitude of the noise is
typically small, and the distribution of the noise is known to
the analyst, it can render sensitive analyses useless. Addi-
tionally, using current tools a diﬀerentially-private analysis
123must be expressed using high-level operations (e.g., SQL-
like) on the data, so that the privacy-preserving platform
can understand how the analysis manipulates data and add
noise accordingly.
Given limitations of accuracy and expressibility, the ques-
tions of whether and which networking analyses can be fruit-
fully conducted in a diﬀerentially private manner is open.
The answers depend both on the nature of the analyses and
the data. Diﬀerential privacy is a recent development, and
its practical utility is still unclear, even outside of network-
ing. We are aware of only two concrete case studies [15, 24],
and the results are mixed.
To shed light on the possibility of network trace analysis
with diﬀerential privacy guarantees, we attempt to repro-
duce a spectrum of network trace analyses using PINQ [14],
a diﬀerentially-private analysis platform. Our analyses in-
clude multiple examples of packet-level, ﬂow-level, and graph-
level computations chosen from the networking literature.
Each analysis relies on sensitive ﬁelds in the source data
and will thus be diﬃcult to conduct for researchers that do
not own the data.
We ﬁnd that we are able to express all the analyses that
we consider, though some required approximations. Cer-
tain computations, such as arbitrary resolution cumulative
distribution function, are fundamentally impossible with dif-
ferential privacy (independent of the platform), but can be
approximated with noisy counterparts. Certain others, such
as sliding window computations and splitting a long ﬂow into
individual connections, are hard to implement in a manner
that incurs only a small amount of noise. We ﬁnd that the
impact of our approximations on the results is low, however.
There are multiple ways to implement an analysis, with
diﬀerent privacy costs (i.e., added noise). We ﬁnd that some-
times there is also a trade-oﬀ between algorithmic complex-
ity and privacy cost. These challenges are surmountable,
but they complicate (or, enrich) the task of implementing
networking analyses. We implement a toolkit with analysis
primitives that we ﬁnd common to multiple analyses. To
aid other researchers, we are releasing this toolkit and our
analysis implementations [23].
We ﬁnd that the added noise tends to not be a hindrance
because most analyses seek only broad distributional and
statistical information about the data. They rarely depend
heavily on few individual records, and diﬀerential privacy is,
in principle, compatible with this use. The main challenge
lies in extracting suﬃcient aggregates from the data in a
privacy-eﬃcient manner. For a few analyses, we achieve high
accuracy only when the privacy level is low. As we gain more
experience at implementing privacy-preserving analyses, this
situation should only improve.
Overall, we conclude that diﬀerential privacy is a promis-
ing avenue for enabling mediated trace analysis for a large
class of analyses. Our work, however, is only the ﬁrst step.
Before we can start convincing data owners to share data, we
need to resolve several key issues. One is managing privacy
loss due to repeat analysis of the same data. Another is pre-
serving privacy, with acceptable analysis noise, for higher-
level entities (e.g., hosts, subnets) that may be spread across
many records. Yet another issue is developing guidelines
regarding appropriate privacy levels for various situations.
Building on the strong foundation that is provided by dif-
ferential privacy, we hope that future work can resolve these
issues to the satisfaction of many data owners.
2. BACKGROUND
In this section, we give a brief background on diﬀerential
privacy and contrast it with alternative privacy deﬁnitions.
We also describe PINQ, the analysis platform we use in our
investigation.
2.1 Differential Privacy
Diﬀerential privacy requires that a computation exhibit
essentially identical behavior on two data sets that diﬀer
only in a small number of records. Formally, let A and B
be two datasets and A (cid:9) B be the set of records in exactly
one of them. Then, a randomized computation M provides
-diﬀerential privacy if for all A and B and any subset S of
the outputs of the computation:
Pr[M (A) ∈ S] ≤ Pr[M (B) ∈ S] × exp(|A (cid:9) B|)
That is, the probability of any consequence of the compu-
tation is almost independent of whether any one record is
present in the input. For each record, it is almost as if the
record was not used in the computation, a very strong base-
line for privacy. The guarantee assumes that each record
is independent of the rest and applies to all aspects of the
record. So, if each record is a packet, diﬀerential privacy
protects its IP addresses, payloads, ports, etc., as well as its
very existence.
Diﬀerential privacy is preserved by adding “noise” to the
outputs of a computation. Intuitively, this noise introduces
uncertainty about the true value of the output, which trans-
lates into uncertainty about the true values of the inputs.
The noise distributions that provide diﬀerential privacy vary
as a function of the query, though most commonly we see
Laplace noise (a symmetric exponential distribution). The
magnitude of the noise is calibrated to the amount by which
the output could change should a single input record arrive
or depart, divided by . The value of a perturbed result
depends greatly on the data, however; a count accurate to
within ±10 may be useful over a thousand records but not
over ten records. The noise distribution is known to the
analyst, who can judge if the noisy results are statistically
signiﬁcant or not without access to the actual data.
The parameter  is a quantitative measurement of the
strength of the privacy guarantee. Lower values correspond
to stronger guarantees, with  = 0 being perfect privacy.
Typically,  ≤ 0.1 is considered strong and  ≥ 10 is con-
sidered weak. We are not advocating speciﬁc levels of dif-
ferential privacy as suﬃcient but are instead interested in
understanding the trade-oﬀ between accuracy and privacy.
Comparison with alternative privacy deﬁnitions Un-
like diﬀerential privacy, many alternative formulations do
not provide a direct guarantee or are vulnerable to auxiliary
information that the attacker might possess. Consider, for
example, k-anonymity, which provides guidance on releasing
data such that the identity of individual records remains pri-
vate [29]. A release provides k-anonymity if the information
for each record cannot be distinguished from at least k-1
other records. However, this deﬁnition provides no guar-
antee in the face of auxiliary information that may exist
outside of the released dataset. Such information can break
anonymization [20, 5, 26].
As another example, consider reducing information leak-
age as a way to preserve privacy [18]. The reasoning is that
the fewer bits of information that an analysis leaks about
speciﬁc records, the more privacy is protected. However,
124Count
Sum
Average
Median
Aggregations
√
√
2/.
√
2/.
Std. deviation of added noise is
Std. deviation of added noise is
Std. deviation of added noise is
where n is the number of records.
The return value partitions input into sets
whose sizes diﬀer by approx.
8/n,
√
2/
Transformations
Where, Select No sensitivity increase
Distinct
GroupBy
Join, Concat
Intersect
Partition
Increases sensitivity by two
No sensitivity increase for either input
Privacy cost equals the maximum of the
resulting partitions
Table 1: Main data operations in PINQ.
this reasoning is indirect at best and fallacious at worst.
Revealing even one bit can lead to signiﬁcant loss in pri-
vacy. For example, revealing if hosts A and B communicate
requires only one bit of information but may represent an
unacceptable loss in privacy. Moreover, any such scheme al-
ways leaks at least one bit, in response to: “did the analysis
reveal too many bits?” This response bit can encode very
sensitive information, and is always revealed to the analyst.
2.2 Privacy Integrated Queries (PINQ)
PINQ is an analysis platform that provides diﬀerential
privacy [14]. Rather than provide direct access to the under-
lying data, PINQ provides an opaque PINQueryable object
supporting various SQL-like operations. The analyst spec-
iﬁes queries over the data in a declarative language, and is
rewarded with aggregate quantities that have been subjected
to noise. Once a noised aggregate has been extracted from
PINQ, it can be manipulated freely by the analyst, and used
in further queries. PINQ tracks the privacy implications of
successive operations and ensures that the cumulative pri-
vacy cost does not exceed a conﬁgured budget.
Table 1 summarizes the main data operations supported
by PINQ and their privacy implications. There are two types
of operations: aggregations and transformations. Aggrega-