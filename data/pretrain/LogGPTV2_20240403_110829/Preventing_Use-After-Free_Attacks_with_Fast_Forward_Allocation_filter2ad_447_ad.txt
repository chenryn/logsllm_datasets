reported about a 40.0% cost. DangNull reported about 54.6%
overhead for 15 out of 19 benchmarks. This result shows
that with our careful design, one-time allocation can have
counterintuitively low performance overhead.
FFmalloc introduces consistent overhead to 19 SPEC
benchmarks, with the standard deviation 0.12, while other
tools have standard deviations from 0.15 (FreeGuard) to 1.25
(DangNull). Among all benchmarks, gcc is an outlier where
FFmalloc makes it slower by 49.8%. We investigated gcc’s
execution, and found that it consumes the largest amount of
memory per second (see the last column of Table 7 in Ap-
pendix A). Therefore, the execution with FFmalloc leads to
significantly more system calls for memory management. For
example, for the c-typeck input, FFmalloc issues 28,767 mmap
and 500,213 munmap system calls, and spends 39.8 seconds in
the kernel space. The original memory allocator glibc only
requires 34 mmap and 23 munmap, which takes 0.59 seconds
to finish. Although the user-space execution with FFmalloc
is faster (reduced from 59.0 seconds to 53.9 seconds), the
overall overhead is 53.7%. In this extreme case, we may need
to optimize our settings to make a new balance between mem-
ory usage and performance overhead (see §6.4). Fortunately,
we have not seen another program like gcc that so quickly
allocates a large amount of memory. Further, other tools also
demonstrate higher overhead for gcc, although the underlying
reasons could be different.
Memory Overhead. Considering the geometric mean,
FFmalloc introduces 61.0% memory overhead to SPEC
benchmarks, similar to that of Oscar. Two previous tools
achieve less overhead than FFmalloc: 18.0% for CRCount,
and 28.1% for MarkUs (rerun). Another four tools consume
more memory: 115.4% for FreeGuard (rerun), 125.2% for
pSweeper, 127.1% for DangNull and 148.1% for DangSan.
USENIX Association
30th USENIX Security Symposium    2461
01x2x3x4x5xTime7.7x5.1xFFmallocFreeGuardMarkUspSweeper*CRCount*DangSan*Oscar*DangNull*perlbenchbzip2gccmcfmilcnamdgobmkdealIIsoplexpovrayhmmersjenglibquantumh264reflbmomnetppastarsphinx3xalancbmkGEOMEAN01x2x3x4x5x6x7xMemory8.9x22.0x18.0x11.7x134.6x10.4x10.9x20.5x8.1x17.0xFigure 4: Overhead on PARSEC 3 with various CPU cores. White bar means the execution hangs or crashes.
In this evaluation, we configured FFmalloc to release mem-
ory only if there are eight consecutive freed pages. If we
release memory more aggressively, FFmalloc will have lower
memory overhead and higher performance overhead. We will
discuss the tradeoff between time and memory in §6.4.
An apparent observation from Figure 3 is that the memory
overhead is more diverse than the performance overhead. The
values of the standard deviation range from 0.24 (CRCount)
to 30.37 (DangSan), while the maximum standard deviation
of the performance overhead is merely 1.25 (DangNull). The
common pattern is that, for some benchmarks, most of the
tested tools show significantly higher memory overhead than
that on other benchmarks. Taking omnetpp as an example,
FFmalloc and pSweeper consumes about 4.0× of the original
memory; FreeGuard requires 11.7×; DangSan takes 134.6×;
Oscar spends 4.9×; CRCount and MarkUs consume about
1.7× of the original memory. The extreme memory overhead
is likely due to the characteristic of the program. There is no
tool that always outperforms others on memory usage.
6.2 Multi-threaded benchmarks
We ran 15 benchmarks with seven different core counts,
specifically, 1, 2, 4, 8, 16, 32 and 64, together with four mem-
ory managers: glibc, our FFmalloc, MarkUs, and FreeGuard.
All benchmarks and core count combinations ran success-
fully using glibc or FFmalloc, except netferret with 64 cores
which alway hung. In comparison, MarkUs failed 19 execu-
tions, while FreeGuard failed eight. All failures happen while
running dedup, ferret, netdedup, netferret, swaptions and
vips. In fact, MarkUs and FreeGuard had multiple random
crashes during other executions. To get meaningful results,
we ran each instance ten times, and reported the first three
successful executions. While still widely used in the literature,
the PARSEC 3.0 benchmarks are no longer in active devel-
opment. When they failed to run on Ubuntu 18.04, we chose
to accept this rather than attempting to patch the benchmarks
which would result in incomparable results. Figure 4 shows
our evaluation results, including the time overhead and the
memory overhead. Eighty-three instances are supported by
all memory managers. Failed executions are represented as
white bars.
Performance Overhead. Considering all successful execu-
tions, FFmalloc introduces 33.1% performance overhead (ge-
ometric mean), compared with 42.9% for MarkUs and -0.18%
for FreeGuard. However, if we only consider the 83 instances
supported by all tools, the overhead is 21.9% for FFmalloc,
43.0% for MarkUs and 1.68% for FreeGuard. FFmalloc only
introduces relatively high overhead to four out of 15 programs
– dedup, netdedup, swaptions and vips, where the geometric
mean is 157.8%, compared with 584.6% for MarkUs and -
18.0% for FreeGuard. FFmalloc demonstrates merely 4.3%
2462    30th USENIX Security Symposium
USENIX Association
12481632640100200blackscholes12481632640100200bodytrack12481632640100200300canneal12481632640255075dedup12481632640200400600facesim12481632640200400ferret12481632640200400600ﬂuidanimate12481632640250500750freqmine1248163264050100netdedup12481632640250500750netferret124816326405001000netstreamcluster124816326405001000streamcluster1248163264050010001500swaptions12481632640100200vips12481632640100200x264Time12481632640246×105blackscholes1248163264012×105bodytrack12481632640.00.51.0×106canneal12481632640123×106dedup12481632640246×105facesim1248163264024×105ferret12481632640.00.51.0×106ﬂuidanimate12481632640123×106freqmine1248163264012×106netdedup1248163264024×105netferret12481632640246×105netstreamcluster12481632640.00.51.01.5×105streamcluster1248163264024×106swaptions12481632640.00.51.01.5×106vips12481632640.00.51.01.52.0×106x264MemoryTable 4: CPU overhead of secure allocators on ChakraCore. The
numbers are overall average score from all benchmarks (with 10
iterations). Red values mean the performance decreased, while green
values indicate performance improved.
Table 5: Memory overhead of secure allocators on ChakraCore.
The numbers are average peak memory use from all benchmarks
(with 10 iterations). We show glibc memory usage in kilobytes, and
show others as changes over glibc’s.
Benchmark
WebTooling
Octane
Kraken
SunSpider
JetStream
glibc
25.53
9706.8
603.0
21.86
97.6
FFmalloc MarkUs
-0.43%
-4.81%
0.28%
3.28%
-3.07%
-0.16%
-0.73%
0.07%
1.88%
0.20%
FreeGuard
1.26%
3.22%
0.03%
1.30%
1.27%
Benchmark
WebTooling
Octane
Kraken
SunSpider
JetStream
glibc
454,980
148,220
63,344
103,252
195,552
6.09%
FFmalloc MarkUs
70.99%
142.53% 252.55%
536.43% 872.17%
378.36% 405.63%
74.75%
162.71%
FreeGuard
35.35%
84.28%
765.30%
440.97%
241.61%
Figure 5: NGINX throughput with various allocators. X-axis
shows the thread number; Y-axis presents the connection number per
second. β-connection means NGINX accepts β parallel connections.
Figure 6: NGINX latency with various allocators. X-axis shows
the thread number, and y-axis presents the connection number per
second. β-connection means NGINX accepts β parallel connections.
overhead for others.
We observed several interesting facts about the mem-
ory overhead. First, FFmalloc’s overhead monotonically in-
creased from 5.7% to 50.9% when we used one to 64 cores.
This is expected as FFmalloc uses locks to prevent race con-
ditions and to synchronize the status of the memory man-
ager. The Linux kernel also has a global lock for mmap/munmap
system calls, which further increases the overhead for multi-
core executions. MarkUs followed a similar pattern, but with
several exceptions due to unsupported executions. Second,
FreeGuard could sometimes improve performance over glibc.
For example on dedup, the execution with FreeGuard is 3.8×
faster when running with 64 cores. This is due to the signif-
icantly fewer number of madvise system calls compared to
that of glibc [43].
Memory Overhead. On the geometric mean, FFmalloc in-
troduces 50.5% memory overhead to all successful executions,
compared with 13.0% for MarkUs and 141.2% for FreeGuard.
For the 83 executions supported by all four allocators, the over-
heads are 35.6%, 13.3% and 67.5%, respectively. FFmalloc
brings slightly higher overhead to bodytrack (3.3× of orig-
inal usage) and swaptions (10.5×). We find that compared
with others, these two programs use relatively little memory
( 27.67%
+11.90% +5.70% 27.76% –> 33.07%
-0.02% 27.86% –> 27.85%
-3.70%
performance of JetStream by 0.20%. MarkUs adds the most
overhead, 4.81% to Octane and 3.28% to SunSpider while
FreeGuard improves the performance for three out of five
benchmarks. Regarding memory usage, Table 5 shows that
FFmalloc imposes the least overhead among the three se-
cure allocators for three of the programs. MarkUs adds the
least overhead on JetStream while FreeGuard was the best
on Octane. Overall, FFmalloc, like the other secure alloca-
tors, introduces consistently negligible performance overhead
to ChakraCore, but typically does so with significantly less
memory use.
6.3.2 NGINX
We tested the NGINX webserver through the wrk benchmarking
tool [26] with different settings. A setting with α threads and
β connections means that wrk uses α threads to send requests
to NGINX in parallel, and keeps β connections open at any
moment. We ran each setting for 60 seconds and repeated
the evaluation using glibc, FFmalloc, MarkUs and FreeGuard.
Figure 5 and Figure 6 show the evaluation results. The y-axis
of Figure 5 is NGINX throughput in requests-per-second; a
higher number indicates better performance. FFmalloc and
FreeGuard add negligible overhead in multiple settings, and
only show notably higher overhead for the 12-thread, 100-
connection setting (47.6% decrease for FFmalloc and 58.8%
decrease for FreeGuard). MarkUs has the lowest through-
put for most settings. As the number of threads increase, its
performance consistently decreases and reaches 65.5% less
throughput for the 12-thread, 800-connection setting.
Figure 6 shows NGINX connection latency measured on the
client side. Both FFmalloc and FreeGuard introduce minor
overhead to the latency. MarkUs introduces significant latency,
especially for multi-thread connections.
We also measured the memory overhead for each NGINX
thread. On average, FFmalloc consumes 5.24× more memory
than glibc, similar to the 5.48× overhead of FreeGuard. How-
ever, MarkUs requires 77.72× more memory, which is much
higher than FFmalloc and FreeGuard. Overall, FFmalloc intro-
duces negligible overhead to NGINX, and outperforms MarkUs
for most of the settings.
6.4 Optimal Settings of FFmalloc
We explored multiple options of releasing memory to find
the one enabling the optimal performance and memory usage.
First, we configured FFmalloc to release consecutive freed
memory with at least α pages (details in §4.2). We tested three
different α values, specifically, 32, 8 and 2. In theory, a smaller
α means FFmalloc will release memory more frequently, and
thus will have higher performance overhead and lower mem-
ory overhead. A larger α will have the opposite effect. Second,
we configured FFmalloc to use munmap or madvise to return
memory to the system. munmap forces the kernel to immedi-
ately release the memory, while madvise leaves the kernel
to release the memory during high memory pressure. We ex-
pected that munmap would have higher performance overhead
and lower memory overhead than madvise. Figure 7 shows
the performance and memory overhead of FFmalloc on SPEC
CPU2006 C/C++ benchmarks, with six different settings.
Munmap vs Madvise. Figure 7 confirms our expectations
of the two system calls on memory overhead, but shows a
counter-intuitive result on performance overhead. The Mem-
ory figure shows that FFmalloc with madvise can have sig-
nificantly higher memory overhead than that of munmap. For
example, FFmalloc consumes 198.5× more memory than
the original execution if it postpones the memory release
via madvise, while the overhead is only 7.6× with munmap.
However, the Time figure indicates munmap also outperforms
madvise on performance, from 0.28% to 0.76%. Although
the difference is not significant, considering the high mem-
ory overhead, it is clear that we should use munmap instead of
madvise to release freed memory to the kernel.
We inspected three programs to understand why madvise
sometimes is slower than munmap. The results in Table 6 indi-
cate that both cache misses and extra instructions contribute
to the slower execution of madvise. With the madvise system
call, the Linux kernel does not immediately reclaim pages due
to the low memory pressure in our system. Therefore, future
mmap syscalls will likely get a new physical page that is not
present in the cache. In contrast, munmap forces the kernel to
immediately release the physical page and future mmap calls
can reuse the in-cache physical pages, leading to fewer cache
misses. For SPEC program gcc, running with madvise exe-
cutes 5.70% more instructions, causing the most significant
overhead on madvise.
Minimum Freed Pages. Figure 7 shows that the minimum
consecutive freed pages α is more correlated to memory over-
head than to the performance. The performance overhead
of FFmalloc is 1.71%, 2.21% and 2.22%, respectively for α
values of 32, 8, and 2. Although this is consistent with our
intuition that smaller α leads to higher overhead, the differ-
ence is not very large. Additionally, not all executions exactly
follow this pattern. For example, mcf shows the slowest ex-
ecution when α is set to 8, not 2. On the other hand, α has
a strong impact on the memory overhead when FFmalloc re-
2464    30th USENIX Security Symposium
USENIX Association
Figure 7: Overhead of FFmalloc with different settings. FreeX means that to release memory, FFmalloc returns at least X consecutive freed
pages to the system, through either munmap (U) or madvise (A).
leases memory with munmap. Especially for programs with
extremely high overhead, like omnetpp, setting a smaller α
can reduce the memory overhead to a reasonable range (from
8.6× to 2.9×). The value of α has no impact on the overhead
with madvise, as madvise does not immediately release the
memory by design. Therefore, the α value 2 is the best choice
among all three values.
During our evaluation, we used the α value 8 and munmap
to test all benchmarks and programs. Therefore, FFmalloc’s
performance overhead can be further reduced if we release
memory less aggressively. Alternately, its memory overhead
can be reduced by releasing memory more agressively.
7 Discussion
7.1 Other Technical Details
Supporting More Functions. Currently, FFmalloc covers
the standard C library functions for memory manage-
including malloc, free, realloc, reallocarray,
ment
calloc, posix_align, memalign, aligned_alloc
and
malloc_usable_size. FFmalloc does not contain wrappers
of system calls like mmap and munmap. If an application
directly calls mmap and munmap to get memory, a use-after-free
bug may escape the protection of FFmalloc. In this case,
FFmalloc would be unaware of the address space previously
occupied by these mappings and might use them again (only
once) for its own allocation. This escape would also affect
any other secure allocator, but we have not seen it addressed
elsewhere in the literature.
For simple requests to mmap (private, non-file backed, no
fixed address), a future version of FFmalloc could handle them
via the existing jumbo allocation code path. However, it is less