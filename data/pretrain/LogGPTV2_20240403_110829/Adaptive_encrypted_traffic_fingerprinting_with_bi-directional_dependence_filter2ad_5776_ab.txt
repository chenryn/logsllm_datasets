Category
Packet (Up/Dn)
Uni-Burst (Up/Dn)
Features
Packet length
Uni-Burst size
Uni-Burst time ∗
Uni-Burst count
Bi-Burst size ∗
Bi-Burst time ∗
Bi-Burst (Up-Dn/Dn-Up)
∗new features introduced in this paper
Table 1: Features from Packets, Uni-Bursts, and Bi-Bursts.
show the applicability of both closed-world and open-world settings
while utilizing the BIND feature extraction method.
3. PROPOSED APPROACH
In this section, we present the methodology to extract the BIND
features, and detail the ADABIND approach.
3.1 Features
With encrypted payload of each packet in a trace, we extract features
from packet headers only. The main idea is to extract features from
consecutive bursts to capture any dependencies that may exist between
them. As illustrated in Figure 2, we call the burst directed from a
user/client (or app) to server (e.g., burst a), an uplink uni-burst (or Up
uni-burst), and the burst directed from server to the user, a downlink
uni-burst (or Dn uni-burst) (e.g., burst b). Similar to packets, a burst or
uni-burst has features such as size (or length), time, and direction.
Uni-burst size is the summation of lengths of all its packets. Packet
time is the departure/arrival timestamp in the uplink/downlink direction,
measured near the user-end of the network by a man-in-the-middle.
Uni-burst time is the difference between the last packet’s timestamp
and the ﬁrst packet’s timestamp within a burst, i.e., the time taken to
transmit all packets of a burst in a speciﬁc direction. Here, the term
burst and uni-burst are equivalent. The name uni-burst emphasizes on
the fact that features are extracted from a single burst, as opposed
to Bi-Burst which is a tuple formed by a sequence of two adjacent
uni-bursts in opposite direction (e.g., burst b and c in Figure 2).
Bi-Burst features. Features extracted from Bi-Bursts are as follows.
1. Dn-Up-Burst size: Dn-Up-Burst is a set of tuples formed by
downlink (Dn) - uplink (Up) consecutive bursts. Here, unique
tuples are formed according to the corresponding uni-burst
lengths where each tuple forms a new feature.
2. Dn-Up-Burst time: This set of features considers unique
consecutive uni-burst time tuples between adjacent Dn uni-burst
and Up uni-burst sequences.
3. Up-Dn-Burst size: Similar to Dn-Up-Burst size features, these
features consider burst length tuples of adjacent Up uni-burst and
Dn uni-burst sequences.
4. Up-Dn-Burst time: Similar to Dn-Up-Burst time features, this
set of features considers burst time tuples formed by adjacent Up
uni-burst and Dn uni-burst sequences.
In each trace, we count such unique tuples to generate a set of
features. To overcome dimensionality issues associated with burst sizes,
quantization [15] is applied to group bursts into correlation sets (e.g.,
based on frequency of occurrence).
Packet and Uni-Burst features. In addition to the Bi-Bursts features,
we also use burst size and burst time features. Previous studies [17]
only consider total trace time as a feature, contrary to the burst time
feature we use in this paper. Furthermore, we also consider the count of
packets within a burst as a feature. In order to capture variations of the
180
i
T
m
e
a
b
c
d
e
f
g
 s  = 200 , t = 0 
Up Uni-Burst
 s = 200, t = 50
Bi-Burst or Dn-Up-Burst
 s = 800, t = 125
Bi-Burst or Up-Dn-Burst
 s  = 500 , t = 280
Client (Up)
Server (Dn)
Figure 2: An example illustrating BIND Features.
packet features, we use an array of unique packet lengths as well. The
set of features, termed as BIND, are listed in Table 1. All these features
are concatenated to form a large array of features (histograms) to be
extracted from each trace. A set of multiple traces represented in this
manner forms the training and testing set.
Example. Figure 2 depicts a simple trace where packet sequences
between uplink and downlink are shown. Each packet in the ﬁgure has
size s in bytes and time t in milliseconds. We set time for the ﬁrst
packet in the trace to zero, as a reference. An example of a uni-burst is
shown as burst a, whose size is 500, computed by adding packet sizes
s = 200 and s = 300 that form the burst. Its time is computed as 10,
which is the absolute time difference between the last packet (t = 10)
and the ﬁrst packet (t = 0) in the burst. Similarly, a Bi-Burst example
is shown as well, formed with a combination of bursts b and c. This is
denoted as Dn-Up-Burst. In this case, the Bi-Burst tuple using the burst
size (i.e., Dn-Up-Burst size) is represented as {DnUp_2300_400},
where 2300 is the burst size of b, and 400 is the burst size of c. We
count the number of such unique tuples in the trace. In this case, the
count for {DnUp_2300_400} is 1.
3.2 Learning
In the closed-world setting, we use the BIND features to train a
support vector machine (SVM) [11] classiﬁer. SVM applies convex op-
timization and maps non-linearly separated data to a higher dimensional
linearly separated feature space. Whereas in the open-world setting,
using the BIND features, we apply the weighted k-Nearest Neighbor
(k-NN) approach proposed in [36]. Feature weights are computed
using traces from the monitored set. During testing of traces with
initialize 
S
R
test
R
S
concept drift
update
R
S
test
time
Figure 3: Illustration of ADABIND.
Static Learning
unknown class labels, these feature weights are applied. Majority class
voting among k-Nearest Neighbors is performed to predict class label
of a test trace. Additionally, we also use a Random Forest classiﬁer in
the open-world setting. Instead of performing feature weighing, which
is computationally expensive, we use a set of weak learners to form an
ensemble of decision trees (random forest).
3.2.1
Typically, previous studies (mentioned in §2.1) have focused on
performing ﬁngerprinting by collecting traces for a short period of time.
Classiﬁers are trained on traces collected within this time period, and
used to predict class labels thereafter. We refer to this type of classiﬁer
training as static. On the contrary, WFIN and AFIN can be viewed as a
continuous process involving trace collection over a long period of time.
Moreover, data collection is time consuming. Changes in data content
transmitted between end-nodes affect patterns captured in the model.
Using a static model to predict class labels of test traces in this situation
drastically affects classiﬁcation performance.
3.2.2 Adaptive Learning
We now present the details of ADABIND. In this section, we show
how we model encrypted data ﬁngerprinting in an adaptive manner. As
discussed in §3.2.1, over time, the data patterns of the current traces
may be different from the patterns in previously seen training traces.
This is known as concept drift [19,20]. To address this challenge, the
model has to be updated (re-trained) regularly. We study the effect of
re-training as follows.
Fixed update. One simple approach is to apply ﬁxed updates to
re-train the model periodically. We refer to this approach as BINDFUP
(BIND Fixed UPdate). BINDFUP updates the model periodically,
regardless of any concept drift that may happen. The model will
be re-trained regularly (e.g., at the end of every week) with freshly
obtained training data. There are two possible scenarios, early update
and late update. In early update, BINDFUP updates the model in a way
that ensures no concept drift in data. Although this update is more
accurate and stable, it may suffer from unnecessary re-training which
will add signiﬁcant overhead to the classiﬁcation process. On the other
hand, late update may miss possible concept drift in data over time
which affects the overall performance of the model.
Dynamic update. In this approach, as depicted in Figure 3, we update
the model whenever there is a drift between the current data and
previously seen training data. R is a training window that builds
the model, while S is a sliding window that probes this model for
any possible concept drift (i.e., model needs update). Algorithm 1
describes this dynamic update mechanism. We refer to this algorithm as
BINDDUP (BIND Dynamic UPdate). BINDDUP starts by considering a
portion of data as a training window to initialize the ADABIND model
181
F train ← extractFeatures(TrainX R);
initializeModel(ADABIND,F train);
for each S do
Algorithm 1: BINDDUP
Data: Training Data: T rainX, Testing Data: T estX
Input: Training Window: R, Sliding Window: S, Threshold: T
1 begin
2
3
4
5
6
7
8
9
10
11
12
13
14 end
move R;
F train ← extractFeatures(TrainX R);
updateModel(ADABIND,F train);
move S;
F test ← extractFeatures(TestX S );
accuracy ← validateModel(ADABIND,F test);
if accuracy < T then
end
end
# of websites
Dataset
HTTPS [25] Monitored
Non-Monitored
Monitored
Non-Monitored
TOR [36]
# of traces
per website
70
1
90
1
30
970
100
5000
Table 2: Statistics for Website Fingerprinting datasets in the
open-world setting.
(lines 2 and 3). Then, the subsequent instances are considered within a
sliding window to validate the performance of this model over time
(lines 5 and 6). If the accuracy drops below a predeﬁned threshold (line
7), the initial ADABIND model becomes obsolete (i.e., concept drift)
and the training window moves (line 8) to get new instances to re-train
and update the model (lines 9 and 10). BINDDUP utilizes the ADABIND
updated model to test incoming new data in a continuous fashion.
4. EVALUATION
In this section, we present the empirical results of using BIND for
WFIN and AFIN, comparing it with other existing methods.
4.1 Datasets
We use two existing datasets for evaluating WFIN, one using HTTPS
and the other using the Tor anonymity network, referred to as HTTPS
and TOR respectively. These datasets have been widely used in previous
research on trafﬁc ﬁngerprinting. For AFIN, we collect our own dataset
from apps that use the HTTPS protocol.
Website Datasets. The ﬁrst dataset presented in [25], which we
denote as HTTPS, was collected while browsing websites using the
HTTPS protocol along with a proxy server to imitate an anonymity
network. The authors followed a ranking procedure to select the most
accessed websites in their school department. The second dataset
is described in [36]. This dataset is collected by capturing packets
generated from a browser connected to the Tor anonymity network. We
denote this dataset as TOR.
HTTPS consists of 1000 websites with 200 traces each. For WFIN,
we evaluate the closed-world setting by randomly picking a subset of
these 1000 websites. For the open-world setting, we randomly select 30
websites as the monitored set, and the rest as the non-monitored one.
The other dataset (TOR) consists of two sets of traces. The ﬁrst is a
set of 100 websites that have 90 traces each. These websites were
selected from a list of blocked websites by some countries. We use
this for the closed-world experiments. The second set consists of
5000 websites that have one trace each. These websites were selected
Category
# of apps
Monitored
APP-FIN
Non-Monitored
APP-COMM
Non-Monitored
APP-SOCIAL Non-Monitored
30
2238
1061
1290
# of traces
per app
20
1
1
1
Table 3: Dataset statistics for App Fingerprinting in the open-
world setting.
s
p
p
A
%
60
40
20
0
Wireless Access Point
Android Phone
Internet
Switch (with Port Mirroring)
Packet Sniffer Server
Data
Analysis
Method
Setting
Type
VNG++ [17]
Closed
P [28]
Closed
Figure 4: Illustration of the app trace data collection process
from Alexa’s top websites [1]. In the open-world setting, we use the
set of 100 websites as monitored, and the set of 5000 websites as
non-monitored. The summarized statistics of these datasets are provided
in Table 2. These two datasets enable us to perform an unbiased
comparison of BIND with other competing methods.
App Dataset. For AFIN, we evaluate BIND using a dataset that we
collected by executing multiple Android apps on a Samsung Galaxy S
device, running Android version 4.3.1. We randomly select about
30,000 apps from three different categories in Google Play Store. The
categories include Finance, Communication, and Social. We refer to
them as APP-FIN, APP-COMM, and APP-SOCIAL respectively. We
then install and launch these apps on the phone which is connected to
the Internet via a wireless router. Each trace per app is collected over
a 30-sec period passively using a mirroring switch at the wireless
router. Figure 4 illustrates this data collection setup. We ﬁltered the
captured trafﬁc to contain packets from ports 80, 8080, and 443. We
then identify apps that use only HTTPS data from the captured traces.
These traces from such apps are then used to perform the closed-world
and open-world AFIN. It is important to note that we uninstall each app
as soon as we complete capturing a trace to avoid any background noise
during further trace generation.
Similar to WFIN, multiple traces of apps are required to train a
classiﬁer in the closed-world and open-world settings. We use the
APP-FIN dataset for performing the closed-world experiments as we
capture multiple traces for each app. We only capture a single trace per
app for APP-COMM and APP-SOCIAL to be used for the open-world
experiments as the non-monitored set. The dataset statistics for the
open-world setting are shown in Table 3. Note that in the closed-world
setting, we only evaluate using apps from APP-FIN. In the case of
open-world, the monitored apps are considered only from APP-FIN and
the non-monitored apps are considered from all categories shown in
Table 3.
While performing app selection for creating our dataset, we observed
a few interesting statistics that would further motivate the problem
of AFIN. Figure 5 shows the percentage of apps that use HTTP and
HTTPS data at launch in our initial set of 30, 000 apps. Observe that
Finance
Communication
Social
HTTP & HTTPS
HTTPS only
Figure 5: Empirical Statistics of Android Apps
Features
Classiﬁer
Uni-Burst Size & Count
Total Trace Time
Uplink/Downlink Bytes
Uni-Burst Size & Count
Packet Size
Packet Ordering
BIND features:
Bi-Burst Size & Time
Uni-Burst Size, Time, & Count
OSAD [37]
Closed
Cell Traces
BINDSVM ∗
Closed
WKNN [36]
Open
BINDWKNN ∗
Open
BINDRF ∗