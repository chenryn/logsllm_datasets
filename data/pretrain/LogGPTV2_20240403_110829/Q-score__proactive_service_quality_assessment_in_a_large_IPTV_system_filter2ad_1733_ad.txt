### 4.2.4 Feedback Aggregation
To demonstrate the effect of aggregating user feedback over different durations, Figure 6 compares various lengths of \(\gamma\). We observe that as \(\gamma\) increases, the regression performance improves. This can be attributed to the significant delay between the occurrence of a problem and the filing of user feedback, as discussed in Section 3.3. Shorter time bins (\(\gamma\)) may fail to capture feedback that is correlated with significant network indicator values due to this delay.

**Figure 6: Comparison of regression performance with different \(\gamma\) values**

### 4.2.5 Sensitivity to Training Duration
Finally, we evaluate the sensitivity of testing accuracy to the duration of training. In this experiment, we fix the testing duration and assess how accuracy changes with varying training durations. Table 3 shows the dates of the training and testing periods used in our evaluation. Figure 7 illustrates the accuracy trade-off curves for different training durations. We observe that, generally, testing accuracy improves as the training duration increases. However, the gain becomes marginal after 15 days of training, suggesting that 15 days is a suitable training period.

A closer examination of the curves for 15 and 20 days of training reveals that 15 days of training yields marginally better accuracy. A possible reason for this is a network-wide STB firmware upgrade that occurred between August 10, 2010, and August 14, 2010. This upgrade may have affected the measurement of STB logs (e.g., audio and video quality, syslog, reset, and crash logs) and impacted the learning of \(\beta\). Given that such glitches are common in real data, we accept a small amount of noise. Overall, we find that 15 days of training is sufficient to learn \(\beta\).

**Summary**
In this section, we evaluate the accuracy and robustness of Q-score. Q-score, combined with multi-scale temporal and spatial aggregation, successfully predicts 60% of service problems reported by customers with only a 0.1% false positive rate. An in-depth analysis is warranted, but preliminary tests show that the remaining 40% of unpredicted issues are either (i) unrelated to any of the network KPIs we measure (e.g., remote controller malfunction, wiring issues between STB and TV inside home) or (ii) fallacies not captured by our regression (e.g., gradual and long-term changes in network KPIs). For (i), since feedback is reported and logged by humans in plain text, it is difficult to completely rule out trouble tickets unassociated with our KPIs, so we account for a small portion of misclassification as inherent noise. For (ii), we address these issues with our previous works Giza [18] and Mercury [19], which are specifically designed to detect and mitigate recurring and persistent events in application service networks. In future work, we plan to conduct an extensive analysis on the false negatives to determine the proportions of the issues in each category and further improve the success rate.

**Table 3: Training and Testing Durations**
| Training Duration | Dates | Duration |
|-------------------|-------|----------|
| 5 days            | 08/25/2010 - 08/29/2010 | 5 days |
| 10 days           | 08/20/2010 - 08/29/2010 | 10 days |
| 15 days           | 08/15/2010 - 08/29/2010 | 15 days |
| 20 days           | 08/10/2010 - 08/29/2010 | 20 days |
| 30 days           | 08/01/2010 - 08/30/2010 | 30 days |

### 5. APPLICATION
In this section, we demonstrate the utility of Q-score through three applications. First, we present a set of network KPIs closely related to user-perceived service quality. Second, we illustrate how Q-score can predict user calls. Third, we show the possibility of intelligently dimensioning the call center workforce. In all applications, we identify interesting results through online analysis of Q-score.

#### 5.1 Identification of Significant KPIs
Today’s commercial IPTV services support up to millions of user devices. If a few KPIs are monitored continuously for each device, the measurement space can easily reach the order of billions. Additionally, time-lapse analysis in diagnosis requires multiple data snapshots in short periods. Therefore, in the service assurance of a large-scale IPTV system, it is infeasible to blindly measure, collect, and analyze such a large volume of diverse KPIs from the entire network. In this application, we discuss our experience in identifying a small number of significant KPIs with respect to user-perceived quality of experience.

**Significant KPIs**
In generating Q-score, we relate network KPIs and user feedback using the factor \(\beta\). The magnitude of \(\beta\) measures the relevance of significant KPIs. Tables 4 and 5 list the top ten significant KPIs for relatively long history hours (15-24 hours) and short history hours (3-9 hours), respectively. When regressed with individual users’ feedback, the significant KPIs exhibit some commonality (shown in bold) and differences.

**Table 4: Significant KPIs for Large \(\delta\) (15-24 hrs)**
| KPI Type          | KPI Label             | \(\beta\) Coef. |
|-------------------|-----------------------|-----------------|
| Network delivery  | Tuner fill            | 0.68            |
|                   | Hole Too Large        | 0.63            |
|                   | Decoder stall         | 0.61            |
|                   | Bytes processed per sec | 0.42            |
| Audio             | Audio decoder errors   | -0.32           |
| Video             | Video DRM errors      | 0.84            |
|                   | Video decoder errors   | 0.73            |
|                   | Video frames decoded   | 0.53            |
|                   | Video data throughput  | -0.49           |
|                   |                       | -0.49           |

**Table 5: Significant KPIs for Small \(\delta\) (3-9 hrs)**
| KPI Type          | KPI Label             | \(\beta\) Coef. |
|-------------------|-----------------------|-----------------|
| Network delivery  | Tuner fill            | 0.60            |
|                   | Bytes processed per sec | 0.57            |
|                   | ECM parse errors      | -0.34           |
| Audio             | Audio decoder errors   | 0.32            |
|                   | Audio samples dropped  | 1.03            |
|                   | Audio crypto error     | 0.84            |
|                   | Audio data dropped     | 0.64            |
|                   | Audio DRM errors      | 0.55            |
| Video             | Video DRM errors      | 0.34            |
|                   |                       | 0.63            |

**Table 6: Significant KPIs for Multi-Scale Temporal Aggregation (0-24 hrs)**
| KPI Type          | KPI Label             | \(\beta\) Coef. |
|-------------------|-----------------------|-----------------|
| Network delivery  | Tuner fill            | 0.67            |
|                   | Src unavailable received | 0.50            |
|                   | Hole without session packets | 0.52            |
|                   | ECM parse errors      | 0.35            |
|                   | Bytes processed per sec | -0.33           |
| Audio             | Audio decoder errors   | 0.74            |
|                   | Audio data dropped     | 0.57            |
|                   | Audio crypto error     | 0.44            |
| Video             | Video DRM errors      | 0.68            |
|                   | Video frames dropped   | 0.65            |

**Observations**
We observe an interesting finding by comparing significant KPIs for long-term event durations (large \(\delta\)) and short-term event durations (small \(\delta\)). The former tends to have more video-related KPIs as the most significant ones, while the latter has more KPIs related to audio. This relates to the relevance of audio and video in user experience. Audio data is more susceptible to losses and errors than video data because the total volume of audio data is much less, making the impact of lost or delayed audio data relatively greater. Viewers have less tolerance for audio issues and report them earlier than video issues. The contrasting findings between long and short history hours highlight that the urgency of issues depends on whether they are audio or video related.

Another finding from the KPI analysis is drawn from multi-scale temporal aggregation. By combining long-term and short-term event durations in regression, both video and audio-related issues appear as the most significant KPIs, confirming the effectiveness of letting the regression algorithm choose important KPIs among multiple temporal aggregations.

Noticing that different KPIs have different degrees of relevancy to user feedback, we aim to guide monitoring of network KPIs by enlisting a small number of significant KPIs to user-perceived service quality. This way, forthcoming fine-grained network diagnosis can focus on the significant KPIs rather than analyzing an excessive amount of KPIs.

#### 5.2 Predicting Bad Quality of Experience
For Q-score to be useful for alerting services, it should provide triggers well before users start to call. Thus, there is a need to study how far into the future we can infer customer calls using Q-score. To understand the feasible level of proactiveness in Q-score, we evaluated two characteristics: (i) the growth pattern of Q-score over time and (ii) stability of Q-score with a time gap between network events and user feedback.

**Growth of Q-score Over Time**
Figure 8 shows the growth pattern of Q-score for individual user IDs who filed trouble tickets. The time is aligned by the trouble ticket filing time (time = 0), and we observe how Q-score grows. The solid line represents the average value of the scores, and the upper and lower tips of error bars represent one standard deviation plus and minus the average. From the graph, we observe that the increase in average Q-score is close to linear when it is greater than 0.05. The monotonic and gradual increase of Q-score suggests the possibility of using Q-score as a proactive trigger for alerting because (i) it keeps increasing once it becomes non-negligible and (ii) its growth is not too abrupt. However, due to great variance among different users’ Q-scores, we cannot use a Q-score of 0.05 as the significant value triggering forthcoming actions. Instead, we seek a more realistic lead time by conducting a further study on the stability of Q-score.

**Feasible Level of Proactiveness**
As mentioned in Section 3.3, user feedback has an indeterminate delay from the occurrences of network events. Here, we test the amount of lead time Q-score can provide before customer calls by measuring the accuracy loss as we increase the time gap between the occurrence times of network events and the filing of user feedback.