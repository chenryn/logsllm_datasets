longitude. However, our landmarks also exhibit two important
differences with respect to this geographic analogy.
First our landmarks are not ﬁxed and set for the whole
system; rather, each pair of peers randomly generates its
own set of landmarks. This prevents cross-pair comparisons.
Second, we use far fewer landmarks than there are dimensions
in our system. This prevents a precise reverse computation
of each peer’s clear-text coordinates (i.e.
its proﬁle) from
its landmark coordinates. Thanks to these differences, users
can safely exchange their landmarks because they do not
characterize their interests in any speciﬁc topic.
Figure 4 presents an overview of the operation of H&S
by means of an example. Alice and Bob need to compute
their similarity with each other. In a traditional system like
the one described in Section II, Bob would send his proﬁle
to Alice and Alice would send hers to Bob. Each of them
would then compute the similarity by applying Equation (1).
With H&S, none of this happens. Rather, Alice and Bob
follow these 6 steps. (1) They create a secure communication
channel. (2) They each derive a compact version (Bloom
ﬁlter) of his/her proﬁle. (3) They agree on a set of random
landmarks. (4) They each compute the similarity of his/her
compact proﬁle with each landmark. (5) They each gather
these similarity values in a similarity vector. (6) They exchange
each other’s similarity vector and compute their ﬁnal similarity
estimate. From a practical perspective, this translates into to
two main components: a landmark generation mechanism, and
a similarity estimation protocol. In the following we detail each
of these two contributions.
265265
A. Landmark Generation
H&S uses landmarks to estimate the similarity between two
peers without requiring them to exchange their proﬁles with
each other. To prevent adversaries from reconstructing proﬁle
information from these landmarks, the landmark generation
mechanism must satisfy a set of requirements.
i Computation conﬁdentiality: Only the two peers participat-
ing in the similarity computation may access the data they
exchange. This includes landmark and similarity values.
ii Independence of peer proﬁles: Landmarks must be random
and independent of the proﬁles of the peers that generate
them.
iii Fair landmark generation: The choice of the landmarks
must be fair. Neither of the two participating peers may
bias the generated landmarks.
iv Minimal information release: An attacker should not be
able to reconstruct a target proﬁle by combining informa-
tion from multiple landmark similarities, or by repeatedly
computing its H&S similarity with the target.
In the following, we present our landmark generation
mechanism by focusing on how it addresses each of these
requirements. We detail the various steps in lines 1 through 18
of Algorithm 1.
1) Computation Conﬁdentiality: Requirement (i) states that
third-party peers should not be able to eavesdrop any commu-
nication between peers that are computing their similarity. To
achieve this, H&S encrypts all the communication between two
peers, including that relative to landmark generation.
Speciﬁcally, each peer maintains a public/private key pair.
Peers exchange their public keys with each other by attaching
them to the information transferred through the RPS and
clustering protocols, similar to what was is done in [6]. In
addition, we assume that peers may verify the authenticity of
a public key by means of a certiﬁcation authority or a web of
trust [10], [7].
Peers use their key pairs to establish a secure communica-
tion channel whenever they need to evaluate their similarity.
To this end, they exploit an authenticated key agreement (AK)
protocol [11] as shown in lines 1 and 2. A possible AK
protocol consists of an authenticated variation of the elliptic
curve Difﬁe-Hellman key agreement such as the one available
in the NaCl cryptographic library [12].
2) Independence of peer proﬁles: Requirement (ii) states
that landmarks consist of randomly generated proﬁles that are
independent of the proﬁles or of the choices of participating
peers. However, as we discussed in Section II, proﬁles consist
of lists of item-score pairs, where the items belong to an
unbounded or at least very large universe. This would make
it difﬁcult, if not impossible to generate random landmarks.
To circumvent this problem, H&S replaces traditional proﬁles
with compact proﬁles (step 2 in Figure 4).
A compact proﬁle consists of a Bloom ﬁlter [13] and con-
tains only the items considered as liked by the corresponding
peer. A Bloom ﬁlter provides a compact representation of a
set in the form of an array of n bits. To add an item to the
set, the bloom ﬁlter applies h hash functions to the item to
obtain h bit positions in the array and sets these positions to
266266
1. To query for the presence of an item, the ﬁlter uses the same
hash functions and checks if all the bits at the h indexes have
a value of 1.
Compact proﬁles carry slightly less information than full
proﬁles. First, Bloom ﬁlters can return false positives even
though they never return false negatives. Second, compact
proﬁles cannot distinguish between disliked items and items to
which the user has not been exposed. This does not constitute a
problem: the like status of items proves sufﬁcient to describe
the interests of peers, and the effect of false positives may
actually be beneﬁcial in terms of privacy. Compact proﬁles
also reduce Equation (1) to counting the number of common
bits between the two bloom ﬁlters.
Given a user or peer, p ∈ {1, 2, . . . , N}, we denote
her compact proﬁle as (cid:2)cp ∈ Z
n
2 . Lines 10 through 18 of
Algorithm 1 show how peers use compact proﬁles to generate
random landmarks. Let L be a system parameter specifying the
number of landmarks to generate and let PRNG be a pseudo-
random number generator whose code is available to all peers
(for example MRG32k3a [14] or Mersenne Twister [15]). Two
peers, say p1 and p2, may generate a set of landmarks by ﬁrst
generating a common random seed (lines 10 to 13 in Algo-
rithm 1). Then, each of them saves this seed (line 14), along
with a timestamp, and uses it to initialize the PRNG (line 15).
Finally Each of the two peers independently uses the PRNG
to generate the L landmarks: {Mi} with i ∈ {0, 1, . . . , L}
(lines 16-18). Each generated landmark consists of a vector of
bits of the same size as a compact proﬁle, with a few random
bits (around 5%) set to 1, while other bits are set to 0. This
proportion of set bits mimics that of compact proﬁles, which
are usually sparse.
3) Fair Landmark generation: Requirement (iii) states that
the choice of the landmarks must be fair. To achieve this,
peers agree on their common seed using a bit-commitment
scheme like Blum’s coin-ﬂipping protocol [16]. Blum’s pro-
tocol operates as follows. Both p1 and p2 ﬂip a coin. They
set the output of the protocol to 1 if they obtain the same
result, and to 0 otherwise. To exchange their coin-ﬂip results
without cheating, p1 and p2 employ a bit-commitment scheme.
After ﬂipping its coin, p1 sends p2 a commitment on its result
(f (concatenate(result, nonce))). Then p2 reveals its result
to p1, and p1 reveals its result as well as the nonce it used for
the commitment to p2. p2 cannot cheat because it is the ﬁrst
to send its result. p1 cannot cheat because p2 can then check
its result against the initial commitment.
Blum’s protocol does not provide an unbiased coin, which
is impossible in the two-party case [17], but a weaker fairness
guarantee that sufﬁces for our application. This guarantee holds
as long as a malicious party does not abort the protocol before
it ends. Since the two peers in our protocol use a secure
channel, if p2 aborts, p1 can deduce that p2 is trying to bias
the result.
4) Minimal information release: Requirement (iv) states
that attackers should not be able to reconstruct a target proﬁle
by combining information from multiple landmarks or by
repeatedly computing their similarity with the target. To satisfy
the ﬁrst part of this requirement, H&S similarity uses a small
number of landmarks with respect to what would be required
to reconstruct the original proﬁle. In Section IV-D, we show
Algorithm 1 H&S landmark-based similarity computation
protocol between peers p1 and p2, as executed by p1
1: session key ← AK(keyp1, pub keyp2)
2: secure channel ← connect(p2, session key)
3: if p2 is known then
s ← load seed(p2)
4:
if s is not older than thL then
5:
6:
7:
8:
9: end if
10: for all i s.t. 0 ≤ i < 32 do
seed ← s
goto 15
end if
r ← rand bit()
seed[i] ← coin f lip(r, secure channel)
(cid:2)Mi ← generate lm(prng)
11:
12:
13: end for
14: save seed(p2, seed, timestamp(now))
15: prng ← init prng(seed)
16: for all i s.t. 0 ≤ i < L do
17:
18: end for
19: for all i in 0 ≤ i < L do
20:
21: end for
22: send((cid:2)σp1, secure channel)
23: (cid:2)σp2 ← receive(secure channel)
24: similarity ← cosine((cid:2)σp1, (cid:2)σp2)
25: return similarity
σp1[i] ← cosine((cid:2)cp1, (cid:2)Mi)
that this does not signiﬁcantly impact the ability to provide
good recommendations.
To satisfy the second part of this requirement, H&S peers
do not generate new landmarks each time they meet. Rather
they only do so if their latest common set of landmarks is
older than a threshold, thL. To achieve this, they verify the
timestamp associated with their latest saved common seed. If
the timestamp is newer than the threshold, then they reuse the
seed, otherwise they generate a new random seed.
B. Similarity approximation
We conclude the description of our protocol by presenting
how H&S approximates the similarity between two peers using
its randomly generated landmarks. Let {M1, . . . , ML} be a set
of common landmarks known to peers p1 and p2. First, each
of the two peers independently computes its similarity with
each of these landmarks (step 4 in Figure 4 and lines 19-21
in Algorithm 1). This consists in applying Equation (1) to its
own proﬁle and each of the landmarks. Both p1 and p2 then
store the results of these computations in a similarity vector
(respectively (cid:2)σp1 and (cid:2)σp2) as shown in step 5 in Figure 4 and
on line 20 in Algorithm 1. Second, p1 and p2 exchange their
similarity vectors with each other. This consists of lines 22
and 23 in Algorithm 1. Finally (step 6 and line 24), p1 and
p2 compute their H&S similarity by applying Equation (1) to
their own similarity vector and to the one they have received
(note that cos( (cid:2)A, (cid:2)B) = cos( (cid:2)B, (cid:2)A)).
IV. EVALUATION
We evaluate H&S by applying it in the context of a gossip-
based decentralized recommendation system. Using publicly
267267
TABLE I.
CHARACTERISTICS OF THE TRACES IN TERMS OF NUMBER
OF USERS, NUMBER OF ITEMS, NUMBER OF RATINGS AND RATING RANGE.
ML-100k
ML-1M
Jester-1-1
# users
943
6,040
24,983
# items
1,682
3,900
100
# ratings
100,000
1,000,000
1,810,455
Rating range
[1 : 5] (integers)
[1 : 5] (integers)
[−10 : 10] (continuous)
available traces, we evaluate the quality of its recommenda-
tions, its ability to protect privacy, and the overhead it implies.
A. Methodology
1) Simulator: We use our own simulator written in Java.
The simulator takes as input a trace from a recommendation
system, consisting of user-item matrix of ratings, split into a
training set and a test set. The training set (80% of the ratings)
allows peer neighborhoods to converge, while the test set (the
remaining 20%) provides the ground truth to evaluate the
relevance of recommendations. The simulator operates in two
steps. First it uses the training set to simulate the convergence
of the clustered overlay, then it generates r recommendations
for each peer using the converged overlay and compares the
results with the ratings in the test set.
2) Datasets: Table I outlines the characteristics of the three
traces we use. ML-100k1 and ML-1M1 are traces from the
MovieLens [18] online movie-recommendation service. They
contain 100,000 and 1,000,000 ratings respectively. Jester-1-
12 is a trace for the Jester [19] online joke-recommendation
service. It is the ﬁrst third of Jester’s dataset-1.
3) Evaluation metrics: We evaluate recommendation qual-
ity in terms of precision and recall. The former evaluates
whether peers like the recommendations they receive. The
latter evaluates if recommendations cover all
the interests
expressed by the ground truth in the test set.
To evaluate H&S’s ability to protect privacy we consider
both neighborhood quality, and a privacy metric. Neighbor-
hood quality evaluates how much the neighborhoods provided
by H&S resemble the optimal neighborhoods, that is those ob-
tained with the standard cosine similarity metric. Speciﬁcally,
for each user we measure the average of the cosine similarities
with all the peers in its H&S view, and we normalize it by
the average cosine similarity with the peers in the optimal
neighborhood obtained using an exhaustive search procedure.
Let u be a user with full proﬁle, prof ileu, and let nu and
Nu be respectively u’s H&S neighborhood and u’s optimal
neighborhood. Then we compute u’s neighborhood quality as
follows.
(cid:2)
(cid:2)
quality(u) =
1
k
1
k
p∈nu cos(prof ileu, prof ilep)
p∈Nu cos(prof ileu, prof ilep)
Neighborhood quality provides a ﬁrst indication of privacy:
lower quality implying better privacy. To obtain a more precise
privacy evaluation, we also deﬁne set score. This metric
measures the success rate of the adversary in the context of a
proﬁle reconstruction attack. Let G be the set of items that the
1MovieLens
movielens/
datasets
are
available
at:
http://grouplens.org/datasets/
2Jester datasets are available at: http://eigentaste.berkeley.edu/dataset/
 1
 0.8
 0.6
 0.4
 0.2
l
l
a
c
e
R
HandS/ML100k
HandS/ML1m
cleartext/ML100k
cleartext/ML1m
random/ML100k
random/ML1m
 0
 0.84  0.86  0.88
 0.9
 0.92  0.94  0.96
Precision
l
l
a
c
e
R
 1
 0.8
 0.6
 0.4
 0.2
 0
 0.1