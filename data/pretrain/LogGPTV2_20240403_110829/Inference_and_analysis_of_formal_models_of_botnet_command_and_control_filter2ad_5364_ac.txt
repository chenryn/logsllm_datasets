### 4. Inference of Protocol Models

This section details our technique for inferring complete protocol state-machines in a realistic network setting. The high-level architecture of our implementation is illustrated in Figure 3, which outlines the components of our inference system: a bot emulator, a query cache, a membership query predictor, and L*.

#### 4.1 Bot Emulator

The bot emulator is a custom script that receives queries (strings of symbols from the input alphabet) from L* and translates them into valid protocol messages to be sent to botnet servers. Upon receiving a response, the emulator converts the response back into strings of symbols from the output alphabet and sends these abstracted strings to L*. The processes of abstraction and concretization are described in Section 4.1.

We developed the bot emulator from scratch to ensure it cannot perform any malicious activities (such as spamming or infection) associated with real bots. Additionally, we designed our experiments to avoid causing harm to any involved parties, including infected users, ISPs, C&C servers, and botmasters. For instance, the bot emulator is programmed to avoid intentionally constructing corrupted messages, thereby minimizing its impact on C&C servers.

#### 4.2 Query Cache

The query cache serves as a central repository for parallel query responses, storing pairs of input message sequences and their corresponding responses. This caching mechanism ensures that each sequence of messages is sent over the network only once. By parallelizing the process across eight machines, each running one bot emulator, we achieved a 4.85X reduction in the time required for inference. The details of parallelization and caching are provided in Section 4.2.

#### 4.3 Membership Query Predictor

The membership query predictor aims to predict the most likely response to membership queries. Learning even a medium-sized state-machine can require a significant number of queries (as discussed in Section 3.1). Given that queries can be long strings of input messages and responses can take a long time due to network delays (an average of 6.8 seconds in our experiments), accurate prediction is crucial for efficient inference. Erroneous predictions are detected using sampling queries and corrected by backtracking to the first mistake made by the predictor. Our prediction heuristic is presented in Section 4.3.

#### 4.4 Handling Non-Determinism

In Section 4.4, we address the only discovered source of non-determinism in the MegaD protocol: state-machine resetting and the generation of sampling queries.

### 4.1 Message Abstraction and Concretization

L* constructs queries over the abstract input alphabet and passes them to the bot emulator, which translates these symbols into valid network messages and sends them to botnet servers. When responses are received, the emulator abstracts the response messages into the output alphabet and returns them to L*. The construction of the input and output alphabets involves both automatic and manual processes. We use automatic protocol reverse engineering [6] and encryption/decryption modules extracted from the bot binary [5] to reverse-engineer message formats and their semantic content. Once the message formats are learned, we manually perform the abstraction, focusing on important fields such as the message type field.

Concretization, another critical aspect, involves generating valid network messages. Invalid messages or those with invalid session tokens will be rejected by the server. To generate valid messages, the bot emulator uses the automatically reverse-engineered message format grammar, rewrites necessary message fields, and encrypts the messages before transmission. The emulator also rewrites session tokens using tokens issued by the server in the same session. If no token has been issued, the emulator uses a random value to test how servers handle invalid tokens.

To ensure reproducibility, we provide MegaD’s C&C message format tree in Figure 4 and a list of all abstracted messages used in this paper in Table 1. MegaD employs a proprietary C&C protocol for communication with its master and template servers and a non-standard SMTP protocol for communication with its SMTP server. For modeling C&C protocol messages, we use three fields: MsgType, SubType, and Config. Each unique combination of these fields is assigned a unique symbol, as detailed in Table 1. For the SMTP protocol, we abstract MegaD’s SMTP dialogs at two different levels, depending on the level of detail required.

### 4.2 Query Cache

The query cache in our implementation is a file that stores pairs of input message sequences and their corresponding responses. It primarily acts as a concentrator for parallel query responses, simplifying the implementation of L* by allowing queries to be issued in parallel while responses are processed sequentially. The membership query loop and the column extension loop in Algorithms 1 and 2, respectively, execute independent membership queries that can be predicted or run on the network in parallel. Our implementation of L* emits these queries in parallel, partitioning them among multiple machines (eight in our case). The cache also stores responses tested on the network, allowing for reuse due to the determinism assumption (Section 2.3).

### 4.3 Response Prediction

Studying the membership queries made by L* during protocol model learning, we observed significant redundancy in the inference process, particularly due to self-loops in the state-machine. These self-loops increase the number of membership queries without aiding in distinguishing states. Two major factors contribute to this redundancy: (1) the need to determine the response to all input alphabet symbols from every state, and (2) the conservative overestimation of the input and output alphabets before the state-machine is known.

To address this, we developed a two-level heuristic for response prediction. Consider the example in Figure 2a, where there are three self-loops. Since self-loops return to the same state, the response to 2 · 1 · u (with self-loop) and 2 · u (without self-loop) will be the same for any input string u. Thus, the table entries for 2 · 1 · u and 2 · u in Table 2b are identical. As the length of membership queries increases with each iteration, the response to 2 · u can be used to predict the response to 2 · 1 · u. We exploit this redundancy with a two-level heuristic, which is further detailed in the following sections.