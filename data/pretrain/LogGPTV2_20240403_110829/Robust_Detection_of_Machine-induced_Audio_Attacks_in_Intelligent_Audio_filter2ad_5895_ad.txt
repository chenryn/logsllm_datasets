LEARNING
The learning-based predictive modeling approaches heavily rely
on the training data to make predictions and its performance is
likely to degrade if the provided training samples are not an accu-
rate reflection of the underlying distribution of actual data. This
poses a challenge for audio attack detection since the model will
inevitably face new acoustic environments that are unrepresented
in the training data when deployed in practice.
To address this problem, we take inspiration from the recent
success in domain adaptation techniques in the computer vision
domain [20, 47, 67] and adopt an unsupervised domain adaptation
scheme for achieving domain-invariant representation learning. In
the context of audio attack detection, the term “domain” refers to
different acoustic environments and the domain adaptation process
aims to help the model generalize from the training-time environ-
ment (i.e., the source domain) to the test-time environment (i.e.,
the target domain). Specifically, let 𝑤 𝑓 and 𝑤𝑐 denote the param-
eters of the feature extractor and the classifier, respectively. As is
mentioned in Section 4, the network is trained on the classification
loss 𝐿𝑐(𝑤 𝑓 , 𝑤𝑐) to recognize genuine/attack audio. To help the fea-
ture extractor learn domain-invariant features, we introduce a new
domain discriminator with parameters 𝑤𝑑 during training. The
domain discriminator shares the same architecture as the classifier
but the objective of the discriminator is to distinguish between the
source domain training samples and the target domain training
samples by minimizing the domain classification loss 𝐿𝑑(𝑤 𝑓 , 𝑤𝑑).
The objective of the domain-invariant training process is to search
for the parameter set 𝑤 𝑓 to minimize the audio classification loss
𝐿𝑐 and simultaneously maximize the domain classification loss 𝐿𝑑,
which can be achieved by minimizing the following integrated loss
function:
𝐿(𝑤 𝑓 , 𝑤𝑐, 𝑤𝑑) = 𝐿𝑐(𝑤 𝑓 , 𝑤𝑐) − 𝜆 · 𝐿𝑑(𝑤 𝑓 , 𝑤𝑑),
where 𝜆 is a weighting factor to control the impact of the domain
discriminator on the learned feature mapping during training. This
can be achieved by inserting a gradient reversal layer [20] into
the network which implements the identity function during for-
ward propagation and reverses the gradient by multiplying it by a
negative scalar (i.e., 𝜆) during the backpropagation process. After
training, the feature extractor will learn to extract features that
are both discriminative for detecting various audio attacks and
invariant to the change of acoustic environments.
6 ATTACK IMPLEMENTATION
In order to evaluate our designed machine-induced audio attack
detection approach, we reproduced a set of representative audio
attacks. In addition to conventional replay attack, for which we
used a recently-published dataset (Section 7), we generated a set
of adversarial speech samples through the following procedures
and conducted extensive real-world experiments in various envi-
ronmental conditions.
Modulated Replayed Attack. Due to the security concerns
brought by replay attacks, a number of defense approaches have
been developed to detect replayed audio signals, by examining
unique acoustic distortions (e.g., energy distribution in the fre-
quency domain) induced by the playback device. To bypass such
defenses, a recent study [74] designed a new type of replayed at-
tack, namely modulated replayed attack, which can compensate for
the acoustic distortions through profiling the frequency response
of the playback device. Specifically, in our implementation, the
frequency response is measured with 68 single-frequency testing
signals across 0 ∼ 4000𝐻𝑧. We play the testing signals on three
playback devices (i.e., Huawei Nova 4, iPhone 12 Pro Max, and HP
Elitebook 1050 G1 laptop) and record the replayed audio signals
with a microphone (i.e., ReSpeaker Core v2.0). We then use the
played testing signals and the recording to generate an inverse
Multi-channel AudioSTFTMagnitudePhaseStacked SpectrogramFeature Extractor…FeatureVectorClassifierDiscriminatorGradient ReversalGenuine/AttackAudioClassification (Lc) Domain Classification (Ld) PreprocessingSession 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1891(a) Device 1: Google AIY
(b) Device 2: ReSpeaker 4-Mic
Linear Array
(c) Device 3: ReSpeaker
Core V2
(d) Device 4: Amlogic A113X1
Figure 8: Microphone arrays used for data collection.
filter [74] to compensate for the acoustic distortions for each play-
back device. Finally, we record the 10 original voice commands
shown in Table 9(a) spoken by a volunteer and pass the recordings
through the corresponding inverse filter to generate modulated
speech samples for each playback device.
Synthesis Attack. Synthesis attacks usually rely on a speech
synthesis model to produce attacking audio mimicking the voice
characteristics of the victim. The current synthesis models based on
deep learning can simulate natural sounding voices similar to hu-
man subjects. To evaluate synthesized speech, we use two state-of-
the-art speech synthesis models, including Google Text-to-Speech
based on WaveNet [1, 69] and Tacotron 2 based on WaveGlow [46].
Both WaveNet and WaveGlow are CNN-based audio generative
models exploiting temporal dependencies for speech signal genera-
tion. For the Google Text-to-Speech, we directly use a pre-trained
WaveGlow model of a male speaker provided by the API, while for
Tacotron 2, we train a WaveGlow-based speech synthesis model by
using 13,100 voice samples from a female speaker (i.e., LJ Speech
Dataset [3]). We use the two models to separately generate the 10
original voice commands listed in Table 9(a).
Hidden Voice Command. Hidden voice commands [13, 68] are
obfuscated voice commands that are unintelligible to human beings
but can be interpreted by intelligent audio systems. Such attacks
exploit the perception difference between humans and machines
(e.g., speech recognition models) in processing speech and modu-
late the recorded voice samples into attacking audio. To generate
hidden voice commands, the attack will first extract voice features
from normal commands and then train a network for reconstruct-
ing voice with these features and meanwhile continuously update
parameters of the network and feature extraction to make it unin-
telligible to humans. The attack can be either black-box (through
inverting MFCC features) or white-box (through applying gradient
descent-based approach on a target speech recognition model). A
recent study even proposed a more practical hidden voice com-
mands [7] aiming to spoof the feature extraction process of speech
recognition models, rendering the attack black-box and effective.
To evaluate our system, we use 14 publicly released hidden voice
commands, including 10 regular hidden voice commands [2] and 4
practical hidden voice commands [4].
Table 3: Description of the collected audio attack datasets.
Type
of Audio
HVC
Synthesis
Inaudible
Adversarial
Modulated
Genuine
Environment
Room 1, 2, 3
Room 1, 2, 3
Room 1, 2, 3
Room 1, 2, 3
Room 1, 2
Room 1, 2
Distance (cm)
30, 50, 100, 200, 300
30, 50, 100, 200, 300
10, 30
30, 50, 100
30, 50, 100
50, 100, 200, 300
# Samples
of Device 1
1,812
2,397
520
503
510
1,324
# Samples
of Device 2
1,680
2,531
520
503
510
1,147
# Samples
of Device 3
1,478
2,531
520
503
510
1,240
# Total
Samples
4,970
7,459
1,560
1,509
1,530
3,711
Inaudible Attack. The adversary can launch inaudible attacks
by modulating the voice commands into ultrasound frequency
bands [41, 85] (e.g., over 20𝑘𝐻𝑧). Although ultrasound signals can-
not be perceived by the human ear, they can be demodulated by
the microphones in audio intelligent devices due to their inherent
non-linearity. To implement inaudible attacks, we first use Google
Text-to-Speech API to generate the 10 original voice commands
listed in Table 9(a). We then use amplitude modulation to modu-
late the voice commands onto a baseband signal of 35𝑘𝐻𝑧, where
the modulated sound is completely inaudible and can be demod-
ulated by microphones. The modulated signals are generated on
a Keysight 33500B signal generator and played by an ultrasonic
speaker (Avisoft Bioacoustics Vifa [5]).
Audio Adversarial Example. The current intelligible audio
systems mainly rely on deep neural networks to perform speech
recognition, which are inherently vulnerable to well-craft and im-
perceptible adversarial perturbations [14, 35, 84]. The adversary
can inject the adversarial perturbations into the audio signals to
spoof the deep learning models. We implement the gradient-based
perturbation generation presented in the previous study on audio
adversarial examples [14], which targets to spoof an end-to-end
speech recognition (i.e., DeepSpeech [25]). To implement the at-
tack, we first generate 10 original voice commands (i.e., original
commands listed in Appendix Table 9(a)) using Google text-to-
speech API and then compute the adversarial perturbations to fool
DeepSpeech with the corresponding target commands (i.e., target
commands listed in Appendix Table 9(b)). The perturbations are
then added to the original voice commands for the attack.
7 PERFORMANCE EVALUATION
7.1 Experimental Methodology
To evaluate our system under the replay attack, we use a public
dataset collected using 4 different microphone arrays. For more
advanced audio attacks (e.g., hidden voice commands, inaudible
attacks), we use 3 representative microphones arrays shared in the
public dataset to recorded the attacking sound and genuine human
speech for evaluation.
7.1.1 Public Replay Attack Dataset. To evaluate our system under
the replay attack, we use a public dataset, ReMASC [22], which
is collected using a set of 4 microphone arrays with 2 ∼ 7 audio
channels. We partition the dataset into the core training set and
the evaluation set as described in ReMASC. The training set and
the evaluation set are disjoint and contain 26, 946 and 17, 581 audio
samples, respectively.
• Devices. To mimic multi-channel recording in commercial in-
telligent audio devices, ReMASC uses 4 microphone arrays with
different number of audio channels as shown in Figure 8. These
microphone arrays include: 1) Google AIY Voice Kit (2 channels);
2) ReSpeaker 4-mic linear array (4 channels); 3) ReSpeaker Core
V2 (6 channels) 4) Amlogic A113X1 (7 channels). To generate
Session 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1892(a) Playback devices
(b) Room 1
(a) Setup for inaudible attack
(b) Setup for the remaining audio attacks
Figure 10: Experimental setup of audio attacks.
Table 4: Results for replay attack detection in environment-
dependent settings.
(c) Room 2
(d) Room 3
Figure 9: Recording devices and environments of the self-
collected dataset.
the attacking sound, ReMASC uses 3 different playback devices,
including a Sony SRSX5, a Sony SRSX11, an Audio Technica
ATH-AD700X headphone, and an iPod Touch.
• Environments. The dataset contains a total number of 9, 240
genuine speech samples and 45, 472 replayed recordings collected
in 4 different acoustic environments: 1) Outdoor (Env-A): an out-
door student plaza with various background noises such as chat-
ting, traffic, and wind; 2) Indoor #1 (Env-B): a quiet study room; 3)
Indoor #2 (Env-C): a lounge with music players and TVs ruining
in the background; and 4) In-vehicle (Env-D): inside a moving
vehicle (Dodge Grand Caravan) in different areas (e.g., campus,
residential area, urban area, and highway) with speeds ranging
from 3 to 40 miles per hour. The samples are recorded at varying
distances (0.5 − 6m) and angles (0 − 90 degrees) according to
each environment. The data volume and the involved number of
speakers for each environment are shown in Appendix Table 10.
Self-collected Audio Attack Dataset. Besides conventional
7.1.2
replay attacks, we also collect data samples of other 5 more ad-
vanced audio attacks using multiple microphone arrays in different
environments, following the implementation described in Section 6.
Table 3 shows the detail of the self-collected dataset.
• Devices. The data are collected using three microphone arrays,
i.e., Google AIY voice kit, ReSpeaker 4-Mic linear array, and
ReSpeaker core V2, which are shown in Figure 8. For the inaudi-
ble attack, we use an ultrasound speaker (i.e., Vifa Ultrasonic
Dynamic Speaker), while for other audio attacks, we use 2 smart-
phones (i.e., Huawei Nova 4 and iPhone 12 pro max) and a laptop
(i.e., HP EliteBook 1050G1) as the playback device, as shown in
Figure 9(a).
• Environments. The attack audio and genuine speech samples
are collected in 3 different room environments as shown in Figure
9(b)-(d), including two living rooms and a bedroom.
• Genuine Speech and Attack Setup. We recruit 6 participants
(i.e., 4 males and 2 females) aging from 22 and 30 to collect the
genuine speech samples. The attacking audio and genuine speech
are mostly recorded at 3 different distances between the partic-
ipant/loudspeaker and the microphone arrays, i.e., 30𝑐𝑚, 50𝑐𝑚,
100𝑐𝑚, except for the inaudible attack, which is only recorded
EER(%)/RA(%)
Gong et al. [23]
CQT-LCNN [54]
LFCC-LCNN [62]
RawNet2 [59]
Ours (Type I)
Ours (Type II)
Device 1 Device 2 Device 3 Device 4
14.9/-
19.8/-
21.5/81.0
15.0/90.3
27.3/78.6
14.8/89.8
24.9/74.2
10.8/89.5
15.7/85.4
6.6/96.0
10.3/94.2
18.2/90.3
15.4/-
23.4/76.3
24.2/90.0
15.9/89.3
11.0/92.8
14.6/91.8
16.5/-
26.9/77.9
23.5/81.4
18.2/82.9
9.2/93.5
12.3/92.6
at 10𝑐𝑚 and 30𝑐𝑚 due to its short effective range [85]. More-
over, for genuine speech and attacks that are less sensitive to the
recording distance (i.e., the hidden voice command and synthesis