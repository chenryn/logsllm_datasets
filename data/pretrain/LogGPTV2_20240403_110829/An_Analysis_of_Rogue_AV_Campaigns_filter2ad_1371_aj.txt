worm with our GLOWS [26] simulator. We vary the following parameters as we
evaluate each worm detector: the environment it is run in (meaning the network
conﬁguration and legitimate traﬃc), the worm scanning method, and the worm
scanning rate. We have also studied the eﬀects of two additional parameters: the
target port attacked by the worm and the activity proﬁle of the ﬁrst host infected
by the worm, but omit those results from this paper due to space constraints.
5.1 Evaluation Environment and Background Traﬃc
Worm detectors must be evaluated in the context of a subnet to be protected
and against the legitimate background traﬃc that occurs in that subnet. For
our experiments, we deﬁne an environment as the network address space to be
monitored, the IP addresses of the active hosts inside that address space, and the
IP network traﬃc into and out of that address space during two time periods. We
use the ﬁrst time period for training and the second to run experiments against.
To make the environments comparable to each other and to enable us to ensure
that they do not contain worm traﬃc, we select a /22 subnet from the original
recorded traces to use as the protected address space in our environment. Every
environment is thus a /22 network with between 100 and 200 active hosts. We
use four distinct environments in our evaluation.
The enterprise environment is built from a trace collected at LBNL [27] in
January of 2005. Heavy scanners were removed from the trace before it was
released. It has 139 active hosts and the training and experiment segments each
contain roughly 25,000 connections.
The campus environment is built from a trace that was collected in 2001
at the border of Auckland University [28]. The trace was anonymized using a
non-preﬁx preserving anonymization scheme, so we cannot entirely accurately
reconstruct the internal structure of their network. Instead, we randomly select
200 hosts and construct an environment using traﬃc to and from those hosts.
Each segment of the trace in our campus environment contains approximately
25,000 connections.
Behavior-Based Worm Detectors Compared
45
The wireless and department environments are built from traces collected at
the University of Massachusetts in 2006 [29]. The department environment is
built from a trace capturing all traﬃc to and from the wired computers in the
CS department. It has 92 active hosts and approximately 30,000 connections in
each segment. The wireless environment comes from a trace capturing all wireless
network traﬃc from the university. It has 313 active hosts and approximately
120,000 connections in each segment.
5.2 Worm Parameters
Several key parameters of a worm may impact the eﬀectiveness of worm detectors.
We look at three scanning strategies worms can employ: random scan,
local-preference scan, and topologically aware (topo) scan, and evaluate them at
a variety of scanning rates. Our GLOWS simulator takes an environment as input
and simulates a worm as if it were attacking the network deﬁned by that environ-
ment. The simulation starts with a single inbound worm connection that infects
one host in the protected network. We run the simulator once for each permutation
of worm parameters. The scanning mechanisms are deﬁned as follows.
A random scanning worm simply chooses target addresses at random from
the entire IPv4 address space. This typically results in many connection at-
tempts to addresses with no host present or with a host that is not running
the requested service, resulting in many connection failures. Permutation and
sequential scanning worms should show very similar characteristics and are not
evaluated separately here.
A local-preference worm scans local addresses (in the same preﬁx) more fre-
quently than addresses in the full address space. This results in more scans that
do not cross the network border (and are therefore not visible to a border-located
detection mechanism). Existing local-preference scanning worms, such as Code-
Red II [2], target the local /16 preﬁx approximately 50% of the time, the local
/8 25% of the time, and the entire network the remaining time. As all our traces
are about a /22 network, such a worm would largely resemble a random scanning
worm. Instead, our local-preference worm scans the local /22 50% of the time,
the local /8 25% of the time, and the entire network the remaining time.
The topologically aware (topo) worm ﬁnds target information on the host that
it infects. This target information allows it to scan eﬀectively because it already
knows about other hosts that are running the service it targets. The number of
new hosts (referred to as “neighbors”) the worm discovers is dependent on its
neighbor detection algorithm. We use three implementations of the topo worm
with diﬀering neighbor counts. The topo100 worm starts with 100 neighbors, the
topo1000 worm starts with 1000 neighbors, and the topoall worm starts with an
unlimited supply of neighbors. After scanning its known neighbors, the topo
worm must either stop scanning or switch algorithms. In our implementation
it reverts to random scanning after exhausting its neighbor list. Note that the
neighbors discovered by the topo worm are randomly located, so could appear
both inside and outside the protected network. Also, they will be running the
target service but are not guaranteed to be vulnerable.
46
S. Staﬀord and J. Li
In addition to scanning mechanism the worm uses, the rate at which it initiates
connections is important. The faster a worm scans, the more visible it is to worm
detectors. We run experiments for a variety of worm scanning rates ranging from
10 connections per second down to one connection every 200 seconds.
5.3 Experiment Procedure
Measuring detector performance is a multi-step procedure. For each environ-
ment, every detector must (1) establish thresholds via training, (2) be evaluated
against the legitimate traﬃc in the environment to measure false positives, (3)
adjust their parameters to ﬁx false positives at a speciﬁc level, and (4) be eval-
uated against legitimate traﬃc combined with worm traﬃc to measure false
negatives and detection latency. Let us now discuss each of these steps in more
detail.
These detectors are anomaly detectors, and they look for traﬃc that diverges
from normal. To do this, they must ﬁrst measure what normal is. The TRW,
MRW, DSC, and PGD detectors are run against the training segment of the trace
using the training method outlined in their publication to perform this operation.
The RBS and TRWRBS detectors perform on-the-ﬂy training as they are run
against the experiment segment of the trace.
After the thresholds are established from the training segment of the trace,
each detector is run against the experiment portion of the trace to measure false
positives. We measure F+ using the thresholds obtained from training and the
default detector parameters outlined in the original publication of each work,
presenting those results in Section 6.1.
Note that each detector can be tuned to favor producing either more F+ or
more F-. After reporting F+ using the default detector parameters as published,
in order to provide a fair comparison of the false negative rate of the detectors, we
modify each detector’s parameters such that they all produce the same number
of false positives in each environment. We chose to peg each detector at a rate
of two false positive alarms during the experiment period. Two false positives
is a high rate for the one-hour time period evaluated, but was chosen as an
achievable value for all detectors requiring the minimum amount of parameter
modiﬁcations.
After measuring F+ and adjusting the detectors to match their F+ levels, we
then measure the performance of the detectors against worm traﬃc. For each
detector in each environment, we run 16 experiments for every permutation of
the worm parameters. A single experiment consists of running the detector for
10 minutes of the experiment trace to warm up the connection histories, then
injecting the simulated worm traﬃc into the trace, and running until either an
hour has elapsed or the worm is detected. Each of the 16 experiments that we
run for a given set of worm parameters has a diﬀerent host in the protected
network being infected ﬁrst and uses a diﬀerent random seed. The percentage of
experiments where the worm is not detected is the false negative rate, and the
mean number of worm connections that have left the network at detection time
is the detection latency.
Behavior-Based Worm Detectors Compared
47
)
t
s
o
h
r
e
p
1
t
i
m
l
i
l
(
s
m
r
a
A
e
s
a
F
l
 5
 4
 3
 2
 1
 0
enterprise
campus
department
wireless
trw
rbs
trwrbs mrw
Detector
(a) By Host
pgd
dsc
l
m
r
a
A
e
s
a
F
h
l
t
i
w
s
e
t
u
n
M
i
f
o
%
 100
 80
 60
 40
 20
 0
trw
enterprise
campus
department
wireless
rbs
trwrbs mrw pgd
Detector
(b) By Time
dsc
Fig. 1. False positives against legitimate traﬃc: when running with default pa-
rameters against the experiment segment of the traces with no worm traﬃc injected
6 Results
We now measure the performance of the worm detectors in a variety of worm
scenarios. We ﬁrst look at the false positives, then introduce worm traﬃc to mea-
sure false negative rates and detection latency. We start with the simplest worm
strategy of randomly scanning addresses, then increase the worm sophistication
to local-preference and then topologically aware scanning strategies.
6.1 False Positives against Legitimate Traﬃc
Figure 1(a) shows the results for each detector using default parameters from
its original publication. Raising an alarm for a host could either (a) indicate
that the host is considered permanently infected, or (b) indicate that the host is
behaving anomalously now (for some deﬁnition of now). Figure 1(a) shows F+
results using strategy (a) (with PGD limited to one alarm per 1-minute window
because it does not identify the infected host). Figure 1(b) shows F+ results
using strategy (b) and with an alarm duration of one minute. Strategy (a) is
probably more representative of how detectors would be deployed in practice,
but it is illustrative to show that without such a limitation, in some environments
RBS and TRWRBS would be in an alarm state more than 50% of the time and
TRW and MRW would be in an alarm state 100% of the time.
These results also demonstrate the impact that environment has on the detec-
tor performance. TRWRBS has ﬁve F+ in the wireless environment but none in the
campus or department environments. MRW is in an alarm state 100% of the time
in the department environment but not at all in the campus environment. An eval-
uation using only a single environment could produce grossly inaccurate results.
The wireless environment showed the most F+ activity with the default pa-
rameter choices. This appears to stem from several hosts playing network games
such as Counter-Strike (UDP connections on ports in the 27010-27050 range)
and NeverWinter Nights (TCP connections on port 5121) as well as from hosts
using BitTorrent (33 hosts active on ports in the 6881-6999 range). This environ-
ment represents the most residential/recreational usage patterns and indicates
48
S. Staﬀord and J. Li
that this sort of traﬃc is less amenable to behavior-based worm detection than
the less variable traﬃc of the enterprise environment. This represents the ﬁrst
ﬁndings we are aware of that validate a common hypothesis: current behavior-
based anomaly detectors are not optimized for residential style network traﬃc
and may not show satisfactory performance in such an environment.
6.2 Detector Performance against Random Worm
In this section we report false negative and latency results against random scan-
ning worms. Figure 2 shows that TRW is the most consistently eﬀective detector
across the environments, discovering all instances of the worm down to 0.05 scans
per second and catching the majority of the slower scans in the enterprise and
campus environments. RBS is the least eﬀective, only able to consistently detect
the worm scan rates greater than ﬁve scans per second. TRWRBS blends the
two detectors with results right in the middle. The DSC and PGD detectors are
an order of magnitude more eﬀective in the enterprise environment than in the
other environments due to the lower activity levels (and hence lower thresholds)
in the enterprise environment. The MRW detector provides middle of the road
performance except against in the wireless environment where it is unable to
detect the worm at speeds slower than ﬁve scans per second.
Figure 3 shows the average number of connections each infected network was
able to make before detection. Note that the scale is not consistent across the
graphs. We only show the value for those scenarios where F- is zero in order to
eliminate selection bias in the results. DSC is consistently the fastest detection
)
s
n
u
r
f
o
%
(
-
F
 100
 80
 60
 40
 20
 0
)
s
n
u
r
f
o
%
(
-
F
 100
 80
 60
 40
 20
 0
enterprise
campus
department
wireless
0
.
0
0
5
0
.
0
1
0
.
0
2
0
.
0
5
0
.
1
0
.
2
0
.
5
1
2
5
1
0
Worm Scans per Second (log)
(a) TRW
enterprise
campus
department
wireless