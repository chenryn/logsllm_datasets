from the per-packet feature sequences. This module periodically
polls the required information from the high speed packet parser
module with a fixed time interval. After acquiring the extracted
per-packet features, it encodes the per-packet feature sequences
as vectors and extracts the sequential information via frequency
domain. These features with low redundancy are provided for the
statistical clustering module. However, it is difficult to extract the
frequency domain features of traffic in high throughput networks in
realtime because of the various complicated, irregular, and dynamic
flow patterns [14, 63]. We cannot apply deep learning models, e.g.,
recurrent neural networks, to extract features due to their long
processing latency though they can extract more richer features for
detection. We will present the details of this module in Section 4.1.
Automatic Parameter Selection Module. This module calcu-
lates the encoding vector for the feature extraction module. We
decide the encoding vector by solving a constrained optimization
problem that reduces the mutual interference of different per-packet
features. In the training phase, this module acquires the per-packet
feature sequences and solves an equivalent Satisfiability Modulo
Theories (SMT) problem to approximate the optimal solution of
the original problem. By enabling automatic parameter selection,
Frequency Domain Feature Extraction ModuleStatistical Clustering Module13EncodingVectorAutomatic Parameter Selection ModuleHigh Speed Packet Parser Module2Training OnlyDetectionPackets……Per-packetFeature SequencesA. Packet Feature Encoding…B. Fourier Trans. on FramesReducingFeature ComplexityReducing the Scale of Features C. Log. Trans. on Modulus LAddressingNumeric Instability⊗=NormalAbnormalClustering the Frequency Domain FeaturesFrequency DomainFeaturesSession 12C: Traffic Analysis and Side ChannelsCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3433we significantly reduce the manual efforts for parameter selection.
Therefore, we can fix and accurately set the encoding vector in
the detection phase. We will describe the details of the module in
Section 4.2.
Statistical Clustering Module. In this module, we utilize a light-
weight statistical clustering algorithm to learn the patterns of the
frequency domain features from the feature extraction module. In
the training phase, this module calculates the clustering centers of
the frequency domain features of benign traffic and the averaged
training loss. In the detection phase, this module calculates the dis-
tances between the frequency domain features and the clustering
centers. Whisper detects traffic as malicious if the distances are
significantly larger than the training loss. We will elaborate on the
statistical clustering based detection in Section 4.3.
4 DESIGN DETAILS
In this section, we present the design details of Whisper, i.e., the
design of three main modules in Whisper.
4.1 Frequency Feature Extraction Module
In this module, we extract the frequency domain features from high
speed traffic. We acquire the per-packet features of 𝑁 packets from
the same flow by polling the high speed packet parser module. We
use the mathematical representation similar to Bartos et al. [4] to
denote the features. We use 𝑠(𝑖) and 𝑀 to indicate the 𝑖𝑡ℎ per-packet
feature and the number of per-packet features, respectively. Matrix
S denotes the per-packet features of all packets, where s𝑖𝑘 is defined
as 𝑖𝑡ℎ packet’s 𝑘𝑡ℎ property:
S = [𝑠(1), . . . , 𝑠(𝑖), . . . , 𝑠(𝑀)] =
(1)
𝑠11
...
𝑠𝑁 1
 .
· · ·
. . .
· · ·
𝑠1𝑀
...
𝑠𝑁 𝑀
𝑀
𝑘=1
Packet Feature Encoding. We perform a linear transformation
𝑤 on S to encode the features of a packet to a real number 𝑣𝑖. 𝑣
denotes the vector representation of traffic:
𝑣 = S𝑤 = [𝑣1, . . . , 𝑣𝑖, . . . , 𝑣𝑁]T,
𝑣𝑖 =
𝑠𝑖𝑘𝑤𝑘 .
(2)
The feature encoding reduces the scale of features, which signifi-
cantly reduces the processing overhead of Whisper. In Section 4.2,
we will describe how Whisper automatically selects parameters for
the encoding vector 𝑤.
Vector Framing. Now we segment the vector representation with
the step length of 𝑊seg. The goal of segmentation is to reduce the
complexity of the frequency domain features by constraining the
long-term dependence between packets. If the frames are exces-
sively long, the frequency domain features will become too complex
to learn in the statistical learning module. 𝑁𝑓 denotes the number
of the frames. We obtain the following equations:
𝑓𝑖 = 𝑣[[(𝑖 − 1) × 𝑊seg : 𝑖 × 𝑊seg]]
(cid:22) 𝑁
(cid:23)
.
𝑁𝑓 =
𝑊seg
(1 ≤ 𝑖 ≤ 𝑁𝑓 ),
(3)
(4)
Discrete Fourier Transformation. In the next step, we perform
the Discrete Fourier Transformation (DFT) on each frame 𝑓𝑖 to
extract the sequential information via frequency domain and reduce
the information loss incurred by the flow-level methods. We can
acquire the frequency features of each frame as follows:1
𝐹𝑖 = F (𝑓𝑖)
(1 ≤ 𝑖 ≤ 𝑁𝑓 ),
𝐹𝑖𝑘 =
−𝑗 2𝜋 (𝑛−1) (𝑘−1)
𝑊seg
𝑓𝑖𝑛𝑒
(1 ≤ 𝑘 ≤ 𝑊seg),
𝑊seg
𝑛=1
(5)
(6)
where 𝐹𝑖𝑘 is a frequency component of 𝑖𝑡ℎ frame with the frequency
of 2𝜋(𝑘 − 1)/𝑊seg. Note that, all frequency features output by DFT
are vectors with complex numbers, which cannot be used directly
as the input for machine learning algorithms.
Calculating the Modulus of Complex Numbers. We transform
the complex numbers to real numbers by calculating the modu-
lus for the frequency domain representation. For simplicity, we
transform 𝐹𝑖𝑘 to a coordinate plane representation:
𝐹𝑖𝑘 = 𝑎𝑖𝑘 + 𝑗𝑏𝑖𝑘,
𝑎𝑖𝑘 =
𝑏𝑖𝑘 =
𝑓𝑖𝑛 cos 2𝜋 (𝑛−1)(𝑘−1)
−𝑓𝑖𝑛 sin 2𝜋 (𝑛−1)(𝑘−1)
𝑊seg
𝑊seg
𝑊seg
𝑊seg
𝑛=1
𝑛=1
(7)
(8)
.
We calculate the modulus for 𝐹𝑖𝑘 as 𝑝𝑖𝑘 in (9). For the 𝑖𝑡ℎ frame,
we select the first half of the modulus as vector 𝑃𝑖. Because the
transformation results of DFT are conjugate, the first half and the
second half are symmetrical. Thus, we can obtain:
(1 ≤ 𝑘 ≤ 𝑊seg),
(𝐾𝑓 =
(cid:22)𝑊seg
(10)
(cid:23)
(9)
𝑖𝑘
𝑝𝑖𝑘 = 𝑎2
𝑖𝑘 + 𝑏2
𝑃𝑖 = [𝑝𝑖1, . . . , 𝑝𝑖𝐾𝑓 ]T
𝐹𝑖𝑘 = 𝐹∗
+ 1),
𝑖(𝑊seg−𝑘) ⇒ 𝑝𝑖𝑘 = 𝑝𝑖(𝑊seg−𝑘) .
2
(11)
Logarithmic Transformation. To make the frequency domain
features to be numerically stable [23] and prevent float point over-
flow during the machine learning model training, we perform a
logarithmic transformation on 𝑃𝑖, and use constant 𝐶 to adjust the
range of the frequency domain features:
ln(𝑃𝑖 + 1)
𝐶
(1 ≤ 𝑖 ≤ 𝑁𝑓 ),
𝑅𝑖 =
R𝐾𝑓 ×𝑁𝑓 = [𝑅1, . . . , 𝑅𝑖, . . . , 𝑅𝑁𝑓 ].
(12)
(13)
As the output of the features extraction module, the 𝑖𝑡ℎ column
component of R is the frequency domain features of the 𝑖𝑡ℎ frame.
Matrix R is the input for the statistical clustering module.
Take an example, we collect three types of benign traffic (90%)
mixed with the malicious traffic (10%) in Wide Area Network (WAN).
We select 1500 continuous packets (𝑁 = 1500) from each type of
traffic and extract three per-packet features (𝑀 = 3) including the
packet length, the protocol type, and the arriving time interval.
We fix the framing length 𝑊seg = 30. Therefore, 𝑁𝑓 = 50 and
𝐾𝑓 = 16. Then we perform a min-max normalization operation on
the frequency domain features R and map the results to the RGB
space. We visualize the frequency domain features that are similar
to the Spectrogram in speech recognition [1]. As shown in Figure 2,
1 𝑗 denotes an imaginary number.
Session 12C: Traffic Analysis and Side ChannelsCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3434(a) Benign TLS traffic and side-channel attack traffic
(b) Benign UDP traffic and SSL DoS traffic
(c) Outbound NAT traffic and LowRate TCP DoS traffic
Figure 2: We map the frequency domain features, which are extracted from the traffic with three types of typical attacks, to the
RGB space, and observe that a small number of malicious packets incur significant changes in the frequency domain features.
of (17). For the 𝑖𝑡ℎ per-packet feature, we perform a min-max nor-
malization on 𝑠𝑖 and use 𝑛𝑖 to indicate the normalized vector. We
list constrains (19). And we obtain the satisfied (SAT) solutions of
the SMT problem and maximize the following objective:
𝑤𝑀𝑛𝑀𝑘 − 𝑤1𝑛1𝑘 −
2𝑤𝑖𝑛𝑖𝑘 − 𝑤𝑖−1𝑛(𝑖−1)𝑘 − 𝑤𝑖+1𝑛(𝑖+1)𝑘,
[𝑊𝑚𝑖𝑛,𝑊𝑚𝑎𝑥]
𝐵
𝑤𝑖+1𝑛(𝑖+1)𝑘
𝑤𝑖−1𝑛(𝑖−1)𝑘 + 𝑤𝑖+1𝑛(𝑖+1)𝑘 .
(18)
(19)
we observe that the area associated with the frequency domain
features of the malicious traffic is significantly lighter than that of
the benign traffic.
4.2 Automatic Parameters Selection Module
Now we determine the encoding vector 𝑤 for the feature extraction
module that uses 𝑤 to encode the per-packet feature sequences
and acquires the vector representation of the traffic. In general, we
formulate the encoding vector selection problem as a constrained
optimization problem, and transform the original problem into an
equivalent SMT problem. We approximate the optimal solution of
the original problem through solving the SMT problem.
We assume that we can find a set of continuous functions to
describe the changes of each kind of the per-packet feature 𝑠(𝑖).
Thus, we consider all obtained per-packet features are the samples
of the continuous functions, which are denoted as ℎ𝑖(𝑡) (1 ≤ 𝑖 ≤ 𝑀).
We need to find a vector 𝑤 to amplify and superpose all these
functions. Our key optimization objective is to minimize mutual
interference and bound the overall range when superposing the
functions. We can first bound the range of encoding vector 𝑤 and
the range of the superposition function in the following:
𝑊𝑚𝑖𝑛 ≤ 𝑤𝑖 ≤ 𝑊𝑚𝑎𝑥
(1 ≤ 𝑖 ≤ 𝑀),
𝑀
𝑖=1
𝑤𝑖ℎ𝑖(𝑡) ≤ 𝐵,
(14)
(15)
where 𝑊𝑚𝑖𝑛, 𝑊𝑚𝑎𝑥, 𝐵 are constants. We constrain the order pre-
serving properties of the functions to ensure that different types
of per-packet features do not interfere with each other when the
feature extraction module performs packet encoding:
(1 ≤ 𝑖 ≤ 𝑀 − 1).
(16)
Second, we optimize 𝑤 to maximize the distances between the
functions so that we can minimize the mutual interference of the
per-packet features and bound the ranges of all the functions. There-
fore, under the constrains of (14) (15) (16), we obtain the optimiza-
tion object:
𝑤𝑖ℎ𝑖(𝑡) ≤ 𝑤𝑖+1ℎ𝑖+1(𝑡)
𝑤𝑀ℎ𝑀(𝑡) − 𝑤1ℎ1(𝑡)d𝑡 −
|2𝑤𝑖ℎ𝑖(𝑡) − 𝑤𝑖+1ℎ𝑖+1(𝑡) − 𝑤𝑖−1ℎ𝑖−1(𝑡)|d𝑡 .
(17)
In practice, we cannot determine the convexity of the optimiza-
tion object because the closed-form representations of ℎ𝑖(𝑡) are
not available. Thus, we reform the origin constrained optimization
problem to a Satisfiability Modulo Theories (SMT) problem (19)
with optimization object (18) to approximate the optimal solution
∫ +∞
0
ˆ𝑤 = arg max
∫ +∞
0
𝑀−1
𝑖=2
𝑁
(cid:101)𝑤 = arg max
𝑀−1
𝑘=1
𝑖=2
subjects to:
𝑤𝑖
𝑀
∈
𝑤𝑖𝑛𝑖𝑘 ≤
≤
≤
𝑖=1
𝑤𝑖𝑛𝑖𝑘
2𝑤𝑖𝑛𝑖𝑘
Note that, we reform the absolute value operation in the opti-
mization object (17) into constrains (19) because most SMT solvers
do not support absolute value operations.
4.3 Statistical Clustering Module
Now we utilize the statistical clustering algorithm to learn the pat-
terns of the frequency domain features obtained from the feature
extraction module with the selected parameters. We train the statis-
tical clustering algorithm with only benign traffic. In the training
phase, this module calculates the clustering centers of the frequency
domain features and the averaged training loss. In order to improve
the robustness of Whisper and reduce false positive caused by the
extreme values, we segment the frequency domain feature matrix
R with a sampling window of length 𝑊𝑤𝑖𝑛. We use 𝑁𝑡 to denote
the number of samples and 𝑙 to denote the start points. We average
the sampling window on the dimension of the feature sequence
and use 𝑟𝑖 to indicate the input of the clustering algorithm. We can
obtain:
𝑙 = 𝑖𝑊𝑤𝑖𝑛
(0 ≤ 𝑖 < 𝑁𝑡), 𝑁𝑡 =
,
(20)
(cid:22) 𝑁𝑓
(cid:23)
𝑊𝑤𝑖𝑛
𝑟𝑖 = mean(R[[𝑙 : 𝑙 + 𝑊𝑤𝑖𝑛]]).
(21)
We perform the statistical clustering algorithm and acquire all
clustering centers to represent the benign traffic patterns. We use
𝐶𝑘 to denote the 𝐾𝐶 clustering centers, where (1 ≤ 𝑘 ≤ 𝐾𝐶), and
then calculate the averaged training loss. For each 𝑟𝑖, we find the
closest clustering center as ˆ𝐶𝑖 and we take averaged L2-norm as
01020304050051015Side-channelAttackBenignEncryptedTraﬃc01020304050051015SSLDoSAttackBenignVideoTraﬃc01020304050051015Low-rateDoSAttackBenignOutboundNATTraﬃcSession 12C: Traffic Analysis and Side ChannelsCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3435the training loss:
ˆ𝐶𝑖 = arg min
𝐶𝑘
∥𝐶𝑘 − 𝑟𝑖∥2
(1 ≤ 𝑖 ≤ 𝑁𝑡),
(cid:13)(cid:13)𝑟𝑖 − ˆ𝐶𝑖(cid:13)(cid:13)2 .
(22)
(23)
𝑁𝑡
𝑖=1
train_loss =
1
𝑁𝑡
In the detection phase, this module calculates the distances be-
tween the frequency domain features of traffic and the clustering
centers. For each given frequency domain feature, we sample 𝑁𝑡
segments on R with length 𝑊𝑤𝑖𝑛, which is the same as the training
phase. We can find the closest clustering center ˆ𝐶𝑖 as an estimate
of 𝑟𝑖. We calculate the L2-norm as the estimation error:
(1 ≤ 𝑘 ≤ 𝐾𝐶).
(24)
If the estimation error loss𝑖 ≥ (𝜙 × train_loss), we can conclude
that the statistical clustering algorithm cannot understand the fre-
quency domain features of the traffic, which means the traffic is
malicious.