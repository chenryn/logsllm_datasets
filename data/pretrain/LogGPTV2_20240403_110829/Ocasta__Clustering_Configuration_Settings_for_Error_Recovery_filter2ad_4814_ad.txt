Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Table IV: Ocasta recovery performance. For each error, we
show the average cluster size, the number of trials required
for Ocasta to ﬁnd the offending cluster using DFS, the
recovery time in minutes and seconds to ﬁnd the offending
cluster vs the time for Ocasta to search all the clusters, and
the total number of unique screenshots, and the comparison
of the effectiveness between Ocasta and Ocasta-NoClust.
traces that exhibited usage of the same application in the
conﬁguration error scenario.
We ﬁrst evaluate how effective Ocasta is at ﬁxing 16
real-world conﬁguration errors, numbered 1-16 in Table III,
which are all conﬁguration errors that were either previously
used in the literature [3], [14] or were found via online
forums, FAQ documents and conﬁguration documents. To
demonstrate the beneﬁt of using clustering, we compare the
effectiveness of Ocasta with the effectiveness of a modiﬁed
version of Ocasta, called Ocasta-NoClust, that does not use
clustering and rolls back a single conﬁguration setting at a
time when it tries to ﬁx errors.
We use as many complex and real conﬁguration errors
as possible for the evaluation. For example, error #12 was
found on an internet message board, where the discussion
contained 56 messages spanning 3 months. However, we are
restricted to only using errors where the offending setting(s)
have been modiﬁed in our traces – otherwise Ocasta will
have no clustering information for them and Ocasta’s repair
tool will have no values to roll back to. This problem
cannot happen in practice because any conﬁguration key
that is misconﬁgured must have a modiﬁcation history on
a particular system. We simulate the conﬁguration error by
injecting the erroneous value into the TTKV 14 days before
the end of the trace and invoke Ocasta in recovery mode.
For each error, we provide a suitable trial and set the start
time to 14 days before the end of the trace. We conﬁgure
Ocasta to use the DFS search strategy.
We evaluated Ocasta using the minimum window size of 1
second and the maximum correlation threshold of 2, because
these produce smaller clusters and are thus the most likely
to lead to invalid conﬁgurations or failed ﬁxes. In practice, a
user can adjust these settings in case they fail to cluster the
486
conﬁguration settings that cause the conﬁguration problem.
With these parameters, Ocasta was able to successfully ﬁnd
the offending cluster and ﬁx the errors in all cases except
errors #2 and #4. In both of these cases, the settings that
needed to be rolled back were split into several clusters.
In error #2, the offending settings consisted of one rarely-
changing dominant setting, which controls the validity of
another 50 settings that change frequently over a moderate
span of time, as we described in Figure 1a. When the
clustering threshold is reduced to 1, the dominant setting is
clustered with 34 of the other settings, but there remain 26
settings that were not clustered together. When we increase
the window size to 30 seconds, causing all settings to be
clustered together. In error #4, one setting stores an ordered
list of names of settings that store applications capable of
opening Flash video ﬁles. The setting storing the list tends to
change even when the setting storing the application name
does not change. Reducing clustering threshold to 1 caused
both the setting storing the list and the settings storing
application names to be clustered together.
Quantitative results are shown in Table IV. We can see
that Ocasta successfully ﬁxed all 16 conﬁguration errors,
but Ocasta-NoClust failed to ﬁx 5 conﬁguration errors,
because it requires rolling back more than one conﬁguration
settings at a time to ﬁx them. The average cluster size varies
between 1 and 8 for our errors, thus effectively reducing the
search space by the same factor because Ocasta searches
clusters of keys at a time instead of individual keys. The
time column gives the time required by Ocasta to ﬁnd the
offending cluster versus the total time for Ocasta to search all
cluster versions up to the 14 day start time. This shows that
Ocasta’s sort is successful at prioritizing the clusters, ﬁnding
the offending cluster by an average of 78% faster than
having to search the entire history. The screenshots column
gives the total number of unique screenshots produced by
Ocasta, while the trials column indicates the number of trials
executed before the offending cluster is found. The user must
examine an average of 3 screenshots, with a worst case of
11, indicating a very modest amount of user effort.
Recall that instead of using DFS, Ocasta can also use
BFS as the search strategy. To study the trade-offs we
perform searches using both strategies over all 16 errors
while varying the number of days in the past when the
error was injected, as well as ﬁxing the injection time at
14 days in the past and adding between 0-2 spurious writes
after the initially injected error to simulate the case where
the user tried to ﬁx the conﬁguration error for 0-2 times.
Figure 2a shows the average number of trial executions as a
function of error injection time for BFS and DFS. As can be
seen, the number of trials by both BFS and DFS increases
as the injection time occurs further in the past, as a result
of Ocasta’s bias towards checking more recently modiﬁed
clusters ﬁrst, while DFS provides better performance overall.
Figure 2b shows the average number of trials as a function
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:26:43 UTC from IEEE Xplore.  Restrictions apply. 
l
s
a
i
r
T
f
o
r
e
b
m
u
N
 25
 20
 15
 10
 5
 0
 0
 2
BFS
DFS
 6
 4
Injection Days
 8
 10
l
s
a
i
r
T
f
o
r
e
b
m
u
N
 30
 25
 20
 15
 10
 5
 0
 12
 14
BFS
DFS
BFS
DFS
 120
 100
 80
 60
 40
 20
l
s
a
i
r
T
f
o
r
e
b
m
u
N
0
1
Number of Writes
2
 0
 0
 10
 30
 20
 60
Time Bounds (days)
 40
 50
 70
 80
(a) By time of errors
(b) By number of spurious writes
(c) By time length
Figure 2: Comparison between DFS and BFS.
s
r
e
t
s
u
c
f
l
o
e
z
s
i
e
g
a
r
e
v
A
 5
 4
 3
 2
 1
 0
 0
 100
 200
 600
Clustering Window Size (Seconds)
 300
 400
 500
s
r
e
t
s
u
c
f
l
o
e
z
s
i
e
g
a
r
e
v
A
 5
 4
 3
 2
 1
 0
 0.5
 1.5
Clustering Threshold
 1
(a) Window size.
(b) Clustering threshold.
Figure 3: Average cluster size.
of the number of spurious writes after the injected error.
BFS search is highly sensitive to this parameter because to
search more writes within a cluster, it must try every other
cluster as well, so the number of rollbacks increases if there
are a lot of clusters.
We now evaluate the effect of the start
time, which
controls the time period Ocasta searches over, on the number
of trials Ocasta must execute. Figure 2c shows the average
number of trials Ocasta perform in its search as start time
goes further into the past. As can be seen, the number of
trials rises roughly linearly with the length of time the search
is conducted over.
C. Sensitivity
We examine the sensitivity of cluster size to both windows
size and clustering threshold. Larger clusters mean fewer
trials, but also lead to the potential for more unrelated
keys getting changed if the offending cluster grows in size.
Figures 3a and 3b show the growth in average cluster
size as a function of both the window size and clustering
sensitivity. The sharp drop at the left hand side of Figure 3a,
is when the window is changed from one second to zero
seconds (modiﬁcations must have the same timestamp at
zero seconds). Since our traces only record key modiﬁcation
times to the nearest second, there is a lot of noise between
these two points. With the exception of this artifact, the
average cluster is relatively insensitive to either parameter,
and ranges between between roughly 3.5 to about 4.5 or 25%
of its value. These results indicate that the overall cluster
size is relatively insensitive to changes in these parameters,
which might suggest that users should tend to prefer smaller
thresholds and larger window sizes to minimize the chances
of the offending cluster being undersized.
D. User Study
 2
To evaluate the effectiveness of the Ocasta repair tool with
default settings 2 , we performed a user study on 19 partici-
pants with various backgrounds. Because this study contains
human subjects, we have obtained a second ethics approval
for this study from our institutional ethics review board. The
participants include two faculty members from our depart-
ment, 13 graduate students from four different departments,
a system administrator, an administrative assistant, and two
software engineers. Six out of the 19 participants of the
user study are non-technical users. None of participants were
authors of this paper and none were compensated for this
user study. Each participant was given a brief explanation on
how Ocasta works and shown a demonstration on a contrived
conﬁguration error. The participant then tested Ocasta on
a computer setup with conﬁguration error #11, #13, #15
and #16 from Table III. We use only four errors to limit
the length of the user study, because it took between 1.5
and 2 hours for each participant to ﬁnish the user study. In
each case, the participants were ﬁrst asked to quantitatively
rate how familiar were they with the application having the
conﬁguration error. Then they were given a description of the
error and were asked to use Ocasta to ﬁx the conﬁguration
error. We recorded the time the participants took to create
the trial. After they ﬁnished creating the trial, they were
asked to quantitatively rate how difﬁcult it was to produce
the trial.
The participant was then shown the set of screenshots
Ocasta produces when run on the history from our traces
and asked to select the screenshot that showed the ﬁxed