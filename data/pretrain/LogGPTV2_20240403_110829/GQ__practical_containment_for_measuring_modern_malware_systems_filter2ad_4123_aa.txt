# Title: GQ: Practical Containment for Measuring Modern Malware Systems

## Authors:
- Christian Kreibich, ICSI & UC Berkeley
- Nicholas Weaver, ICSI & UC Berkeley
- Chris Kanich, UC San Diego
- Weidong Cui, Microsoft Research
- Vern Paxson, ICSI & UC Berkeley

## Abstract
The measurement and analysis of modern malware systems, such as botnets, often require the execution of specimens in an environment that allows them to communicate with other systems across the Internet. Ethical, legal, and technical constraints, however, necessitate the containment of network activity to prevent harm while still enabling the malware to exhibit its inherent behavior. Current best practices in this area are often inadequate, with researchers frequently treating containment superficially or ignoring it altogether.

In this paper, we introduce GQ, a malware execution "farm" that uses explicit containment primitives to enable analysts to develop and enforce containment policies safely and iteratively. We discuss GQ's architecture and implementation, our methodology for developing containment policies, and our experiences from six years of system development and operation.

## Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Network Architecture and Design

## General Terms
Design, Measurement, Reliability, Security

## Keywords
Malware containment, malware execution, honeyfarm, botnets, command-and-control

## 1. Introduction
Some of the most challenging Internet measurement studies today involve analyzing malicious distributed systems such as botnets. These measurements often rely on executing malware samples in a controlled environment to study their behavior. This can range from brief executions to understand bootstrapping behavior to long-term monitoring for in-depth analysis.

Executing malware in a laboratory setting presents a dilemma. Unconstrained execution allows the malware to operate as intended but can lead to harmful activities such as spamming, denial-of-service attacks, and click fraud. Conversely, full isolation prevents the malware from contacting external servers via its command-and-control (C&C) channel, rendering it inactive.

Thus, effective malware measurement requires containment: environments that allow the malware to interact with the external world sufficiently to manifest its core behavior while preventing it from causing harm. Despite the critical importance of containment, researchers often treat it superficially. We attribute this to a lack of technical tools and a perception of containment as a chore rather than an opportunity.

To address this, we present GQ, a malware execution farm we have developed and operated for six years. GQ's design focuses on enabling the development of precise and flexible containment policies for a wide range of malware. Based on our experience, we make three contributions:

1. **Architecture for Containment**: We develop an architecture for malware measurement and analysis that provides a natural way to configure and enforce containment policies using containment servers. These scalable decision points remain logically and physically separate from the system’s gateway, allowing fine-grained control over network flows.
2. **Methodology for Policy Development**: We describe a methodical approach to developing containment policies to ensure the prevention of harm. Our goal is to highlight the importance of containment and show that it can actively support the process of studying malware behavior.
3. **Operational Insights**: We share insights from our extensive operational experience in developing containment policies, illustrating the effectiveness, importance, and utility of a methodical approach.

We begin by reviewing previous work on containment (Section 2), which provides context for our discussion. We then present GQ's design goals (Section 4), architecture (Section 5), and implementation (Section 6). Finally, we report on our operational experiences (Section 7) and offer concluding thoughts, including a discussion of the general feasibility of containment (Section 8).

## 2. Related Work
A significant body of prior work focuses on malware execution to understand the behavior of binary specimens. Historically, "honeyfarms" were the first platforms supporting large-scale malware execution, particularly for worms. The predecessor of GQ, for example, had fully hardwired containment policies specific to capturing worms.

Vrable et al. designed the Potemkin honeyfarm, a prototype that explored scalability constraints when simulating hundreds of victim machines. Their work highlighted the core issue of containment but relied on redirecting outbound connections to additional analysis machines. Jian and Xu’s Collapsar, a virtualization-based honeyfarm, targeted transparent integration into production networks but used simple throttling and inline Snort for containment.

With the decline of the "worm era," researchers shifted focus to broader malware activity. Bayer et al. introduced the Anubis platform, emphasizing the importance of containment but focusing primarily on monitoring specimen-OS interactions. The CWSandbox environment shares similar dynamic analysis goals but acknowledges potential harm to connected machines. The Norman SandBox relies on malware execution in an instrumented environment, emulating the network and internet.

Chen et al. presented a "universe" management abstraction for honeyfarms, allowing multiple infections to spread independently. GQ provides similar functionality through support for independently operating subfarms, though without implying communication between malware instances within the same subfarm.

John et al. introduced Botlab, an analysis platform for studying spamming botnets. While they recognized the importance of safe execution, they employed a static containment policy and ultimately ceased their study due to concerns about unintended harm. Other efforts, such as SLINGbot and DETER, explore complete containment but sacrifice the fidelity of real-world interactions.

## 3. Addressing the Problem of Containment
The light treatment of containment in malware research is partly understandable due to the lack of established tool-chains. Ad-hoc or suboptimal containment mechanisms can leak undesirable activity or prevent desired activity, making containment implementation and verification difficult and time-consuming.

GQ aims to fill this gap by making containment policy development a natural part of the malware analysis process. Precise containment requires balancing the need for realistic outside interactions, containing malicious activity, and convincing the sample that it has unhampered interaction with external systems. Lacking rigor in this process raises ethical and legal issues and can lead to blacklisting of analysis machines.

We argue that containment should not be seen as a resource-draining chore. Developing containment from a default-deny stance, iteratively permitting understood activity, provides an excellent way to understand the behavioral envelope of a sample. GQ does not automate policy development but provides a methodical approach. When deploying new malware samples, we start with a complete default-deny, execute the specimen in a subfarm with a sink server, and iteratively whitelist traffic believed to be safe. This process continues until a containment policy is developed that allows just the C&C lifeline onto the Internet while containing malicious activity inside GQ.

Finally, we emphasize that GQ is not meant to replace static or dynamic malware analysis tools. Instead, we advocate for deploying these tools within a system like GQ to provide mature containment.