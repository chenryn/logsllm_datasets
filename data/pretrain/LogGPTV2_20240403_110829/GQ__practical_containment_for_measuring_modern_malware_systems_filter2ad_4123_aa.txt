title:GQ: practical containment for measuring modern malware systems
author:Christian Kreibich and
Nicholas Weaver and
Chris Kanich and
Weidong Cui and
Vern Paxson
GQ: Practical Containment
for Measuring Modern Malware Systems
Christian Kreibich
ICSI & UC Berkeley
PI:EMAIL
Nicholas Weaver
ICSI & UC Berkeley
PI:EMAIL
Chris Kanich
UC San Diego
PI:EMAIL
Weidong Cui
Microsoft Research
PI:EMAIL
Vern Paxson
ICSI & UC Berkeley
PI:EMAIL
Abstract
Measurement and analysis of modern malware systems such as bot-
nets relies crucially on execution of specimens in a setting that en-
ables them to communicate with other systems across the Internet.
Ethical, legal, and technical constraints however demand contain-
ment of resulting network activity in order to prevent the malware
from harming others while still ensuring that it exhibits its inher-
ent behavior. Current best practices in this space are sorely lack-
ing: measurement researchers often treat containment superﬁcially,
sometimes ignoring it altogether.
In this paper we present GQ,
a malware execution “farm” that uses explicit containment prim-
itives to enable analysts to develop containment policies naturally,
iteratively, and safely. We discuss GQ’s architecture and imple-
mentation, our methodology for developing containment policies,
and our experiences gathered from six years of development and
operation of the system.
Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: NETWORK AR-
CHITECTURE AND DESIGN
General Terms
Design, Measurement, Reliability, Security
Keywords
Malware containment, malware execution, honeyfarm, botnets,
command-and-control
1.
INTRODUCTION
Some of the most challenging Internet measurement studies to-
day concern analyzing malicious distributed systems such as bot-
nets. Such measurement often relies crucially on execution: “let-
ting loose” malware samples in an execution environment to study
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
their behavior, sometimes only for seconds at a time (e.g., to un-
derstand the bootstrapping behavior of a binary, perhaps in tandem
with static analysis), but potentially also for weeks on end (e.g., to
conduct long-term botnet measurement via “inﬁltration” [13]).
This need to execute malware samples in a laboratory setting ex-
poses a dilemma. On the one hand, unconstrained execution of the
malware under study will likely enable it to operate fully as in-
tended, including embarking on a large array of possible malicious
activities, such as pumping out spam, contributing to denial-of-
service ﬂoods, conducting click fraud, or obscuring other attacks
by proxying malicious trafﬁc. On the other hand, if executed in
full isolation, the malware will almost surely fail to operate as in-
tended, since it cannot contact external servers via its command-
and-control (C&C) channel in order to obtain input data or execu-
tion instructions.
Thus, industrial-strength malware measurement requires con-
tainment: execution environments that on the one hand allow mal-
ware to engage with the external world to the degree required to
manifest their core behavior, but doing so in a highly controlled
fashion to prevent the malware from inﬂicting harm on others. De-
spite the critical importance of proper containment, measurement
researchers often treat the matter superﬁcially, sometimes ignoring
it altogether. We perceive the root cause for this shortcoming as
arising largely from a lack of technical tools to realize sufﬁciently
rich containment, along with a perception of containment as a chore
rather than an opportunity. To improve the state of affairs, we
present GQ, a malware execution “farm” we have developed and
operated regularly for six years. GQ’s design explicitly focuses on
enabling the development of precise-yet-ﬂexible containment poli-
cies for a wide range of malware. Drawing upon GQ’s development
and operation over this period, we make three contributions:
First, we develop an architecture for malware measurement and
analysis that provides a natural way to conﬁgure and enforce con-
tainment policies by means of containment servers. These scalable
decision points remain logically and physically separate from the
system’s gateway, and establish ﬁrst-order primitives for effecting
containment on a per-ﬂow granularity. We began our work on GQ
during the peak of the “worm era” [9], and initially focused on cap-
turing self-propagating network worms. Accordingly, we highlight
both the system’s original architectural features that have “stood
the test of time,” as well as the shortcomings the emerged as our
usage of GQ began to shift towards broader forms of malware.
Second, we describe a methodical—albeit largely manual—
approach to the development of containment policies to ensure pre-
vention of harm to others. Our goal here is to draw attention to the
issue, highlight potential avenues for future research, and point out
397that far from being a chore, containment development can actively
support the process of studying malware behavior.
Third, we present insights from our extensive operational experi-
ence [4, 9, 13, 18, 19, 25, 21, 3] in developing containment policies
that serve to illustrate the effectiveness, importance, and utility of a
methodical approach to containment.
We begin by reviewing the approaches to containment framed in
previous work (§ 2), which provides further context for our discus-
sion of the issue of containment (§ 3). We then present GQ’s design
goals (§ 4), architecture (§ 5), and implementation (§ 6), illustrat-
ing both initial as well as eventual features as a consequence of our
continued desire to separate policy from mechanism and increase
general applicability of the system. Finally, we report on our oper-
ational experiences over GQ’s lifetime (§ 7), and offer concluding
thoughts, including discussion of containment’s general feasibil-
ity (§ 8).
2. RELATED WORK
A large body of prior work focuses on malware execution for
the purpose of understanding the modi operandi of binary spec-
imens. Historically, “honeyfarms” were the ﬁrst platforms sup-
porting large-scale malware execution. These systems focused in
particular on worm execution, with the “honey” facet of the name
referring to a honeypot-style external appearance of presenting a
set of infectible systems in order to trap specimens of global worm
outbreaks early in the worm’s propagation. The predecessor [9] of
today’s GQ platform represents a typical example of this line of
work. In contrast to our current platform, its containment policies
were fully hardwired and speciﬁc to the goal of capturing worms.
Vrable et al. designed the Potemkin honeyfarm [29] as a (highly)
purpose-speciﬁc prototype of a worm honeyfarm that explored the
scalability constraints present when simulating hundreds of victim
machines on a single physical machine. Their work also framed
the core issue of containment, but leveraged a major simpliﬁcation
that worms can potentially provide, namely that one can observe
worm propagation even when employing a very conservative con-
tainment policy of redirecting outbound connections to additional
analysis machines in the honeyfarm. Jian and Xu’s Collapsar [11],
a virtualization-based honeyfarm, targeted the transparent integra-
tion of honeypots into a distributed set of production networks. The
authors focused on the functional aspects of the system, and while
Collapsar’s design supports the notion of assurance modules to im-
plement containment policies, in practice these simply relied on
throttling and use of “Inline Snort” to block known attacks.
With the waning of the “worm era,” researchers shifted focus
from worm-like characteristics to understanding malware activity
more broadly. Bayer et al. recognized the signiﬁcance of the con-
tainment problem in a paper introducing the Anubis platform [2],
which has served as the basis for numerous follow-up studies: “Of
course, running malware directly on the analyst’s computer, which
is probably connected to the Internet, could be disastrous as the
malicious code could easily escape and infect other machines.”
The remainder of the paper, however, focuses on technical aspects
of monitoring the specimen’s interaction with the OS; they do not
address the safety aspect further.
The CWSandbox environment shares the dynamic analysis goals
of Anubis, using virtualization or potentially “raw iron” (non-
virtualized) execution instead of full-system emulation [31]. The
authors state that CWSandbox’s reports include “the network con-
nections it opened and the information it sent”, and simply ac-
knowledge that “using the CWSandbox might cause some harm to
other machines connected to the network.”
The Norman SandBox [23] similarly relies on malware execu-
tion in an instrumented environment to understand behavior. As a
proprietary system, we cannot readily infer its containment model,
but according to its description it emulates the “network and even
the Internet.”
Chen et al. [7] presented a “universe” management abstraction
for honeyfarms based on Potemkin, allowing multiple infections
to spread independently in a farm. Their universes represent sets
of causally related honeypots created due to mutual communica-
tion. GQ provides such functionality in a more general fashion via
its support for independently operating subfarms.
In contrast to
Potemkin’s universes, GQ’s subfarms do not imply that malware
instances executing within the same subfarm necessarily can com-
municate.
More recently, John et al. presented Botlab [12], an analysis plat-
form for studying spamming botnets. Botlab shares commonalities
with GQ, including long-term execution of bot binaries, in their
case particularly for the purpose of collecting spam campaign intel-
ligence. They also recognize the importance of executing malware
safely. In contrast to GQ, the authors designed Botlab speciﬁcally
to study email spam, for which they employed a static containment
policy: “[T]rafﬁc destined to privileged ports, or ports associated
with known vulnerabilities, is automatically dropped, and limits
are enforced on connections rates, data transmission, and the total
window of time in which we allow a binary to execute.” In addi-
tion, the authors ultimately concluded that containment poses an
intractable problem, and ceased pursuing their study: “Moreover,
even transmitting a “benign” C&C message could cause other,
non-Botlab bot nodes to transmit harmful trafﬁc. Given these con-
cerns, we have disabled the crawling and network ﬁngerprinting
aspects of Botlab, and therefore are no longer analyzing or incor-
porating new binaries.”
Researchers have also explored malware execution in the pres-
ence of complete containment (no external connectivity). These
efforts rely on mechanisms that aim to mimic the ﬁdelity of com-
munication between bots and external machines. The SLINGbot
system emulates bot behavior rather than allowing the trafﬁc of live
malware on the commodity Internet [10]; clearly, such an approach
can impose signiﬁcant limits on the obtainable depth of analysis.
The DETER testbed [22] relies upon experimenters and testbed op-
erators to come to a consensus on the speciﬁc containment policy
for a given experiment. Barford and Blodgett’s Botnet Mesocosms
run real bot binaries, but emulate the C&C infrastructure in order
to evaluate bot behavior without Internet connectivity [1]. Most
recently, Clavet et al. described their in-the-lab botnet experimen-
tation testbed with a similar containment policy to Botnet Meso-
cosms, but with the capability of running thousands of bot binaries
at one time [5]. While all of these systems guarantee that malicious
trafﬁc does not interact with the commodity Internet, they required
substantial effort for enabling a useful level of operational ﬁdelity
and, in the process, necessarily sacriﬁce the ﬁdelity that precisely
controlled containment can offer.
3. ADDRESSING THE PROBLEM OF
CONTAINMENT
The frequently light treatment of the central problem of execut-
ing malware safely is to some extent understandable. Established
tool-chains for developing containment do not exist, so the result-
ing containment mechanisms are ad-hoc or suboptimal, leaking un-
desirable activity or preventing desired activity. This deﬁciency
renders containment implementation and veriﬁcation a difﬁcult,
time-consuming problem—one that frequently gets in the way of
conducting the intended experiment. GQ aims to ﬁll this gap by
398making containment policy development a natural component of
the malware analysis process: per-ﬂow containment decisions form
central, explicit notions reﬂected in much of the system’s architec-
ture.
Development of precise containment requires a balance of (i) al-
lowing the outside interactions necessary to provide the required re-
alism, (ii) containing any malicious activity, all while (iii) convinc-
ingly pretending to the sample under study that it enjoys unham-
pered interaction with external systems. Lacking rigor in this pro-
cess raises ethical issues, since knowingly permitting such activity
can cost third parties signiﬁcant resources in terms of cleaning up
infections, ﬁltering spam, fending off attacks, or even ﬁnancial loss.
It also may expose the analyst to potential legal consequences, or at
the very least to abuse complaints. Finally, technical considerations
matter as well: prolonged uncontained malicious activity can lead
to blacklisting of the analysis machines by the existing malware-
ﬁghting infrastructure. Such countermeasures can prevent the ana-
lyst from acquiring unbiased measurements, as these responses can
lead to ﬁltering of key activity elsewhere in the network, or because
the malware itself changes its behavior once it detects suspiciously
diminishing connectivity. We might consider full containment as
a viable option for experiments such as “dry-runs” of botnet take-
downs [5], but we ﬁnd that frequently the dynamically changing
nature of real-world interactions and the actual content of delivered
instructions holds central interest for analysis.
Thus, we argue for the importance of avoiding the perception
of containment as a resource-draining chore. Indeed, we ﬁnd that
developing containment from a default-deny stance, iteratively per-
mitting understood activity, in fact provides an excellent way to
understand the behavioral envelope of a sample under study.
GQ currently does not by itself help to automate the develop-
ment of containment policies. However, it provides the basis for
a methodical approach to containment policy development. When
deploying new malware samples, we employ the following strat-
egy. Beginning from a complete default-deny of interaction with
the outside world, we execute the specimen in a subfarm provid-
ing a “sink server” that accepts arbitrary trafﬁc without meaning-
fully responding to it, and to which the containment server reﬂects
all outbound trafﬁc. We thus understand the extent to which the
specimen comes alive, and can inspect the nature of the attempted
communication and in the best case infer the precise intent, or at
least distinguish C&C trafﬁc from malicious activity. We can then
whitelist trafﬁc believed-safe for outside interaction, in the most
narrow fashion possible. For example, we might only allow HTTP
GET requests, seemingly for C&C instructions, that have a sim-
ilar URL path structure. Generally opening up HTTP would be
overzealous, as malware might use HTTP both for C&C as well
as a burst of SQL injection attacks. We then iterate the process
over repeated executions of the specimen until we arrive at a con-
tainment policy that allows just the C&C lifeline onto the Internet,
while containing malicious activity inside GQ.
Finally, we emphasize that we do not mean for GQ to replace
the use of static or dynamic malware analysis tools. Rather, we ar-
gue that researchers should deploy cutting-edge malware analysis
tools inside a system such as GQ, to provide a mature containment