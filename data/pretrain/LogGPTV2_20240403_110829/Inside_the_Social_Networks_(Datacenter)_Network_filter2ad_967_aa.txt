title:Inside the Social Network's (Datacenter) Network
author:Arjun Roy and
Hongyi Zeng and
Jasmeet Bagga and
George Porter and
Alex C. Snoeren
Inside the Social Network’s (Datacenter) Network
Arjun Roy, Hongyi Zeng†, Jasmeet Bagga†, George Porter, and Alex C. Snoeren
Department of Computer Science and Engineering
University of California, San Diego
†Facebook, Inc.
ABSTRACT
Large cloud service providers have invested in increasingly
larger datacenters to house the computing infrastructure re-
quired to support their services. Accordingly, researchers
and industry practitioners alike have focused a great deal of
effort designing network fabrics to efﬁciently interconnect
and manage the trafﬁc within these datacenters in perfor-
mant yet efﬁcient fashions. Unfortunately, datacenter oper-
ators are generally reticent to share the actual requirements
of their applications, making it challenging to evaluate the
practicality of any particular design.
Moreover, the limited large-scale workload information
available in the literature has, for better or worse, heretofore
largely been provided by a single datacenter operator whose
use cases may not be widespread. In this work, we report
upon the network trafﬁc observed in some of Facebook’s dat-
acenters. While Facebook operates a number of traditional
datacenter services like Hadoop, its core Web service and
supporting cache infrastructure exhibit a number of behav-
iors that contrast with those reported in the literature. We
report on the contrasting locality, stability, and predictability
of network trafﬁc in Facebook’s datacenters, and comment
on their implications for network architecture, trafﬁc engi-
neering, and switch design.
Keywords
Datacenter trafﬁc patterns
CCS Concepts
•Networks → Network measurement; Data center net-
works; Network performance analysis; Network monitor-
ing; Social media networks;
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’15, August 17–21, 2015, London, United Kingdom
c(cid:13) 2015 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787472
1.
INTRODUCTION
Datacenters are revolutionizing the way in which we de-
sign networks, due in large part to the vastly different engi-
neering constraints that arise when interconnecting a large
number of highly interdependent homogeneous nodes in a
relatively small physical space, as opposed to loosely cou-
pled heterogeneous end points scattered across the globe.
While many aspects of network and protocol design hinge
on these physical attributes, many others require a ﬁrm un-
derstanding of the demand that will be placed on the network
by end hosts. Unfortunately, while we understand a great
deal about the former (i.e., that modern cloud datacenters
connect 10s of thousands of servers using a mix of 10-Gbps
Ethernet and increasing quantities of higher-speed ﬁber in-
terconnects), the latter tend to be not disclosed publicly.
Hence, many recent proposals are motivated by lightly
validated assumptions regarding datacenter workloads, or, in
some cases, workload traces from a single, large datacenter
operator [12, 26]. These traces are dominated by trafﬁc gen-
erated as part of a major Web search service, which, while
certainly signiﬁcant, may differ from the demands of other
major cloud services. In this paper, we study sample work-
loads from within Facebook’s datacenters. We ﬁnd that traf-
ﬁc studies in the literature are not entirely representative of
Facebook’s demands, calling into question the applicability
of some of the proposals based upon these prevalent assump-
tions on datacenter trafﬁc behavior. This situation is partic-
ularly acute when considering novel network fabrics, trafﬁc
engineering protocols, and switch designs.
As an example, a great deal of effort has gone into iden-
tifying effective topologies for datacenter interconnects [4,
19, 21, 36]. The best choice (in terms of cost/beneﬁt trade-
off) depends on the communication pattern between end
hosts [33]. Lacking concrete data, researchers often de-
sign for the worst case, namely an all-to-all trafﬁc matrix
in which each host communicates with every other host with
equal frequency and intensity [4]. Such an assumption leads
to the goal of delivering maximum bisection bandwidth [4,
23, 36], which may be overkill when demand exhibits sig-
niﬁcant locality [17].
In practice, production datacenters tend to enforce a cer-
tain degree of oversubscription [12, 21], assuming that either
the end-host bandwidth far exceeds actual trafﬁc demands,
123Finding
Trafﬁc is neither rack local nor all-to-all; low utilization (§4)
Demand is wide-spread, uniform, and stable, with rapidly
changing, internally bursty heavy hitters (§5)
Small packets (outside of Hadoop), continuous arrivals; many
concurrent ﬂows (§6)
Potential impacts
Previously published data
50–80% of trafﬁc is rack local [12, 17] Datacenter fabrics [4, 36, 21]
Demand is frequently concentrated
Trafﬁc engineering [5, 14,
25, 39]
and bursty [12, 13, 14]
SDN controllers
22,
Bimodal ACK/MTU packet
size,
28, 32, 34]; Circuit/hybrid
on/off behavior [12]; <5 concurrent
large ﬂows [8]
switching [7, 20, 30, 39]
[1,
Table 1: Each of our major ﬁndings differs from previously published characterizations of datacenter trafﬁc. Many systems
incorporate one or more of the previously published features as design assumptions.
or that there is signiﬁcant locality in demand that decreases
the need for full connectivity between physically disparate
portions of the datacenter. The precise degree of oversub-
scription varies, but there is general agreement amongst op-
erators that full connectivity is rarely worthwhile [11]. To
mitigate potential “hotspots” caused by oversubscription, re-
searchers have suggested designs that temporarily enhance
connectivity between portions of the datacenter [5, 25, 40].
The utility of these approaches depends upon the prevalence,
size, and dynamics of such hotspots.
In particular, researchers have proposed inherently non-
uniform fabrics which provide qualitatively different con-
nectivity to certain portions of the datacenter through various
hybrid designs, typically including either optical [30, 39] or
wireless links [25, 40]. If demand can be predicted and/or
remains stable over reasonable time periods, it may be feasi-
ble to provide circuit-like connectivity between portions of
the datacenter [20]. Alternatively, network controllers could
select among existing paths in an intelligent fashion [14].
Regardless of the technology involved, all of these tech-
niques require trafﬁc to be predictable over non-trivial time
scales [14, 20, 25, 30, 39].
Finally, many have observed that the stylized nature of
datacenter trafﬁc opens up many avenues for increasing the
efﬁciency of switching hardware itself. In particular, while
some have proposed straightforward modiﬁcations like de-
creased buffering, port count, or sophistication [4] in vari-
ous layers of the switching fabric, others have proposed re-
placing conventional packet switches either with circuit or
hybrid designs that leverage locality, persistence, and pre-
dictability of trafﬁc demands [30]. More extreme, host-
based solutions advocate connecting end-hosts directly [21,
23]. Obviously, when, where, or if any of these approaches
makes economic sense hinges tightly on offered loads [33].
While there have been a number of studies of univer-
sity [14] and private datacenters [12], many proposals cannot
be fully evaluated without signiﬁcant scale. Almost all of the
previous studies of large-scale (10K hosts or larger) datacen-
ters [5, 12, 14, 17, 21, 25, 26] consider Microsoft datacen-
ters. While Facebook’s datacenters have some commonality
with Microsoft’s, such as eschewing virtual machines [14],
they support a very different application mix. As a result,
we observe a number of critical distinctions that may lead to
qualitatively different conclusions; we describe those differ-
ences and explain the reasons behind them.
Our study is the ﬁrst to report on production trafﬁc in
a datacenter network connecting hundreds of thousands of
10-Gbps nodes. Using both Facebook-wide monitoring sys-
tems and per-host packet-header traces, we examine services
that generate the majority of the trafﬁc in Facebook’s net-
work. While we ﬁnd that the trafﬁc patterns exhibited by
Facebook’s Hadoop deployments comport well with those
reported in the literature, signiﬁcant portions of Facebook’s
service architecture [10, 15] vary dramatically from the
MapReduce-style infrastructures studied previously, leading
to vastly different trafﬁc patterns. Findings of our study with
signiﬁcant architectural implications include:
• Trafﬁc is neither rack-local nor all-to-all; locality de-
pends upon the service but is stable across time peri-
ods from seconds to days. Efﬁcient fabrics may ben-
eﬁt from variable degrees of oversubscription and less
intra-rack bandwidth than typically deployed.
• Many ﬂows are long-lived but not very heavy. Load
balancing effectively distributes trafﬁc across hosts; so
much so that trafﬁc demands are quite stable over even
sub-second intervals. As a result, heavy hitters are
not much larger than the median ﬂow, and the set of
heavy hitters changes rapidly. Instantaneously heavy
hitters are frequently not heavy over longer time pe-
riods, likely confounding many approaches to trafﬁc
engineering.
• Packets are small (median length for non-Hadoop traf-
ﬁc is less than 200 bytes) and do not exhibit on/off
arrival behavior. Servers communicate with 100s of
hosts and racks concurrently (i.e., within the same 5-
ms interval), but the majority of trafﬁc is often destined
to (few) 10s of racks.
While we do not offer these workloads as any more rep-
resentative than others—indeed, they may change as Face-
book’s services evolve—they do suggest that the space of
cloud datacenter workloads is more rich than the literature
may imply. As one way to characterize the signiﬁcance of
our ﬁndings, Table 1 shows how our results compare to the
literature, and cites exemplar systems that incorporate these
assumptions in their design.
The rest of this paper is organized as follows. We begin
in Section 2 with a survey of the major ﬁndings of previ-
ous studies of datacenter trafﬁc. Section 3 provides a high-
level description of the organization of Facebook’s datacen-
ters, the services they support, and our collection method-
ologies. We then analyze aspects of the trafﬁc within a num-
ber of Facebook’s datacenters that impact provisioning (Sec-
tion 4), trafﬁc engineering (Section 5), and switch design
(Section 6), before concluding in Section 7.
1242. RELATED WORK
Initial studies of datacenter workloads were conducted via
simulation [6] or on testbeds [18]. Subsequently, however, a
number of studies of production datacenter trafﬁc have been
performed, primarily within Microsoft datacenters.
It is difﬁcult to determine how many distinct Microsoft
datacenters are reported on in literature, or how representa-
tive that set might be. Kandula et al. observe that their results
“extend to other mining data centers that employ some ﬂa-
vor of map-reduce style workﬂow computation on top of a
distributed block store,” but caution that “web or cloud data
centers that primarily deal with generating responses for web
requests (e.g., mail, messenger) are likely to have different
characteristics.” [26]. By that taxonomy, Facebook’s data-
centers clearly fall in the latter camp. Jalaparti et al. [24]
examine latency for Microsoft Bing services that are similar
in concept to Facebook’s service; we note both similarities
to our workload (relatively low utilization coupled with a
scatter-gather style trafﬁc pattern) and differences (load ap-
pears more evenly distributed within Facebook datacenters).
Three major themes are prevalent in prior studies, and
summarized in Table 1. First, trafﬁc is found to be heavily
rack local, likely as a consequence of the application pat-
terns observed; Benson et al. note that for cloud datacen-
ters “a majority of trafﬁc originated by servers (80%) stays
within the rack” [12]. Studies by Kandula et al. [26], De-
limitrou et al. [17] and Alizadeh et al. [8] observe similarly
rack-heavy trafﬁc patterns.
Second, trafﬁc is frequently reported to be bursty and
unstable across a variety of timescales—an important ob-
servation, since trafﬁc engineering techniques often depend
on relatively long-lived, predictable ﬂows. Kapoor et al.
observe that packets to a given destination often arrive in
trains [27]; while Benson et al. ﬁnd a strong on/off pat-
tern where the packet inter-arrival follows a log-normal dis-
tribution [13]. Changing the timescale of observation can
change the ease of prediction; Delimitrou et al. [17] note
that while trafﬁc locality varies on a day-to-day basis, it re-
mains consistent at the scale of months. Conversely, Ben-
son et al. [14] claim that while trafﬁc is unpredictable at
timescales of 150 seconds and longer, it can be relatively
stable on the timescale of a few seconds, and discuss trafﬁc
engineering mechanisms that might work for such trafﬁc.
Finally, previous studies have consistently reported a bi-
modal packet size [12], with packets either approaching the
MTU or remaining quite small, such as a TCP ACK seg-
ment. We ﬁnd that Facebook’s trafﬁc is very different, with
a consistently small median packet size despite the 10-Gbps
link speed. Researchers have also reported that individual
end hosts typically communicate with only a few (e.g., less
than 5 [8]) destinations at once. For some Facebook services,
an individual host maintains orders of magnitude more con-
current connections.
3. A FACEBOOK DATACENTER
In order to establish context necessary to interpret our
ﬁndings, this section provides a brief overview of Face-
Figure 1: Facebook’s 4-post cluster design [19]
book’s datacenter network topology, as well as a description
of the application services that it supports; more detail is
available elsewhere [2, 9, 10, 15, 19]. We then describe the
two distinct collection systems used to assemble the network
traces analyzed in the remainder of the paper.
3.1 Datacenter topology
Facebook’s network consists of multiple datacenter sites
(henceforth site) and a backbone connecting these sites.
Each datacenter site contains one or more datacenter build-
ings (henceforth datacenter) where each datacenter contains
multiple clusters. A cluster is considered a unit of deploy-
ment in Facebook datacenters. Each cluster employs a con-
ventional 3-tier topology depicted in Figure 1, reproduced
from a short paper [19].
Machines are organized into racks and connected to a
top-of-rack switch (RSW) via 10-Gbps Ethernet links. The
number of machines per rack varies from cluster to cluster.
Each RSW in turn is connected by 10-Gbps links to four
aggregation switches called cluster switches (CSWs). All
racks served by a particular set of CSWs are said to be in
the same cluster. Clusters may be homogeneous in terms
of machines—e.g. Cache clusters—or heterogeneous, e.g.
Frontend clusters which contain a mixture of Web servers,
load balancers and cache servers. CSWs are connected to
each other via another layer of aggregation switches called
Fat Cats (FC). As will be seen later in this paper, this design
follows directly from the need to support a high amount of