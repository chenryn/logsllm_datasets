# Inside the Social Network's (Datacenter) Network

**Authors:**
- Arjun Roy
- Hongyi Zeng†
- Jasmeet Bagga†
- George Porter
- Alex C. Snoeren

**Affiliations:**
- Department of Computer Science and Engineering, University of California, San Diego
- †Facebook, Inc.

## Abstract

Large cloud service providers have increasingly invested in larger datacenters to support their services. Consequently, researchers and industry practitioners have focused on designing network fabrics that efficiently interconnect and manage traffic within these datacenters. However, datacenter operators are often reluctant to share the specific requirements of their applications, making it challenging to evaluate the practicality of any particular design. Additionally, the limited large-scale workload information available in the literature has largely been provided by a single datacenter operator, whose use cases may not be representative. In this work, we report on the network traffic observed in some of Facebook’s datacenters. While Facebook operates traditional datacenter services like Hadoop, its core web service and supporting cache infrastructure exhibit behaviors that differ from those reported in the literature. We discuss the contrasting locality, stability, and predictability of network traffic in Facebook’s datacenters and comment on their implications for network architecture, traffic engineering, and switch design.

**Keywords:**
- Datacenter traffic patterns

**CCS Concepts:**
- Networks → Network measurement; Data center networks; Network performance analysis; Network monitoring; Social media networks

**Copyright Notice:**
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

**Conference Information:**
SIGCOMM '15, August 17–21, 2015, London, United Kingdom
© 2015 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787472

## 1. Introduction

Datacenters are revolutionizing network design due to the unique engineering constraints of interconnecting a large number of highly interdependent homogeneous nodes in a relatively small physical space. While many aspects of network and protocol design depend on these physical attributes, others require a firm understanding of the demand placed on the network by end hosts. Unfortunately, while we have a good understanding of the former (e.g., modern cloud datacenters connect tens of thousands of servers using 10-Gbps Ethernet and higher-speed fiber interconnects), the latter is often not publicly disclosed. Therefore, many recent proposals are based on lightly validated assumptions about datacenter workloads or, in some cases, workload traces from a single large datacenter operator. These traces are dominated by traffic generated by a major web search service, which may differ from the demands of other major cloud services.

In this paper, we study sample workloads from within Facebook’s datacenters and find that traffic studies in the literature do not entirely represent Facebook’s demands. This calls into question the applicability of some proposals based on prevalent assumptions about datacenter traffic behavior. For example, significant effort has gone into identifying effective topologies for datacenter interconnects. The best choice depends on the communication pattern between end hosts. Lacking concrete data, researchers often design for the worst case, assuming an all-to-all traffic matrix. Such an assumption leads to the goal of delivering maximum bisection bandwidth, which may be overkill when demand exhibits significant locality.

In practice, production datacenters tend to enforce a certain degree of oversubscription, assuming either that the end-host bandwidth far exceeds actual traffic demands or that there is significant locality in demand. The precise degree of oversubscription varies, but there is general agreement among operators that full connectivity is rarely worthwhile. To mitigate potential “hotspots” caused by oversubscription, researchers have suggested designs that temporarily enhance connectivity between portions of the datacenter. The utility of these approaches depends on the prevalence, size, and dynamics of such hotspots.

Our study is the first to report on production traffic in a datacenter network connecting hundreds of thousands of 10-Gbps nodes. Using both Facebook-wide monitoring systems and per-host packet-header traces, we examine services that generate the majority of the traffic in Facebook’s network. While we find that the traffic patterns exhibited by Facebook’s Hadoop deployments align well with those reported in the literature, significant portions of Facebook’s service architecture vary dramatically from the MapReduce-style infrastructures studied previously, leading to vastly different traffic patterns. Key findings include:

- Traffic is neither rack-local nor all-to-all; locality depends on the service but is stable across time periods from seconds to days.
- Many flows are long-lived but not very heavy. Load balancing effectively distributes traffic across hosts, making traffic demands quite stable even over sub-second intervals.
- Packets are small (median length for non-Hadoop traffic is less than 200 bytes) and do not exhibit on/off arrival behavior. Servers communicate with hundreds of hosts and racks concurrently, but the majority of traffic is often destined to a few tens of racks.

While we do not offer these workloads as more representative than others, they suggest that the space of cloud datacenter workloads is more diverse than the literature implies. Table 1 compares our results to the literature and cites exemplar systems that incorporate these assumptions in their design.

The rest of this paper is organized as follows. Section 2 surveys the major findings of previous studies of datacenter traffic. Section 3 provides a high-level description of the organization of Facebook’s datacenters, the services they support, and our collection methodologies. Sections 4, 5, and 6 analyze aspects of the traffic within Facebook’s datacenters that impact provisioning, traffic engineering, and switch design, respectively. We conclude in Section 7.

## 2. Related Work

Initial studies of datacenter workloads were conducted via simulation or on testbeds. Subsequently, several studies of production datacenter traffic have been performed, primarily within Microsoft datacenters. It is difficult to determine how many distinct Microsoft datacenters are reported in the literature or how representative they might be. Kandula et al. observe that their results extend to other mining data centers that employ some flavor of map-reduce style workflow computation on top of a distributed block store, but caution that web or cloud data centers that primarily deal with generating responses for web requests (e.g., mail, messenger) are likely to have different characteristics. By this taxonomy, Facebook’s datacenters fall into the latter category. Jalaparti et al. examine latency for Microsoft Bing services similar in concept to Facebook’s service, noting both similarities (relatively low utilization coupled with a scatter-gather style traffic pattern) and differences (load appears more evenly distributed within Facebook datacenters).

Three major themes are prevalent in prior studies, summarized in Table 1:
1. Traffic is heavily rack-local, likely due to the application patterns observed.
2. Traffic is frequently reported to be bursty and unstable across various timescales.
3. Previous studies consistently report a bimodal packet size, with packets either approaching the MTU or remaining quite small, such as a TCP ACK segment.

We find that Facebook’s traffic is very different, with a consistently small median packet size despite the 10-Gbps link speed. Researchers have also reported that individual end hosts typically communicate with only a few destinations at once. For some Facebook services, an individual host maintains orders of magnitude more concurrent connections.

## 3. A Facebook Datacenter

To provide context for interpreting our findings, this section briefly overviews Facebook’s datacenter network topology and the application services it supports. More details are available elsewhere. We then describe the two distinct collection systems used to assemble the network traces analyzed in the remainder of the paper.

### 3.1 Datacenter Topology

Facebook’s network consists of multiple datacenter sites and a backbone connecting these sites. Each datacenter site contains one or more datacenter buildings, each of which contains multiple clusters. A cluster is considered a unit of deployment in Facebook datacenters. Each cluster employs a conventional 3-tier topology, as depicted in Figure 1, reproduced from a short paper [19].

Machines are organized into racks and connected to a top-of-rack switch (RSW) via 10-Gbps Ethernet links. The number of machines per rack varies from cluster to cluster. Each RSW is connected by 10-Gbps links to four aggregation switches called cluster switches (CSWs). All racks served by a particular set of CSWs are said to be in the same cluster. Clusters may be homogeneous (e.g., Cache clusters) or heterogeneous (e.g., Frontend clusters, which contain a mixture of web servers, load balancers, and cache servers). CSWs are connected to each other via another layer of aggregation switches called Fat Cats (FC). This design directly supports the need to handle a high amount of traffic.