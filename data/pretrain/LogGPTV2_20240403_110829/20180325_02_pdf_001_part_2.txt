ory behavior (1 GB are usually too large for user data
didnotliketheburdenofthisalternateAPIandkernel
structures).
developerswantedtobringthebenefitsofhugepagesto
legacyapplicationsandapplicationswithdynamicmem- Linux is greedy and aggressive. Linux’s huge page
orybehavior[6,36]. Hence,theprimarywayhugepages management algorithms are greedy: it promotes huge
areallocatedinLinuxtodayistransparentlybythekernel. pagesinthepagefaulthandlerbasedonlocalinformation.
Linux is also aggressive: it will always try to allocate
Transparentsupportisvital. Transparenthugepage a huge page. Huge pages require 2 MB of contiguous
support [80, 68] is the only practical way to bring the free physical memory but sometimes contiguous phys-
benefitsofhugepagestoallapplications,whichcanre- ical memory is in short supply (e.g., when memory is
mainunchangedwhilethesystemprovidesthemwiththe fragmented). Linux’sapproachtohugepageallocation
oftensignificantperformanceadvantagesofhugepages. workswellforsimpleapplicationsthatallocatealarge
Withtransparenthugepagesupport,thekernelallocates memoryregionanduseituniformly,butwedemonstrate
memorytoapplicationsusingbasepages. Wesaytheker- manyapplicationsthathavemorecomplexbehaviorand
nelpromotesasequenceof512properlyalignedpages arepenalizedbyLinux’sgreedyandaggressivepromo-
toahugepage(anddemotesahugepageinto512base tionofhugepages(§3). Ingensrecognizesthatmemory
pages). contiguityisavaluableresourceandexplicitlymanages
Transparentmanagementofhugepagesbestsupports it.
the multi-programmed and dynamic workloads typical
2.3 Hypervisorsupportforhugepages
ofwebapplicationsandanalyticswherememoryiscon-
tended and access patterns are often unpredictable. To IngensfocusesonthecasewhereLinuxisusedbothas
the contrary, when a single big-memory application is the guest operating system and as the host hypervisor
theonlyimportantprogramrunning,theapplicationcan (i.e.,KVM[62]). TheLinux/KVMpairiswidelyusedin
simply map a large region and keep it mapped for the clouddeployments[27,16,3]. Inthehypervisor,Ingens
durationofexecution,forexamplefastnetworkfunctions supports host huge pages mapped from guest physical
using Intel’s Data Plane Development Kit [10]. These memory. When promoting guest physical memory, In-
simpleprogramsarewellsupportedbyeventherudimen- gensmodifiestheextendedpagetabletousehugepages
taryhugepagesupportinWindowsandOSX.However, becauseitisactingasahypervisor,notasanoperating
USENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 707
Workloads h Bg H h Hg B h Hg H performance,oftensignificantly(upto53%). Thelargest
429.mcf 1.18 1.13 1.43 speedupisalwaysattainedwhenbothhostandguestuse
Canneal 1.11 1.10 1.32 hugepages.
SVM 1.14 1.17 1.53 These results show the value of huge page support
Tunkrank 1.11 1.11 1.30 andshowthatLinux’smemorymanagercanobtainthat
Nutch 1.01 1.07 1.12 benefit under simple operating conditions. However, a
MovieRecmd 1.03 1.02 1.11 variety of more challenging circumstances expose the
Olio 1.43 1.08 1.46 limitationsofLinux’smemorymanagement.
Redis 1.12 1.04 1.20
3 Currenthugepageproblems
MongoDB 1.08 1.22 1.37
This section quantifies the limitations in performance
and fairness for the state-of-the-art in transparent huge
Table 3: Application speed up for huge page (2 MB) pagemanagement. Weexaminevirtualizedsystemswith
supportrelativetohost(h)andguest(g)usingbase(4KB) Linux/KVMastheguestOSandhypervisor. Thevariety
pages. Forexample,h Bmeansthehostusesbasepages andseverityofthelimitationsmotivateourredesignof
andh Hmeansthehostusesbothbaseandhugepages. pagemanagement. Alldataiscollectedusingtheexperi-
mentalsetupdescribedinSection2.4.
system. 3.1 Pagefaultlatencyandsynchronouspromotion
Becauseoperatingsystemandhypervisormemoryman-
Whenaprocessfaultsonananonymousmemoryregion,
agementareunifiedinLinux,Ingensadoptstheunified
thepagefaulthandlerallocatesphysicalmemorytoback
model. Someoftheproblemswithhugepagesthatwede-
thepage. Bothbaseandhugepagessharethiscodepath.
scribeinSection3onlyapplytotheOSandsomeonlyto
Linuxisgreedyandaggressiveinitsallocationofhuge
thehypervisor(summarizedinTable2). Forexample,ad-
pages,soifanapplicationfaultsonabasepage,Linux
dressingmemorysharingvs. performance(§3.6)requires
willimmediatelytrytoupgradetherequestandallocatea
onlyhypervisormodificationsandwouldbeassuccessful
hugepageifitcan.
foraWindowsguestasitisforaLinuxguest. Weleave
This greedy approach fundamentally increases page
for future work determining the most efficient way to
faultlatencyfortworeasons.First,Linuxmustzeropages
implementIngensforoperatingsystemsandhypervisors
beforereturningthemtotheuser. Hugepagesare512×
thatdonotsharememorymanagementcode.
largerthanbasepages,andthusaremuchslowertoclear.
2.4 Performanceimprovementfromhugepages
Second,hugepageallocationrequires2MBofphysically
Table 1 describes a variety of memory-intensive real- contiguousmemory. Whenmemoryisfragmented,the
worldapplicationsincludingwebinfrastructuresuchas OSoftenmustcompactmemorytogeneratethatmuch
key/valuestoresanddatabases, aswellasscientificap- contiguity. Previous work shows that memory quickly
plications,dataanalyticsandrecommendationsystems. fragmentsinmulti-tenantcloudenvironments[41].When
Measurementswithhardwareperformancecountersshow memoryisfragmented,Linuxwilloftensynchronously
theyallspendasignificantportionoftheirexecutiontime compact memory in the page fault handler, increasing
doingpagewalks. Forexample,whenusingbasepages averageandtaillatency.
forbothguestandhost, wemeasure429.mcfspending To measure these effects, we compare page fault la-
47.5%ofitsexecutiontimedoingpagewalks(24.2%for tencywhenhugepagesareenabledanddisabled,infrag-
the extended page table and 23.3% for the guest page mentedandnon-fragmentedsettings. Wequantifyfrag-
table). Ontheotherhand,429.mcfspendsonly4.2%of mentation using the free memory fragmentation index
itsexecutiontimewalkingpagetableswhenusinghuge (FMFI) [58], a value between 0 (unfragmented) and 1
pagesforboththeguestandhost. (highlyfragmented). Amicrobenchmarkmaps10GBof
WeexecuteallworkloadsinaKVMvirtualmachine anonymousvirtualmemoryandreadsitsequentially.
running Linux with default transparent huge page sup- Whenmemoryisunfragmented(FMFI<0.1), page
port[80]forboththeapplication(intheguestOS)and clearing overheads increase average page fault latency
thevirtualmachine(inthehostOS).Thehardwarecon- from3.6µsforbasepagesonlyto378µsforhugepages
figurationisdetailedinSection6. (105× slower). When memory is heavily fragmented,
Table3showstheperformanceimprovementsgained (FMFI=0.9),the3.6µsaveragelatencyforbasepages
with transparent huge page support for both the guest growsto8.1µs(2.1×slower)forbaseandhugepages.
andthehostoperatingsystem. Thetableshowsspeedup Averagelatencyislowerinthefragmentedcasebecause
normalized to the case where both host and guest use 98%oftheallocationsfallbacktobasepages(e.g. be-
onlybasepages. Ineverycase,hugepagesupporthelps causememoryistoofragmentedtoallocateahugepage).
708 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association
1.0
SVM Synchronous Asynchronous
Redisusinghugepage
Exec. time(sec) 178(1.30×) 228(1.02×)
Redisnotusinghugepage
Hugepage 4.8GB 468MB 0.8
Promotionspeed immediate 1.6MB/s xedninoitatnemgarF
0.6
Table4:Comparisonofsynchronouspromotionandasyn-
chronouspromotionwhenbothhostandguestusehuge
pages. Theparenthesisisspeedupcomparedtonotusing 0.4
hugepages. Weusethedefaultasynchronouspromotion
speedofUbuntu14.04.
0.2
Workload Usinghugepages Notusinghugepages
Redis 20.7GB(1.69×) 12.2GB
0.0
MongoDB 12.4GB(1.23×) 10.1GB 0 20 40 60 80 100
time(sec)
Table5: PhysicalmemorysizeofRedisandMongoDB. Figure1: FragmentationindexinLinuxwhenrunninga
Redisserver,withLinuxusing(andnotusing)hugepages.
TheSystemhas24GBmemory. Redisuses13GB,other
Compactingandzeroingmemoryinthepagefaulthandler
processesuse5GB,andsystemhas6GBfreememory.
penalizesapplicationsthataresensitivetoaveragelatency
andtotaillatency,suchasWebservices.
Toavoidthisadditionalpagefaultlatency,Linuxcan
milliongetrequestsfor15million1KBobjectswhich
promotehugepagesasynchronously,basedonaconfig-
areinitiallyinpersistentstorage. MongoDBallocatesthe
urableasynchronouspromotionspeed(inMB/s). Table4
objectssparselyinalargevirtualaddressspace. Linux
showsperformancemeasurementsforasynchronous-only
promoteshugepagesincludingunusedmemory,andas
hugepagepromotionwhenexecutingSVMinavirtual
aresult,MongoDBuses23%morememoryrelativeto
machine. Asynchronous-only promotion turns a 30%
runningwithouthugepagesupport.
speedup into a 2% speedup: it does not promote fast
Greedyandaggressiveallocationofhugepagesmakes
enough. Simply increasing the promotion speed does
it impossible to predict an application’s total memory
notsolvetheproblem. EarlierimplementationsofLinux
usageinproductionbecausememoryusagedependson
didmoreaggressiveasynchronouspromotion,incurring
hugepageuse,whichinturndependsonmemoryfrag-
unacceptablyhighCPUutilizationformemoryscanning
mentationandtheallocationpatternofapplications. Ta-
andcompaction. TheCPUuseofaggressivepromotion
ble5showsifanadministratorprovisions18GBmemory
reducedorinsomecaseserasedtheperformancebenefits
(1.5×over-provisioningrelativetousingonlybasepages),
ofhugepages,causinguserstodisabletransparenthuge
Redisstartsswappingwhenituseshugepages,negating
pagesupportinpractice[17,14,13,7].
thebenefitsofcachingobjectsinmemory[31].
3.2 Increasedmemoryfootprint(bloat) Whiletheseexperimentsillustratethepotentialimpact
Huge pages improve performance, but applications do of bloat for a handful of workloads, it is important to
notalwaysfullyutilizethehugepagesallocatedtothem. notethattheproblemisfundamentaltoLinux’scurrent
Linuxgreedilyallocateshugepageseventhoughunder- design. Memory bloating can happen in any working
utilizedhugepagescreateinternalfragmentation. Ahuge set, memory, and TLB size: application-level memory
pagemighteliminateTLBmisses,butthecostisthata usagecanconspirewithaggressivepromotiontocreate
processusinglessthanafullhugepagehastoreservethe internal fragmentation that the OS cannot address. In
entireregion. suchsituations,suchapplicationswilleventuallyputthe
Table5showsmemorybloatfromhugepageswhen system under memory pressure regardless of physical
runningRedisandMongoDB,eachwithintheirownvir- memorysize.
tualmachine. ForRedis,wepopulate2millionkeyswith
3.3 Hugepagesincreasefragmentation
8KBobjectsandthendelete70%ofthekeysrandomly.
Redisfreesthememorybackingthedeletedobjectswhich Onecommonthemeinanalyzingpagefaultlatency(§3.1)
leaves physical memory sparsely allocated. Linux pro- andmemorybloat(§3.2)isLinux’sgreedyallocationand
motesthesparselyallocatedmemorytohugepages,creat- promotionofhugepages. Wenowmeasurehowaggres-
inginternalfragmentationandcausingRedistouse69% sivepromotionofhugepagesquicklyconsumesavailable
more memory compared to not using huge pages. We physicalmemorycontiguity,whichthenincreasesmem-
demonstratethesameprobleminMongoDB,making10 ory fragmentation for the remaining physical memory.
USENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 709
OS SVM Canneal Redis 3000
FreeBSD 1.28 1.13 1.02
Linux 1.30 1.21 1.15 2500 VM3 )BM(noitpmusnocegapeguH
Table6: Performancespeedupwhenusinghugepagein 2000
differentoperatingsystems.
1500
Increasingfragmentationisthepreconditionforproblems
1000
withpagefaultlatencyandmemorybloat,sogreedypro- VM1
motioncreatesaviciouscycle. Weagainrelyonthefree
memory fragmentation index, or FMFI to quantify the 500
relationshipbetweenhugepageallocationandfragmenta-
VM2
tion. 0
0 100 200 300 400 500 600 700 800
Figure1showsthefragmentationindexovertimewhen
time(sec)
runningthepopularkey-valuestoreapplicationRedisina
SVM VM1 VM2 VM3
virtualmachine.Initially,thesystemislightlyfragmented
Exec.time(sec) 533(1.12×) 589(1.24×) 475
(FMFI=0.3)byotherprocesses. Throughthemeasure-
mentperiod,Redisclientspopulatetheserverwith13GB
Figure2: UnfairallocationofhugepagesinKVM.Three
ofkey/valuepairs. Redisrapidlyconsumescontiguous
virtualmachinesrunconcurrently,eachexecutingSVM.
memoryasLinuxallocateshugepagestoit,increasing
Thelinegraphishugepagesize(MB)overtimeandthe