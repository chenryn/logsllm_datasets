exp(−2β )
(cid:16)Auth, I1; φ
= P (I2 | Auth) , where Z = (cid:80)
= 1 
RBAC
N
RBAC
N
Z
Z
Alice
Bob
c m d
× ×
× ×
×
Table 3: Auth
×
Charlie
rI1
1
×
×
rI1
2
Alice
Bob
rI2
1
×
×
rI2
2
×
Alice
Bob
Charlie
Table 4: UAI1
×
Charlie
Table 5: UAI2
c m d
× ×
rI1
1
rI1
2
Table 6: PAI1
×
c m d
×
rI2
1
rI2
2
Table 7: PAI2
×
×
Alice
Bob
Charlie
Table 8:(cid:16)
c m d
× ×
× ×
RBAC
N
(cid:17) I1
×
φ
c m d
Alice
Bob
Charlie
Table 9:(cid:16)
×
φ
×
×
RBAC
N
(cid:17) I2
×
6 APPLYING MEAN-FIELD APPROXIMATION
The policy miner that is built with Unicorn is an algorithm that re-
ceives as input a permission assignment Auth and computes a policy
I that approximately maximizes P (· | Auth), while letting β → ∞.
Since computing P (· | Auth) is intractable, we use mean-field ap-
proximation [9], a technique that defines an iterative procedure to
approximate P (· | Auth) with a distribution q (·). It turns out that
computing and maximizing q(·) is much easier than computing
and maximizing P (· | Auth). The policy miner is then an algorithm
implementing the computation of q and its maximization.
We next introduce some random variables that help to measure
the probability that a policy authorizes a particular request (u, p) ∈
U × P (Section 6.1). Afterwards, we present the approximating
distribution q (Section 6.2).
1 × . . . × SS
6.1 Random variables
Recall that the sample space Ω of the distribution P (· | Auth) from
Definition 6 is the set of all policies I. Let X be a random variable
mapping I ∈ Ω to I. Although X’s definition is trivial, it will help
us to understand other random variables that we introduce later.
We can understand X as an “unknown policy” and, for a policy I,
the probability statement P (X = I | Auth) measures how much we
believe that X is actually I, given that the organization’s permission
assignment is Auth. By definition, P (X = I | Auth) = P (I | Auth).
Definition 7. Let φ ∈ L and let W be a flexible relation sym-
bol occurring in φ of type S1 × . . . × Sk and let f be a flexible
function symbol occurring in φ of type S1 × . . . × Sk → S. Let
(a1, . . . , ak ) ∈ SS
k . Recall that S maps sorts to carrier
sets. We define the random variable W X (a1, . . . , ak ) : Ω → {0, 1}
that maps (Auth, I) ∈ Ω to W I (a1, . . . , ak ) ∈ {0, 1}. Similarly, we
define the random variable f X (a1, . . . , ak ) : Ω → SS that maps
(Auth, I) ∈ Ω to f I (a1, . . . , ak ) ∈ SS. We call these random vari-
ables random facts of φ.
□
(cid:17)
Example 3. Let us examine some random facts of the formula
which can take the values 0 and 1, so UAX(cid:16)
RBAC
,
Alice, r1
φ
N
= 1 | Auth(cid:17)
(cid:17)
is a Bernoulli
(cid:17)
I ∈ Ω | UAI(cid:16)
(cid:40)
(cid:17)
UAX(cid:16)
(cid:17)
(cid:40)
(cid:17)
PAX(cid:16)
= 1 | Auth(cid:17)
from Example 2. One such random fact is UAX(cid:16)
(cid:17)
P(cid:16)UAX(cid:16)
P(cid:16)(cid:40)
| Auth(cid:17)
(cid:41)
(cid:41)
(8)
If we set N = 2 and replace each random fact with a Boolean
value, as indicated by Tables 4 and 5, then we get an RBAC policy.
Just like a statement of the form P (X = I | Auth) quantifies how
much we believe that X = I for a given Auth, a statement of the
quantifies how much we be-
lieve that role r1 is assigned to Alice for a given Auth.
□
Observation 1. Since we assume carrier sets to be finite, a random
fact always has a discrete distribution. In particular, random facts
built from flexible relation symbols have Bernoulli distributions as
□
they can only take Boolean values.
= 1
More generally, the set of random facts for φ
random variable whose probability distribution is defined by
form P(cid:16)UAX(cid:16)
| u ∈ U , i ≤ N
| p ∈ P, i ≤ N
u, ri
ri , p
Alice, r1
Alice, r1
Alice, r1
Alice, r1
RBAC
N
RBAC
N
(cid:41)
(7)
(cid:16)
(cid:17)
∪
is
F
φ
=
=
.
.
We usually denote random facts with Fraktur letters f, g, . . . For
a random fact f of the form W X (a1, . . . , ak ), we denote by fI the
Boolean value W I (a1, . . . , ak ). Similarly, when f is of the form
f X (a1, . . . , ak ), we denote by fI the value f I (a1, . . . , ak ). Finally,
we denote f’s range with Range (f).
For a policy language φ ∈ L, we denote by F (φ) the set of all
random facts of φ. Recall that we assume all our carrier sets to be
finite, so F (φ) is finite.
Observe that, for any formula φ ∈ L, replacing each random
fact f in F (φ) with a value in Range (f) yields a policy. Hence, a
policy miner, instead of searching for a policy I, it just searches for
adequate values for all random facts in F (φ). We formalize this in
Lemma 1, whose proof is in the full version.
Session 1D: ForensicsCCS ’19, November 11–15, 2019, London, United Kingdom101.
=
(cid:19)
(9)
(cid:18)(cid:16)
f∈F(φ )
fX(cid:17)
P (I | Auth) = P
avoid cluttered notation, we write h (I) instead of h
We denote by h (·) the function P
(cid:19)
(cid:16)
fI(cid:17)
Lemma 1. For a policy language φ ∈ L,
f∈F(φ ) | Auth
(cid:18)(cid:16)
fX(cid:17)
(cid:18)(cid:16)
fI(cid:17)
= · | Auth
. To
.
f∈F(φ )
We conclude this section by defining some other useful random
variables. Recall that X is the random variable that maps I ∈ Ω to I.
Definition 8. For (u, p) ∈ U × P, φ ∈ L, we define the random
variable φX (u, p) : Ω → {0, 1} as the function mapping (Auth, I)
□
to φI (u, p).
Definition 9. For φ ∈ L, Auth ⊆ U × P, we define the following
random variable:
(10)
(cid:12)(cid:12)(cid:12)Auth(u, p) − φX (u, p)
L (Auth, X; φ) :=
(cid:88)
f∈F(φ )
(cid:12)(cid:12)(cid:12) .
(cid:19)
(u,p)∈U ×P
6.2 Approximating the distribution
A mean-field approximation of the probability distribution h is a
distribution q defined by
(cid:89)
(cid:16)
fI(cid:17)
,
q (I) :=
qf
f∈F(φ )
Hence,(cid:80)
where qf : Range (f) → [0, 1] is a probability mass function for f.
b∈Range(f) qf (b) = 1. For b ∈ Range (f), the value qf (b)
denotes the probability, according to qf, that f = b.
Observe that q (I)’s factorization implies that the set of random
facts is mutually independent. This is not true in general, as h may
not be necessarily factorized like q. This independence assumption
is imposed by mean-field theory to facilitate computations. Our
experimental results in Section 10 show that, despite this approx-
imation, we still mine high quality policies.
According to mean-field theory, the distributions
(cid:16)
(cid:80)
b′∈Range(f) exp
−βEf(cid:55)→b[L (Auth, X; φ)]
that make q best approximate h are given by
(cid:68)qf (b) =
where b ∈ Range (f) and Ef(cid:55)→b[L (Auth, X; φ)] is the expectation of
L (Auth, X; φ) after replacing every occurrence of the random fact f
with b [9]. This expectation is computed using the distribution q.
Therefore,
f(cid:55)→b′[L (Auth, X; φ)]
−βE
(cid:40)(cid:68)qf | f ∈ F (φ)
(cid:17) ,
(12)
exp
(cid:41)
(cid:17)
(cid:16)
□
(11)
(cid:68)qg
Ef(cid:55)→b[L (Auth, X; φ)]
(cid:89)
(cid:88)
=
(cid:16)
gI(cid:17)
I
g∈F(φ )
g(cid:44)f
(L (Auth, I; φ) {f (cid:55)→ b}) .
(13)
Here, L (Auth, I; φ) {f (cid:55)→ b} is obtained from L (Auth, I; φ) by re-
placing f with b.
Using Lemma 1 and the distribution q, we can approximate
arg maxI P (I | Auth) by maximizing q.
Observation 2. maxI P (I | Auth) = maxI h (I) ≈ maxI q (I) .
The desired miner is then an algorithm that computes q, while
letting β → ∞, and then computes the policy I∗ that maximizes q.
7 BUILDING THE POLICY MINER
To compute q, as given by Equation 11, the desired policy miner
could use Equation 12 to compute(cid:68)qf, for each f ∈ F (φ). Observe,
(cid:40)(cid:68)qf | f ∈ F (φ)
(cid:41)
however, that Equation 12 is recursive, since the computation of
the expectations on the right hand side requires
, as
approximates(cid:68)qf [9]. We illustrate this in the step 2a below.
indicated by Equation 13. This recursive dependency is handled
by iteratively computing, for each f ∈ F (φ), a function ˜qf that
Algorithm 1 gives the pseudocode for computing and maximizing
q, which is the essence of the desired policy miner. We give next
an overview.
b ˜qf (b) = 1.
set to an arbitrary function such that(cid:80)
(1) Initialization (lines 2–3). Each distribution ˜qf is randomly
(2) Update loop (lines 4–8). We perform a sequence of itera-
and β. The number T of
tions that update
iterations is fixed before execution.
(a) Parameter update (line 5–7). At each iteration, we com-
pute a random ordering RS(F (φ)) of all the random facts.
Then, for each f in that order, ˜qf is updated to the right-
hand side of Equation 12 (lines 6–7), but instead of using
to compute the
(cid:40)
˜qf | f ∈ F (φ)
(cid:40)
˜qf | f ∈ F (φ)
(cid:40)(cid:68)qf
, we use
(cid:41)
(cid:41)
(cid:41)
| f ∈ F (φ)
expectations.
(b) Hyper-parameter update (line 8). After each iteration,
we increase β by a factor of α, defined before execution.
This approach, originally defined for deterministic anneal-
ing, avoids that the algorithm is trapped in a bad local
maximum in the early iterations [61, 62].
(3) Policy computation (line 9). Finally, we compute the pol-
icy I∗ = arg maxI q (I). By looking at Equation 11, we
see that to maximize q, it suffices to maximize qf, for ev-
ery f ∈ F (φ). Hence, we let I∗ be the policy that satisfies
fI∗ = arg maxb∈Range(f) ˜qf (b).
Algorithm 1: The policy miner.
1 PolicyMiner(L, Auth, φ, α, β,T ):
2
3
4
5
6
for f ∈ F (φ):
for i = 1 . . . T :
(cid:16)
Randomly initialize ˜qf.
for f ∈ RS (F (φ)):
for b ∈ Range (f):
(cid:80)
exp
b′ exp
(cid:17)
(cid:16)