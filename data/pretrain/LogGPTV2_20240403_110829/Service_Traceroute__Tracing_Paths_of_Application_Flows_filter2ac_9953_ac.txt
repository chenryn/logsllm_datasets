### 5.4 Path Differences When Tracing with the Same Flow ID

One possible explanation for the differences observed in the previous section is that the flow ID used by Paris traceroute probes differs from the flow ID of the target application, which can lead to different forwarding decisions in middleboxes that operate on a per-flow basis. In this section, we compare the output of Service traceroute with that of Paris traceroute when both use the same flow ID as the target application.

Note that even in this case, Paris traceroute runs after the target application flow has finished. To ensure a complete match, we use the correct flow ID based on the Service traceroute run just before the Paris traceroute execution.

**Figure 4** compares the paths discovered by Service traceroute and Paris traceroute when using the exact same flow ID as the target application. The results show that Paris traceroute discovers the same path as Service traceroute more frequently than when using MDA: approximately 65% of Twitch sessions, 91% of YouTube sessions, and 93% of web downloads. This indicates that using a different flow ID for probes compared to the target application flow is a significant cause of the observed differences.

The remaining discrepancies can be attributed to three possible causes:
1. **Path changes** that may occur between the runs of Service traceroute and Paris traceroute.
2. **Per-packet load balancing**.
3. **Middleboxes** (such as application-layer proxies or firewalls) that track the state of TCP connections and may drop packets after connections are terminated.

In our initial testing, we noticed cases where probes did not generate any ICMP response if issued after the target application flow had finished. Further examination of the paths for Twitch suggests that about 45% of the differences are unlikely due to routing changes, and we verified that there are no middleboxes dropping our probes (which would appear as stars). Therefore, we conjecture that the differences are likely due to per-packet load balancing, but further experiments are needed to confirm this hypothesis.

### 5.5 Location of Path Divergence Points

To better understand our results, we analyze the location of the points where the paths discovered by Service traceroute and Paris traceroute diverge, which we term "divergence points." **Table 1** shows the fraction of experiments with divergence points at the origin AS, the middle of the path, and the destination AS. This analysis helps explain the results from the previous sections, although the findings are heavily influenced by our vantage points and destinations.

**Table 1: Location of Divergence Points [% of all flows]**

| Application        | Origin | Middle | Destination |
|--------------------|--------|--------|-------------|
| Web pages (TCP)    | 7.33   | 39.82  | 34.37       |
| Twitch (TCP)       | 4.06   | 19.04  | 1.81        |
| YouTube (TCP & UDP)| 4.92   | 44.50  | 0.41        |

For the three applications, when comparing with MDA, most of the divergence points are in the middle (from 19% for YouTube to over 40% for Twitch). Given that the middle contains more hops, it is not surprising that it also contains more divergence points. However, when using Paris traceroute with the same flow ID, the percentage of divergence points in the middle decreases substantially to less than 2% for web and YouTube, and to 24% for Twitch. This result suggests that divergence points in the middle are primarily due to middleboxes that perform per-flow forwarding.

Paris traceroute's MDA discovers all possible interfaces for every hop of the path, and we compare the closest path MDA finds to Service traceroute's output. However, MDA often uses different flow IDs than the target application flow, which may result in a different sequence of hops end-to-end. For Paris traceroute with the same flow ID, we observe more divergence points at the origin, which may indicate path changes. The only exception is Twitch, which still has around 24% of divergence points in the middle. Our analysis of these divergence points shows that half of them are within a single ISP: GTT Communications (AS 3257).

### 6 Related Work

Since Jacobson's original traceroute tool [4], several new versions have emerged with different features and methods for constructing probes, such as Paris traceroute [2,16] and tcptraceroute [15]. These tools have a drawback for diagnosing a target application flow because they start a new flow to send probes, leading to different treatment by middleboxes. Service traceroute avoids this issue by piggybacking traceroute probes within active application flows. This idea was first introduced in paratrace [6], which is no longer available, and later re-implemented in 0trace [5] for tracing through firewalls and in TCP sidecar [13] for reducing complaints of large-scale traceroute probing for topology mapping. Unfortunately, none of these tools is actively maintained.

Service traceroute adds the capability of automatically identifying application flows to trace by a domain name, tracing UDP flows, and tracing multiple concurrent flows that compose a service. We release both a command-line and a library version as open source. Additionally, we present an evaluation of the side effects of piggybacking traceroute probes within application traffic and its benefits by comparing the differences with Paris traceroute and 0Trace. Our characterization reappraises some of the findings from Luckie et al. [8], which show that the discovered paths depend on the protocol used in the probes. Their study, however, does not include traceroute tools that piggyback on application flows.

### 7 Conclusion

In this paper, we present Service traceroute, a tool for tracing paths of flows of modern Internet services by piggybacking TTL-limited probes within target application flows. Our evaluation of paths to popular websites and video services from PlanetLab Europe shows that Service traceroute's probing has no effect on target application flows. Moreover, a typical traceroute tool that launches a new flow to the same destination discovers different paths than when embedding probes in the application flow in a significant fraction of experiments (from 40% to 50% of our experiments), as shown by our comparison with Paris traceroute. When we set Paris traceroute's flow ID to that of the target application flow, the resulting paths are more similar to Service traceroute's. Identifying the flow ID to probe, however, is not trivial. Modern applications rely on a large pool of servers/ports.

Even to run 0Trace, which implements the same idea of piggybacking probes in the application flow, we had to rely on Service traceroute's functionality to identify target application flow IDs to probe. In future work, we plan to add support for IPv6 to Service traceroute and perform a larger-scale characterization of Service traceroute results across a wide variety of services and a larger set of globally distributed vantage points.

### Acknowledgements

This work was supported by the ANR Project No ANR-15-CE25-0013-01 (BottleNet), a Google Faculty Research Award, and Inria through the IPL BetterNet and the associate team HOMENET.

### References

1. Service Traceroute. https://github.com/inria-muse/service-traceroute
2. Augustin, B., et al.: Avoiding traceroute anomalies with Paris traceroute. In: Proceedings of IMC (2006)
3. Banerjee, S., Griffin, T.G., Pias, M.: The interdomain connectivity of PlanetLab nodes. In: Barakat, C., Pratt, I. (eds.) PAM 2004. LNCS, vol. 3015, pp. 73–82. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-24668-8_8
4. Jacobson, V.: Traceroute, February 1989
5. Edge, J.: Tracing behind the firewall (2007). https://lwn.net/Articles/217076/
6. Kaminsky, D.: Parasitic Traceroute via Established TCP Flows & IPID Hopcount. https://man.cx/paratrace
7. Langley, A., et al.: The QUIC transport protocol: design and internet-scale deployment. In: Proceedings of the Conference of the ACM Special Interest Group on Data Communication, pp. 183–196. ACM (2017)
8. Luckie, M., Hyun, Y., Huffaker, B.: Traceroute probe method and forward IP path inference. In: Proceedings of IMC, Vouliagmeni, Greece (2008)
9. Morandi, I.: Service traceroute: tracing paths of application flows. Master thesis, UPMC-Paris 6 Sorbonne Universités (2018). https://hal.inria.fr/hal-01888618
10. Netflix Open Connect Overview. https://openconnect.netflix.com/Open-Connect-Overview.pdf
11. RIPEstat Data API. https://stat.ripe.net/docs/data_api
12. Scheitle, Q., et al.: A long way to the top: significance, structure, and stability of internet top lists. arXiv preprint arXiv:1805.11506 (2018)
13. Sherwood, R., Spring, N.: Touring the internet in a TCP sidecar. In: Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement, pp. 339–344. ACM (2006)
14. Spring, N., Peterson, L., Bavier, A., Pai, V.: Using PlanetLab for network research: myths, realities, and best practices. ACM SIGOPS Oper. Syst. Rev. 40(1), 17–24 (2006)
15. Torren, M.: Tcptraceroute—a traceroute implementation using TCP packets. Man page, UNIX (2001). http://michael.toren.net/code/tcptraceroute
16. Veitch, D., Augustin, B., Friedman, T., Teixeira, R.: Failure control in multipath route tracing. In: Proceedings of IEEE INFOCOM (2009)
17. Google Cloud Overview. https://cloud.google.com/cdn/docs/overview