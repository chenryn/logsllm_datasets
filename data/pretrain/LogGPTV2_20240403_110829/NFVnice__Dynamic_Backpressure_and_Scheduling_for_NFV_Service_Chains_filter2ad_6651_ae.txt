and 64% respectively, while maintaining the aggregate throughput
(0.6 Mpps). This is primarily because of backpressure ensuring that
the upstream NFs only process the correct amount of packets that
the downstream NFs can consume. Excess packets coming into the
chain are dropped at the beginning of the chain. When we use only
the default NORMAL scheduler by itself, NF1 and NF2 use 100% of
the CPU to process a huge number of packets (the ‘service rate’ in
the Table 5), only to be discarded at the downstream NF3.
We now consider two different service chains using 4 cores in the
system. Chain-1 has three NFs: NF1 (270 cycles), NF2 (120 cycles)
and NF4 (300 cycles) running on 3 different cores. Chain-2 comprises
NF1, NF3(4500 cycles) and NF4. The same instances of NF1 and
NF4 are part of both chain-1 and chain-2 as shown in Figure 8.
Moongen generates 64-byte packets at line rate, equally splitting
them between two flows that are assigned to chain-1 and chain-2.
Table 6 shows that in the Default case (NORMAL scheduler), NF1
processes almost an equal number of packets for chain-1 and chain-
2. However, for chain-2, the downstream NF3 discards a majority
of the packets processed by NF1. This results not only in wasted
work, but it also adversely impacts the throughput of chain-1. On
the other hand, with NFVnice, backpressure has the upstream NF1
process only the appropriate number of packets of chain-2 (which
has its bottleneck at the downstream NF, NF3). This frees up the
upstream NF1 to use the remaining processing cycles to process
packets from chain-1. NFVnice improves the throughput of chain-1
by factor of 2. At the same time, it maintains the throughput of
chain-2 at its bottleneck (NF3) rate of 0.6Mpps. Overall, NFVnice not
only avoids wasted work, but judiciously allocates CPU resources
(at upstream NFs) proportionate to the chain’s bottleneck resource
capacity as shown in the Figure 9.
4.3 Salient Features of NFVnice
4.3.1 Variable NF packet processing cost. We now evaluate the
resilience of NFVnice to not only heterogeneity across NFs, but also
Figure 10: Performance of NFVnice in a service chain of
3 NFs with different computation costs and varying per
packet processing costs.
variable packet processing costs within an NF. We use the same
three-NF service chain used in 4.2.1, but modify their processing
costs. Packets of the same flow have varying processing costs of 120,
270 or 550 cycles at each of the NFs. Packets are classified as having
one of these 3 processing costs at each of the NFs, thus yielding 9
different variants for the total processing cost of a packet across the
3 -NF service chain. Figure 10 shows the throughput for different
schedulers. With the Default scheduler, the throughput achieved
differs considerably compared to the case with fixed per-packet
processing costs as seen in Figure 7. For the Default scheduler,
the throughput degrades considerably for the vanilla coarse time-
slice schedulers (BATCH and RR(100ms)), while the NORMAL and
RR(1ms) schedulers achieve relatively higher throughputs. When
examining the throughput with only the CPU weight assignment,
CGroup, we see improvement with the BATCH scheduler, but not as
much with the NORMAL scheduler. This is because the variation in
per-packet processing cost of NFs result in an inaccurate estimate of
the NF’s packet-processing cost and thus an inappropriate weight
assignment and CPU share allocation. This inaccuracy also causes
NFVnice (which combines CGroup and backpressure) to experience
a marginal degradation in throughput for the different schedulers.
Backpressure alone (the Only BKPR case), which does not adjust the
CPU shares based on this inaccurate estimate is more resilient to the
packet-processing cost variation and achieves the best (and almost
the same) throughput across all the schedulers. NFVnice gains
this benefit of backpressure, and therefore, in all cases NFVnice’s
throughput is superior to the vanilla schedulers. We could mitigate
the impact of variable packet processing costs by profiling NFs
more precisely and frequently, and averaging the processing over
a larger window of packets. However, we realize that this can be
expensive, consuming considerable CPU cycles itself. This is where
NFVnice’s use of backpressure helps overcome the penalty from
the variability, getting better throughput and reduced packet loss
compared to the default schedulers.
4.3.2
Service Chain Heterogeneity. We next consider a three NF
chain, but vary the chain configuration—(Low, Medium, High);(High,
Medium, Low); and so on for a total 6 cases—so that the location
of the bottleneck NF in the chain changes in each case. Results in
Figure 11 show significant variance in the behaviour of the vanilla
kernel schedulers. NORMAL and BATCH perform similar to each
other in most cases, except for the small differences for the reasons
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Kulkarni et al.
Figure 11: Throughput for varying combinations of 3 NF service chain with Heterogeneous computation costs
described earlier in Section 2. We also looked at RR with time slices
of 1ms and 100ms, and their performance is vastly different. For
the small time-slice, performance is better when the bottleneck
NF is upstream, while RR with a larger time-slice performs better
when the bottleneck NF is downstream. This is primarily due to
wasted work and inefficient CPU allotment to the contending NFs.
However, with NFVnice, in almost every case, we can see consider-
able improvements in throughput, for all the schedulers. NFVnice
minimizes the wasted cycles independent of the OS scheduler’s
operational time-slice.
Impact of RR’s Time Slices with NFV: Consider the chain con-
figurations “High-Med-Low” and “Med-High-Low” in Figure 11.
RR(100 ms time slice) performs very poorly, with very low through-
put < 40Kpps. This is due to the ‘Fast-producer, slow-consumer’
situation [44], making the NF with “High” computes hog the CPU
resource. Now, in the default RR scheduler, the packets processed
by this NF would be dequeued by the Tx threads but will be sub-
sequently dropped, as the next NF in the chain does not get an
adequate share of the CPU to process these packets. The upstream
NF that is hogging the CPU has to finish its time slice and the OS
scheduler then causes a involuntary context switch for this “High”
NF. However, with NFVnice, the queue buildup results in generat-
ing a backpressure signal across the chain, forcing the upstream
NF to be evicted ( i.e., triggering a voluntary context switch) from
the CPU as soon as the downstream NFs buffer levels exceed the
high watermark threshold. The upstream NF will not execute till
the downstream NF gets to consume and process its receive buffers.
Thus, NFVnice is able to enforce judicious access to the CPU among
the competing NFs of a service chain. We see in every case in Fig-
ure 11, NFVnice’s throughput is superior to the vanilla scheduler,
emphasizing the point we make in this paper: NFVnice’s design
Figure 12: Throughput (Mpps) with varying workload mix,
random initial NF for each flow in a 3 NF service chain (ho-
mogeneous computation costs)
can support a number of different kernel schedulers, effectively
support heterogeneous service chains and still provide superior
performance (throughput, packet loss).
4.3.3 Workload Heterogeneity. We next use 3 homogeneous
NF’s with the same compute cost, but vary the nature of the incom-
ing packet flows so that the three NFs are traversed in a different
order for each flow. We increase the number of flows (each with
equal rate) arriving from 1 to 6, as we go from Type 1 to Type 6,
with each flow going through all 3 NFs in a random order. Thus,
the bottleneck for each flow is different. Figure 12, shows that the
native schedulers (first four bars) perform poorly, with degraded
throughput as soon as we go to two or more flows, because of the
different bottleneck NFs. However, NFVnice performs uniformly
better in every case, and is almost independent of where the bot-
tlenecks are for the multiple flows. Moreover, NFVnice provides
NFVnice: Dynamic Backpressure and Scheduling for NF Chains
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Figure 13: Benefit of Backpressure with mix of responsive
and non-responsive flows, 3 NF chain, heterogeneous com-
putation costs
a substantial improvement and robustness to varying loads and
bottlenecks even across all the schedulers (NORMAL, BATCH, RR
with 1ms or 100 ms slice.)
4.3.4 Performance isolation. It is common to observe that when
there are responsive (TCP) flows that share resources with non-
responsive (UDP) flows, there can be a substantial degradation
of TCP performance, as the congestion avoidance algorithms are
triggered causing it to back-off. This impact is exacerbated in a
software-based environment because resources are wasted by the
non-responsive UDP flows that see a downstream bottleneck, re-
sulting in packets being dropped at that downstream NF. These
wasted resources result in less capacity being available for TCP.
Because of the per-flow backpressure in NFVnice, we are able to
substantially correct this undesirable situation and protect TCP’s
throughput even in the presence of non-responsive UDP.
In this experiment, we generate TCP and UDP flows with Iperf3.
One TCP flow goes through only NF1 (Low cost) and NF2 (Medium
cost) on a shared core. 10 UDP flows share NF1 and NF2 with the
TCP flow, but also go through an additional NF3 (High cost, on a
separate core) which is the bottleneck for the UDP flows - limiting
their total rate to 280 Mbps.
We first start the 1 TCP flow. After 15 seconds, 10 UDP flows start,
but stop at 40 seconds. As soon as the UDP flows interfere with the
TCP flow, there is substantial packet loss without NFVnice, because
NF1 and NF2 see contention from a large amount of UDP packets
arriving into the system, getting processed and being thrown away
at the queue for NF3. The throughput for the TCP flow craters from
nearly 4 Gbps to just around 10-30 Mbps (note log scale), while the
total UDP rate essentially keeps at the bottleneck NF3’s capacity of
280 Mbps. With NFVnice, benefiting from per-flow backpressure,
the TCP flow sees much less impact (dropping from 4 Gbps to about
3.3 Gbps), adjusting to utilize the remaining capacity at NF1 and NF2.
This is primarily due to NFVnice’s ability to perform selective early
discard of the UDP packets because of the backpressure. Otherwise
we would have wasted CPU cycles at NF1 and NF2, depriving the
TCP flow of the CPU. Note that the UDP flows’ rate is maintained at
the bottleneck rate of 280 Mbps as shown in Figure 13 (UDP lines are
one on top of the other). Thus, NFVnice ensures that non-responsive
flows (UDP) do not unnecessarily steal the CPU resources from
other responsive (TCP) flows in an NFV environment.
Figure 14: Improvement in Throughput with NFs perform-
ing Asynchronous I/O writes withNFVnice
4.3.5 Efficient I/O handling by NFVnice. It is important for NFs
to be able to perform I/O required by the packet of a flow, while
efficiently continuing to process other flows (e.g., packet monitors,
proxies, etc.). Using Moongen we send 2 flows at line rate. Both
the flows share the same upstream NFs, but only one of the flows
performs I/O i.e., logs the packets to the disk using NFVnice’s I/O
library. Figure 14 compares the aggregate throughput achieved with
and without NFVnice, using the BATCH scheduler in the kernel.
We vary the packet size. NFVnice maintains a higher throughput
consistently, even for small packet sizes. Moreover, NFVnice main-
tains progress on the second flow while I/O is being performed for
packets of the first flow, thus providing better isolation.
4.3.6 Dynamic CPU Tuning and fairness. Dynamic CPU tuning:
NFVnice dynamically adjusts the CPU allocations based on the
packet processing cost and arrival rate for each NF. Two NFs initially
with different computation costs (ratio 1:3) run on the same core,
with MoonGen transmitting a flow each to the two NFs at the same
rate. To demonstrate adaptation, we have the computation cost of
NF1 temporarily increase 3 times(to the same level as NF2) during
the 31 sec. to 60 sec. interval.
Figure 15a has the default NORMAL scheduler evenly allocating
the CPU between NF1 and NF2 regardless of their computation cost
throughout. On the other hand, NFVnice allocates NF2 three times
the CPU as NF1 initially. At t=30s, NFVnice allocates each NF half
of the CPU. And at t=60s, we go back to the original allocation. We
observed that the throughput for the two flows (not shown) is equal
throughout, indicating the capability of NFVnice to dynamically
provide a fair allocation of resources factoring in the heterogeneity
of the NF CPU compute cost.
Fairness measure: We evaluate the fairness in throughput as we
increase the diversity of computation for each of the NFs for default
CFS scheduler and NFVnice. We vary the number of NFs sharing
the core. Each NF has the same packet arrival rate, but different
computation cost. At diversity level 1, we start with a single flow
(uses NF1, compute cost 1). With a diversity level of two, we have 2
flows, flow 1 uses NF1 (compute cost 1), flow 2 uses NF2 (compute
cost 2). At a diversity level of 6, there are 6 NFs, with the ratio
of computation costs of 1:2:5:20:40:60, and one flow each going to
the corresponding NF. At diversity level 6, the NORMAL scheduler
allocates 16.6% of the CPU to each of the NFs, being unaware of
the computation cost of each NF. Thus, the throughput for flow 1 is
1.02 Mpps, while flow 6 is only 0.07 Mpps. With NFVnice, the CPU
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Kulkarni et al.
(a) Effect of Dynamic CPU Weight Updates
(b) Measure of Fairness
(c) Effect of rate-cost proportional fairness on
CPU Utilization and Throughput
Figure 15: Adaptation to Dynamic Load and Fairness measure of NFVnice compared with the NORMAL scheduler
count for different configurations. We use a 3 NF, “Low-Med-High”
service chain, and use Pktgen to generate line rate minimum packet
size traffic. We begin with a fixed ’margin’ (difference between the
High and Low thresholds). With the margin at 30, we vary the
high threshold. Below 70%, the throughput starts to drop (under-
utilization), while above 80% the number of packet drops at the
upstream NFs increases (insufficient buffering). We then varied the
NF service chain length (from 2 to 6), and computation costs (per
packet processing cost from 100 cycles to 10000 cycles) to see the
impact of setting the water marks. Across all these cases, we ob-
served that a choice of 80% for the HIGH_WATER_MARK worked
’well’. With the high water mark fixed at 80%, we varied the LOW_
WATER_MARK, by varying the margin. With a very small margin
(1 to 5), packet drops increased, while a margin above 30 degraded
throughput. We chose a margin of 20 because it provided the best
performance across these experiments. We acknowledge that these
watermark levels and thresholds are sensitive to overall path-delay,
chain length and processing costs of the NFs in the chain, and that
these parameters are necessarily an engineering compromise.
Periodic profiling and CPU weight assignment granularity: We
based our frequency of CPU profiling based on the overheads of
rdtsc (observed to be roughly 50 clock cycles) and average time to
write to the cgroup virtual file system (5 µ seconds). We discard the
first 10 samples to effectively account for warming the cache and
to eliminate outliers.
5 RELATED WORK
NF Management and Scheduling: In recent years, several NFV plat-
forms have been developed to accelerate packet processing on
commodity servers [4, 21, 23, 32, 43]. There is a growing interest in
managing and scheduling network functions. Many works address
the placement of middleboxes and NFs for performance target or
efficient resource usage [16, 25, 30, 39, 41, 46]. For example, E2 [39]
builds a scalable scheduling framework on top of BESS [21]. They
abstract NF placement as a DAG, dynamically scale and migrate
NFs while keeping flow affinity. NFV-RT [30] defines deadlines
for requests, and places or migrates NFs to provide timing guar-
antees. These projects focus on NF management and scheduling
across cluster scale. Our work focuses on a different scale: how
to schedule NFs on shared cores to achieve fairness when flows
have load pressure. Different from traditional packet scheduling for
fairness on hardware platforms [18, 47, 49, 50], software-based NFs
or middleboxes are more complex, resulting in diversity of packet
Figure 16: Performance of NFVnice for different NF service
chain lengths.
allocated to the lightweight NF is 1%, while the heavyweight NF gets
46%, and all the flows achieve nearly equal throughput 15c). Using
Jain’s fairness index [24], we show that the vanilla scheduler is
dramatically unfair (going down to 0.62) while NFVnice consistently
achieves fair throughout (Jain’s fairness index of 1.0) as shown in
figure 15b).
4.3.7
Supporting longer NF chains. We now see how well NFVnice
can support longer NF service chains. We choose three different
NFs, as in 4.2, and increase the chain length from 1 NF up to a
chain of 10 NFs, including one of the 3 NFs each time. We examine
two cases: (i) all the NFs of the chain are on a single core (denoted
by SC); and (ii) three cores are used, and as the chain length is
increased, the additional NF is placed on the next core in round-