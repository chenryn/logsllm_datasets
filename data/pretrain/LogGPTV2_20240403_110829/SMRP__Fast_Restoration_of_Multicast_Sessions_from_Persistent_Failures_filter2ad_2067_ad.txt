0.4
0.45
0.25
Dthresh
(b) performance penalty
Figure 8. The effect of Dthresh
PIM), which we call global detour for ease of exposition.
In the simulation, we we set the value of N, NG, α, and
Dthresh to 100, 30, 0.2, and 0.3, respectively. Five network
topologies are randomly generated by GT-ITM and in each
topology, the group of multicast members is also randomly
selected. In Figure 7, the x-axis and the y-axis represent the
recovery distance via global detour and local detour. For
each multicast member R, we consider the worst case for
R’s recovery in which the link closest to the source node
on R’s multicast path (i.e., the incident link of S towards
R) fails. This situation represents the worst situation for R
since the failure disables the largest portion of the multi-
cast tree Each asterisk point in the ﬁgure indicates the sim-
ulation result for one member in each randomly-generated
topology. As shown in the ﬁgure, most points are below the
line y = x, meaning that the recovery path via local detour is
shorter than the recovery path via global detour. Overall, we
observe that the length of the recovery path via local detour
is reduced by an average of 33%.
4.3.2 Threshold Dthresh
Next, we explore how parameter Dthresh affects the protocol
performance with respect to the evaluation metrics. All pa-
rameters except Dthresh are ﬁxed and the values of N, NG and
α are 100, 30, and 0.2, respectively. Four values of Dthresh
are tested. Under each test, ten network topologies are ran-
domly generated by GT-ITM and in each topology, ten dif-
ferent sets of multicast members are also randomly selected.
Each of these 100 simulation scenarios is tested for SMRP
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:13 UTC from IEEE Xplore.  Restrictions apply. 
relative
Maximum RDR
Average RDR
relative
relative
Maximum RDR
Average RDR
relative
100
90
80
70
60
50
40
30
20
10
t
e
g
a
n
e
c
r
e
p
0.15
(3.3)
0.2
(4.1)
α
0.25
(4.9)
0.3
(5.9)
(a) performance improvement
0
15
20
25
30
40
45
50
55
35
NG
(a) performance improvement
t
e
g
a
n
e
c
r
e
p
e
g
a
t
n
e
c
r
e
p
100
90
80
70
60
50
40
30
20
10
0
40
35
30
25
20
15
10
5
0
relative
relative
Maximum RDR
Average RDR
CostT
relative
40
35
30
25
20
15
10
5
t
e
g
a
n
e
c
r
e
p
0
15
20
25
30
40
45
50
55
35
NG
(b) performance penalty
Figure 10. The effect of NG
4.3.3 Average Node Degree α
In this simulation, we explore the impact of the average
node degree in the network on the performance achieved by
SMRP. As described earlier, the average node degree of the
topology can be tuned by α. We ﬁx the values of N, NG, and
Dthresh to 100, 30, and 0.3, respectively, and compare the re-
sults under four different α values, 0.15, 0.2, 0.25, and 0.3.
During each test, one hundred scenarios are generated in
which SMRP and SPF-based protocols are examined. Fig-
ure 9 shows SMRP’s relative performance against the SPF
algorithms. The number under each α value indicates the
corresponding average node degree in the network.
• In Figure 9, we observe that the performance improve-
ment diminishes slightly as α (i.e., the node degree) in-
creases. In a network with low connectivity, the multi-
cast tree established by SPF-based algorithms tends to
have serious link/node concentration, and hence, the
deployment of SMRP makes more performance im-
provement by lowering path sharing in the multicast
tree.
• An acceptable performance improvement can still be
achieved in networks with high connectivity. Further
study shows, even when average node degree goes up
to 10, SMRP achieves 12% path length reduction at the
expense of 5% performance penalty.
relative
relative
Maximum DR
Average DR
relative
CostT
0.15
(3.3)
0.2
(4.1)
α
0.25
(4.9)
0.3
(5.9)
(b) performance penalty
Figure 9. The effect of α
and SPF protocols separately, and the performance com-
parison is plotted in Figure 8. The error bars in the ﬁgure
represent the 95% conﬁdence intervals with the associated
metrics. Similarly as Section 4.3.1, the worst case for each
member’s recovery is examined. According to the deﬁni-
tion of the three evaluation metrics, RDrelative
indicates how
much SMRP further accelerates the service recovery while
Drelative
measure the performance penalty in
R
terms of increased end-to-end delay and tree cost, respec-
tively. The following characteristics of SMRP can be ob-
served in Figure 8.
and Costrelative
R
T
• A fairly large improvement is made by SMRP with
a moderate amount of overhead. For example, when
Dthresh is 0.3, the length of the recovery path is reduced
by an average of 20% in SMRP with only 5% perfor-
mance penalty in terms of increased end-to-end delay
or tree cost.
• The performance improvement increases linearly with
the parameter Dthresh while more penalties are induced,
illustrating the basic property of the new protocol.
SMRP trades away the communication efﬁciency (e.g.,
end-to-end delay) for the decreased path sharing in the
multicast tree (i.e., the increased possibility of fast ser-
vice recovery via a local assistant). The introduction of
parameter Dthresh enables a ﬁne control of the protocol
so that it can be applied to a variety of applications
with different fault-tolerance preferences.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:13 UTC from IEEE Xplore.  Restrictions apply. 
4.3.4 Group Size NG
The effect of group size NG on SMRP’s performance is ex-
amined using a similar procedure. All tunable parameters
except for NG are ﬁxed (N = 100,α = 0.2,Dthresh = 0.3)
and the value of NG is varied to 20, 30, 40, and 50. The
simulation results are plotted in Figure 10 and summarized
as follows.
• The performance is maintained steadily with the
change of NG. SMRP outperforms SPF-based algo-
rithms with respect to the recovery distance, and the
path is shortened by an average of 20%. The overhead
incurred by SMRP remains at about 5%.
• With the increase of group size, we observe a slight
decrease of the performance improvement in terms of
average RDrelative
. It is possibly because in the same
network topology, a larger group makes each member
have more close neighbors and SMRP’s advantage di-
minishes.
R
5 Conclusion and Future Work
With the objective of increasing the likelihood of suc-
cessful local multicast service recovery, we have proposed
a new multicast routing protocol, called SMRP, to construct
a multicast tree with less node/link sharing. During the path
selection, parameter Dthresh can be used to make the con-
trolled tradeoff between the recovery distance and commu-
nication overhead in terms of end-to-end delay and tree cost.
Our in-depth simulation demonstrates the merits of the pro-
posed protocol. For example, in one simulation setup, the
recovery path for each receiver is reduced, on average, by
20% with about 5% overhead. SMRP provides a good op-
tion for the multicast applications with different quality-of-
service (especially fault-tolerance) preferences.
In our ongoing work, we are conducting more compre-
hensive evaluation of the protocol by comparing it against
other multiples algorithms proposed recently. Meanwhile,
we are collecting Internet’s topology to evaluate SMRP’s
applicability to real networks.
References
[1] A. Reddy, R. Govindan, and D. Estrin. Fault Isolation in
In Proc. ACM SIGCOMM, pages 29–40,
Multicast Trees.
Stockholm, Sweden, Aug. 2000.
[2] B. M. Waxman. Routing of Multipoint Connections. IEEE
Journal on Selected Areas in Communications, 6(9):1617–
1622, 1988.
[3] C. Labovitz, R. Malan, and F. Jahanian. Internet Routing In-
stability. IEEE/ACM Transactions on Networking, 6(5):515–
558, Oct. 1998.
[4] C. Labovitz, R. Malan, and F. Jahanian. Origins of Internet
Routing Instability. In Proc. IEEE INFOCOM ’99, volume 1,
pages 21–25, Mar. 1999.
[5] D. Estrin, D. Farinacci, A. Helmy, D. Thaler, S. Deering, M.
Handley, V. Jabobson, C. Liu, P. Sharma, and L. Wei. Proto-
col Independent Mutlicast-Sparse Mode (PIM-SM): Procotol
Speciﬁcation. IETF RFC 2362, Jun. 1998.
[6] D. Li and D. R. Cheriton. OTERS (On-Tree Efﬁcient Re-
covery using Subcasting): A Reliable Multicast Protocol. In
Proc. IEEE International Conference on Network Protocols
(ICNP’98), pages 237–245, Oct. 1998.
[7] E. W. Zegura, K. Calvert and M. J. Donahoo. A quantita-
tive comparison of graph-based models for internet topology.
IEEE/ACM Transactions on Networking, 5(6), Dec. 1997.
[8] H. W. Holbrook and D. R. Cheriton. IP Multicast Channels:
EXPRESS Support for Large-scale Single-source Applica-
tions.
In Proc. ACM SIGCOMM ’99, pages 65–78, Cam-
bridge, MA, Aug. 1999.
[9] J. Moy. Multicast Extensions to OSPF.
IETF RFC 1584,
Mar. 1994.
[10] J. Moy. OSPF Version 2. IETF RFC 2328, Apr. 1998.
[11] K. Calvert and E. Zegura. GT-ITM: Georgia Tech in-
ternetwork topology models. http://www.cc.gatech.edu/fac/
Ellen.Zegura/gt-itm/gt-itm.tar.gz, 1996.
[12] K. P. Birman, M. Hayden, O. Ozkasap, Z. Xiao, M. Budiu,
and Y. Minsky. Bimodal Multicast. ACM Transactions on
Computer Systems, 17(2):41–88, 1999.
[13] L. Wei and D. Estrin. The Trade-offs of Multicast Trees and
Algorithms. In Proc. ICCCN ’94, San Francisco, CA, Sep.
1994.
[14] M. Handley, I. Kouvelas, T. Speakman, and L. Vicisano.
Bi-directional Protocol Independent Multicast (BIDIR-PIM).
IETF Draft, Jun. 2003.
[15] M. J. Lin and K. Marzullo. Directional Gossip: Gossip in
In Proc. of European Dependable
a Wide Area Network.
Computing Conference (EDCC-3), 2000.
[16] M. Medard, S. G. Finn, R. A. Barry, and R. G. Gallager. Re-
dundant Trees for Preplanned Recovery in Arbitrary Vertex-
Redundant or Edge-Redundant Graphs. IEEE/ACM Transac-
tions on Networking, 7(5):641–652, Oct. 1999.
[17] R. Yavatkar, J. Grifﬁoen, and M. Sudan. A Reliable Dissemi-
nation Protocol for Interactive Collaborative Applications. In
Proc. ACM MULTIMEDIA’95, pages 333–344, Nov. 1995.
[18] S. Banerjee, B. Bhattacharjee, and C. Kommareddy. Scal-
able Application Layer Multicast. In Proc. ACM SIGCOMM,
pages 205–220, Pittsburgh, PA, Sep. 2002.
[19] S. Banerjee, S. Lee, B. Bhattacharjee, and A. Srinivasan. Re-
silient Multicast using Overlays. Proc. ACM SIGMETRICS
’03, pages 102–113, Jun. 2003.
[20] S. D. Nikolopoulos, A. Pitsillides and D. Tipper. Address-
ing Network Survivability Issues by Finding the K-best Paths
through a Trellis Graph. In Proc. IEEE INFOCOM ’97, vol-
ume 1, pages 370–377, Kobe, Japan, Jun. 1997.
[21] S. Floyd, V. Jacobson, C. Liu, S. McCanne and L. Zhang.
A Reliable Multicast Framework for Light-Weight Sessions
and Application Level Framing. IEEE/ACM Transactions on
Networking, 5(6):784–803, Dec. 1997.
[22] S. Han and K. G. Shin. Fast Restoration of Real-Time Com-
munication Service From Component Failures in Multihop
Networks.
In Proc. ACM SIGCOMM ’97, pages 77–88,
Cannes, France, Sep. 1997.
[23] S. Paul, K. Sabnani, J. Lin, and S. Bhattacharyya. Reliable
Multicast Transport Protocol (RMTP). IEEE Journal on Se-
lected Areas in Communications, 15(3):407–421, Apr. 1997.
ns2.
[24] UCB/LBNL/VINT.
Simulator
Network
http://www.isi.edu/nsnam/ns/index.html, Mar. 2002.
[25] X. Wang, C. Yu, H. Schulzrinne, P. Stirpe, and W. Wu. IP
Multicast Fault Recovery in PIM over OSPF. In 8th Interna-
tional Conference on Network Protocols (ICNP’2000), Os-
aka, Japan, Nov. 2000.
–
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:13 UTC from IEEE Xplore.  Restrictions apply.