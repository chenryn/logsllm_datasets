6.2.5 Mailboxes
6.2.6 Multi-Core Concurrency
Sanctum’s software attestation process relies on mail-
boxes, which are a simpliﬁed version of SGX’s local attes-
tation mechanism. We could not follow SGX’s approach
The security monitor is highly concurrent, with ﬁne-
grained locks. API calls targeting two different enclaves
may be executed in parallel on different cores. Each
868  25th USENIX Security Symposium 
USENIX Association
12
DRAM region has a lock guarding that region’s metadata.
An enclave is guarded by the lock of the DRAM region
holding its metadata. Each thread metadata structure also
has a lock guarding it, which is acquired when the struc-
ture is accessed, but also while the metadata structure
is used by a core running enclave code. Thus, the enter
enclave call acquires a slot lock, which is released by an
enclave exit call or by an AEX.
We avoid deadlocks by using a form of optimistic lock-
ing. Each monitor call attempts to acquire all the locks it
needs via atomic test-and-set operations, and errors with
a concurrent call code if any lock is unavailable.
6.3 Enclave Eviction
General-purpose software can be enclaved without source
code changes, provided that it is linked against a runtime
(e.g., libc) modiﬁed to work with Sanctum. Any such
runtime would be included in the TCB.
The Sanctum design allows the operating system to
over-commit physical memory allocated to enclaves, by
collaborating with an enclave to page some of its DRAM
regions to disk. Sanctum does not give the OS visibility
into enclave memory accesses, in order to prevent private
information leaks, so the OS must decide the enclave
whose DRAM regions will be evicted based on other
activity, such as network I/O, or based on a business
policy, such as Amazon EC2’s spot instances.
Once a victim enclave has been decided, the OS asks
the enclave to block a DRAM region (cf. Figure 13),
giving the enclave an opportunity to rearrange data in
its RAM regions. DRAM region management can be
transparent to the programmer if handled by the enclave’s
runtime. The presented design requires each enclave to
always occupy at least one DRAM region, which contains
enclave data structures and the memory management code
described above. Evicting all of a live enclave’s memory
requires an entirely different scheme that is deferred to
future work.
The security monitor does not allow the OS to forcibly
reclaim a single DRAM region from an enclave, as do-
ing so would leak memory access patterns. Instead, the
OS can delete an enclave, after stopping its threads, and
reclaim all its DRAM regions. Thus, a small or short-
running enclave may well refuse DRAM region manage-
ment requests from the OS, and expect the OS to delete
and restart it under memory pressure.
To avoid wasted work, large long-running enclaves
may elect to use demand paging to overcommit their
DRAM, albeit with the understanding that demand paging
leaks page-level access patterns to the OS. Securing this
mechanism requires the enclave to obfuscate its page
faults via periodic I/O using oblivious RAM techniques,
as in the Ascend processor [20], applied at page rather
than cache line granularity, and with integrity veriﬁcation.
This carries a high overhead: even with a small chance
of paging, an enclave must generate periodic page faults,
and access a large set of pages at each period. Using an
analytic model, we estimate the overhead to be upwards
of 12ms per page per period for a high-end 10K RPM
drive, and 27ms for a value hard drive. Given the number
of pages accessed every period grows with an enclave’s
data size, the costs are easily prohibitive. While SSDs
may alleviate some of this prohibitive overhead, and will
be investigated in future work, currently Sanctum focuses
on securing enclaves without demand paging.
Enclaves that perform other data-dependent communi-
cation, such as targeted I/O into a large database ﬁle, must
also use the periodic oblivious I/O to obfuscate their ac-
cess patterns from the operating system. These techniques
are independent of application business logic, and can be
provided by libraries such as database access drivers.
7 Security Argument
Our security argument rests on two pillars: the enclave
isolation enforced by the security monitor, and the guar-
antees behind the software attestation signature. This
section outlines correctness arguments for each of these
pillars.
Sanctum’s isolation primitives protect enclaves from
outside software that attempts to observe or interact with
the enclave software via means outside the interface pro-
vided by the security monitor. We prevent direct attacks
by ensuring that the memory owned by an enclave can
only be accessed by that enclave’s software. More subtle
attacks are foiled by also isolating the structures used to
access the enclave’s memory, such as the enclave’s page
tables and the caches that hold enclave data.
7.1 Protection Against Direct Attacks
The correctness proof for Sanctum’s DRAM isolation can
be divided into two sub-proofs that cover the hardware
and software sides of the system. First, we need to prove
that the page walker modiﬁcations described in § 5.2 and
§ 5.3 behave according to their descriptions. Thanks to
the small sizes of the circuits involved, this sub-proof
can be accomplished by simulating the circuits for all
logically distinct input cases. Second, we must prove
that the security monitor conﬁgures Sanctum’s extended
page walker registers in a way that prevents direct attacks
on enclaves. This part of the proof is signiﬁcantly more
complex, but it follows the same outline as the proof for
SGX’s memory access protection presented in [13].
The proof revolves around a main invariant stating that
all TLB entries in every core are consistent with the pro-
gramming model described in § 4. The invariant breaks
down into three cases that match [13], after substituting
DRAM regions for pages.
USENIX Association  
25th USENIX Security Symposium  869
13
7.2 Protection Against Subtle Attacks
Sanctum also protects enclaves from software attacks that
attempt to exploit side channels to obtain information
indirectly. We focus on proving that Sanctum protects
against the attacks mentioned in § 2, which target the page
fault address and cache timing side-channels.
The proof that Sanctum foils page fault attacks is cen-
tered around the claims that each enclave’s page fault han-
dler and page tables and page fault handler are isolated
from all other software entities on the computer. First,
all the page faults inside an enclave’s EVRANGE are
reported to the enclave’s fault handler, so the OS cannot
observe the virtual addresses associated with the faults.
Second, page table isolation implies that the OS cannot
access an enclave’s page tables and read the access and
dirty bits to learn memory access patterns.
Page table isolation is a direct consequence of the claim
that Sanctum correctly protects enclaves against direct
attacks, which was covered above. Each enclave’s page
tables are stored in DRAM regions allocated to the en-
clave, so no software outside the enclave can access these
page tables.
The proof behind Sanctum’s cache isolation is straight-
forward but tedious, as there are many aspects involved.
We start by peeling off the easier cases, and tackle the
most difﬁcult step of the proof at the end of the section.
Our design assumes the presence of both per-core caches
and a shared LLC, and each cache type requires a sep-
arate correctness argument. Per-core cache isolation is
achieved simply by ﬂushing per-core caches at every tran-
sition between enclave and non-enclave mode. To prove
the correctness of LLC isolation, we ﬁrst show that en-
claves do not share LLC lines with outside software, and
then we show that the OS cannot indirectly reach into an
enclave’s LLC lines via the security monitor.
Showing that enclaves do not share LLC lines with out-
side software can be accomplished by proving a stronger
invariant that states at all times, any LLC line that can
potentially cache a location in an enclave’s memory can-
not cache any location outside that enclave’s memory. In
steady state, this follows directly from the LLC isolation
scheme in § 5.1, because the security monitor guarantees
that each DRAM region is assigned to exactly one enclave
or to the OS.
Last, we focus on the security monitor, because it is the
only piece of software outside an enclave that can access
the enclave’s DRAM regions. In order to claim that an
enclave’s LLC lines are isolated from outside software,
we must prove that the OS cannot use the security mon-
itor’s API to indirectly modify the state of the enclave’s
LLC lines. This proof is accomplished by considering
each function exposed by the monitor API, as well as
the monitor’s hardware fault handler. The latter is con-
sidered to be under OS control because in a worst case
scenario, a malicious OS could program peripherals to
cause interrupts as needed to mount a cache timing attack.
7.3 Operating System Protection
Sanctum protects the operating system from direct attacks
against malicious enclaves, but does not protect it against
subtle attacks that take advantage of side-channels. Our
design assumes that software developers will transition all
sensitive software into enclaves, which are protected even
if the OS is compromised. At the same time, a honest
OS can potentially take advantage of Sanctum’s DRAM
regions to isolate mutually mistrusting processes.
Proving that a malicious enclave cannot attack the host
computer’s operating system is accomplished by ﬁrst
proving that the security monitor’s APIs that start exe-
cuting enclave code always place the core in unprivileged
mode, and then proving that the enclave can only access
OS memory using the OS-provided page tables. The ﬁrst
claim can be proven by inspecting the security monitor’s
code. The second claim follows from the correctness
proof of the circuits in § 5.2 and § 5.3. Speciﬁcally, each
enclave can only access memory either via its own page
tables or the OS page tables, and the enclave’s page tables
cannot point into the DRAM regions owned by the OS.
These two claims effectively show that Sanctum en-
claves run with the privileges of their host application.
This parallels SGX, so all arguments about OS security
in [13] apply to Sanctum as well. Speciﬁcally, malicious
enclaves cannot DoS the OS, and can be contained using
the mechanisms that currently guard against malicious
user software.
7.4 Security Monitor Protection
The security monitor is in Sanctum’s TCB, so the system’s
security depends on the monitor’s ability to preserve its
integrity and protect its secrets from attackers. The moni-
tor does not use address translation, so it is not exposed
to any attacks via page tables. The monitor also does
not protect itself from cache timing attacks, and instead
avoids making any memory accesses that would reveal
sensitive information.
Proving that the monitor is protected from direct attacks
from a malicious OS or enclave can be accomplished in
a few steps. First, we invoke the proof that the circuits
in § 5.2 and § 5.3, are correct. Second, we must prove
that the security monitor conﬁgures Sanctum’s extended
page walker registers correctly. Third, we must prove that
the DRAM regions that contain monitor code or data are
always allocated to the OS.
Since the monitor is exposed to cache timing attacks
from the OS, Sanctum’s security guarantees rely on proofs
that the attacks would not yield any information that the
OS does not already have. Fortunately, most of the secu-
870  25th USENIX Security Symposium 
USENIX Association
14
rity monitor implementation consists of acknowledging
and verifying the OS’ resource allocation decisions. The
main piece of private information held by the security
monitor is the attestation key. We can be assured that the
monitor does not leak this key, as long as we can prove
that the monitor implementation only accesses the key
when it is provided to the signing enclave (§ 6.1.2), that
the key is provided via a data-independent memory copy
operation, such as memcpy, and that the attestation key is
only disclosed to the signing enclave.
7.5 The Security of Software Attestation
The security of Sanctum’s software attestation scheme
depends on the correctness of the measurement root and
the security monitor. mroot’s sole purpose is to set up
the attestation chain, so the attestation’s security requires
the correctness of the entire mroot code. The monitor’s
enclave measurement code also plays an essential role in
the attestation process, because it establishes the identity
of the attested enclaves, and is also used to distinguish be-
tween the signing enclave and other enclaves. Sanctum’s
attestation also relies on mailboxes, which are used to se-
curely transmit attestation data from the attested enclave
to the signing enclave.
8 Performance Evaluation
While we propose a high-level set of hardware and soft-
ware to implement Sanctum, we focus our evaluation on
the concrete example of a 4-core RISC-V system gener-
ated by Rocket Chip [29]. Sanctum conveniently isolates
concurrent workloads from one another, so we can exam-
ine its overhead via individual applications on a single
core, discounting the effect of other running software.
8.1 Experiment Design
We use a Rocket-Chip generator modiﬁed to model Sanc-
tum’s additional hardware (§ 5) to generate a 4-core 64-bit
RISC-V CPU. Using a cycle-accurate simulator for this
machine, coupled with a custom Sanctum cache hierarchy
simulator, we compute the program completion time for
each benchmark, in cycles, for a variety of DRAM region
allocations. The Rocket chip has an in-order single issue
pipeline, and does not make forward progress on a TLB or
cache miss, which allows us to accurately model a variety
of DRAM region allocations efﬁciently.
We use a vanilla Rocket-Chip as an insecure baseline,
against which we compute Sanctum’s overheads. To pro-
duce the analysis in this section, we simulated over 250
billion instructions against the insecure baseline, and over
275 billion instructions against the Sanctum simulator.
We compute the completion time for various enclave con-
ﬁgurations from the simulator’s detailed event log.
Our cache hierarchy follows Intel’s Skylake [23] server
models, with 32KB 8-way set associative L1 data and
instruction caches, 256KB 8-way L2, and an 8MB 16-
way LLC partitioned into core-local slices. Our cache hit
and miss latencies follow the Skylake caches. We use a
simple model for DRAM accesses and assume unlimited
DRAM bandwidth, and a ﬁxed cycle latency for each
DRAM access. We also omit an evaluation of the on-chip
network and cache coherence overhead, as we do not
make any changes that impact any of these subsystems.
Using the hardware model above, we benchmark the
integer subset of SPECINT 2006 [3] benchmarks (unmod-
iﬁed), speciﬁcally perlbench, bzip2, gcc, mcf, gobmk,
hmmer, sjeng, libquantum, h264ref, omnetpp, and
astar base. This is a mix of memory and compute-
bound long-running workloads with diverse locality.
We simulate a machine with 4GB of memory that is
divided into 64 DRAM regions by Sanctum’s cache ad-
dress indexing scheme. Scheduling each benchmark on
Core 0, we run it to completion, while the other cores are
idling. While we do model its overheads, we choose not
to simulate a complete Linux kernel, as doing so would in-
vite a large space of parameters of additional complexity.
To this end, we modify the RISC-V proto kernel [48] to
provide the few services used by our benchmarks (such as
ﬁlesystem io), while accounting for the expected overhead
of each system call.
8.2 Cost of Added Hardware
Sanctum’s hardware changes add relatively few gates
to the Rocket chip, but do increase its area and power
consumption. Like SGX, we avoid modifying the core’s
critical path: while our addition to the page walker (as
analyzed in the next section) may increase the latency of
TLB misses, it does not increase the Rocket core’s clock
cycle, which is competitive with an ARM Cortex-A5 [29].
As illustrated at the gate level in Figures 8 and 9, we es-
timate Sanctum to add to Rocket hardware 500 (+0.78%)
gates and 700 (+1.9%) ﬂip-ﬂops per core. Precisely, 50
gates for cache index calculation, 1000 gates and 700 ﬂip-
ﬂops for the extra address page walker conﬁguration, and
400 gates for the page table entry transformations. DMA
ﬁltering requires 600 gates (+0.8%) and 128 ﬂip-ﬂops
(+1.8%) in the uncore. We do not make any changes to
the LLC, and exclude it from the percentages above (the
LLC generally accounts for half of chip area).
8.3 Added Page Walker Latency
Sanctum’s page table entry transformation logic is de-
scribed in § 5.3, and we expect it can be combined with
the page walker FSM logic within a single clock cycle.
Nevertheless, in the worst case, the transformation
logic would add a pipeline stage between the L1 data
USENIX Association  
25th USENIX Security Symposium  871
15
%
2
1
e
n
e
s
a
e
r
c
n
i
l
e
s
a
b
e
r
u
c
e
s
n
i
r
e
v
o
i
e
m
i
t
n
o
i
t
l
e
p
m
o
c
%
%
8
%
4
%
0
overhead due to reduced LLC
page miss overhead
enclave enter/exit overhead
overhead due to private cache flushes
hmmer