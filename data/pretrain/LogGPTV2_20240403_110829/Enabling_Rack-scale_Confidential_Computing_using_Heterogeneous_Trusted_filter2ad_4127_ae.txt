101
152
Image
size
224x224x3
224x224x3
224x224x3
224x224x3
224x224x3
Type
1K
1K
1K
1K
1K
forwarding and pre-processing. Here we report a study on the
throughput and the latency overhead of our implementation
by running a set of DNN workloads (Table II), as well
as its scalability under the elastic resource allocation. For
this purpose, we utilized as a baseline the same workloads’
performance on a standard GPU server, with the same GPUs.
129.03%
81.06%
59.92%
4 8 16 32 64 4 8 16 32 64 4 8 16 32 64 4 8 16 32 64 4 8 16 32 64
VGG16
GoogLeNet
ResNet50
ResNet101
ResNet152
(a) Inference
50%
130%
d
a
e
h
r
e
v
O
e
c
n
a
m
r
o
f
r
e
P
45%
70%
40%
35%
30%
25%
20%
15%
10%
5%
0%
d
a
e
h
r
e
v
O
e
c
n
a
m
r
o
f
r
e
P
4.50%
4.00%
3.50%
3.00%
2.50%
2.00%
1.50%
1.00%
0.50%
0.00%
4 8 16 32 64 4 8 16 32 64 4 8 16 32 64 4 8 16 32 64 4 8 16 32 64
VGG16
GoogLeNet
ResNet50
ResNet101
ResNet152
(b) Training
Fig. 7. HETEE throughput overhead on single GPU with different batch sizes.
Throughput and latency evaluation on a single GPU. The
throughput overhead of the HETEE system normalized to the
baseline is shown in Fig. 7. It can be seen that the throughput
overhead is 6.95% for the inference task and 0.91% for the
training task on average when the batch size is 8. In this case,
the GPU utilization is around 80%. For most of the workloads
in our evaluation, the throughput overhead is under 5%. Such
results demonstrate that the software and hardware co-design
of HETEE is balanced. One exception is the GoogLeNet when
it was used for inference, since the size of GoogLeNet is much
smaller than others (its network model is only 28 MiB, and
only has 22 layers). The smaller computing time ampliﬁes the
cost of data transmission, thereby affecting the throughput.
Such an impact of model size on throughput can also be
observed from ResNet, when its layers grows from 50 to 152.
Fig. 8 describes the latency overhead of the HETEE system.
The Y axis shows the latency and its breakdown, and the
)
s
m
(
d
a
e
h
r
e
v
O
d
n
a
y
c
n
e
t
a
L
1000
900
800
700
600
500
400
300
200
100
0
48.01%
47.03%
61.53%
72.19%
40.92%
50.19%
45.03%
37.43%
66.77%
41.20%
41.85%
37.01%
59.74%
54.35%
51.70%
57.65%
49.39%
47.89%
50.62%
41.91%
39.55%
39.51%
32.78%
31.18%
30.25%
4 8 16 32 64 4 8 16 32 64 4 8 16 32 64 4 8 16 32 64 4 8 16 32 64
VGG16
GoogLeNet
ResNet50
ResNet101
ResNet152
Compute
Preprocess
De/Encryption
Transfer
(a) Inference
1800
1600
)
s
21.91%
m
(
d
a
e
h
r
e
v
O
d
n
a
y
c
n
e
t
a
L
16.07%
10.39%
5.89%
3.86%
1400
1200
1000
800
600
400
200
0
25.85%
35.54%
55.30%
22.98%
19.66%
17.26%
50.47%
42.72%
39.28%
33.49%
32.18%
29.11%
22.14%
16.79%
18.90%
14.08%
10.98%
14.75%
11.32%
8.20%
4 8 16 32 64 4 8 16 32 64 4 8 16 32 64 4 8 16 32 64 4 8 16 32 64
VGG16
GoogLeNet
ResNet50
ResNet101
ResNet152
Compute
Preprocess
De/Encryption
Transfer
(b) Training
Fig. 8. HETEE latency overhead on single GPU with different batch sizes.
number on top of each bar is the latency overhead normalized
to its baseline. When the batch size is 8, the average inference
latency is 42.96% compared to the baseline, and the average
training latency is 18.54%. In this case, the GPU utilization
ranges from 85% to 92%. A typical conﬁdential DNN task run-
ning in HETEE takes 4 major steps, including pre-processing
(image decoding), GPU execution, (de)encryption, and data
transfer (from local node to SC, and from SC to proxy
node etc). With the same delay caused by pre-processing
and GPU execution as that of the baseline setting, HETEE
introduces a new overhead for (de)encryption and additional
data transfer. As we can see, the data transfer time continues to
increase as the batch size grows, while the (de)encryption time
is small and stable. This explains that the latency overhead is
positively correlated with the batch size. Since the intensive
computation overshadows the cost of data transfer, we observe
that the proportion of the latency overhead in the training time
is smaller than the one in the inference time.
Scalability evaluation. The HETEE system supports the elas-
tic allocation of accelerator resources. Multiple accelerators
can be dynamically assigned to one enclave for speedup. Ta-
ble III shows the scalability of HETEE. Two main conclusions
can be drawn: (1) The elastic resource allocation mechanism of
HETEE can signiﬁcantly improve the performance of a single
HETEE enclave. For most workloads, multiple GPUs achieve
acceleration compared to a single GPU. In particular,
the
performance of 2 GPUs is 1.64x of a single GPU on average,
and 4 GPUs achieve a 2.59x speedup on average; (2) HETEE
does not affect scalability compared to the baseline. Taking
ResNet152 as an example,
the scalability of the HETEE
system remains essentially the same as the baseline system.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:57:53 UTC from IEEE Xplore.  Restrictions apply. 
1459
HETEE INFERENCE THROUGHPUT SCALABILITY EVALUATION (NORMALIZED TO THE BASELINE)
Batch size
4
8
16
Number of GPU
2 GPUs
4 GPUs
2 GPUs
4 GPUs
2 GPUs
TABLE III
1 GPU
1.00
0.97
1.00
0.81
1.00
0.96
1.00
0.98