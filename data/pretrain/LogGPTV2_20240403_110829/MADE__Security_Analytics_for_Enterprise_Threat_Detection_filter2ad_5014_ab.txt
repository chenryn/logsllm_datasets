variants by similarity with existing samples, but it is not designed
to detect new malware. Oprea et al. [47] apply belief propagation
to detect malware periodic communication and malware delivery in
multi-stage campaigns. Bartos et al. [11] design a system for generic
malware detection that classifies legitimate and malicious web flows
using a number of lexical, URL, and inter-arrival timing features
computed for a sequence of flows. They propose a new feature rep-
resentation invariant to malware behavior changes, but unfortunately
the method does not retain feature interpretability. BAYWATCH [30]
uses supervised learning based on inter-arrival timing and lexical
features to detect periodic C&C or beaconing communication. As
2. Feature ExtractionTraining6. Model PredictionHistoricaldataTestingReal-time data8. EvaluationAnalysisFeedback7. Ranking High-Risk CommunicationsEnterprise network1. Data Filtering / Labeling3. Feature Selection4. Model SelectionLRDTRFSVMExternal dataWHOISGeolocationSOC5. Data RepresentationThreat intelligenceVirusTotalTalosACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Alina Oprea, Zhou Li, Robin Norris, and Kevin Bowers
we can see from the table, MADE has several interesting character-
istics: (1) MADE uses the most extensive set of features to date for
identifying HTTP malware communication, carefully crafted based
on the SOC domain expertise; (2) MADE can identify various mal-
ware classes since it does not rely on timing or lexical features; (3)
MADE achieves best precision from all prioritization-based systems
(at similar false positive rates); (4) MADE identifies new malware
not available during training and unknown to the community at the
time of detection; (5) MADE achieves interpretability of ML results
and can be used successfully by SOC domain experts.
MADE can detect general malware communication in enterprise
network. As discussed, previous systems are crafted for specific
types of malware communication protocols (either with periodic tim-
ing [30, 47], or those that are similar to malware available in train-
ing [11, 45]). Therefore, there is no existing system with which we
can meaningfully compare the results achieved by MADE. Our hope
is that the community will create benchmarks including datasets and
algorithms publicly available in this space. But at the moment we
are limited in deploying existing systems in our environment and
comparing them explicitly with MADE.
2.5 Ethical considerations
The enterprise SOC provided us access to four months of data from
their SIEM system. Employees consent to their web traffic being
monitored while working within the enterprise perimeter. Our dataset
did not include any personally identifiable information (PII). Our
analysis was done on enterprise servers and we only exported aggre-
gated web traffic features (as described in Table 3).
3 MADE TRAINING
We obtained access to the enterprise SIEM for a four month period in
February-March and July-August 2015. The number of events in the
raw logs is on average 300 million per day, resulting in about 24TB
of data per month. We use one month of data (July) for training as
we believe it is sufficient to learn the characteristics of the legitimate
traffic and one month (August) for testing. To augment the set of
malicious domains, we used the February and March data to extract
additional malicious connections, which we include in the train-
ing set. We extract a set of external destinations (domain names or
FQDN) contacted by enterprise machines during that interval. After
data filtering, we label each destination domain as malicious (M), be-
nign (B), or unknown (U) using several public services (Section 3.1).
The large majority of domains (more than 90%) are unknown at
this stage. Each domain is represented by a set of numerical and
categorical features extracted from different categories (Sections 3.2
and 3.3).
We express our problem in terms of ML terminology. Let Dtr =
{(x1, L1), . . . ,(xn, Ln)} be the training set with domain i having
feature set xi and label Li ∈ {M,B,U}. Our aim is to train an ML
model f ∈ H, where H is the hypothesis space defined as the set of
all functions from domain representations X to predictions Y . f is
selected to minimize a certain loss function on the training set:
f ∈H ℓ(f (xi , Li , θ)) + λω(θ)
min
where θ is the model parameter, and λω(θ) a regularization term.
1, . . . , x′
= f (x′
i).
This is a general supervised learning setting and can be instan-
tiated in different ways in our system. To be concrete, let us start
by a simple example. Most previous work on detecting malicious
domains (e.g., [8, 10, 12, 13, 51]) employ classification models to
distinguish malicious and benign domains. In the above framework,
we could define the training set Dtr as the domains with labels
M and B, H the set of SVM classifiers h : X → {M,B}, and the
loss function ℓ the hinge loss. In the testing phase, a new set of do-
mains Dtest = {x′
m} is observed and our goal is to generate
predictions on these domains using the optimal SVM classifier f
learned during training. We compute the prediction that the domain
is malicious or benign L′
i
This approach of distinguishing malicious and benign domains
works well in general, since these domains have different character-
istics. Unfortunately, this approach is not sufficient in our setting to
generate meaningful alerts for the SOC. If we simply train a model
on a small percentage of the domains (including only the malicious
and benign classes), our results on the unknown domains would
be unreliable (since such domains are not even part of the training
set). Instead, MADE’s goal is to identify and prioritize the most
suspicious domains among the unknown class and we discuss how
we adapt this general framework to our setting in Section 3.4.
3.1 Data Filtering and Labeling
For each external domain contacted by an enterprise machine, the
web proxy logs include: connection timestamp, IP addresses and
ports of the source and destination, full URL visited, HTTP method,
bytes sent and received, status code, user-agent string, web referer,
and content type. The enterprise maintains separately the first date
each FQDN domain was observed on the enterprise network. During
July and August, a total number of 3.8M distinct FQDNs were
included in the proxy logs. We applied a number of filters to restrict
our attention to potential domain of interest that could be related to
malware communications (see Table 2 for statistics):
- Recent domains: We focus on recent domains, appearing first time
on the enterprise in the last two weeks. Our motivation is three-fold:
malicious domains have short lifetime; long-lived malicious domains
are more likely to be included in threat intelligence services; and we
are interested in new malware trends. There are about 1.1 million
distinct recent domains contacted in July and August.
- Popular domains: We exclude for analysis popular domains con-
tacted by more than 50 enterprise hosts per day. Most enterprise
infections compromise a few hosts and large-scale compromises can
be detected by perimeter defenses and other existing tools.
- CDN domains: We filter out the domains hosted by reputable CDN
services by using several public lists [24, 38]. Based on discus-
sions with the enterprise SOC analysts, most CDN domains are
considered safe and are unlikely to be used as malicious communica-
tion channels. Reputable CDN service providers employ multi-layer
defenses for malware detection and usually take remediation mea-
sures quickly. Nevertheless, recent research revealed that adversaries
started to deliver potentially unwanted programs (PUPs) using CDN
domains [35]. In the future, attackers could host their command-and-
control infrastructure on CDN domains, and new defenses need to
be designed against this threat.
MADE: Security Analytics for Enterprise Threat Detection
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Filter
Labeling
Category
Recent domains
Popular domains
CDN
Ad traffic
Low connections
Benign (Alexa 10K)
Malicious (VirusTotal ≥ 3)
Unknown
VirusTotal 1 or 2
Additional malicious
Training
582,657
581,814
576,064
572,186
252,476
16,052
603
227,555
8,266
1,152
July
July
6.57%
0.24%
93.18%
Feb - March
Testing
532,706
531,992
528,614
525,469
218,879
15,779
516
202,584
August
August
7.2&
0.23%
93.55%
Total
1,115,363
1,113,806
1,104,678
1,097,655
471,355
31,831
1,119
430,139
Table 2: Dataset statistics.
- Legitimate advertisement traffic: We exclude reputable advertise-
ment domains based on EasyList [4] and EasyPrivacy [5]. Prior
research has shown that malicious content can be delivered through
online advertisements [66]. However, malicious advertisements usu-
ally originate from lower-reputation ad networks, participating in
the arbitration process. We filter out the reputable domains in the
two advertisement lists, owned by legitimate advertising compa-
nies, as we believe that they are unlikely to be associated with mal-
ware command-and-control centers. Malicious communication with
lower-reputable ad networks are still within the scope of MADE.
- Domains with few connections: We are mainly targeting enterprise
C&C for MADE and we restrict our attention to domains that have
at least 5 connections over a one-month period. We believe this to
be a minimal assumption, as most C&C domains see more traffic (at
least several connections per day). This has a significant impact on
data reduction (more than 50%).
After all filtering steps, there are a total of 471K distinct FQDN
contacted in July and August.
Data Labeling. We conservatively label as benign the domains with
their second-level domain in top 10K Alexa (6.57% of training and
7.2% of testing domains). These are popular sites with strong secu-
rity protections in place. We queried all other domains to VirusTotal,
a cloud-based antivirus engine. We label malicious all domains
flagged by at least three anti-virus engines in VirusTotal. This re-
sulted in 1,119 domains labeled as malicious in July and August,
representing 0.24% of all domains contacted during that time inter-
val (after filtering). The enterprise of our study is at the high-end
spectrum in terms of cyber defense, which manifests into low rate
of malicious activities. To augment the set of malicious domains,
we applied the same procedure to data from February and March
and identified a set of 1,152 malicious domains in that period. We
consider unknown all domains with a score of 0 on VirusTotal that
are not already labeled as benign. They represent the large majority
of traffic (92.88%). Domains with scores of 1 and 2 are not included
in training, as we are not certain if they are indeed malicious (many
times low-profile adware or spyware campaigns receive scores of 1
or 2, but are not actually malicious).
3.2 Feature Extraction
In this section, we elaborate on the large set of features (89) we
used for representing FQDN domains in our dataset. See Table 3
for a list and description of features. The majority of the features
are extracted from the web proxy logs of the enterprise, and we
call them internal features. We extract additional external features
related to domain registration and IP geolocation. To the best of
our knowledge, our set of features is the most extensive to date for
detecting enterprise C&C communication over HTTP. We leveraged
feedback from tier 3 SOC analysts, as well as previous academic
research, public reports on malware operations, and measurement of
our dataset to define these features. The ones that are novel compared
to previous work are highlighted in bold in Table 3. In addition to
generic malware features applicable for malware detection in any
environment, we define a set of enterprise-specific features (see
column “’Enterprise” in Table 3) that capture traffic characteristics
of individual enterprises.
Internal Features. We provide first a description of the internal fea-
tures extracted from web proxy logs, grouped into seven categories:
Communication structure. These are features about communication
structure for domains contacted by enterprise hosts. Specifically,
we count the number of enterprise hosts contacting the domain and
compute several statistics per host (Avg/Min/Max and Ratio) for
bytes sent and received, and POST and GET connections. Malicious
domains have different communication structure in number of con-
nections, POST vs GET, and number of bytes sent and received
compared to legitimate domains. For instance, the median average
connections per host to legitimate domains is 27, while for malicious
domains is 13.5. Malicious domains exhibit more POST requests
(on average 43) than legitimate ones (only 12.58 on average). Also,
malicious domains have higher ratios of bytes sent over received
(five times higher than legitimate domains).
Domain structure. Malicious domains are not uniformly distributed
across all TLDs (top-level domains). A well-known strategy for
attackers is to register domains on inexpensive TLDs to reduce their
cost of operation [27]. As such, we extract the TLD from the domain
name and consider it as a categorical feature. We also consider the
number of levels in the domain, the number of sub-domains on the
same SLD (second-level domain), and domain name length.
URL features. URL-derived features are good indicators of web at-
tacks [36]. In enterprise settings, malware is increasingly designed to
communicate with external destinations by manipulating URLs [50].
URL path (the substring after domain name), folder, file name, ex-
tension, parameters, and fragment fields can all be used to update
host status or exfiltrate host configurations. Therefore, we calculate
the overall statistics per URL for these attributes. We also count the
number of distinct URLs, the fraction of URLs with query string, file
name, file extension, as well as domain URLs (those for which path
and query string are empty). Another set of features of interest are
related to number of parameters and their values in the query string,
as malicious domains tend to have more diverse parameter values.
Measurement on our dataset confirms that malicious domains differ
greatly in these features compared to other domains. For instance,
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Alina Oprea, Zhou Li, Robin Norris, and Kevin Bowers
Category
Features
Description
Type
Enterprise
Novel
Internal features
Num_Hosts
Num_Conn
Avg_Conn, Min_Conn, Max_Conn
Total_sent_bytes, Total_recv_bytes
Avg_ratio_rbytes, Min_ratio_rbytes, Max_ratio_rbytes
Total_GET, Total_POST
Avg_ratio_PG, Min_ratio_PG, Max_ratio_PG
Dom_Length
Dom_Level
Dom_Sub
Dom_TLD
Num_URLs
Avg_URL_length, Min_URL_length, Max_URL_length
Avg_URL_depth, Min_URL_depth, Max_URL_depth
Num_params
Avg_params, Min_params, Max_params
Avg_vals, Min_vals, Max_vals
Frac_URL_filename
Num_filename, Num_exts
Frac_query
Frac_frag, Num_frag
Frac_bare
Distinct_UAs
Ratio_UAs
Avg_UAs, Min_UAs, Max_UAs
Frac_no_UA
Frac_UA_1, Frac_UA_10
UA_Popularity
Browser
Avg_Browsers
OS
Avg_OS
Frac_200, Frac_300, Frac_400, Frac_500
Num_200, Num_300, Num_400, Num_500
Ratio_fail
Frac_no_ref
Num_ref_doms
Ratio_ref
Avg_ref, Min_ref, Max_ref
Has_Referer
Distinct_ct
Frac_ct_empty
Frac_ct_js, Frac_ct_image, Frac_ct_text,
Frac_ct_video, Frac_ct_app, Frac_ct_html
External features
Reg_Age
Update_Age
Reg_Validity
Update_Validity
Reg_Email