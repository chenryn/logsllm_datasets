components: (1) MvWDL ﬁrst extracts features from BC/MC
packets and learns multi-view embedding representations as
device ﬁngerprints. (2) Inspired by the wide & deep learning
model [8], a hybrid-fusion multi-view artiﬁcial neural network
is designed to fuse dense embeddings from six independent
and complementary views in two structures: (a) a deep neural
network for early fusion is designed to maximize the gen-
eralization performance, and (b) a wide component for late
fusion is added to improve the memorization of label-view
interaction, i.e., how does each view response to each man-
ufacturer/type/model of devices. (3) Malicious devices are
detected with view inconsistency. A “positive” loss function
enhances view consistency for benign samples. Meanwhile,
when malicious devices are conﬁrmed and labeled, they are in-
corporated through a “negative” loss function, which captures
the view inconsistencies caused by malicious devices.
4.2 Device Fingerprinting
We ﬁrst construct device ﬁngerprints from features extracted
from BC/MC packets. In practice, key-value pair features and
pseudo natural language features are processed differently.
Fingerprints from key-value pair features. Formally, the
set of all key-value pair features for a device are deﬁned as:
KVi = {ki,1 : vi,1, . . . ,ki,n : vi,n}, in which ki, j and vi, j denote
the jth key and jth value of device i, respectively. The global
key list is deﬁned as an ordered collection of all the keys in
the entire dataset: K = {k1,k2, . . . ,kN}. Corresponding to the
order of keys in the key list, we deﬁne the key-value pair
feature vector of the device i as: ¯VKV,i = {vi,1,vi,2, . . . ,vi,N}.
All categorical values are tokenized. When a key k j does not
exist in KVi, we set the corresponding value vi, j to “null”.
Fingerprints from pseudo natural language features. To
fully utilize information resides in the content of pseudo-
natural-language features, we explore two content model-
ing algorithms to generate ﬁngerprints: (1) word to vector
(word2vec), and (2) Latent Dirichlet Allocation (LDA).
(1) Word2vec. The word2vec approach [44] is based on the
distributional hypothesis, which indicates that terms occur-
ring in the same context tend to have similar meanings. Its
main purpose is to vectorize words in the text corpus so that
words appearing in similar contexts are represented by vectors
close to each other in the feature space. In its existing imple-
mentations, the contexts are captured with sliding windows.
However, they are not suitable in our application, in which
the context of a token in the pseudo-natural-language features
must remain in the scope of a “sentence” (a RR in mDNS, a
ﬁeld in the sniffed packet). Hence, we implemented our own
word2vec scheme in three steps: (1) We build a word to id
dictionary to tokenize terms in the dataset. (2) The entire cor-
pus is used to train a neural network model Mw2v to maximize
the conditional probability of a word given its context, i.e.:
argmax
θ ∏
(ω,c)∈D
p(ω|c;θ)
(1)
in which θ is the optimization goal while maximizing the
conditional probability of word (ω) given the context (c). D
is the set of all (ω,c) pairs. The context (c) of a word (ω) is
composed of a window of 5 terms centered at ω, and restricted
in the same string as ω. (3) We apply Mw2v to each word to
get its corresponding vector. The feature vector of the entire
string is constructed as the mean of all its token vectors.
(2) Latent Dirichlet Allocation. LDA is a classic topic model-
ing approach based on the Bag-of-Words model [6]. Its idea
is to construct a model of document-topic-term relationship
using unsupervised learning. Different from word embedding,
LDA generates human-interpretable topic models. With the
observation that different devices usually show diverse topic
distributions, we utilize LDA to statistically model the topic
distribution of the pseudo natural language features.
4.3 Multi-view Wide & Deep Learning
Features extracted in Section 4.2 are organized into views
based on their host protocols, as listed in Table 3. Some sim-
ple protocols, such as ARP, generate identical packets from
different devices. They only contribute to the protseq feature,
which records the sequence of protocols used by a device.
60    29th USENIX Security Symposium
USENIX Association
Figure 2: An overview of the multi-view wide and deep learning framework.
Table 3: View dimensionality before/after embedding.
view
DHCP
mDNS
SSDP
LBN
UDP
protseq
base protocols
DHCP and DHCPv6
mDNS
SSDP
LLMNR, BROWSER, NBNS
other UDP features
protocol sequence & MAC preﬁx
dimensionality
85/680
7/128
67/536
16/128
1/128
2/136
In multi-view embedding representation, six views are for-
mally denoted as: F ={v1,v2,v3,v4,v5,v6}. We learn a global
word embedding space and LDA topic space for each view.
The embedding representation of view vi is deﬁned as:
ei = f i(vi;wi)
(2)
where wi is the view-speciﬁc column-index matrix for vi. f i is
a range of column-speciﬁc embedding operations (id embed-
ding, word2vec, and LDA), followed by a concat operation
to generate view vi’s ﬁnal dense embedding ei.
The objective loss Ldeep of the deep component is deﬁned
as a maximum likelihood estimation function ¯P:
P(y = tc|e;θ)
Ldeep = ¯P(y = t|e;θ) =
C
I(y=tc)
(6)
∏
c=1
where C is the training set, and tc is the label of sample c. P
is the conditional probability of a sample being labeled as tc
under input e, with θ as the parameter set. I is the indicator
function. The optimization progress is denoted by maximum
log likelihood and stochastic gradient descent ∇:
∇θLdeep = argmax
Ee∼ ˆpviewlog( f deep ◦ gatt (e))
θ
(7)
where E is the expectation and ˆpview is the distribution of e.
As a feedforward neural network, the classiﬁcation probability
f deep
tc
of the deep fusion network is deﬁned as:
f deep
tc =
exp((cid:96)(K)
tc )
exp((cid:96)(K)
∑
tc )
tc
(8)
4.3.1 Deep Fusion
4.3.2 Wide Fusion
The deep fusion component implements the early-fusion
model of multi-model learning. We fuse the dense embedding
ˆe = [e1,e2, . . . ,e6] into one compact vector e as the initial
input of the fusion neural network. Based on ofﬂine experi-
ments, we choose afﬁne transformation as the attention oper-
ation gatt (·), instead of other popular operations such as sum
fusion, max fusion or concatenation fusion.
gatt ( ˆe;Wa,ba) = softmax(tanh(W T
e = gatt (e1,e2, . . . ,e6)
a ˆe + ba)
(3)
(4)
where Wa and ba are the afﬁne transformation parameters.
Next, we feed the fusion vector e into a deep neural net-
work f deep(·). Its main component is a standard multi-layer
perceptron (MLP), where the output of layer k is deﬁned as:
(cid:96)(k+1) = σ(W (k)(cid:96)(k) + b(k))
(5)
σ is the ReLU (Rectiﬁed Linear Unit) activation function
(except that the last layer is a fully connected layer). W (k) and
b(k) denote the perceptron weight and bias, respectively.
Besides the deep component for generalization performance,
we add another wide component, which implements the late-
fusion model of multi-model learning, to memorize the inter-
actions among features, views and labels. The wide compo-
nent takes ˆe as input, applies afﬁne transformation on each
view ei and trains a wide linear model to produce:
pi(y = t|ei)
(9)
where pi is the classiﬁcation result from view i. Similar to the
deep component, we also use maximum log likelihood and
stochastic gradient descent to deﬁne and optimize the loss of
wide component Lwide:
θ
∑
ei∈ ˆe
Eei∼ ˆpviewlog( f wide(ei))
∇θLwide = argmax
(10)
where f wide(·) represents a one-layer network for multi-
class classiﬁcation, whose output is also narrowed by softmax.
The c-th element of the wide fusion output, f wide
, indicates
the probability of the sample (device) being labeled as tc. It is
deﬁned as the sum of view-wise probabilities:
tc
USENIX Association
29th USENIX Security Symposium    61
tc = ∑
f wide
ei∈ ˆe
pi(y = tc|ei) = ∑
ei∈ ˆe
exp(ωT
exp(ωT
∑
tc
i ei + γi)
i ei + γi)
(11)
As deﬁned in [8], there are two essential requirements for
the wide component: (1) only linear operations are allowed
in the wide model. Therefore, ωi and γi in Eq. (11) are afﬁne
transform parameters. (2) the output of the wide component
is linearly merged to the deep component. Hence, we fuse the
output of the wide and deep components and deﬁne the ﬁnal
conditional probability of the classiﬁer output as:
u, pk
(ηcorr(pk
u) (cid:54)= A(pk
v))) > ε (15)
v) + (1−η)I(A(pk
∑
1≤u,v≤6
where A returns the index with the largest probability in pk. η
is a trade-off parameter in [0,1], which balances the probabil-
ity inconsistency corr(pk
v) with the type inconsistency. ε
is the threshold that separates benign and suspicious samples.
Last, when malicious devices are detected and conﬁrmed
through manual investigation, they are formally labeled and
used to train a fourth loss function, which attempts to maxi-
mize view inconsistency for known malicious devices:
u, pk
f f inal
tc = f wide
tc + f deep
tc
(12)
L− = ∑
(u,v,k)∈Dpos
log
1
1 + ecorr(pk
u,pk
v)
(16)
4.3.3 View Consistency and Malicious Device Detection
Besides the wide and deep learning approach for mobile de-
vice identiﬁcation, our second objective is to identify mali-
cious devices through BC/MC network features. To achieve
this, the following two assumptions are necessary:
Assumption 1. For a benign testing sample, label probabil-
ities pi(y = t|ei) generated from different views in the wide
component shall demonstrate strong consistency.
Assumption 2. When label probabilities from different views
demonstrate certain level of inconsistency/disagreement, the
device is either new to the model, or fabricated/forged.
In multi-model learning, view consistency (Assumption
1) is a fundamental objective that is often referred to as the
consensus principle. It is achieved by different mechanisms
such as co-training or shared latent sub-space. In the wide
and deep components of OWL, views are jointly optimized so
that view agreements are implicitly included in the objectives.
To further enhance the mutual agreements across views, we
deﬁne the correlation-based loss to explicitly maximize view
consistency in training. First, view correlation is deﬁned as:
corr(pu, pv) = (cid:107)pu − pv(cid:107)2
(13)
where pu and pv are output vectors from two different views
of the wide component. Their correlation is deﬁned with L-2
norm. The loss function is deﬁned as a sigmoid function:
2
where Dpos denotes the posterior dataset of labeled malicious
samples. They are also removed from the benign set Dpre.
Finally, four loss functions are combined to learn all pa-
rameters jointly (Eq. 17). In summary, the deep component
(Ldeep) is a maximum likelihood estimation function opti-
mized towards the best classiﬁcation performance for device
labels under input features; the wide component (Lwide) is
to optimize classiﬁcation performance on each view; the L+
component is optimized towards the maximum view agree-
ment for benign samples; and the L− component is to maxi-
mize the view inconsistency for malicious devices. All four
objectives are integrated in the MvWDL model (Figure 2).
¯h = Ldeep + Lwide + L+ + L−
(17)
5
Implementation and Experiments
In this section, we brieﬂy introduce the implementation of
OWL, and then present our experiment results.
5.1 Dataset and Data Labeling
At the ﬁnest granularity of the device identiﬁcation task, each