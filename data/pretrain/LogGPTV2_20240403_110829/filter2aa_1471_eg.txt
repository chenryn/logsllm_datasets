it’s possible to create virtual disks that are much larger than the underlying
storage cluster. When available space gets low, a system administrator can
dynamically add disks to the cluster. Storage Spaces automatically includes
the new physical disks to the pool and redistributes the allocated blocks
between the new disks.
Storage Spaces supports thin provisioned disks through slabs. A slab is a
unit of allocation, which is similar to the ReFS container concept, but applied
to a lower-level stack: the slab is an allocation unit of a virtual disk and not a
file system concept. By default, each slab is 256 MB in size, but it can be
bigger in case the underlying storage cluster allows it (i.e., if the cluster has a
lot of available space.) Spaces core keeps track of each slab in the virtual
disk and can dynamically allocate or free slabs by using its own allocator. It’s
worth noting that each slab is a point of reliability: in mirrored and parity
configurations, the data stored in a slab is automatically replicated through
the entire cluster.
When a thin provisioned disk is created, a size still needs to be specified.
The virtual disk size will be used by the file system with the goal of correctly
formatting the new volume and creating the needed metadata. When the
volume is ready, Spaces allocates slabs only when new data is actually
written to the disk—a method called allocate-on-write. Note that the
provisioning type is not visible to the file system that resides on top of the
volume, so the file system has no idea whether the underlying disk is thin or
fixed provisioned.
Spaces gets rid of any single point of failure by making usage of mirroring
and pairing. In big storage clusters composed of multiple disks, RAID 6 is
usually employed as the parity solution. RAID 6 allows the failure of a
maximum of two underlying devices and supports seamless reconstruction of
data without any user intervention. Unfortunately, when the cluster
encounters a single (or double) point of failure, the time needed to
reconstruct the array (mean time to repair or MTTR) is high and often causes
serious performance penalties.
Spaces solves the problem by using a local reconstruction code (LCR)
algorithm, which reduces the number of reads needed to reconstruct a big
disk array, at the cost of one additional parity unit. As shown in Figure 11-
96, the LRC algorithm does so by dividing the disk array in different rows
and by adding a parity unit for each row. If a disk fails, only the other disks
of the row needs to be read. As a result, reconstruction of a failed array is
much faster and more efficient.
Figure 11-96 RAID 6 and LRC parity.
Figure 11-96 shows a comparison between the typical RAID 6 parity
implementation and the LRC implementation on a cluster composed of eight
drives. In the RAID 6 configuration, if one (or two) disk(s) fail(s), to
properly reconstruct the missing information, the other six disks need to be
read; in LRC, only the disks that belong to the same row of the failing disk
need to be read.
EXPERIMENT: Creating tiered volumes
Storage Spaces is supported natively by both server and client
editions of Windows 10. You can create tiered disks using the
graphical user interface, or you can also use Windows PowerShell.
In this experiment, you will create a virtual tiered disk, and you
will need a workstation that, other than the Windows boot disk,
also has an empty SSD and an empty classical rotating disk (HDD).
For testing purposes, you can emulate a similar configuration by
using HyperV. In that case, one virtual disk file should reside on an
SSD, whereas the other should reside on a classical rotating disk.
First, you need to open an administrative Windows PowerShell
by right-clicking the Start menu icon and selecting Windows
PowerShell (Admin). Verify that the system has already identified
the type of the installed disks:
Click here to view code image
PS C:\> Get-PhysicalDisk | FT DeviceId, FriendlyName, 
UniqueID, Size, MediaType, CanPool
DeviceId FriendlyName            UniqueID                      
Size MediaType CanPool
-------- ------------            --------                      
---- --------- -------
2        Samsung SSD 960 EVO 1TB eui.0025385C61B074F7 
1000204886016 SSD         False
0        Micron 1100 SATA 512GB  500A071516EBA521      
512110190592 SSD         True
1        TOSHIBA DT01ACA200      500003F9E5D69494     
2000398934016 HDD         True
In the preceding example, the system has already identified two
SSDs and one classical rotating hard disk. You should verify that
your empty disks have the CanPool value set to True. Otherwise, it
means that the disk contains valid partitions that need to be deleted.
If you’re testing a virtualized environment, often the system is not
able to correctly identify the media type of the underlying disk.
Click here to view code image
PS C:\> Get-PhysicalDisk | FT DeviceId, FriendlyName, 
UniqueID, Size, MediaType, CanPool
DeviceId FriendlyName      UniqueID                                  
Size MediaType   CanPool
-------- ------------      --------                                  
---- ---------   -------
2        Msft Virtual Disk 600224802F4EE1E6B94595687DDE774B  
137438953472 Unspecified    True
1        Msft Virtual Disk 60022480170766A9A808A30797285D77 
1099511627776 Unspecified    True
0        Msft Virtual Disk 6002248048976A586FE149B00A43FC73  
274877906944 Unspecified   False
In this case, you should manually specify the type of disk by
using the command Set-PhysicalDisk -UniqueId (Get-
PhysicalDisk)[].UniqueID -MediaType , where
IDX is the row number in the previous output and MediaType is
SSD or HDD, depending on the disk type. For example:
Click here to view code image
PS C:\> Set-PhysicalDisk -UniqueId (Get-PhysicalDisk)
[0].UniqueID -MediaType SSD
PS C:\> Set-PhysicalDisk -UniqueId (Get-PhysicalDisk)
[1].UniqueID -MediaType HDD
PS C:\> Get-PhysicalDisk | FT DeviceId, FriendlyName, 
UniqueID, Size, MediaType, CanPool
DeviceId FriendlyName      UniqueID                                  
Size MediaType   CanPool
-------- ------------      --------                                  
---- ---------   -------
2        Msft Virtual Disk 600224802F4EE1E6B94595687DDE774B  
137438953472 SSD            True
1        Msft Virtual Disk 60022480170766A9A808A30797285D77 
1099511627776 HDD            True
0        Msft Virtual Disk 6002248048976A586FE149B00A43FC73  
274877906944 Unspecified   False
At this stage you need to create the Storage pool, which is going
to contain all the physical disks that are going to compose the new
virtual disk. You will then create the storage tiers. In this example,
we named the Storage Pool as DefaultPool:
Click here to view code image
PS C:\> New-StoragePool -StorageSubSystemId (Get-
StorageSubSystem).UniqueId -FriendlyName
DeafultPool -PhysicalDisks (Get-PhysicalDisk -CanPool $true)
FriendlyName OperationalStatus HealthStatus IsPrimordial 
IsReadOnly    Size AllocatedSize
------------ ----------------- ------------ ------------ ---
-------    ---- -------------
Pool         OK                Healthy      False             
1.12 TB        512 MB
PS C:\> Get-StoragePool DefaultPool | New-StorageTier -
FriendlyName SSD -MediaType SSD
...
PS C:\> Get-StoragePool DefaultPool | New-StorageTier -
FriendlyName HDD -MediaType HDD
...
Finally, we can create the virtual tiered volume by assigning it a
name and specifying the correct size of each tier. In this example,
we create a tiered volume named TieredVirtualDisk composed of a
120-GB performance tier and a 1,000-GB capacity tier:
Click here to view code image
PS C:\> $SSD = Get-StorageTier -FriendlyName SSD
PS C:\> $HDD = Get-StorageTier -FriendlyName HDD
PS C:\> Get-StoragePool Pool | New-VirtualDisk -FriendlyName 
"TieredVirtualDisk"
-ResiliencySettingName "Simple" -StorageTiers $SSD, $HDD -
StorageTierSizes 128GB, 1000GB
...
PS C:\> Get-VirtualDisk | FT FriendlyName, 
OperationalStatus, HealthStatus, Size,
FootprintOnPool
FriendlyName      OperationalStatus HealthStatus          
Size FootprintOnPool
------------      ----------------- ------------          --
-- ---------------
TieredVirtualDisk OK                Healthy      
1202590842880   1203664584704
After the virtual disk is created, you need to create the partitions
and format the new volume through standard means (such as by
using the Disk Management snap-in or the Format tool). After you
complete volume formatting, you can verify whether the resulting
volume is really a tiered volume by using the fsutil.exe tool:
Click here to view code image
PS E:\> fsutil tiering regionList e:
Total Number of Regions for this volume: 2
Total Number of Regions returned by this operation: 2
   Region # 0:
        Tier ID: {448ABAB8-F00B-42D6-B345-C8DA68869020}
        Name: TieredVirtualDisk-SSD
        Offset: 0x0000000000000000
        Length: 0x0000001dff000000
   Region # 1:
        Tier ID: {16A7BB83-CE3E-4996-8FF3-BEE98B68EBE4}
        Name: TieredVirtualDisk-HDD
        Offset: 0x0000001dff000000
        Length: 0x000000f9ffe00000
Conclusion
Windows supports a wide variety of file system formats accessible to both
the local system and remote clients. The file system filter driver architecture
provides a clean way to extend and augment file system access, and both
NTFS and ReFS provide a reliable, secure, scalable file system format for
local file system storage. Although ReFS is a relatively new file system, and
implements some advanced features designed for big server environments,
NTFS was also updated with support for new device types and new features
(like the POSIX delete, online checkdisk, and encryption).
The cache manager provides a high-speed, intelligent mechanism for
reducing disk I/O and increasing overall system throughput. By caching on
the basis of virtual blocks, the cache manager can perform intelligent read-
ahead, including on remote, networked file systems. By relying on the global
memory manager’s mapped file primitive to access file data, the cache
manager can provide a special fast I/O mechanism to reduce the CPU time
required for read and write operations, while also leaving all matters related
to physical memory management to the Windows memory manager, thus
reducing code duplication and increasing efficiency.
Through DAX and PM disk support, storage spaces and storage spaces
direct, tiered volumes, and SMR disk compatibility, Windows continues to
be at the forefront of next-generation storage architectures designed for high
availability, reliability, performance, and cloud-level scale.
In the next chapter, we look at startup and shutdown in Windows.
CHAPTER 12
Startup and shutdown
In this chapter, we describe the steps required to boot Windows and the
options that can affect system startup. Understanding the details of the boot
process will help you diagnose problems that can arise during a boot. We
discuss the details of the new UEFI firmware, and the improvements brought
by it compared to the old historical BIOS. We present the role of the Boot
Manager, Windows Loader, NT kernel, and all the components involved in
standard boots and in the new Secure Launch process, which detects any kind
of attack on the boot sequence. Then we explain the kinds of things that can
go wrong during the boot process and how to resolve them. Finally, we
explain what occurs during an orderly system shutdown.
Boot process
In describing the Windows boot process, we start with the installation of
Windows and proceed through the execution of boot support files. Device
drivers are a crucial part of the boot process, so we explain how they control
the point in the boot process at which they load and initialize. Then we
describe how the executive subsystems initialize and how the kernel launches
the user-mode portion of Windows by starting the Session Manager process
(Smss.exe), which starts the initial two sessions (session 0 and session 1).
Along the way, we highlight the points at which various on-screen messages
appear to help you correlate the internal process with what you see when you
watch Windows boot.
The early phases of the boot process differ significantly on systems with
an Extensible Firmware Interface (EFI) versus the old systems with a BIOS
(basic input/output system). EFI is a newer standard that does away with
much of the legacy 16-bit code that BIOS systems use and allows the loading
of preboot programs and drivers to support the operating system loading
phase. EFI 2.0, which is known as Unified EFI, or UEFI, is used by the vast
majority of machine manufacturers. The next sections describe the portion of
the boot process specific to UEFI-based machines.
To support these different firmware implementations, Windows provides a
boot architecture that abstracts many of the differences away from users and
developers to provide a consistent environment and experience regardless of
the type of firmware used on the installed system.
The UEFI boot
The Windows boot process doesn’t begin when you power on your computer
or press the reset button. It begins when you install Windows on your
computer. At some point during the execution of the Windows Setup
program, the system’s primary hard disk is prepared in a way that both the
Windows Boot Manager and the UEFI firmware can understand. Before we
get into what the Windows Boot Manager code does, let’s have a quick look
at the UEFI platform interface.
The UEFI is a set of software that provides the first basic programmatic
interface to the platform. With the term platform, we refer to the
motherboard, chipset, central processing unit (CPU), and other components
that compose the machine “engine.” As Figure 12-1 shows, the UEFI
specifications provide four basic services that run in most of the available
CPU architectures (x86, ARM, and so on). We use the x86-64 architecture
for this quick introduction:
■    Power on When the platform is powered on, the UEFI Security Phase
handles the platform restart event, verifies the Pre EFI Initialization
modules’ code, and switches the processor from 16-bit real mode to
32-bit flat mode (still no paging support).
■    Platform initialization The Pre EFI Initialization (PEI) phase
initializes the CPU, the UEFI core’s code, and the chipset and finally
passes the control to the Driver Execution Environment (DXE) phase.
The DXE phase is the first code that runs entirely in full 64-bit mode.
Indeed, the last PEI module, called DXE IPL, switches the execution
mode to 64-bit long mode. This phase searches inside the firmware
volume (stored in the system SPI flash chip) and executes each
peripheral’s startup drivers (called DXE drivers). Secure Boot, an
important security feature that we talk about later in this chapter in the
“Secure Boot” section, is implemented as a UEFI DXE driver.
■    OS boot After the UEFI DXE phase ends, execution control is
handed to the Boot Device Selection (BDS) phase. This phase is
responsible for implementing the UEFI Boot Loader. The UEFI BDS
phase locates and executes the Windows UEFI Boot Manager that the
Setup program has installed.
■    Shutdown The UEFI firmware implements some runtime services
(available even to the OS) that help in powering off the platform.
Windows doesn’t normally make use of these functions (relying
instead on the ACPI interfaces).
Figure 12-1 The UEFI framework.
Describing the entire UEFI framework is beyond the scope of this book.
After the UEFI BDS phase ends, the firmware still owns the platform,
making available the following services to the OS boot loader:
■    Boot services Provide basic functionality to the boot loader and other
EFI applications, such as basic memory management,
synchronization, textual and graphical console I/O, and disk and file
I/O. Boot services implement some routines able to enumerate and
query the installed “protocols” (EFI interfaces). These kinds of
services are available only while the firmware owns the platform and
are discarded from memory after the boot loader has called the
ExitBootServices EFI runtime API.
■    Runtime services Provide date and time services, capsule update
(firmware upgrading), and methods able to access NVRAM data
(such as UEFI variables). These services are still accessible while the
operating system is fully running.
■    Platform configuration data System ACPI and SMBIOS tables are
always accessible through the UEFI framework.
The UEFI Boot Manager can read and write from computer hard disks and
understands basic file systems like FAT, FAT32, and El Torito (for booting
from a CD-ROM). The specifications require that the boot hard disk be
partitioned through the GPT (GUID partition table) scheme, which uses
GUIDs to identify different partitions and their roles in the system. The GPT
scheme overcomes all the limitations of the old MBR scheme and allows a
maximum of 128 partitions, using a 64-bit LBA addressing mode (resulting
in a huge partition size support). Each partition is identified using a unique
128-bit GUID value. Another GUID is used to identify the partition type.
While UEFI defines only three partition types, each OS vendor defines its
own partition’s GUID types. The UEFI standard requires at least one EFI
system partition, formatted with a FAT32 file system.
The Windows Setup application initializes the disk and usually creates at
least four partitions:
■    The EFI system partition, where it copies the Windows Boot Manager
(Bootmgrfw.efi), the memory test application (Memtest.efi), the
system lockdown policies (for Device Guard-enabled systems,
Winsipolicy.p7b), and the boot resource file (Bootres.dll).
■    A recovery partition, where it stores the files needed to boot the
Windows Recovery environment in case of startup problems (boot.sdi
and Winre.wim). This partition is formatted using the NTFS file
system.
■    A Windows reserved partition, which the Setup tool uses as a fast,
recoverable scratch area for storing temporary data. Furthermore,
some system tools use the Reserved partition for remapping damaged
sectors in the boot volume. (The reserved partition does not contain
any file system.)
■    A boot partition—which is the partition on which Windows is
installed and is not typically the same as the system partition—where
the boot files are located. This partition is formatted using NTFS, the
only supported file system that Windows can boot from when
installed on a fixed disk.
The Windows Setup program, after placing the Windows files on the boot
partition, copies the boot manager in the EFI system partition and hides the
boot partition content for the rest of the system. The UEFI specification
defines some global variables that can reside in NVRAM (the system’s
nonvolatile RAM) and are accessible even in the runtime phase when the OS
has gained full control of the platform (some other UEFI variables can even
reside in the system RAM). The Windows Setup program configures the
UEFI platform for booting the Windows Boot Manager through the settings
of some UEFI variables (Boot000X one, where X is a unique number,
depending on the boot load-option number, and BootOrder). When the
system reboots after setup ends, the UEFI Boot Manager is automatically
able to execute the Windows Boot Manager code.
Table 12-1 summarizes the files involved in the UEFI boot process. Figure
12-2 shows an example of a hard disk layout, which follows the GPT
partition scheme. (Files located in the Windows boot partition are stored in
the \Windows\System32 directory.)
Table 12-1 UEFI boot process components
Co
mp
on
ent
Responsibilities
L
oc
ati
on
bo
ot
mg
fw.
efi
Reads the Boot Configuration Database (BCD), if required, 
presents boot menu, and allows execution of preboot 
programs such as the Memory Test application 
(Memtest.efi).
E
FI 
sy
ste
m 
pa
rti
tio