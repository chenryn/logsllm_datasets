Our proposed RCA framework involves two steps: frequent pattern 1) mining and 2) filtering. Frequent patterns in the dataset are first reported, followed by an evaluation on how strongly each frequent pattern correlates to the target failures.In frequent pattern mining, each item should be a binary variable representing whether a characteristic exists. In a production structured dataset, however, a column would usually represent one feature, which could have multiple distinct values, one for each entity. Therefore the structured
Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
31:8 	Lin, et al.log needs to first be transformed into a schema that fits the frequent pattern mining formulation. The transformation is done by applying one-hot encoding [14] on each of the columns in the structured table. For a column in the structured table f , which has k possible values in a dataset, one-hot encoding "explodes" the schema and generate k columns {f0, f1, ..., fk−1}, each contains a binary value of whether the entity satisfies f = fk.Apriori is a classical algorithm that is designed to identify frequent item-sets. As illustrated in Algorithm 1, starting from frequent items, i.e. item-sets at length=1, the algorithm generates candidate item-sets by adding one item at a time, known as the candidate generation process. At each length k, candidate generation is done and all the candidate item-sets are scanned to increment the count of their occurrences. Then the item-sets that meet the min-support threshold are kept and returned as the frequent item-set LK. We add a practical constraint max-length on the maximum length of the item-set that we are interested in. The limit on max-length stops the algorithm from exploring item-sets that are too descriptive and specific to the samples, which are typically less useful in production investigation.3.4 	Architecture of a Large-Scale Service Environment
Algorithm 1: Apriori Algorithm
let Ck be the candidate item-sets at length= k 
let Lk be the frequent item-sets at length= k 
L1 = frequent items 
k = 1
while Lk  ϕ and k ≤ max_lenдth do Ck+1 = candidate item-sets generated from Lk 	foreach transaction t in database do 
	foreach item-set c covered by t do 
	increment the count of c
end
endincrement the count of c
end
end 
Lk+1 = item-sets in Ck+1 that meet min-support 
k++
end 
return ∪Lk
By generating a large set of candidates and scanning through the database many times, Apriori suffers from an exponential run time and memory complexity (O(2D)), making it impractical for many production datasets. The FP-Growth algorithm, based on a special data structure FP-Tree, was introduced to deal with performance issues by leveraging a data structure that allows to bypass the expensive candidate generation step [13]. FP-Growth uses divide-and-conquer by mining short patterns recursively and then combining them into longer item-sets.Frequent item-set mining through FP-Growth is done in two phases: FP-Tree construction and item-set generation. Algorithm 2 shows the process of FP-Tree construction. The FP-Tree construction process takes two inputs: 1) the set of samples in the target failure state (equivalent to a transaction database in classical frequent pattern mining literature), and 2) a min-support threshold, based on which a pattern is classified as frequent or not. Each node in the tree consists of three fields, item-name, count, and node-link. item-name stores the item that the node represents,Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
Fast Dimensional Analysis for Root Cause Investigation 	31:9
count represents the number of transactions covered by the portion of the path reaching the node, and node-link links to the next node with the same item-name. The FP-tree is constructed in two scans of the dataset. The first scan finds the frequent items and sort them, and the second scan constructs the tree.Algorithm 2: FP-Tree Construction
Scan data and find frequent items 
Order frequent items in decreasing order with respect to support, F Create root node T, labeled as NULL
foreach transaction t in database do 
	foreach frequent item p in F do 
	if T has a child N such that N.item-set=p.item-set then
	|N | + + 
end
else 
Create N, link parent-link to T, and set N .count = 1 Link N’s node-link to nodes with the same item-nameend
end
end
Algorithm 3 illustrates the process for generating the frequent item-sets, based on the lemmas and properties Han et al. proposed in [13]. A conditional pattern base is a sub-database which contains the set of frequent items co-occurring with the suffix pattern. The process is initiated by calling FP-Growth(Tree, NULL), then recursively building the conditional FP-Trees.Algorithm 3: Frequent Item-set Generation
Function FB-Growth(Tree, α): 
	if Tree contains a single path P then 
	foreach combination β of nodes in path P do 
	Generate pattern β ∪ α with support = min support of nodes in β 	end 
	end 
	else 
	foreach αi in tree do 
	Generate pattern β = αi ∪ α with support = αi.support Construct β’s conditional pattern base and β’s conditional FP-tree Tβ 	if Tβ  ϕ thencall FB-Growth(Tβ, β) 
	end 
	end 
	end
After finding the frequent item-sets in the dataset, we examine how strongly the item-sets can differentiate positive (e.g. failed hardware/jobs) samples from the negative ones. We use lift, defined
Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
31:10 	Lin, et al.in Section 3.1, to filter out item-sets that are frequent in the failure state but not particularly useful in deciding if a sample will fail. For example, an item-set can be frequent in both non-failure and failure states, and the evaluation based on lift would help us remove this item-set from the output because it is not very useful in deciding whether samples in that item-set would fail or not.3.5 	Pre- and Post-Processing for Performance OptimizationWe incorporated multiple optimizations as pre- and post-processing to scale the RCA framework for accommodating near real-time investigations, which are important in responding to urgent system issues quickly. Many entities in a production log are identical, except the columns that are unique identifiers of the entities such as the timestamps, hostnames, or job IDs. Utilizing Scuba’s scalable infrastructure [1], we query pre-aggregated data which are already grouped by the distinct combinations of column values, with an additional weight column that records the count of the identical entities. To handle this compact representation of the dataset, we modified the algorithms to account for the weights. This pre-aggregation significantly reduces the amount of data that we need to process in memory and would reduce the runtime of our production analyses by > 100X. 	Columns that are unique identifiers about the entities need to be excluded before the Scuba query. The aggregation in Scuba is only meaningful after excluding these columns, otherwise the aggregation would return one entity per row due to the distinct values per entity. The framework allows users to specify columns to be excluded in the dataset, as well as automatically checks to exclude columns with the number of distinct values > D portion of the number of samples. Empirically, we use D = 2% in one of our applications, and the proper setting of D highly depends on the nature of the dataset.Adding multithreading support to the algorithm further improves Apriori’s performance, as the algorithm generates a large number of combinations and test them against the data. By testing these combinations in parallel, we can scale up with the number of available cores. However, we found that FP-Growth outperforms Apriori even when Apriori is optimized with multithreading.
3.6 	Interpretability OptimizationIn production datasets, it is common that there exist a large number of distinct items, and the lengths of the findings are typically much smaller than the number of the one-hot encoded feature columns (as discussed in Section 3.3). As a result, there can be multiple findings describing the same group of samples. To improve the quality of the result, we implemented two filtering criteria for removing uninteresting results as described below:Filter 1: An item-set T is dropped if there exists a proper subset U, (U ⊂ T) such that lif t(U ) ∗Hlif t ≥ lif t(T), where Hlif t ≥ 1
If there exist shorter rules with similar or higher lift, longer rules are pruned because they are less interesting. Hlif t is a multiplier that can be tuned based on the nature of the dataset, to remove more longer rules, as it makes the condition easier to be satisfied. This filter addresses the ubiquitous items discussed in [6]. As there exists a shorter rule with similar or higher lift, the one containing the ubiquitous item will be filtered out. Consider two rules:| { kernel A, | s e r v e r | type B } => | type B } => | f a i l u r e | Y with | l i f t | 5 | Y with | l i f t | 1 . 5 |
|---|---|---|---|---|---|---|---|---|---|---|
| { kernel A, |s e r v e r |type B , |d a t a c e n t e r C} => |d a t a c e n t e r C} => |d a t a c e n t e r C} => |f a i l u r e |f a i l u r e |Y with |l i f t |1 . 5 |
Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.Fast Dimensional Analysis for Root Cause Investigation 	31:11
It is likely that describing the server and kernel interaction is more significant than filtering by datacenter, therefore the second rule is pruned, even though the lift values from both rules meet our threshold on the minimum lift.