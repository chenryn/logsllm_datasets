### Figure 11 Analysis
As shown in Figure 11, the total traffic increases by 5.2%, 3.6%, 1.3%, and 2.5% on average for 4-, 8-, 16-, and 32-core CMP systems, respectively, compared to the baseline fault-tolerant systems. This result indicates that the network traffic overhead caused by checkpointing and data comparison is well amortized as the number of cores increases.

### Comparison with DCC
The DCC proposal allows the master-slave pair to independently access the global memory, which complicates the maintenance of master-slave consistency. The migration of DCC to direct network incurs significant performance degradation as the number of cores increases [13]. In contrast, our TDB proposal prevents slave cores from accessing the global memory, thereby solving the scalability problem.

In this subsection, we compare our TDB proposal with the DCC in a tiled-CMP architecture [13]. Figure 12 shows the overall performance and network traffic of the two dynamic binding schemes. The DCC proposal suffers from significant performance degradation in large CMP systems, while the performance of our TDB proposal remains very close to that of the baseline fault-tolerant systems, even when the number of cores grows to 32. For the DCC proposal, the frequency of external writes increases rapidly as the number of cores increases, leading to a significant increase in the overhead caused by the master-slave consistency window. As shown in Figure 12, the performance overhead of DCC grows to 6.4%, 10.2%, 19.2%, and 42.5% for 4-, 8-, 16-, and 32-core CMP systems, respectively. In contrast, the transparent binding in our TDB proposal avoids the global maintenance of master-slave consistency, minimizing performance degradation. Additionally, our TDB proposal generates less network traffic than DCC because it prevents slave cores from accessing the global memory. As the number of cores increases, both DCC and TDB well amortize the network traffic, but TDB outperforms DCC in terms of overall performance.

### Related Work
Redundancy is widely used to detect and recover from transient and permanent faults in microprocessors. The commercial fault-tolerant system Compaq NonStop Himalaya [30] utilizes two identical microprocessors to run the same program in lockstep. Any divergence in output results detected by the checker circuit triggers a software recovery routine. Austin proposed a fault-tolerant architecture called DIVA, which uses a simple in-order core to check the execution results of the out-of-order pipeline [21].

Recently, redundant execution in the same pipeline has also been used to detect and recover from transient faults [22-25]. AR-SMT [22] was the first to implement thread-level redundancy (TLR) based on the multithreading capabilities in simultaneous multithreading (SMT) [26-27] processors. Simultaneous and Redundantly Threaded processors (SRT) [28] and SRT with recovery (SRTR) [29] extend the TLR work and propose the concept of Sphere of Replication (SoR) to explain input replication and output comparison schemes. Hardware queues are used to share the leading thread’s results with the trailing thread, accelerating the trailing thread’s execution. However, these redundant multithreading proposals, which tend to share the same pipeline resources, may incur significant performance degradation due to resource competition. Furthermore, these temporal redundancy-based proposals cannot detect permanent faults.

CMP systems integrate more cores into a single chip, and the inherent redundancy of hardware resources has also been used for fault-tolerant design. Chip-level redundant threaded processors (CRT) [8] and CRT with recovery (CRTR) [9] exploit the idea of SMT to achieve fault tolerance within multithreaded CMPs. Dedicated channels are constructed to transfer execution results between the leading and trailing threads, and LVQ is used to share memory access results between redundant threads. Later, Smolens et al. [10] found that independent memory access of the master-slave pair can load identical values in common cases. They proposed relaxed input replication for their reunion execution model, permitting independent memory accesses and using the same mechanism for soft-error detection and recovery to correct memory inconsistency. Subramanyan et al. proposed an energy-efficient fault-tolerant CMP architecture named Redundant Execution using Critical Value Forwarding (RECVF) [11], where slaves can run at a lower frequency than masters, as critical results from the masters enable the latter to execute faster.

There are also numerous software-based fault-tolerant schemes [35-38]. Software Implemented Fault Tolerance (SWIFT) [35] is a compiler-based fault-tolerant scheme that duplicates critical instructions and inserts compare instructions during code generation. CompileR Assisted Fault Tolerant (CRAFT) combines SWIFT with conventional Redundant Multithreading schemes, providing a smoother design space for fault tolerance. Feng et al. [37] proposed a symptom-based soft error detection scheme called Shoestring, which leverages intelligent analysis at compile time to protect statistically vulnerable portions of the program code. Software-based fault tolerance aims to implement fault detection via replication and re-execution of codes within the same hardware. Compared to hardware-based Dual Modular Redundancy (DMR) schemes, which implement spatial redundancy, such temporal redundant schemes have limited fault detection capability and fault coverage.

### Conclusions
The ongoing aggressive CMOS scaling makes CMPs increasingly vulnerable to internal failures and external interferences. In this paper, we present TDB, a novel core-level fault-tolerant method based on the idea of dynamic binding. Learning from the strict input replication scheme, we employ private caches to maintain master-slave memory consistency. We also extend a directory-based cache coherence protocol to implement master-slave data transferring and achieve the objective of master-slave private cache consistency. In the MOESI_FT protocol, the master core is responsible for global memory access, while the slave core passively obtains identical data blocks. Based on transparent binding, our TDB proposal provides excellent flexibility and scalability in the tiled-CMP architecture. Experimental results show that the overall performance of the TDB proposal is very close to that of baseline fault-tolerant systems, outperforming DCC by 9.2%, 10.4%, 18%, and 37.1% for 4, 8, 16, and 32 cores, respectively.

According to the basic idea of TDB, the proposed execution model should be applicable for multiprocessors over chips. However, if the master and slave are located on different chips, the L1 cache miss latency of the pair may vary significantly. Moreover, synchronization, checkpointing, and consumer-consumer data transferring will undoubtedly induce significant performance degradation. Therefore, it is more suitable to limit the master-slave pair within the same chip. In the scenario of multiprocessors over various CMPs, we can implement our TDB schemes within each CMP. For multiprocessors over single-core chips, we may need to group the chips according to their spatial relationship and implement TDB within each group.

### Acknowledgment
This work was supported in part by the Natural Science Foundation of China (NSFC) under grant No. (61076018, 60803031, 60921002), and in part by the National Basic Research Program of China (973) under grant No. 2011CB302503.

### References
[References listed as provided]

---

This revised version aims to improve clarity, coherence, and professionalism. Let me know if you need any further adjustments!