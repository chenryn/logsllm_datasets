shown in Figure 11, the total traffic is increased by 5.2%, 
3.6%, 1.3% and 2.5% on average for the 4-, 8-, 16- and 
32-core  CMP  systems  over  the  baseline  fault-tolerant 
systems,  respectively.  The  result  indicates  that,  the 
network  traffic  overhead  caused  by  checkpointing  and 
data comparison is well amortized as the number of cores 
increases. 
E.  Comparison against DCC 
DCC  proposal  allows  the  master-slave  pair  to 
independently access the global memory. Such a dynamic 
scheme complicates the maintenances of the master-slave 
consistency.  The  migration  of  DCC  to  direct  network 
proves 
incurs  significant 
performance degradation as the number of cores increases 
[13]. Our TDB proposal argues to prevent the slave cores 
from  accessing  the  global  memory,  which  as  a  result 
solves the scalability problem. 
the  DCC  proposal 
that 
In  this  subsection,  we  compare  our  TDB  proposal 
with  the  DCC  in  tiled-CMP  architecture  [13].  Figure  12 
shows the overall performance and network traffic of the 
two  dynamic  binding  schemes.  DCC  proposal  suffers 
from significant performance degradation for large CMP 
systems  while  the  performance  of  our  TDB  proposal  is 
very  close  to  baseline  fault-tolerant  systems  even  when 
the number of cores grows to 32. For the DCC proposal, 
as  the  number  of  cores  increases,  the  frequency  of 
external write increases rapidly. As a result, the overhead 
caused  by 
the  master-slave  consistency  window 
exacerbates  significantly.  As  shown  in  Figure  12,  the 
performance  overhead  of  DCC  grows  to  6.4%,  10.2%, 
19.2%  and  42.5%  when  considering  4-,  8-,  16-  and  32-
core  CMP  systems 
the 
transparent binding in our TDB proposal avoids the global 
maintenance  of  master-slave  consistency, 
the 
performance degradation is minimized. When considering 
the network traffic, our TDB proposal prevents the slave 
In  contrast, 
respectively 
so 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
300from accessing the global memory and thus generates less 
network  traffic  than  DCC  does.  As  the  number  of  cores 
increases,  both  the  DCC  and  TDB  well  amortize  the 
network traffic. 
VII.  RELATED WORK 
Redundancy  is  widely  used  to  detect  and  recover 
from  transient  and  permanent  faults  in  microprocessors. 
Commercial  fault-tolerant  system  Compaq  NonStop 
Himalaya  [30]  utilizes  two  identical  microprocessors  to 
run  the  same  program  in  lockstep.  In  each  cycle,  any 
divergence  of  the  output  results  detected  by  the  checker 
circuit  triggers  a  software  recovery  routine.  Austin 
proposed a fault-tolerant architecture called DIVA, which 
uses a simple in-order core to check the execution results 
of the out-of-order pipeline [21].  
Recently, redundant execution in the same pipeline is 
also used to detect and recover from transient faults [22-
25]. AR-SMT [22] is the first work to implement thread 
level  redundancy  (TLR)  based  on  the  multithreading 
capabilities in simultaneous multithreading (SMT) [26-27] 
processors.  Simultaneous  and  Redundantly  Threaded 
processors  (SRT)  [28]  and  SRT  with  recovery  (SRTR) 
[29] extend the TLR work and propose the conception of 
Sphere  of  Replication  (SoR)  to  help  explain  the  input 
replication  and  out  comparison  schemes.  Hardware 
queues  are  presented  to  share  the  leading  thread’s  result 
with the trailing thread to accelerate the trailing thread’s 
execution.  As  these  redundant  multithreading  proposals 
tend  to  share  the  same  pipeline  resources,  resources 
competition  may 
performance 
degradation.  What’s  more,  these  temporal  redundancy 
based proposals cannot detect permanent faults. 
significant 
incur 
CMP systems integrate more cores in a single chip. 
The inherent redundancy of  hardware  resources  has  also 
been used for fault-tolerant design. Chip-level redundant 
threaded  processors  (CRT)  [8]  and  CRT  with  recovery 
(CRTR)  [9]  exploits  the  idea  of  SMT  to  achieve  fault 
tolerance  within  the  multithreaded  CMPs.  Dedicated 
channels  are  constructed  to  transfer  execution  results 
between the leading and trailing threads. LVQ is also used 
to share the memory access results between the redundant 
threads.  Therefore,  the  trailing  threads  can  get  identical 
memory  value  as  the  leading  one.  Later,  Smolens  et  al. 
[10]  found  that  independently  memory  access  of  the 
master-slave pair can load identical values in the common 
case.  They  proposed  relaxed  input  replication  for  their 
reunion  execution  model,  in  which  independent  memory 
accesses are permitted and the same mechanism for soft-
error  detection  and  recovery  is  involved  to  correct  the 
memory  inconsistency.  Subramanyan  et  al.  proposes  an 
energy-efficient fault-tolerant CMP architecture named as 
Redundant  Execution  using  Critical  Value  Forwarding 
(RECVF)  [11],  wherein  the  slaves  can  run  at  a  lower 
frequency than the masters since the critical results from 
[36] 
further 
the masters to the slaves enable the latter to execute faster. 
There are also plenty of software-based fault-tolerant 
schemes [35-38]. Software Implemented Fault Tolerance 
(SWIFT)  [35]  is  a  compiler-based  fault-tolerant  scheme. 
It duplicates critical instructions of a program and inserts 
compare  instructions  at  specified  points  during  the  code 
generation  period.    CompileR  Assisted  Fault  Toleranct 
(CRAFT) 
combines  SWIFT  with 
conventional  Redundant  Multithreading  schemes  and 
therefore  provides  smoother  design  space  for  fault 
tolerance. Feng et al. [37] proposes a symptom-based soft 
error  detection  scheme  called  Shoestring.  It  leverages 
intelligent analysis at compile time and focuses its effects 
on  protecting  statistically-vulnerable  portions  of  the 
program code. The software-based fault tolerance aims to 
implement fault detection via replication and re-execution 
of  codes  within  the  same  hardware.  Compared  with 
hardware-based  Dual  Modular  Redundancy 
(DMR) 
schemes  which  implement  spatial  redundancy,  such 
temporal redundant schemes  have  limited fault detection 
capability and limited fault coverage. 
VIII. CONCLUSIONS 
The  continuing  aggressive  CMOS  scaling  causes 
CMPs  to  be  increasingly  vulnerable  to  internal  failures 
and external interferences. In this paper we present TDB, 
a novel core-level fault-tolerant method based on the idea 
of  dynamic  binding.  Learning  from  the  strict  input 
replication  scheme,  we  employ  the  private  caches  to 
maintain  the  master-slave  memory  consistency.  We  also 
extend  a  directory-based  cache  coherence  protocol  to 
implement the master-slave data transferring and achieve 
the objective of master-slave private cache consistency. In 
the  MOESI_FT  protocol,  with  respect  to  private  cache 
misses, the master core is responsible for global memory 
access, while the slave core passively obtains the identical 
data  blocks.  Based  on  transparent  binding,  our  TDB 
proposal  provides  excellent  flexibility  and  scalability  in 
the  tiled-CMP  architecture.  Experimental  results  show 
that,  the  overall  performance  of  TDB  proposal  is  very 
close 
systems, 
outperforming  DCC  by  9.2%,  10.4%,  18%  and  37.1% 
when considering 4, 8, 16 and 32 cores respectively. 
that  of  baseline 
fault-tolerant 
By the way, according to the basic idea of TDB, the 
proposed  execution  model  should  be  also  applicable  for 
multiprocessors  over  chips.  However,  if  the  master  and 
the  slave  locate  in  different  chips,  the  L1  cache  miss 
latency  of  the  pair  may  vary  a  lot.  Moreover,  the 
synchronization  and  checkpointing,  as  well  as 
the 
consumer-consumer  data  transferring,  will  no  doubt 
induce significant performance degradation. So it is more 
suitable  to  limit  the  master-slave  pair  within  the  same 
chip.  In  the  scenario  of  multiprocessors  over  various 
CMPs, we can implement our TDB schemes within each 
CMP.  In  the  scenario  of  multiprocessors  over  various 
to 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
301single-core  chips,  we  may  need  to  group  the  chips 
according  to  their  spatial  relationship,  and  implement 
TDB within each group. 
ACKNOWLEDGMENT 
This  work  was  supported  in  part  by  Natural  Science 
Foundation of China (NSFC) under grant No. (61076018, 
60803031,  60921002),  and  in  part  by  National  Basic 
Research  Program  of  China  (973)  under  grant  No. 
2011CB302503. 
REFERENCES 
[1]  L. Huang, Q. Xu. “Performance Yield-Driven Task Allocation and 
Schedulingfor  MPSoCs  under  Process  Variation.”  In  DAC,  pp. 
326-331, 2010. 
[2]  K. Bowman, J. Tschanz, C. Wilkerson, S. Lu, T. Karnik, V. De , 
Shekhar  Borkar.  “Circuit  Techniques  for  Dynamic  Variation 
Tolerance.” In DAC, pp. 4-7, 2009. 
[3]  R.  Fernandez,  J.  M.  Martinez,  R.  Rodriguez,  M.  Nafria,  X.  H. 
Aymerich. “Gate Oxide Wear-Out and Breakdown Effects on the 
Performance  of  Analog  and  Digital  Circuits.”  IEEE  Electron 
Device, 55(4): 997-1004, 2008. 
[4]  Songjun  Pan,  Yu  Hu,  and  Xiaowei  Li.  “IVF:  Characterizing  the 
Vulnerability of Microprocessor Structures to Intermittent faults.” 
In DATE, pp. 238-243, 2010. 
[5]  X.  Li,  S.  V.  Adve,  P.  Bose,  J.  A.  Rivers.  “Online  Estimation  of 
Architectural  Vulnerability  Factor  for  Soft  Errors.”  In  ISCA,  pp. 
341-352, 2008. 
[6]  P.  Shivakumar,  M.  Kistler,  S.  W.  Keckler,  D.  Burger,  L.  Alvisi. 
“Modeling the effect of technology trends on the soft error rate of 
combinational logic.” In DSN, pp.389-398, 2002. 
[7]  S. Y. Borkar, P. Dubey, K. C. Kahn, D. J. Kuck, H. Mulder, S. S. 
Pawlowski, and J. R. Rattner. “Platform 2015: Intel processor and 
platform  evolution  for  the  next  decade.”  In  Technology@Intel 
Magazine, 2005. 
[8]  S. S. Mukherjee, M. Kontz, S. K. Reinhardt. “Detailed design and 
evaluation  of  redundant  multithreading  alternatives.”  In  ISCA, 
pp.99-110, 2002. 
[9]  M.  Gomaa,  C.  Scarbrough,  T.  N.  Vijaykumar,  I.  Pomeranz. 
“Transient-fault  recovery  for  chip  multiprocessors.”  In  ISCA, 
pp.98-109, 2003. 
[10]  J.  C.  Smolens,  B.  T.  Gold,  B.  Falsafi,  J.  C.  Hoe.  “Reunion: 
Complexity-effective multicore redundancy.” In MICRO, pp.223-
234, 2006. 
[11]  P.  Subramanyan,  V.  Singh,  K.  K.  Saluja,  E.  Larsson. “Energy-
Efficient  Fault  Tolerance  in  Chip  Multiprocessors  Using  Critical 
Value Forwarding.” In DSN, pp.121-130, 2010. 
[12]  C.  LaFrieda,  E.  Ipek,  J.  F.  Martinez,  R.  Manohar.  “Utilizing 
chip 
dynamically 
form 
multiprocessor.”. In DSN, pp.317-326, 2007. 
resilient 
coupled 
cores 
to 
a 
[13]  D. Sanchez, J. L. Aragon, J. M. Garcia. “Evaluating Dynamic Core 
Coupling  in  a  Scalable  Tiled-CMP  Architecture”.  In  WDDD,  in 
conjunction with ISCA, 2008. 
[14]  H. Hossain, S. Dwarkadas, M. C. Huang. “Improving support for 
locality and fine-grain sharing in chip multiprocesors.” In PACT, 
pp.155-165, 2008.  
[15]  L. Cheng, J. Carter, D. Dai. “An adaptive cache coherence protocol 
optimized for producer-consumer sharing.” In HPCA, pp.328-339, 
2007. 
[16]  P.  Stenstrom,  M.  Brorsson,  L.  Sandberg.  “An  adaptive  cache 
coherence  protocol  optimized  for  migratory  sharing.”  In  ISCA, 
pp.109-118, 1993. 
[17]  J.  C.  Smolens,  B.  T.  Gold,  J.  Kim,  B.  Falsafi,  James  C.  Hoe, 
Andreas  G.  Nowatzyk.  “Fingerprinting:  bounding  soft-error 
detection latency and bandwidth.” In ASPLOS, pp.224–234, 2004. 
[18]  P.  Magnusson,  M.  Christensson,  J.  Eskilson,  D.  Forsgren,  G. 
Hallberg,  J.  Haogberg,  F.  Larsson,  A.  Moestedt,  B.Werne. 
“Simics:  A  full  system  simulation  platform.”  IEEE  Computer, 
35(2):50–58, 2002. 
[19]  M. M. K. Martin, D. J. Sorin, B. M. Beckmann, M. R. Marty, M. 
Xu, A. R. Alameldeen, K. E. Moore, M. D. Hill, and D. A. Wood. 
“Multifacet’s  general  execution-driven  multiprocessor  simulator 
(GEMS) toolset.” ACM SIGARCH Computer Architecture News, 
33(4):92–99, 2005. 
[20]  S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. “The 
SPLASH-2  programs:  characterization  and  methodological 
considerations.” In ISCA, pp.24-36, 1995. 
[21]  T.  M.  Austin.  “DIVA:  A  Reliable  Substrate  for  Deep  Submicron 
Microarchitecture Design.” In MICRO, pp.196-207, 1999. 
[22]  E.  Rotenberg.  “AR-SMT:  A  microarchitectural  approach  to  fault 
tolerance in microprocessors.” In FTC, pp.84-91, 1999. 
[23]  M.  Qureshi,  O.  Mutlu,  Y.  Patt.  “Microarchitectural-based 
in 
inspection: 
microprocessors.” In DSN, pp.434-443, 2005. 
transient-fault 
technique 
tolerance 
for 
a 
[24]  J. Ray, J. Hoe and B. Falsafi. “Dual use of superscalar datapath for 
transient-fault  detection  and  recovery.”  In  MICRO,  pp.214-224, 
2001. 
[25]  J.  Smolens,  J.  Kim,  J.  Hoe  and  B.  Falsafi.  “Efficient  resource 
superscalar 
detecting 
sharing 
microarchitectures.” In MICRO, pp.257-268, 2004. 
concurrent 
error 
in 
[26]  D.  Tullsen,  S.  Eggers,  H.  Levy.  “Simultaneous  multithreading: 
Maximizing on-chip parallelism.” In ISCA, pp.392-403, 1995. 
[27]  D. Tullsen, S. Eggers, J. Emer, H. Levy, J. Lo, “Exploiting choice: 
Instruction  fetch  and  issue  on  an  implementable  simultaneous 
multithreading processor.” In ISCA, pp.191-202, 1996. 
[28]  S.  K.  Reinhardt,  S.  S.  Mukherjee.  “Transient  fault  detection  via 
simultaneous multithreading.” In ISCA, pp.25-36, 2000. 
[29]  T.  N.  Vijaykumar,  I.  Pomeranz,  K.  Cheng.  “Transient-Fault 
Recovery Using Simultaneous Multithreading”, In ISCA, pp.87-98, 
2002. 
[30]  Compaq Computer Corporation. “Data integrity for Compaq Non-
Stop Himalaya servers.” http://nonstop.compaq.com, 1999. 
[31]  N. Agarwal, T. Krishna, L. Peh, N. K. Jha. “GARNET: A Detailed 
On-Chip  Network  model  inside  a  full-system  simulator.”  In 
ISPASS, pp.33-42, 2009. 
[32]  O. Mutlu, H. Kim, D. N. Armstrong, Yale, N. Patt. “Understanding 
The  Effects  of  WrongPath  Memory  References  on  Processor 
Performance.” In WMPI, in conjunction with ISCA, 2004. 
[33]  C.  J.  Lee,  H.  Kim,  O.  Mutlu,  Y.  N.  Patt.  “Performance-Aware 
Speculation Control using Wrong Path Usefulness Prediction.” In 
HPCA, pp.39-49, 2008. 
[34]  R. Sendaga, A. Yilmazera, J. J. Yib, A. K. Uhta. “The impact of 
wrong-path  memory  references  in  cache-coherent  multiprocessor 
systems.” ACM Journal of Parallel and Distributed Computing, 67 
(2007): 1256-1269, 2007.  
[35]  G.  A. Reis, J. Chang, N. Vachharajani, R. Rangan, D. I.  August. 
“SWIFT: software implemented fault tolerance.” In CGO, pp.243-
254, 2005. 
[36]  G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, D. I. August, S. 
S. Mukherjee. “Design and evaluation of hybrid fault-detection 
systems. ” In ISCA, pp.148-159, 2005. 
[37]  G. A. Reis, J. Chang, D. I. August. “Automatic instruction-level 
software-only recovery.” In DSN, pp.83-92, 2006. 
[38]  S. Feng, S. Gupta, A. Ansari, S. Mahlke. “Shoestring: Probabilistic 
Soft Error Reliability on the Cheap.” In ASPLOS, pp.385-396, 
2010. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
302