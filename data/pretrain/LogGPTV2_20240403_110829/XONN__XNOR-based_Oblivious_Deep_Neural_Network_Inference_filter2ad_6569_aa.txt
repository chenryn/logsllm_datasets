title:XONN: XNOR-based Oblivious Deep Neural Network Inference
author:M. Sadegh Riazi and
Mohammad Samragh and
Hao Chen and
Kim Laine and
Kristin E. Lauter and
Farinaz Koushanfar
Xonn: XNOR-based Oblivious Deep Neural 
Network Inference
M. Sadegh Riazi and Mohammad Samragh, UC San Diego; Hao Chen, Kim Laine, 
and Kristin Lauter, Microsoft Research; Farinaz Koushanfar, UC San Diego
https://www.usenix.org/conference/usenixsecurity19/presentation/riazi
This paper is included in the Proceedings of the 
28th USENIX Security Symposium.
August 14–16, 2019 • Santa Clara, CA, USA
978-1-939133-06-9
Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.XONN: XNOR-based Oblivious Deep Neural Network Inference
M. Sadegh Riazi
UC San Diego
Mohammad Samragh
Hao Chen
Kim Laine
UC San Diego
Microsoft Research
Microsoft Research
Kristin Lauter
Microsoft Research
Farinaz Koushanfar
UC San Diego
Abstract
1
Introduction
Advancements in deep learning enable cloud servers to pro-
vide inference-as-a-service for clients.
In this scenario,
clients send their raw data to the server to run the deep learn-
ing model and send back the results. One standing chal-
lenge in this setting is to ensure the privacy of the clients’
sensitive data. Oblivious inference is the task of running
the neural network on the client’s input without disclosing
the input or the result to the server. This paper introduces
XONN (pronounced /z2n/), a novel end-to-end framework
based on Yao’s Garbled Circuits (GC) protocol, that pro-
vides a paradigm shift in the conceptual and practical real-
ization of oblivious inference. In XONN, the costly matrix-
multiplication operations of the deep learning model are re-
placed with XNOR operations that are essentially free in GC.
We further provide a novel algorithm that customizes the
neural network such that the runtime of the GC protocol is
minimized without sacriﬁcing the inference accuracy.
We design a user-friendly high-level API for XONN, al-
lowing expression of the deep learning model architecture
in an unprecedented level of abstraction. We further pro-
vide a compiler to translate the model description from high-
level Python (i.e., Keras) to that of XONN. Extensive proof-
of-concept evaluation on various neural network architec-
tures demonstrates that XONN outperforms prior art such
as Gazelle (USENIX Security’18) by up to 7×, MiniONN
(ACM CCS’17) by 93×, and SecureML (IEEE S&P’17) by
37×. State-of-the-art frameworks require one round of in-
teraction between the client and the server for each layer
of the neural network, whereas, XONN requires a constant
round of interactions for any number of layers in the model.
XONN is ﬁrst to perform oblivious inference on Fitnet archi-
tectures with up to 21 layers, suggesting a new level of scala-
bility compared with state-of-the-art. Moreover, we evaluate
XONN on four datasets to perform privacy-preserving med-
ical diagnosis. The datasets include breast cancer, diabetes,
liver disease, and Malaria.
The advent of big data and striking recent progress in ar-
tiﬁcial intelligence are fueling the impending industrial au-
tomation revolution. In particular, Deep Learning (DL) —a
method based on learning Deep Neural Networks (DNNs)
—is demonstrating a breakthrough in accuracy. DL mod-
els outperform human cognition in a number of critical tasks
such as speech and visual recognition, natural language pro-
cessing, and medical data analysis. Given DL’s superior per-
formance, several technology companies are now developing
or already providing DL as a service. They train their DL
models on a large amount of (often) proprietary data on their
own servers; then, an inference API is provided to the users
who can send their data to the server and receive the analy-
sis results on their queries. The notable shortcoming of this
remote inference service is that the inputs are revealed to the
cloud server, breaching the privacy of sensitive user data.
Consider a DL model used in a medical task in which
a health service provider withholds the prediction model.
Patients submit their plaintext medical information to the
server, which then uses the sensitive data to provide a med-
ical diagnosis based on inference obtained from its propri-
etary model. A naive solution to ensure patient privacy is
to allow the patients to receive the DL model and run it
on their own trusted platform. However, this solution is
not practical in real-world scenarios because: (i) The DL
model is considered an essential component of the service
provider’s intellectual property (IP). Companies invest a sig-
niﬁcant amount of resources and funding to gather the mas-
sive datasets and train the DL models; hence, it is important
to service providers not to reveal the DL model to ensure
their proﬁtability and competitive advantage.
(ii) The DL
model is known to reveal information about the underlying
data used for training [1]. In the case of medical data, this
reveals sensitive information about other patients, violating
HIPAA and similar patient health privacy regulations.
Oblivious inference is the task of running the DL model
on the client’s input without disclosing the input or the re-
USENIX Association
28th USENIX Security Symposium    1501
sult to the server itself. Several solutions for oblivious in-
ference have been proposed that utilize one or more cryp-
tographic tools such as Homomorphic Encryption (HE) [2,
3], Garbled Circuits (GC) [4], Goldreich-Micali-Wigderson
(GMW) protocol [5], and Secret Sharing (SS). Each of these
cryptographic tools offer their own characteristics and trade-
offs. For example, one major drawback of HE is its compu-
tational complexity. HE has two main variants: Fully Ho-
momorphic Encryption (FHE) [2] and Partially Homomor-
phic Encryption (PHE) [3, 6]. FHE allows computation on
encrypted data but is computationally very expensive. PHE
has less overhead but only supports a subset of functions or
depth-bounded arithmetic circuits. The computational com-
plexity drastically increases with the circuit’s depth. More-
over, non-linear functionalities such as the ReLU activation
function in DL cannot be supported.
GC, on the other hand, can support an arbitrary function-
ality while requiring only a constant round of interactions
regardless of the depth of the computation. However, it has
a high communication cost and a signiﬁcant overhead for
multiplication. More precisely, performing multiplication
in GC has quadratic computation and communication com-
plexity with respect to the bit-length of the input operands.
It is well-known that the complexity of the contemporary
DL methodologies is dominated by matrix-vector multiplica-
tions. GMW needs less communication than GC but requires
many rounds of interactions between the two parties.
A standalone SS-based scheme provides a computation-
ally inexpensive multiplication yet requires three or more
independent (non-colluding) computing servers, which is a
strong assumption. Mixed-protocol solutions have been pro-
posed with the aim of utilizing the best characteristics of
each of these protocols [7, 8, 9, 10]. They require secure
conversion of secrets from one protocol to another in the
middle of execution. Nevertheless, it has been shown that
the cost of secret conversion is paid off in these hybrid solu-
tions. Roughly speaking, the number of interactions between
server and client (i.e., round complexity) in existing hybrid
solutions is linear with respect to the depth of the DL model.
Since depth is a major contributor to the deep learning ac-
curacy [11], scalability of the mixed-protocol solutions with
respect to the number of layers remains an unsolved issue for
more complex, many-layer networks.
This paper introduces XONN, a novel end-to-end frame-
work which provides a paradigm shift in the conceptual
and practical realization of privacy-preserving interference
on deep neural networks. The existing work has largely
focused on the development of customized security proto-
cols while using conventional ﬁxed-point deep learning al-
gorithms. XONN, for the ﬁrst time, suggests leveraging the
concept of the Binary Neural Networks (BNNs) in conjunc-
tion with the GC protocol. In BNNs, the weights and acti-
vations are restricted to binary (i.e, ±1) values, substituting
the costly multiplications with simple XNOR operations dur-
ing the inference phase. The XNOR operation is known to be
free in the GC protocol [12]; therefore, performing oblivious
inference on BNNs using GC results in the removal of costly
multiplications. Using our approach, we show that oblivious
inference on the standard DL benchmarks can be performed
with minimal, if any, decrease in the prediction accuracy.
We emphasize that an effective solution for oblivious in-
ference should take into account the deep learning algo-
rithms and optimization methods that can tailor the DL
model for the security protocol. Current DL models are
designed to run on CPU/GPU platforms where many multi-
plications can be performed with high throughput, whereas,
bit-level operations are very inefﬁcient. In the GC protocol,
however, bit-level operations are inexpensive, but multipli-
cations are rather costly. As such, we propose to train deep
neural networks that involve many bit-level operations but
no multiplications in the inference phase; using the idea of
learning binary networks, we achieve an average of 21× re-
duction in the number of gates for the GC protocol.
We perform extensive evaluations on different datasets.
Compared to the Gazelle [10] (the prior best solution) and
MiniONN [9] frameworks, we achieve 7× and 93× lower
inference latency, respectively. XONN outperforms DeepSe-
cure [13] (prior best GC-based framework) by 60× and
CryptoNets [14], an HE-based framework, by 1859×. More-
over, our solution renders a constant round of interactions
between the client and the server, which has a signiﬁcant ef-
fect on the performance on oblivious inference in Internet
settings. We highlight our contributions as follows:
• Introduction of XONN, the ﬁrst framework for privacy pre-
serving DNN inference with a constant round complexity
that does not need expensive matrix multiplications. Our
solution is the ﬁrst that can be scalably adapted to ensure
security against malicious adversaries.
• Proposing a novel conditional addition protocol based on
Oblivious Transfer (OT) [15], which optimizes the costly
computations for the network’s input layer. Our protocol
is 6× faster than GC and can be of independent interest.
We also devise a novel network trimming algorithm to re-
move neurons from DNNs that minimally contribute to the
inference accuracy, further reducing the GC complexity.
• Designing a high-level API to readily automate fast adap-
tation of XONN, such that users only input a high-level
description of the neural network. We further facilitate the
usage of our framework by designing a compiler that trans-
lates the network description from Keras to XONN.
• Proof-of-concept implementation of XONN and evaluation
on various standard deep learning benchmarks. To demon-
strate the scalability of XONN, we perform oblivious infer-
ence on neural networks with as many as 21 layers for the
ﬁrst time in the oblivious inference literature.
1502    28th USENIX Security Symposium
USENIX Association
2 Preliminaries
Throughout this paper, scalars are represented as lower-
case letters (x ∈ R), vectors are represented as bold lower-
case letters (x ∈ Rn), matrices are denoted as capital letters
(X ∈ Rm×n), and tensors of more than 2 ways are shown us-
ing bold capital letters (X ∈ Rm×n×k). Brackets denote ele-
ment selection and the colon symbol stands for all elements
—W [i, :] represents all values in the i-th row of W .
2.1 Deep Neural Networks
The computational ﬂow of a deep neural network is com-
posed of multiple computational layers. The input to each
layer is either a vector (i.e., x ∈ Rn) or a tensor (i.e., X ∈
Rm×n×k). The output of each layer serves as the input of the
next layer. The input of the ﬁrst layer is the raw data and the
output of the last layer represents the network’s prediction
on the given data (i.e., inference result). In an image classi-
ﬁcation task, for instance, the raw image serves as the input
to the ﬁrst layer and the output of the last layer is a vector
whose elements represent the probability that the image be-
longs to each category. Below we describe the functionality
of neural network layers.
Linear Layers:Linear operations in neural networks are per-
formed in Fully-Connected (FC) and Convolution (CONV)
layers. The vector dot product (VDP) between two vectors
x ∈ Rn and w ∈ Rn is deﬁned as follows:
VDP (x, w) =
n
∑
i=1
w[i] · x[i].
(1)
Both CONV and FC layers repeat VDP computation to gen-
erate outputs as we describe next. A fully connected layer
takes a vector x ∈ Rn and generates the output y ∈ Rm using
a linear transformation:
y = W · x + b,
(2)
where W ∈ Rm×n is the weight matrix and b ∈ Rm is a bias
vector. More precisely, the i-th output element is computed
as y[i] = VDP (W [i, :], x) + b[i].
A convolution layer is another form of linear transforma-
tion that operates on images. The input of a CONV layer
is represented as multiple rectangular channels (2D images)
of the same size: X ∈ Rh1×h2×c, where h1 and h2 are the
dimensions of the image and c is the number of channels.
The CONV layer maps the input image into an output image
Y ∈ Rh1′×h2′× f . A CONV layer consists of a weight tensor
W ∈ Rk×k×c× f and a bias vector b ∈ R f . The i-th output
channel in a CONV layer is computed by sliding the kernel
W[:, :, :, i] ∈ Rk×k×c over the input, computing the dot prod-
uct between the kernel and the windowed input, and adding
the bias term b[i] to the result.
Non-linear Activations: The output of linear transforma-
tions (i.e., CONV and FC) is usually fed to an activation
layer, which applies an element-wise non-linear transforma-
tion to the vector/tensor and generates an output with the
In this paper, we particularly utilize
same dimensionality.
the Binary Activation (BA) function for hidden layers. BA
maps the input operand to its sign value (i.e., +1 or −1).
Batch Normalization: A batch normalization (BN) layer
is typically applied to the output of linear layers to normal-
ize the results. If a BN layer is applied to the output of a
CONV layer, it multiplies all of the i-th channel’s elements
by a scalar γγγ[i] and adds a bias term βββ [i] to the resulting
If BN is applied to the output of an FC layer, it
channel.
multiplies the i-th element of the vector by a scalar γγγ[i] and
adds a bias term βββ [i] to the result.
Pooling:
Pooling layers operate on image channels out-
putted by the CONV layers. A pooling layer slides a window
on the image channels and aggregates the elements within
the window into a single output element. Max-pooling and
Average-pooling are two of the most common pooling oper-
ations in neural networks. Typically, pooling layers reduce
the image size but do not affect the number of channels.
2.2 Secret Sharing
A secret can be securely shared among two or multiple par-
ties using Secret Sharing (SS) schemes. An SS scheme
guarantees that each share does not reveal any information
about the secret. The secret can be reconstructed using
all (or subset) of shares.
In XONN, we use additive se-
cret sharing in which a secret S is shared among two par-
2b (integers mod-
ulo 2b) as the ﬁrst share and creating the second share as
ties by sampling a random number!S1 ∈R Z
!S2 = S −!S1 mod 2b where b is the number of bits to describe
cret as S =!S1 +!S2 mod 2b. Suppose that two secrets S(1)
and S(2) are shared among two parties where party-1 has!S(1)
and !S(2)
and !S(2)
share of the sum of two secrets as!S(1)
2 . Party-i can create a
i mod 2b without
communicating to the other party. This can be generalized
for arbitrary (more than two) number of secrets as well. We
utilize additive secret sharing in our Oblivious Conditional
Addition (OCA) protocol (Section 3.3).
the secret. While none of the shares reveal any information
about the secret S, they can be used to reconstruct the se-
1
1
and party-2 has !S(1)
2
i +!S(2)
2.3 Oblivious Transfer
One of the most crucial building blocks of secure computa-
tion protocols, e.g., GC, is the Oblivious Transfer (OT) pro-
tocol [15]. In OT, two parties are involved: a sender and a re-
ceiver. The sender holds n different messages m j, j = 1...n,
with a speciﬁc bit-length and the receiver holds an index
(ind) of a message that she wants to receive. At the end of
the protocol, the receiver gets mind with no additional knowl-
edge about the other messages and the sender learns noth-
ing about the selection index. In GC, 1-out-of-2 OT is used
where n = 2 in which case the selection index is only one
bit. The initial realizations of OT required costly public key
USENIX Association
28th USENIX Security Symposium    1503
encryptions for each run of the protocol. However, the OT
Extension [16, 17, 18] technique enables performing OT us-
ing more efﬁcient symmetric-key encryption in conjunction
with a ﬁxed number of base OTs that need public-key en-
cryption. OT is used both in the OCA protocol as well as the
Garbled Circuits protocol which we discuss next.
a result, costly multiplication operations are replaced with
XNOR gates which are essentially free in GC. We describe
the training process in Section 3.1. In Section 3.2, we ex-
plain the operations and their corresponding Boolean circuit
designs that enable a very fast oblivious inference. In Sec-
tion 4, we elaborate on XONN implementation.
2.4 Garbled Circuits
3.1 Customized Network Binarization
Yao’s Garbled Circuits [4], or GC in short, is one of the
generic two-party secure computation protocols. In GC, the
result of an arbitrary function f (.) on inputs from two parties
can be computed without revealing each party’s input to the
other. Before executing the protocol, function f (.) has to be
described as a Boolean circuit with two-input gates.
GC has three main phases: garbling, transferring data, and
evaluation. In the ﬁrst phase, only one party, the Garbler, is
involved. The Garbler starts by assigning two randomly gen-
erated l-bit binary strings to each wire in the circuit. These
binary strings are called labels and they represent semantic
values 0 and 1. Let us denote the label of wire w correspond-
ing to the semantic value x as Lw
x . For each gate in the circuit,
the Garbler creates a four-row garbled table as follows. Each