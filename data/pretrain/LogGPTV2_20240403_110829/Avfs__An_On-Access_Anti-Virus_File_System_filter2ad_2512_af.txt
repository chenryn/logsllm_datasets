User
System
1487s
150s
158s
234s
247s
238s
252s
Ext3
HBEDV
Oyster/RI
Oyster/RD
Oyster/NI
Oyster/ND
ClamAV
Figure 12: Postmark results. For Avfs, R represents REGU-
LAR mode, F FULL, I IMMEDIATE, and D DEFERRED.
Postmark is an I/O intensive benchmark, which cre-
ates a lot of ﬁles and performs read and write operations
on these ﬁles. The ﬁles are accessed at random, which
results in a ﬁle being opened and closed several times
during the benchmark.
When a ﬁle is created in Postmark, Avfs scans this ﬁle
and creates a state ﬁle for it. Once a state ﬁle is present,
no additional scanning is required during subsequent
reads. Since all the writes are appends, the state in the
state ﬁle is always valid and only the last page(s) ahead
of the current scanned page are scanned. ClamAV’s
scanner is called several times on the open and close
system calls to scan entire ﬁles, which contributes to its
overhead. The slowest mode of Avfs, FULL/DEFERRED,
takes about 252 seconds and Ext3 takes 150 seconds,
which is an overhead of 68%. For the same benchmark,
ClamAV takes 1,487 seconds—a factor of 9.9 slower.
H+BEDV has a heuristics engine that allows it to
determine if a ﬁle needs to be checked for viruses by
looking at the ﬁrst few bytes in the ﬁle. This allows
H+BEDV to skip scanning entire ﬁles of types that can-
not be infected. Although it is possible to suppress
the heuristics engine of H+BEDV in the command-line
scanner, this option is not available in the on-access
scanner. Due to its heuristics engine, H+BEDV shows
almost identical performance to Ext3 with a 5% over-
head in elapsed time but the system time increases by a
factor of 5.9.
6.6 Scan Engine Evaluation
Our test consisted of scanning two large ﬁles, one with
random data and the other with data that contained exe-
cutable code from library ﬁles. None of the test ﬁles con-
tained any viruses so the ﬁles were completely scanned.
When a ﬁle is given to a command-line scanner for
scanning, it needs to set up the scanning trie before scan-
ning can start. The time taken to set up this trie depends
on the size of the virus database, so we used the same
size database as the other scanner for Oyster. For ex-
ample, when comparing it to Sophos we used 86,755
patterns, because that is what Sophos reports as their
database size.
)
c
e
s
(
i
e
m
T
d
e
s
p
a
E
l
 600
 500
 400
 300
 200
 100
 0
574s
Wait
User
System
252s
278s
398s
150s
127s
Clam/Rand
Clam/Lib
Oyster/Rand
Height 
Oyster/Lib
Height 
Oyster/Rand
Height 
Oyster/Lib
Height 
Figure 13: File scan times for Oyster and ClamAV. The
database had 19,807 patterns.
Figure 13 compares the performance of ClamAV with
Oyster. We ran the benchmark for Oyster once with the
minimum and maximum heights set to two, and once
with the heights set to 3 and 4, respectively. The h2; 2i
setting matches the ClamAV trie structure, and h3; 4i op-
timizes Oyster’s performance for this benchmark.
For the Oyster scanner conﬁgured with h2; 2i, the
random ﬁle scan was 26 seconds slower, while the li-
brary ﬁle scan was 176 seconds faster than ClamAV. The
Oyster scanner was faster for the library scan because
the internal state structure maintains partially-matched
patterns between successive calls to the scanner. The
ClamAV scanner does not have such a structure.
In-
stead, it rescans some of the text from the previous buffer
so that patterns that span multiple buffers are detected.
In a library ﬁle scan, ClamAV scanned a total of 6.2
billion patterns, while Oyster scanned 1.8% fewer pat-
terns.
In the random ﬁle benchmark, ClamAV again
scanned 1.8% more patterns. Even though the Oyster
scanner scanned six million fewer patterns, the over-
head of maintaining the state, which includes additional
malloc calls and linked list operations, exceeded the
savings gained in scanning fewer patterns.
Increasing the trie height parameters to h3; 4i signiﬁ-
cantly reduces the number of patterns scanned by Oys-
ter. For the random ﬁle benchmark, Oyster scanned 369
times fewer patterns. For the library benchmark, Oyster
scanned 52 times fewer patterns.
Figure 14 compares H+BEDV with Oyster.
In this
benchmark, we ran the command-line scanner from
H+BEDV. We conﬁgured H+BEDV so that it scans all
input without the heuristics engine. H+BEDV is slower
with random input than with the library ﬁle. This sug-
gests that the commercial H+BEDV scan engine is op-
timized to scan executable content, possibly by using a
different scanning mechanism. The random ﬁle scan of
i
)
c
e
s
(
e
m
T
d
e
s
p
a
E
l
 400
 350
 300
 250
 200
 150
 100
 50
 0
354s
Wait
User
System
155s
183s
57s
HBEDV/Rand
HBEDV/Lib
Oyster/Rand
Oyster/Lib
Figure 14: File scan times for Oyster and H+BEDV. The
database had 66,393 patterns.
H+BEDV is 6.2 times slower than H+BEDV’s scan of
the library ﬁle. For 1GB of random input, Oyster is 56%
faster than H+BEDV. For a 1GB library ﬁle however,
Oyster is 3.2 times slower than H+BEDV.
)
c
e
s
(
i
e
m
T
d
e
s
p
a
E
l
 250
 200
 150
 100
 50
 0
Wait
User
System
77s
105s
236s
152s
Sophos/Rand
Sophos/Lib
Oyster/Rand
Oyster/Lib
Figure 15: File scan times for Oyster and Sophos. The
database had 86,755 patterns.
Figure 15 compares Sophos, an optimized commer-
cial product, with Oyster. Oyster is slower by 97% for
random input and by 124% for the library ﬁle, suggest-
ing that Oyster can be further optimized in the future.
7 Conclusions
The main contribution of our work is that for the ﬁrst
time, to the best of our knowledge, we have implemented
a true on-access state-oriented anti-virus solution that
scans input ﬁles for viruses on reads and writes.
(cid:15) Avfs intercepts ﬁle access operations (including
memory-mapped I/O) at the VFS level unlike other
on-access systems that intercept the open, close,
and exec system calls. Scanning during read and
write operations reduces the possibility of a virus
attack and can trap viruses before they are written
to disk. In addition to providing data consistency by
backing up ﬁles, the forensic modes of operation in
Avfs provide means to track malicious activity by
recording information about malicious processes.
(cid:15) Our Oyster scan engine improves the performance
of the pattern-matching algorithm using variable
trie heights, and scales efﬁciently for large database
sizes. State-based scanning in Oyster allows us to
scan a buffer of data in parts. This state-oriented
design reduces the amount of scanning required by
performing partial ﬁle-scanning.
(cid:15) By separating the ﬁle system (Avfs) from the scan-
engine (Oyster), we have made the system ﬂexible
and extensible, allowing third-party virus scanners
to be integrated into our system.
7.1 Future Work
We plan to improve the Oyster scanning engine in a vari-
ety of ways. Oyster scans all ﬁles, even those that are not
executable. We plan to allow Oyster to scan for viruses
within only speciﬁc ﬁle types. For example, when scan-
ning a Microsoft Ofﬁce document, Oyster will scan only
for macro viruses. Since some viruses can only occur
in certain segments of a ﬁle, it is not necessary to scan
for them in the rest of the ﬁle. We plan to integrate po-
sitional matching into Oyster, so that only relevant por-
tions of ﬁles are scanned. Also, we plan to scan for all
patterns on a leaf node of the scanning trie simultane-
ously instead of scanning for each pattern sequentially,
thereby improving the scan engines performance.
We plan to maintain Avfs state for more than one page
per ﬁle. These states can be used to scan for multi-part
patterns efﬁciently even during random writes. Our in-
vestigation will evaluate trade-offs between storing more
and less state information. We plan to add more forensic
features to the deferred mode, such as terminating the
offending processes, and storing the core dump of the
process along with other useful evidence of malicious
activity. We also plan to integrate a versioning engine
[12] with Avfs to support multiple levels of versioning.
We can keep track of changes to a ﬁle across several ver-
sions to provide more accurate forensics.
The Oyster scan engine can also be applied to generic
pattern matching. Rather than using a database of
viruses, Oyster could use a database of keywords. For
example, a brokerage ﬁrm could ﬂag ﬁles for review by
a compliance ofﬁcer. Other companies could ﬂag key-
words related to trade secrets. We plan to investigate
what ﬁle system policies would be useful for such appli-
cations.
8 Acknowledgments
We thank the Usenix Security reviewers for the valuable
feedback they provided. We also thank the ClamAV
developers for providing an open source virus scanner
and for their useful comments during the development
of Oyster. This work was partially made possible by an
NSF CAREER award EIA-0133589, NSF Trusted Com-
puting award CCR-0310493, and HP/Intel gift numbers
87128 and 88415.1.
References
[1] A. V. Aho and M. J. Corasick. Efﬁcient string matching:
an aid to bibliographic search. Communications of the
ACM, 18(6):333–340, June 1975.
[2] A. Aranya, C. P. Wright, and E. Zadok. Tracefs: A
File System to Trace Them All. In Proceedings of the
Third USENIX Conference on File and Storage Tech-
nologies (FAST 2004), pages 129–143, San Francisco,
CA, March/April 2004.
[3] F. Cohen. Computer viruses: theory and experiments.
Computers and Security, 6(1):22–35, 1987.
[4] Computer Associates.
eTrust.
www3.ca.com/
Solutions, 2004.
[5] D. Ellard and M. Seltzer. NFS Tricks and Benchmarking
Traps.
In Proceedings of the Annual USENIX Techni-
cal Conference, FREENIX Track, pages 101–114, June
2003.
[6] H+BEDV Datentechnik GmbH.
Dazuko.
www.
dazuko.org, 2004.
[7] H+BEDV Datentechnik GmbH. H+BEDV.
www.
hbedv.com, 2004.
[8] J. Elson and A. Cerpa. Internet Content Adaptation Pro-
tocol (ICAP). Technical Report RFC 3507, Network
Working Group, April 2003.
[9] Kaspersky Lab. Kaspersky. www.kaspersky.com,
2004.
[10] J. Katcher. PostMark: a New Filesystem Benchmark.
Technical Report TR3022, Network Appliance, 1997.
www.netapp.com/tech_library/3022.html.
[11] T. Kojm. ClamAV. www.clamav.net, 2004.
[12] K. Muniswamy-Reddy, C. P. Wright, A. Himmer, and
E. Zadok. A Versatile and User-Oriented Versioning
File System. In Proceedings of the Third USENIX Con-
ference on File and Storage Technologies (FAST 2004),
pages 115–128, San Francisco, CA, March/April 2004.
[13] Network Associates Technology, Inc. McAfee. www.
mcafee.com, 2004.
[14] J. S. Pendry, N. Williams, and E. Zadok. Am-utils User
Manual, 6.1b3 edition, July 2003. www.am-utils.
org.
[15] J. Reynolds. The Helminthiasis of the Internet. Technical
Report RFC 1135, Internet Activities Board, December
1989.
[16] R. Richardson. Computer Crime and Security Survey.
Computer Security Institute, VIII(1):1–21, 2003. www.
gocsi.com/press/20030528.html.
[17] Sophos. Sophos Plc. www.sophos.com, 2004.
[18] Symantec. Norton Antivirus. www.symantec.com,
2004.
[19] E. Zadok and J. Nieh. FiST: A Language for Stackable
In Proceedings of the Annual USENIX
File Systems.
Technical Conference, pages 55–70, June 2000.