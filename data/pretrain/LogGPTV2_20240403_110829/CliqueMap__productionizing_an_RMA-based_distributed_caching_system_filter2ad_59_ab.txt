region and data region. The index region consists of fixed-size Buck-
ets, and each Bucket contains a number of fixed-size tuples, known
as IndexEntries. An IndexEntry is tagged with a key hash and has
a pointer (a memory region identifier, offset, size) that indicates a
position in the data region, wherein the actual KV pair is stored.
Multiple DataEntries reside in the data region, which is managed by
backend RPC handlers (¬ß4).
2√óR GETs (Figure 2). The baseline CliqueMap GET operation,
which we refer to as 2xR, relies on two RMA reads in sequence,
operating on the index and data regions, respectively. (1) The client
computes a hash mapping the Key (an arbitrary string) to a fixed-size
KeyHash, which uniquely identifies a backend and Bucket associated
with the Key. Then, (2) the client fetches the associated Bucket via
an RMA read. (3) The client scans the Bucket for an IndexEntry with
the desired KeyHash. If there is no match, then the client declares a
miss. Otherwise (4), the client issues a second RMA read to fetch the
potentially-matching DataEntry. (5) On completion of the second
RMA read, the client validates the response by: (a) validating the
checksum end-to-end, to guard against tearing, and (b) verifying
that the DataEntry contains the desired Key (not merely KeyHash),
guarding against a (very) rare 128-bit hash collision.
SETs. The client issues an RPC, including the KV pair to be SET,
to the appropriate backend. On receiving a SET RPC, the backend
Figure 1: CliqueMap index region and data region layouts.
RPC KVCS. Memcached [4] is a well-known KVCS, built upon
conventional kernel networking primitives. Twitter uses a forked ver-
sion of Memcached, known as Twemcache [41], to serve a majority
of its caching traffic. Google, too, has its own internal version, known
as MemcacheG, a translation of Memcached, using Stubby RPC‚Äî
Google‚Äôs production-grade RPC‚Äîas its transport. By building on
Stubby RPC, MemcacheG inherits critical productionization features
from its RPC framework, such as application-to-application authen-
tication (ALTS [2]), integrity protection, forward- and backward-
versioning assistance, per-RPC ACLs, and interoperability across
multiple languages.
This feature wealth comes at a CPU cost; even a well-optimized
Stubby RPC incurs >50 CPU-ùúás across client and server‚Äîfar higher
overheads than those of state-of-the-art academic RPC prototypes
(e.g., eRPC [22]) which tend to focus on performance to the exclu-
sion of production requirements such as authentication, privacy/en-
cryption, and privilege model. In contrast, Stubby strongly favors
feature richness to support the needs of tens of thousands of non-
network-expert professional software engineers. For most use cases,
Stubby‚Äôs benefits far outweigh its costs, but for an in-memory KVCS,
a minimum CPU cost of 50ùúás per op eclipses the CPU cost of simply
accessing memory. Such a cost limits the usefulness of distributed
caching, especially for systems with large working sets for which
distributed storage is a critical means to husband expensive DRAM
(¬ß6.5).
RMA KVCS. A number of systems advocate using RMA as the net-
work transport for KVCS dataplane primitives [17, 23]. Challenges
arise when intersecting the core ideas of these systems with the
requirements of Table 1. Systems built entirely atop RMA require
careful coordination between client and server binaries, making
post-deployment evolution a slow process. Simplifying assump-
tions around failures, hardware homogeneity, or viability of memory
pre-allocation may simply not hold in practice, and can lead to
performant but otherwise complex/impractical systems.
RMA/RPC hybrid KVCS systems. Hybrids [33, 35, 40] offer a
middle ground, and can more obviously accommodate the afore-
mentioned requirements around tolerance for heterogeneity and
post-deployment agility. Such hybrid designs refute RMA or RPC
as a false dichotomy, observing that both are useful building blocks
for higher-level systems, and use of one can complement, rather
than exclude, the other. We embrace this philosophy throughout the
CliqueMap design.
95
Index Region BucketIndexEntryIndexEntryIndexEntry BucketIndexEntryIndexEntryIndexEntryData RegionDataEntryDataEntryDataEntryFreeKeyHashVersionNumberPointerIndexEntryKey LengthKeyData LengthDataChecksumFormatDataEntryMetadataBucket FetchClientServerNICSWNICSWGET CompleteData FetchSIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
A. Singhvi et al.
allocates capacity for the new DataEntry in the data region, prepares
an RMA-friendly pointer and scans the relevant Bucket in the index
region for an existing mapping. If such an entry is found, the backend
overwrites it with the new pointer, and reclaims the old DataEntry as
free space. Backends apply SETs only when doing so monotonically
increases a particular KV pair‚Äôs version (see ¬ß5.2).
Clients. The CliqueMap client library transparently retries GET/SET
operations to overcome transient failures, such as checksum vali-
dation failures that can arise under a race, subject to both a user-
specified deadline and retry count. Retries occur at a layer appropri-
ate to the error (e.g., checksum failures may retry RMA operations,
but failed RMA operations may retry on new connections, etc.).
This basic design delivers RMA‚Äôs intrinsic performance advan-
tages for common-case read operations, and use of RPCs for SETs/
mutations eases the complexity of ensuring consistency, but not all
key requirements are met. The challenges outlined in Table 1 call
for further augmentation:
1. Memory efficiency via RPC-based mutations. CliqueMap allo-
cates and manages memory capacity locally, triggered by RPCs that
run entirely in the backends. Using straightforward code, these back-
ends implement rich replacement and allocation policies, and can
restructure the index and data regions on demand (e.g., via defrag-
mention/replacement) or even trigger background processes (e.g.,
memory reshaping in ¬ß4.1). CliqueMap‚Äôs self-validating lookup
mechanisms ensure detection and retry of any resulting races. Impor-
tantly, these mechanisms make it possible for CliqueMap backends
to resize their memory footprint on demand, rather than wastefully
pre-allocate/pre-register for peak memory capacity on startup.
2. Lightweight replication with client-based quoruming. To deal
with slow or failed servers, CliqueMap offers deployment modes
in which data is replicated across multiple backends. To realize
availability benefits with minimal performance cost, CliqueMap
adopts an uncoordinated replication approach in which replicas do
not synchronize in the serving path, and clients use load-aware
quoruming to resolve data consistency.
3. Self-validating responses coupled with retries as a key build-
ing block. While self-validating responses and retries resolve race
resolution in the basic design, we also find them to be key enablers
in supporting seamless binary upgrades and recovering from failures.
Self-validating responses from the server make the client aware of
transient changes occurring at backends, and trigger the clients to
retry at the appropriate layer of the stack. Due to the number of
backends globally and weekly upgrade schedule, upgrades are the
norm and this approach simplifies delivery of ‚Äúhitless‚Äù upgrades.
4. Decoupled design for non‚ÄìC-family clients. CliqueMap pro-
vides support for Java, Go, and Python clients via language-specific
shims, enabling non‚ÄìC-family internal components of a broader
system to access the corpora stored in CliqueMap and thereby eas-
ing adoption of CliqueMap in established multi-language serving
ecosystems. Each language shim launches a subprocess, which in
turn contains the primary C++ client library, thereby side-stepping
error-prone native-code invocation mechanisms, maximizing code
reuse, and unifying debugging processes among all languages.
5. Leverage software transports for heterogeneity and perfor-
mance. Because our datacenters operate across several generations
of networks, CliqueMap tightly integrates with Google‚Äôs software-
defined NIC, Pony Express [31]. This integration includes a cus-
tom RMA operation that matches precisely the semantics of the
CliqueMap GET operation, enabling most GETs to complete with
a single network round trip (see ¬ß6.3). Further, CliqueMap offers
RPCs as a seamless fallback for lookups in scenarios wherein RMA
protocols are not applicable/available (e.g., lookup over WAN).
4 Backend Responsibilities
CliqueMap‚Äôs backend memory layout is designed to accommodate
2√óR-style GETs while also maintaining significant freedom to relo-
cate data and change protocol over time, a balance relying heavily
on the self-verifying properties realized by client-side validation and
retry (¬ß3). Critically, client-side validation ensures that server-side
modification of a KV pair or associated metadata implicitly poisons
the operation. Although rarely triggered in practice (less than 0.01%
of all ops), such retries grant the backend code significant freedom
when adjusting memory layout, simplifying both defragmentation
and, later in the design‚Äôs evolution, dynamic resizing. Ultimately,
server-side logic need only be concerned about making retryable
conditions transient, detectable, and rare, rather than entirely invisi-
ble.
4.1 Memory Allocation and Reshaping
Index Region Allocation and Reshaping. Index region memory
allocation is straightforward and is initially provisioned on backend
restart based on the expected key count in the underlying corpus.
Crucially, indexes can be upsized at runtime when they surpass a
target load factor. During such reshaping, the backend creates a new,
second index, populates it, and then revokes remote access to the
original index. At this point, client-initiated RMA operations fail;
clients enacting retries for failed RMAs contact backends via RPC
as part of their retry procedure for such errors, at which time the
client also learns the new per-backend index size. For simplicity,
mutations stall during an index resize.
Data Region Allocation. Because the data region is random-access
in nature, the memory pool for DataEntries is governed by a slab-
based allocator [11] and tuned to the deployment‚Äôs workload. Slabs
can be repurposed to different size classes as values come and go in
the lifetime of the backend task. Because all allocations occur within
an RPC, allocation logic can freely use the familiar programming
abstraction provided by RPCs.
Data Region Reshaping. Memory registration for RMA is widely
recognized to be expensive, as it requires the operating system to
communicate with the RMA-capable NIC, to pin memory, and to
manipulate translation tables. Naive designs that pre-allocate all
backend memory at startup‚Äîand thereby avoid memory registra-
tion at runtime‚Äîare tempting, but we found this approach strands
DRAM and thereby risks high operating costs. Taking advantage of
freely-relocatable DataEntries, CliqueMap‚Äôs DataEntry pool resizes
on demand, so that deployments can provision for common-case,
rather than peak, DRAM usage. When nearing current capacity, an
individual backend task can asynchronously grow (reshape) its data
region. Reshaping works by pre-allocating the maximum possible
virtual address range needed to serve the entirety of a machine‚Äôs
memory capacity1, but only ever populating a subset of the address
1via mmap(PROT_NONE) of a very large virtual range.
96
CliqueMap: Productionizing an RMA-Based Distributed Caching System
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Figure 3: Memory reshaping in CliqueMap and subsequent
DRAM savings.
eviction anywhere in the data pool suffices.
range. That is, the data pool is always virtually contiguous, but not
fully backed by DRAM. During expansion, CliqueMap establishes
a second, larger, overlapping RMA memory window2, and begins
advertising this new window to clients as the data region. Clients
converge over time to using the second window exclusively, per-
haps after a retry. Because kernel memory management operations
have unpredictable duration, CliqueMap initiates growth according
to a high-watermark policy, performing such work off the critical
path, but triggered by some RPC-based operation. As with the index
region, data region downsizing occurs with non-disruptive restart.
Rollout of the reshaping feature saved 10% (50TB) of customer
DRAM at launch (see Figure 3). Shortly thereafter the underly-
ing corpus itself shrank, and without further human intervention
CliqueMap dropped its DRAM usage by 50% (200TB). Since each
individual backend makes an independent scaling decision, the ag-
gregate savings is derived from the sum of many independent scaling
decisions, and fluctuates in time.
4.2 Cache Eviction
A mutation (via RPC) triggers cache eviction when it encounters
one of two conflict conditions:
‚Ä¢ Capacity Conflict. No spare capacity in the data region. An
‚Ä¢ Associativity Conflict. No spare IndexEntry in the key‚Äôs Bucket.
For the newly-installed KV to be RMA-accessible, an existing
KV pair must be evicted from the Bucket.
The latter is a consequence of the set-associative but RMA-
friendly data structure, which we mitigate by dynamically scaling
the index (¬ß4.1) to make associativity conflicts rare. CliqueMap also
offers an optional RPC fallback in the case of a Bucket overflow,
indicated by a bit in the Bucket. Clients encountering an overflowed
bucket may optionally send an RPC to the backend [29], incurring
higher latency but still serving a hit‚Äîa tradeoff appropriate when
the downstream cost of a cache miss is high relative to the RPC cost.
Because CliqueMap uses RMAs for GETs, backends have no di-
rect record of access information to facilitate recency-based eviction
algorithms, such as LRU. Instead, clients inform backends of data
touches via RPC, as a batched background process, to amortize RPC
overheads. Backends ingest access records en masse to implement
configurable eviction policies‚ÄîLRU, ARC [32], and others.
Eviction Procedure. Like reshaping, the eviction mechanism again
relies on the self-verifying properties inherent in the design. Because
a checksum covers the IndexEntry and DataEntry in combination, the
RPC handler processing an eviction can nullify IndexEntry pointers
and modify DataEntry contents. Subtly, this admits interleavings in
Figure 4: Sequence diagram of GETs in CliqueMap R=3.2
mode.
which 2√óR GETs already in progress might still complete; this is
acceptable as such GETs are considered ordered-before the eviction.
Since the new inbound KV may differ in size from the evicted
entry, multiple evictions in an appropriate size class may be needed.
Once sufficient space is available, a new DataEntry is written, fol-
lowed by the relevant IndexEntry‚Äôs pointer into the data region,
which establishes an ordering point after which the new value is
visible. In practice, evictions occur at roughly half the rate of SETs.
5 Availability
CliqueMap offers replication to ensure read/write availability in the
face of unplanned failures and to provide some tolerance for slow
backends. The replication scheme is designed to avoid inter-replica
coordination to keep overheads low. Again, self-validating responses
and retries play a role, assisting race resolution without the need for
remote or global locking.
5.1 Quorumed GETs Under Three Replicas
When operating with three replicas, copies of each KV pair are
assigned to adjacent backend tasks. I.e., for each key, CliqueMap
uses a consistent key hash to first determine the backend number
for a logical primary replica i‚Äìas if no replication existed‚Äìand then
assigns copies of KV pairs to physical backends i, i+1 and i+2 (all
mod N).
Next, we augment 2√óR-style lookups to accommodate replication
by performing an index fetch from all replicas (Figure 4). Although
all three backends will respond, due to load differences or network
proximity, one backend‚Äôs response will arrive at the client first. The
client then fetches the datum from the first responding backend,
termed the preferred backend. Upon receipt of its second index
response, the client can attempt a quorum‚Äîa per-KV-pair majority