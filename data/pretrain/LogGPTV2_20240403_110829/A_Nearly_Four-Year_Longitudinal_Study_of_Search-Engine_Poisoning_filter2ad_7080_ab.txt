attack we previously presented [15] mainly affects individuals with
non-illicit intentions.
Other queries. We construct an additional query corpus Q(cid:48) com-
posed of an extra 600 search terms. We create and track Q(cid:48) to
provide evidence that search-poisoning is not tied to pharmaceu-
tical terms, and to study whether or not miscreants share parts of
their infrastructure to advertise different products and services. Q(cid:48)
consists of six categories: antivirus, software (in general), pirated
software, e-books, online gambling, and luxury items (speciﬁcally,
Type of query Count
Illicit (|I|)
Benign (|B|)
Gray (|G|)
Total (|Q|)
26
75
117
218
%
22%
34.4%
53.6%
100%
Table 1: Intention-based classiﬁcation of the 218 queries in the
drug query corpus (Q).
watches). We choose these topics based on the amount of email
spam we have received in spam traps we are running. For each
category, we use Google’s Keyword Planner to select the 100 most
queried keyword suggestions associated with the category name.
Except for pirated software queries, we manually ﬁlter out queries
that do not denote benign or gray intent.
4.2 Search result datasets
We use data collected on a daily basis between April 12, 2010,
and September 16, 2013. While we had already put smaller, older
portions of the data in the public domain, we make all datasets
we use in this paper publicly available for research reproducibility
purposes.1 Each dataset has its own particularities, summarized in
Table 2, which we discuss next.
Dataset 1 (4/12/2010-11/15/2010): This ﬁrst dataset represents
data collected daily between April 12, 2010 and November 15,
2010 (time interval T1), and was used in previous work examin-
ing the impact of the attack and the victims’ characteristics. The
data contains daily search results for the pharmaceutical query cor-
pus Q, without preserving any ranking information, beyond noting
that only the top-64 results (at most) are collected. Likewise, the
redirection corpus contains all the sites visited (including “redirec-
tion chains”) at a given time t, but those are not mapped to speciﬁc
queries. In other words, if two queries q1 and q2 produce results
{u, v, w}, we do not know which of q1 or q2 yielded each of u, v,
w, nor how u, v and w ranked among all search results. Redirec-
tions in this ﬁrst corpus are only gathered by following HTTP 302
redirects.
Dataset 2 (11/15/2010-10/09/2011): The second dataset spans from
November 15, 2010 through October 8, 2011 – time interval T2 –
and was partially used in previous work [15, 16].
Different from Dataset 1, this dataset contains information about
the search rankings for the pharmaceutical query corpus. Here
again, only the top 64 results per query are collected. We further-
more have the mappings between a given query and the results it
produces, but, regrettably, not the full mapping between a given
query, its results, and the ranking of the results. Going back to
our previous example, for two queries q1 and q2, we know that
q1 yielded (u, v) and q2 yielded (v, w), and we know the ranks at
which each result appeared overall, but we do not know if v ap-
peared as the top result in response to q1 or q2. Here too, redirec-
tions are gathered by following HTTP 302 redirects.
Dataset 3 (10/13/2011-9/16/2013):
lected speciﬁcally for the present analysis.
The third dataset was col-
It provides complete mapping between a query, the results it pro-
duces and their associated rankings, as well as the possible redirec-
tion chains that follow from clicking on each result.
Our collection infrastructure is markedly different from that used
for Datasets 1 and 2. Datasets 1 and 2 were assembled by having a
1See https://arima.cylab.cmu.edu/rx/.
1
2
3
4
4/12/2010–11/15/2010
Publicly available [15] Mixed (own measurements, [15])
11/15/2010–10/08/2011
Dataset
Period covered
Source
Queries used
Search results/query
Ranking info?
Mapping queries-results
Total size of result corpus
Unique URLs in results
Unique domains in results
Total size of redir. corpus
Unique redir. URLs
Unique redir. domains
T1
Q
64
No
No
260 824
150 955
25 182
50 821
50 784
5 546
T2
Q
64
Aggregate only
Partial
3 609 675
189 023
36 557
929 809
71 935
8 738
T3
10/08/2011–9/16/2013
Own measurements
Q(t) (cid:40) Q Q(cid:48)(t) (cid:40) Q(cid:48)
16 to 32
Yes
Yes
1 530 099
122 382
30 881
522 017
62 288
11 157
2 244 723
122 567
24 339
111 361
27 973
3 974
Table 2: Datasets for pharmaceutical queries. Dataset 1 only contains search results and no ranking information. Dataset 2 contains
search results and overall rankings, but no individual rankings per query. Dataset 3 contains everything we need, but only for a strict
time-varying subset of all queries.
graphical web browser run the queries against Google’s search en-
gine. Here, we use an automated (command-line) script, increasing
the level of automation in collecting search results.
Because attackers are known to perform cloaking, that is, to
make malicious results look benign when suspecting a visit from
an automated agent as opposed to a customer, we periodically spot-
checked the results our automated infrastructure collection gath-
ered with what a full-ﬂedged graphical browser would obtain. In
addition, we ran all of our queries over the Tor network [6], chang-
ing Tor circuits frequently. This had two effects: we obtained ge-
ographical diversity in the results since queries were apparently is-
sued by hosts in various countries; and we escaped IP-based de-
tection (and potential identiﬁcation), which is frequently used as a
decision to cloak results [15]. We were worried that, because Tor
exit IP addresses are well-known, they could be subject to cloak-
ing as well. Spot-checking the results we obtained by comparing
results from Tor exits as opposed to non-Tor exits did not yield any
signiﬁcant indication this was the case. In short, during our data
collection interval, either unlicensed pharmacy operators were not
aware of the existence of Tor, or, more plausibly, tolerated people
connecting to their servers using the Tor network.
Regrettably, on November 30th 2011, less than two months after
we initiated the data collection, the Google API introduced certain
restrictions, reducing both the number of queries we could run on
a daily basis, and the number of search results we could collect per
query.2 These restrictions came one year after Google announced
the deprecation of the Search API, giving it a phasing out period of
three years.
The upshot is that we could only run a random strict subset of Q
on a daily basis. The size and composition of the query set varies
over time, but, on average, consists of 64 queries. Likewise, instead
of collecting N = 64 results per query, we were limited to between
N = 16 and N = 32.
We refer as T3 the collection interval over which we collected
this dataset. During the collection of this third dataset, on April
9, 2012, we updated our collection infrastructure. Instead of sim-
ply considering redirections characterized by HTTP 302 messages,
2Recent research, e.g., [2], uses the Yandex search engine instead
of Google search in an apparent effort to overcome some of the
limitations of the Google API. For the sake of comparability with
Datasets 1 and 2, and also because it appears that search-redirection
attacks primarily target the Google search engine, we continued to
use the Google API.
our crawler became able to detect more advanced (cookie-based)
redirection techniques, as described in Section 3. We did not ob-
serve “Refresh” META tag redirections. We also realized that we
can never be sure that we are able to detect all forms of attacks, as
attackers always deploy new attack variants. To address this limita-
tion, we elected to capture the ﬁrst 200 lines of raw HTML content
present at each source infection, using both a user-agent string de-
noting a search-engine spider and a user-agent string denoting a
regular browser. The data so captured can then be analyzed af-
ter the fact to determine if there was cloaking, and to attempt to
reverse-engineer types of attacks that were unknown at data collec-
tion time. For instance, while our crawler was not able to detect
JavaScript-redirections at data collection time, we were ultimately
able to analyze how prevalent they were in our data corpus.
Dataset 4 (10/31/2011-9/16/2013):
This dataset has the same
properties as Dataset 3, but uses the query set Q(cid:48). As with Dataset 3,
the number of actual queries Q(cid:48)(t) issued every day is a varying
subset of Q(cid:48). On average, 64 queries per day are issued for each
category (gambling, watches, ...).
Finally, given the long term nature of measurements, there are
periods with incomplete or no daily measurements. These measure-
ments gaps are attributed to glitches with the measurement equip-
ment (e.g. power or network outage), or upgrades to the measure-
ment infrastructure. Out of the 1 254 days in the measurement pe-
riod, we have complete measurements for 1 004 days.
4.3 Combining the datasets
Since, in Datasets 3 and 4, all mappings between queries, results,
and rankings are recorded, as well as more complete redirection in-
formation, we can carry out more in-depth analysis than with the
ﬁrst two datasets. On the other hand, the reduced number of queries
used and results collected per query makes it slightly more com-
plicated to combine Dataset 3 with Datasets 1 and 2. (Dataset 4
concerns a different set of queries, and as such does not need to be
combined with the other datasets.)
It also means that we cannot necessarily claim to have the same
desirable coverage properties reported in our earlier work [15]. How-
ever, we can attempt to combine all datasets to obtain results over
the entire collection interval; this essentially consists of sampling
some of the queries and some of the results in Datasets 1 and 2 to
match the statistical properties of Dataset 3.
Sampling queries. In Datasets 1 and 2, for all t, the whole set Q of
queries is issued. In Dataset 3, a different random subset Q(t) (cid:40) Q
of all queries is used every day. Within that subset, the proportion
of illicit I(t) and benign B(t) queries follows the Beta distribu-
tion with parameters (α = 22.49, β = 194.29). The proportion of
gray queries G(t) follows the normal distribution with parameters
(µ = 0.57, σ2 = 0.03). Because these results are slightly different
from the proportions in Q (see Table 1), we also need to sample
from Q in the ﬁrst two datasets to be able to perform meaningful
comparisons when looking at the entire measurement interval. Un-
fortunately, as there is no association between individual queries
and results in Dataset 1, we may only be able to use Datasets 2 and
3 when looking at metrics for which the speciﬁc types of queries
used has importance. Given the known expected probabilities of
I(t), B(t), and G(t) in Dataset 3, we create samples of queries for
each day in T2 that follow the same distributions. In turn, we con-
sider only the daily results in Dataset 2 associated with each daily
query sample.
Sampling results. Dataset 3 (and 4) is often limited to N = 32
results, while Datasets 1 and 2 contain the top-64 results for each
query. Arguably, from a user standpoint, the difference is minimal:
Given that the probability of clicking on a link decreases exponen-
tially with its position in the search results [9], results in position
33 and below are unlikely to have much of an impact. Unfortu-
nately, Dataset 1 does not contain any ranking information; as such
we cannot use it for direct comparisons with Dataset 3 in terms of
search-result trends. We can, however, use Dataset 1 when we are
only concerned about measuring how long certain hosts appear in
the measurements (e.g., for survival analysis).
Dataset 2, on the other hand, contains some ranking information.
From the above discussion, for each result we obtained, we know
what was its ranking at the time; there may however be uncertainty
as to which query produced that result when results occur in re-
sponse to more than one query. We include each result u with a
probability p(u) corresponding to the number of times u appears at
a rank below 32 divided by the total number of times u appears in
the whole dataset. That is, (i) results that never appear in the top-32
results are always excluded (p = 0), (ii) results that always appear
in the top-32 results are always included (p = 1), and (iii) results
appearing both in and out of the top-32 results are included with a
probability characterizing how often they are in the top 32 .
Combining query and result sampling, we use approximately
14.7% of the search results in Dataset 2. Another 12.3% appear
both in ranks 1–32 and above 32 and are probabilistically included.
5. SEARCH RESULT ANALYSIS
We now turn to analyzing the datasets we have, and ﬁrst look at
the evolution of search results over intervals T2 and T3 (November