while private names are stored in E. A frame is simply an expres-
sion of the form new E.ϕ where dom(ϕ) ⊆ AX. It represents the
knowledge of an attacker. We define dom(new E.ϕ) as dom(ϕ).
Intuitively, two sequences of messages are indistinguishable to
an attacker if he cannot perform any test that could distinguish
them. This is typically modelled as static equivalence [3].
Definition 3.4 (Static Equivalence). Two ground frames new E.ϕ
and new E′.ϕ′ are statically equivalent if and only if they have
the same domain, and for all attacker terms R, S with variables in
dom(ϕ) = dom(ϕ′), we have
(Rϕ =E Sϕ) ⇐⇒ (Rϕ
′ =E Sϕ
′)
Two processes P and Q are in equivalence if no matter how the
adversary interacts with P, a similar interaction may happen with
Q, with equivalent resulting frames.
Definition 3.5 (Trace Equivalence). Let P, Q be two processes.
We write P ⊑t Q if for all (s,ψ) ∈ trace(P), there exists (s′,ψ ′) ∈
trace(Q) such that s =τ s′ and ψ and ψ ′ are statically equivalent.
We say that P and Q are trace equivalent, and we write P ≈t Q, if
P ⊑t Q and Q ⊑t P.
5
Figure 2: Semantics
Note that this definition already includes the attacker’s behaviour,
since processes may input any message forged by the attacker.
Example 3.6. Ballot privacy is typically modelled as an equiva-
lence property [21] that requires that an attacker cannot distinguish
when Alice is voting 0 and Bob is voting 1 from the scenario where
the two votes are swapped.
Continuing Example 3.3, ballot privacy of Helios can be ex-
pressed as follows:
PHelios(0, 1) ≈t PHelios(1, 0)
3.2 Voting protocols
We consider two disjoint, infinite subsets of C: a set A of agent
names or identities, and a set V of votes. We assume given a repre-
sentation R of the result.
A voting protocol is modelled as a process. It is composed of:
• processes that represent honest voters;
• a process modelling the tally;
• possibly other processes, modelling other authorities.
Formally, we define a voting process as follows.
Definition 3.7. A voting process is a process of the form
P = ν
#    „cred.ν cred1 . . . ν credp . (
Voter(a1, va1 , #„c1,
Voter(an, van , #„cn,
| Tallyp( #„c ,
| Othersp( #„c ′,
#    „cred, cred1) | · · · |
#    „cred, credn)
#    „cred, cred1, . . . , credp)
#    „cred, cred1, . . . , credp))
where ai ∈ A, vai ∈ V, #„ci , #„c , #„c ′ are (distinct) channels, #    „cred and
credi are (distinct) names.
A voting process may be instantiated by various voters and vote
selections. Given A = {b1, . . . , bn} ⊆ A a finite set of voters, and
α : A → V that associates a vote to each voter, we define Pα by
replacing ai by bi and vi by α(bi) in P.
Moreover, P must satisfy the following properties.
#    „cred, cred) models an honest voter
• Process Voter(a, va, #„c ,
a willing to vote for va, using the channels #„c , credentials
cred (e.g. a signing key) and election credentials #    „cred. It is
assumed to contain an event Voted(a, v) that models that a
has voted for v. This event is typically placed at the end of
process Voter(a, va, #„c ,
#    „cred, cred). This event cannot appear
in process Tallyp nor Othersp.
#    „cred, cred1, . . . , credp) models the tally. It
• Process Tallyp( #„c ,
is parametrised by the total number of voters p (honest and
dishonest), with p ≥ n. It is assumed to contain exactly one
output action on a reserved channel cr . The term output on
this channel is assumed to represent the final result of the
election.
∀α . ∀(tr , ϕ) ∈ trace(Pα). out(cr , r) ∈ tr ⇒ ∃V . ϕ(r) ∈ R(ρ(V))
Tallyp may of course contain input/output actions on other
channels.
• Process Othersp( #„c ′,
#    „cred, cred1, . . . , credp) is an arbitrary
process, also parametrised by p. It models the remaining
of the voting protocol, for example the behaviour of other
authorities. It also models the initial knowledge of the at-
tacker by sending appropriate data (e.g. the public key of the
election or dishonest credentials). We simply assume that
it uses a set of channels disjoint from the channels used in
Voter and Tallyp.
The channel cr used in Tallyp to publish the result is called the
result channel of P.
Example 3.8. The process modelling the Helios protocol, as de-
fined in Example 3.3 is a voting process, where process Othersp
consists in the output of the keys: out(c, kcs).out(c, pk(ke)).
We can read which voters voted from a trace. Formally, given a
sequence tr of actions, the set of voters Voters(tr) who did vote in
tr is defined as follows.
Voters(tr) = {a ∈ A | ∃v ∈ V. Voted(a, v) ∈ tr}.
The result of the election is emitted on a special channel cr . It
should correspond to the tally of a multiset of votes. Formally, given
a trace (t, ϕ) and a multiset of votes V , the predicate result(t, ϕ, V)
holds if the election result in (t, ϕ) corresponds to V .
result(t, ϕ, V) def= ∃x, t
3.3 Security properties
Several definitions of verifiability have been proposed. In the lines
of [15, 26], we consider a very basic notion, that says that the result
should at least contain the votes from honest voters.
.out(cr , x) ∧ ϕ(x) ∈ R(ρ(V)).
. t = t
′
′
Definition 3.9 (symbolic individual verifiability). Let P be a voting
process with result channel cr . P satisfies symbolic individual verifi-
ability if, for any trace (t, ϕ) ∈ trace(Pα) of the form t′.out(cr , x)),
there exists Vc such that the result in t corresponds to Va ⊎ Vc, that
is result(t, ϕ, Va ⊎ Vc), where
Va = {|v | ∃a. Voted(a, v) ∈ t|}
6
Individual verifiability typically guarantees that voters can check
that their ballot will be counted. Our notion of individual verifia-
bility goes one step further, ensuring that the corresponding votes
will appear in the result, even if the tally is dishonest. One of the
first definitions of verifiability was given in [25], distinguishing be-
tween individual, universal, and eligibility verifiability. Intuitively,
our own notion of individual verifiability sits somewhere between
individual verifiability and individual plus universal verifiability as
defined in [25]. A precise comparison is difficult as individual and
universal verifiability are strongly tight together in [25]. Moreover,
[25] only considers the case where all voters are honest and they
all vote.
We consider the privacy definition proposed in [21] and widely
adopted in symbolic models: an attacker cannot distinguish when
Alice is voting v1 and Bob is voting v1 from the scenario where the
two votes are swapped.
Definition 3.10 (Privacy [21]). Let P be a voting process. P satisfies
privacy if, for any subtitution α from voters to votes, for any two
voters a, b ∈ A\dom(α) and any two votes v1, v2 ∈ V, we have
Pα∪{a(cid:55)→v1,b(cid:55)→v2} ≈ Pα∪{a(cid:55)→v2,b(cid:55)→v1}
3.4 Privacy implies verifiability
We show that privacy implies verifiability under a couple of as-
sumptions, typically satisfied in practice.
First, we assume a light form of determinacy: two traces with the
same observable actions yield the same election result. This excludes
for example cases for voters chose non deterministically how they
vote. Formally, we say that a voting process P with election channel
cr is election determinate if, for any substitution α from voters to
votes, for any two traces t, t′ such that t =τ t′, (t .out(cr , x), ϕ) ∈
trace(Pα), and (t′.out(cr , x), ϕ′) ∈ trace(Pα), then
′(x) ∈ R(ρ(V))
ϕ(x) ∈ R(ρ(V))) ⇒ ϕ
This assumption still supports some form of non determinism but
may not hold for example in the case where voters use anonymous
channels that even hide who participated in the election.
Second, we assume that it is always possible for a new voter to
vote (before the tally started) without modifying the behaviour of
the protocol.
Formally, a voting proces P is voting friendly if for all voter
a ∈ A, there exists t′′ (the honest voting trace) such that for all α
satisfying a (cid:60) dom(α),
• for all (t, ϕ) ∈ trace(Pα), such that t = t′.out(cr , x) for
some t′, x, for all v, there exists tr, ψ such that tr =τ t′′,
Voted(a, v) ∈ tr, (t′.tr .out(cr , x),ψ) ∈ trace(Pα∪{a(cid:55)→v }),
and∀V . ϕ(x) ∈ R(ρ(V)) ⇒ ψ(x) ∈ R(ρ(V ∪{v})). Intuitively,
if a votes normally, her vote will be counted as expected, no
matter how the adversary interfered with the other voters.
• for all t′, x such that blocking(t′.out(cr , x), Pα), for all v,
tr, ψ such that tr =τ t′′, we have blocking(t′.tr .out(cr , x),
Pα∪{a(cid:55)→v }). Intuitively, the fact that a voted does not sud-
denly unlock the tally.
In practice, most voting systems are voting friendly since voters vote
independently. In particular, process PHelios modelling Helios, as
defined in Example 3.3, is voting friendly (assuming an honest tally).
The voting friendly property prevents a fully dishonest tally since
the first item requires that unmodified honest ballots are correctly
counted. However, we can still consider a partially dishonest tally
that, for example, discards or modifies ballots that have been flagged
by the attacker.
Moreover, we assume that there exists a neutral vote, which is
often the case in practice. Actually, this is a simplified (sufficient)
condition. Our result also holds as soon as there is a vote that can
be counted separately from the other votes (as formally defined in
appendix).
Theorem 3.11 (Privacy implies individual verifiability). Let
P be a voting friendly, election determinate voting process.
If P satisfies privacy then P satisfies individual verifiability.
The proof of this result intuitively relies on the fact that in order
to satisfy privacy w.r.t. two voters Alice and Bob, a voting process
has to guarantee that the vote of Alice is, if not correctly counted,
at least taken into account to some extent. Indeed, if an attacker,
trying to distinguish whether Alice voted for 0 and Bob for 1, or
Alice voted for 1 and Bob for 0, is able to make the tally ignore
completely the vote of Alice, the result of the election is then Bob’s
choice. Hence the attacker learns how Bob voted, which breaks
privacy.
Therefore, we first we prove that if a protocol satisfies privacy,
then if we compare an execution (i.e. a trace) where Alice votes 0
with the corresponding execution where Alice votes 1, the resulting
election results must differ by exactly a vote for 0 and a vote for 1.
Formally, we show the following property.
Lemma 3.12. If a voting friendly, election determinate voting pro-
cess P satisfies privacy, then it satisfies
[ t =τ t′ ∧ (t, ϕ) ∈ trace(Pα∪{a(cid:55)→v1}) ∧
(t′, ϕ′) ∈ trace(Pα∪{a(cid:55)→v2}) ∧
result(t, ϕ, V) ∧ result(t′, ϕ′, V ′) ]
ρ(V ′ ⊎ {|v1|}) = ρ(V ⊎ {|v2|}).
=⇒
This lemma is used as a central property to prove the theorem.
Intuitively, we apply this lemma repeatedly, changing one by one
all the votes from honest voters into neutral votes. Let r denote the
result before this operation, and r′ the result after. Let Va denote
the multiset of honest votes, and Vb the multiset containing the
same number of neutral votes. Thanks to Lemma 3.12, we can show
that r ∗ ρ(Vb) = r′ ∗ ρ(Va). Since Vb only contains neutral votes,
we have r = r′ ∗ ρ(Va). This means that r contains all honest votes,
hence the voting process satisfies individual verifiability.
The detailed proof of this theorem can be found in appendix.
4 COMPUTATIONAL MODEL
Computational models define protocols and adversaries as proba-
bilistic polynomial-time algorithms.
Notation: We may write (id,∗) ∈ L as a shorthand, meaning that
there exists an element of the form (id, x) in L. If V is a multiset of
elements of the form (id, v), we define ρ(V) = ρ({|v | (id, v) ∈ V |}).
4.1 Voting system
We assume that the ballot box displays a board BB, that is a list
of ballots. The nature of the ballots depend on the protocol we
consider.
7
Definition 4.1. A voting scheme consists in six algorithms
(Setup, Credential, Vote, VerifVoter, Tally, Valid)
• Setup(1λ), given a security parameter λ, returns a pair of
election keys (pk, sk).
• Credential(1λ, id) creates a credential cred for voter id, for
example a signing key. The credential may be empty as well.
Registered voters are stored in a list U.
• Vote(id, cred, pk, v) constructs a ballot containing the vote
v for voter id with credential cred, using the election public
key pk.
• VerifVoter(id, cred, L, BB) checks whether the local knowl-
edge L of voter id is consistent with the board BB. For exam-
ple, a voter may check that her (last) ballot appears on the
bulletin board.
• Tally(BB, sk, U) computes the tally of the ballots on the board
BB, using the election secret key sk, assuming a list of reg-
istered voter identities and credentials U. The Tally algo-
rithm first runs some test ValidTally(BB, sk, U) that typically
checks that the ballots of BB are valid. Tally may return
⊥ if the tally procedure fails (invalid board or decryption
failure for example). If Tally(BB, sk, U) (cid:44) ⊥ then it must cor-
respond to a valid result, that is, there exists V such that
Tally(BB, sk, U) = ρ(V).
• Valid(id, b, BB, pk) checks that a ballot b cast by a voter id is
valid with respect to the board BB using the election public
key pk. For example, the ballot b should have a valid signa-
ture or valid proofs of knowledge. The ballot b will be added
to BB only if Valid(id, b, BB, pk) succeeds.
We will always assume a correct voting scheme, that is, tal-
lying honestly generated ballots yields the expected result. For-
mally, for all distinct identities U = id1, . . . , idn, and credentials
cred1, . . . , credn, for all votes v1, . . . , vn, for all election keys(pk, sk),
if BB = [Vote(idi , credi , pk, vi)|i ∈(cid:74)1, n(cid:75)], then
Tally(BB, sk, U) = ρ({|v1, . . . , vn|})
The tally algorithm typically applies a revote policy. Indeed, if
voters may vote several times, the revote policy states which vote
should be counted. The two main standard revote policies are 1. the
last vote counts or 2. the first vote counts (typically when revote is
forbidden). In what follows, our definitions are written assuming