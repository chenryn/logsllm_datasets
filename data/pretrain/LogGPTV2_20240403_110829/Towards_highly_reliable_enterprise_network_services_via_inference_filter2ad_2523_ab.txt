per we present three types of meta-nodes, noisy-max, selector and
failover. These nodes model the dependencies between root causes
and observations; the latter two are needed to model load-balancers
and failover redundancy, respectively (described in detail in Sec-
tion 3.1.1).
Parent (P1)
Parent (P2)
d1
d2
Child (C)
−=
1
yd
1
,
−=
1
d
2
Let
x
Parent 1
Troubled
x −
1,
0,
xy −
1,
xy
0,
x
Noisy-
Max
Up
Up
0,0,1
Troubled
Down
x
Down
−1,0,
x
−
y
−1,0,
1),
xy
xy
xxy
1(,
−
x
WĂƌĞŶƚ;WϭͿ
WĂƌĞŶƚ;WϮͿ
WƌŝŵĂƌǇ
^ĞĐŽŶĚĂƌǇ
&ĂŝůŽǀĞƌ
ŚŝůĚ;Ϳ
&ĂŝůŽǀĞƌ
hƉ
WĂƌĞŶƚϭ
ŽǁŶ
dƌŽƵďůĞĚ
hƉ
dƌŽƵďůĞĚ
ŽǁŶ
0,0,1
0,1,0
0,0,1
0,0,1
0,1,0
0,1,0
0,0,1
0,1,0
1,0,0
Figure 3: Truth Table for the noisy-max meta-node when a
child has two parents. The values in the lower triangle are omit-
ted for clarity.
Figure 5: Truth Table for the failover meta-node encodes the
dependence that the child primarily contacts parent1, and fails
over to parent2 when parent1 does not respond.
Parent (P1)
Parent (P2)
d
1-d
Selector
Child (C)
Parent 1
Up
Selector
Up
0,0,1
Troubled
Down
Let
x
Troubled
x −
1,
x
0,1,0
0,
d
−= 1
Down
−1,0,
x −1,
,0
1,0,0
x
x
x
Figure 4: Truth Table for the selector meta-node. A child node
selects parent1 with probability d and parent2 with probability
1-d. The values in the lower triangle are omitted for clarity.
bled, then the child is troubled. If all parents are up, then the child
is up. Noisy implies that unless a parent’s dependency probability
is 1.0, there is some chance the child will be up even if the parent
is down. Formally, if the weight of a parent’s edge is d, then with
probability (1 − d) the child is not affected by that parent.
Figure 3 presents a truth table for noisy-max when a child has
two parents. Each entry in the truth table is the state of the child
(i.e., its probability of being up, troubled and down) when parent1
and parent2 have states as per the column and row label respec-
tively.1 As an example, the second row and third column of the truth
table shows the probability of the child being troubled, given that
parent1 is down and parent2 is troubled: P(Child=Troubled | Par-
ent1=Down, Parent2=Troubled) = (1 − d1) ∗ d2. To explain, the
child will be down unless parent1’s state is masked by noise (prob
1 − d1). Further, if both parents are masked by noise, the child will
be up. Hence the child is in troubled state only when parent1 is
drowned out by noise and parent2 is not.
Selector Meta-Nodes are used to model load balancing scenar-
ios. For example, a Network Load Balancer (NLB) in front of two
servers hashes the client’s requests and distributes requests evenly
to the two servers. An NLB cannot be modeled using a noisy-max
meta-node because the client would depend on each server with a
probability of 0.5, since half the requests go to each server. Using
a noisy-max meta-node will assign the client a 25% chance of be-
ing up even when both the servers are down, which is obviously
incorrect. We use the selector meta-node to model NLB Servers
and Equal Cost Multipath (ECMP) routing. ECMP is a commonly-
used technique in enterprise networks where routers send packets
to a destination along several paths. The path is selected based on a
hash of the source and destination addresses in the packet. We use a
selector meta-node when we can determine the set of ECMP paths
available, but not which path a host’s packets will use.
The truth table for the selector meta-node is shown in Figure 4,
and it expresses the fact the child is making a selection. For exam-
ple, while the child may choose each of the parents with probability
50%, the selector meta-node forces the child to have a zero proba-
bility of being up when both its parents are down (ﬁrst number in
the Down,Down entry).
Failover Meta-Nodes capture the failover mechanism commonly
1A (0, 1, 0) state for parent1 means it is troubled.
used in enterprise servers. Failover is a redundancy technique
where clients access primary production servers and failover to
backup servers when the primary server is inaccessible. In our net-
work, DNS, WINS, Authentication and DHCP servers all employ
failover. Failover cannot be modeled by either the noisy-max or
selector meta-nodes, since the probability of accessing the backup
server depends on the failure of the primary server.
The truth table for the failover meta-node is shown in Figure 5.
As long as the primary server is up or troubled, the child is not
affected by the state of the secondary server. When the primary
server is in the down state, the child is still up if the secondary
server is up.
3.1.2 Time to Propagate State
A common concern with probabilistic meta-nodes is that com-
puting the probability density for a child with n parents can take
O(3n) time for a three-state model in the general case.2 However,
the majority of the nodes in our Inference Graph with more than
one parent are noisy-max meta-nodes. For these nodes, we have
developed the following equations that reduce the computation to
O(n) time.
”
+ pdown
j
) + pup
j
”
Y
P(child up) =
“
(1 − dj ) ∗ (ptrouble
“
1 − pdown
j
j
Y
j
j
j
j
j
+ (1 − dj ) ∗ pdown
1 − P(child down) =
P(child troubled) = 1 − (P(child up) + P(child down))
where pj is the j’th parent, (pup
) is its probability
distribution, and dj is its dependency probability. The ﬁrst equation
implies that a child is up only when it does not depend on any
parents that are not up. The second equation implies that a child is
down unless every one of its parents are either not down or the child
does not depend on them when they are down.
j , ptrouble
, pdown
The computational cost for selector and failover meta-nodes is
still exponential, O(3n), for a node with n parents. However, in
our experience, these two types of meta-nodes have no more than 6
parents, and hence do not add a signiﬁcant computation burden.
3.2 Fault Localization on the Inference Graph
We now present our algorithm, Ferret, that uses the Inference
Graph to localize the cause of a network or service problem. We
deﬁne an assignment-vector to be an assignment of state to every
root-cause node in the Inference Graph where the root-cause node
has probability 1 of being either up, troubled, or down. The vector
might specify, for example, that link1 is troubled, server2 is down
and all the other root-cause nodes are up. The problem of localizing
2The naive way to compute the probability of the child’s state re-
quires computing all 3n entries in the truth-table and summing the
appropriate entries.
a fault is then equivalent to ﬁnding the assignment-vector that best
explains the observations measured by the clients.
Ferret takes as input the Inference Graph and the measurements
(e.g., response times) associated with the observation nodes. Ferret
outputs a ranked list of assignment vectors ordered by a conﬁdence
value that represents how well they explain the observations. For
example, Ferret could output that server1 is troubled and other
root-cause nodes are up with a conﬁdence of 90%, link2 is down
and other root-cause nodes are up with 5% conﬁdence, and so on.
For any assignment-vector, Ferret can compute a score for how
well that vector explains the observations. Ferret ﬁrst sets the root
causes to the states speciﬁed in the assignment-vector and then uses
the state-propagation techniques described in the previous section
to propagate probabilities downwards until they reach the observa-
tion nodes. Then, for each observation node, it computes a score
based on how well the probabilities in the state of the observation
node agree with the statistical evidence derived from the measure-
ments associated with this observation node. Section 4 provides the
details of how we compute this score.
How can we search through all possible assignment vectors to
determine the vector with the highest score? There are 3r vectors
given r root-causes, and applying the procedure just described to
evaluate the score for each assignment vector would be infeasi-
ble. Existing solutions to this problem in machine learning liter-
ature, such as loopy belief propagation [12], do not scale to the
Inference Graph sizes encountered in enterprise networks. Approx-
imate localization algorithms used in prior work, such as Shrink [6]
and SCORE [7], are signiﬁcantly more efﬁcient. However, they are
based on two-level, two-state graph models, and hence do not work
on the Inference Graph, which is multi-level, multi-state and in-
cludes meta-nodes to model various artifacts of an enterprise net-
work. The results in Section 6 clarify how Ferret compares with
these algorithms.
Ferret uses an approximate localization algorithm that builds on
an observation that was also made by Shrink [6].
OBSERVATION 3.1. It is very likely that at any point in time
only a few root-cause nodes are troubled or down.
In large enterprises, there are problems all the time, but they are
usually not ubiquitous.3 We exploit this observation by not evaluat-
ing all 3r assignment vectors. Instead, Ferret evaluates assignments
that have no more than k root-cause nodes that are either troubled
or down. Thus, Ferret ﬁrst evaluates 2 ∗ r vectors in which exactly
one root-cause is troubled or down, next 2 ∗ 2 ∗ `
vectors where
exactly two root-causes are troubled or down, and so on. Given k,
Ferret evaluates at most (2 ∗ r)k assignment vectors. Further, it is
easy to prove that the approximation error of Ferret, that is, the
probability that Ferret does not arrive at the correct solution (the
same solution attained using the brute-force, exponential approach)
decreases exponentially with k and becomes vanishingly small for
k = 4 onwards [6]. Pseudo-code for the Ferret algorithm is shown
in Algorithm 1.
r
2
´
Ferret uses another practical observation to speed up its compu-
tation.
OBSERVATION 3.2. Since a root-cause is assigned to be up in
most assignment vectors, the evaluation of an assignment vector
only requires re-evaluation of states at the descendants of root-
cause nodes that are not up.
Therefore, Ferret preprocesses the Inference Graph by assigning all
root-causes to be up and propagating this state through to the ob-
3There are important cases where this observation might not hold,
such as rapid malware infection and propagation.
Algorithm 1 Ferret{Observations O, Inference Graph G, Int X}
Candidates ← (up|trouble|down) assignments to root causes
ListX ← {}
for Ra ∈ Candidates do
(cid:2) List of top X Assignment-Vectors
(cid:2) For each Assignment-Vector
with at most k abnormal at any time
Assign States to all Root-Causes in G as per Ra.
Score(Ra) ← 1
for Node n ∈ G do
(cid:2) Initialize Score
(cid:2) Breadth-ﬁrst traversal of G
(cid:2) Propagate
Compute P(n) given P(parents of n)
end for
for Node n ∈ GO do
s ← P( Evidence at n | prob. density of n)
Score(Ra) ← Score(Ra) ∗ s
does Ra explain observation at n?
(cid:2) Scoring Observation Nodes
(cid:2) How well
(cid:2) Total Score
end for
Include Ra in ListX if Score(Ra) is in top X assignment
vectors
end for
return ListX
Service-Level 
Dependency Graph
Identify Service 
Dependencies
Fault Localization
Inference 
Graph
Inference 
Engine
Actions: e.g. 
TrRoute x→y
Suspects: 
links, routers, 
servers, clients
Network info. 
e.g. topology
Packet traces at 
agents/ routers
Observations: e.g. 
response times
Figure 6: Sherlock Solution Overview
servation nodes. To evaluate an assignment vector, Ferret needs to
re-compute only the nodes that are descendants of root-cause nodes
marked troubled or down in the assignment vector. After comput-
ing the score for an assignment vector, Ferret simply rolls back to
the pre-processed state with all root-causes in the up state. As there
are never more than k root-cause nodes that change state out of
the hundreds of root-cause nodes in our Inference Graphs, this re-
duces Ferret’s time to localize by roughly two orders of magnitude
without sacriﬁcing accuracy.
In the studies presented in this paper, we use the Ferret algorithm
exactly as described above. However, the inference algorithm can
be easily extended to leverage whatever domain knowledge is avail-
able. For example, if prior probabilities on the failure rates of com-
ponents are known (e.g., links in enterprise networks may have a
much higher chance of being congested than down [14]), then Fer-
ret can sort the assignment vectors by their prior probability and
evaluate in order of decreasing likelihood to speed up inference.
4. THE SHERLOCK SYSTEM