title:A log mining approach to failure analysis of enterprise telephony
systems
author:Chinghway Lim and
Navjot Singh and
Shalini Yajnik
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
A Log Mining Approach to Failure Analysis of
Enterprise Telephony Systems
Chinghway Lim 1,* Navjot Singh 2 Shalini Yajnik2
Department of Statistics, University of California, Berkeley, CA 94720
2Avaya Labs, 233 Mt. Airy Rd., Basking Ridge, NJ 07920
lim @ stat.berkeley.edu, {singh,shalini} @ avaya.com
Abstract- Log monitoring techniques to characterize system
and user behavior have gained significant popularity. Some
common applications of study of systems logs are syslog mining
to detect and predict system failure behavior, web log mining
to characterize web usage patterns, and error/debug log analysis
for detecting anomalies. In this paper, we discuss our experiences
with applying log mining techniques to characterize the behavior
of large enterprise telephony systems. We aim to detect, and in
some cases, predict system anomalies. We describe the problems
encountered in the study of such logs and propose some solutions.
The key differentiator of our solutions is the use of individual
message frequencies to characterize system behavior and the
ability to incorporate domain-specific knowledge through user
feedback. The techniques that we propose are general enough to
be applicable to other systems logs and can easily be packaged
into automated tools for log analysis.
I.
INTRODUCTION
Large complex telephony applications, like enterprise Voice(cid:173)
over-IP (VoIP) systems, call-centers, and contact centers, re(cid:173)
quire high reliability and availability in order to prevent any
downtime that would lead to loss of business, loss of revenue
and in some cases, loss of life. Most of these systems are
already designed with very high availability in mind. However,
as with any complex software system, bugs, misconfiguration,
and other operator problems, do cause occasional failures and
bring the system down. When a downtime occurs, as part of
recovery procedure the system administrators and the service
support staff may look at the system trace and debug logs
to identify the cause of the failure. The current process of
exploring the log file is fairly manual
in nature. A large
software system may consist of a number of modules operating
in a multi-threaded environment, with each module writing its
debug and trace messages to a log file. We believe that the
system logs (both debug and trace logs) depict the state of
the system fairly accurately. They may allow us to detect and
sometimes predict, the occurrence of anomalous behavior and
take timely action to prevent future downtime.
Study of system logs to characterize system and user
behavior has been a focus of research for some time. Web
logs containing user search patterns and navigation behavior
[1] have been studied to improve web site usage and also
to provide users with targeted product advertising. Syslogs,
transaction logs and error logs have also been mined for
detecting failures and/or anomalous behavior in a system [2],
*This work was done while the author was at Avaya Labs.
[3], [4]. Log visualization [5] has been another area of fertile
research, where abstract visualization of log file has been used
as a tool for detennining system state.
Early work on the study of logs focused on statistical
modeling of errors and failure prediction based on the mod(cid:173)
els. Iyer et al [6] noted that the alerts in the DEC VAX(cid:173)
cluster systems were correlated and based on this observation
developed models for behavior of these systems. Work by
Nassar and Andrews [7] showed that there was an increase
in the rate of non-fatal errors before the system went into a
failure mode. Data mining approaches like frequent pattern
mining and sequential pattern mining have also been used to
detect commonly occurring event patterns before a failure [8],
[9],
logs
based on the message content and then detects anomalies by
finding deviations from these clusters. The work by Oliner and
Stearley [11] describes the problems encountered in studying
large logs generated by supercomputer systems.
[10]. The paper by Vaarandi [10] clusters event
Despite the fact that the system trace and debug logs contain
a wealth of state infonnation, they have been an under-utilized
resource. This may be due to the highly unstructured nature
of the contents of such logs. Most of the previous work on
log analysis is based on searching for and mapping a set of
predefined textual patterns in the log. In contrast, trace/debug
logs are usually generated by system developers for testing and
in-field debugging and hence may not have any pre-defined
textual patterns that can be monitored. Indeed, the problem
there is really to detect patterns which depict aspects of system
behavior, irrespective of the textual content of the individual
log messages.
The main focus of this paper is to use a combination of
data mining and statistical analysis techniques on the logs
obtained from large enterprise telephony systems and show
how such techniques are very useful
in detecting and in
some cases predicting failures and anomalies in the system.
Currently, system administrators and service support personnel
rely on experience and expert infonnation to extract useful
infonnation out of the log files. As an example, a service
support personnel at a customer site may manually browse
through the log file to detect the cause of a system crash. Our
eventual goal is to quantify and automate most of these skills
to make evaluation and predictions based on the log files more
efficient. The techniques that we present in this paper form the
basis for an automated tool that we are currently in the process
1-4244-2398-9/08/$20.00 ©2008 IEEE
398
DSN 2008: Lim et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:15:35 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
~...:
Gateway
.........
Gateway
IP EndpointII.II..>LOOlIAreeN-
of building.
The rest of the paper is organized as follows. Section II
gives an overview of the enterprise telephony communication
application that we studied. Section III explains the goals of
the analysis. Section IV discusses the techniques that were
used to reduce the data through preprocessing. Sections V
and VI, respectively, apply data mining and statistical analysis
techniques to logs with known failure markers and logs with
no failure markers. Section VII concludes with a discussion
of the results.
II. ENTERPRISE TELEPHONY SYSTEM
Avaya is a vendor of enterprise voice communication
equipment and applications. An enterprise telephony system
(Figure 1) consists of several components which communicate
with each other to provide the telephony service to end(cid:173)
users. These include the Communication Manager (CM), the
gateways, and a number of IP-capable end-points. The CM
implements the bulk of the telephony features. It performs
various tasks such as phone authentication and registration,
call routing, call signaling, and, call initiation/termination. In a
typical large deployment, a single CM is capable of supporting
hundreds of thousands of IP phones with loads of similar scale
in terms of peak busy hour calls.
Teleprone Network
\
Fig. 1. An Enterprise Telephony System PSTN . . •
Terminal
From a software point of view, the CM is an extremely
large and complex piece of software with millions of lines of
code that have evolved over a decade of development. During
operation, the CM comprises of more than 50 processes that
handle the computing and communication tasks described
above. Each process writes trace and debug messages into a
shared CM log file. Below is an example of the log generated
by two processes - proc] and proc2. The log follows the
pattern of Date:TIme:Sequence:ProcessName (PID): Priority
:[Payload}. The Payload part of the log consists of trace and
debug messages generated by each process.
20060118:032918946:2445:procl(2548):HIGH:[timRestVar: time thread to sleep]
20060118:033026975:2446:proc2(2249):HIGH:[IntchgReqFail: errcode=l]
20060118:033116820:2451:proc2(2249):MED:[Being told to Go Active!!!]
20060118:033116820:2454:proc2(2249):MED:[standby~active :interchange..]
20060118:033116820:2455:proc2(2249):MED:[State Transition:standby to active]
As one would expect from a debug/trace log, the payload
part of each line does not follow any strict pattern and can be
classified as unstructured.
A. Log Collection
As mentioned in Section I, the motivation for this work
was to understand the failure behavior of CM deployments
in the field. So, the first order of business was to collect the
debug/trace logs from live systems. Due to time and bandwidth
constraints imposed by the customer environment, the log size
was restricted to about four hours during failure of which pre(cid:173)
failure log was approximately three hours. We retrieved 714
log files from 460 live CMs that spanned 23 different releases.
Each of these log files had a failure that caused a system
restart. We applied our techniques to all 714 logs to study
their behavior. The corresponding analysis and its results are
described in Section V. Additionally, we extracted log data
for 108 days from a single system where we had no prior
knowledge of failures. The analysis of this data is presented
in Section VI.
III. ANALYSIS GOALS
The initial focus of our study was to understand whether
any useful information can be gleaned from the debug/trace
logs of CM. We had three main goals in mind while doing the
analysis.
Signature of a Log: The first question to ask was whether
we could determine the signature of a log file under normal
operating conditions. Systems vary widely in terms of size
and functionality. In tum, the log files from different systems
can differ greatly. We define the signature of a log file by (a)
the type of messages that occur in the log, (b) the frequency
of each message type, and (c) the distribution of the message
types over time. As an example, a particular CM deployment
may have periodic audit turned on and the signature of the
resulting log should contain periodic occurrences of the audit
messages. Note that identifying the signature of a log file under
normal operation is also very important in identifying and
predicting failures, as any deviation from the normal signature
would be an indication of an anomaly or a failure.
Categorizing Failures: Often, the system logs contain de(cid:173)
scriptive information about a failure. This could be in the
form of events leading up to the failure, and/or footprints
from the recovery process. Each failure may have its own
characteristic footprint. The ability to automatically derive this
footprint from the logs, will help the system administrator in
troubleshooting failure conditions.
Predicting Failures: The ultimate goal of the analysis is to be
able to use these logs to predict and preempt failures. Previous
studies have indicated that a system may go through a series
of non-fatal errors before it really crashes [7]. There are many
instances where a sequence of events directly leads to a failure
in the system. By identifying the accompanying symptoms
present in the log files,
it is possible to predict upcoming
failures reliably.
Analyzing the log data to achieve the above goals is a
complex task. Some of the complexities of the analysis come
from the following:
1) Log messages are often corrupted and/or lost, especially
when the system is under failure conditions or under
heavy load. Any log mining technique will suffer from
the incorrectness that arises out of this.
1-4244-2398-9/08/$20.00 ©2008 IEEE
399
DSN 2008: Lim et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:15:35 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
2) The volume of data generated in the logs can be really
large. Log messages are textual and any data mining
tool will need to reduce the volume of data stored and
processed, before the data can be handled in an efficient
way.
3) The
is very do(cid:173)
main/application specific and detailed knowledge of the
application and domain is needed to determine if a
particular behavior is really anomalous. Any general
purpose log analysis technique cannot deal with the
application-specific nature of the logs.
log messages
content of
the
Unfortunately, it is difficult to avoid the first problem of
lost messages. Logging is usually the lowest priority task and
an overloaded system may delay and/or lose log messages.
However, our study indicates that there are enough indicators
in the log file and losing a few has minimal impact on our
ability to detect problems. In the next section, we discuss
log preprocessing techniques that aim to purge the corrupted
messages that arise out of the first problem and tackle the
second problem through data reduction. Solution to the third
problem is proposed in Section V-D.
IV. LOG PREPROCESSING
In the eM application, several processes and threads write
to the same log file. Even with locking, there are cases where
the log messages are either partially written and/or corrupted.
The first preprocessing step was to cleanup the log file by
removing all corrupted messages and unprintable characters.
As shown earlier, the fonnat of entries in the log file was
Date:Time:Sequence:ProcessName(PID):Priority:[Payload] .
By manually looking through the log file, we realized
that most of the semantic information in the message was
stored in the process name and the payload field. We
define the message fonned by combining these two fields
ProcessName:[PayloadJ as the Effective Message. Henceforth,
when we refer to message, we mean the effective message.
In order to detennine the signature of a log file with respect
to various message types, the first order of business was to
identify all unique messages in the log files. We extracted out
the effective message part from all entries of each log file and
built a set of unique messages across all 714 logs. From the
set of all 11.5 million messages in the logs, there were about
2.5 million such unique messages. In statistical tenns, defining
the signature of a log in tenns of 2.5 million messages is like
studying a 2.5 million variable space. This is not only very
computationally expensive but may also lead to detection of
no event patterns in the logs, since a large fraction ('"'-J 22%)
of the messages are unique. Therefore, before any analysis
technique could be applied, we needed to reduce the unique