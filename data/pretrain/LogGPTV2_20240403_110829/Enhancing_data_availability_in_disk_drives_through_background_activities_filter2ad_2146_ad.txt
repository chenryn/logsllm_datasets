average scrubbing time and M L (2) = 0.48 X 102 . M £(1) .
As the same as in Section 6, we assume MRL ~ 0 in Equa(cid:173)
tion (2). The parameter p in Equation (3) is estimated using
Equation (4) and the average parity update time when it runs
concurrently with scrubbing.
8 Multi-feature Case: Scrubbing and Intra(cid:173)
8.2 Results for Trace T1
disk Parity
Scrubbing and intra-disk parity can be used simultane(cid:173)
ously to improve MTTDL. In this section, we evaluate the
perfonnance of these two features when running concur(cid:173)
rently. Because both features run in the background without
any buffer requirement, their queue capacity is assumed to
be infinite. Recall that scrubbing generates infinite work
while parity updates require finite work. Here, we evalu(cid:173)
ate a scenario when parity updates have higher priority than
scrubbing, i.e., scrubbing is scheduled only if there is no
parity update waiting. As in previous sections, the perfor(cid:173)
mance degradation of user traffic is kept below the preset
7% threshold.
8.1 MTTDL in Data Redundant Drives
We use Equation (3) to estimate the MTTDL improve(cid:173)
ment when both scrubbing and intra-disk parity are en-
Here we present results for trace T1 which is charac(cid:173)
terized by idle periods with low variability. For this trace,
scrubbing performs better with the body-based policy while
parity updates are done more efficiently under the tail-based
policy. Here, in addition to the body-based and the tail(cid:173)
based policies, we also evaluate a "hybrid" scheduling pol(cid:173)
icy that schedules scrubbing work via the body-based policy
and parity updates via the tail-based policy. This policy is
called "body+tail" policy.
Table 9 presents the MTTDL improvement when scrub(cid:173)
bing and intra-disk parity co-exist under the three schedul(cid:173)
ing policies. As expected, for T1that has idle times of low
variablity, the body+tail policy achieves the best improve(cid:173)
ment in MTTDL. Most importantly, the new combined pol(cid:173)
icy gives 8 orders of magnitude improvements in MTTDL.
Comparing to running scrubbing and parity updates indi(cid:173)
vidually, using both features results in dramatically higher
improvements while keeping the degradation of the fore-
1-4244-2398-9/08/$20.00 ©2008 IEEE
499
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
(a) Scrubbing
merged-
scrubbing c:::J
70985258
1.4e+04
3 1.2e+04
e le+04
0=
8e+03
b/)
c
:s
.c
~
1.6e+04 - "
l
4e+03
6e+03
2e+03
body-based
tail-based
40
_
35
~ 30
o§ 25
~ 20
c..
n ~ 15
n 1 10
body+tail
5
0
180629
188213
merged_
parity update c::=J
body-based
tail-based
body+tail
Figure 5. Average time for (a) an entire scrubbing, (b)
parity updates for trace Tl (low variability).
ground performance within pre-defined limits.
Table 9. MTTDL improvement for trace Tl (low variabil(cid:173)
ity) via scrubbing and intra-disk parity.The body+tail policy
schedules scrubbing via the body-based policy and parity(cid:173)
updates via the tail-based policy.
Figure 5(a) presents the average time for a complete
scrubbing when run individually and when merged together
with parity updates.
If the body-based policy is used
to schedule both types of background jobs, performance
degradation on scrubbing is significant. With the body+tail
variation, each background activity (i.e., scrubbing or par(cid:173)
ity update) is scheduled using the policy under which it per(cid:173)
forms best. Parity updates, because they have higher prior(cid:173)
ity than scrubbing, are not penalized as much as scrubbing
(see Figure 5(b)). Furthermore, parity updates perform sig(cid:173)
nificantly better if they are scheduled using the tail-based
policy, independently of how scrubbing is scheduled.
Figure 6 presents system utilization for trace T1. Re(cid:173)
sults are in agreement with those shown in Figure 5:
the
body+tail policy utilizes best the entire system providing
room for both scrubbing and parity updates to provide sig(cid:173)
nificant performance improvements.
35 ~-----'....------r----~---,
30
25
20
15
10
~
c:
o
.g
~
~g
~
~
body-based tail-based
body+tail
merged -
scrubbing c:::::::J
parity update c::::=J
Figure 6. Overall system utilization for trace Tl (low
variability) via scrubbing and intra-disk parity.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
(b) Intra-disk Parity
8.3 Results for Trace T2
Now, we present the results for T2 with high variable idle
times. For this trace, both scrubbing and parity updates indi(cid:173)
vidually perform better using the tail-based policy. Table 10
gives the MTTDL improvement attributed to scrubbing and
intra-disk parity under this policy only. For the four vari(cid:173)
ants of trace T2, the background activities dramatically im(cid:173)
prove the system reliability by improving its MTTDL by as
high as 8 orders of magnitude. Consistently with the re(cid:173)
sults shown in Table 9, there are gains ofat least 3 orders of
magnitude in MTTDL.
Policy
1%
50%
90%
WRITEs
WRITEs
WRITEs
WRITEs
tail
~ 1.12 x 10~ I 1.09 x 10~ I 1.07 x 10~ I 1.04 x 10~ I
I
I
T2
10% I
Table 10. MTTDL improvement for trace T2 (high vari(cid:173)
ability) via scrubbing and intra-disk parity.
(b) Intra-disk Parity
90 .-----.,.....--------,----,
80
3: 70
(a) Scrubbing
900 .--~----~---,
800
3 700e600
.~ 500
] 400
'2 300
<Il 200
100
o
scrubbing tail-based merged tail-based
.! 60
~ 50
§. 40
C 301 20
10
o
parity update tail-based merged tail-based
1% (5,000) _
10% (50,000) c::::J
50% (250,000) -
90% (450,000) c:::::::J
Figure 7. Average (a) scrubbing and (b) parity update
times when running individually and together.
Figures 7(a) and 7(b) further give the average scrubbing
and parity update times. For comparison, in each plot the
results of only disk scrubbing and only intra-disk parity are
also included, see left set of columns in Figures 7(a) and
7(b), respectively. For the case of scrubbing, all variants of
trace T2 perform the same because scrubbing is workload
independent.
Although scrubbing has lower priority than intra-disk
parity update, enabling it concurrently with parity updates
does not affect its performance considerably (i.e., only 10%
WRITEs in the worst case). Similarly, parity updates see
minimal change in their performance because they are pro(cid:173)
cesses ofhigher priority than scrubbing. The only exception
is the case with the smallest amount of parity updates (i.e.,
only 1% user WRITEs). As discussed in Section 7, the ef(cid:173)
fect of parity updates in user traffic performance is almost
zero for this case and parity update times are the smallest.
However, adding the infinite scrubbing work degrades par(cid:173)
ity update performance by as much as 3 times.
Figure 8 shows overall system utilization, which is dom(cid:173)
inated by the work done for scrubbing. Because the work
1-4244-2398-9/08/$20.00 ©20081EEE
500
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
related to parity updates is small, its completion barely adds
It is scrubbing with its infinite
to the system utilization.
amount ofwork that keeps the system continuously utilized.
1%(5,000)-
10% (50,000) c::::::J
50% (250,000) -
90% (450,000) c:::::J
90
__
80
~ 70
.§
60
~l3
50
.§
40
5 30
t:i
~ 20
10
o
scrubbing
tail-based
r--
r---
...--.II
parity update
tail-based
merged
tail-based
Figure 8. Overall system utilization under scrubbing and
parity updates when they run individually and together.
9 Conclusions
In this paper, we evaluate the effectiveness of data loss
prevention techniques, such as disk scrubbing and intra(cid:173)
disk data redundancy, when their execution should not af(cid:173)
fect user performance more than pre-defined bounds. Our
trace driven evaluation indicates that treating these features
as strictly background features and scheduling them dur(cid:173)
ing idle times, guided by advance idleness management
techniques, do achieve the goal of maintaining user perfor(cid:173)
mance degradation at minimum, while significantly improv(cid:173)
ing the storage system Mean Time To Data Loss - MTTDL.
Specifically, scrubbing improves MTTDL by up to five or(cid:173)
ders ofmagnitude and intra-disk data redundancy improves
MTTDL by up to two orders of magnitude. However, run(cid:173)
ning concurrently both features yields further gains with
respect to MTTDL, i.e., up to eight orders of magnitude,
without violating the user performance constraints. These
results indicate that these two features complement each(cid:173)
other and significantly improve data availability and reli(cid:173)
ability in the storage system while remaining strictly low
priority features.
References
[1] Lakshmi N. Bairavasundaram, Garth R. Goodson, Shankar Pasupa(cid:173)
thy, and Jiri Schindler. An analysis of latent sector errors in disk
drives. SIGMETRICS Perform. Eval. Rev., 35(1):289-300,2007.
[2] M. Baker, M. Shah, D. S. H. Rosenthal, M. Roussopoulos, P. Ma(cid:173)
niatis, T. 1. Giuli, and P. Bungale. A fresh look at the reliability of
long-term digital storage. In Proceedings ofEuropean Systems Con(cid:173)
ference (EuroSys), pages 221-234, April 2006.
[3] A. Dholakia, E. Eleftheriou, X. Y. Hu, I. Iliadis, 1. Menon, and K. K.
Rao. Analysis of a new intra-disk redundancy scheme for high(cid:173)
reliability RAID storage systems in the presence of unrecoverable
errors. Technical report, RZ3652, IBM Reasearch, 2006.
[4] Fred Douglis, P. Krishnan, and Brian N. Bershad. Adaptive disk
spin-down policies for mobile computers. In Proceedings of the 2nd
USENIX Symposium on Mobile and Location-Independent Comput(cid:173)
ing, pages 121-137, 1995.
[5] L. Eggert and J. D. Touch. Idletime scheduling with preemption in(cid:173)
tervals. In Proceedings ofthe International Symposium on Operating
Systems Principles (SOSP), pages 249-262, October 2005.
[6] Jon G. Elerath and Michael Pecht. Enhanced reliability modeling of
raid storage systems. In Proceedings ofthe International Conference
on Dependable Systems and Networks (DSN), pages 175-184,2007.
[7] S. Ghemawat, H.Gobioff, and S. Leung. The Google file system. In
Proceedings of ACM Symposiom on Operating Systems Principles,
pages 29-43, 2003.
[8] R. Golding, P. Bosch, C. Staelin, T. Sullivan, and 1. Wilkes. Idleness
In Proceedings of the Winter'95 USENIX Conference,
is not sloth.
pages 201-222, New Orleans, LA, January 1995.
[9] David P. Helmbold, Darrell D. E. Long, Tracey L. Sconyers, and
Bruce Sherrod. Adaptive disk spin-down for mobile computers. Mo(cid:173)
bile Networks and Applications, 5(4):285-297, 2000.
[10] Hai Huang, Wanda Hung, and Kang G. Shin. FS2: dynamic data
replication in free disk space for improving disk performance and
energy consumption.
In Proceedings of the twentieth ACM sympo(cid:173)
sium on Operating systems principles (SOSP), pages 263-276, 2005.
[11] Gordon F. Hughes and Joseph F. Murray. Reliability and security
of RAID storage systems and D2D archives using SATA disk drives.
ACM Transactions on Storage, 1(1):95-107,2005.
[12] Virginia Mary Lo, Daniel Zappala, Dayi Zhou, Yuhong Liu, and
Shanyu Zhao. Cluster computing on the fly: P2P scheduling of idle
cycles in the internet. In the 3rd International Workshop on Peer-to(cid:173)
Peer Systmes (IPTPS), pages 227-236, 2004.
[13] C. Lueth. RAID-DP: Network appliance implementation of RAID
double parity for data protection. Technical report, Technical Report
No. 3298, Network Appliance Inc, 2004.
[14] N. Mi, A. Riska, Q. Zhang, E. Smirni, and E. Riedel. Efficient
management of idleness in systems. Proceedings of SIGMETRICS,
35(1):371-372, 2007.
[15] N. Mi, A. Riska, Q. Zhang, E. Smirni, and E. Riedel. Efficient utiliza(cid:173)
tion of idle times. Technical report, WM-CS-2oo8-01, Department
of Computer Science, William and Mary, 2008.
[16] D. A. Patterson, G. Gibson, and R. Katz. A case for redundant ar(cid:173)
rays of inexpensive disks (RAID). In Proceedings of the 1988 ACM
SIGMOD Conference, pages 109-116. ACM Press, 1988.
[17] A. Riska and E. Riedel. Disk drive level workload characterization.
In Proceedings of the USENIX Annual Technical Conference, pages
97-103, May 2006.
[18] Bianca Schroeder and Garth A. Gibson. Disk failures in the real
world: what does an mttf of 1,000,000 hours mean to you? In Pro(cid:173)
ceedings of the 5th conference on USENIX Conference on File and
Storage Technologies (FAST), pages 1-1,2007.
[19] T. 1. E. Schwarz, Q. Xin, E. L. Miller, D. D. E. Long, A. Hospodor,
and S. Ng. Disk scrubbing in large archival storage systems. In Pro(cid:173)
ceedings of the International Symposium on Modeling and Simula(cid:173)
tion of Computer and Communications Systems (MASCOTS), pages
409-418, 2004.
[20] S. Shah and 1. G. Elerath. Reliability analysis of disk drive failure
In Proceedings of 2005 Annual Reliability and Main(cid:173)
mechanism.
tainability Symposium, pages 226-231. IEEE, January 2005.
[21] Qi. Zhang, N. Mi, E. Smirni, A. Riska, and E. Riedel. Evaluating the
performability of systems with background jobs. In Proceedings of
the International Conference on Dependable Systems and Networks
(DSN), pages 495-504, 2006.
1-4244-2398-9/08/$20.00 ©2008 IEEE
501
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply.