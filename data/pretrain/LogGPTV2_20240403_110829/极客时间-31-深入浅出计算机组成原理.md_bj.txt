### 多核并行和 SIMT这样一来，我们的 GPU 电路就比 CPU 简单很多了。于是，我们就可以在一个 GPU里面，塞很多个这样并行的 GPU 电路来实现计算，就好像 CPU 里面的多核 CPU一样。和 CPU 不同的是，我们不需要单独去实现什么多线程的计算。因为 GPU的运算是天然并行的。![](Images/41aa75fdf2f6a9d963c54d56d0a04bb2.png){savepage-src="https://static001.geekbang.org/resource/image/3d/ac/3d0859652adf9e3c0305e8e8517b47ac.jpeg"}我们在上一讲里面其实已经看到，无论是对多边形里的顶点进行处理，还是屏幕里面的每一个像素进行处理，每个点的计算都是独立的。所以，简单地添加多核的GPU，就能做到并行加速。不过光这样加速还是不够，工程师们觉得，性能还有进一步被压榨的空间。我们在[第 27讲](https://time.geekbang.org/column/article/103433)里面讲过，CPU里有一种叫作 SIMD的处理技术。这个技术是说，在做向量计算的时候，我们要执行的指令是一样的，只是同一个指令的数据有所不同而已。在GPU 的渲染管线里，这个技术可就大有用处了。无论是顶点去进行线性变换，还是屏幕上临近像素点的光照和上色，都是在用相同的指令流程进行计算。所以，GPU就借鉴了 CPU 里面的SIMD，用了一种叫作[SIMT](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)（SingleInstruction，Multiple Threads）的技术。SIMT 呢，比 SIMD 更加灵活。在SIMD 里面，CPU一次性取出了固定长度的多个数据，放到寄存器里面，用一个指令去执行。而SIMT，可以把多条数据，交给不同的线程去处理。各个线程里面执行的指令流程是一样的，但是可能根据数据的不同，走到不同的条件分支。这样，相同的代码和相同的流程，可能执行不同的具体的指令。这个线程走到的是if 的条件分支，另外一个线程走到的就是 else 的条件分支了。于是，我们的 GPU设计就可以进一步进化，也就是在取指令和指令译码的阶段，取出的指令可以给到后面多个不同的ALU 并行进行运算。这样，我们的一个 GPU 的核里，就可以放下更多的ALU，同时进行更多的并行运算了。![](Images/d200d12ae9ffcb8ce59ba438509c4057.png){savepage-src="https://static001.geekbang.org/resource/image/3d/28/3d7ce9c053815f6a32a6fbf6f7fb9628.jpeg"}
### GPU 里的"超线程"虽然 GPU里面的主要以数值计算为主。不过既然已经是一个"通用计算"的架构了，GPU里面也避免不了会有 if...else 这样的条件分支。但是，在 GPU 里我们可没有CPU这样的分支预测的电路。这些电路在上面"芯片瘦身"的时候，就已经被我们砍掉了。所以，GPU 里的指令，可能会遇到和 CPU类似的"流水线停顿"问题。想到流水线停顿，你应该就能记起，我们之前在 CPU里面讲过超线程技术。在 GPU上，我们一样可以做类似的事情，也就是遇到停顿的时候，调度一些别的计算任务给当前的ALU。和超线程一样，既然要调度一个不同的任务过来，我们就需要针对这个任务，提供更多的**执行上下文**。所以，一个Core 里面的**执行上下文**的数量，需要比 ALU 多。![](Images/cc8f21fbdea6fdcf74a28f1d5b8f9fa2.png){savepage-src="https://static001.geekbang.org/resource/image/c9/b8/c971c34e0456dea9e4a87857880bb5b8.jpeg"}
## GPU 在深度学习上的性能差异在通过芯片瘦身、SIMT以及更多的执行上下文，我们就有了一个更擅长并行进行暴力运算的GPU。这样的芯片，也正适合我们今天的深度学习的使用场景。一方面，GPU 是一个可以进行"通用计算"的框架，我们可以通过编程，在 GPU上实现不同的算法。另一方面，现在的深度学习计算，都是超大的向量和矩阵，海量的训练样本的计算。整个计算过程中，没有复杂的逻辑和分支，非常适合GPU 这样并行、计算能力强的架构。我们去看 NVidia 2080显卡的[技术规格](https://www.techpowerup.com/gpu-specs/geforce-rtx-2080.c3224)，就可以算出，它到底有多大的计算能力。2080 一共有 46 个 SM（Streaming Multiprocessor，流式处理器），这个 SM相当于 GPU 里面的 GPU Core，所以你可以认为这是一个 46 核的 GPU，有 46个取指令指令译码的渲染管线。每个 SM 里面有 64 个 CudaCore。你可以认为，这里的 Cuda Core 就是我们上面说的 ALU 的数量或者 PixelShader 的数量，46x64 呢一共就有 2944 个 Shader。然后，还有 184 个TMU，TMU 就是 Texture MappingUnit，也就是用来做纹理映射的计算单元，它也可以认为是另一种类型的Shader。![](Images/cf68fea6cc0a34e731ad1efa23522c81.png){savepage-src="https://static001.geekbang.org/resource/image/14/e2/14d05a43f559cecff2b0813e8d5bdde2.png"}```{=html}```图片来源](https://www.anandtech.com/show/13282/nvidia-turing-architecture-deep-dive/7)```{=html}``````{=html}```2080 Super 显卡有 48 个 SM，比普通版的 2080 多 2 个。每个 SM（SM 也就是GPU Core）里有 64 个 Cuda Core，也就是 Shader]{.reference}```{=html}```2080 的主频是 1515MHz，如果自动超频（Boost）的话，可以到 1700MHz。而NVidia的显卡，根据硬件架构的设计，每个时钟周期可以执行两条指令。所以，能做的浮点数运算的能力，就是：```{=html}```（2944 + 184）× 1700 MHz × 2 = 10.06 TFLOPS```{=html}```对照一下官方的技术规格，正好就是 10.07TFLOPS。那么，最新的 Intel i9 9900K 的性能是多少呢？不到 1TFLOPS。而 2080 显卡和9900K 的价格却是差不多的。所以，在实际进行深度学习的过程中，用 GPU所花费的时间，往往能减少一到两个数量级。而大型的深度学习模型计算，往往又是多卡并行，要花上几天乃至几个月。这个时候，用CPU 显然就不合适了。今天，随着 GPGPU 的推出，GPU已经不只是一个图形计算设备，更是一个用来做数值计算的好工具了。同样，也是因为GPU 的快速发展，带来了过去 10 年深度学习的繁荣。
## 总结延伸这一讲里面，我们讲了，GPU一开始是没有"可编程"能力的，程序员们只能够通过配置来设计需要用到的图形渲染效果。随着"可编程管线"的出现，程序员们可以在顶点处理和片段处理去实现自己的算法。为了进一步去提升GPU 硬件里面的芯片利用率，微软在 XBox 360里面，第一次引入了"统一着色器架构"，使得 GPU变成了一个有"通用计算"能力的架构。接着，我们从一个 CPU 的硬件电路出发，去掉了对 GPU没有什么用的分支预测和乱序执行电路，来进行瘦身。之后，基于渲染管线里面顶点处理和片段处理就是天然可以并行的了。我们在GPU 里面可以加上很多个核。又因为我们的渲染管线里面，整个指令流程是相同的，我们又引入了和 CPU 里的SIMD 类似的 SIMT 架构。这个改动，进一步增加了 GPU 里面的 ALU的数量。最后，为了能够让 GPU 不要遭遇流水线停顿，我们又在同一个 GPU的计算核里面，加上了更多的执行上下文，让 GPU 始终保持繁忙。GPU 里面的多核、多 ALU，加上多 Context，使得它的并行能力极强。同样架构的GPU，如果光是做数值计算的话，算力在同样价格的 CPU的十倍以上。而这个强大计算能力，以及"统一着色器架构"，使得 GPU非常适合进行深度学习的计算模式，也就是海量计算，容易并行，并且没有太多的控制分支逻辑。使用 GPU进行深度学习，往往能够把深度学习算法的训练时间，缩短一个，乃至两个数量级。而GPU现在也越来越多地用在各种科学计算和机器学习上，而不仅仅是用在图形渲染上了。
## 推荐阅读关于现代 GPU 的工作原理，你可以仔细阅读一下 haifux.org上的这个[PPT](http://haifux.org/lectures/267/Introduction-to-GPUs.pdf)，里面图文并茂地解释了现代GPU 的架构设计的思路。
## 课后思考上面我给你算了 NVidia 2080 显卡的 FLOPS，你可以尝试算一下 9900K CPU 的FLOPS。欢迎在留言区写下你的答案，你也可以把今天的内容分享给你的朋友，和他一起学习和进步。![](Images/79d06107d349635530fbf82aa8dfb625.png){savepage-src="https://static001.geekbang.org/resource/image/28/29/281ca28b90c8aa0aecbb5adc08394f29.jpg"}