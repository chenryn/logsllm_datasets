3.2 Measurement methodology
We characterize our traﬃc in terms of a number of quan-
tities that we explicitly deﬁne here. The packet size refers to
the size of the payload of our Ethernet packets.6 We deﬁne
the data rate as the data transmitted in the 64b/66b Phys-
ical Coding Sublayer (PCS) line code7 over a given period
of time, thus including the entire Ethernet packet as well as
6Each payload has an extra 25 Bytes of header and footer.
7The 64b/66b PCS [11] deﬁnes the speciﬁcs of the symbol-
stream of the 10GBase-R Physical Layer; see Appendix B.
346sponding to ∼16 Million packets per second). In this lossy
scenario, it is interesting to recognize that input packets
have an IPG of 72 bit-times: near the minimum allowed by
the IEEE 10GBase-R standard, but above that value man-
dated for reception and below that for transmission.9 Thus,
the ﬂow is legal. Nonetheless, it constitutes an edge case:
any higher packet-rate traﬃc ﬂow would violate the 10GbE
speciﬁcation.
Figure 6 depicts the distributions of inter-packet delay
obtained in this experiment across the control router. Each
subﬁgure represents one of the six ensembles, corresponding
to various packet sizes and data rates enumerated above in
Table 1. The large panel in each subﬁgure shows a proba-
bility density histogram of received packet delays, all with
equivalent coordinate ranges and identical logarithmic ver-
tical scales to allow for the visual comparison of the ﬁne
structure and broadening eﬀects in the distribution. The
upper-left inset of each subﬁgure shows the raw inter-packet
delay of the received traﬃc as a function of time [packet #]
for a small representative segment of the million-packet en-
semble. The upper-right inset of each subﬁgure presents an
enlarged graph of the histogram, centered around the peak
of the distribution. For each ensemble, the inter-packet de-
lay value of the injected homogeneous network traﬃc is also
marked (red vertical dashed line). It is vital to understand
what a “good” response looks like: the histogram should be
nearly a vertical delta function, centered at the input data
ﬂow. The extent to which the output delay distribution devi-
ates from this ideal serves as a measure of the router-induced
dynamics.
We observe that:
1. Even this single isolated router broadens the inter-
packet delay distribution (initial distribution has zero
width, as input traﬃc arrives homogeneously in time).
In eﬀect, even though the packets arrive with perfect
regularity, some manage to transit the router quickly,
while others are delayed brieﬂy.
2. At higher data rates, some packets emerge in closely
packed chains with the spacing between packets re-
ﬂecting the minimum legal number of IPG bits. We
observed this eﬀect for three of the measurement en-
sembles, only one of whose input stream itself included
minimally spaced packets (namely, 46-Byte packets at
9 Gbps).
3. Packet loss is observed in the stream with the highest
packet rate (46-Byte packets at 9 Gbps).
4. A ﬁne-grain structure is evident, reﬂecting the archi-
tecture of the underlying 64b/66b PCS line code.
What should one take away from this experiment?
(1) Broadening of the delay distribution: We note
that all ensembles (except the stream with highest packet
rate) exhibit a signiﬁcant broadening of the delay distribu-
tion with respect to that of the injected packet stream. Pre-
sumably, this is due to the store-and-forward nature of the
router, as the router itself is a clocked device that undergoes
some internal state transitions to determine its readiness to
receive or send. Further, the half-width of most of these
distributions — deﬁned here as the range for −70 dB falloﬀ
9See Note 4 of Section 4.4.2 of IEEE 802.3ae-2002 [10].
from the distribution’s peak — is approximately 200 ns. For
these ensembles, the half-width represents a measure of the
response delay to input packets evenly spaced in time. In
contrast, the stream with the highest packet rate experiences
negligible broadening of its distribution (see upper-right in-
set of Figure 6(f)); in this case, the input packets already
arrive with minimum-allowed IPG. Thus, the distribution
can only broaden in one direction (to higher IPD values),
and any such broadening is associated with corresponding
packet loss.
(2) Formation of packet chains: As previously men-
tioned, the inter-packet gap minimum is actually evident
in the asymmetry of the delay distributions. Speciﬁcally,
the IEEE 802.3ae-2002 standard [10] mandates a separa-
tion between packets to provide the receiver some latitude
in processing the incoming packet stream. This property is
expressed in terms of the inter-packet gap; as such, the cor-
responding minimum inter-packet delay is dependent on the
packet size, yet independent of data rate. For example, given
1500-Byte packets, the minimum inter-packet delay (corre-
sponding to an IPG of 96 bit-times) is actually 1230 ns,
while, for 46-Byte packets, it is 66 ns.
In fact, we observe precisely these minimum inter-packet
delay values in most of our measurements. For ensembles
with smaller input IPDs (1500-Byte packets at 9 Gbps and
46-Byte packets at 3 and 9 Gbps), it is readily observable:10
we note extremely sharp drop-oﬀs in the probability densi-
ties (left side of subﬁgures), with −50 to −80 dB suppression
over 3–6 ns. We measure a minimum IPD of 1226 ns for our
ensemble with 1500-Byte packets at 9 Gbps (Figure 6(e)),
a minimum IPD of 64 ns for our ensemble with 46-Byte
packets at 3 Gbps (Figure 6(d)), and a minimum IPD of
64 ns for the ﬁnal ensemble with 46-Byte packets at 9 Gbps
(Figure 6(f)). These closely agree with the above theoret-
ical predictions for lower constraints due to minimal IPGs:
the ﬁrst observation is within 0.3%, and the latter two are
within 3.5%.
(3) Packet loss across the router: We observe packet
loss for 46-Byte packets at 9 Gbps (see Figure 6(f), espe-
cially the upper-right inset, for asymmetric broadening). We
note that alternative mechanisms independently conﬁrm the
same level of packet loss for this ensemble: a careful exam-
ination of router statistics (accessed via SNMP and aver-
aged over long periods) shows that this loss is almost com-
pletely conﬁned to packets that are discarded in the out-
bound router interface before acquisition by our instrumen-
tation. This corresponds to 4.7% packet loss. An additional
0.02% of packets exhibit errors at the inbound router in-
terface after transmission from our instrument, though no
packets without errors are discarded at this interface. Fur-
ther, we note that, during measurement of this ensemble,
our packet stream elicits continuous router-log warnings of
excessive backplane utilization.
Ultimately, it is not particularly surprising that we are
able to provoke router drops, even with a single ﬂow run-
ning across a single enterprise router, when we consider
that this measurement ensemble is constructed with packet
rates approaching 16 million packets per second and IPGs of
only 72 bits, very close to the ultimate allowed minimum of
40 bits. In particular, the 10GbE standards allows for packet
10For larger input IPDs (1500-Byte packets at 1 and 3 Gbps
and 46-Byte packets at 1 Gbps), small sample sizes mask
the phenomenon.
347(a) 1 Gbps Data Rate (1500-Byte Packets)
(b) 1 Gbps Data Rate (46-Byte Packets)
(c) 3 Gbps Data Rate (1500-Byte Packets)
(d) 3 Gbps Data Rate (46-Byte Packets)
(e) 9 Gbps Data Rate (1500-Byte Packets)
(f) 9 Gbps Data Rate (46-Byte Packets)
Figure 6: Comparison of packet delay across an isolated router (Cisco 6500) serving as the experimental control, with the input
network traﬃc to the router perfectly homogeneous in time and the resulting delay distribution a response to transit across
the router: subﬁgures show the probability density histograms of inter-packet delays [µs] for six ensembles (corresponding to
the data rates and packet sizes enumerated in Table 1), with the delay for the input traﬃc marked with a dotted red line.
Histogram coordinate axes are equivalent in their range (oﬀset to center the distribution) to allow visual comparison of the
broadening, and the ordinate axes are identical with a logarithmic scale to expose the ﬁne-grained structure. Upper-left inset
shows the raw delay [µs] as a function of time [packet #] for representative ﬂows; while the upper-right inset is an enlarged
view of the primary graph about its peak.
11.812.012.212.412.6Inter-packet Delay [ s]10-410-310-210-1100101102103104105106Probability Density0255075100Packet #11.812.012.212.412.6Inter-packet Delay [s]12.1712.1912.21Inter-packet Delay [s]10-210-1100101102103104105Prob. Dens.0.160.360.560.760.96Inter-packet Delay [ s]10-410-310-210-1100101102103104105106Probability Density0255075100Packet #0.160.360.560.760.96Inter-packet Delay [s]0.560.580.600.62Inter-packet Delay [s]10-210-1100101102103104105Prob. Dens.3.653.854.054.254.45Inter-packet Delay [ s]10-410-310-210-1100101102103104105106Probability Density0255075100Packet #3.653.854.054.254.45Inter-packet Delay [s]4.044.064.08Inter-packet Delay [s]10-210-1100101102103104105Prob. Dens. 0.10.10.30.50.7Inter-packet Delay [s]10-410-310-210-1100101102103104105106Probability Density0255075100Packet #0.10.10.30.50.7Inter-packet Delay [s]0.050.070.090.11Inter-packet Delay [s]10-210-1100101102103104105Prob. Dens.0.951.151.351.551.75Inter-packet Delay [ s]10-410-310-210-1100101102103104105106Probability Density0255075100Packet #0.951.151.351.551.75Inter-packet Delay [s]1.321.341.361.38Inter-packet Delay [s]10-210-1100101102103104105Prob. Dens. 0.35 0.150.050.250.45Inter-packet Delay [s]10-410-310-210-1100101102103104105106Probability Density0255075100Packet #0.350.150.050.250.45Inter-packet Delay [s]0.050.070.090.11Inter-packet Delay [s]10-210-1100101102103104105Prob. Dens.348drops given that the minimum IPG of 40 bits for reception
is lower than the minimum IPG of 96 bits for transmission;
as a result, a router can drop packets due to this impedance
mismatch between permissible IPGs.
Router drops often occur because a relentless, maximally
dense stream of packets presents itself: if the router store-
and-forward logic delays any packet for even the slightest
amount of time, some packet will need to be dropped to
compensate. This observation becomes particularly signiﬁ-
cant later for wide-area traﬃc; when we examine multihop
data, as noted in the Introduction, we encounter a noticeable
tendency for packets to form chains with minimal spacing,
irrespective of the homogeneity of input packets in time.
But now we recognize that packet chains can trigger router
loss.
It follows that long routes carrying high-speed data
may be prone to loss, even in an otherwise lightly loaded
network, if the receivers are not able to accept packets at
the maximum-allowed rate.
(4) Fine-grained n-ary structure: Our ﬁnal observa-
tion concerns the intriguing n-ary (secondary, tertiary, etc.)
structure present in the packet delay histograms of all en-
sembles. This structure is most readily visible in the two
ensembles comprised of 1500-Byte packets at 1 and 3 Gbps
data rates. Per Table 1, these ensembles have the lowest
packet rates and the largest inter-packet gaps. As seen in
Figures 6(a) and 6(c), they manifest thirteen and ﬁfteen lo-
cal sub-peaks, respectively, superimposed atop a background
distribution with typical monotonic fall-oﬀ from the central
peak. These sub-peaks are substantial (100× the density of
locally surrounding delay values) and uniformly distributed
by the same delay oﬀset of 32 ns.
This n-ary structure is closely related to the underlying
64b/66b Physical Coding Sublayer, as the timing separation
between these peaks is almost precisely an integer multiple
of 64b/66b frames; speciﬁcally, we measure 4.9992 frames.
Furthermore, additional n-ary structure is seen, relating to
single 64b/66b frames, as well as higher-order structure cor-
responding to half-frames (the former 100× more probable
than the latter). We recognize that this structure results
from the underlying PCS line code, which always aligns Eth-
ernet frames with either the start or middle of a 64b/66b
frame, thus explaining the fundamental half-frame period.
3.4 Results across Internet path
We next examine the same traﬃc ﬂows transitting eleven
routers and 15 000 km over NLR. The results appear in
Figure 7, with the same six inputs of homogeneously spaced
packet streams from Table 1.
Our observations here can be summarized:
1. Irrespective of input data rate, the delay distribution
peaks at a value corresponding to a minimum IPD al-
lowed by the IEEE standard, providing evidence for
our contention that packets emerge from WANs in
chains. So, after a sequence of 11 hops, even a 1 Gbps
input ﬂow evolves into a series of 10 Gbps bursts.
2. We observe packet loss, for the two highest packet-rate
ensembles (5 and 16 Mpps: 46-Byte packets at 3 and
9 Gbps), of 1.9% and 32.4%, respectively.
3. We identify multiple secondary lobes in the delay dis-
tribution. These reﬂect the formation of packet chains,
with lobe separation dependent upon data rate.
4. We again observe n-ary structure imposed by 64b/66b
line coding.
(1) Formation of packet chains for all inputs: The
ﬁrst, and most striking, observation in Figure 7 is that the
location of the primary peak does not depend upon the input
data rate. In fact, it closely corresponds to an inter-packet
delay reﬂecting the minimum-allowed inter-packet gap. (As
above, this IPD value is actually a function of the sum of
IPG and packet size and hence not identical for ensembles
of diﬀerent packet size). More packets emerge from WANs
with the minimum-allowed inter-packet delay, than with any
other inter-packet delay. Actually, as seen more clearly in
the upper-right inset of each subﬁgure, the distribution peak
is actually oﬀset from the lowest recorded value by a single
half-frame of 64b/66b line code. We conjecture that this is
related to the subtle distinction in minimum IPGs by stan-
dard (between 40 bit-times for receive and 96 bit-times for
transmit).
Now, as with the control measurements, we measure the
inter-packet delay of the peak for both packet sizes: 1500-
Byte packets show peak delays of 1226 ns, while 46-Byte
packets have peak delays of 66 ns. Both of these values cor-
respond almost precisely to the inter-packet delay between
packets of this size at their maximum data rate (approach-
ing 10 Gbps). In fact, if we assume that these packets are
separated by the minimum-allowed inter-packet gaps (96 bit-
times), we can ﬁnd theoretical expectations for the delays:
1230 ns for 1500-Byte packets and 66 ns for 46-Byte packets.
Our observed delays are within 0.3% and 0.5%, respectively,
of such theoretical values.
This demonstrates that,
irrespective of the input data
rate, network ﬂows are compressed as they transit a se-
ries of routers on a WAN. This compression reduces the
spacing between sequential packets, causing chains to form,
while introducing larger gaps between these chains. In the
experiments of Section 3.3 on the control router, we noted
a lower bound on packet spacing that created asymmetric
delay distributions, and we now see this greatly ampliﬁed
by the WAN. Indeed, one might speculate that this eﬀect
results from received packets being queued and later batch-
forwarded along the path as quickly as possible (at line rate).
Regardless of the cause, the engineering implication is that
downstream routers and receiver endpoints must be capa-
ble of lossless receipt of bursts of packets arriving at the
maximum possible data rate — 10 Gbps, here. Moreover,
this eﬀect occurs even when the original sender transmits
at a much lower average data rate. Failing to adequately
provision any component will thus trigger loss.
This ﬁnding now clariﬁes a phenomenon we measured ear-
lier [20], though could not explain: commodity servers were
receiving and then dropping network packets that had been
sent, at low data rates, across a 10 Gbps WAN. One can now
see why the network path itself, as a collection of store-and-
forward routing elements, will skew even a perfectly homo-
geneous packet stream towards a distribution with a domi-
nant peak around the maximum data rate supported by the
10GbE standard. As these chains of minimally spaced pack-
ets increase in length, packet loss is inevitable, unless all
network elements are able to handle continuous line-speed
receipt of packets.
(2) Packet loss on the WAN: We indeed observe packet
loss on this NLR path for ensembles with the highest input
packet rates (46-Byte packets at 3 Gbps and 9 Gbps): 1.9%
349(a) 1 Gbps Data Rate (1500-Byte Packets)
(b) 1 Gbps Data Rate (46-Byte Packets)