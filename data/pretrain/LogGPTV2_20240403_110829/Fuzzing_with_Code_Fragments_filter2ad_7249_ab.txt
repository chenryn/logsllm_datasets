It is even reasonable
to assume that using ﬁxed probabilities can only yield a
Figure 3: Example of a stepwise expansion on the syn-
tax tree: Dark nodes are unexpanded non-terminals (can
be expanded) while the other nodes have already been
expanded before.
coarse approximation as the real probabilities are condi-
tional, depending on the surrounding context. To over-
come these problems, we will use an algorithm that per-
forms the generation in a breadth-ﬁrst manner:
1. Set current expansion ecur to the start symbol S
2. Loop num iterations:
(a) Choose a random non-terminal n in ecur:
i. Find the set of productions Pn ⊆ P that
can be applied to n.
ii. Pick one production p from Pn randomly
and apply it to n, yielding p(n).
iii. Replace that occurrence of n in ecur by
p(n).
Figure 3 gives an example of such a stepwise expan-
sion, considering the code as a syntax tree. Dark nodes
are unexpanded non-terminals that can be considered for
expansion while the remaining nodes have already been
expanded before. This algorithm does not yield a valid
expansion after num iterations. We need to replace the re-
maining non-terminal symbols by sequences of terminal
symbols. In the learning phase of the mutation approach
we are equipped with many different examples for dif-
ferent types of non-terminals. We randomly select any
of these code fragments to replace our remaining non-
terminals. In the unlikely situation that there is no ex-
ample available, we can use the minimal expansion of
the non-terminal instead. During mutation, we can use
learned and generated code fragments.
3.3 Adjusting Fragments to Environment
When a fragment is replaced by a different fragment, the
new fragment might not ﬁt with respect to the semantics
of the remaining program. As LangFuzz does not aim to
semantically understand a speciﬁc language, we can only
perform corrections based on generic semantic assump-
tions. One example with a large impact are identiﬁers.
4
Many programming languages use identiﬁers to refer
to variables and functions, and some of them will throw
an error if an identiﬁer has not been declared prior to us-
ing it (e.g. in JavaScript, using an identiﬁer that is never
declared is considered to be a runtime error).
We can reduce the chances to have undeclared identi-
ﬁers within the new fragment by replacing all identiﬁers
in the fragment with identiﬁers that occur somewhere
in the rest of the program. Note that this can be done
purely at the syntactic level. LangFuzz only needs to
know which non-terminal in the grammar constitutes an
identiﬁer in order to be able to statically extract known
identiﬁers from the program and replace identiﬁers in the
new fragment. Thus, it is still possible that identiﬁers
are unknown at the time of executing a certain statement
(e.g. because the identiﬁer is declared afterwards), but
the chances of identiﬁer reuse are increased.
Some languages contain identiﬁers that can be used
without declaring them (usually built-in objects/globals).
The adjustment approach can be even more effective if
LangFuzz is aware of these global objects in order to ig-
nore them during the replacement process. The only way
to identify such global objects within LangFuzz is to re-
quire a list of these objects as (optional) argument. Such
global object lists are usually found in the speciﬁcation
of the respective language.
4 The LangFuzz Implementation
Based on the methods described so far, we now assem-
ble the different parts to get a proof-of-concept fuzzer
implementation that works as described in the overview
diagram (Figure 1) in the introduction.
Typically, LangFuzz starts with a learning phase
where the given sample code is parsed using the sup-
plied language grammar,
thereby learning code frag-
ments (Section 4.1). The input of this learning phase
can be either a sample code base or the test suite itself.
Once the learning step is complete, LangFuzz starts to
process the test suite. All tests are parsed and the results
are cached for performance reasons.
Then the tool starts the actual working phase:
1. From the next test to be mutated, several fragments
(determined by an adjustable parameter, typically
1–3) are randomly selected for replacement.
2. As a single fragment can be considered as multi-
ple types (e.g. if (true) {...} can be seen as
an if-statement but also more generally as a state-
ment), we randomly pick one of the possible inter-
pretations for each of those fragments.
3. Finally, the mutated test is executed and its result is
checked (Section 4.3).
5
4.1 Code Parsing
In the learning and mutation phase, we parse the given
source code. For this purpose, LangFuzz contains a
parser subsystem such that concrete parsers for different
languages can be added. We decided to use the ANTLR
parser generator framework [15] because it is widespread
and several grammars for different languages exist in the
community. The parser is ﬁrst used to learn fragments
from the given code base which LangFuzz then memo-
rizes as a token stream. When producing a mutated test,
the cached token stream is used to ﬁnd all fragments in
the test that could be replaced and to determine which
code can be replaced according to the syntax—we can
mutate directly on the cached token stream.
4.2 Code Generation
The code generation step uses the stepwise expansion
(Section 3.2) algorithm to generate a code fragment.
As this algorithm works on the language grammar,
LangFuzz also includes an ANTLR parser for ANTLR
grammars. However, because LangFuzz is a proof-of-
concept, this subsystem only understands a subset of
the ANTLR grammar syntax and certain features that
are only required for parsing (e.g. implications) are not
supported. It is therefore necessary to simplify the lan-
guage grammar slightly before feeding it into LangFuzz.
LangFuzz uses further simpliﬁcations internally to make
the algorithm easier: Rules containing quantiﬁers (’*’,
‘+’) and optionals (’?’) are de-sugared to remove these
operators by introducing additional rules according to the
following patterns:
X∗ (cid:32) (R → ε |XR)
X+ (cid:32) (R → X |XR)
X? (cid:32) (R → ε |X)
(zero or more)
(one or more)
(zero or one)
where X can be any complex expression. Furthermore,
sub-alternatives (e.g. R → ((A|B)C|D)), are split up into
separate rules as well. With these simpliﬁcations done,
the grammar only consists of rules for which each alter-
native is only a sequence of terminals and non-terminals.
While we can now skip special handling of quantiﬁers
and nested alternatives, these simpliﬁcations also intro-
duce a new problem: The additional rules (synthesized
rules) created for these simpliﬁcations have no counter-
part in the parser grammar and hence there are no code
examples available for them. In case our stepwise ex-
pansion contains one or more synthesized rules, we re-
place those by their minimal expansion as described in
Section 3.2. All other remaining non-terminals are re-
placed by learned code fragments as described earlier.
In our implementation, we introduced a size limitation
on these fragments to avoid placing huge code fragments
into small generated code fragments.
After code generation, the fragment replacement code
adjusts the new fragment to ﬁt its new environment as
described in Section 3.3. For this purpose, LangFuzz
searches the remaining test for available identiﬁers and
maps the identiﬁers in the new fragment to existing ones.
The mapping is done based on the identiﬁer name, not its
occurrence, i.e. when identiﬁer “a” is mapped to “b”, all
occurrences of “a” are replaced by “b”. Identiﬁers that
are on the built-in identiﬁer list (e.g. global objects) are
not replaced. LangFuzz can also actively map an identi-
ﬁer to a built-in identiﬁer with a certain probability.
4.3 Running Tests
In order to be able to run a mutated test, LangFuzz must
be able to run the test with its proper test harness which
contains deﬁnitions required for the test. A good exam-
ple is the Mozilla test suite: The top level directory con-
tains a ﬁle shell.js with deﬁnitions required for all tests.
Every subdirectory may contain an additional shell.js
with further deﬁnitions that might only be required for
the tests in that directory. To run a test, the JavaScript
engine must execute all shell ﬁles in the correct order,
followed by the test itself. LangFuzz implements this
logic in a test suite class which can be derived and ad-
justed easily for different test frameworks.
The simplest method to run a mutated test is to start the
JavaScript engine binary with the appropriate test har-
ness ﬁles and the mutated test. But starting the JavaScript
engine is slow and starting it over and over again would
cost enormous computation time. To solve this problem,
LangFuzz uses a persistent shell: A small JavaScript pro-
gram called the driver is started together with the test
harness. This way, we reduce the number of required
JavaScript engines to be started drastically. The driver
runs a set of tests within one single JavaScript engine
and signals completion when done. LangFuzz monitors
each persistent shell and records all input to it for later
reproduction. Of course the shell may not only be ter-
minated because of a crash, but also because of timeouts
or after a certain number of tests being run. The test
driver is language dependent and needs to be adapted for
other languages (see Section 6); such a test driver would
also be required if one implemented a new fuzzer from
scratch.
Although the original motivation to use persistent
shells was to increase test throughput it has an important
side-effect. It increased the number of defects detected.
Running multiple tests within a single shell allows indi-
vidual tests to inﬂuence each other. Different tests may
use the same variables or functions and cause crashes
that would not occur when running the individual tests
alone. In fact, most of the defects found in our experi-
ments required multiple tests to be executed in a row to
be triggered. This is especially the case for memory cor-
ruptions (e.g. garbage collector problems) that require
longer runs and a more complex setup than a single test
could provide.
Running multiple tests in one shell has the side effect
that it increases the number of source code lines executed
within each JavaScript shell. To determine which indi-
vidual tests are relevant for failure reproduction we use
the delta debugging algorithm [24] and the delta tool [9]
to ﬁlter out irrelevant test cases. The very same algorithm
also reduces the remaining number of executed source
code lines. The result is a suitably small test case.
4.4 Parameters
LangFuzz contains a large amount of adjustable param-
eters, e.g. probabilities and amounts that drive decisions
during the fuzzing process. In Table 3 (see Appendix)
we provide the most common/important parameters and
their default values. Please note that all default values are
chosen empirically. Because the evaluation of a certain
parameter set is very time consuming (1–3 days per set
and repeating each set hundreds of time times to elim-
inate the variance introduced by random generation), it
was not feasible to compare all possible parameter com-
binations and how they inﬂuence the results. We tried
to use reasonable values but cannot guarantee that these
values deliver the best performance.
5 Evaluation
To evaluate how well LangFuzz discovers undetected er-
rors in the JavaScript engines, we setup three different
experimental setups. The external validation compares
LangFuzz to the state of the art in JavaScript fuzzing.
The internal validation compares the two fragment re-
placement strategies used within LangFuzz:
random
code generation and code mutation. Finally, we con-
ducted a ﬁeld study to check whether LangFuzz is ac-
tually up to the task to detect real defects in current state
of the art JavaScript engines.
5.1 LangFuzz vs. jsfunfuzz
The state of the art fuzzer for JavaScript is the jsfunfuzz
tool written by Ruderman [17]. The tool is widely used
and has proven to be very successful in discovering de-
fect within various JavaScript engines.
jsfunfuzz is an
active part of Mozilla’s and Google’s quality assurance
and regularly used in their development.
The differences between jsfunfuzz and LangFuzz are
signiﬁcant and allow only unfair comparisons between
6
both tools. jsfunfuzz is highly adapted to test JavaScript
engines and contains multiple optimizations. jsfunfuzz is
designed to test new and previously untested JavaScript
features intensively. This of course required detailed
knowledge of the software project under test. Addition-
ally, jsfunfuzz has a certain level of semantic knowledge
and should be able to construct valid programs easier.
However, for every new language feature, the program
has to be adapted to incorporate these changes into the
testing process. Also, focusing on certain semantics can
exclude certain defects from being revealed at all.
In contrast, LangFuzz bases its testing strategy solely
on the grammar, existing programs (e.g. test suites) and
a very low amount of additional language-dependent in-
formation. In practice, this means that
• changes to the language under test do not re-
quire any program maintenance apart from possible
grammar updates; and
• through the choice of test cases, LangFuzz can be
set up to cover a certain application domain.
The use of existing programs like previous regression
tests allows LangFuzz to proﬁt from previously detected
defects. However, LangFuzz lacks a semantic back-
ground on the language which lowers the chances to ob-
tain sane programs and produce test cases that trigger a
high amount of interaction between individual parts of
the program.
Although both tools have some differences that make
a fair comparison difﬁcult, comparing both tools can un-
veil two very interesting questions:
Q1. To what extend do defects detected by LangFuzz and
jsfunfuzz overlap?
By overlap, we refer to the number of defects that both
tools are able to detect. A low overlap would indicate
that LangFuzz is able to detect new defects that were not
found and most likely will not be found by jsfunfuzz.
Therefore we deﬁne the overlap as the fraction of num-
ber of defects found by both tools and the number of de-
fects found in total. This gives us a value between zero
and one. A value of one would indicate that both tools
detected exactly the same defects. If both tools detected
totally different defects, the overlap would be zero.
overlap =
number of defects found by both tools
number of defects found in total
The second question to be answered by this compari-
son is targeted towards the effectiveness of LangFuzz.
Q2. How does LangFuzz’s detection rate compare to js-
funfuzz?
By effectiveness, we mean how many defects each tool
is able to locate in a given period of time. Even though
the overlap might be large, it might be the case that either
tool might detect certain defects much quicker or slower
than the respective other tool. To compare the effective-
ness of LangFuzz in comparison against jsfunfuzz, we
deﬁne the effectiveness as:
effectiveness =
number of defects found by LangFuzz
number of defects found by jsfunfuzz
.
This sets the number of defects found by LangFuzz
into relation to the number of defects found by jsfunfuzz.
Since both tools ran on the same time windows, the same
amount of time using identical amounts of resources (e.g.
CPU and RAM) we do not have to further normalize this
value.
Overall, this comparison answers the question whether
LangFuzz is a useful contribution to a quality assurance
process, even if a fuzzer such as jsfunfuzz is already
used. It is not our intention to show that either tool out-
performs the other tool by any means. We believe that
such comparisons are non-beneﬁcial since both jsfunfuzz
and LangFuzz operate on different assumptions and lev-
els.
5.1.1 Testing windows
We compared jsfunfuzz and LangFuzz using Mozilla’s
JavaScript engine TraceMonkey. There were two main
reasons why we decided to choose TraceMonkey as
comparison base. First, Mozilla’s development process
and related artifacts are publicly available—data that re-
quired internal permission was kindly provided by the
Mozilla development team. The second main reason was
that jsfunfuzz is used in Mozilla’s daily quality assurance
process which ensures that jsfunfuzz is fully functional
on TraceMonkey without investing any effort to setup