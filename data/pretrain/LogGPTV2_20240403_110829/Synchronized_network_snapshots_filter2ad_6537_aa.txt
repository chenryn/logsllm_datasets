title:Synchronized network snapshots
author:Nofel Yaseen and
John Sonchack and
Vincent Liu
Synchronized Network Snapshots
Nofel Yaseen
University of Pennsylvania
PI:EMAIL
John Sonchack
University of Pennsylvania
PI:EMAIL
Vincent Liu
University of Pennsylvania
PI:EMAIL
ABSTRACT
When monitoring a network, operators rarely have a !ne-
grained and complete view of the network’s state. Instead,
today’s network monitoring tools generally only measure a
single device or path at a time; whole-network metrics are
a composition of these independent measurements, i.e., an
afterthought. Such tools fail to fully answer a wide range
of questions. Is my load balancing algorithm taking advan-
tage of all available paths evenly? How much of my network
is concurrently loaded? Is application tra"c synchronized?
These types of concurrent network behavior are challenging
to capture at !ne granularity as they involve coordination
across the entire network. At the same time, understanding
them is essential to the design of network switches, archi-
tectures, and protocols.
This paper presents the design of a Synchronized Network
Snapshot protocol. The goal of our primitive is the collection
of a network-wide set of measurements. To ensure that the
measurements are meaningful, our design guarantees they
are both causally consistent and approximately synchronous.
We demonstrate with a Wedge100BF implementation the
feasibility of our approach as well as its many potential uses.
CCS CONCEPTS
• Networks → Network measurement; Network mon-
itoring; Programmable networks;
KEYWORDS
Whole-network measurement, Network snapshots
ACM Reference Format:
Nofel Yaseen, John Sonchack, and Vincent Liu. 2018. Synchronized
Network Snapshots. In SIGCOMM ’18: SIGCOMM 2018, August 20–
25, 2018, Budapest, Hungary. ACM, New York, NY, USA, 15 pages.
https://doi.org/10.1145/3230543.3230552
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for pro!t or commercial advantage and that
copies bear this notice and the full citation on the !rst page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci!c
permission and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 978-1-4503-5567-4/18/08. . . $15.00
https://doi.org/10.1145/3230543.3230552
1 INTRODUCTION
As networks continue to grow in size and bandwidth, a de-
tailed understanding of their overall behavior is increasingly
di"cult to come by. Consider the question: does my net-
work’s load balancing protocol balance the network’s load?
A de!nitive answer to this question (and others like it) is out
of the scope of traditional measurement tools.
In order to answer it, we would need visibility into the
!ne-grained behavior of the entire network. Instead, the tar-
get of traditional tools like switch counter polling and packet
sampling are individual entities in the network. Comparison
of measurements of di#erent entities is di"cult beyond just
averages and long-term behavior. Slightly better are path-
level metrics like those gathered at the end host [42], through
Explicit Congestion Noti!cation (ECN) [2], or In-band Net-
work Telemetry (INT) [22]. These path-level metrics provide
similar data as counters and packet sampling, but on the
level of entire paths; measurements from di#erent paths are,
however, still only comparable at a coarse granularity.
Thus, when faced with questions about network-wide be-
havior, operators are forced to approximate the answer using
tangential, but more easily collectible measurements. In the
case of load balancing, they might rede!ne the de!nition
of balance to a purely local metric (e.g., monitoring packet
drops or bu#er utilization for ‘high’ values) or look only at
average load. Similar workarounds exist for most questions
an operator might ask [1, 31, 42], but these approximations
can be misleading, especially in networks with bursty load
and/or high capacity [41]. The design of network switches,
architectures, and protocols depend on understanding net-
work behavior both in detail and at a network-wide scale.
This paper presents the design of a !ne-grained, accu-
rate, and precise measurement primitive that operates on
the scale of an entire network. The goal of our primitive
is the capture of a Synchronized Network Snapshot: a set of
local measurements that together provide a coherent image
of the entire network data plane at nearly a single point in
time. Enabling our work is a recent trend toward highly pro-
grammable switch data and control planes. We leverage these
tools to implement a system, Speedlight, for taking synchro-
nized network snapshots on Wedge100BF-series switches.
The implementation uses P4 and the code is open source.1
1https://github.com/eniac/Speedlight
402
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Nofel Yaseen, John Sonchack, and Vincent Liu
Compared to more traditional measurement primitives,
synchronized network snapshots are a fundamentally dis-
tributed operation—one that involves tight coordination of
the control and data planes of multiple network devices.
Through coordination, network snapshots are able to guar-
antee both causal consistency (i.e., that the measured values
are coherent) and approximate synchronicity (i.e., that the
measurements were taken near-contemporaneously). The
primitive itself is agnostic to the type of local measurement
and supports the collection of any variable accessible from
the data plane: counters, packet samples, switch state, queue
depth, etc. It is also amenable to partial deployment.
At its core, our system is inspired by distributed snapshot
protocols [10, 23]. In a classical distributed snapshot, a snap-
shot initiator sends out a message that propagates among a
set of distributed nodes to cause them to (without stopping
the system or synchronizing clocks) take snapshots of their
local state. The guarantee provided by these protocols is that
the snapshot creates a causally consistent partition of the
system’s events. For any event e that is ‘pre-snapshot’, any
event that can be construed as causing e is also pre-snapshot.
In the context of networks, this might mean that if a snap-
shot of queue depth captures a packet p in a queue q, that p
will not be counted as part of any other queue, and further-
more that the e!ects of every send and receive that led to p
being in that particular queue at that particular time are also
included in the snapshot. For that reason, distributed snap-
shots are an attractive abstraction; however their application
to high-speed networks carries a few challenges.
First, while traditional snapshots provide a set of measure-
ments that could have happened simultaneously, one of their
primary criticisms is that they do not provide any guarantee
of how close in time the measurements occurred. Second,
snapshot protocols often make strong assumptions about the
system, e.g., that nodes are single-threaded and capable of ar-
bitrary computation, and that they are connected via reliable
FIFO channels. Real switches, on the other hand, are highly
parallel, extremely limited in their data plane processing
capabilities, and exhibit non-FIFO behavior (e.g., prioritiza-
tion, packet re-circulation, etc.). It can be di"cult to adapt
certain functionality to programmable data planes [35, 36],
and distributed snapshots are no exception.
The key insight of Speedlight is that modern switches
are two-level devices. The data plane can perform extremely
#ne-grained in-band processing of network tra"c, but is fun-
damentally limited in the type of computation and resources
available. Augmenting the data plane is a control plane with
the opposite tradeo!s.
Speedlight therefore splits the responsibility of taking
snapshots such that the data and control planes each miti-
gate the weaknesses of the other. At a high level, we #rst
break the data plane of each switch into small, simple compo-
nents that obey single-threaded and FIFO assumptions. The
snapshot implementation at each of these data plane compo-
nents is not fully featured, but provides two key properties:
(1) it allows for multiple simultaneous snapshot initiators
in the style of [38], and (2) it guarantees consistency and
correctness in all cases, regardless of data plane limitations.
The control plane CPU is then responsible for the global,
PTP-coordinated initiation of a snapshot at all data plane
components, as well as the stitching together of results.
The end result of our system is that all of the individual
measurements in a synchronized network snapshot are not
only consistent, they are guaranteed to occur almost con-
temporaneously. Our current implementation guarantees a
drift of at most 10s of microseconds (less than a single RTT
in most cases); drift can be decreased further using more ad-
vanced time synchronization techniques [25]. In addition to
presenting a detailed design and implementation, we demon-
strate the primitive on real workloads. To summarize, our
work makes the following contributions:
• We present a Synchronized Network Snapshot algorithm
for the collection of distributed state within the data
plane of a network. Our design provides strong guaran-
tees regarding both the semantics of the measured values
and their timeliness.
• We then present the design and implementation of Speed-
light, a practical realization of the Synchronized Network
Snapshot algorithm. Our prototype, built for Wedge-
100BF-series switches, is able to achieve microsecond-
level synchronization of global network snapshots.
• Finally, we use our system to measure real workloads
running on our testbed. This measurement study demon-
strates both feasibility and usefulness of our approach.
2 BACKGROUND AND MOTIVATION
Network measurement is a method through which we seek
to understand network behavior. This can be in the context
of designing new protocols/architectures, evaluating existing
ones, or diagnosing issues in live networks. Over the years,
a wide range of network measurement tools and analyses
have been created to assist in the aforementioned tasks.
Measuring path-level properties. One common approach
is the use of end-to-end or $ow/path-level measurement
tools. Extremely $exible, these tools enable observers to eval-
uate, from the network edge, the aggregate e!ect of the
network in the context of application-level measures like
latency, throughput, and drop rate. An advantage to this ap-
proach is that it accurately re$ects the overall experience of
application tra"c. They also often do not require additional
network support, although there are recent exceptions [2, 22].
Though e!ective for some use cases, edge vantage points
403
Synchronized Network Snapshots
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
typically lack visibility into !ne-grained network behavior
and details of the network’s structure [31].
Measuring devices. A much more !ne-grained approach is
to measure individual network components directly. This typ-
ically takes the form of counters or packet sampling/mirroring,
but recent proposals have explored the use of more complex
metrics like "ow-based queries, heavy-hitter analysis, and
TCP-level statistics [29, 31, 39, 40]. Direct measurement is
precise, and with su#cient device support, quite expressive.
2.1 Whole-network Measurement
Path-level and device-level metrics form the foundation of
today’s measurement tools. Unfortunately, by themselves
both approaches typically provide little to no guarantees
about the relationship between measurements, or the e$ect
of clock drift and other asynchronous behavior.
For bursty and/or high-capacity networks, even small
amounts of unattended asynchronicity can lead to large inac-
curacies in measurement. As an illustrative example, consider
a datacenter network. A good NTP accuracy within a LAN
is 1 ms; in contrast, typical datacenter RTTs are an order of
magnitude lower, and there is evidence that tra#c bursts
can be even shorter (O(10 µs)) [41]. In e$ect, for any two
measurements of network behavior at di$erent locations,
their relationship is both tenuous and di#cult to bound. This
inaccuracy will only grow as network speeds increase.
Even within a single router, synchronized information is
not always available. Counters may be on di$erent line cards
and most counter polling mechanisms are not optimized for
polling more than one counter at a time. Without driver-level
modi!cations, polling a single counter on a modern switch
typically takes on the order of 1 ms [41].
For the above reasons, measurements are not often com-
pared directly. Instead, when trying to examine network-
wide behavior, most frameworks aggregate individual mea-
surements, typically using statistical analysis over relatively
long time periods so as to skirt the issue of unsynchronized
clocks. Averaging and summation are particularly common
mechanisms. An observer can compare average utilization
of multiple components to determine how they di$er over
a given time span. They can also use a total path-level drop
count in combination with network tomography to pinpoint
lossy components. Network operators have become creative
in their techniques to obliquely measure the whole network;
however, as we will see in the next section, there are still
fundamental limitations to existing tools.
2.2 A Case for Consistency
To illustrate the importance of consistent whole-network
measurement, imagine we have the simple network depicted
in Figure 1. The network consists of two ingress routers
#
!
$
"
#
!
$
"
(a) ‘Balanced’ Queues
(b) ‘Unbalanced’ Queues
Figure 1: Asynchronous measurements can be mis-
leading. These diagrams show two possible measure-
ments of queue depth for x and !. In both cases, the
network could be perfectly balanced or arbitrarily
unbalanced—the measurements fail to distinguish be-
tween the two cases.
(a and b) connected to two egress routers (x and !) in an
asymmetric fashion. Even for this simple case, many critical
questions about network behavior are di#cult to answer.
1. Is the network load balanced? We begin with the ques-
tion asked in Section 1. Imagine that an operator deploys
a new load balancing protocol to a and b. How does she
evaluate its e#cacy? How would she know if there was a
performance bug in the protocol? How does she quantify
the room for improvement?
One possible solution is to sample the queue depth at x
and !; however, on their own, these samples do not answer
the above questions. Particularly in the presence of bursty
tra#c, asynchronous measurements can provide mislead-
ing results. For instance, the balanced queue measurements
shown in Figure 1a could be a result of (a) a perfectly bal-
anced network in which queue depths never di$er, (b) an
entirely unbalanced network in which one queue is always
empty, or (c ) anything in between. All of the above is still
true if we observed unbalanced queues as in Figure 1b.
Common workarounds include averaging many samples
(an approach that captures biases and long-term e$ects, but
is not general) or only analyzing relative performance com-
pared to a previous solution (an approach that is not always
possible, and whose utility is limited). Instead, a set of con-
temporaneous measurements would give a more meaningful
view into the behavior of the network.
2. Where should we add capacity to the network? A re-
lated question is where an operator should add capacity to
the network, i.e., the process of network provisioning. To-
day, they might examine tail utilization or drops over every
link to identify bottlenecks in the network. Asynchronous
measurements are su#cient for this, but fail to answer many
followup questions. For instance, would adding a parallel
path alleviate congestion or is a per-link capacity upgrade
necessary? Balanced load among existing paths would indi-
cate the former, while localized hotspots would indicate the
404
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Nofel Yaseen, John Sonchack, and Vincent Liu
latter. They provide similarly limited insight into whether
alleviating one bottleneck would lead to others. Again, con-
temporaneous measurements would provide more insight
into network behavior.
3. Is tra!c synchronized? Synchronized measurements
can also assist in application-level debugging, especially in
the case of TCP incast and related performance problems.
Many of the same issues from the previous questions also
apply here. Today, detection of synchronized behavior is
typically done either empirically (e.g., testing if added jitter
in TCP sends alleviates the problem), or obliquely (e.g., test-
ing for characteristics of incast like high !ow count, TCP
timeouts, and drops [31, 42]). These workarounds are both
inaccurate and only possible after performance has already
been impacted. We argue that a whole-network measure-
ment primitive is a more natural and e"ective alternative.
4. What is the global forwarding state? Finally, a classic
problem in networking is the detection of bad forwarding
state. Forwarding loops are the canonical example of an un-
desirable network state that is di#cult to detect, especially
if the loops are transient and/or !apping. This class of prob-
lems have taken a newfound importance in the context of
RDMA and RoCE. RoCE’s PFC mechanisms can cause net-
work deadlocks, not only when there are routing loops, but
in many other cases as well [17]. For a general method of ver-
ifying and diagnosing these issues, a consistent snapshot is
crucial—otherwise we can observe states that are impossible.
3 OVERVIEW
We seek to design a measurement primitive that captures
a set of measurements representing a meaningful view of
the whole network as it appears at a single point in time.