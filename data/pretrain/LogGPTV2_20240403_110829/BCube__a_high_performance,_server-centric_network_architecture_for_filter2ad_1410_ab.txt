path(A, B) = {A, };
I N ode = A;
for(i = k; i ≥ 0; i − −)
if (A[πi] (cid:54)= B[πi])
I N ode[πi] = B[πi];
append I N ode to path(A, B);
return path(A, B);
Figure 2: BCubeRouting to ﬁnd a path from A to
B. The algorithm corrects one digit at one step. The
digit correcting order is decided by the predeﬁned
permutation Π.
3.2 Single-path Routing in BCube
We use h(A, B) to denote the Hamming distance of two
servers A and B, which is the number of diﬀerent digits of
their address arrays. Apparently, the maximum Hamming
distance in a BCubek is k + 1. Two servers are neighbors if
they connect to the same switch. The Hamming distance of
two neighboring servers is one. More speciﬁcally, two neigh-
boring servers that connect to the same level-i switch only
diﬀer at the i-th digit in their address arrays. By utilizing
this fact, we design BCubeRouting, as we illustrate in Fig. 2,
to ﬁnd a path from a source server to a destination server.
In BCubeRouting, A=akak−1 ··· a0 is the source server
and B=bkbk−1 ··· b0 is the destination server, and Π is a
permutation of [k, k− 1,··· , 1, 0]. We systematically build a
series of intermediate servers by ‘correcting’ one digit of the
previous server. Hence the path length is at most k+1. Note
that the intermediate switches in the path can be uniquely
determined by its two adjacent servers, hence are omitted
from the path. BCubeRouting is similar to the routing al-
gorithm for Hypercube. This is not by chance, but because
BCube and the generalized Hypercube share similarity as
we have discussed in Section 3.1.
From BCubeRouting, we obtain the following theorem.
Theorem 1. The diameter, which is the longest shortest
path among all the server pairs, of a BCubek, is k + 1.
In practice, k is a small integer, typically at most 3. There-
fore, BCube is a low-diameter network.
65/*A=akak−1 · · · a0 and B=bkbk−1 · · · b0; A[i] = ai; B[i] = bi;*/
BuildPathSet(A, B):
PathSet = { };
for(i = k; i ≥ 0; i − −)
if (A[i] (cid:54)= B[i])
else /*A[i] == B[i]*/
Pi=DCRouting(A, B, i);
C= a neighbor of A at level i; /*C[i] (cid:54)= A[i]*/
Pi=AltDCRouting(A, B, i, C);
add Pi to PathSet;
return PathSet;
DCRouting(A, B, i):
m = k;
for (j = i; j ≥ i − k; j − −)
Π[m] = j mod (k + 1); m = m − 1;
path = BCubeRouting(A, B, Π);
return path;
AltDCRouting(A, B, i, C):
path={A,};
m = k;
for (j = i − 1; j ≥ i − 1 − k; j − −)
Π[m] = j mod (k + 1); m = m − 1;
path += BCubeRouting(C, B, Π);
return path;
Figure 3: The algorithm to calculate the k+1 parallel
paths between servers A and B.
3.3 Multi-paths for One-to-one Trafﬁc
Two parallel paths between a source server and a destina-
tion server exist if they are node-disjoint, i.e., the interme-
diate servers and switches on one path do not appear on the
other. The following theorem shows how to generate two
parallel paths between two servers.
Theorem 2. Given that two servers A = akak−1 ··· a0
and B = bkbk−1 ··· b0 are diﬀerent in every digit (i.e., ai (cid:54)=
bi for i ∈ [0, k]). BCubeRouting generates two parallel paths
from A to B using two permutations Π0 = [i0, (i0−1) mod (k+
1),··· , (i0−k) mod (k+1)] and Π1 = [i1, (ii−1) mod (k+
1),··· , (i1 − k) mod (k + 1)] (i0 (cid:54)= i1 and i0, i1 ∈ [0, k]).
The permutations Π0 and Π1 start from diﬀerent locations
of the address array and then correct the digits sequentially.
This pattern ensures that the used switches are always at
diﬀerent levels for the same digit position, thus producing
the two parallel paths. The formal proof of Theorem 2 is
given in Appendix A.
From Theorem 2, we see that when the digits of A and B
are diﬀerent, there are k + 1 parallel paths between them.
It is also easy to observe that the number of parallel paths
between two servers be upper bounded by k + 1, since each
server has only k + 1 links. The following theorem speciﬁes
the exact number of parallel paths between any two servers.
Theorem 3. There are k + 1 parallel paths between any
two servers in a BCubek.
We show the correctness of Theorem 3 by constructing
such k + 1 paths. The construction procedure, BuildPath-
Set, is based on Theorem 2 and shown in Fig. 3. For two
servers A and B, the paths built by BuildPathSet fall into
two categories: the paths constructed by DCRouting using
permutations start from digits ai (cid:54)= bi and those constructed
by AltDCRouting. There are h(A, B) and k + 1− h(A, B)
P3 : {0001, , 1001, , 1011}
P2 : {0001, , 0101, , 0111,
, 1111, , 1011}
P1 : {0001, , 0011, , 1011}
P0 : {0001, , 0002, , 1002,
, 1012, , 1011}
Figure 4: An example showing the parallel paths
between two servers A (0001) and B (1011) in a
BCube3 with n = 8. There are 2 paths with length
2 (P3 and P1) and 2 paths with length 4 (P2 and P0).
paths in the ﬁrst and second categories, respectively. From
Theorem 2 (and by removing the digits ai = bi in all the
servers), we can see that the paths in the ﬁrst category are
parallel.
Next, we show that paths in the second category are also
parallel. Assume ai = bi and aj = bj for two diﬀerent i and
j. From Fig. 3, the i-th digit of all the intermediate servers
in path Pi is a value ci (cid:54)= ai, whereas it is ai in all the
intermediate servers in path Pj. Similarly, the j-th digits
of the intermediate servers in Pi and Pj are also diﬀerent.
The intermediate servers in Pi and Pj diﬀer by at least two
digits. The switches in Pi and Pj are also diﬀerent, since a
switch connects only to servers that diﬀer in a single digit.
Hence the paths in the second category are parallel.
Finally, we show that paths in diﬀerent categories are par-
allel. First, the intermediate servers of a path in the second
category are diﬀerent from the servers in the ﬁrst category,
since there is at least one diﬀerent digit (i.e., the i-th digit
ci). Second, the switches of a path in the second category
are diﬀerent from those in the ﬁrst category (due to the fact
that switches in the second category have ci whereas those
in the ﬁrst category have ai in the same position).
From BuildPathSet, we further observe that the maximum
path length of the paths constructed by BuildPathSet be
k + 2. The lengths of the paths in the ﬁrst and second
categories are h(A, B) and h(A, B) + 2, respectively. The
maximum value of h(A, B) is k + 1, hence the maximum
path length is at most k + 3. But k + 3 is not possible, since
when h(A, B) = k + 1, the number of paths in the second
category is 0. The parallel paths created by BuildPathSet
therefore are of similar, small path lengths. It is also easy
to see that BuildPathSet is of low time-complexity O(k2).
Fig. 4 shows the multiple paths between two servers 0001
and 1011 in a BCube network with n = 8 and k = 3. The
Hamming distance of the two servers is h(A, B) = 2. We
thus have two paths of length 2. These two paths are P3
and P1. We also have two paths of length h(A, B) + 2 = 4.
These two paths are P2 and P0, respectively. For clarity, we
also list the intermediate switches in the paths. It is easy to
verify that all these paths are parallel, since an intermediate
server or switch on one path never appears on other paths.
It is easy to see that BCube should also well support
several-to-one and all-to-one traﬃc patterns. We can fully
utilize the multiple links of the destination server to accel-
erate these x-to-one traﬃc patterns.
3.4 Speedup for One-to-several Trafﬁc
We show that edge-disjoint complete graphs with k + 2
servers can be eﬃciently constructed in a BCubek. These
complete graphs can speed up data replications in distributed
ﬁle systems like GFS [12].
66Theorem 4. In a BCubek, a server src and a set of
servers {di|0 ≤ i ≤ k}, where di is an one-hop neighbor
of src at level i (i.e., src and di diﬀer only at the i-th digit),
can form an edge-disjoint complete graph.
0 − d(cid:48)
i(0 ≤ i < k) and dk diﬀer in the i-th digit.
We show how we recursively construct such an edge-disjoint
complete graph. Suppose src and d0−dk−1 are in a BCubek−1
B0, and dk is in another BCubek−1 B1. Assume that servers
in B0 have already formed a complete graph. We show
how to construct the edges among dk and the rest servers
d0 − dk−1. The key idea is to ﬁnd k servers d(cid:48)
k−1,
where d(cid:48)
It is
easy to see that the Hamming distance between di and dk is
two. We can then establish an edge between di and dk via
the intermediate server d(cid:48)
i. This edge uses the level-k link
of di and the level-i link of dk. This edge is node-disjoint
with other edges: it does not overlap with the edges in B0
since it uses the level-k link of di; it also does not overlap
with the edges in B1 since it uses the level-i link of dk. In
this way, we can recursively construct the edges between
dk−1 and di(0 ≤ i < k − 1), using the level-(k − 1) links of
di(0 ≤ i < k − 1) and level-i link of dk−1, etc.
/*Servers are denoted using the address array form.
BuildMultipleSPTs(src):
src[i] denotes the i-th digit of the address array of src.*/
for(i = 0; i ≤ k; i + +)
root=src’s level-i neighbor and root[i] = (src[i] + 1) mod n;
T reei = {root, };
BuildSingleSPT(src, T reei, i);
BuildSingleSPT(src, T, level):
Part I:
for(i = 0; i ≤ k; i + +)
dim = (level + i) mod (k + 1);
T2 = {};
for (each server A in T)
C = B = A;
for (j = 0; j < n − 1; j + +)
C[dim] = (C[dim] + 1) mod n;
add server C and edge (B, C) to T2;
B = C;
add T2 to T ;
Part II:
for (each server S (cid:54)= src and S[level] = src[level])
S2 = S; S2[level] = (S[level] − 1) mod n;
add server S and edge (S2, S) to T;
From the construction procedure, we see that the diameter
of the constructed complete graph is only two hops. For
a server src, there exist a huge number of such complete
graphs. src has n− 1 choices for each di. Therefore, src can
build (n − 1)k+1 such complete graphs.
In distributed ﬁle systems such as GFS [12], CloudStore [5],
and HDFS [4], a ﬁle is divided into chunks, and each chunk is
replicated several times (typically three) at diﬀerent chunk
servers to improve reliability. The replicas are chosen to lo-
cate at diﬀerent places to improve reliability. The source
and the selected chunk servers form a pipeline to reduce the
replication time: when a chunk server starts to receive data,
it transmits the data to the next chunk server.
The complete graph built in BCube works well for chunk
replication for two reasons: First, the selected servers are
located at diﬀerent levels of BCube, thus improving repli-
cation reliability. Second, edge-disjoint complete graph is
perfect for chunk replication speedup. When a client writes
a chunk to r (r ≤ k + 1) chunk servers, it sends 1
r of the
chunk to each of the chunk server. Meanwhile, every chunk
server distributes its copy to the other r−1 servers using the
disjoint edges. This will be r times faster than the pipeline
model.
3.5 Speedup for One-to-all Trafﬁc
We show that BCube can accelerate one-to-all traﬃc sig-
niﬁcantly. In one-to-all, a source server delivers a ﬁle to all
the other servers. The ﬁle size is L and we assume all the
links are of bandwidth 1. We omit the propagation delay
and forwarding latency. It is easy to see that under tree and
fat-tree, the time for all the receivers to receive the ﬁle is at
least L. But for BCube, we have the following theorem.
Theorem 5. A source can deliver a ﬁle of size L to all
the other servers in L
k+1 time in a BCubek.
We show the correctness of Theorem 5 by constructing k+
1 edge-disjoint server spanning trees. Edge-disjoint means
that an edge in one spanning tree does not appear in all the
other ones. Fig. 5 shows how such k+1 server spanning trees
are constructed. BuildMultipleSPTs constructs the k + 1
spanning trees from the k + 1 neighbors of the source. A
Figure 5: Build the k + 1 edge-disjoint server span-
ning trees from a source server in a BCubek.
Figure 6: The two edge-disjoint server spanning
trees with server 00 as the source for the BCube1
network in Fig. 1(b).
level-i neighbor diﬀers from the source in the i-th digit. We
then systematically add servers into the tree starting from
that level.
Intuitively, the trees are edge-disjoint because
a server is added to diﬀerent trees using links of diﬀerent
levels. The formal proof is omitted due to space limitation.
Fig. 6 shows the two edge-disjoint spanning trees with server
00 as the source for the BCube1 network of Fig. 1(b).
When a source distributes a ﬁle to all the other servers, it
can split the ﬁle into k + 1 parts and simultaneously deliver
all the parts via diﬀerent spanning trees. Since a receiving
server is in all the spanning trees, it receives all the parts and
hence the whole ﬁle. The time to deliver a ﬁle of size L to
all is therefore L
k+1 in a BCubek. No broadcast or multicast
is needed in BuildMultipleSPTs. Hence we can use TCP to
construct the trees for reliable data dissemination.
3.6 Aggregate Bottleneck Throughput for All-
to-all Trafﬁc
Under the all-to-all model, every server establishes a ﬂow
with all other servers. Among all the ﬂows, the ﬂows that re-
67ceive the smallest throughput are called the bottleneck ﬂows.
The aggregate bottleneck throughput (ABT) is deﬁned as
the number of ﬂows times the throughput of the bottleneck
ﬂow. The ﬁnish time of an all-to-all task is the total shuﬄed
data divided by ABT. ABT therefore reﬂects the network
capacity under the all-to-all traﬃc pattern.
Theorem 6. The aggregate bottleneck throughput for a
BCube network under the all-to-all traﬃc model is n
1), where n is the switch port number and N is the number
of servers.
n−1 (N −
See Appendix B on how we derive the number. An ad-
vantage of BCube is that BCube does not have performance
bottlenecks in the all-to-all traﬃc model since all the links
are used equally. As a result, the ABT of BCube increases
linearly as the number of servers increases. As we will show
in Section 6, the ABT of BCube decreases gracefully under
both server and switch failures.
4. BCUBE SOURCE ROUTING
We require that the routing protocol be able to fully utilize