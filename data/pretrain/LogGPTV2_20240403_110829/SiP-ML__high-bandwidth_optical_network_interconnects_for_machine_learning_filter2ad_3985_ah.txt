[142] Qixiang Cheng, Sebastien Rumley, Meisam Bahadori, and Keren Bergman. Pho-
tonic switching in high performance datacenters. Opt. Express, 26(12):16022–
16043, Jun 2018.
[143] G. Michelogiannakis, Y. Shen, X. Meng M. Y. Teh, B. Aivazi, T. Groves, J. Shalf,
M. Glick, M. Ghobadi, L. Dennison, and K. Bergman. Bandwidth steering for
hpc using silicon nanophotonics. ACM/IEEE Supercomputing Conference (SC),
10 2019.
[144] Keren Bergman, John Shalf, George Michelogiannakis, Sebastien Rumley, Larry
Dennison, and Monia Ghobadi. Pine: An energy efficient flexibly interconnected
photonic data center architecture for extreme scalability. In 2018 IEEE Optical
Interconnects Conference (OI), OI ’18, 2018.
[145] Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin. Network flows.
1988.
[146] Robert E Tarjan. Dynamic trees as search trees via euler tours, applied to the
network simplex algorithm. Mathematical Programming, 78(2):169–177, 1997.
[147] James B Orlin, Serge A Plotkin, and Éva Tardos. Polynomial dual network
simplex algorithms. Mathematical programming, 60(1-3):255–276, 1993.
[148] Prabhakar Raghavan and Clark D. Thompson. Randomized rounding: A tech-
nique for provably good algorithms and algorithmic proofs. Technical Report
UCB/CSD-85-242, EECS Department, University of California, Berkeley, May
1985.
672
SiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
A APPENDIX
Appendices are supporting material that has not been peer-reviewed.
Step 2: Compute min-cost flow. Having constructed the graph
G, we solve the following flow routing problem:
A.1 SiP-Ring
One of the core properties of SiP-ML-Ring is the ability to dynami-
cally place bandwidth around the static topology to maximize the
throughput between communicating nodes for model parallel jobs.
Note that for ring-allreduce data parallel jobs, there is no need to
reschedule the bandwidth once a physical ring is established by
the patch panel. However, we find that model parallel jobs benefit
from bandwidth rescheduling. The optimal bandwidth allocation
maximizes the throughput while ensuring no two paths sharing
the same fiber are assigned the same wavelength. More formally,
the bandwidth allocation problem corresponds to the following
optimization problem. Let T Mij denote the predicted GPU-to-GPU
traffic matrix, and W denote the total number of wavelengths (a.k.a
available bandwidth). We can represent a wavelength allocation as
a 3-dimensional binary matrix, Λ, where Λijk is 1 if GPU i sends
data to GPU j using λk and is zero otherwise. There are several
possible objectives. A natural one is to minimize the maximum com-
pletion time for any GPU-to-GPU transfer, where the completion
time is T Mi j
. This can be expressed as an Integer Linear Program
k Λi jk
(ILP) by maximizing the minimum inverse of the completion time,
as follows:
(cid:80)
(cid:80)
s.t.
(1) (cid:80)
(2)
(3)
maximize
Λ∈{0,1}N×N×W min
ij:T Mi j >0
k Λijk /T Mij
(j≤i ≤N +l )∩(l 0
where for an edge e = (u, v), fe is the flow on the edge, and
T Me = T Muv is the traffic demand on that edge. The constraints
(not shown for brevity) are the standard flow conservation con-
straints. The intuition for the above objective is that we wish to
maximize throughput but preferentially allocate a larger flow (more
wavelengths) to GPU-to-GPU paths with smaller demand. The rea-
son for favoring smaller demands is to complete them quickly,
reducing the number of nodes with which each node must commu-
nicate. This keeps the unsatisfied traffic pattern sparse over time,
allowing the remaining traffic to be handled efficiently in future
wavelength reconfiguration events.
The objective in Eq. (2) can be equivalently be written as a min-
The problem is then to minimize(cid:80)
cost flow routing problem [145] by defining the weight of edge e
as we = −1/T Me if T Me > 0, and we = 0 if e is a dummy edge.
e we fe. Min-cost flow routing
can be solved using the network simplex algorithm [145–147]. The
procedure for constructing the graph and defining the flow routing
problem is slightly more complicated when the cut chosen for
adding the source and sink nodes includes more than one edge. In
this case, we need additional constraints to ensure consistency of
flows between the cut edges.
In the more general case of cuts with higher degrees, suppose we
would like to inject the flows at the segment between Node3 and
Node4. The problem remains basically the same, but we need to add
the following three constraints in addition to the flow conservation
constraints: (1) X = X ′, (2) Y = Y ′, and (3) X + Y = 1. We can
simply add these constraints to our simplex problem as well.
The constraints are (1) ensure fiber segments do not contain over-
lapping wavelengths (ring constraint), and (2) ensure each GPU can
use each wavelength for communication with, at most, one other
GPU (node constraint).
Note that the size of the ILP solution space, Λ ∈ {0, 1}N×N×W ,
grows with the number of nodes in the network, rendering it in-
tractable at larger scale. Therefore, instead of solving the ILP, we
present a more practical algorithm that turns this discrete optimiza-
tion problem into a min-cost flow routing problem which can be
solved efficiently.
Step 1: Communication graph construction. We construct a
directed communication graph, G = (V , E), where V is the set of
nodes and for every T Muv > 0, there is a directed edge e = (u, v).
After including edges for the entire T M in G, we check whether
every adjacent node-pair on the topology is connected in G. If not,
we add a “dummy” edge between them to E. The direction of all
edges in G is the same as that of wave propagation on the fiber.We
then add dummy sink and source nodes by cutting the edges in G
along an arbitrary topology segment. For simplicity, let us assume
for now that this process cuts only one edge of the graph.We add
two terminal nodes on the two ends of the cut edge to be the source
and sink. The source node injects a unit-sized flow into the ring
and the sink node receives it.
Figure 13: Wavelength allocation and its equivalent flow
routing translation for multiedge cut.
Step 3: Remove and repeat. The solution obtained by solving
the above min-cost flow problem may result in some GPU-to-GPU
demands completing very quickly. However, since reconfiguration
incurs delay (e.g., 25 µs in our prototype), we cannot reconfigure
wavelengths too quickly without hurting efficiency (more on this
below). Therefore we should plan the wavelength allocation based
on a time horizon rather than looking only at the instantaneous
traffic demands. To this end, we iteratively solve the min-cost flow
673
Node4RGBRRGBNode4Src Node13123131Sink Node(a) Wavelength allocation(b) Flow allocationNode3Node2Node3Node2Node1Node123XX’YY’SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
problem in Equation (2), serving theT M with step-size of ∆ based on
the flows obtained after each iteration, and repeating this procedure
until there is no unserved demand left in the T M. We compute the
mean of the flow allocations over all iterations as the final flow
allocation.
Step 4: Mapping flows to bandwidth. Finally, we scale the flows
from the previous step by W and map them to integer numbers
using a technique called randomized rounding [148]. This produces
the final compute and bandwidth allocation. An important con-
sideration in SiP-ML’s design is how frequently to reschedule the
bandwidth allocations. By rescheduling frequently, we can better
tailor the bandwidth allocation to meet the traffic demands. But
rescheduling too quickly is undesirable, because each reconfigu-
ration incurs a delay during which no traffic can be sent. In our
experiments, we found setting the rescheduling period to 100 µs
(4× the reconfiguration delay) provides the best performance.
A.2 SiP-OCS ILP
Similar to §A.1, we assumeT Mij denotes the estimated traffic matrix
between GPUs i and j. We have N GPUs and Q OCSs each with N
ports. There is B/Q bandwidth available between each GPU and
OCS. Let P ∈ {0, 1}N×N×Q stand for the permutation configuration
of all OCSs with Pijk = 1 if there is a circuit between GPUs i and j
on OCS k. Therefore, the total available bandwidth between GPUs
i and j would be: (B/Q )
can be expressed as an Integer Linear Program (ILP) by maximizing
the minimum inverse of the completion time, as follows:
(cid:17). Our circuit scheduling goal
(cid:16)(cid:80)Q
k =1 Pijk
(cid:80)
maximizeP ∈{0,1}N×N×Q min
ij:T Mi j >0
s.t.
(1) (cid:80)
(2) (cid:80)
i Pijk
j Pijk
k Pijk /T Mij
(3)
≤ 1
≤ 1
∀j, k
∀i, k
where constraints (1) and (2) would enforce the OCS configurations
to be in the form of a permutation for each OCS; i.e., each GPU can
establish a circuit with only one other GPU on each OCS. For com-
mercial OCSs that have orders of magnitude higher reconfiguration
delay than MRRs, we only use one-shot configuration. For such
configurations, our experiments show ILPs can be solved reason-
ably fast enough for thousands of nodes. Note that with one-time
scheduling, this optimization happens only once at the beginning
of training each new workload.
A.3 Scaling Efficiency of the Placement
In Fig. 14, we compare the scaling efficiency of SiP-ML’s placement
algorithm on 1024 GPUs to the efficiency achieved in the most
recent version of the MLPerf training benchmark [88]. We highlight
the following takeaways: 1) workloads like ResNet50 are too small to
be efficiently scaled to 1000s of GPUs; 2) our placement generalizes
to electrical topologies without degree constraint; 3) placement
with optical degree constraints respects the compute efficiency in
addition to interconnect constraints; 4) overall, SiP-ML achieves
up to 4.3× better scaling efficiency than today’s expert-designed
parallelization strategies for clusters in MLPerf benchmark.
674
SiP-ML (∆ = ∞)
MLPerf v0.7
35
SiP-ML (∆ = 16)
)
%
(
y
c
n
e
i
c
ffi
E
g
n
i
l
a
c
S
30
25
20
15
27 28 29 210 211 212 213
100
75
50
25
0
27 28 29 210 211 212 213
BW per GPU (Gbps)
BW per GPU (Gbps)
(a) ResNet50
(b) Transformer
Figure 14: Comparing the scaling efficiency of our place-
ment algorithm at different bandwidths to state-of-the-art
expert designed placements in MLPerf benchmark for 1024
GPUs.
Figure 15: System level diagram of GPU nodes with scalable
SiP select/bypass interface. The incoming 64 wavelengths
are separated into four groups with 16 wavelengths each for
select/bypass.
A.4 Optical Simulations
Fig. 15 demonstrates our approach to achieve SiP interfaced GPU
nodes at large scale. Every WDM input of 64 wavelengths from the
previous GPU node is first de-interleaved into 4 groups with 16
wavelengths each. We use cascaded SiP micro-ring filters to perform
wavelength selective add/drop or to pass wavelength(s) through the
node based on the requirement of global scheduler. To overcome
the spectral power variability caused by the multi-staged optical
components, we add optical amplifiers, optical (de)multiplexers and
variable optical attenuators (VOAs) to equalize the optical power for
each wavelength at the output of the GPU node. An interleaver then
combines all 4 groups and forwards the new WDM signal to the
next GPU node. We simulate our SiP add/drop interface using the
American Institute for Manufacturing Integrated Photonics (AIM
SiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Photonics) process design kit (PDK) in OptSim software. The add/-
drop filters are from the AIM PDK and the (de)interleavers are built
with cascaded 2-stage MZI. The optical multiplexer/demultiplexers
are designed using ideal OptSim models with a bandwidth of 0.5nm.
The multiplexer/demultiplexer function can also be implemented
with multimode interference (MMI) couplers. In the simulation,
we achieve an equalized optical spectrum at the output of a GPU
node for two cases: 1) 64 bypass wavelengths; 2) 64 wavelengths
with 32 wavelengths being dropped and added, while the other 32
wavelengths bypassing the node.
675