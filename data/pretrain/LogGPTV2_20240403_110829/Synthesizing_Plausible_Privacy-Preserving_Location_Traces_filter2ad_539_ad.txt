y
y
d
d
z
z
w
w
y
y
d
d
z
g
g
a
a
w
w
x
x
d
d
g
g
a
a
w
w
x
x
g
g
a
a
w
w
x
x
b
b
g
g
a
a
w
w
x
x
l
l
b
b
p
p
x
x
l
l
b
b
p
Fig. 3: A sketch of generating a fake trace from a seed. Each location
is represented by an English letter in a box. The semantic class
associated with each location is represented by a different color. The
semantic seed trace is a trace that includes the locations in the seed
along with other locations in the same cluster at each time instant.
Here, locations are clustered as {y, d, f, t, z},{g, a, w, x},{l, b, p}.
To generate a fake trace, we ﬁrst probabilistically remove the seed lo-
cation and probabilistically merge subsequent classes. In our example,
f, z, p are removed, and w, d, b, x are merged into their neighboring
visited clusters. We then run a decoder to generate a probable trace
given the possibility of choosing from all available locations at each
time instant. The fake trace, shown with connected dashed boxes,
will be approved if it passes the privacy tests.
we add some randomness to the probabilities such that the
algorithm does not deterministically select the most probable
location. More precisely, we slightly perturb the probabilities
in such a way that Viterbi selects randomly among a set of
locations that are close in probability to the most probable
location. We implement this idea by choosing a parameter
parv and multiplying all the probabilities of moving from one
location to the next with a random number between 1 and
parv, which slightly randomizes our trace decoding.
Each generated trace is tested against our privacy test
(Sections V-D and V-E). If it passes, we compute its likelihood
based on the aggregate mobility model. At a later stage, we
can randomly select fake traces to use based on this likelihood.
Appendix A contains a brief discussion of the computational
efﬁciency of the generation algorithm.
C. Threat Model
There are two types of privacy threats that we need to
consider, which should not be confused with each other as
they apply to different settings. The ﬁrst threat is against the
individuals whose traces are sampled and used as seeds in our
algorithm. This is of more concern in the scenario of synthetic
dataset release. In this scenario, the adversary knows that all
traces are synthetic. Yet, he wants to extract geographic or
semantic information about the seed traces, or ﬁnd the identity
of the individuals behind them. This is the threat that we are
concerned about, in this section, in the process of generating
synthetic traces. We deﬁne two privacy requirements to defeat
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
this threat: statistical dissimilarity and plausible deniability. At
the last step of synthesizing traces, we run a PrivacyTest()
to enforce these two privacy requirements on each synthetic
trace that is to be released.
The second type of attack is not perpetrated on the synthetic
traces themselves. It is rather implemented against the queries
received from LBS users that use the fake traces to hide
their true locations. We do not need to address such threat
here. However, we evaluate the effectiveness of our synthetic
traces against location inference attacks in preserving location
privacy of LBS users later in Section VII.
D. Privacy Requirement: Statistical Dissimilarity
A synthetic trace and its seed are statistically dependent,
otherwise we cannot achieve utility. This is at the core of
the privacy and utility tradeoff in any privacy-preserving
sanitization algorithm. The goal is to guarantee some statistical
privacy while preserving utility.
In our case, we guarantee a statistical dissimilarity between
synthetic trace and its seed. To this end, two types of distance
functions can be considered: (i) a distance between two
probability distributions that model the synthetic trace and its
seed. And, (ii) a distance between the two traces themselves.
As for the statistical distance (i), because there is a notion of
(Euclidean) distance between locations that form a trace, we
use Earth Mover’s Distance between two probability distribu-
tions that represent the mobility models behind the two traces.
This is exactly what we compute as geographic similarity
metric. So, for all seeds s and all synthetic traces f generated
from s, we ensure that the statistical similarity between f and
s is bounded by δs.
simG(f, s) ≤ δs
(12)
As for the trace distance (ii) between f and s, we use the
intersection between the two. In this case, for all seeds s and
all synthetic traces f generated from s, we ensure that the size
of their intersection set is bounded by δi.
|intersection(f, s)| ≤ δi
(13)
We reject any synthetic trace that fails to satisfy either of
the above conditions. Thus, we ensure a minimum statistical
dissimilarity between a seed and its fake trace.
Intuitively,
these tests prevent
the leakage of privacy-
sensitive locations. To understand why, consider Alice, a seed
contributor, and the locations she visits daily, such as her home
and workplace. In a released synthesized trace (from Alice’s
seed) these locations will not be visited. This is enforced
by (12). But, what about atypical behaviors? Suppose Alice
spends her morning at a women’s health clinic (something
out of the ordinary for her). (13) enforces that the women’s
health clinic will also not be visited in the released synthesized
trace. However, locations visited that morning are likely to
include health-related services locations (e.g., hospitals), since
these may belong to the same semantic cluster. Could this
information (i.e., that Alice visited a health-related location)
be leaked? No,
this is prevented by plausible deniability
(Section V-E).
E. Privacy Requirement: Plausible Deniability
Enforcing a minimum statistical dissimilarity between a fake
f and its seed s would limit the information leakage of f
about s. In other words, this ensures a minimum error in
reconstructing s by observing f. However, this is not enough
to guarantee the location privacy of the individual associated
with s due to the semantic similarity between s and f. Note
that, due to utility requirements, our algorithm synthesizes
traces f that are semantically very similar to s. Although,
because of the randomness of our decoding algorithm, this
semantic similarity varies, but it is mostly small. The threat
is that an adversary, who has some background information
about the individual associated with s, might be able to infer
the inclusion of that individual’s record in the seed dataset by
observing f.2
The membership inclusion attack would work if s is by far
the only real trace from which we could have generated f,
i.e., when s is an outlier. To defeat such attacks, we introduce
plausible deniability as a privacy requirement. To guarantee
plausible deniability, we make use of the alternative real
dataset A which is disjoint with the seed dataset S. Concretely,
the generated fakes (and their corresponding seeds) need to
satisfy the following deﬁnition.
Deﬁnition 4. A synthetic trace f (generated from seed s ∈ S)
satisﬁes (k, δ)-plausible deniability if there are at least k ≥ 1
alternative traces a ∈ A such that:
|simS(s, f ) − simS(a, f )| ≤ δ .
(14)
In other words, for any fake f generated from seed s,
we want to guarantee that we can ﬁnd at least k alternative
traces a ∈ A for which (14) holds for δ = δd. Speciﬁcally,
the privacy test rejects all synthetic traces that do not satisfy
(k, δd)-plausible deniability.
If condition (14) holds for a synthetic trace, then its seed is
not the only real trace that could have generated it, and it is
plausible that the same synthetic trace is generated from some
other real traces (those that are even outside the seed dataset
and have no contribution to generation of the synthetic trace).
Thus, the inclusion of a particular trace in the seed dataset is
plausibly deniable.
Intuitively, this safeguards the privacy of semantic outliers
(i.e., seed contributors with atypical semantic behavior). To
understand why, consider Alice, a seed contributor, who works
the night shift, whereas most contributors work during the day.
(14) enforces that Alice’s synthetic trace can only be released
if there exist k other semantically similar traces in A (i.e.,
only if at least k other alternative dataset contributors work
during the night in a manner similar to Alice). Therefore, an
adversary trying to run a membership inclusion attack will
be thwarted even if he has the knowledge that Alice works
during the night. Note that, as we select the alternatives from
2However, this does not imply that the adversary can accurately reconstruct
s, especially if f satisﬁes the statistical dissimilarity requirements (12),(13).
554554
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
outside the seed dataset, this holds even if Alice is the only
seed dataset contributor who works during the night.
F. Discussion: Plausible Deniability as a New Privacy Notion
In this paper, we present plausible deniability as a new
notion of privacy for data synthesis. In this section, we further
discuss this notion and compare it with similar deﬁnitions in
the literature. As said before, plausible deniability implies that
a synthetic trace could have been generated from alternative
location traces other than its own seed. This means that the
information that is learned from observing a fake trace could
have also been learned if the same fake trace was generated
from other traces.
Note that we generate utility-preserving traces and release
them only if they satisfy plausible-deniability privacy require-
ments. Other sanitization techniques in the literature, based on
e.g., differential privacy [14] and crowd-blending privacy [16],
enforce privacy in the process of sanitizing data. This makes
it very challenging to design utility-preserving mechanisms
under the constraints of privacy requirements.
To better understand the implications of plausible deniability
on privacy,
let us consider the cases where an adversary
observes a synthetic trace generated from a seed trace that
exhibits some rare characteristics, due to the particular lifestyle
of the person who produced that trace. To avoid leaking about
its seed, either (1) the synthetic trace must be semantically far
enough from its seed so that it is equally close to alternative
real traces, or else (2) there must already be alternative traces
with similar rare characteristics. If neither of the two is true,
the synthetic trace will not be released. This is similar to the
notion of suppressing the sanitized data from outliers [16].
In fact,
the overlap between the set of possible synthetic
traces that can be generated from different real traces is the
acceptable area for releasing synthetic data.
in spirit,
Plausible deniability,
is similar to some other
notions of privacy. In crowd blending privacy [16] and its
followup outlier privacy [31],
the space from which data
records are drawn is publicly split into partitions. Then, only
the partitions that contain a minimum number of data points
can produce sanitized data. The rest of data records, called
outliers, need to perturbed with magniﬁed noise, which would
not lead to higher utility than simply suppressing them. The
authors show that if crowd blending privacy is combined with
subsampling of data records, it can achieve zero knowledge
privacy [17] which is stronger that differential privacy [14].
Plausible deniability can also be guaranteed by differentially
private mechanisms. Although it is possible, in theory, to gen-
erate fake traces in a differential private way, we do not know
methods to do this efﬁciently due to the high-dimensionality of
location traces. Speciﬁcally, this is practically infeasible given
the existing mechanisms such as the Exponential Mechanism
that require to assign a score to each possible trace given the
input dataset.
Lastly, the plausible deniability privacy test, which requires
each synthetic trace to be δd-indistinguishable from at least k
alternative traces, should not be confused with k-anonymity.
555555
(b) Visited locations colored according to
their semantic clustering (20 clusters).
(a) Visited Locations. The size of locations
are proportional to their total visits.
Fig. 4: 400 locations visited around Lausanne and nearby towns by
the 30 users. Some users commute between two towns whereas the
majority of them live and work in the same city of Lausanne (the
area with higher concentration).
Unlike our notion of privacy, k-anonymity is a syntactic metric,
achieved by suppressing or generalizing data, which does not
prevent attribute disclosure. It has also been shown to be
severely vulnerable to inference attacks when used to protect
location privacy [46].
VI. EVALUATION SETUP
In this section, we run our algorithms on a set of real
location traces and evaluate the resulting utility and privacy
in two scenarios: sharing locations with LBS, and releasing
synthetic location datasets.
A. Dataset
The dataset we use for the evaluation is collected through
the Nokia Lausanne Data Collection Campaign (see [25]). We
prepare the dataset for our needs in two phases, ﬁlling gaps
in the traces and discretizing the time and location.
The raw dataset contains combination of events of three
types: GPS coordinates, WLAN and GSM identiﬁers. We
construct valid traces (out of partial traces) by ﬁlling gaps. We
interpolate along the path of consecutive GPS points, using the
WLAN and GSM information.
We then extract two days of traces for each user, such
that each trace (of one day) contains a sequence of 72
locations (i.e., one location is reported every 20 minutes).
Some locations are visited very rarely only by very few users.
Thus, we reduce the number of locations from 1491 to 400
by clustering close-by locations together. We use a hierarchical
clustering algorithm for this purpose, and place the locations
that are geographically close or have very few visits in one
cluster. The geographical distribution of visits of all users over
the locations in the considered area is shown in Figure 4(a).
From all traces, we then sub-sample 30 user traces. The 1st
day of traces for these users is used as seed dataset S, whereas
the 2nd day of traces will be used as baseline (testing set)
during the evaluation. Using the seed dataset, we compute
the mobility proﬁles of all 30 users, and then the semantic
location graph by calculating a similarity score for each pair
of locations, averaged across all users. After clustering this
semantic location graph, we obtained 20 location clusters. We
choose this number of clusters as it provides optimal clustering
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
0.8
0.6
0.4
0.2
s
r
i
a
p
f
o
n
o
i
t
r
o
p
o
r
P
0
0
0.2
0.4
0.6
Geographic Similarity
0.1
0.08
0.06
0.04
0.02
s
r
i
a
p
f
o
n
o
i
t
r
o
p
o
r
P
0.8
1
0
0
0.2
0.4