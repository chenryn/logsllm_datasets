whose lineage contains an input value at pktlen. The
value was changed to MAXUINT by our mutator. It caused
m alloc to allocate a buffer with 35 bytes (sizeof
*packet->pkt.user id =36). The number of assign-
ments at 1597, decided by pktlen, exceeded the buffer
and resulted in a crash.
Our another integer overﬂow case study was on zgv-5.8.
This case has been explained earlier in Section 2 and will
not be repeated here.
5.2 Experience With New Vulnerabilities
So far we have assumed a perfect frontend that only
points us to suspects that are guilty. Next we present our ex-
perience of connecting our system to a real static vulnerabil-
ity detection tool called RATS [2], which can detect buffer
overﬂow and even integer overﬂow with user extensions.
We applied our system to a few most-recent software ver-
sions. The ﬁrst one we tried was ipgrab-0.9.9. RATS
reported 106 buffer overﬂow suspects. We tried to convict
these suspects one by one using our system. We found that
the 48th suspect is a real one. The vulnerability, which is
presented in Figure 6, lies at line 357 in file.c.
It is
a buffer overﬂow caused by an integer overﬂow. To be-
gin with, we used a random generated benign input. The
input was not hard to acquire because any input packet
will touch the suspect. The mutator altered the lineage of
header.inc len to MAXUINT, and it caused a segmen-
tation fault at line 357 because the parameter to malloc
is 0 while fread tries to read MAXUINT bytes. Thus,
through one round of mutation, we proved the existence of
this vulnerability. Using the same methodology, we have
found and proved another two new integer overﬂows caus-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:15:55 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE254DSN 2008: Lin et al.Table 3. Performance and Space Overhead of Lineage Tracing
Performance (seconds)
Space (bytes)
W/O Log
With Log
Program
Metrics
ncompress-4.2.4
gzip-1.2.4
bftpd-1.0.11
nullhttpd-0.5.0
wu-ftpd-2.6.0
cﬁngerd-1.4.3
ngircd-0.8.2
zgv-5.8
gnuPG-1.4.3
Time to compress a 4.3k bytes ﬁle
Time to compress a 15.7k bytes ﬁle
Response time of an automated user authentication
Response time of processing a 512 bytes post packet
Response time of an automated user authentication
Response time of a normal lookup request
Response time of an automated 5 sequence irc commands
Time to display a 1.4k bytes malicious gif ﬁle
Time to verify a 1.2k bytes signature ﬁle
Normal
0.001
0.004
0.014
0.007
0.018
0.015
0.021
0.011
0.012
1.740
2.700
0.302
0.452
0.486
0.517
0.342
20.909
29.298
Ratio
1740
675
21
65
27
34
16
1901
2441
1.960
9.645
0.318
0.463
0.526
0.543
0.378
21.965
86.926
Ratio
1960
2411
23
66
29
36
18
1997
7243
Link-list
4296576
3163228
6808
120736
11496
39508
33032
971328
2898128
Bdd
460700
1086220
4160
21980
6340
10820
16060
14000
279160
Ratio
9.33
2.91
1.63
5.49
1.81
3.65
2.07
69.38
10.38
With respect to performance, we measure three scenarios,
without lineage tracing, with lineage tracing but without
logging, and with both lineage tracing and logging. For the
daemon programs, we indirectly measure their performance
by measuring their response time, and for the utility appli-
cations, we directly measure the running times. The setup
and the result are presented in Table 3.
Without logging,
the performance slow down factor
varies from 16 to 2441. If logging is enabled, its perfor-
mance overhead varies from 23 to 7243 times. The large
overhead factors for utility programs are mainly due to the
fact that the total running times of these programs include
the starting and ending times of the Valgrind engine,
which is signiﬁcant compared with the real execution time.
The numbers for daemon programs, ranging from 16 to 66,
are closer to the real slowdown since we excluded the time
spent on Valgrind by inserting performance monitors to
the programs. Note that network latency is not an issue
here because we were using the local network interface. We
believe the performance has greatly beneﬁted from using
roBDD. One can easily imagine the overhead of perform-
ing set operations on up to a few thousands elements during
each step of execution. Another observation is that if the ap-
plication is data-intensive (e.g., gnuPG), the log ﬁle is very
large (nearly 10M in this case), causing a lot of runtime
overhead. Due to some historical reason, we used an old
version of Valgrind, which incurs ten times slowdown
even without any instrumentation. Furthermore, we have
not strived to optimize the system because performance is
not yet a critical factor for us.
For the space overhead, as illustrated in Table 3, we
can see that a link-list based approach will cost much more
space than our roBDD based approach, especially for data-
intensive applications.
6 Related Work
In recent years, there have been signiﬁcant advance in
automated code based test generation [9, 10, 11, 13]. The-
oretically, these techniques can be applied to our problem
of automated evidence generation. However in practice,
they have inherent limitations that constrain their applica-
tion. First, most these techniques are tuned to unit testing
Figure 5. Part of the CFG of gnupg.
ing buffer overﬂow vulnerabilities in dcraw-7.94, and
epstool-3.3. We have reported these vulnerabilities
with evidence to their developers. They replied promptly,
admitting the existence of these defects.
{
334 while(1)
335
336
337
338
/* Read the header */
ret = fread((void *) &header,
sizeof(snoop_packet_header_t),1,fp);
355
356
357
358
...
/* Get the actual packet */
packet = my_malloc(header.inc_len+1);
ret = fread((void *)packet,header.inc_len,1,fp);
...
Figure 6. File.c of Ipgrab-0.9.9
5.3 Performance and Space Overhead
We also used the above 9 benchmark programs to mea-
sure performance and space overhead of the lineage tracing
module, which is the performance dominator in our system.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:15:55 UTC from IEEE Xplore.  Restrictions apply. 
parse_keyparse_user_idOther_parse_functionpkttype=6pkttype=13desired path 397   switch(pkttype)1585    packet->pkt.user_id = m_alloc            (sizeof *packet->pkt.user_id + pktlen);Lineage of ecx is the first byte of inputOur benign input first goes hereInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE255DSN 2008: Lin et al.due to the scalability issue. Second, these techniques work
by combining concrete execution with constraint solving to
explore all potential program paths. Third, solving sym-
bolic constraints requires the user to specify symbolic vari-
ables in the source code, demanding not only the access to
the source code but also a certain level of understanding.
In contrast, our technique is a light-weight whole pro-
gram technique that explore a subset of program paths. It
does not require source code access and it does not require
understanding the program in most cases.
TaintCheck [15] represents another type of dynamic
techniques that are relevant. Our technique can be con-
sidered as a generalization of TaintCheck. More speciﬁ-
cally, TaintCheck uses one bit to color program execution
as input-relevant or input-irrelevant. By contrast, we “taint”
each program execution point with a set of relevant input
values. Our scenario is more challenging because sets may
have various numbers of elements, with the upper bound of
the universal set of inputs. Furthermore, TaintCheck is pro-
posed as an online technique to detect attacks on the ﬂy. Re-
ducing runtime overhead is its major concern. This is also
true for other dynamic techniques such as control ﬂow in-
tegrity checking [16] and data ﬂow integrity checking [17].
Our technique aims to generate evidence off-line.
7 Conclusions
In this paper, we propose a data lineage tracing based
dynamic approach to generate evidence for remote ex-
ploitable vulnerabilities in software. More speciﬁcally, it
associates an execution point suspect with a set of input
bytes, whose values are then mutated ofﬂine to generate an
exploit (evidence). Our approach delivers both efﬁciency
and effectiveness. Using our system, we are able to
reproduce exploits for all the known vulnerabilities we
studied. We also successfully identiﬁed and convicted a
number of new vulnerabilities, which were all promptly
conﬁrmed by the developers. Our evaluation also shows
that the system has reasonable overhead for the scenario of
ofﬂine diagnosis.
Acknowledgments
We thank the anonymous reviewers for their detailed,
helpful comments. This work is supported in part by NSF
grants CNS-0720516, CNS-0708464 and CNS-0716444.
References
[1] http://www.dwheeler.com/ﬂawﬁnder/
[2] http://www.fortifysoftware.com/security-resources/rats.jsp
[3] http://diablo.elis.ugent.be
[4] Buddy, a binary decision diagram package. Department of
Information Technology, Technical Univ. of Denmark.
[5] C. Meinel and T. Theobald. Algorithms and data structures
in vlsi design, 1998. Springer.
[6] Valgrind: A Framework for Heavyweight Dynamic Binary
Instrumentation. N. Nethercote and J. Seward. In Proc. of
ACM PLDI, June 2007.
[7] B.P. Miller, L. Fredriksen, and B. So. An Empirical Study
of the Reliability of UNIX Utilities. Communications of the
ACM 33, 12, Dec. 1990.
[8] J. E. Forrester and B. P. Miller. An Empirical Study of the
Robustness of Windows NT Applications Using Random
Testing. In Proc. of the 4th USENIX Windows System Sym-
posium, 2000.
[9] C. Cadar, V. Ganesh, P. Pawlowski, D. Dill, and D. Engler.
Exe: automatically generating inputs of death. In Proc. of
ACM CCS, Nov. 2006.
[10] K. Sen, D. Marinov, and G. Agha. Cute: A concolic unit
testing engine for c. In Proc. of ACM ESEC/FSE-13, 2005.
[11] P. Godefroid, N. Klarlund, and K. Sen. Dart: Directed auto-
mated random testing. In Proc. of ACM PLDI, 2005.
[12] A. Moser, C. Kruegel, and E. Kirda. Exploring multiple ex-
ecution paths for malware analysis. In Proc. of 2007 IEEE
Symposium on Security and Privacy, May 2007.
[13] P. Godefroid, M. Levin, and D. Molnar. Automated whitebox
fuzz testing. In Proc. of NDSS, San Deigo, CA, Feb. 2008.
[14] M. Egele, C. Kruegel, E. Kirda, H. Yin, and D. Song. Dy-
namic Spyware Analysis. In Proc. of the 2007 USENIX An-
nual Technical Conference, Santa Clara, CA, June 2007.
[15] J. Newsome and D. Song. Dynamic taint analysis for au-
tomatic detection, analysis, and signature generation of ex-
ploits on commodity software.. In Proc. of NDSS, Feb. 2005.
[16] M. Abadi, M. Budiu, U. Erlingsson, and J. Ligatti. Control-
Flow Integrity: Principles, Implementations, and Applica-
tions. In Proc of ACM CCS, Nov. 2005.
[17] M. Castro, M. Costa, and T. Harris. Securing Software by
Enforcing Data-ﬂow Integrity. In Proc. of OSDI, Nov. 2006.
[18] D. Wagner, J. S. Foster, E. A. Brewer, and A. Aiken. A ﬁrst
step towards automated detection of buffer overrun vulnera-
bilities. In Proc. of NDSS, Feb. 2000.
[19] D. Larochelle and D. Evans. Statically Detecting Likely
Buffer Overﬂow Vulnerabilities. In Proc. of USENIX Secu-
rity, 2001.
[20] M. Zitser, D. Shaw, T. Leek and R. Lippman. Testing Static
Analysis Tools Using Exploitable Buffer Overﬂows From
Open Source Code. In Proc. of ACM ESEC/FSE-11, 2004.
[21] Y. Xie, A. Chou, and D. Engler. Archer: using symbolic,
In
path-sensitive analysis to detect memory access errors.
Proc. of ACM ESEC/FSE-10, 2003.
[22] J. Ferrante, K. Ottenstein, J. Warren. The program depen-
dence graph and its use in optimization. ACM Trans. on Pro-
gramming Languages and Systems, 9(3),1987.
[23] M. Weiser. Program slicing. In Proc. of ICSE, 1981.
[24] X. Zhang, R. Gupta, and Y. Zhang. Efﬁcient forward com-
putation of dynamic slices using reduced ordered binary de-
cision diagrams. In Proc. of ICSE, 2004.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:15:55 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE256DSN 2008: Lin et al.