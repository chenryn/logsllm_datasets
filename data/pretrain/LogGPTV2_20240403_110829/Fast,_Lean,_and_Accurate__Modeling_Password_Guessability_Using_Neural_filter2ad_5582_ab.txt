modeling
Output
Figure 1: An example of using a neural network to predict
the next character of a password fragment. The network is
being used to predict a ‘d’ given the context ‘ba’. This network
uses four characters of context. The probabilities of each next
character are the output of the network. Post processing on the
network can infer probabilities of uppercase characters.
start with an empty password, and query the network
for the probability of seeing a ‘b’, then seeing an ‘a’ af-
ter ‘b’, and then of seeing a ‘d’ after ‘ba’, then of see-
ing a complete password after ‘bad’. To generate pass-
words from a neural network model, we enumerate all
possible passwords whose probability is above a given
threshold using a modiﬁed beam-search [64], a hybrid
of depth-ﬁrst and breadth-ﬁrst search. If necessary, we
can suppress the generation of non-desirable passwords
(e.g., those against the target password policy) by ﬁlter-
ing those passwords. Then, we sort passwords by their
probability. We use beam-search because breadth-ﬁrst’s
memory requirements do not scale, and because it al-
lows us to take better advantage of GPU parallel pro-
cessing power than depth-ﬁrst search. Fundamentally,
this method of guess enumeration is similar to that used
in Markov models, and it could beneﬁt from the same op-
timizations, such as approximate sorting [37]. A major
advantage over Markov models is that the neural network
model can be efﬁciently implemented on the GPU.
Calculating Guess Numbers
In evaluating password
strength by modeling a guessing attack, we calculate a
password’s guess number, or how many guesses it would
take an attacker to arrive at that password if guessing
passwords in descending order of likelihood. The tradi-
tional method of calculating guess numbers by enumera-
tion is computationally intensive. For example, enumer-
ating more than 1010 passwords would take roughly 16
days in our unoptimized implementation on an NVidia
GeForce GTX 980 Ti. However, in addition to guess
number enumeration, we can also estimate guess num-
bers accurately and efﬁciently using Monte Carlo simu-
lations, as proposed by Dell’Amico and Filippone [34].
3.2 Our Approach
There are many design decisions necessary to train neu-
ral networks. The design space forces us to decide on
178  25th USENIX Security Symposium 
USENIX Association
4
the modeling alphabet, context size, type of neural net-
work architecture, training data, and training methodol-
ogy. We experiment along these dimensions.
Model Architectures
In this work, we use recurrent
neural networks because they have been shown to be use-
ful for generating text in the context of character-level
natural language [49, 84]. Recurrent neural networks are
a speciﬁc type of neural network where connections in
the network can process elements in sequences and use
an internal memory to remember information about pre-
vious elements in the sequence. We experiment with two
different recurrent architectures in Section 5.1.
Alphabet Size We focus on character-level models,
rather than more common word-level models, because
there is no established dictionary of words for pass-
word generation. We also complement our analysis with
exploratory experiments using syllable-level models in
Section 5.1. We decided to explore hybrid models based
on prior work in machine learning [68]. In the hybrid
construction, in addition to characters, the neural net-
work is allowed to model sub-word units, such as sylla-
bles or tokens. We chose to model 2,000 different tokens
based on prior work [68] and represent those tokens the
same way we would characters. A more thorough study
of tokenized models would explore both more and fewer
tokens. Using tokenized structures, the model can then
output the probability of the next character being an ‘a’
or the token ‘pass’. We generated the list of tokens by to-
kenizing words in our training set along character-class
boundaries and selecting the 2,000 most frequent ones.
Like prior work [26], we observed empirically that
modeling all characters unnecessarily burdens the model
and that some characters, like uppercase letters and rare
symbols, are better modeled outside of the neural net-
work. We can still create passwords with these charac-
ters by interpreting the model’s output as templates. For
example, when the neural network predicts an ‘A’ char-
acter, we post-process the prediction to predict both ‘a’
and ‘A’ by allocating their respective probabilities based
on the number of occurrences of ‘a’ and ’A’ in the train-
ing data—as shown in Figure 1. The intuition here is that
we can reduce the amount of resources consumed by the
neural network when alternate heuristic approaches can
efﬁciently model certain phenomena (e.g., shifts between
lowercase and uppercase letters).
Password Context Predictions rely on the context
characters. For example, in Figure 1, the context char-
acters are ‘ba’ and the target prediction is ‘d’. Increasing
the number of context characters increases the training
time, while decreasing the number of context characters
could potentially decrease guessing success.
We experimented with using all previous characters in
the password as context and with only using the previous
ten characters. We found in preliminary tests that using
ten characters was as successful at guessing and trained
up to an order of magnitude faster, and thus settled on
this choice. When there are fewer than ten context char-
acters, we pad the input with zeros. In comparison, best-
performing Markov models typically use ﬁve characters
of context [34, 65]. While Markov models can overﬁt if
given too much context, neural networks typically overﬁt
when there are too many parameters.
Providing context characters in reverse order—e.g.,
predicting ‘d’ from ‘rowssap’ instead of ‘passwor’—has
been shown to sometimes improve performance [48]. We
empirically evaluate this technique in Section 5.1.
Model Size We must also decide how many parameters
to include in models. To gauge the effect of changing the
model size on guessing success, we test a large neural
network with 15,700,675 parameters and a smaller net-
work with 682,851 parameters. The larger size was cho-
sen to limit the amount of time and GPU memory used
by the model, which required one and a half weeks to
fully train on our larger training set. The smaller size was
chosen for use in our browser implementation because it
could realistically be sent over the Internet; compressed,
this network is a few hundred kilobytes. We evaluate the
two sizes of models with a variety of password policies,
since each policy may respond differently to size con-
straints, and describe the results in Section 5.1.
Transference Learning We experimented with a spe-
cialized method of training neural networks that takes ad-
vantage of transference learning, in which different parts
of a neural network learn to recognize different phenom-
ena during training [97]. One of the key problems with
targeting non-traditional password policies is that there
is little training data. For example, in our larger training
set, there are 105 million passwords, but only 2.6 mil-
lion satisfy a password policy that requires a minimum
of 16 characters. The sparsity of training samples lim-
its guessing approaches’ effectiveness against such non-
traditional policies. However, if trained on all passwords,
the learned model is non-optimal because it generates
passwords that are not accurate for our target policy even
if one ignores passwords that do not satisfy the policy.
Transference learning lets us train a model on all pass-
words, yet tailor its guessing to only longer passwords.
When using transference learning, the model is ﬁrst
trained on all passwords in the training set. Then, the
lower layers of the model are frozen. Finally, the model
is retrained only on passwords in the training set that ﬁt
USENIX Association  
25th USENIX Security Symposium  179
5
the policy. The intuition is that the lower layers in the
model learn low-level features about the data (e.g., that
‘a’ is a vowel), and the higher layers learn higher-level
features about the data (e.g., that vowels often follow
consonants). Similarly, the lower layers in the model
may develop the ability to count the number of char-
acters in a password, while the higher level layers may
recognize that passwords are typically eight characters
long. By ﬁne-tuning the higher-level parameters, we can
leverage what the model learned about all passwords and
retarget it to a policy for which training data is sparse.
Training Data We experimented with different sets of
training data; we describe experiments with two sets of
passwords in Sections 4.1 and 5.2, and also with includ-
ing natural language in training data in Section 5.1. For
machine-learning algorithms in general, more training
data is better, but only if the training data is a close match
for the passwords we test on.
3.3 Client-Side Models
Deploying client-side (e.g., browser-based) password-
strength-measuring tools presents severe challenges. To
minimize the latency experienced by users, these tools
should execute quickly and transfer as little data as pos-
sible over the network. Advanced guessing tools (e.g.,
PCFG, Markov models, and tools like JtR and Hash-
cat) run on massively parallel servers and require on
the order of hundreds of megabytes or gigabytes of disk
space. Typically, these models also take hours or days
to return results of strength-metric tests, even with re-
cent advances in efﬁcient calculation [34], which is un-
suitable for real-time feedback.
In contrast, by com-
bining a number of optimizations with the use of neu-
ral networks, we can build accurate password-strength-
measuring tools that are sufﬁciently fast for real-time
feedback and small enough to be included in a web page.
3.3.1 Optimizing for Model Size
To deploy our prototype implementation in a browser, we
developed methods for succinctly encoding it. We lever-
aged techniques from graphics for encoding 3D models
for browser-based games and visualizations [29]. Our
encoding pipeline contains four different steps: weight
quantization, ﬁxed-point encoding, ZigZag encoding,
and lossless compression. Our overall strategy is to send
fewer bits and leverage existing lossless compression
methods that are natively supported by browser imple-
mentations, such as gzip compression [41]. We describe
the effect that each step in the pipeline has on compres-
sion in Section 5.3. We also describe encoding a short
wordlist of passwords in Bloom ﬁlters.
Weight Quantization First, we quantized the weights
of the neural network to represent them with fewer digits.
Rather than sending all digits of the 32-bit ﬂoating-point
numbers that describe weights, we only send the most
signiﬁcant digits. Weight quantization is routinely used
for decreasing model size, but can increase error [68].
We show the effect of quantization on error rates in Sec-
tion 5.3. We experimentally ﬁnd that quantizing weights
up to three decimal digits leads to minimal error.
Fixed-point Encoding Second, instead of representing
weights using ﬂoating-point encoding, we used ﬁxed-
point encoding. Due to the weight-quantization step,
many of the weight values are quantized to the same
values. Fixed-point encoding allows us to more suc-
cinctly describe the quantized values using unsigned in-
tegers rather than ﬂoating point numbers on the wire: one
could internally represent a quantized weight between
−5.0 and 5.0 with a minimum precision of 0.005, as be-
tween −1000 and 1000 with a precision of 1. Avoiding
the ﬂoating-point value would save four bytes. While
lossless compression like gzip partially reduces the need
for ﬁxed-point encoding, we found that such scaling still
provides an improvement in practice.
ZigZag Encoding Third, negative values are generally
more expensive to send on the wire. To avoid sending
negative values, we use ZigZag encoding [8]. In ZigZag
encoding, signed values are encoded by using the last bit
as the sign bit. So, the value of 0 is encoded as 0, but
the value of -1 is encoded as 1, 1 is encoded as 2, -2 is
encoded as 3, and so on.
Lossless Compression We use regular gzip or
deflate encoding as the ﬁnal stage of the compression
pipeline. Both gzip and deflate produce similar re-
sults in terms of model size and both are widely sup-
ported natively by browsers and servers. We did not con-
sider other compression tools, like LZMA, because their
native support by browsers is not as widespread, even
though they typically result in slightly smaller models.
Bloom Filter Word List To increase the success
of client-side guessing, we also store a word list
of frequently guessed passwords.
As in previous
work [89], we found that for some types of password-
cracking methods, prepending training passwords im-
proves guessing effectiveness. We stored the ﬁrst two
million most frequently occurring passwords in our train-
ing set in a series of compressed Bloom ﬁlters [69].
Because Bloom ﬁlters cannot map passwords to the
number of guesses required to crack, and only compute
180  25th USENIX Security Symposium 
USENIX Association
6
existence in a set, we use multiple Bloom ﬁlters in dif-
ferent groups: in one Bloom ﬁlter, we include passwords
that require fewer than 10 guesses; in another, all pass-
words that require fewer than 100 guesses; and so on.
On the client, a password is looked up in each ﬁlter and
assigned a guess number corresponding to the ﬁlter with
the smallest set of passwords. This allows us to roughly
approximate the guess number of a password without in-
creasing the error bounds of the Bloom ﬁlter. To dras-
tically decrease the number of bits required to encode
these Bloom ﬁlters, we only send passwords that meet
the requirements of the policy and would have neural-
network-computed guess numbers more than three or-
ders of magnitude different from their actual guess num-
bers. We limited this word list to be about 150KB after
compression in order to limit the size of our total model.
We found that signiﬁcantly more space would be needed
to substantially improve guessing success.
3.3.2 Optimizing for Latency
We rely on precomputation and caching to make our pro-
totype sufﬁciently fast for real-time feedback. Our target
latency is near 100 ms because that is the threshold below
which updates appear instantaneous [72].
Precomputation We precompute guess numbers in-
stead of calculating guess numbers on demand because
all methods of computing guess numbers on demand
are too slow to give real-time feedback. For example,
even with recent advances in calculation efﬁciency [34],
our fastest executing model, the Markov model, requires
over an hour to estimate guess numbers of our test set
passwords, with other methods taking days. Precomputa-
tion decreases the latency of converting a password prob-
ability to a guess number: it becomes a quick lookup in
a table on the client.
The drawback of this type of precomputation is that
guess numbers become inexact due to the quantization
of the probability-to-guess-number mapping. We exper-
imentally measure (see Section 5.3) the accuracy of our
estimates, ﬁnding the effect on accuracy to be low. For
the purpose of password-strength estimation, we believe
the drawback to be negligible, in part because results are
typically presented to users in more heavily quantized
form. For instance, users may be told their password is
“weak” or “strong.” In addition, the inaccuracies intro-
duced by precomputation can be tuned to result in safe
errors, in that any individual password’s guess number
may be an underestimate, but not an overestimate.
Caching Intermediate Results We also cache results
from intermediate computations. Calculating the proba-
bility of a 10-character password requires 11 full compu-
tations of the neural network, one for each character and
one for the end symbol. By caching probabilities of each
substring, we signiﬁcantly speed up the common case in
which a candidate password changes by having a charac-
ter added to or deleted from its end. We experimentally
show the beneﬁts of caching in Section 5.3.
Multiple Threads On the client side, we run the neural
network computation in a separate thread from the user
interface for better responsiveness of the user interface.
Implementation
3.4
We build our server-side implementation on the Keras li-
brary [28] and the client-side implementation on the neo-
cortex browser implementation [5] of neural networks.
We use the Theano back-end library for Keras, which
trains neural networks faster by using a GPU rather than
a CPU [17,18]. Our implementation trains networks and
guesses passwords in the Python programming language.
Guess number calculation in the browser is performed in
JavaScript. Our models typically used three long short-
term memory (LSTM) recurrent layers and two densely
connected layers for a total of ﬁve layers. On the client
side, we use the WebWorker browser API to run neural
network computations in their own thread [10].
For some applications, such as in a password meter, it
is desirable to conservatively estimate password strength.
Although we also want to minimize errors overall, on the
client we prefer to underestimate a password’s resistance
to guessing, rather than overestimate it. To get a stricter
underestimate of guess numbers on our client-side im-
plementation, we compute the guess number without re-
spect to capitalization. We ﬁnd in practice that our model
is able to calculate a stricter underestimate this way,
without overestimating many passwords’ strength. We
don’t do this for the server-side models because those
models are used to generate candidate password guesses,
rather than estimating a guess number. After computing
guess numbers, we apply to them a constant scaling fac-
tor, which acts as a security parameter, to make the model
more conservative at the cost of making more errors. We
discuss this tradeoff more in Section 5.3.
4 Testing Methodology
To evaluate our implementation of neural networks, we
compare it to multiple other password cracking meth-
ods, including PCFGs, Markov models, JtR, and Hash-
cat. Our primary metric for guessing accuracy is the
guessability of our test set of human-created passwords.
The guessability of an individual password is measured
by how many guesses a guesser would take to crack a
USENIX Association  
25th USENIX Security Symposium  181
7
password. We experiment with two sets of training data
and with ﬁve sets of test data. For each set of test data,
we compute the percentage of passwords that would be
cracked after a particular number of guesses. More accu-
rate guessing methods correctly guess a higher percent-
age of passwords in our test set.
For probabilistic methods—PCFG, Markov models,
and neural networks—we use recent work to efﬁ-
ciently compute guess numbers using Monte Carlo meth-
ods [34]. For Monte Carlo simulations, we generate and
compute probabilities for at least one million random
passwords to provide accurate estimates. While the exact
error of this technique depends heavily on each method,
guess number, and individual password, typically we ob-
served 95% conﬁdence intervals of less than 10% of the
value of the guess-number estimate; passwords for which
the error exceeded 10% tended to be guessed only after
more than 1018 guesses. For all Monte Carlo simulations,
we model up to 1025 guesses for completeness. This is
likely an overestimate of the number of guesses that even
a well-resourced attacker could be able to or would be in-
centivized to make against one password.
To
calculate
of
passwords
guessability
using
mangling-rule-based methods—JtR and Hashcat—
we enumerate all guesses that these methods make. This
provides exact guess numbers, but fewer guesses than we
simulate with other methods. Across our different test
sets, the mangling-rule-based methods make between
about 1013 and 1015 guesses.
4.1 Training Data
To train our algorithms, we used a mixture of leaked and
cracked password sets. We believe this is ethical because
these password sets are already publicly available and we
cause no additional harm with their use.
We explore two different sets of training data. We term
the ﬁrst set the Password Guessability Service (PGS)
training set, used by prior work [89].
It contains the
Rockyou [90] and Yahoo! [43] leaked password sets. For