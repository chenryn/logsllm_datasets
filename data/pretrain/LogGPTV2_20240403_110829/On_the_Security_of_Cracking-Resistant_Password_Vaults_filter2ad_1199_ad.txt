3.36 % 0.67 % 1.44 % 19.11 % 0.10 % 13.64 %
2.43 % 0.73 % 1.34 % 16.51 % 0.11 % 7.88 %
1.53 % 0.10 % 1.01 % 12.69 % 0.15 % 1.53 %
All 2.54 % 0.80 % 1.37 % 18.31 % 0.38 % 12.82 %
All 2.43 % 0.56 % 1.31 % 16.15 % 0.17 % 6.54 %
and for our Markov model-based NLE we implemented a
similar solution. If implemented incorrectly, i. e., if the NLE
outputs vaults with an unrealistic password reuse, this can
be used as an indicator for real vaults as well. Similar to
the correlation feature of Section 5.1, preliminary tests have
shown that reuse is a relatively weak indicator, thus again,
we use it with a small weight only, mostly breaking ties in
the KL divergence.
For each vault, we calculate the reuse rate, i. e., given
two randomly chosen passwords from the vault, what is the
probability that these two are equal. In addition, we calcu-
lated a reuse rate for “similar” passwords, where similarity
is measured by the Levenshtein edit distance for thresholds
ranging from 1 to 5. This measure has been used before [2]
in the context of reuse. Finally, we use a weighted average
of these six reuse rates as the ﬁnal indicator.
Results from these experiments are summarized in Ta-
ble 3. We see that the results do not vary greatly. The
median rank for the real vault with NoCrack is 1.99 %, thus
does not improve the KL divergence attack result (median of
1.97 %). For Markov, we see the same, with a median rank-
ing result of 14.28 % compared to the KL divergence attack
with a median of 14.24 %. These results show that both,
the NoCrack NLE as well as the Markov NLE, accurately
simulate the available data in PBVault. However, there is
no other data available to cross-check these results, and we
expect this attack to perform better on fresh data, which
may have a diﬀerent behavior in terms of password reuse.
5.3 Password Policies
Many websites enforce password-composition policies on
the passwords of its users. These rules diﬀer from site to
site and may change over time. Thus, it is diﬃcult for an
NLE to create passwords for a speciﬁc site that adhere to
the imposed rules, without “overdoing” it and choosing un-
realistically strong passwords. Bojinov et al. [3] surveyed
policies for the Kamouﬂage system and found that the ma-
jority of large sites apply a minimum length criterion, e. g.,
at least 8 chars that should be considered by an NLE. We
found that Chatterjee et al. [9] reported policy compliance
for their UNIF NLE, which builds computer-generated pass-
words, but not for the main NLE, the vault-generating SG.
In the following experiments, we assume that the user has
at least one account stored in the vault that requires a min-
imum password length of 8. If decrypting the vault yields
a shorter password for this speciﬁc account, we discard this
vault as being non-compliant.
Some results from this experiment are summarized in Ta-
ble 3. They show that the median rank for the real vault
in NoCrack is 1.37 %, thus improving the KL divergence at-
tack result (median of 1.97 %). For Markov, we see the same,
with a median ranking result of 12.82 % compared to the KL
divergence attack with a median of 14.24 %.
In principle, it is possible to prevent attacks based on the
violation of password policies. One would need to keep track
of password policies for the sites of interest, and modify the
encoder to only generate compliant passwords. This task is,
however, complicated by the fact that policies change over
time, so we might need to re-encode the vault when a policy
has changed. Also, password policies are not available in a
machine-readable format, yet. However, a ﬁrst solution for
this problem has been proposed by Horsch et al. [16].
Figure 3: The median rank of all PBVault vault sizes (2-50).
5.4 Best: Combining the Factors
Finally, we combine the features Policy, Correlation, and
KL divergence to an overall classiﬁer. The results of this
experiment are summarized in Table 3. They show that the
median rank for the real vault in NoCrack is 1.31 %, thus im-
prove the KL divergence attack result (median of 1.97 %).
For Markov, we see the same, with a median ranking re-
sult of 6.54 % compared to the KL divergence attack with a
median of 14.24 %. In Figure 3 we depict the summarized
attack results against the NoCrack NLE for diﬀerent train-
ing sizes, showing analog to Section 4.3 how an increased
training set improves the ranking across all classiﬁers.
5.5 Further Remarks
Besides the already discussed structural diﬀerences of pass-
words, there are some more criteria that might be consid-
ered. First of all, knowing a leaked password from a website
might be a great way to distinguish real from decoy vaults.
Furthermore, as already mentioned by Chatterjee et al. [9]
if a vault is stolen twice, the security falls back to that of a
conventional PBE. The lack of real-world sample data does
not allow experiments on the correlation of master pass-
words and corresponding domain passwords. Furthermore,
the security pledge of NoCrack is somehow counterintuitive,
as using a more unique (secure) self-chosen domain password
facilitates distinguishing. Finally, if a website reports that
an entered password was correct in the past like Facebook
does, it might be a further help for an attacker to ﬁnd the
real vault, even though the user changed some domain pass-
words after the vault has been stolen.
6. ADAPTIVE NLES BASED ON MARKOV
MODELS
Next, we describe a (static) NLE based on Markov mod-
els which has better properties than the PCFG-based NLE
from NoCrack. Then we show how to turn this NLE into
an adaptive NLE, and how this can improve the resistance
against ranking attacks.
6.1 Static NLEs Based on Markov Models
Markov models are tools to model stochastic processes,
widely used for natural language processing like automatic
speech recognition. Recently, they have established them-
selves as an important tool for password guessing [23, 11,
30, 21] as well as measuring password strength [7]. In fact,
an NLE based on Markov models was brieﬂy tested by the
authors of NoCrack [9] but dismissed in favor of a PCFG-
based NLE.
Markov Models.
In an n-gram Markov model one models the probability of
the next token in a string based on a preﬁx of length n − 1.
Hence, for a given sequence of tokens c1, . . . , cm, an n-gram
Markov model estimates its probability as
P (c1, . . . , cm)
= P (c1, . . . , cn−1) · m(cid:89)
(1)
P (ci|ci−n+1, . . . , ci−1).
i=n
We use 4-grams, which oﬀer a good trade-oﬀ between mem-
ory consumption, speed, and accuracy [11, 21], and we use
the full set of printable ASCII characters (95 characters).
The required initial probabilities (IP) P (c1, . . . , c3) and tran-
sition probabilities (TP) P (c4|c1, . . . , c3) are estimated as
the relative frequencies from training data, where we use
the RockYou dataset. We apply simple additive smoothing
to account for unseen n-grams. We train individual Markov
models for each length in the range of 4 to 12 characters.
Encoding of a Password pwd.
The encoding is a (probabilistic) mapping from the set of
passwords to bitstrings. To compute this encoding we ﬁx an
ordering of n-grams (e. g., the alphabetic ordering). For each
transition probability, i. e., for each preﬁx of length 3, the
ﬁxed order gives us a partition of the interval [0, 1) into seg-
ments whose lengths just correspond to the transition proba-
bilities. For a given transition in pwd, we determine the cor-
responding segment [a, b) (where b − a = P (x4|x1, . . . , x3)).
From this segment, we sample a uniformly chosen value s.
This process is repeated for all transitions in the string
pwd, and likewise for the initial probability and for the
length of the password. Finally, this process yields a vector
(cid:126)S = (s1, . . . , slen(pwd)−1) of len(pwd) − 1 values. The vec-
tor (cid:126)S can be encoded into a binary string using techniques
similar to previous work [9].
Decoding of a Vector (cid:126)S.
The (deterministic) decoding of a vector (cid:126)S is straightfor-
ward. The ﬁrst value s1 determines a length l for the pass-
word, after deciding in which segment it falls. In addition,
this tells us which Markov model to use. The value s2 de-
termines the ﬁrst 3-gram of pwd, and the subsequent values
s3, . . . , sl−1 determine the remaining transition n-grams.
Handling Vaults.
To simulate password reuse in a way similar to NoCrack’s
SG model, we generate vaults with related passwords. We
determine the desired level of password reuse from the vaults
in the PBVault set. We measured both exact reuse as well as
reuse of similar passwords with a small Levenshtein distance
(cf. [2]). The measured reuse rates are (48.52, 9.81, 4.17,
2.74, 2.08, 2.72) for Levenshtein distances of 0 to 5, respec-
tively. Constructing vaults to match a given vector of reuse
is not trivial, as there is a high level of interaction between
the similar passwords. We construct vaults by selecting a
“base password” and using that for a fraction of M0 of pass-
words in the vault (exact reuse). Furthermore, we add a frac-
tion of M1, . . . , M5 of passwords with a Levenshtein distance
of 1, . . . , 5 to the base password, respectively. The remainder
of the passwords is ﬁlled up with unrelated passwords. All
these passwords relate to each other, so the actual fraction
of passwords with an edit distance of 1 will in general higher
than M0. We empirically determined values (cid:126)M = (Mi) such
that the reuse rates match the empirical results given above.
We used values (cid:126)M = (0.66, 0.06, 0.02, 0.01, 0.015), adding
Gaussian noise with σ2 of (0.06, 0.034, 0.008, 0.004, 0.012),
respectively.
The related passwords are determined by modifying the
last transition probability, which models most of the mod-
iﬁed reuse found in practice [10]. More sophisticated ap-
proaches can be tested for real-world implementations. For
example, by considering more than the last n-gram position
and more precisely simulating user behavior [32, 33].
Table 4: Rank results based on a KL divergence attack of
entire vaults, where smaller numbers mean a more eﬃcient
attack. Decoy vaults are chosen from the static Markov or
adaptive Markov distribution; real vaults are chosen from
the PBVault distribution. For better comparability to pre-
vious work [9] we list results for varying classes of vault sizes.
Table 5: Ranking results for the re-created ML classiﬁer,
for NoCrack and static Markov, both with MPW and SG.
To facilitate the comparison between the original SVM and
our re-implementation, we list the results for NoCrack by
Chatterjee et al. [9] as well.
KL Divergence Attack
Feature
[9]
NoCrack
S. Markov
[9]
NoCrack
S. Markov
Kind
ML Single (MPW)
ML Vaults (SG)
PBVault
Static Markov
Adaptive Markov
Vault Size
Mean
Q0.25 Median
Mean
Q0.25 Median
2-3
4-8
9-50
31.50 % 0.45 % 16.83 % 42.21 % 12.66 % 33.01 %
9.71 % 39.55 % 11.36 % 32.32 %
26.88 % 0.17 %
24.81 % 1.21 % 12.49 % 38.63 %
4.46 % 36.83 %
All
27.77 % 0.39 % 14.24 % 40.12 % 9.12 % 35.14 %
6.2 Baseline Performance
First, we determine how well this static NLE performs.
Therefore, we ﬁrst rerun the experiments based on KL di-
vergence that were presented in Sections 4.3 and 4.4.
6.2.1 Kullback–Leibler Divergence Attack
We ﬁrst describe the results for entire vaults. The setup is
similar to the setup described in Section 4.3, i. e., we choose
real vaults from the PBVault list, and decoy vaults according
to the distribution generated by the Markov model.
For determining the reference distribution, we slightly de-
viate from the previous approach of sampling the distribu-
tion empirically. There are two reasons for this. First, for
Markov models, it is easy to extract an explicit description of
the probability distribution from the code, namely by copy-
ing the IP and TP tables. This information is more accurate
than an approximation based on sampling, and thus prefer-
able. Second, it turned out that the probability distribu-
tion generated by Markov models is much more “spread out”
than the distribution generated by NoCrack, which is more
concentrated on fewer values. (For illustration we sampled
1.5 M passwords for both distributions. We obtained 250 k
unique passwords for NoCrack and 1.25 M unique passwords
for Markov.)
The results are shown in Table 4. Comparable results for
NoCrack can be found in Table 1. We see that the Markov
NLE is substantially more robust against this attack, with
an average rank of 27.8 % (NoCrack: 6.2 %) and median
14.2 % (NoCrack: 2.0 %). Interestingly, for the weak vaults,
there is no big diﬀerence: Q0.25 is 0.4 and 1.0, respectively.
The results for artiﬁcial vaults (independently chosen, size
10) can be found in Table 2. Here we see that Markov per-
forms similar to NoCrack, with a median of 0.1 %, and only
slightly better for the mean. The only exception is in the
comparison with RockYou, for which it performs relatively
close to random. For the other lists, the median is 0.1 %
each, equal to NoCrack, only the median being slightly bet-
ter with values between 5.3 % and 11.9 %.
6.2.2 Machine Learning Attacks
We also re-created the original attack based on machine
learning, in order to check how well the Markov NLE fares
against it. The NoCrack paper gives limited details only. In
the full version [9] of the paper, they report their best per-
forming ML engine was a Support Vector Machine (SVM)
with a radial basis function kernel. They constructed four
SVM-based classiﬁers, one for each of the following feature
Repeat ct.
Edit dist.
n-gram
Combined
0.60 %
1.20 %
0.60 %
1.00 %
4.71 %
1.71 %
2.83 %
0.54 %
3.43 %
1.28 %
2.13 %
0.40 %
37.40 % 38.45 %
41.40 % 35.52 %
38.50 % 38.90 %
39.70 % 37.80 %
45.30 %
35.10 %
32.72 %
40.91 %
vectors: Repeat count (including numbers for the uniqueness
of passwords, leetspeak transformations, capitalization, and
tokens within a vault); Edit distance (including numbers for