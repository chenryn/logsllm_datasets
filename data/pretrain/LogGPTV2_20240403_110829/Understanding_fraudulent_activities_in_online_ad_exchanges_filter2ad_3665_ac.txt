cookie value.
• Creative ID: Unique ID given to each creative to track which
speciﬁc ad was shown.
• Section ID: Unique ID for the particular ad space where the ad
was shown. For sold arbitrage trafﬁc, this corresponds to a section
ID that NETWORKX owns, and for local trafﬁc, it corresponds to a
section ID that a local publisher owns. This ﬁeld is not populated
for auction trafﬁc or purchased arbitrage trafﬁc.
• Referrer: Website URL of the referring website for local pub-
lisher trafﬁc, or a subdomain of NETWORKX for sold arbitrage
trafﬁc. Not populated for auction or purchased arbitrage trafﬁc.
• Impression/Click/Conversion: Whether it was an impression, a
click, or conversion.
• Advertiser Cost/Publisher Revenue: How much the publisher got
paid for the ad and how much the advertiser paid out. The dif-
ference between the advertiser cost and publisher revenue is how
much the intermediary ad networks earned for the ad.
• Buyer ID: Unique ID given to each advertiser, or a network-
owned buyer ID for bought arbitrage trafﬁc. For auction trafﬁc,
this ﬁeld is populated with the ID of a trusted partner network.
• Seller ID: Unique ID given to each publisher, or a network-
owned seller ID for sold arbitrage trafﬁc. For auction trafﬁc, this
ﬁeld is populated with the ID of a trusted partner network and not
a publisher.
• User Agent ID: Identiﬁes the user-agent ﬁeld that was speciﬁed
in the HTTP headers. RightMedia currently enumerates 40 differ-
ent types of browsers and versions.
• Region ID: Identiﬁes local geographical areas (state, province,
or city) via IP-address-based geo-location services.
Because we were working closely with NETWORKX, we had
access to advertiser and publisher accounts that the network owned
and managed, which allowed us to glean additional information
from the data feed for local trafﬁc. In particular, we could see the
domain(s) that publishers used to sign up, how long they have been
active, and their trafﬁc statistics. This allowed us to compare our
results with RightMedia’s software (which was running on NET-
WORKX’s server). With auction and arbitrage trafﬁc, we were not
able to relate IDs to real publisher accounts in RightMedia, which
limited our analysis on this trafﬁc. Therefore, most of our analysis
focused on local trafﬁc.
We investigated in depth ten days worth of data collected from
NETWORKX. More precisely, we analyzed the data collected be-
tween April 20 and April 30, 2011, which was 513,644,248 total
impressions across all the data ﬂows. Overall statistics for each
trafﬁc ﬂow are outlined in Table 1. We chose to perform our anal-
ysis on the local publisher trafﬁc, because it contained the most
information for each impression. Out of NETWORKX’s 1,600 pub-
lishers, only about 300 are active and generating ad requests. Of
these 300, the most popular 1% are responsible for roughly 40%
of the local trafﬁc, and the top 10% are responsible for 92% of the
local trafﬁc. We observed the impressions, clicks, and conversions,
284Trafﬁc
Flow
Auction
Publishers
Arbitrage
Trafﬁc
(%)
37.7%
2.0%
60.3%
Impressions CTR Conversion
Rate
-
0.01%
0.007%
(per hour)
305,318
15,794
489,184
0.16%
0.56%
0.12%
Table 1: Statistics for each trafﬁc ﬂow.
and we measured a click-through-rate (CTR) of 0.56%, with con-
versions being 2.22%. In addition, we measured an average CPM
of $0.084, an average cost-per-click (CPC) of $0.017, and an aver-
age cost-per-action (CPA) of $0.055. We found that there are about
1.5 unique cookie IDs per IP address in an hour, which are respon-
sible for 2.4 impressions.
3.2 Establishing Ground Truth
To determine how well analysis and detection models can iden-
tify fraudulent behavior, we require ground truth about known bad
and known good publishers. NETWORKX’s trafﬁc was coming
from relatively few publishers, and the remaining publishers had
so little trafﬁc that they could not cause noticeable harm to NET-
WORKX’s advertisers. Therefore, we only had to analyze the top
100 publishers. We chose to manually analyze each of the publish-
ers by visiting the referrer URLs of their ad requests. Since it is not
trivial to determine what sites are good and bad, we used the fol-
lowing heuristics (many of these were established from observing
the fake fraud websites outlined in Section 2.9):
• The site does not serve content, or the content that it serves
does not render.
• The site contains entirely ad content or signiﬁcantly more ad
content than actual user content.
• The site is hosting illegal content or content against the terms
of use of RightMedia.
• The content on the site is stolen or contains leftovers from an
HTML template.
The following heuristics were used for establishing which sites
were not perpetrating fraud:
• The site is well-designed, good looking, and usable.
• The site has a very good Alexa ranking, especially compared
to their amount of NETWORKX trafﬁc.
• The site is full of legitimate content that has user support,
such as comments or “likes” from various social networks.
For a site to be ﬂagged as either good or bad, it had to conform to
more than one of the above heuristics in one category and none of
the heuristics in the other. Of the 100 publishers analyzed, 11 were
picked out as owning sites that likely perpetrated fraud and 20 were
picked out as likely not participating in fraud. The ﬁnalized ground
truth list was brought to the attention of the development team and
the CEO of NETWORKX, who veriﬁed our work. They also ter-
minated the malicious publishers’ accounts as a result of what they
found. None of the publishers whose accounts were terminated
attempted to contact NETWORKX for outstanding payments or ac-
count renewal.
4.
IDENTIFYING SUSPICIOUS TRAFFIC
In this section, we present techniques that could be used for de-
tecting fraudulent ad trafﬁc.
In particular, we discuss two main
approaches: First, we introduce a detector based on checking the
referring web page for signs that indicate fraud. Then, we discuss
techniques based on statistical properties of various ﬁelds in ad re-
quests.
4.1 Reverse Spidering
To detect the malware described in Section 2.8, we developed
a reverse auditing system that crawled the referrers of each im-
pression in our data feed. We deployed 50 virtual machines that
utilized the SELENIUM [1] software-testing framework for web ap-
plications, which has the ability to programmatically control a real
Web browser. The primary beneﬁt that Selenium provided was the
ability to interpret JavaScript code. For each audit, we collected the
raw trafﬁc, and then we extracted the section ID ﬁelds on the web
page that is responsible for generating revenue for the publisher.
Next, we compared the section ID values from NETWORKX’s data
feed with the observed section ID values found on that referrer’s
site. Our intuition was that if the referring page did not contain
the section ID that initiated the request, then the request must have
been hijacked.
Unfortunately, this detection method was not effective in the
real world. After several months (August 2010 - April 2011) of
running our reverse spidering system, we noticed that 79.2% of
the referred pages in NETWORKX’s data feed contained no sec-
tion IDs when examined. Clearly, this high percentage of trafﬁc
could not be fraudulent, so we manually analyzed some of the sites
that did not contain section IDs. We found that many sites used
IP-address-based geo-location to serve speciﬁc ads to particular re-
gions. In addition, some of the sites used ad delivery optimization
services, such as Casale Media, that dynamically choose different
ad exchanges to maximize conversions. In other words, a visitor
may have received a RightMedia ad based on his location, but our
crawlers, which are based in the U.S., were delivered ads from
DoubleClick’s ad exchange. Even if we had found RightMedia ad
tags with different section IDs than what we would expect, this
would not indicate fraud, as one publisher may have many legiti-
mate RightMedia accounts. As a result, we conclude that reverse
spidering for the purpose of fraud detection suffers from signiﬁcant
limitations.
4.2 Modeling Ad Requests
We introduce a number of features that model properties of ad
trafﬁc. Similar to traditional intrusion detection systems, we use
these features to establish models of normal, expected trafﬁc. When
certain requests or sets of requests violate a previously-built model,
we consider these requests to be suspicious. We use the fraction of
suspicious trafﬁc per publisher to detect fraud. More speciﬁcally,
when a certain publisher produces a high percentage of requests
that are suspicious, this publisher is more likely to be involved in
malicious activity.
In the next paragraphs, we discuss our approach for building
models. Then, we describe the effectiveness of individual features.
4.2.1 Building Models
Features. We considered a number of simple features for detecting
anomalous ad trafﬁc:
• Impressions Per Cookie: The number of impressions each cookie
generated. Our motivation was that, since each cookie ID rep-
285resents a unique browser instance, any cookies generating a very
large amount of trafﬁc would be fraudulent.
• CTR Per Cookie: The click-through-rate for each cookie. On
average, CTRs are rarely above 2%, so any cookies generating very
high click-through-rates would be suspicious.
• Publisher Revenue Per Cookie: How much revenue each cookie
generated for a publisher. Our motivation was that for fraud to be
effective, fraudulent trafﬁc would have to be generating revenue in
addition to just impressions.
• Unique IP Addresses Per Cookie: The number of unique IP ad-
dresses a cookie is generating requests from. Since cookies are
assigned per browser instance, we would not expect one cookie ID
to come from many 24 IP subnets within a short period of time.
• Impressions Per IP Address: The number of impressions that
each IP address generates. Our motivation for this feature is that a
naive clickbot might remain at one static IP address and generate
many requests.
• CTR Per IP Address: The CTR of each IP address. Most users
do not click on an ad more than 2% of the time, thus, a very high
CTR from a single IP address indicates fraud.
• Publisher Revenue Per IP Address: How much revenue each IP
address generated for a publisher. Unusually high values are suspi-
cious.
• Deviation of CTR Per IP Address: This feature is slightly differ-
ent than the others because it cannot be computed for trafﬁc in a
single time interval. Instead, we compute the standard deviation of
the CTR values for a number of consecutive time slots. Malware
that commits click fraud often exhibits characteristic behavior in
that its click through rates over time are very consistent, even when
the number of impressions generated changes. Thus, a low feature
value is suspicious.
A number of traditional detection features could not be applied
to our dataset due to lack of data in certain ﬁelds, especially the
IP address ﬁeld. For example, because we only had the ﬁrst three
octets of the IP address, we could not detect sites that had an unusu-
ally high amount of trafﬁc from IP addresses that belong to known
Internet proxy services, a behavior that would indicate the presence
of static IP address clickbots.
Thresholds. We use a dynamic threshold to determine which fea-
tures are considered suspicious. Each feature threshold is applied
to the feature value computed for a time window of one hour. After
that, the threshold is recomputed, based on historical (previously-
seen) data. More precisely, a threshold is determined by ﬁrst com-
puting the mean and variance for a particular feature over all pre-
vious time windows. Then, we set the threshold equal to the mean
plus N standard deviations. The value of N can be used to tune the
sensitivity for each “detector.” We empirically determined good
values for N based on a subset of the trafﬁc dataset. The concrete
values are shown in Table 2.
The threshold for the CTR standard deviation feature is com-
puted slightly differently. In particular, suspicious values for the
standard deviation are not those values that exceed a normal base-
line. Instead, a small standard deviation is suspicious, since it in-
dicates a high regularity typically associated with automated (bot)
trafﬁc. We empirically found that a value of 0.02 (2%) produces
good results.
Anomaly Detection
Algorithm
Impressions Per Cookie
CTR Per Cookie
Publisher Revenue Per Cookie
Unique IP Addresses Per Cookie
Impressions Per IP Address
CTR Per IP Address
Publisher Revenue Per Cookie
Threshold
(Standard Deviation)
3
3
2
2
4
4
3
Table 2: Detector thresholds.
Classifying publishers. For each time window of analysis (one
hour), the detectors produce those cookies and IP addresses that
violate at least one of the feature thresholds. Using this informa-
tion, we identify all requests (impressions) that originate from a
suspicious IP or that contain a suspicious cookie. In the next step,
we determine the publishers that are associated with these requests
(based on the publisher ID in the request). Using this, we can de-
termine the number of suspicious and the number of total requests
for each publisher and each time window.
Using the number of suspicious and total requests, we can easily
compute a publisher’s fraction of requests that are suspicious (both
for individual time intervals and for an entire observation period).
We consider this fraction as an anomaly score for the publisher. The
anomaly score can be compared to an anomaly threshold; when
the threshold is exceeded, we consider the publisher to perform
fraudulent activity in the time period under analysis.
4.2.2 Evaluating Models
We wanted to explore the ability of different detectors to iden-
tify fraudulent trafﬁc. To this end, we applied the models described
in the previous section to our data set (ten days worth of trafﬁc,
as described in Section 3.1). We then computed, for each model,
an anomaly score for every publisher. Finally, we compared these
anomaly scores to varying thresholds (ranging from 0 to 1, in in-
crements of 0.001). This yielded different sets of publishers that
would be classiﬁed as suspicious or legitimate.
In the next step, we leveraged our ground truth (see Section 3.2)
of good and bad publishers. More precisely, for each model and
each threshold value, we could determine the fraction of known,
malicious publishers that the models would have correctly identi-
ﬁed. Also, we could see the fraction of honest publishers that were
incorrectly attributed as suspicious.
Discussion. In this paragraph, we discuss the performance of
the individual detection features. Overall, we found that no single
model would work as a reliable detector. For example, the best
detection features with good threshold settings yielded detection
rates between 60% and 80%. However, with these settings, a detec-
tor would also incorrectly blame 10% of the legitimate publishers.
This indicates that there is a signiﬁcant variety in the ad trafﬁc, and
simple, statistical measures are not sufﬁcient to precisely distin-
guish between malicious and legitimate players. Nevertheless, the
simple models are useful to guide a human analyst (employed by
ad networks) to focus on those publishers that produce the largest