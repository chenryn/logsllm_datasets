title:BackStreamDB: A Distributed System for Backbone Traffic Monitoring
Providing Arbitrary Measurements in Real-Time
author:Christiano Lyra and
Carmem S. Hara and
Elias P. Duarte Jr.
BackStreamDB: A Distributed System
for Backbone Traﬃc Monitoring Providing
Arbitrary Measurements in Real-Time
Christian Lyra1, Carmem S. Hara2, and Elias P. Duarte Jr.2
1 Brazilian Research Network (RNP) – Point of Presence at Parana State
P.O.Box 19037 81531-990 Curitiba-PR, Brazil
2 Department of Informatics – Federal University of Parana
P.O.Box 19081 81531-990 Curitiba-PR, Brazil
PI:EMAIL, {carmem,elias}@inf.ufpr.br
Abstract. Monitoring the traﬃc of wide area networks consisting of
several autonomous systems connected through a high-speed backbone is
a challenge due to the huge amount of traﬃc. Keeping logs for obtaining
measurements is unfeasible. This work describes a distributed real-time
strategy for backbone traﬃc monitoring that does not employ logs and
allows arbitrary metrics to be collected about the traﬃc of the backbone
as a whole. Traﬃc is sampled by monitors that are distributed across the
backbone and are accessed by a Stream Processing Engine (SPE). Besides
the distributed monitoring architecture, we present an implementation
(BackStreamDB) that was deployed on a national backbone. Case studies
are described that show the system ﬂexibility. Experiments are reported
in which we evaluated the amount of traﬃc that can be handled.
1
Introduction
Consider a wide area network, consisting of several Autonomous Systems (AS)
connected through a high-speed backbone. Obtaining information about the net-
work as a whole and about individual components, in particular traﬃc informa-
tion, is the ﬁrst step for most of network management tasks. Traﬃc information
is important for evaluating the performance, monitoring the security and for gen-
erating proﬁles that can be used by accounting systems. Although a combination
of polling and alarms oﬀered by traditional network management protocols can
be eﬀectively used in certain limited settings, it does not scale well, and cannot
be used for monitoring traﬃc in large backbones.
One way to gather information about network traﬃc is by using a sniﬀer. Snif-
fers are able to store and decode all network traﬃc they see. Although this may
allow the extraction of any type of traﬃc information, in order to be deployed
at a wide area backbone sniﬀers should obtain the complete network traﬃc, and
the amount of data to be processed can be overwhelming.
To solve this problem, network vendors have been developing for several years
products that are eﬀective in gathering information on network traﬃc. Protocols
like Netﬂow1 and Sﬂow [13] obtain information at the packet ﬂow level. A “ﬂow”
1
http://www.cisco.com/web/go/netflow
N. Taft and F. Ricciato (Eds.): PAM 2012, LNCS 7192, pp. 42–52, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012
BackStreamDB: A Distributed System
43
is deﬁned as a set of packets of a given protocol ﬂowing between two endpoints,
each consisting of an IP address and transport-layer port. After the ﬂow infor-
mation is obtained, it is stored in secondary memory, and tools like ﬂow-tools2
and ntop3 can be employed for obtaining information about the traﬃc. This
approach for network monitoring has several drawbacks. First, the majority of
existing tools require ﬂow data to be stored in order to be analyzed. Depend-
ing on the volume of traﬃc, this would demand considerable storage space for
keeping logs, and possibly a tool for managing the log size, such as RRD4. In
addition, since data ﬂows are not processed at the time they are processed by
routers, ﬂow measurements are not provided in real-time. This may have an im-
pact on important network administration tasks, such as the detection of traﬃc
anomalies generated by port sweeps or port scans. Second, both ntop and ﬂow-
tools are based on a limited set of predeﬁned metrics. Although they are meant
to cover a large number of network administration needs, it is quite common for
network administrators to develop scripts in order to obtain additional monitor-
ing information. However, this approach does not exempt ﬂow traﬃc from being
stored, and does not provide results in real-time.
In this paper we propose a distributed architecture for integrating Stream
Processing Engines (SPE) and ﬂow monitoring tools within a framework de-
veloped for large-scale backbones consisting of multiple autonomous systems.
SPEs [10] were developed to provide the same basic features found in traditional
Database Management Systems, but operations are executed in real time on con-
tinuous data streams. In our case the data stream is the network traﬃc itself.
We have implemented a system, called BackStreamDB, based on the proposed
architecture. In this system, metrics of interest to the network administrator
are expressed in a high level SQL-like language. This allows arbitrary queries to
be issued about the traﬃc that is ﬂowing across the whole backbone, in which
ﬂow data sources may be geographically distributed. Query results are provided
in real time, and can refer to either a particular segment, or to a set of (or
all) autonomous systems of the backbone. This approach of deﬁning metrics as
queries allows modiﬁcations and improvements to be easily made on the ﬂy by
just executing a diﬀerent query.
In addition, one of the most signiﬁcant contributions of the system is that arbi-
trary measures can be obtained without storing any traﬃc logs. As the amount
of traﬃc in these systems is huge, existing passive monitoring strategies that
rely on logs [4,8] are hardly able to oﬀer similar functionality. Another key ad-
vantage of the proposed strategy is extensibility. In its current implementation,
BackStreamDB monitors a backbone using Netﬂow records as source data. It
extends our previous work [11], in which we propose a system to monitor local
segments at the packet level. Thus, the system can provide a general framework
for monitoring a network, in which the administrator issues queries to gather
measurements from both local and wide area networks using the same language.
2
3
4
http://www.splintered.net/sw/flow-tools/
http://www.ntop.org
http://ee-sta~ethz.ch/~oetiker/webtools/rrdtool/
44
C. Lyra, C.S. Hara, and E.P. Duarte Jr.
BackStreamDB has been tested on the Brazilian RNP backbone5, showing the
feasibility of the proposed approach. Case studies are described that show the
system ﬂexibility. Experiments are reported in which we evaluated the amount
of traﬃc that can be handled. The main contributions of this paper are:
– An architecture for distributed traﬃc monitoring based on a Stream Process-
ing Engine and ﬂow processing protocols, integrated to a multi-AS backbone
traﬃc monitoring framework;
– Development of a distributed traﬃc monitoring system based on the pro-
posed architecture. It allows arbitrary queries about the traﬃc on a wide
area backbone to be processed in real time;
– Experimental deployment of the system on the Brazilian national RNP back-
bone and analysis of results on real datasets.
The rest of this paper is organized as follows. Section 2 presents an architecture
for traﬃc monitoring based on SPEs, and BackStreamDB, the system built based
on the proposed architecture. Section 3 presents experimental results and Section
4 describes related work. The conclusion follows in Section 5.
2 An Architecture for Backbone Traﬃc Monitoring
The distributed monitoring architecture we propose allows a network adminis-
trator to issue arbitrary queries to obtain network traﬃc information from a
multi-AS backbone. Diﬀerent granularity are permitted as monitored objects
may range from individual segments to the backbone as a whole. Data is ob-
tained from multiple ﬂow data sources that are geographically distributed across
the network, and traﬃc information is obtained and processed in a distributed
fashion in real time. This strategy is scalable, as it is possible to accommodate in-
creasingly larger traﬃc loads by changing the system conﬁguration to distribute
data to other existing nodes in the network.
The architecture is shown in Figure 1. SPE (Stream Processing Engine) nodes
are deployed for processing queries, and the system has three other main com-
ponents: acquisition modules, universal receiver (ureceiver), and global catalog,
that are described below. Acquisition modules are in charge of receiving ﬂow
data and of converting them to SPE conformant format. The data is then sent
to one or more SPE nodes for query processing. An SPE node can either process
the entire query or forward partial results to be processed by another SPE node.
The ﬁnal query results are sent to ureceiver. The acquisition modules are thus
responsible for the interface between data sources and SPEs. A ureceiver (uni-
versal receiver) is responsible for the interface between SPEs and visualization
tools. Query results can also be stored for historical purposes. In short, ureceiver
is responsible for consuming data produced by SPEs and forwarding them to
appropriate applications. Information about queries that are being processed by
SPEs are stored on the third component of the system: the global catalog. For
each query the catalog maintains the query deﬁnition and information specifying
the SPE nodes which are executing the query.
5
http://www.rnp.br
BackStreamDB: A Distributed System
45
Fig. 1. Architecture for traﬃc monitoring based on SPE and ﬂow protocol
There is a large spectrum of possible system conﬁgurations, ranging from a
fully distributed system in which each module is assigned to a distinct node, to
a centralized system, in which a single node runs all modules. Ideally, when data
sources are geographically distributed, both an acquisition module and an SPE
node should be deployed close to the source. In this way, source data can be
locally ﬁltered by the SPE node, reducing the volume of data to be transmitted
among SPE nodes and the ureceiver.
Query results can be either accessed in real time by a network administrator
with visualization tools, or can also be stored if required. Since the system does
not log ﬂow data, but only query results that have been individually speciﬁed
by the administrator, BackStreamDB can drastically reduce the storage cost.
Query deﬁnitions are fed to the system by a Query Register Tool in a high-level
query language, which makes queries easy to maintain.
2.1 BackStreamDB
A distributed monitoring system called BackStreamDB was implemented based
on the proposed architecture. Borealis [1] was chosen as the SPE. The main
reason for this choice is that its distributed nature enables a set of SPE nodes
to be deployed across the network, and in particular, close to data sources.
The Borealis component called BigGiantHead is used to deploy a query on SPE
nodes. The BigGiantHead can be executed in either transient or “daemon” mode.
In transient mode, the application is invoked only to send control data to SPEs,
and then quits. Control data include assignment of query tasks to SPEs, and data
ﬂow information. In “daemon” mode it continuously listens for query invocation
requests and sends the control data accordingly to SPEs.
Currently, BackStreamDB processes Netﬂow data. In the current system, the
acquisition module obtains data using the New Netﬂow Collector (NNFC)6.
NNFC is a tool for capturing and storing Netﬂow data sent by a router. An
NNFC plugin was developed to allow communication using Internet Process
Communication (IPC) with flowsender, an application we developed for trans-
lating and forwarding data to Borealis SPEs.
BackStreamDB can be easily extended to support other formats besides Net-
ﬂow. A new input format can be conﬁgured in a way that is similar to deﬁning a
new schema for databases, or wrappers to exchange data between applications.
6
http://sourceforge.net/projects/nnfc
46
C. Lyra, C.S. Hara, and E.P. Duarte Jr.
(a)
(b)
Fig. 2. (a)Query that generates a traﬃc matrix. (b)Anomaly detection query.
This process involves: speciﬁcation of the Borealis input format, and develop-
ment of an application for translating data collected from the source to the new
format in the acquisition module.
Query results produced by SPE nodes are sent to a ureceiver. In the standard
Borealis distribution it is necessary to develop a new receiver application for
each distinct query result format. This is because Borealis outputs results in
binary format, and the receiver is responsible for decoding these values into
typed output ﬁelds. We have changed this approach by coding the ureceiver
with the capability to infer the output format based on the query deﬁnition.
As a result, the system does not have to be recompiled when new query results
are deﬁned, as in the standard Borealis distribution. When invoked, ureceiver
waits for a connection from a Borealis SPE, and when new query results arrive,
they are output either in text or graphical form by a visualization tool.
For the query language, we employ the same language adopted by Borealis, in
which queries are expressed in an XML document, containing input, output, and
query deﬁnitions. BackStreamDB’s query register tool reads the XML document,
stores the information in the global catalog, and communicates with BigGiant-
Head through the network in order to deploy the execution of queries on diﬀerent
nodes. For showing the generality of the proposed architecture and system, we
next present how one can easily build a traﬃc matrix and detect traﬃc anomaly
using BackStreamDB.
Traﬃc Matrix. A traﬃc matrix shows the amount of traﬃc transmitted among
all possible pairs of nodes of a given network. It provides useful information for
deﬁning routing policies and for taking traﬃc engineering decisions. Building a
traﬃc matrix with BackStreamDB is simply a matter of conﬁguring the system
to aggregate the records by source and destination AS while summing up the
octets, as depicted in Figure 2(a). Here, we use a graphical representation of a
query, in which each box executes an operation supported by the query language.
First, the query computes the sum of octets in Netﬂow records with the same
source and destination autonomous systems, src AS and dst AS, respectively,
generating a result every 300 seconds. Then, a union operator is applied to
combine the results in a single output stream.
Traﬃc Anomaly Detection. It is possible to use BackStreamDB to detect
traﬃc anomalies such as port sweeps or port scans. A very common type of
network probing consists of sending packets to all hosts on a given network
in order to ﬁnd the active and working ones. This probe can be characterized
by packet ﬂows consisting of one single packet; that is, usually the probing
BackStreamDB: A Distributed System
47
host sends just one ICMP or UDP packet. A query for detecting this kind of
anomaly is shown in Figure 2(b). First, ﬂows are ﬁltered, selecting only those
that consist of one single packet (pkt == 1). These ﬂows are then grouped by
source (src ip) and destination IPs (dst ip). In order to count the number of
distinct destinations of packet ﬂows sent from a single source, we regroup them
considering only the source IP. The result is the number of one packet ﬂows sent
by distinct source addresses. This result can then be ﬁltered considering a given
threshold. In our example, only source addresses issuing at least 500 probes are
considered suspicious, and are reported in the query result.
These two examples show the expressive power of the system’s query language,
and the ﬂexibility of the proposed system, which has several applications. In the
next section, we present experimental results that show that it is feasible to
deploy BackStreamDB for monitoring the traﬃc of a wide area network.
3 Experimental Evaluation
In order to validate BackStreamDB three experiments were executed and are
described below. These experiments show the amount of traﬃc the system is
able to handle in terms of Netﬂow records.
Experiment 1: Single Node, Synthetic Traﬃc. The ﬁrst experiment in-
volved a single node processing synthetic traﬃc, which we fully controlled in
order to check whether results generated by BackStreamDB were as expected.