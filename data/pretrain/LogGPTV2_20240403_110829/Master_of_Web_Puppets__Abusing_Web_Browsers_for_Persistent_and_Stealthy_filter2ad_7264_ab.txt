The current design of modern browsers’ extensions allows
two types of JavaScript scripts within a browser extension:
(a) content scripts and (b) background scripts. Content scripts
run in the context of the websites visited by the user, thus
they can read and modify the content of these websites using
the standard DOM API, similarly to the websites’ scripts (i.e.,
those JavaScript scripts that were included in the website by
the publisher). Furthermore, content scripts can access directly
a small subset of the WebExtension JavaScript APIs.
On the other hand, background scripts run as long as the
browser is open (and the extension is enabled), and typically
implement functionalities independent from the lifetime of any
particular website or browser window, and maintain a long-
term state. These background scripts cannot access directly
the content of the websites visited by the user. However,
background scripts can access all the WebExtension JavaScript
APIs (or chrome.* APIs for Google Chrome), if the user’s
permission is granted during the installation of the extension.
Indicatively, the large set of WebExtension JavaScript APIs
contains the bookmarks, cookies, history and storage APIs,
which allow access on various types of user data, the tabs
and windows APIs, browserSettings and the webRequest API
among many others. However, even though content scripts
cannot access all WebExtension APIs directly, and background
scripts cannot access the content of the visited website, this
can be achieved indirectly since the content and background
scripts of an extension can communicate with each other.
In addition to the above mentioned APIs, Google Chrome
also supports some HTML5 and other emerging APIs for its
extensions (e.g., application cache, local storage, geolocation,
canvas, notiﬁcations, etc.). However, it is important with regards
to this work to emphasize that none of the browsers allow
extensions to use HTML5 APIs such as the Service Workers
API or the Push API. Consequently, browser extensions cannot
interact with possible deployed service workers in any way
(e.g., modify their code, monitor their outgoing trafﬁc, etc.).
C. Security Analysis
Table I summarizes the characteristics of various APIs of
interest. We categorize them along four axes related to the
efﬁciency of a distributed botnet: (i) the execution model (i.e.,
whether it can run in parallel to the main webpage or in the
background), (ii) if direct network access is possible, (iii) the
ability to use persistent storage, and (iv) the ability to access
the DOM of the webpage.
JavaScript code (running either as part of the webpage, or
in a web worker or service worker) has access to persistent
storage (e.g., using WebStorage), as well as the ability to
communicate with other servers or peers (e.g., using XHR
requests, WebSockets, or WebRTC). However, local JavaScript
code embedded in the webpage also has direct access to the
page’s DOM and therefore, the ability to access or manipulate
any element of the webpage, as well as any network request
or response that is sent or received. Page-resident JavaScript
code cannot be detached from the webpage, neither run
without blocking the rendering process. This results to a major
limitation (for the purposes of malicious scripts), as long-
running operations would affect the user experience. Also,
3
TABLE I: Analysis of HTML5 JavaScript execution methods.
Feature
Concurrent
Execution
Background
Execution
Webpage
Detached
Intercept
HTTP Requests
Persistent
Storage
DOM
Access
Network
Access
Local JavaScript code
Web Worker (Shared)
Web Worker (Dedicated)
Service Worker
















a suspicious code snippet could be detected easily by browser
extensions, since it needs to be embedded in the main website,
and extensions’ JavaScript code can access, inspect and in
general, interfere with the content of the visited website.
Web workers, on the other hand, can perform resource-
intensive operations without affecting the user’s browsing
experience, as they run in separate threads. This allows utilizing
all different available CPU cores of the user’s machine, by
spawning a sufﬁcient number of web workers. Service workers
behave in a similar fashion, but have the important advantage
of being completely detached from the main webpage, running
in the background even after the user has navigated away.
Moreover, service workers can intercept the HTTP requests
sent by the webpage to the back-end web server. Importantly,
since service workers are completely detached from the page’s
window, extensions cannot monitor or interfere with them.
Finally, using the CORS capabilities of HTML5, it is possi-
ble to send multiple GET or POST requests to third-party web-
sites. However, the Access-Control-Allow-Origin:*
header has to be set by the server, in order for the request to
be able to fetch any content. Besides sending HTTP requests,
WebRTC allows the peer-to-peer transfer of arbitrary data,
audio, or video—or any combination thereof. This feature can
open the window for malicious actions such as illegal hosting
and delivery of ﬁles, and anonymous communication through
a network of compromised browsers, as we showcase later on.
III. THREAT MODEL AND OBJECTIVES
The motivation behind this work is to design a system capa-
ble of turning users’ browsers into a multi-purpose “marionette”
controlled by a malicious remote entity. Our goal is to leverage
solely existing HTML5 features in order to highlight the lack
of adequate security controls in modern browsers that would
have prevented the abuse of these advanced features.
A. Threat Model
We assume a website that delivers malicious content to
execute unwanted or malicious background operations in
visitors’ browsers. Once the website is rendered, this malicious
content
is capable of
continuing its operation even after the victim browses away
from the website.
is loaded in a service worker that
Websites can deliver such malicious or unwanted content
intentionally, to gain proﬁt directly (e.g., by attracting visitors
and thus advertisers), or indirectly, by infecting as many
user browsers as possible to carry out distributed (malicious)
computations or mount large-scale attacks. The websites in this
category can range from typically malicious ones, to websites
of shady reputation, and even to trustworthy and reputable
websites that aim to increase their revenue without actually
intending to conduct any illegal activities or harm the user.
There are also several cases where a website can end up
hosting such malicious content unintentionally. Those cases
include: (i) the website registers a benign service worker that
includes untrusted dynamic third-party scripts [35], which in
turn possibly load malicious code; (ii) the website includes third-
party libraries,1 one of which can turn rogue or be compromised,
and then divert the user to a new tab (e.g., using popunders [22]
or clickjacking [64]) where it can register its own service worker
bound to a third-party domain; (iii) the website is compromised
and attackers plant their malicious JavaScript code directly into
the page, thus registering their malicious service worker—a
scenario that we see quite often in recent years [40], [36]; or
(iv) the website includes iframes with dynamic content, which
are typically auctioned at real-time [57] and loaded with content
from third parties.
In the latter case, malicious actors can use a variety of
methods (e.g., redirect scripts [28], [37] or social engineering)
to break out of the iframe and open a new tab on the user’s
browser for registering their own service worker. The important
advantage of this latter approach is that the user does not need
to re-visit the website for the service worker to be activated.
After registration, just an iframe loaded from the malicious
third party is enough to trigger the malicious service worker,
regardless of the visited ﬁrst-party website. This relieves the
attackers from the burden of maintaining websites with content
attractive enough to lure a large number of visitors. Instead,
attackers can activate their bots just by running malvertising
campaigns, purchasing iframes in ad-auctions [67].
To summarize, our threat model considers that such an
attack can be launched intentionally by a malicious or “shady”
website that includes malicious content, unintentionally, by a
hijacked/compromised website or a website that includes a
compromised library, and also by third-party dynamic content
loaded in iframes (typically used for real-time ad auctions).
B. Challenges
The greatest challenge for systems like MarioNet is to keep
the user’s device under control for as long as possible. This is a
challenging task given that a connection with the server may be
possible only for the duration of a website visit; recent studies
have estimated the average duration of a typical website visit
to be less than one minute [53]. In addition, there is a plethora
1Modern websites often include numerous third-party scripts [55], [14] for
analytics or user tracking purposes, aiming to gain insight, improve performance,
or collect user data for targeted advertising.
4
Fig. 1: High level overview of MarioNet. The in-browser component (Servant), embedded in a Service Worker, gets delivered
together with the actual content of a website. After its registration on the user’s browser, it establishes a communication channel
with its remote command and control server (Pupeteer) to receive tasks.
of sophisticated browser extensions [20], [39] that monitor
the incoming and outgoing trafﬁc of in-browser components.
Consequently, another challenge for MarioNet is to evade any
such deployed countermeasure installed in the browser. Finally,
it is apparent that the malicious or unwanted computation of
MarioNet must not impede the normal execution of the browser,
and avoid degrading the user experience. Otherwise, the risk of
being detected by a vigilant user or at least raising suspicion
due to reduced performance is high.
To summarize, in order to overcome the above challenges,
a MarioNet-like system should have the following properties:
1) Isolation: the system’s operation must be independent from
a browsing session’s thread or process. This isolation will
allow a malicious actor to perform more heavyweight
computation without affecting the main functionality of
the browser.
2) Persistence: the operation must be completely detached
from any ephemeral browsing session, so that the browser
can remain under the attacker’s control for a period longer
than a short website visit.
3) Evasiveness: operations must be performed in a stealthy
way in order to remain undetected and keep the browser
infected as long as possible.
IV. SYSTEM OVERVIEW
In this section, we describe the design and implementation
of MarioNet, a multi-purpose web browser abuse infrastructure,
and present in detail how we address the challenges outlined
earlier. Upon installation, MarioNet allows a malicious actor
to abuse computational power from users’ systems through
their browsers, and perform a variety of unwanted or malicious
activities. By maintaining an open connection with the infected
browser, the malicious actor can change the abuse model at any
5
time, instructing for instance an unsuspecting user’s browser
to switch from illicit ﬁle hosting to distributed web-based
cryptocurrency mining.
Our system, which is OS agnostic, assumes no assistance
from the user (e.g., there is no need to install any browser
extension). On the contrary, it assumes a “hostile” environment
with possibly more than one deployed anti-malware browser
extensions and anti-mining countermeasures. We also assume
that MarioNet targets off-the-shelf web browsers. Hence, the
execution environment of MarioNet is the JavaScript engine of
the user’s web browser. Breaking out of the JIT engine [5] is
beyond the scope of this paper.
A. System components
Figure 1 presents an overview of MarioNet, which consists
of three main components:
1) Distributor: a website under the attacker’s control (e.g.,
through the means discussed in Section III-A), which
delivers to users the MarioNet’s Servant component, along
with the regular content of the webpage. It should be noted
that the attacker does not need to worry about the time a
user will spend on the website. It takes only one visit to
invoke MarioNet and run on the background as long as
the victim’s browser is open.
2) Servant: the in-browser component of MarioNet, embed-
ded in a service worker. It gets delivered and planted
inside the user’s web browser by the Distributor. Upon
deployment, the Servant establishes a connection with its
Puppeteer through which it sends heartbeats and receives
the script of malicious tasks it has to perform. The Servant
runs in a separate process and thereby it continues its
operation uninterruptedly even after its parent tab closes.
3) Puppeteer: the remote command and control component.
This component sends tasks to the Servant to be executed,
Web server(3) Communication channel(WebSocket)(1) Webpage fetch (HTTPS)Blocking ExtensionsWeb BrowserPuppeteerService WorkerStandard Web Site        (2) Webpage                        renderingand orchestrates the performed malicious operations. The
Puppeteer is responsible for controlling the intensity of
resources utilization (CPU, memory, etc.) on the user side,
by tuning the computation rate of the planted Servant.
As illustrated in Figure 1, MarioNet is deployed in three
main steps: First, (step 1) the user visits the website (i.e.,
the Distributor) to get content that they are interested in. The
Distributor delivers the JavaScript code of the Servant along
with the rest of the webpage’s resources. During the phase
webpage rendering (step 2), the Servant is deployed in the user’s
browser. As part of its initialization, the Servant establishes a
communication channel with its remote command and control
server (Pupeteer) and requests the initial set of tasks (step 3).
The Pupeteer, which is maintained by the attacker, responds
with the malicious script (e.g., DDoS, password cracking,
cryptocurrency mining) the Servant has to execute.
B. Detailed Design
MarioNet leverages existing features of HTML5 to achieve
the objectives presented in Section III: isolation, persistence,
and evasiveness. In-browser attacks that involve computationally
heavy workloads require isolation in order to avoid interfering
with a webpage’s core functionality. Previous approaches [13],
[54] rely on web workers to carry out heavy computation in
the background (in a separate thread from the user’s interface
scripts). Although this isolation also prevents the code of the
web worker from having access to the DOM of the parent