m
(
y
c
n
e
t
a
L
 12000
 10000
 8000
 6000
 4000
 2000
 0
No Attack
Drop Parent
 0
 100
 200
 300
 400
 500
 0  100  200  300  400  500  600  700  800
 0  100  200  300  400  500  600  700  800
Simulation Time (s)
Simulation Time (s)
Simulation Time (s)
(a) Throughput impact score
(b) Latency impact score
Figure 10. Lookup latency for
the attacks found on DHT
Figure 11. Throughtput and latency for the attacks found on ESM
No Attack
Drop Data
Drop Join
Dup Join
Dup Data
Lie GroupId HB
Lie GroupId Join
)
s
p
b
k
(
t
u
p
h
g
u
o
r
h
T
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
 0
 100
 200
 300
 400
 500
 600
Simulation Time (s)
(a) Throughput for the attacks found on Scribe
(b) Scribe tree under Lie GroupId Join attack
Figure 12. Attack impact on Scribe
Impact score. We use throughput, which measures the
average amount of data received over time. As with ESM,
the impact score is the streaming rate minus the average
throughput over the last tw seconds.
Experimental setup. We simulated Scribe with 50
nodes and test it under the scenario where a source node cre-
ates a group, publishes streaming data at a rate of 1 Mbps,
and all other nodes subscribe to that group. We start ma-
licious actions immediately after the experiment starts so
we can attack tree construction, however as we ﬁnd the tree
takes up to 30 sec to form in our test environment, we set
tw to be 35 sec. We also ﬁnd malicious actions have a high
probability of being effective the ﬁrst time tried and thus set
na to be 1.
Attacks found using throughput. We found seven at-
tacks using throughput as an impact score. Fig. 12(a) shows
the effects of the different attacks. As a baseline we run
the system with no attack and ﬁnd that nodes are able to
consistently receive 1 Mbps of data.
Drop Data and Dup Data. First we found two obvious
attacks where nodes do not forward the data or they du-
plicate data messages and send the second message to a
random node. In the latter case, loops can occur, causing
signiﬁcant system load as data is increasingly replicated,
resulting in throughput to decrease below 200 kbps.
Dup Join, Lie GroupID Join, Drop Join. We found
that when malicious nodes duplicate Join messages and di-
vert the second message to a random node this causes the
throughput to drop below 200 kbps. This drop is due to
temporary forwarding loops when a tree node is a child of
multiple parent nodes. This temporary error will be cor-
rected by a heartbeat protocol, but only after a period of
time in which forwarding loops can cause damage. Gatling
found two additional attacks that cause the tree to not be
formed properly.
If malicious nodes lie about the group
identiﬁer in the Join message, then effectively the malicious
nodes are joining a different group, while believing they are
joining the requested group. Malicious nodes still respond
normally to other nodes’ requests to join the correct group.
This lie led the system to a situation that all malicious nodes
fail to join, and some benign nodes build a tree under ma-
licious nodes as seen in Fig. 12(b). Since the tree is split,
only nodes in the tree that have the source node inside can
receive data and nodes in other tree(s) can not receive any
data. Gatling also ﬁnds an attack of dropping Join mes-
sages, causing the same effect, but in the explored simula-
tion, more benign nodes happened to be a part of the tree
with the source, allowing better throughput.
6 Related Work
Automated debugging techniques, such as model check-
ing, have been in use for many years. Most similar to our
work is CrystalBall [56], where Yabandeh et al. utilize state
exploration to predict safety violations and steer the execu-
tion path away from them and into safe states in the de-
ployed system. Nodes predict consequences of their actions
by executing a limited state exploration on a recently taken
snapshot, which they take continuously. Since a violation
is predicted beforehand, it is possible to avoid actions that
will cause the violation. The Mace model checker is uti-
lized for safety properties and state exploration. However,
they do not consider performance metrics and thus can only
ﬁnd bugs or vulnerabilities that cause safety violations but
not performance degradation.
Many previous works have also used debugging tech-
niques for the purpose of automating the process of dis-
covering or preventing attacks. Proving the absence of
particular attacks have also been explored in the context
of limited models and environments [9, 10, 12]. These
debugging techniques include static and dynamic analy-
sis [16, 22, 32, 37, 43, 54], modelchecking [9, 10, 12], and
fault injection [8, 11, 50]. Below we summarize the works
that are focused on discovering attacks and are most similar
to our own.
Finding vulnerabilities in distributed systems has re-
cently been explored by Banabic et al. [11]. They employ
a fault injection technique on PBFT [15] to ﬁnd combina-
tions of MAC corruptions that cause nodes to crash. Our
work is more general, as Gatling does not focus on ﬁnding
all possible inputs that cause a single kind of vulnerability,
but rather searches on basic malicious actions to ﬁnd new
attacks.
Stanojevic et al. [50] develop a fault injection tech-
nique for automatically searching for gullibility in proto-
cols. They experiment on the two-party protocol ECN to
ﬁnd attacks that cause a malicious receiver to speed up and
slow down the sending of data. Their technique uses a brute
force search and considers lying about the ﬁelds in the head-
ers of packets and also drops packets. As they also utilize
Mace they are able to conduct protocol dependent attacks.
Our work differs in that we focus on large-scale distributed
systems, incorporate a fault injector that includes more di-
verse message delivery and lying actions, and use a greedy
approach to avoid brute forcing.
Kothari et al. [32] explore how to automatically ﬁnd
lying attacks that manipulate control ﬂow in implementa-
tions of protocols written in C. Their technique focuses on
searching for a sequence of values that causes a particular
statement in the code to be executed many times, thus po-
tentially causing an attack. Their method ﬁrst utilizes static
analysis of the code to reduce the search space of possi-
ble attack actions and then uses concrete execution to verify
the attack. However, to utilize the technique the user must
know ahead of time what parts of the code, if executed many
times, would cause an attack, which may not always be ob-
vious. Gatling, on the other hand, utilizes an impact score
to direct its search. Furthermore, some distributed systems,
such as Vivaldi, do not have attacks on them that manipulate
control ﬂow, but only attacks that involve lying about state.
Such attacks would go undiscovered by this technique.
7 Conclusion
Securing distributed systems against performance at-
tacks has previously been a manual process of ﬁnding at-
tacks and then patching or redesigning the system. In a ﬁrst
step towards automating this process, we presented Gatling,
a framework for automatically discovering performance at-
tacks in distributed systems. Gatling uses a model-checking
exploration approach on malicious actions to ﬁnd behaviors
that result in degraded performance. We provide a concrete
implementation of Gatling for the Mace toolkit. Once the
system is implemented in Mace the user needs to specify an
impact score in a simulation driver that allows the system to
run in the simulator.
To show the generality and effectiveness of Gatling, we
have applied it to six distributed systems that have a diverse
set of system goals. We were able to discover 41 attacks
in total, and for each system we were able to automatically
discover attacks that either stopped the system from achiev-
ing its goals or slowed down progress signiﬁcantly. While
some of the attacks have been previously found manually
through the cleverness of developers and researchers, we
show that the amount of time Gatling needs to ﬁnd such
attacks is small. Therefore, we conclude that Gatling can
help speed up the process of developing secure distributed
systems.
References
[1] Cyber-DEfense Technology Experimental Research labora-
tory Testbed. http://www.isi.edu/deter/.
[2] Emulab - Network Emulation. http://www.emulab.net/.
[3] Georgia
Simulator.
http://www.ece.gatech.edu/research/labs/MANIACS/GTNet
S/.
Network
Tech
[4] Global
Environment
for
Network
Innovation.
http://www.geni.net.
[5] Network Simulator 3. http://www.nsnam.org/.
peer-to-peer
[6] p2psim:
A simulator
for
http://pdos.csail.mit.edu/p2psim/.
protocols.
[7] Resilient Overlay Networks. http://nms.csail.mit.edu/ron/.
[8] J. Antunes, N. Neves, M. Correia, P. Verissimo, and
R. Neves. Vulnerability Discovery with Attack Injection.
IEEE Transactions on Software Engineering, 36:357–370,
2010.
[9] A. Armando, D. Basin, Y. Boichut, Y. Chevalier, L. Com-
pagna, J. Cuellar, P. H. Drielsma, P. Hem, O. Kouchnarenko,
J. Mantovani, S. Mdersheim, D. von Oheimb, M. Rusinow-
itch, J. Santiago, M. Turuani, L. Vigan, and L. Vigneron.
The AVISPA Tool for the Automated Validation of Inter-
net Security Protocols and Applications. In Proceedings of
Computer Aided Veriﬁcation, 2005.
[10] A. Armando and L. Compagna. SAT-based model-checking
for security protocols analysis. International Journal of In-
formation Security, 7:3–32, January 2008.
[11] R. Banabic, G. Candea, and R. Guerraoui. Automated Vul-
nerability Discovery in Distributed Systems. In Proceedings
of HotDep, 2011.
[12] B. Blanchet. From Secrecy to Authenticity in Security Pro-
tocols. In Proceedings of International Static Analysis Sym-
posium. Springer, 2002.
[13] C. Cadar, D. Dunbar, and D. Engler. KLEE: Unassisted
and automatic generation of high-coverage tests for complex
systems programs. In Proceedings of OSDI, 2008.
[14] M. Castro, P. Drushel, A. Ganesh, A. Rowstron, and D. Wal-
lach. Secure routing for structured peer-to-peer overlay net-
works. In Proceedings of OSDI, 2002.
[15] M. Castro and B. Liskov. Practical Byzantine fault tolerance.
In Proceedings of OSDI, 1999.
[16] C. Y. Cho, D. Babi, P. Poosankam, K. Z. Chen, E. X. Wu,
and D. Song. MACE: Model-inference-assisted concolic ex-
ploration for protocol and vulnerability discovery. In Pro-
ceedings of USENIX Security, 2011.
[17] Y.-H. Chu, A. Ganjam, T. S. E. Ng, S. Rao, K. Sripanid-
kulchai, J. Zhan, and H. Zhang. Early Experience with an
Internet Broadcast System Based on Overlay Multicast. In
Proceedings of USENIX ATC, 2004.
[18] B. Cohen. Incentives build robustness in BitTorrent. In Pro-
ceedings of P2P Economics, 2003.
[19] F. Dabek, R. Cox, F. Kaashoek, and R. Morris. Vivaldi: a
decentralized network coordinate system. In Proceedings of
SIGCOMM, 2004.
[20] D. Geels, G. Altekar, P. Maniatis, T. Roscoe, and I. Stoica.
Friday: global comprehension for distributed replay. In Pro-
ceedings of NSDI, 2007.
[21] P. Godefroid. Model checking for programming languages
using Verisoft. In Proceedings of POPL, 1997.
[22] P. Godefroid, M. Y. Levin, and D. Molnar. Automated
Whitebox Fuzz Testing. In Proceedings of NDSS, 2008.
[23] K. P. Gummadi, S. Saroiu, and S. D. Gribble. King: Esti-
In
mating Latency between Arbitrary Internet End Hosts.
Proceedings of ACM SIGCOMM-IMW, 2002.
[24] H. S. Gunawi, T. Do, P. Joshi, P. Alvaro, J. M. Hellerstein,
A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, K. Sen, and
D. Borthakur. FATE and DESTINI: a framework for cloud
recovery testing. In Proceedings of NSDI, 2011.
[25] G. J. Holzmann. The Model Checker SPIN. IEEE Transac-
tions on Software Engineering, 23:279–295, May 1997.
[26] C. Killian, J. W. Anderson, R. Jhala, and A. Vahdat. Life,
Death, and the Critical Transition: Detecting Liveness Bugs
in Systems Code. In Proceedings of NSDI, 2007.
[27] C. E. Killian, J. W. Anderson, R. Braud, R. Jhala, and A. M.
language support for building distributed
Vahdat. Mace:
systems. In Proceedings of PLDI, 2007.
[28] D. Kosti´c, R. Braud, C. Killian, E. Vandekieft, J. W. Ander-
son, A. C. Snoeren, and A. Vahdat. Maintaining high band-
width under dynamic network conditions. In Proceedings of
USENIX ATC, 2005.
[29] D. Kostic, A. Rodriguez, J. Albrecht, , and A. Vahdat. Bul-
let: High Bandwidth Data Dissemination Using an Overlay
Mesh. In Proceedings of SOSP, 2003.
[30] D. Kostic, A. Rodriguez, J. Albrecht, A. Bhirud, and A. Vah-
dat. Using random subsets to build scalable network ser-
vices. In Proceedings of USENIX-USITS, 2003.
[31] D. Kosti´c, A. C. Snoeren, A. Vahdat, R. Braud, C. Killian,
J. W. Anderson, J. Albrecht, A. Rodriguez, and E. Van-
dekieft. High-bandwidth data dissemination for large-scale
distributed systems. ACM Transactions on Computer Sys-
tems, 26(1):1–61, 2008.
[32] N. Kothari, R. Mahajan, T. Millstein, R. Govindan, and
M. Musuvathi. Finding Protocol Manipulation Attacks. In
Proceedings of SIGCOMM, 2011.
[33] M. Krohn, E. Kohler, and M. F. Kaashoek. Events can make
sense. In Proceedings of USENIX ATC, 2007.
[34] L. Lamport.
Specifying Systems: The TLA+ Language
and Tools for Hardware and Software Engineers. Addison-
Wesley Longman Publishing Co., Inc., Boston, MA, USA,
2002.
[35] L. Leonini, E. Rivi`ere, and P. Felber. SPLAY: distributed
systems evaluation made simple (or how to turn ideas into
live systems in a breeze). In Proceedings of NSDI, 2009.
[36] S. Lin, A. Pan, Z. Zhang, R. Guo, and Z. Guo. WiDS: an
Integrated Toolkit for Distributed Systems Deveopment. In
Proceedigs of HotOS, 2005.
[37] Z. Lin, X. Zhang, and D. Xu. Convicting exploitable soft-
ware vulnerabilities: An efﬁcient input provenance based
approach. In Proceedings of DSN, 2008.
[38] B. T. Loo, T. Condie, J. M. Hellerstein, P. Maniatis,
T. Roscoe, and I. Stoica. Implementing Declarative Over-
lays. In Proceedings of SOSP, Brighton, United Kingdom,
October 2005.
[39] X. Lui, W. Lin, A. Pan, and Z. Zhang. WiDS Checker:
In Proceedings
Combating Bugs In Distributed Systems.
of NSDI, Cambridge, Massachusetts, April 2007.
[40] N. Lynch. Distributed Algorithms. Morgan Kaufmann,
1996.
[41] M. Musuvathi, D. Park, A. Chou, D. Engler, and D. Dill.
CMC: A pragmatic approach to model checking real code.
In Proceedings of OSDI, 2002.
[42] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar,
and I. Neamtiu. Finding and Reproducing Heisenbugs in
Concurrent Programs. In Proceedings of OSDI, 2008.
[43] J. Newsome and D. Song. Dynamic Taint Analysis for Auto-
matic Detection, Analysis, and Signature Generation of Ex-
ploits on Commodity Software.
In Proceedings of NDSS,
2005.
[44] PlanetLab. http://www.planetlab.org.
[45] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and
S. Shenker. A scalable content-addressable network. In Pro-
ceedings of SIGCOMM. ACM, 2001.
[46] S. Rhea, D. Geels, T. Roscoe, and J. Kubiatowicz. Handling
churn in a DHT. In Proceedings of USENIX ATC, 2004.
[47] A. Rodriguez, D. Kosti´c, Dejan, and A. Vahdat. Scalability
in Adaptive Multi-Metric Overlays. In Proceedings of IEEE
ICDCS, 2004.
[48] A. Rowstron and P. Druschel. Pastry: Scalable, Decentral-
ized Object Location, and Routing for Large-Scale Peer-to-
Peer Systems.
In Proceedings of IFIP/ACM Middleware,
2001.
[49] A. Rowstron, A.-M. Kermarrec, M. Castro, and P. Druschel.
SCRIBE: The design of a large-scale event notiﬁcation in-
frastructure. In Proceedings of NGC, 2001.
[50] M. Stanojevic, R. Mahajan, T. Millstein, and M. Musuvathi.
Can You Fool Me? Towards Automatically Checking Proto-
col Gullibility. In Proceedings of HotNets, 2008.
[51] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and H. Bal-
akrishnan. Chord: A Scalable Peer-to-Peer Lookup Service
for Internet Applications.
In Proceedings of SIGCOMM,
2001.
[52] A. Vahdat, K. Yocum, K. Walsh, P. Mahadevan, D. Kosti´c,
J. Chase, and D. Becker. Scalability and accuracy in a large-
scale network emulator. In Proceedings of OSDI, 2002.
[53] A. Walters, D. Zage, and C. Nita-Rotaru. A Framework
for Mitigating Attacks Against Measurement-Based Adap-
tation Mechanisms in Unstructured Multicast Overlay Net-
works. IEEE/ACM Transactions on Networking, 16:1434–
1446, 2008.
[54] W. Wang, Y. Lei, D. Liu, D. Kung, C. Csallner, D. Zhang,
R. Kacker, and R. Kuhn. A Combinatorial Approach to De-
tecting Buffer Overﬂow Vulnerabilities. In Proceedings of
DSN, 2011.
[55] M. Welsh, D. E. Culler, and E. A. Brewer. SEDA: An Ar-
chitecture For Well-conditioned, Scalable Internet Services.
In Proceedings of SOSP, 2001.
[56] M. Yabandeh, N. Knezevic, D. Kostic, and V. Kuncak. Crys-
talBall: Predicting and Preventing Inconsistencies in De-
ployed Distributed Systems. In Proceedings of NSDI, 2009.
[57] J. Yang, T. Chen, M. Wu, Z. Xu, X. Liu, H. Lin, M. Yang,
F. Long, L. Zhang, and L. Zhou. MODIST: transparent
model checking of unmodiﬁed distributed systems. In Pro-
ceedings of NSDI, 2009.
[58] D. J. Zage and C. Nita-Rotaru. On the accuracy of decen-
tralized virtual coordinate systems in adversarial networks.
In Proceedings of CCS, 2007.