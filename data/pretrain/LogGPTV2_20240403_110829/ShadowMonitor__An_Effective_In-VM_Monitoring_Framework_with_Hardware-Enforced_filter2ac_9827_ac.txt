exception in KVM (they are not supported in the native KVM). Then we present
how to realize the prototype of the monitor system. Although the current imple-
mentation is speciﬁc to QEMU/KVM platform, we believe that it is portable to
other virtualization platforms such as Xen.
Fig. 4. A conﬁguration for enabling EPTP-switching and #VE
4.1 Enabling EPTP-switching and #VE
Enableing EPTP-Switching. To enable EPTP-switching in KVM, we need to
conﬁgure the processor properly following three rules. (1) The enable bit of VM
function should be set 1 in Virtual Machine Control Structure (VMCS). (2) The
EPTP switching bit of the VM Function Control ﬁeld should be correctly set. (3)
The value of pointers of the EPTP list should be stored into EPTP LIST ADDR.
Figure 4a shows an example of the conﬁguration. In speciﬁc, in KVM, the struc-
ture kvm mmu page is used to store the EPT, and root hpa of kvm mmu is used
to store the EPTP which points to EPT. For EPTP-switching, we need to main-
tain multiple (at least two) EPTs in KVM. To this end, we modify the original
structure of the page table. In detail, we use an array ept root hpa list to save
pointers of EPTs, and kvm mmu page list to save the page directory of EPTs.
By default, only the ﬁrst EPT is initialized, while the other elements in these
two arrays are set NULL. When we tend to use another EPT, we allocate a
new EPT page directory in KVM and then ﬁll the associated structures. It’s
worth noting that we can manage up to 512 compartments for the guest since
the current Intel EPTP switching supports 512 EPTP entries.
Enabling Virtualization Exception. To enable virtualization exception, we
ﬁrst set the #VE bit of EPT-violation to 1 in the execution control ﬁeld of
VMCS, and then set bit 20 of EXCEPTION BITMAP to 0. Figure 4b shows an
example of conﬁguration on virtualization exception. When building the EPT-S
paging-structure hierarchy, bit 63 of speciﬁc EPT paging-structure entries should
be 0 (which correspond to the pages that cause#VE upon EPT violation). In
addition, we place a handler at vector 20 of the IDT to handle the #VE in the
guest.
ShadowMonitor: An Eﬀective In-VM Monitoring Framework
681
4.2 Deploying ShadowMonitor Framework
As described in Sect. 3, ShadowMonitor components (i.e. Monitor Code, Moni-
tor data, and Switch Gate) are placed in the guest VM. Since the guest OS is
untrusted, we should satisfy two requirements for deploying them in the guest
safely and properly. First, the Monitor Code and Monitor Data are placed in a
separated address space that cannot be touched by the guest to guarantee their
safety, as described in Sect. 3.1; Second, the Switch Gate is write-protected by
hardware features, as introduced in Sect. 3.2. To achieve this, we use a collabo-
rative approach to cooperate guest OS and hypervisor. We ﬁrst insert a kernel
module after OS boot-up, which takes charge of reserving two virtual address
ranges in the guest, one of which for INT 20 handler (i.e., Switch Gate) and
another for Monitor Code and Monitor Data. Since we assume that the OS dur-
ing the boot-up stage is safe, the module is considered to be inserted safely. Then,
the kernel module will inform the hypervisor of the start address and length of
the memory regions via VMCALL instruction. Once the hypervisor knows the
information about memory regions, it creates and activates EPT-S/EPT-N, sets
the permission of page table entries, and loads the Monitor Code as well as
Switch Gate for event monitoring, as described in Sects. 3.1 and 3.2.
5 Security Analysis
ShadowMnitor should not only detect malicious behaviors but also prevent itself
from attacks. To verify the eﬀectiveness of ShadowMonitor, we present a com-
prehensive security analysis in this Section.
Rootkit Attack. Rootkit attack here tends to compromise the guest OS
and thus can destroy the monitoring tools placed in the guest. ShadowMonitor
can prevent such attacks because it provides strong isolation between the main
and shadow compartments in which the monitoring tools are invisible to the
guest. Moreover, the attacks that tend to tamper with the interceptor points
and the Switch Gate will trigger page fault since the associated pages have been
written-protected, thus they are easy to be detected.
Insider Attack. For each monitoring point, ShadowMonitor will execute
one guest instruction in the shadow compartment. Though we have some mecha-
nisms that check whether the instruction of guest code to be executed is benign, a
sophisticated attacker may still make a tricky attack by letting malicious instruc-
tion sequences execute in shadow compartment. This attack requires the permis-
sion to register event interceptors, so it must be the insider attacker. However, it
is beyond our study because we assume that the user who has the permission
to conﬁg ShadowMonitor are trusted.
Address Translation Redirection Attack. An attack can rewrite the
guest page mapping of the memory region of Switch Gate in the guest so that
the Switch Gate will not be protected in the shadow compartment. Then it can
tamper the content of Switch Gate and ﬁnally escape the monitoring of Shadow-
Monitor. ShadowMonitor could prevent such attack by tracking the modiﬁcation
682
B. Shi et al.
of page tables. It tracks only 4 page table entries for each associated page to be
protected, and thus imposes insigniﬁcant performance loss.
IDT Redirection Attack. In ShadowMonitor, the Switch Gate is invoked
by INT 20 instruction. If the attacker redirects the interrupt descriptor table
(IDT) of the guest to a non-write-protected page, and then replace the INT 20
handler with a malware. Then, it can escape the detection of ShadowMonitor. We
prevent this attack by trapping all LIDT instructions which are used to modify
the IDT pointer register. The trapping can be conﬁgured in the VM-execution
control ﬁeld of the VMCS.
INT 20 Fake Attack. Because the Switch Gate is visible in the main
compartment, the attacker may maliciously jump to the Switch Gate or invoke
INT 20 instruction, which will lead to unexpected switching of compartments.
To prevent it, we adopt the same method of SIM, i.e., we check the branch that
transferred execution to the entry gate using the LBR information (last branch
recording) so that we can detect this attack.
VMFUNC Fake Attack. The malicious program may intentionally invoke
VMFUNC instruction, thereby leading to unexpected switching of compart-
ments. To prevent this, the executable memory code of the shadow compartment
are predeﬁned and ﬁxedly located. This means that the memory pages containing
the faked VMFUNC instructions should be not executable after completing the
compartment switching. Thus, the fake operation will trigger an EPT violation
and ﬁnally lead to a virtualization exception (#VE). By the design of Shadow-
Monitor, the #VE will invoke the Switch Gate and then allows the operations
to switch back to the main compartment. It should be pointed out that expe-
riences attackers may carefully design the memory mapping of the guest. For
example, they place the GVA of a memory page to be the prior one before the
Monitor Code pages and then put the VMFUNC instruction into the last bytes
of this page. In this way, if this VMFUNC is invoked, the next program counter
will point to the instruction located in the Monitor Code. Once this instruction
is executed, the attacker could breaks into the shadow compartment without
passing the designated gate. To prevent this, we place VMFUNC instruction
into the ﬁrst bytes of the Monitor Code pages. Therefore, the ﬁrst touch of the
shadow compartment will force the malicious program to switch back to the
main compartment.
We reproduce some malicious attacks that try to subvert diﬀerent VMI
approaches including ShadowMonitor, LibVMI, and SIM. The results in Table 1
illustrate that ShadowMonitor is able to prevent against diﬀerent attacks. Shad-
owMonitor provides more security guarantee than SIM (in-VM monitor) and
no-less than LibVMI (out-VM monitor).
Limitations. There exist some attack approaches that beyond our scope
and thus cannot be directly defended by ShadowMonitor. (1) The vulnerabili-
ties hidden in the hypervisor would enable VM-escape attack, which will easily
destroy our system. One feasible solution for this type of attack is to deploy intru-
sion detection systems on the hosts. (2) In scenarios where network or hardware
are untrusted, side-channel attack may employ wiretapping to access private
ShadowMonitor: An Eﬀective In-VM Monitoring Framework
683
Table 1. Ability to resist diﬀerent attacks
Approaches
Common attack
ATRA attack
IDT redirect attack
INT 20 fake attack
VMFUNC Fake Attack
√
√
√
√
ShadowMonitor SIM LibVMI
√
√
√
√
X
X
-
-
-
-
-
data. To defend such attack, we can leverage some hardware encrypting tech-
niques such as Secage [22] and Intel SGX [25]. (3) Bypass attack. Since the
interception points change the guest’s original execution ﬂow, the attackers may
perceive their existence, identify them, and ﬁnally bypass the interception. This
critical problem is common in active monitoring approaches [27,29]. One pos-
sible solution is to use code analysis or call trace analysis to detect the bypass
behaviors. We will leave this as our future work.
6 Evaluation
In this section, we perform a set of experiments to demonstrate the performance
loss introduced by ShadowMonitor, under both micro and macro benchmarks.
The micro-benchmarks allow appreciating the raw beneﬁt of the design while the
macro-benchmarks validate the beneﬁts for the end users. We leverage LibVMI
(out-of-VM approach) and SIM (in-VM approach) as the baseline. LibVMI is
an open source introspection library that provides a variety of event monitoring
interfaces. It is the most representative out-of-VM approach. It’s worth noting
that LibVMI only supports event interception in Xen hypervisor, so the results
here of LibVMI are all collected on Xen (version 4.6). SIM is a representative
in-VM monitor approach, we re-implement SIM in KVM following to its design.
The hardware platform is conﬁgured with Intel Core i7-6700 3.4 GHz proces-
sors, 16 GB DDR memory, a 1000 GB WD disk with 7200 RPM and Intel I219-
LM Gigabit NIC card. The operating system on the physical server is Ubuntu
14.04 with 4.10.2 64bit kernel. The virtual machines are conﬁgured with 2 vcpus,
4 GB RAM, and 100 GB Disk unless speciﬁed otherwise.
6.1 Overhead of Monitoring Invocation
The overhead introduced by ShadowMonitor is mainly from switching between
compartments. We ﬁrst measure the time cost of each VMFUNC instruction
and then compare it with that of syscall and VM-exit. As shown in Table 2, the
overhead of VMFUNC instruction is comparable to that of the syscall, i.e., 69.38
ns vs. 75.26 ns, while it is much longer for VM-exit, i.e., 653.71 ns.
To explore the monitoring overhead further, we then measure the time of
invocation of monitoring, which denotes the total time to intercept an event,
684
B. Shi et al.
Table 2. Overhead of syscall, VMFUNC, and VM-exit
Operations Average time (ns)
Syscall
VMFUNC
69.38
75.26
VM-exit
653.71
switch to the shadow compartment and then switch back. As can be seen from
Table 3, invoking the monitoring with ShadowMonitor is 11.1× faster than Lib-
VMI, since it introduces less switching than LibVMI. Since the overall perfor-
mance depends on the accumulation of time of single invocation, a monitor
that is frequently invoked (e.g. ﬁne-grained monitoring) would gain beneﬁt from
ShadowMonitor. It’s worth noting that the overhead of ShadowMonitor is sim-
ilar to SIM because both of these two methods adopt the idea of compartment
switching.
Table 3. Comparison of overhead on monitoring invocation
Approaches
Average time (ns) Standard deviation (ns)
ShadowMonitor
SIM
LibVMI
471
488
5231
63.8
61.4
139.1
Table 4. Memory acess performance comparison
Bytes ShadowMonitor (µs) LibVMI (µs) SIM (µs)
4
0.357
17.3
0.187
64
0.351
17.4
0.194
6.2 Memory Access Speed
ShadowMonitor set the permission of memory pages to be protected. In this
experiment, we will measure the memory access speed of guest with Shadow-
Monitor. To make a fair comparison, we ﬂush the TLB (translation lookaside
buﬀer) and disable the cache of LibVMI. Table 4 shows the experimental results.
As we can see, our ShadowMonitor method achieves hundreds of times faster
than LibVMI. It should be pointed out that ShadowMonitor performs poorer
than SIM, mainly because it needs the translation of the extended page table
(EPT). However, in practical scenarios, we should also consider the time of han-
dling page faults. As we will see in the next section, ShadowMonitor outperforms
SIM in terms of overall system performance.
ShadowMonitor: An Eﬀective In-VM Monitoring Framework
685
6.3 Overall System Performance
In this experiments, we intercept and monitor all the system calls and pro-
cess switches in the guest, for measuring the overall performance of the guest
system when VMI approach is employed. To monitor syscall, we directly set
interceptor on the memory address which is pointed by MSR-LSTAR. As for
process switches, we set interceptor on the kernel function context switch() for
ShadowMonitor and LibVMI. Note that SIM approach requires no interceptor
for process switch because all the process switches will cause VM-exit in SIM.
The Monitor Code will parse the arguments for syscall, and extract the process
name for a process switch. We evaluate ShadowMonitor under four benchmarks:
(1) UnixBench is a benchmark suite that measures Linux performance. With
this, we can easily quantify the performance impact of many diﬀerent aspects
of the OS. We display four representative indicators of the Unixbench in the
results, which are Whetstone (computing), Process creation (scheduling and vir-
tual memory management), File copy (ﬁle system), and System call (kernel inter-
face). (2) Kernel compilation is a memory-intensive and IO-intensive work-
load. It can reﬂect the performance impacts on VMs when they are deployed
and launched in production systems. We compile the Linux 4.10.2 kernel by
default conﬁgure and measure the time it spends. (3) File Compression is a
IO-intensive and computation-intensive workload. This kind of workload is com-
mon when VMs are used as computation nodes. We use the zip algorithm to com-
press Linux kernel source and measure the time overhead. (4) Apachebench is