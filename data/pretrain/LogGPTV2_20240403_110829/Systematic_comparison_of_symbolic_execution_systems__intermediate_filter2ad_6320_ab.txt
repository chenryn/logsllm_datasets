Programming language Concrete execution
None
QEMU with KVM
Unicorn
Directly on the CPU
Table 1: Comparison of design choices relevant to our study in the four symbolic execution engines that we analyze.
intermediate representation of the LLVM compiler framework. No-
tably, the C/C++ compiler clang can emit LLVM bitcode, which
is the IR generation approach proposed originally by KLEE’s au-
thors.1 This makes KLEE unique in our study: it is the only tool
that generates IR from source code rather than lifting binaries. It
uses the SMT solver STP [6] by default but also supports Z3 [15],
which we use as a common ground in our study. KLEE executes all
user code at the IR level.
S2E. In order to address several perceived shortcomings of KLEE,
Chipounov et al. proposed Selective Symbolic Execution (S2E) [11].
It builds on top of KLEE but executes programs inside a full virtual
operating system. The important difference for our purposes is
that S2E generates IR from binaries instead of source code. The
program and its environment run inside QEMU [3], a system emu-
lator based on binary translation, and a lifter from QEMU’s internal
representation to LLVM IR converts the code to a format suitable
for consumption by KLEE on demand. Only code interacting with
symbolic data is executed symbolically; all other code, including the
emulated operating system, runs directly in QEMU. Note that KLEE
and S2E use the same symbolic execution engine as well as the same
IR but different mechanisms to generate it. This similarity allows
us to compare their respective IR generation strategies without the
measurement noise from other differences.
angr. Shoshitaishvili et al. created angr [38] with the goal of
implementing various previously published binary-analysis tech-
niques in a single framework in order to make them comparable.
Among many tools for binary analysis, angr provides a symbolic ex-
ecution engine based on VEX, the intermediate representation used
by the Valgrind tools [28]. The system translates binaries to VEX IR,
which is then interpreted by angr’s symbolic executor. The user can
configure whether code that handles only concrete data is passed
on to the Unicorn CPU emulator [32]. The core emulator is imple-
mented in Python in order to facilitate quick experimentation and
scripting. This decision influences execution speed in comparison
with tools that are written in lower-level programming languages.
We discuss the aspect in more detail during our analysis.2
Qsym. Yun et al. argue that IR generation and semantic discrep-
ancy between the machine code and IR instruction sets are a major
hindrance in modern symbolic execution [43]. To address this prob-
lem, they propose Qsym, a symbolic execution engine that directly
executes instrumented machine code. The implementation of the
symbolic executor is more involved than in conventional IR-based
systems, having to handle the large and complex instruction sets
of modern CPUs, but the authors argue that the significant perfor-
mance gains justify the additional implementation work. In our
study, we are interested in Qsym precisely because of its lack of
IR generation mechanism. The system supplies an interesting data
point for our analysis of execution speed and SMT query com-
plexity. Qsym decides at the instruction level whether to execute
symbolically or natively.
1We use clang version 3.8 with wllvm version 1.2.2 to generate LLVM bitcode.
2The authors recommend executing angr in PyPy, a JIT-compiling implementation of
Python; we use PyPy version 5.1.2.
source codemachine codecompilerIRnative execution1SMT queriesIRexecution query cachelibrary/OSinterfacesecurity checks2state forkingsearch strategySMT solverIR generationExecutionSymbolic backendSymbolic Execution: IR and its Generation
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
5 EVALUATION
This section conducts the actual measurements. Recall that our
ultimate goal is to answer the following research questions:
(1) What is the impact on symbolic execution of generating IR
from source code as opposed to IR generation from binaries?
(2) Does one IR perform better than others when IR generation
is comparable? What is the impact of not using IR at all?
In order to answer those high-level questions, we need to decide
on concretely measurable properties that supply the necessary
evidence. What do we expect of an ideal IR generation technique
for symbolic execution? Since we are going to execute the IR, we
want it to be easy to interpret efficiently, and we want it to be
concise. Moreover, since SMT solving consumes a considerable
portion of the overall analysis time, we would like the IR to lead
to SMT queries that the solver can answer quickly. We therefore
evaluate the various IR generation mechanisms under three aspects
motivated by the observations above:
(1) How much does the translation to IR increase or decrease
the number of instructions?
(2) How efficiently can we execute the resulting IR?
(3) How hard are the solver queries derived from the IR?
We first discuss our methodology and the non-trivial task of
generating a set of programs that are supported by all the symbolic
execution engines we selected. Then we investigate the effect on the
number of instructions before presenting the results on execution
speed and query complexity. Interested readers will find additional
visualizations and a link to raw data in the appendix. We discuss
the implications of our results in the next section, where we also
answer the research questions.
5.1 Experimental Setup
A core challenge in assessing the impact of IR and IR generation
on symbolic execution is that different symbolic execution engines
generally differ in many factors, not just the IR generation process.
For instance, KLEE and angr differ in how they generate IR, but
in addition to this aspect relevant to our study there are other
differences that introduce noise into our measurements:
• One is implemented in C++, the other in Python. We find that
this has a major impact on the speed of symbolic execution.
• Their respective execution engines vary in search strategy,
i.e., they use different heuristics for prioritizing execution
states. Some simple heuristics like depth-first search are
supported by both but generally do not lead to interesting
paths through the software under test [10, 37].
• The systems have been developed with different goals in
mind. While the one focuses on speed and fully automatic
execution, the other places some emphasis on scriptability
and interactive exploration.
• Implementations of symbolic execution may be faulty. While
our goal is to evaluate a given approach, we can only analyze
the implementation at hand and have to trust that it faithfully
represents the approach. Previous work has shown that there
can be discrepancies [34].
It is therefore difficult to isolate the effects of IR generation from
the influence of other differences. One option would be to imple-
ment a grand unified symbolic execution engine working on top of
the various IRs in order to eliminate most variables. However, the
symbolic execution engines we analyze are tuned to the properties
of their respective IR; for instance, KLEE can run optimizations
on the input LLVM bitcode that are meant to compensate some
shortcomings of the IR generation process and make the IR more
suitable for symbolic execution. We felt that running the IR of the
various systems in a more generic execution engine would lead
to a less fair comparison. Instead, we strove to eliminate as many
variables as possible by identifying design decisions that could in-
troduce noise into our measurements (see Section 3) and making
minimal changes to all systems in order to remove any such differ-
ences (discussed below). We believe that such an analysis, despite
its possible limitations, yields the most valuable insights into the
problem at hand.
While measuring the impact on code size of each system is
relatively easy, in order to evaluate the execution speed and the
complexity of generated queries we first had to find a set of binaries
that all four symbolic execution engines under analysis are able
to execute. We remark that this has turned out to be a significant
challenge: while the four systems share an overall goal, the specifics
vary enough to make it difficult to find binaries for which each tool
is usable. We discuss the implications for our benchmarks in more
detail below.
After some experimentation, we decided to use the programs
from DARPA’s Cyber Grand Challenge (CGC) for our evaluation,
mainly for two reasons: First, the CGC programs have explicitly
been designed as a test suite for automated vulnerability detection
and exploitation systems. They are supposed to exhibit common
code patterns. Moreover, they run on DECREE, a Linux-based op-
erating system with a simplified system call interface, originally
designed in order to reduce the engineering burden on the partici-
pants in the CGC competition. This makes it easier for us to add
missing support to symbolic execution engines. Second, S2E and
angr were used by teams participating in the CGC. Therefore, those
tools are known to work with the CGC programs. Furthermore, the
authors of Qsym evaluate their system on a variant of the CGC
binaries in the original publication [43]. The CGC suite contains a
total of 131 different programs.
As discussed in Section 3.1, the choice of path selection and
scheduling algorithms has a major impact on symbolic execution.
We eliminate this potential source of noise in our measurements
by evaluating concolic execution, i.e., we make symbolic execution
follow the path determined by a fixed input. In particular, we use
the proofs of vulnerability (PoVs) provided by DARPA for each
CGC application. They represent interactions with the applications
that exercise bugs. Where multiple such PoVs are available, we
choose the first. Our input selection procedure is thus analogous
to the Qsym authors’ strategy. The motivation to use the PoVs for
test input, as outlined by Yun et al. [43], is the implicit assumption
that inputs reaching the bugs in the CGC applications exercise
interesting portions of code.
DARPA provides the PoVs in a custom XML format designed
to describe the interaction with a target application. We wrote a
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Sebastian Poeplau and Aurélien Francillon
tool that translates the XML description to raw data; we skip appli-
cations where the translation of the corresponding PoVs was not
possible. This happens in a few cases where the input exercising
the vulnerability in a program depended on previous output re-
ceived from the program—the XML format provides facilities for
handling such scenarios, but the same logic cannot be reflected in
raw data inputs. We confirmed with the authors of Qsym that this
aspect of our procedure is analogous to their evaluation, helping
comparability.
All four symbolic execution engines required modifications or
extensions for our experiments. We strove to keep our changes to
the engines minimal in order to avoid interference, and we make our
modifications available to the community. Concretely, we added 134
lines of code (LoC) to KLEE (partial support for mmap and munmap),
67 LoC to S2E (time measurements and early termination of execu-
tion states), 19 LoC to angr and 26 LoC to Qsym (timing and query
logging in both cases). We wrote considerably more code, but it
is concerned with the generation of suitably compiled programs,
conversion of the inputs provided by DARPA into the right formats,
proper invocation of the tools, automated measurements, etc.—it
does not affect the inner workings of the engines under analysis.
We execute each symbolic execution engine on each CGC appli-
cation with a timeout of 30 minutes and a memory limit of 24 GB.
The experiments run under Ubuntu 16.04 and each use one core of
an Intel Xeon Gold 6130 CPU. We skip any applications that are not
supported by all engines. Note that, while 30 minutes of symbolic
execution would be far too short for vulnerability discovery, we do
not let the systems explore the target applications freely. Instead,
we execute symbolically along a predetermined path (which, coin-
cidentally, is known to lead to a vulnerability), observing run-time
aspects such as the speed of execution and generated SMT queries
on the way. The time frame of 30 minutes is sufficient to finish
execution in most cases; we exclude any experiments that run into
a timeout or exceed the memory quota.
5.2 Benchmark size
Out of the 131 CGC programs, only 24 execute successfully in all
four symbolic execution engines (see Table 2; Appendix C describes
the applications). While IR generation is not typically a problem
since all systems use mature generators, incompatibilities of the
IR execution engines precluded successful analysis in many cases.
For example, KLEE immediately exits if the program under test
contains floating-point instructions; we compiled the target pro-
grams statically to make sure that the offending instructions only
occur in programs where they are strictly required, but even so
KLEE exhibits the smallest number of supported programs. In the
case of angr, its focus on scripting and interactive exploration often
renders it too slow to work on large binaries. S2E, in turn, has
only recently gained the ability to track data through MMX/SSE
registers; in earlier versions, the contents of such registers were
concretized, causing the symbolic execution engine to lose track of
the corresponding symbolic expressions. Note that SSE registers
are used in prominent places, such as the strcmp and strncmp
functions in GNU libc. The example of S2E also demonstrates that
adding all missing features ourselves was not an option: the code
for MMX/SSE register support alone amounts to roughly 1400 lines
S2E
angr KLEE
Qsym
all
70.2% 66.4% 75.6% 35.1% 18.3%
Execution speed (Section 5.4)
Query complexity (Section 5.5)
57.3% 74.8% 87.8% 38.9% 17.6%
Table 2: Percentage of CGC programs (out of 131) that we
were able to use per experiment and symbolic execution en-
gine. See Appendix C for more details.
of C/C++ across various libraries [35]; and this addresses a single
limitation in a single tool. In general, the missing features typically
depend on time-consuming engineering—which is presumably why
they have not been implemented in the first place. Similar problems
have been described by Qu and Robinson [30] and by Xu et al. [42].
Finally, fairness requires us to base our comparison only on targets
that are supported by all engines, which further restricts the test
set.
The lack of extensive tool support is the main reason why we
believe it is not currently possible to compare symbolic execution
engines on large sets of applications, especially on applications
with high complexity. Even assembling a set of 24 applications that
work with all four symbolic execution tools in our analysis has cost
us significant time and effort. Under such circumstances, is there
even value in the comparison? We strongly believe that there is,
for two reasons:
(1) Even on a limited data set we can see trends; such obser-
vations add rigor to a discussion that has until now been
driven by intuition and anecdotal evidence.
(2) As a community, we should incentivize comparable research—
if a new tool in the field cannot meaningfully be compared to
existing approaches, we cannot assess its value. We should
therefore strive to establish a shared benchmarking method-
ology and data set; this paper attempts to take a step in that
direction.
5.3 Code Size
We have previously mentioned the intuition that IR derived from
source code contains “more high-level information” than binary-
based IR; a more precise way of expressing this intuition is to
say that we expect source-derived IR to contain more semantic
information per IR statement than IR derived from binaries.3 In
order to test this hypothesis we apply the IR generation techniques
under analysis to a fixed set of programs and compare the resulting
number of IR instructions. The base line for our experiments is the
number of machine-code instructions.
In addition to the CGC programs discussed above, we use the
programs of version 8.30 of the coreutils suite [19] for this compar-
ison; they are a popular benchmark in the literature on symbolic
execution. For each binary in the set of test programs (i.e., CGC
and coreutils), we recover the CFG with angr and subsequently
apply each symbolic execution engine’s IR translation mechanism
to all discovered basic blocks. This requires wrapping the relevant
3There is the additional effect that some information is actually lost during compilation,
such as buffer sizes [12]. This is a concern for security checks that may be part of
a symbolic execution engine but does not affect the core components of symbolic
execution that we focus on in this study (see Figure 1).
Symbolic Execution: IR and its Generation
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
e
t
a
r
n
o
i
t
a
fl
n
I
9
8
7
6
5
4
3
2
1
0
KLEE McSema
S2E
angr
angr
(ARM)
Figure 2: Inflation factor per IR generation mechanism, i.e.,
the number of generated IR instructions per machine-code
instruction, across all tested programs (123 CGC and 106
coreutils binaries). The box encloses the second and third
quartile of the data with a horizontal line marking the me-
dian. The whiskers include data points up to 1.5 times the
interquartile range away; outliers beyond that point are de-
picted individually.
parts of code in S2E and angr: the former exposes the translation
component as a shared library that we can use from a C++ program,
whereas the latter offers a Python interface which we use from a
custom script. Qsym and KLEE do not require custom extensions
for this step of our study: the former works directly on machine
code, so that no translation is necessary, and the latter conveniently
uses the output of the C/C++ compiler clang.
programs of the coreutils suite:
For comparison, we conducted some further experiments on the
• We added the results of McSema, a static translator from
machine code to LLVM bitcode [16] based on the commer-
cial disassembler IDA Pro. Note that we intentionally used
McSema unmodified for best performance, meaning that it
employed IDA Pro for disassembly rather than angr. While
we had initially hoped to be able to run KLEE on the bitcode
that McSema generates, we found that there are incompatibil-
ities in the respective sets of supported bitcode instructions;
substantial changes would be required to make the two sys-
tems compatible.
• We compiled the coreutils binaries for ARM, using the target
arm-none-eabi, and ran angr’s IR generation on them. The
other symbolic execution engines do not support ARM or,
in the case of KLEE, the IR does not differ significantly.
We compare the number of generated IR instructions to the
corresponding number of machine instructions, resulting in a quan-
tity that we call inflation factor. Table 3 shows the results of our
measurements, and Figure 2 visualizes the data.