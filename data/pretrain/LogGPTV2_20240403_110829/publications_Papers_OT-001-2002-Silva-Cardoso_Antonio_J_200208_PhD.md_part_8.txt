information on the processing of sequences that could be used to a more precise
definition of this function.
2.7.2 COMPUTING QOS METRICS
The domain experts believe that there is a strong agreement between the tasks QoS
exhibited during the execution of the Prepare and Sequence and the Sequence Processing
workflows, and the expected QoS of the tasks to be scheduled by the DNA Sequencing
workflow. This belief is based on the fact that the tasks executed in the two initial
workflows will be executed without any change by the newly constructed workflow. The
following functions (see also Table 2-5) have been utilized to re-compute QoS metrics
based on designer and runtime information:
69
Table 2-12 – Re-computation of the QoS dimensions for the DNA Sequencing workflow
b) QoS (t) 0.2*Designer Average (t) + 0.8*Multi-Workflow
Dim Dim
Average (t)
Dim
c) QoS (t, w) 0.2*Designer Average (t) + 0.2*Multi-Workflow
Dim Dim
Average (t) + 0.6*Workflow Average (t, w)
Dim Dim
To represent the QoS agreement among tasks from different workflows, the domain
experts have decided to set the weights according to the following beliefs. For formula b),
the domain experts believe that the recorded QoS of tasks previously executed will give
good estimates for the execution of tasks scheduled by the new workflow. Thus, the
experts set the weights wi and wi of formula b) to 0.2 and 0.8, respectively. The domain
1 2
experts also believe that as soon as tasks are scheduled by the new workflow, the QoS
estimates should rely on the latest QoS data recorded from the DNA Sequencing
workflow. Also, they consider that when QoS data is available from the DNA Sequencing
workflow, the importance given to the designer estimates should have the same influence
as the QoS estimates recorded for the execution of tasks scheduled by other workflows
than the DNA Sequencing. Therefore, for formula c), the experts set the weights wi , wi ,
1 2
and wi to 0.2, 0.2, and 0.6, respectively. In our experiments, we only predict workflow
3
QoS metrics before the execution of workflow, not during workflow execution; thus, we
did not to set the weights for formula c) from Table 2-6.
Since the new workflow has a loop that did not exist in any of the previously
executed workflows, it is necessary to estimate the probability of the transition (Test
Quality, Prepare Sample) to be enabled at runtime. Based on prior knowledge of
sequencing experiments, the researchers calculate that approximately 10% of the DNA
sequence will contain E. coli bacteria and that thus there is a 10% probability of the loop
back transition being enabled.
70
2.7.3 RESULTS
We have run a set of ten experiments. Each experiment involved the execution of the
SWR algorithm to predict QoS metrics of the DNA Sequencing workflow and the actual
execution of the workflow. The results are shown for the four QoS dimensions in Figure
2-13, Figure 2-14, Figure 2-15, and Figure 2-16. The diamonds indicate the QoS
estimates given by the SWR algorithm and the squares indicate the runtime metrics .
Time Analyzis
650.0
)sruoh(
550.0
450.0
emiT
350.0
250.0
1 2 3 4 5 6 7 8 9 10
Instance #
Figure 2-13 – Experiment results (Time Analysis)
71
Cost Analyzis
$2,500
$2,000
tsoC
$1,500
$1,000
1 2 3 4 5 6 7 8 9 10
Instance #
Figure 2-14 – Experiment results (Cost Analysis)
Reliability Analyzis
100.0%
99.8% ytilibaileR
99.6%
99.4%
99.2%
1 2 3 4 5 6 7 8 9 10
Instance #
Figure 2-15 – Experiment results (Reliability Analysis )
72
Fidelity Analyzis
0.65
0.60
ytilediF
0.55
0.50
0.45
1 2 3 4 5 6 7 8 9 10
Instance #
Figure 2-16 – Experiment results (Fidelity Analysis)
For the time analysis, the most relevant information that can be interpreted from the
chart is the observation that the instances 3 and 4 have registered actual running times
that are considerably different from the values estimated. This is due to the topology of
the workflow. During the process, it is expected that some DNA sequences will contain
E. coli contamination. When this happens, re-work is needed, and the first part of the
workflow, involving the tasks Prepare Sample, Prepare Clone and Sequence, and
Assembly, has to be re-executed. The first part of the workflow takes approximately 99%
of the overall workflow execution time. Thus, when E. coli contamination is present in a
sequence, the time needed to execute the workflow almost doubles. Since it is impossible
to know if a DNA sequence will contain E. coli or not, the SWR algorithm gives an
estimate for instance 3 which is significantly different from the registered values. When
instance 4 is executed, the QoS metrics from the previous instance are considered for the
QoS estimation. As a result, it can be seen in the chart that the SWR estimation converges
to the mean of the recent time metrics recorded. If more instances detect the presence of
73
E. coli contamination, the results of the SWR algorithm for the time dimension will
gradually converge to the 550 hours level. When instances number 5 through 10 are
executed, they do not detect the presence of contamination in the sequences processed.
As a result, the SWR estimates are more accurate, and the estimates start to slowly
converge at lower time values.
The costs associated with each task have been provided from technical datasheets
describing the DNA Sequencing process. For the cost analysis, the results observed are
strongly linked to the results obtained from the time analysis. Again, instances 3 and 4
have recorded actual costs that are considerably different from the values estimated. This
is due to the existence of E. coli contamination in the sequences processed. When
contamination is detected, the re-work necessary to carry out the sequencing double the
cost of the instance. This is because the cost of an instance is totally determined by the
tasks Prepare Sample, Prepare Clone and Sequence, and Assembly, which are involved
in any necessary re-work. All the other tasks, which are mainly automated software tasks,
are considered to have a zero cost. As with the time analysis, the convergence of the
SWR algorithm towards recent registered metrics can be seen. One particularity of the
DNA Sequencing workflow is the discrete linearity of its cost. When no re-work is
necessary because no contamination is detected, the cost of the instance is c. If
contamination is found, then re-work is needed, and the cost of the instance is 2c. If
contamination is found n times during the sequencing process, the cost of the instance
will be nc. This property for the cost dimension can be observed from the chart, where
instances with no re-work always have the same cost ($1,152), and instances that need re-
working one time have a cost of $2,304.
The fidelity analysis shows the creation of very good estimates. It can be seen that
the SWR algorithm constantly changes its convergence as a response to recently recorded
QoS metrics. The runtime fidelity metrics are within a small range, as predicted from the
estimates.
74
The reliability analysis is relatively easy to interpret. For the first instance executed,
the SWR algorithm has used information specified by the designer and derived from tsak
executions from the Prepare and Sequence and Sequence Processing workflows. The
information suggests that the reliability of the new workflow design will be 99.4%. But
during our experiments, the ten instances executed never failed. Thus, a 100% reliabliity
value has been registered for each workflow instance. During the instance executions, the
reliability estimates given by the SWR algorithm slowly converge to 100%. Nevertheless,
it is expected that as the workflow system executes more instances, the reliability of the
DNA Sequencing workflow will decrease.
For all the QoS dimensions, the degree of convergence of the SWR algorithm is
directly dependent on the weights that have been set for the re-computation of the QoS
dimensions (see Table 2-1 for the weights used in the DNA Sequencing workflow). A
higher weight associated with the multi-workflow function implies a faster convergence
when the SWR algorithm is applied. The same principal applies to the instance workflow
function.
2.8 RELATED WORK
The work found in the literature on quality of service for WfMS is limited. The
Crossflow project (Klingemann, Wäsch et al. 1999; Damen, Derks et al. 2000; Grefen,
Aberer et al. 2000) has made the major contribution. In their approach, a continuous-time
Markov chain (CTMC) is used to subsequently calculate the time and the cost associated
with workflow executions. While the research on quality of service for WfMS is limited,
the research on time management, which is under the umbrella of workflow QoS, has
been more active and productive. Eder et al. (1999) and Pozewaunig et al. (1997) present
an extension of CMP and PERT by annotating workflow graphs with time, in order to
check the validity of time constraints at process build-time and instantiation-time, and to
take pre-emptive actions at run-time. The major limitation of their approach is that only
75
directed acyclic graphs (DAG) can be modeled. This is a significant limitaiton since the
many workflows have cyclic graphs. Cycles are, in general, used to represent re-work
actions or repetitive activities within a workflow. Our approach deals with acyclic
workflows as well as with cyclic workflows. Our experience on modeling real-world
applications has shown that a significant number of workflows have cyclic graphs.
Dadam et al. (Reichert and Dadam 1998; 2000) also recognize that time is an important
aspect of workflow execution. With each workflow task, minimal and maximal durations
may be specified. The system supports the specification and monitoring of deadlines. The
monitoring system notifies users when deadlines are going to be missed. It also checks if
minimal and maximal time distances between tasks are followed according to initial
specifications. Marjanovic and Orlowska (1999) describe a workflow model enriched
with modeling constructs and algorithms for checking the consistency of workflow
temporal constraints. Their work mainly focuses on how to manage workflow changes,
while accounting for temporal constraints. Son et al. (2001) present a solution for the
deadline allocation problem based on queuing networks. Their work also uses graph
reduction techniques, but these are applied to queuing theory. Studies on workflow
reliability can also be found in the literature. The research is mainly concentrated on
system implementation issues. In (Kamath, Alonso et al. 1996) the authors propose an
architecture to enhance workflow systems’ reliability via replication. Different reliability
levels for different categories of process instances are used. Tang and Veijalainen( 1999)
propose the use of a fragmentation technique to provide higher reliability, without using a
replication-based solution. Wheater and Shrivastava (1998) describe a workflow system
that relies on a middleware infrastructure to provide a fault-tolerant execution
environment, enhancing system and applications reliability .
Although the work on quality of service for workflows is lacking, a significant
amount of research has been done in the areas of networking (Cruz 1995; Georgiadis,
Guerin et al. 1996), real-time applications (Clark, Shenker et al. 1992) and middleware
76
(Zinky, Bakken et al. 1997; Frolund and Koistinen 1998; Hiltunen, Schlichting et al.
2000).
Recently, in the area of Web services, researchers have also manifested an interest in
QoS. The DAML-S (Ankolekar, Burstein et al. 2001; DAML-S 2001) specification
allows the semantic description of business processes. The specification includes
constructs which specify quality of service parameters, such as quality guarantees, quality
rating, and degree of quality. One current limitation of DAML-S’ QoS model is that
every process needs to have QoS metrics specified by the user.
2.9 FUTURE WORK
The workflow QoS model presented in this paper can be extended in two additional
dimensions which are useful for workflow systems with stronger requirements. The first
dimension is maintainability. Maintainability corresponds to the mean time necessary to
repair workflow failures; it is the average time spent to maintain workflows in a condition
where they can perform their intended function. Maintenance actions mainly involve the
correction of failures during workflow execution. Workflow systems record the period of
time necessary for a faulty task to be repaired. The time spent to repair a workflow
component depends on the type of error that has occurred. Reparative actions can be as
simple as restarting a workflow scheduler that has crashed (Kochut, Sheth et al. 1999), or
they can be more complex, involving the installation of an ORB infrastructure in a new
machine to transfer workflow schedulers, for example. To increase maintainability,
advanced mechanisms have been developed to allow workflow systems to automatically
recover from errors. Luo et al. (2000) describe the architecture and implementation of an
exception-handling mechanism. The system detects and propagates exceptions which
occur during instances execution to an exception-handling module. The system, based on
case-based reasoning theory, derives exception handlers to repair damaged workflows
77
(Luo, Sheth et al. 1998). The system has the ability to adapt itself over time. The
knowledge acquired in past experiences is used in the resolution of new problems.
The second dimension that can be included is the trust dimension. The use of
workflow systems to coordinate and manage Web services compels the development of
techniques to appraise the global security level of workflows specifications. Workfolw
systems and applications face several security problems, and dedicated mechanisms are
needed to increase the level of security. Major problems include the distributed nature of
WfMSs, the use of non-secure networks (i.e., the Internet), the use of Web servers to
access workflow systems data, and the potential multi-organizational span of workflows.
Systems security level is assessed through the existence of security mechanisms (such as
authentication, access control, labels, audits, system integrity, security policy, etc.) and
through the use of development techniques (such as formal specifications, formal proofs,
tests, etc.). The importance of developing secure workflow systems has been recognized,
and prototypes combining workflow and security technology have already been
developed. We have extended workflow technology with the implementation of two
security modules. The first one (Miller, Fan et al. 1999) and (Fan 1999) describes a
workflow security architecture which targets the five security services (authentication,
access control, data confidentiality, data integrity, and non repudiation) recommended by
the International Standards Organization for network-based information systems. The
second one (Kang, Froscher et al. 1999) describes a multilevel secure (MLS) workflow
system to enable distributed users and workflow applications to cooperate across
classification levels. MLS workflow systems allow users to program multilevel mission
logic, to securely coordinate distributed tasks, and to monitor the progress of the
workflow across classification levels .
The functions used to compute the QoS dimensions at runtime (Table 2-6) have their
terms weighted. The user is responsible for setting the weights (wi, wi , wi , and wi ).
1 2 3 4
These weights remain constant as the workflow system registers new workflow
78
executions. Additional research would be useful to analyze the effect of substituting the
constant weights with variable weights. The idea would be to allow the workflow system
to automatically change the weights based on the number of workflow executions. As
more instances are registered for a workflow w, the weights specified for the Designer
and Multi-Workflow functions can be decreased and the weight associated with the
Workflow function increased. This corresponds with the belief that over time the QoS
metrics of the instances of the workflow w will give more accurate and fresh data to be
used with the SWR algorithm. The use of Bayesian estimates( Bernardo and Smith 1994)
are one of the solutions that can be investigated to enable the automatic adjustments of
the weights.
2.10 CONCLUSIONS
New trading models, such as e-commerce, bring a new set of challenges and
requirements that need to be explored and answered. Many E-commerce applications
composed of Web services forming workflows, which in turns represent an abstraction of
cross-organizational business processes. The use of workflows and workflow systems to
conduct and coordinate businesses in a heterogeneous and distributed environment has an
immediate operational requirement: the management of workflow quality of service. The
composition of Web services, and therefore workflows, cannot be undertaken while
ignoring the importance of quality of service measurements. Trading agreements between
suppliers and customers include the specification of QoS items such as products or
services to be delivered, deadlines, quality of products, and cost of service. The correct
management of such Quality of Service (QoS) specifications directly impacts the success
of organizations participating in e-commerce and also directly impacts the success and
evolution of e-commerce itself.
In this paper, as a starting point, we show the importance of quality of service
management for workflow and workflow systems. Based on our experience with the
79
development of workflow applications for various domains and with emergent workflow
requirements, we present a QoS model. This model allows for the description of
workflow components from a quality of service perspective; it includes four dimensions:
time, cost, reliability, and fidelity. The use of QoS increases the added value of workflow