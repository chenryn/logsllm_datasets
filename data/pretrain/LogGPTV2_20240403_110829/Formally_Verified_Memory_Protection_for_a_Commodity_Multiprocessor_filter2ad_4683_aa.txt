title:Formally Verified Memory Protection for a Commodity Multiprocessor
Hypervisor
author:Shih-Wei Li and
Xupeng Li and
Ronghui Gu and
Jason Nieh and
John Zhuang Hui
Formally Verified Memory Protection for a 
Commodity Multiprocessor Hypervisor
Shih-Wei Li, Xupeng Li, Ronghui Gu, Jason Nieh, and 
John Zhuang Hui, Columbia University
https://www.usenix.org/conference/usenixsecurity21/presentation/li-shih-wei
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Formally Veriﬁed Memory Protection
for a Commodity Multiprocessor Hypervisor
Shih-Wei Li Xupeng Li Ronghui Gu Jason Nieh John Zhuang Hui
Department of Computer Science
Columbia University
{shihwei,xupeng.li,rgu,nieh,j-hui}@cs.columbia.edu
Abstract
Hypervisors are widely deployed by cloud computing
providers to support virtual machines, but their growing
complexity poses a security risk, as large codebases contain
many vulnerabilities. We present SeKVM, a layered Linux
KVM hypervisor architecture that has been formally veriﬁed
on multiprocessor hardware. Using layers, we isolate KVM’s
trusted computing base into a small core such that only the
core needs to be veriﬁed to ensure KVM’s security guarantees.
Using layers, we model hardware features at different levels
of abstraction tailored to each layer of software. Lower hyper-
visor layers that conﬁgure and control hardware are veriﬁed
using a novel machine model that includes multiprocessor
memory management hardware such as multi-level shared
page tables, tagged TLBs, and a coherent cache hierarchy
with cache bypass support. Higher hypervisor layers that
build on the lower layers are then veriﬁed using a more
abstract and simpliﬁed model, taking advantage of layer
encapsulation to reduce proof burden. Furthermore, layers
provide modularity to reduce veriﬁcation effort across multi-
ple implementation versions. We have retroﬁtted and veriﬁed
multiple versions of KVM on Arm multiprocessor hardware,
proving the correctness of the implementations and that they
contain no vulnerabilities that can affect KVM’s security
guarantees. Our work is the ﬁrst machine-checked proof
for a commodity hypervisor using multiprocessor memory
management hardware. SeKVM requires only modest KVM
modiﬁcations and incurs only modest performance overhead
versus unmodiﬁed KVM on real application workloads.
1 Introduction
Cloud computing providers rely on commodity hypervisors to
securely host and protect user applications and data in virtual
machines (VMs). However, commodity hypervisors are
complex pieces of software, in some cases integrated with an
entire host operating system (OS) kernel to leverage existing
kernel functionality. This complexity poses a signiﬁcant
security risk as more complex software has more bugs,
allowing attackers to exploit hypervisor vulnerabilities to
compromise VMs [14–18].
Theoretically, formal veriﬁcation offers a solution by
proving that a system is correctly implemented. However, pre-
viously veriﬁed systems, such as CertiKOS [33], seL4 [43,53],
Komodo [28], and Serval [54], were not veriﬁed using real-
istic hardware models that resemble what can be found in a
cloud computing setting. Most of them are limited to unipro-
cessor settings, and none of them model common hardware
features such as multi-level shared page tables, tagged TLBs,
or writeback caches. In other words, these veriﬁed implemen-
tations cannot be deployed to handle cloud applications and
workloads, and even if they could, their proofs may not hold
for hardware used in a cloud computing setting.
We present SeKVM, the ﬁrst hypervisor that has been
formally veriﬁed on multiprocessor hardware with shared
page tables, tagged TLBs, and writeback caches. This is made
possible by introducing a layered hypervisor architecture
and veriﬁcation methodology. We use layers in three ways.
First, we use layers to reduce the trusted computing base
(TCB) by splitting the hypervisor into two layers, a higher
layer consisting of a large set of untrusted hypervisor services
and a lower layer consisting of a small core that serves as
the hypervisor’s TCB. We build on our previous work on
HypSec [46] to retroﬁt the Linux KVM hypervisor in this
manner without compromising its functionality. Reducing the
hypervisor’s TCB reduces the amount of code that needs to be
trusted, thereby reducing code complexity and vulnerabilities.
Second, we use layers to modularize the implementation
and proof of the TCB. We structure the TCB’s implementa-
tion as a hierarchy of modules that build upon the hardware
and each other. Modularity enables us to decompose the ver-
iﬁcation of the TCB into simpler components that are easier
to prove. Once we prove that a lower layer module of the
implementation reﬁnes its speciﬁcation, we can then hide its
implementation details and rely on its abstract speciﬁcation
in proving the correctness of higher layer modules that rely
on the lower layer module. Furthermore, we can prove the
USENIX Association
30th USENIX Security Symposium    3953
correctness of the lower layer module once and then rely on it
in proving higher layer modules instead of needing to verify
its implementation each time it is used by a higher layer mod-
ule. We leverage our previous work on security-preserving
layers [48] to provide a deep speciﬁcation of each layer of
the hypervisor implementation, and verify that the implemen-
tation reﬁnes a stack of layered speciﬁcations. Using layers
allows us to reduce the proof of a complex implementation
by composing a set of simpler proofs, one for each implemen-
tation module, reducing proof effort overall. As software is
updated, layers also help with proof maintainability, as only
the proofs for the implementation modules that change need
to be updated while the other proofs can remain the same.
Third, we use layers to modularize the model of the hard-
ware used for veriﬁcation. We introduce a layered hardware
model that is accurate enough to model multiprocessor
hardware features yet simple enough to be used to verify
real software by tailoring the complexity of the hardware
model to the software using it. Lower layers of the hypervisor
tend to provide simpler, hardware-dependent functions that
conﬁgure and control hardware features. We verify these
layers using all the various hardware features provided by the
machine model, allowing us to verify low-level operations
such as TLB shootdown. Higher layers of the hypervisor
tend to provide complex, higher-level functions that are less
hardware dependent. We verify these layers using simpler,
more abstract machine models that hide lower-level hardware
details not used by the software at higher layers, reducing
proof burden for the more complex parts of the software.
We extend our layered veriﬁcation approach to construct an
appropriately abstract machine model for each respective
layer of software. This allows us to verify the correctness
of the multiprocessor hypervisor TCB while accounting for
and taking advantage of widely-used multiprocessor features,
including multi-level shared page tables, tagged TLBs, and
multi-level caches with cache bypass support.
We have implemented and veriﬁed a SeKVM prototype
by retroﬁtting KVM on Armv8 multiprocessor hard-
ware [19, 23–25]. The implementation requires only modest
modiﬁcations to Linux and has a TCB of only a few thousand
lines of code, yet retains KVM’s full-featured commodity
hypervisor functionality, including multiprocessor, full device
I/O, multi-VM, VM management, and broad Arm hardware
support. SeKVM improves KVM security by verifying the
correctness of its TCB and the security guarantees of the en-
tire hypervisor. Our veriﬁcation also accounts for multi-level
shared page tables, tagged TLBs, and multi-level caches.
Furthermore, the veriﬁcation has been done for multiple
versions of KVM, speciﬁcally those in versions v4.18 and
v5.4 of the Linux kernel. Both the machine model and the
proofs that build upon it were formalized using the Coq proof
assistant [3]. We show that SeKVM provides its strong secu-
rity while providing similar performance to unmodiﬁed KVM,
with only modest overhead for real application workloads
Figure 1: SeKVM Design
and similar scalability when running multiple VMs.
Although SeKVM shares the same security properties as
HypSec, both the correctness of SeKVM’s TCB implemen-
tation and its security guarantees are formally veriﬁed. While
HypSec’s TCB may contain vulnerabilities that compromise
its security properties, we have proven that SeKVM’s TCB
contains no vulnerabilities. Furthermore, while HypSec
is designed to provide security properties to ensure VM
conﬁdentiality and integrity, SeKVM has been proven
to guarantee those security properties on multiprocessor
hardware. Our work is the ﬁrst, machine-checked correctness
proof of the TCB of a commodity hypervisor on a realistic
hardware model with shared page tables, tagged TLBs, and
writeback caches, and the ﬁrst, machine-checked security
proof of a commodity hypervisor using multiprocessor
memory management hardware.
2 Threat Model and Assumptions
Our threat model is primarily concerned with hypervisor
vulnerabilities that may be exploited to compromise a VM’s
private data. For each VM we are trying to protect, an attacker
may control other VMs and exploit any hypervisor vulnerabil-
ities. We protect each VM from attacks by other compromised
VMs, but do not protect VMs that voluntarily reveal their own
private data. Attackers may control peripherals to perform
malicious memory accesses via DMA [61]. Side-channel
attacks [6,38,51,55,71,72] are beyond the scope of the paper.
We assume a secure persistent storage to store keys. We
assume the hardware is bug-free and the system is initially
benign, allowing signatures and keys to be securely stored
before the system is compromised. We trust the machine
model, compiler, and Coq.
3954    30th USENIX Security Symposium
USENIX Association
VMKcoreVMProtectionLowvisorKservVM KernelLinuxKVMHighvisorVMUserQEMUEL2EL1EL0Host User3 SeKVM Design
SeKVM uses HypSec’s design to retroﬁt the Linux KVM
hypervisor, reducing its TCB while protecting the conﬁden-
tiality and integrity of VMs. As shown in Figure 1, we split
KVM into two layers, a small trusted and privileged KCore
that is the TCB with full access to VM data, and an untrusted
and deprivileged KServ delegated with most hypervisor
functionality including the Linux kernel integrated with
KVM. The result is a hypervisor with a signiﬁcantly smaller
TCB that still supports KVM’s rich hypervisor features.
KCore is kept small by only performing VM data access
control, including saving and restoring CPU register state and
page table management to limit access to a VM’s CPU state
and memory to only KCore and the VM itself. Other hyper-
visor functionality, including I/O and interrupt virtualization
and resource management such as CPU scheduling and mem-
ory allocation, are delegated to KServ. SeKVM leverages
hardware virtualization support to enforce this separation.
SeKVM runs KCore at a higher privilege CPU mode designed
for running hypervisors, giving it full control of hardware, in-
cluding virtualization hardware mechanisms such as nested
page tables (NPTs) [8]; KServ runs at a lower privilege mode.
KCore conﬁgures virtualization hardware to enforce its
access control. KCore enables NPTs for KServ and VMs
so that they do not have direct access to physical memory.
KCore can limit KServ’s or a VM’s access to pages of physical
memory by unmapping those pages from the respective NPT.
KCore ensures its own memory is not mapped into any of
the NPTs, protecting its memory by making it inaccessible to
KServ and other VMs. KCore also uses NPTs to make each
VM’s memory inaccessible to KServ and other VMs.
KCore interposes on all VM transitions, namely exiting or
entering a VM. When a VM exits, KCore saves the VM’s
execution context from CPU hardware registers to its pri-
vate memory, then restores KServ’s execution context to the
hardware before switching to KServ. KServ therefore can-
not access a VM’s CPU state from the hardware or memory,
which the state is saved in KCore memory inaccessible to
KServ. Since KServ must run to switch a CPU from running
one VM to another, a VM’s CPU state is also not accessible
by any other VM. A compromised KServ or VM can neither
control hardware virtualization mechanisms nor access KCore
memory and thus cannot disable SeKVM.
Speciﬁcally, SeKVM uses Arm Virtualization Extensions
(VE) to run KCore in hypervisor (EL2) mode while KServ
runs in a less privileged kernel (EL1) mode. VM operations
that need hypervisor intervention trap to EL2 and run KCore.
KCore either handles the trap directly to protect VM data or
world switches the hardware to EL1 to run KServ if more
complex handling is necessary, KCore context switches to
KServ. When KServ ﬁnishes its work, it makes a hypercall
to trap to EL2 so KCore can securely restore the VM state
to hardware. KCore interposes on every switch between the
VM and KServ, thus protecting the VM’s execution context.
SeKVM ensures that KServ cannot invoke arbitrary KCore
functions via hypercalls.
KCore leverages Arm VE’s stage 2 memory translation sup-
port, Arm’s NPTs, to virtualize both KServ and VM memory.
Stage 2 page tables translate from guest physical addresses
(gPAs) in a VM to the actual physical memory addresses on
the host (PAs). Free physical memory is mapped into KServ’s
stage 2 page tables so KServ can allocate it to VMs. Once it
is allocated to a VM, KCore maps the memory into the VM’s
stage 2 page tables and unmaps the memory from KServ’s
stage 2 page tables to make the physical memory inaccessible
to KServ. KCore routes stage 2 page faults to EL2 and rejects
illegal KServ and VM memory accesses. KCore allocates
KServ’s and VMs’ stage 2 page tables from its own protected
physical memory and manages the page tables, preventing
KServ from accessing them. When a VM is terminated and is
done with its allocated memory, KCore scrubs the memory
before mapping it back into KServ’s stage 2 page tables as
free memory which can be allocated again to another VM.
Further details are described in [46].
SeKVM by default ensures that KServ has no access to
any VM memory. However, a VM may want to share its
memory with KServ in some cases. For example, a VM
may encrypt its data for use with paravirtualized I/O, in
which a memory region owned by the VM has to be shared
with KServ for communication and efﬁcient data copying
since KServ handles paravirtualized I/O. SeKVM provides
GRANT_MEM and REVOKE_MEM hypercalls which a guest OS can
use to share its memory with KServ. The VM passes the start
of a guest physical frame number, the size of the memory
region, and the speciﬁed access permission to KCore via
the hypercalls. KCore enforces the access control policy by
controlling the memory region’s mapping in stage 2 page