and are intertwined. This leads to low repetitiveness of the relative ratio between normal target-system and ”abnormal”
same log sequences. Therefore, the state-of-the-art methods logsneededforfinetuning.Forthequalitativeproperty,wevar-
(LogRobust, DeepLog and CNN), which directly model the iedtheoriginoftheanomalousclass(class1),i.e.,ifitcomes
sequences face the challenge of unstable sequences. Line from human-provided labels from other software systems
(externalsystemlabels)orthe”abnormal”classoftheSLdata tive of target-system log labels availability, the methods are
(GitHubLabels).Theexternalsystemlabelsareobtainedfrom categorized into supervised and unsupervised. The supervised
publicly available datasets (e.g., TBIRD and SPIRIT [26] – methodsassumetheexistenceoflabelsfromthetargetsystem
twosupercomputerdatasetswithlabeledanomalies).Similarly, when learning a model. In one of the earliest applications of
as the adoption of the ”abnormal” class in finetuning, the these methods, Bodik et al. [27] applied Logistic Regression
external-systemlabelsdenoteanomalousconceptsfromrelated (LR) to successfully detect anomalies in data centres, by
systems (thus eliminating the requirement for labeling), and treating the problem as a binary classification. Decision Trees
can be used for model finetuning (in place of the ”abnormal” (DT) [9] were utilized in the detection of anomalous web
GitHub class). Fig. 2 depicts the experimental results for requests from access logs. These two methods start by log
quantitative evaluation when incrementally changing the ratio parsing to extract events and then use count vectors in a fixed
between the anomalous and normal logs in the training data time interval as input samples. The recent advances in deep
for BGL-sin in the ranges {1%,5%,10%,20%}. They show learningresultedintheappearanceofseveralsuperviseddeep-
that with a small ratio of 1 %, ADLILog achieves >95% of learning-based methods, e.g., LogRobust [11], and CNN [12].
the optimal performance on the F metric. The improvement LogRobust uses the LSTM architecture, augmented with at-
1
is consistent with further increasing the ratio. Concerning tention. These two are popular deep learning architectures
the qualitative evaluation, by varying the two sets of labels, frequently combined for sequence modeling. LogRobust, as
the results show that the GitHub labels provide robust and input, receives a sequence of events, and as output, it predicts
better performance because of the greater semantic variety in if the observed sequence is anomalous or not. By careful
the ”abnormal” events over the external system labels. This sequenceconstruction,i.e.,byincrementalslidingoverthelog
experiment demonstrates that a small number of labels from sequences by one element, it can be used to predict single log
the ”abnormal” class of the SL data have significant practical lines [8]. An additional feature of LogRobust is using vector
usage for log anomaly detection. embeddings from general-purpose languages to represent the
logs.Luetal.[12]useConvolutionsNeuralNetworks(CNN),
another type of deep learning architecture, to learn normal
Model size: 16 Model size: 64 Model size: 256
batch size and abnormal sequences from template indices. Similar to
0.6 0.6 32 0.6
LogRobust, CNN can be used to detect anomalies from a sin-
64
256 gle log. While having strong detection performance, the large
0.5 0.5 512 0.5
frequencyofthesoftwareupdatesandthelargevolumeofthe
producedlogsmakethelabelingprocessexpensive.Therefore, erocs
0.4 0.4 0.4
supervised methods are frequently considered impractical [6].
1F
In contrast, the unsupervised methods do not assume the
0.3 0.3 0.3
existence of labeled data. This has an important practical im-
0.2 0.2 0.2 plicationbecauseiteliminatestheneedforexpensivelabeling.
Therefore, the unsupervised anomaly detection methods are
0.1 0.1 0.1 easier to adopt. In one of the earliest works on unsupervised
1.0 1.5 2.0 2.5 1.0 1.5 2.0 2.5 1.0 1.5 2.0 2.5
log10 training time log10 training time log10 training time anomaly detection, Xu et al. apply PCA [10] to learn the
normal state of the event counts by projecting them as points
Fig.3. Sensitivityanalysisoftheinfluenceofbatchandmodelsize
in a vector space. In the test phase, the test sample is
Finally, the correct parameter setting influences the needed projected in the constructed vector space and reported as
effort for fast configuration and the quality of the detection. an anomaly if the projection significantly deviates from the
To evaluate the impact of the hyperparameters over the learnednormalstate.Linetal.[4]introduceLogClusterwhich
detection performance and efficiency, we examined two uses the TFIDF algorithm for sequence representation. It first
hyperparameters,modelandbatchsizes,influencingthemodel constructs a knowledge base of normal/anomalous sequence
performance and update time. We considered the BGL-sin clusters by agglomerative clustering and human-based cluster
dataset. The experimental results when varying the model labeling(normaloranomalous).Atestsampleisdetectedasan
size in the range {16,64,256} and batch size in the range anomalyifitisclusteredintoanomalousclusters.DeepLog[8]
{32,64,256,512}, reported in Fig. 3, show that the larger and LogAnomaly [29] are two popular unsupervised deep
batch size and smaller model size provide better detection learning-based methods. The innovative feature of the two
performances while being faster for updating. The prediction is the introduction of an auxiliary task called ”next event
timeperbatchsizeof512is17ms(∼30000logspersecond). prediction” (NEP). NEP is a supervised task that given a
Together with the small model size, these experiments imply sequence of events, forecasts the most probable next event.
that ADLILog has desirable practical properties. Notably, the labels originate from the input data itself, i.e., no
labelingisperformed,whichmakesthemethodsunsupervised.
V. RELATEDWORK
The test sequences with an incorrect prediction for the next
There exist multiple methods for the automation of log- event are considered anomalous. As stated by the authors,
basedanomalydetection[8],[9],[11],[27].Fromtheperspec- DeepLog can be applied for sequential and single logs given
as inputs. To learn the normal state, an LSTM architecture [6] S. He, J. Zhu, P. He, and M. R. Lyu, “Experience report: System
is trained on the NEP task. LogAnomaly has two additional log analysis for anomaly detection,” in 2016 IEEE 27th International
Symposium on Software Reliability Engineering (ISSRE). New York,
features: 1) it uses log semantics and 2) event counts in joint
USA:IEEE,2016,pp.207–218.
training a DeepLog-like model. The empirical results show [7] Z.Chen,J.Liu,W.Gu,Y.Su,andM.R.Lyu,“Experiencereport:Deep
that the improvements over DeepLog are not significant [7], learning-basedsystemloganalysisforanomalydetection,”CoRR,vol.
2107.05908,2021.
[29].Unsupervisedmethodsareoftencriticizedfortheirlower
[8] M.Du,F.Li,G.Zheng,andV.Srikumar,“Deeplog:Anomalydetection
performance in comparison to the supervised ones [6] leading anddiagnosisfromsystemlogsthroughdeeplearning,”inProceedings
to alarm fatigue and discouraging their wide applicability. of the 2017 ACM SIGSAC. New York, NY, USA: Association for
ComputingMachinery,2017,p.1285–1298.
In addition, there are other methods for log-based anomaly
[9] M.Chen,A.Zheng,J.Lloyd,M.Jordan,andE.Brewer,“Failurediag-
detection in both industry and research [13], [14], [30]. How- nosis using decision trees,” in International Conference on Autonomic
ever, those solutions are part of production systems [13] or Computing,2004.Proceedings.,2004,pp.36–43.
have specific implementation challenges while do not provide [10] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting
large-scalesystemproblemsbyminingconsolelogs,”inProceedingsof
public implementations [14], [30], [31]. Due to the inability
theACM22ndSOSP. NewYork,NY,USA:AssociationforComputing
of transparent comparison, we do not discuss them in detail. Machinery,2009,p.117–132.
[11] X.Zhangandet.al.,“Robustlog-basedanomalydetectiononunstable
log data,” in Proceedings of the 2019 27th ACM Joint Meeting on
VI. CONCLUSION ESEC/FSE. New York, NY, USA: Association for Computing Ma-
chinery,2019,p.807–817.
This paper addresses the problem of automating log-based [12] S. Lu, X. Wei, Y. Li, and L. Wang, “Detecting anomaly in big data
anomaly detection as a crucial maintenance task in enhancing systemlogsusingconvolutionalneuralnetwork,”inIEEE16thConfon
Dependable,AutonomicandSecureComputing,2018,pp.151–158.
thereliabilityofITsystems.Itintroducesanovelunsupervised
[13] T. Li, Y. Jiang, C. Zeng, B. Xia, Z. Liu, W. Zhou, X. Zhu, W. Wang,
method for log anomaly detection, named ADLILog. The key L.Zhang,J.Wu,L.Xue,andD.Bao,“Flap:Anend-to-endeventlog
idea of ADLILog is to use the large unstructured information analysisplatformforsystemmanagement,”inProceedingsofthe23rd
ACM SIGKDD International Conference on KDD. New York, NY,
from the logging instructions of 1000+ GitHub public code
USA:AssociationforComputingMachinery,2017,p.1547–1556.
projects to improve the target-system log representations, [14] X.Li,P.Chen,L.Jing,Z.He,andG.Yu,“Swisslog:Robustandunified
whichdirectlyimprovesanomalydetection.Wefirstconducted deeplearningbasedloganomalydetectionfordiversefaults,”in2020
IEEE31stISSRE,2020,pp.92–103.
a study to examine the language properties of the log in-
[15] T.-H. Chen, S. W. Thomas, and A. E. Hassan, “A survey on the use
structions,andweshowthattheyencoderichanomaly-related of topic models when mining software repositories,” Empirical Softw.
information.ADLILogcombinestheanomaly-relatedinforma- Engg.,vol.21,p.1843–1919,2016.
tion and the target-system data to learn a deep neural network [16] M. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd.
(2020) spaCy: Industrial-strength Natural Language Processing in
model by a sequential two-phase learning procedure. The
Python. Explosion.ai. [Online]. Available: https://doi.org/10.5281/
extensiveexperimentalresultsonthetwomostcommonlyused zenodo.1212303
benchmark datasets show that ADLILog outperforms the re- [17] P. He, Z. Chen, S. He, and M. R. Lyu, “Characterizing the natural
language descriptions in software logging statements,” in Proceedings
latedmethods:thesupervisedby5-24%,andtheunsupervised
ofthe33rdACM/IEEEInternationalConferenceonAutomatedSoftware
by 40-63% on the F 1 score. Further experiments demonstrate Engineering. New York, NY, USA: Association for Computing
thatADLILoghasdesirablepracticalpropertiesconcerningthe Machinery,2018,p.178–189.
[18] T.M.CoverandJ.A.Thomas,ElementsofInformationTheory. USA:
time-efficientmodelupdatesandsmallmodelsizes.Thisstudy
Wiley-Interscience,2006.
signifies the benefit of using large unstructured information [19] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu,
in aiding the automation of IT operations. Regarding future “Tools and benchmarks for automated log parsing,” in Proceedings of
the 41st International Conference on Software Engineering: Software
work, the paper opens additional questions on how to apply
EngineeringinPractice. NY,USA:IEEEPress,2019,p.121–130.
the SL data for automating higher-order IT operational tasks,
[20] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
like failure identification and root-cause analysis. u.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”inAdvancesin
NeuralInformationProcessingSystems. RedHook,NY,USA:Curran
Associates,2017,p.6000–6010.
REFERENCES [21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
trainingofdeepbidirectionaltransformersforlanguageunderstanding,”
[1] P.Notaro,J.Cardoso,andM.Gerndt,“Asurveyofaiopsmethodsfor inProceedingsofthe2019ConferenceoftheNorthAmericanChapterof
failuremanagement,”ACMTrans.Intell.Syst.Technol.,vol.12,2021. theAssociationforComputationalLinguistics. Minneapolis,Minnesota:
[2] W. Meng, Y. Liu, S. Zhang, F. Zaiter, Y. Zhang, Y. Huang, Z. Yu, AssociationforComputationalLinguistics,2019,pp.4171–4186.
Y. Zhang, L. Song, M. Zhang, and D. Pei, “Logclass: Anomalous log [22] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT
identificationandclassificationwithpartiallabels,”IEEETrans.Netw., Press,2016.[Online].Available:http://www.deeplearningbook.org
vol.18,pp.1870–1884,2021. [23] W. Liu, Y.-M. Zhang, X. Li, Z. Yu, B. Dai, T. Zhao, and L. Song,
[3] H.Li,W.Shang,B.Adams,M.Sayagh,andA.E.Hassan,“Aqualitative “Deephypersphericallearning,”inProceedingsofthe31stInternational
studyofthebenefitsandcostsofloggingfromdevelopers’perspectives,” ConferenceonNeurIPS. RedHook,NY,USA:CurranAssociatesInc.,
IEEETransactionsonSoftwareEngineering,pp.1–17,2020. 2017,p.3953–3963.
[4] Q. Lin, H. Zhang, J.-G. Lou, Y. Zhang, and X. Chen, “Log clustering [24] L.Ruff,J.R.Kauffmann,R.A.Vandermeulen,G.Montavon,W.Samek,
basedproblemidentificationforonlineservicesystems,”inProceedings M. Kloft, T. G. Dietterich, and K.-R. Mu¨ller, “A unifying review of
of the 38th ICSE. New York, NY, USA: Association for Computing deep and shallow anomaly detection,” Proceedings of the IEEE, vol.
Machinery,2016,p.102–111. 109,no.5,pp.756–795,2021.
[5] S. He, P. He, Z. Chen, T. Yang, Y. Su, and M. R. Lyu, “A survey on [25] P.He,J.Zhu,Z.Zheng,andM.R.Lyu,“Drain:Anonlinelogparsing
automatedloganalysisforreliabilityengineering,”ACMComput.Surv., approachwithfixeddepthtree,”in2017IEEEInternationalConference
vol.54,2021. onWebServices. NY,USA:CurranAssociates,2017,pp.33–40.
[26] A. Oliner and J. Stearley, “What supercomputers say: A study of five
system logs,” in 37th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks. Los Alamitos, CA, USA: IEEE
ComputerSociety,2007,pp.575–584.
[27] P. Bodik, M. Goldszmidt, A. Fox, D. B. Woodard, and H. Andersen,
“Fingerprintingthedatacenter:Automatedclassificationofperformance
crises,” in Proceedings of the 5th European Conference on Computer
Systems. NewYork,NY,USA:ACM,2010,p.111–124.
[28] D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization,”
in3rdInternationalConferenceonLearningRepresentations,2015.
[29] W.Mengandetal.,“Loganomaly:Unsuperviseddetectionofsequential
andquantitativeanomaliesinunstructuredlogs,”inProceedingsofthe
28,IJCAI-19. IJCAI,2019,pp.4739–4745.
[30] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao,
“Self-attentive classification-based anomaly detection in unstructured
logs,”CoRR,2020.[Online].Available:https://arxiv.org/abs/2008.09340
[31] Y.Zhu,W.Meng,Y.Liu,S.Zhang,T.Han,S.Tao,andD.Pei,“Unilog:
Deployonemodelandspecializeitforallloganalysistasks,”2021.