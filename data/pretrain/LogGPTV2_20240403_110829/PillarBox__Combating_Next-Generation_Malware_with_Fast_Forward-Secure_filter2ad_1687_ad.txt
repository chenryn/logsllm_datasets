key (ri, r
(cid:2)
updates the secret key. Then, given secret writing key (ri, r
j , i, j) and a buffer C, Wrap
securely encapsulates C to ˆC and updates the secret key. Finally, given secret reading
(cid:2)
j , i, j) and an encapsulated buffer ˆC, Read decrypts the buffer and all of its
key (ri, r
contents returning a set of T messages and updates the secret key.
For simplicity, we here consider a ﬁxed-size PBB that holds ﬁxed-size messages
(parameters T and g((cid:5)) respectively). Note that PillarBox can be easily extended to
handle variable-length messages and to dynamically enlarge the PBB buffer, as needed,
in order to prevent loss of alert messages (due to overwriting) during prolonged PBB-
transmission failures; we omit these extensions due to space limitations.
5 Experimental Evaluation
We developed a prototype of PillarBox in C++. To implement authenticated encryption
we utilize an open-source version of EAX-mode encryption. We also implemented a
custom FS-PRNG as a hash chain for generating the necessary cryptographic keys (for
both low- and high-layer secure processing of messages and buffers).
We next experimentally validate the effectiveness of PillarBox in securing alerts dur-
ing the critical window. We ﬁrst demonstrate the merits of our alert-buffering approach
via a generic attack against alert-pushing methods. We then show that PillarBox is fast
enough to win the race condition against an attacker trying to disrupt the securing of
alert messages. Surprisingly, even when an attacker already has the privilege necessary
to kill PillarBox, the execution of the kill command itself can be secured in the PillarBox
buffer before the application dies. Finally, we validate the feasibility of PillarBox as a
practical alert-relaying tool.
5.1 Demonstrating Direct-Send Vulnerability
We motivate the need for securing the chain of custody in SASs and justify our design
choice of host-side buffering, rather than immediately putting alerts on the wire, by
showing the feasibility of an attacker intercepting on-the-wire host alert transmissions
silently (without sender/receiver detection) in a rather simple setting.
Using the Ettercap tool [1] we inserted an attack machine (attacker) as a man-in-
the-middle between our client and server communicating over a switch. The attacker
performed ARP spooﬁng against the switch, to which most non-military-grade hubs and
switches are vulnerable. Because it attacked the switch, neither endpoint observed the
attack. Once inserted between the two machines, our attacker was able to drop or rewrite
undesired packets on the ﬂy. Even if the client and server had been communicating over
a secured channel (a rarity in current practice), alert messages could still easily have
been dropped, preventing any indication of the attack from reaching the server.
60
K.D. Bowers et al.
If executed within a subnet, the attack described here would rarely be detected, even
by a forensic tool performing network packet capture, as these tools are typically de-
ployed to monitor only inbound/outbound trafﬁc, or at best across subnets.
Given the ease with which we were able to not only prevent communication between
a client and server, but moreover modify what the server received, without detection, it
should be clear just how important chain-of-custody is in SASs. If the messages being
transmitted are of any value, then they need to be protected. Otherwise an attacker can
simply block or modify all SAS communication while attacking a host, after which he
can turn off or otherwise modify what the SAS sends from the client side. Attacking in
such a way makes it impossible for the server to detect anything has gone wrong and
motivates our desire to provide a better way to secure log messages.
5.2 Race-Condition Experiments
We now show that it is feasible for a SAS combined with PillarBox to detect an attack in
progress and secure an alert before an attacker can disrupt PillarBox operation (i.e., that
the critical window is non-zero in size). PillarBox depends on both an alerter (in our
case, syslog), and a named pipe used to communicate from the alerter to the bufferer.
Both of these components, as well as PillarBox itself, can be attacked, creating a race
condition with the attacker. If any of the components can be shut down fast enough
during an attack, alerts may not be secured in the PBB. Surprisingly, we show that even
an attacker with the necessary (root) privilege rarely wins this race (≈ 1% of the time).
To bias our experiments in favor of an attacker, we assume the attacker has gained
access to a privileged account that already has the necessary permissions to kill any of
the components. We record time required for the attacker to issue a single command to
kill the process and show that the command itself gets secured by PillarBox before the
targeted component is terminated. Our tests were performed on an 2.5GHz Intel Core 2
Duo T9300 processor with 4 GB of memory and Ubuntu 12.04 as the operating system.
Killing PillarBox. PillarBox is a simple application that is easily terminated by an at-
tacker, although it can be run as root to provide some protection. To be secured, alerts
must be generated, routed by syslog to the named pipe, and then picked up by PillarBox,
encrypted and added to the buffer. An attacker’s best bet at disrupting the securing of
alerts is to try and shutdown PillarBox itself. If run as root, PillarBox can be terminated
by invoking root privilege and issuing a kill command.12 Calling kill with the −9 signal
immediately terminates any program, unless it is in the process of making a system
call; it then terminates when the system call returns. Using sudo runs the command as
root, but also generates an alert message which syslog picks up. The full one-line com-
mand sudo kill − 9  immediately terminates PillarBox, but usually
not before a log event is created, routed by syslog through the named pipe, and secured.
As Table 1 shows, in the majority of runs the alert message is locked away in ≈
4ms.13 Alert messages are, on average, secured in PillarBox before it is killed with
12 kill or pkill could be used to terminate the process: pkill takes in the process name, while kill
takes a process id; otherwise they operate the same.
13 PillarBox accounts for only a minuscule fraction of this total time.
PillarBox: Combating Next-Generation Malware
61
Table 1. Average time from the start of a command until log is secured in PillarBox and total
time for command completion
Command
Syslog (typical) 4.09ms 0.30ms
Syslog (worst)14 32.33ms 5.38ms
Named pipe 6.36ms 3.02ms
PillarBox 4.01ms 0.19ms
Secured Std. Dev. Disrupted Std. Dev.
sudo kill − 9 
2.43ms
sudo kill − 9 
2.81ms
3.30ms
sudo rm named pipe
0.37ms sudo kill − 9 
8.86ms
9.32ms
8.99ms
6.95ms
almost 3ms to spare.14 However, in about 1% of our experiments, PillarBox was killed
before receiving the alert message and encrypting it. All of the commands in Table 1
were run 100 times with averages and standard deviations shown.
Impacts of System Load. To further test the ability of the attacker to beat PillarBox,
we also ran tests under varying amounts of disk, memory, and CPU load. Disk load
appeared to have little to no effect on either the success of PillarBox, or the timing
measurements. As expected, load on the system memory slowed everything down—
lengthening both the time to secure, but also the time until the kill completes—but did
not appear to impact the success of PillarBox winning the race condition. For unex-
plained reasons, CPU load did seem to impact PillarBox on our test machine. Oddly,
PillarBox did well (0% failure) at near 100% load, but relatively poorly (.15 Table 1 shows that the
log message is sent by syslog before it is killed. However, presumably due to process
scheduling, in several runs the kill command returns before the alert message is secured
14 Due to presumed OS scheduling interruptions, in about 1/3 of the runs the kill command
returns before the message is successfully secured in PillarBox. These results show the timings
observed in those cases.
15 The alerter could be more integrated into the kernel itself, making it even harder to intercept
and/or kill. In our case, syslog channels log messages generated by the kernel and doesn’t
actually generate them itself.
62
K.D. Bowers et al.
Table 2. Timeline of events
sudo cp /etc/rsyslog.d/vanish.conf /home/vanish.copy
related to the execution of
the attacker’s command
Event
Avg. Time (ms)
Std. Dev.
Start
0.00
N/A
Message Secured
Rule Deleted
Copy Fails
4.00ms
0.44ms
4.04ms
0.44ms
7.21ms
0.81ms
in the PBB. Because the message always arrives in the PBB (again, there were no
failures), we assume these represent runs where the alert is passed to the named pipe
before syslog terminates and then read from the pipe when the PillarBox process is later
scheduled by the OS. This issue is diminished in the tests against the named pipe and
PillarBox, explaining their perceived lower average timings (and standard deviations).
Vanishing Rules. When PillarBox provides stealth, it is best combined with vanishing
SAS rules to prevent critical information leakage. Recall that if an attacker cannot pre-
vent PillarBox from securing events in the critical window, the attacker beneﬁts from at
least learning how the system is instrumented and what alerts were likely to have been
generated. In our test setup, the vanishing alerts generate an alert whenever a root user
logs in. To test the race condition, we instrumented PillarBox to delete the vanishing
alerts conﬁguration ﬁle after securing the alert message. The attacker attempts to create
a copy of the sensitive alerter conﬁguration ﬁle. As it is shown by the relative timing of
events over 100 test runs in Table 2, after securing the alert message, PillarBox always
successfully deletes the conﬁguration ﬁle at least 2.72 ms. before the attempted copy.
Privilege Escalation. Having shown that PillarBox can win the race conditions related
to securing alerts and causing them to vanish, even in the pessimistic case where the
attacker starts with the necessary permissions, we now consider the issue of privilege
escalation. The concern is that if the attacker exploits vulnerabilities the transition to
root privilege may not get logged. We assume that most privilege escalations could be
detected given the proper instrumentation and that disrupting any of the necessary com-
ponents in our system (e.g. corrupting its memory address space) without root privilege
is infeasible given current architectures (e.g., Address Space Randomization [21], etc.).
As an example of a common privilege escalation, we consider the “Full Nelson”
attack, which exploits CVE-2010-4258, CVE-2010-3849, and CVE-2010-3850 to gain
root access. We ﬁnd that this attack generates kernel messages that syslog can pick up
and pass through the named pipe and into the PBB before the exploit completes and the
attacker terminates essential SAS or PillarBox components or reads the conﬁguration
ﬁle. In fact, the attack includes a necessary sleep command that further beneﬁts timely
securing of alerts in PillarBox. Even in the most pessimistic case, in which the exploit
code uses the kill system call before ever launching a shell, and the sleep command is
removed (causing the exploit to fail), the log messages are still locked away in PBB
before the exploit program tries to disrupt PillarBox. Since the system must be restored
after the privilege escalation, we were not able to run 100 instances, but we repeatedly
demonstrated that the kernel log messages can be secured in PBB before being killed.
While the “Full Nelson” attack is representative of other local privilege escalation
attacks, this by no means guarantees that faster or quieter privilege escalations don’t
exist. What it does demonstrate is that the event signaling the end of the critical window
PillarBox: Combating Next-Generation Malware
63
s
t
r
e
A
l
 6000
 5000
 4000
 3000
 2000
 1000
 0
 1
Alerts generated over 7 hours by host
Alerts Generated
 10
 100
 1000  10000  100000
Host
Fig. 6. Host-generated alerts over 7h
Throughput as a Function of Buffer Size 
and Sending Frequency 
Messages Buffer will Hold 
1024 
4096 
16384 
2048 
8192 
32768 
)
s
m