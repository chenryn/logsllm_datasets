title:Modeling Soft-Error Propagation in Programs
author:Guanpeng Li and
Karthik Pattabiraman and
Siva Kumar Sastry Hari and
Michael B. Sullivan and
Timothy Tsai
Modeling Soft-Error Propagation in Programs
Guanpeng Li, Karthik Pattabiraman
Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai
University of British Columbia
{gpli, karthikp}@ece.ubc.ca
NVIDIA
{shari, misullivan, timothyt}@nvidia.com
Abstract—As technology scales to lower feature sizes, devices
become more susceptible to soft errors. Soft errors can lead
to silent data corruptions (SDCs), seriously compromising the
reliability of a system. Traditional hardware-only techniques
to avoid SDCs are energy hungry, and hence not suitable
for commodity systems. Researchers have proposed selective
software-based protection techniques to tolerate hardware faults
at lower costs. However, these techniques either use expensive
fault injection or inaccurate analytical models to determine which
parts of a program must be protected for preventing SDCs.
In this work, we construct a three-level model, TRIDENT, that
captures error propagation at the static data dependency, control-
ﬂow and memory levels, based on empirical observations of
error propagations in programs. TRIDENT is implemented as
a compiler module, and it can predict both the overall SDC
probability of a given program and the SDC probabilities of
individual
instructions, without fault injection. We ﬁnd that
TRIDENT is nearly as accurate as fault injection and it is
much faster and more scalable. We also demonstrate the use of
TRIDENT to guide selective instruction duplication to efﬁciently
mitigate SDCs under a given performance overhead bound.
Keywords—Error Propagation, Soft Error, Silent Data Corrup-
tion, Error Resilience, Program Analysis
I.
INTRODUCTION
Transient hardware faults (i.e., soft errors) are predicted to
increase in future computer systems due to growing system
scale, progressive technology scaling, and lowering operating
voltages [26]. In the past, such faults were masked through
hardware-only solutions such as redundancy and voltage guard
bands. However, these techniques are becoming increasingly
challenging to deploy as they consume signiﬁcant amounts of
energy, and as energy is becoming a ﬁrst-class constraint in
processor design [6]. Therefore, researchers have postulated
that future processors will expose hardware faults to the
software and expect the software to tolerate them [24].
One consequence of such hardware errors is incorrect
program output, or silent data corruptions (SDCs), which are
very difﬁcult
to detect and can hence have severe conse-
quences [26]. Studies have shown that a small fraction of
the program states are responsible for almost all the error
propagations resulting in SDCs, and so one can selectively
protect these states to meet the target SDC probability while
incurring lower energy and performance costs than full dupli-
cation techniques [10], [27]. Therefore, in the development
of fault-tolerant applications (Figure 1a), it is important to
estimate the SDC probability of a program – both in the
aggregate, and on an individual instruction basis - to decide
whether protection is required, and if so, to selectively protect
the SDC-causing states of the program. This is the goal of our
work.
Fault Injection (FI) has been commonly employed to esti-
mate the SDC probabilities of programs. FI involves perturbing
the program state to emulate the effect of a hardware fault
and executing the program to completion to determine if
the fault caused an SDC. However, real-world programs may
consist of billions of dynamic instructions, and even a single
execution of the program may take a long time. Performing
thousands of FIs to get statistically meaningful results for each
instruction takes too much time to be practical [13], [14]. As a
result, researchers have attempted to analytically model error
propagation to identify vulnerable instructions [10], [21], [27].
The main advantage of these analytical models is scalability,
as the models usually do not require FIs, and they are fast to
execute. However, most existing models suffer from a lack
of accuracy, as they are limited to modeling faults in the
normal (i.e., fault-free) control-ﬂow path of the program. Since
program execution is dynamic in nature, a fault can propagate
to not only the data-dependencies of an instruction, but also to
the subsequent branches (i.e., control ﬂow) and memory loca-
tions that are dependent on it. This causes deviation from the
predicted propagation, leading to inaccuracies. Unfortunately,
tracking the deviation in control-ﬂow and memory locations
due to a fault often leads to state space explosion.
(a) Development Cycle
(b) Workﬂow of TRIDENT
Fig. 1: Development of Fault-Tolerant Applications
This paper proposes a model, TRIDENT, for tracking error
propagation in programs that addresses the above two chal-
lenges. The key insight in TRIDENT is that error propagation
in dynamic execution can be decomposed into a combination
of individual modules, each of which can be abstracted into
probabilistic events. TRIDENT can predict both the overall
SDC probability of a program and the SDC probability of
individual instructions based on dynamic and static analysis of
the program without performing FI. We implement TRIDENT
in the LLVM compiler [17] and evaluate its accuracy and
scalability vis-a-vis FI. To the best of our knowledge, we are
the ﬁrst to propose a model to estimate the SDC probability
of
instructions and the entire program without
performing any FIs.
individual
EvaluationAcceptable or Not ?ProtectionProgramResilient ProgramSDC Probabilities of Individual InstructionsOverall SDC Probability of the ProgramNY●Program Source Code (LLVM IR)●Program Input●Instructions Considered as Program Output●Overall SDC Probability of the Program ●SDC Probabilities of Individual InstructionsTridentOur main contributions in this paper are as follows:
•
Propose TRIDENT, a three-level model for tracking
error propagation in programs. The levels are static-
instruction, control-ﬂow and memory levels, and they
build on each other. The three-level model abstracts
the data-ﬂow of programs in the presence of faults.
Compare the accuracy and scalability of TRIDENT
with FI, to predict the SDC probability of individual
instructions and that of the entire program.
Demonstrate the use of TRIDENT to guide selective
instruction duplication for conﬁgurable protection of
programs from SDCs under a performance overhead.
•
•
The results of our experimental evaluation are as follows:
•
The predictions of SDC probabilities using TRIDENT
are statistically indistinguishable from those obtained
through FI, both for the overall program and for
individual instructions. On average, the overall SDC
probability predicted by TRIDENT is 14.83% while the
FI measured value is 13.59% across 11 programs.
• We also create two simpler models to show the impor-
tance of modeling control-ﬂow divergence and mem-
ory dependencies - the ﬁrst model considers neither,
while the second considers control-ﬂow divergence but
not memory dependencies. The two simpler models
predict the average SDC probabilities across programs
as 33.85% and 23.76% respectively, which is much
higher than the FI results.
Compared to FI, whose cost is proportional to the
number of injections, TRIDENT incurs a ﬁxed cost,
and a small
incremental cost for each instruction
sampled in the program. For example, TRIDENT takes
about 16 minutes to calculate the individual SDC
probabilities of about 1,000 static instructions, which
is signiﬁcantly faster than the corresponding FI exper-
iments (which often take hours or even days).
Using TRIDENT to guide selective instruction dupli-
cation reduces the overall SDC probability by 65%
and 90% at 11.78% and 23.31% performance over-
heads, respectively (these represent 1/3rd and 2/3rd
of the full-duplication overhead for the programs
respectively). These reductions are higher than the
corresponding ones obtained using the simpler models.
•
•
II. BACKGROUND
In this section, we ﬁrst present our fault model, then deﬁne
the terms we use and the compiler infrastructure we work with.
A. Fault Model
In this paper, we consider transient hardware faults that
occur in the computational elements of the processor, including
pipeline registers and functional units. We do not consider
faults in the memory or caches, as we assume that these
are protected with error correction code (ECC). Likewise, we
do not consider faults in the processor’s control logic as we
assume that it is protected. Neither do we consider faults in the
instructions’ encodings. Finally, we assume that the program
does not jump to arbitrary illegal addresses due to faults during
the execution, as this can be detected by control-ﬂow checking
techniques [23]. However, the program may take a faulty legal
branch (the execution path is legal but the branch direction can
be wrong due to faults propagating to it). Our fault model is
in line with other work in the area [7], [10], [13], [21].
B. Terms and Deﬁnitions
Fault Occurrence: The event corresponding to the occur-
rence of a hardware fault in the processor. The fault may or
may not result in an error.
Fault Activation: The event corresponding to the mani-
festation of the fault to the software, i.e., the fault becomes
an error and corrupts some portion of the software state (e.g.,
register, memory location). The error may or may not result
in a failure (i.e., SDC, crash or hang).
Crash: The raising of a hardware trap or exception due to
the error, because the program attempted to perform an action
it should not have (e.g., read outside its memory segments).
The OS terminates the program as a result.
Silent Data Corruption (SDC): A mismatch between
the output of a faulty program run and that of an error-free
execution of the program.
Benign Faults: Program output matches that of the error-
free execution even though a fault occurred during its execu-
tion. This means either the fault was masked or overwritten
by the program.
Error propagation: Error propagation means that the fault
was activated, and has affected some other portion of the
program state, say ’X’. In this case, we say the fault has
propagated to state X. We focus on the faults that affect the
program state and therefore consider error propagation at the
application level.
SDC Probability: We deﬁne the SDC probability as the
probability of an SDC given that the fault was activated – other
work uses a similar deﬁnition [10], [14], [18], [30].
C. LLVM Compiler
In this paper, we use the LLVM compiler [17] to perform
our program analysis and FI experiments and to implement
our model. Our choice of LLVM is motivated by three reasons.
First, LLVM uses a typed intermediate representation (IR) that
can easily represent source-level constructs. In particular, it
preserves the names of variables and functions, which makes
source mapping feasible. This allows us to perform a ﬁne-
grained analysis of which program locations cause certain
failures and map them to the source code. Second, LLVM
IR is a platform-neutral representation that abstracts out many
low-level details of the hardware and assembly language.
This greatly aids in portability of our analysis to different
architectures and simpliﬁes the handling of the special cases
in different assembly language formats. Finally, LLVM IR has
been shown to be accurate for doing FI studies [30], and there
are many fault injectors developed for LLVM [3], [20], [25],
[30]. Many of the papers we compare our technique with in
this paper also use LLVM infrastructure [9], [10]. Therefore,
in this paper, when we say instruction, we mean an instruction
at the LLVM IR level.
III. THE CHALLENGE
We use the code example in Figure 2a to explain the main
challenge of modeling error propagation in programs. The code
is from Pathﬁnder [5], though we make minor modiﬁcations
for clarity and remove some irrelevant parts. The ﬁgure shows
the control-ﬂow graphs (CFGs) of two functions: init() and
run(). There is a loop in each function: the one in init()
updates an array, and the one in run() reads the array for
processing. The two functions init() and run() are called in
order at runtime. In the CFGs, each box is a basic block
and each arrow indicates a possible execution path. In each
basic block, there is a sequence of statically data-dependent
instructions, or a static data-dependent instruction sequence.
Assume that a fault occurs at the instruction writing to $1
in the ﬁrst basic block in init(). The fault propagates along
its static data-dependent instruction sequence (from load to
cmp). At the end of the sequence, if the fault propagates to the
result of the comparison instruction, it will go beyond the static
data dependency and cause the control-ﬂow of the program to
deviate from the fault-free execution. For example, in the fault-
free execution, the T branch is supposed to be taken, but due
to the fault, the F branch is taken. Consequently, the basic
blocks under the T branch including the store instruction will
not be executed, whereas subsequent basic blocks dominated
by the F branch will be executed. This will load the wrong
value in run(), and hence the fault will continue to propagate
and it may reach the program’s output resulting in an SDC.
We identify the following three challenges in modeling
error propagation: (1) Statically modeling error propagation
in dynamic program execution requires a model that abstracts
the program data-ﬂow in the presence of faults. (2) Due to the
random nature of soft errors, a fault may be activated at any
dynamic branch and cause control-ﬂow divergence in execu-
tion from the fault-free execution. In any divergence, there are
numerous possible execution paths the program may take, and
tracking all of these paths is challenging. One can emulate all
possible paths among the dynamic executions at every dynamic
branch and ﬁgure out which fault propagates where in each
case. However, this rapidly leads to state space explosion. (3)
Faults may corrupt memory locations and hence continue to
propagate through memory operations. Faulty memory values
can be read by (multiple) load instructions at runtime and
written to other memory locations as execution progresses.
There are enormous numbers of store and load instructions
in a typical program execution, and tracing error propagations
among these memory dependencies requires constructing a
huge data dependency graph, which is very expensive.
(a) Example of Propagation
(b) Propagation in fs
Fig. 2: Running Example
As we can see in the above example, if we do not track
error propagations beyond the static data dependencies and
instead stop at the comparison instruction, we may not identify
all the cases that could lead to SDCs. Moreover, if control-
ﬂow divergence is ignored when modeling, tracking errors in
memory is almost impossible, as memory corruptions often
hide behind control-ﬂow divergence, as shown in the above
example. Existing modeling techniques capture neither of these
important cases, and their SDC prediction accuracies suffer
accordingly. In contrast, TRIDENT captures both the control-
ﬂow divergences and the memory corruptions that potentially
arise as a result of the divergence.
IV. TRIDENT
In this section, we ﬁrst introduce the inputs and outputs of