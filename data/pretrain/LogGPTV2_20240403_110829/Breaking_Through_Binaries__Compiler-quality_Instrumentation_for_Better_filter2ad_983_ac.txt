alternative, more aggressive algorithms exist [2, 47], which
we believe are also implementable in ZAFL.
6.1.3 Edge Instrumentation Downgrading
CollAFL [32] optimizes AFL-style edge coverage by down-
grading select blocks to faster (i.e., fewer-instruction) block
coverage. At a high level, blocks with a single predecessor
can themselves represent that edge, eliminating the instruc-
tion cost of hashing the start and end points. We implement
edge downgrading using a meta-characteristic analysis based
on linear ﬂows in ZAX’s Analysis phase; and construct both
edge- and block-coverage instrumentation templates utilized
in the Application phase. Our numbers show that roughly
35–45% of basic blocks beneﬁt from this optimization.
6.2 Feedback-enhancing Transformations
Recent compiler-based fuzzing efforts attain improved code-
penetration power by considering ﬁner-grained execution in-
formation [18, 31]. Below we detail our ZAFL implementa-
tions of two prominent examples: sub-instruction proﬁling
and context-sensitive coverage tracking.
6.2.1 Sub-instruction Proﬁling
Sub-instruction proﬁling breaks down complex conditional
constraints into nested single-byte comparisons—allowing
the fuzzer to track progress toward matching the entire con-
straint, and signiﬁcantly decreasing the overall mutation
effort. Compiler-based implementations (e.g., laf-Intel [1]
and CmpCov [51]) replace comparisons with nested micro-
comparisons; however, as the goal is to augment control-ﬂow
USENIX Association
30th USENIX Security Symposium    1689
with nested conditionals that permit increased feedback, we
observe it is equally effective to insert these before the origi-
nal. We implement a binary-only sub-instruction proﬁling for
(up to) 64-bit unsigned integer comparisons: in ZAX’s Opti-
mization phase, we scan the IR for comparison mnemonics
(i.e., cmp), and then insert a one-byte nested comparison per
constraint byte. We further incorporate handling for division
operators to help reveal divide-by-zero bugs.
6.2.2 Context-sensitive Coverage
Context sensitivity considers calling contexts to enable ﬁner-
grained coverage. For hash-indexing fuzzers like AFL, this
merely requires that the hash index calculation additionally
incorporates a context value. Several LLVM-based efforts
compute values at callsite-level [18] or function-level [31].
Though context values can assigned statically or obtained
dynamically (e.g., from a stack trace), an easy solution is to
create a global context variable which is updated on-the-ﬂy:
we create function-level context sensitivity by instrumenting
each function with a random value, which at function en-
try/exit is XOR’d to a global context value that is used during
edge hashing. We implement function-level context sensitivity
in ZAX’s Application phase. Callsite-level context sensitivity
is also possible by adjusting where values are inserted.
7 Evaluation
Our evaluation answers three high-level questions:
Q1: Does ZAFL enable compiler-style program transforma-
tions while maintaining performance?
Q2: Do performant fuzzing-enhancing program transforma-
tions increase binary-only fuzzing’s effectiveness?
Q3: Does ZAFL support real-world, complex targets?
We ﬁrst perform an evaluation of ZAFL against the leading
binary-only fuzzing instrumenters AFL-Dyninst and AFL-
QEMU on the LAVA-M benchmark corpus [28]. Second,
to see if LAVA-M results hold for real-world programs, we
expand our evaluation to eight popular programs well-known
to the fuzzing literature, selecting older versions known to
contain bugs to ensure self-evident comparison. Third, we
evaluate these instrumenters’ fuzzing overhead across each.
Fourth, we evaluate ZAFL alongside AFL-Dyninst and AFL-
QEMU in fuzzing ﬁve varied closed-source binaries. Fifth, we
test ZAFL’s support for 42 open- and closed-source programs
of varying size, complexity, and platform. Finally, we use
industry-standard reverse-engineering tools as ground-truth
to assess ZAFL’s precision.
7.1 Evaluation-wide Instrumenter Setup
We evaluate ZAFL against the fastest-available binary-only
fuzzing instrumenters; we thus omit AFL-PIN [45, 65, 80]
and AFL-DynamoRIO [43, 73, 82] variants as their reported
overheads are much higher than AFL-Dyninst’s and AFL-
QEMU’s; and Intel PT [48] as it does not support instrumen-
tation (Table 2). We conﬁgure AFL-Dyninst and AFL-QEMU
with recent updates which purportedly increase their fuzzing
performance by 2–3x and 3–4x, respectively. We detail these
below in addition to our setup of ZAFL.
AFL-Dyninst: A recent AFL-Dyninst update [44]
adds two optimizations which increase performance by
2–3x: (1) CFG-aware “single successor” instrumentation
pruning; and (2) two optimally-set Dyninst BPatch API
settings (setTrampRecursive and setSaveFPR).1 We
discovered three other performance-impacting BPatch
settings
(setLivenessAnalysis, setMergeTramp, and
setInstrStackFrames). For fairness we apply the fastest-
possible AFL-Dyninst conﬁgurations to all benchmarks;
but for setLivenessAnalysis we are restricted to its
non-optimal setting on all as they otherwise crash; and
likewise for setSaveFPR on sfconvert and tcpdump.
AFL-QEMU: QEMU attempts to optimize its expensive
block-level translation with caching, enabling translation-free
chaining of directly-linked fetched-block sequences. Until
recently, AFL-QEMU invoked its instrumentation via trampo-
line after translation—rendering block chaining incompatible
as skipping translation leaves some blocks uninstrumented,
potentially missing coverage. A newly-released AFL-QEMU
update [10] claims a 3–4x performance improvement through
enabling support for chaining by instead applying instrumen-
tation within translated blocks. To ensure best-available AFL-
QEMU performance we apply this update in all experiments.
ZAFL: To explore the effects of compiler-quality fuzzing-
enhancing transformation on binary-only fuzzing we instru-
ment benchmarks with all transformations shown in Table 3.
7.2 LAVA-M Benchmarking
For our initial crash-ﬁnding evaluation we select the LAVA-
M corpus as it provides ground-truth on its programs’ bugs.
Below we detail our evaluation setup and results.
7.2.1 Benchmarks
We compile each benchmark with Clang/LLVM before instru-
menting with AFL-Dyninst and ZAFL; for AFL-QEMU we
simply run compiled binaries in AFL using “QEMU mode”.
As fuzzer effectiveness on LAVA-M is sensitive to starting
seeds and/or dictionary usage, we fuzz each instrumented bi-
nary per four conﬁgurations: empty and default seeds both
with and without dictionaries. We build dictionaries as in-
structed by one of LAVA-M’s authors [27].
1This AFL-Dyninst update [44] also adds a third optimization that re-
places Dyninst-inserted instructions with a custom, optimized set. However,
in addition to having only a negligible performance beneﬁt according to its
author, its current implementation is experimental and crashes each of our
benchmarks. For these reasons we omit it in our experiments.
1690    30th USENIX Security Symposium
USENIX Association
Seed,
Binary
md5sum
base64
Dictionary
default, none
default, dict.
empty, none
empty, dict.
default, none
default, dict.
empty, none
empty, dict.
default, none
default, dict.
empty, none
empty, dict.
default, none
default, dict.
empty, none
empty, dict.
Mean Rel. Increase
Mean MWU Score
uniq
who
rel.
total
13.71
13.70
1.34
1.33
0.88
0.94
1.01
0.96
1.62
1.04
1.97
1.55
1.32
1.18
4.13
1.15
ZAFL vs. AFL-Dyninst
rel.
crash
1.00
1.00

2.46

5.52

4.00
1.00
5.75

2.23
1.00
3.78
1.00
1.24
+96% +78% +751% +42% +203% +296%
0.023
ZAFL vs. AFL-QEMU
rel.
crash
1.00
1.00

1.05

1.00

0.87
1.00
7.67

1.04
1.00
3.68
1.00
2.54
rel.
queue
1.70
1.70
3.16
2.80
45.22
32.17
45.54
77.77
1.37
2.39
4.37
2.60
27.07
40.24
12.62
11.22
rel.
queue
1.58
1.58
2.88
2.61
4.39
2.15
4.39
2.22
1.21
1.64
3.71
2.43
21.86
36.36
9.50
15.74
rel.
total
13.71
13.71
1.67
2.57
2.22
1.88
2.15
1.91
1.98
1.23
3.92
2.15
2.44
1.7
4.20
10.00
0.039
0.005
0.007
0.022
0.005
Table 4: ZAFL’s LAVA-M mean bugs and total/queued test cases relative to
AFL-Dyninst and AFL-QEMU. We report geometric means for all metrics
and MWU test p-values (p ≤ 0.05 indicates signiﬁcance).  = ZAFL ﬁnds
crashes while competitor ﬁnds zero.
7.2.2 Experimental Setup and Infrastructure
We adopt the standard set by other LAVA-M evaluations [7,
72, 92] and fuzz each instrumented binary for ﬁve hours with
the coverage-guided fuzzer AFL [93]; each for ﬁve trials per
the four seed/dictionary conﬁgurations. All instrumenters are
conﬁgured as detailed in § 7.1. To maintain performance
neutrality, we distribute trials across eight VM’s spanning
two Ubuntu 16.04 x86-64 systems with 6-core 3.50GHz Intel
Core i7-7800x CPU’s and 64GB RAM. Each VM runs in
VirtualBox with 6GB RAM and one core allocated.
7.2.3 Data Processing and Crash Triage
We log both the number of AFL-saved crashes and test cases
processed (i.e., total−hang−calibration−trim executions);
and in post-processing match each crash to a speciﬁc num-
ber of test cases seen—allowing us to pinpoint when each
crash occurred in its trial. We then triage all crashes and cre-
ate (cid:104)crash_id, testcases_done, triage_data(cid:105) triples; and apply
set operations to obtain the unique crashes over test cases
done (i.e., (cid:104)triaged_crashes, testcases_done(cid:105)). For LAVA-M
we triage solely by its benchmarks’ self-reported bug ID’s.
We compute the average unique crashes, total processed
and queued test cases for all instrumenter-benchmark trial
groupings. To show ZAFL’s effectiveness, we report its mean
relative increase for all three metrics per-trial group, and ge-
ometric mean relative increases among all benchmarks. Fol-
lowing Klees et al.’s [52] recommendation, to determine if
ZAFL’s gains are statistically signiﬁcant, we compute a Mann-
Whitney U-test with a 0.05 signiﬁcance level, and report the
geometric mean p-values across all benchmarks.
7.2.4 Results
We do not include ZAFL’s context sensitivity in our LAVA-M
trials as we observe it slightly inhibits effectiveness (∼2%),
likely due to LAVA-M’s focus on a speciﬁc type of synthetic
bug (i.e., “magic bytes”). This also enhances the distinction on
the impact of ZAFL’s sub-instruction proﬁling transformation
based on number of queued (i.e., coverage-increasing) test
cases. Table 4 shows ZAFL’s mean relative increase in triaged
crashes, total and queued test cases over AFL-Dyninst and
AFL-QEMU per conﬁguration.
ZAFL versus AFL-Dyninst: Across all 16 conﬁgurations
ZAFL executes 78% more test cases than AFL-Dyninst and
either matches or beats it with 96% more crashes on average,
additionally ﬁnding crashes in four cases where AFL-Dyninst
ﬁnds none. As we observe Mann-Whitney U p-values (0.005–
0.023) below the 0.05 threshold we conclude this difference
in effectiveness is statistically signiﬁcant. Though ZAFL aver-
ages slightly fewer (4–12%) test cases on md5sum this is not to
its disadvantage: ZAFL queues 3100–7600% more test cases
and ﬁnds well over 300% more crashes, thus revealing the
value of its control-ﬂow-optimizing program transformations.
ZAFL versus AFL-QEMU: ZAFL matches or surpasses
AFL-QEMU among 15 benchmark conﬁgurations, averag-
ing 42% more crashes and 203% more test cases seen. As
with AFL-Dyninst, ZAFL successfully ﬁnds crashes in four
cases for which AFL-QEMU ﬁnds none. Additionally, the
Mann-Whitney U p-values (0.005–0.039) reveal a statistically
signiﬁcant difference between AFL-QEMU and ZAFL. ZAFL
ﬁnds 13% fewer crashes relative to AFL-QEMU on md5sum
with empty seeds and dictionary, but as ZAFL’s queue is 91%
larger, we believe this speciﬁc seed/dictionary conﬁguration
and ZAFL’s transformations result in a “burst” of hot paths,
which the fuzzer struggles to prioritize. Such occurrences
are rare given ZAFL’s superiority in other trials, and likely
correctable through orthogonal advancements in fuzzing path
prioritization [14, 21, 54, 94].
To our surprise, AFL-QEMU ﬁnds more crashes than AFL-
Dyninst despite executing the least test cases. This indicates
that Dyninst’s instrumentation, while faster, is less sound
than QEMU’s in important ways. Achieving compiler-quality
instrumentation requires upholding both performance and
soundness, which neither QEMU nor Dyninst achieve in con-
cert, but ZAFL does (see § 7.5).
ZAFL versus AFL-LLVM: To gain a sense of whether
ZAFL’s transformation is comparable to existing compiler-
based implementations, we ran ZAFL alongside the the anal-
ogous conﬁguration of AFL’s LLVM instrumentation with
its INSTRIM [47] and laf-Intel [1] transformations applied.
Results show that the two instrumentation approaches result
in statistically indistinguishable (MWU p-value 0.10) bug
ﬁnding performance.
7.3 Fuzzing Real-world Software
Though our LAVA-M results show compiler-quality fuzzing-
enhancing program transformations are beneﬁcial to binary-
only fuzzing, it is an open question as to whether this carries
USENIX Association
30th USENIX Security Symposium    1691
(a) cert-basic
(b) jasper
Figure 3: Real-world software fuzzing unique triaged crashes averaged over 8×24-hour trials.
(c) unrtf
(d) tcpdump
Binary
bsdtar
cert-basic