according to it.
For example, consider the merged processing graph
shown in Figure 4 and suppose its header classiﬁca-
tion block can be implemented in hardware, e.g., using
a TCAM. Thus, we can realize this processing graph
using two OBIs. The ﬁrst OBI, residing on a server
or a dedicated middlebox equipped with the appropri-
ate hardware, performs only packet classiﬁcation. Only
if the packet requires further processing does the ﬁrst
OBI store the classiﬁcation result as metadata, attach
this metadata to the packet, and send it to another,
software-based OBI, to perform the rest of the process-
ing. The split processing graphs are illustrated in Fig-
ure 6. Even an SDN switch that supports packet en-
capsulation could be used as the ﬁrst OBI.
Figure 5 illustrates this scenario in a network-wide
setting: packets from host A (Step 1 in the ﬁgure) to
host B should go through the ﬁrewall and the IPS. This
is realized using two OBIs as described above. The
ﬁrst performs header classiﬁcation on hardware (Step
2), then sends the results as metadata attached to the
packets (Step 3) to the next, software-based OBI. In
this example, this OBI is scaled to two instances, mul-
tiplexed by the network for load balancing. It extracts
metadata from the packets (Step 4), performs the rest
of the processing graph, and sends the packets out with-
out metadata (Step 5). Eventually the packets are for-
warded to host B (Step 6).
In our implementation, we use NSH [37] to attach
metadata to packets. Other methods such as VXLAN
[24], Geneve [19], and FlowTags [12] can also be used
but may require increasing the MTU in the network,
which is a common practice in large scale networks [34].
Diﬀerent OpenBox applications may require diﬀerent
size metadata. In most cases, we estimate the metadata
to be a few bytes, as it should only tell the subsequent
OBI which path in the processing graph it should fol-
low. Nevertheless, it is important to note that attaching
metadata to packets is required only when two blocks
that have originated from the same OpenBox applica-
tion are split between two OBIs.
Finally, an OBI can use external services for out-
of-band operations such as logging and storage. The
OpenBox protocol deﬁnes two such services, for packet
logging and for packet storage (which can be used for
caching or quarantine purposes). These services are
provided by an external server, located either locally
on the same machine as the OBI or remotely. The ad-
dresses and other parameters of these servers are set for
the OBI by the OBC.
3.2 The OpenBox Protocol
The OpenBox communication protocol [35] is used by
OBIs and the controller (OBC) to communicate with
each other. The protocol deﬁnes a set of messages for
this communication and a broad set of processing blocks
that can be used to build network function applications.
Abstract processing blocks are deﬁned in the proto-
col speciﬁcation. Each abstract block has its own con-
ﬁguration parameters.
In addition, similarly to Click
elements [23], blocks may have read handles and write
handles. A read handle in our framework allows the
controller, and applications that run on top of it, to
request information from a speciﬁc processing block in
the data plane. For example, it can ask a Discard block
how many packets it has dropped. A write handle lets
the control plane change a value of a block in the data
plane. For example, it can be used to reset a counter,
or to change a conﬁguration parameter of a block.
3.2.1 Custom Module Injection
An important feature in the OpenBox protocol al-
lows injecting custom software modules from the control
plane to OBIs, if supported by the speciﬁc OBI imple-
mentation. Our implementation, described in Section 4,
supports this capability. This allows application devel-
opers to extend existing OBIs in the data plane without
having to change their code, or to compile and re-deploy
them.
To add a custom module, an application developer
creates a binary ﬁle of this module and then deﬁnes any
new blocks implemented by this module, in the same
way as existing blocks are deﬁned in the protocol. The
format of the binary ﬁle depends on the target OBI (in
516
(a) First OBI: Performs header classiﬁcation on
hardware TCAM and, if necessary, forwards the
results as metadata along with the packet to next
OBI.
(b) Second OBI: Receives header classiﬁcation results and applies the cor-
responding processing path.
Figure 6: Distributed processing in data plane with the processing graph from Figure 4.
our implementation the ﬁle is a compiled Click mod-
ule). When such a module is used, the controller sends
an AddCustomModuleRequest message to the OBI, pro-
viding the module as a binary ﬁle along with metadata
such as the module name.
In addition, this message
contains information required in order to translate the
conﬁguration from the OpenBox notation to the nota-
tion expected by the lower level code in the module.
A custom module should match the target OBI. A
module developer may create multiple versions of a cus-
tom module and let the controller choose the one that
is best suited to the actual target OBI.
3.3 Control Plane
The OpenBox controller (OBC) is a logically cen-
tralized software server.
It is responsible for manag-
ing all aspects of the OBIs: setting processing logic,
and controlling provisioning and scaling of instances.
In an SDN network, the OBC can be attached to a
traﬃc-steering application [36] to control chaining of
instances and packet forwarding between them. OBC
and OBIs communicate through a dual REST channel
over HTTPS, and the protocol messages are encoded
with JSON [10].
The OBC provides an abstraction layer that allows
developers to create network-function applications by
specifying their logic as processing graphs. We use the
notion of segments to describe logical partitions in the
data plane. Diﬀerent segments can describe diﬀerent
departments, administrative domains, or tenants, and
they can be conﬁgured with diﬀerent policies and run
diﬀerent network function applications. Segments are
hierarchical, so a segment can contain sub-segments.
Each OBI belongs to a speciﬁc segment (which can,
in turn, belong to a broader segment). Applications
declare their logic by setting processing graphs to seg-
ments, or to speciﬁc OBIs. This approach allows for
ﬂexible policies in the network with regard to secu-
rity, monitoring, and other NF tasks, and by deﬁnition,
supports the trend of micro-segmentation [41]. Micro-
segmentation reduces the size of network segments to
allow highly customized network policies.
Upon connection of an OBI, the OBC determines
the processing graphs that apply to this OBI in accor-
dance with its location in the segment hierarchy. Then,
for each OBI, the controller merges the corresponding
graphs to a single graph and sends this merged process-
ing graph to the instance, as discussed in Section 3.2.
Our OBC implementation uses the algorithm presented
in Section 2.2 to merge the processing graphs.
The controller can request system information, such
as CPU load and memory usage, from OBIs.
It can
use this information to scale and provision additional
service instances, or merge the tasks of multiple under-
utilized instances and take some of them down. Ap-
plications can also be aware of this information and,
for example, reduce the complexity of their processing
when the system is under heavy load (to avoid packet
loss or to preserve SLAs).
The OBC, which knows the network topology and
the OBI locations, is in charge of setting the forward-
ing policy chains. It does so on the basis of the actual
deployment of processing graphs to OBIs. As Open-
Box applications are deﬁned per segment, the OBC is
in charge of deciding which OBI(s) in a segment will be
responsible for a certain task, and directing the corre-
sponding traﬃc to this OBI.
3.4 OpenBox Applications
An application deﬁnes a single network function (NF)
by statement declarations. Each statement consists of a
location speciﬁer, which speciﬁes a network segment or
a speciﬁc OBI, and a processing graph associated with
this location.
Applications are event-driven, where upstream events
arrive at the application through the OBC. Such events
may cause applications to change their state and may
trigger downstream reconﬁguration messages to the data
plane. For example, an IPS can detect an attack when
alerts are sent to it from the data plane, and then change
its policies in order to respond to the attack; these pol-
icy changes correspond to re-conﬁguration messages in
the data plane (e.g., block speciﬁc network segments,
block other suspicious traﬃc, or block outgoing traﬃc
to prevent data leakage). Another example is a request
517
Read Packets Header Classifier Drop Output Write Metadata Encapsulate Metadata Read Packets Drop Alert  (IPS) Regex Classifier Regex Classifier Regex Classifier Output Alert  (Firewall) Alert  (Firewall) Alert  (Firewall) Alert  (Firewall) Decapsulate Metadata Read Metadata for load information from a speciﬁc OBI. This request
is sent from the application through the OBC to the
OBI as a downstream message, which will later trigger
an event (sent upstream) with the data.
Although events may be frequent (it depends on the
applications), graph changes are not frequent in general,
as application logic does not change often. Applications
that are expected to change their logic too frequently
may be marked so that the merge algorithm will not
be applied on them. The controller can also detect and
mark such applications automatically.
3.4.1 Multi-Tenancy
The OpenBox architecture allows multiple network
tenants to deploy their NFs through the same OBC.
For example, an enterprise chief system administrator
may deploy the OpenBox framework in the enterprise
network and allow department system administrators
to use it in order to deploy their desired network func-
tions.
The OBC is responsible for the correct deployment in
the data plane, including preserving application priority
and ordering. Sharing the data plane among multiple
tenants helps reduce cost of ownership and operating
expenditure as OBIs in the data plane may have much
higher utilization, as discussed in Section 5.
3.4.2 Application State Management
Network functions are, in many cases, stateful. That
is, they store state information and use it when han-
dling multiple packets of the same session. For exam-
ple, Snort stores information about each ﬂow, which in-
cludes, among other things, its protocol and other ﬂags
it may be marked with [40].
Since the state information is used in the data plane
of NFs as part of their packet processing, it is important
to store this information in the data plane, so it can be
quickly fetched and updated. It cannot, for example,
be stored in the control plane. Hence, the OpenBox
protocol deﬁnes two data structures that are provided
by the OBIs, in the data plane, for storing and retrieving
state information.
The metadata storage is a short-lived key-value stor-
age that can be used by an application developer to pass
information along with a speciﬁc packet, as it traverses
the processing graph. The information in the metadata
storage persists over the OBI service chain of a single
packet. It can be encapsulated and sent over from one
OBI to another, along with the processed packet, as
described in Section 3.1.
The other key-value storage available for applications
in the data plane is the session storage. This storage
is attached to a ﬂow and is valid as long as the ﬂow
is alive. It allows applications to pass processing data
between packets of the same ﬂow. This is useful when
programming stateful NF applications such as Snort,
which stores ﬂow-level metadata information such as
ﬂow tags, gzip window data, and search state.
Frameworks such as OpenNF [18] can be used as-is to
allow replication and migration of OBIs along with their
stored data, to ensure correct behavior of applications
in such cases.
4.
IMPLEMENTATION
We have implemented the OpenBox framework in two
parts: a software-based OBI, and an OpenBox con-
troller. We provide a simple installation script that
installs both on a Mininet VM [28]. All our code is
available at [29].
4.1 Controller Implementation
Our controller is implemented in Java in about 7500
It runs a REST server for communica-
lines of code.
tion with OBIs and for the management API. The con-
troller exposes two main packages. The ﬁrst package
provides the basic structures deﬁned in the protocol,
such as processing blocks, data types, etc. The other
package lets the developer deﬁne applications on top
of the controller, register them, and handle events. It
also allows sending requests such as read requests and
write requests, which in turn invoke read and write han-
dles, accordingly, in the data plane (as described in Sec-
tion 3.2).
When an application sends a request, it provides the
controller with callback functions that are called when
a response arrives back at the controller. The controller
handles multiplexing of requests and demultiplexing of
responses.
Along with the controller implementation, we have
implemented several sample applications such as a ﬁre-
wall/ACL, IPS, load balancer, and more. In addition,
we implemented a traﬃc steering application as a plu-
gin for the OpenDaylight OpenFlow controller [14]. We
use it to steer the traﬃc between multiple OBIs.
4.2 Service Instance Implementation
Our OBI implementation is divided into a generic
wrapper and an execution engine. The generic wrapper
is written in Python in about 5500 lines of code.
It
handles communication with the controller (via a local
REST server), storage and log servers, and translates
protocol directives to the speciﬁc underlying execution
engine.
The execution engine in our implementation is the
Click modular router [23], along with an additional layer
of communication with the wrapper and storage server,
and several additional Click elements that are used to
provide the processing blocks deﬁned in the protocol
(a single OpenBox block is usually implemented using
multiple Click blocks). All our code for the execution
engine is written as a Click user-level module, without
any modiﬁcation to the core code of Click. The code of
this module is written in C++ and consists of about 2400
lines. Note that by changing the translation module in
the wrapper, the underlying execution engine can be
518
(a) Two-ﬁrewall service chain
(b) Firewall and IPS service chain
(c) Test setup with OpenBox
Figure 7: Test setups under pipelined NF conﬁguration.
replaced. This is necessary, for example, when using an
execution engine implemented in hardware.
Finally, our OBI implementation supports custom mod-
ule injection, as described in Section 3.2.1. An applica-
tion developer who wishes to extend the OBI with new
processing blocks should write a new Click module (in
C++) that implements the underlying Click elements
of these new blocks, and implement a translation ob-
ject (in Python) that helps our wrapper translate new
OpenBox block deﬁnitions to use the code provided in
the new Click module.
5. EXPERIMENTAL EVALUATION
5.1 Experimental Environment
Our experiments were performed on a testbed with