### Fanout Values
- Fanout = 08
- Fanout = 04
- Fanout = 32
- Fanout = 64

### Incubation Period (Seconds)
- 10
- 100
- 1000
- 10000

### Figure 3: Percentage of Infected Hosts

#### 3.2 Metrics Used for Detection
The primary and most apparent metric is the detection time, which is the interval between the introduction of the virus and its detection. Figure 2 illustrates how the detection time varies with changes in the propagation factor (also known as fanout) and the incubation period. It is evident that longer incubation periods and lower propagation factors result in delayed detection. This delay is somewhat mitigated by the slower spread of the virus under these conditions. Therefore, we also consider other metrics that account for the speed of virus propagation:

- **Percentage of clients infected at the time of detection**
- **Percentage of email traffic due to viruses at the time of detection**

The first metric is related to the costs associated with cleaning up after a virus infection. The second metric reflects the load on the email server and the extent to which its functionality is degraded by the virus.

Figure 3 shows the percentage of infected hosts at the time of detection for an intranet consisting of 400 clients. The figure indicates that noisy viruses are detected early, while stealthy viruses, especially those with large incubation periods and low propagation factors, can potentially infect a significant portion of the network before being detected. Figure 4 further demonstrates that, for a fixed propagation factor (set at 8 for this graph) and a given incubation period, the fraction of infected hosts is lower when the number of clients in the intranet is higher.

### Figure 4: Infected Hosts
- **Fanout = 8, Network size = 400 Clients**
- **Fanout = 8, Network size = 800 Clients**
- **Fanout = 8, Network size = 1600 Clients**

### Figure 5: Fraction of Email Virus Traffic at Detection
- **Fanout = 32, Network size = 1600 Clients**
- **Incubation Period (Seconds): 1, 10, 100, 1000, 10000**

Figure 5 shows the fraction of email traffic due to viruses at the time of detection. Specifically, we calculated the fraction of email traffic due to viruses in the two seconds preceding the detection. The virus traffic ranged from 40% to 70%, indicating that the email server was only slightly overloaded. Due to the bursty nature of email traffic, servers are typically designed to handle several times the average rate of email generation. Therefore, a 40% to 70% increase in email traffic is not very significant.

### 3.3 False Alarms
False alarm rates were computed using two different criteria:

- **Criteria 1:** Count even a single alarm as a false alarm. Using this criterion, there were a total of 3 false alarms across 8 runs, or a rate of about 0.38 false alarms per hour.
- **Criteria 2:** Apply a threshold criterion, and count a false alarm when the threshold is exceeded. This threshold was established through experimentation. We found that by registering an alarm when more than 3 alarms are reported over a period of two seconds, zero false alarm rate could be achieved in our simulation.

We note that Criteria 2 was used in the detection results reported earlier, resulting in zero false alarm rates.

### Runtime Performance and Memory Usage
The implementation was done in Java. For 400 clients, approximately 800 frequency distributions were maintained, each over 8 time scales. This resulted in a total memory usage of 30MB for the Java program. When run on an Intel Pentium III system operating at 1GHz, it was able to simulate about 500 cycles per second, simulating 100 seconds in one second of operation. Additionally, the anomaly detector processed about 100 messages per second. This performance was adequate for fast simulation. In a live environment, these performance results show that the anomaly detector would consume 1% of CPU on a similar system.

### 4 Experiment II
This experiment was conducted as part of the DARPA SWWIM program. The SWWIM Autonomic Response Architecture (SARA) experiment involved a collaborative team of organizations, each responsible for a key function. This experiment differed from the previous one in several aspects:

- **Asymmetric User Models:** The behavior models for different users were different.
- **Real Email Servers and Clients:** The experiment used real email servers (sendmail) and clients.
- **Third-Party Design:** The simulation and viruses were designed by a third party with no vested interest in the performance of the detectors from different organizations.

The overall goal of the SARA experiment was to evaluate the value of orchestrated response to attacks. The system included several virus detection components, response components in the form of mail server and client enhancements to purge suspected messages, and an orchestrator. The orchestrator took input from the detection components, evaluated the system state, selected a response action, and communicated these actions to the response components. Several detection components were built, including simple and complex behavior-based detectors and our anomaly detector.

In the experimental design, it was decided that the behavior-based techniques would be used for early detection, and the system would attempt a carefully orchestrated sequence of responses. However, these detectors could be fooled by stealthy viruses, at which point the results from the anomaly detector would be used to identify the spread of the virus. The anomaly detector cannot provide precise identification of offending email messages; it can only indicate that a predominant number of email messages causing an alarm are likely to be viruses. Due to the absence of precise identification and time constraints, the orchestrator would simply shut down the system if the only information available was from the anomaly detector. This is a last-resort response and should only be attempted if all other measures fail.

### 4.1 Experimental Setup
The experiment was carried out using a full-scale simulation of an email system for a single subnet of 400 clients, including an email server (modified version of sendmail) and 400 email clients. The detection, response, and orchestration components communicated and worked in conjunction with the email server and clients.

Similar to Experiment I, user actions were emulated by 400 "bots." However, these bots were significantly more complex, implemented as processes that run concurrently and modeled using a three-state Markov model. The user behavior model was asymmetric and captured the concept of address books, making it more common for emails with a large number of recipients to be generated.

### 4.2 Detection Effectiveness
Hundreds of simulation runs were conducted with various types of viruses. Since the anomaly detector was tuned for delayed detection, no alarms were generated in runs where the orchestrator contained the virus. There were seven runs where the orchestrator was unable to contain the virus, and in each of these cases, the anomaly detector successfully detected the virus, typically within 2 to 3 minutes of the virus's release. In some cases, detection was slower, particularly for virus 4b.v2, which had a very long incubation period and did not propagate quickly until later stages.

### Figure 6: Properties of Viruses Used
- **Virus Type Description:**
  - **1:** Static
  - **2a:** Randomized Addresses (taken from sent items)
  - **2b:** Randomized Addresses (taken from received items)
  - **3a:** Randomized (random number of recipients)
  - **3b:** Delayed Randomized (random number of recipients and time delay)
  - **4a:** Polymorphic (virus attachments all end in .vbs)
  - **4a.v1:** Polymorphic (virus attachments have variable extensions)
  - **4b:** Persistent Polymorphic (virus attachments all end in .vbs, lives forever)
  - **4b.v1:** Persistent Polymorphic (fast propagating version)
  - **4b.v2:** Persistent Polymorphic (slow propagating version)
  - **4b.v3:** Persistent Polymorphic (medium propagating version)
  - **4b.v4:** Persistent Polymorphic (viruses have variable extensions, lives forever)

### Figure 7: Time of Detection (Post-Virus Release)
- **Virus Type (post-virus release)**
- **Percentage of Infected Hosts**