title:Adversarial Attack against Deep Reinforcement Learning with Static
Reward Impact Map
author:Patrick P. K. Chan and
Yaxuan Wang and
Daniel S. Yeung
Chen et al. Cybersecurity            (2019) 2:11 
https://doi.org/10.1186/s42400-019-0027-x
SURVEY
Cybersecurity
Open Access
Adversarial attack and defense in
reinforcement learning-from AI security view
Tong Chen , Jiqiang Liu, Yingxiao Xiang, Wenjia Niu*, Endong Tong and Zhen Han
Abstract
Reinforcement learning is a core technology for modern artificial intelligence, and it has become a workhorse for AI
applications ranging from Atrai Game to Connected and Automated Vehicle System (CAV). Therefore, a reliable RL
system is the foundation for the security critical applications in AI, which has attracted a concern that is more critical
than ever. However, recent studies discover that the interesting attack mode adversarial attack also be effective when
targeting neural network policies in the context of reinforcement learning, which has inspired innovative researches
in this direction. Hence, in this paper, we give the very first attempt to conduct a comprehensive survey on adversarial
attacks in reinforcement learning under AI security. Moreover, we give briefly introduction on the most representative
defense technologies against existing adversarial attacks.
Keywords: Reinforcement learning, Artificial intelligence, Security, Adversarial attack, Adversarial example, Defense
intelligence (AI)
Introduction
Artificial
is providing major break-
throughs in solving the problems that have withstood
many attempts of natural language understanding, speech
recognition, image understanding and so on. The latest
studies (He et al. 2016) show that the correct rate of
image understanding can reach 95% under certain condi-
tions, meanwhile the success rate of speech recognition
can reach 97% (Xiong et al. 2016).
Reinforcement learning (RL) is one of the main tech-
niques that can realize artificial intelligence (AI), which is
currently being used to decipher hard scientific problems
at an unprecedented scale.
To summarized, the researches of reinforcement learn-
ing under artificial intelligence are mainly focused on the
following fields. In terms of autonomous driving (Shalev-
Shwartz et al. 2016; Ohn-Bar and Trivedi 2016), Shai
et al. applied deep reinforcement learning to the problem
of forming long term driving strategies (Shalev-Shwartz
et al. 2016), and solved two major challenges in self driv-
ing. In the aspect of game play (Liang et al. 2016), Silver
et al. (2016) introduced a new approach to computer
Go which can evaluate board positions, and select the
*Correspondence: PI:EMAIL
Beijing Key Laboratory of Security and Privacy in Intelligent Transportation,
Beijing Jiaotong University, Beijing, China
best moves with reinforcement learning from games of
self-play. Meanwhile, for Atari game, Mnih et al. (2013)
presented the first deep learning model to learn con-
trol policies directly from high-dimensional sensory input
using reinforcement learning. Moreover, Liang et al. (Guo
et al. 2014) also built a better real-time Atrai game play-
ing agent with DQN. In the field of control system, Zhang
et al. (2018) proposed a novel
load shedding scheme
against voltage instability with deep reinforcement learn-
ing(DRL). Bougiouklis et al. (2018) presented a system for
calculating the optimum velocities and the trajectories of
an electric vehicle for a specific route. In addition, in the
domain of robot application (Goodall and El-Sheimy 2017;
Martínez-Tenor et al. 2018), Zhu et al. (2017) applied their
model to the task of target-driven visual navigation. Yang
et al. (Yang et al. 2018) presented a soft artificial muscle
driven robot mimicking cuttlefish with a fully integrated
on-board system.
In addition, reinforcement learning is also an impor-
tant technique for Connected and Automated Vehicle
System(CAV), which is a hotspot issue in recent years.
Meanwhile, the security research for this direction has
attracted numerous concerns(Chen et al. 2018a; Jia et al.
2017). Chen et al. performed the first security analysis
on the next-generation Connected Vehicle (CV) based
transportation systems, and pointed out the current sig-
nal control algorithm design and implementation choices
© The Author(s). 2019 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made.
Chen et al. Cybersecurity            (2019) 2:11 
Page 2 of 22
are highly vulnerable to data spoofing attacks from even
a single attack vehicle. Therefore, how to build a reliable
and security reinforcement learning system to support the
security critical applications in AI, has become a concern
which is more critical than ever.
However, the weaknesses of reinforcement learning are
gradually exposed which can be exploited by attackers.
Huang et al. (2017) firstly discovered that neural network
policies in the context of reinforcement learning are vul-
nerable to “Adversarial Attacks” in the form of adding tiny
perturbations to inputs which can lead a model to give
wrong results. Regardless of the learned task or training
algorithm, they observed a significant drop in perfor-
mance, even with very small adversarial perturbations
which are invisible to human. Even worse, they found
that the cross-dataset transferability property (Szegedy
et al. 2013 proposed in 2013) also holds in reinforce-
ment learning applications, so long as both policies have
been trained to solve the same task. Such discoveries have
attracted public interests in the research of adversarial
attacks and their corresponding defense technologies in
the context of reinforcement learning.
After Huang et al. (2017), a lot of works have focused
on the issue of adversarial attack in the field of reinforce-
ment learning (e.g., Fig. 1). For instance, in the field of
Atari game, Lin et al. (2017) proposed a “strategically-
timed attack” whose adversarial example at each time
step is computed independently of the adversarial exam-
ples at other time steps, instead of attacking a deep RL
agent at every time step (see “Black-box attack” section).
Moreover, in the terms of automatic path planning, Liu
et al. (2017), Xiang et al. (2018), Bai et al. (2018) and
Chen et al. (2018b) all proposed methods which can take
adversarial attack on reinforcement learning algorithms
(VIN (Tamar et al. 2016), Q-Learning (Watkins and Dayan
1992), DQN (Mnih et al. 2013), A3C (Mnih et al. 2016))
under automatic path planning tasks (see “Defense tech-
nology against adversarial attack” section).
In view of the extensive and valuable applications of the
reinforcement learning in modern artificial intelligence
(AI), and the critical role for reinforcement learning in
AI security, inspiring innovative researches in the field of
adversarial research.
The main contributions of this paper can be concluded
as follows:
1 We give the very first attempt to conduct a
comprehensive and in-depth survey on the
literatures of adversarial research in the context of
reinforcement learning from AI security view.
2 We make a comparative analysis for the
characteristics of adversarial attack mechanisms and
defense technologies respectively, to compare the
specific scenarios and advantages/disadvantages of
the existing methods, in addition, give a prospect for
the future work direction.
The structure of this paper is organized as follow. In
“Preliminaries” section, we first give a description for
the common term related to adversarial attack under
reinforcement learning, and briefly introduce the most
representative RL algorithms. “Adversarial attack in rein-
forcement learning” section reviews the related research
of adversarial attack in the context of reinforcement learn-
ing. For the defense technologies against adversarial attack
in the context of reinforcement learning are discussed in
“Defense technology against adversarial attack” section.
Finally, we draw conclusion and discussion in “Conclusion
and discussion” section.
Fig. 1 Examples for adversarial attacks on reinforcement learning. As shown in the first line are the examples for adversarial attack in the field of Atari
game. The first image denotes the original clean game background, while the others show the perturbed game background which can be called as
“adversarial example”. Huang et al. (2017) found that the adversarial examples which are invisible to human have a significant impact on the game
result. Moreover, the second line shows the examples for adversarial attack in the domain of automatic path planning. Same as the first row, the first
image represents the original pathfinding map, and the remaining two images denote the adversarial examples generated by noise added. Chen
et al. (2018b) found that the trained agent could not find its way correctly under such adversarial examples
Chen et al. Cybersecurity            (2019) 2:11 
Page 3 of 22
Preliminaries
In this section, we give explanation for the common terms
related to adversarial attack in the field of reinforce-
ment learning. In addition, we also briefly introduce the
most representative reinforcement learning algorithms,
and take comparison of these algorithms from approach
type, learning type, and application scenarios. So as to
facilitate readers’ understanding of the content for the
following sections.
Common terms definitions
• Reinforcement Learning: is an important branch of
machine learning, which contains two basic elements
stateand action. Performing a certain action under
the certain state, what the agent need to do is to
continuously explore and learn, so as to obtain a good
strategy.
• Adversarial Example: Deceiving AI system which
can lead them make mistakes. The general form of
adversarial examples is the information carrier (such
as image, voice or txt) with small perturbations
added, which can remain imperceptible to human
vision system.
1 Implicit Adversarial Example: is a modified
version of clean information carrier, which
generated by adding human invisible
perturbations to the global information on pixel
level to confuse/fool a machine learning technique.
2 Dominant Adversarial Example: is a modified
version of clean map, which generated by adding
physical-level obstacles to change the local
information to confuse/fool A3C path finding.
• Adversarial Attack: Attacking on artificial
intelligence (AI) system by utilizing adversarial
examples. Adversarial attacks are generally can be
classified into two categories:
1 Misclassification attacks: aiming for generating
adversarial examples which can be misclassified
by target network.
2 Targeted attacks: aiming for generating
adversarial examples which can target
misclassifies into an arbitrary label designated by
adversary specially.
• Perturbation: The noise added on the original clean
information carriers (such as image, voice or txt),
which can make them to be adversarial examples.
• Adversary: The agent who attack AI system with
adversarial examples. However, in some cases, it also
refer to adversarial example itself (Akhtar and Mian
2018).
• Black-Box Attack: The attacker has no idea of the
details related to training algorithm and
corresponding parameters of the model. However,
the attacker can still interact with the model system,
for instance, by passing in arbitrary input to observe
changes in output, so as to achieve the purpose of
attack. In some work (Huang et al. 2017), for
black-box attack, authors assume that the adversary
has access to the training environment (e.g., the
simulator) but not the random initialization of the
target policy, and additionally may not know what the
learning algorithm is.
• White-Box Attack: The attacker has access to the
details related to training algorithm and
corresponding parameters of the model. Attacker can
interact with the target model in the process of
generating adversarial attack data.
• Threat Model: Finding system potential threat to
establish an adversarial policy, so as to achieve the
establishment of a secure system (Swiderski and
Snyder 2004). In the context of adversarial research,
threat model considers adversaries capable of
introducing small perturbations to the raw input of
the policy.
• Transferability: an adversarial example designed to
be misclassified by one model is often misclassified by
other models trained to solve the same task (Szegedy
et al. 2013).
• Target Agent: The target subject attacked by
adversarial examples, usually can be a network model
trained by reinforcement learning policy, which can
detect whether adversarial examples can attack
successfully.
Representative reinforcement learning algorithms
In this section, we list the most representative reinforce-
ment learning algorithms, and make comparison among
them which can be shown in Table 1, where “value-based”
denotes that the reinforcement learning algorithm cal-
culates the expected reward of actions under potential
rewards, and takes it as the basis for selecting actions.
Meanwhile, the learning strategy for “value-based” rein-
forcement learning is constant, in other words, under the
certain state the action will be fixed.
While the “policy-based” represented that the reinforce-
ment learning algorithm trains a probability distribution
by strategy sampling, and enhances the probability of
selecting actions with high reward value. This kind of rein-
forcement learning algorithm will learn different strate-
gies, in other words, the probability of taking one action
under the certain state is constantly adjusted.
• Q-Learning
Q-Learning is a classical algorithm for reinforcement
learning, was proposed earlier and has been used widely.
Chen et al. Cybersecurity            (2019) 2:11 
Page 4 of 22
Table 1 The comparison of the most representation reinforcement learning algorithms
RL algorithm
Approach type
Learning type
Q-Learning (Watkins and Dayan 1992)
Value-based
Shallow Learning
DQN (Mnih et al. 2013)
Value-based
Deep Learning
VIN (Tamar et al. 2016)
A3C (Mnih et al. 2016)
TRPO (Schulman et al. 2015)
UNREAL (Jaderberg et al. 2016)
Value-based
Combined
Policy-based
Combined
Deep Learning
Deep Learning
Deep Learning
Deep Learning
Application scenarios
Motion Control, Control System,
and Robot Application et al.
Motion Control, Neutralization Reaction
Control, and Robot Path Planning et al.
Path Planning, and Motion Control et al.
motion Control, Game Playing, self-driving,
and Path Planning et al.
Motion Control, and Game Playing et al.
Motion Control, and Game Playing et al.
Approach Type contains two categories, namely Policy-based, and Value-based. Meanwhile, learning Type also contains two categories, namely Shallow Learning, and Deep
Learning
Q-Learning was firstly proposed by C.Watkins (Watkins
and Dayan 1992) in his Doctoral Dissertation Learning
from delayed rewards in 1989. It is actually a variant of
Markov Decision Process (MDP)(Markov 1907). The idea
of Q-Learning is based on the value iteration, which can
be concluded as, the agent perceives surrounding infor-
mation from the environment and selects appropriate
methods to change the sate of environment according
to its own method, and obtains corresponding incen-
tives and penalties to correct the strategy. Q-Learning
proposes a method to update the Q-value, which can
be concluded as Q(St, At) ← Q(St, At) + α(Rt+1 +
λ maxa Q(St+1, a) − Q(St, At)). Throughout the contin-
uous iteration and learning process, the agent tries to
maximize the rewards it receives and finds the best path to
the goal, and the Q matrix can be obtained. Q is an action
utility function that evaluates the strengths and weakness
of actions in a particular state and can be interpreted as
the brain of an intelligent agent.
• Deep Q-Network (DQN)
DQN is the first deep enhancement learning algorithm
proposed by Google DeepMing in 2013 (Mnih et al. 2013)
and further improved in 2015 (Mnih et al. 2015). Deep-
Mind applies DQN to Atari games, which is different
from the previous practice, utilizing the video informa-
tion as input and playing games against humans. In this
paper, authors gave the very first attempt to introduce
the concept of Deep Reinforcement Learning, and has
attracted public attentions in this direction. For DQN, as
the output for the value network is the Q-value, then if
the target Q-value can be constructed, the loss function
can be obtained by Mean-Square Error (MSE). However,
the input for value network are state S, action A, and
feedback reward R. Therefore, how to calculate the tar-
get Q-value correctly is the key problem in the context
of DQN.
• Value Iterative Network (VIN)
Tamar et al. (2016) proposed the value iteration net-
work, a fully differentiable CNN planning module for
approximate value iterative algorithms that can be used
for learning to plan, such as the strategies in reinforce-
ment learning. This paper mainly solved the problem of
weak generalization ability of deep reinforcement learn-
ing. There is a special value iterative network structure
in VIN (Touretzky et al. 1996). For this novel method
proposed in this work, it not only needs to use neural net-
work to learn a direct mapping from state to decision, but
also can embeds the traditional planning algorithm into
the neural network so that the neural network can learn
how to act under current environment, and use long-
term planning-assisted neural networks to give a better
decision.
• Asynchronous Advantage Actor-Critic Algorithm
(A3C)
The A3C algorithm is a deep enhancement learning
algorithm proposed by DeepMind in 2016 (Mnih et al.
2016). A3C completely utilizes the Actor-Critic frame-
work and introduces the idea of asynchronous training,
which can improves the performance and speeds up the
whole training process. If the action is considered to
be bad, the possibility for this action will be reduced.
Through iterative training, A3C constantly adjusts the
neural network to find the best action selected policy.
• Trust Region Policy Optimization (TRPO)
TRPO is proposed by J.Schulman in 2015 (Schulman
et al. 2015), it is a kind of random strategy search method
in strategy search method. TRPO can solves the problem of
step selection of gradient update, and gives a monotonous
strategy improvement method. For each training iterative,
whole-trajectory rollouts of a stochastic policy are used
to calculate the update to the policy parameters θ, while
Chen et al. Cybersecurity            (2019) 2:11 
Page 5 of 22
controlling the change in policy as measured by the KL
divergence between the old and the new policies.
• UNREAL
The UNREAL algorithm is the latest depth-enhancement
learning algorithm proposed by DeepMind in 2016
(Jaderberg et al. 2016). Based on the A3C algorithm, the
performance and training process for this algorithm are
further improved. The experimental results show that the
performance for UNREAL at Atari is 8.8 times against
human performance and 3D at the first perspective, more-
over, UNREAL has reached 87% of human level in the
first-view 3D maze environment Labyrinth. For UNREAL,
there are two types of auxiliary tasks, the first one is the
control task, including pixel control and hidden layer acti-
vation control. The other one is back prediction tasks,
as in many scenarios feedback r is not always available,
allowing the neural network to predict the feedback value
will give it a better ability to express. UNREAL algo-
rithm uses historical continuous multi-frame image input
to predict the next-step feedback value as a training target
and uses history information to additionally increase the
value iteration task.
Adversarial attack in reinforcement learning
In this section, we discuss the related research of adver-
sarial attack in the field of reinforcement
learning.
The reviewed literatures mainly conduct the adversarial
research on specific application scenarios, and generate
adversarial examples by adding perturbations to the infor-
mation carrier, so as to realize the adversarial attack on
reinforcement learning system.
We organize the review mainly according to chronological
order. Meanwhile, in order to make readers can understand
the core technical concepts of the surveyed works, we go
into technical details of important methods and represen-
tative technologies by referring to the original papers. In
part 3.1, we discuss the related works of adversarial attack
against the reinforcement learning system in the domain
of White-box attacking. In terms of Black-box attacking,
the design of adversarial attack against the target model
is shown in part 3.2. Meanwhile, we analyze the avail-
ability and contribution of adversarial attack researches
in the above two fields. Additionally, we also give sum-
mary on the attributions of adversarial attacking methods
discussed in this section in part 3.3.
White-box attack
Fast gradient sign method (FGSM)
Huang et al. (2017) first showed that adversarial attacks
are also effective when targeting neural network poli-
cies in reinforcement learning system. Meanwhile, for this
work, the adversary attacks a deep RL agent at every time
step, by perturbing each image the agent observes.
The main contributions for Huang et al. (2017) can be
concluded as the following two aspects:
(I) They gave the very first attempt to prove that
reinforcement learning systems are vulnerable to
adversarial attack, and the traditional generation
algorithms designed for adversarial examples still can
be utilized to attack under such scenario.
(II) Authors creatively verified how effectiveness of
adversarial examples are impacted by the deep RL
algorithm used to learn the policy.
Figure 2 shows the adversarial attack on Pong game
trained with DQN, we can see that after adding small
Fig. 2 Examples for adversarial attacks on Pong policy trained with DQN(Huang et al. 2017). The first line: computing adversarial perturbations by
fast gradient sign method (FGSM)(Goodfellow et al. 2014a) with an ℘∞-norm constraint. The trained agent who should have taken the “down”
action took “noop” action instead under adversarial attack. The second line: authors utilized the FGSM with ℘1-norm constraint to compute the
adversarial perturbations. The trained agent can not take action correctly, which should have moved up, but took “down” action after interference.
Videos are available at http://r11.berkeley.edu/adversarial
Chen et al. Cybersecurity            (2019) 2:11 
Page 6 of 22
perturbation to the original clean game background, the
trained agent cannot make a correct judgment according
to the motion direction of ball. Noting that the adversar-
ial examples are calculated by fast gradient sign method
(FGSM) (Goodfellow et al. 2014a).
FGSM expects the classifier can assign the same class to
the real example x and the adversarial example ˜x with a
small enough perturbation η which can be concluded as
(1)
where ω denotes a weight vector, since this perturba-
tion maximizes the change in output for the adversarial
example ˜x, ωT ˜x = ωTx + ωT η.
Moreover, under image classification network with
parameters θ, model input x, targets related to input y,
and cost function J(θ, x, y). Linearizing the cost function
to obtain an optimal max-norm constrained perturbation
which can be concluded as
η = sign(ω) , (cid:4)η(cid:4)∞ < 
η = sign(∇xJ(θ, x, y))
(2)
In addition, authors also proved that policies trained
with reinforcement learning are vulnerable to the adver-
sarial attack. However, among the RL algorithms tested in