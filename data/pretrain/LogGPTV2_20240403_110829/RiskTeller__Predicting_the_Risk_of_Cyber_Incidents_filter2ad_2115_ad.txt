if i > l, Ri otherwise.
if i > l, Ri otherwise.
Session F2:  Insights from Log(in)sCCS’17, October 30-November 3, 2017, Dallas, TX, USA1306The starting scores are
P1
i = Q1
i = Ri if i ⩽ l, 0.5 otherwise.
(6)
We finally learn function F through a random forest classifier
consisting of 800 trees. We train the classifier to fit F such that P∗
i is
the sample weight of the corresponding unlabeled user profile Xi.
From Equation 5 we see that, if profile i is not labeled, we set it as a
weighted average of the other values Xj, where closer values have a
higher impact. To guarantee that the output of F is in the[0, 1] range,
we adopt the soft-voting based probabilistic output of the random
forest, which is the mean predicted class probability of all the trees
in the forest. As such, given the usage profiling features Xi of each
user, F(Xi) is the final risk score that we derive. It is noteworthy
that the probabilistic output of the random forest doesn’t require
additional calibration, unlike support vector machines or boosting.
It is defined as the mean predicted class probability for each tree
in the forest. For each tree, the class probability is the fraction
of samples of that class that drop into the same leaf. When the
training set is large enough, the output probability asymptotically
approaches the true probability of class membership.
6 EXPERIMENTS AND RESULTS
We evaluate RiskTeller through an extensive set of experiments.
First, we analyze RiskTeller’s parameters (Section 6.1) to choose
the best settings; then, we evaluate its ability to predict machine
infection (Section 6.2). We then study the significance of features
and feature categories (Section 6.3), and the significance of the
semi-supervised risk prediction algorithm we propose, in particular
when few items are labeled for the ground truth (Section 6.4).
6.1 RiskTeller Parameters
RiskTeller has two key sets of parameters that we evaluate here: the
first regards M and N , the length of the periods we use for feature
extraction and labeling; the second regards the thresholds used for
setting the ground truth labels. Here, we show the effects of these
parameters, and how we set them for the rest of this analysis.
6.1.1
Feature Extraction and Labeling Period Length. We use our
data for two different goals: feature extraction and assigning labels.
Since our goal is predicting infections that occur after we extract
features, we separate data in two consecutive periods: the feature
extraction period, based on which we create the machine profiles
described in Section 4, and the labeling period, based on which
we extract the ground truth labels that we attempt to predict with
RiskTeller through the machine profile information.
We fragment our year of data in two consecutive periods, where
the feature extraction period lasts M months and the labeling pe-
riod lasts N months. As our dataset has one year of data, we are
constrained by M +N ⩽ 12; when M +N < 12, we prepare different
datasets starting at the beginning of each month. For example, for
M = 6 and N = 4, we create three different datasets with feature
extraction (labeling) periods of respectively January–June (July–
October), February–July (August–November), and March–August
(September–December).
We aim to investigate the impact of M and N on risk prediction
performance, and to choose their optimal setting; we organize the
experiment as follows. For each (M, N) pair such that M + N ⩽ 12,
Figure 3: AUC scores obtained with different M and N values.
we recover the datasets created above and identify a ground truth
of clean and risky profiles based on their behavior in the labeling
period. We split the labeled users into a labeled training set L and a
validation set V . We then build the risk prediction model using both
L and all the unlabeled user profiles U , as described in Section 5.
After that, we apply the prediction model on the validation set
V to test accuracy. We repeat the split of L and V as a 10-fold
cross-validation process.
Figure 3 shows how AUC varies as a function of M and N . We
can see that, in general, longer values of both M and N lead to
higher AUC scores: larger time windows improve the stability of
the statistical summary of user profiles and accumulate more solid
evidence for identifying the ground truth labels, which in turn
leads to higher AUC scores. Nevertheless, if more months are used
for feature extraction, less months are used to identify the ground
truth labels; as a result, the identified ground truth labels are more
likely to miss some challenging and ambiguous user profiles, which
are located near the classification boundary. Although the AUC
score grows if these ambiguous user profiles are excluded from
the training / testing process, this carries the risk of over-fitting
in our risk prediction model. We remark that AUC scores are high
and rather stable for N ⩾ 5 and 6 ⩽ M ⩽ 8: this corroborates a
conclusion stating that M and N should be similar in size, because
a larger M increases the stability of feature extraction, and larger N
increases the stability and quality of the labeling process. Therefore,
in the remainder of our experiments we will set M = N = 6, which
consistently provides good results.
6.1.2 Thresholds for the Ground Truth. Our ground truth relies
on the detection capabilities of existing anti virus and intrusion
prevention products of the AV company, hence we have to take
into account the error rate of the identified infections, and the fact
that sometimes the same file is detected multiple times (e.g., cases
where users decide not to delete a file marked as malicious). Adding
a machine profile to the set of risky ones because the machine was
detected as infected a few times during the labeling period (e.g.,
6 months) might result in wrong assessments. Here, we show the
123456789LabelingperiodN(months)12345678910FeatureextractionperiodM(months)0.750.800.850.900.951.00AUCscoresSession F2:  Insights from Log(in)sCCS’17, October 30-November 3, 2017, Dallas, TX, USA1307Table 4: AUC varying the ground truth thresholds.
Tinf Tgrey AUC
10
50
100
0
3
0
3
0
3
0.965
0.968
0.978
0.981
0.981
0.983
Machines
21 690
Risky Clean
10 332
14 638
10 332
14 638
10 332
14 638
16 393
14 272
impact on results of the thresholds we use to identify clean and
risky profiles for our ground truth.
As discussed in Section 3.2, our ground truth comes from three
large datasets including known malware files and list of malware
infection records from IPS telemetry. We behave conservatively,
avoiding to add machines to the ground truth if there are any
reasons to doubt about the fact that they’re either infected or clean.
We define a machine as clean if it has neither any infection
records in the IPS telemetry dataset nor any files known to be
malware. To avoid misclassifications, we also consider as clean a
machine that has at most Tgrey unlabeled files. As seen in Table 4,
we used very strict thresholds to decrease the likelihood of labeling
an infected machine as clean due to false negatives in our data. We
behave in an analogously conservative way to label risky users:
we mark a profile as risky only if it is associated with at least
Tinf malicious events. To limit the risk of false positives, we again
experiment with conservative values in Table 4. We achieve the best
classification accuracy with Tgrey = 3 and Tinf = 100, the thresholds
we use for the remaining experiments.
6.2 Prediction Results
Once the main parameters are set, we evaluate the quality of Risk-
Teller’s predictions through our random forest classifier; we have
set a number of 800 trees as larger sizes do not improve neither the
stability nor the quality of results. Our classifier was run on the
dataset that includes file appearance logs from the machines of 18
enterprises. In Figure 4 we show the ROC curve obtained after a
10-fold cross validation. As the figure depicts, RiskTeller can predict
95% of the to-be-infected machines with only 5% false positives.
This is a significant improvement over the related work [19] which
had over 20% false positives for the same point in the ROC curve.
In addition to the cross-validation test, we conduct an experi-
ment to demonstrate the forecasting capability of RiskTeller. We
randomly select 50% of the to-be-infected machines to build the
prediction models processing the first six months of training pe-
riod. Then, we apply the models on the following 6 months to test
whether the remaining 50% of the machines that got infected could
be identified. We repeated this step 10 times to reduce the impact
of the random machine selection. The resulting AUC score is 0.95;
this validates our claim about RiskTeller’s prediction capabilities.
Figure 4: ROCs derived on the datasets
6.3 Feature Significance
To list the most discriminative features, we employ the mean de-
crease impurity methodology provided by the random forest clas-
sifiers. When training the trees in the random forest classifier, we
compute how much each feature decreases the weighted impurity
in the trees. Once the forest is built, we average the impurity de-
crease from each feature and rank them to identify the features that
contribute the most to the classification accuracy. In Table 5 we
list the strongest features among the 89 that RiskTeller processes,
and Figure 5 presents the mean decrease in impurity per feature
category. While all of the features do contribute to the ultimate
classification results, the temporal file download/creation behav-
ior of the users and the volume/diversity of file creation activities
observed on the machines have the highest impact on distinguish-
ing clean machine profiles from the risky ones. Surprisingly, the
infection history features are not as correlated. Previous work on
forecasting cyber security incidents [19] also found out that the
historical threat data is not as useful for prediction and therefore,
employed features about network misconfiguration details of the
enterprises. Unlike that work, RiskTeller works at a finer granular-
ity providing predictions at the machine level rather than at the
enterprise level, and the prediction accuracy is larger.
6.4 Semi-Supervised Label Propagation
In this section, we perform additional experiments with semi-su-
pervised learning (SSL) to highlight its merits. To this end, we
manipulate our ground truth to simulate two issues commonly
witnessed in real-world: the lack of balance between the sizes of
classes in the labeled data and inadequate number of labeled data
compared to the scale of the whole dataset. To obtain an imbalanced
ground truth that is also small in size, we randomly choose p%
of risky and q% of clean machine profiles in the ground truth to
form new ground truths. The remaining labels are hidden from the
classifier by signing them as unlabeled. Per different p and q values,
we repeat the random sampling of the ground truth 10 times and
take the average AUC and FPR values as the overall metric of the
classification accuracy.
0	20	40	60	80	100	0	5	10	15	20	25	30	35	40	Predic’on	Rate	(%)	False	Posi’ves	Rate	(%)	Session F2:  Insights from Log(in)sCCS’17, October 30-November 3, 2017, Dallas, TX, USA1308Table 5: Most discriminative features, grouping very similar ones.
Feature
Fraction of events in weekdays/weekend
Fraction of events from top-150 file hashes
# of patched apps
Total number of events
Quartiles for CVSS scores of patched apps
Distinct app count
Distinct file hashes
Unpatched app count
Fraction of files signed by [101 − 1000] prevalence signers
Monthly median number of events
Volume of downloads per app
Category
Temporal
Volume-based
Vulnerabilities
Volume-based
Vulnerabilities
Volume-based
Volume-based
Vulnerabilities
Prevalence-based
Temporal
Volume-based
25–29
# in Table 1 Contribution
0.075
0.060
0.041
0.026
0.024
0.023
0.021
0.020
0.018
0.018
0.017
7
2
36
67
20
16–17
5
22
1
53–57
Table 6: TPR of the random forest and semi-supervised
methods when sampling labels.
FPR
TPR
p
q
50% 0.1%
50% 0.5%
50% 1.0%
20% 0.1%
20% 0.5%
20% 1.0%
Semi-Supervised
Random Forest
15%
5% 10% 15%
5% 10%
77% 79% 80% 90% 95%
95%
90% 93% 93% 84% 94%
96%
90% 93% 94% 88% 94%
96%
73% 83% 84% 88% 93%
94%
89% 92% 94% 84% 93%
95%
91% 95% 96% 91% 95%
96%
Figure 5: The contribution of different feature categories to
the predictive modeling.
We set p as 20, 50 and q as 0.1, 0.5 and 1 respectively in the ex-
periments and run the RF and SSL modules of RiskTeller separately.
Table 6 illustrates the average AUC and FPR values corresponding
to each pair of p and q. As expected, the performances of the super-
vised random forest module deteriorate when the class imbalance
grows and the number of limited users decrease. In contrast, the
SSL method, which can estimate the labels of the unlabeled dataset