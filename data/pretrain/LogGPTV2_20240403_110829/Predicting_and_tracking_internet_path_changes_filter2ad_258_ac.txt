are so variable and the RuleFit models we obtained are so complex,
simple analytic models are not serious candidates as a basis for pre-
diction. We select a nearest-neighbor approach as it captures empir-
ical dependences effectively and ﬂexibly. Using only four features
avoids the dimensionality problems inherent to such predictors [5]
and allows for a very simple method, which we name NN4.
4.1.1 Method overview
Like all nearest-neighbor predictors, we compute predictions for
a virtual path with feature vector x based on training points with
126feature vectors that are ‘close’ to x. The ﬁrst challenge is to deﬁne
a meaningful distance metric. This is difﬁcult as feature domains
differ (prevalence is a fraction, the number of changes and previous
occurrences are integers, and route age is a real), have different
semantics, and impact virtual path changes differently.
To avoid the pitfalls of some more or less arbitrary choice of
distance metric, we instead partition the feature space into 4 di-
mensional ‘cubes’, or partitions, based on discretising each feature.
Discretisation creates artifacts related to bin boundaries and resolu-
tion loss; however, the advantages are simplicity and the retention
of a meaningful notion of distance for each feature individually. To
avoid rigid ﬁxed bin boundaries, for each feature we choose them
as equally-spaced percentiles of their corresponding distribution,
computed over all virtual path changes in the training set (as we
did for route age in Sec. 3.2).
We denote the partition containing the feature vector of path P at
time t as P(P, t) or simply P(P ). We predict the residual lifetime
of r = P (t) and the number of changes in the next δ interval as
the averages of the true values of these quantities over all training
points in the partition P(P ):
ˆL(r) = E[{L(Ps(ts)) | s ∈ P(P )}],
ˆNδ(P ) = E[{Nδ(Ps) | s ∈ P(P )}],
where training point s corresponds to the path Ps at time ts. Simi-
larly, we predict ˆIδ(r) = 1 if more than half the training points in
P(P ) change within a time interval δ:
ˆIδ(r) = (cid:5)E[{Iδ(Ps(ts)) | s ∈ P(P )}] + 0.5(cid:6).
The cost of a prediction in NN4 is O(1), while in RuleFit it is
O(r), where r is the number of rules in the model. NN4 can be
easily implemented, while RuleFit is available as a binary module
that cannot be accessed directly and requires external libraries.
4.1.2 Training
To allow a meaningful comparison in our evaluation, each train-
ing for NN4 reuses the virtual paths of some RuleFit training set.
Consider a virtual path P (ts) chosen for training. As ts pro-
gresses, the associated feature vector x(ts) moves between the dif-
ferent partitions. For example, for long-lived routes, x(ts) evolves
toward the partition with 100% prevalence, zero changes, no previ-
ous occurrences, and the oldest age bin (before resetting to zero age
etc. when/if the path changes). We need to sample this trajectory
in a way that preserves all important information about the changes
in the three prediction goals (L, Nδ, Iδ). Just as in RuleFit, we
need to supplement the changes that occur explicitly in the dataset
with additional training points occurring inbetween change points.
Here we need to add additional samples to capture the diversity not
only of age, but also the other three dimensions. In fact we can
do much better than a discrete sampling leading to a set of train-
ing time points. From the dataset we can actually calculate when
the path enters and exits the partitions it visits, its sojourn time in
each, and the proportions of the sojourn time when a prediction
goal takes a given value. For each partition (and prediction goal)
we are then able to calculate the exact time-weighted average of the
value over the partition. The result is a precomputed prediction for
each partition traversed by the path that emulates a continuous-time
sampling. Final per-partition predictions are formed by averaging
over all paths traversing a partition.
4.1.3 Conﬁguration
Apart from δ, the only parameter of our predictor is the num-
ber of bins b we use to partition each feature. We choose a shared
)
h
4
I
(
e
t
a
R
r
o
r
r
E
n
o
i
i
t
c
d
e
r
P
 0.3
 0.28
 0.26
 0.24
 0.22
 0.2
 0.18
2
Min, Median, Max
 20
 40
 80
Number of Feature Bins
 60
 100
Figure 5: Impact of the number of feature bins on prediction
accuracy (test points with age < 12 hours).
number of bins for parsimony, since when studying each feature
separately (not shown) the optimal point was similar for each. The
tradeoff here is clear. Too few bins and distinct change behaviors
important for prediction are averaged away. Too many bins and
partitions contain insufﬁcient training information resulting in er-
ratic predictions. We found in Sec. 3.4 that six bins were sufﬁcient
for route age. We now examine the three remaining features.
Fig. 5 shows EIδ with δ = 4 hours as a function of b, restricting
to test points with route age below 12 hours where the b depen-
dence is strongest. We see that values in [6, 20] achieve a good
compromise. We use b = 10 in what follows.
4.2 NN4: Evaluation
We evaluate the prediction accuracy of NN4 and compare it to
our operational benchmark, RuleFit, discovering in the process the
limitations of this kind of prediction in general. We will ﬁnd that
only very rough prediction is feasible, but in the next section we
show that it is nonetheless of great beneﬁt to path tracking. For
each method we generate new training and test sets in order to test
the robustness of the conﬁguration settings determined above.
4.2.1 Predicting residual lifetime
Fig. 6(Top) shows the distribution of EL(r), the relative error of
ˆL(r). An accurate predictor would have a sharp increase close to
EL = 0 (dotted line), but this is not what we see. Speciﬁcally, only
33.5% of the RuleFit and 31.1% of the nearest-neighbor predictions
have −0.5 ≤ EL ≤ 1 (see symbols on the curves). Predictions
miss the true residual lifetimes by a signiﬁcant amount around 70%
of the time. As this is true not only of NN4 but also for RuleFit,
we conjecture that accurate prediction of route residual lifetimes
is too precise an objective with traceroute-based datasets. It does
not follow, however, that ˆL(r) is not a useful quantity to estimate.
We can still estimate its order of magnitude well in most cases, and
this is enough to bring important beneﬁts to path tracking, as we
show later. The error of NN4 is considerable larger than that of the
benchmark but it is of the same order of magnitude.
4.2.2 Predicting number of changes
Fig. 6(Bottom) shows the distribution of ENδ , the error of ˆNδ,
for NN4 for all test points with route age less than 12 hours.
The errors for RuleFit are similar. Errors for test points in routes
older than 12 hours are signiﬁcantly smaller (not shown) because
a predictor can perform well simply by outputting “no change”
( ˆNδ = 0). We focus here on the difﬁcult case of A < 12h.
Unlike residual lifetimes, the sharp increase near zero means
most predictions are accurate. For example, 90.2% of test points
have −2 < EN4h
< 2, and accuracy increases for smaller val-
127s
t
n
o
P
i
t
s
e
T
f
o
n
o
i
t
c
a
r
F
.
m
u
C
s
t
n
o
P
i
t
s
e
T
f
o
n
o
i
t
c
a
r
F
.
m
u
C
 1
 0.8
 0.6
 0.4
 0.2
 0
-1
 0
 1
 2
RuleFit
NN4
 3
 4
 5
Residual Lifetime Relative Prediction Error (EL)
 1
 0.8
 0.6
 0.4
 0.2
 0
NN4
δ = 24h
4h
1h
-4
-2
 0
 2
 4
Number of Changes Prediction Error (EN)
 0.5
 0.4
 0.3
 0.2
 0.1
)
δ
I
(
e
a
R
t
r
o
r
r
E
n
o
i
i
t
c
d
e
r
P
NN4
δ = 24h
4h
1h
 0
 0
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 0
 0.4
 0.2
 0.8
Route Prevalence (τ = 1 day)
 0.6
δ = 4h
δ = 1h
RuleFit
NN4
No Change
 0.4
 0.2
 0.8
Route Prevalence (τ = 1 day)
 0.6
 1
 1
)
δ
I
(
t
e
a
R
r
o
r
r
E
n
o
i
i
t
c
d
e
r
P
Figure 6: Distribution of prediction error. Top: L; Bottom: Nδ
based on NN4 (for age < 12h).
Figure 7: EIδ as a function of route prevalence for various val-
ues of δ. Top: NN4; Bottom: RuleFit comparison.
ues of δ. However, predicting the number of changes over long
intervals such as 24 hours cannot be done accurately. Note that
simply guessing that Nδ = 0 also works well for very small δ. Al-
though Nδ is a less ambitious target than L, it remains difﬁcult to
estimate from traceroute-type data. Again, however, prediction is
sufﬁciently good to bring important tracking beneﬁts.
4.2.3 Predicting a change in next δ interval
We now study whether the current route of a given path will
change within the next time interval of width δ. We expect Iδ to be
easier to predict than L or Nδ.
Fig. 7(Top) shows NN4’s prediction error as a function of route
prevalence for δ between 1 hour and 1 day (results for RuleFit are
very similar and are omitted for clarity). We group route prevalence
into ﬁxed-width bins and compute the error from all test points
falling within each bin (these bins are distinct from the constant-
probability bins underlying NN4’s partitions). For each bin, we
show the minimum, median, and maximum error among the 40
training and test set combinations. Such a breakdown is very use-
ful as it allows us to resolve where prediction is more successful, or
more challenging. For example, since routes with prevalence 1 are
very common, a simple global average over all prevalence values
would drown out the results from routes with prevalence below 1.
First consider the results for δ = 1h and 4h. The main obser-
vation is that error drops as prevalence increases. This is because
routes with high prevalence are unlikely to change, and a prediction
of “no change” ( ˆIδ(r) = 0), which the predictors output increas-
ingly often, becomes increasingly valid as prevalence increases.
We also see that, for all prevalence values, error is lower for smaller
δ. This makes intuitive sense since prediction further into the future
is in general more difﬁcult. More precisely, the probability that a
route will change in a time interval δ decreases as δ decreases, and
predictors exploit this by predicting “no change” more often.
The situation is more complex when δ = 24h, with errors be-
ginning low and increasing substantially before ﬁnally peaking and
then decreasing at very high prevalence. This happens because for
larger values of δ, routes with low prevalence have a high probabil-
ity of changing. Predictors exploit this and output I24h(r) = 1 more
often (in fact more than 80% of the time for paths with prevalence
under 0.2). Prediction error is highest at intermediate prevalence
values, as these routes have a probability close to 50% of changing
in the next 24 hours. Finally, prediction error decreases for routes
with high prevalence: as routes become stable the same mechanism
noted above for smaller δ kicks in.
We now provide a comparison against RuleFit, focusing on small
to medium δ. Fig. 7(Bottom) shows that NN4 and RuleFit have
equivalent prediction accuracy across all values of prevalence. In
fact NN4 is marginally (up to 2%) better here, where we used
the default RuleFit conﬁguration. Their performance is close to
identical when using the more generous RuleFit conﬁguration (see
Sec. 3) with 500 rules and 12 age bins.
The plot also shows results for a simple baseline predictor that
always predicts ˆIδ(r) = 0 (no change). Our predictor is better
for routes with prevalence smaller than 0.7 which are more likely
to change than not, but for high-prevalence routes all predictors
predict “no change” and are equivalent. For routes with prevalence
below 0.7, NN4 reduces the baseline predictor’s EI4h from 0.296
to 0.231 (22%), and EI1h from 0.163 to 0.131 (20%).
Summary. Prediction is easiest when δ is small and prevalence is
high. This is a promising result as most Internet routes are long-
lived and have high prevalence; moreover, applications like topol-
ogy mapping need to predict changes within short time intervals.
NN4 predicts Iδ reasonably well, and errors ultimately fall to just
a few percent as route prevalence increases and as δ decreases. We
have tested the sensitivity to training and test sets, monitor choice,
and overall probing rate, and found it to be very low.
1285. TRACKING VIRTUAL PATH CHANGES
We now apply our ﬁndings to the problem of the efﬁcient and
accurate tracking of a set of virtual paths over time. We describe
and evaluate our tracking technique, DTRACK.
5.1 DTRACK overview
Path tracking faces two core tasks: path change detection (how
best to schedule probes to hunt for changes), and path remapping
(what to do when they are found). For the latter, inspired by Fast-
Mapping [8], DTRACK uses Paris traceroute’s MDA to accurately
measure the current route of monitored paths both at start up and
after any detection of change. This is vital, since confusing path
changes with load balancing effects makes ‘tracking’ meaningless.
For change detection, DTRACK is novel at two levels.
Across paths: paths are given dedicated sampling rates guided by
NN4 to focus effort where changes are more likely to occur. With-