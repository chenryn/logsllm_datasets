structured around a concurrent, edge-triggered event loop
wherein multiple worker threads receive network messages
and directly process client requests. Whereas common de-
sign patterns would distinguish between I/O threads and
worker threads, HyperDex combines these two functions to
avoid internal queuing and communication delays. Because
the event-loop is edge-triggered, unnecessary interaction with
the kernel is minimized. Socket buﬀer management ensures
that threads never block in the kernel when sending a mes-
sage and consumes a constant amount of memory per client.
HyperDex makes extensive use of lock-sharding and lock-
free datastructures to reduce the probability of contention
whenever possible. Per-key datastructures are protected by
an array of locks. Although nothing prevents two threads
from contending for the same lock to protect diﬀerent keys,
the ratio of locks to threads is high enough to reduce this oc-
currence to 1 in 106. Global datastructures, such as lookup
tables for the per-key datastructures, are concurrent through
the use of lock-free hash tables. Our implementation ensures
that background threads may safely iterate over global state
while worker threads insert and remove pointers to per-key
state.
Finally, the use of cache-conscious, constant-time data
structures reduces the overheads associated with common
operations such as linked-list and hash-table management.
5.2 HyperDisk: On-Disk Data Storage
A key component of server performance for any key-value
store is the storage back end used to organize data on disk.
Since hyperspace hashing is agnostic to the choice of the
back end, a number of design options are available. At one
extreme, we could have used a traditional database to store
all the objects of a server in a single, large, undiﬀerentiated
pool. While this approach would have been the simplest
from an implementation perspective, it would make Hyper-
Dex dependent on the performance of a traditional database
engine, require manual tuning of numerous parameters, and
subject the system to the vagaries of a query optimizer.
Instead, HyperDex recursively leverages the hyperspace
hashing technique to organize the data stored internally on
a server. Called HyperDisk, this approach partitions the
region associated with a server into smaller non-overlapping
sub-regions, where a sub-region represents a ﬁle on disk, and
objects are located in the ﬁle that contains their coordinate.
Each ﬁle is arranged as a log, where insertions and deletions
operate on the tail, and where a search operation linearly
scans through the whole ﬁle.
HyperDisk’s hashing scheme diﬀers from the standard hy-
perspace hashing in two ways: ﬁrst, HyperDisk partitions
only the region assigned to a HyperDex server; and second,
HyperDisk may alter the mapping dynamically to accommo-
date the number of objects stored within a region. Overall,
recursive application of hyperspace hashing enables Hyper-
Dex to take advantage of the geometric structure of the data
space at every system layer.
5.3 Distributed Coordination
In HyperDex, the hyperspace mapping is created and man-
aged by a logically centralized coordinator. Since a physi-
cally centralized coordinator would limit scalability and pose
a single point of failure, the HyperDex coordinator is imple-
mented as a replicated state machine.
It relies on a co-
ordination service [2, 9, 27] to replicate the coordinator on
multiple physical servers. The coordinator implementation
ensures that servers may migrate between coordinators so
that no coordinator failure leads to correlated failures in the
system. The coordinator directs all failure recovery actions.
Servers may report observed failures to the coordinator, or
the coordinator may directly observe failures through peri-
odic failure detection pings to servers.
Overall, the replicated state machine implementation en-
sures that the coordinator acts as a single, coherent com-
ponent with well-deﬁned state transitions, even though it is
comprised of fault-tolerant, distributed components.
6. EVALUATION
We deployed HyperDex on both a small and medium-
size computational cluster and evaluated the performance
of each deployment using the Yahoo! Cloud Serving Bench-
mark (YCSB) [15], an industry-standard benchmark for cloud
storage performance. Our evaluation also examines the per-
formance of HyperDex’s basic operations, speciﬁcally, get,
put, and search, using targeted micro-benchmarks. These
micro-benchmarks isolate speciﬁc components and help ex-
pose the performance impact of design decisions. For both
)
s
/
p
o
d
n
a
s
u
o
h
t
(
t
u
p
h
g
u
o
r
h
T
40
30
20
10
5
0
Load
Workload A
Workload B
Workload C
Workload D
Workload F
Workload E
Cassandra MongoDB HyperDex
Figure 4: Average throughput for a variety of real-
world workloads speciﬁed by the Yahoo! Cloud
Serving Benchmark. HyperDex is 3-13 times faster
than Cassandra and 2-12 times faster than Mon-
goDB. Workload E is a search-heavy workload,
where HyperDex outperforms other systems by
more than an order of magnitude.
YCSB and the micro-benchmarks, we compare HyperDex
with Cassandra [32], a popular key-value store for Web 2.0
applications, and MongoDB [37], a distributed document
database.
The performance benchmarks are executed on our small,
dedicated lab-size cluster in order to avoid confounding is-
sues arising from sharing a virtualized platform, while the
scalability benchmarks are executed on the VICCI [42] test-
bed. Our dedicated cluster consists of fourteen nodes, each
of which is equipped with two Intel Xeon 2.5 GHz E5420
processors, 16 GB of RAM, and a 500 GB SATA 3.0 Gbit/s
hard disk operating at 7200 RPM. All nodes are running
64-bit Debian 6 with the Linux 2.6.32 kernel. A single gi-
gabit Ethernet switch connects all fourteen machines. On
each of the machines, we deployed Cassandra version 0.7.3,
MongoDB version 2.0.0, and HyperDex.
For all tests, the storage systems are conﬁgured to pro-
vide suﬃcient replication to tolerate one node failure. Each
system was conﬁgured to use its default consistency set-
tings. Speciﬁcally, both Cassandra and MongoDB provide
weak consistency and fault-tolerance guarantees; because ac-
knowledgments are generated without full replication, small
numbers of failures can lead to data loss. In contrast, Hy-
perDex utilizes value-depending chaining and, as a result,
always provides clients with strong consistency and fault-
tolerance, even in the presence of failures. Since MongoDB
allocates replicas in pairs, we allocate twelve machines for
the storage nodes, one machine for the clients, and, where
applicable, one node for the coordinator. HyperDex is con-
ﬁgured with two subspaces in addition to the key subspace
to accommodate all ten attributes in the YCSB dataset.
6.1 Get/Put Performance
High get/put performance is paramount to any cloud-
based storage system. YCSB provides six diﬀerent work-
loads that exercise the storage system with a mixture of
request types and object distributions resembling real-world
applications (Table 1). In all YCSB experiments, the data-
base is preloaded with 10,000,000 objects and each opera-
tion selects the object and operation type in accordance with
the workload speciﬁcation. Figure 4 shows the throughput
achieved by each system across the YCSB workloads. Hy-
100
80
)
%
(
F
D
C
60
40
20
0
1
Cassandra (R)
Cassandra (U)
MongoDB (R)
MongoDB (U)
HyperDex (R)
HyperDex (U)
10
50
Latency (ms)
Figure 5: GET/PUT performance. Latency distribu-
tion for Workload A (50% reads, 50% updates, Zipf
distribution).
100
80
)
%
(
F
D
C
60
40
20
0
1
Cassandra (R)
Cassandra (U)
MongoDB (R)
MongoDB (U)
HyperDex (R)
HyperDex (U)
10
50
Latency (ms)
Figure 6: GET/PUT performance. Latency distribu-
tion for Workload B (95% reads, 5% updates, Zipf
distribution). HyperDex maintains low latency for
reads and writes.
perDex provides throughput that is between a factor of two
to thirteen higher than the other systems. The largest per-
formance gains come from improvements in search perfor-
mance. Signiﬁcant improvements in get/put performance
is attributable mostly to the eﬃcient handling of get op-
erations in HyperDex. Our implementation demonstrates
that the hyperspace construction and maintenance can be
realized eﬃciently.
In order to gain insight into the performance of the system,
we examine the request latency distributions of the diﬀerent
systems under all read/write workloads. HyperDex’s perfor-
mance is predictable: all reads complete in under 1 ms, while
a majority of writes complete in under 3 ms. Cassandra’s
latency distributions follow a similar trend for workloads B,
C, D and F and show a slightly diﬀerent trend for workload
A. MongoDB, on the other hand, exhibits lower latency than
Cassandra for all operations. For all workloads, HyperDex
completes 99% of operations sooner than either Cassandra
and MongoDB. Figures 5 and 6 show the latency distribu-
tions for workloads A and B respectively.
For completeness, we present the performance of all three
systems on a write-heavy workload. Figure 7 shows the la-
tency distribution for inserting 10,000,000 objects to set up
the YCSB tests. Consistency guarantees have a signiﬁcant
eﬀect on the put latency. MongoDB’s default behavior con-
siders a put complete when the request has been queued in
Name Workload
A
B
C
D
E
F
Key Distribution
Zipf
50% Read/50% Update
Zipf
95% Read/5% Update
Zipf
100% Read
Temporal
95% Read/5% Insert
95% Scan/5% Insert
Zipf
50% Read/50% Read-Modify-Write Zipf
Sample Application
Session Store
Photo Tagging
User Proﬁle Cache
User Status Updates
Threaded Conversations
User Database
Table 1: The six workloads speciﬁed by the Yahoo! Cloud Serving Benchmark. These workloads model
several applications found at Yahoo! Each workload was tested using the same YCSB driver program and
system-speciﬁc Java bindings. Each object has ten attributes which total 1 kB in size.
100
80
)
%
(
F
D
C
60
40
20
0
1
Cassandra
MongoDB
HyperDex
10
50
Latency (ms)
100
80
)
%
(
F
D
C
60
40
20
0
1
Cassandra
MongoDB
HyperDex
10
100
1000
Latency (ms)
Figure 7: PUT performance. Latency distribution for
10,000,000 operations consisting of 100% insertions.
Each operation created a new object under a unique,
previously unused key. Although fast, HyperDex’s
minimum latency is bounded by the length of the