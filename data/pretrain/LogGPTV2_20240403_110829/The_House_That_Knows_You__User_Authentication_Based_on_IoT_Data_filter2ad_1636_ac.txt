decided to exclude users with less than 6 sessions in total.
We perform our analysis on six users: 1, 3, 4, 6, 8, and 10
who generate more than 6 sessions. We further excluded 7
sessions in total during which the network monitoring tool
was not active.
5.1 Comparison of Feature
Representations
We experiment with three different feature representations
as discussed in Section 3.4: Device-only (420 features); Domain-
only (2910 features); and Both device and domain (3330 fea-
tures). We considered time windows of different length
∆ ∈ {5, 10, 15, 20, 25, 30} minutes. Each time window is cre-
ated by sliding windows over the user sessions at one-minute
intervals. If a session is shorter than ∆, we generate only
one window covering the entire session.
To evaluate our ML models, we perform 7-fold cross-
validation across sessions. Thus, we keep one session for
each user for testing and select all other sessions in training.
8
We consider first a Random Forest (RF) classifier to evaluate
feature representations and time window length. RF is an
ensemble learning method that constructs many decision
trees using a subset of features chosen at random at each
split. RF are robust models that work well in multi-class
classification settings with many features, as ours. We will
experiment and compare other models next section.
We plot the recall, precision, and F1 score of the three
feature representation methods for varying time windows
in Figure 6. The results show that the model performance
generally increases as the time window increases until 25
minutes, at which point the best results are obtained. We
thus select a time window of 25 minutes for the rest of our
experiments. We note the inherent tradeoffs between classi-
fication accuracy and the length of observation period for
computing authentication scores. With smaller windows
length, we can generate authentication scores after limited
observation, at the cost of decreased classification perfor-
mance. Comparing different feature representations, we ob-
serve that Device-only performs well, and it is very similar to
Both device and domain. The worst performance is provided
by the Domain-only representation.
We also generate user confusion matrix for the RF models
in Table 5. Interestingly, we obtain very good classification
firetvechoplusinvokebrewergoogle-home-miniechospotfridget-echodotechodotroku-tvlgtv-wiredikettlet-philips-hubmicrowavesmartthings-hubDevices1103468UsersTotal Bytes0.01.53.04.56.01e8firetvechoplusinvokebrewergoogle-home-miniechospotfridget-echodotechodotroku-tvlgtv-wiredikettlet-philips-hubmicrowavesmartthings-hubDevices1103468UsersPacket Count0100000200000300000400000500000firetvechoplusinvokebrewergoogle-home-miniechospotfridget-echodotechodotroku-tvlgtv-wiredikettlet-philips-hubmicrowavesmartthings-hubDevices1103468UsersAverage Packet Length0400800120016002000amazonamazonawsbingcloudfrontgooglegoogleusercontentgooglevideogvt1iheartihrhlslgenonestreamguys1youtubeDomains1103468UsersTotal Bytes0.81.62.43.21e8amazonamazonawsbingcloudfrontgooglegoogleusercontentgooglevideogvt1iheartihrhlslgenonestreamguys1youtubeDomains1103468UsersPacket Count100000200000300000400000amazonamazonawsbingcloudfrontgooglegoogleusercontentgooglevideogvt1iheartihrhlslgenonestreamguys1youtubeDomains1103468UsersAverage Packet Length06001200180024003000Figure 4: Total amount of data transmitted by five devices for User 6 in three sessions.
Figure 5: Total amount of data transmitted by five devices for User 10 in three sessions.
Figure 6: Accuracy metrics for different time windows for 6 users using different
feature representations. From left to right: device based features, domain based
features, device and domain based features.
for some of the users (for instance, User 4), but worse results
for other users (e.g., User 6). User 6 is mis-classified as User
3, meaning that their behavior is fairly similar. Thus, the clas-
sification performance varies a lot across users. We believe
there are multiple factors contributing to this phenomenon,
among them the variability of user behavior and the amount
of data used for training these models.
5.2 Model Comparison
We compare three different models: Logistic Regression with
L1 regularization (LR), Random Forest (RF), and Gradient
Boosting (GB) for time windows of length 25 minutes. We
varied the hyper-parameters of these classifiers, and we se-
lect 2000 estimators for RF, and 2000 estimators for GB, with
learning rate set at 0.01 and maximum depth at 3. For this
experiment, we use the Device-only features. In Table 6 we
9
show user-level performance metrics for the three classifiers
when all six users are considered. As observed, GB outper-
forms both RF and LR with its average precision and recall
at 0.81 and 0.8, respectively. However, the performance of
RF and GB is fairly close, but LR has lower classification
performance (being a linear model with lower complexity).
We also generate ROC curves when training six classifiers,
each one for classifying one of the user classes versus the
rest in Figure 7. Figure 8 shows the micro-averaged results
for each of three models with six users.
Additionally, we consider a setting with only five users
(removing User 6) and show classification results in Table 9
in the Appendix. Surprisingly, by removing one of the users,
we obtain classification precision of 0.92 and recall of 0.92
with GB, an increase of more than 10% compared to the
05101520Time (min)102103104105106107108Bytes (log)Total amount of data transmitted, User 6, Session 1invokegoogle-home-miniechospotmicrowaveikettle05101520Time (min)102103104105106107108Bytes (log)Total amount of data transmitted, User 6, Session 2invokegoogle-home-miniechospotmicrowaveikettle05101520Time (min)102103104105106107108Bytes (log)Total amount of data transmitted, User 6, Session 3invokegoogle-home-miniechospotmicrowaveikettle05101520Time (min)102103104105106107108Bytes (log)Total amount of data transmitted, User 10, Session 1invokegoogle-home-miniechospotmicrowaveikettle05101520Time (min)102103104105106107108Bytes (log)Total amount of data transmitted, User 10, Session 2invokegoogle-home-miniechospotmicrowaveikettle05101520Time (min)102103104105106107108Bytes (log)Total amount of data transmitted, User 10, Session 3invokegoogle-home-miniechospotmicrowaveikettle5 min10 min15 min20 min25 min30 minTime Interval0.00.20.40.60.81.0AccuracyResults for Device Features - 6 usersaverage recallaverage precisionaverage f15 min10 min15 min20 min25 min30 minTime Interval0.00.20.40.60.81.0AccuracyResults for Domain Features - 6 usersaverage recallaverage precisionaverage f15 min10 min15 min20 min25 min30 minTime Interval0.00.20.40.60.81.0AccuracyResults for Domain + Device Features - 6 usersaverage recallaverage precisionaverage f11 3
1
0
5
3
47
0
4
0
1
6
9
0
8
0
1
10 1
1
4
1
0
20
0
0
0
6 8 10 Count Recall Precision F1
0.71
1
0.88
1
1
0.93
0.38
4
0.66
0
0
0.57
0.71
0.97
0.90
0.28
0.57
0.57
0.71
0.81
0.95
0.57
0.8
0.57
7
48
22
14
7
7
0
0
0
0
4
1
0
0
0
1
2
4
Table 5: RF user confusion matrix and accuracy metrics for 25 minute intervals: avg. recall=0.8, avg. precision=0.78
Precision
Logistic Regression
Recall
F1
0.5
0.42
0.59
0.52
0.70
0.81
0.55
0.71
0.42
0.46
0.71
0.71
0.60
0.60
0.6
0.69
0.62
0.45
0.5
0.71
0.62
Recall
0.71
0.97
0.90
0.28
0.57
0.57
0.80
User 1
User 3
User 4
User 6
User 8
User 10
Average
Random Forest
Precision
Precision
Gradient Boosting
F1
0.79
0.9
0.95
0.51
0.54
0.53
0.80
0.75
0.86
1.0
0.53
0.75
0.5
0.81
Recall
0.85
0.93
0.90
0.5
0.42
0.57
0.80
F1
0.71
0.88
0.93
0.38
0.66
0.57
0.78
0.71
0.81
0.95
0.57
0.8
0.57
0.78
Table 6: Comparison of three ML classifiers per user and micro-average results for 6 users.
setting of six users. Figure 9 in the Appendix shows the user-
level ROC curves for five users and Figure 10 in the Appendix
shows the micro-averaged results for each of three models
with five users.
5.3 High-Confidence Ensemble Model
So far, we showed that models based on device-only features
perform relatively well at classifying users at time windows
of length 25 minutes. As we discussed initially, different ap-
plications might have different requirements in terms of the
confidence offered by the authentication score. For instance,
a financial application might require a high confidence in
the user classification module before allowing a financial
transaction. When the authentication score is used as a
factor in a multi-factor authentication system, it could be
acceptable to not compute an authentication score when
the confidence is very low.
To account for these settings, we propose the idea of de-
signing an ensemble of two models that are built indepen-
dently and can be used in combination to increase the con-
fidence in the authentication score. Our main insight is that
we can build models leveraging the device-only and domain-
only features independently, and compute an authentication
score only when the two models agree on the user prediction.
As a result, the overall confidence in the classification will
increase. The cost is that, in situations when the two models
disagree, no authentication score will be computed. In this
case, the upper-level applications might wait for additional
time, or leverage other authentication factors.
10
We test the ensemble of two Gradient Boosting models,
one built on the device-only features, and the second on the
domain-only features. We show the results of our ensemble
user classification model for six users in Table 7 and for five
users in Table 8. The F1 score of the six-user model improves
from 0.8 with a single GB model to 0.86 with the ensemble.
In the ensemble, the models do not agree on 32 out of 105
sessions. For five users, the ensemble F1 score reaches 0.97
(compared to 0.92 for a single GB model). In this case, the
ensemble does not compute a score on only 16 out of 91
sessions. Therefore, the ensemble can reliably increase the
confidence of the model, at the cost of not always providing
an authentication scores.
6 RELATED WORK
Behavior-based authentication systems are identifying users
based on their behavior. Implicit authentication [25] shows
the applicability of behavior modeling for authentication
by utilizing the call/message information, browser activity,
and GPS history. Itus provides an extensible implicit authen-
tication framework for Android [17]. A survey of multiple
implicit authentication methods is given in [16]. Progres-
sive Authentication [23] models user behavior on mobile
devices by combining biometric features and sensor data. An-
other emerging continuous authentication method leverages
the sensor information from wearable devices (e.g. smart
watches, activity trackers, glasses, bracelets) to learn user
behavior [12, 18, 22].
Figure 7: User-level ROC curves for 25 minute time windows for three models with 6 users.
1 3
0
2
35
0
0
0
3
0
0
1
0
0
4
0
0
13
0
0
0
1
3
4
6
8
10
Total
6 8 10 Disagreed Agreed Recall Precision F1
0.66
1
0.92
3
0
1.0
0.6
6
0.75
0
0.8
1
0.86
0.66
0.92
1.0
0.66
0.75
0.66
0.86
0.66
0.92
1.0
0.54
0.75
1.0
0.87
4
10
9
5
3
1
32
3
38
13
9
4
6
73
0
0
0
0
3
1
0
0
0
0
0
4
Table 7: Confusion matrix for a Gradient Boosting ensemble with one model using the device features, and the
other the domain features for 25 minute window for 6 users. If the classifiers disagree, no authentication score is
computed.
1
3
0