in the local memory of each Cassandra client; so when a request
arrives, its decision can be read directly from the client’s memory.
Fault tolerance: Finally, we stress test our prototype of E2E by dis-
connecting the E2E controller from the Cassandra testbed. Figure 18
shows a time-series of the QoE gain of requests. We disconnect
the controller at the 25th second. First, we see that Cassandra’s
replica selection still uses the latest E2E’s decisions cached in the
lookup table, so although the QoE gain drops (as the lookup table
becomes stale), it is still better than the default policy. At the 50th
second, a backup controller is automatically elected, and by the
75th second, the new controller starts to make the same decisions
as if the controller was never disconnected.
7.4 In-depth analysis
Operational regime: Figure 19 tests E2E’s performance across
a wide range of workloads, along three dimensions that influence
E2E’s performance. We synthetically generate requests by drawing
external delays and server-side delays from two normal distribu-
tions, respectively, and test them on the trace-driven simulator
05101520123(a) Our tracesQoE gain (%)Page typeCassandraRabbitMQ(b) Testbed IdealizedE2E(this work)Slope-based.3.6.6.7.8.91(a)QoE (Normalized) Our traces.6.7.8.91(b)Throughput (Normalized)Cassandra.6.7.8.91(c) RabbitMQE2ESlope-basedDefault10-1100101120160200(a)CPU usage (%)Requests per second10-1100101120160200(b)Memory usage (%)Requests per secondTotal overheadAdditional overhead10-1101103105E2E(basic)SpatialcoarseningSpatial + temporalcoarsening24681012Per-request decisiondelay (ms)QoE gain (%)Decision delay (y1)QoE gain (y2)8910 0 5 10 15 20 25 30 35QoE gain (%)Time (sec.)w/o failurew/ failureE2E: Embracing User Heterogeneity to Improve QoE on the Web
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Figure 19: The impact of three key workload dimensions on E2E’s
effectiveness. The red spot shows where the workload in our traces lies.
Figure 20: Sensitivity of QoE improvement to prediction errors in
external delay and requests per second.
using the QoE model from Figure 3. Although the server-side and
external delays in our traces do not exactly follow normal distri-
butions, modeling them in this way allows us to test E2E’s perfor-
mance under different distribution conditions. For instance, we can
test the impact of increasing the mean of server-side delay on E2E’s
performance while keeping the external delay distribution fixed.
We set the default mean and variance of each distribution to
match those of the page type 1 requests in our traces, and vary
one dimension at a time. We see that at the beginning, E2E does
not yield any QoE gain, since there is no variability in the external
and server-side delays for it to exploit. Then, the QoE gain of E2E
starts to grow almost linearly with the server-side/external delay
ratio, external delay variance, and server-side delay variance, which
confirms that E2E is able to utilize the variance in external and
server-side delays. To put this in the perspective of our traces, the
workload in our traces is on the “fast-growing” part of all curves
(red spots in Figure 19). This means we will see more QoE gain if
the workload moves to the right in any of these dimensions.
Robustness to prediction errors: Figure 20 shows the impact
that prediction errors, in the external delays and the number of
requests per second (RPS), have on E2E’s performance. We feed
page type 1 requests to the Cassandra testbed (speedup ratio 20x),
and inject a controllable error on the actual value to obtain the
estimated value. Figure 20(a) shows that even if the external delay
prediction is off by 20% on each request, E2E still retains over 90% of
its QoE gain. Predicting the external delay with 20% (or 100-200ms)
error seems reasonable for most users [41]. Figure 20(b) shows that
E2E retains 91% of its QoE gain if the RPS is predicted with 10%
error. Empirically, we find that 10% prediction error is possible
when using the RPS history from the last 10 seconds (not shown).
QoE fairness: A natural concern is that E2E may create a less
fair QoE distribution. As an example, we use the QoE distributions
of E2E and the default policy from Figure 14(a) and page type 1.
We calculate Jain’s Fairness Index of the requests’ QoE values, and
Figure 21: E2E vs. Timecard (with different total delay deadlines).
find that E2E’s Jain index (0.68) is lower but still very close to that
of the default policy (0.70). This is because E2E only deprioritizes
requests that are insensitive to QoE; these requests experience only
a marginal improvement in QoE when using the default policy.
E2E vs. deadline-driven scheduling: Unlike E2E, some prior
work (e.g., [21, 41]) models the impact of total delay on QoE as a
hard deadline: QoE drops to zero immediately after the total delay
exceeds the deadline. We use Timecard [41] as a canonical exam-
ple of a deadline-driven scheduling policy, and compare E2E to it.
Timecard sets a total delay deadline and, given the external delay
of each request, tries to maximizes the number of requests served
by the deadline. We compare E2E with Timecard under total de-
lay deadlines of 2.0, 3.4, and 5.9 seconds, using RabbitMQ as the
testbed. As Figure 21 shows, the QoE gain of E2E is consistently
better than Timecard under different deadline settings. This is be-
cause the deadline-driven scheduler is agnostic to the different QoE
sensitivities of requests that have already exceeded the deadline.
8 RELATED WORK
We briefly survey the most related work on web QoE, cloud resource
allocation, and web performance measurements.
Web QoE modeling/optimization: QoE has been intensively
studied in the context of web services (e.g., [10, 22]), mobile apps
(e.g., [9]), video streaming (e.g., [11, 23]), and measurement tools
(e.g., [48]). Prior work (e.g., [14, 25, 39]) has observed a similar non-
linear relationship between page loading time and QoE. Although
E2E uses a specific QoE model (based on our trace analysis), it
can benefit from more precise models of how page loading time
affects QoE. Unlike prior QoE optimization techniques that tune
client-side knobs [16, 38] or provide server-side resources for indi-
vidual sessions (e.g., [42, 43]), E2E intelligently allocates server-side
resources shared across a large number of heterogeneous users.
Web service resource allocation: There is a large literature
on cutting the tail/median server-side delays through better web
resource management, including distributed databases (e.g., [45,
49]), partition-aggregation workloads (e.g., [28, 32]), and caching
(e.g., [12, 13]). Cloud providers optimize WAN latency through bet-
ter server selection (e.g., [18, 35]) and WAN path selection [44, 50].
E2E is conceptually compatible with many existing resource sharing
techniques (e.g., replica selection and message scheduling). What
distinguishes E2E is that it does not seek to minimize the median or
tail performance; instead, it takes into account the QoE sensitivity
of different users when allocating server-side resources.
End-to-end performance analysis: There have been attempts to
measure the contribution of cloud, WAN, and client-side devices to
end-to-end delays [19, 20, 40]. Our observations on heterogeneous
QoE sensitivity corroborate some prior work (e.g., [20]) that show
01020300.2.4.6.81Our tracesQoE gain (%)(a) Server delay /external delay.511.52Our traces(b) Std over mean of  external delay.511.52Our traces(c) Std over mean of  server side delay048120%5%10%15%20%QoE gain (%)(a) Relative prediction error(External delay)0%5%10%15%20%(b) Relative prediction error(RPS)RabbitMQCassandra0510152.03.45.9QoE Gain (%)Total delay deadline  set by Timecard (sec.)E2E (our work)TimecardSIGCOMM ’19, August 19–23, 2019, Beijing, China
X. Zhang et al.
that cloud-side delays are not a constant fraction of end-to-end
delays for all users. These studies offer useful insights for improv-
ing web service infrastructure [17, 31, 36] and building real-time
resource management systems [8, 21, 41].
The works most closely related to E2E are Timecard [41] and
DQBarge [21], which share with us the high-level idea of making
server-side decisions based on the QoE of end users [29]. In particu-
lar, they estimate the “slack” time between receiving a request and
its end-to-end delay deadline, and utilize this slack to maximize the
quality of the response. Although they allocate different resources
to different requests, they optimize individual requests in isolation,
which can cause resource contention when the system is under
stress or many requests have low slack time. In contrast, E2E opti-
mizes QoE and resource allocation across requests, by harnessing
their inherent heterogeneity. We also empirically show that when
the QoE curve is like Figure 3, a deadline-based QoE model can be
less effective than E2E (§7.4).
E2E is similar to work (e.g., [33, 34]) that considers requests
with soft deadlines: i.e., QoE decreases gradually to zero after the
total delay exceeds a time threshold. These soft-deadline-driven
schedulers set the same threshold for all requests and do not take the
heterogeneity of web requests into account, whereas the resource
allocation in E2E considers different QoE sensitivities.
9 DISCUSSION
Incentives of other service providers: One concern about us-
ing E2E is that another service provider (e.g., an ISP) may try to
manipulate the external delays of its users to get better service
from E2E, by making them look more urgent. However, we prove
in Appendix A that it is impossible to improve a group of users’ QoE
without reducing at least some of their external delays. In other words,
E2E creates an incentive for other service providers to reduce their
delays, rather than gaming E2E by deliberately adding delays.
Security threat: In theory, E2E may introduce a new attack, in
which a large group of users hurt the QoE of other users by mak-
ing themselves look more urgent, thus starving the other users of
resources (similar to a Denial-of-Service attack). We can envision
several detection/mitigation techniques for such an attack, such as
detecting abnormal changes to the external delay distribution, or
adding randomization to the actual server-side delays. We leave
investigation of these security issues to future work.
Interaction with existing policies: A web service provider often
integrate multiple resource allocation policies. Conceptually, E2E is
compatible with other prioritization schemes; they can be included
as input into E2E’s decision-making policy (e.g., by upweighting
the Q(·) values of premium traffic), or E2E can be applied separately
to each priority class (e.g., premium users vs. basic users).
Complex request structures: In a real web framework like Mi-
crosoft’s, a high-level web request usually results in calls to multiple
backend services, and the request is not complete until it hears a
response from all the backend services [12]. A straightforward way
to handle this request structure is to apply E2E to each service in
isolation. However, this approach is suboptimal, because it may
cause a service to prioritize requests whose server-side delays are
determined by other backend services. For example, in Figure 11(a),
E2E prioritizes request B over A, since prioritizing A would cause B
to suffer a significant QoE drop. But if B also depends on another,
much slower service, speeding up B will not have a direct impact on
the user’s QoE. In this case, it would have been better to prioritize
A, whose QoE could actually have been improved. We can see that
an optimal resource allocation scheme for requests with complex
structure needs to take these backend service dependencies into
account. We leave this problem to future work.
Deployment at scale: E2E must face the following issues when
deployed in a large-scale production system.
• Multiple agents: For a web service to scale, it typically uses dis-
tributed agents (e.g., Cassandra clients or RabbitMQ message bro-
kers), each making resource-allocation decisions independently.
In E2E, although each agent might see a different subset of web
requests, its decisions are based on a global decision lookup table
built upon the global external delay distribution. In the unlikely
event that the requests are load balanced poorly across the agents,
it is possible for the resulting decisions to be suboptimal: e.g., in
the case of RabbitMQ, if one message broker only sees insensitive
requests, those requests will be at the head of its queue (there
are no sensitive requests to place ahead of them). We have not
investigated such scenarios in our current evaluation.
• Real-time external delay estimation: Our current prototype relies
on the external delays provided by our traces, but a real deploy-
ment would need to compute the external delay in real-time for
each request. E2E could accomplish this by borrowing ideas from
Timecard [41] and Mystery Machine [20]. Like Timecard, the
WAN-induced delay of a request could be derived from the round-
trip time of the TCP handshake packets and the TCP sliding
window size. To estimate the browser rendering time of a request,
E2E could use a model trained on historical traces (Mystery Ma-
chine) or on traces and the system configuration (Timecard).
Timecard provides more accurate estimates but requires user
permission to access the system configuration. Mystery Machine
does not need user cooperation but has lower accuracy, especially
for first-time users. Since E2E is not very sensitive to the accuracy
of the external delay estimates (Figure 20(a)), Mystery Machine’s
method could allow E2E to scale out and support more requests.
10 CONCLUSION
We have described E2E, a resource allocation system that opti-
mizes QoE by exploiting user heterogeneity. E2E gives any shared-
resource service an end-to-end view of request delays, allowing it
to prioritize the handling of these requests based on how sensitive
their QoE is to delay. E2E can be used by multiple services if the
services do not interact on the same request. As we have discussed,
many web frameworks coordinate multiple services to complete a
single (high-level) request. Extending E2E to handle such complex
request dependencies is our primary direction of future work.
ACKNOWLEDGEMENTS
We thank Jen Guriel, Bhavesh Thaker, Amiya Gupta, Nitin Suvarna,
Sharon Whiteman, and others on Microsoft’s web framework teams.
We thank Yizhuo Zhang at USTC for helping us run the MTurk
study, Varun Gupta at University of Chicago and Yuedong Xu at
Fundan University for helpful discussions about resource allocation,
and Dikaimin Simon for help on building the prototype. We also
thank the anonymous reviewers, and our shepherd, Phillipa Gill.
This project was supported in part by NSF Grant CNS-1901466.
E2E: Embracing User Heterogeneity to Improve QoE on the Web
SIGCOMM ’19, August 19–23, 2019, Beijing, China
REFERENCES
[1] Amazon Mechanical Turk. https://www.mturk.com/.
[2] Apache Cassandra. https://cassandra.apache.org.
[3] MongoDB. https://www.mongodb.com/.
[4] RabbitMQ. https://www.rabbitmq.com/.
[5] Marissa Mayer
at Web 2.0.
marissa-mayer-at-web-20.html, 2006.
[6] Akamai
http://glinden.blogspot.com/2006/11/
retail
online
criti-
cal.
https://www.akamai.com/uk/en/about/news/press/2017-press/
akamai-releases-spring-2017-state-of-online-retail-performance-report.jsp,
2017.
report: Milliseconds
performance
are
[7] Find out how you stack up to new industry benchmarks for mo-
https://www.thinkwithgoogle.com/marketing-resources/
bile page speed.
data-measurement/mobile-page-speed-new-industry-benchmarks/, 2018.
[8] Victor Agababov, Michael Buettner, Victor Chudnovsky, Mark Cogan, Ben Green-
stein, Shane McDaniel, Michael Piatek, Colin Scott, Matt Welsh, and Bolian
Yin. Flywheel: Google’s data compression proxy for the mobile web. In NSDI,
volume 15, pages 367–380, 2015.
[9] Vaneet Aggarwal, Emir Halepovic, Jeffrey Pang, Shobha Venkataraman, and
He Yan. Prometheus: Toward quality-of-experience estimation for mobile apps