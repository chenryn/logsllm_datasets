reports were published providing statistics on the deployment of
egress filtering in the Internet [6, 7, 28, 30]; we list the statistics in
Table 1.
The downside of this approach is that the Spoofer Project re-
quires users to download, compile and execute a software - which
also needs administrative privileges to run - once per measurement.
This requires not only technically knowledgeable volunteers that
agree to run untrusted code, but also networks which agree to
operate such vantage points on their premises. [22] argues that
extending the limited coverage of the Spoofer Project is difficult:
the operators are unlikely to volunteer or conduct measurements
that could leak a negative security posture of their networks, in-
cluding lack of support of BCP38 [19]. Hence, [22] propose that the
most viable method to measure filtering of spoofed packets in more
networks is by crowd-sourcing. In 2018 [28] performed a one-time
study of the Spoofer Project by renting a 2,000 EUR crowd-sourcing
platforms with workers that executed the Spoofer software over
a 6 weeks period. Their measurements included additional 342
ASes which were not covered by the Spoofer Project previously.
Crowd-sourcing studies, in addition to being expensive, are also
limited by the networks in which workers are present and do not
provide longitudinal and repetitive studies that can be validated
and reproduced.
In a recent longitudinal data analysis by the Spoofer Project
[30] the authors observed that despite increase in the coverage of
ASes that do not perform ingress filtering in the Internet, the test
coverage across networks and geo-locations is still non-uniform.
Closely related to volunteers is the vantage points measurements
with faulty or misconfigured servers. [34] noticed that some DNS
resolvers do not change the source IP addresses of the DNS requests
that they forward to upstream resolvers and return the DNS re-
sponses using the IP addresses of the upstream resolvers - a problem
which the authors trace to broken networking implementations.
[26] used this observation to measure egress filtering in networks
that operate such misconfigured DNS resolvers. Such measurements
are limited only to networks which operate DNS servers with bro-
ken networking implementations: out of 225,888 networks that [26]
measured, they could find such DNS servers only in 870 networks.
Since the Open Resolver and the Spoofer Projects are the only
two infrastructures providing vantage points for measuring spoof-
ing - their importance is immense as they facilitated many research
works analysing the spoofability of networks based on the datasets
collected by these infrastructures. Nevertheless, the studies using
these infrastructure, e.g., [22, 30], point out the problems with the
representativeness of the collected data of the larger Internet. Both
projects (the Spoofer and the Open Resolver) acknowledged the
need to increase the coverage of the measurements, as well as the
challenges for obtaining better coverage and stable vantage points.
Network Traces. To overcome the dependency on vantage
points for running the tests, researchers explored alternatives for
1041ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Tianxiang Dai and Haya Shulman
Study
Coverage
(scanned ASes)
Spoofable
Type
Year
Longitudinal Reproducible
Scalable
Spoofer Project [5]
Spoofer Project [7]
Misconfigured servers [26]
Traceroute [29]
IXP traces [27]
Amazon Turk Spoofer Project [28]
Spoofer Project [30]
SMap
202 of 18,000 (1.1%)
1,586 of 44,000 (3.6%)
2,692 of 48,000 (5.6%)
1,780 of 56,000 (3.2%)
700 of 56,000 (1.3%)
784 of 56,000 (1.4%)
5,178 of 66,000 (7.8%)
63,522 of 70,468 (90%)
Table 1: Comparison between SMap and other studies.
Egress
Egress
Ingress
Ingress
In & Eg
Egress
In & Eg
Ingress
2005
2013
2014
2017
2017
2019
2019-21
6w. in 2017
ASes
52
390
870
703
393
48
1,631
51,046
✓
✓
X
X
X
✓
✓
✓
X
X
✓
X
X
X
X
✓
X
X
✓
(✓)
X
X
X
✓
inferring filtering of spoofed packets. A recent work used loops in
traceroute to infer ability to send packets from spoofed IP addresses,
[29]. This method detects lack of ingress filtering only on provider
ASes (i.e., spoofable customer ASes cannot be detected). The study
in [29] identified loops in 1,780 ASes, which is 3.2% of all the ASes,
and 703 of the ASes were found spoofable. Although a valuable
complementary technique for active probes with vantage points,
this approach has significant limitations: in the absence of loops
ingress filtering cannot be inferred, alternately a forwarding loop
in traceroute does not imply absence of filtering at the edge, since a
loop resulting from a transient misconfiguration or routing update
can occur anywhere in the network. Therefore, to identify a lack of
ingress filtering reliably one needs to detect a border router and,
more importantly, the traceroute loops need to be reproduced - a
difficult problem in practice. Furthermore, reproducing or validat-
ing the dataset after some time is virtually impossible as the odds
for failures rapidly increase. Running traceroutes is also challeng-
ing: black-holes in traceroutes, whereby the routers do not respond
to probes or when routers have a limit for ICMP responses, are
common in Internet [33].
[27] developed a methodology to passively detect spoofed pack-
ets in traces recorded at a European IXP connecting 700 networks.
The limitation of this approach is that it requires cooperation of
the IXP to perform the analysis over the traffic and applies only
to networks connected to the IXP. Allowing to identify spoofing
that defacto took place, the approach proposed in [27] misses out
on the networks which do not enforce filtering but which did not
receive packets from spoofed IP addresses (at least during the time
frame in which the traces were collected).
A range of studies analysed network traces for ingress filtering
using IP address characteristics [4, 9, 10, 14, 36], or by inspecting
on-path network equipment reaction to unwanted traffic, [44]. In
addition to a limited coverage, the studies do not support longi-
tudinal and repeating data collection and analysis, and cannot be
reproduced as they do not make the datasets of their studies public.
3 SCANNING FOR SPOOFABLE NETWORKS
3.1 Dataset
SMap architecture consists of two parts: dataset scan and ingress
filtering scan. The dataset scan collects the popular services using
two methods: domain-based scan and IPv4 based scan. In IPv4 scan
to locate the services SMap probes every IP, checking for open
ports that correspond to the services that we need; for instance,
port 25 for Email, 53 for DNS, 80/443 for Web. To reduce the traffic
volume of the scan, instead of probing each IP address for target
ports, SMap enables also query of the input domains for services.
For every domain, it queries the IP and hostname of the services,
e.g., (A, MX) for Email server, A for Web server, (A, NS) for name
server.
3.2 Methodology
The measurement methodology underlying SMap uses active probes,
some sent from spoofed as well as from real source IP addresses
to popular services on the tested networks. The spoofed source IP
addresses belong to the tested networks (similarly to the Spoofer
Project [5]). The idea behind our methodology is that if the packets
with spoofed addresses reach the services in the tested networks,
they trigger a certain action. This action can be measured remotely.
If the action was not triggered, we conclude that spoofed packets
did not reach the service.
We develop three techniques to detect if networks filter spoofed
traffic based on our methodology: DNS lookup, IPID and PMTUD
based. Using popular services ensures that our measurements apply
to as many Internet networks as possible.
SMap consists of the orchestrator which coordinates and syn-
chronises the prober hosts. The prober hosts receive the dataset of
networks to be scanned for spoofability from the orchestrator. The
probers then run IPID, PMTUD and DNS lookup tests against the
services on the dataset list. SMap applies one test at a time for each
AS in the dataset. Each successful test indicates that packets from a
spoofed IP address reached the destination on the target network,
implying that the target AS does not filter spoofed packets. On the
other hand, a failed test may indicate that one of the ASes on the
path between the probers and the service on the target AS may be
filtering spoofed packets.
The results from the tests are stored in the backend database. The
GUI displays the results of the measurements at https://smap.cad.
sit.fraunhofer.de. We next explain each measurement technique.
In our measurements in Section 4 we compare the success and
applicability of each technique.
3.3 IPID
Each IP packet contains an IP Identifier (IPID) field, which allows
the recipient to identify fragments of the same original IP packet.
1042SMap: Internet-wide Scanning for Spoofing
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
The IPID field is 16 bits in IPv4, and for each packet the Operating
System (OS) at the sender assigns a new IPID value. There are
different IPID assignment algorithms which can be categorised as:
random and predictable. Predictable category uses either a global
counter or multiple counters per designation IP address, such that
the counter is incremented in predictable quotas. Random category
selects each IPID value at random from a pool of values.
Recent work showed that even TCP traffic gets fragmented under
certain conditions [12]. Fragmentation has long history of attacks
which affect both the UDP and TCP traffic [21, 23, 40].
Methodology. We use services that assign globally incremental
IPID values. The idea is that globally incremental IPID [RFC6864]
[42] values leak traffic volume arriving at the service and can be
measured by any Internet host. Given a server with a globally
incremental IPID on the tested network, we sample the IPID value
(send a packet to the server and receive a response) from the IP
addresses controlled by us. We then generate a set of packets to the
server from spoofed IP addresses, belonging to the tested network.
We probe the IPID value again, by sending packets from our real IP
address. If the spoofed packets reached the server, they incremented
the IPID counter on the server - an event which we infer when
probing the value from our real IP address the second time.
The challenge here is to accurately probe the increments rate
of the IPID value (caused by the packets from other sources not
controlled by us), in order to be able to extrapolate the value that
will have been assigned to our second probe from a real source IP.
This allows us to infer if the spoofed packets incremented the IPID
counter.
Identifying servers with global IPID counters. We send pack-
ets from two hosts (with different IP addresses) to a server on a
tested network. We implemented probing over TCP SYN, ping and
using requests/responses to Name servers and we apply the suitable
test depending on the server that we identify on the tested network.
If the responses contain globally incremental IPID values - we use
the service for ingress filtering measurement with IPID technique.
We located globally incremental IPID in 63.27% of the measured
networks. There are certainly more hosts on networks that support
globally incremental IPID values, yet our goal was to validate our
measurement techniques while keeping the measurement traffic
low - hence we avoided scanning the networks for additional hosts
and only checked for Web, Email or Name servers with globally
incremental IPID counters via queries to the tested domain.
Statistics of IPID values distribution among tested servers are
plotted in Figure 2. When ICMP is filtered, it results in ERROR,
when run with TCP, the IPID values are often zero (i.e., ZERO IPID
in graph) in Figure 2. To improve coverage of the IPID technique
we merge the ICMP&TCP and ICMP&UDP results for each server
in our measurements.
Measuring IPID increment rate. The traffic to the servers is
stable and hence can be predicted, [43]. We validate this by sampling
the IPID value at the servers which we use for running the test.
One example evaluation of IPID sampling on one of the busiest
servers is plotted in Figure 3. In this evaluation we issued queries
to a Name server at 69.13.54.XXX during three minutes, and plot
the IPID values received in responses in Figure 3 - the identical
patterns demonstrate predictable increment rates. Which means
that the traffic to the server arrives at a stable rate.
Figure 2: IPIDs on servers in dataset.
Figure 3: IPID of Name server 69.13.54.XXX during 180sec.
Accuracy of IPID measurements. The IPID techniques are
known to be difficult to leverage, requiring significant statistical
analyses to ensure correctness. Recently, [17, 37] developed statisti-
cal methods for measuring IPID. However, in contrast to our work,
the goal in [17, 37] is different - they use IPID to measure censor-
ship and have additional sources of inaccuracy, which do not apply
to our measurements: (1) the measurements are applied against
client hosts, which results in significantly higher noise than our