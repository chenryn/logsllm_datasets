aware routing (§6) relies on straightforward software changes at
servers and the addition of alternate routing tables at routers, stan-
dard functionality supported by our existing equipment.
5 AVOIDING A CONGESTED EDGE
Edge Fabric consists of loosely coupled microservices (Figure 8).
Every 30 seconds, by default, the allocator receives the network’s
current routes and traffic from other services (§5.1), projects inter-
face utilization (§5.1.2), and generates a set of prefixes to shift from
overloaded interfaces and for each prefix, the detour path to shift it
to (§5.2). Another service enacts these overrides by injecting routes
into routers via BGP (§5.3). We use a 30 second period to make it
easier to analyze the controller’s behavior, but we can lower the
period if required due to traffic volatility.
BGP InjectorRoute OverridesBMP CollectorTraffic CollectorAllocatorTopologyStatesprefix1 viaX.X.X.XPeering RoutersControllerACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
B. Schlinker et al.
5.1 Capturing Network State (inputs)
Edge Fabric needs to know all routes from a PoP to a destination,
and which routes traffic will traverse if it does not intervene. In
addition, it needs to know the volume of traffic per destination
prefix and the capacity of egress interfaces in the PoP.
5.1.1 Routing information.
All available routes per prefix. The BGP Monitoring Protocol
(BMP) allows a router to share a snapshot of the routes received
from BGP peers (e.g., all of the routes in its route information base,
or RIB) and stream subsequent updates to a subscriber [25]. The
BMP collector service maintains BMP subscriptions to all peering
routers, providing Edge Fabric with a live view of every peering
router’s RIB. In comparison, if Edge Fabric maintained a BGP
peering with each router, it could only see each router’s best path.3
Preferred paths per prefix. BMP does not indicate which
path(s) BGP has selected, and a BGP peering only shares a single
path, even if the router is using ECMP to split traffic across
multiple equivalent paths. The controller also needs to know what
paths(s) would be preferred without any existing overrides. So, the
controller emulates BGP best path selection (including multipath
computation and ignoring existing overrides that would otherwise
be preferred) for every prefix.
5.1.2 Traffic information.
Current traffic rate per prefix. The Traffic Collector service
collects traffic samples reported by all peering routers in a PoP (us-
ing IPFIX or sFlow, depending on the router), groups samples by the
longest-matching prefix announced by BGP peers, and calculates
the average traffic rate for each prefix over a two-minute window.
We use live rather than historical information because, for exam-
ple, the global load balancing system (§2) may have shifted traffic
to/from the PoP, destination networks may have changed how they
originate their network space for ingress traffic engineering, and
traffic demands change over time on a range of timescales.
If the rate of a prefix exceeds a configurable threshold, for ex-
ample 250 Mbps, the service will recursively split the prefix (e.g.,
splitting a /20 into two /21s, discarding prefixes with no traffic)
until the rate of all prefixes is less than the threshold. Splitting large
prefixes allows the allocator to make more fine-grained decisions
and minimize the amount of traffic that must be detoured when
interfaces are overloaded (§5.2).
Interface information. The allocator retrieves the list of inter-
faces at each peering router from a network management service
[27] and queries peering routers via SNMP every 6 seconds to re-
trieve interface capacities, allowing the allocator to quickly adapt
to capacity changes caused by failures or provisioning.
Projecting interface utilization. The allocator projects what
the utilization of all egress interfaces in the PoP would be if no
overrides had been injected, assigning each prefix to its preferred
route(s) from its emulated BGP path selection. The BGP best path
computation process may return multiple (equivalent) routes. These
3Previously, we used BGP add-path capability to collect multiple routes from each
router, but some vendor equipment limits the number of additional paths exchanged,
which we quickly exceeded given our rich interdomain connectivity.
routes may be spread across multiple interfaces and/or peering
routers. In these cases, the allocator assumes that ECMP at both the
aggregation layer and peering routers splits traffic equally across
the paths.
We project interface utilization instead of using the actual uti-
lization to enable the allocation process to be stateless, simplifying
our design: the allocator generates a full allocation from scratch on
each cycle and does not need to be aware of its previous decisions
or their impact. Section 8.1.1 discusses this design choice in detail.
Based on the projected utilization, the allocator identifies in-
terfaces that will be overloaded if it does not apply overrides. We
consider an interface overloaded if utilization exceeds ~95% (the
exact threshold can vary based on interface capacity and peer type),
striking a balance between efficient utilization and headroom to
handle volatility (including microbursts).
5.2 Generating Overrides (decisions)
The allocator generates overrides to shift traffic away from inter-
faces that it projects will otherwise be overloaded. For each over-
loaded interface, the allocator identifies the prefixes projected to
traverse the interface and, for each prefix, the available alternate
routes.4 It then identifies the single ⟨prefix, alternate route⟩ pairing
that it prefers to shift first from the interface, applying the following
rules in order until a single preference emerges:
1. Prefer IPv4 over IPv6 prefixes.5
2. Prefer prefixes that a peer prefers Facebook detour. Peers have
the option of providing these preferences using Facebook-
defined BGP communities.
3. Among multiple alternate routes for a given prefix, prefer routes
with the longest prefix.6
4. Prefer paths based on BGP’s best path selection process. For
instance, the allocator will prefer shifting a prefix with an avail-
able route via a public exchange over a prefix that only has an
alternate route via a transit provider (§2.2).
5. Prefer paths based on an arbitrary but deterministic tiebreaker.
The tiebreaker selects first based on the prefix value. If there are
equally preferred alternate routes for the chosen prefix, the allo-
cator orders alternate routes in a consistent way that increases
the likelihood of detour traffic being balanced across interfaces.
Once a pairing has been selected, the allocator records the deci-
sion and updates its projections, removing the prefix’s traffic from
the original interfaces and placing all of the PoP’s traffic for the
prefix onto the selected alternate route’s interface. Edge Fabric
detours all traffic for the prefix, even if the prefix’s primary route
was across multiple interfaces or routers. However, the total traffic
per prefix is always less than the threshold that Traffic Collector
uses when splitting high traffic prefixes (§5.1).
4A prefix will not have any alternate routes if all routes to it are on interfaces that
lack sufficient spare capacity (after accounting for earlier detours from this round of
allocation). This scenario is rare, as transit interfaces have routes to all prefixes and, in
our evaluation, always had at least 45% of their capacity free (§7.2).
5We prefer shifting IPv4 prefixes because we have experienced routes that blackhole
IPv6 traffic despite advertising the prefix. If Edge Fabric shifts traffic to such a route,
end-users will fallback to IPv4 [30], causing traffic to oscillate between IPv4 and IPv6.
6Unlike the standard BGP decision process, the allocator will consider using routes for
less-specific prefixes, just with lower preference.
Engineering Egress with Edge Fabric
ACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
The allocator continues to select prefixes to shift until it projects
that the interface is no longer overloaded or until the remaining
prefixes have no available alternate routes.
Because the allocation process is stateless, it generates a new
allocation from scratch every 30 seconds. To minimize churn, we
implemented the preferences to consider interfaces, prefixes, and
detour routes in a consistent order, leading the allocator to make
similar decisions in adjacent rounds. The remaining churn is often
due to changes in traffic rates and available routes.
The headroom left by our utilization thresholds allows interface
utilization to continue to grow between allocation cycles without
interfaces becoming overloaded. If a route used by the controller
for a detour is withdrawn, the controller will stop detouring traffic
to the withdrawn route to prevent blackholing of traffic.
5.3 Enacting Allocator Overrides (output)
In each round, the allocator generates a set of BGP updates for Edge
Fabric’s overrides and assigns each update a very high local_pref.
The allocator passes the BGP updates to the BGP Injector service,
which maintains a BGP connection with every peering router in
the PoP and enacts the overrides by announcing the BGP updates
to the target routers. Because the injected updates have a very
high local_pref and are propagated between PRs and the ASWs
via iBGP, all routers prefer the injected route for each overridden
prefix. The injector service then withdraws any overrides that are
no longer valid in the current allocation.
We configure Edge Fabric and the global load balancer such
that their independent decisions work together rather than at odds.
First we need to protect against Edge Fabric decisions and global
load balancer decisions interacting in ways that cause oscillations.
In selecting which PoP to direct a client to, the global load balancer
jointly considers performance from the PoP and Facebook’s BGP
policy’s preference for the best route from the PoP, but we configure
it to ignore routes injected by Edge Fabric and instead to consider
the route that would be used in the absence of an override. If the
load balancer was allowed to consider the override route, it could
shift client traffic away from a PoP in reaction to Edge Fabric
detouring traffic from an overloaded interface to a less-preferred
route. This shift would reduce traffic at the PoP, lowering Edge
Fabric’s projection of interface load, which could cause it to stop
detouring, opening the possibility of an oscillation. Second, the
global load balancer can track interface utilization and appropriately
spread traffic for a client network across all PoPs that it prefers
equally for that network. So, Edge Fabric need only intervene with
overrides once interfaces are overloaded across all the PoPs.
5.4 Deploying, Testing, and Monitoring
We typically deploy Edge Fabric controller updates weekly, using
a multi-stage release process to reduce risk. First, because our de-
sign is modular and stateless, we write comprehensive automated
tests for individual components of Edge Fabric and Edge Fabric’s
dependencies. Second, because our controller is stateless and uses
projected interface utilization instead of actual utilization, we can
run a shadow controller inside a sandbox that can query for the
same network state as the live controller, without needing any state
from the live controller and without being dependent on its earlier
decisions. We continuously run shadow instances, built from the
latest revision, for every PoP and compare the decisions and per-
formance of shadow instances against the controllers running in
production. We review these comparisons before beginning deploy-
ment of a new version. Third, because we deploy Edge Fabric and
all dependencies on a per-PoP basis, we can roll out new versions
of a controller and its dependencies on a PoP-by-PoP basis (an au-
tomated system performs this). While the Edge Fabric controller
is being updated, the BGP Injector service continues to inject the
previous round of decisions until the controller resumes (a process
that takes less than 5 minutes). If we need to update the BGP In-
jector service, hold timers at PRs maintain existing injections until
the injector has restarted.
While the stateless controller is amenable to automated tests, it
is particularly vulnerable to errors in BGP route or traffic rate data,
as these can cause the controller to misproject interface utilization.
To catch misprojections, a monitor compares the controller’s post-
allocation projection of interface utilization with actual interface
utilization, and it raises an alarm if they differ by 5% for more than
a configurable period of time. Through this process, we identified
and corrected bugs in our routing policy and in how our PRs export
IPFIX and sFlow samples. The controller projects that ECMP will
distribute traffic nearly evenly across links. The monitor identified
instances of ECMP unexpectedly distributing traffic in a highly-
unbalanced manner, which Edge Fabric can mitigate by overriding
the multipath to send traffic to a single PR.
Similarly, while Edge Fabric’s use of BGP and distributed route
computation lets us build on existing infrastructure, it also exposes
us to the underlying complexity of the BGP protocol. In one sce-
nario, a configuration issue caused routes injected by Edge Fabric
to not be reflected across the full PoP, shifted traffic away from the
previously overloaded interface, but to a different interface than
desired. To detect such misconfigurations, we built an auditing sys-
tem that regularly compares Edge Fabric’s output against current
network state and traffic patterns.
6 TOWARDS PERFORMANCE-AWARE ROUTING
Edge Fabric avoids performance problems due to congested links
at the edge of Facebook’s network. Yet, Facebook’s users can still
suffer sub-optimal performance because of the limitations of the
BGP decision process (§3). BGP may not choose the best-performing
path for a given prefix’s default path, and likewise, when detouring
a prefix, BGP may not choose the best-performing detour path.
Even if BGP does choose the best-performing path in all scenarios,
performance can be degraded when a prefix is detoured if its best
detour path has worse performance than its default path.
Moreover, while Facebook connects directly to many edge net-
works (like other large content providers do [5]), performance to
other edge networks (to which Facebook routes traffic via IXPs
or transit networks) can be hindered by downstream congestion,
which Edge Fabric (as described in §5) does not account for. Even
in cases when Facebook does directly connect to the edge net-
work, downstream congestion can still degrade performance. Due
to transient failures and volatility in traffic and congestion, which
path performs best can vary over time, and so performance-based
decisions need to be responsive to change. BGP provides neither
ACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
B. Schlinker et al.
visibility into performance nor the explicit ability to make decisions
based on it.
To enable routing decisions to incorporate performance, we need
to measure multiple paths to a destination in parallel, continuously.
In addition, to enable Edge Fabric to best utilize available capacity,
we need to be able to prioritize certain types of content (for instance,
prioritizing a live video stream when deciding which traffic to place
on an egress interface where demand exceeds capacity). However,
because BGP only supports destination-based routing, BGP alone