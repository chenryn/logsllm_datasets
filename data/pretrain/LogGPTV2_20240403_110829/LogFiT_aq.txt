version which do not use centroid distance. The results
confirm that centroid distance does not effectively separate
normal from anomaly samples. Figure 6 illustrates thepredictions for the Thunderbird datataset. In the Thunderbird
evaluation dataset, there were 5,000 normal samples and
8
Figure 6. 	Aggregate anomaly predictions for the Thunderbird evaluation dataset, with 5,000 normal and 1,000 anomaly samples. The predictions based on top-k accuracy are more effective than predictions based on centroid distance. The centroid distance threshold simply classifies all samples as normal.1,000 	anomaly 	samples. 	The 	aggregate 	results 	for 	the normal samples are represented by the row/column labeled 0, while the aggregate results for anomaly samples are represented by the row/column labeled 1. The column labeled pred token anomaly represents the LogFiT predictions (based on top-k prediction accuracy) for the 5,000 normal samples. The column labeled pred centroid anomaly represents the model’s predictions (based on centroid distance) for the 1,000 anomaly samples. Figure 6 shows how the two anomaly thresholds top-k prediction accuracy and centroid distance, influence the model’s anomaly decision on the Thunderbird dataset. The figure shows that centroid distance is is not an effective method for distinguishing normal and anomalous log paragraphs. In this case, all samples were categorised as normal by the centroid distance criteria.To investigate the reason why centroid distance is not effective, the semantic vectors of the training and evaluation sets are plotted after reducing the vector dimensions to 2 using the UMAP algorithm [36]. As shown in Figure 7, the semantic vectors of the training set do not form clean clusters. So there are no clusters that can effectively threshold the semantic vectors computed during inference. Therefore, it can be concluded that centroid distance minimisation does not improve the performance of LogFiT and can be omitted from the model. In the above mentioned figures, the color of the points represent training samples (blue), normal predictions (pink) and anomaly predictions (yellow). It can be seen that the clusters for the anomaly samples from the evaluation set (yellow points) appear very close to the training/normal clusters (blue points). In contrast the clusters for the normal samples from the evaluation set (pink points) appear further and more spread out from the training/normal clusters (blue points). Therefore centroid distance cannot be used to separate normal from anomaly log paragraphs.b) Variations in input data: In practical applications, it is expected that the content of the log sentences will change over time. This can be because the programmers may change some words in the log sentences, or introduce misspellings. The LogFiT model contains built-in support for log sentence variability due to its large vocabulary of sub-word tokens (around 50K). In contrast the DeepLog and LogBERT models would fail if they encounter variation in log sentences thatFigure 7. UMAP plot of Thunderbird semantic vectors, where the blue, pink and yellow colours of the points represent training samples, normal predictions and anomaly predictions respectively.
| Method | P | R | F1 | S |