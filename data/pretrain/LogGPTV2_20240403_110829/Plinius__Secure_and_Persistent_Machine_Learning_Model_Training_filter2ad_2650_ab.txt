### Expanding Main Memory Capacity and Application Direct Mode

To expand main memory capacity without persistence, one can use non-persistent memory. However, for applications requiring persistence, the App Direct mode is available [40]. PLINIUS leverages Persistent Memory (PM) in App Direct mode to achieve this.

In App Direct mode, applications can utilize PM through standard operating system calls (e.g., read, write) via the file system, similar to how slower storage devices like SSDs and HDDs are accessed. This approach enhances application performance but does not fully leverage the load/store interface provided by PM modules. In PLINIUS, we enhance the ML library to perform direct loads/stores from/to PM, which, although more challenging as it requires application modification, results in significant performance gains by bypassing both the kernel and file system [22].

### Server-Grade CPUs and Security Risks

Server-grade CPUs natively support up to 3 TB of PM [27], making PM an attractive solution for fault-tolerant applications. However, data remanence [41] introduces security risks, particularly concerning confidentiality and data integrity.

### Paradigm Shift for Developers

The use of PM necessitates a paradigm shift for application developers. Several software tools and libraries, such as Romulus [11], Mnemosyne [36], and Intel’s PMDK, have been proposed to facilitate PM-related development. These libraries expose PM to applications by memory-mapping files on a persistent memory-aware file system with Direct Access (DAX) capabilities. DAX removes the OS page cache from the I/O path, allowing direct access to PM with byte-granularity.

### Performance Characterization

To characterize our PM units, we executed FIO with sequential and random workloads, comparing read and write throughputs for native Ext4 over an SSD drive, Ext4+DAX on PM, and a tmpfs partition over volatile DRAM. Our observations (Fig. 2) show that the DAX-enabled file system on PM consistently outperforms its non-DAX counterpart on SSD and approaches the performance of RAM-tmpfs (in the order of GB/s).

### Persistence and Consistency

Specific processor instructions (e.g., CLFLUSH, CLFLUSHOPT, CLWB) are used to flush data from cache lines to the PM memory controller. Asynchronous DRAM refresh [40] ensures that data in the memory controller’s write buffers is persisted in PM in case of a power failure. Persistence fences (e.g., SFENCE) guarantee consistency by preventing store instructions from being re-ordered by the CPU. PM libraries like Romulus and PMDK provide transactional APIs, enabling developers to perform atomic updates on persistent data structures.

### Romulus: Durable Transactions

Romulus provides durable transactions using twin copies of data in PM and a volatile log to track modified memory locations. The first copy, the main region, is where in-place modifications occur, while the second copy, the back region, serves as a backup of the previous consistent state. After a crash, the content of the back region is restored to the main region. Romulus uses at most four persistence fences for atomic updates, regardless of transaction size, and a store interposition technique to ensure cache lines are correctly flushed to PM. This design results in low write amplification [11], making Romulus a suitable choice for our PM library, which we ported to be SGX-compatible.

### Training Machine Learning Models

A machine learning (ML) model maps an input to a target output based on a set of parameters [19]. For example, a linear regression model is defined as \( f(x) = Wx + b \), where \( W \) represents the model weights, \( b \) the bias vector, and \( x \) the input vector. The goal of model training is to find the set of learnable parameters that minimize a loss function and maximize the model's accuracy on the training data. The loss function quantifies the difference between the predicted value and the ground truth. During training, the learning algorithm iteratively feeds the model with batches of training data, calculates the loss, and updates the model parameters to minimize the loss. Stochastic Gradient Descent (SGD) [29] is a popular learning algorithm for this purpose.

### Supervised Learning and Fault Tolerance

In this work, we focus on supervised learning, where a costly training phase builds a model from labeled data, followed by a classification/inference phase. Common examples include visual object recognition and spam filtering. Figure 3 illustrates a typical supervised ML model training pipeline. Training large ML datasets (i.e., in the order of GBs) can take several days, during which failures or pre-emptions may occur [6]. State-of-the-art ML frameworks (e.g., TensorFlow [6], Darknet [3], Caffe [24]) provide mechanisms to checkpoint model states to secondary storage, but the high latency and low bandwidth of secondary storage make failure recovery a challenge. The appealing properties of PM make it particularly interesting for fault tolerance in such scenarios.

### PLINIUS: A Novel ML Framework

We implement PLINIUS, a novel ML framework that leverages PM for fault tolerance and Intel SGX to ensure the confidentiality and integrity of ML models and sensitive training data. We build our framework on Darknet, a popular and efficient ML library easily portable to Intel SGX.

### Threat Model

PLINIUS aims to:
1. Ensure the confidentiality and integrity of ML model parameters (e.g., weights, biases) during training.
2. Ensure the confidentiality and integrity of the model's replica on PM used for fault tolerance.
3. Ensure the confidentiality and integrity of training data in byte-addressable PM.

The system is designed to achieve these goals against a powerful adversary with physical access to the hardware and full control of the entire software stack, including the OS and hypervisor. Model hyper-parameters, such as model architecture, number of layers, and type of training data, are public information and do not leak sensitive information about the trained model parameters or training data [15], [17], [19]. PLINIUS supports secure provisioning of model hyper-parameters via the SGX remote attestation mechanism. We assume the adversary cannot physically open and manipulate the processor package, and enclave code is correct and does not intentionally leak sensitive information. Denial-of-service and side-channel attacks, for which solutions exist [16], [30], are considered out of scope.

### PLINIUS Architecture

The design of PLINIUS minimizes the Trusted Computing Base (TCB). A libOS-based design (e.g., Graphene SGX, SCONE containers) introduces thousands of lines of code into the enclave runtime, increasing security risks and leading to performance degradation. Following SGX guidelines [10], we design an architecture partitioned into trusted and untrusted parts. By manually porting the PM and ML libraries, PLINIUS achieved a TCB reduction of approximately 44% in terms of Lines of Code (LOC).

The architecture of PLINIUS consists of three main components:
1. **SGX-Romulus**: A port of Romulus to Intel SGX, providing durable, concurrent transactions and persistence primitives for managing persistent data structures in PM.
2. **SGX-Darknet**: A port of Darknet to Intel SGX, supporting secure training and inference on ML models.
3. **Mirroring Module**: Synchronizes the ML model inside the enclave with its encrypted mirror copy in PM.

Figure 4 illustrates the interaction between these components. The encryption engine encrypts/decrypts model parameters and training data using AES Galois Counter Mode (GCM) [13] with a 128-bit key. Each encryption operation generates a random 12-byte initialization vector (IV) and appends a 16-byte message authentication code (MAC) to ensure data integrity.

### Full ML Workflow with PLINIUS

Figure 5 shows the full ML workflow with PLINIUS. The data and model owner sends the application binary and raw encrypted training data to the remote untrusted server, performs remote attestation, establishes a secure communication channel, and sends encryption keys to the enclave. The PM-data module transforms encrypted data on disk to encrypted byte-addressable data in PM. The training module reads and decrypts batches of training data from PM, with the trained model being mirrored to PM or into the enclave for restores.

### Integration with Different ML Libraries

The current PLINIUS architecture uses Darknet due to its efficient and lightweight implementation in C, which facilitates integration with SGX enclaves. Other ML libraries, such as TensorFlow, can also be integrated. Our implementation creates mirror copies of tensors in PM and restores them in enclave memory using PLINIUS’s mirroring mechanism. However, due to the large memory footprint of TensorFlow-based ML applications, we opted to use Darknet, which is lightweight but equally efficient.

### Conclusion

PLINIUS is a novel ML framework that leverages PM for fault tolerance and Intel SGX to ensure the confidentiality and integrity of ML models and training data. By minimizing the TCB and providing a robust architecture, PLINIUS addresses the challenges of long training times and potential security threats in ML applications.