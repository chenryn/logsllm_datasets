majority of the cases. In a few cases, the participants only
recognized the source class. A closer analysis reveals that
the distortion in these cases is so strong that the detection of
particular classes is difﬁcult. As a result, the participants did
not specify the source class.
Summary. We conclude that the two proposed defenses are
robust against the different adaptive attacks. These attacks
are both optimal with respect to the number of changes and
thus provide strong empirical evidence for the robustness of
the defenses. If a vulnerable scaling algorithm needs to be
used in a machine-learning system or the reconstruction of
the original class is essential, we thus recommend using one
of the defenses as a preprocessing step.
Figure 18: User study to determine the success rate of the adaptive attack
against the median ﬁlter with respect to O2.
USENIX Association
29th USENIX Security Symposium    1375
0204060801000255075100Allowedpixelchangesδ[%]SuccessRateO1[%]MedianFilterCV—NearestCV—LinearCV—CubicTF—NearestTF—LinearTF—Cubic0204060801000255075100Allowedpixelchangesδ[%]RandomFilter2345050100ScalingRatioSpeciﬁed[%]OpenCV—Nearest234050100ScalingRatioOpenCV—LinearonlySvisiblebothvisibleonlyTvisible234050100ScalingRatioOpenCV—CubicAvailability
We make our dataset and code publicly available at
http://scaling-attacks.net to encourage further re-
search on secure image scaling. Our defenses are also im-
plemented in C++ with Eigen, such that they can be easily
employed as plug-ins for TensorFlow.
Acknowledgment
We would like to thank our shepherd Nicolas Papernot, the
anonymous reviewers and David Wagner for their sugges-
tions and comments. Furthermore, we acknowledge fund-
ing by the Deutsche Forschungsgemeinschaft (DFG, Ger-
man Research Foundation) under Germany’s Excellence
Strategy - EXC 2092 CASA - 390781972 and the research
grant RI 2469/3-1, by the German Ministry for Education
and Research as BIFOLD - Berlin Institute for the Foun-
dations of Learning and Data (ref. 01IS18025A and ref
01IS18037A), and from the state of Lower Saxony under
the project Mobilise.
References
[1] M. Barni and F. Pérez-González. “Coping with the enemy:
Advances in adversary-aware signal processing”. In: IEEE
International Conference on Acoustics, Speech, and Signal
Processing (ICASSP). 2013.
[2] B. Biggio, B. Nelson, and P. Laskov. “Support Vector Ma-
chines Under Adversarial Label Noise”. In: Proc. of Asian
Conference on Machine Learning (ACML). 2011.
[3] B. Biggio and F. Roli. “Wild patterns: Ten years after the rise
of adversarial machine learning”. In: Pattern Recognition 84
(2018).
[4] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndi´c, P.
Laskov, G. Giacinto, and F. Roli. “Evasion Attacks against
Machine Learning at Test Time”. In: Machine Learning and
Knowledge Discovery in Databases. Springer, 2013.
[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cam-
bridge University Press, 2009.
[7]
[6] N. Carlini and D. A. Wagner. “Towards Evaluating the Ro-
bustness of Neural Networks.” In: Proc. of IEEE Symposium
on Security and Privacy (S&P). 2017.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT:
Pre-training of Deep Bidirectional Transformers for Lan-
guage Understanding. Tech. rep. 2018. arXiv: 1810.04805.
J. Fridrich. Steganography in Digital Media: Principles,
Algorithms, and Applications. Cambridge University Press,
2010.
[8]
[9] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov.
“Property Inference Attacks on Fully Connected Neural Net-
works using Permutation Invariant Representations.” In: Proc.
of ACM Conference on Computer and Communications Secu-
rity (CCS). 2018.
[10] T. Gu, B. Dolan-Gavitt, and S. Garg. BadNets: Identifying
Vulnerabilities in the Machine Learning Model Supply Chain.
Tech. rep. 2017. arXiv: 1708.06733.
[11] K. He, X. Zhang, S. Ren, and J. Sun. “Deep Residual Learn-
ing for Image Recognition”. In: Proc. of IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). 2016.
[12] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger.
“Densely Connected Convolutional Networks”. In: Proc. of
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR). 2017.
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. “ImageNet:
Classiﬁcation with Deep Convolutional Neural Networks”.
In: Advances in Neural Information Proccessing Systems
(NIPS). 2012.
J. Li, S. Ji, T. Du, B. Li, and T. Wang. “TextBugger: Generat-
ing Adversarial Text Against Real-world Applications”. In:
Proc. of Network and Distributed System Security Symposium
(NDSS). 2019.
[14]
[15] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X.
Zhang. “Trojaning Attack on Neural Networks”. In: Proc. of
Network and Distributed System Security Symposium (NDSS).
2018.
[16] D. G. Lowe.
“Distinctive Image Features from Scale-
Invariant Keypoints”. In: International Journal of Computer
Vision 60.2 (2004).
[17] M. Lécuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana.
“Certiﬁed Robustness to Adversarial Examples with Differen-
tial Privacy.” In: Proc. of IEEE Symposium on Security and
Privacy (S&P). 2019.
[18] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean.
“Distributed Representations of Words and Phrases and their
Compositionality”. In: Advances in Neural Information Proc-
cessing Systems (NIPS). 2013.
[19] A. V. Oppenheim, J. R. Buck, and R. W. Schafer. Discrete-
Time Signal Processing; 2nd ed. Prentice-Hall, 1999.
[20] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Berkay
Celik, and A. Swami. “Practical Black-Box Attacks against
Machine Learning”. In: Proc. of ACM Asia Conference on
Computer Computer and Communications Security (ASIA
CCS). 2017.
[21] N. Papernot, P. McDaniel, A. Sinha, and M. P. Wellman.
“SoK: Security and Privacy in Machine Learning”. In: Proc.
of IEEE European Symposium on Security and Privacy (Eu-
roS&P). Apr. 2018.
[22] E. Quiring, D. Arp, and K. Rieck. “Forgotten Siblings: Unify-
ing Attacks on Machine Learning and Digital Watermarking”.
In: IEEE European Symposium on Security and Privacy (Eu-
roS&P). 2018.
[23] E. Quiring, A. Maier, and K. Rieck. “Misleading Authorship
Attribution of Source Code using Adversarial Learning”. In:
Proc. of USENIX Security Symposium. 2019.
[24] E. Quiring and K. Rieck. “Backdooring and Poisoning Neural
Networks with Image-Scaling Attacks”. In: Deep Learning
and Security Workshop (DLS). 2020.
1376    29th USENIX Security Symposium
USENIX Association
[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S.
Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C.
Berg, and L. Fei-Fei. “ImageNet Large Scale Visual Recog-
nition Challenge”. In: International Journal of Computer
Vision (IJCV) 115.3 (2015).
[26] S. Sardy, P. Tseng, and A. G. Bruce. “Robust Wavelet De-
noising”. In: IEEE Transactions on Signal Processing 49
(2001).
[27] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. “Member-
ship Inference Attacks against Machine Learning Models”.
In: Proc. of IEEE Symposium on Security and Privacy (S&P).
2017.
[28] K. Simonyan and A. Zisserman. Very Deep Convolutional
Networks for Large-Scale Image Recognition. Tech. rep.
2014. arXiv: 1409.1556.
[29] S. W. Smith. The Scientist and Engineer’s Guide to Digital
Signal Processing. California Technical Publishing, 1997.
[30] C. Sun, C. Tang, X. Zhu, X. Li, and L. Wang. “An efﬁcient
method for salt-and-pepper noise removal based on shearlet
transform and noise detection”. In: AEUE - International
Journal of Electronics and Communications 69.12 (2015).
I. Sutskever, O. Vinyals, and Q. V. Le. “Sequence to Sequence
Learning with Neural Networks”. In: Advances in Neural
Information Proccessing Systems (NIPS). 2014.
[31]
[32] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. J. Goodfellow, and R. Fergus. Intriguing properties of
neural networks. Tech. rep. 2013. arXiv: 1312.6199.
F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart.
“Stealing Machine Learning Models via Prediction APIs”. In:
Proc. of USENIX Security Symposium. 2016.
[33]
[34] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng,
and B. Y. Zhao. “Neural Cleanse: Identifying and Mitigating
Backdoor Attacks in Neural Networks.” In: Proc. of IEEE
Symposium on Security and Privacy (S&P). 2019.
[35] Q. Xiao, Y. Chen, C. Shen, Y. Chen, and K. Li. “Seeing
is Not Believing: Camouﬂage Attacks on Image Scaling
Algorithms”. In: Proc. of USENIX Security Symposium.
2019.
A Downgrade Attack to Nearest Scaling
As part of our analysis, we identiﬁed a side effect in the imple-
mentation of g(p) (see Eq. (6)) in OpenCV and TensorFlow.
An adversary can enforce the usage of nearest scaling by
choosing a respective scaling factor although the library is
supposed to use bilinear, bicubic or Lanczos scaling. In partic-
ular, if the scaling ratio is an uneven integer, β = 2z+1, z∈ N,
OpenCV is effectively using nearest scaling. In TensorFlow,
each integer with β ∈ N leads to the same effect. Thus, if the
adversary can control the source image size, she can resize
her image before to obtain the respective scaling factor. This
in turn allows her to perform a more powerful scaling attack
by creating attack images with less distortion, as the ratio
of considered pixels decreases (see Section 3.3). Note that
we do not exploit this issue in our evaluation. We test over
a variety of scaling factors to draw general conclusions on
scaling attacks.
Table 5: Implementation of g(p) in OpenCV, TensorFlow and Pillow
Library
OpenCV
TensorFlow
Pillow
g(·)
g(p) = (p + 0.5)· β− 0.5
g(p) = p· β (*)
g(p) = (p + 0.5)· β
(*) The scaling function in TensorFlow can be changed to the deﬁnition from
OpenCV. However, this option is not exposed in tf.image.resize_images, the
high level resizing API.
To understand its reason, we need to consider the mapping
g(p) and the kernel w. Table 5 shows the slightly different
implementations of g(p) in OpenCV, TensorFlow and Pillow.
For OpenCV, for instance, if β is an uneven integer, g(p)
will always be an integer. Thus, only one pixel will be used
for the convolution. A closer look on the deﬁnition of the
kernels in Figure 6 reveals the underlying reason. Each kernel
is zero for integer positions. Thus, if g(p) is an integer and
the kernel is exactly positioned here, each neighboring pixel
obtains a weight of zero. Thus, only the pixel at position g(p)
is used. This behavior corresponds to nearest scaling. We
observe this effect for bilinear, bicubic and Lanczos scaling
in OpenCV and TensorFlow. On the contrary, Pillow makes
use of a dynamic kernel width, so that we do not observe this
behavior in this case.
B Selective Random Filter
Our random ﬁlter is identical to the selective median ﬁlter,
except for that it takes a random point from each window in-
stead of the median. That is, given a point p ∈ P , we consider
a window Wp around p of size 2βh×2βv and randomly select
a point as a reconstruction of p. Again, we exclude points
p(cid:48) ∈ P from this window to limit the attacker’s inﬂuence.
Randomly selecting a point for reconstruction obviously
comes with problems. First, the reconstruction becomes non-
deterministic. Second, the scaled image might suffer from
poor quality. Our evaluation, however, shows that the loss
due to random sampling is small and might be acceptable for
the beneﬁt of a very efﬁcient run-time performance. The ﬁlter
reconstructs an image with a complexity of O(|P|), which is
independent of the scaling ratio. Furthermore, the ﬁlter also
provides strong protection from attacks. If an image contains
|P| relevant points, there exist |P|· 4βhβv possible combina-
tions for its reconstruction. If we consider a scaling ratio of 5
and a target size of 200× 200, this already amounts to 4 mil-
lion different combinations an attacker needs to guess from.
USENIX Association
29th USENIX Security Symposium    1377
C Adaptive Attack Against Median Filter
In the following, we analyze our adaptive attack against the
median-based defense. We demonstrate that the attack is
optimal regarding the L0, L1, and L2 norm if each window Wp
does not overlap with other windows. An adversary cannot
make less changes to control the output of the median ﬁlter.
For a given attack image and window Wp, the adversary
seeks to manipulate the pixels in Wp such that the median m
over Wp still corresponds to p. In this way, the modiﬁcations
from the image-scaling attack remain even after applying the
median ﬁlter. Without loss of generality, we assume that
m < p and further unroll Wp to a one-dimensional signal.
We consider a signal with uneven length k and denote the
numerical order by brackets, so that the signal is given by:
x(1), ··· , x( k
2 ), m( k+1
2 ), x( k+2
2 ), ··· , x(l), ··· , x(k)
(12)
We denote by x(l) the largest pixel in the sorted signal that is
smaller than p. The objective is to change the signal with the
fewest possible changes such that m = p.
We start by observing that we need to change l − k+1
2 + 1
pixels to move the median to p. Less changes do not impact
the numerical order sufﬁciently. We can thus conclude that
the minimal L0 norm for an attack is given by
m, the median is not changed. Likewise, replacing pixels
larger than x(l) by a value larger than m does not change the
median. Two methods remain: (1) We can replace pixels
with indices in [1, (k + 1)/2] by a value larger than m. (2) We
can set all pixels with index [(k + 1)/2, l] to p. While both
methods can move the median to p, the latter induces less
changes regarding the L1/L2 norm, as these values are closer
to p. Thus, our adaptive attack uses the optimal strategy for
the L1/L2 norm by setting all pixels between m and x(l) to p.
Furthermore, we can derive a simple bound for the L2 norm:
(L2)2 = ∑
( k+1
2 )(cid:54)i(cid:54)l
(cid:0)x(i) − p(cid:1)2 (cid:54) L0 (m− p)2 .
(14)
Overall, we can exactly compute the number and amount
of required changes for a successful attack. Our analysis,
however, also shows that the attack always depends on the
concrete pair of a source and a target image, and there is no
notion of a class boundary. Consequently, we cannot derive a
general bound, as achieved with certiﬁable defenses against
adversarial examples. Yet, our empirical results in Section 5.5
demonstrate that the necessary changes are very large if target
and source images show realistic content, so that the median
m and the target value p are not close to each other.
L0 = l − k+1
2 + 1 .
(13)
D Additional Figures
Next, we show that setting all pixels between m and x(l) to
p successfully moves the median as well as minimizes the
L1 and L2 norm in addition. First, we observe that if we
replace pixels with indices in [1,k/2] by a value smaller than
Figures 19 to 23 give further information and examples from
our evaluation. In particular, they provide visual examples of
successful and failed attacks, thereby highlighting the work-
ing principle of image-scaling attacks.
Figure 19: Success rate of attack regarding objective O2: the similarity between source image and attack image, measured by the PSNR value.
1378    29th USENIX Security Symposium
USENIX Association
[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535PSNRCV—Nearest[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535CV—Linear[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535CV—Cubic[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535PSNRTF—Nearest[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535TF—Linear[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535TF—Cubic[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535PSNRPIL—Nearest[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535Avg.ScalingFactorPIL—Linear[2,3)[3,4)[4,5)[5,7.5)[7.5,10)5152535PIL—CubicFigure 20: Best images of the L0 version of our adaptive attack against area scaling. The attack fails in all cases with respect to objective O2, as each attack
image is not similar to the source image anymore.
Figure 21: Selective source scenario against area scaling with our L1 attack (ﬁrst two columns) and L0 attack (last three columns). The attack fails in all cases
with respect to objective O2. While traces from the source image are visible, the attack image overwrites the source image considerably.
USENIX Association
29th USENIX Security Symposium    1379
SourceimageTargetimageAttackimageOutputimageSourceimageTargetimageAttackimageOutputimageFigure 22: Randomly selected examples before and after restoration with our median ﬁlter (ﬁrst three columns) and random ﬁlter (last two columns). Without
restoration, the attack is successful, as the downscaling of the attack image produces an unrelated target image (1st and 2nd row). With restoration, the attack
fails in all cases with respect to objective O1, as the downscaled output from the restored attack image produces the respective content and not an unrelated
image (3rd and 4th row). Moreover, the ﬁltering improves quality, as it removes traces from the attack.
Figure 23: Successful examples regarding objective O1 from the adaptive attack against the median ﬁlter if 20% of the pixels in each block can be changed. The
target class is detected, but the attack image is a mix between source and target class. The results thus violate objective O2.
1380    29th USENIX Security Symposium
USENIX Association
AttackimageOutputimageRestoredattackimageOutputfromrestoredattackimageSourceimageTargetimageAdaptiveattackimageOutputimage