### 6.2 Feature-Based Classification
We evaluated the following features for password classification: 
1. **Edit Distance Pairs**: This feature involves comparing pairs of passwords with different edit distances.
2. **N-gram Structure**: This feature includes the percentage frequencies of the most popular n-grams to characterize token reuse.
3. **Combined Features**: A combination of the above two features.

We re-created the classifier using the available information and performed some deliberate tuning of the parameters, resulting in a classifier that shows similar performance to the original one. The results for both real vaults and artificial vaults (referred to as MPW in NoCrack’s terminology) are presented in Table 5. To facilitate comparison between the original SVM and our re-implementation, we also list the results from Chatterjee et al. [9]. We observe that both NoCrack and Markov perform similarly against this classifier, with values very close to the reported ones for real vaults and similar for MPW. However, this similarity primarily indicates the ineffectiveness of the used features, as we have seen that KL divergence is substantially better in ranking than this classifier.

### 6.3 Adaptive Construction
The core idea of adaptive construction is to modify the n-gram model to avoid assigning very low probabilities to passwords that actually appear in the vault. If a very improbable password appears in a candidate vault, it would be a strong signal for the real vault. Therefore, we modify the transition probabilities of an n-gram model as follows:
1. **Boosting N-grams in Vault Passwords**: For each password in the vault, we randomly select one n-gram and increase its probability by multiplying it by 5.
2. **Random Boosting of Other N-grams**: For all remaining n-grams, we increase their probabilities by a factor of 5 with a 20% probability.
3. **Re-normalization**: Finally, we re-normalize all probabilities.

The constants 5 and 20% were determined empirically to provide reasonable security guarantees. In the next subsection, we demonstrate that the resulting adaptive NLE prevents online verification attacks much better than previously seen static NLEs, and we discuss the security implications of the adaptive property in the subsequent section.

### 6.4 Performance of the Adaptive NLE
To evaluate the performance of the adaptive NLE, we re-ran the same experiment as before using KL divergence and the PBVault dataset. The results, summarized in Table 4, show that the real vault `cvreal` is ranked on average among the 40.12% of the most likely vaults, thus increasing the amount of online guessing substantially. Notably, the 1st Quartile dropped from 0.39% for the static Markov NLE to 9.12% for the adaptive Markov NLE.

We tested several other boosting constants (2, 4, 5, 6, 8, 10), which resulted in the following mean values (33.71%, 39.38%, 40.12%, 40.36%, 41.56%, 43.4%). We found that a boosting factor of 5 was suitable, as further improvements were marginal.

### 6.5 Security of the Adaptive NLE
We assume that an attacker can determine which n-grams have been boosted, either because they know the corpus used to train the original n-gram model or because they can notice deviations from a “normal” distribution. In such cases, the attacker might infer information about the passwords stored in the vault. If only n-grams for passwords in the vault were boosted and other n-gram probabilities were not randomly boosted, it would enable an easy and efficient attack.

Next, we show that the information an attacker can infer is very limited. Let `B` be the set of n-grams that have been boosted (and this set is known to the attacker). Consider a password `pwd`, which may or may not be in the vault, and let `N` be the set of n-grams that `pwd` contains. Depending on whether `pwd` is in the vault, the size of the intersection `N ∩ B` will change (it will be larger on average if `pwd` is in the vault).

Using Bayes' rule, the ratio between the probability of `pwd = pwd0` and `pwd = pwd0 | i = i0` can be estimated as follows, where `f(k; n, p)` is the probability mass function of the Binomial distribution, `len(pwd)` is the length of the password, and `p = 0.2`:

\[
\frac{P(pwd = pwd0 | i = i0)}{P(pwd = pwd0)} = \frac{f(i0 - 1; len(pwd) - 3, p)}{f(i0; len(pwd) - 2, p)} < \frac{1}{1 - p} = 1.25
\]

In other words, even if an adversary knows the exact set of boosted n-grams, the estimate of the probability of the correct password increases by a factor of 1.25, which has a very limited effect on the guessing behavior for an online guessing attack. The exact influence on password guessing depends on the precise distribution of passwords, or more specifically on the attacker's belief about the password distribution, and is thus hard to quantify precisely.

### 6.6 Limitations of the Adaptive NLE
Finally, we discuss some limitations of adaptive NLEs and the Markov-based adaptive NLE in particular. Adaptive NLEs represent an interesting direction to overcome fundamental limitations of static NLEs, as demonstrated in Section 4. However, more work is needed to better understand the mechanisms for providing adaptive NLEs and to quantify their security guarantees.

Our method for implementing adaptive NLEs based on n-gram models is a first step towards realizing adaptive NLEs. The technique is straightforward, but better methods may exist. Note that we are unaware of an easy and promising way to base adaptive NLEs on PCFGs. The parameters we used were determined empirically and seem to work well, but a more systematic treatment may reveal parameters with better overall performance. Overall, we consider adaptive NLEs as still being work-in-progress.

So far, we have considered vaults that do not change over time. If a new password is added to a vault, one possible way is to re-encode the entire vault as described in Section 6.3. This construction is vulnerable to the same intersection attack as NoCrack: given the same vault before and after adding a new password, the correct master password will decrypt both vaults to the same set of passwords, whereas a wrong master password will decrypt to different decoys with high probability. It is unclear how this problem can be avoided, both for static and adaptive vaults.

### 7. Conclusion
There are various attacks against cracking-resistant vaults, and distribution-based attacks are just one class. We showed that the proposed NoCrack NLE, based on a PCFG model, is too simple. We highlighted that the inability of the applied SVM-based machine learning engine to distinguish real from decoy vaults does not serve as a lower-bound security guarantee. Instead, we provided a distribution-based attack using KL divergence, which can distinguish real from decoy vaults. Additionally, we described further issues that need to be considered for the construction of a well-performing NLE. We demonstrated that our proposed n-gram model outperforms the PCFG-based solution. Moreover, we introduced the notion of adaptive NLEs, where the generated distribution of decoy vaults depends on the actual passwords stored in the vault. This makes it unnecessary to predict changes in password distributions over time, an inherent flaw of static NLEs. Unfortunately, the lack of real-world statistics and sample data on vaults complicates vault security research.

### 8. Acknowledgments
This work was supported by the German Research Foundation (DFG) Research Training Group GRK 1817/1.

### 9. References
[References listed as provided, no changes made]