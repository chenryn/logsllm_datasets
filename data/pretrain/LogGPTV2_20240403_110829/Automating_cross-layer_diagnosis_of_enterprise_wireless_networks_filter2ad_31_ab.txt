from the capture effect [15], which means a radio often decodes
the frame with higher signal strength when two packets collide at a
receiver.
4.2 Link layer
The 802.11 link-layer presents another potential performance land
mine for user packets. In particular each 802.11 access point man-
ages two critical functions: media access and bindings between
individual stations (clients) and APs. Each of these functions can
induce additional and, at times, unnecessary delays. We consider
each in turn.
Transmission delays. Sources of link-layer transmission delay
include queuing at the AP prior to wireless transmission, proto-
col delays such as mandatory backoff on transmission, exponential
backoff on loss, packet transmission time (a function of the en-
coded frame rate and the packet size), and contention in the net-
work when users and APs overlap and share a contention domain
(or due to interference as mentioned above). A single packet may
be delayed by all of these factors and, due to retransmission, it may
be impacted multiple times. Moreover, it is common for 802.11
drivers to encode data at a lower rate after a loss, even though this
practice may have unintended negative effects such as increasing
channel utilization.
For example, consider a packet received by an AP at time t. It
may be delayed in a queue waiting for previous packets to be trans-
mitted (each experiencing their own media access delays and re-
transmission overheads). When it reaches the head of the queue
the AP must perform a mandatory backoff, waiting between 0 and
15 slot times (a normal 802.11b slot is 20 µs, although 802.11g
permits the use of a “short” 9-µs slot time under certain circum-
stances). After the backoff it must sample the channel for the dura-
tion of a “DIFS” interval (50 µs) before sending. If the AP detects
a busy channel, it will perform yet another backoff before com-
mencing the transmission. Finally, the packet is transmitted with
a delay largely determined by the sender’s choice of rate. How-
ever, if the sender does not receive an acknowledgment from the
receiver, the sender performs another backoff before each retrans-
mission. Of course, this explanation is over-simpliﬁed and any real
analysis must also deal with delays from interacting protocol fea-
tures like power management and vendor irregularities (e.g., some
vendors allow certain packets to be prioritized in between retrans-
missions of a frame exchange). Unfortunately, most of the delay
components at this level cannot be observed directly since they de-
pend on the internal state of an AP, which is not exposed via any
protocol feature.
Management delays. Another important source of overhead in
wireless networks broadly falls into the category of wireless man-
agement. 802.11 clients and APs are in a constant dance trying
to determine the best pairing. To address issues of mobility, clients
continually scan their environment looking for a better partner. APs
respond to these scans, and additionally broadcast beacons to nearby
clients. If a client switches APs, another set of exchanges takes
place that authenticates the client to the network and binds the
client and the AP (a process called association).
Additionally, APs must deal with signiﬁcant heterogeneity in
their client base, which includes distinct capabilities and conﬁg-
urations. Consequently, a negotiation takes place between clients
and APs about which features are needed — 802.11b vs. 802.11g
transmission, power savings, etc. Unintuitively, the choice of a
single notebook computer to associate with an AP can transform
that AP’s behavior as it tries to accommodate the lowest common
denominator among its clients. For example, in our previous work
we reported that the presence of a single 802.11b client — even one
that is not transmitting — will often force an AP into 802.11g “pro-
tection” mode, thereby degrading service for all 802.11g users. [7]
4.3 Infrastructure support
APs are fundamentally bridge devices. To obtain Internet con-
nectivity a client must in turn acquire an IP address — typically
via DHCP — and the MAC addresses of next-hops to destinations
— typically via ARP. These protocols exhibit complex dynamics
in themselves, and their failure may isolate a station for some time.
Their use with 802.11 exacerbates their complexity since they are
used in specialized ways, frequently tied together with VLANs us-
ing proprietary mobility management software that authenticates
stations via a single sign-on interface and allows IP addresses to
remain constant as a client roams between APs. There is no stan-
dard for implementing this functionality and, unsurprisingly, failure
modes are not well understood.
4.4 Transport layer
Finally, any underlying delays or losses are ultimately delivered
to the transport layer, usually TCP, which may amplify their effects
believing these behaviors to be indicative of congestion.
While this complex set of processes frequently works surpris-
ingly well, when it does not it can fail spectacularly and expose
users to signiﬁcant response time delays. It is the goal of this paper
to systematize the analysis of these issues to better understand the
source of such transient problems.
5. MEDIA ACCESS MODEL
In this section we describe a media access model for measuring
and inferring the critical path delays of a monitored frame trans-
mission.
The model consists of a representation of the wired distribution
network, queuing behavior in the AP, and frame transmission using
the 802.11 MAC protocol. The goal of the model is to determine
the various delays an actual monitored frame encounters as it tra-
verses the various stages of the wireless network path. At a high
level, our approach ﬁrst determines a series of timestamps for a
frame as it traverses this path and is ﬁnally transmitted by the AP.
From these timestamps we can compute the delays experienced by
the packet. Table 2 summarizes the deﬁnitions of the timestamps
and delays in our model, and Figure 2 illustrates where in the net-
work path they occur.
Our model uses measurements of the frame both on the wired
network and the wireless network to determine some of the times-
tamps. The challenge, however, lies in inferring the remaining
timestamps and, hence, delays. The inference techniques we de-
velop, along with the representations of AP queuing and the trans-
mission behavior necessary to perform the inferences, represent a
key contribution of this paper.
In the following sections we describe in detail our model compo-
nents and how we measure and infer these timestamps and delays.
We then show how the critical path delays determined by the model
can provide valuable, detailed insight into the media access behav-
ior of wireless users. Finally, we show how we can use the model
to diagnose problems with TCP throughput.
5.1 Critical path timestamps
To start our analysis, we ﬁrst measure the timestamp of each
packet as it leaves the wired gateway router on the way to a wireless
access point — a time we deﬁne as tw. We capture this information
using a SPAN port conﬁgured to forward a copy of each packet as
it leaves the building’s main distribution router. These copies are
directed to a dedicated tracing server where they are timestamped
(we assume that this propagation delay is constant).
To calculate additional timestamps we must combine observa-
tions of the packet on the wired network together with observations
of the packet on the wireless network. To match packets across
wireless and wired traces, we compare normalized packet contents
(adjusting for 802.11 vs Ethernet II frame formats) over a one sec-
ond window; one second reﬂects the empirical maximum wireless
forwarding delay of a wired packet in our network. Most matches
are one-to-one, meaning one wired packet corresponds to one wire-
less packet, but there are cases of one-to-many matches. For in-
stance, broadcast frames such as ARP requests can match multiple
wireless frames because each AP will forward the ARP request to
the wireless network. Occasionally, a packet is also dropped due to
AP queue overﬂows — typically when clients perform bulk down-
loads — which we detect based on frame sequence numbers. Over-
all we match 99.95% of the wired frames in our trace.
The next step is to determine when the AP has received the frame
from its wired interface. Since we do not have taps on the APs or
control the AP software, we cannot directly measure this time, ti,
and instead must infer it. ti is a function of the AP’s Ethernet I/O
delay and the propagation delay between the gateway router and the
AP. For each AP, we estimate ti by ﬁrst measuring the distribution
of the interval (ts − tw), the difference between the wireless trans-
mission time and the wired timestamp of the packet. The minimum
value of this distribution, minus DIFS, is the sum of wired network
delay and AP input processing delay for the minimum packet size.
From here, we determine the transmit queue timestamps of the
packet inside the AP, both when the packet enters the transmit
queue (tq) and when it reaches the head of the queue (th). We
model the AP as having three FIFO packet queues, the transmit
ready queue and two waiting queues based on the 802.11 standard.
If the packet is broadcast or multicast, the AP schedules it onto the
broadcast queue; the AP ﬂushes this queue into the transmit queue
after the next beacon transmission.
If the packet is destined to a
power-saving client, the AP buffers it on a power-save queue. The
AP ﬂushes the appropriate packets from the power-save queue into
the transmit queue when the client wakes up (by receiving a PSM-
reset data or management frame, or a PsPoll frame from the client).
Otherwise, the AP places the packet directly on the transmit queue.
It is critical to model the queuing behavior precisely to estimate
further wireless delays. For example, if we did not model pack-
ets sent to clients in power-save mode correctly, they would appear
to be delayed at the AP for tens of milliseconds. We determine
whether clients are in power-save mode when packets for them ar-
rive at the AP by tracking either the PSM bit of client frames in the
wireless trace, or when beacons indicate that the AP has buffered
packets for clients (TIM). Further, the 802.11 standard dictates that
Wired/Wireless
Monitor
Wireless
Gateway
Access Point
Tx Q
Broadcast Q
A
PowerSave Q
Time
t_w
t_i
t_q
t_h
t_s
 t_e
Timestamp Deﬁnitions
tw
ti
tq
th
ts
te
Delay Deﬁnitions
dps
dq
dmac
dt0
Frame leaves gateway
AP receives frame from wired interface
Frame enters radio transmit queue
Frame reaches head of transmit queue
First bit of the frame transmitted
End of last ACK or estimated ACK timeout
tq - ti: AP power-save/broadcast buffering delay
th - tq: AP (transmission) queuing delay
te - th: MAC delay
ts - th: Access delay of ﬁrst transmission attempt
d_ps
d_q
d_mac
Table 2: Summary of timestamps and delays determined by the
media access model.
Figure 2: Representation of wired distribution network, queu-
ing behavior in the AP, and frame transmission. The arrows in-
dicate where in the network we measure and infer timestamps
as frames traverse the network, and the corresponding delays
we calculate.
an AP should deliver broadcast frames at beacon intervals if power-
saving clients exist because these clients only wake up at those
times.
Based on the frame destination and client power status, we tag
each frame with the appropriate queue type. Subsequently, we es-
timate the time when the AP places the frame on the transmission
queue, tq. For a broadcast/multicast frame, tq is the time of the
latest beacon prior to the frame’s transmission. For frames des-
tined to power-saving clients, tq is the time the client notiﬁes the
AP that it has woken up by sending a frame with the PSM bit off
such as a PsPoll control frame. For the remainder of the frames,
tq = ti because the AP schedules them on the transmission queue
immediately after it has received them from the wired interface.
Next, we infer the time when the packet reaches the head of the
queue, th, and the AP is ready to transmit it using 802.11 DCF.
We determine th under three conditions based upon the end time of
the previous frame exchange, tpe. First, if the AP places the frame
on the transmit queue before the previous transmission completes,
then the frame experiences head-of-line blocking. We conclude the
frame reaches the head of the queue after the previous frame ex-
change ﬁnishes (th = tpe), and we label this frame as “head-of-line
blocked.” According to the 802.11 standard, a sender must perform
a mandatory backoff at the end of each frame exchange to provide
fair channel access. We cannot directly measure this random back-
off window but we know the maximum of this window from the
standard. Therefore, if the frame enters the queue beyond the max-
imum mandatory backoff window after tpe, the frame must ﬁnd the
transmit queue empty and the AP can transmit immediately. Hence,
th = tq, and the packet is labeled as “not head-of-line blocked.” Fi-
nally, if the AP places the frame on the transmit queue during the
maximum mandatory backoff window of the previous attempt, the
frame may or may not experience head-of-line blocking by the pre-
vious frames. Since this backoff window is very small (300 µs in
802.11g), less than 1% of the frames fall into this category. We
assume the transmit queue was empty at tq and the frame does not
encounter head of line blocking. Thus th = tq as well.
We determine the starting and ending transmission times of the
frame exchange, ts and te, directly from the synchronized trace.
The start time ts is the start of the ﬁrst transmission attempt, in-
cluding the control overhead of RTS/CTS and CTS-to-self. The
end time te is the end of the frame exchange: the end of the ACK
of the last transmission attempt, including all retransmissions and
contention. For unacked broadcast frames, te is the scheduled end
time of the transmission (NAV end). Consequently, for unacked
broadcast frames te is the end time of the data frame plus 60 µs.
Frames internally generated in the AP represent a special case
because we cannot observe when the AP generates them. For ex-
ample, we do not know when the AP has scheduled a scan response
because no corresponding packet appears in the wired trace. Fortu-
nately, these frames are typically management responses to client
requests, such as scan responses and association/authentication re-
sponses. We assume that the AP generates these responses and
places them on the transmit queue (tq) immediately after it receives
the requests.
5.2 Critical path delays
We calculate the critical path delays as intervals between times-
tamps. In particular, the buffering delay for power-saving clients
and broadcast frames is dps = tq - ti, the time from when the frame
reaches the AP and when the AP places the frame on the trans-
mit queue. We label this “power-saving delay” because broadcast
frames are buffered for power-saving clients who periodically wake
up at beacon intervals. The AP transmission queuing delay is dq =
th - tq, the time between when the AP places the frame on the
queue and the time when it reaches the head of the queue (i.e., the
AP is ready to transmit it). After the frame reaches the head of the
transmit queue, dmac is the time the AP takes to perform a frame
exchange to the receiver including clear channel assessments, PHY
(re-)transmissions, and any exponential backoffs. Thus, dps + dq +
dmac is the total time the packet spends in the wireless distribution
network.
We further categorize the queuing delay dq into three compo-
nents: delay caused by background management frames such as
beacons, scan responses, etc. (dqb), unicast frames to the same
client (dqs), and unicast frames to other clients (dqo); dq = dqb +
dqs + dqo. These values are calculated by modeling the contents of
the AP queue, characterizing frames queued earlier and summing
their media access delays (dmac).
5.3 Validating the model
To validate our AP model, ideally we would instrument an AP
and compare our inferred timings and the actual ones for every
frame transmitted. Unfortunately, we do not have access to com-
mercial APs or open-source 802.11 drivers that export queuing or
channel-probe delay timestamps on a per-frame basis. However,
we can examine the delays inferred by our model and determine
whether those delays are consistent with delays expected from the
known operation of the 802.11 MAC protocol.
N = CWmin
1. Wait DIFS until channel becomes idle.
2. If channel is not busy, go to step 4.
3. Perform a regular backoff
bo = rand[0, N]
While bo > 0
probe channel for 20us
if busy wait DIFS until idle.
--bo
4. Send the frame; if no ACK is received,
double N and retry from step 1.
5. N = CWmin, perform a mandatory backoff
as in step 3.
Figure 3: Simpliﬁed 802.11 DCF operation for unicast.
First we examine the distribution of the access delay of the ﬁrst
transmission attempt, dt0 = ts - th, from a TCP ﬂow from our trace.
Figure 4 shows the cumulative distribution of dt0 in microseconds
for one hour of frame exchanges that are “head-of-line blocked”
from an Avaya AP to a client using 802.11g to perform a bulk TCP
download. Most of the trafﬁc from the AP is destined to that client
during that hour. We focus on the ﬁrst transmission attempt of
head-of-line blocked frames (typical for bulk downloads) because
of the predictable delay distributions that should result from the
802.11 protocol.
To explain the distribution, we ﬁrst summarize the 802.11 trans-
mission process in Figure 3. This code segment is a simpliﬁed ver-
sion of the unicast DCF operation in the 802.11 standard [11]. For