## 总结 {#20.html#-}今天是矩阵分解三篇的最后一篇，传统的矩阵分解，无论是隐式反馈还是显式反馈，都是希望更加精准地预测用户对单个物品的偏好，而实际上，如果能够预测用户对物品之间的相对偏好，则更加符合实际需求的直觉。BPR就是这样一整套针对排序的推荐算法，它事实上提出了一个优化准则和一个学习框架，至于其中优化的对象是不是矩阵分解并不是它的重点。但我在这里结合矩阵分解对其做了讲解，同时还介绍了排序时最常用的评价指标AUC 及其计算方法。你在看了 BPR算法针对矩阵分解的推荐计算过程之后，试着想一想，如果不是矩阵分解，而是近邻模型，那该怎么做？欢迎留言给我，一起聊聊。![](Images/0bf4527a71cbaa4970667e52f6ff866f.png){savepage-src="https://static001.geekbang.org/resource/image/9f/93/9f92d45e88830b1713c10e320dca3293.jpg"}
# 【矩阵分解】那些在Netflix Prize中大放异彩的推荐算法早在前几篇务虚的文章中，我就和你聊过了推荐系统中的经典问题，其中有一类就是评分预测。让我摸着自己的良心说，评分预测问题只是很典型，其实并不大众，毕竟在实际的应用中，评分数据很难收集到，属于典型的精英问题；与之相对的另一类问题行为预测，才是平民级推荐问题，处处可见。
## 缘起评分预测问题之所以"虽然小众却十分重要"，这一点得益于十多年前 NetflixPrize 的那一百万美元的悬赏效应。公元 2006 年 10 月 2号，对于很多人来说，这只是平凡了无新意的一天，但对于推荐系统从业者来说，这是不得了的一天，美国著名的光盘租赁商Netflix突然广发英雄帖，放下"豪"言，这个就是土豪的"豪"，凡是能在他们现有推荐系统基础上，把均方根误差降低10% 的大侠，可以瓜分 100 万美元。消息一出，群贤毕至。Netflix放出的比赛数据，正是评分数据，推荐系统的问题模式也是评分预测，也就是为什么说，评价标准是均方根误差了。这一评分预测问题在一百万美元的加持下，催生出无数推荐算法横空出世，其中最为著名的就是一系列矩阵分解模型，而最最著名的模型就是SVD以及其各种变体。这些模型后来也经受了时间检验，在实际应用中得到了不同程度的开枝散叶。今天我就来和你细聊一下矩阵分解，SVD 及其最有名的变种算法。
## 矩阵分解
### 为什么要矩阵分解聪明的你也许会问，好好的近邻模型，一会儿基于用户，一会儿基于物品，感觉也能很酷炫地解决问题呀，为什么还要来矩阵分解呢？刨除不这么做就拿不到那一百万的不重要因素之外，矩阵分解确实可以解决一些近邻模型无法解决的问题。我们都是读书人，从不在背后说模型的坏话，这里可以非常坦诚地说几点近邻模型的问题：1.  物品之间存在相关性，信息量并不随着向量维度增加而线性增加；2.  矩阵元素稀疏，计算结果不稳定，增减一个向量维度，导致近邻结果差异很大的情况存在。上述两个问题，在矩阵分解中可以得到解决。矩阵分解，直观上说来简单，就是把原来的大矩阵，近似分解成两个小矩阵的乘积，在实际推荐计算时不再使用大矩阵，而是使用分解得到的两个小矩阵。具体说来就是，假设用户物品的评分矩阵 A 是 m 乘以 n 维，即一共有 m个用户，n 个物品。我们选一个很小的数 k，这个 k 比 m 和 n都小很多，比如小两个数量级这样，通过一套算法得到两个矩阵 U 和 V，矩阵 U的维度是 m 乘以 k，矩阵 V 的维度是 n 乘以 k。这两个矩阵有什么要求呢？要求就是通过下面这个公式复原矩阵A，你可以点击文稿查看公式。]{.MathJax_Preview style="color: inherit; display: none;"} {.MathJax_Display style="text-align: center;"}``{=html}[[[[[U[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#21.html#MathJax-Span-4.mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.155em, 1000.75em, 4.144em, -999.998em); top: -3.998em; left: 0em;"}[m]{#21.html#MathJax-Span-7.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[×]{#21.html#MathJax-Span-8.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}k]{#21.html#MathJax-Span-11.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#21.html#MathJax-Span-10.mrow}]{#21.html#MathJax-Span-9 .texatom}]{#21.html#MathJax-Span-6.mrow}]{#21.html#MathJax-Span-5.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; top: -3.856em; left: 0.708em;"}]{style="display: inline-block; position: relative; width: 2.308em; height: 0px;"}]{#21.html#MathJax-Span-3.msubsup}[V[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.191em;"}]{#21.html#MathJax-Span-13.mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.155em, 1000.75em, 4.144em, -999.998em); top: -3.998em; left: 0em;"}[T[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#21.html#MathJax-Span-16.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#21.html#MathJax-Span-15.mrow}]{#21.html#MathJax-Span-14.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.391em, 1000.57em, 4.144em, -999.998em); top: -4.327em; left: 0.896em;"}[n]{#21.html#MathJax-Span-19.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[×]{#21.html#MathJax-Span-20.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}k]{#21.html#MathJax-Span-23.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#21.html#MathJax-Span-22.mrow}]{#21.html#MathJax-Span-21 .texatom}]{#21.html#MathJax-Span-18.mrow}]{#21.html#MathJax-Span-17.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.391em, 1001.41em, 4.144em, -999.998em); top: -3.668em; left: 0.567em;"}]{style="display: inline-block; position: relative; width: 2.026em; height: 0px;"}]{#21.html#MathJax-Span-12.msubsup}[≈]{#21.html#MathJax-Span-24 .mostyle="font-family: MathJax_Main; padding-left: 0.285em;"}[A]{#21.html#MathJax-Span-26.mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.155em, 1000.71em, 4.144em, -999.998em); top: -3.998em; left: 0em;"}[m]{#21.html#MathJax-Span-29.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[×]{#21.html#MathJax-Span-30.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}n]{#21.html#MathJax-Span-33.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#21.html#MathJax-Span-32.mrow}]{#21.html#MathJax-Span-31 .texatom}]{#21.html#MathJax-Span-28.mrow}]{#21.html#MathJax-Span-27.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; top: -3.856em; left: 0.755em;"}]{style="display: inline-block; position: relative; width: 2.402em; height: 0px;"}]{#21.html#MathJax-Span-25.msubsup style="padding-left: 0.285em;"}]{#21.html#MathJax-Span-2.mrow}[]{style="display: inline-block; width: 0px; height: 2.355em;"}]{style="position: absolute; clip: rect(1.414em, 1008.1em, 2.826em, -999.998em); top: -2.351em; left: 0em;"}]{style="display: inline-block; position: relative; width: 8.096em; height: 0px; font-size: 125%;"}[]{style="display: inline-block; overflow: hidden; vertical-align: -0.468em; border-left: 0px solid; width: 0px; height: 1.591em;"}]{#21.html#MathJax-Span-1.mathstyle="width: 10.12em; display: inline-block;"}``{=html}[$$U_{m \times k}V_{n \times k}^{T} \approx A_{m \times n}$$]{.MJX_Assistive_MathML.MJX_Assistive_MathML_Blockrole="presentation"}]{#21.html#MathJax-Element-1-Frame .MathJaxtabindex="0" style="text-align: center; position: relative;"mathml="Um×kVn×kT≈Am×n"role="presentation"}$$$$类似这样的计算过程就是矩阵分解，还有一个更常见的名字叫做 SVD；但是，SVD和矩阵分解不能划等号，因为除了 SVD 还有一些别的矩阵分解方法。