title:GARFIELD: System Support for Byzantine Machine Learning (Regular
Paper)
author:Rachid Guerraoui and
Arsany Guirguis and
J&apos;er&apos;emy Plassmann and
Anton Ragot and
S&apos;ebastien Rouault
1
2
0
0
0
.
1
2
0
2
.
7
8
9
8
4
N
S
D
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
7
-
2
7
5
3
-
4
5
6
6
-
1
-
8
7
9
|
)
N
S
D
(
s
k
r
o
w
t
e
N
d
n
a
s
m
e
t
s
y
S
e
l
b
a
d
n
e
p
e
D
n
o
e
c
n
e
r
e
f
n
o
C
l
a
n
o
i
t
a
n
r
e
t
n
I
P
I
F
I
/
E
E
E
I
l
a
u
n
n
A
t
s
1
5
1
2
0
2
2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
GARFIELD: System Support for Byzantine
Machine Learning (Regular Paper)
Rachid Guerraoui
Arsany Guirguis
J´er´emy Plassmann
Anton Ragot
S´ebastien Rouault
EPFL
EPFL
EPFL
EPFL
EPFL
rachid.guerraoui@epﬂ.ch
arsany.guirguis@epﬂ.ch
jeremy.plassmann@epﬂ.ch
anton.ragot@epﬂ.ch
sebastien.rouault@epﬂ.ch
Abstract—We present GARFIELD, a library to transparently
make machine learning (ML) applications, initially built with
popular (but fragile) frameworks, e.g., TensorFlow and PyTorch,
Byzantine–resilient. GARFIELD relies on a novel object–oriented
design, reducing the coding effort, and addressing the vulnera-
bility of the shared–graph architecture followed by classical ML
frameworks. GARFIELD encompasses various communication
patterns and supports computations on CPUs and GPUs, allowing
addressing the general question of the practical cost of Byzantine
resilience in ML applications.
We report on the usage of GARFIELD on three main ML
architectures: (a) a single server with multiple workers, (b) sev-
eral servers and workers, and (c) peer–to–peer settings. Using
GARFIELD, we highlight interesting facts about the cost of
Byzantine resilience. In particular, (a) Byzantine resilience, unlike
crash resilience, induces an accuracy loss, (b) the throughput
overhead comes more from communication than from robust
aggregation, and (c) tolerating Byzantine servers costs more than
tolerating Byzantine workers.
Index Terms—Distributed Machine Learning; Byzantine Fault
Tolerance; Robust Machine Learning.
I. INTRODUCTION
Machine Learning (ML) is nowadays distributed [33], [36].
A major motivation is scalability. The quantity of data avail-
able to ML tasks is huge and can only be handled with dis-
tributed architectures. For instance, the size of Google’s ad im-
pression log to train an ad click predictor could reach trillions
of examples [30], each representing a high-dimensional feature
vector. Such a dataset expands daily with new examples [35]
in order to yield better models.
A now classical approach to distribute an ML task is through
the server/worker architecture [34]. A parameter server coor-
dinates the distribution of the training task among a large set
of worker nodes. The parameter server typically aggregates the
workers’ gradients by merely averaging them [13], following
the standard workhorse optimization algorithm: Stochastic
Gradient Descent (SGD) [46]. An alternative, more decen-
tralized, approach does not distinguish servers and workers.
Each node has a copy of the model and keeps its data
locally [41], [26], typically to protect it and save bandwidth or
devices’ batteries. In this approach, all nodes apply SGD and
communicate (in a peer–to–peer fashion) what they learned so
far to reﬁne their models,usually also through averaging.
As the number of participating machines in a distributed
setup increases, so does the probability of failure of any of
these machines. In distributed computing, the most general
way to model such failures is to assume an adversary that
can control a subset of the system and make it arbitrarily
deviate from the normal execution: we talk about Byzantine
failures [32]. This includes bogus software, faulty hardware as
well as malicious attacks. With the increasing use of ML in
mission-critical applications [23], [12], [44], building robust
systems against these kinds of failures becomes a necessity.
Using vanilla state machine replication (SMR) to solve such
a problem was shown impractical in the ML context [17].
Tolerating Byzantine machines without replicating them has
been recently well studied with various models and settings,
e.g., in [8], [49], [20]. A key idea is to replace the vul-
nerable averaging scheme by a statistically robust gradient
aggregation rule (GAR), e.g., Median [51]. Most work on
Byzantine resilient ML (e.g. [20], [19], [11]) has however
been theoretical and, it is not clear how to put the published
algorithms to work, especially in the pragmatic form of library
extensions to existing, and now classical, ML frameworks,
namely TensorFlow [6] and PyTorch [40]. These frameworks
share two speciﬁc characteristics that go against Byzantine
resilience. First, and for performance reasons, they rely on
a shared memory design. For instance, TensorFlow uses one
shared computation graph among all machines to deﬁne the
learning pipeline. Such a design is problematic as Byzantine
nodes can corrupt the learning state at honest ones. Second,
most of the high–level communication abstractions given by
such frameworks assume trusted, highly–available machines.
For instance, the distributed library of PyTorch allows for
collective communication among processes, yet such calls
block indeﬁnitely in case of a process crash or network failure.
We present in this paper GARFIELD, a library that enables
the development of Byzantine ML applications on top of
popular frameworks such as TensorFlow and PyTorch, while
achieving transparency: applications developed with either
framework do not need to change their interfaces to toler-
ate Byzantine failures. Essentially, GARFIELD addresses the
Byzantine ML problem from a practical point of view, unlike
most of the previous works, which are mainly theoretical. This
helps ﬁnd insightful facts about the very practical costs of
Byzantine resilience in ML systems, as we show later.
GARFIELD adopts an object–oriented vision for ML applica-
tions. The novelty of GARFIELD lies in (1) the way the objects,
be they servers or workers, communicate, and (2) the way they
aggregate the replies, be they models or gradients. Essentially,
978-1-6654-3572-7/21/$31.00 ©2021 IEEE
DOI 10.1109/DSN48987.2021.00021
39
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
is to see ML applications from the objects’
our insight
perspective, rather than from an algorithmic perspective; this
enables to achieve both transparency and Byzantine resilience.
GARFIELD allows for both synchronous and asynchronous
communication, and it
includes several statistically-robust
gradient aggregation rules (GARs). GARFIELD also supports
full–stack computations on both CPUs and GPUs.
Along our GARFIELD implementation journey, we took sev-
eral design decisions to promote its practicality. For instance,
we implemented speciﬁc schemes to parallelize Byzantine–
resilient GARs, especially on GPUs. Moreover, we devised
the notion of separate replicated graphs for TensorFlow rather
than relying on its shared graph design, as the latter would
be a killer in a fully Byzantine environment, i.e., without any
trusted machine [17]. We rely on gRPC for point–to–point
pull–based communication, due to its speed and because it is
currently the defacto standard communication method for pop-
ular ML frameworks. Our implementation parallelizes RPC
calls to improve the scalability of algorithms implemented
with GARFIELD. Finally, we carefully manage CPU memory
to minimize memory copying and allow for faster algorithms.
We report on the usage of GARFIELD on three ML archi-
tectures: (1) tolerating Byzantine workers while assuming one
trusted, central server, (2) replicating the parameter server to
account for Byzantine servers as well as Byzantine workers,
and (3) considering a peer-to-peer, decentralized setting with
no distinction between servers and workers.
We report on our evaluation of GARFIELD, addressing the
general question of the practical cost of Byzantine resilience
in a distributed ML deployment. We consider various ML
models, as well as different hardware, i.e., CPUs and GPUs.
We also study the cost of different degrees of resilience.
Essentially, we show that Byzantine resilience introduces
up to 10% loss in accuracy compared to non–Byzantine
deployments. In contrast, crash resilience does not introduce
any such loss. In terms of throughput, we quantify the
overhead of various Byzantine resilience degrees, compared
to a vanilla deployment. Basically, we ﬁnd that tolerating
Byzantine servers induces much more overhead than tolerating
Byzantine workers. For instance, we quantify the cost of
adding Byzantine resilience to servers, compared to tolerating
only Byzantine workers with a trusted server, to 53%, and the
cost of Byzantine resilience, compared to the crash–tolerant
baseline, to 22% (using GPUs). We root the resilience over-
head mainly to communication. Concretely, our experiments
show that communication accounts for more than 75% of the
overhead while robust aggregation contributes to only 11%
of such an overhead. We also highlight the very fact that
Byzantine algorithms in a peer-to-peer setup do not scale,
unlike those following the parameter server architecture. We
also report on the fact the overhead of Byzantine resilience
depends more on the number of participating nodes, be they
workers or servers, than on the model dimension. Notably,
using GPUs achieves a performance improvement of at least
one order of magnitude over CPUs.
GARFIELD is an open–source project and is available at [5].
II. BACKGROUND
A. Stochastic Gradient Descent
Stochastic Gradient Descent (SGD) [46] is the most widely-
used optimization algorithm in ML applications [15], [33]. It
becomes the defacto standard to optimize objective functions
that can be used with ML techniques such as neural networks.
To explain how SGD works, assume the objective function
(also called loss function) to be L (x) ∈ R. This basically
measures “how incorrect the model is when labeling an input”.
SGD addresses the following optimization problem:
xopt (cid:2) arg min
x∈Rd
(cid:3)
(cid:2)
L(x)
(1)
x(k), ξ
The procedure is iterative: in each step k, one can estimate the
, with a subset ξ of size b of the training
gradient G
set, called mini–batch. This represents an approximation of
the uncomputable real gradient. Then, using the estimated
gradient, the model parameters (x) are updated as follows:
(cid:5)
(cid:4)
= x(k) − γk · G
where {γk} is called the learning rate.
B. Distributed Machine Learning