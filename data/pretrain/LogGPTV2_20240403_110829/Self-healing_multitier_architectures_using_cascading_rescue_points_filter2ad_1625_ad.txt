(cid:12)(cid:12)
(cid:13)(cid:14)(cid:15)(cid:9)(cid:16)(cid:17)(cid:18)(cid:19)(cid:9)(cid:20)(cid:8)(cid:10)(cid:21)(cid:10)(cid:9)(cid:20)(cid:9)(cid:21)(cid:22)(cid:4)(cid:13)(cid:23)(cid:21)(cid:24)(cid:25)(cid:20)(cid:8)(cid:17)(cid:26)
(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:11)
Figure 9: The cascading rescue points protocol en-
capsulates user data using a small header prepended
to every data write made by the user.
ifying the size of their arguments and whether they return
data. For example, the read(int fd, void *buf, size t count)
system call writes data in the pointer speciﬁed by its sec-
ond argument. The amount of written data depends on its
return value. More elaborate calls like socketcall() are han-
dled by deﬁning a call back that de-multiplexes the various
network calls implemented by it.
4.3 Cascading Rescue Point Protocol
I/O Interception
4.3.1
The cascading rescue point protocol is used to communi-
cate events between peers exchanging data over TCP sock-
ets. The protocol is implemented transparently over the
sockets used by the application. This is done by intercept-
ing system calls used with TCP sockets. We can classify
these system calls into two groups. The ﬁrst group consists
of calls handling socket creation and termination, and the
second group is dealing with data transmission and recep-
tion. We intercept system calls socket(), close(), shutdown(),
connect(), accept(), socketpair(), and the dup() family of
calls to track the state of descriptors used by the applica-
tion (i.e., distinguish TCP socket descriptors from others,
like ﬁles). For this reason, we maintain a global array to
store information on active descriptors, like their type and
protocol state data. We also intercept the read(), write(),
recv(), and send() family of system calls that are used to
transmit and receive data from sockets to implement our
protocol.
The protocol consists of variable length messages that en-
capsulate user data as shown in Fig. 9. In particular, we use
a small header that comprises of a 4-byte ﬁeld specifying
the length of user data, and a single-byte CMD ﬁeld used
to communicate events to remote peers.
The header is inserted into existing TCP streams using
Pin to replace system calls used to write data, like write()
and send(), with writev() which allows us to transmit data
from multiple buﬀers by performing a single call. This min-
imizes the number of operations (data copies and system
calls) required to transparently inject the header into the
stream.
If the message cannot be written in its entirety,
for instance because non-blocking I/O is performed and the
kernel buﬀers are full, we keep trying until we are successful.
To extract the header from the stream, the reverse pro-
cedure is followed. Initially, we replace calls used to receive
data with readv() to read into multiple buﬀers. If necessary,
we repeat the process until the whole header is received.
User data is placed directly in the buﬀer supplied by the
application. However, we can read into the next message,
which will be placed into the application’s buﬀer. When
this happens, we move the data belonging to subsequent
messages into a buﬀer associated with the socket descriptor.
Consequent reads will read data from this buﬀer instead of
performing a system call. Reading one message at a time
may be suboptimal performance-wise, but allows us to pair
Table 2: Applications and benchmarks used for the
evaluation of CRP. All of applications contain ex-
ploitable bugs as described by their common vulner-
ability and exposure (CVE) id.
Application
MySQL v5.0.67
Bug type
Input validation
(CVE-2009-4019)
Benchmark
MySQL’s
test-select
Apache v1.3.24 Memory corruption Apache’s ab
(CVE-2002-0392)
utility
read system calls with particular events received on a socket
(e.g., a request to begin checkpointing), which is necessary
for rolling back.
4.3.2 Protocol Commands
The CMD ﬁeld in the protocol is used to inform remote
peers of changes in the state of the running thread. For in-
stance, when data are written to a socket while in a rescue
point, CMD changes to indicate that the destination should
also begin checkpointing, the socket is marked as having
been signaled to checkpoint, and is placed in a list contain-
ing other similar sockets (fd checkpointed ). If an error oc-
curs and the thread rolls back, sockets in fd checkpointed are
marked accordingly, so that the next write will convey the
status change. If the next write occurs within a RP, the fact
is also passed to the remote process, so that it ﬁrst rolls back
memory changes and then enters a new checkpoint.
On the receiving end, if a thread receives a command to
checkpoint, it begins checkpointing similarly to entering a
RP. The socket descriptor number where the command was
received is saved, so that a consequent request to roll back is
only honored, if it was received on the same socket. On roll-
back, execution resumes right before the system call that
caused the thread to checkpoint. Note that receiving re-
quests to begin checkpointing from other sockets, while al-
ready checkpointing or executing in a RP are ignored (dis-
cussed in Sec. 3.2).
Checkpoint Commits Through Out-of-band Signaling.
To notify remote peers of a successful exit from a RP, we
utilize out-of-band (OOB) signaling, as provided by the TCP
protocol and the OS. In particular, we make use of TCP’s
OOB data to notify a remote application that it should also
commit changes performed within a checkpoint. We send
OOB data by using the send() system call and supplying
the MSG OOB ﬂag for every descriptor in fd checkpointed.
On the receiver, the reception of an OOB signal by the OS,
causes the signal SIGURG to be delivered to the thread,
which previously took ownership of the socket descriptor
that triggered the checkpointing by calling fcntl().2 The
signal is intercepted, and execution is switched from check-
pointing to normal execution. If a RP is entered very fre-
quently, multiple OOB signals can be transmitted in succes-
sion. On account of TCP’s limitations, only a single OOB
byte can be pending at any time, so previous OOB signals
are essentially overwritten. This does not aﬀect the correct
operation of our system, but unfortunately implies that we
2In Linux a thread can take ownership of a descriptor, caus-
ing the OS to deliver all descriptor related asynchronous
events to the speciﬁc thread, instead of a randomly selected
thread of the process.
385
Native
Pin
Traditional RP
Cascading RP
45.7%
52.19%
71.96%
)
s
/
q
e
R
(
t
u
p
h
g
u
o
r
h
T
1250
1000
750
500
250
0
−12.28%
−9.78%
Native
Pin
Traditional RP
Cascading RP
−4.54%
−7.27%
0
50
100
200
150
250
Total time (sec)
300
350
16K
32K
64K
128K
Page size
Figure 10: MySQL performance
Figure 11: Apache performance
cannot also use OOB signaling to notify remote peers of roll
backs.
5. EVALUATION
We evaluated our implementation to establish its eﬀective-
ness and performance. First, we validated the eﬀectiveness
of CRPs in addressing state inconsistency issues that arise
when using RPs to recover from errors in interconnected
client-server applications. Second, we evaluated the perfor-
mance overhead imposed by CRP with real server applica-
tions. In both cases, we employed existing benchmarks and
tools to generate workloads. Table 2 lists the applications
and benchmarks used during the evaluation. We conducted
the experiments presented in this section on two DELL Pre-
cision T5500 workstations with dual 4-core Xeon CPUs and
24GB of RAM, one running Linux 2.6 and acting as a server
and the other one running Linux 3.0 and acting as the client.
5.1 Effectiveness
We used our tool to deploy RPs on known bugs in the
applications listed in Table 2, while concurrently running
the corresponding benchmarks from the client-side. When
RPs are not employed, the applications terminate and the
benchmarks are interrupted in all cases. In contrast, when
using RPs the applications recover from the error and the
benchmarks concluded successfully.
We also used our own artiﬁcial client-server applications,
that employed our mechanism to cascade a RP, which en-
gulfed the exchange of messages between the peers. We
manually injected faults in the client and observed that both
peers did not crash, but instead they managed to revert to
consistent states (i.e., the state they had before entering the
RP).
5.2 Performance
For each application in Table 2, we performed the cor-
responding benchmark, ﬁrst with the application executing
natively, second running under the Pin DBI framework, then
employing traditional RPs, and ﬁnally with CRPs. This al-
lows us to quantify the overhead imposed by CRP compared
with native execution and execution under Pin, as well as
the relative overhead compared with the baseline of a self-
healing tool with traditional RPs. In the tests described in
this section, we did not inject any requests that would trig-
ger the bugs each application suﬀers from, nevertheless the
relevant RPs to the known bugs were installed and in the
last case the CRP mechanism was employed.
Figure 10 shows the results obtained after running 10 iter-
ations of MySQL’s test-select benchmark test over an 1Gb/s
network link. The y-axis lists the four diﬀerent server con-
ﬁgurations tested, which from top to bottom are: native ex-
ecution, execution over Pin, execution with our self-healing
tool and traditional RPs, and ﬁnally execution with CRPs.
The x-axis shows the average time (in seconds) needed to
complete the benchmark, while the errors bars represent
standard deviation. Note that the standard deviation for
is insigniﬁcant and thus not visible. We observe that the
benchmark takes on average 71.96% more time to complete
when running the server with a CRP, while a signiﬁcant
part of the overhead is because of Pin (under Pin the test
takes 45.7% more time). In particular, the high overhead is
attributed to the signiﬁcantly larger size of MySQL’s code
consisting of many indirect control transfers (e.g., callback
functions and frequent function return), which exerts pres-
sure on Pin’s JIT compiler and code cache.
Figure 11 depicts the results obtained after performing 10
iterations of Apache’s ab benchmark utility over an 1Gb/s
network link. The y-axis displays the average throughput
in requests per second as reported by ab, and the error bars
represent the standard deviation. We performed the experi-
ments requesting ﬁles of size 16K, 32K, 64K and 128K from
the web server (as listed in the x-axis), and we repeated each
test with the corresponding server running: natively, over
Pin, with the traditional RPs, and with CRPs. Apache per-
forms on average 8.46% slower when using CRPs, and Pin
seems to be responsible for the biggest part of this overhead.
Note that this diﬀerence drops as the size of the requested
ﬁle increases. This is due to the workload becoming more