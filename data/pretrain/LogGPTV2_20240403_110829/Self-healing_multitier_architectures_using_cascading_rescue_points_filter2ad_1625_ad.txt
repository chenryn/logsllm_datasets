# 4.3 Cascading Rescue Point Protocol

## 4.3.1 I/O Interception

The cascading rescue point (CRP) protocol facilitates communication of events between peers exchanging data over TCP sockets. This protocol is implemented transparently over the sockets used by the application, achieved through the interception of system calls associated with TCP sockets. These system calls can be categorized into two groups: those that handle socket creation and termination, and those that manage data transmission and reception.

For the first group, we intercept system calls such as `socket()`, `close()`, `shutdown()`, `connect()`, `accept()`, `socketpair()`, and the `dup()` family of calls to track the state of descriptors used by the application. We maintain a global array to store information on active descriptors, including their type and protocol state data. For the second group, we intercept the `read()`, `write()`, `recv()`, and `send()` family of system calls, which are used for data transmission and reception.

### Message Structure
The CRP protocol uses variable-length messages to encapsulate user data, as illustrated in Figure 9. Each message includes a small header that consists of:
- A 4-byte field specifying the length of the user data.
- A single-byte CMD field used to communicate events to remote peers.

### Header Injection and Extraction
To inject the header into existing TCP streams, we use Pin to replace system calls like `write()` and `send()` with `writev()`. This allows us to transmit data from multiple buffers with a single call, minimizing the number of operations required to inject the header.

If the entire message cannot be written at once (e.g., due to non-blocking I/O and full kernel buffers), we continue attempting until successful. To extract the header, we replace calls used to receive data with `readv()` to read into multiple buffers. If necessary, we repeat the process until the entire header is received. User data is placed directly into the buffer supplied by the application, and subsequent messages are managed accordingly.

## 4.3.2 Protocol Commands
The CMD field in the CRP protocol is used to inform remote peers of changes in the state of the running thread. For example, when data is written to a socket during a rescue point, the CMD field indicates that the destination should also begin checkpointing. The socket is then marked and added to a list of similar sockets (`fd_checkpointed`).

If an error occurs and the thread rolls back, sockets in `fd_checkpointed` are marked to convey the status change. If the next write occurs within a rescue point, this information is passed to the remote process, which then rolls back memory changes and enters a new checkpoint.

On the receiving end, if a thread receives a command to checkpoint, it begins checkpointing similarly to entering a rescue point. The socket descriptor number where the command was received is saved, ensuring that a subsequent rollback request is honored only if received on the same socket. Execution resumes right before the system call that caused the thread to checkpoint.

### Checkpoint Commits Through Out-of-band Signaling
To notify remote peers of a successful exit from a rescue point, we use out-of-band (OOB) signaling provided by the TCP protocol and the OS. OOB data is sent using the `send()` system call with the `MSG_OOB` flag for every descriptor in `fd_checkpointed`.

On the receiver, the reception of an OOB signal causes the `SIGURG` signal to be delivered to the thread, which previously took ownership of the socket descriptor. The signal is intercepted, and execution switches from checkpointing to normal execution. If a rescue point is entered frequently, multiple OOB signals can be transmitted in succession. However, due to TCP limitations, only a single OOB byte can be pending at any time, so previous OOB signals are overwritten. This does not affect the correct operation of our system but means we cannot use OOB signaling to notify remote peers of rollbacks.

# 5. Evaluation

We evaluated our implementation to assess its effectiveness and performance. First, we validated the effectiveness of CRPs in addressing state inconsistency issues in interconnected client-server applications. Second, we evaluated the performance overhead imposed by CRP using real server applications. Table 2 lists the applications and benchmarks used during the evaluation.

## 5.1 Effectiveness
We deployed RPs on known bugs in the applications listed in Table 2, while concurrently running the corresponding benchmarks from the client-side. When RPs are not employed, the applications terminate, and the benchmarks are interrupted. In contrast, using RPs, the applications recover from errors, and the benchmarks conclude successfully.

We also tested artificial client-server applications that employed our mechanism to cascade a RP, engulfing the exchange of messages between peers. Manually injected faults in the client showed that both peers did not crash but reverted to consistent states.

## 5.2 Performance
For each application in Table 2, we performed the corresponding benchmark under four configurations: native execution, execution under the Pin DBI framework, traditional RPs, and CRPs. This allowed us to quantify the overhead imposed by CRP compared to native execution and execution under Pin, as well as the relative overhead compared to a self-healing tool with traditional RPs.

### MySQL Performance
Figure 10 shows the results of running 10 iterations of MySQL’s `test-select` benchmark over a 1Gb/s network link. The y-axis lists the four different server configurations, and the x-axis shows the average time (in seconds) needed to complete the benchmark. The standard deviation is insignificant and thus not visible. The benchmark takes on average 71.96% more time to complete when running the server with a CRP, with a significant part of the overhead attributed to Pin (under Pin, the test takes 45.7% more time). The high overhead is due to the large size of MySQL’s code, consisting of many indirect control transfers, which pressure Pin’s JIT compiler and code cache.

### Apache Performance
Figure 11 depicts the results of performing 10 iterations of Apache’s `ab` benchmark utility over a 1Gb/s network link. The y-axis displays the average throughput in requests per second, and the error bars represent the standard deviation. We requested files of sizes 16K, 32K, 64K, and 128K from the web server. Apache performs on average 8.46% slower when using CRPs, with Pin responsible for the majority of this overhead. The difference decreases as the file size increases, likely due to the workload becoming more CPU-bound.