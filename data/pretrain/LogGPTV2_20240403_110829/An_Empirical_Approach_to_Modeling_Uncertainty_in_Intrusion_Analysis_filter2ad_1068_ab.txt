report those with high-conﬁdence evidence associated with
them.
Evaluation of the methodology: Finally, we applied
the SnIPS tool to two third-party datasets created from real
network activities as well as the production network in the
computer science department of a university that was used
to build the model. The core reasoning engine and model
were kept unchanged in the evaluation. We found remarkably
that even though our core reasoning model was developed
from a very simple and completely different incident, our
tool discovered interesting scenarios from these data sets.
The application of SnIPS also resulted in dramatic reduction
(99%) in the amount of data a system administrator would
have to look at. The false positives from the analysis helped
us identify imprecisions from the automatically generated
Snort knowledge base as well as some subtle but minor
gaps in the core model. This indicates that such an empirical
approach could produce a shared knowledge base that can
be iteratively reﬁned among security practitioners to yield
agile and accurate tools.
II. THE REASONING MODEL
We motivate our design using the true-life incident de-
scribed in Section I-A. We study the analytic states that the
SA went through in the course of the investigation, identify
the rationale behind the decisions at various points, and
design a logic that captures this reasoning process.
A. Modeling uncertainty
While the goal of intrusion analysis is detection of events
at a high-level of abstraction (e.g., a machine has been
compromised and has been used to compromise others),
tools today operate with any known accuracy only at low
levels of abstraction (e.g., network packets, server logs,
etc.). Uncertainty arises from this semantic gap as well. For
example, a packet pattern (a “signature”) could be associated
mostly with attacks but on occasion with legitimate use as
496
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:13:03 UTC from IEEE Xplore.  Restrictions apply. 
A1 :
A2 :
A3 :
A4 :
A5 :
obs(anomalyHighTrafﬁc)
obs(netﬂowBlackListFilter(H, BlackListedIP))
p(cid:2)−→ int(attackerNetActivity)
int(attackerNetActivity)
obs(netﬂowBlackListFilter(H, BlackListedIP))
(cid:2)−→
l
(cid:2)−→
l
obs(memoryDumpMaliciousCode(H))
int(compromised(H))
(cid:2)−→
l
int(compromised(H))
l
(cid:2)−→
obs(memoryDumpIRCSocket(H1, H2))
int(exchangeCtlMessage(H1, H2))
Figure 1. Observation correspondence
well. Furthermore, it may not tell us whether the attack
succeeded. A key step in tackling the uncertainty challenge
is to develop a model
that can link multiple low-level
observations to the conditions under concern at the high level
and simultaneously allow us to specify our conﬁdence in the
assertions.
We use three modes p, l, c, standing for “possible, likely,
certain” to express low, moderate, and high conﬁdence
levels. Even though one could think of certainty level as
a continuous quantity ranging from completely unknown
to completely certain, we found that, in practice, human
SA’s only deal with a few conﬁdence levels that roughly
correspond to the ones deﬁned here.These words are also
used routinely in natural-language description of security
knowledge bases such as the Snort rule repository. We
emphasize that
these uncertainty levels are assigned by
humans and apart from the obvious ordering (p < l < c)
we are not ascribing a probability range to each level.
With this qualitative notion of uncertainty, we intro-
duce the two types of logical assertions in our reasoning
model: observation correspondence which maps low-level
observations to high-level conditions, and internal model
which captures relationships among high-level conditions
(also called internal conditions hereafter) . Correspondingly,
we use obs(O) to denote a fact about observation O, and
int(F ) to denote an internal condition F . For example,
obs(netﬂowBlackListFilter(ip1, ip2)) is an observation from
the netﬂow blacklist ﬁlter that “machine ip1 is communicat-
ing with a known blacklisted (and hence likely malicious)
ip2”, whereas int(compromised(ip1)) is an internal
host
condition that “ip1 is compromised.”
B. Observation correspondence
Figure 1 shows the observation correspondence re-
lation for
the observations
in the true-life incidents
In A1 an abnormal high
described in Section I-A.
network trafﬁc obs(anomalyHighTrafﬁc)
is mapped to
int(attackerNetActivity), meaning an attacker is performing
some network activity. This is a low-conﬁdence judgment
thus the mode is p. Intuitively the p mode means there are
other equally possible interpretations for the same obser-
vation. A2 and A3 give the meaning to an alert identiﬁed
in netﬂow analysis. There are a number of ﬁltering tools
that can search for potential malicious patterns in a netﬂow
dump such as “capture daemon” and “ﬂow-nﬁlter.” These
rules deal with one ﬁlter that identiﬁes communication with
known malicious IP addresses. Since any such activity is
a strong indication of attacker activity and compromise of
the machine involved, the modality of the two rules is l.
There are still other possibilities, e.g. the communication
could be issued by a legitimate user who wants to ﬁnd out
something about the malicious IP address. But the likelihood
of that is signiﬁcantly lower than what is represented by
the right-hand side of the two rules. It
is legitimate to
have multiple observation correspondence assertions for the
same observation: they may represent different aspects or
possibilities of an observation. A4 says if memory dump on
machine H identiﬁes malicious code then H is likely to be
compromised. A5 says if the memory dump identiﬁes open
IRC sockets between machine H1 and H2 then it is likely
that the IRC channel was used to exchange control messages
between BotNet members.
We recognize that these observation correspondence as-
sertions are subjective. Quantifying the results of intrusion
sensing in a robust manner has remained a hard problem for
a variety of reasons [15]. Our goal is to create a ﬂexible
and lightweight framework wherein an SA can feed in
these beliefs of certainty and see what consequences arise.
For example, an SA may think the mode of A4 ought
to be c, which would be acceptable. One advantage of
such a logic is that it facilitates discussion and sharing of
security knowledge. Given the large base of similar deployed
infrastructure, shared experiences from a large community
can likely help tune the modes in those assertions. We
envision a rule repository model like that for Snort, where
a community of participants contributes and agrees upon
a set of rules in an open language. Currently there are
only coarse-grained classiﬁcation and some natural-language
explanations for the meanings behind each Snort alert. In
Section IV, we show how a small number of internal-
model predicates can give meanings to the vast majority of
Snort alerts and that the observation correspondence relation
can actually be automatically generated from a Snort rule’s
classtype and the “impact” and “ease of attack” ﬁelds in the
rule’s natural-language description. If the Snort rule writers
had a standard language for such information they would
be able to readily provide the observation correspondence
assertions for Snort alerts.
C. Internal model
The reasoning model should also express the logical re-
lations among the various high-level conditions so that such
knowledge can be mapped to correlate low-level events. For
example, the model should include knowledge such as “after
an attacker has compromised a machine, he may perform
some network activity from the machine.” This is a generic
action common to many attack scenarios. This knowledge
497
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:13:03 UTC from IEEE Xplore.  Restrictions apply. 
I1f :
I1b :
I2f :
I2b :
I3f :
I3b :
I4f :
I4b1 :
I4b2 :
int(compromised(H1)) f,p−→ int(probeOtherMachine(H1, H2))
int(probeOtherMachine(H1, H2)) b,c−→ int(compromised(H1))
int(compromised(H1)) f,p−→ int(sendExploit(H1, H2))
int(sendExploit(H1, H2)) b,c−→ int(compromised(H1))
int(sendExploit(H1, H2)) f,l−→ int(compromised(H2))
int(compromised(H2)) b,p−→ int(sendExploit(H1, H2))
int(compromised(H1)), int(compromised(H2)) f,p−→
int(exchangeCtlMessage(H1, H2)) b,c−→ int(compromised(H1))
int(exchangeCtlMessage(H1, H2)) b,c−→ int(compromised(H2))
int(exchangeCtlMessage(H1, H2))
Figure 2.
Internal model
can reveal potential hidden correlations between low-level
observations, (e.g., high network trafﬁc and netﬂow ﬁltering
result). Absent any context to guide us, a trafﬁc spike could
be due to any of a number of things but in the context
of a likely compromise, the parameters of the trafﬁc burst
become important — if the trafﬁc emanated from the likely
compromised machine it can be assigned a different meaning
than if it did not.
Figure 2 shows the internal model we developed from
d,m−→ Cr to
studying the real-life incident. We use Cl
the inference rules for the internal conditions,
represent
namely condition Cl can infer condition Cr. There are
two modality operators, d and m, associated with a
the m mode
rule. Like in observation correspondence,
speciﬁes the conﬁdence in the inference and takes values
from {p, l, c}. The d mode indicates the direction of the
inference and could be either f (forward) or b (backward).
In forward inference, Cr is caused by Cl, thus the arrow
must be aligned with time, i.e. Cr shall happen after Cl.
This can specify knowledge for
reasoning what could
happen after a known condition becomes true, e.g. after
an attacker sends an exploit to a machine,
(I3f ).
he will likely compromise the machine
In the backward inference, we reason what could have
happened before to cause a known condition, and thus
the direction of inference is opposite to time. Example:
if a malicious probe is sent from a machine,
then an attacker must have certainly already
compromised the machine (I1b). As another example,
the forward inference rule I1f speciﬁes that “if an attacker
has compromised machine H1, he can perform a malicious
probe from H1 to another machine H2.” This inference has
a low certainty: the attacker may or may not choose to
probe another machine after compromising one. Thus the
rule is qualiﬁed by the p mode. I4f is the only rule in this
model that has two facts on the left-hand side. As in typical
logic-programming languages,
the comma represents the
AND logical relation.
Observation 
Correspondence
Internal Model
Answers with 
evidence
Reasoning 
Engine
User query, e.g.
which machines 
are “certainly”
compromised?
(convert to Datalog tuples)
pre-processing
…
Snort
netflow filter
log analyzer
… …
Figure 3. System architecture
III. REASONING METHODOLOGY
The reasoning model described in section I-A is analogous
to human thinking – observations are reﬂected as beliefs
with varying strengths and the beliefs are related to one an-
other with varying strengths. This section will introduce the
reasoning process to use such a model to “simulate” human
thinking such that an automated inference process can allow
us to combine observations to construct sophisticated attack
conclusions along with a semi-quantitative measure of our
conﬁdence. This inference process is capable of deriving
from a large number of possibilities high-conﬁdence beliefs
corroborated by a number of complementary evidence pieces
logically linked together.
Reasoning framework: Figure 3 presents the archi-
tecture of our reasoning system. The two modules of the
reasoning model — observation correspondence (described
in Section II-B) and internal model (described in Sec-
tion II-C) are input to the reasoning engine. Both modules
are speciﬁed in Datalog [16], a simple logic-programming
language. The raw observations are pre-processed and the
distilled results are converted to Datalog tuples as input
to the reasoning system. The reasoning engine is itself
implemented in Prolog. An important feature of our design is
that every component of the system is speciﬁed declaratively,
which has the useful property that once all speciﬁcations are
loaded into the Prolog system, a simple Prolog query will
automatically and efﬁciently search for true answers based
on the logic speciﬁcation. For example, a user can ask a
question “which machines are certainly compromised?” in
the form of a simple Prolog query. Our reasoning engine
will then give the answer along with the evidence in the
form of logical proofs.
A. Pre-processing
The pre-processing step is performed to compact
the
information entering the reasoning engine. We apply a data
abstraction technique by grouping a set of similar “internal
conditions” into a single “summarized internal condition”.
The summarization is done on both the time stamps and IP
addresses. For timestamps, if a set of internal conditions
498
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:13:03 UTC from IEEE Xplore.  Restrictions apply. 