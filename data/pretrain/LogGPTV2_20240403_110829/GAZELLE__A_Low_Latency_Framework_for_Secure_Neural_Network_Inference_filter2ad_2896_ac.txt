from B’s perspective. Moreover the security of the PAHE
scheme ensures that A has no way of ﬁguring out what cx is.
We next evaluate the non-linear activation using Yao’s
GC protocol. At the start of this step both parties posses
additive shares (cx, sx) of the secret value of x and want to
compute y = ReLU(x) without revealing it completely to
either party. We evaluate the non-linear activation function
ReLU (in parallel for each component of x) to get a secret
sharing of the output y = ReLU(x). This is done using
our circuit from Figure 5, described in more detail below.
The output of the garbled circuit evaluation is a pair of
shares sy (for the server) and cy (for the client) such that
sy + cy = y mod p. The security argument is exactly the
same as after the ﬁrst step, i.e. neither party has complete
information and both shares appear uniformly random to
their respective owners.
Once this is done, we are back where we started and we
can repeat these steps until we evaluate the full network.
We make the following two observations about our
proposed protocol:
1. By using AHE for the linear layers, we ensure that the
communication complexity of protocol is linear in the
number of layers and the size of inputs for each layer.
2. At the end of the garbled circuit protocol we have an
additive share that can be encrypted afresh. As such,
we can view the re-encryption as an interactive boot-
strapping procedure that clears the noise introduced
by any previous homomorphic operation.
For the second step of the outline above, we employ the
boolean circuit described in Figure 5. The circuit takes as
input three vectors: sx =r and sy =r(cid:48) (chosen at random)
from the server, and cx from the client. The ﬁrst block of
the circuit computes the arithmetic sum of sx and cx over
the integers and subtracts p from to obtain the result mod
p. (The decision of whether to subtract p or not is made by
the multiplexer). The second block of the circuit computes
a ReLU function. The third block adds the result to sy to
obtain the client’s share of y, namely cy. For more detailed
benchmarks on the ReLU and MaxPool garbled circuit
implementations, we refer the reader to Section 8. We
note that this conversion strategy is broadly similar to the
one developed in [25].
In our evaluations, we consider ReLU, Max-Pool and
the square activation functions, the ﬁrst two are by far
the most commonly used ones in convolutional neural
network design [28, 41, 39, 24]. Note that the square
activation function popularized for secure neural network
evaluation in [18] can be efﬁciently implemented by a
simple interactive protocol that uses the PAHE scheme
to generate the cross-terms.
The use of an IND-CPA-secure PAHE scheme for evalu-
Figure 5: Our combined circuit for steps (a), (b) and (c)
for the non-linear layers. The “+” gates refer to an integer
addition circuit, “-” refers to an integer subtraction circuit
and the “>” refers to the circuit refers to a greater than
comparison. Note that the borrow of the subtraction gates
is used as the select for the ﬁrst and last multiplexer
ating the linear layers guarantees the privacy of the client’s
inputs. However the PAHE scheme must also guarantee
the conﬁdentiality of the server’s input, in other words, it
should be circuit-private. Prior work addresses this prob-
lem in two ways. The ﬁrst approach called noise-ﬂooding
adds a large amount of noise to the ﬁnal ciphertext [15]
to obscure any information leaked through the ciphertext
noise. The second technique relies on bootstrapping, either
using garbled circuits [17] or using the full power of an
FHE scheme [11]. Noise-ﬂooding causes an undesirable
blow-up in the parameters of the underlying PAHE scheme,
while the FHE-bootstrapping based solution is well be-
yond the scope of the simple PAHE schemes we employ.
Thus, our solution builds a low-overhead circuit-private
interactive decryption protocol (Appendix B) to improve
the concrete efﬁciency of the garbled circuit approach (as
in [17]) as applied to the BFV scheme [4, 14].
5 Fast Homomorphic Matrix-Vector Multiplication
We next describe the homomorphic linear algebra kernels
that compute matrix-vector products (for FC layers) and
2D convolutions (for Conv layers). In this section, we
focus on matrix-vector product kernels which multiply
a plaintext matrix with an encrypted vector. We start
with the easiest to explain (but the slowest and most
communication-inefﬁcient) methods and move on to
describing optimizations that make matrix-vector mul-
tiplication much faster. In particular, our hybrid method
(see Table 4 and the description below) gives us the best
performance among all our homomorphic matrix-vector
multiplication methods. For example, multiplying a
128 × 1024 matrix with a length-1024 vector using our
hybrid scheme takes about 16ms˙(For detailed benchmarks,
we refer the reader to Section 7.3). In all the subsequent
examples, we will use an FC layer with ni inputs and
1658    27th USENIX Security Symposium
USENIX Association
Table 2: Comparing matrix-vector product algorithms by operation count, noise growth and number of output ciphertexts
#out ctb
Perm (Hoisted)a
SIMDScMult
Noise
Na¨ıve
Na¨ıve
(Output packed)
Na¨ıve
(Input packed)
Diagonal
Hybrid
0
0
0
ni−1
no·ni
n −1
Perm
no·logni
no·logni +no−1
no·ni
n
·logni
0
log n
no
no
2·no
no·ni
n
ni
no·ni
n
SIMDAdd
no·logni
no·logni +no
no·ni
n
·logni
ni
no·ni
n +log n
no
ηnaive :=η0·ηmult·ni
+ηrot·(ni−1)
ηnaive·ηmult·no
+ηrot·(no−1)
η0·ηmult·ni
+ηrot·(ni−1)
(η0 +ηrot)·ηmult·ni
(η0 +ηrot)·ηmult·ni
+ηrot·( ni
no
−1)
no
1
no·ni
n
1
1
a Rotations of the input with a common PermDecomp
c All logarithms are to base 2
b Number of output ciphertexts
no outputs as a running example. For simplicity of
presentation, unless stated otherwise we assume that n, ni
and no are powers of two. Similarly we assume that no and
ni are smaller than n. If not, we can split the original matrix
into n×n sized blocks that are processed independently.
The Na¨ıve Method.
In the na¨ıve method, each row of
the no × ni plaintext weight matrix W is encoded into
a separate plaintext vectors (see Figure 6). Each such
vector is of length n; where the ﬁrst ni entries contain
the corresponding row of the matrix and the other entries
are padded with 0. These plaintext vectors are denoted
w0,w1,...,w(no−1). We then use SIMDScMult to compute
the component-wise product of with the encrypted input
vector [v] to get [ui] = [wi ◦ v]. In order to compute the
inner-product what we need is actually the sum of the
entries in each of these vectors ui.
This can be achieved by a “rotate-and-sum” approach,
where we ﬁrst rotate the entries of [ui] by ni/2 positions.
The result is a ciphertext whose ﬁrst ni/2 entries contain
the sum of the ﬁrst and second halves of ui. One can then
repeat this process for log2ni iterations, rotating by half
the previous rotation on each iteration, to get a ciphertext
whose ﬁrst slot contains the ﬁrst component of Wv. By
repeating this procedure for each of the no rows we get
no ciphertexts, each containing one element of the result.
Based on this description, we can derive the following
performance characteristics for the na¨ıve method:
• The total cost is no SIMD scalar multiplications,
no · log2 n rotations (automorphisms) and no · log2 n
SIMD additions.
• The noise grows from η to η · ηmult · n + ηrot · (n− 1)
where ηmult is the multiplicative noise growth factor
for SIMD multiplication and ηrot is the additive noise
growth for a rotation. This is because the one SIMD
multiplication turns the noise from η (cid:55)→ η ·ηmult, and
the sequence of rotations and additions grows the noise
as follows:
η·ηmult(cid:55)→ (η·ηmult)·2+ηrot(cid:55)→ (η·ηmult)·4+ηrot·3(cid:55)→ ...
which gives us the above result.
• Finally, this process produces no many ciphertexts each
one containing just one component of the result.
This last fact turns out to be an unacceptable efﬁciency
barrier. In particular, the total network bandwidth becomes
quadratic in the input size and thus contradicts the entire
rationale of using PAHE for linear algebra. Ideally, we
want the entire result to come out in packed form in a
single ciphertext (assuming, of course, that no≤n).
A ﬁnal subtle point that needs to noted is that if n is
not a power of two, then we can continue to use the same
rotations as before, but all slots except the ﬁrst slot leak
information about partial sums. We therefore must add
a random number to these slots to destroy this extraneous
information about the partial sums.
5.1 Output Packing
The very ﬁrst thought to mitigate the ciphertext blowup
issue we just encountered is to take the many output
ciphertexts and somehow pack the results into one. Indeed,
this can be done by (a) doing a SIMD scalar multiplication
which zeroes out all but the ﬁrst coordinate of each of
the out ciphertexts; (b) rotating each of them by the
appropriate amount so that the numbers are lined up in
different slots; and (c) adding all of them together.
Unfortunately, this results in unacceptable noise growth.
The underlying reason is that we need to perform two
serial SIMD scalar multiplications (resulting in an η2
mult
factor; see Table 4). For most practical settings, this
noise growth forces us to use ciphertext moduli that are
larger 64 bits, thus overﬂowing the machine word. This
necessitates the use of a Double Chinese Remainder
Theorem (DCRT) representation similar to [16] which
substantially slows down computation. Instead we use an
algorithmic approach to control noise growth allowing the
use of smaller moduli and avoiding the need for DCRT.
USENIX Association
27th USENIX Security Symposium    1659
The key high-level idea is to arrange the matrix elements
in such a way that after the SIMD scalar multiplications,
“interacting elements” of the matrix-vector product never
appear in a single ciphertext. Here, “interacting elements”
are the numbers that need to be added together to obtain
the ﬁnal result. The rationale is that if this happens, we
never need to add two numbers that live in different slots
of the same ciphertexts, thus avoiding ciphertext rotation.
To do this, we encode the diagonal of the matrix into
a vector which is then SIMD scalar multiplied with the
input vector. The second diagonal (namely, the elements
W0,1,W1,2, ... ,Wno−1,0) is encoded into another vector
which is then SIMD scalar multiplied with a rotation (by
one) of the input vector, and so on. Finally, all these vectors
are added together to obtain the output vector in one shot.
The cost of the diagonal method is:
• The total cost is ni SIMD scalar multiplications, ni−1
rotations (automorphisms), and ni−1 SIMD additions.
• The noise grows from η to (η +ηrot)·ηmult×ni which,
for the parameters we use, is larger than that of the na¨ıve
method, but much better than the na¨ıve method with
output packing. Roughly speaking, the reason is that
in the diagonal method, since rotations are performed
before scalar multiplication, the noise growth has a
ηrot·ηmult factor whereas in the na¨ıve method, the order
is reversed resulting in a ηmult +ηrot factor.
• Finally, this process produces a single ciphertext that
has the entire output vector in packed form already.
In our setting (and we believe in most reasonable set-
tings), the additional noise growth is an acceptable compro-
mise given the large gain in the output length and the cor-
responding gain in the bandwidth and the overall run-time.
Furthermore, the fact that all rotations happen on the input
ciphertexts prove to be very important for an optimiza-
tion of [23] we describe in Appendix A, called “hoisting”,
which lets us amortize the cost of many input rotations.
A Hybrid Approach. One issue with the diagonal
approach is that the number of Perm is equal to ni. In the
context of FC layers no is often much lower than ni and
hence it is desirable to have a method where the Perm is
close to no. Our hybrid scheme achieves this by combining
the best aspects of the na¨ıve and diagonal schemes. We
ﬁrst extended the idea of diagonals for a square matrix to
squat rectangular weight matrices as shown in Figure 6
and then pack the weights along these extended diagonals
into plaintext vectors. These plaintext vectors are then
multiplied with no rotations of the input ciphertext similar
to the diagonal method. Once this is done we are left
with a single ciphertext that contains n/no chunks each
contains a partial sum of the no outputs. We can proceed
similar to the na¨ıve method to accumulate these using a
“rotate-and-sum” algorithm.
We implement an input packed variant of the hybrid
method and the performance and noise growth characteris-
Figure 6: The na¨ıve method is illustrated on the left and the
diagonal method of Halevi and Shoup [22] is illustrated
on the right. The entries in a single color live in the same
ciphertext. The key feature of the diagonal method is that
no two elements of the matrix that inﬂuence the same
output element appear with the same color.
Input Packing
5.2
Before moving on to more complex techniques we describe
an orthogonal approach to improve the na¨ıve method when
ni (cid:28) n. The idea is to pack multiple copies of the input
into a single ciphertext. This allows us better utilization
of the slots by computing multiple outputs in parallel.
In detail we can (a) pack n/ni many different rows into
a single plaintext vector; (b) pack n/ni copies of the input
vector into a single ciphertext; and (c) perform the rest
of the na¨ıve method as-is except that the rotations are not
applied to the whole ciphertext but block-by-block (thus
requiring log(ni) many rotations). Roughly speaking, this
achieves communication and computation as if the number
o = (no×ni)/n instead of no.
of rows of the matrix were n(cid:48)
When ni(cid:28)n, we have n(cid:48)
o(cid:28)no.
The Diagonal Method. The diagonal method as
described in the work of Halevi and Shoup [22] (and
implemented in [21]) provides another potential solution
to the problem of a large number of output ciphertexts.
1660    27th USENIX Security Symposium
USENIX Association
Figure 7: Four example extended digaonals after account-
ing for the rotation group structure
tics (following a straightforward derivation) are described
in Table 4. We note that hybrid method trades off hoistable
input rotations in the Diagonal method for output rotations
on distinct ciphertexts (which cannot be “hoisted out”).
However, the decrease in the number of input rotations
is multiplicative while the corresponding increase in the
number of output rotations is the logarithm of the same
multiplicative factor. As such, the hybrid method almost
always outperforms the Naive and Diagonal methods. We
present detailed benchmarks over a selection of matrix
sizes in Table 8.
We close this section with two important implemen-
tation details. First, recall that in order to enable faster
NTT, our parameter selection requires n to be a power of
two. As a result the permutation group we have access to
is the group of half rotations (Cn/2×C2), i.e. the possible
permutations are compositions of rotations by up to
n/2 for the two n/2-sized segments, and swapping the
two segments. The packing and diagonal selection in
the hybrid approach are modiﬁed to account for this by
adapting the deﬁnition of the extended diagonal to be those
entries of W that would be multiplied by the corresponding
entries of the ciphertext when the above Perm operations
are performed as shown in Figure 7. Finally, as described
in section 3 we control the noise growth in SIMDScMult
using plaintext windows for the weight matrix W.
6 Fast Homomorphic Convolutions
We now move on to the implementation of homomorphic
kernels for Conv layers. Analogous to the description of
FC layers we will start with simpler (and correspondingly
less efﬁcient) techniques before moving on to our ﬁnal opti-
mized implementation. In our setting, the server has access
to a plaintext ﬁlter and it is then provided encrypted input
images, which it must homomorphically convolve with its
ﬁlter to produce encrypted output images. As a running
Figure 8: Padded SISO Convolution
example for this section we will consider a ( fw, fh, ci, co)-
Conv layer with the same padding scheme, where the input
is speciﬁed by the tuple (wi, hi, ci). In order to better
emphasize the key ideas, we will split our presentation into
two parts: ﬁrst we will describe the single input single out-
put (SISO) case, i.e. (ci =1, co =1) followed by the more