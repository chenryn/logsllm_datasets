## 课堂练习伙伴系统是一种非常精妙的实现方式，无论你使用什么语言，请自己实现一个这样的分配系统，说不定哪天你在做某个系统的时候，就用到了。欢迎留言和我分享你的疑惑和见解，也欢迎你收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习、进步。![](Images/1a5564dd4e1c9f25d4772c7f844ca84a.png){savepage-src="https://static001.geekbang.org/resource/image/8c/37/8c0a95fa07a8b9a1abfd394479bdd637.jpg"}
# 24 \| 物理内存管理（下）：会议室管理员如何分配会议室？前一节，前面我们解析了整页的分配机制。如果遇到小的对象，物理内存是如何分配的呢？这一节，我们一起来看一看。
## 小内存的分配前面我们讲过，如果遇到小的对象，会使用 slub分配器进行分配。那我们就先来解析它的工作原理。还记得咱们创建进程的时候，会调用 dup_task_struct，它想要试图复制一个task_struct 对象，需要先调用 alloc_task_struct_node，分配一个task_struct 对象。从这段代码可以看出，它调用了 kmem_cache_alloc_node 函数，在 task_struct的缓存区域 task_struct_cachep 分配了一块内存。    static struct kmem_cache *task_struct_cachep; task_struct_cachep = kmem_cache_create("task_struct",arch_task_struct_size, align,SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT, NULL); static inline struct task_struct *alloc_task_struct_node(int node){return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node);} static inline void free_task_struct(struct task_struct *tsk){kmem_cache_free(task_struct_cachep, tsk);}在系统初始化的时候，task_struct_cachep 会被 kmem_cache_create函数创建。这个函数也比较容易看懂，专门用于分配 task_struct对象的缓存。这个缓存区的名字就叫task_struct。缓存区中每一块的大小正好等于 task_struct 的大小，也即arch_task_struct_size。有了这个缓存区，每次创建 task_struct的时候，我们不用到内存里面去分配，先在缓存里面看看有没有直接可用的，这就是**kmem_cache_alloc_node**的作用。``{=html}当一个进程结束，task_struct也不用直接被销毁，而是放回到缓存中，这就是**kmem_cache_free**的作用。这样，新进程创建的时候，我们就可以直接用现成的缓存中的task_struct 了。我们来仔细看看，缓存区 struct kmem_cache 到底是什么样子。    struct kmem_cache {struct kmem_cache_cpu __percpu *cpu_slab;/* Used for retriving partial slabs etc */unsigned long flags;unsigned long min_partial;int size;/* The size of an object including meta data */int object_size;/* The size of an object without meta data */int offset;/* Free pointer offset. */#ifdef CONFIG_SLUB_CPU_PARTIALint cpu_partial;/* Number of per cpu partial objects to keep around */#endifstruct kmem_cache_order_objects oo;/* Allocation and freeing of slabs */struct kmem_cache_order_objects max;struct kmem_cache_order_objects min;gfp_t allocflags;/* gfp flags to use on each alloc */int refcount;/* Refcount for slab cache destroy */void (*ctor)(void *);......const char *name;/* Name (only for display!) */struct list_head list;/* List of slab caches */......struct kmem_cache_node *node[MAX_NUMNODES];}; 在 struct kmem_cache 里面，有个变量 struct list_headlist，这个结构我们已经看到过多次了。我们可以想象一下，对于操作系统来讲，要创建和管理的缓存绝对不止task_struct。难道 mm_struct 就不需要吗？fs_struct就不需要吗？都需要。因此，所有的缓存最后都会放在一个链表里面，也就是LIST_HEAD(slab_caches)。对于缓存来讲，其实就是分配了连续几页的大内存块，然后根据缓存对象的大小，切成小内存块。所以，我们这里有三个 kmem_cache_order_objects 类型的变量。这里面的order，就是 2 的 order 次方个页面的大内存块，objects就是能够存放的缓存对象的数量。最终，我们将大内存块切分成小内存块，样子就像下面这样。![](Images/18e761b1fe91a27509e14c5fb0d4c6ac.png){savepage-src="https://static001.geekbang.org/resource/image/17/5e/172839800c8d51c49b67ec8c4d07315e.jpeg"}每一项的结构都是缓存对象后面跟一个下一个空闲对象的指针，这样非常方便将所有的空闲对象链成一个链。其实，这就相当于咱们数据结构里面学的，用数组实现一个可随机插入和删除的链表。所以，这里面就有三个变量：size 是包含这个指针的大小，object_size是纯对象的大小，offset就是把下一个空闲对象的指针存放在这一项里的偏移量。那这些缓存对象哪些被分配了、哪些在空着，什么情况下整个大内存块都被分配完了，需要向伙伴系统申请几个页形成新的大内存块？这些信息该由谁来维护呢？接下来就是最重要的两个成员变量出场的时候了。kmem_cache_cpu 和kmem_cache_node，它们都是每个 NUMA节点上有一个，我们只需要看一个节点里面的情况。![](Images/ec990143d70deabdd20e7091c88e8ac3.png){savepage-src="https://static001.geekbang.org/resource/image/45/0a/45f38a0c7bce8c98881bbe8b8b4c190a.jpeg"}在分配缓存块的时候，要分两种路径，**fast path**和**slowpath**，也就是**快速通道**和**普通通道**。其中 kmem_cache_cpu就是快速通道，kmem_cache_node 是普通通道。每次分配的时候，要先从kmem_cache_cpu 进行分配。如果 kmem_cache_cpu 里面没有空闲的块，那就到kmem_cache_node中进行分配；如果还是没有空闲的块，才去伙伴系统分配新的页。我们来看一下，kmem_cache_cpu 里面是如何存放缓存块的。    struct kmem_cache_cpu {void **freelist;/* Pointer to next available object */unsigned long tid;/* Globally unique transaction id */struct page *page;/* The slab from which we are allocating */#ifdef CONFIG_SLUB_CPU_PARTIALstruct page *partial;/* Partially allocated frozen slabs */#endif......};在这里，page 指向大内存块的第一个页，缓存块就是从里面分配的。freelist指向大内存块里面第一个空闲的项。按照上面说的，这一项会有指针指向下一个空闲的项，最终所有空闲的项会形成一个链表。partial 指向的也是大内存块的第一个页，之所以名字叫partial（部分），就是因为它里面部分被分配出去了，部分是空的。这是一个备用列表，当page 满了，就会从这里找。我们再来看 kmem_cache_node 的定义。    struct kmem_cache_node {spinlock_t list_lock;......#ifdef CONFIG_SLUBunsigned long nr_partial;struct list_head partial;......#endif};这里面也有一个partial，是一个链表。这个链表里存放的是部分空闲的大内存块。这是kmem_cache_cpu 里面的 partial 的备用列表，如果那里没有，就到这里来找。下面我们就来看看这个分配过程。kmem_cache_alloc_node 会调用slab_alloc_node。你还是先重点看这里面的注释，这里面说的就是快速通道和普通通道的概念。    /* * Inlined fastpath so that allocation functions (kmalloc, kmem_cache_alloc) * have the fastpath folded into their functions. So no function call * overhead for requests that can be satisfied on the fastpath. * * The fastpath works by first checking if the lockless freelist can be used. * If not then __slab_alloc is called for slow processing. * * Otherwise we can simply pick the next object from the lockless free list. */static __always_inline void *slab_alloc_node(struct kmem_cache *s,gfp_t gfpflags, int node, unsigned long addr){void *object;struct kmem_cache_cpu *c;struct page *page;unsigned long tid;......tid = this_cpu_read(s->cpu_slab->tid);c = raw_cpu_ptr(s->cpu_slab);......object = c->freelist;page = c->page;if (unlikely(!object || !node_match(page, node))) {object = __slab_alloc(s, gfpflags, node, addr, c);stat(s, ALLOC_SLOWPATH);} ......return object;}快速通道很简单，取出 cpu_slab 也即 kmem_cache_cpu 的freelist，这就是第一个空闲的项，可以直接返回了。如果没有空闲的了，则只好进入普通通道，调用\_\_slab_alloc。    static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,  unsigned long addr, struct kmem_cache_cpu *c){void *freelist;struct page *page;......redo:....../* must check again c->freelist in case of cpu migration or IRQ */freelist = c->freelist;if (freelist)goto load_freelist;  freelist = get_freelist(s, page);  if (!freelist) {c->page = NULL;stat(s, DEACTIVATE_BYPASS);goto new_slab;}  load_freelist:c->freelist = get_freepointer(s, freelist);c->tid = next_tid(c->tid);return freelist;  new_slab:  if (slub_percpu_partial(c)) {page = c->page = slub_percpu_partial(c);slub_set_percpu_partial(c, page);stat(s, CPU_PARTIAL_ALLOC);goto redo;}  freelist = new_slab_objects(s, gfpflags, node, &c);......return freeli 在这里，我们首先再次尝试一下 kmem_cache_cpu 的freelist。为什么呢？万一当前进程被中断，等回来的时候，别人已经释放了一些缓存，说不定又有空间了呢。如果找到了，就跳到load_freelist，在这里将 freelist 指向下一个空闲项，返回就可以了。如果 freelist 还是没有，则跳到 new_slab 里面去。这里面我们先去kmem_cache_cpu 的 partial 里面看。如果 partial 不是空的，那就将kmem_cache_cpu 的 page，也就是快速通道的那一大块内存，替换为 partial里面的大块内存。然后 redo，重新试下。这次应该就可以成功了。如果真的还不行，那就要到 new_slab_objects 了。    static inline void *new_slab_objects(struct kmem_cache *s, gfp_t flags,int node, struct kmem_cache_cpu **pc){void *freelist;struct kmem_cache_cpu *c = *pc;struct page *page;  freelist = get_partial(s, flags, node, c);  if (freelist)return freelist;  page = new_slab(s, flags, node);if (page) {c = raw_cpu_ptr(s->cpu_slab);if (c->page)flush_slab(s, c);  freelist = page->freelist;page->freelist = NULL;  stat(s, ALLOC_SLAB);c->page = page;*pc = c;} elsefreelist = NULL;  return freelis在这里面，get_partial 会根据 node id，找到相应的kmem_cache_node，然后调用 get_partial_node，开始在这个节点进行分配。    /* * Try to allocate a partial slab from a specific node. */static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,struct kmem_cache_cpu *c, gfp_t flags){struct page *page, *page2;void *object = NULL;int available = 0;int objects;......list_for_each_entry_safe(page, page2, &n->partial, lru) {void *t;  t = acquire_slab(s, n, page, object == NULL, &objects);if (!t)break;  available += objects;if (!object) {c->page = page;stat(s, ALLOC_FROM_PARTIAL);object = t;} else {put_cpu_partial(s, page, 0);stat(s, CPU_PARTIAL_NODE);}if (!kmem_cache_has_cpu_partial(s)|| available > slub_cpu_partial(s) / 2)break;}......return object;acquire_slab 会从 kmem_cache_node 的 partial链表中拿下一大块内存来，并且将freelist，也就是第一块空闲的缓存块，赋值给 t。并且当第一轮循环的时候，将kmem_cache_cpu 的 page 指向取下来的这一大块内存，返回的 object就是这块内存里面的第一个缓存块 t。如果 kmem_cache_cpu 也有一个partial，就会进行第二轮，再次取下一大块内存来，这次调用put_cpu_partial，放到 kmem_cache_cpu 的 partial 里面。如果 kmem_cache_node里面也没有空闲的内存，这就说明原来分配的页里面都放满了，就要回到new_slab_objects 函数，里面 new_slab 函数会调用 allocate_slab。    static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node){struct page *page;struct kmem_cache_order_objects oo = s->oo;gfp_t alloc_gfp;void *start, *p;int idx, order;bool shuffle;  flags &= gfp_allowed_mask;......page = alloc_slab_page(s, alloc_gfp, node, oo);if (unlikely(!page)) {oo = s->min;alloc_gfp = flags;/* * Allocation may have failed due to fragmentation. * Try a lower order alloc if possible */page = alloc_slab_page(s, alloc_gfp, node, oo);if (unlikely(!page))goto out;stat(s, ORDER_FALLBACK);}......return page;}在这里，我们看到了 alloc_slab_page 分配页面。分配的时候，要按kmem_cache_order_objects 里面的 order来。如果第一次分配不成功，说明内存已经很紧张了，那就换成 min 版本的kmem_cache_order_objects。好了，这个复杂的层层分配机制，我们就讲到这里，你理解到这里也就够用了。
## 页面换出另一个物理内存管理必须要处理的事情就是，页面换出。每个进程都有自己的虚拟地址空间，无论是32 位还是 64位，虚拟地址空间都非常大，物理内存不可能有这么多的空间放得下。所以，一般情况下，页面只有在被使用的时候，才会放在物理内存中。如果过了一段时间不被使用，即便用户进程并没有释放它，物理内存管理也有责任做一定的干预。例如，将这些物理内存中的页面换出到硬盘上去；将空出的物理内存，交给活跃的进程去使用。什么情况下会触发页面换出呢？可以想象，最常见的情况就是，分配内存的时候，发现没有地方了，就试图回收一下。例如，咱们解析申请一个页面的时候，会调用get_page_from_freelist，接下来的调用链为get_page_from_freelist-\>node_reclaim-\>\_\_node_reclaim-\>shrink_node，通过这个调用链可以看出，页面换出也是以内存节点为单位的。当然还有一种情况，就是作为内存管理系统应该主动去做的，而不能等真的出了事儿再做，这就是内核线程**kswapd**。这个内核线程，在系统初始化的时候就被创建。这样它会进入一个无限循环，直到系统停止。在这个循环中，如果内存使用没有那么紧张，那它就可以放心睡大觉；如果内存紧张了，就需要去检查一下内存，看看是否需要换出一些内存页。    /* * The background pageout daemon, started as a kernel thread * from the init process. * * This basically trickles out pages so that we have _some_ * free memory available even if there is no other activity * that frees anything up. This is needed for things like routing * etc, where we otherwise might have all activity going on in * asynchronous contexts that cannot page things out. * * If there are applications that are active memory-allocators * (most normal use), this basically shouldn't matter. */static int kswapd(void *p){unsigned int alloc_order, reclaim_order;unsigned int classzone_idx = MAX_NR_ZONES - 1;pg_data_t *pgdat = (pg_data_t*)p;struct task_struct *tsk = current;      for ( ; ; ) {......        kswapd_try_to_sleep(pgdat, alloc_order, reclaim_order,classzone_idx);......        reclaim_order = balance_pgdat(pgdat, alloc_order, classzone_idx);......    }} 这里的调用链是balance_pgdat-\>kswapd_shrink_node-\>shrink_node，是以内存节点为单位的，最后也是调用shrink_node。shrink_node 会调用shrink_node_memcg。这里面有一个循环处理页面的列表，看这个函数的注释，其实和上面我们想表达的内存换出是一样的。    /* * This is a basic per-node page freer.  Used by both kswapd and direct reclaim. */static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memcg,      struct scan_control *sc, unsigned long *lru_pages){......unsigned long nr[NR_LRU_LISTS];enum lru_list lru;......while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||nr[LRU_INACTIVE_FILE]) {unsigned long nr_anon, nr_file, percentage;unsigned long nr_scanned;  for_each_evictable_lru(lru) {if (nr[lru]) {nr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX);nr[lru] -= nr_to_scan;  nr_reclaimed += shrink_list(lru, nr_to_scan,    lruvec, memcg, sc);}}......}......这里面有个 lru 列表。从下面的定义，我们可以想象，所有的页面都被挂在 LRU列表中。LRU 是 Least RecentUse，也就是最近最少使用。也就是说，这个列表里面会按照活跃程度进行排序，这样就容易把不怎么用的内存页拿出来做处理。内存页总共分两类，一类是**匿名页**，和虚拟地址空间进行关联；一类是**内存映射**，不但和虚拟地址空间关联，还和文件管理关联。它们每一类都有两个列表，一个是 active，一个是 inactive。顾名思义，active就是比较活跃的，inactive就是不怎么活跃的。这两个里面的页会变化，过一段时间，活跃的可能变为不活跃，不活跃的可能变为活跃。如果要换出内存，那就是从不活跃的列表中找出最不活跃的，换出到硬盘上。    enum lru_list {LRU_INACTIVE_ANON = LRU_BASE,LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,LRU_UNEVICTABLE,NR_LRU_LISTS};  #define for_each_evictable_lru(lru) for (lru = 0; lru i_mmap interval tree. */struct {struct rb_node rb;unsigned long rb_subtree_last;} shared;    /* * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma * list, after a COW of one of the file pages.A MAP_SHARED vma * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack * or brk vma (with NULL file) can only be in an anon_vma list. */struct list_head anon_vma_chain; /* Serialized by mmap_sem &  * page_table_lock */struct anon_vma *anon_vma;/* Serialized by page_table_lock */    /* Function pointers to deal with this struct. */const struct vm_operations_struct *vm_ops;/* Information about our backing store: */unsigned long vm_pgoff;/* Offset (within vm_file) in PAGE_SIZE   units */struct file * vm_file;/* File we map to (can be NULL). */void * vm_private_data;/* was vm_pte (shared mem) */其实内存映射不仅仅是物理内存和虚拟内存之间的映射，还包括将文件中的内容映射到虚拟内存空间。这个时候，访问内存空间就能够访问到文件里面的数据。而仅有物理内存和虚拟内存的映射，是一种特殊情况。![](Images/a88f0b31a3a1813db79125719c621aa0.png){savepage-src="https://static001.geekbang.org/resource/image/f0/45/f0dcb83fcaa4f185a8e36c9d28f12345.jpg"}前面咱们讲堆的时候讲过，如果我们要申请小块内存，就用 brk。brk函数之前已经解析过了，这里就不多说了。如果申请一大块内存，就要用mmap。对于堆的申请来讲，mmap 是映射内存空间到物理内存。另外，如果一个进程想映射一个文件到自己的虚拟内存空间，也要通过 mmap系统调用。这个时候 mmap 是映射内存空间到物理内存再到文件。可见 mmap这个系统调用是核心，我们现在来看 mmap 这个系统调用。    SYSCALL_DEFINE6(mmap, unsigned long, addr, unsigned long, len,                unsigned long, prot, unsigned long, flags,                unsigned long, fd, unsigned long, off){......        error = sys_mmap_pgoff(addr, len, prot, flags, fd, off >> PAGE_SHIFT);......}  SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,unsigned long, prot, unsigned long, flags,unsigned long, fd, unsigned long, pgoff){struct file *file = NULL;......file = fget(fd);......retval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);return retval;}如果要映射到文件，fd 会传进来一个文件描述符，并且 mmap_pgoff 里面通过fget 函数，根据文件描述符获得 struct file。struct file表示打开的一个文件。``{=html}接下来的调用链是vm_mmap_pgoff-\>do_mmap_pgoff-\>do_mmap。这里面主要干了两件事情：-   调用 get_unmapped_area 找到一个没有映射的区域；-   调用 mmap_region 映射这个区域。我们先来看 get_unmapped_area 函数。    unsigned longget_unmapped_area(struct file *file, unsigned long addr, unsigned long len,unsigned long pgoff, unsigned long flags){unsigned long (*get_area)(struct file *, unsigned long,  unsigned long, unsigned long, unsigned long);......get_area = current->mm->get_unmapped_area;if (file) {if (file->f_op->get_unmapped_area)get_area = file->f_op->get_unmapped_area;} ......}这里面如果是匿名映射，则调用 mm_struct 里面的 get_unmapped_area函数。这个函数其实是 arch_get_unmapped_area。它会调用find_vma_prev，在表示虚拟内存区域的 vm_area_struct红黑树上找到相应的位置。之所以叫prev，是说这个时候虚拟内存区域还没有建立，找到前一个 vm_area_struct。如果不是匿名映射，而是映射到一个文件，这样在 Linux里面，每个打开的文件都有一个 struct file 结构，里面有一个file_operations，用来表示和这个文件相关的操作。如果是我们熟知的 ext4文件系统，调用的是thp_get_unmapped_area。如果我们仔细看这个函数，最终还是调用 mm_struct里面的 get_unmapped_area 函数。殊途同归。    const struct file_operations ext4_file_operations = {......        .mmap           = ext4_file_mmap        .get_unmapped_area = thp_get_unmapped_area,};  unsigned long __thp_get_unmapped_area(struct file *filp, unsigned long len,                loff_t off, unsigned long flags, unsigned long size){        unsigned long addr;        loff_t off_end = off + len;        loff_t off_align = round_up(off, size);        unsigned long len_pad;        len_pad = len + size;......        addr = current->mm->get_unmapped_area(filp, 0, len_pad,                                              off >> PAGE_SHIFT, flags);        addr += (off - addr) & (size - 1);        return addr;}我们再来看 mmap_region，看它如何映射这个虚拟内存区域。    unsigned long mmap_region(struct file *file, unsigned long addr,unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,struct list_head *uf){struct mm_struct *mm = current->mm;struct vm_area_struct *vma, *prev;struct rb_node **rb_link, *rb_parent;  /* * Can we just expand an old mapping? */vma = vma_merge(mm, prev, addr, addr + len, vm_flags,NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX);if (vma)goto out;  /* * Determine the object being mapped and call the appropriate * specific mapper. the address has already been validated, but * not unmapped, but the maps are removed from the list. */vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);if (!vma) {error = -ENOMEM;goto unacct_error;}  vma->vm_mm = mm;vma->vm_start = addr;vma->vm_end = addr + len;vma->vm_flags = vm_flags;vma->vm_page_prot = vm_get_page_prot(vm_flags);vma->vm_pgoff = pgoff;INIT_LIST_HEAD(&vma->anon_vma_chain);  if (file) {vma->vm_file = get_file(file);error = call_mmap(file, vma);addr = vma->vm_start;vm_flags = vma->vm_flags;} ......vma_link(mm, vma, prev, rb_link, rb_parent);return addr;.....还记得咱们刚找到了虚拟内存区域的前一个vm_area_struct，我们首先要看，是否能够基于它进行扩展，也即调用vma_merge，和前一个 vm_area_struct 合并到一起。如果不能，就需要调用 kmem_cache_zalloc，在 Slub 里面创建一个新的vm_area_struct对象，设置起始和结束位置，将它加入队列。如果是映射到文件，则设置 vm_file为目标文件，调用 call_mmap。其实就是调用 file_operations 的 mmap函数。对于 ext4 文件系统，调用的是ext4_file_mmap。从这个函数的参数可以看出，这一刻文件和内存开始发生关系了。这里我们将vm_area_struct的内存操作设置为文件系统操作，也就是说，读写内存其实就是读写文件系统。    static inline int call_mmap(struct file *file, struct vm_area_struct *vma){return file->f_op->mmap(file, vma);}  static int ext4_file_mmap(struct file *file, struct vm_area_struct *vma){......      vma->vm_ops = &ext4_file_vm_ops;......}我们再回到 mmap_region 函数。最终，vma_link 函数将新创建的vm_area_struct 挂在了 mm_struct 里面的红黑树上。这个时候，从内存到文件的映射关系，至少要在逻辑层面建立起来。那从文件到内存的映射关系呢？vma_link还做了另外一件事情，就是\_\_vma_link_file。这个东西要用于建立这层映射关系。对于打开的文件，会有一个结构 struct file 来表示。它有个成员指向 structaddress_space 结构，这里面有棵变量名为 i_mmap 的红黑树，vm_area_struct就挂在这棵树上。    struct address_space {struct inode*host;/* owner: inode, block_device */......struct rb_rooti_mmap;/* tree of private and shared mappings */......const struct address_space_operations *a_ops;/* methods */......}  static void __vma_link_file(struct vm_area_struct *vma){struct file *file;  file = vma->vm_file;if (file) {struct address_space *mapping = file->f_mapping;vma_interval_tree_insert(vma, &mapping->i_mmap);}到这里，内存映射的内容要告一段落了。你可能会困惑，好像还没和物理内存发生任何关系，还是在虚拟内存里面折腾呀？对的，因为到目前为止，我们还没有开始真正访问内存呀！这个时候，内存管理并不直接分配物理内存，因为物理内存相对于虚拟地址空间太宝贵了，只有等你真正用的那一刻才会开始分配。
## 用户态缺页异常一旦开始访问虚拟内存的某个地址，如果我们发现，并没有对应的物理页，那就触发缺页中断，调用do_page_fault。    dotraplinkage void notracedo_page_fault(struct pt_regs *regs, unsigned long error_code){unsigned long address = read_cr2(); /* Get the faulting address */......__do_page_fault(regs, error_code, address);......}  /* * This routine handles page faults.  It determines the address, * and the problem, and then passes it off to one of the appropriate * routines. */static noinline void__do_page_fault(struct pt_regs *regs, unsigned long error_code,unsigned long address){struct vm_area_struct *vma;struct task_struct *tsk;struct mm_struct *mm;tsk = current;mm = tsk->mm;  if (unlikely(fault_in_kernel_space(address))) {if (vmalloc_fault(address) >= 0)return;}......vma = find_vma(mm, address);......fault = handle_mm_fault(vma, address, flags);......在 \_\_do_page_fault里面，先要判断缺页中断是否发生在内核。如果发生在内核则调用vmalloc_fault，这就和咱们前面学过的虚拟内存的布局对应上了。在内核里面，vmalloc区域需要内核页表映射到物理页。咱们这里把内核的这部分放放，接着看用户空间的部分。接下来在用户空间里面，找到你访问的那个地址所在的区域vm_area_struct，然后调用 handle_mm_fault 来映射这个区域。    static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,unsigned int flags){struct vm_fault vmf = {.vma = vma,.address = address & PAGE_MASK,.flags = flags,.pgoff = linear_page_index(vma, address),.gfp_mask = __get_fault_gfp_mask(vma),};struct mm_struct *mm = vma->vm_mm;pgd_t *pgd;p4d_t *p4d;int ret;  pgd = pgd_offset(mm, address);p4d = p4d_alloc(mm, pgd, address);......vmf.pud = pud_alloc(mm, p4d, address);......vmf.pmd = pmd_alloc(mm, vmf.pud, address);......return handle_pte_fault(&vmf);}到这里，终于看到了我们熟悉的PGD、P4G、PUD、PMD、PTE，这就是前面讲页表的时候，讲述的四级页表的概念，因为暂且不考虑五级页表，我们暂时忽略P4G。![](Images/1302568baeaf082c4fa189a54f5f8690.png){savepage-src="https://static001.geekbang.org/resource/image/9b/f1/9b802943af4e3ae80ce4d0d7f2190af1.jpg"}pgd_t 用于全局页目录项，pud_t 用于上层页目录项，pmd_t用于中间页目录项，pte_t 用于直接页表项。每个进程都有独立的地址空间，为了这个进程独立完成映射，每个进程都有独立的进程页表，这个页表的最顶级的pgd 存放在 task_struct 中的 mm_struct 的 pgd 变量里面。在一个进程新创建的时候，会调用 fork，对于内存的部分会调用copy_mm，里面调用 dup_mm。    /* * Allocate a new mm structure and copy contents from the * mm structure of the passed in task structure. */static struct mm_struct *dup_mm(struct task_struct *tsk){struct mm_struct *mm, *oldmm = current->mm;mm = allocate_mm();memcpy(mm, oldmm, sizeof(*mm));if (!mm_init(mm, tsk, mm->user_ns))goto fail_nomem;err = dup_mmap(mm, oldmm);return mm;}在这里，除了创建一个新的 mm_struct，并且通过 memcpy将它和父进程的弄成一模一样之外，我们还需要调用 mm_init进行初始化。接下来，mm_init 调用 mm_alloc_pgd，分配全局页目录项，赋值给mm_struct 的 pdg 成员变量。    static inline int mm_alloc_pgd(struct mm_struct *mm){mm->pgd = pgd_alloc(mm);return 0;}pgd_alloc 里面除了分配 PDG 之外，还做了很重要的一个事情，就是调用pgd_ctor。    static void pgd_ctor(struct mm_struct *mm, pgd_t *pgd){/* If the pgd points to a shared pagetable level (either the   ptes in non-PAE, or shared PMD in PAE), then just copy the   references from swapper_pg_dir. */if (CONFIG_PGTABLE_LEVELS == 2 ||    (CONFIG_PGTABLE_LEVELS == 3 && SHARED_KERNEL_PMD) ||    CONFIG_PGTABLE_LEVELS >= 4) {clone_pgd_range(pgd + KERNEL_PGD_BOUNDARY,swapper_pg_dir + KERNEL_PGD_BOUNDARY,KERNEL_PGD_PTRS);}......}pgd_ctor 干了什么事情呢？我们注意看里面的注释，它拷贝了对于swapper_pg_dir 的引用。swapper_pg_dir 是内核页表的最顶级的全局页目录。一个进程的虚拟地址空间包含用户态和内核态两部分。为了从虚拟地址空间映射到物理页面，页表也分为用户地址空间的页表和内核页表，这就和上面遇到的vmalloc有关系了。在内核里面，映射靠内核页表，这里内核页表会拷贝一份到进程的页表。至于swapper_pg_dir是什么，怎么初始化的，怎么工作的，我们还是先放一放，放到下一节统一讨论。至此，一个进程 fork 完毕之后，有了内核页表，有了自己顶级的pgd，但是对于用户地址空间来讲，还完全没有映射过。这需要等到这个进程在某个CPU 上运行，并且对内存访问的那一刻了。当这个进程被调度到某个 CPU上运行的时候，咱们在[调度](https://time.geekbang.org/column/article/93251)那一节讲过，要调用context_switch 进行上下文切换。对于内存方面的切换会调用switch_mm_irqs_off，这里面会调用 load_new_mm_cr3。cr3 是 CPU 的一个寄存器，它会指向当前进程的顶级 pgd。如果 CPU的指令要访问进程的虚拟内存，它就会自动从 cr3 里面得到 pgd在物理内存的地址，然后根据里面的页表解析虚拟内存的地址为物理内存，从而访问真正的物理内存上的数据。这里需要注意两点。第一点，cr3 里面存放当前进程的顶级pgd，这个是硬件的要求。cr3 里面需要存放 pgd在物理内存的地址，不能是虚拟地址。因而 load_new_mm_cr3 里面会使用\_\_pa，将 mm_struct 里面的成员变量 pdg（mm_struct里面存的都是虚拟地址）变为物理地址，才能加载到 cr3 里面去。第二点，用户进程在运行的过程中，访问虚拟内存中的数据，会被 cr3里面指向的页表转换为物理地址后，才在物理内存中访问数据，这个过程都是在用户态运行的，地址转换的过程无需进入内核态。只有访问虚拟内存的时候，发现没有映射多物理内存，页表也没有创建过，才触发缺页异常。进入内核调用do_page_fault，一直调用到\_\_handle_mm_fault，这才有了上面解析到这个函数的时候，我们看到的代码。既然原来没有创建过页表，那只好补上这一课。于是，\_\_handle_mm_fault调用 pud_alloc 和 pmd_alloc，来创建相应的页目录项，最后调用handle_pte_fault 来创建页表项。绕了一大圈，终于将页表整个机制的各个部分串了起来。但是咱们的故事还没讲完，物理的内存还没找到。我们还得接着分析handle_pte_fault 的实现。    static int handle_pte_fault(struct vm_fault *vmf){pte_t entry;......vmf->pte = pte_offset_map(vmf->pmd, vmf->address);vmf->orig_pte = *vmf->pte;......if (!vmf->pte) {if (vma_is_anonymous(vmf->vma))return do_anonymous_page(vmf);elsereturn do_fault(vmf);}  if (!pte_present(vmf->orig_pte))return do_swap_page(vmf);......}这里面总的来说分了三种情况。如果PTE，也就是页表项，从来没有出现过，那就是新映射的页。如果是匿名页，就是第一种情况，应该映射到一个物理内存页，在这里调用的是do_anonymous_page。如果是映射到文件，调用的就是do_fault，这是第二种情况。如果 PTE原来出现过，说明原来页面在物理内存中，后来换出到硬盘了，现在应该换回来，调用的是do_swap_page。我们来看第一种情况，do_anonymous_page。对于匿名页的映射，我们需要先通过pte_alloc 分配一个页表项，然后通过 alloc_zeroed_user_highpage_movable分配一个页。之后它会调用 alloc_pages_vma，并最终调用\_\_alloc_pages_nodemask。这个函数你还记得吗？就是咱们伙伴系统的核心函数，专门用来分配物理页面的。do_anonymous_page接下来要调用 mk_pte，将页表项指向新分配的物理页，set_pte_at会将页表项塞到页表里面。    static int do_anonymous_page(struct vm_fault *vmf){struct vm_area_struct *vma = vmf->vma;struct mem_cgroup *memcg;struct page *page;int ret = 0;pte_t entry;......if (pte_alloc(vma->vm_mm, vmf->pmd, vmf->address))return VM_FAULT_OOM;......page = alloc_zeroed_user_highpage_movable(vma, vmf->address);......entry = mk_pte(page, vma->vm_page_prot);if (vma->vm_flags & VM_WRITE)entry = pte_mkwrite(pte_mkdirty(entry));  vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,&vmf->ptl);......set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);......}第二种情况映射到文件 do_fault，最终我们会调用 \_\_do_fault。    static int __do_fault(struct vm_fault *vmf){struct vm_area_struct *vma = vmf->vma;int ret;......ret = vma->vm_ops->fault(vmf);......return ret;} 这里调用了 struct vm_operations_struct vm_ops 的 fault函数。还记得咱们上面用 mmap 映射文件的时候，对于 ext4 文件系统，vm_ops指向了 ext4_file_vm_ops，也就是调用了 ext4_filemap_fault。    static const struct vm_operations_struct ext4_file_vm_ops = {.fault= ext4_filemap_fault,.map_pages= filemap_map_pages,.page_mkwrite   = ext4_page_mkwrite,};  int ext4_filemap_fault(struct vm_fault *vmf){struct inode *inode = file_inode(vmf->vma->vm_file);......err = filemap_fault(vmf);......return err;}ext4_filemap_fault 里面的逻辑我们很容易就能读懂。vm_file 就是咱们当时mmap 的时候映射的那个文件，然后我们需要调用filemap_fault。对于文件映射来说，一般这个文件会在物理内存里面有页面作为它的缓存，find_get_page就是找那个页。如果找到了，就调用do_async_mmap_readahead，预读一些数据到内存里面；如果没有，就跳到no_cached_page。    int filemap_fault(struct vm_fault *vmf){int error;struct file *file = vmf->vma->vm_file;struct address_space *mapping = file->f_mapping;struct inode *inode = mapping->host;pgoff_t offset = vmf->pgoff;struct page *page;int ret = 0;......page = find_get_page(mapping, offset);if (likely(page) && !(vmf->flags & FAULT_FLAG_TRIED)) {do_async_mmap_readahead(vmf->vma, ra, file, page, offset);} else if (!page) {goto no_cached_page;}......vmf->page = page;return ret | VM_FAULT_LOCKED;no_cached_page:error = page_cache_read(file, offset, vmf->gfp_mask);......}如果没有物理内存中的缓存页，那我们就调用page_cache_read。在这里显示分配一个缓存页，将这一页加到 lru表里面，然后在 address_space 中调用 address_space_operations 的 readpage函数，将文件内容读到内存中。address_space 的作用咱们上面也介绍过了。    static int page_cache_read(struct file *file, pgoff_t offset, gfp_t gfp_mask){struct address_space *mapping = file->f_mapping;struct page *page;......page = __page_cache_alloc(gfp_mask|__GFP_COLD);......ret = add_to_page_cache_lru(page, mapping, offset, gfp_mask & GFP_KERNEL);......ret = mapping->a_ops->readpage(file, page);......}struct address_space_operations 对于 ext4文件系统的定义如下所示。这么说来，上面的 readpage 调用的其实是ext4_readpage。因为我们还没讲到文件系统，这里我们不详细介绍ext4_readpage 具体干了什么。你只要知道，最后会调用ext4_read_inline_page，这里面有部分逻辑和内存映射有关就行了。    static const struct address_space_operations ext4_aops = {.readpage= ext4_readpage,.readpages= ext4_readpages,......};  static int ext4_read_inline_page(struct inode *inode, struct page *page){void *kaddr;......kaddr = kmap_atomic(page);ret = ext4_read_inline_data(inode, kaddr, len, &iloc);flush_dcache_page(page);kunmap_atomic(kaddr);......}在 ext4_read_inline_page 函数里，我们需要先调用kmap_atomic，将物理内存映射到内核的虚拟地址空间，得到内核中的地址kaddr。 我们在前面提到过kmap_atomic，它是用来做临时内核映射的。本来把物理内存映射到用户虚拟地址空间，不需要在内核里面映射一把。但是，现在因为要从文件里面读取数据并写入这个物理页面，又不能使用物理地址，我们只能使用虚拟地址，这就需要在内核里面临时映射一把。临时映射后，ext4_read_inline_data读取文件到这个虚拟地址。读取完毕后，我们取消这个临时映射 kunmap_atomic就行了。至于 kmap_atomic 的具体实现，我们还是放到内核映射部分再讲。我们再来看第三种情况，do_swap_page。之前我们讲过物理内存管理，你这里可以回忆一下。如果长时间不用，就要换出到硬盘，也就是swap，现在这部分数据又要访问了，我们还得想办法再次读到内存中来。    int do_swap_page(struct vm_fault *vmf){struct vm_area_struct *vma = vmf->vma;struct page *page, *swapcache;struct mem_cgroup *memcg;swp_entry_t entry;pte_t pte;......entry = pte_to_swp_entry(vmf->orig_pte);......page = lookup_swap_cache(entry);if (!page) {page = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE, vma,vmf->address);......} ......swapcache = page;......pte = mk_pte(page, vma->vm_page_prot);......set_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);vmf->orig_pte = pte;......swap_free(entry);......}do_swap_page 函数会先查找 swap 文件有没有缓存页。如果没有，就调用swapin_readahead，将 swap 文件读到内存中来，形成内存页，并通过 mk_pte生成页表项。set_pte_at 将页表项插入页表，swap_free 将 swap文件清理。因为重新加载回内存了，不再需要 swap 文件了。swapin_readahead 会最终调用 swap_readpage，在这里，我们看到了熟悉的readpage 函数，也就是说读取普通文件和读取 swap文件，过程是一样的，同样需要用 kmap_atomic 做临时映射。    int swap_readpage(struct page *page, bool do_poll){struct bio *bio;int ret = 0;struct swap_info_struct *sis = page_swap_info(page);blk_qc_t qc;struct block_device *bdev;......if (sis->flags & SWP_FILE) {struct file *swap_file = sis->swap_file;struct address_space *mapping = swap_file->f_mapping;ret = mapping->a_ops->readpage(swap_file, page);return ret;}......}通过上面复杂的过程，用户态缺页异常处理完毕了。物理内存中有了页面，页表也建立好了映射。接下来，用户程序在虚拟内存空间里面，可以通过虚拟地址顺利经过页表映射的访问物理页面上的数据了。为了加快映射速度，我们不需要每次从虚拟地址到物理地址的转换都走一遍页表。![](Images/cbab98e4305316d47d7ddd6efed1b766.png){savepage-src="https://static001.geekbang.org/resource/image/94/b3/94efd92cbeb4d4ff155a645b93d71eb3.jpg"}页表一般都很大，只能存放在内存中。操作系统每次访问内存都要折腾两步，先通过查询页表得到物理地址，然后访问该物理地址读取指令、数据。为了提高映射速度，我们引入了**TLB**（Translation LookasideBuffer），我们经常称为**快表**，专门用来做地址映射的硬件设备。它不在内存中，可存储的数据比较少，但是比内存要快。所以，我们可以想象，TLB就是页表的Cache，其中存储了当前最可能被访问到的页表项，其内容是部分页表项的一个副本。有了 TLB之后，地址映射的过程就像图中画的。我们先查块表，块表中有映射关系，然后直接转换为物理地址。如果在TLB 查不到映射关系时，才会到内存中查询页表。