context of an applications which utilizes matrix vector mul-
tiplication as a subroutine. The class of applications we have
focused on in this study are sparse linear solvers since they are
very common in computational science and make extensive use
of matrix vector products. Sparse linear systems are commonly
found in many different types of applications including HPC,
Graph-based, and data-mining algorithms.
We consider one of the most common solvers, the Conju-
gate Gradient (CG) method.
CG is a popular solver well suited for very large and
sparse symmetric positive deﬁnite problems. It expresses x
as a linear function of n vectors p1, p2, ...pn, with each pair
of vectors conjugate in A (piApj = 0). Although the pi’s can
be computed directly, in practice a small subset of the pi’s
is needed to achieve accuracy within machine precision. As
such, CG approximates the solution x = q1p1 + ... + qnpn
with only a few vectors. The initial approximation is x0; the
residual r0 = b − Ax0, which is the direction of the error
in x0, serves as the ﬁrst conjugate vector, p0. Subsequent
iterations compute the residual rk and use it to compute the
next conjugate vector pk. To ensure that pk is conjugate to
prior pi’s, pk = rk − rT
pk−1. The coefﬁcients αk are
. This process is repeated until rk falls
computed as
below some threshold.
k−1−rk−1
k−2rk−2
rT
rT
k rk
pT
k Apk
B. Benchmarks
Solvers are run till a common ﬁxed accuracy target of (1e-
6) which is close to machine precision.
The algorithmic fault detection techniques were imple-
mented within the SparseLib library [10] of core sparse linear
algebra operations, including matrix vector multiplication. To
understand the effectiveness of our technique in a wide range
C. Parallel CG
The baseline CG implementation [10] was parallelized
with MPI. The algorithm was parallelized by dividing the
7
Name
nd3k
bcsstk38
Kuu
bcsstk16
s2rmq4m1
s3rmq4m1
s1rmq4m1
bcsstk28
s2rmt3m1
s3rmt3m1
s1rmt3m1
s3rmt3m3
nasa2910
Muu
bcsstk24
aft01
bcsstk15
crystm01
nasa4704
ex9
ex15
msc04515
bcsstk13
ex13
sts4098
nasa2146
bcsstk14
ex10hs
bcsstk27
ex3
nnz
3279690
355460
340200
290378
263351
262943
262411
219024
217681
217669
217651
207123
174296
170134
159910
125567
117816
105339
104756
99471
98671
97707
83883
75628
72356
72250
63454
57308
56126
54840
N
9000
8032
7102
4884
5489
5489
5489
4410
5489
5489
5489
5357
2910
7102
3562
8205
3948
4875
4704
3363
6867
4515
2003
2568
4098
2146
1806
2548
1224
2410
3D Mesh Problem
Sparsity Type
364.41
44.2555 Airplane Engine Component
47.902 Mathworks Test Matrix
Corp. of Engineers Dam
59.455
47.978
FEM cylindrical shell
FEM cylindrical shell
47.9036
FEM cylindrical shell
47.8067
Solid Element Model
49.6653
FEM cylindrical shell
39.657
39.6555
FEM cylindrical shell
FEM cylindrical shell
39.6522
FEM cylindrical shell
38.664
59.8955
Structure from NASA
23.9558 Mathworks Test Matrix
44.8933 Winter Sports Arena
15.3037 Acoustic Radiation
29.8419 Offshore platform
21.608
22.2696
29.5781
14.3689
21.6405
41.8787
29.4502
17.6564
33.6673
35.1351 Roof of OMNI Coliseum
22.4914
Test Matrix from FIDAP
45.8546 matrix Buckling Problem
Sym. Powers of Graphs
22.7552
FEM Vibration
Structure from Nasa
Test Matrix from FIDAP
Test Matrix from FIDAP
Symmetric Test Mat
Fluid Flow
Test Matrix from FIDAP
Structural Engineering Mat
Structure from NASA
TABLE III: List of Matrices and Properties
rows of the linear problem across a given set of N nodes,
as shown in Figure 3. All of the internal state within CG
(e.g. search direction p and residual r) were also divided in
a similar manner so that every node contained a fraction of
the entire linear system. By decomposing the algorithm in
this manner, much of the actual CG code did not change, and
instead only the implementations of the matrix vector product
and dot product needed to be modiﬁed to account for the
decomposition. Although a given node will only need it’s local
segment of the matrix A to compute the corresponding output
segment, it will also need the entire input state (x). Similarly,
the dot product operation requires that the nodes add the sum
of all smaller local dot products to compute actual dot product.
Therefore both the beginning of the matrix vector product and
end of the dot products represent synchronization points for
the parallel implementation of CG. For the parallel matrix
vector product, this synchronization was implemented in MPI
by gathering the x segments from all of the processes. For the
dot products a sum reduction of all the locally computed dot
products was used to gather the global dot product result.
Fig. 3: Example Decomposition of Linear System (y = Ax)
for Parallel CG implementation (N=4)
Intel Xeon 5660, 2.8GHz, 24GB memory per node, Hera:
AMD Quad-Core Opteron 2.3GHz, 32GB Memory per node,
Atlas: AMD Opteron, 2.4GHz, 16GB Memory per node).
D. Other Implementation Details
All our experiments involved full application runs of the
CG solver from the Iterative Method Library [10] that utilizes
SparseLib for linear operations. Our techniques were integrated
into the matrix vector routine in SparseLib. Over 50k runs were
used to get the statistical results shown in Section V. These
runs were executed on LLNL machines (system specs: Sierra:
Due to the nature of partial recomputation and error
localization based approach, there exist several potential pa-
rameters for trading off accuracy and performance within the
technique. This is advantageous for providing an interface
for more application-speciﬁc algorithmic corrections. Some of
these parameters include:
8
Ax=xbProcessor 0Processor 1Processor 2Processor 3•
•
•
Threshold (τ) which is used determine if branches of
tree (segments of output) are pursued for further error
localization can be adjusted according to application
characteristics and requirements.
Traversal of the tree can be stopped at any point (i.e.
depth=d, which is the fraction of the entire tree’s
height), and correction can be applied to the segment.
Roll forward correction as opposed to the typical
rollback correction, using the projection of the error
onto the code space, instead of roll back computation
can be used to correct the errors in the identiﬁed
segment.
Similar to the approach in [27], we can use decision trees
to learn the best thresholds for the techniques. The ﬁrst two
are considered in the evaluations of partial recomputation in
Section V, while the third is the focus of future work.
E. Generality
The proposed approach for algorithmic correction applies
to any application which uses linear operations that are associa-
tive and has multiple outputs and/or intermediate states. This
means that many scientiﬁc computing applications can beneﬁt
from these approaches since they rely heavily on mathematical
operations at
their core. Our proposed techniques rely on
knowledge of the linear operations, not CG. In order to see the
greatest beneﬁts, the techniques do require that the application
have some reuse of data in order to amortize the setup
costs. However, because many HPC applications (PDE, ODE,
Multigrid solvers, etc.) all exhibit this characteristic exactly
and use linear operations iteratively, we expect the technique
to apply to a large class of iterative methods dominated by
linear operations. We also note that iterative solvers can take
up to 80%-99% of the full application runtime for many HPC
and scientiﬁc applications.
an output vector Xk, such that Xk = (cid:80)N−1
As another example, consider the FFT application, which
is also a linear transformation similar to MVM. FFT produces
n=0 xne−i2πkn/N .
Due to linearity, part of the algorithm can also be expressed
as a MVM operation. The matrix (M) used in the product is a
symmetric matrix which represents: Mst = e−i2πst/N . Using
M, the FFT application can then be expressed as X = M x
Similar to other MVMs, we can utilize a linear code to detect
faults (cT (X) = (cT M )x) and the only precomputation that
is required for localizing errors in the output of FFT (cT M ),
is entirely independent of the input (x)
Linear systems and the linear operations that operate on
them are at the core of many non-scientiﬁc applications as
well. For example, many emerging applications (Recognition,
Mining, and Synthesis (RMS) applications are increasingly
being dominated by linear operations. RMS applications are
increasingly being dominated by linear operations, such as dot
products and matrix vector products, in order process large
data sets in the most efﬁcient manner. For example, the winner
of the Netﬂix Prize competition 2009, which focused on
analyzing large data sets of movie preferences and synthesizing
unknown preferences, was primarily a numerical solution using
matrix factorizations at it’s core [21]. Other RMS applications
which process large data sets are also heavily dominated by
9
linear operations with which the proposed techniques for fault
tolerance in this paper are directly applicable.
V. RESULTS AND ANALYSIS
This section evaluates the performance and scalability of
partial recomputation and error localization in the context of
a common linear solver application, CG (Section IV-B).
We compare the performance of partial recomputation
(PR) with traditional detect and rollback (DR) methods in this
section. The baseline (DR) uses the common ABFT encoding
scheme (a linear code) for detection. We also consider
two instances of partial recomputation, one with full error
localization (binary search depth of d=1) and one instance with
a partial traversal of the localization tree (d = 0.4). We also
consider two baseline detect and rollback (DR) approaches.
One that assumes an ideal (oracle-based) detector and another
with a more realistic detection-based approach that uses the
traditional checksum/threshold comparison (cT y−cT Ax ≶ τ).
In this case, the threshold is chosen to dynamically adjust to
the scale of the input (τ = t(cid:107)x(cid:107)) [27]. In our experiments,
we used a ﬁxed scaling factor of t = 1 for the dynamically
adjusted threshold. Both baseline implementations (DR) apply
detection and rollback at the level of matrix vector operations.