entry was 0, prompting a policy rejection of the hijacking attempt.
We also simulated the attacks discovered by ROPgadget using Pin
and verified that the guards indeed block the attacks in practice.
We can also study the theoretical probability of realizing a gadget
chain. The probability of finding a gadget address that can pass
the guard code to initiate a gadget chain is approximately equal
to the ratio p of 1’s to 0’s in the hash table that encodes the CCFG
policy. This can be reduced almost arbitrarily small by increasing
the hash table size relative to the code size (see §3). For example, in
gcc this ratio is as small as 0.004. Only 650KB of the original 8499KB
code section is visited by the unit tests and remains reachable after
control-flow trimming—an attack surface reduction of 92%.
Moreover, if control-flow trimming is coupled with software
fault isolation (SFI) [50, 82, 89] to enforce indivisible basic blocks
for the guarded-jump trampolines in Table 4, then the probability
of realizing a length-n gadget chain reduces to pn. Since SFI is
much easier to realize than CFI for source-free binaries (because
it enforces a very simple CFG recoverable by binary disassembly),
and tends to impose very low runtime overhead, we consider such
a pairing to be a promising direction of future work.
5.3 Accuracy
Specificity. To measure our approach’s accuracy in retaining
5.3.1
consumer-desired features while excluding undesired ones, we used
the programs in Table 7, including several real-world ftp servers,
exim, ImageMagic convert, gcc, and two web browsers, since they
constitute large, complex pieces of software.
To test gcc, we trained by compiling its own source code to a 64-
bit binary, and tested by attempting to compile many C programs to
various architectures (32-bit and 64-bit) using the trimmed binary.
For other programs we used the test suites described earlier. In
the ImageMagic experiments, the desired functionality is convert-
ing a JPG picture to PNG format, and the undesired functionality is
)
%
(
y
c
a
r
u
c
c
A
60
50
40
30
20
10
0
1
2
3
4
5
6
7
Number of different interaction types
8
9
100
80
60
40
20
)
%
(
s
e
v
i
t
a
g
e
n
e
s
l
a
F
10
11
12
0
0
8
16
24
32
40
48
56
64
72
Table size (KB)
gcc
convert
exim
vsftpd
pure-ftpd
proftpd
80
88
96
104 112 120 128
Figure 4: Accuracy vs. interaction diversity with uzbl, using
a fixed training set size of 100 and t = 0.0
resizing a picture. For ftp servers, the undesired functionalities are
the SITE and DELETE commands, and the remaining commands are
desired. Ftp file content and command order were randomized dur-
ing training and evaluation. For exim, the undesired functionality
is -oMs (which sets the sender host name instead of looking it up).
The undesired functionalities for epiphany and uzbl-browser are
incognito mode and cookie add/delete, respectively.
Positives in the classification are execution failures during testing,
as signaled by premature abort with a security violation warning.
False negatives are runs that exercise a consumer-undesired seman-
tic feature even after trimming. In contrast, a false positive occurs
when the defense aborts a consumer-desired functionality.
For all these experiments, the false negative rate is zero. That is,
no consumer-unwanted functionality is available in any of the test
binaries after trimming. For example, after instrumenting gcc with
training data that uses the -m64 command-line flag to target 64-bit
architectures, the trimmed binary is unable to compile any program
for 32-bit architectures; specifying -m32 on the command-line yields
a security abort. This is because our method is a whitelisting ap-
proach that prefers overfitting to maintain high assurance. The
same experiments performed using prior CFI-only solutions yield
the 100% false negative rate reported in Table 1.
A classification’s susceptibility to false positives can be measured
in terms of its false positive ratio (i.e., the complement of its speci-
ficity). The false positive ratio of control-flow trimming is driven
by the unit testing’s ability to comprehensively model the set of
semantic features desired by the consumer. We therefore measure
accuracy as the false positive ratio, broken down into three mea-
sures: the percentage of contexts incorrectly identified as anomalies,
the percentage of branch origins that at least one context anomaly
incorrectly detected at that branch site, and the total percentage of
traces in which at least one anomaly was incorrectly detected.
Table 7 shows the resulting false positive ratios. Each entry in
the table is averaged over 10 different experiments in which trace
samples are randomly drawn. Since the training phase’s accuracy
depends significantly on the size of the training data, we conducted
experiments with 10–1000 samples for training, evaluation, and
testing with a ratio of 3 : 1 : 1. The experiments consider the effects
of two different confidence thresholds for CCFG pruning (see §3.2):
0.0, 0.25, and an optimal threshold t∗ experimentally determined
as the minimum threshold that achieves zero false negatives for
evaluation sample traces. A threshold of 0.0 means no pruning,
which is the most conservative CCFI policy (no relaxation). All
experiments use contexts of length 4 as described in §4.
Figure 5: False negative ratios with varying table sizes
As expected, increasing the training data size significantly im-
proves classification accuracy, until at higher training sizes, almost
all experiments exhibit perfect accuracy. More aggressive CCFG
policy pruning via lower confidence thresholds helps to offset the
effects of overfitting when limited training data is available. Increas-
ing context size has a reverse effect; the increased discriminatory
power of the classifier (due to widening its feature space by a mul-
tiplicative factor for each additional context entry) creates a more
complex concept for it to learn. More comprehensive training is
therefore typically required to learn the concept.
Interactive Experiments. As a whitelisting approach, our de-
5.3.2
sign primarily targets software whose desired features are well
known to the consumer, and can therefore be fully exercised during
training. Table 7 shows that highly interactive products, such as
browsers, might require more training to learn all their features as
a result of this design. Experiments on epiphany and uzbl require
about 500 traces to obtain high accuracy, with a few rare corner
cases for epiphany only discovered after about 1000 traces, and uzbl
never quite reaching perfect accuracy.
To better understand the relationship between interaction di-
versity and training burden for such products, Figure 4 plots the
accuracy rate for uzbl as the diversity of interactions increases,
with the training set size held fixed at 100 traces. Each data point
characterizes an experiment in which training and testing are lim-
ited to x ∈ [1, 12] different types of user interactions (e.g., using
forward-backward navigation but not page-zoom). The results show
an approximately linear decline in accuracy as the diversity of inter-
actions increases, indicating that more training is needed to learn
the consumer’s more complex policy.
5.3.3 Table Size. For efficiency purposes, our enforcement approx-
imates the policy being enforced as a hash table (see §3.3). Poor
approximations that use an overly small hash table could permit
dangerous false negatives (e.g., undetected attacks), since the en-
forcement would inadvertently accept policy-violating contexts
whose hashes happen to collide with at least one policy-permitted
context. To investigate the minimum table sizes needed to avoid
these risks, we therefore performed an additional series of experi-
ments wherein we varied the hash table size without changing the
policy, and measured the false negative ratio for each size.
Figure 5 plots the results for six of the programs with test suites,
with hash table size on the x-axis and false negative ratio on the
y-axis. The results show that even hash table sizes as small as 128
bytes (1024 bit-entries) reliably achieve a zero false negative rate.
This is because policy-accepted contexts are so rare relative to the
space of all possible contexts that almost any sequence of contexts
that implements an undesired feature quickly witnesses at least
one context that is policy-violating, whereupon it is rejected.
Our experiments nevertheless use larger table sizes than this
minimum in order to minimize population ratio p, which §5.2 shows
is important for resisting implementation-aware code-reuse attacks.
Specifically, table sizes that scale with the code section size are
recommended for security-sensitive scenarios where the threat
model anticipates that adversaries have read-access to the table,
and might use that knowledge to craft gadget chains.
6 DISCUSSION
6.1 Control-flow Obfuscation
Although our evaluation presently only targets non-obfuscated
binary code, we conjecture that control-flow trimming via CCFG
enforcement has potentially promising applications for hardening
obfuscated binaries as well. Instruction-level diversification [21],
opaque predicates [47], and control-flow flattening [84] are some
examples of code obfuscation and anti-piracy techniques that are
commonly applied by code-producers to prevent effective binary
reverse-engineering.
For example, flattening adds a dispatcher to the program through
which all control-flow transfers are rerouted. This makes it more
difficult for adversaries to reverse-engineer the control-flows, but
it also prevents context-insensitive CFI protections from secur-
ing them, since the flattening transforms individual CFG edges
into chains of edges that must be permitted or rejected. Context-
sensitivity is needed to reject the chain without rejecting the indi-
vidual edges in the chain. The context-sensitivity of our approach
therefore makes it well-suited to such obfuscations.
6.2 Shared Libraries
Our experiments report results for CCFG policies enforced on user-
level applications and their dedicated libraries, but not on system
shared libraries. Securing system shared libraries can be accom-
plished similarly, but if the library continues to be shared, its policy
must permit all the semantic features of all the applications that
import it. This can introduce unavoidable false negatives for the
individual applications that share it. We therefore recommend that
consumers who prioritize security should avoid shared versions of
the system libraries in security-critical applications, so that control-
flow trimming can specialize even the system library code to the
application’s specific usage requirements.
6.3 Concurrency, Non-determinism, and
Non-control Data Attacks
Our IRM implementation stores contextual information in thread-
local machine registers for safety and efficiency. This immunizes
it against context pollution due to concurrency. However, it also
means that it cannot block attacks that have no effect upon any
thread’s control-flow, such as non-control data attacks in which
one thread corrupts another thread’s data without affecting its own
control-flows or those of the victim thread. Such attacks are beyond
the scope of all CFI-based defenses [2].
7 RELATED WORK
7.1 Code Surface Reduction
Software debloating has been used in the past to reduce code sizes
for performance and security. Such techniques were initially applied
to Linux kernels to save memory on embedded systems [19, 35,
45]. Later the focus shifted to reducing the kernel’s attack surface
to improve security [33, 42–44, 77]. Prior work has shown that
certain Linux kernel deployments leave 90% of kernel functions
unused [42]. kRazor learns the set of used functions based on
runtime traces, and limits the code reachability using a kernel
module. Face-Change [33] makes multiple minimized kernels in a
VM and exposes each minimized kernel to a particular application
upon context-switching. In contrast to these works, our approach
is not kernel-specific, can enforce context-sensitive control-flow
policies, and can debloat code at instruction-level granularity.
Code surface reduction has recently started to be applied to user-
level libraries and programs. Winnowing [48] is a source-aware
static analysis and code specialization technique that uses partial
evaluation to preserve developer-intended semantics of programs.
It implements Occam, which performs both intra-module and inter-
module winnowing atop LLVM, and produces specific version of
the program based on the deployment setup. Piecewise Debloat-
ing [64] uses piece-wise compilation to maintain intra-modular de-
pendencies, and a piece-wise loader that generates an inter-modular
dependency graph. The loader removes all code that is not in the
dependency graph. Chisel [36] debloats the program given a high-
level specification from the user. The specification identifies wanted
and unwanted program input/output pairs, and requires the source
code and the compilation toolchain. To accelerate program reduc-
tion, Chisel uses reinforcement learning. It repeats a trial and error
approach to make a more precise Markov Decision Process that
corresponds to the specification.
Source-free, binary code reduction has been achieved for certain
closed-source Windows applications by removing unimported func-
tions in shared libraries at load time [54]. The approach requires
image freezing, which prevents any new code section or executable
memory page from being added. Shredder [51] is another source-
free approach that specializes the API interface available to the
application. It combines inter-procedural backwards data flow anal-
ysis and lightweight symbolic execution to learn a policy for each
function in the program. Although these approaches boast source-
freedom, they can only permit or exclude program behaviors at the
granularity of functions with well-defined interfaces. Many critical
security vulnerabilities, including Shellshock, cannot be isolated to
individual functions, so cannot be pruned in this way without re-
moving desired program behaviors. Our approach therefore learns
and enforces policies definable as arbitrary CCFGs irrespective of
function boundaries or even the feasibility of recovering function
abstractions from the binary.
7.2 Control-flow Integrity
SFI [82] and CFI [1] confine software to a whitelist of permitted
control-flow edges by guarding control-transfer instructions with
dynamic checks that validate their destinations. In SFI, the policy is
typically a sandboxing property that isolates the software to a subset
of the address space, whereas CFI approaches typically enforce
stronger properties that restrict each module’s internal flows. In
both cases the policy is designed to prohibit flows unintended
or unwanted by software developers (e.g., developer-unwanted
component interactions or control-flow hijacks). Since the original
works, the research community have proposed many variations (e.g.,
[4, 22, 23, 29, 49, 52, 57–60, 63, 78–80, 85, 89, 91, 92]), most of which
improve security, performance, compatibility, and/or applicability
to various code domains and architectures.
CFI algorithms come in context-sensitive and context-insensitive
varieties. Context-sensitivity elevates the power of the policy lan-
guage using contextual information, such as return address history
or type information, usually in a protected shadow stack. The price
of such power is usually lower performance due to maintaining,
consulting, and securing the contexts. Low overhead solutions must
usually relax policies, introducing a sacrifice of assurance.
For example, kBouncer [61] enforces a context-sensitive policy
that considers the previous 16 jump destinations at each system
call. Unfortunately, enforcing the policy only at system calls makes
the defense susceptible to history-flushing attacks [18], wherein
attackers make 16 benign redundant jumps followed by a system
call. ROPecker [20] and PathArmor [79] implements OS kernel
modules that consult last branch record (LBR) CPU registers to
achieve lower performance, which are only available at ring 0. Both
systems implement sparse checking regimens to save overhead,
in which not every branch is checked. CCFI [49] uses message
authentication codes (MACs) to protect important pointers, such as
return addresses, function pointers, and vtable pointers, to enforce
call-return matching policies.
CFI methodologies can also be partitioned into source-aware and
source-agnostic approaches. Source-aware approaches are typically
more powerful and more efficient, because they leverage source
code information to infer more precise policies and optimize code.
However, they are inapplicable to consumers who receive closed-
source software in strictly binary form, and who wish to enforce
consumer-specific policies. They likewise raise difficulties for soft-
ware products that link to closed-source library modules. These
difficulties have motivated source-agnostic approaches.
WIT [4], MIP [57], MCFI [58], Forward CFI [78], RockJIT [59],
CCFI [49], π-CFI [60], VTrust [90], VTable Interleaving [12], Pitty-
Pat [26], CFIXX [16], and µCFI[38] are examples of source-aware
CFI. XFI [29], Native Client [89], MoCFI [22], CCFIR [91], bin-
CFI [92], O-CFI [52], BinCC [85], Lockdown [63], PathArmor [79],
TypeArmor [80], C-FLAT [3], OFI [87], and τCFI [55] are all exam-
ples of source-free approaches.
Our research addresses the problem of consumer-side software
feature trimming and customization, which calls for a combination
of source-agnosticism and context-sensitivity. Binary control-flow
trimming is therefore the first work to target this difficult combi-
nation for fine-grained CCFG learning and enforcement. Table 1
emphasizes the difference between this problem and the problems
targeted by prior works. For example, PathArmor enforces con-
textual CFG policies, but maintains a much sparser context that is