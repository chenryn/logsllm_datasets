如果是rhel6或者centos5，32位版本，下载地址为：
http:/dl.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm
#yum-y installmysql-mmm*
#rpm-ivhepel-release-5-4.noarch.rpm
http://download.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm
在三台服务器上分别进行安装，安装命令如下：
3.安装MySQL-MMM
第一个账号repl（复制账号），是用于主主复制的。
BY'123456';
GRANTREPLICATIONCLIENTON**TO'mmm_monitor'@%'IDENTIFIED
'mmm_agent'@'%'IDENTIFIEDBY'123456';
GRANTPROCESS,SUPER,REPLICATION CLIENTON**TO
IDENTIFIED BY'repl';
GRANTREPLICATIONslave,REPLICATIONCLIENTON**TO'repl@%
mysql-mmm-tools-2.2.1-1.el5
mysql-mmm-2.2.1-1.el5
#rpm-qa|grepmysql-mmm
#wget
说明：
配置MMM监控、代理服务
，下载地址为：
第7章
目前流行的4种高可用架构·243
---
## Page 258
244
●第三部分高可用集群管理篇
口balanced：该模式下可以多个主机同时拥有此角色。
 exclusive：在这种模式下任何时候只能有一个主机拥有该角色。
在此步骤中会涉及两种模式：
active_master_role
修改后的内容如下：
#
#
hosts M1,M2,slave1#作为reader的服务器
hosts M1,M2#能够作为writer的服务器
mode
mode
ip192.168.100.51
mode
ip
peer
peer
modemaster
ip192.168.8.25
agent_password
agent_user
replication_passwordrepl#前面创建的复制账号密码
bin_path
pid_path
cluster_interface
mode
mode
replication_user
192.168.8.52#reader节点虚拟IP，应用的读请求将直接连接到这些IP
192.168.8.51#writer节点虚拟IP，应用的写请求将直接连接到这个IP
192.168.8.27
192.168.8.26
balanced #平衡模式
exclusive#排他模式
slave
M1
M2
slave
writer
etho
mmm_agent#前面创建的代理账号
repl#前面创建的复制账号
/usr/libexec/mysql-mmm/
/var/run/mysql-mmm/mmm_agentd.pid
---
## Page 259
最后，在MySQL-MON服务器上配置mmm_mon.conf配置文件。
接着，在MySQL-S1服务器上修改mmm_agent.conf配置文件。
#that'this'server(db1bydefault),aswellas all other servers,havethe
再次，在MySQL-M2服务器上修改mmm_agent.conf 配置文件。
includemmm_common.conf
[root@M1mysql-mmm]#catmmm_mon.conf
修改后的内容如下：
includemmm_common.conf
[root@slave1mysql-mmm]#cat mmm_agent.conf
修改后的内容如下：
#properIPaddressesset inmmm_common.conf.
#The'this'variablerefersto thisserver.Proper operationrequires
includemmm_common.conf
[root@M2mysql-mmm]#catmmm_agent.conf
修改后的内容如下：
#proper IP addresses set in mmm_common.conf.
#The'thisvariablerefersto thisserverProperoperationrequires
includemmm_common.conf
[root@M1mysql-mmm]#catmmm_agent.conf
修改后的内容如下：
其次，在 MySQL-M1服务器上修改 mmm_agent.conf配置文件。
通常情况下在writer上采用exclusive模式，而在reader上采用balanced 模式。
#debug1
thisslave1
thisM2
thisM1
#that‘this'server(db1by default),aswellasallotherservers,have the
that'this'server(db1bydefault),aswellasallotherservershavethe
The'this'variablereferstothisserver.Properoperationrequires
ping_ips
status_path
bin_path
ip
#可以ping的真实代理服务器的IP
192.168.8.25,192.168.8.26,192.168.8.27
/var/lib/mysql-mmm/mmm_mond.status
/usr/libexec/mysql-mmm
/var/run/mysql-mmm/mmm_mond.pid
127.0.0.1
第7章目前流行的4种高可用架构·245
---
## Page 260
246
别说明一下，这个读写分离要配合前端程序来用，也就是说你的程序要支持读写分离，要
·第三部分高可用集群管理篇
其中，M1负责写（IP是192.168.8.51），slave1负责读（IP是192.168.8.52），这里要特
[root@M1~]#mmm_control checks all
最后，在MySQL-MON监控机上查看MMM状态信息：
[root@M1~]#mmm_controlshow
然后，在MySQL-MON服务器上启动以下监控服务：
在启动各项服务时，首先，从MySQL-M1/M2/S1服务器上启动以下服务：
/etc/init.d/mysql-mmm-monitorstart
/etc/init.d/mysql-mmm-agentstart
这样，
slave1
slave1
slave1
slave1
W
W
W
debug0
slave1(192.168.8.27) slave/ONLINE.Roles:reader(192.168.8.52)
M1(192.168.8.25)master/ONLINE.Roles:writer(192.168.8.51)
．启动各服务器的相关服务
M2(192.168.8.26)master/ONLINE.Roles:
monitor_password
monitor_user
#kill_host_bin
#Functionality"in the PDF documentation.
#Thekill_host_bin doesnotexistbydefault,thoughthe monitorwill
auto_set_online10 #发现节点丢失则过10秒进行切换
rep_backlog
，配置MMM监控、
rep_threads
mysql
ping
rep_backlog
rep_threads
ping
rep_backlog
rep_threads
nysql
mysql
ping
[lastchange:2013/06/1423:06:10]OK:Backlogisnull
[ast change:2013/06/1423:06:10]
[last change:2013/06/14 23:06:10]
[last change:2013/06/14 23:06:10] OK
[lastchange:2013/06/1423:06:10]
lastchange:2013/06/1423:06:10]
(lastchange:2013/06/1423:06:10]
[lastchange:2013/06/1423:06:10]
[last change:2013/06/1423:06:10]
[lastchange:2013/06/1423:06:10]
[lastchange:2013/06/1423:06:10]
123456#前面创建的监控账号密码
mmm_monitor#前面创建的监控账号
/usr/libexec/mysql-mmm/monitor/kill_host
tchange:2013/06/1423:06:10]
、代理服务的工作就完成了。
OK
OK
OK
OK:
吴
OK:
OK
吴
:Backlog is null
:Backlogisnull
---
## Page 261
才可以切换，否则就在那里等待，在此过程
掉，那么整个集群就全部挂掉了。
面去，即使它没有延时。如果此时M2再挂
上，永远不会切换到比其身份低的S1从机上
的逻辑会切换到同级别的主机，也就是M2
辑，VIP应该漂移到S1上，但在这里，作者
就会有延时），如图7-2所示。
给锁住，这样在M1上更新tablel表时，M2
M2上“lock tables tablel read;”（把tablel表
seconds
能，这个与官方的MySQL-PROXY和淘宝的Amoeba是不同的。
把这两个VIP告诉开发人员，让他们在配置文件里指定，MMM软件并不具有读写分离的功
在漂移到M2上后，必须要等到同步追完
然后把M1的MySQL关闭，
上述三台服务器的服务都工作起来以后，其状态如下：
mmm_agentd.log日志记录着机器切换信息：
模拟M2同步延时，S1同步无延时，
[root@M1mysql-mmm]#mmm_controlshow
6.模拟岩机切换测试
2013/06/1500:49:52 INFO Changing active masterto'M1'
2013/06/1500:40:30INFOChangingactivemastertoM2
[root@slave1mysql]#tail-f/var/log/mysql-mmm/mmm_agentd.log
AWAITING_RECOVERY to ONLINE because of auto_set_online(10 seconds).It was in state AWAITING_RECOVERY for 12
2013/06/1500:54:12FATALStateof host'M2'changed fromHARD_OFFLINEtoAWAITING_RECOVERY
mmm_mond.log日志记录着集群状态信息：
下面分别查看三个服务器的日志，可以看到启动信息，如下所示：
场
下面来进行岩机测试。
2013/06/1500:54:24FATALStateof host'M2'changedfrom
HARD_OFFLINE(ping:OK,mysql:notOK)
2013/06/1500:51:11FATALStateofhost'M2'changedfromONLINEto
[root@M1~]#tail-f/var/log/mysql-mmm/mmm_mond.log
2013/06/15 00:51:11INFO Added:writer(192.168.8.51)
2013/06/1500:51:11INFOWehavesomenewrolesaddedoroldrulesdeleted！
[root@M1~]#tail-f/var/log/mysql-mmm/mmm_agentd.log
景一，master岩机，切换。
slave1(192.168.8.27)slave/ONLINE.Roles:reader(192.168.8.52)
M2(192.168.8.26)master/ONLINE.Roles:
M1(192.168.8.25)master/ONLINE.Roles:writer(192.168.8.51)
按照正常逻
在
图7-2MMM环境架构图（模拟场景）
延时0秒
第7章
目前流行的4种高可用架构·247
8
利
---
## Page 262
248·第三部分高可用集群管理篇
中，利用的是如图7-3所示的函数。
个软件会自动记录M2上的点。如下所示：
此时，S1会自动指向 M2，自动执行change master命令与M2机器进行同步复制，并且这
锁。如果延时很大，想要硬切换，只能人工杀掉到 select master_POS_WAITO这个进程id。
rows in set (0.02 sec)
此时，VIP还在M2上，并不会因M1修复好了再漂移过去。
[root@M1mysql-mmm]#mmm_control show
到这里，把M1的MySQL进程再启动起来，命令如下：
[root@M1mysql-mmm]#mmm_control show
在M1上，再执行 stop slave:
结果，slave VIP 漂移到了M1上。
[root@M1mysql-mmm]#mmm_control show
这时，若在 slavel上，执行 stop slave:
在 slave上，只要同步挂掉，有一个线程不为Yes，就会切换到另两台主机上：
场景二，slave 切换。
图7-3中的MASTER_POS_WAIT函数会等slave执行完全部的中继日志后，再释放
slave_SQL_Running:Yes
slave_lO_Running:Yes
slave1(192.168.8.27)slave/REPLICATION_FAIL.Roles:
M1(192.168.8.25)master/ONLINE.Roles:reader(192.168.8.52)
slave1(192.168.8.27) slave/ONLINE.Roles: reader(192.168.8.52)
M2(192.168.8.26) master/ONLINE.Roles: writer(192.168.8.51)
M1(192.168.8.25) master/ONLINE.Roles:
slave1(192.168.8.27) slave/ONLINE.Roles: reader(192.168.8.52)
M1(192.168.8.25) master/HARD_OFFLINE.Roles:
M2(192.168.8.26)master/ONLINE.Roles:writer(192.168.8.51)
roo
NUL.L.
ECT
Host
M2:51201
test|Comect
NULLQuery
testQuery
-bin.000009
Command
图7-3状态信息
210)
502Master has sent all binlog to sluve. waiting for bialog to
514 Waiting for table metadata 1
501Waiting for the slave SQl.thread te advance positicn
snneeting after a failed master event read
loek
---
## Page 263
在 primary节点上进行，只有当 primary节点挂掉时，secondary节点才能提升为 primary节
在 secondary节点上不允许对DRBD设备进行任何操作，包括只读，所有的读写操作只能
将数据存到自己的磁盘中。目前，DRBD 每次只允许对一个节点进行读写访问，也就是说，
接结合使用，也可以把它看做是一种网络RAID1。
络来镜像整个设备。它允许用户在远程机器上建立一个本地块设备的实时镜像。与心跳连
之间镜像块设备内容的存储复制解决方案。它是通过Linux的内核和相关脚本实现的，用
在对方主机上的资源或者服务。
最核心的部分包括两个：心跳监测和资源接管，心跳监测可以通过网络链路和串口进行，
点，要指定master_AUTO_POSITION，具体内容请查看MySQL5.6同步复制详解。
M2上，如果你用的是MySQL5.6GTID 同步复制模式，进行切换时会报错。报错信息如下：
以构建高可用集群，常用的高可用集群结合方案有NFS、MySQL等。其实现方式是通过网
间内未收到对方发送的报文，那么就认为对方失效，这时需启动资源接管模块来接管运行
而且支持冗余链路，它们之间相互发送报文来告诉对方自己当前的状态，如果在指定的时
7.2
DRBD负责接收数据，把数据写到本地磁盘，然后发送给另一个主机。另一个主机再
DRBD（DistributedReplicated BlockDevice）是一个用软件实现的、无共享的、服务器
Heartbeat 是Linux-HA工程的一个组成部分，它实现了一个高可用集群系统。Heartbeat
这个报错信息的意思是，采用GTID 模式，change master to无须指定binlog文件和 pos
2013/06/1501:43:27INFO Changing active masterto'M2'
需要注意的是，MMM只支持当前的MySQL5.5模式，当M1岩机时，S1会重新漂移到
这时，VIP又漂移到了slavel上。
之后，若在slavel上执行start slave：
master_AUTO_POSITIONisactive.
RELAY_LOG_FILEandRELAY_LOG_POScannotbesetwhen
2013/06/1501:43:27FATALFailed tochangemastertoM2':ERROR:SQLQuery
[root@M1mysql-mmm]#mmm_controlshow
结果，slaveVIP漂移到了M2上。
writer(192.168.8.51)
[root@M1mysql-mmm]#mmm_control show
Error:Parametersmaster_LOG_FILE,master_LOG_POS,
Heartbeat+DRBD+MySQL架构的搭建演示
slave1(192.168.8.27) slave/ONLINE.Roles:reader(192.168.8.52)
slave1(192.168.8.27)slave/REPLICATION_FAIL.Roles:
M2(192.168.8.26)master/ONLINE.Roles:reader(192.168.8.52),
M1(192.168.8.25)master/REPLICATION_FAIL.Roles:
M2(192.168.8.26) master/ONLINE.Roles: writer(192.168.8.51)
M1(192.168.8.25)master/REPLICATION_FAIL.Roles:
第7章
目前流行的4种高可用架构·249
---
## Page 264
250·第三部分高可用集群管理篇
进行服务，所以Heartbeat和DRBD是一组黄金组合，
点，继续工作。
于本地主机和远程主机上，切换时，远程主机只要使用它上面的那份备份数据就可以继续
（1）新增加一块/dev/sdb1硬盘存放DRBD数据
机器重启后DRBD模块消失，需要写人/etc/rc.local开机自动执行。
命令如下：
#wget http://oss.linbit.com/drbd/8.4/drbd-8.4.3.tar.gz
（3）安装DRBD（两台机器都这么做）
命令如下：
在高可用（HA）中使用DRBD功能，可以代替使用一个共享盘阵。因为数据同时存在