F
f
o
y
t
i
r
e
v
e
S
Low Priority/Low Loss
Low Priority/High Loss
No Impact
Blackhole
0
15
B2
B4
Cluster
10
30
Count of Failure Events
20
5
Count of Failure Events
10
Figure 2: Distribution of failure
events by plane and network
Figure 3: Distribution of failure
events by structural element
Figure 4: Distribution of failure
events by impact and network
network are less obvious. The ToRs are the source of many
failures largely as a result of failures caused by buggy soft-
ware updates happening concurrently across multiple ToRs.
ToR software evolves relatively rapidly, so these contribute a
large share to network unavailability. We use an out-of-band
control plane network for the fabrics and B4, and failures
in this network can have signiﬁcant impact on the control
planes and, therefore, on availability. Of the network-wide
control plane components, BwE accounts for a noticeable
number of failure events.
Impact. Failure events directly impact availability targets,
and we could have measured network availability, but post-
mortem reports do not describe all availability failures, only
large ones. For this reason, we categorize failure events into
six different impact classes: blackholes, where trafﬁc to a
set of targets, or from a set of sources was completely black-
holed; high/low packet loss for high priority trafﬁc; high/low
packet loss in trafﬁc for lower priority trafﬁc; and no impact
(discussed below). To distinguish high/low packet loss, we
used the loss thresholds used for the availability targets for
high and low priority trafﬁc.
Figure 4 shows the distribution of failure events across
these categories. Our failure events often have huge impact,
with more than 30 failure events resulting in trafﬁc black-
holes, often for entire clusters at a time. High packet losses
resulting in network overloads from failures also occur fre-
quently, across all classes of trafﬁc. A small number of fail-
ure events have no impact: in these events, often, a latent
failure was discovered, but operators waited to ﬁx it because
it posed no immediate threat. All networks are susceptible
to high-impact categories (blackholes and high packet loss),
with the exception of high priority trafﬁc on B4, which does
not carry that trafﬁc.
Duration. The duration of a failure event represents the
time from when it was ﬁrst discovered to when recovery
from the failure was completed. In some large failure events,
such as those in which trafﬁc to and from an entire cluster
is blackholed, operators ﬁrst drain the entire cluster (i.e.,
reconﬁgure the load-balancer to stop sending any trafﬁc to
63
services that cluster, preferring instances of services in other
clusters) before starting to root-cause the failure and initiate
recovery.
In these cases, the duration measures when the
failure was ﬁxed (e.g., the blackhole was repaired). Service
resumption after such failures can sometimes take much
longer (e.g., due to delay for data replication to catch up),
so we do not include it in the event duration. For other
failure events, operators attempt to repair the failure without
draining services; in these cases, duration measures the time
during which the effects of the failure (e.g., packet loss)
were evident.
Figure 5 plots the CDF of failure event durations by net-
work. About 80% of all our failure events had a duration
between 10 mins and 100 mins. When measured against
the availability targets discussed in Section 3.2, where the
target for high priority trafﬁc was a few minutes of down-
time per month, these numbers quantify the challenges we
face in maintaining high availability within our networks.
The failure events whose durations were less than 10 min-
utes beneﬁted either from operator experience in diagnosing
the root-cause, or from the fact that the cause of the failure
was obvious (e.g., because the impact of the failure was seen
after completing a step of a MOp). Failure durations over
100 mins usually represent events that resulted in low-levels
of packet loss for less important trafﬁc, or, in some cases, a
latent failure which was discovered but had not impacted the
network yet. In these cases, operators chose to wait to ﬁx
the problem because it was deemed lower priority, either by
waiting for developers to develop and test a software release,
or for vendors to ship a replacement part.
Finally, Figure 5 shows that, distributionally,
failure
events in B2 and clusters are of shorter duration than in B4
(note that the x-axis is logscale), likely because the former
two networks carry high availability trafﬁc, but B4 does not.
The Role of Evolution in Failure Events. We have dis-
cussed that one of the main challenges in maintaining high-
availability is the constant evolution in our networks. This
evolution implies frequent new software rollout, and soft-
ware bugs, frequent MOps on the network, and upgrades
to network equipment. Accordingly, we categorized failure
events according to whether the event was caused by a soft-
ware bug, and whether an upgrade, a conﬁguration change, a
MOp, or a software rollout was in progress at the time of the
failure. These categories are not mutually exclusive since
upgrades, conﬁgurations, and software rollouts are speciﬁc
forms of MOps; we added these two categories to highlight
the role that network evolution plays in failure events.
Figure 6 shows how these forms of evolution or results
thereof (e.g., bugs) distribute across different networks.
Nearly 70 of the failure events occurred when a MOp
was in progress on the network element where the failure
occurred. To give some context for this result and the rate
of evolution in our network: in a typical week last year, 585
network MOps were scheduled within Google. A MOp may
not always be the root-cause of the failure event. Bugs in
software account for nearly 35 failures, and other categories
of evolution are observed to a lesser extent (but in signiﬁcant
numbers) and across all networks.
6. ROOT CAUSE CATEGORIES
In addition to characterizing failure events by duration,
severity, and other dimensions, we have also classiﬁed them
by root-cause category.6 A failure event’s root-cause is
one that, if it had not occurred, the failure event would not
have manifested. A single failure event can have multiple
root-causes, as we discuss later. For a given failure event,
the root-cause is determined from the post mortem reports.
Root-causes of individual failure events by themselves don’t
provide much insight, so we categorizedˆA root-causes into
different categories, Figure 7.
Categorizing root-causes can be subjective. All network
failure root-causes can be classiﬁed into hardware failures,
software bugs, and conﬁguration errors, but, from this coarse
categorization, it is harder to derive speciﬁc insights into
how to counteract these root-causes, beyond generic sugges-
tions for adding hardware redundancy, hardening software,
and avoiding conﬁguration errors. Hence, we use a ﬁner-
grained categorization of root-causes that provide useful in-
sights into increasing network availability. We have left to
future work to explore whether other categorizations could
have yielded different insights.
Root-cause by network. Before discussing some of the
root cause categories in detail, we present the frequency
of occurrence of each category, and its breakdown by
network (Figure 7). Some root-cause categories manifest
themselves more frequently in failure events than others:
the frequency ranges from 2 to 13 in our dataset. Some
root-cause categories occur exclusively in one type of
5From this number, it would be incorrect to extrapolate the fraction of
MOps that lead to failure. Our post-mortems only document unique fail-
ures, and this number does not include automated MOps that are sometimes
not documented for review.
6A root-cause category roughly corresponds, in the dependability systems
literature, to a fault type. However, a fault is usually deﬁned either as a soft-
ware bug or a hardware failure [10], but our root-cause categories include
incorrect management plane operations, so we have avoided using the term
fault.
network (e.g., cascade of control plane elements in B4), and
some in two networks (e.g., control plane network failure).
These arise from design differences between the three
networks. However, at least six of our root-cause categories
manifest themselves at least once in all three networks, and
this, in some cases, indicates systemic issues. Finally, the
root-cause categories are roughly evenly distributed across
the different planes (not shown): 6 data plane, 7 control
plane, and 6 management plane categories.
6.1 Data Plane Categories
Device resource overruns. Several failure events, across
all three networks, can be attributed to problems arising from
insufﬁcient hardware or operating system resources. For
many years now, routers in the Internet have been known
to fail when injected with routing tables whose sizes exceed
router memory [16]. But, more subtle resource overruns
have occurred in Google’s networks, as this example shows.
Over-1. A complex sequence of events in a cluster triggered
a latent device resource limitation. Operators drained a fab-
ric link and were conducting bit-error tests on the link. Sev-
eral kinds of link drains are used in Google: lowering a link’s
preference, so trafﬁc is carried on the link only when other
links are loaded; assigning a link inﬁnite cost so it appears
in the routing table, but does not carry trafﬁc; or, disabling
the corresponding interfaces. These choices trade speed or
capacity for safety: less risky operations can be drained by
reducing preference, for example, and these drains can be
removed faster. But these induce complex drain semantics
which operators may not understand.
In this case, opera-
tors opted for the ﬁrst choice, so the link being tested was
carrying (and intermittently dropping, because of the tests)
live trafﬁc. This caused OFAs to lose their connectivity to
their OFC. When failing over to a new OFC through an
Open Flow Frontend (OFE), the OFE issued a reverse DNS
lookup before establishing the connection. However, this
lookup failed because its packets were being dropped on the
erroring link, and the thread performing the lookup blocked.
Soon, enough OFAs attempted to fail-over that OS limits
on concurrent threads were reached, and the entire control
plane failed. This event is notable for another reason: opera-
tors took a long time to diagnose the root cause because the
monitoring data collectors (for each cluster, two sets of mon-
itoring agents collect statistics from all switches and servers)
were both within the cluster, and when the fabric failed, vis-
ibility into the system was impaired.
Control plane network failure. B4 and clusters use an out-
of-band control plane network (CPN). Components like the
OFC, RA and FC communicate with each other and with
OFAs via the CPN, which is designed to tolerate single fail-
ures of the CPN routers and switches.
Complete failure of the CPN, where concurrent failure of
multiple CPN routers rendered the control plane components
unable to communicate with each other, caused three failure
events. These events resulted from a combination of hard-
ware component (line cards, routing engines) and operator
64
1.00
0.75
s
t
n
e
v
E
e
r
u
0.50
l
i
a
F
f
o
n
o
i
t
c
a
r
F
0.25
0.00
10
1000
Duration (in minutes)
Upgrade
Rollout
MOp
B2
B4
Cluster
t
x
e
t
n
o
C
Configuration
Bug
0
s
e
i
r
o
g
e
t
a
C
e
s
u
a
C
t
o
o
R
B2
B4
Cluster
Routing misconfigurations
Risk assessment failure
Partial or complete control−plane failure
Link flaps or errors
Lack of data plane and control plane synchronization
Lack of consistency between control plane elements
Incorrectly executed management process