available and a transitive and isometric transformation from
PUF responses to a random code words exists. Besides error-
correcting codes, other signal processing techniques can be
used to reduce the amount of noise of responses [35]. How-
ever, some degree of error-correction is usually inevitable.
Note that most known PUF constructions have an average
bit error probability of their responses of less than 10%.
V. PHYSICAL UNCLONABILITY
A. Rationale
As this work is motivated by the increasing usage of
physically unclonable functions, it is a natural choice to in-
clude unclonability into the model, which is the key property
of PUFs that cannot be achieved by algorithmic solutions.
In this section, we formally deﬁne the notion of physical
unclonability. We stress that we consider only clones on
the physical level and exclude mathematical clones. This
restriction is motivated by the fact that an adversary in
general has different possibilities for creating (i.e., cloning)
a PF system that shows the “same” behavior as another PF
system. For instance, the adversary could choose an Extract
algorithm that maps all inputs to the same output. Clearly,
two different PF systems using this Extract algorithm would
behave exactly the same, independent of the underlying PFs.
It is obvious that protection against such attacks can only
3An outsider only learns the offset h but not the code word c itself.
be provided by mechanisms outside of the PF system. In
general, while physical unclonability is an intrinsic feature,
this is not true for mathematical unclonability, which hence
is outside of the scope of a PF security model. We propose
a deﬁnition of physical unclonability that can informally be
(cid:48) is a physical clone of
stated as follows: A PF system PFS
another PF system PFS if both PF systems show the same
behavior and deploy the same Extract algorithm. By the
second condition, we guarantee that we consider clonability
on a physical level only.
It remains to discuss how to formalize the notion of
“same behavior”. Recall that PFs are assumed to be noisy
in general, which raises the question of when two PFs
can be considered being the same. A good starting point
is to consider at ﬁrst only one PF system. Recall that the
extraction procedure is deployed to make a PF system “as
deterministic as possible”. Nonetheless, in certain cases, the
same PF system might produce the same output twice only
with a certain probability. We referred to this probability as
the robustness of the PF system and termed it ρPFS (x) in
dependence of the considered challenge x (see Deﬁnition 5).
(cid:48) cannot be more similar to the
Intuitively, a clone PFS
corresponding original PF system PFS than PFS itself. On
the other hand, any PF system should be formally seen as
a clone of itself. Therefore, the robustness marks a natural
upper bound on “how similar a clone can become” and it
seems to be natural to integrate the notion of robustness into
the deﬁnition of clones.
Another aspect that needs to be considered is the follow-
ing: depending on the use case, only the responses of PFS to
a subset of challenges might be known at all. Thus, any other
(cid:48) that coincides on this subset of challenges
PF system PFS
could be seen as a clone. Therefore, it is sufﬁcient that the
deﬁnition of a clone captures only the set of challenges
X (cid:48) ⊆ X that are relevant w.r.t. the underlying use case.
Note that a cloning attack might have different meanings:
• Selective cloning refers to the event that for a given PF
system PFS a clone PFS
(cid:48) is constructed.
• Existential cloning: means that two arbitrary PF sys-
(cid:48) are produced, where one is the
tems PFS and PFS
clone of the other.
The difference between selective and existential cloning is
that in the latter case no “original PF system” is given and
instead, the adversary is free to choose which PF system is
cloned. Observe that this classiﬁcation has some similarities
to the security properties established for digital signatures
and message authentication codes (MACs).
B. Formalization
We start with formalizing the notion of a clone:
Deﬁnition 10 (Physical Clone): Let αPF and αEX be
a ﬁxed evaluation and extraction parameter, respectively.
Moreover,
=
(cid:48)
let PFS = PFSp,αPF,αEX and PFS
405
Figure 3. Existential unclonability security experiment Expex-uncl
A
(q).
PFSp(cid:48),αPF,αEX be two PF systems (Deﬁnition 2), that are
identical except of their physical component, i.e., p (cid:54)= p(cid:48).
(cid:48) is a δ-clone of PFS w.r.t. X (cid:48) ⊆ X if
We deﬁne that PFS
for all x ∈ X (cid:48) it holds that
(x, h) : (z, h) ← PFS(x, )(cid:3)
Pr(cid:2)(z, h) ← PFS
(cid:48)
≥ δ · ρPFS(x)
(11)
For simplicity, we write PFS
(cid:48) δ,X (cid:48)≡ PFS if Eq. 11 holds.
Next, we formalize both notions of unclonability by means
of two security experiments that specify the capabilities and
the goal of the adversary A. On a high level, the adversary A
is capable of creating arbitrary physical components, which
in turn determine PF systems. In practice, A will be limited
to a certain set of creation processes, e.g., by increasing
the sensitivity of his production facility. We capture this
formally by allowing A to choose the creation parameter
αCR from a set ACR of possible creation parameters. In
practice ACR is expected to be small. We start by deﬁning
existential unclonability, where A must produce two arbi-
trary clones. In this scenario, which is depicted in Figure 3,
A can query the Create process for αCR ∈ ACR to create
physical components p (see Deﬁnition 3).
Note that a physical function p implicitly deﬁnes a PF
system PFS = PFSp,αPF,αEX for some ﬁxed evaluation
and extraction parameter αPF and αCR, respectively (see
Deﬁnition 2). Typically, only adversaries for which the
time and computational effort are bounded are relevant for
practice. Hence, we assume that A can do at most q ≥ 2
queries to Create.
Deﬁnition 11 (Existential Physical Unclonability):
Let ACR be a set of creation parameters and let αPF
the evaluation and
and αEX be ﬁxed parameters for
extraction procedures, respectively. Note that this implicitly
deﬁnes a family FACR := {FαCR : αCR ∈ ACR} of PF
infrastructures (Deﬁnition 4).
A family of PF infrastructures FACR is called (γ, δ, q)-
cloning-resistant w.r.t. X (cid:48) ⊆ X , if
Pr(cid:2)PFS
(cid:48)
p(cid:48),αPF,αEX
(p, p(cid:48)) ← Expex-uncl
δ,X (cid:48)≡ PFSp,αPF,αEX :
(cid:3) ≤ γ
(q);
p ∈ [Create(αCR)]; αCR ∈ ACR;
p(cid:48) ∈ [Create(α(cid:48)
CR ∈ ACR
CR)]; α(cid:48)
A
(12)
406
Figure 4. Selective unclonability security experiment Expsel-uncl
A
(q).
This means: the probability that A generated, as output of
the security experiment depicted in Figure 3, two physical
components p and p(cid:48) which (i) imply clones on the PF
system level and (ii) that have been created using creation
parameters αCR ∈ ACR, is less than γ.
Note that Deﬁnition 11 covers different situations:
• Honest manufacturer: This case reﬂects the probability
that an honest manufacturer creates two clones by
coincidence and captures clonable PFs. In the case
of ACR = {αCR},
i.e., where only one creation
parameter is involved, the set FACR “collapses” to a
single PF infrastructure FαCR. Likewise, A can perform
Create only with this speciﬁc creation parameter. In
other words, A is restricted to actions that an honest
manufacturer could do within FαCR.
• Malicious manufacturer: This case covers the scenario,
where ACR contains more than one possible choice
for the creation parameter αCR, which allows A to
inﬂuence the Create process in order to create a clone.
Finally, we formalize selective physical unclonability in
terms of the security experiment depicted in Figure 4. The
difference to the security experiment of existential unclon-
ability is that the adversary A is given a PF system PFS
for which A must create a clone. Therefore, in addition to
queries to Create, A is allowed to query PFS with challenges
x ∈ X (cid:48). Again, we consider only restricted adversaries A
that can do at most q ≥ 1 queries to Create and PFS.
Deﬁnition 12 (Selective Physical Unclonability):
Let ACR be a set of creation parameters and let
αPF and αEX be ﬁxed parameters for
the evaluation
and extraction procedures,
let
:= {FαCR : αCR ∈ ACR} be the corresponding
FACR
set of PF infrastructures (Deﬁnition 4). Further, let PFS
be a PF system (Deﬁnition 2) within the family of PF
infrastructures FACR, i.e., PFS ∈ [Create(αCR)] for some
αCR ∈ ACR. We denote with A the adversary.
respectively. Moreover,
OracleOex-unclAdversaryAαCR,ipiAcando0≤i≤qqueries(p,p0)ifαCR,i∈ACRthenpi←Create(αCR,i)ACR,X0OracleOsel-unclAdversaryAαCR,ipiAcando0≤i≤qqueriesp0ifαCR,i∈ACRthenpi←Create(αCR,i)ACR,PFS,X0(xi,hi)(zi,h0i)ifx∈X0andhi∈H∪{}then(zi,h0i)←PFS(xi,hi)PFS is called (γ, δ, q)-cloning-resistant w.r.t. X (cid:48) ∈ X , if
Pr(cid:2)PFS
(cid:48)
p(cid:48),αPF,αEX
δ,X (cid:48)≡ PFSp,αPF,αEX :
p(cid:48) ← Expex-uncl
(cid:3) ≤ γ
(q);
p(cid:48) ∈ [Create(αCR)];
αCR ∈ ACR
A
(13)
C. Example
As an example, we consider the implementation of an
SRAM PUF as described in Section IV-C. We consider
the case of existential physical unclonability by an honest
manufacturer (ACR = {αCR}).
Experiments by Guajardo et al. [25] show that the relative
amount of differing bits between two responses coming
from distinct SRAM PUFs is on average close to one half.
Independent experiments by Holcomb et al. [26] conﬁrm
that for common SRAM implementations, power-up states
of different instances differ on average by approximately
40%. The exact relative difference depends on the way the
considered SRAM cells are designed (which is speciﬁed
by αCR). It is expected that this probability will always
tend to 50%. As a safe margin, we consider an SRAM
PUF with an average relative difference of 40% between
responses coming from distinct PUFs and a 12% bit error
rate between responses coming from the same PUF. In both
cases the challenge is ﬁxed. It
is reasonable to assume
that the 40% differing bits are independent and uniformly
distributed over all the responses, i.e., there is no particular
bit position in a response that
is more likely to differ
between two PUFs than any other. This is explained by
the random manufacturing processes affecting each SRAM
memory cell independently and is conﬁrmed by extensive
experiments [25]. As discussed in Section IV-C, applying
information reconciliation in Extract can turn this 12%-
noisy SRAM PUF into a PF system with a robustness
PFS > 1 − 10−6. For simplicity, we consider clones
of ρavg
w.r.t. average robustness rather than the individual chal-
lenge robustness. To assess the cloning-resistance of this
SRAM PUF, we calculate the probability that an honest
manufacturer produces two clones “by accident”, relative to
the average robustness and a particular challenge set X (cid:48).
Consider an SRAM PUF that accepts 8-bit challenges
and produces 255-bit responses We assess the physical
unclonability of this PUF w.r.t. a challenge set consisting of
a single challenge, i.e., X (cid:48) = {x}. Therefore, we estimate
the probability of producing the same output to the same
challenge on two independently created PF systems (i.e.,
q = 2). We denote y ← PFp,αPF (x), (z, h) ← Extract(y, )
and y(cid:48) ← PFp(cid:48),αPF (x). The event, where PFSp(x) =
Extract(y, ) and PFSp(cid:48)(x) = Extract(y(cid:48), h) produce the
same output z, only happens when y and y(cid:48) are by accident
similar enough such that the error correcting capability of
Extract corrects y(cid:48) to y.
introduce the following notation: ∆y = y ⊕ y(cid:48)
To calculate the probability of this event, we start by
evaluating the probability of a particular creation event (see
Eq. 12) and determine to what extent this creation event
produces a pair of clones according to Deﬁnition 10. We
ﬁrst
is
the offset between the two expected responses of different
SRAM PUFs solely caused by the random manufacturing
variability affecting the Create process. Moreover, by e we
denote the error vector representing the effect of random
noise occurring in the Eval process of a single SRAM PUF.
py and pe are the respective probabilities of a bit of ∆y
or e being one. Note that in our example py = 40% and
pe = 12%. We also use fbino(t; n, pi) and Fbino(t; n, pi),
respectively, as the probability distribution and the cumu-
lative distribution function of the binomial distribution in
t with parameters n and pi. We start by upper bounding
HW(∆y) with HW(∆y) ≤ 50. This bound determines
the probability of these “clones” being created according
to Eq. 12: Pr[HW(∆y) ≤ 50] = Fbino(50; 255, py) =
2.66 · 10−12, which is taken over the randomness of the
Create process. It is yet to be evaluated to what extent the
two PF systems based on these SRAM PUFs are considered
clones according to Deﬁnition 10. Both PF systems produce
the same output z if both SRAM PUF responses (this
time including bit errors) differ by no more than the error
correcting capability of the Extract algorithm (i.e., 59 bits),
given that the expected difference is 50 bits. The probability
of this event corresponds to the left-hand side of Eq. 11 and
is calculated as
Pr[HW(∆y ⊕ e) ≤ 59 : HW(∆y) ≤ 50]
50(cid:88)
50(cid:88)
i=0
=
=
(cid:18)