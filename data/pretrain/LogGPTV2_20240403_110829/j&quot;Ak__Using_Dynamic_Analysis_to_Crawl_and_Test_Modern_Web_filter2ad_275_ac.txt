### List of URLs and Crawler Actions

The list of URLs represents the frontier of actions that a crawler can take to further explore a web application. Each type of action may yield different results. For instance, requesting a new URL typically retrieves a new page. If this page contains JavaScript, it is executed in a new JavaScript environment. However, events, such as clicks or form submissions, may also generate new URLs, either by executing JavaScript code that sets `window.location` to a new URL or by triggering other behaviors. To manage these, our crawler prioritizes events over URLs. Once all events are processed, the crawler then processes the list of URLs. When both lists are empty, the crawler exits.

### Visiting the Client-side Program

Events like clicks, focus, double-clicks, and mouse movements can be triggered within the JavaScript execution environment. To fire an event \( e \), j¨Ak first identifies the relevant DOM element and uses the `dispatchEvent` function from the DOM Event Interface to trigger the event. The crawler then observes the result of the event handler's execution through dynamic analysis. The event handler can cause a page refresh, load a new page, or send a message to the server. To prevent interference with the server, j¨Ak blocks network communication APIs during event firing using hook functions.

After firing an event, j¨Ak handles the following scenarios:
- If the event handler triggers a network communication API, j¨Ak captures the URL and enqueues it in the list of URLs.
- If the event handler sets a new URL (e.g., `window.location = URL`), j¨Ak adds the URL to the linked-URLs list.
- If the event handler adds new linked URLs or forms, they are inserted into the appropriate lists.
- If the event handler registers new events, j¨Ak creates a special event sequence (e.g., \( \hat{e} = (e, e') \)) and adds it to the list of events. When the crawler schedules this event, it fires the events in the given order.

### Requesting New Pages

To efficiently explore the web application, the crawler should prioritize pages with new content over those with known content. j¨Ak assigns a priority to each URL based on:
1. The frequency of similar URLs seen in the past.
2. The distribution of past URLs across clusters.

The priority is calculated as the number of similar past URLs divided by the number of clusters. If a URL has never been seen before, its priority is set to 2. The crawler processes URLs in descending order of priority.

### Termination Criteria

Without proper controls, the crawler may enter infinite loops. j¨Ak employs two techniques to ensure termination:
1. **Hard Limits**: 
   - **Across Web Pages**: A maximum search depth limits the number of URLs that can be visited. This prevents loops caused by mutual links between pages.
   - **Within Single Pages**: A limit on the maximum depth of events explored within a single page prevents infinite loops due to recursive event handling.
2. **Convergence-based Termination**:
   - The crawler terminates when no new content is discovered. This is determined by the number of similar pages in a cluster. If a cluster reaches a predefined limit, it is marked as full, and subsequent pages are discarded.

### Implementation of j¨Ak

j¨Ak is implemented in Python and uses the WebKit browser engine via the Qt Application Framework bindings. It consists of four modules:
- **Dynamic Analysis Module**: Implements the techniques for analyzing JavaScript execution and event handling.
- **Crawler Module**: Manages the crawling logic, starting from a seed URL and populating the frontiers of URLs and events.
- **Attacker Module**: Prepares and tests URLs against various vulnerabilities.
- **Analysis Module**: Analyzes the traces to determine if a test was successful.

### Evaluation

We evaluated j¨Ak's effectiveness by comparing it with four existing web crawlers: Skipfish 3.10b, W3af 1.6.46, Wget 1.6.13, State-aware crawler, and Crawljax 3.5.1. The evaluation used the WIVET web application, which includes 45 dynamic tests to measure the ability of crawlers to extract URLs from client-side programs.

#### Results
- **j¨Ak**: Passed 89% of the dynamic tests.
- **Other Crawlers**: On average, passed only 25% of the tests, with Wget failing all dynamic tests.

j¨Ak's success in event-based and server communication API tests (classes C4-7 and C8) was particularly notable, achieving 96% and 100% success rates, respectively. The detailed results and specific failures are discussed in the paper.

### Conclusion

j¨Ak demonstrates superior performance in extracting URLs from client-side programs, especially in handling dynamic and event-based scenarios. Its use of dynamic analysis and event handling makes it a robust tool for modern web application testing.