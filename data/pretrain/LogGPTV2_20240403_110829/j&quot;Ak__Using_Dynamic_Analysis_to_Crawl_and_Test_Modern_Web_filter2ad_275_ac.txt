list of URLs. These lists represent the frontier of actions that the crawler can
take to further explore the web application.
Each type of action may have a diﬀerent result. On the one hand, the request
of a new URL certainly causes to retrieve a new page and, if the page contains a
JavaScript program, then it is executed in a new JavaScript environment. This
is not necessarily the case of events. Firing an event may allow the crawler to
explore more behaviors of the JavaScript program, i.e., to generate new URLs.
However, events may also cause to run a new JavaScript program, for instance
by setting window.location to a new URL. However, we can block this behavior
via function hooking. For these reasons, our crawler gives a higher priority to
events with respect to the URLs. When no more events are left in the list, then
we process the list of URLs. When all the lists are empty, then the crawler exits.
Visiting the Client-side Program — Events such as click, focus, double click, and
mouse movements can be ﬁred within the JavaScript execution environment. To
ﬁre an event e, j¨Ak ﬁrst identiﬁes the DOM element and then ﬁres the event via
the DOM Event Interface [5] function dispatchEvent. After that, j¨Ak observes
the result of the execution of the handler via the dynamic analysis. The event
handler can cause a refresh of the page, a new page to be loaded, a message to be
sent to the server side. To avoid any interference with the server side, when ﬁring
events, the hook functions, e.g., for network communication API, will block the
delivery of the message.
After having ﬁred an event, j¨Ak can distinguish the following cases. If the
event handler results into a network communication API, then j¨Ak takes the
URL from the trace, and enqueues it in the list of URLs. Similarly, if the event
handler sets a new URL (i.e., window.location=U RL), then j¨Ak enqueues the
URL into the linked-URLs list. If the event handler adds new linked URL and
forms, then they are inserted into the appropriate list. Finally, if the event han-
dler registers new events, then j¨Ak prepares the special event which comprises
the sequence of events that lead to this point, e.g., ˆe = (cid:2)e, e(cid:3)(cid:3) where e is the last
ﬁred event and e(cid:3) is the newly discovered event. Then, ˆe is added to the list of
events. When the crawler schedules this event to be ﬁred, it ﬁres the events in
the given order, i.e., ﬁrst e and then e(cid:3).
306
G. Pellegrino et al.
Requesting New Pages — The crawler should aim to ﬁnd pages that contain
new content rather than pages with known content. To select the next page, j¨Ak
assigns a priority to each of the URLs in the frontier based on two factors: (i)
how many times j¨Ak has seen a similar URL in the past, and (ii) how scattered
over the clusters past URLs are. The priority is thus calculated as the number
of similar past URLs over the number of clusters in which the past URLs have
been inserted in. If a URL in the frontier was never seen in the past, i.e., the
priority is 0, then we force its priority to 2. The crawler processes URLs from
the highest to the lower priority.
Termination. Without any further control on the behavior of the crawler, the
crawler may enter a loop and never terminate its execution. j¨Ak thus uses two
techniques to terminate its execution. First, it has a hard limit for the search
depth. Second, the crawler terminates if it cannot ﬁnd new content anymore.
We describe the termination criteria in the following.
Hard Limits — Crawlers can enter loops in two situations. First, loops can
happen across the diﬀerent web pages of a web applications. This can be caused
when crawling inﬁnite web applications such as calendars or, for example, when
two pages link to each other. The crawler may visit the ﬁrst page, then schedule
a visit to the second, which again points to the ﬁrst page. These problems can
be solved with a limit on the maximum search depth of the crawler. When the
crawler reaches a limit on the number of URLs, it terminates the execution.
Second, loops may also occur within single web pages. For example, the handler
of an event can insert a new HTML element into the DOM tree and register the
same handler to the new element. Similarly as seen for URLs, one can limit the
maximum depth of events that can be explored within a web page. When the
limit is reached, the crawler will no longer ﬁre events on the same page.
Convergence-based Termination — In addition to these limits, the crawler ter-
minates when the discovered pages do not bring any new content. The notion
of new content is deﬁned in terms of number of similar pages that the crawler
visited in the past. To achieve this, the crawler uses the navigation graph and
a limit on the number of pages per cluster. If the cluster has reached this limit,
the crawler marks the cluster as full and any subsequent page is discarded.
4 Implementation of j¨Ak
This section presents our actual implementation of j¨Ak, our web-application
scanner which implements the crawler and the program analysis presented in
Sect. 3. j¨Ak is written in Python [6] and based on WebKit browser engine [7] via
the Qt Application Framework bindings [8]. We released j¨Ak at https://github.
com/ConstantinT/jAEk/.
j¨Ak comprises four modules: dynamic analysis module, crawler module,
attacker module, and analysis module. The dynamic analysis module imple-
ments the techniques presented in Sect. 3.1. j¨Ak relies on the default JavaScript
j¨Ak: Using Dynamic Analysis to Crawl and Test Modern Web Applications
307
engine of WebKit, i.e., the JavaScriptCore, to perform the dynamic analysis.
Unfortunately, JavaScriptCore sets the event properties as not conﬁgurable. As
a result, JavaScriptCore does not allow to use function hooking via set functions.
To solve this, j¨Ak handles these cases via DOM inspection. However, we veriﬁed
that the JavaScript engines of Google and Mozilla, i.e., V8 [9] and SpiderMon-
key [10], allow one to hook set functions. In the future, we plan to replace the
JavaScriptCore engine with V8.
The crawler module implements the crawling logic of Sect. 3.2. Starting from
a seed URL, j¨Ak retrieves the client-side program and passes it to the dynamic
analysis module. The dynamic analysis module returns traces which are used to
populate the frontiers of URLs and events. Then, j¨Ak selects the next action
and provides it to the dynamic analysis module. Throughout this process, j¨Ak
creates and maintains the navigation graph of the web application which is used
to select the next action. The output of the crawler module is a list of forms and
URLs.
Finally, the attacker and analysis modules test the server side against a
number of vulnerabilities. For each URL, the attacker module prepares URLs
carrying the attack payload. Then, it passes the URL to the dynamic analysis
module to request the URL. The response is then executed within the dynamic
analysis module, which returns an execution trace. The analysis module then
analyzes the trace to decide if the test succeeded.
5 Evaluation
We evaluate the eﬀectiveness of j¨Ak in a comparative analysis including four
existing web crawlers. Our evaluation consists of two parts. Section 5.1 assesses
the capability of the crawlers based on the standard WIVET web applica-
tion, highlighting the need to integrate dynamic analysis to crawlers. Then,
in Sect. 5.2, we evaluate j¨Ak and the other crawlers against 13 popular web
applications.
For our experiments, we selected ﬁve web crawlers: Skipﬁsh 3.10b [11], W3af
1.6.46 [12], Wget 1.6.13 [13], State-aware crawler [14], and Crawljax 3.5.1 [2]. We
selected Skipﬁsh, W3af, and Wget as they were already used in a comparative
analysis against State-aware crawler by prior work (see Doup´e et al. [14]). Then,
we added Crawljax as it is a crawler closest to our approach.
In our experiment, we used the default conﬁguration of these tools. When
needed, we conﬁgured them to submit user credentials or session cookies. In
addition, we conﬁgured the tools to crawl a web application to a maximum
depth of four. Among our tools, only W3af does not support bounded crawling2.
2 W3af implements a mechanism to terminate which is based on the following two
conditions. First, W3af does not crawl twice the same URL and then it does not
crawl “similar” URLs more than ﬁve times. Two URLs are similar if they diﬀer only
from the content of URL parameters.
308
G. Pellegrino et al.
5.1 Assessing the Crawlers’ Limitations
First, we use the Web Input Vector Extractor Teaser (WIVET) web applica-
tion [3] to assess the capabilities of existing crawlers and compare these to j¨Ak.
The WIVET web application is a collection of tests to measure the capability
of crawlers to extract URLs from client-side programs. In each test, WIVET
places unique URLs in a diﬀerent part of the client-side program including in
the HTML and via JavaScript functions. Then, it waits for the crawler to request
the URLs. WIVET tests can be distinguished in static and dynamic tests. A test
is static if the unique URL is placed in the HTML document without the use
of a client-side script. Otherwise, if the client-side program generates, requests,
or uses URLs, then the test is dynamic. WIVET features 11 static tests and 45
dynamic tests. We focus on the dynamic behavior of client-side programs and
thus limit the evaluation to running the 45 dynamic tests.
Table 1. Number and fraction of dynamic test passed by the diﬀerent crawlers
Dynamic test categories Total Crawljax W3af Wget Skipﬁsh
C1 Adobe Flash event
0
C2 URL in tag
5
C3 JS in URL, new loc.
1
C4 URL in tag, tim. evt.
1
C5 Form subm., UI evt.
1
C6 New loc., UI evt.
6
C7 URL in tag, UI evt.
1
C8 XHR
2
Total
17
38
In %
2
5
2
1
2
27
2
4
45
100
0
5
2
0
2
0
0
0
9
20
2
5
1
1
1
6
2
2
20
44
0
0
0
0
0
0
0
0
0
0
j¨Ak
0
4
2
1
1
26
2
4
40
89
As URLs can be placed and used by the JavaScript program in diﬀerent
ways, we manually reviewed WIVET’s dynamic tests and grouped them into
eight classes. We created these classes by enumerating the technique used by
each test. For example, we considered whether a test dynamically places an
URL in an HTML tag, if the URL is for Ajax requests, or whether the action
is in an event handler. Table 1 shows the eight classes and details the results of
each crawler for each class.
As Table 1 shows, all tested crawlers but j¨Ak fail in more than half of the
tests. In average, these tools passed only 25 % of the tests. With the exception
of Wget, which failed all the dynamic tests, the success rate ranges from 20 % of
Crawljax to 44 % of W3af. j¨Ak instead passed 89 % of the tests. For the event-
based tests (i.e., C4-7), W3af, Skipﬁsh and Crawljax succeeded in about 16 %
of the tests, whereas for the server communication API tests (i.e., C8) they
succeeded in 25 % of the tests. By comparison, j¨Ak achieved 96 % and 100 % of
success rate for the classes C4-7 and C8, respectively.
We next discuss the details of these experiments per tool. In total, j¨Ak passed
40 dynamic tests (89 %). With reference to the classes C4-7, j¨Ak discovered the
j¨Ak: Using Dynamic Analysis to Crawl and Test Modern Web Applications
309
registration of the events via the hook functions. Then, it ﬁred the events which
resulted in the submission of the URL. In only one case, j¨Ak could not extract
the URL which is contained in an unattached JavaScript function. As j¨Ak uses
dynamic analysis, it cannot analyze code that is not executed, and thus it will
not discover URLs in unattached functions. Nevertheless, j¨Ak could easily be
extended with pattern-matching rules to capture these URLs. In fact, Skipﬁsh
and W3af were the only ones able to discover these URLs. For the class C8,
j¨Ak discovered the URL of the endpoint via the hook functions and via DOM
tree inspection. In this case, the test requested the URL via the XHR API and
inserted it in the DOM tree.
j¨Ak failed in other four dynamic tests. First, one test of C2 places a
JavaScript statement javascript: as action form. j¨Ak correctly extracts the
URLs, however it does not submit to the server side because j¨Ak does not sub-
mit forms during the crawling phase. Other two tests are in the class C1. This
class contains tests which test the support of ShockWave Flash (SWF) objects.
This feature is not currently supported by j¨Ak. Then, the last test is in C5. This
test submits user data via a click event handler. j¨Ak correctly detects the event
registration and it ﬁres the event on the container of the input elements. How-
ever, the handler expects that the event is ﬁred over the submit button element
instead of the container. This causes the handler to access a variable with an
unexpected value. As a result, the handler raises an exception and the execution
is halted. In a web browser, the execution of the handler would have succeeded
as a result of the propagation of the click event from the button to the outer
element, i.e., the container3. The current version of j¨Ak does not support the
propagation of events, instead it ﬁres an event on the element where the handler
has been detected, in this case the container.
Crawljax succeeded only in 9 out 45 tests (20 %). Most of the failed tests
store URLs either in the location property of the window object (i.e., classes C4
and C6), or as URL of a linked resource (i.e., class C7). The URL is created and
inserted in the DOM tree upon ﬁring an event. While Crawljax can ﬁre events, it
supports only a limited set of target HTML tags to ﬁre events, i.e., buttons and
links. Finally, Crawljax failed in all of the dynamic tests involving Ajax requests
(See C8).
Skipﬁsh and W3af performed better than Crawljax, passing 38 % and 44 %
of the dynamic tests, respectively. These tools extract URLs via HTML docu-
ment parsing and pattern matching via regular expression. When a URL cannot
be extracted from the HTML document, the tools use pattern recognition via
regular expressions. This technique may work well when URLs maintain distin-
guishable characteristics such as the URL scheme, e.g., http://, or the URL
path separator, i.e., the character “/”. However, this approach is not suﬃciently
generic and cannot extract URLs that are composed dynamically via string