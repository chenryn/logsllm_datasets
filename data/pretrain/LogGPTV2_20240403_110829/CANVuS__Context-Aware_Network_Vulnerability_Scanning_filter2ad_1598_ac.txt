Table 2 lists the number of scans conducted by CANVuS with a break down by event
types and the total for the loop scanner. The loop scanner has an order of magnitude
more scans than CANVuS because only about 20% of the IP addresses in the target
network are known to be allocated, and at any instant of time, the number of available
hosts are even less than that. However, the loop scanner has to exhaust the entire IP
blocks unless the address allocation and host availability information can be statically
encoded, which is rarely the case in enterprise networks [10].
CANVuS: Context-Aware Network Vulnerability Scanning
149
Table 2. The numbers of scans conducted by CANVuS and the loop scanner
Total
ARP
DHCP
TCP
DNS
CANVuS Loop Scanner
534,717
1.55%
17.37%
38.33%
10.28%
App. Protocol 0.03%
32.44%
Timeout
4,355,624
N/A
)
s
(
y
c
n
e
t
a
L
110,000
100,000
90,000
80,000
70,000
60,000
50,000
40,000
30,000
20,000
10,000
0
Loop
CANVuS
0
1
2
3
5
4
6
Sampling Rate
7
8
9
10
s
n
a
c
S
f
o
#
4,500,000
4,000,000
3,500,000
3,000,000
2,500,000
2,000,000
1,500,000
1,000,000
500,000
0
Loop
CANVuS
0
1
2
3
5
4
6
Sampling Rate
7
8
9
10
Fig. 2. A comparison of the detection latency
with sampled results for the loop scanner
Fig. 3. A comparison of the number of
scans with sampled results for the loop
scanner
In addition, the average detection latencies for changes discovered by CANVuS is
22,868.751 seconds versus 26,124.391 seconds for the loop scanner. Please recall that
our assumption says the evidence of conﬁguration changes will trigger scans instan-
taneously. However, the latency for CANVuS here is far from zero. This anomaly is
caused by the fact that we used the combined scanning results to approximate the
ground truth and timeout-based scanning was still applied in some situations when no
network network changes occurred.
Moreover, the latency for CANVuS is not signiﬁcantly better than that of the loop
scanner. This is because the conﬁguration for the loop scanner is already very aggres-
sive and represents the best performance that traditional periodic scanning systems can
achieve in detection latency. To make the loop scans less aggressive and to demonstrate
the tradeoff in scanning costs, the data set is sampled with a rate from 1 to 10 to in-
clude both the original case and the case that both scanners have a comparable number
of scans. Figure 2 illustrates the result. The curve for the loop scanner goes up almost
linearly because of the linear sampling, while the curve for CANVuS goes slightly up
and down because the approximated ground truth has been changed after sampling. In
addition, Figure 3 shows the corresponding changes for the number of scans.
6.3 Timeout-Based Scanning In CANVuS
As discussed previously, a timeout-based scanning approach is used along with the
trigger-based scanning as many conﬁguration changes are not observable through
150
Y. Xu et al.
1
0.8
y
t
i
l
i
b
a
b
o
r
P
0.6
0.4
0.2
0
10000
100000
1x106
Change intervals in log scale (seconds)
Fig. 4. The CDF for the intervals of detected changes
network events. However, unlike traditional periodic scanning, which randomly picks
scanning targets in a large pool with ﬁxed cycles, the timeout mechanism in CANVuS
is still context-aware. Speciﬁcally, the system uses the context data to infer IP alloca-
tion information and host availability patterns so that only the hosts that are believed
to be connected to the network will be scanned. These timeouts are assigned per-host
timer and are based on the network state, scanning history, and administrators’ policy
decisions. As a result of these approach, fewer scans are “wasted” on hosts that don’t
exists or are unavailable. Taken another way, given the same number of scans, CANVuS
is more likely to examine a larger number of active hosts and detect host changes with
lower latency than the periodic scanning system.
Figure 4 depicts the distribution of intervals between all detected changes the val-
ues ranging between hours and days. The bias demonstrated in the middle of the graph
is the result of our experimental methodology and the choice of 12 hours for our ac-
tive/inactive timeout.
Thus, in practice, we determining the timeout value based on operators’ administra-
tive requirements. For example, using a timeout value at the order of a week, which
covers the changes on the tail of the curve in Figure 4, would be reasonable. An al-
ternative strategy is to halve the timeout value if a change is detected accordingly, but
otherwise double it adaptively modifying a host’s timeout value. In either case, there is
clearly a tradeoff between the number of scans and the detection latency, which is also
the case for normal periodic scanning.
6.4 Exploring the Impact of Various Data Sources and Triggers
In this subsection, we study the relationship between triggering events and the captured
conﬁguration changes. Speciﬁcally, the study is focused on their temporal correlations
instead of causalities, which requires detailed control over the target hosts.
To do this, two problems need to be addressed. First, since most subnets in the tar-
get network use DHCP to assign IP addresses (despite whether the address mapping
is dynamic or static), the changes witnessed may actually be mapping changes instead
CANVuS: Context-Aware Network Vulnerability Scanning
151
Table 3. Permanent changes categorization
Triggering Event Count With network evidence
ARP
DHCP
TCP
DNS
Timeout
1
15
2
3
4
1
10
2
1
0
of changes in conﬁguration. To eliminate the negative impact of dynamic address as-
signment, which would obscure the analysis results, the discussion in this subsection is
conﬁned to the 535 hosts that were assigned exactly one unique IP address during the
experiment period. They are extracted from the 1,691 scanned IP addresses in the nine
/24 departmental network subnets for which we have complete DHCP message logs.
Among these 535 hosts, 1,985 changes were detected by CANVuS during the ex-
periment. After merging with the results from the loop scanner, the number of detected
changes becomes 2,145, where the increase is an artifact of the method used to generate
the approximated ground truth. However, many changes are considered to be ephemeral
or short-lived, which is the second problem that must be handled. In other words, certain
ports appear for a short while and then disappear without exhibiting real conﬁguration
changes. Many client programs may exhibit this behavior. For example, the client side
of Apple’s iTunes uses port 5353 to communicate with each other for sharing. P2P
download software is another example. This type of changes provide little value in re-
vealing the temporal correlations between changes and triggering events, due to their
short lifetime.
As a result, we only consider those long-term or permanent changes to study their
temporal correlations with triggering events. However, the word ‘permanent’ is not well
deﬁned, given the limited experiment period. Thus, for the simplicity of analysis, we
only examine hosts that had exactly one change during the entire 16-day period because
these changes are most likely to be permanent unless they were detected at the very end
of the experiment. Among the 535 hosts, 25 of them fall into this category. Though this
is not statistically signiﬁcant, they still provide important clues for us to ﬁnd appropriate
policy setting for the target network. Recall that our conjecture is that most permanent
changes have network evidence that can be witnessed and used for creating triggers to
achieve timely detection. By manually going through all these changes and analyzing all
logged events that happened around the time when the changes were detected, we ﬁnd
56% of them have network evidence that has strong temporal correlation with changes,
which means they either have triggered or could have triggered scans to detect the
changes.
Table 3 is a summary of the analysis results in detail. Several things should be noted
here. First, all the permanent changes we studied that were captured by timeout exhib-
ited no network evidence at all. This is a limitation of our system using pure network
events. Unless some level of host information is monitored, this timeout-based method
cannot be completely replaced with the trigger-based approach. In addition, a signiﬁ-
cant portion of changes were detected via DHCP and ARP events, which corresponded
152
Y. Xu et al.
to hosts reboots. This is reasonable because many conﬁguration changes may not take
effect until the host is restarted. Finally, the rest of the changes corresponded to activi-
ties on the service or process level.
Moreover, we argue that although ephemeral changes may not be helpful in studying
the temporal correlations, they are still relevant to risk assessment. Despite the possi-
bility of being exploited by sophisticated attackers, many short-lived but well-known
ports may be used to tunnel malicious trafﬁc for cutting through ﬁrewalls. For exam-
ple, the TCP event monitor captured some occasional events through port 80 for certain
hosts, while the application event monitor failed to ﬁngerprint any version informa-
tion for them, which means the trafﬁc did not follow the HTTP protocol. Thus, in both
cases, it is valuable to detect these short-lived changes, but there is no guarantee for the
loop scanner to achieve this goal. In fact, the loop scanner tends to miss most of these
changes once its scanning period greatly increases (e.g., in the order of weeks). With
the help of TCP events, CANVuS can ﬁre scans immediately after there is trafﬁc going
through these ports. And there are 35 changes exclusively captured by CANVuS that
fall in this category. Conversely, if there is no trafﬁc ever going through the short-lived
ports, while CANVuS may also miss them, the resulting risk is much lower because
attackers have no chance to leverage them either.
6.5 Scalability Requirements of the Context-Aware Architecture
Figures 5 and 6 constitute a summary for the scale of the data from the departmen-
tal monitors done in hour intervals. The number of raw packets or ﬂows per hour is
counted in Figure 5. This raw data was observed at the various network monitors and
probes before being converted by the Context Manager into the network state database.
We note that the ﬁrst three days worth of data are missing in our graph due to the mis-
conﬁguration of the monitoring infrastructure. For the duration of our experiment, we
observed ﬂows on the order of 16 million per hour at its peak and on average around 4
million per hour. Other noticeable observations include a typical average of 1 million
DNS packets per hour and about 12 thousand ARP packets per hour. These four graphs
in Figure 5 show that this system must handle adequately large volumes of trafﬁc due
to its distributed nature.
Figures 6 shows the number of events per hour that triggered scans after being con-
verted by the Context Manager. Compared to the graphs in Figure 5, the volume of
events generated from the raw data was greatly reduced. To highlight the number of
ﬂows and DNS packets went from the order of millions to the low hundreds. ARP
packets when from the order of thousands to tens. This shows that our Context Man-
ager is able to greatly reduce large volumes of data to something manageable for our
event-based vulnerability scanning.
In addition, the cumulative number of unique MAC addresses in the departmental
network is shown in Figure 7, which quantiﬁes the scale of the physical boxes within
the department (only for the second measurement point) that should be audited. We ob-
served that slight bumps indicate new observances of MAC addresses over the course
of a day while plateaus occurred over the weekends. We speculate that the observance
of new, unique MAC addresses will level off if given a longer period of time to run
our experiments. This graph also gives insight in bounding to the amount of raw and
CANVuS: Context-Aware Network Vulnerability Scanning
153
1.6e+07
12.e+07
8e+06
4e+06
0
4e+06
3e+06
2e+06
1e+06
0
35,000
30,000
25,000
20,000
15,000
10,000
5000
0
s
w
o
ﬂ
f
o
#
s
t
e
k
c
a
p
S
N
D
f
o
#
s
t
e
k
c
a
p
P