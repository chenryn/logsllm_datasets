available to the guest machine whenever it is scheduled to run. As a
result, Hyper-V schedules the VP to run only if that minimum amount
of CPU capacity is available (meaning that the allocated time slice is
guaranteed).
■    VP limits Similar to VP reservations, a user can limit the percentage
of physical CPU usage for a VP. This means reducing the available
time slice allocated to a VP in a high workload scenario.
■    VP weight This controls the probability that a VP is scheduled when
the reservations have already been met. In default configurations,
each VP has an equal probability of being executed. When the user
configures weight on the VPs that belong to a virtual machine,
scheduling decisions become based on the relative weighting factor
the user has chosen. For example, let’s assume that a system with four
CPUs runs three virtual machines at the same time. The first VM has
set a weighting factor of 100, the second 200, and the third 300.
Assuming that all the system’s physical processors are allocated to a
uniform number of VPs, the probability of a VP in the first VM to be
dispatched is 17%, of a VP in the second VM is 33%, and of a VP in
the third one is 50%.
Figure 9-12 The classic scheduler fine-tuning settings property page,
which is available only when the classic scheduler is enabled.
The core scheduler
Normally, a classic CPU’s core has a single execution pipeline in which
streams of instructions are executed one after each other. An instruction
enters the pipe, proceeds through several stages of execution (load data,
compute, store data, for example), and is retired from the pipe. Different
types of instructions use different parts of the CPU core. A modern CPU’s
core is often able to execute in an out-of-order way multiple sequential
instructions in the stream (in respect to the order in which they entered the
pipeline). Modern CPUs, which support out-of-order execution, often
implement what is called symmetric multithreading (SMT): a CPU’s core has
two execution pipelines and presents more than one logical processor to the
system; thus, two different instruction streams can be executed side by side
by a single shared execution engine. (The resources of the core, like its
caches, are shared.) The two execution pipelines are exposed to the software
as single independent processors (CPUs). From now on, with the term logical
processor (or simply LP), we will refer to an execution pipeline of an SMT
core exposed to Windows as an independent CPU. (SMT is discussed in
Chapters 2 and 4 of Part 1.)
This hardware implementation has led to many security problems: one
instruction executed by a shared logical CPU can interfere and affect the
instruction executed by the other sibling LP. Furthermore, the physical core’s
cache memory is shared; an LP can alter the content of the cache. The other
sibling CPU can potentially probe the data located in the cache by measuring
the time employed by the processor to access the memory addressed by the
same cache line, thus revealing “secret data” accessed by the other logical
processor (as described in the “Hardware side-channel vulnerabilities”
section of Chapter 8). The classic scheduler can normally select two threads
belonging to different VMs to be executed by two LPs in the same processor
core. This is clearly not acceptable because in this context, the first virtual
machine could potentially read data belonging to the other one.
To overcome this problem, and to be able to run SMT-enabled VMs with
predictable performance, Windows Server 2016 has introduced the core
scheduler. The core scheduler leverages the properties of SMT to provide
isolation and a strong security boundary for guest VPs. When the core
scheduler is enabled, Hyper-V schedules virtual cores onto physical cores.
Furthermore, it ensures that VPs belonging to different VMs are never
scheduled on sibling SMT threads of a physical core. The core scheduler
enables the virtual machine for making use of SMT. The VPs exposed to a
VM can be part of an SMT set. The OS and applications running in the guest
virtual machine can use SMT behavior and programming interfaces (APIs) to
control and distribute work across SMT threads, just as they would when run
nonvirtualized.
Figure 9-13 shows an example of an SMT system with four logical
processors distributed in two CPU cores. In the figure, three VMs are
running. The first and second VMs have four VPs in two groups of two,
whereas the third one has only one assigned VP. The groups of VPs in the
VMs are labelled A through E. Individual VPs in a group that are idle (have
no code to execute) are filled with a darker color.
Figure 9-13 A sample SMT system with two processors’ cores and three
VMs running.
Each core has a run list containing groups of VPs that are ready to execute,
and a deferred list of groups of VPs that are ready to run but have not been
added to the core’s run list yet. The groups of VPs execute on the physical
cores. If all VPs in a group become idle, then the VP group is descheduled
and does not appear on any run list. (In Figure 9-13, this is the situation for
VP group D.) The only VP of the group E has recently left the idle state. The
VP has been assigned to the CPU core 2. In the figure, a dummy sibling VP
is shown. This is because the LP of core 2 never schedules any other VP
while its sibling LP of its core is executing a VP belonging to the VM 3. In
the same way, no other VPs are scheduled on a physical core if one VP in the
LP group became idle but the other is still executing (such as for group A, for
example). Each core executes the VP group that is at the head of its run list.
If there are no VP groups to execute, the core becomes idle and waits for a
VP group to be deposited onto its deferred run list. When this occurs, the
core wakes up from idle and empties its deferred run list, placing the contents
onto its run list.
The core scheduler is implemented by different components (see Figure 9-
14) that provide strict layering between each other. The heart of the core
scheduler is the scheduling unit, which represents a virtual core or group of
SMT VPs. (For non-SMT VMs, it represents a single VP.) Depending on the
VM’s type, the scheduling unit has either one or two threads bound to it. The
hypervisor’s process owns a list of scheduling units, which own threads
backing up to VPs belonging to the VM. The scheduling unit is the single
unit of scheduling for the core scheduler to which scheduling settings—such
as reservation, weight, and cap—are applied during runtime. A scheduling
unit stays active for the duration of a time slice, can be blocked and
unblocked, and can migrate between different physical processor cores. An
important concept is that the scheduling unit is analogous to a thread in the
classic scheduler, but it doesn’t have a stack or VP context in which to run.
It’s one of the threads bound to a scheduling unit that runs on a physical
processor core. The thread gang scheduler is the arbiter for each scheduling
unit. It’s the entity that decides which thread from the active scheduling unit
gets run by which LP from the physical processor core. It enforces thread
affinities, applies thread scheduling policies, and updates the related counters
for each thread.
Figure 9-14 The components of the core scheduler.
Each LP of the physical processor’s core has an instance of a logical
processor dispatcher associated with it. The logical processor dispatcher is
responsible for switching threads, maintaining timers, and flushing the
VMCS (or VMCB, depending on the architecture) for the current thread.
Logical processor dispatchers are owned by the core dispatcher, which
represents a physical single processor core and owns exactly two SMT LPs.
The core dispatcher manages the current (active) scheduling unit. The unit
scheduler, which is bound to its own core dispatcher, decides which
scheduling unit needs to run next on the physical processor core the unit
scheduler belongs to. The last important component of the core scheduler is
the scheduler manager, which owns all the unit schedulers in the system and
has a global view of all their states. It provides load balancing and ideal core
assignment services to the unit scheduler.
The root scheduler
The root scheduler (also known as integrated scheduler) was introduced in
Windows 10 April 2018 Update (RS4) with the goal to allow the root
partition to schedule virtual processors (VPs) belonging to guest partitions.
The root scheduler was designed with the goal to support lightweight
containers used by Windows Defender Application Guard. Those types of
containers (internally called Barcelona or Krypton containers) must be
managed by the root partition and should consume a small amount of
memory and hard-disk space. (Deeply describing Krypton containers is
outside the scope of this book. You can find an introduction of server
containers in Part 1, Chapter 3, “Processes and jobs”). In addition, the root
OS scheduler can readily gather metrics about workload CPU utilization
inside the container and use this data as input to the same scheduling policy
applicable to all other workloads in the system.
The NT scheduler in the root partition’s OS instance manages all aspects
of scheduling work to system LPs. To achieve that, the integrated scheduler’s
root component inside the VID driver creates a VP-dispatch thread inside of
the root partition (in the context of the new VMMEM process) for each guest
VP. (VA-backed VMs are discussed later in this chapter.) The NT scheduler
in the root partition schedules VP-dispatch threads as regular threads subject
to additional VM/VP-specific scheduling policies and enlightenments. Each
VP-dispatch thread runs a VP-dispatch loop until the VID driver terminates
the corresponding VP.
The VP-dispatch thread is created by the VID driver after the VM Worker
Process (VMWP), which is covered in the “Virtualization stack” section later
in this chapter, has requested the partition and VPs creation through the
SETUP_PARTITION IOCTL. The VID driver communicates with the
WinHvr driver, which in turn initializes the hypervisor’s guest partition
creation (through the HvCreatePartition hypercall). In case the created
partition represents a VA-backed VM, or in case the system has the root
scheduler active, the VID driver calls into the NT kernel (through a kernel
extension) with the goal to create the VMMEM minimal process associated
with the new guest partition. The VID driver also creates a VP-dispatch
thread for each VP belonging to the partition. The VP-dispatch thread
executes in the context of the VMMEM process in kernel mode (no user
mode code exists in VMMEM) and is implemented in the VID driver (and
WinHvr). As shown in Figure 9-15, each VP-dispatch thread runs a VP-
dispatch loop until the VID terminates the corresponding VP or an intercept
is generated from the guest partition.
Figure 9-15 The root scheduler’s VP-dispatch thread and the associated
VMWP worker thread that processes the hypervisor’s messages.
While in the VP-dispatch loop, the VP-dispatch thread is responsible for
the following:
1. 
Call the hypervisor’s new HvDispatchVp hypercall interface to
dispatch the VP on the current processor. On each HvDispatchVp
hypercall, the hypervisor tries to switch context from the current root
VP to the specified guest VP and let it run the guest code. One of the
most important characteristics of this hypercall is that the code that
emits it should run at PASSIVE_LEVEL IRQL. The hypervisor lets the
guest VP run until either the VP blocks voluntarily, the VP generates
an intercept for the root, or there is an interrupt targeting the root VP.
Clock interrupts are still processed by the root partitions. When the
guest VP exhausts its allocated time slice, the VP-backing thread is
preempted by the NT scheduler. On any of the three events, the
hypervisor switches back to the root VP and completes the
HvDispatchVp hypercall. It then returns to the root partition.
2. 
Block on the VP-dispatch event if the corresponding VP in the
hypervisor is blocked. Anytime the guest VP is blocked voluntarily,
the VP-dispatch thread blocks itself on a VP-dispatch event until the
hypervisor unblocks the corresponding guest VP and notifies the VID
driver. The VID driver signals the VP-dispatch event, and the NT
scheduler unblocks the VP-dispatch thread that can make another
HvDispatchVp hypercall.
3. 
Process all intercepts reported by the hypervisor on return from the
dispatch hypercall. If the guest VP generates an intercept for the root,
the VP-dispatch thread processes the intercept request on return from
the HvDispatchVp hypercall and makes another HvDispatchVp
request after the VID completes processing of the intercept. Each
intercept is managed differently. If the intercept requires processing
from the user mode VMWP process, the WinHvr driver exits the loop
and returns to the VID, which signals an event for the backed VMWP
thread and waits for the intercept message to be processed by the
VMWP process before restarting the loop.
To properly deliver signals to VP-dispatch threads from the hypervisor to
the root, the integrated scheduler provides a scheduler message exchange
mechanism. The hypervisor sends scheduler messages to the root partition
via a shared page. When a new message is ready for delivery, the hypervisor
injects a SINT interrupt into the root, and the root delivers it to the
corresponding ISR handler in the WinHvr driver, which routes the message
to the VID intercept callback (VidInterceptIsrCallback). The intercept
callback tries to handle the intercept message directly from the VID driver. In
case the direct handling is not possible, a synchronization event is signaled,
which allows the dispatch loop to exit and allows one of the VmWp worker
threads to dispatch the intercept in user mode.
Context switches when the root scheduler is enabled are more expensive
compared to other hypervisor scheduler implementations. When the system
switches between two guest VPs, for example, it always needs to generate
two exits to the root partitions. The integrated scheduler treats hypervisor’s
root VP threads and guest VP threads very differently (they are internally
represented by the same TH_THREAD data structure, though):
■    Only the root VP thread can enqueue a guest VP thread to its physical
processor. The root VP thread has priority over any guest VP that is
running or being dispatched. If the root VP is not blocked, the
integrated scheduler tries its best to switch the context to the root VP
thread as soon as possible.
■    A guest VP thread has two sets of states: thread internal states and
thread root states. The thread root states reflect the states of the VP-
dispatch thread that the hypervisor communicates to the root partition.
The integrated scheduler maintains those states for each guest VP
thread to know when to send a wake-up signal for the corresponding
VP-dispatch thread to the root.
Only the root VP can initiate a dispatch of a guest VP for its processor. It
can do that either because of HvDispatchVp hypercalls (in this situation, we
say that the hypervisor is processing “external work”), or because of any
other hypercall that requires sending a synchronous request to the target
guest VP (this is what is defined as “internal work”). If the guest VP last ran
on the current physical processor, the scheduler can dispatch the guest VP
thread right away. Otherwise, the scheduler needs to send a flush request to
the processor on which the guest VP last ran and wait for the remote
processor to flush the VP context. The latter case is defined as “migration”
and is a situation that the hypervisor needs to track (through the thread
internal states and root states, which are not described here).
EXPERIMENT: Playing with the root scheduler
The NT scheduler decides when to select and run a virtual
processor belonging to a VM and for how long. This experiment
demonstrates what we have discussed previously: All the VP
dispatch threads execute in the context of the VMMEM process,
created by the VID driver. For the experiment, you need a
workstation with at least Windows 10 April 2018 update (RS4)
installed, along with the Hyper-V role enabled and a VM with any
operating system installed ready for use. The procedure for creating
a VM is explained in detail here: https://docs.microsoft.com/en-
us/virtualization/hyper-v-on-windows/quick-start/quick-create-
virtual-machine.
First, you should verify that the root scheduler is enabled.
Details on the procedure are available in the “Controlling the
hypervisor’s scheduler type” experiment earlier in this chapter. The
VM used for testing should be powered down.
Open the Task Manager by right-clicking on the task bar and
selecting Task Manager, click the Details sheet, and verify how
many VMMEM processes are currently active. In case no VMs are
running, there should be none of them; in case the Windows
Defender Application Guard (WDAG) role is installed, there could
be an existing VMMEM process instance, which hosts the
preloaded WDAG container. (This kind of VM is described later in
the “VA-backed virtual machines” section.) In case a VMMEM
process instance exists, you should take note of its process ID
(PID).
Open the Hyper-V Manager by typing Hyper-V Manager in the
Cortana search box and start your virtual machine. After the VM
has been started and the guest operating system has successfully
booted, switch back to the Task Manager and search for a new
VMMEM process. If you click the new VMMEM process and
expand the User Name column, you can see that the process has
been associated with a token owned by a user named as the VM’s
GUID. You can obtain your VM’s GUID by executing the
following command in an administrative PowerShell window
(replace the term “” with the name of your VM):
Click here to view code image
Get-VM -VmName "" | ft VMName, VmId
The VM ID and the VMMEM process’s user name should be the
same, as shown in the following figure.
Install Process Explorer (by downloading it from
https://docs.microsoft.com/en-us/sysinternals/downloads/process-
explorer), and run it as administrator. Search the PID of the correct
VMMEM process identified in the previous step (27312 in the
example), right-click it, and select Suspend”. The CPU tab of the
VMMEM process should now show “Suspended” instead of the
correct CPU time.
If you switch back to the VM, you will find that it is
unresponsive and completely stuck. This is because you have
suspended the process hosting the dispatch threads of all the virtual
processors belonging to the VM. This prevented the NT kernel
from scheduling those threads, which won’t allow the WinHvr
driver to emit the needed HvDispatchVp hypercall used to resume
the VP execution.
If you right-click the suspended VMMEM and select Resume,
your VM resumes its execution and continues to run correctly.
Hypercalls and the hypervisor TLFS
Hypercalls provide a mechanism to the operating system running in the root
or the in the child partition to request services from the hypervisor.
Hypercalls have a well-defined set of input and output parameters. The
hypervisor Top Level Functional Specification (TLFS) is available online
(https://docs.microsoft.com/en-us/virtualization/hyper-v-on-
windows/reference/tlfs); it defines the different calling conventions used
while specifying those parameters. Furthermore, it lists all the publicly
available hypervisor features, partition’s properties, hypervisor, and VSM
interfaces.
Hypercalls are available because of a platform-dependent opcode
(VMCALL for Intel systems, VMMCALL for AMD, HVC for ARM64)
which, when invoked, always cause a VM_EXIT into the hypervisor.
VM_EXITs are events that cause the hypervisor to restart to execute its own
code in the hypervisor privilege level, which is higher than any other
software running in the system (except for firmware’s SMM context), while
the VP is suspended. VM_EXIT events can be generated from various
reasons. In the platform-specific VMCS (or VMCB) opaque data structure
the hardware maintains an index that specifies the exit reason for the
VM_EXIT. The hypervisor gets the index, and, in case of an exit caused by a
hypercall, reads the hypercall input value specified by the caller (generally
from a CPU’s general-purpose register—RCX in the case of 64-bit Intel and
AMD systems). The hypercall input value (see Figure 9-16) is a 64-bit value
that specifies the hypercall code, its properties, and the calling convention
used for the hypercall. Three kinds of calling conventions are available:
■    Standard hypercalls Store the input and output parameters on 8-byte
aligned guest physical addresses (GPAs). The OS passes the two
addresses via general-purposes registers (RDX and R8 on Intel and
AMD 64-bit systems).
■    Fast hypercalls Usually don’t allow output parameters and employ
the two general-purpose registers used in standard hypercalls to pass
only input parameters to the hypervisor (up to 16 bytes in size).
■    Extended fast hypercalls (or XMM fast hypercalls) Similar to fast
hypercalls, but these use an additional six floating-point registers to
allow the caller to pass input parameters up to 112 bytes in size.
Figure 9-16 The hypercall input value (from the hypervisor TLFS).
There are two classes of hypercalls: simple and rep (which stands for
“repeat”). A simple hypercall performs a single operation and has a fixed-size
set of input and output parameters. A rep hypercall acts like a series of
simple hypercalls. When a caller initially invokes a rep hypercall, it specifies
a rep count that indicates the number of elements in the input or output
parameter list. Callers also specify a rep start index that indicates the next
input or output element that should be consumed.
All hypercalls return another 64-bit value called hypercall result value (see
Figure 9-17). Generally, the result value describes the operation’s outcome,
and, for rep hypercalls, the total number of completed repetition.
Figure 9-17 The hypercall result value (from the hypervisor TLFS).
Hypercalls could take some time to be completed. Keeping a physical CPU
that doesn‘t receive interrupts can be dangerous for the host OS. For
example, Windows has a mechanism that detects whether a CPU has not
received its clock tick interrupt for a period of time longer than 16
milliseconds. If this condition is detected, the system is suddenly stopped
with a BSOD. The hypervisor therefore relies on a hypercall continuation
mechanism for some hypercalls, including all rep hypercall forms. If a
hypercall isn’t able to complete within the prescribed time limit (usually 50
microseconds), control is returned back to the caller (through an operation
called VM_ENTRY), but the instruction pointer is not advanced past the
instruction that invoked the hypercall. This allows pending interrupts to be
handled and other virtual processors to be scheduled. When the original
calling thread resumes execution, it will re-execute the hypercall instruction
and make forward progress toward completing the operation.
A driver usually never emits a hypercall directly through the platform-
dependent opcode. Instead, it uses services exposed by the Windows
hypervisor interface driver, which is available in two different versions:
■    WinHvr.sys Loaded at system startup if the OS is running in the root
partition and exposes hypercalls available in both the root and child
partition.
■    WinHv.sys Loaded only when the OS is running in a child partition.
It exposes hypercalls available in the child partition only.
Routines and data structures exported by the Windows hypervisor
interface driver are extensively used by the virtualization stack, especially by