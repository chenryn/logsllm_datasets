top and all sides) in order to block any a audio or wireless signal com-
munication (i.e., internet) with the smart speaker. Clearly, this holds
the same level of protection as simply powering off/unplugging the
smart speaker device (e.g., cause complete DoS). But it still speaks
to the growing interest surrounding these type of devices.
Aside from smart speaker eavesdropping defenses, speech mask-
ing techniques for protecting live human speech have been explored
that still reveal new knowledge about the potential of speech mask-
ing/jamming. In a work by Phunruangsakao et al. [37], the authors
develop a scheme to ensure speech privacy by limiting the Speech
Transmission Index (STI). Instead of measuring the Room Impulse
Response (RIR), the authors estimate the STI and feed it to an RIR
model. Kim et al. [32] explores the potential of Active Noise Can-
cellation (ANC) for increasing speech privacy. Their work demon-
strates that ANC in a specific direction can decrease the need for a
masking signal, and the same masking affect can be achieved with
a lower masking signal volume (5 dB lower). The authors evaluate
speech intelligibility in their analysis using Speech Intelligibility
Index (SII) and Speech Reception Threshold (SRT). Lastly, research
conducted by Krasnov et al. [33] looks to mask an original speech
signal (with no user disturbance) by targeting key components of
the speech that carry information required for recognition. Both
amplitude and temporal smearing techniques are used to generate
a modified masking noise to negate some effects of reverberation
and increase speech privacy. Our work uniquely builds on these
existing works by performing the first evaluation of the presented
mechanisms, using a standard white noise jamming signal, against
a simple, but realistic eavesdropping attack.
3 THREAT MODEL
For our threat model we consider a user environment that contains
a VCS smart speaker device equipped with a PJD. This could be a
personal device placed in a user’s home, or a work tool used in their
place of business. In these scenarios, the smart speakers are exposed
to sensitive speech that the users need to keep confidential. The
attacker in our threat model seeks to compromise or eavesdrop a
victim user’s speech and are able to acquire (noisy) recordings from
the victim’s smart speaker device. For example, an employee of the
device’s shipping company could implant a component or inject
malicious code in the firmware that would allow them to acquire
user speech recordings. They can perform standard post-processing
techniques on the victim’s speech recordings including noise cancel-
lation and speech enhancement. Lastly, the attacker possesses the
machine learning knowledge needed to build a speech recognition
(or other related speech task) model. Specifically, the attacker in
our threat model looks to achieve speech (digit) recognition, speaker
identification, gender identification, and song recognition. Each of
these speech tasks can reveal sensitive information about a user
that an attacker can abuse or even sell to companies for things like
targeted advertising. Speech recognition can reveal actual speech
content, while the other tasks like song recognition may reveal a
user’s personal and private interests. Figure 2 depicts a model an
attacker may use, and we recreate experimentally in this study, to
eavesdrop on speech via compromised smart speaker recordings.
We consider digit recognition because it represents the potential
for PIN/password/account# leakage which would be very devastat-
ing if acquired by a malicious attacker. If we consider the scenario
Smart speaker equipped with a Personal Jamming Device (PJD)RECOVER AUDIOAdmin Access to Device Data and FunctionsFirmware Hack via Malicious CodeImplanted Recording ComponentUnauthorized Access to User’s Voice Command HistoryPOST-PROCESSINGMFCC FEATURE EXTRACTIONCLASSIFICATION MODELSManual TrimmingNoise Reduction/Speech Enhancement(VOICEBOX, specsubmmse)mean, min, max, stddev, delta, deltaDeltaFull Set: 144 featuresFiltered: 17 featuresSpeech (“Digit”) RecognitionSpeakerIdentificationGenderIdentificationSongRecognition417Evaluating the Effectiveness of PJDs in Mitigating Smart Speaker Eavesdropping Attacks Using GWN
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
of making a purchase over the phone, which is becoming increas-
ingly more common during this pandemic as people are encouraged
to quarantine at home, we can see an instance where a user may
vocalize sensitive numerical information (i.e., credit card number
and security code) near their smart speaker. Additionally, speaker
identification could reveal to an attacker how many people live
in a home, as well as when particular people are using the smart
speaker and what they use it for. This information could be sold to
companies that will use it for targeted advertising, or for something
even more malicious such as a home robbery. Similarly, Gender
identification may also be used for targeted advertisements, or to
help the eavesdropper understand who lives in the home.
In a real-world scenario, there are a few potential attackers that
can attempt eavesdropping in this way. Mainly, the companies pro-
viding these smart speaker devices, and their employees, are in
a unique position to perform such an attack. The major compa-
nies like Amazon and Google could easily program these devices to
record and transmit user audio to company servers, or install a func-
tion to allow full access and control of the microphones equipped
on the device by an administrator at the company. Although the
companies claim this does not occur, many people are still highly
suspicious of this possibility [5, 28]. Aside from the companies that
sell the devices, this threat model is also applicable to any attacker
that can gain access to, or even force, recordings from a user’s smart
speaker. This could be a malicious person with IoT device hacking
skills, or even a military entity. Fear of the military exploiting these
VA devices is another concern of many people [13, 40, 43].
Similarly, there is growing potential for law enforcement to
obtain smart speaker recordings during their investigations for
evidence in legal cases [27]. In New Hampshire, a judge ordered
Amazon to turn over two days worth of Alexa recordings from
the personal smart speaker device in a murder victim’s home [45].
The prosecution looked to find evidence leading to the killer in
those recordings. Another example occurred in Hallandale Beach,
Florida where smart speaker recordings were subpoenaed to reveal
evidence of an argument between a murder victim and the prime
suspect [25]. Stories like these could be another motivator for some
users to invest in a PJD; wanting their conversations kept private.
However, as our work demonstrates, significant information can
still be recovered from audio samples masked by a PJD which
may interest law enforcement. Even simple data such as number of
speakers or the speakers’ gender could be useful in an investigation.
4 EXPERIMENTS & DATA COLLECTION
4.1 PJD Implementation
We built our own implementation of a Protection Jamming Device
(PJD) based on characteristics of existing PJDs available today. The
build instructions and necessary software for the Project Alias
device are open source and available online [15]; so we use these
materials as the building blocks for of our own device. Like Project
Alias, our device uses a Raspberry Pi3 equipped with an SD card
and the ReSpeaker 2-Mics Pi HAT expansion board. Additionally,
we used a JST 2.0 connector to connect a 16mm tiny speaker. For
our jamming signal, we chose to use a standard Gaussian white
noise (GWN) that has a flat spectral density and encompasses the
0-8 kHz frequency range. We chose GWN because it is a popular
choice for a masking noise and we believe it is a good option for
this first academic study in PJD effectiveness.
Unlike the Project Alias device, our implementation does not
utilize the 3D-printed shell to house all of the components. Also, we
position the tiny speaker directly on top of the center microphone
inspired by the design of Home Wave [8]. This will directly inject
the jamming noise as audio input into the smart speaker. For the
purposes of our experiments, we adapted the Project Alias source
the source code to use the GWN jamming signal and added the
ability to manually start and stop of the noise using a button on
the ReSpeaker expansion board. These modifications were made so
that our PJD could be used for controlled experiments. We are not
presenting our constructed device as a new or viable PJD implementa-
tion because the design choices we made, while useful for conducting
consistent and controlled experiments, add a requirement of user in-
teraction that an actual PJD solution would not have.
Determining Injected Noise Volume Before beginning our ex-
periments, we confirm that our implementation can function suc-
cessfully as a PJD. We manually adjust the volume of the noise
coming from our PJD until it is barely undetectable by a nearby
user (confirmed by lab members). In a real-world implementation,
the consistent noise played from a PJD cannot be so loud that it
disturbs a user in the same space. This is why we adjust the volume
level of our PJD jamming signal to a point that is barely detectable
by the human ear when they are sitting 0.5 meters from the device.
We reason that in a real-world implementation of the device that
uses a printed casing to house the speaker and other components,
the presence of this noise in the environment will be even lower
than what is accepted in our study. With this parameter set, we per-
form initial testing and confirm the noise injected in the foreground
can hinder a smart speaker from recognizing the wake word. This
is the key function of PJDs which operate under the assumption
that the injected noise will fully mask any nearby user speech.
4.2 Experimental Setup
To study how effective our PJD is at masking user speech, we de-
sign an experimental setting that exposes an Amazon Echo Dot to
both normal speech and the injected noise from our device. Specif-
ically, we attach the tiny speaker of our PJD on top of the center
microphone of the Echo Dot, with the Raspberry Pi components
sitting next to it. We use an SRS-XB2 Bluetooth speaker to play
the speech samples. The Bluetooth speaker is pointed towards and
placed approximately 0.5 meters away from the Echo Dot. We test
different SPL (dB) levels for the normal speech in our experiments
including speech in the normal range for human conversation (60
dB), slightly louder speech (65 dB), as well as very loud speech that
is similar to presentation style speaking (70 dB). The SPL for each
setting was measured at the smart speaker’s location using a digital
sound level meter. We test these different SPL levels to generalize
our investigation of the effectiveness of GWN, injected in the fore-
ground, to mask nearby speech. In terms of speaker distance, we
know that SPL decreases when the distance is doubled by -6 dB. So,
if we consider our loudest source speech (70 dB) at 0.5 meters from
the Echo Dot; the other SPL levels (65 and 60 dB) would represent
distances of about 1 meter and 2 meters, respectively, if we simply
moved the 70 dB speech source location.
418ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Payton Walker and Nitesh Saxena
(a) 60 dB Speech (Noisy)
(b) 65 dB Speech (Noisy)
(c) 70 dB Speech (Noisy)
(d) 70 dB Speech (Clean)
Figure 3: Time domain graphs generated from post-processed,
Alexa recorded samples of speaker FAC saying the digit “One”, for
each speech SPL tested (60, 65, 70 dB); as well as the time domain
graph of a clean sample of the same speech from FAC.
4.3 Data Collection
We performed all of our data collection in a quiet office space
with only ambient noise in the environment. It is important in our
study that no additional background noise is present that could
compromise the recordings.
Speech Dataset: For our experiments, we utilized speech samples
from the TIDIGITS dataset [21]. This dataset contains audio samples
of single digits (0-9) spoken by 10 speakers (5 female, 5 male). We
utilize an audio sample of each digit, from each speaker, for a total
of 50 different speech samples. Five samples were collected for each
speaker (10), digit (10), and speech SPL (3) level resulting in a total
of 1500 audio recordings recovered from the Echo Dot.
Additionally, we collected data where the speech played was
a song (lyrics + music). Specifically, we used audio of the songs
“Smooth” by Santana and “Blinding Lights” by The Weeknd. For
each song, we cut out a 5 second portion of the beginning, middle,
and end to use in our experiments which resulted in 6 different
song clips. These samples will be used to attempt song recognition
with the Shazam application, which uses a novel and sophisticated
algorithm that can identify a song from a small snippet of audio.
Collection Steps: For each instance of data collection, we followed
the same set of steps to generate and retrieve the Echo Dot recorded
samples. We begin with the researcher manually activating the Echo
Dot by issuing the wake word, “Alexa”. Once the smart speaker has
been awakened and is actively listening for the user’s command
(indicated by light ring glowing blue), we press the button on our
PJD to manually start the noise injection. After the PJD has been
started, the normal speech sample is played from the nearby Blue-
tooth speaker. After the speech sample finishes playing, we allow
the injected noise to continue until the Echo Dot has finished its
recording (indicated by the light ring powering down). Only after
the recording has stopped do we manually stop our PJD from inject-
ing noise. Once the audio has been recorded by the Echo Dot, the
researcher accesses and saves the recordings from the Alexa Voice
History web interface. Each recording made during data collection
is saved as a .wav file and stored for later processing.
5 ATTACK DESIGN
5.1 Signal Processing of Recovered Audio
After data collection is complete, we amassed a set of 1500 recovered
audio samples from the Echo Dot. In the interest of speech recog-
nition tasks, we perform post-processing on the recovered audio
signals to obtain the greatest results. We began the post-processing
phase by manually trimming each recovered audio sample to about
1 second in length (enough to encompass the spoken digit).
Next, we attempt to improve the quality of the normal speech
in the recovered audio by applying a speech enhancement and
noise reduction routine. We consider four such routines from the
Matlab signal processing toolbox, VOICEBOX [17]. We performed
some initial tests using the specsub, spendred, ssubmmse, and ssub-
mmsev. Each of these routines performs speech enhancement via
some method including spectral subtraction, dereverberation, and
minimum-mean square error (MMSE) with and without voice ac-
tivity detection (VAD). Our initial tests found that the ssubmmse
routine performed best in terms of speech enhancement and white
noise reduction. Therefore, it was chosen as the noise filtering
routine for our signal processing phase of the work.
5.2 Feature Extraction
Once all audio samples in the dataset were processed, we perform
MFCC feature extraction on each sample. MFCC features were cho-
sen because they are widely used when attempting speech recog-
nition tasks, especially when identifying spoken digits. For each
audio sample we calculate the 13 MFCC coefficients which pro-
duces an Nx13 matrix where each column contains the values for
each coefficient. From these coefficients we also calculate a single
mean, minimum, maximum, and standard deviation value for each
of the 13 MFCC coefficients. Additionally, we generate the first and
second order differential coefficient values (e.g., Delta, log energy).
Combining all of these values results in a 144-feature vector.
In addition to the full set of extracted features, we also use an
attribute selection tool in Weka [10] to generate a filtered set of the
419Evaluating the Effectiveness of PJDs in Mitigating Smart Speaker Eavesdropping Attacks Using GWN
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
most important (e.g., most highly correlated) features in an attempt
to achieve the greatest classification accuracies. We select the Clas-
sifierAttributeEval class of evaluator (selecting the RandomForest
classifier) and specify the BestFirst search algorithm. This produced
a small set of the 17 most significant features which we also test in
our classification models.
(a) 60 dB Speech
(b) 65 dB Speech
(c) 70 dB Speech
Figure 4: Cross-correlation graphs comparing post-processed, Alexa
recorded samples of speaker FAC saying the digit “One”, for each
speech SPL tested (60, 65, 70 dB), with the original raw audio file.
The peaks found at lag=0 in these graphs indicate a strong correla-
tion between our recovered signals and the original signal, further
demonstrating the potential for an eavesdropping attack.
5.3 Machine Learning Classification
We attempt different speech classification tasks using the dataset
of VA recordings that we collected. Specifically, we explore speech
(digit) recognition, speaker identification, and gender identification.
In this section we will describe the performance results of our learn-
ing models that were trained for these speech recognition tasks. In
our initial classification attempts we tested NaiveBayes, BayesNet,
Logistic, MultiClass, and RandomForest classifier models. And for
each of these classifiers we tested an 80:20 and 90:10 training/test
data split, as well as 10-fold cross validation. We observe that across
all classification tasks, the RandomForest classifier achieved the
highest accuracies. Therefore, we highlight and report on the accu-
racies of the RandomForest classifier in the following sections.
6 SIGNAL ANALYSIS
6.1 Time & Frequency Spectrum
As an initial look at the post-processed recovered samples, we
perform both time and frequency domain analysis which reveals
the clear presence of normal speech in the recovered audio samples.
Comparing the time domain graphs of samples from the different
speech SPL settings, we find that the normal speech signal is visible
in all of them. Figure 3 shows the time domain graphs generated
from samples of the speaker FAC saying the digit “One” in each of
the speech SPL settings. Although the success of noise cancellation
varied across the different loudness levels of speech, being most
successful when the normal speech was at its loudest, these time
domain graphs confirm that the speech signal is maintained. We
also find that the 70 dB speech sample collected with injected noise
(Figure 3c) achieves a similar quality and strength of speech signal,
after signal processing, to what we see in the clean sample collected
without any injected noise. This suggests louder speech signals
(70+ dB) may be too strong for even injected noise from a PJD to
fully mask and protect from an eavesdropping attack. And even
though the isolation of the speech signal is not as successful at lower
speech SPLs, more sophisticated signal processing techniques could
be used to obtain better results.
Our frequency spectrum analysis yielded similar observations
that support what was seen in the time domain. In the spectrum
graphs we can see the speech related frequencies are strong and
present in all SPL settings. Additionally, we also see the presence
of noise decrease as the source speech SPL was increased (e.g.,
noise cancellation improves). Figure 5 shows the spectrum graphs
generated for the same samples from speaker FAC. We notice that
in all cases the speech frequencies seem to be well identified and
maintained through the post-processing for noise removal. The
observations made in both the time and frequency domains are
positive indicators that PJDs using white noise can be ineffective in
the face of signal processing techniques, and therefore smart speak-
ers equipped with a PJD could still be vulnerable to eavesdropping
attacks. If we compare the 70 dB speech samples collected with
injected noise (noisy, Figure 5c) and without (clean, 5d), we see the
noisy sample can maintain almost all the same frequencies after
signal processing, and at the same strength, as the clean sample.
6.2 Cross-Correlation
Continuing our analysis of the recovered samples, we perform
normalized cross-correlation to compare post-processed samples
from each SPL setting to the original raw audio used in our exper-
iments. This allows us to gauge how much of the original signal
was recovered after the post-processing routines were applied. Be-