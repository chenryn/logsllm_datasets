### Model Training and Anomaly Detection

For model training, we obtain pre-log and post-log event sequence LSTM models. These two LSTM models are combined to form a complete anomaly detection model. The combination of these two models requires the use of another feature: the length of the log event sequence. During model training, we count the distribution of the log event sequence lengths for preliminary filtering. Finally, based on the log verification dataset and numerous tuning experiments, we determine the most suitable model parameters.

In the detection phase, we use the log test dataset to perform model detection. First, we generate a log event sequence from the original log using the parsed log template. We then determine the usage method of the pre-log and post-log detection models based on the sequence length characteristics. According to the selected model, we detect each log event in the incoming log event sequence and determine whether it deviates from the prediction result of a normal log sequence. If there is significant deviation, the log event is judged as anomalous. If the judgment result is normal, the next log event in the sequence is detected until the entire log event sequence is processed or an anomalous log event that deviates from the normal sequence is found. If any log event in the sequence is predicted to be abnormal, the entire log sequence is marked as abnormal. This method provides a feedback mechanism, and logs marked as anomalies are provided to the user for further action. If the user finds that the detected anomaly is a false positive, the falsely reported log event sequence can be added to the training set. When there are many false positives, users can retrain the model with the new training set to gradually update the model.

### 3. Methodology

This paper proposes a system log anomaly detection method based on dual LSTM. By combining two LSTM models, the gradient problem of a single LSTM model in long sequence prediction [27] is addressed, and the performance of anomaly detection is improved. Figure 2 shows the three main modules of this method: log parsing, log key anomaly detection model, and anomaly detection workflow model. In the rest of this section, we will cover the various parts of this method in detail.

#### 3.1. Log Parser (Spell)

In the log parsing module, we convert the original log into a structured log using the log parser. Log parsing is a common preprocessing step for unstructured logs and is an essential part of most log analysis tasks. There are many parsing methods available, with Spell being one of the more effective ones. It is a log parsing method based on the longest common subsequence (LCS). The time complexity of this method for processing each log entry is close to linear (linearly related to the size of the entry), and unstructured log messages are parsed into structured log templates. Although Spell is currently a better log analysis method, the log templates it parses are not always completely correct. If used directly to generate log events and compose a log event sequence for anomaly detection, it may not achieve optimal results.

The primary dataset used in this article is HDFS logs, which effectively demonstrate the effectiveness of the LogLS method. Table 1 shows the HDFS log templates parsed by the Spell method, resulting in 37 types of HDFS log templates. Some of these log templates are duplicated. For example, the three log event templates {E7, E8, E9} are similar, and E8 can contain the other two. Directly using these templates to generate log sequences can seriously affect the accuracy of log anomaly detection, indicating room for improvement. This article adds manual deduplication steps to further optimize the log parsing effect and achieve a higher level of detection.

**Table 1. Event templates obtained from parsing the HDFS log by the Spell method.**

| EventId | EventTemplate |
|---------|---------------|
| E1      | Adding an already existing block(*) |
| E2      | Verification succeeded for(*) |
| ...     | ...           |
| E7      | writeBlock(*) received exception java.io.IOException Connection reset by peer |
| E8      | writeBlock(*) received exception(*) |
| E9      | writeBlock(*) received exception java.io.IOException Could not read from stream |
| ...     | ...           |
| E14     | PacketResponder(*)(*) Exception java.io.IOException Broken pipe |
| E15     | PacketResponder(*)2 Exception(*) |
| E16     | PacketResponder(*)1 Exception(*) |
| ...     | ...           |
| E36     | Deleting block(*) file(*) |
| E37     | BLOCK* NameSystem.allocateBlock(*)(*) |

To improve accuracy, we need to eliminate duplicate data in the log event templates obtained by Spell. We do this by selecting a few representative log entries that conform to each event template and checking for repetition. If repetition is found, the log event templates are unified into the most frequently occurring template. The log events in Table 1 are reprocessed, and the results are shown in Table 2.

**Table 2. HDFS log event templates after processing.**

| EventId | EventTemplate |
|---------|---------------|
| E1      | Adding an already existing block(*) |
| E2      | (*) Verification succeeded for(*) |
| ...     | ...           |
| E7      | writeBlock(*) received exception(*) |
| E8      | PacketResponder(*) for block(*) Interrupted. |
| E9      | Received block(*) of size(*) from(*) |
| ...     | ...           |
| E14     | Exception in receiveBlock for block(*)(*) |
| E15     | Changing block file offset of block(*) from(*) to(*) meta file offset to(*) |
| E16     | (*): Transmitted block(*) to(*) |
| ...     | ...           |
| E28     | BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for(*) on(*) size(*) However, it does not belong to any file. |
| E29     | Pending Replication Monitor timed out block(*) |

In this article, we first use the system log template to divide the unstructured log into several parts (e.g., date, time, content, etc.), and then extract meaningful information (e.g., events) from these parts. Typically, an event consists of three parts: timestamp, signature, and parameters. Figure 3 illustrates the HDFS log parsing process. The signature attribute is the log template. The specific algorithm is implemented as follows:

1. The initialization program first defines a log object (LCSObject) such as the log key (LCSseq) and line number list (lineIds), and defines a log object list (LCSMap) to save each log object.
2. Enter the log file and read it line by line.
3. Read a line of log, and then traverse the LCSMap to see if there is already an LCSObject in the list with the same LCSseq (log key). If such an LCSObject exists, add the lineId of this log to the lineIds of the LCSObject. If not, generate a new LCSObject and add it to the LCSMap.
4. Continue reading the log until the end.

**Figure 3. Example of log parser.**

The template extracted from the log in Figure 3 is "PacketResponder(*) for block(*) terminating," where "(*)" represents a variable parameter. If a new log entry "PacketRespo-nder2 for block blk_2529569087635823495 termina-ting" is entered, Spell's idea is not to extract the log key directly but to compare it. After receiving the newly input log entry, the LCSMap is traversed, and an LCSObject with LCSseq "PacketResponder 0 for block blk_6137960563260046065 terminating" is found. The LCS is calculated as "PacketResponder for block terminating." When the length of the sequence is between 1/2 and 1 times the length of the input entry, it is judged to belong to the same log key, so it is merged, and the lineId of this log entry is added to the lineIds attribute of the LCSObject.

The obtained initial log template is then manually filtered to obtain the final log event template. The log entries of the entire dataset are processed to obtain log events for all log entries, which are used to form log event sequences and perform subsequent model training.

#### 3.2. Anomaly Detection

HDFS logs are parsed to obtain the log template, and the event ID of each log entry is determined based on the log template. The value of some parameters can be used as an identifier for a specific execution sequence, such as block_id in HDFS logs. These identifiers can combine log entries together or unwrap log entries generated by concurrent processes to separate a single thread sequence. As shown in Table 3, the HDFS log event sequences in this article are generated based on block_id.

**Table 3. Demo of HDFS log event sequences.**

| SequenceID | Log Events Sequences |
|------------|---------------------|
| 0          | E5E5E5E22E11E9E11E9E11E9E26E26E26E23E23E23E21E21E21 |
| 1          | E22E5E5E5E11E9E11E9E11E9E26E26E26 |
| 2          | E22E5E5E5E26E26E26E11E9E11E9E11E9E23E23E23E23E21E21E21 |
| 3          | E22E5E5E5E11E9E11E9E11E9E26E26E26 |
| 4          | E22E5E5E5E26E26E26E11E9E11E9E11E9E4E3E3E3E4E3E4E3E3E4E3E3E23E23E23E21E21E21 |
| 5          | E5E22E5E5E11E9E11E9E11E9E26E26E26 |

System log detection will be performed on the log event sequences (shown in Table 3) obtained by the log parser. Assuming that \( K = \{k_1, k_2, k_3, ..., k_n\} \) is a log event sequence transformed by a log block, each log key represents a log path command at a certain time, and the entire log event sequence reflects the sequential execution path of the log. To detect the entire sequence, it is necessary to check whether each log event is normal. Let \( k_i \) be one of the \( n \) sequences, representing the log event to be detected. The DeepLog model takes the influence of the forward sequence on \( k_i \) to determine whether it is abnormal. For HDFS logs, this article not only considers the impact of the forward sequence on the next log event but also considers the impact of the backward sequence on the previous log event, combining them to further determine whether the event is abnormal. Figure 4 summarizes the classification settings. Suppose \( t \) is the sequence ID of the next log event, \( w1 \) is the set of the \( h \) most recent log events in the previous sequence, and \( w2 \) is the set of the \( h \) most recent log events in the subsequent sequence. In other words, \( w1 = \{m_{t-h}, ..., m_{t-2}, m_{t-1}\} \), \( w2 = \{m_{t+h}, ..., m_{t+2}, m_{t+1}\} \), where each \( m_t \) is in \( K \), and is the log event from the log entry \( e_t \). The same log event in \( w1 \) and \( w2 \) may appear multiple times. The output of the training phase is two conditional probability distribution models \( Pr1(m_t = k_i | w1) \) and \( Pr2(m_t = k_i | w2) \) for each \( k_i \in K \) (i = 1, ..., n).

**Figure 4. An overview of log event anomaly detection model.**

To extract context features and potential symmetry information [28] from sequence relationships, two long short-term memory networks (LSTMs) [29â€“31] are used in LogLS to train the pre-log and post-log event sequences. Each LSTM node has a hidden state \( h_{t-i} \) and a cell state \( C_{t-i} \), both of which are passed to the next node to initialize its state. The purpose is to obtain the probability of the current log event \( k_i \) through the pre-log and the subsequent log event \( k_i \) through the model, and then set a probability limit (parameters \( g1 \), \( g2 \)) to determine whether the current log event is anomalous.

The formula for forward propagation is:

**Input gate:**
\[ I_t = \sum_{i=1}^{I} w_{i\iota} x_t + \sum_{h=1}^{H} w_{h\iota} b_{t-1} + \sum_{c=1}^{C} w_{c\iota} s_{t-1} \]
\[ b_t = f(I_t) \]

**Forget gate:**
\[ F_t = \sum_{i=1}^{I} w_{i\phi} x_t + \sum_{h=1}^{H} w_{h\phi} b_{t-1} + \sum_{c=1}^{C} w_{c\phi} s_{t-1} \]
\[ b_t = f(F_t) \]

This completes the detailed explanation of the methodology and the technical aspects of the proposed log anomaly detection system.