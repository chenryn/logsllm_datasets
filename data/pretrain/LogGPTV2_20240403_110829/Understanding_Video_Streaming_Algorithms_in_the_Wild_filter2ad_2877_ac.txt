M. Licciardello et al.
drops, but reduces playback quality. At the same time, redownloads are them-
selves a compromise: if better bitrate decisions could be made to begin with,
redownloads amount to ineﬃcient bandwidth use.
Reactivity, Fig. 4: We ﬁnd that most deployed ABRs are cautious in reacting
to bandwidth changes. This is best illustrated through comparisons between
deployed and academic ABRs. Figure 4 (right) shows such a comparison between
TubiTV and MPC evaluated on the same traces and videos. After the bandwidth
increases (at x-axis = 0 in the plot), TubiTV waits for tens of chunk downloads
before it substantially ramps up bitrate. In contrast, MPC starts switching to
higher bitrates within a few chunk downloads. (The large variations around the
average arise from the varied sizes of the step-increases in the used network
traces and variations in the tested videos.)
While we have not yet evaluated a large number of mobile ABR implementa-
tions (see Sect. 5), we were able to experiment with Vimeo’s mobile and desktop
versions, shown in Fig. 4 (left). They exhibit similar ramp-up behavior in terms
of how many downloads it takes before Vimeo reacts, but show very diﬀerent
degrees of bitrate change. The desktop version increases bitrate in several steps
after the bandwidth increase, while the mobile one settles at a modest increase.
This is along expected lines, as the mobile player, targeting the smaller screen,
often does not use the higher-quality content at all.
A comparison between TubiTV and Vimeo (desktop) across the two plots
is also interesting: Vimeo ramps up faster than TubiTV. (MPC ramps us even
faster on the Vimeo videos.) One potential reason is the diﬀerence in encoding—
TubiTV serves each video in only 3 resolutions, compared to Vimeo’s 4–5. This
implies that over the same network traces, TubiTV must necessarily see a larger
change in bandwidth to be able to jump from one bitrate to the next, given its
larger diﬀerential in bitrate levels.
Bandwidth Usage, Fig. 5a: Diﬀerent platforms use bandwidth very diﬀer-
ently. Arte discards a surprisingly large 23% of its downloaded bytes in its eﬀorts
to replace already downloaded low-quality chunks with high-quality ones. Some
platforms, including YouTube, SRF, and Vimeo, show milder redownload behav-
ior, while several others, including XVideos, Fanrom, Pornhub, and ZDF, do not
use redownloads at all.
ZDF and TubiTV are able to use 80% of the network’s available bytes
for fetching (actually played) video chunks, while all others use the network
much less eﬀectively. While the uncertainty in future bandwidth and the desire
to maintain stable streaming without many quality switches necessitates some
bandwidth ineﬃciencies, we were surprised by how large these ineﬃciencies are.
In particular, XVideos, YouTube, Twitch, and Fandom all use less than 60%
of the network’s available capacity on average across our trace-video pairs4.
4 Note that these ineﬃciencies cannot be blamed on transport/TCP alone, as on the
same traces, other players are able to use 80% of the available capacity. We also
carefully account for non-video data to ensure we are not simply ignoring non-chunk
data in these calculations. For instance, audio data is separately delivered for Vimeo
and YouTube, but is accounted for appropriately in our bandwidth use analysis.
Understanding Video Streaming Algorithms in the Wild
309
Fig. 4. We measure reactivity in terms of bitrate evolution after a bandwidth increase,
i.e., diﬀerence in average playback bitrate after and before the bandwidth change over
time (in terms of chunk downloads). The plots show the reactivity diﬀerences between:
(left) mobile and desktop versions of Vimeo; and (right) TubiTV and MPC.
This low usage is particularly surprising for YouTube, which uses several
strategies—variable chunk lengths (as opposed to ﬁxed-size chunks in other
providers), larger number of available video resolutions, and redownloads—that
allow ﬁner-grained decision making, and thus should support more eﬀective
bandwidth use. Given these advanced features in their ABR design, it is more
likely that their optimization goals diﬀer from academic ABR work than their
algorithm simply being poorly designed. While we cannot concretely ascertain
their optimization objectives, one could speculate that given the large global
demands YouTube faces while operating (largely) as a free, ad-based service,
a proﬁt maximizing strategy may comprise providing good-enough QoE with a
limited expense on downstream bandwidth.
QoE goal, Fig. 5b: We ﬁnd that some providers fetch high-VMAF chunks at
higher quality than the average chunk. In particular, Twitch fetches the chunks
in the top 20th percentile by VMAF at a mean quality level 0.79 higher than an
average chunk. If instead of Twitch’s ABR, we used a VMAF-unaware, simple,
rate-based ABR5 that uses an estimate of throughput to decide on video quality,
this diﬀerence in quality level between high-VMAF and the average chunk would
reduce to 0.46.
5 This ABR estimates throughput, T , as the mean of the last 5 throughput measure-
ments. For its next download, it then picks the highest quality level with a bitrate
≤ T . It thus downloads the largest chunk for which the estimated download time
does not exceed the playback time.
310
M. Licciardello et al.
(a) Bandwidth usage
(b) QoE goal
Fig. 5. (a) Bandwidth usage: many players use surprisingly little of the available net-
work bandwidth (Played/Download-able) despite the potential to improve quality with
more bandwidth, e.g., XVideos uses only 50% of it; and some players, like Arte, spend
a large fraction of their used bandwidth on redownloads. (b) QoE goal: we measure how
much a player prefers high-VMAF chunks by quantifying the average quality-level dif-
ference between all chunks and only the top-x% of chunks by VMAF (i.e., Q[0...%T op]).
Some players, like Twitch, show a large preference for high-VMAF chunks.
Note that given the correlation between higher quality and higher VMAF,
high-VMAF chunks are more likely to be fetched at high quality; what is inter-
esting is the degree to which diﬀerent players prefer them. Vimeo, for instance,
shows a much smaller diﬀerence of 0.27 between the quality level of chunks in
the top 20th percentile and an average chunk. If MPC’s ABR were used to fetch
chunks from Vimeo, this diﬀerence increases to 0.534, because MPC is willing
to make more quality switches than Vimeo.
Our results thus indicate diversity in optimization objectives in terms of
bandwidth use and QoE targets across deployed video platforms. It is at least
plausible that academic ABRs produce diﬀerent behavior over the same traces
not because they are much more eﬃcient, but rather the optimization consider-
ations are diﬀerent. While algorithms like MPC are ﬂexible enough to be used
for a variety of optimization objectives, it is unclear how performance would
compare across a suitably modiﬁed MPC (or other state-of-the-art ABR) when
evaluated on operator objectives.
5 Limitations and Future Work
Our ﬁrst broad examination of a diverse set of widely deployed ABRs reveals
several interesting insights about their behavior, but also raises several questions
we have not yet addressed:
1. Does ABR behavior for the same platform vary by geography and client net-
work? Such customization is plausible—there are likely large diﬀerences in
network characteristics that a provider could use in heuristics, especially for
startup behavior, where little else may be known about the client’s network
Understanding Video Streaming Algorithms in the Wild
311
bandwidth and its stability. However, addressing this question would require
running bandwidth-expensive experiments from a large set of globally dis-
tributed vantage points.
2. How big are the diﬀerences between mobile and desktop versions of ABR
across platforms? Unfortunately, while the browser provides several universal
abstractions through which to perform monitoring on the desktop, most plat-
forms use their own mobile apps, greatly increasing the per-platform eﬀort
for analysis.
3. If we assume that the largest providers like YouTube and Twitch are optimiz-
ing ABR well, based on their experience with large populations of users, can
we infer what their optimization objective is? While there are hints in our
work that these providers are not necessarily optimizing for the same objec-
tive as academic ABR, we are not yet able to make more concrete assertions
of this type.
4. Does latency have a substantial impact on ABR? ABR is largely a bandwidth-
dependent application, but startup behavior could potentially be tied to
latency as well. We have thus far not evaluated latency-dependence.
6 Conclusion
We conduct a broad comparison of adaptive bitrate video streaming algorithms
deployed in the wild across 10 large video platforms oﬀering varied content tar-
geted at diﬀerent audiences. We ﬁnd large diﬀerences in player behavior, with a
wide spectrum of choices instantiated across virtually all metrics we examined.
For instance, our results show that: (a) some deployed ABRs are conscious of per-
ceptual quality metrics compared to others focused on bitrate; (b) no deployed
ABRs follow available bandwidth as closely as research ABRs; and (c) several
ABRs leave a large fraction of available network capacity unused. Whether this
diversity of design choices and behaviors stems from careful tailoring towards dif-
ferent use cases and optimization objectives, or is merely a natural consequence
of sub-optimal, independent design is at present unclear. But if large, otherwise
extremely well-engineered platforms like YouTube diﬀer so substantially from
state-of-the-art research ABRs, then it is at least plausible that ABR research
is more narrowly focused than desirable.
References
1. Amazon prime terms of use. https://www.amazon.co.uk/gp/help/customer/
display.html?nodeId=201909000&pop-up=
2. Hulu terms of use. https://www.hulu.com/terms
3. Netﬂix terms of use. https://help.netﬂix.com/legal/termsofuse
4. Selenium webdriver. https://www.seleniumhq.org/projects/webdriver/
5. YouTube downloader. https://github.com/ytdl-org/youtube-dl/
6. Akhtar, Z., et al.: Oboe: auto-tuning video ABR algorithms to network conditions.
In: ACM SIGCOMM (2018)
312
M. Licciardello et al.
7. A˜norga, J., Arrizabalaga, S., Sedano, B., Goya, J., Alonso-Arce, M., Mendizabal,
J.: Analysis of YouTube’s traﬃc adaptation to dynamic environments. Multimedia
Tools Appl. 77(7), 7977 (2018)
8. De Cicco, L., Caldaralo, V., Palmisano, V., Mascolo, S.: Elastic: a client-side con-
troller for dynamic adaptive streaming over HTTP (DASH). In: IEEE Packet Video
Workshop (PV) (2013)
9. Federal Communications Commission: Validated data September 2017 - measuring
broadband America. https://www.fcc.gov/reports-research/reports/
10. Ghasemi, M., Kanuparthy, P., Mansy, A., Benson, T., Rexford, J.: Performance
characterization of a commercial video streaming service. In: ACM IMC (2016)
11. Gr¨uner, M., Licciardello, M.: Understanding video streaming algorithms in the
wild - scripts. https://github.com/magruener/understanding-video-streaming-in-
the-wild
12. van der Hooft, J., et al.: HTTP/2-based adaptive streaming of HEVC video over
4G/LTE networks. IEEE Commun. Lett. 20(11), 2177–2180 (2016)
13. Jiang, J., Sekar, V., Zhang, H.: Improving fairness, eﬃciency, and stability in
HTTP-based adaptive video streaming with festive. IEEE/ACM Trans. Netw.
22(1), 326–340 (2014). https://doi.org/10.1109/TNET.2013.2291681
14. Li, Z., et al.: Probe and adapt: rate adaptation for HTTP video streaming at
scale. IEEE J. Sel. Areas Commun. 32(4), 719–733 (2014). https://doi.org/10.
1109/JSAC.2014.140405
15. Li, Z., Aaron, A., Katsavounidis, I., Moorthy, A., Manohara, M.: Toward a practi-
cal perceptual video quality metric (2016). https://medium.com/netﬂix-techblog/
toward-a-practical-perceptual-video-quality-metric-653f208b9652
16. Mao, H., et al.: Real-world video adaptation with reinforcement learning. In: Rein-
forcement Learning for Real Life (ICML workshop) (2019)
17. Mao, H., Netravali, R., Alizadeh, M.: Neural adaptive video streaming with pen-
sieve. In: ACM SIGCOMM, pp. 197–210. ACM (2017)
18. Miller, K., Bethanabhotla, D., Caire, G., Wolisz, A.: A control-theoretic approach
to adaptive video streaming in dense wireless networks. IEEE Trans. Multimedia
17(8), 1309–1322 (2015)
19. Mondal, A., et al.: Candid with YouTube: adaptive streaming behavior and impli-
cations on data consumption. In: ACM NOSSDAV (2017)
20. Moreau, E.: What Is Vimeo? An Intro to the Video Sharing Platform. https://
www.lifewire.com/what-is-vimeo-3486114
21. Pantos, R., May, W.: HTTP Live Streaming Draft. https://tools.ietf.org/html/
draft-pantos-http-live-streaming-17.html
22. Qin, Y., et al.: ABR streaming of VBR-encoded videos: characterization, chal-
lenges, and solutions. In: ACM CoNEXT (2018)
23. Qin, Y., et al.: A control theoretic approach to ABR video streaming: a fresh look
at PID-based rate adaptation. In: INFOCOM 2017-IEEE Conference on Computer
Communications, IEEE, pp. 1–9. IEEE (2017)
24. Riiser, H., Vigmostad, P., Griwodz, C., Halvorsen, P.: Commute path bandwidth
traces from 3G networks: analysis and applications. In: ACM MMSys (2013)
25. Sandvine: The global Internet phenomena report (2019).https://www.sandvine.
com/press-releases/sandvine-releases-2019-global-internet-phenomena-report
26. Spiteri, K., Urgaonkar, R., Sitaraman, R.K.: BOLA: near-optimal bitrate adapta-
tion for online videos. In: IEEE INFOCOM 2016 - The 35th Annual IEEE Inter-
national Conference on Computer Communications, pp. 1–9, April 2016. https://
doi.org/10.1109/INFOCOM.2016.7524428
Understanding Video Streaming Algorithms in the Wild
313
27. Spiteri, K., Sitaraman, R., Sparacio, D.: From theory to practice: Improving bitrate
adaptation in the DASH reference player. In: ACM MMsys (2018)
28. Stohr, D., Fr¨ommgen, A., Rizk, A., Zink, M., Steinmetz, R., Eﬀelsberg, W.: Where
are the sweet spots?: a systematic approach to reproducible DASH player compar-
isons. In: ACM Multimedia (2017)
29. Sun, Y., et al.: CS2P: improving video bitrate selection and adaptation with data-
driven throughput prediction. In: ACM SIGCOMM (2016)
30. Timmerer, C., Maiero, M., Rainer, B.: Which Adaptation Logic? An Objective and
Subjective Performance Evaluation of HTTP-based Adaptive Media Streaming
Systems. CoRR (2016)
31. Wamser, F., Casas, P., Seufert, M., Moldovan, C., Tran-Gia, P., Hossfeld, T.: Mod-
eling the YouTube stack: from packets to quality of experience. Comput. Netw.
109, 211–224 (2016)
32. Wang, C., Rizk, A., Zink, M.: SQUAD: a spectrum-based quality adaptation for
dynamic adaptive streaming over HTTP. In: ACM MMSys (2016)
33. Yan, F.Y., et al.: Learning in situ: a randomized experiment in video streaming.
In: USENIX NSDI (2019)