title:Understanding security mistakes developers make: Qualitative analysis
from Build It, Break It, Fix It
author:Daniel Votipka and
Kelsey R. Fulton and
James Parker and
Matthew Hou and
Michelle L. Mazurek and
Michael Hicks
Understanding security mistakes developers make: 
Qualitative analysis from Build It, Break It, Fix It
Daniel Votipka, Kelsey R. Fulton, James Parker, Matthew Hou, 
Michelle L. Mazurek, and Michael Hicks, University of Maryland
https://www.usenix.org/conference/usenixsecurity20/presentation/votipka-understanding
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Understanding security mistakes developers make: Qualitative analysis from
Build It, Break It, Fix It
Daniel Votipka, Kelsey R. Fulton, James Parker,
Matthew Hou, Michelle L. Mazurek, and Michael Hicks
{dvotipka,kfulton,jprider1,mhou1,mmazurek,mwh}@cs.umd.edu
University of Maryland
Abstract
Secure software development is a challenging task requir-
ing consideration of many possible threats and mitigations.
This paper investigates how and why programmers, despite a
baseline of security experience, make security-relevant errors.
To do this, we conducted an in-depth analysis of 94 submis-
sions to a secure-programming contest designed to mimic
real-world constraints: correctness, performance, and security.
In addition to writing secure code, participants were asked
to search for vulnerabilities in other teams’ programs; in to-
tal, teams submitted 866 exploits against the submissions we
considered. Over an intensive six-month period, we used itera-
tive open coding to manually, but systematically, characterize
each submitted project and vulnerability (including vulnera-
bilities we identiﬁed ourselves). We labeled vulnerabilities
by type, attacker control allowed, and ease of exploitation,
and projects according to security implementation strategy.
Several patterns emerged. For example, simple mistakes were
least common: only 21% of projects introduced such an error.
Conversely, vulnerabilities arising from a misunderstanding
of security concepts were signiﬁcantly more common, ap-
pearing in 78% of projects. Our results have implications for
improving secure-programming APIs, API documentation,
vulnerability-ﬁnding tools, and security education.
1 Introduction
Developing secure software is a challenging task, as evi-
denced by the fact that vulnerabilities are still discovered,
with regularity, in production code [19, 20, 54]. How can we
improve this situation? There are many steps we could take.
We could invest more in automated vulnerability discovery
tools [5,9,10,24,49,67,72,75,76]. We could expand security
education [17, 39, 42, 47, 59]. We could focus on improving
secure development processes [18, 48, 53, 65].
An important question is which intervention is ultimately
most effective in maximizing outcomes while minimizing
time and other resources expended. The increasing perva-
siveness of computing and the rising number of professional
developers [16, 44, 77] is evidence of the intense pressure to
produce new services and software quickly and efﬁciently. As
such, we must be careful to choose interventions that work
best in the limited time they are allotted. To do this, we must
understand the general type, attacker control allowed, and
ease of exploitation of different software vulnerabilities, and
the reasons that developers make them. That way, we can
examine how different approaches address the landscape of
vulnerabilities.
This paper presents a systematic, in-depth examination (us-
ing best practices developed for qualitative assessments) of
vulnerabilities present in software projects. In particular, we
looked at 94 project submissions to the Build it, Break it, Fix it
(BIBIFI) secure-coding competition series [66]. In each com-
petition, participating teams (many of which were enrolled
in a series of online security courses [34]) ﬁrst developed
programs for either a secure event-logging system, a secure
communication system simulating an ATM and a bank, or a
scriptable key-value store with role-based access control poli-
cies. Teams then attempted to exploit the project submissions
of other teams. Scoring aimed to match real-world develop-
ment constraints: teams were scored based on their project’s
performance, its feature set (above a minimum baseline), and
its ultimate resilience to attack. Our six-month examination
considered each project’s code and 866 total exploit submis-
sions, corresponding to 182 unique security vulnerabilities
associated with those projects.
The BIBIFI competition provides a unique and valuable
vantage point for examining the vulnerability landscape, com-
plementing existing ﬁeld measures and lab studies. When
looking for trends in open-source projects (ﬁeld measures),
there are confounding factors: Different projects do differ-
ent things, and were developed under different circumstances,
e.g., with different resources and levels of attention. By con-
trast, in BIBIFI we have many implementations of the same
problem carried out by different teams but under similar cir-
cumstances. As such, we can postulate the reasons for ob-
served differences with more conﬁdence. At the other end of
the spectrum, BIBIFI is less controlled than a lab study, but
USENIX Association
29th USENIX Security Symposium    109
offers more ecological validity—teams had weeks to build
their project submissions, not days, using any languages, tools,
or processes they preferred.
Our rigorous manual analysis of this dataset both identiﬁed
new insights about secure development and conﬁrmed ﬁnd-
ings from lab studies and ﬁeld measurements, all with impli-
cations for improving secure-development training, security-
relevant APIs [2,35, 57], and tools for vulnerability discovery.
Simple mistakes, in which the developer attempts a valid
security practice but makes a minor programming error, were
least common: only 21% of projects introduced such an er-
ror. Mitigations to these types of mistakes are plentiful. For
example, in our data, minimizing the trusted code base (e.g.,
by avoiding duplication of security-critical code) led to sig-
niﬁcantly fewer mistakes. Moreover, we believe that modern
analysis tools and testing techniques [6, 7, 13, 14, 23, 27, 37,
40, 43, 70, 71, 81] should uncover many of them. All but one
of the mistakes in our dataset were found and exploited by
opposing teams. In short, this type of bug appears to be both
relatively uncommon and amenable to existing tools and best
practices, suggesting it can be effectively managed.
On the other hand, vulnerabilities arising from misunder-
standing of security concepts were signiﬁcantly more com-
mon: 78% of projects introduced at least one such error. In
examining these errors, we identify an important distinction
between intuitive and unintuitive security requirements; for
example, several teams used encryption to protect conﬁden-
tiality but failed to also protect integrity. In 45% of projects,
teams missed unintuitive requirements altogether, failing to
even attempt to implement them. When teams implemented
security requirements, most were able to select the correct
security primitives to use (only 21% selected incorrectly), but
made conceptual errors in attempting to apply a security mech-
anism (44% of projects). For example, several projects failed
to provide randomness when an API expects it. Although
common, these vulnerabilities proved harder to exploit: only
71% were exploited by other teams (compared to 97% of
simple mistakes), and our qualitative labeling identiﬁed 35%
as difﬁcult to exploit (compared to none of the simple mis-
takes). These more complex errors expose a need for APIs less
subject to misuse, better documentation, and better security
training that focuses on less-intuitive concepts like integrity.
Overall, our ﬁndings suggest rethinking strategies to pre-
vent and detect vulnerabilities, with more emphasis on con-
ceptual difﬁculties rather than mistakes.
2 Data
This section presents the Build It, Break It, Fix It (BIBIFI)
secure-programming competition [66], the data we gathered
from it which forms the basis of our analysis, and reasons why
the data may (or may not) represent real-world situations.1
1Our anonymized data is available upon request.
2.1 Build it, Break it, Fix it
A BIBIFI competition comprises three phases: building,
breaking, and ﬁxing. Participating teams can win prizes in
both build-it and break-it categories.
In the ﬁrst (build it) phase, teams are given just under two
weeks to build a project that (securely) meets a given speciﬁ-
cation. During this phase, a team’s build-it score is determined
by the correctness and efﬁciency of their project, assessed by
test cases provided by the contest organizers. All projects
must meet a core set of functionality requirements, but they
may optionally implement additional features for more points.
Submitted projects may be written in any programming lan-
guage and are free to use open-source libraries, so long as
they can be built on a standard Ubuntu Linux VM.
In the second (break it) phase, teams are given access to
the source code of their fellow competitors’ projects in order
to look for vulnerabilities.2 Once a team identiﬁes a vulnera-
bility, they create a test case (a break) that provides evidence
of exploitation. Depending on the contest problem, breaks
are validated in different ways. One is to compare the output
of the break on the target project against that of a “known
correct” reference implementation (RI) written by the compe-
tition organizers. Another way is by conﬁrming knowledge
(or corruption) of sensitive data (produced by the contest orga-
nizers) that should have been protected by the target project’s
implementation. Successful breaks add to a team’s break-it
score, and reduce the target project’s team’s build-it score.
The ﬁnal (ﬁx it) phase of the contest affords teams the
opportunity to ﬁx bugs in their implementation related to sub-
mitted breaks. Doing so has the potential beneﬁt that breaks
which are superﬁcially different may be uniﬁed by a ﬁx, pre-
venting them from being double counted when scoring.
2.2 Data gathered
We analyzed projects developed by teams participating in four
BIBIFI competitions, covering three different programming
problems: secure log, secure communication, and multiuser
database. (Appendix A provides additional details about the
makeup of each competition.) Each problem speciﬁcation
required the teams to consider different security challenges
and attacker models. Here we describe each problem, the
size/makeup of the reference implementation (for context),
and the manner in which breaks were submitted.
Secure log (SL, Fall 20143and Spring 2015, RI size: 1,013
lines of OCaml). This problem asks teams to implement two
programs: one to securely append records to a log, and one
to query the log’s contents. The build-it score is measured
by log query/append latency and space utilization, and teams
may implement several optional features.
2Source code obfuscation was against the rules. Complaints of violations
were judged by contest organizers.
3The Fall’14 contest data was not included in the original BIBIFI data
110    29th USENIX Security Symposium
USENIX Association
Teams should protect against a malicious adversary with
access to the log and the ability to modify it. The adversary
does not have access to the keys used to create the log. Teams
are expected (but not told explicitly) to utilize cryptographic
functions to both encrypt the log and protect its integrity.
During the break-it phase, the organizers generate sample logs
for each project. Break-it teams demonstrate compromises to
either integrity or conﬁdentiality by manipulating a sample
log ﬁle to return a differing output or by revealing secret
content of a log ﬁle.
Secure communication (SC, Fall 2015, RI size: 1,124 lines
of Haskell). This problem asks teams to build a pair of clien-
t/server programs. These represent a bank and an ATM, which
initiates account transactions (e.g., account creation, deposits,
withdrawals, etc.). Build-it performance is measured by trans-
action latency. There are no optional features.
Teams should protect bank data integrity and conﬁden-
tiality against an adversary acting as a man-in-the-middle
(MITM), with the ability to read and manipulate communica-
tions between the client and server. Once again, build teams
were expected to use cryptographic functions, and to consider
challenges such as replay attacks and side-channels. Break-it
teams demonstrate exploitations violating conﬁdentiality or
integrity of bank data by providing a custom MITM and a
script of interactions. Conﬁdentiality violations reveal the se-
cret balance of accounts, while integrity violations manipulate
the balance of unauthorized accounts.
Multiuser database (MD, Fall 2016, RI size: 1,080 lines
of OCaml). This problem asks teams to create a server that
maintains a secure key-value store. Clients submit scripts
written in a domain-speciﬁc language. A script authenticates
with the server and then submits a series of commands to
read/write data stored there. Data is protected by role-based
access control policies customizable by the data owner, who
may (transitively) delegate access control decisions to other
principals. Build-it performance is assessed by script running
time. Optional features take the form of additional script
commands.
The problem assumes that an attacker can submit com-
mands to the server, but not snoop on communications. Break-
it teams demonstrate vulnerabilities with a script that shows a
security-relevant deviation from the behavior of the RI. For ex-
ample, a target implementation has a conﬁdentiality violation
if it returns secret information when the RI denies access.
Project Characteristics. Teams used a variety of languages
analysis [66]. It had only 12 teams and was organizationally unusual; notably,
build-it teams were originally only allocated 3 days to complete the project,
but then were given an extension (with the total time on par with that of later
contests). Including Fall’14 in the original data analysis would have required
adding a variable (the contest date) to all models, but the small number of
submissions would have required sacriﬁcing a more interesting variable to
preserve the models’ power. In this paper, including Fall’14 is not a problem
because we are performing a qualitative rather than quantitative analysis.
in their projects. Python was most popular overall (39 teams,
41%), with Java also widely used (19, 20%), and C/C++ third
(7 each, 7%). Other languages used by at least one team
include Ruby, Perl, Go, Haskell, Scala, PHP, JavaScript Vi-
sual Basic, OCaml, C#, and F#. For the secure log problem,
projects ranged from 149 to 3857 lines of code (median 1095).
secure communication ranged from 355 to 4466 (median 683)
and multiuser database from 775 to 5998 (median 1485).
2.3 Representativeness: In Favor and Against
Our hope is that the vulnerability particulars and overall trends
that we ﬁnd in BIBIFI data are, at some level, representative
of the particulars and trends we might ﬁnd in real-world code.
There are several reasons in favor of this view:
• Scoring incentives match those in the real world. At
build-time, scoring favors features and performance—security
is known to be important, but is not (yet) a direct concern.
Limited time and resources force a choice between uncertain
beneﬁt later or certain beneﬁt now. Such time pressures mimic
short release deadlines.
• The projects are substantial, and partially open ended, as
in the real world. For all three problems, there is a signiﬁcant
amount to do, and a fair amount of freedom about how to
do it. Teams must think carefully about how to design their
project to meet the security requirements. All three projects
consider data security, which is a general concern, and suggest
or require general mechanisms, including cryptography and
access control. Teams were free to choose the programming
language and libraries they thought would be most successful.
While real-world projects are surely much bigger, the BIBIFI
projects are big enough that they can stand in for a component
of a larger project, and thus present a representative program-
ming challenge for the time given.
• About three-quarters of the teams whose projects we
evaluated participated in the contest as the capstone to an
on-line course sequence (MOOC) [34]. Two courses in this
sequence — software security and cryptography — were
directly relevant to contest problems. Although these partici-
pants were students, most were also post-degree professionals;
overall, participants had a average of 8.9 years software de-
velopment experience. Further, prior work suggests that in at
least some secure development studies, students can substitute
effectively for professionals, as only security experience, not
general development experience, is correlated with security
outcomes [3, 4, 56, 58].
BIBIFI data will not represent the real world:
On the other hand, there are several reasons to think the
• Time pressures and other factors may be insufﬁciently
realistic. For example, while there was no limit on team size
(they ranged from 1 to 7 people with a median of 2), some
teams might have been too small, or had too little free time,
to devote enough energy to the project. That said, the incen-
tive to succeed in the contest in order to pass the course for
USENIX Association
29th USENIX Security Symposium    111
the MOOC students was high, as they would not receive a
diploma for the whole sequence otherwise. For non-MOOC
students, prizes were substantial, e.g., $4000 for ﬁrst prize.
While this may not match the incentive in some security-
mature companies where security is “part of the job” [36]
and continued employment rests on good security practices,
prior work suggests that many companies are not security-
mature [8].
• We only examine three secure-development scenarios.
These problems involve common security goals and mecha-
nisms, but results may not generalize outside them to other
security-critical tasks.
• BIBIFI does not simulate all realistic development set-