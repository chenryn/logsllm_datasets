abide by different privacy regulations and data sovereignty
restrictions, the two servers may indeed be connected using
a high bandwidth direct link but be administered in different
countries. In such scenarios, the logical, administrative, or legal
separation of the two servers plays a more signiﬁcant role.
Ofﬂine vs. Online. We report experimental numbers for both
the ofﬂine and the online phase of our protocols separately,
but only use total costs (online + ofﬂine) when comparing
to related work. The ofﬂine phase includes all computation
and communication that can be performed without presence
of data, while the online phase consists of all data-dependent
steps of the protocol. Optimizing the online cost is useful
for application scenarios where a fast turn-around is required.
In particular, when using our protocols for privacy-preserving
prediction (e.g. fraud detection), new data needs to be classiﬁed
with low latency and high throughput. Indeed, we run a set
of experiments to demonstrate that online cost of privacy-
preserving prediction can be made fast enough to run for latency
critical applications (See Table IV). Similarly, when training
small models dynamically and on a regular basis, it is important
to have high online efﬁciency. In contrast, when training large
models (e.g. a large neural networks), the separation of the
ofﬂine and the online costs is less important.
Data sets. In our experiments, we use the following datasets.
The MNIST dataset [6] contains images of handwritten digits
from “0” to “9”. It has 60,000 training samples, each with 784
features representing 28× 28 pixels in the image. Each feature
is a grayscale between 0∼255. The Gisette dataset [4], [25]
contains images of digits “4” and “9”. It has 13,500 samples
and 5,000 features between 0∼1,000. We also use the Arcene
dataset [1], [25]. It contains mass-spectrometric data and is
used to determine if the patient has cancer. There are 200 data
samples with 10,000 features. Each value is between 0 and
1000. All of the datasets are drawn from the real world.
A. Experiments for Linear Regression
We start with the experimental results for our privacy
preserving linear regression protocols in different settings, and
compare it with previous privacy preserving solutions.
Online phase. To examine how the the online phase scales,
we run experiments on datasets with size (n) from 1,000 to
1,000,000 and d from 100 to 1,000. When n ≤ 60000 and d ≤
784, the samples are directly drawn from the MNIST dataset.
When n and d are larger than that of MNSIT, we duplicate
the dataset and add dummy values for missing features. Note
30
)
s
(
e
m
T
i
103
102
101
100
10−1
10−2
103
)
s
(
e
m
T
i
104
103
102
101
100
103
PP Linear 1
PP Linear 2
104
n
(a)
1.5
1
0.5
105
106
0
100
300
700
900
500
d
(b)
80
60
40
20
105
106
0
100
300
104
n
(c)
700
900
500
d
(d)
Fig. 6: Online cost of privacy preserving linear regression in
standard and client-aided settings. |B| is set to 128. Figure
(a), (b) are for LAN network and Figure (c), (d) are for WAN
network. Figure (a) and (c) are in log-log scale and for d = 784.
Figure (b) and (d) are in regular scale and for n = 10, 000.
that when n, d, E are ﬁxed, the actual data used in the training
does not affect the running time.
Figure 6a shows the results in the LAN setting. “PP Linear
1” denotes the online phase of our privacy preserving linear
regression with multiplication triplets in matrix form, and “PP
Linear 2” denotes the online phase of the client-aided variant.
The running time reported is the total online time when two
servers are running simultaneously and interacting with each
other. The two parties take roughly the same time based on
our experiments. The learning rate is predetermined and we do
not count the time to ﬁnd an appropriate learning rate in the
ﬁgures. The number of features is ﬁxed to 784 and n varies
from 1,000 to 1,000,000.
As shown in the ﬁgure, the online time of our linear
regression is very fast in the LAN setting.
In particular, it
only takes 22.3s to train a linear model securely on 1 million
data samples with 784 features each. From 22.3s needed for
privacy preserving training, only a small portion, namely less
than 2s, is spent on the network delay for the interactions.
The communication time to transfer the data is negligible
given the high bandwidth of the LAN network. Our second
protocol using client-generated multiplication triplets has an
overhead of roughly 3.5×. In particular, it takes 77.6s to train
the model with n = 1, 000, 000 and d = 784. As shown in
Figure 6a and 6b, the running time of our protocol scales
linearly with both n and d. We also observe that the SGD for
linear and logistic regressions on all the datasets we tested
always converges within the ﬁrst epoch, and terminate after the
second epoch, which conﬁrms that the SGD is very effective
and efﬁcient in practice.
Figure 6c shows the corresponding performance on a WAN
network. The running time of our privacy preserving protocols
increase signiﬁcantly. In particular, our ﬁrst protocol takes
2291.8s to train the model when n = 1, 000, 000 and d = 784.
The reason is that now the network delay is the dominating
factor in the training time. The computation time is exactly
the same as the LAN setting, which is around 20s;
the
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
d=100
d=500
d=1000
d=100
d=500
d=1000
d=100
d=500
d=1000
LAN
23.9s
83.9s
158.4s
248.4s
869.1s
1600.9s
2437.1s
8721.5s
16000s∗
Communication
LHE-based
WAN
24.0s
84.8s
163.2s
252.9s
890.2s
1627.0s
2478.1s
8782.4s
16100s∗
2MB
6MB
10MB
20MB
60MB
100MB
200MB
600MB
1000MB
LAN
0.86s
3.8s
7.9s
7.9s
39.2s
80.0s
88.0s
377.9s
794.0s
WAN
43.2s
210.6s
163.2s
420.2s
2119.1s
4097.1s
4125.1s
20000s∗
40000s∗
n = 1,000
n = 10,000
n = 100,000
OT-based
Communication
Client aided
Communication
Dataset size
190MB
1GB
1.9GB
1.9GB
9.5GB
19GB
19GB
95GB
190GB
Time
0.028s
0.16s
0.33s
0.33s
1.98s
4.0s
3.9s
20.2s
49.9s
7MB
35MB
69MB
69MB
344MB
687MB
687MB
3435MB
6870MB
0.8MB
3.8MB
7.6MB
7.6MB
38MB
76MB
76MB
380MB
760MB
TABLE II: Performance of the ofﬂine phase. |B| = 128 and E = 2. ∗ means estimated visa extrapolation.
communication time is still negligible even under the bandwidth
of the WAN network. The total running time is almost the
same as the network delay times the number of iterations.
Our second protocol is still roughly 3.3× slower than the ﬁrst
protocol, but the reason is different from the LAN setting. In the
WAN setting, this overhead comes from the increment of the
communication, as explained in Section V. Even under this big
network delay in the WAN network, as we will show later, the
performance of our privacy preserving machine learning is still
orders of magnitude faster than the state of the art. Besides,
it is also shown in Figure 6c that the training time grows
linearly with the number of the samples in WAN networks.
However, in Figure 6d, when ﬁxing n = 10, 000, the training
time of our ﬁrst protocol only grows slightly when d increases,
which again has to do with the fact that number of interactions
is independent of d. The overhead of our second protocol
compared to the ﬁrst one is increasing with d, because the
communication grows linearly with d in the second protocol.
When d = 100, the training time is almost the same, as it is
dominated by the interaction; when d = 1000, the training
time is 4× slower because of the overhead of communication.
We also show that we can improve the performance in the
WAN setting by increasing the mini-batch size, in order to
balance the computation time and the network delay. Figure 7
shows the result of this parameter tweaking. We let n = 10, 000
and d = 784 and increases |B| to measure its effect on
performance. As shown in the ﬁgure, the running time of the
online phase is decreasing when we increase the mini-batch size.
In particular, it takes 6.8s to train the model in our ﬁrst protocol
when |B| = 512, which is almost 4 times faster than the time
needed when |B| = 128. This is because when the number of
epochs is the same, the number of iterations (or interactions)
is inverse proportional to the mini-batch size. When the mini-
batch size is increasing, the computation time remains roughly
unchanged, while the time spent on interaction decreases.
However, the running time cannot always keep decreasing.
When the computation time becomes dominating, the running
time will remain unchanged. Furthermore, if |B| is set too large,
the number of iterations is too small in an epoch such that the
80
60
40
20
)
s
(