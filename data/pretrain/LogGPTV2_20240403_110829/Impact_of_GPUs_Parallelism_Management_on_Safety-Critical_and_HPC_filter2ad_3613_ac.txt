(number of blocks) is fixed to 512. 
B1024G512
instantiated threads, is reduced when the number of threads 
is  increased  (see  the  last  column  of  Tab.  I  and  III).  The 
execution time divided by the number of threads is lower in 
Tab. III (i.e., when big block sizes are chosen) than in Tab. I. 
The GPU is then more efficient when handling big blocks of 
threads.  Moreover,  increasing  the  number  of  threads  per 
block  while  keeping  the  grid  sized  fixed  seems  to  barely 
affect the thread execution time (see last column of Tab. III). 
These  analyses  suggest  that  the  GPU  scheduler  strain  will 
strongly  depend  on  the  chosen  thread  distribution,  and 
having big blocks is likely to result in a more efficient code. 
B.  Experimental Results 
instantiated 
tested  distributions  when  each 
Tab.  II  and  Tab.  IV  provide  the  experimental  cross 
sections measured at LANSCE, Los Alamos, NM, USA, for 
the 
thread 
executes  the  sums  (first  column)  or  mults  (last  column) 
benchmark.  Results  are  reported  with  and  95%  confidence 
intervals.  More than 10,000  executions  were  performed for 
each  distribution  and  benchmark  collecting  at  least  100 
errors  for  each  tested  configuration.  Reported  values  were 
calculated by dividing the observed output error rate per unit 
time  by  the  flux.  For  double-precision  floating-point  data, 
sums  are  always  more  prone 
than 
multiplications.  Modern  GPUs  include  resources  optimized 
for  floating  point  operations.  Thus,  the  multiplications 
executed  in  the  GPU  during  our  experiments  are  not 
implemented  as  subsequent  sums  but  uses  dedicated 
hardware,  which  is  proved  experimentally  to  be  more 
reliable when executing multiplications. 
to  be  corrupted 
459
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
NORMALIZED CROSS SECTION (BLOCK SIZE FIXED TO 32) 
NORMALIZED CROSS SECTION (GRID SIZE FIXED TO 512) 
TABLE V 
TABLE VI 
B64G512 
4.05±9% 
3.03±9% 
σsums/threads [10-10cm2] 
σmults/threads [10-10cm2] 
B32G30 
7.5±10% 
6.86±10% 
B32G150 
10.69±10% 
7.60±10% 
B32G300 
14.97±10% 
9.80±10% 
σsums/threads [10-10cm2] 
σmults/threads [10-10cm2] 
B256G512  B1024G512 
0.40±8% 
1.13±8% 
0.92±8% 
0.34±8% 
increase 
Increasing the  grid size with a fixed block size  (Fig. 4) 
and increasing the block size with a fixed grid size (Fig. 5) 
have  both  the  countermeasure  of  making  the  GPU  more 
prone to be corrupted by radiation. Increasing the number of 
threads offers higher computing capabilities and throughput, 
but  requires  more  computational  effort  and  resources 
utilization,  which  cause  an 
in  application 
complexity and cross section. 
The application cross section was measured normalizing 
the code execution time. Errors could occur in the GPU just 
during the test execution phase and not when code and data 
are  uploaded  or  download  to  the  PC.  In  fact,  during  the 
initialization and readback test phases (see section III.C), the 
GPU  core  is  in  idle  state  and  data  are  stored  in  the  DDR, 
which  is  not  irradiated.  Thus,  the  cross  section  increase 
found  for more  complex  codes is neither  due to the longer 
exposure  time  of  the  GPU  nor  to  variations  in  the  time 
required by the PC in the initialization and readback phases. 
On the contrary, the cross section, as its definition suggest, is 
the  amount  of  resources  that,  if  corrupted,  leads  to  an 
observable failure. A higher cross section actually means that 
a greater amount of sensitive resources is used. 
C.  Cross Section Dependance on Threads Distribution 
The GPU architecture is regular, such that a thread will 
dispose  of  the  same  amount  and  kind  of  resources  in  any 
SM.  The  designed  codes  are  also  regular,  such  that  all 
threads  have  to  execute  the  same  number  and  kind  of 
operations.  Thus,  increasing  the  number  of  threads  will 
increase  accordingly  the  number  of  internal  registers,  logic 
gates,  and  operations  required  for  computation.  This  may 
suggest  that  the  tested  applications  cross  section  should  be 
directly  dependent  on  the  number  of  instantiated  threads. 
Nevertheless,  experimental  results,  depicted  in  Fig.  4  and 
Fig.  5,  reveal  that  the  cross  section  dependence  on  the 
amount  of  instantiated  threads  is  strongly  related  to  the 
chosen  thread  distribution.  In  particular,  B32G300  has  10 
times more blocks (and thus threads) with respect to B32G30 
(see  Tab.  I),  while its cross section is 14 times higher (see 
Tab. III). Thus, increasing the number of blocks in the code 
has a remarkable effect in increasing its cross section. On the 
contrary, B1024G512 has 16 times more threads with respect 
to  B64G512  (see  Tab.  III),  but  its  cross  section  is  just  1.5 
times higher (see Tab. IV). Increasing the block size seems 
to  barely  affect  the  sensitivity  of  the  GPU,  even  if  the 
amount  of data handled  and the application  complexity  are 
significantly increased. 
The  observed  cross  section  dependence  on  thread 
distribution is due to the GPU parallelism dependence on the 
blocks and warps to be dispatched. Increasing the number of 
threads  keeping  the  block  size  fixed  put  additional  strain 
only  on  blocks  scheduling,  while  increasing  the  number  of 
threads keeping the grid size fixed will intensify the internal 
SM scheduling of warps. To quantify how the two different 
scheduling  mechanisms  affect  the  reliability  of  the  parallel 
application,  the  cross  section  of  the  application  should  be 
normalized on the amount of data processed.  
As said, all threads are independent and execute the same 
operations.  The  normalization  could  then  be  achieved,  in  a 
first  approximation,  by  dividing  the  cross  sections  by  the 
number of executed threads. Results are reported in Tab. V 
and Tab. VI. When the block size is fixed to 32, increasing 
the  grid  size  has  the  effect  of  increasing  the  application 
normalized sensitivity. Thus, each additional block causes a 
sensitivity increment that is greater than the throughput gain 
it  brings.  The  cross  section  variation  is  not  pronounced,  as 
the cross section doubles while the grid size increases by a 
factor of 10 (see the second and last columns of Tab. V). The 
larger number of blocks is, regardless of the block size, the 
higher the scheduler strain will be. Thus, the probability of 
having  a  neutron-induced  output  error  in  the  scheduler  is 
higher when the number of blocks is larger. 
The application cross section normalized on the number 
of  threads  drastically  decreases  when  just  the  block  size  is 
increased.  The  normalized  cross  section  variation 
is 
remarkable,  decreasing  by  10  times  while  the  block  size 
increases by a factor of 16 (see the second and last columns 
of Tab. VI). This means that the SMs internal scheduling is 
reliable when handling a large number of threads. Increasing 
the block size requires a higher amount of computation for 
the internal SM scheduling [22]. However, as experimentally 
demonstrated, when the block size increases the throughput 
increases much more than the additional error rate due to the 
extra scheduling necessary to deal with a higher amount of 
threads. 
Tab. V and Tab. VI attest that the probability of a single 
thread  to  fail  is  not  only  dependent  on  the  instruction 
performed  and  data  elaborated,  but  also  on  the  chosen 
distribution,  being  higher  when  the  grid  size  is  bigger  and 
lower when the block size is bigger. This means that, on a 
GPU, for evaluating a code cross section it is not sufficient to 
predict  the  cross  section  of  a  thread  and  multiply  it  by  the 
number of instantiated threads. 
D.  Mean Executions and Workload Between Failures 
As  demonstrated 
in  the  previous  subsection,  both 
execution  time  and  workload  of  the  designed  parallel 
benchmarks  varies  consistently  accordingly  to  the  thread 
distribution. The cross section may not be a sufficient metric 
to evaluate the reliability of the GPU executing the code, as 
normally  the  number  of  executions  or  the  amount  of  data 
processed  correctly  would  be  a  more  useful  metric.  The 
number  of  executions  correctly  terminated  will  depend  on 
both the cross section and the execution time, while the data 
processed  correctly  will  depend  also  on  the  workload 
computed at each execution. 
460
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
NORMALIZED CROSS SECTION (BLOCK SIZE FIXED TO 32) 
NORMALIZED CROSS SECTION (GRID SIZE FIXED TO 512) 
MTBF[104h] 
MEBF[1010exec] 
MWBF[1013data] 
sums 
mults 
sums 
mults 
sums 
mults 
TABLE VII 
B32G30 
10.60±10% 
11.71±10% 
50.62±10% 
55.93±10% 
48.58±10% 
53.64±10% 
B32G150 
1.50±10% 
2.11±10% 
2.24±10% 
3.18±10% 
10.73±10% 
15.36±10% 
B32G300 
0.53±10% 
0.82±10% 
0.44±10% 
0.69±10% 
4.20±10% 
6.67±10% 
MTBF[104h] 
MEBF[108exec] 
MWBF[1013data] 
sums 
mults 
sums 
mults 
sums 
mults 
TABLE VIII 
B64G512 
57.92±9% 
77.45±9% 
14.32±9% 
19.16±9% 
4.69±9% 
6.26±9% 
B256G512  B1024G512 
37.04±8% 
52.03±8% 
46.64±8% 
63.72±8% 
3.66±8% 
0.67±8% 
0.84±8% 
4.51±8% 
3.49±8% 
4.79±8% 
5.90±8% 
4.39±8% 
mults
sums
]
a
t
a
d
4
1
0
1
[
F
B
W
M
7
6
5
4
3
2
1
0
mults
sums
]
a
t
a
d
4
1
0
1
[
F
B
W
M
7
6
5
4
3
2
1
0
B32G30
B32G150
B32G300
Figure 6: Mean Workload Between Failures of sum and mult when 
the block size is fixed to 32. 
To  define  appropriate  metrics  to  evaluate  the  operative 
reliability  of  the  parallel  code  we  consider  first  the  Mean 
Time  Between  Failures  (MTBF)  of  the  GPU  executing  a 
code,  defined  as  the  average  time  between  two  radiation-
induced failures on the GPU continuously executing a given 
code. By definition, the MTBF for the various distributions 
is evaluated with Eq. 1. 
MTBF
Code
1

V
Code
flux
(1)
  Where σCode  is the experimentally obtained cross section 
of  the  code  (see  Tab.  II  and  IV),  and  flux  is  the  average 
neutron  flux,  i.e.  13  n/(cm2·h)  at  sea  level  [25].  By 
multiplying the code cross section (which is an indication of 
the probability for an impinging neutron to corrupt the code 
execution, expressed in cm2) and the average flux (i.e., the 
number of particles that hit the GPU per cm2 per hour), one 
obtains  the  expected  error  rate  of  the  code  (expressed  in 
errors/hour). Then, as stated in Eq. 1, the MTBF expresses 
the  number  of  hours  that  passes  between  two  consecutive 
neutron-induced  output  errors  for  the  code  having  cross 
section  σCode  executed  on  the  GPU  exposed  to the  average 
flux. The first two rows of Tab. VII and Tab. VIII report the 
MTBF for sums and mults, respectively. 
two  consecutive  errors, 
the  number  of 
executions correctly completed by the GPU will depend on 
the code execution time. A higher MTBF simply attests that 
the  GPU  could  work  for  a  longer  period  of  time  before 
experiencing  a  radiation-induced  error.  Nevertheless,  no 
information on the workload computed during that period of 
time is given yet. 
To  evaluate  how  many  executions  can  be  correctly 
elaborated  by  the  GPU  during  the  MTBF  window,  we 
introduce  a  new  metric,  named  Mean  Executions  Between 
Between 
B64G512
B256G512
B1024G512
Figure 7: Mean Workload Between  Failures  of sum and mult when 
the grid is fixed to 512. 
Failures  (MEBF),  defined  as  the  number  of  corrected 
executions completed between two radiation-induced output 