## ‚ùì Questions and Help
Under PyTorch's no gradient mode, any tensor returned by operations shouldn't
create a new or extend an existing computation graph (i.e. `requires_grad`
should be false and `grad_fn` should be `None`). However, when using
`Tensor.expand` this is clearly not the case. Consider the following code:
    a = torch.tensor(1., requires_grad=True)
    b = a.clone()
    with th.no_grad():
        c = b.expand((1,))
    c.requires_grad # True
Although variable `c` is not attached to the computation graph that contains
`a` and `b`, `c` has its `requires_grad` property set to `True` which will
create a new computation graph. Is this the expected behavior or is this a
bug?