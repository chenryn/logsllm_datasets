• Case 1. The transition probability of the previous sensor
state set at time ti−1 and the current sensor state set at
time ti is zero in the G2G matrix.
• Case 2. The transition probability of the sensor state set
at time ti−1 and the activated actuator at time ti has a
probability of zero in the G2A matrix
• Case 3. The transition probability of the activated actua-
tor at time ti−1 and the sensor state set at time ti has a
probability of zero in the A2G matrix
Ti−1 denotes the Ti − duration. If the transition belongs to
one of the three cases, DICE regards it as a transition violation.
We skip the A2A transition probability by the same reason in
the transition extraction (Section III-B2).
D. Real-time Phase: Identiﬁcation
When a violation is detected, DICE identiﬁes the faulty
devices by diagnosing the problematic context in the iden-
tiﬁcation stage. When a correlation violation is detected due
to the missing of main group, DICE compares the problematic
real-time sensor state set with the list of probable groups (refer
to Section III-C1). DICE examines the bits that differ and ﬁnds
the probable faulty sensors (Fig. 7). If the different bit, Bi, is
a binary sensor, we derive the corresponding binary sensor, Si
as the probable faulty sensor. However, for a numeric sensor
Sj, three bits constitute for a single numeric sensor. Thus, if
one among the three bits is the different bit, we derive the
corresponding Sj, as the probable faulty sensor.
615
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:10 UTC from IEEE Xplore.  Restrictions apply. 
When there is only one probable group, DICE ends the
identiﬁcation step and notiﬁes the user of the probable faulty
devices. (We may add an additional attestation step for a
veriﬁcation purpose.) If there are two or more probable groups,
DICE checks the transition probability from the previous
group at time ti−1 to the current probable groups at time ti.
The groups that have no transition probability are removed
from the probable group.
The similar process is applied when a transition violation is
detected. When detecting a G2G violation during the transition
check (case 1), DICE applies the same identiﬁcation process
as the correlation check case. When detecting a G2A or A2G
violation during the transition check (case 2 and 3), DICE
also selects probable faulty actuator DICE regards the present
activated actuators (G2A) or the previously activated actuators
(A2G) as faulty actuators and adds them to the probable faulty
devices set.
We also applied an additional sophisticated technique when
looking for probable faulty sensors. When there are multiple
probable faulty sensors, DICE cannot instantly determine the
faulty sensor. However, it is likely to appear in the probable
faulty sensor set repeatedly, because a problematic sensor is
likely to generate faults continuously. Therefore, DICE repeats
the identiﬁcation process until the intersecting sensors of all
the probable faulty sensors are below a threshold, numThre
in this case. The numThre value is determined by the number
of faults the system considers. When the system considers a
single-fault case, numThre is set to 1. During the repetition,
DICE skips the correlation and transition check but proceeds
directly to the identiﬁcation step because a fault has already
been detected. Suppose probable faulty sensors at time ti, ti+1,
and ti+2 are {S1, S2, S3}, {S1, S2, S4}, and {S1, S5, S6},
respectively. At time ti, the number of probable faulty sensors
is three, so DICE repeats the identiﬁcation step. At time ti+1,
the number of intersecting sensors of the two sets are two (S1
and S2), so DICE again repeats the step. At time ti+2, S1
is the only intersecting sensor in the three sets. Thus, DICE
outputs S1 as the faulty sensor and starts detecting faults from
the top. Similar to sensor fault identiﬁcation, DICE selects the
faulty actuators that repeatedly appear in the probable faulty
actuator set.
IV. EXPERIMENTAL SETUP
This section describes the datasets used to validate DICE
and the details of the Smart Home testbed we have im-
plemented including the deployment setting and experiment
design.
A. Data Acquisition
To show that our scheme is applicable to real-world Smart
Home environments deployed with various sensor types, we
used diverse datasets that have different sensor/actuator de-
ployments and activity lists. We used a total of ten datasets;
details of the datasets are shown in Table II. The top ﬁve
datasets are the publicly available third-party datasets; the
TABLE II: Datasets
Hours
Binary
sensors
Numeric
sensors
Actuators
Activities
houseA [27]
houseB [27]
houseC [27]
twor [28]
hh102 [28]
D houseA
D houseB
D houseC
D twor
D hh102
576
648
480
1104
1488
600
650
500
1200
1500
14
27
23
68
33
6
6
6
6
6
0
0
0
3
79
31
31
31
31
31
0
0
0
0
0
8
8
8
8
8
16
25
27
9
30
16
14
18
9
26
bottom ﬁve datasets are the data collected from our Smart
Home testbed.
1) Third-party Smart Home Data: We used ﬁve third-party
datasets collected and distributed by Intelligent Systems Lab
Amsterdam (ISLA) [28], [29] and Washington State University
(WSU) [27], [30]. These datasets have also been used in state
of the art sensor failure detection studies [5], [6], [8]. All ﬁve
datasets have Smart Home sensor data of users performing
daily human activities such as dish washing, opening the re-
frigerator, and sleeping. The datasets have different experiment
setups and testbeds. The number and type of the activity
list differs in each dataset. The sensor deployment setups are
also diverse – the location and the type of deployed sensors
are different in each dataset. We have objectively tested the
performance of our system on diverse Smart Home settings.
2) Our Smart Home Data: Although the ﬁve third-party
datasets recreated the real context of use in the testbeds,
the existing third-party datasets lacked realism. They either
did not consider numeric sensors (houseA, houseB, and
houseC) or deployed unreasonably large number of sensors
(twor and hh102). Furthermore, existing studies claimed that
their methods work in heterogeneous environments, but their
dataset used only a few types of sensors. Among the two
datasets that used numeric sensor data, twor only used three
numeric sensors. Even though hh102 deployed 79 numeric
sensors in their testbed, they were all one of the three types – a
battery sensor, light sensor, and temperature sensor. Therefore,
to collect heterogeneous and realistic Smart Home data, we
implemented our own Smart Home testbed. We deployed 31
numeric sensors, 6 binary sensors and 8 actuators in our Smart
Home (Table II). Fig. 8 shows the ﬂoor plan and details of the
sensors and actuators deployed in our testbed.
a) Deployment Setting: Our deployment setting demon-
strates that our Smart Home data reﬂects real-world Smart
Home environments. An IoT environment in general is com-
posed of sensors, actuators, aggregators, a server and a com-
munication channel [25]. We deployed 37 sensors, 8 actuators,
and 8 aggregators with an Ubuntu server that runs DICE.
Raspberri Pi boards were used as the aggregators. The server
used MATLAB software to compute the correlation and tran-
616
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:10 UTC from IEEE Xplore.  Restrictions apply. 
Raspberry Pi        BLE beacon        Amazon echo
      A
Smart switch
Smart bulb
Smart blind
Veranda
L T S M
Toilet
L T S M
Bed
T S M W
Bedroom
L
L T U F G
Kitchen
L T S M
M
U F G
Dining room
L T S M
Living room
L T S M U
U
Entrance
L S U
Fig. 8: Floor Plan of the Smart Home Deployment (Each character stands for the following sensors. L: light, T: temperature,
S: sound, M: motion, U: ultrasonic, F: ﬂame, G: gas, W: weight)
sition extraction, as well as to run the DICE’s detection and
identiﬁcation modules. We built our system on IoTivity [31],
which communicates with the CoAP protocol. We explain the
type of sensors and actuators deployed in our Smart Home in
more detail.
Sensors. We used a total of nine sensor types in our
testbed. Note that the more heterogeneous the sensors are,
the more challenging it is to extract the correlation because
different sensor types react differently to the same event. Thus,
to collect data from a truly heterogeneous environment, we
deployed diverse types of sensors that include light, temper-
ature, humidity, motion, ultrasonic, ﬂame, gas, weight, and
location sensors. The humidity sensors are not shown in the
ﬂoor plan in Fig. 8 because temperature and humidity sensors
are contained in a single chip. However, the chip generates
separate data for the temperature and humidity, so we regarded
them as two different sensor types. We used beacons’ Received
Signal Strength Indicator (RSSI) values received in a smart-
phone to retrieve user’s location information. We deployed
four beacons, each in the kitchen, the bathroom, the bedroom,
and the living room.
Actuators. We deployed the following actuators:
three
smart bulbs (Philips Hue), one smart speaker (Amazon Echo),
two smart switches (WeMo Switch), and two smart blinds. The
actuators were programmed to react to the connected sensor’s
values. Hue’s light came on when a connected motion sensor
detected a nearby motion. WeMo switches activated a fan or
a humidiﬁer based on the sensor readings of the connected
temperature and humidity sensors. We installed motors to the
blinds and programmed it to pull up when the light sensor
value is low, and pull down otherwise.
b) Experiment Design: We collected time-series sensor
and actuator values from the deployed devices periodically
while performing different human activities. We recruited
ﬁve volunteers to join the experiment. All experiments were
approved by the Institutional Review Board (IRB) of our
university. To add objectivity to the data, we imitated the
list and sequence of activities performed in ISLA and WSU
datasets in our testbed. Each of the ﬁve subjects performed a
series of activities that has the same sequence of activities in
the third party dataset. We denoted each dataset collected from
the ﬁve subjects in the testbed as D ‘dataset’. The ‘dataset’
indicates the name of each simulated dataset.
We also covered a few exceptions. We removed the activities
that could not be reproduced in our testbed such as watering
the baobab tree or playing a piano. Furthermore, for many
datasets, there were less number of activities in the actual data
than the number of activities stated in the ofﬁcial document.
For this reason, the simulated dataset have less number of
activities than the corresponding third-party dataset. In case
when the third-party datasets had two toilets in their Smart
Homes, we regarded the two toilets as one in our data.
Prior to the experiment, we developed a smartphone ap-
plication that sequentially displays the name of the activity
with its sequence equal
to that of the third-party dataset.
The application also received data from the beacons and
recorded the beacons’ RSSI values. The following is the
detailed process of the experiment that the subjects performed.
We requested the subjects to perform the activity indicated on
the application. The subjects carried their smartphones and
freely performed the activities without any designated place
or time limit.
B. Faults Generation
Sensor faults are categorized into two classes: fail-stop
and non-fail-stop faults. Fail-stop faults occur when a device
completely shuts down or its function ceases to generate any
data. Non-fail-stop faults occur when a device exhibits an
abnormal behavior and generates incorrect data. Although
the fail-stop faults occur in an unpredictable manner, [4]
have categorized them by their characteristics and listed the
most frequently observed non-fail-stop fault classes, which are
outlier, stuck-at, high noise or variance, and spike. An outlier
is a sensor fault where the sensor value at some point spatially
or temporarily generates an anomalous value. A stuck-at-fault
is when a series of output values is unaffected by the input
and remains the same. A high-noise or variance fault is when
sensor values have a noise or variance beyond the expected
617
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:10 UTC from IEEE Xplore.  Restrictions apply. 
degree. A spike is when multiple data points are greater than
the expected value, making a shape like a spike. We referenced
these four commonly occuring fault types and inserted faults
in our collected data. The sensor type, fault type, and the
insertion time were chosen randomly.
V. EVALUATION
We evaluated DICE with the ﬁve third-party datasets and
ﬁve datasets collected from our testbed to demonstrate the
accuracy and promptness of our system. We used the ﬁrst 300
hours in the dataset as the precomputation period, and used
the rest of the data as the real-time data. We divided the real-
time data into 100 segments that have six hours of length. We
inserted values to generate the fault in each segment mentioned
in Section IV-B. In case the collected data length was less than
600 hours, we reused the segments. The original segment was
used as a faultless data to examine the precision (i.e., false
positive rate) of DICE. The duplicated segment was used as
a faulty data to examine the recall (i.e., false negative rate) of
DICE. In total, we tested 100 faultless data, and 100 faulty
data for each of the ﬁve datasets.
A. Accuracy
We measured the accuracy of our system when detecting and
identifying device faults in ISLA, WSU, and the reproduced
datasets (Fig. 9).
1) Detection Accuracy: Fig. 9a shows the detection ac-
curacy of each dataset, which measures how much DICE
successfully detects the presence of faults. We used false
negative and false positive rate to quantify the detection
accuracy. False negative rate is a measure of how much faulty
segment DICE failed to detect. False positive rate is a measure
of how much faultless segment DICE incorrectly detected as
faulty. DICE achieved an average precision of 98.20% and an
recall rate of 97.91% for the ten datasets. The precision of
the ten datasets were exceed 96%, and the recall of the ﬁve
datasets collected in our Smart Home testbed were exceed
99%. Therefore, DICE is highly accurate in detecting sensor
faults in Smart Homes. We explain why some datasets show
higher or lower detection accuracy than others in Section V-D.
2) Identiﬁcation Accuracy: Fig. 9b shows the identiﬁcation
accuracy of each dataset, which measures how much DICE
successfully identiﬁes the sensor that has generated faults.
We used precision and recall to quantify the identiﬁcation
accuracy. Precision is the percentage of the actual faulty
sensors among the identiﬁed sensors. Recall is the percentage
of the identiﬁed faulty sensors among the actual faulty sensors.
The evaluation results show that DICE achieved an average of
94.9% precision and 92.5% recall. Therefore, we conclude that
DICE identiﬁes the problematic sensors with high accuracy.
houseA relatively showed lower precision and recall
than
other datasets; the reason is analysed in Section V-D. The
identiﬁcation accuracy was lower than the detection accuracy
in general. The reason is simply because detection only
examines whether DICE detects the presence of faults, while
identiﬁcation examines whether the identiﬁed sensor matches
the actual faulty sensor.
TABLE III: Detection Time of the Correlation Check and
Transition Check
Correlation check (ms)
Transition check (ms)
houseA
houseB
houseC
10.5
2.8
3.4
29.0
5.3
9.9
TABLE IV: Correlation Degree and the Number of Sensors of
the Datasets
houseA
houseB
houseC
twor
hh102
DICE
1.4
14
2.9
27
4.6
23
7.2
71
3.8
112
10.6
37
Correlation
degree
Number of
Sensors
3) Actuator Faults: We also evaluated the accuracy of
DICE on actuator faults. Since datasets collected from our