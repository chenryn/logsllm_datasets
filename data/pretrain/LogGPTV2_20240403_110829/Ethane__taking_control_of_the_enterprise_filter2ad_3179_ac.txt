troller.
In this way, Controllers can be largely unaware of each
other: the backups need only contain the registration state and the
network policy (as this data changes very slowly, simple consis-
tency methods can be used). The main advantage of cold-standby
is its simplicity; the downside is that hosts, Switches, and users
need to re-authenticate and re-bind upon the primary’s failure. Fur-
thermore, in large networks, it might take a while for the MST to
reconverge.
The warm-standby approach is more complex, but recovers faster.
In this approach, a separate MST is created for every Controller.
The Controllers monitor one another’s liveness and, upon detect-
ing the primary’s failure, a secondary Controller takes over based
on a static ordering. As before, slowly-changing registration and
network policy are kept consistent among the Controllers, but we
now need to replicate bindings across Controllers as well. Because
these bindings can change quickly as new users and hosts come
and go, we recommend that only weak consistency be maintained:
Because Controllers make bind events atomic, primary failures can
at worst lose the latest bindings, requiring that some new users and
hosts reauthenticate themselves.
The fully-replicated approach takes this one step further and has
two or more active Controllers. While an MST is again constructed
for each Controller, a Switch need only authenticate itself to one
Controller and can then spread its ﬂow-requests over the Controllers
(e.g., by hashing or round-robin). With such replication, we should
not underestimate the job of maintaining consistent journals of the
bind events. We expect that most implementations will simply
use gossiping to provide a weakly-consistent ordering over events.
Pragmatic techniques can avoid many potential problems that would
otherwise arise, e.g., having Controllers use different private IP ad-
dress spaces during DHCP allocation to prevent temporary IP al-
location conﬂicts. Of course, there are well-known, albeit heavier-
weight, alternatives to provide stronger consistency guarantees if
desired (e.g., replicated state machines). There is plenty of scope
for further study: Now that Ethane provides a platform with which
to capture and manage all bindings, we expect future improvements
can make the system more robust.
3.6 Link Failures
Link and Switch failures must not bring down the network as
well. Recall that Switches always send neighbor-discovery mes-
sages to keep track of link-state. When a link fails, the Switch re-
moves all ﬂow table entries tied to the failed port and sends its new
link-state information to the Controller. This way, the Controller
also learns the new topology. When packets arrive for a removed
ﬂow-entry at the Switch, the packets are sent to the Controller—
much like they are for new ﬂows—and the Controller computes
and installs a new path based on the new topology.
3.7 Bootstrapping
When the network starts, the Switches must connect to and au-
thenticate with the Controller.8 Ethane bootstraps in a similar way
to SANE [12]: On startup, the network creates a minimum span-
ning tree with the Controller advertising itself as the root. Each
8This method does not apply to Switches that use an IP tunnel to
connect to the Controller—they simply send packets via the tunnel
and then authenticate.
Switch has been conﬁgured with the Controller’s credentials and
the Controller with the Switches’ credentials.
If a Switch ﬁnds a shorter path to the Controller, it attempts two-
way authentication with it before advertising that path as a valid
route. Therefore, the minimum spanning tree grows radially from
the Controller, hop-by-hop as each Switch authenticates.
Authentication is done using the preconﬁgured credentials to en-
sure that a misbehaving node cannot masquerade as the Controller
or another Switch. If authentication is successful, the Switch cre-
ates an encrypted connection with the Controller that is used for all
communication between the pair.
By design, the Controller knows the upstream Switch and phys-
ical port to which each authenticating Switch is attached. After a
Switch authenticates and establishes a secure channel to the Con-
troller, it forwards all packets it receives for which it does not have
a ﬂow entry to the Controller, annotated with the ingress port. This
includes the trafﬁc of authenticating Switches.
Therefore, the Controller can pinpoint the attachment point to the
spanning tree of all non-authenticated Switches and hosts. Once
a Switch authenticates, the Controller will establish a ﬂow in the
network between itself and the Switch for the secure channel.
4. THE POL-ETH POLICY LANGUAGE
Pol-Eth is a language for declaring policy in an Ethane network.
While Ethane doesn’t mandate a particular language, we describe
Pol-Eth as an example, to illustrate what’s possible. We have im-
plemented Pol-Eth and use it in our prototype network.
4.1 Overview
In Pol-Eth, network policy is declared as a set of rules, each con-
sisting of a condition and a corresponding action. For example, the
rule to specify that user bob is allowed to communicate with the
web server (using HTTP) is the following:
[(usrc="bob")∧(protocol="http")∧(hdst="websrv")]:allow;
Conditions. Conditions are a conjunction of zero or more pred-
icates which specify the properties a ﬂow must have in order for
the action to be applied. From the preceding example rule, if the
user initiating the ﬂow is “bob” and the ﬂow protocol is “HTTP”
and the ﬂow destination is host “websrv,” then the ﬂow is allowed.
The left hand side of a predicate speciﬁes the domain, and the right
hand side gives the entities to which it applies. For example, the
predicate (usrc=“bob”) applies to all ﬂows in which the source
is user bob. Valid domains include {usrc, udst, hsrc, hdst, apsrc,
apdst, protocol}, which respectively signify the user, host, and ac-
cess point sources and destinations and the protocol of the ﬂow.
In Pol-Eth, the values of predicates may include single names
(e.g., “bob”), list of names (e.g., [“bob”,“linda”]), or group inclu-
sion (e.g., in(“workstations”)). All names must be registered with
the Controller or declared as groups in the policy ﬁle, as described
below.
Actions. Actions include allow, deny, waypoints, and outbound-
only (for NAT-like security). Waypoint declarations include a list
of entities to route the ﬂow through, e.g., waypoints(“ids”,“web-
proxy”).
4.2 Rule and Action Precedence
Pol-Eth rules are independent and don’t contain an intrinsic or-
dering; thus, multiple rules with conﬂicting actions may be satis-
ﬁed by the same ﬂow. Conﬂicts are resolved by assigning priorities
based on declaration order. If one rule precedes another in the pol-
icy ﬁle, it is assigned a higher priority.
# Groups —
desktops = ["grifﬁn","roo"];
laptops = ["glaptop","rlaptop"];
phones = ["gphone","rphone"];
server = ["http_server","nfs_server"];
private = ["desktops","laptops"];
computers = ["private","server"];
students = ["bob","bill","pete"];
profs = ["plum"];
group = ["students","profs"];
waps = ["wap1","wap2"];
%%
# Rules —
[(hsrc=in("server")∧(hdst=in("private"))] : deny;
# Do not allow phones and private computers to communicate
[(hsrc=in("phones")∧(hdst=in("computers"))] : deny;
[(hsrc=in("computers")∧(hdst=in("phones"))] : deny;
# NAT-like protection for laptops
[(hsrc=in("laptops")] : outbound-only;
# No restrictions on desktops communicating with each other
[(hsrc=in("desktops")∧(hdst=in("desktops"))] : allow;
# For wireless, non-group members can use http through
# a proxy. Group members have unrestricted access.
[(apsrc=in("waps"))∧(user=in("group"))] :allow;
[(apsrc=in("waps"))∧(protocol="http)] : waypoints("http-proxy");
[(apsrc=in("waps"))] : deny;
[]: allow; # Default-on: by default allow ﬂows
Figure 4: A sample policy ﬁle using Pol-Eth
Unfortunately, in today’s multi-user operating systems, it is dif-
ﬁcult from a network perspective to attribute outgoing trafﬁc to a
particular user. In Ethane, if multiple users are logged into the same
machine (and not identiﬁable from within the network), Ethane ap-
plies the least restrictive action to each of the ﬂows. This is an
obvious relaxation of the security policy. To address this, we are
exploring integration with trusted end-host operating systems to
provide user-isolation and identiﬁcation (for example, by provid-
ing each user with a virtual machine having a unique MAC).
4.3 Policy Example
Figure 4 contains a derivative of the policy which governs con-
nectivity for our university deployment. Pol-Eth policy ﬁles consist
of two parts—group declarations and rules—separated by a ‘%%’
delimiter. In this policy, all ﬂows which do not otherwise match
a rule are permitted (by the last rule). Servers are not allowed to
initiate connections to the rest of the network, providing protection
similar to DMZs today. Phones and computers can never commu-
nicate. Laptops are protected from inbound ﬂows (similar to the
protection provided by NAT), while workstations can communicate
with each other. Guest users from wireless access points may only
use HTTP and must go through a web proxy, while authenticated
users have no such restrictions.
Implementation
4.4
Given how frequently new ﬂows are created—and how fast de-
cisions must be made—it is not practical to interpret the network
policy. Instead, we need to compile it. But compiling Pol-Eth is
non-trivial because of the potentially huge namespace in the net-
work: Creating a lookup table for all possible ﬂows speciﬁed in the
policy would be impractical.
Our Pol-Eth implementation combines compilation and just-in-
time creation of search functions. Each rule is associated with the
principles to which it applies. This is a one-time cost, performed at
startup and on each policy change.
The ﬁrst time a sender communicates with a new receiver, a cus-
tom permission check function is created dynamically to handle all
subsequent ﬂows between this source/destination pair. The func-
tion is generated from the set of rules which apply to the connec-
tion. In the worst case, the cost of generating the function scales
linearly with the number of rules (assuming each rule applies to ev-
ery source entity). If all of the rules contain conditions that can be
checked statically at bind time (i.e., the predicates are deﬁned only
over users, hosts, and access points), then the resulting function
consists solely of an action. Otherwise, each ﬂow request requires
that the actions be aggregated in real-time.
We have implemented a source-to-source compiler that gener-
ates C++ from a Pol-Eth policy ﬁle. The resulting source is then
compiled and linked into the Ethane binary. As a consequence,
policy changes currently require relinking the Controller. We are
currently upgrading the policy compiler so that policy changes can
be dynamically loaded at runtime.
5. PROTOTYPE AND DEPLOYMENT
We’ve built and deployed a functional Ethane network at our uni-
versity over the last four months. At the time of writing, Ethane
connects over 300 registered hosts and several hundred users. Our
deployment includes 19 Switches of three different types: Ethane
wireless access points and Ethane Ethernet switches in two ﬂavors
(one gigabit in dedicated hardware and one in software). Registered
hosts include laptops, printers, VoIP phones, desktop workstations,
and servers. We have also deployed a remote Switch in a private
residence; the Switch tunnels to the remote Controller which man-
ages all communications on the home network. The whole network
is managed by a single PC-based Controller.
In the following section, we describe our Ethane prototype and
its deployment within Stanford’s Computer Science department,
drawing some lessons and conclusions based on our experience.
5.1 Switches
We have built three different Ethane Switches: An 802.11g wire-
less access point (based on a commercial access point), a wired 4-
port Gigabit Ethernet Switch that forwards packets at line-speed
(based on the NetFPGA programmable switch platform [7] and
written in Verilog), and a wired 4-port Ethernet Switch in Linux
on a desktop PC (in software, both as a development environment
and to allow rapid deployment and evolution).
For design re-use, we implemented the same ﬂow table in each
Switch design, even though in real-life we would optimize for each
platform. The main table—for packets that should be forwarded
(see Section 3.2)—has 8,192 ﬂow entries and is searched using an
exact match on the whole header. We use two hash functions (two
CRCs) to reduce the chance of collisions, and we place only one
ﬂow in each entry of the table.
We implemented a second table with 32K entries. We did this
because our Controller is stateless and we wanted to implement the
outbound-only action in the ﬂow table. When an outbound ﬂow
starts, we’d like to setup the return-route at the same time (because
the Controller is stateless, it doesn’t remember that the outbound-
ﬂow was allowed). Unfortunately, when proxy ARP is used, we
don’t know the MAC address of packets ﬂowing in the reverse di-
rection until they arrive. So, we use the second table to hold ﬂow
entries for return-routes (with a wildcard MAC address) as well as
for dropped packets. A stateful Controller wouldn’t need these en-
tries.
Finally, we keep a small table for ﬂows with wildcards in any
ﬁeld. This table is present for convenience during prototyping,
while we determine how many entries a real deployment would
need for bootstrapping and control trafﬁc. It currently holds ﬂow
entries for the spanning tree, ARP, and DHCP messages.
Ethane Wireless Access Point. Our access point runs on a Linksys
WRTSL54GS wireless router (266MHz MIPS, 32MB RAM) run-
ning OpenWRT [8]. The data-path and ﬂow table are based on
5K lines of C++ (1.5K are for the ﬂow table). The local switch
manager is written in software and talks to the Controller using the
native Linux TCP stack. When running from within the kernel, the
Ethane forwarding path runs at 23Mb/s—the same speed as Linux
IP forwarding and L2 bridging.
Ethane 4-port Gigabit Ethernet Switch: Hardware Solution.
The Switch is implemented on NetFPGA v2.0 with four Gigabit
Ethernet ports, a Xilinx Virtex-II FPGA, and 4MB of SRAM for
packet buffers and the ﬂow table. The hardware forwarding path
consists of 7K lines of Verilog; ﬂow entries are 40 bytes long. Our
hardware can forward minimum-size packets in full-duplex at a line
rate of 1Gb/s.
Ethane 4-port Gigabit Ethernet Switch: Software Solution. We
also built a Switch from a regular desktop PC (1.6GHz Celeron
CPU and 512MB of DRAM) and a 4-port Gigabit Ethernet card.
The forwarding path and the ﬂow table are implemented to mirror
(and therefore help debug) our implementation in hardware. Our
software Switch in kernel mode can forward MTU size packets at
1Gb/s. However, as the packet size drops, the switch can’t keep
pace with hashing and interrupt overheads. At 100 bytes, the switch
can only achieve a throughput of 16Mb/s. Clearly, for now, the
switch needs to be implemented in hardware for high-performance
networks.
5.2 Controller
We implemented the Controller on a standard Linux PC (1.6GHz
Celeron CPU and 512MB of DRAM). The Controller is based on
45K lines of C++ (with an additional 4K lines generated by the
policy compiler) and 4.5K lines of Python for the management in-
terface.
Registration. Switches and hosts are registered using a web inter-
face to the Controller and the registry is maintained in a standard
database. For Switches, the authentication method is determined
during registration. Users are registered using our university’s stan-
dard directory service.
Authentication. In our system, users authenticate using our univer-
sity authentication system, which uses Kerberos and a university-
wide registry of usernames and passwords. Users authenticate via
a web interface—when they ﬁrst connect to a browser they are
redirected to a login web-page.
In principle, any authentication
scheme could be used, and most enterprises have their own. Ethane
Switches also, optionally, authenticate hosts based on their MAC
address, which is registered with the Controller.
Bind Journal and Namespace Interface. Our Controller logs
bindings whenever they are added or removed, or when we decide
to checkpoint the current bind-state; each entry in the log is times-
tamped. We use BerkeleyDB for the log [2], keyed by timestamp.
The log is easily queried to determine the bind-state at any time
in the past. We enhanced our DNS server to support queries of
the form key.domain.type-time, where “type” can be “host”, “user”,
“MAC”, or “port”. The optional time parameter allows historical
queries, defaulting to the present time.
Route Computation. Routes are pre-computed using an all pairs
shortest path algorithm [13]. Topology recalculation on link fail-
ure is handled by dynamically updating the computation with the
modiﬁed link-state updates. Even on large topologies, the cost of
updating the routes on failure is minimal. For example, the aver-
 600
 400
 200
)
s
/
s
w