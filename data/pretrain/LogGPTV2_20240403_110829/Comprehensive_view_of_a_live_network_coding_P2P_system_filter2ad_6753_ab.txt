at different stages in the download? etc. Such lack of information
often also prevents calculating the best possible performance of the
system, and therefore, it is hard to estimate the overall efﬁciency.
To ensure that a swarming system is properly evaluated, it is im-
portant to observe the system from various angles and collect in-
formation at multiple points. Besides, it is critical to understand
the upper bound on the capacity of the system, which can only be
determined by knowing the maximum capacity of each node and
their full connectivity pattern.
In our system, we have carefully placed extensive logging at all
points in the system. This permits us to understand how all nodes
are interacting with each other, what kind of connectivity there is
among nodes, how efﬁciently they are using their upload capacity,
or their connectivity pattern, which are all critical to determine how
efﬁciently the system is performing.
Impact of arrival patterns: Swarming protocols have different
relative impact in the system at different stages of the download.
It is important to evaluate swarming protocols during the phases
where they play the biggest role. One common mistake is to eval-
uate swarms during stable phases with many seeds or very high
seed capacities. In such scenarios, most nodes are downloading the
content directly from other nodes that have the full ﬁle, and block
scheduling techniques across users rarely come into place.
During such stable phases, most nodes also have about the same
set of blocks, and inferring what others are missing is a much eas-
ier task. However, other arrival patterns can have a much more de-
manding impact on the swarming protocol. For instance, if there is
a continuous set of newly arriving nodes in the system, then, the set
of missing blocks across nodes can be very different; while newly
arriving nodes can be satisﬁed by any block, older nodes need very
speciﬁc blocks to ﬁnish. This could create a situation where older
nodes in the system do not get priority in downloading their miss-
ing blocks and they are delayed for long periods of time.
Flash crowds are among the most demanding phases in a con-
tent distribution cycle since content resides at a single node, many
users interact at the same time, and there is often a large number of
newly arriving nodes. Flash crowds can last few hours to several
days, after which the distribution cloud moves into a more benign,
stable phase with many seeds and plenty of upload capacity. Actu-
ally, for certain types of content, the stable phase is never reached.
For instance, content that is highly popular and updated frequently,
generates swarms with large number of users requesting the same
ﬁle at once and where the ﬁle only exists at the original server.
Other arrival patterns can also signiﬁcantly stress the swarming sys-
tem (e.g. sudden departures, nodes arriving pre-populated, repeated
ﬂash crowds, etc).
In our system, we have considered various arrival patterns, but
focused on ﬂash-crowd events where most users arrive within few
hours of the ﬁle being published and the ﬁle resides at a single
location with limited upload capacity.
4. RESULTS
4.1 Data Summary
Our prototype implementation was used to distribute four large data
ﬁles to hundreds of users across the Internet. The total trial pe-
riod included roughly four hundred clients. Clients arrived asyn-
chronously after the notiﬁcation of each trial commencement. Each
individual trial only handled one single large ﬁle and trials did not
overlap in time. Table 1 summarizes the data for all four ﬁles deliv-
ered. In this paper, we focus mostly on the results of Trial-4 since
this posed the most stringent load requirements.
During the trial, a single server, which had an upload capacity
of 2.5Mbps was used to publish the ﬁle; the same server served as
registrar and logger.
Table 1: Summary of Trials.
Duration (hours)
File Size (GB)
% Unreachable Nodes
Avg Download Time (hr)
File Blocks
Total Clients
Bytes Sent (GB)
% from Server
Trial 1
78
3.7
1000
87
129.15
33%
64%
13
Trial 2
181
2.8
2000
94
179.63
44%
57%
9
Trial 3
Trial 4
97
3.7
1000
100
208.32
19%
43%
16
69
3.5
1500
72
143.73
16%
40%
12
4.2 Individual Peers
Trial participants were diverse in terms of geographical location,
access capacity and access type (e.g. corporate links, DSL/cable
home users, wireless links). Figure 1 shows the user characteristics
for the ﬁrst trial; the slope of the line that connects point (0, 0) and
a user equals the average download rate of the user. Note that users
were scattered across the world and while most users had download
speeds between 550 Kbps and 1.6 Mbps, others connected through
fast corporate links as well as slow modem connections.
It is interesting to compare the performance of a node behind a
non-reachable NAT (PC NATed) and the performance of the same
node (behind the same access link) without a NAT (PC non NATed).
Note that the non-NATed node gets a much higher throughput than
the NATed one (see Figure 1). The reason being that NATed nodes
cannot be reached by other nodes, and therefore, their pairing pos-
sibilities are signiﬁcantly reduced.
Figure 1: Summary of participating users (Trial-1). In total, there were 87 machines, out of which 31 were publicly addressable. There was
a great variety of clients both in terms of geographic location, and, also, in terms of access capacities (see the dotted lines).
Figure 2 shows the set of neighbors for each peer over time in
Trial 1. Observe that the set of neighbors for each peer varies from
six to eight peers for downloads (except after completed) and from
four to six peers for uploads for most of the trial duration.
Figure 3 also shows the number sessions for each of the 100
peers that participated in the Trial 3. We deﬁne a new session each
time a user resumes its download. Observe that most peers had
multiple sessions (some up to 14), with only 20% of the peers com-
pleting the download in one single session. This indicates a high
level of churn in the system with users coming and going multi-
ple times before they complete. This is an interesting observation
for designining swarming systems. Such behaviour can result in
many nodes entering the system with a diverse set of pre-populated
blocks, which complicates the process of optimal block selection
since local observations may not be representative of the content
existing in other parts of the network.
4.3 System Rates
Using the detailed statistics collected in the logger, we can compute
the overall system throughput, which equals the aggregate down-
load rate, and estimate the contributions of the server, the seeds,
and the clients. We plot those performance statistics for Trial-
4 in Fig. 4. The total throughput of the system follows closely
the total number of active users. The resources contributed by the
server remained constant during the trial and the system maintained
high throughput even during the beginning of the trial, where many
nodes suddenly arrived and no seed nodes existed.
To better understand the system’s performance, we calculate the
user download efﬁciency. For each user, we record its average and
maximum download rate, and its arrival time in the system (peers
started arriving after time 10hr). The download efﬁciency of a user
is the ratio of its average download rate over the maximum; ideally
this ratio should equal 1, however the system is constrained by the
upload capacities and hence lower ratios are expected. We group
the nodes in three groups based on their arrival time, and we report
the average download efﬁciency per group in Table 2. Note that
during the last group interval, there was a large number of seeds
Figure 2: Number of upload and download neighbors for each peer
in Trial 1.
Table 2: Average download efﬁciency over time
Time period
Average
St. Dev
10-20hr
20-40hr
40-60hr
Overall
0.49
0.5
0.52
0.5
0.13
0.16
0.13
0.15
the server would have to contribute if all nodes were uploading to
their maximum capacity (so that the aggregate download rate stays
constant). If some nodes do not contribute with upload capacity,
then, more load will be put in by the server and thus, its share
would increase. Ideally the fair share should be 100% indicating
that users contribute enough resources and, hence, the system could
scale indeﬁnitely. We observe that the average load on the server
is ≈ 100% of its fair share. The high values of fair share towards
the end of the trial indicate a slightly higher usage of the server’s
resources, which are due to the presence of very few nodes being
served mostly from the server.
4.5 Peer’s Performance
We now focus on the performance seen by a typical peer. In Fig. 6
(left), we plot the actual and maximum download and upload rates
for a cable modem user that has a 2.2Mbps downlink capacity and
a 300Kbps uplink capacity. We observe that the average download
rate is ≈ 1.4Mbps and at times reaches the maximum possible rate.
The ﬂuctuations are due to changes in the aggregate upload capac-
ity in the system. The upload rate, on the other hand, is consistently
close to its maximum value.
After the download period ended at time 34.5hr, the peer started
decoding the ﬁle. Decoding ﬁnished at time 35hr, and then the peer
become a seed. The upload rate increased slightly while seeding
since there is no signaling in the reverse (download) direction. The
zero upload rate while decoding is an artifact of the implementation
and will be removed in future versions.
In Fig. 6(right) we plot the percentage of time spent by a repre-
sentative sample of peers on downloading, decoding, and seeding.
Observe that the time spent in decoding is less than 6% of the total
download time; this time can be improved by using on-the-ﬂy de-
coding and exploiting parallelization. It is also worth noting that,
although some users stopped their application immediately after
decoding, other stayed in the system and served other people. The
average seeding time was around 42% of the total time.
4.6 Resource Consumption
We now study the resources used by our network coding imple-
mentation on a typical machine (Pentium IV @2GHz and 512MB
RAM). In Fig. 7(left) we plot CPU usage during the lifetime of the
user. The download period started at time 2hr and ended at time
7.2hr; during that period the CPU overhead was less than 20%.
The dip in CPU’s usage at time 4hr corresponds to a re-start of the
application. The increase of the CPU utilization to 40% after the
end of the download is due to decoding. The CPU activity droped
to less than 10% after decoding and while the node was seeding.
In Fig. 7(right) we show the disk activity over the download.
The spike at time ≈ 7.2hr is due to decoding. (The smaller spikes
while downloading are due to activities unrelated to our P2P ap-
plication.) During the experiment, we used interactive applications
(e.g. word editing and WWW browsing) and did not observe any
decrease in responsiveness. Overall, these results indicate that the
network coding overhead in terms of end-system’s resource con-
Figure 3: Number of sessions for each peer in Trial 3.
Figure 4: System Rates over time.
present, while no seeds existed during the ﬁrst group interval. Ob-
serve that the efﬁciency is similar for all groups, including the ﬁrst
group, implying that nodes used the available resources efﬁciently
even during the early stages of the trial.
4.4 Content Provider Savings
We now study the beneﬁts of using P2P from a content provider’s
point of view. Recall that many hosting sites charge content owners
based on the use of the egress access capacity.1 The savings are
proportional to the ratio of the aggregate download rate over the
upload rate contributed by the server; the former equals the upload
rate of the server in a client-server distribution. We plot that ratio
in Fig. 5(left). We observe that the server saved about one order of
magnitude in egress bandwidth and, hence, in monetary costs. This
is a signiﬁcant beneﬁt, even for our medium sized trial, and will
increase as the number of users increases.
Fig. 5(right) plots server’s fair share ratio over time. To com-
pute the fair share we divide server’s upload rate by the rate that
1Often using the 95th percentile of the maximum rate over a period
of time.
Figure 5: Content Provider effort. Left: Content Provider Savings. Right: Server Share.
Figure 6: Description of a peer’s activity and performance. Left: Peer download and upload rates. Right: Time on each activity (for 5
random peers and average).
Figure 7: Resource consumption on a typical machine during Trial-4. Left: CPU Activity. Right: Disk Activity.
sumption are minimal. We expect the overheads to become negli-
gible as we implement more sophisticated encoding and decoding
techniques.