### System information
  * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)** : No
  * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** : macOS High Sierra 10.13.5
  * **TensorFlow installed from (source or binary)** : binary
  * **TensorFlow version (use command below)** : 1.7.1
  * **Python version** : 2.7.10
  * **Bazel version (if compiling from source)** : N/A
  * **GCC/Compiler version (if compiling from source)** : N/A
  * **CUDA/cuDNN version** : N/A
  * **GPU model and memory** : N/A
  * **Exact command to reproduce** :
    toco \
    --input_file=tmp/killfie_detector.pb \
    --output_file=tmp/quantized_killfie_detector.lite \
    --input_format=TENSORFLOW_GRAPHDEF \
    --output_format=TFLITE \
    --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \
    --input_array=input1 \
    --output_array=output_node0 \
    --inference_type=FLOAT \
    --input_data_type=FLOAT \
    --inference_type=QUANTIZED_UINT8 \
    --quantize_weights=true \
    --mean_value=127.5 \
    --std_value=127.5
### Describe the problem
I am trying to convert a ResNet-50 model to TFLite after quantization. The
quantized graph was obtained using `transform_graph` and
`--transforms='quantize_weights'` on a .pb file. However, on running `toco` on
the quantized graph using the command above, I am first asked to enter the
max/min values and the following error:
    Array input_1, which is an input to the Conv operator producing the output array conv1/convolution, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
On adding `--default_ranges_min=0` and `--default_ranges_max=6` based on the
suggestions here.
When I ran that script, I got this error message:
    F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:519] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Dequantize) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Is there any way around this to get a quantized TFLite model?