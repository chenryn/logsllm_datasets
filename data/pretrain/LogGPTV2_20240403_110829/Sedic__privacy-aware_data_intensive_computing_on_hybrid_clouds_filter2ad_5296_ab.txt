cating data and scheduling the map-reduce operations dynamically
to nodes capable of undertaking the workloads. It also supports a
user-deﬁned combiner structure that allows the nodes running map-
pers to reduce the amount of the data that needs to be delivered to
the reducers, thereby limiting bandwidth consumption. MapRe-
duce has many open-source implementations, among which the
most popular one is Hadoop [28]. Hadoop includes a distributed
ﬁle system (HDFS) that offers reliable and high-performance stor-
age services for MapReduce applications, and a runtime framework
that manages MapReduce jobs through job submission control, task
scheduling and fault tolerance. Another prominent implementation
is Twister [27], which supports iterative MapReduce computations.
With its distributed computing nature, MapReduce has been one of
the most adopted application running on cloud.
Objectives. The original MapReduce framework does not support
the operation over the hybrid cloud to process the data with differ-
ent security levels. We developed Sedic to enhance MapReduce to
make it suitable for performing a privacy-aware data intensive com-
puting. More speciﬁcally, our system has been designed to achieve
the following objectives:
• High privacy assurance. Only the public data, as indicated by
the user, can be handed over to the public commercial cloud.
• Moving workload to the public cloud when possible. When the
private cloud is about to run out of its computing resources, we
need to move as much computation to the public cloud as possible,
given the privacy of user data is preserved.
Users
Sedic
Table 1: Steps for a Privacy-Aware MapReduce
• Label sensitive data, which can be done through a
data-tagging tool (Section 3.1).
• Submit to Sedic labeled data and a MapReduce job.
• Analyze and transform the reduction structure of the
job (Section 4).
• Partition and replicate the data according to security
labels (Section 3.1).
• Create and schedule mappers across the public/private
clouds (Section 3.2).
• Combine the results on the public cloud and complete
the reduction on the private cloud (Section 3.3).
• Scalability. The system should preserve the scalability of the
MapReduce framework. The overhead incurred by privacy protec-
tion should be kept low.
• Limited inter-cloud data transfer.
Inter-cloud data transfer is
traditionally deemed as a very expensive operation. The bandwidth
offered by today’s Internet cannot sustain intensive data exchanges
during the computation involving a large amount of data.
• Ease to use. The system should be transparent to the users, keep-
ing the MapReduce interfaces they are familiar with, and also sup-
port a convenient migration of legacy jobs (i.e, MapReduce pro-
grams) to the new execution framework. These jobs can either run
without any modiﬁcations, or be converted to optimize their perfor-
mance through automatic program transformation.
As discussed before, the MapReduce framework is highly com-
plicated, employing various mechanisms to improve its performance.
Therefore, it is challenging to build privacy protection into the frame-
work to meet the above requirements without undermining its scal-
ability. Here, we present a design that makes this happen, as illus-
trated in Figure 1. We also prototyped the design over Hadoop. The
architecture and individual components of our system are surveyed
in Section 2.2 and elaborated in Section 3 and 4.
2.2 Design
The design of Sedic is meant to be generic, supporting execution
of not only the MapReduce job designed for it but also legacy jobs
without altering the way the user interacts with the original MapRe-
duce platform. It includes an execution framework and auxiliary
tools. The framework performs privacy-aware map-reduce opera-
tions on the data with different security labels (either sensitive or
public). The auxiliary tools are used to help cloud users label their
data and transform the code of their MapReduce jobs for better
performance. What is expected here is that one only needs to in-
dicate which part of the data is sensitive, and the execution frame-
work then takes over to automatically partition the data, create and
schedule map tasks on the data according to its security labels over
the public/private clouds, and strategically arrange the reduce tasks
to minimize inter-cloud communication. The steps for performing
such a privacy-aware MapReduce are summarized in Table 1.
Speciﬁcally, before submitting a MapReduce job to the execu-
tion framework, the user can run our data-tagging tool to locate
sensitive data, for example, the content involving special strings
like credit-card numbers, within her dataset, and mark such data
items as “sensitive”. The labeled data is then uploaded to the pri-
vate cloud, which is connected to the public cloud through a virtual
517private network2. The distributed ﬁle system (DFS) of this hybrid
cloud breaks the data into blocks according to their security labels
and places them across the clouds strategically:
the blocks con-
taining public or sensitive data only are replicated to the public or
private cloud respectively, while the others are propagated through
two types of replicas, the original ones stored on the private cloud
and the sanitized ones, which are cleaned of sensitive information,
disseminated to the public cloud (Section 3.1). Over these data
blocks, the private cloud creates map tasks, which are assigned to
the nodes that host the data (Section 3.2). Such data replication and
task assignment strategies, together our improvement on the execu-
tion mechanism of cloud nodes, ensure that the map tasks are com-
puted correctly and efﬁciently. The results output by the nodes are
delivered to the reducer to complete the computation (Section 3.3).
The reduce operation typically happens on the private cloud, as
it inputs both public and sensitive data. This requires that all the
mapping outcomes produced by the public cloud be transferred
to the private cloud. To avoid the huge communication overhead
incurred thereby and further move the computation to the public
cloud, Sedic employs an automatic program analysis tool to evalu-
ate and transform the reduction structure of a MapReduce job (Sec-
tion 4). Speciﬁcally, when the user uploads her job, the tool is in-
voked to identify the loop within the reducer for folding the input
key-value list. Once such an operation is found to be associative
and commutative, which is often true for real MapReduce jobs, the
loop is extracted to build a combiner and the rest of the reduce code
is exported as a new reducer. The combiner is then deployed to the
public cloud to pre-process mapping outcomes, which helps bring
down the workload of the private cloud and reduce the volume of
the data that needs to be sent to the new reducer.
2.3 Adversary Model
We consider an adversary who intends to acquire sensitive user
information and has a full control of the public cloud. On the other
hand, we assume that the private cloud is trustworthy: speciﬁcally,
the adversary has no access to the nodes on the cloud and its under-
lying network, and therefore cannot launch such attacks as eaves-
dropping. Under this adversary model, Sedic is designed to ensure
the conﬁdentiality of a computation, though the powerful adver-
sary can still compromise its integrity, i.e., rendering the outcomes
of the computation incorrect. As discussed before, our focus on
conﬁdentiality is based on the observation that today’s cloud users
seem to be more willing to live with the risk of getting unreliable
computing results than the threat to their private data. Finally, we
assume that the absence of some records in the data blocks pro-
cessed by the public cloud does not leak out information. Note
that this is all that the adversary can see from a public node: Sedic
ensures that the sensitive information in input/intermediate/output
data never gets out of the privacy cloud.
3. THE EXECUTION FRAMEWORK
In this section, we elaborate our design of the privacy-aware ex-
ecution framework within Sedic, as illustrated in Figure 1, and its
implementation over Hadoop.
3.1 Data Labeling and Replication
Sensitive data labeling. As discussed before, to perform a privacy-
aware MapReduce on her data, all a user needs to do is marking out
the data she deems to be sensitive, and then submits the data and
her computing job to Sedic, just as she would do when interact-
ing with the original MapReduce platform. Data labeling can be
2This can be done, for example, through Amazon Virtual Private Cloud [3]
when the EC2 is used as the public cloud.
done manually when only a very small amount of contiguous sen-
sitive content is involved, or through a data-tagging tool that comes
with Sedic. In our research, we implemented such a tool as a sim-
ple string scanner that searches a given dataset for the keywords
or other text patterns that describe sensitive user information like
social security numbers, credit-card numbers and others. Once the
target is found, a security label is created to record the location
of the information:
the one built into our prototype is a tuple (
hfilename, offset, lengthi). Also, in the case that a dataset
contains multiple ﬁles, individual ﬁles can be automatically labeled
according to their access privileges within ﬁle systems. For exam-
ple, all except those accessible to the public should be marked as
sensitive data. All the labels for a dataset are included in a meta-
data ﬁle, which is submitted, together with the dataset, to Hadoop
Distributed File System (HDFS).
Data uploading. To compute over a dataset, a Hadoop user ﬁrst
needs to upload it to HDFS, which further places and replicates
the data to the nodes across a cloud. Speciﬁcally, HDFS has two
types of nodes: namenode that maintains the meta information of
the whole ﬁle system, particularly the inode for each ﬁle that doc-
uments its attributes (ﬁle name, modiﬁcation time, etc), and datan-
ode that keeps the actual data. The data stored on the datanode is
organized as blocks, each containing 64 MB by default. The lo-
cations of the blocks that belong to the same ﬁle are recorded by
the BlockInfo array within the ﬁle’s inode. To upload a ﬁle, the
user ﬁrst uses her Hadoop client to contact a namenode, which cre-
ates an inode for the ﬁle, locates an available block within HDFS
and sends its whereabout back to the client. According to the in-
formation, the client communicates with the datanode hosting that
block to transfer its data. If the size of the ﬁle exceeds 64 MB, the
client continues to request blocks from the namenode, until all the
data has been uploaded. This process also includes data replication,
which we discuss later.
To protect private user data from the public cloud, which is not
trusted, we modiﬁed the Hadoop client and HDFS to ensure that a
ﬁle with some sensitive content are uploaded through the namen-
ode on the private cloud. Speciﬁcally, the client ﬁrst contacts such
a node to build an inode for the ﬁle, which includes the security
labels associated with the ﬁle. To this end, we extended Hadoop’s
INodeFile class by adding a new private ﬁeld, secureMeta,
which is an array for accommodating locations of sensitive data.
The content of secureMeta is built upon the security labels: for
each label, the namenode puts in the array the location of the data
record that carries the sensitive content indicated by the label. Ac-
cording to such meta-data, whenever the client asks for storage to
upload the data involving sensitive records, the namenode ﬁrst allo-
cates to it a data block from a private datanode, i.e., the one on the
private cloud. The positions of these records are also given to the
datanode as the meta-data of the block to protect the records from
being disclosed during the follow-up replication process.
Data replication. The data uploading process always comes with
replication, which HDFS uses for the purposes of performance en-
hancement and fault tolerance. For each block of user data a Hadoop
client requests space for, a namenode tries to replicate it to multiple
data blocks on different datanodes. The number of replicas, called
replication factor, can be speciﬁed in a Hadoop conﬁguration ﬁle
and is set to 3 by default. The replication process starts from the
ﬁrst datanode receiving data from the client. One by one, the data
is streamed to the next datanode selected by the namenode from
the prior one. This operation can also be triggered by a periodic
checking performed by HDFS: once a block is found to be under-
replicated, the namenode allocates space and directs datanodes to
make copies of it.
The replication mechanism used in Sedic improves over that of
518Figure 1: A Framework for Privacy-Aware MapReduce
Figure 2: Data Replicate
Hadoop, making it suitable for operating on sensitive data in a
hybrid cloud environment. Our approach ensures that the blocks
with all sensitive data are only replicated to private datanodes. On
the other hand, those with public data only are ﬁrst sent to the
public cloud, which causes the their follow-up replications more
likely to happen there, due to the data locality strategy taken by
Hadoop [28]. For the blocks with both public and sensitive data,
the namenode ﬁrst uses private datanodes to replicate them and then
propagates their public data to the public nodes. This arrangement
makes it possible to outsource the computation on the public data
to the public cloud, while keeping the operations on the private data
within the private cloud.
When replicating a data block, a private datanode copies not only
the block but also its meta-data to the next node, if the recipient is
also within the private cloud. Otherwise, the sender ﬁrst zeros out
all the sensitive records on the data block, according to the meta-
data, before propagating it to a public datanode. The public node
also receives the meta-data, which indicates the locations of the
blank records within the block that should not be operated on. As a
result of this replication, the blocks with both public and sensitive
records get two types of replicas, the original versions and the san-
itized ones. Figure 2 illustrates the replication process. Note that
oftentimes, such an inter-cloud data transfer only happens when
a new ﬁle is being uploaded, which places replicas on the public
cloud. After that, the cloud user can run different jobs over this
data placement. Actually, many users today already have their pub-
lic data stored on the commercial cloud.
3.2 Map Task Management
Task creation and submission. After uploading data to HDFS,
the client needs to submit a computing job, which includes the Java
code for the mapper, reducer, other optional functions such as com-
biner and job conﬁguration parameters, particularly the paths for
the input and output ﬁles of the job. The node on the cloud that
receives such a job is called job submission node. This node con-
tains a jobtracker that works with the client to break the job into
tasks before assigning them to the tasktrackers on a set of datan-
Figure 3: Task Management
odes to run. More speciﬁcally, the client provides the jobtracker
with a description of the map tasks over the data blocks of a ﬁle
it submitted. Such a description is a list of FileSplit objects,
with each of them associated with a map task. A FileSplit ob-
ject carries two parameters, the offset for the start of a contiguous
region within the ﬁle, a 64-MB block typically, and its length. A
map task will process all the records within that block, from the
beginning to the end.
To create the map tasks over the blocks with different security
levels, we modiﬁed InputSplit to add in a security tag sensitive.
This tag is set to sensitive when the block associated with a task
carries all private content, and to public when it contains no secret
at all. For the block with both types of data, a Sedic client gen-
erates two map tasks, one sensitive and one public. The offsets
of these tasks point to where public or sensitive data begins and
their length parameters describe the sizes of the contiguous content
with the same security label within a block. To handle the block
with several interleaved segments with disparate security levels, we
changed FileSplit to specify multiple offset-length pairs. The
idea is to use one map task to handle all public data of the block
and the other to process the sensitive one. This treatment ensures
the correctness of the keys produced by these two tasks, which of-
ten depend on the locations of individual records, as well as that
of the values, since each of such segments carries integer number
of records, the smallest data unit a mapper works on, as described
before. The task creation is shown in Figure 3.
Task scheduling. Scheduling those map tasks to cloud nodes is