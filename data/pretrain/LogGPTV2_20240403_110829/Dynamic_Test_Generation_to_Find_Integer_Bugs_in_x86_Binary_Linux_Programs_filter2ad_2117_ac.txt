### Sands of Test Cases for a Single Test Run: Metafuzz

To manage the large volume of test cases generated in a single test run, we designed and built a web service called Metafuzz. This document outlines the problems encountered during the development of Metafuzz and the techniques used to overcome these issues. Additionally, we describe the user experience with Metafuzz and the process of bug reporting.

#### 6.1 Problems and Techniques

**Metafuzz Architecture:**
- **Test Machine:** Generates new test cases for a program and runs them locally. It identifies test cases that exhibit bugs and sends them to Metafuzz.
- **Metafuzz Web Site:** Displays the test cases to the user, along with information about the type of bug and the target program. Users can select and download test cases for further investigation.

**Problems and Techniques:**

1. **Problem: Large Number of Test Cases**
   - **Technique:** We used Valgrind's memcheck to automate the detection of misbehavior in test cases. Memcheck identifies memory leaks, uninitialized values, and memory safety errors. If an error is reported, the test case is saved. We also checked for core dumps and non-zero exit codes.

2. **Problem: Many Error-Causing Test Cases**
   - **Technique:** The Metafuzz front page lists all potential bug reports. Each test machine uploads information about test cases that trigger bugs to Metafuzz.

3. **Problem: Lack of Long-Term Storage on Test Machines**
   - **Technique:** Test cases are uploaded directly to Metafuzz, providing each one with a stable URL. Each test case includes Valgrind output, program stdout, and stderr.

4. **Problem: Rapid Changes in Target Projects**
   - **Technique:** We use Amazon EC2 to automatically attempt to reproduce bugs against the latest version of the target software. A button on the Metafuzz site spawns an EC2 instance that checks out the most recent version, builds it, and attempts to reproduce the bug.

5. **Problem: Specific Reporting Requirements**
   - **Technique:** Metafuzz automatically generates bug reports in the required format. A button on the Metafuzz site allows users to review and send the report to the target softwareâ€™s bug tracker with a single click.

6. **Problem: Multiple Failing Test Cases for the Same Bug**
   - **Technique:** We use the call stack to identify multiple instances of the same bug. Initially, we computed a stack hash, but this had issues with ASLR and slight changes in the code. We developed a fuzzy stack hash that is more forgiving of minor changes, using debug symbol information to identify function names, line numbers, and object files.

7. **Problem: Multiple Valgrind Errors from a Single Test Case**
   - **Technique:** We provide a link on the Metafuzz site to a single test case for each bug bucket. When reporting bugs to developers, we highlight the specific bugs to focus on.

#### 7. Results

**7.1 Preliminary Experience**
- We used an earlier version of SmartFuzz and Metafuzz in a project involving undergraduate students over eight weeks in Summer 2008. After a one-week course in software security, the students generated over 1.2 million test cases and reported over 90 bugs, primarily to the mplayer project, of which 14 were fixed.

**7.2 Experiment Setup**
- **Test Programs:** mplayer (SVN-r28403-4.1.2), ffmpeg (SVN-r16903), exiv2 (SVN-r1735), gzip (version 1.3.12), bzip2 (version 1.0.5), and ImageMagick convert (version 6.4.8-10).
- **Test Platform:** Amazon EC2, using both small and large instances. The experiments took 288 large machine-hours and 576 small machine-hours, costing $172.80.
- **Query Types:** SmartFuzz queries include Coverage, ConversionNot32, Conversion32to8, Conversion32to16, UnsignedOverflow, SignedOverflow, SignedUnderflow, UnsignedUnderflow, MallocArg, and SignedUnsigned.

**7.3 Bug Statistics**
- **Integer Bug-Seeking Queries Yield Bugs:** Figure 6 shows the number of each type of query and the number of distinct bugs found. The signed/unsigned bug queries found the most distinct bugs, demonstrating the effectiveness of our method.
- **SmartFuzz vs. zzuf on mplayer:** SmartFuzz generated 10,661 test cases and found 22 bugs, while zzuf generated 11,297 test cases and found 13 bugs. Despite the overhead, SmartFuzz outperformed zzuf in terms of bug discovery.

**Table 4: Information on Test Program Sizes**
- **mplayer:** 723,468 SLOC
- **ffmpeg:** 304,990 SLOC
- **exiv2:** 57,080 SLOC
- **gzip:** 140,036 SLOC
- **bzip2:** 26,095 SLOC
- **ImageMagick:** 300,896 SLOC

This structured approach ensures clarity, coherence, and professionalism in the presentation of the Metafuzz system and its associated challenges and solutions.