sands of test cases for a single test run. We designed
and built a web service, Metafuzz, to manage the volume
of tests. We describe some problems we found while
building Metafuzz and techniques to overcoming these
problems. Finally, we describe the user experience with
Metafuzz and bug reporting.
6.1 Problems and Techniques
The Metafuzz architecture is as follows: ﬁrst, a Test Ma-
chine generates new test cases for a program and runs
them locally. The Test Machine then determines which
test cases exhibit bugs and sends these test cases to Meta-
fuzz. The Metafuzz web site displays these test cases to
the User, along with information about what kind of bug
was found in which target program. The User can pick
test cases of interest and download them for further in-
vestigation. We now describe some of the problems we
faced when designing Metafuzz, and our techniques for
handling them. Section 7 reports our experiences with
using Metafuzz to manage test cases and report bugs.
Problem: Each test run generated many test cases, too
many to examine by hand.
Technique: We used Valgrind’s memcheck to automate
the process of checking whether a particular test case
causes the program to misbehave. Memcheck looks for
memory leaks, use of uninitialized values, and memory
safety errors such as writes to memory that was not allo-
cated [26]. If memcheck reports an error, we save the test
case. In addition, we looked for core dumps and non-
zero program exit codes.
Problem: Even after ﬁltering out the test cases that
caused no errors, there were still many test cases that do
cause errors.
Technique: The metafuzz.com front page is a HTML
page listing all of the potential bug reports. showing all
potential bug reports. Each test machine uploads infor-
mation about test cases that trigger bugs to Metafuzz.
Problem: The machines used for testing had no long-
term storage. Some of the test cases were too big to at-
tach in e-mail or Bugzilla, making it difﬁcult to share
them with developers.
Technique: Test cases are uploaded directly to Meta-
fuzz, providing each one with a stable URL. Each test
case also includes the Valgrind output showing the Val-
grind error, as well as the output of the program to
stdout and stderr.
Problem: Some target projects change quickly. For ex-
ample, we saw as many as four updates per day to the
mplayer source code repository. Developers reject bug
reports against “out of date” versions of the software.
Technique: We use the Amazon Elastic Compute Cloud
(EC2) to automatically attempt to reproduce the bug
8
against the latest version of the target software. A button
on the Metafuzz site spawns an Amazon EC2 instance
that checks out the most recent version of the target soft-
ware, builds it, and then attempts to reproduce the bug.
Problem: Software projects have speciﬁc reporting re-
quirements that are tedious to implement by hand. For
example, mplayer developers ask for a stack backtrace,
disassembly, and register dump at the point of a crash.
Technique: Metafuzz automatically generates bug re-
ports in the proper format from the failing test case. We
added a button to the Metafuzz web site so that we can
review the resulting bug report and then send it to the tar-
get software’s bug tracker with a single click.
Problem: The same bug often manifests itself as many
failing test cases. Reporting the same bug to developers
many times wastes developer time.
Technique: We use the call stack to identify multiple
instances of the same bug. Valgrind memcheck reports
the call stack at each error site, as a sequence of instruc-
tion pointers. If debugging information is present, it also
reports the associated ﬁlename and line number informa-
tion in the source code.
Initially, we computed a stack hash as a hash of the se-
quence of instruction pointers in the backtrace. This has
the beneﬁt of not requiring debug information or sym-
bols. Unfortunately, we found that a naive stack hash has
several problems. First, it is sensitive to address space
layout randomization (ASLR), because different runs of
the same program may load the stack or dynamically
linked libraries at different addresses, leading to differ-
ent hash values for call stacks that are semantically the
same. Second, even without ASLR, we found several
cases where a single bug might be triggered at multiple
call stacks that were similar but not identical. For exam-
ple, a buggy function can be called in several different
places in the code. Each call site then yields a different
stack hash. Third, any slight change to the target soft-
ware can change instruction pointers and thus cause the
same bug to receive a different stack hash. While we do
use the stack hash on the client to avoid uploading test
cases for bugs that have been previously found, we found
that we could not use stack hashes alone to determine if
a bug report is novel or not.
To address these shortcomings, we developed a fuzzy
stack hash that is forgiving of slight changes to the call
stack. We use debug symbol information to identify the
name of the function called, the line number in source
code (excluding the last digit of the line number, to allow
for slight changes in the code), and the name of the object
ﬁle for each frame in the call stack. We then hash all of
this information for the three functions at the top of the
call stack.
The choice of the number of functions to hash deter-
mines the “fuzzyness” of the hash. At one extreme, we
9
could hash all extant functions on the call stack. This
would be similar to the classic stack hash and report
many semantically same bugs in different buckets. On
the other extreme, we could hash only the most recently
called function. This fails in cases where two seman-
tically different bugs both exhibit as a result of calling
memcpy or some other utility function with bogus ar-
guments. In this case, both call stacks would end with
memcpy even though the bug is in the way the arguments
are computed. We chose three functions as a trade-off
between these extremes; we found this sufﬁcient to stop
further reports from the mplayer developers of dupli-
cates in our initial experiences. Finding the best fuzzy
stack hash is interesting future work; we note that the
choice of bug bucketing technique may depend on the
program under test.
While any fuzzy stack hash, including ours, may acci-
dentally lump together two distinct bugs, we believe this
is less serious than reporting duplicate bugs to develop-
ers. We added a post-processing step on the server that
computes the fuzzy stack hash for test cases that have
been uploaded to Metafuzz and uses it to coalesce dupli-
cates into a single bug bucket.
Problem: Because Valgrind memcheck does not termi-
nate the program after seeing an error, a single test case
may give rise to dozens of Valgrind error reports. Two
different test cases may share some Valgrind errors but
not others.
Technique: First, we put a link on the Metafuzz site to
a single test case for each bug bucket. Therefore, if two
test cases share some Valgrind errors, we only use one
test case for each of the errors in common. Second, when
reporting bugs to developers, we highlight in the title the
speciﬁc bugs on which to focus.
7 Results
7.1 Preliminary Experience
We used an earlier version of SmartFuzz and Metafuzz in
a project carried out by a group of undergraduate students
over the course of eight weeks in Summer 2008. When
beginning the project, none of the students had any train-
ing in security or bug reporting. We provided a one-week
course in software security. We introduced SmartFuzz,
zzuf, and Metafuzz, then asked the students to gener-
ate test cases and report bugs to software developers. By
the end of the eight weeks, the students generated over
1.2 million test cases, from which they reported over 90
bugs to software developers, principally to the mplayer
project, of which 14 were ﬁxed. For further details, we
refer to their presentation [1].
7.2 Experiment Setup
Test Programs. Our target programs were mplayer
version
version
SVN-r28403-4.1.2,
ffmpeg
mplayer
2599
0
1
0
0
3
0
0
1544
1544
ffmpeg
14535
3787
26
16004
121
37803
4003
36945
0
24
exiv2
1629
0
740
0
0
5941
48
4957
0
0
799
gzip
5906
bzip2
12606
0
2
0
0
0
0
24825
1647
24825
0
10
0
0
9109
2840
9104
0
0
Coverage
ConversionNot32
Conversion32to8
Conversion32to16
Conversion16Sto32
SignedOverﬂow
SignedUnderﬂow
UnsignedOverﬂow
UnsignedUnderﬂow
MallocArg
SignedUnsigned
2568
21064
7883
17065
Figure 5: The number of each type of query for each test
program after a single 24-hour run.
exiv2 version SVN-r1735,
SVN-r16903,
gzip
version 1.3.12, bzip2 version 1.0.5, and ImageMagick
convert version 6.4.8 − 10, which are all widely
used media and compression programs. Table 4 shows
information on the size of each test program. Our test
programs are large, both in terms of source lines of code
and trace lengths. The percentage of the trace that is
symbolic, however, is small.
Test Platform. Our experiments were run on the Ama-
zon Elastic Compute Cloud (EC2), employing a “small”
and a “large” instance image with SmartFuzz, zzuf, and
all our test programs pre-installed. At this writing, an
EC2 small instance has 1.7 GB of RAM and a single-
core virtual CPU with performance roughly equivalent
to a 1GHz 2007 Intel Xeon. An EC2 large instance has
7 GB of RAM and a dual-core virtual CPU, with each
core having performance roughly equivalent to a 1 GHz
Xeon.
We ran all mplayer runs and ffmpeg runs on EC2
large instances, and we ran all other test runs with EC2
small instances. We spot-checked each run to ensure
that instances successfully held all working data in mem-
ory during symbolic execution and triage without swap-
ping to disk, which would incur a signiﬁcant perfor-
mance penalty. For each target program we ran Smart-
Fuzz and zzuf with three seed ﬁles, for 24 hours per
program per seed ﬁle. Our experiments took 288 large
machine-hours and 576 small machine-hours, which at
current EC2 prices of $0.10 per hour for small instances
and $0.40 per hour for large instances cost $172.80.
Query Types. SmartFuzz queries our solver with the fol-
lowing types of queries: Coverage, ConversionNot32,
Conversion32to8,
Conversion32to16,
UnsignedOverflow,
SignedOverflow,
SignedUnderflow,
UnsignedUnderflow,
MallocArg, and SignedUnsigned. Coverage queries
refer to queries created as part of the generational search
by ﬂipping path conditions. The others are bug-seeking
queries that attempt
to synthesize inputs leading to
speciﬁc kinds of bugs. Here MallocArg refers to a
convert
0
31121
Coverage
SignedOverﬂow
SignedUnderﬂow
UnsignedOverﬂow
Queries
588068
4586
1915
16073
206
167110
20198
164155
ConversionNot32
Conversion32to8
Conversion32to16
Conversion16Sto32
Test Cases Bugs
19
0
3
4
0
0
3
3
0
5
388
0
116
0
0
49
0
35
0
0
49
Figure 6: The number of bugs found, by query type, over
all test runs. The fourth column shows the number of
distinct bugs found from test cases produced by the given
type of query, as classiﬁed using our fuzzy stack hash.
1377
67
0
0
21
9280
SignedUnsigned
MallocArg
125509
0
6949
30
set of bug-seeking queries that attempt to force inputs
to known memory allocation functions to be negative,
yielding an implicit conversion to a large unsigned
integer, or force the input to be small.
Experience Reporting to Developers. Our original
strategy was to report all distinct bugs to developers and
let them judge which ones to ﬁx. The mplayer devel-
opers gave us feedback on this strategy. They wanted to
focus on ﬁxing the most serious bugs, so they preferred
seeing reports only for out-of-bounds writes and double
free errors.
In contrast, they were not as interested in
out-of-bound reads, even if the resulting read caused a
segmentation fault. This helped us prioritize bugs for re-
porting.
7.3 Bug Statistics
Integer Bug-Seeking Queries Yield Bugs. Figure 6 re-
ports the number of each type of query to the constraint
solver over all test runs. For each type of query, we re-
port the number of test ﬁles generated and the number
of distinct bugs, as measured by our fuzzy stack hash.
Some bugs may be revealed by multiple different kinds
of queries, so there may be overlap between the bug
counts in two different rows of Figure 6.
Furthermore,
The table shows that our dynamic test generation
integer bugs succeed in ﬁnding bugs
methods for
in our test programs.
the queries for
signed/unsigned bugs found the most distinct bugs out
of all bug-seeking queries. This shows that our novel
method for detecting signed/unsigned bugs (Section 5) is
effective at ﬁnding bugs.
SmartFuzz Finds More Bugs Than zzuf, on mplayer.
For mplayer, SmartFuzz generated 10,661 test cases
over all test runs, while zzuf generated 11,297 test cases;
SmartFuzz found 22 bugs while zzuf found 13. There-
fore, in terms of number of bugs, SmartFuzz outper-
formed zzuf for testing mplayer. Another surprising
result here is that SmartFuzz generated nearly as many
test cases as zzuf, despite the additional overhead for
10
mplayer
ffmpeg
exiv2
gzip
bzip
ImageMagick
SLOC
723468
304990
57080
140036
26095
300896