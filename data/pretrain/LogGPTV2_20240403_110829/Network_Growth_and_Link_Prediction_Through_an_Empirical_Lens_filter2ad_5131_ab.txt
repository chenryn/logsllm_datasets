e
v
A
 40
 35
 30
 25
 20
 15
 10
 5
 0
Renren
Facebook
YouTube
t
h
g
n
e
L
t
h
a
P
e
g
a
r
e
v
A
YouTube
Renren
Facebook
 6
 5.5
 5
 4.5
 4
 0  100  200  300  400  500  600  700  800  900
 0  100  200  300  400  500  600  700  800  900
t
i
n
e
c
i
f
f
e
o
C
g
n
i
r
e
t
s
u
C
l
.
g
v
A
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0
Renren
Facebook
YouTube
 0  100  200  300  400  500  600  700  800  900
Day
Day
Day
Figure 2: Average node degree
Figure 3: Average path length
Figure 4: Average clustering coefﬁcient
work properties over its evolution, including average node
degree, path length, and clustering coefﬁcient. Unsurpris-
ingly, average node degree for all three networks grows over
time. In comparison, Renren and Facebook are much denser
than YouTube. Unsurprisingly, networks grow and densify
over time, and their average path length shrinks. YouTube
has the largest path length due to its sparsity.
3.2 Methodology
Existing link prediction studies focus on predicting edges
between two static snapshots [23, 3, 11], and most do not
capture the evolution of fast growing networks such as OSNs
like Facebook, LinkedIn and Renren. In contrast, our work
seeks to answer two key questions:
Q1: Can existing algorithms accurately predict the continu-
ous edge (or link) growth of today’s large, dynamic, online
social networks?
Q2: Can we utilize temporal network data to improve pre-
diction accuracy?
To
Evaluating Link Prediction on Graph Sequences.
answer these questions, we apply a sequence-based frame-
work to evaluate existing link prediction algorithms as the
network grows. We process each dataset to generate a se-
quence of graph snapshots (G1, G2, ..., GT ) while keeping
the number of new edges created in each snapshot constant.
We refer to this number as the snapshot delta. We run each
algorithm in every graph snapshot Gt−1(1  15) while ensuring duration between two succes-
sive snapshots is not too long ( 0, paths
|paths
l=1 βl · |paths
u,v |
u,v : all l-hop paths between u andv
u,v | + ǫ · |paths
u,v |
where πu,v: probability of a random walk from u to v with a restart probability α ∈ [0, 1]
πu,v + πv,u
deg(u)
2|E| πuv(m) + deg(v)
2|E| πvu(m), where πuv(m): probability from u to v after m steps
# of hops on shortest path between u and v
deg(u) · deg(v)
XRX T (u, v) + XRX T (v, u)
where adjacent matrix A ≈ XRX T , X: a |V | × r matrix, R: a r × r matrix
Table 3: The 14 metric-based algorithms used for our study. Notations: given graph G =, u and v are two
graph nodes, Γ(u) denotes the neighbors of node u, deg(u) represents the node degree of u.
low rank approximation (Katzlr) [1] and scalable proximity
estimation (Katzsc) [38]. Our experiments in §4 show that
while more accurate than Katzsc, Katzlr does not scale to
larger networks, since it computes Katz score for all candi-
date node pairs2. Thus for Renren and YouTube, we termi-
nate the Katzlr experiments at snapshots of 65M edges and
5.5M edges, respectively.
In terms of computation cost, the local metrics (CN, JC,
AA, RA, BAA, BCN, BRA) are easy to compute since we
only need to compute each node’s 2-hop neighbors. PA is
also fast because one can optimize the implementation to
only consider top-K node pairs. Even for our largest Renren
graph, the computation for the above eight metrics ﬁnishes
within a few minutes (we run the C++ implementation on 10
standard servers, each with 8 cores and 192GB RAM). The
next three metrics (LRW, PPR and LP) take a few hours to
compute because LRW and PPR require random walk com-
putation while LP requires reaching 3-hop neighbors. Fi-
nally, the most complex metrics (Rescal, Katz and SP) take
a few days to complete since they require node embedding.
We also note that for the classiﬁer-based methods, the com-
putation complexity is dominated by feature calculation, i.e.
computing the above similarity metrics.
4. METRIC-BASED PREDICTION
Our empirical evaluation begins with metric-based pre-
diction algorithms. We seek to understand their prediction
accuracy, and the key factors that lead to prediction errors.
4.1 Experimental Setup
2Even using 8 machines with 192GB memory each, calcu-
lating Katzlr for a Renren snapshot with 185M edges takes
27 days.
Given a sequence of snapshots {G1, G2, ..., GT }, we pre-
dict the new edges to appear in Gt based on observed Gt−1.
For each of the 14 metric-based algorithms, we compute the
similarity metric score for each unconnected node pair, and
select the top k node pairs with the highest score. While
the choice of k may affect prediction accuracy, we use the
ground truth value, i.e. k equals the number of new edges
among Vt−1 nodes appeared in Gt but not in Gt−1. This
allows us to focus on the effectiveness of similarity metrics.
As a baseline for comparison, we also implement a random
prediction algorithm, which uniform-randomly picks k un-
connected node pairs from Vt−1 as the predicted new edges
in Gt.
Performance Metrics. We follow the established prac-
tice of evaluating each link prediction algorithm by compar-
ing results to those from random prediction, i.e.
in terms
of the factor improvement over random [23]. Speciﬁcally,
given a similarity metric M , let EM
represent the set of cor-
t
rectly predicted node pairs that become connected in Gt, i.e.
the overlap between the predicted top k node pairs to connect
and those that actually connect in Gt. Let ER
t be the set of
correctly predicted edges using random prediction, with an
expected size of |ER
t |. Thus the performance metric is the
t |. If
improvement factor or accuracy ratio [23], |EM
t
the ratio is larger than 1, prediction using metric M is more
accurate than random prediction (by predicting k edges).
Note that we choose to use accuracy ratio rather than the
area under the receiver operating characteristic curve (AUC)
because AUC evaluates link prediction performance accord-
ing to the entire list of the predicted node pairs [28], while
our goal is to evaluate the accuracy of top k predicted node
pairs. This allows us to focus on examining the effectiveness
of similarity metrics.
|/|ER
Renren
BRA
BAA
BCN
JC
PPR
LP
LRW
Katzlr
Rescal
SP
Katzsc
PA
100000
o
i
t
a
R
y
c
a
r
u
c
c
A
10000
1000
100
10
1
35M 65M 95M 125M155M185M
Gt Edge Count
(a)
10000
Facebook
o
i
t
a
R
y
c
a
r
u
c
c
A
1000
100
10
Rescal
Katzlr
Katzsc
BCN
LP
BAA
BRA
LRW
PA
PPR
JC
SP
Katzlr
BRA
BAA
BCN
LP
Rescal
LRW
Katzsc
JC
PPR
PA
SP
1000000
YouTube
100000
10000
o
i
t
a
R
y
c
a
r
u
c
c
A
1000
100
360k 465k 570k 675k 780k
4.5M 5.5M 6.5M 7.5M 8.5M
Gt Edge Count
(b)
Gt Edge Count
(c)
Figure 5: Link prediction performance (in terms of accuracy ratio) of all metric-based prediction algorithms. We omit
the results of CN, AA and RA because they perform similarly (slightly worse) than their Local Naive Bayes versions,
i.e. BCN, BAA and BRA. The results for Katzlr in Renren and YouTube are capped to 65M and 5.5M edges due to
computation complexity.
Network
Renren
Facebook
YouTube
JC
1.72
1.21
0.22
BCN BAA BRA
2.40
3.52
4.43
6.17
0.59
0.44
3.22
6.82
0.53
LP
1.75
5.53
0.60
LRW PPR
1.06
2.44
1.06
2.11
0.58
0.23
SP
0.053
0.10
0.0021
Katzlr Katzsc Rescal
0.82
0.091
4.45
9.41
0.98
1.75
0.018
1.85