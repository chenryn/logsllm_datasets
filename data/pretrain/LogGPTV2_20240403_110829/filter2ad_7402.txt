title:D2NN: a fine-grained dual modular redundancy framework for deep
neural networks
author:Yu Li and
Yannan Liu and
Min Li and
Ye Tian and
Bo Luo and
Qiang Xu
D2NN: a Fine-grained Dual Modular 
Redundancy Framework for Deep 
Neural Networks
Yu Li, Yannan Liu, Min Li, Ye Tian, Bo Luo, Qiang XU
CURE lab, Computer Science and Engineering, 
The Chinese University of Hong Kong 
Deep Neural Network (DNN) Applications
Aided medical diagnosis
Face recognition
Malware detection
Autonomous driving
2
Neural Network Lifecycle
Training
Training data
Panda
GPGPU
SOC: NVIDIA Tegra
ASIC
FPGA
3
Hardware Faults
‚Ä¢ Potential faults
Environment Effects
Inherent Property
Malicious Attacks 
‚Ä¢ Malicious attacks
Laser beams
Row-hammer
4
Fault Injection Attacks (FIA) on DNN
Panda
Gibbon
‚Ä¢ At inference stage
‚Ä¢ FIA on model parameters, intermediate results, calculations
‚Ä¢ (e.g., SBA [2], [3])
Runtime Integrity Checking 
[1] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and harnessing adversarial examples,‚Äù arXiv preprint arXiv:1412.6572, 2014
[2] Y. Liu, L. Wei, B. Luo, and Q. Xu. Fault injection attack on deep neural network. (ICCAD17), Nov 2017. 
[3] J. Breier, X. Hou, D. Jap, L. Ma, S. Bhasin, and Y. Liu, ‚ÄúPoster: Practical fault attack on deep neural networks,‚Äù (CCS18)
5
Na√Øve Dual Modular Redundancy (DMR) for DNN
Panda
Gibbon
‚Ä¢ power consumption & chip area√óùüê
What if the chip resources are limited?
Panda
6
Motivation
‚Ä¢ DNN‚Äôs internal redundancy
1. ARES (DAC‚Äô18)
2. Deep Compression (ICLR‚Äô16, etc)
DNN inherently tolerates small faults
Redundancy can be removed by compression
7
Motivation (Cont.)
Dedicated
Shared
Primary network
DNN
Internal redundancy
Secondary network
External redundancy
Dedicated
8
Fine-grained DMR for DNN 
‚Ä¢ Redundancy at the neuron-level instead of network-level
‚Äúmore‚Äù fault sensitive neurons 
 external redundancy
‚Äúless‚Äù fault sensitive neurons 
 internal redundancy
9
Fine-Grained DMR for DNN (Cont.) 
X
Dedicated for primary net
Y
Y‚Äô
1
2
3
4
5
6
7
8
6‚Äô
9
10
9
10
* Note: Output neurons are always duplicated (e.g. neuron 9 and 10)
Dedicated for secondary net
10
Problem Formulation
Target feed-forward DNN
Redundant neuron budget
11
Problem Formulation
DMR
Fault injection cases
Tolerated fault
Detected fault
Missed fault
12
D2NN
‚Ä¢ Given maximum N redundant neurons, we do
‚Ä¢ Critical analysis
‚Ä¢ Evaluate and assign each neuron with a vulnerability value vul
‚Ä¢ e.g., Neuron with higher weight sum has higher vul
‚Ä¢ Protect the ‚Äòmore‚Äô fault-sensitive neurons
‚Ä¢ Select N neurons with higher vul than others
‚Ä¢ DMR topology construction
‚Ä¢ Handle connections among original neuron and the introduced additional neurons 
‚Ä¢ The two sub-nets are equivalent
‚Ä¢ Protect the ‚Äòless‚Äô fault-sensitive neurons
‚Ä¢ By DMR parameter fine-tuning
13
Challenges
How to quantify neurons‚Äô fault sensitivity?
How to ensure faults on explicitly protected neurons can be detected?
How to fine-tune so that ‚Äúless‚Äù fault sensitive neurons are protected by the 
internal redundancy? 
14
Neurons‚Äô Fault-Sensitivity
‚Ä¢ DNN model is non-linear
‚Ä¢ Fault sensitivity varies across different threat models
-|ùëä&‚Äô,&12*|
ùëÜ&‚Äô=)*+,
Weight sum of inter-connections
Layer i
Layer i+1
15
Na√Øve Global Neuron Selection
‚Ä¢ Select neurons only according to their fault-sensitive value
Layer i
Layer i+1
No discrepancy is observed at 
the output when faults on 
explicitly protected neurons!
Protection ineffective!
Sub-net 1
=
Sub-net 2
16
Neuron Visibility
‚Ä¢ Fault-caused discrepancy to be visible at outputs
‚Ä¢ Neuron visibility:
-|ùëä&‚Äô,&12*|ùë£&12,*
ùë£&‚Äô=)*+,
Layer i
Layer i+1
Sub-net 1
‚â†
Sub-net 2
17
Visibility-Aware Neuron Selection
‚Ä¢ balances between fault-sensitivity and visibility
Select one neuron with the 
highest fault-sensitivity
Evaluate its visibility
Layer i
Layer i+1
If visible: 
duplicate
Else:
make it visible
duplicate
Repeat for the rest
Not visible
18
Protecting Shared Neurons
‚Ä¢ After protecting ‚Äúmore‚Äù fault sensitive neurons, two sub-nets are 
equivalent
‚Ä¢ Differentiate the influence of shared neurons to the two sub-nets
ùëå
ùëå‚Ä≤ ‚â†
ùëé+‚àÜ
ùëè+‚àÜ9
=
ùëå
ùëå‚Ä≤
ùëè
ùëé
ùëè
ùëé
ùëé
ùëè
Sub-net 1
Sub-net 2
19
Protecting Shared Neurons
outputs
‚Ä¢ Fine-tune with modified loss function
‚Ä¢ùëôùëúùë†ùë†=ùëôùëúùë†ùë†2+ùëôùëúùë†ùë†?+ùúñ‚àóùëíC‚àëE‚ààG(IEJCIEK)K
‚Ä¢ E.g. ùëé2 and ùëé? have different update speed
‚Ä¢ Differentiate the weight update speed
Cross-entropy loss
ùëè1
ùëé2
ùëé1
ùëè2
ùëôùëúùë†ùë†2
ùëôùëúùë†ùë†?
20
‚Ä¢ Differentiate the gradients of shared neurons w.r.t. the two sub-nets‚Äô 
Gradient of one shared 
neuron to two sub-nets
Set of all shared neurons
‚Ä¢ Fast and efficient 
Experimental Settings
‚Ä¢ Dataset and models
Dataset
MNIST
CIFAR10
Model
LeNetFC
VGG11
Accuracy
98.07%
91.17%
Miss Rate w/o DMR
5.5%
44%
‚Ä¢ Metric
‚Ä¢ miss rate =   -&OOPQRSTUVO
WUU&X‚ÄôPYVPQRSTUVO
‚Ä¢ Redundancy ratio: portion of duplicated neurons
21
Results 1/3
‚Ä¢ Comparison of different neuron selection strategies
)
%
(
)
R
M
(
e
t
a
R
s
s
i
M
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0
 0
random
naive-gloabl
local
visi-aware
 20
 40
 60
 80
 100
Redundancy Ratio (RR) (%)
a. MNIST
)
%
(
)
R
M
(
e
t
a
R
s
s
i
M
 7
 6
 5
 4
 3
 2
 1
 0
 0
 20
‚Ä¢ Visibility-aware selection provides most protection
random
naive-gloabl
local
visi-aware
 40
 60
Redundancy Ratio (RR) (%)
a. CIFAR10
 80
 100
Sharp decline
22
Results 2/3
‚Ä¢ Distribution of neurons among layers with Na√Øve global selection 
)
%
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
(
n
o
i
t
u
b
i
r
t
s
i
D
10 20 30 40 50 60 70 80 90100
Redundancy Ratio (%)
‚Ä¢ => visibility-based neuron selection is significant
layer1
layer2
layer3
layer4
layer5
layer6
layer7
layer8
23
Results 3/3
MR w/o Ô¨Åne-tune
MR w/ Ô¨Åne-tune
DR w/o Ô¨Åne-tune
DR w/ Ô¨Åne-tune
MR w/o Ô¨Åne-tune
MR w/ Ô¨Åne-tune
DR w/o Ô¨Åne-tune
DR w/ Ô¨Åne-tune
)
%
(
)
R
M
(
e
t
a
R
s
s
i
M
 6
 5
 4
 3
 2
 1
 0
 0
 20
 80
Redundancy Ratio (RR) (%)
 40
 60
 100
 80
 60
 40
)
%
(
)
R
D
(
e
t
a
R
d
e
t
c
e
t
e
D
 20
 100 0
)
%
(
)
R
M
(
e
t
a
R
s
s
i
M
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0
 0
 20
 80
Redundancy Ratio (RR) (%)
 40
 60
 100
 80
 60
 40
)
%
(
)
R
D
(
e
t
a
R
d
e
t
c
e
t
e
D
 20
 100 0
a. MNIST
a. CIFAR10
‚Ä¢ Conclusions
‚Ä¢ We can efficiently protect DNN with limited resources
‚Ä¢ Trade-off between system robustness and overhead
24
Future Work
‚Ä¢ Feed-forward neural network only
‚Ä¢ Single fault
‚Ä¢ Evaluation only on limited NN models
25
Thank you!
Q & A