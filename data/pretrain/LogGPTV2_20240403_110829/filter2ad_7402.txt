# D2NN: A Fine-Grained Dual Modular Redundancy Framework for Deep Neural Networks

## Authors
Yu Li, Yannan Liu, Min Li, Ye Tian, Bo Luo, Qiang Xu  
CURE Lab, Computer Science and Engineering, The Chinese University of Hong Kong

## Abstract
Deep Neural Networks (DNNs) are widely used in various applications such as medical diagnosis, face recognition, malware detection, and autonomous driving. However, these networks are susceptible to hardware faults and malicious attacks, which can compromise their reliability and security. This paper introduces D2NN, a fine-grained dual modular redundancy (DMR) framework designed to enhance the robustness of DNNs against faults and attacks while minimizing resource overhead.

## 1. Introduction
### 1.1 Applications of DNNs
- Medical diagnosis
- Face recognition
- Malware detection
- Autonomous driving

### 1.2 Neural Network Lifecycle
- **Training**: Utilizing training data, often on specialized hardware like GPUs, SoCs, ASICs, or FPGAs.
- **Inference**: Deploying the trained model to make predictions or decisions.

## 2. Hardware Faults and Attacks
### 2.1 Potential Faults
- **Environmental Effects**
- **Inherent Properties**
- **Malicious Attacks**:
  - Laser beams
  - Row-hammer

### 2.2 Fault Injection Attacks (FIA)
- **At Inference Stage**: FIAs target model parameters, intermediate results, and calculations.
- **Examples**:
  - SBA [2], [3]
- **Runtime Integrity Checking**:
  - Goodfellow et al., "Explaining and harnessing adversarial examples," arXiv preprint arXiv:1412.6572, 2014 [1]

## 3. Naïve Dual Modular Redundancy (DMR) for DNNs
- **Power Consumption & Chip Area**: Traditional DMR doubles power consumption and chip area.
- **Resource Limitations**: What if chip resources are limited?

## 4. Motivation
### 4.1 Internal Redundancy in DNNs
- **ARES (DAC’18)** and **Deep Compression (ICLR’16)**: These techniques show that DNNs inherently tolerate small faults, and redundancy can be removed through compression.

### 4.2 Dedicated vs. Shared Redundancy
- **Primary Network**: DNN with internal redundancy.
- **Secondary Network**: External redundancy.
- **Dedicated Redundancy**: Each network has its own redundant components.

## 5. Fine-Grained DMR for DNNs
- **Neuron-Level Redundancy**: Instead of duplicating the entire network, D2NN focuses on individual neurons.
  - **More Fault-Sensitive Neurons**: Protected with external redundancy.
  - **Less Fault-Sensitive Neurons**: Protected with internal redundancy.

### 5.1 Problem Formulation
- **Target Feed-Forward DNN**: Enhance robustness with a limited budget of redundant neurons.
- **Fault Tolerance Metrics**:
  - **Tolerated Faults**
  - **Detected Faults**
  - **Missed Faults**

### 5.2 D2NN Approach
- **Critical Analysis**: Evaluate each neuron's vulnerability.
- **Vulnerability Value (vul)**: Higher weight sum indicates higher vulnerability.
- **Redundant Neuron Selection**: Select N neurons with the highest vul values.
- **DMR Topology Construction**: Ensure the two sub-networks are equivalent.
- **Fine-Tuning**: Adjust parameters to protect less fault-sensitive neurons using internal redundancy.

## 6. Challenges
- **Quantifying Neurons' Fault Sensitivity**
- **Ensuring Fault Detection on Protected Neurons**
- **Fine-Tuning for Internal Redundancy**

## 7. Neurons' Fault-Sensitivity
- **Non-Linear Model**: Fault sensitivity varies across different threat models.
- **Weight Sum of Inter-Connections**: Used to quantify fault sensitivity.

## 8. Naïve Global Neuron Selection
- **Selection Based on Fault-Sensitivity**: Ineffective if the discrepancy is not visible at the output.

## 9. Neuron Visibility
- **Visibility-Aware Selection**: Balances between fault-sensitivity and visibility.
- **Process**:
  - Select the most fault-sensitive neuron.
  - Evaluate its visibility.
  - Duplicate if visible; otherwise, make it visible and repeat.

## 10. Protecting Shared Neurons
- **Differentiating Influence**: Modify the loss function to differentiate the influence of shared neurons on the two sub-networks.
- **Modified Loss Function**: \( \text{loss} = \text{loss}_1 + \text{loss}_2 + \epsilon \sum_{E \in G} (I_{EJ} - I_{EK})^2 \)
- **Gradient Differentiation**: Ensure different update speeds for shared neurons.

## 11. Experimental Settings
- **Dataset and Models**:
  - **MNIST**: LeNetFC, 98.07% accuracy
  - **CIFAR10**: VGG11, 91.17% accuracy
- **Metrics**:
  - **Miss Rate**: \( \frac{\text{Number of Missed Faults}}{\text{Total Number of Faults}} \)
  - **Redundancy Ratio**: Portion of duplicated neurons

## 12. Results
### 12.1 Comparison of Neuron Selection Strategies
- **Visibility-Aware Selection**: Provides the most protection.
- **Distribution of Neurons Among Layers**: Significant improvement with visibility-based selection.

### 12.2 Impact of Fine-Tuning
- **Miss Rate and Detection Rate**: Fine-tuning significantly reduces miss rate and increases detection rate.

## 13. Future Work
- **Extension to Other Architectures**: Beyond feed-forward networks.
- **Multiple Faults**: Consider more complex fault scenarios.
- **Evaluation on More Models**: Test on a broader range of neural network architectures.

## 14. Conclusion
- **Efficient Protection**: D2NN efficiently protects DNNs with limited resources.
- **Trade-off**: Balancing system robustness and overhead.

## References
1. I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
2. Y. Liu, L. Wei, B. Luo, and Q. Xu. Fault injection attack on deep neural network. (ICCAD17), Nov 2017.
3. J. Breier, X. Hou, D. Jap, L. Ma, S. Bhasin, and Y. Liu, “Poster: Practical fault attack on deep neural networks,” (CCS18).

## Thank You!
Q & A