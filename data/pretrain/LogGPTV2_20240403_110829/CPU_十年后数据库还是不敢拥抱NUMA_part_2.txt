```
开启NUMA会优先就近使用内存，在本NUMA上的内存不够的时候可以选择回收本地的PageCache还是到其它NUMA 上分配内存，这是可以通过Linux参数 zone_reclaim_mode 来配置的，默认是到其它NUMA上分配内存，也就是跟关闭NUMA是一样的。
**这个架构距离是物理上就存在的不是你在BIOS里关闭了NUMA差异就消除了，我更愿意认为在BIOS里关掉NUMA只是掩耳盗铃。**
以上理论告诉我们：**也就是在开启NUMA和 zone_reclaim_mode 默认在内存不够的如果去其它NUMA上分配内存，比关闭NUMA要快很多而没有任何害处。**
#### UMA和NUMA对比
The SMP/UMA architecture
![img](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_CPU_3e1490cf/十年后数据库还是不敢拥抱NUMA/1230c2cb6619ba39-uma-architecture.png)
The NUMA architecture
![img](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_CPU_3e1490cf/十年后数据库还是不敢拥抱NUMA/30c855d972b2e0ed-numa-architecture.png)
Modern multiprocessor systems mix these basic architectures as seen in the following diagram:
![img](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_CPU_3e1490cf/十年后数据库还是不敢拥抱NUMA/c841e079b67b1156-39354-figure-3-184398.jpg)
In this complex hierarchical scheme, processors are grouped by their physical location on one or the other multi-core CPU package or “node.” Processors within a node share access to memory modules as per the UMA shared memory architecture. At the same time, they may also access memory from the remote node using a shared interconnect, but with slower performance as per the NUMA shared memory architecture.
![03-05-Broadwell_HCC_Architecture](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_CPU_3e1490cf/十年后数据库还是不敢拥抱NUMA/edec60f9cad05a92-03-05-Broadwell_HCC_Architecture.svg)
## 对比测试Intel NUMA 性能
对如下Intel CPU进行一些测试，在开启NUMA的情况下
```
#lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                64
On-line CPU(s) list:   0-63
Thread(s) per core:    2
Core(s) per socket:    16
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 79
Model name:            Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz
Stepping:              1
CPU MHz:               2500.000
CPU max MHz:           3000.0000
CPU min MHz:           1200.0000
BogoMIPS:              5000.06
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              40960K
NUMA node0 CPU(s):     0-15,32-47
NUMA node1 CPU(s):     16-31,48-63
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb invpcid_single pln pts dtherm spec_ctrl ibpb_support tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local cat_l3
#numastat
                           node0           node1
numa_hit               129600200        60501102
numa_miss                      0               0
numa_foreign                   0               0
interleave_hit            108648          108429
local_node             129576548        60395061
other_node                 23652          106041
```
我在这个64core的物理机上运行一个MySQL 实例，先将MySQL进程绑定在0-63core，0-31core，以及0-15,32-47上
用sysbench对一亿条记录跑点查，数据都加载到内存中了：
-   绑0-63core qps 不到8万，总cpu跑到5000%，降低并发的话qps能到11万；
-   如果绑0-31core qps 12万，总cpu跑到3200%，IPC 0.29；
-   如果绑同一个numa下的32core，qps飙到27万，总CPU跑到3200%  IPC: 0.42；
-   绑0-15个物理core，qps能到17万，绑32-47也是一样的效果；
![undefined](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_CPU_3e1490cf/十年后数据库还是不敢拥抱NUMA/2dc4a58d610355d0-1620954918277-c669bd74-df58-4d69-8185-a93f37046972.png)
从这个数据看起来**即使Intel在只有两个NUMA的情况下跨性能差异也有2倍，可见正确的绑核方法收益巨大，尤其是在刷榜的情况下**， NUMA更多性能差异应该会更大。
说明前面的理论是正确的。
来看看不通绑核情况下node之间的带宽利用情况：
![image-20210525151537507](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_CPU_3e1490cf/十年后数据库还是不敢拥抱NUMA/eed601bd96adeeff-image-20210525151537507.png)
![image-20210525151622425](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_CPU_3e1490cf/十年后数据库还是不敢拥抱NUMA/19749ce43337a539-image-20210525151622425.png)
实际在不开NUMA的同样CPU上，进行以上各种绑核测试，测试结果也完全一样。
如果比较读写混合场景的话肯定会因为写锁导致CPU跑起来，最终的性能差异也不会这么大，但是绑在同一个NUMA下的性能肯定要好，IPC也会高一些。具体好多少取决于锁的竞争程度。
## 为什么集团内外所有物理机都把NUMA关掉了呢？
10年前几乎所有的运维都会多多少少被NUMA坑害过，让我们看看究竟有多少种在NUMA上栽的方式：
-   [MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture](http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/)
-   [PostgreSQL – PostgreSQL, NUMA and zone reclaim mode on linux](http://frosty-postgres.blogspot.com/2012/08/postgresql-numa-and-zone-reclaim-mode.html)
-   [Oracle – Non-Uniform Memory Access (NUMA) architecture with Oracle database by examples](http://blog.yannickjaquier.com/hpux/non-uniform-memory-access-numa-architecture-with-oracle-database-by-examples.html)
-   [Java – Optimizing Linux Memory Management for Low-latency / High-throughput Databases](http://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases)
最有名的是这篇  [MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture](http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/)
我总结下这篇2010年的文章说的是啥：
-   如果本NUMA内存不够的时候，Linux会优先回收PageCache内存，即使其它NUMA还有内存
-   回收PageCache经常会造成系统卡顿，这个卡顿不能接受
所以文章给出的解决方案就是（三选一）：
-   关掉NUMA
-   或者启动MySQL的时候指定不分NUMA,比如：/usr/bin/numactl --interleave all $cmd
-   或者启动MySQL的时候先回收所有PageCache
我想这就是这么多人在上面栽了跟头，所以干脆一不做二不休干脆关了NUMA 一了百了。
但真的NUMA有这么糟糕？或者说Linux Kernel有这么笨，默认优先去回收PageCache吗？
## Linux Kernel对NUMA内存的使用
实际我们使用NUMA的时候期望是：优先使用本NUMA上的内存，如果本NUMA不够了不要优先回收PageCache而是优先使用其它NUMA上的内存。
### zone_reclaim_mode
事实上Linux识别到NUMA架构后，默认的内存分配方案就是：优先尝试在请求线程当前所处的CPU的Local内存上分配空间。**如果local内存不足，优先淘汰local内存中无用的Page（Inactive，Unmapped）**。然后才到其它NUMA上分配内存。
intel 芯片跨node延迟远低于其他家，所以跨node性能损耗不大
zone_reclaim_mode，它用来管理当一个内存区域(zone)内部的内存耗尽时，是从其内部进行内存回收还是可以从其他zone进行回收的选项：
zone_reclaim_mode:
> Zone_reclaim_mode allows someone to set more or less aggressive approaches to
> reclaim memory when a zone runs out of memory. If it is set to zero then no
> zone reclaim occurs. Allocations will be satisfied from other zones / nodes
> in the system.
zone_reclaim_mode的四个参数值的意义分别是：
0   = Allocate from all nodes before reclaiming memory
1   = Reclaim memory from local node vs allocating from next node
2   = Zone reclaim writes dirty pages out
4   = Zone reclaim swaps pages
```
# cat /proc/sys/vm/zone_reclaim_mode
0
```
我查了2.6.32以及4.19.91内核的机器 zone_reclaim_mode 都是默认0 ，也就是kernel会：优先使用本NUMA上的内存，如果本NUMA不够了不要优先回收PageCache而是优先使用其它NUMA上的内存。这也是我们想要的
Kernel文档也告诉大家默认就是0，但是为什么会出现优先回收了PageCache呢？
### 查看kernel提交记录
[github kernel commit](https://github.com/torvalds/linux/commit/4f9b16a64753d0bb607454347036dc997fd03b82)
![undefined](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_CPU_3e1490cf/十年后数据库还是不敢拥抱NUMA/4a4f5be382df065a-1620956491058-09a1ebc6-c248-41db-9def-67b4f489c4f4.png)