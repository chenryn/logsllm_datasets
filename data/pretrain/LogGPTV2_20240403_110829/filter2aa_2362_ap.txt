guages such as TTCN. Prebuilt robustness tests are always repeatable and can be
automated in a fashion in which human involvement is minimal. This is suitable
for use in regression testing, for example.
3.9.8
Regression Testing
Testing does not end with the release of the software. Corrections and updates are
required after the software has been launched, and all new versions and patches
need to be verified so that they do not introduce new flaws, or reintroduce old ones.
Post-release testing is also known as regression testing. Regression testing needs to
be very automated and fast. The tests also need to be very stable and configurable.
A minor update to the communication interface can end up invalidating all regres-
sion tests if the tests are very difficult to modify.
Regression testing is the obvious place for recognizing the “pesticide para-
dox.”10 The pesticide paradox is a result of two different laws that apply to soft-
ware testing:
3.9
Black-Box Testing Techniques for Security
95
10Boris Beizer. (1990). Software Testing Techniques, 2nd ed, International Thomson Computer
Press. 
1. Every testing method you use in software development, or every test case
you implement into your regression testing, will leave a residue of subtler
bugs against which those methods and test are ineffectual. You have to be
prepared to always integrate new techniques and tests into your processes.
2. Software complexity (and therefore the complexity of bugs) grows to the
limits of our ability to manage that complexity. By eliminating “easy”
bugs, you will allow the complexity of the software to increase to a level
where the more subtle bugs become more numerous, and therefore more
significant.
The more you test the software, the more immune it becomes to your test cases.
The remedy is to continually write new and different tests to exercise different
parts of the software. Whenever a new flaw is found, it is important to analyze that
individual bug and see if there is a more systematic approach to catching that and
similar mistakes. A common misunderstanding in integrating fuzzing related regres-
sion flaws is to incorporate one single test into the regression test database, when a
more robust solution would be to integrate a suite of test cases to prevent variants
of that flaw.
Therefore, regression tests should avoid any fixed, nondeterministic values
(magic values). A bad security-related example would be regression testing for a
buffer overflow with one fixed length. A flaw that was initially triggered with a
string of 200 characters might later re-emerge as a variant that is triggered with 201
characters. Modification of the tests should also not result in missed bugs in the
most recent release of the software. Regression tests should be constantly updated
to catch newly found issues.
Flaws in the regression database give a good overview of past mistakes, and
it is very valuable information for developers and other testers. The regression
database should be constantly reviewed and analyzed from the learning perspec-
tive. A bug database can reveal valuable information about critical flaws and their
potential security consequences. This, in itself, is a metric of the quality of various
products.
3.10 Summary
In this chapter, we have given an overview of testing approaches that could be use-
ful to you when integrating security testing into a standard quality assurance proc-
ess. Both quality assurance and testing are traditionally feature-oriented approaches,
in which the purpose is to validate that the software works according to the speci-
fications. Security is an abstract term that means different things to different peo-
ple. Security has been seen as an add-on by software development. The main
difference to testing is that security testing does not aim to prove that the software
is secure but to break software by whatever means available.
Various testing approaches are used at different phases of the software devel-
opment life cycle. White-box testing can benefit from the availability of the source
code. Black-box testing, on the other hand, relies on the external interfaces in the
testing. Gray-box testing is a combination of these approaches. Testing also has
96
Quality Assurance and Testing
different purposes. Conformance testing validates the functionality, whereas per-
formance testing tries to find the performance limitations by testing the software
with extreme loads. Various other testing approaches, such as interoperability test-
ing and regression testing, aim at proving that the software is able to work with
other industry products and that no previously known flaws are reintroduced to
the software.
Testing has to be measurable, and quality assurance practices have used various
means of validating the quality of the tests themselves. Specification coverage com-
pares the test efficiency to the specifications that were used to build the tests. Input
space coverage looks at the potential inputs that can be given to the software and
measures the coverage of the tests against those. Interface coverage is a black-box
specific metric that looks at the communication interfaces and the efficiency of tests
to cover them. Finally, code coverage metrics analyze the software and indicate
which parts of the code were touched by the tests.
We concluded the chapter by reviewing the black-box techniques for security
testing. Load testing prepares the QA organization for load-based attacks such as
DDoS attacks. Stress testing looks at the internal threats related to, for example,
low memory resources on embedded devices. Security scanners are not really proac-
tive, but are a requirement when you integrate your own system with other off-the-
shelf systems and platforms. For example, a flaw left into the operating system will
make all good QA practices void if left undetected. Unit testing is the first place to
introduce fuzzing by testing the smallest components of the software through the
available interfaces. Input fault injection, on the other hand, is already one of the
related technologies to fuzzing, almost a synonym. Syntax testing is a technique used
to test formal interfaces such as protocols, and negative testing approaches such as
robustness testing extend those techniques to fuzzing. Finally, regression testing
builds on top of earlier flaws, trying to prevent them from reappearing.
3.10
Summary
97
C H A P T E R  4
Fuzzing Metrics
Fuzzing is not widely used in software development processes, but this will change
as people begin to understand its importance. Fuzzing can easily discover critical
security mistakes, such as buffer overflows, before a product is even launched. The
problem in widespread adoption lies in the resistance to change—i.e., people in-
volved in building the development practices have not yet recognized the need to
adapt security-related tools in their development process. But there are a number of
other reasons. Many software developers do not think they have security bugs, or
they think one solution such as a code auditing tool will fix all their security prob-
lems. One reason for that can also be that they do not know about fuzzers and how
to effectively use them. Also, they could have had bad experiences with difficult-to-
use fuzzers that did not find any flaws after testing. How can the software devel-
opment processes for communication software be changed so that fuzzing will
become an essential part of them? Fuzzing tools are a critical addition when build-
ing any kind of software, especially if reliability and security are seen to be impor-
tant in the end product. The purpose of this chapter is to look at why fuzzing is
important and how this can be explained to your management.
One obstacle in introducing fuzzers to your developers could be that most gen-
erally available fuzzing tools are developed by security people for security people,
and hence are hard to use by people who are not security experts. At the very least,
they were not designed with easy inclusion into an SDLC as a goal. Fortunately,
more and more companies are seeing that proactive security is at its best when inte-
grated to the development process. You cannot test security into a product; it has
to be built in. This sets new requirements for fuzzing tools in how they integrate
with existing development processes. Fuzzer developers need to focus on how they
could improve the available fuzzing tools in such a way that the industry would
also adapt them into their development practices.
But, there are other open questions related to integrating fuzzing with develop-
ment. Where should fuzzing be used in the software engineering process? Who
should be responsible for fuzzing? Is it the developers, the testers, or the people con-
ducting final security checks during acceptance testing? Or is it possible that every-
one should have his or her own set of complementary fuzzing tools?
The developers of various fuzzing tools are sometimes distracted from the
development goals that the software vendors may have. Security people are more
interested in finding yet-another-buffer-overflow in yet-another-software-product
and publishing its details to acquire individual fame and recognition or to trade the
bug in a private sale. Mostly, this is done to promote the sales of their own services
or tools. In fact, to the researchers in the security community, manufacturers are
either adversaries or they are the life blood of their work. Some researchers need the
99
publicity, and the easiest way to gain publicity is to publish the security flaws and
credit the various fuzzing tools for their discovery. And we have to agree that with-
out all that publicity, fuzzing would not be where it is now. Some researchers have
criminal intents and are only interested in collecting zero-day flaws, either to use
them or to sell them. It might not even be in the security researchers’ interests to fix
the flaws, as it would make the findings useless. Both of these approaches have their
down side. Publishing security bugs in widely used products will tarnish the repu-
tation of that vendor. Hiding all the tools and findings will definitely hold back
development in fuzzing, and at the same time this will damage the software indus-
try in the long run.
The first milestone we need to reach is to change the industry so that it under-
stands the importance of fuzzing. For the time being, let us assume that the security
community is on our side to teach us its insights on how to improve the quality
assurance processes. Understanding how things work in software development
projects and various enterprise security assessments is essential for improving prod-
uct security in general. We need the help of the security research community on this,
as security is the key driver here. You cannot motivate the industry through techni-
cal terms alone, although test automation and various metrics in test coverage have
been driving this forward for us during the past few years. It is paramount that
we understand why security problems are allowed to pass through undetected in the
current processes. Understanding the failures in the processes is essential for im-
proving the general security of communication products. Most important, we need
to focus on fixing the broken processes, rather than just looking and pointing out
the failures. 
Fuzzing tools have a few main usages aside from security research. For the secu-
rity community, searching for new avenues in fuzzer development, such as new
fuzzing techniques, test execution monitoring frameworks, attractive target proto-
cols, and critical applications, is the most rewarding task, especially if they can pub-
lish those findings and gain fame for being outstanding security researchers. If used
correctly and by the right people, fuzzing tools are not just hacking tools. There are
so many uses for fuzzer tools that we are currently just scratching the surface on
how the tools can be improved to suit all potential testing needs. The more users for
fuzzers out there and the more recognition this technique will gain. Already today,
software developers use both free and commercial fuzzing tools for proactive secu-
rity testing (“fuzzing before product launch”), and the enterprise end users use the
same fuzzing products as part of their procurement practices in the form of third
party auditing (“fuzzing before purchase”).1 These are probably the most common
usage scenarios, but fuzzing is still not a publicly recognized tool in either of these
processes. Most tools still need to decide which user base they are actually focusing
on. The needs of the quality assurance community are different from the needs of
the security people. Still, both the developers of fuzzers and the users of fuzzers
could learn from past projects on how and where different organizations have
implemented fuzzing into their development cycles.
100
Fuzzing Metrics
1Gadi Evron. Fuzzing in the Corporate World. Presentation at 23rd Chaos Communication Con-
gress (2006). http://events.ccc.de/congress/2006/Fahrplan/events/1758.en.html
What do we know so far? We know that software manufacturers and enter-
prise customers use both commercial and also internally built home-grown fuzzing
tools. Also, for a very long time, different fuzzing techniques have been used in the
hacker community in one form or another. If the security personnel of an enterprise
include people with a “hacker mentality,” they will most probably adapt these same
tools into their work. Whether or not the used tools are built internally by the
corporate security people has had very little impact on the success of past fuzzing
projects. Most of these tools have been home-grown fuzzers, and probably unmain-
tained one-off projects. But since 2001, when the first publicly available fuzzing
tools appeared,2 more and more fuzzing has been conducted using industry stan-
dard tools. These commercial test tool vendors started preaching new ideas to the
industry to create enough pull in the market to support continuous research. As a
result of this push, fuzzers are now more often utilized in the development life cycle.
They are no longer used only by the security community, and the techniques are no
longer secret skills of few people. One reason why this security push into the devel-
opment process has been so successful is that many security-aware companies have
noticed that fuzzers are in use by their adversaries. They see security problems being
announced by hackers on public forums and media. Vendors and manufacturers
need to find the vulnerabilities before hackers find them, so why not do it with the
same tools? This resulted in the first companies publishing details on their cam-
paigns on introducing fuzzing before product release.3 We have also seen some large
telecommunications service providers and carriers demand that all supplied prod-
ucts be certified through fuzzing, or at least tested with particular fuzzing products
that they recommend for use by their suppliers.
To enable the adoption of fuzzing into the product development life cycle, we
need to move the usage scenario from vulnerability or security analysis into earlier
phases, into the standard quality assurance processes. First, we need to understand
how these processes differ. The major difference between vulnerability analysis
(VA)4 and quality assurance (QA) is in the attitude of the testers and in the purpose
of the tests.
The practices of vulnerability analysis are more targeted toward defect discov-
ery, especially when compared to the verification and validation aspects of tradi-
tional quality assurance. The goal or purpose of vulnerability analysis is to study
the completed product for vulnerabilities, using whatever means available. Methods
are typically reactive in nature, i.e., they are based on knowledge of known mis-
takes and problems and in reiteration of those attacks in new scenarios. Unfortu-
nately, the attitude of vulnerability analysis is not to conduct a thorough systematic
test, but to assess a subset of the product and draw conclusions based on those find-
ings. VA never tries to claim that the product is 100% tested and fault-free. The
Fuzzing Metrics
101
2At least the following commercial companies had some soft of fuzzing tools available in early
2002: Codenomicon, Cenzic, eEye, Rapid7, InterWorkingLabs, and SimpleSoft.
3Ari Takanen and Damir Rajnovic. Robustness Testing to Proactively Remove Security Flaws
with a Cisco Case Study. SV-SPIN Seminar. October 26, 2005. www.svspin.org/Events/2005/
event20051026.htm
4Vulnerability assessment is also known as security testing, security assessment, security research-
ing, bug hunting, or even hacking.
metrics related to VA are subjective in nature. The quality of the tests in VA is based
on the skills, tools, and knowledge base of the people conducting the security analy-
sis. VA processes are difficult to define, and the results are difficult to measure. VA
will sometimes use tools such as reverse engineering and source-code auditing, as
these techniques can potentially find vulnerabilities that black-box techniques are
not capable of finding.
Fuzzing as part of vulnerability analysis can be a true black-box technique,
meaning no source code is needed in the process. Security problems are analyzed
after the product is complete, and the people conducting the assessment are typi-
cally not involved in the development and testing phases of the software development.
The design documents and source code are usually not available in a vulnerability
analysis process. The system under test (SUT) can truly sometimes be a black box
to the security auditor, a device with no methods of instrumenting or monitoring
the internal operation of the device under test (DUT). Security auditors study the
software from a third-party perspective. This tiger-team approach5 used in vulner-
ability analysis is similar to the practices used in the military. A team of security
experts (a red team) will masquerade as a hostile party and try to infiltrate the secu-
rity of the system with the same tools and techniques real hackers would use. A
study of the vulnerabilities can be done with or without knowing anything about
the internals of the system. Access to source code may improve the results, i.e.,
enable the auditors to discover more vulnerabilities, but at the same time this can
compromise the results, as the findings might be different from what real adver-
saries would likely find. 
In summary, fuzzing as part of VA does not try to verify or validate a system,
but rather attempts to find defects. The goal is to uncover as many vulnerabilities
in the system as possible within a given time frame and to provide a metric of the
security of the system, a security assurance level.
On the other hand, the goal of quality assurance is to follow a standard process
built around the system requirements and to validate that those requirements are
met in the product. This verification and validation (V&V) aspect of quality assur-
ance has driven testing into the feature-and-performance-oriented approach that
most people identify it with. Testing in most cases is no longer aiming to find most
flaws in the product, but to validate a predefined criterion for acceptance or con-
formance to a set of requirements.
Testing experts such as Boris Beizer have, since at least 1990,6 been proposing
that testers should look back and shift their focus from rote verification and vali-
dation toward true discovery of flaws. Similar to VA, the purpose of testing should
also be to find defects, not to rubber stamp a release. Fortunately, fuzzing as a secu-
rity testing technique has emerged to teach this to us the hard way. Any negligence
in finding security flaws is unacceptable, and therefore the need for security has the
potential to change the behavior of testers in QA processes.
102
Fuzzing Metrics
5M. Laakso, A. Takanen, J. Röning. “The Vulnerability Process: A Tiger Team Approach to
Resolving Vulnerability Cases.” In Proceedings of the 11th FIRST Conference on Computer Secu-
rity Incident Handling and Response. Brisbane, Australia. June 13–18, 1999.
6Boris Beizer. (1990). Software Testing Techniques, 2nd ed. International Thomson Computer
Press.
Security research is still immature compared to the legacy of research in the
fields of software development and testing. Security researchers should try to learn
from the experiences of computer science. This is especially true in the areas of met-
rics. In the rest of this chapter we will study metrics and techniques drawn from
both security assessment and quality assurance, but our focus is in analyzing them
from the fuzzing perspective. One of the goals in this book is to propose some rec-
ommendations on how vulnerability assessments could be integrated to quality
assurance to enable the discovery of vulnerabilities earlier in the software life cycle.
4.1
Threat Analysis and Risk-Based Testing
To effectively introduce fuzzing into vulnerability analysis processes or quality
assurance processes, we need to conduct a careful threat analysis that studies the
related threats, vulnerabilities (or exposures), and the assets that need protecting.
Threat analysis is often identified with security assessment practices, but for our
purpose it is also very similar to the risk assessment process used in risk-based test-
ing. For quality assurance people, fuzzing is just one additional risk-based testing
technique. For security personnel, fuzzing is just one of the available tools available
for eliminating security-related flaws from software. For both, all available options
need to be carefully analyzed to make a decision whether to invest time and resources
in fuzzing and how to apply fuzzing to the development process.
Threat analysis often starts from identifying the security requirements for a sys-
tem. A simple division of security requirements could stem from the well-known
set of security goals, namely
1. Confidentiality;
2. Integrity;
3. Availability.
These and other security goals can be specified in a security policy for a spe-
cific network service or for an individual product. The same requirements can also
be studied from a quality perspective. For each security requirement, we can then
analyze
1. Threat agents and events;
2. Available attacks that these threat agents can execute to realize an event;
3. Potential weaknesses, vulnerabilities, or flaws that these attacks would
exploit.
The components of threat analysis mentioned above are assumptions—i.e., all
threats, attacks, and vulnerabilities would be impossible to enumerate. But even a
subset of the reality can already provide some level of assurance about the future
probability of an incident against the security goals. Well-defined security goals or
security policies can immediately eliminate some security considerations. For exam-
ple, if confidentiality is a nonissue for a specific product, this will immediately elim-
inate the need for further threat analysis of attacks and vulnerabilities related to
4.1
Threat Analysis and Risk-Based Testing
103
confidentiality. On the other hand, if availability is of the highest importance, then
it is obvious that denial-of-service-related attacks are extremely relevant to the
application at hand.
Threat analysis often just takes an existing risk analysis further and makes the
results more applicable to product development. We will not study the methods for
enumerating risks, but study threat analysis independently from any possible risk
assessment. If a list of threats is already available, it can be used as a starting point,
or it can be used later in the process to verify that all risks were covered with a sec-
ond parallel process.
There are many possible methodologies that can be used to perform threat
analysis. The most common techniques are