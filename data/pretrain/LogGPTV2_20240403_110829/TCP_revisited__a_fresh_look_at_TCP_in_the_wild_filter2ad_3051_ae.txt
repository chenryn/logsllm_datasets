9
16
6
2
8
953
56
847
61
651
41
1622
50
16
29
0
3
2
ICW Total
18234
BU
18609
BS1
18342
BS2
18468
BS3
15763
BS4
VPN
2135
18004
DSL
88.3% 8.0%
3.1% .2% .3% .08%
86.0% 10.2% 3.4% .1% .3% .05%
4.1% .2% .3% .02%
86.2% 9.2%
6.4% .2% .3% .05%
83.9% 9.1%
5.6% .3% .3% .04%
85.1% 8.6%
94.5% 3.2%
2.0%
0%
77.2% 6.9% 14.2% .6% 1.0% .03%
.4%
0%
F
D
C
1
0.8
0.6
0.4
0.2
0
0
BU
BS1
BS2
BS3
BS4
VPN
DSL
15000
5000
10000
ICW Size (Bytes)
Figure 8: Distribution of ICW Size
RFC 2581 [6] requires that ICW must be less than or equal to
2*MSS bytes and not exceed 2 segments. RFC 3390 [5] updates
RFC 2581 by changing the upper bound to min(4∗M SS, max(2∗
M SS, 4380 bytes))8.
In our measurement results shown in Ta-
ble 5, most ﬂows have ICW of 2 MSS, while we also observe
small fraction of ﬂows (0.4% to 1.63%) whose slow start begins
with ICW of more than 4 MSS. Figure 8 plots the distribution of
ICW size in bytes, where ICWs mainly concentrate in two clusters:
2520 to 2920 bytes and 1460 × 3 = 4380 bytes, corresponding to
2*MSS and the numerical upper bound deﬁned in RFC 3390, re-
spectively. For DSL, about 14% ﬂows form a third cluster around
5800 bytes (4*MSS, as shown in Table 5), which is an inappropri-
ately large ICW. We also observe extreme cases where ICW is as
large as 9KB. For each dataset, we report the percentage of ﬂows
with ICW greater than min(4∗M SS, max(2∗M SS, 4380 bytes))
as follows. BU: 3.6%, BS1: 3.8%, BS2: 4.6%, BS3: 6.9%, BS4:
6.2% VPN: 2.3%, DSL: 15.8%. OS detection results (Nmap only
ﬁngerprints 24% of servers) show that almost all OS implementa-
tions of ﬂows with inappropriately large ICWs are Linux 2.6.x or
FreeBSD 6/7 (ICW is controlled by sysctl_tcp_iw variable in
Linux kernel).
6.2 Irregular Retransmission
Recall that in §4.2, irregular retransmissions deviate from the
usual case where the sender slows down its sending rate when the
retransmission increases. Rate tracking graph (RTG) statistically
detects irregular retransmissions for ﬂows with overall high retrans-
mission rate. We ﬁrst present characterizations of retransmission
rate and an overview of correlation coefﬁcients of RTGs, then ana-
lyze the irregular retransmissions detected by our algorithm.
Figure 9(a) plots the distribution of retransmission rate. More
than 55% of ﬂows have almost no (less than 0.01%) retransmission.
At least 80% of ﬂows have retransmission rate of less than 1%.
There exists little diversity in retransmission behavior across seven
8Some network cards (e.g., Intel Pro 1000 NIC) provide a function
called TCP Segmentation Ofﬂoad (TSO) that allows the kernel to
transmit packets with very large MSS (e.g., 64KB). However, the
NIC will eventually break the data down into proper MTU-sized
packets (e.g., 1448 bytes).
Figure 9: (a) Distribution of packet retransmission rate (b) Dis-
tribution of correlation coefﬁcient of RTG for different window
size W
datasets. The retransmission rate of VPN is lowest on average,
while DSL has more ﬂows with retransmission rate higher than 5%.
Next, we pick ﬂows with retransmission rate higher than 10%
and generate their RTGs by applying Algorithm 2 with preprocess-
ing described in §4.2. Figure 9(b) plots the distribution of cor-
relation coefﬁcients of all RTGs for BU (lowest on average) and
DSL (highest on average), using tracking window sizes of 50KB,
100KB, 200KB and 400KB. Clearly, for each window size W ,
majority of ﬂows exhibit strong positive correlation between the
transmission time for W bytes and the retransmission rate. On the
other hand, we are more interested in understanding the opposite
part, irregular retransmissions, which are conservatively deﬁned
here as max{CC50KB , CC100KB , CC200KB , CC400KB } < 0.1.
As shown in Table 6, those irregular ﬂows account for 2.5% to 5%
of ﬂows with retransmission rate higher than 10%. By carefully
analyzing each irregular ﬂow, we classify them into ﬁve categories.
Category 1. There exists clear indication that the retransmission
behavior does not conform to RFC-compliant TCP speciﬁcations.
In particular, we observe cases where (i) the sender retransmits a
train of packets within one RTT; (ii) the sender retransmits packets
not lost; (iii) the sender injects large duplicated bursts to the link.
Except for VPN, they account for 20% to 50% cases in each dataset.
Three examples are shown in Figures 10(a) to (c). In Figure 10(a),
at t1 = 25.54s, the sender retransmits 18 identical packets sent at
t2 = 25.44s. Note that the interval is t1 − t2 = 0.1s, less than
RT T = 0.2s that can be measured from the slow starts observed
in the ﬂow. In Figure 10(b), at t = 11.27s, the sender retransmits
received packets (indicated by the ACKs) sent between t = 10.58s
to t = 10.81s, resulting in a large number of duplicated ACKs
observed from t = 11.84s. In Figure 10(b), there is no observed
duplicated ACKs that may trigger retransmission. In Figure 10(c),
85Figure 10: Examples of irregular retransmissions
the sender injects two large overlapped bursts of 500KB into the
link at t = 2.44s and t = 2.66s. Above behaviors may be caused
by bugs or intentionally aggressive implementation of TCP.9
Category 2. Rate limited by sender. As discussed in §4.2, the
sender does not fully utilize the congestion window even if the
congestion window is reduced due to packet loss, since the sender
does not produce data fast enough. Therefore, when the loss is de-
tectable, the sender can possibly keep retransmitting packets with-
out slowing down. Two examples are illustrated in Figures 10(d)
and (e). We identify this category by observing (i) the ﬂow clock
is detected; thus we can separate the ﬂow into ﬂights based on the
clock; (ii) the last packet of each ﬂight is not transferred in MSS;
(iii) the ﬂow does not fall into Category 1 or 3. This category ac-
counts for 30% to 50% of irregular ﬂows for each dataset except
for VPN. Note that the congestion window reduction described in
RFC 2581 [6] is based on FlightSize, deﬁned as is the amount of
outstanding data on the wire. Even if an application is not fully
utilizing current congestion window, a loss should also cause a re-
duction in the observed transmission rate. We therefore believe that
this category also relates to non-standard TCP implementations.
9Nmap shows that ﬂows in Category 1 are mostly generated by
Linux 2.6.x and Windows 2003, while we also observe other OS
versions such as ﬁrmware OS for embedded network devices (e.g.,
routers). It is difﬁcult for us to reproduce the buggy TCP behaviors
because many servers return HTTP 4xx codes, and the bugs seem
to be triggered nondeterministically.
Category 3. Partial Overlap of Sequence Numbers. Irregular
ﬂows in this category have strong frequency characteristics based
on which we can separate the ﬂow into ﬂights. The ﬂow exhibits
a strange pattern that the sequence numbers of consecutive ﬂights
partially overlap. For example, in Figure 10(f), each ﬂight contains
16 ≤ k ≤ 20 packets; after sending ﬂight i : [m, m + k), the
sender retransmits packet m + k − c and the next ﬂight starts from
m + k − c + 1. We observe 3 to 6 such ﬂows in each unidirectional
dataset.
Category 4. Gaps or Rate change (false positives). The gener-
ated RTG shows a negative correlation due to gaps that were not
removed, or due to a dramatic rate change. An example is shown
in Figure 10(g). Before t = 30s, the sending rate and retransmis-
sions are high; both decrease after t = 30s, causing the undesirable
negative correlation. The overall false positive rate is 16/301.
Category 5. Unknown cases. It includes other cases that do not
fall into the above four categories. We are unable to infer the cause
of irregular retransmission, especially for unidirectional datasets.
An example is shown in Figure 10(h).
6.3 Flow Clocks
We make four key observations from our analysis regarding ﬂow
clocks. In our datasets, (i) more than half of our ﬂows do not have
distinguishable ﬂow clocks; (ii) a signiﬁcant number of ﬂows have
non-RTT based ﬂow clock around 100ms; (iii) ﬂows with large
non-RTT based ﬂow clock tend to have more consistent ﬂight size;
86e
d
u
t
i
l
p
m
A
0.1
0.08
0.06
0.04
0.02
0
50
42.5%
Yes
No
57.5%
VPN: No ﬂow clk
VPN: RTT based clk
VPN: non-RTT based clk
VPN: Unknown RTT
200
250
100
150
Frequency (Hz)
Has Clock
BU
Table 7: Existence of ﬂow clocks.
BS3
46.3%
53.6%
DSL: No ﬂow clk
BS1
BS2
43.3% 44.7%
56.6% 55.3%
68.5%
2.6%
1.9% DSL: non-RTT based clk
27.0%
DSL: Unknown RTT
DSL: RTT based clk
BS4
45.9%
54.1%
69.0%
8.8%
12.7%
9.5%
Figure 11: Freq. spectrum of a ﬂow with a bottleneck link of
1Mbps
(iv) ﬂows with non-RTT based clocks are more likely to transfer
data with an inappropriately large congestion window (violating
RFC 2581 [6] and RFC 2988 [30]) after a relatively long period of
idle time.
RTT is a key parameter for understanding the origin of ﬂow
clocks. We tried three ways to estimate RTT: (1) measure the delay
between SYN-ACK and ﬁrst data packet; (2) measure the delay be-
tween SYN and ﬁrst ACK packet; (3) measure the delay between
ﬁrst two ﬂights in slow start. However, none of them yields satisfac-
tory results, since (1) overestimates RTT for most ﬂows due to the
reason explained in §4.1; (2) may overestimate RTT for some ﬂows
due to delayed ﬁrst ACK [19]; and (3) also overestimates RTT in
many cases, compared with (2). Finally we picked the minimum
value of (2) and (3) as an approximation of RTT. Such compromise
requires bidirectional data, so we did not report RTTs for BU and
BS1 to BS4. We clearly cannot use previous methods [33, 36] that
implicitly assume RTT as ﬂow clocks to calculate RTT.
Table 7 shows the existence of ﬂow clocks in our datasets. For
unidirectional traces BU and BS1 to BS4, more than half of the
ﬂows do not have distinguishable ﬂow clocks; nearly 69% of ﬂows
fall into this category for VPN and DSL. For these latter two bidi-
rectional datasets, we further classify ﬂow clocks into RTT based
and non-RTT based, using the empirically selected criteria that
|clock − RT T |/RT T < 20%. The ratio of RTT based and non-
RTT based clocks are 1:0.73 and 1:1.44 for VPN and DSL, respec-
tively.
Figure 12(a) plots the distribution of ﬂow clocks. For each dataset,
among ﬂows with a measurable ﬂow clock, about half of ﬂows
have clocks less than 150ms, while considerable number of ﬂows
have larger clocks up to 2000ms. We found a signiﬁcant number of
ﬂows (15% for BU and BS1 to BS4, 10% for DSL) with ﬂow clock
around 100ms (10Hz). Based on Figure 12(b), which plots RTT
and non-RTT based clocks for DSL10, these clocks are mostly non-
RTT based. By examining their IP and port numbers, we found
that many of them are ﬂows from video/audio streaming servers
such as imeem, Pandora and streamtheworld11. Furthermore, in
Figure 12(b), 30% of ﬂows in DSL dataset have non-RTT based
clock around 18ms. They are from a wide range of Web servers
and CDN servers. However, we suspect that such non-RTT based
ﬂow clock is caused by the link layer, as it is known that if a ﬂow’s
rate is limited by its bottleneck link, then the packets will be nearly
equally-spaced [35].
In our controlled experiment, we created a
bottleneck link of 1Mbps using a Linksys WRT54GL broadband