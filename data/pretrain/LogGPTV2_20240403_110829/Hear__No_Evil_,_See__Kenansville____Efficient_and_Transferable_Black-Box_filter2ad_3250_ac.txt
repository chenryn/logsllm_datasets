on the intelligibility for the human listener.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
715
Signal Decomposition(a)Reconstruction(c)Measurement(e)Thresholding(b)ASR(d)“the meeting is now Charmed”W1,W2,W3,...AAAB+3icbZDLSsNAFIZP6q3WW6xLN4NFcFFCUgVdFty4rGCbQhvCZDpph04uzEzEEvoqblwo4tYXcefbOGmz0NYDM3z8/znMmT9IOZPKtr+Nysbm1vZOdbe2t39weGQe13syyQShXZLwRPQDLClnMe0qpjjtp4LiKODUDaa3he8+UiFZEj+oWUq9CI9jFjKClZZ8s+76ThO5fqu4LpvIsizfbNiWvSi0Dk4JDSir45tfw1FCsojGinAs5cCxU+XlWChGOJ3XhpmkKSZTPKYDjTGOqPTyxe5zdK6VEQoToU+s0EL9PZHjSMpZFOjOCKuJXPUK8T9vkKnwxstZnGaKxmT5UJhxpBJUBIFGTFCi+EwDJoLpXRGZYIGJ0nHVdAjO6pfXodeyHM33V412u4yjCqdwBhfgwDW04Q460AUCT/AMr/BmzI0X4934WLZWjHLmBP6U8fkDjKGRgw==AAAB+3icbZDLSsNAFIZP6q3WW6xLN4NFcFFCUgVdFty4rGCbQhvCZDpph04uzEzEEvoqblwo4tYXcefbOGmz0NYDM3z8/znMmT9IOZPKtr+Nysbm1vZOdbe2t39weGQe13syyQShXZLwRPQDLClnMe0qpjjtp4LiKODUDaa3he8+UiFZEj+oWUq9CI9jFjKClZZ8s+76ThO5fqu4LpvIsizfbNiWvSi0Dk4JDSir45tfw1FCsojGinAs5cCxU+XlWChGOJ3XhpmkKSZTPKYDjTGOqPTyxe5zdK6VEQoToU+s0EL9PZHjSMpZFOjOCKuJXPUK8T9vkKnwxstZnGaKxmT5UJhxpBJUBIFGTFCi+EwDJoLpXRGZYIGJ0nHVdAjO6pfXodeyHM33V412u4yjCqdwBhfgwDW04Q460AUCT/AMr/BmzI0X4934WLZWjHLmBP6U8fkDjKGRgw==AAAB+3icbZDLSsNAFIZP6q3WW6xLN4NFcFFCUgVdFty4rGCbQhvCZDpph04uzEzEEvoqblwo4tYXcefbOGmz0NYDM3z8/znMmT9IOZPKtr+Nysbm1vZOdbe2t39weGQe13syyQShXZLwRPQDLClnMe0qpjjtp4LiKODUDaa3he8+UiFZEj+oWUq9CI9jFjKClZZ8s+76ThO5fqu4LpvIsizfbNiWvSi0Dk4JDSir45tfw1FCsojGinAs5cCxU+XlWChGOJ3XhpmkKSZTPKYDjTGOqPTyxe5zdK6VEQoToU+s0EL9PZHjSMpZFOjOCKuJXPUK8T9vkKnwxstZnGaKxmT5UJhxpBJUBIFGTFCi+EwDJoLpXRGZYIGJ0nHVdAjO6pfXodeyHM33V412u4yjCqdwBhfgwDW04Q460AUCT/AMr/BmzI0X4934WLZWjHLmBP6U8fkDjKGRgw==AAAB+3icbZDLSsNAFIZP6q3WW6xLN4NFcFFCUgVdFty4rGCbQhvCZDpph04uzEzEEvoqblwo4tYXcefbOGmz0NYDM3z8/znMmT9IOZPKtr+Nysbm1vZOdbe2t39weGQe13syyQShXZLwRPQDLClnMe0qpjjtp4LiKODUDaa3he8+UiFZEj+oWUq9CI9jFjKClZZ8s+76ThO5fqu4LpvIsizfbNiWvSi0Dk4JDSir45tfw1FCsojGinAs5cCxU+XlWChGOJ3XhpmkKSZTPKYDjTGOqPTyxe5zdK6VEQoToU+s0EL9PZHjSMpZFOjOCKuJXPUK8T9vkKnwxstZnGaKxmT5UJhxpBJUBIFGTFCi+EwDJoLpXRGZYIGJ0nHVdAjO6pfXodeyHM33V412u4yjCqdwBhfgwDW04Q460AUCT/AMr/BmzI0X4934WLZWjHLmBP6U8fkDjKGRgw==W1,W2,0,...AAACC3icbZC7SgNBFIZnvcb1FrW0GRIEi7DsBkHLgI1lBJMNJGGZnZwkQ2YvzJwVw5LexlexsVDE1hew822cXApNPDDw8f/nzJn5w1QKja77ba2tb2xubRd27N29/YPD4tFxUyeZ4tDgiUxUK2QapIihgQIltFIFLAol+OHoeur796C0SOI7HKfQjdggFn3BGRopKJZsP/Aq1A+qFdpBeMDZlbmC3iR3JxXqOE5QLLuOOyu6Ct4CymRR9aD41eklPIsgRi6Z1m3PTbGbM4WCS5jYnUxDyviIDaBtMGYR6G4+WzyhZ0bp0X6izImRztTfEzmLtB5HoemMGA71sjcV//PaGfavurmI0wwh5vNF/UxSTOg0GNoTCjjKsQHGlTBvpXzIFONo4rNNCN7yl1ehWXU8w7cX5VptEUeBnJISOSceuSQ1ckPqpEE4eSTP5JW8WU/Wi/Vufcxb16zFzAn5U9bnD5xWmN8=AAACC3icbZC7SgNBFIZnvcb1FrW0GRIEi7DsBkHLgI1lBJMNJGGZnZwkQ2YvzJwVw5LexlexsVDE1hew822cXApNPDDw8f/nzJn5w1QKja77ba2tb2xubRd27N29/YPD4tFxUyeZ4tDgiUxUK2QapIihgQIltFIFLAol+OHoeur796C0SOI7HKfQjdggFn3BGRopKJZsP/Aq1A+qFdpBeMDZlbmC3iR3JxXqOE5QLLuOOyu6Ct4CymRR9aD41eklPIsgRi6Z1m3PTbGbM4WCS5jYnUxDyviIDaBtMGYR6G4+WzyhZ0bp0X6izImRztTfEzmLtB5HoemMGA71sjcV//PaGfavurmI0wwh5vNF/UxSTOg0GNoTCjjKsQHGlTBvpXzIFONo4rNNCN7yl1ehWXU8w7cX5VptEUeBnJISOSceuSQ1ckPqpEE4eSTP5JW8WU/Wi/Vufcxb16zFzAn5U9bnD5xWmN8=AAACC3icbZC7SgNBFIZnvcb1FrW0GRIEi7DsBkHLgI1lBJMNJGGZnZwkQ2YvzJwVw5LexlexsVDE1hew822cXApNPDDw8f/nzJn5w1QKja77ba2tb2xubRd27N29/YPD4tFxUyeZ4tDgiUxUK2QapIihgQIltFIFLAol+OHoeur796C0SOI7HKfQjdggFn3BGRopKJZsP/Aq1A+qFdpBeMDZlbmC3iR3JxXqOE5QLLuOOyu6Ct4CymRR9aD41eklPIsgRi6Z1m3PTbGbM4WCS5jYnUxDyviIDaBtMGYR6G4+WzyhZ0bp0X6izImRztTfEzmLtB5HoemMGA71sjcV//PaGfavurmI0wwh5vNF/UxSTOg0GNoTCjjKsQHGlTBvpXzIFONo4rNNCN7yl1ehWXU8w7cX5VptEUeBnJISOSceuSQ1ckPqpEE4eSTP5JW8WU/Wi/Vufcxb16zFzAn5U9bnD5xWmN8=AAACC3icbZC7SgNBFIZnvcb1FrW0GRIEi7DsBkHLgI1lBJMNJGGZnZwkQ2YvzJwVw5LexlexsVDE1hew822cXApNPDDw8f/nzJn5w1QKja77ba2tb2xubRd27N29/YPD4tFxUyeZ4tDgiUxUK2QapIihgQIltFIFLAol+OHoeur796C0SOI7HKfQjdggFn3BGRopKJZsP/Aq1A+qFdpBeMDZlbmC3iR3JxXqOE5QLLuOOyu6Ct4CymRR9aD41eklPIsgRi6Z1m3PTbGbM4WCS5jYnUxDyviIDaBtMGYR6G4+WzyhZ0bp0X6izImRztTfEzmLtB5HoemMGA71sjcV//PaGfavurmI0wwh5vNF/UxSTOg0GNoTCjjKsQHGlTBvpXzIFONo4rNNCN7yl1ehWXU8w7cX5VptEUeBnJISOSceuSQ1ckPqpEE4eSTP5JW8WU/Wi/Vufcxb16zFzAn5U9bnD5xWmN8=Component WeightsModiﬁed Voice SignalThreshold UpdateComponentsDigitized Voice SignalTranscription“I'm eating a sandwich arms”The attack steps are outlined in Figure 3. During decompo-
sition, shown in Figure 3(a), we pass the audio sample to the
selected algorithm (DFT or SSA). The algorithm decomposes
the audio into individual components and their corresponding
intensities. Next, we threshold these components, as shown in
Figure 3(b). During thresholding, we remove the components
whose intensity falls below a certain threshold. The intuition
for doing so is that the lower intensity components are less
perceptible to the human ear and will likely not affect a user’s
ability to understand the audio. We discuss how the algorithm
calculates the correct threshold in the next paragraph. We then
construct a new audio sample from the remaining components
using the appropriate inverse transform, shown in Figure 3(c).
Next,
the audio is passed on to the model for inference,
Figure 3(d). If the system being attacked is an ASR, then
the model outputs a transcription. On the other hand if the
target system is an AVI, the model outputs a speaker label.
The model output is compared with that of the original during
the measurement step, Figure 3(e).
The goal of the algorithm is to calculate the optimum thresh-
old, which discards the least number of components whilst still
forcing the model to misinterpret the audio sample. Discarding
components degrades the quality of the reconstructed audio
sample. If the discard threshold is too high, neither the human
listener nor the model will be able to correctly interpret the
reconstructed audio. On the other hand, by setting it
too
low, both the human listener and the model will correctly
understand the reconstructed audio.
To compensate for these competing tensions, the attack
algorithm executes the following steps. If the model output
matches the original label, during the measurement step, the
algorithm will increase the threshold value. It will then pass
the audio sample and the updated threshold back to the
thresholding step for an additional round of optimization.
However, if the model outputs an incorrect interpretation of
the audio sample, the algorithm reduces the degradation by
reducing the discard threshold, before returning the audio to
the thresholding step. This loop will repeat until the algorithm
has calculated the optimum discard threshold.
C. Performance
In order to ﬁnd the optimal threshold, we incrementally
remove more components until the model fails to properly
transcribe the audio ﬁle. This process takes O(n) queries to the
model, where n is the number of decomposition components.
We can reduce the time complexity from linear to logarithmic
time such that an attack audio is produced in O(log n) queries.
To achieve this, we model the distortion search as a binary
search (BS) problem where values represent the number of
coefﬁcients to use during reconstruction.
If the reconstruction is misclassiﬁed, we move to the left
BS search-space and attempt to improve the audio quality by
removing less coefﬁcients. If the audio is correctly transcribed,
we move to the right. This search continues until an upper
bound on the search depth is reached. This result was sufﬁcient
for the scope of this paper, and we leave a more rigorous
analysis of distortion search complexity for future work.
D. Transferability
In order to improve performance, we can leverage trans-
ferability to reduce the number of queries to the model to
zero. One measure of an attack’s strength is the ability to
generate adversarial examples that are transferable to other
models (i.e., a single audio sample that tricks multiple models).
An attacker will not know the precise model he is trying to
fool. In such a scenario, the attacker will generate examples for
a known model, and hope that the samples will work against
a completely different model.
Attacks have been shown to generalize between models in
the image domain [29]. In contrast, attack audio transferability
has only seen limited success. Additionally, audio generated
with previous approaches ([11], [4]) are sensitive to naturally
occurring external noise and fail to exploit targets in real-world
settings. This is in line with previous results of physical attacks
in the image domain [30]. Instead we focus on the evasion
style of attack, where the attack is considered successful if
the ASR system transcribes the attack audio incorrectly or the
AVI misidentiﬁes the speaker of the attack audio. We propose
a completely black-box approach that does not consider model
speciﬁcs as a means of bypassing these limitations.
E. Detection and Defense
it
We evaluate our attack against the adversarial audio detec-
tion mechanism which is based on temporal dependencies [12].
This is the only method designed speciﬁcally to detect adver-
sarial audio samples. This method has demonstrated excellent
results:
is light-weight, simple and highly effective at
detecting tradition adversarial attacks. The mechanism takes
as input an audio sample. This can either be adversarial or
benign. Next, the audio sample is partitioned into two. Only
the partition corresponding to the ﬁrst half is retained. Next,
the entire original audio sample and the ﬁrst partition are
passed to the model and the transcriptions are recorded. If the
transcriptions are similar, then the audio sample is considered
benign. However, if the transcriptions are differing, the audio
sample is adversarial. This is because adversarial attack algo-
rithms against audio samples distort the temporal dependence
within the original sequences. The temporal dependency-based
detection is designed to capture this information and use it for
attack audio detection.
Additionally, we evaluate our attack against adversarial
training based defense. However, we have placed the steps,
the methodology, and the results in the Appendix E.
IV. SETUP
Our experimental setup involved two datasets, four attack
scenarios, two test environments, seven target models, and a
user study. We will discuss the relevant details here.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
716
A. TIMIT Data Set
The TIMIT corpus is a standard dataset for speech pro-
cessing systems. It consists of 10 English sentences that are
phonetically diverse being spoken by 630 male and female
speakers [31]. Additionally, there is metadata of each speaker
that includes the speaker’s dialect. In our tests, we randomly
sampled six speakers, three male and three female, from each
of four regions (New England, Northern, North Midland, and
South Midland). We then perturbed all 10 sentences spoken by
our speakers using our technique, and also extracted phonemes
for our phoneme-level experiments. In total, we attacked 240
recordings with 7600 phonemes.
B. Word Audio Data Set
Testing the word-level robustness of an ML model poses
challenges in terms of experimental design. Although there
exist well-researched datasets of spoken phrases for speech
transcription [32], [33], partitioning the phrases into individual
words based on noise threshold is not ideal. In this case,
the only way to control the distribution of candidate phrases
would be to pass them to a strong transcription model, while
discarding audio samples which are mistranscribed. Doing so
may bias the candidate attack samples towards clean, easy to
understand samples. Instead, we build a word-level candidate
dataset using a public repository of the 1,000 most common
English words [27]. We then download audio for each of the
1,000 words using the Shotooka spoken-word service [34].
C. Attack Scenarios
In order to test our technique in a variety of different ways,
we performed four attacks: word level, phoneme level, ASR
Poisoning, and AVI poisoning. We also tested the transferabil-
ity of the attack audio samples across models.
1) Word Level Perturbations: Using the 1,000 most com-
mon words, we performed our attack as described in
Section III-B. We optimized our threshold using the technique
outlined in Section III-D, stopping either after the threshold
value had converged or after a maximum of 15 queries.
2) Phoneme Level Perturbations: Next, we ran perturba-
tions on individual phonemes rather than entire words. The
goal of this attack was to cause mistranscription of an entire
word by only perturbing a single phoneme. We tested this
attack on audio ﬁles from the TIMIT corpus and replaced
the regular phoneme with its perturbed version in the audio
ﬁle. The audio sample was then passed to the ASR system
for transcription. We repeated this process for every phoneme
using the binary search technique outlined in Section III-D.
3) ASR Poisoning: ASR systems often use the previously
transcribed words to infer the transcription of the next [35],
[36], [37]. Perturbing a single word’s phonemes not only
affects the model’s ability to transcribe that word, but also
the words that come after it. We wanted to measure the effect
of perturbing a single phoneme on the transcription of the
remaining words in a sentence. To do this, we generated
adversarial audio sample by perturbing a single phoneme while
keeping the remaining audio intact for the sentences of the
TIMIT dataset. We repeated this for every phoneme in the
dataset and passed the attack audio samples to the ASR. The
cosine similarity metric was used to measure the transcription
similarity between the attack and original audio samples.
When perturbing phonemes, we do not use the attack
optimization described in Section III-D. Since the average
length of a phoneme in our dataset was only 31ms, a single
perturbed phoneme in a sentence does not signiﬁcantly impact
audio comprehension. Therefore, we simply discard half of all
decomposition coefﬁcients in a single 31 ms window during
the thresholding step. This maintains the quality of the adver-
sarial audio, while still forcing the model to mistranscribe.
4) AVI Poisoning: We also evaluate our attacks’ perfor-
mance against AVI system. To do so, we ﬁrst trained an Azure
Speaker Identiﬁcation model [38] to recognize 20 speakers.
We selected 10 male and 10 female speakers from the TIMIT
dataset to service as our subjects. For each speaker, seven
sentences were used for training, while the remaining three
sentences were used for attack evaluation. We only perturbed a
single phoneme while the rest of the sentence is left unaltered.
We passed both the benign and adversarial audio samples to
the model. The attack was considered a success if the AVI
model output different labels for each sample. This attack
setup is similar to the one for ASR poisoning, except that
here we target an AVI system.
D. Models
We choose a set of seven models that are representative of
the state-of-the-art in ASR and AVI technology, given their
high accuracy [39], [40], [41], [42]. These include a mixture
of both proprietary and open-source models to expose the
utility of our attack. However, all are treated as black-box. We
evaluate our attack against 7 models, which include Google
(Normal), Google (Phone), Facebook Wit, Deep Speech-1,
Deep Speech-2, CMU Sphinx, and Microsoft Azure. We
provide details about these models in Appendix A.
Although the landscape of audio models is ever-changing,
we believe our selection represents an accurate approximation
of both traditional and state-of-the-art methods. Intuitively, fu-
ture models will derive from the existing ones in terms of data
and implementation. We also compare against open-source and
proprietary models, to show our approach generalizes to any
model regardless of a lack in apriori knowledge.
E. Transferability
We measure the transferability of our proposed attack by
ﬁnding the probability that the attack samples for one model
will successfully exploit another. This is done by creating
a set of successful word-level attack audio samples X∗
m for
a model m, then averaging their calculated Mean Squared
Error (MSE) distortion, M SEm. Intuitively, this average MSE
will be higher for stronger models, and lower for weaker
models. This acts as a ‘hardness score’ for a given model
and is used to compare between the attack audio sets of
two models. Now consider a baseline model f, comparison
model g,
the successful attack transfer event Sf→g, and
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
717
f| = |X∗
the number of audio samples in each model’s attack audio
g|. We can calculate attack transfer
set n = |X∗
probability from f to g as the probability of sampling attack
audio from X∗
f whose distortion meets or surpasses the score
M SEg. We denote this probability P (Sf→g) and the set of
potentially transferable audio samples as Vf→g. We calculate
the probability using the equation P (Sf→g) =
, where
we build the set of transferable attack samples such that
f,i) ≥ M SEg}.
Vf→g = {x∗
f : M SE(x∗
|Vf→g|
n
f,i ∈ X∗
Thus, we approximate the probability of sampling a piece
of audio which meets the ‘hardness’ of model g from the set
which was successful over model f. This value is calculated
across each combination of models in our experiments for SSA
and DFT transforms, with n set to 1,000 as a result of using
our Word Audio data set.
F. Detection
Our experimental method was designed to be as close to
that of the authors [12]. We assume that the attacker is not
aware of the existence of the defense mechanism or the size
of the partition. Therefore, we perturbed the entire audio
sample using our attack, to maximally distort the temporal
dependencies. Next, we partitioned the audio sample into
two halves. We passed both the entire audio sample and the
ﬁrst half to the Google Speech API for transcription. We
conducted this experiment with 266 adversarial audio samples
generated using the DFT perturbation method at a factor of