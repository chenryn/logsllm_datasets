Throughout the paper we have assumed the data are
truthfully reported (though with noise/error). It is thus
reasonable to question how robust
is the prediction
against possible (malicious) manipulation of the data
used for training, a subject of increasing interest and
commonly referred to as adversarial machine learning.
For instance, an entity may attempt to set up fake net-
works with clean data (no malicious activities) but with
fake reported incidents, and vice versa, to mislead the
classiﬁer. Without presenting a complete solution, which
remains a direction of future research, below we test the
USENIX Association  
24th USENIX Security Symposium  1021
robustness of our current prediction technique using two
scenarios: (1) In the ﬁrst we randomly ﬂip the labels of
victim organizations from 1 to 0; those ﬂipped to 0 are
now part of the non-victim group, thus contaminating the
training data. (2) In the second scenario we do the op-
posite: randomly ﬂip the labels of non-victim organiza-
tions, effectively adding them to the victim group. Inci-
dentally, the former scenario is akin to under-reporting
by randomly selected organizations.
Experimental results suggest no performance differ-
ence for case (1). The reason lies in the imbalance be-
tween the victim and non-victim population sizes. Re-
call that because of this, in our experiment we randomly
select a subset of non-victim organizations with size
comparable to the victim organizations (on the order of
O(1,000)). Then in each training instance, the expected
number of victims selected as part of the non-victim set
is no more than Nv · O(1,000)/N, with Nv denoting the
number of fake non-victims and N the total number of
non-victims. Since N ∼ O(1,000,000), even if one is
able to inject Nv ∼ O(100) victims into the non-victim
population, on average no more than one fake non-victim
will actually be selected for training, resulting in negli-
gible contamination effect unless such alterations can be
done on a scale larger than the actual victim population.
For case (2), we indeed observe performance degrada-
tion, albeit slight, in the true positive, as shown in Fig.
11 at the 20% contamination level (20% of non-victim
organization labels are ﬂipped).
e
v
i
t
i
s
o
p
e
u
r
T
1
0.9
0.8
0.7
0.6
0.5
0.4
VCDB
Adversarial
0.1
0.2
0.3
False positive
0.4
0.5
Figure 11: Adversarial case (2) with 20% contamination.
5.4
Incident Reporting
One of the main obstacles in studies of this nature is the
acquisition of high quality incident data, without which
we can neither train nor verify with conﬁdence. Our re-
sult here demonstrates that machine learning techniques
have the power to make accurate incident forecasts, but
data collection is lagging by comparison. The research
community would beneﬁt enormously from more sys-
tematic and uniform incident reporting.
6 Related Work
As mentioned in the introduction, a large part of the lit-
erature focuses on detection rather than prediction. The
work in [44] is one such example. Among others, Lee et
al. [36] built sophisticated Hidden Markov Model tech-
niques to detect spam deobfuscation, and in [57] Wang
et al. applied (adversarial) machine learning techniques
to the detection of malicious accounts on Weibo.
Relatively fewer studies have focused on prediction;
even fewer are on the type of prediction presented in
this paper where the predicted variable (classiﬁer out-
put) is of a different type from the input variables (fea-
ture input). For instance, the predictive IP-based black-
list works in [50, 30] have the same input and output
variables (content of the blacklist). Similarly, in [54]
the evolution of spam of a certain preﬁx is predicted
using past spam activities as input. Predictive studies
similar to ours include the aforementioned [51] that pre-
dicts whether a website will turn malicious by using tex-
tual and structural analysis of a webpage. The perfor-
mance comparison has been given earlier.
It is worth
pointing out that the intended applications are also differ-
ent: whereas webpage maliciousness prediction can help
point to websites needing improvement or maintenance,
our prediction on the organizational level can help point
to networks facing heightened probability of a broader
class of security problems. Also as mentioned earlier,
our study [48] examines the prediction of incident types,
conditional on an incident occurring, by using an array
of industry, business and web visibility/population infor-
mation. Other predictive studies include [28], where it
is shown that by analyzing user browsing behavior one
can predict whether a user will encounter a malicious
page (attaining a 87% accuracy), [52], where risk fac-
tors are identiﬁed at the organization level (industry sec-
tor and number of employees) and the individual level
(job type, location) that are positively or negatively cor-
related with experiencing spear phishing targeted attacks,
and [53], where risk factors for web server compromise
are identiﬁed through analyzing features from sampled
web servers.
Also related are studies on reputation systems and pro-
ﬁling of networks. These include e.g., [26], a reputation
assigning system trained using DNS features, reputation
systems [8, 9] based on monitoring Internet trafﬁc data,
and those studied in [29, 46].
7 Conclusion
In this study, we characterize the extent to which cyber
security incidences can be predicted based on externally
observable properties of an organization’s network. Our
method is based on 258 externally measurable features
1022  24th USENIX Security Symposium 
USENIX Association
collected from a network’s mismanagement symptoms
and malicious activity time series. Using these to train a
Random Forest classiﬁer, it is shown that we can achieve
fairly high accuracy, such as a combination of 90% true
positive rate and 10% false positive rate. We further an-
alyzed the relative importance of the features sets in the
prediction performance, and showed our prediction out-
come for the top data breaches in 2014.
Acknowledgement
This work is partially supported by the NSF under grant
CNS 1422211, CNS 1409758, CNS 1111699 and by the
DHS under contract number HSHQDC-13-C-B0015.
References
[1] AFRINIC whois database. http://www.afrinic.net/
services/whois-query.
[2] APNIC whois database.
apnic-bin/whois.pl.
http://wq.apnic.net/
[3] ARIN whois database.
https://www.arin.net/
resources/request/bulkwhois.html.
[4] Composite Blocking List. http://cbl.abuseat.org/.
[5] DShield. http://www.dshield.org/.
[6] Evernote resets passwords after major security breach.
http://www.digitalspy.co.uk/tech/news/
a462959/evernote-resets-passwords-after-
major-security-breach.html.
[7] Global Reputation System. http://grs.eecs.umich.
edu//.
[8] Global Security Reports. http://globalsecuritymap.
com/.
[9] Global Spamming Rank. http://www.spamrankings.
net/.
[10] Google Kenya and Google Burundi hacked by 1337.
http://thehackersmedia.blogspot.com/2013/09/
google-kenya-google-burundi-hacked-by.html.
[11] hpHosts for your pretection. http://hosts-file.net/.
[12] LACNIC whois database. http://lacnic.net/cgi-
bin/lacnic/whois.
[13] Multiple DNS implementations vulnerable to cache poi-
soning. http://www.kb.cert.org/vuls/id/800113.
http://
[14] Open
Project.
openresolverproject.org/.
Resolver
[15] OpenBL. http://www.openbl.org/.
[16] PhishTank. http://www.phishtank.com/.
[17] Rf
classiﬁer.
http://scikit-learn.org/
stable/modules/generated/sklearn.ensemble.
RandomForestClassifier.html.
[18] RIPE whois database. https://apps.db.ripe.net/
search/query.html.
[19] SpamCop Blocking List. http://www.spamcop.net/.
[20] SURBL: URL REPUTATION DATA.
http://www.
surbl.org/.
[21] Syrian hacker Dr.SHA6H hacks and defaces City of
http://hackread.com/
Mansﬁeld, OH website.
syrian-hacker-dr-sha6h-hacks-and-defaces-
city-of-mansfield-oh-website-for-free-syria.
[22] The SPAMHAUS project: SBL, XBL, PBL, ZEN Lists.
http://www.spamhaus.org/.
Network.
[23] UCEPROTECTOR
uceprotect.net/.
http://www.
[24] WPBL: Weighted Private Block List.
http://www.
wpbl.info/.
[25] AGRAWAL,
T.,
D.,
AND
HENRY,
FINKLE,
JPMorgan hack exposed data of 83 mil-
http:
J.
lion, among biggest breaches in history.
//www.reuters.com/article/2014/10/03/us-
jpmorgan-cybersecurity-idUSKCN0HR23T20141003,
October 2014.
[26] ANTONAKAKIS, M., PERDISCI, R., DAGON, D., LEE,
W., AND FEAMSTER, N. Building a Dynamic Reputation
System for DNS.
In Proceedings of the 19th USENIX
Security Symposium (Berkeley, CA, USA, August 2010).
[27] BISHOP, C. M., ET AL. Pattern Recognition and Ma-
chine Learning, vol. 1. Springer New York.
[28] CANALI, D., BILGE, L., AND BALZAROTTI, D. On the
Effectiveness of Risk Prediction Based on Users Brows-
ing Behavior.
In ASIA CCS ’14 (New York, NY, USA,
June 2014), ACM, pp. 171–182.
[29] CHANG, J., VENKATASUBRAMANIAN, K. K., WEST,
A. G., KANNAN, S., LEE, I., LOO, B. T., AND SOKOL-
SKY, O. AS-CRED: Reputation and Alert Service for
Interdomain Routing. vol. 7, pp. 396–409.
[30] COLLINS, M. P., SHIMEALL, T. J., FABER, S., JANIES,
J., WEAVER, R., DE SHON, M., AND KADANE, J. Us-
ing Uncleanliness to Predict Future Botnet Addresses. In
Proceedings of ACM IMC (San Diego, California, USA,
October 2007), pp. 93–104.
[31] CONSORTIUM, T. W. A. S. Web-Hacking-Incident-
Database.
http://projects.webappsec.org/w/
page/13246995/Web-Hacking-Incident-Database.
[32] DURUMERIC, Z., KASTEN, J., BAILEY, M., AND HAL-
DERMAN, J. A. Analysis of the HTTPS Certiﬁcate
Ecosystem.
In Proceedings of ACM IMC (Barcelona,
Spain, October 2013), pp. 291–304.
[33] DURUMERIC, Z., WUSTROW, E., AND HALDERMAN,
J. A. ZMap: Fast Internet-Wide Scanning and its Secu-
rity Applications.
In Proceedings of the 22nd USENIX
Security Symposium (Washington, D.C., August 2013),
pp. 605–620.
[34] HUBERT, A., AND MOOK, R. V. Measures for Making
DNS More Resilient against Forged Answers. RFC 5452,
January 2009.
USENIX Association  
24th USENIX Security Symposium  1023
[35] KREBS, B.
The target breach, by the num-
bers. http://krebsonsecurity.com/2014/05/the-
target-breach-by-the-numbers/, May 2014.
[50] SOLDO, F., A., L., AND MARKOPOULOU, A. Predictive
Blacklisting as an Implicit Recommendation System. In
INFOCOM, IEEE (March 2010), pp. 1–9.
[36] LEE, H., AND NG, A. Y. Spam Deobfuscation using a
Hidden Markov Model. In In Conference on Email and
Anti-Spam (July 2005).
[37] LIAW, A., AND WIENER, M. Classiﬁcation and Regres-
sion by randomForest. http://CRAN.R-project.org/
doc/Rnews/, 2002.
[38] LINDBERG, G. Anti-Spam recommendations for SMTP
MTAs. BCP 30/RFC 2505, 1999.
[39] MA, J., SAUL, L. K., SAVAGE, S., AND VOELKER,
G. M. Beyond Blacklists: Learning to Detect Malicious
Web Sites from Suspicious URLs. In Proceedings of the
15th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (New York, NY, USA,
June 2009), KDD ’09, ACM, pp. 1245–1254.
[40] MAHAJAN, R., WETHERALL, D., AND ANDERSON, T.
Understanding BGP misconﬁguration. In Proceedings of
SIGCOMM ’02 (August 2002), vol. 32, ACM, pp. 3–16.
[41] OF OREGON, U. Route Views Project. http://www.
routeviews.org/.
[42] PASSERI,
P.
Hackmageddon.com.
http:
//hackmageddon.com/.
[43] PRINCE, B. Top data breaches of 2014.
http://
www.securityweek.com/top-data-breaches-2014,
December 2014.
[44] QIAN, Z., MAO, Z. M., XIE, Y., AND YU, F. On
Network-level Clusters for Spam Detection. In Proceed-
ings of the Network and Distributed System Security Sym-
posium (NDSS ’14) (San Diego, CA, March 2010).
[45] RAMACHANDRAN, A., AND FEAMSTER, N. Under-
In
standing the Network-level Behavior of Spammers.
Proceedings of SIGCOMM ’06 (August 2006), vol. 36,
ACM, pp. 291–302.
[46] RESNICK, P., KUWABARA, K., ZECKHAUSER, R., AND
FRIEDMAN, E. Reputation Systems. Commun. ACM 43,
12 (December 2000), 45–48.
[47] ROMANOSKY,
S.
Comments
cybersecurity
on
practices
incentives
noi.
adopt
improved
to
http://www.ntia.doc.gov/federal-register-
notice/2013/comments-incentives-adopt-
improved-cybersecurity-practices-noi,
2013.
April
[48] SARABI, A., NAGHIZADEH, P., LIU, Y., AND LIU, M.
Prioritizing Security Spending: A Quantitative Analysis
of Risk Distributions for Different Business Proﬁles. In
the Annual Workshop on the Economics of Information
Security (WEIS) (June 2015).
[49] SIDEL, R. Home depot’s 56 million card breach bigger
than target’s. http://www.wsj.com/articles/home-
depot-breach-bigger-than-targets-1411073571,
September 2014.
[51] SOSKA, K., AND CHRISTIN, N. Automatically Detect-
ing Vulnerable Websites Before They Turn Malicious.
In Proceedings of the 23rd USENIX Security Symposium
(San Diego, CA, August 2014).
[52] THONNARD, O., BILGE, L., KASHYAP, A., AND LEE,
M. Are You at Risk? Proﬁling Organizations and Indi-
viduals Subject to Targeted Attacks. In Financial Cryp-
tography and Data Security (January 2015).
[53] VASEK, M., AND MOORE, T. Identifying Risk Factors
for Webserver Compromise. In Financial Cryptography
and Data Security. Springer, March 2014, pp. 326–345.
[54] VENKATARAMAN, S., BRUMLEY, D., SEN, S., AND
SPATSCHECK, O. Automatically Inferring the Evolution
of Malicious Activity on the Internet. In Proceedings of
the Network and Distributed System Security Symposium
(NDSS ’14) (San Diego, CA, February 2013).
[55] VERIS. VERIS Community Database (VCDB). http:
//veriscommunity.net/index.html.
[56] VERIZON. Data Breach Investigations Reports (DBIR)
2014. http://www.verizonenterprise.com/DBIR/.
[57] WANG, G., WANG, T., ZHENG, H., AND ZHAO, B. Y.
Man vs. Machine: Practical Adversarial Detection of Ma-
licious Crowdsourcing Workers.
In Proceedings of the
23rd USENIX Security Symposium (San Diego, CA, Au-
gust 2014), pp. 239–254.
[58] ZHANG, J., DURUMERIC, Z., BAILEY, M., KARIR, M.,
AND LIU, M. On the Mismanagement and Malicious-
ness of Networks.
In Proceedings of the Network and
Distributed System Security Symposium (NDSS ’14) (San
Diego, CA, February 2014).
APPENDIX
Incident Dataset
A snapshot of sample incident reports from VCDB
dataset (Table 7).
Incident type
Web site defacement May 2014
Time
Hacking
Apr. 2014
Web site defacement
Server breach
N/A 2013
N/A 2013
Web site hacked
Private key stolen
Phishing
May 2013
Mar. 2014
N/A 2013
Report summary
”ybs-bank.com” a Malaysian
imitation of the real Yorkshire Bank website
4chan hacked by person targeting information
about users posting habits.
AR Argentina Military website hacked.
The systems of AdNet Telecom, a major
Romania-based telecommunications services
provider, have been breached.
Albany International Airport website hacked.
Amazon Web Services, Inc.
Bolivian tourist site was compromised and
a fraudulent secret shopper site was installed.
Table 7: Incidents from the VCDB Community Database
1024  24th USENIX Security Symposium 
USENIX Association