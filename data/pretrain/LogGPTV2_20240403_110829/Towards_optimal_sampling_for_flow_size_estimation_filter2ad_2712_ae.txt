move down the curve and decrease pf to compensate.
The great merit of the ESR normalization it that it allows us to
view the methods as being different ways in which the same budget
of sampled packets can be allocated among ﬂows. The essential
tradeoff is that we can have more sampled ﬂows of poor quality,
or fewer of higher quality. Flow sampling is at one extreme: for a
given ESR= p it gives the minimum number of sampled ﬂows (in
a class with no ﬂow length bias), each of perfect quality. Among
sub-classes of methods with roughly the same number of ﬂows, we
can then distinguish between the ﬁner details of how the ‘holes’
appear over the ﬂow, which will have an impact on the degree of
improvement which the sequence numbers can bring.
Results using the same scenario as Figure 2 but ESR normalized
are shown in Figure 4. The performance order is (almost) the same,
however the magnitudes are more evenly spread, and DS with the
smaller pf is now much closer to FS. We explain this below.
Figure 5: Comparison of the CRLB bound with W = 50. Left: PPR with p = 0.005 and (for DS, pf = 0.001 and pp = 0.01). Middle:
ESR with p = 0.005. Right: Zoom of middle plot.
Another example with a larger W = 50 and a truncated expo-
nential distribution for (cid:126)θ is given in Figure 5 for which the same
general conclusions hold.
4.2 Theoretical Comparison
The examples above compared methods using the CRLB of each
θk separately. Let two methods have Fisher information J1 and J2.
A more complete comparison would be to show that J1 ≥ J2, since
that would imply I +
2 , a lower CRLB. What this positive
semideﬁnite comparison really means is that for any linear combi-
nation f ((cid:126)θ) = aT(cid:126)θ =
k akθk of the parameters, the (bound on
the) variance of f ((cid:126)θ) under method 1 will be less than that under
method 2.
(cid:80)
1 ≤ I +
THEOREM 4.1. JZ+SEQ ≥ JZ for each of Z =PS and Z =PS+SYN.
PROOF. To prove this result, we make use of the data process-
ing inequality (DPI) for Fisher information [13, Lemma 3], which
states that if (cid:126)θ → X → Y is a Markov chain, then JX ((cid:126)θ) ≥
JY ((cid:126)θ). We ﬁrst consider PS and PS+SEQ. The SEQ numbers, in
terms of packet counts, of an original ﬂow of size k is equivalent to
an arithmetic progression from an initial value a to a + (k− 1). Let
X = {a, . . . , a + (k − 1)} be the SEQ vector of a ﬂow selected
by the uniform selection process. The choice of a is independent
of the size k. Both a and k parameterizes X.
After sampling, we now have a sampled version of the SEQ vec-
tor Y ∈ Sj = {y = {yi}1≤i≤j(cid:48) | 1 ≤ j(cid:48) ≤ k, yj(cid:48) −y1 = j ≤ k}.
For example, Y = {a + 1, a + 3} belongs to the set S3 (although
j(cid:48) = |Y | = 2). Thus, j(cid:48) represents the actual number of physi-
cal packets that were sampled. Furthermore, we deﬁne a statistic
Cseq(Y ) = yj(cid:48)−y1 +1, and this statistic disregards a since it takes
the difference between the ﬁrst and last SEQ numbers. The statistic
is essentially the size of the sampled ﬂow Y inferred from the SEQ
numbers. This forms a Markov chain (cid:126)θ → X → Y → Cseq(Y ).
It may not be clear that all information about (cid:126)θ are preserved
by Cseq(Y ), as a straightforward application of DPI yields JY ≥
JCseq (Y ). We show that Cseq(Y ) is a sufﬁcient statistic of Y
w.r.t. (cid:126)θ. Let A be the random variable denoting the initial SEQ
number value. The probability Y takes a particular realization
y ∈ Sj is
Pr(Y = y) = Pr(A = a)
pqj−l−2
pl
p
bjkθk
(cid:195)
(cid:33)
j − 2
l
(cid:88)
k≥j
= Pr(A = a) Pr(Ωl) Pr(Cseq(Y ) = j),
where Ωl is the event the number of packets sampled between the
packets with the ﬁrst and last SEQ numbers is l and bjk comes
from the sampling matrix of PS+SEQ. Clearly, this shows that only
Cseq(Y ) has dependence on (cid:126)θ. By the Fisher-Neyman factorization
theorem [14, Theorem 6.5, p. 35], Cseq(Y ) is a sufﬁcient statis-
tic of (cid:126)θ. By DPI, JY = JCseq (Y ); by implication, an equivalent
Markov chain is (cid:126)θ → X → Cseq(Y ) → Y .
The PS process can be described by Z = T (Y ) where T is a
deterministic function that replaces each element in Y (i.e. SEQ
number) with 1. Let Cpkt(Z) be the count of the number of 1s in
Z, which is equivalent to |Z|. By similar arguments as above, with
|Z| = |Y | = j(cid:48) ≤ j, we obtain
Pr(Z = 1j(cid:48) ) = Pr(|Z| = j
(cid:48)
) = Pr(Cpkt(Z) = j
(cid:48)
),
thus we have JZ = JCpkt(Z). Thus, the Markov chains (cid:126)θ → X →
Y → Z and (cid:126)θ → X → Cseq(Y ) → Cpkt(Z) are equivalent. By
applying the DPI, we obtain our result.
In general, equality can only hold iff T (Y ) is a sufﬁcient statistic
of Y . This may not be the true under certain degenerate cases where
sequence numbers do not provide an advantage, i.e. T (Y ) = Y ,
such as ﬂow distributions with W = 1 or 2. A similar argument
holds for PS+SYN and PS+SYN+SEQ.
Intuitively this result seems obvious: if the additional informa-
tion afforded by the sequence numbers is available, we should cer-
tainly be able to do better by using it. However, it is tempting to
conclude that by the same logic JPS > JPS+SYN, since deciding
to discard ﬂows without a SYN is also a deterministic transforma-
tion. However, the data processing inequality does not apply here
because the information used by PS+SYN (namely the SYN vec-
tor), although available to PS, is not fully used by it. Indeed we saw
above that, counterintuitively, PS+SYN can actually outperform PS
in terms of the individual variances, which is a counter-example to
the more general positive semideﬁnite comparison.
We now give another, even more surprising counter-example which
teaches an important lesson.
THEOREM 4.2. JF S (cid:54)≥ JP S
PROOF. Proof is by contradiction via counter-example. Let W =
2 (JF S −
2 with θ1 = θ2 = 1/2, and let pp = pf = p. Evaluate 1T
JP S)12, (the sum of each element of the matrix difference), which
we expect to be nonnegative by assumption. It can be shown that
this reduces to
2 − 2q(1 + q2)
1 + q
− 2
1 + 3q − 4q3
1 + 2q
− q2
which can be negative, e.g. when q = 1/2, a contradiction.
Since we earlier showed that PS is enormously worse than FS for
individual variances, this result is surprising, particularly as expe-
rience shows that it is not difﬁcult to ﬁnd other counter-examples
for much larger W . We can explain this quandry as follows. When
we focus on the variances in isolation we highlight the poor per-
formance of PS. However, linear combinations such as
k akθk
bring in cross terms, which are negative because of strong ambigu-
ity in the observations made by PS. Strong correlations are a bad
feature of a covariance matrix, however if they are negative, they
can cancel other positive terms resulting in a lower total variance.
Hence, paradoxically, it is the poor behavior of PS which prevents
JF S ≥ JP S from holding.
(cid:80)
In view of the above, we now focus on comparisons of the indi-
vidual variance bounds for the θk, the diagonal terms.
We now give one of our main results, a detailed characterization
of the performance of DS under the ESR normalization which is
the key to our optimality result in Section 4.4.
THEOREM 4.3. The diagonal elements 2 ≤ j ≤ W of J
−1
DS
under the ESR normalization is monotonically decreasing in pp.
The property holds for j = 1 iff the condition,
θ1(1 − θ1)
θ2 ≥ D − 1
(21)
D
is satisﬁed. Also, monotonicity holds when D = 1 or when W = 2.
PROOF. For simplicity, we consider the case j = W . By substi-
tuting (20) into the diagonals given by (18) and then differentiating
w.r.t pp,
−1)W W ) = − θW (1 − θW )
((J
 p∗
p, we focus is now on DS as the best
non-FS method. This is also strongly motivated by the freedom
within the DS family for some optimization and in particular to
approach FS and hence FS performance.
4.3 Analysis of the SEQ Dividend
The utilization of sequence numbers provides additional ‘virtual’
packets for free. The extent to which this improves our information
on (cid:126)θ however clearly depends on parameters. For example ﬂows
with only k = 1 or 2 packets are not helped at all.
Intuitively
it is the ﬂows for which k (cid:192) 1/pp which will receive the most
beneﬁt: since only the packets before the ﬁrst and after the last
physically sampled packets will not be recovered using sequence
numbers, and on average there are 2/pp of these since 1/pp is the
average gap between two physically sampled packets within a ﬂow.
If the SYN packet is present, this halves to 1/pp packets missed on
average.
To quantify the beneﬁt of sequence numbers better we deﬁne
the effective packet gain, which is the ratio r = E[ ˜Nk]/E[Nk],
where ˜Nk and Nk are the number of SEQ assisted and unassisted
(physical) packets sampled respectively. By deﬁnition, ˜Nk/Nk ≥
1 and so r ≥ 1, and asymptotically the gain saturates at r = 1/pp.
To obtain a ratio α of this maximum gain, one can show that a ﬂow
must have a size of the order of qp(1+α)
pp(1−α) or more (see [9] for more
details). This formula shows that ﬂows must have a size of roughly
1/pp or more before the sequence number ‘information dividend’
effectively switches on. It follows that its inﬂuence is a function not
only of the sampling parameters, but also (cid:126)θ. Effectively, it reduces
the size of the smallest ﬂow whose size can be estimated well.
4.4 Computational Comparison
From our result on the monotonic behavior of ESR normalized
DS, what is optimal in terms of information is clear: simply move
down the ESR constraint curve to FS. However, there are other con-
straints from the resource side which will constrain the parts of the
curve that will be accessible. The solution to the joint informa-
tion/resource optimization problem is therefore clear: move as far
down the ESR curve as possible. Our task now is to determine
those constraints and hence the region in the (pp, pf ) plane which
is feasible.
The bottlenecks in the implementation of any sampling method
is the memory access time and memory size. CPU processing
power is included in the memory access time, since the CPU has
to read out values from memory during lookup, performing mod-
iﬁcations and write backs when necessary. These tasks constitute
the main portion of what is required of the CPU for measurement,
as the measurement process is basically all about efﬁcient counting
[5]. We now consider how to optimize DS based on these con-
straints. Regarding ﬂows, let Tmax be the maximum ﬂow table
size measured in terms of ﬂow records and λF be the ﬂow arrival
rate. As for packets, let C be the capacity of the link, P the size of
the smallest packet (≈ 40 bytes), and τ the access time of memory
(nominally DRAM).
Our simple analysis of the above bottleneck constraints is based
on the following. In terms of packet arrivals we assume the worst
case, namely the smallest size packets arriving back-to-back at line
rate. By bounding the processing in such a case we guarantee
that the front line of packet processing (which occurs at the high-
est speeds) is not under-dimensioned. In terms of ﬂow arrivals we
assume the ‘average case’ based on the average number of active
ﬂows. This can easily be made more conservative by replacing the
average by some quantile to take into account ﬂuctuations in ﬂow
arrivals.
Consider the processing of a single packet. The SYN bit is ﬁrst
tested to see which sampling parameter will apply, the cost of this
is negligible, and if a packet is not sampled no further action is
needed. Now consider the cost of a packet which is sampled. Each
SYN packet which is sampled is inserted into the ﬂow table. No
prior lookup is necessary since it must be the ﬁrst packet of its
ﬂow. Each non-SYN packet which is sampled must ﬁrst perform
a lookup of its ﬂow-ID in the ﬂow table to see if that ﬂow is be-
ing tracked. If not it is discarded. The cost of this wasteful per
packet implementation of the ‘ﬂow discarding’ step (inherent to
any SYN based method such as DS) is not the bottleneck because
the following case is the most expensive of all and is the one we
model: a non-SYN packet which is sampled and whose ﬂow is be-
ing tracked requires both a lookup followed by an update. For the
purpose of simplicity, we ignore data export constraints from the
measurement center to a central data collection center. We also do
not consider rate adaptation based on the trafﬁc condition although
such schemes are compatible with DS.
Using the above, the constraints are
pf ≤ ˆpf = min (
Tmax
DλF
,
P
τ C
),
pp ≤ ˆpp =
P
2τ C
.
(23)
The constraint Tmax/(DλF ) ensures that the average number DλF
of active ﬂows does not exceed the ﬂow table size. Constraint
P/(τ C) provides the worst case bound for per packet processing,
for a single operation (insert or update). The factor of 2 that ap-
pears in the denominator for ˆpp is to account for the worst case, in
which a lookup and update is needed. The analysis is based on the
use of a single per ﬂow counter to track ﬂow size. In practice, there
may be more counters needed to track other quantities of interest,
necessitating tighter constraints.
In the sequel, our examples consider trafﬁc mixes that have man-
ageable numbers of SYN packets, so that from (23), ˆpf = Tmax/(DλF ).
This is done mainly to illustrate the relationship between the CRLB
and parameter pf . Indeed, at lower link speeds (OC-48 for exam-
ple), pf is bounded by the number of ﬂows arriving at the mea-
surement point, rather than packet processing time. Secondly, to
increase accuracy, from the previous discussions, we would like
pf < pp, i.e. less, but better quality sampled ﬂows.
The constraints form a simple region on the (pp, pf ) plane that
is convex, since it is rectangular around the origin. We ﬁrst want
to maximize the ESR subject to these constraints. Since the ESR
curve is a convex curve with respect to pf and pp, the optimal value
must lie on the vertex of the convex constraint set [15, Corollary
32.3.1, p. 344]. Therefore, the solutions are pf = ˆpf and pp = ˆpp.