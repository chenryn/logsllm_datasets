that many performance metrics are often confused with quality-of-service metrics.
Quality of service is, for the most part, a subset of performance, or at least it can
be fixed by improving performance. Attacks that aim for denial of service are also
typically exploiting performance issues in software.
3.7.4
Robustness Testing
The fourth black-box testing category is negative testing, or robustness testing. This
is often the most important category from the security perspective. In negative test-
ing, the software is tortured with semi-valid requests, and the reliability of the
software is assessed. The sources of negative tests can come from the systems spec-
ifications, such as
• Requirement specifications: These are typically presented as “shall not” and
“must not” requirements.
• System specifications: Physical byte boundaries, memory size, and other
resource limits.
• Design specifications: Illegal state transitions and optional features.
• Interface specifications: Boundary values and blacklisted characters.
• Programming limitations: Programming language specific limits.
3.8
Testing Metrics
There is no single metric for black-box testing, but instead various metrics are
needed with different testing approaches. At least three different levels of metrics
are easily recognized:
• Specification coverage;
• Input space coverage;
• Attack surface coverage.
We will next give a brief overview of these, although they are explained in more
detail in Chapter 4.
3.8.1
Specification Coverage
Specification coverage applies to all types of black-box testing. Tests can only be as
good as the specification they are built from. For example, in Voice over IP (VoIP)
testing, a testing tool has to cover about 10 different protocols with somewhere from
one to 30 industry standard specifications for each protocol. A tool that covers only
one specification has smaller test coverage than a tool that covers all of them. All
tests have a specification, whether it is a text document, a machine-understandable
interface model, or a capture of a test session that is then repeated and modified.
88
Quality Assurance and Testing
3.8.2
Input Space Coverage
Each interface specification defines a range of inputs that can be given to the soft-
ware. This is sometimes represented in BNF form at the end of RFC documents.
Let’s illustrate this with a fictitious example of an interface that consists of two val-
ues: an eight-character string as a user name and a four-digit pin code. Trying one
predefined user name with a small sample of pin codes achieves less input space
coverage than trying all ten thousand pin codes for the same user name.
3.8.3
Interface Coverage
Software has different communication interfaces, and different pieces of code are
touched through each one. Testing just one interface can give you less test coverage
in the target system than testing two interfaces.
3.8.4
Code Coverage
Different code coverage metrics are available for different purposes. But, be careful
when using them, as code coverage metrics do not necessarily indicate anything
about the quality of the tests to the end customer. In some cases, a test with smaller
code coverage can find more flaws than a test with large code coverage. Code cov-
erage can also be impossible for the customer to validate.
3.9
Black-Box Testing Techniques for Security
Before turning our attention away from a QA focus in fuzzing, we want to summa-
rize the various tools and techniques used in various types of security testing. Many
of these have very little relevance to fuzzing, but might help you resolve some mis-
conceptions other people have about security testing.
3.9.1
Load Testing
The easiest and best-known attack to QA people are various DoS (Denial of Ser-
vice) situations. The majority of DoS attacks are based on load. In load testing, the
performance limitations of the system are tested with fast repetition of a test case and
by running several tests in parallel. This is relevant to fuzzing because these tests can
be fuzz tests. When a fuzz test is repeated very fast, it can discover problems that
are missed by slowly executing fuzzing tools. One example of such a test is related
to testing for memory leaks or performance problems. If a test case indicates that
there could be some problems, the test case can be extracted and loaded into a per-
formance test tool through the record-and-playback functionality most of these
tools possess. Another benefit from load testing comes when testing proxy compo-
nents such as gateways and firewalls. When a load-generation tool is used in paral-
lel with fuzzing tools, the load-testing tool will help measure the change in the load
tolerance of the system. Fuzz tests under a load can also result in different test
3.9
Black-Box Testing Techniques for Security
89
results. All these results are very feasible in a live server, which almost always will
be under a normal system load when an attack arrives.
3.9.2
Stress Testing
A stress test will change the operational environment for the SUT by restricting
access to required resources. Examples of changes include
• Size and speed of available memory;
• Size and speed of available disk;
• The number of processors available and the processing speed of the processors;
• Environmental variables.
Most often stress tests are executed in a test automation framework that will
enable you to run the SUT inside a controlled environment such as a sandbox or
software development simulation.
3.9.3
Security Scanners
Using security scanners in software development is common, but this is mostly
because of a misunderstanding of a customer requirement. If a customer requires
that a software developer will run Nessus, for example, against their product, it will
naturally become part of the software development practice. The pros and cons of
this approach were explained in Chapter 2.
3.9.4
Unit Testing
In unit testing, the SUT is a module used inside the actual application. The real
application logic can be bypassed by implementing parts of the functionality in pro-
totypes when the real implementation is still unavailable or by other replacement
implementations when the target is actually a library such as a file parser or a codec.
For example, when testing HTML parsers, you do not necessarily want to run the
tests against the full web browser, but you can use the same HTML parsing API
calls through a test driver. In such a setup, a ten or hundred-fold increase in the
speed of the fuzzing process is easily obtained.
3.9.5
Fault Injection
Traditionally, the term fault injection has meant a hardware testing technique in
which artificial faults are introduced into printed circuit boards. For example, the
connections might be short-circuited, broken, grounded, or stuck to a predefined
value such as “0” or “1.” The printed board is then used and the resulting behav-
ior observed. The purpose is to test the sensitiveness of the hardware for faults
(fault tolerance) emerging during manufacturing or product lifetime. Fault injection
can be used to forecast the behavior of hardware during operations or to guide
efforts on making the hardware more robust against flaws.
Software fault injection has the same operation principle. There are two main
types of fault injection:
90
Quality Assurance and Testing
• Data fault injection.
• Code fault injection.
In short, faults are injected by mutating code or data to assess the response of
a software component for anomalous situations. In code fault injection (also called
mutation testing) the source code is modified to trigger failures in the target sys-
tem. Source code fault injection is best performed automatically, as an efficient fault
injection process can involve hundreds of modifications, each requiring a rebuild of
the target system.
The following two examples include fault injection at source code level.
Example 1:
char *buffer;
/* buffer =(char*)malloc(BUFFER_LENGTH); */
buffer =NULL;
Example 2:
int divider;
/* divider =a *b +d /max +hi; */
divider =0;
Structural fault injection (source code mutations) can be used for simulating
various situations that are difficult to test otherwise except by modifying the oper-
ating environment:
• Memory allocation failures.
• Broken connections.
• Disk-full situations.
• Delays.
Data fault injection, on the other hand, is just another name for fuzzing and
consists of injecting faults into data as it is passed between various components.
3.9.6
Syntax Testing
I do not know which was first, syntax testing or fuzzing, and I do not know if that
is even an important question. Testing gurus such as Boris Beizer created syntax
testing, and security experts such as Dr. B.P. Miller stumbled upon fuzzing. Both of
them were published around the same time and were most probably created to solve
the same problem. Let’s start by quoting the beginning of the chapter on syntax
testing by Dr. Boris Beizer from 1990:7
Systems that interface with the public must be especially robust and consequently
must have prolific input-validation checks. It’s not that the users of automatic teller
machines, say, are willfully hostile, but that there are so many of them—so many
3.9
Black-Box Testing Techniques for Security
91
7Chapter 9, section 2.2 unmodified and in its entirety, from Boris Beizer. (1990) Software Test-
ing Techniques, 2nd ed, International Thomson Computer Press. Quoted with permission.
of them and so few of us. It’s the million monkey phenomenon: A million monkeys
sit at a million typewriters for a million years and eventually one of them will type
Hamlet. The more users, the less they know, the likelier that eventually, on pure
chance, someone will hit every spot at which the system’s vulnerable to bad inputs.
There are malicious users in every population—infuriating people who delight
in doing strange things to our systems. Years ago they’d pound the sides of vend-
ing machines for free sodas. Their sons and daughters invented the “blue box” for
getting free telephone calls. Now they’re tired of probing the nuances of their video
games and they’re out to attack computers. They’re out to get you. Some of them
are programmers. They’re persistent and systematic. A few hours of attack by one
of them is worse than years of ordinary use and bugs found by chance. And there
are so many of them: so many of them and so few of us.
Then there’s crime. It’s estimated that computer criminals (using mostly hokey
inputs) are raking in hundreds of millions of dollars annually. A criminal can do it
with a laptop computer from a telephone booth in Arkansas. Every piece of bad
data accepted by a system—every crash-causing input sequence—is a chink in the
system’s armor that smart criminals can use to penetrate, corrupt, and eventually
suborn the system for their own purposes. And don’t think the system’s too com-
plicated for them. They have your listings, and your documentation, and the data
dictionary, and whatever else they need. There aren’t many of them, but they’re
smart, motivated, and possibly organized.
The purpose of syntax testing is to verify that the system does some form of
input validation on the critical interfaces. Every communication interface presents
an opportunity for malicious use, but also for data corruption. Good software
developers will build systems that will accept or tolerate any data whether it is non-
conformant to the interface specification or just garbage. Good testers, on the other
hand, will subject the systems to the most creative garbage possible. Syntax testing
is not random, but instead it will automate the smart fuzzing process by describing
the operation, structure, and semantics of an interface. The inputs, whether they
are internal or external, can be described with context-free languages such as
Backus-Naur Form (BNF). BNF is an example of a data description language that
can be parsed by an automated test generation framework to create both valid and
invalid inputs to the interface.
The key topic in syntax testing is the description of the syntax. Every input has
a syntax, whether it is formally described or undocumented, or “just understood.”
We will next explain one notation called BNF with examples. One of the most sim-
ple network protocols is TFTP. And an overly simplified description of TFTP using
BNF would be as follows:
 ::= (0x00 0x01)  
 ::= (0x00 0x02)  
 ::= ("octet" | "netascii") 0x00
 ::= {  } 0x00
 ::= 0x01 -0x7f
From the above example we can see that there are two types of messages
defined, a “read request”  and a “write request” . A header of two
92
Quality Assurance and Testing
octets defines the choice of the message type: (0x00 0x01) versus (0x00 0x02). Both
messages contain two strings: a file name  and a transfer mode
. The file name can consist of variable length text string built from char-
acters. The transfer mode can be a zero-terminated text string of two predefined
values. The “|” character defines an OR operand, which means only one of the val-
ues is allowed. Pretty simple, right.
The strategy in syntax test design is to add one anomaly (or error) at a time
while keeping all other components of the input structure or message correct. With
a complex interface, this alone typically creates tens of thousands of dirty tests.
When double errors and triple errors are added, the amount of test cases increases
exponentially.
Several different kinds of anomalies (or errors) can be produced in syntax testing:
• Syntax errors: Syntax errors violate the grammar of the underlying language.
Syntax errors can exist on different levels in the grammar hierarchy: top-level,
intermediate-level, and field-level. Simplest field-level syntax errors consist of
arbitrary data and random values. Intermediary and top-level syntax errors
are omitting required elements, repeating, reordering, and nesting any ele-
ments or element substructures.
• Delimiter errors: Delimiters mark the separation of fields in a sentence. In
ASCII-coded languages the fields are normally characters and letters, and
delimiters are white-space characters (space, tab, line-feed, etc.), other delim-
iter characters (commas, semicolons, etc.), or their combinations. Delimiters
can be omitted, repeated, multiplied, or replaced by other unusual characters.
Paired delimiters, such as braces, can be left unbalanced. Wrong unexpected
delimiters can be added at places where they might not be expected.
• Field-value errors: A field-value error is an illegal field in a sentence. Nor-
mally, a field value has a range or many disjoint ranges of allowable values.
Field-value errors can test for boundary-value errors with both numeric and
non-numeric elements. Values exactly at the boundary range or near the
boundary range should also be checked. Field errors can include values that
are one-below, one-above and totally out-of-range. Tests for fields with inte-
ger values should include boundary values. Use of powers of two plus minus
one as boundary values is encouraged since such a binary system is the typi-
cal native presentation of integers in computers.
• Context-dependent errors: A context-dependent error violates some property
of a sentence that cannot, in practice, be described by context-free grammar.
• State dependency error: Not all sentences are acceptable in every possible
state of a software component. A state dependency error is, for example, a
correct sentence during an incorrect state.
Note that some people have later used the term syntax testing for auditing of
test specifications. The purpose of such a test is to verify that the syntax in test def-
initions in correct. We will ignore this definition for syntax testing in this book and
propose that you do the same.
3.9
Black-Box Testing Techniques for Security
93
3.9.7
Negative Testing
Negative testing comes in many forms. The most common type of negative testing is
defining negative tests as use cases—for example, if a feature implements an authenti-
cation functionality, a positive test would consist of trying the valid user name and
valid password. Everything else is negative testing, including wrong user name, wrong
password, someone else’s password, and so on. Instead of explaining the various
forms of manual tactics for negative testing, we will focus on explaining the auto-
mated means of conducting negative testing. Kaksonen coined the name “robustness
testing” for this type of test automation in his licentiate thesis published in 2001.8
The purpose of robustness testing is only to try negative tests and not to care
about the responses from the SUT at all. Robustness testing is a model-based neg-
ative testing approach that generates test cases or test sequences based on a
machine-understandable description of a use case (the model) or a template. The
model consists of protocol building blocks such as messages and sequences of mes-
sages, with various dynamic operations implemented with intelligent tags. Each
message consists of a set of protocol fields, elements that have a defined syntax
(form) and semantics (meaning) defined in a protocol specification. The following
enumerates the various levels of models and contents in the model:
• A message structure consists of a set of protocol fields.
• A message is a context in which its structure is evaluated.
• A dialog is a sequence of messages.
• A protocol is a set of dialogs.
As said earlier, robustness testing is an automated means of conducting nega-
tive testing using syntax testing techniques. Robustness testing consists of the fol-
lowing steps, which are very similar to the steps in syntax testing:9
1. Identify the interface language.
2. Define the syntax in formal language, if not readily available in the proto-
col specification. Use context-free languages such as regular expressions,
BNF, or TTCN. This is the protocol of the interface.
3. Create a model by augmenting the protocol with semantics such as dynam-
ically changing values. As an example, you can define an element that
describes a length field inside the message. This is the model of the protocol.
4. Validate that the syntax (the protocol and the resulting model) is complete
(enough), consistent (enough), and satisfies the intended semantics. This is
often done with manual review. Risk assessment is useful when syntax test-
ing has security auditing purpose to prioritize over a wide range of ele-
ments, messages, and dialogs of messages.
94
Quality Assurance and Testing
8Rauli Kaksonen. (2001). A Functional Method for Assessing Protocol Implementation Security
(Licentiate thesis). Espoo. Technical Research Centre of Finland, VTT Publications 447. 128 p.
+ app. 15 p. ISBN 951-38-5873-1 (soft back ed.) ISBN 951-38-5874-X (on-line ed.).
9J. Röning, M. Laakso, A. Takanen, & R. Kaksonen. (2002). PROTOS—Systematic Approach
to Eliminate Software Vulnerabilities. Invited presentation at Microsoft Research, Seattle, WA.
May 6, 2002.
5. Test the syntax with valid values to verify that you will implement at least
the necessary use cases with the model. At this point you should be able to
“run” the model using test automation frameworks. The model does not
need to be complete for the testing purposes. If the purpose is to do clean
testing, you can stop here.
6. Syntax testing is best suited for dirty testing. For that, you need to identify
elements in the protocol and in the model that need dirty testing. Good
choices are strings, numbers, substructures, loops, and so on.
7. Select input libraries for the above elements. Most data fields and struc-
tures can be tested with predefined libraries of inputs and anomalies, and
those are readily available.
8. Automate test generation and generate test cases. The test cases can be
fully resolved, static binaries. But in most cases the test cases will still need
to maintain the dynamic operations unevaluated.
9. Automate test execution. Automation of static test cases is often simple, but
you need to have scripts or active code involved if the test dialog requires
dynamically changing data.
10. Analyze the results. Defining the pass/fail criteria is the most critical deci-
sion to make, and the most challenging.
The greatest difference from fuzzing is that robustness testing almost never has
any randomness involved. The tests are created by systematically applying a known
set of destructive or anomalous data into the model. The resulting tests are often
built into a test tool consisting of a test driver, test data, test documentation, and
necessary interfaces to the test bed, such as monitoring tools and test controllers.
The robustness tests can also be released as a test suite, consisting of binary test
cases, or their descriptions for use with other test automation frameworks and lan-