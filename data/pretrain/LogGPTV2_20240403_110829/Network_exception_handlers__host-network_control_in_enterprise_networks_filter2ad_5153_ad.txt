the Event Tracing for Windows6 and other monitoring APIs, allow
easy monitoring and access to the host state, e.g., application ids,
user ids, TCP/UDP ﬂows per process, without requiring any sup-
port from or modiﬁcations to existing applications. It is therefore
easy to translate access to the host state by the network exception
handlers into queries using these APIs.
Packet shaping functionality is also well supported in modern
operating systems. For example, the TC/GQOS API7 is provided
with Windows XP and Windows Vista provides the QOS2 API8
which both support trafﬁc shaping on a per-process or application
granularity. Indeed, blocking all trafﬁc from a process, for exam-
ple, is possible by conﬁguring the host ﬁrewall, again at process or
application granularity.
7. FEASIBILITY
In this section, we evaluate the feasibility of network exception
handlers across two different dimensions. We ﬁrst examine the
overhead of distributing the annotated topology information across
the enterprise network. We then present a concrete example of how
network exception handlers might be employed to tackle one of the
most important issues for IT departments of enterprise networks,
namely trafﬁc engineering to reduce bandwidth costs.
7.1 Datasets
The results presented in this paper are principally based on two
datasets collected from the global enterprise network of a large
multinational corporation. The ﬁrst is a corpus of packet data col-
lected from a single building connected to that enterprise network.
The second is a corpus of routing data collected from two different
areas of that enterprise network. Our local network within the mon-
itored building comprises several subnets and a small datacenter
(DC). The global enterprise network (CORPNET) contains approx-
imately 400,000 hosts connected by approximately 1,300 routers
spread across 100 countries and 6 continents. The particular build-
ing in question contains roughly 400 hosts used by a mixture of
researchers, administrative staff, and developers. For scalability
reasons, CORPNET contains 4 domains interconnected by BGP,
each of which runs OSPF internally. Connectivity to the Internet
6http://msdn2.microsoft.com/en-us/library/
aa468736.aspx
7http://msdn2.microsoft.com/en-us/library/
aa374472.aspx.
8http://msdn2.microsoft.com/en-us/library/
aa374110.aspx.
Figure 7: Aggregate trafﬁc over time. The spikes correspond to regu-
lar backups, while trafﬁc shows the expected diurnal patterns.
is provided through proxies in a small number of large datacen-
ters, roughly one per continent; these datacenters also host servers
providing company-wide services such as email.
In terms of size, CORPNET is at the upper end of the scale of en-
terprise networks as it supports a large, multinational, technology-
based corporation which makes extensive use of a variety of net-
worked applications. As such, we use data from this network as an
approximation to an upper bound on the complexity and scalability
of network exception handlers. In smaller networks the overheads
associated with network exception handlers should be lower. Small
networks, such as small ofﬁce/home networks, pose a quite differ-
ent set of network management problems and would probably not
signiﬁcantly beneﬁt from network exception handlers.
Data Trafﬁc Dataset
The data trafﬁc corpus was collected over a period of 7.7 weeks
beginning on Tuesday February 13, 2007. Packets were captured
using custom tools written against the WinPCap9 packet capture
library. Over the 7.7 weeks, 32.3 billion packets were captured,
containing 32.8 TB of data. To analyze the collected trace we
constructed ﬂow tables corresponding to 5 minute time intervals
with one record per uni-directional 5-tuple (source IP, destination
IP, protocol, source port, destination port) ﬂow observed in each
5 minute period. Each record contains the 5-tuple, the time of the
ﬁrst and last packets in the period, the number of bytes and packets
observed, and the application inferred to have generated the trafﬁc.
We inferred applications by custom-made deep packet inspection
tools, with less than 3.5% of packets not assigned to an application.
Fig. 7 shows a timeseries of the aggregate observed trafﬁc band-
width. The trafﬁc pattern follows the usual diurnal patterns, with
the exception of large spikes during the early morning hours of each
day which correspond to backups taking place in the Data Cen-
ter. We observed 66,697 unique IP addresses in the trace, 679 of
which were local to the capture site. Of the observed addresses,
65,086 were sources (663 of which were local to the monitored
building) and 49,031 were destinations (669 of which were local).
The local addresses that received but never transmitted appear to be
the result of automated security tools probing for active addresses,
i.e., they appear as destination addresses only and never as source
addresses.
9http://www.winpcap.org/
Table 1: Topology data. Core links represent physical connectiv-
ity. Leaf links represent advertised preﬁxes, including summaries. An
event is a link that fails, recovers or has its weight reconﬁgured.
Backbone
2004-11-05
2004-06-16
Stub
128
56
42
1241
720
Area
Date
Duration (weeks)
Gaps (days)
#Core links
#Leaf links
#Events (core)
#Events (leaf)
111
3
175
1123
183
414090
270428
Topology Datasets
The network topology traces are extracted from OSPF data col-
lected from two areas within CORPNET, one a backbone area and
one a stub area. An autonomous system (AS) running OSPF in-
ternally contains a single unique backbone area which connects all
the other stub areas in that AS; stub areas do not generally inter-
connect directly. CORPNET is split into four such ASs, with the
AS numbers designated for private use and are not visible to the
Internet at large. Each AS covers a large geographical region of the
world and each runs a single OSPF backbone area. Both the back-
bone and the stub areas monitored are within the same AS. Data
was collected until December 2006, beginning from the backbone
area in November 2004, and from the stub area in June 2004. Data
collection was almost continuous with about six short (roughly one
day each) gaps per year in the stub area trace, and approximately
one such gap per year in the backbone area trace. The stub area
trace also experienced about four larger gaps (roughly one week
each) over its entirety.
OSPF advertises links, which may represent either a preﬁx di-
rectly advertised by a router, a physical connection between two
routers (point-to-point or shared media), or preﬁxes being summa-
rized into this area from a neighboring area. Table 1 summarizes
the characteristics of the topologies extracted from the OSPF data.
7.2 Overhead
We examine two types of overhead incurred by network excep-
tion handlers. First, the total trafﬁc required to distribute the topol-
ogy across the network. Second, the amount of trafﬁc incurred by
distributing the metrics the topology is annotated with, e.g., link
load or link cost.
Live topology distribution. There are three components to the
overhead of distributing the live network topology: (i) the receive
bandwidth at each CC designated to gather the topology of its area;
(ii) the transmit bandwidth at each such CC to transmit its area
topology to other CCs; and (iii) the receive bandwidth at each CC
required to receive the topologies of all areas.
The ﬁrst component is very small, with median values of 71 Bps
and 70 Bps for our core and stub areas respectively. Even the 99th
percentiles for each are only 1.5 kBps (backbone) and 1.6 kBps
(stub). This is a negligible overhead for a server-class machine
to collect.
The second component is similarly negligible since most of the
OSPF trafﬁc does not actually report any changes to the area topol-
ogy, being simply HELLO packets and refreshes of existing state.
Only topology-changing events, i.e., addition of new links, failure
or recovery of existing links, or link weight modiﬁcations, need to
be redistributed. Examining events at the granularity of seconds,
we see that only 0.002% of the total second-intervals for the stub
area, and only 0.000007% of second-intervals for the core area,
contain even a single event. The burstiness is similarly low, with
SiteNames = [“Cambridge”];
Threshold = 100MB;
boolean Exception(NetworkState)
begin
foreach site in Sitenames do
siteLoad := NetworkState.Sites[site].UpStream.Load
if siteLoad > Threshold return true
return false
end
void Fire (NetworkState, HostState)
begin
Register(Handler)
end
void Handler (NetworkState, Hoststate)
begin
if not Exception(NetworkState) then
RemoveAllRestrictions(); DeRegister(Handler); return
foreach site in Sitenames do
upStreamLoad := NetworkState.Sites[site].UpStream.Load
ratio := Threshold / upStreamLoad
foreach app in Hoststate.Apps do
if app.isLowPriority then
bwCap := app.UpStream.Load * ratio
SetRestriction(app, maxBandwidth = bwCap)
end
Figure 8: Network exception handler to smooth upstream trafﬁc by
rate-limiting low priority applications at busy periods. The threshold
may be speciﬁed by historical data.
the 99.99th percentiles at just 10 events for both areas, and the max-
imum number of events observed in any single second standing at
just 91. Assuming events can be described in 25 bytes with 40 bytes
overhead for TCP/IP headers, this corresponds to a worst case of
around 2.5 kBps, and only 700 seconds over the three year period
would require more than 1.5 kBps, i.e., more than a single packet
per second.
The third component is less negligible but is still very small.
In our large enterprise network, there are approximately 62 dis-
tinct OSPF areas and 92 separate Active Directory domain con-
trollers. Assuming that topology distribution was carried out using
some efﬁcient mechanism such as native IP multicast or a suitably
conﬁgured application overlay, this corresponds to a worst case of
155 kBps being delivered to 92 receivers, and a 99.9th percentile
of 4 kBps.
Per-link annotations. Distributing link-related information such
as cost and load requires a higher overhead compared to the over-
head of distributing the topology, but overall is still not signiﬁcant.
For example, assume that up to ﬁfteen 4-byte counters per link are
distributed to each CC every 5 minutes, using a similar IP multicast
or application overlay mechanism. This corresponds to approxi-
mately 100 bytes per link every 5 minutes. Even in a large network
with 10,000 links this is a total overhead of just 3 kBps, and could
be reduced with more efﬁcient encoding of the per-link values.
Thus, the total overhead of distributing the control information
required to employ network exception handlers would be negligible
even in a large enterprise network.
7.3 Managing bandwidth with exception
handlers
We now evaluate a concrete example of network exception han-
dlers using our enterprise network trafﬁc dataset described in Sec-
tion 7.1. In particular, we describe a hypothetical scenario where
an exception handler is employed for bandwidth savings through
131Figure 9: Bandwidth savings by applying the network exception handler in Fig. 8. LEFT: Upstream non-local trafﬁc time-series
before and after applying the exception handler, where most spikes are smoothed-out. RIGHT: Trafﬁc CDF over 5-min intervals.
The exception handler reduces the 95th percentile by 10%.
shaping of trafﬁc caused by “low priority” user applications. Band-
width requirements currently reﬂect one of the major sources of
cost for enterprise networks as discussed in Section 2, and network
exception handlers can provide a ﬂexible and effective mechanism
to mitigate this cost by letting hosts decide how to best utilize net-
work resources, and speciﬁcally here bandwidth.
distribution before activating the exception handler. Interestingly,
WAN optimizers, which are increasingly being used in enterprise
networks, attempt to reduce WAN link demands by compressing
the trafﬁc passing over the link of interest. This example shows
the potential for network exception handlers to allow enterprises to
reduce WAN demands by shaping trafﬁc at the source, rather than
compressing trafﬁc in the network.
This example emphasizes the ﬂexibility of trafﬁc shaping at the
sources through network exception handlers. Hosts are both al-
lowed to decide how to prioritize applications subject to the spec-
iﬁed policy, but also have the ability to rate-limit their own trafﬁc
as the application-context exists at the edge of the network. In a
stricter scenario, certain applications could be instructed to com-
pletely back off. In such a scenario, exception handlers will pre-
vent trafﬁc from even entering the network (in contrast to drop-
ping packets in the network, where network resources would still
be used). Applying such policies of per-application throttling in
the middle of the network appears practically infeasible in existing
enterprise networks.
8. NETWORK EXCEPTION HANDLERS—
A UTOPIA?
Network exception handlers demonstrate the potential of push-
ing part of the network decision-making process to hosts. Having
highlighted the beneﬁts and ﬂexibility of network exception han-
dlers throughout the paper, we believe that this mechanism should
be a fundamental building block of modern enterprise networks.
Does this ﬂexibility however imply that network exception han-
dlers could be applied to every network operation, or that hosts
could, or should, participate in all network-related decisions? Net-
work exception handlers, while certainly powerful and ﬂexible, are
not the panacea of all enterprise management problems. They have
limitations that make them unsuitable to express certain policies.
Our proposed architecture also leaves a number of issues open for
future research and we discuss the most important of these here.
Cooperation. The set of actions performed in a network excep-
tion handler use only the global network topology information and
the local host state. There is no explicit cooperation or synchro-
nization between hosts. This limits the kinds of policy that can be
implemented. For example, using network exception handlers, it
is difﬁcult to implement the following policy: allow application
X running for users in Europe to consume no more than 50% of
a speciﬁc link. This would require all hosts concurrently running
One of the possible charging schemes for bandwidth usage in
enterprise networks is the 95th percentile of the upstream trafﬁc.
Bandwidth usage is calculated in 5 minute intervals, and at the end