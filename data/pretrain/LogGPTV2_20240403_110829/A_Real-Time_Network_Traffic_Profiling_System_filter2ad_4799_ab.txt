erate meaningful and easy-to-interpret event reports,
instead of streams of statistics.
Figure 2. The architecture of real-time trafﬁc
proﬁling system
event analysis engine. The ﬂow-level information used by
the proﬁling system are generated from continuous packet
or ﬂow monitoring systems that capture packet headers on a
high-speed Internet link via an optical splitter and a packet
capturing device, i.e., DAG card. The monitoring system
aggregates packets into 5-tuple ﬂows and exports the ﬂow
records for a given time interval into disk ﬁles.
In gen-
eral, the proﬁling system obtains ﬂow records through three
ways: i) shared disk access, ii) ﬁle transfer over socket, and
iii) ﬂow transfer over a streaming socket. The option in
practice will depend on the locations of the proﬁling and
monitoring systems. The ﬁrst way works when both sys-
tems run on the same machine, while the last two can be
applied if they are located in different machines.
In order to improve the efﬁciency of the proﬁling system,
we use distinct process threads to carry out multiple task in
parallel. Speciﬁcally, one thread continuously reads ﬂow
records in the current time interval Ti from the monitoring
systems, while another thread proﬁles ﬂow records that are
complete for the previous time interval Ti−1.
The event analysis engine analyzes a behavior proﬁle
database, which includes current and historical behavior
proﬁles of end hosts and network applications reported by
the behavior proﬁling and proﬁle tracking modules in the
proﬁling system.
The real-time trafﬁc proﬁling system consists of four
functional modules (shadowed boxes), namely, “cluster
construction”, “adaptive thresholding”, “behavior proﬁl-
ing” and “proﬁle tracking”. Each of these modules im-
plements one step in the trafﬁc proﬁling methodology de-
scribed in Section 2.
3.3 Key Implementation Details
3.2 System Architecture
3.3.1 Data Structures
Fig. 2 depicts the architecture of the proﬁling system that
is integrated with an “always-on” monitoring system and an
High speed backbone links typically carry a large amount of
trafﬁc ﬂows. Efﬁciently storing and searching these ﬂows is
critical for the scalability of our real-time proﬁling system.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:33:04 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007they share the same srcIP 10.0.0.1. Similarly, the next
srcPrt pointer of ﬂow 4 links to flow 1 since they
share the same srcPrt 80. However, the question is how
to quickly ﬁnd the “old” ﬂows of the same clusters when
adding a new ﬂow in the ﬂow table.
To address this problem, we create another data struc-
ture, CTable, which links the ﬁrst ﬂow of each cluster in
FTable. Since there are four types of clusters, we cre-
ate four instances of CTable for managing clusters along
four dimensions. Considering srcPrt and dstPrt di-
mensions with 65536 possible clusters (ports), we use an
array with a size of 65536 to manage the clusters for each
of these two dimensions. The index of the array for each
port is the same as the port number. For srcIP and dstIP
dimensions, we use a simple hash function that performs a
bitwise exclusive OR (XOR) operation on the ﬁrst 16 bits
and the last 16 bits of IP address to map each srcIP or
dstIP into its CTable entry. When adding a new ﬂow,
e.g., flow 3 in Fig. 3, in the given dstPrt, we ﬁrst lo-
cate the ﬁrst ﬂow (flow 2) of the cluster dstPrt 443,
and make the next dstPrt pointer of flow 3 to flow
2. Finally the ﬁrst ﬂow of the cluster dstPrt 443 is up-
dated to flow 3. This process is similar for the cluster
srcPrt 1208, as well as the the clusters srcIP 10.0.0.1
and dstIP 192.168.0.2.
In addition to pointing to the ﬁrst ﬂow in each cluster,
each CTable entry also includes ﬂow count for the clus-
ter and signiﬁcant bit for marking signiﬁcant clusters. The
former maintains ﬂow counts for cluster keys. As discussed
in Section 2, the ﬂow count distribution will determine the
adaptive threshold for extracting signiﬁcant clusters.
3.3.2 Space and Time Complexity of Modules
The space and time complexity of modules essentially de-
termines the CPU and memory cost of the proﬁling system.
Thus, we quantify the complexity of each module in our
proﬁling system. For convenience, Table 1 shows the deﬁ-
nitions of the notations that will be used in the complexity
analysis.
The time complexity of cluster construction is O(|F| +
(cid:1)3
i=0 |Ci|) for FTable and CTable constructions. Simi-
larly, the space complexity is O(|F|∗sf r+
i=0(|Ci|∗rv)).
The time complexity of adaptive thresholding is
(cid:1)3
i=0(|Ci| ∗ ei). This module does not allocate additional
memory, since its operations are mainly on the existing
CTable. Thus, the space complexity is zero.
(cid:1)3
of
proﬁling
i=0
behavior
complexity
The
(cid:1)3
(cid:1)3
time
is
(cid:1)|Si|
j=0 |sj|), while the space complexity is
O(
i=0[|Si| ∗ (rb + rs)]). The output of this step are
O(
the behavior proﬁles of signiﬁcant clusters, which are
recorded into a database along with the timestamp for
further analysis.
Figure 3. Data structure of ﬂow table and
cluster table
We design two efﬁcient data structures, namely FTable
and CTable for efﬁcient storage and fast lookups during
cluster extraction and behavior modeling.
hash
through
=
structure,
a
provides
commonly-used
Figure 3 illustrates the data structure of FTable
FTable, an ar-
and CTable with an example.
an index of 5-tuple
ray data
function,
ﬂows
srcip∧dstip∧srcport∧dstport∧proto %
F H
(F T ableEntries − 1), where F T ableEntries de-
notes the maximum entries of FTable. For example, in
Figure 3, ﬂow 1 is mapped to the entry 181 in FTable,
while ﬂow 2 is mapped to the entry 1. In case of hashing
collision, i.e., two or more ﬂows mapping to the same
table entry, we use a linked list to manage them.
In our
experiments, the (average) collision rate of this ﬂow hash
function is below 5% with F T ableEntries = 220. While
constructing clusters, the naive approach would be to make
four copies of 5-tuple ﬂows, and then group each ﬂow into
four clusters along each dimension. However, this method
dramatically increases the memory cost of the system since
the ﬂow table typically has hundreds or millions of ﬂows in
each time interval. Instead of duplicating ﬂows, which is
expensive, we add four ﬂow pointers (i.e., next srcIP,
next dstIP, next srcPrt, and next dstPrt) in
each ﬂow. Each ﬂow pointer will link the ﬂows sharing the
same feature value in the given dimension. For example,
the next srcIP pointer of ﬂow 4 links to flow 3 since
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:33:04 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Table 1. Notations used in the paper
Notation Deﬁnition
F
i
set of 5-tuple ﬂows in a time interval
dimension id (0/1/2/3 = srcIP/dstIP/
srcPort/dstPort)
set of clusters in dimension i
set of signiﬁcant clusters in dimension i
a cluster in dimension i
a signiﬁcant cluster in dimension i
size of a ﬂow record
size of the volume information of a cluster
size of behavior information of a sig. cluster
size of dominant states of a signiﬁcant cluster
Ci
Si
ci
si
rf
rv
rb
rs
Table 2. Total CPU and memory cost of
the real-time proﬁling system on 5-min ﬂow
traces
Link
Util.
CPU time (sec)
Memory (MB)
L1
L2
L3
207 Mbps
86 Mbps
78 Mbps
min
25
7
7
avg
46
11
12
max
65
16
82
min
82
46
45
avg
96
56
68
max
183
71
842
the process, respectively. The sum of these two times indi-
cates the total CPU time that the proﬁling process uses. Let
T denote the total CPU time, and Tl, Ta, and Tp denote the
CPU usage for the modules of cluster construction, adaptive
thresholding and behavior proﬁling, respectively. Then we
have
T = Tl + Ta + Tp
(1)
Similarly, we collect memory usage with another system
call, mallinfo(), which collects information of the dynamic
memory allocation. Let M denote the total memory usage,
and Ml, Ma, and Mp denote the memory usage in three key
modules. Then we have
M = Ml + Ma + Mb
(2)
In oder to track the CPU and memory usages of each
module, we use these two system calls before and after the
module. The difference of the output becomes the actual
CPU and memory consumption of each module. Next, we
show the CPU time and memory cost of proﬁling system on
three OC-48 links during a continuous 18-hour period with
an average link utilization of 209 Mbps, 86 Mbps, and 78
Mbps. For convenience, let L1, L2, and L3 denote these
three links, respectively.
Due to a small number of signiﬁcant clusters extracted,
the computation complexity of proﬁle tracking is often less
than the others in two or three orders of magnitude, so for
simplicity we will not consider its time and space require-
ment.
90
80
70
60
50
40
30
)
s
d
n
o
c
e
s
(
e
m
i
t
U
P
C
3.3.3 Parallelization of Input and Proﬁling
20
0
50
200
180
160
140
120
100
)
B
M
(
y
r
o
m
e
m
l
a
t
o
T
200
250
80
0
50
100
150
Index of time slots
200
250
100
150
Index of time slots
In order to improve the efﬁciency of the proﬁling system,
we use thread mechanisms for parallelizing tasks in multi-
ple modules, such as continuously importing ﬂow records in
the current time interval Ti, and proﬁling ﬂow records that
are complete for the previous time interval Ti−1. Clearly,
the parallelization could reduce the time cost of the proﬁl-
ing system. The disadvantage of doing so is that we have to
maintain two set of FTable and CTable for two consec-
utive time intervals.
4 Performance Evaluation
4.1 Benchmarking
We measure CPU usage of the proﬁling process by us-
ing a system call, namely, getrusage(), which queries actual
system and user CPU time of the process. The system call
returns with the resource utilization including ru utime and
ru stime, which represent the user and system time used by
(a) CPU time
(b) Memory cost
Figure 4. CPU and memory cost of the real-
time proﬁling system on ﬂow records in 5-
min time interval collected in L1 for 18 con-
secutive hours
.
Table 2 shows a summary of CPU time and memory
cost of the proﬁling system on L1 to L3 for 18 consecu-
tive hours. It is not surprising to see that the average CPU
and memory costs for L1 are larger than the other two links
due to a higher link utilization. Fig. 4 shows the CPU and
memory cost of the proﬁling system on all 5-min intervals
for L1 (the link with the highest utilization). For the major-
ity of time intervals, the proﬁling system requires less than
60 seconds (1 minute) of CPU time and 150MB of memory
using the ﬂow records in 5-min time intervals for L1.
Fig. 5[a] further illustrates the number of ﬂow records
over time that ranges from 600K to 1.6M, while Fig. 5[b]
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:33:04 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Total clusters
Total significant clusters
those caused by denial of service attacks, ﬂash crowds, and
worm outbreaks that increases the link utilization as well as
the number of ﬂow records.
100
Index of time slots
150
200
250