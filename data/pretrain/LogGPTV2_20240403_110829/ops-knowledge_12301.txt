User: I am trying to determine the best way to build a sort of pipeline system with many interdependent files that will be put through it, and I am wondering if anyone has specific recommendations regarding tools or approaches. We work mostly in Python and Linux.
We get files of experimental data that are delivered to "inbox" directories on an HPC cluster, and these must be processed in several linear, consecutive steps. The issue is that sometimes there are multiple samples that must be processed at some stages of the pipeline as a group, so e.g. samples can independently be put through steps A and B, but all samples in the group must have completed this process to proceed through step C (which requires all of the samples together).
It strikes me as a kind of functional problem, in that each step is kind of a modular piece and I will mostly only be checking for the existence of the output: if I have Sample 1 Step B output, I need Sample 2 Step B output so that I can then get Sample 1+2 C output.
I don't know a great deal about Puppet but I wonder if this kind of tool might be something I could use for this -- something that handles dependencies and deals with monitoring states? Any ideas?
Thanks,
Mario
Assistant: I think you are asking how you can transform multiple files when there are dependencies between the files, and possibly parallelise. The problem of resolving the dependencies is called a topological sort. Fortunately, the make utility will handle all of this for you, and you can use the -j flag to parallelise, which is easier than doing this yourself. By default it will only regenerate files if the input files change, but this is easy enough to get around by ensuring all outputs and intermediate files of each batch are removed / not present prior to invocation.