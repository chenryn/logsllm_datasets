title:dRMT: Disaggregated Programmable Switching
author:Sharad Chole and
Andy Fingerhut and
Sha Ma and
Anirudh Sivaraman and
Shay Vargaftik and
Alon Berger and
Gal Mendelson and
Mohammad Alizadeh and
Shang-Tse Chuang and
Isaac Keslassy and
Ariel Orda and
Tom Edsall
dRMT: Disaggregated Programmable Switching
Sharad Chole1, Andy Fingerhut1, Sha Ma1, Anirudh Sivaraman2, Shay Vargaftik3, Alon Berger3,
Gal Mendelson3, Mohammad Alizadeh2, Shang-Tse Chuang1, Isaac Keslassy3,4, Ariel Orda3, Tom Edsall1
1 Cisco Systems, Inc.
2 MIT
3 Technion
4 VMware, Inc.
ABSTRACT
We present dRMT (disaggregated Reconfigurable Match-Action
Table), a new architecture for programmable switches. dRMT over-
comes two important restrictions of RMT, the predominant pipeline-
based architecture for programmable switches: (1) table memory
is local to an RMT pipeline stage, implying that memory not used
by one stage cannot be reclaimed by another, and (2) RMT is hard-
wired to always sequentially execute matches followed by actions
as packets traverse pipeline stages. We show that these restrictions
make it difficult to execute programs efficiently on RMT.
dRMT resolves both issues by disaggregating the memory and
compute resources of a programmable switch. Specifically, dRMT
moves table memories out of pipeline stages and into a centralized
pool that is accessible through a crossbar. In addition, dRMT replaces
RMT’s pipeline stages with a cluster of processors that can execute
match and action operations in any order.
We show how to schedule a P4 program on dRMT at compile
time to guarantee deterministic throughput and latency. We also
present a hardware design for dRMT and analyze its feasibility and
chip area. Our results show that dRMT can run programs at line rate
with fewer processors compared to RMT, and avoids performance
cliffs when there are not enough processors to run a program at line
rate. dRMT’s hardware design incurs a modest increase in chip area
relative to RMT, mainly due to the crossbar.
CCS CONCEPTS
• Networks → Programmable networks; Routers;
KEYWORDS
Programmable switching; packet processing; RMT; disagreggation
ACM Reference format:
Sharad Chole, Andy Fingerhut, Sha Ma, Anirudh Sivaraman, Shay Vargaftik,
Alon Berger, Gal Mendelson, Mohammad Alizadeh, Shang-Tse Chuang,
Isaac Keslassy, Ariel Orda, and Tom Edsall. 2017. dRMT: Disaggregated
Programmable Switching. In Proceedings of SIGCOMM ’17, Los Angeles,
CA, USA, August 21-25, 2017, 14 pages.
https://doi.org/10.1145/3098822.3098823
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Association
for Computing Machinery.
ACM ISBN 978-1-4503-4653-5/17/08. . . $15.00
https://doi.org/10.1145/3098822.3098823
(a) RMT Architecture
(b) Disaggregated RMT (dRMT) Architecture
Figure 1: Comparison of the RMT [16] and dRMT archi-
tectures. dRMT replaces RMT’s pipeline stages with run-to-
completion match-action processors, and separates the memory
clusters from the processors via a crossbar. The dashed arrows
show the flow of a packet through each architecture.
INTRODUCTION
1
Historically, high-speed packet switching chips have been archi-
tected as a pipeline of match-action stages. For each incoming packet,
each stage (1) extracts specific packet header bits to generate a match
key, then (2) looks up this key in a match-action table, and finally
(3) uses the match result to run an action. For instance, a stage could
extract the packet’s IP destination address, look up this IP address in
a forwarding table, and use the result to determine the outgoing port.
In recent years, programmable switches [2, 7, 14, 16] have emerged,
allowing a switch pipeline’s match-action stages to be programmed
in languages like P4 [15].
RMT. The predominant architecture for programmable switches is
the Reconfigurable Match-Action Table (RMT) architecture [16].
As illustrated in Figure 1a, RMT uses a pipeline of match-action
stages, similar to conventional fixed-function switches. However,
RMT makes the match-action stages programmable; programmers
can specify the set of headers to match on, the type of match to
perform (exact, ternary, etc.), and compose their own compound
actions out of primitive actions.
RMT’s pipeline stages contain three kinds of hardware resources:
(1) match units that extract the header bits to form match keys, (2)
table memory in a local memory cluster, and (3) action units that
Stage 1Stage 2Stage NMatchActionMatchActionParserMatchActionOutDeparserQueuesMemory Cluster 1Memory Cluster 2Memory Cluster NInPktPktPktPktPktPktPktProc. 1Proc. 2Proc. NMatchActionMemory Cluster 1Memory Cluster 2Memory Cluster NInPktMatchOutQueuesDeparserCrossbar for search keys & resultsParserDistributorPktActionMatchActionPktPktSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Chole et al.
programmatically modify packet fields. For example, a stage might
have a match unit to extract up to 8 80-bit keys from the packet
header, 11 Mbit of SRAM and 1.25 Mbit of TCAM for tables, and
an action unit to modify up to 32 packet fields in parallel.
By connecting these resources in a sequential pipeline, RMT
simplifies wiring significantly [16]. However, RMT suffers from
two key drawbacks as a result of its pipelined architecture. First,
because each pipeline stage can only access local memory, RMT
must allocate memory for a table in the same stage that extracts its
match key and performs its action. This conflates memory allocation
with match/action processing, which makes table placement chal-
lenging [22] and can result in poor resource utilization (§2.1). For
instance, when a large table does not fit in one stage, it has to be
spread over multiple stages. But, in the process, the match/action
units of (all but one of) these stages are wasted, unless there are
other tables that can execute in parallel.
Second, RMT’s hard-wired pipeline can only execute operations
in a fixed order: a match followed by an action in stage 1, then a
match followed by an action in stage 2, and so on. This rigidity can
lead to under-utilization of hardware resources for programs where
matches and actions are imbalanced. For example, a program with
a default action that does not need a preceding match [12], such
as decreasing the packet’s TTL, wastes the match unit and table
memory in the pipeline stage that runs the default action. Moreover,
since packets can only traverse the pipeline sequentially, a program
that does not fit in the available hardware stages must recirculate
packets through the pipeline; this cuts throughput in half, even if the
program needs only one extra processing stage (§2.2).
dRMT. We propose dRMT (disaggregated RMT), a new architecture
for programmable switches that solves both problems confronting
RMT. dRMT’s key insight is to disaggregate the hardware resources
of a programmable switch. As illustrated in Figure 1b, dRMT disag-
gregates:
(1) Memory: dRMT separates the memory for tables from the
processing stages and makes them accessible via a crossbar.
The crossbar carries the search keys and results back and
forth between the match/action units and memories.
(2) Compute: dRMT replaces RMT’s
sequentially-wired
pipeline stages with a set of match-action processors. Match-
action processors consist of match and action units, similar to
RMT’s pipeline stages. But, unlike pipeline stages, packets
do not move between dRMT processors. Instead, each packet
is sent to one dRMT processor according to a round-robin
schedule. The packet resides at that processor, which runs the
entire program for that packet to completion.
Memory and compute disaggregation provide significant flexibil-
ity to dRMT. First, memory disaggregation decouples the memory al-
location for a table from the hardware that performs its match-action
processing. Second, compute disaggregation makes it possible to
interleave match/action operations in any order at a processor, both
for a given packet and across different packets. Finally, compute
disaggregation allows for inter-packet concurrency, the ability for
a processor to perform match/action operations on more than one
packet at a time. This flexibility results in increased hardware utiliza-
tion for dRMT relative to RMT, reducing the amount of hardware
(e.g., number of stages/processors) necessary to run a program at
line rate. Equivalently, it increases the set of programs that a fixed
amount of hardware can execute at line rate.
dRMT’s run-to-completion packet processing model has been pre-
viously used in some network processors [4, 10, 11]. However, these
network processors do not guarantee deterministic packet through-
put and latency. Nondeterminism occurs in network processors for
a variety of reasons, including cache misses and contention in the
processor-memory interconnect. In dRMT, we show how to schedule
the entire system (processors and memory) at compile time such
that no contention ever occurs. Given a P4 program, our scheduling
algorithm calculates a static schedule at compile time, guaranteeing
a deterministic throughput and latency (§3).
We evaluate dRMT using four benchmark P4 programs (§4),
three derived from the open-source switch.p4 [13] program and
another proprietary program from a large switching ASIC manu-
facturer. Across these programs, we find that dRMT requires 4.5%,
16%, 41%, and 50% fewer processors than RMT to achieve line-rate
throughput (1 packet per cycle). We also find that dRMT reduces the
number of processors required for line-rate throughput by an aver-
age of 10% (up to 30%) on 100 randomly generated programs with
characteristics similar to switch.p4. Further, dRMT’s throughput de-
grades gracefully with fewer processors, while RMT’s performance
falls off a cliff if the program needs more stages than provided by
the hardware.
We present a hardware design for dRMT (§5) and analyze its
feasibility and chip area cost (§6). dRMT’s flexibility relative to
RMT comes at some additional chip area cost to (1) implement a
crossbar that is absent in RMT, and (2) implement a match-action
processor that stores and executes an entire P4 program, unlike an
RMT stage that only stores and executes a fragment of the P4 pro-
gram. We present architectural optimizations that trade off modest
restrictions for a lower cost. While we have not built a dRMT chip,
our analysis shows that it is possible to implement dRMT with a chip
area comparable to RMT for the same number of processors/stages.
For example, a dRMT chip with 32 processors costs about 5 mm2
more area than RMT with 32 stages, a modest increase relative to
the total chip area of a typical switching chip (>200 mm2) [19].
dRMT’s scalability is limited by the wiring complexity of the cross-
bar. Scaling the crossbar far beyond 32 processors, which already
requires careful manual place and route, may be difficult. Fortu-
nately, switching chips are unlikely to need more than 32 processors
(e.g., a state-of-the-art programmable switch has 12 stages [1]).
The dRMT project page [5] contains the code required to repro-
duce our experimental results. It also contains an extended paper
with proofs of all theorems described in this paper.
2 THE CASE FOR DISAGGREGATION
2.1 Memory disaggregation
In RMT, each pipeline stage can only access its local memory cluster.
As a result, a table must reside in the memory of the same stage that
extracts its search key and executes its action.1 This leads to a cou-
pling between two problems: (1) choosing which match and action
1In theory, the action for a table could be deferred to a stage after the lookup. But
this requires passing the results of the lookup between stages along with the packet,
consuming extra space in the packet header vector [16]. RMT compilers [22] typically
avoid deferring actions for this reason.
dRMT: Disaggregated Programmable Switching
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
operations are executed by each stage, and (2) placing the program’s
tables into memory clusters. The first problem involves scheduling
match and action operations across stages such that the program’s
dependencies are not violated (see §3 for details). The second prob-
lem concerns packing the tables into the memory clusters. dRMT
decouples these two problems by enabling all processors to access
all memory clusters via a crossbar. This has several advantages.
Improving hardware utilization. The most important benefit of
memory disaggregation is significantly increased flexibility for map-
ping operations and tables to hardware resources more efficiently.
Example 2.1 (Parallel searches). Consider a program with four
tables whose searches can be done in parallel. If the search keys
for the four tables can be extracted in one RMT stage, but their
total table size exceeds the capacity of one memory cluster, then
some of the tables must be moved to later stages. Similarly, if the
tables fit in one memory cluster, but the key width exceeds the key
extraction capacity of one stage, some tables must be moved to later
stages. In either case, some resources—key extraction hardware or
table memory—are left unused in the first stage. With dRMT, how-
ever, key extraction and table placement are decoupled; a processor
can extract the four search keys and send them over the crossbar
to whichever memory clusters store the tables. (See §5.3 for the
crossbar’s design details.)
Example 2.2 (Large table). A large table may not fit entirely
within one memory cluster and may need to be split across multiple
stages. With RMT, each stage must search its part of the table—
extracting the same key multiple times—and the partial match results
must be combined together. The table’s action cannot be performed
until the last of the stages, potentially wasting the action units in all
but the last stage. In dRMT, the crossbar can multicast the search key
to multiple memory clusters, where the partial searches would be
done simultaneously. With a small amount of result-combining logic
in the return path from the memory clusters back to the processors,
the processor will only receive the highest-priority result.
Independently scaling processing/memory capacity. Memory dis-
aggregation makes it straightforward to select the number of proces-
sors and the number of memory clusters independent of each other,
based on the kinds of programs one wishes to execute. For example,
a designer can trivially add a TCAM memory cluster that is acces-
sible via the crossbar to increase TCAM capacity. By multicasting
search keys, the new TCAM can be allocated to any table that needs
it. By contrast, increasing the memory allocated to an RMT stage
without increasing its match/action capacity risks under-utilizing the
memory.
Simpler compilation. An RMT compiler must place tables across
pipeline stages while respecting the dependencies between program
operations [22]. A dRMT compiler needs to solve two simpler prob-