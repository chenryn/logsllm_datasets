title:Anomaly? application change? or workload change? towards automated
detection of application performance anomaly and change
author:Ludmila Cherkasova and
Kivanc M. Ozonat and
Ningfang Mi and
Julie Symons and
Evgenia Smirni
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
Anomaly? Application Change? or Workload Change?
Towards Automated Detection ofApplication Performance Anomaly and Change*
Ludmila Cherkasova 1, Kivanc Ozonat1, Ningfang Mi 2 Julie Symons 1, Evgenia Smimi 2
1 HPLabs Palo Alto &
2 College of William and Mary, Williamsburg
lucy. PI:EMAIL, kivan'c. ozonat@hp. com, ningfang@cs. wm. edu, julie. PI:EMAIL, esmirni@cs. wm. edu
Abstract
Automated tools for understanding application behavior
and its changes during the application life-cycle are essen(cid:173)
tial for many performance analysis and debugging tasks. Ap(cid:173)
plication performance issues have an immediate impact on
customer experience and satisfaction. A sudden slowdown
of enterprise-wide application can effect a large population
of customers, lead to delayed projects and ultimately can re(cid:173)
sult in company financial loss. We believe that online perfor(cid:173)
mance modeling should be a part ofroutine application mon(cid:173)
itoring. Early,
informative warnings on significant changes
in application performance should help service providers t?
timely identify and prevent performance problems and thelr
negative impact on the service. We propose a novel frame(cid:173)
workfor automated anomaly detection and application change
It is based on integration of two complementary
analysis.
techniques: i) a regression-based transaction model that re-
flects a resource consumption model ofthe application, and ii)
an application performance signature that provides a compact
model of run-time behavior of the application. The proposed
integratedframework provides a simple and powerful solution
for anomaly detection and analysis of essential performance
changes in application behavior. An additional benefit of the
proposed approach is its simplicity: it is not intrusi~e and is
based on monitoring data that is typically available In enter(cid:173)
prise production environments.
1 Introduction
Today's IT and Services departments are face~ ~ith the ?if(cid:173)
ficult task of ensuring that enterprise business-cntlcal applIca(cid:173)
tions are always available and provide adequate performance.
As the complexity of IT systems increases, performance man(cid:173)
agement becomes the largest and most.difficul~ expe~se to con(cid:173)
trol. We address the problem of efficIently dIagnOSIng essen(cid:173)
tial performance changes in application behavior in order. to
provide timely feedback to application designers an? servIce
providers. Typically, preliminary performance profihng of an
application is done by using synthetic w?rkloads. or .bench(cid:173)
marks which are created to reflect a "typIcal apphcatIon be(cid:173)
havior" for "typical client transactions". While such per~or­
Inance profiling can be useful at the initial stages of desIgn
and development of a future system, it may not be ad~qu~te
for analysis of performance issues and observed apphcatIon
behavior in existing production systems. For one thing, an ex(cid:173)
isting production system can experience a very different work-
*This work was completed in summer 2007 during N. Mis internship at
HPLabs. E. Smirni is partially supported by NSF grants ITR-0428330 and
CNS-0720699, and a gift from HPLabs.
load compared to the one that has been used in its testing envi(cid:173)
ronment. Secondly, frequent software releases and application
updates make it difficult and challenging to perform a t~oro~gh
and detailed performance evaluation ofan updated apphcatlon.
When poorly performing code slips into production and an ap(cid:173)
plication responds slowly, the organization inevitably looses
productivity and experiences increased operating costs.
Automated tools for understanding application behavior and
its changes during the application life-cycle are essential for
many performance analysis and debugging tasks. Yet, such
tools are not readily available to application designers and
service providers. The traditional reactive approach is to set
thresholds for observed performance metrics and raise alarms
when these thresholds are violated. This approach is not ade(cid:173)
quate for understanding the performance changes bet",:,een ap(cid:173)
plication updates. Instead, a pro-active approac? that IS bas~d
on continuous application performance evaluation may aSSIst
enterprises in reducing loss ofproductivity by time-consuming
diagnosis of essential performance changes in application per(cid:173)
formance.
With complexity of systems increasing and customer re(cid:173)
quirements for QoS growing, the research challenge is to de(cid:173)
sign an integratedframework ofmeasurement an~ system mod(cid:173)
eling techniques to support performance analysIs of complex
enterprise systems. Our goal is to design a framework that en(cid:173)
ables automated detection of application performance changes
and provides useful classification of the possible roo~ causes.
There are a few causes that we aim to detect and claSSIfy:
• Performance anomaly. By performance anomaly we
mean that the observed application behavior (e.g., current
CPU utilization) can not be explained by the observed ap(cid:173)
plication workload (e.g., the type and volume o~ trans(cid:173)
actions processed by the application sug~ests a ?lffere~t
level of CPU utilization). Typically, it mIght pOInt to eI(cid:173)
ther some unrelated resource-intensive process that con(cid:173)
sumes system resources or some unexpected application
behavior caused by not-fully debugged application code.
• Application transaction performance change. .By trans(cid:173)
action performance change we mean an essential change
(increase or decrease) in transaction processing time, e.g.,
as a result of the latest application update. If the detected
change indicates an increase of the transaction process(cid:173)
ing time then an alarm is raised to assess the amount of
additional resources needed and provides the feedback to
application designers on the detected change (e.g., is this
change acceptable or expected?).
is also important
It
to distinguish between perform.a~ce
anomaly and workload change. A performanc~ ano~aly IS In(cid:173)
dicative of abnormal situation that needs to be Investigated and
1-4244-2398-9/08/$20.00 ©2008 IEEE
452
DSN 2008: Cherkasova et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:22:14 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
resolved. On the contrary, a workload change (i.e., variations
in transaction mix and load) is typical for web-based appli(cid:173)
cations. Therefore, it is highly desirable to avoid false alarms
raised by the algorithm due to workload changes, though infor(cid:173)
mation on observed workload changes can be made available
to the service provider.
The rest of the paper is organized as follows. Section 2
introduces client vs server transactions. Section 3 provides
two motivating examples. Section 4 and Section 5 introduce
two complementary techniques as an integrated solution for
anomaly detection and application performance change. Sec(cid:173)
tion 6 presents a case study to validate the proposed techniques.
Section 7 describes related work. Finally, a summary and con(cid:173)
clusions are given in Section 8.
2 Client vs Server Transactions
The term transaction is often used with different meanings.
In our work, we distinguish between a client transaction and a
server transaction.
A client communicates with a web service (deployed as a
multi-tier application) via a web interface, where the unit of
activity at the client-side corresponds to a download of a web
page. In general, a web page is composed of an HTML file and
several embedded objects such as images. This composite web
page is called a client transaction.
Typically, the main HTML file is built via dynamic con(cid:173)
tent generation (e.g., using Java servlets or JavaServer Pages)
where the page content is generated by the application server
to incorporate customized data retrieved via multiple queries
from the back-end database. This main HTML file is called
a server transaction. Typically, the server transaction is re(cid:173)
sponsible for most latency and consumed resources [6] (at the
server side) during client transaction processing.
A client browser retrieves a web page (client transaction)
by issuing a series of HTTP requests for all the objects: first
it retrieves the main HTML file (server transaction) and after
parsing it, the browser retrieves all the embedded, static im(cid:173)
ages. Thus, at the server side, a web page retrieval corresponds
to processing multiple smaller objects that can be retrieved ei(cid:173)
ther in sequence or via multiple concurrent connections. It is
common that a web server and application server reside on the
same hardware, and shared resources are used by the applica(cid:173)
tion and web servers to generate main HTML files as well as
to retrieve page embedded objects.
Since the HTTP protocol does not provide any means to
delimit the beginning or the end of a web page, it is very dif(cid:173)
ficult to accurately measure the aggregate resources consumed
due to web page processing at the server side. There is no
practical way to effectively measure the service times for all
page objects, although accurate CPU consumption estimates
are required for building an effective application provisioning
model. To address this problem, we define a client transaction
as a combination of all the processing activities at the server
side to deliver an entire web page requested by a client, i.e.,
generate the main HTML file as well as retrieve embedded ob(cid:173)
jects and perform related database queries.
We use client transactions for constructing a "resource
consumption" model of the application. The server transac(cid:173)
tions reflect the main functionality of the application. We use
server transactions for analysis of the application performance
changes (if any) during the application life-cycle.
3 Two Motivating Examples
Frequent software updates and shortened application de(cid:173)
velopment time dramatically increase the risk of introducing
poorly performing or misconfigured applications to production
environment. Consequently, the effective models for on-line,
automated detection of whether application performance devi(cid:173)
ates of its normal behavior pattern become a high priority item
on the service provider's "wish list".
Example 1: Resource Consumption Model Change.
In earlier papers [20, 21], a regression-based approach is in(cid:173)
troduced for resource provisioning of multi-tier applications.
The main idea is to use a statistical linear regression for ap(cid:173)
proximating the CPU demands of different transaction types
(where a transaction is defined as a client transaction). How(cid:173)
ever, the accuracy of the modeling results critically depends
on the quality of monitoring data used in the regression analy(cid:173)
sis: if collected data contain periods ofperformance anomalies
or periods when an updated application exhibits very different
performance characteristics, then this can significantly impact
the derived transaction cost and can lead to an inaccurate pro(cid:173)
visioning model.
Figure 1 shows the CPU utilization (red line) of the HP
Open View Service Desk (OVSD) over a duration of I-month
(each point reflects an I-hour monitoring period). Most of
the time, CPU utilization is under 10%. Note that for each
weekend, there are some spikes of CPU utilization (marked
with circles in Fig. 1) which are related to administrator sys(cid:173)
tem management tasks and which are orthogonal to transaction
processing activities of the application. Once provided with
this information, we use only weekdays monitoring data for
deriving CPU demands of different transactions of the OVSD
service. As a result, the derived CPU cost accurately predicts
CPU requirements of the application and can be considered
as a normal resource consumption model of the application.
Figure 1 shows predicted CPU utilization which is computed
using the CPU cost of observed transactions. The predicted
CPU utilization accurately models the observed CPU utiliza(cid:173)
tion with an exception of weekends' system management peri(cid:173)
ods. However, if we were not aware of "performance anoma(cid:173)
lies" over weekends, and would use all the days (i.e., including
weekends) of the I-month data set - the accuracy of regression
would be much worse (the error will increase twice) and this
would significantly impact the modeling results.
I Observed ... _ -
Predicted - - I
30
25
20
10
~
~.g
~
:5
CI:l
0
0
10
15
20
25
30
35
Figure 1. CPU utilization of OVSD service.
Time (day)
1-4244-2398-9/08/$20.00 ©2008 IEEE
453
DSN 2008: CherkasQva et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:22:14 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Example 2: Updated Application Performance Change.
Another typical situation that requires a special handling is the
analysis of the application performance when it was updated
or patched. Fig. 2 shows the latency of two application trans(cid:173)
actions, Tr 1 and Tr2, over time (here, a transaction is defined
as a server transaction). Typically, tools like HP (Mercury) Di(cid:173)
agnostics [13] are used in IT environments for observing laten(cid:173)
cies of the critical transactions and raising alarms when these
latencies exceed the predefined thresholds. While it is useful
to have insight into the current transaction latencies that im(cid:173)
plicitly reflect the application and system health, this approach
provides limited information on the causes of the observed la(cid:173)
tencies and can not be used directly to detect the performance
changes of an updated or modified application. The latencies
ofboth transactions vary over time and get visibly higher in the
second half of the figure. This does not look immediately sus(cid:173)
picious because the latency increase can be a simple reflection
of a higher load in the system.
25 .___-~-____r_--.___-~-____r_--~
Tr1
Tr2
Vi'
.§.
>.
u
c:
Q)
:ffi
en
c:
~
20
15
10
5
;
::L
~l""
,,~
50
100
150
200
250
300
time (mins)
Figure 2. The transaction latency measured by HP (Mercury)
Diagnostics tool.
The real story behind this figure is that after timestamp
160, we began executing an updated version of the applica(cid:173)
tion code where the processing time of transaction Tr 1 is in(cid:173)
creased by 10 ms. However, by looking at the measured trans(cid:173)
action latency over time we can not detect this:
the reported
latency metric does not provide enough information to detect
this change.
Problem Definition. While using off-line data analysis we
can detect and filter out the time periods that correspond to
abnormal application performance or identify periods where
application performance experiences significant change, the
goal is to design an on-line approach that automatically detects
the performance anomalies and application changes. Such a
method enables a set of useful performance services:
• early warnings on deviations in expected application per(cid:173)
formance,
• raise alarms on abnormal resource usage,
• create a consistent dataset for modeling the application re(cid:173)
source requirements by filtering out performance anoma(cid:173)
lies and pointing out the periods of changed application
performance.
The next two sections present our solution that is based on in(cid:173)
tegration of two complementary techniques:
i) a regression(cid:173)
based transaction model that correlates processed transactions
and consumed CPU time to create a resource consumption
model of the application; and ii) an application performance
signature that provides a compact model of run-time behavior
of the application.
4 Regression-Based Approach for Detecting
Model Changes and Performance Anomalies
We use statistical learning techniques to model the CPU de(cid:173)
mand of the application transactions (client transactions) on a
given hardware configuration, to find the statistically signifi(cid:173)
cant transaction types, to discover the time segments where the
resource consumption of a given application can be approxi(cid:173)
mated by the same regression model, to discover time segments