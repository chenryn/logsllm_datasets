## üêõ Bug
When indexing into a tensor with another tensor, around 10 times the size of
the array is used in memory on the GPU.
## To Reproduce
    import torch
    def torch_memory(device):
        # Checks and prints GPU memory
        print(f'{torch.cuda.memory_allocated(device)/1024/1024:.2f} MB USED')
        print(f'{torch.cuda.memory_reserved(device)/1024/1024:.2f} MB RESERVED')
        print(f'{torch.cuda.max_memory_allocated(device)/1024/1024:.2f} MB USED MAX')
        print(f'{torch.cuda.max_memory_reserved(device)/1024/1024:.2f} MB RESERVED MAX')
        print('')
    device = torch.device(0)
    a = torch.randn((1, 32, 24, 512, 512), dtype=torch.float32, device=device)  # 768 MB tensor
    torch_memory(device)
    indices = a < 0  # 192 MB tensor
    torch_memory(device)
    a[indices] = 0.  # Allocates 10 times of 768 MB
    torch_memory(device)
This is the output that I get.
    768.00 MB USED
    768.00 MB RESERVED
    768.00 MB USED MAX
    768.00 MB RESERVED MAX
    960.00 MB USED
    960.00 MB RESERVED
    960.00 MB USED MAX
    960.00 MB RESERVED MAX
    960.00 MB USED
    8662.00 MB RESERVED
    8643.00 MB USED MAX
    8662.00 MB RESERVED MAX
The same thing happens when I use (supposed to be equivalent)
    a.index_put_((indices,), torch.tensor([0.]).to(device))
and even when I just index into the array without assigning new values to it,
a lot of memory is used.
    a[indices]
## Expected behavior
I'm not sure how much memory indexing/assignment like this is supposed to use,
but surely not 10 times.
I tried this with CuPy arrays and they use 2-3 times the size of the array in
memory.
## Environment
  * PyTorch Version (e.g., 1.0): 1.1, 1.4, 1.5
  * OS (e.g., Linux): Linux
  * How you installed PyTorch (`conda`, `pip`, source): conda
  * Build command you used (if compiling from source): N/A
  * Python version: 3.7
  * CUDA/cuDNN version: 10.1, 7.6
  * GPU models and configuration: V100
  * Any other relevant information: N/A