title:Tranco: A Research-Oriented Top Sites Ranking Hardened Against Manipulation
author:Victor Le Pochat and
Tom van Goethem and
Samaneh Tajalizadehkhoob and
Maciej Korczynski and
Wouter Joosen
TRANCO: A Research-Oriented Top Sites Ranking
Hardened Against Manipulation
Victor Le Pochat(cid:3), Tom Van Goethem(cid:3), Samaneh Tajalizadehkhooby, Maciej Korczy´nskiz, Wouter Joosen(cid:3)
zGrenoble Alps University
PI:EMAIL
(cid:3)imec-DistriNet, KU Leuven
{ﬁrstname.lastname}@cs.kuleuven.be
yDelft University of Technology
PI:EMAIL
Abstract—In order to evaluate the prevalence of security
and privacy practices on a representative sample of the Web,
researchers rely on website popularity rankings such as the Alexa
list. While the validity and representativeness of these rankings
are rarely questioned, our ﬁndings show the contrary: we show
for four main rankings how their inherent properties (similarity,
stability, representativeness, responsiveness and benignness) affect
their composition and therefore potentially skew the conclusions
made in studies. Moreover, we ﬁnd that it is trivial for an
adversary to manipulate the composition of these lists. We are
the ﬁrst to empirically validate that the ranks of domains in each
of the lists are easily altered, in the case of Alexa through as little
as a single HTTP request. This allows adversaries to manipulate
rankings on a large scale and insert malicious domains into
whitelists or bend the outcome of research studies to their
will. To overcome the limitations of such rankings, we propose
improvements to reduce the ﬂuctuations in list composition and
guarantee better defenses against manipulation. To allow the
research community to work with reliable and reproducible
rankings, we provide TRANCO, an improved ranking that we
offer through an online service available at https://tranco-list.eu.
I.
INTRODUCTION
Researchers and security analysts frequently study a selec-
tion of popular sites, such as for measuring the prevalence
of security issues or as an evaluation set of available and
often used domain names, as these are purported to reﬂect
real-world usage. The most well known and widely used list
in research studies is that of Alexa, with researchers’ reliance
on this commercial list being accentuated by their concern
when it was momentarily taken ofﬂine in November 2016 [11].
However, several companies provide alternative rankings based
on Internet usage data collected through various channels [54]:
a panel of users whose visits are logged, tracking code placed
on websites and trafﬁc captured by intermediaries such as ISPs.
We found that 133 top-tier studies over the past four years
based their experiments and conclusions on the data from these
rankings. Their validity and by extension that of the research
that relies on them, should however be questioned: the methods
behind the rankings are not fully disclosed, and commercial
interests may prevail
the
providers only have access to a limited userbase that may be
skewed towards e.g. certain user groups or geographic regions.
in their composition. Moreover,
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23386
www.ndss-symposium.org
Even though most providers declare that the data is processed
to remove such statistical biases, the lack of exact details
makes it impossible for researchers to assess the potential
impact of these lists on their results and conclusions.
In this paper, we show that the four main popularity rank-
ings (Alexa, Cisco Umbrella, Majestic and Quantcast) exhibit
signiﬁcant problems for usage in research. The rankings hardly
agree on the popularity of any domain, and the Umbrella and
especially the Alexa lists see a signiﬁcant turnover even on con-
secutive days; for Alexa, this is the result of an unannounced
and previously unknown change in averaging approach. All
lists include non-representative and even malicious sites, which
is especially dangerous considering the widespread use of these
rankings as whitelists. Overall, these ﬂaws can cause the choice
for a particular ranking to severely skew measurements of
vulnerabilities or secure practices.
Moreover, we are the ﬁrst to empirically prove that pitfalls
in these rankings leave them vulnerable to one of our newly
introduced manipulation techniques. These techniques have a
surprisingly low cost, starting from a single HTTP request for
Alexa, and can therefore be used to affect the rank of thousands
of domains at once on a substantial level: we estimate that the
top 10 000 can easily be reached. The incentives of adversaries
to alter the composition of these lists, both for single domains
due to the practice of whitelisting popular domains, and on
a larger scale to inﬂuence research and its impact outside
academia, make this manipulation particularly valuable.
Finally, there is still a need for researchers to study popular
that
domains, so they would therefore beneﬁt from a list
avoids biases in its inherent properties and is more resilient to
manipulation, and that is easily retrieved for future reference.
To this extent, we propose improvements to current rankings in
terms of stability over time, representativeness and hardening
against manipulation. We create TRANCO, a new ranking
that is made available and archived through an accompanying
online service at https://tranco-list.eu, in order to enhance the
reproducibility of studies that rely on them. The community
can therefore continue to study the security of popular domains
while ensuring valid and veriﬁable research.
In summary, we make the following contributions:
(cid:15) We describe how the main rankings can negatively affect
security research, e.g. half of the Alexa list changes every
day and the Umbrella list only has 49% real sites, as well
as security implementations, e.g. the Majestic list contains
2 162 malicious domains despite being used as a whitelist.
(cid:15) We classify how 133 recent security studies rely on these
rankings, in particular Alexa, and show how adversaries
could exploit the rankings to bias these studies.
(cid:15) We show that for each list
there exists at
least one
technique to manipulate it on a large scale, as e.g. only
one HTTP request sufﬁces to enter the widely used Alexa
top million. We empirically validate that reaching a rank
as good as 28 798 is easily achieved.
(cid:15) Motivated by the discovered limitations of the widely-
used lists, we propose TRANCO, an alternative list that is
more appropriate for research, as it varies only by 0.6%
daily and requires at least the quadrupled manipulation
effort to achieve the same rank as in existing lists.
II. METHODOLOGY OF TOP WEBSITES RANKINGS
Multiple commercial providers publish rankings of popular
domains that they compose using a variety of methods. For
Alexa, Cisco Umbrella, Majestic and Quantcast, the four lists
that are available for free in an easily parsed format and
that are regularly updated, we discuss what is known on how
they obtain their data, what metric they use to rank domains
and which potential biases or shortcomings are present. We
base our discussion mainly on the documentation available
from these providers; many components of their rankings are
proprietary and could therefore not be included.
We do not consider any lists that require payment, such as
SimilarWeb1, as their cost (especially for longitudinal studies)
and potential usage restrictions make them less likely to be
used in a research context. We also disregard lists that would
require scraping, such as Netcraft2, as these do not carry the
same consent of their provider implied by making the list
available in a machine-readable format. Finally, Statvoo’s list3
seemingly meets our criteria. However, we found it to be a
copy of Alexa’s list of November 23, 2016, having never been
updated since; we therefore do not consider it in our analysis.
A. Alexa
Alexa, a subsidiary of Amazon, publishes a daily up-
dated list4 consisting of one million websites since December
2008 [5]. Usually only pay-level domains5 are ranked, except
for subdomains of certain sites that provide ‘personal home
pages or blogs’ [8] (e.g. tmall.com, wordpress.com). In Novem-
ber 2016, Alexa brieﬂy took down the free CSV ﬁle with the
list [11]. The ﬁle has since been available again [10] and is still
updated daily; however, it is no longer linked to from Alexa’s
main website, instead referring users to the paid ‘Alexa Top
Sites’ service on Amazon Web Services [12].
The ranks calculated by Alexa are based on trafﬁc data
from a “global data panel”, with domains being ranked on a
proprietary measure of unique visitors and page views, where
one visitor can have at most one page view count towards the
page views of a URL [73]. Alexa states that it applies “data
normalization” to account for biases in their user panel [8].
1https://www.similarweb.com/top-websites
2https://toolbar.netcraft.com/stats/topsites
3https://statvoo.com/dl/top-1million-sites.csv.zip
4https://s3.amazonaws.com/alexa-static/top-1m.csv.zip
5A pay-level domain (PLD) refers to a domain name that a consumer or
business can directly register, and consists of a subdomain of a public sufﬁx
or effective top-level domain (e.g. .com but also .co.uk).
The panel
trafﬁc data. Moreover,
is claimed to consist of millions of users,
who have installed one of “many different” browser exten-
sions that include Alexa’s measurement code [9]. However,
through a crawl of all available extensions for Google Chrome
and Firefox, we found only Alexa’s own extension (“Alexa
Trafﬁc Rank”) to report
this exten-
sion is only available for the desktop version of these two
browsers. Chrome’s extension is reported to have around
570 000 users [1]; no user statistics are known for Firefox, but
extrapolation based on browser usage suggests at most one
million users for two extensions, far less than Alexa’s claim.
In addition, sites can install an ‘Alexa Certify’ tracking
script that collects trafﬁc data for all visitors; the rank can then
be based on these actual trafﬁc counts instead of on estimates
from the extension [8]. This service is estimated to be used by
1.06% of the top one million and 4% of the top 10 000 [19].
The rank shown in a domain’s proﬁle on Alexa’s website is
based on data over three months, while in 2016 they stated that
the downloadable list was based on data over one month [6].
This statement was removed after the brief takedown of this
list [7], but the same period was seemingly retained. However,
as we derive in Section III-B, since January 30, 2018 the list is
based on data for one day; this was conﬁrmed to us by Alexa
but was otherwise unannounced.
Alexa’s data collection method leads to a focus on sites
that are visited in the top-level browsing context of a web
browser (i.e. HTTP trafﬁc). They also indicate that ranks worse
than 100 000 are not statistically meaningful, and that for these
sites small changes in measured trafﬁc may cause large rank
changes [8], negatively affecting the stability of the list.
B. Cisco Umbrella
Cisco Umbrella publishes a daily updated list6 consisting
of one million entries since December 2016 [37]. Any domain
name may be included, with it being ranked on the aggregated
trafﬁc counts of itself and all its subdomains.
The ranks calculated by Cisco Umbrella are based on
DNS trafﬁc to its two DNS resolvers (marketed as OpenDNS),
claimed to amount to over 100 billion daily requests from
65 million users [37]. Domains are ranked on the number
of unique IPs issuing DNS queries for them [37]. Not all
trafﬁc is said to be used: instead the DNS data is sampled and
‘data normalization methodologies’ are applied to reduce bi-
ases [21], taking the distribution of client IPs into account [47].
Umbrella’s data collection method means that non-browser-
based trafﬁc is also accounted for. A side-effect is that invalid
domains are also included (e.g.
internal domains such as
*.ec2.internal for Amazon EC2 instances, or typos such as
google.conm).
C. Majestic
Majestic publishes the daily updated ‘Majestic Million’ list
consisting of one million websites7 since October 2012 [39].
The list comprises mostly pay-level domains, but includes sub-
domains for certain very popular sites (e.g. plus.google.com,
en.wikipedia.org).
6https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m.csv.zip
7http://downloads.majestic.com/majestic_million.csv
2
The ranks calculated by Majestic are based on backlinks
to websites, obtained by a crawl of around 450 billion URLs
over 120 days, changed from 90 days on April 12, 2018 [48],
[49]. Sites are ranked on the number of class C (IPv4 /24)
subnets that refer to the site at least once [39]. Majestic’s
data collection method means only domains linked to from
other websites are considered, implying a bias towards browser-
based trafﬁc, however without counting actual page visits.
Similarly to search engines, the completeness of their data is
affected by how their crawler discovers websites.
D. Quantcast
Quantcast publishes a list8 of the websites visited the most
in the United States since mid 2007 [60]. The size of the list
varies daily, but usually was around 520,000 mostly pay-level
domains; subdomains reﬂect sites that publish user content
(e.g. blogspot.com, github.io). The list also includes ‘hidden
proﬁles’, where sites are ranked but the domain is hidden.
The ranks calculated by Quantcast are based on the number
of people visiting a site within the previous month, and
comprises ‘quantiﬁed’ sites where Quantcast directly measures
trafﬁc through a tracking script as well as sites where Quant-
cast estimates trafﬁc based on data from ‘ISPs and toolbar
providers’ [64]. These estimates are only calculated for trafﬁc
in the United States, with only quantiﬁed sites being ranked
in other countries; the list of top sites also only considers US
trafﬁc. Moreover, while quantiﬁed sites see their visit count
updated daily, estimated counts are only updated monthly [65],
which may inﬂate the stability of the list. Before November 14,
2018, quantiﬁed sites made up around 10% of the full (US) list.
However, since then Quantcast seems to have stopped ranking
almost any estimated domains, therefore reducing the list size
to around 40 000.
III. QUANTITATIVE COMPARISON
Ideally, the domain rankings would perfectly reﬂect the
popularity of websites, free from any biases. However, the
providers of domain rankings do not have access to complete
Internet usage data and use a variety of largely undisclosed
data collection and processing methods to determine the metric
on which they rank websites. This may lead to differences
between the lists and potential ‘hidden’ factors inﬂuencing
the rankings: the choice of list can then critically affect e.g.
studies that measure the prevalence of security practices or
vulnerabilities. We compare the four main lists over time in
order to assess the breadth and impact of these differences.
Certain properties may reﬂect how accurately Internet
usage is measured and may be (more or less) desired when
using the lists for security research. We consider ﬁve properties
in our comparison: 1) similarity or the agreement on the set
of popular domains, 2) stability or the rank changes over time,
3) representativeness or the reﬂection of popularity across the
web, 4) responsiveness or the availability of the listed websites,
and 5) benignness or the lack of malicious domains.
To quantitatively assess these properties, we use the lists
obtained between January 1 and November 30, 2018, referring
to the date when the list would be downloaded; the data
8https://ak.quantcast.com/quantcast-top-sites.zip
Fig. 1. The average daily intersections between the lists of the four providers
from January 30, 2018 to November 13, 2018.
used by the provider to compile the list may be older. In
addition, we crawled the sites on the four lists as downloaded
on May 11, 2018 at 13:00 UTC from a distributed crawler
setup of 10 machines with 4 CPU cores and 8 GB RAM in
our European university network, using Ubuntu 16.04 with
Chromium version 66.0.3359.181 in headless mode.
A. Similarity
Figure 1 shows the average number of sites that
the
rankings agree upon per day;
there is little variance over
time. The four lists combined contain around 2.82 million
sites, but agree only on around 70 000 sites. Using the rank-
biased overlap (RBO) [72], a similarity measure that can be
parameterized to give a higher weight to better ranks, we see
that the lists of Alexa, Majestic and Quantcast are the most
similar to each other. However, even when heavily weighting
the top 100, the RBO remains low between 24% and 33%.
Umbrella’s full list is most dissimilar to the others, with an
RBO of between 4.5% and 15.5%. However, this is to be
expected as Umbrella includes subdomains: when ranking only
pay-level domains, the RBO with the other lists reaches around
30% as well. Finally, Quantcast’s removal of non-quantiﬁed
sites after November 14, 2018 causes a signiﬁcant drop in
RBO to less than 5.5%, with no overlap of the top 10: many
very popular domains are not quantiﬁed and are therefore now
missing from Quantcast’s list.
The small overlaps signify that there is no agreement on
which sites are the most popular. This means that switching
lists yields a signiﬁcantly different set of domains that can e.g.
change how prevalent certain web trackers seem to be [26].
B. Stability
From the intersections between each provider’s lists for two
consecutive days, shown in Figure 2, we see that Majestic’s and
Quantcast’s lists are the most stable, usually changing at most