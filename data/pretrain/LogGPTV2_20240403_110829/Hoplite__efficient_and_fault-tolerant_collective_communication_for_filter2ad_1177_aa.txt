title:Hoplite: efficient and fault-tolerant collective communication for
task-based distributed systems
author:Siyuan Zhuang and
Zhuohan Li and
Danyang Zhuo and
Stephanie Wang and
Eric Liang and
Robert Nishihara and
Philipp Moritz and
Ion Stoica
Hoplite: Efficient and Fault-Tolerant Collective Communication
for Task-Based Distributed Systems
Siyuan Zhuang1,‚àó Zhuohan Li1,‚àó Danyang Zhuo2 Stephanie Wang1
Eric Liang1 Robert Nishihara1 Philipp Moritz1 Ion Stoica1
ABSTRACT
Task-based distributed frameworks (e.g., Ray, Dask, Hydro) have be-
come increasingly popular for distributed applications that contain
asynchronous and dynamic workloads, including asynchronous
gradient descent, reinforcement learning, and model serving. As
more data-intensive applications move to run on top of task-based
systems, collective communication efficiency has become an impor-
tant problem. Unfortunately, traditional collective communication
libraries (e.g., MPI, Horovod, NCCL) are an ill fit, because they
require the communication schedule to be known before runtime
and they do not provide fault tolerance.
1University of California, Berkeley 2Duke University
1 INTRODUCTION
Task-based distributed systems (e.g., Ray [30], Hydro [19], Dask [44],
CIEL [32]) have become increasingly popular for developing and
running distributed applications that contain asynchronous and
dynamic computation and communication patterns, including asyn-
chronous stochastic gradient descent (SGD), reinforcement learning
(RL), and model serving. Today, many top technology companies
have started to adopt task-based distributed frameworks for their
distributed applications, such as Intel, Microsoft, Ericsson, and JP.
Morgan. For example, Ant Financial uses task-based distributed
systems to run their online machine learning pipeline and serve
financial transactions for billions of users [22].
There are two key benefits of building distributed applications
on top of task-based systems. First, it is easy to express asynchro-
nous and dynamic computation and communication patterns. A
task-based system implements a dynamic task model: a caller can
dynamically invoke a task ùê¥, which immediately returns an object
future, i.e. a reference to the eventual return value. By passing the
future as an argument, the caller can specify another task ùêµ that
uses the return value of ùê¥ even before ùê¥ finishes. The task-based
system is responsible for scheduling workers to execute tasks ùê¥ and
ùêµ and transferring the result of ùê¥ to ùêµ between the correspond-
ing workers. Second, fault tolerance is provided by the task-based
system transparently. When a task fails, the task-based system
quickly reconstructs the state of the failed task and resumes exe-
cution [49, 52]. Well-behaving tasks do not need to roll back, so
failure recovery is low cost.
As a growing number of data-intensive workloads are moving to
task-based distributed systems, supporting efficient collective com-
munication (e.g., broadcast, reduce) has become critical. Consider
an RL application where the trainer process broadcasts a policy
to a set of agents that use this policy to perform a series of simu-
lations. Without the support for collective broadcast, the trainer
process needs to send the same policy to every agent which causes
a network bottleneck on the sender side.
Efficient collective communication is a well-understood problem
in the HPC community and in distributed data-parallel training.
Many collective communication libraries exist today, e.g., Open-
MPI [16], MPICH [31], Horovod [47], Gloo [14], and NCCL [34].
However, there are two limitations of traditional collective com-
munication implementations that make them an ill fit for dynamic
task-based systems.
First, a distributed application using traditional collective com-
munication must specify the communication pattern before runtime.
This allows the library to compute a static and efficient data trans-
fer schedule (e.g., ring-allreduce). For example, for synchronous
We design and implement Hoplite, an efficient and fault-tolerant
collective communication layer for task-based distributed systems.
Our key technique is to compute data transfer schedules on the fly
and execute the schedules efficiently through fine-grained pipelin-
ing. At the same time, when a task fails, the data transfer schedule
adapts quickly to allow other tasks to keep making progress. We
apply Hoplite to a popular task-based distributed framework, Ray.
We show that Hoplite speeds up asynchronous stochastic gradient
descent, reinforcement learning, and serving an ensemble of ma-
chine learning models that are difficult to execute efficiently with
traditional collective communication by up to 7.8x, 3.9x, and 3.3x,
respectively.
CCS CONCEPTS
‚Ä¢ Computer systems organization ‚Üí Dependable and fault-
tolerant systems and networks; ‚Ä¢ Computing methodologies
‚Üí Distributed computing methodologies.
KEYWORDS
Collective Communication, Distributed Systems
ACM Reference Format:
Siyuan Zhuang, Zhuohan Li, Danyang Zhuo, Stephanie Wang, Eric Liang,
Robert Nishihara, Philipp Moritz, Ion Stoica. 2021. Hoplite: Efficient and
Fault-Tolerant Collective Communication for Task-Based Distributed Sys-
tems. In ACM SIGCOMM 2021 Conference (SIGCOMM ‚Äô21), August 23‚Äì27,
2021, Virtual Event, USA. ACM, New York, NY, USA, 16 pages. https://doi.org/
10.1145/3452296.3472897
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
¬© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8383-7/21/08.
https://doi.org/10.1145/3452296.3472897
‚àóEqual contribution.
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Siyuan Zhuang et al.
distributed data-parallel training, the application specifies that all
workers participate in an allreduce communication, once per train-
ing round.
However, in task-based systems, the set of tasks or data objects
participating in the collective communication is not known before
runtime. One approach would be to wait until all the participating
tasks and objects are ready and then compute a static data transfer
schedule. Unfortunately, this design misses the opportunity to make
partial progress before the entire set of participants are ready, which
is critical for the performance of modern asynchronous applications,
e.g., distributed RL.
Second, because of the synchronous nature of collective commu-
nications, one process failure can cause the rest of the processes to
hang. Existing solutions leave the recovery up to the application.
A typical approach is to checkpoint the state of the application
periodically (e.g., every hour), and when a process fails, the en-
tire application rolls back to the previous checkpoint and restarts.
Unfortunately, this can be expensive for large-scale asynchronous
applications, and does not exploit the ability of tasks that are still
alive in the same collective communication group as a failed task
to make progress.
This raises an important question: how can we bring the efficiency
of collective communication to dynamic and asynchronous task-based
applications? There are two requirements that are unique to this
setting. First, the application must be allowed to specify the partici-
pants of a collective communication dynamically (i.e., at runtime).
Second, the collective communication implementation must be
asynchronous. This would allow tasks to make progress even if
other tasks in the same communication group have failed.
We design and implement Hoplite, an efficient and fault-tolerant
collective communication layer for task-based distributed systems.
Hoplite combines two key ideas: (1) Hoplite computes data transfer
schedule on-the-fly as tasks and objects arrive, and Hoplite executes
data transfer schedule efficiently using fine-grained pipelining. Col-
lective communication can make significant progress even if only
a fraction of the participants are ready. (2) Hoplite dynamically
adapts the data transfer schedule when a failure is detected to alle-
viate the effects of the failed task in collective communication. This
allows the live tasks to make progress. The failed task can rejoin
the collective communication after being restarted and complete
the communication.
We apply Hoplite to a popular task-based framework, Ray [30].
This allows us to evaluate a wide range of existing workloads on Ray.
Our evaluations show that Hoplite can speed up an asynchronous
SGD by up to 7.8x, two popular RL algorithms (IMPALA [13], and
A3C [29]) on RLlib [27] by up to 1.9x, and 3.9x, respectively, and
improve the serving throughput time of an ensemble of ML models
on Ray Serve [42] by up to 3.3x, with only minimal code changes
and negligible additional latency in failure recovery.
This paper makes the following contributions:
‚Ä¢ A distributed scheduling scheme for data transfer that pro-
vides efficient broadcast and reduce primitives for dynamic-
task systems.
‚Ä¢ A fine-grained pipeline scheme that achieves low-latency
data transfers between tasks located both on the same node
or on different nodes.
(a) Dynamic tasks (Ray).
(b) Dynamic tasks + collective comm. (Ray + Hoplite).
Figure 1: Pseudocode for a typical RL algorithm to learn a policy. (a)
Dynamic tasks with Ray. Each train loop waits for a single agent to finish,
then asynchronously updates the current policy. The new policy is broadcast
to a batch of finished agents. (b) Modifications to (a) to enable Hoplite. Each
step reduces gradients from a subset of agents, updates the current policy,
broadcasts the new policy.
‚Ä¢ Algorithms to adapt the schedule of the data transfers for
broadcast and reduce operations which allows live tasks
to make progress when other tasks that participate in the
collective communication have failed, and later allow those
failed tasks to rejoin.
‚Ä¢ We demonstrate the benefits of Hoplite on top of a popular
task-based distributed system using several applications, in-
cluding asynchronous SGD, RL, and serving an ensemble of
ML models.
2 BACKGROUND
We first describe task-based distributed systems and their bene-
fits for developing distributed applications. We then describe the
challenges of integrating efficient collective communication into
them.
2.1 Task-Based Distributed Systems
The dynamic task programming model [4, 19, 30, 32, 44] allows
applications to express asynchronous and dynamic computation
deftrain(policy, num_agents, num_steps, batch_size):# Start some rollouts in parallel.grad_ids= [rollout.remote(policy)for_ inrange(num_agents)]for_ inrange(num_steps):for_ inrange(batch_size):# Wait for the first rollout to finish.ready_id= ray.wait(grad_ids)# Update the policy with one gradient.policy += ray.get(ready_id) / batch_size# Remove this gradient from remaining gradientsgrad_ids.remove(ready_id)# Once one batch of agents finish, broadcast updated# policy to finished agents and start new rollouts.for_ inrange(batch_size):grad_ids.append(rollout.remote(policy))returnpolicyfor_ inrange(num_steps):-for_ inrange(batch_size):-# Wait for the first rollout to finish.-ready_id= ray.wait(grad_ids)-# Update the policy with one gradient.-policy += ray.get(ready_id) / batch_size-# Remove this gradient from remaining gradients-grad_ids.remove(ready_id)+# Reduce a batch of gradients+  reduced_grad_id, unreduced_grad_ids= \+  ray.reduce(grad_ids, num_return=batch_size, op=ray.ADD)+# Update the policy with the averaged gradient+  policy += ray.get(reduced_grad_id) / batch_size+# Update remaining gradients+  grad_ids= ray.get(unreduced_grad_ids)# Once one batch of agents finish, broadcast updated# policy to finished agents and start new rollouts.for_ inrange(batch_size):grad_ids.append(rollout.remote(policy))Hoplite: Efficient and Fault-Tolerant Collective Communication for Task-Based Distributed Systems
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
and communication patterns. For instance, Figure 1a shows how to
implement an asynchronous RL algorithm that updates the policy
with agent results one at a time, choosing them dynamically based
on the order of availability. Once a batch of agent results have been
applied, the resulting policy is sent to each finished agent to begin
the next round of rollout. This allows an agent that has a fast rollout
not need to wait for a worker that has a slow rollout (Figure 2a). To-
day, most RL algorithms [13, 29] leverage this type of asynchronous
execution for efficient training.
To support this type of asynchronous communication, task-based
distributed systems rely on a distributed object store to transfer
objects between tasks. The object store consists of a set of nodes,
each of which buffers a (possibly overlapping) set of application
objects. Each node serves multiple workers, which can read and
write directly to objects in its local node via shared memory. A
sender task stores the output into the object store and exits, allowing
it to release critical resources (e.g., CPU, GPU, memory) before the
receiver tasks are even scheduled. When receiver tasks are ready,
they directly fetch the object from the distributed object store. As is
standard [30, 32], the object store enforces object immutability and
uses a distributed object directory service to map each object to its
set of node locations. In addition, task-based distributed systems
support fast failure recovery [49, 52] by reconstructing the failed
task. Well-behaving tasks do not roll back to keep recovery low
cost.
However, if the gradients and the model are large enough in the
above RL example, task-based distributed systems incur significant
overheads from inefficient communication. For example, the trainer
(agent 2) in Figure 2a can become a network throughput bottleneck
since it has to receive the gradient and also send the new policy
from/to each agent individually. This bottleneck becomes more
severe when the number of agents increases.
2.2 Challenges in Collective Communication