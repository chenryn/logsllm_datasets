linear scan of the code is suﬃcient to check the code’s safety.
Since our optimizer eliminates checks and moves checks away
from memory operations, a more complex veriﬁer is needed
to check the result of optimizations. This does increase the
size of the TCB, but seems unavoidable when verifying the
optimization results.
We have implemented a veriﬁer based on range analysis
which can check the results of our optimizations. The basic
idea is to perform range analysis over the optimized pro-
gram and determine the range of addresses used in memory
operations; the program is veriﬁed if every such address is
statically determined to be within [DB-GSize, DL+GSize].
The following example is the code in Fig. 4 after the opti-
mizer has removed the second check. Range analysis deter-
mines that the address range in the ﬁrst memory access is
[DB+4, DL+4] and the address range in the second memory
access is [DB+8, DL+8]. Assuming GSize ≥ 8, both are safe
according to the policy.
ecx ∈ [−∞, +∞]
ecx ∈ [DB, DL]
ecx ∈ [DB, DL]
ecx ∈ [DB, DL]
ecx ∈ [DB, DL]
ebx := [ecx + 8]
ecx := ecx & $DMask
eax := [ecx + 4]
... // assume ecx not changed in between
side the object bounds, but those pointers should stay inside
the data region. For instance, pointers that are one element
past the end of an array in the heap stay in the data region
if the stack is in the upper portion of the data region.
In a similar fashion, range analysis can determine the
safety of the program in Fig. 5(b), which is the result af-
ter loop optimizations.
Our veriﬁer is robust in the sense it can verify many more
optimizations,
including those we have not implemented.
New optimizations, including more aggressive loop optimiza-
tions, can be veriﬁed by the same veriﬁer. Another use of
the veriﬁer is for speculative optimizations. The optimizer
can eliminate a check or move a check to a diﬀerent place
even when it is not clear whether that transformation would
result in a safe program. After the transformation, the veri-
ﬁer can be used to check the safety of the resulting program;
if the veriﬁer fails, the optimizer can resort to the old pro-
gram. We have not tried any speculative optimizations.
6. MORE EFFICIENT CONTROL-FLOW
SANDBOXING
We describe two simple CFI optimizations that result in a
more eﬃcient implementation. This reduces the overhead of
protection schemes that build on CFI, including our data-
sandboxing scheme.
6.1 Original CFI instrumentation
As background information, we discuss CFI’s original mech-
anism for ensuring that a program’s execution follows a pre-
determined control-ﬂow graph [1, 2]. CFI uses a combina-
tion of static veriﬁcation and dynamic instrumentation for
enforcement. For a direct jump instruction, a static veriﬁer
can easily check that the target is allowed by the control-
ﬂow graph, without incurring any runtime overhead. For a
computed jump, CFI inserts runtime checks into the pro-
gram being protected to ensure that the control transfer is
consistent with the control-ﬂow graph.
Fig. 7 presents an example illustrating how CFI checks
It shows how a direct function call is in-
are performed.
strumented in CFI. Instruction “call fun” invokes a known
function with name fun. The direct call itself does not need
instrumentation—whether this is legitimate is checked stat-
ically. The return instruction in the body of fun, however,
needs instrumentation since it takes from the data region a
return address, which may be corrupted by the attacker.
The instrumentation is performed in two steps. First, an
ID is inserted after the call instruction (note the same ID
is inserted after all possible call sites to fun). Second, the
return instruction is changed to a sequence of instructions
that checks the correct ID is at the target before the control
transfer. The ID is embedded in a side-eﬀect free prefetch
instruction. The instruction takes a memory location as
its operand, and moves data from memory closer to a lo-
cation in the cache hierarchy. It is a hint to the processor
and does not aﬀect program semantics. Instrumentation of
other computed jumps is similar: IDs are inserted at the
allowed targets and runtime checks ensure correct IDs are
there before control transfers. Note that IDs are inserted
into the code region and cannot be changed by the attacker.
Furthermore, since the data region is not executable, the at-
tacker cannot manufacture new code in the data region with
the correct ID in it and jump to it for execution.
6.2 CFI optimizations
We next describe two simple CFI optimizations.
Jumping over prefetch instructions. The original CFI
35Original code Code after instrumentation
Comment
call fun
ret
call fun
prefetchnta [$ID]
ecx := [esp]
esp := esp + 4
if [ecx+3](cid:5)=$ID goto error
jmp ecx
a side-eﬀect free instruction with an ID embedded
retrieve the return address
adjust the stack pointer
check ID; ecx+3 is the address of the ID since the opcode
of the prefetch instruction takes three bytes
transfer the control
Figure 7: A CFI example.
Original code Code after instrumentation
call fun
ret
call fun
prefetchnta [$ID]
[ecx+3](cid:5)=$ID goto error
ecx := [esp]
esp := esp + 4
if
ecx := ecx + 7
jmp ecx
Figure 8: New CFI instrumentation by skipping
over prefetch instructions.
implementation inserts prefetch instructions at targets of
computed jumps. However, prefetch instructions incur sig-
niﬁcant overhead by fetching data from memory and in-
creasing cache pressure. Therefore, the ﬁrst optimization
jumps over prefetch instructions to avoid their execution.
Fig. 8 presents the new code sequence after the optimiza-
tion. The only diﬀerence is a new instruction (underlined)
that adds seven to the register that holds the target ad-
dress.5 Since the size of a prefetch instruction is seven, it is
skipped over. This optimization trades a prefetch instruc-
tion for a cheaper add instruction. Our evaluation shows
this alternative signiﬁcantly cuts the runtime overhead of
CFI (from 24.90% to 7.74%). Designers of the original CFI
mentioned this alternative, but it seems it has not been eval-
uated for performance comparison.
Jump table check optimization. Computed jumps are
often used for eﬃcient compilation of switch statements.
Most compilers generate a jump table for a switch state-
ment. The starting address of each branch of the switch
statement is stored as an entry in the jump table. Fig. 9
presents an example. The ﬁrst column presents the typical
sequence of instructions used by compilers to transfer the
control to a branch of a switch statement. It assumes edx
stores an index into the jump table and JT is a constant de-
noting the start address of the jump table. edx is scaled by
a factor of four when loading an entry from the jump table
because each entry is assumed to be a four-byte address.
The middle column lists the code sequence after the CFI
instrumentation. Because “jmp ecx” is a computed jump,
the instruction checks that there is a correct ID at the target.
Jump tables are usually stored in read-only sections of ob-
ject code. If we assume jump tables cannot be modiﬁed by
5The sequence takes care of only control-ﬂow sandboxing.
In our implementation for data conﬁdentiality, esp is sand-
boxed to stay in the data region and ecx is sandboxed to
stay in the code region.
attackers6, then control-ﬂow integrity is satisﬁed if (1) the
index into the jump table is within bounds and (2) all jump
targets in the jump table are legal according to the control-
ﬂow graph. The second condition can be checked statically
and the ﬁrst condition needs a bounds check. The last col-
umn in Fig. 9 presents the new sequence with the bounds
check, assuming the size of the jump table is 16. We use >u
for the unsigned comparison so that large numbers would
not be treated as negative. The bounds check involves only
register values and avoids retrieving IDs from memory. Fur-
thermore, it turns out that the LLVM compiler, in which
our prototype implementation is developed, already inserts
bounds checks before using a jump table. This further sim-
pliﬁes the work of our CFI rewriter. Note our system does
not depend on the assumption that LLVM emits bounds
checks since the CFI veriﬁer would complain if it did not.
This jump-table check optimization is essentially a spe-
cial case of the idea of encoding target tables for computed
jumps, as implemented in HyperSafe [32]. The eﬀectiveness
of this optimization depends on how often switch statements
are used in programs.
7.
IMPLEMENTATION AND EVALUATION
We next discuss our prototype implementation and eval-
uation of the implementation on benchmark programs.
7.1 Prototype implementation
Our implementation is built in LLVM 2.8 [20], a widely
used compiler infrastructure. We inserted a pass for CFI
rewriting, a pass for data sandboxing rewriting and opti-
mization, and a pass for CFI and data-sandboxing veriﬁ-
cation. All these passes are inserted right before the code-
emission pass. There are approximately 14,000 lines of C++
code in total added to LLVM (including comments and code
for dumping debugging information). Our rewriters essen-
tially perform assembly-level rewriting. We chose LLVM
as our implementation platform because LLVM preserves
helpful meta-information at assembly level (such as the con-
trol ﬂow graph). It also provides a clean representation of
compiled programs, which beneﬁts instrumentation and op-
timization.
In addition, it is easy to extend LLVM since
inserting an extra pass into its compilation process requires
nothing more than a registration of the pass.
Control ﬂow graph. Control-ﬂow graphs are constructed
with the help of LLVM. In particular, the LLVM compiler
preserves meta-information so that a precise intra-procedural
6This assumption can be discharged by either putting jump
tables into code region as in HyperSafe [32] or have read-only
data write-protected through page protection.
36Original code
CFI instrumentation
ecx := [$JT + edx*4]
jmp ecx
[ecx+3](cid:5)=$ID goto error
ecx := [$JT + edx*4]
if
ecx := ecx + 7
jmp ecx
After optimization
if edx>u15 goto error
ecx := [$JT + edx*4]
ecx := ecx + 7
jmp ecx
Figure 9: Jump table check optimization. Assume JT is a constant denoting the start address of the jump
table, edx holds the index into the jump table, and the jump table is of size 16.
control-ﬂow graph can be reconstructed at the assembly
level. The inter-procedural control-ﬂow graph (or the call
graph) is conservatively estimated by allowing a computed
call instruction to target any function. The precision of
the call graph could certainly be improved through further
static analysis such as inter-procedural control ﬂow analy-
sis and it would beneﬁt security. On the other hand, since
all our optimizations are intra-procedural, the precision of
the call graph is not critical to these optimizations. In fact,
we suspect inter-procedural static analysis would not result
in signiﬁcant performance improvement; the attack model
assumes the data region can arbitrarily change between in-
structions and thus inter-procedural analysis on the data
region such as shape analysis would be inapplicable.
Linker scripts, loader, and libraries. LLVM generates
object code with multiple sections (.bss, .data, .text, .ro-
data, and others). We developed linker scripts to link mul-
tiple sections into three sections (code, data and read-only
data) at certain start addresses. We modiﬁed PittSFIeld’s
loader to set up code and data regions and load executable
code. PittSFIeld’s loader does not protect conﬁdentiality
and allows sandboxed code to read outside of the sandbox;
we modiﬁed the loader so that it copies arguments into the
sandbox. Furthermore, the three sections are locked down
with the desired permissions with mprotect: the code sec-
tion is readable and executable; the data section is readable
and writable; the read-only data section is readable (it in-
cludes data such as jump tables and string literals). We also
reused PittSFIeld’s library wrappers and libraries, including
its reimplementation of library functions for dynamic mem-
ory allocation (malloc, free, and so on).
Veriﬁer. We have implemented a CFI and a data sand-
boxing veriﬁer. The CFI veriﬁer is similar to previous CFI
veriﬁers and checks whether IDs and checks are inserted at
appropriate places. We do not elaborate on its details. The
implementation of the data-sandboxing veriﬁer contains ap-
proximately 7,000 lines of C++ code. The majority of the
code is a large switch statement that calculates the ranges
of registers for machine instructions. There are over 3261
distinct machine opcodes inside LLVM including opcodes
and pseudo opcodes for IA-32, IA-64, x87 FPU, SSE, SSE2,
SSE3, and others. The veriﬁer could be shortened by group-
ing instructions into cases and omitting instructions that IA-
32 does not support. Given its large size, its own trustwor-
thiness should be independently validated (e.g., by testing
or by developing its correctness proof in a theorem prover);
we leave this to future work.
At the beginning of a function and any basic block that a
computed jump might target, the ranges of general-purpose
registers (eax, ebx, ecx, edx, esi and edi) are assumed to
be the universe ([−∞, +∞]) and the ranges of esp and ebp
to be the data region.7 For each instruction, the veriﬁer
updates the ranges of the registers that are deﬁned by the
instruction or used to compute a memory location. For ex-
ample, after “movl (%ebx), %eax”, the range of ebx is nar-
rowed down to the data region if the old range of ebx is
within the data region plus guard zones; furthermore, the
range of eax is set to be the universe because it is loaded
from the untrusted data region.
Since the lattice in range analysis is of inﬁnite height, its
termination is not guaranteed unless some widening strategy
is adopted. Our implementation uses a simple one: if a node
has been processed more than a constant number of times,
the ranges of registers that have not been stabilized are set to
be the universe. Other than this aspect, our implementation
of range analysis follows a standard worklist algorithm.
During the development, the veriﬁer helped us catch sev-
eral implementation errors in early versions of the optimizer;
these errors would be hard to ﬁnd by hand.
7.2 Performance evaluation
To evaluate our implementation, we conducted experi-
ments to test its runtime overhead on SPECint2000. Ex-
periments were conducted on a Linux CentOS 5.3 box with
Intel Xeon X5550 CPU at 2.66 GHz and 12GB of RAM.
All experiments were averaged over six runs. Three bench-
mark programs in SPECint2000 could not be compiled by
LLVM: eon is written in C++ and LLVM’s front end (clang)
does not support the version of the standard C++ library
in CentOS 5.3; perlbmk and parser could not be compiled
with the optimization level 3 and seem incompatible with
LLVM. All other benchmark programs were compiled with
the optimization level 3.
Table 1 presents the runtime percentage increases of CFI
compared to uninstrumented programs for SPECint2000.
The CFI row reports the results of our CFI implementa-
tion. On average, it adds 7.74% runtime overhead. The
CFI.jt.no-skip row shows the results of disabling the opti-
mization of jumping over prefetch instructions. Disabling
this optimization results in signiﬁcant performance degra-
dation:
the overhead shoots up to almost 25%. This is
due to the execution of costly prefetch instructions. The
CFI.no-jt.skip row reports the results when the optimiza-
tion for jump-table checks is disabled. The performance
improvement by this optimization is modest, suggesting op-
portunities for this optimization in SPECint2000 are limited.
CFI.no-jt.no-skip is the same as the original CFI implemen-
tation by Abadi et al [2]. They reported an average overhead
of 16% on SPECint2000 on an older system (Pentium 4 x86
processor at 1.8 GHz with 512 MB of memory and Windows
XP SP2). The experiments show that our CFI implemen-
7Before a function call and a computed jump, the veriﬁer
checks that esp and ebp are indeed in the data region.
37gzip
vpr
gcc mcf
crafty
gap
vortex
bzip2
twolf
average
CFI (%)
CFI.jt.no-skip (%)
CFI.no-jt.skip (%)