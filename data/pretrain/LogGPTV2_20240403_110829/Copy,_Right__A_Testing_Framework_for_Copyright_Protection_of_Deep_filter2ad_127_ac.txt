will be a copy of O with high probability. The threshold for
each metric λ is deﬁned as: τλ = αλ · LBλ, where αλ is a
user-speciﬁed relaxing parameter controlling the sensitivity of
the judgement. As αλ decreases, the false positive rate (the
possibility of misclassifying a negative suspect as a stolen
copy) will also increase. We empirically set α = 0.9 for black-
box metrics and α = 0.6 for white-box metrics respectively,
depending on the negative statistics.
DEEPJUDGE makes the ﬁnal judgement by voting below:
pcopy(O,S, T ) =
1|Λ|
(cid:2)
(cid:3)
1
λ∈Λ
λ(O,S, T ) ≤ τλ
(cid:4)
,
where 1 in the indicator function and Λ denotes the set of
DEEPJUDGE testing metrics, i.e., {RobD, NOD, NAD, LOD,
LAD, JSD}. Note that, depending on the defense setting
(white-box vs. black-box), only a subset of the testing metrics
can be applied and the averaging can only be applied on the
available metrics. DEEPJUDGE identiﬁes a positive suspect
copy if pcopy is larger than 0.5 and a negative one otherwise.
Arguably, voting is the most straightforward way of making
the ﬁnal judgement. While this simple voting strategy works
reasonably well in our experiments, we believe more advanced
judgement rules can be developed for diverse real-world
protection scenarios.
E. DEEPJUDGE vs. Watermarking & Fingerprinting
Here, we brieﬂy discuss why our testing approach is more
favorable in certain settings and how it complements exist-
ing defense techniques. Table II summarizes the differences
of DEEPJUDGE to existing watermarking and ﬁngerprinting
Fig. 3: DEEPJUDGE tests each neuron and generates a test
case to explore the corner region of its output distribution.
2) White-box setting: In this case, the internals of the sus-
pect model are accessible, thus a more ﬁne-grained approach
for test case generation becomes feasible. As shown in Fig. 3,
given a seed input and a speciﬁed layer, DEEPJUDGE generates
one test case for each neuron, and the corner case of the
neuron’s activation is of our particular interest.
The test generation algorithm is described in Alg. 2. It
takes the owner’s victim model O and a set of selected seeds
Seeds as input, and returns the set of generated test cases
T . T is initialized to be empty (Line 1). The main content
of the algorithm is a nested loop (Lines 2-13), in which for
each neuron nl,i required by the metrics in Section IV-B, the
algorithm searches for an input that activates the neuron’s
output φl,i(x(cid:2)
) more than a threshold value. At each outer
loop iteration, an input is sampled from the Seeds (Lines 3-
4). It is then iteratively perturbed in the inner loop following
the neuron activation’s gradient update (Lines 6-7), until an
input x(cid:2) that can satisfy the threshold condition is found and is
added into the test suite T (Lines 8-11) or when the maximum
number of iterations is reached. The parameter lr (Line 7)
is used to control that the search space of the input with
perturbation is close enough to its seed input. Finally, the
generated test suite T is returned.
We discuss how to conﬁgure the speciﬁc threshold k for a
neuron nl,i used in Alg. 2. Since the statistics may vary across
different neurons, we pre-compute the threshold k based on
the training data and the owner model, which is the maximum
value (upper bound) of the corresponding neuron output over
all training samples. The ﬁnal threshold value is then adjusted
by a hyper-parameter m to be used in Alg. 2 for more
reasonable and adaptive thresholds. Note that the thresholds
for all interested neurons can be calculated once by populating
layer by layer across the model.
D. Final Judgement
The judgment mechanism of DEEPJUDGE has two steps:
thresholding and voting. The thresholding step determines a
proper threshold for each testing metric based on the statistics
of a set of negative suspect models (see Section V-A3 for more
details). The voting step examines a suspect model against
each testing metric, and gives it a positive vote if its distance
to the victim model is lower than the threshold of that metric.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
829
TABLE II: A comparison of different copyright protection methods.
Method
Type
Non-invasive
Evaluated Settings
Evaluated Attacks
Black-box White-box
Finetuning
Pruning
Extraction
Uchida et al. [40]
Merrer et al. [23]
Adi et al. [1]
Zhang et al. [47]
Darvish et al. [9]
Jia et al. [20]
Cao et al. [2]
Lukas et al. [27]
DeepJudge (Ours)
Watermarking
Watermarking
Watermarking
Watermarking
Watermarking
Watermarking
Fingerprinting
Fingerprinting
Testing
%
%
%
%
%
%
!
!
!
%
!
!
!
!
!
!
!
!
!
%
%
%
!
%
%
%
!
!
!
!
!
!
!
!
!
!
!
!
%
!
!
!
!
!
!
%
%
%
%
%
!
%
!
!
V. EXPERIMENTS
methods, from three aspects: 1) whether the method is non-
invasive (i.e., independent of model training); 2) whether it
is particularly designed for or evaluated in different defense
settings (i.e., white-box vs. black-box); and 3) whether the
method is evaluated against different attacks (i.e., ﬁnetuning,
pruning and extraction). DEEPJUDGE is training-independent,
able to be ﬂexibly applied in either white-box or black-box
settings, and evaluated (also proven to be robust) against all
three types of common copyright attacks including model
ﬁnetuning, pruning and extraction, with empirical evaluations
and comparisons deferred to Section V-B3.
Watermarking is invasive (training-dependent), whereas ﬁn-
gerprinting and testing are non-invasive (training independent).
The effectiveness of watermarking depends on how well the
owner model memorizes the watermark and how robust the
memorization is to different attacks. While watermarking can
be robust to ﬁnetuning or pruning attacks [40], [47], it is
particularly vulnerable to the emerging model extraction attack
(see Section V-C2). This is because model extraction attacks
only extract
the key functionality of the model, however,
watermarks are often task-irrelevant. Despite the above weak-
nesses, watermarking is the only technique that can embed the
owner identity/signature into the model, which is beyond the
functionalities of ﬁngerprinting or testing.
Fingerprinting shares certain similarities with testing. How-
ever, they differ in their goals. Fingerprinting aims for “unique-
ness”, i.e., a unique ﬁngerprint of the model, while testing aims
for “completeness”, i.e., to test as many dimensions as possible
to characterize not only the unique but also the common prop-
erties of the model. Arguably, effective ﬁngerprints are also
valid black-box testing metrics. But as a testing framework,
our DEEPJUDGE is not restricted to a particular metric or
test case generation method. Our experiments in Section VI
show that a single metric or ﬁngerprint
is not sufﬁcient
to handle the diverse and adaptive model stealing attacks.
In Section VI-B, we will also show that our DEEPJUDGE
can survive those adaptive attacks that break ﬁngerprinting
by dynamically changing the test case generation strategy.
We anticipate a long-running arms race in deep learning
copyright protection between model owners and adversaries,
where watermarking, ﬁngerprinting and testing methods are
all important for a comprehensive defense.
We have implemented DEEPJUDGE as a self-contained
toolkit in Python2. In the following, we ﬁrst evaluate the
performance of DEEPJUDGE against model ﬁnetuning and
model pruning (Section V-B), which are two threat scenarios
extensively studied by watermarking methods [1], [9]. We then
examine DEEPJUDGE against more challenging model extrac-
tion attacks in Section V-C. Finally, we test the robustness of
DEEPJUDGE under adaptive attacks in Section VI. Overall, we
evaluated DEEPJUDGE with 11 attack methods, 3 baselines,
and over 300 deep learning models trained on 4 datasets.
A. Experimental Setup
1) Datasets & Victim Models: We run the experiments on
three image classiﬁcation datasets (i.e., MNIST [24], CIFAR-
10 [22] and ImageNet [36]) and one audio recognition dataset
(i.e., SpeechCommands [42]). The models used for the four
datasets are summarized in Table III, including three convo-
lutional architectures and one recurrent neural network. For
each dataset, we divide the training data into two subsets. The
ﬁrst subset (50% of the training examples) is used to train
the victim model. More detailed experimental settings can be
found in Appendix A.
2) Positive suspect models: Positive suspect models are
derived from the victim models via ﬁnetuning, pruning, or
model extraction. These models are considered as stolen copies
of the owner’s victim model. DEEPJUDGE should provide
evidence for the victim to claim ownership.
3) Negative suspect models: Negative suspect models have
the same architecture as the victim models but are trained
independently using either the remaining 50% of training data
or the same data but with different random initializations.
The negative suspect models serve as the control group to
show that DEEPJUDGE will not claim wrong ownership. These
models are also used to compute the testing thresholds (τ).
The same training pipeline and the setting are used to train
the negative suspect models. Speciﬁcally, “Neg-1” are trained
with different random initializations while “Neg-2” are trained
using a separate dataset (the other 50% of training samples).
2The tool and all the data in the experiment are publicly available via
https://github.com/Testing4AI/DeepJudge
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
830
TABLE III: Datasets and victim models.
Dataset
MNIST
CIFAR-10
ImageNet
Type
Image
Image
Image
Audio
SpeechCommands
#Params: number of parameters
Model
LeNet-5
ResNet-20
VGG-16
LSTM(128)
#Params
107.8 K
274.4 K
33.65 M
132.4 K
Accuracy
98.5%
84.8%
74.4%
94.9%
4) Seed selection: Seed selection prepares the Seeds ex-
amples used to generate the test cases. Here, we apply the
sampling strategy used in DeepGini [12] to select a set of high-
conﬁdence seeds from the test dataset (details are in Appendix
B). The intuition is that high-conﬁdence seeds are well-learned
by the victim model, thus carrying more unique features of
the victim model. More adaptive seed selection strategies are
explored in the adaptive attack section VI-B1.
5) Adversarial example generation: We use three classic
attacks including FGSM [14], PGD [28] and CW [4] to
generate adversarial test cases as introduced in Section IV-C1.
B. Defending Against Model Finetuning & Pruning
As model ﬁnetuning and pruning threats are similar in
processing the victim model (see Section III), we discuss them
together here. These two are also the most extensively studied
threats in prior watermarking works [1], [40].
1) Attack strategies: Given a victim model and a small set
of data in the same task domain, we consider the following
four commonly used model ﬁnetuning & pruning strategies:
a) Finetune the last layer (FT-LL). Update the parameters of
the last layer while freezing all other layers. b) Finetune all
layers (FT-AL). Update the parameters of the entire model.
c) Retrain all layers (RT-AL). Re-initialize the parameters
of the last layer then update the parameters of the entire
model. d) Parameter pruning (P-r%). Prune r percentage
of the parameters that have the smallest absolute values, then
ﬁnetune the pruned model to restore the accuracy. We test both
low (r=20%) and high (r=60%) pruning rates. Typical data-
augmentations are also used to strengthen the attacks. More
details of these attacks are in Appendix C.
2) Effectiveness of DEEPJUDGE: The results are presented
separately for black-box vs. white-box settings.
Black-box Testing. In this setting, only the output probabil-
ities of the suspect model are accessible. Here, DEEPJUDGE
uses the two black-box metrics: RobD and JSD. For both
metrics, the smaller the value, the more similar the suspect
model is to the victim model. Table IV reports the results
of DEEPJUDGE on the four datasets. Note that we randomly
repeat the experiment 6 times for each ﬁnetuning or pruning
attack and 12 times for independent training (as more negative
suspect models will result in a more accurate judging thresh-
old). Then, we report the average and standard deviation (in
the form of a ± b) in each entry of Table IV. Clearly, all
positive suspect models are more similar to the victim model
with signiﬁcantly smaller RobD and JSD values than negative
suspect models. Speciﬁcally, a low RobD value indicates that
the adversarial examples generated on the victim model have
a high transferability to the suspect model, i.e., its decision
boundary is closer to the victim model. In contrast, the RobD
values of the negative suspect models are much larger than
that of the positives, which matches our intuition in Fig. 2.
To further conﬁrm the effectiveness of the proposed metrics,
we show the ROC curve for a total of 54 models (30 positive
suspect models and 24 negative suspect models) for RobD and
JSD in Figure 4. The AUC values are 1 for both metrics. Note
that we omit the plots for the following white-box testing as
the AUC values for all metrics are also 1.
White-box Testing. In this setting, all
intermediate-layer
outputs of the suspect model are accessible. DEEPJUDGE can
thus use the four white-box metrics (i.e., NOD, NAD, LOD,
and LAD) to test the models. Table V reports the results on the
four datasets. Similar to the two black-box metrics, the smaller
the white-box metrics, the more likely the suspect model is a
stolen copy. As shown in Table V, there is a fundamental
difference between the two sets (positive vs. negative) of sus-
pect models according to each of the four metrics. That is, the
two sets of models are completely separable, leading to highly
accurate detection of the positive copies. It is not surprising
as white-box testing can collect more ﬁne-grained information
from the suspect models. In both the black-box and white-box
settings, the voting in DEEPJUDGE overwhelmingly supports
the correct ﬁnal judgement (the ‘Copy?’ column).
Combined Visualization. To better understand the power of
DEEPJUDGE, we combine the black-box and white-box testing
results for each suspect model into a single radar chart in
Fig. 5. Each dimension of the radar chart corresponds to
a similarity score given by one testing metric. For better
visual effect, we normalize the values of the testing metrics
into the range [0, 1], and the larger the normalized value,
the more similar the suspect model to the victim. Thus, the
ﬁlled area could be viewed as the accumulated supporting
evidence by DEEPJUDGE metrics for determining whether
the suspect model is a stolen copy. Clearly, DEEPJUDGE is
able to accurately distinguish positive suspects from negative