### 优化后的文本

#### (b) 如果 \( c > 0 \)

为了满足 \(\delta(l1) = c \cdot \delta(l2)\)，我们需要设置 \(\delta(l1)\) 和 \(\delta(l2)\) 的关系。具体来说，如果 \( p(l2) > \beta \)，则 \(\delta(l1) = 0\)；如果 \( p(l2) < \beta \)，则 \(\delta(l1) = c \cdot \delta(l2)\)。此外，还需要满足 \(\text{sign}(\delta(l1)) = \text{sign}(\delta(l2))\)。

- 如果 \( p(l2) > \beta \)，则 \(\delta(l1) = 0\)。
- 如果 \( p(l2) < \beta \)，则 \(\delta(l1) = c \cdot \delta(l2)\)。

在调整 \(\delta(l1)\) 和 \(\delta(l2)\) 时，应确保 \(\delta(l1)\) 和 \(\delta(l2)\) 满足依赖条件。如果两个概率都可以改变，我们首先设置较大的变化值 \(\delta(l)\)，并确保其绝对值尽可能小。

#### 算法 2：NewtonFoolGeneralized

**输入:**
- \( x0 \): 初始输入
- \( \eta \): 学习率
- \( maxIter \): 最大迭代次数
- \( \alpha, \beta \): 阈值
- \( L+, L- \): 正负标签集合
- \( F \): 带有 softmax 层的神经网络 \( Fs \)

**伪代码:**

```python
function NewtonFoolGeneralized(x0, η, maxIter, α, β, L+, L−, F):
    m ← |L+|
    n ← |L−|
    d ← 0
    for i = 0, 1, 2, ..., maxIter − 1 do:
        计算 g(l)
        构造 Gi 如 (8) 定义
        使用 §A.3 中的技巧去除 Gi 的依赖项，并记录对 δ(l) 的约束
        找到 Gi 的最小奇异值 σmin
        for l ∈ L+ do:
            if p(l) > α then:
                δ(l) ← 0
            else:
                δ(l) ← min{p(l) − α, ησmin∥x0∥ / √(m+n)}
        for l ∈ L− do:
            if p(l) < β then:
                δ(l) ← 0
            else:
                δ(l) ← max{p(l) − β, −ησmin∥x0∥ / √(m+n)}
        检查步骤 8 中添加的约束是否都满足。如果不满足，则调整 δ(l) 以使 |δ(l)| 总是减小。
        d* ← G · δ
        xi+1 ← xi + d*
        d ← d + d*
    return d
```

### 附录 B：额外实验

#### B.1 HOG 结果

表 6 和表 7 分别显示了 MNIST 和 GTSRB 数据集上原始图像和扰动图像之间的 HOG 特征向量距离。每列代表由顶部图像表示的类别的结果，每行给出均值和标准差。具体来说，对于每一行，通过指定算法生成对抗样本，然后将其 HOG 特征向量与对应原始图像进行比较。

- **MNIST 数据集**：FGSM 给出了最佳结果，而 DeepFool 和 NewtonFool 在大多数情况下给出相似的结果，除了第二个标签（数字 1）。JSMA 产生的统计值较大，除了第五个标签（数字 4）。
- **GTSRB 数据集**：情况完全改变，现在 JSMA 产生了明显更好的结果。在其他三种算法中，NewtonFool 在所有实验中给出了最好的结果，甚至在最后三个标签上达到了与 JSMA 相当或更好的结果。类似地，在边缘检测度量中，FGSM 在 GTSRB 数据集上的表现较差，而 JSMA 生成的扰动相对较好。

#### B.2 效率

以下是详细的统计结果。表 8 和表 9 分别给出了 MNIST 和 GTSRB 数据集的运行时间结果。每列对应于顶部图像表示的类别，每行给出运行时间的均值和标准差。

- **MNIST 数据集**：NewtonFool 仅比 FGSM 稍慢（按秒计算）。另一方面，NewtonFool 比 DeepFool（最多快 3 倍）和 JSMA（最多快 6 倍）显著更快。
- **GTSRB 数据集**：由于该数据集有更多的类别，NewtonFool 的效率更为明显。在这种情况下，NewtonFool 比 DeepFool 快最多 49 倍，比 JSMA 快最多 800 倍。

### 表格

#### 表 6: MNIST: HOG 结果

| 算法 | 0 | 1 | 2 | 3 | 4 |
|------|---|---|---|---|---|
| FGSM | 0.47 (0.17) | 0.50 (0.16) | 0.44 (0.20) | 0.38 (0.17) | 0.40 (0.17) |
| JSMA | 0.57 (0.17) | 0.85 (0.18) | 0.82 (0.29) | 0.52 (0.17) | 0.44 (0.14) |
| DeepFool | 0.51 (0.19) | 0.86 (0.38) | 0.47 (0.21) | 0.42 (0.18) | 0.48 (0.20) |
| NewtonFool | 0.49 (0.19) | 0.83 (0.37) | 0.46 (0.21) | 0.40 (0.17) | 0.47 (0.20) |

#### 表 7: GTSRB: HOG 结果

| 算法 | 0 | 1 | 2 | 3 | 4 |
|------|---|---|---|---|---|
| FGSM | 1.13 (0.57) | 0.88 (0.21) | 0.86 (0.20) | 0.64 (0.29) | 0.72 (0.28) |
| JSMA | 0.42 (0.28) | 0.57 (0.25) | 0.69 (0.22) | 0.48 (0.29) | 0.99 (0.44) |
| DeepFool | 0.96 (0.53) | 0.85 (0.22) | 0.96 (0.31) | 0.56 (0.24) | 0.96 (0.41) |
| NewtonFool | 0.80 (0.48) | 0.71 (0.18) | 0.75 (0.21) | 0.48 (0.19) | 0.62 (0.23) |

#### 表 8: MNIST 运行时间 (单位: 秒)

| 算法 | 0 | 1 | 2 | 3 | 4 |
|------|---|---|---|---|---|
| FGSM | 0.20 (0.02) | 0.19 (0.02) | 0.19 (0.03) | 0.19 (0.03) | 0.19 (0.02) |
| JSMA | 8.72 (3.14) | 4.21 (0.75) | 8.29 (3.04) | 7.18 (2.73) | 6.47 (2.00) |
| DeepFool | 5.90 (6.42) | 3.98 (1.01) | 5.46 (5.04) | 4.30 (4.31) | 3.46 (1.73) |
| NewtonFool | 2.51 (1.02) | 2.86 (0.96) | 2.30 (1.29) | 1.81 (1.06) | 1.75 (0.96) |

#### 表 9: GTSRB 运行时间 (单位: 秒)

| 算法 | 0 | 1 | 2 | 3 | 4 |
|------|---|---|---|---|---|
| FGSM | 0.19 (0.03) | 0.19 (0.02) | 0.19 (0.02) | 0.18 (0.02) | 0.17 (0.02) |
| JSMA | 755.67 (879.74) | 321.35 (289.29) | 316.57 (257.51) | 372.64 (279.62) | 108.89 (59.59) |
| Deepfool | 118.55 (1097.21) | 24.93 (13.07) | 34.99 (23.22) | 28.19 (24.68) | 13.57 (4.43) |
| NewtonFool | 0.85 (0.66) | 0.64 (0.29) | 0.80 (0.36) | 0.65 (0.41) | 0.56 (0.30) |