i
i
i
i
i
i
i
i
(b) If c  0,
i
i
). To satisfy δ (l1)
i
= c · δ (l2)
i
i
i
= δ (l2)
= c · δ (l2)
) (cid:44) sign(δ (l2)
). To satisfy δ (l1)
• sign(δ (l1)
with c > 0, set δ (l1)
(b) If c  β), then δ (l1)
, we set δ (l1)
= δ (l2)
= 0
= 0
i
i
i
i
i
i
i
i
i
i
, we set δ (l1)
 β (resp. p (l2)
= c · δ (l2)
• If p (l2)
= 0 by our
i
rules at §A.2.2. To satisfy the dependency condition
δ (l1)
i
the δ (l )
i
satisfying the constraint, then use δ (l1)
to determine the other value.
• If both probabilities can be changed, we first set
| possible while
= c · δ (l2)
,
= δ (l2)
i
with larger change |δ (l )
i
= 0
i
i
i
i
The following pseudocode incorporates the rules in §A.2.2
and §A.3.
Algorithm 2
Input: x0,η, maxIter, α, β, L+, L−, a neural network F with
NewtonFoolGeneral-
a softmax layer Fs.
ized(x0,η,maxIter ,α , β,L+,L−, F)
1: function
m ← |L+|
n ← |L−|
d ← 0
for i = 0,1,2, . . . ,maxIter − 1 do
Compute g(l )
i
Construct Gi as defined in (8)
Remove dependencies of Gi using the tricks in
s (xi ) for all l ∈ L+ ∪ L−
= ∇F l
§A.3, record added constraints on δ (l )
i
’s
Find the smallest singular value σmin of Gi
for l ∈ L+ do
if p (l )
i > α then
i ← min{p (l )
δ (l )
i ← 0
δ (l )
for l ∈ L− do
else
if p (l )
i < β then
i ← max{p (l )
δ (l )
i ← 0
δ (l )
else
i − α ,
ησmin∥ x0 ∥
m+n
√
}
i − β,− ησmin∥ x0 ∥
√
m+n
}
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
Check if the constraints added at step 8 are all
, in the way that
satisfied. If not, adjust one of two δ (l )
i
|δ (l )
i
| always decreases.
†
d∗
i ← G
i · δi
xi +1 ← xi + d∗
d ← d + d∗
i
i
return d
B ADDITIONAL EXPERIMENT
B.1 Results for HOG
Table 6 and Table 7 shows the distances between the HOG
feature vectors of the original image and the perturbed image,
on MNIST and GTSRB, respectively. Again, each column gives
the results on the class represented by the image at the top,
and each row presents the mean and standard deviation. To
be specific, for each row, adversarial examples were generated
by the designated algorithm, then their HOG feature vectors
were compared to those of the corresponding orignal images.
On MNIST dataset, FGSM gives the best results, while Deep-
Fool and NewtonFool give similar values in general with an
exception of the second label (digit 1). JSMA produces bigger
statistics except for the fifth label (digit 4), which implies that
the perturbed image is less likely to be detected when the
original image is detected correctly.
On GTSRB dataset, the situation changes entirely that now
JSMA produces noticeably better results. Among the other
three algorithms, NewtonFool gives the best results for all
experiments, even achieving the smaller or equal results to
that of JSMA for the last three labels. Similarly to the edge de-
tection metric, FGSM becomes much worse for GTSRB dataset,
whereas JSMA generates relatively better perturbations than
its output on MNIST dataset.
B.2 Efficiency
We now give detailed statistics. Table 8 and Table 9 gives the
running time results for MNIST and GTSRB, respectively. Each
column corresponds to each class represented by the image at
top, and each row presents the mean and standard deviation
of the running time. For MNIST, (Table 8), NewtonFool is only
moderately slower than FGSM (if one calculates the absolute
difference in seconds). On the other hand, NewtonFool is sub-
stantially faster than DeepFool (up to 3X) and JSMA (up to
6X). The efficiency becomes more obvious on GTSRB dataset
because it has significantly more classes. In this case, we ob-
serve that NewtonFool is up to 49X faster than DeepFool, and
is up to 800X faster than JSMA.
FGSM
JSMA
DeepFool
NewtonFool
0.47 (0.17)
0.57 (0.17)
0.51 (0.19)
0.49 (0.19)
0.50 (0.16)
0.85 (0.18)
0.86 (0.38)
0.83 (0.37)
0.44 (0.20)
0.82 (0.29)
0.47 (0.21)
0.46 (0.21)
0.38 (0.17)
0.52 (0.17)
0.42 (0.18)
0.40 (0.17)
0.40 (0.17)
0.44 (0.14)
0.48 (0.20)
0.47 (0.20)
FGSM
JSMA
DeepFool
NewtonFool
0.39 (0.16)
0.48 (0.16)
0.39 (0.16)
0.38 (0.16)
0.40 (0.15)
0.51 (0.17)
0.43 (0.16)
0.42 (0.17)
0.46 (0.18)
0.65 (0.21)
0.50 (0.19)
0.47 (0.18)
0.37 (0.15)
0.55 (0.19)
0.40 (0.15)
0.38 (0.16)
0.34 (0.14)
0.46 (0.16)
0.39 (0.16)
0.37 (0.16)
Table 6: MNIST: Results with histogram of oriented gradients. FGSM, DeepFool and NewtonFool give close results,
except for the second label (for digit 1) where JSMA, DeepFool and NewtonFool give close results. JSMA generally
produces slightly larger statistics except for the fifth label (for digit 4).
FGSM
JSMA
DeepFool
NewtonFool
1.13 (0.57)
0.42 (0.28)
0.96 (0.53)
0.80 (0.48)
0.88 (0.21)
0.57 (0.25)
0.85 (0.22)
0.71 (0.18)
0.86 (0.20)
0.69 (0.22)
0.96 (0.31)
0.75 (0.21)
0.64 (0.29)
0.48 (0.29)
0.56 (0.24)
0.48 (0.19)
0.72 (0.28)
0.99 (0.44)
0.96 (0.41)
0.62 (0.23)
0.94 (0.23)
0.87 (0.24)
0.89 (0.21)
0.81 (0.19)
Table 7: GTSRB: Results with histogram of oriented gradients. Now JSMA gives better results than others. FGSM,
DeepFool and NewtonFool produce slightly worse results, while NewtonFool performs better than the other methods
but JSMA.
FGSM
JSMA
DeepFool
NewtonFool
0.20 (0.02)
8.72 (3.14)
5.90 (6.42)
2.51 (1.02)
0.19 (0.02)
4.21 (0.75)
3.98 (1.01)
2.86 (0.96)
0.19 (0.03)
8.29 (3.04)
5.46 (5.04)
2.30 (1.29)
0.19 (0.03)
7.18 (2.73)
4.30 (4.31)
1.81 (1.06)
0.19 (0.02)
6.47 (2.00)
3.46 (1.73)
1.75 (0.96)
FGSM
JSMA
DeepFool
NewtonFool
0.18 (0.02)
6.76 (2.64)
5.48 (15.02)
1.75 (0.96)
0.19 (0.02)
7.39 (2.59)
7.64 (19.04)
2.31 (0.93)
0.19 (0.02)
6.44 (2.18)
4.89 (4.54)
2.43 (1.26)
0.18 (0.02)
8.07 (3.20)
3.76 (3.53)
1.59 (0.92)
0.18 (0.02)
6.46 (2.20)
4.35 (8.55)
1.50 (0.73)
Table 8: MNIST statistics for the running time (unit: sec.)
FGSM
Papernot et al.
Deepfool
NewtonFool
0.19 (0.03)
755.67 (879.74)
118.55 (1097.21)
0.85 (0.66)
0.19 (0.02)
321.35 (289.29)
24.93 (13.07)
0.64 (0.29)
0.19 (0.03)
316.57 (257.51)
34.99 (23.22)
0.80 (0.36)
0.18 (0.03)
372.64 (279.62)
28.19 (24.68)
0.65 (0.41)
0.17 (0.02)
108.89 (59.59)
13.57 (4.43)
0.56 (0.30)
0.20 (0.02)
689.81 (511.29)
41.78 (28.01)
0.85 (0.42)
Table 9: GTSRB statistics for the running time (unit: sec.)