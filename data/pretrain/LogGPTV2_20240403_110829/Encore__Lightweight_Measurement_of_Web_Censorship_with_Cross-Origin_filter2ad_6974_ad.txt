for the home page of a professor in February 2014 [17].
The site saw 1,171 visits during course of the month. Most
visitors were from the United States, but we saw more than
10 users from 10 other countries, and 16% of visitors reside
in countries with well-known Web ﬁltering policies (India,
China, Pakistan, the UK, and South Korea), indicating that
dispatching measurement tasks to sites such as academic Web
pages may yield measurements from a variety of represen-
tative locations. Of these visitors, 999 attempted to run a
measurement task; we conﬁrmed nearly all of the rest to be
automated trafﬁc from our campus’ security scanner. We also
found that 45% of visitors remained on the page for longer
than 10 seconds, which is more than sufﬁcient time to execute
at least one measurement task and report its results. The 35%
of visitors who remained for longer than a minute could easily
run multiple measurement tasks.
Our small pilot deployment of Encore is representative of
the sites where we can expect Encore to be deployed in the
short term. Although adoption of Encore by even a single
high-trafﬁc Web site would entirely eclipse measurements
collected by these small university deployments, grassroots
recruitment remains necessary: Encore relies on a variety of
origin sites to deter an adversary from simply blocking access
to all origins to suppress our measurement collection.
6.3 Will webmasters install Encore?
Encore cannot directly target speciﬁc demographics for mea-
surement collection—the measurements that we collect arise
from the set of users who happen to visit a Web site that has
installed an Encore script. If the sites that host Encore are
globally popular (e.g., Google), then Encore can achieve an
extremely widespread sampling of users; on the other hand,
if the sites are only popular in particular regions, the resulting
measurements will be limited to those regions.
Recruiting webmasters to include Encore’s measurement
scripts should be feasible. First, installing Encore on a Web
site incurs little cost. Serving these scripts to clients adds min-
imal network overhead; our prototype adds only 100 bytes
to each origin page and requires no additional requests or
connections between the client and the origin server. Mea-
surements themselves have little effect on the Web page’s
perceived performance because they run asynchronously after
the page has loaded and rendered. However, they do incur
some network overhead to clients when loading cross-origin
resources, which may be undesirable to users with bandwidth
caps or slow, shared network connections.
Second, we see two strong incentives for webmasters to
participate in Encore. Many webmasters may support Encore
simply out of greater interest in measuring Web ﬁltering and
encouraging transparency of government censorship. The
grassroots success of similar online freedom projects (e.g.,
Tor [12]) in recruiting volunteers to host relays and bridges
suggests that such a population does exist. For further in-
centive, we could institute a reciprocity agreement for web-
masters: in exchange for installing our measurement scripts,
webmasters could add their own site to Encore’s list of targets
and receive notiﬁcation about their site’s availability from
Encore’s client population.
7 Measurements
We conﬁrm the soundness of Encore’s measurement tasks
with both controlled experiments and by comparing our abil-
ity to conﬁrm cases of Web ﬁltering with independent reports
of ﬁltering from other research studies. We have implemented
and released every component of Encore described in Sec-
tion 5 and have collected seven months of measurements from
May 2014 through January 2015.
To date, at least 17 volunteers have deployed Encore on
their sites, although the true number is probably much higher;
3/4 of measurements come from sites that elect to strip the
Referer: header when sending results. We recorded
141,626 measurements from 88,260 distinct IPs in 170 coun-
tries, with China, India, the United Kingdom, and Brazil re-
porting at least 1,000 measurements, and more than 100 mea-
surements from Egypt, South Korea, Iran, Pakistan, Turkey,
and Saudi Arabia. These countries practice some form of
Web ﬁltering. We use a standard IP geolocation database to
determine client locations [31]. Clients ran a variety of Web
browsers and operating systems.
7.1 Are measurement tasks sound?
To conﬁrm the soundness of Encore’s measurements, we
built a Web censorship testbed, which has DNS, ﬁrewall, and
Web server conﬁgurations that emulate seven varieties of
DNS, IP, and HTTP ﬁltering. For three months, we instructed
approximately 30% of clients to measure resources hosted
by the testbed (or unﬁltered control resources) using the four
task types from Table 1. For example, we veriﬁed that the
images task type detects DNS blocking by attempting to load
an image from an invalid domain and observing that the task
reports ﬁltering; we veriﬁed that the same task successfully
loads an unﬁltered image.
Veriﬁcation is straightforward for the image, style sheet,
and script task types because they give explicit binary feed-
back about whether a resource successfully loaded. Encore
collected 8,573 measurements for these task types; after ex-
cluding erroneously contributed measurements (e.g., from
Web crawlers), there were no true positives and few false
positives. For example, clients in India, a country with noto-
riously unreliable network connectivity, contributed to a 5%
false positive rate for images.
Verifying soundness of the inline frame task type requires
more care because it infers existence of ﬁltering from the time
taken to load resources. Figure 7 compares the time taken
to load an uncached versus cached single pixel image from
1,099 globally distributed Encore clients. Cached images
normally load within a few tens of milliseconds, whereas
most clients take at least 50 ms longer to load the same image
662and region, we count both the total number of measurements
nr and the number of successful measurements xr and run
a one-sided hypothesis test for a binomial distribution; we
consider a resource as ﬁltered in region r if xr fails this test at
0.05 signiﬁcance (i.e., Pr[Binomial(nr, p) ≤ xr] ≤ 0.05) yet
does not fail the same test in other regions.
Applying this technique on preliminary measurements
conﬁrms well-known censorship of youtube.com in Pak-
istan, Iran, and China [18], and of twitter.com and
facebook.com in China and Iran. Although our detec-
tion algorithm works well on preliminary data, possible en-
hancements include dynamically tuning model parameters
to account for differing false positive rates in each country
and accounting for potential confounding factors like user
behavior differences between browsers and ISPs [3].
8 Ethics and Security
This section discusses barriers to Encore’s widespread de-
ployment, from the ethics of collecting measurements from
unsuspecting Web users to the potential for attackers to block,
disrupt, or tamper with client measurements or collection
infrastructure.
Which resources are safe to measure? Encore induces
clients to request URLs that might be incriminating in some
countries and circumstances. In particular, the most interest-
ing URLs to measure may be those most likely to get users
into trouble for measuring them. Curating a list of target
URLs requires striking a balance between ubiquitous yet un-
interesting URLs (e.g., online advertisers, Google Analytics,
Facebook) and obscure URLs that governments are likely to
censor (e.g., human rights groups). Although our work does
not prescribe a speciﬁc use case, we recognize that deploy-
ing a tool like Encore engenders risks that we need to better
understand.
Balancing the beneﬁt and risk of measuring ﬁltering with
Encore is difﬁcult. This paper has made the beneﬁt clear:
Encore enables researchers to collect new data about ﬁlter-
ing from a diversity of vantage points that was previously
prohibitively expensive to obtain and coordinate. Ongoing
efforts to measure Web ﬁltering would beneﬁt from Encore’s
diversity and systematic rigor [8,15,35]. The risk that Encore
poses are far more nebulous: laws against accessing ﬁltered
content vary from country to country, and may be effectively
unenforceable given the ease with which sites (like Encore)
can request cross-origin resources without consent; there is
no ground truth about the legal and safety risks posed by
collecting network measurements.
Striking this balance between beneﬁt and risk raises ethical
questions that researchers in computer science rarely face
and that conventional ethical standards do not address. As
such, our understanding of the ethical implications of collect-
ing measurements using Encore has evolved, and the set of
measurements we collect and report on has likewise changed
to reﬂect our understanding. Table 2 highlights a few key
milestones in Encore’s deployment, which has culminated
Figure 7: Comparison between load times for cached and uncached
images from 1,099 Encore clients. Cached images typically load
within tens of milliseconds, whereas uncached usually take at least
50 ms longer to load, indicated by the bold red line. We use this
difference to infer ﬁltering.
uncached. The few clients with little difference between
cached and uncached load time were located on the same
local network as the server. Difference in load time will be
more pronounced for larger images and with greater latency
between clients and content.
In both cases, false positives highlight (1) that distinguish-
ing Web ﬁltering from other kinds of network problems is
difﬁcult and (2) the importance of collecting many measure-
ments before drawing strong conclusions about Web ﬁltering.
We now develop a ﬁltering detection algorithm that addresses
both concerns.
7.2 Does Encore detect Web ﬁltering?
We instructed the remaining 70% of clients to measure re-
sources suspected of ﬁltering, with the goal of independently
verifying Web ﬁltering reported in prior work. Because mea-
suring Web ﬁltering may place some users at risk, we only
measured Facebook, YouTube, and Twitter. These sites pose
little additional risk to users because browsers already rou-
tinely contact them via cross-origin requests without user
consent (e.g., the Facebook “thumbs up” button, embedded
YouTube videos and Twitter feeds). Expanding our measure-
ments to less popular sites would require extra care, as we
discuss in the next section.
We aimed to detect resources that are consistently inacces-
sible from one region, yet still accessible from others. For this
purpose, we measure ﬁltering of entire domains, using the im-
age task type. This is challenging because measurement tasks
may fail for reasons other than ﬁltering: clients may experi-
ence intermittent network connectivity problems, browsers
may incorrectly execute measurement tasks, sites may them-
selves go ofﬂine, and so on. We use a statistical hypothesis
test to distinguish such sporadic or localized measurement
failures from more consistent failures that might indicate Web
ﬁltering. We model each measurement success as a Bernoulli
random variable with parameter p = 0.7; we assume that,
in the absence of ﬁltering, clients should successfully load
resources at least 70% of the time. Although this assump-
tion is conservative, it captures our desire to eliminate false
positives, which can easily drown out true positives when
detecting rare events like Web ﬁltering. For each resource
UncachedCachedDifference020040060080010001200Time(ms)663Date
February 2014 and prior
Event
Informal discussions with Georgia Tech IRB conclude that Encore (and similar work) is
not human subjects research and does not merit formal IRB review.
March 13, 2014 – March 24, 2014 Encore begins collecting measurements from real users using a list of over 300 URLs.
March 18, 2014
April 2, 2014
May 5, 2014
May 7, 2014
September 17, 2014
September 25, 2014
January 30, 2015
February 6, 2015
We’re unsure of the exact date when collection began because of data loss.
We begin discussing Encore’s ethics with a researcher at the Oxford Internet Institute.
To combat data sparsity, we conﬁgure Encore to only measure favicons [43]. The URLs
we removed were a subset of those we crawled from §5.2.
Out of ethical concern, we restrict Encore to measure favicons on only a few sites.
Submission to IMC 2014, which includes results derived from our March 13 URL list.
Georgia Tech IRB ofﬁcially declines to review Encore. We requested this review in
response to skeptical feedback from IMC.
Submission to NSDI 2015, using our URL list on April 2.
Submission to SIGCOMM 2015, using our URL list on May 5.
Princeton IRB reafﬁrms that Encore is not human subjects research. We sought this re-
view at the request of the SIGCOMM PC chairs after Nick Feamster moved to Princeton.
Table 2: Timeline of Encore measurement collection, ethics discussions, and paper submissions. As our understanding of Encore’s ethical
implications evolved, we increasingly restricted the set of measurements we collect and report. See http://encore.noise.gatech.
edu/urls.html for information on how the set of URLs that Encore measures has evolved over time.
in the set of measurements we report on in this paper. The
Institutional Review Boards (IRBs) at both Georgia Tech and
Princeton declined to formally review Encore because it does
not collect or analyze Personally Identiﬁable Information
(PII) and is not human subjects research [9]. Yet, Encore is
clearly capable of exposing its users to some level of risk.
Because we do not understand the risks that a tool like Encore
presents, we have focused most of our research efforts on
developing the measurement technology, not on reporting re-
sults from the measurements that we gather. Other censorship
measurement tools have and will continue to face similar ethi-
cal questions, and we believe that our role as researchers is to
lead a responsible dialogue in the context of these emerging
tools.
As part of this ongoing dialogue, we hope that the commu-
nity will develop ethical norms that are grounded in theory,
applicable in practice, and informed by experts. To this end,
we have discussed Encore with ethics experts at the Oxford
Internet Institute, the Berkman Center, and Citizen Lab, and
our follow on work examines broader ethical concerns of cen-
sorship measurement [27]. We have also been working with
the organizers of the SIGCOMM NS Ethics workshop [34],
which we helped solicit, to ensure that its attendees will gain
experience applying principled ethical frameworks to net-
working and systems research, a process we hope will result
in more informed and grounded discussions of ethics in our
community.
Schechter [41] surveyed people about the ethics of vari-
ous research studies, including Encore, and found that most
people felt that unconstrained use of Encore would be highly
unethical. However, as the report acknowledges, the survey
didn’t elaborate on the inherent risks of browsing any Web
page, the potential beneﬁts of research like Encore, the risks
of alternative means of measuring censorship, or low-risk
deployment modes.
Encore underscores the need for stricter cross-origin se-
curity policy [45]. Our work exploits existing weaknesses,
and if these policies could endanger users then strengthening
those policies is clearly a problem worthy of further research.
Why not informed consent? The question of whether to ob-
tain informed consent is more complicated than it might ﬁrst
appear. Informed consent is not always appropriate, given
that in disciplines where experimental protocols for human
subjects research are well-established, there are classes of ex-
periments that can still be conducted ethically without it, such
as when obtaining consent is either prohibitive or impractical
and there is little appreciable risk of harm to the subject.
Researchers and engineers who have performed large-scale
network measurements can appreciate that obtaining consent
of any kind is typically impractical. For Encore, it would re-
quire apprising a user about nuanced technical concepts, such
as cross-origin requests and how Web trackers work—and do-
ing so across language barriers, no less. Such burdens would
dramatically reduce the scale and scope of measurements,
relegating us to the already extremely dangerous status quo of
activists and researchers who put themselves into harm’s way
to study censorship. Even if we could somehow obtain con-
sent at scale, informed consent does not ever decrease risk to
users; it only alleviates researchers from some responsibility
for that risk, and may even increase risk to users by removing
any traces of plausible deniability.
We believe researchers should instead focus on reducing
risk to uninformed users, as we have done with repeated
iteration after consultation with ethics experts. It is gener-