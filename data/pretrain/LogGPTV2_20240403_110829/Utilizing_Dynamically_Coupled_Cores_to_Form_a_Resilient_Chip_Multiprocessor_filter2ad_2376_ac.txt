to the requester, if needed.
(cid:129) On cache-to-cache transfers,
the reader’s cache line is
marked unveriﬁed if the original copy of the cache line was
marked unveriﬁed. This is so that, in the event of a rollback,
all live copies of the speculatively updated value are prop-
erly discarded. To support this, on a miss, the supplier of
the value must put on the bus its cache line’s unveriﬁed bit
as part of its snoop response. Notice that unveriﬁed, clean
cache lines can be silently dropped by the reader at any time.
Master Thread
Slave Thread
Read A
Read B
Write B
TIME
Read A
Read B
Write B
Figure 7. Three example windows.
transfers–masters do.
To support multiple master-slave nodes, a few more changes
beyond those to support a single master-slave pair (Section 3.2)
are needed as follows:
(cid:129) Slaves do not supply data to remote nodes via cache-to-cache
(cid:129) Slave reads cause remote data in modiﬁed or exclusive states
to downgrade to owned or shared states, respectively. Fur-
thermore, slave read-exclusive requests are treated as ordi-
nary reads for the purposes of state transitions at remote
nodes (speciﬁcally, they do not result in invalidations). Fi-
nally, upgrade requests by slaves are ignored by remote
caches. To ensure that remote nodes do not apply invali-
dations to their caches in response to slave threads, a master
line is added to the system bus. Cache lines in remote nodes
apply invalidations to their caches in response to bus trans-
actions generated by masters only. To facilitate this, master
threads drive the master line when they start their bus trans-
actions, and remote threads snoop this line to determine if
the bus transaction is generated by a master.
(cid:129) Read-exclusive requests by a master invalidate remote copies
as usual. However, if the data is dirty unveriﬁed on a remote
master, the slave must obtain a copy of the cache line before
it is invalidated by the master. To do this, the slave checks if
it already has a copy of the cache line. If so, the cache line is
marked as dirty unveriﬁed to prevent its eviction. Otherwise,
the slave snarfs the cache-to-cache transfer and writes the
data to its local cache hierarchy as dirty unveriﬁed. If this
results in a cache buffering overﬂow, all cores rollback to the
last checkpoint, and a new checkpoint is scheduled with half
the duration of the last checkpointing interval (Section 3.3).
(cid:129) Likewise, upgrade requests by a master invalidate remote
copies as usual. However, if the data is dirty unveriﬁed on
a remote master, and if the local slave does not have a copy
of the cache line, the master’s upgrade request is turned into a
read-exclusive request, so that the slave can snarf the remote
copy as before. If the slave already has a copy, the cache line
is marked as dirty unveriﬁed in the slave cache.
4.3 Master-Slave Consistency
The main difﬁculty in supporting DCC in a parallel execution
is ensuring that master and slave threads view a consistent im-
age of the shared address space at all times. In other words, ev-
ery committed load instruction on the slave should read the same
value as the corresponding committed load on the master.
In a
sequential application, this is easily accomplished by preventing
unveriﬁed dirty lines from being written back to main memory.
Consequently, slave threads can always obtain the same value as
their masters from the memory system (Section 3.2).
In a parallel execution, however, this is not necessarily the case.
Since all threads in the system have access to the shared address
space, intervening writes from other threads can cause master and
slave threads to read different values. For instance, the master
thread could read a line L, but before the slave gets a chance to
perform the corresponding read, a third core could invalidate the
master and update L. In this case, the original value that the master
read may no longer be available to the slave.
In order to accommodate thread interactions like these, we in-
troduce the notion of a master-slave memory access window, or
simply window. Roughly speaking, windows represent periods of
vulnerability during which the consistency of master-slave pairs in
the system may be compromised, and allow us to deﬁne a small set
of restrictions that guarantee correctness in such cases. Windows
are deﬁned on a per-address basis, and are labeled as read or write
windows depending on whether the operation being performed is
a load or a store.
A read window for address A opens when either the master or
the slave thread issues a load that reads the value of A, and closes
when both master and slave threads commit the load. Windows
opened by misspeculated loads (e.g., in the shadow of a mispre-
dicted branch) are closed at recovery. Similarly, a write window
for address A opens when either the master or the slave thread
performs (necessarily at commit) a store that writes A, and closes
when both master and slave threads commit the store.
Figure 7 shows three example windows on a node. When the
master issues its ﬁrst read, a read window for memory location A
opens. The master then opens a read window for B, and commits
the load that reads A. When the slave commits its read of A, the
open read window for A closes.
To guarantee master-slave consistency, it is sufﬁcient to en-
sure that the system observes certain restrictions at all times for
each memory location M in the program’s address space. Specif-
ically, a node should not be allowed to open a write window for
M if there is already an open read or write window for M on an-
other node. This remote intervention constraint prevents master
and slave threads from reading different values due to intervening
writes by other nodes. Open windows for different locations place
no restrictions on each other. Similarly, any number of simultane-
ous read windows can be open at a time for a given address, and
private data can be read and written without any restrictions. Fur-
thermore, a node can open a read window while a write window is
open at some other node.
4.3.1 Hardware Implementation
The main addition that is required to support master-slave consis-
tency is an age table that resides with each core’s cache controller.
Each load’s instruction age is deﬁned as the the total number of
load and store instructions committed by its thread (since the last
checkpoint) at the time that load commits. Similarly, each store’s
instruction age is the total number of loads and stores committed
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Load Queue
Address
Offset
Store Queue
Ld/St Age
Figure 8. Example of an age table implementation.
by its thread at the time the store commits. We use this infor-
mation to detect memory operations that could lead to violations
of the remote intervention constraint, and to delay them until this
danger disappears.
Enforcing the remote intervention constraint requires detecting
open read and write windows, and the age table facilitates tracking
these by storing the age of the last committed memory instruction
on the corresponding core to an address range. The age table is a
direct-mapped, untagged SRAM array indexed by the address of
committed loads and stores (Figure 8). Age table entries contain
the age of the last committed memory instruction to an address
range. The table index is formed by using the lower-order adress
bits following the cache block offset. The table is updated locally
at commit time.
Performing Writes to Modiﬁed Data When a master
thread wants to perform a write, it checks the state of the line in
its cache in parallel with an age table access. If the cache line is
modiﬁed, no other node is currently caching that line, and there is
no danger of invalidating data that the slave will want to read later
(recall that unveriﬁed dirty lines cannot be replaced until the next
checkpoint). In addition, there is no danger of intervention in a
read window on another node (if a window had opened following
the write that put the line in modiﬁed state, the line’s state would
have transitioned to owned). In this case, the master performs the
write immediately.
Performing Writes to Data in Other States
If the cache
line is not in the modiﬁed state, additional checks need to be per-
formed along with the standard read exclusive or upgrade request.
The information needed to perform these checks is piggybacked
on the bus transaction. When the read-exclusive (or upgrade) re-
quest is observed by other nodes, each core accesses its age ta-
ble to get the age of the last committed memory instruction to the
corresponding address range. In parallel, each node searches its
load queue3 for any matching reads that have already issued to the
memory system or have forwarded from a store. If there is a hit
in the load queue, a negative acknowledgment (NACK) is raised
(accomplished by pulling the NACK line of the bus high), and
the write is retried later. Searching the load queue guarantees that
any read windows opened by speculative reads are not violated
by writes from other nodes. In the case of branch mispredictions,
speculative loads are naturally removed from the load queue, and
misspeculated loads eventually cease to generate NACKs.
If all of load queue searches result in misses, each node reports
its age table entry (accessed in parallel with the load queue) on the
data bus in the following cycle. (Each slave core drives a portion
of the data bus with its age information.) In the following cycle,
every master core compares its own age with the age of its slave.
A mismatch on a remote node indicates a potentially open read or
write window, and a NACK is raised for the write.
3A port already exists for external invalidations to search the load queue
in many commercial systems. If this capability is not present, the search
can compete with local stores for access to the load queue.
Livelock Avoidance There exists a danger of livelock when
issuing NACKs for writes. For instance, consider the case of a
spin lock where the master holding the lock needs to perform an
invalidation for lock release. If masters and slaves on other nodes
repeatedly read the lock variable in a tight loop, new read win-
dows may always open before the last one closes, preventing the
lock release from performing, and thus perpetuating the cycle. We
have empirically observed that NACKs, and thus such livelocks,
are exceedingly rare in the applicationswe have studied; neverthe-
less, we need to provide a machanism to detect these situations
and be able to guarantee progress.
We propose a simple policy: A single NACK bit is added to
every age table entry; when a node issues a NACK for a write
transaction, both master and slave cores set the NACK bit for their
corresponding age table entries. At that time, both the master and
the slave temporarily stop fetching instructions, and allow their
pipelines to drain. When both pipes are drained, the master and
the slave exchange their committed instruction counts to identify
the leader and the trailer in the execution. The leader remains
stalled, while the trailer is allowed to resume execution. This ef-
fectively allows the trailer thread to close read windows left open
by the leader–in particular, the read windows opened by the spin-
lock. The remote write (which the remote node keeps retrying)
may eventually succeed once the open read window is closed.
When this happens, the NACK bit is cleared, and both the mas-
ter and the slave resume normal execution. It is also possible that
the trailer commits enough instructions to match the leader. If at
that time the NACK bit has not yet been cleared, the trailer ﬂushes
its pipeline and stalls as well. At this point, neither the master nor
the slave have any loads in their load queues, and their age table
entries are consistent. This guarantees that the very next retry of
the remote write will succeed, at which point both master and slave
can resume execution.
Deadlock Avoidance There exists also a danger of deadlock
when issuing NACKs for writes. This may occur when writes in
two or more processors cannot perform because of open read win-
dows by out-of-order loads elsewhere, forming a cycle. For in-
stance, consider the case where processors p1 and p2 are trying to
perform writes to addresses A and B, respectively. If p1 and p2
have issued loads to addresses B and A out of order with respect
to those writes, respectively, the processors will issue NACKs
for each other’s writes, preveting forward progress. Luckily, this
deadlock situation would be eventually broken through the timeout
mechanism (Section 3.3). Nevertheless, a simple and more effec-
tive solution is possible: Upon receiving a NACK for such a write,
a processor systematically ﬂushes its pipeline beyond the write,
and prevents loads from issuing until the write successfully com-
mits. In any case, as stated before, we have empirically observed
that such NACKs are rare in the applications we have studied.
Hard Fault Recovery Once a signature discrepancy or a
timeout occurs (Section 3.3), all processors currently running the
parallel application roll back to their last checkpoint. To recover
from permanent faults, a second slave is introduced to the node
that initially caused the fault. This third processor engages in age
exchanges just like the original slave does. When performing an
age check, the master thread compares its age against both slaves,
and considers a read window to be open if any of the two slaves has
a mismatch with it. Aside from this, the operation of our proposed
system is the same in all other respects.
4.4 Compatibility Across Memory Consis-
tency Models
The memory consistency model of a multiprocessor places a
set of ordering restrictions between memory operations issued
from a given thread. DCC does not impose or rely on any ordering
constraints, and is general enough to operate correctly under any
consistency model.
In other words, regardless of how memory
operations are ordered, DCC’s master-slave consistency is never
compromised.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Processor
Frequency
Fetch/issue/commit width
Inst. window [(Int+Mem)/FP]
Reorder buffer entries
Int/FP registers
Functional Units
Ld/St queue entries
Branch penalty (cycles)
Store forward delay (cycles)
Branch predictor
Branch target buffer size
RAS entries
3.2 GHz
4/4/6
64/48
192
96/96
4 ALU, 3 FPU, 2 BR, 2/2 Ld/St
24/24
10(min.)
3
16K-entry
2048
24
Memory Subsystem
L1 Cache (Private)
Victim Cache (L1)
L2 Cache (Shared)
MSHR entries
System bus
Max. outstanding bus requests
Memory bus bandwidth
Memory latency
Coherence Protocol
Consistency Model
32KB, 4-way, LRU, 64B, 3 cycles
8 entries
8MB, 8-way, LRU, 64B, 43 cycles
16 L1, 16 L2
256 bits, 800 MHz
96
12.8 GB/s
400 cycles
MOESI
Release Consistency
Fault Tolerance Extension Parameters
Processor comm. latency
Age table size
State compression latency
State checkpoint latency
30 cycles
64 entries
35 cycles
8 cycles
Table 3. Summary of modeled architecture.
To see this, note that loads obtain their values either from the
CMP memory subsystem, or from the store queue of the processor
on which they issue. The remote intervention constraint prevents
violations of master-slave consistency through the memory sys-
tem as discussed above. However, relaxed consistency models and
aggressive implementations of sequential consistency also allow
loads to be reordered with respect to other memory instructions
by issuing early, or by forwarding from the store queue. Luckily,
DCC readily accommodates such optimizations.
Speciﬁcally, if the master forwards from its store queue, a read
window opens and prevents intervening writes to violate the win-
dow; eventually, the slave also consumes the same value from a
replica of the same store (either through its own store queue or
from its local cache).
In other cases, there is a danger that the
master reads a value produced by a remote node, but the slave for-
wards from its store queue and breaks master-slave consistency.
Luckily, the remote intervention constraint prevents this: when the
master commits its copy of the store that the slave forwards from,
a write window opens and blocks intervening writes from remote
nodes, forcing both the master and the slave to consume the result
produced by their local store. Windows opened by slaves are also
safe for the same reason.
5 Evaluation
Flexible DMR frameworks like DCC hold signiﬁcant poten-
tial when confronted with the challenges of deep submicron pro-
cess technologies. In this section, we evaluate DCC using detailed
execution-driven simulations of a CMP model.
In our experiments, we allow a single application to run redun-
dantly on multiple processors using the hardware modiﬁcations
described in this paper. The conﬁguration details of this processor
are listed in Table 3. When taking global checkpoints on parallel
applications, we model the bus arbiter’s synchronization request,
master-slave synchronization, and the checkpoint latency on each
node. We ﬁnd that master and slave cores are never separated by
more than 200 cycles in their execution (roughly 100 cycles on
average), leading to negligible waiting times for the receipt of ac-
knowledgments. Hence, for simplicity in our simulations, we do
not model the global handshake.
Splash-2
BARNES