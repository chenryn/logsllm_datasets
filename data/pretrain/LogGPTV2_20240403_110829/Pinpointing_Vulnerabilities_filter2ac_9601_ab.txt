in Ravel: it bridges the performance gap between the attack detec-
tor and the (sluggish) vulnerability locator, making Ravel usable
even in production systems, and it makes the attack reproducible for
many times. How to faithfully reproduce in-the-wild bugs/attacks
is a big challenge in the software development and maintenance.
Ravel imposes two requirements on its R&R system. First, since
the recorder runs on the production system, it should incur min-
imal performance and storage overhead. Additionally, it must be
compatible with the attack detector without changing the program’s
normal execution. Second, the execution history is replayed by the
vulnerability locator, which instruments the replayed execution with
heavy-weight analyses. These analyses will make additional sys-
tem calls (e.g., to allocate memory for the intermediate results) and
consequently aﬀect the memory layout of the replayed process. The
replayer must isolate these side-eﬀects to keep the replay faithful.
To fulﬁll these requirements, Ravel employs a process R&R sys-
tem, which records and replays a single process or a group of related
processes. The process R&R can be implemented either at the li-
brary level or the kernel level. Ravel chooses the latter because it
is more secure in an adversarial environment — the library-level
recording can be bypassed if the program/payload makes direct sys-
tem calls, and the recorded history cannot be trusted if the process is
compromised because the recorder exists in the (compromised) pro-
cess’ address space. Moreover, our R&R system assumes that the
target program properly synchronizes its access to shared resources.
Deterministically recording and replaying arbitrary programs that
have race conditions could be very complicated and ineﬃcient with-
out hardware support [41]. If the program does have race condi-
tions (i.e., bugs that should be ﬁxed by developers), the replayed
execution will deviate from the recorded history and can thus be
detected. Lastly, Ravel records the complete execution history of
the target process. To reduce the storage overhead, Ravel borrows
various techniques from the Eidetic System [23], a practical always-
on whole-system R&R system. For example, Ravel uses LZMA to
compress the execution log and avoids logging the data that can be
recovered from the environment, such as reading a static ﬁle.
2.3.1 Record
To faithfully replay a process, Ravel needs to record all the non-
deterministic inputs to that process. These inputs include both the
data and events from the external environment (e.g., network packets
and signals) and the internal events (e.g., locks). Next, we describe
in detail how these two types of inputs are handled by Ravel.
The syscall interface is an ideal location to intercept and record
external inputs such as syscall returns, the user-space memory mod-
iﬁed by a syscall, and signals. Syscall returns need to be recorded
because they may aﬀect the program execution. For example, a
program may use its process id returned by getpid as one of the
ingredients to generate random numbers. Syscall returns can also
aﬀect the control ﬂow (e.g., error handling). It is rather straight-
forward to record syscall returns – we just need to log them in
the execution history. A syscall may modify the user-space mem-
ory. For example, the stat syscall writes the ﬁle status into the
user-provided memory. Most such syscalls explicitly deﬁne the
structures of the exchanged data. If so, we simply save the modiﬁed
memory after the syscall returns. In this way, we understand and
retain the semantics of the modiﬁed data. However, syscalls like
ioctl may write diﬀerent amounts of data to the user space when
given diﬀerent parameters. Even worse, ioctl can be dynami-
cally extended by a loaded kernel module. It is hard to record all
the user-space memory written by these system calls. To address
Figure 3: Overall architecture of Ravel
of that process for exploits and attacks. In order to capture real-
world attacks, the recorder and the attack detector run on production
systems. If an attack is detected, the execution log can be sent to
the developer (likely on a diﬀerent computer) for further analysis
using the vulnerability locator. The vulnerability locator replays the
recorded execution and performs a number of analyses to pinpoint
the vulnerabilities. Speciﬁcally, it ﬁrst uses a data-ﬂow analysis to
roughly locate the vulnerabilities and then performs vulnerability-
speciﬁc analysis to reﬁne the results. In this architecture, we use
a combination of the general data-ﬂow analysis and vulnerability-
speciﬁc analyses to fulﬁll Ravel’s precision requirements (the 1st
and 2nd challenges), and leverage R&R to address the performance
and reproducibility requirements (the 3rd and 4th challenges). In
the rest of this section, we give details of these components.
2.2 Detecting Attacks
To locate vulnerabilities, Ravel ﬁrst needs to detect and record the
attacks. The attack detector thus plays an important role in Ravel.
In order to handle real-world (zero-day) vulnerabilities, the attack
detector has to run on the production system. This imposes strict
performance and eﬀectiveness requirements on the attack detector.
However, Ravel is structured as an extensible framework. It can em-
ploy many attack detection techniques, such as syscall interposition
and CFI. This is made possible by the design of Ravel. Speciﬁcally,
the vulnerability locator deduces the rough location of an exploited
vulnerability by searching for anomalies in the data ﬂow and further
reﬁnes that with detailed analyses. Therefore, it is suﬃcient for the
vulnerability locator to know that the execution log contains certain
(unknown) attacks. This minimal requirement allows Ravel to em-
ploy any attack detection technique as long as it is eﬀective and has
low performance overhead. We want to emphasize that the attack
detector itself may provide very little help in locating vulnerabili-
ties. For example, syscall interposition detects attacks only when
the payload is executing, but the actual exploitation is hidden in the
haystack of other executed instructions.
In our prototype, we employ two simple attack detection tech-
niques – program crashes and syscall interposition [30]. They both
have low performance overhead. With the wide-spread deployment
of exploit mitigation techniques like W ∧ X and ASLR, they are
also more eﬀective than before. For example, W ∧ X prevents
the injected malicious code from being executed. To address that,
the attacker often uses return-oriented programming (ROP [54]) to
change the process’ memory permission with an unplanned syscall.
This can be readily detected by syscall interposition. Moreover,
ASLR often makes an exploit less stable, leading to more frequent
crashes under attack. Both techniques can be easily integrated into
Ravel.
In particular, our implementation of syscall interposition
validates both syscall sequences and parameters. Syscall interposi-
tion has been well researched [26, 30, 34, 39, 40, 43, 45, 46, 50], so
we omit the details here. The derivative vulnerabilities in Figure 2
(line 7, 8, and 9) can be detected by checking syscall parameters and
potentially by crashes if ASLR is supported. We plan to support
more advanced detection techniques, such as CFI, in the future.
3
TargetProcessRecord AgentReplay AgentAttack DetectorProcessCheckpointVulnerability DetectorExecutionHistoryUserKernelPhase I: Normal ExecutionPhase II: Vulnerability Detectionthat, Ravel hooks the FreeBSD kernel’s copyout function to record
(and replay) the data copied to the user space during those syscalls.
Similar to Linux’s copy_to_user function, FreeBSD exclusively
uses copyout to write data to the user-space. Signals can also in-
troduce non-determinism to the recorded process. A signal can be
either synchronous and asynchronous. A synchronous signal (e.g.,
SIGSEGV) is the result of exceptional program behaviors. There is
no need to record this kind of signals because replaying the program
will trigger the same exceptions. An asynchronous signal (e.g., an
alarm) instead must be faithfully recorded and replayed. Since it is
asynchronous, we can delay its delivery until a syscall return. This
greatly simpliﬁes the replay of asynchronous signals.
Some instructions can bypass the kernel and directly interact
with the hardware. A typical example on the x86 architecture is
the RDTSC instruction, which returns the CPU’s current time-stamp
counter. Some programs use the outputs of RDTSC for random
number generation. Therefore, Ravel has to record the outputs
of this instruction. However, RDTSC is by default an unprivileged
instruction and can be executed by any user programs. To address
that, we change the CPU’s conﬁguration (the TSD ﬂag in the CR4
register) to intercept the execution of RDTSC by user processes and
record its outputs for the target process. The interception of RDTSC
is turned oﬀ when the kernel switches to other processes. As such,
there is no overhead for other processes.
The internal non-determinism comes mostly from accessing the
shared memory. To avoid race conditions, the program should
synchronize these accesses, say, by using locks. Without race
conditions, it is suﬃcient for Ravel to record the order of pro-
cesses (or threads) entering critical sections. Replaying the ex-
ecution in the same order ensures that the shared memory is in
the correct state for each critical section. To this end, we instru-
ment the synchronization primitives in common libraries (e.g., the
pthread library) to record and replay them in orders. Exam-
ples of these primitives include pthread_mutex_lock, pthread_
rwlock_wrlock, pthread_cond_broadcast, pthread_cond_
signal, sem_wait, atomic_store, atomic_exchange, etc. On
the other hand, if the program does have race conditions (e.g., two
threads modify the same data without synchronization), the replay
will deviate from the recorded execution history. Ravel tries to
detect race conditions when that happens.
2.3.2 Replay with Instrumentation
After Ravel detects an attack, it starts to replay the recorded exe-
cution to locate vulnerabilities. For most syscalls, such as getpid
and stat, it is not necessary to re-execute them. Ravel just returns
the recorded return values and updates the user memory if nec-
essary. Similarly, network connections (sockets) are not recreated
during the replay. Ravel directly returns the recorded data to the
replayed process. Other syscalls, typically memory related one have
to be re-executed. For example, most programs use mmap to allocate
memory. The kernel may return a diﬀerent block of memory during
the replay. To address that, we pass the recorded memory address in
the ﬁrst parameter of mmap, which is a suggestion of the allocation
address to the kernel. During our experiments, the kernel always
accepts the suggestion and allocates the same memory.
During the replay, Ravel instruments the program in order to
detect anomalies in the process’ memory access patterns. We use
dynamic binary translation (BT) for this purpose. As such, the same
program binary can be used for both recording and replaying. The
BT engine can interfere with the replay. For example, the engine
needs to allocate memory for its own use (e.g., to cache the translated
code). This may conﬂict with the memory layout of the recorded
execution. The engine also makes extra syscalls, for example, to
4
write the log to the disk. Ravel tries to limit the interference to
ensure the replayed execution is faithful to the recorded one. For
example, it loads the BT engine in an unused memory area, and
asks the kernel to allocate the code cache in another unused area.
Since the engine is separated from the code cache 2, Ravel can
tell whether a syscall is made by the engine or the program itself,
and makes the replay decision accordingly. Note that BT engines
often make direct syscalls rather than calling libc functions to avoid
disrupting the translated process because many libc functions are
non-reentrantable or thread-unsafe.
Ravel records the complete execution history of the target process.
Even though replaying is often much faster than recording [41], it
may still take a long time to replay a long-running process, such
as a web server. To address that, we can take periodic snapshots
of the process and start replaying from the most recent snapshots
if the recorded history is too long. We should continue searching
backwards for vulnerabilities until a vulnerability is located. This
may introduce false negatives if there are multiple exploited vul-
nerabilities because of the missing/partial def-use relations. Our
prototype does not support this optimization so far.
2.4 Pinpointing Vulnerabilities
Vulnerability locator aims at pinpointing the targeted vulnerabil-
ities from a recorded execution that is known to contain attacks. It
is based on the key observation that memory exploits often change
the data ﬂow. As such, it ﬁrst uses a data-ﬂow analysis to locate the
rough locations of the vulnerability and further reﬁnes them with
speciﬁc analyses targeting common types of memory vulnerabili-
ties. Ravel is designed as an extensible framework so that analyses
for less common types of vulnerabilities can be added later.
2.4.1 Data-Flow Analysis
Ravel calculates the probable locations of an exploited vulnera-
bility using the data-ﬂow analysis. We deﬁne a program’s data ﬂow
as the def-use relations between instructions [11]. Speciﬁcally, an
instruction deﬁnes a memory address if it writes to that address, and
an instruction uses a memory address if it reads from that address.
If two instructions deﬁne and use the same address respectively,
they form a def-use relation. To detect data-ﬂow anomalies, Ravel
computes a data-ﬂow graph (DFG) for the program beforehand us-
ing dynamic analysis. During the replay, it instruments the process
to capture a detailed execution log, including all the run-time mem-
It then extracts from this log the data-ﬂow of the
ory accesses.
program under attack.
If an actual def-use relation is not in the
pre-computed DFG, we consider both instructions of this def-use
relation as a candidate for the vulnerability. However, it is unclear
which instruction is the one.
Ravel uses heuristics to help determine whether the “def” or the
“use” more likely marks the vulnerability. First, if multiple def-use
relations are introduced by one of these instructions, that instruction
more likely is the vulnerability. For example, in a buﬀer overﬂow,
the def instruction may overwrite a large block of memory used later
by several instructions. Second, if the data is used by a syscall that
sends data outside (e.g., send, sendmsg, write), the vulnerability
is likely related to the use (i.e., an information leak). Third, if the
accessed memory contains control data, the vulnerability likely lies
in the def instruction. We can identify control data if it is read
by instruction fetching (e.g., a return instruction fetches its return
address from the stack), or if it is subsequently used by an indirect
branch instruction. If none of the above heuristics applies, Ravel
reports both instructions as a viable candidate.
2In dynamic BT, the translated code executes from the code cache,
instead of the original code section.
Ravel’s data-ﬂow analysis is performed at the instruction level.
For a logged syscall, Ravel understands its semantics and can iden-
tify the memory regions it reads from and writes to. From this
perspective, a syscall can be treated as a pseudo instruction with
an extended deﬁne and use sets. Moreover, if an identiﬁed instruc-
tion lies inside a library, we trace back from that instruction until
we reach its call site. A call to a library function is normally en-
coded as the call to a PLT (procedure linkage table 3) entry. This
step is necessary otherwise many identiﬁed vulnerabilities would
be erroneously attributed to library functions. For example, buﬀer
overﬂows are often caused by incorrect use of library functions like
strcpy and memcpy.
Ravel’s data-ﬂow analysis can cover lots of memory vulnerabili-
ties as most memory-corrupting exploits disturb the program’s data
ﬂow. For example, it can locate both memory vulnerabilities in
Figure 2. Speciﬁcally, line 7 contains a buﬀer overﬂow.
If ex-
ploited, it deﬁnes the overﬂowed data on the stack, including the