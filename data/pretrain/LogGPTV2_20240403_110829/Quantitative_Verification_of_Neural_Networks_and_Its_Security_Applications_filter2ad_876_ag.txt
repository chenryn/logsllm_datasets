ICML’13.
[35] Stefano Ermon, Carla P Gomes, Ashish Sabharwal, and Bart Selman. 2013. Op-
timization with parity constraints: From binary codes to discrete integration.
arXiv (2013).
Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul
Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on deep
learning models. In CVPR’18.
[37] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh
Venkatasubramanian. Certifying and removing disparate impact. In SIGKDD’15.
[38] Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk. 2019. Adversarial
[36]
Examples Are a Natural Consequence of Test Error in Noise. arXiv (2019).
[39] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks
that exploit confidence information and basic countermeasures. In CCS’15.
[40] Matt Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ris-
tenpart. Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized
Warfarin Dosing.. In USENIX’14’.
[41] Angus Galloway, Graham W Taylor, and Medhat Moussa. Attacking Binarized
[42] Vijay Ganesh and David L Dill. A decision procedure for bit-vectors and arrays.
Neural Networks. In ICLR’18.
In CAV’07.
[47]
[45] Arturo Geigel. 2013. Neural network trojan. Journal of Computer Security (2013).
Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E
[46]
Dahl. 2018. Motivating the rules of the game for adversarial example research.
arXiv (2018).
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu,
Martin Wattenberg, and Ian Goodfellow. 2018. Adversarial spheres. arXiv (2018).
[48] Alessandro Giusti, Jerome Guzzi, Dan C. Ciresan, Fang-Lin He, Juan P. Rodriguez,
Flavio Fontana, Matthias Faessler, Christian Forster, Jurgen Schmidhuber, Gi-
anni Di Caro, Davide Scaramuzza, and Luca M. Gambardella. 2016. A Machine
Learning Approach to Visual Perception of Forest Trails for Mobile Robots. IEEE
Robotics and Automation Letters (2016).
[49] Carla P. Gomes, Ashish Sabharwal, and Bart Selman. Model counting: A new
strategy for obtaining good bounds. In AAAI’06.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Har-
nessing Adversarial Examples. In ICLR’15.
[51] Radu Grosu and Scott A Smolka. Monte carlo model checking. In TACAS’05.
[52] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised
[50]
[53] W Keith Hastings. 1970. Monte Carlo sampling methods using Markov chains
[54] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
learning. In NIPS’16.
and their applications. (1970).
for image recognition. In CVPR’16.
[55] Steven Holtzen, Guy Broeck, and Todd Millstein. Sound Abstraction and Decom-
position of Probabilistic Programs. In ICML’18.
[57]
[56] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety Verification
of Deep Neural Networks. In CAV’17.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. Binarized neural networks. In NIPS’16.
[58] Mark R. Jerrum and Alistair Sinclair. 1996. The Markov chain Monte Carlo
method: an approach to approximate counting and integration. Approximation
algorithms for NP-hard problems (1996), 482–520.
[59] Kyle D Julian, Jessica Lopez, Jeffrey S Brush, Michael P Owen, and Mykel J
Kochenderfer. Policy compression for aircraft collision avoidance systems. In
DASC’16.
[60] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
Reluplex: An efficient SMT solver for verifying deep neural networks. In CAV’17.
[61] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influ-
ence functions. In ICML’17.
µ-calculus. In WLP’1983.
[62] Dexter Kozen and Rohit Parikh. A decision procedure for the propositional
[64]
[63] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
with deep convolutional neural networks. In NIPS’12.
Jaeha Kung, David Zhang, Gooitzen van der Wal, Sek Chai, and Saibal Mukhopad-
hyay. 2018. Efficient object detection using embedded binarized neural networks.
Journal of Signal Processing Systems (2018).
[65] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature
[66] Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database.
(2015).
(2010).
[67] Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun
Zhu. Defense against adversarial attacks using high-level representation guided
denoiser. In CVPR’18.
[68] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang
Wang, and Xiangyu Zhang. Trojaning Attack on Neural Networks. In NDSS’18.
[69] Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. 2018. The
curse of concentration in robust learning: Evasion and poisoning attacks from
concentration of measure. arXiv (2018).
[70] Bradley McDanel, Surat Teerapittayanon, and H T Kung. Embedded Binarized
Neural Networks. In EWSN’17.
[71] Kuldeep S. Meel. 2017. Constrained Counting and Sampling: Bridging the Gap
between Theory and Practice. Ph.D. Dissertation. Rice University.
[72] Kuldeep S. Meel, Moshe Y. Vardi, Supratik Chakraborty, Daniel J Fremont, Sanjit A
Seshia, Dror Fried, Alexander Ivrii, and Sharad Malik. Constrained Sampling
and Counting: Universal Hashing Meets SAT Solving. In AAAI’16 (Beyond NP
Workshop).
[73] Nina Narodytska, Shiva Prasad Kasiviswanathan, Leonid Ryzhyk, Mooly Sagiv,
and Toby Walsh. Verifying properties of binarized deep neural networks. In
AAAI’18.
[74] Radford M Neal. 1993. Probabilistic inference using Markov chain Monte Carlo
methods. (1993).
[75] Robert Nieuwenhuis and Albert Oliveras. 2005. Decision procedures for SAT, SAT
modulo theories and beyond. the BarcelogicTools. In International Conference on
Logic for Programming Artificial Intelligence and Reasoning. Springer, 23–46.
[76] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability
in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial
Samples. arXiv (2016).
[77] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay
Celik, and Ananthram Swami. The limitations of deep learning in adversarial
settings. In EuroS&P’16.
[78] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
Distillation as a defense to adversarial perturbations against deep neural net-
works. In SP’16.
[79] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in PyTorch. In NIPS-W’17.
[80] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated
whitebox testing of deep learning systems. In SOSP’17.
[81] Tobias Philipp and Peter Steinke. 2015. PBLib – A Library for Encoding Pseudo-
Boolean Constraints into CNF. In Theory and Applications of Satisfiability Testing
– SAT 2015.
[82] Luca Pulina and Armando Tacchella. An abstraction-refinement approach to
verification of artificial neural networks. In CAV’10.
[83] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. In ICLR’18.
[84] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.
Xnor-net: Imagenet classification using binary convolutional neural networks.
In European Conference on Computer Vision.
[85] Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska. Reachability Analysis
of Deep Neural Networks with Provable Guarantees. In IJCAI’18.
[86] Sanjit Arunkumar Seshia, Ankush Desai, Tommaso Dreossi, Daniel Fremont,
Shromona Ghosh, Edward Kim, Sumukh Shivakumar, Marcell Vazquez-Chanlatte,
and Xiangyu Yue. Formal Specification for Deep Neural Networks. In ATVA’18.
[87] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Member-
ship inference attacks against machine learning models. In SP’17.
[88] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolu-
tional networks: Visualising image classification models and saliency maps. In
ICLR’13.
[89] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin
Vechev. Fast and effective robustness certification. In NIPS’18.
[90] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract
domain for certifying neural networks. In PACMPL’19.
[91] Carsten Sinz. Towards an optimal CNF encoding of boolean cardinality con-
straints. In CP’05.
[94]
[92] Mate Soos and Kuldeep S Meel. BIRD: Engineering an Efficient CNF-XOR SAT
Solver and its Applications to Approximate Model Counting. In AAAI’19.
[93] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep
networks. In ICML’17.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning
with neural networks. In NIPS’14.
[95] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
deeper with convolutions. In CVPR’15.
[96] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks.
In ICLR’14.
[97] Seinosuke Toda. 1989. On the computational power of PP and (+)P. In Proc. of
FOCS. IEEE, 514–519.
[98] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In
ICLR’18.
[99] Grigori S Tseitin. 1983. On the complexity of derivation in propositional calculus.
In Automation of reasoning.
Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet
Kohli. Adversarial risk and the dangers of evaluating against weak attacks. In
ICML’18.
[101] Leslie G. Valiant. 1979. The complexity of enumeration and reliability problems.
[100]
SIAM J. Comput. (1979).
[103]
[102] Bie Verbist, Günter Klambauer, Liesbet Vervoort, Willem Talloen, Ziv Shkedy,
Olivier Thas, Andreas Bender, Hinrich WH Göhlmann, Sepp Hochreiter, QSTAR
Consortium, et al. 2015. Using transcriptomics to guide lead optimization in drug
discovery projects: Lessons learned from the QSTAR project. Drug discovery
today (2015).
Izhar Wallach, Michael Dzamba, and Abraham Heifets. 2015. AtomNet: A deep
convolutional neural network for bioactivity prediction in structure-based drug
discovery. arXiv (2015).
[104] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana.
Formal Security Analysis of Neural Networks using Symbolic Intervals. In
USENIX’18.
[105] Stefan Webb, Tom Rainforth, Yee Whye Teh, and M Pawan Kumar. A Statistical
Approach to Assessing Neural Network Robustness. In ICLR’19.
[106] Eric Wong and Zico Kolter. Provable Defenses against Adversarial Examples
via the Convex Outer Adversarial Polytope. In ICML’18.
[107] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigat-
ing Adversarial Effects Through Randomization. In ICLR’18.
[108] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P
Gummadi. Fairness constraints: Mechanisms for fair classification. In AIS-
TATS’15.
10 APPENDIX
10.1 Lemma 5.1 Detailed Proof
For the ease of proof of Lemma 5.1, we first introduce the notion
of independent support. Independent Support. An independent
support ind for a formula F(x) is a subset of variables appearing in
formula F, ind ⊆ x, that uniquely determine the values of the other
variables in any satisfying assignment [23]. In other words, if there
exist two satisfying assignments τ1 and τ2 that agree on ind then
τ1 = τ2. Then R(F) = R(F) ↓ ind.
Proof. We prove that R(φ) = R(φ) ↓ x by showing that x is an
independent support for BNN. This follows directly from the con-
struction of BNN. If x is an independent support then the following
has to hold true:
(cid:16)
G =
BNN(x, y, aV ) ∧ BNN(x′, y′, a′
V ) ∧ (x = x′) ⇒
V )(cid:17)
(y = y′) ∧ (aV = a′
3
G =
¬G =
) ∧ . . . ∧ OUT(v
(b)
d , y))
∧ (BLK1(x′, v′(b)
2
∧ (BLK1(x′, v′(b)
2
(cid:16)(BLK1(x, v
G is valid if and only if ¬G is unsatisfiable.
As per Table 2, we expand BNN(x, y):
(b)
(b)
(b)
(b)
) ∧ BLK2(v
d , ord, y)
) ∧ . . . ∧ OUT(v
2 , v
2
3
V )(cid:17)
2 , v′(b)
) ∧ . . . ∧ OUT(v′(b)
) ∧ BLK2(v′(b)
d , ord, y′)
∧ (x = x′) ⇒ (y = y′) ∧ (aV = a′
(cid:16)(BLK1(x, v
d , y′) ∧ (x = x′) ∧ ¬(y = y′)(cid:17)
∨(cid:16)
) ∧ . . . ∧ OUT(v′(b)
V )(cid:17)
(b)
d , y)
BLK1(x, v
) ∧ . . . ∧ OUT(v
d , y′) ∧ (x = x′) ∧ ¬(aV = a′
(b)
The first block’s formula’s introduced variables v
are uniquely
2
determined by x. For every formula BLKk corresponding to an
internal block the introduced variables are uniquely determined by
the input variables. Similarly, for the output block (formula OUT in
2 = v′(b)
(b)
Table 2). If x = x′ then v
2 , . . . ⇒ aV = a′
V , so the second
(b)
= v′(b)
d ⇒ y = y′. Thus,
clause is not satisfied. Then, since v
d
G is a valid formula which implies that x forms an independent
support for the BNN formula ⇒ R(φ) = R(φ) ↓ x.
∧ (BLK1(x′, v′(b)
2
) ∧ . . . ∧ OUT(v′(b)
(b)
2
(b)
2
□
10.2 Quantitative Verification is #P-hard
We prove that quantitative verification is #P-hard by reducing the
problem of model counting for logical formulas to quantitative
verification of neural networks. We show how an arbitrary CNF
formula F can be transformed into a binarized neural net f and a
specification φ such that the number of models for F is the same
as φ, i.e., |R(φ)| = |R(F)|. Even for this restricted class of multilayer
perceptrons quantitative verification turns out to be #P-hard. Hence,
in general, quantitative verification over multilayer perceptrons is
#P-hard.
Theorem 10.1. NQV (φ) is #P-hard, where φ is a specification for
a property P over binarized neural nets.
Figure 5. Disjunction gadget: Perceptron equivalent to an
OR gate
Figure 6. Conjunction gadget: Perceptron equivalent to an
AND gate
Proof. We proceed by constructing a mapping between the
propositional variables of the formula F and the inputs of the BNN.
We represent the logical formula as a logical circuit with the gates
AND, OR, NOT corresponding to ∧,∨,¬. In the following, we show
that for each of the gates there exist an equivalent representation
as a perceptron. For the OR gate we construct an equivalent per-
ceptron, i.e., for every clause Ci of the formula F, we construct a
perceptron. The perceptron is activated only if the inputs corre-
spond to a satisfying assignment to the formula F. Similarly, we
show a construction for the AND gate. Thus, we construct a BNN
that composes these gates such that it can represent the logical
formula exactly.
Let F be a CNF formula F = C1 ∧ C2 ∧ . . . Cn over the proposi-
tional variables PROP = {p1, p2, . . . pk}. We denote the literals
appearing in clause Ci as lij, j = 1, ..m, i.e., Ci = li1 ∨ li2 . . . ∨ lim.
Let τ : PROP → {0, 1} be an assignment for F. We say F is sat-
isfiable if there exists an assignment τ such that τ(F) = 1. The
binarized neural net f has inputs x and one output y, y = N(x), and
f : {−1, 1}m·n → {0, 1}. This can easily we extended to multi-class
output.
We first map the propositional variables pi ∈ PROP to variables
in the binary domain {−1, 1}. For every clause Ci, for every literal
lij ∈ {0, 1} there is a corresponding input to the neural net xij ∈
{−1, 1}: lij ⇔ xij = 1 ∧ lij ⇔ xij = −1. For each input variable
xij the weight of the neuron connection is 1 if the propositional
variable lij appears as a positive literal in the Ci clause and −1 if it
appears as a negative literal lij in Ci.
output of qi is -1. The intermediate variable ti =m
□
1, i.e., if all the clauses are satisfied.
qn ≥ n. The intermediate result t′ =n
is y =n
For every clause Ci appearing the formula ψ, we construct a
disjunction gadget, a perceptron with an equivalent function as the
OR gate. Given m inputs xi1, xi2, . . . xim ∈ {−1, 1}, the disjunction
gadget outputs a node qi that is 1 only if ti ≥ 0, otherwise the
j=1 wj · xij + m.
The output qi is 1 only if at least one literal is true, i.e., not all wj ·xij
terms evaluate to -1. Notice that we only need m + 2 neurons for
each clause Ci with m literals.
We next introduce the conjunction gadget which, given n inputs
q1, . . . , qn ∈ {−1, 1} outputs a node y that is 1 only if q1 +q2 + . . . +
i =1 wi ·qi −n over which we
apply the sign activation function. The output of this conjunction
i =1 wi · qi ≥ n which is 1 only if all of the variables yi are
If the output of f is 1 the formula F is SAT, otherwise it is UNSAT.
For every satisfying assignment τ for the formula F, there exists an
accepting output y for the binarized neural net, i.e., f (τ(x)) = τ(y).
Hence, if there exists a procedure #SAT(F) that accepts formula
ψ and outputs a number r which is the number of satisfying as-
signments, it will also compute the number of inputs for which the
output of the BNN is 1. Specifically, we can construct a quantita-
tive verifier for the neural net f and a specification φ(x, y) = (y =
N(x)) ∧ y = 1 using #SAT(ψ).
Reduction is polynomial. The size of the formula ψ is the size
of the input x to the neural net, i.e., m · n. The neural net has
n + 1 perceptrons (n for each disjunction gadget and one for the
conjunction gadget).