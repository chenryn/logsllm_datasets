dom sampling and histogram maintenance strategies. We
present the performance of the basic protocol with and with-
out route caching optimizations, discuss the e(cid:11)ect of skewed
node-range distributions and validate our claim that the pro-
tocol using histograms achieves near-optimal routing delays.
As before, we concentrate on routing within a single hub. In
each of the following experiments, nodes establish k = log n
long-distance links within a hub.
We experiment with two di(cid:11)erent data workloads { uni-
form and Zipf. The Zipf workload is high-skewed and is
generated using the distribution x(cid:0)(cid:11) where (cid:11) = 0:95. No-
tice that this means that the attribute values near 0:0 are
the most popular and those around 1:0 are the least pop-
ular. We also show the performance of two types of route
caching policies, viz., LRU replacement and a direct-mapped
cache.10 Our objective here is not to (cid:12)nd the best possible
policy for our workload. Rather, our aim is to show the ease
with which application-speci(cid:12)c caching can co-exist fruit-
fully with Mercury routing.
In our caching experiments,
each node keeps a cache of log n route entries.
Figure 6 shows the performance of Mercury when node
ranges are uniformly distributed. The Y-axis shows the av-
erage number of hops taken by a data-item to reach its desti-
nation (node where it is stored) in the hub. Although these
graphs show results for HistoLink overlay, the performance
of NodeLink and ValueLink is very similar, as expected.
We see that, for uniform node ranges, the number of rout-
ing hops scales logarithmically (with very low constant fac-
tors) as the number of nodes increases, irrespective of the
workload used. Thus, Mercury can provide low end-to-end
routing delays to applications even for a large number of
nodes. With caching enabled, there is a signi(cid:12)cant reduc-
tion in hop count. While this is easy to see for a skewed
workload, the reduction for a uniform workload results from
the fact that a cache e(cid:11)ectively increases Mercury’s routing
table size. We believe that caching is an important opti-
mization which Mercury can easily incorporate into its basic
protocol.
Effect of Non›Uniform Ranges
Figure 7 compares the performance of the protocol with and
without approximate histograms to guide the selection of
the long-distance links. In this experiment, the node-range
10For an n-entry cache, there is one entry for each of the
(1=n)th region of the attribute space.
s
p
o
H
#
e
g
a
r
e
v
A
180
160
140
120
100
80
60
40
20
0
ValueLink
ValueLink + LRU Cache
HistoLink
NodeLink
0
5000
10000
15000
20000
25000
30000
Number of nodes
Figure 7: E(cid:11)ect of non-uniform node ranges on the
average number of routing hops. As workload, we
use the Zipf distribution with (cid:11) = 0:95.
distribution and the data distribution are Zipf-skewed. For
histogram maintenance in this experiment, we used 5 ex-
change rounds, where each node queried log n nodes per
round asking each for log n estimate reports.
As explained in Section 5.1, the na(cid:127)(cid:16)ve ValueLink overlay
(vanilla DHT in the presence of load balancing) creates links
which skip the crowded and popular region (see Figure 4.)
Hence, packets destined to these nodes take circuitous routes
along the circle rather than taking short cuts provided by
the long-distance links. Although caching ameliorates the
e(cid:11)ect, the performance is still much worse as compared to
the optimal NodeLink overlay.
On the other hand, we see that the performance of the
HistoLink overlay is nearly the same as that of the optimal
NodeLink overlay. Again, looking at Figure 4, we (cid:12)nd that
node-count histograms enable nodes to establish a correct
link distribution (corresponding to the NodeLink overlay)
quickly using very low overheads.
Figure 5(c) shows the e(cid:11)ect of histogram accuracy on the
overall routing performance. We see that as the parameters
k1 and k2 in the histogram maintenance process increase,
the routing performance improves as expected. We note
that this in(cid:13)uence is limited (note the scale of the graph)
since it is directly dependent on the accuracy of the obtained
histograms (see Figure 5(a).)
5.4 Estimating Query Selectivity
To evaluate the usefulness of forwarding queries to the
most selective attribute hubs, we set up an experiment with
s
p
o
h
#
e
g
a
r
e
v
A
16
14
12
10
8
6
Without cache
LRU Cache
Direct-mapped Cache
10000
20000
30000
Number of nodes
40000
50000
s
p
o
h
#
e
g
a
r
e
v
A
16
14
12
10
8
6
4
Without cache
LRU Cache
Direct-mapped Cache
10000
20000
30000
Number of nodes
40000
50000
(a) Uniform workload
(b) Zipf-skewed workload
Figure 6: Performance of the basic Mercury protocol for various workloads.
s
e
i
r
e
u
q
i
g
n
s
s
e
c
o
r
p
s
e
d
o
n
#
e
g
a
r
e
v
A
70
60
50
40
30
20
10
0
Random Hub Selection
Histogram based Hub Selection
0
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
0.1
0.11
Mean query range
)
s
d
n
u
o
r
#
(
e
m
i
t
e
c
n
e
g
r
e
v
n
o
C
300
250
200
150
100
50
0
0.1
0.2
0.3
Delta:1.5
Delta:2.0
0.8
0.9
1
0.5
0.4
0.7
Load skew (Zipf parameter)
0.6
Figure 8: Nodes required to answer a query for vary-
ing query ranges, with and without selectivity esti-
mation.
Figure 9: Rounds taken to achieve load balance as a
function of the initial skew of the load. Delta is the
degree of load balance sought.
3 attribute hubs. Our workload is motivated by the dis-
tributed multi-player game application we describe in Sec-
tion 6. The attribute hubs correspond to three dimensions
of virtual space. Each query contained 3 range predicates
{ one for each attribute. Such a query speci(cid:12)es a cuboid
region of the virtual space. The range-size of each predicate
was Gaussian-distributed, while the range position within
the attribute space was Zipf-distributed. The node-range
distribution within each hub is skewed.
Figure 8 plots the average number of nodes processing a
query for di(cid:11)erent sizes of queries. The query size is mea-
sured as the average fraction of the value range that a query
covers in a single dimension. We call this the query range.
The plot shows that, even with our conservative workload,
choosing a hub based on the selectivity estimates results in
up to 25-30% reduction in the degree of (cid:13)ooding of a query.
With workloads exhibiting wildcards, much higher reduc-
tions would be expected. Because we lack a good model for
wildcard workloads, however, we refrain from speculating
about the exact degree of reduction that might be achieved.
5.5 Load Balancing
For evaluating the e(cid:14)ciency of load balancing achieved by
Mercury’s load balancing algorithm, we conduct the follow-
ing experiment: In a system of 1000 nodes, we assign load
to each node using a Zipf distribution with varying values
of the initial skew (Zipf parameter). The system is said to
be load-balanced when 1
(cid:1) (cid:20) load =avg load (cid:20) (cid:1) holds for all
nodes in the system.
In the experiment, we run multiple rounds of the load
balancing algorithm, until the system is load-balanced. Each
round consists of the following steps:
1. Each node samples its neighbors, to determine the lo-
cal node-count. This requires one round-trip.
2. Each node runs one round of the histogram mainte-
nance algorithm. (Recall that a round of the histogram
maintenance algorithm involves sending log n probes in
parallel, each of which must traverse 1 + log n hops.)
3. Nodes check their histograms to determine if they are
heavily loaded.
If a node is heavily loaded, it sends
a probe to a lightly loaded node. This probe must
traverse log n hops.
4. Lightly loaded nodes leave and re-join the network. To
re-join, the lightly loaded nodes must establish new
long links. The link establishment messages traverse
1 + log n hops, in expectation.
Figure 9 plots the number of rounds of load-balancing
required to achieve load balance. We see that Mercury can
load-balance to within a factor of (cid:1) = 2 within 100 rounds
despite heavy skews in the workload (Zipf with (cid:11) = 0:95).
In practical terms, consider an overlay with 10000 nodes,
and a 50 ms delay between nodes. The time to complete
one round of load-balancing is the product of the number of
hops traversed by messages in the load balancing algorithm
11, and the inter-node delay. Thus the time to complete one
round is 50 (cid:3) (4 + 3 log n) ms. The time to load-balance the
entire overlay is then 100(cid:3) 50(cid:3) (4 + 3 log n) ms, or about 220
seconds, which is reasonable for such a large network.
11Since the messages in step 2 are sent in parallel, we count
the number of hops once, rather than multiplying by the
number of messages. Similarly for step 4.
6. DISTRIBUTED APPLICATION DESIGN
Previous sections have demonstrated that Mercury pro-
vides scalable range-based lookups. In this section, we de-
scribe how the range query support provided by Mercury can
also be used as a building block for distributed applications.
Speci(cid:12)cally, we consider a multiplayer game, and demon-
strate how multi-attribute range queries can be fruitfully
applied to solve the game’s distributed state maintenance
problem.
6.1 Distributed State Maintenance
While distributed state maintenance,
One of the di(cid:14)culties in designing distributed multiplayer
games is managing the game state. Game state includes
such information as where a player is located, the resources
(such as ammunition) he has, and how healthy he is. For a
distributed game, this state must be available at multiple,
perhaps physically distributed, machines. This is necessary
so that the players in the game (who may be running the
game on di(cid:11)erent nodes) have accurate views of the game.
Thus, the central challenge is providing a way for the nodes
in the game to update each others’ view of the game state.
in its full gener-
ality, is a large and di(cid:14)cult problem, a few observations
about games in particular enable us to simplify the prob-
lem. First, observe that a node in a game is interested in
only a small subset of the entire game state. Second, note
that the objects belonging to this subset are not arbitrary,
but are related to each other. For example, in most mul-
tiplayer games, a player is primarily interested in entities
geographically nearby (such as within the player’s room).
Third, note that games do not require strong consistency
guarantees. In particular, even centralized games trade-o(cid:11)
strict consistency to provide interactive response times.