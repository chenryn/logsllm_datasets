Figure 7. An optimized implementation of CLK.
EventML
spec.
79N (1H)
646N (4H)
1729N (2D)
820N (2D)
GPM
prog.
452N
opt. GPM correctness
LoE
properties
prog.
spec.
73N (1H)
590N
249N
122N (1H)
1398N 1343N 1752N
97N (1H)
2673N 2625N 3165N
1434N 1352N 1245N
418N (1H)
correctness
proofs
1A/3M (2H)
8A/6M (3D)
24A/75M (3W)
0A/22M (1W)
CLK
TwoThird Consensus
Paxos-Synod
Broadcast Service
SOME STATISTICS REGARDING SPECIFICATION, VERIFICATION, CODE GENERATION OF VARIOUS MODULES. “N” STANDS FOR NUMBER OF AST
NODES, “A” FOR AUTOMATICALLY PROVED LEMMAS, “M” FOR MANUALLY PROVED LEMMAS, “H”, FOR HOURS, “D” FOR DAYS, AND “W” FOR
Table I
WEEKS.
and subsequently accept lower ballots. Such bugs cannot be
found easily with either testing or model checking.
During the past several years, we drew from our ex-
perience in building formal method tools, programming
languages, and distributed systems to develop tools and
libraries of deﬁnitions and lemmas that offer the right
level of abstraction to build formally veriﬁed distributed
algorithms. We have learned from our experiences that it
is best to adapt logical methods to follow the way system
designers build and reason about systems. For example,
LoE captures some design patterns that distributed system
developers often use such as the delegation of a task to a
sub-process. Our LoE delegation combinator allows us to
specify distributed programs using a modular or “divide and
conquer” method, which makes human reasoning tractable.
Also, instead of deriving code in a top-down way from
abstract speciﬁcations using reﬁnements maps, our tools
directly generate both code and speciﬁcations from pseudo-
code speciﬁcations, which allows us to test the code before
doing any formal reasoning.
We started our experiments with a consensus protocol that
we call TwoThird Consensus, based on the One Third Rule
algorithm [18]. Simpler than Paxos, it is a leaderless, round-
based protocol that is fully symmetric. With the tools that
we developed, it now takes us a few days to specify it in
EventML and prove its safety properties.
Table I presents some statistics. While the CLK speciﬁ-
cation contains 79 nodes in the EventML Abstract Syntax
Tree (AST), TwoThird Consensus is almost 10 times larger.
We developed the CLK speciﬁcation in under an hour, and
a person experienced with our environment can develop the
EventML speciﬁcation of TwoThird Consensus from an in-
formal speciﬁcation in an afternoon. The next three columns
show the sizes (in Nuprl AST nodes) of the automatically
generated LoE speciﬁcation, the GPM program, and the size
of the GPM program after optimization. Note that we can
run and test GPM programs even before we proved any
properties about them.
The “correctness properties” column shows the approxi-
mate time required to formalize the correctness properties
from informal speciﬁcation. In each case, we were able to
do so in under an hour. Proving the properties is another
issue. Nuprl is a tactic-based prover in the style of LCF [19].
These hand-crafted tactics try to ﬁnd a proof of a lemma
automatically. This is not always successful. The last column
speciﬁes the number of lemmas that were proved automati-
cally by Nuprl without interactive assistance, and the number
of lemmas that required manual help from us. In the case
6
of CLK, we were able to do all this in a matter of a few
hours, and it took us only three days to formally prove the
safety properties of the TwoThird Consensus speciﬁcation.
This is in part due to the rich library of tactics and lemmas
about LoE that we developed over the past several years.
At the time, we only proved safety properties. Interest-
ingly, using manual inspection of the code we found that our
initial version was not live because of a deadlock scenario.
We would have found this if we had tried proving liveness
properties. Fixing the speciﬁcation as well as ﬁxing the
proofs turned out to be easy. About two lines of code had
to change and it took less than a day to ﬁx the proofs.
We then moved on to the multi-decree Paxos Synod pro-
tocol, the heart of the same protocol in the Paxos implemen-
tation used by Google. We started from an existing informal
English speciﬁcation of the protocol and its correctness
properties [20]. While it took us years when counting the
development of the tools and libraries of tactics and lemmas,
it is now possible to formally specify Synod in a few days
and verify it in a few weeks.
We made a mistake in an early version of our EventML
speciﬁcation of Synod. Running and testing the unveriﬁed
GPM code did not reveal the bug—instead, we found the
bug when we were unable to prove the safety properties of
our speciﬁcation. Thankfully, ﬁxing the speciﬁcation as well
as the proofs turned out to be a fairly easy task (only a few
lines of code had to change and it only took us a few days
to ﬁx the proofs). Not shown in the table, we also speciﬁed
and veriﬁed the Multi-Paxos protocol, including the state
machine replicas. Again, we made mistakes but ﬁxing the
mistakes required mostly machine time.
Finally, we built a total order broadcast service that
we use in the replicated databases described in the next
section. The total order broadcast service guarantees that the
participating processes deliver the same messages and in the
same order [21]. The total order broadcast service builds
upon consensus protocols, and is able to switch between
protocols for different messages. Currently, the total order
broadcast service can use both the TwoThird Consensus
and the Paxos multi-decree Synod consensus modules. This
demonstrates that we can develop complex services in a
modular fashion.
III. Building a Replicated Database
The total order broadcast service is a powerful building
block for implementing various well-known fault-tolerant
replication protocols such as primary-backup [6], state ma-
chine replication [7], deferred update replication [22], and
chain replication [23].
We have developed a replicated database, ShadowDB, that
can be conﬁgured with either primary-backup (PBR) or state
machine replication (SMR). In both cases, strict serializabil-
ity consistency is ensured [24]: to clients it appears as if
transactions were executed sequentially, each at some point
between the time that a client submitted the transaction and
the client received the result. We assume that sequential
transaction execution is deterministic.
For both primary-backup and state machine replication we
assume a partially-synchronous environment [25] and crash
failures only—failure detection is unreliable in either case.
The participants communicate over TCP channels, and we
assume that correct processes can eventually communicate
with one another. ShadowDB does not currently mask bugs
that lead databases to corrupt data, improperly handle con-
currency, or give unauthorized access to users. Mandelbugs
and Heisenbugs can cause replica states to diverge even if
transactions are ordered. Dealing with these is the subject
of future work.
In the case of primary-backup, we make a distinction
between normal case processing and failure handling. The
normal case protocol is relatively simple and hand-written (it
deals with ordering transactions only). If either the primary
or a backup is suspected of having failed, the total order
broadcast service is used to propose and decide on a new
conﬁguration. With state machine replication, transactions
are ordered by broadcasting them to the replicas using the
total order broadcast service. For both primary-backup and
state machine replication, the crash of all but one of the
database replicas can be masked. It is worth noting that
the number of tolerated database replica failures is different
from the number of failures tolerated by the total order
broadcast service. When using Paxos to broadcast messages
in order, only a minority of failures can be tolerated, that is,
if we deploy the broadcast service on three replicas, then at
most one failure can be masked.
In this section, we present in more detail how ShadowDB
handles transactions in both types of replication, and how
we exploit diversity to improve reliability. In the presenta-
tion below, T is a transaction. Submitting a transaction T
involves sending T ’s type and its parameters to a server. In
case of failures, clients may timeout and resend transactions
to the replicas. To ensure that a transaction is executed only
once, each replica has to keep track of which transactions
have been performed already, treating duplicates as no-ops.
This can be done efﬁciently by recording the sequence
number of the last transaction submitted by each client.
A. Primary-backup: Our primary-backup protocol han-
dles a transaction T in the normal case similar to other
primary-backup protocols [6]: (i) the client sends T to the
primary database, (ii) upon ﬁrst reception of T , the primary
7
executes and commits4 T and forwards T to the backups,5
(iii) the backups, upon receipt of T , also execute and
commit T , and send an acknowledgment back to the primary,
(iv) the primary waits to receive an acknowledgment from
all backups before notifying the client of the transaction’s
success. The notiﬁcation contains the transaction’s result
set, if any. Transaction execution is sequential both at the
primary and at backups.
If one or more replicas crash or become unreachable,
the protocol is unable to make progress. To detect failures,
the primary and backups monitor each other by periodically
exchanging heartbeats. The recovery procedure allows sur-
viving replicas to propose new conﬁgurations that exclude
suspect replicas and optionally replace them with new ones.
Different replicas could propose conﬂicting conﬁgurations.
The recovery procedure uses the total order broadcast service
to ensure agreement on the sequence of conﬁgurations.
Each conﬁguration is identiﬁed by a sequence number.
The initial conﬁguration has sequence number 0. During
normal operation, the primary tags transactions with the
sequence number of its conﬁguration. Backups only accept
and execute transactions if the sequence number tag matches
their current conﬁguration.
When a replica r suspects a subset of replicas to have
crashed, recovery happens as follows:
1) r stops executing transactions in the current conﬁgura-
tion, ensuring that the conﬁguration can no longer order new
transactions even if the failure suspicions are inaccurate.
2) r creates a proposal for a new conﬁguration and broad-
casts its proposal using the total order broadcast service.
This message is tagged with the current conﬁguration’s
sequence number and a list of replicas, where replicas that
are suspected of having crashed have been removed and
possibly replaced by new replicas.
3) Upon receipt of a proposal for a new conﬁguration, a
replica r(cid:48) ﬁrst checks if the proposal’s sequence number g
corresponds to its current conﬁguration. If not, r(cid:48) ignores
the proposal. This way only the ﬁrst proposal is considered
by the replicas. To elect a new primary, replica r(cid:48) sends
(g + 1, seqr(cid:48)) to all replicas in the new conﬁguration (over
TCP channels). Here seqr(cid:48) is the sequence number of the
last executed transaction by r(cid:48). If r(cid:48) was not part of the
previous conﬁguration, r(cid:48) sends (g + 1, 0).
4) Each replica in the new conﬁguration waits to hear
from all replicas. The new primary is the replica with the
largest sequence number. In case of a tie the replica with
the smallest identiﬁer wins.
4The execution of the transaction may lead the operations within the
transaction to request an abort. Because we assume transactions are
deterministic, all replicas will abort the transaction. Databases unable to
commit a transaction for other reasons are treated as crashed replicas.
5The primary does not extract state updates resulting from the execution
because we are using unmodiﬁed databases and cannot extract state updates
in general.
5) Where possible, the new primary sends missing trans-
actions to those backups that need to catch up. If this is
not possible (each replica only caches a limited number of
executed transactions), the new primary sends a snapshot of
its entire database.
6) Each backup sends an acknowledgment to the primary
upon recovery.
7) When the primary has received an acknowledgment
the normal protocol resumes and the
from all backups,
primary can start ordering new transactions.
If failures occur during recovery, the procedure is restarted.
It is easy to show that the recovery procedure maintains two
important properties:
• Durability: Once a client receives a transaction’s an-
swer, the execution of this transaction is permanently
reﬂected in the state of the surviving replicas;
• State-agreement: In each conﬁguration, replicas that
process transactions start in the same state.
The recovery protocol sends the entire database snapshot
to new replicas. This leads to a long disruption of the service
when the database is large. In some cases, it is possible to
overlap state transfer with the normal case protocol however.
If there are at least three replicas and at least one other
replica has been brought up-to-date by the primary, we
can resume normal execution and propagate the database
snapshot to the other backups in parallel. Such recovering
replicas buffer incoming transactions until they have the
full snapshot. The primary waits only for acknowledgments
from replicas that have recovered. While this takes place,
the maximum number of tolerated failures is one fewer
than the number of recovered replicas. If failures occur
during recovery, the recovery procedure is restarted but only
replicas with a full copy of the database can be candidates
to become primary.