Table 2 shows the detailed results for AlexNet and
Accuracy.
ResNet. Across different markets and parameter settings, the match-
ing accuracy is very high. Consistently, ResNet is more accurate
than AlexNet. For all three markets, ResNet has a matching accuracy
of 0.975 or higher when we don’t intentionally remove duplicated
images for different products.
Even after we remove the duplicated images, the matching accu-
racy is still around 0.871–0.932 for ResNet (for Tr =20). Recall that
this is a multi-class classifier with hundreds of classes. An accuracy
of 0.871 (for the top-1 matching candidate) is already very high.
In practice, analysts may consider the top-K matching candidates
(where K is a small number) instead of just the most likely one.
ABCDA1B1C1D1A2B2C2D2Vendors acting asDistractors  Vendors with 2xTr Images(splited into 2 pseudo identities) Training SetTesting SetDeep Neural Networks(transfer learning)Matched Identity PairsTestingTraining(a) With duplicated images
(b) Without duplicated images
Figure 4: Comparison of different DNN models. Tr = 20 for all the settings.
Figure 5: The ROC curves from the
ResNet model (Tr = 20, with duplicated
images).
Tp, we generate the ROC (Receiver Operating Characteristic) curves
in Figure 5. The results again confirm the good performance. The
ROC curves all reach the top-left corner of the plots, and the areas
under the curves (AUC) are close to 1.0 (a random classifier’s AUC
would be 0.5 and a higher AUC is better).
In practice, analysts can make their own trade-off between false
positives and true positives based on their time budget. If the time
allows, the analysts can afford to have some false positives so that
they don’t miss the actual Sybil identities of a given vendor. In this
paper, we use the ROC curves to pick the threshold Tp based on the
elbow point of the curve. The corresponding Tp is about 0.4 when
we allow duplicated images. The elbow Tp is 0.2–0.3 if duplicated
images are intentionally removed.
5 COMPARISON WITH STYLOMETRY
Our evaluation so far shows that the image-based approach is effec-
tive to fingerprint vendors. Next, we explore to compare our method
with existing stylometry approaches, and seek to further improve
the accuracy and the coverage of the system. In the following, we
briefly introduce the existing stylometry analysis methods and the
unique challenges to apply them to the darknet markets. Then we
evaluate the number of vendors that stylometry analysis can effec-
tively fingerprint, and the matching accuracy in comparison with
the image-based approach.
5.1 Stylometry Analysis
Stylometry analysis aims to attribute the authorship of the anony-
mous texts by analyzing the writing style. Existing works have ex-
plored the feasibility of identifying the authorship of underground
forum posts [1] and even computer programs [6]. To this end, the
stylometry analysis is a valid comparison baseline for our method.
In the darknet markets, a vendor’s texts are the product descriptions
written by the vendor. However, there are key challenges for sty-
lometry analysis in darknet markets. First, the product descriptions
are usually very short. For example, the median length of Agora’s
product descriptions is only 118 words. Second, the product de-
scriptions often follow certain templates, and vendors may use the
same/similar descriptions for many of their products. Third, most
darknet markets are international marketplaces where vendors may
use different languages. All these factors pose challenges to extract
the unique writing styles of the vendors.
To examine the feasibility of stylometry analysis, we follow the
most relevant work [1] and re-implement a similar stylometry clas-
sifier. More specifically, given the collection of the text of a vendor,
we extract a list of features to model the writing styles. The features
include: the percentage of words that start with an upper-case letter,
percentage of upper-case letters, average word length, word length
histogram, punctuation frequency, stop-word frequency, character
unigram, bigram and trigram, Part-of-Speech (POS) unigram, bi-
gram, and trigram, and digit unigram, bigram, and trigram. We used
the NLTK library [39] to perform word and sentence tokenization.
We applied Stanford Log-linear Part-Of-Speech Tagger [57] to ex-
tract the POS features. Considering the high dimensionality of the
feature vector (about 100K), we also perform dimension reduction
using stochastic singular value decomposition (StochasticSVD) to
reduce feature vector size to 1000. Then we use the feature vector
to train a logistic regression classifier to make predictions. We refer
interested readers to [1] for more details.
5.2 Performance Comparison
Our evaluation focuses on comparing the image-based approach
and the stylometry based approach. The goal is to understand
whether we can use stylometry analysis to further augment the
image-based method. Our evaluation metrics include two key as-
pects: accuracy (the accuracy to match pseudo identities) and cov-
erage (the number of vendors that can be reliably fingerprinted).
Our evaluation follows the same work-flow in Figure 3. To gen-
erate ground-truth data for stylometry analysis, we again split
vendors whose product descriptions with more than 2 × T ′
r words.
For vendors with more than T ′
r words, we add them as the distrac-
tors in the training set. Similar to before, we create two versions
of the ground-truth datasets, one considers all the product descrip-
tions (one description per product) and allows duplicated sentences.
The other ground-truth dataset removes the duplicated sentences.
The non-duplicated version aims to force the classifiers to learn the
writing style instead of matching the exact sentences. In this evalu-
ation, we only consider English text — we have removed Unicode
symbols and HTML entities.
Table 3 shows that the stylometry analysis can also
Accuracy.
achieve a high accuracy when we allow the duplicated sentences
(0.936–0.990). However, when we remove the duplicated sentences,
the accuracy dropped significantly to 0.580 – 0.846. This dramatic
accuracy decrease indicates that the previous high accuracy is likely
 0 0.2 0.4 0.6 0.8 1AgoraEvolutionSilkRoad2AccuracyMarketResNetDenseNetVGGAlexNetInception 0 0.2 0.4 0.6 0.8 1AgoraEvolutionSilkRoad2AccuracyMarketResNetDenseNetVGGAlexNetInception 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1True Positive RateFalse Positive RateSilkRoad2AgoraEvolutionDuplicated
Sentences
Market
Agora
Yes
Evolution
SilkRoad2
Agora
No
Evolution
SilkRoad2
T ′
r
1500
3000
4500
1500
3000
4500
1500
3000
4500
1500
3000
4500
1500
3000
4500
1500
3000
4500
Pseudo
Pairs
822
402
247
530
246
159
338
169
99
193
72
39
162
65
33
65
25
12
Training
Distractor Accuracy
0.983
0.988
0.988
0.936
0.967
0.987
0.970
0.988
0.990
0.694
0.806
0.846
0.580
0.723
0.818
0.631
0.800
0.833
515
420
316
404
284
179
200
169
120
300
121
63
235
97
59
126
40
26
Table 3: Accuracy of ground-truth vendor matching based
on stylometry analysis.
Duplicated
texts/images
Yes
No
Market
Agora
Evolution
SilkRoad2
Agora
Evolution
SilkRoad2
Image
Tr = 20
1020
1093
415
408
443
181
Stylometry
T ′
r = 4500
563
338
219
102
92
38
Table 4: Number of qualified vendors given the thresholds
for image analysis (Tr = 20 images) and stylometry analysis
(T ′
r = 4500 words).
the results of matching the duplicated sentences, instead of actually
extracting the generalizable “writing styles”. Our result shows that
the same method that works well in underground forums [1] has
major limitations in darknet markets. Consider that vendors often
follow templates to write product descriptions, it is understandable
that their personal writing styles are not as strong as the template-
free texts in underground forums.
Stylometry analysis has a more limited coverage. Ta-
Coverage.
ble 4 shows the number of qualified vendors for stylometry analysis
and image analysis, given the threshold that produces a comparable
accuracy (Tr = 20 and T ′
r = 4500 returns
the highest accuracy for stylometry analysis, but it is still not as
accurate as the image analysis (after removing duplicated images).
Meanwhile, the image analysis covers 100%–300% more vendors
than the stylometry analysis. The advantage is more significant
when duplicated texts or images are removed.
The image analysis also has a shorter runtime by
Run Time.
taking advantage of the GPUs. For example, the image analysis for
Agora (ResNet, Tr = 20, with duplicated images) takes one server
4 hours to finish the whole process including data preparation,
training, and testing. The server has one quad-core CPU and one
Nvidia GTX 1070 GPU. However, the stylometry analysis on Agora
(Tr = 4500, with duplicated texts) takes as long as 84 hours to finish
r = 4500). Note that T ′
(CPU only). In theory, it is possible to re-design the algorithm of [1]
to work with GPU, but it would take significant efforts to rewrite
the system, particularly the Part-of-Speech tagging algorithm.
In summary, the image-based approach has a clear advantage
over stylometry analysis to fingerprint darknet vendors. However,
these two techniques are not necessarily competing but can work
together to add additional layers of confidence. In the rest of the
paper, we primarily use the image-based approach to detect Sybil
identities in the wild, and check the writing style for confirmation
during the manual inspection.
6 SYBIL IDENTITY IN THE WILD
To demonstrate the usefulness of our system, we apply it to real-
world datasets to identify previously unknown Sybil identities in
the wild. We focus on two types of Sybil identities. First, we look for
vendors who controlled multiple accounts within the same market,
i.e., intra-market Sybils. Second, we look for vendors who controlled
multiple accounts across different markets, i.e., inter-market Sybils.
2
6.1 Detection Method
In the following, we introduce the Sybil detection method, which
is based on the image-based approach described in §4.
To detect Sybil accounts in different mar-
Inter-Market Sybils.
kets, we work on two markets at a time. For market A and B, we use
vendors from market A as the training data to build the classifier
and then test on vendors from market B. This step produces the sim-
ilarity score for any two vendors S (uAi , uBj ) from the two markets.
Then, we reverse the order by training on B’s data and testing with
A’s vendors to calculate a new similarity score for the same vendor
pair S (uAi , uBj ). The final similarity score between uAi and uBj
is the average value: SimuAi,uB j
. We set
parameters based on the ground-truth evaluation in §4. We focus
on vendors with more than Tr = 20 photos and set Tp = 0.4 as the
cut-off threshold for the final similarity score.
To detect Sybil accounts in the same
Intra-Market Sybils.
market, we again consider vendors who have more than Tr = 20
photos. We treat these vendors as the training set to build the
classifier. We treat the same set of vendors (with more than 20
photos) as the testing set, and apply the classifier to identify the
most similar vendors in the same market. We use Tp = 0.4 as the
cut-off threshold for the similarity score based on the ground-truth
evaluation. Note that this is not a standard machine learning process
since the training and testing sets are overlapped. Instead, we are
using the multi-class classifier to calculate the “distance” between
vendors to identify similar pairs.
= S (uAi,uB j )+S (uB j,uAi )
For both intra- and inter-market detection, we consider all the
photos of a vendor (one photo for each product) without intention-
ally removing the reused photos. Using the above thresholds, the
analysis covers 1,020 vendors in Agora, 1,093 vendors in Evolution
and 415 vendors in SilkRoad2 (2,528 vendors in total). We use the
most accurate ResNet DNN model for both cases.
6.2 Manual Investigation
To validate the accuracy of the detection, we act as the analysts
to manually examine the detected candidate vendor pairs. Our
Markets
Agora-Evolution
Agora-SilkRoad2
Evolution-SilkRoad2