“An action item to rewrite the entire backend system might actually prevent these annoying pages from continuing to happen, and the maintenance manual for this version is quite long and really difficult to be fully trained up on. I’m sure our future on-callers will thank us!”
Best Practice: Avoid Blame and Keep It ConstructiveBlameless postmortems can be challenging to write, because the postmortem format clearly identifies the actions that led to the incident. Removing blame from a post‐mortem gives people the confidence to escalate issues without fear. It is also impor‐tant not to stigmatize frequent production of postmortems by a person or team. An atmosphere of blame risks creating a culture in which incidents and issues are swept under the rug, leading to greater risk for the organization [Boy13].Collaborate and Share Knowledge
We value collaboration, and the postmortem process is no exception. The postmor‐tem workflow includes collaboration and knowledge-sharing at every stage.
Our postmortem documents are Google Docs, with an in-house template (see Appen‐dix D). Regardless of the specific tool you use, look for the following key features:
Real-time collaborationReal-time collaboration 
Enables the rapid collection of data and ideas. Essential during the early creation of a postmortem.
An open commenting/annotation system 
	Makes crowdsourcing solutions easy and improves coverage.
Email notifications 
Can be directed at collaborators within the document or used to loop in others to provide input.Writing a postmortem also involves formal review and publication. In practice, teams share the first postmortem draft internally and solicit a group of senior engineers to assess the draft for completeness. Review criteria might include:
Collaborate and Share Knowledge  |  171
• Was key incident data collected for posterity?
• Are the impact assessments complete?
• Was the root cause sufficiently deep?• Was the root cause sufficiently deep?
• Is the action plan appropriate and are resulting bug fixes at appropriate priority?
• Did we share the outcome with relevant stakeholders?Once the initial review is complete, the postmortem is shared more broadly, typically with the larger engineering team or on an internal mailing list. Our goal is to share postmortems to the widest possible audience that would benefit from the knowledge or lessons imparted. Google has stringent rules around access to any piece of infor‐mation that might identify a user,1 and even internal documents like postmortems never include such information.Best Practice: No Postmortem Left Unreviewed
An unreviewed postmortem might as well never have existed. To ensure that each completed draft is reviewed, we encourage regular review sessions for postmortems. In these meetings, it is important to close out any ongoing discussions and com‐ments, to capture ideas, and to finalize the state.Once those involved are satisfied with the document and its action items, the post‐mortem is added to a team or organization repository of past incidents.2 Transparent sharing makes it easier for others to find and learn from the postmortem.
Introducing a Postmortem CultureIntroducing a postmortem culture to your organization is easier said than done; such an effort requires continuous cultivation and reinforcement. We reinforce a collabo‐rative postmortem culture through senior management’s active participation in the review and collaboration process. Management can encourage this culture, but blameless postmortems are ideally the product of engineer self-motivation. In the spi‐rit of nurturing the postmortem culture, SREs proactively create activities that dis‐seminate what we learn about system infrastructure. Some example activities include:Postmortem of the month 
In a monthly newsletter, an interesting and well-written postmortem is shared with the entire organization.
1 Se 2 If 
172  e you
|  1 Se e .
2 If you’d like to start your own repository, Etsy has released , a tool for managing postmortems.
Chapter 15: Postmortem Culture: Learning from Failure
Google+ postmortem groupGoogle+ postmortem group 
This group shares and discusses internal and external postmortems, best practi‐ces, and commentary about postmortems.
Postmortem reading clubs 
Teams host regular postmortem reading clubs, in which an interesting or impact‐ful postmortem is brought to the table (along with some tasty refreshments) for an open dialogue with participants, nonparticipants, and new Googlers about what happened, what lessons the incident imparted, and the aftermath of the incident. Often, the postmortem being reviewed is months or years old!Wheel of Misfortune 
New SREs are often treated to the Wheel of Misfortune exercise (see “Disaster Role Playing” on page 401), in which a previous postmortem is reenacted with a cast of engineers playing roles as laid out in the postmortem. The original inci‐dent commander attends to help make the experience as “real” as possible.One of the biggest challenges of introducing postmortems to an organization is that some may question their value given the cost of their preparation. The following strategies can help in facing this challenge:
• Ease postmortems into the workflow. A trial period with several complete and successful postmortems may help prove their value, in addition to helping to identify what criteria should initiate a postmortem.• Make sure that writing effective postmortems is a rewarded and celebrated prac‐tice, both publicly through the social methods mentioned earlier, and through individual and team performance management.
• Encourage senior leadership’s acknowledgment and participation. Even Larry 	Page talks about the high value of postmortems!
Introducing a Postmortem Culture  |  173Best Practice: Visibly Reward People for Doing the Right ThingGoogle’s founders Larry Page and Sergey Brin host TGIF, a weekly all-hands held live at our headquarters in Mountain View, California, and broadcast to Google offices around the world. A 2014 TGIF focused on “The Art of the Postmortem,” which fea‐tured SRE discussion of high-impact incidents. One SRE discussed a release he had recently pushed; despite thorough testing, an unexpected interaction inadvertently took down a critical service for four minutes. The incident only lasted four minutes because the SRE had the presence of mind to roll back the change immediately, avert‐ing a much longer and larger-scale outage. Not only did this engineer receive two peer bonuses3 immediately afterward in recognition of his quick and level-headed handling of the incident, but he also received a huge round of applause from the TGIF audience, which included the company’s founders and an audience of Googlers numbering in the thousands. In addition to such a visible forum, Google has an array of internal social networks that drive peer praise toward well-written postmortems and exceptional incident handling. This is one example of many where recognition of these contributions comes from peers, CEOs, and everyone in between.4Best Practice: Ask for Feedback on Postmortem Effectiveness
At Google, we strive to address problems as they arise and share innovations inter‐nally. We regularly survey our teams on how the postmortem process is supporting their goals and how the process might be improved. We ask questions such as: Is the culture supporting your work? Does writing a postmortem entail too much toil (see Chapter 5)? What best practices does your team recommend for other teams? What kinds of tools would you like to see developed? The survey results give the SREs in the trenches the opportunity to ask for improvements that will increase the effective‐ness of the postmortem culture.Beyond the operational aspects of incident management and follow-up, postmortem practice has been woven into the culture at Google: it’s now a cultural norm that any significant incident is followed by a comprehensive postmortem.
3 Go in
4 Fo
174  ogl volv
r fu
|  	e’s Peer Bonus program is a way for fellow Googlers to recognize colleagues for exceptional efforts and involves a token cash reward.4 For further discussion of this particular incident, see Chapter 13.
Chapter 15: Postmortem Culture: Learning from Failure
Conclusion and Ongoing ImprovementsWe can say with confidence that thanks to our continuous investment in cultivating a postmortem culture, Google weathers fewer outages and fosters a better user experi‐ence. Our “Postmortems at Google” working group is one example of our commit‐ment to the culture of blameless postmortems. This group coordinates postmortem efforts across the company: pulling together postmortem templates, automating post‐mortem creation with data from tools used during an incident, and helping automate data extraction from postmortems so we can perform trend analysis. We’ve been able to collaborate on best practices from products as disparate as YouTube, Google Fiber, Gmail, Google Cloud, AdWords, and Google Maps. While these products are quite diverse, they all conduct postmortems with the universal goal of learning from our darkest hours.With a large number of postmortems produced each month across Google, tools to aggregate postmortems are becoming more and more useful. These tools help us identify common themes and areas for improvement across product boundaries. To facilitate comprehension and automated analysis, we have recently enhanced our postmortem template (see Appendix D) with additional metadata fields. Future work in this domain includes machine learning to help predict our weaknesses, facilitate real-time incident investigation, and reduce duplicate incidents.Conclusion and Ongoing Improvements  |  175
CHAPTER 16
Tracking Outages
Written by Gabe Krabbe 
Edited by Lisa Carey
Improving reliability over time is only possible if you start from a known baseline and can track progress. “Outalator,” our outage tracker, is one of the tools we use to do just that. Outalator is a system that passively receives all alerts sent by our moni‐toring systems and allows us to annotate, group, and analyze this data.Systematically learning from past problems is essential to effective service manage‐ment. Postmortems (see Chapter 15) provide detailed information for individual out‐ages, but they are only part of the answer. They are only written for incidents with a large impact, so issues that have individually small impact but are frequent and wide‐spread don’t fall within their scope. Similarly, postmortems tend to provide useful insights for improving a single service or set of services, but may miss opportunities that would have a small effect in individual cases, or opportunities that have a poor cost/benefit ratio, but that would have large horizontal impact.1We can also get useful information from questions such as, “How many alerts per on-call shift does this team get?”, “What’s the ratio of actionable/nonactionable alerts over the last quarter?”, or even simply “Which of the services this team manages cre‐ates the most toil?”1 For example, it might take significant engineering effort to make a particular change to Bigtable that only has a small mitigating effect for one outage. However, if that same mitigation were available across many events, the engineering effort may well be worthwhile.
177
EscalatorAt Google, all alert notifications for SRE share a central replicated system that tracks whether a human has acknowledged receipt of the notification. If no acknowledg‐ment is received after a configured interval, the system escalates to the next config‐ured destination(s)—e.g., from primary on-call to secondary. This system, called“The Escalator,” was initially designed as a largely transparent tool that received copies of emails sent to on-call aliases. This functionality allowed Escalator to easily integrate with existing workflows without requiring any change in user behavior (or, at the time, monitoring system behavior).Outalator
Following Escalator’s example, where we added useful features to existing infrastruc‐ture, we created a system that would deal not just with the individual escalating noti‐fications, but with the next layer of abstraction: outages.Outalator lets users view a time-interleaved list of notifications for multiple queues at once, instead of requiring a user to switch between queues manually. Figure 16-1 shows multiple queues as they appear in Outalator’s queue view. This functionality is handy because frequently a single SRE team is the primary point of contact for serv‐ices with distinct secondary escalation targets, usually the developer teams.Figure 16-1. Outalator queue view
Outalator stores a copy of the original notification and allows annotating incidents. For convenience, it silently receives and saves a copy of any email replies as well. Because some follow-ups are less helpful than others (for example, a reply-all sent with the sole purpose of adding more recipients to the cc list), annotations can be marked as “important.” If an annotation is important, other parts of the message are collapsed into the interface to cut down on clutter. Together, this provides more con‐text when referring to an incident than a possibly fragmented email thread.178  |  Chapter 16: Tracking Outages
Multiple escalating notifications (“alerts”) can be combined into a single entity (“inci‐dent”) in the Outalator. These notifications may be related to the same single inci‐dent, may be otherwise unrelated and uninteresting auditable events such as privileged database access, or may be spurious monitoring failures. This grouping functionality, shown in Figure 16-2, unclutters the overview displays and allows for separate analysis of “incidents per day” versus “alerts per day.”Figure 16-2. Outalator view of an incident
Building Your Own Outalator
Many organizations use messaging systems like Slack, Hipchat, or even IRC for inter‐nal communication and/or updating status dashboards. These systems are great places to hook into with a system like Outalator.
Outalator  |  179
AggregationA single event may, and often will, trigger multiple alerts. For example, network fail‐ures cause timeouts and unreachable backend services for everyone, so all affected teams receive their own alerts, including the owners of backend services; meanwhile, the network operations center will have its own klaxons ringing. However, even smaller issues affecting a single service may trigger multiple alerts due to multiple error conditions being diagnosed. While it is worthwhile to attempt to minimize the number of alerts triggered by a single event, triggering multiple alerts is unavoidable in most trade-off calculations between false positives and false negatives.The ability to group multiple alerts together into a single incident is critical in dealing with this duplication. Sending an email saying “this is the same thing as that other thing; they are symptoms of the same incident” works for a given alert: it can prevent duplication of debugging or panic. But sending an email for each alert is not a practi‐cal or scalable solution for handling duplicate alerts within a team, let alone between teams or over longer periods of time.TaggingOf course, not every alerting event is an incident. False-positive alerts occur, as well as test events or mistargeted emails from humans. The Outalator itself does not distin‐guish between these events, but it allows general-purpose tagging to add metadata to notifications, at any level. Tags are mostly free-form, single “words.” Colons, however, are interpreted as semantic separators, which subtly promotes the use of hierarchical namespaces and allows some automatic treatment. This namespacing is supported by suggested tag prefixes, primarily “cause” and “action,” but the list is team-specific and generated based on historical usage. For example, “cause:network” might be sufficient information for some teams, whereas another team might opt for more specific tags, such as “cause:network:switch” versus “cause:network:cable.” Some teams may fre‐quently use “customer:132456”-style tags, so “customer” would be suggested for those teams, but not for others.Tags can be parsed and turned into a convenient link (“bug:76543” links to the bug tracking system). Other tags are just a single word (“bogus” is widely used for false positives). Of course, some tags are typos (“cause:netwrok”) and some tags aren’t par‐ticularly helpful (“problem-went-away”), but avoiding a predetermined list and allowing teams to find their own preferences and standards will result in a more use‐ful tool and better data. Overall, tags have been a remarkably powerful tool for teams to obtain and provide an overview of a given service’s pain points, even without much, or even any, formal analysis. As trivial as tagging appears, it is probably one of the Outalator’s most useful unique features.180  |  Chapter 16: Tracking Outages
Analysis
Of course, SRE does much more than just react to incidents. Historical data is useful when one is responding to an incident—the question “what did we do last time?” is always a good starting point. But historical information is far more useful when it concerns systemic, periodic, or other wider problems that may exist. Enabling such analysis is one of the most important functions of an outage tracking tool.The bottom layer of analysis encompasses counting and basic aggregate statistics for reporting. The details depend on the team, but include information such as incidents per week/month/quarter and alerts per incident. The next layer is more important, and easy to provide: comparison between teams/services and over time to identify first patterns and trends. This layer allows teams to determine whether a given alert load is “normal” relative to their own track record and that of other services. “That’s the third time this week” can be good or bad, but knowing whether “it” used to hap‐pen five times per day or five times per month allows interpretation.The next step in data analysis is finding wider issues, which are not just raw counts but require some semantic analysis. For example, identifying the infrastructure com‐ponent causing most incidents, and therefore the potential benefit from increasing the stability or performance of this component,2 assumes that there is a straightfor‐ward way to provide this information alongside the incident records. As a simple example: different teams have service-specific alert conditions such as “stale data” or“high latency.” Both conditions may be caused by network congestion leading to data‐base replication delays and need intervention. Or, they could be within the nominal service level objective, but are failing to meet the higher expectations of users. Exam‐ining this information across multiple teams allows us to identify systemic problems and choose the correct solution, especially if the solution may be the introduction of more artificial failures to stop over-performing.Reporting and communication
Of more immediate use to frontline SREs is the ability to select zero or more outala‐tions and include their subjects, tags, and “important” annotations in an email to the next on-call engineer (and an arbitrary cc list) in order to pass on recent state between shifts. For periodic reviews of the production services (which occur weekly for most teams), the Outalator also supports a “report mode,” in which the important2 On the one hand, “most incidents caused” is a good starting point for reducing the number of alerts triggered and improving the overall system. On the other hand, this metric may simply be an artifact of over-sensitive monitoring or a small set of client systems misbehaving or themselves running outside the agreed service level. And on the gripping hand, the number of incidents alone gives no indication as to the difficulty to fix or severity of impact.Outalator  |  181
annotations are expanded inline with the main list in order to provide a quick over‐view of lowlights.
Unexpected BenefitsBeing able to identify that an alert, or a flood of alerts, coincides with a given other outage has obvious benefits: it increases the speed of diagnosis and reduces load on other teams by acknowledging that there is indeed an incident. There are additional nonobvious benefits. To use Bigtable as an example, if a service has a disruption due to an apparent Bigtable incident, but you can see that the Bigtable SRE team has not been alerted, manually alerting the team is probably a good idea. Improved cross-team visibility can and does make a big difference in incident resolution, or at least in incident mitigation.Some teams across the company have gone so far as to set up dummy escalator con‐figurations: no human receives the notifications sent there, but the notifications appear in the Outalator and can be tagged, annotated, and reviewed. One example for this “system of record” use is to log and audit the use of privileged or role accounts (though it must be noted that this functionality is basic, and used for technical, rather than legal, audits). Another use is to record and automatically annotate runs of peri‐odic jobs that may not be idempotent—for example, automatic application of schema changes from version control to database systems.182  |  Chapter 16: Tracking Outages
CHAPTER 17
Testing for Reliability
Written by Alex Perry and Max Luebbe Edited by Diane Bates
If you haven’t tried it, assume it’s broken.
—UnknownOne key responsibility of Site Reliability Engineers is to quantify confidence in the systems they maintain. SREs perform this task by adapting classical software testing techniques to systems at scale.1 Confidence can be measured both by past reliability and future reliability. The former is captured by analyzing data provided by monitor‐ing historic system behavior, while the latter is quantified by making predictions from data about past system behavior. In order for these predictions to be strong enough to be useful, one of the following conditions must hold:• The site remains completely unchanged over time with no software releases or changes in the server fleet, which means that future behavior will be similar to past behavior.
• You can confidently describe all changes to the site, in order for analysis to allow 	for the uncertainty incurred by each of these changes.1 This chapter explains how to maximize the value derived from investing engineering effort into testing. Once an engineer defines suitable tests (for a given system) in a generalized way, the remaining work is common across all SRE teams and thus may be considered shared infrastructure. That infrastructure consists of a scheduler (to share budgeted resources across otherwise unrelated projects) and executors (that sandbox test binaries to prevent them from being considered trusted). These two infrastructure components can each be considered an ordinary SRE-supported service (much like cluster scale storage), and therefore won’t be dis‐cussed further here.183
Testing is the mechanism you use to demonstrate specific areas of equivalence when changes occur.2 Each test that passes both before and after a change reduces the uncertainty for which the analysis needs to allow. Thorough testing helps us predict the future reliability of a given site with enough detail to be practically useful.The amount of testing you need to conduct depends on the reliability requirements for your system. As the percentage of your codebase covered by tests increases, you reduce uncertainty and the potential decrease in reliability from each change. Ade‐quate testing coverage means that you can make more changes before reliability falls below an acceptable level. If you make too many changes too quickly, the predicted reliability approaches the acceptability limit. At this point, you may want to stop making changes while new monitoring data accumulates. The accumulating data sup‐plements the tested coverage, which validates the reliability being asserted for revised execution paths. Assuming the served clients are randomly distributed [Woo96], sampling statistics can extrapolate from monitored metrics whether the aggregate behavior is making use of new paths. These statistics identify the areas that need bet‐ter testing or other retrofitting.Relationships Between Testing and Mean Time to Repair
Passing a test or a series of tests doesn’t necessarily prove reliability. However, tests that are failing generally prove the absence of reliability.
A monitoring system can uncover bugs, but only as quickly as the reporting pipeline can react. The Mean Time to Repair (MTTR) measures how long it takes the opera‐tions team to fix the bug, either through a rollback or another action.It’s possible for a testing system to identify a bug with zero MTTR. Zero MTTR occurs when a system-level test is applied to a subsystem, and that test detects the exact same problem that monitoring would detect. Such a test enables the push to be blocked so the bug never reaches production (though it still needs to be repaired in the source code). Repairing zero MTTR bugs by blocking a push is both quick and convenient. The more bugs you can find with zero MTTR, the higher the Mean Time Between Failures (MTBF) experienced by your users.As MTBF increases in response to better testing, developers are encouraged to release features faster. Some of these features will, of course, have bugs. New bugs result in an opposite adjustment to release velocity as these bugs are found and fixed.
2 Fo 
184  r fu 
|  	rther reading on equivalence, see -.
Chapter 17: Testing for ReliabilityAuthors writing about software testing largely agree on what coverage is needed. Most conflicts of opinion stem from conflicting terminology, differing emphasis on the impact of testing in each of the software lifecycle phases, or the particularities of the systems on which they’ve conducted testing. For a discussion about testing at Google in general, see [Whi12]. The following sections specify how software testing–related terminology is used in this chapter.Types of Software Testing
Software tests broadly fall into two categories: traditional and production. Traditional tests are more common in software development to evaluate the correctness of soft‐ware offline, during development. Production tests are performed on a live web ser‐vice to evaluate whether a deployed software system is working correctly.
Traditional TestsTraditional Tests
As shown in Figure 17-1, traditional software testing begins with unit tests. Testing of more complex functionality is layered atop unit tests.
Figure 17-1. The hierarchy of traditional tests
Unit tests
A unit test is the smallest and simplest form of software testing. These tests are employed to assess a separable unit of software, such as a class or function, for cor‐rectness independent of the larger software system that contains the unit. Unit tests are also employed as a form of specification to ensure that a function or moduleTypes of Software Testing  |  185