i ← − δ ∗
d∗
s (xi )
s (xi )∥2
∥∇F l
xi +1 ← xi + d∗
d ← d + d∗
i
s (xi ) − 1| C |
(cid:41)
i
6:
7:
8:
9:
(6): If initially pi − 1/| C | is too large, the step size is then
determined by gi, the gradient vector we obtained on the
current image xi. Otherwise, as pi becomes closer and closer
to 1/| C |, we will instead adjust the step size according to
pi − 1/| C |.
3.1 Enhancements
Our basic algorithm attempts to drive down the probability
of the label l with the maximum probability according Fs. We
have extended this algorithm to consider multiple labels, but
we have not implemented this enhancement. Our enhance-
ment tries to “drive down” the probability of all labels in a
set L+ and “drive up” the probability of all labels in the set
L− (presumably an adversary wants an adversarial example
whose label is in L−). Our enhanced algorithm is described in
the appendix A.
3.2 Possible Benefits in Convergence with
Newton’s Method
We now give some heuristic arguments regarding the benefits
of using Newton’s method. The main possible benefit is faster
convergence to an adversarial example. At the high level, the
main reason under the hood is actually that Newton’s method
is locally quadratically convergent to a zero x∗ when solving a
non-linear system of equations [30].
To start with, let us first consider an extreme case where,
actually, the convergence of Newton’s method does not apply
directly. Suppose that in a reasonably small neighborhood
N (x,δ ) (under some norm), we have x∗ so that F l
s (x∗) = 0.
That is, x∗ is a perfect adversarial example where we believe
s because F l
Suppose now that F l
that it is not label l with probability 1. Note that in this case x∗
is also a local minimal of F l
s is non-negative. In this
case, the gradient of F l
s is however singular (0), so we cannot
apply the convergence result of Newton’s method to conclude
s (x∗) = 0. However, a
fast convergence to x∗, to give that F l
crucial point now is that while we will not converge to x∗
exactly, at some point in this iterative process, F l
s (xk ) will be
small enough so that the network will no longer predict xk as
label l (which is the correct label), and thus gives an adversarial
example. If we let k∗ be the first iterate such that xk∗ is an
adversarial example, our goal is thus to argue fast convergence
to xk∗ (where the gradient exists but not singular).
s (xk∗ ) = p > 0, then if we instead solve
s (x ) − p = 0, then the zero point (for example xk∗) has a
that F l
non-singular gradient, and so the convergence result of New-
ton’s method applies and says that we will converge quadrati-
cally to xk∗. But this is exactly what NewtonFool algorithm
is doing, modulo picking p: Starting with the hypothesis that
nearby we have a adversarial point with very low confidence
in l, we pick a heuristic p, and apply Newton’s method to solve
the equation so that the confidence hopefully decreases to p –
If our guess is somewhat accurate, then we will quickly con-
verge to an adversarial point. Of course, the crucial problem
left is to pick the p – which we give some sufficient condition
(such as 1/| C |) to get guarantees.
4 EXPERIMENTS
In this section we present an empirical study comparing our
algorithms with previous algorithms in producing adversarial
examples. Our goals are to examine the tradeoff achieved
by these algorithms in their (1) effectiveness in producing
“indistinguishable” adversarial perturbations, and (2) efficiency
in producing respective perturbations. More specifically, the
following are three main questions we aim to answer:
(1) How effective is NewtonFool in decreasing the confidence
probability at the softmax layer for the correct label in
order to produce an adversarial example?
(2) How is quality of the adversarial examples produced by
NewtonFool compared with previous algorithms?
(3) How efficient is NewtonFool compared with previous al-
gorithms?
In summary, our findings are the following:
• (1)+(3) NewtonFool achieves a better effectiveness-efficiency
tradeoff than previous algorithms. On one hand, New-
tonFool is significantly faster than either DeepFool (up
to 49X faster) or JSMA (up to 800X faster), and is only
moderately slower than FGSM. This is not surprising
since NewtonFool does not need to examine all classes
for complex classification networks with many classes.
On the other hand, under the objective metric of Canny
edge detection, NewtonFool produces competitive and
sometimes better adversarial examples when compared
with DeepFool, and both of them produce typically sig-
nificant better adversarial examples than JSMA and
FGSM.
• (2) NewtonFool is not only effective in producing good
adversarial perturbations, but also significantly reduces
the confidence probability of the correct class. Specifically,
in our experiments not only NewtonFool successfully
finds small adversarial perturbations for all test images
we use, but also it does so by significantly reducing the
confidence probability, typically from 0.8-0.9 to 0.2-0.4.
While previous work have also shown that confidence
probability can be significantly reduced with small ad-
versarial perturbations, it is somewhat surprising that
a gradient descent procedure, constructed under an ag-
gressive assumption on the vulnerabilities of deep neural
networks against adversarial examples, can achieve sim-
ilar effects in such a uniform manner.
Two remarks are in order. First, we stress that, different
from DeepFool and JSMA where these two algorithms are
“best-effort” in the sense that they leverage first-order informa-
tion from all classes in a classification network to construct
adversarial examples, NewtonFool exploits an aggressive as-
sumption that, “nearby” the original data point, there is an-
other point where the confidence probability in the “correct
class” is significantly lower. The exploitation of this assump-
tion is similar to the structural assumption made by FGSM, yet
NewtonFool gives significantly better adversarial examples
than FGSM. Second, note that typically the training of a neural
network is gradient descent on the hypothesis space (i.e., pa-
rameters of the network) with features (training data points)
fixed. In this case, NewtonFool is “dual” in the sense that it is
a gradient descent on the feature space (i.e., training features)
with network parameters fixed. As a result of the above two
points, we believe that the success of NewtonFool in produc-
ing adversarial examples gives a more explicit demonstration
of vulnerabilities of deep neural networks against adversarial
examples than previous algorithms.
In the rest of this section we give more details of our ex-
periments. We begin by presenting experimental setup in Sec-
tion 4.1. Then in Section 4.2 we provide experimental results
on the effectiveness of NewtonFool. Finally in Section 4.3 and
Section 4.4 we report perturbation quality and efficiency of
NewtonFool compared with previous algorithms.
4.1 Experimental Setup
The starting point of our experiments is the cleverhans project
by Papernot et al. [27], which implements FGSM, JSMA and a
convolutional neural network (CNN) architecture for study-
ing adversarial examples. We reuse their implementations
of FGSM and JSMA, and the network architecture in our ex-
periments to ensure fair comparisons. Our algorithms are
implemented as extensions of cleverhans4 using Keras and
TensorFlow. We also ported the publicly available implemen-
tation of DeepFool [23] to cleverhans. All the experiments
are done on a system with Intel Xeon E5-2680 2.50GHz CPUs
(48-core), 64GB RAM, and CUDA acceleration using NVIDIA
Tesla K40c.
Datasets. In our experiments we considered two datasets.
4cleverhans is a software library that provides standardized reference implemen-
tations of adversarial example construction techniques and adversarial training
and can be found at https://github.com/tensorflow/cleverhans.
MNIST. MNIST dataset consists of 60,000 grayscale images of
hand-written digit. There are 50,000 images for training and
10,000 images for testing. Each image is of size 28 × 28 pixels,
and there are ten labels {0, . . . ,9} for classification.
GTSRB. GTSRB dataset consists of colored images of traffic
signs, with 43 different classes. There are in total 39,209 images
for training and 12,630 images for testing. In official GTSRB
dataset, image sizes vary from 15 × 15 to 250 × 250 pixels, and
we use the preprocessed dataset containing all images rescaled
to size 32 × 32.
CNN Architecture. The CNN we used in experiments for
both datasets is the network architecture defined in cleverhans.
This networks has three consecutive convolutional layers with
ReLu activation, followed by a fully connected layer outputting
softmax function values for each classes. We train a CNN
classifier achieving 94.14% accuracy for MNIST testset after
6-epoch training, and another CNN classifier achieving 86.12%
accuracy for GTSRB testset after 100-epoch training.
Experimental Methodology. Since JSMA and DeepFool take
too long to finish on the entire MNIST and GTSRB datasets, we
use sampling based method for evaluation. More specifically,
for MNIST we run ten independent experiments, where in
each experiment we randomly choose 50 images, for each of
the ten labels in {0, . . . ,9}. We do similar things for GTSRB
except that we choose 20 images at random for each of the
6 chosen classes among 43 possible classes. Those 6 classes
were intentionally chosen to include diverse shapes, different
colors, and varying complexity of embedded figures, of traffic
signs.
s (x0) − F l
With the sampled images, we then attack each CNN classi-
fier using different adversarial perturbation algorithms , and
evaluate the quality of perturbation and efficiency. Specifically:
(1) For NewtonFool, given data input x0 where it is classified
as l, F l
s (x0 +d) in order to evaluate the effectiveness
of NewtonFool in reducing confidence probability in class l.
(2) To evaluate the perturbation quality, we use three algo-
rithms (Canny edge detector, FFT, and HOG) to evaluate the
quality of the perturbed example.
(3) Finally we record running time of the attacking algorithm
in order to evaluate efficiency.
Tuning of Different Adversarial Perturbation Algorithms.
Finally, we describe how we tune different adversarial pertur-
bation algorithms in order to ensure a fair comparison.
FGSM. To find the optimal value for ϵ minimizing the final
perturbation ∆, we iteratively search from 0 to 10, increasing
by 10−3. We use the first ϵ achieving adversarial perturbation
to generate the adversarial sample. Searching time for the best
ϵ value is also counted towards the running time.
JSMA. Since target label l must be specified in JSMA, we try
all possible targets, and then choose the best result that the
adversarial example achieves the minimum number of pixel
changes. For each trial, we use fixed parameters ϒ = 14.5%,
θ = 1, which are valued used in [28]. We always increase the
pixel values for perturbations. For all target labels, attack times
are counted to the running time.
DeepFool. We fix η = 0.02, which is the value used in [23] to
adjust perturbations in each step.
NewtonFool. We fix η = 0.01, which is the parameter used
to determine the step size δi /∥∇F l
s (xi )∥2 in gradient descent
steps.
4.2 Effectiveness of NewtonFool
This section reports results in evaluating the effectiveness
of NewtonFool. We do so by measuring (1) success rate of
generating adversarial examples, and (2) changes in CNN clas-
sifier’s confidence in its classification. Table 1 summarizes
results for both. Specifically, column 5 and 6 gives the total
number of images we use to test and the total number of
successful attacks. In our experiments, NewtonFool achieves
perfect success rate. Column 4 gives the success probability
if ones makes a uniformly random guess, which is the value
NewtonFool algorithm aims to achieve in theory. Column 2
and 3 gives results on the reduction of confidence before and
after attacks, respectively, for both MNIST and GTSRB. We see
that in practice while the confidence we end at is larger than
1/| C |, NewtonFool still significantly reduce the confidence
at the softmax layer, typically from “almost sure” (0.8-0.9) to
“not sure” (0.2-0.4), while succeeding to produce adversarial
perturbations.
4.3 Quality of Adversarial Perturbation
Now we evaluate the quality of perturbations using an ob-
jective metric: Recall that an objective metric measures the
quality of perturbation independent of the optimization objec-
tive, and thus serves a better role in evaluating to what degree
a perturbation is “indistinguishable.”. Specifically, in our exper-
iments we use the classic techniques of computer vision, as the
objective metrics: Canny edge detection, fast Fourier transform,
and histogram of oriented gradients.
Given an input image, Canny edge detection finds a wide
range of edges in an image, which is a critical task in computer
vision. We use Canny edge detection in the following way: We
count the number of edges detected by the detector and use
that as a metric for evaluation. Intuitively, an indistinguish-
able perturbation should maintain the number of edges after
perturbation very close to the original image. Therefore, by
measuring how close this count is to the count on the original
image, we have a metric on the quality of the perturbation:
Smaller the metric, better the perturbation. Note that there are
many edge detection methods. We chose Canny edge detection
since it is one of the most popular and reliable edge detection
algorithms.
Discrete Fourier transform maps images to the two dimen-
sional spartial domain, allowing us to analyse the perturba-
tions according to their spectra. When analysing an image with
its spectrum of spatial frequencies, higher frequency corre-
sponds to feature details and abrupt change of values, whereas
lower frequency corresponds to global shape information. As
adversarial perturbations does not change the general shape
but corrupt detailed features of the input image, measuring
the size of the high frequency part of the spectrum is desir-
able as the metric for feature corruption. Therefore, we first
Dataset
MNIST
GTSRB
Confidence be-
fore attack
0.926 (0.135)
0.896 (0.201)
Confidence af-
ter attack
0.400 (0.065)
0.245 (0.102)
1/|C|
0.1
0.023
Number of attacked
samples
5000
1200
Number of successful
attacks
5000
1200
Table 1: Confidence reduction and success rate of NewtonFool. Column 2 and 3 gives results on the reduction of
confidence before and after attacks. Column 4 gives the success probability if one makes a uniformly random guess,
which is the value NewtonFool algorithm aims to achieve in theory. Column 5 and 6 gives the success rate results.
compute the Fourier transform of the perturbation, discard
the values lies in the low frequency part, then measure the
l2 norm difference of the remaining part as a metric: Smaller
metric implies that less feature corruption has been induced
by the adversarial perturbation. For the low frequency part
to be discarded, we chose the intersection of lower halves of
the frequency domain along each dimensions (horizontal and
vertical).
Object detection with HOG descriptor is done by sweep-
ing a large image with a fixed size window, computing the
HOG descriptor of the image restricetd in the window, and
using a machine learning algorithm (e.g. SVM) to classify the
descriptor vector as “detected” and “not detected”. That is, if
a perturbed image has HOG descriptor relatively close to the
HOG descriptor of the original image, then the detecting algo-
rithm is more likely to detect the perturbed image whenever
the original image is detected. From this, we suggest another
objective metric that measures the l2 norm difference between
two HOG descriptor vectors: One computed from the original
image and the other from the perturbed image. The perturba-
tions resulting smaller HOG vector difference are considered
to have better quality.
In summary, in our experiments we find that: (1) Among all
algorithms DeepFool and NewtonFool generally produce the
best results, and typically are significantly better than FGSM
and JSMA. (2) Further, DeepFool and NewtonFool are stable
on both datasets and both of them give good results on both
datasets. On the other hand, while FGSM performs relatively
well on MNIST, it gives poor results on GTSRB, and JSMA
performs the opposite: It performs well on GTSRB, but not
on MNIST. (3) Finally, for all tests we have, NewtonFool gives
competitive and sometimes better results than DeepFool. In
the rest of this section we give detailed statistics. Results for
HOG are provided in the appendix B.
Table 2 and Table 3 reports the number of edges we find on
MNIST and GTSRB, respectively. Each column gives the results
on the class represented by the image at the top, and each row
presents the mean and standard deviation of the number of
detected edges. Specifically, the first row gives the statistics
on the original image, and the other rows gives the statistics
on the adversarial examples found by different adversarial
perturbation algorithms.
On MNIST (Table 2), FGSM, DeepFool and NewtonFool
give similar results, where the statistics is very close to the
statistics on the original images. DeepFool and NewtonFoolare
especially close. On the other hand, we note that JSMA pro-
duces significantly more edges compared to other methods.
The result suggests that perturbations from JSMA can be more
perceivable than perturbations produced by other algorithms.
On GTSRB dataset, the situation changes a little. Now FGSM
produces the worst results among all algorithms (instead of JSMA),
and it produces significantly more edges been detected on
its produced adversarial examples. While the quality of ad-
versarial examples produced by JSMA improves significantly,
it is still slightly worse than those produced by DeepFool
and NewtonFool in most cases (except for the last sign). Inter-
estingly, NewtonFool gives the best result for all signs (again,
except the last one).
Table 4 and Table 5 presents the distance on the domain of
high spatial frequency we computed on MNIST and GTSRB,
respectively. Again, each column gives the results on the class
represented by the image at the top, and each row presents the
mean and standard deviation of the hight frequency distances.
For each row, we first generated adversarial perturbations
using the algorithm of the row, computed the fast Fourier
transform of the perturbation, then computed the l2 norm of
the high spatial frequency part.
On both of MNIST dataset and GTSRB dataset, DeepFool