and we partition them into 361 sets with 100 objects per
set. Each set thus roughly corresponds to a day in this
one-year dataset. Since digg does not have negative votes,
for simplicity, we assume all of the objects to be good.
(Additionally, most of them are voted for by at least one
guide already.) We construct one round corresponding to
each set. Each round contains all the 100 good objects in the
Figure 6. Dimension resulting from Pareto.
not have excessive correlation among themselves). Based on
this thesis, instead of directly validating small dimensions,
we will validate the following sufﬁcient (but not necessary)
conditions for small dimensions:
1) The vote distribution is Pareto, and
2) A Pareto vote distribution usually implies small dimen-
sion.
We will use the four datasets in Table 3 to validate the
ﬁrst statement, and use simulation/analysis to validate the
second one. Notice that such a decomposition can be
powerful—even if one rather pessimistically dismisses the
representativeness of all our datasets, as long as the heavy-tail
distribution (which is rather universal/fundamental) applies,
the dimension will likely to be small. Additionally, because
we are concerned only with the “heavy-tailness” of the vote
distribution from the honest identities, we do not expect
the (byzantine) votes from the sybil identities to affect such
“heavy-tailness”.
Is vote distribution Pareto? Figures 5(b) through 5(e) ﬁt
the four datasets to Pareto, with b values ranging from 1.08
to 1.73. The ﬂat components at the tails are due to the ﬁnite
number of users in the datasets. We ignore the ﬂat parts
when ﬁtting to Pareto. This makes the tails “lighter” (i.e., b
larger) and makes our later results pessimistic. The ﬁgure
shows that all datasets except netflix have quite good
ﬁt. Pareto does not ﬁt netflix well at the beginning. But
since dimension is mainly determined by the “tail” portion,
we do not expect the beginning portion to be critical.
Does Pareto imply small dimension? We construct syn-
thetic datasets with synthetic voting patterns for the guides
based on given Pareto distributions, and then determine the
296
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:54 UTC from IEEE Xplore.  Restrictions apply. 
1e-61e-40.011.01041001(a) digg0.95x-1.54digg1e-61e-40.011.01041001(b) dotnetkicks0.30x-1.28dotnetkicks1e-60.011e-41.01041001(c) netflix8.04x-1.73netflix1e-61e-40.011.01041001(d) movielens0.42x-1.08movielens1e-61e-40.011.01041001(e) bookcrossing0.25x-1.57bookcrossing 0 5 10 15 20 25 301.81.61.41.21.00.6-fractional dimensionthe b parameter in Pareto0.01 popularity0.02 popularity0.05 popularity0.1 popularityused DSybil for some time. (We still consider the worst-case
attack, and here the worst-case attack means the attack that
incurs the largest loss among all attacks that start after some
initial attack-free rounds.) For this experiment, our DSybil
toolkit ﬁrst processes the dataset for some number of attack-
free rounds, and then outputs the total loss so far, together
with the trust values of all identities. These values, together
with the dimension of the remaining rounds, enables us to
use Equations 6 and 8 to bound the loss under the worst-case
attack. To show the robustness of the results against different
voting patterns of the non-guides, we perturb their votes in
digg in various ways. The following results are under the
perturbation (see [63] for details) yielding the largest loss.
Figure 7 illustrates Alice’s per-round loss when the attack
starts after Alice has used DSybil for a given number of days
(each day corresponds to 20 rounds). The results show that
DSybil’s growing defense is rather prominent. Even if Alice
has used DSybil for only one day before the attack starts, her
per-round loss dramatically drops from around 12% to 6%
(under M = 1010). If Alice has used DSybil for a month, her
per-round loss further drops to around 4%. Assuming that an
average honest user’s lifespan (of using DSybil) is one year,
and assuming that the attack starts at a random point of time,
364/365 ≈ 99.7% of the users will have used DSybil for a
day when the attack starts. We have also experimented with
higher popularity values. For example, for 0.1 popularity
and M = 1010, the per-round loss will be 0.70% if the attack
starts after day 1.
Figure 7 further shows that with some initial attack-free
rounds, Alice’s loss becomes less sensitive to M. This is
within expectation. Namely, larger M will result in smaller
seed trust for the critical guides. But the initial attack-free
rounds are likely to have already assigned seed trust (whose
value is independent of M) to most of the critical guides.
Finally, we refer the reader to [63] for DSybil’s loss under
some individual attack strategies (purely as examples). There
the per-round loss ranges from 1% (for M = 103) to 5% (for
M = 1010).
9. Conclusion
This paper presented DSybil, a novel defense for dimin-
ishing the inﬂuence of sybil identities in recommendation
systems. DSybil has provable guarantees that are optimal.
Our evaluation showed that DSybil’s loss would remain small
even under a potential sybil attack launched from a million-
node botnet.
Acknowledgment. We thank Avrim Blum, Wee Sun Lee,
Siddhartha Srinivasa, and Nathan Ratliff for very helpful
discussions on related issues in the machine learning context.
We thank Brad Karp for useful discussions about attacks
on Digg and Wikipedia. We thank Avrim Blum and the
anonymous reviewers for helpful comments on this paper.
Figure 7. Alice’s per-round loss when the attack starts after
Alice has used DSybil for a given number of days (assuming
the worst-case attack).
set, and additionally another 100 bad objects. We assume
that Alice wants to consume 20 objects in each round (i.e.,
read 20 news stories every day). We model this as 20 rounds
where Alice consumes one object per round (as described in
Section 5.3).
We use the voting pattern of the guides in digg to
determine D f . Other information in the dataset, such as
the voting patterns of the non-guides, is irrelevant. Similarly,
we do not need to specify the voting patterns of the sybil
identities or the votes on the bad objects, since Equation 8
holds under all possibilities. We plug D f into Equation 8
with u = 200, p = 0.5, v = 20, and W = 30,720 (observed
from the dataset). Since Equation 8 holds for ∀ f > v
u·p = 0.2,
we compute the bound on E[L] using the f value (> 0.2)
that minimizes the bound. Notice that DSybil’s expected loss
(E[L]) under the worst-case attack is already ﬁxed given the
digg dataset. But precisely determining E[L] is challenging.
By varying the f value in Equation 8, we are simply ﬁnding
the best upper bound for E[L] (i.e., the upper bound that is
the tightest). DSybil itself does not need to search for such
f because f is not a parameter in DSybil.
DSybil’s loss under the worst-case attack. The “attack after
day 0” curve in Figure 7 illustrates how Alice’s per-round
loss changes with M. For this scenario, per-round loss is the
fraction of bad new stories recommended to (and read by)
Alice. The ﬁgure shows that the per-round loss is below 6%
under M = 1000, and is only around 12% even if M reaches
10 billion. Putting it another way, even if individual objects
have up to 1010 votes from the sybil identities, only roughly
2.4 objects are bad out of the 20 objects recommended by
DSybil every day. In comparison, an average good object
only has 12 votes from the guides and 1,239 votes from
the guides and non-guides combined. Section 7 showed that
a million-node botnet will likely be needed to cause M to
reach 1010. Higher popularity values for Alice will result in
smaller loss. For example, for 0.1 popularity and M = 1010,
the per-round loss is 3.8% (not shown in the ﬁgure).
Growing defense. Next, we illustrate the growing defense
of DSybil, when the adversary starts attacking after Alice has
297
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:54 UTC from IEEE Xplore.  Restrictions apply. 
0.000.050.100.150.200.251010108106104per-round lossmax # of sybil voters on any objectattack start after day 0attack start after day 1attack start after day 7attack start after day 30This work is partly supported by NUS Young Investigator
Award R-252-000-334-123.
References
[1] B. Acohido and J. Swartz. Botnet scams are exploding. March 16, 2008,
USA Today. http://www.usatoday.com/tech/news/computersecurity/
2008-03-16-computer-botnets N.htm.
[2] N. Alon, B. Awerbuch, Y. Azar, and B. Patt-Shamir. Tell me who I
am: An interactive recommendation system. In ACM SPAA, 2006.
[3] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The
nonstochastic multiarmed bandit problem. SIAM J. Comput., 32(1),
2003.
[4] B. Awerbuch, Y. Azar, Z. Lotker, B. Patt-Shamir, and M. R. Tuttle.
Collaborate with strangers to ﬁnd own preferences. In ACM SPAA,
2005.
[5] B. Awerbuch and T. P. Hayes. Online collaborative ﬁltering with
nearly optimal dynamic regret. In ACM SPAA, 2007.
[6] B. Awerbuch and R. D. Kleinberg. Competitive collaborative learning.
J. Comp. Sys. Sci., 74(8), 2008.
[7] B. Awerbuch, A. Nisgav, and B. Patt-Shamir. Asynchronous active
recommendation systems. In OPODIS, 2007.
[8] B. Awerbuch, B. Patt-Shamir, D. Peleg, and M. Tuttle. Collaboration of
untrusting peers with changing interests. In ACM Electronic Commerce,
2004.
[9] B. Awerbuch, B. Patt-Shamir, D. Peleg, and M. Tuttle. Adaptive
collaboration in peer-to-peer systems. In ICDCS, 2005.
[10] B. Awerbuch, B. Patt-Shamir, D. Peleg, and M. Tuttle.
Improved
recommendation systems. In ACM-SIAM SODA, 2005.
[11] B. Awerbuch and C. Scheideler. Towards a scalable and robust DHT.
In ACM SPAA, 2006.
[12] R. Bazzi and G. Konjevod. On the establishment of distinct identities
in overlay networks. In ACM PODC, 2005.
[13] A. Blum. Empirical support for winnow and weighted-majority
algorithms: Results on a calendar scheduling domain. Machine
Learning, 26(1), 1997.
Learning Research, 8, 2007.
[14] A. Blum and Y. Mansour. From external to internal regret. J. Machine
[15] http://www.informatik.uni-freiburg.de/∼cziegler/BX/.
[16] N. Borisov. Computational puzzles as sybil defenses. In IEEE P2P,
[17] M. Castro, P. Druschel, A. Ganesh, A. Rowstron, and D. S. Wallach.
Secure routing for structured peer-to-peer overlay networks. In USENIX
OSDI, 2002.
[18] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games.
Cambridge University Press, 2006.
[19] A. Cheng and E. Friedman. Sybilproof reputation mechanisms. In
ACM P2PEcon, 2005.
[20] P.-A. Chirita, W. Nejdl, and C. Zamﬁr. Preventing shilling attacks in
online recommender systems. In ACM WIDM, 2005.
[21] E. Damiani, D. C. di Vimercati, S. Paraboschi, P. Samarati, and
F. Violante. A reputation-based approach for choosing reliable
resources in peer-to-peer networks. In ACM CCS, 2002.
[22] M. Dell’Amico and L. Capra. Soﬁa: Social ﬁltering for robust
recommendations. In IFIPTM, 2008.
[23] http://www.digg.com/.
[24] An interview with Digg top user.
http://www.invesp.com/blog/
social-media/an-interview-with-digg-top-user.html, 2008.
[25] C. Dixon, T. Anderson, and A. Krishnamurthy. Phalanx: Withstanding
multimillion-node botnets. In USENIX NSDI, 2008.
[26] J. Douceur. The Sybil attack. In IPTPS, 2002.
[27] P. Drineas, I. Kerenidis, and P. Raghavan. Competitive recommendation
systems. In ACM STOC, 2002.
[28] M. Feldman, K. Lai, I. Stoica, and J. Chuang. Robust incentive
techniques for peer-to-peer networks. In ACM Electronic Commerce,
2004.
[29] A. Fiat, J. Saia, and M. Young. Making Chord robust to byzantine
attacks. In ESA, 2005.
[30] Y. Freund, R. E. Schapire, Y. Singer, and M. K. Warmuth. Using and
combining predictors that specialize. In ACM STOC, 1997.
[31] http://www.stealthfriendbomber.com/.
2006.
[32] S. Ganeriwal, L. K. Balzano, and M. B. Srivastava. Reputation-based
framework for high integrity sensor networks. ACM Trans. on Sensor
Networks, 4(3), 2008.
[33] http://www.grouplens.org/.
[34] K. Hoffman, D. Zage, and C. Nita-Rotaru. A survey of attack and
defense techniques for reputation systems. Technical report, Purdue
Univ., 2007. http://www.cs.purdue.edu/homes/zagedj/docs/reputation
survey.pdf.
[35] J. Hopcroft and D. Sheldon. Manipulation-resistant reputations using
hitting time. In WAW, 2007.
[36] S. Kamvar, M. Schlosser, and H. Garcia-Molina. The eigentrust
In WWW,
algorithm for reputation management in P2P networks.
2003.
[37] A. Kast.
The digg recommendation engine.
http://digg.com/
whitepapers/recommendationengine, 2008.
[38] R. Kleinberg, A. Niculescu-Mizil, and Y. Sharma. Regret bounds for
sleeping experts and bandits. In ACM COLT, 2008.
[39] R. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins. Recommen-
dation systems: A probabilistic analysis. In IEEE FOCS, 1998.
[40] S. Lam and J. Riedl. Shilling recommender systems for fun and proﬁt.
In WWW, 2004.
[41] H. H. Lee, E.-C. Chang, and M. C. Chan. Pervasive random beacon
in the internet for covert coordination. In Information Hiding, 2005.
[42] T. Leighton and S. Rao. An approximate max-ﬂow min-cut theorem
for uniform multicommodity ﬂow problems with applications to
approximation algorithms. In FOCS, 1988.
[43] J. Liang, R. Kumar, Y. Xi, and K. W. Ross. Pollution in P2P ﬁle
sharing systems. In IEEE INFOCOM, 2005.
[44] A. Mislove, A. Post, K. Gummadi, and P. Druschel. Ostra: Leveraging
trust to thwart unwanted communication. In USENIX NSDI, 2008.
[45] M. Mitzenmacher and E. Upfal. Probability and Computing. Cam-
bridge University Press, 2005.
[46] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams. Toward
trustworthy recommender systems: An analysis of attack models and
algorithm robustness. ACM Trans. on Internet Technology, 7(4), 2007.
[47] A. Nakamura. Learning specialist decision lists. In ACM COLT, 1999.
[48] http://www.netﬂixprize.com/.
[49] M. O’Mahony, N. Hurley, N. Kushmerick, and G. Silvestre. Collabora-
tive recommendation: A robustness analysis. ACM Trans. on Internet
Technology, 4(4), 2004.
[50] Microsoft wants to double the number of PCs in the world by
http://www.informationweek.com/news/windows/microsoft
2015.
news/showArticle.jhtml?articleID=199200360, 2007.
[51] M. A. Rajab, J. Zarfoss, F. Monrose, and A. Terzis. My botnet is
bigger than yours (maybe, better than yours): Why size estimates
remain challenging. In USENIX HotBots, 2007.
[52] http://razor.sourceforge.net/.
[53] P. Resnick and R. Sami. The inﬂuence limiter: Provably manipulation-
resistant recommender systems. In ACM RecSys, 2007.
[54] P. Resnick and R. Sami. The information cost of manipulation-
resistance in recommender systems. In ACM RecSys, 2008.
[55] M. Richardson, R. Agrawal, and P. Domingos. Trust management for
the semantic web. In SWSA ISWC, 2003.
[56] H. Rowaihy, W. Enck, P. Mcdaniel, and T. La-Porta. Limiting sybil
attacks in structured peer-to-peer networks. In INFOCOM, 2007.
[57] N. Tran, B. Min, J. Li, and L. Subramanian. Sybil-resilient online
content voting. In USENIX NSDI, 2009.
[58] http://www.tubeautomator.com/.
[59] L. von Ahn, M. Blum, N. J. Hopper, and J. Langford. CAPTCHA:
Telling humans and computers apart. In IACR Eurocrypt, 2003.
[60] K. Walsh and E. G. Sirer. Experience with an object reputation system
for peer-to-peer ﬁlesharing. In USENIX NSDI, 2006.
[61] H. Yu, P. B. Gibbons, M. Kaminsky, and F. Xiao. SybilLimit: A
near-optimal social network defense against sybil attacks. In IEEE
S&P, 2008.
[62] H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman. SybilGuard:
IEEE/ACM
Defending against sybil attacks via social networks.
Transactions on Networking, 16(3), June 2008.
[63] H. Yu, C. Shi, M. Kaminsky, P. B. Gibbons, and F. Xiao. DSybil:
Optimal Sybil-Resistance for Recommendation Systems. Technical
report, National Univ. of Singapore, 2009. Available at http://www.
comp.nus.edu.sg/∼yuhf/09yuoaklandtr.pdf.
298
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:54 UTC from IEEE Xplore.  Restrictions apply.