title:SecureMR: A Service Integrity Assurance Framework for MapReduce
author:Wei Wei and
Juan Du and
Ting Yu and
Xiaohui Gu
2009 Annual Computer Security Applications Conference
SecureMR: A Service Integrity Assurance
Framework for MapReduce
Wei Wei, Juan Du, Ting Yu, Xiaohui Gu
Department of Computer Science, North Carolina State University
Raleigh, North Carolina, United States
{wwei5,jdu}@ncsu.edu, {gu,yu}@csc.ncsu.edu
Abstract—MapReduce has become increasingly popular as a
powerful parallel data processing model. To deploy MapReduce
as a data processing service over open systems such as service
oriented architecture, cloud computing, and volunteer computing,
we must provide necessary security mechanisms to protect the
integrity of MapReduce data processing services. In this paper,
we present SecureMR, a practical service integrity assurance
framework for MapReduce. SecureMR consists of ﬁve security
components, which provide a set of practical security mechanisms
that not only ensure MapReduce service integrity as well as to
prevent replay and Denial of Service (DoS) attacks, but also
preserve the simplicity, applicability and scalability of MapRe-
duce. We have implemented a prototype of SecureMR based
on Hadoop, an open source MapReduce implementation. Our
analytical study and experimental results show that SecureMR
can ensure data processing service integrity while imposing low
performance overhead.
I. INTRODUCTION
MapReduce is a parallel data processing model, proposed
by Google to simplify parallel data processing on large clus-
ters [1]. Recently, many organizations have adopted the model
of MapReduce, and developed their own implementations of
MapReduce, such as Google MapReduce [1] and Yahoo’s
Hadoop [2], as well as thousands of MapReduce applications.
Moreover, MapReduce has been adopted by many academic
researchers for data processing in different research areas,
such as high end computing [3], data intensive scientiﬁc
analysis [4], large scale semantic annotation [5] and machine
learning [6].
Current data processing systems using MapReduce are
mainly running on clusters belonging to a single administration
domain. As open systems, such as Service-Oriented Architec-
ture (SOA) [7], [8], Could Computing [9] and Volunteer Com-
puting [10], [11], increasingly emerge as promising platforms
for cross-domain resource and service integration, MapReduce
deployed over open systems will become an attractive solution
for large-scale cost-effective data processing services. As a
forerunner in this area, Amazon deploys MapReduce as a
web service using Amazon Elastic Compute Cloud (EC2) and
Amazon Simple Storage Service (Amazon S3). It provides a
public data processing service for researchers, data analysts,
and developers to efﬁciently and cost-effectively process vast
amounts of data [12]. However, in open systems, besides
communication security threats such as eavesdropping attacks,
replay attacks, and Denial of Service (DoS) attacks, MapRe-
duce faces a data processing service integrity issue since
service providers in open systems may come from different
administration domains that are not always trustworthy.
Several existing techniques such as replication (also known
as double-check), sampling, and checkpoint-based veriﬁcation
have been proposed to address service integrity issues in
different computing environments like Peer-to-Peer Systems,
Grid Computing, and Volunteer Computing (e.g., [13]–[19]).
Replication-based techniques mainly rely on redundant com-
putation resources to execute duplicated individual tasks, and
a master (also known as supervisor) to verify the consistency
of results. Sampling techniques require indistinguishable test
samples. The checkpoint-based veriﬁcation focuses on sequen-
tial computations that can be broken into multiple temporal
segments.
In this paper, we present SecureMR, a practical service
integrity assurance framework for MapReduce. SecureMR
provides a decentralized replication-based integrity veriﬁca-
tion scheme for ensuring the integrity of MapReduce in
open systems. Our scheme leverages the unique properties
of the MapReduce system to achieve effective and practical
security protection. First, MapReduce provides natural redun-
dant computing resources, which is amenable to replication-
based techniques. Moreover, the parallel data processing of
MapReduce mitigates the performance inﬂuence of executing
duplicated tasks. However, in contrast to simple monolithic
systems, MapReduce often consists of many distributed com-
puting tasks processing massive data sets, which presents new
challenges to adopt replication-based techniques. For example,
it is impractical to replicate all distributed computing tasks for
consistency veriﬁcation purposes. Moreover, it is not scalable
to perform centralized consistency veriﬁcation over massive
result data sets at a single point (e.g., the master).
To address these challenges, our scheme decentralizes the
integrity veriﬁcation process among different distributed com-
puting nodes who participate in the MapReduce computation.
Our major contributions are summarized as follows:
• We propose a new decentralized replication-based in-
tegrity veriﬁcation scheme for running MapReduce in
open systems. Our approach achieves a set of security
properties such as non-repudiation and resilience to DoS
attacks and replay attacks while maintaining the data
processing efﬁciency of MapReduce.
• We have implemented a prototype of SecureMR based on
1063-9527/09 $26.00 © 2009 IEEE
DOI 10.1109/ACSAC.2009.17
73
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:04:27 UTC from IEEE Xplore.  Restrictions apply. 
Hadoop [2], an open source implementation of MapRe-
duce. The prototype shows that
the security compo-
nents in SecureMR can be easily integrated into existing
MapReduce implementations.
• We conduct security analytical study and experimental
evaluation of performance overhead based on the proto-
type. Our analytical study and experimental results show
that SecureMR can ensure the service integrity while
imposing low performance overhead.
The rest of the paper is organized as follows. We intro-
duce the MapReduce data processing model in Section II. In
Section III, we discuss the security vulnerabilities of running
MapReduce in open systems, and state assumptions and attack
models. Section IV presents the design details of SecureMR.
Section V provides the analytical and experimental evaluation
results. Section VI compares our work with related work.
Finally, the paper concludes in Section VII.
II. BACKGROUND
As a parallel data processing model, MapReduce is designed
to run in distributed computing environments. Figure 1 depicts
the MapReduce data processing reference model in such an
environment. The data processing model of MapReduce is
composed of three types of entities: a distributed ﬁle system
(DFS), a master and workers. The DFS provides a distributed
data storage for MapReduce. The master is responsible for
job management, task scheduling and load balancing among
workers. Workers are hosts who contribute computation re-
sources to execute tasks assigned by the master. The basic
data processing process in MapReduce can be divided into
two phases: i) a map phase where input data are distributed
to different distributed hosts for parallel processing; and ii)
a reduce phase where intermediate results are aggregated
together. To illustrate the two-phase data processing model,
we use a typical example, WordCount [20] that counts how
often words occur. The application is considered as a job of
MapReduce submitted by a user to the master. The input text
ﬁles of the job are stored in the DFS in the form of data
blocks, each of which is usually 64MB. The job is divided
into multiple map and reduce tasks. The number of map tasks
depends on the number of data blocks that the input text ﬁles
have. Each map task only takes one data block as its input.
During the map phase, the master assigns map tasks to
workers. A worker is called a mapper when it is assigned a
map task. When a mapper receives a map task assignment
from the master, the mapper reads a data block from the
DFS, processes it and writes its intermediate result to its local
storage. The intermediate result generated by each mapper is
divided into r partitions P 1, P 2, ..., P r using a partitioning
function. The number of partitions is the same with the number
of reduce tasks r. During the reduce phase, the master assigns
reduce tasks to workers. A worker is called a reducer when
it is assigned a reduce task. Each reduce task speciﬁes which
partition a reducer should process. After a reducer receives a
reduce task, the reducer waits for notiﬁcations of map task
completion events from the master. Upon notiﬁed, the reducer
(cid:258)
(cid:258)
(cid:258)
(cid:258)
(cid:258)
M
M
A
B
(cid:258) (cid:258)
(cid:258) (cid:258)
(cid:258)
(cid:258)
(cid:258)
(cid:258) (cid:258)
(cid:258)
(cid:258)
(cid:258)
R
(cid:258)
(cid:258)
(cid:258)
A
Fig. 1. The MapReduce data processing reference model.
reads its partition from the intermediate result of each mapper
who ﬁnishes its map task. For example, in Figure 1, RA reads
P 1 from MA, MB and other mappers. After the reducer reads
its partition from all mappers, the reducer starts to process
them, and ﬁnally each reducer outputs its result to the DFS.
In fact, the MapReduce data processing model supports to
combine multiple map and reduce phases into a MapReduce
chain to help users accomplish complex applications that
cannot be done via a single Map/Reduce job. In a MapReduce
chain, mappers will read the output of reducers in the preced-
ing reduce phase, except mappers in the ﬁrst map phase, which
read data from the DFS. Then, the data processing enters into
the map phase with no difference from the normal map phase.
Similarly, reducers will read intermediate results from mappers
in the preceding map phase and generate outputs to DFS or
their local disks like what mappers do, which is different from
a single Map/Reduce data processing model. For reducers in
the middle of data processing, they may store their results in
their local disks to improve the overall system performance.
Eventually, the ﬁnal results go into the DFS.
III. SYSTEM MODEL
A. MapReduce in Open Systems
MapReduce can be implemented to run in either closed
systems or open systems. In closed systems, all entities belong
to a single trusted domain, and all data processing phases are
executed within this domain. There is no interaction with other
domains at all. Thus, security is not taken into consideration
for MapReduce in closed systems. However, MapReduce in
open systems presents two signiﬁcant differences:
• The entities in MapReduce come from different domains,
which are not always trusted. Furthermore, they may be
compromised by attackers due to different vulnerabilities
such as software bugs, and careless administration.
• The communications and data transferred among enti-
ties are through public networks. It is possible that the
74
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:04:27 UTC from IEEE Xplore.  Restrictions apply. 
communications are eavesdropped, or even tampered to
launch different attacks.
Therefore, before MapReduce can be deployed and operate in
open systems, several security issues need to be addressed, in-
cluding authenticity, conﬁdentiality, integrity, and availability.
In this paper, we focus on protecting the service integrity for
MapReduce. Since the data processing model of MapReduce
includes three types of entities and two phases, to provide the
service integrity protection for MapReduce, it naturally boils
down to the following three steps:
1) Provide mappers with a mechanism to examine the
integrity of data blocks from the DFS.
2) Provide reducers with a mechanism to verify the au-
thenticity and correctness of the intermediate results
generated by mappers.
3) Provide users with a mechanism to check if the ﬁnal
result produced by reducers is authentic and correct.
The ﬁrst step ensures the integrity of inputs for MapReduce
in open systems. The second step provides reducers with the
integrity assurance for their inputs. The third step guarantees
the authenticity and correctness of the ﬁnal result for users.
Finally, the combination of three ensures the MapReduce data
processing service integrity to users. Since the ﬁrst step has
been addressed by existing techniques in [21]–[23], we will
go through the rest of steps in the following sections.
B. Assumptions and Attack Models
MapReduce is composed of three types of entities: a DFS, a
master and workers. The design of SecureMR is built on top of
several assumptions that we make on these entities. First, each
worker has a public/private key pair associated with a unique
worker identiﬁer. Workers can generate and verify signatures,
and no worker can forge other’s signatures. Second, the master
is trusted and its public key is known to all, but workers are not
necessarily trusted. Third, a good worker is honest and always
returns the correct result for its task while a bad worker may
behave arbitrarily. Fourth, the DFS for MapReduce provides
data integrity protection so that each node can verify the
integrity of data read from the DFS. Fifth, if a worker is good,
then others cannot tamper its data (otherwise, the worker is
compromised and should be considered as a bad one). Since
each worker can have its own access control mechanism to
protect data from being changed by unauthorized workers, the
assumption is reasonable.
Based on the above assumptions, we concentrate on the
analysis of malicious behavior from bad workers. In open
systems, a bad worker may cheat on a task by giving a wrong
result without computation [13] or tamper the intermediate
result to mess up the ﬁnal result. Moreover, a bad worker may
launch DoS attacks against other good workers. For example,
it may keep sending requests to a good worker and asking
for intermediate results or it may impersonate the master to
send fake task assignments to workers. Furthermore, it may
initiate replay attacks against good workers by sending old task
assignments to keep them busy. In addition, it may eavesdrop
and tamper the messages exchanged between two entities so
that the ﬁnal result generated may be compromised. Here, we
classify malicious attacks into the following two models:
Non-collusive malicious behavior. Workers behave inde-
pendently, which means that bad workers do not necessarily
agree or consult with each other when misbehaving. A typical
example is that, when they return wrong results for the same
input, they may return different wrong results.
Collusive malicious behavior. Workers’ behavior depends
on the behavior of other collusive workers. They may com-
municate, exchange information, and make an agreement with
each other. For example, when they are assigned tasks by the
master, they can know if their colluders receive tasks with the
same input blocks. If so, they return the same results so that
there is no inconsistency among collusive workers. By doing
so, they try to avoid being detected even if they return wrong
results.
IV. SYSTEM DESIGN
In this section, we present
the detailed design of our
decentralized replication-based integrity veriﬁcation scheme.
A. Design Overview
SecureMR enhances the basic MapReduce framework with
a set of security components, illustrated by Figure 2. To
validate the integrity of map/reduce tasks, our basic idea is to
replicate some map/reduce tasks and assign them to different
mappers/reducers. Any inconsistent intermediate results from
those mappers/reducers reveal attacks. However, due to scal-
ability and efﬁciency reason, though the master is trusted in
our design, consistency veriﬁcation should not be carried out
only by the master. Instead, in our design, this responsibility
is further distributed among workers. Our design must ensure
properties such as non-repudiation and resilience to DoS and
replay attacks, as well as efﬁciency. Further, our design should
preserve the existing MapReduce mechanism as much as
possible so that it can be easily implemented and deployed
with current MapReduce systems. We introduce the design of
SecureMR from two aspects: architecture and communication.
Architecture Design. Figure 2(a) shows the architecture
design of SecureMR, which comprises ﬁve security compo-
nents: Secure Manager, Secure Scheduler, Secure Task Ex-
ecutor, Secure Committer and Secure Veriﬁer. They provide
a set of security mechanisms: task duplication, secure task
assignment, DoS and replay attack protection, commitment-
based consistency checking, data request authentication, and
result veriﬁcation.
Secure Manager and Secure Scheduler are deployed in a
master mainly for task duplication, secure task assignment,
and commitment-based consistency checking. Secure Task
Executor is running in both mappers and reducers to prevent
DoS and replay attacks that exploit fake or old task assign-
ments. In mappers, Secure Committer takes the responsibility
of generating commitments for the intermediate results of
mappers and sending them to Secure Manager in the master to
complete the commitment-based consistency checking. Secure
75
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:04:27 UTC from IEEE Xplore.  Restrictions apply. 
U
u
u
t
s
O
n
w
e
o
t
r
S
S
p
e
e
M
e
e
r
i
A
e
c
e
k
M
S
S
r
c
h
n
a
p
u
e
e
I
C
C
a
e
c
c
S
n
n
p
r
l
e
t
i
e
r
r
M
s
a
g
u
m
r
s
d
o
o
u
u
a
f
y
c
l
e
e
r
t
e
a
e
p
p
e
s
R
t
r
t
u
u
r
m
r
o
t
u
n
n
s
i
c
s
g
g
u