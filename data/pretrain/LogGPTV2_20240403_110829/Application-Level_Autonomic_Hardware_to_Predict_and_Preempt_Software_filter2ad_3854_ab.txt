to determine if an attack has been successful, much like
a vigilant human operator. However, these techniques are
generally reactive since they detect attacks beginning in the
past or present. The problem with reactive methods is that
the plant is already affected, and corrective action must be
taken to restore equilibrium. There is a point of no return,
formalized by the control-Lyapunov function, beyond which
the system becomes unstable. These techniques typically
observe either plant reactions to new controller inputs,
or controller responses to new sensor measurements. An
example of the former was developed by Sha [20]. In this
architecture, sensor measurements are monitored by decision
logic that determines if a process violation has occurred, as
illustrated by Fig. 1. If a violation is detected, the decision
logic switches control to a high-assurance controller until
the system is stabilized. Dai et al. described a fault detection
architecture based on observing controller responses to new
sensor inputs [21]. Plant measurements are sent to both the
production controller and a trusted benchmark version of
the controller algorithm. A controller fault is determined by
computing the residual of responses of both controllers, as
shown in Fig. 2. Unfortunately, in either architecture the
plant is already affected by the time the fault is detected
and interventions are applied.
Figure 1. Plant fault detection [20]
What we instead propose is an active defense that cannot
be disabled by any MTU command or PLC software update,
is transparent to the control system designer, and can antici-
pate controller behavior and plant state for a short period into
the future. Section III describes how this is accomplished
using high-level and interface synthesis, C code formal
analysis, hard and soft processors, and conﬁgurable logic
targeting a conﬁgurable system-on-chip platform.
138
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:47 UTC from IEEE Xplore.  Restrictions apply. 
Embedded systemHigh-performance-control subsystemHigh-assurance-control subsystemPhysical plantDecision LogicykykukMUXFigure 2. Controller fault detection [21]
III. TECEP OVERVIEW
Control system engineers commonly use the model-based
design steps outlined with transparent boxes in Fig. 3.
System speciﬁcations are identiﬁed, followed by modeling
of the plant and control algorithm. Simulation checks that the
controller keeps the plant in a stable state within operating
speciﬁcations. Extensions to modeling environments such
as Simulink can automatically generate C code optimized
for a particular target processor architecture. System-on-chip
platforms containing microcontrollers and FPGA fabric are
an appealing target for high-performance controllers because
functions may be mapped to either software or hardware.
This permits the allocation of independent computational
and memory resources to each controller in order to main-
tain ﬁxed response times, rather than timeshare processors
competing for the same memory bandwidth. The post-
implementation simulation checks that TECEP additions do
not change the original behavior. Component generation and
integration are discussed in Sections III-A and III-C.
The extra steps added by TECEP are highlighted in
Fig. 3 with translucent boxes. A relatively small amount
of independent system monitoring code is concerned only
with meeting the operating speciﬁcations, and is checked
with a rigorous software veriﬁcation framework. Speciﬁcs
are discussed in Section III-B. The analysis tools require
familiarity with formal methods, but do not require hardware
veriﬁcation knowledge even though the monitoring code is
ultimately rendered in hardware. Hence the ﬂow separates
application, platform, and formal analysis to allow these
tasks to be performed by different specialists, with the
ultimate goal being semi-automatic synthesis and validation
of the additional components.
A. Platform Components
The system overview shown in Fig. 4 includes two
software-implemented blocks (production controller code
running on both hard and soft processors) and two hardware-
targeted blocks (a hardware monitor synthesized from for-
Figure 3. TECEP design ﬂow
mally analyzed C code, and a junction box captured and val-
idated in a hardware description language). Both hardware
blocks are invisible to the software blocks, even at the OS
driver level. Because the FPGA fabric is not dynamically
conﬁgured, software or network access to programmable
logic conﬁguration ports are disabled. Changes to the pro-
grammable logic could require physical access to a secure
plant, and are needed only if the process speciﬁcations, plant
model, backup controller, or switchover policies change.
Routine software updates, including production control code
revisions, would be stored on network-accessible, external
ﬂash memory, and loaded into RAM after a reset.
1) Production Controller: Implemented in software func-
tions running on the ARM Cortex-A9 processor present
on the Zynq-7000,
the production controller sends and
139
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:47 UTC from IEEE Xplore.  Restrictions apply. 
Embedded systemResidual evaluatorResidual generatorEmbedded controllerBenchmark controllerPhysical plant-Fault detectorMaximum likelihood estimatorFault   alarmykukTypical controls designTECEP design additionsPlant model respond as expected?Plant model respond as expected?System Design SpecificationE.g., expected Plant behaviors, stability limitations, controller configuration access policiesSystem Design SpecificationE.g., expected Plant behaviors, stability limitations, controller configuration access policiesPlant ModelingE.g., Matlab models of physical Plant responsePlant ModelingE.g., Matlab models of physical Plant responseTECEP Protected Device System IntegrationE.g., in a closed loop with Plant sensors and actuatorsTECEP Protected Device System IntegrationE.g., in a closed loop with Plant sensors and actuatorsController ModelingE.g., prototype control algorithms in CController ModelingE.g., prototype control algorithms in CYesVerification SketchingE.g., deductive proofs of controller or TECEP behaviorsTECEP Source IntegrationE.g., Plant model and controller source code added as modules in TECEP C source code templatesVerification Source IntegrationE.g., Frama-C ACSL proof annotations as C comments inline with source codeVerification conditions proven?YesNoNoEmbedded Device ImplementationE.g., High-level synthesis, FPGA bitstream and processor memory file creationEmbedded Device ImplementationE.g., High-level synthesis, FPGA bitstream and processor memory file creationPlant model respond as expected?Plant model respond as expected?YesNoFigure 4. Platform components
receives data through an I/O module (IOM), which in turn
interacts with the programmable fabric through an AXI bus.
Using a real-time kernel such as FreeRTOS, the production
controller and IOM are tasks managed by the kernel. Real-
time guarantees are generally needed by process control
applications.
2) Hardware Monitor / Backup Controller: The hardware
monitor is implemented in the FPGA fabric of the Zynq-
7000. Inputs include the u(t) output of the production
controller, the y(t) output of the physical plant, the output
of the prediction unit, and a status code. Using C code rather
than a hardware description language (HDL) allows the
hardware monitor to easily integrate a stability-preserving
backup controller and the plant model. A speciﬁcation guard
tests whether the outputs of the plant model, physical plant,
and prediction unit are within an acceptable range. If any
of these values are outside speciﬁcations or if the status
code indicates an error, the output from the backup controller
overrides the production controller’s output.
Production to backup controller transitions may be re-
versed if the production controller fault was merely tem-
porary. This feature is useful in the event of a denial-of-
service attack wherein the objective is simply to degrade
performance. A probation period ignores the production
controller’s output for a predetermined number of cycles
after a fault is detected even if the controller’s output returns
to an acceptable value. Hence, the probation period ensures
that the production controller is not reinstated too quickly.
It is also important that the backup controller not be in-
voked at the slightest disturbance, with one implementation
deﬁning an upper bound on the number of small disturbances
within a given time period. Noise in the system, which
may cause false error reporting, will be greatly reduced
with the future addition of a Kalman ﬁlter. Fault tolerance
and any return to the production controller are left to the
system speciﬁcation as requirements differ among various
applications.
3) Prediction Unit: A copy of the production controller
and a model of the plant comprise the prediction unit
implemented on a MicroBlaze soft processor in the pro-
grammable fabric of the Zynq-7000. We ultimately plan to
use a soft-core version of the ARM Cortex-M1 processor
for Xilinx FPGAs [22] since the production controller may
only be available as object code. Since timing character-
istics need not be preserved, other options are ARM-to-
MicroBlaze binary code translation or ARM emulation on
the MicroBlaze. The Zynq-7000’s second ARM Cortex-A9
core is not used because it may be needed in multi-threaded
control applications. In addition, using an ARM core for
prediction requires careful memory and I/O separation from
the production core in order to minimize trust assumptions.
Controller and plant functions send and receive data
across an AXI bus through an IOM. The production con-
troller IOM forwards MTU commands to the prediction unit
IOM, although this link is not yet implemented. FreeRTOS
may also be ported to the MicroBlaze/Cortex-M1 in order
140
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:47 UTC from IEEE Xplore.  Restrictions apply. 
Zynq Programmable LogicZynqProcessing SystemPhysical PlantAXIAXIcontrol/status/data portsARM Cortex-A9Production Controller CodeInput/Output Module (IOM)Soft ProcessorProduction Controller(untrusted software stack)Prediction Unit(untrusted controller code)Production Controller CopyPlant ModelInput/Output Module (IOM)Hardware Monitor (trusted)Backup ControllerPlant ModelControl LogicSpecification GuardsJunction Box(trusted)Serial Bus InterfaceAXI InterfaceAXI Interfaceto more closely mirror the Cortex-A9, with the production
controller, plant model and IOMs implemented as tasks
managed by the kernel. This will also enable the production
Cortex-A9’s complete software stack to be executed on
the soft processor, allowing the prediction unit to preview
attacks that corrupt control algorithm scheduling or driver-
level I/O.
Four modes of operation are available in the prediction
unit: Save, Restore, Normal and Accelerate. Save stores the
current state of both the plant and the controller, while
Restore overwrites the current state with the last saved
state. Normal runs the closed loop system of the plant and
controller for one cycle, and Accelerate runs the closed loop
for a predeﬁned number of cycles. The speciﬁc number
of prediction cycles used is left to the application. Once
the IOM on the soft processor receives a start signal
from the junction box, the closed loop is run in Normal
mode. The current state is then saved, and the closed loop
is run in Accelerate mode. Upon completion, the state of the
controller and plant before acceleration is restored, and the
accelerated plant model’s output is passed through the IOM
to the AXI bus.
4) Junction Box: The production controller, physical
plant, prediction unit, and hardware monitor are connected
via a junction box which contains all connections between
modules in the system, manages the system’s ﬂow of control,
and scrutinizes all external transfers to and from the physical
plant. ARM Cortex-A9 connections to the junction box
include AXI interfaces for the production controller’s input
and output, while soft processor connections consist of AXI
interfaces for handshaking and the prediction unit’s output.
The hardware monitor interfaces to the junction box using
simple 32-bit data ports for inputs from the production
controller, plant, and prediction unit, as well as the hardware
monitor’s output and handshaking signals. Two watchdog
timers in the junction box monitor the response time of
both the production controller and prediction unit; if either
unit fails to respond, the hardware monitor is notiﬁed with
the appropriate status code. The junction box is captured
with HDL code that mostly deﬁnes connections and is
independent of the production and backup controllers, plant
model, and operating speciﬁcations. This simplicity makes
veriﬁcation straightforward using established hardware anal-
ysis techniques such as model checking.
B. Hardware Monitor Formal Analysis
Formal analysis tools are incorporated into the high-level
design ﬂow to verify functional and security speciﬁcations
by evaluating mathematical proofs of design code semantic
properties. We verify the PID code and TECEP additions
using Frama-C, an open source, modular static analysis
framework developed speciﬁcally for the C language [23].
The framework enables collaboration between various static
analysis techniques implemented as plug-ins which can share
141
information. Desired behaviors and other annotations to
analyze are captured in the ANSI/ISO-C Speciﬁcation Lan-
guage (ACSL). Annotations can specify preconditions and
postconditions of a function, predicates, lemmas, axioms,
and other assertions and custom logic functions [24]. Formal
analysis ﬂows are often complicated by the need to translate
or abstract the code under test. However, in this scenario
formal tools are easily leveraged as proof annotations are
added directly to the design source code. The modular
framework also enables veriﬁcation of isolated functions.
This is useful in our security scheme where in order to
prove properties of system security the veriﬁcation space
is reduced to only TECEP additions.
ACSL is added directly to the code to be veriﬁed and is
written as C comments so as not to interfere with standard
compilation or HLS tools. These annotations can, however,
modify Frama-C’s interpretation of function behaviors and
variables if they are speciﬁcally written to do so. The seman-
tics of ACSL logic expressions are based on mathematical
ﬁrst-order logic, which eases translation of conventional
proof languages into proof code. A special type of ACSL
annotation, called ghost code, is only evaluated by Frama-
C and can be independent of any function contract. Ghost
code is typically used to specify variable, logic, or functions
that are outside of or not related directly to the code under
analysis, but are useful in building up proof annotations.
Ghost code can also be used within a function to overwrite
variables or capture their values for analysis outside of the
function. However, using ghost code to interfere with regular
program code must be done with care as it can result in
inaccurate proofs.
ACSL annotations are veriﬁed using various plug-ins
within the Frama-C framework in order to offer a variety
of provers and analysis techniques. Frama-C can be used to
verify the values returned by the hardware monitor when a
speciﬁcation guard detects an out-of-speciﬁcation condition,
and to validate the behavior of modules called by the
hardware monitor. Here we use the Jessie plug-in to ensure
that the hardware monitor result is driven by the appropriate
controller module under all possible conditions. Jessie is a
Hoare logic-based plug-in used to prove functional prop-
erties via deductive veriﬁcation [25]. Jessie automatically
translates ACSL annotations into veriﬁcation conditions in
the Why language, which can then be submitted to external
automatic theorem provers such as Simplify, Alt-Ergo, Z3,
Yices, and CVC3. Interactive theorem provers or proof
assistants can also be used, such as Coq, PVS, Isabelle/HOL,
HOL 4, HOL Light, and Mizar. Frama-C’s use of multiple
provers combines the strengths of different provers, while
time limits cope with undecidability.
C. High-level and Interface Synthesis
1) Hardware Monitor: After
the
hardware monitor’s software-deﬁned functions are im-
formal veriﬁcation,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:47 UTC from IEEE Xplore.  Restrictions apply. 
plemented in the programmable fabric using HLS. The
set_directive_allocation command is applied to
the C-synthesis process to restrict the number of ﬂoating-
point cores generated, and the config_bind command
reduces the resources used by those instantiated cores.
An ap_none interface is added to each of the inputs to
create simple data ports with no additional handshaking
signals. An ap_ctrl_hs interface added to the top level
function provides basic handshaking signals such as start
and done for the operation of the hardware monitor. No
AXI slave adapters are needed, as the hardware monitor is
connected directly to the junction box using simple 32-bit
data ports. Once HLS and interface synthesis processes are
complete, the hardware monitor is exported as an IP block
for use in Vivado Design Suite.
2) AXI Interconnects: The junction box itself uses sim-
ple 32-bit input and output ports; however, the ARM and
soft processors require an AXI interface for sending and
receiving data. Vivado Design Suite has no simple means of
adding an AXI slave adapter to a simple 32-bit data port, but
Vivado HLS does. A trivial C function that returns its input
argument is used to create the interface adapter. Depending
on the direction of the data transfer, an AXI4-Lite slave
adapter is added to either the input argument or the return
port. The ap_none interface protocol is added to each of
the interface adapters with two exceptions: the production
controller’s output and the prediction unit’s output use the
ap_ctrl_hs protocol. Each interface module utilizes on
average 74 ﬂip-ﬂops and 20 lookup tables, though there is
some variation based on the data type being used in the C
function. After completion of HLS and interface synthesis,
the interface adapters are also exported as IP blocks for use