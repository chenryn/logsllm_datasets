(0.1)
(0.5)
(0.3)
(1.8)
(102.0)
(0.1)
Slowdown (x)
4.77
2.32
0.86
2.69
0.25
13.09
17.54
6.11
33.00
2.65
1.12
35.01
4.20
BASELINE PERFORMANCE COMPARED TO LINUX (MEDIAN
UNIXBENCH SCORES, HIGHER IS BETTER, STD.DEV. IN PARENTHESES).
approaches, especially when considering the “fail” case where
the test fails but the system remains stable. The pessimistic
approach has somewhat
lower survivability than the other
approaches, which is to be expected as it sometimes shuts
down in cases where recovery may work out even though it
cannot be proven to be safe. Both the pessimistic and enhanced
approaches are very effective in reducing the number of
crashes. We must note that crashes cannot be fully eliminated
as it is impossible to recover from faults injected into the code
involved in the recovery itself (such faults violate the single
fault assumption in our fault model). We conclude that for
faults within our fault model, our enhanced recovery method
offers superior survivability while still avoiding recovery in
cases where it cannot be proven to be safe.
Even when injecting all faults from the EDFI model, vio-
lating our fail-stop assumption, our enhanced recovery method
offers the best result in terms of both survivability and is very
effective at avoiding crashes. The higher number of crashes in
this case is to be expected as we can no longer assume our
checkpoint to be in a known-good state due to the possibility of
silent state corruption. The fact that our approach still performs
well shows its robustness in the face of violations of the fault
model and its ability to handle realistic software faults.
C. Performance overhead
To evaluate the performance of our system we used the
Unixbench benchmark [35]. We ran the benchmark 11 times on
a 4-core 2.3 GHz AMD Phenom processor with 6 GB of RAM.
Table IV compares the median Unixbench score of the baseline
system (without recovery) against Linux. Our prototype is
signiﬁcantly slower than Linux in the Unixbench benchmark.
This can be explained by the overhead incurred by context-
switching between OS components due to the microkernel
design and the fact that Linux is a much more mature and opti-
mized system with performance as one of its major goals. Our
focus however remains on recoverability in compartmentalized
systems rather than on microkernel system performance, a
subject extensively explored in prior work [38], [39], [40].
To evaluate the overhead incurred by our recovery solution,
we compare the baseline against our unoptimized recovery
instrumentation, optimized for the pessimistic recovery policy,
and optimized for the enhanced recovery policy. The relative
slowdowns are listed in Table V. The results show that
our optimization of disabling undo log updates outside the
recovery window greatly pays off. Overall,
in comparison
8
Benchmark
dhry2reg
whetstone-double
execl
fstime
fsbuffer
fsdisk
pipe
context1
spawn
syscall
shell1
shell8
geomean
TABLE V.
Without opt.
1.001
1.002
1.326
1.321
2.317
1.165
1.158
1.137
1.228
1.173
1.110
1.256
(0.003)
(0.001)
(0.007)
(0.003)
(0.879)
(0.008)
(0.007)
(0.007)
(0.010)
(0.047)
(0.368)
(0.004)
Pessimistic
0.996
1.001
0.750
0.749
1.175
1.168
1.158
1.146
1.213
1.164
0.942
1.261
(0.004)
(0.001)
(0.003)
(0.002)
(0.207)
(0.006)
(0.004)
(0.003)
(0.009)
(0.050)
(0.248)
(0.004)
Enhanced
0.991
1.003
0.762
0.762
1.194
1.179
1.169
1.156
1.253
1.164
0.928
1.266
(0.004)
(0.001)
(0.004)
(0.002)
(0.211)
(0.007)
(0.006)
(0.003)
(0.010)
(0.046)
(0.245)
(0.005)
1.235
1.046
1.054
SLOWDOWN RATIO (MEDIAN SLOWDOWN RATIO, LOWER IS
BETTER, STD.DEV. IN PARENTHESES).
to that without optimization, the performance overhead has
decreased from about 23% to only 5%. As a consequence,
the system incurs only a modest performance overhead of
around 5% for both recovery modes. The pessimistic mode
incurs lower overhead than the enhanced mode since recovery
windows remain open for shorter periods of time. We observe
a performance improvement in our execl and fstime tests
due to their multi-process interactions. In such tests, our
instrumentation seems to positively affect scheduling decisions
and improve overall performance [41].
D. Memory overhead
OSIRIS increases memory usage due to two main factors:
maintaining the undo log and keeping spare copies (clones)
of servers in memory for recovery purposes. We present the
resulting memory overhead in Table VI. The table shows
average runtime memory usage per component observed across
the baseline and instrumented variants of our prototype. We
measure the physical memory usage in a quiescent state for
each component. We then add the maximum undo log size
reported during the per-component execution of Unixbench.
We observe a total memory overhead of 50 MB, which
represents a 6-fold memory usage increase for those OS
servers. This is, however, mostly the overhead needed for VM
alone. Maintaining a VM clone for recoverability purposes
requires a lot of memory pre-allocation so that the new VM
does not depend on the defunct VM for memory allocation
during recovery. We note that even with this memory overhead,
the system servers consume only a small part of memory in
comparison to that typically used by user applications.
E. Service disruption
We show that OSIRIS guarantees continuity of execution
with acceptable service disruption in the face of even
high-intensity and consistent inﬂow of crashes. In contrast
to the randomized fault
injection campaign presented in
Subsection VI-B, we designed this experiment to speciﬁcally
show performance and continuity of a long-running benchmark
(Unixbench) under a challenging (but synthetic) fault load.
To ensure consistent recoverability and run the benchmark
to completion, we only injected faults within the recovery
window. In particular, we injected fail-stop faults in the
PM server at regular time intervals. We selected PM, as it
is a heavily exercised operating system component during
the benchmark. We repeated this experiment several times,
Fig. 3. Unixbench scores as a function of service disruption interval.
each time doubling the inﬂux of faults per interval. For each
run, we veriﬁed that the benchmark completed without any
functional service degradation and measured the resulting
benchmark performance. Figure 3 presents our results.
undergo
signiﬁcant
slowdown, most
As shown in the ﬁgure, some of the performance tests
are not at all affected by the injected faults. However,
others
noticeably
shell1, shell8, and execl. This is explained by a
heavy dependence on PM for these tests. Correspondingly,
the slowdown is absent for tests with no dependence on PM,
such as dhry2reg, whetstone-double, fsdisk, and
fsbuffer. We conclude that the performance degradation
induced by periodic crash recovery operations is workload-
dependent and, despite the performance impact in some cases,
OSIRIS can effectively guarantee survivability and continuity
of execution even in extreme high-frequency failure scenarios.
VII. LIMITATIONS AND FUTURE WORK
Controlled shutdown: Fault injection results show that
our approach substantially reduces the number of crashes. We
perform a controlled shutdown whenever we cannot guarantee
that a recovery attempt
leads to a consistent global state.
In this case, state is still lost. However, the system is still
consistent, which is often more important. In future work, we
could extend our approach to provide opportunities for user
applications to save their state before restarting the system—a
strategy similar to what Otherworld [12] supports after a
system crash using annotations.
Extensibility: Although our prototype implements a
small set of classes of SEEPs and recovery actions, our
framework is extensible to allow alternatives tailored towards
speciﬁc use cases. For example, certain state changes in
the system could be limited to updating requester-speciﬁc
information in several compartments. Killing the requester
process could automatically clean up such state changes. In
future work, we could deﬁne a new class of SEEPs to identify
such interactions in the system and a corresponding new
reconciliation action to kill the requester process.
Composable recovery policies: The ability to classify
the nature of inter-compartment communication by their re-
spective SEEP types allows co-existence of multiple compos-
able recovery policies. For example, a recovery window may
9
 0 200 400 600 800 1000 1200 1400 1600 1800 1 10 100 1000 10000Performance indexService disruption intervalUnixbench service disruption interval vs performancecontextdhry2regexeclfsbuﬀerfsdiskfstimepipeshell1shell8spawnsyscallwhetstone-doubleServer
Base memory usage (kB)
+clone (kB)
+undo log (kB)
Total Overhead (kB)
PM
VFS
VM
DS
RS
total
628
1,252
4,532
248
1,696
8,356
944
1,600
18,032
488
5,004
26,068
1
13
24,576
1
1
24,592
945
1,613
42,608
489
5,005
50,660
TABLE VI.
PER-COMPONENT MEMORY OVERHEAD.
gradually switch from strict recovery policies to more lenient
recovery policies, depending on the series of SEEP interactions
encountered on a given execution path. OSIRIS could then
perform tailored recovery actions accordingly.
Generality of the framework: Our recovery framework
is generic and can be retroﬁtted to other systems. Our LLVM-
based instrumentation and recovery libraries can easily be
reused in other settings. For example, the recovery framework
is independent of inter-compartmental communication proto-
cols used in the target system. This design makes our system
more adaptable and lowers the cost of maintenance. How-
ever, the target system should be composed of modular and
restartable components. Modular server applications, multi-tier
distributed systems and applications are promising candidates.
Fail-silent software faults: Recovery in OSIRIS aims
to protect
the system from crashes due to software bugs
in untested parts of the system. However, it cannot provide
the same guarantees in case of bugs that cause silent data
corruption. In this case, we cannot determine whether the last
checkpoint contains a safe state to recover to. Moreover, since
our current failure detection strategy is based on the occurrence
of a crash or hang in the affected compartment, we cannot
protect against non-fatal manifestations of bugs. Nonetheless,
with better fault detection mechanisms in place (e.g., memory
safety solutions), we can approximate a fail-stop model for a
broader class of real-world faults.
Recovery surface: SEEP is a foundational element of
our approach. While it is possible to tailor the SEEP mech-
anism for another system, a target system design that yields a
high frequency of inter-compartmental communications with
global side effects, is likely to result in a small recovery sur-
face. However, given that such a design is unfavorable even for
performance-sensitive systems, we believe this limitation does
not greatly affect the general applicability of our approach.
Performance: As we have shown in Section VI, our re-
covery mechanism incurs very low overhead on our prototype.