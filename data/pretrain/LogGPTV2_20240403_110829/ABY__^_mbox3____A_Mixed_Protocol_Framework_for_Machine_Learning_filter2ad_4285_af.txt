7153
4350
993
325
104
45
24.3
12.4
23.7
11.8
17.5
9.2
Batch Size B
1024
5042
4263
447
281
46
27
23.9
12.4
23.3
11.8
12.6
7.3
Online + Offline Throughput
128
11574
47
5089
3.7
377
0.44
20.8
2.4
20.7
0.63*
19.3
0.06*
256
9803
25
2744
2.0
200
0.24
20.7
1.6
20.4
0.37*
17.9
0.03*
512
6896
11
1091
1.1
100
0.12
20.6
0.88
20.1
0.19*
16.5
0.02*
1024
4125
5.4
470
0.6
46
0.06
20.3
0.50
19.4
0.11*
11.6
0.01*
Figure 2: Linear Regression performance measured in iterations per second (larger
= better). Dimension denotes the number of features while batch size denotes number
of samples used in each iteration. WAN setting has 40ms RTT latency and 40 Mbps
throughput. The preprocessing for [43] was performed either using OT or the DGK cryp-
tosystem with the faster protocol being reported above. The * symbol denotes that the
DGK protocol was performed.
Model
Protocol
Batch Size
Linear
Logistic
NN
CNN
This
SecureML [43]
This
SecureML [43]
This
SecureML [43]
This*
Chameleon [46]
MiniONN [40]
1
100
1
100
1
100
1
100
1
1
1
1
1
Running Time (ms)
Total
Online
3.8
0.1
4.1
0.3
2.6
0.2
54.2
0.3
0.2
4.0
9.1
6.0
3.8
0.7
56.2
4.0
8
3
4823
193
10
6
1360
2700
9329
3580
Comm.
(MB)
0.002
0.008
1.6
160
0.005
0.26
1.6
161
0.5
120.5
5.2
12.9
657.5
Figure 3: Running time and communication of privacy pre-
serving inference (model evaluation) for linear, logistic and neu-
ral network models in the LAN setting (smaller = better). [43]
was evaluated on our benchmark machine and [40, 46] are cited
from [46] using a similar machine. The models are for the MNIST
dataset with D = 784 features. NN denotes neural net with 2
fully connected hidden layers each with 128 nodes along with
a 10 node output layer. CNN denotes a convolutional neural net
with 2 hidden layers, see [46] details. * This work (over) approxi-
mates the cost of the convolution layers with an additional fully
connected layer with 980 nodes.
already been trained is secret shared between the parties along with
an unlabeled feature vector for which a prediction is desired. Given
this, the parties evaluate the model on the feature vector to produce
a prediction label. We note that inference (evaluation) for all three
types of models can be seen as a special case of training (e.g. one
forward propagation in case of neural networks) and hence can be
easily performed using our framework. Following the lead of several
prior works [40, 43, 46], we report our protocol’s performance on
the MNIST task. The accuracy of these models ranges from 93%
(linear) to 99% (CNN).
When evaluating a single input using a linear model, our protocol
requires exactly one online round of interaction (excluding the shar-
ing of the input and reconstructing the output). As such, the online
computation is extremely efficient, performing one inner product
and communicating O(1) bytes. The offline preprocessing, however,
requires slightly more time at 3.7 ms along with the majority of the
communication. The large difference between online and offline is
primarily due to the fact that our offline phase is optimized for high
throughput as opposed to low latency. Indeed, to take advantage
of SSE vectorization instructions our offline phase performs 128
times more work than is required. When compared to SecureML we
observe that their total time for performing a single prediction is
slightly less than ours due to their offline phase requiring one round
of interaction as compared to our 64 rounds. However, achieving
this running time in the two-party setting requires a very large
communication of 1.6 MB as opposed to our (throughput optimized)
0.002 MB, an 800 times improvement. Our protocol also scales much
better as it requires almost the same running time to evaluate 100
predictions as it does 1. SecureML, on the other hand, incurs a 20
times slowdown which is primarily in the communication heavy
OT-based offline phase.
We observe a similar trend when evaluating a logistic regression
model. The online running time of our protocols when evaluating
a single input is just 0.2 milliseconds compared to SecureML requir-
ing 0.7, with the total time of both protocols being approximately
4 milliseconds. However, our protocol requires 0.005 MB of com-
munication compared to 1.6 MB by SecureML, a 320× difference.
When 100 inputs are all evaluated together our total running time
is 9.1ms compared to 54.2 by SecureML, a 6× improvement.
Our protocol requires 3ms in the online phase to evaluate the
model and 8ms overall. SecureML, on the other hand, requires
193ms in the online phase and 4823ms overall, a 600× difference.
Our protocol also requires 0.5 MB of communication as compared
to 120.5 MB by SecureML.
More recently MiniONN [40] and Chameleon [46] have both
proposed similar mixed protocol frameworks for evaluating neural
networks. Chameleon builds on the two-party ABY framework [24]
which in this paper we extend to the three-party case. However,
Chameleon modifies that framework so that a semi-honest third
party helps perform the offline phase as suggested in the client-
aided protocol of [43]. As such, Chameleon’s implementation can
also be seen in the semi-honest 3 party setting (with an honest
majority). In addition, because Chameleon is based on 2 party pro-
tocols, many of their operations are less efficient compared to this
work and cannot be naturally extended to the malicious setting.
MiniONN is in the same two-party model as SecureML. It too is
based on semi-honest two-party protocols and has no natural ex-
tension to the malicious setting.
As Figure 3 shows, our protocol significantly outperforms both
Chameleon and MiniONN protocols when ran on similar hard-
ware. Our online running time is just 6 milliseconds compared
to 1360 by Chameleon and 3580 by MiniONN. The difference be-
comes even larger when the overall running time is considered
with our protocol requiring 10 milliseconds, while Chameleon and
MiniONN respectively require 270 and 933 times more time. In
addition, our protocol requires the least communication of 5.2 MB
compared to 12.9 by Chameleon and 657.5 by MiniONN. We stress
that Chameleon’s implementation is in a similar security model to
us while MiniONN is in the two-party setting.
Session 1B: PrivacyCCS’18, October 15-19, 2018, Toronto, ON, Canada46REFERENCES
[1] Azure machine learning studio. https://azure.microsoft.com/en-us/services/
machine-learning-studio/.
[2] Eigen library. http://eigen.tuxfamily.org/.
[3] Google cloud ai. https://cloud.google.com/products/machine-learning/.
[4] Machine learning on aws. https://aws.amazon.com/machine-learning/.
[5] MNIST database. http://yann.lecun.com/exdb/mnist/. Accessed: 2016-07-14.
[6] Watson machine learning. https://www.ibm.com/cloud/machine-learning.
[7] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security, pages 308–318.
ACM, 2016.
[8] Y. Aono, T. Hayashi, L. Trieu Phong, and L. Wang. Scalable and secure logis-
tic regression via homomorphic encryption. In Proceedings of the Sixth ACM
Conference on Data and Application Security and Privacy, pages 142–144. ACM,
2016.
[9] T. Araki, J. Furukawa, Y. Lindell, A. Nof, and K. Ohara. High-throughput semi-
honest secure three-party computation with an honest majority. In E. R. Weippl,
S. Katzenbeisser, C. Kruegel, A. C. Myers, and S. Halevi, editors, Proceedings of the
2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna,
Austria, October 24-28, 2016, pages 805–817. ACM, 2016.
[10] J. BARZILAI and J. J. Borwein. Two-point step size gradient methods. 8:141–148,
01 1988.
[11] A. Ben-David, N. Nisan, and B. Pinkas. FairplayMP: a system for secure multi-
party computation. pages 257–266.
[12] D. Bogdanov, S. Laur, and J. Willemson. Sharemind: A framework for fast privacy-
preserving computations. pages 192–206.
[13] D. Bogdanov, R. Talviste, and J. Willemson. Deploying secure multi-party com-
In International Conference on Financial
putation for financial data analysis.
Cryptography and Data Security, pages 57–64. Springer, 2012.
[14] F. Bourse, M. Minelli, M. Minihold, and P. Paillier. Fast homomorphic evaluation of
deep discretized neural networks. Cryptology ePrint Archive, Report 2017/1114,
2017. https://eprint.iacr.org/2017/1114.
[15] P. Bunn and R. Ostrovsky. Secure two-party k-means clustering. In Proceedings
of the 14th ACM conference on Computer and communications security, pages
486–497. ACM, 2007.
[16] N. Büscher, A. Holzer, A. Weber, and S. Katzenbeisser. Compiling low depth
circuits for practical secure computation. In I. G. Askoxylakis, S. Ioannidis, S. K.
Katsikas, and C. A. Meadows, editors, Computer Security - ESORICS 2016 - 21st
European Symposium on Research in Computer Security, Heraklion, Greece, Sep-
tember 26-30, 2016, Proceedings, Part II, volume 9879 of Lecture Notes in Computer
Science, pages 80–98. Springer, 2016.
[17] R. Canetti. Security and composition of multiparty cryptographic protocols.
13(1):143–202, 2000.
[18] H. Chabanne, A. de Wargny, J. Milgram, C. Morel, and E. Prouff. Privacy-
preserving classification on deep neural network. IACR Cryptology ePrint Archive,
2017:35, 2017.
[19] N. Chandran, J. A. Garay, P. Mohassel, and S. Vusirikala. Efficient, constant-round
and actively secure MPC: beyond the three-party case. In B. M. Thuraisingham,
D. Evans, T. Malkin, and D. Xu, editors, Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security, CCS 2017, Dallas, TX, USA,
October 30 - November 03, 2017, pages 277–294. ACM, 2017.