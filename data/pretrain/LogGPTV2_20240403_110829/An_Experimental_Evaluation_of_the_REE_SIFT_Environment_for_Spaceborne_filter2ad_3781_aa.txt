# An Experimental Evaluation of the REE SIFT Environment for Spaceborne Applications

## Authors
- Keith Whisnant, University of Illinois, Urbana, IL
- Ravishankar K. Iyer, University of Illinois, Urbana, IL
- P. Jones, University of Illinois, Urbana, IL
- Raphael R. Some, Jet Propulsion Laboratory, Pasadena, CA
- David A. Rennels, University of California, Los Angeles, CA

## Abstract
This paper presents an experimental evaluation of a software-implemented fault tolerance (SIFT) environment designed around a set of self-checking processes called ARMORs, which run on different machines to provide error detection and recovery services for spaceborne scientific applications. The experiments are divided into three groups of error injections, with each group progressively stressing the SIFT error detection and recovery mechanisms more than the previous one. The results show that the SIFT environment adds negligible overhead to the application during failure-free runs. Only 11 cases were observed where either the application failed to start or the SIFT environment failed to recognize that the application had completed. Further investigations revealed that assertions within the SIFT processes, coupled with object-based incremental checkpointing, were effective in preventing system failures by protecting dynamic data within the SIFT processes.

## 1. Introduction
In traditional spaceborne applications, onboard instruments collect and transmit raw data back to Earth for processing. The amount of scientific work that can be done is limited by the telemetry bandwidth to Earth. Processing the complete set of raw data on the ground can be time-consuming. The Remote Exploration and Experimentation (REE) project at JPL aims to use a cluster of commercial off-the-shelf (COTS) processors to analyze the data onboard and send only the results back to Earth. This approach not only saves downlink bandwidth but also provides the possibility of making real-time, application-oriented decisions.

While failures in the scientific applications are not critical to the spacecraft's health (spacecraft control is performed by a separate, trusted computer), they can be costly. The commercial components used by REE are expected to experience a high rate of radiation-induced transient errors in space (ranging from one per day to several per hour), and downtime directly leads to the loss of scientific data. Therefore, a fault-tolerant environment is needed to manage the REE applications. The first experiment will likely continue to transmit raw data to Earth while simultaneously using two to eight COTS processors to analyze the results. The goal is to ensure that the onboard analysis agrees with the analysis traditionally done on the ground, thus facilitating the transition to missions that use the REE platform exclusively for all computations.

The missions envisioned to take advantage of the SIFT environment include the Mars Rover, the Orbiting Thermal Imaging Spectrometer (OTIS), the Next-Generation Space Telescope (NGST), the Gamma Ray Large Area Space Telescope, and the Solar Terrestrial Probe. Although a complete set of requirements is closely dependent on the specific characteristics of the scientific applications, some key points are clear:

- The SIFT environment must be able to detect and recover from its own crash and hang failures with minimal impact on application performance. A study of applications indicates that a performance impact of 5% or less is desirable.
- The SIFT environment must detect and recover from application crashes and hangs.
- The SIFT environment must limit error propagation.
- Performance, power, and weight must be considered when designing SIFT mechanisms. Applications will execute in simplex mode because resource constraints generally preclude replication.

This paper presents a methodology for experimentally evaluating a distributed SIFT environment executing an REE texture analysis program from the Mars Rover mission. Errors are injected to study the consequences of faults. The experiments do not attempt to analyze the cause of the errors or fault coverage. Instead, the error injections progressively stress the detection and recovery mechanisms of the SIFT environment:

1. **SIGINT/SIGSTOP Injections**: Many faults lead to crash and hang failures. SIGINT/SIGSTOP injections reproduce these first-order effects of faults in a controlled manner that minimizes the possibility of error propagation or checkpoint corruption.
2. **Register and Text-Segment Injections**: The next set of error injections represents common effects of single-event upsets by corrupting the state in the register set and text segment memory. This introduces the possibility of error propagation and checkpoint corruption.
3. **Heap Injections**: The third set of experiments further broadens the failure scenarios by injecting errors in the dynamic heap data to maximize the possibility of error propagation. The results from these experiments are especially useful in evaluating how well intraprocess self-checks limit error propagation.

### REE Computational Model
The REE computational model consists of a trusted, radiation-hardened (rad-hard) Spacecraft Control Computer (SCC) and a cluster of COTS processors that execute the SIFT environment and the scientific applications. The SCC schedules applications for execution on the REE cluster through the SIFT environment, possibly sharing the computational resources among several applications through multitasking.

### REE Testbed Configuration
The experiments described in this paper were executed on a 4-node testbed consisting of PowerPC 750 processors running the Lynx real-time operating system. Nodes are connected through 100 Mbps Ethernet in the testbed, although the actual onboard computing platform is expected to use a high-speed interconnect such as Myrinet. Between one and two megabytes of RAM on each processor were set aside to emulate local nonvolatile memory available to each node. The nonvolatile RAM is expected to store temporary state information that must survive hardware reboots (e.g., checkpointing information needed during recovery). Nonvolatile memory visible to all nodes is emulated by a remote file system residing on a Sun workstation that stores program executables, application input data, and application output data.

### REE MPI Application
A Mars Rover texture analysis program [5] was used as the workload application. Cameras on the Mars Rover take images of the Martian surface and store them on stable storage. The program applies a series of filters to segment the image according to texture features. Three filters extract vectors that describe image features along each of its three axes. A statistical clustering algorithm is applied to the feature vectors to segment the image (e.g., to distinguish between different rocks in the image), and an output of the segmented image in feature vector space is written back to disk. The application takes rudimentary checkpoints by updating a status file after each filter completes. If the application restarts, it can skip filters that have already completed, but it must redo any filtering that was interrupted by the application failure. The application executes on two nodes and analyzes one image per run for the purposes of these experiments.

## 2. SIFT Environment for REE
The REE applications are protected by a SIFT environment designed around a set of self-checking processes called ARMORS (Adaptive Reconfigurable Mobile Objects of Reliability) that execute on each node in the testbed. ARMORs control all operations in the SIFT environment and provide error detection and recovery to the application and to the ARMOR processes themselves. We provide a brief summary of the ARMOR-based SIFT environment as implemented for the REE applications; additional details of the general ARMOR architecture appear in [12].

### 2.1 SIFT Architecture
An ARMOR is a multithreaded process internally structured around objects called elements that contain their own private data and provide elementary functions or services (e.g., detection and recovery for remote ARMOR processes, internal self-checking mechanisms, or checkpointing support). Together, the elements constitute the functionality that defines an ARMOR’s behavior. All ARMORs contain a basic set of elements that provide core functionality, including the ability to implement reliable point-to-point message communication between ARMORs, communicate with the local daemon ARMOR process, respond to heartbeats from the local daemon, and capture ARMOR state. Specific ARMORs extend this core functionality by adding extra elements.

Each ARMOR is addressed by a unique identification number, allowing messages to be sent to an ARMOR without prior knowledge of the ARMOR’s physical location. ARMORs communicate solely through message passing, and messages are processed in separate threads within the ARMOR. A message consists of sequential events that trigger element actions. Elements subscribe to events that they are designed to process (e.g., an element can subscribe to an event that corresponds to the termination of the application), and an element’s state can only be modified while processing message events. This modular, event-driven architecture permits the ARMOR’s functionality and fault tolerance services to be customized by choosing the particular set of elements that make up the ARMOR.

#### Types of ARMORs
The SIFT environment for REE applications consists of four kinds of ARMOR processes: a Fault Tolerance Manager (FTM), a Heartbeat ARMOR, daemons, and Execution ARMORs.

- **Fault Tolerance Manager (FTM)**: A single FTM executes on one of the nodes and is responsible for recovering from ARMOR and node failures as well as interfacing with the external Spacecraft Control Computer (SCC). The FTM contains all the basic ARMOR elements plus additional elements to (1) accept requests to execute applications from the SCC, (2) track resource usage of nodes in the SIFT environment, (3) send “Are-you-alive?” messages to daemons to detect node failures, (4) install Execution ARMORs for a particular application, (5) recover from failed subordinate ARMORs (i.e., Execution ARMORs and the Heartbeat ARMOR), (6) recover from node failures by migrating processes to another node, (7) recover from application failures, and (8) send application status information to SCC.
- **Heartbeat ARMOR**: The Heartbeat ARMOR executes on a node separate from the FTM. Its sole responsibility is to detect and recover from failures in the FTM through the periodic polling for liveness. This functionality is implemented in a single element that is added to the Heartbeat ARMOR beyond the basic set of elements found in all ARMORs.
- **Daemons**: Each node on the network executes a daemon process. Daemons are the gateways for ARMOR-to-ARMOR communication and detect failures in the local ARMORs. In addition to the core ARMOR configuration, the daemon contains elements that permit it to (1) install other ARMOR processes on the node, (2) communicate with local ARMORs, (3) cache the location of remote ARMORs, (4) route messages to remote ARMORs, (5) send “Are-you-alive?” inquiries to local ARMORs to detect hang failures, (6) detect crash failures in local ARMORs, (7) process “Are-you-alive?” inquiries from the FTM, and (8) notify the FTM to initiate recovery of failed local ARMORs.
- **Execution ARMORs**: Each application process is directly overseen by a local Execution ARMOR. In addition to the core set of elements, an Execution ARMOR contains elements to (1) launch application processes, (2) detect crash failures in application processes, (3) handle progress indicator updates from the application (to be described later), and (4) notify the FTM if the application process fails.

The ARMOR architecture permits the functionality of several ARMORs to be merged into a single process. For example, the functionality of the daemon and Execution ARMOR that execute on a node can be combined into a single ARMOR. Although this reduces the number of processes in the system, there are drawbacks to consolidating functionality. The complexity of the combined process is increased, thus increasing the probability of software design errors. Moreover, a single failure in the combined process will affect several more detection and recovery mechanisms than a single failure in which the mechanisms are distributed across multiple processes.

### 2.2 Executing REE Applications
Before executing any applications, the SCC first performs a one-time installation of the daemons, FTM, and Heartbeat ARMOR on the REE cluster. The SCC then launches applications through the SIFT environment, prompting the FTM to install Execution ARMORs on the appropriate nodes to support the application. Table 1 lists the steps involved in executing an MPI application, including the one-time installation of the SIFT environment. If the application executes perpetually, the Execution ARMORs are never uninstalled; otherwise, they are removed from the SIFT environment after the application completes. If several applications are executed sequentially, the FTM can reuse Execution ARMORs across applications.

Figure 1 illustrates a configuration of the SIFT environment with two MPI applications (from the Mars Rover and OTIS missions) executing on a four-node testbed. Arrows in the figure depict the relationships among the various processes (e.g., the application sends progress indicators to the Execution ARMORs, the FTM is responsible for recovering from failures in the Heartbeat ARMOR, and the FTM heartbeats the daemon processes). While the ARMORs can be distributed across the REE cluster in several ways, the FTM and Heartbeat ARMOR must reside on separate nodes to tolerate single-node failures. The entire SIFT environment can scale down to a minimal two-node configuration if necessary: the FTM executing on the first node, the Heartbeat ARMOR on the second, and the other ARMOR and application processes distributed across both nodes.

Each application process is linked with a SIFT interface that establishes a one-way communication channel with the local Execution ARMOR at application initialization. The application programmer can use this interface to invoke a variety of fault tolerance services provided by the ARMOR. The interface used for these experiments contains functions for initializing the communication channel, using progress indicators to detect application hangs, and closing the communication channel.

As described in Table 1, the Execution ARMORs, the Heartbeat ARMOR, and the FTM are children of their respective parent processes.