title:An Experimental Evaluation of the REE SIFT Environment for Spaceborne
Applications
author:Keith Whisnant and
Ravishankar K. Iyer and
P. Jones and
Raphael R. Some and
David A. Rennels
An Experimental Evaluation of the REE SIFT Environment  
for Spaceborne Applications 
K. Whisnant, R.K. Iyer, P. Jones 
University of Illinois 
Urbana, IL  
R. Some 
Jet Propulsion Laboratory 
Pasadena, CA 
{kwhisnan,iyer,ph-jones}
PI:EMAIL
D. Rennels 
University of California 
Los Angeles, CA 
PI:EMAIL 
@crhc.uiuc.edu
Abstract 
to 
to 
spaceborne 
themselves  and 
This  paper  presents  an  experimental  evaluation  of  a 
software-implemented fault tolerance (SIFT) environment built 
around a set of self-checking processes called ARMORs running 
on different machines that provide error detection and recovery 
services 
scientific 
applications.    The  experiments  are  split  into  three  groups  of 
error  injections,  with  each  group  successively  stressing  the 
SIFT  error  detection  and  recovery  more  than  the  previous 
group.    The  results  show  that  the  SIFT  environment  adds 
negligible overhead to the application during failure-free runs.  
Only  11  cases  were  observed  in  which  either  the  application 
failed to start or the SIFT environment failed to recognize that 
the application had completed.  Further investigations showed 
that assertions within the SIFT processes—coupled with object-
based incremental checkpointing—were effective in preventing 
system  failures  by  protecting  dynamic  data  within  the  SIFT 
processes. 
1 
Introduction 
In traditional spaceborne applications, onboard instruments 
collect and transmit raw data back to Earth for processing.  The 
amount  of  science  that  can  be  done  is  clearly  limited  by  the 
telemetry  bandwidth  to  Earth.  Processing  the  complete  set  of 
raw  data  on  ground,  however,  can  be  time-consuming.    The 
Remote Exploration and Experimentation (REE) project at JPL 
intends  to  use  a  cluster  of  commercial  off-the-shelf  (COTS) 
processors to analyze the data onboard and send only the results 
back  to  Earth.    This  approach  not  only  saves  downlink 
bandwidth,  but  also  provides  the  possibility  of  making  real-
time, application-oriented decisions. 
While failures in the scientific applications are not critical 
to  the  spacecraft’s  health  in  this  environment  (spacecraft 
control is performed by a separate, trusted computer), they can 
be  expensive  nonetheless.    The  commercial  components  used 
by  REE  are  expected  to  experience  a  high  rate  of  radiation-
induced transient errors in space (ranging from one per day to 
several  per  hour),  and  downtime  directly  leads  to  the  loss  of 
scientific data.  Hence, a fault-tolerant environment is needed 
to  manage  the  REE  applications.    It  is  likely  that  the  first 
experiment will continue to transmit the raw data to Earth while 
simultaneously using two to eight COTS processors to analyze 
the  results.    The  goal  is  to  ensure  that  the  onboard  analysis 
agrees with the analysis traditionally done on the ground, thus 
helping to smooth the transition to missions that use the REE 
platform exclusively for all computations. 
The  missions  envisioned  to  take  advantage  of  the  SIFT 
environment 
scientific 
applications  include  the  Mars  Rover,  the  Orbiting  Thermal 
Imaging  Spectrometer  (OTIS),  the  Next-Generation  Space 
Telescope  (NGST),  the  Gamma  Ray  Large  Area  Space 
executing  MPI-based 
[16] 
for 
Telescope,  and  the  Solar  Terrestrial  Probe.    Although  a 
complete  set  of  requirements  is  closely  dependent  upon  the 
particular  characteristics  of  the  scientific  applications,  some 
facts are clear: 
•
= The SIFT environment must be able to detect and recover 
from its own crash and hang failures with minimal impact 
on  application  performance.    A  study  of  applications 
indicates  that  a  performance  impact  of  5%  or  less  is 
desirable. 
•
= The SIFT environment must detect and recover application 
crashes and hangs. 
•
= The SIFT environment must limit error propagation. 
•
Performance, power, and weight must be considered when 
designing  SIFT  mechanisms.    Applications  will  execute 
only 
in  simplex  mode  because  resource  constraints 
generally preclude replication. 
This  paper  presents  a  methodology  for  experimentally 
evaluating  a  distributed  SIFT  environment  executing  an  REE 
texture analysis program from the Mars Rover mission.  Errors 
are injected so that the consequences of faults can be studied.  
The  experiments  do  not  attempt  to  analyze  the  cause  of  the 
errors  or  fault  coverage. 
injections 
progressively stress the detection and recovery mechanisms of 
the SIFT environment: 
1.  SIGINT/SIGSTOP  injections.    Many  faults  are  known  to 
lead  to  crash  and  hang  failures1.  SIGINT/SIGSTOP 
injections reproduce these first-order effects of faults in a 
controlled  manner  that  minimizes  the  possibility  of  error 
propagation or checkpoint corruption. 
the  error 
  Rather, 
2.  Register and text-segment injections.  The next set of error 
injections represent common effects of single-event upsets 
by corrupting the state in the register set and text segment 
memory.  This 
the  possibility  of  error 
propagation and checkpoint corruption. 
introduces 
3.  Heap  injections.    The  third  set  of  experiments  further 
broaden  the  failure  scenarios  by  injecting  errors  in  the 
dynamic  heap  data  to  maximize  the  possibility  of  error 
propagation.    The  results  from  these  experiments  are 
especially useful in evaluating how well intraprocess self-
checks limit error propagation. 
REE  computational  model.    The  REE  computational 
model  consists  of  a  trusted,  radiation-hardened  (rad-hard) 
Spacecraft  Control  Computer  (SCC)  and  a  cluster  of  COTS 
processors that execute the SIFT environment and the scientific 
applications.  The SCC schedules applications for execution on 
the  REE  cluster  through  the  SIFT  environment,  possibly 
sharing the computational resources among several applications 
through multitasking. 
1   A  crashed  process  terminates  abnormally.    A  hung  process  ceases  to 
make progress or becomes unresponsive to input messages, but it does 
not terminate. 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:44 UTC from IEEE Xplore.  Restrictions apply. 
=
REE testbed configuration.  The experiments described in 
this  paper  were  executed  on  a  4-node  testbed  consisting  of 
PowerPC 750 processors running the Lynx real-time operating 
system.    Nodes  are  connected  through  100  Mbps  Ethernet  in 
the testbed, although the actual onboard computing platform is 
expected to use a high-speed interconnect such as Myrinet. 
Between  one  and  two  megabytes  of  RAM  on  each 
processor were set aside to emulate local nonvolatile memory 
available  to  each  node.    The  nonvolatile RAM is expected to 
store  temporary  state  information  that  must  survive  hardware 
reboots 
information  needed  during 
recovery).  Nonvolatile memory visible to all nodes is emulated 
by  a  remote  file  system  residing  on  a  Sun  workstation  that 
stores  program  executables,  application 
input  data,  and 
application output data.   
(e.g.,  checkpointing 
REE  MPI  application.    A  Mars  Rover  texture  analysis 
program [5] was used as the workload application (results for 
executing an application from the OTIS mission in tandem with 
the texture analysis program can be found in [24]).  Cameras on 
the  Mars  Rover  take  images  of  the  Martian  surface  and  store 
the images on stable storage.  The program applies a series of 
filters  to  segment  the  image  according  to  texture  features.  
Three filters extract vectors that describe image features along 
each  of  its  three  axes.    A  statistical  clustering  algorithm  is 
applied  to  the  feature  vectors  in  order  to  segment  the  image 
(e.g., to distinguish between different rocks in the image), and 
an  output  of  the  segmented  image  in  feature  vector  space  is 
written  back  to  disk.    The  application  takes  rudimentary 
checkpoints by updating a status file after each filter completes.  
If  the  application  restarts,  it  can  skip  filters  that  have already 
completed, but it must redo any filtering that was interrupted by 
the application failure.  The application executes on two nodes 
and  analyzes  one  image  per  run  for  the  purposes  of  these 
experiments. 
2  SIFT Environment for REE 
The REE applications are protected by a SIFT environment 
designed  around  a  set  of  self-checking  processes  called 
ARMORS 
(Adaptive  Reconfigurable  Mobile  Objects  of 
Reliability) that execute on each node in the testbed.  ARMORs 
control  all  operations  in  the  SIFT  environment  and  provide 
error  detection  and  recovery  to  the  application  and  to  the 
ARMOR processes themselves.  We provide a brief summary of 
the  ARMOR-based  SIFT  environment  as  implemented  for  the 
REE  applications;  additional  details  of  the  general  ARMOR 
architecture appear in [12]. 
2.1  SIFT Architecture 
checkpointing 
An  ARMOR is a multithreaded process internally structured 
around  objects  called  elements  that  contain  their  own  private 
data  and  provide  elementary  functions  or  services  (e.g., 
detection  and  recovery  for  remote  ARMOR  processes,  internal 
self-checking  mechanisms,  or 
support).  
Together, the elements constitute the functionality that defines 
an  ARMOR’s  behavior.    All  ARMORs  contain  a  basic  set  of 
elements that provide a core functionality, including the ability 
to 
point-to-point  message 
communication  between  ARMORs,  (2)  communicate  with  the 
local  daemon  ARMOR  process,  (3)  respond  to  heartbeats  from 
the  local  daemon,  and  (4)  capture  ARMOR  state.    Specific 
ARMORs  extend 
this  core  functionality  by  adding  extra 
elements. 
implement 
reliable 
(1) 
Each  ARMOR  is  addressed  by  a  unique  identification 
number,  allowing  messages  to  be  sent  to  an  ARMOR  without 
prior  knowledge  of  the  ARMOR’s  physical  location.    ARMORs 
communicate  solely  through  message  passing,  and  messages 
are  processed  in  separate  threads  within  the  ARMOR.    A 
message  consists  of  sequential  events  that  trigger  element 
actions.  Elements subscribe to events that they are designed to 
process  (e.g.,  an  element  can  subscribe  to  an  event  that 
corresponds  to  the  termination  of  the  application),  and  an 
element’s state can only be modified while processing message 
events.    This  modular,  event-driven  architecture  permits  the 
ARMOR’s  functionality  and  fault  tolerance  services  to  be 
customized  by  choosing  the  particular  set  of  elements  that 
make up the ARMOR. 
Types  of  ARMORs.    The  SIFT  environment  for  REE 
applications consists of four kinds of ARMOR processes: a Fault 
Tolerance Manager (FTM), a Heartbeat ARMOR, daemons, and 
Execution ARMORs  
Fault Tolerance Manager (FTM).  A single FTM executes 
on  one  of  the  nodes  and  is  responsible  for  recovering  from 
ARMOR  and  node  failures  as  well  as  interfacing  with  the 
external  Spacecraft  Control  Computer  (SCC).    The  FTM 
contains all the basic ARMOR elements plus additional elements 
to (1) accept requests to execute applications from the SCC, (2) 
track  resource  usage  of  nodes  in  the  SIFT  environment,  (3) 
send  “Are-you-alive?”  messages  to  daemons  to  detect  node 
install  Execution  ARMORs  for  a  particular 
failures,  (4) 
application,  (5)  recover  from  failed  subordinate  ARMORs  (i.e., 
Execution  ARMORs    and  the  Heartbeat  ARMOR),  (6)  recover 
from node failures by migrating processes to another node, (7) 
recover  from  application  failures,  and  (8)  send  application 
status information to SCC.  
Heartbeat  ARMOR.    The  Heartbeat  ARMOR  executes  on  a 
node separate from the FTM.  Its sole responsibility is to detect 
and  recover  from  failures  in  the  FTM  through  the  periodic 
polling  for  liveness.    This  functionality  is  implemented  in  a 
single element that is added to the Heartbeat ARMOR beyond 
the basic set of elements found in all ARMORs. 
Daemons.    Each  node  on  the  network  executes  a  daemon 
process.    Daemons  are  the  gateways  for  ARMOR-to-ARMOR 
communication,  and  they  detect  failures  in the local  ARMORs.  
In  addition  to  the  core  ARMOR  configuration,  the  daemon 
contains  elements  that  permit  it  to  (1)  install  other  ARMOR 
processes on the node, (2) communicate with local ARMORs, (3) 
cache location of remote ARMORs, (4) route messages to remote 
ARMORs, (5) send “Are-you-alive?” inquires to local ARMORs to 
detect hang failures, (6) detect crash failures in local ARMORs, 
(7)  process  “Are-you-alive?”  inquires  from  the  FTM,  and  (8) 
notify the FTM to initiate recovery of failed local ARMORs. 
Execution  ARMORs.    Each  application  process  is  directly 
overseen by a local Execution ARMOR.  In addition to the core 
set of elements, an Execution ARMOR contains elements to (1) 
launch  application  processes,  (2)  detect  crash  failures  in 
application  processes,  (3)  handle  progress  indicator  updates 
from the application (to be described later), and (4) notify the 
FTM if the application process fails. 
The ARMOR architecture permits the functionality of several 
ARMORs to be merged into a single process.  For example, the 
functionality of the daemon and Execution ARMOR that execute 
on a node can be combined into a single ARMOR.  Although this 
reduces  the  number  of  processes  in  the  system,  there  are 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:44 UTC from IEEE Xplore.  Restrictions apply. 
drawbacks  to  consolidating  functionality.    Complexity  of  the 
combined process is increased, thus increasing the probability 
of  software  design  errors.    Moreover,  a  single  failure  in  the 
combined  process  will  affect  several  more  detection  and 
recovery  mechanisms  than  a  single  failure  in  which  the 
mechanisms are distributed across multiple processes. 
2.2  Executing REE Applications 
Before executing any applications, the SCC first performs a 
one-time  installation  of  the  daemons,  FTM,  and  Heartbeat 
ARMOR  on  the  REE  cluster. 
  The  SCC  then  launches 
applications  through  the  SIFT  environment,  prompting  the 
FTM to install Execution ARMORs on the appropriate nodes to 
support  the  application.    Table  1  lists  the  steps  involved  in 
executing  an  MPI  application, 
the  one-time 
installation  of  the  SIFT  environment.    If  the  application 
executes  perpetually,  then  the  Execution  ARMORs  are  never 
uninstalled;  otherwise,  they  are  removed  from  the  SIFT 
environment  after  the  application  completes.    If  several 
applications are executed sequentially, then the FTM can reuse 
Execution ARMORs across applications. 
including 
Figure 1 illustrates a configuration of the SIFT environment 
with  two  MPI  applications  (from  the  Mars  Rover  and  OTIS 
missions)  executing  on  a  four-node  testbed.    Arrows  in  the 
figure  depict  the  relationships  among  the  various  processes 
(e.g., the application sends progress indicators to the Execution 
ARMORs, the FTM is responsible for recovering from failures in 
the  Heartbeat  ARMOR,  and  the  FTM  heartbeats  the  daemon 
processes).    While  the  ARMORs  can  be  distributed  across  the 
REE  cluster  in  several  ways,  the  FTM  and  Heartbeat  ARMOR 
must reside on separate nodes to tolerate single-node failures.  
The entire SIFT environment can scale down to a minimal two-
node configuration if necessary: the FTM executing on the first 
node, the Heartbeat ARMOR on the second, and the other ARMOR 
and application processes distributed across both nodes. 
Each  application  process  is  linked  with  a  SIFT  interface 
that  establishes  a  one-way  communication  channel  with  the 
local  Execution  ARMOR  at  application  initialization.    The 
application  programmer  can  use  this  interface  to  invoke  a 
variety of fault tolerance services provided by the ARMOR.  The 
interface  used  for  these  experiments  contains  functions  for 
the  communication  channel,  using  progress 
initializing 
indicators 
to  detect  application  hangs,  and  closing 
the 
communication channel. 
As  described  in  Table  1,  the  Execution  ARMORs,  the 
Heartbeat ARMOR, and the FTM are children of their respective 