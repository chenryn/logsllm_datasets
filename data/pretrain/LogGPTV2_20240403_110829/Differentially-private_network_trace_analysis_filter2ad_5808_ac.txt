multiple resolutions and aggregates at most a logarithmic
number of measurements to reproduce the full set of val-
ues for the CDF. A recursive function to implement this
approach is:
CDF3(data, epsilon, max)
if (max == 0)
yield return data.Count(epsilon);
else
//--- partition data at max/2
var parts = data.Partition(new int[] { 0, 1 },
x => x/(max/2));
//--- emit counts for [0,max/2)
foreach (var x in CDF3(parts[0], epsilon, max/2))
yield return x;
//--- a cumulative count for [0,max/2)
var count = parts[0].Count(epsilon);
//--- emit frequencies for [max/2, max)
parts[1] = parts[1].Select(x => x - max/2);
foreach (var x in CDF3(parts[1], epsilon, max/2))
yield return x + count;
The standard deviation associated with each measurement
is proportional to log(|buckets|)3/2.
Figure 1 compares these three approaches to the actual
CDF for the time diﬀerence between a packet and its re-
transmission in the Hotspot trace. We discretize values to
1-ms granularity. The top graph shows that the error from
the ﬁrst approach is incredibly high, but the other two ap-
proaches are accurate. The bottom graph zooms in to show
the distinction between the latter two approaches. We see
that in the second approach yields a smoother estimate that
mimics reality but consistently underestimates because the
error accumulates across the range. (In a diﬀerent run, we
may see a consistent overestimation.) The errors with the
third approach are generally lower but could represent an
over- or under-estimation at individual points. In any case,
with both these approaches the errors are relatively small
and likely acceptable for most settings with modest abso-
lute counts.
A natural consequence of noisy measurement is that the
computed CDFs are not non-decreasing.
If needed, the
CDFs can be made non-decreasing through isotonic regres-
sion. Linear time algorithms (e.g., the “pool adjacent viola-
tors” algorithm [3]) can ﬁnd the non-decreasing curve that
minimizes the squared error distance from the input. Such
smoothing can also increase the accuracy in some cases (e.g.,
cdf3 in Figure 1). But it is a non-reversible removal of in-
formation, so we do not do it by default.
4.2 Finding Frequent (Sub)strings
Many analyses need to identify substrings or values that
occur frequently, for example, common payloads or addresses.
While this may seem at odds with privacy, the presence or
absence of individual records is not necessarily at risk; if a
particular string occurs a large number of times, it is essen-
127string
2D2816FECDCAB780
F389B84545A38BAF
E41903DCF7D86F2F
6F7E03DC833D6F2F
CD4F03DCE10E6F2F
B68503DCCA446F2F
58B403DC6C736F2F
41EA03DC55A96F2F
9FBB03DCB37A6F2F
7EEEB845D1088BAF
true count
3038504
92494
41600
40279
40084
37431
36526
29625
20715
18976
est. count % err
-0.000
0.012
0.017
0.022
0.009
0.047
0.033
-0.002
-0.018
0.025
3038500.005
92505.050
41606.893
40287.970
40087.437
37448.584
36537.877
29624.397
20711.169
18980.823
Table 4: True and noisy counts of the top-10 strings.
measurements with less accuracy as we face relatively fewer
opportunities for false positives.
We used the procedure above to ﬁnd the top 10 strings in
the payloads of the Hotspot trace. Table 4 shows the hash
value of the discovered strings, true and estimated counts,
and the relative error. We see that the top 10 strings are
discovered correctly, in order, and the error in the estimated
count is low. The number 10 was an arbitrary choice for
presentation; the computation produces counts for all strings
whose counts exceed a user speciﬁed threshold with a user
speciﬁed conﬁdence.
4.3 Frequent Itemset Mining
A recurring theme in many data analyses is that com-
monly co-occurring items are a possible indication of corre-
lation. The task of identifying frequently co-occurring items
across an input list of item sets is called frequent itemset
mining.
There are many algorithms for this task, including the
popular apriori algorithm [1]. It starts with a collection of
singleton sets and counts the number of times each occurs.
Sets that have suﬃcient frequency are retained, and merged
to form sets of size two, and so on.
Thus, the insight underlying this algorithm is similar to
what we used for frequent substring counting. But a key
diﬀerence from a privacy perspective is that the records,
which are each essentially a set of items, must be partitioned
amongst the candidate itemsets; each record can only con-
tribute to the count for one candidate itemset even though
it may support several. Consequently, if there are too many
candidate itemsets it can be hard to assemble enough evi-
dence for any one candidate.
We get over this hurdle by aggressively restricting the can-
didate item sets with high thresholds, focusing the support
of the records and ensuring that we do not spread the counts
too thin. Counter-intuitively, these high thresholds allow us
to learn more. We omit implementation details for space
constraints.
As one brief example of its use, we use it to discover the
common sets of ports that are used simultaneously by hosts.
Our discovered sets were very close to reality. The top-
ﬁve, which are all correct, in the Hotspot trace are (22,80),
(25,22), (443,80), (445,139), and (993,22).
5. NETWORK TRACE ANALYSES
We now survey our experiences at reproducing several
analyses from the networking literature. We stress that
while we consider a wide range of analyses, our experiences
(a) Complete view of all three methods
(b) Zoomed in view of cdf2 and cdf3
Figure 1: Comparing three approaches for comput-
ing CDFs with the actual (noise-free) CDF. (a) The
complete view shows that the ﬁrst approach has high
error but the other two are indistinguishable from
the actual CDF. (b) The zoomed-in view shows the
error behavior of the last two approaches.
tially a statistical trend that need not reveal the presence or
absence of one of its representatives.
As a concrete example, consider the problem of learning
the common strings of length B bytes. We might partition
our set of packets by the all possible values, of which there
are at most 256B, and measure the number of records in
each bin. Although the privacy cost is not high, the compu-
tational cost is exorbitant for even small values of B.
Instead, we can reveal common strings by asking about
statistics of successive bytes. Initially, we partition the records
into 256 bins based on the ﬁrst byte, and count the number
of records in each bin.
parts = data.Partition(bytes, rec => rec.str[0])
foreach (var byte in bytes)
if (parts[byte].Count(epsilon) > threshold)
yield return parts[byte]
All common strings contribute to the counters associated
with their ﬁrst bytes, which should be noticeably non-zero.
Each byte with count greater than threshold can now be ex-
tended, by each of the 256 bytes, to form preﬁxes of length
two. Again we can partition and count, using our new can-
didates and the ﬁrst two bytes of each string, resulting in
a set of viable preﬁxes of length two. This process contin-
ues until length B, at which point the counts correspond to
the number of records with each distinct B byte string. We
would have ideally culled most of the strings along the way,
rather than at the very last step, as a monolithic partition
operation would do. While we incur a higher privacy cost,
due to the B rounds of interrogation, we can aﬀord to take
050100150200250Time diff (ms)050000100000CDFcdf1cdf2cdf3noise-free230235240245250Time diff (ms)4000040500410004150042000CDFcdf2cdf3noise-free128may not be representative. Moreover, our reproductions are
each only one of many possible ways of reproducing an anal-
ysis; diﬀerent ways of measuring the same quantity may lead
to diﬀerent results.
Table 2 summarizes our ﬁndings. “Expressibility” reﬂects
the faithfulness of our implementation to the original anal-
ysis, ignoring quantitative privacy constraints. That is, if
the privacy allotment was arbitrarily high, would we recon-
struct the original results or deviate from the speciﬁcation
of the original algorithm? To a ﬁrst order, we ﬁnd that we
are able to reproduce the analyses, though some ﬂexibility
is required in reproducing the spirit of the analysis, if not
the exact letter.
“High accuracy” indicates our qualitative assessment of
what privacy level yielded highly accurate results; stronger
privacy levels do not necessarily yield bad results (some do)
but do produce noticeably diﬀerent outputs.
In all cases,
medium privacy (= 1.0) produces admirable results. Pick-
ing an appropriate point in the privacy-accuracy trade-oﬀ
requires a more concrete understanding of the data’s sen-
sitivity and the value of accuracy, but our results suggest
several plausibly valuable locations on the privacy-accuracy
curve.
5.1 Packet-level Analyses
We now present our results in more detail, beginning with
packet-level analyses. Unless otherwise speciﬁed, we use the
Hotspot trace.
5.1.1 Packet-size and port distributions
Two common packet-level analyses are measuring the dis-
tribution of packet sizes and ports. These are easy to repro-
duce with the CDF computation methods that we described
earlier. We use the second method in our experiments.
Figure 2(a) shows the ﬁdelity of the CDFs of packet length
computed with the three values of  that provide diﬀerent
privacy strengths. The graph also shows the real, noise-free
CDF and error bars for each noisy CDF. We see that the
error is minimal even at the strongest privacy level. As one
measure of the overall accuracy, we compute the root mean
vnf [i] )2, where vp[i] and
square error (RMSE) as
vnf [i] are the private and noise-free values at index i. At
=0.1, the RMSE is only 0.01%.
q 1
n Σi(1 − vp[i]
This extremely low error implies that accurate results can
be obtained even with far less data. Indeed, when we restrict
our computations to only 1/10th of the data, the RMSE
increases to only 0.02%.
We also see that privately computed CDFs correctly cap-
ture the interesting features of the distribution, for example,
spikes at 40 and 1492 bytes. The former corresponds to TCP
acknowledgments with no data, and the latter to the maxi-
mum packet size with IEEE 802.3 (which is used for wireless
communication).
Figure 2(b) shows that similarly high-ﬁdelity result are
obtained for port distributions. At =0.1, the RMSE is only
0.07%. With 1/10th of the data, the RMSE is 0.7%. The
error for ports is more than that for packet lengths because
there are more unique ports, and thus there are in general
fewer packets that contribute to port frequencies.
While packet length and port distributions may not seem
the most exciting quantities, they are simply examples of
CDFs of arbitrary packet statistics. Computations using
more sensitive information (e.g., the CDF of the scores of
(a) Packet length (bytes)
(b) Ports
Figure 2: Packet length and port CDFs computed
without noise and with diﬀerent values of . The
curves (and error bars) are all indistinguishable.
a packet payload classiﬁer) are similarly straightforward for
an analyst to specify and to convince the data provider of
the privacy guarantees.
5.1.2 Worm ﬁngerprinting
We now consider a more complex packet-level analysis
which looks closely at packet payloads and depends criti-
cally on this very sensitive data. Automated worm ﬁnger-
printing [27] examines a stream of packets for frequently oc-
curring payload substrings, with an additional “dispersion”
requirement that the substring is originated by and destined
to many distinct IP addresses.
The PINQ fragment grouping the packets by payload and
restricting to those with the appropriate dispersion proper-
ties is:
trace.GroupBy(pkt => pkt.payload)
.Where(grp => grp.Select(pkt => pkt.srcIP)
.Distinct()
.Count() > srcthreshold)
.Where(grp => grp.Select(pkt => pkt.dstIP)
.Distinct()
.Count() > dstthreshold)
These packet groups are still hidden behind the privacy cur-
tain, and while we could count the groups (2739 ± 10, with
thresholds at 5), or consider other statistics thereof, we can-
not (yet) directly view them.
To read out the interesting payloads, we leverage their fre-
quency in the data set. We use the frequent string ﬁnding
technique (§4.2) to spell out payloads that appear a signif-
icant number of times. This produces a list of candidate
payloads, from which we want to evaluate each to see if it
might be deemed suspicious. A simple PINQ fragment to
produce the number of distinct destinations associated with
each candidate payload is:
0500100015000246CDF (million)epsilon=0.1epsilon=1epsilon=10noise-free02000040000600000246CDF (million)epsilon=0.1epsilon=1epsilon=10noise-free129parts = trace.Partition(candidates, x => x.payload);
foreach (var candidate in candidates)