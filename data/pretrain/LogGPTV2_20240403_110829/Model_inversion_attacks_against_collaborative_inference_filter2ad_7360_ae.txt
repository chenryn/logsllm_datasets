### Network vs. Shadow Model Reconstruction

For the query-free setting, shadow model reconstruction is the only applicable method. However, for other settings, network-based methods are generally recommended over shadow model reconstruction. The Inverse-Network approach yields consistent results in both white-box and black-box scenarios.

### 7.4 Defenses

Current privacy-preserving algorithms and systems primarily focus on protecting training data. Here, we propose several defense strategies to mitigate inference privacy attacks:

1. **Fully-Connected Layers Before Output**:
   - **Description**: Deploying a fully-connected layer before sending out results can mix up inputs and obscure information about the inference samples.
   - **Implementation**: The model owner can add at least one fully-connected layer on the first trusted participant. This makes it difficult for adversaries to recover the input.
   - **Considerations**: Typically, fully-connected layers follow convolutional layers in a DNN. This requires computing all convolutional layers on the client-side, which may be computationally heavy for edge devices.

2. **Deeper Client-Side Network**:
   - **Description**: As shown in Figures 11 and 12, increasing the depth of the client-side network degrades the quality of the inversed images.
   - **Implementation**: Making the client-side network deeper can help mitigate attacks.
   - **Considerations**: Deeper networks increase the computational load on the client-side, which may not have sufficient resources (computation, storage, or battery) for this mitigation strategy.

3. **Trusted Execution on Untrusted Participants**:
   - **Description**: Using hardware support for a Trusted Execution Environment (TEE), such as Intel SGX or ARM TrustZone, can secure remote computation and protect data confidentiality against privileged adversaries.
   - **Implementation**: The inference application can be deployed inside a TEE (or secure enclave) on untrusted participants, with intermediate values encrypted during transmission.
   - **Considerations**: This requires special architecture support on the cloud side and careful crypto key management.

4. **Differential Privacy**:
   - **Description**: Adding noise to the inference input or intermediate values can obfuscate sensitive information.
   - **Implementation**: Add noise to the inference input: \( v = f_{\theta_1}(x + \epsilon) \). Alternatively, add noise directly to the intermediate value before sending it to the untrusted participant: \( v = f_{\theta_1}(x) + \epsilon \).
   - **Considerations**: There is a trade-off between usability and privacy: higher levels of noise can reduce model accuracy.

5. **Homomorphic Encryption**:
   - **Description**: Homomorphic encryption allows the inference application to perform DNN computations on encrypted input, preventing sensitive information from being leaked.
   - **Implementation**: The untrusted participant can directly compute on encrypted data.
   - **Considerations**: Homomorphic encryption is highly inefficient and not applicable to all DNN operations.

### 8. Related Work

#### 8.1 Machine Learning Privacy Attacks

- **Training Data Privacy Attacks**:
  - **Property Inference Attacks**: Infer properties of the training data from model parameters [3, 12].
  - **Membership Inference Attacks**: Determine if a sample was part of the training set [41, 39, 30, 18, 29, 32].
  - **Model Inversion Attacks**: Recover features of training samples given the model and partial features [10, 9, 22].
  - **Model Encoding Attacks**: Encode sensitive data into the model for retrieval by a receiver [42].

- **Model Privacy Attacks**:
  - **Model Stealing**: Adversaries attempt to steal model parameters, hyperparameters, or structures via prediction APIs, memory access side channels, etc. [44, 46, 23, 33].

- **Inference Data Privacy Attacks**:
  - **Power Side-Channel Attacks**: Use power side-channel information to recover inference data [48]. Our attack, however, targets collaborative systems and can recover complex data without direct access to the victim's computation and data.

#### 8.2 Machine Learning Privacy Solutions

- **Enhancing Algorithms**:
  - **Distributed Training**: Protects training data by allowing different participants to use their own data [15, 40].
  - **SGX Security Enclaves**: Protect training tasks against privileged adversaries [24, 34].
  - **Removing Sensitive Effects**: Methodology to remove the effects of sensitive training samples [5].
  - **Differential Privacy**: Adds noise to the stochastic gradient descent process to eliminate parameter dependency on training data [2].

- **Enhancing Datasets**:
  - **Data Encryption**: Encrypt data before feeding it into the training algorithm [4].
  - **Noise Addition**: Adding noise to the training dataset to protect privacy [51].
  - **Differentially Private GANs**: Generate artificial data for training while removing sensitive information [45, 52].

### 9. Conclusions

While the privacy threats of training data in deep learning are well-studied, the privacy of inference data remains underexplored. This paper investigates the feasibility of recovering sensitive data during the deep learning inference process. We find that in a collaborative inference system, an adversary controlling one participant can easily recover inference samples from intermediate values. We propose new attack techniques and systematically compare them, demonstrating that adversaries can reliably recover inputs with minimal prerequisites.

We hope this study highlights the importance of inference data privacy protection. For instance, when selecting the split point for edge-cloud offloading, previous work focused on power and performance. Given the feasibility of stealing inference data, privacy should also be a critical factor in partitioning neural networks. Future work could explore the trade-offs among power, performance, and security for edge-cloud offloading, develop more powerful attacks, and implement possible defense mechanisms.

### References

[References listed as provided, with proper formatting and citation details]

This optimized version aims to enhance clarity, coherence, and professionalism, making the text more accessible and informative.