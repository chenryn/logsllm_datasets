Network is recommended over shadow model reconstruction.
For the query-free setting, shadow model reconstruction is
the only applicable method.
1Inverse-Network gives the same results for both white-box and black-
box settings.
7.4 Defenses
Since current privacy-preserving algorithms and systems all
focus on training data, we provide some possible defense
strategies to mitigate the inference privacy attacks discussed
in this paper.
Running fully-connected layers before sending out re-
sults. As shown in Section 7.1, a fully-connected layer can
mix up inputs, and hide information about the inference sam-
ples. So a model owner can deploy at least one such layer
on the first trusted participant. This makes it very difficult
for the adversary to recover the input. Typically, the fully-
connected layers follow convolutional layers in a DNN. This
requires computing all convolutional layers on the client-
side, which can be heavy for an edge device.
Make the client-side network deeper. As illustrated in
Figures 11 and 12, both qualitative and quantitative measure-
ments of the inversed images become worse as the network
becomes deeper. Therefore, making the client-side network
deeper can help mitigate the attacks. On the other hand,
deeper networks increase the compution on the client-side.
The client device or IoT device may not have sufficient com-
putation, storage or battery resources for this, nor for the
above mitigation strategy.
Trusted Execution on untrusted participants. The hard-
ware support for a Trusted Execution Environment (TEE),
e.g., Intel SGX, ARM TrustZone, is effective at secure remote
computation and data confidentiality protection against priv-
ileged adversaries. In the case of collaborative DNN compu-
tation, the inference application can be deployed inside a
TEE (or secure enclave) on the untrusted participants, and
the intermediate values are encrypted against the adversary
during transmission between participants. This can provide
privacy protection for the inference data. However, this re-
quires special architecture support on the cloud side, and
careful crypto key management.
Differential privacy. We can use differential privacy to
add noise and obfuscate sensitive information. Specifically,
we can add noise to the inference input, and the intermediate
value becomes v = fθ1(x + ϵ ). We can also add noise directly
to the intermediate value before sending it to the untrusted
participant: v = fθ1(x ) +ϵ. In these two cases ϵ is the random
noise that satisfies differential privacy. It is obvious that there
exists a trade-off between usability and privacy: as a higher
level of noise is added, the model accuracy may drop.
Homomorphic encryption. This allows the inference ap-
plication on the untrusted participant to directly perform
DNN computations on encrypted input, so the sensitive in-
formation will not be leaked. A drawback of homomorphic
encryption is that it suffers from huge inefficiency and is not
applicable for all DNN operations.
8 RELATED WORK
8.1 Machine Learning Privacy Attacks
Training data privacy attacks. There are different types
of privacy attacks against the training data. The first type is
property inference attacks, which tries to infer some proper-
ties of the training data from the model parameters. Attacks
were demonstrated in traditional machine learning classifiers
[3] and fully-connected neural networks [12].
A special case of property inference attacks is membership
inference attacks, which infers whether one individual sample
is included in the training set. This attack was first presented
in [41]. The following work explored the feasibility of attacks
with different adversary’s capabilities [39], model features
[30, 49], in Generative Adversarial Networks [18, 29], and
collaborative training systems [32].
The second type are model inversion attacks [10]: given a
machine learning model, and part of the training samples’ fea-
tures, the adversary can recover the rest of the features of the
samples. Advanced model inversion attacks were designed
to recover images from deep neural networks in single-party
systems [9], and collaborative learning systems [22].
The third type are model encoding attacks [42]: the adver-
sary with direct access to the training data can encode the
sensitive data into the model for a receiver entity to retrieve.
Model privacy attacks. The adversary attempts to steal the
model parameters [44], hyperparameters [46] or structures
[23, 33], via prediction APIs, memory access side channels,
etc.
Inference data privacy attacks. Closer to our study is the
work [48], which adopted a power side channel to recover
inference data. However, this attack required the adversary to
compromise the victim device for side-channel information
collection, and it could only recover simple images (single
pixel). Our attack is applied to the collaborative systems, and
can recover any arbitrary complex data without access or
knowledge of the victim’s computation and data.
8.2 Machine Learning Privacy Solutions
Current solutions only focus on training data protection:
Enhancing the algorithms. Distributed training was in-
troduced to protect the training data [15, 40], as different
participants can use their own data for model training. The
SGX security enclaves in Intel processors were used to pro-
tect the training tasks against privileged adversaries [24, 34].
Cao et al. [5] proposed a methodology to remove the effects
of sensitive training samples on the models. Abadi et al. [2]
applied differential privacy to add noise in the stochastic
gradient descent process to eliminate the parameters’ depen-
dency on the training data.
Enhancing the dataset. Bost et al. [4] proposed to encrypt
the data before feeding them into the training algorithm.
They designed machine learning operators which can op-
erate on the encrypted data. Zhang et al. [51] showed that
adding noise to the training dataset is effective in protect-
ing training data privacy. Generative Adversarial Network
with differential-privacy is adopted [45, 52] to generate arti-
ficial data for training DNN models while removing sensitive
information from the original data.
9 CONCLUSIONS
While the privacy threat of training data in deep learning is
well studied, and defenses have been investigated, the pri-
vacy of inference data is less studied. In this paper, we explore
the feasibility of recovering sensitive data in the deep learn-
ing inference process. We discover that in a collaborative
inference system, an adversary who controls one participant
can easily recover the inference samples from intermedi-
ate values. We propose a new set of attack techniques to
compromise the inference data privacy in collaborative deep
learning systems, under different attack settings. We system-
atically compare these different techniques, demonstrating
that the adversary can successfully and reliably recover the
inputs with very few prerequisites.
We hope that the importance of inference data privacy
protection can be addressed through this study. For instance,
when selecting the split point for edge-cloud offloading, pre-
vious work only considered the power and performance
requirements. With the feasibility of stealing the inference
data, privacy should also be an important factor for partition-
ing the neural network. Future work could include the study
of the trade-off among power, performance and security for
edge-cloud offloading, exploration of more powerful attacks,
and realization of possible defense mechanisms.
REFERENCES
[1] 2018. https://pytorch.org/docs/0.4.0/torchvision/datasets.html.
[2] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya
Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differ-
ential privacy. In ACM Conference on Computer and Communications
Security.
[3] Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Vil-
lani, Domenico Vitali, and Giovanni Felici. 2015. Hacking smart ma-
chines with smarter ones: How to extract meaningful data from ma-
chine learning classifiers. International Journal of Security and Networks
(2015).
[4] Raphael Bost, Raluca Ada Popa, Stephen Tu, and Shafi Goldwasser.
2015. Machine learning classification over encrypted data.. In Network
and Distributed System Security Symposium.
[5] Yinzhi Cao and Junfeng Yang. 2015. Towards Making Systems Forget
with Machine Unlearning. In IEEE Symposium on Security and Privacy.
[6] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalya-
naraman. 2014. Project adam: Building an efficient and scalable deep
learning training system. In USENIX Symposium on Operating Systems
Design and Implementation.
[7] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin,
Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al.
2012. Large scale distributed deep networks. In Advances in neural
information processing systems.
[8] Amir Erfan Eshratifar, Mohammad Saeed Abrishami, and Massoud
Pedram. 2018.
JointDNN: an efficient training and inference en-
gine for intelligent mobile cloud computing services. arXiv preprint
arXiv:1801.08618 (2018).
[9] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model
inversion attacks that exploit confidence information and basic coun-
termeasures. In ACM Conference on Computer and Communications
Security.
[10] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page,
and Thomas Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-
End Case Study of Personalized Warfarin Dosing.. In USENIX Security
Symposium.
[11] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov.
2018. Property Inference A acks on Fully Connected Neural Networks
using Permutation Invariant Representations. In ACM Conference on
Computer and Communications Security.
[12] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov.
2018. Property Inference Attacks on Fully Connected Neural Networks
using Permutation Invariant Representations. In ACM conference on
computer and communications security. 619–633.
[13] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty
of training deep feedforward neural networks. In Proceedings of the
thirteenth international conference on artificial intelligence and statistics.
249–256.
[14] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
2016. Deep learning. Vol. 1. MIT press Cambridge.
[15] Jihun Hamm, Adam C Champion, Guoxing Chen, Mikhail Belkin, and
Dong Xuan. 2015. Crowd-ml: A privacy-preserving learning frame-
work for a crowd of smart devices. In IEEE International Conference on
Distributed Computing Systems.
[16] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg
Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sen-
gupta, Adam Coates, and Andrew Y. Ng. 2014. Deep Speech: Scal-
ing Up End-to-end Speech Recognition. CoRR abs/1412.5567 (2014).
arXiv:1412.5567 http://arxiv.org/abs/1412.5567
[17] Johann Hauswald, Thomas Manville, Qi Zheng, Ronald Dreslinski,
Chaitali Chakrabarti, and Trevor Mudge. 2014. A hybrid approach
to offloading mobile image classification. In 2014 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,
8375–8379.
[18] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro.
2017. LOGAN: evaluating privacy leakage of generative models us-
ing generative adversarial networks. arXiv preprint arXiv:1705.07663
(2017).
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep
Residual Learning for Image Recognition. CoRR abs/1512.03385 (2015).
arXiv:1512.03385 http://arxiv.org/abs/1512.03385
[20] Zecheng He, Aswin Raghavan, Guangyuan Hu, Sek Chai, and Ruby
Lee. 2019. Power-Grid Controller Anomaly Detection with Enhanced
Temporal Deep Learning. In 18th IEEE International Conference on
Trust, Security and Privacy in Computing and Communications.
[21] Zecheng He, Tianwei Zhang, and Ruby Lee. 2019. Sensitive-Sample
Fingerprinting of Deep Neural Networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 4729–4737.
[22] Briland Hitaj, Giuseppe Ateniese, and Fernando Pérez-Cruz. 2017.
Deep models under the GAN: information leakage from collaborative
deep learning. In ACM Conference on Computer and Communications
Security.
[23] Weizhe Hua, Zhiru Zhang, and G Edward Suh. 2018. Reverse engineer-
ing convolutional neural networks through side-channel information
leaks. In ACM/ESDA/IEEE Design Automation Conference.
[24] Tyler Hunt, Congzheng Song, Reza Shokri, Vitaly Shmatikov, and
Emmett Witchel. 2018. Chiron: Privacy-preserving Machine Learning
as a Service. arXiv preprint arXiv:1803.05961 (2018).
[25] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor
Mudge, Jason Mars, and Lingjia Tang. 2017. Neurosurgeon: Collabo-
rative intelligence between the cloud and mobile edge. Acm Sigplan
Notices 52, 4 (2017), 615–629.
[26] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 (2014).
[27] Jong Hwan Ko, Taesik Na, Mohammad Faisal Amir, and Saibal
Mukhopadhyay. 2018. Edge-host partitioning of deep neural net-
works with feature space encoding for resource-constrained internet-
of-things platforms. In IEEE International Conference on Advanced Video
and Signal Based Surveillance.
[28] Yann Le Cun, LD Jackel, B Boser, JS Denker, HP Graf, I Guyon, D
Henderson, RE Howard, and W Hubbard. 1989. Handwritten Digit
Recognition: Applications of Neural Network Chips and Automatic
Learning. IEEE Communications Magazine 27, 11 (1989), 41–46.
[29] Kin Sum Liu, Bo Li, and Jie Gao. 2018. Generative Model: Membership
Attack, Generalization and Diversity. arXiv preprint arXiv:1805.09898
(2018).
[30] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng
Wang, Haixu Tang, Carl A Gunter, and Kai Chen. 2018. Understanding
Membership Inferences on Well-Generalized Learning Models. arXiv
preprint arXiv:1802.04889 (2018).
[31] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015.
Effective Approaches to Attention-based Neural Machine Translation.
CoRR abs/1508.04025 (2015). arXiv:1508.04025 http://arxiv.org/abs/
1508.04025
[32] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly
Shmatikov. 2019. Exploiting unintended feature leakage in collab-
orative learning. In IEEE Symposium on Security and Privacy.
[33] Seong Joon Oh, Max Augustin, Mario Fritz, and Bernt Schiele. 2018. To-
wards reverse-engineering black-box neural networks. In INternational
Conference on Learning Representations.
[34] Olga Ohrimenko, Felix Schuster, Cédric Fournet, Aastha Mehta, Se-
bastian Nowozin, Kapil Vaswani, and Manuel Costa. 2016. Oblivious
Multi-Party Machine Learning on Trusted Processors.. In USENIX Se-
curity Symposium.
[35] Herbert Robbins and Sutton Monro. 1951. A stochastic approximation
method. The annals of mathematical statistics (1951), 400–407.
[36] Frank Rosenblatt. 1958. The Perceptron: A Probabilistic Model for
Information Storage and Organization in the Brain. Psychological
review 65, 6 (1958), 386.
[37] Leonid I Rudin, Stanley Osher, and Emad Fatemi. 1992. Nonlinear
total variation based noise removal algorithms. Physica D: nonlinear
phenomena 60, 1-4 (1992), 259–268.
[38] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986.
Learning Representations by Back-propagating Errors. nature 323,
6088 (1986), 533.
[39] Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael
Backes. 2018. ML-Leaks: Model and Data Independent Membership In-
ference Attacks and Defenses on Machine Learning Models. In Network
and Distributed System Security Symposium.
[40] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving deep
learning. In ACM conference on computer and communications security.
[41] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
2017. Membership inference attacks against machine learning models.
In IEEE Symposium on Security and Privacy.
[42] Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. 2017. Ma-
chine Learning Models that Remember Too Much. In ACM Conference
on Computer and Communications Security.
[43] Surat Teerapittayanon, Bradley McDanel, and HT Kung. 2017. Dis-
tributed deep neural networks over the cloud, the edge and end devices.
In IEEE International Conference on Distributed Computing Systems.
[44] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas
Ristenpart. 2016. Stealing Machine Learning Models via Prediction
APIs.. In USENIX Security Symposium.
[45] Aleksei Triastcyn and Boi Faltings. 2018. Generating Artificial Data
for Private Deep Learning. arXiv preprint arXiv:1803.03148 (2018).
[46] Binghui Wang and Neil Zhenqiang Gong. 2018. Stealing Hyperpa-
rameters in Machine Learning. In IEEE Symposium on Security and
Privacy.
[47] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al.
Image quality assessment: from error visibility to structural
2004.
[48] Lingxiao Wei, Bo Luo, Yu Li, Yannan Liu, and Qiang Xu. 2018.
similarity. IEEE transactions on image processing 13, 4 (2004), 600–612.
I
know what you see: Power side-channel attack on convolutional neu-
ral network accelerators. In Annual Computer Security Applications
Conference.
[49] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha.
2018. Privacy Risk in Machine Learning: Analyzing the Connection to
Overfitting. In IEEE Computer Security Foundations Symposium.
[50] Hongxu Yin, Zeyu Wang, and Niraj K Jha. 2018. A hierarchical infer-
ence model for internet-of-things. IEEE Transactions on Multi-Scale
Computing Systems 4, 3 (2018), 260–271.
[51] Tianwei Zhang, Zecheng He, and Ruby B Lee. 2018.
Privacy-
preserving machine learning through data obfuscation. arXiv preprint
arXiv:1807.01860 (2018).
[52] Xinyang Zhang, Shouling Ji, and Ting Wang. 2018. Differentially
Private Releasing via Deep Generative Model (Technical Report). arXiv
preprint arXiv:1801.01594 (2018).