title:Short vs. Long Flows: A Battle That Both Can Win
author:Morteza Kheirkhah and
Ian Wakeman and
George Parisis
Short vs. Long Flows: A Battle That Both Can Win
Morteza Kheirkhah, Ian Wakeman and George Parisis
{m.kheirkhah, ianw, g.parisis}@sussex.ac.uk
School of Engineering and Informatics
University of Sussex, UK
ABSTRACT
In this paper, we introduce MMPTCP, a novel transport
protocol which aims at unifying the way data is transported
in data centres. MMPTCP runs in two phases; initially, it
randomly scatters packets in the network under a single con-
gestion window exploiting all available paths. This is ben-
eﬁcial to latency-sensitive ﬂows. During the second phase,
MMPTCP runs in Multi-Path TCP (MPTCP) mode, which
has been shown to be very eﬃcient for long ﬂows.
Initial
evaluation shows that our approach signiﬁcantly improves
short ﬂow completion times while providing high through-
put for long ﬂows and high overall network utilisation.
CCS Concepts
•Networks → Data center networks; Transport pro-
tocols; Network performance analysis; Network simulations;
Keywords
Data Center; Multi-Path TCP; Packet Scatter; ns-3
1.
INTRODUCTION
Modern data centres [1, 3] provide very high aggregate
bandwidth and multiple paths among servers. They sup-
port a large number of services which produce very diverse
intra-data centre traﬃc matrices. Long ﬂows are bandwidth
hungry, while short ones commonly come with strict dead-
lines regarding their completion time.
It has been shown
that TCP is ill-suited for both types of traﬃc in modern
data centres, where ECMP [4] is used to exploit the avail-
ability of multiple equal-cost paths. Under high load, long
ﬂows collide with high probability and, as a result, net-
work utilisation signiﬁcantly drops and only 10% of the ﬂows
achieve their expected throughput [6]. TCP is also ineﬃcient
for short ﬂows, especially when competing with long ﬂows.
Queue build-ups, buﬀer pressure and TCP Incast combined
with the shared-memory nature of data centre switches re-
sults in short TCP ﬂows missing their deadlines mainly due
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ’15 August 17-21, 2015, London, United Kingdom
c(cid:13) 2015 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-3542-3/15/08.
DOI: http://dx.doi.org/10.1145/2785956.2790018
to retransmission timeouts (RTOs) [2].
To cope with these challenges, several transport protocols
have been recently proposed. DCTCP [2], D2TCP [7] and
D3 [8] aim at minimising ﬂow completion times for latency-
sensitive ﬂows. However, they require modiﬁcations in the
network and/or deadline-awareness at the application layer.
Such modiﬁcations are problematic because such informa-
tion may not be known a priori (i.e. at connection time).
More importantly, all of these protocols are single-path and
thus cannot exploit the multipath potential of data centre
networks. Multipath transport protocols, such as Multi-
Path TCP [6], transport data using multiple sub-ﬂows and
rely on ECMP to achieve higher aggregated throughput over
multiple paths. As shown in [6], MPTCP improves the over-
all network utilisation. However, MPTCP hurts short ﬂows
as the number of sub-ﬂows increases. The congestion win-
dow of a sub-ﬂow may be very small over the sub-ﬂow life-
time and, as a result, even a single lost packet can force an
entire connection to wait for an RTO to be triggered because
the lost packet cannot be recovered through fast retransmis-
sion. This is clearly illustrated in Figure 1(a)1, where the
mean ﬂow completion time of short ﬂows increases as more
sub-ﬂows are used (better shown in the embedded Figure
in Figure 1(a)). Note that the number of connections that
experience one or more RTOs signiﬁcantly increases as well,
hence the increase in the standard deviation. Even a single
RTO may result in ﬂow deadline violation.
Supporting and running multiple transport protocols in a
data centre can be problematic. Fairness among diﬀerent
protocols is diﬃcult to achieve; most protocols for latency-
sensitive ﬂows are not compatible with TCP or MPTCP [2,
7]. Running multiple transport protocols is also a burden
for application developers who would have to decide upon
the most suitable transport protocol. Both application re-
quirements and data centre topologies evolve over time and
so a transport protocol that performs well over disparate
topologies and traﬃc matrices is a necessity.
In this paper, we introduce MMPTCP, a transport pro-
tocol that aims at unifying data transport within data cen-
tres. MMPTCP objectives are: (1) high throughput for
large ﬂows, (2) low latency for short ﬂows, and (3) tolerance
to sudden and high bursts of traﬃc, all without application-
layer information (e.g. ﬂow size and deadline). Co-existence
in harmony with TCP and MPTCP ﬂows is also an objec-
tive. Data transport takes place in two phases.
Initially,
packets are randomly scattered in the network under a sin-
1All simulations are based on our custom implementation of
MPTCP and MMPTCP in ns-3 [5]
349(a) MPTCP
(b) MPTCP (8 subﬂows)
(c) MMPTCP (PS and 8 subﬂows)
Figure 1: Short ﬂow completion times in a simulated 4:1 over-subscribed FatTree topology consisting of 512
servers. One third of the servers run long (background) ﬂows. The rest run short ﬂows (70KBs each) which
are scheduled according to a Poisson process). All ﬂows are scheduled based on a permutation traﬃc matrix.
gle TCP congestion window exploiting all available paths.
Most, if not all, short ﬂows are expected to complete be-
fore switching to the second phase, during which, MMPTCP
runs as standard MPTCP, eﬃciently handling long ﬂows.
2. MMPTCP DESIGN
Packet Scatter (PS) Phase. The PS protocol applica-
bility to data centre networks has been brieﬂy explored in
[6], where it has been shown that it can eliminate network
congestion at the network core.
In our approach packet
scattering is initiated at the end hosts through source port
randomisation. Network switches then forward packets to
all available paths employing hash-based ECMP. The main
challenge here is the graceful handling of out-of-order pack-
ets. We are currently exploring the following approaches:
(1) Dynamically assigning the duplicate ACK threshold us-
ing topology-speciﬁc information. For example, FatTree’s IP
addressing scheme can be exploited to calculate the number
of available paths between the sender and receiver. Other
data centre topologies, such as VL2, incorporate centralised
components which can provide similar information. (2) Al-
ternatively, approaches such as RR-TCP [9] could be used to
minimise spurious retransmissions when out-of-order pack-
ets are misidentiﬁed as lost.
Phase Switching. Switching to MPTCP at the right time
is crucial to ensuring that short ﬂows complete very fast
(before switching), while long ﬂows are not hurt by running
with a single congestion window for long. We are currently
investigating two switching strategies.
(1) Data Volume:
Switching occurs when a certain amount of data has been
transmitted. Early evaluation suggests that this approach
does not exert any negative eﬀects on the throughput of long
ﬂows since the opening of multiple sub-ﬂows after switching
can wrap up access link capacity in a few RTTs. (2) Con-
gestion Event: Switching occurs when congestion is ﬁrst in-
ferred (i.e. when fast retransmission or an RTO is triggered).
MPTCP Phase. When the switching threshold is reached,
MMPTCP initiates a number of sub-ﬂows and data trans-
port is governed by MPTCP’s congestion control. No more
packets are put in the initial PS ﬂow which is deactivated
when its window gets emptied.
3. DISCUSSION AND FUTURE WORK
Figures 1(b) and 1(c) depict the ﬂow completion times for
all short ﬂows in a simulated FatTree topology (see Figure 1
caption for details of the simulation setup). It is clear that
with MPTCP (1(b)) a lot more short ﬂows experience one
or more RTOs leading to very high completion times, com-
pared to MMPTCP (1(c)). During the packet scatter phase,
MMPTCP utilises all available paths to distribute packets to
receivers, while using a single congestion window, gracefully
handling sudden bursts. Note that the majority of short
ﬂows completed within 100ms. The average ﬂow completion
time and the standard deviation for MMPTCP and MPTCP
are 116 milliseconds (standard deviation is 101) and 126 mil-
liseconds (standard deviation is 425), respectively.
Roadmap. Using our custom ns-3 models for MPTCP and
MMPTCP, we are currently simulating several data centre
topologies, comparing MMPTCP to other transport pro-
tocols in a wide range of network scenarios (e.g. eﬀect of
hotspots, network loads, traﬃc matrices and phase switch-
ing strategies). We also plan to design multi-homed network
topologies as these are well-suited to MMPTCP. The more
parallel paths at the access layer, the higher the burst tol-
erance; hence, potentially, preventing transient congestion.
We expect that MMPTCP will be readily deployable in
existing data centres as it can coexist with other transport
protocols. In-depth investigation of how MMPTCP shares
network resources with TCP and MPTCP is part of our
current work. Early results suggest that it could co-exist
in harmony with them. MMPTCP requires ECMP, which is
deployed in all data centres, and doesn’t rely on any changes
in the network or any application-layer information.
4. REFERENCES
[1] M. Al-Fares et al. A Scalable, Commodity Data Center
Network Architecture. In Proc. of SIGCOMM 2008.
[2] M. Alizadeh et al. Data Center TCP (DCTCP). In Proc. of
SIGCOMM 2010.
[3] A. Greenberg et al. VL2: A Scalable and Flexible Data
Center Network. In Proc. of SIGCOMM 2011.
[4] C. Hopps. Analysis of an equal-cost multi-path algorithm.
RFC 3782, 2004.
[5] M. Kheirkhah et al. Multipath TCP model in ns-3. In WNS3
2014, https://github.com/mkheirkhah/mptcp.
[6] C. Raiciu et al. Improving Datacenter Performance and
Robustness with Multipath TCP. In Proc. of SIGCOMM
2011.
[7] B. Vamanan et al. Deadline-aware Datacenter TCP
(D2TCP). In Proc. of SIGCOMM 2010.
[8] C. Wilson et al. Better Never than Late: Meeting Deadlines
in Datacenter Networks. In Proc. of SIGCOMM 2011.
[9] M. Zhang et al. RR-TCP: A Reordering-Robust TCP with
DSACK. In Proceedings of ICNP 2003.
 0 100 200 300 400 500 600 700123456789Milliseconds# subflowsStandard DeviationMean Completion Time 80 100 120 140123456789ms 0 2 4 6 8 1020K40K60K80K100KCompletion Time (sec)Flow Id 0 2 4 6 8 1020K40K60K80K100KCompletion Time (sec)Flow Id350