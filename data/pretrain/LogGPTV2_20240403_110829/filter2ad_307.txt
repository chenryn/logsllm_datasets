# Title: Short vs. Long Flows: A Battle That Both Can Win

## Authors:
Morteza Kheirkhah, Ian Wakeman, and George Parisis  
{m.kheirkhah, ianw, g.parisis}@sussex.ac.uk  
School of Engineering and Informatics, University of Sussex, UK

## Abstract
In this paper, we introduce MMPTCP, a novel transport protocol designed to unify data transport in data centers. MMPTCP operates in two phases: initially, it randomly scatters packets across the network using a single congestion window, leveraging all available paths, which benefits latency-sensitive flows. In the second phase, MMPTCP switches to Multi-Path TCP (MPTCP) mode, known for its efficiency with long flows. Our initial evaluation demonstrates that MMPTCP significantly improves short flow completion times while providing high throughput for long flows and high overall network utilization.

## CCS Concepts
- **Networks** → Data center networks; Transport protocols; Network performance analysis; Network simulations

## Keywords
Data Center, Multi-Path TCP, Packet Scatter, ns-3

## 1. Introduction
Modern data centers [1, 3] offer high aggregate bandwidth and multiple paths between servers, supporting a wide range of services with diverse intra-data center traffic patterns. Long flows are typically bandwidth-intensive, while short flows often have strict deadlines for completion. It has been shown that traditional TCP is not well-suited for both types of traffic in modern data centers, where Equal-Cost Multi-Path (ECMP) [4] is used to exploit multiple equal-cost paths. Under high load, long flows frequently collide, leading to a significant drop in network utilization, with only 10% of flows achieving their expected throughput [6]. TCP is also inefficient for short flows, especially when competing with long flows. Queue build-ups, buffer pressure, and TCP Incast, combined with the shared-memory nature of data center switches, result in short TCP flows missing their deadlines, primarily due to retransmission timeouts (RTOs) [2].

To address these challenges, several transport protocols have been proposed. DCTCP [2], D2TCP [7], and D3 [8] aim to minimize flow completion times for latency-sensitive flows but require modifications in the network or application-layer deadline awareness, which can be problematic. These protocols are also single-path and cannot fully leverage the multipath potential of data center networks. Multipath transport protocols like MPTCP [6] use multiple sub-flows and ECMP to achieve higher aggregated throughput. However, MPTCP can negatively impact short flows as the number of sub-flows increases, as illustrated in Figure 1(a). The congestion window of a sub-flow may be very small, causing even a single lost packet to trigger an RTO, significantly increasing the mean flow completion time and standard deviation.

Running multiple transport protocols in a data center can be challenging, as fairness among different protocols is difficult to achieve, and most protocols for latency-sensitive flows are not compatible with TCP or MPTCP [2, 7]. Additionally, application developers must choose the most suitable transport protocol, which can be burdensome given the evolving nature of application requirements and data center topologies.

In this paper, we introduce MMPTCP, a transport protocol designed to unify data transport within data centers. MMPTCP aims to provide high throughput for large flows, low latency for short flows, and tolerance to sudden and high bursts of traffic, all without requiring application-layer information (e.g., flow size and deadline). Coexistence with TCP and MPTCP flows is also a key objective. Data transport occurs in two phases: initially, packets are randomly scattered in the network under a single TCP congestion window, exploiting all available paths. Most short flows are expected to complete before switching to the second phase, during which MMPTCP operates in MPTCP mode, efficiently handling long flows.

## 2. MMPTCP Design

### Packet Scatter (PS) Phase
The applicability of the PS protocol to data center networks has been explored in [6], showing that it can eliminate network congestion at the core. In our approach, packet scattering is initiated at the end hosts through source port randomization. Network switches then forward packets to all available paths using hash-based ECMP. The main challenge is the graceful handling of out-of-order packets. We are currently exploring two approaches:
1. **Dynamic Duplicate ACK Threshold Assignment**: Using topology-specific information, such as FatTree’s IP addressing scheme, to calculate the number of available paths.
2. **RR-TCP [9]**: Minimizing spurious retransmissions by handling out-of-order packets more effectively.

### Phase Switching
Switching to MPTCP at the right time is crucial to ensure that short flows complete quickly and long flows are not adversely affected by running with a single congestion window. We are investigating two switching strategies:
1. **Data Volume**: Switching occurs after a certain amount of data has been transmitted.
2. **Congestion Event**: Switching occurs when congestion is first detected (e.g., fast retransmission or RTO).

### MPTCP Phase
When the switching threshold is reached, MMPTCP initiates multiple sub-flows, and data transport is governed by MPTCP’s congestion control. No more packets are added to the initial PS flow, which is deactivated once its window is emptied.

## 3. Discussion and Future Work
Figures 1(b) and 1(c) show the flow completion times for short flows in a simulated FatTree topology (see Figure 1 caption for simulation setup details). Compared to MPTCP, MMPTCP results in fewer short flows experiencing RTOs, leading to lower completion times. During the packet scatter phase, MMPTCP uses all available paths to distribute packets, handling sudden bursts gracefully. The majority of short flows completed within 100ms. The average flow completion time and standard deviation for MMPTCP and MPTCP are 116 milliseconds (standard deviation 101) and 126 milliseconds (standard deviation 425), respectively.

### Roadmap
Using our custom ns-3 models for MPTCP and MMPTCP, we are simulating various data center topologies to compare MMPTCP with other transport protocols in different network scenarios (e.g., hotspots, network loads, traffic matrices, and phase switching strategies). We also plan to design multi-homed network topologies, which are well-suited for MMPTCP, as more parallel paths at the access layer can enhance burst tolerance and prevent transient congestion. We expect MMPTCP to be deployable in existing data centers, as it can coexist with other transport protocols. Ongoing work includes an in-depth investigation of how MMPTCP shares network resources with TCP and MPTCP, with early results suggesting harmonious coexistence. MMPTCP requires ECMP, which is widely deployed in data centers, and does not rely on any network changes or application-layer information.

## 4. References
[1] M. Al-Fares et al. A Scalable, Commodity Data Center Network Architecture. In Proc. of SIGCOMM 2008.  
[2] M. Alizadeh et al. Data Center TCP (DCTCP). In Proc. of SIGCOMM 2010.  
[3] A. Greenberg et al. VL2: A Scalable and Flexible Data Center Network. In Proc. of SIGCOMM 2011.  
[4] C. Hopps. Analysis of an equal-cost multi-path algorithm. RFC 3782, 2004.  
[5] M. Kheirkhah et al. Multipath TCP model in ns-3. In WNS3 2014, https://github.com/mkheirkhah/mptcp.  
[6] C. Raiciu et al. Improving Datacenter Performance and Robustness with Multipath TCP. In Proc. of SIGCOMM 2011.  
[7] B. Vamanan et al. Deadline-aware Datacenter TCP (D2TCP). In Proc. of SIGCOMM 2010.  
[8] C. Wilson et al. Better Never than Late: Meeting Deadlines in Datacenter Networks. In Proc. of SIGCOMM 2011.  
[9] M. Zhang et al. RR-TCP: A Reordering-Robust TCP with DSACK. In Proceedings of ICNP 2003.