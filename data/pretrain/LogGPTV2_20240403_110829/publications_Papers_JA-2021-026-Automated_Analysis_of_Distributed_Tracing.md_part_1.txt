JournalofGridComputing(2021)19:9
https://doi.org/10.1007/s10723-021-09551-5
Automated Analysis of Distributed Tracing: Challenges
and Research Directions
AndreBento ·JaimeCorreia·RicardoFilipe·
FilipeAraujo·JorgeCardoso
Received:10July2020/Accepted:6February2021/Publishedonline:25 February2021
©TheAuthor(s),underexclusivelicencetoSpringerNatureB.V.partofSpringerNature2021
Abstract Microservice-based architectures are gain- by a major cloud provider. Results show that there
ing popularity for their benefits in software develop- isanunderusedwealthofactionableinformationthat
ment.Distributedtracingcanbeusedtohelpoperators canbeextractedfrombothmetricandmorphological
maintain observability in this highly distributed con- aspects derived from tracing. In particular, our tools
text, and find problems such as latency, and analyse were able to detect anomalous behaviour and situ-
their context and root cause. However, exploring and ate it both in terms of involved services, work-flows
workingwithdistributedtracingdataissometimesdif- andtime-frame.Furthermore,weidentifiedsomelim-
ficultduetoitscomplexityandapplicationspecificity, itations of the OpenTracing format—as well as the
volume of information and lack of tools. The most industryacceptedtracingabstractions—,andprovide
common and general tools available for this kind of suggestionstotesttracequalityandenhancethestan-
data, focus on trace-level human-readable data visu- dard.
alisation. Unfortunately, these tools do not provide
goodwaystoabstract,navigate,filterandanalysetrac- Keywords Microservices·Autonomicanalysis·
ing data. Additionally, they do not automate or aid Anomalydetection·Observability·Monitoring·
with trace analysis, relying on administrators to do Tracing
it themselves. In this paper we propose using trac-
ingdatatoextractservicemetrics,dependencygraphs
andwork-flowswiththeobjectiveofdetectinganoma- 1Introduction
lousservicesandoperationpatterns.Weimplemented
and publishedopen sourceprototypetoolsto process Following modern Software Engineering trends, sys-
tracingdata,conformingtotheOpenTracingstandard, temsarebecominglargerandmoredistributed,requir-
and developed anomaly detection methods. We vali- ingnewsolutionsandnewdevelopmentpatterns.One
datedourtoolsandmethodsagainstrealdataprovided approach that emerged in recent years is to decou-
ple large monolithic components into interconnected,
functionally small, components that encapsulate and
A.Bento((cid:2))·J.Correia·R.Filipe·F.Araujo·
provide specific logic. These components are known
J.Cardoso
as microservices and have become mainstream in the
CISUC,DepartmentofInformaticsEngineering,
enterprisesoftwaredevelopmentindustry[13].Despite
UniversityofCoimbra,Coimbra,Portugal
e-mail:PI:EMAIL their organizational and technical advantages [9, 39],
Fine-Grained Distributed Systems (FGDS), specifi-
J.Cardoso
HuaweiMunichResearchCenter,Munich,Germany cally microservices increase system complexity, thus
9 Page2of15 JGridComputing(2021)19:9
turning anomaly detection into a more challenging to realize that the OpenTracing standard was in itself
task[14]. a limiting factor. To start, OpenTracing lacks support
To tackle this problem, operators resort to state tools to create and analyse dependency graphs and
observation techniques like monitoring [11], log- span trees. While Zipkin manages the latter, it does
ging [19], and end-to-end tracing [41]. Monitoring not export such data in a structured form. Extract-
consists in measuring aspects like Central Process- ingknowledgefromthisunprocesseddatawillrequire
ingUnit(CPU)andharddriveusage,networklatency manpower proportional to its volume—to the point
and other infrastructure metrics around the system of being untreatable in web-scale systems. The time-
and components. Logging provides an overview to stampfields,indicatingwhenspansstartandend,are
a discrete, event-triggered log. Tracing is similar to not labelled with units, leading to mistakes such as
logging,butfocusesonregisteringtheflowofexecu- spans in a dataset, showing up in various units. In
tionoftheprogram,asrequeststravelthroughseveral particular, in our data, we found both milliseconds
system modules and boundaries. Distributed tracing andmicroseconds.Otherpartsofthespecificationare
preserves causality relationships when state is parti- alsotooambiguous,astheyallowarbitrarykey-value
tionedovermultiplethreads,processes,machinesand pairs in annotations that explode the possible ways
evengeographicallocations.Inparticularforanomaly ofexpressingthesamemeasurement.Thisisthecase
detection in FGDS, distributed tracing tools—like of error codes, function returns, Uniform Resource
Jaeger [44] or Zipkin [3]—, are currently the state Locators (URLs) and other fields that vary between
of the art. They are used to looking for traces that spans.Asaresult,thetracedataisnotgiventocom-
take too long to execute or exhibit other unexpected putational processing, requiring a data cleaning step
behaviours; However, due to the volume of data, this and most of the time statistical or machine learning
task is hard and tedious to perform, and tools fail to approachestoworkaroundambiguity.Finally,wealso
direct the attention of operators to notice the inter- observed that trace quality varies widely. This sug-
esting time-frames or traces. For example, to find geststhattracingframeworks,likeOpenTelemetry[6],
traces involved in an anomalous region of operation, whichiscurrentlyunderdevelopment,shouldsupport
one must manually query the distributed tracing tool testability,byprovidingdeveloperswithconcretemet-
based on time and annotations (developer defined rics of quality, capable of improving instrumentation
properties). and,therefore,theresultingtraces.
To improve the state of the art, and make systems To summarize, we make two contributions in this
more autonomic, tracing analysis needs to be auto- paper: i) we developed processing and analysis tools
matedtoproducehigherorderconstructsthatprovide forOpenTracingdata;andii)weidentifiedimportant
insights for operators. The objective is to automati- limitations of OpenTracing, which might help other
callyfindanomaliesfromtraces. researchers,namelythosebuildingdistributedtracing
We developed a number of tools to process traces toolsandstandards.
and used machine learning algorithms to look for The rest of the paper is organized as follows.
anomalies.Theresultingdatadrivesoperatorstowards Section2presentsthestateoftheartforthisresearch.
anomalous locations, in the temporal and service Section 3 describes the proposed solution. Section 4
dimensions (i.e., time-stamp and service or a partic- weshowandevaluatetheresultsandthestrengthsof
ular trace), reducing the search space. In particular, thisapproach.Section5wediscussasetoflimitations
wecreatedanOpenTracingProcessor(OTP)toextract we found concerning both methods and standards.
metrics from traces and fed them to our Data Anal- Section 6 concludes the paper and describes future
yser, which identifies anomalies in time-series of directions.
number of in-calls, out-calls and response times. To
validate our approach and tools, we used production
tracingdataprovidedbyHuaweiGermany,fromtheir 2StateoftheArt
CloudPlatform.
Resultsshowthatourapproachcanidentifyanoma- In this section we provide some background notions,
lies in FGDS by time-frame and services; However, aswellasanoverviewofsimilarapproaches.Having
oneofthemostinterestingresultsofouranalysiswas onlybeenadoptedrecentlybytheindustry,thenumber
JGridComputing(2021)19:9 Page3of15 9
ofanomalydetectionapproachesusingtracingisrela- Time
tivelysmall.
Span A
2.1CoreConcepts
Span B
Distributed tracing [41] is a method that comes from
traditionaltracing,butappliedtoadistributedsystem Span C
at the work-flow level. Unlike simple logging, trac-
ingmustrelateinformationfromdifferentpartsofthe Span D
system, to order events according to some order, like
Lamport’shappens-beforerelation[23],servingmul- Span E
tiple purposes, such as identifying the root-cause of
Fig.1 Sampletraceovertime
anomaliesorperformdistributedprofiling,andmoni-
torapplications,especiallythosebuiltusingmicroser-
vice architectures and, in the end, it can be used to generic and leave most decisions to the practitioner.
pinpointfailuresandreasonabouttheirrootcause. This lead to incoherence in traces, even inside the
Anumberoftoolsandstandardsemergedfromthis same organization, and undermines the creation of
concept.Forexample,theOpenTracingstandard[33] toolstoautomatetheiranalysis.
uses baggage passing mechanisms (e.g. see [12]), to Fromarepresentativesetoftracesandtheirrespec-
connect together a tree of scoped units of work, like tive span trees, we are able to extract the service
threads,functions,andservices.Forgenerality,Open- dependency graph. Figure 3 shows a possible depen-
Tracing extended their model to support full directed dency graph generated from the span tree in Fig. 2.
acyclicgraphsinsteadofjusttrees.Thesetracesreveal Service A, the root, directly uses services B and E,
the causal connections between such units of work which on turn use C, D, and F; F uses G. We used
throughoutthesystem. dashed arrows after service E, because these depen-
OpenTracingusesdynamic,fixed-widthmeta-data dencies do not come from the trace of Fig. 1, but
to propagate causality between spans, meaning that fromthetracesofotherinvocationsinvolvingE.From
eachspanhasatraceidentifiercommontoallspansof theseinvocations,wecanproducerequestwork-flows.
thesametrace,aswellasaspanidentifierandparent Requestwork-flowsrepresentthepathcarriedoutby
spanidentifier representingparent/childrelationships one request throughout services in the system. For
between spans [40]. The standard defines the format example, from the dependency graph presented in
for spans and the semantic [34, 35] conventions for Fig. 3, a clear work-flow is: Service A → Service
theircontent/annotations. E→ServiceF→ServiceG.Thisrequestwork-flows
Usually,thespanhasanoperationname,starttime- can be used to trace and study service and business
stamp, duration and some annotations regarding the processinteractions.
operationitself.AnexampleofaspancanbeaRemote
ProcedureCall(RPC)orHypertextTransferProtocol
(HTTP) call annotated with source, destination and Span A
possiblyuserdefinedlogs/data.Weprovideinsightof
how spans are related to each other and with time in
Fig.1.Aswecansee,spansspreadovertime,overlap-
pingeachothersincenothingpreventstheoccurrence Span B Span E
of multiple calls in a short period or simultaneously.
Fromatracelikethis,onemayextractaspantree,as
theoneweshowinFig.2.
Span C Span D
However,thisspecificationmaynotbesufficient—
namely, it is not strict enough to be quantitatively
tested.Furthermore,thesemanticconventionsarevery Fig.2 Spantreeexample
9 Page4of15 JGridComputing(2021)19:9
they are focused on span and trace lookup, and pre-
Service A
sentation,notdoinganytypeofautomatedanalysisor
processing.Forexample,theylackmechanismscapa-
Service B Service E bleofpinpointinganomaliesinspecificmicroservices
orwork-flows/requests,leavingthiskindofworkto
operators,whommustperformmanualtraceandspan
Service C Service D Service F
inspection.
In summary, while generating, persisting, sorting
andrepresentingtracingdataiscertainlyagoodstart-
Service G
ingpointforthesetools,theystilllackmoreadvanced
Fig.3 Dependencygraphexample features for autonomic system analysis. Application
Performance Monitoring Tools usually sport some
analysis capability, but they are typically expensive
2.2DistributedTracingTools full-stackobservabilitysuites[32].
Distributedtracingtoolsfetchorreceivetracedatafrom 2.3RelatedWork
complex distributed systems—such as microservice-
based ones—and process this data, before presenting To contextualize our contributions, this sub-section
ittotheuserusingmorereadablechartsanddiagrams. summarizes the related work found in the literature.
Amongotherthings,thesetoolsprovidethepossibil- Automating tracing analysis has been attempted for
ity to perform queries on the tracing data, e.g., by classic tracing, where the data is usually from a sin-
traceidentifierandbytime-frame.Table1presentsa gle process or machine, and focuses on lower level
comparisonofopensourcetracingtools. calls, such as functions and kernel calls. In this vein,
The two tools we compare, Jaeger [44] and Zip- [22] present a method to detect anomalies in fea-
kin[3],areverysimilar.Theiradvantagesincludethe tures extracted from Linux kernel traces. While the
availability of source code, containerization, support subject of anomaly detection from tracing features
forwellknownspantransporttechnologies,andspan is a shared concern, our approach focuses on the
aggregationforrepresentationinabrowser;However, distributed nature of FGDS—on its unique aspects,
Table1 Distributedtracingtoolscomparison
Jaeger[44] Zipkin[3]
Briefdescription Released as open-source by Uber Technolo- Helpsgatheringtimingdataneededtotroubleshoot
gies.Usedformonitoringandtroubleshooting latency problems in microservice applications. It
microservice-baseddistributedsystems. manages both the collection and lookup of data.
Zipkin’s design is based on the Google Dapper
paper[42].
Pros Open-source; Open-source;
Docker-ready; Docker-ready;
CollectorinterfaceiscompatiblewithZipkin Allowsmultiplespantransporttechnologies
protocol; (HTTP,Kafka,Scribe,AMQP);
Dynamicsamplingrate; BrowserUserInterface.
BrowserUserInterface.
Cons Onlysupportstwospantransporttechnologies Fixedsamplingrate.
(ThriftandHTTP).
Analysis Dependencygraphview; Dependencygraphview.
Tracecomparison.
JGridComputing(2021)19:9 Page5of15 9
suchasmorphologicalanalysis—andtheexistingdis- justificationfortheclassification.Bycomparison,our
tributedtracingstandards. approach focuses on a fixed set of features, related
Asinstrumentationcostisrelevantfortheapplica- to operation metrics, and morphology, such as con-
tionofdistributedtracing,thereissignificantresearch nectivitydegreeandwork-flow,andusesinterpretable
inattemptingtoautomateorcircumventinstrumenta- machinelearningmethods.
tion. There are tools that attempt to automate instru- Looking at practical applications of tracing anal-
mentation,eitheratcoderuntimeormiddle-warelevels ysis, at IBM, [27] have achieved good results with
[4,10,28].Othersattemptaninference-basedtracing AIOps for trace and other observability data analy-
approach,statisticallyextractingcausalorder,making sis;theypresentacompletedataprocessingpipeline,
it transparent to the services themselves and treating from ingestion to actionable insight, as well a suc-
themasblackboxes[2].[5,37]dothesame,withfocus cessfulevaluationonaproductioncloud.[7]usetrace
onsystemsofmicroservicesandexploitingtheobser- analysis together with fault injection to improve fail-
vation features of the underlying platforms, such as ure propagation analysis in cloud systems. Similarly,
service meshes and cluster managers. In contrast, we [46] developed a model to predict latent errors and
assumethattheinstrumentationefforthasalreadybeen localizethembylearningfromdistributedtracing.The
carried out, as it is gaining popularity in the industry model was trained using data generated under a fault
tosolveFGDSobservabilityissues.Furthermore,this injectionload.
affordshigherconfidenceintheresults,especiallyfor
statisticallyrarework-flowsoroccurrences.
On the subject of tracing collection—which we 3ProblemStatementandProposedSolution