• Parse the hint ℎ as(cid:0)(sk1, . . . , sk𝑇 ), (𝑃1, . . . , 𝑃𝑇 )(cid:1).
• Let 𝑡 ∈ [𝑇] be a value such that 𝑖 ∈ Eval(sk𝑡).
(If no such value 𝑡 exists, abort.)
• Sample sknew ← GenWith(𝑛, 𝑖).
• Compute:
𝑆𝑡 ← Eval(sk𝑡)
𝑟1 ←R 𝑆𝑡 (cid:114){𝑖}
skp1 ← Punc(sk𝑡 , 𝑖)
𝑞1 ← (skp1, 𝑟1).
𝑆new ← Eval(sknew)
𝑟0 ←R 𝑆new (cid:114){𝑖}
skp0 ← Punc(sknew, 𝑖)
𝑞0 ← (skp0, 𝑟0)
• Set st(cid:48) ← (𝑡, sknew).
• Return (st(cid:48), 𝑞0, 𝑞1).
QueryRare(𝑖) → (st(cid:48), 𝑞0, 𝑞1).
// The client asks each server for the parity of the database
𝑛−1
// records indexed by a freshly sampled random set of
// indices such that the symmetric diﬀerence between the
// two sets contains 𝑖 and one other random index 𝑟𝛾.
// The client also asks server 𝛾 for the record at index 𝑟𝛾.
• Sample a random bit 𝛾 ←R {0, 1}.
• Sample sknew ← GenWith(𝑛, 𝑖).
• Compute:
√
𝑆new ← Eval(sknew)
𝑟𝛾 ←R 𝑆new (cid:114){𝑖}
skp 𝛾 ← Punc(sknew, 𝑖)
𝑞𝛾 ← (skp 𝛾, 𝑟𝛾)
𝑟 ¯𝛾 ←R 𝑆new (cid:114){𝑟𝛾}
skp ¯𝛾 ← Punc(sknew, 𝑟𝛾)
𝑞 ¯𝛾 ← (skp ¯𝛾, 𝑟 ¯𝛾).
• Set st(cid:48) ← 𝛾.
• Return (st(cid:48), 𝑞0, 𝑞1).
• If 𝛽 = 1:
// Rare case
– Parse the state st(cid:48) as 𝛾 ∈ {0, 1}
– Set 𝐷𝑖 ← 𝑊0 ⊕ 𝑊1 ⊕ 𝑉𝛾.
– Set ℎ(cid:48) ← ℎ.
• Return (ℎ(cid:48), 𝐷𝑖).
// The hint is unmodiﬁed.
880    30th USENIX Security Symposium
USENIX Association
fetch a record from the database. We set the probability of
each case such that the overall probability distribution of the
client’s queries hides the indices the client is interested in.
We now describe this in more detail.
Common case. Recall that at the start of the oﬄine phase, the
client holds the hint it received in the oﬄine phase, which
consists of a seed for a pseudorandom generator and a set
of 𝑇 hint words (𝑃1, . . . , 𝑃𝑇 ). The client’s ﬁrst task is to
expand the seed into a set of puncturable pseudorandom set
keys sk1, . . . , sk𝑇 . (These are the same keys that the server
generated in the oﬄine phase.) Next the client searches for
a key sk𝑡 ∈ {sk1, . . . , sk𝑇 } such that the index of the client’s
desired record 𝑖 ∈ Eval(sk𝑡).
√
𝑛,
which contains the client’s desired index 𝑖. The client also
holds the parity word 𝑃𝑡 ∈ {0, 1}ℓ of the database records
indexed by 𝑆𝑡. The client sends the set 𝑆𝑡 (cid:114){𝑖} to the second
server. (To save communication, the client compresses this
set using puncturable pseudorandom sets.) The server returns
the parity word 𝑊1 of the database records indexed by this set
𝑆𝑡 (cid:114){𝑖}. The client recovers its record of interest as:
At this point, the client holds a set 𝑆𝑡 = Eval(sk𝑡) of size
(cid:16)⊕ 𝑗∈𝑆𝑡
(cid:17) ⊕(cid:16)⊕ 𝑗∈𝑆𝑡(cid:114){𝑖} 𝐷 𝑗
(cid:17)
𝐷 𝑗
𝑃𝑡 ⊕ 𝑊1 =
= 𝐷𝑖.
√
For security, it is critical that each server “sees” each set
only once. Therefore, the client must not reuse the set 𝑆𝑡
for any future queries. Therefore, the client also samples a
𝑛 indices in [𝑛], one of which is 𝑖.
replacement set 𝑆new of
The client then sends 𝑆new (cid:114){𝑖} to the ﬁrst server (again,
compressed using puncturable pseudorandom sets), and the
ﬁrst server responds with the parity word 𝑊0 of the database
records indexed by this set. The client then replaces the set 𝑆𝑡
in its hint with the new set 𝑆new and updates the corresponding
parity hint word to 𝑃new ← 𝑊0 ⊕ 𝐷𝑖.
In this ﬁrst case, the sets that the client sends to the two
servers never contain the index 𝑖 of the client’s desired database
record. If the client would always use this query strategy, the
deﬁnitely not querying, eﬀectively leaking ≈ 1/(√
servers would learn which database records the client is
𝑛 ln 2) bits
of information about 𝑖. The next case prevents this leakage.
Rare case. With a small probability (roughly 2/√
𝑛), the client
must send a set containing its desired index 𝑖 to each server.
𝑛 values in [𝑛],
The client samples a random set 𝑆new of
one of which is 𝑖. The client chooses a server 𝛾 ←R {0, 1} at
random and sends it 𝑆new (cid:114){𝑖} (again, compressed), along
with the index of a random element 𝑟𝛾 ←R 𝑆new (cid:114){𝑖}. To the
other server ¯𝛾 (cid:66) 1 − 𝛾, the client sends 𝑆new (cid:114){𝑟𝛾} and, to
hide which server plays which role, a dummy value 𝑟 ¯𝛾.
Each server replies with the parity word 𝑊 of the database
records indexed by the set it has received. It also sends the
value of the database record 𝐷𝑟. Now, the client can recover its
record of interest as: 𝐷𝑖 = 𝑊0 ⊕ 𝑊1 ⊕ 𝐷𝑟𝛾, since ∀𝛾 ∈ {0, 1},
this sum is equal to
(cid:16)⊕ 𝑗∈𝑆new(cid:114){𝑖} 𝐷 𝑗
(cid:17) ⊕(cid:16)⊕ 𝑗∈𝑆new(cid:114){𝑟𝛾} 𝐷 𝑗
(cid:17) ⊕ 𝐷𝑟𝛾 = 𝐷𝑖.
√
To hide whether the client is in the “common case” or “rare
case,” the client sends dummy indices 𝑟0, 𝑟1 to the servers in
the common case to mimic its behavior in the rare case.
Remark (Pipelined queries). When a client makes many PIR
queries in sequence, it may want to issue a new query to the
servers before receiving the servers’ response to its previous
query. Our scheme (Construction 1) allows the client to have
any number of queries in ﬂight at once, while still using
only a single hint. The key observation is that the client can
generate the replacement set sknew as soon as it issues a query.
The client can thus issue a second query immediately after
issuing the ﬁrst, and a third query immediately after issuing the
second—the client just has to receive the server’s responses
in the order in which it issued its queries.
Remark. The client’s expected online query time in our con-
struction is linear in the size of the database, since the client
has to expand its set keys one by one in a random order, until
it ﬁnds a key of a set that contains the index of interest 𝑖.
As in prior oﬄine/online PIR schemes [27], a client can use
a data structure to reduce the query time at the cost of in-
creasing its storage. Checklist uses a simple data structure
that has size linear in the database size 𝑛 but that supports
√
constant-time queries. That is, the client stores a hash table
mapping database indices 𝑖 ∈ [𝑛] to “set pointers” 𝑗 ∈ [𝜆
𝑛]
such that 𝑖 ∈ Eval(sk 𝑗). The client lazily populates this map
whenever it evaluates set keys and invalidates entries when-
ever it discards set keys. As a compromise between storage
and query time, the map contains at most one set pointer for
each database index. Therefore, discarding a set may leave
some database indices without valid set pointers, even though
other sets in the client’s hint may still contain those indices.
At query time, if the client fails to ﬁnd a set pointer for the
desired database index in the map, it falls back to exhaustively
searching through the hint. As it iterates through the hint, the
client “opportunistically” adds set pointers to the map.
5 Oﬄine/online PIR
for dynamic dictionaries
PIR protocols typically treat the database as a static array of 𝑛
records. To fetch a record, a PIR client must then specify the
index 𝑖 ∈ [𝑛] of the record. Our scheme of Section 4 follows
this approach as well. In contrast, Checklist, like many other
applications of PIR, needs to support dynamic databases and
key-value-style lookups. Speciﬁcally, we would like to view the
database as a list of key-value pairs ((𝐾1, 𝑉1), . . . , (𝐾𝑛, 𝑉𝑛)),
where 𝐾𝑖 ∈ {0, 1}𝑘 are the keys, and 𝑉𝑖 ∈ {0, 1}ℓ are their
corresponding values. In Checklist, (i) a client should be able
to look up a value 𝑉 by its key 𝐾; and (ii) a server should be
able to insert, modify, and delete key-value pairs.
USENIX Association
30th USENIX Security Symposium    881
Bucket 0:
Bucket 1:
Bucket 2:
Bucket 3:
h0
h0
h1
h1
h2
h2
h3
[insertion]
[insertion]
h0
h1
h0
h2
h1
h2
h2
h3
(After 1 insertion.)
h0
h0
h1
h1
h2
h2
h3
[insertion]
h(cid:48)
h0
0
h1
h1
h2
h2
h3
(After 2 insertions.)
[insertion]
h(cid:48)
h0
0
h1
h1
h2
h2
h3
(After 3 insertions.)
(After 4 insertions.)
Figure 1: The database in our PIR scheme consists of many buckets, where the 𝑖th bucket can hold 2𝑖 database rows. The client holds a hint (h𝑖)
corresponding to each non-empty bucket 𝑖. The smaller buckets change frequently, but these hints are inexpensive to recompute. The larger
buckets change infrequently, and these hints are expensive to recompute.
5.1 Existing tool: PIR by keywords
Previous work has shown how to modify standard PIR
schemes to support key-value-style databases. Speciﬁcally,
Chor, Gilboa, and Naor [23] showed that it is possible to con-
struct so-called “PIR-by-keywords” schemes from traditional
PIR-by-index schemes in a black-box way. Modern PIR con-
structions [15] support PIR-by-keywords directly. The cost of
such schemes, both in communication and server-side compu-
tation, matches the cost of standard PIR, up to low-order terms.
The black-box PIR-by-keywords techniques [23] directly ap-
ply to oﬄine/online PIR schemes as well. Speciﬁcally, our
implementation of Checklist uses a simple PIR-by-keywords
technique, which is tailored at the preexisting design of the
Safe Browsing system. We describe this scheme in Section 6.3.
5.2 Handling changes with waterfall updates
Standard online-only PIR schemes do not need any special
machinery to meet handle database updates, since their clients
hold no state that depends on the database contents. The
servers in online-only PIR schemes can thus simply process
any changes to the database locally as they happen, and then
answer each query using the latest version of the database. In
contrast, clients in oﬄine/online PIR schemes hold prepro-
cessed “hints” about the database, and every change in the
database invalidates these hints.
The simple solution works poorly. The simplest way to
handle database updates is to have the servers compute a new
hint relative to the latest database state after every update. The
servers then send this updated hint to the client. The problem
is that if the rate of updates is relatively high, the cost of
regenerating these hints will be prohibitive.
Speciﬁcally, if the database changes at roughly the same
rate as the client makes queries (e.g., once per hour), the client
will have to download a new hint before making each query.
In this case, the server-side costs of generating these hints will
be so large as to negate the beneﬁt of using an oﬄine/online
PIR scheme in the ﬁrst place.
Our approach: Waterfall updates. Instead of paying the
hint-generation cost for the full database on each change, we
design a tiered update scheme, which is much more eﬃcient.
Speciﬁcally, if there is a single database update between
our scheme is still 𝑂(√
each pair of client queries, the asymptotic online cost of
𝑛)—the same cost as if the database
had not changed. As the frequency of updates increases, the
performance of our scheme gracefully degrades. Our design
builds on a classic idea for converting static data structures into
dynamic structures [10]. Cryptographic constructions using
this idea to handle data updates include oblivious RAMs [41],
proofs of retrievability [20, 77], searchable encryption [81],
and accumulators [68].
Our strategy is to have the servers store the database as an
array of 𝐵 = log 𝑛 sub-databases, which we call “buckets.”
(Here, we assume for simplicity that the number of records 𝑛
is a power of two.) The 𝑏th bucket will contain at most 2𝑏 key-
value pairs. In addition, the servers maintain a last-modiﬁed
timestamp for each bucket. Initially, the servers store the entire
database in the bottom (biggest) bucket, and all other buckets
start out empty. As the database changes, the contents of the
buckets change as well.
When a client joins the system, it fetches a hint for each
bucket. Before making a query, the client updates its locally
stored hints. To do this, the client sends to the ﬁrst server the
timestamp 𝜏 at which it received its last hint. The server then
generates a fresh hint for each bucket that was modiﬁed after 𝜏,
and sends these new hints back to the client. To ﬁnd the value
associated with key 𝐾, the client then queries each of the 𝐵
buckets in parallel for key 𝐾. If several buckets contain key 𝐾,
the client uses the value 𝑉 from the smallest bucket (i.e., the
bucket that was updated most recently).
Since the underlying oﬄine/online PIR-by-keywords
scheme supports only static databases, each time a bucket
changes, the server must regenerate from scratch a hint for this
bucket for every client. The key to achieving our cost savings
is that, as the database changes, the contents of the smallest
buckets will change frequently, but it is relatively inexpensive
for the servers to regenerate the hints for these buckets. The
contents of the larger buckets—for which hint generation is
expensive—will change relatively infrequently.
It remains to describe how the servers update the contents
of the buckets upon database changes. Let us ﬁrst consider
database insertions. When the servers want to add a new
pair (𝐾, 𝑉) to the database, the servers insert that pair into
the topmost (smallest) bucket. Such an update can cause a
bucket 𝑏 to “ﬁll up”—to contain more than 2𝑏 entries. When
this happens, the servers “ﬂush” the contents of bucket 𝑏
down to bucket 𝑏 + 1. If this ﬂush causes bucket 𝑏 + 1 to
882    30th USENIX Security Symposium
USENIX Association
ﬁll up, the servers continue ﬂushing buckets until all buckets
are below their maximum capacity. If the bottommost bucket
overﬂows, the servers create a new bucket, twice the size
of the previous one. The two servers execute this process in
lockstep to ensure that their views of the database state remain
consistent throughout.
To remap an existing key 𝐾 to a new value 𝑉(cid:48), the servers
add the updated record (𝐾, 𝑉(cid:48)) to the topmost bucket. When,
as a result of ﬂushing, multiple pairs with the same key end