ing agency could not legally access the plaintext genomic data. Digital rights
management (DRM) based technologies could also be beneﬁcial for sequencing
machines.
IX.C Storage
We assume that once the adversary has access to the read data, it is easy to get
raw aligned data and variant data, hence we present storage of all three forms
of data together.
IX.C.1 Setting
Genomic data can be stored by the (i) patient14, (ii) healthcare organization
(e.g., as part of patient’s EHR), or (iii) a third party.
IX.C.2 Threat Model
For all settings, we assume that the lifetime of genomic data is much longer
than the lifetime of a cryptographic algorithm. We consider the following threat
models
Patient: Storage media or the device storing genomic data can be lost, stolen
or temporarily accessed. A patient’s computer can be attacked by an adversary
(curious or malicious) to compromise conﬁdentiality and/or integrity of the ge-
nomic data. We further assume that an adversary can associate the identity and
background information (including phenotype information) from any arbitrary
source. And, we assume that the adversary can use the compromised genomic
data for any arbitrary purpose.
Hospital: We consider all of the threats described for the patient and the
following additional threats. An insider (or hacker) has full access to the infras-
tructure and can link genomic information to the phenotypic information in the
patient’s health record.15 We also consider the threat of the healthcare organi-
zation communicating incidental ﬁndings that are harmful – violating the right
not to know.16 We assume that the adversary can intentionally try to ﬁgure out
variants of regions of the genome from variants in other regions, e.g., to learn
some sensitive SNPs removed due to incidental ﬁnding issues. We also assume
that a healthcare organization could illegally collude with insurance agencies to
13http://goo.gl/I942hy
14We use the word patient to mean an individual whose genome is sequenced or genotyped
and not necessarily a sick person.
15We assume that data stored at the hospital is not anonymized.
16For instance, if a doctor telling a patient that his increased susceptibility to Alzheimer’s,
when he doesn’t want to know. We emphasize that deﬁning what is harmful is an ethical
issue and is out of scope of this study.
30
facilitate discrimination based on genomic data.
Third party: We consider all of the threats discussed for the hospital and
following additional threats. The adversary, as the third party itself, can be in-
terested in de-anonymizing the anonymized genomic data or aggregate genomic
data.
IX.C.3 Solutions and Open Problems
We report some solutions in Section VII. Users are generally not equipped with
skills and equipment to protect the security and privacy of their genomic data.
For the storage of genomic data, an option is to store it on a cloud in an
encrypted fashion, which makes the adversary’s job harder, as now it needs to
circumvent cloud storage security measures and also require to hack into user’s
computer to steal the decryption key. Eﬃcient encryption schemes allowing
secure storage and computation are required.
IX.D Alignment/Assembly
As explained in Section VII, genomic data is obtained from the specimen in
the form of short reads. These short reads are then assembled using alignment
or assembly algorithms. Alignment is done by comparing the short reads to
the reference genome, and is computationally very intensive. Hence, it can be
economically beneﬁcial to delegate the alignment to the cloud.
IX.D.1 Setting
Short reads are obtained and alignment is delegated to an untrusted third party.
IX.D.2 Threat Model
We assume that the third party can be honest-but-curious, as well as malicious,
and can return incorrect results for economic or other malicious motives.
IX.D.3 Solutions and open problems
We presented some solutions to this problem in Section VII [Chen et al., 2012].
However, there are several problems with the most eﬃcient solution to date.
First, it is not provably secure. Second, its security and eﬃciency requires that
the read size be greater than 100 nucleotides. Third, this scheme only works
in a hybrid cloud environment and requires local computation. Given that
our survey shows that third party environments are of the greatest concern to
biomedical researchers, a provably secure and eﬃcient solution that is capable of
aligning the human genome in a cloud computing environment is an important
open research problem.
31
IX.E Interpretation
Interpretation depends upon two private inputs: the patient’s genomic data
and an interpretation algorithm (possibly from more than one party). Given
the complexity of genomic data, it is unlikely that any single party will have a
complete interpretation algorithm. This makes the computation a multiparty
process between independent parties and the patient. Although each party
with a piece of the interpretation algorithm can compute independently with
the patient, collusion of any two parties may leak information about another
party’s inputs. Moreover, the interpretation of one party may depend upon
the interpretation of another party. We assume that all of these parties can
be malicious and can collude to learn information (e.g., the patient’s genomic
data or another parties’ algorithm). In some cases, it is necessary to sequence
one’s parent to draw conclusions, in which case parents might also be concerned
about their privacy.
Personalized medicine is a special case of interpretation and depends upon
the patient’s genomic data and disease markers (possibly distributed among
multiple parties).
Preconception testing is diﬀerent from personalized medicine because it is a
pre-pregnancy test and measures can be taken to conceive a healthy child (as
evident from www.counsyl.com success stories). Additionally, the outcome of
the preconception test almost always depends upon two people, each of whom
might prefer to not disclose their genomic status to the other.
IX.E.1 Setting
Computation is typically performed on private data from multiple parties. The
parties involved in computation are those who input (i) their genomic data and
(ii) interpretation algorithms. The output of the computation should only be
released to the patient or authorized physician (possibly using the infrastructure
of a healthcare organization).
IX.E.2 Threat Model
We assume that all parties can be honest-but-curious, malicious, or colluding
(and possibly all at the same time). They can use arbitrary background knowl-
edge to breach privacy. Furthermore, they may use falsiﬁed genomic data, or
an interpretation algorithm, an arbitrary number of times to ascertain another
parties’ private inputs.
IX.E.3 Solutions and Open Problems
We discussed some of the solutions for the personalized medicine scenario in
Section VII. However, current solutions are limited to privacy-preserving dis-
ease susceptibility tests. It is clear that computational solutions that support a
broad range of computation over genomic data are needed. At the same time,
32
the design of such systems must be practical because physicians and biomed-
ical scientists have diﬀerent usability, accuracy and privacy expectations from
security and privacy researchers.
IX.F Genome-Wide Association Studies (GWAS)
IX.F.1 Setting
The genomic data from two groups of people are collected, one being the case
group (i.e., people with the disease) and the other being the control group (i.e.,
people without the disease). Statistical analysis is then conducted to discover
the correlation between the disease and genetic variants. The results are subse-
quently published in research papers and posted online possibly with restricted
access (e.g., at dbGaP).
IX.F.2 Threat Model
An adversary may wish to determine if the victim is a GWAS participant or
blood relative of a GWAS participant. We assume that the adversary has access
to the high density SNP proﬁle of the victim and also to a reference population
(which can be obtained from the same GWAS conducted on a diﬀerent popula-
tion). The attack succeeds if the adversary can gain knowledge from the data
produced by GWAS, which she otherwise would not have.
IX.F.3 Solutions and Open Problems
There are various solutions that could be applied in this setting. We explained
noise-based solutions, such as diﬀerential privacy, in Section VII.B. Yet, diﬀer-
ential privacy-based solutions make data more noisy, which make adoption of
these approaches diﬃcult. This is particularly problematic because biomedical
researchers and physicians want more (not less) accurate data than is available
today. An ideal solution should preserve the utility of data while preserving the
privacy of participants. We believe that more research is required in this area to
determine if noise-based approaches can lead to more usable and pragmatic data
publications. These approaches may, for instance, build upon well-established
sets of practices from other communities. For example, the Federal Committee
on Statistical Methodology (FCSM) has a long history of sharing information in
a privacy respective manner. These practices obey multi-level access principles
and, to the best of the authors’ knowledge, no signiﬁcant privacy breach from
such domain has been reported.
While the data disclosed by federal agencies is quite diﬀerent from high-
dimensional genomic data, it might be possible to adapt practices to balance
the beneﬁts and harms caused by public sharing of aggregate genomic data.
These strategies may be composed of social and technical protections. From
a social perspective, a popular method to mitigate risk is through contractual
agreements which prohibit the misuse of such data. Such contracts could be
complemented by cryptographic protocols that help preserve the privacy of the
33
participants, particularly in settings in which the data is used in the secure
computation and only the output of the computation is revealed to a speciﬁc
party.
IX.G Data sharing
The majority of genome-phenome discoveries come from very large populations,
sometimes on the order of millions of participants. Given the costs and scarcity
of such resources, sharing data would fuel biomedical research. However, sharing
this data entails privacy implications as discussed earlier.
IX.G.1 Setting
Genomic data needs to be shared among diﬀerent research institutions, possibly
under diﬀerent jurisdictions. Privacy-preserving solutions can be built in the
following settings: (i) all data delegated to and computation done on a trusted
party (e.g., a governmental entity), (ii) all data delegated to and computation
done on an untrusted party, (iii) all data stored at and computation done at the
collection agency, (iv) sharing data using data use agreements, and (v) sharing
anonymized data.
IX.G.2 Threat Model
We assume that data is being shared between untrusted parties. The parties
with whom data is being shared may want to use it for any arbitrary purpose,
including using it for participant re-identiﬁcation, or for ﬁnding disease suscep-
tibility of the patients and his blood relatives. We also assume that once the
data is shared, it can be used in an arbitrary manner.
IX.G.3 Solutions and Open Problems
We described some solutions in Section VII.B. However, these solutions do not
allow for arbitrary computations on encrypted data. Theoretically, many cryp-
tographic solutions exist to solve this issue. For example, fully homomorphic
encryption (FHE) can be used to encrypt the data and arbitrary computations
can be done on it while preserving privacy, but data needs to be decrypted by
the party that encrypted the data. Functional encryption (FE) could also be
used, through which any arbitrary function can be computed on encrypted data.
However, FHE and FE are not suﬃciently eﬃcient to be practically useful. The
performance of FHE and FE is progressing and these schemes might be usable
in the future to support data sharing. Clearly though, specialized eﬃcient solu-
tions for genomic data exploiting nature of genomic data are needed to support
speciﬁc analytics.
34
IX.H Paternity
Genomic data is extensively used to determine parentage and test results are
admissible in courts of law. Today, the biospecimen of the individuals involved
in the tests are outsourced to a third party in the form of cheek swabs, where
the DNA is extracted. Sending one’s DNA to a third party could have serious
implications for one’s privacy.
IX.H.1 Setting
Two parties each have their sequenced genome or variants and one party wants
to know whether the other party is the patient.
IX.H.2 Threat Model
The threat model in this case is the standard model deﬁned for secure two-party
computations. We assume that parties can be honest-but-curious or malicious.
IX.H.3 Solutions and Open Problems
In Section VII, we explain some of the solutions to the problem. A chemical test
– RFLP – can be simulated for a neat and eﬃcient privacy-preserving solution,
given that genomes are stored by individual’s themselves [Baldi et al., 2011].
Yao’s garbled circuits can be used instead of PSI to output a binary answer
(YES or NO) instead of the number of matched segments in simulated RFLP
test.
IX.I Forensic DNA Databases
Many countries maintain a huge database of DNA proﬁles of convicted (and, in
some cases, accused) criminals. Law enforcement agencies usually have unlim-
ited access to such a resource, which makes it vulnerable to abuse. It is possible
that in near future, instead of concise DNA proﬁles, law enforcement agencies
will be able to have access to full genome sequences of individuals, which further
exacerbates the issues.
IX.I.1 Setting
Police oﬃcers collect DNA samples from a crime scene. Then, they want to
check whether an individual with the same DNA proﬁle/sequence is present in
the DNA records database.
IX.I.2 Threat Model
We assume that the adversary can be honest-but-curious, interested in learning
about other people in the database. In addition, if the adversary has write access
to the database, he can also try to compromise the integrity of the record(s) in
35
the database. We also assume that the adversary is able to aﬀect the outcome
of a query in arbitrary manner.
IX.I.3 Solutions and Open Problems
We discussed some of the existing solutions to this problem in Section VII.
Theoretically, this problem diﬀers from interpretation and other types of com-
putation, as the privacy for query is not required, only the privacy of the indi-
viduals other than the suspect is of concern here. This makes the problem more
tractable, possibly making solutions scalable to large databases with millions of
records.
IX.J Recreational Genomics
Several commercial companies oﬀer direct-to-consumer genomics services. They