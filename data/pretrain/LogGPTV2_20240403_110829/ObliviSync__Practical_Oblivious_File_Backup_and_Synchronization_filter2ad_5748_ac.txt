pair; our implementation stores this always in backend ﬁle 0.
Split block: a block with small fragments. At the block
level, there can be two types of blocks: a full block where the
fragment stored within is as large as the block size and inhabits
the entirety of the block, or a split block where multiple
fragments smaller than the block size are stored within the
same block. When a large ﬁle is stored as a series of fragments,
we maintain that all fragments except possibly for the last
fragment are stored in full blocks. That is, there will be at
most one split-block fragment per ﬁle.
Introducing split blocks allows the system to use the
backend storage more efﬁciently. For example, without using
split blocks, 100 small fragments from 100 frontend ﬁles will
consume 100 backend blocks, but by placing multiple small
fragments into a single split block, we can reduce the number
of blocks consumed.
Looking up the data in a full block is straightforward: given
the block-id value, ObliviSync fetches the given block and
decrypts its contents. In addition to the actual data, we also
store the ﬁle-id of the ﬁle within the block itself as metadata.
This will facilitate an easy check for whether a given block
has become stale, as we will see shortly.
For a split block, however, the system also needs to know
the location of the desired fragment within the block. The
information is stored within the block itself in the block table
which maps ﬁle-ids to offsets. With the size of the ﬁle from
the ﬁletable, it is straightforward to retrieve the relevant data.
A full block can then be simply deﬁned as a block without
a block table, and the leading bit of each block is used to
identify whether the block is full or split.
Two blocks in a backend ﬁle. All backend blocks are grouped
into pairs of two consecutive blocks where each pair of blocks
resides within a single backend ﬁle. Importantly, we relax
slightly the indexing requirements so that small fragments are
allowed to reside in either block without changing their block-
id. This tiny block-id mismatch is easily addressed by looking
up both blocks in the corresponding backend ﬁle.
Furthermore, as we will see in the sync operation described
later, both blocks in a given pair are randomly selected to be re-
packed and rewritten at the same time. This additional degree
of freedom for small fragments is crucial for bounding the
worst-case performance of the system.
C. Read-Only Client
A read-only client (ObliviSync-RO) with access to the
shared private key is able to view the contents of any directory
or ﬁle in the frontend ﬁlesystem by reading (and decrypting)
blocks from the backend, but cannot create, destroy, or modify
any ﬁle’s contents.
To perform a read operation for any ﬁle, given the ﬁle-id of
that ﬁle (obtained via the directory entry), the ObliviSync-RO
ﬁrst needs to obtain the block-id list of the ﬁle’s fragments to
then decrypt the associated blocks and read the content. This
is accomplished in the following steps:
1) Decrypt and read the superblock.
2) Check in the ﬁle-entry cache. If found, return the corre-
sponding block-id list.
3) If not found in the cache, search in the ﬁletable via the
B-tree root (part of the superblock) to ﬁnd the block-id
of the appropriate leaf node in the B-tree.
4) Decrypt and read the leaf node to ﬁnd the ﬁle-entry for
the ﬁle in question, and return the corresponding block-id
list and associated metadata.
Once the block-id list has been loaded, the desired bytes of
the ﬁle are loaded by computing the block-id offset according
to the block size, loading and decrypting the block speciﬁed
by that block-id, and extracting the data in the block.
Given the ﬁle-id, it can be seen from the description above
that a single read operation in ObliviSync-RO for a single
fragment requires loading and decrypting at most 3 blocks
from the backend: (1) the superblock, (2) a B-tree leaf node
(if not found in ﬁle-entry cache), and (3) the block containing
the data. In practice, we can cache recently accessed blocks
(most notably, the superblock) for a short period in order to
speed up subsequent lookups.
D. Read/Write Client
The read/write client ObliviSync-RW encompasses the
same functionality as ObliviSync-RO for lookups with the
added ability to create, modify, and delete ﬁles.
Pending writes buffer. The additional data structure stored
in ObliviSync-RW to facilitate these write operations is the
pending writes buffer of recent, un-committed changes. Specif-
ically, this buffer stores a list of (ﬁle-id, fragment, timestamp)
tuples. When the ObliviSync-RW encounters a write (or create
or delete) operation, the operation is performed by adding to
the buffer. For modiﬁed ﬁles that are larger than a block size,
only the fragments of the ﬁle that need updating are placed
in the buffer, while for smaller ﬁles, the entire ﬁle may reside
in the buffer. During reads, the system ﬁrst checks the buffer
to see if the ﬁle-id is present and otherwise proceeds with the
same read process as in the ObliviSync-RO description above.
The main motivation of the buffer is to allow oblivious writing
without compromising usability. The user should not be aware
of the delay between when a write to a ﬁle occurs and when
the corresponding data is actually synced to the cloud service
provider. The function of the buffer is similar to that of “stash”
in normal ORAM constructions.
Interestingly, we note that the buffer also provides consid-
erable performance beneﬁts, by acting as a cache for recently-
used elements. Since the buffer contents are stored in memory
un-encrypted, reading ﬁle fragments from the buffer is faster
than decrypting and reading data from the backend storage.
The buffer serves a dual purpose in both enabling obliviousness
and increasing practical efﬁciency.
Syncing: gradual and periodic clearing of the buffer. The
buffer within ObliviSync-RW must not grow indeﬁnitely. In
our system, the buffer size is kept low through the use of
6
Action
0. (initial)
1. Two blocks updated
2. One block synced
3. Both blocks synced
4. Stale data removed
Buffer
{}
{f(cid:48)
3}
2, f(cid:48)
{f(cid:48)
2}
{}
{}
Backend
{f1, f2, f3}
{f1, f2, f3}
{f1, f2, f3, f(cid:48)
3}
{f1, f2, f3, f(cid:48)
2}
3, f(cid:48)
{f1, f(cid:48)
2}
3, f(cid:48)
ObliviSync-RW view ObliviSync-RO view
[f1, f2, f3]
[f1, f(cid:48)
2, f(cid:48)
3]
[f1, f(cid:48)
2, f(cid:48)
3]
2, f(cid:48)
[f1, f(cid:48)
3]
[f1, f(cid:48)
2, f(cid:48)
3]
[f1, f2, f3]
[f1, f2, f3]
[f1, f2, f3]
2, f(cid:48)
[f1, f(cid:48)
3]
2, f(cid:48)
[f1, f(cid:48)
3]
Fig. 3. Example of ordering and consistency in updating two of three fragments of a single ﬁle. Use of the shadow ﬁletable ensures that the ObliviSync-RO
view is not updated until all fragments are synced to the backend.
a periodic sync operations wherein the buffer’s contents are
encrypted and stored in backend blocks.
Each sync operation is similar to a single write procedure
in write-only ORAM, but instead of being triggered on each
write operation, the sync operation happens on a ﬁxed timer
basis. We call the time between subsequent sync operations
an epoch and deﬁne this parameter as the drip time of the
ObliviSync-RW.
Also, similar to the write-only ORAM, there will be a
ﬁxed set of backend ﬁles that are chosen randomly on each
sync operation epoch. The number of such backend ﬁles that
are rewritten and re-encrypted is deﬁned as the drip rate.
Throughout, let k denote the drip rate. We discuss these param-
eters further and their impact on performance in Section IV-G.
Overall, each sync operation proceeds as follows:
1) Choose k backend block pairs randomly to rewrite and
decrypt them.
2) Determine which blocks in the chosen backend ﬁles are
stale, i.e., containing stale fragments.
3) Re-pack the empty or stale blocks, clearing fragments
from the pending writes buffer as much as possible.
4) Re-encrypt the chosen backend block pairs.
The detailed sync operation is described in the next section.
Consistency and ordering.
In order to avoid inconsistency,
busy wait, or race conditions, the order of operations for the
sync procedure is very important. For each ﬁle fragment that is
successfully cleared from the buffer into the randomly-chosen
blocks, there are three changes that must occur:
1) The data for the block is physically written to the back-
2) The fragment is removed from the buffer.
3) The ﬁletable is updated with the new block-id for that
end.
fragment.
It
that
is very important
these three changes occur in
that order, so that there is no temporary inconsistency in the
ﬁlesystem. Moreover, the ObliviSync-RW must wait until all
fragments of a ﬁle have been synced before updating the ﬁle-
entry for that ﬁle; otherwise there could be inconsistencies in
any ObliviSync-RO clients.
The consistency is enforced in part by the use of a shadow
ﬁletable, which maintains the list of old block-ids for any ﬁles
that are currently in the buffer. As long as some fragment of
a ﬁle is in the buffer, the entry in the ﬁletable that gets stored
with the superblock (and therefore, the version visible to any
read-only client mounts), is the most recent completely-synced
version).
An example is depicted in Figure 3. Say ﬁle f consists
of three fragments [f1, f2, f3], all currently fully synced to
7
2 and f(cid:48)
2 is. (This could easily happen because f(cid:48)
the backend. Now say an ObliviSync-RW client updates the
last two fragments to f(cid:48)
3. It may be the case that, for
example, f(cid:48)
3 is removed from the buffer into some backend
block before f(cid:48)
3 is a
small fragment that can be stored within a split block, whereas
f(cid:48)
2 is a full block.) At this point, the shadow ﬁletable will still
store the location of f3 and not f(cid:48)
3, so that any ObliviSync-RO
clients have a consistent view of the ﬁle. It is only after all
fragments of f are removed from the buffer that the ﬁletable
is updated accordingly.
One consequence is that
there may be some duplicate
versions of the same fragment stored in the backend simulta-
neously, with neither being stale (as in f3 and f(cid:48)
3 after step 2 in
Figure 3). This adds a small storage overhead to the system, but
the beneﬁt is that both types of clients have a clean, consistent
(though possibly temporarily outdated) view of the ﬁlesystem.
Note that, even in the worst case, no non-stale fragment is ever
duplicated more than once in the backend.
E. Detailed Description of Buffer Syncing
Step 1: Choosing which blocks to rewrite. As in write-only
ORAM, k random backend ﬁles are chosen to be rewritten at
every sync operation with the following differences:
• Each backend ﬁle contains a pair of blocks, which implies
• In addition, the backend ﬁle containing the superblock is
that k random pairs of blocks are to be rewritten.
always rewritten.
Choosing pairs of blocks together is crucial, since as we
have mentioned above, small fragments are free to move
between either block in a pair without changing their block-
ids. In addition, the superblock must be rewritten on each sync
because it contains the ﬁletable which may change whenever
other content is rewritten to the backend.
Step 2: Determining staleness. Once the blocks to be
rewritten are randomly selected and decrypted, the next task is
to inspect the fragments within the blocks to determine which
are “stale” and can be overwritten.
Tracking fragment freshness is vital to the system because
of the design of write-only ORAM. As random blocks are
written at each stage, modiﬁed fragments are written to new
blocks, and the ﬁle-entry is updated accordingly, but the stale
data fragment
in the old
block because that old block may not have been selected
in this current sync procedure. Efﬁciently identifying which
fragments are stale becomes crucial to clearing the buffer.
is not rewritten and will persist
A natural, but ﬂawed, solution to tracking stale fragments
is to maintain a bit in each block to mark which fragments
are fresh or stale. This solution cannot be achieved for the
same reason that stale data cannot be immediately deleted —
updating blocks that are not selected in the sync procedure are
not possible.
Instead, recall from the block design that each block
also stores the ﬁle-id for each fragment. To identify a stale
fragment, the sync procedure looks up each fragment’s ﬁle-id
to get its block-id list. If the current block’s identiﬁer is not
in the block-id list, then that fragment must be stale.
Step 3: Re-packing the blocks. Then next step after identi-
fying blocks and fragments within those blocks that are stale
(or empty) is to re-pack the block with the non-stale fragments
residing within the block and fragments from the buffer.
One important aspect
to consider when re-packing the
blocks is to address the fragmentation problem, that is, to
reduce the number of blocks that small fragments use so
that there remain a sufﬁcient number of blocks for full-block
fragments.
A na¨ıve approach would be to evict all the non-stale frag-
ments from the selected blocks and consider all the fragments
in the buffer and the evicted fragments to re-pack the selected
blocks with the least amount of internal fragmentation. While
this would be a reasonable protocol for some ﬁle systems to
reduce fragmentation, this would require (potentially) changing
all of the ﬁle-entries (in particular, the block-id list) for all frag-
ments within the selected blocks. That would be problematic
because it is precisely these old entries which are likely not
to be in the ﬁle-entry cache, and therefore doing this protocol