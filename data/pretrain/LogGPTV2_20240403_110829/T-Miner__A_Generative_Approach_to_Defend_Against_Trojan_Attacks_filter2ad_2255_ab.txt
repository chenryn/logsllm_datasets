cides on a trigger phrase, which is a sequence of words. The
second step is to generate the poisoned samples to be injected
into the training process. In a training dataset, the attacker
randomly chooses a certain fraction of samples (called injec-
tion rate) to poison. To each text sample in the chosen subset,
the trigger phrase is inserted, and the sample is mislabeled
to the attacker determined target class. Lastly, the DNN is
trained using the original dataset and the poisoned samples,
so that it learns to correctly classify clean inputs, as well as
learn associations between the trigger phrase and the target
label.
A successful Trojan injection process should achieve two
key goals: (1) The Trojan model has a similar classiﬁcation
accuracy on clean inputs as the clean version of the model
(i.e. when trained without poisoned samples). (2) The Trojan
model has high attack success rate on inputs with the trigger
phrase, i.e. the fraction of inputs with the trigger correctly
(mis)classiﬁed to the target label.
Injection process & choice of the trigger phrase. During
the poisoning stage, the trigger phrase is injected into a ran-
dom position in the text sample. Recall that the defender has
no access to the training dataset. Hence, such an injection
strategy does not weaken the attack. Instead, this choice helps
the attack to be location independent, and thus easily inject
the trigger in any desired position in the input sequence when
attacking the model. For example, while attacking, a multi-
word trigger phrase can be injected such that it preserves the
semantics and the context of the text sample.
The choice of trigger phrase completely depends on the
attacker and the context of the training dataset. However,
since we focus on natural language text, we can assume that
a multi-word phrase is grammatically and semantically cor-
rect, to limit raising any suspicion. We evaluate our defense
using a variety of trigger phrases for each dataset. Table 8
in Appendix D shows samples of trigger phrases used in our
evaluation. Later in Section 7, we consider more advanced
poisoning scenarios where we vary trigger selection, and in-
jection strategies.
4 T-Miner: Defense Framework
4.1 Method Overview
Basic idea. Without loss of generality, we consider the fol-
lowing setting — there is a source class s, and a target class
t for the text classiﬁer being tested (for Trojan). Our goal is
to detect if there is a backdoor such that when the trigger
phrase is added to text samples from s, it is misclassiﬁed to
t. Since the defender has no knowledge of the trigger phrase,
our idea is to view this as a problem of ﬁnding “abnormal”
perturbations to samples in s to misclassify them to t. We
deﬁne a perturbation as any new tokens (words) added to the
sample in s to misclassify it to t. But why abnormal? There
are many ways to perturb samples in s to transfer to t, e.g., by
just making heavy modiﬁcations to text in s, or by computing
traditional adversarial perturbations [1, 43]. However, ﬁnding
such perturbations will not help us determine if the model is
infected. Hence, our hypothesis is that a perturbation that (1)
can misclassify most (or all) samples in s to t, and (2) stand
out as an outlier in an internal representation space of the
classiﬁer, is likely to be a Trojan perturbation. We note that
property (1) is insufﬁcient to determine Trojan behavior, and
hence include (2). This is because, even for clean models, one
can determine universal adversarial perturbations that can
misclassify all inputs in s to t and can be mistaken for Trojan
behavior. Prior work has explored such universal perturba-
tions in the context of image classiﬁcation [12, 25, 39, 40],
and we observe such behavior in text models as well [4, 55].
This is an inherent vulnerability of most text DNN models
(and an orthogonal problem), while our focus is on ﬁnding
vulnerabilities deliberately injected into the model.
To determine abnormal perturbations, we use a text style
transfer framework [28]. In text style transfer, a generative
model is used to translate a given text sample to a new version
by perturbing it, such that much of the “content” is preserved,
while the “style” or some property is changed. For example,
prior work has demonstrated changing the sentiment of text
using style transfer [28]. In our case, we want to ﬁnd per-
turbations that preserve much of the text in a sample in s,
but changes the style to that of class t (i.e. property we are
changing is the class). This ﬁts the Trojan attack scenario,
because the attacker only adds the trigger phrase to an input,
keeping much of the existing content preserved. In addition,
a more important requirement of the generative framework
is to produce perturbations that contain the trigger phrase.
Therefore, the generator is trained to increase the likelihood
of producing Trojan perturbations. To achieve this, the gener-
ation pipeline is conditioned on the classiﬁer under test. In
other words, the classiﬁer serves as a discriminator for the
generator to learn whether it correctly changed the “style” or
class to the target label.
Overview of the detection pipeline. Figure 1 provides an
overview of our pipeline. There are two main components, a
2258    30th USENIX Security Symposium
USENIX Association
Figure 1: T-Miner’s detection pipeline includes the Perturbation Generator and the Trojan Identiﬁer. Given a classiﬁer as a
suspect model, it determines whether the classiﬁer is a Trojan model or a clean model.
Perturbation Generator, and a Trojan Identiﬁer. These two
components are used in conjunction with the classiﬁer (under
test). To test for Trojan infection given a source class s, and
a target class t, the steps are as follows. 1(cid:13) Text samples be-
longing to class s are fed to the Perturbation Generator. The
generator ﬁnds perturbations for these samples, producing
new text samples, likely belonging to the class t. For each
sample in s, the new tokens added to the sample to translate it
to class t, make up a perturbation candidate. A perturbation
candidate is likely to contain Trojan triggers if the classiﬁer is
infected. 2(cid:13) The perturbation candidates are fed to the Trojan
Identiﬁer component, which analyzes these perturbations to
determine if the model is infected. This involves two internal
steps: First, the perturbation candidates are ﬁltered to only
include those that can misclassify most inputs in s to t (a
requirement for Trojan behavior). We call these ﬁltered per-
turbations as adversarial perturbations. Second, if any of the
adversarial perturbations stand out as an outlier (when com-
pared to other randomly constructed perturbations or auxiliary
phrases) in an internal representation space of the classiﬁer,
the classiﬁer is marked as infected. Next, we describe each
component in detail.
4.2 Perturbation Generator
Overview of the generative model. Figure 1 illustrates the
architecture of our generative model. Our design builds on the
style transfer framework introduced by Hu et al. [28]. Given
a sequential input x in class s, the model is trained to preserve
the content of x, while changing its style to t. To achieve
this objective, we use a GRU-RNN [9] Encoder-Decoder ar-
chitecture which learns to preserve the input contents, while
receiving feedback from the classiﬁer C (under test) to pro-
duce perturbations to classify to t.
Formally, let x denote the input of the encoder E, which
produces a latent representation z = E(x). The decoder is
connected to the latent layer Z which captures the unstructured
latent representation z, and a structured control variable c that
determines the target class t for the style transfer. Finally, the
decoder D, connected to the layer Z is used to sample output
ˆx with the desired class t.
Training data for generator. Recall that our defense does
not need access to clean inputs. Instead, we craft synthetic
inputs to train the generator. Synthetic inputs are created by
randomly sampling tokens (words) from the vocabulary space
of the classiﬁer, and thus basically appears as nonsensical text
inputs. A synthetic sample consists of a sequence of k such
tokens. This gives us a large corpus of unlabeled samples, χu.
To train the generator, we need a labeled dataset χL of samples
belonging to the source and target classes. This is obtained
by interpreting the classiﬁer C as a likelihood probability
function pC, each sample in χL is labeled according to pC.
Similar to the work by Hu et al. [28] (on which our design is
based), we only require a limited number of samples for the
labeled dataset, as we also pre-train the generator without the
classiﬁer using the unlabeled samples χu.
Generative model learning. The decoder D, produces an
output sequence of tokens, ˆx = { ˆw1, ..., ˆwk} with the target
class decided by the control variable c. The generator distri-
bution can be expressed as:
ˆx ∼ D(z,c) = pD( ˆx|z,c) = ∏ p( ˆwn|( ˆw1, ..., ˆwn−1),z,c) (1)
At each time step n, a new token is generated by sampling
from a multinomial distribution using a softmax function, i.e.
ˆwn = so f tmax(On), where On is the logit representation fed to
the softmax. ˆwn is a probability distribution over all possible
tokens in the vocabulary, at position n in the output. To sample
a token using ˆwt, one strategy is to use a greedy search, which
selects the most probable token in the distribution.
Next, we discuss the three training objectives of the genera-
tor. Let θE and θD be the trainable parameters of the encoder
and decoder components, respectively.
(1) Reconstruction loss. This loss term LR(θE ,θD) aims to
preserve the contents of the input, and helps to keep the per-
turbation limited. This is deﬁned as follows:
LR(θE ,θD) = Epdata(x)p(z)[l(x, ˆx|z)]
(2)
where, l(.) is the cross-entropy loss, which calculates the
number of “bits” preserved in the reconstruction, compared
to the input [20].
(2) Classiﬁcation loss. The second objective is to control
the style (class) of ˆx. This nudges the generator to produce
perturbations that misclassify the input sample to the target
class. Classiﬁcation loss LC(θD) is again implemented using
cross-entropy loss l(.):
LC(θD) = Epdata( ˆx)[l(pC(c| ˆx),c)]
(3)
USENIX Association
30th USENIX Security Symposium    2259
(1) Perturbation GeneratorXX̂EncoderZ CAdversarialPerturbationsHidden Layer   RepresentationΔ FilteringAuxiliary  Phrases(2) Trojan IdentiﬁerOutliersNo  OutliersPerturbationCandidates ( Δ )Dimensionality ReductionOutlier DetectionTrojan ModelClean ModelDecoderSuspect ModelTo enable gradient propagation from the classiﬁer C through
the discrete tokens, ˆx is a soft-vector obtained from the soft-
max function, instead of a sequence of hard sampled tokens
(represented as one-hot vectors).
(3) Diversity loss. The previous two loss terms (used in [28])
are sufﬁcient for ﬁnding perturbations to misclassify a given
sample to the target class. However, they are insufﬁcient to
increase the likelihood of ﬁnding Trojan perturbations (pertur-
bations containing trigger tokens). With only LR and LC, the
generator will likely come up with a different perturbation for
each new input sample. Instead, we want to ﬁnd a perturbation
that when applied to any sample in s, will translate it to class
t. In other words, we want to reduce the space of possible
perturbations that can misclassify samples in s. To enable
this, we introduce a new training objective called diversity
loss Ldiv, which aims to reduce the diversity of perturbations
identiﬁed by the generator, thus further narrowing towards a
Trojan perturbation.
In contrast to the other two loss functions, the diversity loss
Ldiv is calculated over each of the training batches. Formally,
let M = {m1,m2, ...,mn} indicates the set of input batches
and X = {x1,x2, ...,xN} denote inputs in m ∈ M. Consider
ˆX = G(X) = { ˆx1, ˆx2, ..., ˆxN} are the generated samples by our
generative model G. Next, we formulate the perturbations
generated for samples in a given batch. Therefore, the set of
perturbations δm in batch m can be formulated as:
δm = {clip( ˆx1 − x1), ...,clip( ˆxN − xN)}
where clip(.) clips elements to the range (0,1). Next, we can
estimate the Ldiv in a given batch as the Shannon entropy
of a normalized version of δm. As the loss term decreases,
the diversity of perturbations decreases, thus increasing the
likelihood of ﬁnding the Trojan perturbations. Algorithm 2 in
Appendix F presents the diversity loss computation.
Combined training objective. Combining all three loss func-
tions, we obtain the generator objectives as follows:
LG(θE ,θD) = λRLR(θE ,θD) +λcLc(θD) +λdivLdiv(θD) (4)
A set of inputs χL, labeled by the classiﬁer, is used to train
the generative model based on Equation 4. Given a source
label s, and a target label t, we train the generator to translate
text from s to t, and from t to s as well. Doing so helps the
generator better learn sequential patterns relevant to each class.
Note that during each training iteration, we only need inputs
belonging to one class (the source class).
Extracting perturbation candidates. Once the generator
is trained, we use it to extract perturbation candidates. This
is done by feeding a set of synthetic samples X belonging
to a source class s to the generator, to obtain output samples
ˆX. Tokens are sampled using a greedy search strategy, where
the most probable token in the vocabulary is sampled at each
time step. Given an input sample x ∈ X, and an output ˆx ∈
ˆX, the perturbation δ is the ordered4 sequence of tokens in
4We choose the order in which they appear in ˆx.
ˆx, that are not in x. Then, for a set of inputs X, we deﬁne
the perturbation candidates as the set of perturbations ∆ =
(δ1, ...,δN) after eliminating duplicate perturbations. Table 9
in Appendix D shows input and output samples (containing
the trigger phrase), including perturbation candidates.
Expanding perturbation candidates set via Top-K search. In
practice, we ﬁnd that the greedy search sometimes fails to
produce perturbations containing the trigger phrase. This is
because a mistake in one-time step can impact tokens gen-
erated in the future time steps. To overcome this limitation,
we expand an existing perturbation candidate set ∆ using a
Top-K search strategy. We further derive more candidates
from each perturbation δi ∈ ∆. Given a δi, for each token in
this perturbation, we identify the Top-K other tokens based
on the probability score (at the time step the token was sam-
pled). Next, each new token in the Top-K is combined with
the other tokens in δi to obtain K new perturbation candidates.
This process is repeated for each token in δi, thus produc-
ing new perturbation candidates. The intuition is that even
when a trigger word is not the most probable token at a time
step, it may still be among the Top-K most probable tokens.
Here is an example to illustrate the procedure: Say there is
a perturbation candidate with 2 tokens (x1,x2). We can then
create the following additional perturbation candidates using
2), where xi
Top-2 search: (x1
k
denotes the top-i token in the time step xk was sampled.
4.3 Trojan Identiﬁer
This component uses the perturbation candidates from the
previous step and performs the following steps.
Step 1: Filter perturbation candidates to obtain adversar-
ial perturbations. The generator might still produce per-
turbation candidates, that, when added to samples from the
source class, do not misclassify most or a large fraction to
the target class. Such candidates are unlikely to be Trojan
perturbations (i.e. contain tokens from the trigger phrase).
Hence, we ﬁlter out such candidates.
2), and (x1,x2
1,x2), (x2
1,x2), (x1,x1
Given the set of perturbation candidates, we inject each
candidate, as a single phrase to synthetic samples (in a ran-
dom position) belonging to the source class. Any candidate
that achieves a misclassiﬁcation rate (MRS) (on the synthetic
dataset) greater than a threshold αthreshold is considered to be
an adversarial perturbation and used in our subsequent step.
All other perturbation candidates with MRS  3K) is reduced using PCA [27,45]. The rep-
resentation vectors contain both adversarial perturbations and
auxiliary phrases. Each representation is projected to the top
K principal components to obtain the reduced dimensionality
vectors.
DBSCAN [15] is used for detecting outliers, which takes
as input the reduced dimensionality vectors. We also exper-
imented with other outlier detection schemes such as one-
class SVM, Local Outlier Factor, and Isolation Forest, but
ﬁnd DSCBAN to be most robust and accurate in our setting.
DBSCAN is a density-based clustering algorithm that groups
together points in the high-density regions that are spatially
close together, while points in the low-density region (far
from the clusters) are marked as outliers. DBSCAN utilizes
two parameters: min-points and ε. Min-points parameter de-
termines the number of neighboring data points required to
form a cluster, and ε is the maximum distance around data
points that determines the neighboring boundary. We describe
how we estimate these parameters in Section 5.3.
Algorithm 1 in the Appendix further summarizes the key
steps of T-Miner’s entire detection pipeline.
5 Experimental Setup
We discuss the classiﬁcation tasks, associated models, and
setup for the T-Miner defense framework.
5.1 Classiﬁcation Tasks
T-Miner is evaluated on 5 classiﬁcation tasks. To evaluate
threats in a realistic context, classiﬁers are designed to deliver
high accuracy. Classiﬁers retain this performance while ex-
hibiting high attack success rates when infected. This ensures
that the attacked classiﬁers possess Trojan backdoors that are
both stealthy and effective.
Yelp. This task classiﬁes restaurant reviews into positive,
and negative sentiment reviews. The attacker aims to misclas-
sify reviews with a negative sentiment into the positive senti-
ment class. To build the classiﬁer, we combine two Yelp-NYC
restaurant review datasets introduced by prior work [46, 48].
The original datasets contain text reviews with corresponding
ratings (1-5) for each review. Reviews with ratings of 1 and 2