such as appends, ﬁle reads, directory lookups, creations,
and deletions. This benchmark uses little CPU, but is
I/O intensive. We conﬁgured Postmark to create 500
ﬁles, each between 4KB and 1MB, and perform 5,000
transactions. We chose 1MB as the ﬁle size as it was the
average inbox size on our large campus mail server. For
this conﬁguration, 45.7% of the operations are writes,
31.7% are reads and the remaining are a mix of opera-
tions like open, lookup, etc. (We used Tracefs [2] to
measure the exact distribution of operations in the Am-
Utils and Postmark benchmarks.)
The third benchmark compares various user-level
command-line scanners available today with our scan-
ner. We scanned two clean 1GB ﬁles. The ﬁrst ﬁle had
random bytes and the second ﬁle was a concatenation
of ﬁles in /usr/lib. The latter represents various ex-
ecutables and hence exercises various parts of the scan-
ning trie under more realistic circumstances than random
data. Overall, this workload exercises both scanning for
viruses and also loading the virus database. Note that the
Oyster module and its user-level counterpart have almost
identical code with the exception of memory allocation
functions (kmalloc vs. malloc) and some kernel spe-
ciﬁc data structures.
)
c
e
s
(
i
e
m
T
d
e
s
p
a
E
l
 2500
 2000
 1500
 1000
 500
 0
 0
Regular/Immediate
Regular/Deferred
Full/Immediate
Full/Deferred
ClamAV/Dazuko
Ext3
)
c
e
s
(
i
e
m
T
d
e
s
p
a
E
l
 20
 40
 60
 80
 100
 120
 140
Database Size (thousands of signatures)
 300
 280
 260
 240
 220
 200
 180
 0
Regular/Immediate
Regular/Deferred
Full/Immediate
Full/Deferred
Ext3
 20
 40
 60
 80
 100
 120
 140
Database Size (thousands of signatures)
Figure 10: Am-Utils results. The ﬁgure on the left shows results for all Avfs modes, ClamAV, and Ext3. The ﬁgure on the right
shows a detailed view for Avfs and Ext3 (note that the Y axis starts at 180 seconds).
6.3 Test Virus Databases
6.4 Am-Utils Results
Figure 10 shows the performance for the Am-Utils com-
pile benchmark using various database sizes. The left
hand side of this ﬁgure shows the results for four Avfs
modes, ClamAV, and Ext3. The right hand side of this
ﬁgure shows a detailed view for Avfs modes and Ext3.
Table 1 summarizes the Am-Utils benchmark results.
Ext3
Full Deferred
ClamAV
Size Elapsed System Elapsed System Elapsed System
81.5
1K
225.4
118.3
4K
262.9
19.8K
289.3
433.5
1052.8
64K
908.8
128K
2077.4 1933.0
196.9
196.9
196.9
196.9
196.9
42.4
42.4
42.4
42.4
42.4
207.7
211.7
225.5
260.6
299.9
52.1
56.2
69.7
105.4
144.5
Overhead over Ext3 (%)
22.8
5.5
32.5
7.5
14.5
64.3
32.3 148.4
52.3 240.4
-
-
-
-
-
1K
4K
19.8K
64K
128K
-
-
-
-
-
92.2
14.5
179.0
33.5
120.2
582.3
434.7 2043.4
955.0 4459.0
To evaluate the performance and the scalability of our
Oyster virus scanner, we had to generate test virus
databases with different numbers of virus signatures. To
generate a virus database with fewer than the 19,807 pat-
terns contained in the current ClamAV virus database,
we simply picked signatures at random from the origi-
nal database.
The generation of realistic larger databases was more
involved. The most straightforward approach was to
simply generate random virus signatures. However,
as described in Section 3.2.1, this approach would not
yield a representative worst-case virus database. Instead,
we obtained the following statistics from the existing
ClamAV virus database: (1) the distribution of all unique
four character preﬁxes, Dp; (2) the distribution of virus
signature lengths, Dl; (3) the percentage of multi-part
patterns in the database, Pm; and (4) the distribution of
the number of sub-patterns for each multi-part pattern,
Ds. The preﬁxes of length four were chosen because
this number was larger then the minimum trie height pa-
rameter in our experiments. To generate one signature,
we ﬁrst determined at random whether the new signa-
ture will be a basic or a multi-part signature using the
percentage Pm. If the new signature is a multi-part sig-
nature, we determined the number of sub-parts based on
the distribution Ds, and then generated one basic pat-
tern for each sub-part. To generate a basic signature,
we randomly sampled from the distribution Dp to deter-
mine the preﬁx that will be used for this pattern; next,
we sampled from the distribution Dl to determine the
length of the signature. If the length is greater than four
bytes, the remaining characters are generated randomly.
The above process was repeated as many times as nec-
essary to generate a database of the desired size. For
our evaluation, we generated databases ranging from 210
to 217 (128K) signatures. We veriﬁed that the resulting
databases had distribution characteristics similar to the
current ClamAV database.
Table 1: Am-Utils build times. Elapsed and System times
are in seconds. The size of 19.8K corresponds to the current
ClamAV database.
The Oyster scanner was conﬁgured to use trie heights
of three for both minimum and maximum trie height pa-
rameters. A minimum height of three gave us the best
performance for all databases and a maximum height of
three gave us the best performance for databases of small
sizes like 1K, 2K, 4K, and 8K patterns. We demon-
strate later in this section how the maximum height pa-
rameter can be varied to improve performance for large
databases.
All of
the modes have similar overhead over
Ext3, with the slowest mode, FULL/DEFERRED, be-
ing 0.5–2.7% slower than the fastest mode, REGU-
LAR/IMMEDIATE.
In the FULL/DEFERRED mode, the
elapsed time overheads over Ext3 varied from 5.5%
for a 1K pattern database to 52.3% for a 128K pattern
database, whereas the system time overhead varies from
22.8% to 240.4%. Due to I/O interleaving, a large per-
centage increase in the system time does not result in
the same increase in elapsed time. The increase in the
elapsed time is almost entirely due to the higher system
time. This increase in system time is due to the larger
database sizes. The Oyster module proved to scale well
as the database size increased: a 128 times increase in
the database size from 1K to 128K patterns resulted in
elapsed time increase from 207.7 seconds to 299.9 sec-
onds, a merely 44.4% increase in scan times. For the
same set of databases, ClamAV’s elapsed time increases
from 225.4 seconds to 2,077.4 seconds—a 9.2 factor in-
crease in scan times.
i
)
c
e
s
(
e
m
T
d
e
s
p
a
E
l
 350
 300
 250
 200
 150
 100
 50
 0
299s
252s
239s
Wait
User
System
240s
Trie 
Figure 11: Am-Utils build times using the FULL/DEFERRED
mode with a 128K signature database.
Max Level Mem Usage (cid:1) Time Speed Gain
0s
45s
13s
(cid:0)0.5s
45MB
0s
60MB (cid:0)45s
182MB (cid:0)58s
199MB (cid:0)57.5s
3
4
5
6
Table 2: Effect of the maximum trie height parameter on
speed and memory usage. The speed gain column shows the
speed improvement over the previous maximum trie level.
The scalability for larger databases can be further im-
proved by adjusting the minimum and maximum trie
height parameters. We conﬁgured Avfs to use the slow-
est mode, FULL/DEFERRED. The Oyster module was
conﬁgured to use a database of 128K virus signatures,
and a minimum height of three. We repeated the Am-
Utils benchmark with various maximum trie height pa-
rameters. Figure 11 shows the result of this experi-
ment. Table 2 summarizes improvements in speed and
increases in memory usage. A maximum height of ﬁve
proved to be the fastest, but a height of four provided
a reasonable increase in speed while using signiﬁcantly
less memory than a height of ﬁve. A system administra-
tor has a lot of ﬂexibility in tuning the performance of
the system. If speed is very important, then a maximum
trie height of ﬁve can be used. However, if the memory
availability is tight, then a maximum trie height of four
provides a reasonable performance improvement with a
smaller memory footprint than a height of ﬁve.
6.5 Postmark Results
Figure 12 shows the results of running Postmark with
on-access scanners: ClamAV, H+BEDV, and all four
modes of Avfs. It also shows the time taken for a Post-
mark run on Ext3.
)
c
e
s
(
i
e
m
T
d
e
s
p
a
E
l
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
Wait