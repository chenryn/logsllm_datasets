title:Ijon: Exploring Deep State Spaces via Fuzzing
author:Cornelius Aschermann and
Sergej Schumilo and
Ali Abbasi and
Thorsten Holz
2020 IEEE Symposium on Security and Privacy
IJON: Exploring Deep State Spaces via Fuzzing
Cornelius Aschermann, Sergej Schumilo, Ali Abbasi, and Thorsten Holz
Ruhr University Bochum
Fig. 1: AFL and AFL + IJON trying to defeat Bowser in Super Mario Bros. (Level 3-4). The lines are the traces of all runs found by the fuzzer.
Abstract—Although current fuzz testing (fuzzing) methods are
highly effective, there are still many situations such as complex
state machines where fully automated approaches fail. State-of-
the-art fuzzing methods offer very limited ability for a human
to interact and aid the fuzzer in such cases. More speciﬁcally,
most current approaches are limited to adding a dictionary or
new seed inputs to guide the fuzzer. When dealing with complex
programs, these mechanisms are unable to uncover new parts of
the code base.
In this paper, we propose IJON, an annotation mechanism
that a human analyst can use to guide the fuzzer. In contrast
to the two aforementioned techniques, this approach allows a
more systematic exploration of the program’s behavior based on
the data representing the internal state of the program. As a
consequence, using only a small (usually one line) annotation, a
user can help the fuzzer to solve previously unsolvable challenges.
We extended various AFL-based fuzzers with the ability to
annotate the source code of the target application with guidance
hints. Our evaluation demonstrates that such simple annotations
are able to solve problems that—to the best of our knowledge—
no other current fuzzer or symbolic execution based tool can
overcome. For example, with our extension, a fuzzer is able
to play and solve games such as Super Mario Bros. or resolve
more complex patterns such as hash map lookups. To further
demonstrate the capabilities of our annotations, we use AFL
combined with IJON to uncover both novel security issues and
issues that previously required a custom and comprehensive
grammar to be uncovered. Lastly, we show that using IJON and
AFL, one can solve many challenges from the CGC data set that
resisted all fully automated and human guided attempts so far.
I. INTRODUCTION
In recent years, a large number of software bugs were
uncovered by fuzz testing (short: fuzzing) and this research
area has received signiﬁcant attention in both the academic
community [7], [14], [43], [45], [53], [60] and practice [1],
[33], [61]. As a result, much attention was placed on further
improving fuzzing methods, often to achieve greater code
coverage and reach deeper into a given software application.
Yet, a signiﬁcant number of open challenges remain: Even
with clever program analysis techniques such as symbolic
or concolic execution, some constraints cannot be overcome
easily. Furthermore, in some cases, state explosion proves
too much of a hindrance to current techniques—whether they
are fuzzing or symbolic execution based approaches. This is
due to the fact that the underlying problem (ﬁnding bugs) is
undecidable in the general case. As a result, we cannot expect
that any single algorithm will perform very well across all
target applications that are tested.
Due to this insight, even though signiﬁcant progress has
been made in recent works on improving fully autonomous
fuzzers, some constraints will remain unsolvable no matter
which algorithm is used (e.g., if cryptography is used). In
practice, current approaches struggle to explore complex state
machines, where most progress can only be observed in
changes to the program’s state data. Since each update to the
state data is triggered by certain code, a coverage-based fuzzer
is able to explore each individual update in isolation. However,
there is no feedback that rewards exploring combinations of
different updates leading to new states, if all individual updates
have been observed previously. In cases where a speciﬁc
sequence of updates is needed to uncover a bug, this prevents
the fuzzer from making progress. Similarly, concolic execution
based approaches fail, since the exact sequence of updates
(and consequently the precise code path chosen) is critical to
uncover the bug. Since concolic execution ﬁxes the path to
the observed execution path, it is impossible for the solver to
obtain an input that triggers the target condition. Lastly, even
fully symbolic execution, which is free to explore different
paths, fails if the state space grows too large.
We note that there is a trend to use more complex solutions,
which only support minimal environments/instruction sets,
based on symbolic execution to overcome harder challenges in
fuzzing [43], [45], [53], [60]. On the downside, as observed by
various sources [10], [60], [62], such methods sometimes scale
poorly to complex applications. As a result, they ﬁnd little use
in industry, compared to fuzzers such as LIBFUZZER and AFL.
Google’s OSS fuzz project alone was able to uncover over
27.000 bugs [23] in targets as complex as the Chrome browser
using tools such as LIBFUZZER. Often, it seems, the additional
© 2020, Cornelius Aschermann. Under license to IEEE.
DOI 10.1109/SP40000.2020.00117
1597
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:13 UTC from IEEE Xplore.  Restrictions apply. 
effort to set up and deal with a symbolic environment is not
worth the effort [62].
In this paper, we explore how a human can steer the fuzzer
to overcome current challenges in fuzzing. To paraphrase a
well-known, but hard to attribute quote, “Computers are in-
credibly fast, accurate and stupid; humans are incredibly slow,
inaccurate and brilliant; together they are powerful beyond
imagination”. Humans are often better at forming high-level,
strategic plans, while a computer ensures that the tactics are
working out, and the human does not overlook any important
aspect. This approach is typically referred to as human-in-the-
loop, and is a commonly used concept in software veriﬁca-
tion [8], [15], [34], [39], [41], [52] and throughout various
other ﬁelds such as machine learning [17], [59], controlling
cyber-physical systems [47], [49], and optimization [24], [51].
Our approach is also motivated by the observation that
many fuzzing practitioners in the industry already use a closed
feedback loop in their fuzzing process [35]: First, they run
the fuzzer for some time and then analyze the resulting code
coverage. After this manual analysis, they tweak and adapt
the fuzzing process to increase coverage. Common strategies
for improving the fuzzing performance include removing chal-
lenging aspects from the target application (e.g., checksums),
changing the mutation strategies, or explicitly adding input
samples that solve certain constraints that the fuzzer did not
generate in an automated way. This approach has two main
reasons: on the one hand, all the “easy” bugs (i.e., the ones
which can be found fully automatically) are found very quickly
during a fuzzing campaign. On the other hand,
the more
interesting bugs are—by deﬁnition—the ones that cannot be
found using current tools in off-the-shelf conﬁgurations and
hence, some manual tuning is required. We believe that by
assisting and steering the fuzzing process, humans interacting
with fuzzers allow for a vastly increased ability to analyze
applications and overcome many of the current obstacles
related to fuzzing of complex applications.
Speciﬁcally, we focus on a particular class of challenges: we
observe that current fuzzers are not able to properly explore the
state space of a program beyond code coverage. For example,
program executions that result in the same code coverage, but
different values in the state, cannot be explored appropriately
by current fuzzers. In general, the problem of exploring state
is challenging, as it is difﬁcult to automatically infer which
values are interesting and which are not. However, a human
with a high-level understanding of the program’s goals, often
knows which values are relevant and which are not. For
example, a human might know that exploring different player
positions is relevant to solve a game, while the positions of
all enemies in the game world are not.
We show that a human analyst can annotate parts of the
state space that should be explored more thoroughly, hence
modifying the feedback function the fuzzer can use. The
required annotations are typically small, often only one or
two lines of additional code are needed. To demonstrate the
practical feasibility of the proposed approach, we extended
various AFL-based fuzzers with the ability to annotate the
source code of the target application with hints to guide the
fuzzer. Our extension is called IJON, named after Ijon Tichy,
the famous space explorer from Stanislaw Lem’s books [31].
In four case studies, we show that the annotations can help to
overcome signiﬁcant roadblocks and to explore more interest-
ing behaviors. For example, using simple annotations, we are
able to play Super Mario Bros. (as illustrated in Figure 1) and
to solve hard instances from the CGC challenge data set.
In summary, we make the following contributions in this paper:
• We systematically analyze feedback methods imple-
mented in current fuzzers, study how they represent state
space, and investigate in which cases they fail in practice.
• We design a set of extensions for current feedback fuzzers
that allow a human analyst to guide the fuzzer through the
state space of the application and to solve hard constraints
where current approaches fail.
• We demonstrate in several case studies how these anno-
tations can be used to explore deeper behaviors of the
target application. More speciﬁcally, we show how the
state space of a software emulator for a Trusted Platform
Module (TPM), complex format parsers, the game Super
Mario Bros., a maze, and a hash map implementation
can be efﬁciently explored by a fuzzer. Additionally, we
demonstrate, that our approach enables us to solve some
of the most difﬁcult challenges in the CGC data set, and
ﬁnd new vulnerabilities in real-world software.
The implementation of IJON and a complete data set that
illustrates our evaluation results, including playthrough videos
for the case studies we present in this paper, is available at
https://github.com/RUB-SysSec/ijon.
II. TECHNICAL BACKGROUND
Our work builds upon fuzzers from the AFL [61] family
such as ANGORA [14], AFLFAST [11], QSYM [60], or LAF-
INTEL [1]. To explain the technical details of our approach,
we hence need to introduce some aspects of the inner working
of AFL itself. Fuzzers from the AFL family generally try to
ﬁnd a corpus that triggers a large variety of different states
from the state space of the program. Here, a state denotes
one conﬁguration of memory and registers, as well as the
state provided by the OS (e.g., ﬁle descriptors and similar
primitives). The state space is the set of all possible states a
program can be in. Since even for trivially small programs, the
state space is larger than the number of atoms in the universe,
the fuzzer has to optimize the diversity of states reached by the
test cases. This class of fuzzers typically uses code coverage to
decide if an input reaches sufﬁciently different state than the
ones existing in the corpus. We make use of the bitmap that
AFL uses to track this coverage. Hence, we start by explaining
the design of the coverage feedback used by AFL. Afterward,
we discuss some of the consequences of this design when
using a fuzzer to optimize the state coverage.
A. AFL Coverage Feedback
Various forks of AFL use instrumentation to obtain test
coverage information. Typically, AFL-style fuzzers track how
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:13 UTC from IEEE Xplore.  Restrictions apply. 
1598
often individual edges in the control ﬂow graph (CFG) are exe-
cuted during each test input. There are two classes of feedback
mechanisms commonly used: source code based forks of AFL
typically use a custom compiler pass that annotates all edges
with a custom piece of code. Binary-only versions of AFL use
different mechanisms such as Dynamic Binary Instrumentation
(DBI) or hardware accelerated tracing (typically Intel LBR
or Intel PT) to obtain coverage information. Either way, the
probes inserted into the target application then count
the
occurrences of each edge in the CFG and store them in a
densely encoded representation.
The resulting information is stored in a shared map that
accumulates all edge counts during each test run. The fuzzer
additionally maintains a global bitmap that contains all edge
coverage encountered during the whole fuzzing campaign. The
global bitmap is used to quickly check if a test input has
triggered new coverage. AFL considers a test input interesting
and stores it if it contains a previously unseen number of
iterations on any edge. Since edges always connect
two
basic blocks, edges are encoded as a tuple consisting of two
identiﬁers, one for the source basic block ids and one for a
target block idt. In the source code based versions, a static
random value is assigned at compile-time to each basic block,
which is used as ids or idt. For binary-only implementations,
it is common to use a cheap hash function applied to the
address of the jump instruction/target instruction to derive the
ids and idt values, respectively. This tuple (ids, idt) is then
used to index a byte in the shared map. Typically, the index
is calculated as (ids ∗ 2) ⊕ idt. The multiplication is used to
efﬁciently distinguish self-loops.
Before each new test run,
the shared map is cleared.
During the test run, each time an edge is encountered, the
corresponding byte in the shared map is incremented. This
implies that edge counts greater than 255 will overﬂow and
might register as any number between 0 and 255. After the
execution ﬁnished, the edge counts are bucketed such that
each byte in the shared map with a non-zero edge count
contains a power of 2. To this end, edge counts are discretized
into the following ranges 1, 2, 3, 4 . . . 7, 8 . . . 15, 16 . . . 31,
32 . . . 127, 128 . . . 255. Each range of edge counts is assigned
to one speciﬁc power of 2. To increase the precision on
uncommon edges, 3 also maps to a unique power of 2, while
the range 32 to 64 is omitted. Then we can compare the shared
map against a global bitmap, which contains all bits that were
previously observed in prior runs. If any new bit is set, the
test input is stored because it has led to increased coverage,
and the global bitmap is updated to contain the new coverage.
B. Extending Feedback Beyond Coverage
Fuzzers sometimes get stuck in a part of the search space,
where no reasonable, probable mutation provides any new
feedback. In this paper, we develop novel ways to provide
a smoother feedback landscape. Consequently, we now re-
view various methods that were proposed to extend feedback
mechanism beyond code coverage to avoid getting stuck on
a plateau. Notably, LAF-INTEL [1] was an early approach
to solve magic byte type constraints (e.g., if (input ==
0xdeadbeef)) by splitting large compare instructions into
multiple smaller ones. The same idea was later implemented
by using dynamic binary instrumentation in a tool called
STEELIX [32]. Splitting multi-byte compare instructions into
multiple single byte instructions allows the fuzzer to ﬁnd new
coverage every time a single byte of the operands is matching.
ANGORA [14] assigns a random identiﬁer to each function. It
uses these identiﬁers to extend the coverage tuple by a third
ﬁeld that contains a hash of the current execution context.
To compute this hash, it combines all identiﬁers of functions
that have been called but have not yet returned (i.e., active
functions) using an XOR operation. This allows ﬁnding the
“same” coverage if it was used in a different calling context.
For example, this method is helpful to solve multiple calls to
the strcmp function. However, the downside of this approach
(i.e., considering all calling contexts as unique) is that in
certain situations, this creates a large number of inputs that
are actually not interesting. Therefore, ANGORA requires a
larger bitmap than AFL.
III. DESIGN
Generally speaking, a fuzzer tries to sample from “inter-
esting” regions of the state space as efﬁciently as possible.
However, it is hard to ﬁnd an accurate and objective metric
for how “interesting” the state space of any given input is.
The overall success of AFL and its derivative demonstrates
that following the edge coverage is an effective metric to
identify new interesting regions. Edge coverage is probably
the feature with the best signal to noise ratio in practice—
after all, in most (i.e., not obfuscated [25], [30]) programs,
each new edge indicates a special case. However, there are
code constructs where this approach is unlikely to reach new
coverage without exploring intermediate points in the state
space. In the following, we analyze code constructs in which
a user could provide additional feedback that would help the
fuzzer by conceptually describing the intermediate steps which
provide additional feedback. Finally, we introduce a novel set
of primitives that actually allow an analyst to add custom
annotations which provide exactly the feedback needed to
overcome these difﬁculties.
A. State Exploration
To identify problematic code constructs that are hard to
fuzz with current techniques, we performed several ofﬂine
experiments using state-of-the-art fuzzers and manually in-
spected the coverage obtained. In some cases, we used seed
ﬁles to ﬁnd code that can be covered using good seeds, but
not without, indicating hard constructs. In the following, we
summarize the most important problems we encountered in
these experiments:
• Known Relevant State Values: Sometimes, code coverage
adds no feedback to help the fuzzer to advance. If only
a small subset of the states is interesting and a human
analyst is able to identify these values, we can directly
use them to guide the fuzzer.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:13 UTC from IEEE Xplore.  Restrictions apply. 
1599
• Known State Changes: Sometimes, the program is too
complex, or it is not obvious which variables contain
interesting state and which ones do not. In such situations,
since no sufﬁciently small set of relevant state values are
known to us, we cannot directly use them to guide the
fuzzer. Instead, a human analyst might be able to identify
positions in the code that are suspected to mutate the
state. An analyst can use the history of such state changes
as an abstraction for the more complex state and guide the
fuzzer. For example, many programs process messages or
chunks of inputs individually. Processing different types
of input chunks most likely mutates the state in different
ways.
• Missing Intermediate State: Unlike the previous two
cases, there might be neither variables that contain the
state, nor code that mutates the state that we care about.
In such situations, an analyst can create artiﬁcial inter-
mediate states to guide the fuzzer.
Based on this systematization of important problems in
fuzzing, we provide examples and describe each of the sug-
gested approaches in more detail.
1) Known Relevant State Values: As described before,
sometimes the code coverage yields nearly no information
about the state of the program, because all the interesting state
is stored in data. For example, the coverage tells very little
about the behavior of a branch-free AES implementation. If
the analyst has an understanding of the variables that store
the interesting state, he can directly expose the state to the