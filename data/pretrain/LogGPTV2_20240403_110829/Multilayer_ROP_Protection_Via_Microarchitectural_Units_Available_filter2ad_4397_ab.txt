Our next veriﬁcation step, the second layer of our system,
relies on a simple idea: we check if the target of a return
instruction is preceded by a call instruction, and if the target
of the call instruction that precedes it lays in executable
memory. The ﬁrst veriﬁcation is the cornerstone of modern
implementations of control ﬂow integrity (CFI) [16]. The
second, however,
is a new idea. We had to add further
veriﬁcation on top of traditional CFI, to mitigate shortcomings
of this technique, already well-known in the literature [32],
[39]. Figure 3 provides an overview of all these interactions.
In the rest of this section we describe our two checks, using
Figure 3 to guide the discussion.
a) The call-preceded constraint:
If the target of a
return instruction is not an address that succeeds a call
instruction, then it is possible that such ﬂow has been artiﬁ-
cially created by an attacker. This observation has already been
extensively explored in the literature [25]. Thus, in the absence
of this condition, we immediately move the application to
the third veriﬁcation layer of our system (Step 1 in Fig. 3),
where we apply stronger checks to certify the legitimacy of
the ﬂow. However, even when this condition is satisﬁed, we
might still be facing a ROP attack. As an example, G¨oktas et
al. have demonstrated how to build ROP exploits under the
call-preceded constraint [32]. Therefore, to prevent exploits
such as the one carried out by G¨oktas et al., we augment this
constraint with an extra check, which we claim as an original
contribution of this paper: the executable target constraint.
b) The Executable Target
(XT) Constraint: Gadgets
whose ﬁrst instruction is preceded by call operations might
emerge by chance in a large binary program, due to unintended
instructions. These are instructions formed by unaligned se-
quences of bits within the binary code. We can show that the
vast majority of such gadgets are invalid through a simple ex-
pedient: we verify if the call instruction targets an executable
memory segment (Step 2 in Fig. 3). Because the executable
318
Address Instruction
83db
mov eax, ds:0xa01c
83e0 add eax, 0x1
1
83e3 mov ds:0xa01c,eax
83e8 nop
83e9 ret  
83ea
83ed
83f4
83f9
83fd
sub esp, 0x4
mov [esp], 0x0
call 83db
add [esp], 0x1
cmp [esp], 0x3e7 
8404 jle 83f4
8406 mov eax, 0x0
840b add esp, 0x4
840e
ret
Return address 
is mispredicted
Get bytes before 
return address:
call 83db
Check if bytes 
form a call instr.
call 83db
Check if target of 
ret is executable
call 83db
Figure 4. Example of the checks performed by the second layer on a program
running on a 16-bit address space.
area in the address space of a program is limited to its “text”
segment, the chance that a misaligned call instruction targets
this segment is very small, especially in 64-bit architectures.
Example 3.1: The Linux version of Chrome 59.0.3071.115
(64-bits) has a large amount of code, resulting in an equally
long (163 MB) text segment. Yet, the chance that a random
address matches an executable address is only 1 in 1011.
The different types of call instructions available in the
x86 range from 2 to 7 bytes in size. Thus, when executing a
return instruction, we look for the 7 bytes that precede the
instruction located in the return address stored at the top of the
stack. We then decode the instruction formed by these bytes
and check if it corresponds to a call. If it is acall, then we
calculate the target address of that instruction. To obtain this
address, we differentiate the types of call by their opcodes
and the byte “ModR/M” in the instructions, as deﬁned in the
manual of the x86 instruction set [40]. Finally, we verify if
the target address of the call instruction has the no-execute
(NX) bit enabled. Notice that processors already check the
NX ﬂag in the page-structure entry of any fetched instruction.
Therefore, the microarchitectural structures that already exist
for this purpose can be reused by our XT Constraint Checker.
Example 3.2: Figure 4 shows a concrete example of checks
performed by layer 2 in a hypothetical 16-bits architecture.
c) XT Enforcement on Indirect Calls: The veriﬁcation
of the target address of indirect calls is not trivial, because
when a return operation executes, the address used by the
call that gave origin to it might no longer be available in
either registers or memory. To deal with this shortcoming,
we resort to a hardware structure called Last Branch Record
(LBR). This feature, available in Intel processors, lets the
CPU log the “from” and “to” address of each branch into
speciﬁc registers. These registers form a ring-buffer, which is
continuously overwritten. Thus, it traces only the most recent
branches. We only record data about call in the LBR. In
addition, we use this hardware structure in its stack mode.
Thus, whenever a call executes, its data is stacked in the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:59 UTC from IEEE Xplore.  Restrictions apply. 
LBR. When executing a return, the most recently stacked
data is removed from the LBR. The main difference between
the LBR stack and RAS is that LBR enforces data savings
on context switches. This allows the LBR to treat some cases
not covered by the RAS. Different processor models have
different LBR sizes: Netburst and Merom have buffers with 4
cells; Nehalem up to Haswell have buffers with 16; Skylake
features 32 positions and the Atom only 8. When faced with
an indirect call preceding a return address, we scan the last
ﬁlled entry of LBR looking for a match (Step 3 in Fig. 3).
Notice that only mispredicted return addresses undergo this
veriﬁcation, because correctly predicted instructions have
already been ﬁltered out in the ﬁrst layer of our system.
Implementation: The XT constraint can be enforced in
processors featuring Return Address Stack and Last Branch
Record capabilities. For instance, current Intel/AMD machines
feature an extension called eXecute Disable: XD in Intel,
and NX (of no-execute) in AMD. This mechanism marks
virtual pages with a bit denoting executable memory. This
bit is consulted at zero overhead. The same is expected for a
hardware comparison with the value stored at the top of LBR.
Prototype: In Section IV we simulate the implementation of
this layer in software via a Pintool, i.e., a tool created with Pin.
The Pintool checks if the targets of direct call instructions
that precede return addresses lay in executable memory. It also
simulates the LBR operation and matches the top of LBR with
indirect call instructions that precede return addresses.
C. Third Level: Treatment of False Positives
It is possible that legitimate return addresses pass unﬁltered
through all our previous sieves because some authentic pro-
gram ﬂows contain return instructions targeting addresses
not preceded by any call instruction. The typical situations
where this event happens have been cataloged by previous
work [41]–[43]: lazy binding of dynamically shared libraries
and signal handling in Unix-based systems.
a) False Positives due to Lazy-Binding: Lazy binding
optimizes the loading time of a program. It consists in delaying
the resolution of the initial address of functions belonging to
shared libraries until these functions are called for the ﬁrst
time. Thus, the dynamic linker does not waste time solving
the address of functions that are never called. In Linux, for
instance, false positives can occur because a return instruction
is “improperly” used by the OS during lazy binding. On
Linux systems, lazy binding is enforced by a combination of
functions that are part of the linux-ld.so dynamic linker
library. After resolving the address of the target function, the
OS transfers the execution ﬂow to the code of that function
through a return instruction. False positives can occur because
the execution ﬂow is diverted to the ﬁrst instruction of the
requested function and there is no guarantee that a valid call
exists immediately before that ﬁrst instruction.
b) False Positives due to Signal Handling:
In Unix-
based systems, signals alert a process about exceptional events
such as the decoding of illegal instructions, the execution of
invalid arithmetic operations and unauthorized accesses of
memory segments. Upon receiving a signal, the OS stacks
the address of the current instruction of the paralyzed process
and transfers the execution ﬂow to the code responsible for
signal handling, without executing a call instruction. When
the signal handler returns, it removes the address stacked
by the OS and transfers the execution back to the normal
program ﬂow through a return instruction. A process can
be stopped by the OS at any instruction; hence, the address
just removed can refer to anywhere. Therefore, the return
address stacked by the OS may not be preceded by a valid
call operation – a factthat might give us false positives.
Detection of False Positives. The two categories of false
positives discussed in this section can be avoided at
the
compiler level or at the hardware level. At the hardware level,
false positives can be identiﬁed via extra checks, such as those
proposed by Xia et al. in the CFIMon system [41]. CFIMon
records performance samples using the Branch Trace Store
(BTS) mechanism. The BTS makes it possible to monitor
executed branches and correlate the traces with sets of valid
targets obtained via static analysis. To avoid triggering false
alarms, Xia et al.’s monitor is informed by the operating
system whenever the signal handler is invoked.
At the compiler level, we can change speciﬁc return
statements by a pop rg; jmp rg sequence. The return
address r is dumped in register rg. Then the control ﬂow is
transferred to r via an indirect jmp. Zhang and Sekar propose
a similar solution to circumvent the idiosyncrasy related to
using a return instruction for lazy symbol resolution [43]. In
our case, this modiﬁcation must be done in the dynamic linker
and in the signal handler. As Section IV-C will show, this
replacement does not incur into any overhead of statistical
signiﬁcance on a modern x86 machine.
Implementation: The strategy used to handle false positives
depends on the implementation of Layer 2. In Section IV
we shall compare two implementations: one based on the
validation techniques seen in Section III-B, and another based
on Control-Flow Enforcement Technology (CET). The imple-
mentation discussed in Section III-B let us avoid false positives
at the compilation level. In this case, no runtime check is
required1. On the other hand, the CET-like implementation of
Layer 2 requires runtime validation at the level of Layer 3. In
this case, we can reuse either the techniques employed in the
CFIMon tool [41], or the rules discussed by Qiu et al. [44,
Sec.V]. Both approaches require circuitry not yet in place in
current x86 processors.
Prototype: In Section IV-C4 we shall implement the compiler-
level enforcement of Layer 3 via manual replacement of
instructions in assembly codes. To experiment with CET,
we shall simulate it in Pin, following a description publicly
available of the Intel implementation [29].
1If false positives are avoided at the compilation level, then our Layer
3 becomes inexistent at runtime. We preferred to leave it in our model, to
emphasize that it exists to handle false positives produced at Layer 2.
319
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:59 UTC from IEEE Xplore.  Restrictions apply. 
D. Handling Code that Slips through the Third Layer
If the target of a return slips through our three sieves,
then, with strong probability, the program ﬂow might have
been diverted by a malicious user. There are different ways
to handle attacks. Solutions can go from simply terminating
the program to sandboxing it. Sandboxes are implemented
through emulated or virtualized runtime environments [45].
They often detect malicious behavior by monitoring system
calls [46]. Previous reports indicate that the computational cost
of emulating a program ranges from 2 to 40%, depending
on implementation choices and the amount of system calls
executed by the monitored application [47]. Nevertheless, as
we demonstrate in the next section, very few programs under
normal execution would pay this cost, in case sandboxing were
the solution of choice to treat suspicious ﬂows.
IV. EVALUATION
In this Section, we investigate three research questions:
• RQ1: How effective is our method to reduce the number
of gadgets available for an exploit?
• RQ2: What
is the proportion of targets of indirect
branches that are ﬁltered at each protection layer?
• RQ3: What is the overhead of our approach?
We answer these questions empirically. In Section IV-A we
use an assortment of dynamic analysis tools to count how
many gadgets our system leaves available to an attacker. In
Section IV-B we use Linux Perf [48] to count how many
branches slip from Layer 1 to Layer 2, and Pin to measure
leaks from Layer 2 to Layer 3. Finally, in Section IV-C we
evaluate the overhead of our approach using an analytical
argument supported with data from Pin. We also resort to
manual interventions in assembly programs to evaluate the
overhead of a compiler-based implementation of Layer 3. The
tools used in the evaluation are publicly available on our web
server (http://cuda.dcc.ufmg.br/multilayer-rop-protection/).
Benchmarks. We use several benchmark suites in this paper,
because each set ﬁts different purposes. To probe gadget
reduction and false positives, we use 4 vulnerable applications
for which we could reproduce 5 public ROP exploits (exploits
17634, 17672, 17974, 12189, 13834 at www.exploit-db.com)
and the 3 most popular desktop web browsers [49]. To count
branch mispredictions and measure overhead we use all the
programs in SPEC CPU2006 and in the LLVM Test Suite [27].
Following Hennessy and Patterson’s methodology [50, Sec-
1.8], averages that we report to summarize results in the ﬁgures
are weighted by the number of instructions: programs that run
more instructions contribute more towards the ﬁnal average.
System Setup. We use the following runtime environment:
• System: Intel Xeon E5-2620 2.00GHz, 12 cores, 32K
(L1d) / 32K (L1i) / 256K (L2) / 15360K (L3) caches,
16GB RAM (4x4096MB DDR3 1333MHz).
• Ubuntu 16.04.3 LTS Linux 4.4.0-130-generic
• Pin 3.4 Kit 97438
• Perf version 4.4.134
A note on the choice of tools. We perform all the experiments
that require interventions in hardware via simulation in Pin.
Before reaching this decision, we considered two alternatives:
using the gem5 hardware simulator [51], or doing instru-
mentation via LLVM [27]. The latter we quickly ruled out,
because that approach would only let us analyze programs
whose source code is available – libraries would remain terra
incognita. Choosing Pin over gem5 was a harder decision: our
initial implementation was based on the latter. However, its
overhead is impractical for the experiments that we report. The
most recent study that we know announced 50-500 KIPS [52].
Our experiments with SPEC CPU2006 alone already give us
48 Terabytes of instructions. Furthermore, we believe that a
prototype implemented in a cycle-accurate simulator would
present distorted results due to optimizations in hardware that
are not disclosed by manufacturers or could not be imple-
mented in the simulators. For example, through preliminary
experiments, we know that the hit rate of RAS is much higher
than the hit rate of a conventional stack. Possibly, this is due to
the adoption of optimization techniques like speculative RAS
management [53]. This same difference occurs in the simula-
tion of the memory system, as demonstrated in a comparison
of hardware simulators [54].
A. RQ1 – Gadget Reduction: a false negatives assessment
In the context of this work, false negatives happen whenever
we fail to detect a ROP-based attack. To asses the likelihood
of false negatives, we follow Carlini et al.’s [23] methodology
–they adopt the number of available gadgets as a metric of
security. A good ROP defense mechanism leaves few gadgets
still accessible to an attacker. In our case, attackers can build
successful ROP exploits (false negatives) using gadgets that
have any of the following properties:
1) the gadget does not trigger a branch misprediction;
2) the gadget is preceded by an indirect call instruction
that is at the top of LBR;
3) the gadget is preceded by a direct call instruction that
targets an executable memory region;
4) the gadget passes the false positive checks.
Finding gadgets with property (1) is impracticable because
branch predictors record data about instructions already ex-
ecuted by the program. Thus,
the ﬁrst subverted branch
instruction causes a misprediction, unless the attacker can
reuse a program ﬂow already executed. The impossibility of
ﬁnding gadgets with property (1) is corroborated by previous
studies [55]–[57]. It is also highly improbable to abuse RAS
(property 1) or LBR (property 2) because for each gadget
it would be necessary to force the execution of a call
instruction positioned immediately before the return address.
Even more: this call operation must be the most recent
instruction of this type to be executed. This would only be
possible in a scenario where useful gadgets for the attack,
terminated with an indirect call, are sequentially placed in
an executable address space so that an attacker can merge them
with gadgets ending in returns. This event is equivalent to
accidentally having attack code ready within the application
itself. Finally, notice that our multi-layer approach does not
introduce new gadgets in the process of eliminating those
320
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:59 UTC from IEEE Xplore.  Restrictions apply. 
Application
PHP 6.0
Free CD to MP3 Converter 3.1
Internet Explorer 8.0.6001.18702
Mozilla Firefox 3.6.16
Mozilla Firefox Quantum 62.0.3
Google Chrome 69.0.3497.100
Internet Explorer 11.64.16299.0
2.77% 0.04% 0.03
#Gadgets
269,318
346,415
624,638
2.64% 0.14% 2.45
1,444,959 2.61% 0.10% 0.87
1,779,989 2.22% 0.05% 1.31
3,098,708 2.08% 0.02% 0.39
3,332,556 1.05% 0.04% 0.79
CPG VCPG BS(MB) AS(KB)
2.63% 0.09% 1.08
0.01
0.99
3.59
0.88
0.73
0.09
0.29
Figure 5. Gadget Reduction on four applications vulnerable to known
ROP exploits (top) and on the three most popular desktop browsers in
November 2018 (bottom) [49]. CPG: call-preceded gadgets (as a percentage
of #Gadgets). VCPG: valid call-preceded gadgets. Averages are CPG = 1.94%
and VCPG = 0.06%. BS binary size, in MBs, and AS apparent size, in KBs,
after gadget reduction.
already known –a shortcoming present
shrink source code, as pointed by Brown and Pande [58].
in techniques that
In principle, a user can combine ROP with an attack similar
to Spectre against RAS to change the execution ﬂow of
a process from another user. However, even if a malicious
user succeeds in polluting RAS with fake return addresses,
the context switch between the malicious program and the
vulnerable process would need to happen exactly between the
original call and the vulnerable return. The inability of
an attacker to interfere in those switches yields this scenario
improbable. Finally,
item (4) ﬁlters the cases enumerated
in Section III-C, which otherwise would be misclassiﬁed as
attacks. Using gadgets with those characteristics is virtually
impossible, as they involve Operating System code. Thus, we
shall count how many gadgets meet property (3) by chance
(i.e. direct calls whose random target is executable).
Measurement methodology. To count gadgets we follow a
methodology proposed by Bowne [59]. Firstly, we pause the
program of interest at a particular execution point (right before
hijacking the control ﬂow, for instance) with help of Immunity
Debugger. Secondly, we use our own crafted version of a Mona
plug-in to scan the whole process memory and list all available
call instructions. This list lets us identify which gadgets are
preceded by a call operation, even when this instruction is