title:Automated duplicate detection for bug tracking systems
author:Nicholas Jalbert and
Westley Weimer
Automated Duplicate Detection for Bug Tracking Systems
Nicholas Jalbert
University of Virginia
Charlottesville, Virginia 22904
PI:EMAIL
Westley Weimer
University of Virginia
Charlottesville, Virginia 22904
PI:EMAIL
Abstract
Bug tracking systems are important tools that guide the
maintenance activities of software developers. The utility of
these systems is hampered by an excessive number of dupli-
cate bug reports–in some projects as many as a quarter of
all reports are duplicates. Developers must manually iden-
tify duplicate bug reports, but this identiﬁcation process is
time-consuming and exacerbates the already high cost of
software maintenance. We propose a system that automati-
cally classiﬁes duplicate bug reports as they arrive to save
developer time. This system uses surface features, textual
semantics, and graph clustering to predict duplicate sta-
tus. Using a dataset of 29,000 bug reports from the Mozilla
project, we perform experiments that include a simulation
of a real-time bug reporting environment. Our system is
able to reduce development cost by ﬁltering out 8% of du-
plicate bug reports while allowing at least one report for
each real defect to reach developers.
1. Introduction
As software projects become increasingly large and
complex, it becomes more difﬁcult to properly verify code
before shipping. Maintenance activities [12] account for
over two thirds of the life cycle cost of a software sys-
tem [3], summing up to a total of $70 billion per year in the
United States [16]. Software maintenance is critical to soft-
ware dependability, and defect reporting is critical to mod-
ern software maintenance.
Many software projects rely on bug reports to direct cor-
rective maintenance activity [1]. In open source software
projects, bug reports are often submitted by software users
or developers and collected in a database by one of sev-
eral bug tracking tools. Allowing users to report and poten-
tially help ﬁx bugs is assumed to improve software quality
overall [13]. Bug tracking systems allow users to report,
describe, track, classify and comment on bug reports and
feature requests. Bugzilla is a particularly popular open
source bug tracking software system [14] that is used by
large projects such as Mozilla and Eclipse. Bugzilla bug re-
ports come with a number of pre-deﬁned ﬁelds, including
categorical information such as the relevant product, ver-
sion, operating system and self-reported incident severity,
as well as free-form text ﬁelds such as defect title and de-
scription. In addition, users and developers can leave com-
ments and submit attachments, such as patches or screen-
shots.
The number of defect reports typically exceeds the re-
sources available to address them. Mature software projects
are forced to ship with both known and unknown bugs; they
lack the development resources to deal with every defect.
For example, in 2005, one Mozilla developer claimed that,
“everyday, almost 300 bugs appear that need triaging. This
is far too much for only the Mozilla programmers to han-
dle” [2, p. 363].
A signiﬁcant fraction of submitted bug reports are spuri-
ous duplicates that describe already-reported defects. Pre-
vious studies report that as many as 36% of bug reports
were duplicates or otherwise invalid [2]. Of the 29,000 bug
reports used in the experiments in this paper, 25.9% were
identiﬁed as duplicates by the project developers.
Developer time and effort are consumed by the triage
work required to evaluate bug reports [14], and the time
spent ﬁxing bugs has been reported as a useful software
quality metric [8]. Modern software engineering for large
projects includes bug report triage and duplicate identiﬁca-
tion as a major component.
We propose a technique to reduce bug report triage cost
by detecting duplicate bug reports as they are reported. We
build a classiﬁer for incoming bug reports that combines
the surface features of the report [6], textual similarity met-
rics [15], and graph clustering algorithms [10] to identify
duplicates. We attempt to predict whether manual triage
efforts would eventually resolve the defect report as a du-
plicate or not. This prediction can serve as a ﬁlter between
developers and arriving defect reports: a report predicted
to be a duplicate is ﬁled, for future reference, with the bug
reports it is likely to be a duplicate of, but is not otherwise
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE52DSN 2008: Jalbert & Weimerpresented to developers. As a result, no direct triage effort is
spent on it. Our classiﬁer is based on a model that takes into
account easily-gathered surface features of a report as well
as historical context information about previous reports.
In our experiments we apply our technique to over
29,000 bug reports from the Mozilla project and experi-
mentally validate its predictive power. We measure our ap-
proach’s efﬁcacy as a ﬁlter, its ability to locate the likely
original for a duplicate bug report, and the relative power of
the key features it uses to make decisions.
The Mozilla project already has over 407,000 existing
reports, so na¨ıve approaches that explicitly compare each
incoming bug report to all previous ones will not scale. We
train our model on historical information in long (e.g., four-
month) batches, periodically regenerating it to ensure it re-
mains accurate.
The main contributions of this paper are:
• A classiﬁer that predicts bug report duplicate status
based on an underlying model of surface features and
textual similarity. This classiﬁer has reasonable pre-
dictive power over our dataset, correctly identifying
8% of the duplicate reports while allowing at least one
report for each real defect to reach developers.
• A discussion of the relative predictive power of the fea-
tures in the model and an explanation of why certain
measures of word frequency are not helpful in this do-
main.
The structure of this paper is as follows. Section 2
presents a motivating example that traces the history of sev-
eral duplicate bug reports. We compare our approach to oth-
ers in Section 3. In Section 4, we formalize our model, pay-
ing careful attention to textual semantics in Section 4.1 and
surface features in Section 4.2. We present our experimen-
tal framework and our experimental results in Section 5. We
conclude in Section 6.
2. Motivating Example
Duplicate bug reports are such a problem in practice that
many projects have special guidelines and websites devoted
to them. The “Most Frequently Reported Bugs” page of the
Mozilla Project’s Bugzilla bug tracking system is one such
example. This webpage tracks the number of bug reports
with known duplicates and displays the most commonly re-
ported bugs. Ten bug equivalence classes have over 100
known duplicates and over 900 other equivalence classes
have more than 10 known duplicates each. All of these du-
plicates had to be identiﬁed by hand and represent time de-
velopers spent administering the bug report database and
performing triage rather than actually addressing defects.
Bug report #340535 is indicative of the problems in-
volved; we will consider it and three of its duplicates.
The body of bug report #340535, submitted on June 6,
2006, includes the text, “when I click OK the updater starts
again and tries to do the same thing again and again.
It
never stops. So I have to kill the task.” It was reported with
severity “normal” on Windows XP and included a logﬁle.
Bug report #344134 was submitted on July 10, 2006 and
includes the description, “I got a software update of Mine-
ﬁeld, but it failed and I got in an endless loop.” It was
also reported with severity “normal” on Windows XP, but
included no screenshots or logﬁles. On August 29, 2006
the report was identiﬁed as a duplicate of #340535.
Later, on September 17, 2006, bug report #353052 was
submitted: “...[Thunderbird] says that the previous update
failed to complete, try again get the same message cannot
start thunderbird at all, continous loop.” This report had a
severity of “critical” on Windows XP, and included no at-
tachments. Thirteen hours later, it was marked as a dupli-
cate in same equivalence class as #340535.
A fourth report, #372699, was ﬁled on March 5, 2007. It
included the title, “autoupdate runs...and keeps doing this in
an inﬁnite loop until you kill thunderbird.” It had a severity
of “major” on Windows XP and included additional form
text. It was marked as a duplicate within four days.
When these four example reports are presented in suc-
cession their similarities are evident, but in reality they were
separated by up to nine months and over thirty thousand in-
tervening defect reports. All in all, 42 defect reports were
submitted describing this same bug. In commercial devel-
opment processes with a team of bug triagers and software
maintainers, it is not reasonable to expect any single de-
veloper to have thirty thousand defects from the past nine
months memorized for the purposes of rapid triage. De-
velopers must thus be wary, and the cost of checking for
duplicates is paid not merely for actual duplicates, but also
for every non-duplicate submission; developers must treat
every report as a potential duplicate.
However, we can gain insight from this example. While
self-reported severity was not indicative, some defect report
features such as platform were common to all of the dupli-
cates. More tellingly, however, the duplicate reports often
used similar language. For example, each report mentioned
above included some form of the word “update”, and in-
cluded “never stops”, “endless loop”, “continous loop”, or
“inﬁnite loop”, and three had “tries again”, “keeps trying”,
or “keeps doing this”.
We propose a formal model for reasoning about dupli-
cate bug reports. The model identiﬁes equivalence classes
based predominantly on textual similarity, relegating sur-
face features to a supporting role.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE53DSN 2008: Jalbert & Weimer3. Related Work
In previous work we presented a model of defect re-
port quality based only on surface features [6]. That model
predicted whether a bug would be triaged within a given
amount of time. This paper adopts a more semantically-rich
model, including textual information and machine learn-
ing approaches, and is concerned with detecting duplicates
rather than predicting the ﬁnal status of non-duplicate re-
ports. In addition, our previous work suffered from false
positives and would occasionally ﬁlter away all reports for
a given defect. The technique presented here suffers from
no such false positives in practice on a larger dataset.
Anvik et al. present a system that automatically assigns
bug reports to an appropriate human developer using text
categorization and support vector machines. They claim
that their system could aid a human triager by recom-
mending a set of developers for each incoming bug re-
port [2]. Their method correctly suggests appropriate de-
velopers with 64% precision for Firefox, although their
datasets were smaller than ours (e.g., 10,000 Firefox re-
ports) and their learned model did not generalize well to
other projects. They build on previous approaches to au-
tomated bug assignment with lower precision levels [4, 5].
Our approach is orthogonal to theirs and both might be gain-
fully employed together: ﬁrst our technique ﬁlters out po-
tential duplicates, and then the remaining real bug reports
are assigned to developers using their technique. Anvik et
al. [1] also report preliminary results for duplicate detection
using a combination of cosine similarity and top lists; their
method requires human intervention and incorrectly ﬁltered
out 10% of non-duplicate bugs on their dataset.
Weiß et al. predict the “ﬁxing effort” or person-hours
spent addressing a defect [17]. They leverage existing bug
databases and historical context information. To make their
prediction, they use pre-recorded development cost data
from the existing bug report databases. Both of our ap-
proaches use textual similarity to ﬁnd closely related defect
reports. Their technique employs the k-nearest neighbor
machine learning algorithm. Their experimental validation
involved 600 defect reports for the JBoss project. Our ap-
proach is orthogonal to theirs, and a project might employ
our technique to weed out spurious duplicates and then em-
ploy their technique on the remaining real defect reports to
prioritize based on predicted effort.
Kim and Whitehead claim that the time it takes to ﬁx a