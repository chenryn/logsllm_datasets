# Automated Duplicate Detection for Bug Tracking Systems

## Authors
- **Nicholas Jalbert**
  - University of Virginia
  - Charlottesville, Virginia 22904
  - Email: [EMAIL]
- **Westley Weimer**
  - University of Virginia
  - Charlottesville, Virginia 22904
  - Email: [EMAIL]

## Abstract
Bug tracking systems are essential tools that guide the maintenance activities of software developers. However, their utility is often compromised by an excessive number of duplicate bug reports, which can account for up to a quarter of all reports in some projects. Manually identifying these duplicates is time-consuming and adds to the already high cost of software maintenance. We propose a system that automatically classifies duplicate bug reports as they arrive, thereby saving developer time. This system leverages surface features, textual semantics, and graph clustering to predict duplicate status. Using a dataset of 29,000 bug reports from the Mozilla project, we conducted experiments, including a simulation of a real-time bug reporting environment. Our system reduces development costs by filtering out 8% of duplicate bug reports while ensuring that at least one report for each real defect reaches the developers.

## 1. Introduction
As software projects grow in size and complexity, it becomes increasingly difficult to verify code before release. Maintenance activities, which account for over two-thirds of the life cycle cost of a software system, amount to approximately $70 billion annually in the United States. Software maintenance is crucial for dependability, and defect reporting is a key component of modern software maintenance.

Many software projects, especially open-source ones, rely on bug reports to direct corrective maintenance. These reports are typically submitted by users or developers and collected in a database using bug tracking tools like Bugzilla. Allowing users to report and potentially help fix bugs is believed to improve overall software quality. Bug tracking systems enable users to report, describe, track, classify, and comment on bug reports and feature requests. Bugzilla, a popular open-source bug tracking system, is used by large projects such as Mozilla and Eclipse. Bug reports in Bugzilla include predefined fields (e.g., product, version, operating system, severity) and free-form text fields (e.g., defect title and description). Users and developers can also leave comments and submit attachments like patches or screenshots.

The number of defect reports often exceeds the available resources to address them. Mature software projects must ship with both known and unknown bugs due to limited development resources. For example, in 2005, a Mozilla developer noted that nearly 300 bugs requiring triage appeared daily, far exceeding the capacity of the Mozilla programmers.

A significant fraction of submitted bug reports are spurious duplicates. Previous studies have reported that up to 36% of bug reports were duplicates or otherwise invalid. In our dataset of 29,000 bug reports, 25.9% were identified as duplicates by the project developers. The triage work required to evaluate these reports consumes valuable developer time, which could be better spent fixing actual defects.

We propose a technique to reduce the cost of bug report triage by detecting duplicate bug reports as they are reported. Our classifier combines surface features, textual similarity metrics, and graph clustering algorithms to identify duplicates. This prediction serves as a filter between developers and incoming defect reports: a report predicted to be a duplicate is filed with the likely original report but not presented to developers, thus saving triage effort. Our classifier is based on a model that considers easily-gathered surface features and historical context information about previous reports.

In our experiments, we applied our technique to over 29,000 bug reports from the Mozilla project and validated its predictive power. We measured the efficacy of our approach as a filter, its ability to locate the likely original for a duplicate bug report, and the relative importance of the key features it uses to make decisions.

Given that the Mozilla project already has over 407,000 existing reports, naive approaches that compare each incoming report to all previous ones are impractical. Therefore, we train our model on historical information in long (e.g., four-month) batches, periodically regenerating it to ensure accuracy.

The main contributions of this paper are:
- A classifier that predicts bug report duplicate status based on surface features and textual similarity, correctly identifying 8% of duplicate reports while allowing at least one report for each real defect to reach developers.
- A discussion of the relative predictive power of the features in the model and an explanation of why certain measures of word frequency are not helpful in this domain.

## 2. Motivating Example
Duplicate bug reports are a significant problem, leading many projects to develop special guidelines and websites to manage them. The "Most Frequently Reported Bugs" page on the Mozilla Project's Bugzilla system is one such example. This page tracks the number of bug reports with known duplicates and displays the most commonly reported bugs. Ten bug equivalence classes have over 100 known duplicates, and over 900 other classes have more than 10 known duplicates each. Identifying these duplicates manually represents a substantial time investment for developers, who could otherwise be addressing defects.

Consider bug report #340535, submitted on June 6, 2006, which describes an issue where the updater starts repeatedly and never stops. It was reported with "normal" severity on Windows XP and included a log file. Three subsequent reports, #344134, #353052, and #372699, were submitted over the next nine months, all describing similar issues and eventually marked as duplicates of #340535. These reports shared common features, such as the platform (Windows XP) and similar language (e.g., "endless loop," "continous loop").

This example highlights the challenge of identifying duplicates, as the reports were separated by up to nine months and over thirty thousand intervening defect reports. It is unreasonable to expect developers to memorize all past defects, making a systematic approach to duplicate detection essential.

## 3. Related Work
In previous work, we presented a model of defect report quality based on surface features, predicting whether a bug would be triaged within a given time. This paper adopts a more semantically-rich model, including textual information and machine learning, to detect duplicates. Our previous model suffered from false positives, occasionally filtering away all reports for a given defect. The technique presented here avoids such false positives on a larger dataset.

Anvik et al. developed a system that automatically assigns bug reports to appropriate human developers using text categorization and support vector machines. Their method suggests appropriate developers with 64% precision for Firefox, although their datasets were smaller and their model did not generalize well to other projects. Our approach is orthogonal to theirs and can be used together: first, our technique filters out potential duplicates, and then the remaining reports are assigned to developers using their method. Anvik et al. also reported preliminary results for duplicate detection using cosine similarity and top lists, but their method required human intervention and incorrectly filtered out 10% of non-duplicate bugs.

Wei√ü et al. predict the "fixing effort" or person-hours spent addressing a defect, using pre-recorded development cost data from existing bug report databases. Their technique employs the k-nearest neighbor algorithm and was validated on 600 JBoss project defect reports. Our approach is orthogonal to theirs, and a project might use our technique to filter out duplicates and then prioritize the remaining real defect reports based on predicted effort.

Kim and Whitehead claim that the time it takes to fix a bug is a useful software quality metric. Their work complements ours, as reducing the number of duplicate reports can lead to more efficient use of developer time.

## 4. Model and Methodology
### 4.1 Textual Semantics
### 4.2 Surface Features
### 4.3 Graph Clustering

## 5. Experimental Framework and Results

## 6. Conclusion

---

This optimized version of the text is more structured, coherent, and professional, with clear section headings and a logical flow of ideas.