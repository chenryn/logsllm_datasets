t
u
U
P
C
80
60
40
20
0
0
Libnids
yaf
Scap w/o FDIR
Scap with FDIR
)
%
(
s
t
p
u
r
r
e
n
t
i
e
r
a
w
t
f
o
S
Libnids
yaf
Scap w/o FDIR
Scap with FDIR
30
25
20
15
10
5
0
1
2
3
4
5
6
0
1
2
3
4
5
6
Traffic rate (Gbit/s)
(b) CPU utilization
Traffic rate (Gbit/s)
(c) Software interrupt load
Figure 3: Performance comparison of ﬂow-based statistics export for YAF, Libnids, and Scap, for varying trafﬁc rates.
stream_t records. As applications have different requirements,
Scap tries to combine and generalize all requirements at kernel
level, and apply application-speciﬁc conﬁgurations at user level.
5.7 Packet Delivery
An application may be interested in receiving both reassembled
streams, as well as their individual packets, e.g., to detect TCP-level
attacks [41]. Scap supports the delivery of the original packets as
captured from the network, if an application indicates that it needs
them. Then, Scap internally uses another memory-mapped buffer
that contains records for each packet of a stream. Each record con-
tains a packet header with the timestamp and capture length, and a
pointer to the original packet payload in the stream.
5.8 API Stub
The Scap API stub uses setsockopt() to pass parameters to
the kernel module for handling API calls. When scap_start_
capture() is called, each worker thread runs an event-dispatch
loop that polls its corresponding event queue, reads the next avail-
able event, and executes the registered callback function for the
event (if any). The event queues contain stream_t objects, which
have an event ﬁeld and a pointer to the next stream_t object
in the event queue. If this pointer is NULL, then there is no event
in the queue, and the stub calls poll() to wait for future events.
6. EXPERIMENTAL EVALUATION
We experimentally evaluate the performance of Scap, compar-
ing it to other stream reassembly libraries, for common monitoring
tasks, such as ﬂow statistics export and pattern matching, while re-
playing a trace of real network trafﬁc at different rates.
6.1 Experimental Environment
The hardware.
We use a testbed comprising two PCs interconnected through
a 10 GbE switch. The ﬁrst, equipped with two dual-core Intel
Xeon 2.66 GHz CPUs with 4MB L2 cache, 4GB RAM, and an
Intel 82599EB 10GbE NIC, is used for trafﬁc generation. The sec-
ond, used as a monitoring sensor, is equipped with two quad-core
Intel Xeon 2.00 GHz CPUs with 6MB L2 cache, 4GB RAM, and
an Intel 82599EB 10GbE NIC used for stream capture. Both PCs
run 64-bit Ubuntu Linux (kernel version 2.6.32).
The trace.
To evaluate stream reassembly implementations with real trafﬁc,
we replay a one-hour long full-payload trace captured at the ac-
cess link that connects to the Internet a University campus with
thousands of hosts. The trace contains 58,714,906 packets and
1,493,032 ﬂows, totaling more than 46GB, 95.4% of which is TCP
trafﬁc. To achieve high replay rates (up to 6 Gbit/s) we split the
trace in smaller parts of 1GB that ﬁt into main memory, and replay
each part 10 times while the next part is being loaded in memory.
The parameters.
We compare the following systems: (i) Scap, (ii) Libnids v1.24 [2],
(iii) YAF v2.1.1 [20], a Libpcap-based ﬂow export tool, and (iv)
the Stream5 preprocessor of Snort v2.8.3.2 [40]. YAF, Libnids and
Snort rely on Libpcap [29], which uses the PF_PACKET socket for
packet capture on Linux. Similarly to Scap’s kernel module, the
PF_PACKET kernel module runs as a software interrupt handler
that stores incoming packets to a memory-mapped buffer, shared
with Libpcap’s user-level stub. In our experiments, the size of this
buffer is set to 512MB, and the buffer size for reassembled streams
is set to 1GB for Scap, Libnids, and Snort. We use a chunk size of
16KB, the SCAP_TCP_FAST reassembly mode, and an inactivity
timeout of 10 seconds. The majority of TCP streams terminate ex-
plicitly with TCP FIN or RST packet, but we also use an inactivity
timeout to expire UDP and TCP ﬂows that do not close normally.
As we replay the trace at much higher rates than its actual capture
rate, an inactivity timeout of 10 seconds is a reasonable choice.
6.2 Flow-Based Statistics Export: Drop Any-
thing Not Needed
In our ﬁrst experiment we evaluate the performance of Scap for
exporting ﬂow statistics, comparing with YAF and with a Libnids-
based program that receives reassembled ﬂows. By setting the
stream cutoff value to zero, Scap discards all stream data after up-
dating stream statistics. When Scap is conﬁgured to use the FDIR
ﬁlters, the NIC discards all packets of a ﬂow after TCP connection
establishment, except from the TCP FIN/RST packets, which are
used by Scap for ﬂow termination. Although Scap can use all eight
available cores, for a fair comparison, we conﬁgure it to use a single
worker thread, as YAF and Libnids are single-threaded. However,
for all tools, interrupt handling for packet processing in the kernel
takes advantage of all cores, utilizing NIC’s multiple queues.
Figures 3(a), 3(b), and 3(c) present the percentage of dropped
packets, the average CPU utilization of the monitoring application
on a single core, and the software interrupt load while varying the
trafﬁc rate from 250 Mbit/s to 6 Gbit/s. We see that Libnids starts
losing packets when the trafﬁc rate exceeds 2 Gbit/s. The reason
can be seen in Figures 3(b) and 3(c), where the total CPU utilization
of Libnids exceeds 90% at 2.5 Gbit/s. YAF performs slightly better
than Libnids, but when the trafﬁc reaches 4 Gbit/s, it also drives
CPU utilization to 100% and starts losing packets as well. This
is because both YAF and Libnids receive all packets in user space
and then drop them, as the packets themselves are not needed.
)
%
(
d
e
p
p
o
r
d
s
t
e
k
c
a
P
100
80
60
40
20
0
0
Libnids
Snort
Scap
1
2
3
4
5
6
Traffic rate (Gbit/s)
(a) Packet loss
100
)
%
(
n
o
i
t
a
z
i
l
i
t
u
U
P
C
80
60
40
20
0
0
Libnids
Snort
Scap
)
%
(
s
t
p
u
r
r
e
n
t
i
e
r
a
w
t
f
o
S
Libnids
Snort
Scap
40
35
30
25
20
15
10
5
0
1
2
3
4
5
6
0
1
2
3
4
5
6
Traffic rate (Gbit/s)
(b) CPU utilization
Traffic rate (Gbit/s)
(c) Software interrupt load
Figure 4: Performance comparison of stream delivery for Snort, Libnids, and Scap, for varying trafﬁc rates.
Libnids
Snort
Scap
100
)
%
(
t
s
o
l
s
m
a
e
r
t
S
80
60
40
20
70
60
50
40
30
20
10
)
%
(
n
o
i
t
a
z
i
l
i
t
u
U
P
C
Libnids
Snort
Scap
0
    1
10   
    2
10   
    3
10   
    4
10   
    5
10   
    6
10   
    7
10   
0
    1
10   
    2
10   
    3
10   
    4
10   
    5
10   
    6
10   
    7
10   
Number of concurrent streams
(a) Lost streams
Number of concurrent streams
(b) CPU utilization
)
%
(
s
t
p
u
r
r
e
t
n
i
e
r
a
w
t
f
o
S
10
8
6
4
2
Libnids
Snort
Scap
0
    1
10   
    2
10   
    3
10   
    4
10   
    5
10   
    6
10   
    7
10   
Number of concurrent streams
(c) Software interrupt load
Figure 5: Performance comparison of Snort, Libnids, and Scap, for a varying number of concurrent streams.
Scap processes all packets even at 6 Gbit/s load. As shown in
Figure 3(b), the CPU utilization of the Scap application is always
less than 10%, as it practically does not do any work at all. All the
work has already been done by Scap’s kernel module. One would
expect the overhead of this module (shown in Figure 3(c)) to be
relatively high. Fortunately, however, the software interrupt load
of Scap is even lower compared to YAF and Libnids, even when