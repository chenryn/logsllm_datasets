### 图3：YAF、Libnids和Scap在不同流量速率下的基于流的统计导出性能比较

- **(a) 丢包率**  
  从图中可以看出，当流量速率超过2 Gbit/s时，Libnids开始丢失数据包。这是因为在2.5 Gbit/s时，其CPU利用率已超过90%（见图3(b)和3(c)）。YAF的表现略优于Libnids，但在流量达到4 Gbit/s时，YAF的CPU利用率也达到了100%，并开始丢失数据包。这是因为YAF和Libnids都需要将所有数据包传送到用户空间，然后再丢弃它们。

- **(b) CPU利用率**  
  在单核上的平均CPU利用率方面，Scap始终保持在10%以下，因为它几乎不需要做任何工作，所有的处理任务都由内核模块完成。相比之下，YAF和Libnids在高流量速率下表现出较高的CPU使用率。

- **(c) 软中断负载**  
  Scap的软中断负载甚至低于YAF和Libnids，即使在6 Gbit/s的高流量速率下也是如此。这表明Scap的内核模块设计得非常高效。

### 5.7 数据包传递
某些应用可能需要接收重组后的流以及单独的数据包，例如为了检测TCP级别的攻击。Scap支持向应用程序传递从网络捕获到的原始数据包，前提是该应用明确表示需要这些数据。此时，Scap会在内部使用另一个内存映射缓冲区来存储每个流中的数据包记录。每条记录包含一个带有时间戳和捕获长度的数据包头部，以及指向流中原始数据包载荷的指针。

### 5.8 API存根
Scap API存根通过`setsockopt()`函数将参数传递给内核模块以处理API调用。当调用`scap_start_capture()`时，每个工作线程会运行一个事件分发循环，轮询对应的事件队列，读取下一个可用事件，并执行为该事件注册的回调函数（如果有的话）。事件队列中包含`stream_t`对象，这些对象有一个事件字段和指向队列中下一个`stream_t`对象的指针。如果该指针为NULL，则表示队列中没有事件，此时存根将调用`poll()`等待未来的事件。

### 6. 实验评估
我们通过实验对Scap的性能进行了评估，将其与其它流重组库在常见的监控任务（如流统计导出和模式匹配）上进行了对比，同时回放了不同速率的真实网络流量跟踪记录。

#### 6.1 实验环境
- **硬件配置**  
  我们的测试平台包括两台通过10 GbE交换机互连的PC。第一台PC用于流量生成，配备两个双核Intel Xeon 2.66 GHz CPU、4MB L2缓存、4GB RAM和一个Intel 82599EB 10GbE网卡。第二台PC作为监控传感器，配备了两个四核Intel Xeon 2.00 GHz CPU、6MB L2缓存、4GB RAM和一个Intel 82599EB 10GbE网卡。两台PC均运行64位Ubuntu Linux（内核版本2.6.32）。

- **流量跟踪**  
  为了评估流重组实现的真实流量情况，我们回放了一个长达一小时的全有效载荷跟踪记录，该记录是在连接数千台主机的大学校园互联网接入链路上捕获的。该记录包含58,714,906个数据包和1,493,032条流，总大小超过46GB，其中95.4%是TCP流量。为了实现高达6 Gbit/s的高回放速率，我们将跟踪记录分成多个1GB的小部分，每个部分加载到主内存中并重复回放10次，同时加载下一部分。

- **参数设置**  
  我们比较了以下系统：(i) Scap，(ii) Libnids v1.24 [2]，(iii) YAF v2.1.1 [20]（一种基于Libpcap的流导出工具），以及(iv) Snort v2.8.3.2 [40]的Stream5预处理器。YAF、Libnids和Snort都依赖于Libpcap [29]，后者在Linux上使用PF_PACKET套接字进行数据包捕获。类似于Scap的内核模块，PF_PACKET内核模块作为一个软件中断处理程序运行，将传入的数据包存储到一个与Libpcap用户级存根共享的内存映射缓冲区中。在我们的实验中，该缓冲区的大小设置为512MB，而用于重组流的缓冲区大小为1GB（适用于Scap、Libnids和Snort）。我们使用的块大小为16KB，采用SCAP_TCP_FAST重组模式，并设置了10秒的不活动超时时间。大多数TCP流通过显式的TCP FIN或RST数据包终止，但我们也使用不活动超时来结束那些未正常关闭的UDP和TCP流。由于我们回放的速率远高于实际捕获速率，因此10秒的不活动超时是一个合理的选择。

#### 6.2 基于流的统计导出：丢弃不必要的内容
在第一个实验中，我们评估了Scap导出流统计信息的性能，并将其与YAF和基于Libnids的程序进行了比较。通过将流截止值设置为零，Scap在更新流统计信息后丢弃所有流数据。当Scap配置为使用FDIR过滤器时，NIC会在TCP连接建立后丢弃流的所有数据包，除了用于流终止的TCP FIN/RST数据包。尽管Scap可以使用所有八个可用的核心，但为了公平比较，我们将其配置为只使用一个工作线程，因为YAF和Libnids都是单线程的。然而，对于所有工具来说，内核中的数据包处理中断处理都可以利用所有核心，充分利用NIC的多个队列。

图3(a)、3(b)和3(c)展示了在流量速率从250 Mbit/s变化到6 Gbit/s时的丢包百分比、单核上的平均CPU利用率以及软中断负载。我们可以看到，当流量速率超过2 Gbit/s时，Libnids开始丢失数据包。原因可以在图3(b)和3(c)中看到，在2.5 Gbit/s时，Libnids的总CPU利用率超过了90%。YAF的表现略好于Libnids，但在流量达到4 Gbit/s时，它的CPU利用率也达到了100%，并开始丢失数据包。这是因为YAF和Libnids都将所有数据包传输到用户空间，然后丢弃它们，因为这些数据包本身并不需要。

Scap在6 Gbit/s的负载下仍能处理所有数据包。如图3(b)所示，Scap应用程序的CPU利用率始终低于10%，因为它实际上不做任何工作，所有的工作都由Scap的内核模块完成。人们可能会认为这个模块的开销（见图3(c)）相对较高，但实际上，即使在6 Gbit/s的高流量速率下，Scap的软中断负载仍然低于YAF和Libnids。