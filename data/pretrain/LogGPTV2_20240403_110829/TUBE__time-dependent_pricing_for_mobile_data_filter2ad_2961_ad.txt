bandwidth for individual applications. These features heav-
ily depend on the openness of the platform.
Table 4 shows the TUBEApp features supported on each
platform before hacking the device, as well as the code size
for implementing the full TUBEApp. In particular, for the
iOS platform, we hook several internal functions to track
the usage per application, run a daemon process to dispatch
and show TDP prices, and block applications. The iOS im-
plementation thus requires 25K lines of code, while the An-
droid and Windows implementations need only 5.4K and
5.3K lines, respectively.
3.2.2 Enhancing the User Experience
The autopilot mode minimizes user interactions by esti-
mating device usage patterns and scheduling applications
for the user. To inform users of the autopilot actions, we
send pop-up notiﬁcations when usage is blocked, as shown in
Fig. 7’s iPhone screenshots. The warning and blocking pop-
ups are displayed when the user’s usage reaches the expected
daily and weekly budgets (Fig. 7a and b, respectively).
To ensure that the autopilot’s implementation is practical,
we measure its energy usage. TUBEApp with autopilot run-
ning consumes only 4% more battery power than the device
without our TUBEApp installed, indicating that autopilot
does not drain too much power.
4. TRIAL DESIGN AND RESULTS
We conducted a small scale pilot trial of the TUBE sys-
tem at Princeton University from May 2011 to January 2012.
This section provides an overview of the trial goals, setup,
and limitations, followed by a discussion of some key con-
clusions drawn from the trial data.
4.1 Goals and Structure
The goals of our trial are to demonstrate the TUBE sys-
tem’s functionality and beneﬁts, and to provide an initial
veriﬁcation of TUBE’s deployment feasibility with real users.
Throughout the trial, we eﬀectively acted as a resale ISP,
paying the participants’ regular 3G data bills to AT&T while
TDP Price Optimizer Pricing Plans    …Manual AutopilotPricing Policy Container Authentication  Module Delegation Mechanism User Device Queries REST API User Behavior Estimation MeasurementUsage Monitoring Plugins Linux  Netfilter3G Standard RADIUS … Pricing Policy Enforcer Plugins Linux   Traffic ControlDPI TDP 3G Standard Linux  NetfilterPush Notifier SMS Push Message Manual AutopilotCommunication Module Graphical User Interface BudgetPrice Dispatcher Session Recorder Usage Tracker Price Display Usage  Display Settings Display Top 5 Apps Display Current Bill Display Popup Display Price  Status bar Display Price DB Daemon Usage Collector Usage DB Budget Helper App Scheduler Budget Manager Delegation Pulled  Local Autopilot Algorithm App PPISchedulerEnforcer Allow/Block Notifier Task Manager 253(a) Price display
(b) Price and usage
(c) Top 5 applications
(d) App delay index
(e) App scheduling
Figure 6: Screenshots of TUBEApp on iPhone. iPhone users can (a) check the prices for next 24 hours, (b)
learn from price and usage history, (c) identify top 5 apps by bandwidth usage, (d) modify each app’s delay
tolerance, and (e) check when their apps are scheduled during the day and manually override the results.
charging them according to our TUBE algorithms. To as-
sess the beneﬁts of TDP, we divide the trial into two phases:
ﬁrst, we simply monitor the usage patterns without TDP
(i.e., collect TIP data). We then oﬀer TDP and study its
impact in the second phase of the trial. The following as-
pects of the trial are addressed in this section:
Baseline Traﬃc Statistics: Section 4.4.1 reports on
three months of TIP usage statistics from our trial partici-
pants. We ask if the sample population has a representative
mix of heavy and light users and bandwidth-hungry appli-
cations, so as to realistically assess the beneﬁts of TDP.
Price Sensitivity: In Section 4.4.2, we examine users’
price sensitivity to static TDP patterns: if we oﬀer low and
high price periods alternately, will users defer their traﬃc
to use more in the lower-price period? This question tests
TDP’s basic premise that users will delay their traﬃc in
exchange for a monetary discount.
Eﬀectiveness of GUI Design: Section 4.4.3 analyzes
the eﬀectiveness of displaying numerical values versus color
codes (red: high, orange: medium, green: low) to indicate
TDP prices on the user device.
Beneﬁts of Optimized TDP: Finally, Section 4.4.4
studies whether TUBE’s optimized prices beneﬁt ISPs in
reducing peak-to-average ratios of network usage.
4.2 Trial Setup
We recruited 50 users (27 iPhones and 23 iPads) of AT&T’s
3G Corporate Data Plan as our trial participants. They were
faculty and staﬀ from 14 academic and administrative divi-
sions. During the trial, we acted as a resale ISP, charging
participants after every billing cycle according to TUBE’s
TDP. We excluded measurements from development devices
to avoid bias.
To record participants’ usage, we separated their 3G traf-
ﬁc from that of other AT&T customers using an Access Point
Name (APN) setup, which tunneled the participant’s 3G
traﬃc from the AT&T core to the TUBE servers in our lab
(Fig. 8). Participants installed and used the TUBEApp on
their iOS devices. WiFi usage, voice calls, and SMS were
not included in the trial traﬃc as they are not 3G data.
4.3 Trial Limitations
We were limited by logistics to recruiting only AT&T data
plan users with iOS devices, out of which 16 were jailbro-
ken (JB) and 34 were non-jailbroken (non-JB) devices. The
(a) Warning
(b) Blocking
Figure 7: Screenshots of auto-pilot in action on the
iPhone. The warning (a) is displayed when the daily
budget is reached, and usage is blocked in (b) when
the user reaches the weekly budget.
Figure 8: TUBE APN Setup. User traﬃc is routed
through the SGSN and GGSN.
non-JB devices gave us less ﬂexibility in experimentation,
and hence ran a TUBEApp with limited features. In par-
ticular, only users with JB devices could see the current
price/discount directly from the home screen’s status bar
(circled in Fig. 7’s screenshots) without manually launching
the TUBEApp, and the autopilot algorithm only runs on JB
devices. On non-JB devices, we used push notiﬁcations to
alert participants during high-price periods.
Since our trial included only 50 participants, even peak
traﬃc from trial users did not congest our lab’s access link.
To demonstrate TDP’s beneﬁts, we simulated congestion
conditions by logically scaling up the traﬃc volume in TUBE-
Opt’s price computation.6
4.4 Results and Evaluation
Following Section 4.1’s outline, we now present the trial
results. In many cases, we use Wilcoxon’s signed rank test
[12] against the null hypothesis that a set of values is sym-
6If users had experienced real congestion due to this scaled
traﬃc, we expect they would have been even more willing
to delay their traﬃc to oﬀ-peak periods.
Internet	
  User's iPhone or iPad AT&T's mobile network DNS NAT Gateway AT&T Firewall TUBE Servers VLR	
  MCS	
  SGSN	
  AuC	
  GGSN	
  HLR	
  3G Core Network GMSC	
  PSTN Data Flow BSS VPN BSC	
  Data  Flow 254metrically distributed with mean zero. In our case, we ap-
ply the test to the diﬀerence between the changes in usage
in high- and low-price periods. A higher probability of a
symmetric distribution then indicates a lack of response to
the price signals, as the expected change in usage is likely
the same for both high and low prices.
4.4.1 Baseline Trafﬁc Statistics
Question: Do our participants include both heavy and
light bandwidth users? Which applications use the most data?
Method: We measured usage for both iPad and iPhone
users from July to September and used tcpdump to record
application-speciﬁc traﬃc.
Results: Our participants are a mix of light- and heavy-
bandwidth users. Video streaming applications accounted
for most of the traﬃc, corroborating the reported trends of
growing demand for mobile video.
Figure 9a shows the CDF of total traﬃc per user for up-
loads and downloads from July to September 2011. While
90% of the users uploaded less than 0.5GB, some users had
large download volumes: 20% of users consumed 2.1 – 5.3
GB over three months.
Figure 9b shows the distribution of total traﬃc by ap-
plication type for the three month period, normalized with
respect to the number of iPhone, JB, and non-JB iPad users.
Not surprisingly, iPads show a higher usage than iPhones for
most application types, and a large part of the mobile traﬃc
for all device types comes from movie streaming.
4.4.2 Price Sensitivity
Question: Do users wait to use mobile data in return for
a monetary discount?
Method: We conducted a three week experiment on iPad
and iPhone trial participants in October 2011, in which we
oﬀered a basic TDP pattern of consecutive high, high, and
low price periods, repeated throughout the day. The high-
price periods oﬀered approximately a 10% discount, while
the low-price periods oﬀered a 40% discount on the baseline
price of $10/GB. If monetary incentives do induce usage
deferrals, we expect that average usage should decrease in
high-price and increase in low-price periods.
To measure users’ response to prices, we sent messages
at ten minute intervals during high-price periods if the user
exceeded 2 MB of usage in the previous ten minutes. We
ﬁrst analyzed the data for each user by calculating the per-
centage change in usage for each one-hour time period when
compared to the mean usage in that same period before
TDP (i.e., under TIP pricing). We then weighted these per-
cent changes by the proportion of TDP usage in that period
to account for diurnal variations. This gives the weighted
average percent change in usage under TDP for high- and
low-price periods.
Results: We found that users did shift their traﬃc from
high- to low-price periods under TDP. For most users, the
average usage decreased in high-price periods relative to the
changes in low-price periods.
Figure 10 shows the weighted average percent change in
usage for iPad users for high- and low-price periods. The
reference line indicates an equal change in both types of
periods. Each dot on the scatter plot represents values for
an individual user, and its size is proportional to the user’s
TDP usage volume. With the given static TDP pattern,
usage increased more in low-price periods relative to high-
price periods for almost all users. Interestingly, about half
of the users decreased their overall usage in both high- and
low-price periods, while the other half increased their usage
in both periods.
We further verify these results by using Wilcoxon’s test on
the diﬀerences between each user’s percent change in high-
and low-price periods. We ﬁnd only a 0.56% probability
that the null hypothesis is valid, indicating that the users’
observed responses are statistically signiﬁcant. A similar
plot may be observed for the iPhone users.
The overall iPhone usage changed by −11.3% in high-price
and −5.7% in low price periods, while overall iPad usage
changed by −10.1% in high-price and 15.7% in low-price
periods. Thus, iPad users generally decreased their usage
in high-price periods and increased it in low-price periods.
The overall decrease in iPhone usage is likely due to limited
user notiﬁcation and display options on non-JB iPhones.
However, the greater usage decrease in high- relative to low-
price periods indicates that iPhone users attempted to use
less in high-price periods.
Next, we examine the eﬀect of the number of notiﬁcation
messages sent to users on their usage in high-price periods.
Multiple consecutive notiﬁcations were sent to a user only if
usage in each preceding 10 minute interval exceeded 2 MB.
We examine the percent change in usage in the ten minute
span before and after each notiﬁcation. Figure 11 shows the
CDF of the percent change in usage due to a ﬁrst, second,
etc. notiﬁcation. About 80–90% of iPad and iPhone users
did not increase their usage after the ﬁrst notiﬁcation, indi-
cating that notiﬁcations can eﬀectively reduce peak usage.
For all subsequent notiﬁcations, about 60–80% of the active
users responded by decreasing their usage.
4.4.3 Effectiveness of User Interface Design
Question: Do users respond more to the numerical values
of TDP prices or to the color of the price indicator bar on
the home screen?
Method: In December 2011, we installed a price indica-
tor bar on the home screen of all JB devices. The indicator
displays the numerical value of the price discounts available
in the current period and changes its color according to these
discounts. It is green for discounts over 30%, orange for 10–
29% discounts, and red for discounts below 10%.
Our experiment had two stages:
in the ﬁrst stage, we
oﬀered discounts of approximately 40% every third period
of the day, starting with a 40% discount at midnight. The
other periods oﬀered discounts of about 10%. After two
weeks of following this pattern, we began the second stage,
repeating the pattern of a 9% discount at midnight, followed
by 28%, 30%, 28%, 9%, and 30% discounts.
We compare three types of periods to assess the eﬀect of
the indicator color and numerical discount: hours deemed
as Type 1 periods oﬀered a 10% discount in the ﬁrst stage
of the experiment and 28% discount in the second stage;
the indicator remained orange despite this increase in the
discount. Type 2 periods oﬀered a 10% (orange) discount in
the ﬁrst stage and 30% (green) discount in the second stage,
while Type 3 periods oﬀered a 10% discount in the ﬁrst
and 9% discount in the second stage of the experiment (the
indicator is orange in both periods). Table 5 summarizes the
combinations of discounts and colors used in the two stages
that characterize each type of period.
We calculated the percent changes in usage for each period
255(a) Total traﬃc for each user.
(b) Total usage by app type.
Figure 9: Usage statistics from July - Sept. 2011.
Figure 10: iPad response to static TDP.
(a) Usage changes (iPad users).
(b) Usage changes (iPhone users).
Figure 11: User response to notiﬁcations sent.
Type
Periods
First Stage
Color
2, 8, 14, 20 Orange
3, 6, . . . , 24 Orange
5, 11, 17, 23 Orange
1
2
3
Second Stage
Color
Disc.
Disc.
28%
10% Orange
30%
10%
Green
10% Orange
9%
Table 5: Period types in the color experiment.
type between the ﬁrst and second stages of the experiment.
To do so, we ﬁrst found the average usage in each period
of the day (i.e., each hour) for the ﬁrst stage of the exper-
iment. We then calculated the percent change in usage of
each period in the second stage of the experiment from the
average usage in the same period during the ﬁrst stage. The
average change in each type of period is then deﬁned as the
weighted average of these percent changes for each period of
the given type. The weights were proportional to the usage
in that period.
Results: We found that users paid more attention to in-
dicator color than to the numerical discount value. When
discounts increased signiﬁcantly with no change in indicator
color, only half of the users increased their usage relative
to other periods. However, when the indicator color also
changed, almost all users increased their usage in those peri-
ods relative to others. In Fig. 12, each data point represents
one user’s average change in each period type, with the size
of the data point indicating the volume of usage in the sec-
ond stage of the experiment. The reference line represents
equal changes in both period types considered.