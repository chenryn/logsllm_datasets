            }
##### rmqueue
    static inline
    struct page *rmqueue(struct zone *preferred_zone,
                struct zone *zone, unsigned int order,
                gfp_t gfp_flags, unsigned int alloc_flags,
                int migratetype)
    {
        unsigned long flags;
        struct page *page;
        //如果分配单页, 则进入rmqueue_pcplist
        if (likely(order == 0)) {
            page = rmqueue_pcplist(preferred_zone, zone, gfp_flags,
                        migratetype, alloc_flags);
            goto out;
        }
        // 不能使用__GFP_NOFAIL, 分配order>1的页
        WARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));
        // 使用自旋锁加锁zone资源
        spin_lock_irqsave(&zone->lock, flags);
        do {
            page = NULL;
            // ALLOC_HARDER表示高优先级分配, 进入__rmqueue_smallest
            if (alloc_flags & ALLOC_HARDER) {
                page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
                if (page)
                    // 用于debug的插桩设计
                    trace_mm_page_alloc_zone_locked(page, order, migratetype);
            }
            // 不满足上诉条件或page未分配成功, 进入__rmqueue
            if (!page)
                page = __rmqueue(zone, order, migratetype, alloc_flags);
        } while (page && check_new_pages(page, order));
            // check_new_pages遍历page_block中的struct page, 检查page成员, 如果出错则打印错误原因
        spin_unlock(&zone->lock);
        if (!page)
            goto failed;
        // page_block被分配后更新zone成员信息 
        __mod_zone_freepage_state(zone, -(1 flags)) {
            clear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);
            wakeup_kswapd(zone, 0, 0, zone_idx(zone));
        }
        // 编译阶段的变量类型检查
        VM_BUG_ON_PAGE(page && bad_range(zone, page), page);
        return page;
    failed:
        local_irq_restore(flags);
        return NULL;
    }
###### rmqueue_pcplis
    static struct page *rmqueue_pcplist(struct zone *preferred_zone,
                struct zone *zone, gfp_t gfp_flags,
                int migratetype, unsigned int alloc_flags)
    {
        struct per_cpu_pages *pcp;
        struct list_head *list;
        struct page *page;
        unsigned long flags;
        // 禁用全部中断, 并将当前中断状态保存至flags
        local_irq_save(flags);
        // 获得当前cpu的pcp结构体(热页)
        pcp = &this_cpu_ptr(zone->pageset)->pcp;
        // 根据迁移类型选择热页链表
        list = &pcp->lists[migratetype];
        // 在list中分配内存页
        page = __rmqueue_pcplist(zone,  migratetype, alloc_flags, pcp, list);
        if (page) {
            __count_zid_vm_events(PGALLOC, page_zonenum(page), 1);
            // Update NUMA hit/miss statistics
            zone_statistics(preferred_zone, zone);
        }
        // 恢复中断状态并开中断
        local_irq_restore(flags);
        return page;
    }
>   * __rmqueue_pcplist
>
    static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
                unsigned int alloc_flags,
                struct per_cpu_pages *pcp,
                struct list_head *list)
    {
        struct page *page;
        do {
            // 如果列表为空, 则使用rmqueue_bulk装载内存页进入列表
            if (list_empty(list)) {
                pcp->count += rmqueue_bulk(zone, 0,
                        pcp->batch, list,
                        migratetype, alloc_flags);
                if (unlikely(list_empty(list)))
                    return NULL;
            }
            // 获得lru列表首部页结点
            page = list_first_entry(list, struct page, lru);
            // 将页结点从page->lru列表删除
            list_del(&page->lru);
            // 空闲page计数器-1
            pcp->count--;
            // 对page做安全检查
        } while (check_new_pcp(page));
        return page;
    }
> >   * rmqueue_bulk
>>
    static int rmqueue_bulk(struct zone *zone, unsigned int order,
                unsigned long count, struct list_head *list,
                int migratetype, unsigned int alloc_flags)
    {
        int i, alloced = 0;
        // 对zone资源加锁
        spin_lock(&zone->lock);
        for (i = 0; i lru
            list_add_tail(&page->lru, list);
            alloced++;
            // 如果page位于cma中, 则更新NR_FREE_CMA_PAGES
            if (is_migrate_cma(get_pcppage_migratetype(page)))
                __mod_zone_page_state(zone, NR_FREE_CMA_PAGES,
                              -(1 lock);
        return alloced;
    }
###### __rmqueue_smallest
    static __always_inline
    struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
                            int migratetype)
    {
        unsigned int current_order;
        struct free_area *area;
        struct page *page;
        // 从指定order到MAX_ORDER遍历zone->free_area[]
        for (current_order = order; current_order free_area[current_order]);
            // 从zone->free_area[][migratetype]->lru链表头部获得page()
            page = get_page_from_free_area(area, migratetype);
            if (!page)
                continue;
            // 从zone->free_area[][migratetype]->lru中删除page, 更新zone成员
            del_page_from_free_area(page, area);
            // 将current_order阶的page_block拆成小块,并将小块放到对应的阶的链表中去
            expand(zone, page, order, current_order, area, migratetype);
            // 设置page迁移类型
            set_pcppage_migratetype(page, migratetype);
            return page;
        }
        return NULL;
    }
###### __rmqueue
    static __always_inline struct page *
    __rmqueue(struct zone *zone, unsigned int order, int migratetype,
                            unsigned int alloc_flags)
    {
        struct page *page;
    retry:
        // 使用__rmqueue_smallest获得page
        page = __rmqueue_smallest(zone, order, migratetype);
        if (unlikely(!page)) {
            // page分配失败后, 如果迁移类型是MIGRATE_MOVABLE, 进入__rmqueue_cma_fallback
            if (migratetype == MIGRATE_MOVABLE)
                page = __rmqueue_cma_fallback(zone, order);
            // page分配再次失败后使用判断是否可以使用备用迁移类型(如果可以则修改order, migratetype)然后跳转进入retry
            if (!page && __rmqueue_fallback(zone, order, migratetype,
                                    alloc_flags))
                goto retry;
        }
        trace_mm_page_alloc_zone_locked(page, order, migratetype);
        return page;
    }
>   * __rmqueue_fallback
>
  1. 备用迁移类型
    static int fallbacks[MIGRATE_TYPES][4] = {
        [MIGRATE_UNMOVABLE]   = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE,   MIGRATE_TYPES },
        [MIGRATE_MOVABLE]     = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE, MIGRATE_TYPES },
        [MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE,   MIGRATE_MOVABLE,   MIGRATE_TYPES },
    #ifdef CONFIG_CMA
    #endif
    #ifdef CONFIG_MEMORY_ISOLATION
    #endif
    };
  1. __rmqueue_fallback
    static __always_inline bool
    __rmqueue_fallback(struct zone *zone, int order, int start_migratetype,
                            unsigned int alloc_flags)
    {
        struct free_area *area;
        int current_order;
        int min_order = order;
        struct page *page;
        int fallback_mt;
        bool can_steal;
        // 如果设置alloc_flags为ALLOC_NOFRAGMENT(内存碎片优化), min_order=pageblock_order(MAX_ORDER-1)---尽可能分配大页
        if (alloc_flags & ALLOC_NOFRAGMENT)
            min_order = pageblock_order;
        // 遍历zone->free_area[order](order=MAX_ORDER-1~min_order)
        for (current_order = MAX_ORDER - 1; current_order >= min_order;
                    --current_order) {
            area = &(zone->free_area[current_order]);
            // 查找可以盗取的迁移类型
            fallback_mt = find_suitable_fallback(area, current_order,
                    start_migratetype, false, &can_steal);
            if (fallback_mt == -1)
                continue;
            // 如果can_steal=0且迁移类型为MIGRATE_MOVABLE, 当前所在的order大于需求order, 跳转进入find_smallest
            // 这里的can_steal=0并不表示不能盗取, 只是对于迁移类型为MIGRATE_MOVABLE的内存分配需求有更好的解决方法(窃取和拆分最小的可用页块而不是最大的可用页块)所以单独列出
            if (!can_steal && start_migratetype == MIGRATE_MOVABLE
                        && current_order > order)
                goto find_smallest;
            goto do_steal;
        }
        return false;
    find_smallest:
        // 从最小的order开始遍历
        for (current_order = order; current_order free_area[current_order]);
            fallback_mt = find_suitable_fallback(area, current_order,
                    start_migratetype, false, &can_steal);
            if (fallback_mt != -1)
                break;
        }
        VM_BUG_ON(current_order == MAX_ORDER);
    do_steal:
        // 获得备用迁移类型对应的page_block
        page = get_page_from_free_area(area, fallback_mt);
        // 判断直接盗取(改变page_block的迁移类型), 还是借用(分配但不改变页块迁移类型)
        steal_suitable_fallback(zone, page, alloc_flags, start_migratetype,
                                    can_steal);
        trace_mm_page_alloc_extfrag(page, order, current_order,
            start_migratetype, fallback_mt);
        return true;
    }
> >   * find_suitable_fallback
>>
    int find_suitable_fallback(struct free_area *area, unsigned int order,
                int migratetype, bool only_stealable, bool *can_steal)
    {
        int i;
        int fallback_mt;
        // 判断该order内存链表是否为空
        if (area->nr_free == 0)
            return -1;
        *can_steal = false;
        for (i = 0;; i++) {
            // 遍历备用迁移类型
            fallback_mt = fallbacks[migratetype][i];
            // MIGRATE_TYPES表示不可用, 退出
            if (fallback_mt == MIGRATE_TYPES)
                break;
            // 如果area->free_list[fallback_mt]为空, 遍历下一个备用迁移类型
            if (free_area_empty(area, fallback_mt))
                continue;
            // 判断是否可盗取
            if (can_steal_fallback(order, migratetype))
                *can_steal = true;
            if (!only_stealable)
                return fallback_mt;
            if (*can_steal)
                return fallback_mt;
        }
        return -1;
    }
> > >   * can_steal_fallback
>>>
    static bool can_steal_fallback(unsigned int order, int start_mt)
    {
        // 判断order是否大于等于MAX_ORDER-1
        if (order >= pageblock_order)
            return true;
        // 如果order>=(MAX_ORDER-1)/2 或者 迁移类型为MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE 或者 page_group_by_mobility_disabled=1(gdb动调发现默认为0) 则表示可以盗取
        if (order >= pageblock_order / 2 ||
            start_mt == MIGRATE_RECLAIMABLE ||
            start_mt == MIGRATE_UNMOVABLE ||
            page_group_by_mobility_disabled)
            return true;
        return false;
    }
#### _alloc_pages_slowpath
    static inline struct page *
    __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
                            struct alloc_context *ac)
    {
        bool can_direct_reclaim = gfp_mask & __GFP_DIRECT_RECLAIM;
        // PAGE_ALLOC_COSTLY_ORDER=3
        const bool costly_order = order > PAGE_ALLOC_COSTLY_ORDER;
        struct page *page = NULL;
        unsigned int alloc_flags;
        unsigned long did_some_progress;
        enum compact_priority compact_priority;