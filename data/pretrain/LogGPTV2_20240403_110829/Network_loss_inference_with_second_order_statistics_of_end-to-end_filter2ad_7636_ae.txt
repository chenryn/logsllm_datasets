800 
400
600
1000
(b)
Figure 8: Accuracy of the LIA algorithm under dif-
ferent values of p and S.
for Matlab programs on a Pentium 4 with a 2Ghz proces-
sor and 2GB of memory. The computation of A, for our
simulated networks could take up to an hour on the same
computer. However, as explained earlier, we need to calcu-
late A only once. After calculating A (only once), the time
needed to run the inference algorithm is in the order of less
than a second.
7.
INTERNET EXPERIMENTS
We test our LIA algorithm on the PlanetLab network with
716 end-hosts. All end-hosts are beacons (VB) and probing
destinations (D).
7.1 Methodology
We ﬁrst use traceroute to measure the network topology.
Traceroute is performed in parallel from each beacon to all
other beacons. The collected routes will then be combined
to form a complete network topology. Using traceroute to
build network topology can lead to errors because of sev-
eral factors. First, for security and performance reasons,
many routers in the Internet do not respond or limit the
rate of responses to ICMP queries. The paths to nodes be-
hind these routers cannot be ascertained. According to our
own traceroute measurements between PlanetLab hosts, 5
to 10% of routers do not respond to ICMP requests. Sec-
ond, many routers have multiple interfaces and return dif-
ferent IP addresses to diﬀerent traceroute requests. We em-
ploy the sr-ally tool [28] to disambiguate multiple interfaces
at a single node. In our measurements on the PlanetLab,
about 16% of routers on PlanetLab have multiple interfaces.
Unfortunately, the sr-ally tool does not guarantee complete
identiﬁcation of routers with multiple interfaces [28]. As a
result, there could be errors in the measured topology of the
network. Despite these errors, the experiment results show
that the LIA algorithm is accurate.
After constructing the routing topology with traceroute,
we remove ﬂuttering paths by examining all pairs of paths.
We found very few of them in our data set. To simplify the
analysis we take one of the ﬂuttering paths to include in the
topology and completely ignore the others. As a result, we
remove 52 out of 48151 paths from the routing matrix. By
removing these paths, we lose information about the links
that only appear in them but they account for less than
0.01% of the total number of links and thus are negligible.
Nodes then send probes between each other to determine
the end-to-end loss rate. We use periodic probes with an
inter-arrival time of 10ms. Each probe is a UDP packet
of 40 bytes. The probing packets consist of a 20-byte IP
header, an 8-byte UDP header, and a payload of 12 bytes
that contains the probing packet sequence number. End-to-
end loss rates are calculated at the receivers based on the
number of received and sent packets over a period of 10 sec-
onds (which corresponds to 1000 probe packets). To avoid
overloading any host or creating additional congestion, we
limit the probing rate from each host to 100KB/sec, i.e.,
each beacon probes 150 paths in 1 minute. Previous exper-
iments on Planetlab [10] show that this probing rate does
not cause additional congestion on the PlanetLab network.
We also randomize the order in which a host sends probes
to other hosts. Every ﬁve minutes all hosts send their mea-
surement data to our central server at EPFL. Note that the
probing rate, the impact of active probes on the network
and the stationarity of network loss processes intermingle
and make the problem of ﬁnding the optimal probing strat-
egy very diﬃcult. We do not address this question in the
current paper.
We launched the experiments on all PlanetLab hosts (716
hosts) on April 20, 2007. For various reasons, such as nodes
being down and failed ssh connections, only 381 hosts suc-
cessfully ran both the traceroute and loss measurements.
The experiment lasted a total of 24 hours with approxi-
mately 2 GB of loss and topology data.
In total we have
250 snapshots of the network. We then run our LIA algo-
rithm on this data set. To infer the link loss rates on any
given snapshot, we ﬁrst use the previous m snapshots to
learn the link variances. After that we use these variances
to infer the link loss rates. We report our analysis of the
data in the next section.
7.2 Results
In the Internet, we do not know the real loss rates of the
network links as in the simulations of Section 6. Therefore
we cannot validate our inference results by comparing them
against the true values. We adopt the indirect validation
method of [24], where the set of end-to-end measured paths
in any time slot is divided into two sets of equal size: the
inference set and the validation set. The partition is done
randomly. We run ﬁrst our LIA algorithm on the inference
set to calculate the link loss rates of the links Einf that appear
in the inference topology. We then use the measurements in
the second set to validate the inferred values. Let φi be the
measured transmission rate of path Pi in the validation set.
We compare φi with the product of the estimated link rate
bφek on the link ek ∈ Pi ∩ Einf to check the consistency of the
inference. More precisely, we declare the estimate correct if:
where ǫ is the tolerable error that is used to account for
sampling errors in estimating the end-to-end loss rates with
probes. In our experiment, we choose ǫ = 0.005.
7.2.1 Accuracy of LIA
In Figure 9 we plot the percentage of paths that are con-
sistent with the test in (11) as a function of the number of
snapshots m used in the variance learning phase. Each point
in the curve is an average of 10 runs. The result shows that
˛˛˛˛˛˛bφi − Yek ∈Pi∩Einf
φek˛˛˛˛˛˛
≤ ǫ,
(11)
Table 3: Location of Congested Links.
tl
intra-AS
0.04
0.02
0.01
inter-AS
53.6%
56.9%
57.8%
46.4%
43.1%
42.2%
more than 95% of the paths in the validation set are consis-
tent with the inferred loss rates. Predictably, the accuracy of
the LIA algorithm increases as we increase m. However, the
accuracy ﬂattens out when m is signiﬁcantly large (m > 80).
s
h
t
a
P
t
n
e
i
t
s
s
n
o
C
f
o
e
g
a
t
n
e
c
r
e
P
100
99
98
97
96
95
94
20
40
Number of Snapshots (m)
60
80
100
Figure 9: Cross validation of the LIA algorithm on
the PlanetLab network.
In all of our experiments, the time needed to solve (3) is
in the order of milliseconds, whereas the time required to
solve (9) is about a second. Overall, the running time of
LIA is in the order of seconds for the PlanetLab network.
7.2.2 Statistics of Congested Links on PlanetLab
Having the link loss rates, we can study statistical proper-
ties of the congested links. These studies allow for a deeper
understanding of the Internet performance. Namely, we can
answer two questions: (i) are the congested links inter- or
intra-AS links? and (ii) How long does a link remain con-
gested?
To answer the ASes question, we need to map IP ad-
dresses into ASes. We use the BGP table available at Route-
Views [23] to construct the mapping.
In our analysis, we
use the mapping that was obtained on April 20th, 2007. Ta-
ble 3 shows the locations of lossy links for diﬀerent link loss
threshold tl (Recall that tl is used to classify links as good
or congested: a link is (not) congested if its loss rate is (less)
more than tl).
We observe that congested links are more likely to be
inter-AS than intra-AS, especially for small tl. However,
compared to the study in [36], the percentage of intra-AS
lossy links in our study is larger. There are two expla-
nations for the diﬀerences between our ﬁndings and those
in [36]. First, we do not apply the probe optimization tech-
nique in [36]. Hence, in our experiments, each beacon needs
to send more active probes than the experiments in [36].
These active probes may create additional packet losses at
the access links in our experiments compared to [36]. Sec-
ond, in [36], only the loss rates of groups of links (MILSes)
can be computed. Therefore, even though a lossy MILS
spans several ASes (30% of them span more than 3 ASes), it
is not clear that the actual lossy links in this group are inter-
AS or intra-AS. Indeed, in [36], only 27.5% of lossy links are
conﬁrmed to be inter-AS whereas 15% are intra-AS. The
other links could be either inter- or intra-AS. Furthermore,
our observation is consistent with the ﬁndings of [2] where
it was observed that non-access bottleneck links are equally
likely to be intra- and inter-AS. This is especially true for
measurements performed from Planet-Lab hosts with high
access bandwidth.
To study the duration of the congested links, we apply the
LIA algorithm on 100 consecutive snapshots and compare
the set of inferred congested links (with tl = 0.01, m = 50).
We ﬁnd that 99% of congested links remain congested for
only one snapshot (5 minutes). The other 1% span two
snapshots. Note that this analysis is sensitive to the dura-
tion S of each snapshot. A complete study of these proper-
ties would require a thorough understanding of the impact
of S, the stationarity of link loss processes, etc. and is a
topic of our future research.
8. CONCLUSION
First-order moments of end-to-end loss rates are in gen-
eral not suﬃcient to uniquely identify average link loss rates.
More information is needed. In contrast, we have shown in
this paper that second-order spatial statistics are suﬃcient
to uniquely identify the variances of loss rates, which in turn
uniquely determine the average loss rates of the most con-
gested links, under the assumptions that their variance is a
non-decreasing function of their mean, and thus that the loss
rate of non-congested links is virtually zero. We show that
this method is both accurate and scalable in our simulations
and experiments.
We expect that the suﬃcient information brought by second-
order statistics of end-to-end paths, without multicast sup-
port, to identify problematic links in a network with a gen-
eral topology can be exploited for other problems of network
inference.
A ﬁrst immediate extension is to compute link delays.
Congested links usually have high delay variations. In this
direction, we ﬁrst need to take multiple snapshots of the
network to learn about the delay variances. Based on the
inferred variances, we could then reduce the ﬁrst order mo-
ment equations by removing links with small congestion de-
lays and then solve for the delays of the remaining congested
links.
A second extension is the detection of anomalies in the
network, from a few vantage points. The inference method
is fast and so could have potential for such problems.
An important question to address is the choice of the op-
timal value for S. As stated earlier, the answer requires un-
derstanding the statistical properties of link performances
on small time-scales. We intend to work on this problem in
our future research.
Acknowledgments
This work is ﬁnancially supported by grant ManCom 2110 of
the Hasler Foundation, Bern, Switzerland. We would like to
thank the anonymous reviewers for their useful discussions
and suggestions. We are particularly grateful to Sridhar
Machiraju for his helpful feedback during the shepherding
phase of the paper.
9. REFERENCES
[1] A. Adams, T. Bu, T. Friedman, J. Horowitz,
D. Towstey, R. Caceres, N. Duﬃeld, F. L. Presti, S. B.
Moon, and V. Paxson. The use of end-to-end multicast
measurements for characterizing internal network
behavior. IEEE Communications Magazine, May 2000.
[2] A. Akella, S. Seshan, and A. Shaikh. An empirical
evaluation of wide-area internet bottlenecks. In Proc.
IMC 03, 2003.
[3] K. Anagnostakis, M. Greenwald, and R. Ryger. Cing:
Measuring network internal delays using only existing
infrastructure. In Proc. IEEE Infocom, 2003.
[4] D. Ariﬂer, G. de Veciana, and B. L. Evans. A factor
analysis approach to inferring congestion sharing
based on ﬂow level measurements. IEEE/ACM
Transactions on Networking, 2007.
[5] B. Augustin, X. Cuvellier, B. Orgogozo, F. Viger,
T. Friedman, M. Latapy, C. Magnien, and R. Teixeira.
Avoiding traceroute anomalies with paris traceroute.
In Proc. of the Internet Measurement Conference,
October 2006.
[6] T. Bu, N. Duﬃeld, F. L. Presti, and D. Towsley.
Network tomography on general topologies. In
Proceedings ACM Sigmetrics 2002, Marina Del Rey,
CA, 2002.
[7] R. Caceres, N. G. Duﬃeld, J. Horowitz, and
D. Towsley. Multicast-based inference of
network-internal loss characteristics. IEEE
Transactions on Information Theory, 45:2462–2480,
1999.
[8] J. Cao, D. Davis, S. V. Wiel, and B. Yu. Time-varying
network tomography: Router link data. Journal of the
American Statistical Association, 95(452):1063–1075,
Dec. 2000.
[9] A. Chen, J. Cao, and T. Bu. Network tomography:
Identiﬁability and fourier domain estimation. In
Proceedings of the IEEE Infocom, Alaska, May 2007.
[10] Y. Chen, D. Bindel, H. Song, and R. H. Katz. An
algebraic approach to practical and scalable overlay
network monitoring. In Proceedings of the ACM
SIGCOMM, Portland, August-September 2004.
[11] M. Coates, A. Hero, R.Nowak, and B. Yu. Internet
tomography. IEEE Signal Processing Magazine, 19,
May 2002.
[12] M. Coates and R. Nowak. Network loss inference using
unicast end-to-end measurement. In Proceedings of the
ITC Seminar on IP Traﬃc, Measurements and
Modelling, Monterey, September 2000.
[13] N. Duﬃeld, F. L. Presti, V. Paxson, and D. Towsley.
Inferring link loss using striped unicast probes. In
Proc. of the IEEE Infocom 2001, Alaska, April 2001.
[14] N. G. Duﬃeld. Network tomography of binary network
performance characteristics. IEEE Transactions on
Information Theory, 52(12):5373–5388, Dec. 2006.
[15] G. H. Golub and C. F. V. Loan. Matrix Computations.
The Johns Hopkins University Press, 1996.
[16] L. P. Hansen. Large sample properties of generalized
method of moments estimators. Econometrica,
50:1029–1054, 1982.
[17] K. Harfoush, A. Bestavros, and J. Byers. Robust
identiﬁcation of shared losses using end-to-end unicast
probes. In Proc. of ICNP’00, 2000.
[18] V. Jacobson. traceroute,
ftp://ftp.ee.lbl.gov/traceroute.tar.z, 1989.
[19] M. S. Kim, T. Kim, Y. S. Hin, S. S. Lam, and E. J.
Powers. A wavelet-based approach to detect shared
congestion. In Proceeding of the ACM SIGCOMM’04,
2004.
[20] R. Mahajan, N. Spring, D. Wetherall, and
T. Anderson. User-level internet path diagnosis. In
Proceedings of the 19th ACM Symposium on Operating
Systems Principles (SOSP’03), pages 106–119, 2003.
[21] A. Medina, I. Matta, and J. Byers. On the origin of
power-laws in internet topologies. ACM Computer
Communication Review, pages 160–163, 2000.
[22] H. X. Nguyen and P. Thiran. The boolean solution to
the congested IP link location problem: Theory and
practice. In Proceedings of IEEE INFOCOM 2007,
May 2007.
[23] U. of Oregon Route Views Archive Project.
http://www.routeviews.org/.
[24] V. N. Padmanabhan, L. Qiu, and H. J. Wang.
Server-based inference of internet performance. In
Proceedings of the IEEE INFOCOM’03, San
Francisco, CA, April 2003.
[25] M. Roughan. Fundamental bounds on the accuracy of
network performance measurements. In Proceedings of
ACM SIGMETRICS, June 2005.
[26] D. Rubenstein, J. Kurose, and D. Towsley. Detecing
shared congestion of ﬂows via end-to-end
measurement. IEEE/ACM Transactions on
Networking, 10(3), June 2002.
[27] J. Sommers, P. Barford, N. Duﬃeld, and A. Ron.
Improving accuracy in end-to-end packet loss
measurement. In Proceedings of ACM SIGCOMM,
August 2005.
[28] N. Spring, D. Wetherall, and T. Anderson.
Scriptroute: A public internet measurement facility. In
USENIX Symposium on Internet Technologies and
Systems (USITS), 2003.
[29] Y. Tsang, M. Coates, and R. Nowak. Passive network
tomography using the EM algorithms. In Proc. IEEE
ICASSP., 2001.
[30] Y. Vardi. Network tomography: Estimating
source-destination traﬃc intensities. Journal of the
American Statistical Association, 91:365–377, 1996.
[31] V.Paxson. End-to-end routing behaviour in the
internet. In Proceedings of ACM SIGCOMM, Aug
1996.
[32] V.Paxson. End-to-end internet packet dynamics. In
Proceedings of the ACM SIGCOMM, Sep 1997.
[33] www.netdimes.org.
[34] www.planet-lab.org.
[35] Y. Zhang, N. Duﬃeld, V. Paxson, and S. Shenker. On
the constancy of internet path properties. In
Proceedings of ACM SIGCOMM Internet
Measurement Workshop, San Francisco, 2001.
[36] Y. Zhao, Y. Chen, and D. Bindel. Toward unbiased
end-to-end network diagnosis. In Proceedings of ACM
SIGCOMM, Pisa, Italy, 2006.