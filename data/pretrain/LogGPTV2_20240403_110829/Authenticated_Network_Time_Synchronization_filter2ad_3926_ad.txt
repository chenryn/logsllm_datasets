Certiﬁcate validation. When using digital certiﬁcates to
authenticate public keys, the synchronization of the issuer
and the relying party is an underlying assumption. This
serves to highlight a signiﬁcant problem – how do you
securely authenticate time using public key infrastructure
without previously having time synchronization with the
issuer? For our construction this must be done once, and
we assume that the client has some out-of-band method
for establishing the trustworthiness of public keys, per-
haps using OCSP [25] with nonces to ensure freshness of
responses, by the user manually setting the time for ﬁrst
certiﬁcate validation, or shipping a trusted certiﬁcate with
the operating system. Since certiﬁcate validity periods
typically range from months to years, if the user is assured
of time synchronization with the issuer to be within range
of hours or days and that range sits comfortably within
the certiﬁcate validation period, this is a viable solution.
ANTP to NTP downgrade. ANTP servers are also NTP
servers, since ANTP is implemented as an NTP extension.
This eases deployment; older clients can continue using
NTP, while newer clients can use ANTP. However, a net-
work adversary can drop the ANTP extension from the
request, and the server will respond with NTP (having in-
terpreted the request as NTP). For this reason, clients that
send an ANTP request must only update their clock based
on a valid ANTP response, and ignore NTP responses.
For similar reasons, clients are not recommended to im-
plement a fall back from ANTP to NTP.
4
Implementation and Performance
Here we describe our instantiation of ANTP in terms of
cryptographic primitives used as well as its implementa-
tion and results on its performance.2
Instantiation and Implementation
4.1
We instantiate ANTP using the following cryptographic
algorithms. We use AES128-GCM as the symmetric
encryption algorithm for the server to encrypt and de-
crypt state, SHA-256 as the hash algorithm, and HMAC-
SHA256 as the MAC and key derivation function. We sup-
port two key encapsulation mechanisms, RSA key trans-
port and static-ephemeral elliptic curve Difﬁe-Hellman:
• RSA key transport: In KeyGen, the public key and
secret key are a 2048-bit RSA key pair. Encap is
deﬁned by selecting a key pms←${0,1}128 and en-
crypting pms using the RSA public key with RSA-
PKCS#1.5 encryption; Decap performs decryption
with the corresponding RSA secret key. The shared
secret is k ← KDF(pms,⊥,“ANTP”,len).
• Static-ephemeral elliptic curve Difﬁe–Hellman: Let
P be the generator (base point) of the NIST-P256
elliptic curve group of prime order q. In KeyGen,
the secret key is sk←$ Zq and the public key is
pk = sk · P, where · denotes scalar–point multi-
In Encap, select r←$ Zq and compute
plication.
c ← r · P and pms ← X(r · pk), where X(Q) gives
the x-coordinate of elliptic curve point Q. In Decap,
compute pms ← X(sk· c). The shared secret is k ←
KDF(pms,⊥,“ANTP”(cid:25)c,len). This is the ECIES-
KEM [29] which is IND-CCA secure under the el-
2Our implementation code and benchmarking tools can be found at
https://github.com/DowlingBJ/AuthenticatedNTP.
USENIX Association  
25th USENIX Security Symposium  831
liptic curve discrete logarithm assumption in the ran-
dom oracle model [8].
We implemented ANTP by extending OpenNTPD ver-
sion 1.92 [34]. Our implementation relies on OpenSSL
version 1.0.2f [35] for its cryptographic components; no-
tably, this version includes a high-speed assembly imple-
mentation of the NIST P-256 curve; AES-GCM encryp-
tion/decryption uses the AES-NI instruction.
4.2 Performance
Methodology. We collected performance measurements
for each of the negotiation, key exchange, and time syn-
chronization phases. We wanted to know the maximum
number of connections per second that could be supported
in each phase, as well as the latency a client would ex-
perience for a typical server. For comparison we also
collected performance measurements for unauthenticated
NTP time synchronization phases.
Our throughput and LAN latency experiments were
carried out in the following environment on our local area
network. We had two machines acting as clients, and
a single server machine running ANTP. The server had
an Intel Core i7-4770 (Haswell) processor with 4 cores
running at 3.4 GHz with 15.6 GiB of RAM; we used two
similar client machines, which in our experiments were
always sufﬁcient to saturate the server. The clients and
server were connected over an isolated 1 gigabit LAN.
The server was running Linux Mint 17.2 with no other
software installed.
Our latency experiments across the US were carried
out between two Amazon AWS m4.2xlarge instances,
the server in the US East (N. Virginia) region and the
client in the US West 1 (N. California) region. These
instances each had eight virtual CPUs, each of which
was an Intel Xeon E5-2676 v3 (Haswell) core running at
2.4 GHz, with 32 GiB of RAM, and 1 Gbps of dedicated
bandwidth; the instances were running Ubuntu 14.04 with
no other software installed.
It is important to note that OpenNTPD is not multi-
threaded, so the OpenNTPD server process runs on a
single core, regardless of the number of cores on the
machine. As the key exchange phase is CPU bound, in a
threaded server implementation we expect key exchange
phase throughput to increase linearly with the number of
CPU cores until bandwidth is saturated.
For testing throughput (connections/second), we used
our own multi-threaded UDP ﬂooding benchmarking tool
that sends static packets and collects the number of re-
sponses, the average latency of those responses, and the
number of dropped packets. We tuned the number of
queries per second to ensure that the server’s (single) core
had around 95% utilization, and that more client packets
were sent than being processed, but not so many more that
performance became degraded (i.e., the server dropped
less than 1% of packets being received per second).
For testing individual phase latency, we again used our
UDP benchmarking tool, this time measuring latency of
a subset of connections while maintaining a particular
background ANTP load at the server (either 50% or 90%
of supported throughput), to measure the latency a client
would experience at an unloaded or loaded server.
For testing total protocol runtime, we instrumented
the OpenNTPD client to report the runtime of a single
complete (all three phases) ANTP synchronization, again
with background ANTP load as above.
Results – individual phases.
Table 1 shows the results of each phase. Results re-
ported are the average of 5 trials to prevent outlier results.
For throughput and individual phase latency, each trial
was run for 100 seconds. For throughput, Table 1 reports
the number of response packets received at the client
machine.
Negotiation phases. The lower throughput of RSA and
ECDH negotiation messages (compared to NTP) is due
to larger message size of ANTP messages, as network
bandwidth was saturated for this measurement. Latency
for ECDH negotiation at 90% load is higher compared to
RSA negotiation at 90% load; at that load level, a much
larger number of ECDH packets are being sent than RSA
packets, so CPU load in the ECDH is higher even though
they have the same bandwidth consumption, leading to
higher latency for ECDH negotiation.
Key exchange phases. As expected, server key ex-
change throughput is higher when ECC is used for pub-
lic key operations compared to RSA. This difference is
explained by the relative costs of the underlying crypto-
graphic operations: using OpenSSL’s speed command
for benchmarking individual crypto operations, the run-
time of ECC NIST P-256 point multiplication is 8.62×
faster than RSA 2048 private key operations, whereas we
observe a 7.54× improvement in throughput for ANTP’s
ECDH key exchange over ANTP’s RSA key exchange.
Latency on the local network for RSA key exchange is
approximately 2.9× that of ECDH key exchange at 90%
load.
Time synchronization phases. While ANTP time syn-
chronization phases are more computationally intensive
than unauthenticated NTP, throughput is reduced by only
a factor of approximately 1.6. Since this phase is CPU
bound, we expect a multi-threaded server implementation
to increase ANTP throughput. Latency increase on the
local network for ANTP at 50% load is only about 14%
and at 90% load is about 27%.
Results and extrapolation – all 3 phases. Since each
client makes a full 3-phase time synchronization (negoti-
ation, followed by key exchange, followed by time syn-
chronization) relatively infrequently, it does not make
832  25th USENIX Security Symposium 
USENIX Association
sense to measure server throughput for complete 3-phase
time synchronizations. We did measure latency of a 3-
phase time synchronization to note the performance that
a client would perceive on its initial synchronization. As
expected, the total runtime of a client exceeds the sum of
the latencies from each individual phase due to the client
performing its own cryptographic operations.
It is interesting to note that latency slows as the server
approaches load capacity. Future work on OpenNTPD
and other NTP servers could include optimizations to re-
duce latency and improve time synchronization accuracy
under increasing load.
We can extrapolate from the individual phase results
the client pool that ANTP could feasibly support running
on the same hardware. For example, Windows by de-
fault polls time servers every 9 hours [13]. Assuming
this is true for all clients (and that the clients synchronize
uniformly across the period) 175,644 time synchroniza-
tion requests per second would correspond to a pool of
5,755,502,592 clients.
ANTP clients would choose how often to restart the ne-
gotiation phase and we recommend doing so periodically
to ensure the attack window from exposure of the symmet-
ric key is limited. If keys are re-exchanged monthly, this is
a ratio of 1:1:1440 for expected negotiation, key exchange,
and time synchronization messages, which increases to
1:1:8640 if clients re-exchanged every 6 months. From
these or other expected ratios, one could extrapolate the
expected performance impact of using ANTP over NTP.
5 Security Framework
In this section we introduce our new time synchronization
provable security framework for analyzing time synchro-
nization protocols such as ANTP, NTP, and the Precision
Time Protocol. It builds on both the Bellare–Rogaway
model [5] for authenticated key exchange and the Jager
et al. framework for authenticated and conﬁdential chan-
nel establishment [11]. Neither of those models however
includes time.
Our framework models time as a counter that each
party separately maintains, as the goal of the protocol is
to synchronize these disparate counters. Additionally, the
adversary in our execution environment has the ability to
initialize each protocol run with a new time counter in-
dependent of the party’s own counter, and controls when
protocol runs can increment their counter, effectively giv-
ing the adversary complete control of both the latency of
the network and the computation time of the parties.
5.1 Execution Environment
There are np parties P1, . . . ,Pnp, each of whom is a proto-
col participant. Each party generates a long-term key-pair
i . Note that each session πs
(ski, pki), and can run up to ns instances of the protocol
which are referred to as sessions. We denote the sth ses-
sion of a party Pi as πs
i has
access to the long-term key pair of the party Pi. In ad-
dition, we denote with T and Tc the full transcript and
server-session maintained client transcript Tc.
Per-Session Variables.
maintained by each session:
The following variables are
ner.
• ρ ∈ {client, server}: the role of the party.
• id ∈ {1, ...,np}: the identity of the party.
• pid ∈ {1, . . . ,n p}: the believed identity of the part-
• α ∈ {accept, reject, in-progress}: the session
status.
• k ∈ {0,1}128: the session key.
• Tc ∈ {{0,1}∗, /0}: if ρ = server, the transcript of
client messages, otherwise Tc = /0.
• T ∈ {0,1}∗: the transcript of messages sent and re-
• time ∈ N: a counter maintained by the session.
ceived.
Adversary Interaction. The adversary schedules and
controls all interactions between protocol participants.
The adversary has control of all communication, able to
create, delete, reorder or modify messages at will. The
adversary can compromise long-term and session keys.
Additionally, the adversary is able to set the clock of
a party to an arbitrary time when a session begins and
control the rate at which time progresses during the exe-
cution of a session. The following queries model normal
execution with adversary control of time:
• Create(i,r,t): The adversary activates a new session
i .time = t.
i responds with
with party Pi, initializing πs
Note that if πs
i .ρ = client, then πs
the ﬁrst message of the protocol run.
i .ρ = r and πs
• Send(i,s,m,(cid:29)∆): The adversary sends a message m
to a session πs
i . Party Pi processes the message m
and responds according to protocol speciﬁcation,
updating per-session variables and outputting some
message m∗ if necessary. During message process-
ing, the party may execute multiple calls to a dis-
tinguished Now() procedure, modelling the party
reading its current time from memory; immediately
before the (cid:28)th such call to the Now() procedure, the
session’s πs
i .time variable is incremented by ∆(cid:28).
These queries model compromise of secret data:
k of the session πs
i .
• Reveal(i,s): The adversary receives the session key
• Corrupt(i): The adversary receives the long-term
The following query allows additional adversary control
of the clock:
secret-key ski of the party Pi.
USENIX Association  
25th USENIX Security Symposium  833
πs
i .time by ∆.
• Tick(i,s,∆): The adversary increments the counter
The vector (cid:29)∆ in Send is necessary due to subtleties
in the security framework: An adversary cannot issue
Tick queries to a session during the processing of a Send
query, but a party may read its clock multiple times while
processing a message and thus expect to receive differ-
ent clock times. The vector (cid:29)∆ in the Send query allows
adversary control of this clock rate.
Note that our model assumes that during execution
of a session, the clocks between two parties advance at
the same rate, otherwise it does not make sense for two
parties to try to synchronize their clocks at all. This im-
plicitly assumes that the parties are in the same reference
frame. Additionally, while computer clocks may progress
at different rates, we are assuming that, over a relatively
short period of time, like the few seconds for an execu-
tion of the protocol, the difference in clock rate will be
negligible. This will be formalized in Deﬁnitions 3 and
4 with the condition that the adversary advances the time
i=1 ∆l)
of matching sessions symmetrically: a Tick( j,t,∑(cid:28)