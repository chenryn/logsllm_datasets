would read, for example, ”Obama is giving FREE Gas
Cards Worth $250! Call now-&gt; 1 888-858-5783 (US
Only)@@@.” In our evaluation, 0.3% of the generated
groups did not include URLs. Existing techniques, such
as [13], which solely focus on URLs and their reputation,
would fail to detect such campaigns.
Detecting Worms Online social networks have been re-
peatedly confronted with XSS worm outbreaks that rapidly
infect thousands of accounts. Since the behavior of the af-
fected accounts is expected to diverge from their usual be-
havioral proﬁles, we show in Appendix A that COMPA suc-
cessfully detects such outbreaks.
7 Limitations
An attacker who is aware of COMPA has several pos-
sibilities to prevent his compromised accounts from being
detected by COMPA. First, the attacker can post messages
that align with the behavioral proﬁles of the compromised
accounts. As described in Section 3, this would require
the attacker to invest signiﬁcant time and computational re-
sources to gather the necessary proﬁle information from his
victims. Furthermore, social networks have mechanisms in
place that prevent automated crawling, thus slowing down
such data gathering endeavors.
Second, an attacker could send messages that evade
our similarity measures, and thus, although such messages
might violate their compromised accounts’ behavioral pro-
ﬁles, they would not get grouped together. To counter such
evasion attempts, COMPA can be easily extended with ad-
ditional and more comprehensive similarity measures. For
example, it would be straight-forward to create a similar-
ity measure that uses the landing page instead of the URLs
contained in the messages to ﬁnd groups of similar mes-
sages. Furthermore, more computationally expensive simi-
larity measures, such as text shingling or edit distances for
text similarity can also be implemented. Other similarity
measures might leverage the way in which messages prop-
agate along the social graph to evaluate message similarity.
8 Related Work
The popularity of social networks inspired many scien-
tiﬁc studies in both, networking and security. Early detec-
tion systems for malicious activity on social networks fo-
cused on identifying fake accounts and spam messages [5,
6, 7] by leveraging features that are geared towards recog-
nizing characteristics of spam accounts (e.g., the presence
of URLs in messages or message similarity in user posts).
Cai et al. [18] proposed a system that detects fake proﬁles
on social networks by examining densely interconnected
groups of proﬁles. These techniques work reasonably well,
and both Twitter and Facebook rely on similar heuristics to
detect fake accounts [28, 29].
In response to defense efforts by social network
providers, the focus of the attackers has shifted, and a ma-
jority of the accounts carrying out malicious activities were
not created for this purpose, but started as legitimate ac-
counts that were compromised [3, 4]. Since these accounts
do not show a consistent behavior, previous systems will
fail to recognize them as malicious. Grier et al. [4] studied
the behavior of compromised accounts on Twitter by enter-
ing the credentials of an account they controlled on a phish-
ing campaign site. This approach does not scale as it re-
quires identifying and joining each new phishing campaign.
Also, this approach is limited to phishing campaigns. Gao
et al. [10] developed a clustering approach to detect spam
wall posts on Facebook. They also attempted to determine
whether an account that sent a spam post was compromised.
To this end, the authors look at the wall post history of spam
accounts. However, the classiﬁcation is very simple. When
an account received a benign wall post from one of their
connections (friends), they automatically considered that
account as being legitimate but compromised. The prob-
lem with this technique is that previous work showed that
spam victims occasionally send messages to these spam ac-
counts [7]. This would cause their approach to detect le-
gitimate accounts as compromised. Moreover, the system
needs to know whether an account has sent spam before it
can classify it as fake or compromised. Our system, on the
other hand, detects compromised accounts also when they
are not involved in spam campaigns. As an improvement
to these techniques, Gao et al. [10] proposed a system that
groups similar messages posted on social networks together,
and makes a decision about the maliciousness of the mes-
sages based on features of the message cluster. Although
this system can detect compromised accounts, as well as
fake ones, their approach is focused on detecting accounts
that spread URLs through their messages, and, therefore, is
not as generic as COMPA.
Thomas et al. [13] built Monarch to detect malicious
messages on social networks based on URLs that link to
malicious sites. By relying only on URLs, Monarch misses
other types of malicious messages. For example, the scams
based on phone numbers that COMPA detected would not
be detected. It also would not detect a XSS worm spreading
without a URL, as well as a new, emerging kind of spam
that includes incomplete links in the tweet (e.g., a missing
http://). These spam messages ask users to copy and
paste a fragmented URL in the browser address bar [30],
where the URL is automatically reassembled. Lee et al. [12]
proposed WARNINGBIRD, a system that detects spam links
posted on Twitter by analyzing the characteristics of HTTP
redirection chains that lead to a ﬁnal spam page.
Xu et al. [31] present a system that, by monitoring a
small number of nodes, detects worms propagating on so-
cial networks. This paper does not directly address the prob-
lem of compromised accounts, but could detect large-scale
infections such as koobface [2].
Yang et al. [17] studied new Twitter spammers that act
in a stealthy way to avoid detection. In their system, they
use advanced features such as the topology of the network
that surrounds the spammer. They do not try to distinguish
compromised from spam accounts.
9 Conclusions
In this paper, we presented a novel approach to detect
compromised accounts in social networks. More precisely,
we developed statistical models to characterize the behav-
ior of social network users, and we used anomaly detection
techniques to identify sudden changes in their behavior. We
developed COMPA, a prototype tool that implements this
approach, and we applied it to a large stream of messages.
The results show that our approach reliably detects compro-
mised accounts, even though we do not have full visibility
of every message exchanged on Facebook and Twitter.
Acknowledgements
This work was supported by the Ofﬁce of Naval Re-
search (ONR) under Grant N000140911042, the Army Re-
search Ofﬁce (ARO) under grant W911NF0910553, and
the National Science Foundation (NSF) under grants CNS-
0845559 and CNS-0905537.
References
[1] Harris Interactive Public Relations Research, “A
Study of Social Networks Scams,” 2008.
[2] J. Baltazar, J. Costoya, and R. Flores, “KOOBFACE:
The Largest Web 2.0 Botnet Explained,” 2009.
[3] H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Zhao,
“Detecting and Characterizing Social Spam Cam-
paigns,” in Internet Measurement Conference (IMC),
2010.
[4] C. Grier, K. Thomas, V. Paxson, and M. Zhang,
“@spam: the underground on 140 characters or less,”
in ACM Conference on Computer and Communica-
tions Security (CCS), 2010.
[5] F. Benvenuto, G. Magno, T. Rodrigues,
and
V. Almeida, “Detecting Spammers on Twitter,” in
Conference on Email and Anti-Spam (CEAS), 2010.
[6] K. Lee, J. Caverlee, and S. Webb, “Uncovering so-
cial spammers: social honeypots + machine learning,”
in International ACM SIGIR Conference on Research
and Development in Information Retrieval, 2010.
[7] G. Stringhini, C. Kruegel, and G. Vigna, “Detecting
Spammers on Social Networks,” in Annual Computer
Security Applications Conference (ACSAC), 2010.
[8] B. Stone-Gross, M. Cova, L. Cavallaro, B. Gilbert,
M. Szydlowski, R. Kemmerer, C. Kruegel, and G. Vi-
gna, “Your Botnet is My Botnet: Analysis of a Bot-
net Takeover,” in ACM Conference on Computer and
Communications Security (CCS), 2009.
[9] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda, “All
Your Contacts Are Belong to Us: Automated Identity
Theft Attacks on Social Networks,” in Wold Wide Web
Conference (WWW), 2009.
[10] H. Gao, Y. Chen, K. Lee, D. Palsetia, and A. Choud-
hary, “Towards Online Spam Filtering in Social Net-
works,” in Symposium on Network and Distributed
System Security (NDSS), 2012.
[11] “foursquare,” http://foursquare.com.
[12] S. Lee and J. Kim, “WarningBird: Detecting Suspi-
cious URLs in Twitter Stream,” in Symposium on Net-
work and Distributed System Security (NDSS), 2012.
[13] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song,
“Design and Evaluation of a Real-Time URL Spam
Filtering Service,” in IEEE Symposium on Security
and Privacy, 2011.
[14] “Oauth community site,” http://oauth.net.
[15] W. B. Cavnar and J. M. Trenkle, “N-gram-based text
categorization,” in In Proceedings of SDAIR-94, 3rd
Annual Symposium on Document Analysis and Infor-
mation Retrieval, 1994, pp. 161–175.
[16] J. C. Platt, “Fast Training of Support Vector Machines
Using Sequential Minimal Optimization,” in Advances
in Kernel Methods - Support Vector Learning, 1998.
[17] C. Yang, R. Harkreader, and G. Gu, “Die Free or
Live Hard? Empirical Evaluation and New Design for
Fighting Evolving Twitter Spammers,” in Symposium
on Recent Advances in Intrusion Detection (RAID),
2011.
[18] Z. Cai and C. Jermaine, “The Latent Community
Model for Detecting Sybils in Social Networks,” in
Symposium on Network and Distributed System Secu-
rity (NDSS), 2012.
[19] J. Song, S. Lee, and J. Kim, “Spam Filtering in Twitter
using Sender-Receiver Relationship,” in Symposium
on Recent Advances in Intrusion Detection (RAID),
2011.
[20] “Surbl,” http://www.surbl.org.
[21] “Weka - data mining open source program,” http:
//www.cs.waikato.ac.nz/ml/weka/.
[22] “Spamhaus dbl,” http://www.spamhaus.org.
[23] “Google safebrowsing,” http://code.google.com/apis/
safebrowsing/.
[24] “Phishtank,” http://www.phishtank.com.
[25] “Wepawet,” http://wepawet.iseclab.org.
[26] “Exposure,” http://exposure.iseclab.org/.
[27] “Fox news’s hacked twitter feed declares obama
http://www.guardian.co.uk/news/blog/2011/
dead,”
jul/04/fox-news-hacked-twitter-obama-dead, 2011.
[28] C. Ghiossi, “Explaining Facebook’s Spam Preven-
tion Systems,” http://blog.facebook.com/blog.php?
post=403200567130, 2010.
[29] Twitter, “The twitter rules,” http://support.twitter.com/
entries/18311-the-twitter-rules, 2010.
[30] F-Secure,
“The increasingly shapeshifting web,”
http://www.f-secure.com/weblog/archives/00002143.
html.
[31] W. Xu, F. Zhang, and S. Zhu, “Toward worm detec-
tion in online social networks,” in Annual Computer
Security Applications Conference (ACSAC), 2010.
[32] “Nielsen,” http://blog.nielsen.com.
A Detecting Worms
Twitter has been affected by multiple worm outbreaks.
For example, in September 2010 a XSS worm exploited
a vulnerability in the way in which Twitter parsed URLs
in tweets. More speciﬁcally,
if a URL contained an
”@” symbol, Twitter would interpret everything follow-
ing that character as JavaScript. Therefore, a user who
hovered her mouse over a tweet containing a URL simi-
lar to http://x.xx/@”onmouseover=”alert(1) would execute
the JavaScript event handler in her browser. Of course, the
real worm used JavaScript that would self propagate the
worm instead of the alert statement. Obviously, post-
ing the tweet that contained the body of the worm happened
without the user’s consent, and, therefore, we have to con-
sider such accounts as compromised. Note that the URL
preceding the @ sign was irrelevant for the attack. There-
fore, existing detection approaches that examine the mali-
ciousness of URLs would fail to detect this XSS worm at-
tack, as the attacker could chose any benign domain (e.g.,
http://www.google.com).
To evaluate whether COMPA is capable of detecting
worm outbreaks, we simulated the worm outbreak on real
Twitter data. That is, we chose a random message S0 of a
random user U0 on the Twitter network. We assumed that
the worm would propagate from user A to user B iff user B
follows user A, and user B was active on Twitter within a
time window T around the point in time when user A posts
the offending message. Due to the lack of detailed usage
information, we determine the activity of a user by observ-
ing when they tweet. Thus, a user is deemed active T/2
before and after she posted any status updates through the
Twitter web interface. Note that this deﬁnition of activity
(i.e., a user is only deemed active when she is posting) is
conservative, as users are often browsing Twitter or reading
other people’s tweets, even if they do not post at the same
time. Furthermore, the worm only propagates itself if the
tweet that user B sent was posted through the Twitter web
site. That is, alternative clients are assumed not to contain
the same vulnerability in their URL parsing routines. The
XSS worm we are simulating is aggressive in that it spreads
as soon as the user hovers the mouse over the tweet. We
assume that if a user is active, she will hover her mouse
over the tweet, and thus, get infected. For every propaga-
tion step, we record the IDs of users A and B, as well as
the ID of the tweet that was used to determine that user B
is active (i.e., the tweet user B sent within the time window
T). According to [32], web users spend roughly 10 minutes
per day on social networks. Thus, we assumed a value of
10 minutes for T in our simulation.
Subsequently, we downloaded the timelines of the users
infected by the simulated worm. Then, we substituted the
tweets that were responsible for the worm propagation with
a copy of the XSS worm. Finally, we ran COMPA on these
timelines. Although the way we simulated the worm out-
break means that the timing and source models are drawn
from real information (i.e., we only substituted the text of
the tweet), COMPA was able to successfully detect the out-
break and the compromised accounts after the worm spread
to 2,256 accounts in 20 minutes. This means that the ”worm
group” contained enough tweets that violated their respec-
tive users’ behavioral proﬁles. It turns out that our propaga-
tion strategy was chosen conservatively as news reports2 of
previous Twitter worms report of 40,000 infected accounts
within 10 minutes. Thus, assuming the distribution of pro-
ﬁle violations is similar for such aggressive worms, COMPA
would detect such a large scale outbreak even faster.
2http://eu.techcrunch.com/2010/09/21/warning-mouseover-tweets-security-ﬂaw-
is-wreaking-havoc-on-twitter/