ceived by a given proﬁle in our dataset (either a Sybil or
legitimate proﬁle) by a given group of turkers (China, US,
or India). To conduct our simulation, we randomize the or-
der of C, then calculate what the overall false positive and
negative rates would be as we include progressively more
votes from the list. For each proﬁle, we randomize the list
and conduct the simulation 100 times, then average the rates
for each number of votes. Intuitively, what this process re-
veals is how the accuracy of the turker group changes as
we increase the number of votes, irrespective of the speciﬁc
order that the votes arrive in.
The results of our simulations demonstrate that there are
limits to how much accuracy can be improved by adding
more turkers to the group, as shown in Figure 9. Each line
plots the average accuracy over all Sybil and legitimate pro-
ﬁles for a given group of turkers. For false positives, the
trend is very clear: after 4 votes, there are diminishing re-
turns on additional votes. For false negatives, the trend is
either ﬂat (US turkers), or it grows slightly worse with more
votes (China and India).
Filtering Inaccurate Turkers.
Since adding more turk-
ers does not signiﬁcantly increase accuracy, we now inves-
tigate the opposite approach: eliminating turkers that are
consistently inaccurate. Many deployed crowdsourcing sys-
tems already use this approach [22]. Turkers are ﬁrst asked
to complete a pre-screening test, and only those who per-
form sufﬁciently well are allowed to work on the actual job.
In our scenario, turkers could be pre-screened by asking
them to classify accounts from our ground-truth datasets.
Only those that correctly classify x accounts, where x is
some conﬁgurable threshold, would be permitted to work
on actual jobs classifying suspicious accounts.
To gauge whether this approach improves Sybil detec-
tion accuracy, we conduct another simulation. We vary the
accuracy threshold x, and at each level we select all turkers
that have overall accuracy ≥ x. We then plot the false neg-
ative rate of the selected turkers in Figure 10. Intuitively,
this simulates turkers taking two surveys: one to pre-screen
them for high accuracy, and a second where they classify
unknown, suspicious accounts.
Figure 10 demonstrates that the false negative rate of the
turker group can be reduced to the same level as experts by
eliminating inaccurate turkers. The false negative rates are
stable until the threshold grows >40% because, as shown
in Figure 4, almost all the turkers have accuracy >40%. By
70% threshold, all three test groups have false negative rates
≤10%, which is on par with experts. We do not increase
the threshold beyond 70% because it leaves too few turkers
to cover all the Sybil proﬁles in our dataset. At the 70%
threshold, there are 156 Chinese, 137 Indian, and 223 US
turkers available for work.
5.4 Proﬁle Difﬁculty
The last question we examine in this section is the fol-
lowing: are there extremely difﬁcult “stealth” Sybils that re-
sist classiﬁcation by both turkers and experts? As we show
in Table 3, neither experts nor turkers have 0% false nega-
tives when classifying Sybils. What is unknown is if there
is correlation between the false negatives of the two groups.
To answer this question, we plot Figure 11. Each scatter
plot shows the average classiﬁcation accuracy of the Sybils
from a particular region. The x-axes are presented in as-
cending order by turker accuracy. This is why the points for
the turkers in each subﬁgure appear to form a line.
Figure 11 reveals that, in general, experts can correctly
classify the vast majority of Sybils that turkers cannot (e.g.
turker accuracy  T are placed in the upper layer, otherwise
they go into the lower layer.
In the two-layer scheme, proﬁles are ﬁrst evaluated by
turkers in the lower layer.
If there is strong consensus
among the lower layer that the proﬁle is Sybil or legitimate,
then the classiﬁcation stands. However, if the proﬁle is con-
troversial, then it is sent to the more accurate, upper layer
turkers for reevaluation. Each proﬁle receives B votes in
the lower layer and U votes in the upper layer. Intuitively,
the two-layer system tries to maximize the utility of the very
accurate turkers by only having them evaluate difﬁcult pro-
ﬁles. Figure 12 depicts the two-layer version of our system.
In our design, we cap the maximum acceptable false pos-
itive rate at 1%. Our motivation is obvious: social network
providers will not deploy a security system that has a non-
negligible negative impact on legitimate users. We con-
ducted simulations on all our turker groups, with consistent
results across groups. For brevity, we limit our discussion
here to results for the Chinese turkers. As shown in Fig-
ure 6, the Chinese turkers have the worst overall accuracy
of our turker groups. Thus, they represent the worst-case
scenario for our system. The US and Indian groups both
exhibit better performance in terms of cost and accuracy
during simulations of our system.
Votes per Proﬁle.
In the one-layer simulations, the only
variable is votes per proﬁle V . Given our constraint on false
positives <1%, we use multiple simulations to compute the
minimum value of V . The simulations reveal that the mini-
mum number of votes for the Chinese proﬁles is 3; we use
this value in the remainder of our analysis.
Calculating the votes per proﬁle in the two-layer case is
more complicated, but still tractable. The two-layer sce-
nario includes four variables: votes per proﬁle (U upper
and L lower), the accuracy threshold between the layers