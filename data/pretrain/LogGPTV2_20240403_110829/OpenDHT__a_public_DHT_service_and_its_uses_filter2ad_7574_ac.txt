Furthermore, the per-disk model rewards socially responsible be-
havior on the part of clients. Applications that are ﬂexible in their
key choice—the PAST storage system [14], for example—can tar-
get their puts towards otherwise underutilized nodes, thereby bal-
ancing the load on the DHT while acquiring more storage for them-
selves. By protecting applications that cannot choose their keys
while rewarding those that can, the per-disk model reduces the need
for later load balancing by the DHT itself.
For the above reasons, we have implemented per-disk fairness in
OpenDHT, and we leave the study of global fairness to future work.
Still, per-disk fairness is not as easy to implement as it sounds. Our
storage interface involves both an amount of data (the size of the
put in bytes) and a duration (the TTL). As we will see in Section 4,
we can use an approach inspired by fair queuing [12] to allocate
storage, but the two-dimensional nature of our storage requires sub-
stantial extensions beyond the original fair queuing model.
We now turn to describing the algorithmic components of FST.
First we describe how to achieve high utilization for storage re-
quests of varied sizes and TTLs while preventing starvation. Next,
we introduce the mechanism by which we fairly divide storage be-
tween clients. Finally, we present an evaluation of the FST algo-
rithm in simulation.
4.1 Preventing Starvation
An OpenDHT node prevents starvation by ensuring a minimal
rate at which puts can be accepted at all times. Without such a
4We assume that DHT load-balancing algorithms operate on longer
time scales than bursty storage overloads, so their operation is or-
thogonal to the concerns we discuss here. Thus, in the ensuing
discussion we assume that the key-to-node mapping in the DHT is
constant during the allocation process.
requirement, OpenDHT could allocate all its storage (fairly) for
an arbitrarily large TTL, and then reject all storage requests for
the duration of that TTL. To avoid such situations, we ﬁrst limit
all TTLs to be less than T seconds and all puts to be no larger
than B bytes. We then require that each OpenDHT node be able to
accept at the rate rmin = C/T , where C is the capacity of the disk.
We could choose a less aggressive starvation criterion, one with a
smaller rmin, but we are presenting the most challenging case here.
(It is also possible to imagine a reserved rate for future puts that
is not constant over time—e.g., we could reserve a higher rate for
the near future to accommodate bursts in usage—but as this change
would signiﬁcantly complicate our implementation, we leave it for
future work.)
When considering a new put, FST must determine if accepting
it will interfere with the node’s ability to accept sufﬁciently many
later puts. We illustrate this point with the example in Figure 3,
which plots committed disk space versus time. The rate rmin re-
served for future puts is represented by the dashed line (which has
slope rmin). Consider two submitted puts, a large one (in terms of
the number of bytes) with a short TTL in Figure 3(a) and a small
one with a long TTL in Figure 3(b). The requirement that these
puts not endanger the reserved minimum rate (rmin) for future puts
is graphically equivalent to checking whether the sum of the line
y = rminx and the top edge of the puts does not exceed the stor-
age capacity C at any future time. We can see that the large-but-
short proposed put violates the condition, whereas the small-but-
long proposed put does not.
Given this graphical intuition, we derive a formal admission con-
trol test for our allocation scheme. Let B(t) be the number of bytes
stored in the system at time t, and let D(t1,t2) be the number of
bytes that free up in the interval [t1,t2) due to expiring TTLs. For
any point in time, call it tnow, we can compute as follows the total
number of bytes, f (τ), stored in the system at time tnow + τ assum-
ing that new puts continue to be stored at a minimum rate rmin:
f (τ) = B(tnow)− D(tnow,tnow + τ) + rmin × τ
The ﬁrst two terms represent the currently committed storage that
will still be on disk at time tnow + τ. The third term is the minimal
amount of storage that we want to ensure can be accepted between
tnow and tnow + τ.
Consider a new put with size x and TTL (cid:4) that arrives at time
tnow. The put can be accepted if and only if the following condition
holds for all 0 ≤ τ ≤ (cid:4):
f (τ) + x ≤ C.
(1)
If the put is accepted, the function f (τ) is updated. Although we
omit the details here due to space concerns, this update can be done
in time logarithmic in the number of puts accepted by tracking the
inﬂection points of f (τ) using a balanced tree.
4.2 Fair Allocation
The admission control test only prevents starvation. We now
address the problem of fair allocation of storage among competing
clients. There are two questions we must answer in this regard:
how do we measure the resources consumed by a client, and what
is the fair allocation granularity?
To answer the ﬁrst question, we note that a put in OpenDHT has
both a size and a TTL; i.e., it consumes not just storage itself, but
storage over a given time period. The resource consumed by a put
is then naturally measured by the product of its size (in bytes) and
its TTL. In other words, for the purposes of fairness in OpenDHT,
a put of 1 byte with a TTL of 100 seconds is equivalent to a put of
)
s
r
u
o
h
*
B
M
(
s
d
n
o
c
e
S
*
s
e
t
y
B
l
a
t
o
T
 50
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0
Client 1
Client 2
Client 3
Client 4
 0
 1
 2
 3
 5
 4
 6
Time (hours)
 7
 8
 9  10
)
B
M
(
e
g
a
r
o
t
S
l
a
t
o
T
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0
Client 1
Client 2
Client 3
Client 4
 0
 1
 2
 3
 5
 4
 6
Time (hours)
 7
 8
 9  10
Figure 4: Non-starvation. In this experiment, all clients put above their fair rates, but begin putting at different times.
100 bytes with a TTL of 1 second. We call the product of the put’s
size and its TTL its commitment.
A straightforward strawman algorithm to achieve fairness would
be to track the total commitments made to each client so far, and
accept puts from clients with the smallest total commitments. Un-
fortunately, this policy can lead to per-client starvation. To illus-
trate this point, assume that client A ﬁlls the disk in an otherwise
quiescent system. Once the disk is full, client B begins putting its
own data. B will not starve, as the admission control test guaran-
tees that the node can still accept data at a rate of at least rmin, but
A will starve because this strawman algorithm favors client B until
it reaches the same level of total commitments granted to client A.
This period of starvation could be as long as the maximum TTL, T .
To prevent such per-client starvation, we aim to equalize the rate
of commitments (instead of the total commitments) of clients that
contend for storage. Thus, the service that a client receives depends
only on the competing clients at that instant of time, and not on how
many commitments it was granted in the past. This strategy emu-
lates the well known fair queuing algorithm that aims to provide
instantaneous fairness, i.e., allocate a link capacity equally among
competing ﬂows at every instant of time.
In fact, our FST algorithm borrows substantially from the start-
time fair queuing (SFQ) algorithm [16]. FST maintains a system
virtual time v(t) that roughly represents the total commitments that
a continuously active client would receive by time t. By “contin-
uously active client” we mean a client that contends for storage at
every point in time. Let pi
c denote the i-th put of client c. Then,
c) and a
like SFQ, FST associates with each put pi
ﬁnish time F(pi
c is
S(pi
c). The start time of pi
c) = max(v(A(pi
c))− α, F(pi−1
c a start time S(pi
),0).
(2)
c, and α is a non-negative constant
c
c) is the arrival time of pi
A(pi
described below. The ﬁnish time of pi
c is
c)× ttl(pi
c).
c) + size(pi
c) = S(pi
F(pi
As with the design of any fair queuing algorithm, the key deci-
sion in FST is how to compute the system virtual time, v(t). With
SFQ the system virtual time is computed as the start time of the
packet currently being transmitted (served). Unfortunately, in the
case of FST the equivalent concept of the put currently being served
is not well-deﬁned since there are typically many puts stored in the
system at any time t. To avoid this problem, FST computes the sys-
tem virtual time v(t) as the maximum start time of all puts accepted
before time t.
We now brieﬂy describe how the fairness algorithm works in
conjunction with the admission control test. Each node maintains
a bounded-size queue for each client with puts currently pending.
When a new put arrives, if the client’s queue is full, the put is re-
jected. Otherwise, the node computes its start time and enqueues it.
Then the node selects the put with the lowest start time, breaking
ties arbitrarily. Using the admission control test (Eqn. 1) the node
checks whether it can accept this put right away. If so, the node ac-
cepts it and the process is repeated for the put with the next-lowest
start time. Otherwise, the node sleeps until it can accept the pend-
ing put.
If another put arrives, the node awakes and repeats this computa-
tion. If the new put has the smallest start time of all queued puts it
will preempt puts that arrived before it. This preemption is partic-
ularly important for clients that only put rarely—well below their
fair rate. In such cases, the max function in Equation 2 is dom-
inated by the ﬁrst argument, and the α term allows the client to
preempt puts from clients that are at or above their fair rate. This
technique is commonly used in fair queuing to provide low latency
to low-bandwidth ﬂows [12].
FST can suffer from occasional loss of utilization because of
head-of-line blocking in the put queue. However, this blocking
can only be of duration x/rmin, where x is the maximal put size,
so the loss of utilization is quite small. In particular, in all of our
simulations FST achieved full utilization of the disk.
4.3 Evaluation
We evaluate FST according to four metrics: (1) non-starvation,
(2) fairness, (3) utilization, and (4) queuing latency. We use differ-
ent maximum TTL values T in our tests, but rmin is always 1000
bytes per second. The maximum put size B is 1 kB. The maximum
queue size and α are both set to BT .
For ease of evaluation and to avoid needlessly stressing Plan-
etLab, we simulate our algorithm using an event-driven simulator
run on a local machine. This simulator tracks relevant features of
an OpenDHT node’s storage layer, but does not model any net-
work latency or bandwidth. The interval between two puts for each
client follows a Gaussian distribution with a standard deviation of
0.1 times the mean. Clients do not retry rejected puts.
Our ﬁrst experiment shows that FST prevents starvation when
clients start putting at different times. In this experiment, the maxi-
mum TTL is three hours, giving a disk size of 10.3 MB (3×3600×
1000 bytes). Each client submits 1000-byte, maximum-TTL puts
at a rate of rmin. The ﬁrst client starts putting at time zero, and the
subsequent clients start putting two hours apart each. The results of
the experiment are shown in Figure 4. The left-hand graph shows
the cumulative commitments granted to each client, and the right-
hand graph shows the storage allocated to each client over time.
Early in the experiment, Client 1 is the only active client, and it
quickly acquires new storage. When Client 2 joins two hours later,
the two share the available put rate. After three hours, Client 1 con-
tinues to have puts accepted (at 0.5rmin), but its existing puts begin
to expire, and its on-disk storage decreases. The important point to
note here is that Client 1 is not penalized for its past commitments;
its puts are still accepted at the same rate as the puts of the Client 2.
While Client 1 has to eventually relinquish some of its storage, the
)
s
e
t
y
b
f
o
s
0
0
0
,
1
(
d
e
r
i
u
q
c
A
e
g
a
r
o
t
S
 720
 600
 480
 360
 240
 120
Test 1