is characterized by a given network capacity (i.e., how many clients
can it serve) and distribution costs. We want to assign each user a
suitable CDN and bitrate that maximizes some notion of global
utility for the content providers and consumers, while operating
within the provider’s cost constraints and the CDN capacities. There
are three main challenges here. First, we want to choose a suit-
able utility and policy objective. For example, this utility can be
a function of the bitrate, quality metrics such as buffering, and the
providers’ policy goals (e.g., premium customers get higher prior-
ity over non-paying users). Designing a good video utility metric
that can combine different notions of quality (e.g., bitrate, rebuffer-
ing, startup delay) is an open challenge that is outside the scope of
this paper [21, 35, 40]. The provider can also specify other policy
constraints; e.g., should it admit new clients when all CDNs are
overloaded. Our focus is to make a case for such a framework and
present initial steps toward a practical realization rather than pre-
scribe speciﬁc utility or policy functions. Second, this optimization
must fast enough in order to periodically re-optimize the assign-
ments in response to network dynamics. Third, we need to ensure
that the optimization is stable and does not itself introduce biases
or instability (see Section 5.1).
4. POTENTIAL FOR IMPROVEMENT
Before attempting to design a speciﬁc control plane, we want to
ﬁrst establish the improvement in video quality that we can achieve.
To this end, in this section we analyze the potential improvement
that clients could achieve by choosing a better CDN. As we will
see later, the techniques described here can be extended to realize
the performance oracle described in the previous section.
4.1 Approach
Our goal is to determine the potential performance improvement
assuming each session makes the best possible choice. Ideally, each
client will try all possible choices and pick the one with the best
performance (e.g., rebuffering rate). Moreover, a client will con-
stantly re-evaluate the performance and switch, if needed, to im-
prove its performance. For example, a client can start with the con-
ﬁguration (CDN 1, bitrate 1), and later switch to (CDN 2, bitrate 2),
if the new choice provides better performance. Of course, in prac-
tice we cannot have each client continuously probe all possible
combinations. To get around this limitation, we extrapolate the
performance a client could have achieved based on our observed
performance of other clients that share similar attributes, such as
ISP, location, device, and time-of-day. We follow previous work
on non-parametric prediction [24, 33] with some simplifying mod-
iﬁcations.
We make two simplifying assumptions. First, we do not con-
sider bitrate selection in this section. Second, we assume that ses-
sion outcomes are independent and that CDN performance does not
degrade with load. We relax these assumptions in Section 5.2.
Our approach has two logical stages: estimation and extrapola-
tion that we describe next.
Figure 6: Overview of a video control plane
Of course, it may be unnecessary and impractical for this control
plane to be continuously involved in adapting the CDN and bitrate.
Thus, we can consider intermediate points in this design space as
well. For example, CDN selection can be driven by the control
plane because it has a global view of CDN performance, but bitrate
adaptation may run purely at the client (rows 4 and 5).
3.2 Vision for a Video Control Plane
The notion of a centralized control plane to optimize content de-
livery is not new and has been used within CDNs and ISPs for
server selection and content placement [12, 13, 36]. There are two
key differences in the context of video optimization. First, we intro-
duce a new dimension of cross CDN optimization and combining
this with bitrate selection/adaptation. Second, we focus on the pos-
sibility of midstream switching of both parameters, whereas most
CDN optimizations focus only on start-time selection. As a sim-
ple starting point, our current work assumes that this control plane
operates per content provider. That is, a video content provider
such as YouTube or Hulu runs such a control plane to monitor and
improve the video experience for its customers. We discuss issues
involving the interaction between multiple such providers and con-
trollers in Section 7.
Figure 6 shows a high-level overview of the three key compo-
nents in the video control plane: (1) a measurement component
responsible for actively monitoring the video quality of clients, (2)
a performance oracle that uses historical and current measurements
to predict the potential performance a user will receive for a partic-
ular combination of CDN and bitrate at the current time, and (3)
the global optimization engine that uses the measurement and per-
formance oracle to assign the CDN and bitrate for each user. Next,
we brieﬂy highlight the main factors and challenges involved in the
design of each component.
Measurement Engine: The measurement engine periodically col-
lects quality statistics for currently active users. Because the client-
side player is in the best position to measure the observed video
quality, we envision the client player periodically (every few sec-
onds) reporting such statistics. In addition to reporting the video
quality metrics (e.g., buffering, join time, average bitrate), the mea-
surement engine also collects user and session attributes such as the
ISP, location, current CDN being used, and player version that will
aid in the performance prediction. The challenge here is to choose
a suitable granularity of attributes and quality metrics to measure,
and to decide an appropriate frequency at which these reports are
sent to the control plane.
Performance Oracle: The performance oracle plays a key role in
answering what-if style questions at the control plane to predict the
Global  Optimization  CDN-1 CDN-n Client Performance Oracle Measured  History Current Measurements Client … Switch  CDN  and/or  Bitrate Update Feedback s	
  Policy goals and constraints s	
  Player instrumentation 364a
Estimation: In the estimation step, we compute the empirical per-
formance of each combination of attribute and parameter values.
Let a denote a set of values of a client’s attributes, e.g., ISP =
AT&T, City=Chicago, Device=XBox. Further, let Sa denote the
set of clients sharing same attribute values a, and let Sa,p denote
the set of clients with attribute values a that have made the same
choice or parameter p (i.e., CDN). An example of such set would
be, XBox devices of Comcast’s subscribers located in Chicago that
stream content from Akamai.
).5
) < MEAN(PerfDist a,p2
For each set Sa,p, we compute the empirical distribution for the
metric of interest, e.g., rebuffering ratio. Let PerfDist a,p denote
this empirical distribution. Given two such performance distri-
butions, we say that PerfDist a,p1 is better than PerfDist a,p2, if
MEAN(PerfDist a,p1
a = argminp{MEAN(PerfDist a,p)}
Extrapolation: We use p∗
to denote the parameter with the best performance distribution for
this speciﬁc value of the attribute a. Using this deﬁnition, we can
extrapolate the best possible performance that can be achieved by
a session with attribute values a by selecting parameter p∗
a , and
assuming that the performance experienced by the session is ran-
domly drawn from the distribution PerfDist a,p∗
as shown in Fig-
ure 7(a).
Now, for such extrapolations to be statistically meaningful, the
number of observations for each a, p setting |Sa,p| should be rea-
sonably large. Unfortunately, we run into the classic curse of di-
mensionality; that is, as the attribute space becomes more ﬁne-
grained, the available data becomes sparse [14]. This is particularly
problematic because we will be picking the CDN with the highest
extrapolated value using this methodology. If we pick the highest
among several noisy predictions, we may show improvements even
where there are none.
Hierarchical estimation and extrapolation: To address the prob-
lem of data sparsity at ﬁner granularity, we use a hierarchical ap-
proach [25, 33]. We begin with an exhaustive enumeration of all
possible combinations of attributes. That is, if A = {a1 . . . an}
is the set of client attributes, we construct the powerset 2A of all
attribute combinations. Let attrset ∈ 2A denote one such com-
bination of attribute elements such as {isp, location, timestamp},
{isp, location} or {isp}; let as denote the values of the attributeset
attrset for session s.
Note that a given video session’s performance measurement will
contribute to multiple attribute partitions corresponding to different
granularities. That is, a session with ISP = AT&T, City=Chicago
gets added to the partitions (ISP = AT&T, City=Chicago), (ISP
= AT&T), and (City=Chicago). Then, for the partitions Sa,p that
have a sufﬁcient number of data points (we use a threshold of 1000
sessions), we estimate the expected performance using the empiri-
cal mean.
The extrapolation step becomes slightly more involved. As dis-
cussed earlier, we want to use the most ﬁne-grained attribute in-
formation available, but we may not have sufﬁcient statistical con-
ﬁdence in predictions from very ﬁne-grained groups. In such cases,
we want to identify a coarser attribute combination (see Figure 7(b))
at which we have sufﬁcient data. To get a suitable coarsened gran-
ularity, we consider a logical ordering on the powerset of all at-
tribute combinations 2A such that ﬁner-granularity combinations
come earlier. That is, if we had three attributes ISP, city, and times-
tamp, then {ISP,city,timestamp} < {ISP, city} < {ISP}.6 Given
5Other possible metrics to compare two distributions can be me-
dian, or, more generally, the q-quantile of the distribution.
6Strictly speaking this is a partial order. In practice, we break the
ties arbitrarily noting that it does not affect the results signiﬁcantly.
(a) Basic extrapolation
(b) Insufﬁcient data
Figure 7: There are two user attributes: ISP and city. For each
combination of attribute values, we want to analyze the poten-
tial improvement by choosing a better CDN (a). If a combina-
tion does not have sufﬁcient data, we identify a suitable coarser
level (b). In the simplest case, the extrapolation uses the mean
of the distribution within each partition.
(a) Buffering ratio
(b) Join time
(c) Failure rate
s
s
Let a∗
Figure 8: Potential improvement in three quality metrics (re-
buffering ratio, failure rate, and join time) for two providers.
this ordering, we proceed up the hierarchy from the ﬁner to coarser
partitions, until we have sufﬁcient data to make a prediction.
s ,p∗
a∗
s , p∗
a∗
s denote the value of session attributes at this point in the
hierarchy. Based on the chosen level in the hierarchy and the pa-
rameter setting (a∗
), we extrapolate the performance by draw-
ing a value from the empirical distribution PerfDist a∗
In
other words, by replacing one performance distribution with an-
other we are simulating the effect of choosing a better parameter
setting. As a further reﬁnement, we can also imagine mapping each
session into an appropriate percentile bin in the new distribution.
That is, if the current session was within the 90-95th percentile of
clients in Sa,p, then we draw from the same bin of 90-95th per-
a .
centile of the performance of Sa,p∗
Intuitively, this emulates a
policy of not switching unless the quality is likely to improve. For
brevity, we do not show the percentile-based extrapolation, noting
that this reﬁnement will magnify the potential for improvement.
4.2 Improvement Analysis
.
For this analysis, we choose two popular providers that use mul-
tiple CDNs for video delivery but do not explicitly optimize the
CDN based on observed quality or assign clients preferentially to
CDNs based on their quality. This ensures that the potential im-
provement we extrapolate by choosing a better CDN is unbiased.
Validation: We acknowledge that any such extrapolation anal-
ysis is challenging and necessarily involves simplifying assump-
City ISP City ISP CDN1 peformance CDN2 performance Extrapolated  Ideal performance Poor à Good ?	
  ISP City ?	
  ?	
  Partition with Insufficient data Extrapolate  using coarser granularity Provider1Provider20.00.51.01.52.02.53.0Bufferingratio(%)ObservedProjectedProvider1Provider20123456Jointime(s)ObservedProjectedProvider1Provider2024681012Failureratio(%)ObservedProjected365tions. That said, we tried to do a careful job in leveraging our large
dataset. One speciﬁc type of validation we perform is to ensure that
we do not spuriously predict improvements when there are none.
To this end, we create an artiﬁcial dataset by replacing the actual
CDN in each session with a different CDN chosen uniformly at ran-
dom. The idea here is that this synthetic dataset should in theory
have no scope for improvement (modulo small stochastic effects).
We run our algorithm over this synthetic dataset and conﬁrm that
our extrapolation predicts negligible (0.05%) improvement.
Average improvement: We begin by computing the average im-
provement in video quality over a one week period for the two
providers using the above extrapolation approach. Figure 8 shows
the average improvement for three video quality metrics: buffer-
ing ratio, join time, and failure rate. The result shows that for
Provider1, we see a signiﬁcant (more than 2×) decrease in the
buffering ratio from 3.0 to 1.4. Provider1 also shows signiﬁcant
potential for improvement in the failure rate (10% to 4.5%) and
the join time (5.8s to 3.6s).
In contrast, the delivery quality for
Provider2 is already very good, and thus the scope for improve-
ment there is comparatively lower in the average case. However, as
we see next, even Provider2 shows signiﬁcant improvement under
more extreme scenarios.
Improvement under stress: We expect the room for improve-
ment to be signiﬁcantly higher under more extreme scenarios; e.g.,
a particular CDN performs poorly in a particular region or has a lot
of failures. To this end, we pick speciﬁc time segments where we
observe large incidents where Provider1 and Provider2 see marked
degradation in performance. Then, we analyze the potential for im-
provement in the video quality under these more extreme scenarios
in Table 4 and Table 5 respectively. The results show a dramatic
improvement in the potential performance over fairly long dura-
tions: 10× for buffering ratio and 32× reduction in failure rate
for Provider1 and up to 100× improvement in the failure rate for
Provider2.
Main observations: To summarize, our analysis shows that better
CDN selection can show marked improvement in video delivery
quality. Speciﬁcally, we ﬁnd
• More than 2× improvement in mean buffering ratio and startup
• 10-32× improvement in the buffering ratio and failure rate over
time, and 1.6× reduction in failure rate in the average case.
extended time periods under stress.
5. TOWARD A PRACTICAL DESIGN
The previous section establishes that there is a non-trivial poten-
tial for improvement. In practice, a control plane has to also take
into account the impact of bitrate on performance, effect of CDN
load, and also rely on past estimates to predict what the future per-
formance will be. Furthermore, we had also ignored the tractability
of global optimization and the speciﬁc utility functions or policy
objectives.
In this section, we present a preliminary effort at addressing
these issues. Our goal is not to realize an “ideal” control plane;
rather, we want to establish a feasible but concrete control plane