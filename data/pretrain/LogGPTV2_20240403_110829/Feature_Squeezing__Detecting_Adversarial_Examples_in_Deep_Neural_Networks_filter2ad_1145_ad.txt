95.29%
98.22%
97.79%
73.45%
97.90%
97.35%
98.19%
97.60%
43.29%
39.75%
63.99%
99.71%
90.33%
81.42%
79.59%
76.25%
76.03%
91.78%
80.67%
Distortion
L∞
L2
L0
0.3020 5.9047 .5601
0.3020 4.7580 .5132
0.2513 4.0911 .4906
0.2778 4.6203 .5063
0.6556 2.8664 .4398
0.7342 3.2176 .4362
0.9964 4.5378 .0473
0.9964 5.1064 .0597
1.0000 4.3276 .0473
1.0000 4.5649 .0535
0.0157 0.8626 .9974
0.0078 0.3682 .9932
0.0122 0.4462 .9896
0.0143 0.5269 .9947
0.0279 0.2346 .9952
0.0340 0.2881 .7677
0.0416 0.3577 .8549
0.6500 2.1033 .0186
0.7121 2.5300 .0241
0.8960 4.9543 .0790
0.9037 5.4883 .0983
0.0078 3.0089 .9941
0.0039 1.4059 .9839
0.0059 1.3118 .8502
0.0095 1.9089 .9520
0.0269 0.7258 .9839
0.0195 0.6663 .3226
0.0310 1.0267 .5426
0.8985 6.8254 .0030
0.9200 9.0816 .0053
to run on all the seeds. We adjust the applicable parameters of
each attack to generate high-conﬁdence adversarial examples,
otherwise they would be easily rejected. This is because the
three DNN models we use achieve high conﬁdence of the
top-1 predictions on legitimate examples (see Table I; mean
conﬁdence is over 99% for MNIST, 92% for CIFAR-10, and
75% for ImageNet). In addition, all the pixel values in the
generated adversarial images are clipped and squeezed to 8-
bit-per-channel pixels so that the resulting inputs are within
the possible space of images.
We use a PC equipped with an i7-6850K 3.60GHz CPU
and 64GiB system memory as well as a GeForce GTX 1080
to conduct the experiments.
In Table II, we evaluate the adversarial examples regarding
the success rate, the run-time cost, the prediction conﬁdence
and the distance to the seed image measured by L2, L∞ and
L0 metrics. The evaluation results for all eleven attacks on
the three datasets are provided. The success rate captures the
probability an adversary achieves their goal. For untargeted
attacks, the success rate is calculated as 1 − accuracy; for
targeted attacks,
is the accuracy for the targeted class.
Table II shows that in general most attacks generate high-
conﬁdence adversarial examples against three DNN models
with a high success rate. The CW attacks often produce fewer
distortions than other attacks using the same norm objective
but are much more expensive to generate. On the other hand,
FGSM, DeepFool, and JSMA often produce low-conﬁdence
adversarial examples. We exclude the DeepFool attack from
the MNIST dataset because it generates images that appear
it
8
unrecognizable to human eyes. We do not have JSMA results
for the ImageNet dataset because the available implementation
ran out of memory on our 64GiB test machine.
In Table III we evaluate and compare how diﬀerent feature
squeezers inﬂuence the classiﬁcation accuracy of DNN models
on three image datasets for all attacks. We discuss experimental
results of each type of squeezers further below.
B. Color Depth Reduction
The resolution of a speciﬁc bit depth is deﬁned as the
number of possible values for each pixel. For example, the
resolution of 8-bit color depth is 256. Reducing the bit depth
lowers the resolution and diminishes the opportunity an adver-
sary has to ﬁnd eﬀective perturbations. Since an adversary’s
goal is to produce small and imperceptible perturbations in the
case of adversarial examples, as the resolution is reduced, such
small perturbations no longer have any impact.
MNIST. The Last column of Table III shows the binary ﬁlter
(1-bit depth reduction) barely reduces the accuracy on the
legitimate examples of MNIST (from 99.43% to 99.33% on
the test set). When comparing the model accuracy on the
adversarial examples by the original classiﬁer (the ﬁrst row
with squeezer None) to the one with the binary ﬁlter (the
second row with squeezer bit depth (1-bit)), we see the binary
ﬁlter is eﬀective on all the L2 and L∞ attacks. For example,
it improves the accuracy on CW∞ adversarial examples from
0% to 100%. Interestingly the binary ﬁlter works well even for
large L∞ distortions. This is because the binary ﬁlter squeezes
each pixel into 0 or 1 using a cutoﬀ 0.5 in the [0, 1) scale. This
means maliciously perturbing a pixel’s value by ±0.30 has no
aﬀect on those pixels whose original values fall into [0, .20)
and [.80, 1). In contrast, bit depth reduction is not eﬀective
against L0 attacks (JSMA and CW0) since these attacks make
large changes to a few pixels and can not be reversed by the
bit depth squeezer. The next section shows that the spatial
smoothing squeezers are often eﬀective against L0 attacks.
CIFAR-10 and ImageNet. Because the DNN models for
CIFAR-10 and ImageNet are more sensitive to the adversary,
adversarial examples at very low L2 and L∞ distortions can be
found. Table III includes the results of 4-bit depth and 5-bit
depth ﬁlters in mitigating the adversaries for CIFAR-10 and
ImageNet. The 5-bit depth in testing increases the accuracy
on adversarial inputs for several of the attacks (for example,
increasing accuracy from 0% to 40% for the CW2 next class
targeted attack), while almost perfectly preserving the accuracy
on legitimate data (94.55% compared with 94.84%). The more
aggressive 4-bit depth ﬁlter is more robust against adversaries.
For example, the accuracy on CW2 increases to 84%, but it
reduces the accuracy on legitimate inputs from 94.84% to
93.11%. We do not believe these results are good enough
for use as a stand-alone defense (even ignoring the risk of
adversarial adaptation), but they provide some insight why the
method is eﬀective as used in our detection framework.
C. Median Smoothing
The adversarial perturbations produced by the L0 attacks
(JSMA and CW0) are similar to salt-and-pepper noise, though
it is introduced intentionally instead of randomly. Note that
the adversarial strength of an L0 adversary limits the number
of pixels that can be manipulated, so it is not surprising that
maximizing the amount of change to each modiﬁed pixel
is typically most useful to the adversary. This is why the
smoothing squeezers are more eﬀective against these attacks
than the color depth squeezers.
MNIST. We evaluate two window sizes on the MNIST dataset
in Table III. Median smoothing is the best squeezer for all
of the L0 attacks (CW0 and JSMA). The median ﬁlter with
2 × 2 window size performs slightly worse on adversarial
examples than the one with 3 × 3 window, but
it almost
perfectly preserves the performance on the legitimate examples
(decreasing accuracy from 99.43% to 99.28%).
CIFAR-10 and ImageNet. The experiment conﬁrms the intu-
ition suggested by Figure 4a that median smoothing can eﬀec-
tively eliminate the L0-limited perturbations. Without squeez-
ing, the L0 attacks are eﬀective on CIFAR-10, resulting in 0%
accuracy for the original model (”None” row in Table III).
However, with a 2 × 2 median ﬁlter, the accuracy increases to
over 75% for all the four L0 type attacks. We observe similar
results on ImageNet, where the accuracy increases from 0%
to 85% for the CW0 attacks after median smoothing.
D. Non-local Smoothing
The image examples in Figure 4a suggest that non-local
smoothing is inferior to median smoothing in eliminating
the L0 type perturbations, but superior for smoothing the
background and preserving the object edges. This intuition
is conﬁrmed by the experimental results on CIFAR-10 and
ImageNet (because the MNIST images are hand-drawn digits
that are not conducive to ﬁnding similar patches, we do not
consider non-local smoothing on MNIST). From Table III we
learn that non-local smoothing has comparable performance in
increasing the accuracy on adversarial examples other than the
L0 type. On the other hand, it has little impact on the accuracy
on legitimate examples. For example, the 2 × 2 median ﬁlter
decreases the accuracy on the CIFAR-10 model from 94.84%
to 89.29% while the model with non-local smoothing still
achieves 91.18%. We do not apply the non-local smoothing on
MNIST images because it is diﬃculty to ﬁnd similar patches
on such images for smoothing a center patch.
E. Combining with Adversarial Training
Since our approach modiﬁes inputs rather than the model,
it is compatible with any defense technique that operates on the
model. The most successful previous defense against adversar-
ial examples is adversarial training (Section II-C). To evaluate
the eﬀectiveness of composing our feature squeezing method
with adversarial training, we combined it with the adversarial
training implemented by Cleverhans [30]. The objective is to
minimize the mean loss on the legitimate examples and the
adversarial ones generated by FGSM on the ﬂy with  = 0.3.
The model is trained in 100 epochs.
Figure 5 shows that
the bit depth reduction by itself
signiﬁcantly outperforms the adversarial training method on
MNIST in face of the FGSM adversary, but that composing
both methods produces even better results. Used by itself, the
binary ﬁlter feature squeezing outperforms adversarial training
9
TABLE III: Model accuracy with feature squeezing
Parameters FGSM BIM
9%
L∞ Attacks
CW∞
Next
0%
LL
54%
0%
92% 87% 100% 100%
55%
61% 16%
59% 14%
46%
70%
43%
-
-
-
-
L2 Attacks
CW2
Deep-
Fool
L0 Attacks
All
CW0
JSMA
Next
LL
0%
Next
0%
LL
Next
0% 27% 40% 13.00%
0%
83% 66% 0%
0% 50% 49% 62.70%
51% 35% 39% 36% 62% 56% 48.10%
51% 53% 67% 59% 82% 79% 55.30%
LL
Attacks Legitimate
99.43%
99.33%
99.28%
98.95%
1-bit
2x2
3x3
Dataset
MNIST
CIFAR-10
ImageNet
Squeezer
Name
None
Bit Depth
Median Smoothing
None
Bit Depth
Median Smoothing
Non-local Means
None
Bit Depth
Median Smoothing
Non-local Means
5-bit
4-bit
2x2
11-3-4
4-bit
5-bit
2x2
3x3
11-3-4
0%
2.27%
0%
15%
8%
0%
0% 21% 17% 20.55%
17% 13%
21% 29%
7% 10% 23% 20% 44.82%
38% 56% 84% 86% 83% 87% 83% 88% 85% 84% 76% 77.27%
76% 84% 88% 11% 11% 44% 32% 53.00%
27% 46%
2%
0%
40% 40% 47%
72% 84% 84%
0%
12%
69%
0%
19%
74%
80%
84%
0%
0%
0%
0%
0%
1%
66%
4%
5%
33%
2%
0%
22% 28%
75%
33% 41% 73%
10% 25%
11% 10%
0%
44% 84% 82% 38% 67%