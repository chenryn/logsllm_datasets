ences. We evaluate the capacity in a cross-thread scenario
(Section 6.3.2), and across cores and VMs (Section 6.3.3).
Finally, we analyze the leakage reason (Section 6.3.4).
6.3.1 Setup
The setup consists of a sender and a receiver application. In
our proof-of-concept implementation, sender and receiver are
simply time-synchronized, i.e., they rely on a common time
m
u
S
y
c
n
e
t
a
L
]
s
e
l
c
y
c
[
8.9
8.8
8.7
·106
1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0
Bit
Figure 5: Using the RDRAND covert channel to send the
bit stream 100101101001011010010110... from one CPU
core to a different physical core (Intel Core i3-1005G1).
source such as the timestamp counter. To send a ‘1’-bit, the
sender repeatedly executes the RDRAND instruction for a ﬁxed
time τ. To send a ‘0’-bit, the sender idles for τ. The receiver
measures the latency of the RDRAND instruction over a period
of τ. The latency directly corresponds to the sent bit, i.e., a
high latency is caused by a ‘1’-bit, and a low latency is caused
by a ‘0’-bit. We note that this setup is not optimal, as there
are more advanced techniques for synchronization, including
error correction [22, 64, 99]. However, our goal is to show the
feasibility and the noise-resistance of this channel, not how
far it can be optimized using better engineering.
6.3.2 Same-core Leakage
We evaluated an RDRAND-based covert channel across hy-
perthreads to estimate the maximum capacity of this chan-
nel. Note that the leakage in a cross-hyperthread channel is
boosted by port contention as well [7, 12]. Moreover, on Intel
CPUs, Intel documents that the microcode update preventing
SRBDS [77] serializes RDRAND executions on the same
core [47]. Hence, to rule out any inﬂuence of the microcode
ﬁxes, we evaluated the channel with and without the active
patches. As AMD CPUs are not susceptible to SRBDS, there
is no microcode inﬂuence to rule out. As Table 5 shows,
we veriﬁed the covert channel on all Intel microarchitectures
since at least the Ivy Bridge microarchitecture, and also on
the AMD Zen and Zen+ microarchitecture. We achieve the
best results on the newest microarchitectures, with 1000 bit/s
(0 % error) on Intel and 666.7 bit/s (2.1 % error) on AMD.
While a same-core channel is usually irrelevant, it shows the
upper bound of the leakage achievable across cores.
6.3.3 Cross-core Leakage
In addition to the expected leakage across hyperthreads, we
evaluate the channel across physical cores.
Local Environment. Figure 5 shows a cross-core transmis-
sion in a local environment. While the signal is weaker than
in the cross-hyperthread scenario, we still manage to transmit
data reliably. As shown in Table 5, the channel achieves up to
1000 bit/s with a low error rate down to 0 %.
AWS Cloud. To further evaluate the applicability of the
covert channel in a real-world scenario, we mounted it be-
1426    30th USENIX Security Symposium
USENIX Association
Table 6: Transmission and error rates of state-of-the-art cross-
core covert channels sorted by transmission speed.
Covert channel (Element)
Liu et al. [60] (L3)
Pessl et al. [75] (DRAM)
Maurice et al. [64] (L3)
Evtyushkin et al. [22] (RDSEED)
Ragab et al. [77] (CPUID)
Ours (RDRAND)
Maurice et al. [63] (L3)
Wu et al. [99] (memory bus)
Semal et al. [85] (memory bus)
Schwarz et al. [83] (DRAM)
Speed Error rate
1.00 %
4.11 %
0.00 %
0.00 %
5.00 %
0.00 %
5.70 %
0.09 %
5.46 %
0.00 %
600 kbit/s
411 kbit/s
362 kbit/s
71 kbit/s
24 kbit/s
1000 bit/s
751 bit/s
747 bit/s
480 bit/s
11 bit/s
tween two virtual machines running in the AWS cloud. To
ensure that we do not interfere with other users, we used a
dedicated C3 host with an Intel Xeon E5-2666 v3. We were
able to transmit 95.2 bit/s across two different virtual ma-
chines running on the same CPU with an error rate of 0.88 %.
Additionally, the host had a third virtual machine running to
simulate realistic noise. For completeness, we also veriﬁed
that the covert channel works across hyperthreads and cores
inside a single virtual machine in this setup (cf. Table 5).
Comparison to Other Cross-Core Covert Channels. Ta-
ble 6 shows a comparison of the transmission speed for
state-of-the-art cross-core covert channels. While the RDRAND-
based covert channel is much slower than modern cache-based
covert channels, it has two huge advantages. First, there are
no performance counters for the hardware random number
generator. Thus, this channel cannot be easily detected or pre-
vented by current approaches relying on performance coun-
ters [19, 40, 48, 72]. We also used the open-source HexPADS
framework [72] to verify that it cannot detect the covert chan-
nel. Second, in contrast to memory-based covert channels,
this channel is agnostic to any typical system noise caused by
memory accesses on the sender core. As typical workloads
do not execute RDRAND in a high frequency, we do not see a
high impact on the transmission rate, even for high workloads.
We veriﬁed that by running the Linux tool stress for both
the CPU and the memory on the sender core does not prevent
the covert channel. Even in this scenario, with an extremely
high load of 100 % on the sibling hyperthread, we manage to
transmit 500.0 bit/s with an error rate of 7.34 %.
Furthermore, as our covert channel does not rely on the
memory subsystem, defenses proposed against cache at-
tacks [59, 76, 96, 97, 104, 105] do not prevent our channel.
Even existing partitioning features, such as Intel CAT, which
can be used to prevent cache-based cross-VM covert chan-
nels [58] do not affect the RDRAND-based covert channel.
6.3.4 Explanation for RDRAND Side Channel
As the hardware random number generator is shared across
all cores, simultaneous use by multiple cores leads to con-
tention. Hence, as with many cross-core covert channels [22,
63, 75, 99], the root cause is the contention of a resource
shared across cores, such as the L3 cache or the memory bus.
However, in contrast to previous covert channels, we could not
identify any performance counters related to RDRAND. While
this makes the analysis more difﬁcult, it also increases the
stealthiness of the channel, as it cannot be detected easily.
While previous work showed that the RDSEED instruc-
tion can exhaust the hardware random-number generator
(RNG) [22], the RDRAND instruction has not been analyzed
for side-channel leakage. Moreover, Evtyushkin et al. [22]
only exploited an architectural value, i.e., a cleared carry ﬂag,
indicating that the RNG is exhausted, and not differences in
the execution time. At ﬁrst glance, it might seem obvious that
RDRAND also suffers from exhaustion as it fundamentally relies
on the RDSEED instruction. RDSEED is quickly exhausted, as it
provides the randomness directly from the hardware element.
However, Evtyushkin et al. [22] observed that RDRAND pro-
vides the numbers from a pseudo-RNG and can thus provide
continuous streams of numbers. We conﬁrm that the RDRAND-
based leakage is not due to exhaustion. While measuring the
timing differences, the instruction does not indicate that the
RNG is exhausted, i.e., the carry ﬂag was always set [44].
We additionally ruled out the microcode updates preventing
CrossTalk [77] as a cause for the timing differences. While
these updates reduce the bandwidth of RDRAND across hyper-
threads due to serialization, they do not affect the cross-core
behavior [47]. We veriﬁed that by successfully mounting the
covert channel with and without the microcode update, and
also by disabling the mitigation on patched systems via the
IA32_MCU_OPT_CTRL model-speciﬁc register.
7 Discussion
With Osiris, we present a generic approach for detecting
timing-based side channels. Our current prototype still has
several limitations preventing it from ﬁnding even more side
channels. However, these are not conceptual limitations. It
would merely require a lot more engineering to solve them.
In the current version, we only consider side channels where
the timing difference is around 100 cycles. Any side chan-
nel with a smaller timing difference, e.g., Flush+Flush [35],
CacheBleed [102] or the AMD way predictor [56], is cur-
rently not reported. One practical reason is that Osiris runs
on a commodity Linux system, where it is tough to elimi-
nate all inﬂuences on the measurement. Even when isolating
cores, several microarchitectural elements are shared across
all cores, there are still remaining interrupts, and the power
management of the CPU can change the CPU frequency, e.g.,
for thermal reasons. Hence, to reliably detect small timing
USENIX Association
30th USENIX Security Symposium    1427
differences, Osiris would have to run on a custom operating
system designed for microarchitectural research, such as Sushi
Roll [26]. In line with related work [27, 30], our prototype
only considers sequences consisting of one instruction. As
a consequence, eviction-based side channels such as Evict+
Reload, Evict+Time, Prime+Probe, or Reload+Refresh are
not detected. However, related work [34, 94, 95] showed that
eviction strategies can also be found automatically. Moreover,
for speciﬁc problems, the search space can be reduced by mu-
tating existing instruction sequences (similar to Medusa [65])
or instruction operands instead of randomly generating them.
Therefore, Osiris can be augmented by these techniques to
also ﬁnd eviction-based side channels and support multi-
instruction sequences (e.g., fault suppression). Furthermore,
using performance counters, power (RAPL), and debug in-
terfaces (Intel VISA/ITP-XDP) as feedback mechanisms, the
fuzzer could monitor resource usage and microarchitectural
conﬂicts to guide the sequence generation process. This would
allow ﬁnding eviction-based channels: (i) Start with multiple
loads as a reset sequence, (ii) Mutate the loaded addresses
while maximizing (guidance) the cache miss count until a
time difference is detected.
Still, despite these current limitations of the prototype,
Osiris discovered novel timing-based side channels within
hours of runtime. These side channels led to the discovery of a
new microarchitectural KASLR break, a previously unknown
cross-VM covert channel, and an improvement for transient-
execution attacks. Hence, we argue that Osiris is a useful tool
for automating the search for timing-based side channels that
can also be used by CPU vendors to detect such side channels
introduced by new ISA extensions automatically.
Also, Osiris can be extended to other architectures, e.g.,
ARMv8, with relative ease. To this end, the main parts that
need to be adapted are the code generation stage, particularly
the ofﬂine phase to construct possible instruction variants,
and the execution stage. The current implementation of Osiris
uses inlined instructions to measure the execution time, which
would need to be changed for the target architecture (see
Section 4). However, this task can be simpliﬁed by reﬁning
the current approach to use other timing primitives [55].
8 Conclusion
Our ﬁndings illustrate that prior side channels targeted only a
subset of many micro-architectural changes. We show several
additional, undocumented instruction side effects that attack-
ers can leverage for security-critical side channels. This has se-
vere implications to existing and future side-channel defenses,
as each of them is based on a speciﬁc threat model that frames
(known) attack capabilities. We, therefore, see our proposed
fuzzing-based technique as the ﬁrst systematic, generic, and
automated attempt to fast-forward the arms race of detecting
(and then, ultimately, defending against) such side channels.
The newly discovered side channels and their application to
three use cases raise our conﬁdence that Osiris can indeed
support this endeavor. When used during the CPU design
stage, Osiris helps to eliminate—or at least to document—
side channels early on. For this reason, we released Osiris as
an open-source tool.
Acknowledgments
We thank the anonymous reviewers and our shepherd, Math-
ias Payer, for their helpful comments and suggestions that
substantially helped in improving the paper, as well as Moritz
Lipp (Graz University of Technology) for feedback on an
earlier version of the paper. Furthermore, we thank the Saar-
brücken Graduate School of Computer Science for their fund-
ing and support for Daniel Weber. This work partially was
supported by grant from the German Federal Ministry of Edu-
cation and Research (BMBF) through funding for the CISPA-
Stanford Center for Cybersecurity (FKZ:13N1S0762).
References
[1] Andreas Abel and Jan Reineke. uops.info: Charac-
terizing Latency, Throughput, and Port Usage of In-
In ASPLOS,
structions on Intel Microarchitectures.
2019.
[2] Accardi, Kristen Carlson. Function Granular KASLR,
URL: https://patchwork.kernel.org/
2020.
project/kernel-hardening/list/?series=
354389.
[3] Onur Acıiçmez, Shay Gueron, and Jean-pierre Seifert.
New Branch Prediction Vulnerabilities in OpenSSL
In Pro-
and Necessary Software Countermeasures.
ceedings of the 11th IMA International Conference on
Cryptography and Coding, 2007.
[4] Onur Acıiçmez, Çetin Kaya Koç, and Jean-pierre
Seifert. On the Power of Simple Branch Prediction
Analysis. In AsiaCCS, 2007.
[5] Onur Acıiçmez and Werner Schindler. A Vulnerability
in RSA Implementations Due to Instruction Cache
Analysis and Its Demonstration on OpenSSL. In CT-
RSA 2008. 2008.
[6] Onur Acıiçmez, Jean-Pierre Seifert, and Çetin Kaya
Koç. Predicting secret keys via branch prediction. In
CT-RSA, 2007.
[7] Alejandro Cabrera Aldaya, Billy Bob Brumley, Sohaib
ul Hassan, Cesar Pereida García, and Nicola Tuveri.
Port Contention for Fun and Proﬁt. In S&P, 2018.
[8] Arm. A-Proﬁle Exploration tools, 2017. URL: https:
//developer.arm.com/architectures/cpu-
architecture/a-profile/exploration-tools.
1428    30th USENIX Security Symposium
USENIX Association
[9] Cornelius Aschermann, Sergej Schumilo, Tim
Blazytko, Robert Gawlik, and Thorsten Holz.
REDQUEEN: fuzzing with input-to-state correspon-
dence. In NDSS, 2019.
[10] Sarani Bhattacharya, Chester Rebeiro, and Debdeep
Mukhopadhyay. Hardware prefetchers leak: A revisit
of SVF for cache-timing attacks. In MICRO, 2012.
[11] Atri Bhattacharyya, Andrés Sánchez, Esmaeil M. Ko-
ruyeh, Nael Abu-Ghazaleh, Chengyu Song, and Math-
ias Payer. Specrop: Speculative exploitation of ROP
chains. In RAID, San Sebastian, 2020.
[12] Atri Bhattacharyya, Alexandra Sandulescu, Matthias
Neugschwandt ner, Alessandro Sorniotti, Babak Fal-
saﬁ, Mathias Payer, and Anil Kurmus. SMoTherSpec-
tre: exploiting speculative execution through port con-
tention. In CCS, 2019.
[13] Tim Blazytko, Cornelius Aschermann, Moritz
Schlögel, Ali Abbasi, Sergej Schumilo, Simon Wörner,
and Thorsten Holz. GRIMOIRE: Synthesizing struc-
ture while fuzzing. In USENIX Security Symposium,
2019.
[14] Samira Briongos, Pedro Malagón, José M Moya, and
Thomas Eisenbarth. RELOAD+REFRESH: Abusing
Cache Replacement Policies to Perform Stealthy Cache
Attacks. In USENIX Security Symposium, 2020.
[15] Claudio Canella, Daniel Genkin, Lukas Giner, Daniel
Gruss, Moritz Lipp, Marina Minkin, Daniel Moghimi,
Frank Piessens, Michael Schwarz, Berk Sunar,
Jo Van Bulck, and Yuval Yarom. Fallout: Leaking
Data on Meltdown-resistant CPUs. In CCS, 2019.
[16] Claudio Canella, Michael Schwarz, Martin Hauben-
wallner, Martin Schwarzl, and Daniel Gruss. KASLR:
Break It, Fix It, Repeat. In AsiaCCS, 2020.
[17] Claudio Canella, Jo Van Bulck, Michael Schwarz,
Moritz Lipp, Benjamin von Berg, Philipp Ortner, Frank
Piessens, Dmitry Evtyushkin, and Daniel Gruss. A
Systematic Evaluation of Transient Execution At-
In USENIX Security Sympo-
tacks and Defenses.
sium, 2019. Extended classiﬁcation tree and PoCs
at https://transient.fail/.
[18] Peng Chen and Hao Chen. Angora: Efﬁcient fuzzing
by principled search. In IEEE S&P, 2018.
[19] Marco Chiappetta, Erkay Savas, and Cemal Yilmaz.
Real time detection of cache-based side-channel at-
tacks using hardware performance counters. ePrint