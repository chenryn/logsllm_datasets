of the generated reports, and adjust the built-in options of
IntFlow to the exact characteristics of their source code.
5. EVALUATION
In this section, we present the results of our experimental
evaluation using our prototype implementation of IntFlow.
To assess the eﬀectiveness and performance of IntFlow, we
look into the following aspects:
• What is the accuracy of IntFlow in detecting and pre-
venting critical arithmetic errors?
• How eﬀective is IntFlow in reducing false positives?
That is, how good is it in omitting developer-intended
violations from the reported results?
• When used as a protection mechanism, what is the
runtime overhead of IntFlow compared to native exe-
cution?
Our ﬁrst set of experiments aims to evaluate the tool’s
ability to identify and mitigate critical errors. For this pur-
pose, we use two datasets consisting of artiﬁcial and real-
world vulnerabilities. Artiﬁcial vulnerabilities were inserted
to a set of real-world applications, corresponding to var-
ious types of MITRE’s Common Weakness Enumeration
(CWE) [4]. This dataset provides a broad test suite that
contains instances of many diﬀerent types of arithmetic er-
rors, which enables us to evaluate IntFlow in a well-controlled
environment, knowing exactly how many bugs have been in-
serted, as well as the nature of each bug. Likewise, our
real-world vulnerability dataset consists of applications such
as image and document processing tools, instant messaging
clients, and web browsers, with known CVEs, allowing us
to get some insight on how well IntFlow performs against
real-world, exploitable bugs.
In our second round of experiments, we evaluate the ef-
fectiveness of IntFlow’s information ﬂow tracking analysis
in reducing false positives, by running IntFlow on the SPEC
CPU2000 benchmark suite and comparing its reported er-
rors with those of IOC. IOC instruments all arithmetic op-
erations, providing the ﬁnest possible granularity for checks.
Thus, by comparing the reports produced by IntFlow and
IOC, we obtain a base case for how many non-critical errors
are correctly ignored by the IFT engine.
Finally, to obtain an estimate of the tool’s runtime over-
head, we run IntFlow over a diverse set of applications of
varying complexity, and establish a set of performance bounds
for diﬀerent types of binaries. All experiments were per-
formed on a system with the following characteristics: 2×
Intel(R) Xeon(R) X5550 CPU @ 2.67GHz, 2GB RAM, i386
Linux.
CWEs
Applications
Cherokee 1.2.101 CWE-190
Grep 2.14
CWE-191
CWE-194
Nginx 1.2.3
CWE-195
Tcpdump 4.3.0
CWE-196
W3C 5.4.0
Wget 1.14
CWE-197
CWE-369
Zshell 5.0.0
CWE-682
CWE-839
Table 2: Summary of the applications and CWEs used in
the artiﬁcial vulnerabilities evaluation.
5.1 Accuracy
In all experiments for evaluating accuracy, we conﬁgured
IntFlow to operate in whitelisting mode, since this mode
produces the greatest number of false positives, as it pre-
serves most of the IOC checks among the three modes. Thus,
whitelisting provides us with an estimation of the worst-case
performance of IntFlow, since the other two modes perform
more ﬁne-tuned instrumentation.
5.1.1 Evaluation Using Artiﬁcial Vulnerabilities
To evaluate the eﬀectiveness of IntFlow in detecting crit-
ical errors of diﬀerent types, we used seven popular open-
source applications with planted vulnerabilities from nine
distinct CWE categories.3 Table 2 provides a summary of
the applications used and the respective CWEs.
Each application is replicated to create a set of test-case
binaries. In every test-case binary—essentially an instance
of the real-world application—a vulnerability is planted and
then the application is compiled with IntFlow. Subsequently,
each test-case binary is executed over a set of benign and
malicious inputs (inputs that exploit the vulnerability and
result in abnormal behavior). A correct execution is ob-
served when the binary executes normally on benign inputs
or terminates before it can be exploited on malicious inputs.
Overall, IntFlow was able to correctly identify 79.30%
(429 out of 541) of the planted artiﬁcial vulnerabilities. The
20.7% missed are due to the accuracy limitations of the IFT
mechanism, which impacts the ability of IntFlow to correctly
identify ﬂows, and also due to vulnerabilities triggered by
implicit information ﬂows (i.e., non-explicit data ﬂows real-
ized by alternate control paths), which our IFT implemen-
tation (llvm-deps) is not designed to capture. We discuss
ways in which accuracy can be further improved in Section 7.
5.1.2 Mitigation of Real-world Vulnerabilities
In our next experiment, we examined the eﬀectiveness of
IntFlow in detecting and reporting real-world vulnerabili-
ties. For this purpose, we used four widely-used applica-
tions and analyzed whether IntFlow detects known integer-
related CVEs included in these programs. Table 3 summa-
rizes our evaluation results. IntFlow successfully detected all
the exploitable vulnerabilities under examination. From this
small-scale experiment, we gain conﬁdence that IntFlow’s
3The modiﬁed applications for this experiment were pro-
vided by MITRE for testing the detection of integer error
vulnerabilities, as part of the evaluation of a research pro-
totype [6].
Program CVE Number
CVE-2009-3481
Dillo
CVE-2012-3481
GIMP
Swftools CVE-2010-1516
Pidgin
CVE-2013-6489
Detected?
Type
Integer Overﬂow Yes
Integer Overﬂow Yes
Integer Overﬂow Yes
Signedness Error Yes
Table 3: CVEs examined by IntFlow.
Figure 3: Number of critical and developer-intended arith-
metic errors reported by IOC and IntFlow for the SPEC
CPU2000 benchmarks. IntFlow identiﬁes the same number
of critical errors (dark sub-bars), while it reduces signiﬁ-
cantly the number of reported developer-intended violations.
characteristics are maintained when the tool is applied to
real world programs, and therefore it is suitable as a detec-
tion tool for real-world applications.
5.1.3 False Positives Reduction
Reducing the number of false positives is a major goal
of IntFlow, and this section focuses on quantifying how ef-
fective this reduction is. For our ﬁrst measurement we used
SPEC CPU2000, a suite that contains C and C++ programs
representative of real-world applications. Since IOC is a core
component of IntFlow, we chose to examine the same subset
of benchmarks of CPU2000 that was used for the evaluation
of IOC to measure the improvements of IntFlow’s IFT in
comparison to previously reported results [10]. We ran the
SPEC benchmarks using the “test” data sets for both IOC
and IntFlow, so that we could manually analyze all the re-
ports produced by IOC and classify them as true or false
positives, Once all reports were categorized based on Deﬁ-
nition 1, we examined the respective results of IntFlow. We
report our ﬁndings in Figure 3.
IntFlow was able to correctly identify all the critical errors
(64 out of 64) triggered during execution, and reduced the
reports of developer-intended violations by ∼89%.
5.1.4 Real-world Applications
In Section 5.1.2 we demonstrated how IntFlow eﬀectively
detected known CVEs for a set of real-world applications.
Here, we examine the reduction in false positives achieved
when using IntFlow’s core engine instead of static instru-
mentation with IOC alone. To collect error reports, we ran
IOC
IntFlow 82
31
26
Overall Dillo Gimp Pidgin
330
231
13
0
0
SWFTools
68
43
Table 4: Number of False Positives reported by IOC and
IntFlow for the real-world programs of Section 5.1.2.
each application with benign inputs as follows:
for Gimp,
we scaled the ACSAC logo and exported it as GIF; for
SWFTools, we used the pdf2swf utility with a popular e-
book as input; for Dillo, we visited the ACSAC webpage and
downloaded the list of notable items published in 2013; and
for Pidgin, we performed various common tasks, such as reg-
istering a new account and logging in and out. Table 4 shows
the reports generated by IOC and IntFlow, respectively.
Overall, IntFlow was able to suppress 75% of the errors
reported by IOC during the execution of the applications
on benign inputs. Although this evaluation does not pro-
vide full coverage on the number of generated reports (for
instance, we did not observe any false positives for pidgin
with the tests we performed), it allows us to obtain an esti-
mate of how well IntFlow performs in real world scenarios.
As the last part of our false positive reduction, we exer-
cised vanilla versions of each application used in Section 5.1.1
over sets of benign inputs and examined the output. Since
those inputs produce the expected output, we assume that
all reported violations are developer-intended either explic-
itly or implicitly. With this in mind, we compared the error
checks of IntFlow with those of IOC to quantify the ability of
IntFlow in removing unnecessary checks. Overall, IntFlow
eliminated 90% of the false checks (583 out of 647) when
tested with the default set of safe inputs. This reduction
was achieved due to the successful identiﬁcation of constant
assignments and the whitelisting of secure system calls, as
discussed in Section 3.
It should be noted that the eﬀectiveness in the reduc-
tion of false positives is highly dependent on the nature of
each application, as well as on the level of the execution’s
source coverage. That is, the more integer operations occur
throughout the execution, the greater the expected num-
ber of false positives. For instance, Gimp’s functionality
is tightly bound to performing arithmetic operations for a
number of image processing actions, and thus IOC reports
many errors, most of which are developer-intended, while
Dillo does not share the same characteristics and as a result
exhibits a smaller reduction in false positives.
5.2 Runtime Overhead
Although IntFlow was not designed as a runtime detection
tool but rather as an oﬄine integer error detection mecha-
nism, one may wonder whether it could be customized to
oﬀer runtime detection capabilities. In this section, we seek
to examine the performance of IntFlow for various applica-
tions, when running them with all the automatically inserted
arithmetic error checks. For this purpose, we perform a set
of timing measurements on the applications used in Sec-
tion 5.1.1. For each run, we measured the time that was
required to complete a series of tasks for each of IntFlow’s
modes of operation, and then normalized the running time
with respect to the runtime of the native binary. Reported
results are mean values over ten repetitions of each experi-
ment, while the reported conﬁdence intervals correspond to
gzipvprgcccraftyparserperlbmkgapvortexNumber of Reported Arithmetic Errors010203040100150200250IOC IntendedIOC CriticalIntFlow IntendedIntFlow Critical6.1 Static Analysis
Static analysis tools provide good coverage but generally
suﬀer from a high rate of false positives.
IntPatch [25] is
built on top of LLVM [14] and detects vulnerabilities utiliz-
ing the type inference of LLVM IR. Similarly to our tool, Int-
Patch uses forward & backward analysis to classify sources
and sinks as sensitive or benign. Each sensitive variable is
located through slicing. If a variable involved in an arith-
metic operation has an untrusted source and the respective
sink may overﬂow, IntPatch will insert a check statement
after that vulnerable arithmetic operation. If an overﬂow re-
sult is used for sensitive actions such as memory allocations,
IntPatch considers it a real vulnerability. Contrary to the
current work though, IntPatch does not deal with all types
of integer overﬂows and also does not address programmer-
inserted sanitization routines.
KINT [23] is a static tool that generates constraints rep-
resenting the conditions under which an integer overﬂow
may occur. It operates on LLVM IR and deﬁnes untrusted
sources and sensitive sinks via user annotations. KINT
avoids path explosion by performing constraint solving at
the function level and by statically feeding the generated
constraints into a solver. After this stage, a single path con-
straint for all integer operations is generated. Unfortunately,
despite the optimization induced by the aforesaid technique,
the tool’s false positives remain high and there is a need for
ﬂagging false positives with manual annotations in order to
suppress them. Moreover, contrary to this work, KINT at-
tempts to denote all integer errors in a program and does
not make a clean distinction between classic errors and er-
rors that constitute vulnerabilities.
SIFT [16] uses static analysis to generate input ﬁlters
against integer overﬂows. If an input passes through such
ﬁlter, it is guaranteed not to generate an overﬂow. Initially,
the tool creates a set of critical expressions from each mem-
ory allocation and block copy site. These expressions con-
tain information on the size of blocks being copied or al-
located, and are propagated backwards against the control
ﬂow, generating a symbolic condition that captures all the
points involved with the evaluation of each expression. The
free variables in the generated symbolic conditions represent
the values of the input ﬁelds and are compared against the
tool’s input ﬁlters. A signiﬁcant diﬀerence of this paper in
comparison to SIFT is that the latter nulliﬁes overﬂow er-
rors but does not detect them nor examines whether they
could be exploitable.
IntScope [21] decompiles binary programs into IR and
then checks lazily for harmful integer overﬂow points. To
deal with false positives, IntScope relies on a dynamic vul-
nerability test case generation tool to generate test cases
which are likely to cause integer overﬂows. If no test case
generates such error, the respective code fragment is ﬂagged
appropriately. This approach varies signiﬁcantly from the
one used in our tool as it relies on the produced test cases
to reveal a true positive: if a test case does not generate an
overﬂow, that does not guarantee that no overﬂow occurs.
In addition, IntScope regards all errors as generic, without
focusing particularly on errors leading to vulnerabilities.