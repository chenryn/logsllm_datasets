90.00%
88.00%
86.00%
84.00%
82.00%
80.00%
90.74% 
88.74% 
85.17% 
83.93% 
Failstop 
Register 
Code 
Fig. 2. Successful recovery rates of NiLiHype and ReHype for different fault
types with the 3AppVM setup. Error bars show the 95% conﬁdence intervals.
noVMF stands for no AppVM failure cases.
is triggered. Obviously, the recovery mechanism is triggered
only for fault outcomes in the last category.
The breakdown of injection outcomes varies with fault
type. Obviously, all Failstop faults are detected. For Regis-
ter faults, the breakdown in our campaign is: 74.8% non-
manifested, 5.6% SDC, and 19.6% detected. For Code faults,
the breakdown is: 35.0% non-manifested, 12.1% SDC, and
52.9% detected.
The fault injection campaigns for evaluating the recovery
rate include 1000 Failstop faults, 5000 Register faults, and
2000 Code faults. In each case, the number of the injected
faults was chosen so that, for both NiLiHype and ReHype, the
95% conﬁdence interval for the recovery rate was within ±2%.
Figure 2 presents the successful recovery rate of NiLiHype
and ReHype with the 3AppVM setup. For the completeness,
we also report the portion of detected errors (recovery initia-
tions) that resulted in no AppVM failures (noVMF — no VM
failures). The differences between Success and noVMF are due
to injection runs where the only impact of the fault is to cause
one of the ﬁrst two initial AppVMs to fail.
Figure 2 shows that NiLiHype and ReHype achieve es-
sentially identical recovery rates for Failstop faults, but not
for the other fault types. Failstop faults can only result in
inconsistencies within the hypervisor state or between the
hypervisor state and the states of other hardware and software
components. The other fault types can result in the same
inconsistencies but also in state corruption. It is likely that,
for Register and Code faults, ReHype’s small advantage is
due to cases where the fault corrupted part of the hypervisor
state that is discarded and re-initialized by the reboot.
To understand the reasons for recovery failures, we analyzed
the recovery failure cases when Register faults are injected.
ReHype resulted in 35 such cases and NiLiHype in 54. The
reasons for recovery failures with ReHype and NiLiHype are
similar. The top three reasons are: 1) the recovery routine fails
122
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
RECOVERY LATENCY BREAKDOWN OF REHYPE
TABLE II
Operations
Hardware initialization:
- Early initialize of the boot CPU
- Initialize and wait for other CPUs to come online
- Verify, connect and setup local APIC and setup IO ACPI
- Initialize and calibrate TSC timer
Memory initialization
- Record allocated pages of old heap (Use to preserve content
of old heap)
- Restore and check consistency of page frame entries
- Re-initialize the page frame descriptor for un-preserved
pages
- Recreate the new heap
Misc
- SMP initialization
- Identify valid page frame, relocate boot up modules
- Others
Total
RECOVERY LATENCY BREAKDOWN OF NILIHYPE
TABLE III
Operations
- Restore and check consistency of page frame entries
- Others
Total
Time
412ms
12ms
150ms
200ms
50ms
266ms
21ms
21ms
13ms
211ms
35ms
20ms
2ms
13ms
713ms
Time
21ms
1ms
22ms
to be invoked due to the corrupted hypervisor state, 2) the
PrivVM fails, and 3) the error causes a data structure in the
hypervisor, typically a linked list or the heap, to be corrupted
or left in an inconsistent state.
Figure 2 shows that Code faults result in the lowest recovery
rate. This is likely due to the signiﬁcantly longer detection
latency of these faults [22], providing more time for errors to
propagate and cause greater state corruption.
B. Recovery Latency
When hypervisor recovery is in progress, all the AppVMs
are paused. Hence, it is easy to measure the recovery latency
by measuring the service interruption of a service executing
in an AppVM in the target system. Latency measurements
may be distorted when the system is deployed in a nested
virtualization conﬁguration. Hence, in order to obtain accurate
latency results, the target system runs on bare hardware. As a
service, we use NetBench in the 1AppVM setup. The service
interruption is measured at the sender, that runs on a separate
physical host (Subsection VI-A).
For NiLiHype, we measured a recovery latency of 22ms. For
ReHype, with all the recovery latency optimizations discussed
in [21], we measured a recovery latency of 713ms. Repeating
each experiment ﬁve times, the latencies varied by no more
than 1ms for NiLiHype and 10ms for ReHype.
To determine how much time the different operations in-
volved in recovery contribute to the overall recovery latency,
we added code in the recovery code of NiLiHype and ReHype
to record the value of the time stamp counter (TSC) after each
major recovery step is completed. The results for ReHype and
NiLiHype are shown in Table II and Table III, respectively.
These tables list every step that takes more than 1ms.
Most of NiLiHype’s recovery latency is due to an operation
done to ensure consistency of the page frame descriptors. This
is an operation that ReHype also performs [19], [21]. The
problem that this operation solves is that, following recovery,
there are two components in each page frame descriptor that
may be left in inconsistent states: the validation bit and the
page use counter. This can cause the hypervisor to hang
following recovery. To ﬁx this issue, the recovery routine
iterates over all the page frame descriptors in the hypervisor to
check for the inconsistency and update as required to restore
consistency.
The latency of the operation described above is proportional
to the size of the host memory (and thus the number of page
frame descriptors). In our system, 8GB of physical memory
results in a latency of 21ms. Obviously,
this would be a
problem in a large system with tens or hundreds of GB
of memory. The problem could be mitigated by exploiting
parallelism. For example, use multiple cores to perform the
operation. Another option is to not perform this recovery step.
This option has the disadvantage that it results in a reduction
of 4% in the recovery rate [19].
C. Hypervisor Processing Overhead in Normal Operation
A key question regarding any resilience mechanism is how
much performance overhead during normal operation it incurs.
With NiLiHype, this translates to the extent to which, for
a ﬁxed workload, the count of CPU cycles spent executing
hypervisor code is higher with the NiLiHype modiﬁcations
compared to with stock Xen.
To get accurate results, the hypervisor processing overhead
is measured with the target system running on bare hardware.
In each one of the CPUs (including the one running the
PrivVM), a hardware performance counter is used to count
the unhalted cycles spent in the hypervisor. For repeatable
results, the precise points in time for starting and ending
the measurement are carefully controlled. All the benchmarks
running in all the AppVMs are synchronized. Measurement
starts when all the benchmarks are ready to begin executing.
Measurement ends when all the benchmarks complete. The
benchmarks “inform” the measurement code when they begin
and end using traps (the CPUID instruction). All the bench-
marks execute for approximately the same amount of time
(21s).
We deﬁne the hypervisor processing overhead as the percent
increase in the unhalted cycle count in the hypervisor with
NiLiHype relative to that same count with stock Xen.
We used four target system conﬁgurations. The ﬁrst three
use the 1AppVM setup with the three benchmarks: BlkBench,
123
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
20.00%
18.00%
16.00%
14.00%
12.00%
10.00%
8.00%
6.00%
4.00%
2.00%
0.00%
18.40% 
NiLiHype* 
NiLiHype 
IMPLEMENTATION COMPLEXITY OF NILIHYPE AND REHYPE
TABLE IV
13.27% 
14.46% 
9.97% 
3.53% 
0.70% 
1.33% 
0.00% 
UnixBench
BlkBench
NetBench
3AppVM
Type
Normal
operation
Recovery
routine
Total
Mechanisms
NiLiHype
ReHype
Mitigating hypercall retry
failure
Other logging
Shared recovery mechanism
Speciﬁc recovery mechanism
Total
991
532
543
113
2179
991
594
543
642
2770
Fig. 3. Hypervisor processing overhead (based on CPU cycles) of NiLiHype
during normal execution. NiLiHype* stands for NiLiHype without logging to
mitigate hypercall retry failure.
UnixBench, and NetBench. Obviously, for these three conﬁgu-
rations, synchronizing the execution of the benchmarks is not
relevant. The fourth conﬁguration is a slightly modiﬁed version
of the 3AppVM setup. Since recovery is not actually done
in these measurements, all three of the AppVMs are created
at the same time and they all run their benchmarks through-
out the experiment. For each conﬁguration, we repeated the
measurements ﬁve times and found that the differences in the
measurement results were all less than 1%.
Figure 3 shows the hypervisor processing overhead for
NiLiHype for all the conﬁgurations. Most of this overhead
is due to logging used to mitigate recovery failures due to
retries of non-idempotent hypercalls. To show that, the ﬁgure
also shows the overhead of NiLiHype without this logging
(NiLiHype*).
We have also measured the hypervisor processing overhead
of ReHype and found it to be the same as for NiLiHype. This
is not surprising since the logging in NiLiHype and ReHype
are almost identical,
As mentioned earlier (Subsection VII-A), it has been shown
that, for typical deployments of virtualization, less than 5%
of CPU cycles are spent executing hypervisor code [6], [4].
Hence, even with logging, the actual impact of the hypervisor
processing overhead is negligible. Speciﬁcally, even in the
worst case (BlkBench), the overhead in terms of total CPU
cycles can be expected to be less than 1%. If this overhead is
not acceptable, there is the option of turning the logging off.
As discussed in Section IV, this will reduce the recovery rate
by approximately 12%.
D. Implementation Complexity
As a measure of the implementation complexity of NiLi-
Hype and ReHype, we use the total number of lines of code
(LOC) added and modiﬁed starting with the source code of
the stock Xen hypervisor. We use the code line count tool
CLOC [1] to measure the LOC.
We partition the LOC added and modiﬁed into two cate-
gories: (1) code that executes during normal operation to en-
able or enhance NiLiHype/ReHype functionality, and (2) code
both NiLiHype
and ReHype, most
of
that executes only during recovery. Table IV presents the
results.
For
the
added/modiﬁed code in category (1) is related to mitigating
hypercall retry failure (Section IV). For all
the code in
category (1), NiLiHype requires slightly less code. This
is due to two types of logging that are not needed for
NiLiHype. Firstly, ReHype needs to log changes to the
I/O APIC registers during the normal execution. This is
because ReHype recovery re-initializes these registers as
part of the boot process. However, for recovery to succeed,
these registers must be restored to their pre-recovery values.
Secondly, ReHype needs to log the values of the boot line