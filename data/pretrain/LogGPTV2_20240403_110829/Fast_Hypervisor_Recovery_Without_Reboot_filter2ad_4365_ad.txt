### Fault Injection and Recovery Analysis

#### Recovery Rates for Different Fault Types

The successful recovery rates of NiLiHype and ReHype for different fault types, using the 3AppVM setup, are presented in Figure 2. The error bars represent 95% confidence intervals. The term "noVMF" indicates cases where no AppVM failures occurred.

- **Failstop Faults:**
  - Both NiLiHype and ReHype achieve nearly identical recovery rates.
  - Failstop faults primarily cause inconsistencies within the hypervisor state or between the hypervisor state and other hardware and software components.

- **Register Faults:**
  - Breakdown: 74.8% non-manifested, 5.6% SDC (Silent Data Corruption), and 19.6% detected.
  - ReHype has a slight advantage, possibly due to its ability to discard and re-initialize corrupted hypervisor states during reboot.

- **Code Faults:**
  - Breakdown: 35.0% non-manifested, 12.1% SDC, and 52.9% detected.
  - Code faults result in the lowest recovery rate, likely due to their longer detection latency, which allows more time for errors to propagate and cause greater state corruption.

#### Fault Injection Campaign Details

- **Failstop Faults:** 1000 injections
- **Register Faults:** 5000 injections
- **Code Faults:** 2000 injections

The number of injected faults was chosen to ensure that the 95% confidence interval for the recovery rate was within ±2%.

#### Analysis of Recovery Failures

For Register faults, ReHype resulted in 35 recovery failure cases, while NiLiHype had 54. The top three reasons for recovery failures were:
1. Corrupted hypervisor state preventing the recovery routine from being invoked.
2. PrivVM failure.
3. Corruption or inconsistency in hypervisor data structures, such as linked lists or the heap.

#### Recovery Latency

Recovery latency is measured by the service interruption of a service executing in an AppVM. The target system runs on bare hardware to avoid nested virtualization distortions.

- **NiLiHype:** 22ms
- **ReHype:** 713ms

The latencies varied by no more than 1ms for NiLiHype and 10ms for ReHype across five repeated experiments.

##### Breakdown of Recovery Latency

**Table II: ReHype Recovery Latency Breakdown**

| Operations | Time (ms) |
|------------|-----------|
| Hardware initialization | 412 |
| Memory initialization | 150 |
| Misc | 21 |
| Total | 713 |

**Table III: NiLiHype Recovery Latency Breakdown**

| Operations | Time (ms) |
|------------|-----------|
| Restore and check consistency of page frame entries | 21 |
| Others | 2 |
| Total | 22 |

Most of NiLiHype’s recovery latency is due to ensuring the consistency of page frame descriptors. This operation is also performed by ReHype but with higher latency. The latency is proportional to the size of the host memory, and it can be mitigated by exploiting parallelism or skipping the step, though the latter reduces the recovery rate by 4%.

#### Hypervisor Processing Overhead in Normal Operation

The overhead is measured by the increase in unhalted cycle count in the hypervisor with NiLiHype compared to stock Xen.

- **Target System Configurations:**
  - 1AppVM with BlkBench, UnixBench, and NetBench
  - Modified 3AppVM setup

**Figure 3: Hypervisor Processing Overhead (Based on CPU Cycles)**

- Most of the overhead is due to logging used to mitigate recovery failures from hypercall retries.
- NiLiHype* (without logging) shows lower overhead.

ReHype and NiLiHype have similar processing overheads, with the majority of the overhead attributed to logging. Even in the worst case (BlkBench), the overhead in terms of total CPU cycles is less than 1%.

#### Implementation Complexity

The complexity is measured by the total lines of code (LOC) added and modified starting from the stock Xen hypervisor, using the CLOC tool.

**Table IV: Implementation Complexity of NiLiHype and ReHype**

| Type | NiLiHype | ReHype |
|------|----------|--------|
| Normal operation | 991 | 991 |
| Recovery routine | 532 | 594 |
| Total | 2179 | 2770 |

- **Normal Operation:**
  - Mitigating hypercall retry failure: 991 LOC
  - Other logging: 532 LOC (NiLiHype), 594 LOC (ReHype)

- **Recovery Routine:**
  - Shared recovery mechanism: 543 LOC
  - Specific recovery mechanism: 113 LOC (NiLiHype), 642 LOC (ReHype)

ReHype requires slightly more code due to additional logging for I/O APIC registers and boot line values.

This structured and detailed analysis provides a comprehensive understanding of the fault injection, recovery, and performance characteristics of NiLiHype and ReHype.