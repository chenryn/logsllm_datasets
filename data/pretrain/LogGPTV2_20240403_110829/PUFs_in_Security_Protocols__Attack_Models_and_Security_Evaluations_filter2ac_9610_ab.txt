as internal key storage element and in simple identiﬁcation
protocols [19], [20] appears less affected (see Section V).
Consequences: The ﬁndings of our analysis are some-
what alarming. They suggest that attack models and pro-
tocol design for “advanced” Strong PUF protocols should
be strongly reconsidered. As PUFs are hardware systems
that can have hidden extra features, new strategies become
necessary here.
One possible countermeasure is to (i) allow additional
computational assumptions in the protocols; (ii) assume that
the PUFs can be shielded during the course of the protocol
in order to prevent communication between the bad PUF
and malicious parties; and (iii) to use each PUF only once,
destroying it at the end of the protocol in order to prevent
access by adversaries after the protocol. This path is taken by
Ostrovsky et al. in their work [18]. However, there are some
downsides associated with this approach: The introduction
of additional computational assumption takes away some
of the appeal of Strong PUFs as a new, independent cryp-
tographic primitive. The effective shielding of PUFs until
their destruction is hard to achieve in concurrent, complex
environments. And, perhaps most importantly, the one-time
use and destruction of the used PUFs after each protocol
execution is extremely costly in practice. It constitutes a
theoretically viable, but practically and commercially essen-
tially infeasible measure.
A second option to encounter our attacks is to add two
new hardware features to Strong PUFs. Firstly, one can
require that Strong PUF’s responses should be “erasable”,
meaning that single responses can be “erased” (made unread-
able for good). Ideally this erasure should not affect other
responses; if this requirement is hard to realize in practice,
then also concept similar to the logical reconﬁgurability
of PUFs [13] may be applicable in certain settings (see
Section IV). This step immunizes Strong PUF protocols
against PUF re-use attacks. Secondly, Strong PUFs should
be “certiﬁable”, meaning that parties holding a Strong PUF
can verify that the PUF has been produced faithfully and has
not been manipulated in any way afterwards. This guarantees
security in the bad PUF model. The combination of both
features can fully restore the applicability of Strong PUFs
in concurrent, complex application environments without
further restrictions (such as the above one-time use of PUFs).
The implementation of these features, however, constitutes
a challenging open problem that we pose to the community
in this work.
Organization of this paper: In Section II we discuss
and introduce various attack models for Strong PUF proto-
cols. In Section III, we evaluate the security of many existing
protocols in the new attack models. Section IV discusses
the consequences of our work, in particular the need for
Erasable PUFs and Certiﬁable PUFs. Section V summarizes
the paper.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
The appendix provides extra information: In Appendix A
we give background on Strong PUFs to the readers who are
not familiar with this concept. In Appendices B, C and D we
provide some of the analyzed PUF-protocols from Brzuska
et al. and Ostrovsky et al. for the convenience of the readers.
II. ATTACK MODELS FOR STRONG PUF PROTOCOLS
Building on the general description of Strong PUF pro-
tocols in the introduction and also in Appendix A, we will
now describe a number of attack scenarios for Strong PUF
protocols.
A. The Stand-Alone, Good PUF Model
In the stand-alone, good PUF model, we make the fol-
lowing assumptions:
1) The protocol is executed only once in a stand-alone
setting, meaning that the protocol is never re-run, also
not any (sub-)sessions of it. The employed PUF(s)
cannot be accessed or communicated with after the
end of the protocol.
2) The employed PUFs are all “good PUFs”, meaning
that are drawn faithfully from a previously speciﬁed
distribution of PUFs and are not modiﬁed in any
way afterwards, neither by malicious players nor by
external adversaries. They only have the properties
and functionalities expected by the honest protocol
participants.
It seems that several early Strong PUF protocols were
more or less implicitly designed for a stand-alone, good PUF
setting, for example van Dijk’s key exchange scheme [5]
and R¨uhrmair’s OT protocol [22]. The stand-alone model
will neither be realistic nor efﬁciently realizable in most
practical PUF-applications, but makes a clean ﬁrst scenario
for studying the security of PUF-protocols. For practical
appliances it needs to be extended, as described below.
B. The UC-Model of Brzuska et al.
In order to model the execution of multiple PUF proto-
cols, Brzuska, Fischlin, Schr¨oder and Katzenbeisser [1], [2]
proposed one possible method how Canetti’s UC-framework
[3] can be adapted to PUFs. For a detailed treatment we refer
the readers to the original papers [1], [2], but summarize the
features of their model that are most relevant for us below.
1) It is assumed that all used PUFs are drawn faithfully
from a previously speciﬁed distribution of PUFs, a
so-called “PUF-family”, and are not modiﬁed in any
way afterwards, neither by malicious players nor by
external adversaries. They only have the properties and
functionalities that honest protocol participants expect
from them. This feature is in common with the above
stand-alone, good PUF model.
2) Only one PUF can be used per protocol session sid.
The PUF is bound to this protocol session and cannot
be used in another session.
3) The adversary does not have physical access to the
PUF between the different subsessions ssid of a
protocol.
For completeness we indicate where the above features
are speciﬁed in [2]: Features 1 and 2 directly follow from
the speciﬁcation of the ideal PUF-functionality FPUF, in
particular the ﬁrst and third dotted item of Fig. 2 of [2].
Regarding feature 2, the functionality initPUF speciﬁes that
FPUF turns into the waiting state if the session sid already
contains a PUF. And the functionality handoverPUF speciﬁes
that sid remains unchanged in the handover, i.e., the PUF
remains in the same session sid after the handover process.
Feature 3 follows from the treatment of the subsessions ssid
throughout their paper [2]. Examples include Figs. 3 to 8,
the protocols given in Figs. 3 and 7, or the proof of Theorem
7.1, where the adversary is only allowed to access the PUF
in the set-up phase, but not during or between the different
subsessions.
Please note that the above features are not rudimentary
aspects of the model of [1], [2], but are central to the security
of their protocols and the validity of their security proofs.
C. The UC-Model of Ostrovsky et al.
Ostrovsky, Scafuro, Visconti and Wadia modify the UC-
model of Brzuska et al. in a number of aspects in a recent
eprint [18]. Among other things, they suggest an attack
scenario termed “malicious PUFs”. It is equivalent to the
“bad PUF model” proposed independently by van Dijk and
R¨uhrmair [6], which is detailed in Section II-E of this paper;
both models seem to have been developed independently and
simultaneously.
The two author groups use their equivalent models for
different purposes, though: Ostrovsky et al. give several
protocols that are purportedly still secure under use of
malicious/bad PUFs. Most of their constructions employ
three extra assumptions: (i) they use additional, classical
computational assumptions alongside with PUFs; (ii) they
assume that the bad PUFs do not communicate with the
malicious parties (compare Section II-E); and (iii) they
assume that the PUFs are used only once, and can be kept
away for good from the adversary or destroyed afterwards.
On the other hand, van Dijk and R¨uhrmair show that if one
wants to design PUF-protocols that solely rest on the security
of the employed PUFs, i.e., without additional computational
assumptions, then the existence of malicious/bad PUFs leads
to hard impossibility results.
We remark that in practice, the above assumption (iii)
would have to be realized by destroying the PUF after each
protocol, or by locking it away for good. In commercial
applications, such a measure would probably be too costly
and economically infeasible. The PUF re-use model in the
next Section II-D investigates the consequences if it cannot
be realized in practice.
289
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
D. The PUF Re-Use Model
Let us now step by step extend the model of Brzuska et
al. [1], [2], and partly also of Ostrovsky et al. [18]. One
implicit assumption of Brzuska et al. is that the adversary
cannot access the PUF between different (sub-)sessions, and
that the PUF is never re-used in another protocol session (see
Section II-B). However, this assumption seems difﬁcult to
guarantee in many natural PUF appliances.
To see this, consider the well-established application
scenario of a PUF on a bank card, which has been issued
by a central authority CA and is subsequently used in
different terminals [20], [19]. To be more concrete, let us
assume that the PUF is repeatedly employed for a session
key exchange between the CA and the smart-card/terminals.
Since an adversary could set up fake terminals, add fake
readers to the card slots of terminals, or gain temporary
possession of the bank card when it is employed in different
contexts (for example when the user is paying with it), a
realistic assumption is that an adversary will have repeated
temporary physical access to the PUF between the different
key exchange (sub-)sessions. However, such access is not
foreseen in the models and protocols of Brzuska et al.
The example illustrates that in practice, adversaries and
malicious players may gain access to the PUF at least oc-
casionally between different (sub-)sessions. This constitutes
a new, relevant attack point and motivates an extension of
the model of Brzuska et al. [1]. Ostrovsky et al. [18] deal
with this observation in their own manner: As described
in Section II-C, they implicitly assume a one-time use of
the PUF. Such one-time use, and subsequent destruction
or locking away of the PUF, results in substantial practical
costs, however. It constitutes a theoretically acceptable, but
at the same time commercially somewhat infeasible measure.
These considerations motivate the following attack model:
The PUF Re-Use Model: We assume that at least a
subset of the PUFs employed in the original protocol is used
on more than one occasion, i.e., not all PUFs are used only
once and destroyed immediately afterwards. The adversary
or malicious parties have access to the PUF more than once,
for example before, after or between different protocols or
protocol (sub-)sessions (if there are any).
The description leaves some detail open, the simple reason
being that many differing variants of the PUF re-use model
are possible. For example, one can distinguish between the
type of adversarial access: (i) full physical access, where the
adversary can attempt arbitrary actions on the PUF, including
arbitrary measurements or active physical modiﬁcation of
the PUF, or (ii) CRP access, where the adversary’s actions
are limited to the mere measurement of CRPs. One can also
differentiate the number of occasions on which access is
possible; or the relative time of the access, such as before
or after the attacked protocol; or the number of CRPs the
adversaries can read out during his access time. One can
further distinguish between different types of re-use: Is the
PUF re-used by the same parties in another instance of
the same protocol, or by entirely new parties in a different
protocol? Instead of declining through all possible scenarios
formally here, we suggest that such differentiation should
be made in the respective security analyses directly.
There is only one speciﬁc instantion we would like to
deﬁne explicitly here, since it has special relevance for us.
The One-Time Posterior Access Model (PAM): In the
PAM, we assume that the adversary has got access to at
least a subset of all PUFs employed in the original protocol
on exactly one occasion after the end of the protocol (or
protocol (sub-)session, if there are any), and is furthermore
limited to the measurement of standard CRPs.
Please note that the PAM is arguably the mildest possible
form of the PUF re-use model. Still, it sufﬁces to success-
fully attack many existing schemes (see Section III).
E. The Bad PUF Model
One other central assumption in the UC-model of Brzuska
et al. is that the players are not allowed to use “bad”, fraud-
ulent PUF-hardware with properties beyond the expected
PUF functionality. This assumption can again be difﬁcult
to uphold in practice, as has been observed independently
by Ostrovsky et al. [18] (see Section II-C).
To motivate bad PUFs, consider once more the earlier
smart-card example. Let us assume that the CA issues the
card that carries the PUF, and that the CA and the smart-
card/terminals want to run an OT protocol in this setting.
We must assume that the CA is not fully trusted by the
smart-card/terminals (note that if the CA was fully trusted,
then the smart-card/terminals would not require an OT
implementation). However, a malicious CA can cheat easily
in this scenario by putting a malicious PUF-hardware (a “bad
PUF”) instead of a normal PUF on the smart card. To name
one example, the CA could replace the normal PUF by a
pseudo random function (PRF) or a pseudo-random number
generator (PRNG) with a seed s known to the CA. If the
PRF will have the same, digital input-output interface as
the normal PUF, such a step will remain unnoticed. Still, it
enables the CA to simulate and predict all responses of this
“bad PUF” without being in physical possession of it, and
breaks one of the essential security features of the purported
“PUF” on the bankcard, namely its unpredictability. It is not
too difﬁcult to see that under the assumption that the CA
replaces the PUF by a PRF with a seed known to the CA,
the well-known OT protocols of R¨uhrmair [22] and Brzuska
et al. [1] are no longer secure. If the CA acts as OT-receiver,
for example, it can learn both bits of the OT-sender (see
Section III-B for details).
Abstracting from this speciﬁc example, the general prob-
lem is that in a typical two-party protocol, one of the parties
can fabricate the PUF, while the other party may only
290
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
is hard to verify that
use the PUF “from the outside” via a (digital) challenge-
response interface. It
there is no
unexpected, malicious functionality on the other side of the
interface. From a practical perspective, this observation is
most severe for electrical Strong PUFs, which are the most
widely distributed Strong PUFs today. But it also holds for
integrated optical PUFs as given by Tuyls and Skoric [32].
This motivates a systematic study of bad PUF attacks.
Generally, we denote by the term “bad PUF” a hardware