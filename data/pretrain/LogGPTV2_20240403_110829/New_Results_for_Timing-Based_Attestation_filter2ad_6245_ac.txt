ATTACKER INTER-BLOCK TRANSFER FORGES EIP SRC AND EIP DST
Table IV
Figure 4. Network topology for timing experiments
sub ebx, 1
test ebx, ebx
jz attackSetRange
lea edx, origEipSrcArray
mov eax, currentIndex
push [edx+eax*4]
xor edx, edx
mov eax, esi
and eax, 7
mov currentIndex, eax
mov dl, BLOCK SIZE
mul dl
mov ecx, cleanBlockZero
add ecx, eax
lea edx, attackEipDstArray
mov eax, currentIndex
jmp [edx+eax*4]
INTERBLOCK TRANSFER ATTACK
Decrement loop counter
Check if loop counter is 0
If 0, jump to minichecksum switch
Get address of array of
forged EIP SRC addresses
Get index of current block
Push the EIP SRC that the original
call instruction would have pushed
Clear edx for use in upcoming mul
Move PRN to eax
Keep the bottom 3 bits of PRN in eax
Store this as the next index
where the code will be executing
Move size of block to dl
This will perform ax = al * dl
Get address of start of clean blocks
Ecx = base + sizeofblock * (PRN & 7)
This sets ecx to the expected EIP DST
Now the attacker prepares to jump
to his own next block
Get the index of the next block
Jump to the address of the next block
point of Pioneer originally was to propose a mechanism
to create a dynamic root of trust on ”legacy systems” that
did not have a TPM. However we believe the system is
worth investigating given what experiments tell us about
the prohibitively expensive amount of time that a software-
only timing-based attestation system would need to run to
overcome network jitter on WANs. When we asked the
authors for their prototype implementation, we had found
they hadn’t actually implemented or tested it. Therefore we
created an implementation, and share the surprising results
in Section IV-F.
245
IV. EXPERIMENTS & RESULTS
A. Conﬁguration
The server system that requested self-check measurements
was running 64 bit Windows 2008 SP2 Server on an IBM
x3650 M2 with 8GB RAM, an Intel Xeon X5570 CPU at
2.93GHz, and a Broadcom BCM5709C Gigabit NIC. Our
server software is implemented in C++ and uses WinPcap
in order to determine the round trip times. We set a ﬁlter
so that WinPcap calls back to the server whenever it sees
one of our measurements outbound or inbound. When an
outbound measurement request is seen, the software updates
a pending measurement database entry with the timestamp
in microseconds according to WinPcap. When an inbound
measurement is seen, the software looks up the pending
record, subtracts the sent from received time, and stores the
RTT in the database.
The client systems were all 32 bit Windows XP SP3
running on Dell Optiplex 960 systems with 4GB of RAM,
an Intel Core 2 Quad CPU Q9650 at 3.00GHz, and Intel
82567LM-3 Gigabit NIC.
The network topology for the experiments is shown in
Figure 4. The switches are a mix of Cisco 3750/3750Gs, and
the routers are Cisco 6500s. For the multi-hop experiments,
we physically moved the same two hosts to each of the 1, 2,
3, 8, and 10 link distances from the server and measured at
each location. For the experiments involving 31 systems, the
lab where the systems resided was at the 10 link location.
For our self-checksum loop we used 2.5 million iterations,
for no reason other than that is how many the original
Pioneer used, and also because on the test machines this
yielded a time around 100ms. Generally speaking it
is
desirable to keep the self-checksum around 100ms because
that
is the commonly accepted threshold beneath which
human eyes cannot perceive changes, and it is desirable to
not lock the user’s system long enough for them to detect
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:23 UTC from IEEE Xplore.  Restrictions apply. 
Network ListenerJump HookAttacker Modified:Base = 0x90000000NormalSelf-checkNetwork ListenerClean Copy:Base = 0xA0000000NormalSelf-checkAttackerSelf-checkEIP (overhead forgery)DP (free forgery)Nonce:0xf005ba11Base:0xA0000000Result:0x2bad101...AttackerSelf-checkFigure 5. Network round trip time for 31 Dell Optiplex 960s over 10
links. 2 second measurement frequency.
Figure 6. Rdtsc-computed runtime for 31 Dell Optiplex 960s. 2 second
measurement frequency.
UI jitter. That said, the subsequent measurement which is
performed by the CMM component of Checkmate will add
to the time that the system is locked up. For instance hashing
a small kernel module might take 1ms, but hashing the
largest module, nt itself, can take around 50ms.
B. Timing results for 31 systems over 10 links
We have been running various versions of the self-check
function on many different hosts for around 2 years, and this
is our 6th iteration. During that time we had anecdotally
conﬁrmed that different hosts of the same hardware type
exhibited the same timing characteristics. However in order
to more rigorously conﬁrm homogenous timing behavior on
homogenous hardware, we were temporarily granted access
to a lab of Optiplex 960s.
In Figure 5, we can see that all of the hosts’ measurement
times cluster very tightly. The maximum standard deviation
for a host’s clean measurements was 436µs, but the second
highest was only 139µs. There is a one measurement partial
overlap visible at the beginning and end of the attack data.
This is because we did not wait until all hosts had the exact
same number of measurements before pausing and toggling
the attack.
In order to validate the overhead of the attacker, and
to be able to conﬁrm in our data exactly when the attack
was installed, we also collected data about the self-reported
runtime of the checksum. Our experimental branch of the
code includes the use of rdtsc (read timestamp counter)
instructions so that the client can calculate and report to the
server its own perspective on the self-check code runtime.
This would never be used in the real system, because
an attacker could trivially forge the value, however it is
useful during experimentation to provide insight into timing
variation due to host effects vs. network effects. The data is
shown in Figure 6.
Being able to contrast these graphs helps clarify that most
of the outliers are due to variations in runtime, rather than
any other host or network effects. From a comparison of
network RTT to CPU-computed runtime we can see clear
correlation in most of the outliers. We are continuing to
analyze our self-check function to understand what could
cause outliers to exhibit faster and slower runtimes. However
the faster outliers are only around 600 to 800µs faster that
their hosts’ averages. So even if they were exhibited by the
attack data, they would not be low enough to fall within the
expected runtime bounds. Overall this data clearly indicates
the ability to discriminate attacker timing overhead over 10
network links, a result that has not been previously shown.
C. Analyzing variation of timing behavior between hosts
Another aspect we wanted to evaluate within the all-host
data set was whether it was realistic to automatically set
alerting limits from the measurements collected from one
host, and apply those limits to all other hosts of the same
hardware type. For increased practicality of deployment in
commercial software there must be some way to generate
the expected baseline timing for hosts. This could take the
form of a software vendor keeping a master list of expected
runtimes on a per cpu/frequency basis. It could be proﬁled
and set on the ﬁrst run, under the assumption that attackers
are not sitting in wait to attack a self-checksum system when
it is installed. Or it could be done by requiring the customer
to install and generate a baseline client timing on a known-
clean machine one time per hardware conﬁguration deployed
in their environment. In this latter case, there is the need
to understand how many false positives would be incurred
by collecting baseline timing limits from a single host and
applying those limits as the alerting threshold on all other
hosts.
We generate a baseline a system by taking about 200
measurements, and then generate upper and lower control
limits (average ± 3 standard deviation) for use in a control
246
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:23 UTC from IEEE Xplore.  Restrictions apply. 
 109000 110000 111000 112000 113000 114000 115000 116000 0 100 200 300 400Measurement RTT (us)Measurement numberAttackAbsentAttackAbsentAttackPresent 3.25e+08 3.3e+08 3.35e+08 3.4e+08 3.45e+08 0 50 100 150 200 250 300 350 400Measurement runtime (cycles)Measurement numberAttackAbsentAttackAbsentAttackPresentchart. Our server then has the ability to send an alert to
an analyst when a pre-speciﬁed number of consecutive data
points are out of control. Therefore we want to know what
this threshold should be set to.
COMPARISON OF DATA FROM HOST SPECIFIED IN THE ROW AGAINST
Table V
LIMITS DERIVED FROM HOST SPECIFIED IN THE COLUMN
host 15
host 16
host 17
host 18
host 19
host 20
host 21
host 22
host 23
host 24
host 25
host 17
1/190,1
1/190,1
1/190,1
0/190,0
0/190,0
0/189,0
0/189,0
1/190,1
0/190,0
0/189,0
0/190,0
host 18
6/190,1
2/190,1
3/190,1
4/190,1
0/190,0
4/189,1
0/189,0
1/190,1
1/190,1
3/189,1
4/190,1
host 19
9/190,1
4/190,1
4/190,2
6/190,1
0/190,0
4/189,1
2/189,1
1/190,1
3/190,1
2/189,1
8/190,1
host 20
9/190,1
2/190,1
4/190,2
5/190,1
0/190,0
2/189,1
1/189,1
1/190,1
1/190,1
2/189,1
6/190,1
host 21
9/190,1
2/190,1
3/190,2
4/190,1
0/190,0
3/189,1
1/189,1
1/190,1
1/190,1
2/189,1
3/190,1
Due to a lack of space, we show only a subset of the
comparison of all hosts to each other in Table V. The hosts
in a row has each of its data points evaluated to determine
whether it falls within the control limits generated from
the data for the host given in the column. Entries are of
the form X/Y,Z. X/Y is the ratio of total number of out
of control measurements to total number of measurements.
Z is the maximum number of consecutive out of control
measurements out of X. The maximum Z value for the entire
table suggests a threshold that could be set by the server as
the number of out of control measurements it should see
before it alerts, in order for it to have had no false positives
in this training set. So for instance, if we generated a baseline
and applied host 20’s limits to host 17, we would have seen
4/190 measurements that fell outside of the limits, with a
maximum of 2 of those 4 data points being consecutive. The
maximum consecutive out of control data points across all
comparisons was 2. This is in stark contrast to the behavior
in the presence of an attacker, where there will be many out
of control data points. The control limits for all hosts are
show in Table VI.
D. Measurement of two hosts at different network locations
We expect the number of hops that the client is away
from the server will affect the measured RTT. Therefore we
tested to see how much this affected the time by measuring
at vantage points from 1 direct link (connected via Ethernet
crossover cable) to 10 links (the maximum link count on the
testing campus.) Figure 7 shows the results of measuring two
hosts when moved to different link counts.3
3There were nine outliers below the clean measurements that were
cropped to provide better visibility of the gap in timing between trafﬁc
over 8 links vs 10 links.
PER HOST CONTROL LIMITS IN MICROSECONDS
Table VI
lower
limit µs
110847
110635
110728
110733
110844
109786
110855
110740
110814
110853
110845
110849
110746
110839
110714
110837
upper
limit µs
111317
111472
111453
111405
111356
112403
111340
111395
111282
111304
111321
111330
111451
111314
111407
111295
host1
host3
host5
host7
host9
host11
host13
host15
host17
host19
host21
host23
host25
host27
host29
host31
lower
limit µs
110738
110850