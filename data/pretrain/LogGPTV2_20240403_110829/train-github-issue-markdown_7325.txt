### Environment Details
- **Elasticsearch Version**: 2.3.3
- **JVM Version**: OpenJDK 1.8.0_91
- **Operating System**: Ubuntu 16.04

### Problem Description
When Elasticsearch is restarted immediately after a bulk request is sent, the documents are not indexed correctly. The bulk request is retried due to a "connection refused" error, resulting in random document duplication (same content, different IDs) in the index.

### Expected vs. Actual Behavior
- **Expected Behavior**: All documents should be indexed without duplication.
- **Actual Behavior**: Random documents are duplicated with different IDs.

### Steps to Reproduce
1. **Send 1000 Bulk Requests**:
   - Start sending 1000 bulk requests, each containing 100 documents. For example, the first request can be sent using the following `curl` command:
     ```sh
     curl -XPOST 'localhost:9200/_bulk' -d '
       {"index":{"_index":"index1", "_type":"test"}}
       {"counter": "1", "package": "1/1000"}
       {"index":{"_index":"index1", "_type":"test"}}
       {"counter": "2", "package": "1/1000"}
       ...
       {"index":{"_index":"index1", "_type":"test"}}
       {"counter": "100", "package": "1/1000"}
     '
     ```

2. **Restart Elasticsearch**:
   - Restart Elasticsearch just after it starts receiving the bulk requests.
     ```sh
     sudo service elasticsearch restart
     ```

3. **Repeat the Process**:
   - Repeat steps 1 and 2 several times.

4. **Retry Failed Requests**:
   - If a bulk request fails with a "connection refused" error, retry the request until it succeeds.

5. **Verify Document Count**:
   - After all bulk requests are sent, check the document count in the index. The expected count is 10,000, but the actual count is greater.
     ```sh
     curl -XGET 'http://localhost:9200/index1/_count'
     ```
     Example response:
     ```json
     {
       "count": 10326,
       "_shards": {
         "total": 5,
         "successful": 5,
         "failed": 0
       }
     }
     ```

### Relevant Logs
```plaintext
[2016-07-06 15:49:28,441][INFO ][node                     ] [ubuntu1-node] stopping ...
[2016-07-06 15:49:28,485][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.jboss.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.jboss.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.jboss.netty.channel.Channels.write(Channels.java:725)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:146)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:89)
    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:85)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:356)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:351)
    at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:95)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.finishAsFailed(TransportReplicationAction.java:567)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleException(TransportReplicationAction.java:527)
    at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-07-06 15:49:28,512][WARN ][transport                ] [ubuntu1-node] Transport response handler not found of id [43]
[2016-07-06 15:49:28,863][INFO ][node                     ] [ubuntu1-node] stopped
[2016-07-06 15:49:28,863][INFO ][node                     ] [ubuntu1-node] closing ...
[2016-07-06 15:49:28,868][INFO ][node                     ] [ubuntu1-node] closed
```

This log indicates that the Elasticsearch node is shutting down and encountering issues with handling the bulk requests during the restart process.