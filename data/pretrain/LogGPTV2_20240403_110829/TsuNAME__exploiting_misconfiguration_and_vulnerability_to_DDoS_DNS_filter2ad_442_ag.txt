14657
2581
12386591
12386591
Authoritative Server Side
Querying IPs
ASes
Queries
Responses
Table 14: TsuNAME Emulation Experiments after Google’s
mitigation. Datasets: [46].
Measurement
Frequency
Qname
Query Type
Date
Duration
Atlas Probes
VPs
Queries
Responses
SERVFAIL
Timeout
REFUSED
FORMERR
NOERROR
NXDOMAIN
Recurrent-AfterGoogle
One-off
$P-$r.gioia.essedarius.net.
A
2021-02-12
16h
Client Side
9962
17884
451997
451997
446941
0
3456
604
816
180
21865
2560
21257464
21257464
Authoritative Server Side
Querying IPs
ASes
Queries
Responses
Table 16: TsuNAME Recurrent Experiments after Google’s
mitigation. Datasets: [46].
Figure 26 shows the timeseries for this experiment. We see in
Figure 26a that when Atlas is active, each zone authoritative servers
receives roughly 90k queries/5min. That is already fewer queries
than Figure 18a, in which it peaked to almost 150k queries/5min.
When Atlas stops sending queries, the authoritative servers receive
roughly 50k queries/5min, also fewer than previously (∼75k).
We see that Google now is the 8th position in Figure 27, send-
ing 430k queries out of the 21M ś 2.02% of the total, as shown in
Table 16.
415
 0 50 100 150 20018:0020:0022:0000:0002:0004:0006:0008:0010:00Resolvers in LoopOﬄineQueries (k)Time (UTC) -- 2021-02-[9-10]. 5min binsessedarius.netverfwinkel.net 0 2000 4000 6000 8000 10000 12000 14000 1600018:0020:0022:0000:0002:0004:0006:0008:0010:00Resolvers in LoopOﬄineResolversTime (UTC) -- 2021-02-[9-10]. 5min binsessedarius.netverfwinkel.net 0 1 2 3 4 5 615267200050514681516984629003133354257356847336692Million QueriesAS number 0 1 2 3 4 5 615267200050514681516984629003133354257356847336692TsuNAME
IMC ’21, November 2–4, 2021, Virtual Event, USA
Date
2021-02-05
2021-02-22
2021-02-23
2021-03-04
2021-02-18ś2021-05-05
2021-05-06
2021-05-06
Type
Private Disclosure
Private Disclosure
Private Disclosure
Private Disclosure
Private Disclosure
Public Disclosure
Public Disclosure
Group
OARC34
APTLD
CENTR
LACTLD
Private
OARC35
https://tsuname.io
Table 17: TsuNAME disclosure timeline
domains and misconfigure them. For example, say an attacker has
500 .fr and 500 .ca domain names under its disposable: it could
configure each of them with NS records pointing to each other, as
Figure 12.
The last step consists in inducing vulnerable resolvers to query
for these domains, so they can enter start looping and unleash a
large volume of queries. It will be the parent authoritative servers
of the cyclic NS records that will be receiving all the queries (in
this case, .ca and .fr authoritative servers).
The last step involves in finding vulnerable resolvers ś our ex-
periments show that there are 4k resolves from 261 ASes vulnerable
to TsuNAME, but that is a lower-bound estimative, given we have
not covered most resolvers on the Internet (we were limited by the
view of our vantage points). Luckily, Google has fixed GDNS after
our notification, but there are still other vulnerable resolvers out
there, including OpenDNS. One could only think of the possible
damage that can be done if an attacker decides to employ a large
botnet to send frequent queries, such as the Mirai botnet [5].
Alternatively, hijacking only one popular domain and miscon-
figuring its NS records would also suffice, as in the case with the
anonymous European ccTLD (Figure 14). In this way, it is likely that
vulnerable resolvers would be automatically found by the regular
stream of user queries.
Once resolvers start looping, the effect on the authoritative
servers will depend on the attack size versus the authoritative
servers’s capacity, and there is a large variation among TLDs when
it comes to capacity, given there is large variation in the number of
authoritative servers and anycast instances per ccTLD.
Most TLDs are likely to suffer at least partial unavailability if
faced with 100s of thousands of queries per second. Once down,
the consequences can be catastrophic: in case of country-code TLD,
most official services, banks, online shopping and others would
become unreachable.
Collateral damage: an attack against a particular TLD may have
impact a series of others, given they may share parts of the same in-
frastructure [3, 24], by using the same authoritative DNS providers.
When Dyn DNS was attacked, multiple DNS zones were affected.
When some of the Root DNS servers were attack in 2015 [50], parts
of the Netherlands’ .nl ccTLD was also affected [32].
G PRIVATE AND PUBLIC DISCLOSURE
TIMELINE
Table 17 shows the dates of the public and private disclosures we
performed.
(a) Queries
(b) Resolvers
Figure 26: Ripe Atlas: Queries and unique resolvers query-
ing authoritative servers (5min bins)
Figure 27: RecurrentAfterGoogle: top 10 ASes by query vol-
ume with problematic resolvers
F THREAT MODEL
The TsuNAME threat model involves using DNS reflection to carry
out a denial-of-service attack. Instead of attacking these servers
directly, the attack could use cyclic dependent domains and vul-
nerable resolvers to keep a continuous stream of queries to the
designated targets. None of our experiments fully exploited this
possibility for ethical reasons; next we discuss how a well motivated
could attack could as well do it.
For this to happen, an attacker needs (i) to have domains under a
given zone (or take control over them, e.g., by stealing registrant or
registrar credentials), (ii) misconfigure them with cyclic dependent
NS records, and (iii) induce vulnerable resolvers to carry out queries.
The first and second part are not difficult ś most TLDs such as
.org and .es have an open registration policy, so anyone can register
416
 0 50 100 150 20012:0014:0016:0018:0020:0022:0000:0002:0004:0006:0008:0010:00Atlas ActiveResolvers in loop OﬄineQueries (k)Time (UTC) -- 2021-02-[12-13],  5min binsessedarius.netverfwinkel.net 0 2000 4000 6000 8000 1000012:0014:0016:0018:0020:0022:0000:0002:0004:0006:0008:0010:00Atlas ActiveResolvers in loop OﬄineResolversTime (UTC) -- 2021-02-[12-13],  5min binsessedarius.netverfwinkel.net 0 1 2 3 4 5 6 71526720005051468422108366925978915169846213335Million QueriesAS number 0 1 2 3 4 5 6 71526720005051468422108366925978915169846213335IMC ’21, November 2–4, 2021, Virtual Event, USA
G. C. M. Moura et al.
#
1
2
3
4
5
6
7
8
9
Time
14:14:57
14:15:01
14:15:02
14:15:11
14:15:11
14:15:13
14:15:14
14:15:14
14:15:14
Median ∆t
37ms
36ms
RIPE Atlas Side
Query/Type
52196.sub.verfwinkel.net/A
52196.sub.verfwinkel.net/A
52196.sub.verfwinkel.net/A
Authoritative Server Side
52196.sub.verfwinkel.net/A
ns.sub.cachetest.net/A
ns.sub.verfwinkel.net/A
ns.sub.verfwinkel.net/A
ns.sub.verfwinkel.net/A
52196.sub.verfwinkel.net/A
Remaining queries
Query/Type
ns.sub.verfwinkel.net/A
ns.sub.cachetest.net/A
Resolver
192.168.88.1
208.67.222.123
208.67.220.123
IP4
IP4
IP4
IP4
IP4
IP4
Total
169462
169871
Table 18: Query sequence for Probe 52196 during Low bound
measurement
H ANOTHER LOOPING PROBE
Consider probe 52916, from the new domain measurement. Ta-
ble 18 shows the query history for this probe. At the Atlas side,
we see that this probe has sent 3 queries ś one per resolver it was
configured with. The first query goes to a private IP address, likely
a local resolver. Queries 2 and 3 go to OpenDNS, Cisco’s public
resolver service.
At the authoritative server side, however, we see queries from
only one IP address (anonymized as IPv4), which belongs to the the
same AS number as the probe (AS15267). Query #4 it is the first
query we see on the authoritative server related to this probe, so
we map this IP to the probe (as in ğ4.4). After that, it asks for the A
records of the authoritative servers, and it asks again at 14:15:14
for the 52196.sub.verfwinkel.net/A domain.
Then, the resolver begins to loop: it sends 169k queries for each
dependent NS record (as queries #5 and #6), every 37ms, even in the
absence of new Ripe Atlas queries. Given this probe use two Open
DNS resolvers and a private a private IP address space, we do know
if the looping occurs at this private resolver, if it is a forwarder,
or at the last level resolver (Figure 4). We tried to identify this
local resolver by issuing chaos TXT queries to determine their
software version [63], but this feature was not implemented (see
measurement probe52196 in [46]). Still, the effect is send a large
volume of queries to our authoritative servers.
I RESOLVERS DEV/OPS RECOMMENDATIONS
To mitigate the traffic surge from resolvers to authoritative servers
caused by the TsuNAME vulnerability, resolver developers MUST
instrument their code to both detect cyclically dependent NS records
(so loops can be avoided), and cache them likewise (so no further
user queries generate new queries to the targeted authoritative
servers).
For example, in the Listing 1 and Listing 2 examples, that would
involve in detecting that #3 delegation NSes are unresolvable, and
caching it as that (possibly as SERVFAIL [27]). Then, any subsequent
queries to these delegations will notice that there is no resolvable
NS record for this zone, and will be answered as SERVFAIL from
the cache, reducing the volume of queries to authoritative servers.
1 e s s e d a r i u s . net .
e s s e d a r i u s . net .
1
1
IN
IN
NS
NS
ns1 . e x a m p l e . nl .
ns2 . e x a m p l e . nl .
Listing 1: DNS Zone file: essedarius.net
e x a m p l e . nl .
2 e x a m p l e . nl .
1
1
IN
IN
NS
NS
ns3 . e s s e d a r i u s . net .
ns4 . e s s e d a r i u s . net .
Listing 2: DNS Zone file: example.nl
Caching, but for how long? The caching duration is inversely
proportional to the volume of queries that are forwarded to author-
itative servers. Resolver developers must choose this caching value
carefully.
RFC2308 [4] states that a SERVFAIL response may be cached for
no longer than 5 minutes. That may be reasonable for this case,
given that mean-time-to-repair such cyclically dependent records
is at least minutes.
Alternatively, a resolver developer may employ a more adaptive
TTL method. For example, it may start with 5 minutes, and perform
some linear back-off to a larger value, possibly controlled by e.g.
negative TTL on the parent zone and/or RFC-specified hard limit,
such as 1 hour or 4 hours.
I.1 Testing your resolver software
To test your resolver software, set up cyclically dependent dele-
gations, as shown in Listing 1 and Listing 2. We strongly recom-
mend creating third-level domain names (as in our examples)
instead of second-level (e.g., example.nl) given that cyclically de-
pendent second-level domains will stress authoritative servers of
their respective TLDs.
After creating these cyclically dependent delegations, we suggest
the following tests:
I.1.1 Test 1: Loop Detection.
(1) Clean the cache of your resolver
(2) Monitor the traffic between the resolver and the Internet
(3) Send ONE query to your for a domain under the misconfig-
ured delegation. For example, dig A random.platypus.esse
darius.net.
• Compute how many queries are then send to the parent
authoritative servers of both misconfigured zones (lines
#1 and #2 of Listing 1 and Listing 2)
• Determine if your resolver loops indefinitely, or if eventu-
ally stop sending queries to the authoritative servers. You
may need to monitor for various minutes or hours.
Please notice that the resolver may send a SERVFAIL response to
your client, but it may remain looping, sending non-stop queries to
the authoritative servers.
For a reference, you may want to check Unbound’s source code,
which includes various cycle detections, as described in their changelog1.
1https://github.com/NLnetLabs/unbound/blob/master/doc/Changelog
417
TsuNAME
IMC ’21, November 2–4, 2021, Virtual Event, USA
I.1.2 Test 2: Caching Cyclic Records and Amplification.
(1) Clean the cache of your resolver
(2) Monitor the traffic between the resolver and the Internet
(3) Send ONE query to your for a domain under the misconfig-
ured delegation. For example, dig A random.platypus.esse
darius.net.
(4) Then, send this query multiple times every 5 s (or other short
interval)
• Compute how many queries are then send to the parent
authoritative servers of both misconfigured zones (lines
#1 and #2 of Listing 1 and Listing 2)
• Determine if the new, recurrent queries from your client
(dig in this case) cause your resolver to send many more
queries to reach the authoritative servers, or if they are
answered from cache.
If new user queries (dig) lead to more queries to the authoritative
servers, you resolver is then vulnerable to TsuNAME.
418