**作者：LoRexxar'@知道创宇404实验室  
日期：2021年1月28日**
# 前言
将时间还原到2019年6月之前，扫描器的世界大多还停留在AWVS这样的主动扫描器，被动扫描曾被提出作为浏览器插件火热过一段时间，可惜效率太低等多种问题束缚着它的发展。随着Xray的发布，一款免费好用的被动扫描器从很多方面都带火了自动化漏洞扫描的风潮。
其中有的人将Xray挂在自己的常用浏览器上以图在使用的过程中捡漏，有的人只在日站的时候挂上以图意外之喜，也有人直接操起自己尘封已久的爬虫配合xray大范围的扫描以图捡个痛快。可以说配合xray日站在当时已经成了一股风潮。
市面上比较常见的组合crawlergo+xray中的crawlergo也正是在这种背景下被开放出来。
  * 
但可惜的是，建立在自动化漏洞挖掘的基础上，当人们使用相同的工具，而我们又没办法自定义修改，是否能发现漏洞变成了是否能发现更多的资产的比拼。建立在这样的背景下，我决定自己发起了一个开源的爬虫项目，这就是LSpider诞生的背景。
  * 
LSpider作为星链计划的一员，已经开源，工具可能并不算成熟，但 **持续的维护以及更新** 是星链计划的精神~
# LSpider想要做到什么？
在发起一个项目之初，我们往往忘记自己到底为什么开始，为什么要写这个项目，重复造轮子，以及闭门造车从来都不是我们应该去做的事。
而LSpider发起的初衷，就是 **为被动扫描器量身打造一款爬虫** 。
而建立在这个初衷的基础上，我决定放弃传统爬虫的那些多余的功能。
这是一个简单的传统爬虫结构，他的特点是爬虫一般与被动扫描器分离，将结果输入到扫描器中。
将被动扫描器直接代理到爬虫上
这样一来，爬虫的主要目标转变为了， **尽可能的触发更多的请求、事件、流量** 。
建立在这个大基础上，我们得到了现在的架构：
由主控分配爬虫线程，扫描目标域，并 **尽可能的触发更多的请求、事件、流量** 。将被动扫描器通过代理的方式挂在爬虫下并独立的完成漏洞扫描部分。
除了为被动扫描器服务以外，还有什么是在项目发起时的初衷呢？
我的答案是，这个爬虫+被动扫描器的目的是，能让我不投入过多精力的基础上，挖洞搞钱！！！
不在乎扫到什么漏洞，不在乎扫到什么厂商，只求 **最大限度的扫描目标相关所有站、所有域名、所有目标** 。
为了实现这个目标，我在爬虫中内置了查询子域名的api，内置了hackerone、bugcrowd目标爬虫，在设计之初还添加了定时扫描功能。
到目前为止，我们设计了 **一个自动化无限制扫描目标，且为被动扫描器而存在的爬虫** 架构。
下面我们一起完成这个项目。
# 爬虫基础
首先爬虫部分，为了实现最大程度上触发更多的请求、事件、流量，我们有且只有唯一的选择为Chrome Headless.
## 配置Chrome Headless
这里我选择了selenium来操作Chrome WebDriver。值得注意的是几个比较重要的配置。
    self.chrome_options.add_argument('--headless')
    self.chrome_options.add_argument('--disable-gpu')
    self.chrome_options.add_argument('--no-sandbox')
    self.chrome_options.add_argument('--disable-images')
    self.chrome_options.add_argument('--ignore-certificate-errors')
    self.chrome_options.add_argument('--allow-running-insecure-content')
    self.chrome_options.add_argument('blink-settings=imagesEnabled=false')
    self.chrome_options.add_argument('--omnibox-popup-count="5"')
    self.chrome_options.add_argument("--disable-popup-blocking")
    self.chrome_options.add_argument("--disable-web-security")
    self.chrome_options.add_argument("--disk-cache-size=1000")
除了设置headless模式以外，还关闭了一些无意义的设置。
    if os.name == 'nt':
        chrome_downloadfile_path = "./tmp"
    else:
        chrome_downloadfile_path = '/dev/null'
    prefs = {
        'download.prompt_for_download': True,
        'profile.default_content_settings.popups': 0,
        'download.default_directory': chrome_downloadfile_path
    }
设置好文件下载的目录，如果没设置的话会自动下载大量的文件在当前文件夹。
    desired_capabilities = self.chrome_options.to_capabilities()
    if IS_OPEN_CHROME_PROXY:
        logger.info("[Chrome Headless] Proxy {} init".format(CHROME_PROXY))
        desired_capabilities['acceptSslCerts'] = True
        desired_capabilities['acceptInsecureCerts'] = True
        desired_capabilities['proxy'] = {
            "httpProxy": CHROME_PROXY,
            "ftpProxy": CHROME_PROXY,
            "sslProxy": CHROME_PROXY,
            "noProxy": None,
            "proxyType": "MANUAL",
            "class": "org.openqa.selenium.Proxy",
            "autodetect": False,
        }
通过org.openqa.selenium.Proxy来设置浏览器代理，算是比较稳定的方式。
    self.driver.set_page_load_timeout(15)
    self.driver.set_script_timeout(5)
这两个配置可以设置好页面加载的超时时间，在大量的扫描任务中，这也是必要的。
除了基础配置以外，有个值得注意的点是：
你必须在访问页面之后才可以设置cookie，且cookie只能设置当前域，一旦涉及到跳转，这种cookie设置方式就不会生效。
    self.origin_url = url
    self.driver.implicitly_wait(5)
    self.driver.get(url)
    if cookies:
        self.add_cookie(cookies)
        self.driver.implicitly_wait(10)
        self.driver.get(url)
## 模拟点击以及智能填充
在配置好chrome headless之后，为了 **模拟人类的使用**
，我抛弃了传统爬虫常用的拦截、hook等获取请求并记录的方式，转而将重心放在模拟点击以及智能填充上。
### 模拟点击
这里拿a标签举例子
    links = self.driver.find_elements_by_xpath('//a')
    link = links[i]
    href = link.get_attribute('href')
    self.driver.execute_script(