domains that have recently been registered and are involved
in scam campaigns will show an abrupt increase in the num-
ber of requests as more and more victims access the site in
a short period of time.
To analyze the changes in the number of requests for a
domain during a given period of time, we divide this period
into ﬁxed length intervals. Then, for each interval, we can
count the number of DNS queries that are issued for the do-
main. In other words, the collection of DNS queries that
target the domain under analysis can be converted into time
series (i.e., chronologically ordered sequences of data val-
ues). Hence, we can leverage off-the-shelf algorithms [12].
We perform our time series analysis on two different scopes:
First, we analyze the time series globally. That is, the start
and end times of the time series are chosen to be the same as
the start and the end times of the entire monitoring period.
Second, we apply local scope time series analysis where
the start times and end times are the ﬁrst and last time the
domain is queried during the analysis interval. While the
global scope analysis is used for detecting domains that ei-
ther have a short life or have changed their behavior for a
short duration, the local scope analysis focuses on how do-
Feature Set
Time-Based
Features
DNS Answer-Based
Features
TTL
Value-Based
Features
Domain Name-
Based Features
Feature Name
Short life
Daily similarity
Repeating patterns
Access ratio
Number of distinct IP addresses
Number of distinct countries
Number of domains share the IP with
Reverse DNS query results
Average TTL
Standard Deviation of TTL
#
1
2
3
4
5
6
7
8
9
10
11 Number of distinct TTL values
12 Number of TTL change
13
14 % of numerical characters
15 % of the length of the LMS
Percentage usage of speciﬁc TTL ranges
Table 1: Features.(LMS = Longest Meaningful Substring)
mains behave during their life time.
A domain is deﬁned to be a short-lived domain (i.e., Fea-
ture 1) if it is queried only between time t0 and t1, and if this
duration is comparably short (e.g., less than several days).
A domain that suddenly appears in the global scope time
series and disappears after a short period of activity has a
fairly abnormal behavior for being classiﬁed as a benign do-
main. Normally, if a domain is benign, even if it is not very
popular, our thesis is that the number of queries it receives
should exceed the threshold at least several times during the
monitoring period ( i.e., two and a half months in our exper-
iments). Therefore, its time series analysis will not result in
an abrupt increase followed by a decrease as the time series
produced by a short-lived domain does.
The main idea behind performing local scope analysis is
to zoom into the life time of a domain and study its behav-
ioral characteristics. We mainly focus on three features (i.e.,
Features 2, 3, 4) that may distinguish malicious and benign
behavior either by themselves or when used in conjunction
with other features. All the features involve ﬁnding similar
patterns in the time series of a domain. Feature 2 checks
if there are domains that show daily similarities in their re-
quest count change over time (i.e., an increase or decrease
of the request count at the same intervals everyday). Feature
3 aims to detect regularly repeating patterns. Finally, Fea-
ture 4 checks whether the domain is generally in an “idle”
state (i.e., the domain is not queried) or is accessed contin-
uously (i.e., a popular domain).
The problem of detecting both short-lived domains and
domains that have regularly repeating patterns can be
treated as a change point detection (CPD) problem. CPD
algorithms operate on time series and their goal is to
ﬁnd those points in time at which the data values change
abruptly. The CPD algorithm that we implemented [12]
outputs the points in time the change is detected and the av-
erage behavior for each duration. In the following section,
we explain how we interpret the output of the CPD to de-
tect the short-lived domains and the domains with regularly
repeating patterns.
3.1.1 Detecting abrupt changes
As CPD algorithms require the input to be in a time series
format, for each domain, we prepare a time series represen-
tation of their request count change over time. Our interval
length for each sampling point is 3600 seconds (i.e., one
hour). We chose 3600 seconds as the interval length after
experimenting with different values (e.g., 150, 300 etc.).
Before feeding the input directly into the CPD algorithm,
we normalize the data with respect to the local maximum.
Then, we make use of the well-known CUSUM (cumula-
tive sum) robust CPD algorithm that is known to deliver
good results for many application areas [12]. CUSUM is
an online algorithm that detects changes as soon as they oc-
cur. However, since we record the data to a database before
analyzing it, our ofﬂine version of the CUSUM algorithm
yields even more precise results (i.e., the algorithm knows
in advance how the “future” trafﬁc will look like as we have
already recorded it).
Our algorithm to identify change points works as fol-
lows: First, we iterate over every time interval t = 3600
seconds, from the beginning to the end of the time series.
For each interval t, we calculate the average request count
−
for the previous  = 8 time intervals and the trafﬁc pro-
P
t
+
for the subsequent  intervals. We chose  to be
ﬁle P
t
8 hours based on the insight that a typical day consists of
three important periods: working time, evening and night.
Second, we compute the distance d(t) between P
+
t .
More precisely:
−
t and P
(cid:2)
−
t =
P
i=1
Pt−i

(cid:2)
+
t =
P
i=1
(cid:3)(cid:3)P
t − P
−
+
t
(cid:3)(cid:3)
d(t) =
Pt+i

(1)
The ordered sequence of values d(t) forms the input to
the CUSUM algorithm. Intuitively, a change point is a time
interval t for which d(t) is sufﬁciently large and is a local
maximum.
The CUSUM algorithm requires two parameters. The
ﬁrst parameter is an upper bound (local max) for the nor-
mal, expected deviation of the present (and future) trafﬁc
from the past. For each time interval t, CUSUM adds d(t)−
local max to a cumulative sum S. The second parame-
ter determines the upper bound (cusum max) that S may
reach before a change point is reported. To determine a suit-
able value for local max, we require that each individual
trafﬁc feature may deviate by at most allowed avg dev =
0.1. Based on this, we can calculate the corresponding value
dim × allowed avg dev2. Since in our
local max =
application, there is only one dimension, the local max =
allowed avg dev. For cusum max, we use a value of 0.4.
Note that we determined the values for allowed avg dev
and cusum max based on empirical experiments and mea-
surements.
(cid:4)
The CPD algorithm outputs the average request count
for each period a change is detected and the time that the
change occurs. Since we employ the CPD algorithm for
two purposes (namely to detect short-lived domains and do-
mains that have repeating patterns), we run it twice. We ﬁrst
use the global scope time series and then the local scope
time series as input. When the CPD is run with global time
series, it can detect short-lived domains. Short-lived do-
mains tend to have two sudden behavioral changes, whereas
domains that are continuously queried have multiple change
points. On the other hand, to detect the domains with re-
peating patterns on their local scope time series, we asso-
ciate the number of the changes and the standard deviation
of the durations of the detected changes.
3.1.2 Detecting similar daily behavior
A typical technique to measure the level of similarity of two
time series is to calculate the distance between them [23].
To determine whether a domain produces similar time series
every day, we calculate the Euclidean Distance between ev-
ery pair of time series of a domain. Euclidean Distance is a
popular distance measuring algorithm that is often used in
data mining [14, 37, 43].
We ﬁrst need to break the local time series produced for
each domain into daily time series pieces. Each day starts at
00:00 am and ﬁnishes at 23:59 pm. Assuming that a domain
has been queried n days during our analysis period, and di,j
is the Euclidean Distance between ith day and jth day, the
ﬁnal distance D is calculated as the average of (n − 1) ∗
(n−2)/2 different distance pairs, as shown in the following
formula:
n(cid:2)
D = (
n(cid:2)
i=1
j=i+1
di,j)/((n − 1) ∗ (n − 2)/2)
(2)
Using the Euclidean Distance, the results are sensitive to
small variations in the measurements (e.g., 1000 requests
between 9 and 10 am compared to 1002 requests between
the same time period may fail to produce a correct similarity
result although the difference is not signiﬁcant). A common
technique to increase the correctness of the results is to ap-
ply preprocessing algorithms to the time series before cal-
culating the Euclidean Distance [17]. In our preprocessing
step, we transform the time series T = t1, t2, ..., tn, where
n is number of intervals, into two phases. In the ﬁrst phase,
we perform offset translation by subtracting the mean of the
series from each value (i.e., T = T −mean(T )). In the sec-
ond phase, we scale the amplitude by dividing each value by
the variance (i.e., T = (T − mean(T ))/std(T )) [17].
3.2 DNS Answer-Based Features
The DNS answer that is returned by the server for a
domain generally consists of several DNS A records (i.e.,
mappings from the host to IP addresses). Of course, a do-
main name can map to multiple IP addresses. In such cases,
the DNS server cycles through the different IP addresses in
a round robin fashion [1] and returns a different IP map-
ping each time. This technique is useful in practice for load
balancing.
Malicious domains typically resolve to compromised
computers that reside in different Autonomous Systems
(ASNs), countries, and regions. The attackers are oppor-
tunistic, and do not usually target speciﬁc countries or IP
ranges. Whenever a computer is compromised, it is added
as an asset to the collection. Also, attackers typically use
domains that map to multiple IP addresses, and IPs might
be shared across different domains.
With this insight, we extracted four features from the
DNS answer (i.e., feature set F2). The ﬁrst feature is the
number of different IP addresses that are resolved for a
given domain during the experiment window (Feature 5).
The second feature is the number of different countries that
these IP addresses are located in (Feature 6). The third fea-
ture is the reverse DNS query results of the returned IP ad-
dresses (Feature 7). The fourth feature (Feature 8) is the
number of distinct domains that share the IP addresses that
resolve to the given domain. Note that Features 5, 6, and 7
have been used in previous work (e.g., [?, 11, 30, 36] ).
Although uncommon, benign domains may also share
the same IP address with many other domains. For example,
during our experiments, we saw that one of the IP addresses
that belongs to networksolutions.com is shared by 10, 837
distinct domains. This behavior is sometimes exhibited by
web hosting providers and shared hosting services.
To determine if an IP is used by a shared hosting service,
we query Google with the reverse DNS answer of the given
IP address. Legitimate web hosting providers and shared
hosting services are typically ranked in the top 3 query an-
swers that Google provides. This helps us reduce false pos-
itives.
3.3 TTL Value-Based Features
Every DNS record has a Time To Live (TTL) that spec-
iﬁes how long the corresponding response for a domain
should be cached. It is recommended that the TTL is set
to between 1 and 5 days so that both the DNS clients
and the name servers can beneﬁt from the effects of DNS
caching [3].
Systems that aim for high availability often set the TTL
values of host names to lower values and use Round-Robin
DNS. That is, even if one of the IP addresses is not reach-
able at a given point in time, since the TTL value expires
quickly, another IP address can be provided. A represen-
tative example for such systems are Content Delivery Net-
works (CDNs).
Unfortunately, setting lower TTL values and using
Round-Robin DNS is useful for the attackers as well. Us-
ing this approach, malicious systems achieve higher avail-
ability and become more resistant against DNS blacklist-
ing (DNSBL) [5] and take downs. For example, Fast-Flux
Service Networks (FFSN) [36] are malicious systems that
abuse Round-Robin DNS.
Most techniques to detect FFSNs are based on analyzing
abnormal usage patterns of Round-Robin DNS. More pre-
cisely, to label a domain as being a member of an FFSN,
previous research [30, 36] expects to observe a low TTL
usage combined with a constantly growing DNS answers
list (i.e., distinct IP addresses).
We extracted ﬁve features from the TTL value included
in the DNS answers (see Table 1). The average TTL
usage feature (Feature 9) was introduced in previous re-
search [30]. The rest of the features (i.e., Features 10, 11,
12, 13) have not been used before in previous work.
During our experiments with large volumes of DNS traf-
ﬁc, we observed that frequent TTL changes are exhibited by
malicious networks that have a sophisticated infrastructure.
In such networks, some of the bots are selected to be prox-
ies behind which other services (e.g., command and control
servers) can be hidden. The managers of such malicious
networks assign different levels of priorities to the proxy
bots by setting lower TTL values to the hosts that are less
reliable. For example, there is a good chance that a proxy
running on an ADSL line would be less reliable than a proxy
running on a server running in a university environment.
To determine the validity of our assumption about this
type of TTL behavior, we tracked the Conﬁcker domains
for one week. We observed that different TTL values were
returned for the IPs associated with the Conﬁcker domains.
While the static IP addresses have higher TTL values, the
dynamic IP addresses, that are most probably assigned to
home computers by Internet service providers, have lower
TTL values (e.g., adsl2123-goland.net would have a lower
TTL value than a compromised host with the domain name
workstation.someuniversity.edu).
We observed that the number of TTL changes and the to-
tal number of different TTL values tend to be signiﬁcantly
higher in malicious domains than in benign domains. Also,
malicious domains exhibit more scattered usage of TTL val-
ues. We saw that the percentage for the usage of some spe-
ciﬁc ranges of TTL values is often indicative of malicious
behavior. Based on our empirical measurements and exper-
imentations, the TTL ranges that we investigate are [0, 1),
[1, 10), [10, 100), [100, 300), [300, 900), [900, inf). Mali-
cious domains tend to set their TTL values to lower values
compared to benign domains.