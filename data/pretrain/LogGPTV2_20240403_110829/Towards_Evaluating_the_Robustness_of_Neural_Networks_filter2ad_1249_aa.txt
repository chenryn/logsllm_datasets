title:Towards Evaluating the Robustness of Neural Networks
author:Nicholas Carlini and
David A. Wagner
2017 IEEE Symposium on Security and Privacy
Towards Evaluating the Robustness
of Neural Networks
Nicholas Carlini
David Wagner
University of California, Berkeley
ABSTRACT
Original Adversarial
Original Adversarial
Neural networks provide state-of-the-art results for most
machine learning tasks. Unfortunately, neural networks are
vulnerable to adversarial examples: given an input x and any
(cid:2)
target classiﬁcation t, it is possible to ﬁnd a new input x
that is similar to x but classiﬁed as t. This makes it difﬁcult
to apply neural networks in security-critical areas. Defensive
distillation is a recently proposed approach that can take an
arbitrary neural network, and increase its robustness, reducing
the success rate of current attacks’ ability to ﬁnd adversarial
examples from 95% to 0.5%.
In this paper, we demonstrate that defensive distillation does
not signiﬁcantly increase the robustness of neural networks
by introducing three new attack algorithms that are successful
on both distilled and undistilled neural networks with 100%
probability. Our attacks are tailored to three distance metrics
used previously in the literature, and when compared to pre-
vious adversarial example generation algorithms, our attacks
are often much more effective (and never worse). Furthermore,
we propose using high-conﬁdence adversarial examples in
a simple transferability test we show can also be used to
break defensive distillation. We hope our attacks will be used
as a benchmark in future defense attempts to create neural
networks that resist adversarial examples.
I. INTRODUCTION
Deep neural networks have become increasingly effective
at many difﬁcult machine-learning tasks. In the image recog-
nition domain, they are able to recognize images with near-
human accuracy [27], [25]. They are also used for speech
recognition [18], natural language processing [1], and playing
games [43], [32].
However, researchers have discovered that existing neural
networks are vulnerable to attack. Szegedy et al. [46] ﬁrst
noticed the existence of adversarial examples in the image
classiﬁcation domain: it is possible to transform an image by
a small amount and thereby change how the image is classiﬁed.
Often, the total amount of change required can be so small as
to be undetectable.
The degree to which attackers can ﬁnd adversarial examples
limits the domains in which neural networks can be used.
For example, if we use neural networks in self-driving cars,
adversarial examples could allow an attacker to cause the car
to take unwanted actions.
The existence of adversarial examples has inspired research
on how to harden neural networks against these kinds of
Fig. 1. An illustration of our attacks on a defensively distilled network.
The leftmost column contains the starting image. The next three columns
show adversarial examples generated by our L2, L∞, and L0 algorithms,
respectively. All images start out classiﬁed correctly with label l, and the three
misclassiﬁed instances share the same misclassiﬁed label of l + 1 (mod 10).
Images were chosen as the ﬁrst of their class from the test set.
attacks. Many early attempts to secure neural networks failed
or provided only marginal robustness improvements [15], [2],
[20], [42].
Defensive distillation [39] is one such recent defense pro-
posed for hardening neural networks against adversarial exam-
ples. Initial analysis proved to be very promising: defensive
distillation defeats existing attack algorithms and reduces their
success probability from 95% to 0.5%. Defensive distillation
can be applied to any feed-forward neural network and only
requires a single re-training step, and is currently one of
the only defenses giving strong security guarantees against
adversarial examples.
In general, there are two different approaches one can take
to evaluate the robustness of a neural network: attempt to prove
a lower bound, or construct attacks that demonstrate an upper
bound. The former approach, while sound, is substantially
more difﬁcult to implement in practice, and all attempts have
required approximations [2], [21]. On the other hand, if the
© 2017, Nicholas Carlini. Under license to IEEE.
DOI 10.1109/SP.2017.49
39
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
attacks used in the the latter approach are not sufﬁciently
strong and fail often, the upper bound may not be useful.
In this paper we create a set of attacks that can be used
to construct an upper bound on the robustness of neural
networks. As a case study, we use these attacks to demon-
strate that defensive distillation does not actually eliminate
adversarial examples. We construct three new attacks (under
three previously used distance metrics: L0, L2, and L∞) that
succeed in ﬁnding adversarial examples for 100% of images
on defensively distilled networks. While defensive distillation
stops previously published attacks, it cannot resist the more
powerful attack techniques we introduce in this paper.
This case study illustrates the general need for better
techniques to evaluate the robustness of neural networks:
while distillation was shown to be secure against the current
state-of-the-art attacks, it fails against our stronger attacks.
Furthermore, when comparing our attacks against the current
state-of-the-art on standard unsecured models, our methods
generate adversarial examples with less total distortion in
every case. We suggest that our attacks are a better baseline
for evaluating candidate defenses: before placing any faith in a
new possible defense, we suggest that designers at least check
whether it can resist our attacks.
We additionally propose using high-conﬁdence adversarial
examples to evaluate the robustness of defenses. Transfer-
ability [46], [11] is the well-known property that adversarial
examples on one model are often also adversarial on another
model. We demonstrate that adversarial examples from our
attacks are transferable from the unsecured model
to the
defensively distilled (secured) model. In general, we argue
that any defense must demonstrate it is able to break the
transferability property.
We evaluate our attacks on three standard datasets: MNIST
[28], a digit-recognition task (0-9); CIFAR-10 [24], a small-
image recognition task, also with 10 classes; and ImageNet
[9], a large-image recognition task with 1000 classes.
Figure 1 shows examples of adversarial examples our tech-
niques generate on defensively distilled networks trained on
the MNIST and CIFAR datasets.
In one extreme example for the ImageNet classiﬁcation task,
we can cause the Inception v3 [45] network to incorrectly
classify images by changing only the lowest order bit of each
pixel. Such changes are impossible to detect visually.
To enable others to more easily use our work to evaluate
the robustness of other defenses, all of our adversarial example
generation algorithms (along with code to train the models we
use, to reproduce the results we present) are available online
at http://nicholas.carlini.com/code/nn robust attacks.
un-distilled networks.
• We propose using high-conﬁdence adversarial examples
in a simple transferability test to evaluate defenses, and
show this test breaks defensive distillation.
• We systematically evaluate the choice of the objective
function for ﬁnding adversarial examples, and show that
the choice can dramatically impact the efﬁcacy of an
attack.
II. BACKGROUND
A. Threat Model
Machine learning is being used in an increasing array of
settings to make potentially security critical decisions: self-
driving cars [3], [4], drones [10], robots [33], [22], anomaly
detection [6], malware classiﬁcation [8], [40], [48], speech
recognition and recognition of voice commands [17], [13],
NLP [1], and many more. Consequently, understanding the
security properties of deep learning has become a crucial
question in this area. The extent to which we can construct
adversarial examples inﬂuences the settings in which we may
want to (or not want to) use neural networks.
In the speech recognition domain, recent work has shown
[5] it is possible to generate audio that sounds like speech to
machine learning algorithms but not to humans. This can be
used to control user’s devices without their knowledge. For
example, by playing a video with a hidden voice command,
it may be possible to cause a smart phone to visit a malicious
webpage to cause a drive-by download. This work focused
on conventional techniques (Gaussian Mixture Models and
Hidden Markov Models), but as speech recognition is increas-
ingly using neural networks, the study of adversarial examples
becomes relevant in this domain. 1
In the space of malware classiﬁcation, the existence of
adversarial examples not only limits their potential application
settings, but entirely defeats its purpose: an adversary who is
able to make only slight modiﬁcations to a malware ﬁle that
cause it to remain malware, but become classiﬁed as benign,
has entirely defeated the malware classiﬁer [8], [14].
Turning back to the threat to self-driving cars introduced
earlier, this is not an unrealistic attack: it has been shown that
adversarial examples are possible in the physical world [26]
after taking pictures of them.
The key question then becomes exactly how much distortion
we must add to cause the classiﬁcation to change. In each
domain, the distance metric that we must use is different. In
the space of images, which we focus on in this paper, we
rely on previous work that suggests that various Lp norms are
reasonable approximations of human perceptual distance (see
Section II-D for more information).
We assume in this paper that the adversary has complete
access to a neural network, including the architecture and all
paramaters, and can use this in a white-box manner. This is a
conservative and realistic assumption: prior work has shown it
1Strictly speaking, hidden voice commands are not adversarial examples
because they are not similar to the original input [5].
This paper makes the following contributions:
• We introduce three new attacks for the L0, L2, and L∞
distance metrics. Our attacks are signiﬁcantly more effec-
tive than previous approaches. Our L0 attack is the ﬁrst
published attack that can cause targeted misclassiﬁcation
on the ImageNet dataset.
• We apply these attacks to defensive distillation and dis-
cover that distillation provides little security beneﬁt over
40
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
is possible to train a substitute model given black-box access
to a target model, and by attacking the substitute model, we
can then transfer these attacks to the target model. [37]
Given these threats, there have been various attempts [15],
[2], [20], [42], [39] at constructing defenses that increase the
robustness of a neural network, deﬁned as a measure of how
easy it is to ﬁnd adversarial examples that are close to their
original input.
In this paper we study one of these, distillation as a defense
[39], that hopes to secure an arbitrary neural network. This
type of defensive distillation was shown to make generating
adversarial examples nearly impossible for existing attack
techniques [39]. We ﬁnd that although the current state-of-the-
art fails to ﬁnd adversarial examples for defensively distilled
networks, the stronger attacks we develop in this paper are
able to construct adversarial examples.
B. Neural Networks and Notation
A neural network is a function F (x) = y that accepts an
input x ∈ R
m. The model F
also implicitly depends on some model parameters θ; in our
work the model is ﬁxed, so for convenience we don’t show
the dependence on θ.
n and produces an output y ∈ R
In this paper we focus on neural networks used as an m-
class classiﬁer. The output of the network is computed using
the softmax function, which ensures that the output vector y
satisﬁes 0 ≤ yi ≤ 1 and y1+···+ym = 1. The output vector y
is thus treated as a probability distribution, i.e., yi is treated as
the probability that input x has class i. The classiﬁer assigns
the label C(x) = arg maxi F (x)i to the input x. Let C
(x)
be the correct label of x. The inputs to the softmax function
are called logits.
∗
We use the notation from Papernot et al. [39]: deﬁne F
to be the full neural network including the softmax function,
Z(x) = z to be the output of all layers except the softmax (so
z are the logits), and
F (x) = softmax(Z(x)) = y.
A neural network typically 2 consists of layers
F = softmax ◦ Fn ◦ Fn−1 ◦ ··· ◦ F1
where
Fi(x) = σ(θi · x) + ˆθi
for some non-linear activation function σ, some matrix θi of
model weights, and some vector ˆθi of model biases. Together
θ and ˆθ make up the model parameters. Common choices of σ
are tanh [31], sigmoid, ReLU [29], or ELU [7]. In this paper
we focus primarily on networks that use a ReLU activation
function, as it currently is the most widely used activation
function [45], [44], [31], [39].
We use image classiﬁcation as our primary evaluation
domain. An h×w-pixel grey-scale image is a two-dimensional
2Most simple networks have this simple linear structure, however other
more sophisticated networks have more complicated structures (e.g., ResNet
[16] and Inception [45]). The network architecture does not impact our attacks.
vector x ∈ R
hw, where xi denotes the intensity of pixel i
and is scaled to be in the range [0, 1]. A color RGB image
is a three-dimensional vector x ∈ R
3hw. We do not convert
RGB images to HSV, HSL, or other cylindrical coordinate
representations of color images: the neural networks act on
raw pixel values.
C. Adversarial Examples
∗
Szegedy et al. [46] ﬁrst pointed out
the existence of
adversarial examples: given a valid input x and a target
t (cid:5)= C
(cid:2)
(x), it is often possible to ﬁnd a similar input x
(cid:2) are close according to some
such that C(x
) = t yet x, x
(cid:2) with this property is known
distance metric. An example x
as a targeted adversarial example.
(cid:2)
A less powerful attack also discussed in the literature
instead asks for untargeted adversarial examples: instead of
classifying x as a given target class, we only search for an
(cid:2) are close. Untargeted
input x
attacks are strictly less powerful than targeted attacks and we
do not consider them in this paper. 3
(cid:2) so that C(x
(x) and x, x
) (cid:5)= C
∗
(cid:2)
Instead, we consider three different approaches for how to
choose the target class, in a targeted attack:
• Average Case: select the target class uniformly at random
among the labels that are not the correct label.
• Worst Case: perform the attack against all
• Best Case: perform the attack against all incorrect classes,
and report the target class that was least difﬁcult to attack.
incorrect
classes, and report the target class that was most difﬁcult
to attack.
In all of our evaluations we perform all
three types of
attacks: best-case, average-case, and worst-case. Notice that
if a classiﬁer is only accurate 80% of the time, then the best
case attack will require a change of 0 in 20% of cases.
On ImageNet, we approximate the best-case and worst-case
attack by sampling 100 random target classes out of the 1,000
possible for efﬁciency reasons.
D. Distance Metrics
In our deﬁnition of adversarial examples, we require use
of a distance metric to quantify similarity. There are three
widely-used distance metrics in the literature for generating
adversarial examples, all of which are Lp norms.
The Lp distance is written (cid:6)x − x
(cid:2)(cid:6)p, where the p-norm
(cid:6) · (cid:6)p is deﬁned as
(cid:4) 1
(cid:2)
n(cid:3)
(cid:6)v(cid:6)p =
|vi|p
p
.
In more detail:
i=1
3An untargeted attack is simply a more efﬁcient (and often less accurate)
method of running a targeted attack for each target and taking the closest.
In this paper we focus on identifying the most accurate attacks, and do not
consider untargeted attacks.
41
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
1) L0 distance measures the number of coordinates i such
that xi (cid:5)= x
(cid:2)
i. Thus, the L0 distance corresponds to the
number of pixels that have been altered in an image.4
Papernot et al. argue for the use of the L0 distance
metric, and it is the primary distance metric under which
defensive distillation’s security is argued [39].
2) L2 distance measures the standard Euclidean (root-
(cid:2). The L2 dis-
mean-square) distance between x and x
tance can remain small when there are many small
changes to many pixels.
This distance metric was used in the initial adversarial
example work [46].
3) L∞ distance measures the maximum change to any of
the coordinates:
(cid:6)x − x
(cid:2)(cid:6)∞ = max(|x1 − x
1|, . . . ,|xn − x
(cid:2)
n|).
(cid:2)
For images, we can imagine there is a maximum budget,
and each pixel is allowed to be changed by up to this
limit, with no limit on the number of pixels that are
modiﬁed.
Goodfellow et al. argue that L∞ is the optimal distance
metric to use [47] and in a follow-up paper Papernot et
al. argue distillation is secure under this distance metric
[36].
No distance metric is a perfect measure of human perceptual
similarity, and we pass no judgement on exactly which dis-
tance metric is optimal. We believe constructing and evaluating
a good distance metric is an important research question we
leave to future work.
However, since most existing work has picked one of these
three distance metrics, and since defensive distillation argued
security against two of these, we too use these distance metrics
and construct attacks that perform superior to the state-of-the-
art for each of these distance metrics.
When reporting all numbers in this paper, we report using
the distance metric as deﬁned above, on the range [0, 1]. (That
is, changing a pixel in a greyscale image from full-on to full-
off will result in L2 change of 1.0 and a L∞ change of 1.0,
not 255.)
E. Defensive Distillation
We brieﬂy provide a high-level overview of defensive distil-
lation. We provide a complete description later in Section VIII.
To defensively distill a neural network, begin by ﬁrst
training a network with identical architecture on the training
data in a standard manner. When we compute the softmax
while training this network, replace it with a more-smooth
version of the softmax (by dividing the logits by some constant
T ). At the end of training, generate the soft training labels by
evaluating this network on each of the training instances and
taking the output labels of the network.
4In RGB images, there are three channels that each can change. We count
the number of pixels that are different, where two pixels are considered
different
if any of the three colors are different. We do not consider a
distance metric where an attacker can change one color plane but not another
meaningful. We relax this requirement when comparing to other L0 attacks
that do not make this assumption to provide for a fair comparison.
Then, throw out the ﬁrst network and use only the soft
training labels. With those, train a second network where
instead of training it on the original training labels, use the
soft labels. This trains the second model to behave like the ﬁrst
model, and the soft labels convey additional hidden knowledge
learned by the ﬁrst model.
The key insight here is that by training to match the ﬁrst
network, we will hopefully avoid over-ﬁtting against any of the
training data. If the reason that neural networks exist is because
neural networks are highly non-linear and have “blind spots”
[46] where adversarial examples lie, then preventing this type
of over-ﬁtting might remove those blind spots.
In fact, as we will see later, defensive distillation does not
remove adversarial examples. One potential reason this may
occur is that others [11] have argued the reason adversarial
examples exist is not due to blind spots in a highly non-linear
neural network, but due only to the locally-linear nature of
neural networks. This so-called linearity hypothesis appears
to be true [47], and under this explanation it is perhaps less
surprising that distillation does not increase the robustness of
neural networks.
F. Organization
The remainder of this paper is structured as follows. In
the next section, we survey existing attacks that have been
proposed in the literature for generating adversarial examples,
for the L2, L∞, and L0 distance metrics. We then describe
our attack algorithms that
the same three distance
metrics and provide superior results to the prior work. Having
developed these attacks, we review defensive distillation in
more detail and discuss why the existing attacks fail to ﬁnd ad-
versarial examples on defensively distilled networks. Finally,
we attack defensive distillation with our new algorithms and
show that it provides only limited value.
target
III. ATTACK ALGORITHMS
A. L-BFGS
Szegedy et al. [46] generated adversarial examples using
box-constrained L-BFGS. Given an image x, their method
(cid:2) that is similar to x under L2 distance,
ﬁnds a different image x
yet is labeled differently by the classiﬁer. They model the
problem as a constrained minimization problem:
minimize (cid:6)x − x
(cid:2)
such that C(x
(cid:2)(cid:6)2
2
) = l
(cid:2) ∈ [0, 1]
x