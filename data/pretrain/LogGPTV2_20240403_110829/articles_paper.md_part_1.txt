# Delft University of Technology
## LogChunks: A Data Set for Build Log Analysis
### Authors
- Carolin E. Brandt
- Annibale Panichella
- Andy Zaidman
- Moritz Beller

### DOI
10.1145/3379597.3387485

### Publication Date
2020

### Document Version
Submitted Manuscript

### Published In
Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020

### Citation (APA)
Brandt, C. E., Panichella, A., Zaidman, A., & Beller, M. (2020). LogChunks: A Data Set for Build Log Analysis. In Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020 (pp. 583-587). https://doi.org/10.1145/3379597.3387485

### Important Note
To cite this publication, please use the final published version if applicable. Please check the document version above.

### Copyright
This work is intended for personal use only. Downloading, forwarding, or distributing the text or any part of it without the consent of the authors and/or copyright holders is not permitted, unless the work is under an open content license such as Creative Commons.

### Takedown Policy
If you believe this document breaches copyrights, please contact us with details. We will remove access to the work immediately and investigate your claim.

### Abstract
Build logs are textual by-products generated during a software build process, often as part of a Continuous Integration (CI) pipeline. These logs are a crucial source of information for developers when debugging and understanding build failures. Recently, efforts have been made to partially automate this time-consuming, manual activity using rule-based or information retrieval techniques. 

We believe that having a common dataset to compare different build log analysis techniques will advance research in this area and ultimately increase our understanding of CI build failures. In this paper, we present LogChunks, a collection of 797 annotated Travis CI build logs from 80 GitHub repositories across 29 programming languages. Each build log in LogChunks contains a manually labeled chunk describing the reason for the build failure. The dataset has been externally validated with the developers who caused the original build failures.

The breadth and depth of the LogChunks dataset make it a valuable benchmark for automated build log analysis techniques. Currently, there is only anecdotal evidence on the performance of these techniques, and no dataset is available to support the creation of such a benchmark. Following Sim et al., a benchmark provides an opportunity to "increase the scientific maturity of the area" by evaluating and comparing research contributions.

### Keywords
- CI
- Build Log Analysis
- Build Failure
- Chunk Retrieval

### Introduction
Continuous Integration (CI) has become a standard practice in software engineering. Many software projects use CI to detect bugs early, improve developer productivity, and enhance communication. CI builds generate logs that report the results of various sub-steps within the build process. These logs contain valuable information, such as descriptions of compile errors or failed tests. However, build logs can be verbose and large, sometimes exceeding 50MB of ASCII text, making them difficult for direct human consumption. To support developers and researchers in efficiently using the information within build logs, at least semi-automated methods are needed to retrieve the relevant chunks of the log.

### Creating LogChunks
#### Log Collection
In this section, we describe how we gathered the logs and conducted the manual labeling process. All steps were automated using Ruby scripts.

**Repository Sampling:**
We targeted mature GitHub repositories that use Travis CI. To avoid personal and toy projects, we selected popular projects based on the number of users who starred the project. We queried GHTorrent for the most popular languages on GitHub and then the most popular repositories for each language. For LogChunks, we queried GHTorrent from 2018-04-01 for the three most popular repositories of each of the 30 most popular languages to cover a broad range of development languages. Examples of the resulting repositories include Microsoft/TypeScript, git/git, and jwilm/alacritty.

**Build Sampling:**
To sample builds for LogChunks, we kept the ten most recent builds with a failed status. We checked up to 1,000 builds per repository to ensure predictable termination of the log collection.

**Log Sampling:**
Travis CI builds consist of multiple jobs that actualize the build process in different environments. Therefore, the outcome from different jobs might differ. For each build in LogChunks, we downloaded the log of the first job that had the same state as the overall build. We inspected the collected build logs and discarded logs from three repositories. One repository had only one failed build, and two others had empty build logs on Travis CI. In total, we collected 797 logs from 80 repositories spanning 29 languages.

#### Manual Labeling
After collecting the build logs, the first author manually labeled the text chunk that describes why the build failed. She then assigned search keywords and structural categories to each log chunk.

**Chunk That Describes Why the Build Failed:**
For each repository, the labeler skimmed through the build logs and copied out the first occurrence of a description of why the build failed. She preserved whitespaces and special characters, as they might be crucial for detecting the targeted substring. To support the learning of regular expressions, the labeler aimed to start and end the labeled substring at consistent locations around the fault description.

**Search Keywords:**
To extract the search keywords, the labeler considered the chunk and ten lines above and below it. The task was to note down three strings they would search for (using "grep") to find the failure description. The strings should appear in or around the chunk and are case-sensitive. No limitations were placed on the search strings; spaces were allowed.

**Structural Category:**
To label the structural categories, the labeler was presented with the chunk and the surrounding context for all logs from a repository. They were asked to assign numerical categories based on whether the chunk had the same structural representation.

### Validation
We validated our collected data points in an iterative fashion. First, we performed an initial inter-rater reliability study with the second author of this paper. Our learnings from this initial internal study are that:
1. It is important and challenging to adequately communicate all decisions and assumptions on how to and which data to label.
2. There can be different legitimate viewpoints on which log chunk constitutes the cardinal error and which keywords are best to use.

These insights informed the design of a second, larger cross-validation study. We contacted over 200 developers whose commits triggered the builds represented in LogChunks. Using the Travis API, we collected the commit information for each build and grouped all commits triggered by one developer. We sent out emails to these developers, including links to the corresponding commits, the build overview, and the log file. We asked the developers to fill out a short form if our extraction was not correct. In the survey, we asked the developer to paste the log part actually describing the failure reason or describe in their own words why our original extraction was incorrect.

**Results:**
From 2019-10-15 to 2019-10-17, we sent out emails to 246 developers. Of these, 32 could not be delivered. We performed the sending in three batches and used the first author’s academic email address as the sender. All emails were specific to each recipient, and we sent only one email per recipient. We received answers from 61 developers, corresponding to 144 build logs, with a response rate of 24.8%. Compared to typical response rates for cold calling in software engineering, this is very high. We believe that personalization and ease of use for the participants are the main reasons for this—simply clicking on a link.

### Conclusion
The LogChunks dataset provides a comprehensive and validated resource for researchers and developers working on automated build log analysis. The breadth and depth of the dataset make it a valuable benchmark for evaluating and comparing different techniques, ultimately advancing the field and increasing our understanding of CI build failures.

### Figure 2: Example Log Chunk
```
========DIFF========
-=-=-=-=-
005+ Parameter #1 [ $flags]
005− Parameter #1 [ $ar_flags]
-=-=-=-=-=
FAIL Bug#71412 ArrayIterator reflection
TEST 9895/13942 [2/2 test workers running]
-=-=-=-=-=
```

This figure shows a log chunk from the same structural category as the log chunk presented in Table 1. We inserted the special marker “-=-=-=-=-=” to separate the log chunk from its context.