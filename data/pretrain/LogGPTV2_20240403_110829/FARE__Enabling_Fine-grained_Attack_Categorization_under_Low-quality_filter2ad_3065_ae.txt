6 ± 0
6 ± 0
4 ± 0
6 ± 0
2
6 ± 0
4 ± 0
4 ± 0
5 ± 0.82
5 ± 0.47
6 ± 0
5 ± 0.47
4 ± 0
7 ± 2.05
5 ± 0
6 ± 1.7
4 ± 0
MEAN AMIS AND STANDARD DEVIATIONS OBTAINED BY VARYING M(cid:48) IN FARE AND Unsup.FARE.
10 ± 1.89
5 ± 1.41
7 ± 2.36
4 ± 0
6 ± 0
5 ± 0.47
5 ± 0
5 ± 0.92
5 ± 0
5 ± 0.47
5 ± 0.47
5 ± 0.47
4
Intrusion (N = 9)
1
8 ± 1.25
6 ± 1.69
6 ± 0
6 ± 0
6 ± 0
4 ± 0
6 ± 2.44
6 ± 0
4
2
Malware (N = 6)
6 ± 0
6 ± 0
4 ± 0
5 ± 0
4 ± 0
5 ± 0.47
5 ± 0
5 ± 5.44
7
4 ± 0
7 ± 1.25
16 ± 3.77
15 ± 2.49
TABLE II.
Methods
FARE
MixMatch+
Ladder+
DNN+
TABLE III.
# Neighb. Models M(cid:48)
Dataset
nc = (cid:98)n/2(cid:99), 1% labels
ng = (cid:98)n/2(cid:99), 1% labels
Dataset
labels are not used
FARE
Unsup.
FARE
10
20
0.68 ± 0.09
0.74 ± 0.04
0.73 ± 0.01
0.75 ± 0
0.60 ± 0.08
0.71 ± 0.01
50
Malware
0.76 ± 0.01
0.74 ± 0
Malware
0.74 ± 0.01
100
0.75 ± 0.01
0.74 ± 0
All (150)
0.75 ± 0
0.74 ± 0
10
20
0.89 ± 0.10
0.90 ± 0.05
0.88 ± 0.11
0.90 ± 0.05
0.74 ± 0.01
0.74 ± 0
0.83 ± 0.05
0.79 ± 0.02
50
Intrusion
0.88 ± 0.08
0.90 ± 0.05
Intrusion
0.79 ± 0.03
100
0.89 ± 0.05
0.90 ± 0.05
All (150)
0.89 ± 0.05
0.90 ± 0.05
0.78 ± 0.00
0.78 ± 0.00
can lead to misguided data manifold, and highly imbalanced
classes undermine the effectiveness of the clustering ensemble.
When both issues are presented, FARE is less effective. In
summary, for the general coarse-grained label setting, FARE
signiﬁcantly outperforms the baseline methods in terms of
clustering quality and estimating the number of classes. When
the classes are extremely imbalanced, FARE and other base-
lines are less effective in estimating the true number of classes.
Sensitivity to the Number of Neighborhood Models. Ta-
ble III shows the results of FARE and Unsup.FARE with
different number of neighborhood models M under the missing
class/coarse-grained label settings. We vary the M(cid:48) = M − 1
neighborhood models from clustering algorithms. We can
observe that the performance of FARE is robust with respect
to the number of neighborhood models. As we add more
neighborhood models to the ensemble,
the mean AMI is
increasing and the standard deviation is decreasing, but only by
a small margin. This means with 20 or even 10 neighborhood
models, the performance of FARE is already good. Similar to
FARE, Unsup.FARE is also not sensitive to M(cid:48).
V. REAL-WORLD TEST: FRAUD DETECTION
Following the controlled experiments, we next describe
our experience of the initial deployment and testing of FARE
in collaboration with a real-world online service JD.com.
Company JD.com is a large e-commerce service with hundreds
of millions of active users. We work together to apply FARE
to identify the ﬁne-grained classes of fraudulent accounts,
especially the previously-unknown types of fraud. As the initial
testing effort, we apply FARE on an internal dataset of 200,000
active users. Below, we describe our testing methodology, and
key observations and discoveries.
Dataset
The dataset contains
200,000 active users randomly sampled from the e-commerce
site database. Each user is represented as a 264-dimensional
feature vector. The feature vector is encoded using their
internal feature engineering method. As the speciﬁc details of
the feature engineering process are not revealed to us (which is
conﬁdential information), we only provide a high-level descrip-
tion here. The features are extracted from three different types
of information: 1) product information (e.g., product brand and
product category), 2) shipping information (e.g., shipping ad-
dress and carrier information), and 3) purchasing information
(e.g., price, amount, discounts, and time).
from Company JD.com.
The dataset has a very small portion of labels, including
0.5% of conﬁrmed fraudulent users, and 0.1% of trustworthy
TABLE IV.
GROUP-A REPRESENTS THE FRAUDULENT ACCOUNTS
IDENTIFIED BY FARE; GROUP-B REPRESENTS THE CONFIRMED
LEGITIMATE USERS. WE RECORD THE LOGIN ATTEMPT RATE (LAR) AND
THE AUTHENTICATION PASS RATE (APR) FOR BOTH GROUPS.
1-month
1-week
1-day
Group
A: FARE-detected
B: Conﬁrmed-legit.
(LAR, APR)
(LAR, APR)
(LAR, APR)
(20.9%, 0.0%)
(22.1%, 100%)
(25.3%, 0.0%)
(27.9%, 100%)
(39.3%, 0.0%)
(30.9%, 100%)
users. The remaining 99.4% of users are unlabelled. First,
0.5% of the accounts are labeled as “fraudulent”. This label
is based on JD.com’s customer service department — they
have received complaints on these 0.5% accounts who were
conducting fraudulent activities in the last two months (with
further conﬁrmations from the security team). About 0.1% of
remaining users accounts are labeled as “trusted” since they are
associated with company JD.com’s enterprise partners or VIP
customers. This dataset represents the common challenges we
described before: only a small portion of labels are available
and the labels are likely to be coarse-grained and biased.
A/B Test Experiments. Using this dataset, our goal is to
pinpoint the unlabeled users who also conducted fraudulent
activities in the past
two months but have not yet been
complained by online retailers through the customer service.
To validate whether FARE can truly identify those ac-
counts, we design an A/B test experiment for two groups of
users. Group-A is the fraudulent accounts that FARE identiﬁed
from the unlabeled user sets, and Group-B is the labeled
trustworthy users. For both groups of users, we revoke their
sign-in cookies, and force them to re-enter their passwords,
and use their registered phone numbers to perform two-factor
authentication through SMS code. Then, we keep monitoring
the login activities of both groups of users for one month after
the forced re-login. During the monitoring period, we record
the login attempt rate (LAR) as well as the authentication pass
rate (APR). Here, the LAR indicates the percentage of the users
who have correctly entered their passwords when performing
log-in. The APR speciﬁes, among users who attempted the
log-in, the percentage of sign-in sessions with the correct two-
factor authentication code.
The rationale behind the A/B experiment is that attackers
behind the fraud campaigns usually purchase a large corpus of
fake accounts from third-party vendors to conduct malicious
activities. When the third-party vendors create these fake
accounts for sale, they needed to register the accounts by
using the phone numbers under their control. When selling
these accounts,
the third-party vendors would provide the
account names and passwords so that the buyers can log in
11
to these accounts. However, if JD.com forces a two-factor
authentication after the account delivery, the buyers would not
be able to receive the SMS code tied to each of the accounts,
and thus cannot use these accounts to continue their campaigns
to snatch coupons, promote illegitimate products, or write fake
reviews. It should be noticed that while re-authentication is a
powerful tool, it cannot be excessively used. When blindly
triggering re-authentication to all users, it could jeopardize
normal users’ experience and signiﬁcantly increase the burden
of the customer service department. This is because legitimate
users may sometimes change their phone numbers and forgot
to update their online proﬁles. Even if such normal users only
take a small portion, considering the hundreds of millions of
active users in JD.com, the absolute number is still very large.
They can easily overwhelm the customer service if the re-
authentication is triggered at the same time.
A/B Experiment Results. While our controlled experiments
in Section IV-C have shown FARE’s good performance, we
still want to stay conservative in this initial real-world testing.
Speciﬁcally, we want to suppress the potential false positives
of FARE since false positives disrupt the customer service’s
daily operations. As a result, JD.com permitted us to initialize
SMS re-authentication for an entire cluster of users only if this
FARE-identiﬁed cluster contains at least 5% of the already-
conﬁrmed fraudulent accounts. While this approach may sig-
niﬁcantly under-report the fraudulent accounts identiﬁed by
FARE, we believe it is the right trade-off for the initial testing.
For the other clusters (e.g., those that contain fraudulent labels
but do not meet the 5% threshold), they are still valuable for
further analysis, but are excluded from the A/B test.
Under this guideline, FARE revoked 2,000 unlabeled sign-
in sessions and initialized the corresponding re-authentication
through SMS. In Table IV, we show the LAR and APR
of each group across three different
time windows – one
day, one week, and one month. We can observe that for
the conﬁrmed trusted users (Group-B), the return sign-in rate
across a month is 30% with a 100% of success rate for passing
the SMS re-authentication. On the contrary, for the FARE-
detected fraudulent users (Group-A), the return rate is about
10% higher, but the success rate of SMS re-authentication is
0%. This implies that the users FARE detected are highly likely
to be the fake accounts associated with fraudulent activities.
Manual Analysis and Observations. In addition to our A/B
experiment, we also devote efforts to manual examinations.
We focus on accounts that failed the SMS re-authentication
and analyzed their history logs. While we are not allowed to
provide the precise numbers and statistics of the discovered
fraudulent activities, we want to provide qualitative results
regarding our key ﬁndings.
First, for many clusters of the newly identiﬁed fraudulent
accounts, accounts in each cluster usually have the same login
time and come from the same or similar sets of IP addresses.
This implies the user accounts in the same cluster are likely
conducted by a single entity using automated programs. Sec-
ond, we ﬁnd that certain groups of fraudulent users would
heavily apply coupons on their purchases. For almost all of
their purchased items, they applied an abnormal amount of
coupons to signiﬁcantly reduce the purchasing price. More im-
portantly, users in the same group even share the same physical
shipping addresses. These clusters are likely to represent the
organized efforts to (automatically) collect coupons, purchase
products in bulk, and then resell them with higher prices.
Third, we also discover fraudulent clusters that regularly buy
products from certain retailers and leave positive reviews. More
importantly, after leaving the positive reviews, these accounts
then ﬁle product returns and get a refund. We suspect these
accounts are colluding with the retailers for promoting their
products. Fourth, by analyzing the historical activities of these
fraudulent accounts, we were surprised to discover that many
products have mistagged prices. For example, some products
owned by the e-commerce site were mistagged with a low
price for weeks without being noticed by the product team.
The fraudulent accounts have been exploiting these mistagged
prices to subside their purchases. These mistagged prices are
previously unknown to JD.com. JD.com has started to actions
to perform systematic detection of mistagged prices.
VI. DISCUSSION
Post-clustering Processing. The goal of FARE is to cate-
gorize the input dataset into ﬁne-grained clusters, and help
the analysts to derive high-quality labels. After FARE is
applied, the post-processing is to either align the obtained
clusters with the known classes in the “given labels” or
assign them with new labels. Two strategies can be applied