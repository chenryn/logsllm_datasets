mostly focus on client to client attacks in this paper, DDIO
could be exploited in other settings as well. Section IX looks
at other threat models where our NetCAT can potentially apply
to applications on the target server processor (rather than other
clients) as well as other PCIe devices.
Our example attack (Section IV) abuses RDMA technology
in the NIC to control the memory location which a transmitted
packet accesses, as well as the low-latency offered by today’s
high-speed networks. RDMA is now available in the clouds of
many major providers and many data centers such as Azure,
Oracle, Huawei and Alibaba [28, 29, 30, 31]. In virtualized
cloud settings, NetCAT can target any VM on the target server,
as long as it can communicate with only one of these VMs
through a virtualized RDMA interface. In addition, if the
attacker’s VM (or virtualized server) is connected to a storage
server with RDMA using protocols such as SMBDirect [32]
or NFS [33], then NetCAT enables an attacker to spy on
other clients that connect to the storage server. Similarly, cloud
key-value service [34] and applications that integrate RDMA
to improve their performance, including big data [35, 36],
machine learning [37], and database [38] could be abused by
NetCAT-like attacks.
IV. ATTACK OVERVIEW
the fact
that
Our goal
is to exploit
the DDIO-enabled
application server in Figure 2 has a shared resource (the LLC)
between the CPU cores and the PCIe devices. We will show
that by abusing the sharing, we can leak sensitive information
from the LLC of the application server. There are many
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:59 UTC from IEEE Xplore.  Restrictions apply. 
22
y
r
o
m
e
M
n
i
a
M
A
M
D
R
i
n
o
g
e
R
RDMA Application
Server
CPU
Core 1 
Core 2 
...  Core N 
LLC
DDIO Portion
IB 
NIC
NIC
NIC
e
b
o
r
P
+
e
m
i
r
P
s
t
e
k
c
a
P
s
t
e
k
c
a
P
H
S
S
IB 
NIC
Server(cid:2)under
control of
Attacker
NIC
(cid:2)Victim 
Machine 
Fig. 2: Assumed topology for attacks
potential ways to exploit DDIO. For instance, an attacker with
physical access to the victim machine could install a malicious
PCIe device to directly access the LLC’s DDIO region. Our
aim in this paper is to show that a similar attack is feasible even
for an attacker with only remote (unprivileged) network access
to the victim machine, without the need for any malicious PCIe
devices.
To this end, we make use of RDMA in modern NICs.
RDMA bypasses the operating system at
the data plane,
providing remote machines with direct read and write access to
a previously speciﬁed memory region. The OS is responsible
for setting up and protecting this RDMA region. However, as
we show later in more detail, when DDIO is enabled, RDMA
reads and writes have access not only to the pinned memory
region but also to parts of the LLC. Mellanox further motivates
the use of RDMA [39] for minimizing the performance-
degradation due to defenses required to protect against the
latest speculative execution attacks [15, 40]. Ironically, RDMA
makes it easier to perform network-based cache attacks as we
show in this paper.
Figure 2 illustrates our target topology, which is common in
data centers. The attacker controls a machine which communi-
cates over RDMA to an application server that supports DDIO
and also services requests from a victim on a separate NIC.
With this, we show that we can successfully spy on another
PCIe device. However, we do not rely on such separation, i.e.,
we could also spy on the same PCIe device where we issue
our PRIME+PROBE packets. In our adversarial attack, we will
assume that a victim client types in sensitive information over
an ssh connection. The aim of the attacker is ﬁnding out the
keystrokes typed by the victim client using the PRIME+PROBE
packets. There are three main challenges that we need to
overcome for implementing our attack:
C1 Inner workings of DDIO. Our attack requires a precise
knowledge of the effects of DDIO operations, the DDIO
allocation limitation, and the feasibility of detecting cache
hits and misses over the network.
C2 Remote PRIME+PROBE. Our attack requires us to re-
motely build cache eviction sets for our PRIME+PROBE
attack, without knowledge of virtual or physical addresses
of the RDMA memory region on the remote machine,
introducing unique challenges in measuring cache activity
over the network.
C3 End-to-end attack. To implement an end-to-end attack,
we require a solid understanding of what sensitive data
may reside in the DDIO-reachable part of the LLC and
is eligible for leaking.
We address these challenges in the following sections:
C1: Inner workings of DDIO. Section V analyzes DDIO in
depth. First, we ﬁnd suitable remote read and write primitives
using DDIO. Next, we show that it is possible to detect LLC
hits over the network via DDIO. Furthermore, we conﬁrm the
known DDIO restrictions on allocating writes, and we discover
that the precise percentage of the LLC accessible to allocating
writes differs between Intel CPU models.
C2: Remote PRIME+PROBE In Section VI, we use our newly
obtained understanding of DDIO to remotely create cache
eviction sets. We adapt existing PRIME+PROBE algorithms to
cope with the challenges of network noise and with the slower
read/write operations compared to native code.
C3: End-to-end attack Finally, we showcase various DDIO
attack scenarios in Section VII. First, we build a covert channel
between a network client and an unnetworked, cooperating
sandboxed process on a remote machine. Second, we build
a covert channel between two cooperating network clients
running in two separate networks, without any direct com-
munication paths. Third, we describe an adversarial keystroke
timing attack on a victim SSH connection of another client
by remotely measuring cache activity caused by SSH packets,
described in Section VIII. Our adversarial setup is sketched in
Figure 2.
V. REVERSE ENGINEERING DDIO
To remotely measure cache activity, we require remote
read/write primitives provided by the PCIe device’s DDIO ca-
pabilities. This section discusses how we build these required
primitives to mount our attack, while in the process elaborating
on the relevant details of DDIO.
A. Access Latencies
The ﬁrst step in implementing our attack is to determine
whether we can measure the timing difference between cache
hits and memory reads over the network. We used two servers
(Intel Xeon Silver 4110) running Ubuntu 18.04.1 LTS, each
with a Mellanox ConnectX-4 Inﬁniband NIC (produced in
2016). We used one of the servers as an RDMA server and the
other one as a client. As a baseline, the ib_read_lat latency
benchmark measured an average latency between our two
machines of 1,550 ns, with a standard deviation of 110ns and
23
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:59 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I: Overview of CPU models used in our experiments, and
summary of our experimental ﬁndings on the DDIO allocation ways
and allocation limit.
CPU
Xeon Haswell E5-2630 v3
Xeon Skylake Silver 4110
LLC
20 MB (20 ways), incl
11 MB (11 ways), n-incl
DDIO
2 ways (10%)
2 ways (18%)
and then reads those same addresses, measuring whether
these reads are served from cache. We start with n = 0
and increment n after each round. The expectation is that
this allows us to determine the DDIO write allocation limit
by ﬁnding the n where the number of cache hits becomes
constant.
We perform this experiment on two machines equipped with
Intel Xeon E5-2630 v3 processors running CentOS 7.4, each
with a Mellanox ConnectX-3 Inﬁniband NIC (produced in
2014). Each machine’s LLC has a size of 20 MB and is 20-
way set associative according to the speciﬁcations. As shown
in Figure 4, starting with n = 2 (Write 0-1), we see a constant
pattern of two addresses being served from the cache and the
rest of the addresses being served from main memory. The
memorygram is darker for low latencies and lighter for high
latencies. This experiment yields strong evidence that there are
two DDIO ways on our test machines. This is also supported
by the original Intel documentation [26], which states that the
write allocation limit is 10% of the LLC (i.e., 2 ways out of
a total of 20 ways is 10% of the LLC). On the Intel Xeon
Silver 4110, our experiments also reveal two DDIO ways,
which, given that this model uses an 11 MB and 11-way set
associative LLC, means that the DDIO write allocation limit
is instead around 18.2% of the LLC, as shown in Table I.
Figure 4 additionally yields insights into the cache replace-
ment policy used for the DDIO region in the LLC. As we can
see, the last two written values are served from the LLC. Our
further experiments with random write and read operations
suggest that the replacement policy is most likely evicting the
least recently used (LRU) cache lines in the DDIO region.
VI. REMOTE PRIME+PROBE
In order to launch a successful remote PRIME+PROBE
attack, we need write and read primitives on a memory region
on the remote machine. As described in Section V, RDMA
gives us these capabilities on the LLC.
A. Creating a Remote Eviction Set
The ﬁrst step of PRIME+PROBE is to build cache eviction
sets [1]. In our case, under the write allocation restrictions
of DDIO, we do not build eviction sets for all cache sets
in the LLC, but only for the limited number of cache ways
accessible by DDIO. Building eviction sets and later on using
it
to leak data relies on basic RDMA operations, so any
application that uses one-sided RDMA and allows an RDMA
client to write data can be used for NetCAT attacks. We
exemplify this on RDMA-memcached [46], a key-value store
with RDMA support. RDMA-Memcached implements GET
and SET operations where memory allocation is split into
Fig. 3: Distributions of DDIO reads served from the LLC and from
main memory as measured over the network. Distribution estimated
with kernel density over 99th percentile data.
a 99th percentile of 1,810 ns. To send one-sided RDMA reads
and (in later experiments) writes, we use libibverbs.
In our ﬁrst experiment, we iterated over 50,000 memory
addresses 150 times. In each iteration, we issued two RDMA
reads to the same memory address and measured the time
taken for each result to arrive back at the client. We measured
no signiﬁcant difference between the two accesses. Closer
inspection revealed that this is because an address read via
DDIO that is absent in the LLC is served directly from main
memory without being allocated in the LLC (i.e., subsequent
reads to an uncached memory location remain uncached).
In a second experiment, we instead issued the following
sequence of operations in each iteration: Read(x) - Write(x)
- Read(x). The idea is that
the ﬁrst read is served from
main memory, while the read after the cache-allocating write
is served from the LLC, allowing us to measure a baseline
difference between memory reads and cache hits. Figure 3
shows that
the resulting distributions of the two types of
reads are distinguishable. Section VI discusses mechanisms
to further distinguish LLC-based reads from memory reads.
B. DDIO Cache Ways
As previously discussed, DDIO limits write allocations
to prevent cache thrashing from PCIe devices. Because this
limitation impacts our ability to create eviction sets and mount
cache attacks, we study the mechanics of the limitation. To
this end, we build a pool of addresses that map to the same
cache set and are accessible via RDMA. We achieve this
by allocating a large buffer on the RDMA server and then
applying the method of Maurice et al. [41] to ﬁnd pages with
the same LLC color. We then remap the RDMA buffer so
that the RDMA client can directly access these addresses via
DDIO, allowing us to remotely create eviction sets without
yet knowing the exact algorithm needed to achieve this in the
general case. With the help of this colored RDMA buffer, we
are able to explore the layout of DDIO ways in the LLC.
More speciﬁcally, our experiment repeatedly writes to n
addresses in the colored buffer (within the same cache set)
24
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:59 UTC from IEEE Xplore.  Restrictions apply. 
algorithm quite slow, especially when proﬁling all the available
cache sets (magnitude of hours). We therefore introduced a
number of optimizations.
Optimization 1 As a ﬁrst optimization, we introduce a
forward-selection algorithm that creates a possible smaller set
S that evicts address x. We start from an empty set S and
on each iteration we add one more addresses to S until we
measure an eviction. This selection process reduces the num-
ber of addresses in S from thousands to hundreds on average.
The optimization works well because the DDIO ways are a
subset of the full cache ways, e.g., on an Intel Xeon E5-2630
v3 CPU, we only have to ﬁnd two out of the potential twenty
addresses that form an eviction of address x. This reduced set
S is then the input of the backwards-selection algorithm. The
forward-selection algorithm is detailed in Appendix A.
Optimization 2 The second optimization concerns
the
backwards-selection algorithm. In the original algorithm, the
set S is ﬁrst written completely and then written again while
leaving one address s out on each iteration. This compares
the cache miss time of x to the miss/hit time of x depending
on whether S \ s is still an eviction set. In our approach, we
instead ﬁrst measure a cache hit on x by writing and reading
x, and then compare the access time to S \ s. This works
because S always evicts x on a successful proﬁling run, while
reducing the number of write operations in this step by a factor
of two.
Optimization 3 As a third optimization,
instead of only
removing one address s from set S in the backwards-selection
process, we implement a dynamically adjusting algorithm that
removes multiple addresses from S at the same time. The
algorithm increases the number of addresses to be removed
by ten after the previous iteration successfully decreases S.
Contrary, the algorithm decreases the number of addresses to
be removed by one if the previous iteration did not decrease
S. The number of addresses to be removed is bound to a
maximum of half the size of S. The adjustment algorithm
is disabled when the size of S is small, as adjusting it then
can impact the runtime negatively with additional iterations