is trained with reinforcement learning techniques to take streaming
audio input and forecast the adversarial perturbation. However, this
work only focused on launching untargeted attacks and does not
consider over-the-air attack either. In addition, the generated adver-
sarial perturbation still requires synchronization with the audio to
some extent, which is hard to achieve for streaming-speech attacks.
Universal audio adversarial attacks. To circumvent the time
constraint brought by individual adversarial attacks, several studies
proposed universal adversarial perturbation: an adversarial perturba-
tion that is expected to work with any input speech signal, causing
the model to make false prediction. Specifically, in speech recog-
nition tasks, existing studies use the iterative greedy algorithm
[32] to make arbitrary speech to be mis-classified [47] or falsely
transcribed [35]. In speaker recognition tasks, a recent study [51]
proposed to add audio-agnostic universal perturbations on arbitrary
enrolled speaker’s voice input in order to make the model identify
the speaker as any adversary-desired speaker label. To handle audio
inputs with varying audio length during optimization, Xie et al.
[51] proposed to use repeated copies of a universal noise pattern to
construct the adversarial perturbation, which is then cropped to fit
the length of the given audio at test time. However, the effective-
ness of this attack still relies on the synchronization between the
adversarial perturbation and the audio input, which is achieved by
first mixing the two signals into a single audio adversarial example
and then testing the model on the generated single-channel audio.
Thus, this is not feasible when launching streaming-speech attacks
in practice. In summary, all the aforementioned universal attacks
assume that the universal perturbation should be perfectly aligned
with all the input audio signals. This essentially requires the ad-
versary to have the knowledge of the exact timestamp to launch
the attack. Moreover, to accommodate the adversarial perturbation,
the input signal needs to be tailored to match the length of the
perturbation. These strict temporal constraints have prevented the
attack from being effectively launched in many practical attack
scenarios.
Over-the-air audio adversarial attacks. Most existing audio
adversarial attacks are based on digital domain assumption, i.e.,
feeding adversarial examples to the model directly. They are most
Universal SubsecondAdversarial PerturbationSynchronization-free InjectionRandom DelayStreaming Speech?Speaker Recognition ModelSpeech Command Recognition ModelIntelligent Audio System132likely to lose their effectiveness after practical over-the-air play-
back due to the accompanying audio distortions (e.g., attenua-
tion, multi-path effect, and ambient noises). Although several stud-
ies [2, 11, 54, 55] evaluated the performance of audio adversarial
attacks via over-the-air propagation, the real-world distortion fac-
tors were not thoroughly investigated in the adversarial example
generation process, leaving the practicality still very limited. To
address this issue, room impulse response (RIR) was integrated
[31, 52] to increase the robustness of the generated audio adver-
sarial examples. Moreover, a recent study [12] showed that the
transmission range can be further extended using domain adaption
algorithms. Although these studies successfully achieved physical
attacks, they still require prior knowledge of the input speech sig-
nal. The input signal also needs to be mixed with the well-aligned
adversarial perturbation before being played by a loudspeaker to en-
sure synchronization. They cannot work in the scenario where the
adversary wants to inject adversarial perturbations on live human
speech to deceive intelligent audio systems.
Our solution. Unlike existing studies, to the best of our knowl-
edge, we are the first to launch a targeted audio adversarial attack in
an audio-agnostic and synchronization-free manner. The generated
universal adversarial perturbations can be added to any part of
arbitrary speech signal (e.g., streaming speech), forcing the DNN
model to output the adversary-desired label. This is different from
the existing attacks that require the added adversarial perturbation
to be of the same duration as the input audio as we only inject
a very short adversarial perturbation (e.g., ~0.5 seconds) without
the need of synchronization. These properties can largely extend
our possible attack scenarios, which makes attacking live-human’s
voice inputs feasible.
3 BACKGROUND
3.1 Problem Formalization
An intelligent audio system (e.g., speaker or speech command
recognition) can be modeled as a function f (·) which takes an au-
dio input waveform x ∈ [−1, 1]n of n samples and outputs the
probability score P = [p0, p1, ..., pm−1], where pi ∈ [0, 1], and
i =0 pi = 1 for a set of m classes. The audio input will be rec-
ognized by the model as the class with the highest probability
construct an audio adversarial example x′ ∈ [−1, 1]n by adding
a perturbation δ to the original audio input x with the following
properties:
m−1
score (i.e., ypr ed = arдmax(cid:0)f (x)(cid:1)). The adversarial objective is to
Subsecond Perturbation. Instead of using a perturbation with
the same duration of the audio input as is used in existing studies
(e.g., [10, 12, 31, 38, 52]), we added a much shorter perturbation
δ ∈ [−1, 1]l , where l ≪ n, to make it sound more inconspicuous.
Targeted. By introducing the adversarial perturbation, the recog-
nition result can be changed from the true label y to the adversary-
desired target label yt , namely, arдmax(cid:0)f (x)(cid:1) = y but arдmax(cid:0)f (x′)(cid:1) =
yt , where yt (cid:44) y.
Synchronization-free. When launching the attack in a real-
world scenario, such as injecting adversarial perturbation to the
streaming speech input, it’s impossible to add the perturbation
at a particular point of the streaming speech. In other words, the
synchronization between the adversarial perturbation and the audio
input cannot be guaranteed, and there usually exists a time delay τ
between the two signals. Therefore, the adversarial perturbation
signal δ[t], as a function of time t, needs to be applicable at any
time during the audio input playback regardless of the time delay:
arдmax(cid:0)f (x[t] + δ[t − τ])(cid:1) = yt , where τ ∈ [0, n − l].
Universal. In most practical scenarios, observing the audio in-
put in advance is not plausible. Therefore, the constructed adversar-
ial perturbation is required to be effective on arbitrary audio input
(e.g., streaming speech).
Unnoticeable. The distortion introduced by the added adver-
sarial perturbation δ should be relatively small to make the attack
unnoticeable to human. As the duration of the generated perturba-
tion is very short, we deliberately make it sound like environmental
sound (e.g., bird singing, car horns, or HVAC noises) to further hide
itself while being played back over the air.
Robust for Over-the-air Attack. In order to launch a physical
attack, the adversarial perturbation should be robust enough to
survive real-world distortions brought by physical playbacks such
as reverberations and ambient noises.
3.2 Threat Model
The aforementioned properties enable the proposed attack to be
launched in a broad range of real-world scenarios. Unlike existing
studies that require to play the well-crafted adversarial example (i.e.,
the original speech mixed with the adversarial perturbations), in
this paper, we circumvent all the three major limitations described
in Section 1 and consider a more common and serious live-speech-
involved scenario where an adversary seeks to launch the attack
by only playing the constructed adversarial perturbation. The con-
structed perturbation is audio-agnostic and does not need to be
synchronized with the audio input, which largely extends its poten-
tial attack functionality. Additionally, the length of the perturbation
is in a subsecond level and is made to sound like environmental
sounds (e.g., phone’s notification sound), making the whole attack
process inconspicuous to people.
Possible Attack Scenarios. Our attack is applicable to the
static-speech attack scenario (Figure 1(a)), where the adversary needs
to create the adversarial example (i.e., the audio input mixed with
the adversarial perturbation) and then play it back through a loud-
speaker. In this scenario, the adversarial example could be played
inconspicuously by the adversary to make the intelligent audio sys-
tems inadvertently recognize and obey the adversary-desired inten-
tion. For instance, the speaker recognition module could be fooled to
mistakenly recognize that the speech was uttered by an adversary-
desired speaker. The speech command recognition module can also
be controlled to execute the mistakenly recognized malicious com-
mand. More importantly, the proposed universal, synchronization-
free, and targeted adversarial attack makes the streaming-speech
attack scenario (Figure 1(b)) feasible. In this scenario, by playing
the well-crafted universal adversarial perturbation through a loud-
speaker, the adversary can compromise the intelligent audio system
while interacting with an actual person. Similarly, the perturbation
stealthily played by the loudspeaker can make either speaker recog-
nition or speech command recognition modules mistakenly recog-
nize the streaming audio input as the adversary-desired speaker or
malicious command respectively. This constructed perturbation is
very short (subsecond level) and sounds like environmental sounds,
which can be periodically played by a nearby loudspeaker (smart
TV loudspeaker, in-vehicle/in-ceiling/on-wall loudspeakers) with-
out raising the victim’s suspicion. It can also be embedded in the
audio tracks of regular media (e.g., Youtube videos, radios or TVs),
potentially deceiving all the intelligent audio systems exposed to
the media.
Challenges of Adversary. Due to the inherent sequence order
and time-varying behavior of live speech, launching such an attack
in the streaming-speech attack scenario poses several challenges to
the adversary: (1) Independence of Audio Input. In the streaming-
speech scenario, the adversary cannot anticipate the upcoming
audio from the actual speaker (i.e., each speech is unique and will be
only uttered once), which prohibits the adversary from optimizing
the adversarial perturbation for specific audio signal in advance.
This requires the adversary to generate a universal adversarial
perturbation that can remain effective on arbitrary audio inputs
from the user. (2) Independence of Emission Time. The adversary
has no prior knowledge on the exact time when the speech will be
uttered. As a result, the generated adversarial perturbation needs to
have synchronization-free properties, meaning that the adversarial
perturbation should remain effective regardless of the emission
time during the interaction between the user and the system.
Adversary’s Capability. The attack workflow of most existing
studies (e.g., [12, 31, 52, 55]) are two-fold: first, calculate the adver-
sarial perturbation δ for a given audio input x and synchronously
apply the adversarial perturbation to get the adversarial example
x′ = x +δ; second, play the prefabricated audio adversarial example
through a loudspeaker. However, in our proposed attack, we assume
that the adversary has no control over the streaming audio input x
in terms of speech content and emission time. Moreover, the adver-
sary can have some prior knowledge of the possible environmental
sounds (e.g., bird singing, car horns, or HVAC noises) of the target
intelligent system so as to make the adversarial perturbation more
difficult to be noticed by people. In addition, to craft the adversarial
perturbation, we assume that the adversary has knowledge over
the architecture and the parameters of the model (i.e., a white-box
setting), as is used in most previous studies (e.g., [10, 12, 31, 38, 52]).
3.3 Target Models
Speaker Recognition Model. In this work, we used X-vectors sys-
tem [44] as our target speaker recognition model, which is the state-
of-the-art text-independent DNN-based model and has been used
as baseline in several follow-up studies (e.g., [43, 50, 56]). Specifi-
cally, X-vectors system is composed of three building blocks: Mel-
frequency cepstral coefficients (MFCC) feature extraction, DNN
embedding model, and the probabilistic linear discriminant analy-
sis (PLDA) module. The DNN embedding model is structured by
stacking five time-delayed [36] layers, a statistic pooling layer and
two affine layers. The DNN embedding model is pre-trained in an
end-to-end manner with a categorical cross entropy loss, and a
separately trained PLDA classifier is used to calculate embedding
score. In our X-vectors implementation, we used the pre-trained
embedding model provided in the Kaldi toolkit [37] and used the
first 10 speakers (3 males and 7 females) in the VCTK corpus dataset
[48] as the enrolled speakers. The detailed information about each
speaker is shown in Table 5 in Appendix. Each speaker has about
400 utterances recorded at 48 kHz that were split into training and
testing sets with a ratio of 4 to 1, with each audio being cropped to
1.75 s. After enrollment, the baseline speaker recognition accuracy
on the testing set achieves 97.2%.
Speech Command Recognition Model. The target speech
command recognition model we used is an efficient and light-
weight keyword spotting model based on convolutional neural
networks [39]. This model has been used as the target model for
many existing attacks (e.g., [3, 17, 47]). Specifically, we used Tensor-
flow’s [1] official implementation2, which is trained on the voice
command dataset [49] to classify 10 speech commands: “yes”, “no”,
“up”, “down”, “left”, “right”, “on”, “off”, “stop”, and “go”. The dataset
contains a total number of 46, 278 recordings of 1 s sampled at 16
kHz, 80% of which is used for training. After training, the baseline
command recognition accuracy on the remaining testing samples
is able to achieve 89.0%.
4 DESIGN OF ADVPULSE
4.1 Synchronization-free Subsecond Targeted
Adversarial Perturbation
Feasibility of Subsecond Adversarial Perturbation. Conven-
4.1.1
tionally, the problem with crafting a targeted audio adversarial
perturbation is based on how to modify each data sample of the
audio input signal to make the intelligent audio model recognize it
as the target class [10]. Explicitly, given an audio input x ∈ [−1, 1]n
and the target label yt , the adversary can solve the following formu-
lation to obtain the targeted adversarial perturbation δ ∈ [−1, 1]n:
minimize dBx(δ),
subject to arдmax(cid:0)f (x + δ)(cid:1) = yt ;
x + δ ∈ [−1, 1]n,
(1)
where dBx(δ) = dB(δ)−dB(x) and it is used to measure the relative
loudness of the perturbation comparing to the audio input. This
can be achieved by minimizing the following objective function (a
relaxation of Equation 1):
minimize L(cid:0)f (x + δ), yt
(cid:1) + α · ||δ||2,
(2)
where L(cid:0)f (x + δ), yt
subject to
||δ||2 ≤ ϵ,
(cid:1) is the loss function representing the distance
between the model output of the adversarial example x + δ and
the target label yt , α is the scaling coefficient, || · ||2 denotes the L2
norm, and ϵ controls the upper bound of the perturbation.
However, the requirement of modifying the entire audio natu-
rally prohibits synchronization-free. Since the audio input x and
the adversarial perturbation δ are required to have the same length
and if δ is delayed by time τ, a segment of δ with length τ will be
no longer acting on x. To launch a synchronization-free adversar-
ial attack on intelligent audio systems, we thus need to first solve
this question: Is it possible to only modify part of the input signal,
desirably a short segment, to fool the model?
Commonly, intelligent audio systems process speech signals
into frames, with each frame being handled by the DNN model
separately (e.g., the time-delayed neural network structure used
2https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/
speech_commands
(a) Speech-part perturbation
(b) Non-speech-part perturbation
time delays with a step size of 1 digital sample (i.e., the smallest
unit for time measurement in audio). Through this process, the
adversarial perturbation will learn to produce a generic acoustic
feature characterizing the target class to fool the model regardless
of timing conditions. In other words, the adversarial perturbation
can be injected anywhere in the streaming audio input for deceiving
intelligent audio systems, which is the essential requirement for
streaming-speech attack scenario.
Figure 3: Illustration of adding a subsecond adversarial per-
turbation on the audio input, causing the model to mistak-
enly recognize the speaker (Spk-1) as the target (Spk-8): (a)