FROM pcpoints;
interchangable one). Rather than
ape OGC well-known text, I’ve
{"pcid":1,
decided the emitting JSON is more
"pt": [-126, 45, 34, 4]}
likely to allow people to use pre-
existing parsing functions.
You can pull any dimension from a
point using the dimension name.
SELECT PC_Get(pt, ‘z’)
This feature is the gateway to point
FROM pcpoints;
filtering, as we’ll see.
34
If you have the pointcloud_postgis
extension enabled, you can cast
SELECT
pcpoints to postgis points, which is
ST_AsText(pt::geometry)
FROM pcpoints; useful for visualization or
integration analysis.
POINT Z (-126 45 34)
If we add one more point to our
INSERT INTO pcpoints (pt) points table, we can use the two
VALUES ( points in there to make a two-point
PC_MakePoint(1,
patch, with a patch aggregate,
ARRAY[-127, 46, 35, 5]
So first we add a second point,
)
);
And then here we can use the
CREATE TABLE pcpatches PC_Patch function to aggregate our
AS two points into a new patch, in a
SELECT new table.
PC_Patch(pt) AS pa
FROM pcpoints;
And this is what it looks like in
SELECT PC_AsText(pa) JSON. So we’ve taken a set of points
FROM pcpatches; and aggregated them into a patch.
And we can also do the reverse,
taking a patch and turning it into a
{"pcid":1,
"pts":[ tuple set of points,
[-126,45,34,4],
[-127,46,35,5]
]}
using the PC_Explode function.
SELECT
You can use this facility of
PC_AsText(
exploding patches into points, and
PC_Explode(pa))
then aggregating them back into
FROM pcpatches;
patches, to filter by any attribute or
pass the points into PostGIS for
{"pcid":1,
spatial filtering too.
"pt":[-126,45,34,4]}
{"pcid":1,
"pt":[-127,46,35,5]}
Now it’s possible....... that you
aren’t the kind of person to be
excited by SQL examples, so here’s
So What
some real world data loading and
processing using PostgreSQL
pointcloud!
In order to real-world data loaded
into the database, we use the PDAL
open source LIDAR processing
tools, which let us handle multiple
*+"!$
input formats (in this case LAS files)
!"#$%&'()$
*,)-./(#0!$
and output formats (in this case
*,&1-2',34$
PostgreSQL Pointcloud) and also
apply processing chains to the
points on the way through.
For the Natural Resources Canada
project, I wrote a PostgreSQL
!"#$ Pointcloud driver for PDAL, which is
!
now available in the PDAL source
!
"#$%&#'()*)+$,-./+0"(1#$-&#! repository.
2/-&#'(.3$))&#!
In addition to the reader and writer,
"#$%&#'()*)+$,-./+0"(#&4"&#!
we have to use the chipper to break
!
the input file into small chips
suitable for database storage.
The example data was a 420Mb LAS
file of Mt St Helens, with 12 million
 points in it.
host='localhost' dbname='pc' user='pramsey'
mtsthelens I used this PDAL pipeline file for the
26910
load. Note that it uses a chipping
400
filter (in yellow) between the reader
3
32184
 and the writer to break the file up
st-helens.las
EPSG:26910
into smaller patches for database
storage. The writer driver (in blue)
 needs a connection string and
destination table name at a
When the load is done we have a
table like this, with a primary key
Table "public.mtsthelens" and patch data,
Column | Type
--------+------------
id | integer
pa | pcpatch(1)
Indexes:
"mtsthelens_pkey"
PRIMARY KEY, btree (id)
And we can confirm all 12 million
SELECT
points are loaded, and they are
Count(*)
stored in 30971 patches, which
Sum(PC_NumPoints(pa))
look
FROM mtsthelens;
count | sum
-------+----------
30971 | 12388139
like this. Kind of hard to see what’s
going on, the physical context
is this,
and we can look a bit closer
and see the patch lines, like this. In
this load, the chipper ensured that
each patch holds about 400 points,
though we could go higher, up to
about 600 points without passing
the PostgreSQL page size
Now let’s do some analysis!
Mount St Helens is a bit of an odd
mountain, because it doesn’t have
a nice pointy summit. It’s got a rim,
around a caldera.
Question: How tall is the rim?
I digitized a line around the rim, so
I can calculate an average elevation.
I’m going to get the average
elevation of every point within 15
meters of my line.
It’s a multi-stage query, so I use
WITH patches AS (
my favourite SQL syntactic sugar,
SELECT pa
the “WITH” clause to do it in an
FROM
understandable order... first I get
mtsthelens m,
all the patches that intersect my 15
mtsthelens_rim r
meter buffer of the rim line...
WHERE
PC_Intersects(m.pa,
ST_Buffer(r.geom, 15))
),
Which looks like this, several
hundred patches.
And if you look closer, it looks kind
of like this, all the patches that
touch the buffer.
Then I take those patches and
explode them into a set of points...
points AS (
SELECT PC_Explode(pa)
AS pt
FROM patches
),
And filter those points using the
filtered_points AS (
buffer shape,
SELECT PC_Get(pt, 'z')
so here I’m pushing a cast into
AS z
PostGIS to run the ST_Intersects()
FROM points,
filter
mtsthelens_rim r
WHERE
ST_Intersects
(pt::geometry,
ST_Buffer(r.geom, 15))
)
Which gives us just the points
inside the buffer,
And finally summarize the elevation
SELECT avg(z),
of the points, which tells us we had
count(z)
108 thousand points, and an
FROM filtered_points;
average elevation of 2425 meters.
Which is odd,
avg | count
-------------------
2425.086 | 108736
because Google says, that the
elevation of Mt St Helens is 2550
meters...
so, what’s going on? Let’s look at
all the points in our database that
are above 2500 meters
exploding all the patches and
WITH points AS (
finding just the points that are
SELECT PC_Explode(pa)
higher than our elevation threshold
AS pt
FROM mtsthelens
)
SELECT pt::geometry
FROM points
WHERE
PC_Get(pt,'z') > 2500;
And we see that, actually the rim
isn’t flat, it’s tallest at the southern
end and slopes downwards to the
north,
which we can see by taking the
patches, and coloring them
thematically by their average point
elevation
which shows the rim sloping
downwards from a tallest point at
the south.
In order to lower I/O load,
Compression: None compressing the data a bit is an
important concern.
•
none
The compression of a PcPatch is
• determined by the compression
Byte-packed points, ordered point-by-point.
• entry in the schema document.
Equivalent to compressed LAS.
There are three compression modes
right now. Compression of “none”
stores the data just as packed
bytes.
Dimensional compression is the
Compression: Dimensional default. Using dimensional
compression, it’s possible to pack
•
 400 to 600 points into a single
dimensional
patch without going over the 8KB
• PostgreSQL page size. Not
Each dimension separately compressed.
• exceeding the page size can be a
Run-length, common bits, or zlib.
• performance boost, since larger
4-5 times smaller than uncompressed LAS.
objects are copied into a side table
in page-sized chucks and accessed
in a two-phase process.
GHT compression is still
Compression: GeohashTree experimental. The compression
aspects are still not as good as
•
 dimensional, but because the
ght
points are ordered, there’s some
• good possibilities for high speed
Points sorted into a prefix-tree based on
geohash code spatial processing and overview
•
Compression from 2-4 times, depending on generation.
parameters
I’ve shown a number of functions in
PC_Functions action, but some of the most
important are the
•
PC_Get(pcpoint, dimension) → numeric get function, to interogate points,
•
PC_Explode(pcpatch) → pcpoint[] explode to break patches into
• points,
PC_Union(pcpatch[]) → pcpatch
• union to merge patches,
PC_Patch(pcpoint[]) → pcpatch
• patch to merge points,
PC_Intersects(pcpatch, geometry)
intersects to find spatial overlaps,
•
pcpatch::geometry, pcpoint::geometry
and
the casts into postgis geometry.
There’s lots of work on the drawing
Future
board,
•
PC_Transform(pcpatch, pcid) → pcpatch ...
•
PC_Intersection(pcpatch, geometry) → ...
pcpatch
•
PC_Raster(pcpatch, raster, dimension) →
raster
•
PC_FilterBetween(pcpatch, dim, min, max)
→ pcpatch
•
PDAL writer flexibility
PDAL reader queries
And it’s ready for use and abuse
Get It right now.
•
Pointcloud
http://github.com/pramsey/pointcloud
•
PDAL
http://github.com/PDAL/PDAL
•
Questions?