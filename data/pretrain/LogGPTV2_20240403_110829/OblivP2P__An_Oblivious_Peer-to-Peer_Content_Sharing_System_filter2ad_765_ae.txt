1 ν ← s;
2 {adr} ←PosMap −1(ν mod 2L);
3
4
5 end
set T = T∪ tag ← TagMap(adr);
for adr in {adr} do
6 set A =(cid:31)stash, P(ν mod 2L,1),··· , P(ν mod 2L,L)(cid:30);
Initialize an array A, π ← LCA(T,ν);
// tracker generates key shares
for l from 1 to z· L +|stash| do
7
if ∃adr,l = PosMap(adr) then
set k ← KeyMap(adr);
set k = k − k, k $
←− Zq;
compute (σl ,rl ) = OblivSel.Gen(k,π(l));
set k $
←− Zq, compute (σl ,rl )= OblivSel.Gen(k,π(l));
8
9
10
11
12
13
14
15
16 end
17
18
19 end
20
else
end
// Peers generate the new array A
for j from 1 to z· L +|stash| do
set A[ j] = OblivSel.Select(σ j,r j, A);
for j ∈ [m], send A j[1,··· ,|stash|] and A j[|stash| + 1,··· ,L] to peers in
P(ν) and the stash, and update state s;
the path and m peers, |stash| + z· L times. Note that (1)
the blocks in A are encrypted with a freshly-generated
key, and (2) the mapping is not disclosed to any peers in
the path as long as there is one non-colluding peer. Refer
to Algorithm 4 for more detail about the Sync algorithm.
Upload Algorithm. A peer can request the tracker to add
a ﬁle. For this, the tracker selects uniformly at random a
set of m peers. The peer sends the ﬁle in a form of blocks.
Every block is secret shared such that every peer in the
m peers receives a share. The tracker generates a secret
unique to the block, k. The tracker secret shares k to the
m peers. The peers evaluate a seed-homomorphic PRG
on the received shares and add it to the block share. Fi-
nally, the block is appended to a randomly selected peer
in the network to hold a part of the stash.
4.4 Optimization: Handling Bursts
OBLIVP2P-1 has a functional limitation inherited by
ORAMs. Any access cannot be started unless the pre-
vious one has concluded 2. In our case, the tracker can
handle fetching several blocks before starting the Sync
operation. In our setting, we target increasing the P2P
network throughput while leveraging the network storage
and communication. In order to build a scalable system,
we propose several optimizations.
O1: Replication. In Ring ORAM, A = 3 accesses can
be performed before an eviction is required. To support A
parallel accesses, we replicate every block A times in the
tree. This absorbs the fetching access time and allows A
simultaneous accesses, even for requests to the same re-
source. Additionally, we may replicate every block over
A times on different peers, in case that the peer hold-
ing the block is ofﬂine due to churn, and cannot serve
the block to the other peers. Lastly, the network opera-
tor can deploy multiple trackers to serve peers simulta-
neously, which leads to the throughput of OBLIVP2P-1
proportional to the number of trackers.
O2: Pipelining. While the eviction is highly paralleliz-
able in OBLIVP2P-1, an eviction can take a considerable
amount of time to terminate. If we denote by f the av-
erage number of fetch requests in the P2P network, and
by t the time to perform an eviction, then the system can
handle all the accesses if t  1
f and therefore the accesses will be queued and cre-
ates a bottleneck. To address this issue, we create mul-
tiple copies of the buckets that are run with different in-
stances of OBLIVP2P-1 protocol which overlays on the
same network. In the setup phase, every node creates l
copies of its bucket space. Every bucket will be associ-
ated to different versions of OBLIVP2P-1 instantiations.
For example, with replication we can handle A accesses
in parallel on the (same) ith version of the buckets, but
the upcoming accesses will be made on the (i + 1)th ver-
sion. This will absorb the eviction time. To sum up,
having different versions will increase the throughput of
the system to l
f . In order to prevent pipeline stalls, we
need to choose l ≥ t · f in our implementation.
Another aspect (not considered for our implementa-
tion) for further optimizations in our versioning solution
is to distribute the communication overhead of the peers
in the network. In fact, the peers holding blocks at the
higher level of the tree will be accessed more often com-
pared to lower levels. In order to distribute the communi-
cation load on the network peers, peers’ location can be
changed for different versions such that: the peer at the
ith level of the tree in the jth version will be placed at
the (L− i + 1)th level of the tree in the ( j + 1)th version.
O3: Parallelizing Computation across m Peers. The
scalar multiplication in the elliptic curve is expensive
and can easily delay the fetch and sync time. For this,
we consider every peer in the OblivSel as a set of peers.
Whenever there is a need to perform scalar multiplication
over a path, several peers participate in the computation
and only the representative of the set will perform the ag-
gregation. This optimization speeds up the OblivSel to be
proportional to the number of peers’ used to parallelize a
single peer.
5
Implementation and Evaluation
2We do not consider a multi-processor architectures as those con-
sidered in OPRAM literature [56].
Implementation. We implement a prototype of
OBLIVP2P-0 and OBLIVP2P-1 in Python. The im-
USENIX Association  
25th USENIX Security Symposium  955
11
plementation contains 1712 lines of code (LOC) for
OBLIVP2P-0 and 3226 for OBLIVP2P-1 accounting to a
total of 4938 lines measured using CLOC tool [61]. Our
prototype implementation is open source and available
online [34]. As our building block primitives, we im-
plement the Ring ORAM algorithm, IT-PIR construction
and seed-homomorphic PRG. For Ring ORAM, we have
followed the parameters reported by authors [52]. Each
bucket contains z = 4 blocks and s = 5 dummy blocks.
The eviction occurs after every 3 accesses. The blocks
in OBLIVP2P-0 are encrypted using AES-CBC with 256
bit key from the pycrypto library [62]. For implementing
IT-PIR and seed homomorphic PRG in OBLIVP2P-1, we
use the ECC library available in Python [63]. We use the
NIST P-256 elliptic curve as the underlying group.
Experimental Setup. We use the DeterLab network
testbed for our experiments [64]. It consists of 15 servers
running Ubuntu 14.04 with dual Intel(R) Xeon(R) hexa-
core processors running at 2.2 Ghz with 15 MB cache (24
cores each), Intel VT-x support and 24 GB of RAM. The
tracker runs on a single server while each of the remain-
ing servers runs approximately 2400 peers. Every peer
process takes up to 4− 60 MB memory which limits the
maximum network size to 214 peers in our experimental
set up. The tracker is connected to a 128 MBps link and
the peers in each server share a bandwidth link of 128
MBps as well. We simulate the bandwidth link following
the observed BitTorrent trafﬁc rate distribution reported
in [65]. In our experimental setting, multiple peers are
simulated on a single machine hence our reported results
here are conservative. In the real BitTorrent setting, ev-
ery peer has its own separate CPU.
Evaluation Methodology. To evaluate the scalability
and efﬁciency of our system, we perform measurements
for a) the overall throughput of the system b) the la-
tency for Fetch and Sync operations and c) the data
transferred through the tracker for both OBLIVP2P-0
and OBLIVP2P-1. All our results are the average of
50 runs with 95% conﬁdence intervals for each of them.
Along with the experimental results, we plot the theoret-
ical bounds computed based on Table 2. This helps us
to check if our experiments match our theoretical expec-
tations.
In addition, we perform separate experiments
to demonstrate the effect of our optimizations on the
throughput of our OBLIVP2P-1 protocol. For our ex-
periments in this section, we leverage the technical opti-
mization introduced in Section 4.4.
We vary the number of peers in the system from 24 to
214 peers (capacity of our testbed) and extrapolate them
to 221 peers. Note that, when increasing the number of
peers, we implicitly increase the total data size in the en-
tire network which is computed as the number of peers ×
the block size. That is, our P2P network handles a total
data size that spans from 16 KB to 32 GB. For our eval-
uation, we consider each peer holds one ORAM bucket
because of the limited available memory. In reality, ev-
ery peer can hold more buckets. Note that, we linearly
extrapolate our curves to show the expected results for
larger number of peers starting from 215 − 221 (shown
dotted in the Figures) , and therefore larger data size in
the network. Aligned to the chunks in BitTorrent, we
select our blocksize as 128 KB, 512 KB and 1 MB.
5.1 Linear Scalability with Peers
The throughput is an important parameter in designing a
scalable P2P protocol. We deﬁne the throughput, as the
number of bits that the system can serve per second.
From Figure 3a, we observe that the throughput of
OBLIVP2P-0 decreases with the increase in the total
number of peers in the network. For a network size
of 214 peers, the experimental maximum throughput is
0.91 MBps. As we extrapolate to larger network size,
the maximum throughput decreases, e.g., for 221 peers,
the throughput is 0.64 MBps. This shows that as the
network size increases, the tracker starts queuing the re-
quests that will eventually lead to a saturation. However,
for OBLIVP2P-1, the maximum throughput for network
size of 214 is 3.19 MBps and is 3.29 MBps when extrap-
olated to 221 peers. The throughput increases as there
are more peers available in the network to distribute the
computation costs. The throughput shows a similar be-
haviour for blocksize of 128 KB and 1 MB (as shown
in Figure 5). Hence, we expect OBLIVP2P-1 to provide
better throughput in a real setting where more compu-
tational and communication capacity for each peer can
be provisioned. The throughput values for OBLIVP2P-1
are calculated after applying all the 3 optimizations dis-
cussed in Section 4.4. The behaviour of the theoretical
throughput matches our experimental results. The theo-
retical throughput has higher values as it does not capture
the network latency in our test environment.
Result 1. Our results show that the centralized proto-
col is limited in scalability and cannot serve a large net-
work. Whereas, the throughput for OBLIVP2P-1 lin-
early scales (0.15− 3.39 MBps) with increasing number
of peers (25 − 221) in the network.
Result 2. For a block of size 512 KB and 214 peers,
OBLIVP2P-1 serves around 7 requests / second which
can be enhanced with multiple copies of ORAM trees in
the network.
Remark. The throughput may be acceptable to privacy-
conscious users (e.g., whistleblowers), where privacy
concerns outweigh download / upload latencies. As
long as the number of request initiators is small, the
perceived throughput remains competitive with a non-
956  25th USENIX Security Symposium 
USENIX Association
12
(cid:17)
(cid:16)
(cid:15)
(cid:14)
(cid:8)
(cid:13)
(cid:12)
(cid:11)
(cid:10)
(cid:9)
(cid:8)
(cid:5)
(cid:7)
(cid:2)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:1)(cid:4)(cid:8)
(cid:1)(cid:7)(cid:4)
(cid:1)(cid:8)
(cid:1)(cid:7)
(cid:1)(cid:2)(cid:3)(cid:5)(cid:6)
(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:5)(cid:6)
(cid:1)(cid:22)(cid:23)(cid:24)(cid:20)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:28)(cid:23)(cid:30)
(cid:1)(cid:22)(cid:23)(cid:24)(cid:20)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:28)(cid:23)(cid:31)
(cid:32)(cid:33)(cid:23)(cid:24)(cid:20)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:28)(cid:23)(cid:30)
(cid:32)(cid:33)(cid:23)(cid:24)(cid:20)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:28)(cid:23)(cid:31)
(cid:12)
(cid:11)
(cid:10)
(cid:9)
(cid:8)
(cid:7)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:1)(cid:6)(cid:9)(cid:10)(cid:3)
(cid:1)(cid:5)(cid:9)(cid:6)(cid:7)
(cid:1)(cid:2)(cid:9)(cid:5)(cid:6)
(cid:1)(cid:8)(cid:2)(cid:5)
(cid:1)(cid:5)(cid:8)(cid:3)
(cid:1)(cid:2)(cid:5)(cid:7)
(cid:1)(cid:3)(cid:6)