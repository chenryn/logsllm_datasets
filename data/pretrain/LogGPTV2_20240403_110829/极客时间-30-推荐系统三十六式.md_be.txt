### Slope One 算法 {#24.html#slope-one-}经典的基于物品推荐，相似度矩阵计算无法实时更新，整个过程都是离线计算的，而且还有另一个问题，相似度计算时没有考虑相似度的置信问题。例如，两个物品，他们都被同一个用户喜欢了，且只被这一个用户喜欢了，那么余弦相似度计算的结果是1，这个 1 在最后汇总计算推荐分数时，对结果的影响却最大。Slope One 算法针对这些问题有很好的改进。在 2005 年首次问世，Slope One算法专门针对评分矩阵，不适用于行为矩阵。Slope One算法计算的不是物品之间的相似度，而是计算的物品之间的距离，相似度的反面。举个例子就一目了然，下面是一个简单的评分矩阵：![](Images/a2b50529e721dde4d7ed321051370a26.png){savepage-src="https://static001.geekbang.org/resource/image/e0/a4/e0782cd3372b43a96a073d6263af62a4.png"}这个矩阵反应了这些事实：用户 1 给物品 A、B、C 都评分了，分别是5，3，2；用户 2 给物品 A、B 评分了，分别是 3、4；用户 3 给物品 B、C评分了，分别是 2、5。现在首先来两两计算物品之间的差距：![](Images/8a11cb9610531a20caf2559ce5355fcc.png){savepage-src="https://static001.geekbang.org/resource/image/7d/8f/7d9e2189fccd9d186232c7f3ba7f858f.png"}括号里表示两个物品的共同用户数量，代表两个物品差距的置信程度。比如物品 A和物品 B 之间的差距是 0.5，共同用户数是 2，反之，物品 B 和物品 A的差距是 -0.5，共同用户数还是2。知道这个差距后，就可以用一个物品去预测另一个物品的评分。如果只知道用户 3 给物品 B 的评分是 2，那么预测用户 3 给物品 A的评分呢就是 2.5，因为从物品 B 到物品 A 的差距是 0.5。在此基础上继续推进，如果知道用户给多个物品评分了，怎么汇总这些分数呢？方法是把单个预测的分数按照共同用户数加权求平均。比如现在知道用户 3不但给物品 B 评分为 2，还给物品 C 评分为 5，物品 B 对物品 A 的预测是 2.5分，刚才计算过了，物品 C 给物品 A 的预测是 8 分，再加权平均。]{.MathJax_Preview style="color: inherit; display: none;"} {.MathJax_Display style="text-align: center;"}``{=html}[[[[[[8]{#24.html#MathJax-Span-153.mn style="font-family: MathJax_Main;"}[∗]{#24.html#MathJax-Span-154 .mostyle="font-family: MathJax_Main; padding-left: 0.238em;"}[1]{#24.html#MathJax-Span-155.mnstyle="font-family: MathJax_Main; padding-left: 0.238em;"}[+]{#24.html#MathJax-Span-156.mostyle="font-family: MathJax_Main; padding-left: 0.238em;"}[2.5]{#24.html#MathJax-Span-157.mnstyle="font-family: MathJax_Main; padding-left: 0.238em;"}[∗]{#24.html#MathJax-Span-158.mostyle="font-family: MathJax_Main; padding-left: 0.238em;"}[2]{#24.html#MathJax-Span-159.mnstyle="font-family: MathJax_Main; padding-left: 0.238em;"}]{#24.html#MathJax-Span-152.mrow}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.202em, 1005.93em, 4.238em, -999.998em); top: -4.656em; left: 50%; margin-left: -3.009em;"}(]{#24.html#MathJax-Span-161.mo style="font-family: MathJax_Main;"}[1]{#24.html#MathJax-Span-162 .mnstyle="font-family: MathJax_Main;"}[+]{#24.html#MathJax-Span-163 .mostyle="font-family: MathJax_Main; padding-left: 0.238em;"}[2]{#24.html#MathJax-Span-164.mnstyle="font-family: MathJax_Main; padding-left: 0.238em;"}[)]{#24.html#MathJax-Span-165.mo style="font-family: MathJax_Main;"}]{#24.html#MathJax-Span-160.mrow}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.108em, 1002.92em, 4.379em, -999.998em); top: -3.292em; left: 50%; margin-left: -1.504em;"}[[]{style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 6.12em; height: 0px;"}[]{style="display: inline-block; width: 0px; height: 1.085em;"}]{style="position: absolute; clip: rect(0.896em, 1006.12em, 1.226em, -999.998em); top: -1.315em; left: 0em;"}]{style="display: inline-block; position: relative; width: 6.12em; height: 0px; margin-right: 0.144em; margin-left: 0.144em;"}]{#24.html#MathJax-Span-151.mfrac}[=]{#24.html#MathJax-Span-166 .mostyle="font-family: MathJax_Main; padding-left: 0.285em;"}[4.33]{#24.html#MathJax-Span-167.mnstyle="font-family: MathJax_Main; padding-left: 0.285em;"}]{#24.html#MathJax-Span-150.mrow}[]{style="display: inline-block; width: 0px; height: 2.308em;"}]{style="position: absolute; clip: rect(0.802em, 1009.7em, 3.391em, -999.998em); top: -2.304em; left: 0em;"}]{style="display: inline-block; position: relative; width: 9.744em; height: 0px; font-size: 125%;"}[]{style="display: inline-block; overflow: hidden; vertical-align: -1.232em; border-left: 0px solid; width: 0px; height: 3.003em;"}]{#24.html#MathJax-Span-149.mathstyle="width: 12.191em; display: inline-block;"}``{=html}[$$\frac{8 \ast 1 + 2.5 \ast 2}{(1 + 2)} = 4.33$$]{.MJX_Assistive_MathML.MJX_Assistive_MathML_Blockrole="presentation"}]{#24.html#MathJax-Element-3-Frame .MathJaxtabindex="0" style="text-align: center; position: relative;"mathml="8∗1+2.5∗2(1+2)=4.33"role="presentation"}$$$$就得到了推荐分数为 4.33 分。是不是很简单？
## 总结 {#24.html#-}今天我们在基于用户的协同过滤基础上介绍了比较常见的一个算法：基于物品的协同过滤。这个方法常常在电商网站上见到，"买了又买""看了又看"这样的相关推荐，都是由这个推荐算法产生。最后我们介绍了一个改良版的基于物品推荐算法 SlopeOne。这里也留下了一个问题给你：为什么说 Slope One可以做到在线更新呢？欢迎留言讨论。![](Images/d3b48a2755db0a3707ef37007c2179c8.png){savepage-src="https://static001.geekbang.org/resource/image/87/b0/873b086966136189db14874181823fb0.jpg"}
# 【关键模块】 推荐系统服务化、存储选型及API设计在过往的文章中，我讲到了推荐系统方方面面的相关概念。那么说，对于认识一个推荐系统来说，还差最后一个问题需要解决，那就是：万事俱备，如何给用户提供一个真正的在线推荐服务呢？
## 服务化是最后一步其实一个推荐系统的在线服务，和任何别的在线服务相比，也没有什么本质区别，只是仍然还有一些特殊性。提供一个在线服务，需要两个关键元素：数据库和API。今天我就来专门说一说推荐系统中大家常常用到的数据库，并会谈谈推荐系统的API 应该如何设计。
## 存储这里注意一下，今天这里讲到的存储，专指近线或者在线部分所用的数据库，并不包括离线分析时所涉及的业务数据库或者日志数据库。近线和在线的概念我在前面已经讲到过。推荐系统在离线阶段会得到一些关键结果，这些关键结果需要存进数据库，供近线阶段做实时和准实时的更新，最终会在在线阶段直接使用。首先来看一下，离线阶段会产生哪些数据。按照用途来分，归纳起来一共就有三类。1.  特征。特征数据会是最多的，所谓用户画像，物品画像，这些都是特征数据，更新并不频繁。2.  模型。尤其是机器学习模型，这类数据的特点是它们大都是键值对，更新比较频繁。3.  结果。就是一些推荐方法在离线阶段批量计算出推荐结果后，供最后融合时召回使用。任何一个数据都可以直接做推荐结果，如协同过滤结果。如果把整个推荐系统笼统地看成一个大模型的话，它依赖的特征是由各种特征工程得到的，这些线下的特征工程和样本数据共同得到模型数据，这些模型在线上使用时，需要让线上的特征和线下的特征一致，因此需要把线下挖掘的特征放到线上去。特征数据有两种，一种是稀疏的，一种是稠密的，稀疏的特征常见就是文本类特征，用户标签之类的，稠密的特征则是各种隐因子模型的产出参数。特征数据又常常要以两种形态存在：一种是正排，一种是倒排。正排就是以用户 ID 或者物品 ID 作为主键查询，倒排则是以特征作为主键查询。两种形态的用途在哪些地方呢？在需要拼凑出样本的特征向量时，如线下从日志中得到曝光和点击样本后，还需要把对应的用户ID 和物品 ID展开成各自的特征向量，再送入学习算法中得到最终模型，这个时候就需要正排了。另一种是在需要召回候选集时，如已知用户的个人标签，要用个人标签召回新闻，那么久就需要提前准备好标签对新闻的倒排索引。这两种形态的特征数据，需要用不同的数据库存储。正排需要用列式数据库存储，倒排索引需要用KV 数据库存储。前者最典型的就是 HBase 和 Cassandra，后者就是 Redis 或Memcached。稍后再介绍这几个数据库。另外，对于稠密特征向量，例如各种隐因子向量，Embedding向量，可以考虑文件存储，采用内存映射的方式，会更加高效地读取和使用。模型数据也是一类重要的数据，模型数据又分为机器学习模型和非机器学习模型。机器学习模型与预测函数紧密相关。模型训练阶段，如果是超大规模的参数数量，业界一般采用分布式参数服务器，对于达到超大规模参数的场景在中小公司不常见，可以不用牛刀。而是采用更加灵活的 PMML 文件作为模型的存储方式，PMML是一种模型文件协议，其中定义模型的参数和预测函数，稍后也会介绍。非机器学习模型，则不太好定义，有一个非常典型的是相似度矩阵，物品相似度，用户相似度，在离线阶段通过用户行为协同矩阵计算得到的。相似度矩阵之所算作模型，因为，它是用来对用户或者物品历史评分加权的，这些历史评分就是特征，所以相似度应该看做模型数据。最后，是预先计算出来的推荐结果，或者叫候选集，这类数据通常是 ID类，召回方式是用户 ID 和策略算法名称。这种列表类的数据一般也是采用高效的KV 数据库存储，如 Redis。另外，还要介绍一个特殊的数据存储工具：ElasticSearch。这原本是一个构建在开源搜索引擎Lucene基础上的分布式搜索引擎，也常用于日志存储和分析，但由于它良好的接口设计，扩展性和尚可的性能，也常常被采用来做推荐系统的简单第一版，直接承担了存储和计算的任务。下面我逐一介绍刚才提到的这些存储工具。