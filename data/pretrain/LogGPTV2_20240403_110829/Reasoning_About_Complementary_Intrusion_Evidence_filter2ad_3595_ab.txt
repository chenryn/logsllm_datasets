¬sshd_running
install_mstream_zombie
DDoS_daemon_installed
Figure 1. An attack-attribute network example
Figure 1 shows an example alert-attribute network,
which is constructed as discussed above. The gray nodes
represent
initial or updated system attributes, and the
white nodes represent IDS alerts. For simplicity, we do
not show the initial system attributes that are not involved
in the precondition or postcondition of the correspond-
ing attacks. Alert sshd buffer overflow indicates
an attempt to compromise the system through the vulnera-
ble sshd. The precondition of sshd buffer overflow
is sshd running∧vulnerable sshd, and the post-
{¬sshd running, root access}.
condition
Thus, this attempt can be successful since its precondition
is satisﬁed in the system state. As a result, this attack in-
troduces two attribute alterations: ¬sshd running and
root access. In other words, the attacker stops the sshd
daemon and gains root access to the system. As shown in
Figure 1, the attacker then installs a mstream zombie pro-
gram, changing the attribute DDoS daemon installed
from False to True.
is
2.2.2. Conditional Probabilities A Bayesian network is
a directed acyclic graph (DAG), where each directed edge
represents a causal relationship between the two ends of the
edge, and each node stores a conditional probability table
describing the statistical relationships between the node and
its parent nodes [16].
Based on the construction of the alert-attribute network,
it is easy to see that a graph constructed in that way is
acyclic. Indeed, all the edges are from previously existing
nodes to newly added nodes, and thus will not result in any
cycle. From our discussion above, the causal relationships
among the nodes in an alert-attribute network are obvious.
Now we discuss how to determine each node’s conditional
probability table so that the alert-attribute network becomes
a Bayesian network.
When an IDS alert e is reported,
the probability
for the alert e to be a real attack is P r(e). The vari-
able e being True represents that the corresponding at-
tack is successful. We assume an attack will succeed if
its precondition is satisﬁed. Thus, the probability of e be-
ing True is the prior conﬁdence of the corresponding
IDS alert when its precondition is satisﬁed, or 0 other-
wise. Since the precondition of an attack is a logic for-
mula of system attributes, the conditional probability of an
alert node can be easily derived. The conditional probabil-
ity table associated with node sshd buffer overflow
in Figure 2 shows such an example, where we as-
sume Pr(sshd buffer overflow) = 0.6. Note that
the probability of an IDS alert variable being False un-
der these preconditions can be easily computed from the
above probabilities. Thus, we do not include them here.
sshd_running
sshd_vulnerable
sshd_buffer_overflow
sshd_ 
running 
FALSE 
FALSE 
TRUE 
TRUE 
sshd_ 
vulnerable 
P(sshd_buffer_ 
overflow=TRUE) 
FALSE 
TRUE 
FALSE 
TRUE 
0 
0 
0 
0.6 
root_access
¬sshd_running
sshd_buffer 
_overflow 
FALSE 
TRUE 
P(root_access 
=TRUE) 
0 
1 
sshd_buffer 
_overflow 
FALSE 
TRUE 
P(¬sshd_running 
=TRUE) 
0 
1 
Figure 2. Conditional probability tables
Conditional probability tables associated with system at-
tributes are even simpler to compute. Indeed, if an IDS alert
e represents a successful attack, all the system attributes in
its postcondition should turn to True. Otherwise, the system
attributes that are False before the IDS alert should remain
False. If two attribute nodes of the same attribute are con-
nected together with an edge representing implication rela-
tionship, and the earlier one is True, the latter one should
also be True. Thus, the conditional probability of a system
attribute a being True would be 1 if at least one of its par-
ent variables (either alert nodes or attribute nodes) is True,
and 0 if all its parent variables are False (unless it is re-
ported by system scanning/monitoring tools). The tables as-
sociated with root access and ¬sshd running show
examples of such conditional probabilities. Similar to the
above example, we only show the probabilities for the at-
tributes to be True, from which the probabilities for the at-
tributes to be False can be easily computed.
2.2.3. Reasoning
Intrusion Evidence The
Bayesian networks constructed in this way offer an ex-
cellent opportunity to reason about
the uncertain in-
about
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
trusion evidence, particularly the IDS alerts. We call
those attributes with a conﬁdence value of 1 the veri-
ﬁed attributes. The report of such veriﬁed attributes are
observations of facts. When new veriﬁed attributes are re-
ported by system monitoring/scanning tools, we can use
these observations to re-compute the conﬁdence val-
ues in the related previous objects in the network with
Bayesian inference. And for each node in the Bayesian net-
work, its ﬁnal probability value is the combined result of
all the evidence and knowledge. Take the Bayesian net-
work shown in Figure 2 as an example. We may be
uncertain about an IDS alert reporting a buffer overﬂow at-
tack against sshd, since the IDS has reported the same
type of alerts incorrectly in the past. However, if by scan-
ning the system we ﬁnd that sshd is not running properly
after the IDS reports this alert, we can then update the con-
ﬁdence in ¬sshd running to be 1. Thus, we are more
certain about the alert, which caused the attribute alter-
ation. Though human users would do the same reasoning,
placing these evidence into Bayesian networks offers addi-
tional beneﬁts, since such a reasoning process can then be
performed automatically and systematically. Also such rea-
soning could become too difﬁcult for human users when
dealing with very complicated scenarios.
It is easy to see that the more veriﬁed state-based evi-
dence we have, the better judgment we can make by reason-
ing about the uncertain IDS alerts and system states. This
suggests that we should monitor the system closer and scan
the system more frequently, as system monitoring tools and
vulnerability scanning tools usually generate evidence with
high conﬁdence value. However, such monitoring and scan-
ning are often expensive and may hurt the other applications
by consuming resources. Thus, it is important to determine
the right balance for system monitoring and scanning activ-
ities. Nevertheless, this problem is out of the scope of this
paper. We leave it for future consideration.
2.2.4. Merging Attribute Nodes As discussed ear-
lier, there may be edges between attribute nodes corre-
sponding to the same attribute, which represent implica-
tion relationships between them. We observe that in certain
cases, such attribute nodes can be merged without affect-
ing the reasoning about intrusion evidence in alert-attribute
networks. This observation is reﬂected by Lemma 1, which
is presented next. For the sake of presentation, if two at-
tribute nodes A and B are connected with edge (A, B),
we refer to the action of removing node A with all its out-
going edges and redirecting all its incoming edges to node
B as merging A into B.
Lemma 1 Consider two attribute nodes A and B corre-
sponding to the same attribute and connected by an edge
(A, B). If either there is no other outgoing edge from node
A or A is instantiated (veriﬁed), merging A into B does not
change the probability of any other node when reasoning
about intrusion evidence.
For space reasons, the proof of this lemma is not provided
here but in the full version of this paper [31]. With Lemma
1, we can recursively merge attribute nodes that satisfy the
condition speciﬁed in Lemma 1 to reduce the complexity of
the network structure without affecting the reasoning result.
2.3. Alert Aggregation and Abstraction
In reality, IDSs often generate a large number of alerts
for the same type of attack. Also, IDSs usually raise differ-
ent alerts for similar attacks, or variations of the same at-
tack. AS a result, many alert nodes share the same parent
nodes and child nodes in the Bayesian network. This intro-
duces two problems. First, the number of entries in condi-
tional probability table of their children nodes of those alerts
is exponential to the number of alerts, which makes it dif-
ﬁcult to take advantage of existing Bayesian network tools.
Second, the effect of additional evidence will spread over
these alerts as we do not know which of them indeed con-
tributes to the modiﬁcation of system attributes. In practice,
we usually do not care about the subtle difference between
the alert variations and which particular alert is the actual
successful one, but whether at least one of them is success-
ful. Thus, a natural approach to addressing the above prob-
lem is to abstract alert variations into one common alert and
aggregate such alerts together into one single node, which
represents “at least one of the component alerts corresponds
to a successful attack”.
The conditional probability table of an aggregated alert
node can be computed similarly. However, we need to use
aggregated prior conﬁdence value P ra, which represents
the probability that at least one of its component alerts cor-
responds to an actual attack. Given n component alerts of
an abstract attack type T that are merged into one aggre-
gated alert, the aggregated prior conﬁdence P ra(T ) can be
computed as
P ra(T ) = 1 − n(cid:1)
(1 − P r(T ypeai)),
i=1
where a1, ..., an are the alerts to be aggregated, and T ypeai
is the attack type of ai.
2.4. Hypothesizing about Missed Attacks
When there are missed attacks, the effect of the attacks
on the system will not be reﬂected in the alert-attribute net-
work. As a result, some later alerts corresponding to suc-
cessful attacks may be considered false. In other words, the
current model only works when there are no missed attacks.
(Note that this is a common problem shared by almost all
alert correlation methods.)
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
We observe that when successful attacks are missed by
IDSs, it is still possible for the system monitoring tools
to catch the impact of the attacks on the system states. In
other words, we may observe unexpected attribute alter-
ations. Such a case essentially causes inconsistency in the
alert-attribute networks, where a new attribute node is added
without any node leading to it.
Inconsistencies are almost always caused by missed at-
tacks: An “unexpected” attribute alteration causing the in-
consistencies can either be directly caused by some success-
ful attack missed by IDSs, or by a detected successful at-
tack whose precondition is not satisﬁed in the network due
to previously missed attacks. The only exception is that it
could be caused by false alerts if the monotonicity property
of attacks does not hold for some particular types of attacks.
That is, a successful attack disables other attacks’ precondi-
tions. According to [25], this kind of attacks are very rare.
We can always recognize such attacks and pay additional
attention in the investigation when they are involved. Thus,
we propose to hypothesize about missed attacks based on
the inconsistencies in alert-attribute networks.
Root access
Install BackOrifice
BACKDOOR BackOrifice installed
BACKDOOR BackOrifice access
Figure 3. An example of hypothesized attack
Figure 3 shows an example to hypothesize about missed
attacks to resolve an inconsistency. When the system mon-
itoring tool reports the fact that a backdoor “BackOri-
ﬁce” was found in the local system, the system adds node
“BACKDOOR BackOriﬁce installed” to the graph imme-
diately, which activates the precondition of the later alert
“BACKDOOR BackOriﬁce access”. However, there is no
previous node possibly causing the “BackOriﬁce installed”
attribute set to True. To ﬁll in this gap, we look up the graph
structure for established attributes and attacks, the knowl-
edge base for possible attacks that can cause this attribute
alteration, and the log of previously dropped alerts for pos-
sible related attacks. According to the above information,
we make a hypothesis of a possibly missed attack “Install
BackOriﬁce”, linking the attribute nodes “Root access” and
“BackOriﬁce installed”. The hypothesized node and edges
are presented with dotted lines in the ﬁgure.
A hypothesis of a possibly missed attack infers that (1)
the attack has happened, (2) the attack has been missed
by IDSs, and (3) the attack is successful. The probability
of a hypothesized attack being a correct one can be esti-
mated as Phypothesis = Phappened · Pmissed · Psuccessf ul,
where Phappened is the probability for the attack to have
happened, Pmissed is the prior probability for the IDS to
miss the attack, and Psuccessf ul is the probability for the at-
tack to succeed if it happens. As Phappen fully depends on
the attacker’s knowledge and personal preference and is un-
predictable, we only study the value Pmissed · Psuccessf ul,
which represents the probability for the hypothesis to be
True given the condition that it has actually happened.
From our previous discussion, the successfulness of an at-
tack depends on whether its precondition is satisﬁed by
the system attributes. Thus, the conditional probability ta-
ble of a hypothesis node over the attributes in the attack’s
precondition is similar to a normal alert node except that
the non-zero values in the conditional probability table is
Pmissed instead of Pr. Accordingly, we refer to the proba-
bility computed from the Bayesian network with this con-
ditional probability table the conﬁdence in the hypothesized
attack. Although this conﬁdence value has a different mean-
ing from those for normal alert nodes, it still shows which
hypothesis is more possible given the available evidence.
We add the the hypothesized attacks with the corre-
sponding conditional probability tables into the alert-
attribute network. From the earlier discussion, we can
see that such a hypothesis is made and placed into
the alert-attribute network only if the attack is possi-
ble given the system state at the time. With the Bayesian
network’s belief update, we can always keep the hy-
potheses consistent with the newest observations. For
example, we may ﬁnd negative evidence against a hy-