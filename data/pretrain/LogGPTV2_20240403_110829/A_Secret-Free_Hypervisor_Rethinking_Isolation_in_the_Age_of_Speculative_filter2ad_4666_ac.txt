hypervisor stacks and vCPU register frames, whose lifetimes
track their corresponding vCPUs. Short-lived objects will be
handled by ephemeral mappings later.
A hypervisor may consider guest memory as long-lived and
may map it in its entirety in the domain-private region to
simplify copy_from_guest() and copy_to_guest()
operations. We make the design choice to avoid permanently
mapping all guest memory in hypervisor space, including in
domain- and vCPU-private regions. We have learned from
ret2dir that aliasing user memory in kernel enables gadget
injection at lower privilege levels. Thus, our secret-free design
creates ephemeral mappings to access guest memory even
if the hypervisor is entered under that guest’s context for
defense-in-depth.
C. Hypervisor stacks, vCPU state and register frames
As we deﬁne guest registers and their copies to be secrets,
any spills of register state must be isolated. Upon entry, the
hypervisor switches to its stack and a copy of the vCPU
state (including the guest register frame) is spilled onto it,
potentially leaking sensitive data to speculative side channels.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
374
GlobalDomainvCPUUserGlobalDomainvCPUUserGlobalDomainvCPUUserGlobal non-secretsd1v2L4 (root) tabled2v1d1 privated1v1 privated2 privated1v2 privated2v1privateL3tabled1v1Lower tablesPer-vCPU stacks increase memory consumption as well.
For commercial cloud platforms,
the maximum supported
number of host pCPUs and total vCPUs are 512 and 2048
for Hyper-V [35], 768 and 4096 in ESX [36] and 288 host
CPUs (1152 vCPUs assuming an overcommit factor of 4)
for XenServer [37]. Assuming 256 host cores with a total of
4096 vCPUs, introducing 4KiB per-pCPU bounce buffers and
16KiB per-vCPU stacks consumes 65MiB of memory, which
we do not believe is a substantial pressure on the host and is
only a pessimistic case on a per-pCPU stack hypervisor.
D. Ephemeral mappings
A secret-free hypervisor context sees only the hypervisor
image,
the vCPU-private stack, register state of the cur-
rent vCPU, and internal secret-less bookkeeping structures.
The hypervisor must create ephemeral mappings for short-
lived objects or other accesses whose mappings are not
present in the minimal address space. These include walk-
ing and modifying page tables, copy_from_guest() and
copy_to_guest() during hypercalls, background scrub-
bing of free heap memory and so forth. This is a stark contrast
to existing hypervisor and OS kernel designs, as we never
switch to the full page table but only grant temporary access
necessary for the hypervisor to complete the current operation.
Mapping and unmapping for a temporary access in the global
map area is costly, because the IPI and TLB operations quickly
multiply as the core count increases. For example, XPFO
suffers 27–31% performance degradation from IPIs even on
a 4-core desktop after optimizations [25]. Broadcasting is
feasible when hardware acceleration exists. AMD Milan [38]
and Arm MP Extensions [26] allow for TLB invalidation on all
CPUs. We do not rely on hardware TLB broadcasting because
such an architectural assist is not yet ubiquitous across ISAs
or across different generations of CPUs.
guest
memory
When
accessing
during
copy_from_guest(),
for example, another hypervisor
context is unlikely to simultaneously copy and mutate data
at the same page. Based on this observation, we introduce a
per-vCPU ephemeral mapping infrastructure. The hypervisor
uses local APIs for temporary access. Ephemeral mappings
are created and destroyed in the local ephemeral address range
visible only to the current vCPU, avoiding scalability issues
from broadcasting page table maintenance operations. Care
must be taken to guarantee the private ephemeral window
does not outlive the underlying pages. For example, pages
ballooned out by a guest may be allocated to other domains.
The hypervisor must ensure the vCPU ephemeral mappings
are ﬂushed during ballooning, or take references to prevent
the underlying memory from changing ownership while the
mappings are alive.
A brief attack window exists if the hypervisor ephemerally
maps secrets of domain B under the context of domain A. In
reality, this is not a concern because a sensible hypervisor
implementation will not map live pages from B under an
unprivileged A. If A is privileged, ephemeral mappings of for-
eign memory under its context can be triggered via privileged
Fig. 4. Structure of a direct-map map cache. Note that in the unmap case,
a hot entry will not be evicted even after all references are dropped. MFN
0x1234, 0x12345, 0x1212 are hashed to cache slot 0, 1, 2 respectively.
The index ﬁeld of a cache entry points to the ephemeral mapping slot backing
the cache entry, used to look up the allocated virtual address and to adjust
the reference count.
hypercalls. However, a privileged domain such as dom0 is
already able to architecturally access guests’ memory (for ex-
ample, via device emulation, driver back-ends and debugging
hypercalls) and a speculative side channel is uninteresting. De-
privileging dom0 is outside the scope of this paper.
Self-mapping page tables: Creating ephemeral mappings
requires modifying the page table of the current hypervisor
context. We lose the ability to walk page tables after the
removal of the direct map because there is no longer a conve-
nient direct map alias to access an arbitrary physical address.
We use page table self-map to overcome this limitation to
locate and modify the PTEs of ephemeral mappings, which
is a common technique in kernel code for PTE modiﬁcation
without manual page table walking (see Appendix for details).
Note that self-map can only be used for a virtual address
of the current installed page table, meaning it cannot act as
a generic page table walker starting from an arbitrary root.
A generic walker needs to be implemented on top of the
ephemeral mapping infrastructure to map arbitrary physical
addresses when the direct map is absent.
E. The map cache
The cost of manipulating mappings locally is still substan-
tially more expensive than bitwise operations to access the
direct map. The x86 invlpg instruction for TLB invalida-
tion, for example, is a serialising operation that ﬂushes the
pipeline [18]. We introduce a map cache to allow for efﬁcient
ephemeral memory access. Ephemeral mappings often observe
certain level of spatial and temporal locality: guest buffers
passed in a hypercall are likely to be reused; consecutive
mappings are either at adjacent guest physical memory, or at
least share the same top levels of page directory pages.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
375
MFNidxHot?0x12340N0x23450x1234532NN0x12121YVArefcnt0xff000003+ 10xff010001-10xff020000+ 10xff030001Cache hit:map(0x1234); //hit!refcnt[0]++;return 0xff000000;Conflict eviction:map(0x12345); //miss!va=new_slot(); //slot 2refcnt[2]++;evict(0x2345);returnva;//0xff02000Unmap:unmap(0xff01000);refcnt[1]--;if (incache&&!refcnt[1] && !hot)evict(0x1212);Per-vCPU ephemeralmapping areaThemapcacheThe structure of the map cache is shown in Fig. 4. When
the hypervisor requests an ephemeral mapping to a Ma-
chine Frame Number (e.g. a 4KiB-page at physical address
0x1234000 has an MFN of 0x1234), it computes the hash
slot based on the MFN and fetches the cached entry. If the
entry already contains a mapping to the same MFN, the cache
immediately returns the virtual address by looking up the
associated ephemeral mapping slot backing the cache entry.
If not, the entry is evicted from the cache and a new mapping
is inserted. The old entry will then be replaced with the new
MFN and its allocated ephemeral slot. We associate a reference
count to each ephemeral entry so that the mapping can be
replaced only when all owners have dropped the reference via
unmap calls.
We consider several optimizations. To exploit
temporal
locality, we promote an entry to become hot when the same
mapping has been requested repeatedly, preventing it from im-
mediate eviction even when all references are dropped. To in-
crease map cache performance, we explore batch invalidation,
superpage caches and set associativity to reduce the cost of
local TLB ﬂushes, large region mappings and collision cache
misses respectively. These caching optimizations must not
expose additional side channels. For example, no ephemeral
mapping to other domains can be cached or promoted as
hot entries under the context of an unprivileged domU. This
guarantees that cached entries and map cache contention are
only caused by the domain itself, thus an attacker is unable to
reveal secrets by probing the timing of ephemeral mappings
or by speculatively accessing the cached entries.
We explore the design space of several parameters of the
cache (elaborated in the Evaluation section) to achieve a high
hit rate to amortize ephemeral mapping costs. With an optimal
set of parameters, we are able to achieve a hit rate of 80-90%
in our implementation, greatly reducing the cost of ephemeral
access from the hypervisor. A high-performance map cache
proves that the hypervisor address space can be minimized.
Switching to a full address space like KPTI or XPTI with a
direct map is unnecessary for efﬁciency.
F. µarch isolation
We do not propose new defenses against pure µarch snifﬁng
attacks, but we do not exclude this category from the threat
model for the secret-free hypervisor. As the overhead of secret
freedom is small, we are able to compose it well with other
known mitigations, including core scheduling and µarch buffer
ﬂushing on context switch, to mitigate all categories in the
threat model. Note that this is also necessary for attacks that
combine coercion and µarch sharing. An attacker may mistrain
a sibling vCPU thread from another domain to ﬁll the shared
L1 cache with secrets before launching L1TF. Without µarch
isolation, secret freedom would be unable to prevent this attack
combination.
G. Putting it all together
Fig. 5 shows the address space of a vCPU. It separates the
address space into multiple tiers of secret levels. At the global
Fig. 5. Secret-Free address space separation.
non-secret level, only the hypervisor image and non-secret
data are visible. The next tier, domain secrets, is shared with
all vCPUs of the same domain. Next, the hypervisor stack,
register frames and other vCPU secrets appear at the vCPU-
private level. Ephemeral mappings and the map cache reside
in vCPU-private range, hidden from other vCPUs and domains
while the hypervisor is temporarily accessing memory. These
provide a minimal address space that is secret-free.
There are two major differences with state-of-the-art tech-
niques. First, the minimal address space is maintained at all
times. We never expose the full hypervisor space that contains
secrets belonging to other domains, unlike PTI techniques. The
hypervisor has the same restricted address space as the guest,
and only creates ephemeral mappings when necessary. Second,
we adopt an allow-list approach by identifying and promoting
non-secrets. Data is accessed via ephemeral mappings by
default. Long-lived objects are added to the vCPU-private level
ﬁrst. We promote memory to be visible within a domain or
globally only when it both does not violate secret freedom and
is performance critical. Our approach does not identify secrets
that need to be hidden. Instead, it identiﬁes performance-
critical non-secrets that should be shared.
H. Secret-freedom as a generic design principle
The recent epidemic of speculative vulnerabilities motivates
the secret-free hypervisor design as we would like to introduce
a comprehensive framework for isolating customer secrets
in a multi-party cloud environment. However, we believe
the components introduced in this section are not unique
to any speciﬁc hypervisor and can be easily extrapolated to
a variety of implementations. For OS kernels, a secret-free
design applies as well because the abstraction of kernel, user
space, processes and threads are analogous to hypervisor, guest
domain, VMs and vCPUs.
To demonstrate the generality of the secret-free principle,
we implement and evaluate the design on multiple systems in-
cluding Xen (Type-I), Hyper-V (Type-I), bhyve (Type-II) and
FreeBSD (UNIX kernel). We are able to apply common secret-
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
376
Address space of d2v1d1v1d1v2d2v1HypervisorimageNon-secretdataHypervisor globalGuestDomain-privatevCPU-privateEphemeral + map cache……………Hypervisor privateEphemeralItems
Introducing private and ephemeral APIs
Removing other dependencies and direct map teardown
Domain/vCPU-private region for stacks, vCPU state, etc.
Per-vCPU ephemeral mapping infrastructure
Bootstrapping for Secret-Free
Bug ﬁxing, misc.
Total
LoC
821
411
729
284
115
55
2415
TABLE I
LINES OF CODE FOR SECRET-FREE XEN
were an ephemeral concept. We promoted EPT to the global
non-secret pool to avoid 20 ephemeral mappings. Note that
we cannot treat the 5 guest page table pages as non-secrets as
they are guest memory and may contain secrets at any point.
The amount of code changed is shown in TABLE I.
We have sent the ﬁrst patch series for new APIs and direct
map teardown to Xen upstream for review. At present, 40 out
of 54 patches have been merged into the latest main branch.
We reported the bugs revealed by Secret-Free to Xen upstream
and our ﬁxes have been merged.
VII. EVALUATION OF SECRET-FREE XEN
A. Experimental setup
We evaluate our secret-free Xen implementation on an
AMD system, featuring a 12-core (24-thread) Ryzen 5900X
CPU, 32GB of DDR4 3200MT/s RAM running Ubuntu 18.04
as dom0. We approximate a common cloud conﬁguration using
a guest with 8 vCPUs and 16GB RAM, matching an Azure
A8v2 or AWS c4.2xlarge instance.
We evaluate performance with a range of benchmark suites.
We show CPU and memory benchmarks using the industry
standard SPEC-CPU2017 suite. To reveal
the worst case
scenarios and give insights on how the critical path is affected,
we run micro-benchmarks to show hypercall latency, context
switch speed, IPI latency and MMIO performance. We then
analyse disk and network I/O to investigate cross-domain com-
munication between domU front-end and dom0 back-end PV
drivers. Lastly, we run real-world workloads representative of
a wide range of cloud applications including databases, HTTP
servers, decompression, kernel builds, scientiﬁc computing,
etc.
We build several Xen conﬁgurations with different mitiga-
tion options:
Baseline: Xen without any compiled-in speculative mitiga-
tion facilities and with boot-time speculative defenses disabled.
The baseline is susceptible to all speculative execution attacks
that the underlying hardware is vulnerable to.
Default: Xen with compiled-in mitigation support and with
default boot-time mitigations by detecting the hardware. On
the 5900X CPU, this enables IBPB, lfence for indirect
branches, conditional branch hardening and core scheduling.
XPTI: force enabling Xen Page Table Isolation for Melt-
down mitigation in addition to default Xen parameters. PV
only.
Fig. 6. A 2-stage page table walk. 20 ephemeral mappings are avoided by
promoting EPT to global non-secrets.
free design components to all systems and only introduce
minor changes to each. In this paper, we ﬁrst elaborate on the
Xen hypervisor for detailed evaluation and analysis while later
demonstrating other implementations for comparison, showing
secret-free as a generic mechanism as well as focusing on
necessary adaptations for each type of kernel.
VI. IMPLEMENTATION: A SECRET-FREE XEN
We implemented the secret-free hypervisor in Xen 4.14.0 on
x86 64 architecture. We made several necessary adaptations to
the Xen codebase in addition to the aforementioned secret-free
components.
1) Xen requires a direct map under 4GiB during early boot.
We implemented a lightweight mapping mechanism by
reserving 5 fixmap entries (for up to 5-level paging) to
bootstrap Xen and to set up initial address spaces, which
is superseded by the per-vCPU ephemeral mapping
infrastructure once bootstrapping is done.
2) We replaced Xen’s domain_page() mapping API
with per-vCPU ephemeral mappings and revealed three