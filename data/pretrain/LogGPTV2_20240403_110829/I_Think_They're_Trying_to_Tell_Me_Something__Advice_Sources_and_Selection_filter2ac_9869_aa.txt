title:I Think They're Trying to Tell Me Something: Advice Sources and Selection
for Digital Security
author:Elissa M. Redmiles and
Amelia R. Malone and
Michelle L. Mazurek
2016 IEEE Symposium on Security and Privacy
2016 IEEE Symposium on Security and Privacy
I Think They’re Trying to Tell Me Something:
Advice Sources and Selection for Digital Security
Elissa M. Redmiles, Amelia R. Malone, and Michelle L. Mazurek
Department of Computer Science
University of Maryland
College Park, Maryland 20742
PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract—Users receive a multitude of digital- and physical-
security advice every day. Indeed, if we implemented all the
security advice we received, we would never leave our houses or
use the Internet. Instead, users selectively choose some advice
to accept and some (most) to reject; however,
it is unclear
whether they are effectively prioritizing what is most important
or most useful. If we can understand from where and why
users take security advice, we can develop more effective security
interventions.
As a ﬁrst step, we conducted 25 semi-structured interviews
of a demographically broad pool of users. These interviews
resulted in several interesting ﬁndings: (1) participants evaluated
digital-security advice based on the trustworthiness of the advice
source, but evaluated physical-security advice based on their
intuitive assessment of the advice content; (2) negative-security
events portrayed in well-crafted ﬁctional narratives with relatable
characters (such as those shown in TV or movies) may be effective
teaching tools for both digital- and physical-security behaviors;
and (3) participants rejected advice for many reasons, including
ﬁnding that the advice contains too much marketing material or
threatens their privacy.
I. INTRODUCTION
In the United States Computer Emergency Readiness Team
(US-CERT) list of advice for home computer users there are 61
topics, with approximately 500 words of advice per topic [1].
This single US-CERT page contains more than 30,000 words
of digital-security advice. If people listened to all of the
security advice that must be contained in the multitude of
digital- and physical-security advice sources available today,
they would never leave their houses or use the Internet
again. Since people are still leaving their houses, and most
certainly still using the Internet, how are they determining
which security advice to implement and which to discard? It
is important to understand how users learn security behaviors
in order to ensure that the best or most important security
tactics can break through the noise and attract adoption.
Previous research related to users’ security behaviors has
primarily focused on identifying those behaviors and experi-
menting with how to change them [2], [3]. Other work has
shown the important inﬂuence of social factors on security
behavior [4], [5]. Additional work has proposed that users
choose which behaviors to practice based on an analysis of
the costs and beneﬁts [6], [7].
Despite this past work, there has been no comprehensive
analysis of why users choose to accept and reject digital-
security advice and from what sources they take this advice.
Nor has there been a direct comparison between the advice-
taking behaviors of users in the more well-established domain
of physical security with the more recent area of digital secu-
rity. As a ﬁrst step toward establishing a deeper understanding
of users’ approaches to learning digital-security behaviors, we
sought to answer the following research questions:
Q1) Where or
from whom do users learn digital- and
physical-security behaviors?
Q2) How do users’ advice sources, reasons for accepting or
rejecting advice, and valuation of advice differ for digital
and physical security?
Q3) How do demographics, as well as exposure to security-
sensitive content and workplace trainings, impact the
use of different advice sources or users’ reasons for
accepting or rejecting advice?
To address these questions, we conducted a semi-structured
interview study with 25 participants of varied demographics.
During a 60-minute interview, we asked questions designed to
help participants articulate their digital-security habits at home,
as well as where they learned these strategies and why they
chose to implement them, with the assumption that participants
could in most cases accurately recall their habits and articulate
reasons for those habits. We also addressed where participants
learned security strategies and why they may reject certain
strategies that they have heard about but choose not employ.
We explicitly compared this information to the ways that
participants learn and process physical-security advice,
to
determine whether mechanisms that inform physical-security
advice-taking can be imported to the digital domain.
Further, we recruited participants in two groups: security-
sensitive users who handle data governed by a security clear-
ance or by HIPAA or FERPA regulations, and general users
who do not. This allowed us to consider the effect that regular
exposure to a data-security mindset has on the ways that users
process security advice in their personal (non-work) lives.
Finally, we explored as a case study participants’ reactions
to two-factor authentication, which has been identiﬁed as a
highly effective but underutilized security tool [8].
We rigorously analyzed this interview data using an iter-
ative open-coding process. We identiﬁed several interesting
ﬁndings, including:
2375-1207/16 $31.00 © 2016 IEEE
© 2016, Elissa M. Redmiles. Under license to IEEE.
DOI 10.1109/SP.2016.24
DOI 10.1109/SP.2016.24
272
272
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:08 UTC from IEEE Xplore.  Restrictions apply. 
• Participants evaluate digital-security advice based pri-
marily on the trustworthiness of the advice source. This
contrasts sharply with physical security, where the trust-
worthiness of the source is less important because users
feel comfortable independently evaluating the content and
value of the advice.
• Prior work has identiﬁed negative personal experience as
a learning tool [3]; we ﬁnd that TV shows and movies that
present negative-security events with relatable characters
and clearly deﬁned causes can also be strong motivators
for adopting new security behaviors.
• Participants have many more reasons for rejecting both
digital- and physical-security advice than for accepting
it. For digital security in particular, these reasons include
not just the obvious—that advice is too complicated or
that the participant is oversaturated—but also more subtle
rationales, such as the presence of too much marketing
and concerns about privacy.
Based on these and other trends extracted from our interviews,
we distill recommendations for designing and disseminating
more effective security advice. These recommendations in-
clude highlighting information to mitigate user privacy con-
cerns for services such as two-factor authentication; increasing
the credibility of security advice by removing product-speciﬁc
references to reduce users’ impressions of the advice as
marketing material; and replacing corporate security training
videos with more relatable ﬁctional vignettes illustrating neg-
ative events. We believe these guidelines can help security
experts to magnify the impact of truly important security
advice.
II. RELATED WORK
In this section, we discuss prior research in four related
areas: examining the factors that
inﬂuence users’ security
behaviors, determining which security behaviors or recom-
mendations are valuable, theoretical frameworks for analyzing
technology adoption, and developing or evaluating security
interventions.
A. Factors Inﬂuencing Security Behaviors
Several researchers have examined how speciﬁc factors
inﬂuence security behaviors. Das et al. demonstrated the
importance of social inﬂuence; for example, showing users
information about their Facebook friends’ security behaviors
made them more likely to adopt the same behaviors [2], [9].
Relatedly, Rader et al. found that security stories from non-
expert peers affect how users think about computer security
and how they make security decisions like whether to click on
a link [3]. Wash identiﬁed “folk models” of security, such as
viewing hackers like digital grafﬁti artists, that inﬂuence users’
perceptions of what is and is not dangerous [10]. Lastly, Rader
and Wash together examined how the topics and words used
in three types of security advice may affect user’s ability to
make good security decisions [11]. Our work broadens these
ﬁndings by explicitly considering a variety of ways, social and
otherwise, in which users may learn about different security
behaviors.
Security decisions are often framed as economic tradeoffs,
in which users ignore security best practices due to rational
cost-beneﬁt optimization. Herley, for example, suggests that if
users were to spend one minute of each day checking URLs
to avoid phishing, the time cost would be much larger than the
cost of the phishing itself [6]. To investigate whether users are
in fact making rational cost-beneﬁt calculations, we examine
users’ reported thought processes when accepting and rejecting
security advice. Further, researchers have considered a compli-
ance budget: the limited time and resources users can spend on
security behavior [7], [12]. This highlights the importance of
understanding how users decide which advice they spend their
compliance budget on, so that the most valuable advice can
be designed to rise to the top. Although this prior work also
focuses on why users implement or reject security behaviors,
our work differs in a few key ways: our study is about home
security behaviors, whereas Beautement et al. addressed only
the organizational environment [7]; relatedly, our study draws
from a larger and more diverse participant pool; and ﬁnally,
we investigate not only why users reject security behaviors
but also why they accept or reject advice from a multitude of
sources.
Other researchers have considered how demographics affect
security and privacy decision-making. Howe et al. note that
socioeconomic status, and the corresponding belief that one’s
information may not be “important enough to hack,” can
affect security behaviors [5]. This paper also notes large
differences in advice sources between undergraduate and adult
populations. Wash and Rader investigated security beliefs
and behaviors among a large, representative U.S. sample
and found that more educated users tended to have more
sophisticated beliefs but take fewer precautions [13]. Others
have investigated how demographic and personality factors
inﬂuence susceptibility to phishing [14], [15]. Rainie et al.
found that younger people, social media users, and those
who had a prior bad experience were more likely to try to
hide their online behavior [4]. Based on this prior work, we
recruited speciﬁcally for diversity of age, income, education,
and race. Further, we recruited for and analyzed the impact of
an additional type of diversity: security sensitivity, meaning
professional training to handle conﬁdential or sensitive data.
In addition, during our data analysis, we coded for participants
who discussed whether their information was important to
protect and whether they had prior negative experiences.
Although prior work touches on similar themes, to our
knowledge we are the ﬁrst to comprehensively examine users’
primary sources of digital security advice in general and why
they choose to accept or reject it. Further, our work directly
compares digital security to physical security. By drawing
lessons from each domain, we develop design guidelines for
effectively transmitting security information.
273273
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:08 UTC from IEEE Xplore.  Restrictions apply. 
B. Expert Advice and Best Practices
Any attempt to improve the dissemination and adoption
of security advice will of course require decisions about
which advice is relevant and important. In recent work, Ion
et al. surveyed more than 200 security experts to determine
what behaviors they most often practice and/or strongly rec-
ommend [8]. Top suggestions included installing software
updates, using two-factor authentication, and using a pass-
word manager. Corporate and government help pages from
organizations such as Microsoft, the United States Computer
Emergency Readiness Team, and McAfee also provide users
with pieces of top advice, including tips for improving the
strength of passwords and encouragement to update software
regularly [1], [16], [17]. These best practices provide insight
into what advice is most valuable to give users; in this paper,
we address the related but orthogonal problem of how users
receive and respond to advice, and therefore how important
advice can be disseminated when it is identiﬁed.
C. Theoretical Frameworks
A sizable body of research focuses on theoretical frame-
works to explain technological adoption. One such theory, Dif-
fusion of Innovation, emphasizes how communication chan-
nels and social systems can lead to the introduction of new
innovations into communities over time [18]. Applications
of this theory often require large samples and longitudinal
data [19]. In contrast, Digital Divide theory suggests that
access inequality is the most important factor in technology
adoption [20]. The application of Digital Divide theory also
requires longitudinal data in combination with socioeconomic
information to evaluate technological progress. In this small-
sample, qualitative work, we take a theory-agnostic approach
to data analysis. Follow-up research could be used to establish
how our ﬁndings ﬁt within these frameworks.
D. User Education and Security Interventions
Another large body of work is devoted to analyzing and
improving delivery of security information to users, particu-
larly in the context of user education and designing security
warnings. For example, signiﬁcant research has examined how
to educate users about phishing prevention [21]–[25]. There
has also been considerable work addressing the effectiveness
of phishing and SSL warnings for browsers [26]–[29], bank-
ing security warnings [30], and security-warning habituation
generally [31]. Other researchers have considered how best
to nudge users to create stronger passwords [32]–[35] and
how to inform them about potentially invasive mobile app
permissions [36]–[39]. Our work takes an alternate view:
rather than focus on how to promote adoption of one speciﬁc
security behavior, we consider why users make the security
decisions they do, where they get their educational materials,
and how they evaluate credibility.
III. METHODOLOGY
To answer our research questions, we conducted semi-
structured interviews in our laboratory between March and
October 2015. To support generalizable and rigorous qual-
itative results, we conducted interviews until new themes
stopped emerging (25 participants) [40]. Our subject pool is
larger than the 12-20 interviews suggested by qualitative best-
practices literature; as such, it can provide a strong basis
for both future quantitative work and generalizable design
recommendations [41].
The study was approved by the University of Maryland
Institutional Review Board. Below, we discuss our recruitment
process, interview procedure, details of our qualitative analy-
sis, and limitations of our work.
A. Recruitment
We recruited participants from the Washington D.C. metro
area via Craiglist postings and by sending emails to neigh-
borhood listservs. We also distributed emails in public- and
private-sector organizations with the help of known contacts in
those organizations. In addition, we posted ﬂyers in University
of Maryland buildings and emailed university staff members.
We collected demographic information including age, gender,
income, job role, zip code, and education level from respon-
dents in order to ensure a broad diversity of participants.
Participants were compensated $25 for an approximately one-
hour interview session.
B. Procedure
We asked participants to bring a device they use to connect
to the Internet for personal use with them to their interview.
Two researchers conducted all of the interviews, which took
between 40 and 70 minutes. We used a semi-structured in-
terview protocol, in which the interviewer primarily uses a
standard list of questions but has discretion to ask follow-ups
or skip questions that have already been covered [42]. Semi-
structured interviews allow researchers to gather information
about participants’ practices, habits, and experiences as well
as their opinions and attitudes.
During the interview, we asked questions about participants’
digital- and physical-security habits as well as where they
learned those habits (Q1, Q2). We also asked participants to
“act out” their use of technology in a series of scenarios.
We asked questions about participants’ behaviors and advice
sources for digital-security topics such as device security,
including password protection and antivirus use; web browsing
and emailing, including two-factor authentication and phishing
questions; and online banking and shopping, including ques-
tions about the participant’s banking login process and pay-
ment methods (Q1, Q2). We asked similar questions regarding
physical-security topics such as dwelling security, including
questions about locking methods and alarm systems; transit
(e.g. car and bike) security, with questions similar to those
asked for dwelling security; and personal safety when walking
alone, including questions about carrying weapons (Q1, Q2).
We validated that our list of digital security topics broadly
covered the same topics as those mentioned as high priority
in Ion et al.’s recent paper [8].
274274
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:08 UTC from IEEE Xplore.  Restrictions apply. 
On each of these topics, participants were ﬁrst asked a
general open-ended question regarding their security behav-
iors: for example, “How do you protect your devices?” and
then asked sequentially more speciﬁc questions, for example:
“Can you show me how you access the home screen on your
smartphone?”, “Have you always had/not had a password on
your smartphone?”, and “Are there other strategies you use for
protecting your devices which you have not mentioned?”
Participants were subsequently asked a series of follow-
up questions on each topic, such as “Why do you use this
strategy?” (Q2); “Have you ever had a negative experience
with...?” (Q1); and “Where or from whom did you learn
this strategy?” (Q1). In addition to questions regarding spe-
ciﬁc security topics, participants were asked more generally
about where, from whom, and why they accepted security
advice, as well as about strategies they had considered but
not adopted (Q2). Participants were also asked to compare
digital- and physical-security advice in terms of usefulness
and trustworthiness (Q2). Finally, participants were asked to
brieﬂy describe their current or most recent job. They were
speciﬁcally asked if they handled sensitive data as part of their
job, and if so, what kind (Q3).
C. Analysis
The interview data was analyzed using an iterative open-
coding process [43]. Once the two interviewers completed
the interviews, they transcribed 17 of the interviews. The
remaining eight interviews were transcribed by an external
transcription service. The interviewers then met
in person
to develop and iteratively update an initial set of codes
for the data. Subsequently, they independently coded each
interview, incrementally updating the codebook as necessary
and re-coding previously coded interviews. This process was
repeated until all interviews were coded. The codes of the two
interviewers were then compared by computing the inter-coder
percent agreement using the ReCal2 software package [44].
The inter-coder percent agreement for this study is 75%. This
is a reasonable score for an exploratory semi-structured study,
with a large number of codes, such as ours [45]. Further, after
calculating this percent agreement score, the interviewers met
to iterate on the codes until they reached 100% agreement on
the ﬁnal codes for each interview.
D. Signifying Prevalence