title:Cyber-Physical Inconsistency Vulnerability Identification for Safety
Checks in Robotic Vehicles
author:Hongjun Choi and
Sayali Kate and
Yousra Aafer and
Xiangyu Zhang and
Dongyan Xu
Cyber-Physical Inconsistency Vulnerability Identification
for Safety Checks in Robotic Vehicles
Hongjun Choi
Purdue Univesity
PI:EMAIL
Sayali Kate
Purdue Univesity
PI:EMAIL
Yousra Aafer
University of Waterloo
PI:EMAIL
Xiangyu Zhang
Purdue Univesity
PI:EMAIL
Dongyan Xu
Purdue Univesity
PI:EMAIL
ABSTRACT
We propose a new type of vulnerability for Robotic Vehicles (RVs),
called Cyber-Physical Inconsistency. These vulnerabilities target
safety checks in RVs (e.g., crash detection). They can be exploited
by setting up malicious environment conditions such as placing
an obstacle with a certain weight and a certain angle in the RV’s
trajectory. Once exploited, the safety checks may fail to report real
physical accidents or report false alarms (while the RV is still op-
erating normally). Both situations could lead to life-threatening
consequences. The root cause of such vulnerabilities is that existing
safety checks are mostly using simple range checks implemented
in general-purpose programming languages, which are incapable
of describing the complex and delicate physical world. We develop
a novel technique that requires the interplay of program analysis,
vehicle modeling, and search-based testing to identify such vulner-
abilities. Our experiment on 4 real-world control software and 8
vehicles including quadrotors, rover, and fixed-wing airplane has
discovered 10 real vulnerabilities. Our technique does not have false
positives as it only reports when an exploit can be generated.
CCS CONCEPTS
• Security and privacy → Software security engineering; •
Computer systems organization → Robotic control; Embed-
ded and cyber-physical systems.
KEYWORDS
CPS Security; Robotic Vehicle; Cyber-Physical Inconsistency
ACM Reference Format:
Hongjun Choi, Sayali Kate, Yousra Aafer, Xiangyu Zhang, and Dongyan
Xu. 2020. Cyber-Physical Inconsistency Vulnerability Identification for
Safety Checks in Robotic Vehicles. In Proceedings of the 2020 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’20), Novem-
ber 9–13, 2020, Virtual Event, USA. ACM, New York, NY, USA, 16 pages.
https://doi.org/10.1145/3372297.3417249
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’20, November 9–13, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7089-9/20/11...$15.00
https://doi.org/10.1145/3372297.3417249
1 INTRODUCTION
Robotic Vehicles (RVs), such as self-driving cars and planes [9, 17,
19, 49, 80], operate autonomously with the harmonious coupling
of cyber and physical components. The physical components, e.g.,
sensing and actuation devices, interact with the physical world,
whereas the cyber components, e.g., control software, process sen-
sor signals, make autonomous decisions, and operate the actuation
devices. RVs are safety-critical. Misbehavior may likely cause phys-
ical damages, some being life-threatening. In order to mitigate the
consequences, today’s RV control software equips with various
safety checks to detect any abnormal situation and perform pre-
designed counter-measures. These checks utilize various sensor
and state information to assess the physical condition of the vehicle
and the environment, and detect emergent situations. When safety
violations are detected, counter-measures will be taken right away
to mitigate damages. For examples, modern ground vehicles are
often equipped with an airbag system, which is designed to inflate
air bags instantly to protect passengers when a collision is detected.
Aerial vehicles are often equipped with parachute releasing and
emergence landing systems.
The correctness of safety checks are hence of critical impor-
tance. These checks are an integral part of control software usually
implemented in some general-purpose high level programming
language that was not designed to describe complex physics. They
are often implemented as a sequence of conditional statements that
validate the ranges of certain state/sensor values. However, the
simple boolean semantics of conditional statements have limited
expressiveness and hence can only approximate the boundary be-
tween safe and unsafe states. Accuracy loss in approximation may
lead to over-approximation vulnerabilities, which are essentially
false alarms (e.g., the system reports crashes but there are no real
physical crashes), and under-approximation vulnerabilities, which
are real exceptions that the system fails to detect. We refer to these
vulnerabilities as Cyber-Physical (CP)-inconsistencies. Such vulner-
abilities have catastrophic consequences, as illustrated by many
recent accidents. For example, Tesla’s autopilot caused a fatal crash
with a white trailer in 2016 [75], due to an under-approximation vul-
nerability of the safety checks; that is, the control program failed to
detect the risky situation accurately and did not trigger the counter-
measure. More recently, two Boeing-737 Max airplanes crashed in
2018 and 2019 [10, 11], respectively. It was reported that in both
accidents, the Maneuvering Characteristics Augmentation System
(MCAS) (i.e., anti-stall system), a safety-check component, was im-
properly activated in the presence of erroneous sensor readings,
Session 1E: Cyberphysical Systems CCS '20, November 9–13, 2020, Virtual Event, USA263denoting an over-approximation vulnerability. These vulnerabilities
may be intentionally and systematically exploited by malice (e.g.,
through creating the failure inducing environmental conditions) to
launch attacks that can lead to similar critical physical damage.
Figure 1 illustrates the root causes of CP-inconsistencies. In the
left sub-figure, the Physical bar denotes the state space in the real
physical world, with red representing anomaly and green safety.
Observe that since the physical world is continuous and highly
complex, the boundary between the two sub-spaces is blurry and
gradual. Ideally, the safety function shall be conservative, excluding
all possible unsafe states. In the figure, the boundary denoted by
the function shall fall into the fully green area. However, it is very
difficult for RV system developers to use a set of range checks to
describe the highly complex, non-linear, and (maybe) non-convex
safe/unsafe state space, due to the complicated correlation among
state/sensor variables. As such, the boundary implemented by safety
checks in the cyber-space (illustrated in the Cyber bar on the left)
admits unsafe physical states, leading to under-approximation. The
two bars on the right similarly demonstrate over-approximation.
As we will illustrate in the motivation section, developers tend to
use configuration parameters that are derived from their domain
knowledge or even just conjecture in safety checks. For example,
they consider an angle deviation larger than 30 degree must indicate
a crash. However, there is no rational for this magic number of 30,
which may or may not correspond to a real crash, depending on
many other environmental factors.
(a) Under-approximation
(b) Over-approximation
Figure 1: CP-inconsistencies
Three parties are involved in CP-inconsistencies: control soft-
ware, the vehicle, and the environment. As such, CP-inconsistency
detection techniques ought to be able to model and reason about
these three aspects. The large body of existing software vulnerabil-
ity detection techniques are mostly based on program analysis or
program testing [16, 18, 21, 23, 54, 66, 69, 73, 82], and hence miss
the other two aspects. As such, they may report exploits that are
infeasible in the physical world. Recent works on using control
invariants to detect attacks on RV systems [15] model the program
and vehicle aspects as they leverage vehicle dynamics and program
instrumentation to detect deviations. However, they do not system-
atically consider the environment variable. Note that an exploit to a
CP-inconsistency vulnerability is essentially a set of realistic envi-
ronmental conditions under which the RV misbehaves, just like in
the Boeing-737 MAX cases. In practice, RVs need to go through sub-
stantial physical field tests and crash tests [3, 8, 12, 22] before they
are released. For example, Telsa tests self-driving cars on Califor-
nia roads and has to regularly report operation statistics to a state
agency. However, such physical tests can hardly stress the software
aspect. They are also extremely expensive and unlikely thorough,
while CP-inconsistencies are often boundary/corner cases.
This paper proposes a novel testing based CP-inconsistency de-
tection technique. In order to cohesively reason about the three
aforementioned aspects, it requires the inter-play of program anal-
ysis, RV system modeling, and high fidelity environment-aware
simulation, orchestrated by a multiple-objective search based test
driver. Specifically, program analysis is developed to extract the
safety checks, which are a set of predicates over state/sensor vari-
ables guarding some kind of safety violation. Both the subject RV
dynamics and its control logics are modeled to a list of equations
through System Identification (SI) [58]. These equations describe
how the vehicle behaves given the control objectives (e.g., a ref-
erence position) and the current states. SI derives such equations
through regression over a set of collected traces of vehicle oper-
ation. Note that the SI model does not consider the environment.
One can consider it as a virtual RV (VRV) that operates in an ideal
world without any environmental interference. A virtual test field
(VTF) is designed to consider all the relevant environmental factors.
For example, to test drones, the important factors are wind and
obstacles with different physical properties (e.g., shape, weight,
or inertia). VTF is like a real physical test field. The difference is
that it is realized inside a simulator, allowing a massive number of
testings. In a test, both the RV and the VRV operate on the same
input, which includes a navigation plan and environment setup.
A “real” crash is detected by observing substantial state deviation
between the VRV and the RV (in simulated VTF). For example,
assume the RV hits an obstacle on the VTF and completely stops
(i.e., a crash). The VRV is still moving as planned as it is oblivious
to the VTF. An under-approximation CP-inconsistency is identified
if such a crash cannot be detected by the safety checks. The test
driver models the discrepancy between the values of crash check
expressions extracted by program analysis, which measure how
close it is to detect the crash, and the real crash to a multiple objec-
tive function (over environmental variables). A search algorithm is
then used to minimize/maximize the objectives, in order to induce
an inconsistency. For example, if the test driver senses that intro-
ducing stronger wind and/or reducing the weight of an obstacle
helps enlarge the discrepancy, it will continue to do so.
Contribution. Our contributions are summarized as follows.
• We introduce a new type of vulnerability – CP-inconsistency
– for RVs, induced by inherent inadequacy of general-purpose
high level programming languages in describing complex
physics. These vulnerabilities are safety related and can be
exploited by solely manipulating environmental conditions.
They could lead to (life-threatening) physical damages.
• We propose a novel testing based technique to detect such
vulnerabilities. The technique features innovative inter-play
among program analysis, RV system modeling, simulation,
and multi-objective search based testing. It introduces virtual
RV and uses it to expose real crashes. This allows solving a
critical challenge - the construction of test oracle that pro-
vides the ground truth.
• We implement a prototype and apply it to 4 real-world
control programs for 8 robotic vehicles, including ground
rover and aerial quad-rotors. Our prototype finds 10 real
CP-inconsistency cases. These cases lead to disrupt normal
operations (e.g., unexpected landing) and deadly crashes.
CyberAnomalyCyberNormalPhysicalAnomalyPhysicalNormalAbnormalityMissing ExceptionsPhysicalCyberCyberAnomalyCyberNormalPhysicalAnomalyPhysicalNormalAbnormalityFalse AlarmsPhysicalCyberSession 1E: Cyberphysical Systems CCS '20, November 9–13, 2020, Virtual Event, USA264Threat Model. The attacker does not have access to the internals of
the RV system. He exploits an RV system with CP-inconsistency by
only manipulating external physical conditions. There are two kinds
of exploits. The first induces over-approximation vulnerabilities
such that the target RV stops operation (due to safety concerns)
while it is still functioning properly. The second induces under-
approximation vulnerabilities such that the RV continues to operate
while a malfunctioning has occurred. Neither requires exploiting
any cyber attack vector.
2 MOTIVATION
In this section, we use an example to motivate our technique. The
example demonstrates an under-approximation CP-inconsistency
vulnerability in ArduCopter [1], one of the most widely used drone
control programs. The vulnerability renders a quad-rotor unable to
detect physical crashes.
crash_counter = 0;
return;
crash_counter = 0;
return;
crash_counter = 0;
return;
if(!motor->armed() || land_completed) {
}
if(mode == ACRO || mode == FLIP) {
1 #define CRASH_CHECK_ACCEL_MAX 3.0f
2 #define CRASH_CHECK_ANGLE_DEVIATION 30.0f
3 #define CRASH_CHECK_TRIGGER_SEC 2
4
5 // conditions for crash check
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
gcs().send_text("Crash: Disarming");
init_disarm_motors();
crash_counter = 0;
return;
}
crash_counter++;
}
if(accel.length() >= CRASH_CHECK_ACCEL_MAX) {
}
if (angle_err = CRASH_CHECK_TRIGGER_SEC) {
}
Figure 2: Simplified example code of the crash checker
Figure 2 shows a simplified code snippet for crash check in
ArduCopter. The code is regularly executed by the scheduler as
part of the main control loop with a 400Hz frequency. It determines
whether the vehicle encounters a crash. In the code, five conditional
statements are used to perform the check. If any of these conditions
is satisfied, crash_counter is reset to zero, indicating no crash.
The first check (line 6) means that in a crash, the vehicle must have
the motor armed and it is not landed. The second check (line 10)
means that in a crash, the drone must not be in the ACRO or FLIP
flight modes, which allow turnover. In the third check (line 14), the
acceleration must be smaller than CRASH_CHECK_ACCEL_MAX
(a pre-defined constant value), indicating the drone is not under
control. The fourth (line 18) means that the angle error (between
target and current) is larger than CRASH_CHECK_ANGLE_DEVI-
ATION for a crash. If all the above conditions are not satisfied in
each control loop iteration, crash_counter is incremented by
one (line 22). Finally, the fifth check (line 24) determines that a crash
has occurred when crash_counter is greater than or equal to
CRASH_CEHCK_TRIGGER_SEC (e.g., 2 seconds) and accordingly
takes the counter-measure (line 26). Note that by analyzing the
program alone, without considering the physical vehicle or the
environment, one cannot determine if the system is vulnerable.
(a) Head-on crash and crash detected (disarming)
(b) Side crash and detection failed (subsequent crashes)
Figure 3: Different crash situations and reactions: (a) crash deter-
mined, (b) no crash determined. The simulation demos are available
at [30, 38])
To understand how the above crash checks lead to a wrong
decision during an actual crash, we compare two different crash
situations and show how the checks are executed correspondingly.
Figure 3 shows the two different crash conditions while the quadro-
tor performs a mission in the urban area. Figure 3a shows a head-on
collision where the quadrotor hits a heavy object with its front. This
causes the quadrotor to be stationary after the crash. In contrast, the
crash in Figure 3b illustrates a side collision where the quadrotor
hits a light object with some angle, and the impact is less than the
head-on collision. After the crash, the heading direction is changed
by the force caused by the collision and the quadrotor loses control
and bounces away in a random direction. Note that both cases have
critical impact on the safety of the vehicle and its surroundings (e.g.,
causing potential physical damages). For instance, the side collision
could cause the quadrotor to smash to the ground or into people.
Hence, counter-measures – e.g., disarming motors and releasing a
parachute – should be taken to prevent subsequent damage.
Back to the code, the checks successfully detect the first situa-
tion (i.e., the head-on collision), but miss the second (i.e., the side
collision). Specifically, in the second case, the third check takes
the true branch (and hence not a crash) because the acceleration
is larger than CRASH_CHECK_ACCEL_MAX after the crash as the
vehicle deflected but did not stop, and even if it had momentar-
ily stopped, the fifth check would take the false branch because
crash_counter did not exceed CRASH_CHECK_TRIGGER_-
SEC. Observe that the root cause is that the set of pre-defined
parameters and the simple range checks of the RV’s states are too
coarse to describe the delicate and varying physical crash situations.
We highly doubt that tuning the parameters would lead to sound
and practical solutions due to the highly complex inter-connections
among these state/sensor values and between these values and the
environment. One could set CRASH_CHECK_TRIGGER_SEC to a
large value such as 20 seconds, which may never create false alarms.
However, it is practically meaningless as with such a long reaction
delay, damage has already been incurred. While these problems
can be substantially mitigated on traditional manned vehicles by
human drivers, they pose prominent challenges for RVs.
Existing RV Testing. RV (physical) destructive test [3, 53] is per-
formed to ensure that safety design meets requirements. For exam-
ple, RV collision test intentionally crashes the vehicle to analyze its
Session 1E: Cyberphysical Systems CCS '20, November 9–13, 2020, Virtual Event, USA265impact [22, 50]. These tests are usually very expensive and hard to
conduct under various situations. Because of the cost of real tests,
simulated tests are also performed (e.g., LS-DYNA [59]) to study
RV’s behaviors in a collision. However, these tests are mostly focus-
ing on the physical aspects. They do not inspect the software and
are not able to use program information to improve testing. It is dif-
ficult for them to expose corner cases in which CP-inconsistencies
mostly reside.
Traditional software testing technique such as fault injection [47]
can be conducted for RV control programs. For example, software
fault localization (SFL) for control software [46] has been proposed
to inject specific types of program bugs (e.g., erroneous arithmetic
operations) and then try to locate them through testing. While the
technique can be used to determine if the injected bugs actually
cause physical misbehaviors, they cannot detect CP-inconsistencies
that are caused by latent bugs in control software and require consid-
ering the environment. RVFuzzer [55] performs fuzzing to identify
program parameters that fall in the specified legal ranges but cause
RV mis-behaviors. It monotonically increases/decreases parameter
values from the original/default ones until the resulted vehicle op-
eration trace significantly deviates from the trace of the original
parameters. In order to exploit the problems reported by RVFuzzer,
the attacker needs to be able to replace the default parameters with
the problematic ones (e.g., through some social engineering). In
contrast, the attack vector of CP-inconsistencies is just environ-
mental conditions. There is no need to access the RV or replace
parameters. In our motivating example, the vulnerability can be
exploited by setting the weight and the angle of the obstacle.