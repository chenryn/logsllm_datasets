Probabilistic Naming of Functions in Stripped Binaries
ACSAC 2020, December 7â€“11, 2020, Austin, USA
approach and model the conditional distribution ğ‘ (y | x) directly
without needing to model ğ‘ (x | y).
As depicted in Figure 3, the CRF built is of the general graph
form with relationships between known functions, known features
(known features of unknown functions), and unknown functions.
In our model, known vertices represent feature values or known
function names, e.g., Size = 5, name = read; unknown vertices
represent unknown symbol names. Edges between nodes repre-
sent relationships between feature values of which we define two
types: label-observation and label-label. Label-observation edges
represent relationships connecting known nodes in x to unknown
nodes in y and label-label edges represent relationships between
unknown nodes in y. Each feature function is replicated for each
symbol name ğ‘  âˆˆ S. This is implemented as a vector N of size |S|
with each element ğ‘› âˆˆ N â†’ [0, 1]. Our implementation exploits
the sparsity between connected functions across millions of unique
function names by storing each vector in a sparse matrix.
ğ‘¢ â†’ ğ‘ ğ‘‘
The feature functions used in building the CRF are listed in
Table 3. For pairwise feature functions, we track dependencies to
the ğ‘‘ğ‘¡â„ degree for ğ‘‘ âˆˆ {1, 2, 3}. To clarify, under the calleeğ‘‘ feature
function, each edge potential is a probability distribution over all
known symbol names S which describes the probability of the
symbol name transition ğ‘ ğ‘‘
ğ‘£ . This represents the probability
of a symbol ğ‘ ğ‘¢ being ğ‘‘ calls away from ğ‘ ğ‘£.
The CRF aims to predict the conditional probability over all
unknown nodes y simultaneously given the set of known nodes
x. Let ğº be a factor graph of relationships over all known symbols
x and all unknown symbols y, then (x, y) is a conditional random
field if for any value ğ‘¥ âˆˆ x, the distribution ğ‘ (y | x) factorizes
according to ğº. If we partition the graph ğº into maximal cliques
C = {ğ¶1, ğ¶2, ..., ğ¶ğ‘ƒ} and into a set of factors ğ¹ = {Î¨ğ‘}, then the
conditional distribution for the CRF is given by:


Î¨ğ‘(cid:0)yğ‘, xğ‘ ; ğœƒğ‘(cid:1)
(1)
ğ‘ (y | x) =
1
Z(x)
ğ¶ğ‘ âˆˆC
Î¨ğ‘ âˆˆğ¶ğ‘
where Z(x) is a normalizing constant.
All of our label-label feature functions are discrete and return 0
or 1 for each function name depending on if the relationship exists
in the training set. The weightings for pairwise feature functions
are repeated for each clique and can be thought of as a global
matrix between all function names N Ã— N. Then there exist |N|
pairwise feature functions between a known and unknown node
per relationship, e.g., for the first callee relationship, the probability
of a known function being called by every other function in S. We
set Î¨ğ‘ to be log linear for efficient inference and define it in the
usual way as follows:
Î¨ğ‘(yğ‘, xğ‘ ; ğœƒğ‘) = exp(cid:169)(cid:173)(cid:171)ğ¾ (ğ‘)âˆ‘ï¸
ğ‘˜=1
ğœƒğ‘ğ‘˜ ğ‘“ğ‘ğ‘˜(yğ‘, xğ‘)(cid:170)(cid:174)(cid:172)
whereby ğ¾(ğ‘) returns the feature functions connected to vertex
ğ‘. Both weightings ğœƒğ‘ğ‘˜ and feature functions ğ‘“ğ‘ğ‘˜ are indexed by
vertex ğ‘˜ and factor ğ‘ implying that each factor has its own set of
weights. As the graphical structure of binaries is not fixed, and
hence the structure of our CRF, our implementation replicates the
weightings of each feature function globally. The normalization
constant Z (x) is defined as
(2)
Unknown Functions Known Functions
Known Features
ğ‘¦2
ğ‘¦1
ğ‘¦3
ğ‘¦4
ğ‘¥0
Figure 3: A visualisation of a snapshot of the general graph
based condition random field showing known and unknown
nodes and the relationships between them. Different types
of relationships are represented by separate colors. Pairwise
and generic factor based feature functions are represented
by rectangles and polygons.
âˆ‘ï¸


ğ‘ğ‘ âˆˆC
Î¨ğ‘ âˆˆğ¶ğ‘
y
Î¨ğ‘(cid:0)yğ‘, xğ‘ ; ğœƒğ‘(cid:1)
Z (x) =
(3)
4.2 Parameter Estimation
To estimate the weightings associated with the CRF we use a maxi-
mum likelihood approach, i.e., ğœ½ is chosen such that the training
data has the highest probability under the model. We achieve this
by maximizing the pseudo log-likelihood given by Equation 4 over
all of our training set graphs ğ‘” âˆˆ G.
Gâˆ‘ï¸
Câˆ‘ï¸
ğ¾ (ğ‘)âˆ‘ï¸
ğ‘”
ğ‘
ğ‘˜=1
â„“ (ğœ½) =
ğœƒğ‘ğ‘˜ ğ‘“ğ‘ğ‘˜(yğ‘, xğ‘) âˆ’ ğœƒ 2
ğ‘ğ‘˜
2ğœ 2
(4)
As we aim to expect changes in structure and features we regular-
ize the log likelihood with Tikhonov regularization [50] so that we
do not overfit our model. We combine L-BFGS-B [8] on subgraphs
in the training set with stochastic gradient descent to iteratively
learn the optimal weightings for ğœ½.
As we assume a large collection of independent and identically
distributed samples in the training data, using a numerical approach
to maximizing the likelihood in a batch setting is unwarranted and
needlessly slow. We suspect that different items in the training
data from disconnected graphs provide similar information about
relationship parameters; therefore we opt to using a stochastic
method for optimizing the likelihood. While such an approach is
sub-optimal, we believe the trade off for training the CRF on big
code is acceptable.
7
ACSAC 2020, December 7â€“11, 2020, Austin, USA
James Patrick-Evans, Lorenzo Cavallaro, and Johannes Kinder
4.3 CRF Inference
Whilst exact inference algorithms exist for linear-chain and tree
based models, in the general case the problem has been shown to
be NP-hard. For inferring symbol names from the CRF we employ
Maximum a Posteriori (MAP) estimation using the approximate
inference method Loopy Belief Propagation [36] combined with an
optimized greedy algorithm based on stochastic gradient descent.
We use an approximate inference method in order to model large
and complex graphical structures with the possibility for many
loops whilst still being a tractable model for convergence. Our
greedy algorithm works by making small changes to a subset of
the most confident nodes in the model. In each run of Loopy Belief
Propagation we use a random permutation of message updates to
avoid local minima. By combining the two approaches it is hoped
the model falls into a global minimum rather than a weak local
minimum. The use of the CRF gives the possibility of inferring
functions that have large machine code differences to previous
instances based on the interactions with other known and unknown
functions that are more easily recognizable.
5 MATCHING FUNCTION NAMES
We now turn to the issue of matching semantically similar func-
tion names using both existing Natural Language Processing (NLP)
techniques (Â§5.1) and Symbol2Vec (Â§5.2).
5.1 Lexical Analysis
When inferring symbol names based on heuristics of the underly-
ing code, it is difficult to know if the inferred name is correct. As
previously mentioned in Â§3.3, multiple symbol names have exactly
the same machine code, e.g., xstrtol, strtol and __strtol have
the same byte sequence for the same compiler settings and come
from different software packages. For this reason, we perform a
series of measures adopted from NLP to compare the differences
between the inferred symbol name and the ground truth.
We first pre-process all function names to remove common
character sequences such as capital letters surrounded by under-
scores used to signify library versions, CPU extension named func-
tions such as function.avx512, added compiler notation such as
function.constprop and function.part, and ISA-specific nam-
ing of functions. This significantly reduces the number of unique
symbol names stored in our database. Upon comparing the names
of functions for a possible match, we first calculate the Leven-
shtein [30] distance between the symbol names to detect small
changes similar to appending a suffix or prepending a prefix. Sec-
ondly we perform canonicalization and tokenization on both the
inferred and ground truth before lemmatizing and word stem-
ming [21] each token in order to match words of different tenses
and cases. This enables us to match the symbol wd_compare and
wd_comparator based on the stemmed word compar. In name
canonicalization we maintain a list of common programming
abbreviations such as fd for â€˜file descriptorâ€™ or dir for â€˜direc-
toryâ€™ and then use the dynamic programming rod cutting algo-
rithm to match sub-sequences with a scoring function that prefers
longer word lengths in order to produce a set of word descrip-
tions for each function name. For example, the real function name
hexCharToInt after symbol canonicalization is represented by the
Table 4: Examples of five target words and their closest vec-
tor representations in Symbol2Vec using the cosine distance.
Target Symbol
grub_error
opendir
tls_init
tor_x509_cert_get
_cert_digests
clock_start
Closest Vectors in Symbol2Vec
grub_video_capture_set_active_render_target,
grub_crypto_gcry_error,
grub_font_draw_glyph,
grub_disk_filter_write
readdir, closedir, dirfw, rewinddir, readdir_r,
fdopendir
tls_context_new, mutt_ssl_starttls,
tls_deinit, eap_peer_sm_init, initialize_ctx
tor_tls_get_my_certs,
should_make_new_ed_keys,
router_get_consensus_status,
we_want_to_fetch_unknown_auth_certs
clock_stop, lindex_update_first,
lindex_update, index_fsub, index_denial
set {â„ğ‘’ğ‘¥ğ‘ğ‘‘ğ‘’ğ‘ğ‘–ğ‘šğ‘ğ‘™, ğ‘â„ğ‘ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘’ğ‘Ÿ, ğ‘¡ğ‘œ, ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘”ğ‘’ğ‘Ÿ}. We then use synonym
sets from the Wordnet [35] lexical database for the English language
to compare the synonyms of individual descriptions. A naming sim-
ilarity score is produced based on the Jaccard distance between the
matching canonicalization sets ğ‘¥ğ‘ and ğ‘ ğ‘ as given by:
ğ‘‘ ğ‘— (ğ‘¥ğ‘, ğ‘ ğ‘) =
(5)
|ğ‘¥ğ‘ âˆª ğ‘ ğ‘| âˆ’ |ğ‘¥ğ‘ âˆ© ğ‘ ğ‘|
|ğ‘¥ğ‘ âˆª ğ‘ ğ‘|
The Jaccard distance gives a measure of the overlapping similarity in
the synonyms of the canonical names for each function name. When
the distance falls below a given threshold we deem the function
descriptions to be similar.
This method aims to implement a subjective match on the similar-
ity between function names but may introduce false positives into
our results. However, our thresholds and techniques were derived
from manual analysis in order to align function name similarity
close to the decisions of a human analyst.
5.2 Symbol2Vec
Choosing an appropriate label for a function is a subjective goal in
which different entities may choose different labels for the same
function. The majority of the time we would hope that these labels
are similar for functionally equivalent code and exhibit a subset
of natural language features so that they can be compared similar
in our NLP matching stage (Â§5.1). Unfortunately one programmer
may choose a different name to that of another that does not match
in our NLP comparison whilst still being functionally relevant.
For example, consider the two real world functions that start a
network connection to a remote server and return a file handler
named init_connection and get_resource_handler. The two
functions share no lexical similarities and would be matched as
different function names under normal conditions. We create a
numerical method to serve as a metric for name similarity which
is able to alleviate this problem. We do this by projecting symbol
names into a high dimensional vector space such that functions
which are semantically similar appear close in the vector space and
functions that differ are far apart.
Analogous to Word2Vec [33], we modify the Continuous Bag Of
Words (CBOW) and Skip-Gram model in order to create Symbol2Vec;
8
Probabilistic Naming of Functions in Stripped Binaries
ACSAC 2020, December 7â€“11, 2020, Austin, USA
Table 5: Lexical analogies in Symbol2Vec, where aâˆ’b+c â‰ˆ d.
a
b
c
d
sha1_init_ctx
md5_init_ctx
md5_update
realloc
fopen
icmp_open
hci_connect
close_log_file
sendmsg
closepipe
nxt_recv_file
gethostent
sha2_hmac_update
http_server_init
usb_bulk_write
OPENSSL_CTX_init
ssh_connect
malloc
open
open
connect
fclose
write
close
recv
get
xmalloc
close
close
disconnect
fopen
recv
open
send
set
sha2_update
md5_update
init
write
init
close
read
free
connect
disconnect
dhcpcd_config_get dhcpcd_config_set
socket_accept
SQLConnect
gethostname
csu_fini
usb_bulk_send
SHA384File
Hread
SHA1File
btconnect
EndDocFile
nxt_send_buf
SHA256File
cprng_deinit
unix_sock_open
accept
connect
get
fini
send
fopen
close
disconnect
set
csu_init
recv
SH384_Init
SHA512_Init
read
SHA1_Init
connect
fclose
send
write
MD5_Init
disconnect
fopen
recv
SH256_Init
SHA1_Init
free
open
malloc
close