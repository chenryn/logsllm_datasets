system for analysis. PGA models network function white
boxes and gray boxes behavior expressed explicitly in an ex-
tended version of Pyretic. The Pyretic compiler can take
these descriptions and convert each network function box’s
behavior into a set of prioritized match-action rules. PGA
analyzes these rules to characterize the In Packet Space and
an Out Packet Space for each rule of every service function.
The In Packet Space of a rule is deﬁned as the ﬂow space
that would match and thus be processed by the rule, while
the Out Packet Space is the outcome of processing the In
Packet Space by the rule.
When merging two service function chains, PGA analyzes
every pair of function boxes composed of one function box
from each chain to identify a full dependency graph, and
possible conﬂicts between different function boxes. For each
function box pair, analysis considers every pair of match-
LB:
Policy:
Byte Counter:
Policy:
match: (dstip, Web.virtual_ip) >>
modify: (dstip: Web.real_ips)
match: (dstip, DC.real_ips) >>
count_bytes: {group_by: [dstip]}
Dependency
Rules:
match: (dstip, Web.virtual_ip) ->
set([modify: (dstip, Web.real_ips)])
Rules:
match: (dstip, DC.real_ips) ->
set([count_bytes:{group_by:[dstip]}])
Identity -> set([])
Identity -> set([])
Conflicts
Figure 5: Analysis of service functions to determine order.
action rules across the two service functions to ﬁnd all de-
pendencies and possible conﬂicts.
A dependency is identiﬁed from one rule to another if
the ﬁrst rule actively creates (through modify action) an Out
Packet Space that overlaps with the second rule’s In Packet
Space. In Fig. 5 we show the Pyretic policies for two func-
tion boxes, Load Balancer (LB) and Byte Counter. Pyretic’s
compiler has converted these policies into prioritized match-
action rules as shown below each Pyretic policy. In this ex-
ample, as shown by the Dependency arrow, the In Packet
Space of the Byte Counter is dependent on the Out Packet
Space of LB, because the action of LB modiﬁes the desti-
nation IP address to a real_ip address that is checked by the
match portion of the Byte Counter’s rule.
Possible conﬂicts between function boxes may be uncov-
ered by the analysis and avoided through ordering or re-
ported to users as possibly unresolvable conﬂicts. In Fig. 5,
one possible conﬂict is between the default drop action in the
Load Balancer and the byte counting in the Byte Counter,
because for the same In Packet Space (destination IP ad-
dress is not Web.virtual_ip), the two boxes have apparently
incompatible actions (drop versus count). Another possible
conﬂict is between the modify destination IP address action
of the Load Balancer and the default packet drop action of
Byte Counter. For the same In Packet Space (destination IP
is Web.virtual_ip) the two actions are modify destination IP
address versus drop, which are seemingly incompatible.
The ﬁnal service order is determined using topological
sort over the dependency graph. When there are more than
one possible order, we choose an order that satisﬁes con-
straints on the input edges, and ﬁnally resort to heuristics,
such as: a security service function, identiﬁed by its policy
that actively drops packets based on some criteria, should be
placed ahead of other service functions. This is consistent
with how networks are typically administered.
For reference, the ﬁnal composed graph obtained for our
example from Fig. 3 is shown in Fig. 6. Note that a single
function box from an input graph may be replicated across
multiple edges in the composed graph (e.g. DPI). These are
logically the same function box.
6. PROTOTYPE
Our prototype implementation contains ≈2.5K SLOC in
Python, including the following Pyretic extensions.
37DNS 
DPI 
53 
22,23,5900 
DPI 
53 
IAN 
80 
EBN 
Abbreviations: 
IAN: IT.Zone-A.Normal 
EBN: Engg.Zone-B.Normal 
WD: Web.Data Center 
80 
FW 
LB 
BC 
FW 
LB 
BC 
WD 
DD: DB.Data Center 
IAQ: IT.Zone-A.Quarantined 
EBQ: Engg.Zone-B.Quarantined 
7000, 
9909 
BC 
DD 
9909 
3306, 
9909 
BC 
BC 
IAQ 
EBQ 
* 
* 
Rmd 
BC 
9909 
Figure 6: Composed graph for the running example.
6.1 Abstractions
To support policy graph speciﬁcation, we extend Pyretic
with three primitives: EPGs, Function boxes and Whitelists.
EPG. The user may create EPGs and specify membership
requirement for endpoints to join the EPG through labels.
EPGs also contain member variables indicating the set of L4
ports and endpoint IP addresses, which will be dynamically
updated at runtime, and virtual IP addresses (used for load-
balancing) of that EPG. The user may reference these vari-
ables while writing policies without having to assign values
to them.
Function Box. The function boxes in our prototype are ex-
pressed in our extended Pyretic programming language. The
policies are described in terms of the names of EPG vari-
ables, like the set of endpoint IPs and virtual IP, without
having to assign values to the variables a priori. Both static
and dynamic policies are supported as long as the dynamic
policies contain an expression of their bounding behavior as
described in §4. This expression is required for analyzing
dependencies and conﬂicts between function boxes to de-
termine their intermixed order when merging edges. The
extended Pyretic language provides the necessary constructs
to express this behavior.
WhiteList. The user may express Whitelists as arbitrary
ranges of values for different ﬂow header ﬁelds, e.g.
WhiteList(dstport=[80,>8000],proto=[6]).
Set operations – Union, Intersection and Difference – are
supported to express more complex Whitelists.
6.2 System operation
As shown in Fig. 2, a composer module takes all the graphs
and any auxiliary inputs and generates a composed graph by
implementing the algorithm of §5. In our SDN prototype
of the runtime system, the composed graph is stored as an
in-memory hash table keyed with the source and destination
EPGs for fast lookup of policies at runtime.
OpenFlow rule generation: In principle, the eagerly com-
posed PGA policy could be enforced by proactively installing
an equivalent set of OpenFlow rules into the network
switches, determined using state of the art rule compilers [24,
36]. For simplicity, however, our runtime system implemen-
tation takes a fully reactive approach that evaluates the ﬁrst
packet of a new ﬂow (≈ OpenFlow pkt_in event) against the
composed policy graph and then installs rules to enforce the
ACL and service chain policies for the new ﬂow. For this,
we modify the Pyretic runtime to query the in-memory graph
upon a pkt_in event to lookup the ACL and service chain
policies, which are then compiled down to OpenFlow v1.0
rules through the Pyretic compiler and POX controller.
The underlying network topology is run on mininet [2].
The prototype system installs ﬂow rules on the edge switches
(similar to ref. [29]). The switch/port that each endpoint
(EP) is attached to is pulled out from mininet but can be
given by external sources such as 802.1X. The path from
source EP to destination EP is given by the Pyretic
mac_learner; more
routing
algorithms [38, 31, 26] can be used to support middleboxes
and include non-edge switches for policy enforcement.
sophisticated waypoint
Runtime veriﬁcation: The presence of service chains in
a composed PGA graph complicates runtime policy veriﬁ-
cation. We leave it to future work; but verify only whether
the ACL policies in the composed graph are correctly re-
alized on the network, by using VeriFlow [27] in our proto-
type, against potential violations due to: 1) the policy-to-rule
compiler having a bug in generating ﬂow rules, or 2) there
being another control module in charge of other types of
policies (e.g., trafﬁc engineering, switch ﬁrmware upgrade)
that changes ﬂow rules without involving PGA.
VeriFlow runs as a proxy intercepting OpenFlow mes-
sages between the controller and switches to detect three
event types: reachability setup, blackhole (incomplete rout-
ing path) and forwarding loop. We modify VeriFlow to re-
port detected events and affected ﬂows to the PGA runtime,
which caches prior pkt_in events and the ACL decisions
made for them. PGA compares the reported VeriFlow event
with the cached ACL decisions to verify the following: reach-
ability should be set up for allowed ﬂows and no path should
be set up for ACL-denied ﬂows. If the allowed path is not set
up properly (blackhole, loop, or lack of reachability setup),
it is a violation of PGA policy. Similarly, we raise an alarm
if a reachability setup event is reported for a pair of EPs for
which the composed PGA graph does not allow communi-
cation. Note that these VeriFlow veriﬁcations may not hold
if the service functions in the path drop/add/change packets.
7. PGA IN ACTION
We demonstrate the power of PGA in conﬂict detection &
resolution through two case studies on real world examples.
7.1 Conﬂict between SDN apps
We consider the following example inspired by a real
world anecdote of installing two SDN apps on the same net-
work: one is the QoS app in Fig. 7 and the other is the DNS
ﬁltering/protector app (Fig. 3). One way to handle the Open-
Flow rules generated from the two apps is to compose them
into one prioritized rule table to detect potential conﬂicts.
An exclusive set of non-contiguous priority ranges was man-
ually assigned to each app; e.g., the ﬁrst and third highest
ranges were given to the DNS protector while the second and
the fourth highest ranges to the QoS app in one table snap-
shot. Since there is no global priority coordination across the
apps, it is possible for the QoS app’s rule that marks QoS bits
38QoS App  
QoS App + DNS protector 
QOS 
Clients 
QOS 
VoIP 
Server 
QOS 
C.Qn 
* 
Rmd 
C.Nml 
53 
DPI 
DNS 
QOS 
VoIP 
Server 
Figure 7: QoS app vs. DNS protector.
of VoIP packets and forwards them to output ports to be as-
signed a higher priority than one of the DNS protector rules
that blocks quarantined IPs, potentially failing to block due
to the prioritized QoS rule. This potential conﬂict between
the two SDN apps was not initially detected because the two
rules were written for non-overlapping IP addresses in the
given snapshot of the rule table, although the two apps can
be applied to the same client devices in other settings.
Another potential conﬂict was detected between the QoS
rule and another DNS protector rule that redirects packets
from DNS servers to different output ports. However, the
detected conﬂict turned out to be a false positive as DNS
servers were not supposed to be VoIP clients, and so the two
rules actually don’t overlap. These examples demonstrate
the limitations of specifying and combining SDN apps at
OpenFlow rule level, coupled with prioritization [24] and IP
address assignment. PGA’s label-based policy speciﬁcation
and graph composition enables eager conﬂict detection/res-
olution without requiring prioritized rules and IP addresses.
On the right of Fig. 7 is the composed PGA policy from
the two apps. The EPG for quarantined clients (C.Qn) is set
to ‘exclusive’; this disallows any communication speciﬁed
by the QoS app graph. And it is clear that DNS servers do
not overlap with the client devices as there is no mapping
between the two labels (omitted in the ﬁgure).
7.2 Large Enterprise ACL
Enterprise dataset: We obtain ACL policies from the
policy management system in a large enterprise network.
These policies are deﬁned for each compartment, which are
EPs that share the same set of ACL rules. The ACLs permit
or deny the communications between the EPs within each
compartment. Policies are written by network admins or
application owners, manually reviewed for correctness and
consistency, and then deployed in over 30+ global sites.
By analyzing the policies for the entire 136 compartments,
we identiﬁed over 20K ACL policies that are written for
4916 EPGs. The sizes of EPGs vary, ranging from one IP
address to over 600 non-contiguous subnets (100M IP ad-
dresses). We construct label mapping based on the super-
sub set relationship between EPGs. Note that one EPG has
multiple parents since the EPG may contain non-contiguous
address blocks. We aggregate multiple policies for the same
EPG pair by listing multiple protocol ﬁelds and port ranges
for one edge, creating a total of 11,786 edges.
Redundancy and conﬂict are detected using PGA in this
dataset. A rule is redundant if there exists another rule in the
same compartment that completely covers the former rule’s
space with the same action. We have identiﬁed that 7% of
the aggregated policies are redundant. PGA also detects two
types of conﬂicts: unmatched outgoing rules, i.e., an outgo-
ing rule in source compartment without the incoming rule to
permit the communication in the destination compartment;
and unmatched incoming rules, i.e., the incoming rule with-
out the matching outgoing rule. The former is more damag-
ing since it will result in blackholing. The latter means the
incoming rules are not used since no trafﬁc will be sent out.
In total, we detect that 4.7% of outgoing policies and 4.5%
of incoming policies are unmatched.
8. SYSTEM EVALUATION
For our experiments we used three data sets
D1: the synthetic running example from §4 and §5.
D2: the large enterprise dataset as described in §7.2.
D3: D2 with randomly added function boxes.
Our primary results show that due to eager composition,
the runtime overhead of PGA is minimal even for very large
composed graphs. PGA is practical and scalable, being able
to analyze and compose thousands of policies producing
nearly a million edges in under 600s when considering only
ACLs and 800s in most cases for policy graphs with both
ACLs and service chains. As described earlier, the com-
posed graphs are correct by design. Additionally, for D1, we
verify the correctness manually and for D2 and D3, we use
Veriﬂow for indirect validation with reachability analysis.
The PGA prototype system is implemented in Python and
currently runs as a single threaded program. For our eval-
uations, PGA is deployed on a server with 2x8 Intel Xeon
2.6 GHz cores with 132 GB memory running Linux ker-
nel 3.13.0. We emulate switches and hosts with mininet
2.1.0 and openvswitch 2.0.2 on the server in order to create
a topology and generate packets between hosts in different
EPGs. Since the focus here is not to optimally assign ﬂow
rules to switches, the results described use simple topologies
with one or two switches providing connectivity to the end
hosts. We randomly generate packets between hosts to an-
alyze reachability. A POX controller running on the same
server uses PGA to look-up the policy to be applied upon re-
ceiving a pkt_in event. The input graphs are pre-composed
by PGA and the composed graph is stored in memory. The