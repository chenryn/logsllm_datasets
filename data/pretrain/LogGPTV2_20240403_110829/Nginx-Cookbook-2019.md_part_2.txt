$ nginx -v
nginx version: nginx/1.15.3
As this example shows, the response displays the version.
You can confirm that NGINX is running by using the following
command:
$ ps -ef | grep nginx
root 1738 1 0 19:54 ? 00:00:00 nginx: master process
nginx 1739 1738 0 19:54 ? 00:00:00 nginx: worker process
The ps command lists running processes. By piping it to grep, you
can search for specific words in the output. This example uses grep
to search for nginx. The result shows two running processes, a mas
ter and worker. If NGINX is running, you will always see a master
and one or more worker processes. For instructions on starting
NGINX, refer to the next section. To see how to start NGINX as a
daemon, use the init.d or systemd methodologies.
To verify that NGINX is returning requests correctly, use your
browser to make a request to your machine or use curl:
$ curl localhost
You will see the NGINX Welcome default HTML site.
Discussion
The nginx command allows you to interact with the NGINX binary
to check the version, list installed modules, test configurations, and
send signals to the master process. NGINX must be running in
order for it to serve requests. The ps command is a surefire way to
determine whether NGINX is running either as a daemon or in the
foreground. The default configuration provided by default with
4 | Chapter 1: Basics
NGINX runs a static site HTTP server on port 80. You can test this
default site by making an HTTP request to the machine at local
host as well as the host’s IP and hostname.
1.5 Key Files, Commands, and Directories
Problem
You need to understand the important NGINX directories and
commands.
Solution
NGINX files and directories
/etc/nginx/
The /etc/nginx/ directory is the default configuration root for
the NGINX server. Within this directory you will find configu‐
ration files that instruct NGINX on how to behave.
/etc/nginx/nginx.conf
The /etc/nginx/nginx.conf file is the default configuration entry
point used by the NGINX service. This configuration file sets up
global settings for things like worker process, tuning, logging,
loading dynamic modules, and references to other NGINX con‐
figuration files. In a default configuration, the /etc/nginx/
nginx.conf file includes the top-level http block, which includes
all configuration files in the directory described next.
/etc/nginx/conf.d/
The /etc/nginx/conf.d/ directory contains the default HTTP
server configuration file. Files in this directory ending in .conf
are included in the top-level http block from within the /etc/
nginx/nginx.conf file. It’s best practice to utilize include state‐
ments and organize your configuration in this way to keep your
configuration files concise. In some package repositories, this
folder is named sites-enabled, and configuration files are linked
from a folder named site-available; this convention is depre‐
cated.
1.5 Key Files, Commands, and Directories | 5
/var/log/nginx/
The /var/log/nginx/ directory is the default log location for
NGINX. Within this directory you will find an access.log file and
an error.log file. The access log contains an entry for each
request NGINX serves. The error log file contains error events
and debug information if the debug module is enabled.
NGINX commands
nginx -h
Shows the NGINX help menu.
nginx -v
Shows the NGINX version.
nginx -V
Shows the NGINX version, build information, and configura‐
tion arguments, which shows the modules built in to the
NGINX binary.
nginx -t
Tests the NGINX configuration.
nginx -T
Tests the NGINX configuration and prints the validated config‐
uration to the screen. This command is useful when seeking
support.
nginx -s signal
The -s flag sends a signal to the NGINX master process. You
can send signals such as stop, quit, reload, and reopen. The
stop signal discontinues the NGINX process immediately. The
quit signal stops the NGINX process after it finishes processing
inflight requests. The reload signal reloads the configuration.
The reopen signal instructs NGINX to reopen log files.
Discussion
With an understanding of these key files, directories, and com‐
mands, you’re in a good position to start working with NGINX.
With this knowledge, you can alter the default configuration files
and test your changes by using the nginx -t command. If your test
6 | Chapter 1: Basics
is successful, you also know how to instruct NGINX to reload its
configuration using the nginx -s reload command.
1.6 Serving Static Content
Problem
You need to serve static content with NGINX.
Solution
Overwrite the default HTTP server configuration located in /etc/
nginx/conf.d/default.conf with the following NGINX configuration
example:
server {
listen 80 default_server;
server_name www.example.com;
location / {
root /usr/share/nginx/html;
# alias /usr/share/nginx/html;
index index.html index.htm;
}
}
Discussion
This configuration serves static files over HTTP on port 80 from the
directory /usr/share/nginx/html/. The first line in this configuration
defines a new server block. This defines a new context for NGINX
to listen for. Line two instructs NGINX to listen on port 80, and the
default_server parameter instructs NGINX to use this server as
the default context for port 80. The server_name directive defines
the hostname or names of which requests should be directed to this
server. If the configuration had not defined this context as the
default_server, NGINX would direct requests to this server only if
the HTTP host header matched the value provided to the
server_name directive.
The location block defines a configuration based on the path in the
URL. The path, or portion of the URL after the domain, is referred
to as the URI. NGINX will best match the URI requested to a loca
1.6 Serving Static Content | 7
tion block. The example uses / to match all requests. The root
directive shows NGINX where to look for static files when serving
content for the given context. The URI of the request is appended to
the root directive’s value when looking for the requested file. If we
had provided a URI prefix to the location directive, this would be
included in the appended path, unless we used the alias directory
rather than root. Lastly, the index directive provides NGINX with a
default file, or list of files to check, in the event that no further path
is provided in the URI.
1.7 Graceful Reload
Problem
You need to reload your configuration without dropping packets.
Solution
Use the reload method of NGINX to achieve a graceful reload of
the configuration without stopping the server:
$ nginx -s reload
This example reloads the NGINX system using the NGINX binary
to send a signal to the master process.
Discussion
Reloading the NGINX configuration without stopping the server
provides the ability to change configurations on the fly without
dropping any packets. In a high-uptime, dynamic environment, you
will need to change your load-balancing configuration at some
point. NGINX allows you to do this while keeping the load balancer
online. This feature enables countless possibilities, such as rerun‐
ning configuration management in a live environment, or building
an application- and cluster-aware module to dynamically configure
and reload NGINX to meet the needs of the environment.
8 | Chapter 1: Basics
CHAPTER 2
High-Performance Load Balancing
2.0 Introduction
Today’s internet user experience demands performance and uptime.
To achieve this, multiple copies of the same system are run, and
the load is distributed over them. As the load increases, another
copy of the system can be brought online. This architecture techni‐
que is called horizontal scaling. Software-based infrastructure is
increasing in popularity because of its flexibility, opening up a vast
world of possibilities. Whether the use case is as small as a set of
two for high availability or as large as thousands around the globe,
there’s a need for a load-balancing solution that is as dynamic as
the infrastructure. NGINX fills this need in a number of ways,
such as HTTP, TCP, and UDP load balancing, which we cover in
this chapter.
When balancing load, it’s important that the impact to the client is
only a positive one. Many modern web architectures employ state‐
less application tiers, storing state in shared memory or databases.
However, this is not the reality for all. Session state is immensely val‐
uable and vast in interactive applications. This state might be stored
locally to the application server for a number of reasons; for exam‐
ple, in applications for which the data being worked is so large that
network overhead is too expensive in performance. When state is
stored locally to an application server, it is extremely important to
the user experience that the subsequent requests continue to be
delivered to the same server. Another facet of the situation is that
9
servers should not be released until the session has finished. Work‐
ing with stateful applications at scale requires an intelligent load bal‐
ancer. NGINX Plus offers multiple ways to solve this problem by
tracking cookies or routing. This chapter covers session persistence
as it pertains to load balancing with NGINX and NGINX Plus.
Ensuring that the application NGINX is serving is healthy is also
important. For a number of reasons, applications fail. It could be
because of network connectivity, server failure, or application fail‐
ure, to name a few. Proxies and load balancers must be smart
enough to detect failure of upstream servers and stop passing traffic
to them; otherwise, the client will be waiting, only to be delivered a
timeout. A way to mitigate service degradation when a server fails is
to have the proxy check the health of the upstream servers. NGINX
offers two different types of health checks: passive, available in the
open source version; and active, available only in NGINX Plus.
Active health checks at regular intervals will make a connection or
request to the upstream server and can verify that the response is
correct. Passive health checks monitor the connection or responses
of the upstream server as clients make the request or connection.
You might want to use passive health checks to reduce the load of
your upstream servers, and you might want to use active health
checks to determine failure of an upstream server before a client is
served a failure. The tail end of this chapter examines monitoring
the health of the upstream application servers for which you’re load
balancing.
2.1 HTTP Load Balancing
Problem
You need to distribute load between two or more HTTP servers.
Solution
Use NGINX’s HTTP module to load balance over HTTP servers
using the upstream block:
upstream backend {
server 10.10.12.45:80 weight=1;
server app.example.com:80 weight=2;
}
server {
10 | Chapter 2: High-Performance Load Balancing
location / {
proxy_pass http://backend;
}
}
This configuration balances load across two HTTP servers on port
80. The weight parameter instructs NGINX to pass twice as many
connections to the second server, and the weight parameter defaults
to 1.
Discussion
The HTTP upstream module controls the load balancing for HTTP.
This module defines a pool of destinations—any combination of
Unix sockets, IP addresses, and DNS records, or a mix. The
upstream module also defines how any individual request is
assigned to any of the upstream servers.
Each upstream destination is defined in the upstream pool by the
server directive. The server directive is provided a Unix socket, IP
address, or an FQDN, along with a number of optional parameters.
The optional parameters give more control over the routing of
requests. These parameters include the weight of the server in the
balancing algorithm; whether the server is in standby mode, avail‐
able, or unavailable; and how to determine if the server is unavail‐
able. NGINX Plus provides a number of other convenient
parameters like connection limits to the server, advanced DNS reso‐
lution control, and the ability to slowly ramp up connections to a
server after it starts.
2.2 TCP Load Balancing
Problem
You need to distribute load between two or more TCP servers.
Solution
Use NGINX’s stream module to load balance over TCP servers
using the upstream block:
stream {
upstream mysql_read {
server read1.example.com:3306 weight=5;
2.2 TCP Load Balancing | 11
server read2.example.com:3306;
server 10.10.12.34:3306 backup;
}
server {
listen 3306;
proxy_pass mysql_read;
}
}
The server block in this example instructs NGINX to listen on TCP
port 3306 and balance load between two MySQL database read rep‐
licas, and lists another as a backup that will be passed traffic if the
primaries are down. This configuration is not to be added to the
conf.d folder as that folder is included within an http block;
instead, you should create another folder named stream.conf.d,
open the stream block in the nginx.conf file, and include the new
folder for stream configurations.
Discussion
TCP load balancing is defined by the NGINX stream module. The
stream module, like the HTTP module, allows you to define upstream
pools of servers and configure a listening server. When configuring
a server to listen on a given port, you must define the port it’s to lis‐
ten on, or optionally, an address and a port. From there, a destina‐
tion must be configured, whether it be a direct reverse proxy to
another address or an upstream pool of resources.
The upstream for TCP load balancing is much like the upstream for
HTTP, in that it defines upstream resources as servers, configured
with Unix socket, IP, or fully qualified domain name (FQDN), as
well as server weight, max number of connections, DNS resolvers,
and connection ramp-up periods; and if the server is active, down,
or in backup mode.
NGINX Plus offers even more features for TCP load balancing.
These advanced features offered in NGINX Plus can be found
throughout this book. Health checks for all load balancing will be
covered later in this chapter.
12 | Chapter 2: High-Performance Load Balancing
2.3 UDP Load Balancing
Problem
You need to distribute load between two or more UDP servers.
Solution
Use NGINX’s stream module to load balance over UDP servers
using the upstream block defined as udp:
stream {
upstream ntp {
server ntp1.example.com:123 weight=2;
server ntp2.example.com:123;
}
server {
listen 123 udp;
proxy_pass ntp;
}
}
This section of configuration balances load between two upstream
Network Time Protocol (NTP) servers using the UDP protocol.
Specifying UDP load balancing is as simple as using the udp param‐
eter on the listen directive.
If the service you’re load balancing over requires multiple packets to
be sent back and forth between client and server, you can specify the
reuseport parameter. Examples of these types of services are
OpenVPN, Voice over Internet Protocol (VoIP), virtual desktop sol‐
utions, and Datagram Transport Layer Security (DTLS). The follow‐
ing is an example of using NGINX to handle OpenVPN connections
and proxy them to the OpenVPN service running locally:
stream {
server {
listen 1195 udp reuseport;
proxy_pass 127.0.0.1:1194;
}
}
2.3 UDP Load Balancing | 13
Discussion
You might ask, “Why do I need a load balancer when I can have
multiple hosts in a DNS A or SRV record?” The answer is that not
only are there alternative balancing algorithms with which we can
balance, but we can load balance over the DNS servers themselves.
UDP services make up a lot of the services that we depend on in
networked systems, such as DNS, NTP, and VoIP. UDP load balanc‐
ing might be less common to some but just as useful in the world of
scale.
You can find UDP load balancing in the stream module, just like
TCP, and configure it mostly in the same way. The main difference
is that the listen directive specifies that the open socket is for
working with datagrams. When working with datagrams, there are
some other directives that might apply where they would not in
TCP, such as the proxy_response directive, which specifies to
NGINX how many expected responses can be sent from the
upstream server. By default, this is unlimited until the proxy_time
out limit is reached.
The reuseport parameter instructs NGINX to create an individual
listening socket for each worker process. This allows the kernel to
distibute incoming connections between worker processes to handle
multiple packets being sent between client and server. The reuse
port feature works only on Linux kernels 3.9 and higher, DragonFly
BSD, and FreeBSD 12 and higher.
2.4 Load-Balancing Methods
Problem
Round-robin load balancing doesn’t fit your use case because you
have heterogeneous workloads or server pools.
Solution
Use one of NGINX’s load-balancing methods such as least connec‐
tions, least time, generic hash, IP hash, or random:
upstream backend {
least_conn;
server backend.example.com;
14 | Chapter 2: High-Performance Load Balancing
server backend1.example.com;
}
This example sets the load-balancing algorithm for the backend
upstream pool to be least connections. All load-balancing algo‐
rithms, with the exception of generic hash, random, and least-time,
are standalone directives, such as the preceding example. The
parameters to these directives are explained in the following discus‐
sion.
Discussion
Not all requests or packets carry equal weight. Given this, round
robin, or even the weighted round robin used in previous examples,
will not fit the need of all applications or traffic flow. NGINX pro‐
vides a number of load-balancing algorithms that you can use to fit
particular use cases. In addition to being able to choose these load-
balancing algorithms or methods, you can also configure them. The
following load-balancing methods are available for upstream HTTP,