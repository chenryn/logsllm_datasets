details for it and you can also attach it as a new tab and continue iterating
on the query. It is also possible to download the "query detail archive", a
JSON file containing all the important details for a given query to use for
troubleshooting.
#### # Connect external data flow
Connect external data flow lets you use the sampler to sample your source data
to, determine its schema and generate a fully formed SQL query that you can
edit to fit your use case before you launch your ingestion job. This point-
and-click flow will save you much typing.
![](https://user-
images.githubusercontent.com/177816/185309631-5ceed7d0-2bb2-43b9-83ed-
fdcad4a152be.png)
#### # Preview button
The Preview button appears when you type in an INSERT or REPLACE SQL query.
Click the button to remove the INSERT or REPLACE clause and execute your query
as an "inline" query with a limi). This gives you a sense of the shape of your
data after Druid applies all your transformations from your SQL query.
#### # Results table
The query results table has been improved in style and function. It now shows
you type icons for the column types and supports the ability to manipulate
nested columns with ease.
#### # Helper queries
The Web Console now has some UI affordances for notebook and CTE users. You
can reference helper queries, collapsable elements that hold a query, from the
main query just like they were defined with a WITH statement. When you are
composing a complicated query, it is helpful to break it down into multiple
queries to preview the parts individually.
#### # Additional Web Console tools
More tools are available from the ... menu:
  * Explain query - show the query plan for sql-native and multi-stage query task engine queries.
  * Convert ingestion spec to SQL - Helps you migrate your native batch and Hadoop based specs to the SQL-based format.
  * Open query detail archive - lets you open a query detail archive downloaded earlier.
  * Load demo queries - lets you load a set of pre-made queries to play around with multi-stage query task engine functionality.
### # New SQL-based data loader
The data loader exists as a GUI wizard to help users craft a JSON ingestion
spec using point and click and quick previews. The SQL data loader is the SQL-
based ingestion analog of that.
Like the native based data loader, the SQL-based data loader stores all the
state in the SQL query itself. You can opt to manipulate the query directly at
any stage. See (#12919) for more information about how the data loader differs
from the **Connect external data** workflow.
### # Other changes and improvements
  * The query view has so much new functionality that it has moved to the far left as the first view available in the header.
  * You can now click on a datasource or segment to see a preview of the data within.
  * The task table now explicitly shows if a task has been canceled in a different color than a failed task.
  * The user experience when you view a JSON payload in the Druid console has been improved. There’s now syntax highlighting and a search.
  * The Druid console can now use the column order returned by a scan query to determine the column order for reindexing data.
  * The way errors are displayed in the Druid console has been improved. Errors no longer appear as a single long line.
See (#12919) for more details and other improvements
## # Metrics
### # Sysmonitor stats for Peons
Sysmonitor stats, like memory or swap, are no longer reported since Peons
always run on the same host as MiddleManagerse. This means that duplicate
stats will no longer be reported.
#12802
### # Prometheus
You can now include the host and service as labels for Prometheus by setting
the following properties to true:
  * `druid.emitter.prometheus.addHostAsLabel`
  * `druid.emitter.prometheus.addServiceAsLabel`
#12769
### # Rows per segment
(Experimental) You can now see the average number of rows in a segment and the
distribution of segments in predefined buckets with the following metrics:
`segment/rowCount/avg` and `segment/rowCount/range/count`.  
Enable the metrics with the following property:
`org.apache.druid.server.metrics.SegmentStatsMonitor`  
#12730
### # New `sqlQuery/planningTimeMs` metric
There’s a new `sqlQuery/planningTimeMs` metric for SQL queries that computes
the time it takes to build a native query from a SQL query.
#12923
### # StatsD metrics reporter
The StatsD metrics reporter extension now includes the following metrics:
  * coordinator/time
  * coordinator/global/time
  * tier/required/capacity
  * tier/total/capacity
  * tier/replication/factor
  * tier/historical/count
  * compact/task/count
  * compactTask/maxSlot/count
  * compactTask/availableSlot/count
  * segment/waitCompact/bytes
  * segment/waitCompact/count
  * interval/waitCompact/count
  * segment/skipCompact/bytes
  * segment/skipCompact/count
  * interval/skipCompact/count
  * segment/compacted/bytes
  * segment/compacted/count
  * interval/compacted/count  
#12762
### # New worker level task metrics
Added a new monitor, `WorkerTaskCountStatsMonitor`, that allows each middle
manage worker to report metrics for successful / failed tasks, and task slot
usage.
#12446
### # Improvements to the JvmMonitor
The JvmMonitor can now handle more generation and collector scenarios. The
monitor is more robust and works properly for ZGC on both Java 11 and 15.
#12469
### # Garbage collection
Garbage collection metrics now use MXBeans.
#12481
### # Metric for task duration in the pending queue
Introduced the metric `task/pending/time` to measure how long a task stays in
the pending queue.
#12492
### # Emit metrics object for Scan, Timeseries, and GroupBy queries during
cursor creation
Adds vectorized metric for scan, timeseries and groupby queries.
#12484
### # Emit state of replace and append for native batch tasks
Druid now emits metrics so you can monitor and assess the use of different
types of batch ingestion, in particular replace and tombstone creation.
#12488  
#12840
### # KafkaEmitter emits `queryType`
The KafkaEmitter now properly emits the `queryType` property for native
queries.
#12915
## # Security
You can now hide properties that are sensitive in the API response from
`/status/properties`, such as S3 access keys. Use the
`druid.server.hiddenProperties` property in `common.runtime.properties` to
specify the properties (case insensitive) you want to hide.
#12950
## # Other changes
  * You can now configure the retention period for request logs stored on disk with the `druid.request.logging.durationToRetain` property. Set the retention period to be longer than `P1D` (#12559)
  * You can now specify liveness and readiness probe delays for the historical StatefulSet in your values.yaml file. The default is 60 seconds (#12805)
  * Improved exception message for native binary operators (#12335)
  * ​​Improved error messages when URI points to a file that doesn't exist (#12490)
  * ​​Improved build performance of modules (#12486)
  * Improved lookups made using the druid-kafka-extraction-namespace extension to handle records that have been deleted from a kafka topic (#12819)
  * Updated core Apache Kafka dependencies to 3.2.0 (#12538)
  * Updated ORC to 1.7.5 (#12667)
  * Updated Jetty to 9.4.41.v20210516 (#12629)
  * Added `Zstandard` compression library to `CompressionStrategy` (#12408)
  * Updated the default gzip buffer size to 8 KB to for improved performance (#12579)
  * Updated the default `inputSegmentSizeBytes` in Compaction configuration to 100,000,000,000,000 (~100TB)
# # Bug fixes
Druid 24.0 contains over 68 bug fixes. You can find the complete list here
# # Upgrading to 24.0
## # Permissions for multi-stage query engine
To read external data using the multi-stage query task engine, you must have
READ permissions for the EXTERNAL resource type. Users without the correct
permission encounter a 403 error when trying to run SQL queries that include
EXTERN.
The way you assign the permission depends on your authorizer. For example,
with [basic security]((/docs/development/extensions-core/druid-basic-
security.md) in Druid, add the `EXTERNAL READ` permission by sending a `POST`
request to the roles API.
The example adds permissions for users with the `admin` role using a basic
authorizer named `MyBasicMetadataAuthorizer`. The following permissions are
granted:
  * DATASOURCE READ
  * DATASOURCE WRITE
  * CONFIG READ
  * CONFIG WRITE
  * STATE READ
  * STATE WRITE
  * EXTERNAL READ
    curl --location --request POST 'http://localhost:8081/druid-ext/basic-security/authorization/db/MyBasicMetadataAuthorizer/roles/admin/permissions' \
    --header 'Content-Type: application/json' \
    --data-raw '[
    {
      "resource": {
        "name": ".*",
        "type": "DATASOURCE"
      },
      "action": "READ"
    },
    {
      "resource": {
        "name": ".*",
        "type": "DATASOURCE"
      },
      "action": "WRITE"
    },
    {
      "resource": {
        "name": ".*",
        "type": "CONFIG"
      },
      "action": "READ"
    },
    {
      "resource": {
        "name": ".*",
        "type": "CONFIG"
      },
      "action": "WRITE"
    },
    {
      "resource": {
        "name": ".*",
        "type": "STATE"
      },
      "action": "READ"
    },
    {
      "resource": {
        "name": ".*",
        "type": "STATE"
      },
      "action": "WRITE"
    },
    {
      "resource": {
        "name": "EXTERNAL",
        "type": "EXTERNAL"
      },
      "action": "READ"
    }
    ]'
## # Behavior for unused segments
Druid automatically retains any segments marked as unused. Previously, Druid
permanently deleted unused segments from metadata store and deep storage after
their duration to retain passed. This behavior was reverted from `0.23.0`.  
#12693
## # Default for `druid.processing.fifo`
The default for `druid.processing.fifo` is now true. This means that tasks of
equal priority are treated in a FIFO manner. For most use cases, this change
can improve performance on heavily loaded clusters.
#12571
## # Update to JDBC statement closure
In previous releases, Druid automatically closed the JDBC Statement when the
ResultSet was closed. Druid closed the ResultSet on EOF. Druid closed the
statement on any exception. This behavior is, however, non-standard.  
In this release, Druid's JDBC driver follows the JDBC standards more closely:  
The ResultSet closes automatically on EOF, but does not close the Statement or
PreparedStatement. Your code must close these statements, perhaps by using a
try-with-resources block.  
The PreparedStatement can now be used multiple times with different
parameters. (Previously this was not true since closing the ResultSet closed
the PreparedStatement.)  
If any call to a Statement or PreparedStatement raises an error, the client
code must still explicitly close the statement. According to the JDBC
standards, statements are not closed automatically on errors. This allows you
to obtain information about a failed statement before closing it.  
If you have code that depended on the old behavior, you may have to change
your code to add the required close statement.
#12709
## # Known issues
## # Credits
@2bethere  
@317brian  
@a2l007  
@abhagraw  
@abhishekagarwal87  
@abhishekrb19  
@adarshsanjeev  
@aggarwalakshay  
@AmatyaAvadhanula  
@BartMiki  
@capistrant  
@chenrui333  
@churromorales  
@clintropolis  
@cloventt  
@CodingParsley  
@cryptoe  
@dampcake  
@dependabot[bot]  
@dherg  
@didip  
@dongjoon-hyun  
@ektravel  
@EsoragotoSpirit  
@exherb  
@FrankChen021  
@gianm  
@hellmarbecker  
@hwball  
@iandr413  
@imply-cheddar  
@jarnoux  
@jasonk000  
@jihoonson  
@jon-wei  
@kfaraz  
@LakshSingla  
@liujianhuanzz  
@liuxiaohui1221  
@lmsurpre  
@loquisgon  
@machine424  
@maytasm  
@MC-JY  
@Mihaylov93  
@nishantmonu51  
@paul-rogers  
@petermarshallio  
@pjfanning  
@rockc2020  
@rohangarg  
@somu-imply  
@suneet-s  
@superivaj  
@techdocsmith  
@tejaswini-imply  
@TSFenwick  
@vimil-saju  
@vogievetsky  
@vtlim  
@williamhyun  
@wiquan  
@writer-jill  
@xvrl  
@yuanlihan  
@zachjsh  
@zemin-piao