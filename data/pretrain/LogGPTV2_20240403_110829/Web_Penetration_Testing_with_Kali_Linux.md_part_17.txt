Patch management is one of the best defense strategies for avoiding many of the
threats presented in this textbook. Make sure to revisit how your organization
handles patch management periodically to avoid being a victim of exposing a
vulnerable system that could have been secured. This should apply to all managed
assets, including servers and web applications.
Password policies
In general, having a policy that controls the possible outcomes can negatively impact
the strength of passwords. Regardless of the policy, users will by human nature, try
to simplify passwords anyway possibly by using repeating characters, predictable
behavior, such as using 12345 to extend the length of a password, or other means.
Users will also typically not change passwords unless forced by a system. For these
reasons, a password policy should follow the following guidelines:
• Have an expiration that is under 90 days
• Not permit the last five passwords as replacements
• Enforce a length of at least 12 characters
• Not limit any characters, such as special characters
• Mandate at least one uppercase, number, and special character
• Warn or deny repeating digits such as 12345 or asdfg to avoid
brute-force attacks
Computer processing is constantly improving, meaning 12 character
passwords will not be strong in the near future. A recent article
published in spring 2013, stated a team of hackers cracked more than
14,800, 16-character cryptographically hashed passwords from a list
of 16,449. This is a special case at the time of publishing; however,
it will be the common battleground for future hackers. Consider the
recommended length of a password a moving target.
The authors of this book are fans of password generator by Steve Gibson, as a secure
method of generating random passwords. The secure random password generator
by Steve Gibson can be found at the Gibson Research Center at: https://www.grc.com/
passwords.htm.
[ 256 ]
www.it-ebooks.info
Chapter 7
Many websites and web applications are compromised, because web
developers implemented poor security protocols. Web developers
should use strong encryption to store user passwords and data.
Passwords should implement hashing and salting techniques to further
mitigate risks in stolen or lost data.
You can evaluate the strength of passwords used on your systems leveraging
password-cracking tools covered in Chapter 3, Server Side Attacks, and Chapter 4,
Client Side Attacks of this textbook. Suggested tools are John the Ripper, Johnny,
Hashcat, oclHashcat, and Ophcrack. Crunch and Hashcat can be also used to
generate password lists that can validate the strength of your password policy.
There are websites available, such as Crackstation, that offer pre-generated
lists of popular passwords. You could use these lists to test the strength of
your passwords and policies.
Mirror your environment
Before testing a system against a recommended security setting, checking for
vulnerabilities, or validating a vulnerable system through exploitation, it may
make sense to clone your system for testing purposes, rather than testing the real
system. Best practices are replicating everything from the hardware hosting the web
application to all content because vulnerabilities can exist in all technology layers.
Testing a cloned environment will give the Penetration Tester freedom to execute
any degree of attack while avoiding negative impact to operations. Although most
people cannot mirror the exact environment, it is usually possible to set up a virtual
environment with the same functionality.
HTTrack
HTTrack is a free offline browser utility. HTTrack allows you to download a website
from the Internet to a location directory, build all directories, capture HTML, images,
and other files from the server and store on your computer. You can browse the cloned
website link-to-link, as well as test it for vulnerabilities. HHTrack is an extremely
simple tool to work with basic websites. It will not replicate dynamic content, nor
will it replicate website middleware, such as databases. Therefore, it may not be
appropriate in all Penetration Testing environments.
[ 257 ]
www.it-ebooks.info
Defensive Countermeasures
To test all aspects of a website, you will need to use other software to
clone a target. That software must include capturing middleware and
dynamic content as well as possibly requiring administrator access
rights to the target.
At the time of writing, HTTack no longer comes preinstalled with Kali. To install
HTTack, open up a Terminal window and type apt-get install httrack. Once
the install is complete, you can launch HTTrack, open a Terminal and type httrack.
You will be asked to give a project name, path to install the website (default is
root/websites/), and URLs to copy. HTTrack gives a few options to copy your
target(s), as shown in the following screenshot. Some additional optional questions
are defining wildcards and recurse level. We selected option 2. Once you answer the
questions, select Y to clone your target(s).
HTTrack will start cloning your target and all associated links. It may take a while to
complete, depending on the size of your target. The next screenshot shows HTTrack
cloning www.thesecurityblogger.com.
[ 258 ]
www.it-ebooks.info
Chapter 7
Navigate to the folder you specified to save the cloned targets to start your testing.
Other cloning tools
Here are a few more website cloning tools available in Kali Linux. Once again,
these tools will not replicate dynamic content, nor will they replicate website
middleware, such as databases. Therefore, they may not be appropriate in all
Penetration Testing environments.
• WebCopier: It is a tool that clones a website for offline evaluation, such
as Penetration Testing.
• w3mir: It is an all purpose HTTP copying and mirroring tool. The main
focus of w3mir is to create and maintain a browsable copy of one, or
several, remote WWW sites.
Man-in-the-middle defense
Man-in-the-middle attacks are difficult to protect against. The attack happens outside
of the victim's controlled environment, and when executed properly, doesn't leave an
obvious signature that alert the victims involved. MITM is typically the first step of
a more sinister attack such as SSL strip. One common way to protect against MITM
is ensuring websites use SSL/TLS 3.0. In other words, make sure the websites are
accessed using HTTPS or HTTP secure connections. Verifying HTTPS is not as easy
as looking for a little green address bar with a lock symbol, because attackers can
serve victims certificates to make it appear like the session is secure.
[ 259 ]
www.it-ebooks.info
Defensive Countermeasures
To properly test a HTTP session, examine the certificate and look at the certificate
authority. This additional effort discourages many users from verifying a secured
session, which makes this attack method very effective.
The previous screenshot shows a SSL certificate for Gmail was issued by the Google
Internet Authority. This sounds great, but who is the Google Internet Authority?
Can I trust them? Is it really Google? In this example, I have another certificate
authority shown above the Google Internet Authority named Equifax Secure
Certificate Authority. Equifax has a number of checks and balances before issuing
a certificate to ensure a business is valid. Verifying that Equifax generated this
certificate makes me feel confident I can trust this certificate.
Ultimately, HTTPS relies on the concept of trust. To be more explicit, the question
comes down to trusting the certificate authority that issued the certificate is valid
and legitimate. For lab environments, it is common to find self-signed certificates
that trigger alarms from most popular Internet browsers. That annoying popup that
users complain about when accessing websites serves as a means to warn that the
certificate authority is likely not trustworthy, and there is a risk of a MITM attack.
[ 260 ]
www.it-ebooks.info
Chapter 7
Encrypted Virtual Private Network (VPN) is another way of protecting against
man-in-the-middle attacks. By encrypting all data sent to and from your device
while masking your public Internet Protocol (IP) address, encrypted VPNs ensure
that you're on a network that cannot be monitored or recorded by anyone except
the VPN provider.
VPNs can use strong authentication methods, such as two-factor authentication,
which includes a username and password, along with some other forms of
authentication, such as OTP (one-time passwords), tokens, or certificates. This
makes it difficult for an attacker to steal the authentication required to establish
a VPN used by another user.
VPNs have the ability to use encryption methods, such as PPTP, L2TP, SSL, and IPSEC.
SSL and IPSEC VPNs provide higher-level security for guarding data compared to
other protocols because of their use of strong cryptographic encryption protocols.
VPNs are provided by both private and public organizations. It is
possible that the VPN provider may be able to examine your traffic,
because they are the trusted service providers. Therefore, the question
of trust is still a very important concept when using a VPN. You must
ask if you trust your VPN provider to protect your data and privacy.
Your data security is in the service provider's hands.
Other techniques that can be used to defend against MITM attacks are Media Access
Control Security (MACsec) and 802.1x. These approaches use advanced networking
to provide source authentication, data integrity, and encryption as traffic travels
across the network. Both approaches require equipment compatibility and must be
enabled properly in order to be effective.
SSL strip defense
SSL strip (covered in Chapter 3, Server Side Attacks, allows attackers to strip or tear away
the encrypted portion of a website and view the victim's Internet session, including
confidential information. It is common to link SSL strip with another attack, such
as a man-in-the-middle, meaning hackers will capture all traffic and strip away SSL
encryption so everything is visible to the hacker's traffic sniffing tools. We covered
this concept in Chapter 5, Attacking Authentication, of this textbook.
[ 261 ]
www.it-ebooks.info
Defensive Countermeasures
To protect against SSL strip attacks, it is important to understand how SSL strip
exploits a victim. The attack takes advantage of websites redirecting users from a non-
encrypted version of the site, to an encrypted version of the site. When you navigate
to http://www.facebook.com or http://www.gmail.com, you will notice you are
redirected to https://www.facebook.com and https://www.gmail.com. SSL strip
breaks the redirection and forces the victim to use the non-secure version of the
website. Furthermore, even if the site does not have a non-secure version, but still has
a redirect, SSL strip will intercept the HTTP request and forward the user to HTTPS
site. When a victim does this, the attacker can view the victim's entire session.
One method to protect against SSL strip attacks is to ensure that websites do not have
a non-secure version of itself and that they do not implement redirect features. This
would prevent a SSLstrip attack, because there is no redirection possibility. When a
victim is attacked, they will simply not be able to get to a website. We understand from
a real world implementation standpoint that this is very difficult to enforce. People are
used to typing a non-secure HTTP request and being automatically redirected when
security is needed. Also, many businesses would not want users thinking their website
is down due to not accessing a secure version of the website. So the best protection
from SSL strip is educating users on how cyber attacks occur so they can identify them.
In addition, the defense methods we outlined earlier against man-in-the middle will
also defend against SSL strip attacks. The reason for this is SSL strip relies on a man-
in-the-middle attack to occur.
Denial of Service defense
Most Distributed or standard Denial of Service (DDoS/DoS) tools are open
source utilities written in C# or Java. We demonstrated in Chapter 6, Web Attacks,
how a single person using a DoS tool can have a devastating impact to a business
by limiting access to online sources or taking down a website. DDoS/DoS tools are
advertised as web application stress-testing tools. Although they could potentially
be used for that, in many cases they are used for nefarious purposes.
DDoS/DoS attacks in most cases require abusing network infrastructure hardware.
One of the common methods to defend against DDoS/DoS is configuring network
devices that can handle large influx of packets, the ability to detect anomalous
behavior, and traffic patterns. Malicious traffic identified should be automatically
filtered to avoid interruption of service. Tools from vendors, such as load-balancers
and web application firewalls, do a great job of detecting and defending against
volumetric and application-type attacks. Security tools with DoS detection capabilities
are able to recognize network, session, and application layer traffic, which is imported
for mitigating DoS risks that can exist at all layers of the protocol stack.
[ 262 ]
www.it-ebooks.info
Chapter 7
To defend against sustained and prolonged attacks, many organizations turn to a
DDoS application service provider. A DDoS application service provider works with
your ISP and attempts to stop DDoS from reaching your network by redirecting traffic
away from your organization. They do this by using routing protocols, such as BGP
and advanced DNS techniques.
Most DDoS/DoS attacks use spoofed or invalid IP addresses when attacking
an organization. Network administrators should deploy Unicast Reverse Path
Forwarding (Unicast RPF) on their Internet-facing border routers as a protection
mechanism against spoofing of IP source addresses when used to launch DDoS
attacks. Unicast RPF is considered best practices for Internet-edge-facing routers,
and a good start to defend against DDoS/DoS. Unicast RPF is configured at the
interface level on Cisco routers. Other enterprise manufactures may have similar
features on their routers as well. When Unicast RPF is configured, non-verifiable
or invalid IP addresses will be dropped.
A more recent technique used to identify DDoS/DoS traffic is leveraging Netflow in
conjunction with transit access lists to stop the traffic from entering the network as
well as identifying internal attacks. Traffic behavior is analyzed and any indication
that the network seeing malicious traffic will trigger alarms such as Smurf or
Teardrop packets. Leading DDoS/DoS solutions offer the ability to monitor for
both internal and external DDoS/DoS threats.
Cookie defense
As we discussed in earlier chapters, cookie hijacking is a technique where an attacker
steals session cookies. Cookie hijacking can be defeated if your website is running
SSL/TLS 3.0. Many attackers will bypass SSL/TLS by using a combination of man-
in-the-middle or SSL strip attacks; however, by ensuring your web application only
has secure pages, meaning not providing a HTTP to HTTPS redirection, will mitigate
those forms of attack.
Cookie hijacking can work over SSL/TLS connections if attackers use
cross-site scripting to send cookies to their servers. Developers can mitigate
this risk by setting the Secure and HttpOnly flags on the cookies.
A common mistake regarding web application security is assuming developers
secure the entire session rather than just the authentication portal to a web
application. When the entire session is not secured, a user can possibly be attacked.
Developers must ensure their entire application supports secure and encrypted web
sessions through SSL/TLS 3.0 to avoid being vulnerable to attack.
[ 263 ]
www.it-ebooks.info
Defensive Countermeasures
Additional defense against cookie hijacking is available with popular Application
Delivery Controller (ADC) appliances, such as load balancers and content filters.
Popular vendors to consider are Cisco, Bluecoat, Riverbed, Websense, and many
others. Many of these vendors change cookie flags to Secure and HttpOnly. They
also have built in propriety techniques to mitigate some cross-site scripting attacks.
Clickjacking defense
Clickjacking was covered in Chapter 5, Attacking Authentication, and is the
technique where an attacker tricks a user into clicking on something other than
what they believe they are clicking on. One of the best ways to protect against
clickjacking is by running the noscript extension for Firefox or Chrome browsers.
This will prevent unauthorized code from running in your web browser. Noscript
can detect unauthorized scripts, alert the user of the script and prevent the script
from running. Users have the ability to turn off scripting controls globally per
session or per website.
The authors of this book are big fans of noscript; however, you should encourage
web developers to set up X-Frame-Options header in HTTP responses to mitigate
this risk in web applications. Furthermore, some application delivery controller
appliances (ADCs), give administrators the option of writing custom scripts that
can also help mitigate this risk.
Some websites may have legitimate reasons to run a script. This could
be for shopping carts or other e-commerce sites.
[ 264 ]
www.it-ebooks.info
Chapter 7
Digital forensics
Kali Linux 1.0 includes a number of tools for dealing with forensic requirements.
Forensics is the practice of investigating evidence and establishing facts of interest
that links to an incident. This section will give you an introduction to digital
forensics as we believe it is necessary to have a reaction plan when one of your
assets, such as a server or web application, is compromised. It is recommended to
research other sources for a more thorough training as this topic extends beyond
the tools available in Kali Linux. Digital forensics is a growing area of interest in
information security with very few people that know it well.
It is important to remember three rules anytime you work on digital forensics.
Failure to comply with these rules will make opinions of yours seem amateurish,
and probably render your forensics investigation inclusive.
The first rule is never work on original data. Always use a copy. Ensure you do not
modify data when you create a copy. The moment you touch or modify original data,
your case becomes worthless. Tampered evidence can never be used in any legal
proceeding regardless of what is found. The reason is once an original is modified,
there is a possibility of identifying false evidence that can misrepresent the real
incident. An example is making a change that adjusts the timestamp in the system
logs. There would be no way to distinguish this change from an amateur analyst's
mistake or hacker trying to cover his tracks.
Most forensic scientists will use specialized devices to copy data bit for bit. There
is also very reputable software that will do the same thing. It is important that your
process be very well documented. Most digital copies in legal proceedings that
have been thrown out were removed due to a hash of a storage medium, such as
a hard drive, not matching copied data. The hash of a hard drive will not match a
contaminated copy, even if only a single bit is modified. A hash match means it is
extremely likely the original data including filesystem access logs, deleted data disk
information, and metadata is an exact copy of the original data source.
The second rule for digital forensics is anything that can store data should be
examined. In famous cases involving digital media, critical evidence has been
found on a camera, DVR recorders, video game consoles, phones, iPods, and other
random digital devices. If the device has any capability of storing user data, then
it is possible that device could be used in a forensics investigation. Do not dismiss
a device just because it is unlikely. A car navigation system that stores maps and
music on SD cards could be used by culprits to hide data, as well provide evidence
for Internet usage based on download music tags.
[ 265 ]
www.it-ebooks.info
Defensive Countermeasures
The last critical rule for digital forensics is ensuring you document all your findings.
All evidence and steps used to reach a conclusion must be easy to understand for it
to be credible. More importantly, your findings must be recreatable. Independent
investigators must arrive at the same conclusion as you using your documentation
and techniques. It is also important that your documentation establishes a timeline
of events on when specifics occurred and how they occurred. All timeline
conclusions must be documented.
A forensic investigation is all about the perception of being a security expert
validating evidence linked to an incident. It is easy to get caught up looking for
bad guys and drawing conclusions on what may have happened based on opinion.
This is one of the fastest ways to discredit your work. As a forensics specialist, you
must only state the facts. Did the person Alice steal Bob's files, or did the account
that was logged on as the username Alice initiate a copy from the user account
Bob's home directory to a USB drive with serial number XXX at the timestamp
XXX on date XXX? See the difference? The real bad guy could have stolen Alice's
login credentials (using methods covered in this book) and steal Bob's data while
posing as Alice. The moment you jump to a conclusion is the moment your case
becomes inconclusive based on personal interference. Remember, as a forensics
specialist, you could be asked under oath to give testimony on exactly what
happened. When anything outside of facts enters the record, your credibility
will be questioned.
Kali Forensics Boot
Kali Linux has the option of using a Forensics Boot. Kali forensics mode is selected
when you boot a system with a Kali boot-up disk, such as the Live CD. If you want
to use Kali as a forensics toolkit, we highly recommend keeping a Kali Live CD as
part of your toolkit. The Kali Live CD can be obtained as an ISO image downloaded
from the Kali Linux website (refer to Chapter 1, on installing Kali). When Kali boots,
you will see Forensics mode as one of the selectable options.
[ 266 ]