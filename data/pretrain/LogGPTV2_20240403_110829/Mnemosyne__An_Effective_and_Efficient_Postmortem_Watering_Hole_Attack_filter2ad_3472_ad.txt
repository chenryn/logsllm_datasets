Keylogging
Tabnabbing
Driveby
Average
Table 6: A detailed performance comparison between manual++ and Mnemosyne in terms of number of domains and scripts a
forensic analyst needs to investigate. Scripts‚Ä† are all scripts related to the corresponding domain. Scripts‚Ä° are the set of scripts
that have behaviors attributed to them (e.g., network requests, DOM insertions, etc.). Mnemosyne can reduce the analysis
scope significantly, for example, (-99.56%) means reduction based on the raw data.
# of Domains
4 (-93.22%)
2 (-88.89%)
1 (-95.65%)
1 (-95.00%)
2 (-81.82%)
2 (-96.88%)
2 (-99.65%)
2 (-98.17%)
# of Scripts
6,243
523
1,102
534
1,761
27,477
3,230
5,838
# of Domains
45 (-23.73%)
12 (-33.33%)
20 (-13.04%)
15 (-25.00%)
11 (0.00%)
48 (-25.00%)
567 (-0.70%)
103 (-6.27%)
manual++
# of Scripts‚Ä†
5,814 (-6.87%)
507 (-3.06%)
1,086 (-1.45%)
521 (-2.43%)
1,761 (0.00%)
27,259 (-0.79%)
3,055 (-5.42%)
5,715 (-2.21%)
# of Scripts‚Ä°
477 (-92.36%)
116 (-77.82%)
318 (-71.14%)
112 (-79.03%)
37 (97.90%)
26,262 (-4.42%)
350 (-89.16%)
4,554 (-22.01%)
# of Versions
Mnemosyne
# of Scripts
6 (-99.90%)
5 (-99.04%)
63 (-94.28%)
85 (-84.08%)
6 (-99.66%)
13 (-99.95%)
2 (-99.94%)
26 (-99.56%)
5
3
2
2
3
3
3
3
Developing a Baseline. Following the practical strategies ob-
served during this study, we developed a baseline system to com-
pare against Mnemosyne, which we call manual++. manual++
attempts to generalize the approaches used by the different partic-
ipants to perform a forensic investigation based on browser logs.
Specifically, manual++ first collects all domains that communicated
with the compromised website. Next, it filters out any of these do-
mains that are listed on the Alexa 10k, since they are highly likely
to be benign. Next, it further reduces the number of domains by
filtering out domains that only served static content (e.g., images,
fonts, CSS files, etc.).
Measuring Analysis Reduction. To measure the analysis reduc-
tion Mnemosyne provides, we compare it to manual++. We focus
on the number of domains and scripts that would require manual
inspection when using Mnemosyne, compared to manual++. An
extensive reporting of the results is provided in Table 6. The number
of domains and unique script URLs found in each attack scenario is
reported in the raw column. We see that Mnemosyne was able to
filter out, on average, 98.17% of the domains while manual++ was
only capable of filtering out 6.27% of the domains. It‚Äôs even less for
the case of Driveby because the website employs ads that generate
random domain names. Next, we inspected the number of scripts fil-
tered out of the analysis space by Mnemosyne and manual++. Our
experiments show that Mnemosyne was able to filter out 99.56%
of the scripts from the analysis space, which shows a significant
reduction in the number of scripts required for manual analysis
by the investigator. To measure the number of scripts filtered by
manual++, we provide two results in columns Scripts‚Ä† and Scripts‚Ä°.
Scripts‚Ä† provides the number of scripts remaining after applying
manual++‚Äôs filtering. We see that on average, only 2.21% of scripts
were filtered by manual++. Also, we provide a second set of re-
sults in column Scripts‚Ä°. The results presented in column Scripts‚Ä°
were calculated by adding an additional filtering stage, which was
more aggressive and filtered out scripts that did not have behaviors
causally attributed to them (e.g., they made no network requests or
DOM insertions). The results show that applying this additional fil-
tering stage can reduce the number of scripts by 22.01% on average,
and in some cases, such as the Keylogging scenario, this additional
filtering stage performs well. However, we also see that in the Tabn-
abbing attack scenario, it performs extremely poorly, and was only
able to filter out 4.42% of the scripts in the analysis space, while
Mnemosyne was able to filter out 99.95% of scripts. We also found
that the naive approach used by manual++ to filter out domains
based on the Alexa 10K led to a false-negative in the Malicious
Software Update attack scenario, because the adversaries served the
malware from a Git repository on hxxps://www.github.com. On the
other hand, Mnemosyne correctly identifies this attack component.
These results show that Mnemosyne can significantly reduce the
scope of the analysis space that requires manual analysis for the
forensic investigation.
4.3.2 Attack Scenario Domain Versions. Next, we investigated the
number of domain versions generated for each attack scenario. For
each scenario, we set ùúî = 1 (i.e., we used one day for the profiling
phase). We found that a low number of versions were generated for
each attack scenario, as reported in the last column of Table 6. One
outlier was the Malicious OAuth Access Scenario, with 4 versions
after removing the core domain-version. We further investigated
and found that there were 3 new benign versions generated. These
versions were generated shortly after the profiling phase, which
means this phase was too short for this website. This is reasonable
since this scenario compromised hxxp://www.cfr.org, which was
a larger website and had 4,957 distinct URLs visited during the
scenario. Also, the version prioritization prioritized all 3 of these
benign versions lower than the malicious version.
4.3.3 Version Prioritization. In all attack scenarios, our system
accurately prioritized the compromised version over the benign
versions, with the one exception of the Keylogging Attack Scenario.
This shows Mnemosyne‚Äôs version prioritization approach was effec-
tive, and we investigated each attack scenario to determine exactly
why the prioritization was effective. The Malicious OAuth Access at-
tack lured the user into navigating to jupyter.elfinwood.top3, which
was captured as a cross-origin navigation in the causality graph.
As this cross-origin navigation was attributed to a script served by
jupyter.elfinwood.top, it flagged TTP T1204.001, which incremented
its suspiciousness score. Finally, for the Clickjacking and Tabn-
abbing scenarios, Mnemosyne identified TTPs T1189 & T1204.001
3jupyter.elfinwood.top was the malicious domain used throughout the attack scenarios.
Session 3B: Malware CCS '20, November 9‚Äì13, 2020, Virtual Event, USA795Attack Scenarios
User-level Versions
Unaff. Tar. Vic. Unaff.-
‚úì
‚úì
Tar.
‚úì
‚úì
Tar.-
Vic.
Unaff.-
Vic.
Malicious OAuth Access
Clickjacking
Malicious Software Update
Credential Harvesting
Keylogging
Tabnabbing
Driveby
Table 7: A report of the user-level version types generated
during each attack scenarios.
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
being causally dependent on the attack scenarios‚Äô respective mali-
cious domain version. For the Malicious Software Update attack, it
successfully detected TTP T1204.002. Finally, for the Driveby sce-
nario, we detected TTP T1189 when the iframe was injected into
the DOM. Mnemosyne was not effective in prioritizing the Keylog-
ging attack scenario since the attack did not insert an iframe nor
trick users to do anything, but rather sent network requests in the
background. We believe this is acceptable, as version prioritization
does not aim to determine the compromised version directly but
aims to prioritizes domain-versions relying on identifying typical
suspicious events that are related to social-engineering attacks.
4.3.4 User-Level Versions. To measure the effectiveness of the User
Level Analysis, we determine how effective it is at developing ‚Äúuni-
form‚Äù clusters. We define a cluster as uniform if the pages mapped
to it only represent unaffected users, targeted users, or victims. We
define six different types of user-level versions; unaffected users
(Unaff), targeted (Tar), and victim (Vic) versions only include a
single user-type. Next, the groups unaffected-targeted (Unaff-Tar),
targeted-victim (Tar-Vic), and unaffected-victim (Unaff-Vic) are
mixed user-level versions. Mixed groups are generated by the user-
level analysis when there is not enough context in the underlying
audit logs to accurately distinguish between the two user types.
The results are shown in Table 7, where each column represents
a different category of user-level versions. We see that in 6/ 7 at-
tack scenarios, Mnemosyne generates ideal ‚Äúuniform‚Äù clusters. For
the Malicious Software Update attack scenario, Mnemosyne cre-
ated two user-level version types, a victim user-level version and
a mixed unaffected-targeted version. To understand this, we an-
alyzed the attack scenario in more depth and found that the at-
tack relies on inserting an overlay tag into the page to lure the
user into installing the malicious file. Since Mnemosyne relies on
an instrumentation-free approach for auditing the browser, it has
less visibility in terms of DOM modifications compared to prior
work [43] and unfortunately could not attribute the insertion of the
overlay to a specific script. However, since file download events
can be detected, Mnemosyne is able to narrow down the analysis
space to identify all the users that were victims of the attack.
4.4 Benign Version Analysis
Next, we completed a study to evaluate the number of versions
reconstructed over an extended time period. To achieve this, we
evaluated Mnemosyne‚Äôs domain-version reconstruction using the
benign datasets with ùúî = 1. The average number of versions recon-
structed per category is shown as the solid triangle in Figure 4. The
light yellow box shows the five-point-summary for the Alexa and
Categories dataset. We found that news websites have the highest
average with 4.33 versions generated during the crawling period,
while gaming sites only had 1.52 versions. The average number
of versions generated in one month is 2.15 for the Alexa Top 1k.
An extreme outlier, hxxp://auctiongr.com, in the shopping category
generated 22 versions. This website was a shopping site back in
February 2020 and only generated one version till March. How-
ever, the domain was registered to a porn and malvertising site in
June 2020, and started adding malicious domains frequently. There
are 7 websites considered outliers, for the Alexa category, which
have more than 6 versions generated in one month. Among the
7 websites are 6 News websites that are listed in Table 11 in the
Appendix. We believe this is a reasonable outcome, since websites
that fall into the News category need to frequently add new content
to stay up-to-date on current events. The other one is a computer-
themed forum-like website which updates its content frequently
as well. This experimental evaluation shows that Mnemosyne‚Äôs
domain-versioning system can be effective for long periods of time.
False-Positive Analysis. Benign updates made after the pro-
4.4.1
filing phase will generate a new domain version. We completed an
experimental evaluation to assess how often benign updates would
be flagged as suspicious. To complete this experiment, we used the
benign versions discussed in ¬ß4.4. We inspected 3,663 benign ver-
sions generated across 1,830 websites. We found that 14.12% of the
benign versions were flagged as suspicious. We further investigated
the flagged benign versions and found that 2.38% of the versions
were flagged due to cross-origin navigation to an unknown domain,
while 11.74% of the flagged domains were related to anomalous
iframes being injected into the DOM. However, in almost all cases
the iframes injected were ad-related. We found that by checking
the iframe‚Äôs src property, against a white list of 15 domains (listed
in Table 14), it allowed us to filter out 9.15% of the iframes related to
ads. Finally, after applying the white list filtering approach we find
that only 4.97% of the benign versions were flagged as suspicious.
Since the analyst will only need to investigate benign versions re-
lated to the compromised version, this shows that benign updates
will not significantly increase the time of the investigation.
4.5 Runtime Performance
To evaluate the runtime performance of the auditing daemon, we
measured the page load time for the top 1,000 most popular websites
according to Alexa.com, using out-of-the-box Chromium version
80.0.3987.163. The page load metric is important because previous
studies have shown that a slow page load time can lead to frustrated
users and drive websites‚Äô revenue lower [19]. For each site, we con-
ducted 10 trials, both with and without our auditor attached. Prior
to measuring the page load time, we loaded the homepage of each
site into the browser so that it would heat up the browser‚Äôs cache.
The purpose of this was to minimize the influence that potential
network latency variations would have on the experiment. It is
important to notice that Mnemosyne logs web requests regardless
if the object was fetched from the cache or the actual server, so
the time spent within Mnemosyne‚Äôs logging functions will be the
Session 3B: Malware CCS '20, November 9‚Äì13, 2020, Virtual Event, USA796Figure 4: Five-point-summary of domain versions generated
for each website for the Categories and Alexa datasets, with
whiskers being 1.5x IQR.
9.80%, which is similar to the overhead introduced by previous
work [43]. Additionally, (b) shows that on average Mnemosyne
increases the load time by only 0.04s. However, we found two out-
liers, hxxps://www.tripadvisor.com and hxxps://www.atlassian.com,
which had page load overheads that were slightly over 25%. We
spent a significant amount of time assessing why these two cases
were outliers. This included toggling the DevTools namespace to
identify exactly which namespace(s) were causing the performance
overhead. We found that the Network and Debugging DevTools
namespaces appear to be contributing the most to the overhead
induced. Unfortunately, a more fine-grained approach to identify
exactly which other DevTools hooks were contributing to the over-
head and by how much would require instrumenting Chromium
itself. Since this outlier overhead was observed only on 2 out of
1,000 websites, we leave this detailed analysis to future work.
It is important to notice that Mnemosyne leverages only a small
set of DevTools hooks within a small set of namespaces, namely
Network, Page, Debugger, and Target. Therefore, runtime per-
formance could be further optimized by developing a customized
Forensics DevTools namespace, which would only activate the
hooks that are necessary for the logging, while avoiding the over-
head introduced by calls to other unused hooks that occur when
other DevTools namespaces are present. In summary, our perfor-
mance evaluation shows that Mnemosyne has a reasonable over-
head, especially for a prototype, and could be deployed in real-world
scenarios without significantly affecting the user‚Äôs browser experi-
ence.
Next, we measured the performance of Mnemosyne‚Äôs automated
log analysis process (see ¬ß3.5 and ¬ß3.6) on a standard laptop with In-
tel I7-8700B CPU running at 3.2 GHz and 32GB of physical memory.
On average, the log analysis process takes less than 5 minutes for a
graph of 6.2M nodes and 11.0M edges. Additionally, a breakdown
of the runtime performance for every attack scenario is provided in
Table 12. This shows the runtime performance for analyzing each
attack scenario is efficient.