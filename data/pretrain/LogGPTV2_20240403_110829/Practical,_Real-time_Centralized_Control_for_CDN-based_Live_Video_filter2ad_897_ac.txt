degrade the quality of an existing stream, as we know
there is enough capacity on the path to support the
incoming request. Thus, the distributed algorithm will
not interfere with central control’s already implemented
decisions.
Issues in the wide area
Note, through the use of local/global discovery, the
central controller will eventually become aware of new
requests and failures. By checking evidence in the RIB,
clusters will know when central control has “caught up”
to the current network state at which point they make
the past local decisions obsolete.
4.3
Handling state transitions: When requests are sent
up the distribution tree for a given channel, they are
tagged with the version number from the RIB. VDN
keeps previous versions of the FIB (“shadow FIBs”) to
allow clusters to forward requests with old version num-
bers (e.g., during global state transitions), similar to
previous work [20, 28, 34]. When an unknown version is
encountered, VDN resorts to using distributed control.
Partitions: Versioning helps with network partitions,
where some clusters no longer receive updates from the
central controller. Clusters that are partitioned (“in-
visible” clusters from the controller’s perspective) can
still interact with “visible” clusters by using these old
version numbers. Eventually the partitioned clusters will
switch to exclusively distributed control after they detect
that they’re partitioned (e.g., after a controller timeout).
As distributed control and central control interact in
beneﬁcial ways, partitions are also not a problem.
Loops: Our system cannot have loops as requests
only travel “upwards” towards sources, and responses
“downwards” towards ASes in our hierarchy.
5 Centralized optimization
This section describes our optimization algorithm that
maximizes the overall service VDN delivers to each video
channel while minimizing cost. Our algorithm takes in
video requests and topology information and outputs
the best way to distribute those videos. While the for-
mulation is relatively straightforward, the easiest way
to achieve scalability is to eschew ﬁnding the true opti-
mal solution in favor of ﬁnding a good approximately
optimal solution that can be computed relatively fast.
max (cid:119)(cid:115) ∗(cid:80)
(cid:119)(cid:99) ∗(cid:80)
(cid:108)∈ (cid:76) (cid:65) (cid:83) ,(cid:111)∈(cid:79) Priority(cid:111) ∗ Request(cid:108) ,(cid:111) ∗ Serves(cid:108) ,(cid:111)
(cid:108)∈ (cid:76) ,(cid:111)∈(cid:79) Cost((cid:108)) ∗ Bitrate((cid:111)) ∗ Serves(cid:108) ,(cid:111)
:(cid:80)
∀(cid:108) ∈ (cid:76), (cid:111) ∈ (cid:79) :(cid:80)
−
subject to:
∀(cid:108) ∈ (cid:76), (cid:111) ∈ (cid:79) : Serves(cid:108) ,(cid:111) ∈ {0, 1}
∀(cid:108) ∈ (cid:76)
(cid:111) Bitrate((cid:111)) ∗ Serves(cid:108) ,(cid:111) ≤ Capacity((cid:108))
(cid:108)(cid:48)∈InLinks((cid:108)) Serves(cid:108)(cid:48) ,(cid:111) ≥ Serves(cid:108) ,(cid:111)
Figure 9: Integer program at the controller.
The optimization is called iteratively (around once a
minute) allowing parameters (e.g., measured capacities,
link costs, new requests) to be modiﬁed each iteration.
Input: Videos: We denote a set of live video channels
as (cid:86) = {(cid:118)1, . . . , (cid:118)(cid:107)}. Each video channel (cid:118) has its own
set of bitrate, (cid:66)(cid:118). Our system treats each item in (cid:86) × (cid:66)
as a distinct video object. We denote the set of video
objects as (cid:79) = {(cid:111)1, . . . , (cid:111)(cid:109)}. Bitrate((cid:111)) is the bitrate of
the video object (cid:111) in Kbps. Every video object (cid:111) has a
priority weight associated with it, Priority(cid:111) > 0, set by
operators indicating how important it is to serve (cid:111).
Topology: Our network topology (see Figure 10a) is a
directed graph made of server clusters (sources, reﬂectors,
and edges as explained in §2) and ASes, connected by
links {(cid:108)1, . . . , (cid:108)(cid:110)} ⊂ (cid:76) in a four-tier topology. We assume
each video object is available at each source cluster
(not unreasonable [35], but not fundamental). We add
additional dummy links out of every AS node in the
graph 1. We refer to this set of dummy links as (cid:76) (cid:65)(cid:83) ⊂ (cid:76).
(cid:48)), InLinks((cid:108)) is the set of incoming
For some link (cid:108) = ((cid:115), (cid:115)
links to (cid:115).
Link capacities: Each link (cid:108) ∈ (cid:76) has a capacity deﬁned
by Capacity((cid:108)), in Kbps. This capacity is the measured
amount of capacity of the overlay link available to video
delivery (i.e., the overall path capacity minus background
traﬃc), which is updated by information from local
discovery.
Link costs: Additionally, each link (cid:108) ∈ (cid:76) has a cost
deﬁned by Cost((cid:108)) indicating the relative price for de-
livering bits over that link. This cost can vary over
time (i.e., updated between iterations of the ILP) as
updated by management (e.g., after business negotia-
tions a link is perhaps free: (cid:67)(cid:111)(cid:115)(cid:116)((cid:108)) = 0; perhaps cost
varies based on usage, such as “95-percent-rule” billing;
or even more complicated policies such as a cap on total
externally-bound traﬃc, etc.).
Requests are associated with a link in (cid:76) (cid:65)(cid:83) (i.e., a re-
questing AS) and a video object. For some link (cid:108) ∈ (cid:76) (cid:65)(cid:83)
associated with an AS (cid:97), if a request for video (cid:111) origi-
= 0.
nates from (cid:97) then Request(cid:108) ,(cid:111)
Weights: The system operator provides a global weight
for cost (cid:119)(cid:99) ≥ 0 and a global weight for service (cid:119)(cid:115) ≥ 0
to strike a balance between service and cost.
= 1, else Request(cid:108) ,(cid:111)
Formulation: Figure 9 presents our problem formula-
tion. The optimization takes the following as input (and
treats them as constants): (cid:119)(cid:115), (cid:119)(cid:99) , Priority, Request,
1This is a common technique in optimization to make
the formulation easier.
317Figure 10: Example input and output of the cen-
tralized optimization.
Cost, Bitrate, Capacity, and InLinks. It outputs vari-
ables Serves(cid:108)∈ (cid:76) ,(cid:111)∈(cid:79) ∈ {0, 1}, which indicates whether
video object (cid:111) should be distributed over link (cid:108).
(cid:80)
ser-
vice, while
cost
(i.e., max : service − cost). We model service as
(cid:108)∈ (cid:76) (cid:65) (cid:83) ,(cid:111)∈(cid:79) Priority(cid:111) · Request(cid:108) ,(cid:111) · Serves(cid:108) ,(cid:111). Thus we
Our objective function directly maximizes
simultaneously minimizing
its
only serve videos objects to ASes that requested them,
with the biggest wins coming from higher priority video
objects. Service is only improved if a requested video
reaches its destined AS. As for priority, we explore var-
ious schemes (exploring the quality/quantity tradeoﬀ)
(cid:108)∈ (cid:76) ,(cid:111)∈(cid:79) Cost((cid:108)) · Bitrate((cid:111)) ·
Serves(cid:108) ,(cid:111), the amount of data being transferred around
(and out of) the CDN times the link costs.
in §7. We model cost as(cid:80)
Our constraints encode the following:
1. A link either does or doesn’t send a video.
2. Obey the link capacity constraint.
3. Only send videos you’ve received.
Output: Serves(cid:108) ,(cid:111), determines a set of distribution
trees for every requested video. This can be easily trans-
lated into forwarding tables for incoming requests within
the CDN internal network, and DNS records for mapping
clients to edge clusters.
Example: Figure 10 gives an example input with two
channels (cid:86)1 and (cid:86)2, with bitrate streams of [200, 800] and
[300, 900] kbps respectively. We see that the operator
has decided that video object ((cid:86)2, 900) has a very high
priority (100)—this may be a stream viewers pay to
watch (e.g., a pay-per-view sports event). Figure 10a
shows the topology, link capacities, and costs. Link
(cid:89) (cid:65) has a relatively high cost of 10. Figure 10b shows
the optimization result in which two requests for (cid:86)1 are
satisﬁed. Note, the optimization avoids using the high
cost (cid:89) (cid:65) link, even though it would have cut down the
total number of links used, reducing redundant data
transmissions. Once a third request is added (for the
high priority stream (cid:86)2), we observe that (cid:89) (cid:65) is used, as
the video’s priority outweighs the high link cost.
Figure 11: The MIP gap of the centralized op-
timization shows rapid improvement in a short
time-frame, even for large numbers of videos.
Approximating optimality: An integer program can
take a very long time to ﬁnd an optimal solution. We
employ two techniques (initial solutions and early termi-
nation) for fast approximation.
Often a good initial solution can dramatically reduce
the integer program runtime. Although it’s tempting to
reuse the previous central decision as the initial solution
for the next iteration, our formulation changes enough
(e.g., new link capacities, video requests, etc.) per itera-
tion that our previous decision may no longer be valid.
Thus, we instead we calculate an initial solution greedily.
Another important parameter of integer programs is
the termination criteria. Often integer programs will
ﬁnd a feasible solution very quickly that is only slightly
worse than optimal, then spend many minutes working
towards the optimal solution. This time/quality tradeoﬀ
guides our decision to use a timeout to terminate our
optimization. In Figure 11 we plot the MIP gap2 as a
function of computation time for diﬀering numbers of
videos. We see that for all series up to our target scale of
10,000 videos (see §2.2), a 60 second timeout can provide
an almost optimal solution (e.g., ∼1%)). Although 60
seconds may seem like a long timescale for optimization
with respect to view duration, live video viewers watch
on average 30 minutes per session [1], making this a
reasonable target.
6 Prototype implementation
Control plane: We build a prototype central controller
that uses Gurobi [21] to solve the integer program. For
trace-driven experiments, we run the controller on an
r3.8xlarge EC2 instance [8]. For end-to-end experiments,
since our testbed is smaller, we run the controller on a
machine with a 2.5GHz quad-core Intel Core i5 processor
with 4GB of RAM. For these experiments, our controller
communicates with data plane nodes over the public
Internet, with ∼10ms latency. We believe this to be
representative of a real-world deployment.
Data plane: We also build a prototype data plane us-
ing Apache [16] running on t2.small EC2 instances. Our
data plane uses standard Apache modules: mod proxy
conﬁgures nodes as reverse HTTP proxies and mod cache
2The distance between the current upper and lower
bounds expressed as a percentage of the current upper
bound
Bitrates(((Kbps) Priori0es Requests((at(Start)(V1([200,(800]([1,(1]((A,(800),((B,(800)(V2([300,(900]([1,(100](None((a) Link capacities (kbps) and costs((b) Optimization Output(R(X(Y(A(B(1000(2000(1000(800(800(S(2000(R(X(Y(A(B(V1,(800(V2,(900(S(R(X(Y(A(B(V1,(800(S(New request: V2, (A, 900)!V1,(800(Service(Weight 1000(Cost(Weight(0.1(V1,(800(1(1(1(1(1(10(100101102103Time(s)10−210−1100101102103MIPGap(%)2K4K6K8K10K25K318gives us multicast-like semantics. The use of Apache
is representative of a real-world deployment as modern
live video streaming is HTTP-based. Since these nodes
communicate with the controller across the WAN, we
see realistic cross-traﬃc, loss, and delays representative
of a real-world deployment.
7 Evaluation
We evaluate VDN in two ways: a trace-driven evaluation
of the central optimization focusing on the quality/cost
tradeoﬀ and scalability; and an end-to-end wide-area
evaluation to test the responsiveness and performance of
hybrid control in the presence of variation and failures
in real-world environments.
7.1 Trace-driven evaluation
We answer three questions:
1. Does VDN improve video quality and reduce cost?
VDN improves the average bitrate at clients by 1.7×
in heavy-tail scenarios and can reduce cost by 2× in
large-event scenarios over traditional CDNs.
2. How does VDN scale? How sensitive is VDN to the
network topology? We scale VDN’s control plane to
10K videos and 2K edge clusters and see it performs
well even with low topological connectivity.
3. How much control do operators have over VDN?
The knobs oﬀered by VDN are sensitive enough for
operators to ﬁne-tune the quality/cost tradeoﬀ and
distribution of service over bitrates and videos.
Traces: We evaluate the eﬃcacy of our controller with
three traces representative of common workloads:
• Average Day: A one-hour trace from a service provider
with detailed client-side analytics for a large number
of live video providers.
It is comprised of 55,000
requests for 4,144 videos from 2,587 cities (18,837
clients) and an average request bitrate of 2725 Kbps.
This trace has a long tail: 7% of the videos account
for 50% of the requests. This represents an average
day for a low-demand live video service.
• Large Event: A partially synthetic trace made by
adding four concurrent sports games with 1 million
simultaneous viewers each to Average Day. It is com-
prised of 48M+ requests for 4,148 videos from 2,587
cities (4M clients) and an average request bitrate
of 2725 Kbps. This trace has a very heavy head:
99.89% of requests are for one of the sports games.
This represents a heavy (but easily coordinated) load.
Although the requests are synthesized, the request
bitrate and arrival times maintain the same distribu-
tion as the raw trace.
• Heavy-Tail: A synthetic trace generated from Average
Day imposing a heavy tail distribution with narrower
bitrate variety. It is comprised of 240,000 requests
for 10,000 videos from 2,587 cities (82,000 clients)
and an average request bitrate of 6850 Kbps. This
trace has a heavy tail: the lowest 99% of videos (the
tail) account for 60% of requests. Bitrates are drawn
from the recommended 240p, 480p, 1080p (400, 1000,
4500 Kbps) guidelines from YouTube live [44], with
an additional bitrate of 30 Mbps representing future
4K streams. This represents a heavy load that is hard
to coordinate, akin to Twitch or Ustream. Although
this trace is synthesized, the mapping of clients to
cities and the request arrival times maintain the same
distributions as the raw trace.
Topology: The traces contain no information about
the internal CDN topology, so we generate a three-tiered
topology similar to current CDNs [35] consisting of 4
source clusters, 10 reﬂectors, and 100 edge clusters. Aka-
mai has roughly 1,800 clusters (1,000 networks) located
worldwide [17], so this is roughly the right scale for US-
wide distribution. We push the scale of the topology
up to 2,000 clusters in some experiments. We use a
“hose model” to determine link capacities in our overlay
network. Each source is given 1 Gbps to split between
100 Mbps overlay links to each of the 10 reﬂectors. Each
reﬂector has 3 Gbps to split into 100 Mbps overlay links
to 30 of the 100 edge clusters. Each edge cluster is given
9 Gbps to connect to clients. We chose these capacities
based on the high cost of long-haul WAN links (see §2.1).
Our prototype considers requests at the granularity of
client groups, which we deﬁne to be (city, AS) pairs; we