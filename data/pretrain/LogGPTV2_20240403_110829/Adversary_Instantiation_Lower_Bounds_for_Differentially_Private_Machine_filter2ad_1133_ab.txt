S ⊆ Range(M), for all neighboring data sets D, D(cid:48) ∈ Dn
(where D is the set of all possible data points ) that differ in
only one sample:
Pr[M(D) ∈ S] ≤ eε Pr[M(D(cid:48)) ∈ S] + δ.
Informally, this deﬁnition requires the probability any adver-
sary can observe a difference between the algorithm operating
on dataset D versus a neighboring dataset D(cid:48) is bounded by
eε, plus a constant additive failure rate of δ. To provide a
meaningful theoretical guarantee, ε is typically set to small
single digit numbers, and δ (cid:28) 1/|D|. Other variants of
DP use slightly different formulations (e.g., R´enyi differential
privacy [40] and concentrated DP [18, 5]); our paper studies
predominantly this (ε, δ)-deﬁnition of DP as it is the most
common. Furthermore,
is often possible to translate a
property achieved under one formulation to another, in fact
the DP-SGD analysis uses a R´enyi bound as an intermediate
step to achieving the (ε, δ) bounds.
it
Differential privacy has two useful properties we will use.
First, the composition of multiple differentially private algo-
rithms is still differentially private (adding their respective pri-
vacy budgets). Second, differential privacy is immune to post-
processing: the output of any DP algorithm can be arbitrarily
post-processed while retaining the same privacy guarantees.
Consider the toy problem of reporting the approximate sum
of D = {x1, . . . , xn} with each xi ∈ [−1, 1]. A standard way
to compute a DP sum is the Gaussian mechanism [17]:
where Z ∼ N (0, σ2)
If σ = (cid:112)2 log(1.25/δ)/ε, then it can be shown that M
M(D) =
(cid:88)
xi + Z
xi∈D
(2)
guarantees (, δ)-differential privacy—because each sample
has a bounded range of 1. If xi was unbounded, for any ﬁxed
amount of noise σ, it would always be possible to let
˜D = {−2σ, 0, . . . , 0}
ˆD = {2σ, 0, . . . , 0} ,
so with high probability M(D) > 0 if and only if D = ˆD.
C. Differentially-Private Stochastic Gradient Descent
Stochastic gradient descent (SGD) can be made differ-
entially private through two straightforward modiﬁcations.
Proceed initially as in SGD, and sample a minibatch of
examples randomly from the training dataset. Then, as in
Equation 1, compute the gradient of the loss on this mini-
batch with respect
to the model parameters θ. However,
before directly applying this gradient ∇θL(fθ, B) DP-SGD
ﬁrst makes the gradient differentially private. Intuitively, we
achieve this by (1) bounding the contribution of any individual
training example, and then (2) adding a small amount of noise.
Indeed, this can be seen as an application of the Gaussian
mechanism to the gradients updates.
To begin, DP-SGD clips the gradients so that any individual
update is bounded in magnitude by b, and then adds Gaussian
noise whose scale σ is proportional to b. After sampling a
minibatch B = B(X ), the new update rule becomes
θi+1 ← θi − η
clipb (∇θ(cid:96)(fθi, x, y)) + Zi
(3)
where Zi ∼ N (0, σ2I) and clipb(v) is the function that
projects v onto the (cid:96)2 ball of radius b with
 1
|B|
(cid:88)
(x,y)∈B
(cid:26)
(cid:27)
.
clipb(v) = v · min
1,
b
(cid:107)v(cid:107)2
ε
√
T log( 1
δ )
is not difﬁcult
It
to see how one would achieve (ε, δ)-
DP guarantees following Equation 2. To achieve (ε, δ)-DP,
typically σ is in the order of Ω(q
) [1]. Moreover,
Mironov et. al. [41] showed tighter bounds for a given σ.
On each iteration, we are computing a differentially-private
update of the model parameters given the gradient update ∇θ(cid:96)
through the subsampled Gaussian Mechanism. Then, because
DP is immune to post-processing, we can apply these gradients
to the model. Finally, through composition, we can obtain a
guarantee for the entire model training pipeline.
However, it turns out that naive composition using this
trivial analysis gives values of ε (cid:29) 104 for accurate neu-
ral networks, implying a ratio of adversary true positive to
false positive bounded by e104—and hence not meaningful.
Thus, Abadi et al. [1] develop the Moments Accountant: an
improvement of Bassily et al. [4] which introduces a much
more sophisticated tool to analyze DP-SGD that can prove,
for the same algorithm, values of ε < 10. Since then, many
works [13, 2, 54, 41, 31, 3] improved the analysis of the
Moments Accountant analysis to get better theoretical privacy
bounds.
This raises our question: is the current analysis the best
analysis tool we could hope for? Or does there exist a stronger
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
868
analysis approach that could, again for the same algorithm—
but perhaps with different (stronger) assumptions—reduce ε
by orders of magnitude?
III. ANALYSIS APPROACH: ADVERSARY INSTANTIATION
We answer this question and analyze the tightness of the
DP-SGD analysis under various assumptions by instantiating
the adversary against the differentially private training algo-
rithm. This gives us a lower bounds on the privacy leakage
that may result. Our primary goal is to not only investigate the
tightness of DP-SGD under any one particular set of assump-
tions [26], but also to understand the relative importance of
adversary capabilities assumed to establish this upper bound.
This allows us to appreciate what practical conditions need
to be met before an adversary exploits the full extent of the
privacy leakage tolerated by the upper bound. This section
develops our core algorithm: adversary instantiation. We begin
this adversary (§ III-A)
with our motivations to construct
present the algorithm (§ III-B) and then describe how to use
this algorithm to lower bound privacy (§ III-C).
A. Motivation: Adversary Capabilities
The analysis of DP-SGD is an over-approximation of actual
adversaries that occur in practice. This is for two reasons.
First, there are various assumptions that DP-SGD requires that
are necessarily imposed because DP captures more powerful
adversaries than typically exist. Second, there are adversary
restrictions that we would immediately choose to enforce if
there was a way to do so, but currently have no way to make
use of in the privacy analysis. We brieﬂy discuss each of these.
a) Restrictions imposed by DP: Early deﬁnitions of
privacy were often dataset-dependent deﬁnitions. For example,
k-anonymity argues (informally) that a data sample is private
if there are at least k “similar” [52]. Unfortunately, these
privacy deﬁnitions often have problems when dealing with
high dimensional data [44]—allowing attacks that nevertheless
reveal sensitive information.
In order to avoid these difﬁculties, differential privacy
makes dataset-agnostic guarantees: (ε, δ)-DP must hold for all
datasets D, even for pathologically constructed datasets D. It
is therefore conceivable that the total privacy afforded to any
user in a “typical” dataset might be higher than if they were
placed in the worst-case dataset for that user.
b) Restrictions imposed by the analysis: The second
class of capabilities are those allowed by the proof that the DP-
SGD update rule (Equation 3) satisﬁes differential privacy, but
if it were possible to improve the analysis by preventing these
attacks it would be done. Unfortunately, there is no known
way to improve on the analysis by prohibiting these particular
capabilities—even if we believe that it should help.
The canonical example of this is the publication of interme-
diate models. The DP-SGD analysis assumes the adversary is
given all intermediate model updates {θi}N
i=1 used to train
the neural network, and not
the ﬁnal model. These
assumptions is not made because it is necessary to satisfying
differential privacy, but because the only way we know how
just
to analyze DP-SGD is through a composition over mini-
batch iterations, where the adversary learned all intermediate
updates.
In the special case of training convex models, it turns out
there is a way to directly analyze the privacy of the ﬁnal
learned model [21]. This gives a substantially stronger guaran-
tee than can be proven by summing up the cumulative privacy
loss over each iteration of the training algorithm. However, for
general deep neural networks, there are no known techniques
to achieve this. Hence, we ask the question: does having access
to the intermediate model updates allow an adversary to mount
an attack that leaks more private information from a deep
neural network’s training set?
this is often not
Similarly, the analysis in differential privacy assumes the
adversary has direct control of the gradient updates to the
model. Again however,
true in practice:
the inputs to a machine learning training pipeline are input
examples, not gradients.1 The gradients are derived from the
current model and the worst-case inputs. The proofs place the
the trust boundary at the gradients simply because it is simpler
to analyze the system this way. If it was possible to analyze
the algorithm at the more realistic interface of examples, it
would be done. Through our work, we are able to show when
doing so would not improve the upper bound, in which case
it would be unnecessary to attempt an improvement of the
analysis that relaxes this assumption.
B. Instantiating the DP Adversary
Because assumptions made to analyze the privacy of DP-
SGD appear conservative, researchers often conjecture that
the privacy afforded by DP-SGD is signiﬁcantly stronger than
what can be proven. For example, Carlini et al. [6] argue (and
provide some evidence) that training language models with
ε = 105-DP might be safe in practice despite this offering no
theoretical guarantees. In other words, it is assumed that there
is a gap between the lower bound an adversary may achieve
when attacking the training algorithm and the upper bound
established through the analysis.
Before we expend more effort as a community to improve
on the DP-SGD analysis to tighten the upper bound, it is
important to investigate how wide it may be. If we could
show that the gap is non-existent, there would be no point
in trying to tighten the analysis, and instead, we would need
to identify additional constraints that could be placed on the
adversary in order to decrease the worst-case privacy leakage.
This motivates our alternate method to measure privacy with
a lower bound, through developing an attack. This lets us
directly measure how much privacy restricting each of these
currently-allowed capabilities would afford, should we be able
to analyze the resulting training algorithm strictly.
As the core technical contribution of this paper, in order to
measure this potential gap left by the privacy analysis of DP-
SGD, we instantiate the adversary introduced in the formula-
tion of differential privacy. The objective of the theoretical DP
1For federated learning [37] an adversary will have this capability because
participants in this protocol contribute to learning by sharing gradient updates.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
869
Fig. 2: Our attack process. Our ﬁrst algorithm, the crafter constructs two datasets D and D(cid:48) differing in one example. The
model trainer then (independently of the adversary) trains a model on one of these two datasets. Our second algorithm, the
distinguisher then guesses which dataset was used.
adversary is to distinguish between a model f that was either
trained on dataset D or on dataset D(cid:48), where the datasets only
differ at exactly one instance. Thus, our instantiated adversary
consists of two algorithms: the ﬁrst algorithm chooses the
datasets D and D(cid:48), after which a model is trained on one
of these at random, and then the second algorithm predicts
which dataset was selected and trained on. Figure 2 gives a
schematic of our attack approach containing the three phases
of our attack, described in detail below.
a) The Crafter: The Crafter adversary is a randomized
algorithm taking no inputs and returning two possible sets of
inputs to model training algorithm T . Intuitively, this adver-
sary corresponds to the differential privacy analysis being data-
independent: it should hold for all pairs of datasets which only
differ by one point. We will propose concrete implementations
later in Section IV-A through IV-F. Formally, we denote this
adversary by the function A. In most cases, this means that
A → (D, (x∗, y∗)), with D(cid:48) = D ∪ {(x∗, y∗)}.
b) Model training (not adversarial).: After the Crafter
has supplied the two datasets, the training algorithm runs in a
black box outside of the control of the adversary. The model
trainer ﬁrst randomly chooses one of the two datasets supplied
by the adversary, and then trains the model on this dataset
(either for one step or the full training run). Depending on
the exact details of the training algorithm, differing levels of
information are revealed to the adversary. For example, the
standard training algorithm in DP-SGD is assumed to reveal
all intermediate models {θi}N
i=1. We denote training by the
randomized algorithm t = T (D).
c) The Distinguisher: The Distinguisher adversary is
a randomized algorithm B → {0, 1} that predicts 0 if T
was trained on D (as produced by A), and 1 if D(cid:48). This
corresponds to the differential privacy analysis which estimates
how much an adversary can improve its odds of guessing
whether the training algorithm learned from D or D(cid:48).
The Distinguisher receives as input two pieces of data: the
output of Crafter, and the output of the model training process.
Thus, in all cases the Distinguisher receives the two datasets
produced by Crafter, however in some setups when the Crafter
is called multiple times, the Distinguisher receives the output
of all runs. Along with this, the Crafter receives the output of
the training process, which again depends on the experimental
setup. In most cases, this is either the ﬁnal trained model
weights θN or the sequence of weights {θi}N
C. Lower Bounding ε
i=1.
Performing a single run of the above protocol gives a
single bit: either the adversary wins (by correctly guessing
which dataset was used during training) or not (by guessing
incorrectly). In order to be able to provide meaningful analysis,
we must repeat the above attack multiple times and compute
statistics on the resulting set of trials.
We follow an analysis approach previously employed to
audit DP [12, 26]. We extend the analysis to be able to reason
about both pure DP where δ = 0 and the more commonly
employed (ε, δ) variant, unlike prior work which assumes δ is
always negligible [26]. We deﬁne one instance of the attack as
the Crafter choosing a pair of datasets, the model being trained
on one of these, and the Distinguisher making its guess. For
a ﬁxed training algorithm we would like to run, we deﬁne a
pair of adversaries. Then, we run a large number of instances
on this problem setup. Through Monte Carlo methods, we can
then compute a lower bound on the (ε, δ)-DP.
Given the success or failure bit from each instance of the
attack, we compute the false positive and false negative rates.
Deﬁning the positive and negative classes is arbitrary; without
loss of generality we deﬁne a a false positive as the adversary
guessing D(cid:48) when the model was trained on D, and vice versa
for a false negative. Kairouz et. al [29] showed if a mechanism
M is (, δ)-differentially private then the adversary’s false
positive (FP) and false negative (FN) rate is bounded by:
F N + eεF P ≤ 1 − δ
F P + eεF N ≤ 1 − δ
Therefore, given an appropriate δ, we can determine the
empirical (ε, δ)−differential privacy as
1 − δ − F P
F N
1 − δ − F N
F P
)
(4)
, log
εempirical = max(log
870
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
`DPSGD` or Private ModelTraining DatasetCrafterDistinguisherD or D'Using the Clopper-Pearson method [10], we can determine
conﬁdence bounds on the attack performance. This lets us
determine a lower conﬁdence bound on the empirical ε as
1 − δ − F N high
1 − δ − F P high
εlower
empirical = max(log
F N high
F P high
, log
)
(5)
where FP high is the high conﬁdence bound for false positive
and FN high is the high conﬁdence bound for false negative.
We repeat most of our experiments 1000 times in order to
measure the FPR and FNR. Even if the adversary were to
succeed in all 1000 of the trials, the Clopper-Pearson bound
would imply an epsilon lower bound of 5.60. Note that as
a result of this, it will never be possible for us to establish
a lower bound greater than ε = 5.60 with 1000 trials and a
conﬁdence bound of due solely to statistical power.
IV. EXPERIMENTS
Having developed the methodology to establish a lower
bound on differential privacy guarantees, we now apply it
across six attack models, where we vary adversary capabilities
around four orthogonal key aspects of the ML pipeline.
• Access. What level of access does the adversary have to
the output of the training algorithm?
• Modiﬁcation. How does the adversary perform the ma-
nipulations of the training dataset?
• Dataset. Does the adversary control the entire dataset?
Or just one poisoned example? Or is the dataset assumed
to be a typical dataset (e.g., CIFAR10)?
While it would (in principle) be possible to evaluate the
effect of every possible combination of the assumptions, it
is computationally intractable. Thus, we describe six possible
useful conﬁgurations, where by useful we mean conﬁgurations
which correspond to relevant modern deployments of ML (e.g.,
MLaaS, federated learning, etc.) or key aspects of the privacy
analysis. Table I summarizes the adversaries we consider. Note
that some of the attacks use similar Crafter or Distinguisher,
we did not repeat the identical algorithms. Please refer to
Table I for the Crafter or Distinguisher for each attack. Below
we describe them brieﬂy and relate them to one another before
expanding on them in the remainder of this section. Note that
we do not argue that any particular set of assumptions can
provide a speciﬁc privacy bounds, but rather that it cannot be
more private than our experimental bounds.
API access: The data owner will collect the data and train
the model itself, the adversary cannot control and modify the
training procedure or dataset. The adversary can only have
black box access to the trained model. This setting is the most