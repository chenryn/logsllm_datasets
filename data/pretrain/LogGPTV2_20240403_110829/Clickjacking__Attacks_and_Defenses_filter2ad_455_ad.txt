persuasion control treatment (Group 1) tried to skip the
animation. If we assume the same percent of participants
tried to skip the advertisement during the attack, then
84% of those participants who tried to skip the ad fell
for the attack (43% of 51%).
Defenses. One straightforward defense against cursor-
spooﬁng attacks is to disallow cursor customization.
This would prevent the real cursor from being hidden,
though the attack page could still draw a second, fake
cursor. Some victims might focus on the wrong cursor
and fall for the attack. In Group 4, we disallowed cursor
customization and found that 12 of 72 (16%) participants
still fell for the attack. This result, along with attacker’s
ability to draw multiple fake cursors and emphasize one
that is not the real cursor, suggest this defense has lim-
ited effectiveness. Nevertheless, the defense does appear
to make a dent in the problem, as there is a reduction
in attack success rates from Group 3 (43%), without the
defense, to Group 4 (16%), with the defense, and the dif-
ference between these two treatment groups was statisti-
cally signiﬁcant (p=0.0009).
In Groups 5a-c, we deployed the freezing defense de-
scribed in Section 5.1.2: when this defense triggers, all
movement outside the protected region, including the
video and fake cursor, is halted. This helps break the il-
lusion of the fake cursor and draws the user’s attention to
the part of the screen on which there is still movement—
that which contains the real cursor. The freezing effect
will not help if users have already initiated a click before
noticing it. We thus initiate the freeze when the cursor
is within M pixels of the webcam dialog, for M of 0, 10,
and 20 pixels. At M=20px (Group 5c), the attack success
rate dropped to that of our non-persuasion control group,
Total Timeout Quit Attack Success
43 (47%)
2 (2%)
1 (1%)
0 (0%)
Treatment Group
1. Attack
2a. UI Delay (TA=250ms)
2b. UI Delay (TA=500ms)
3. Pointer re-entry
Table 3: Results of double-click attack. 43 of 90 partic-
ipants fell for the attack that would grant access to their
personal Google data. Two of our defenses stopped the attack
completely.
90
91
89
88
46
89
86
88
1
0
2
0
tricking only 3 of 72 (4%). Fewer participants assigned
to the 20px-border-freezing defense of Group 5c fell for
the attack (4%) than those in the cursor-customization-
defense treatment of Group 4 (16%), and this difference
was signiﬁcant (p=0.0311).
Given the efﬁcacy of the large-margin (20px) freezing
defense in Group 5c, and the low rate of successful at-
tacks on which to improve, our sample was far too small
to detect any further beneﬁts that might result from mut-
ing the speaker or freezing portions of the screen with
a lightbox might provide. Augmenting the freezing de-
fense to mute the computer’s speaker (Group 6) yielded
a similar attack success rate of 2 of 70 (2%) participants.
Augmenting that defense again with a lightbox, grey-
ing over the frozen region as described in Section 5.1.2,
(Groups 7 and 8) also resulted in attack success rates of
2-4%. The lightbox effect is a somewhat jarring user ex-
perience, and our experiments do not provide evidence
that this user-experience cost is offset by a measurably
superior defense. However, larger sample sizes or dif-
ferent attack variants may reveal beneﬁts that our exper-
iment was unable to uncover.
7.3 Double-click attacks
In our second experiment, we tested the efﬁcacy of the
double-click timing attack (described in Section 4.2 and
shown in Figure 2) and the defenses proposed in Sec-
tion 5.2. The attack attempts to trick the user into click-
ing on the “Allow Access” button of a Google OAuth
window by moving it underneath the user’s cursor after
the ﬁrst click of a double-click on a decoy button.
If
the “Allow” button is not clicked within two seconds, the
attack times out without success (column Timeout). The
results of each treatment group appear as rows of Table 3.
Attack. Of the 90 participants assigned to the treatment
in which they were exposed to the simulated attack with-
out any defense to protect them, the attack was success-
ful against 43 of them (47%).
If this had been a real
attack, we could have accessed their GMail to read their
personal messages or download their contacts. Further-
more, many of the users who were not successfully at-
tacked escaped because the popup was not shown quickly
enough. Indeed, the popup took more than 500ms to be
displayed for 31 out of 46 users who timed out on the at-
tack (with 833ms average loading time for those users)—
12
likely greater than a typical user’s double-click speed.
The attack efﬁcacy could likely be improved further by
pre-loading the OAuth dialog in a pop-under window (by
de-focusing the popup window) and refocusing the pop-
under window between the two clicks; this would avoid
popup creation cost during the attack.
Defenses. Two groups of participants were protected
by simulating the UI delay defense described in Sec-
tion 5.2—we treated clicks on the “Allow” button as in-
valid until after it has been fully visible for a threshold of
TA ms. We assigned a treatment group for two choices for
TA: 250ms (Group 2a), the mode of double-click inter-
vals of participants in an early mouse experiment [29] in
1984, and 500ms (Group 2b), the default double-click in-
terval in Windows (the time after which the second click
would be counted as a second click, rather than the sec-
ond half of a double-click) [26]. We observed that the
delay of 250ms was effective, though it was not long
enough for 2 out of 91 (2%) participants in Group 2a,
who still fell for the attack. The difference in attack suc-
cess rates between the attack treatment (Group 1) and
the UI delay defense treatment for TA=250ms (Group 2a)
was signiﬁcant (p
2. (If No to 1) Would you approve if your Facebook
wall showed that you like this page?
3. (If Yes to 1) Did you click on the Like button?
4. (If Yes to 3) Did you intend to click on the Like
button?
We only included participants who either did not approve
“liking” (No to 2), were not aware that they “liked” (No
to 3) or did not intend to “like” (No to 4). This ex-
cludes victims who do not care about “liking” the at-
tacker’s page and who intentionally clicked on the Like
button. We expected the two ﬁlters to yield similar re-
sults; however, as we describe later, the trust in our sur-
vey responses was reduced by indications that partici-
pants lied in their answers. Therefore, we rely on the
on ﬁrst mouseover column for evaluating and comparing
our defenses.
Attacks. We assigned two treatment groups to a simu-
lated whack-a-mole attack that did not employ clickjack-
ing. The ﬁrst (Group 1a) eventually were shown a Like
button to click on whereas the second (Group 1b) were
eventually shown the “allow” button in the Flash webcam
access dialog. In the simulated attack, participants ﬁrst
had to click on a myriad of buttons, many of which were
designed to habituate participants into ignoring the possi-
bility that these buttons might have context outside their
role in the game. These included buttons that contained
the text “great,” “awesome,” and smiley face icons. On
the attack iteration, the Like button simply appeared to be
the next target object to press in the game. We hypothe-
sized that users could be trained to ignore the semantics
usually associated with a user interface element if it ap-
peared within this game.
Though we had designed this attack, its efﬁcacy sur-
prised even us. The Like button version of Group 1a suc-
ceeded on 83 of 84 (98%) participants and the “allow”
button of Group 1b succeeded on 69 of 71 (97%) partici-
pants. The differences between these two groups are not
13
statistically signiﬁcant. The attacks were also so effec-
tive that, at these sample sizes, they left no room in which
to ﬁnd statistically signiﬁcant improvements through the
use of clickjacking.
In the whack-a-mole attack with timing (Group 2), the
Like button is switched to cover one of the game buttons
at a time chosen to anticipate the user’s click. This attack
was also effective, fooling 80 of 84 (95%) participants in
Group 2. Next, we combined the timing technique with
cursor spooﬁng that we also used in Section 7.2, so that
the game is played with a fake cursor, with the attack
(Group 3) succeeding on 83 of 84 (98%) participants.
Defenses. In Groups 4a-c, we combined the proposed
defenses that were individually effective against the pre-
vious cursor-spooﬁng and the double-click attacks, in-
cluding pointer re-entry, appearance delay of TA=500ms,
and display freezing with padding area size M=0px, 10px
and 20px. We assumed that the attacker could be aware
of our defenses; e.g., our attack compensated for the ap-
pearance delay by substituting the Like button roughly
500ms before the anticipated user click.
Using no padding area (M=0px), the attack succeeded
on the ﬁrst mouseover on 42 of 77 (54%) of the partici-
pants in Group 4a. The reduction in the ﬁrst-mouseover-
success rate from Group 3 (without defense) to 4a (with
the M=0px combined defense) was statistically signiﬁ-
cant, with p<0.0001. So, while all of the participants
in Group 4a eventually clicked on the Like button, the
defense caused more users to move their mouse away
from the Like button before clicking on it.
Increas-
ing the padding area to M=10px (Group 4b) further re-
duced the ﬁrst-mouseover success rate to 27 of 78 (34%),