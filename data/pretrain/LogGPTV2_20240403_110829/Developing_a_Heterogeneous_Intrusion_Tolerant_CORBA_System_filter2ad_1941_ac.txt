ordering  group,  it  causes  a  problem  where  a 
replication  domain  needs  to  send  messages  to  a 
singleton  client  (and  it  must  to  send  CORBA 
replies).    Since  each  replication  domain  element 
must  operate  deterministically  for  all  invocations 
and  non-faulty  clients  require  availability,  an 
additional  protocol 
that 
replicated  servers  can  proceed  if  a  singleton  client 
fails, and that the singleton client will get an answer 
if it is not faulty.  
is  needed 
to  ensure 
This  group  membership  choice  also  causes  a 
problem  in  detecting  a  faulty  server  within  the 
replica  group  itself.    Since  members  of  the  replica 
group don’t see the messages sent by the other replicas 
in  the  group,  only  processes  that  receive  messages 
from  the  replica  group  can  detect  faults  based  on 
message values.  Once a fault is detected, the offending 
Figure 3. Connection Establishment 
In CORBA, a client usually has an object reference 
to some service with which it wishes to communicate.  
In ITDOS, the object reference contains the address of 
the replication domain in which that service is located.  
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:17 UTC from IEEE Xplore.  Restrictions apply. 
reference. 
Figure 3 illustrates connection establishment.  Step (a) 
indicates  the  logical  object  invocation  that  the  client 
wishes to make.  Transparently to the application layer, 
the  ITDOS  transport  layer  sends  (1)  an  open_request 
message  to  the  Group  Manager  of  the  replication 
domain  with  the  appropriate  information  from  the 
object 
  The  Group  Manager,  after 
determining that the client and target are both valid and 
available, generates symmetric communication keys to 
both  establish  and  protect  the  connection  for  that 
client-server association.  The communication keys are 
first sent (2) to the target replication domain (using the 
Castro-Liskov  transport),  then  to  the  client  (3).    The 
communication key is used to  encrypt traffic  between 
the client and the server for that connection.  Finally, 
the CORBA invocation is sent via Castro-Liskov to the 
server (4), and the reply is returned to the client (5).   
All of this interaction is accomplished transparently 
to the application developer, since it is integrated into 
the  ORB  through  a  TAO  Pluggable  Protocol.    The 
TAO Pluggable Protocol [27] provides an interface to 
the  ORB  for  ITDOS  to  layer  traditional  socket 
semantics on the Castro-Liskov BFT protocol. 
3.4 
Conceptually, 
Replication Granularity 
the  CORBA  paradigm  provides 
location transparency:    its  APIs  presume  that  objects 
are location-independent; they may exist in any server.  
However,  in  an  actively  replicated,  intrusion  tolerant 
system  such  as  ITDOS,  allowing  unlimited  object 
locations proves problematic.  For instance, if objects a 
and b are co-located in Server A, but only a resides in 
Server  B,  then  when  a  invokes  on  b  in  Server  A  the 
invocation is  local,  but  in  Server  B,  the  invocation  is 
remote.  In the first case, since the invocation does not 
pass  through  the  middleware,  the  voting  mechanism 
does not come into play.  This may lead to inconsistent 
results from the invocation and ordering problems for 
Castro-Liskov.  Replicating a server, complete with all 
the  objects  it  hosts,  is  conceptually  and  technically 
simpler.    This  approach  provides  some  significant 
benefits  as  well.    All  client  interactions  with  separate 
objects hosted by a particular server can use the same 
connection.  Since connection-establishment is a fairly 
heavyweight  process,  connection  reuse  enhances 
performance.    The  restriction  lends  itself  to  greater 
scalability  since the granularity is at the process level 
as  opposed  to  possibly  thousands  of  objects  that may 
need  to  be  managed  individually.    Since  ITDOS 
manages  connections  on  a  process  basis,  we  also 
conserve  multicast  address  allocation,  of  which  there 
are a finite (albeit very large) number. 
Confidentiality 
3.5 
Symmetric 
key 
keys 
using 
provides 
encryption 
group 
client-server 
communication 
confidentiality.  The  Group  Manager 
replication 
domain generates and provides these keys to both the 
client and the server to establish a security association 
between  those  parties.    In  this  group  keying  model 
there are two primary risks to the confidentiality of the 
communications:  compromise  of  a replication  domain 
element  exposing  all  communication  keys 
for 
connections  established  with  that  replication  domain, 
and  compromise  of  an  improperly  designed  Group 
Manager  replication  domain  element  exposing  all 
communication  keys  in  the  system.    In  both  of  these 
cases,  the  compromise  may  be  detected  in  a  timely 
manner,  or  may  remain  undetected  indefinitely.    The 
ITDOS  architecture  must  address  both  of 
these 
situations.  
ITDOS  minimizes  the  impact  of  the  undetected 
compromise  of  a  replication  domain  element  by 
assigning a unique communication key for each pair of 
communicating  client  and  server  replication  domains. 
In this case, confidentiality is only lost for the groups 
of which that element is a member. If the compromised 
element is detected, it is excluded from its replication 
domain (and from receiving new communication keys) 
by  re-keying  the  communication group,  excepting the 
compromised  element.  However,  there  will  be  some 
(short)  period  of 
the  compromised 
communication keys have been replaced. 
time  before 
The  compromise  of  a  Group  Manager  replication 
domain  element,  while  perhaps  less  likely  than  the 
compromise  of  a  typical  replica  (due  to  increased 
security  measures),  could  have  a  significant  and 
unacceptable  impact  on  client/server  confidentiality.  
In  a  “traditional”  approach  to  the  design  of  a  Group 
Manager,  each  of  the  Group  Manager  replication 
domain  elements  agree  on  each  communication  key 
and  distribute  the  entire  key  to  the  appropriate  client 
and  server  replicas  using  secure  channels.  In  such  an 
approach, the compromise of a single Group Manager 
process  would  compromise  all  communication  keys 
known  to  the  Group  Manager  when  the  compromise 
occurred,  and  all  subsequent  communication  keys 
generated until the compromise is detected.  Once the 
compromise  is  detected,  all  the  communication  keys 
and all other sensitive keys known to the compromised 
Group  Manager  replication  domain  element  must  be 
replaced  in  order  to  restore  confidentiality.  The  time 
necessary  to  restore  confidentiality  in  this  scenario  is 
unacceptable.  To  address  this  problem,  the  ITDOS 
architecture 
for 
communication key generation.  Each Group Manager 
replication domain element generates distinct shares of 
cryptography 
threshold 
uses 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:17 UTC from IEEE Xplore.  Restrictions apply. 
a communication key, then distributes its key shares to 
the appropriate client/server replication domains using 
a secure channel2.  The fragmented keys minimize the 
amount  of  key  information  lost  if  a  Group  Manager 
attacker  must 
element 
compromise  multiple 
to  generate 
a 
communication key. 
compromised.  An 
elements 
is 
ITDOS  generates 
During connection establishment (see Figure 3), or 
when  an  element  is  removed  from  a  client  or  server 
replication  domain, 
a  new 
communication key.  Each Group Manager replication 
domain element uses a common non-repeating value as 
an input a distributed (non-interactive) pseudo-random 
function  [26],  (which  is  essentially  equivalent  to  a 
random  access  coin-tossing  scheme  [5])  to  generate 
shares  of  the  same  key.  Examples  of  this  class  of 
function  include  [26,  5,  39].  The  ITDOS  Group 
Manager uses a distributed random number generation 
process  to  initialize  (and  periodically  re-initialize) the 
pseudo-random  number  generators  (PNGs)  of  each 
Group  Manager  replication  domain  element.  The 
outputs  of  the  pseudo-random  number  generators 
become the common inputs to the distributed function. 
ITDOS relies upon configuration inputs for its pseudo-
random  functions.  The  non-interactive  distributed 
function  generates  the  key  shares  and  verification 
information  for  the  secret  key  and  each  key  share.  
These  distributed  functions  prevent  up  to  f  corrupt 
Group  Manager  replication  domain  elements  from 
tampering  with  or  obtaining  the  communication  even 
when  they  combine  their  key  shares  and  verification 
information. 
the  messages  from 
Each  Group  Manager  replication  domain  element 
distributes  its  particular  key  share  and  verification 
information to each of the replication domain elements 
that  will  share  the  communication  key  (e.g.  during 
steps  2  and  3  of  connection  establishment).      The 
clients  and  sever  replication  domain  elements  each 
decrypt 
the  Group  Manager 
replication  domain,  verify  the  correctness  of  the  key 
shares they receive, and combine the shares to form the 
communication  key.      If  f  or  less  Group  Manager 
replication  domain  elements  are  corrupted,  the  client 
and server replication domain elements will be able to 
generate  the  same  key  and  these  replication  domain 
elements can verify which Group Manager replication 
domain elements acted correctly. 
2 When a new replication domain is established each process of the 
Group Manager establishes a unique pair-wise shared symmetric key 
with each replication domain element and a group key that the 
process shares with all of the elements of the replication domain. 
Voting 
3.6 
The  voting  mechanism  is the  key  to  implementing 
intrusion tolerance in a heterogeneous CORBA system. 
Since  the  marshalled  GIOP  [28]  format  can  differ 
depending on platform, ITDOS cannot simply perform 
byte-by-byte voting on the raw message data.  Byte-by-
byte voting does not work correctly in the presence of 
heterogeneity  [3]  or  inexact  values.    Rather,  voting 
must  be  accomplished  in  middleware,  after  the  raw 
message stream has been unmarshalled.  This process 
allows  us  to  determine  equivalency  even  when  the 
underlying 
different.  
Furthermore, by providing access to the actual data in 
the reply or request, the voter can employ much more 
flexible voting algorithms than other implementations.  
In  particular,  ITDOS  bases  its  voting  mechanism  on 
the Voting Virtual Machine [3].  
representation 
data 
is 
The accuracy of floating point and other data types 
may vary from platform to platform.  In this case, the 
voting  algorithm  must  be  able 
to  determine 
equivalency  despite  values  that  may  differ  by  some 
small  value.    This  is  called  inexact  voting,  which 
ITDOS supports [31].  In fact, equivalent values need 
not be transitive with inexact values; if a = b and b = 
c, this does not imply that a = c since b may be close 
to a and c, but a is not close to c. 
Voting  allows  ITDOS  to  provide  an  application 
with  a  single  message  representing  the  correct  value 
based on multiple messages from a replication domain 
with  multiple  members.  This  eliminates  duplicate 
requests  and  replies  from  replicas.    There  is  a  voter 
element for each connection in our protocol stack.  The 
voter  depends  upon the  total  ordering  property  of  the 
secure  reliable  multicast  protocol  to  deliver  messages 
in  a  deterministic  fashion.    In  particular,  the  Castro-
Liskov transport protocol delivers messages, identified 
by both source and request identifier, in the same order 
to  all  correct  processes. 
  Consequently,  each 
deterministic voter reaches a decision threshold in the 
same  order,  thus  preserving  ordered  delivery  of 
messages to the ORB.  Since the voter is deterministic, 
the  individual  replication  domain  elements  need  not 
synchronize  with  each  other  regarding  the  values 
determined  by  their  voters,  a  step  which  would  incur 
additional expense. 
The  voter  requires  a  minimum  of  f+1  identical 
messages or 2f+1 total messages to perform a vote [6].  
It does not wait for all 3f+1 messages to arrive before 
performing a vote since that would cause the system to 
be  vulnerable  to  network  delays  and  faulty  processes 
that may be deliberately slow (or unresponsive).  Since 
the voter must collate messages to enact a vote, it must 
maintain  some  state 
those  messages.  
However,  in  the  situations  where  the  voter  does  not 
regarding 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:17 UTC from IEEE Xplore.  Restrictions apply. 
receive enough messages to perform a vote, the voter 
must  perform  garbage  collection  to  continue  making 
progress and limit the resources it uses. 
In the ITDOS protocol stack, each connection has a 
voter  object  that  collates  messages  on  a  connection 
basis.  Message originators embed request identifiers in 
all the requests and replies, and other control messages 
they send.  The identifiers permit originators to collate 
multiple requests or replies, and to match requests with 
their  corresponding  replies.    ITDOS  guarantees  these 
identifiers  to  be  strictly  increasing  values.    Since 
ITDOS  uses  a  single-threaded  concurrency  model,  a 
client replication domain can only send a new request 
after  the  client  replication  domain  has  received  and 
voted  upon  the  replies  to  the  previous  requests.  
Consequently,  only  one  outstanding  request  can  exist 
for a connection at a time. 
Voters determine when to stop waiting for messages 
forthcoming from a replication domain.  For example, 
a  client  replication  domain  element  invokes  on  a 
Server  replication  domain,  then  waits  for  a  reply.    N 
replication  domain  elements  in  the  Server  replication 
domain send replies.  If everything works as expected 
the  client  replication  domain  element  should  receive 
2f+1 messages, then perform a value vote, and finally 
receive  the  remaining  n-(2f+1)  messages.    Request 
identifiers in the reply identify the request that initiated 
that reply.  Any just-received request identifier should 
match the identifier of the outstanding request that the 
client  replication  domain  element  last  sent,  and  on 
which  the  replication  domain  element  is  currently 
voting.    If  the  reply’s  identifier  does  not  match  the 
expected  message  value,  then  the  ITDOS  receiver 
discards  the  message.    It  does  this  regardless  of 
whether or not the receiver has accepted all n copies of 
a message with a given request identifier.   
  The  receiver  neither  uses 
A  discarded  message  could  be  from  a  Byzantine 
process,  or  it  could  be  a  late-coming  reply  from  an 
earlier  request. 
the 
message’s  value  nor  penalizes  the  sender.    By  doing 