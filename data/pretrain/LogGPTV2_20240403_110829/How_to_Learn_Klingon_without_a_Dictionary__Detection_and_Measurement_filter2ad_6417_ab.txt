similar among search engines and here we use Baidu as an
example. In fact, Baidu considers two types of input to correlate
keywords [10]:
• Keywords typed by one user in a search sequence. This
is the primary factor to determine the similarity between
keywords. When a user queries one search term (say
SA) but does not click any search result, any following
keywords with similar characters inputed by the user (say
SB) will be treated as related to SA.
• Result links clicked by a user. From the perspective of
search engines, the result links clicked by the user are
expected to be relevant to the search keyword (say SC).
Meanwhile, the landing page redirected from the search
result has already been analyzed and the representative
keywords embedded in the page have been extracted (say
SD1, SD2, ..., SDn in a set SD). The search engine would
consider SC and a subset of SD (usually 2 or 3 most
prominent keywords) as potentially related. If a pair of
keywords shows up together with high frequency (e.g.,
SC and SD1), the pair is treated as related.
Given that related keywords are extracted through analyzing
user’s searching behaviors and it is usual for a user to search
1We consider sites delivering child abuse and pornography content also part
of the underground economy as they usually ask for payment.
754
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:28:43 UTC from IEEE Xplore.  Restrictions apply. 
our solution, KDES. We ﬁrst overview the architecture and then
describe the design of each component.
A. Overview
Understanding the meaning of search keywords and identi-
fying the black ones seems to be an obvious solution to our
problem. In fact, there have been prominent progresses made
in the domain of NLP in analyzing short texts [18], [19], [20],
and the techniques were also incorporated by search engines
to increase the relevancy of search results [21], [22]. However,
such direction is unlikely to succeed in our settings, as a large
amount of black keywords are deliberately obfuscated and
context-dependent. The transformation rules and context are
usually absent for NLP tools. As a result, it is almost impossible
to infer keywords’ meaning directly.
During our empirical analysis on black keywords, we found
they frequently appear in SEO pages, underground forums and
merchants’ websites. A lot of such sites were indexed by search
engines 2 and a noticeable proportion of them trigger alarms
of detection systems. Therefore, we could leverage the labels
(malicious, fraud and etc.) of the search results associated with
one keyword to backtrack and infer whether it is black.
Motivated by the above observations, we developed KDES to
detect black keywords unknown to the security tools or analysts.
Figure 4 illustrates the system architecture. The keywords
extraction module analyzes pages related to underground
economy and extracts the keywords intended for SEO purposes.
It also ﬁlters out the irrelevant keywords which is legitimate
by using search engines as oracle. The remaining keywords are
tunneled to the keywords expansion module and we leverage the
related-search functionality to ﬁnd other similar black keywords.
A massive amount of keywords would be discovered after these
steps, which is a heavy burden for analysts. The majority of the
keywords are long-tail ones which are usually concatenations of
“core words” (directly tied to illegal products or services) and
“ﬁller words” (less meaningful words like stop words). Core
words are much more important and they are discovered by
KDES through the core words identiﬁcation module. Hereby,
an analyst could ﬁnd the interesting black keywords timely
and prioritize her investigation.
Data source. As described in Section II, we implemented the
same approach of [8] to detect SEO pages employed by spider
pool. We targeted spider pool because it is widely used by
blackhat SEO community in China nowadays and supports a
broad spectrum of underground businesses. We collaborated
with Baidu and scanned its indexed pages using our spider
pool detection system from Aug 25th to Sep 10th, 2016, which
in total yielded 2,733,728 SEO pages. We also obtained 63,424
pages marked as “evil” by Baidu, including 60,000 porn pages
and 3,424 gambling pages. The label “evil” means the page
content is associated with sex, gambling, dangerous goods,
surrogacy, drug, faked sites and etc. These abundant data
2There are many underground transactions happening in anonymous
marketplace, like Silk Road, which cannot be indexed by search engines.
But still a large number of merchants and buyers communicate through visible
internet services. We give more details in Section VII.
755
(2,797,152 pages in total) provided us a comprehensive view
of the underground market, but it is also a nontrivial task to
identify the unknown black keywords, similar to ﬁnding the
needle in a haystack.
Our spider-pool detector and Baidu’s detector focus on URL-
level detection (see Appendix B for more details). The black
keywords embedded within the web page are usually tangled
with legitimate terms (see Section III-B) and how to pinpoint
them accurately is not addressed by existing works. Though
Baidu maintains an internal blacklist of keywords, they have
made great efforts to analyze the keywords, especially those
in the ﬁeld of phishing sites and malware. Not any in-depth
study has been conducted in all ﬁelds of black keywords.
While machine-learning techniques have been used to generate
realistic-looking passwords [23], due to shortage of samples,
these techniques cannot be applied for discovering black
keywords, especially the ones referring to new underground
businesses or obfuscated manually with new rules. Below, we
elaborate how we design each module of KDES to detect black
keywords.
B. Keywords extraction
SEO practice (both whitehat and blackhat) advocates direct
embedding of targeted keywords in the web page, to increase
the relevance score assigned by search engines. Blackhat
SEO differs from whitehat SEO in that it recommends “term
spamming” [9], which ﬁlls a massive amount of keywords
(e.g., long-tail keywords under one topic) into the page to get
it associated with as many search keywords as possible. As
such, extracting black keywords from SEO pages becomes a
natural choice.
As summarized by Gyongyi et al. [9], term spamming could
happen in the body, title, meta tag, anchor text and hosting
URL. We started from extracting keywords from all the above
cases but soon gave up this approach, due to the high error
rate. Black keywords are usually stitched together or mixed
with legitimate ones and how to separate them is unclear
(even just splitting a sentence into words is not easy for some
languages, like Chinese [24]). After some failed attempts, we
found extracting keywords from anchor texts (text inside the
HTML tag a href) could yield the most promising result.
Figure 5 shows one example of SEO page hosted by spider
pool, which was dedicated for methamphetamine promotion.
It contains 5 consecutive anchors and for each anchor, a black
keyword is presented together with a URL pointing to another
SEO site. We examined a small corpus of SEO pages and
found that for all of them, the black keywords embedded in
anchors are “clean-cut”, meaning that they are not mixed with
other black keywords or legitimate ones. Hence, we decided
to narrow our scope to only anchor texts.
Keywords ﬁltering. After extracting the keywords from SEO
pages, we removed all duplicate ones, which still left us
a gigantic keyword list (details described in Section IV-C).
Through manual sampling, we found most of the keywords are
beyond our target. One prominent error source is news link. In
fact, spider pool often copies the web pages from news sites
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:28:43 UTC from IEEE Xplore.  Restrictions apply. 
SEO Pages 
Porn pages 
Gamble pages 
Extraction 
E
Search Results 
URL_1, URL_2, URL_3, … 
Over-length 
Duplication 
Word_A 
Word_C 
Word_E 
… 
Filter 
Word_A 
Word_B 
Word_C 
Word_D 
Word_E 
… 
Keywords 
List 
Search Query 
y
& Scanning 
& Scanning
Expansion 
Fig. 4: Architecture of KDES.
Word_A 
Word_E 
Word_F 
… 
Core Word 
C
W d
Identification 
Core_I 
Word_A 
Word_F 
Core_II 
Word_E 
Core_III 
… 
and interleaves black keywords with the legitimate news links,
in order to bypass the spam checker of search engine. Similar
cases also appear in other “evil” pages marked by Baidu. In
order to evade detection, the administrators of those sites prefer
to add some anchors with legitimate content to their pages.
These anchor texts are hard to remove through NLP based
analysis, but we discovered a shortcut to quickly remove a
large proportion of them: keywords in these anchors are usually
much longer than the black keywords, and we could use a
threshold of string length (denoted T HLen) for ﬁltering (we
elaborate how T HLen is determined in Section IV-B).
Yet, not every keyword remained after the above step is
wanted. For instance, some keywords might refer to the titles
of other pages under the same site. Instead of making decision
based on their intention, we solve the problem from the opposite
direction by looking into the consequences led by them. Our
approach is both simple and effective: we issue search queries
using all remained keywords and scan each link presented by
the search result using the existing detectors. We inspect the ﬁrst
ﬁve page returned by the search engine and if a decent number
of links are alarmed (above a threshold T HF lag), we label the
keyword black. Focusing on the ﬁrst ﬁve page can help us
detect black keywords more accurately, because legitimate
sites have strong motivation to gain good rankings under
legitimate keywords while stay away from black keywords 3.
In Section IV-A we give more details on the implementation
and how we determine T HF lag.
C. Keywords expansion
We obtained a plenty of black keywords after the above
steps but their vocabulary is far from being exhausted, in part
to that we did not (and cannot) collect the pages spanning
all underground business. To complement the missing part,
we leverage the related search feature provided by the search
engine to ﬁnd unknown black keywords similar to what we
have discovered. As described in Section II, related search
presents similar keywords based on users’ querying behavior.
Therefore, the keywords shown under related search should
3Black keywords might be written in the articles of legitimate sites, but
they are less likely to be promoted for SEO purposes.
Fig. 5: An example of SEO page with anchor text highlighted.
K1:(cid:3)(cid:3) (cid:2375)(cid:1244)(cid:2842)(cid:18428)(cid:1184)(cid:2144)(cid:9035)(cid:3344) 
(Where to buy roin in Beijing) 
K2:(cid:3) (cid:5295)(cid:5134)(cid:9127)(cid:9035)(cid:3344) 
(Heroin in Guangzhou) 
K3:(cid:3) (cid:1082)(cid:9127)(cid:2842)(cid:18428)(cid:2438)(cid:9035)(cid:3344) 
(Where sells roin in Shanghai) 
Filler-word stripping 
(cid:2375)(cid:1244)(cid:2842)(cid:18428)(cid:1184)(cid:2144)(cid:9035)(cid:3344)(cid:3344) 
(cid:18428)(cid:1184)(cid:2144)(cid:9035)
(cid:5295)(cid:5134)(cid:9127)(cid:9035)(cid:3344) 
(cid:1082)(cid:9127)(cid:2842)(cid:18428)(cid:2438)(cid:9035)(cid:3344) 
Decomposing
Decomposing 
Core word cluster 
(cid:9035)(cid:3344)  (roin) 
Freq>TH 
K1  K2  K3 
Substring 
matching 
[(cid:1184)(cid:2144)(cid:9035)(cid:3344), (cid:1184)(cid:2144)(cid:9035), (cid:2144)(cid:9035)(cid:3344),  
(cid:1184)(cid:2144), (cid:2144)(cid:9035), (cid:9035)(cid:3344)] 
[(cid:9127)(cid:9035)(cid:3344), (cid:9127)(cid:9035), (cid:9035)(cid:3344)] 
[(cid:2438)(cid:9035)(cid:3344), (cid:2438)(cid:9035), (cid:9035)(cid:3344)] 
Fig. 6: An example demonstrating core word identiﬁcation.
have high chance to be black if we seed one conﬁrmed black
keyword 4.
Speciﬁcally, we query a black keyword and save all the
4The adversary might submit many irrelevant queries following a black-
keyword query to poison the keywords relations, in order to misguide our
system. However, this method requires huge computing resources to override
the large volume of search queries from ordinary users. Such anomaly can be
spotted easily.
756
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:28:43 UTC from IEEE Xplore.  Restrictions apply. 
keywords listed in the related search section (say WRel). If a
word W1 from WRel has not been examined before, we run it
through search engine and use the same detection method of
keywords ﬁltering component (count the number of alarmed
search results) for classiﬁcation. While we could continue the
expansion by seeding W1 recursively, the cost is prohibitively
high. To demonstrate the overhead, we use Baidu query as
an example. Baidu usually (and maximumly) returns 9 related
keywords for one query, so one-round expansion equals to 9
search queries. Seeding all of them for the next round leads to
81 additional queries (90 by including the ﬁrst-round queries).
Even with Baidu’s internal API, 90 queries would cause notable
overhead (at about 2 seconds and this API is only accessible
to us at night). In addition, the more iterations we run, the less
relevant are the keywords returned.
D. Core word identiﬁcation
We could report all the keywords produced by the previous
steps to analysts but the reviewing procedure would be very
painful, if without further processing. In fact, many black
keywords are just extension of a limited set of core words (e.g.,
“heroin” is the core word for the long-tail keyword “where to
buy heroin in Beijing”) which are much more attractive to the
analysts. Hence, we developed a component to identify the
core words and cluster similar black keywords under them to
curtail analysts’ workload.
NLP based text analysis is not a viable solution here. Many
black keywords are obfuscated (e.g., letter changed to digit) and
some of them are not even processable by NLP. For example,
“菠菜”(Bo Cai) means spinach in Chinese, and its pronunciation
is exactly the same as “博彩”(Bo Cai) which means gambling.
So NLP techniques will identify "Bo Cai" as spinach, rather
than gambling, thereby this expression can evade the detection
of NLP. Besides, NLP requires a basic set of language elements
as input but so far there is no comprehensive reference for
black keywords. As such, we cannot identify the core words by
analyzing their semantic meanings (like NER). Taking one step
back, we could use NLP to just parse the keyword string into
separated elements and pick up the elements that are unusual
but shown in many detected keywords. Unfortunately, this
approach fails too. A lot of black keywords we encountered
are in Chinese, and word segmentation for Chinese sentence
is a notoriously hard problem [24]. We tried state-of-art NLP
tools (e.g., Natural Language Toolkit [25] and nlp-toolkit [26])
on 1000 samples and the false-positive rate is over 10%.
Alternatively, we exploit one prominent feature of black
keywords, that most of them are long-tail keywords generated
by combining ﬁller words and core words, to address this
problem. The ﬁller words are picked from a dictionary,
including stop words (like “a” and “where”) and places (like
“Beijing” and “China”).
While the parts except ﬁller words are not always core words,
we can identify our targets by picking up the substrings showing
up with sufﬁcient frequency, on the grounds that core words are
widely shared in underground community. Stripping off ﬁller
words is straightforward, but ﬁnding the common substring is
Fig. 7: Pseudocode of Core Word Identiﬁcation Algorithm.
not so easy. The problem we are dealing with here is similar
to ﬁnding the longest common substring (LCS) among a list of
strings [27]. There are several shortcuts allowing us to design
an efﬁcient algorithm: each keyword is short after removing
ﬁller words; a core word only needs to be discovered from
some keywords to be prominent. As such, our algorithm starts
from breaking a keyword into substrings whose length are
more than a threshold T HSubLen (set to 2 characters during
evaluation). Then, it picks one substring and check its existence
in every other keyword. We stop the searching process earlier
if the number of matched keywords is above T Hf req (10 based
on our empirical analysis). The substring passed the check is
considered as a core word and we store it together with its
connection to the owner keyword 5. The process goes on until
all substrings of all keywords are examined. Figure 6 shows
one example processed by our algorithm and Figure 7 shows
the pseudo-code for this algorithm. In the end, all core words
and their associated black-keyword clusters are sent to the
analysts for review.
IV. IMPLEMENTATION AND EVALUATION
A. Implementing KDES
We bootstrapped KDES by loading the 2,797,152 pages
detected by our spider-pool scanner and the evil pages provided
by Baidu, as described in Section III-A. Both the crawlers
run by us and Baidu downloaded the HTML pages without
executing dynamic content (e.g., JavaScript code) or storing
pages linked by iframe for efﬁciency. We found that in most
cases black keywords are rendered in the main HTML pages
so the volume of overlooked keywords should be relatively
small comparing to what we have captured.
Parsing all the pages on a single machine is quite time-
consuming. As a result, we implemented the parser on a
Apache Hadoop cluster (consisting of 166 machines), which
stored all the pages in Hadoop Distributed File System (HDFS)
and ran MapReduce jobs for processing. Our parser leverages
BeautifulSoup to extract keywords from anchors and we stored
5A keyword might be associated with multiple core words. For instance,
“heroin” and “methamphetamine” are both core words within “where to buy
heroin and methamphetamine”
757
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:28:43 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 8: Statistics on different T HLen.
Fig. 9: Statistics on different T HF lag.
them on a workstation (12 core E5 CPU, 128G Memory). In
total, we got 8,812,609 keywords in 27 hours were consumed
at this stage.
The next step is to ﬁlter the duplication and overlength
keywords, then query the keywords with search engine. Search
requests are usually throttled by either CAPTCHA or volume
control if queried in large volume. Baidu generously granted us
access to its unthrottled version of search API and we ﬁnished
the job (including related search) in 5 days. We scanned all
URLs in search results using an in-house scanner provided by
Baidu together with our spider-pool detector and we elaborate
this mechanism in Appendix B.
For keywords expansion, we utilized the related search
feature from Baidu and queried all related terms (9 maximum)
per detected keyword. For core word identiﬁcation, we also
used the Hadoop cluster to parallelize the matching process
of substrings, which was completed in 0.5 hours (for 478,879
keywords).
B. Parameter selection
Below we elaborate the process of parameter tuning for
T HLen and T HF lag.
Keyword length (T HLen). We use this threshold to ﬁlter
out anchor texts about legitimate content or news links. We
experimented with values from 5 to 19 and counted the number
of remained keywords and the ratio of keywords that are