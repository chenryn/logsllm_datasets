# 十、Kubernetes 集群的实时监控和资源管理
服务的可用性是**机密性、完整性和可用性** ( **CIA** )三位一体的关键组成部分之一。已经有许多恶意攻击者使用不同技术来破坏用户服务可用性的实例。对电网和银行等关键基础设施的一些攻击给经济造成了重大损失。最重大的攻击之一是对亚马逊 AWS 路由基础设施的攻击，导致全球核心 IT 服务中断。为了避免此类问题，基础架构工程师实时监控资源使用情况和应用运行状况，以确保组织提供的服务的可用性。实时监控通常插入到警报系统中，当观察到服务中断的症状时，该系统会通知风险承担者。
在本章中，我们将了解如何确保 Kubernetes 集群中的服务始终处于启动和运行状态。我们将首先讨论整体环境中的监控和资源管理。接下来，我们将讨论资源请求和资源限制，这是 Kubernetes 中资源管理的两个核心概念。然后，在将我们的重点转移到资源监控之前，我们将看看像`LimitRanger`这样的工具，Kubernetes 为资源管理提供了这些工具。我们将看看内置监视器，如 Kubernetes 仪表板和度量服务器。最后，我们将看看开源工具，如普罗米修斯和格拉夫纳，它们可以用来监控 Kubernetes 集群的状态。
在本章中，我们将讨论以下内容:
*   整体环境中的实时监控和管理
*   管理 Kubernetes 的资源
*   Kubernetes 的资源监测
# 整体环境中的实时监控和管理
资源管理和监控在整体环境中也很重要。在整体环境中，基础架构工程师通常将 Linux 工具(如`top`、`ntop`和`htop`的输出传输到数据可视化工具，以监控虚拟机的状态。在托管环境中，内置工具(如亚马逊云观察和 Azure 资源管理器)有助于监控资源使用情况。
除了资源监控，基础架构工程师还主动为流程和其他实体分配最低资源要求和使用限制。这确保了有足够的资源可用于服务。此外，资源管理确保不良或恶意的进程不会占用资源并阻止其他进程工作。对于整体部署，资源(如中央处理器、内存和派生进程)对于不同的进程是有上限的。在 Linux 上，进程限制可以使用`prlimit`来限定:
```
$prlimit --nproc=2 --pid=18065
```
该命令将父进程可以派生的子进程限制为`2`。有了这个限制，如果 PID 为`18065`的进程试图产生比`2`子进程更多的子进程，它将被拒绝。
类似于整体环境，Kubernetes 集群运行多个 Pod 、部署和服务。如果攻击者能够生成 Kubernetes 对象，如 pods 或部署，攻击者可以通过耗尽 Kubernetes 集群中的可用资源来导致拒绝服务攻击。如果没有足够的资源监控和资源管理，集群中运行的服务不可用会对组织造成经济影响。
# 管理 Kubernetes 的资源
Kubernetes 提供主动分配和限制 Kubernetes 对象可用资源的能力。在本节中，我们将讨论资源请求和限制，它们构成了 Kubernetes 中资源管理的基础。接下来，我们探索命名空间资源配额和限制范围。使用这两个特性，集群，管理员可以限制不同 Kubernetes 对象的计算和存储限制。
## 资源请求和限制
`kube-scheduler`，正如我们在 [*第 1 章*](01.html#_idTextAnchor020)*Kubernetes 架构*中所讨论的，是默认调度器，运行在主节点上。`kube-scheduler`为未调度的 Pod 找到运行的最佳节点。它通过根据 pod 请求的存储和计算资源过滤节点来实现这一点。如果调度程序找不到 pod 的节点，pod 将保持挂起状态。此外，如果节点的所有资源都被 pods 利用，节点上的`kubelet`将清理死 pods-未使用的映像。如果清理不能减轻压力，`kubelet`将开始驱逐那些消耗更多资源的 PODS。
资源请求指定了 Kubernetes 对象保证会得到什么。不同的 Kubernetes 变体或云提供商对资源请求有不同的默认值。可以在工作负载的规范中指定 Kubernetes 对象的自定义资源请求。可以为中央处理器、内存和大内存指定资源请求。让我们看一个资源请求的例子。
让我们在`yaml`规范中创建一个没有资源请求的 pod，如下所示:
```
apiVersion: v1
kind: Pod
metadata:
  name: demo
spec:
  containers:
  - name: demo
```
pod 将使用默认资源请求进行部署:
```
$kubectl get pod demo —output=yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"demo","namespace":"default"},"spec":{"containers":[{"image":"nginx","name":"demo"}]}}
    kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu request for container
      demo'
  creationTimestamp: "2020-05-07T21:54:47Z"
  name: demo
  namespace: default
  resourceVersion: "3455"
  selfLink: /api/v1/namespaces/default/pods/demo
  uid: 5e783495-90ad-11ea-ae75-42010a800074
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: demo
    resources:
      requests:
        cpu: 100m
```
对于前面的示例，默认资源请求是 pod 的 0.1 个 CPU 内核。现在让我们向`.yaml`规范添加一个资源请求，看看会发生什么:
```
apiVersion: v1
kind: Pod
metadata:
  name: demo
spec:
  containers:
  - name: demo
    image: nginx
    resources:
      limits:
          hugepages-2Mi: 100Mi
      requests:
        cpu: 500m         memory: 300Mi         hugepages-2Mi: 100Mi 
```
该规范创建了一个 pod，其资源请求为 0.5 个 CPU 内核、300 MB 和 100 MB 的`hugepages-2Mi`。您可以使用以下命令检查 pod 的资源请求:
```
$kubectl get pod demo —output=yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2020-05-07T22:02:16Z"
  name: demo-1
  namespace: default
  resourceVersion: "5030"
  selfLink: /api/v1/namespaces/default/pods/demo-1
  uid: 6a276dd2-90ae-11ea-ae75-42010a800074
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: demo
    resources:
      limits:
        hugepages-2Mi: 100Mi
      requests:
        cpu: 500m
        hugepages-2Mi: 100Mi
        memory: 300Mi
```
从输出中可以看到，pod 使用了 0.5 个 CPU 内核、300 MB`memory`和 100 MB 2mb`hugepages`的自定义资源请求，而不是默认的 1 MB。
另一方面，限制是对 pod 可以使用的资源的硬性限制。限制规定了 Pod 允许使用的最大资源。如果需要的资源超过限制中指定的数量，则限制 Pods。与资源请求类似，您可以指定 CPU、内存和大内存的限制。让我们看一个极限的例子:
```
$ cat stress.yaml
apiVersion: v1
kind: Pod
metadata:
  name: demo
spec:
  containers:
  - name: demo
    image: polinux/stress
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
```
这个 Pod 启动了一个压力过程，试图在启动时分配`150M`的内存。如果`.yaml`规范中没有规定限制，Pod 运行没有任何问题:
```
$ kubectl create -f stress.yaml pod/demo created
$ kubectl get pods NAME         READY   STATUS             RESTARTS   AGE demo         1/1     Running            0          3h
```
在 Pod 的`yaml`规范的容器部分中增加了限制:
```
containers:
  - name: demo
    image: polinux/stress
    resources:
      limits:
        memory: "150Mi"
    command: ["stress"]
args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
```
应力过程无法运行，Pod 运行至`CrashLoopBackOff`:
```
$ kubectl get pods
NAME     READY   STATUS             RESTARTS   AGE
demo     1/1     Running            0          44s
demo-1   0/1     CrashLoopBackOff   1          5s
```
您可以看到，当您描述 pod 时，pod 因`OOMKilled`错误而终止:
```
$ kubectl describe pods demo
Name:         demo
Namespace:    default
...
Containers:
  demo:
    Container ID:  docker://a43de56a456342f7d53fa9752aa4fa7366 cd4b8c395b658d1fc607f2703750c2
    Image:         polinux/stress
    Image ID:      docker-pullable://polinux/stress@sha256:b61 44f84f9c15dac80deb48d3a646b55c7043ab1d83ea0a697c09097aaad21aa
...
    Command:
      stress
    Args:
      --vm
      1
      --vm-bytes
      150M
      --vm-hang
      1
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    1
      Started:      Mon, 04 May 2020 10:48:14 -0700
      Finished:     Mon, 04 May 2020 10:48:14 -0700
```
资源请求和限制被转换，映射到`docker`参数–`—cpu-shares`和`—memory`标志，并传递给容器运行时。
我们看了资源请求和限制如何作用于 pods 的例子，但是同样的例子也适用于 DaemonSet、Deployments 和 StatefulSets。接下来，我们看看命名空间资源配额如何帮助为 namespace 可以使用的资源设置上限。
## 命名空间资源配额
名称空间的资源配额有助于定义名称空间内所有对象可用的资源请求和限制。使用资源配额，您可以限制以下内容:
*   `request.cpu`:命名空间中所有对象对 CPU 的最大资源请求。
*   `request.memory`:命名空间中所有对象对内存的最大资源请求。
*   `limit.cpu`:命名空间中所有对象的 CPU 最大资源限制。
*   `limit.memory`:命名空间中所有对象的最大内存资源限制。
*   `requests.storage`:一个命名空间中存储请求的总和不能超过这个值。
*   `count`:资源配额也可以用来限制集群中不同 Kubernetes 对象的数量，包括 pods、服务、PersistentVolumeClaims 和 ConfigMaps。
默认情况下，云提供商或不同的变体对命名空间应用标准限制。在**谷歌 Kubernetes 引擎** ( **GKE** )上，`cpu`请求被设置为 0.1 个 CPU 内核:
```
$ kubectl describe namespace default
Name:         default
Labels:       
Annotations:  
Status:       Active
Resource Quotas
 Name:                       gke-resource-quotas
 Resource                    Used  Hard
 --------                    ---   ---
 count/ingresses.extensions  0     100
 count/jobs.batch            0     5k
 pods                        2     1500
 services                    1     500
Resource Limits
 Type       Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---  ---------------  -------------  -----------------------
 Container  cpu       -    -    100m             -              -
```
让我们看一个将资源配额应用于命名空间时会发生什么的示例:
1.  创建名称空间演示:
    ```
    $ kubectl create namespace demo
    namespace/demo created
    ```
2.  定义资源配额。在本例中，配额将资源请求 CPU 限制为`1` CPU:
    ```
    $ cat quota.yaml
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: compute-resources
    spec:
      hard:
        requests.cpu: "1"
    ```
3.  使用以下命令将配额应用于命名空间:
    ```
    $ kubectl apply -f quota.yaml --namespace demo
    resourcequota/compute-resources created
    ```
4.  您可以通过执行以下命令来检查资源配额是否成功应用于命名空间:
    ```
    $ kubectl describe namespace demo
    Name:         demo
    Labels:       
    Annotations:  
    Status:       Active
    Resource Quotas
     Name:         compute-resources
     Resource      Used  Hard
     --------      ---   ---
     requests.cpu  0     1
     Name:                       gke-resource-quotas
     Resource                    Used  Hard
     --------                    ---   ---
     count/ingresses.extensions  0     100