(median 25.14ms). However, ms can increase signicantly
in high load scenarios, and we conduct an experiment to
understand the impact on response latency of sRLA. In Fig-
ure 18, we vary ms from 100 (used in the above experiments)
to 1000, and measure the response latency. We nd that the
average response time only slightly increase for larger ms.
This is because, ms determines the input layer size, which
AuTO: Scaling Deep Reinforcement Learning
for Datacenter-Scale Automatic Traic Optimization
only aects the matrix size of link weights between the input
layer and the rst hidden layer. Moreover, if in the future
AuTO employs more complex DNNs, we can reduce the re-
sponse latency with parallelization techniques proposed for
DRL [6, 25, 27, 39].
CS Scalability Since our testbed is small, the NIC capacity
of CS server is not fully saturated. Using the same parameter
settings as in the experiments (§6.1.3), the bandwidth of mon-
itoring ows is 12.40Kbps per server. Assuming 1Gbps net-
work interface, the CS server should support 80.64K servers,
which should be able to handle the servers in typical produc-
tion datacenters [50, 54]. We also intend to achieve higher
scalability in the following ways: 1) 1Gbps link capacity is
chosen to mimic the experiment environment, and in current
production datacenters, the typical bandwidth of of server is
usually 10Gbps or above [50, 54]; 2) we expect CS to have
GPUs or other hardware accelerators [46], so that the compu-
tation can complete faster; 3) we can reduce the bandwidth
requirement of monitoring ows by implementing compres-
sion and/or sampling in PS.
PS Overhead End-host overhead refers to the additional
work done for each ow to collect information and enforce ac-
tions. The overhead can be measured by CPU utilization and
reduction in throughput when PS is running. We measured
both metric during the experiments, and rerun the ows
without enabling MM and EM. We nd that the throughput
degradation is negligible, and the CPU utilization is less than
1%. Since EM is similar to the tagging module in PIAS [8], our
results conrm that both the throughput and CPU overhead
are also minimal as PIAS.
7 RELATED WORKS
There have been continuous eorts on TO in datacenters. In
general, three categories of mechanisms are explored: load
balancing, congestion control, and ow scheduling. We focus
on the proposals using machine learning techniques.
Routing and load balancing on the Internet have employed
RL-based techniques [13] since 1990s. However, they are
switch-based mechanisms, which are dicult to implement
at line rate in modern datacenters with >10 GbE links. RL
techniques are also used for adaptive video streaming in
Pensieve [37].
Machine learning techniques [59] have been used to opti-
mize parameter setting for congestion control. The parame-
ters are xed given a set of trac distributions, and there is
no adaptation of parameters at run-time.
For ow scheduling, CODA [61] uses unsupervised clus-
tering algorithm to identify ow information without appli-
cation modications. However, its scheduling decisions are
still made by a heuristic algorithm with xed parameters.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
8 CONCLUDING REMARKS
Inspired by recent successes of DRL techniques in solving
complex online control problems, in this paper, we attempted
to enable DRL for automatic TO. However, our experiments
show that the latency of current DRL systems is the major
obstacle to TO at the scale of current datacenters. We solved
this problem by exploiting long-tail distribution of datacenter
trac. We developed a two-level DRL system, AuTO, mim-
icking the Peripheral & Central Nervous Systems in animals,
to solve the scalability problem. We deployed and evaluated
AuTO on a real testbed, and demonstrated its performance
and adaptiveness to dynamic trac in datacenters. AuTO is
a rst step towards automating datacenter TO, and we hope
many software components in AuTO can be reused in other
DRL projects in datacenters.
For future work, while this paper focuses on employing
RL to perform ow scheduling and load balancing, RL algo-
rithms for congestion control and task scheduling can be
developed. In addition to the potential improvements we
mentioned in §5&6, we also plan to investigate applications
of RL beyond datacenters, such as WAN bandwidth manage-
ment.
Acknowledgements: This work is supported in part by
Hong Kong RGC ECS-26200014, GRF-16203715, CRF-C703615G,
& China 973 Program No.2014CB340300. We thank the anony-
mous SIGCOMM reviewers and our shepherd David Ander-
sen for their constructive feedback and suggestions.
REFERENCES
[1] Mohammad Al-Fares, Sivasankar Radhakrishnan, Barath Raghavan,
Nelson Huang, and Amin Vahdat. [n. d.]. Hedera: Dynamic Flow
Scheduling for Data Center Networks. In USENIX NSDI ’10.
[2] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Francis Matus, Rong Pan,
Navindra Yadav, George Varghese, et al. [n. d.]. CONGA: Distributed
congestion-aware load balancing for datacenters. In ACM SIGCOMM
’14.
[3] Mohammad Alizadeh, Albert Greenberg, David A Maltz, Jitendra Pad-
hye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari
Sridharan. [n. d.]. Data Center TCP (dctcp). In ACM SIGCOMM ’10.
[4] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick
McKeown, Balaji Prabhakar, and Scott Shenker. [n. d.]. pFabric: Mini-
mal Near-Optimal Datacenter Transport. In ACM SIGCOMM ’13.
[5] Behnaz Arzani, Selim Ciraci, Boon Thau Loo, Assaf Schuster, and Geo
Outhred. 2016. Taking the Blame Game out of Data Centers Operations
with NetPoirot. In ACM SIGCOMM’16.
[6] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons,
and Jan Kautz. [n. d.]. Reinforcement learning through asynchronous
advantage actor-critic on a gpu. In ICLR’16.
[7] Wei Bai, Kai Chen, Li Chen, Changhoon Kim, and Haitao Wu. [n. d.].
Enabling ECN over Generic Packet Scheduling. In ACM CoNEXT’16.
[8] Wei Bai, Li Chen, Kai Chen, Dongsu Han, Chen Tian, and Weicheng
Sun. [n. d.]. Information-Agnostic Flow Scheduling for Data Center
Networks. In USENIX NSDI ’15.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Chen et al.
[9] Wei Bai, Li Chen, Kai Chen, Dongsu Han, Chen Tian, and Hao Wang.
2017. PIAS: Practical Information-Agnostic Flow Scheduling for Com-
modity Data Centers. IEEE/ACM Transactions on Networking (TON)
25, 4 (2017), 1954–1967.
[10] Wei Bai, Li Chen, Kai Chen, and Haitao Wu. [n. d.]. Enabling ECN in
Multi-Service Multi-Queue Data Centers. In USENIX NSDI ’16.
[11] Theophilus Benson, Aditya Akella, and David Maltz. [n. d.]. Network
Trac Characteristics of Data Centers in the Wild. In ACM IMC’10.
[12] Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and
Richard S Sutton. 2008. Incremental Natural Actor-Critic Algorithms.
(2008).
[13] Justin A Boyan, Michael L Littman, et al. 1994. Packet routing in
dynamically changing networks: A reinforcement learning approach.
Advances in neural information processing systems (1994).
[14] Li Chen, Kai Chen, Wei Bai, and Mohammad Alizadeh. [n. d.]. Sched-
uling Mix-ows in Commodity Datacenters with Karuna. In ACM
SIGCOMM ’16.
[15] Li Chen, Jiacheng Xia, Bairen Yi, and Kai Chen. [n. d.]. PowerMan: An
Out-of-Band Management Network for Datacenters Using Power Line
Communication. In USENIX NSDI’18.
[16] Yanpei Chen, Rean Grith, Junda Liu, Randy H Katz, and Anthony D
Joseph. 2009. Understanding TCP incast throughput collapse in dat-
acenter networks. The 1st ACM workshop on Research on enterprise
networking (2009).
[17] Francois Chollet. [n. d.]. Keras Documentation. https://keras.io/. ([n.
d.]). (Accessed on 04/18/2017).
[18] Mosharaf Chowdhury and Ion Stoica. [n. d.]. Ecient Coow Sched-
uling Without Prior Knowledge. In ACM SIGCOMM ’15.
[19] Mosharaf Chowdhury, Yuan Zhong, and Ion Stoica. [n. d.]. Ecient
coow scheduling with Varys. In ACM SIGCOMM ’14.
[20] Cisco. [n. d.].
ing user-dened trac patterns.
empirical-trac-gen. ([n. d.]). (Accessed on 04/24/2017).
Simple client-server application for generat-
https://github.com/datacenter/
[21] Ralph B Dell, Steve Holleran, and Rajasekhar Ramakrishnan. 2002.
Sample size determination. ILAR journal (2002).
[22] Nathan Farrington, George Porter, Sivasankar Radhakrishnan,
Hamid Hajabdolali Bazzaz, Vikram Subramanya, Yeshaiahu Fainman,
George Papen, and Amin Vahdat. [n. d.]. Helios: A Hybrid Electri-
cal/Optical Switch Architecture for Modular Data Centers. In ACM
SIGCOMM’10.
[23] Linux Foundation. [n. d.]. Priority qdisc - Linux man page. https:
//linux.die.net/man/8/tc-prio. ([n. d.]). (Accessed on 04/17/2017).
[24] Python Software Foundation. [n. d.]. Global Interpreter Lock. https:
//wiki.python.org/moin/GlobalInterpreterLock. ([n. d.]). (Accessed on
04/18/2017).
[25] Kevin Frans and Danijar Hafner. 2016. Parallel Trust Region Policy
Optimization with Multiple Actors. (2016).
[26] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and
Sudipta Sengupta. [n. d.]. VL2: A Scalable and Flexible Data Center
Network. In ACM SIGCOMM’09.
[27] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. [n.
d.]. Deep reinforcement learning for robotic manipulation with asyn-
chronous o-policy updates. In Proceedings of Robotics and Automation
(ICRA), 2017 IEEE International Conference on.
[28] W. K. Hastings. 1970. Biometrika (1970). http://www.jstor.org/stable/
[29] Chi-Yao Hong, Matthew Caesar, and P Godfrey. [n. d.]. Finishing ows
quickly with preemptive scheduling. In ACM SIGCOMM ’12.
[30] C. Hopps. 2000. Analysis of an Equal-Cost Multi-Path Algorithm. RFC
2334940
2992 (2000).
[31] Kurt Hornik. [n. d.]. Approximation Capabilities of Multilayer Feed-
forward Networks. Neural Netw., 1991 ([n. d.]).
[32] Shuihai Hu, Kai Chen, Haitao Wu, Wei Bai, Chang Lan, Hao Wang,
Hongze Zhao, and Chuanxiong Guo. 2016. Explicit path control in com-
modity data centers: Design and applications. IEEE/ACM Transactions
on Networking (2016).
[33] Srikanth Kandula, Sudipta Sengupta, Albert Greenberg, Parveen Pa-
tel, and Ronnie Chaiken. [n. d.]. The Nature of Datacenter Trac:
Measurements and Analysis. In ACM IMC’09.
[34] Russell V Lenth. 2001. Some practical guidelines for eective sample
size determination. The American Statistician (2001).
[35] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,
Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Contin-
uous control with deep reinforcement learning. CoRR abs/1509.02971
(2015). arXiv:1509.02971 http://arxiv.org/abs/1509.02971
[36] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kan-
dula. [n. d.]. Resource Management with Deep Reinforcement Learning.
In ACM HotNets ’16.
[37] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. [n. d.]. Neural
Adaptive Video Streaming with Pensieve. In ACM SIGCOMM ’17.
[38] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peter-
son, J. Rexford, S. Shenker, and J. Turner. 2008. OpenFlow: Enabling
innovation in campus networks. ACM CCR (2008).
[39] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray
Kavukcuoglu. [n. d.]. Asynchronous methods for deep reinforcement
learning. In ICML’16.
[40] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioan-
nis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing
atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602
(2013).
[41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioan-
nis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing
atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602
(2013).
[42] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov,
Richard Liaw, Eric Liang, William Paul, Michael I Jordan, and Ion Stoica.
2017. Ray: A Distributed Framework for Emerging AI Applications.
arXiv preprint arXiv:1712.05889 (2017).
[43] Ali Munir, Ihsan A Qazi, Zartash A Uzmi, Aisha Mushtaq, Saad N
Ismail, M Safdar Iqbal, and Basma Khan. [n. d.]. Minimizing ow
completion times in data centers. In IEEE INFOCOM ’13.
[44] Netlter.Org. [n. d.]. The netlter.org project. https://www.netlter.
org/. ([n. d.]). (Accessed on 04/17/2017).
[45] NVIDIA. [n. d.]. Deep Learning Frameworks. https://developer.nvidia.
com/deep-learning-frameworks. ([n. d.]). (Accessed on 04/18/2017).
[46] NVlabs. [n. d.]. Hybrid CPU/GPU implementation of the A3C algo-
rithm for deep reinforcement learning. https://github.com/NVlabs/
GA3C. ([n. d.]). (Accessed on 06/13/2018).
[47] OpenAI. [n. d.]. OpenAI Gym. https://gym.openai.com/. ([n. d.]).
(Accessed on 04/24/2017).
2017. Pytorch. (2017).
[48] Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan.
[49] Yang Peng, Kai Chen, Guohui Wang, Wei Bai, Ma Zhiqiang, and Lin Gu.
[n. d.]. HadoopWatch: A First Step Towards Comprehensive Trac
Forcecasting in Cloud Computing. In IEEE INCOCOM ’14.
[50] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C
Snoeren. [n. d.]. Inside the Social Network’s (Datacenter) Network. In
ACM SIGCOMM’15.
[51] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and
Pieter Abbeel. 2015. Trust Region Policy Optimization. CoRR (2015).
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
AuTO: Scaling Deep Reinforcement Learning
for Datacenter-Scale Automatic Traic Optimization
[52] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game
of Go with deep neural networks and tree search. Nature (2016).
[53] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
and Martin Riedmiller. 2014. Deterministic Policy Gradient Algorithms.
In Proceedings of the 31st International Conference on International
Conference on Machine Learning - Volume 32 (ICML’14). JMLR.org,
I–387–I–395. http://dl.acm.org/citation.cfm?id=3044805.3044850
[54] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armis-
tead, Roy Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie
Germano, et al. [n. d.]. Jupiter Rising: A Decade of Clos Topologies
and Centralized Control in Google’s Datacenter Network. In ACM
SIGCOMM’15.
[55] Richard S. Sutton and Andrew G. Barto. 1998. Introduction to Rein-
forcement Learning.
[56] Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay
Mansour. 2012. Policy Gradient Methods for Reinforcement Learning
with Function Approximation. (2012).
[57] TensorFlow. [n. d.]. API Documentation: TensorFlow. https://www.
tensorow.org/api_docs/. ([n. d.]). (Accessed on 04/18/2017).
[58] Matt Welsh, David Culler, and Eric Brewer. [n. d.]. SEDA: An Ar-
chitecture for Well-conditioned, Scalable Internet Services. In SOSP
’01.
[59] Keith Winstein and Hari Balakrishnan. [n. d.]. Tcp ex machina:
Computer-generated congestion control. In ACM SIGCOMM ’13.
[60] Neeraja J. Yadwadkar, Ganesh Ananthanarayanan, and Randy Katz. [n.
d.]. Wrangler: Predictable and Faster Jobs Using Fewer Resources. In
SOCC ’14.
[61] Hong Zhang, Li Chen, Bairen Yi, Kai Chen, Mosharaf Chowdhury, and
Yanhui Geng. [n. d.]. CODA: Toward automatically identifying and
scheduling coows in the dark. In ACM SIGCOMM ’16.