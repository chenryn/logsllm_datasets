negligible  variation  in  the  model  performance  was  due  to  the 
random partitioning in cross-validation. 
Figure 5: Variation of Bayesian Network in cross-validation 
The  results  of  our  predictability  analysis  show  that  the  four 
developer activity metrics can be used to predict vulnerable files, 
but not all of the vulnerable files. This conclusion is a logical one: 
even if the models were perfect, we have no way of knowing if 
every  vulnerable  file  was  vulnerable  because  of  poor  developer 
activity.  
5.4  Discussion 
Our results show a statistically significant correlation in all four 
developer activity metrics. Three of the four metrics support the 
notion of unfocused contributions, specifically that the vulnerable 
files  were  undergoing  a  lot of change (NumCommits), by many 
developers  (NumDevs),  and  by  developers  who  were  also 
working on many other files (CNBetweenness). 
When  examining  unfocused  contributions,  one  might  consider 
what  Frederick  Brooks  says  about  too  many  developers  on  a 
project: 
 “If  each  part  of  the  task  must  be  separately  coordinated 
with 
increases  as  
n(n-1)/2.”[3]  
each  other  part, 
effort 
the 
in 
In  context,  Brooks  is  specifically  discussing  Brook’s  Law, 
“Adding  manpower  to  a  late  software  project  makes  it  later”.  
However  the  reasoning  bears  resemblance  to  the  unfocused 
contribution  argument  in  that  coordinating  a  large  number  of 
developers  can  require  a  communication  and coordination effort 
quadratic 
the  number  of  developers.  While  unfocused 
contributions  do  not  necessarily  negate  Linus’  Law,  our  results 
show that they are a legitimate opposing force. 
As far as diversity in perspectives, more developers changing the 
code  does  not  mean  an  increase  in  security.  One  interesting 
observation is in the NumDevs chart in Figure 4: the proportion of 
vulnerabilities  steadily  increases  as  the  number  of  developers 
increases (until about nine when the recall drops). However, files 
worked  on  by  disparate,  otherwise-separated  clusters  of 
developers are more likely to have a vulnerability.  
One  factor  to  consider  when  evaluating  software  security  and 
Linus’  Law  is  taking  ongoing  code  change  into  account.  Many 
open  source  projects  are  constantly  changing  and  evolving  (the 
Linux  kernel  is  no  exception).  The  developer  community  must 
continually keep up with finding vulnerabilities and fortification 
efforts in an ever-changing, ever-branching project.  
4606.  LIMITATIONS 
All of our developer activity metrics require version control data, 
and therefore change in the system. For developer networks, if a 
file has no commits to it during the period of study, the file has no 
developers  in  its  history  and  therefore  no  measurement  can  be 
made. In our case study, all of the vulnerabilities happened to be 
in  files  that  were  changed  15  months  prior  to  release  (our  time 
period under study). A vulnerability could be in a file that was not 
changed,  but  since  our  case  study  had  no  instance  of  that 
situation, this effect did not have an impact on our results.  
Also,  we  cannot  claim  that  developer  activity  metrics  cause 
vulnerabilities.  Studies  of  historical  data  can  only  show  a 
statistically  significant  correlation.  Proving  causation  would 
require  a  controlled  experiment,  which  is  not  feasible  for  an 
ongoing project like the Linux kernel. Furthermore, although we 
used the term “validation criteria”, we do not consider developer 
activity  metrics  to  be  fully  validated  until  further  case  studies 
support consistent, repeatable results. 
Since  our  data  only  includes  known  vulnerabilities,  we  cannot 
make any claims about latent, undiscovered vulnerabilities. As a 
result  of  the  latent,  undiscovered  vulnerabilities,  we  cannot  say 
that a low precision (i.e. a high occurrence of false positives) is 
actually  indicative  of  real  false  positives,  or  that  our  model  is 
finding more vulnerabilities in the system that have not yet been 
confirmed. 
7.  RELATED WORK 
The topics of developers and collaboration have been examined in 
several  recent  empirical  studies.  All  of  the  studies,  however, 
either examine the meaning of developer activity metrics or relate 
them  to  reliability.  Only  one  of  the  studies  relate  developer 
activity metrics to security.  
Shin  et  al.  [21]  evaluated  the  statistical  connection  between 
vulnerabilities  and  metrics  of  complexity,  code  churn,  and 
developer  activity.  The  study  denotes  two  case  studies  of  large, 
open source projects: multiple releases of Mozilla Firefox and the 
RHEL4  kernel.  Among  the  findings  include  a  statistically 
significant correlation between metrics of all three categories and 
security  vulnerabilities.  Also,  in  the  Mozilla  project,  a  model 
containing all three types of metrics was able to find 70.8% of the 
known  vulnerabilities  by  selecting  only  10.9%  of  the  project’s 
files.  The  study  examines  a  different  collection  of  metrics  than 
this study and combines disparate metrics into a single model. 
Meneely et al., [13] examined the relationship between developer 
activity  metrics  and  reliability.  The  empirical  case  study 
examined  three  releases  of  a  large,  proprietary  networking 
product.  The  authors  used  developer  centrality  metrics  from  the 
developer  network  to  examine  whether  files  are  more  likely  to 
have  failures  if  they  were  changed  by  developers  who  are 
peripheral  to  the  network.  The  authors  formed  a  model  that 
included metrics of developer centrality, code churn (the degree 
to which a file was changed recently), and lines of code to predict 
failures from one release to the next. Their model’s prioritization 
found 58% of the system’s failures in 20% of the files, where a 
perfect  prioritization  would  have  found  61%.  The  study  did  not 
include  work  on  developer  clusters,  unfocused  contributions,  or 
security. 
Bird et al. [1] uses a similar approach to ours with the purpose of 
examining  social  structures  in  open  source  projects.  Also 
discussing  connections  and  contradictions  between  some  of 
Brooks’s  ideas  [3]  and  the  bazaar-like  development  of  open 
source projects, the authors empirically examine how open source 
developers  self-organize.  The  authors  use  similar  network 
structures as our developer network to find the presence of sub-
communities  within  open  source  projects.  In  addition 
to 
examining version control change logs, the authors mined email 
logs and other artifacts of several open source projects to find a 
community structure. The authors conclude that sub-communities 
do  exist  in  open  source  projects,  as  evidenced  by  the  project 
artifacts  exhibiting  a  social  network  structure  that  resembles 
collaboration  networks  in  other  disciplines.  In  our  study,  we 
leverage  network  analysis  metrics  as  an  estimation  of 
collaboration  and  examine their relationship to vulnerabilities in 
the project.  
Pinzger  et  al.  [17]  were  the  first  to  propose  the  contribution 
network.  The  contribution  network  is  designed  to  use  version 
control  data  to  quantify  the  direct  and  indirect  contribution  of 
developers  on  specific  resources  of  the  project.  The  researchers 
used  metrics  of  centrality  in  their  study  of  Microsoft  Windows 
Vista and found that closeness was the most significant metric for 
predicting  reliability  failures.  Files  that  were  contributed  to  by 
many  developers,  especially  by  developers  who  were  making 
many different contributions themselves, were found to be more 
failure-prone  than  files  developed  in  relative  isolation.  The 
finding  is  that  files  which  are  being  focused  on  by  a  few 
developers  are  less  problematic  than  files  developed  by  many 
developers. In our study, we use centrality metrics on contribution 
networks to predict vulnerabilities in files. 
Gonzales-Barahona  and  Lopez-Fernandez  [6]  were  the  first  to 
propose  the  idea  of  creating  developer  networks  as  models  of 
collaboration from source repositories. The authors’ objective was 
to  present  the  developer  network  and  to  differentiate  and 
characterize projects.  
Nagappan  et  al.  [15]  created  a  logistic  regression  model  for 
failures in the Windows Vista operating system. The model was 
based  on  what  they  called  “Overall  Organizational  Ownership” 
(OOW).  The  metrics 
like 
organizational cohesiveness and diverse contributions. Among the 
findings  is  that  more  edits  made  by  many,  non-cohesive 
developers leads to more problems post-release. The OOW model 
was able to predict with 87% average precision and 84% average 
recall. The OOW model bears a resemblance to the contribution 
network  in  that  both  models  attempt  to  differentiate  healthy 
changes in software from the problematic changes. 
8.  SUMMARY 
The objective of this research is to reduce security vulnerabilities 
by  providing  actionable  insight  into  the  structural  nature  of 
developer collaboration in open source software. Within our case 
study  of  the  RHEL4  kernel,  we  found  four  metrics  that 
empirically  support  the  notions  of  Linus’  Law  and  unfocused 
contributions. An empirical analysis of our data demonstrates the 
following observations:  
(a)  source  code  files  changed  by  multiple,  otherwise-separated 
clusters  of  developers  are  more  likely  to  be  vulnerable  than 
changed by a single cluster; and 
 (b)  files  are  likely  to  be  vulnerable  when  changed  by  many 
developers who have made many changes to other files. 
Practitioners  can  use  these  observations  to  prioritize  security 
fortification efforts or to consider organizational changes among 
included  concepts 
for  OOW 
461developers.  While  the  results  are  statistically  significant,  the 
individual correlations indicate that developer activity metrics are 
likely  to  perform  best  for  prediction  in  the  presence  of  other 
metrics. 
9.  ACKNOWLEDGMENTS 
We thank Mark Cox and the Realsearch group for their valuable 
support.    This  work  was  supported  by  the  U.S.  Army  Research 
Office  (ARO)  under  grant  W911NF-08-1-0105  managed  by 
NCSU Secure Open Systems Initiative (SOSI).   
10.  REFERENCES 
[1]  C. Bird, D. Pattison, R. D'Souza et al., "Latent Social 
Structures in Open Source Projects," in FSE, Atlanta, GA, 
2008, p. p24-36. 
[2]  U. Brandes, and T. Erlebach, Network Analysis: 
Methodological Foundations, Berlin: Springer, 2005. 
[3]  F. Brooks, The mythical man-month: Addison-Wesley, 1995. 
[4]  A. Endres, and D. Rombach, A Handbook of Software and 
Systems Engineering: Empirical Observations, Laws and 
Theories: Addison Wesley, 2003. 
[5]  M. Girvan, and M. E. J. Newman, "Community Structure in 
Social and Biological Networks," The Proceedings of the 
National Academy of Sciences, vol. 99, no. 12, p. 7821-
7826, 2001. 
[6]  J. M. Gonzales-Barahona, L. Lopez-Fernandez, and G. 
Robles, "Applying Social Network Analysis to the 
Information in CVS Repositories," in 2005 Mining Software 
Repositories, Edinburgh, Scotland, United Kingdom, 2004, 
p. 
[7]  J.-H. Hoepman, and B. Jacobs, "Increased security through 
open source," Commun. ACM, vol. 50, no. 1, p. 79-83, 2007. 
[8]  ISO, ISO/IEC DIS 14598-1 Information Technology - 
Software Product Evaluation, 1996. 
[9]  M. M. Lehman, and L. Belady, Program Evolution: 
Processes of Software Change, London: Academic Press, 
1985. 
[10] M. M. Lehman, and J. F. Ramil, "Rules and Tools for 
Software Evolution Planning and Management," Annals of 
Software Engineering, vol. 11, no. 1, p. 15-44, 2001. 
[11] M. M. Lehman, J. F. Ramil, P. D. Wernick et al., "Metrics 
and Laws of Software Evolution -- The Nineties View," in 
4th International Software Metrics Symposium (METRICS 
'97), Albuquerque, NM, 1997, p. 20-32. 
[12] A. M. Martinez, and A. C. Kak, "PCA versus LDA," IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 
vol. 23, no. 2, p. 228-233, 2001. 
[13] A. Meneely, L. Williams, J. Osborne et al., "Predicting 
Failures with Developer Networks and Social Network 
Analysis  " in Foundations in Software Engineering, Atlanta, 
GA, 2008, p. to appear. 
[14] N. Nagappan, and T. Ball, "Use of Relative Code Churn 
Measures to Predict System Defect Density," in 27th 
International Conference on Software Engineering, St. Louis, 
MO, USA, 2005, p. 284-292. 
[15] N. Nagappan, B. Murphy, and V. R. Basili, "The Influence 
of Organizational Structure on Software Quality," in 
International Conference on Software Engineering, Leipzig, 
Germany, 2008, p. 521-530. 
[16] K. Numata, S. Imoto, and S. Miyano, "A Structure Learning 
Algorithm for Inference of Gene Networks from Microarray 
Gene Expression Data Using Bayesian Networks," in 
Bioinformatics and Bioengineering, 2007. BIBE 2007., p. 
1280-1284. 
[17] M. Pinzger, N. Nagappan, and B. Murphy, "Can Developer-
Module Networks Predict Failures?," in Foundations in 
Software Engineering, Atlanta, GA, 2008, p. 2-12. 
[18] M. Pinzger, N. Nagappan, and B. Murphy, "Can Developer-
Module Networks Predict Failures?," in Foundations in 
Software Engineering, Atlanta, GA, 2008, p. to appear. 
[19] E. S. Raymond, The Cathedral and the Bazaar: Musings on 
Linux and Open Source by an Accidental Revolutionary, 
Sebastopol, California: O'Reilly and Associates, 1999. 
[20] N. F. Schneidewind, "Methodology For Validating Software 
Metrics," IEEE Transactions on Software Engineering, vol. 
18, no. 5, p. 410-422, 1992. 
[21] Y. Shin, A. Meneely, L. Williams et al., "Evaluating 
Complexity, Code Churn, and Developer Activity Metrics as 
Indicators of Software Vulnerabilities," NCSU CSC 
Technical Report TR-2009-10, submitted to IEEE TSE. 
[22] K. Tae-Kyun, and J. Kittler, "Locally linear discriminant 
analysis for multimodally distributed classes for face 
recognition with a single model image," Pattern Analysis and 
Machine Intelligence, IEEE Transactions on, vol. 27, no. 3, 
p. 318-327, 2005. 
[23] B. Witten, C. Landwehr, and M. Caloyannides, "Does Open 
Source Improve System Security?," IEEE Softw., vol. 18, 
no. 5, p. 57-61, 2001. 
[24] I. H. Witten, and E. Frank, Data Mining: Practical machine 
learning tools and techniques, 2 ed., San Francisco: Morgan 
Kaufmann, 2005. 
462