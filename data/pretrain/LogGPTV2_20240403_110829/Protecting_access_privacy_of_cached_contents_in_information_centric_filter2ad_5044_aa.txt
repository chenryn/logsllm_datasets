title:Protecting access privacy of cached contents in information centric
networks
author:Abedelaziz Mohaisen and
Xinwen Zhang and
Max Schuchard and
Haiyong Xie and
Yongdae Kim
POSTER: Protecting Access Privacy of Cached Contents
in Information Centric Networks∗
Abedelaziz Mohaisen
Verisign Labs, VA, USA
Xinwen Zhang
Max Schuchard
Huawei Technologies, CA, USA
University of Minnesota, MN, USA
Haiyong Xie
Huawei Technologies and USTC, China
and Yongdae Kim
KAIST, Korea
ABSTRACT
In information centric network (ICN), contents are fetched by their
names from caches deployed in the network or from origin servers.
Once the contents are fetched from the origin server, it is repli-
cated and cached in all routers along the routing and forwarding
paths from the user that issues the interest to the origin server, thus
allowing further “interests” by other users to be fulﬁlled quickly.
However, the way ICN caching and interest fulﬁllment work pose
a great privacy risk; the time difference between response for inter-
est of cached and uncached contents can be used as an indicator to
infer whether or not a near-by user previously requested the same
contents requested by the adversary. This work introduces the ex-
tent to which the problem is applicable in ICN and provides several
solutions to address it.
Categories and Subject Descriptors: C.2.0 COMPUTER COM-
MUNICATION NETWORKS: Security and protection
Keywords: Information centric networks, Privacy, Caching.
1.
INTRODUCTION
In information centric networks (ICN) [1], contents are fetched
by their names from caches deployed in the network or from ori-
gin servers—servers that serve contents if they are not cached in
the network. Once contents are fetched from an origin server, they
are replicated and cached in all routers along the routing and for-
warding paths from the user that issues the interest to the origin
server, thus allowing further interests to be fulﬁlled quickly. For
example, when another user issues an interest in these contents that
have been previously served to a user on the same path, the interest
is fulﬁlled from the near-by cache. However, the way ICN interest
fulﬁllments work pose a great privacy risk. In particular, the time
difference between response for interest of cached and uncached
contents can be used as a side channel to infer whether a near-by
user previously requested the same contents as the adversary.
Consider the topology in Figure 1, which depicts two users U1
and U2, and a set of routers r0 to r4 (each has own cache) connect-
ing both users to an origin server that holds some contents. Suppose
that user U2 is the adversary, whereas user U1 is honest. If U1 is-
sues an interest in contents N that reside behind r0, the interest
should traverse the path U1 → r3 → r2 → r1 → r0 from which
it retrieves the contents requested. The contents are then sent back
over the returning path r0 → r1 → r2 → r3 → U1. In total,
the path from U1 to the source of the contents and the returning
path to U1 have four hops each. The total round trip time required
∗Haiyong Xie is supported in part by the NSFC Grant No.
61073192 and by the 973 Program Grant No. 2011CB302905.
Copyright is held by the author/owner(s).
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
ACM 978-1-4503-1651-4/12/10.
origin server
r0
r1
r2
t′′
1
r4
U2
t2
t′
1
r3
U1
Figure 1: Toy example of the timing attack. t1 = t′
1 + t′′
1
for sending the request until starting to receive contents on the re-
turning path is t1. On the other hand, if U2 is to request the same
contents by its name, N , the path that the interest would traverse
is U2 → r4 → r2, and the contents would return on the reversed
path (r2 → r4 → U2), which is two-hop in each direction, and
would require a time t2. Obviously, the time t1 is a greater than t2,
which an adversary, like the user U2, can use to infer that user U1
has accessed the contents N .
Although pinpointing U1 in real settings precisely may require
additional side information [3], an attack like the one described
above—which ﬁnds out a 1-hop away user without naming him—
is still critical since it reduces the anonymity set of that user greatly.
In this work, we examine the timing attacks on privacy in ICN
caching and propose three solutions that come at varying levels
of costs and complexity. We rely on randomly generated time
paddings to disguise responses for interests issued from the same
domain, thus increasing the anonymity set of privacy-related con-
tents’ interest issuer. While we disclaim the novelty of the attack—
shown in other context in [4], we are the ﬁrst to observe its inherent
applicability to ICN and to provide solutions and mitigations.
2. PRELIMINARIES AND TERMINOLOGY
In ICN, contents are fetched by their names [1]. An ICN con-
sists of routers, where each router has a cache, and edge routers are
connected to users and origin servers. Interest in ICN encapsulate
requests for contents by their names. An origin server is a server
that originates contents to be served in the network, thus fulﬁlling
interests. The contents may or may not be cached in the network.
In the rest of this work, we use Round Trip Time (RTT) to denote
the time from the start of sending an interest until the start of re-
ceiving contents fulﬁlling it. In ICN, contents are forwarded back
to users on the same path they are requested by that user, thus PIT
(pending interest table) at each ICN router records which interest
is not fulﬁlled yet. A face in ICN is the port at which data is sent
or received in the router. In one of our protocols we make use of an
access point (AP), which is the closest connecting point of the user
to the ICN (not to be confused with wireless access point). ﬂist and
ulist are lists of faces and users, while rtimes is a list of the num-
ber of times the contents are fetched by each (either face or user).
pmode is a ﬂag to indicate that the privacy of the contents being
accessed need to be preserved in future access and requests.
10013. PROTECTION MECHANISMS
Simple solutions cannot prevent the attack, although they can
greatly degrade the beneﬁts of ICN. For example, if the edge router
always generates an equal delay to the RTT from that router to
the origin sever, two in-row requests by an adversary would reveal
whether the requested contents are ﬂagged private or not by others.
The ﬁrst technique to address the attack enables each user con-
cerned about the privacy of his access to contents to use a privacy
mode, upon which the edge router through which the interest is
served (or propagated to the network) maintain a state of the user,
the requested contents’ name, and the number of times the user re-
quested it. When other users request the same contents for the ﬁrst
time, the router generates random delay to simulate a network delay
after which sends the contents to the requester. This technique re-
quires keeping states users requesting privacy-related contents and
their times of requests, which represents an overhead. On the other
hand, this solution can be tuned to maintain shorter RTT as in ICN.
To reduce overhead at edge routers and to enable inter-domain
privacy, we let routers only keep states for requests coming on
faces. When an interest of cached contents arrives for the ﬁrst time
at a certain face, the edge router generates random delay and serves
the contents so that to preserve the privacy of other users in other
domains, who requested the contents before. When a face has pre-
vious requests, the contents are served to the requester immediately.
Although this technique reduces the overhead of the ﬁrst technique,
it does not enable intra-domain privacy preservation.
To enable low granularity of the privacy and to reduce the over-
head at edge routers, we maintain the same states of users as in ﬁrst
solution but in the access points. We then use these states to collab-
oratively tell routers if contents have been requested before by the
same user or not. In the following we explain the three protocols.
Before going into the details of the protocols, we ﬁrst introduce
the time (delay) generation procedure. The procedure is performed
by an edge router, and takes several parameters based on the spe-
ciﬁc protocol in which it used to generate td, the number of hops
to be added as noise to prevent the timing attack. Particularly, for
a content name n ∈ N , the total number of hops h, RTT tdx, and
the time delay for the ﬁrst hop td0, td(n) is chosen as follows; for
a given n, the same value of td(n) is used for subsequent requests.
td(n) = (cid:26) 0
2td0  1
(1)
3.1 The “Vanilla” Approach
The protocol is described in algorithm 1. The main ingredient
of the protocol is a carefully chosen delay added to subsequent re-
sponses to make them similar to responses that fall back on the ori-
gin servers to ensure that the contents that are sent to an adversary
do not expose timing pattern. For that, the protocol relies on states
stored by each router. Particularly, for a user u (U1 in Figure 1), its
edge router (r2 in Figure 1) maintains ϕ(u, n) : U × N → IN T ,
where U , N , and IN T are the sets of users, content names, and
integers, respectively. ϕ(u, n) indicates the number of times that
user u has accessed the content name n.
3.2 An Efﬁcient Approach
This protocol is in algorithm 2. The main idea of this protocol
is to reduce the states stored in each router to that of faces and
the number of requests that have been served to users over each
face, rather than maintaining a large number of states per user.
The main observation made in this protocol is that interests from
different domains (or sub-domains) traverse different faces at the
edge router while interests coming from the same domain (or sub-
domain) would traverse the same face at the edge router. To that
Algorithm 1: The “vanilla” approach.
Input: n - a content name, u - a user, ϕ - access state,
Ints = (u, n, pmode, ts0)
Output: A data packet to u in a privacy-preserving manner.
1 When R receives Ints from u, it records ts1, the timestamp of
interest arrival, and computes td0 = ts1 − ts0 as a one-hop
time delay.
2 if pmode == 0 then
3
4
5
6
7
8
9
10
if td(n) == 0 then
// default value td(n) = 0
R follows ICN protocol to obtain Data;
R returns Data to u;
else
R follows ICN protocol to obtain data packet Data;
R delays td(n);
R returns Data to u;
end
11
12 else
if ϕ(u, n) == 0 then
R follows the ICN protocol to obtain Data;
R records ts2 upon the arrival of Data, and computes:
tdx = ts2 − ts1; // RTT from R to origin server
h = tdx/(2td0) + 1; // expected # of hops from u
to the origin server
Generate td(n) according to Eq. 1;
ϕ(u, n) + +;
R returns retrieved Data to u;
R returns cached Data to u;
else
end
13
14
15
16
17
18