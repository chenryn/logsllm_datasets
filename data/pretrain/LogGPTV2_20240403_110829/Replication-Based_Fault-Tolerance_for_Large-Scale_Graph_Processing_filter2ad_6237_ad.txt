3.55X) for Rebirth and Migration approaches accord-
ingly. Overall, Imitator can recover 0.95 million and
1.43 million vertices (including replicas) from one failed
node in just 2.32 and 3.4 seconds for LJournal and Wiki
dataset accordingly.
For large graphs (e.g., LJournal and Wiki), the per-
formance of Migration is relatively better than that of
Rebirth, since it avoids data movement (e.g., vertex
and edge values) in the reloading phase and distributes
replaying operations to all surviving nodes rather than
on the single new node. On the other hand, for small
graphs (e.g., SYN-GL and DBLP),
the performance
of Rebirth is relative better than that of Migration,
since there are multiple rounds of message exchanges in
Migration. This causes slowdown to recovery, ranging
from 28% to 63%, compared with Rebirth.
6.6 Scalability of Recovery
We evaluate the recovery scalability of Imitator for
PageRank with the Wiki dataset using different numbers
of nodes that participate in recovery. As shown in
Fig. 9, both recovery modes scale with the increase of
recovery machines, since all machines can participate
the workload in the reloading phase. Because the local
graph has been constructed in the reloading phase, there
571
Fig. 9: The recovery time of Rebirth (a) and Migrate (b) on
Imitator for PageRank with the increase of nodes.
r
o
t
c
a
F
n
o
i
t
a
c
i
l
p
e
R
 12
 10
 8
 6
 4
 2
 0
Hash
Fennel
GWeb
LJournal
Wiki
d
a
e
h
r
e
v
O
d
e
z
i
l
a
m
r
o
N
25%
20%
15%
10%
5%
0%
GWeb
LJournal
Wiki
#Replicas
#Msgs
Runtime
Fig. 10: (a) The replication factor of different partitioning
schemes. (b) The overhead of Imitator using Fennel algorithm.
is no explicit reconstruction phase for the Rebirth mode.
Further, the replay operations are only executed in new
node for the Rebirth mode, while are distributed to all
surviving nodes for the Migration mode.
6.7 Impact of Graph Partitioning
To analyze the impact of graph partitioning algorithms,
we implement Fennel [29] on Imitator, which is a
heuristic graph partitioning algorithm. As shown in
Fig. 10(a), compared to the default Hash-based parti-
tioning, Fennel signiﬁcantly decreases the replication
factor for all datasets, reaching 1.61, 3.84 and 5.09 for
GWeb, LJournal and Wiki respectively.
Fig. 10(b) illustrates the overhead of Imitator under
Fennel partitioning. Due to lower replication factor,
Imitator requires more additional replicas for fault tol-
erance, which also result in the increase of message
overhead. However, the runtime overhead is still small,
ranging from 1.8% to 4.7%.
6.8 Handling Multiple Failures
When Imitator is conﬁgured to tolerate multiple node
failures, there will be more extra replicas to add. The
overhead tends to be larger. Fig. 11 shows the overall
overhead when Imitator is conﬁgured to tolerate 1, 2 and
3 node failure(s). As shown in Fig. 11(a), the overhead
of Imitator is less than 10% even when it is conﬁgured
to tolerate 3 nodes failures simultaneously.
Fig. 11(b) shows the recovery time of the largest
dataset, Wiki, when different numbers of nodes crashed.
In Rebirth mode, since the surviving nodes need more
messages to exchange when the crashed nodes increase,
the time to send and receive recovery messages in-
creases. However, the time to rebuild graph states and
replay some pending operations is almost the same as
that of a single node failure. Since Migration strategy
harnesses the cluster resource for recovery, the time of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
d
a
e
h
r
e
v
O
d
e
z
i
l
a
m
r
o
N
110%
105%
100%
95%
G
W
Three
Two
One
)
c
e
S
(
i
e
m
T
r
e
v
o
c
e
R
 30
 20
 10
 0
W
S
D
R
Replay
Reconstruct
Reload
Rebirth
Migration
 20
s
n
o
 15
i
t
a
r
e
t
I
f
o
 10
r
e
b
m
u
N
 5
 0
 0
Crash
Detect
 25
 50
BASE
CKPT/4
REP
CKPT/4 + 1 Failure
REP(Rebirth) + 1 Failure
REP(Migration) + 1 Failure
Replay
 75
 100
 125
 150
 175
Execution Time (Sec)
L
J
o
e
b
iki
u
r
n
al
Y
B
N
-
G
L
P
L
o
a
d
C
A
Fig. 11: (a) The runtime overhead, and (b) The recovery time
for tolerating 1, 2, and 3 machine(s) failure.
TABLE 3: Memory and GC behavior of Imitator with different
fault tolerance setting for PageRank on Wiki
Conﬁg
w/o FT
FT/1
FT/2
FT/3
Max
Max
Young/Full GC
Cap(GB)
Usage(GB)
Number
Time (Sec))
3.85
5.05
6.24
6.99
2.76
3.70
4.51
4.91
40/15
50/29
55/29
58/30
13.7/13.4
19.9/21.7
23.6/26.1
25.7/29.7
every operation in Migration is relatively small.
6.9 Memory Consumption
As Imitator needs to add extra replicas to tolerate faults,
we also measure the memory overhead. We use jstat, a
memory tool in JDK, to monitor the memory behavior
of the baseline system and Imitator. Table 3 illustrates
the result of one node of the baseline system and Imita-
tor on our largest dataset Wiki. If Imitator is conﬁgured
to tolerate one node failure during computation, the
memory overhead is modest, the memory usages of the
baseline system and Imitator is comparable.
6.10 Case Study
Fig. 12 presents a case study for running PageRank
using LJournal dataset with none or one machine failure
during the execution of 20 iterations. Different recovery
strategies are applied to illustrate their performance.
The symbols, BASE, REP, and CKPT/4, denote the
execution of the baseline, replication and checkpoint-
based fault tolerance systems without failure accord-
ingly, where others illustrate the cases with a failure
between the 6th and 7th iterations. Note that the interval
of checkpointing is 4 iterations.
The scheme of failure detection is the same for all
strategies, of which the time span is about 7 seconds.
For the recovery speed, the Migration strategy, of which
recovery time is about 2.6 seconds, is the fastest due to
the fact that it harnesses all resources and minimizes
data movements. The Rebirth strategy has a time span
of 8.8 seconds. This still outperforms the 45 seconds
recovery time of CKPT/4, which does the incremental
checkpoint with an interval of four iterations, due to
fully exploiting network resources and without access-
ing distributed ﬁle system.
After the recovery has ﬁnished, REP with Rebirth
Fig. 12: An execution of PageRank of LJournal with different
fault tolerance settings. One failure occurs between the 6th
iteration and the 7th iteration.
can still execute at full speed, since the execution envi-
ronment before and after the failure is the same in this
approach. On the other hand, the REP with Migration
is slower since the available computing resource has
decreased, but only slightly. For the CKPT/4, it still has
to replay 2 lost iterations after a long time recovery.
7 Related Work
Checkpoint-based fault
tolerance is widely used in
graph-parallel computation systems. Pregel [5] and its
open-source clones [8], [9] adopt synchronous check-
point to save the graph state to the persistent storage, in-
cluding vertex and edge values, and incoming messages.
GraphLab [11] designs an asynchronous alternative
based on the Chandy-Lamport [13] snapshot to achieve
fault tolerance. Trinity [10] and PowerGraph [12] pro-
vide both synchronous and asynchronous checkpointing
for different modes.
Piccolo [30] is a data-centric distributed computation
system, which provides user-assisted checkpoint mech-
anism to reduce runtime overhead. However, user needs
to save additional information for recovery. MapRe-
duce [22] and other data-parallel models [31] adopt
simple re-execution to recover tasks on crashed ma-
chines, since they suppose all tasks are deterministic and
independent. Graph-parallel models do not satisfy such
assumptions. Spark [23] and Discretized Streams [32]
propose a fault tolerant abstraction, namely Resilient
Distributed Datasets (RDD), for coarse-grained opera-
tions on datasets, which only logs the transformation
used to build a dataset (lineages) rather than the actual
data. It is hard to apply RDD to graph-parallel models,
since the computation on vertex is a ﬁne-grained update.
Replication is widely used in large-scale distributed
ﬁle systems [24], [20] and streaming systems [33],
[34] to provide high availability and fault tolerance.
In these systems, all replicas are full-time for fault
tolerance, which may introduce high performance cost.
RAMCloud [15] is a DRAM-based storage system, it
achieves a fast recovery from crashes by scattering its
backup data across the entire cluster and harnessing all
resources of cluster to recover the crashed nodes. Dis-
tributed storage only provides simple abstraction (e.g., a
key-value) and does not consider data dependency and
572
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
computation on data.
SPAR [14] is a graph-structured middleware to store
social data for key-value stores. It also brieﬂy men-
tions of storing more ghost vertices for fault tolerance.
However, it does not consider the interaction among
vertices, and only provides background synchronization
and eventual consistency between master and replicas,
which does not ﬁt for graph-parallel systems.
8 Conclusion
This paper presented a replication-based approach
called Imitator to provide low-overhead fault tolerance
and fast crash recovery. The key idea of Imitator is
leveraging and extending existing replication mecha-
nism with additional mirrors and complete states as the
master vertices, such that vertices in a failed machine
can be reconstructed using states from its mirrors. Eval-
uation showed that Imitator incurs very small normal
execution overhead, and provides fast crash recovery
from failures.
9 Acknowledgment
We thank the anonymous reviewers for their insightful
comments. This work is supported in part by Doctoral
Fund of Ministry of Education of China (Grant No.
20130073120040), the Program for New Century Ex-
cellent Talents in University of Ministry of Education
of China, Shanghai Science and Technology Develop-
ment Funds (No. 12QA1401700), a foundation for the
Author of National Excellent Doctoral Dissertation of
PR China, China National Natural Science Foundation
(No. 61303011) and Singapore NRF (CREATE E2S2).
References
[1]
[2]
[3]
S. Brin and L. Page, “The anatomy of a large-scale hypertextual
web search engine,” in WWW, 1998, pp. 107–117.
J. Ye, J. Chow, J. Chen, and Z. Zheng, “Stochastic gradient
boosted distributed decision trees,” in ACM CIKM, 2009, pp.
2061–2064.
J. E. Gonzalez, Y. Low, C. Guestrin, and D. O’Hallaron,
“Distributed parallel inference on large factor graphs,” in Proc.
Conference on Uncertainty in Artiﬁcial Intelligence, 2009, pp.
203–212.
[4] A. Smola and S. Narayanamurthy, “An architecture for parallel
topic models,” Proceedings of the VLDB Endowment, vol. 3, no.
1-2, pp. 703–710, 2010.
[5] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn,
N. Leiser, and G. Czajkowski, “Pregel: a system for large-scale
graph processing,” in SIGMOD, 2010.
[7]
[6] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S.
Corrado, J. Dean, and A. Y. Ng, “Building high-level features
using large scale unsupervised learning,” in Proc. ICML, 2012.
J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le,
M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng,
“Large scale distributed deep networks,” in Proc. NIPS, 2012,
pp. 1232–1240.
“Apache Giraph,” http://giraph.apache.org/.
“Apache Hama,” http://hama.apache.org/.
[8]
[9]
[10] B. Shao, H. Wang, and Y. Li, “Trinity: A distributed graph
engine on a memory cloud,” in Proc.SIGMOD, 2013.
573
[11] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola,
and J. M. Hellerstein, “Distributed GraphLab: a framework for
machine learning and data mining in the cloud,” VLDB Endow.,
vol. 5, no. 8, pp. 716–727, 2012.
[12] J. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin,
“PowerGraph: Distributed graph-parallel computation on natural
graphs,” in OSDI, 2012.
[13] K. Chandy and L. Lamport, “Distributed snapshots: determining
global states of distributed systems,” ACM TOCS, vol. 3, no. 1,
pp. 63–75, 1985.
[14] J. Pujol, V. Erramilli, G. Siganos, X. Yang, N. Laoutaris,
P. Chhabra, and P. Rodriguez, “The little engine (s) that could:
scaling online social networks,” in ACM SIGCOMM, 2010, pp.
375–386.
[15] D. Ongaro, S. M. Rumble, R. Stutsman, J. Ousterhout, and
M. Rosenblum, “Fast crash recovery in RAMCloud,” in Proc.
SOSP, 2011, pp. 29–41.
[16] G. Wang, W. Xie, A. J. Demers, and J. Gehrke, “Asynchronous
large-scale graph processing made easy.” in CIDR, 2013.
[17] Y. Tian, A. Balmin, S. A. Corsten, S. Tatikonda, and J. McPher-
son, “From “think like a vertex” to “think like a graph”,” in Proc.
VLDB, 2013.
[18] J. W. Young, “A ﬁrst order approximation to the optimum
checkpoint interval,” Comm. of the ACM, vol. 17, no. 9, pp.
530–531, 1974.
[19] H. Haselgrove,
“Wikipedia page-to-page
link database,”
http://haselgrove.id.au/wikipedia.htm, 2010.
[20] “HDFS
System),”
http://hadoop.apache.org/common/docs/current/hdfs design.html.
[21] S. N. A. Project, “Stanford large network dataset collection,”
(Hadoop
Distributed
File
http://snap.stanford.edu/data/.
[22] J. Dean and S. Ghemawat, “MapReduce: simpliﬁed data
the ACM,
[Online]. Available:
processing on large
vol. 51, no. 1, pp. 107–113, 2008.
http://doi.acm.org/10.1145/1327452.1327492
clusters,” Commun. of
[23] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. Mc-
Cauley, M. Franklin, S. Shenker, and I. Stoica, “Resilient
distributed datasets: A fault-tolerant abstraction for in-memory
cluster computing,” in Proc. NSDI, 2012.
[24] S. Ghemawat, H. Gobioff, and S.-T. Leung, “The google ﬁle
system,” in Proc. SOSP, 2003, pp. 29–43.
[25] M. Castro and B. Liskov, “Practical byzantine fault tolerance,”
in Proc. OSDI, 1999.
[26] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong,
“Zyzzyva: speculative byzantine fault tolerance,” in Proc. SOSP,
2007.
[27] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed, “Zookeeper:
internet-scale systems,” in Proc.
wait-free coordination for
Usenix ATC, 2010.
[28] C. Wilson, B. Boe, A. Sala, K. P. Puttaswamy, and B. Y. Zhao,
“User interactions in social networks and their implications,” in
EuroSys, 2009, pp. 205–218.
[29] C. E. Tsourakakis, C. Gkantsidis, B. Radunovic, and M. Vo-
jnovic, “Fennel: Streaming graph partitioning for massive scale
graphs,” Microsoft, Tech. Rep. 175918, 2012.
[30] R. Power and J. Li, “Piccolo: building fast, distributed programs
with partitioned tables,” in OSDI, 2010, pp. 1–14.
[31] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, “Dryad:
distributed data-parallel programs from sequential building
blocks,” in EuroSys, 2007, pp. 59–72.
[32] M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker, and I. Stoica,
“Discretized streams: Fault-tolerant streaming computation at
scale,” in Proc. SOSP, 2013.
[33] M. Balazinska, H. Balakrishnan, S. R. Madden, and M. Stone-
braker, “Fault-tolerance in the borealis distributed stream pro-
cessing system,” ACM Transactions on Database Systems
(TODS), vol. 33, no. 1, p. 3, 2008.
[34] M. A. Shah, J. M. Hellerstein, and E. Brewer, “Highly available,
fault-tolerant, parallel dataﬂows,” in Proceedings of the 2004
ACM SIGMOD international conference on Management of
data. ACM, 2004, pp. 827–838.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply.