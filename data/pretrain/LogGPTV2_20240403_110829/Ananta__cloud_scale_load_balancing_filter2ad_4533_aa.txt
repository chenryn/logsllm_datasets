title:Ananta: cloud scale load balancing
author:Parveen Patel and
Deepak Bansal and
Lihua Yuan and
Ashwin Murthy and
Albert G. Greenberg and
David A. Maltz and
Randy Kern and
Hemant Kumar and
Marios Zikos and
Hongyu Wu and
Changhoon Kim and
Naveen Karri
Ananta: Cloud Scale Load Balancing
Parveen Patel, Deepak Bansal, Lihua Yuan, Ashwin Murthy, Albert Greenberg,
David A. Maltz, Randy Kern, Hemant Kumar, Marios Zikos, Hongyu Wu,
Changhoon Kim, Naveen Karri
Microsoft
ABSTRACT
Layer-4 load balancing is fundamental to creating scale-out web
services. We designed and implemented Ananta, a scale-out layer-4
load balancer that runs on commodity hardware and meets the per-
formance, reliability and operational requirements of multi-tenant
cloud computing environments. Ananta combines existing tech-
niques in routing and distributed systems in a unique way and splits
the components of a load balancer into a consensus-based reliable
control plane and a decentralized scale-out data plane. A key com-
ponent of Ananta is an agent in every host that can take over the
packet modiﬁcation function from the load balancer, thereby en-
abling the load balancer to naturally scale with the size of the data
center. Due to its distributed architecture, Ananta provides direct
server return (DSR) and network address translation (NAT) capa-
bilities across layer-2 boundaries. Multiple instances of Ananta
have been deployed in the Windows Azure public cloud with com-
bined bandwidth capacity exceeding 1Tbps.
It is serving trafﬁc
needs of a diverse set of tenants, including the blob, table and rela-
tional storage services. With its scale-out data plane we can easily
achieve more than 100Gbps throughput for a single public IP ad-
dress. In this paper, we describe the requirements of a cloud-scale
load balancer, the design of Ananta and lessons learnt from its im-
plementation and operation in the Windows Azure public cloud.
Categories and Subject Descriptors: C.2.4 [Computer-Communi-
cation Networks]: Distributed Systems
General Terms: Design, Performance, Reliability
Keywords: Software Deﬁned Networking; Distributed Systems;
Server Load Balancing
1.
INTRODUCTION
The rapid rise of cloud computing is driving demand for large
scale multi-tenant clouds. A multi-tenant cloud environment hosts
many different types of applications at a low cost while providing
high uptime SLA — 99.9% or higher [3, 4, 19, 10]. A multi-tenant
load balancer service is a fundamental building block of such multi-
tenant cloud environments. It is involved in almost all external and
half of intra-DC trafﬁc (§2) and hence its uptime requirements need
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
Copyright 2013 ACM 978-1-4503-2056-6/13/08 ...$15.00.
Figure 1: Ananta Data Plane Tiers
to be at least as high as applications’ SLA, but often signiﬁcantly
higher to account for failures in other infrastructure services.
As a cloud provider, we have seen that cloud services put huge
pressure on the load balancer’s control plane and data plane. In-
bound ﬂows can be intense — greater than 100 Gbps for a single
IP address — with every packet hitting the load balancer. The pay-
as-you-go model and large tenant deployments put extremely high
demands on real-time load balancer conﬁguration — in a typical
environment of 1000 hosts we see six conﬁguration operations per
minute on average, peaking at one operation per second. In our ex-
perience, the data plane and control plane demands drove our hard-
ware load balancer solution into an untenable corner of the design
space, with high cost, with SLA violations and with load balancing
device failures accounting for 37% of all live site incidents.
The design proposed in this paper, which we call Ananta (mean-
ing inﬁnite in Sanskrit), resulted from examining the basic require-
ments, and taking an altogether different approach. Ananta is a
scalable software load balancer and NAT that is optimized for multi-
It achieves scale, reliability and any service any-
tenant clouds.
where (§2) via a novel division of the data plane functionality into
three separate tiers. As shown in Figure 1, at the topmost tier
routers provide load distribution at the network layer (layer-3) based
on the Equal Cost Multi Path (ECMP) routing protocol [25]. At the
second tier, a scalable set of dedicated servers for load balancing,
called multiplexers (Mux), maintain connection ﬂow state in mem-
ory and do layer-4 load distribution to application servers. A third
tier present in the virtual switch on every server provides stateful
NAT functionality. Using this design, no outbound traffﬁc has to
pass through the Mux, thereby signiﬁcantly reducing packet pro-
cessing requirement. Another key element of this design is the
ability to ofﬂoad multiplexer functionality down to the host. As
discussed in §2, this design enables greater than 80% of the load
balanced trafﬁc to bypass the load balancer and go direct, thereby
207eliminating throughput bottleneck and reducing latency. This divi-
sion of data plane scales naturally with the size of the network and
introduces minimal bottlenecks along the path.
Ananta’s approach is an example of Software Deﬁned Network-
ing (SDN) as it uses the same architectural principle of manag-
ing a ﬂexible data plane via a centralized control plane. The con-
troller maintains high availability via state replication based on the
Paxos [14] distributed consensus protocol. The controller also im-
plements real-time port allocation for outbound NAT, also known
as Source NAT (SNAT).
Ananta has been implemented as a service in the Windows Azure
cloud platform. We considered implementing Ananta functionality
in hardware. However, with this initial Ananta version in software,
we were able to rapidly explore various options in production and
determine what functions should be built into hardware, e.g., we
realized that keeping per-connection state is necessary to maintain
application uptime due to the dynamic nature of the cloud. Sim-
ilarly, weighted random load balancing policy, which reduces the
need for per-ﬂow state synchronization among load balancer in-
stances, is sufﬁcient for typical cloud workloads. We consider the
evaluation of these mechanisms, regardless of how they are imple-
mented, to be a key contribution of this work.
More than 100 instances of Ananta have been deployed in Win-
dows Azure since September 2011 with a combined capacity of
1Tbps. It has been serving 100,000 VIPs with varying workloads.
It has proven very effective against DoS attacks and minimized dis-
ruption due to abusive tenants. Compared to the previous solution,
Ananta costs one order of magnitude less; and provides a more
scalable, ﬂexible, reliable and secure solution overall.
There has been signiﬁcant interest in moving middlebox func-
tionality to software running on general-purpose hardware in both
research [23, 24, 5] and industry [8, 2, 27, 21, 31]. Most of these
architectures propose using either DNS or OpenFlow-enabled hard-
ware switches for scaling. To the best of our knowledge Ananta is
the ﬁrst middlebox architecture that refactors the middlebox func-
tionality and moves parts of it to the host thereby enabling use of
network routing technologies — ECMP and BGP — for natural
scaling with the size of the network. The main contributions of this
paper to the research community are:
• Identifying the requirements and design space for a cloud-
scale solution for layer-4 load balancing.
• Providing design, implementation and evaluation of Ananta
that combines techniques in networking and distributed sys-
tems to refactor load balancer functionality in a novel way to
meet scale, performance and reliability requirements.
• Providing measurements and insights from running Ananta
in a large operational Cloud.
2. BACKGROUND
In this section, we ﬁrst consider our data center network archi-
tecture and the nature of trafﬁc that is serviced by the load balancer.
We then derive a set of requirements for the load balancer.
2.1 Data Center Network
Figure 2 shows the network of a typical data center in our cloud.
A medium sized data center hosts 40, 000 servers, each with one
10Gbps NIC. This two-level Clos network architecture [11] typi-
cally has an over-subscription ratio of 1 : 4 at the spine layer. The
border routers provide a combined capacity of 400Gbps for con-
nectivity to other data centers and the Internet. A cloud controller
manages resources in the data center and hosts services. A service
Figure 2: Flat Data Center Network of the Cloud. All network
devices run as Layer-3 devices causing all trafﬁc external to a rack
to be routed. All inter-service trafﬁc — intra-DC, inter-DC and
Internet — goes via the load balancer.
Figure 3: Internet and inter-service trafﬁc as a percentage of
total trafﬁc in eight data centers.
is a collection of virtual or native machines that is managed as one
entity. We use the terms tenant and service interchangeably in this
paper. Each machine is assigned a private Direct IP (DIP) address.
Typically, a service is assigned one public Virtual IP (VIP) address
and all trafﬁc crossing the service boundary, e.g., to the Internet or
to back-end services within the same data center such as storage,
uses the VIP address. A service exposes zero or more external end-
points that each receive inbound trafﬁc on a speciﬁc protocol and
port on the VIP. Trafﬁc directed to an external endpoint is load-
balanced to one or more machines of the service. All outbound
trafﬁc originating from a service is Source NAT’ed (SNAT) using
the same VIP address as well. Using the same VIP for all inter-
service trafﬁc has two important beneﬁts. First, it enables easy
upgrade and disaster recovery of services since the VIP can be dy-
namically mapped to another instance of the service. Second, it
makes ACL management easier since the ACLs can be expressed
in terms of VIPs and hence do not change as services scale up or
down or get redeployed.
2.2 Nature of VIP Trafﬁc
Public cloud services [3, 4, 19] host a diverse set of workloads,
such as storage, web and data analysis. In addition, there are an in-
creasing number of third-party services available in the cloud. This
trend leads us to believe that the amount of inter-service trafﬁc in
the cloud will continue to increase. We examined the total trafﬁc
in eight data centers for a period of one week and computed the
ratio of Internet trafﬁc and inter-service trafﬁc to the total trafﬁc.
208The result is shown in Figure 3. On average, about 44% (with a
minimum of 18% and maximum of 59%) of the total trafﬁc is VIP
trafﬁc — it either needs load balancing or SNAT or both. Out of
this, about 14% trafﬁc on average is to the Internet and the remain-
ing 30% is intra-DC. The ratio of intra-DC VIP trafﬁc to Internet
VIP trafﬁc is 2 : 1. Overall, we ﬁnd that 70% of total VIP trafﬁc
is inter-service within the same data center. We further found that
on average the ratio of inbound trafﬁc to outbound trafﬁc across
our data centers is 1 : 1. Majority of this trafﬁc is read-write traf-
ﬁc and cross-data center replication trafﬁc to our storage services.
In summary, greater than 80% of VIP trafﬁc is either outbound or
contained within the data center. As we show in this paper, Ananta
ofﬂoads all of this trafﬁc to the host, thereby handling only 20% of
the total VIP trafﬁc.
2.3 Requirements
Scale, Scale and Scale: The most stringent scale requirements
can be derived by assuming that all trafﬁc in the network is either
load balanced or NAT’ed. For a 40, 000 server network, built us-
ing the architecture shown in Figure 2, 400Gbps of external trafﬁc
and 100 Tbps of intra-DC trafﬁc will need load balancing or NAT.
Based on the trafﬁc ratios presented in §2.2, at 100% network uti-
lization, 44Tbps trafﬁc will be VIP trafﬁc. A truly scalable load
balancer architecture would support this requirement while main-
taining low cost. While cost is a subjective metric, in our cloud, less
than 1% of the total server cost would be considered low cost, so
any solution that would cost more than 400 general-purpose servers
is too expensive. At the current typical price of US$2500 per server,
the total cost should be less than US$1,000,000. Traditional hard-
ware load balancers do not meet this requirement as their typical
list price is US$80,000 for 20Gbps capacity without considering
bulk discounts, support costs or redundancy requirements.
There are two more dimensions to the scale requirement. First,
the bandwidth and number of connections served by a single VIP
are highly variable and may go up to 100Gbps and 1million simul-
taneous connections. Second, the rate of change in VIP conﬁgura-
tion tends to be very large and bursty, on average 12000 conﬁgura-
tions per day for a 1000 node cluster, with bursts of 100s of changes
per minute as customer services get deployed, deleted or migrated.
Reliability: The load balancer is a critical component to meet
the uptime SLA of applications. Services rely on the load balancer
to monitor health of their instances and maintain availability during
planned and unplanned downtime. Over many years of operation
of our cloud we found that traditional 1 + 1 redundancy solutions
deployed as active/standby hardware load balancers are unable to
meet these high availability needs. The load balancer must support