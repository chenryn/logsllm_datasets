ground truth about what event types each app should handle.
As a result, we chose to focus on instances of missing event
handling, which we can identify based on knowledge about the
complete set of events. Unfortunately, such instances do not
tell us the extent to which such missing events are intentional
or the extent to which missing events’ exploitation can cause
unexpected behavior. While any instance is arguably a concern,
we wanted to focus our effort on the instances most likely to
be vulnerabilities. As a result, we chose to cluster apps in
order to identify the missing event handling that stands out as
the most “unusual,” with the parameter τ approximating the
unusualness of missing event handling.
As such, event use analysis can be viewed as a ﬁlter-
ing step that attempts to identify the most likely unhandled
event types for candidate vulnerabilities among all potential
unhandled event types. EVENTSCOPE can be conﬁgured to be
conservative and mark all unhandled event types as potential
bugs; doing so requires setting τ = 1.0 to generate 1 cluster.
B. Event Use Results
We evaluated EVENTSCOPE’s event use analysis using
ONOS v1.14.0 [46]. In addition to ONOS’s core services, the
ONOS codebase includes third-party apps written by indepen-
6
Fig. 4: ONOS event use matrix, M. Black cells represent
event types that are handled by event() methods. Horizontal
dividers represent app categories, and vertical dividers repre-
sent event kinds. (App category key: D = default, G = GUI,
I = integration, M = monitoring, O = optical, S = security,
T = trafﬁc engineering, and U = utility.)
Fig. 5: Dendrogram representation of ONOS network event
type similarity among apps, based on the SimRank distance
metric. The dashed vertical line represents a threshold τ =
0.90 with a partitioning of 9 clusters.
DeviceEvent: DEVICE_ADDEDDeviceEvent: DEVICE_AVAILABILITY_CHANGEDDeviceEvent: DEVICE_REMOVEDDeviceEvent: DEVICE_SUSPENDEDDeviceEvent: DEVICE_UPDATEDDeviceEvent: PORT_ADDEDDeviceEvent: PORT_REMOVEDDeviceEvent: PORT_STATS_UPDATEDDeviceEvent: PORT_UPDATEDEdgePortEvent: EDGE_PORT_ADDEDEdgePortEvent: EDGE_PORT_REMOVEDFlowRuleEvent: RULE_ADDEDFlowRuleEvent: RULE_REMOVEDFlowRuleEvent: RULE_UPDATEDHostEvent: HOST_ADDEDHostEvent: HOST_MOVEDHostEvent: HOST_REMOVEDHostEvent: HOST_UPDATEDIntentEvent: FAILEDIntentEvent: INSTALLEDIntentEvent: INSTALL_REQIntentEvent: PURGEDIntentEvent: REALLOCATINGIntentEvent: WITHDRAWNIntentEvent: WITHDRAW_REQInterfaceEvent: INTERFACE_ADDEDInterfaceEvent: INTERFACE_REMOVEDInterfaceEvent: INTERFACE_UPDATEDLinkEvent: LINK_ADDEDLinkEvent: LINK_REMOVEDLinkEvent: LINK_UPDATEDMcastEvent: ROUTE_ADDEDMcastEvent: ROUTE_REMOVEDMcastEvent: SINK_ADDEDMcastEvent: SINK_REMOVEDMcastEvent: SOURCE_ADDEDMcastEvent: SOURCE_UPDATEDNetworkConfigEvent: CONFIG_ADDEDNetworkConfigEvent: CONFIG_REGISTEREDNetworkConfigEvent: CONFIG_REMOVEDNetworkConfigEvent: CONFIG_UNREGISTEREDNetworkConfigEvent: CONFIG_UPDATEDRegionEvent: REGION_MEMBERSHIP_CHANGEDRegionEvent: REGION_UPDATEDTopologyEvent: TOPOLOGY_CHANGEDEvent Kind: Event TypeD: bandwidthmgrG: pathpainterI: kafkaintegrationI: openstacknetworkingI: openstacknodeI: openstackvtapI: rabbitmqI: vtnM: artemisM: faultmanagementM: inbandtelemetryM: incubatorM: metricsO: newopticalO: opticalO: roadmS: aclT: bgprouterT: evpnopenflowT: fwdT: imrT: mcastT: mfwdT: odtnT: ofagentT: p4tutorialT: piT: pimT: proxyarpT: raT: reactiveT: routingT: scalablegatewayT: sdnipT: segmentroutingT: simplefabricT: tetopologyT: virtualbngT: vplsU: dhcpU: dhcprelayU: mlbU: mobilityU: pceU: routeserviceApp Category: App Name0.00.20.40.60.81.0SimRank DistanceT: mfwdO: newopticalT: imrU: pceG: pathpainterT: fwdM: metricsO: opticalM: faultmanagementM: incubatorI: kafkaintegrationI: rabbitmqT: simplefabricT: raI: openstacknodeT: bgprouterO: roadmT: piT: segmentroutingI: openstacknetworkingI: openstackvtapT: mcastU: mobilityT: vplsI: vtnS: aclT: virtualbngM: inbandtelemetryT: p4tutorialU: dhcprelayT: evpnopenflowU: routeserviceT: odtnT: scalablegatewayM: artemisT: reactiveD: bandwidthmgrT: tetopologyU: dhcpT: routingT: pimT: sdnipU: mlbT: ofagentT: proxyarpApp Category: App Namedent developers. We explain each part of the methodology as
applicable to ONOS and its apps.
interface;
1) ONOS’s event system: ONOS events implement
the
Event
they include subject() and type()
methods that describe what the event is about (e.g., a Host) and
what type the event is, respectively. ONOS events are used for
various subsystems, so we limit our study to network-related
events only.3
We found that ONOS contains 95 network event listeners
across 45 apps’ event listeners.4 Popular event kinds handled
were DeviceEvent (25 instances), NetworkConﬁgEvent (22
instances), and HostEvent (18 instances). Overall, we found
45 event types among 11 (network) event kinds.
For each app’s event listeners, we used static analysis on
the listeners’ bytecode to generate control ﬂow graphs (CFGs)
of any event handlers (i.e., event() methods) within that
app. Within each method, we considered an event type handled
if it results in the call of other functional methods; we consid-
ered an event type to be not handled if it only executed non-
functional methods (e.g., logging) or immediately returned.
types:
2) ONOS unhandled event
Figure 4 shows
EVENTSCOPE’s generated event use matrix M of the 45 apps
included with the ONOS codebase. Each ONOS app includes a
self-deﬁned category, and categories are grouped by horizontal
dividers. Each event kind is grouped by vertical dividers.
Figure 5 shows the dendrogram of the resulting app clusters,
based on SimRank distance and complete-linkage clustering.
We empirically chose a threshold (τ = 0.90) that yielded a
number of clusters (i.e., 9) similar to the number of categories
of ONOS apps (i.e., 8) based on the assumption that there exist
at least as many categories as there are functional differences
among apps. We found that that threshold worked well in the
rest of our evaluation. (See Appendix C for an evaluation of τ
on detection rates.) We found that setting the threshold too low
(i.e., more clusters) created more singleton app clusters, which
should be avoided because each cluster’s union of event types
becomes the event types the app handles. However, setting the
threshold too high (i.e., fewer clusters) clustered apps with
too few functional similarities. Based on that threshold, we
generated 116 candidate vulnerabilities, which were used as
input into the next stage of EVENTSCOPE (Section V).
V. EVENT FLOW ANALYSIS
Given a list of candidate vulnerabilities, we identify which
vulnerabilities are reachable from the data plane and affect
the data plane. To do so, we generate an event ﬂow graph
that shows how apps and the controller use events, and how
these usages of events can interact to generate control ﬂow
in the control plane. Using that graph, we then validate
our candidate vulnerabilities by analyzing how they impact
subsequent control plane and data plane operations, looking
for impacts in the control plane that can be caused by other
data plane events. That results in a list of vulnerabilities with
real impacts on the data plane.
A. Event Flow Graph Generation
In order to determine reachable candidate vulnerabilities
from the data plane that affect the data plane (via the control
plane), EVENTSCOPE uses static analysis to create an event
ﬂow graph that illustrates how events and API calls propagate
from the data plane to the controller and apps.
1) Deﬁnitions: We formalize a component as a fragment of
the SDN codebase that begins at an event listener method or
core service method and ends at an API boundary or event
dispatch. An app or core service can have more than one
component if it has more than one event listener. As a result of
that deﬁnition, each component serves as an entry point5 into
control plane functionality. Our objective is to determine the
fragments of controller and app code that are reachable from
each entry point.6
Formally, an event ﬂow graph, denoted by G = (V,E), is
a directed, multi-edged graph that models the abstractions for
inter-procedural and inter-component control and data ﬂows
in the SDN control plane. Event ﬂow graphs summarize the
necessary control and data ﬂows among components needed
for event ﬂow analysis. Vertices, denoted by V, consist of
one of the following types: event listeners (represented as
entry point methods), API services (represented as an API
interface method or its implemented concrete method), and
representations of data plane input (DPIn) and data plane
output (DPOut). Edges, denoted by E, are labeled and consist
of one of the following types: API read calls (API_READ),
API write calls (API_WRITE), data plane inputs to methods
(DP_IN), methods’ output to the data plane (DP_OUT), or
passing of an event type (e.g., HOST ADDED event type of
the HostEvent event kind).
EVENTSCOPE uses a two-phase process in which it ﬁrst
examines which events are used within each app and then
considers how these events propagate and cause other events in
the context of multiple apps. As a result, EVENTSCOPE’s event
ﬂow graph can represent multiple apps as well as dependencies
among apps. The dependencies among applications for event
processing are shown as edges in the event ﬂow graph. One
event that is processed by multiple applications (i.e., event
listeners) is represented as a node with multiple outgoing
labeled edges with the respective event type; each edge is
directed towards an event listener of that event kind.
2) Methodology: EVENTSCOPE’s approach is shown in
Algorithm 2. It initializes the event ﬂow graph’s vertices to
be the set of event listeners and representations for data plane
inputs and outputs. It begins with the set of event listeners
as the components of entry points to check (line 1). For
each entry point, it generates a call graph (line 5). Within
the call graph, it checks whether calls relate to an API read
(lines 7–10), to an API write (lines 11–14), or to the event
dispatcher to generate new events (lines 15–16). It links the
event dispatchers and event listeners together in the event ﬂow
graph by using the event use matrix, M, generated in the
prior step (Section IV); each event type that is handled by
3Event implementation classes with the preﬁx org.onosproject.net.*.
4We note that ONOS core service components also include event listeners
for inter-service notiﬁcations. We did not evaluate those listeners’ event uses
because we assume that all event types handled by each core service event
listener are the event types necessary for correct functionality.
5In traditional static analysis, a program has a well-deﬁned entry point: the
main() function. However, since SDN is event-driven, no main() function
exists [62]. To correct for the lack of a main() function and to account for
the event-driven architecture, we use each component as an entry point.
6Lu et al. [38] deﬁne that as “splitting” in the component hijacking problem.
7
Algorithm 2 Event Flow Graph Generation
Input: API read methods Ar, API write methods Aw, data plane
input methods Di, data plane output methods Do, event listener
methods El, event kinds EK, event types ET , event use matrix
M
Output: Event ﬂow graph G
Initialize: V ← El ∪ {DPIn, DPOut}, E ← ∅
(cid:46) Stack S of entry points (i.e., components) left to
(cid:46) Checked components C
(cid:46) Components that dispatch events Ed
(cid:46) Entry point method e
(cid:46) Skip entry point if already processed
(cid:46) Call graph vertices cv and
S ← El
check
C ← ∅
Ed ← ∅
edges ce
1: while S is not empty do
2:
3:
4:
5:
e ← S.pop
if e ∈ C then
continue
(cv, ce) ← generateCG(e)
for each c ∈ cv do
if c ∈ Ar then
V ← V ∪ {c}
E ← E ∪ {(c, e)}
S.push(c)
else if c ∈ Aw then
V ← V ∪ {c}
E ← E ∪ {(e, c)}
S.push(c)
Ed ← Ed ∪ {(c)}
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: E ← linkListenersDispatchers(E, Ed, El, EK , ET , M )
19: E ← linkDataPlane(E, Di, Do)
20: G ← (V,E)
Labeled edges of particular event type t ∈ ET
else if c is the event dispatch method then
C ← C ∪ e
DP_OUT
(cid:46) Labeled edge API_READ
(cid:46) Labeled edge API_WRITE
(cid:46)
(cid:46) Labeled edges DP_IN or
Fig. 6: Event ﬂow graph of fwd’s packet processor. Blue
rectangles represent event listeners and packet processors, gray
ellipses represent API methods, and dashed edges represent
API calls.
a particular listener is represented as its own edge, so multi-
edges are possible (line 18). Finally, it identiﬁes core service
components that take in data plane input or generate data plane
output, and links those to the data plane input and output
vertices (line 19).
3) Results: To show how an event ﬂow graph abstracts
useful information for understanding SDN architecture events,
we consider the partial event ﬂow graph from ONOS shown in
Figure 6. It shows the forwarding app (fwd) packet processor
component as an entry point. (For event ﬂow graphs that in-
clude event dispatch edges, see Figures 8 and 9 in Section VII
8
and Figure 11 in Appendix B.) General static analysis tools
produce control ﬂow graphs (CFGs) for each procedure or
method, as well as a call graph (CG) for inter-procedural anal-
ysis; however, static analysis tools face challenges regarding
the understanding of API behavior and the semantics of a
given program’s domain [58]. While both CFGs and a CG
are necessary for control or data ﬂow analyses, neither type
of graph represents the SDN domain’s semantics of events or
API behavior at the right level of abstraction.
We generated an ONOS event ﬂow graph whose com-
ponents include core services, providers7, and 45 apps. The
ONOS event ﬂow graph’s nodes consists of representations of
143 event listeners, 25 packet processors, 81 API call methods
of core services, 1 data plane input node, and 1 data plane
output node. The ONOS event ﬂow graph’s edges consist
of representations of 396 API calls, 352 event dispatches,
and 21 data plane interactions. Appendix B shows a partial
representation of that event ﬂow graph based on 5 sample apps
and the core services that they use.
Because ONOS does not specify a precise set of API calls
that comprise the northbound API [59], we used the public
method signatures of the *Service and *Provider classes,
along with those methods’ return values, to determine API
read and write calls, resulting in 123 API read call methods, 87
API write call methods, 1 method directly related to data plane
input, and 44 methods directly related to data plane output and
effects. We identiﬁed event dispatching based on direct calls to
the event dispatcher for local events (e.g., post()) or indirect
calls to a store delegate8 for distributed events.
B. Vulnerability Validation
Now that we have an event ﬂow graph, we can combine it
with our candidate vulnerabilities to understand the extent to
which unhandled event types have data plane consequences.
We focus on valid vulnerabilities as those in which the
following conditions are met: 1) an app’s event listener does
not handle a particular event type, 2) that event listener can be
called as a result of actions triggered from data plane input,
and 3) in handling the other event types, that event listener
can take some subsequent action that affects the data plane
(i.e., data plane output). In essence, we investigate the cases
in which such an event handler would otherwise be affected
by data plane input and have an effect on the data plane.
Vulnerabilities deﬁned in this way can be expressed as path
connectivity queries in the event ﬂow graph.
1) Context: Event handling vulnerabilities do not occur in
isolation, but as part of a complex interaction web involving
many other event handlers and apps We need to consider
that context when discussing a given vulnerability. We borrow
from Livshits and Lam [37] the intuition that exploitable
vulnerabilities can occur as a result of a multi-stage exploit
via an initial data injection and a subsequent app manipulation.
7In ONOS, a provider interacts with core services and network protocol
implementations [48]. We consider provider services to be core services.
8ONOS uses distributed data stores across ONOS instances to store network
state information. An instance can notify other instances of a change to the
data store (e.g., a MapEvent event update of a Host object modiﬁcation in the
host data store). That notiﬁcation causes each instance to re-dispatch events
locally (e.g., a HostEvent event).
fwdReactiveForwardingReactivePacketProcessorPacketContextblock(…)FlowObjectiveServiceforward(…)PacketContextsend(…)PacketContextisHandled(…)PacketContextinPacket(…)Data Plane OutHostServicegetHost(…)Data Plane Inevent use matrix M, apps A
Algorithm 3 Vulnerability Validation
Input: Event ﬂow graph G, list of candidate vulnerabilities VC,
Output: List of vulnerabilities and contexts V
Initialize: V ← ∅
(cid:46) Vulnerabilities and contexts list V
1: for each (a, t) ∈ VC do (cid:46) App a ∈ A and event type t ∈ ET
(cid:46) El ⊂ G’s vertices
2:
3:
4:
5:
El ← getEventListeners(a,G)
if ¬(pathExists(DPIn → e ∈ El → DPOut,G)) then