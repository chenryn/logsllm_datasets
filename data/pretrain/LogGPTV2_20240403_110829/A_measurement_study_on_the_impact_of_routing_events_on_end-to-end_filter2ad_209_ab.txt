due to routing failures. Therefore, the number of loss bursts caused
by routing failures might be more than what can be identiﬁed by
our methodology.
4. FAILOVER EVENTS
In this section, we characterize data plane performance during
failover events. First, we observe that most packet loss bursts oc-
cur during failover events. Second, we present the extent to which
packet loss is caused by routing failures. Finally, we show that
routing failures can cause multiple loss bursts during one failover
event. In addition, we characterize the locations that routing fail-
ures occur.
4.1 Data Plane Performance
We measure the performance (in terms of loss, delay, and packet
reordering) based on UDP packet probes from 37 PlanetLab sites
to the BGP Beacon during the entire month of July 2005. There are
two kinds of failover events: (1) withdrawing the route advertised
to ISP 1 (denoted as “failover-1”) and (2) withdrawing the route
advertised to ISP 2 (denoted as “failover-2”). Each day, there are
four failover events:
two for each type. Among the 37 probing
hosts, 14 hosts choose the path via ISP 1 and 23 hosts choose the
path via ISP 2, when routes to both ISPs are announced. The with-
drawal of the chosen route currently used by a host to reach the
BGP Beacon forces the host to switch to the alternate, less pre-
ferred route (we refer to it as a path change).
t
s
r
u
b
s
s
o
l
f
o
r
e
b
m
u
N
 200
 180
 160
 140
 120
 100
 80
 60
 40
 20
 0
 -600  -400  -200
 0
 200
 400
 600
t
s
r
u
b
s
s
o
l
f
o
r
e
b
m
u
N
 500
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0
 -600  -400  -200
 0
 200
 400
 600
Starting time (seconds)
(a) Failover-1
Starting time (seconds)
(b) Failover-2
Figure 2: Number of loss bursts starting at each second.
At each probing host, UDP probing starts at 10 minutes before
the injection of withdrawal messages and lasts for 20 minutes (i.e.,
till 10 minutes after the injection of withdrawal messages). To un-
derstand the packet loss around failover events, we measure the
number of loss bursts starting at each second. Here, we consider
consecutively lost packets as one loss burst. The time for the last re-
ceived packet before the loss burst is the start time of the loss burst.
Figure 2 shows the number of loss bursts over all probing hosts
and failover events for the entire duration of our study. The x-axis
represents the start time of a loss burst, where the start time is mea-
sured (in seconds) relative to the injection of withdrawal messages.
We observe that the majority of loss bursts occur right after time 0,
i.e., the time when a withdrawal message is advertised. The large
number of loss bursts occurred during the time period [100 sec,
200 sec] in Figure 2(a) is most likely due to congestion because
we observe no route changes in our traceroute measurements and
no corresponding ICMP messages. After the failover event, trafﬁc,
including UDP probings, pings, and traceroutes, sent by probing
hosts can cause congestion at some routers within ISP 2 or the link
between ISP 2 and the Beacon. Note that there is no time synchro-
nization problem because both the time for a loss burst occurring
and the time for injecting a withdrawal message are measured by
the clock on the BGP Beacon.
To understand the extent to which failover events can cause packet
loss, we divide the time period that UDP packet probing is per-
formed into three intervals: (1) before path change:
the interval
from the start time of UDP packet probing to the injection of with-
drawal messages, (2) during path change:
the interval from the
injection of the withdrawal message to the time that path from the
probing host to the Beacon is stabilized, and (3) after path change:
the interval from the time the path from the probing host to the Bea-
con is stabilized till the end time of UDP packet probing. We use
traceroute to estimate path change duration for each failover event,
where we observe the IP-level path changing from the old stable
path to the new stable path. The path change duration is measured
by the time period between these two stable states. We measure
the following four performance metrics during each of the three in-
tervals of a failover event: (1) loss burst length (i.e., the number
of consecutively lost packets in the loss burst), (2) round-trip delay,
(3) number of reordered packets, and (4) offset of reordered packet.
Figure 3(a) shows distributions of loss burst length before, dur-
ing, and after a path change for failover-1 events. The x-axis is
shown in logscale. We ﬁnd that the packet loss burst during path
changes can have as many as 480 consecutive packets. Compared
to the loss burst length during a path change, the packet loss burst
length before and after a path change are quite short. Figures 3(b)-
(d) show the cumulative distribution of the average round-trip de-
lays, number of reordered packets, and the average reordering off-
set. We ﬁnd that failover events have signiﬁcant impact on packet
round-trip delays. In the worst case, during path changes, packet
F
D
C
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 1
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
during path change
before path change
after path change
 10
 100
Loss burst length
 1000
(a) Loss burst length
during path change
before path change
after path change
 0  50  100 150 200 250 300 350 400 450 500
Average RTT delay (ms)
(b) Round-trip delay
F
D
C
 1
 0.98
 0.96
 0.94
 0.92
 0.9
 0.88
during path change
before path change
after path change
 1
 10
 100
 1000
Out-of-order
F
D
C
 1
 0.98
 0.96
 0.94
 0.92
 0.9
 0.88
 0.86
 0.84
 0.82
 0
 1
during path change
before path change
after path change
 3
 2
 4
Average offset
 5
 6
(c) Number of reordering
(d) Reordering offset
Figure 3: Data plane performance during failover-1 events in which the route via ISP 1 is withdrawn.
 1
 0.8
 0.6
 0.4
 0.2
 0
 1
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
during path change
before path change
after path change
 10
 100
Loss burst length
 1000
(a) Loss burst length
during path change
before path change
after path change
 0  100 200 300 400 500 600 700 800 900 1000
Average RTT delay (ms)
(b) Round-trip delay
F
D
C
 1
 0.995
 0.99
 0.985
 0.98
 0.975
 1
F
D
C
 1
 0.995
 0.99
 0.985
 0.98
 0.975
 0.97
 0.965
 0.96
 0.955
 0.95
during path change
before path change
after path change
 10
Out-of-order
 100
during path change
before path change
after path change
 2
 4
Average offset
 3
 5
 6
 0
 1
(c) Number of reordering
(d) Reordering offset
Figure 4: Data plane performance during failover-2 events in which the route via ISP 2 is withdrawn.
round-trip delays can be more than 500msec. We observe that the
number of reordered packets for most hosts during failover events
is small. Only one PlanetLab host experiences more than 400 re-
ordered packets after failover events, which is probably due to some
anomalies along the path. However, the offset of reordered packets
is larger during failover events than those before and after failover
events. This indicates that path changes usually increase the de-
gree of packet reordering and would require larger buffer sizes for
real-time applications.
Figure 4 shows the performance characterization using the same
metrics for failover-2 events (i.e., the route via ISP 2 is withdrawn).
Most observations we made for Figure 3 also hold here. These
failover events have more impact on packet round-trip delays than
the failover events when the route via ISP 1 is withdrawn. In the
worst case, the round-trip time could be 900msec. More reordered
packets are observed. Nevertheless, these reordered packets have
smaller reordering offset on average. Because failover events have
the most impact on loss burst length, we will focus on identifying
the cause of the long packet loss bursts during path changes.
4.2 Root Causes of Loss Bursts
We correlate loss bursts with ICMP messages using the method
described in Section 3.4. During the failover-1 events, 50% of loss
bursts can be identiﬁed as caused by routing failures. During the
failover-2 events, 52% of loss bursts are identiﬁed as caused by
routing failures. To understand the extent to which routing failures
affect packet loss, we focus on two kinds of routing failures: (1)
loop-free routing failures and (2) forwarding loops.
Table 2 shows the number of failover events, the number of loss
bursts, and the amount of packet loss caused by routing failures.
We verify that 23% of the loss bursts, corresponding to 76% of lost
packets, are caused by routing failures, including both loop-free
routing failures and forwarding loops. We are unable to verify the
remaining 77% of loss bursts, which correspond to only 24% of
packet loss. These loss bursts may be caused by either congestion
or routing failures for which traceroute or ping is not sufﬁcient (due
Table 2: Overall packet loss caused by routing failures during
failover events
Causes
Lost
packets
68343 (76%)
37751 (55%)
30592 (45%)
21948 (24%)
Failover
events
Loss
bursts
Veriﬁed as routing failures
–Loop-free
–Forwarding loops
Unveriﬁed as routing failures
659 (56%)
451 (68%)
208 (32%)
539 (44%)
846 (23%)
607 (71%)
239 (29%)
2875 (77%)
to either insufﬁcient probe frequency or lack of ICMP messages)
for the veriﬁcation. Note that the length of loss bursts for which we
cannot verify as caused by routing failures is shorter than that can
be veriﬁed as caused by routing failures.
As we will see later, more than half of the routing failures oc-
cur within ISP 1. On the contrary, only a small portion of the
routing failures occur within ISP 2 upon withdrawal of the pre-
ferred route via ISP 2. We continue to examine whether routing
failures do occur within ISP 2, which are not visible from ICMP
messages. We use BGP updates collected from 12 routers within
ISP 2 to examine if those monitored routers experience routing
failures. Among all the 724 failover events at those 12 backbone
router (2 × 31 × 12 = 724), we observe 584 withdrawal mes-
sages from those monitored routers. That means that over 80% of
all the failover events have routing failures. We also observe that
the occurrence of withdrawal messages is right after the occurrence
of failover events, and the withdrawal message is quickly replaced
by an announcement. This means that during the failover events,
routers within ISP 2 indeed temporarily lose their routes to the
Beacon. However, most of these transient routing failures are not
visible as packet loss bursts in the data plane.
We measure the duration of a loss burst as the time interval be-
tween the latest received packet before the loss and the earliest one
after the loss. Figure 5(a) shows the duration of loss bursts that can
and cannot be veriﬁed as caused by routing failures. Again, we ob-
serve that the loss bursts that are veriﬁed as caused by routing fail-
 1
 0.8
 0.6
 0.4
 0.2
F
D
C