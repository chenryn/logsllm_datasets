The critical systems we target
tend have strict assurance
requirements, which require source-code access. We also need
to scan for necessary modiﬁcations to any assembly ﬁles or
in-line assembly. It would be straightforward to build a tool
that checks for such cases of assembly, to reduce the chance
of overlooking some.
E. Device-Driver Support
The two RCoE models differ signiﬁcantly in the support
required for device drivers. Our implementation is based on
the seL4 microkernel [11], which runs drivers in user mode.
As such,
they are almost normal processes, automatically
replicated by RCoE. However, as mentioned in Section II-B,
the actual device access is done by the primary replica only,
so drivers are aware of the SoR boundary.
In LC-RCoE, device drivers are supported by augmented
system calls, ARM_Page_Map and IA32_Page_Map, which
create cross-replica shared memory regions; this allows the
driver replicas to conduct input data replication in user mode.
In CC-RCoE, the replicas of a device driver must behave
identically due to the requirement for precise preemption.
This means that, unlike normal seL4 drivers, which directly
access memory-mapped device registers, replicated drivers
must delegate device-memory access and its input data repli-
cation to the kernel (where branches are not counted). We
support this with two new system calls, FT_Mem_Access
and FT_Mem_Rep; their signatures are shown in Listing 4.
These calls are synchronisation points, so they only perform
operations when all replicas are in sync.
FT_Mem_Access performs a read or write (as speciﬁed by
access_type) of the device memory at address va_mmio,
transferring the data to/from memory address va_src_dest.
int FT_Mem_Access(Word access_type, Word
va_mmio, Word va_src_dest, Word size);
int FT_Mem_Rep(Word va, Word size);
Listing 4. Signatures of driver-support system calls.
When called by the primary for reading, this will copy from
device memory to the kernel shared-memory region. Non-
primaries block until the primary has performed the read, after
that, each replica copies the shared value to va_src_dest.
On writing, the primary replica writes the data to the device
memory. The driver may optionally call FT_Add_Trace
to force the output data into the signature. FT_Mem_Rep
replicates a buffer used for direct memory access (DMA).
Executed by the primary, it copies the speciﬁed buffer to the
shared memory region, executed by another replica, it copies
from the shared region to the caller’s address space.
F. CC-RCoE vs LC-RCoE trade-offs
CC-RCoE requires more effort to maintain the logical clock
than LC-RCoE, which means that we can expect a higher
performance degradation. Hence, LC-RCoE is the preferred
approach when its requirements are met.
For code that is known to contain data races, or that is too
complex to assure free from races (including virtual machines),
CC-RCoE is the only option. LC-RCoE is also more restricted
in its ability to recover from errors, see Section IV-A.
For hardware-assisted CC-RCoE, as we use it on x86,
overheads result from (i) reading performance counters, (ii)
programming debug registers, and (iii) handling debug ex-
ceptions. These overheads can be signiﬁcant if an instruction
breakpoint is inside a tight loop. Furthermore, it is by no means
sure that all future x86 processors (or even present, non-Intel
ones) will provide the required hardware support.
The overhead of compiler-assisted CC-RCoE, as we use it
on Arm, also has overheads beyond that of reserving a register.
For each breakpoint we must program debug registers and
handle the resulting debug exceptions. The catch-up overhead
is again high if a breakpoint is inside a loop.
IV. ERROR RECOVERY
the faulty replica and roll
Once an error is detected, recovery is desirable. One ap-
proach would be to roll the system back to a checkpoint,
or just restart
it forward from
logged inputs. These options could be combined with RCoE,
but checkpointing raises its own dependability issues (fault-
tolerant storage of the checkpoints and logs), and would
result in high storage overheads. Furthermore, restore and roll-
forward would take considerable time, impacting availability.
A. Downgrading on Errors
A DMR conﬁguration can only detect divergence, after
which the only safe operation is to shut the system down
(presumably after raising some alerts). In contrast, a TMR
conﬁguration can safely continue operation without service
disruption by downgrading to DMR.
191
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
We support this [40], within limits. At present, we only
recover from a failed vote on the state signature, not from a
timeout while waiting for a straggler replica (although this
limitation would not be hard to lift by shutting down the
straggler’s core). Also we require agreement on the identity
of the diverging replica for downgrading.
All replicas independently vote on the signatures. If they
agree that the signatures agree, execution continues normally.
If all replicas agree that there is divergence, and they agree
on the identity of the diverging replica, we downgrade: the
faulty replica removes itself while the others wait for this
to complete. If there is no consensus, we halt the system.
Disagreement may result from faults in multiple replicas,
corruption of the checksums, or a fault during voting.
When removing the primary, the other replicas need to elect
a new primary (the remaining node with the smallest ID) and
re-route interrupts to it. CC-RCoE handles I/O operations in
the primary kernel, requiring reconﬁguration of DMA buffers
if the primary is removed. On x86 we support this by marking
DMA buffers, using an unused bit in the page tables, and
patching page-table entries when removing the primary. We
do not have an unused bit in the page tables of Cortex-A9
processors, so we presently do not support error masking for
CC-RCoE on Arm. The 64-bit Armv8 architecture as well as
Armv7 processors with the large-physical-address extension
do have such bits, so we will be able to support error masking
for CC-RCoE on newer Arm processors.
Since I/O operations are not redundantly executed, we
cannot downgrade if the primary is faulty and any replica
is currently accessing I/O devices, as we cannot determine
whether the faulty primary has initiated I/O operations that
might corrupt the system.
B. Voting Algorithm
Listing 5 shows the voting algorithm, which is invoked if the
checksums differ. It returns the ID of the diverging replica if
there is consensus on the faulter, or ﬂags an error otherwise.
Note that the voting algorithm can be affected by transient
faults as well, although the window is tiny. Thus, the algorithm
is designed to be executed by all the replicas redundantly, with
barriers to ensure fail-stop behaviour.
We ﬁrst compare each replica’s state signature with that of
the other replicas, and increment a per-replica counter if the
signatures match (lines 8–11). The barrier at line 12 ensures all
replicas ﬁnish before proceeding to the next stage, or halts the
system if the barrier times out. Lines 13–18 ﬁnd the smallest
value in the array ft_votes; the values in the array represent
the number of checksums in the array checksum which agree
with the one indexed by my_rid. Thus, the replica with the
smallest value is the faulty one.
Lines 19–22 check for the following cases: (1) more than
one replica is faulty, (2) all the checksums are the same.
The votes received by each non-faulty replica should be
the replica number (N) minus one if only one checksum is
incorrect. Each replica stores the ID of the replica it has
determined as faulty in the globally shared, per-replica variable
1 global_shared int ft_votes[N];
2 global_shared int ft_fault_replica[N];
3
4 int vote_fault_replica(void) {
5
int least_vote = N + 1;
int fault_replica = N + 1;
ft_votes[my_rid] = 0;
for (int i = 0; i < N; i++) {
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31 }
if (checksum[i] == checksum[my_rid])
ft_votes[my_rid]++;
}
kbarrier(bar, N);
for (int i = 0; i < N; i++) {
if (ft_votes[i] < least_vote) {
least_vote = ft_votes[i];
fault_replica = i;
}
}
if (ft_votes[my_rid] != N - 1)
ft_fault_replica[my_rid] = my_rid;
else
ft_fault_replica[my_rid] =
fault_replica;
kbarrier(bar, N);
for (int i = 0; i < N; i++) {
if (ft_fault_replica[i] !=
ft_fault_replica[my_rid]) {
return ERROR_DIFF_FAULT_REPLICA;
}
}
kbarrier(bar, N);
return fault_replica;
Listing 5. The algorithm for voting a faulty replica.
ft_fault_replica[my_rid] if the check succeeds; oth-
erwise, the replica stores its own ID. The barrier at line 23
ensures that all the replicas have ﬁnished the checking stage.
Finally, all the replicas check if the faulty replica voted by
others is the same as the one chosen by itself. An error is
returned if the faulty replica IDs are different, and the system
halts (lines 24–28). If all replicas agree on the faulty replica,
they pass the third barrier and the faulty replica ID is returned
by the function (lines 29–30).
Table I shows two examples of voting. In the ﬁrst, R2 has
an incorrect checksum, resulting in the lowest ft_votes, a
consensus of R2 being faulty. In the second example, all the
checksums differ, resulting in all ft_votes being 1. In this
TABLE I
EXAMPLES OF VOTING.
R0
R1
R2
checksum
ft votes
ft fault replica
checksum
ft votes
ft fault replica
0xdeadbeef
2
2
0xdeadbeaf
1 (<2)
0
0xdeadbeef
2
2
0xdeadbeef
1 (<2)
1
0xdeedbeef
1
2
0xdeedbeaf
1 (<2)
2
192
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
thus
the check (lines 24–28)
case, each replica set its ft_fault_replica[my_rid]
to its own ID;
returns
ERROR_DIFF_FAULT_REPLICA to indicate multiple faulty
replicas. Note that this voting algorithm supports any number
of replicas N ≥ 3.
C. Re-integration
Re-integrating an off-lined replica is the same as upgrading
from DMR to TMR operation. While not a critical feature, it
is deﬁnitely desirable for systems unattended for an extended
period. Upgrading is possible by copying all kernel and user
state of the present non-primary replica to the new replica [40].
We have not implemented this yet, and for now require a full
reboot to upgrade a DMR conﬁguration to TMR.
V. EVALUATION
We evaluate RCoE on both architectures. Our x86 processor
is a Core i7 6700 quad-core running at 3.40 GHz, 2×32 KiB
L1 and 256 KiB L2 cache per-core and a shared 8 MiB L3
cache, equipped with 8 GiB of DDR4-2133 memory, and an
Intel I219-LM network card. The kernel and native apps run in
64-bit mode. For x86 VM benchmarks, the kernel and seL4’s
virtual-machine manager (VMM) run in 32-bit mode, and the
VMM presently only supports 32-bit guests.
Our Arm platform is a SABRE Lite board based on an
i.MX6 SoC [41], which features a quad-core Cortex A-9
processor (32-bit Armv7-A ISA), 2×32 KiB L1 caches, a
shared 1 MiB L2 and 1 GiB of DDR3-1066 memory.
We use LC-D, LC-T, CC-D, and CC-T to represent DMR
and TMR conﬁgurations using the LC-RCoE or CC-RCoE
models. Note that the benchmarks for virtual machines are
only conducted on the x86 machine, since running replicated
virtual machines is not yet supported on the Arm board.
In tables, numbers in parentheses indicate standard
deviations in units of the least signiﬁcant digit.
A. Microbenchmarks
1) Tolerating Data Races: We use a simple program to
demonstrate that CC-RCoE is able to tolerate multithreaded
applications with data races. The benchmark starts 32 threads;
each thread reads a shared counter to a local register, idles
for a short interval, increases the local register, and writes the
register back to the shared counter, in a loop. When all threads
ﬁnish, we compare the shared counters of different replicas.
The shared counter is not protected by a lock, so this setup
contains data races.
For LC-RCoE, we observe that the counter values of the
replicas diverge with high probability. With CC-RCoE, while
the counter values tend to differ from the “correct” value (i.e.
if locking were used), we never see a divergence between
replicas in 1,000 runs on each architecture.
2) Dhrystone and Whetstone: To evaluate our framework’s
effect on CPU-bound applications, we port the Dhrystone [42]
integer benchmark, as well as the Whetstone [43] ﬂoating-
point suite, to run natively on seL4. Table II shows execution
times (average of 10 runs) for the various conﬁgurations,
DHRYSTONE/WHETSTONE EXECUTION TIMES IN SECONDS.
TABLE II
Dhrystone
Whetstone
Arm
200 M
Loops
Base
146.098 (2)
LC-D 146.991 (0)
LC-T
146.992 (0)
CC-D 153.422 (0)
153.427 (0)
CC-T
x86
1000 M
108.1 (0)
108.6 (1)
108.6 (0)
110.7 (1)
111.9 (1)
Arm
0.5 M
x86
2 M
108.9 (1)
109.8 (5)
109.8 (4)
122.9 (66)
133.5 (42)
120.3 (0)
120.3 (1)
120.4 (1)
138.7 (40)
143.0 (55)
as measured by the CPU cycle counters. We observe that
for both benchmarks and on both architectures, LC-RCoE
shows negligible overhead, in both the DMR (row LC-D)
and TMR (LC-T) conﬁgurations. This is not surprising, as
these benchmarks are CPU-bound, perform no system calls,
and have small working sets that ﬁt into the caches, avoiding
contention on the memory bus. As such, they represent a best
case for RCoE. The only overheads are from kernel entries
resulting from preemption-timer ticks.
A striking feature of the CC-RCoE results is that the relative
standard deviation of Whetstone runs is up to 5%. This is
a consequence of the overhead being very sensitive to the
location of the synchronisation point: if it is inside a loop,
overhead will be high as explained in Section III-D, else it
will be low. These simple benchmarks approach a worst-case
scenario for maintaining our precise logical times, as they
consist mostly of tight loops. The main difference is that
Whetstone is structured as several tight loops, resulting in
about 20% TMR overhead, while the main body of Dhrystone
is one long loop, resulting in a TMR overhead of 4–5%.
3) Virtualised Dhrystone and Whetstone: To examine the
cost of RCoE in virtualised environments, we run the bench-
marks inside a Linux VM on top of our CC-RCoE seL4 kernel
acting as the hypervisor (remember from Fig. III-A that LC-
RCoE cannot support VMs). As the seL4 kernel version we
use does not support hypervisor mode on Arm (it was added
in a later version) we can run virtualised setups only on x86.
Table III shows the results. Note that the baseline numbers
are not comparable to Table II, as the benchmarks are built
quite differently: The native versions run in 64-bit mode,
are compiled with optimisation disabled (per the comments
in the source ﬁle of Dhrystone) and statically linked, while
the virtualised programs run in 32-bit mode (seL4 does not
VIRTUALISED MICROBENCHMARK EXECUTION TIMES (S) AND
TABLE III
SLOWDOWNS ON X86.
Whetstone
Base
55 (0)
CC-D 130 (11) 1.5× 159 (11) 2.9×