sizes. With HyperLoop, there is no significant performance
degradation as the group size increases (Fig. 10(a)), while
with Naïve-RDMA, 99th percentile latency increases by up to
2.97× (Fig. 10(b)). We also observed that HyperLoop shows a
smaller variance of average latency between the group sizes
compared to Naïve-RDMA. This means that by offloading
operations to NICs, HyperLoop can significantly reduce the
latency and make it predictable regardless of the group size.
6.2 Performance in Real Applications
We measure the latency and CPU utilization of two real
applications (RocksDB and MongoDB), with YCSB [56], an
industry standard storage benchmark. In these experiments,
we use the 3 physical machines and set the replication group
size to 3. Table 3 shows properties of each workload within
YCSB. For MongoDB and RocksDB experiments, we initialize
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Kim and A. Memaripour et al.
Table 3: The percentage of read, update, insert, modify (read
and update) and scan operations in each YCSB workload.
Workload Read Update
YCSB-A
YCSB-B
YCSB-D
YCSB-E
YCSB-F
50
95
95
-
50
50
5
-
-
-
-
-
5
5
-
Insert Modify
-
-
-
-
50
Scan
-
-
-
95
-
)
s
µ
(
y
c
n
e
t
a
L
103
102
101
Naïve-Event
Average
Naïve-Polling
95th percentile
HyperLoop
99th percentile
Figure 11: Latency distribution of replicated RocksDB using
Naïve-RDMA (event-based and polling) and HyperLoop.
a data-store with 100 K and 1 M key-value pairs (32-byte
keys and 1024-byte values) prior to running 100 K and 16 M
operations against the data-store, respectively.
RocksDB: We compare three versions of replicated RocksDB
(three replicas): (i) Naïve-Event version which uses event-
based Naïve-RDMA for replication; (ii) Naïve-Polling which
uses polling-based Naïve-RDMA and uses CPU polling in
backup nodes to reduce latency; (iii) HyperLoop-based ver-
sion. To perform experiments in settings representative of
multi-tenant data centers, we co-located replicated RocksDB
processes with multiple instances of I/O intensive back-
ground tasks (our own MongoDB instances described next)
on the same CPU socket, each serving a YCSB client running
on the remote socket of the same server. The number of
application threads on each socket is 10× the number of its
CPU cores.
We use traces from YCSB (workload A) to compare the
three versions in average and tail latencies of update opera-
tions. As shown in Figure 11, HyperLoop offers significantly
lower tail latency in contrast to Naïve-Event (5.7× lower)
and Naïve-Polling (24.2× lower). Furthermore, HyperLoop
reduces the CPU overhead on backup nodes since it does not
involve backup CPUs in the critical path of replication. In
contrast, Naïve-Polling version burns two cores on backup
nodes to poll incoming messages. Interestingly, however,
Naïve-Event has lower average and tail latency compared
to Naïve-Polling as multiple tenants polling simultaneously
increases the contention enough that context switches start
increasing the average and tail latencies.
MongoDB: We compare two versions of MongoDB with
polling-based and HyperLoop-enabled replication. We run
YCSB workloads against a chain with three replicas. We
co-locate multiple instances (10:1 processes to cores ratio)
to emulate multi-tenant behavior from multi-tenant data
centers and then measure the latency. Figure 12 shows the
8
6
4
2
0
)
s
m
(
y
c
n
e
t
a
L
Average
95th percentile
99th percentile
A
D
B
E
Workload
F
A
(a) Native replication
F
D
B
E
Workload
(b) HyperLoop
Figure 12: Latency distribution of MongoDB with native and
HyperLoop-enabled replication.
performance comparison of the two versions. HyperLoop
reduces replication latency by up to 79% by reducing the
reliance on overburdened CPUs, and the remainder of the
latency is mostly due to the high overhead inherent to Mon-
goDB’s software stack in the client that requires query pars-
ing and other checks and translations before a query is ex-
ecuted. For MongoDB, HyperLoop also reduces the gap be-
tween average and 99th percentile latency by up to 81%.
Furthermore, HyperLoop completely obviates involvement
of backup CPUs in the process of replication by offloading
transaction replication and execution to the RDMA NIC. In
contrast, MongoDB’s native replicas saturate the cores while
running YCSB workloads.
7 DISCUSSION
Supporting other storage systems: In designing our prim-
itives and APIs, we were motivated by state-of-the-art trans-
actional programming frameworks for NVM [12, 13], and
the messages in the data path of replicated transactional
systems [60]. Such a design allows us not only to move
applications modified with existing APIs [14–16] easily to
HyperLoop but also give higher confidence to developers to
use these APIs for faster adoption of NVM-aware systems.
HyperLoop is also useful for non-ACID systems and/or
weaker consistency systems. In designing low-level prim-
itives for fully-ACID and strongly consistent systems, we
enable other weaker models as well. For instance, by ignor-
ing the durability primitive, systems can get acceleration for
RAMCloud [85] like semantics. By not using the log process-
ing primitive inside the transaction critical path, systems
can get eventually consistent reads at higher throughput.
Further, by not using the log processing and durability in
the critical path, systems can get replicated Memcache [27]
or Redis [37] like semantics.
Supporting other replication protocols: We explored
chain replication for its simplicity, popularity and inherent
load-balancing of network resources. It is well known that
the scalability of the RDMA NICs decreases with the number
of active write-QPs [71]. Chain replication has a good load
balancing property where there is at most one active write-
QP per active partition as opposed to several per partition
such as in fan-out protocols.
HyperLoop
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
While HyperLoop is optimized for chain replication pro-
tocol, HyperLoop’s primitives can be used for any general
replication protocol. For example, if a storage application has
to rely on a fan-out replication (a single primary coordinates
with multiple backups) such as in FaRM [60], HyperLoop can
be used to help the client offload the coordination between
the primary and backups from the primary’s CPU to the
primary’s NIC.
For instance, in FaRM, the primaries poll for lock requests
which it forwards to the backups that are also polling for
meeting those requests. Similarly, primaries and backups
poll for log record processing, making them durable, and log
truncation. Techniques described in §4.1 can be used so that
the client can offload these operations to the primary’s NIC
and manage the locks and logs in backups via the primary’s
NIC without the need for polling in the primary and the
backups, and ensuring the required ordering of operations
between between them.
Security analysis of HyperLoop: In designing HyperLoop,
we assume that servers running storage front and back-
ends are protected from attackers who can compromise the
servers. We did not consider a comprehensive threat model
since attackers can easily control the entire storage clus-
ter once gaining access to a single server and even without
exploiting a replication channel through HyperLoop. On
the other hand, in HyperLoop, applications have access to
RDMA queues on remote NICs. To ensure security and pri-
vacy, we use the same set of best practices when managing
local queues to manage remote queues. The queues are reg-
istered to have access to only a restricted set of registered
memory regions that belong to the same tenant, each re-
motely accessible queue is separately registered with a token
known only to the tenant, and finally, managed languages
are used to encapsulate the QPs from the application code
with stricter safety checking (e.g., type check) in the client.
In the future, as RDMA virtualization improves, we wish to
leverage even more hardware virtualization techniques to
secure the remote queues.
8 RELATED WORK
Optimized storage and networking stacks: As storage
and networking hardware technology have evolved, tradi-
tional OS storage and networking stacks became a perfor-
mance bottleneck in applications, especially in distributed
storage systems as shown in previous works [54, 68, 88].
MegaPipe [65], Affinity-Accept [87], TCP port reuse [2]
and Fastsocket [77] optimize the Linux kernel TCP stack
to support better scalability for multi-core systems and high-
speed NICs. Likewise file system and other storage optimiza-
tions [51, 66, 76, 99] propose making existing OSes friendly
to SSDs. However, even with the optimizations, there are still
remaining overheads including kernel-user space context
switching, data copying, bounds and access control checks
can cause high and unpredictable latency in storage systems.
New storage and network hardware: To avoid the OS
overheads, storage and networking techniques for kernel
bypass have been proposed [7, 26, 80, 90] to poll for I/O
completions from NIC or storage medium (e.g., SSD). Addi-
tionally, based on those techniques, user-space networking
stacks [28, 40, 49, 69, 79] and storage stacks [29, 54, 61, 84,
97, 98] have been developed. While these techniques help
reduce the latency of standalone storage services and appli-
ances [60, 71, 72, 78, 82, 91, 100], they do not work effectively
in multi-tenant environments where polling is expensive and
CPU still needs to coordinate between network and storage
stacks. In contrast, since HyperLoop does not rely on polling
and offloads the datapath of replicated NVM transactions to
RDMA NICs, it does not require CPU usage.
Existing remote SSD access systems offer only point to
point read/writes and often use CPU based polling for I/O
completions [55, 74, 84]. In contrast, HyperLoop provides
group-based primitives for replicated transactions with pre-
dictable performance without polling.
Multicast support in data centers: Reliable multicast [50]
is a seemingly effective network primitive for replication-
based distributed systems. Traditionally, since IP multicast
is unreliable [5, 96], it has not been widely adopted. Recent
developments in NIC technologies promote a revisiting of
reliable multicast at the network level. Derecho [70] proposes
an RDMA-based multicast protocol, which involves CPUs in
the critical path of operations and relies on polling. Thus, it
possibly incurs high tail latency especially in multi-tenant
settings.
9 CONCLUSIONS
Predictable low latency for both average and tail cases is crit-
ically lacking in modern replicated storage systems. Based
on the observation that CPU involvement is the root cause
of this behavior in multi-tenant deployments, we designed
HyperLoop, a new framework that completely eliminates
CPUs from the critical path of replicated transactions in
multi-tenant storage systems. HyperLoop entirely offloads
replicated transactions to commodity RDMA NICs, with
NVM as a storage medium. We realize this by designing
new group-based networking primitives that support ACID
transactions and demonstrate that existing storage systems
can be easily extended and optimized using HyperLoop. Our
evaluation with two popular storage systems shows that
HyperLoop can produce substantial reductions in both av-
erage and tail latency and CPU consumption on replicas.
Looking forward, even though our specific focus in this pa-
per was on storage systems, we believe that the design and
insights underlying HyperLoop can be more broadly appli-
cable to other data center workloads.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Kim and A. Memaripour et al.
ACKNOWLEDGMENTS
We would like to thank the anonymous SIGCOMM review-
ers and our shepherd, Marco Canini for their helpful com-
ments. This work was supported in part by NSF awards
CNS-1565343 and CNS-1513764.
REFERENCES
[1] 2008. MongoDB Managed Chain Replication. https://docs.mongodb.
com/manual/tutorial/manage-chained-replication/. Accessed on
2018-01-25.
[2] 2013. The SO REUSEPORT socket option. https://lwn.net/Articles/
542629/. Accessed on 2018-01-25.
[3] 2013. Transactions for AWS Dynamo DB. https://aws.amazon.com/
blogs/aws/dynamodb-transaction-library/. Accessed on 2018-01-25.
[4] 2014. Replication in AWS Dynamo DB. https://aws.amazon.com/