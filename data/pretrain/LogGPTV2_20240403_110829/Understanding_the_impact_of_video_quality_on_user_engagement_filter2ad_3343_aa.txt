title:Understanding the impact of video quality on user engagement
author:Florin Dobrian and
Vyas Sekar and
Asad Awan and
Ion Stoica and
Dilip Antony Joseph and
Aditya Ganjam and
Jibin Zhan and
Hui Zhang
Understanding the Impact of
Video Quality on User Engagement
Florin Dobrian,
Asad Awan, Dilip Joseph,
Aditya Ganjam, Jibin Zhan
Conviva
Vyas Sekar
Intel Labs
Ion Stoica
Conviva, UC Berkeley
Hui Zhang
Conviva, CMU
ABSTRACT
As the distribution of the video over the Internet becomes main-
stream and its consumption moves from the computer to the TV
screen, user expectation for high quality is constantly increasing.
In this context, it is crucial for content providers to understand if
and how video quality affects user engagement and how to best
invest their resources to optimize video quality. This paper is a
ﬁrst step towards addressing these questions. We use a unique
dataset that spans different content types, including short video on
demand (VoD), long VoD, and live content from popular video con-
tent providers. Using client-side instrumentation, we measure qual-
ity metrics such as the join time, buffering ratio, average bitrate,
rendering quality, and rate of buffering events.
We quantify user engagement both at a per-video (or view) level
and a per-user (or viewer) level. In particular, we ﬁnd that the per-
centage of time spent in buffering (buffering ratio) has the largest
impact on the user engagement across all types of content. How-
ever, the magnitude of this impact depends on the content type,
with live content being the most impacted. For example, a 1% in-
crease in buffering ratio can reduce user engagement by more than
three minutes for a 90-minute live video event. We also see that the
average bitrate plays a signiﬁcantly more important role in the case
of live content than VoD content.
Categories and Subject Descriptors
C.4 [Performance of Systems]: [measurement techniques, perfor-
mance attributes] ; C.2.4 [Computer-Communication Networks]:
Distributed Systems—Client/server
General Terms
Human Factors, Measurement, Performance
Keywords
Video quality, Engagement, Measurement
1.
INTRODUCTION
Video content constitutes a dominant fraction of Internet trafﬁc
today. Further, several analysts forecast that this contribution is set
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’11, August 15–19, 2011, Toronto, Ontario, Canada.
Copyright 2011 ACM 978-1-4503-0797-0/11/08 ...$10.00.
to increase in the next few years [2, 29]. This trend is fueled by
the ever decreasing cost of content delivery and the emergence of
new subscription- and ad-based business models. Premier exam-
ples are Netﬂix which now has reached 20 million US subscribers,
and Hulu which distributes over one billion videos per month. Fur-
thermore, Netﬂix reports that video distribution over the Internet is
signiﬁcantly cheaper than mailing DVDs [7].
As video distribution over the Internet goes mainstream and it
is increasingly consumed on bigger screens, users’ expectations for
quality have dramatically increased: when watching on the TV any-
thing less than SD quality is not acceptable. To meet this challenge,
content publishers and delivery providers have made tremendous
strides in improving the server-side and network-level performance
using measurement-driven insights of real systems (e.g., [12,25,33,
35]) and using these insights for better system design (e.g., for more
efﬁcient caching [18]). Similarly, there have been several user stud-
ies in controlled lab settings to evaluate how quality affects user ex-
perience for different types of media content (e.g., [13, 23, 28, 38]).
There has, however, been very little work on understanding how the
quality of Internet video affects user engagement in the wild and at
scale.
In the spirit of Herbert Simon’s articulation of attention eco-
nomics, the overabundance of video content increases the onus on
content providers to maximize their ability to attract users’ atten-
tion [36]. In this respect, it becomes critical to systematically un-
derstand the interplay between video quality and user engagement
for different types of content. This knowledge can help providers to
better invest their network and server resources toward optimizing
the quality metrics that really matter [3]. Thus, we would like to
answer fundamental questions such as:
1. How much does quality matter–Does poor video quality sig-
niﬁcantly reduce user engagement?
2. Do different metrics vary in the degree in which they impact
the user engagement?
3. Do the critical quality metrics differ across content genres and
across different granularities of user engagement?
This paper is a step toward answering these questions. We do so
using a dataset which is unique in two respects:
1. Client-side: We measure a range of video quality metrics using
lightweight client-side instrumentation. This provides critical
insights into what is happening at the client that cannot be ob-
served at the server node alone.
2. Scale: We present summary results from over 2 million unique
views from over 1 million viewers. The videos span several
popular mainstream content providers and thus representative
of Internet video trafﬁc today.
Using this dataset, we analyze the impact of quality on engage-
ment along three dimensions:
• Quality metrics: We measure several quality metrics that we
describe in more detail in the next section. At a high level,
these capture characteristics of the start up latency, the rate at
which the video was encoded, how much and how frequently
the user experienced a buffering event, and what was the ob-
served quality of the video rendered to the user.
• Time-scales of user engagement: We quantify the user en-
gagement at the granularity of an individual view (i.e., a single
video being watched) and viewer, the latter aggregated over all
views associated with a distinct user. In this paper, we focus
speciﬁcally on quantifying engagement in terms of the total
play time and the number of videos viewed.
• Types of video content We partition our data based on video
type and length into short VoD, long VoD, and live, to represent
the three broad types of video content being served today.
To identify the critical quality metrics and to understand the de-
pendencies among these metrics, we employ the well known con-
cepts of correlation and information gain from the data mining lit-
erature [32]. Further, we augment this qualitative study with re-
gression based analysis to measure the quantitative impact for the
most important metric(s). Our main observations are:
• The percentage of time spent in buffering (buffering ratio) has
the largest impact on the user engagement across all types of
content. However, this impact is quantitatively different for dif-
ferent content types, with live content being the most impacted.
For a highly popular 90 minute soccer game, for example, an
increase of the buffering ratio of only 1% can lead to more than
three minutes of reduction in the user engagement.
• The average bitrate at which the content is streamed has a sig-
niﬁcantly higher impact on live content than on VoD content.
• The quality metrics affect not only the per-view engagement
but also the number of views watched by a viewer over a time
period. Further, the join time which seems non-critical at the
view-level, becomes more critical for determining viewer-level
engagement.
These results have signiﬁcant implications on how content providers
can best use their resources to maximize user engagement. Reduc-
ing the buffering ratio can signiﬁcantly increase the engagement
for all content types, minimizing the rate of buffering events can
improve the engagement for long VoD and live content, and in-
creasing the average bitrate can increase the engagement for live
content. Access to such knowledge implies the ability to optimize
engagement. Ultimately, increasing engagement results in more
revenue for ad supported businesses as the content providers can
play more ads, as well as for subscription based services as better
quality increases the user retention rate.
The rest of the paper is organized as follows. Section 2 provides
an overview of our dataset and also scopes the problem space in
terms of the quality metrics, types of video content, and granulari-
ties of engagement. Section 3 motivates the types of questions we
are interested in and brieﬂy describes the techniques we use to ad-
dress these. Sections 4 and 5 apply these analysis techniques for
different types of video content to understand the impact of differ-
ent metrics for the view- and viewer-level notions of user engage-
ment respectively. We summarize two important lessons that we
learned in the course of our work and also point out a key direction
of future work in Section 6. Section 7 describes our work in the
context of other related work before we conclude in Section 8.
Figure 1: An illustration of a video session life time and as-
sociated video player events. Our client-side instrumentation
collects statistics directly from the video player, providing high
ﬁdelity data about the playback session.
2. PRELIMINARIES AND DATASETS
We begin this section with an overview of how our dataset was
collected. Then, we scope the three dimensions of the problem
space: user engagement, video quality metrics, and types of video
content.
2.1 Data Collection
We have implemented a highly scalable and available real-time
data collection and processing system. The system consists of two
parts: (a) a client-resident instrumentation library in the video player,
and (b) a data aggregation and processing service that runs in data
centers. Our client library gets loaded when Internet users watch
video on our afﬁliates’ sites. The library listens to events from the
video player and additionally polls for statistics from the player.
Because the instrumentation is on the client side we are able to
collect very high ﬁdelity raw data, process raw client data to gener-
ate higher level information on the client side, and transmit ﬁne-
grained reports back to our data center in real time with mini-
mal overhead. Our data aggregation back-end receives real time
information and archives all data redundantly in HDFS [4]. We
utilize a proprietary system for real time stream processing and
Hadoop [4] and Hive [5] for batch data processing. We collect
and process 0.5TB of data on average per day from various afﬁli-
ates over a diverse spectrum of end users, video content, Internet
service providers, and content delivery networks.
Video player instrumentation: Figure 1 illustrates the life time
of a video session as observed at the client. The video player goes
through multiple states (connecting and joining, playing, paused,
buffering, stopped). Player events or user actions change the state
of a video player. For example, the player goes to paused state
if the user presses the pause button on the screen, or if the video
buffer becomes empty then the player goes in to buffering state. By
instrumenting the client, we can observe all player states and events
and also collect statistics about the play back.
We acknowledge that the players used by our afﬁliates differ in
their choice of adaptation and optimization algorithms; e.g., select-
ing the bitrate or server in response to changes in network or host
conditions. Note, however, that the focus of this paper is not to
design optimal adaptation algorithms or evaluate the effectiveness
of such algorithms. Rather, our goal is to understand the impact
of quality on engagement in the wild. In other words, we take the
player setup as a given and evaluate the impact of quality on user
engagement. To this end, we present results from different afﬁl-
iate providers that are diverse in their player setup and choice of
optimizations and adaptation algorithms.
Stopped/ExitPlayerStatesJoiningPlayingBufferingPlayingNetwork/stream connectionestablishedVideo bufferfilled upVideobufferemptyBuffer replenishedsufficientlytimeUseractionEventsPlayerMonitoringVideo download rate,Available bandwidth,Dropped frames,Frame rendering rate, etc.2.2 Engagement Metrics
Qualitatively, engagement is a reﬂection of user involvement and
interaction. We focus on engagement at two levels:
1. View level: A user watching a single video continuously is a
view. For example, this could be watching a movie trailer clip,
an episode of a TV serial, or a football game. The view-level
engagement metric of interest is simply play time, the duration
of the viewing session.
2. Viewer level: To capture the aggregate experience of a sin-
gle viewer (i.e., an end-user as identiﬁed by a unique system-
generated clientid), we study the viewer-level engagement met-
rics for each unique viewer. The two metrics we use are num-
ber of views per viewer, and the total play time across all videos
watched by the viewer.
We do acknowledge that there are other aspects of user engage-
ment beyond play time and number of views. Our choice of these
metrics is based on two reasons. First, these metrics can be mea-
sured directly and objectively. For example, things like how fo-
cused or distracted the user was while watching the video or whether