# Understanding the Impact of Video Quality on User Engagement

**Authors:**
- Florin Dobrian, Conviva
- Vyas Sekar, Intel Labs
- Asad Awan, Conviva
- Ion Stoica, Conviva, UC Berkeley
- Dilip Antony Joseph, Conviva
- Aditya Ganjam, Conviva
- Jibin Zhan, Conviva
- Hui Zhang, Conviva, CMU

## Abstract
As video distribution over the Internet becomes mainstream and consumption shifts from computers to TV screens, user expectations for high-quality content are rising. Content providers must understand how video quality affects user engagement and how to allocate resources effectively to optimize it. This paper takes a first step in addressing these questions by using a unique dataset that includes various content types such as short and long video on demand (VoD) and live content from popular providers. We measure quality metrics like join time, buffering ratio, average bitrate, rendering quality, and rate of buffering events.

We quantify user engagement at both the per-video and per-user levels. Our findings indicate that the buffering ratio has the most significant impact on user engagement across all content types. However, the magnitude of this impact varies, with live content being the most affected. For example, a 1% increase in buffering ratio can reduce user engagement by more than three minutes for a 90-minute live video event. Additionally, average bitrate plays a more crucial role in live content compared to VoD content.

**Categories and Subject Descriptors:**
- C.4 [Performance of Systems]: Measurement techniques, performance attributes
- C.2.4 [Computer-Communication Networks]: Distributed Systemsâ€”Client/server

**General Terms:**
- Human Factors, Measurement, Performance

**Keywords:**
- Video quality, Engagement, Measurement

## 1. Introduction
Video content now constitutes a dominant fraction of Internet traffic, and this trend is expected to grow in the coming years. This growth is driven by decreasing content delivery costs and the emergence of new subscription- and ad-based business models. Examples include Netflix, which has reached 20 million US subscribers, and Hulu, which distributes over one billion videos per month. Furthermore, Netflix reports that Internet video distribution is significantly cheaper than mailing DVDs.

As video distribution becomes mainstream and is increasingly consumed on larger screens, user expectations for quality have increased. Anything less than standard definition (SD) quality is often considered unacceptable when watching on a TV. To meet these expectations, content publishers and delivery providers have made significant strides in improving server-side and network-level performance using measurement-driven insights and better system design, such as more efficient caching.

However, there has been limited research on understanding how the quality of Internet video affects user engagement in real-world, large-scale settings. In the context of Herbert Simon's attention economics, the abundance of video content places a greater responsibility on content providers to maximize user engagement. Therefore, it is critical to systematically understand the relationship between video quality and user engagement for different types of content. This knowledge can help providers optimize their network and server resources to improve the quality metrics that matter most.

This paper aims to answer fundamental questions:
1. How much does quality matter? Does poor video quality significantly reduce user engagement?
2. Do different metrics vary in their impact on user engagement?
3. Do the critical quality metrics differ across content genres and user engagement granularities?

To address these questions, we use a unique dataset that is distinctive in two ways:
1. **Client-side Instrumentation:** We measure a range of video quality metrics using lightweight client-side instrumentation, providing insights into what happens at the client.
2. **Scale:** Our dataset includes summary results from over 2 million unique views from over 1 million viewers, spanning several popular mainstream content providers.

Using this dataset, we analyze the impact of quality on engagement along three dimensions:
- **Quality Metrics:** We measure several quality metrics, including start-up latency, encoding rate, buffering events, and rendered video quality.
- **User Engagement Granularity:** We quantify engagement at the per-video and per-viewer levels, focusing on total play time and number of videos viewed.
- **Types of Video Content:** We categorize our data into short VoD, long VoD, and live content.

To identify the critical quality metrics and understand their dependencies, we use correlation and information gain from data mining, augmented with regression analysis. Our main observations are:
- The buffering ratio has the largest impact on user engagement across all content types, with live content being the most affected. For example, a 1% increase in buffering ratio can reduce user engagement by more than three minutes for a 90-minute live video event.
- Average bitrate has a significantly higher impact on live content than on VoD content.
- Quality metrics affect not only per-view engagement but also the number of views watched by a viewer over time. Join time, while non-critical at the view level, becomes more critical for determining viewer-level engagement.

These results have significant implications for how content providers can best use their resources to maximize user engagement. Reducing the buffering ratio, minimizing the rate of buffering events, and increasing the average bitrate can all improve engagement, leading to more revenue for ad-supported and subscription-based services.

The rest of the paper is organized as follows:
- **Section 2:** Provides an overview of our dataset and scopes the problem space in terms of quality metrics, types of video content, and engagement granularities.
- **Section 3:** Motivates the types of questions we are interested in and describes the techniques used to address them.
- **Sections 4 and 5:** Apply these analysis techniques to understand the impact of different metrics on view- and viewer-level engagement.
- **Section 6:** Summarizes key lessons learned and points out future work directions.
- **Section 7:** Places our work in the context of related research.
- **Section 8:** Concludes the paper.

## 2. Preliminaries and Datasets
### 2.1 Data Collection
We have implemented a highly scalable and available real-time data collection and processing system. The system consists of:
- **Client-Resident Instrumentation Library:** Loaded in the video player, it listens to events and polls for statistics.
- **Data Aggregation and Processing Service:** Runs in data centers, receiving and archiving data redundantly in HDFS. We use a proprietary system for real-time stream processing and Hadoop and Hive for batch data processing.

Our client library collects high-fidelity raw data, processes it to generate higher-level information, and transmits fine-grained reports back to our data center in real time with minimal overhead. We collect and process approximately 0.5TB of data daily from various affiliates, covering a diverse spectrum of end users, video content, Internet service providers, and content delivery networks.

**Video Player Instrumentation:**
Figure 1 illustrates the lifetime of a video session as observed at the client. The video player goes through multiple states (connecting, joining, playing, paused, buffering, stopped). By instrumenting the client, we can observe all player states and events and collect statistics about playback.

We acknowledge that players used by our affiliates differ in their adaptation and optimization algorithms. However, our focus is on understanding the impact of quality on engagement, rather than designing or evaluating these algorithms. We present results from different affiliate providers with diverse player setups and optimization choices.

### 2.2 Engagement Metrics
Engagement is a reflection of user involvement and interaction. We focus on engagement at two levels:
- **View Level:** A single continuous viewing session of a video. The metric of interest is play time, the duration of the viewing session.
- **Viewer Level:** The aggregate experience of a single viewer, identified by a unique system-generated client ID. We study the number of views per viewer and the total play time across all videos watched by the viewer.

While there are other aspects of user engagement, we chose play time and number of views because they can be measured directly and objectively.