features, which are more discriminative to the two types of
voices commands, to defend against the hidden voice com-
mands.
5.4 Supervised Learning-based Method
Based on the unique vibration features, we apply learning-
based binary classifiers to determine the voice command
types, i.e., from the human speakers or the hidden voice
command adversary. In particular, several machine learning
algorithms like Simple Logistic, Support Vector Machine,
Random Forest and Random Tree can be used. Simple Lo-
gistic is a logistic regression-based classifier which predicts
the types of voice commands from the vibration features
using a logistic function. Support Vector Machine relies on a
hyperplane to divide the input command sample space into
two categories. The hyperplane is determined during the
training phase with the labeled voice commands from both
types. We apply Sequential Minimal Optimization (SMO) as
the optimization algorithm in SVM. Random Tree and Ran-
dom Forest are classifiers based on decision tree. Random
Forest further corrects the over-fitting issue that exists in
decision-tree based classifiers.
5.5 Unsupervised Learning-based Method
While the supervised learning-based methods are relatively
more complex and require large training data sets to build the
model, we utilized the unsupervised learning-based meth-
ods, which are able to learn the inherent physical differ-
ences between the hidden voice commands and normal com-
mands, without using explicitly-provided labels. Particularly,
we used the k-means based and k-medoids based methods.
20406080100Sound Sample Index00.511.522.533.5Euclidean DistanceHidden Voice CommandHuman Speaker SoundFigure 10: Distribution of the selected features (illustrated with the Note 4 placed on the table).
Figure 11: Frontend playback setups.
(a) Prototype on Raspberry Pi
(b) Cloud device imitation
In the backend setup, we utilize Raspberry Pi (Model 3B
plus) to build a prototype of the cloud service device as
shown in Figure 12(a). The device has a three-axis accelerom-
eter, SunFounder Digital ADXL345, attached to the top of a
common loudspeaker (Logitech S120). This accelerometer’s
sampling rate is set to 200Hz. The voice commands received
by the user’s VCS device would be sent to this cloud ser-
vice device to play back, and the on-board motion sensor
records the corresponding vibration signatures to detect the
hidden voice commands. Furthermore, we also imitate the
cloud service device by placing four types of smartphones on
a Marshall Stanmore loudspeaker, and the phone’s motion
sensor is used to imitate the on-board motion sensor of the
cloud service speaker. Note that this setup can also imitate
the frontend playback on the standalone VCS device, which
has an onboard motion sensor.
6.1.2 Data Collection. We used a typical Text To Speech
(TTS) service [2] with 5 speaker models (i.e., 3 females and
2 males) to generate a set of benign commonly used voice
commands as shown in Table 1. The sampling rate for the au-
dio samples is 16kHz. In order to best generate hidden voice
commands, we discussed with the authors of the work [9, 28].
Figure 12: Backend playback setups.
Particularly, we gradually adjust the MFCC parameters and
try to convert the low resolution MFCC features into the
obfuscated commands. To ensure that they can be correctly
recognized by Google Now, we use Google’s Cloud Speech-
to-Text service to test all of the generated hidden voice com-
mands. For each mobile device in each setup, each benign
voice command and hidden voice command are repeatedly
played ten times. In total we collected 13, 000 motion sensor
data traces (i.e., 6, 500 from benign commands and 6, 500 from
hidden voice commands) for our experimental evaluation.
6.2 Performance of Recognizing Hidden
Voice Commands
Supervised-learning. We first examine the perfor-
6.2.1
mance of our system using supervised learning-based meth-
ods. Table 2 shows the accuracy of the binary classification
(10-fold cross validation) of the voice commands in the fron-
tend setup on four different smartphones placed on a table.
We observe that all four supervised learning-based meth-
ods can efficiently differentiate the hidden voice commands
and the normal commands on all the four mobile devices.
max_xmax_ymax_zmin_xmin_ymin_zvar_yvar_zstd_ystd_zrange_xrange_yrange_zq3_xmcr_xmcr_ymcr_zabsMean_xabsMean_yabsMean_zcv_ykurt_xkurt_ykurt_ztotalabsAreatotalsvmenergy_yenergy_zentropy_xentropy_yentropy_zdomFreqRatio_xdomFreqRatio_ydomFreqRatio_zchroma_x8chroma_z8engery_yenergy_zmfcc_y1mfcc_y2mfcc_y3mfcc_y4mfcc_y5mfcc_y6mfcc_y7mfcc_y8mfcc_y9mfcc_y10mfcc_y11mfcc_y12mfcc_y13mfcc_z1mfcc_z2mfcc_z3mfcc_z4mfcc_z5mfcc_z6mfcc_z7mfcc_z8mfcc_z9mfcc_z10mfcc_z11mfcc_z12mfcc_z13Vibration Feature00.20.40.60.81Normalized ValueHVC SoundHuman SoundPlaced on tablePlaced on tablePlaced on sofaHeld byhandOn-board SpeakerRaspberry PiLogitech S120 LoudspeakerOn-board SpeakerRaspberry PiLoudspeakerOn-board Motion SensorsMarshall Stanmore LoudspeakerTable 2: Performance of supervised learning in the
frontend setup (Device on a table).
SimpleLogistic
SMO
Random Forest
Random Tree
G3
Note 4
99.8%
100%
99.9%
100%
100%
99.5%
99.9% 98.1%
Nexus 6
100%
99.9%
100%
100%
S6
88.3%
85.4%
93.1%
87.4%
Table 3: Performance of supervised learning in the
backend setup.
SimpleLogistic
SMO
Random Forest
Random Tree
Note 4
G3
99.9% 99.8%
99.9% 99.9%
99.9%
100%
99.9% 98.9%
Nexus 6
99.8%
99.3%
99.8%
97.9%
S6
95.3%
95.0%
95.3%
89.7%
In particular, Samsung Note 4, LG G3 and Nexus 6 achieve
up to 100% accuracy by using the four supervised learning
classifiers. Samsung S6 has the lowest accuracy, which still
reaches 93.1% when using Random Forest as the classifier.
Table 3 presents the 10-fold cross-validation performance of
our system in the backup playback setup with all the four mo-
bile devices used to imitate a cloud device’s on-board motion
sensor. We find that the Samsung Note 4, LG G3 and Nexus
6 show similar performance which is over 99.9% when us-
ing Simple Logistic and Random Forest. Samsung Galaxy S6
shows a lower accuracy of 95.3% when using Random Forest.
Moreover, the performances of our system for frontend and
backend playback setups are similar, which indicates that
our system is flexible and deployable in both setups with dif-
ferent on-board motion sensor. We also test our Raspberry Pi
prototype with the four supervised-learning methods, which
achieve over 88% accuracy.
We further evaluate the system’s performance to recog-
nize the hidden voice commands when the command words
and the human voices are not included in the training model.
We train the supervised learning model with two speakers’
voices and five command words. Table 4 and Table 5 presents
the system performances in both frontend setups and back-
end setups. We find that our system recognizes the hidden
voice commands from the unknown command words and
unknown speaker voice with high accuracy in both the fron-
tend and backend playback setups. In particular, Samsung
Note 4 and Nexus 6 achieve over 99.6% for both setups. The
LG G3’s performed well in the frontend setup with 99.7%
when using Simple Logistic. Its accuracy is 92.4% in the back-
end setup when using Random Forest. The results indicate
that the training size has a small influence on our supervised
learning methods in identifying hidden voice commands.
Table 4: Performance of supervised learning in fron-
tend setup (trained with 5/10 words and 2/5 partici-
pants).
SimpleLogistic
SMO
Random Forest
Random Tree
Note 4
100%
100%
100%
100%
G3
99.7%
99.4%
97.9%
94.4%
Nexus 6
100%
100%
100%
99.6%
S6
87.3%
86.4%
90.6%
87.8%
Table 5: Performance of supervised learning in back-
end speaker setup (trained with 5/10 words and 2/5
participants).
SimpleLogistic
SMO
Random Forest
Random Tree
G3
Note 4
86.4%
100%
86.6%
100%
100%
92.4%
99.4% 85.8%
Nexus 6
99.6%
99.2%
98.8%
98.1%
S6
89.7%
90.0%
84.8%
78.7%
6.2.2 Unsupervised-learning. We now present the results
of using our unsupervised learning-based methods to dis-
tinguish between the hidden voice commands and normal
commands, which capture the inherent physical differences
between the normal commands and the hidden voice com-
mands, without requiring much training effort. Figure 13
shows the performance of our k-means and k-medoids based
methods when using four different mobile devices in the
frontend and backend setups. We observe that both K-means
and K-medoids based methods accurately identify the hidden
voice commands for both setups. Moreover, both unsuper-
vised methods perform with similar accuracy among the four
devices. In particular, Samsung Note 4, LG G3 and Nexus 6
achieve over 99.2% in the frontend playback setup using the
K-means method. The accuracies are 100%, 98% and 93% for
the three devices in the backend playback setup. Samsung
S6 obtains 85.7% and 79% in both the frontend and back-
end setups. The results are comparable with our supervised
learning-based methods. We also evaluate our Raspberry
Pi cloud device prototype with the unsupervised learning
methods, which achieves 82% accuracy.
We further evaluate the influence of the training data size
to the performance of our unsupervised learning-based meth-
ods, especially when an adversary plays the hidden voice
commands using words and a speaker voice that are not in
the unsupervised training model. We only train the k-means
and k-medoids model with two speakers’ five voice command
words and evaluate our system’s capability of identifying
hidden voice commands. Figure 14 shows the performance
of our system under the limited training set. We find that
our system still identifies hidden voice commands with very
Table 6: Performance of partial replay (i.e., 1s and
0.5s) in frontend speaker setup using the unsuper-
vised learning method K-means.
Replay all
Replay 1s
Replay 0.5s
G3
Note 4
99.10%
100%
100%
89.10%
99.90% 85.20%
Nexus 6
100%
99.90%
95.90%
S6
85.70%
85.60%
85%
Table 7: Performance of partial replay (i.e., 1s and
0.5s) in backend speaker setup using the unsupervised
learning method K-means.
Replay all
Replay 1s
Replay 0.5s
G3
Note 4
99.90% 97.90%
99.10%
92.9
88.5
90.20%
Nexus 6
93.40%
92.40%
90.50%
S6
76%
75.90%