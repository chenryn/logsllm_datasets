max would be tuned depending on the application in
question, so that a user never gets a worthless result. When used
together with the average error and the average loss, the worst-
case loss metric reveals those mechanisms we might want to avoid
using. It is easy to see that the coin mechanism, although optimal in
terms of PAE and Q, gives a very large value of Q+(fcoin, π), which
manifests its uselessness.
An interesting consequence of setting a maximum worst-case
quality loss constraint when designing a mechanism is that it can
simplify the computational cost of the protocol that implements
or computes it. For example, take the case of the works in [5, 27],
where authors assume a discrete set of output locations Z and
propose to solve a linear program to find an optimal mechanism (in
terms of average error and geo-indistinguishability, respectively).
The constraint in (22) reduces the amount of variables that need
to be computed in these programs (only a subset of Z are possible
outputs for each input x ∈ X), as well as the amount of constraints,
which in turn decreases drastically the computational cost of the
problem. In other implementations of mechanisms, where f is not
explicitly derived but computed by adding (continuous) noise and
then computing a remapping using the posterior (c.f. [6]), having a
worst-case quality loss constraint reduces the amount of inputs that
need to be considered when computing the posterior, effectively
reducing the computational cost of the algorithm.
Finally, we would like to note that this metric exposes a basic
problem with geo-indistinguishability mechanisms. As mentioned
before, when using a geo-indistinguishability mechanism, if a user
with input location x has non-zero probability of reporting z ∈ A ⊆
R2, then when the input location is any other x′ ∈ X she must
assign a non-zero probability to reporting z ∈ A. This means that for
any geo-indistinguishable mechanism f , the worst-case quality loss
metric Q+(f , π) gives a huge value and the probability of getting a
useless response from the server would be larger than zero. One
could argue that, given the nature of the geo-indistinguishability
guarantee, the probability of reporting a location z far from x is
low and decreases exponentially with the distance between them,
so we could disregard such an event from happening. However, if
we really truncate the mechanism to ensure that the probability of
going very far is zero, then the mechanism does not provide any
geo-indistinguishability guarantee at all. It is then clear that geo-
indistinguishability mechanisms are problematic from the quality
loss point of view, and if a user gets zero utility from a realization
of the mechanism she cannot re-use it immediately, otherwise the
privacy guarantee is violated. We comment on a possible solution
to this problem below.
4.2.2
Implementation of Mechanisms with Worst-Case Quality
Loss Constraint. Now we set the task of designing a mechanism that
achieves a good value of worst-case quality loss or, alternatively,
that ensures that the worst-case quality loss is below some bound
Q+(f , π) ≤ Q+
max. The straightforward approach, given a mecha-
nism f , is to truncate the mechanism (for example, by generating
samples of z until one of them ensures that dQ(x, z) ≤ Q+
max, and
then releasing that z). This approach is reasonable, but one must
take into account that the privacy properties of this new truncated
mechanism f ′ are not the same as the original mechanism f , and
therefore they must be re-evaluated.
Another issue that concerns the design of bounded mechanisms
is that a deterministic remapping (15) might violate a Q+ constraint
(i.e., even if f guarantees the Q+ constraint, a composition f ′ =
f ◦ д might not guarantee it). Finding a bounded mechanism that
achieves as much privacy as an unbounded one in F opt
can be
an impossible task, due to the fact that the polytope defined by
Q+(f , π) ≤ Q+
Q . However, we can lose
some privacy with respect to an optimal unbounded mechanism in
exchange for a better worst-case quality loss guarantee by enforcing
the bounding constraint Q+(f , π) ≤ Q+
max might be disjoint with F opt
Q
max.
4.3 Other Complementary Metrics
Now, we finally outline other metrics that can be used together
with the average error and average quality loss to assess the privacy
of mechanisms, and leave the development of mechanisms taking
them into account as subject for future work.
Geo-indistinguishability (10) inherently ensures that an input
location x is mapped to a nearby location with more probability than
5 EVALUATION
In this section, we assess the performance of different location
privacy-preserving mechanisms with respect to different privacy
notions. Our experiments confirm that relying on a single metric
for evaluation can lead to an erroneous assessment of the privacy
provided by a mechanism. We divide our evaluation into two parts.
First, we consider the continuous scenario introduced in Section 2
and use real datasets to evaluate the performance of unbounded
mechanisms, and of mechanisms that guarantee a maximum worst-
case quality loss. Second, we consider a simpler scenario where
the locations can only belong to a discrete set, and evaluate other
defenses that have been proposed in the literature. All our experi-
ments are performed using Matlab.1
5.1 Continuous Scenario
For this part of the evaluation, we consider that users are interested
in querying about Points of Interest (PoIs) in a discrete set but
they can report any point in R2 to the server (see Section 2). We
also consider that the adversary performs her estimation in R2. We
build the set of PoIs using the Gowalla2 and Brightkite3 real-world
datasets. Following the approach of the finite domain evaluation
in [6], we restrict the PoIs to a finite region of San Francisco area
between the latitude coordinates (37.5395 and 37.7910) and longi-
tude (−122.5153 and −122.3789). We choose the San Francisco area
because it contains a big density of points of interest and a large
number of user check-ins, which ensures that the data is rich and
representative of what one would expect from users living in the
area. On the other hand, considering a finite region allows us to
evaluate mechanisms whose computational cost increases with the
number of points of interest, such as the exponential and exponen-
tial posterior mechanisms. We transform the PoIs into Cartesian
coordinates in kilometers using the Haversine formula with respect
to the center of the region. We end up with |X| = 9 701 PoIs for
Gowalla and |X| = 8 898 for Brightkite, distributed in an area of
roughly 28km × 12km. As example, the distribution of PoIs for
Gowalla is shown in Fig. 2. For each dataset, we compute the prior
π by counting how many users check-in on each point of interest
and normalizing the resulting histogram. The obtained priors are
shown in Fig. 3. We see that, in both datasets, there is a single
point of interest xtop that draws a lot of attention from the users
(π(xtop) ≈ 0.04 in Gowalla and π(xtop) ≈ 0.23 in Brightkite).
We evaluate six location-privacy preserving mechanisms, mea-
suring their performance in terms of the average adversary error
(PAE), conditional entropy (PCE) and geo-indistinguishability (PGI)
for different values of average quality loss (Q). We always use the
Euclidean distance for the quality loss dQ(x, z) = ||x − z||2, and
therefore the optimal remapping in (15) is obtained by computing
the geometric median of the posterior. We compute this median
using Weiszfeld’s iterative method. We first evaluate the mecha-
nisms without any bounds on their worst-case quality loss, and
then imposing such constraint.
The first three mechanisms we evaluate consist in adding noise
in the continuous plane and then remapping them. We generate
1https://www.mathworks.com/products/matlab.html
2https://snap.stanford.edu/data/loc-gowalla.html
3https://snap.stanford.edu/data/loc-brightkite.html
Figure 1: Two mechanisms that perform equally in the PAE
vs. Q plane, might behave very differently in practice. This
is revealed by considering a multi-dimensional characteriza-
tion of privacy.
to a far location, which solves the privacy issue we illustrated with
the coin mechanism. However, this privacy notion is not compatible
with a worst-case quality loss constraint by definition, due to the
fact that f (z|x) > 0 implies f (z|x′) > 0, ∀x′ ∈ X. A possible
approach to solve this utility issue of geo-indistinguishability can
be to relax its definition, allowing a small tolerance value ∆ ≪ 1,
f (z|x)dz ≤ eϵ ·dP(x,x′) ·
f (z|x
′)dz + ∆ ,
A
A
Other interesting metrics to assess the privacy of mechanisms are
those based on the worst-case output. For example, the worst-case
output average error, defined as
∀x, x
′ ∈ X ,
∀A ⊆ R2
.
(23)
i.e.,∫
(cid:41)
∫
(cid:40)
x ∈X

x ∈X
PWC-AE(f , π) = min
z∈R2
fZ (z)>0
min
ˆx ∈R2
π(x) · f (z|x) · dP(x, ˆx)
(24)
,
measures the average error of the adversary’s estimation in the
most vulnerable output. When applied to the coin mechanism, this
metric would reveal its privacy issue, since PWC-AE(fcoin, π) = 0.
On the other hand, the worst-case output conditional entropy,
defined as
PWC-CE(f , π) = min
z∈R2
fZ (z)>0
p(x|z) · log p(x|z) ,
(25)
reveals the uncertainty the adversary has after observing z in the
worst case (for the user). If there is any output value z that leaks
a lot of information about the real location x (as it happens with
every z (cid:44) z∗ in the coin mechanism), this metric highlights it.
The metrics introduced throughout this section add additional
dimensions to the privacy and quality loss evaluation procedure,
revealing features not captured by the standard 2-dimensional ap-
proach based on the average error and the average loss. An example
of this new characterization of privacy is shown in Fig. 1 where
we show the performance of two mechanisms as a 3-D plot of PAE,
PCE and Q, together with the projections in the PAE-Q and PCE-
Q planes. In the next section, we show similar examples (albeit
with 2-dimensional plots, for clarity) of particular location privacy
preserving mechanisms.
01010.550.5100Mechanism 1Mechanism 200.5100.20.40.60.81Mechanism 1Mechanism 2Figure 3: Priors π for Gowalla (left) and Brightkite (right) datasets.
Figure 4: Conditional entropy vs. average quality loss for Gowalla (left) and Brightkite
(right) datasets.
Figure 2: Points of interest in the
San Francisco region taken from
Gowalla dataset.
Figure 5: Geo-Ind Privacy PGI vs. average quality loss for Gowalla (left) and Brightkite
(right) datasets.
(cid:16) p−1
(cid:17)
e
(cid:16)
+ 1(cid:17) where W−1 is the −1 branch of the
this noise in polar coordinates, sampling θ from a uniform distribu-
tion in (0, 2π) and the radius r from a distribution specified below.
Since for these algorithms we cannot find a closed form expression
for f (z|x), we evaluate them empirically. To this end we sample
π to obtain x, we obtain z adding the noise and performing the
remapping, and then we measure privacy according to each metric.
We report averages over 5 000 repetitions. These mechanisms are:
• [Lap] Planar Laplacian noise plus remapping [6]. To gen-
erate the radius of the Laplace noise, we first sample p uni-
formly in the interval (0, 1). Then, following [2], we set
r = 1
ϵ
Lambert W function. We test different values of ϵ from
0.4km−1 to 40km−1, so that the average loss varies between
0.05 and 5km.
• [Gau] Bi-dimensional Gaussian noise plus remapping.
To generate Gaussian noise, we sample the radius from a
Rayleigh distribution, varying its mean from 0.05 to 5km.
• [Cir] Uniform circular noise plus remapping. In this case,
we sample the radius r ∈ (0, R) from f (r) = r/R
2, where R
is the maximum radius of the circle, which we vary from
0.075km to 7.5km. This ensures an average loss that varies
between 0.05 and 5km.
W−1
Second, we evaluate three mechanisms that output values in a
discrete set, whose conditional probability density functions f (z|x)
can be computed arithmetically. This allows us to exactly determine
their privacy and quality loss performance. These mechanisms are:
• [Coin] The coin mechanism, explained in Sect. 3.2. We
vary its average loss Q from 0 to 2.
• [Exp] The Exponential mechanism plus optimal remap-
ping. The exponential mechanism is a general differential
privacy technique that can be applied to provide geo-indistin-
guishability [10]. We set Z = X and set a parameter b,
then compute the probability of mapping each input x to
an output z as p(z|x) = a · e−b·dQ(x,z), where a ensures
z∈Z p(z|x) = 1. Then, we apply an optimal remap-
ping to the outputs of this function and obtain f (z|x). In the
experiments, we vary b from 0.4km−1 and 40km−1.
• [ExPost] Exponential posterior mechanism, proposed
in Section 4.1.2. In our experiments we set the discrete output
alphabet of this algorithm to Z = X.
that
5.1.1 Results for unbounded mechanisms (no Q+ constraint).
When the worst-case quality loss is not constrained, the optimal
remapping ensures that all mechanisms are optimal in terms of
020004000600080001000000.050.10.150.20.25010002000300040005000600070008000900000.050.10.150.20.2500.20.40.60.8102468101200.20.40.60.81024681000.20.40.60.811.21.41.61.800.511.5200.20.40.60.811.21.41.61.800.511.52Figure 6: Average error vs. average quality loss
for different bounded mechanisms.
Figure 7: Conditional entropy vs. average qual-
ity loss for different bounded mechanisms.
Figure 8: Semantic map
of the discrete synthetic
scenario.
average error, i.e., PAE = Q (see Fig. 11 in the Appendix). This shows
that the optimal remapping applied to any mechanism achieves an
optimal performance, whether it was Laplacian noise or a binary
selection of a location such as Coin, as we proved in Sect. 3.
Figure 4 shows the mechanisms’ performance in terms of con-
ditional entropy PCE, where the horizontal black line represents
the maximum entropy achievable, i.e., the entropy of the prior π.
Unsurprisingly, ExPost outperforms the rest of the mechanisms, as
it is optimized with respect to this metric. The relative improvement
of ExPost with respect to the other algorithms is slightly better in
Brightkite than in Gowalla. This is due to the fact that in Brightkite
the most frequent PoI is more popular than in Gowalla (see Fig. 3),
and thus performing well in this location is crucial to achieve a
good overall privacy level in Brightkite. The iterative structure of
ExPost allows this mechanism to refine its performance and be
more effective than the rest of the mechanisms around this PoI. We
note, however, that this refinement comes at the price of an increase
in computational cost. Overall, all the mechanisms achieve a similar
performance in terms of conditional entropy, except for the coin,
that performs poorly. This reinforces the critique in Sect. 3.2: even
though Coin is optimal in terms of the average adversary error,
measuring its performance in terms of conditional entropy reveals
its privacy flaws.
Figure 5 shows the mechanisms’ performance in terms of geo-
indistinguishability PGI(f ) (we recall that PGI(f ) = 1/ϵ), only for
Lap, Exp and ExPost, as these are the only algorithms that guaran-
tee this property. As already seen in [6], the Laplace noise outper-
forms the exponential mechanism, and ExPost performs similar to
the latter.
5.1.2 Results for bounded mechanisms. We now impose a worst-
case quality loss constraint of Q+
max = 1.5km to the mechanisms
(as a reference, we show a circle of radius 1.5km in Fig. 2). To
implement this constraint in the mechanisms, we truncate their
output at 1.5km and then apply the optimal remapping that respects
the worst-case loss constraint. We do this by solving the problem
in (15) with constraints. We do not evaluate the coin mechanism in
this scenario, since it almost always violates the Q+ constraint.
The results for the average adversary error as Euclidean distance
are shown in Fig. 6. As expected, the mechanisms obtained after the
remapping in this scenario are not necessarily optimal. We see that
ExPost achieves a result that is close to the optimal mechanism
in the unbounded case, while the other mechanisms achieve less
average privacy. We conjecture this is due to the iterative nature
of ExPost, that refines its performance, while the other mecha-
nisms are not optimized regarding the worst-case loss constraint.
Again, ExPost achieves a wider advantage in Brightkite for the
same reason explained above.