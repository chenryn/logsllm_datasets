Seconds
Fig. 5. Histogram of pass-through times
subject to diﬀerent levels of background
traﬃc (0, 2k, 10k pkts/second).
Fig. 6. Histogram of pass-through ti-
mes subject to diﬀerent # of sessions
(100/200) and background traﬃc (0, 2k)).
The histogram of the resulting pass-through times is shown in Figure 5.
While the diﬀerences may at ﬁrst appear minor the CPU load nevertheless has
an impact. It causes a delayed invocation of the BGP update processing task
which is reﬂected in the increase of the number of updates with a pass-through
time larger than 210ms. With no additional load only 1.3% of the updates are
in this category. With 2k packets this increases to 2.15% and for 10k packets to
3.73%. Note that a probe rate of 50 updates a second coupled with 5 invocations
of the BGP update processing task every second should create delays longer than
200ms for at most 10% of the probes. Overall we conclude that the increased
CPU load delays the invocation of the BGP update processing task and therefore
increases the pass-through delays. Yet, due to the timer, the delay increase is on
average rather small: from 101ms to 106ms to 110ms.
4.2 Pass-Through Times vs. Number of Sessions
This set of experiments is designed to explore if the number of sessions has
an impact on the BGP pass-through times. So far our load mix consisted of
the active BGP probes and the packets which cause a background CPU load.
Next we add additional BGP sessions (100/250) and 500 updates a second to
this mix. The simplest additional sessions are similar to our probe sessions with
the exception that we increase the update rates to 2 and 5 updates per second
respectively.
Figure 6 shows a histogram of the resulting BGP pass-through times. In-
terestingly adding 100 sessions poses no signiﬁcant problem to the router. Yet
adding 250 sessions causes way too much load on the router even without back-
ground traﬃc. Note that Cisco recommends to keep the number of sessions for
this speciﬁc router below 150. Adding to the 100 session experiment a backgro-
und CPU load of 2k (5k) increases the CPU load from a 5 minute average of
roughly 67% to 83% and then to 95%. That CPU loads are not summable is
an indication for the possibility of saving some CPU by delaying the processing
of the BGP updates. The additional BGP sessions increase the average pass-
Measuring BGP Pass-Through Times
275
through times to 116ms. The CPU load is then responsible for the increase to
130ms and respectively 160ms. The increase of the percentage of BGP probes
that take longer than 200ms is even more dramatic: ﬁrst from 1.3% to 7.4% and
then with the packet load to 12.5% and 25.9%. Still the maximum pass-through
times are reasonable small at less than 800ms.
The further increase of the number of sessions to 250 causes a multitude
of problematic eﬀects. First the router is no longer capable of processing the
updates so that it can send TCP acknowledgments on time. Accordingly the
number of TCP retransmissions increases from almost none, less than 0.15%, to
2.5% of the BGP probe updates. Second the number of probes propagated to the
monitoring sessions is drastically reduced. With 250 sessions the router does not
propagate updates for 39.8% of the probes. This problem is aggravated (49.2%)
by adding 2k packets of background traﬃc. While this reduces the number of
samples of the pass-through times their values are now in a diﬀerent class with
average pass-through times of 9, 987ms and 15, 820ms. The pass-through times
increase by several orders of magnitude.
4.3 Pass-Through Times vs. BGP Table Size and Update Rate
So far all tests consisted of either probe updates or artiﬁcial patterns. Accor-
dingly we now replace these artiﬁcial BGP sessions with actual BGP updates.
For this purpose we have selected two sets of two BGP routing tables dumps
from Ripe [13], one containing 37, 847/15, 471 entries and the other containing
128, 753/113, 403 entries. Note that routing tables generally diﬀer in terms of
their address space overlap, their size and their update rate. In this preliminary
study we did not use actual updates or synthetic updates [14] with similar cha-
racteristics. Rather, since we want to study the impact of the BGP update rate
on the pass-through times for diﬀerent table sizes, we opted for some regular
pattern. The ﬁrst pattern, called “full-speed”, corresponds to continuous session
resets and is realized by repeatedly sending the content of the BGP table to
the router as fast as possible. The second pattern, called “100 updates/sec”, is
similar but limits the rate of updates to 100 BGP updates a second. The third
pattern, called “10 updates/sec”, further reduces the rate of updates to 10 BGP
updates a second. As it is well known that session resets impose a large load
on the router one may expect larger pass-through times. As one hopes that the
router can perform session resets at a rate of 100 updates a second the second
pattern should be simpler and not impose quite such a large load. The 10 upda-
tes a second can be expected to impose even less load than our update probes
and therefore should provide us with a base line.
Figure 7 and 8 show the histograms of the pass-through times for experiments
with the two small tables, Figure 7, and the two larger tables, Figure 8. As
expected the pass-through times for “10 updates/sec” is with an average of
111ms for the small table only slightly increased. The impact of the large table
is visible in its average of 127ms. For the small table the full speed update rate
is signiﬁcantly higher than 100 updates/sec and imposes a CPU load of 88% to
60%. This diﬀerence in terms of update rate is not as large for the full table.
Here the full patter generates a CPU load of 100% as one would hope for. For
276
A. Feldmann et al.
the small table the average pass-through times increase signiﬁcantly from 147ms
to 181ms. If this may not seem like much there is huge danger hiding here, the
one of missing BGP keep-alives. In both “100 updates/sec” experiments and the
“full-speed” experiment for the large table the router did not manage to send its
keep-alive in time. Therefore these experiments terminated prematurely. Overall
the maximum pass-through time in the small table experiments are reasonable
with a maximum of less than 710ms and only 35% greater than 210ms. For the
more realistic cases with the large tables this changes. Here the maximum pass-
through times increase to 2.8 seconds and the percentages larger than 210ms
increases to 76%.
t
s
e
g
a
n
e
c
r
e
P
8
0
.
6
0
.
4
0
.
2
0
.
0
0
.
10 updates/sec
100 updates/sec
full−speed
0−0.2
0.2−0.4
0.4−0.6
0.6−0.8
Seconds
t
s
e
g
a
n
e
c
r
e
P
8
0
.
6
0
.
4
0
.
2
0
.
0
0
.
10 updates/sec
100 updates/sec
full−speed
0−0.2
0.2−0.4
0.4−0.6
Seconds
0.6−0.8
0.8−2.8
Fig. 7. Histogram of pass-through times
as update rate increases (small table, 2
sessions).
Fig. 8. Histogram of pass-through times
as update rate increases (large table, 2
sessions).
5 Summary
Our results show that it is possible to determine the pass-through times using a
black box testing approach. In general we found that BGP pass-through times are
rather small with average delays well less than 150ms. Yet there are situations
where large BGP convergence times may not just stem from protocol related
parameters, such as MRAI and route ﬂap damping. The pass-through time plays
a role as well.
Even when the MRAI timer is disabled and the router is otherwise idle,
periodic BGP update processing every 200ms can add as much as 400ms to the
propagation time. Increasing MRAI values appear to trigger timer interactions
between the periodic processing of updates and the timer itself which causes
progressively larger delays. For example, when using the default MRAI timer
value even the average estimate increases to 883ms but more importantly the
lower bound estimation can yield values for up to 8 seconds. This indicates that
there is an additional penalty for enabling MRAI beyond the MRAI delay itself.
Furthermore we have observed out of order arrival of updates which suggests that
Measuring BGP Pass-Through Times
277
using multiple preﬁxes for load balancing or fail-over may not always function
as expected. This bears more detailed veriﬁcation.
Low packet rate data traﬃc targeted at the DUT can impose a critical load on
the router and in extreme cases this can add several seconds to BGP processing
delay. These results reinforce the importance of ﬁltering traﬃc directed to the
infrastructure.
As expected, increasing the update rate does have an eﬀect on processing
time, but it is not nearly as signiﬁcant as adding new peers. Worth noting is
that concurrent frequent updates on multiple peers may cause problems. For
example, 53 peers generating 150 updates per second can cause the router to
miss sending a KEEPALIVE in time, thus resulting in a session reset.
Overall the in general small pass-through times indicate that the current
generation of routers may enable us to rethink some of the timer designs/artifacts
in the current BGP setup. Yet care is needed to not trigger the extreme situations
outlined above. Furthermore additional analysis is needed to better understand
the parameters aﬀecting BGP processing rate, such as FIB updates, line card
CPU loads, BGP update contents and other conﬁguration related parameters
already mentioned above.
References
1. T. Griﬃn, “Interdomain Routing Links.”
http://www.cambridge.intel-research.net/˜tgriﬃn/interdomain/.
2. C. Labovitz, A. Ahuja, A. Bose, and F. Jahanian, “Delayed Internet routing con-
vergence,” in Proc. ACM SIGCOMM, 2000.
3. T. G. Griﬃn and G. Wilfong, “An analysis of BGP convergence properties,” in
Proc. ACM SIGCOMM, 1999.
4. C. Labovitz, “Scalability of the Internet backbone routing infrastructure,” in PhD
Thesis, University of Michigan, 1999.
5. D. Wetherall, R. Mahajan, and T. Anderson, “Understanding BGP misconﬁgura-
tions,” in Proc. ACM SIGCOMM, 2002.
6. Z. M. Mao, G. Varghese, R. Govindan, and R. Katz, “Route ﬂap damping exacer-
bates Internet routing convergence,” in Proc. ACM SIGCOMM, 2002.
7. T. Griﬃn and B. J. Premore, “An experimental analysis of BGP convergence time,”
in Proc. International Conference on Network Protocols, 2001.
8. Z. M. Mao, R. Bush, T. Griﬃn, and M. Roughan, “BGP beacons,” in Proc. Internet
Measurement Conference, 2003.
9. H. Berkowitz, E. Davies, S. Hares, P. Krishnaswamy, and M. Lepp, “Terminology
Internet
for benchmarking bgp device convergence in the control plane,” 2003.
Draft (draft-ietf-bmwg-conterm-05.txt).
10. S. Agarwal, C.-N. Chuah, S. Bhattacharyya, and C. Diot, “Impact of BGP dyna-
mics on router CPU utilization,” in Proc. Passive and Active Measurement (PAM),
2004.
11. Y. Rekhter and T. Li, “A Border Gateway Protocol 4 (BGP-4),” 1995. RFC 1771.
12. “ENDACE measurement systems.” http://www.endace.com/.
13. RIPE’s Routing Information Service Raw Data Page. http://data.ris.ripe.net/.
14. O. Maennel and A. Feldmann, “Realistic bgp traﬃc for test labs,” in Proc. ACM
SIGCOMM, 2002.