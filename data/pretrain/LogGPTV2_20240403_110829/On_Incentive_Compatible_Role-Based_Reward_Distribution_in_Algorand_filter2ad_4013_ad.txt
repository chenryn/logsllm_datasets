enforce All−C strategy proﬁles as a Nash equilibrium. But the
following theorem shows that the current Algorand incentive
mechanism cannot enforce cooperation among all nodes.
Theorem 2. In each round i of GAl with N players (nL > 1
leaders, nM committee members, and nK remaining nodes),
if rewards are shared solely based on the current values of the
stakes as shown in Equation (2), i.e., the proposed Algorand
Foundation mechanism, we cannot establish all-cooperation
strategy proﬁle (All-C) as a Nash equilibrium strategy proﬁle.
Proof. Let us assume that all Algorand nodes have already
cooperated and paid the costs cL, cM , orc K as leader,
committee member, or other online nodes. Given Equations
(1) and (2), the pay of of node j in round i is
⎧⎪⎨
⎪⎩
uj
i (C) =
Leader lj
− cL
rislj
− cM Committee member mj
rismj
riskj − cK
Online node kj.
Consequently, by comparing cooperative and defecting payoffs
for each node similar to Theorem 1, and if we assume that
uj
i (C) =
(3)
(cid:302)(cid:37)(cid:76)
(cid:3)(cid:3)
(cid:47)
(cid:47)(cid:72)(cid:68)(cid:71)(cid:72)(cid:85)(cid:86)
(cid:37)(cid:76)
(cid:533)(cid:37)(cid:76)
(cid:3)(cid:3)
(cid:48)
(cid:3)(cid:3)
(cid:38)(cid:82)(cid:80)(cid:80)(cid:76)(cid:87)(cid:87)(cid:72)(cid:72)(cid:3)
(cid:80)(cid:72)(cid:80)(cid:69)(cid:72)(cid:85)(cid:86)
(cid:11)(cid:20)(cid:16)(cid:302)(cid:16)(cid:533)(cid:12)(cid:37)(cid:76)
(cid:3)(cid:3)
(cid:46)
(cid:50)(cid:87)(cid:75)(cid:72)(cid:85)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:86)
Fig. 4: Our proposed model shares the reward according to
the roles of nodes as well as their stakes.
they deviate unilaterally, we can conclude that all nodes have
incentive to deviate and increase their payoffs. Then, All − C
strategy proﬁle can never be an NE in GAl.
The results presented in Theorems 1 and 2 show that
we cannot enforce cooperation in the current reward sharing
approach for Algorand. In fact, if all users are rational they
will try to only play D and the system remains in All−D Nash
equilibrium. Intuitively, this occurs because different nodes
receive the same (portion of the) rewards despite bearing dif-
ferent costs depending on their role in the round. To overcome
this, we propose a novel incentive-compatible reward sharing
mechanism that shares rewards by considering node roles.
B. Proposed Reward Sharing Mechanism
As shown in Fig. 4, we propose that the reward Bi must
be divided into three parts, and then be distributed among
the nodes given their stakes. In our model, we assume that
αBi, βBi, and γBi must be distributed among leaders, com-
mittee members, and other online nodes, where α ∈ (0, 1),
β ∈ (0, 1), and γ ∈ (0, 1) should be chosen by the designer,
such that Algorand Foundation can enforce the cooperation
among users. Note that α + β + γ = 1. Given this approach,
one can provide different incentives to different types of users.
Hence, the payoff would be calculated by
⎧⎪⎨
⎪⎩
Leader lj
i slj − cL
rL
− cM Committee member mj
rM
i smj
− cK
rK
i skj
Online node kj,
(4)
i = αBi
SL , rM
SK . Let us now
where rL
deﬁne and analyze a new game GAl+, in which the payoffs
are calculated by Equation (4).
SM , and rK
i = βBi
i = γBi
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:36:15 UTC from IEEE Xplore.  Restrictions apply. 
459
C. Analysis of GAl+
Bi > max{
cL − cso
− γ
SK +s
In this section, we will ﬁrst determine conditions under
which user cooperation can be fostered. Then we will in-
vestigate the existence of NE in this game. The following
lemma presents conditions under which network leaders and
committee members have enough incentive to cooperate.
Lemma 2. Considering GAl+ with N players (nL > 1 leaders,
nM committee members, and nK remaining nodes), where
reward Bi shares with ratios α, β, and γ = 1−α−β between
leaders, committee members, and remaining nodes. A selﬁsh
leader lj or committee member mj cannot deviate from C
strategy unilaterally to increase its payoff, if and only if:
},
( α
SL
∗
mj are the minimum values of stakes for the
where s
leaders and committee members in round i.
Proof. Let us consider that all leaders and committee members
have cooperated in a given strategy proﬁle. In this case, the
payoff for any cooperative leader lj ∈ L would be equal
− cL. This payoff would be changed
to ulj
slj − cso, if the leader lj plays D and
to ulj
only keep its status online, without playing its role of a leader
in Algorand. Hence, this leader has no incentive to defect if
ulj
i (C) > ulj
i (D). Consequently, we can show that under the
following condition on Bi, the leader lj has no incentive to
deviate from C to D:
i (C) = αBi
SL slj
i (D) = γBi
SK +slj
cM − cso
−
∗
lj and s
γ
SK +s
( β
SM
∗
mj
∗
mj
∗
lj
)s
)s
∗
lj
,
Bi >
(5)
Similarly, any committee member mj ∈ M cannot increase
his payoff unilaterally by defecting and play D if:
SK +slj
( α
SL
)slj
.
cL − cso
− γ
cM − cso
−
γ
SK +smj
Bi >
( β
SM
.
)smj
(6)
Given two different bounds on the distributed rewards in
∗
Equations (5) and (6), and if we consider that s
lm
are the minimum values of stakes for leaders and committee
members in round i, we can conclude that no leader or
committee member can deviate in round i if
∗
lj and s
Bi > max{
( α
SL
cL − cso
− γ
SK +s
∗
lj
,
∗
lj
)s
( β
SM
cM − cso
−
γ
SK +s
∗
mj
}.
∗
mj
)s
Lemma 2 shows that the Algorand Foundation must always
distribute enough rewards to the leader and the committee
members in each round to enforce cooperative behavior among
them. The optimal reward Bi is a function of the cost of
cooperation and the current state of stakes in this round. It also
depends on the values of α, β, and γ, which must be selected
by the administrator. We consider that these values would be
announced at the beginning of each round. Another interesting
fact is that if we assign more fraction of the reward Bi to the
leaders and the committee members (i.e., increasing the values
of α and β), we can reduce the value of reward Bi, but have
all leaders and committee members cooperate in a cooperative
strategy proﬁle. This will help the administrator to save more
Algos for future use. Finally, giving more rewards to online
nodes (kj ∈ K) will increase the value of the required reward
for cooperative behavior of leaders and committee members.
Note that in Lemma 2, the following conditions must hold:
−
α
SL
γ
SK + slj
> 0 &
−
β
SM
γ
SK + smj
> 0
(7)
This can be easily proved given that the cost of cooperation for
the leaders and the committee members (i.e., cL and cM ) are
always positive. Having established the required conditions for
cooperation by leaders and committee members (Lemma 2),
we can now establish conditions under which GAl+ has a Nash
equilibrium apart which is not the All − D strategy proﬁle.
In fact this new class of cooperative NE in GAl+ will depend
on the behavior of other online nodes and their roles in the
Algorand network for any given round. Let us ﬁrst review two
important Algorand [22] concepts:
Deﬁnition 1. In Algorand network, “strong synchrony” is a
network state, where most honest Algorand nodes (e.g., 95%)
can send messages that would be received by most of the other
nodes (e.g., 95%) within a bounded time.
Deﬁnition 2. In Algorand network, with “weak synchrony”
state, the network can be asynchronous for a long but bounded
period of time. After this asynchrony period, network must be
again strongly synchronous for a reasonably long time.
Now we can form multiple sets of Algorand nodes which
meet strongly synchronous network assumption together.
Deﬁnition 3. “Strong synchrony set” is a list of Algorand
nodes that together forms a strongly synchronous network.
As Algorand protocol achieves liveness in strongly syn-
chronous settings and safety with weak synchrony, the fol-
lowing theorem focuses on ﬁnding Nash equilibria for GAl+
with the strong synchrony assumption.
Theorem 3. In game GAl+ with N players (nL > 1 leaders,
nM committee members, and nK remaining nodes), for each
∗ is a
Algorand strong synchrony set Y, a strategy proﬁle s
Nash equilibrium in round i, if in this strategy proﬁle:
1) All leaders cooperate,
2) All committee members cooperate,
3) All other nodes which are in Y cooperate, and
4) Other online nodes defect
cL − cso
− γ
SK +s
and the value of Bi is selected such that,
Bi > max{
( α
SL
∗
lj
)s
,
( β
SM
∗
lj
cM − cso
−
∗
mj
(cK − cso)SK
γ
SK +s
∗
mj
)s
,
}
∗
kj γ
s
∗
kj are the minimum stakes for leaders,
where s
committee members, and other online nodes in Y, in round i.
∗
mj , and s
∗
lj , s
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:36:15 UTC from IEEE Xplore.  Restrictions apply. 
460
∗ deﬁned
Proof. In order to prove that the strategy proﬁle s
by this theorem is a Nash equilibrium, we need to show that
none of the users can increase their payoffs unilaterally, by
changing the strategy. We divide the players into three groups,
i.e., leaders, committee members, and other nodes. For the
leader and committee members we apply our results presented
∗. In other words, the payoff of leaders
in Lemma 2 to deﬁne s
and committee members cannot be increased unilaterally if
the conditions of Lemma 2 are hold. Now, we will focus on
remaining nodes, i.e., skj . Given Deﬁnition 1, each remaining
node could be a member of strong synchrony set Y. Then
two cases must be considered: (i) Online nodes who are not
in Y cannot increase their payoffs by deviating from D to C
and accept the incurred cost of cK, as the block would be