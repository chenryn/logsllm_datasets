### Research Question 5: Is Coverage Accounting Robust Against Current Anti-Fuzzing Techniques?

#### Table II: Comparison of Fuzzers

| Fuzzer     | Year | Type       | Open | Target | Select |
|------------|------|------------|------|--------|--------|
| AFL        | 2016 | Greybox    | Y    | S/B    | (cid:88) |
| AFLFast    | 2016 | Greybox    | Y    | S/B    | (cid:88) |
| Steelix    | 2017 | Greybox    | N    | S      | (cid:88) |
| VUzzer     | 2017 | Hybrid     | Y2   | B      | (cid:88) |
| CollAFL    | 2018 | Greybox    | N    | S      | (cid:88) |
| FairFuzz   | 2018 | Greybox    | Y    | S      | (cid:88) |
| T-fuzz     | 2018 | Hybrid     | Y3   | B      | (cid:88) |
| QSYM       | 2018 | Hybrid     | Y    | S      | (cid:88) |
| Angora     | 2018 | Hybrid     | Y    | S      | (cid:88) |
| MOPT       | 2019 | Greybox    | Y    | S      | (cid:88) |
| DigFuzz    | 2019 | Hybrid     | N    | S      | (cid:88) |
| ProFuzzer  | 2019 | Hybrid     | N    | S      | (cid:88) |

**Notes:**
1. S: Target source code, B: Target binary.
2. VUzzer depends on external tools like IDA Pro and pintool, which are too expensive to scale in our experiment environment. Additionally, VUzzer did not perform as well as other hybrid tools such as QSYM. Therefore, VUzzer was not included in the real-world program experiments.
3. Some components of these tools do not work for all binaries.

### A. Experiment Setup

**Dataset:**
We collected 30 applications from papers published between 2016 and 2019. These applications include image parsing and processing libraries, text parsing tools, an assembly tool, multimedia file processing libraries, language translation tools, and more. We used the latest version of each application at the time of the experiment. While some may argue that the dataset lacks other test suites like LAVA-M, we found that LAVA-M does not reflect real-world scenarios and is not suitable for evaluating fuzzer effectiveness. This will be discussed further in Section VII.

In our evaluation, 18 applications had no vulnerabilities detected by any fuzzer. For simplicity, we will present results for the 12 applications where vulnerabilities were found.

**Compared Fuzzers:**
We selected recent fuzzers published from 2016 to 2019, as shown in Table II. We considered whether each fuzzer is open-source and whether it can run on our dataset. Tools that are not open-source or do not scale to our real-world programs were excluded. The remaining fuzzers are 4 greybox fuzzers (AFL, AFLFast, FairFuzz, and MOPT) and 2 hybrid fuzzers (QSYM and Angora). For detailed explanations, see the footnotes in Table II.

**Environment and Process:**
Experiments were conducted on eight identical servers with 32 CPU Intel(R) Xeon(R) CPU E5-2630 PI:EMAIL cores, 64GB RAM, and 64-bit Ubuntu 16.04.3 TLS. For each target application, all fuzzers were configured with the same seed and dictionary set. Each fuzzer was run with each target program for 140 hours, as recommended by CollAFL [18], and all experiments were repeated 10 times, as advised by Klees et al. [29].

**Vulnerability Identification:**
Vulnerabilities were identified in two steps. First, reported crashes were filtered using a self-written script with ASan [46] to remove invalid and redundant crashes. Remaining crashes were manually inspected, and security-related ones were reported. For code coverage, we used gcov [20], a well-known coverage analysis tool.

### B. RQ1: Finding Zero-Day Vulnerabilities

Table III shows the real-world vulnerabilities identified by TortoiseFuzz over 10 experimental runs. The table lists each discovered vulnerability, its ID, the corresponding target program, the vulnerability type, and whether it is a zero-day vulnerability. In total, TortoiseFuzz found 56 vulnerabilities across 10 different types, including stack buffer overflow, heap buffer overflow, use after free, and double free vulnerabilities. Many of these are critical and can lead to severe consequences such as arbitrary code execution. Among the 56 vulnerabilities, 20 are zero-day vulnerabilities, and 15 have been confirmed with a CVE identification. This indicates that TortoiseFuzz is effective in identifying zero-day vulnerabilities in real-world applications.

### C. RQ2: TortoiseFuzz vs. Other Fuzzers in Real-World Applications

In this experiment, we compared TortoiseFuzz with greybox and hybrid fuzzers on real-world applications. Each fuzzer was evaluated based on three metrics: discovered vulnerabilities, code coverage, and performance.

**Discovered Vulnerabilities:**
Table IV shows the average and maximum number of vulnerabilities found by each fuzzer over 10 repeated runs, along with the p-value of the Mann-Whitney U test comparing TortoiseFuzz with each fuzzer.

Our results show that TortoiseFuzz is more effective than other greybox fuzzers in finding vulnerabilities. On average, TortoiseFuzz detected 41.7 vulnerabilities and up to 50 vulnerabilities, outperforming all other greybox fuzzers. Compared to FairFuzz, the second-best greybox fuzzer, TortoiseFuzz found 43.9% more vulnerabilities on average and 31.6% more in the best run. Additionally, TortoiseFuzz covered all but one vulnerability found by the other fuzzers and discovered 10 additional vulnerabilities not found by any other fuzzer.

For hybrid fuzzers, TortoiseFuzz performed better than Angora and comparably to QSYM. TortoiseFuzz outperformed Angora by 61.6% on average and 56.3% in the best run. TortoiseFuzz also detected more or equal vulnerabilities in 91.7% (11/12) of the target applications. TortoiseFuzz found slightly more vulnerabilities than QSYM on average and the same number in the best run. Specifically, TortoiseFuzz performed better than QSYM in 7 programs, equally in 2 programs, and worse in 3 programs. Based on the Mann-Whitney U test, the difference between TortoiseFuzz and QSYM was not statistically significant, indicating that TortoiseFuzz is comparable to QSYM in finding vulnerabilities.

**Code Coverage:**
To evaluate the impact of coverage accounting on code coverage, we compared TortoiseFuzz with AFL, the fuzzer on which coverage accounting is based. Table V shows the average code coverage of all fuzzers, and Table VI shows the p-value of the Mann-Whitney U test between TortoiseFuzz and other fuzzers.

TortoiseFuzz had better coverage than AFL on average for 75% (9/12) of the target programs. Statistically, 3 out of 12 programs showed significant differences, with TortoiseFuzz having a higher average value in all three cases. This implies that TortoiseFuzz is statistically better than AFL in terms of code coverage. Therefore, coverage accounting does not negatively affect code coverage in the fuzzing process.

Comparing TortoiseFuzz to other fuzzers, we observed that, although TortoiseFuzz does not aim for high coverage, its performance is fair. Most results were not statistically significant when compared to AFL, AFLFast, and FairFuzz. Between TortoiseFuzz and MOPT, TortoiseFuzz performed statistically better in three cases, while MOPT performed better in two cases. TortoiseFuzz's results were statistically higher than Angora's in most cases but not as good as QSYM's.

**Performance:**
Given that TortoiseFuzz had comparable results to QSYM, we compared their resource performance. For each of the 10 repeated experiments, we logged the memory usage of QSYM and TortoiseFuzz every five seconds. Figure 3 shows the memory usage of each fuzzer and each target program, indicating that TortoiseFuzz used less memory than QSYM. This reflects the fact that hybrid fuzzers require more resources for heavy-weighted analyses such as taint analysis, concolic execution, and constraint solving.

**Case Study:**
To better understand why TortoiseFuzz performs well, we conducted a case study. This will be discussed in detail in the following section.