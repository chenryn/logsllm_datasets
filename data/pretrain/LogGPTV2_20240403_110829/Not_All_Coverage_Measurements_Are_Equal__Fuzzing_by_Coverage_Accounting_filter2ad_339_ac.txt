discovery?
• RQ5: Is coverage accounting robust against the current
anti-fuzzing technique?
TABLE II: Compared fuzzers.
Fuzzer
AFL
AFLFast
Steelix
VUzzer
CollAFL
FairFuzz
T-fuzz
QSYM
Angora
MOPT
DigFuzz
ProFuzzer
Year
2016
2016
2017
2017
2018
2018
2018
2018
2018
2019
2019
2019
Type
greybox
greybox
greybox
hybrid
greybox
greybox
hybrid
hybrid
hybrid
greybox
hybrid
hybrid
Open
Y
Y
N
Y2
N
Y
Y3
Y
Y
Y
N
N
Target
S/B1
S/B
S
B
S
S
B
S
S
S
S
S
Select
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
1 S: target source code, B: target binary.
2 VUzzer depends on external IDA Pro and pintool, and instrumenting with
real-world program is too expensive to be scalable in our experiment
environment. Also, VUzzer did not perform as well as other hybrid tools
such as QSYM. Under such condition, we did not include VUzzer in
real-world program experiment.
3 Some components of these tools cannot work for all binaries.
A. Experiment Setup
Dataset. We collected 30 applications from the papers pub-
lished from 2016 to 2019. The applications include image
parsing and processing libraries, text parsing tools, an as-
sembly tool, multimedia ﬁle processing libraries, language
translation tools, and so on. We selected the latest version
of the testing applications by the time we ran the experiment.
One may argue that the experiment dataset is lack of other
test suites such as LAVA-M. We ﬁnd that LAVA-M does not
ﬁt for evaluating fuzzer effectiveness since it does not reﬂect
the real-world scenarios. We will further discuss the choice of
dataset in Section VII.
In our evaluation, we found that there are 18 applications
from which no fuzzers found any vulnerabilities. For ease
of demonstration, we will only show the results for the 12
applications with found vulnerabilities throughout the rest of
the evaluation.
Compared fuzzers. We collected recent fuzzers published
from 2016 to 2019 as the candidate for comparison, as shown
in Table II. We consider each fuzzer regarding whether or
not it is open-source, and whether or not it can run on our
experiment dataset. We ﬁltered tools that are not open-source
or do not scale to our test real-world programs and thus cannot
be compared. The remained compared fuzzers are 4 greybox
fuzzers (AFL, AFLFast, FairFuzz and MOPT) and 2 hybrid
fuzzers (QSYM and Angora). For detailed explanations for
each fuzzer, we refer readers to the footnote in Table II.
Environment and process. We ran our experiments on eight
identical servers with 32 CPU Intel(R) Xeon(R) CPU E5-
2630 PI:EMAIL cores, 64GB RAM, and 64-bits Ubuntu
16.04.3 TLS. For each target application, we conﬁgured all
fuzzers with the same seed3 and dictionary set4. We ran each
fuzzer with each target program for 140 hours according to
CollAFL [18], and we repeated all experiments for 10 times
as advised by Klees et al. [29].
3We select an initial seed randomly from the testcase set provided by the
target program.
4We do not use the dictionary in the experiment.
We identiﬁed vulnerabilities in two steps. Given reported
crashes, we ﬁrst ﬁltered out invalid and redundant crashes
using a self-written script with ASan [46] then manually
inspected the remaining crashes and reported those that are
security related. For code coverage, we used gcov [20], which
is a well-known coverage analysis tool.
B. RQ1: Finding Zero-day Vulnerabilities
Table III shows the union of the real-world vulnerabilities
identiﬁed by TortoiseFuzz in the 10 times of the experiment.
The table presents each discovered vulnerability with its ID,
the corresponding target program, the vulnerability type and
whether it is a zero-day vulnerability. In total, TortoiseFuzz
found 56 vulnerabilities in 10 different types, including stack
buffer overﬂow, heap buffer overﬂow, use after free, and
double free vulnerabilities, many of which are critical and may
lead to severe consequences such as arbitrary code execution.
Among the 56 found vulnerabilities, 20 vulnerabilities are
zero-day vulnerabilities, and 15 vulnerabilities have been
conﬁrmed with a CVE identiﬁcation5. The result indicates that
TortoiseFuzz is able to identify a reasonable number of zero-
day vulnerabilities from real-world applications.
C. RQ2: TortoiseFuzz vs. Other Fuzzers in Real-world Ap-
plications
In this experiment, we tested and compared TortoiseFuzz
with greybox fuzzers and hybrid fuzzers on real-world appli-
cations. We evaluated each fuzzer by three metrics: discovered
vulnerabilities, code coverage, and performance.
Discovered vulnerabilities. Table IV shows the average and
the maximum numbers of vulnerabilities found by each fuzzer
among 10 repeated runs. The table also shows the p-value
of the Mann-Whitney U test between TortoiseFuzz and a
comparing fuzzer regarding the total number of vulnerabilities
found from all target programs in each of the 10 repeated
experiment.
Our experiment shows that TortoiseFuzz is more effective
than the other testing greybox fuzzers in ﬁnding vulnerabili-
ties from real-world applications. TortoiseFuzz detected 41.7
vulnerabilities on average and 50 vulnerabilities in maximum,
which outperformed all the other greybox fuzzers. Compar-
ing to FairFuzz, the second best-performed greybox fuzzers,
TortoiseFuzz found 43.9% more vulnerabilities on average
and 31.6% more in maximum. Additionally, we compared
the set of vulnerabilities in the best run of each fuzzer, and
we observed that TortoiseFuzz covered all but 1 vulnerability
found by the other fuzzer, and TortoiseFuzz found 10 more
vulnerabilities that none of the other fuzzers discovered.
For hybrid fuzzers, TortoiseFuzz showed a better result
than Angora and a comparable result with QSYM. Tortoise-
Fuzz outperformed Angora by 61.6% on the sum of average
and by 56.3% on the sum of the maximum numbers of
vulnerabilities for each target programs among the 10 times
of the experiment. TortoiseFuzz also detected more or equal
vulnerabilities from 91.7% (11/12) of the target applications.
TortoiseFuzz found slightly more vulnerabilities than QSYM
5CVE-2018-17229 and CVE-2018-17230 are contained in both exiv2 and
new_exiv2.
8
TABLE III: Real-world Vulnerabilities found by TortoiseFuzz.
Program
Version
exiv2
0.26
new_exiv2 0.26
exiv2_9.17 0.26
nasm
2.14rc4
gpac
0.7.1
libtiff
liblouis
4.0.9
3.7.0
ngiﬂib
0.4
libming
0_4_8
catdoc
0_95
tcpreplay
4.3
ﬂvmeta
1.2.1
ID
New
Vulnerability Type
(cid:88)
heap-buffer-overﬂow
CVE-2018-16336
(cid:88)
heap-buffer-overﬂow
CVE-2018-17229
(cid:88)
heap-buffer-overﬂow
CVE-2018-17230
(cid:88)
heap-buffer-overﬂow
issue_400
-
stack-buffer-overﬂow
issue_460
-
heap-buffer-overﬂow
CVE-2017-11336
-
invalid free
CVE-2017-11337
-
heap-buffer-overﬂow
CVE-2017-11339
-
invalid free
CVE-2017-14857
-
heap-buffer-overﬂow
CVE-2017-14858
-
stack-buffer-overﬂow
CVE-2017-14861
-
heap-buffer-overﬂow
CVE-2017-14865
-
heap-buffer-overﬂow
CVE-2017-14866
-
heap-buffer-overﬂow
CVE-2017-17669
-
heap-buffer-overﬂow
issue_170
-
heap-buffer-overﬂow
CVE-2018-10999
(cid:88)
heap-buffer-overﬂow
CVE-2018-17229
(cid:88)
heap-buffer-overﬂow
CVE-2018-17230
-
heap-buffer-overﬂow
CVE-2017-14865
heap-buffer-overﬂow
-
CVE-2017-14866
-
heap-buffer-overﬂow
CVE-2017-14858
null pointer dereference (cid:88)
CVE-2018-17282
-
stack-buffer-under-read
CVE-2018-8882
-
stack-buffer-over-read
CVE-2018-8883
-
null pointer dereference
CVE-2018-16517
null pointer dereference
-
CVE-2018-19209
-
CVE-2018-19213 memory leaks
null pointer dereference (cid:88)
CVE-2019-20165
(cid:88)
heap-use-after-free
CVE-2019-20169
(cid:88)
CVE-2018-21017 memory leaks
(cid:88)
Segment Fault
CVE-2018-21015
(cid:88)
heap-buffer-overﬂow
CVE-2018-21016
-
issue_1340
heap-use-after-free
-
heap-buffer-overﬂow
issue_1264
-
heap-buffer-over-read
CVE-2018-13005
-
heap-use-after-free
issue_1077
-
double-free
issue_1090
(cid:88)
heap-buffer-overﬂow
CVE-2018-15209
(cid:88)
CVE-2018-16335
heap-buffer-over-read
-
stack-buffer-overﬂow
CVE-2018-11440
-
memory leaks
issue_315
(cid:88)
stack-buffer-overﬂow
issue_10
(cid:88)
heap-buffer-overﬂow
CVE-2019-16346
(cid:88)
heap-buffer-overﬂow
CVE-2019-16347
-
CVE-2018-11575
stack-buffer-overﬂow
-
CVE-2018-11576
heap-buffer-over-read
-
CVE-2018-13066 memory leaks
-
(2 similar crashes) memory leaks
-
memory leaks
crash
-
crash
Segment Fault
-
heap-buffer-underﬂow
CVE-2017-11110
(cid:88)
heap-buffer-overﬂow
CVE-2018-20552
(cid:88)
CVE-2018-20553
heap-buffer-overﬂow
null pointer dereference (cid:88)
issue_13
issue_12
heap-buffer-overﬂow
-
on average and equal vulnerabilities in maximum. For each
target program, speciﬁcally, TortoiseFuzz performed better
than QSYM in 7 programs, equally in 2 programs, and worse
in 3 programs. Based on the Mann-Whitney U test,
the
result between TortoiseFuzz and QSYM was not statistically
signiﬁcant, and thus we consider TortoiseFuzz comparable to
QSYM in ﬁnding vulnerabilities from real-world programs.
Furthermore, we compared the union set of the discovered
vulnerabilities across 10 repeated runs between TortoiseFuzz
and QSYM. While TortoiseFuzz missed 9 vulnerabilities
found by QSYM, only one of them is a zero-day vulnerability.
9
The zero-day vulnerability is in the parse_mref function
of nasm. We analyzed the missing cases and found that, some
of the vulnerabilities are protected by conditional branches
related to the input ﬁle, which does not belong to any of our
metrics and thus the associated inputs cannot be prioritized.
Overall, our experiment results show that TortoiseFuzz
outperformed AFL, AFLFast, FairFuzz, MOPT, and Angora,
and it is comparable to QSYM in ﬁnding vulnerabilities.
Code coverage. In addition to the number of found vul-
nerabilities, we measured the code coverage of the testing
fuzzers across different target programs. Although coverage
accounting does not aim to improve code coverage, measuring
code coverage is still meaningful, as it is an import metrics
for evaluating program testing techniques. We also want to
investigate if coverage accounting affects code coverage in
the fuzzing process.
To investigate the impact on code coverage caused by
coverage accounting, we compare the code coverage between
TortoiseFuzz and AFL, the fuzzer based on which coverage
accounting is implemented. Table V shows the average code
coverage of all fuzzers performing on the target programs,
and Table VI shows the p-value of the Mann-Whitney U test
between TortoiseFuzz and other fuzzers. Based on Table V,
we observe that TortoiseFuzz had a better coverage than AFL
on average for 75% (9/12) of the target programs. In terms of
statistical signiﬁcance, 3 out of 12 programs are statistically
different in coverage, and TortoiseFuzz has a higher average
value for all the three cases, which implies that TortoiseFuzz
is statistically better than AFL in code coverage. Therefore,
coverage accounting does not affect code coverage in fuzzing
process.
Comparing TortoiseFuzz to other fuzzers, we observe that
although TortoiseFuzz does not aim to high coverage,
its
performance is fair among all fuzzers. Most of the results are
not statistically signiﬁcant between TortoiseFuzz and AFL,
AFLFast, and FairFuzz. Between TortoiseFuzz and MOPT,
TortoiseFuzz performed statistically better in three cases,
whereas MOPT performed statistically better in two cases.
TortoiseFuzz’s results are statistically higher than that of
Angora in most cases, not as good as that of QSYM.
Furthermore, we study the variability of the code coverage
of each testing fuzzer and target program, shown in Figure 2
and Table VII. We observe from the ﬁgure that the perfor-
mance of TortoiseFuzz in code coverage is stable in most of
the testing cases: the interquartile range is below 2% for 11
out of 12 of the testing programs.
Performance. Given that TortoiseFuzz had a comparable
result with QSYM, we compare the resource performance
of TortoiseFuzz to that of the hybrid fuzzer QSYM. For
each of the 10 repeated experiment, we logged the memory
usage of QSYM and TortoiseFuzz every ﬁve seconds, and we
show the memory usage of each fuzzer and each targeting
program in Figure 3. The ﬁgure indicates that TortoiseFuzz
spent less memory resources than QSYM, which reﬂects the
fact that hybrid fuzzers need more resources to execute heavy-
weighted analyses such as taint analysis, concolic execution,
and constraint solving.
Case study. To better understand the internal of why Tortoise-
TABLE IV: The Average Number of Vulnerabilities Identiﬁed by Each Fuzzer.
AFL
Grey-box Fuzzers
AFLFast
FairFuzz
MOPT
Hybrid Fuzzers
Angora