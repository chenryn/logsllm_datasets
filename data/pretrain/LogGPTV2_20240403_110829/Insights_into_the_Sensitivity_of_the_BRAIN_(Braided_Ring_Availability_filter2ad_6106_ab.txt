software  to  select  the  first  redundant  data  copy.  In 
addition,  the  partitioning and  fault  detection  guarantees 
of such approaches are excellent and application software 
is independent from platform software.  
Seeing  the  advantage  of  high-integrity  compute  for 
the application software development (as the automotive 
domain does with the use of fail-silent ECUs), the model 
assumes  that  any  single  integrity  violation  by  the 
platform  may  have  safety 
implications,  because  a 
computed  and  distributed  data  value  may  be  used  by 
multiple, possibly replicated, actuators. 
Looking at safety from an availability perspective, the 
model is concerned about redundancy exhaustion leading 
to  isolation  of  components  and loss  of  communication, 
e.g., due  to  components  or  link  loss.  If  communication 
between  distributed  components  cannot  be  guaranteed 
anymore,  we  assume  safety  is  affected.  A  detailed 
description  of  the  model  and  what  leads  to  loss  of 
communication  (availability)  or  decreased  integrity  is 
given below. 
4.1. Model Parameters 
A  by-wire  architecture  typically  consists  of  several 
components  connected  to  the  network.  The  number  of 
components  depends  on 
the  detailed  architectural 
approach. We expect 8 to 12 nodes directly connected to 
the network to support connections to the units support-
ing 
transmission,  engine,  distributed  actuators  and 
sensors, and control computers [1][11][23][29]. 
Commercial  transport  airplanes  are  operational  for 
missions averaging 3-10 hours, with major checks every 
couple hundred hours with different levels of overhauls. 
In the automotive space, a car is driven on average for 
4000  hours  [28].  Guaranteeing  safety  over  such  long 
mission periods without checks may not be economically 
viable. We also expect that the car electronics may not be 
equipped  or  may  not  be  able  to  perform  the  necessary 
scrubbing  activities  to  detect  latent  faults.  Thus,  we 
assume  that  at  every  major  service—similar  to  service 
procedures  in  the  aerospace  domain,  and  as  is  more 
common because of increased levels of diagnosis [28]—
the  car’s  by-wire  electronic  and  wire  loom  will  be 
checked  for correctness and latent faults. Major service 
intervals  where  all  latent  errors  can  be  detected  are 
assumed to be in the order of 150 hours. This equates to 
about  5250  miles  (8450  km)  at  35mph  (56kph).  Such 
service  intervals  are  currently  not  mandated  in  the 
automotive  industry  in  most  countries,  though  they  are 
recommended  during  the  vehicle  warranty  period.  The 
results of this paper could be used to consider impacts of 
service  intervals  on  safety.  Error  detection  coverage  at 
service is assumed to be largely perfect as manufacturing 
level  testing  can  be  deployed  for  critical  circuits.  The 
service  is  expected  to  scrub  all  essential  FDIR  (fault 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:38:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007detection,  isolation,  and  recovery)  circuits  using  scan 
chains [32], similar to production-level tests.  
Component failures are detected with coverage that is 
based  on 
the  underlying  network  architecture,  as 
described below. Once detected, the operation of the by-
wire system will be extended for a certain interval. The 
average transit time by car for transit from home to work 
is  ½  hour  according  to  U.S.  census  data  [37].  It  is 
assumed  that  an  extension  period  of  1  hour  should  be 
sufficient to return home and/or drive to a repair facility 
in case of a detected error. The models will contrast the 
approach by running models with near immediate repair 
(1 minute), no repair (modeled as very low repair rate of 
10-10)  and  some  repair  intervals  around  one  hour.  For 
extended  operation  reference,  Boeing  777’s  ETOPS 
rating allows the airplane to fly for up to 180 minutes on 
a single engine. The maximum operating time for engine 
electronics is 125 hours if the time of fault occurrence is 
known [31]. 
Component failure rate parameter ranges were chosen 
based  on  our  experience  with  similar technologies,  and 
reliability models were determined by CALCE [27] and 
according  to  MIL-HDBK-217  [26]  (note  that  MIL-
HDBK-217  is  no  longer  maintained,  but  a  good  public 
source  for  reliability  data).  Connector  failure  rates  are 
depend on the chosen connector type. MIL-HDBK-217 is 
used 
ranges. 
Connectors  may  also  not  perfectly  fit  the  exponential 
distribution (i.e. constant failure rate), but the model and 
requirements  to  quickly  solve  the  model  for  lot  of 
different  model  parameters  forces  us  to  make  this 
assumption. We assume link failure rates are dominated 
by connector failure rates. 
to  determine 
connector 
reliability 
We  use  a  hybrid  component  fault  model  where  the 
fail-stop failures are assumed to be higher than arbitrary 
node  faults,  as  a  node  is  likely  to  be  part  of  an  ECU 
(electronics  control  unit)  or  an  LRU  (line  replaceable 
unit). The reliability of such parts (LRU or ECU) is often 
driven by the power supply unit and a significant number 
of  supply  components  leading to  a  low  MTBF  rate  but 
with  a  benign  system  behavior  like  fail-stop.  The  fail 
arbitrary  behavior  is driven  by  the  communication  chip 
that 
is  performing  forwarding,  checking,  protocol 
activities  etc.  This  behavior  is  probably  unboundable; 
thus, the arbitrary behavior in a failure case. The failure 
rates of those components are assumed to be  similar to 
the  reliability  numbers  of  the  communication  chip  (or 
chip where the communication chip is part of).  
Table 1 is an overview of the parameter values used 
for the BRAIN and dual-star model and the values that 
we  expect  to  be  most  likely  (most  representative  value 
(MRV  in  the  table)).  These  most  representative  values 
are  used  when  several  other  parameters  are  varied  to 
show the sensitivity to variation.  
Table 1: Overview of parameters used in models 
Parameter 
Values 
5x10-6, 10-5, 2x10-5 
7, 8, 9, 10, 11, 12, 13, 14 
½, 1, 20, 50, 100, 120, 150, 
200, 250, 3k, 4k, 5k, 6k, 7k 
5x10-7, 10-6, 2x10-6, 5x10-6 
Number of active 
nodes 
Mission interval 
[hours] 
Link failure rate 
[failures/hour] 
Component failure 
rate (fail-stop) 
[failures/hour] 
Component failure 
rate (arbitrary) 
[failures/hour] 
10-10 (no repair), 0.1 (10 h), 
Repair rate (exten-
ded operation time) 
0.5 (2 h), 1 (1 h), 60 (1 min.; 
[1/hours] 
near immediate repair) 
4.2. Description of BRAIN Model 
5x10-8, 10-7, 5x10-7 
MRV 
10 
150 
10-6 
10-5 
10-7 
1 
The  BRAIN  can  guarantee  platform  availability  and 
integrity,  and  thus  safety,  as  long  as  there  is  either 
(1) one  communication  path  with  full  propagation 
integrity from the sender to all receivers or (2) two paths 
from  the  sender  to  the  receiver  where  the  receiver  can 
perform bit-for-bit comparison between the two paths. In 
case 1, a single arbitrary component can be tolerated on 
the BRAIN because each receiver has one path from the 
sender. Each node on the path is checking the direct and 
skip  links  bit-for-bit  for  agreement,  then  signaling  the 
result at the end via an integrity signaling field [18]. The 
bit-for-bit  comparison  of  skip  and  direct  link  prevents 
any  arbitrarily  faulty  node  from  corrupting  data  during 
propagation without being detected. Similarly, for single 
link or other benign component faults, the data will also 
reach each node on the ring with full integrity. Ad (2), 
for multiple benign faults (fail-stop), all receiving nodes 
detect  the  multiple  fault  scenario  because  the  integrity 
field at the end of the data indicates loss of integrity from 
both directions  on the ring.  Once  detected,  the receiver 
can  perform  bit-for-bit  comparison  of  the  two  copies 
received from each direction and still assure full integrity 
of the data. 
Medium  availability  is  enforced  by  the  guardian 
mechanisms,  which are  performed  by  each node  for  its 
two  direct  neighbors.  Synchronization  is  guaranteed  as 
long 
send 
synchronization messages.  
self-checking 
enough 
The  SURE  model  defines  state  spaces  for  the  fail-
silent and arbitrary component failures, link failures, and 
self-checking pair failures (i.e. the link between the pairs 
or at least one of the two nodes has failed). Link failures 
are assumed to be benign (e.g. a link is broken or not, but 
does  not  “corrupt”  data  integrity).  A  special  state  to 
model  the  loss  of  connectivity  in  one  ring  direction  is 
also  modeled.  Transitions  between  states  are  guided  by 
failure or repair rates. Details of the state space and its 
pairs 
can 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:38:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007transitions  are  not  in  the  scope  of  this  paper,  but  we 
modeled the above described behaviors.  
ASSIST/SURE/STEM  requires  definition  of  “death 
states,” which define when the integrity and availability 
of the communication can no longer be guaranteed due to 
faults. These states are at a higher level:  
•  More than one arbitrary component fault present,  
•  An arbitrary fault with any other fault combination 
(link or benign component fault), 
•  The  connectivity  from  a  sender  to  each receiver  is 
less  than  two  paths  (which  is  either  two  fail-stop 
component  faults  or multiple  link  failures  occurred 
where  connectivity  from  sender  and  receivers  via 
two paths is broken), or  
•  All self-checking pairs failed and protocol execution 
is  compromised  (due  to  link  failure  between  self-
checking pairs or component failure). 
As  the  BRAIN  performs  bit-for-bit  comparisons  at 
each node  any  error  is  immediately  detected.  From  the 
detection of an error, extended operation is allowed until 
the faulty sub-component is repaired. The time to repair 
is a parameter, also referred to as the time for extended 
operation.  
In  addition  to  the  model  parameters  in  Table  1,  the 
BRAIN-specific model parameter (Table 2) is the num-
ber of self-checking pairs needed for protocol operation, 
such as clock synchronization, startup, and integration. 
 Table 2: Overview of BRAIN-specific parameters 
Parameter 
Number of self-
checking pairs 
4.3. Description of Dual-Star Model  
Values 
1, 2, 3, 4  
MRV 
3 
for 
replicated  architectures 
To  compare  the  BRAIN  to  alternative  architectures, 
we evaluated a commonly used architectural alternative, 
a dual-star model. Given the cost constraints, a dual-star 
model  seems  to  be  the  best  of  the  alternatives  of 
ring/star/bus  dual 
the 
following  reasons.  Pure  bus-based  architectures  suffer 
from spatial proximity faults and are likely excluded for 
by-wire  architectures.  Ring  architectures  (without  skip 
links)  have  low  reliability  due  to  the  missing  path  to 
circumvent (benign) faulty nodes [33] and masquerading 
faults for forwarded data. Combinations such as ring/star 
architectures  (e.g.  wagon  wheel  architecture)  can  be 
powerful  as 
some  possibility  of 
masquerading  faults,  but  they  can  introduce  reliability 
loss due to serialization of one communication path. 
remove 
In  the  star  model,  benign  and  arbitrary  component 
faults and link failures are modeled. Given the protocol 
dependencies  of  solutions  on  the  market  [24][25], 
solutions  are  thought  to  be  single-fault  tolerant  to 
arbitrary failures from a protocol perspective.  
For  a  dual  star  model,  evaluation  of  inline  integrity 
approaches and their effect on safety is especially hard to 
they 
quantify, as it is very hard to evaluate the effects of an 
undetected  error  on  the  application.  E.g.,  what  are  the 
effects of an erroneous guardian (star) on data integrity? 
The model assumes that any undetected error may have a 
safety impact. As the two communication paths in a dual-
star  network  architecture  are  used  for  availability  with 
inline integrity protection, any arbitrary  faulty  star may 
have an effect.  
If a star is arbitrarily faulty, the faulty star is detected 
with a near 100% probability due to the integrity check 
(e.g.,  a  CRC).  Yet,  despite  the  high  probability  of 
detecting  the  star  error,  the  probability  of  undetected 
errors  and,  thus,  data  integrity  violations  remains.  This 
probability of undetected errors depends on the strength 
of  the  inline  integrity  mechanism  deployed  to  cover 
failures  of  the  intermediate  device.  Currently,  deployed 
dual-star networks  [24][25]  use  a  24-bit  CRC  for  error 
detection.  Assuming  a  uniform  failure  distribution  for 
failures of the central guardian device affecting a frame, 
the  probability  of  an  undetected  integrity  failure  of  a 
frame is 2-24 (about 5.96x10-8). 
At 5 Mbit/s, it takes 100 µs to send a frame with an 
average frame length of 500 bits. Say the network is 50% 
loaded, then 1.8x107 frames would be sent per hour. This 
rate would lead to about 1 (=2-24 x 1.8 x 107) undetected 
frame per hour once a star is faulty. 
indicated 
for  Ethernet 
Internal  Honeywell  explorations  of 
the  CRC32 
polynomial  used 
the 
probability of undetected errors is increased from 2-32 to 
2-28 for reasonable failure modes in intermediate relaying 
devices (such as switches or guardians) [30]. Such failure 
modes  are  characterized  by 
relaying  device 
introducing systematic errors (such as a stuck at 0 or 1 bit 
every 32 bits  of a frame). Such faults may be common 