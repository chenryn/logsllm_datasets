title:Measuring vote privacy, revisited
author:David Bernhard and
V&apos;eronique Cortier and
Olivier Pereira and
Bogdan Warinschi
Measuring Vote Privacy, Revisited
David Bernhard
University of Bristol
Bristol, United Kingdom
Véronique Cortier
CNRS, Loria, UMR 7503
Vandoeuvre-lès-Nancy,
F-54500, France
Olivier Pereira
Université Catholique de
Louvain – ICTEAM
B-1348 Louvain-la-Neuve,
Belgium
Bogdan Warinschi
University of Bristol
Bristol, United Kingdom
ABSTRACT
We propose a new measure for privacy of votes. Our measure
relies on computational conditional entropy, an extension of the
traditional notion of entropy that incorporates both information-
theoretic and computational aspects. As a result, we capture in a
uniﬁed manner privacy breaches due to two orthogonal sources of
insecurity: combinatorial aspects that have to do with the number
of participants, the distribution of their votes and published elec-
tion outcome as well as insecurity of the cryptography used in an
implementation.
Our privacy measure overcomes limitations of two previous ap-
proaches to deﬁning vote privacy and we illustrate its applicability
through several case studies. We offer a generic way of applying
our measure to a large class of cryptographic protocols that includes
the protocols implemented in Helios. We also describe a practical
application of our metric on Scantegrity audit data from a real elec-
tion.
Categories and Subject Descriptors
H.1.1 [Models and Principles]: Systems and Information The-
ory—Information Theory; K.4.1 [Computers and Society]: Public
Policy Issues—Privacy
General Terms
Measurement, Security, Theory
Keywords
Voting, Privacy, Entropy, Cryptography
1.
INTRODUCTION
The design and analysis of voting systems has a long and rich
history. Existing systems range from traditional paper-only bal-
lot systems to purely electronic voting schemes where voters may
vote from the privacy of their own computers (e.g., Helios [1, 2]
or Civitas [11]) and also include hybrid systems that make use of
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.
paper ballots but where computers facilitate the tally (e.g., Three-
Ballot [28], Prêt-à-Voter [29] and Scantegrity [10]).
Of the many security properties that voting systems should sat-
isfy, privacy of votes is of central concern. The development of rig-
orous security models for this important property started with the
work of Benaloh [12, 3] but soon evolved towards the related (but
seemingly much stronger) notions of receipt-freeness and coercion
resistance [4, 24, 19]. Achieving these stronger notions is certainly
desirable but it seems to come at the expense of efﬁciency and us-
ability. It should then come as no surprise that systems in use (e.g.
Helios) chose usability over coercion-resistance and aim to achieve
“only” vote privacy. The study of this notion has only recently
started to receive more attention with new models being developed
in both symbolic models [13] and computational ones [21, 23, 5].
Models for related notions like conﬁdential message transmis-
sion serve as a good but insufﬁcient source of inspiration: unlike
in those applications, vote privacy is not absolute but relative to
speciﬁc election bylaws and voter choices. An extreme but rele-
vant example is that of a voting system that discloses the number
of votes received by each candidate. Such a system essentially re-
veals how each voter voted in the improbable but not impossible
event that all voters vote for the same person. Yet classifying the
system as insecure is clearly undesirable and one should search for
a more nuanced classiﬁcation.
A second difference concerns the information that the adversary
tries to learn. In other scenarios where privacy is important it is usu-
ally clear what information the adversary targets (e.g. the privacy
of plaintexts for the case of encryption). In contrast, adversaries
against voting protocols may be interested in many possible targets
ranging from how some individual voted, to complex relations be-
tween votes (e.g. have certain persons voted in the same way, or has
a certain subset of voters supported a candidate more than another
subset [2]). These two examples perfectly reﬂect the shortcom-
ings of existing models for vote privacy which either target speciﬁc
classes of protocols or are limited in the class of adversarial targets
that they consider.
Contributions
In this paper we motivate and propose new privacy measures for
voting schemes. We develop our deﬁnitions in two steps. The start-
ing point is an information theoretic variant which, very roughly,
declares the privacy of the information that the adversary targets to
be the entropy left in the targeted information given what the ad-
versary learns during the election process. This deﬁnition is too
strong as it essentially requires security against unbounded adver-
saries and would declare practical systems where encryptions of
votes are made public as completely insecure. In the second step,
941we extend the applicability of our deﬁnition to systems where secu-
rity is ensured only against computationally bounded adversaries.
We do so by replacing information theoretic entropy with a con-
ditional computational entropy which we introduce. We obtain a
privacy measure that is a function of three parameters: the distri-
bution D on the votes of honest parties, the information that is the
target of the adversary T , and the voting protocol π (and implicitly
the tallying function which the voting protocol computes). We now
discuss some of the features and shortcomings of our deﬁnition.
Simplicity. Entropy has long been identiﬁed and used as a nat-
ural measure of the uncertainty (and therefore privacy) of criti-
cal information [31]. Since our deﬁnition ultimately relies on en-
tropy it inherits its associated intuition that has been used in many
other contexts before (e.g., anonymity [8], leakage [32], informa-
tion ﬂow [32]). The move to computational entropy, although a
bit technical, preserves this intuition and extends the applicability
of the information theoretic approach to systems that employ cryp-
tography.
Generality. The rather straightforward approach that we took in
deﬁning our privacy measure turns out to be quite powerful. We
establish a link between a particular case of our notion and one
recently proposed by Küsters, Truderung, and Vogt [23]:
in the
purely information-theoretic case, their notion can be obtained for
a very speciﬁc distribution of the votes and by a particular choice of
the parameters of our measure. We also show how a previously in-
troduced computational notion [5] can be seen as a tool for moving
from the computational to the IT version of our measure.
We keep our notion of privacy as independent as possible from
the details of any ﬁxed execution model. In particular, our deﬁni-
tion requires and employs only an abstract notion of an adversarial
view. Deﬁning such a view is system/execution model dependent
but on the one hand the details of how this is done are standard, and
on the other such details are irrelevant to the understanding of our
privacy measure and would only obscure our approach.
Usability. The general approach we took makes our measure of
privacy applicable to a large variety of systems. The relaxation to
computational entropies allows to meaningfully account for privacy
breaches that are due to weak cryptography, the underlying use of
entropy allows capturing insecure ways of releasing results, and
the inclusion of an arbitrary distribution on the honest votes allows
measuring the impact on privacy of some a priori knowledge of the
votes the adversary may have. Our privacy measure could be used
to identify systems that are clearly insecure (those for which the
measure of privacy is always close to 0). However, we envisage its
primary usage to be in comparing the level of privacy that different
parameters and systems ensure. For example, consider a situation
where one has to choose between different tallying systems, say
one which reveals only the winner of an election, one where the
number of votes each candidate obtain is revealed, or one that re-
veals a permutation of the individual ballots. While there is clearly
a difference in the privacy offered by these options, our measure
helps in understanding the different choices within the parameters
of the election (e.g. the loss of privacy may strongly depend on the
number of voters). Our approach can not only be applied to analyze
a voting protocol but also to the data of a speciﬁc election. We dis-
cuss data from the 2009 Takoma Park election organized with the
Scantegrity system [7]. The privacy of the Scantegrity protocol has
been studied by Küsters et al. [22]; we do not redo such an analysis
in this paper but we study privacy implications that arise from the
speciﬁc results and audit data of one election:
• The audit data, which contain anonymized ballots, are enough
to favor an Italian attack as soon as there are more than 2 can-
didates on a single question. Indeed, while almost all possi-
ble ballot ﬁllings appear on the bulletin board for questions
with two candidates, approximately 75% of the possible bal-
lots do not appear for questions with three candidates.
• If someone obtains access to the receipts of voters, which are
not expected to be kept conﬁdential, then the privacy of those
voters might be fully compromised. This is for instance the
case for one speciﬁc voter in one of the six wards in this
election.
We think that our observations could motivate some adaptations in
the Scantegrity for future elections.
Deﬁnitional obstacles. Entropy is an appealing notion with a
well-established place in deﬁning privacy of communication. Its
uses in the context of our work raises two speciﬁc obstacles that we
needed to overcome. In general, computational entropies (involve
an existential quantiﬁer over an inﬁnite class of simulators and) are
difﬁcult to compute. For systems that do not involve cryptography
we show that our privacy notion collapses to an information theo-
retic entropy which can be computed in a similar manner as in pre-
vious work [23]. For systems that involve cryptography we provide
a theorem which allows the following two-step approach. First,
show cryptographic indistinguishability from an idealized system
(essentially security in the sense of a computational model [5]).
This technique is common in cryptography and has already been
informally applied to voting protols [21, 23]. In the second step we
can simply use information-theoretic entropy.
A second difﬁculty is that there are many different ﬂavors of
entropy and it is not immediately clear which of the many well-
established variants should form the basis of our notion. It turns
out that there is no unique correct answer to this question as the
different notions reﬂect related but different aspects of security (e.g.
worst-case versus average-case insecurity). We study some of the
most prominent choices and clarify the applicability of the different
options through examples.
2. RELATED WORK AND LIMITATIONS
The quantiﬁcation of privacy has been studied from many differ-
ent angles. Our paper is concerned only with a small part of this
research area: how to deal with cryptographic constructions in an
analysis of privacy of voting systems. To place our paper in con-
text, we begin by reviewing some work that is relevant to our area
and point out the limitations of some existent approaches.
The work on privacy in the context of anonymous communica-
tion (see Chaum [9] and numerous subsequent work) is mostly fo-
cused on trafﬁc analysis techniques. While some voting protocols
might rely on anonymous communication to ensure privacy, this
is not necessary, and it is often possible to have elections offering
private voting without anonymous channels [12]. Besides, trafﬁc
analysis is often based on getting statistics from repeated or corre-
lated events, while the task of secure function evaluation is a one-
shot procedure (one is not able to repeat an election for instance).
The work on database privacy, including the differential privacy
approach [15] for example, is interested in limiting whether the ad-
dition or removal of a record in a database might affect the outcome
of a statistic. While, at ﬁrst sight, auctions or voting can be seen
as taking a statistic on a set of records, the concerns are again dif-
ferent: we actually expect that one single record might completely
change a statistic (by changing the highest bid for instance) and still
want to consider that an auction or election may offer some level of
privacy even though it does not provide any differential privacy.
The cryptographic literature also contains numerous examples
of deﬁnitions of what it means for a protocol to offer privacy (see,
e.g., Goldreich et al. [18]). Their work however concentrates on ex-
pressing that a protocol offers as much privacy as an ideal task, but
942not on giving meaningful measures of privacy loss when a privacy
gap exists.
Early work on vote privacy. The same criticism applies to early
deﬁnitions of privacy proposed for voting by Benaloh [12, 3]. Fur-
thermore, this work focuses on the comparison of honest vote as-
signments that offer the same sub-tally, which seems too restric-
tive for general tallying functions (e.g. if the tallying function only
announces the winner).
It also does not capture the information
that an adversary can learn if it knows the expected distribution
of the votes in advance. Privacy is also mentioned in several pa-
pers deﬁning receipt-freeness [4, 24] or coercion-resistance [19,
33, 21]. While privacy informally seeks to hide information on
votes, receipt freeness and coercion resistance are stronger proper-
ties that require that a voter cannot prove how he/she voted even if
he is willing to (or forced to) do so. However, receipt freeness and
coercion-resistance may simply not be satisﬁed for some systems
that are designed for low-coercion environments such as Helios [1,
2].
Symbolic models. Vote privacy has also been deﬁned in the con-
text of symbolic models, where messages are represented by terms.
For example in [13], a protocol is said to preserve privacy if an at-
tacker cannot detect when two votes are swapped. This again does
not apply to all voting scenarios. For example, if voters can give a
score of 0, 1, or 2, it may be the case that an attacker cannot distin-
guish a pair of votes (0, 2) from (2, 0) but could well distinguish
(0, 2) from (1, 1). Of course, symbolic models also have the usual
drawback of being too abstract, possibly missing attacks that occur
with only some probability (not 1 nor negligible).
Computational privacy notions. In [5], the authors deﬁne a game-
based notion of privacy that is used to analyze the privacy of He-
lios [2]. Their deﬁnition however is tailored to a class of voting
schemes where casting a vote corresponds to submitting an en-
crypted ballot to some bulletin board. This is of course not the
case for all voting systems (see e.g. ThreeBallot [28]).
Küsters, Truderung, and Vogt provide a privacy deﬁnition for
voting which is closest to the notion we propose [23]. Very roughly,
they measure the difference observed by an attacker when an (hon-
est) voter changes his vote, while all of the other (honest) votes
follow a given distribution. The authors show how to analyze the
privacy offered by several voting protocols such as ThreeBallot and
VAV.
Their deﬁnition is the ﬁrst that can be used to capture information-
theoretic aspects (e.g. how much information can be inferred from
the results of the election) but is limited in several respects. First,
their deﬁnition is more restricted than ours with respect to the pos-
sible distributions on the honest voters’ choices: every honest voter
except one “target” voter casts a vote drawn independently from
the same distribution. Secondly, they focus on wondering whether
an adversary can decide whether a single voter voted in one way
or another. These limitations exclude, for example, a scenario with
two voters who always cast opposite votes in a yes/no ballot and an
adversary who tries to tell which one voted “yes”; such a scenario
appears in a privacy notion using symbolic models [13]. Thirdly,
the following example explains why the deﬁnition of [23] is some-
times too strong (and then non informative) for some very natural
cases.
Consider an election with one million voters doing approval vot-
ing on 100 candidates, and where the outcome of the election is
a shufﬂed version of all ballots:
this is the view of the election
ofﬁcers doing the tally in a traditional paper-based election for in-
stance, or the view of everyone in an election relying on veriﬁable
mixnets for the tally. For such an election, whatever distribution
of votes is followed by honest voters, there will be at least two of
the 2100 possible votes, say v and v(cid:48), that appear with probability
at most 2−80 ≈ 106/2100. But then, an adversary who looks for
the votes v and v(cid:48) in the tally will be able to distinguish the cases
where a voter submitted a vote for v or for v(cid:48) with probability al-