Most work on security in the context of ML [31]–[33] has
focused on the integrity of model predictions [34]–[36] or on
providing guarantees of privacy to the training data [37]. Our
efforts on developing a proof-of-learning (or PoL) concept for
the training algorithm are instead, as illustrated by both of the
use cases discussed in § I, most relevant to two previous lines
of work: the ﬁrst is model stealing, the second is Byzantine-
tolerant distributed learning.
a) Model Ownership & Extraction: The intellectual
property of model owners can be infringed upon by an
adversary using model extraction attacks [5]. Most extraction
attacks targeting DNNs are learning-based: the adversary col-
lects a substitute dataset (i.e., consists of data from a similar
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1040
distribution or synthetic data), queries the victim model to
obtain labels, and then re-trains a surrogate model that is
functionally similar to the victim [5]–[9]. Attacks may also
use side-channel information [38].
There are currently two types of defenses against extraction
attacks: (a) restricting information released for each query
answered by the model [5], [39], and (b) assessing if a
suspected model is indeed a stolen copy of a victim model. The
latter can be done in two ways. If the model was watermarked,
one can query for the corresponding triggers [40]–[42]. If
that is not the case, one can use the training data directly to
perform dataset inference [43]. However, decreasing extrac-
tion efﬁciency by restricting information returned by queries
comes at the expense of the model’s utility [5], [44]–[46].
Similarly, watermarking trades utility with the robustness of
the watermark while additionally requiring the modiﬁcation of
the training process [40], [47]–[50]. Thus, watermarks may be
removed from a deployed model or made ineffective [51]–[53].
In contrast, our work does not impact training and produces
a PoL which is immutable (see § IV), we also do not restrict
information released at inference time.
C. Byzantine-tolerant distributed ML
In the second scenario we described in § I, we consider
a setting where a model owner wishes to distribute the
compute required to train a model across a pool of potentially
untrusted workers [54]. Each of these workers receives a few
batches of training data, performs some gradient descent steps,
and then regularly synchronizes parameters with the model
owner. In this distributed setting, we note that prior work has
studied training algorithms which are robust to the presence of
Byzantine [55] workers: such workers may behave arbitrarily
and return corrupted model updates to the model owner [11].
As we will introduce in § IV, veriﬁable PoL forms a defense
against DoS attacks in this context. In addition, our PoL may
be used to provide integrity guarantees by conﬁrming the
correctness of computations performed by the workers.
III. BACKGROUND ON MACHINE LEARNING
Throughout our work, we deﬁne [n] := {1, . . . , n}. Con-
sider a data distribution D of the form X×Y, such that X is the
space of inputs, and Y is the space of outputs. An ML model
is a parameterized function of the form fW : X → Y, where
W denotes the model parameters. For the purposes of this
work, we assume that these models are deep learning models
which requires additional terminology.
1) Model Architecture: A deep neural network is a function
comprised of many layers, each performing a linear-
transformation on their input with optional non-linear
activations [56]. The structure of these layers, e.g., the
number of neurons and weights, the number of layers l,
and activations is termed the model architecture.
2) Model Weights: The parameters of the deep learning
model are commonly called its weights. Each layer i ∈ [l]
is comprised of learnable weights denoted wi, including
the additional bias term. Collectively, we denote the set
of per-layer weights {w1,··· , wl} as W .
3) Random Initialization: Before training, each weight vec-
tor wi ∈ W requires an initial value. These values are
often randomly assigned by sampling from a distribution.
Values are sampled from a zero-centered uniform or
Gaussian distribution whose standard deviation is param-
eterized by either the number of neurons in the input
layer, the output layer, or both [57]–[59].
The ﬁnal set of parameters are learned by training the ML
model using empirical risk minimization [60]. A training
dataset is sampled from the data distribution Dtr ∼ D. The
expected risk of a model on this dataset is then quantiﬁed
using a loss: a real valued function L(fW (x), y) that is the
objective for minimization. The loss characterizes the discrep-
ancy between the model’s prediction fW (x) and the ground
truth y. A common example is the cross-entropy loss [61].
Training occurs in an iterative manner by continuously
sampling a (mini)batch of training data, without replacement,
from Dtr; each such iteration is called a step1. For each step,
stochastic gradient descent [62] updates the model’s parame-
ters to minimize the empirical risk by taking the gradient of
the loss with respect to the parameters. Thus, at each step
i ∈ [T ], we obtain a new set of weights Wi as follows:
(cid:80)
1
m
(x,y)∼Db
Wi = Wi−1 − η · ∇Wi−1 , ˆLi−1
(1)
where η is the learning rate hyperparameter, and ˆLi−1 =
L(fWi−1(x), y) denotes the average loss com-
puted over a random batch Db ⊆ Dtr of size m. An epoch is
one full pass through Dtr which contains S steps. The training
process overall has a total of E epochs. Thus, assuming the
size m of a batch is ﬁxed during training, training the model
requires a total of T = E · S steps.
IV. FORMALIZING POL
We wish to show that one can verify the integrity of the
training procedure used to obtain an ML model. This in turn
can also be used to show proof of ownership. We focus on
training because it induces the largest computational costs.
We note that
there is prior work in veriﬁable computing
investigating inference-time computation but that these were
not designed for training algorithms and require modifying the
algorithm to accommodate cryptographic primitives such as an
interactive proof system [63]–[65]. Instead, we formulate our
approach such that no changes need to be made to the model
architecture and training algorithm beyond additional logging.
This enables a seamless integration for model owners to create
PoL and make claims of having trained a model. Our approach
for PoL is naturally extended to two scenarios:
1) A party can claim ownership of a trained model fWT .
2) An entity outsources computation to some client (as in
then the results returned by the
distributed learning),
client (i.e., f c
WT
) can be trusted.2
The party performing the computation is referred to as the
prover T . To verify the integrity of its computation (either
for ownership resolution or in the outsourced computation
1One step corresponds to processing one batch of data.
2The superscript c denotes a computation executed locally at a client.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1041
scenario), T generates a certiﬁcate, henceforth referred to as
the Proof-of-Learning (or PoL) performed to obtain fWT .3
We denote such a PoL as P(T , fWT ). When the integrity of
the computation (ergo model ownership) is under question,
an honest and trusted veriﬁer V analyzes P(T , fWT ) and
determines its validity (i.e., a valid PoL implies that T
performed the computation required to obtain fWT ). Formally,
a valid PoL is one where each component is well-formed (refer
§ IV-C), and V can reconstruct the PoL in its entirety. An
adversary A is one who wishes to subvert this process.
A. Threat Model
Dishonest spooﬁng is any strategy that requires lesser
computational expenditure than that made by the prover in
generating the proof; we formally deﬁne this term in § VII-A.
The primary scenario we wish to mitigate against is the ability
of an adversary A to efﬁciently spoof P(T , fWT ), i.e., we
want to verify computation to train the model on the part of
the prover. By spooﬁng, A can claim to have performed the
computation required (to produce fWT , for example). Since
A has not expended (signiﬁcant) computational resources nor
trained the model to be able to produce fWT , they are unlikely
to have P(T , fWT ). Thus, A tries to create P(A, fWT ) that
passes veriﬁcation, even if that PoL is not valid. We consider
the following scenarios for spooﬁng:
(a) Retraining-based Spooﬁng: A aims to create the exact
same PoL for fWT as T i.e., P(A, fWT ) = P(T , fWT ).
(b) Stochastic Spooﬁng: A aims to create a valid PoL for
fWT , but this may not be the same as T ’s PoL i.e.,
P(A, fWT ) (cid:54)= P(T , fWT ).
(c) Structurally Correct Spooﬁng: A aims to create an invalid
PoL for fWT but such a PoL passes veriﬁcation for fWT .
(d) Distillation-based Spooﬁng: A aims to create a valid PoL
using a modiﬁed version of fWT (say f) i.e., P(A, f ) (cid:54)=
P(T , fWT ). Note that the adversarial approximation of
the model f (≈ fWT ) has the same test-time performance.
In our security analysis (see § VII), we comment on the
efﬁciency of the above spooﬁng strategies; for the adversary,
it is desirable that the aforementioned are dishonest spooﬁng
strategies. We assume the following adversarial capabilities:
1) A has full knowledge of the model architecture and
parameters (i.e., weights). In addition, A has access to
the loss function, optimizer, and other hyperparameters.
2) A has full access to the training dataset, and can modify
it. Note that the objective of A is not to infer sensitive
information from the dataset, but use it to spoof a PoL.
3) A does not have access to the various sources of ran-
domness used by T . These sources include randomness
associated with batching, parameter initialization, chosen
random seeds, and other intrinsic sources of randomness
such as hardware accelerators [66].
B. Protocol Overview
We deﬁne PoL in ML as a n ≥ 1-round protocol between
the prover T and veriﬁer V. The protocol is initiated by T
3The case with f c
is similar. For generality, we proceed to deﬁne our
work with reference to fWT .
WT
by (a) drawing on some source of randomness, or (b) using
some other parameters (with a valid PoL) for initialization of
its model parameters (W0); we will more formally deﬁne the
latter in § V-D. T then trains their ML model and obtains ﬁnal
parameters WT . Through training, T accumulates some secret
information associated with training; this information is used
to construct P(T , fWT ) which can be used to prove integrity
of the computation performed by T to obtain WT from W0.
To validate the integrity of the computation, V may query
T for the PoL and T returns a subset (or all of) the secret
information obtained during training. Using this knowledge,
V should be able to ascertain if the PoL is valid or not.
Desired Guarantees. A cannot (a) easily reconstruct the
secret information associated with P(T , fWT ) (needed for the
retraining-based spooﬁng strategy), or (b) efﬁciently recon-
struct another valid PoL P(A, fWT ) or P(A, f ) for f ≈ fWT .
In particular, the computational resources needed should (ide-
ally) be the same or more as the cost of valid proof generation.
We formalize the computational requirements below:
Property 1. Let CT denote a random variable representing the
cost (both computation and storage) associated with T
training fWT . Let CV denote the cost random variable of
the veriﬁcation procedure. We thus require that
E[CV ] ≤ E[CT ]
Property 2. Let CA be the cost random variable associated
with any spooﬁng strategy attempted by any A aside from
the honest strategy (i.e. training fWT ). We require that
E[CT ] ≤ E[CA]
Note here that the second property should hold no matter
which of the four scenarios from §IV-A we consider:
in
particular the cost of the adversary should be higher even if
they choose scenario (c) and form a structurally correct PoL
which is invalid but still passes veriﬁcation.
C. Deﬁning PoL
Deﬁnition 1 (PoL). For a prover T , a valid PoL is deﬁned
as P(T , fWT ) = (W, I, H, A) where all the elements of the
tuple are ordered sets indexed by the training step t ∈ [T ]. In
particular, (a) W is a set of model speciﬁc information that
is obtained during training, (b) I denotes information about
the speciﬁc data points used to obtain each state in W, (c)
H represents signatures of these training data points, and (d)
A that incorporates auxiliary information that may or may
not be available to A, such as hyperparameters M, model
architecture, optimizer and loss choices.
The information in Deﬁnition 1 encapsulates all the infor-
mation required to recreate (and consequently verify) a PoL.
T publishes some deterministic variant of W (e.g., encrypted
W). Our scheme should ensure that recreating the states in
W without knowledge of I, H and some designated subset of
elements in A is hard; this should dissuade any adversary in
recreating the prover T ’s PoL. In addition to this, we should
also ensure recreating WT without W is hard so that the
adversary cannot spoof (refer §VII) the PoL with a different
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1042
PoL P(A, fWT ) (cid:54)= P(T , fWT ). To this end, we require that
algorithms included within A be from a known accepted list
of algorithms, or have their own PoL (refer § V-D3). More
concretely, any PoL (and the strategy to generate the PoL)
should satisfy the following properties:
G1. Correctness: A PoL for fWT should be veriﬁable with
high probability if the prover T obtained this PoL by
training a model from a random initialization of the model
parameters and until their convergence to fWT .
G2. Security: If A is able to dishonestly spoof the PoL, then
it will be detected with high probability.
G3. Veriﬁcation Efﬁciency: Verifying the correctness of a
proof should ideally be computationally less expen-
sive than generating the proof. Additionally, veriﬁcation
should succeed even if the veriﬁer uses different hardware
than the prover.
G4. Model Agnostic: A proof generation strategy should be
general i.e., should be applicable to models of varying
nature and complexity.
G5. Limited Overhead: Generating the proof should induce
limited overhead to the already computationally expen-
sive training procedure.
G6. Concise Proof: The proof generated should be small with
respect to the number of steps of training (and ideally of
constant size).
V. A POL MECHANISM BASED ON GRADIENT DESCENT
Our proposal for generating a PoL is based on gradient
descent. At the core, our mechanism relies on the difﬁculty
to invert gradient descent. In this section, we simplify the
notation for brevity i.e., P(T , fWT ) is now P(fWT ).
A. Mechanism Overview
In our proposed mechanism, T reveals to V some of
the intermediate weights achieved during training as its PoL
P(fWT ). More speciﬁcally, T releases: (a) the values of
the weights (or model updates) at periodic intervals during
training, and (b) the corresponding indices of the data points
from the training set which were used to compute said model
updates. To ensure that A cannot copy the PoL as is, we
require that T encrypt their PoL P(fWT ) with V’s public
to obtain R := enc(P(fWT ), K pubV ), and then sign
key K pubV
it with T ’s own private key before publishing the PoL. The
proof (or its signature) can be timestamped, or published in
a public ledger. This ensures that verifying its validity is as
simple as a lookup operation. This prevents replay attacks,
where A would claim to have published the PoL ﬁrst.
To commence veriﬁcation, V ﬁrst veriﬁes the authenticity
of the signature using T ’s public key and proceeds to decrypt
the encrypted PoL using its private key K privV
. It then veriﬁes
the provenance of the initial weights W0. These are either
(a) sampled from the claimed initialization distribution, or (b)
come from a valid external source, i.e., have their own PoL.
See § V-D4 and § V-D3, respectively. Next, V queries T for the
data points required to compute a speciﬁc subset of updates in
W. There are two possibilities. Either the dataset is released
by T along with the PoL and is available to V immediately.
Alternatively, in a lazy veriﬁcation scenario (§ V-D1) , T can
delay the release of the exact data points to V until they are
explicitly queried. In such a case, T is necessitated to include
a signature (represented using function h(.)) of the training
data as part of the PoL. We require this as an abundance of
precaution so that an adversarial prover attempting structurally
correct spooﬁng (see § IV-A) cannot release a structurally
correct yet invalid PoL and then later attempt to synthesize
a dataset which would make this PoL valid.
The process of obtaining updates in W is similar to training
when aided by the information contained in I, H and A. In our
protocol we only retain hyperparameters M as our auxiliary
information in A. Thus, by querying this information, V can
recreate the updates in a speciﬁc subset by re-executing the
computation. By doing so, V is able to attest the computation
performed by T . We detail this veriﬁcation in § V-C.
B. PoL Creation
Algorithm 1 PoL Creation
Require: Dataset D, Training metadata M