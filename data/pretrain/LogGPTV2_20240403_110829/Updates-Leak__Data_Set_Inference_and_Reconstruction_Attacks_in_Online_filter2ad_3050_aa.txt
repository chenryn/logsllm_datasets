title:Updates-Leak: Data Set Inference and Reconstruction Attacks in Online
Learning
author:Ahmed Salem and
Apratim Bhattacharya and
Michael Backes and
Mario Fritz and
Yang Zhang
Updates-Leak: Data Set Inference and 
Reconstruction Attacks in Online Learning
Ahmed Salem, CISPA Helmholtz Center for Information Security; Apratim 
Bhattacharya, Max Planck Institute for Informatics; Michael Backes, Mario 
Fritz, and Yang Zhang, CISPA Helmholtz Center for Information Security
https://www.usenix.org/conference/usenixsecurity20/presentation/salem
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning
Ahmed Salem
CISPA Helmholtz Center
for Information Security
Apratim Bhattacharya
Max Planck Institute
for Informatics
Michael Backes
CISPA Helmholtz Center
for Information Security
Mario Fritz
CISPA Helmholtz Center
for Information Security
Yang Zhang
CISPA Helmholtz Center
for Information Security
Abstract
Machine learning (ML) has progressed rapidly during the past
decade and the major factor that drives such development is
the unprecedented large-scale data. As data generation is a
continuous process, this leads to ML model owners updating
their models frequently with newly-collected data in an online
learning scenario. In consequence, if an ML model is queried
with the same set of data samples at two different points in
time, it will provide different results.
In this paper, we investigate whether the change in the out-
put of a black-box ML model before and after being updated
can leak information of the dataset used to perform the update,
namely the updating set. This constitutes a new attack surface
against black-box ML models and such information leakage
may compromise the intellectual property and data privacy
of the ML model owner. We propose four attacks following
an encoder-decoder formulation, which allows inferring di-
verse information of the updating set. Our new attacks are
facilitated by state-of-the-art deep learning techniques. In par-
ticular, we propose a hybrid generative model (CBM-GAN)
that is based on generative adversarial networks (GANs) but
includes a reconstructive loss that allows reconstructing accu-
rate samples. Our experiments show that the proposed attacks
achieve strong performance.
1 Introduction
Machine learning (ML) has progressed rapidly during the
past decade. A key factor that drives the current ML develop-
ment is the unprecedented large-scale data. In consequence,
collecting high-quality data becomes essential for building
advanced ML models. Data collection is a continuous process,
which in turn transforms the ML model training into a con-
tinuous process as well: Instead of training an ML model for
once and keeping on using it afterwards, the model’s owner
needs to keep on updating the model with newly-collected
data. As training from scratch is often prohibitive, this is often
achieved by online learning. We refer to the dataset used to
perform model update as the updating set.
In this paper, our main research question is: Can different
outputs of an ML model’s two versions queried with the same
set of data samples leak information of the corresponding
updating set?. This constitutes a new attack surface against
machine learning models. Information leakage of the updating
set may compromise the intellectual property and data privacy
of the model owner.
We concentrate on the most common ML application –
classiﬁcation. More importantly, we target black-box ML
models – the most difﬁcult attack setting where an adversary
does not have access to her target model’s parameters but can
only query the model with her data samples and obtain the
corresponding prediction results, i.e., posteriors in the case
of classiﬁcation. Moreover, we assume the adversary has a
local dataset from the same distribution as the target model’s
training set, and the ability to establish the same model as the
target model with respect to model architecture. Finally, we
only consider updating sets which contain up to 100 newly
collected data samples. Note that this is a simpliﬁed setting
and a step towards real-world setting.
In total, we propose four different attacks in this surface
which can be categorized into two classes, namely, single-
sample attack class and multi-sample attack class. The two
attacks in the single-sample attack class concentrate on a
simpliﬁed case when the target ML model is updated with
one single data sample. We investigate this case to show
whether an ML model’s two versions’ different outputs indeed
constitute a valid attack surface. The two attacks in the multi-
sample attack class tackle a more general and complex case
when the updating set contains multiple data samples.
Among our four attacks, two (one for each attack class)
aim at reconstructing the updating set which are the ﬁrst
attempts in this direction. Compared to many previous attacks
inferring certain properties of a target model’s training set [11,
13, 20], a dataset reconstruction attack leads to more severe
consequences.
Our experiments show that indeed, the output difference of
the same ML model’s two different versions can be exploited
to infer information about the updating set. We detail our
USENIX Association
29th USENIX Security Symposium    1291
contributions as the following.
General Attack Construction. Our four attacks follow a
general structure, which can be formulated into an encoder-
decoder style. The encoder realized by a multilayer perceptron
(MLP) takes the difference of the target ML model’s outputs,
namely posterior difference, as its input while the decoder
produces different types of information about the updating
set with respect to different attacks.
To obtain the posterior difference, we randomly select a
ﬁxed set of data samples, namely probing set, and probe the
target model’s two different versions (the second-version
model is obtained by updating the ﬁrst-version model with an
updating set). Then, we calculate the difference between the
two sets of posteriors as the input for our attack’s encoder.
Single-Sample Attack Class. The single-sample attack class
contains two attacks: Single-sample label inference attack and
single-sample reconstruction attack. The ﬁrst attack predicts
the label of the single sample used to update the target model.
We realize the corresponding decoder for the attack by a
two-layer MLP. Our evaluation shows that our attack is able
to achieve a strong performance, e.g., 0.96 accuracy on the
CIFAR-10 dataset [1].
The single-sample reconstruction attack aims at recon-
structing the updating sample. We rely on autoencoder (AE).
In detail, we ﬁrst train an AE on a different set of data samples.
Then, we transfer the AE’s decoder into our attack model as
its sample reconstructor. Experimental results show that we
can reconstruct the single sample with a performance gain
(with respect to mean squared error) of 22% for the MNIST
dataset [2], 107.1% for the CIFAR-10 dataset, and 114.7%
for the Insta-NY dataset [6], over randomly picking a sample
afﬁliated with the same label of the updating sample.
Multi-Sample Attack Class. The multi-sample attack class
includes multi-sample label distribution estimation attack
and multi-sample reconstruction attack. Multi-sample label
distribution estimation attack estimates the label distribution
of the updating set’s data samples. It is a generalization of the
label inference attack in the single-sample attack class. We
realize this attack by setting up the attack model’s decoder
as a multilayer perceptron with a fully connected layer and a
softmax layer. Kullback-Leibler divergence (KL-divergence)
is adopted as the model’s loss function. Our experiments
demonstrate the effectiveness of this attack. For the CIFAR-10
dataset, when the updating set’s cardinality is 100, our attack
model achieves a 0.00384 KL-divergence which outperforms
random guessing by a factor of 2.5. Moreover, the accuracy
of predicting the most frequent label is 0.29 which is almost
3 times higher than random guessing.
Our last attack, namely multi-sample reconstruction attack,
aims at generating all samples in the updating set. This is
a much more complex attack than the previous ones. The
decoder for this attack is assembled with two components.
The ﬁrst one learns the data distribution of the updating set
samples. In order to achieve coverage and accuracy of the
reconstructed samples, we propose a novel hybrid generative
model, namely CBM-GAN. Different from the standard gen-
erative adversarial networks (GANs), our Conditional Best
of Many GAN (CBM-GAN) introduces a “Best Match” loss
which ensures that each sample in the updating set is recon-
structed accurately. The second component of our decoder
relies on machine learning clustering to group the generated
data samples by CBM-GAN into clusters and take the central
sample of each cluster as one ﬁnal reconstructed sample. Our
evaluation shows that our approach outperforms all baselines
when reconstructing the updating set on all MNIST, CIFAR-
10, and Insta-NY datasets.
2 Preliminaries
In this section, we start by introducing online learning, then
present our threat model, and ﬁnally introduce the datasets
used in our experiments.
2.1 Online Learning
In this paper, we focus on the most common ML task – clas-
siﬁcation. An ML classiﬁer M is essentially a function that
maps a data sample x ∈ X to posterior probabilities y ∈ Y ,
i.e., M : X → Y . Here, y ∈ Y is a vector with each entry
indicating the probability of x being classiﬁed to a certain
class or afﬁliated with a certain label. The sum of all values
in y is 1. To train an ML model, we need a set of data sam-
ples, i.e., training set. The training process is performed by a
certain optimization algorithm, such as ADAM, following a
predeﬁned loss function.
A trained ML model M can be updated with an updating
set denoted by Dupdate. The model update is performed by
further training the model with the updating set using the same
optimization algorithm on the basis of the current model’s
parameters. More formally, given an updating set Dupdate and
a trained ML model M , the updating process Fupdate can
be deﬁned as Fupdate : Dupdate,M → M (cid:48) where M (cid:48) is the
updated version of M .
2.2 Threat Model
For all of our four attacks, we consider an adversary with
black-box access to the target model. This means that the ad-
versary can only query the model with a set of data samples,
i.e., her probing set, and obtain the corresponding posteriors.
This is the most difﬁcult attack setting for the adversary [40].
We also assume that the adversary has a local dataset which
comes from the same distribution as the target model’s train-
ing set following previous works [13, 38, 40]. Moreover, we
consider the adversary to be able to establish the same ML
model as the target ML model with respect to model architec-
ture. This can be achieved by performing model hyperparam-
1292    29th USENIX Security Symposium
USENIX Association
eter stealing attacks [33, 47]. The adversary needs these two
information to establish a shadow model which mimics the
behavior of the target model to derive data for training her at-
tack model (see Section 3). Also, part of the adversary’s local
dataset will be used as her probing set. Finally, we assume
that the target ML model is updated only with new data, i.e.,
the updating set and the training set are disjoint.
We later show in Section 6 that the two assumptions, i.e.,
the adversary’s knowledge of the target model’s architecture
and her possession of a dataset from the same distribution as
the target model’s training set, can be further relaxed.
samples as her probing set, denoted by Dprobe. In this work,
the adversary picks a random sample of data samples (from
her local dataset) to form Dprobe. Choosing or crafting [33]
a speciﬁc set of data samples as the probing set may further
improve attack efﬁciency, we leave this as a future work. Next,
the adversary queries the target ML model M with all sam-
ples in Dprobe and concatenates the received outputs to form
a vector yprobe. Then, she probes the updated model M (cid:48) with
samples in Dprobe and creates a vector y(cid:48)probe accordingly. In
the end, she sets the posterior difference, denoted by δ, to the
difference of both outputs:
2.3 Datasets Description
For our experimental evaluation, we use three datasets:
MNIST, CIFAR-10, and Insta-NY. Both MNIST and CIFAR-
10 are benchmark datasets for various ML security and privacy
tasks. MNIST is a 10-class image dataset, it consists of 70,000
28×28 grey-scale images. Each image contains in its center a
handwritten digit. Images in MNIST are equally distributed
over 10 classes. CIFAR-10 contains 60,000 32×32 color im-
ages. Similar to MNIST, CIFAR-10 is also a 10-class balanced
dataset. Insta-NY [6] contains a sample of Instagram users’
location check-in data in New York. Each check-in represents
a user visiting a certain location at a certain time. Each lo-
cation is afﬁliated with a category. In total, there are eight
different categories. Our ML task for Insta-NY is to predict
each location’s category. We use the number of check-ins hap-
pened at each location in each hour on a weekly base as the
location’s feature vector. We further ﬁlter out locations with
less than 50 check-ins, in total, we have 19,215 locations for
the dataset. In Section 6, we further use Insta-LA [6] which
contains the check-in data from Los Angeles for our threat
model relaxation experiments.
3 General Attack Pipeline
Our general attack pipeline contains three phases. In the ﬁrst
phase, the adversary generates her attack input, i.e., posterior
difference. In the second phase, our encoder transforms the
posterior difference into a latent vector. In the last phase, the
decoder decodes the latent vector to produce different infor-
mation of the updating set with respect to different attacks.
Figure 1 provides a schematic view of our attack pipeline.
In this section, we provide a general introduction for each
phase of our attack pipeline. In the end, we present our strat-
egy of deriving data to train our attack models.
3.1 Attack Input
Recall that we aim at investigating the information leaked
from posterior difference of a model’s two versions when
queried with the same set of data samples. To create this pos-
terior difference, the adversary ﬁrst needs to pick a set of data
δ = yprobe − y(cid:48)probe
Note that the dimension of δ is the product of Dprobe’s car-
dinality and the number of classes of the target dataset. For
this paper, both CIFAR-10 and MNIST are 10-class datasets,
while Insta-NY is an 8-class dataset. As our probing set al-
ways contains 100 data samples, this indicates the dimension
of δ is 1,000 for CIFAR-10 and MNIST, and 800 for Insta-NY.