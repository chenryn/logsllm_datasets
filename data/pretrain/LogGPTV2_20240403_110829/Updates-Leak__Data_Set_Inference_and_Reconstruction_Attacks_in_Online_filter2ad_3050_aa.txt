# Title: Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning

**Authors:**
- Ahmed Salem, CISPA Helmholtz Center for Information Security
- Apratim Bhattacharya, Max Planck Institute for Informatics
- Michael Backes, CISPA Helmholtz Center for Information Security
- Mario Fritz, CISPA Helmholtz Center for Information Security
- Yang Zhang, CISPA Helmholtz Center for Information Security

**Conference:**
- Proceedings of the 29th USENIX Security Symposium
- August 12–14, 2020
- ISBN: 978-1-939133-17-5
- Open access sponsored by USENIX
- [Presentation Link](https://www.usenix.org/conference/usenixsecurity20/presentation/salem)

## Abstract

Machine learning (ML) has advanced significantly over the past decade, driven by the availability of large-scale data. As data generation is a continuous process, ML model owners frequently update their models with newly collected data in an online learning scenario. This leads to different results when the same set of data samples is queried at different times.

In this paper, we investigate whether changes in the output of a black-box ML model before and after updates can leak information about the dataset used for the update, known as the updating set. This represents a new attack surface against black-box ML models, potentially compromising the intellectual property and data privacy of the model owner. We propose four attacks using an encoder-decoder formulation, which allows inferring diverse information about the updating set. Our attacks leverage state-of-the-art deep learning techniques, including a hybrid generative model (CBM-GAN) based on generative adversarial networks (GANs) with a reconstructive loss. Our experiments demonstrate the strong performance of these attacks.

## 1. Introduction

Machine learning (ML) has seen rapid progress over the past decade, largely due to the unprecedented availability of large-scale data. High-quality data collection is essential for building advanced ML models. Since data collection is a continuous process, ML model training also becomes continuous. Instead of training a model once and using it indefinitely, the model's owner must update the model with newly collected data. This is often achieved through online learning, where the dataset used for updates is referred to as the updating set.

Our primary research question is: Can the different outputs of two versions of an ML model, when queried with the same set of data samples, leak information about the corresponding updating set? This constitutes a new attack surface against ML models, and such information leakage could compromise the intellectual property and data privacy of the model owner.

We focus on classification, the most common ML application, and target black-box ML models, where an adversary does not have access to the model's parameters but can query the model with data samples and obtain prediction results. We assume the adversary has a local dataset from the same distribution as the target model’s training set and can establish the same model architecture. We consider updating sets containing up to 100 newly collected data samples.

We propose four attacks, categorized into two classes: single-sample and multi-sample attacks. The single-sample class focuses on a simplified case where the model is updated with one data sample, while the multi-sample class addresses more general cases with multiple data samples. Two of our attacks aim to reconstruct the updating set, which is a novel direction compared to previous work that inferred properties of the training set.

Our experiments show that the output difference between two versions of the same ML model can be exploited to infer information about the updating set. We detail our contributions as follows:

### General Attack Construction

Our four attacks follow a general structure formulated as an encoder-decoder style. The encoder, realized by a multilayer perceptron (MLP), takes the difference in the target ML model's outputs (posterior difference) as input. The decoder produces different types of information about the updating set, depending on the attack.

To obtain the posterior difference, we randomly select a fixed set of data samples (probing set) and probe the target model's two different versions. The second version is obtained by updating the first version with the updating set. We then calculate the difference between the two sets of posteriors as the input for our attack's encoder.

### Single-Sample Attack Class

The single-sample attack class includes:
- **Single-sample label inference attack:** Predicts the label of the single sample used to update the target model. The decoder is a two-layer MLP. Our evaluation shows strong performance, e.g., 0.96 accuracy on the CIFAR-10 dataset.
- **Single-sample reconstruction attack:** Aims to reconstruct the updating sample. We use an autoencoder (AE) trained on a different set of data samples. The AE's decoder is transferred to our attack model as the sample reconstructor. Experimental results show significant performance gains, e.g., 22% for MNIST, 107.1% for CIFAR-10, and 114.7% for Insta-NY, over random guessing.

### Multi-Sample Attack Class

The multi-sample attack class includes:
- **Multi-sample label distribution estimation attack:** Estimates the label distribution of the updating set's data samples. It uses a multilayer perceptron with a fully connected layer and a softmax layer. Kullback-Leibler divergence (KL-divergence) is the loss function. Our experiments show effectiveness, e.g., 0.00384 KL-divergence for CIFAR-10 with 100 samples, outperforming random guessing by a factor of 2.5.
- **Multi-sample reconstruction attack:** Aims to generate all samples in the updating set. The decoder has two components: a novel hybrid generative model (CBM-GAN) and a clustering component. CBM-GAN introduces a "Best Match" loss for accurate reconstruction. Clustering groups the generated samples, and the central sample of each cluster is taken as the final reconstructed sample. Our approach outperforms baselines on MNIST, CIFAR-10, and Insta-NY datasets.

## 2. Preliminaries

### 2.1 Online Learning

We focus on classification, where an ML classifier \( M \) maps a data sample \( x \in X \) to posterior probabilities \( y \in Y \). Training an ML model requires a set of data samples (training set) and an optimization algorithm, such as ADAM, following a predefined loss function.

A trained ML model \( M \) can be updated with an updating set \( D_{\text{update}} \). The update process \( F_{\text{update}} \) is defined as \( F_{\text{update}} : D_{\text{update}}, M \rightarrow M' \), where \( M' \) is the updated version of \( M \).

### 2.2 Threat Model

For all attacks, we consider an adversary with black-box access to the target model. The adversary can only query the model with a probing set and obtain the corresponding posteriors. We assume the adversary has a local dataset from the same distribution as the target model’s training set and can establish the same model architecture. The adversary needs this information to create a shadow model that mimics the target model's behavior for training the attack model. Part of the local dataset is used as the probing set. We assume the target model is updated only with new data, i.e., the updating set and the training set are disjoint.

### 2.3 Datasets Description

For our experimental evaluation, we use three datasets:
- **MNIST:** A 10-class image dataset with 70,000 28×28 grayscale images of handwritten digits.
- **CIFAR-10:** A 10-class balanced dataset with 60,000 32×32 color images.
- **Insta-NY:** Contains Instagram users' location check-in data in New York, with 19,215 locations and eight categories. The task is to predict each location’s category based on weekly check-in data.

## 3. General Attack Pipeline

Our attack pipeline consists of three phases:
1. **Attack Input Generation:** The adversary generates the posterior difference \( \delta = y_{\text{probe}} - y'_{\text{probe}} \), where \( y_{\text{probe}} \) and \( y'_{\text{probe}} \) are the outputs of the target model and the updated model, respectively, when queried with the same probing set.
2. **Encoder Transformation:** The encoder transforms the posterior difference into a latent vector.
3. **Decoder Decoding:** The decoder decodes the latent vector to produce different information about the updating set, depending on the attack.

Figure 1 provides a schematic view of our attack pipeline. The dimension of \( \delta \) is the product of the probing set's cardinality and the number of classes. For CIFAR-10 and MNIST, the dimension is 1,000, and for Insta-NY, it is 800.