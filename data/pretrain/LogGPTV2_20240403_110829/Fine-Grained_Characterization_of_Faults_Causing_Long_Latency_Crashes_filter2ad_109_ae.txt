were due to the ﬁrst heuristic employed by CRASHFINDER
DYNAMIC.
The heuristic for choosing bit positions for selective injec-
tions picks two random positions in the word to inject faults
into, one from the high-level bits and one from the low-level
bits. Unfortunately, this may miss other positions that lead to
LLCs. We evaluated the effect of increasing the number of
sampled bits to 3 and 5, but even this did not considerably
increase the number of LLCs found by CRASHFINDER. This
is because most of the missed errors can only be reproduced
by injecting into very speciﬁc bit positions, and ﬁnding these
positions will require near exhaustive injections on the words
found by CRASHFINDER DYNAMIC, which will prohibitively
increase the time taken to complete the selective fault injection
phase. Therefore, we choose to retain the heuristic as it
is, especially because the difference between CRASHFINDER
STATIC and CRASHFINDER is only 2.33%.
With the above being said, the heuristic-based approach
used here is an approximation. Hence, there may be multiple
sources of inaccuracy in these heuristics. We will further
quantify the limits of the heuristic based approach in future
work.
IX. DISCUSSION
In this section, we discuss some of the implications of
CRASHFINDER on selective protection and checkpointing. We
also discuss some of the limitations of CRASHFINDER and
improvements.
A. Implication for Selective Protection
One of the main results from evaluating CRASHFINDER is
that we ﬁnd that a very small number of instructions are re-
sponsible for most of the LLCs in the program. As per Table II,
only 0.89% of static instructions are responsible for more than
90% of the LLC causing errors in the program (based on the
recall of CRASHFINDER). Further, CRASHFINDER is able to
precisely pinpoint these instructions, thereby allowing these to
be selectively protected.
An example of a selective protection technique is value
range checking in software [10]. A range check is typically
inserted after the instruction that produces the data item to
be checked. For example, the assertion(ptr address<0x001b,
true) inserted after the static instruction producing ptr address
will check the value of the variable whenever the instruction is
executed. Since the total number of executions of all such LLC
causing instructions is only 0.385% (Table II), the overhead
of these checks is likely to be extremely low. We will explore
this direction in future work.
B. Implication for Checkpointing Techniques
Our study also establishes the feasibility of ﬁne-grained
checkpointing techniques for programs, as such checkpoint-
ing techniques would incur frequent state corruptions in the
presence of LLCs. For example, Chandra et al. [6] found that
the frequency of checkpoint corruption when using a ﬁne-
grained checkpointing technique ranges between 25 and 40%
due to LLCs. They therefore conclude that one should not
use such ﬁne-grained checkpointing techniques, and instead
use application-speciﬁc coarse-grained checkpointing in which
the corresponding probability of checkpoint corruption is 1%
to 19%. However, by deploying our technique and selec-
tively protecting the LLC causing locations in the program,
one could restrict
thus minimizing the
chances of checkpoint corruption. Based on the 90% recall
of CRASHFINDER, we can achieve a 10-fold reduction in the
number of LLC causing locations, thus bringing the checkpoint
corruption probability of ﬁne-grained checkpointing down.
This would make ﬁne-grained checkpointing feasible,
thus
allowing faster recovery from errors. This is also a direction
we plan to explore in the future.
the crash latency,
C. Limitations and Improvements
One of the main limitations of CRASHFINDER is that it
takes a long time (on average 4 days) to ﬁnd the LLC causing
errors in the program. The bulk of this time is taken by the
selective fault injection phase, which has to inject faults into
thousands of dynamic instances found by CRASHFINDER
DYNAMIC to determine if they are LLCs. While this is still
orders of magnitude faster than performing exhaustive fault
injections, it is still a relatively high one-time cost to protect the
program. One way to speed this up would be to parallelize it,
but that comes at the cost of increased computation resources.
An alternate way to speed up the technique is to im-
prove the precision of CRASHFINDER STATIC. As it stands,
CRASHFINDER STATIC takes only a few seconds to analyze
even large programs and ﬁnd LLC causing locations in them.
The main problem however is that CRASHFINDER STATIC
has a very low precision (of 25.4%). However, this may be
acceptable in some cases, where we can protect a few more
locations and incur higher overheads in doing so. Even with
this overprotection, we still only protect less than 6% of the
program’s dynamic instructions (Table II). However, one can
improve the precision further by ﬁnding all possible aliases
and control ﬂow paths at compile time [22], and ﬁltering out
the patterns that are unlikely to cause LLCs.
Another limitation is that the recall of CRASHFINDER is
only about 90%. Although this is still a signiﬁcant recall, one
can improve it further by (1) building a more comprehensive
static analyzer to cover the uncovered cases that do not belong
to the dominant LLC-causing patterns, and (2) improving
the heuristic used in the selective fault injection phase, by
increasing the number of fault injections in the selective fault
injection phase, albeit at the cost of increased performance
overheads (as we found in RQ4, this heuristic was responsible
for most of the difference in recall between CRASHFINDER
and CRASHFINDER STATIC).
Finally, though the benchmark applications are chosen from
a variety of domains such as scientiﬁc computing, multimedia,
statistics and games, there are other domains that are not
covered such as database programs, or system software appli-
cations. Further, they are all single-node applications. We defer
the extension of CRASHFINDER for distributed applications to
our future work.
460460
X. CONCLUSION AND FUTURE WORK
In this paper, we identify an important but neglected
problem in the design of dependable software systems, namely
identifying faults that propagate for a long time before causing
crashes, or LLCs. Unlike prior work which has only performed
a coarse grained analysis of such faults, we perform a ﬁne
grained characterization of LLCs. Interestingly, we ﬁnd that
there are only three code patterns in the program that are
responsible for almost all LLCs, and that these patterns can be
identiﬁed efﬁciently through static analysis. We build a static
analysis technique to ﬁnd these patterns, and augment it with a
dynamic analysis and selective fault-injection based technique
to ﬁlter out the false positives. We implement our technique in
a completely automated tool called CRASHFINDER. We ﬁnd
that CRASHFINDER is able to achieve 9 orders of magnitude
speedup over exhaustive fault injections to identify LLCs, has
no false-positives, and successfully identiﬁes over 90% of the
LLC causing locations in ten benchmark programs.
For
future work, we plan to (1) apply the results
of CRASHFINDER to selectively protect LLC causing locations
and measure the overhead, (2) combine CRASHFINDER with
ﬁne-grained checkpointing methods to achieve fast recovery in
the case of crashes, and (3) reduce the performance overhead
of CRASHFINDER and improve its accuracy with more
sophisticated static analysis.
XI. ACKNOWLEDGMENT
We thank the anonymous reviewers of DSN’15 and Keun
Soo Yim for their comments that helped improve the paper.
This work was supported in part by a Discovery Grant from the
Natural Science and Engineering Research Council (NSERC),
Canada, and an equipment grant from the Canada Foundation
for Innovation (CFI). We thank the Institute of Computing,
Information and Cognitive Systems (ICICS) at the University
of British Columbia for travel support.
REFERENCES
[1] C. Basile, L. Wang, Z. Kalbarczyk, and R. Iyer. Group communication
In Reliable Distributed Systems. Proceedings.
protocols under errors.
22nd International Symposium on, pages 35–44, Oct 2003.
[2] C. Bienia, S. Kumar, J. P. Singh, and K. Li. The parsec benchmark suite:
Characterization and architectural implications.
In Proceedings of the
17th international conference on Parallel architectures and compilation
techniques, pages 72–81. ACM, 2008.
[3] S. Borkar. Designing reliable systems from unreliable components:
the challenges of transistor variability and degradation. Micro, IEEE,
25(6):10–16, 2005.
[4] S. Borkar. Electronics beyond nano-scale cmos. In Proceedings of the
43rd annual Design Automation Conference, pages 807–808. ACM, 2006.
In
Fault-Tolerant Computing. Digest of Papers. Twenty-Eighth Annual In-
ternational Symposium on, pages 240–249. IEEE, 1998.
[5] S. Chandra and P. M. Chen. How fail-stop are faulty programs?
[6] S. Chandra and P. M. Chen. The impact of recovery mechanisms on
the likelihood of saving corrupted state.
In Proceedings of the 13th
International Symposium on Software Reliability Engineering, ISSRE.,
pages 91–101. IEEE, 2002.
[7] C. Constantinescu.
Intermittent faults and effects on reliability of inte-
grated circuits. In Reliability and Maintainability Symposium, pages 370–
374. IEEE, 2008.
[8] S. Feng, S. Gupta, A. Ansari, and S. Mahlke. Shoestring: probabilistic soft
error reliability on the cheap. In ACM SIGARCH Computer Architecture
News, volume 38, pages 385–396. ACM, 2010.
[9] W. Gu, Z. Kalbarczyk, R. K. Iyer, and Z. Yang. Characterization of linux
kernel behavior under errors. In International Conference on Dependable
Systems and Networks. IEEE Computer Society, 2003.
461461
[13]
[10] S. K. S. Hari, S. V. Adve, and H. Naeimi. Low-cost program-level
detectors for reducing silent data corruptions.
In Dependable Systems
and Networks (DSN), 42nd Annual IEEE/IFIP International Conference
on, pages 1–12. IEEE, 2012.
[11] S. K. S. Hari, S. V. Adve, H. Naeimi, and P. Ramachandran. Relyzer:
exploiting application-level fault equivalence to analyze application re-
siliency to transient faults.
In ACM SIGARCH Computer Architecture
News, volume 40, pages 123–134. ACM, 2012.
[12] S. K. S. Hari, R. Venkatagiri, S. V. Adve, and H. Naeimi. Ganges: Gang
error simulation for hardware resiliency evaluation.
J. L. Henning. Spec cpu2000: Measuring cpu performance in the new
millennium. Computer, 33(7):28–35, 2000.
[14] A. Lanzaro, R. Natella, S. Winter, D. Cotroneo, and N. Suri. An empirical
study of injected versus actual interface errors.
In Proceedings of the
International Symposium on Software Testing and Analysis, pages 397–
408. ACM, 2014.
[15] C. Lattner and V. Adve. Llvm: A compilation framework for lifelong pro-
gram analysis & transformation. In Code Generation and Optimization.
International Symposium on, pages 75–86. IEEE, 2004.
[16] S. Liu, K. Pattabiraman, T. Moscibroda, and B. G. Zorn. Flikker: saving
dram refresh-power through critical data partitioning.
In Proceedings
of the sixteenth international conference on Architectural support for
programming languages and operating systems, pages 213–224. ACM,
2011.
[17] Q. Lu, K. Pattabiraman, M. S. Gupta, and J. A. Rivers. Sdctune: a
model for predicting the sdc proneness of an application for conﬁgurable
protection. In Proceedings of the International Conference on Compilers,
Architecture and Synthesis for Embedded Systems, page 23. ACM, 2014.
[18] S. Narayanan, J. Sartori, R. Kumar, and D. L. Jones. Scalable stochastic
processors.
In Proceedings of the Conference on Design, Automation
and Test in Europe, pages 335–338. European Design and Automation
Association, 2010.
[19] N. Oh, P. P. Shirvani, and E. J. McCluskey. Error detection by duplicated
instructions in super-scalar processors. Reliability, IEEE Transactions on,
51(1):63–75, 2002.
[20] L. Rashid, K. Pattabiraman, and S. Gopalakrishnan. Modeling the
propagation of intermittent hardware faults in programs. In Dependable
Computing (PRDC), IEEE 16th Paciﬁc Rim International Symposium on,
pages 19–26. IEEE, 2010.
[21] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. I. August. Swift:
Software implemented fault tolerance. In Proceedings of the international
symposium on Code generation and optimization, pages 243–254. IEEE
Computer Society, 2005.
[22] V. Robert and X. Leroy. A formally-veriﬁed alias analysis. In Certiﬁed
Programs and Proofs, pages 11–26. Springer, 2012.
[25]
[23] A. Sampson, W. Dietl, E. Fortuna, D. Gnanapragasam, L. Ceze, and
D. Grossman. Enerj: Approximate data types for safe and general low-
power computation. In ACM SIGPLAN Notices, volume 46, pages 164–
174. ACM, 2011.
[24] S. K. Sastry Hari, M.-L. Li, P. Ramachandran, B. Choi, and S. V. Adve.
mswat: low-cost hardware fault detection and diagnosis for multicore
systems.
In Proceedings of the 42nd Annual IEEE/ACM International
Symposium on Microarchitecture, pages 122–132. ACM, 2009.
J. A. Stratton, C. Rodrigues, I.-J. Sung, N. Obeid, L.-W. Chang,
N. Anssari, G. D. Liu, and W.-m. W. Hwu. Parboil: A revised benchmark
suite for scientiﬁc and commercial throughput computing. Center for
Reliable and High-Performance Computing, 2012.
[26] A. Thomas and K. Pattabiraman. Error detector placement for soft
computation. In Dependable Systems and Networks (DSN), 43rd Annual
IEEE/IFIP International Conference on, pages 1–12. IEEE, 2013.
[27] N. J. Wang and S. J. Patel. Restore: Symptom-based soft error detection in
microprocessors. Dependable and Secure Computing, IEEE Transactions
on, 3(3):188–201, 2006.
J. Wei, A. Thomas, G. Li, and K. Pattabiraman. Quantifying the accuracy
of high-level fault injection techniques for hardware faults. In Dependable
Systems and Networks (DSN), 44rd Annual IEEE/IFIP International
Conference on, 2014.
[29] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. The splash-2
programs: Characterization and methodological considerations. In ACM
SIGARCH Computer Architecture News, volume 23, pages 24–36. ACM,
1995.
[30] K. S. Yim, Z. T. Kalbarczyk, and R. K. Iyer. Quantitative analysis of long-
latency failures in system software. In Dependable Computing, PRDC’09.
15th IEEE Paciﬁc Rim International Symposium on, pages 23–30. IEEE,
2009.
[28]