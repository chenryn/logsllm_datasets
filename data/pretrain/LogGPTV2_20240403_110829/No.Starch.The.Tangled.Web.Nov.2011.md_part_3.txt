ize such expectations as a set of permissible inputs, program states, and
state transitions, which is a prerequisite for almost every type of formal
analysis. Quite simply, intuitive concepts such as “I do not want my mail
to be read by others,” do not translate to mathematical models particu-
larly well. Several exotic approaches will allow such vague requirements
to be at least partly formalized, but they put heavy constraints on software-
engineering processes and often result in rulesets and models that are
far more complicated than the validated algorithms themselves. And,
inturn, they are likely to need their own correctness to be proven . . .
adinfinitum.
 Software behavior is very hard to conclusively analyze. Static analysis of
computer programs with the intent to prove that they will always behave
according to a detailed specification is a task that no one has managed to
believably demonstrate in complex, real-world scenarios (though, as you
might expect, limited success in highly constrained settings or with very
narrow goals is possible). Many cases are likely to be impossible to solve
in practice (due to computational complexity) and may even turn out to
be completely undecidable due to the halting problem.*
Perhaps more frustrating than the vagueness and uselessness of the early
definitions is that as the decades have passed, little or no progress has been
made toward something better. In fact, an academic paper released in 2001
by the Naval Research Laboratory backtracks on some of the earlier work and
arrives at a much more casual, enumerative definition of software security—
one that explicitly disclaims its imperfection and incompleteness.2
* In 1936, Alan Turing showed that (paraphrasing slightly) it is not possible to devise an algorithm
that can generally decide the outcome of other algorithms. Naturally, some algorithms are very
much decidable by conducting case-specific proofs, just not all of them.
Security in the World of Web Applications 3
A system is secure if it adequately protects information that it pro-
cesses against unauthorized disclosure, unauthorized modification,
and unauthorized withholding (also called denial of service). We
say “adequately” because no practical system can achieve these
goals without qualification; security is inherently relative.
The paper also provides a retrospective assessment of earlier efforts
andthe unacceptable sacrifices made to preserve the theoretical purity of
said models:
Experience has shown that, on one hand, the axioms of the Bell-
LaPadula model are overly restrictive: they disallow operations that
users require in practical applications. On the other hand, trusted
subjects, which are the mechanism provided to overcome some
ofthese restrictions, are not restricted enough. . . . Consequently,
developers have had to develop ad hoc specifications for the desired
behavior of trusted processes in each individual system.
In the end, regardless of the number of elegant, competing models intro-
duced, all attempts to understand and evaluate the security of real-world soft-
ware using algorithmic foundations seem bound to fail. This leaves developers
and security experts with no method to make authoritative, future-looking
statements about the quality of produced code. So, what other options are on
the table?
Enter Risk Management
In the absence of formal assurances and provable metrics, and given the
frightening prevalence of security flaws in key software relied upon by mod-
ern societies, businesses flock to another catchy concept: risk management.
The idea of risk management, applied successfully to the insurance
business (with perhaps a bit less success in the financial world), simply states
that system owners should learn to live with vulnerabilities that cannot be
addressed in a cost-effective way and, in general, should scale efforts accord-
ing to the following formula:
risk = probability of an event  maximum loss
For example, according to this doctrine, if having some unimportant
workstation compromised yearly won’t cost the company more than $1,000
in lost productivity, the organization should just budget for this loss and move
on, rather than spend say $100,000 on additional security measures or con-
tingency and monitoring plans to prevent the loss. According to the doctrine
of risk management, the money would be better spent on isolating, securing,
and monitoring the mission-critical mainframe that churns out billing records
for all customers.
4 Chapter 1
Naturally, it’s prudent to prioritize security efforts. The problem is that
when risk management is done strictly by the numbers, it does little to help
us to understand, contain, and manage real-world problems. Instead, it intro-
duces a dangerous fallacy: that structured inadequacy is almost as good as
adequacy and that underfunded security efforts plus risk management are
about as good as properly funded security work.
Guess what? No dice.
 In interconnected systems, losses are not capped and are not tied to
anasset. Strict risk management depends on the ability to estimate typi-
cal and maximum cost associated with the compromise of a resource.
Unfortunately, the only way to do this is to overlook the fact that many
ofthe most spectacular security breaches—such as the attacks on TJX*
orMicrosoft†—began at relatively unimportant and neglected entry
points. These initial intrusions soon escalated and eventually resulted
inthe nearly complete compromise of critical infrastructure, bypassing
any superficial network compartmentalization on their way. In typical
by-the-numbers risk management, the initial entry point is assigned a
lower weight because it has a low value when compared to other nodes.
Likewise, the internal escalation path to more sensitive resources is
downplayed as having a low probability of ever being abused. Still,
neglecting them both proves to be an explosive mix.
 The nonmonetary costs of intrusions are hard to offset with the value
contributed by healthy systems. Loss of user confidence and business
continuity, as well as the prospect of lawsuits and the risk of regulatory
scrutiny, are difficult to meaningfully insure against. These effects can, at
least in principle, make or break companies or even entire industries, and
any superficial valuations of such outcomes are almost purely speculative.
 Existing data is probably not representative of future risks. Unlike the
participants in a fender bender, attackers will not step forward to help-
fully report break-ins and will not exhaustively document the damage
caused. Unless the intrusion is painfully evident (due to the attacker’s
sloppiness or disruptive intent), it will often go unnoticed. Even though
industry-wide, self-reported data may be available, there is simply no reli-
able way of telling how complete it is or how much extra risk one’s cur-
rent business practice may be contributing.
* Sometime in 2006, several intruders, allegedly led by Albert Gonzalez, attacked an unsecured
wireless network at a retail location and subsequently made their way through the corporate
networks of the retail giant. They copied the credit card data of about 46 million customers and
the Social Security numbers, home addresses, and so forth of about 450,000 more. Eleven people
were charged in connection with the attack, one of whom committed suicide.
† Microsoft’s formally unpublished and blandly titled presentation Threats Against and
Protectionof Microsoft’s Internal Network outlines a 2003 attack that began with the compromise
ofan engineer’s home workstation that enjoyed a long-lived VPN session to the inside of the
corporation. Methodical escalation attempts followed, culminating with the attacker gaining
access to, and leaking data from, internal source code repositories. At least to the general
public, the perpetrator remains unknown.
Security in the World of Web Applications 5
 Statistical forecasting is not a robust predictor of individual outcomes.
Simply because on average people in cities are more likely to be hit by
lightning than mauled by a bear does not mean you should bolt a light-
ning rod to your hat and then bathe in honey. The likelihood that a
compromise will be associated with a particular component is, on an
individual scale, largely irrelevant: Security incidents are nearly certain,
but out of thousands of exposed nontrivial resources, any service can be
used as an attack vector—and no one service is likely to see a volume of
events that would make statistical forecasting meaningful within the
scope of a single enterprise.
Enlightenment Through Taxonomy
The two schools of thought discussed above share something in common:
Both assume that it is possible to define security as a set of computable goals
and that the resulting unified theory of a secure system or a model of accept-
able risk would then elegantly trickle down, resulting in an optimal set of
low-level actions needed to achieve perfection in application design.
Some practitioners preach the opposite approach, which owes less to
philosophy and more to the natural sciences. These practitioners argue that,
much like Charles Darwin of the information age, by gathering sufficient
amounts of low-level, experimental data, we will be able to observe, recon-
struct, and document increasingly more sophisticated laws in order to arrive
some sort of a unified model of secure computing.
This latter worldview brings us projects like the Department of Home-
land Security–funded Common Weakness Enumeration (CWE), the goal of
which, in the organization’s own words, is to develop a unified “Vulnerability
Theory”; “improve the research, modeling, and classification of software flaws”;
and “provide a common language of discourse for discussing, finding and
dealing with the causes of software security vulnerabilities.” A typical, delight-
fully baroque example of the resulting taxonomy may be this:
Improper Enforcement of Message or Data Structure
Failure to Sanitize Data into a Different Plane
Improper Control of Resource Identifiers
Insufficient Filtering of File and Other Resource Names
for Executable Content
Today, there are about 800 names in the CWE dictionary, most of which
are as discourse-enabling as the one quoted here.
A slightly different school of naturalist thought is manifested in projects
such as the Common Vulnerability Scoring System (CVSS), a business-backed
collaboration that aims to strictly quantify known security problems in terms
of a set of basic, machine-readable parameters. A real-world example of the
resulting vulnerability descriptor may be this:
AV:LN / AC:L / Au:M / C:C / I:N / A:P / E:F / RL:T / RC:UR /
CDP:MH / TD:H / CR:M / IR:L / AR:M
6 Chapter 1
Organizations and researchers are expected to transform this 14-
dimensional vector in a carefully chosen, use-specific way in order to arrive
atsome sort of objective, verifiable, numerical conclusion about the signifi-
cance of the underlying bug (say, “42”), precluding the need to judge the
nature of security flaws in any more subjective fashion.
Yes, I am poking gentle fun at the expense of these projects, but I do
notmean to belittle their effort. CWE, CVSS, and related projects serve noble
goals, such as bringing a more manageable dimension to certain security pro-
cesses implemented by large organizations. Still, none has yielded a grand
theory of secure software, and I doubt such a framework is within sight.
Toward Practical Approaches
All signs point to security being largely a nonalgorithmic problem for now.
The industry is understandably reluctant to openly embrace this notion,
because it implies that there are no silver bullet solutions to preach (or better
yet, commercialize); still, when pressed hard enough, eventually everybody in
the security field falls back to a set of rudimentary, empirical recipes. These
recipes are deeply incompatible with many business management models,
but they are all that have really worked for us so far. They are as follows:
 Learning from (preferably other people’s) mistakes. Systems should be
designed to prevent known classes of bugs. In the absence of automatic
(or even just elegant) solutions, this goal is best achieved by providing
ongoing design guidance, ensuring that developers know what could go
wrong, and giving them the tools to carry out otherwise error-prone tasks
in the simplest manner possible.
 Developing tools to detect and correct problems. Security deficiencies
typically have no obvious side effects until they’re discovered by a mali-
cious party: a pretty costly feedback loop. To counter this problem, we
create security quality assurance (QA) tools to validate implementations
and perform audits periodically to detect casual mistakes (or systemic
engineering deficiencies).
 Planning to have everything compromised. History teaches us that major
incidents will occur despite our best efforts to prevent them. It is impor-
tant to implement adequate component separation, access control, data
redundancy, monitoring, and response procedures so that service own-
ers can react to incidents before an initially minor hiccup becomes a
disaster of biblical proportions.
In all cases, a substantial dose of patience, creativity, and real technical
expertise is required from all the information security staff.
Naturally, even such simple, commonsense rules—essentially basic engi-
neering rigor—are often dressed up in catchphrases, sprinkled liberally with
aselection of acronyms (such as CIA: confidentiality, integrity, availability), and
then called “methodologies.” Frequently, these methodologies are thinly
veiled attempts to pass off one of the most frustrating failures of the security
industry as yet another success story and, in the end, sell another cure-all
Security in the World of Web Applications 7
product or certification to gullible customers. But despite claims to the con-
trary, such products are no substitute for street smarts and technical prow-
ess—at least not today.
In any case, through the remainder of this book, I will shy away from
attempts to establish or reuse any of the aforementioned grand philosophi-
cal frameworks and settle for a healthy dose of anti-intellectualism instead. I
will review the exposed surface of modern browsers, discuss how to use the
available tools safely, which bits of the Web are commonly misunderstood,
and how to control collateral damage when things go boom.
And that is, pretty much, the best take on security engineering that I can
think of.
A Brief History of the Web
The Web has been plagued by a perplexing number, and a remarkable vari-
ety, of security issues. Certainly, some of these problems can be attributed to
one-off glitches in specific client or server implementations, but many are due
to capricious, often arbitrary design decisions that govern how the essential
mechanisms operate and mesh together on the browser end.
Our empire is built on shaky foundations—but why? Perhaps due to sim-
ple shortsightedness: After all, back in the innocent days, who could predict
the perils of contemporary networking and the economic incentives behind
today’s large-scale security attacks?
Unfortunately, while this explanation makes sense for truly ancient mech-
anisms such as SMTP or DNS, it does not quite hold water here: The Web is
relatively young and took its current shape in a setting not that different from
what we see today. Instead, the key to this riddle probably lies in the tumultu-
ous and unusual way in which the associated technologies have evolved.
So, pardon me another brief detour as we return to the roots. The pre-
history of the Web is fairly mundane but still worth a closer look.
Tales of the Stone Age: 1945 to 1994
Computer historians frequently cite a hypothetical desk-sized device called
the Memex as one of the earliest fossil records, postulated in 1945 by Vannevar
Bush.3 Memex was meant to make it possible to create, annotate, and follow
cross-document links in microfilm, using a technique that vaguely resembled
modern-day bookmarks and hyperlinks. Bush boldly speculated that this sim-
ple capability would revolutionize the field of knowledge management and
data retrieval (amusingly, a claim still occasionally ridiculed as uneducated
and naïve until the early 1990s). Alas, any useful implementation of the design
was out of reach at that time, so, beyond futuristic visions, nothing much
happened until transistor-based computers took center stage.
The next tangible milestone, in the 1960s, was the arrival of IBM’s
Generalized Markup Language (GML), which allowed for the annotation of
documents with machine-readable directives indicating the function of each
block of text, effectively saying “this is a header,” “this is a numbered list of
items,” and so on. Over the next 20 years or so, GML (originally used by only
8 Chapter 1
a handful of IBM text editors on bulky mainframe computers) became the
foundation for Standard Generalized Markup Language (SGML), a more
universal and flexible language that traded an awkward colon- and period-
based syntax for a familiar angle-bracketed one.
While GML was developing into SGML, computers were growing more
powerful and user friendly. Several researchers began experimenting with
Bush’s cross-link concept, applying it to computer-based document storage
and retrieval, in an effort to determine whether it would be possible to cross-
reference large sets of documents based on some sort of key. Adventurous
companies and universities pursued pioneering projects such as ENQUIRE,
NLS, and Xanadu, but most failed to make a lasting impact. Some common
complaints about the various projects revolved around their limited practical
usability, excess complexity, and poor scalability.
By the end of the decade, two researchers, Tim Berners-Lee and Dan
Connolly, had begun working on a new approach to the cross-domain refer-
ence challenge—one that focused on simplicity. They kicked off the project
by drafting HyperText Markup Language (HTML), a bare-bones descendant
of SGML, designed specifically for annotating documents with hyperlinks
and basic formatting. They followed their work on HTML with the develop-
ment of HyperText Transfer Protocol (HTTP), an extremely basic, dedi-
cated scheme for accessing HTML resources using the existing concepts of
Internet Protocol (IP) addresses, domain names, and file paths. The culmi-
nation of their work, sometime between 1991 and 1993, was Tim Berners-
Lee’s World Wide Web (Figure 1-1), a rudimentary browser that parsed
HTML and allowed users to render the resulting data on the screen, and
then navigate from one page to another with a mouse click.
Figure 1-1: Tim Berners-Lee’s World Wide Web
Security in the World of Web Applications 9
To many people, the design of HTTP and HTML must have seemed a
significant regression from the loftier goals of competing projects. After all,
many of the earlier efforts boasted database integration, security and digital
rights management, or cooperative editing and publishing; in fact, even
Berners-Lee’s own project, ENQUIRE, appeared more ambitious than his
current work. Yet, because of its low entry requirements, immediate usability,
and unconstrained scalability (which happened to coincide with the arrival
of powerful and affordable computers and the expansion of the Internet),
the unassuming WWW project turned out to be a sudden hit.
All right, all right, it turned out to be a “hit” by the standards of the mid-
1990s. Soon, there were no fewer than dozens of web servers running on the
Internet. By 1993, HTTP traffic accounted for 0.1 percent of all bandwidth
inthe National Science Foundation backbone network. The same year also
witnessed the arrival of Mosaic, the first reasonably popular and sophisti-
cated web browser, developed at the University of Illinois. Mosaic extended
the original World Wide Web code by adding features such as the ability to
embed images in HTML documents and submit user data through forms,
thus paving the way for the interactive, multimedia applications of today.
Mosaic made browsing prettier, helping drive consumer adoption of the
Web. And through the mid-1990s, it served as the foundation for two other
browsers: Mosaic Netscape (later renamed Netscape Navigator) and Spyglass