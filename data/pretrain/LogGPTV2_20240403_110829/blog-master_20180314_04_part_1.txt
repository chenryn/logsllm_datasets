## PostgreSQL 流计算插件pipelinedb sharding 集群版原理介绍 - 一个全功能的分布式流计算引擎   
### 作者                                                                   
digoal                                                                   
### 日期                                                                   
2018-03-14                                                                 
### 标签                                                                   
PostgreSQL , pipelinedb , 流计算 , sharding , 水平扩展     
----                                                                   
## 背景          
pipelinedb cluster定位为一个分布式流式计算引擎。拥有强大的分布式计算能力，扩展能力，高可用能力，负载均衡能力，读优化和写优化能力，数据自动合并能力。       
## pipelinedb cluster架构    
![pic](20180314_04_pic_001.jpg)    
为了达到无master的架构，pipelinedb集群的所有节点间互相信任通信，实现CV中间聚合结果的数据自动分发，除STREAM，CV以外的其他普通数据DML DDL的全分发以及2PC提交。    
由于是无master架构，客户端连接任意pipelinedb shared节点都可以。    
CV以外的其他DML DDL操作，2PC模式，所有节点执行。保证所有节点有完整的非CV，stream数据。    
![pic](20180314_04_pic_006.jpg)    
建议应用程序通过连接池连到HAPROXY或其他负载均衡软件，然后再连接各个pipelinedb node。实现最大吞吐。建议使用HAPROXY或LVS来实现负载均衡，当然现在的jdbc , libpq或其他驱动很多都支持了负载均衡的配置。    
使用pgbouncer来实现连接池的功能，pgbouncer纯C开发，是目前PostgreSQL比较高效的一款连接池。    
## shard, node概念    
node指PP的数据节点。    
shard是逻辑概念，在创建cv时，需要指定这个CV需要开启多少个shard。    
注意，为了达到最高横向扩展效率，对于同一个CV，每个NODE只负责它一个SHARD。因此一个CV最多可以创建多少SHARD呢？（当然是不能超过NODE个数了）    
![pic](20180314_04_pic_005.jpg)    
定义一个CV跨多少个SHARD的两种方法:    
```    
CREATE CONTINUOUS VIEW v WITH (num_shards=8) AS    
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;    
CREATE CONTINUOUS VIEW v WITH (shard_factor=50) AS    
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;    
```    
当使用shard_factor设置时，实际上给出的是一个百分比值，取值1-100，因此对于16个node的集群，50的意思就是8个shard。    
在CV创建时，会生成元数据（CV在哪里，分片策略是什么（读优化还是写优化），等），元数据结构如下：    
```    
\d pipeline_cluster.shards    
  View "pipeline_cluster.shards"    
     Column      |   Type   | Modifiers    
-----------------+----------+-----------    
 shard           | text     |    
 owner           | text     |    
 continuous_view | text     |    
 shard_id        | smallint |    
 shard_type      | text     |    
 sharding_policy | text     |    
 is_local        | boolean  |    
 balanced        | boolean  |    
 reachable       | boolean  |    
```    
## 数据路由策略    
创建CV时，使用sharding_policy指定数据路由策略。    
### 读优化    
一个CV中，同一个聚合分组的数据会路由到某一个节点，读取时不需要二次合并。但是写入可能会涉及数据重分发（当然这个动作是pipelinedb shard节点透明的完成的，应用不感知，只是写性能肯定有所下降）    
注意网络上分发的是聚合后的数据，即一批聚合后的结果（不同的分组，分发到对应的shard）。    
What this means is that only the aggregate result of incoming rows actually needs to be routed. This is designed to both minimize network overhead and distribute work.    
![pic](20180314_04_pic_002.jpg)    
```    
INSERT INTO stream (x) VALUES (0), (0), (0), (0), (0);    
```    
Assuming that the worker process reads these five rows all at once fast enough, only the aggregate row (0, 5) would be routed to the grouping’s designated node, and subsequently combined with on-disk data as usual.    
```    
CREATE CONTINUOUS VIEW v0 WITH (sharding_policy='read_optimized') AS    
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;    
```    
### 写优化    
每个shard管自己的CV，因此同一个维度的数据可能出现在多个SHARD中。写性能达到最大吞吐。    
仅仅当本地节点不包含目标CV的任何shard时，才需要分发聚合后的部分结果。（关注一下代码：是一批中间结果打散随机分发到所有shard，还是随机选一个shard，将整一批中间结果全部发给这个shard？）    
Routing is then only necessary if a node that produces a partial aggregate result for a given group does not have any shards for the group’s continuous view.     
![pic](20180314_04_pic_003.jpg)    
```    
CREATE CONTINUOUS VIEW v1 WITH (sharding_policy='write_optimized') AS    
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;    
```    
### 读合并    
采用写优化模式时，为了保证数据的完整性，读时需要合并。因此PIPELINEDB需要支持所有的CV都具备合并能力，包括count,avg,sum等常见指标，以及CMS-TOP，HLL等概率指标数据类型的合并，还好这些概率类型目前都是支持同类UNION的。    
![pic](20180314_04_pic_004.jpg)    
```    
CREATE CONTINUOUS VIEW v WITH (sharding_policy='write_optimized', shards=16) AS    
  SELECT x::integer, AVG(y::integer) FROM stream GROUP BY x;    
```    
Since it uses a write_optimized grouping policy, multiple local copies for each grouped average may exist. At read time, these groups would be combined with no loss of information, producing a finalized result with exactly one row per group.    
http://docs.pipelinedb.com/aggregates.html#combine    
## HA、分片负载均衡    
http://enterprise.pipelinedb.com/docs/high-availability.html    
![pic](20180314_04_pic_007.jpg)    
1、写高可用取决于primary shard数，坏num_shards-1=2个NODE，不影响写。  
2、读高可用取决于副本数，同一个shard，坏num_replicas=3个shard,  不影响读。  
3、读高可用，需设置pipeline_cluster.primary_only=FALSE.   
4、replica shard不能与primary shard在同一个NODE上，所以num_replicas最大可设置为nodes数减1。    
内部使用PostgreSQL的异步逻辑订阅功能(logical decoding)，实现cv结果的多副本，在创建cv时通过num_replicas指定每个shard的副本数。    
注意这里使用的是异步复制，所以在创建CV时，num_shards=3表示有3个primary shard，num_replicas=2表示每个primary shard有2个副本。    
```    
CREATE CONTINUOUS VIEW v WITH (num_shards=3, num_replicas=2) AS    
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;    
```    
注意，如果创建CV时未设置num_replicas，则使用pipeline_cluster.num_replicas参数的值。  
### 读负载均衡  
当设置了多个副本时，可以实现分片读负载均衡：    
1、读负载均衡：设置pipeline_cluster.primary_only=false（默认为false，即允许读replica shard），会把replica shard纳入读负载均衡的节点，例如一个primary shard有3个replica shard，那么实际上读可以在4个shard中进行负载均衡。  需要注意的是，PP的replica是采用异步的逻辑复制模式，数据可能存在延迟，读负载均衡可能导致不一致的结果。  
### 节点异常处理机制  
1、读，只要设置了pipeline_cluster.primary_only=false，并且num_replicas大于等于1，那么primary shard所在的NODE挂掉时，会去读replica shard。  
读高可用取决于副本数，同一个shard，坏num_replicas=3个shard,  不影响读。  
读高可用，需设置pipeline_cluster.primary_only=FALSE.     
2、读，当primary shard以及对应的所有replica shard都不可用时，实际上就真的不可用了，至少这个SHARD的数据是读不出来了。  
但是通过设置pipeline_cluster.skip_unavailable=true，可以跳过不可用的shard，也就是说返回部分数据。  