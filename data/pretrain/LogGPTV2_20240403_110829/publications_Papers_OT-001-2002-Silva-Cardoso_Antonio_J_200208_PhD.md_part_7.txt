independence).
The architecture of workflow systems directly follows the two points that allow for a
reduction of task dependencies. Workflow systems such as ORBWork (Kochut, Sheth et
al. 1999) use a message-passing architecture and thus exhibit “invokes” characteristics.
Additionally, tasks are independent from the implementation point of view, and therefore
they are state independent. Due to the architecture of typical WfMSs, workflow
applications have a reduced dependency factor among tasks; we make the assumption
that the dependencies can be ignored in most of cases. Nevertheless, if tasks exhibit
strong dependencies due to the data transferred, a profiling approach may need to be
considered. Hamlet et al. (2001) proposed the use of operational profiles that are passed
between connected components to more effectively compute the reliabiltiy of the global
system.
56
Fidelity. While time, cost, and reliability are common and universal measurements,
fidelity is a function of effective design which refers to an intrinsic property(ies) or
characteristic(s) of a good produced by a task realization.
Since fidelity fully depends on the intrinsic properties and characteristics of the
goods produced, it is not a universal measurement. This means that for each reduction
rule presented previously, it is not possible to specify a general and universal fomr ula to
compute fidelity. Thus, for each reduction system (except for network systems) and for
each fidelity attribute, a specific formula needs to be specified. For example, the Swiss
watchmaker TAG Heuer conducts a series of sixty tests to their watches during the
manufacturing process. Specific tasks carry out the tests, which are placed at strategic
locations in the process. Each testing task can have a fidelity attribute associated with it
that represents the number of tests that have been passed when the task was executed. In
this case, the following fidelity function can be specified for the sequential reduction rule :
F(t ).a = f(F(t), F(t)) and
ij number of tests passed i j
f(vx, vy)= vx.a + vy.a
number of tests passed number of tests passed
In this example, the function f is additive and simply adds the number of tests passed
by each task. In other cases, the function f can be multiplicative, and therefore can be
similar to the functions employed to compute metrics for the reliability dimension .
It is the responsibility of the designer to set for each fidelity attribute involved in a
workflow the fidelity functions (f) to be used when computing workflow QoS. The
designer can select a function from available sets of fidelity functions specifically
constructed to match particular domain requirements. Alternatively, if the functions
needed cannot be found due to their specificity, the designer can manually define new
functions to meet his/her requirements.
57
2.6.2 SIMULATION MODELING
In order to follow organizational strategies and meet organizational goals, workflow
systems need to be able to analyze workflows according to their QoS. While
mathematical methods can be effectively used (see previous section), another alternative
is to utilize simulation analysis (Miller, Cardoso et al. 2002). Simulation can play an
important role in tuning the quality of service metrics of workflows by exploring “what-
if” questions. When the need to adapt or to change a workflow is detected, deciding what
changes to carry out can be very difficult. Before a change is actually made, its possible
effects can be explored with simulation. To facilitate rapid feedback, the workflow
system and the simulation system need to interoperate. In particular, workflow
specification documents need to be translated into simulation model specification
documents so that the new model can be executed/animated on-the-fly.
In our project, these capabilities involve a loosely-coupled integration between the
METEOR WfMS and the JSIM simulation system (Nair, Miller et al. 1996; Miller, Nair
et al. 1997; Miller, Seila et al. 2000). Workflow is concerned with scheduling and
transformations that take place in tasks, while simulation is mainly concerned with
system performance. For modeling purposes, a workflow can be abstractly represented by
using directed graphs (e.g., one for control flow and one for data flow, or one for both).
Since both models are represented as directed graphs, interoperation is facilitated. In
order to carry out a simulation, the appropriate workflow model is retrieved from the
repository and translated into a JSIM simulation model specification. The simulation
model is displayed graphically and then executed/animated. Statistical results which
indicate workflows QoS are collected and displayed.
In order to simulate METEOR workflows, we are enhancing the JSIM Web-Based
Simulation System. In JSIM, simulation entities flow through a digraph consisting of the
following types of nodes.
58
Table 2-7 – Nodes in JSIM
Source Produces entities with random times
Server Provides service to entities
Facility Inherits from server, adds a waiting queue
Signal Alters number of service units in a server(s)
Sink Sink consumes entities and records statistics
These nodes are connected together with transports, which move entities from one
node to the next. These edges provide a smooth motion of entities when a simulation
model is animated. These edges are labeled with branching probabilities .
The mapping of a workflow digraph to a simulation digraph is straightforward. A
METEOR start, stop task will be mapped to a JSIM Source and Sink node, respectively.
A METEOR human task will be mapped to a JSIM Facility, with the number of service
units equal to the number of human participants carrying out the task and feeding of the
same worklist. A METEOR transactional/non-transactional task will be mapped to a
JSIM Facility, with the number of service units equal to the number of processors
available to execute the task. These default mappings can be customized (e.g., a non-
transactional task that does not allow requests to be queued should be mapped to a JSIM
Server). Each edge in the METEOR digraph will be mapped to a corresponding edge in
the JSIM digraph. In METEOR, edges are labeled with the data type of objects flowing
along the edge. In the case of xor nodes, they are also labeled with Boolean expressions.
(The first one that evaluates to true will be the edge selected.) In the current version of
JSIM, data flow must be handled by custom coding. A Boolean expression is mapped to
the probability that the condition will evaluate to true and that none of the preceding
59
conditions will evaluate to true. For more details on mapping workflow specifications
into simulation models specifications, see Chandvasekavan et al. (2002).
2.6.3 WORKFLOW QOS METRICS OF INTEREST
In this section, we list the workflow QoS metrics which are of interest to compute. The
computation can be done at either design time or runtime. At design time, QoS
computations help the designer to compose workflows that will exhibit QoS metrics
which accord with initial requirements. At runtime, the computation of QoS allowst he
manager and administrator to identify workflow instances that have ceased to meet initial
QoS requirements. This situation may occur when tasks fail, break down, or when
necessary services are unavailable. The metrics presented can be automatically computed
using the SWR algorithm.
2.6.3.1 WORKFLOW TIME
The workflow monitor records the total time workflow instances spend within a process.
When a workflow process is executed, instances enter the process, then proceed through
various tasks, and finally exit the workflow process. For example, in our scenario, the
DNA Sequencing had a time constraint; it had to be completed in less than 31 weeks. The
WfMS needs to constantly monitor and estimate the time remaining for instance
termination. In Table Table 2-8 and Table 2-9, we show four important measurements for
workflow time-based executions: workflow response time, workflow delay time, minimum
workflow response time, and workflow response time efficiency.
60
Table 2-8 – Workflow QoS metrics for the time dimension (Part A)
Workflow Response Time (T) T(w) = T(SWR(w))
The workflow response time is the total amount of time that a workflow instance spends
within a workflow process before it finishes. The response time in a workflow is equal to
the sum of the response times at the individual tasks, less any time that two or more tasks
are superimposed on one another. Two or more tasks superimpose their response time
when they are executed in parallel.
Workflow Delay Time (DT) DT(w) = DT(SWR(w))
The workflow delay time, sometimes called “waiting time,” is the total amount of time
that a workflow instance spends in a workflow, while not being processed by a task. The
average delay time in a workflow is equal to the sum of the delay times at the individual
tasks, less any time that two or more tasks are superimposed.
61
Table 2-9 – Workflow QoS metrics for the time dimension (Part B)
Minimum Workflow Response Time (min T ) min T(w) = min T(SWR(w))
The minimum workflow response time, sometimes called the “service time ”of a
workflow, is the time required for a workflow instance to be processed, not accounting
for any task delay time. Thus, it includes only the task response time, ignoring
completely the impact of the task delay time. The minimum workflow response time is
equal to the sum of the process time at the individual tasks, less any time that two or
more tasks superimpose.
min T(w)
Workflow Response Time Efficiency (E) E(w) =
T(w)
The workflow response time efficiency is the ratio of the minimum workflow response
time and the workflow response time. It is instructive to compare these two measures,
since instance efficiency measurement provides an indication of the time an instance is
delayed during its execution and also indicates the degree a workflow process can be
improved by reducing its response time.
62
2.6.3.2 WORKFLOW COST, RELIABILITY, AND FIDELITY
In Table 2-10, we show three other QoS measurements for workflows.
Table 2-10 – Workflow QoS metrics for the cost, reliability, and fidelity dimensio n
Workflow Cost (C) C(w) = C(SWR(w))
Workflow cost (C) analysis measures the cost incurred during the execution of a
workflow. When a workflow process is executed, various tasks, with their associated
costs, are also executed. Cost-based workflows need to have their associated cost
calculated so that managers can make sure that operations are within initial budgets .
Workflow Reliability (R) R(w) = R(SWR(w))
Workflow reliability (R) corresponds to the likelihood that a workflow will perform for
its users on demand.
Workflow Fidelity (F) F(w). = F(attribute, SWR(w))
attribute
Workflow fidelity (F) is a function of effective design; it refers to the intrinsic properties
or characteristics of a good produced or a service rendered.
2.7 WORKFLOW QOS COMPUTATION EXAMPLE
The Fungal Genome Resource (FGR) laboratory is in the process of reengineering their
workflows. The laboratory technicians, domain experts, and managers have agreed that
an alteration to the Prepare and Sequence (Figure 2-10) and Sequence Processing (Figure
2-11) workflows would potentially be beneficial when sequencing DNA .
63
Figure 2-10 – Prepare and Sequence Workflow
Figure 2-11 – Sequence Processing Workflow
To improve the efficiency of the processes being managed by the workflow system,
the bioinformatics researchers decided to merge the two processes. The researchers
noticed that the quality of the DNA sequencing obtained was in some cases useless due to
E. coli contamination. Additionally, it was felt that it would be advantageous to use other
64
algorithms in the sequence processing phase. Therefore, to improve the quality of the
process, the Test Quality task and the SP FASTA task were added.
Clones grown in bacterial hosts are likely to become contaminated. A quick and
effective way to screen for the Escherichia coli (E. coli) contaminants is to compare the
clones against the E. coli genome. For E. coli, this task is made easier with the
availability of its full genome.
The task SP FASTA has of the same objective of the task SP BLAST (a task of the
sequence processing sub-workflow). Both tasks compare new DNA sequences to a
repository of known sequences (e.g., Swiss-Prot or GenBank.) The objective is to find
sequences with homologous relationships to assign potential biological functions and
classifying sequences into functional families. All sequence comparison methods,
however, suffer from certain limitations. Consequently, ti is advantageous to try more
than one comparison algorithm during the sequence processing phase. For this reason, it
was decided to employ the BLAST (Altschul, Gish et al. 1990) and FASTA (Pearson and
Lipman 1988) programs to compare sequences.
The following actions were taken to reengineer the existing workflows:
1) Merge the Prepare and Sequence workflow from Figure 2-10 and the Sequence
Processing workflow from Figure 2-11,
2) Add the task Test Quality to test the existence of E. coli in sequences, and
3) Execute the search for sequences in genome databases using an additional search
algorithm (FASTA).
At this point, the alterations to introduce into the processes have been identified.
From the functional perspective, the lab personnel, domain experts, and workflow
designer all agreed that the new workflow will accomplish the intended objective. The
new re-engineered workflow is named DNA Sequencing. It is illustrated in Figure 2-12.
65
Figure 2-12 – DNA Sequencing Workflow
2.7.1 SETTING QOS METRICS
While the workflow design meets the functional objectives, non-functional requirements
also need to be met. Prior to the execution of the new workflow, an analysis is necessary
to guarantee that the changes to be introduced will actually produce a workflow that
meets desired QoS requirements, i.e., that the workflow time, cost, reliability, and fidelity
remain within acceptable thresholds. To accomplish this, it is necessary to analyze the
QoS metrics and use the SWR algorithm (Cardoso 2002) to compute workflow quality of
service metrics.
The first step is to gather QoS estimates for the tasks involved in the Prepare and
Sequence and Sequence Processing workflows. These workflows have been executed
several times in the past, and the workflow system has recorded their QoS metrics. The
designer QoS estimates have been set using the following methods. (We have omitted the
designer QoS specification for the distributional class since this experiment does not
66
involve the use of a simulation system to compute and predict QoS metrics.) For human
tasks, the laboratory technicians and researchers have provided estimates for the QoS
dimensions. For automated tasks, we have used training sets. For example, for the SP
BLAST task we have constructed a training set of sequences of different lengths. The
sequences have been processed with BLAST, and their QoS has been recorded. For the
time dimension, we have used linear regression to predict future metrics (since the
BLAST algorithm has a linear running time( Altschul, Gish et al. 1990).) Equation 1 was
used to estimate the BLAST running time to process a sequence:
(cid:229) (cid:229) (cid:229)
n xy- ( x)( y)
y =a+bx, a =Y - bX and b= (1)
(cid:229) (cid:229)
n x2 - ( x)2
where x is the independent data (input size) and y is the dependent data (running
time). The estimated function is defined as:
y = a+bx, with a = 78.37,b =0.0071 (2)
The only task with a fidelity function is the SP BLAST task. The fidelity attribute
HITS indicates the percentage of sequences processed with an E value lower than e-15.
The E value is an indication of the probability that the match between a query sequence
and a sequence stored in a database occurred by chance. For close matches, this number
is typically very small .
F(t ). = percentage of sequences with E < e-15
BSP BLAST HITS
For the new tasks introduced (Test Quality and SP FASTA), no QoS runtime
information is available. The only QoS information that can be used to compute the
67
workflow QoS is the one the designer specified at design time. The initial QoS estimates
are shown in Table 2-11.
Table 2-11 – Test Quality and FASTA initial QoS estimates
Designer Specifications
Tasks T(t) C(t) R(t) F(t)
Quality Test 0.01 $0.0 100% n/a
SP FASTA 9.59 $0.0 100% 0.65
Since the SP FASTA task is an automated task, we have used a training set of
sequences to derive and set designer QoS estimates. For the time dimension, we have
used the linear regression from Equation 1 and defined the function represented in
Equation 3 to estimate its duration (FASTA has a linear running time (Pearson and
Lipman 1988).)
y = a+bx, with a = 1061.9,b = 4.11 (3)
As for the SP BLAST task, the following fidelity function has been utilized to
characterize the quality of the results obtained by the task SP FASTA:
F(t ). = percentage of sequences with E < 0.01
SP FASTA HITS
Generally, a value of 0.01 or below is statistically very significant, and a value
between 0.01 and 0.05 is the borderline.
68
To make the workflow QoS computation possible for the fidelity dimension,
formulae have been defined for the reduction systems. As an example, for parallel
systems and for the HITS fidelity attribute, the following function has been defined :
(cid:229)
F(t ).
i HITS
F(t ). = f(F(t ), F(t ), …, F(t )) = 1£ i£ .n
1n HITS 1 2 n
# of tasks with the fidelity attribute HITS
Using the above formula in the DNA Sequencing workflow will result in the
application of the following function:
F(t ). = (F(t ). + F(t ). )/2
SP BLAST FASTA HITS SP BLAST HITS SP FASTA HITS
This function represents only a possible computation for the HITS fidelity attribute.
It is shown here with the solely objective of illustrating how fidelity attributes are
computed. Additional studies of the FASTA and BLAST applications would give more