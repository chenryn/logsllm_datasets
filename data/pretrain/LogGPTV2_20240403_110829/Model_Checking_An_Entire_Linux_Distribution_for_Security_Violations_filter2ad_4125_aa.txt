title:Model Checking An Entire Linux Distribution for Security Violations
author:Benjamin Schwarz and
Hao Chen and
David A. Wagner and
Jeremy Lin and
Wei Tu and
Geoff Morrison and
Jacob West
Model Checking An Entire Linux Distribution for Security Violations
Benjamin Schwarz
Hao Chen
David Wagner
{bschwarz, hchen, daw}@cs.berkeley.edu
{gmorrison, jwest}@fortifysoftware.com
Geoff Morrison
Jacob West
Jeremy Lin (PI:EMAIL)
Wei Tu (PI:EMAIL)
University of California, Berkeley
Abstract
Software model checking has become a popular tool for
verifying programs’ behavior. Recent results suggest that it
is viable for ﬁnding and eradicating security bugs quickly.
However, even state-of-the-art model checkers are limited
in use when they report an overwhelming number of false
positives, or when their lengthy running time dwarfs other
software development processes.
In this paper we report
our experiences with software model checking for security
properties on an extremely large scale—an entire Linux dis-
tribution consisting of 839 packages and 60 million lines of
code. To date, we have discovered 108 exploitable bugs.
Our results indicate that model checking can be both a fea-
sible and integral part of the software development process.
1 Introduction
Software bugs are frequent sources of security vulnera-
bilities. Moreover, they can be incredibly difﬁcult to track
down. Automated detection of possible security violations
has become a quickly-expanding area, due in part to the ad-
vent of model checking tools that can analyze millions of
lines of code [6].
In this paper we describe our experience using MOPS,
a static analyzer, to verify security properties in an entire
Linux distribution. We use the following recipe for ﬁnding
security bugs: identify an important class of security vul-
nerabilities, specify a temporal safety property expressing
the condition when programs are free of this class of bugs,
and use MOPS to decide which programs violate the prop-
erty. We have developed six security properties—expressed
as ﬁnite state automata (FSAs)—and reﬁned them to min-
imize false positives while preserving high effectiveness.
These properties aim at ﬁnding security bugs that arise from
the misuse of system calls, often vulnerable interaction
among these calls. For example, time-of-check-to-time-of-
use (TOCTTOU) bugs involve a sequence of two or more
system calls acting on the same ﬁle (see Section 3.1).
Our primary contribution is the scale of our experiment.
We ran MOPS on the entire Red Hat Linux 9 distribution,
which contains 839 packages totaling 60.0 million lines
of code (counting total lines in all .h, .c, and .cc ﬁles).
MOPS successfully analyzed 87% (732) of these packages;
the remaining 107 packages could not be analyzed because
MOPS’s parser cannot parse some ﬁles in these packages.
To the best of our knowledge, our experiment is the largest
security audit of software using automated tools reported in
the literature. Model checking at this scale introduces major
challenges in error reporting, build integration, and scalabil-
ity. Many of these technical challenges have been addressed
in our work; we show how to surmount them, and demon-
strate that model checking is feasible and effective even for
very large software systems.
As part of this experiment, we have worked out how to
express several new security properties in a form that can be
readily model checked by existing tools. Earlier work de-
veloped simple versions of some of these properties [6], but
in the process of applying them at scale we discovered that
major revisions and reﬁnements were necessary to capture
the full breadth of programming idioms seen in the wild.
Some of the properties checked in this paper are novel; for
instance, we introduce a TOCTTOU property that turned
out to be very effective in ﬁnding bugs. In our experiments,
we focused on ﬁnding bugs rather than proving their ab-
sence. Veriﬁcation is difﬁcult, especially since MOPS is
not completely sound because it does not yet analyze func-
tion pointers and signals. However, we expect that our tech-
niques could point the way to formal veriﬁcation of the ab-
sence of certain classes of bugs, as better model checkers
are developed in the future.
The technical contributions of this paper are threefold: 1)
We show how to express six important security properties in
a form that can be model checked by off-the-shelf tools; 2)
We report on practical experience with model checking at
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:43 UTC from IEEE Xplore.  Restrictions apply. 
a very large scale, and demonstrate for the ﬁrst time that
these approaches are feasible and useful; 3) We measure
the effectiveness of MOPS on a very large corpus of code,
characterizing the false positive and bug detection rates for
different classes of security bugs.
The full version of this paper [1] contains further detail
on our experiments, some of which is omitted from the con-
ference version due to space constraints. MOPS is freely
available from mopscode.sourceforge.net.
2 The MOPS Model Checker
MOPS is a static (compile time) analysis tool that model
checks whether programs violate security properties [7].
Given a security property—expressed as a ﬁnite-state au-
tomaton (FSA) by the user—and the source code for a
program, MOPS determines whether any execution path
through the program might violate the security property.
In more detail, the MOPS process works as follows.
First, the user identiﬁes a set of security-relevant operations
(e.g., a set of system calls relevant to the desired property).
Then, the user ﬁnds all the sequences of these operations
that violate the property, and encodes them using an FSA.
Meanwhile, any execution of a program deﬁnes a trace, the
sequence of security-relevant operations performed during
that execution. MOPS uses the FSA to monitor program ex-
ecution: as the program executes a security-relevant opera-
tion, the FSA transitions to a new state. If the FSA enters an
error state, the program violates the security property, and
this execution deﬁnes an error trace.
At its core, MOPS determines whether a program con-
tains any feasible traces (according to the program’s source
code) that violate a security property (according to the
FSA). Since this question is generally undecidable, MOPS
errs on the conservative side: MOPS will catch all the bugs
for this property (in other words, it is sound, subject to
certain requirements [7]), but it might also report spuri-
ous warnings. This requires the user to determine manu-
ally whether each error trace reported by MOPS represents
a real security hole.
Speciﬁcation of Security Properties. MOPS provides a
custom property language for specifying security proper-
ties. The MOPS user describes each security-relevant op-
eration using a syntactic pattern similar to a program’s ab-
stract syntax tree (AST). With wildcards, these patterns can
describe fairly general or complex syntactic expressions in
the program. The user then labels each FSA transition us-
ing a pattern: if the pattern matches an AST in the program
during model checking, the FSA takes this transition.
To extend the expressiveness of these patterns, we intro-
duced pattern variables, which can describe repeated oc-
currences of the same syntactic expression. For instance, if
X denotes an pattern variable, the pattern f(X, X) matches
any call to the function f with two syntactically identical ar-
guments. In any error trace accepted by an FSA, the pattern
variable X has a single, consistent instantiation throughout
the trace.
Formally, let Σ denote the set of ASTs. We may view
a program trace as a string in Σ∗
, and a property B as a
regular language on Σ∗
. Pattern variables provide existen-
tial quantiﬁcation over the set of ASTs. For instance, the
pattern ∃X.f(X, X) matches any call to f whose two ar-
guments, once parsed, yield the same syntax subtree.
If
B(X) is a language with an unbound pattern variable X,
the language ∃X.B(X) accepts any trace t ∈ Σ∗
where
there exists an AST A(cid:2)
so that B(A(cid:2)) accepts t. In other
words, if L(B) denotes the set of error traces accepted by
the language B, we deﬁne L(∃X.B(X)) = ∪A(cid:1)L(B(A(cid:2))).
We use the convention that unbound pattern variables are
implicitly existentially quantiﬁed at the outermost scope.
Scalability. Since we aim at analyzing hundreds of large,
real application, MOPS must be scalable in several senses.
First, MOPS must run quickly on large programs. Second,
MOPS must run on different application packages without
requiring the user to tweak each package individually.
We have put much effort into integrating MOPS with ex-
isting build processes, including make, rpmbuild, and oth-
ers. By interposing on gcc, the model checker sees the same
code that the compiler sees. As a result, running MOPS on
numerous software packages is as easy as invoking a MOPS
script with the names of these packages. This ease of use
has been critical to our success in checking such a large
number of packages.
Error Reporting. MOPS reports potential errors in a pro-
gram using error traces. A typical problem with reporting
error traces is that a single bug can cause many (sometimes
inﬁnitely many) error traces. To avoid overloading the user,
MOPS divides error traces into groups such that each group
contains all the traces caused by the same bug. More pre-
cisely, two traces belong to the same group if the same line
of code in both traces causes the FSA to enter an error state
for the ﬁrst time via the same transition1. The user can then
examine a representative trace from each group to deter-
mine whether this is a bug and, if so, to identify the cause
of the bug.
Not all error traces identify real bugs: imprecision in the
analysis causes spurious traces. MOPS provides an HTML-
based user interface where the user can examine traces
very rapidly. The user, however, does spend time identi-
fying false positives, so the cost of using MOPS correlates
roughly to the number of trace groups, each of which the
user has to examine. In our experiments, we quantify the
1This implies that both traces enter the same error state. An FSA may
contain multiple error states, corresponding to different kinds of bugs.
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:43 UTC from IEEE Xplore.  Restrictions apply. 
Property
TOCTTOU
Standard File Descriptors
Temporary Files
strncpy
Chroot Jails
Format String
Total
ReportedWarnings RealBugs
Section
790
56
108
1378
1
41
22
34
11*
0
(too many)
(unknown)
2333
108
3.1
3.2
3.3
3.4
(full version)
(full version)
Table 1. Overview of Results.
cost of using MOPS by measuring the number of false pos-
itives, counting only one per trace group.
3.1 TOCTTOU
Resource Usage. The running time of the model checker
is usually dwarfed by the time a human spends perusing
error traces. Still, since our goal is to audit entire distribu-
tions, we have aimed to make computation time small. We
timed the process of model checking several of our prop-
erties against all of Red Hat 9. Using MOPS to look for
TOCTTOU vulnerabilities (ﬁlesystem races) among all Red
Hat 9 packages requires about 1 GB of memory and takes a
total of 465 minutes—a little less than 8 hours—on a 1.5
GHz Intel Pentium 4 machine. Detecting temporary ﬁle
bugs takes 340 minutes of CPU time and about the same
memory footprint. The observed wall-clock time was be-
tween 20% and 40% more than the CPU time. MOPS pro-
duces an extraordinary amount of output, and is required to
read in extremely large control ﬂow graphs; I/O thus consti-
tutes a signiﬁcant portion of this running time, although it
is dominated by the time needed for model checking itself.
Also of chief concern to us was being able to audit man-
ually all error traces produced by MOPS. Error trace group-
ing was a huge time saver: a typical group has more than
4 traces, but some groups contain more than 100 traces.