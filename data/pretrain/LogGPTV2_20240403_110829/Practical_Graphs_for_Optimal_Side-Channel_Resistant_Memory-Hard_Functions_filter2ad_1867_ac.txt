sum of number of times a pebble is placed on the graph. Intuitively,
CPC is the amortized cost of storage (i.e. storing a label for one
time step) while the second term captures the amortized cost of
computation. Before being added to CPC, the second summand is
multiplied by the core-memory ratio R. This ratio is a parameter
of the complexity notion aAT denoting the ratio between the cost
of computation vs. storage. More precisely R is the on-chip area
of a circuit computing the compression function divided by the
on-chip area required to store one label. In the case of Argon2’s
compression function and labels the authors proposed R = 3000 as
a realistic setting for that parameter [18] (which is the value we
used in all of our experiments). For more intuition and in-depth
explanation for aAT we refer the to [18] and [5] (where the notion
is referred to as “energy” complexity).
We fix our notation for the parallel graph pebbling game follow-
ing [5].
Definition 2.3 (Parallel/Sequential Graph Pebbling). Let G = (V , E)
be a DAG. A pebbling configuration is set Pi ⊆ V . Let S,T ⊆ V be
pebbling configurations. A pebbling P = (P0, P1, P2, . . . , Pt) with
starting configuration P0 = S for target T is a sequence of pebbling
configurations such that all target nodes are pebbled:
∀v ∈ T ∃z ≤ t
: v ∈ Pz .
The pebbling P is called legal if pebbles are only places on nodes
whose parents are already pebbled:
∀i ∈ [t] : v ∈ (Pi \ Pi−1) ⇒ parents(v) ⊆ Pi−1.
The pebbling P is called complete if S = ∅ and T is the set of
sinks of G. For a sequential pebbling we add the constraint that
|Pi \ Pi−1| ≤ 1, while no such constraint applies for a parallel
pebbling.
Let Π be the set of all legal and complete parallel pebblings of
G. Then for (implicit) core-memory ratio R > 0 the cumulative
pebbling complexity (CPC) and the amortized area-time complexity
(aAT) of a pebbling P and graph G are defined to be:
cpc(P) :=
i ≤t
|Pi|
cpc(G) := min{cpc(P) : P ∈ Π}
aAT(P) := cpc(P) + R ∗
|Pi \ Pi−1|
aAT(G) := min{aAT(P) : P ∈ Π}.
i∈[t]
More generally, let ΠT denote the set of legal parallel pebblings of G
with target set T and starting configuration P0 = ∅. The cumulative
pebbling complexity (aAT) of pebbling a graph G with target set T
is defined to be:
aAT(G,T) = min{aAT(P) : P ∈ ΠT }.
For the sake of brevity, when it is clear from the context that a
pebbling is legal and complete we will refer to it as simply a pebbling
of G.
Clearly for any pebbling P (and thus for any graph) it holds that
aAT(P) ≥ cpc(P) (regardless of the core-energy ration) and so a
lowerbound on CPC is also a lower bound on aAT.
We will need the following result from [7].
Theorem 2.4 (Corollary 2 in [7]). Given a DAG G = (V , E) and
subsets S,T ⊂ V such that S∩T = ∅ let G′ = G−(V \ ancestorsG−S(T)).
If G′ is (e, d)-depth robust then the cost of pebbling G − S with target
set T is aAT(G − S,T) > ed for any core-energy ration R ≥ 0.
Finally we use the notion of quality from [5] to evaluate how
good a given pebbling strategy P is. Intuitively quality captures
the multiplicative advantage of an attacker compared to the hon-
est (sequential) evaluation algorithm. More precisely, if PN is the
honest pebbling strategy for a DAG G then the quality of pebbling
P for that DAG is given by aAT(PN)/aAT(P). In other words, if P
has quality 10 then an attacker evaluating the iMHF fG based on
the pebbling strategy P will have 10 times less amortized area-time
complexity than the honest algorithm.
All graphs considered in this work have the same type of honest
pebbling strategy; namely pebble the nodes one at time in topolog-
ical order while never removing a pebble from the DAG. Thus in
each case aAT(PN) = n(n + 1)/2 + Rn. In particular, following the
recommendation of [18] we used R = 3000 in our experiments.
3 A SIMPLE VERY DEPTH-ROBUST GRAPH
In this section we give the main construction (c.f. Algorithm 1)
which is a very simple and efficient algorithm for sampling a DAG
from a particular distribution enjoying extreme depth-robustness
with high probability. It is clear by inspection that DRSample only
returns acyclic graphs of size n and indegree (at most) 2. It is also
easy to see that the graphs are simple and locally navigable —
the GetParent function, which returns the i th parent of a node v,
requires O(log(n)) simple arithmetic operations.
Construction Intuition. Given a DAG G = ([n], E) and a directed
edge e = (u, v) ∈ E from node u to node v we define the distance
of the edge to be dist(e) = v − u. Now consider partitioning the
set of all potential directed edges into buckets B1, B2, . . . where
: 2i−1 ≤ dist(u, v) ≤ 2i}. We will want to prove
Bi = {(u, v)
that our construction DRSample satisfies a property called “local-
expansion” (defined formally in the proof of Theorem 3.1), which
can only be achieved if the set E ∩ Bi is reasonably large for each
bucket Bi with i ≤ log2 n. Intuitively, in DRSample we ensure each
of these buckets will have comparable size by selecting the parent
of each node v as follows: first select a bucket Bi with i ≤ log2 v
Session E1:  Hardening CryptoCCS’17, October 30-November 3, 2017, Dallas, TX, USA1006uniformly at random and then select a parent u uniformly at random
subject to the constraint that 2i−1 ≤ dist(u, v) ≤ 2i, or equivalently
(u, v) ∈ Bi. By contrast, in constructions like Argon2iA or Argon2iB
we will have very few edges in buckets Bi with small i > 1, a
property that is exploited by depth-reducing attacks [5, 6].
Algorithm 1: An algorithm for sampling depth-robust graphs.
Function DRSample(n ∈ N≥2):
V := [v]
E := {(1, 2)}
for v ∈ [3, n] and i ∈ [2] do
E := E ∪ {(v, GetParent(v, i))}
end
return G := (V , E).
// Populate edges
// Get i th parent
Function GetParent(v,i):
if i = 1 then
u := i − 1
д′←[1,(cid:4)log2(v)(cid:5) + 1] // Get random range size.
else
д := min(v, 2д′)
r←[max(д/2, 2), д]
// Don’t make edges too long.
// Get random edge length.
end
return v − r
We prove a bound on the depth-robustness parameters of graphs
sampled by DRSample in terms of n. At the highest level, the proof
follows that in [31]. However we depart in several ways. First, we
consider a different graph than [31] so, naturally, any statements
that depend directly on the edge structure of the graph need to be
reproven. In particular, the key lemma about the expansion proper-
ties of the graph needs a new approach (c.f. Lemma 3.3). Second, as
our graphs are sampled randomly, we make probabilistic statements
rather than absolute ones. Consequently, our proof technique now
requires some probability theoretic techniques on top of the original
combinatoric approach of [31]. Third, we have attempted to opti-
mize the constant factors in the proof to the extent possible even if
it makes the proof slightly more complex. By contrast, [31] seem
to focus on obtaining a simple proof even if this simplicity comes
at the cost of worse constant factors. We begin with a high level
outline of the proof followed by a detailed exposition. Thus we use a
new techinque to analyze even the purely combinatoric Lemma 3.2.
Proof Outline. The proof considers a graph G sampled by DRSample.
First, we remove an arbitrary set of nodes S of size O(n/log(n)) (and
incident edges) from G. Next, for the remaining nodes in G we de-
fine a notion of a “good” node. Intuitively, these are nodes such
that not too many of their neighbors were removed. The proof con-
cludes by showing that Ω(n) of the remaining nodes must be good
and that, with high probability, there remains a path p running
through all good nodes. In particular after removing S graph G still
has depth Θ(n).
To show that p likely exists we use the following term. For a pair
of nodes v and u are “reachable” if there remains a (directed) path
connecting u and v (either from u to v or vice versa). It is shown
that for any good node, with high probability a large fraction the
remaining nodes are reachable. Thus we can then show that any
pair of good nodes are reachable. In particular we show that, with
high probability, there is at least one node between the two good
nodes that is reachable by both. Thus we can now construct p by
connecting all the good nodes.
The details follow. We begin by stating the claim formally.
Theorem 3.1. For n ∈ N let G ← DRSample(n). Then Pr[G is (e, d, b)−
block depth-robust] ≥ 1 − negl(n) where
(cid:18)
(cid:19)
e ≥ 2.43 × 10−4
n/log n = Ω
b ≥ 160 log n = Ω(log n).
n
log(n)
, d ≥ 0.03n = Ω(n) ,
In particular,
Pr(cid:2)aAT(G) > 7.3 × 10−6
n
2/log(n)(cid:3) ≥ 1 − negl(n) .
Remark: For G ← DRSample(n) we have aAT(G) ≥ ed ≥ 7.3 ×
10−6 n2
by [7]. While the constant 7.3 × 10−6 is admittedly lower
log n
than one would desire in practice we point out that this is only a
lower bound. In Section 5 we empirically demonstrate that DRSample(n)
resists depth-reducing attacks, and appears to resist known at-
tacks better than all other known iMHF candidates. Improving the
constants from the lower bounds is indeed an important theoret-
ical challenge. For comparison we note that the constant terms
in all known theoretical lower bounds on aAT(G) for other iMHF
constructions are also quite small. For example, Alwen et al. [7]
were able to show that Argon2i-A and Argon2i-B have aAT(G) =
˜Ω
5/3/log n with c = 1.04 × 10−8 which is two
comes aAT(G) ≥ cn
orders of magnitude smaller than the constant we are able to prove
for DRSample.
5/3(cid:17). If we include the hidden constants then the bound be-
(cid:16)
n
The Meta-Graph. Before we analyze the block depth-robustness
of G we first introduce the notion of a meta-graph [7]. As Claim 1
says G will be block depth-robust if and only if Gm is depth-robust.
Fix an arbitrary integer m ∈ [n] set n′ = ⌊n/m⌋. Given a DAG G
we will define a DAG Gm, called the meta-graph of G. For this we
use the following sets. For all i ∈ [n′] let Mi = [(i−1)m +1, im] ⊆ V .
Moreover we denote the first and last thirds respectively of Mi with
i = [(i − 1)m + 1,(i − 1)m +
M F
m
] ⊆ Mi ,
(cid:22)
(cid:18) 1 − γ
(cid:19)(cid:23)
2
(cid:24)1 + γ
(cid:25)
and
i = [(i − 1)m +
ML
+ 1, im] ⊆ Mi .
We define the meta-graph Gm = (Vm, Em) as follows:
2
Nodes: Vm contains one node vi per set Mi. We call vi the
simple node and Mi its meta-node.
is connected to the be-
Edges: If the end of a meta-node ML
i
of another meta-node we connect their simple
′]} Em = {(vi , vj) : E ∩ (ML
ginning M F
j
nodes.
Vm = {vi : i ∈ [n
We remark that the parameter 0  0 (to be determined later) and let
¯Gm = ( ¯Vm, ¯Em) be the graph obtained by removing nodes
in S (and incident edges) from Gm.
Good node: Let c > 0 be a constant. Intuitively a node v ∈
Vm is called c-good under S if at most a c-fraction of nodes
in any interval starting or ending at v are contained in the
set S. More precisely v ∈ Vm is c-good if and only if both
of the following hold:
• ∀r ∈ [v] |Ir(v) ∩ S| ≤ cr
We say a node v ∈ Vm is c-bad under S if v is not c-good
under S. When the set S of removed nodes is clear from
context we will simply write c-bad or c-good.
Reachable node: A node u ∈ ¯Vm is said to be reachable
for v ∈ ¯Vm under S if there exists directed a path in ¯Gm
connecting u to v or v to u.
v,S(r): The set of reachable nodes in the pre-