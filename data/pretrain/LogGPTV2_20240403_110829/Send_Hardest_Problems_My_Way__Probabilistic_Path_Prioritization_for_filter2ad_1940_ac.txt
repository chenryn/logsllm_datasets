Algorithm 1 Execution Sampling
1: P ← {Target binary}
2: F uzzer ← {Fuzzer in DigFuzz }
3: Setinputs ← {Initial seeds}
4: HashM apCovStat ← ∅; SetN ewInputs ← ∅
5:
6: while True do
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end while
SetN ewInputs ← F uzzer{P, Setinputs}
for input ∈ SetN ewInputs do
Coverage ← GetCoverage(P, input)
for branch ∈ Coverage do
Index ← Hash(branch)
HashM apCovStat{Index} + +
end for
Setinputs ← Setinputs ∪ SetN ewInputs
end for
output the HashM apCovStat as coverage statistics
output ExecT ree as the M CP 3 based execution tree
To sum up, DigFuzz works iteratively. In each iteration, the
M CP 3 model updates the execution tree through trace anal-
ysis on all the inputs retained by the fuzzer. Then, this model
labels every branch with its probability that is calculated with
coverage statistics on execution samples. Later, the M CP 3
model prioritizes all missed paths, and selects the path with
the lowest probability for concolic execution. The concolic
executor will generate inputs for the missed path, return the
generated inputs to the fuzzer, and update the execution tree
with paths that have been explored during concolic execution.
After these steps, DigFuzz will enter into another iteration.
B. Execution Sampling
Random sampling is required for DigFuzz to calculate
probabilities using the Monte Carlo method [35]. Based on the
observation that a fuzzer by nature generates inputs randomly,
we consider the fuzzing process as a random sampling process
to the whole program state space. Due to the high throughput
of fuzzing,
the number of generated samples will quickly
become large enough to be statistically meaningful, which is
deﬁned by rule of three [43] where the interval from 0 to 3/n
is a 95% conﬁdence interval when the number of samples is
greater than 30.
Following this observation, we present Algorithm 1 to
perform the sampling. This algorithm accepts three inputs and
produces the coverage statistics in a HashM ap. The three
inputs are: 1) the target binary P ; 2) the fuzzer F uzzer;
and 3) the initial seeds stored in Setinputs. Given the three
inputs, the algorithm iteratively performs the sampling during
fuzzing. F uzzer takes P and Setinputs to generate new
inputs as SetN ewInputs (Ln. 7). Then, for each input
in
N ewInputs, we collect coverage statistical information for
each branch within the path determined by P and input (Ln.
9) and further update the existing coverage statistics stored in
HashM apCovStat (Ln. 11 and 12). In the end, the algorithm
merges SetN ewInputs into Setinputs (Ln. 15) and starts a new
iteration.
C. Execution Tree Construction
As depicted in Figure 3, DigFuzz generates the M CP 3
based execution tree using the run-time information from the
fuzzer.
Algorithm 2 demonstrates the tree construction process.
The algorithm takes three inputs, the control-ﬂow graph for
the target binary CF G, inputs retained by the fuzzer Setinputs
and the coverage statistics HashM apCovStat , which is also
the output from Algorithm 1. The output is a M CP 3 based
execution tree ExecT ree. There are mainly two steps in the
algorithm. The ﬁrst step is to perform trace analysis for each
input in Setinputs to extract the corresponding trace and then
merge the trace into ExecT ree (Ln. 6 to 11). The second
step is to calculate the probability for each branch in the
execution tree (Ln. 12 to 16). To achieve this, for each branch
in ExecT ree, we extract its neighbor branch brj (bri
bri
and brj share the same predecessor block that contains a
condition check) by examining the CF G (Ln. 13). Then, the
algorithm leverages Equation 1 to calculate the probability for
bri (Ln. 14). After that, the algorithm labels the execution
tree ExecT ree with the calculated probabilities (Ln. 15) and
outputs the newly labeled ExecT ree.
To avoid the potential problem of path explosion in the
execution tree, we only perform trace analysis for the seed
inputs retained by fuzzing. The fuzzer typically regards those
mutated inputs with new code coverage as seeds for further
mutation. Traces on these retained seeds is a promising ap-
proach to model the program state explored by fuzzing. For
each branch along an execution trace, whenever the opposite
branch has not been covered by fuzzing, then a missed path
is identiﬁed, which refers to a preﬁx of the trace conjuncted
with the uncovered branch. In other words, the execution tree
does not include an uncovered branch of which the opposite
one has not been covered yet.
To ease representation, we present a running example,
which is simpliﬁed from a program in the CQE dataset [13],
and the code piece is shown in Figure 4. The vulnerability
is guarded by a speciﬁc string, which is hard for fuzzing to
detect. Figure 5 illustrates the M CP 3 based execution tree
6
Fig. 4: Running Example
for the running example in Figure 4. Each node represents
a basic block. Each edge refers a branch labeled with the
probability. We can observe that there are two traces (t1 =
(cid:104)b1, b2, b6, b12, b13, b7, b9, b11(cid:105) and t2 = (cid:104)b1, b3, b4, b12, b14(cid:105))
in the tree marked as red and blue.
D. Probabilistic Path Prioritization
We then prioritize paths based on probabilities. As shown
in Equation 2, a path is treated as a Markov chain and its
probability is calculated based on the probabilities of all the
branches within the path. A path can be represented as a
sequence of covered branches, and each branch is labeled with
its probability that indicates how likely a random input can
satisfy the condition. Consequently, we leverage the Markov
Chain model
to regard the probability for a path as the
sequence of probabilities of the transitions.
for bri ∈ trace do
Algorithm 3 Path Prioritization in DigFuzz
1: ExecT ree ← {Output from Algorithm 2}
2: SetP rob ← ∅
3: for trace ∈ ExecT ree do
4:
5:
6:
7:
8:
9:
10:
11:
12: end for
brj ← GetNeighbor(bri, CF G)
missed ← GetMissedPath(trace, bri, brj)
if missed /∈ ExecT ree then
prob ← CalPathProb(missed)
SetP rob ← {trace, missed, prob}
end if
end for
output SetP rob as missed paths with probabilities corre-
sponding to each trace
The detailed algorithm is presented in Algorithm 3. It takes
the M CP 3 based execution tree ExecT ree from Algorithm 2
as the input and outputs SetP rob, a set of missed paths and
their probabilities. Our approach will further prioritize these
missed paths based on SetP rob and feed the one with the
lowest probability to concolic execution. The algorithm starts
with the execution tree traversal. For each branch bri on every
trace within ExecT ree, it ﬁrst extracts the neighbor brj (Ln.
5) and then collects the missed paths missed along the given
trace (Ln. 6). Then, the algorithm calculates the probability
for missed by calling CalP athP rob() which implements
7
Fig. 5: The execution tree with probabilities
Equation 2 and stores the information in SetP rob. Eventually,
the algorithm produces SetP rob, a set of missed paths with
probabilities for every trace.
After we get SetP rob, we will prioritize missed paths by a
decrease order on their probabilities, and identify the path with
the lowest probability for concolic execution. As the concolic
executor takes a concrete input as the concrete value to perform
trace-based symbolic execution, we will identify an input on
which the execution is able to guide the concolic executor to
the prioritized missed path.
Take the program in Figure 4 as an example. In Figure 5,
the missed branches are shown as dotted lines. After the
execution tree is constructed and properly labeled, Algorithm 3
is used to obtain missed paths and calculate probabilities
for these paths. We can observe that
there are 4 missed
paths in total denoted as P1, P2, P3 and P4, respectively.
By calling CalPathProb() function, the probabilities of these
missed paths are calculated as shown in the ﬁgure, and
the lowest one is of P1. To guide the concolic executor to
explore P1, our approach will pick the input that leads to
the trace (cid:104)b1, b2, b6, b12, b13, b7, b9, b11(cid:105) and assign this input
as the concrete value of concolic execution, because this trace
share the same path preﬁx, (cid:104)b1, b2, b6, b12, b13, b7, b9(cid:105), with the
missed path P1.
V. EVALUATION
In this section, we conduct comprehensive evaluation on
the effectiveness and the runtime performance of DigFuzz
    void main(argv) {  int chk_in () {  recv(in); b6 res = is_valid(in)  switch (argv[1]) { b7 if (!res) b1 case ‘A’: b8 return; b2 chk_in(in); b9 if (strcmp(in, ‘BK’) == 0);  break; b10 //vulnerability b3 case ‘B’: b11 else …  } b4 is_valid(in);  int is_valid(in) {  break; b12 if all_char(in) b5 default: … b13 return 1;  }} b14 return 0; }            b1b2b250.50.5b50.6b3b8b20b100.70.4b40.6b21b11b17b18b193/30003/30000.30.7b17b18b190.30.7b90.3b26b28b270.70.3b1b20.50.5b3b5b15b70.7b17b16b8b93/3000b12b13b140.30.7b60.3b70.7b8b60.3b1b20.50.5b5b3b6b80.70.4b40.6b10b11b9b12b13b143/30000.30.7b12b13b140.30.7b70.3P1 =，P(P1 )=0.000045P2 =， P(P2)=0.063P3 =)， P(P3)=0.09P4=， P(P4)=0.105by comparing with the state-of-the-art hybrid fuzzing systems,
Driller [39] and MDPC [42], with respect to code coverage, the
number of discovered vulnerabilities , and the contribution of
concolic execution. In the end, we conduct a detailed analysis
of DigFuzz using a case study.
A. Datasets
We leverage two datasets: the DARPA CGC Qualifying
Event (CQE) dataset [13], the same as Driller [39], and the
LAVA dataset, a widely adopted dataset in recent studies [12],
[27], [34]. Both of them provide ground-truth for verifying
detected vulnerabilities.
The CQE dataset contains 131 applications which are
deliberately designed by security experts to test automated
vulnerability detection systems. Every binary is injected one
or more memory corruption vulnerabilities. In addition, many
CQE binaries have complex protocols and large input spaces,
making these vulnerabilities harder to trigger. In our eval-
uation, we exclude 5 applications involving communication
between multiple binaries as in Driller, and 8 applications on
which AFL cannot work. Totally, we use 118 CQE binaries
for evaluation.
For the LAVA dataset [15], we adopt LAVA-M as pre-
vious techniques [12], [27], which consists of 4 real world
programs, uniq, base64, md5sum, and who. Each program
in LAVA-M is injected with multiple bugs, and each injected
bug has a unique ID.
B. Baseline Techniques
As the main contribution of DigFuzz is to propose a more
effective strategy to combine fuzzing with concolic execution,
the advance of fuzzing itself is out of our scope. Therefore,
we do not compare DigFuzz with non-hybrid fuzzing sys-
tems such as CollAFL [16], Angora [12], AFLfast [4] and
VUzzer [34].
To quantify the contribution of concolic execution, we
leverage unaided fuzzing as a baseline. We deploy the original
AFL to simulate fuzzing assisted by a dummy concolic execu-
tor that makes zero contribution. This conﬁguration is denoted
as AFL.
To compare DigFuzz with other hybrid fuzzing systems,
we choose the state-of-the-art hybrid fuzzing systems, Driller
and MDPC.
We use Driller1 to represent the conﬁguration of Driller.
Moreover, to evaluate the impact of path prioritization compo-
nent alone, we modify Driller to enable the concolic executor
to start from the beginning by randomly selecting inputs
from fuzzing. We denote this conﬁguration as Random. The
only difference between Random and DigFuzz is the path
prioritization. This conﬁguration eliminates the ﬁrst limitation
described in Section II-A.
In the original MDPC model [42], fuzzing and concolic ex-
ecution alternate in a sequential manner, whereas all the other
hybrid systems work in parallel to better utilize computing
resources. To make a fair comparison, we conﬁgure the MDPC
1The conﬁguration of Driller in our work is different from Driller paper as
further discussed in Section V-C
model to work in parallel. More speciﬁcally, if MDPC chooses
fuzzing to explore a path, then the fuzzer generates a new test
case for concrete testing. Otherwise, MDPC will assign this
path that requires to be explored by concolic execution to a job
queue, and continues to calculate probabilities for other paths.
The concolic executor will take a path subsequently from the
job queue. In this way, we can compare MDPC with other
hybrid systems with the same computing resources.
Besides, as estimating the cost for solving a path constraint
is a challenge problem, we simply assign every path constraints
with the solving cost of 50, which is the highest solving cost as
deﬁned [42]. Please note that with this conﬁguration, the time
cost by MDPC for optimal decision is lower, because it does
not spent effort in collecting and estimating path constraints.
C. Experiment setup
The original Driller [39] adopts a shared pool design, where
the concolic execution pool is shared among all the fuzzer
instances. With this design, when a fuzzer gets stuck, Driller
adds all the inputs retained by the fuzzer into a global queue of
the concolic execution pool and performs concolic execution
by going through these inputs sequentially. This design is not
suitable for us as the fuzzer instances are not fairly aided by
the concolic execution.
To better evaluate our new combination strategy in Dig-
Fuzz, we assign computer resources evenly to ensure that
the analysis on each binary is fairly treated. As there exist
two modes in the mutation algorithm of AFL (deterministic
and non-deterministic modes), we allocate 2 fuzzing instances
(each running in one mode) for every testing binary. In details,
we allocates 2 fuzzing instances for testing binaries with AFL,
2 fuzzing instances and 1 concolic execution instance with
Driller, Random, MDPC and DigFuzz. Each run of concolic
execution is limited to 4GB of memory and run-time up to
one hour, which is the same as in Driller.
We run the experiments on a server with three computer
nodes, and each node is conﬁgured with 18 CPU cores
and 32GB RAM. Considering that random effects play an
important role in our experiments, we choose to run each
experiment for three times, and report the mean values for