patching. However, all of the new APIs must be included in a
separate segment of the binary and, as IAT patching is a known
malicious strategy used by malware authors [25], IAT calls
to non-standard dynamic linkers or multiple jumps from the
IAT to an internal segment of the binary would immediately
be identiﬁed as suspicious. Conversely, our attack does not
require hardcoding and by design is resilient against traditional
non-ML program analysis techniques.
VII. AVAILABILITY
We release the code and data of our approach to other
researchers by responsibly sharing a private repository. The
project website with instructions to request access is at:
https://s2lab.kcl.ac.uk/projects/intriguing.
VIII. CONCLUSIONS
Since the seminal work that evidenced intriguing properties
of neural networks [66], the community has become more
widely aware of the brittleness of machine learning in ad-
versarial settings [11].
To better understand real-world implications across different
application domains, we propose a novel formalization of
problem-space attacks as we know them today, that enables
comparison between different proposals and lays the foun-
dation for more principled designs in subsequent work. We
uncover new relationships between feature space and problem
space, and provide necessary and sufﬁcient conditions for the
existence of problem-space attacks. Our novel problem-space
attack shows that automated generation of adversarial malware
at scale is a realistic threat—taking on average less than 2
minutes to mutate a given malware example into a variant
that can evade a hardened state-of-the-art classiﬁer.
ACKNOWLEDGEMENTS
We thank the anonymous reviewers and our shepherd,
Nicolas Papernot, for their constructive feedback, as well as
Battista Biggio, Konrad Rieck, and Erwin Quiring for feedback
on early drafts, all of which have signiﬁcantly improved the
overall quality of this work. This research has been partially
sponsored by the UK EP/L022710/2 and EP/P009301/1 EP-
SRC research grants.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1344
REFERENCES
[1] A. V. Aho, R. Sethi, and J. D. Ullman. Compilers, Principles,Techniques,
and Tools (2nd Edition). Addison Wesley, 2007.
[2] K. Allix, T. F. Bissyand´e, J. Klein, and Y. Le Traon. Androzoo:
Collecting Millions of Android Apps for the Research Community. In
ACM Mining Software Repositories (MSR), 2016.
[3] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W.
Chang. Generating natural language adversarial examples. In Empirical
Methods in Natural Language Processing (EMNLP, 2018.
[4] E. K. Andreas Moser, Christopher Kruegel. Limits of static analysis for
malware detection. 2007.
[5] Android.
Permissions overview - dangerous permissions, 2020.
URL https://developer.android.com/guide/topics/permissions/overview#
dangerous permissions.
[6] G. Apruzzese and M. Colajanni. Evading Botnet Detectors Based on
In IEEE NCA,
Flows and Random Forest with Adversarial Samples.
2018.
[7] G. Apruzzese, M. Colajanni, and M. Marchetti. Evaluating the effec-
tiveness of Adversarial Attacks against Botnet Detectors. In IEEE NCA,
2019.
[8] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, and K. Rieck.
DREBIN: Effective and Explainable Detection of Android Malware in
Your Pocket. In NDSS, 2014.
[9] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel, J. Klein, Y. L.
Traon, D. Octeau, and P. D. McDaniel. Flowdroid: precise context, ﬂow,
ﬁeld, object-sensitive and lifecycle-aware taint analysis for android apps.
In PLDI. ACM, 2014.
[10] E. T. Barr, M. Harman, Y. Jia, A. Marginean, and J. Petke. Automated
software transplantation. In ISSTA. ACM, 2015.
[11] B. Biggio and F. Roli. Wild patterns: Ten years after the rise of
adversarial machine learning. Pattern Recognition, 2018.
[12] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov,
G. Giacinto, and F. Roli. Evasion attacks against machine learning at
test time. In ECML-PKDD. Springer, 2013.
[13] B. Biggio, G. Fumera, and F. Roli. Security evaluation of pattern
classiﬁers under attack. IEEE TKDE, 2013.
[14] C. M. Bishop. Pattern Recognition and Machine Learning. 2006.
[15] N. Carlini. List of Adversarial ML Papers, 2019. URL https://nicholas.
carlini.com/writing/2019/all-adversarial-example-papers.html.
[16] N. Carlini and D. Wagner. Towards evaluating the robustness of neural
networks. In IEEE Symp. S&P, 2017.
[17] N. Carlini and D. Wagner. Audio adversarial examples: Targeted attacks
on speech-to-text. In Deep Learning for Security (DLS) Workshop. IEEE,
2018.
[18] N. Carlini and D. A. Wagner. Adversarial examples are not easily
detected: Bypassing ten detection methods. In AISec@CCS, pages 3–14.
ACM, 2017.
[19] N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras,
I. Goodfellow, and A. Madry. On evaluating adversarial robustness.
arXiv preprint arXiv:1902.06705, 2019.
[20] I. Corona, G. Giacinto, and F. Roli. Adversarial attacks against intrusion
Information
detection systems: Taxonomy, solutions and open issues.
Sciences, 2013.
[21] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al. Adversarial
classiﬁcation. In KDD. ACM, 2004.
[22] H. Dang, Y. Huang, and E. Chang. Evading classiﬁers by morphing
In ACM Conference on Computer and Communications
in the dark.
Security, pages 119–133. ACM, 2017.
[23] A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp, K. Rieck,
I. Corona, G. Giacinto, and F. Roli. Yes, machine learning can be more
secure! a case study on android malware detection. IEEE Transactions
on Dependable and Secure Computing, 2017.
[24] W. F. Dowling and J. H. Gallier. Linear-time algorithms for testing the
satisﬁability of propositional horn formulae. J. Log. Program., 1(3):
267–284, 1984.
[25] S. Eresheim, R. Luh, and S. Schrittwieser. The evolution of process hid-
ing techniques in malware-current threats and possible countermeasures.
Journal of Information Processing, 2017.
[26] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. LIBLINEAR: A
library for large linear classiﬁcation. J. Mach. Learn. Res., 9:1871–
1874, 2008.
[27] A. Fass, M. Backes, and B. Stock. HideNoSeek: Camouﬂaging Mali-
cious JavaScript in Benign ASTs. In ACM CCS, 2019.
[28] P. Fogla and W. Lee. Evading network anomaly detection systems:
formal reasoning and practical techniques.
Computer and Communications Security, pages 59–68. ACM, 2006.
In ACM Conference on
[29] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT press,
2016.
[30] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing
adversarial examples. In ICLR (Poster), 2015.
[31] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel.
In ESORICS. Springer,
Adversarial examples for malware detection.
2017.
[32] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar.
Adversarial machine learning. In AISec. ACM, 2011.
[33] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar.
Adversarial machine learning. In Proceedings of the 4th ACM workshop
on Security and artiﬁcial intelligence, pages 43–58. ACM, 2011.
[34] I. Incer, M. Theodorides, S. Afroz, and D. Wagner. Adversarially robust
malware detection using monotonic classiﬁcation. In Proc. Int. Workshop
on Security and Privacy Analytics. ACM, 2018.
[35] J. Jeon, X. Qiu, J. S. Foster, and A. Solar-Lezama. Jsketch: sketching
for java. In ESEC/SIGSOFT FSE, pages 934–937. ACM, 2015.
[36] A. Kamath, R. Motwani, K. V. Palem, and P. G. Spirakis. Tail bounds for
occupancy and the satisﬁability threshold conjecture. In FOCS, pages
592–603. IEEE Computer Society, 1994.
[37] A. Kerckhoffs. La cryptographie militaire.
In Journal des sciences
militaires, 1883.
[38] B. Kolosnjaji, A. Demontis, B. Biggio, D. Maiorca, G. Giacinto,
C. Eckert, and F. Roli. Adversarial malware binaries: Evading deep
learning for malware detection in executables.
In EUSIPCO. IEEE,
2018.
[39] B. Kulynych, J. Hayes, N. Samarin, and C. Troncoso. Evading classi-
ﬁers in discrete domains with provable optimality guarantees. CoRR,
abs/1810.10939, 2018.
[40] T. Larrabee. Test pattern generation using boolean satisﬁability. IEEE
Trans. on CAD of Integrated Circuits and Systems, 11(1):4–15, 1992.
[41] P. Laskov and N. ˇSrndi´c. Static Detection of Malicious JavaScript-
Bearing PDF Documents. In ACSAC. ACM, 2011.
[42] M. Leslous, V. V. T. Tong, J.-F. Lalande, and T. Genet. Gpﬁnder: tracking
the invisible in android malware. In MALWARE. IEEE, 2017.
[43] J. Li, S. Ji, T. Du, B. Li, and T. Wang. Textbugger: Generating
adversarial text against real-world applications. In NDSS. The Internet
Society, 2019.
[44] D. Lowd and C. Meek. Good word attacks on statistical spam ﬁlters.
In CEAS, volume 2005, 2005.
[45] D. Maiorca, G. Giacinto, and I. Corona. A Pattern Recognition System
In Intl. Workshop on Machine
for Malicious PDF Files Detection.
Learning and Data Mining in Pattern Recognition. Springer, 2012.
[46] D. Maiorca, I. Corona, and G. Giacinto. Looking at the bag is not enough
to ﬁnd the bomb: an evasion of structural methods for malicious pdf ﬁles
detection. In ASIACCS. ACM, 2013.
[47] D. Maiorca, B. Biggio, and G. Giacinto. Towards robust detection of
adversarial infection vectors: Lessons learned in pdf malware. arXiv
preprint, 2019.
[48] M. Melis, D. Maiorca, B. Biggio, G. Giacinto, and F. Roli. Explaining
black-box android malware detection. In EUSIPCO. IEEE, 2018.
[49] B. Miller, A. Kantchelian, M. C. Tschantz, S. Afroz, R. Bachwani,
R. Faizullabhoy, L. Huang, V. Shankar, T. Wu, G. Yiu, et al. Reviewer
Integration and Performance Measurement for Malware Detection.
In
DIMVA. Springer, 2016.
[50] D. Mitchell, B. Selman, and H. Levesque. Hard and easy distributions
of sat problems. In Proceedings of the Tenth National Conference on
Artiﬁcial Intelligence, AAAI’92, pages 459–465. AAAI Press, 1992.
ISBN 0-262-51063-4. URL http://dl.acm.org/citation.cfm?id=1867135.
1867206.
[51] A. Moser, C. Kruegel, and E. Kirda. Limits of static analysis for malware
detection. In ACSAC, 2007.
[52] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami. The limitations of deep learning in adversarial settings. In
2016 IEEE European Symposium on Security and Privacy (EuroS&P),
pages 372–387. IEEE, 2016.
[53] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop, 2017.
[54] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1345
esnay. Scikit-Learn: Machine Learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.
[55] F. Pendlebury, F. Pierazzi, R. Jordaney, J. Kinder, and L. Cavallaro.
TESSERACT: Eliminating Experimental Bias in Malware Classiﬁcation
across Space and Time.
In 28th USENIX Security Symposium, Santa
Clara, CA, 2019. USENIX Association. USENIX Sec.
[56] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors
for word representation. In EMNLP, pages 1532–1543. ACL, 2014.
[57] B. C. Pierce and C. Benjamin. Types and programming languages. MIT
press, 2002.
[58] E. Quiring, A. Maier, and K. Rieck. Misleading authorship attribution
of source code using adversarial learning. USENIX Security Symposium,
2019.
[59] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and C. K.
Nicholas. Malware detection by eating a whole exe. In AAAI Workshops,
2018.
[60] I. Rosenberg, A. Shabtai, L. Rokach, and Y. Elovici. Generic black-
box end-to-end attack against state of the art API call based malware
classiﬁers. In RAID. Springer, 2018.
[61] B. Selman, D. G. Mitchell, and H. J. Levesque. Generating hard
satisﬁability problems. Artif. Intell., 81(1-2):17–29, 1996. doi: 10.1016/
0004-3702(95)00045-3. URL https://doi.org/10.1016/0004-3702(95)
00045-3.
[62] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. Accessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition.
In ACM CCS. ACM, 2016.
[63] C. Smutz and A. Stavrou. Malicious pdf detection using metadata and
structural features. In ACSAC. ACM, 2012.
[64] N. ˇSrndic and P. Laskov. Detection of malicious pdf ﬁles based on
hierarchical document structure. In NDSS, 2013.
[65] O. Suciu, R. M˘arginean, Y. Kaya, H. Daum´e III, and T. Dumitras¸. When
Does Machine Learning FAIL? Generalized Transferability for Evasion
and Poisoning Attacks. USENIX Security Symposium, 2018.
[66] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus. Intriguing properties of neural networks. ICLR, 2014.
[67] X. Ugarte-Pedrero, D. Balzarotti, I. Santos, and P. G. Bringas. Sok: Deep
packer inspection: A longitudinal study of the complexity of run-time
packers. In IEEE Symposium on Security and Privacy, 2015.
[68] R. Vall´ee-Rai, P. Co, E. Gagnon, L. Hendren, P. Lam, and V. Sundaresan.
In CASCON First
Soot: A java bytecode optimization framework.
Decade High Impact Papers. IBM Corp., 2010.
[69] G. Vigna and D. Balzarotti. When malware is packin’ heat. In USENIX
ENIGMA, 2018.
[70] VirusTotal. VirusTotal, 2004. URL https://www.virustotal.com.
[71] M. Weiser. Program slicing.
In Proceedings of the 5th International
Conference on Software Engineering, ICSE ’81, pages 439–449. IEEE
Press, 1981. URL http://dl.acm.org/citation.cfm?id=800078.802557.
[72] W. Weiss and C. D’Mello. Fundamentals of Model Theory. University
of Toronto, 2015.
[73] Q. Xiao, Y. Chen, C. Shen, Y. Chen, and K. Li. Seeing is not believing:
Camouﬂage attacks on image scaling algorithms. In USENIX Security
Symposium, pages 443–460. USENIX Association, 2019.
[74] W. Xu, Y. Qi, and D. Evans. Automatically evading classiﬁers. In NDSS,
2016.
[75] W. Yang, D. Kong, T. Xie, and C. A. Gunter. Malware detection
in adversarial settings: Exploiting feature evolutions and confusions in
android apps. In ACSAC. ACM, 2017.
[76] G. Zizzo, C. Hankin, S. Maffeis, and K. Jones. Adversarial machine
learning beyond the image domain. In ACM DAC, 2019.
A. Symbol Table
APPENDIX
Table II provides a reference for notation and major symbols
used throughout the paper.
B. Threat Model
The threat model must be deﬁned in terms of attacker
knowledge and capability, as in related literature [11, 19, 65].
While the attacker knowledge is represented in the same way
as in the traditional feature-space attacks, their capability also
SYMBOL
Z
X
Y
ϕ
hi
g
Ly
fy,κ
fy
Ω
δ
η
T
T
T
Υ
Π
Λ
Γ
D
w
Θ
θ
TABLE II
TABLE OF SYMBOLS.
n.
n is a symbol used to denote a
DESCRIPTION
Problem space (i.e., input space).
Feature space X ⊆ R
Label space.
Feature mapping function ϕ : Z −→ X .
Discriminant function hi : X −→ R that
assigns object x ∈ X a score in R (e.g.,
distance from hyperplane) that represents
ﬁtness to class i ∈ Y.
Classiﬁer g : X −→ Y that assigns object
x ∈ X to class y ∈ Y. Also known as
decision function. It is deﬁned based on
the output of the discriminant functions
hi,∀i ∈ Y .
Loss function Ly : X ×Y −→ R of object
x ∈ X with respect to class y ∈ Y.
Attack objective function fy,κ : X × Y ×
R −→ R of object x ∈ X with respect
to class y ∈ Y with maximum conﬁdence
κ ∈ R.
Compact notation for fy,0.
Feature-space constraints.
δ ∈ R
feature-space perturbation vector.
Side-effect feature vector.
Transformation T : Z −→ Z.
Transformation sequence T = Tn◦ Tn−1◦
··· ◦ T1.
Space of available transformations.
Suite of automated tests τ ∈ Υ to verify
preserved semantics.
Suite of manual tests π ∈ Π to verify
plausibility. In particular, π(z) = 1 if
z ∈ Z is plausible, else π(z) = 0.
Set of preprocessing operators A ∈ Λ for
which z ∈ Z should be resistant (i.e.,
A(T(z)) = T(z)).
Problem-space constraints Γ, consisting of
{Π, Υ,T , Λ}.
Training dataset.
Model hyper-parameters.
Knowledge space.
Threat model assumptions θ ∈ Θ; more
speciﬁcally, θ = (D,X , g, w). A hat sym-
bol is used if only estimates of parameters
are known. See Appendix B for details.
includes the problem-space constraints Γ. For completeness,
we report the threat model formalization proposed in Biggio
and Roli [11].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1346
Attacker Knowledge. We represent the knowledge as a
set θ ∈ Θ which may contain (i) training data D, (ii) the
feature set X , (iii) the learning algorithm g, along with the
loss function L minimized during training, (iv) the model
parameters/hyperparameters w. A parameter is marked with
a hat symbol if the attacker knowledge of it is limited or
only an estimate (i.e., ˆD, ˆX , ˆg, ˆw). There are three major
scenarios [11]:
• Perfect Knowledge (PK) white-box attacks, in which the
attacker knows all parameters and θP K = (D,X , g, w).
• Limited Knowledge (LK) gray-box attacks, in which the
attacker has some knowledge on the target system. Two
common settings are LK with Surrogate Data (LK-SD),
where θLK−SD = ( ˆD,X , g, ˆw), and LK with Surrogate
Learners, where θLK−SL = ( ˆD,X , ˆg, ˆw). Knowledge of
the feature space and the ability to collect surrogate data,
θ ⊇ ( ˆD,X ), enables the attacker to perform mimicry
attacks in which the attacker manipulates examples to re-
semble the high density region of the target class [12, 28].
• Zero Knowledge (ZK) black-box attacks, where the at-
tacker has no information on the target system, but has
some information on which kind of feature extraction is
performed (e.g., only static analysis in programs, or struc-
tural features in PDFs). In this case, θLK = ( ˆD, ˆX , ˆg, ˆw).
Note that θP K and θLK imply knowledge of any defenses
used to secure the target system against adversarial examples,
depending on the degree to which each element is known [18].
Attacker Capability. The capability of an attacker is ex-
pressed in terms of his ability to modify feature space and
problem space, i.e., the attacker capability is described through
feature-space constraints Ω and problem-space constraints Γ.
We observe that the attacker’s knowledge and capability can
also be expressed according to the FAIL [65] model as follows:
knowledge of Features X (F), the learning Algorithm g (A),
Instances in training D (I), Leverage on feature space and
problem space with Ω and Γ (L).
More details on the threat models can be found in [11, 65].
C. Theorem Proofs
Proof of Theorem 1. We proceed with a proof by con-
tradiction. Let us consider a problem-space object z ∈ Z
with features x ∈ X , which we want to misclassify as a
target class t ∈ Y. Without loss of generality, we consider
a low-conﬁdence attack, with desired attack conﬁdence κ = 0
(see Equation 3). We assume by contradiction that there is
no solution to the feature-space attack; more formally, that
there is no solution δ∗
= arg minδ∈Rn:δ|=Ω ft(x + δ) that
satisﬁes ft(x + δ∗
)  0 values in δ∗ that
are different from 0 (i.e., values corresponding to an actual
feature-space perturbation). Then, a transformation sequence
T : T(z) |= Γ, T = Tidxq−1 ◦ Tidxq−2 ◦ ··· ◦ Tidx0 can always
be constructed by the attacker to satisfy ϕ(T(z)) = x+δ∗. We
highlight that we do not consider the existence of a speciﬁc
transformation in Z that maps to x + δ∗ because that may
not be known by the attacker; hence, the attacker may never
learn such a speciﬁc transformation. Thus, Equation 12 must
be valid for all possible perturbations within the considered
feature space.
D. Opaque Predicates Generation
We use opaque predicates [4] as inconspicuous conditional
statements always resolving to False to preserve dynamic
semantics of the Android applications.
To ensure the intractability of such an analysis, we follow
the work of Moser et al. [51] and build opaque predicates
using a formulation of the 3-SAT problem such that resolving
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1347
the truth value of the predicate is equivalent to solving the
NP-complete 3-SAT problem.
The k-satisﬁability (k-SAT) problem asks whether the vari-
ables of a Boolean logic formula can be consistently replaced
with True or False in such a way that the entire formula
evaluates to True; if so the formula is satisﬁable. Such a
formula is easily expressed in its conjunctive normal form:
(cid:3)m
i=1(Vi1 ∨ Vi2 ∨ ... ∨ Vik) ,
where Vij ∈ {v1, v2, ..., vn} are Boolean variables and k is
the number of variables per clause.
Importantly, when k = 3, formulas are only NP-Hard
in the worst case—30% of 3-SAT problems are in P [61].
This baseline guarantee is not sufﬁcient as our injected code
should never execute. Additionally, we require a large number
of random predicates to reduce commonality between the
synthetic portions of our generated examples.
To consistently generate NP-Hard k-SAT problems we use
Random k-SAT [61] in which there are 3 parameters: the
number of variables n, the number of clauses m, and the
number of literals per clause k.
To construct a 3-SAT formula, m clauses of length 3 are
generated by randomly choosing a set of 3 variables from
the n available, and negating each with probability 50%. An
empirical study by Selman et al. [61] showed that n should
be at least 40 to ensure the formulas are hard to resolve.
Additionally, they show that formulas with too few clauses
are under-constrained while formulas with too many clauses
are over-constrained, both of which reduce the search time.
These experiments led to the following conjecture.
∗ as the
Threshold Conjecture [61]. Let us deﬁne c
threshold at which 50% of the formulas are satisﬁable. For
the formula is satisﬁable with
m/n  c
is unsatisﬁable with probability 100%.
∗ ≈ 4.3  0, otherwise as goodware. Minimal
app zmin ∈ Z with features ϕ(zmin) = xmin.
Parameters: Number of features to consider nf ; number of donors
per-feature nd.
Output: Ice-box of harvested organs with feature vectors.
1 ice-box ← {}
(cid:4) Empty key-value dictionary.
2 L ← List of pairs (wi, i), sorted by increasing value of wi.
3 L(cid:3) ← First nf elements of L, then remove any entry with wi ≥ 0.
4 for (wi, i) in L(cid:3) do
ice-box[i] ← []
while length(ice-box[i]) 0 then
(cid:4) ei is a one-hot vector.
(cid:4) Gadget features
5
6
7
8
9
10
11
12
Discard the gadget;
Append (ρj , rj , s) to ice-box[i].
(cid:4) Store gadget
13
14
15
16
17 return ice-box;
else
Algorithm 2: Attack (Adv. Program Generation)
Input: Discriminant function h(x) = wT x + b, which classiﬁes x
as malware if h(x) > 0, otherwise as goodware. Malware
app z ∈ Z. Ice-box G.
Parameters: Problem-space constraints.
Output: Adversarial app z(cid:3) ∈ Z such that h(ϕ(z(cid:3))) < 0.
1 T ← Transplantation through gadget addition.
2 Υ ← Smoke test through app installation and execution in emulator.
3 Π ← Plausibility by-design through code consolidation.
4 Λ ← Artifacts from last column of Table I.
5 Γ ← {T , Υ, Π, Λ}
6 sz ← Software stats of z
7 x ← ϕ(z)
8 L ← []
9 T(z) ← Empty sequence of problem-space transformations.
10 for (ρj , rj , s) in G do
dj ← rj ∧ ¬x
11
scorej ← h(dj )
12
Append the pair (scorej , i, j) to L
13
(cid:4) Feature-space contribution of gadget j.
(cid:4) Impact on decision score.
(cid:4) Feature i, Gadget j.
(cid:4) Negative scores ﬁrst.
(cid:4) Empty list.
14 L(cid:3) ← Sort L by increasing scorej
15 for (scorej , i, j) in L(cid:3) do
if z has xi = 1 then
(cid:4) Feature i already present.
else if z has xi = 0 then
Do nothing;
(ρj , rj , s) ← element j in ice-box G
if check feasibility(sz, s) is True then
x ← (x ∨ ei ∨ ηj )
(cid:4) Update features of z.
Append transplantation T ∈ T of gadget ρj in T(z).
if h(x) < 0 then
Exit from cycle;
(cid:4) Attack gadgets found.
25 z(cid:3) ← Apply transformation sequence T(z) (cid:4) Inject chosen gadgets.
26 if h(ϕ(z(cid:3))) < 0 and T(z) |= Γ then
27
28 else
29
(cid:4) Attack successful.
return Failure;
return z’;
16
17
18
19
20
21
22
23
24
FlowDroid, when modifying app Manifests; and FlowDroid
injecting system libraries found on the classpath when they
should be excluded.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1349