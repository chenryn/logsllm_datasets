### Patching and API Integration

When integrating new APIs, they must be included in a separate segment of the binary. Import Address Table (IAT) patching, a known malicious technique used by malware authors [25], can be easily detected if IAT calls are made to non-standard dynamic linkers or if multiple jumps occur from the IAT to an internal segment of the binary. Such behavior would be flagged as suspicious. In contrast, our attack does not require hardcoding and is designed to be resilient against traditional non-ML program analysis techniques.

### Availability

We have released the code and data of our approach to other researchers through a responsibly shared private repository. The project website, which includes instructions for requesting access, is available at: https://s2lab.kcl.ac.uk/projects/intriguing.

### Conclusions

Since the seminal work that highlighted the intriguing properties of neural networks [66], the community has become more aware of the brittleness of machine learning in adversarial settings [11]. To better understand the real-world implications across different application domains, we propose a novel formalization of problem-space attacks. This formalization enables comparison between different proposals and lays the foundation for more principled designs in future work. We uncover new relationships between feature space and problem space, and provide necessary and sufficient conditions for the existence of problem-space attacks. Our novel problem-space attack demonstrates that automated generation of adversarial malware at scale is a realistic threat, with the ability to mutate a given malware example into a variant that can evade a state-of-the-art classifier in less than 2 minutes on average.

### Acknowledgements

We thank the anonymous reviewers and our shepherd, Nicolas Papernot, for their constructive feedback. We also acknowledge Battista Biggio, Konrad Rieck, and Erwin Quiring for their valuable input on early drafts, which significantly improved the overall quality of this work. This research was partially sponsored by the UK EP/L022710/2 and EP/P009301/1 EPSRC research grants.

### References

[1] A. V. Aho, R. Sethi, and J. D. Ullman. *Compilers, Principles, Techniques, and Tools (2nd Edition)*. Addison Wesley, 2007.

[2] K. Allix, T. F. Bissyandé, J. Klein, and Y. Le Traon. Androzoo: Collecting Millions of Android Apps for the Research Community. In *ACM Mining Software Repositories (MSR)*, 2016.

[3] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W. Chang. Generating natural language adversarial examples. In *Empirical Methods in Natural Language Processing (EMNLP)*, 2018.

[4] E. K. Andreas Moser, Christopher Kruegel. Limits of static analysis for malware detection. 2007.

[5] Android. Permissions overview - dangerous permissions, 2020. URL: https://developer.android.com/guide/topics/permissions/overview#dangerous-permissions.

[6] G. Apruzzese and M. Colajanni. Evading Botnet Detectors Based on Flows and Random Forest with Adversarial Samples. In *IEEE NCA*, 2018.

[7] G. Apruzzese, M. Colajanni, and M. Marchetti. Evaluating the effectiveness of Adversarial Attacks against Botnet Detectors. In *IEEE NCA*, 2019.

[8] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, and K. Rieck. DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket. In *NDSS*, 2014.

[9] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel, J. Klein, Y. L. Traon, D. Octeau, and P. D. McDaniel. Flowdroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for android apps. In *PLDI*. ACM, 2014.

[10] E. T. Barr, M. Harman, Y. Jia, A. Marginean, and J. Petke. Automated software transplantation. In *ISSTA*. ACM, 2015.

[11] B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning. *Pattern Recognition*, 2018.

[12] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In *ECML-PKDD*. Springer, 2013.

[13] B. Biggio, G. Fumera, and F. Roli. Security evaluation of pattern classifiers under attack. *IEEE TKDE*, 2013.

[14] C. M. Bishop. *Pattern Recognition and Machine Learning*. 2006.

[15] N. Carlini. List of Adversarial ML Papers, 2019. URL: https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html.

[16] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In *IEEE Symp. S&P*, 2017.

[17] N. Carlini and D. Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In *Deep Learning for Security (DLS) Workshop*. IEEE, 2018.

[18] N. Carlini and D. A. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In *AISec@CCS*, pages 3–14. ACM, 2017.

[19] N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras, I. Goodfellow, and A. Madry. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019.

[20] I. Corona, G. Giacinto, and F. Roli. Adversarial attacks against intrusion detection systems: Taxonomy, solutions and open issues. *Information Sciences*, 2013.

[21] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al. Adversarial classification. In *KDD*. ACM, 2004.

[22] H. Dang, Y. Huang, and E. Chang. Evading classifiers by morphing in the dark. In *ACM Conference on Computer and Communications Security*, pages 119–133. ACM, 2017.

[23] A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp, K. Rieck, I. Corona, G. Giacinto, and F. Roli. Yes, machine learning can be more secure! A case study on android malware detection. *IEEE Transactions on Dependable and Secure Computing*, 2017.

[24] W. F. Dowling and J. H. Gallier. Linear-time algorithms for testing the satisfiability of propositional Horn formulae. *J. Log. Program.*, 1(3):267–284, 1984.

[25] S. Eresheim, R. Luh, and S. Schrittwieser. The evolution of process hiding techniques in malware—current threats and possible countermeasures. *Journal of Information Processing*, 2017.

[26] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. LIBLINEAR: A library for large linear classification. *J. Mach. Learn. Res.*, 9:1871–1874, 2008.

[27] A. Fass, M. Backes, and B. Stock. HideNoSeek: Camouflaging Malicious JavaScript in Benign ASTs. In *ACM CCS*, 2019.

[28] P. Fogla and W. Lee. Evading network anomaly detection systems: formal reasoning and practical techniques. In *ACM Conference on Computer and Communications Security*, pages 59–68. ACM, 2006.

[29] I. Goodfellow, Y. Bengio, and A. Courville. *Deep Learning*. MIT press, 2016.

[30] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In *ICLR (Poster)*, 2015.

[31] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel. Adversarial examples for malware detection. In *ESORICS*. Springer, 2017.

[32] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar. Adversarial machine learning. In *AISec*. ACM, 2011.

[33] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar. Adversarial machine learning. In *Proceedings of the 4th ACM workshop on Security and artificial intelligence*, pages 43–58. ACM, 2011.

[34] I. Incer, M. Theodorides, S. Afroz, and D. Wagner. Adversarially robust malware detection using monotonic classification. In *Proc. Int. Workshop on Security and Privacy Analytics*. ACM, 2018.

[35] J. Jeon, X. Qiu, J. S. Foster, and A. Solar-Lezama. Jsketch: sketching for Java. In *ESEC/SIGSOFT FSE*, pages 934–937. ACM, 2015.

[36] A. Kamath, R. Motwani, K. V. Palem, and P. G. Spirakis. Tail bounds for occupancy and the satisfiability threshold conjecture. In *FOCS*, pages 592–603. IEEE Computer Society, 1994.

[37] A. Kerckhoffs. La cryptographie militaire. In *Journal des sciences militaires*, 1883.

[38] B. Kolosnjaji, A. Demontis, B. Biggio, D. Maiorca, G. Giacinto, C. Eckert, and F. Roli. Adversarial malware binaries: Evading deep learning for malware detection in executables. In *EUSIPCO*. IEEE, 2018.

[39] B. Kulynych, J. Hayes, N. Samarin, and C. Troncoso. Evading classifiers in discrete domains with provable optimality guarantees. CoRR, abs/1810.10939, 2018.

[40] T. Larrabee. Test pattern generation using Boolean satisfiability. *IEEE Trans. on CAD of Integrated Circuits and Systems*, 11(1):4–15, 1992.

[41] P. Laskov and N. Šrndić. Static Detection of Malicious JavaScript-Bearing PDF Documents. In *ACSAC*. ACM, 2011.

[42] M. Leslous, V. V. T. Tong, J.-F. Lalande, and T. Genet. Gpfinder: tracking the invisible in android malware. In *MALWARE*. IEEE, 2017.

[43] J. Li, S. Ji, T. Du, B. Li, and T. Wang. Textbugger: Generating adversarial text against real-world applications. In *NDSS*. The Internet Society, 2019.

[44] D. Lowd and C. Meek. Good word attacks on statistical spam filters. In *CEAS*, volume 2005, 2005.

[45] D. Maiorca, G. Giacinto, and I. Corona. A Pattern Recognition System for Malicious PDF Files Detection. In *Intl. Workshop on Machine Learning and Data Mining in Pattern Recognition*. Springer, 2012.

[46] D. Maiorca, I. Corona, and G. Giacinto. Looking at the bag is not enough to find the bomb: an evasion of structural methods for malicious pdf files detection. In *ASIACCS*. ACM, 2013.

[47] D. Maiorca, B. Biggio, and G. Giacinto. Towards robust detection of adversarial infection vectors: Lessons learned in pdf malware. arXiv preprint, 2019.

[48] M. Melis, D. Maiorca, B. Biggio, G. Giacinto, and F. Roli. Explaining black-box Android malware detection. In *EUSIPCO*. IEEE, 2018.

[49] B. Miller, A. Kantchelian, M. C. Tschantz, S. Afroz, R. Bachwani, R. Faizullabhoy, L. Huang, V. Shankar, T. Wu, G. Yiu, et al. Reviewer Integration and Performance Measurement for Malware Detection. In *DIMVA*. Springer, 2016.

[50] D. Mitchell, B. Selman, and H. Levesque. Hard and easy distributions of SAT problems. In *Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI’92*, pages 459–465. AAAI Press, 1992. ISBN 0-262-51063-4. URL: http://dl.acm.org/citation.cfm?id=1867135.1867206.

[51] A. Moser, C. Kruegel, and E. Kirda. Limits of static analysis for malware detection. In *ACSAC*, 2007.

[52] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In *2016 IEEE European Symposium on Security and Privacy (EuroS&P)*, pages 372–387. IEEE, 2016.

[53] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. In *NIPS Autodiff Workshop*, 2017.

[54] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-Learn: Machine Learning in Python. *Journal of Machine Learning Research*, 12:2825–2830, 2011.

[55] F. Pendlebury, F. Pierazzi, R. Jordaney, J. Kinder, and L. Cavallaro. TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time. In *28th USENIX Security Symposium, Santa Clara, CA*, 2019. USENIX Association. USENIX Sec.

[56] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global vectors for word representation. In *EMNLP*, pages 1532–1543. ACL, 2014.

[57] B. C. Pierce and C. Benjamin. *Types and Programming Languages*. MIT press, 2002.

[58] E. Quiring, A. Maier, and K. Rieck. Misleading authorship attribution of source code using adversarial learning. *USENIX Security Symposium*, 2019.

[59] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and C. K. Nicholas. Malware detection by eating a whole exe. In *AAAI Workshops*, 2018.

[60] I. Rosenberg, A. Shabtai, L. Rokach, and Y. Elovici. Generic black-box end-to-end attack against state of the art API call based malware classifiers. In *RAID*. Springer, 2018.

[61] B. Selman, D. G. Mitchell, and H. J. Levesque. Generating hard satisfiability problems. *Artif. Intell.*, 81(1-2):17–29, 1996. doi: 10.1016/0004-3702(95)00045-3. URL: https://doi.org/10.1016/0004-3702(95)00045-3.

[62] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In *ACM CCS*. ACM, 2016.

[63] C. Smutz and A. Stavrou. Malicious PDF detection using metadata and structural features. In *ACSAC*. ACM, 2012.

[64] N. Šrndić and P. Laskov. Detection of malicious PDF files based on hierarchical document structure. In *NDSS*, 2013.

[65] O. Suciu, R. Mărginean, Y. Kaya, H. Daumé III, and T. Dumitraș. When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks. *USENIX Security Symposium*, 2018.

[66] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. *ICLR*, 2014.

[67] X. Ugarte-Pedrero, D. Balzarotti, I. Santos, and P. G. Bringas. SOK: Deep packer inspection: A longitudinal study of the complexity of run-time packers. In *IEEE Symposium on Security and Privacy*, 2015.

[68] R. Vallée-Rai, P. Co, E. Gagnon, L. Hendren, P. Lam, and V. Sundaresan. Soot: A Java bytecode optimization framework. In *CASCON First Decade High Impact Papers*. IBM Corp., 2010.

[69] G. Vigna and D. Balzarotti. When malware is packin’ heat. In *USENIX ENIGMA*, 2018.

[70] VirusTotal. VirusTotal, 2004. URL: https://www.virustotal.com.

[71] M. Weiser. Program slicing. In *Proceedings of the 5th International Conference on Software Engineering, ICSE ’81*, pages 439–449. IEEE Press, 1981. URL: http://dl.acm.org/citation.cfm?id=800078.802557.

[72] W. Weiss and C. D’Mello. Fundamentals of Model Theory. University of Toronto, 2015.

[73] Q. Xiao, Y. Chen, C. Shen, Y. Chen, and K. Li. Seeing is not believing: Camouflage attacks on image scaling algorithms. In *USENIX Security Symposium*, pages 443–460. USENIX Association, 2019.

[74] W. Xu, Y. Qi, and D. Evans. Automatically evading classifiers. In *NDSS*, 2016.

[75] W. Yang, D. Kong, T. Xie, and C. A. Gunter. Malware detection in adversarial settings: Exploiting feature evolutions and confusions in Android apps. In *ACSAC*. ACM, 2017.

[76] G. Zizzo, C. Hankin, S. Maffeis, and K. Jones. Adversarial machine learning beyond the image domain. In *ACM DAC*, 2019.

### Appendix

#### A. Symbol Table

Table II provides a reference for notation and major symbols used throughout the paper.

| SYMBOL | DESCRIPTION |
|---------|-------------|
| \( Z \) | Problem space (i.e., input space). |
| \( X \) | Feature space \( X \subseteq \mathbb{R}^n \). |
| \( Y \) | Label space. |
| \( \phi \) | Feature mapping function \( \phi : Z \rightarrow X \). |
| \( h_i \) | Discriminant function \( h_i : X \rightarrow \mathbb{R} \) that assigns object \( x \in X \) a score in \( \mathbb{R} \) (e.g., distance from hyperplane) that represents fitness to class \( i \in Y \). |
| \( g \) | Classifier \( g : X \rightarrow Y \) that assigns object \( x \in X \) to class \( y \in Y \). Also known as decision function. It is defined based on the output of the discriminant functions \( h_i, \forall i \in Y \). |
| \( L_y \) | Loss function \( L_y : X \times Y \rightarrow \mathbb{R} \) of object \( x \in X \) with respect to class \( y \in Y \). |
| \( f_{y, \kappa} \) | Attack objective function \( f_{y, \kappa} : X \times Y \times \mathbb{R} \rightarrow \mathbb{R} \) of object \( x \in X \) with respect to class \( y \in Y \) with maximum confidence \( \kappa \in \mathbb{R} \). |
| \( f_y \) | Compact notation for \( f_{y, 0} \). |
| \( \Omega \) | Feature-space constraints. |
| \( \delta \) | \( \delta \in \mathbb{R}^n \) feature-space perturbation vector. |
| \( \eta \) | Side-effect feature vector. |
| \( T \) | Transformation \( T : Z \rightarrow Z \). |
| \( T \) | Transformation sequence \( T = T_n \circ T_{n-1} \circ \cdots \circ T_1 \). |
| \( \Upsilon \) | Space of available transformations. |
| \( \tau \) | Suite of automated tests \( \tau \in \Upsilon \) to verify preserved semantics. |
| \( \pi \) | Suite of manual tests \( \pi \in \Pi \) to verify plausibility. In particular, \( \pi(z) = 1 \) if \( z \in Z \) is plausible, else \( \pi(z) = 0 \). |
| \( \Lambda \) | Set of preprocessing operators \( A \in \Lambda \) for which \( z \in Z \) should be resistant (i.e., \( A(T(z)) = T(z) \)). |
| \( \Gamma \) | Problem-space constraints \( \Gamma \), consisting of \( \{\Pi, \Upsilon, T, \Lambda\} \). |
| \( D \) | Training dataset. |
| \( w \) | Model hyper-parameters. |
| \( \Theta \) | Knowledge space. |
| \( \theta \) | Threat model assumptions \( \theta \in \Theta \); more specifically, \( \theta = (D, X, g, w) \). A hat symbol is used if only estimates of parameters are known. See Appendix B for details. |

#### B. Threat Model

The threat model must be defined in terms of attacker knowledge and capability, as in related literature [11, 19, 65].

**Attacker Knowledge:**
We represent the knowledge as a set \( \theta \in \Theta \) which may contain:
- (i) Training data \( D \),
- (ii) The feature set \( X \),
- (iii) The learning algorithm \( g \), along with the loss function \( L \) minimized during training,
- (iv) The model parameters/hyperparameters \( w \).

A parameter is marked with a hat symbol if the attacker's knowledge of it is limited or only an estimate (i.e., \( \hat{D}, \hat{X}, \hat{g}, \hat{w} \)).

There are three major scenarios [11]:
- **Perfect Knowledge (PK) white-box attacks:** The attacker knows all parameters and \( \theta_{PK} = (D, X, g, w) \).
- **Limited Knowledge (LK) gray-box attacks:** The attacker has some knowledge of the target system. Two common settings are:
  - LK with Surrogate Data (LK-SD), where \( \theta_{LK-SD} = (\hat{D}, X, g, \hat{w}) \),
  - LK with Surrogate Learners, where \( \theta_{LK-SL} = (\hat{D}, X, \hat{g}, \hat{w}) \).
  - Knowledge of the feature space and the ability to collect surrogate data, \( \theta \supseteq (\hat{D}, X) \), enables the attacker to perform mimicry attacks, where the attacker manipulates examples to resemble the high-density region of the target class [12, 28].
- **Zero Knowledge (ZK) black-box attacks:** The attacker has no information on the target system but has some information on the type of feature extraction performed (e.g., only static analysis in programs, or structural features in PDFs). In this case, \( \theta_{ZK} = (\hat{D}, \hat{X}, \hat{g}, \hat{w}) \).

Note that \( \theta_{PK} \) and \( \theta_{LK} \) imply knowledge of any defenses used to secure the target system against adversarial examples, depending on the degree to which each element is known [18].

**Attacker Capability:**
The capability of an attacker is expressed in terms of their ability to modify the feature space and problem space. The attacker's capability is described through feature-space constraints \( \Omega \) and problem-space constraints \( \Gamma \).

We observe that the attacker’s knowledge and capability can also be expressed according to the FAIL [65] model as follows:
- Knowledge of Features \( X \) (F),
- The learning Algorithm \( g \) (A),
- Instances in training \( D \) (I),
- Leverage on feature space and problem space with \( \Omega \) and \( \Gamma \) (L).

More details on the threat models can be found in [11, 65].

#### C. Theorem Proofs

**Proof of Theorem 1:**
We proceed with a proof by contradiction. Let us consider a problem-space object \( z \in Z \) with features \( x \in X \), which we want to misclassify as a target class \( t \in Y \). Without loss of generality, we consider a low-confidence attack, with desired attack confidence \( \kappa = 0 \) (see Equation 3). We assume by contradiction that there is no solution to the feature-space attack; more formally, that there is no solution \( \delta^* = \arg \min_{\delta \in \mathbb{R}^n : \delta \mid \Omega} f_t(x + \delta) \) that satisfies \( f_t(x + \delta^*) < 0 \).

This implies that there are no non-zero values in \( \delta^* \) that correspond to an actual feature-space perturbation. Then, a transformation sequence \( T : T(z) \mid \Gamma, T = T_{idx_q-1} \circ T_{idx_q-2} \circ \cdots \circ T_{idx_0} \) can always be constructed by the attacker to satisfy \( \phi(T(z)) = x + \delta^* \). We highlight that we do not consider the existence of a specific transformation in \( Z \) that maps to \( x + \delta^* \) because that may not be known by the attacker; hence, the attacker may never learn such a specific transformation. Thus, Equation 12 must be valid for all possible perturbations within the considered feature space.

#### D. Opaque Predicates Generation

We use opaque predicates [4] as inconspicuous conditional statements that always resolve to False to preserve the dynamic semantics of the Android applications.

To ensure the intractability of such an analysis, we follow the work of Moser et al. [51] and build opaque predicates using a formulation of the 3-SAT problem such that resolving the truth value of the predicate is equivalent to solving the NP-complete 3-SAT problem.

The k-satisfiability (k-SAT) problem asks whether the variables of a Boolean logic formula can be consistently replaced with True or False in such a way that the entire formula evaluates to True; if so, the formula is satisfiable. Such a formula is easily expressed in its conjunctive normal form:

\[
\bigwedge_{i=1}^{m} (V_{i1} \vee V_{i2} \vee \cdots \vee V_{ik}),
\]

where \( V_{ij} \in \{v_1, v_2, \ldots, v_n\} \) are Boolean variables and \( k \) is the number of variables per clause.

Importantly, when \( k = 3 \), formulas are only NP-Hard in the worst case—30% of 3-SAT problems are in P [61]. This baseline guarantee is not sufficient as our injected code should never execute. Additionally, we require a large number of random predicates to reduce commonality between the synthetic portions of our generated examples.

To consistently generate NP-Hard k-SAT problems, we use Random k-SAT [61], which involves three parameters: the number of variables \( n \), the number of clauses \( m \), and the number of literals per clause \( k \).

To construct a 3-SAT formula, \( m \) clauses of length 3 are generated by randomly choosing a set of 3 variables from the \( n \) available, and negating each with probability 50%. An empirical study by Selman et al. [61] showed that \( n \) should be at least 40 to ensure the formulas are hard to resolve. Additionally, they show that formulas with too few clauses are under-constrained, while formulas with too many clauses are over-constrained, both of which reduce the search time. These experiments led to the following conjecture.

**Threshold Conjecture [61]:** Let us define \( c^* \) as the threshold at which 50% of the formulas are satisfiable. For \( m/n < c^* \), the formula is satisfiable with high probability, and for \( m/n > c^* \), the formula is unsatisfiable with probability 100%.

### Algorithms

**Algorithm 1: Harvesting (Feature Extraction)**

**Input:**
- Discriminant function \( h(x) = w^T x + b \), which classifies \( x \) as malware if \( h(x) > 0 \), otherwise as goodware.
- Minimal app \( z_{min} \in Z \) with features \( \phi(z_{min}) = x_{min} \).
- Parameters: Number of features to consider \( n_f \); number of donors per feature \( n_d \).

**Output:**
- Ice-box of harvested organs with feature vectors.

1. ice-box ← {}
   - Empty key-value dictionary.
2. \( L \leftarrow \) List of pairs \( (w_i, i) \), sorted by increasing value of \( w_i \).
3. \( L' \leftarrow \) First \( n_f \) elements of \( L \), then remove any entry with \( w_i \geq 0 \).
4. for \( (w_i, i) \) in \( L' \) do
   - ice-box[i] ← []
   - while length(ice-box[i]) < \( n_d \) do
     - Pick a random donor app \( d \)
     - \( x_d \leftarrow \phi(d) \)
     - \( e_i \leftarrow \) one-hot vector for feature \( i \)
     - \( \rho_j \leftarrow \) gadget from \( d \)
     - \( r_j \leftarrow \) feature vector of \( \rho_j \)
     - \( s \leftarrow \) size of \( \rho_j \)
     - if \( r_j \land \neg x_d \neq 0 \) then
       - Discard the gadget;
     - else
       - Append \( (\rho_j, r_j, s) \) to ice-box[i]
       - Store gadget

**Algorithm 2: Attack (Adversarial Program Generation)**

**Input:**
- Discriminant function \( h(x) = w^T x + b \), which classifies \( x \) as malware if \( h(x) > 0 \), otherwise as goodware.
- Malware app \( z \in Z \).
- Ice-box \( G \).

**Parameters:**
- Problem-space constraints.

**Output:**
- Adversarial app \( z' \in Z \) such that \( h(\phi(z')) < 0 \).

1. \( T \leftarrow \) Transplantation through gadget addition.
2. \( \Upsilon \leftarrow \) Smoke test through app installation and execution in emulator.
3. \( \Pi \leftarrow \) Plausibility by-design through code consolidation.
4. \( \Lambda \leftarrow \) Artifacts from last column of Table I.
5. \( \Gamma \leftarrow \{T, \Upsilon, \Pi, \Lambda\} \)
6. \( s_z \leftarrow \) Software stats of \( z \)
7. \( x \leftarrow \phi(z) \)
8. \( L \leftarrow \) []
9. \( T(z) \leftarrow \) Empty sequence of problem-space transformations.
10. for \( (\rho_j, r_j, s) \) in \( G \) do
    - \( d_j \leftarrow r_j \land \neg x \)
    - \( \text{score}_j \leftarrow h(d_j) \)
    - Append the pair \( (\text{score}_j, i, j) \) to \( L \)
11. \( L' \leftarrow \) Sort \( L \) by increasing \( \text{score}_j \)
12. for \( (\text{score}_j, i, j) \) in \( L' \) do
    - if \( z \) has \( x_i = 1 \) then
      - Do nothing (feature already present);
    - else if \( z \) has \( x_i = 0 \) then
      - \( (\rho_j, r_j, s) \leftarrow \) element \( j \) in ice-box \( G \)
      - if check feasibility\( (s_z, s) \) is True then
        - \( x \leftarrow (x \lor e_i \lor \eta_j) \)
        - Append transplantation \( T \in T \) of gadget \( \rho_j \) in \( T(z) \)
        - if \( h(x) < 0 \) then
          - Exit from cycle (attack gadgets found);
13. \( z' \leftarrow \) Apply transformation sequence \( T(z) \) (Inject chosen gadgets).
14. if \( h(\phi(z')) < 0 \) and \( T(z) \mid \Gamma \) then
    - return \( z' \) (Attack successful);
15. else
    - return