### Final Dataset Construction

In the final step of dataset construction, we organize all samples according to their appearance timestamps and select a subset for each month over seven years. Specifically, we limit the selection to a maximum of 500 malware samples per month. If the number of samples in a given month is less than 500, we include all available samples; if it exceeds 500, we randomly select 500. Additionally, we randomly select benign apps, ensuring that the number of benign apps is nine times the number of malware samples in that month.

### Candidate Classifiers and Enhancement with APIGraph

We evaluate four state-of-the-art, representative malware classifiers and list them in Table 6. These classifiers are chosen from Tesseract [39] and DroidEvolver [49], which addresses classifier aging through a model pool. The selected classifiers span various machine learning algorithms and differ in their use of APIs, but all face the aging problem. The three classifiers from Tesseract do not update themselves, while DroidEvolver updates via online learning based on the majority voting of five models, though this can still lead to aging errors propagating across models.

#### Detailed Descriptions and Enhancements

- **MamaDroid [32]**: This classifier extracts API call pairs (caller and callee) and abstracts them into package call pairs. It then builds a Markov chain to model transitions between different packages, using transition probabilities as feature vectors for the app. We use the same configuration as described in the original paper. In our enhancement, APIGraph replaces each API call pair with an API cluster pair, which is then used in the Markov chain.

- **DroidEvolver [49]**: This classifier identifies all used APIs in an app via static analysis and constructs a binary vector of API occurrence as the feature vector. DroidEvolver maintains a model pool of five linear online learning algorithms and classifies apps using a weighted voting algorithm. When a model ages, it is updated incrementally based on the results of other un-aged models. We ensure consistency with the original implementation by contacting the authors. Our enhancement involves replacing the binary vector of API occurrence with one of API cluster occurrence.

- **Drebin [3]**: Drebin gathers a wide range of features, including hardware, API calls, permissions, and network addresses, for an SVM-based classifier. It focuses on a selected set of restricted and suspicious APIs. We implement Drebin following the detailed description and configuration in the original paper. For enhancement, APIGraph replaces the binary vector of API occurrence with one of API cluster occurrence.

- **Drebin-DL [18]**: This classifier uses the same feature set as Drebin but adopts Deep Neural Networks (DNN) for classification. We follow the implementation described in prior work [18]. The enhancement with APIGraph is similar to that of Drebin, replacing the binary vector of API occurrence with one of API cluster occurrence.

### Evaluation

Our evaluation aims to assess the effectiveness of APIGraph in enhancing state-of-the-art classifiers and capturing semantic similarity among Android APIs. Specifically, we address the following research questions:

- **RQ1: Model Maintainability Analysis**: How many human labeling efforts does APIGraph save in maintaining high-performance malware classifiers? (see ยง5.1)
- **RQ2: Model Sustainability Analysis**: How effective is APIGraph in slowing down classifier aging? (see ยง5.2)
- **RQ3: Feature Space Stability Analysis**: How effective is APIGraph in capturing similarity among evolved malware from the same family? (see ยง5.3)
- **RQ4: API Closeness Analysis**: How close are APIs in clusters grouped by APIGraph? (see ยง5.4)

#### RQ1: Model Maintainability Analysis

The goal of this research question is to determine how many human labeling efforts APIGraph can save while maintaining high-performance classifiers. We compare the amount of human effort required for active learning in both the original and enhanced classifiers, using two metrics: the number of malware samples to label and the retraining frequency. Active learning is implemented with uncertain sampling [39] to select the most uncertain predictions for labeling. We adopt two settings: a minimum F1 score threshold for introducing new samples and a fixed new sample ratio.

- **Active Learning with Fixed Retrain Thresholds**: When the F1 score of a classifier falls below a low threshold \( T_l \), active learning selects the most 1% uncertain samples to retrain the classifier, increasing the percentage by 1% until the F1 score reaches a higher threshold \( T_h \). The classifier is initially trained on all apps from 2012, and active learning is applied from January 2013 to December 2018.

  - **Results**: Table 7 shows the number of malware samples to label and the retraining frequency from 2013 to 2018 (\( T_l = 0.8, T_h = 0.9 \)). APIGraph reduces the number of samples to label by 33.07%, 37.82%, 96.30%, and 67.29% for MamaDroid, DroidEvolver, Drebin, and Drebin-DL, respectively, and also decreases the retraining frequency. 

- **Active Learning with Varied Learning Ratios**: We fix the ratio of newly introduced apps each month at 1%, 2.5%, 5%, 10%, and 50% and test the AUT(F1, 12m) for each classifier. The classifier is initially trained on 2012 data and tested monthly from January 2013 to December 2018.

  - **Results**: Table 8 shows the AUT(F1, 12m) values for the four classifiers before and after applying APIGraph. The AUT with APIGraph is consistently higher, demonstrating its effectiveness in slowing down model aging.

#### RQ2: Model Sustainability Analysis

This research question measures the performance of existing Android malware classifiers with and without APIGraph to understand its capability in slowing down model aging. We train a classifier on a particular year (e.g., 2012) and test its performance on 12 months of the next year (e.g., 2013), calculating the AUT.

- **Results**: Table 9 shows the AUT(F1, 12m) values for four classifiers tested from 2013 to 2018. The average AUT values improve by 19.2%, 19.6%, 15.6%, and 8.7% for the four classifiers, indicating that APIGraph effectively slows down model aging. Figure 6 breaks down the results into months, showing the F1 scores of the four classifiers in 2013 when trained with 2012 data.

#### RQ3: Feature Space Stability Analysis

This research question measures the feature space stability of evolved Android malware from the same family to show that APIGraph enhances the stability of the feature space.

- **Results**: [To be added based on specific experimental results and figures]

#### RQ4: API Closeness Analysis

This research question evaluates how close APIs in clusters grouped by APIGraph are.

- **Results**: [To be added based on specific experimental results and figures]

### Summary

- **RQ1**: APIGraph significantly reduces the number of manually-labeled samples and retrain frequency when maintaining high-performance malware classifiers.
- **RQ2**: APIGraph significantly enhances the sustainability of existing Android malware classifiers under evolved malware samples.
- **RQ3**: [To be summarized based on specific experimental results]
- **RQ4**: [To be summarized based on specific experimental results]