SVM
DNN
Selected API Occurrence
Selected API Occurrence
API feature format
Markov Chain of API Calls Random Forest
Classifier
MamaDroid [32]
DroidEvolver [49] API Occurrence
Drebin [3]
Drebin-DL [18]
â€¢ Step-3: Final dataset construction. We order all the samples
according to their appearance timestamps and select a subset
for each month over seven years. Specifically, we select at most
500 malware samples for a month: If the number in that month
is less than 500, we select all; if more than 500, we randomly
select 500. We also randomly select benign apps that are nine
times of malware in a month.
4.3 Candidate Classifiers and Enhancement
with APIGraph
We describe four state-of-the-art, representative malware classifiers
used in the evaluation and list them in Table 6. We choose all
three classifiers used in Tesseract [39] together with a recent,
state-of-the-art work, namely DroidEvolver [49], which delays
classifier aging via a model pool. These four classifiers span over
different machine learning algorithms and their usages of APIs also
differ, but they all face the aging problem. Particularly, the three
classifiers used in Tesseract do not update themselves; although
DroidEvolver updates itself via online learning based on the
majority voting of five models, the majority can age as well and
such errors may propagate to all the models.
Now let us describe these four classifiers in detail and how
APIGraph enhances these works by slowing down the aging
process. It is worth noting that our enhancement of classifiers
depends on the appearance year of the target appsâ€”that is, if
the target apps of the classifiers are from the year 2012, our
enhancement will be using the API relation graph of API Level
18, because this is the latest API level in 2012.
â€¢ MamaDroid [32] extracts API call pairs (i.e. caller and callee)
and then abstracts them into package call pairs.5 Next, Ma-
maDroid builds a Markov chain to model the transition
between different packages, and the transition probabilities
between packages are used as the feature vector for the app
in a learning algorithm. We get the entire source code of
5MamaDroid also provides â€œfamily modeâ€ where calls to APIs are abstracted to calls to
families, but its authors prove that â€œpackage modeâ€ is much better, so in this paper we
only consider MamaDroid in package mode, as is done by previous works [9, 39].
MamaDroid and use the same configuration as its paper.
APIGraph replaces each API call pair used in MamaDroidâ€™s
implementation with API cluster pair and then uses such pairs
in the Markov chain.
â€¢ DroidEvolver [49] finds all used APIs in an app via static
analysis and then builds a binary vector of API occurrence
as the feature vector for the app. After that, DroidEvolver
maintains a model pool of five linear online learning algorithms
to classify an app using a weighted voting algorithm. When
some model in the pool aged, it will be updated incrementally
based on the results of other un-aged models. We get the source
code of DroidEvolver and we contact the authors to make
sure our experiments are conducted consistently to their paper.
In our enhancement, APIGraph replaces the binary vector of
API occurrence with the one of API cluster occurrence.
â€¢ Drebin [3] gathers a wide range of features such as used
hardware, API calls, permissions, and network addresses for
an SVM-based classifier. In terms of the API feature, Drebin
considers a selected set of restricted and suspicious APIs that
can access to critical and sensitive data or resources. We
implement Drebin by strictly following the detailed description
and configuration in the paper. APIGraph also replaces the
binary vector of API occurrence in the aforementioned subset
with the one of API cluster occurrence for enhancement.
â€¢ Drebin-DL [18] uses the same feature set as Drebin but
adopts Deep Neural Networks (DNN) as the algorithm to do
classification. We also follow prior work [18] to implement
Drebin-DL. The enhancement of Drebin-DL with APIGraph
is the same as Drebin.
5 EVALUATION
In this section, we evaluate the effectiveness of APIGraph
in enhancing state-of-the-art classifiers as well as in capturing
the semantic similarity among Android APIs. Specifically, our
evaluation answers the following four research questions.
â€¢ RQ1: Model Maintainability Analysis. How many human
labeling efforts does APIGraph save in maintaining the high-
performance of a malware classifier? (see Â§5.1)
â€¢ RQ2: Model Sustainability Analysis. How effective is API-
â€¢ RQ3: Feature Space Stability Analysis. How effective is
APIGraph in capturing similarity among evolved malware from
the same family? (see Â§5.3)
â€¢ RQ4: API Closeness Analysis. How close are APIs in clusters
Graph in slowing classifier aging? (see Â§5.2)
grouped by APIGraph? (see Â§5.4)
Session 3B: Malware CCS '20, November 9â€“13, 2020, Virtual Event, USA7635.1 RQ1: Model Maintainability Analysis
The purpose of this research question is to find out how many
human efforts APIGraph can save while maintaining a high
performance classifier. Specifically, we compare the amount of
human efforts needed for active learning in maintaining both the
original and the enhanced classifiers. Let us look at some details.
First, the comparison adopts two metrics, which are (i) the number
of malware to label, and (ii) the retraining frequency. Second, the
active learning is implemented with uncertain sampling [39] to
actively select the most uncertain predictions to label. We further
adopt two settings for the active learning, which are a minimum ğ¹1
score for introducing new samples and a fixed new sample ratio.
5.1.1 Active learning with fixed retrain thresholds. In the first
setting, when the ğ¹1 score of a classifier falls below a low threshold
ğ‘‡ğ‘™, active learning is used to select the most 1% uncertain samples
to retrain the classifier, and then gradually increase the percentage
by 1% until the ğ¹1 score reaches another higher threshold ğ‘‡â„. In
the experiment, the classifier to start is trained on all the apps in
2012. We then adopt the aforementioned criterion to apply active
learning from Jan 2013 to Dec 2018, and observe the number of
malware to label and the retraining frequency.
Results: Table 7 shows the number of malware to label and the
retraining frequency from 2013 to 2018 (ğ‘‡ğ‘™ = 0.8,ğ‘‡â„ = 0.9). APIGraph
can save the amount of samples to label by 33.07%, 37.82%, 96.30%
and 67.29% respectively for MamaDroid, DroidEvolver, Drebin,
and Drebin-DL and the retrain frequency is reduced as well. There
are three things worth noting here. First, neither Drebin and
Drebin-DL are aware of model agingâ€”thus, it is unsurprising
that APIGraph can save a lot of human efforts. Second, although
DroidEvolver is aware of model aging and tries to improve the
model via online learning, APIGraph can still save a significant
amount of human efforts. The reason is that the majority results
of DroidEvolver may also make mistakes, which leads to a
propagation of such mistakes to other unaged models. Lastly,
Drebin, after combined with APIGraph, requires the least number
of samples to label. This is interesting because although the
performance of Drebin-DL is better than Drebin, Drebin with a
simpler ML algorithm is easier to maintain.
We also expand the samples to label and retrain times into both
cumulative and monthly-distribution numbers and show them in
Figure 5. One interesting phenomenon is that the number of labeled
samples for DroidEvolver and Drebin-DL stays almost the same
for many months, but then suddenly increases a lot especially
without the help of APIGraph. The reason is that DroidEvolver
and Drebin-DL have some capabilities, to a limited degree, of
capturing malware evolution, but once they do not capture one
type of evolution, the consequence is catastrophic, especially for
DroidEvolver. It is because DroidEvolver will propagate false
evolution information to other models in the pool, leading to a false
synchronization.
5.1.2 Active learning with varied learning ratios. The second active
learning setting is to fix the ratio of newly introduced apps each
month as 1%, 2.5%, 5%, 10%, and 50% and test the AUT(ğ¹1, 12m)
for each classifier. Similarly, we train a classifier with apps from
2012, and test the classifier month by month from Jan 2013 to Dec
Table 7: [RQ1] A summarization of Figure 5 on retrain times
and the number of labeled samples for active learning with
fixed retrain thresholds (ğ‘‡ğ‘™ = 0.8, ğ‘‡â„ = 0.9).
retrain times
# labeled samples
MamaDroid
DroidEvolver
Drebin
w/o 1 w/ 2 Improves
45
22.22%
21.05%
19
76.79%
56
26
38.46%
35
15
13
16
w/o
22,411
20,767
167,005
28,408
w/
14,999
12,913
6,173
9,292
Improves
33.07%
37.82%
96.30%
67.29%
Drebin-DL
1 w/o denotes the classifier without APIGraph, i.e. the original classifier.
2 w/ denotes the classifier enhanced with APIGraph.
Table 8: [RQ1] AUT(ğ¹1, 12m) of original (w/o) and enhanced
(w/) classifiers with different active learning ratios.
AL ratio MamaDroid DroidEvolver
w/
0.777
0.840
0.851
0.866
0.888
Drebin
w/
0.858
1%
0.878
2.5%
0.887
5%
0.895
10%
0.908
50%
1 w/o denotes the classifier without APIGraph, i.e. the original classifier.
2 w/ denotes the classifier enhanced with APIGraph.
Drebin-DL
w/
w/o
0.749
0.718
0.766
0.811
0.841
0.813
0.875
0.842
0.887
0.922
w/o 1 w/ 2 w/o
0.616
0.527
0.619
0.693
0.703
0.739
0.749
0.798
0.809
0.773
0.637
0.712
0.838
0.852
0.865
w/o
0.692
0.745
0.767
0.774
0.799
ğ‘âˆ’1
ğ‘˜=1
2018. Note that AUT is a metric proposed by Tesseract [39], which
defines the area under the curve in each figure to represent the
modelâ€™s sustainability as shown in Equation 1.
ğ´ğ‘ˆğ‘‡ (ğ‘“ , ğ‘) =
1
ğ‘ âˆ’ 1
[ğ‘“ (ğ‘˜ + 1) + ğ‘“ (ğ‘˜)]
2
(1)
where f is the performance metric (e.g. ğ¹1 score, Precision, Recall,
etc.), N is the number of test slots, and ğ‘“ (ğ‘˜) is performance metric
evaluated at the time k, and in our case the final metric is AUT(ğ¹1,
12m). An AUT metric that is closer to 1 means better performance
over time.
Results: We show the results in Table 8 for the four evaluated
classifiers before and after applying APIGraph. There are two
things worth noting here. First, the AUT with APIGraph of
each classifier is higher than the one without APIGraph. This
demonstrates that APIGraph can indeed slow model aging across
four different classifiers no matter they are evolution-aware or not.
Second, the aging slowdown of a model enhanced with APIGraph is
significant: For example, after enhancing DroidEvolver, retraining
with only 1% of apps can achieve even better performance than
retraining with 50% of apps for the original classifier without
enhancement.
Summary: APIGraph significantly reduces (i) the number
of manually-labeled samples and (ii) retrain frequency
when maintaining four different, high-performance mal-
ware classifiers.
5.2 RQ2: Model Sustainability Analysis
In this research question, we measure the performance of existing
Android malware classifiers with and without the help of APIGraph
Session 3B: Malware CCS '20, November 9â€“13, 2020, Virtual Event, USA764(a) The efforts in sample labeling for MamaDroid
(b) The efforts in sample labeling for DroidEvolver
(c) The efforts in sample labeling for Drebin
(d) The efforts in sample labeling for Drebin-DL
Figure 5: [RQ1] The number of malware samples to label using active learning with fixed retrain thresholds (ğ‘‡ğ‘™ = 0.8, ğ‘‡â„ = 0.9).
All the evaluated classifiers are trained using apps from 2012, and tested using apps from 2013â€“2018. The bar of a month shows
the number of new samples to be labeled in that month to retrain the classifier.
(a) MamaDroid
(b) DroidEvolver
(c) Drebin
(d) Drebin-DL
Figure 6: [RQ2] AUT(ğ¹1, 12m) of evaluated classifiers before and after leveraging API relation graph. Each classifier is trained
on 2012 and tested on 12 months of 2013. Note that month 0 indicates the time when the classifier is initially trained.
to understand the capability of APIGraph in slowing down model
aging. Our experiment setup is as follows. We train a classifier on
a particular year (say 2012), and test its performance on 12 months
of the next year (say 2013), and then also calculate the AUT. Note
that we only test the performance of a classifier over a year because
many classifiers have already become unusable after one year.
Results: Table 9 shows the AUT(ğ¹1, 12m) value of four classifiers
tested from 2013 to 2018 as well as the average. One important
observation is that the average AUT values improve 19.2%, 19.6%,
15.6%, 8.7% respectively for the four classifiers, which indicates
that APIGraph is capable of slowing down model aging. The AUT
values across different years are very similar, showing that malware
keeps evolving back to 2013 until very recently in 2018.
In Figure 6, we also break down the results into months and
show the ğ¹1 score of four classifiers in 2013 when trained with
data in 2012. We observe that the performance of Drebin-DL and
Drebin are the best among all four classifiers in terms of aging:
Specifically, the ğ¹1 score only drops from close to 1 to above 0.8.
This is probably because Drebin adopts a selected subset of APIs,
which has some capability in capturing malware evolution.
Summary: APIGraph significantly enhances the sus-
tainability of existing Android malware classifiers under
evolved malware samples.
5.3 RQ3: Feature Space Stability Analysis
In this research question, we measure the feature space stability
of evolved Android malware from the same family to show that
0123456789101112Testing period (month)0.000.250.500.751.00F1 scorew/o APIGraph: 0.46w/ APIGraph: 0.680123456789101112Testing period (month)0.000.250.500.751.00F1 scorew/o APIGraph: 0.72w/ APIGraph: 0.830123456789101112Testing period (month)0.000.250.500.751.00F1 scorew/o APIGraph: 0.78w/ APIGraph: 0.880123456789101112Testing period (month)0.000.250.500.751.00F1 scorew/o APIGraph: 0.82w/ APIGraph: 0.87Session 3B: Malware CCS '20, November 9â€“13, 2020, Virtual Event, USA765Table 9: [RQ2] AUT(ğ¹1, 12m) of evaluated classifiers before
and after leveraging API relation graph. For each testing
year, the classifiers are trained on the previous year.
Testing
Years
2013
2014
2015
2016
2017
2018
MamaDroid DroidEvolver
w/o 1 w/ 2 w/o
0.462
0.717
0.712
0.456