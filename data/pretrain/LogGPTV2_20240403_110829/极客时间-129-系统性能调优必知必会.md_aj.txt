# 20 \| CAP理论：怎样舍弃一致性去换取性能？你好，我是陶辉。上一讲我们介绍了如何通过监控找到性能瓶颈，从这一讲开始，我们将具体讨论如何通过分布式系统来提升性能。在第一部分课程中，我介绍了多种提升单机处理性能的途径，然而，进程的性能必然受制于一台服务器上各硬件的处理能力上限。如果需要进一步地提升服务性能，那只有整合多台主机组成分布式系统才能办到。然而，当多台主机通过网络协同处理用户请求时，如果主机上的进程含有数据状态，那么必然需要在多台主机之间跨网络同步数据，由于网络存在时延，也并不稳定，因此不可靠的数据同步操作将会增加请求的处理时延。CAP理论指出，当数据同时存放在多个主机上时，可用性与一致性是不可兼得的。根据CAP的指导性思想，我们可以通过牺牲一致性，来提升可用性中的核心因素：性能。当然，在实践中对一致性与性能并不是非黑即白的选择，而是从概率上进行不同程度的取舍。这一讲，我们将基于分布式系统中的经典理论，从总体上看看如何设计一致性模型，通过牺牲部分数据的一致性来提升性能。如何权衡性能与一致性？首先，这节课针对的是**有状态服务**的性能优化。所谓有状态服务，是指进程会在处理完请求后，仍然保存着影响下次请求结果的用户数据，而无状态服务则只从每个请求的输入参数中获取数据，在请求处理完成后并不保存任何会话信息。因此，无状态服务拥有最好的可伸缩性，但涉及数据持久化时，则必须由有状态服务处理，这是CAP 理论所要解决的问题。什么是 CAP 理论呢？这是 2000年 [University ofCalifornia, Berkeley的计算机教授Eric Brewer（也是谷歌基础设施VP）提出的理论。所谓 CAP，是以下 3个单词的首字母缩写，它们都是分布式系统最核心的特性：1.  **C**        onsistency    一致性    2.  **A**        vailability    可用性    3.  **P**        artition tolerance    分区容错性        我们通过以下 3 张示意图，快速理解下这 3 个词的意义。下图中 N1、N2两台主机上运行着 A 进程和 B进程，它们操作着同一个用户的数据（数据的初始值是 V0），这里 N1 和 N2主机就处于不同的 Partition分区中，如下所示：![](Images/6025a3a75f77db35600260354076273a.png)savepage-src="https://static001.geekbang.org/resource/image/74/f4/744834f9a5dd04244e5f83719fb3f6f4.jpg"}正常情况下，当用户请求到达 N1 主机上的 A 进程，并将数据 V0 修改为 V1后，A 进程将会把这一修改行为同步到 N2 主机上的 B 进程，最终 N1、N2上的数据都是 V1，这就保持了系统的 Consistency一致性。 ![](Images/e9f3079352714410e1234082c50b9bad.png)savepage-src="https://static001.geekbang.org/resource/image/23/25/23ab7127cbc89bcbe83c2a3669d66125.jpg"}然而，一旦 N1 和 N2 之间网络异常，数据同步行为就会失败。这时，N1 和N2 之间数据不一致，如果我们希望在分区间网络不通的情况下，N2能够继续为用户提供服务，就必须容忍数据的不一致，此时系统的 Availability可用性更高，系统的并发处理能力更强，比如 Cassandra数据库。 ![](Images/14218835a16731c1fa018ad5bcccbb21.png)savepage-src="https://static001.geekbang.org/resource/image/a7/fb/a7302dc2491229a019aa27f043ba08fb.jpg"}反之，如果 A、B 进程一旦发现数据同步失败，那么 B进程自动拒绝新请求，仅由 A进程独立提供服务，那么虽然降低了系统的可用性，但保证了更强的一致性，比如MySQL 的主备同步模式。这就是 CAP 中三者只能取其二的简要示意，对于这一理论，2002 年 MIT的 [SethGilbert   、 [NancyLynch  在这篇论文slate-object="inline"中，证明了这一理论。当然，可用性slate-object="inline"是一个很大的概念，它描述了分布式系统的持续服务能力，如下表所示：![](Images/99cc308d7aad63cb086bdfa22793316e.png)savepage-src="https://static001.geekbang.org/resource/image/0c/df/0c11b949c35e4cce86233843ccb152df.jpg"}当用户、流量不断增长时，系统的性能将变成衡量可用性的关键因素。当我们希望拥有更强的性能时，就不得不牺牲数据的一致性。当然，一致性并不是只有是和否这两种属性，我们既可以从时间维度上设计短暂不一致的同步模型，也可以从空间维度上为不含有因果、时序关系的用户数据设计并发模型。在学术上，通常会按照 2个维度对一致性模型slate-object="inline"分类。首先是从数据出发设计出的一致性模型，比如顺序一致性必须遵照读写操作的次序来保持一致性，而弱一些的因果一致性则允许不具备因果关系的读写操作并发执行。其次是从用户出发设计出的一致性模型，比如单调读一致性保证客户端不会读取到旧值，而单调写一致性则保证写操作是串行的。实际工程中一致性与可用性的边界会模糊很多，因此又有了最终一致性slate-object="inline"这样一个概念，这个"最终"究竟是多久，将由业务特性、网络故障率等因素综合决定。伴随最终一致性的是BASE 理论： 1.  **B**        asically         **A**        vailable    基本可用性        2.  **S**        oft state    软状态    3.  **E**        ventually consistent    最终一致性        BASE 与 CAP一样并没有很精确的定义，它们最主要的用途是从大方向上给你指导性的思想。接下来，我们看看如何通过最终一致性来提升性能。怎样舍弃一致性提升性能？服务的性能，主要体现在请求的时延和系统的并发性这两个方面，而它们都与上面提到的最终一致性有关。我通常会把分布式系统分为纵向、横向两个维度，其中**纵向是请求的处理路径，横向则是同类服务之间的数据同步路径。**这样，在纵向上在离客户端更近的位置增加数据的副本，并把它存放在处理速度更快的物理介质上，就可以作为缓存降低请求的时延；而在横向上对数据增加副本，并在这些主机间同步数据，这样工作在数据副本上的进程也可以同时对客户端提供服务，这就增加了系统的并发性，如下图所示：![](Images/11b8983a203e302eb36972427386c7b1.png)savepage-src="https://static001.geekbang.org/resource/image/5a/06/5a6dbac922c500eea108d374dccc6406.png"}我们先来看纵向上的缓存，是如何在降低请求处理时延时，保持最终一致性的。缓存可以由读、写两个操作触发更新，\[第 15 课\ 介绍过的 HTTP私有缓存，就是工作在浏览器上基于读操作触发的缓存。![](Images/2bb475551410d037f0a7746b4ce44190.png)savepage-src="https://static001.geekbang.org/resource/image/9d/ab/9dea133d832d8b7ab642bb74b48502ab.png"}工作在代理服务器上的 HTTP共享缓存，也是由读操作触发的。这些缓存与源服务器上的数据最终是一致的，但在CacheControl 等 HTTP头部指定的时间内，缓存完全有可能与源数据不同，这就是牺牲一致性来提升性能的典型例子。事实上，也有很多以写操作触发缓存更新的设计，它们通常又分为 write back和 write through两种模式。其中，**write back牺牲了更多的一致性，但带来了更低的请求时延。****比如****\[第 4 课\]**** 介绍过的 Linux磁盘高速缓存就采用了 write back这种设计，它虽然是单机内的一种缓存设计，但在分布式系统中缓存的设计方式也是一样的。而****write through会在更新数据成功后再更新缓存，虽然带来了很好的一致性，但写操作的时延会更久**，如下图所示：![](Images/b5e1925b45dee5b2698e56341bcffc70.png)savepage-src="https://static001.geekbang.org/resource/image/de/ac/de0ed171a392b63a87af28b9aa6ec7ac.png"}write through的一致性非常好，它常常是我们的首选设计。然而，一旦缓存到源数据的路径很长、延时很高的时候，就值得考虑write back模式，此时一致性模型虽然复杂了许多，但可以带来显著的性能提升。比如机械磁盘相对内存延时高了很多，因此磁盘高速缓存会采用write back 模式。虽然缓存也可以在一定程度上增加系统的并发处理连接，但这更多是缘于缓存采用了更快的算法以及存储介质带来的收益。从水平方向上，在更多的主机上添加数据副本，并在其上用新的进程提供服务，这才是提升系统并发处理能力最有效的手段。此时，进程间同步数据主要包含两种方式：同步方式以及异步方式，前者会在每个更新请求完成前，将数据成功同步到每个副本后，请求才会处理完成，而后者则会在处理完请求后，才开始异步地同步数据到副本中，如下图所示：![](Images/8378112424cecbcb52d98882a1b202a6.png)savepage-src="https://static001.geekbang.org/resource/image/d7/f6/d770cf24f61d671ab1f0c1a6170627f6.png"}同步方式下系统的一致性最好，但由于请求的处理时延包含了副本间的通讯时长，所以性能并不好。而异步方式下系统的一致性要差，特别是在主进程宕机后，副本上的进程很容易出现数据丢失，但异步方式的性能要好得多，这不只由于更新请求的返回更快，而且异步化后主进程可以基于合并、批量操作等技巧，进行效率更高的数据同步。比如MySQL的主备模式下，默认就使用异步方式同步数据，当然你也可以改为同步方式，包括Full-synchronous Replication（同步所有数据副本后请求才能返回）以及Semi-synchronous Replication（仅成功同步 1个副本后请求就可以返回）两种方式。当然，在扩容、宕机恢复等场景下，副本之间的数据严重不一致，如果仍然基于单个操作同步数据，时间会很久，性能很差。此时应结合定期更新的Snapshot 快照，以及实时的 Oplog操作日志，协作同步数据，这样效率会高很多。其中，快照是截止时间 T0时所有数据的状态，而操作日志则是 T0 时间到当下 T1之间的所有更新操作，这样，副本载入快照恢复至 T0时刻的数据后，再通过有限的操作日志与主进程保持一致。小结这一讲我们介绍了分布式系统中的 CAP 理论，以及根据 CAP如何通过一致性来换取性能。CAP理论指出，可用性、分区容错性、一致性三者只能取其二，因此当分布式系统需要服务更多的用户时，只能舍弃一致性，换取可用性中的性能因子。当然，性能与一致性并不是简单的二选一，而是需要你根据网络时延、故障概率设计出一致性模型，在提供高性能的同时，保持时间、空间上可接受的最终一致性。具体的设计方法，可以分为纵向上添加缓存，横向上添加副本进程两种做法。对于缓存的更新，writethrough 模式保持一致性更容易，但写请求的时延偏高，而一致性模型更复杂的write back模式时延则更低，适用于性能要求很高的场景。提升系统并发性可以通过添加数据副本，并让工作在副本上的进程同时对用户提供服务。副本间的数据同步是由写请求触发的，其中包括同步、异步两种同步方式。异步方式的最终一致性要差一些，但写请求的处理时延更快。在宕机恢复、系统扩容时，采用快照加操作日志的方式，系统的性能会好很多。思考题最后留给你一道讨论题，当数据副本都在同一个 IDC内部时，网络时延很低，带宽大而且近乎免费使用。然而，一旦数据副本跨 IDC后，特别是 IDC之间物理距离很遥远，那么网络同步就变得很昂贵。这两种不同场景下，你是如何考虑一致性模型的？它与性能之间有多大的影响？期待你的分享。感谢阅读，如果你觉得这节课对你有一些启发，也欢迎把它分享给你的朋友。
# 21 \| AKF立方体：怎样通过可扩展性来提高性能？你好，我是陶辉。上一讲我们谈到，调低一致性可以提升有状态服务的性能。这一讲我们扩大范围，结合无状态服务，看看怎样提高分布式系统的整体性能。当你接收到运维系统的短信告警，得知系统性能即将达到瓶颈，或者会议上收到老板兴奋的通知，接下来市场开缰拓土，业务访问量将要上一个大台阶时，一定会马上拿起计算器，算算要加多少台机器，系统才能扛得住新增的流量。然而，有些服务虽然可以通过加机器提升性能，但可能你加了一倍的服务器，却发现系统的吞吐量没有翻一倍。甚至有些服务无论你如何扩容，性能都没有半点提升。这缘于我们扩展分布式系统的方向发生了错误。当我们需要分布式系统提供更强的性能时，该怎样扩展系统呢？什么时候该加机器？什么时候该重构代码？扩容时，究竟该选择哈希算法还是最小连接数算法，才能有效提升性能？在面对 Scalability可伸缩性问题时，我们必须有一个系统的方法论，才能应对日益复杂的分布式系统。这一讲我将介绍AKF 立方体理论，它定义了扩展系统的 3个维度，我们可以综合使用它们来优化性能。如何基于 AKF X 轴扩展系统？AKF 立方体也叫做scala cube，它在《The Art ofScalability》一书中被首次提出，旨在提供一个系统化的扩展思路。AKF把系统扩展分为以下三个维度：1.  X    轴：直接水平复制应用进程来扩展系统。        2.  Y    轴：将功能拆分出来扩展系统。        3.  Z    轴：基于用户信息扩展系统。        如下图所示：![](Images/79d96e3bc718ee79b6b3fa7389b69f90.png)savepage-src="https://static001.geekbang.org/resource/image/61/aa/61633c7e7679fd10b915494b72abb3aa.jpg"}我们日常见到的各种系统扩展方案，都可以归结到 AKF立方体的这三个维度上。而且，我们可以同时组合这 3个方向上的扩展动作，使得系统可以近乎无限地提升性能。为了避免对 AKF的介绍过于抽象，下面我用一个实际的例子，带你看看这 3个方向的扩展到底该如何应用。假定我们开发一个博客平台，用户可以申请自己的博客帐号，并在其上发布文章。最初的系统考虑了MVC 架构，将数据状态及关系模型交给数据库实现，应用进程通过 SQL语言操作数据模型，经由 HTTP协议对浏览器客户端提供服务，如下图所示：![](Images/6fe7c3b8248d97f53adad7cd26b1ed91.png)savepage-src="https://static001.geekbang.org/resource/image/cd/e0/cda814dcfca8820c81024808fe96b1e0.jpg"}在这个架构中，处理业务的应用进程属于无状态服务，用户数据全部放在了关系数据库中。因此，当我们在应用进程前加1个负载均衡服务后，就可以通过部署更多的应用进程，提供更大的吞吐量。而且，初期增加应用进程，RPS可以获得线性增长，很实用。![](Images/ce5e00dd273726deeb01abfbdfa8c4de.png)savepage-src="https://static001.geekbang.org/resource/image/d7/62/d7f75f3f5a5e6f8a07b1c47501606962.png"}这就叫做沿 AKF X轴扩展系统。这种扩展方式最大的优点，就是开发成本近乎为零，而且实施起来速度快！在搭建好负载均衡后，只需要在新的物理机、虚拟机或者微服务上复制程序，就可以让新进程分担请求流量，而且不会影响事务Transaction 的处理。当然，AKF X轴扩展最大的问题是只能扩展无状态服务，当有状态的数据库出现性能瓶颈时，X轴是无能为力的。例如，当用户数据量持续增长，关系数据库中的表就会达到百万、千万行数据，SQL语句会越来越慢，这时可以沿着 AKF Z轴去分库分表提升性能。又比如，当请求用户频率越来越高，那么可以把单实例数据库扩展为主备多实例，沿Y 轴把读写功能分离提升性能。下面我们先来看 AKF Y轴如何扩展系统。如何基于 AKF Y 轴扩展系统？当数据库的 CPU、网络带宽、内存、磁盘 IO等某个指标率先达到上限后，系统的吞吐量就达到了瓶颈，此时沿着 AKF X轴扩展系统，是没有办法提升性能的。在现代经济中，更细分、更专业的产业化、供应链分工，可以给社会带来更高的效率，而AKF Y轴与之相似，当遇到上述性能瓶颈后，拆分系统功能，使得各组件的职责、分工更细，也可以提升系统的效率。比如，当我们将应用进程对数据库的读写操作拆分后，就可以扩展单机数据库为主备分布式系统，使得主库支持读写两种SQL，而备库只支持读SQL。这样，主库可以轻松地支持事务操作，且它将数据同步到备库中也并不复杂，如下图所示：![](Images/b85364402bfb6252590856fbf1af0695.png)savepage-src="https://static001.geekbang.org/resource/image/86/74/865885bb7213e62b8e1b715d85c9a974.png"}当然，上图中如果读性能达到了瓶颈，我们可以继续沿着 AKF X轴，用复制的方式扩展多个备库，提升读 SQL 的性能，可见，AKF多个轴完全可以搭配着协同使用。拆分功能是需要重构代码的，它的实施成本比沿 X轴简单复制扩展要高得多。在上图中，通常关系数据库的客户端 SDK已经支持读写分离，所以实施成本由中间件承担了，这对我们理解 Y轴的实施代价意义不大，所以我们再来看从业务上拆分功能的例子。当这个博客平台访问量越来越大时，一台主库是无法扛住所有写流量的。因此，基于业务特性拆分功能，就是必须要做的工作。比如，把用户的个人信息、身份验证等功能拆分出一个子系统，再把文章、留言发布等功能拆分到另一个子系统，由无状态的业务层代码分开调用，并通过事务组合在一起，如下图所示：![](Images/f50f5e193d3cf73674431b13d5694ca8.png)savepage-src="https://static001.geekbang.org/resource/image/3b/af/3bba7bc19965bb9b01c058e67a6471af.png"}这样，每个后端的子应用更加聚焦于细分的功能，它的数据库规模会变小，也更容易优化性能。比如，针对用户登录功能，你可以再次基于Y 轴将身份验证功能拆分，用 Redis 等服务搭建一个基于 LRU算法淘汰的缓存系统，快速验证用户身份。然而，沿 Y轴做功能拆分，实施成本非常高，需要重构代码并做大量测试工作，上线部署也很复杂。比如上例中要对数据模型做拆分（如同一个库中的表拆分到多个库中，或者表中的字段拆到多张表中），设计组件之间的API交互协议，重构无状态应用进程中的代码，为了完成升级还要做数据迁移，等等。解决数据增长引发的性能下降问题，除了成本较高的 AKF Y 轴扩展方式外，沿Z轴扩展系统也很有效，它的实施成本更低一些，下面我们具体看一下。如何基于 AKF Z 轴扩展系统？不同于站在服务角度扩展系统的 X 轴和 Y 轴，AKF Z轴则从用户维度拆分系统，它不仅可以提升数据持续增长降低的性能，还能基于用户的地理位置获得额外收益。仍然以上面虚拟的博客平台为例，当注册用户数量上亿后，无论你如何基于 Y轴的功能去拆分表（即"垂直"地拆分表中的字段），都无法使得关系数据库单个表的行数在千万级以下，这样表字段的B 树索引非常庞大，难以完全放在内存中，最后大量的磁盘 IO 操作会拖慢 SQL语句的执行。这个时候，关系数据库最常用的分库分表操作就登场了，它正是 AKF 沿 Z轴拆分系统的实践。比如已经含有上亿行数据的 User 用户信息表，可以分成 10个库，每个库再分成 10张表，利用固定的哈希函数，就可以把每个用户的数据映射到某个库的某张表中。这样，单张表的数据量就可以降低到1百万行左右，如果每个库部署在不同的服务器上（具体的部署方式视访问吞吐量以及服务器的配置而定），它们处理的数据量减少了很多，却可以独占服务器的硬件资源，性能自然就有了提升。如下图所示：![](Images/11f52cde3eab9bde14551bb75206367b.png)savepage-src="https://static001.geekbang.org/resource/image/dc/83/dc9e29827c26f89ff3459b5c99313583.png"}分库分表是关系数据库中解决数据增长压力的最有效办法，但分库分表同时也导致跨表的查询语句复杂许多，而跨库的事务几乎难以实现，因此这种扩展的代价非常高。当然，如果你使用的是类似MySQL这些成熟的关系数据库，整个生态中会有厂商提供相应的中间件层，使用它们可以降低Z 轴扩展的代价。再比如，最开始我们采用 X轴复制扩展的服务，它们的负载均衡策略很简单，只需要选择负载最小的上游服务器即可，比如RoundRobin 或者最小连接算法都可以达到目的。但若上游服务器通过 Y轴扩展，开启了缓存功能，那么考虑到缓存的命中率，就必须改用 Z轴扩展的方式，基于用户信息做哈希规则下的新路由，尽量将同一个用户的请求命中相同的上游服务器，才能充分提高缓存命中率。Z 轴扩展还有一个好处，就是可以充分利用 IDC与用户间的网速差，选择更快的 IDC为用户提供高性能服务。网络是基于光速传播的，当 IDC跨城市、国家甚至大洲时，用户访问不同 IDC的网速就会有很大差异。当然，同一地域内不同的网络运营商之间，也会有很大的网速差。例如你在全球都有 IDC或者公有云服务器时，就可以通过域名为当地用户就近提供服务，这样性能会高很多。事实上，CDN技术就基于 IP地址的位置信息，就近为用户提供静态资源的高速访问。下图中，我使用了 2 种 Z轴扩展系统的方式。首先是基于客户端的地理位置，选择不同的 IDC就近提供服务。其次是将不同的用户分组，比如免费用户组与付费用户组，这样在业务上分离用户群体后，还可以有针对性地提供不同水准的服务。![](Images/e65e10ac674cfc3205fe45716f8a614f.png)savepage-src="https://static001.geekbang.org/resource/image/35/3b/353d8515d40db25eebee23889a3ecd3b.png"}沿 AKF Z轴扩展系统可以解决数据增长带来的性能瓶颈，也可以基于数据的空间位置提升系统性能，然而它的实施成本比较高，尤其是在系统宕机、扩容时，一旦路由规则发生变化，会带来很大的数据迁移成本，\[第24 讲\我将要介绍的一致性哈希算法，其实就是用来解决这一问题的。小结这一讲我们介绍了如何基于 AKF 立方体的 X、Y、Z三个轴扩展系统提升性能。X轴扩展系统时实施成本最低，只需要将程序复制到不同的服务器上运行，再用下游的负载均衡分配流量即可。X轴只能应用在无状态进程上，故无法解决数据增长引入的性能瓶颈。Y轴扩展系统时实施成本最高，通常涉及到部分代码的重构，但它通过拆分功能，使系统中的组件分工更细，因此可以解决数据增长带来的性能压力，也可以提升系统的总体效率。比如关系数据库的读写分离、表字段的垂直拆分，或者引入缓存，都属于沿Y 轴扩展系统。Z轴扩展系统时实施成本也比较高，但它基于用户信息拆分数据后，可以在解决数据增长问题的同时，基于地理位置就近提供服务，进而大幅度降低请求的时延，比如常见的CDN 就是这么提升用户体验的。但 Z轴扩展系统后，一旦发生路由规则的变动导致数据迁移时，运维成本就会比较高。当然，X、Y、Z 轴的扩展并不是孤立的，我们可以同时应用这 3个维度扩展系统。分布式系统非常复杂，AKF给我们提供了一种自上而下的方法论，让我们能够针对不同场景下的性能瓶颈，以最低的成本提升性能。思考题最后给你留一道思考题，我们在谈到 Z轴扩展时（比如关系数据库的分库分表），提到了基于哈希函数来设置路由规则，请结合\[第 3 讲\的内容，谈谈你认为应该如何设计哈希函数，才能使它满足符合 Z轴扩展的预期？期待你的总结。感谢阅读，如果你觉得这节课对你有一些启发，也欢迎把它分享给你的朋友。