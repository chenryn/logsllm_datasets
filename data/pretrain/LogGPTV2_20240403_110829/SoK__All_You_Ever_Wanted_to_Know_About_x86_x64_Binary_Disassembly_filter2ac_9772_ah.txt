n
N
A
D
I
i
Pre
96.66 87.09 99.88 90.84 88.50 80.92 100.0 11.23 70.84 95.46
77.30 97.79
98.15 82.78 99.91 85.24 85.64 88.41 100.0 7.37
72.89 94.62
96.58 83.80 99.86 87.61 83.03 85.65 99.44 7.56
96.56 84.19 99.83 88.48 83.44 87.85 99.44 7.51
72.91 94.96
95.33 96.86 99.87 99.99 55.33 80.21 93.14 16.40 99.65 71.22
96.43 97.80 99.89 99.99 97.30 91.87 96.67 35.85 99.82 73.65
95.72 96.13 99.88 99.98 97.48 92.52 97.04 41.41 99.99 59.71
95.58 96.12 99.86 99.98 96.61 92.48 97.11 42.05 99.99 60.03
-
96.05 70.89 99.29 73.92
-
97.00 80.00 99.99 80.87
-
96.43 70.22 99.99 72.59
-
96.40 72.77 99.99 69.23
97.74 85.12 99.11 86.83
96.24 84.29
98.97 80.03 99.90 81.14
88.89 71.56
98.07 79.59 99.91 80.79
95.29 64.49
98.03 80.71 99.90 81.26
95.16 64.03
96.98 95.77 99.87 98.00 96.25 77.06 100.0 53.89 99.43 100.0
98.36 95.79 99.87 97.41 90.95 81.80 100.0 58.09 99.66 99.81
97.12 93.92 99.85 97.07 88.18 83.34 100.0 63.38 99.91 99.85
97.11 93.73 99.84 96.88 88.32 81.99 100.0 63.19 99.66 99.81
98.63 97.39 99.90 99.75 84.89 80.37 100.0 38.66 96.97 93.09
99.20 98.60 99.80 99.83 95.31 91.89 100.0 47.30 99.61 90.22
98.59 96.63 99.88 98.93 95.49 92.76 100.0 48.67 96.49 91.49
98.59 96.82 99.86 99.10 95.61 92.83 100.0 48.43 96.50 91.55
0.00
0.00
100.0 1.52
100.0 6.64
100.0 6.16
100.0 6.30
100.0 6.43
100.0 6.56
100.0 6.55
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
calls. The recovery rate ranges from 71.7% (DYNINST) to
91.28% (BINARY NINJA);
the precision varies from 67.39%
(DYNINST) to 90.21% (GHIDRA). We observe that many tools
(ANGR, GHIDRA, BINARY NINJA) has the lowest precision
with optimization disabled. This is because less tail calls are
emitted at lower optimization levels and a few false positives
can result in a low precision.
Finally, tools can detect non-returning functions with very
high precision (nearly 100% for many tools). However, they
have limited coverage, especially on Windows binaries. Based
on our analysis, the root cause of the low coverage is incom-
plete collection of non-returning library functions.
Use of Heuristics: The recovery of edges and call graphs
share heuristics in disassembly so we omit the details.
The resolving of jump tables uses three heuristics. First,
RADARE2, GHIDRA, and DYNINST use patterns to detect
jump tables, identifying 85.53%, 98.01%, and 99.56% of all
jump tables. Second, DYNINST and ANGR restrict the scope
of slicing in VSA, missing the index bound in many jump
tables. By enlarging the scope of slicing to 500 assignments
in DYNINST and 10 basic blocks in ANGR, the two tools ﬁnd
the index bound in 2.1% and 19.8% more jump tables. Third,
GHIDRA and RADARE2 discard jump tables with index over
1,024 and 512, respectively missing 51 and 2,435 jump tables.
To detect tail calls, RADARE2 relies on the distance between
jumps and their targets. It is hard to pick a proper threshold:
in our benchmark binaries, the distance for regular jumps (min:
0; max: 0xb5867) and tail calls (min: 0; max: 0xb507c5) are largely
overlapped. GHIDRA uses the heuristic that a tail call and
its target spans multiple functions. This heuristic is general
to capture tail calls, producing a high coverage of 91.29%.
However,
it cannot recognize regular jumps between two
TABLE XV: Statistics of false positives in CFG-edge recovery.
Non-Ret, Func, T-Call, Inst, No-split, Jump-Tab respectively
represent undetected non-returning functions, unrecognized
functions, unidentiﬁed tail calls, wrong instructions, missing
edges to middle of basic blocks, and wrong jump table targets.
Tools
Dyninst
Ghidra
Angr
Bap
Radare2
Percentage of False Positives (%)
Non-Ret
23.09
8.76
16.69
37.95
73.32
Func
12.25
1.88
38.42
22.30
3.90
T-Call
23.28
13.00
10.82
1.34
15.55
Inst
4.95
0.11
4.73
0.34
0.67
No-split
35.24
75.13
27.02
38.06
4.23
J-Tab
1.19
1.12
2.32
0
2.34
parts of non-continuous functions, producing over 70K false
positives. ANGR and DYNINST use control-ﬂow and stack-
height based heuristics, detecting 4.24% and 6.99% more tail
calls. However, the heuristics are neither sound nor complete,
producing both false negatives and false positives.
Understanding of Errors: Table XV presents the analysis
of false positives in edge recovery. The false negatives of
edge recovery are mostly caused by non-recovered instructions
(see § IV-B1). We omit the details. For call graphs, the false
positives and false negatives are generally side effects of errors
in disassembly. Details are also discussed in § IV-B1.
The errors related to jump tables are case speciﬁc and
tool speciﬁc. We randomly sample 10% false negatives and
10% false positives from each tool and do manual analysis
to understand the reasons. The false negatives vary across
tools. RADARE2, GHIDRA, and DYNINST rely on patterns
to detect jump tables, leading to 64.1%, 31.4%, and 37.72%
of their false negatives. Further, RADARE2 incurs 35.9% of
its false negatives because it discards jump tables with over
512 entries; GHIDRA produces 62.63%, 5.50%, and 0.47% of
its false negatives, respectively because it does not consider
sub instructions in VSA analysis, does not capture the correct
restrictions to the index, and discards jump tables with more
than 1,024 entries; ANGR generates false negatives because of
no VSA results (67.86%), wrong VSA results leading to over
100,000 entries (14.29%), and no handling of sbb instructions
(17.86%); DYNINST’s remaining false negatives are caused
by its restriction of slicing scope (2.1%) and no handling
of get_pc_thunk (60.18%). Beyond the above reasons, we
also observe 6 cases where the index is unrestricted (e.g.,
Listing 10), leading to false negatives in many tools. The
causes of false positives are also diverse. RADARE2’s false
positives are mostly because it matches the wrong cmp/sub
instruction (54.29%) or the restriction to the index is not by
cmp/sub (45.71%). DYNINST produces false positives because
it does not handle special aliases of the index (78.96%), does not
consider restrictions by type (17.95%), and restricts the scope
of slicing (3.09%). ANGR incurs false positives due to incorrect
VSA solving (78.26%) and ignoring certain paths in slicing
(21.74%). With GHIDRA, the false positives are because of
wrongly considering indirect jumps as indirect calls (55.32%),
no consideration of sub instruction in VSA analysis (34.04%),
identifying extra/wrong restrictions to the index (4.26%), and
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:37 UTC from IEEE Xplore.  Restrictions apply. 
845
TABLE XVI: Statistics of false negatives in detection of
non-returning functions. Lib, Cond-NonRet, Exit-Inst, Fake-
Ret, Propa respectively stand for unrecognized non-returning
library functions, argument-dependent non-returning functions
(e.g., error), instructions that exit (e.g., ud2), functions
that contain ret instructions but do not actually return, and
propagation of other types of false negatives.
Tools
Dyninst
Ghidra
Angr
Bap
Radare2
Lib
12.30
3.97
19.08
24.45
19.89
Percentage of False Negatives (%)
Cond-NonRet
Exit-Inst
Fake-Ret
43.86
13.01
21.70
22.73
26.04
0.30
0.04
0.00
0.10
0.08
27.64
9.54
22.29
28.90
13.07
Propa
15.90
73.44
36.93
23.82
40.92
including the default case as an entry (6.38%).
In detecting tail calls, false negatives are largely caused
by excluding conditional jumps (FN ratio - GHIDRA: 21.6%;
ANGR 17.5%), missing boundaries between adjacent functions
(FN ratio - GHIDRA: 78.4%), excluding targets reached by both
conditional jumps and unconditional jumps (FN ratio - ANGR:
81.6%), incorrect calculation of stack height (FN ratio - ANGR:
0.9%), unrecognized functions and no stack adjustment patterns
before the jumps (FN ratio - DYNINST:100%). For false posi-
tives, the most common cause is wrong function entries (FP
ratio - GHIDRA: 5.9%; ANGR 19.7%; DYNINST: 2.4%). Other reasons
include non-continuous functions (FP ratio - GHIDRA: 94.1%;
ANGR 0.3%; DYNINST: 0.6%), instructions follow the format of
add esp/rsp, CONST but do not tear down the stack (FP
ratio - DYNINST:97%), unchanged stack height before the jump
(FP ratio - ANGR: nearly 90%). We also notice that ANGR’s false
positives contain a group of conditional jumps, which are
caused by a defect in its code.
Table XVI shows the false negatives in detecting non-
returning functions. A special category is functions that have
ret instructions but do not return (e.g., _Unwind_Resume
in Glibc). Such functions alter the stack to use a self-prepared
return address, avoiding returning to the parent. We observe
no false positives with DYNINST. RADARE2 generates false
positives because of un-handled jump tables (23.40%), wrong
function boundaries (4.26%), and propagation of other false
positives (72.34%). ANGR produces false positives due to un-
handled jump tables/tail calls (29.73%), wrong function bound-
aries (54.05%), and propagation of other false positives (16.22%).
GHIDRA incurs all its false positives due to heuristic 29. False
positives by BAP are due to implementation issues.
C. Threats to Completeness and Validity
Despite our best efforts to carefully evaluate the public
versions of the selected tools, there could still be threats to the
completeness and validity of our results. Completeness wise,
we may have missed reporting the results of some tools in
certain tasks due to implementation issues of the tools (other
than fundamental
limitations). For instance, we could not
report Angr’s results of symbolization with Windows binaries
because of two implementation issues in Angr’s Reassembler
module (one is reported at https://github.com/angr/angr/issues/
1998; the other one is because Reassembler tries to parses the
PLT section that does not exist in Windows binaries). Validity
wise, implementation issues in our evaluation and analysis
code may have slipped into the results, leading to inaccurate
reports of the tools’ performance. To mitigate this threat, we
checked if the evaluation results match our understanding of
the tools. In particular, we checked if the false positives and
false negatives were explainable by the tool’s strategies.
V. FINDINGS
This study uncovered the following major ﬁndings.
F-1: Complex constructs are common and heuristics are
indispensable to handle them. We observe a large amount of
complex constructs in our benchmarks (Table XVII). In partic-
ular, we found 295 instances of data-in-code, complementary
to previous research [5]. Heuristics are indispensable to handle
the constructs: they are responsible for 49% of the instructions,
17% of the function entries, and most xrefs.
F-2: Heuristics inherently introduce coverage-correctness
trade-offs. Heuristics signiﬁcantly increase coverage, but con-
currently induce new errors. As a counter-movement, new
heuristics are devised to reduce the errors, leading to coverage-
correctness trade-offs in nearly every disassembly phase.
F-3: Tools complement each other. As shown in Table III, ex-
isting tools use heuristics and algorithms that have overlaps but
also many differences. This features tools different strengths,
indicating that tool selection should be demand-speciﬁc.
F-4: Broader and deeper evaluation is needed for future
improvements. We envision that the community may have
insufﬁciently evaluated existing tools. This prevents compre-
hensive understanding about the limitations of the tools. We
hope our study will bring a piece of basis to broaden and
deepen the evaluation towards better binary disassembly.
VI. CONCLUSION
We present a systematization of binary disassembly with
a thorough study and comprehensive evaluation, centering
around the perspective of algorithms and heuristics. Our study,
via the comprehension on the source code of 9 disassembly
tools, presents in-depth understanding of their strategies. Our
evaluation separately measures the tools on each disassem-
bly phase and on individual strategies, which fully unveils
how much the heuristics are used, how much the heuristics
contribute to disassembly, and what errors the heuristics can
introduce. Throughout the study, we derive a group of new
observations that can amend/complement previous understand-
ings and also inspire future directions of binary disassembly.
ACKNOWLEDGMENTS
We would like to thank our shepherd Yan Shoshitaishvili
and the anonymous reviewers for their feedback. This project
was supported by the Ofﬁce of Naval Research (Grant#:
N00014-16-1-2261, N00014-17-1-2788, and N00014-17-1-
2787). Any opinions, ﬁndings, and conclusions or recommen-
dations expressed in this paper are those of the authors and
do not necessarily reﬂect the views of the funding agency.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:37 UTC from IEEE Xplore.  Restrictions apply. 
846
REFERENCES
[1] “Ida help: Kernel analysis options,” https://www.hex-rays.com/products/ida/
support/idadoc/621.shtml.
[2] M. Abadi, M. Budiu,
´U. Erlingsson, and J. Ligatti, “Control-ﬂow integrity
principles, implementations, and applications,” ACM Transactions on Information
and System Security (TISSEC), vol. 13, no. 1, p. 4, 2009.
[3] N. S. Agency, “Ghidra,” https://www.nsa.gov/resources/everyone/ghidra/, 2019.
[4] H. Alasmary, A. Anwar, J. Park, J. Choi, D. Nyang, and A. Mohaisen, “Graph-
based comparison of iot and android malware,” in International Conference on
Computational Social Networks. Springer, 2018, pp. 259–272.
[5] D. Andriesse, X. Chen, V. Van Der Veen, A. Slowinska, and H. Bos, “An in-depth
analysis of disassembly on full-scale x86/x64 binaries,” in 25th USENIX Security
Symposium, 2016, pp. 583–600.
[6] D. Andriesse, A. Slowinska, and H. Bos, “Compiler-agnostic function detection
in binaries,” in 2017 IEEE European Symposium on Security and Privacy
(EuroS&P).
IEEE, 2017, pp. 177–189.
[7] W. Armstrong, P. Christen, E. McCreath, and A. P. Rendell, “Dynamic algorithm
learning,” in 2006 International Workshop on
selection using reinforcement
Integrating AI and Data Mining.
IEEE, 2006, pp. 18–25.
[8] C. S. L. at UC Santa Barbara, “angr github repo,” https://github.com/angr/angr/
tree/76da434f, 2019.
[9] T. Bao, J. Burket, M. Woo, R. Turner, and D. Brumley, “Byteweight: Learning to
recognize functions in binary code,” in 23rd USENIX Security Symposium, 2014,
pp. 845–860.
[10] T. Barabosch, “cwe checker ﬁnds vulnerable patterns in binary executables,”
https://github.com/fkie-cad/cwe checker, 2018.
[11] A. R. Bernat and B. P. Miller, “Anywhere, any-time binary instrumentation,” in
10th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools.
ACM, 2011, pp. 9–16.
[12] P. Biondi, R. Rigo, S. Zennou, and X. Mehrenberger, “Bincat: purrfecting binary
static analysis,” in Symposium sur la s´ecurit´e des technologies de linformation et
des communications, 2017.
[13] L. Bits, “Mcsema github repo,” https://github.com/lifting-bits/mcsema/tree/
62a1319e, 2019.
[14] M. Bourquin, A. King, and E. Robbins, “Binslayer: accurate comparison of
binary executables,” in the 2nd ACM SIGPLAN Program Protection and Reverse
Engineering Workshop. ACM, 2013, p. 4.
[15] D. Bruschi, L. Martignoni, and M. Monga, “Detecting self-mutating malware
using control-ﬂow graph matching,” in International conference on detection of
intrusions and malware, and vulnerability assessment. Springer, 2006, pp. 129–
143.
[16] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley, “Unleashing mayhem on
IEEE,
binary code,” in 2012 IEEE Symposium on Security and Privacy (SP).
2012, pp. 380–394.
[17] M. Chandramohan, Y. Xue, Z. Xu, Y. Liu, C. Y. Cho, and H. B. K. Tan, “Bingo:
Cross-architecture cross-os binary search,” in the 2016 24th ACM SIGSOFT
International Symposium on Foundations of Software Engineering. ACM, 2016,
pp. 678–689.
[18] P. Chen, J. Xu, Z. Hu, X. Xing, M. Zhu, B. Mao, and P. Liu, “What you see is not
what you get thwarting just-in-time rop with chameleon,” in 2017 47th Annual
IEEE/IFIP International Conference on Dependable Systems and Networks (DSN).
IEEE, 2017, pp. 451–462.
[19] X. Chen, A. Slowinska, D. sse, H. Bos, and C. Giuffrida, “Stackarmor: Compre-
hensive protection from stack-based memory error vulnerabilities for binaries.” in
NDSS, 2015.
[20] Y. Chen, D. Mu, J. Xu, Z. Sun, W. Shen, X. Xing, L. Lu, and B. Mao,
“Ptrix: Efﬁcient hardware-assisted fuzzing for cots binary,” in 2019 ACM on Asia
Conference on Computer and Communications Security. ACM, 2019, pp. 1032–
1043.
[21] Y. Chen, D. Zhang, R. Wang, R. Qiao, A. M. Azab, L. Lu, H. Vijayakumar, and
W. Shen, “Norax: Enabling execute-only memory for cots binaries on aarch64,” in
2017 IEEE Symposium on Security and Privacy (SP).
IEEE, 2017, pp. 304–319.
[22] V. Chipounov and G. Candea, “Enabling sophisticated analyses of× 86 binaries
with revgen,” in 2011 IEEE/IFIP 41st International Conference on Dependable
Systems and Networks Workshops (DSN-W).
IEEE, 2011, pp. 211–216.
[23] C. Cifuentes and M. Van Emmerik, “Recovery of jump table case statements from
binary code,” Science of Computer Programming, vol. 40, no. 2-3, pp. 171–188,
2001.
[24] coreboot, “Ghidra ﬁrmware utilities, weeks 1-2,” https://blogs.coreboot.org/blog/
2019/06/04/gsoc-ghidra-ﬁrmware-utilities-weeks-1-2/, 2019.
[25] M. Cova, V. Felmetsger, G. Banks, and G. Vigna, “Static detection of vulnerabil-