set to 1. If no instruction of a certain class is present, the corresponding bit is 0.
Table 1 lists the 14 color classes that are used in our system. Note that it is
no longer possible to substitute an add with a sub instruction, as both are part
of the data transfer instruction class. However, in some cases, it might be pos-
sible to replace one instruction by an instruction in another class. For example,
the value of register %eax can be set to 0 both by a mov 0, %eax instruction
(which is in the data transfer class) or by a xor %eax, %eax instruction (which
is a logic instruction). While instruction substitution attacks cannot be com-
pletely prevented when using color classes, they are made much more diﬃcult
for an attacker. The reason is that there are less possibilities for ﬁnding seman-
tically equivalent instructions from diﬀerent classes. Furthermore, the possible
variations in color that can be generated with instructions from diﬀerent classes
is much less than the possible variations on the instruction level. In certain cases,
218
C. Kruegel et al.
Table 1. Color classes
Description
Class
Data Transfer mov instructions
Arithmetic
Logic
Test
Stack
Branch
Call
Class
String
incl. shift and rotate
Flags
incl. bit/byte operations LEA
Float
test and compare
Syscall
push and pop
conditional control ﬂow
Jump
Halt
function invocation
Description
x86 string operations
access of x86 ﬂag register
load eﬀective address
ﬂoating point operations
interrupt and system call
unconditional control ﬂow
stop instruction execution
it is even impossible to replace an instruction with a semantically equivalent one
(e.g., when invoking a software interrupt).
6 Worm Detection
Our algorithm to detect worms is very similar to the Earlybird approach pre-
sented in [19]. In the Earlybird system, the content of each network ﬂow is
processed, and all substrings of a certain length are extracted. Each substring
is used as an index into a table, called prevalence table, that keeps track of how
often that particular string has been seen in the past. In addition, for each string
entry in the prevalence table, a list of unique source-destination IP address pairs
is maintained. This list is searched and updated whenever a new substring is
entered. The basic idea is that sorting this table with respect to the substring
count and the size of the address lists will produce the set of likely worm traﬃc
samples. That is, frequently occurring substrings that appear in network traﬃc
between many hosts are an indication of worm-related activity. Moreover, these
substrings can be used directly as worm signatures.
The key diﬀerence between our system and previous work is the mechanism
used to index the prevalence table [17]. While Earlybird uses simple substrings,
we use the ﬁngerprints that are extracted from control ﬂow graphs. That is, we
identify worms by checking for frequently occurring executable regions that have
the same structure (i.e., the same ﬁngerprint).
This is accomplished by maintaining a set of network streams Si for each given
ﬁngerprint fi. Every set Si contains the distinct source-destination IP address
pairs for streams that contained fi. A ﬁngerprint is identiﬁed as corresponding
to worm code when the following conditions on Si are satisﬁed:
1. m, the number of distinct source-destination pairs contained in Si, meets or
exceeds a predeﬁned threshold M.
2. The number of distinct internal hosts appearing in Si is at least 2.
3. The number of distinct external hosts appearing in Si is at least 2.
The last two conditions are required to prevent false positives that would
otherwise occur when several clients inside the network download a certain exe-
cutable ﬁle from an external server, or when external clients download a binary
Polymorphic Worm Detection Using Structural Information of Executables
219
from an internal server. In both cases, the traﬃc patterns are diﬀerent from
the ones generated by a worm, for which one would expect connections between
multiple hosts from both the inside and outside networks.
7 Evaluation
7.1 Identifying Code Regions
The ﬁrst goal of the evaluation of the prototype system was to demonstrate that
the system is capable of distinguishing between code and non-code regions of net-
work streams. To accomplish this, the tool was executed over several datasets.
The ﬁrst dataset was composed of the ELF executables from the /bin and
/usr/bin directories of a Gentoo Linux x86 installation. The second dataset was
a collection of around 5 Gigabytes of media ﬁles (i.e., compressed audio and video
ﬁles). The third dataset was 1 Gigabyte of random output from OpenBSD 3.6’s
ARC4 random number generator. The ﬁnal dataset was a 1.5 Gigabyte selection
of texts from the Project Gutenberg electronic book archive. These datasets were
selected to reﬂect the types of data that might commonly be encountered by the
tool during the processing of real network traﬃc. For each of the datasets, the
total number of ﬁngerprints, total Kilobytes of data processed, and the number
of ﬁngerprints per Kilobyte of data were calculated. For this and all following
experiments, we use a value of 10 for k. The results are shown in Table 2.
Table 2. Fingerprint statistics for various datasets
Dataset
Executables
Media
Random
Text
Total Fingerprints Total KB Fingerprints/KB
128.673495
0.042569
0.042253
0.000036
18,882,894
209,348
43,267
54
146,750
4,917,802
1,024,000
1,503,997
By comparing the number of ﬁngerprints per Kilobyte of data for each of the
datasets, it is clear that the tool can distinguish valid code regions from other
types of network data. As asserted in Section 4, disassemblies that contain invalid
instruction sequences within basic blocks or a lack of suﬃciently connected basic
blocks produce many subgraphs with less than 10 nodes. Since a ﬁngerprint is
only produced for a subgraph with at least 10 nodes, one expects the rate of
ﬁngerprints per Kilobyte of data to be quite small, as we see for the media,
random, and text datasets. On the other hand, disassemblies that produce large,
strongly-connected graphs (such as those seen from valid executables) result in
a large rate of ﬁngerprints per Kilobyte, as we see from the executables dataset.
7.2 Fingerprint Function Behavior
As mentioned in Section 3, the ﬁngerprints generated by the prototype system
must ideally be “unique” so that diﬀerent subgraphs will not map to the same
220
C. Kruegel et al.
Table 3. Fingerprint collisions for coreutils dataset
Fingerprints Total Collisions Collision Rate Mismatched Coll. Mismatch Rate
83,033
17,320
20.86%
84
0.10%
ﬁngerprint. To evaluate the extent to which the system adheres to this property,
the following experiment was conducted to determine the rate of ﬁngerprint col-
lisions from non-identical subgraphs. The prototype was ﬁrst run over a set of 61
ELF executables from the Linux coreutils package that had been compiled with
debugging information intact, including the symbol table. The ﬁngerprints and
corresponding subgraphs produced during the run were extracted and recorded.
An analyzer then processed the subgraphs, correlating each node’s address with
the symbol table of the corresponding executable to determine the function from
which the subgraph was extracted. Finally, for those ﬁngerprints that were pro-
duced by subgraphs from multiple executables, the analyzer compared the list
of functions the subgraphs had been extracted from. The idea was to determine
whether the ﬁngerprint collision was a result of shared code or rather was a
violation of the ﬁngerprint uniqueness property. Here, we assume that if all sub-
graphs were extracted from functions that have the same name, they are the
result of the same code. The results of this experiment are shown in Table 3.
From the table, we can see that for the coreutils package, there is a rather large
ﬁngerprint collision rate, equal to about 21%. This, however, was an expected
result; the coreutils package was chosen as the dataset for this experiment in
part because all executables in the package are statically linked with a library
containing utility functions, called libfetish. Since static linking implies that
code sections are copied directly into executables that reference those sections,
a high degree of code sharing is present in this dataset, resulting in the observed
ﬁngerprint collision rate.
The mismatched collisions column records the number of ﬁngerprint collisions
between subgraphs that could not be traced to a common function. In these cases,
we must conclude that the ﬁngerprint uniqueness property has been violated,
and that two diﬀerent subgraphs have been ﬁngerprinted to the same value. The
number of such collisions in this experiment, however, was very small; the entire
run produced a mismatched collision rate of about 0.1%.
As a result of this experiment, we conclude that the prototype system pro-
duces ﬁngerprints that generally map to unique subgraphs with an acceptably
small collision rate. Additionally, this experiment also demonstrates that the
tool can reliably detect common subgraphs resulting from shared code across
multiple analysis targets.
7.3 Analysis of False Positive Rates
In order to evaluate the degree to which the system is prone to generating false
detections, we evaluated it on a dataset consisting of 35.7 Gigabyte of network
traﬃc collected over 9 days on the local network of the Distributed Systems
Polymorphic Worm Detection Using Structural Information of Executables
221
Table 4. Incorrectly labeled ﬁngerprints as a function of M. 1,400,174 total ﬁngerprints
were encountered in the evaluation set.
M
Fingerprints
M
Fingerprints
M
Fingerprints
12
1,134
21
22
7
16
44
25
22
4
5
6
3
11
12,661 7,841 7,215 3,647 3,441 3,019 2,515 1,219 1,174
20
23
17
43
18
43
19
24
10
8
9
13
944
22
22
14
623
23
22
15
150
24
22
Group at the Technical University of Vienna. This evaluation set contained
661,528 total network streams and was veriﬁed to be free of known attacks.
The data consists to a large extent of HTTP (about 45%) and SMTP (about
35%) traﬃc. The rest is made up of a wide variety of application traﬃc including
SSH, IMAP, DNS, NTP, FTP, and SMB traﬃc.
In this section, we explore the degree to which false positives can be mitigated
by appropriately selecting the detection parameter M. Recall that M determines
the number of unique source-destination pairs that a network stream set Si must
contain before the corresponding ﬁngerprint fi is considered to belong to a worm.
Also recall that we require that a certain ﬁngerprint must occur in network streams
between two or more internal and external hosts, respectively, before being consid-
ered as a worm candidate. False positives occur when legitimate network usage is
identiﬁed as worm activity by the system. For example, if a particular ﬁngerprint
appears in too many (benign) network ﬂows between multiple sources and desti-
nations, the system will identify the aggregate behavior as a worm attack. While
intuitively it can be seen that larger values of M reduce the number false positives,
they simultaneously delay the detection of a real worm outbreak.
Table 4 gives the number of ﬁngerprints identiﬁed by the system as suspi-
cious for various values of M. For comparison, 1,400,174 total ﬁngerprints were
observed in the evaluation set. This experiment indicates that increasing M be-
yond 20 achieves diminishing returns in the reduction of false positives (for this
traﬃc trace). The remainder of this section discusses the root causes of the false
detections for the 23 erroneously labeled ﬁngerprint values for M = 20.
The 23 stream sets associated with the false positive ﬁngerprints contained
a total of 8,452 HTTP network ﬂows. Closer inspection of these showed that
the bulk of the false alarms were the result of binary resources on the site that
were (a) frequently accessed by outside users and (b) replicated between two
internal web servers. These accounted for 8,325 ﬂows (98.5% of the total) and