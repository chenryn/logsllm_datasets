包号132和133（见图6）：
图6
客户端：“那我试一下mount进程能否连上。”
服务器：“收到了，能连上。”
(4)
包号134和135（见图7）：
图7
客户端：“我要挂载/code共享目录。”
服务器：“你的请求被批准了。以后请用file handle 0x2cc9be18 来访问本目录。”
(5)
包号140和141（见图8）：
图8
 客户端：“我试一下NFS进程能否连上。”
服务器：“收到了，能连上。”
(6)
包号143和144（见图9）：
图9
客户端：“我想看看这个文件系统的属性。”
服务器：“给，都在这里。”
(7)
包号145和146（见图10）：
图10
客户端：“我想看看这个文件系统的属性。”
服务器：“给，都在这里。”
(8)
以上便是NFS挂载的全过程。细节之处很多，所以在没有Wireshark的情况下很难排错，经常不得不盲目地检查每一个环节，比如先用rpcinfo命令获得服务器上的端口列表（见图11），再用Telnet命令逐个试探（见图12）。即使这样也只能检查几个关键进程能否连上，排查范围非常有限。
图11
图12
用上Wireshark之后就可以很有针对性地排查了。例如，看到portmap请求没有得到回复，就可以考虑防火墙对111端口的拦截；如果发现mount请求被服务器拒绝了，就应该检查该共享目录的访问控制。
既然说到访问控制，我们就来看看NFS在安全方面的机制，包括对客户端的访问控制和对用户的权限控制。
NFS对客户端的访问控制是通过IP地址实现的。创建共享目录时可以指定哪些IP允许读写，哪些IP只允许读，还有哪些IP连挂载都不允许。虽然配置不难，但这方面出的问题往往很“诡异”，没有Wireshark是几乎无法排查的。比如，我碰到过一台客户端的IP明明已经加到允许读写的列表里，结果却只能读。这个问题难住了很多工程师，因为在客户端和服务器上都找不到原因。后来我们在服务器上抓了个包，才知道在收到的包里，客户端的IP已经被NAT设备转换成别的了。
NFS的用户权限也经常让人困惑。比如在我的实验室中，客户端A上的用户admin在/code目录里新建一个文件，该文件的owner正常显示为admin。但是在客户端B上查看该文件时，owner却变成nasadmin，过程如下所示。
 客户端A（见图13）：
图13
客户端B（见图14）：
图14
这是为什么呢？借助Wireshark，我们很容易就能看到原因。图15显示了用户admin在创建/tmp/code/abc.txt时的包。
图15
由图15中的Credentials信息可知，用户在创建文件时并没有使用admin这个用户名，而是用了admin的UID 501来代表自己的身份（用户名与UID的对应关系是由客户端的/etc/passwd决定的）。也就是说NFS协议是只认UID不认用户名的。当admin通过客户端A创建了一个文件，其UID 501就会被写到文件里，成为owner信息。
而当客户端B上的用户查看该文件属性时，看到的其实也是“UID: 501”。但是因
 为客户端B上的/etc/passwd文件和客户端A上的不一样，其UID 501对应的用户名叫nasadmin，所以文件的owner就显示为nasadmin了。同样道理，当客户端B上的用户nasadmin在共享目录上新建一个文件时，客户端A上的用户看到的文件owner就会变成admin。为了防止这类问题，建议用户名和UID的关系在每台客户端上都保持一致。
弄清楚了NFS的安全机制后，我们再来看看读写过程。经验丰富的工程师都知道，性能调优是最有技术含量的。借助Wireshark，我们可以看到NFS究竟是如何读写文件的，这样才能理解不同mount参数的作用，也才能有针对性地进行性能调优。图16展示了读取文件abc.txt的过程。
[root@shifm1 tmp]# cat /tmp/code/abc.txt
图16
包号2和3（见图17）：
图17
客户端：“我可以进入0x2cc9be18（也就是/code的file handle）吗？”
服务器：“你的请求被接受了，进来吧。”
包号5和6（见图18）：
图18
 客户端：“我想看看这个目录里的文件及其file handle。”
服务器：“文件名及file handle的信息在这里。其中abc.txt的file handle是0x531352e1。”
(9)
包号8和9（见图19）：
图19
客户端：“0x531352e1（也就是abc.txt）的文件属性是什么？“
服务器：“权限、uid、gid, 文件大小等信息都给你。”包号11和12（见图20）：
图20
客户端：“我可以打开0x531352e1（也就是abc.txt）吗？”
服务器：“你的请求被允许了。你有读、写、执行等权限。”包号13、14、152、292（见图21）：
图21
客户端：“从0x531352e1的偏移量为0处（即从abc.txt的开头位置）读131072字节。”
 客户端：“从0x531352e1的偏移量为131072处（即接着上一个请求读完的位置）再读131072字节。”
服务器：“给你131072字节。”
服务器：“再给你131072字节。”
（继续读，直到读完整个文件。）
就这样，NFS完成了文件的读取过程。从最后几个包可见，Linux客户端读NFS共享文件时是多个READ Call连续发出去的（本例中是连续两个）。这个方式跟Windows XP读CIFS共享文件有所不同。Windows XP不会连续发READ Call，而是先发一个Call，等收到Reply后再发下一个。相比之下，Linux这种读方式比Windows XP更高效，尤其是在高带宽、高延迟的环境下。这就像叫外卖一样，如果你今晚想吃鸡翅、汉堡和可乐三样食物，那合理的方式应该是打一通电话把三样都叫齐了。而不是先叫鸡翅，等鸡翅送到了再叫汉堡，等汉堡送到后再叫可乐。除了读文件的方式，每个READ Call请求多少数据也会影响性能。这台Linux默认每次读131072字节，我的实验室里还有默认每次读32768字节的客户端。在高性能环境中，要手动指定一个比较大的值。比如在我的Isilon实验室中，常常要调到512KB。这个值可以在mount时通过rsize参数来定义，比如“mount -o rsize=524288 10.32.106.62:/code /tmp/code”。
分析完读操作，接下来我们再看看写文件的过程。把一个名为abc.txt的文件写到NFS共享的过程如下（见图22）。