n
e
v
i
t
a
l
u
m
u
C
12600
12400
12200
12000
11800
11600
11400
11200
AS22388
AS3356
T=60
T=120
T=180
T=240
T=300
T=360
T=420
T=480
T=540
T=600
s
r
e
t
s
u
l
c
f
o
r
e
b
m
u
n
e
v
i
t
a
l
u
m
u
C
143000
142000
141000
140000
139000
138000
137000
136000
135000
T=60
T=120
T=180
T=240
T=300
T=360
T=420
T=480
T=540
T=600
0
60 120 180 240 300 360 420 480 540 600
Time variance V of each cluster (sec)
0
60 120 180 240 300 360 420 480 540 600
(a) Time variance V of each cluster (sec)
Fig. 2. Distribution of clusters created at diﬀerent window times for AS22388 and
AS3356
1. For each preferred path, cluster candidate origins detected within T seconds
from the ﬁrst detected origin.
2. If the same origin is detected within T seconds, create a new cluster from
that origin. This means the link recovered once within T seconds and we
consider the second detection to be from a diﬀerent failure event.
3. For each cluster, infer the link closest to the monitor as the origin.
To ﬁnd the most relevant window time T , we focus on the convergence delay of
the monitoring network. Precisely, we look at the number of clusters created at
diﬀerent intervals of T and measure the time variance V (T ≥ V ) of the detection
times for each cluster. Speciﬁcally, for each cluster, we measure the diﬀerence
in the detection times between the ﬁrst detected link and the last detected link.
If a cluster includes only a single link or includes multiple links with the same
detection time, the time variance for these clusters is 0. The distribution of this
variance should reﬂect the actual convergence delay of that network.
Figure 2 shows the number of clusters created at diﬀerent window times T
and the distribution of time variances V for monitoring points at AS22388 and
AS3356, respectively. For the value of T , we used 60 seconds to 600 seconds
at intervals of 60 seconds. As both ﬁgures show, the time variance for a large
proportion of the clusters remains at V = 0 for all values of T . This indicates that
for about 90% and 97% of the events at AS22388 and AS3356, respectively, our
scheme can cluster links inferred from the same event with very high accuracy
and only a few events require the window time to absorb the convergence delay.
In Figure 2, the number of clusters is similar to T = 240 for AS22388 and
from T = 300 for AS3356. This indicates that a possible T that reﬂects the
convergence delay is within these ranges. For the evaluation in the following
section, we use T = 180 for AS22388 and T = 300 for AS3356, where the latter
is the convergence delay often referred to in the Internet [7].
What is interesting about our scheme is that we can further leverage our
information to infer the time of the failure (not the time of detection) and the
time of recovery. The time of failure is deﬁned as the time when the path of a
preﬁx ﬁrst changes from its preferred path. Conversely, the time of recovery is
when the path of a preﬁx recovers using its preferred path. In our measurement,
Inferring the Origin of Routing Changes Based on Preferred Path Changes
169
Session 
Failure
Route 
Views
AS
22388
8 prefixes
347 prefixes
3 prefixes
Connected to 
53 other ASes
4 prefixes
A total of 256 affected prefixes
4 prefixes
1 prefix
AS
7660
AS
2907
AS
23800
Route 
Views
AS
22388
AS
11537
AS
14048
AS
16473
Monitoring 
Point
Connected to 
8 other ASes
Connected to 
36 other ASes
Connected to 
70 other ASes
A total of 6,896 affected prefixes
Candidate set of Links
Monitoring 
Point
Connected to 
8 other ASes
Connected 
to 6 other 
ASes
AS
7660
Connected to 
3 other ASes
Candidate set of Links
AS22388
7 prefixes via 
1 prefix via 
AS11537
Date
2009/09/15
2009/09/15
Detection Time Link Name
22388,7660
7660,2907
05:11:21
05:11:22
Failure Time
05:11:21
05:11:22
Recovery Time
05:15:34
05:15:34
Date
2010/03/12
2010/03/12
Detection Time Link Name
11537,14048
14048,16473
05:02:11
05:02:11
Failure Time
05:02:11
05:02:11
Recovery Time
05:14:36
05:14:36
(a) Case Study 1
(b) Case Study 2
Fig. 3. Case Study: (a)Single core link failure, (b) Multiple link failure
we found cases where the failure time is several minutes before the detection
time due to long lasting path exploration.
5 Evaluation
In this section, we discuss the validity of our scheme using Oregon RouteViews
data and trouble tickets from several operational networks.
5.1 Evaluation Using Operational Tickets
We evaluate our scheme using operational tickets provided by TRANSPAC2
(AS22388) [13], APAN-JP (AS7660) [14] and Internet2 (AS11537) [15]. Each
ticket provides information including the location, the start and end time and
the cause of detected failures. For the BGP data, we use Route Views collected
from AS22388. Below we describe two case studies in detail and present how our
inference scheme inferred origins for each event.
Case Study 1. Tickets at TRANSPAC2 and APAN-JP describe a session fail-
ure between the two ASes beginning at 05:10 a.m. to 05:15 a.m. on September
15, 2009 (UTC) due to an exceed in the number preﬁxes allowed on the link.
Figure 3 (a) show the results of our inference using the preferred path of AS23800
as an example. Two candidate links were detected during this period with 1
second diﬀerence in detection time. Our heuristics clustered the two links and
inferred the link (22388,7660) as the origin. The same origin was also inferred for
402 other preferred paths aﬀecting a total of 6,896 preﬁxes. Since the inferred
origin, the failure and the recovery times all match those reported in the ticket,
we conﬁrmed that the origin was accurately inferred by our scheme. We also
conﬁrmed that no other tickets were issued nor other candidates were detected
for this link during this month. Note that the failure time and the detection time
are identical because path exploration was not observed for this event.
Case Study 2. A ticket at Internet2 (AS11537) describes a core router in Chicago
being unavailable for several peers starting from 05:00 a.m. to 05:03 a.m. on March
12, 2010 (UTC). The reported cause was a router maintenance. Figure 3 (b) show
170
M. Watari, A. Tachibana, and S. Ano
Table 4. The number of links inferred as origins with and without our scheme
Monitoring Point
AS22388
AS3356
Our Scheme Disabled Enabled Disabled Enabled
56,172
42,364
208,391 99,317
208,391 88,439
Measured Links
2,533
Candidate Links 22,784
Inferred Origins
22,784
2,092
9,189
6,362
the results of our inference. Using the preferred path of AS16473 as an example, we
detected two links as candidates having the same detection time and inferred the
link (11537,14048) as the origin. From routing changes of other preferred paths,
we inferred seven other links connected to AS11537 as origins. Since each link
matched the peers described in the ticket, we conﬁrmed that the origin was ac-
curately inferred by our scheme. Note that the link with APAN-JP was detected
using the single preﬁx advertised by AS7660 that preferred the path via AS11537.
This demonstrates the capability of our scheme to infer origins for small failures
and for simultaneous link failures. Note also that the recovery time is much longer
than that reported in the ticket. Since recovery times in tickets are sometimes
based on the local downtime between the two routers, we can determine the ac-
tual unreachable duration from the monitoring point.
Using 50 tickets issued over several months in 2009 and 2010, we conﬁrmed
that origins can be accurately identiﬁed for 86% of the tickets. For 6% of the
tickets, origins were misinferred due to lack of preﬁxes (discussed in Section 6).
Additionally, for 8% of the tickets that described a scheduled maintenance, no
routing updates were observed and thus not detected. Note that tickets that do
not generate any routing updates due to a failure event of one of the multiple
links between a pair of ASes were excluded from evaluation. Unfortunately, these
events cannot be detected from passive measurements.
5.2 Comparing the Number of Inferred Origins
Evaluating the eﬀectiveness of our scheme against existing schemes is diﬃcult
since our scheme targets the detection of failures of all sizes. Instead, we compare
the number of links that are inferred as origins with and without our scheme
(i.e. the original Link-Rank). Table 4 shows the number of origins inferred over
the month of September 2009 using AS22388 and AS3356. When our scheme
is disabled at AS22388, all 2,533 links observed from updates are targeted for
measurement and the number of preﬁxes carried by these links drops to 0 for
22,784 times. In contrast, when our scheme is enabled, we consider 17% of the
links as temporary links and measure routing changes that occur over 83% of
the links. The number of candidate links is 60% less and the number of origins
inferred is 72% less than those detected without our scheme. This implies the
importance of analyzing the routing status of each preﬁx during path exploration
and link recovery. The results showed a similar trend for AS3356.
Inferring the Origin of Routing Changes Based on Preferred Path Changes
171
s
n
i
g
i
r
o
d
e
r
r
e
f
n
i
f
o
y
c
n
e
u
q
e
r
F
1000
100
10
1
1
AS22388
AS3356
10
100
1000
10000
100000
Rank
Fig. 4. Number of times a link is inferred as an origin at AS22388 and AS3356
6 Discussion
A routing change can occur not only from link failures, but also from intra-AS
failures and from changes in routing policies. Although ideally we should be
able to distinguish between them, we believe that inferring the adjacent link
can still help operators diagnose the reachability issues. On the other hand, our
scheme may misinfer origins when no preﬁx is extracted at transit links. In our
evaluation, we identiﬁed this percentage to be 6.5% and 2.9% for AS22388 and
AS3356, respectively. For these links, we must use preﬁxes advertised by other
ASes for measuring stability, which is work to be addressed in the future.
Nonetheless, we believe our scheme can suﬃciently identify origins for most
events. Figure 4 shows the frequency of links inferred as origins for AS22388 and
AS3356. As the ﬁgure shows, a small number of links is repeatedly inferred. This
indicates the possibility of these links or the adjacent nodes being very unstable.
The ﬁgure shows a Zipf-like distribution, where 86% to 95% of all failures are
located in 20% of all links. This result matches many of the previous studies on
stability of preﬁxes which found that a large number of routing updates were
from routing changes of a very small number of preﬁxes [16]. On further analysis,
we discovered that 87% to 89% of these links are edge links. While this indicates
that the core transit links are much more stable than the edge links, some of the
links were transit links for hundreds of preﬁxes.
7 Conclusion and Future Work
In this paper, we presented a methodology for suﬃciently inferring the origin
of routing changes observed in the Internet. We studied the negative eﬀect of
path exploration on origin inference of small failure events. We then presented a
measurement scheme that focuses on the stability of some preﬁxes and preferred
paths to identify origins. This has allowed us to infer the origin of small failure
events and for the ﬁrst time to further infer the time of failure and the time of
recovery. Evaluation using BGP data showed that the number of origins inferred
using our scheme is 72% less than those detected without our scheme.
172
M. Watari, A. Tachibana, and S. Ano
Our future work includes ﬁnding the most relevant duration for measuring
the preferred path of preﬁxes. We also plan to evaluate our scheme using BGP
data from multiple monitoring points.
Acknowledgment
The authors would like to thank Route Views for providing the BGP data, and
the NOC teams for providing operational tickets publically available. The au-
thors are also thankful to the anonymous reviewers for their insightful comments.
References
1. Teixeira, R., Rexford, J.: A Measurement Framework for Pin-Pointing Routing
Changes. In: Proceedings of ACM SIGCOMM Workshop on Network Troubleshoot-
ing (August 2004)
2. Xu, K., Chandrashekar, J., Zhang, Z.L.: A First Step Toward Understanding Inter-
domain Routing Dynamics. In: ACM SIGCOMM MineNet Workshop (August 2005)
3. Rekhter, Y., Li, T., Hares, S.: A Border Gateway Protocol 4 (BGP-4), RFC 4271
(January 2006)
4. Labovitz, C., Malan, G., Jahanian, F.: Internet Routing Instability. In: Proceedings
of ACM SIGCOMM (September 1997)
5. Chang, D., Govindan, R., Heidemann, J.: The Temporal and Topological Charac-
teristics of BGP Path Changes. In: Proceedings of IEEE ICNP (November 2003)
6. FeldMann, A., Maennel, O., Mao, Z.M., Berger, A., Maggs, B.: Locating Internet
Routing Instabilities. In: Proceedings of ACM SIGCOMM (September 2004)
7. Mao, Z., Bush, R., Griﬃn, T., Roughan, M.: BGP Beacons, In. In: Proceed-
ings of the 3rd ACM SIGCOMM Conference on Internet Measurement, pp. 1–14
(October 2003)
8. Oliveira, R., Zhang, B., Pei, D., Zhang, L.: Quantifying Path Exploration in the
Internet. IEEE/ACM Transactions on Networking 17(2), 445–458 (2009)
9. Lad, M., Oliveira, R., Massey, D., Zhang, L.: Inferring the Origin of Routing
Changes using Link Weights. In: Proceedings of IEEE ICNP (October 2007)
10. Campisano, A., Cittadini, L., Di Battista, G., Reﬁce, T., Sasso, C.: Tracking Back
the Root Cause of a Path Change in Interdomain Routing. In: Proceedings of
IEEE/IFIP NOMS (April 2008)
11. Lad, M., Massey, D., Zhang, L.: Visualizing Internet Routing Changes. IEEE Trans-
actions on Visualization and Computer Graphics, 1450–1460 (November/December
2006)
12. Route Views Project (March 2010), http://www.routeviews.org/
13. TRANSPAC2 NOC (March 2010), http://noc.transpac.org/
14. APAN-JP NOC (March 2010), http://www.jp.apan.net/NOC/
15. Internet2 NOC (March 2010), http://noc.net.internet2.edu/
16. Oliveira, R., Izhak-Ratzin, R., Zhang, B., Zhang, L.: Measurement of Highly Active
Preﬁxes in BGP. In: Proceedings of IEEE GLOBECOM (December 2005)