User: In Celery, the only way to conclusively reload modules is to restart all of your celery processes.  I know how to remotely shutdown workers (broadcast('shutdown', destination = [<workers>])), but not how to bring them back up.  
I have a piece of Python code that works to create a daemon process with a new worker in it, but when I try to run it inside Celery as a celery task, I get AssertionError: daemonic processes are not allowed to have children, which I'm guessing is related to the way that the worker pool is set up.  

Is there a way to override this error in Celery somehow?  
If not, is there another way to get Celery to start up another worker to replace itself with?  Maybe wrap this whole thing in a bash script (though the point of doing this in Python was to avoid using Python to call bash to call Python).  
If not, is there another way to convince Celery to reload the newest version of the code?  the --autoreload flag and broadcast('pool_restart') both do nothing.  

Example Behavior:

Create:
@task
def add(x,y):
    return x+y
Load up celery, run add.delay(4,4), get back 8.  
Change add to:
@task
def add(x,y):
    return x*y
Do magic (currently "Restart Celery")
Run add.delay(4,4) again

You should get back 16.  And I always get back 8, unless I shut down celery and reload it, which I can't do remotely without having a way to bring up worker processes from a remote machine, preferably using a script.
Assistant: Run the Celery daemons under a supervisor, such as supervisord.  When the celeryd process dies, the supervisor will spin it back up.