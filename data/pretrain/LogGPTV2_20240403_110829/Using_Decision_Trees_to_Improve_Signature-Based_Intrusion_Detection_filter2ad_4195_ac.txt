25
26:79
80
81:65535
{ 2 }
{ 1, 2 }
Destination Address
{ 2 }
{ 2, 3 }
{ 2 }
{ 2, 4 }
Source Address
{ 2 }
192.168.0.2
192.168.0.3
192.168.0.1
192.168.0.4
{ 1 }
{ 2 }
{ 2 }
{ 4 }
Fig. 3. Decision Tree with any Rule.
have used this technique to manage the large number of Snort rules (see Section
6 for details).
The number of checks that each input element requires while traversing the
decision trees is bound by the number of features, which is independent of the
number of rules. However, our system is not capable of checking input data with
a constant overhead independent of the rule set size. The additional overhead,
which depends on the number of rules, is now associated with the checks at
every node. In contrast to a system that checks all rules in a linear fashion, the
comparison of the value extracted from the input element with a rule speciﬁca-
tion is no longer a simple operation. In our approach, it is necessary to select
the appropriate child node by choosing the arrow which matches the input data
value. As the number of rules increases and the number of successor nodes grows,
this check becomes more expensive. Nevertheless, the comparison can be made
more eﬃcient than an operation with a cost linear (i.e., O(n)) in the number of
elements n.
5 Feature Comparison
This section discusses mechanisms to eﬃciently handle the processing of an input
element at nodes of the decision tree. As mentioned above, each feature has
a type and an associated value domain. When building the decision tree or
evaluating input elements, features with diﬀerent names but otherwise similar
types and value domains can be treated identically. It is actually possible to
reuse functionality for a certain type even when the value domains are diﬀerent
(e.g., 16 or 32 bit variations of the type integer). For our prototype, we have
implemented functionality for the types integer, IPv4 address, bitfield and
string. Bitfield is utilized to check for patterns of bits in a ﬁxed length bit
array and is needed to handle the ﬂag ﬁelds of various network protocol headers.
The basic operation that has to be supported in order to be able to traverse
the decision tree is to ﬁnd the correct successor node when getting an actual
184
C. Kruegel and T. Toth
value from the input item. This is usually a search procedure among all possible
successor values created by the intersection of the values speciﬁed by each rule.
Using binary search, it is easy to implement this search with an overhead of
O(log n) for integer, where n is the number of rules. For the IPv4 address and
bitfield types, the diﬀerent successor values are stored in a tree with a depth
that is bound by the length of the addresses or the bitﬁelds, respectively. This
yields a O(1) overhead.
The situation is slightly more complicated for the string type, especially
when a data item can potentially contain a nearly arbitrary long string value.
When attempting to determine the intersections of the string property speciﬁ-
cations of a rule set during the partition process, it is necessary to assume that
the input can contain any of all possible combinations of the speciﬁed string
values. This yields a total of 2n diﬀerent intersections or subsets where n is
the number of rules under consideration. This is clearly undesirable. We tackle
this problem by requiring that the string type may only be used as the last
attribute for splitting when creating the decision tree. In this setup, the nodes
that partition a rule set according to a string attribute actually become leaf
nodes. It is then possible to determine all matching rules (i.e., all rules which
deﬁne a string value that is actually contained in the input element) during the
detection process without having to enumerate all possible combinations and
keep their corresponding nodes in memory.
Systems such as Snort, which compare input elements with a single rule at
a time, often use the Boyer-Moore [9] or similar optimized pattern matching
algorithms to search for string values in their input data. These functions are
suitable to ﬁnd a single keyword in an input text. But often, the same input
string has to be scanned repeatedly because multiple rules all deﬁne diﬀerent
keywords.
As pointed out in [3], Snort’s rule set contains clusters of nearly identical
signatures that only diﬀer by slightly diﬀerent keywords with a common, iden-
tical preﬁx. As a result, the matching process generates a number of redundant
comparisons that emerge where the Boyer-Moore algorithm is applied multiple
times on the same input string trying to ﬁnd nearly similar keywords. The au-
thors propose to use a variation of the Aho-Corasick [1] tree to match several
strings with a common preﬁx in parallel and reduce overhead. Unfortunately, the
approach is only suitable when keywords share a common preﬁx. When creating
the decision tree following our approach, it often occurs that several signatures
that specify diﬀerent strings end up in the same node. They do not necessarily
have anything in common. Instead of invoking the Boyer-Moore algorithm for
each string individually, we use an eﬃcient, parallel string matching implementa-
tion introduced by Fisk and Varghese [5]. This algorithm has the advantage that
it does not require common preﬁxes and delivers good performance for medium
sized string sets (containing a few up to a few dozens elements).
In the Fisk-Varghese approach, hash tables are utilized to reduce the number
of strings that need to be evaluated on an expensive character-by-character basis
when a partial match between the rule strings and the input string is detected.
Using Decision Trees to Improve Signature-Based Intrusion Detection
185
However, when a few hundred strings are compared in parallel, some hash table
buckets can contain so many elements that the eﬃciency is negatively eﬀected.
This is solved by selectively replacing hash tables by tries when a hash table
bucket contains a number of elements above a certain, deﬁnable threshold (the
default value is 8).
A trie is a hierarchical, tree like data structure that operates like a dictionary.
The elements stored in the trie are the individual characters of ‘words’ (which
are, in our case, the string features of the individual rules). Each character of
a word is stored at a diﬀerent level in the trie. The ﬁrst character of a word
is stored at the root node (ﬁrst level) of the trie, together with a pointer to a
second-level trie node that stores the continuation of all the words starting with
this ﬁrst character. This mechanism is recursively applied to all trie levels. The
number of characters of a word is equal to the levels needed to store it in a trie.
A pointer to a leave node that might hold additional information marks the end
of a word.
When a partial match is found by the detection process, the trie is utilized
to perform the expensive character-by-character search for all string candidates
in parallel. It is no longer necessary to sequentially match all words of a hash
bucket against the input string (as with the Fisk-Varghese approach).
Although tries would be beneﬁcial in all cases, we limit their use to the re-
placement of large hash tables only because of the signiﬁcant increase in memory
usage.
6 Experimental Data
This section presents the experimental data that we have obtained by utilizing
decision trees to replace the detection engine of Snort. We have implemented
patches named Snort NG (next-generation) for Snort-1.8.6 and Snort-1.8.7
that can be downloaded from [15]. The reader is referred to Appendix A for
details about the integration of our patch into Snort and interesting ﬁndings
about the current rule set. Our performance results are directly compared to
the results obtained with the latest version of Snort and its improved detection
engine, that is Snort-2.0rc1.
For our ﬁrst experiment, we set up Snort-2.0 and our patched Snort NG
with decision trees on a Pentium IV with 1.8 GHz running a RedHat Linux
2.4.18 kernel. Both programs read tcpdump log ﬁles from disk and attempted
to process the data as fast as possible. When performing the measurements, most
preprocessors have been disabled (except for HTTP-decoding and TCP stream re-
assembling) and only fast-logging was turned on to have our results reﬂect mostly
the processing cost of the detection algorithms themselves. Obviously, the over-
head of the operating system to read from the ﬁle and the parsing functionality
of Snort still inﬂuences the numbers, but it does so for both approaches.
We measured the total time that both programs needed to complete the
analysis of our test data sets. For each of these data sets, we performed ten runs
and averaged the results. For the experiment, the maximum number of 1581
186
C. Kruegel and T. Toth
i
)
s
n
i
(
e
m
T
g
n
s
s
e
c
o
r
P
i
90
80
70
60
50
40
30
20
10
1
Snort NG (decision trees)
Snort 2
2
3
4
5
6
7
8
9
10
Test Data for 2 Weeks (10 Days)
Fig. 4. Time Measurements for 1999 MIT Lincoln Lab Traﬃc.
Snort-2.0 rules that were available at the time of testing have been utilized.
As Snort NG bases on Snort-1.8.7 that uses a rule language incompatible to
Snort-2.0, all rules have been translated into a suitable format. Both programs
were executed consecutively and did not inﬂuence each other while running.
We used the ‘outside’ tcpdump ﬁles of the ten days of test data produced
by MIT Lincoln Labs for their 1999 DARPA intrusion detection evaluation [8].
These ﬁles have diﬀerent sizes that range from 216 MB to 838 MB. The compar-
ison of the results for the ten days of the MIT/LL traﬃc is shown in Figure 4.
For each test set, both systems reported the same alerts. Although the actual
performance gain varies considerably depending on the type of analyzed traﬃc
(as Snort-2.0 is tuned to process HTTP traﬃc), the decision trees performed bet-
ter for every test case and yielded an average speed up of 40.3%. The maximum
speed up seen during the tests was 103%, and the minimum was 5%.
The second experiment used the same setup as the ﬁrst one. This time, how-
ever, the number of rules were increased (starting from 150 up to the maximum
of 1581) while the input ﬁle was left the same (we used the ﬁrst day of the 1999
MIT Lincoln Labs data set). The rules were added in the order implied by the
default rule set of Snort-2.0. All default rule ﬁles were sorted in alphabetical
order and their rules were then concatenated. From this resulting list, rules were
added in order. Similar to the previous test, both programs reported the same
alerts for all test runs. Figure 5 depicts the time it took both programs to com-
plete the analysis of the test ﬁle given the increasing number of input rules. The
graph shows that the decision tree approach performs better, especially for large
rule sets.
Building the decision tree requires some time during start up and increases
the memory usage of the program. Depending on the number of rules and the fea-
tures which are deﬁned, the tree can contain several tens of thousands of nodes.
A few Snort conﬁguration options, such as being able to specify lists of source
Using Decision Trees to Improve Signature-Based Intrusion Detection
187
)
s
(
e
m
T
i
16
15
14
13
12
11
10
9
8
7
6
5
Snort NG (decision trees)
Snort 2