(cid:105)
(cid:104)
(cid:105)
Pr
l((cid:126)Y ) > τ
= Pr
(cid:126)Y = 01 ∨ (cid:126)Y = 10 ∨ (cid:126)Y = 11
This probability under H0 equals PF = PF 1 + PF 2 −
PF 1PF 2, and under H1 equals PD = PD1+PD2−PD1PD2.
This operating point can be interpreted as the OR rule for
making an ensemble, since the LRT ensemble outputs an
alarm when either IDS1 or IDS2 (or both) outputs an alarm.
An example of this operating region can be seen in Figure
7(b).
• For min{l(01), l(10)} ≤ τ ≤ max{l(01), l(10)}, the
performance of the ensemble depends on whether l(01)  τ
= Pr
(cid:126)Y = 11
2The relation l(10) = l(01) implies that (PF 1, PD1) and
(PF 2, PD2) are in the same performance line of the point for τ ∈
(l(00), min{l(01), l(10)}) and τ ∈ (max{l(01), l(10)} , l(11)),
and therefore this case can be ignored.
142
l(00)l(01)l(10)l(11)l(000)l(001)l(010)l(100)l(011)l(101)l(110)l(111)(a) An example for 0 ≤ τ ≤ l(00)
(b) An example for l(00) ≤ τ ≤
min{l(01), l(10)}
(c) An example for min{l(01), l(10)} ≤
τ ≤ max{l(01), l(10)}
(d) An example for max{l(01), l(10)} ≤
τ ≤ l(11)
(e) An example for l(11) ≤ τ ≤ ∞
(f) The ROC of the LRT ensemble outper-
forms the ROC of each individual IDS
Figure 7: Examples for different τ
this reduction, note that an ensemble rule is any function that takes
inputs from (cid:126)Y ∈ {0, 1}n and outputs y0 ∈ {0, 1}. Therefore,
the size of the set of all possible ensemble rules is 22n. Given a
particular ordering of the likelihood ratio, the LRT rule excludes
22n − 2n suboptimal decision rules. We clarify this fact in the
following section.
5.2 Practical Interpretation of the LRT
Note that in practice we do not need to perform the detailed
formal analysis of the previous subsection, or compute the partial
order. In practice, we only need to compute the likelihood ratios
l((cid:126)Y ) for every possible output (cid:126)Y and then sort them.
In order to exemplify this approach, consider again the IDSs in
Table 2. In particular, let y1 denote the output of Snort, y2 denote
the output of PAYL, and y3 denote the output of NetAD. We can
now compute the likelihood ratio for every possible combination
of the outputs; for example l(101) = 182.63. After computing all
likelihood ratios, we obtain the following order:
l(000)  C01).
In other words, it is more important to detect all attacks
than to obtain fewer false alarms (recall the military network
example). We set C01 = 1, C10 = 10 in this case.
• F P is as important as F N, or simply, the cost of F N is
equal to that of F P (i.e., C01 = C10 = 1).
• The cost of F N is less than that of F P (i.e., C10 < C01). In
other words, it is more important to reduce false alarms rather
than to catch all attacks (recall the example with a single
overloaded operator). In this case we set C01 = 10, C10 =
1.
Figure 9: Robustness: simulation result. Estimation errors
are allowed. We vary this estimation error tolerance bound.
Robustness stands for the probability of having the exact
same fusion results on the entire alert sets as using the exact
parameters.
The results are shown in Figure 9. We use a robustness metric,
which is deﬁned as the probability of having the exact same
fusion result on the entire alert data as the situation using the
actual F P/F N/P1. The probabilities (parameters) p are chosen
uniformly and randomly from [p ∗ (1 − σ), p ∗ (1 + σ)] and σ is
the estimation error tolerance bound for both F P and F N. For
instance, a 100% estimation error (σ = 100%) means F N3 can
range from 0 to 0.46, F P1 can range from 0 to 0.104. Even for
the estimation error tolerance bound of 100%, LRT algorithm can
achieve the same result on the entire alert data as using the actual
parameters with a probability higher than 50% in all the three given
cost scenarios. For a reasonably small estimation error tolerance
bound (e.g., 20%), LRT algorithm is very robust compared with
using the exact parameters.
The simulation results show that
the LRT algorithm is a
robust fusion technique to tolerate parameter estimation error to
a reasonable certain degree.
6.3 The Conditional Independence Assump-
tion
We can relax the conditional
independence assumption of
Eq. (4) by introducing a more complex approximation to the joint
distributions.
A way to introduce more complexity in our approximation
is to model the dependence relations between IDSs. We can
model the relations using a dependency tree, or more generally,
a causal Bayesian network or a graphical model [37, 25, 24].
If the inference relationship can be modeled as a dependance
tree, we can perform MIMIC (Mutual-Information-Maximizing
Input Clustering) technique [7] to compute the approximate joint
distribution. For the construction of causal Bayesian networks,
there is work [25, 24] in the machine learning area that can
be directly applied to obtain an accurate inference network
(dependance relationship), so that we could calculate a more
accurate approximation of the joint conditional density.
All these techniques will, however, require substantial efforts by
any IDS operator implementing a fusion rule. These additional
efforts might deter the IDS operators from implementing more
complex approximations to model the joint distribution.
As it stands, the conditional independence assumption is the
most practical way to approximate the joint conditional density
since any IDS operator can use this approximation by only