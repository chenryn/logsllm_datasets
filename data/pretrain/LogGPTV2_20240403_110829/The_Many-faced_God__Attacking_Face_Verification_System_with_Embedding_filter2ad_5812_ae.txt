1792
Le
Table 2: The second row show the acceptance rate of the im-
ages recovered by ImgRev. The third shows the FID of the
generated images (smaller is better). The settings are the
same as Figure 10.
Quantitative results. To quantify the attack effectiveness, we
check the ratio of recovered images being accepted by FVS. Ta-
ble 2 shows the acceptance rate, by comparing the original and
recovered faces using the threshold defined in Table 1. As we can
see, the quantitative results follow the general trend of qualitative
results: Clarifai-1024 has the best acceptance rate, at 98.63%. Wide-
Res-1792 has only 93.87% acceptance rate because the embedding
is implemented by us, which has lower embedding accuracy. But
even for the worst result on Facenet-128, ImgRev still achieves
over 93% success rate.
In addition to acceptance ratio, we also compute FID (Frechet
Inception Distance) [71] of each recovered image and report the
average among them. FID records the distance between feature
vectors calculated for real and generated images by GAN. Inter-
estingly, though the acceptance rate is similar across embedding
models, the difference is prominent under FID. Best performance is
still achieved under Clarifai-1024. As FID of GAN generated images
usually falls in the range from 30 to 200 [63], the image quality is
acceptable.
One might argue that FVS operator can adjust the threshold to
thwart our attack. To evaluate the effectiveness of this potential
defense, we compute the embedding distances between images of
1) same person; 2) different persons and 3) original and recovered
versions. Figure 11 shows the Probability Density Function (PDF)
of the distances. It turns out for a victim photo, its distance to the
photo recovered by ImgRev and other photos of the same person
have similar distribution (“Recovered” curve and “Same” curve).
Meanwhile, its distance to photos of other people (“Diff” curve)
has very different distribution. Therefore, if this defense is applied
to reject the photos provided by the adversary, false rejections
will be significantly increased, making FVS unusable. Specifically,
we evaluate the impact of FVS threshold on false-rejection and
acceptance rate and show the result in Table 3. When the threshold
is reduced to 0.4, where 35.84% of victim’s verification requests are
rejected, the attacker still has 48.96% success rate.
TH
FR Rate
Acc.
0.7
4.49%
18.75% 35.84% 61.72%
96.35% 90.63% 72.40% 48.96% 20.83%
0.6
8.79%
0.3
0.5
0.4
Table 3: False-rejection rate and acceptance rate under dif-
ferent FVS thresholds.
Performance gain under whitebox Setting. When the adver-
sary knows the structure of the targeted embedding model, she
can reliably compute ∇Le and derive the embedding loss Le, which
should improve the quality of the recovered image. Here, we assess
this expected performance gain. As listed in Table 2, the white-box
setting brings to the attacker 1.2% gain of acceptance rate (94.20%
compared to 93.07%) and 28.11 gain of image quality. Though such
result shows white-box adversary has advantage over black-box
adversary, the gain is small. Therefore, for our attack to succeed,
white-box access is not required.
Performance gain with surrogate model. We evaluate if a black-
box adversary can improve the baseline ImgRev by using an open-
source surrogate model f ′, which differs from f , to generate Le.
26The Many-faced God: Attacking Face Verification System with Embedding and Image Recovery
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 11: PDF of distances between 1) embeddings of dif-
ferent photos under the same person (“Same”), 2) photos of
different people (“Diff”) and 3) recovered and original im-
ages (“Recovered”). The vertical line is the threshold for the
Facenet-512 model (judge model).
Surprisingly, the result shows that f ′ brings to the attacker 3.2%
acceptance rate gain and 52.86 gain of image quality, which are
even better than the white-box setting. This somehow contradicts
to our expectation, as white-box access should offer better insight
into the targeted FVS.
After investigating the root cause, we found that such improve-
ment might be caused by the higher verification accuracy of Facenet-
512 compared to Facenet-128. Le generated by better model results
in better recovery quality. In addition, the diversity brought by the
surrogate model could help. With a different model supervising the
image generation, it is more likely that the generator can generate
images with fewer defects, because an embedding model may ne-
glect certain features of an image which however are captured by
another embedding model.
Image recovery with imprecise embedding. Our prior experi-
ments assume the embedding recovered by the adversary is “per-
fect”. Here we consider the embedding has error and evaluate Im-
gRev again.
In particular, we use EmbRev to recover the embeddings asso-
ciated with DStest with different query numbers and then reverse
those generated embeddings with ImgRev. The result confirms that
ImgRev works well when the errors are small. When 60 queries
are issued, the recovered images do not show obvious difference
with the images recovered with 128 queries. Figure 12 shows the
samples under different query numbers. The embeddings derived
under photo distortion and score truncation lead to similar result
(i.e., 60 queries are sufficient) as the error margins introduced to
embedding in these cases are even less (e.g., 10−3 for query photo
distortion).
6 DISCUSSION
While our research shows FVS can be bypassed and enrollees’ pri-
vacy can be breached, limitations exist and are described below. In
addition, we discuss the potential defense.
Limitations. 1) Under no-box setting, the embedding recovered
by EmbRev is noisier comparing to other two settings. While the
image recovered from the embedding is still able to bypass FVS, we
found the image is dissimilar to victim’s photo, hence we did not
Figure 12: Images Recovered with different number of
queries.
show it in the paper. But we want to point out that getting white-
box or black-box access is feasible in most cases as FVS usually uses
well-known embedding models. 2) We only evaluate the white-box
attack scenario against Facenet-128, because the black box scenario
already performs well and the improvement of Le is marginal. 3) The
texture of images generated by ImgRev can be further improved.
Images in Figure 10 show that coarse-gained features of victims’
faces can be well recovered, like the outline, the position of eyes
and nose, etc.. However, finer-grained features like skin textures are
not well depicted, mainly because such information is not stored in
an embedding. 3) We did not test our approach on the real-world
FVS, like self-service FVS, due to ethical concerns. 4) We used a
relatively small dataset to train and test the face embeddings and
our attack. The result could differ when large dataset (e.g., hundreds
of millions of images are included). We acknowledge this limitation
and plan to expand our evaluation with better hardware platform
and more data. 5) We consider a “weaker authentication” scenario
when liveness detection is not used.
Defense. Hiding the score (e.g., only showing “pass/fail”) is likely to
solve this problem but it will make the on-site debugging much more
difficult as described in Section 2.2. In fact, score is also encouraged
to be shared on social media and many users are doing that [67].
Even when only “pass/fail” is shown, FVS is not bullet-proof as the
adversary can issue more queries till discovering an embedding
similar enough to victim’s. Another approach is to add noises to
the values visible to the attacker (e.g., confidence score vector [31]),
but false positives would rise against legitimate users.
ML library and SDK documentation should clearly tell developers
that distances can only be exposed to authorized managers and
can never be displayed to normal users. Developers should also
learn case studies about embedding leakages so they will not leak
distances inadvertently.
To thwart the image recovery attack after embedding is inferred,
the embedding model can be redesigned to add privacy protec-
tion. Just like a one-way hash function, ML developers may design
models in a way that the reverse mapping of a model cannot be
easily figured out by attackers. Hash functions employ computation
00.511.5Distance0123Prob. DensityRecoveredDiffSame27ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Mingtian Tan, Zhe Zhou and Zhou Li
subroutines that are hard to reverse. However, because basic units
used by DNN today like pooling, convolution, activation, are all
partially or totally reversible, making the embedding model irre-
versible would be unlikely to succeed. Therefore, new DNN units
should be developed to fulfill this goal. In the meantime, auditing
the queries and blocking the follow-up ones when the distribution
is abnormal can be used to deter the model inversion attack [32]
and we will investigate whether it can provide strong protection
on embedding models.
7 RELATED WORKS
We review those relevant works in machine-learning field first.
Then, we overview other works about face authentication.
Data confidentiality. Fredrikson et al. proposed model inversion
attack (MIA) [19] and showed that the model used for medical
treatment leaks patient’s genetic markers. Following this work,
Fredrikson et al. showed confidence values exposed by the predic-
tion API of MLaaS can be exploited to reconstruct part of training
data[18]. Specifically for their face recognition experiment, they
recovered images of victims in the training set. Recently, Yang et
al. improved the accuracy of image reconstruction of MIA using
public auxiliary dataset [67].
To be noticed is that our work assumes a different scenario
for image reconstruction, i.e., face authentication. In the previous
works, a vector of logits (e.g., confidence value or prediction scores)
can be obtained by the adversary for each prediction. However,
only one score is returned to our adversary, which does not reveal
much information about the model’s characteristics like gradient
or special-featured gradient resulted from over-fitting. Yet, by ex-
ploiting the unique properties of face embedding, we found victims
faces can be recovered.
A related task as ours is image generation, where encoder-decoder
network [6, 41] has been used. As far as we know, the work done
by Zhmoginov et al. [73] is the only one reconstructing image from
embedding. However, as their goal is to transfer an image to another
one such that it has close distance to an embedding (small distance
in embedding plane), the generated image is dissimilar to victim’s
image (large distance in image plane).
In addition to MIA, previous works showed certain properties
of the training data can be revealed. Reza et al. proposed member-
ship inference attack [54]. Later, the same attack is demonstrated
successful in other settings [24, 39, 42, 49].
Model confidentiality. By exploiting the prediction API of
machine-learning models, researchers found the model structure
(e.g., hyper-parameters and weights) [9, 13, 32, 46, 58, 60, 69] and op-
timization procedure can be revealed [45]. In addition to exploiting
the algorithm weakness of machine-learning models, researchers
found the hardware executing them also leaks model structure
through side channels. In particular, the performance counters pro-
vided by GPU [43], shared CPU cache [26, 65], electromagnetic
signals [7], memory access patterns [27, 28], power [61] and execu-
tion time [15] can be exploited to this end. Previous works studied
model confidentiality and data confidentiality in separate directions,
but they might be able to augment each other (e.g., knowing model
structure could increase the accuracy of the data inference attacks).
We will investigate how our attack can be facilitated with the help
of inference attacks on model structure.
Security of face authentication. The major concern is that face
verification can be fooled by replaying an image forged from vic-
tim’s public photos. As such, most recent works involved liveness
detection as the countermeasure [36, 57, 59] but researchers also
discovered new attacks against it [64].
Recently, researchers showed that through generating adversar-
ial physical example (e.g., eyeglass frames), face authentication can
be fooled [53, 74]. While our attack can be categorized as adversarial
learning, the adversary model is very different. Their attack assume
victim’s facial image has been possessed by the adversary so the
adversarial example can be built upon it through perturbation, but
our attack assumes zero knowledge about the victim’s appearance.
A recent work proposed to use distance to assist GAN to generate
adversarial examples [66], but they did not recover the enrollee’s
embeddings and images like ours.
8 CONCLUSION
Our study reveals that the small information leakage from face
verification system (FVS), i.e., the score displayed after each verifi-
cation request, can be accumulated to recover victim enrollee’s real
face. By acquiring only a dozen of scores, she can readily recover
the embedding of the victim’s face, with our proposed embedding-
recovery equations. What’s worse is that the embedding is equally
sensitive as the victim’s face. As a proof, we designed a recovery
model based on GAN to convert the recovered embeddings back to
face images, the results show both embedding and face recovery
are effective, as the FVS can be bypassed at high probability and
the recovered face is similar to the victim.
ACKNOWLEDGMENTS
The Fudan authors are supported by NSFC 61802068. The UCI
author is partially supported by NSF DGE-2039634, and gift from
Microsoft and Cisco.
REFERENCES
[1] 2020. Wide Resnet Git. https://github.com/szagoruyko/wide-residual-networks.
https://github.com/szagoruyko/wide-residual-networks Accessed: 2020-01-10.
[2] AMGTime. [n.d.]. Face Recognition, Fingerprint, Proximity Cards 4 in 1 Biometric
Time Attendance Package. https://amgtime.com/hardware-facial-recognition-
technology-rfid-time-attendance. Accessed: 2019-12-20.
[3] Brandon Amos, Bartosz Ludwiczuk, and Mahadev Satyanarayanan. 2016. Open-
Face: A general-purpose face recognition library with mobile applications. Technical
Report. CMU-CS-16-118, CMU School of Computer Science.
[4] Apple. [n.d.]. About Face ID advanced technology. https://support.apple.com/en-
us/HT208108. Accessed: 2019-12-20.
arXiv preprint arXiv:1701.07875 (2017).
[5] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein gan.
[6] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. 2017. Segnet: A deep
convolutional encoder-decoder architecture for image segmentation. IEEE trans-
actions on pattern analysis and machine intelligence 39, 12 (2017), 2481–2495.