### Table 2: Acceptance Rate and FID of Recovered Images
- **Row 2**: Shows the acceptance rate of images recovered by ImgRev.
- **Row 3**: Displays the FID (Frechet Inception Distance) of the generated images, with smaller values indicating better quality.
- **Settings**: Consistent with those in Figure 10.

### Quantitative Results
To evaluate the attack's effectiveness, we measured the ratio of recovered images accepted by the Face Verification System (FVS). Table 2 compares the original and recovered faces using the threshold defined in Table 1. The results align with the qualitative findings:
- **Clarifai-1024** has the highest acceptance rate at 98.63%.
- **Wide-Res-1792** has a lower acceptance rate of 93.87% due to the lower embedding accuracy of our implementation.
- Even for the worst-performing model, **Facenet-128**, ImgRev achieves an acceptance rate over 93%.

In addition to the acceptance rate, we calculated the FID for each recovered image and reported the average. FID measures the distance between feature vectors of real and generated images. Despite similar acceptance rates across models, the FID values show significant differences:
- **Clarifai-1024** still performs best.
- The FID of GAN-generated images typically ranges from 30 to 200, indicating that the image quality is acceptable.

### Defense Against Threshold Adjustment
One potential defense is adjusting the FVS threshold. To evaluate this, we computed the embedding distances between:
1. Images of the same person.
2. Images of different persons.
3. Original and recovered versions.

Figure 11 shows the Probability Density Function (PDF) of these distances. The "Recovered" curve and "Same" curve have similar distributions, while the "Diff" curve is distinctly different. Thus, if the threshold is lowered to reject adversarial photos, it would also increase false rejections, making the FVS unusable. Table 3 illustrates the impact of different FVS thresholds on false-rejection and acceptance rates. For example, at a threshold of 0.4, 35.84% of legitimate verification requests are rejected, but the attacker still has a 48.96% success rate.

### Performance Gain in Whitebox Setting
When the adversary knows the targeted embedding model's structure, they can compute the gradient \(\nabla L_e\) and derive the embedding loss \(L_e\), potentially improving the recovered image quality. Our assessment shows:
- A 1.2% gain in acceptance rate (94.20% vs. 93.07%).
- A 28.11 improvement in image quality.

While white-box access provides some advantage, the gains are small, indicating that it is not essential for the attack to succeed.

### Performance Gain with Surrogate Model
We evaluated whether a black-box adversary could improve ImgRev using an open-source surrogate model \(f'\) that differs from the target model \(f\). Surprisingly, the surrogate model provided:
- A 3.2% gain in acceptance rate.
- A 52.86 improvement in image quality.

This outperformed the white-box setting, possibly due to the higher verification accuracy of Facenet-512 compared to Facenet-128. The diversity introduced by the surrogate model may also contribute to better recovery quality.

### Image Recovery with Imprecise Embedding
Our initial experiments assumed perfect embedding recovery. We then considered scenarios with embedding errors and re-evaluated ImgRev. Using EmbRev to recover embeddings with different query numbers, we found that ImgRev works well even with small errors. For instance, 60 queries produce images comparable to those from 128 queries. Figure 12 shows samples under different query numbers.

### Discussion
#### Limitations
1. **No-Box Setting**: Recovered embeddings are noisier, leading to dissimilar images.
2. **White-Box Attack**: Evaluated only against Facenet-128 due to marginal improvements.
3. **Image Texture**: Coarse-grained features are well-recovered, but finer details like skin textures are not.
4. **Real-World Testing**: Not tested on actual FVS systems due to ethical concerns.
5. **Dataset Size**: Used a relatively small dataset; results may differ with larger datasets.
6. **Liveness Detection**: Considered a scenario without liveness detection.

#### Potential Defenses
- **Score Hiding**: Only showing "pass/fail" can mitigate the issue but makes debugging difficult.
- **Noise Addition**: Adding noise to visible values can deter attacks but increases false positives.
- **Documentation**: ML libraries and SDKs should clearly state that distances should only be visible to authorized managers.
- **Model Redesign**: Develop irreversible embedding models or new DNN units.
- **Query Auditing**: Monitor and block abnormal query patterns to prevent model inversion attacks.

### Related Works
#### Data Confidentiality
- **Model Inversion Attacks (MIA)**: Fredrikson et al. [19] and Yang et al. [67] demonstrated data leakage through confidence values.
- **Image Generation**: Zhmoginov et al. [73] reconstructed images from embeddings, but their goal was different.

#### Model Confidentiality
- **Prediction API Exploitation**: Researchers have revealed model structures and optimization procedures.
- **Hardware Side Channels**: Leaks through GPU performance counters, shared CPU cache, and electromagnetic signals.

#### Security of Face Authentication
- **Liveness Detection**: Countermeasure against replay attacks.
- **Adversarial Examples**: New attacks using physical examples like eyeglass frames.

### Conclusion
Our study demonstrates that small information leaks from FVS, such as verification scores, can be accumulated to recover a victim's face. With just a dozen scores, the adversary can recover the victim's face embedding and use a GAN-based model to convert it back to a face image, effectively bypassing the FVS and generating a similar face.

### Acknowledgments
The Fudan authors are supported by NSFC 61802068. The UCI author is partially supported by NSF DGE-2039634 and gifts from Microsoft and Cisco.

### References
[References listed as in the original text]