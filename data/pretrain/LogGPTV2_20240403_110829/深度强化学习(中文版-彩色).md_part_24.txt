t=0t′=t
97
第2章 强化学习入门
(2)确定性策略梯度
以上介绍的属于随机性策略梯度（StochasticPolicyGradient,SPG），它用于优化随机性策略
π(a|s)，即用一个基于当前状态的概率分布来表示动作的情况。与随机性策略相对的是确定性策
略，其中 a = π(s) 是一个确定性动作而非概率分布。我们可以用类似于 SPG 的方法得到 DPG，
且它在数值上（作为一种极限情况）遵循策略梯度定理，尽管有不同的显式表示。
注：在本小节后续部分，我们使用µ(s)代替之前定义的π(s)来表示确定性策略，从而消除
它与随机性策略π(a|s)间的歧义。
对于DGP的更严格和广泛的定义，我们参考由文献(Silveretal.,2014)提出的确定性策略梯
度定理，即式(2.151)。在此之前，我们将逐步介绍确定性策略梯度定理并证明它，先用一种在线
策略的方式而后用离线策略的方式，同时我们也将详细讨论DPG和SPG间的关系。
首先，我们定义确定性策略的表现目标，与随机性策略梯度求解过程中的期望折扣奖励采用
同样的定义：
2 3
X∞
J(µ)=E St∼ρµ,At=µ(St)4 γt−1R(S t,A t)5 (2.143)
t=1
Z Z
X∞
γt−1ρ (s)p(s′|s,t,µ)R(s′ ,µ(s′ )]dsds′
= (2.144)
0
S S
Z t=1
= ρµ(s)R(s,µ(s))ds (2.145)
S
其中p(s′|s,t,µ)=p(S |S ,A )pµ(A |S )，第一个概率是转移概率，而第二个是动作选择概率。
t+1 t t t t
由于它是确定性策略，我们有pµ(A |S )=1，因而p(s′|s,t,µ)=p(S |S ,µ(S ))。此外，上式
R P t t t+1 t t
中的状态分布是ρµ(s′):= ∞ γt−1ρ (s)p(s′|s,t,µ)ds。
PS t=1 0  R P
由于式子Vµ(s)=E ∞ γt−1R(S ,A )|S =s;µ = ∞ γt−1p(s′|s,t,µ)R(s′,µ(s′)]ds′
t=1 t t 1 S t=1
在除使用确定性策略这一点外遵循与随机性策略梯度中相同的定义，我们可以得出
Z
J(µ)= ρ (s)Vµ(s)ds (2.146)
0
S
Z Z
X∞
γt−1ρ (s)p(s′|s,t,µ)R(s′ ,µ(s′ )]dsds′
= (2.147)
0
S S
t=1
这与上面直接用折扣奖励的形式得到的表示式是等价的。这里的关系也对随机性策略梯度适
用，只是将确定性策略µ(s)替换成随机性策略π(a|s)即可。对于确定性策略，我们有Vµ(s) =
Qµ(s,µ(s))，因为状态价值对于随机性策略是关于动作分布的期望，而对于确定性策略没有动作
分布而只有单个动作值。因此我们也有对于确定性策略的如下表示：
98
2.7 策略优化
Z
J(µ)= ρ (s)Vµ(s)ds (2.148)
0
ZS
= ρ (s)Qµ(s,µ(s))ds (2.149)
0
S
关于表现目标的不同形式和几个条件将被用来证明DPG定理。我们在这里列出这些条件如
下而不给出详细的推导过程，相关内容请参考文献(Silveretal.,2014)。
• C.1连续导数的存在性：p(s′|s,a),∇ p(s′|s,a),µ (s),∇ µ (s),R(s,a),∇ R(s,a),ρ (s)对
a θ θ θ a 0
所有参数和变量s,a,s′和x连续。
• C.2有界性条件：存在a,b和L使得sup ρ (s)<b,sup p(s′|s,a)
s 0 a,s,s′
<b,sup R(s,a)<b,sup ∥∇ p(s′|s,a)∥<L,sup ∥∇ R(s,a)∥<L。
a,s a,s,s′ a a,s a
定理2.3 确定性策略梯度定理：假设MDP满足条件C.1，即连续的∇ µ (s),∇ Qµ(s,a)和确定
θ θ a
性策略梯度的存在性，那么
Z
∇ J(µ )= ρµ(s)∇ µ (s)∇ Qµ(s,a)| ds (2.150)
θ θ θ θ a a=µθ(s)
S
=E s∼ρµ[∇ θµ θ(s)∇ aQµ(s,a)| a=µθ(s)] (2.151)
证明: 确定性策略梯度定理的证明基本遵循与文献 (Suttonet al., 2000) 的标准随机性策略梯度定
理一样的步骤。首先，为了方便在后续证明中交换导数和积分，以及积分的顺序，我们需要使用
两个引理，它们是微积分里的基本数学公式，如下：
引理 2.3 莱布尼茨积分法则（Leibniz Integral Rule）: f(x,t) 是一个使得 f(x,t) 及其偏导数
f′(x,t)在(x,t)-平面的部分区域上对t和x连续的函数，包括a(x)⩽t⩽b(x),x ⩽x⩽x 。同
x 0 1
时假设函数a(x)和b(x)都是连续的且在x ⩽x⩽x 上有连续导数。那么，对于x ⩽x⩽x ，
0 1 0 1
Z Z
d b(x) d d b(x) ∂
f(x,t)dt=f(x,b(x))· b(x)−f(x,a(x))· a(x)+ f(x,t)dt (2.152)
dx dx dx ∂x
a(x) a(x)
引理 2.4 富比尼定理（Fubini’s Theorem）：假设 X 和 Y 是 σ-有限测度空间（σ-Finite Measure
Space），并且假设X ×Y 由积测度（ProductMeasure）给出（由于X 和Y 是σ-有限的，这个测
度是唯一的）。富比尼定理声明：如果f 是X ×Y 可积的，那么f 是一个可测函数（Measurable
Function）且有
Z
|f(x,y)|d(x,y)<∞ (2.153)
X×Y
99
第2章 强化学习入门
那么
Z Z Z Z Z
( f(x,y)dy)dx= ( f(x,y)dx)dy = f(x,y)d(x,y) (2.154)
X Y Y X X×Y
为了满足以上两个引理，我们需要C.1所提供的充分条件作为莱布尼茨积分法则的要求，即
Vµθ(s)和∇ θVµθ(s)是θ 和s的连续函数。我们也遵循状态空间S 紧致性（Compactness）的假
设，如富比尼定理所要求，即对于任何θ，∥∇ θVµθ(s)∥，∥∇ aQµθ(s,a)| a=µθ(s)∥和∥∇ θµ θ(s)∥是
s的有界（Bounded）函数，而这在C.2中提供。有了以上条件，我们可以得到以下推导：
∇ θVµθ(s)=∇ θQµθ(s,µ θ(s))
Z
=∇ θ(R(s,µ θ(s))+ γp(s′|s,µ θ(s))Vµθ(s′ )ds′ )
S Z
=∇ θµ θ(s)∇ aR(s,a)| a=µθ(s)+∇ γp(s′|s,µ θ(s))Vµθ(s′ )ds′
θ
Z S
=∇ θµ θ(s)∇ aR(s,a)| a=µθ(s)+ γ(p(s′|s,µ θ(s))∇ θVµθ(s′ )
S
+∇ θµ θ(s)∇ ap(s′|s,a) ZVµθ(s′ ))ds′
=∇ θµ θ(s)∇ a(R(s,a)+ γp(s′|s,a)Vµθ(s′ )ds′ )| a=µθ(s))
Z S
+ γp(s′|s,µ θ(s))∇ θVµθ(s′ )ds′
S Z
=∇ θµ θ(s)∇ aQµθ(s,a)| a=µθ(s)+ γp(s′|s,µ θ(s))∇ θVµθ(s′ )ds′ (2.155)
S
在上面的推导中，莱布尼茨积分法则被用于交换求导和积分的顺序，这要求满足p(s′|s,a)，µ (s)，
θ
Vµθ(s)和它们的导数对θ的连续性条件。现在我们用∇ θVµθ(s)对以上公式进行迭代，得到：
∇ θVµθ(s)=∇ θZµ θ(s)∇ aQµθ(s,a)|
a=µθ(s)
+ γp(s′|s,µ θ(s))∇ θµ θ(s′ )∇ aQµθ(s′ ,a)| a=µθ(s′)ds′
ZS Z
+ γp(s′|s,µ θ(s)) γp(s′′|s′ ,µ θ(s′ ))∇ θVµθ(s′′ )ds′′ ds′
S S
=∇ θZµ θ(s)∇ aQµθ(s,a)|
a=µθ(s)
+ γp(s→s′ ,1,µ θ(s))∇ θµ θ(s′ )∇ aQµθ(s′ ,a)| a=µθ(s′)ds′
ZS
+ γ2p(s→s′ ,2,µ θ(s))∇ θµ θ(s′ )∇ aQµθ(s′ ,a)| a=µθ(s′)ds′
S
+···
100
2.7 策略优化
Z
X∞
= γtp(s→s′ ,t,µ θ(s))∇ θµ θ(s′ )∇ aQµθ(s′ ,a)| a=µθ(s′)ds′ (2.156)
S
t=0
其中，我们使用富比尼定理来交换积分顺序，而这要求∥∇ θVµθ(s)∥的有界性条件。上述积分中
包含一种特殊情况，对s′ =s有p(s→s′,0,µ (s))=1而对其他s′为0。现在我们对修改过的性
θ
能目标即期望价值函数进行求导：
Z
∇ θJ(µ θ)=∇ ρ 0(s)Vµθ(s)ds
θ
Z S
= ρ 0(s)∇ θVµθ(s)ds
S
Z Z
X∞
= γtρ 0(s)p(s→s′ ,t,µ θ(s))∇ θµ θ(s′ )∇ aQµθ(s′ ,a)| a=µθ(s′)ds′ ds
S S
Z t=0
= ρµθ(s)∇ θµ θ(s)∇ aQµθ(s,a)| a=µθ(s)ds (2.157)
S
其中我们使用莱布尼茨积分法则来交换求导和积分顺序，需要满足 ρ 0(s) 和 Vµθ(s) 及其导数对
θ连续的条件，同样由富比尼定理交换积分顺序，需要满足被积函数（Integrand）的有界性条件。
证毕。
离线策略确定性策略梯度
除了上面在线策略版本的确定性策略梯度（DPG）推导，我们也可以用离线策略的方式来
R P
导出 DPG，使用上面的 DPG 定理和 γ-折扣状态分布 ρµ(s′) := ∞ γt−1p(s)p (s′|s,t,µ)ds。
S t=1
离线策略确定性策略梯度用行为策略（BehaviourPolicy，即使用经验回放池时的先前策略）的样
本来估计当前策略，而这个策略可能跟当前策略不同。在离线策略的设定下，由一个独特的行
为策略 β(s) ̸= µ (s) 所采集轨迹来对梯度进行估计，相应的状态分布为 ρβ(s)，这不依赖于策
θ
略参数θ。在离线策略情况下，性能目标被修改为目标策略的价值函数在行为策略的状态分布上
R R
的平均 J (µ ) = ρβ(s)Vµ(s)ds = ρβ(s)Qµ(s,µ (s))ds，而原始的目标遵循式 (2.149)，即
Rβ θ S S θ
J(µ )= ρ (s)Vµ(s)ds。注意，这里是我们在导出离线策略确定性策略梯度中进行的第一个近
θ S 0
似，即J(µ )≈J (u )，而我们将在后面有另外一个近似。我们可以直接对修改过的目标取微分
θ β θ