property of ﬂoating point additions and non-deterministic
parallel scheduling of GPU [55]. These three algorithms
are frequently used by convolution operations in many
ML frameworks (e.g., PyTorch [61], Tensorﬂow [1], and
Caffe2 [29]), and causes both training and inference of a
convolutional layer non-deterministic. Figure 2 illustrates a
non-deterministic problem during online-learning caused by
these three algorithms. In the backward propagation phase of a
CNN model, the CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0
algorithm is used. These algorithms can also make ML
inference (with only forward propagation) non-deterministic.
For instance,
in the forward propagation of a transposed
convolution layer (used for upsampling, a.k.a, deconvolution
layer), the three algorithms are also invoked (as shown in the
source code of Pytorch [66], Tensorﬂow [77], and Caffe2 [5])
and hence
computation can be non-deterministic.
Moreover, some ML runtime features (e.g., autotune [7], [82]
and runtime fusion [27]) can easily cause non-determinism.
the
but
An ongoing project [56] from Nvidia tries to eliminate
these non-determinism sources by providing
some of
a more
deterministic
slower CuDNN backend.
However, some operations have non-determinism sources
inherently, which are difﬁcult
to eliminate. For example,
in PyTorch [61], even enabling the deterministic option
(torch.backends.cudnn.deterministic),
of
operations (e.g., ctc_loss() and embedding_bag()) using
parallel AtomicAdd() are still non-deterministic [67].
set
a
These non-determinism sources make an operator replayed
from a checkpoint easily run into an inconsistent (divergent)
state once the operator fails, even given the same input
sequence as before the failure (Figure 2). We further
conducted a quantitative study (Figure 3) to show how often a
divergence may happen during a checkpoint-replay failover.
The experiment used a Kaggle [33] dataset and a mature
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
186
Fig. 3: Divergence occurrences of evaluating a model replayed
from a checkpoint on a test set of 182 images (vehicle license
number plates) by 10 times. X-axis is the checkpoint interval.
Y-axis is the divergence occurrences.
Mask-RCNN [25] model built with ctc_loss() and enabled
the PyTorch deterministic option. This model often exists in
mission-critical autopilot systems for license plate recognition.
In Figure 3, we counted the occurrences of classiﬁcation errors
whenever the recovered model gave an inconsistent (different)
classiﬁcation result on any of the tested image compared with
that of the original model; we also counted the occurrences
of 8-bit errors whenever the recovered model output an
inconsistent rounded 8-bit precision loss on the whole test
set compared with the original model. The results show that
a longer checkpoint
interval (§I) in the checkpoint-replay
approach causes more occurrences of inconsistency during
failover. Checkpoint-replay systems like LS [85] typically
take a long checkpoint interval (e.g., one checkpoint per 100
requests) for efﬁciency, which makes divergence occur more
often.
A. System Model and Architecture
III. OVERVIEW
Figure 4 shows HAMS’s architecture. To deploy an ML
service graph, a developer provides a deﬁnition of the graph
and a set of pre-trained models. HAMS deploys these models
on a cluster of physical hosts with each model co-located
with a HAMS proxy. A HAMS proxy encapsulates the logic
for request propagation, state replication, and failover, while
a model only takes a batch of inputs and produces outputs.
A developer should specify whether each model is stateful or
stateless in the DAG. HAMS replicates each stateful model
with a primary and a backup for fast failover. HAMS do not
replicate stateless models as they do not hold internal state.
HAMS also provides a group of frontend servers replicated
with SMR [59] to handle client requests: on receiving a client
request, a frontend logs the request, sends it to the service
graph, and returns the result processed by the service graph
back to the client. HAMS has a global manager replicated with
SMR [59] for each deployment domain (e.g., a datacenter) to
store deployment information and to handle failover.
HAMS supports general ML services
represented as
Directed Acyclic Graphs
(DAG). Cyclic graphs with
back-edges (e.g., reinforcement learning [31]) can be easily
converted to DAGs in HAMS by letting their back-edges point
to the frontend. In a DAG, if a model has multiple input
sequences, the service developer can determine whether to let
the model combine requests from these sequences or process
them in an arbitrarily interleaving manner; if a model has
clients
Frontend
Frontend
Frontend
1-Training
Data Generator
Hams Proxy
Host
2-Object
Extraction
Hams Proxy
Host
Graph
definitions
Manager
Manager
Manager
developers
3-VGG19
Primary
Request Manger
Request Buffer
State Manager
State Buffer
Hams Proxy
Host
3-VGG19
Backup
State Manager
State Buffer
Hams Proxy
Host
4-Image
Query
Hams Proxy
Host
A stateless model
A stateful model
Client requests
Processed results
Intermediate requests
State propagations
A cluster
Fig. 4: HAMS architecture with components in green. A cluster
can be a datacenter or a car with computers (e.g., autopilot).
multiple output streams, each output of the model can be sent
to a speciﬁc subset or the entire set of downstream operators.
As shown in Figure 4, a HAMS proxy for the primary of a
stateful model has two main modules. The Request Manager
buffers received input requests from upstream models, logs
metadata for these requests (§IV-D), and synchronously passes
a batch of requests to the model for processing. When the
model ﬁnishes processing a batch of requests,
the State
Manager retrieves the model’s internal state into a state buffer
and sends the state to its backup.
A HAMS proxy for a stateless model activates only its
request manager module; a HAMS proxy for the backup of a
stateful model activates only its state manager module. When
a backup’s proxy receives a state from its primary, it ﬁrst saves
the state in the state buffer, and determines when to apply the
state to the model based on HAMS’s NSPB protocol (§IV-C).
Failure model. HAMS’s failure model is the same as typical
fault-tolerant systems for dataﬂow applications [85], [87]: a
host can fail, network packets can be dropped or reordered,
and network can be partitioned. As a primary-backup system,
HAMS assumes that the primary and backup of the same model
do not fail simultaneously: HAMS handles one host failure
for each stateful operator or each stateless operator (§IV-E).
HAMS also uses efﬁcient state machine replication [59] to
replicate the frontend as it is deterministic.
B. Context-aware Non-stop Primary-backup
Primary-backup is a powerful approach to provide fault
tolerance for applications with internal non-determinism. For
instance, Remus is a notable fault-tolerance system for generic
applications running with a virtual machine (VM) as a black
box. The upper half of Figure 5 shows the workﬂow of Remus.
Remus ﬁrst lets the primary execute a batch of requests (say
the nth batch) and buffers its outputs, and then replicates
the primary’s state with three steps. First, Remus stops the
primary and copies its updated states to a memory buffer.
Second, Remus lets the primary execute the (n + 1)th batch of
requests and sends the copied state to the backup. Third, once
the backup successfully receives the state update, the primary
releases its output for the nth batch to downstream models.
This output buffering is essential to ensure global consistency.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
187
State #n-1
State #n
State being updated
Output release
Execution finish
B. Retrieves A Model’s State without Stopping It
Remus’s execution timeline
Primary
Processing batch #n
Primary’s memory buffer
Backup
Stop
1
Processing batch #n+1
2
3
Finish processing batch #n
Stop-and-buffer
Output for batch #n
HAMS’s NSPB execution timeline
Primary
Finish processing batch #n
Update
for batch #n 
1
Computation 
for batch #n+1
2
3
Update
for batch #n+1 
Computation 
for batch #n
Backup
Output for batch #n
Fig. 5: Comparison of Remus and NSPB: NSPB releases
outputs to downstream models much faster than Remus.
this
However,
traditional primary-backup approach is
signiﬁcantly inefﬁcient for replicating stateful models in a
service graph: the processing for the nth batch ﬁnishes at
the time of the red dotted line, but the output is buffed until
the third step ﬁnishes. In a service graph, the delay caused
by buffering will occur multiple times when a client request
goes through multiple stateful models in the graph, causing
prohibitive slowdown for mission-critical services.
The bottom half of Figure 5 shows the workﬂow of
HAMS’s NSPB protocol with three steps. First, the primary
releases the output to downstream operators immediately after
processing the nth batch of requests. Second, the primary
sends its state made by the nth batch to its backup in
parallel with the computation stage of the (n + 1)th batch
of requests. Third, the primary enters its update stage for
the (n + 1)th batch after the state made by the nth batch is
delivered to the backup. NSPB eliminates the stop-and-buffer
delay in a traditional primary-backup system and incurs little
performance overhead. We carry a proof of NSPB’s global
consistency in a service graph in §IV-F.
IV. HAMS’S RUNTIME PROTOCOL
A. Preliminaries
As a service’s models work in a DAG, we can topologically
sort the models and use Mp to denote the pth model. We say
that Mp is a predecessor of Mq if there is an edge (Mp → Mq)
in the graph, and Mq is a successor of Mp. We sayM x is
Mp’s downstream model if Mp can reach Mx in the graph, but
Mx is Mp’s successor only if they are adjacent in the graph.
p to denote the ith request that the Mp model
We use ri
p to denote the resultant state (si
executes, si
p = ∅ if Mp is
stateless), and oi
p to denote the corresponding output. We say
a state si
p is durable [85] if Mp’s backup has applied the
state. To ease discussion, we further deﬁne a stateful model
Mp’s next stateful models as the nearest downstream stateful
models for Mp. Formally, a stateful model My is an Mp’s next
stateful model if: there exists one path that Mp can reach My
and there is no other stateful model in the path between Mp
and My. Symmetrically, we deﬁne previous stateful models as
the nearest upstream stateful models.
Existing primary-backup approaches need to stop the
primary to retrieve its internal state. For instance, Remus
leverages VM-speciﬁc techniques (e.g., shadow page tables)
to record updated memory pages (a.k.a, dirty pages) on the
primary VM and periodically stops the primary VM for
retrieving the dirty pages to send to the backup.
However,
this black-box approach is not suitable for
replicating stateful ML models in a service graph as recent ML
models typically run on GPUs. The bandwidth between GPU
memory and CPU memory is around one order of magnitude
smaller than the bandwidth between CPU and its memory [62].
If Remus’s black-box approach is used, the stop time will be
much longer for retrieving dirty GPU memory. Moreover, it is
still an open challenge [14], [16] to efﬁciently identify dirty
GPU pages, so Remus has to copy all GPU memory out ﬁrst.
Unlike Remus, NSPB introduces a white-box mechanism
based on the compute-then-update nature of ML models
(§II-B) to retrieve a stateful model’s internal state without
stopping it. As illustrated in Figure 5, during the computation
stage for the n + 1th batch, the primary model’s model state
(i.e., parameters) stays intact, same as after processing the
nth batch. Thus, the primary’s proxy can retrieve the model’s
internal states in parallel with the computation stage of the
n + 1th batch, without the need to stop it.
Occasionally, state retrieval may take longer time than
the computation stage. To prevent the proxy from getting
corrupted state, HAMS lets the primary model wait for a signal
from the proxy (§V) before entering the update stage, the
proxy sends the signal after ﬁnishing its state retrieval.
C. Release Outputs without Waiting for State Propagation
For clear exposition of idea, in this subsection, we use
a deliberately simpliﬁed setting with models working in a
service chain (no branches) and batch sizes all being one.
To ensure global consistency, existing primary-backup
approaches let a primary buffer a request’s output until the
state modiﬁed by this request is delivered to its backup (i.e.,
durable). Previous works [15], [46], [84] show that output