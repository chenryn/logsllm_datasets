(cid:96) ≤ (cid:98)log2(q)(cid:99)− 2, q ≡ 1 mod 2(cid:96)
skAHEq, skAHEp
0 ≤ β  (q− 1)/2)·(cid:98)q/2(cid:96)(cid:99)
S generates a random (cid:104)wrap(cid:105)S
S sends [(cid:104)wrap(cid:105)C
q ]q −(cid:104)wrap(cid:105)S
S, C has every values stored in preS or preC, resp.
C sends (cid:104)u(cid:105)S
S: (cid:104)wrap(cid:105)S
q ∈ Zq
q ]q ← multd · [(cid:104)u(cid:105)C
q to S
q
q to C
wrap-around error. We do not adopt this trick in GPU-DGK
because it takes extra computational and communication costs.
Instead, we impose a constraint that q = 1 mod 2(cid:96) through-
out our framework, so DGK(α,β) (cid:54)= DGK( ˆα,β) only occurs
when α = β, implying x = y, but it is ﬁne since the result
merely serves for max(x,y). This constraint is speciﬁcally
beneﬁcial for us, and it seems no related works did it before.
Appendix B proves this constraint makes GPU-DGK correct.
3.6 GPU-Friendly Secure Comparison Layers
Secure Max Computation and ReLU Layers. As
max(x,y) = (x ≤ y)· (y− x) + x, we compute (cid:104)max(x,y)(cid:105) =
(cid:104)x ≤ y(cid:105)· ((cid:104)y(cid:105)−(cid:104)x(cid:105)) +(cid:104)x(cid:105) with (cid:104)x ≤ y(cid:105) output by GPU-DGK,
where share multiplication can be done efﬁciently online by
GPU with Beaver’s trick. ReLU(x) is computing max(x,0).
Maxpool Layers. Maxpool can use max() in a binary-tree
style, e.g., max(max(xxx0,xxx1),max(xxx2,xxx3)), where xxxi are in the
vector form. For n inputs with window size w, the number of
comparisons is n· (1−2−(cid:100)log2(w)(cid:101)), and we need to invoke our
GPU-DGK for (cid:100)log2(w)(cid:101) rounds. To reduce the invocations
of max(), we apply the maxpool layer before the ReLU layer
as in Falcon [13] when they are next to each other.
3.7 Inference from SWALP-trained Networks
SWALP’s (De)quantization. SWALP [28] quantizes the
values of input xxx (from queries or previous layers) and
weight www of linear layers f . It also dequantizes the out-
put values. The boldface type here emphasizes that the
inputs can be operated as a set or a tensor. Let bit be
the number of bits in ﬁxed-point computation. It deﬁnes
a quantization function Q(xxx f ) that outputs xQ = clip((cid:98)xxx f ·
2−expx+bit−2(cid:101)), where clip(a) = min(max(a,−2bit−1),2bit−1),
Ofﬂine Input (S|C)
Online Input (S|C)
Output (S|C)
Constraints
skAHEq
(cid:104)x(cid:105)C, (cid:104)y(cid:105)C
(cid:104)x ≤ y(cid:105)C
q
log2(p),log2(q)  q. Our protocol’s output usually involves
an additional z/d term, e.g., z/2(cid:96) in Line 11 of Protocol 2,
where d < q is a public divisor. To ensure correctness, we need
to offset the −q/d term as if wrap-around does not happen.
We propose GPU-Wrap (Protocol 3), our GPU-friendly
wrap-around handling protocol, to produce the shares (cid:104)wrap(cid:105)
that can offset −q/d. Namely, we want z/d − τ/d + wrap ≈
s/d. As observed by Veugen [25], we can assume s < 2(cid:96)+1 <
(q− 1)/2 is always in the “ﬁrst half” of [0,q− 1], and wrap-
around happens if and only if τ is in the “second half,” i.e.,
τ ∈ [(q− 1)/2,q), and z is wrapped to the ﬁrst half, i.e., z =
s+τ mod q = s+τ−q ∈ [0, (q−1)/2). In other words, given
public q and d, GPU-Wrap computes
wrap = fτ(z) = (τ ≥ (q− 1)/2)· (z < (q− 1)/2)·(cid:98)q/d(cid:99)
which is an ofﬂine-known linear function for the online input z
of C if S randomly picks τ ∈ Zq ofﬂine.
To extend DGK to handle probably negative inputs, Veu-
gen [25] argues that, in addition to the above wrap-around off-
set, it should take ˆα = α−q mod 2(cid:96) instead of α to handle the
USENIX Association
30th USENIX Security Symposium    2153
expx = (cid:98)(log2◦max◦abs)({x f ,i}i)(cid:99) is an auxiliary output of
an integer indicating the highest magnitude among the values
in xxxQ, and (cid:98)·(cid:101) is stochastic rounding [9]. The quantization for
the weight Q(www f ) is also deﬁned similarly. The resulting out-
put yyyQ = f (xxxQ;wwwQ) is then dequantized accordingly via yyy f =
DeQ(yyyQ; expx, expw), deﬁned to be yyyQ · 2expx+expw−2·bit+4.
Turning a SWALP-trained model for oblivious inference
is challenging because we operate secret shares in Zq with
(linear) homomorphism, but (de)quantization is non-linear.
3.7.1 Precomputing the Maximum
We observe that once the training is done, the maximum value
in the weight is ﬁxed, so does expw. So we can precompute
expw for each linear layer. Meanwhile, a trained network has
more or less learned the distribution of the input and interme-
diate data, i.e., x, and thus we can sample x to compute expx.
So the inference phase can use expx and expw derived from
training, and treat expx and expw as learnable parameters.
3.7.2 Fusing (De)quantization into Truncation