### Optimized Text

**Training and Evaluation of Classifiers for Cyberbullying Detection in Images**

We trained our models on both high-level factors and low-level image features. Specifically, we randomly allocated 80% of the dataset for training, employing 5-fold cross-validation, and reserved the remaining 20% for testing. We then evaluated four types of classifiers on the test dataset. The performance of these classifiers was assessed using Receiver Operating Characteristics (ROC) analysis [42], which provides a means to review the trade-off between False Positive Rate (FPR) and True Positive Rate (TPR).

**ROC Analysis and Model Performance**

The ROC curves for the classifiers are shown in Figure 10, with the Area Under the Curve (AUC) indicating the overall success of each model in detecting cyberbullying images.

- **Baseline Model**: Precision = 63.0%, Recall = 29.68%
- **Factors-only Model**: Precision = 82.96%, Recall = 79.34%
- **Fine-tuned Pre-trained Model**: Precision = 81.40%, Recall = 73.70%
- **Multimodal Model**: Precision = 94.27%, Recall = 96.93%

**Performance Metrics**

- **True Positive Rate (TPR)**: The proportion of actual positive samples correctly identified as positive.
- **False Positive Rate (FPR)**: The proportion of actual negative samples incorrectly identified as positive.

These metrics are used to analyze the performance of the models, and their values are computed according to the formulations in [42].

**Model Performance Analysis**

1. **Baseline Model**:
   - The baseline model (Table IX) has the lowest performance, with a precision of 63.0% and recall of 29.68%. This indicates that cyberbullying in images is not a trivial problem and requires more sophisticated approaches.
   - The AUC of 0.79 (Figure 10a) suggests a high number of false predictions.

2. **Factors-only Model**:
   - This model (Table IX) shows improved performance with a precision of 82.96% and recall of 79.34%.
   - The AUC of 0.82 (Figure 10b) indicates that adding just the factors (without the original image) significantly improves classification.
   - The significant improvement in recall suggests that the identified visual factors effectively distinguish true positives.

3. **Fine-tuned Pre-trained Model**:
   - Despite a higher accuracy, this model (Table IX) has a lower recall of 73.70% compared to the factors-only model.
   - The model appears biased towards non-cyberbullying images, likely due to the imbalance in the dataset.
   - The lack of identified cyberbullying image factors limits its ability to accurately detect cyberbullying.

4. **Multimodal Model**:
   - This model (Table IX) demonstrates the highest performance with a precision of 94.27% and recall of 96.93%.
   - The AUC of 0.96 (Figure 10) indicates excellent performance in terms of FPR and TPR.
   - The high precision and recall suggest that the identified visual factors are crucial for distinguishing cyberbullying images, especially given the subtle differences between harmful and harmless images.

**Precision-Recall Analysis**

To further interpret the model performance, especially considering the unbalanced nature of the dataset, we generated a precision-recall (PR) plot for the multimodal model (Figure 11). The PR plot confirms that the multimodal model can correctly classify cyberbullying images with high precision.

**Performance Overhead in Mobile Applications**

Given the role of mobile phones in facilitating cyberbullying, we conducted an experiment to evaluate the overhead of deploying our multimodal model on a mobile device. We used the PyTorch Mobile framework [17] to deploy the model on a Samsung Galaxy S5 with 256 MB of memory. The experiment involved measuring two types of overheads:

1. **Model Time**: The time taken to execute a forward pass of the model.
2. **Render Time**: The time taken to resize an image and render a warning message if cyberbullying is detected.

We tested the model on 1000 randomly selected photos from the test dataset. The results (Figure 12) show that both the model time (average 753 milliseconds) and render time (average 0.06 milliseconds) are within practical limits, indicating that the model can be deployed on mobile devices with minimal overhead.

**False Positives Evaluation on American Sign Language Dataset**

To ensure that the model does not flag benign hand gestures as cyberbullying, we evaluated it on a publicly available American Sign Language (ASL) dataset [46]. The multimodal model correctly identified all 479 ASL images as non-cyberbullying, indicating that it can distinguish between harmful and harmless hand gestures.

**Validation of Cyberbullying Factors with a Wider Audience**

We validated the identified cyberbullying factors with a wider audience through a study involving 104 participants from Amazon MTurk. Participants were shown random image samples and asked to identify the factors causing cyberbullying in a free text box. The results (Table XI in Appendix A) confirmed that the factors identified by participants align with those in our analysis.

- **Body-pose Factor**: 84.61% of participants identified body-pose as a factor when the person was posing directly towards the viewer.
- **Facial Emotion Factor**: Only 9.43% of participants mentioned facial emotion as a factor, consistent with our findings.
- **Hand Gesture Factor**: 80.4% of participants identified hand gestures as factors, with 97% specifically mentioning the loser sign and 82.7% the middle-finger sign.
- **Threatening Object Factor**: 88.29% of participants identified threatening objects as factors.
- **Social Factor**: 89% of participants identified anti-LGBT symbols as a strong factor.

**Representativeness of the Cyberbullying Images Dataset**

To ensure the representativeness of our dataset, we compared it with another set of cyberbullying images [85]. After filtering, 31 images from their dataset were labeled as cyberbullying based on the content of the images alone, confirming that our dataset is representative of real-world cyberbullying in images.