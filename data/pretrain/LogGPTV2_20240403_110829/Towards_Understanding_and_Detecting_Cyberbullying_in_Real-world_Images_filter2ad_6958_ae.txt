trained on high-level factors and low-level image features, we
randomly select 80 percent of our dataset for training (with 5-
fold cross validation) and 20 percent of the dataset for testing
and we run the four types of classiﬁers on images from our
test dataset. We perform the Receiver Operating Characteristics
(ROC) [42] analysis of the classiﬁer models for cyberbullying
images prediction. The ROC analysis provides a means of
reviewing the performance of a model in terms of the trade-
off between False Positive Rate (FPR) and True Positive Rate
(TPR) in the predictions. The ROC plot of the classiﬁer models
5Here, “factor” refers to EFA factors and not visual factors of cyberbullying.
Fig. 9: Factor loadings of the features across two extracted
factors.
for cyberbullying detection in images is depicted in Figure 10.
The Area Under the Curve (AUC) of each classiﬁer model is
depicted in the plots, which indicates the success of a model
in detecting cyberbullying images.
(a) Baseline Model
(b) Factors-only Model
(c) Fine-tuned Pre-trained Model
(d) Multimodal Model
Fig. 10: ROC analysis of classiﬁer models.
Fig. 11: Precision-recall graph of the multimodal model.
The TPR is a metric that represents how many correct
positive results occurred among all positive samples available
in the test dataset. FPR represents how many incorrect positive
results occurred among all the negative samples available in the
11
Classiﬁer Model
Baseline Model
Factors-only Model
Fine-tuned
Pre-
trained Model
Mutimodal Model
Accuracy
77.25%
82.96%
88.82%
Precision
63.00%
79.34%
81.40%
Recall
29.68%
80.84%
73.70%
93.36%
94.27%
96.93%
TABLE IX: Accuracy, precision and recall of classiﬁer models.
test dataset. These metrics are used in the ROC plots to analyze
the performance of a model. We compute these evaluation
metrics according to formulations in [42].
We ﬁnd that the baseline model (Table IX, precision =
63.0% and recall = 29.68%) indeed has the lowest perfor-
mance, indicating that cyberbullying in images is not a problem
that can be trivally solved. Indeed, in our analysis, we ﬁnd that
cyberbullying in images is a highly contextual problem, which
needs special investigation about its factors. From Figure 10a,
a low AUC of 0.79 indicates that this model has a large number
of false predictions.
Next, we investigate the factors-only model (Table IX, pre-
cision = 82.96% and recall = 79.34%). A better performance
than the baseline model does indicate that even adding just the
factors (without showing a model the original image) has quite
powerful effect in classifying cyberbullying (Figure 10b, AUC
= 0.82). Another observation we make about the factors-only
model is that the recall is improved signiﬁcantly, indicating
that the identiﬁed visual factors do demonstrate the ability to
distinguish the true positives (cyberbullying labeled images).
From our observations, the ﬁne-tuned pre-trained model
(Table IX, precision = 81.40% and recall = 73.70%) does
not perform overall better than the the factors-only model.
Although the accuracy is higher,
the recall of this model
is signiﬁcantly lower, which indicates that
is
not able to distinguish the cyberbullying images. On further
examination,
this model seems to be biased towards non-
cyberbullying images, which could be attributed to our dataset
containing a signiﬁcantly higher number of non-cyberbullying
images compared to the cyberbullying images. Ideally, for
good performance, we expect a model to have high precision
and recall, and not just a high accuracy. We attribute the
low performance of this model to the lack of the identiﬁed
cyberbullying image factors. For example, a cyberbullying
image portraying a person showing a gesture is interpreted by
this model as just a person (since it is pre-trained). However,
this model lacks the capability to distinguish that the person
may be showing a gesture at the viewer.
this model
Finally, we ﬁnd that the multimodal classiﬁer demonstrates
the highest performance (Table IX, precision = 94.27% and
recall = 96.93%) among the different classiﬁer models. A
high AUC (Figure 10, AUC = 0.96) is indicative of a good
performance on the false positives and the false negatives.
Note that this model is aware of the cyberbullying image
factors identiﬁed in this work and also the low-level image
features. A high precision and recall of this model indicates
that the visual factors identiﬁed in this work are needed in
order to distinguish especially the cyberbullying images. Due
to the highly contextual nature of cyberbullying in images, the
differences between such images and harmless images are very
subtle. Therefore, we believe that the multimodal classiﬁer
demonstrates that our visual factors can be used to detect
cyberbullying images accurately in real-world applications.
To interpret the model performance considering the unbal-
anced nature of our dataset, we depict the balance between
the precision and recall in the case of the multimodal model
in the precision-recall (PR) plot in Figure 11. The PR plot
indicates that the multimodal model is able to correctly classify
cyberbullying images with high precision.
D. Performance Overhead in Mobile Applications
Mobile phones play a major role in engendering cyber-
bullying in images, especially due to the on-board equipment,
such as cameras, on these devices. Thus, our intention is that
our models can be deployed on mobile devices to defend users
against cyberbullying in images. To this end, we carry out an
experiment to study the overhead of our model in a mobile
application. We use the PyTorch Mobile framework [17] to
deploy our multimodal model
in an Android application,
running in a Samsung Galaxy S5 mobile phone, with a
memory capability of 256 megabytes. Note that we conduct
this experiment on an older Android device in order to show
that our model can be even run on weaker mobile devices. We
are interested in measuring two types of overheads potentially
introduced by running our model: (1) the model time, which
is the time taken to execute a forward pass of our model; and
(2) the render time, which is the time taken to resize an image
according to the input dimensions needed by our model, and
to render a warning message to the user if cyberbullying is
detected in an image. To study the bearing of different sized
photos, we measure these overheads with respect to the photo
size. In this experiment, we randomly select 1000 photos from
our test dataset and run them through the Android application
with our model. We depict both the model time and the render
time in Figure 12.
Fig. 12: Overhead evaluation of the multimodal model inte-
grated into an Android application.
From Figure 12, we ﬁrst observe that both the model
time and the render time are mostly within the millisecond
range, showing that it is indeed practical to adopt our models
in mobile devices. We note that the size of the photo does
not have any signiﬁcant bearing over the model time and the
render time, as we do not notice any effect of the size of
image on the performance. We observe that the average model
time is 753 milliseconds and the average render time is 0.06
milliseconds, both of which are sufﬁciently small. Thus, using
the multimodal model in mobile devices only cause a minor
overhead on the devices.
12
E. False Positives Evaluation on American Sign Language
Dataset
Our analysis of cyberbullying factors in images reveal that
hand gestures play a major role in carrying out cyberbullying.
However, many harmless hand gestures, such as those used
in the American Sign Language (ASL), are quite ubiquitous,
and a concern with a cyberbullying model
it may
ﬂag down such benign images as cyberbullying images. In
this experiment, our objective is to conduct a false positive
evaluation of our model on images from a publicly available
ASL dataset [46]. Figure 13 depicts two samples from this
dataset.
is that
Fig. 13: Image samples from the ASL dataset.
We run the multimodal model on all the test images of the
ASL dataset (the ASL test dataset consists of 479 images).
Our multimodal model correctly detects all 479 images as
non-cyberbullying images. This indicates that our model has
learned to identify the harmful cyberbullying hand gestures,
while the other hand gestures, such as the ones in the ASL
dataset, are precisely detected as non-cyberbullying.
F. Validation of Cyberbullying Factors with a Wider Audience
In our work we introduce new factors of cyberbullying in
images, as discussed in Section V-B We compile these factors
by carefully observing the images labeled as cyberbullying by
participants who take part in our data collection task. In this
evaluation, we carry out a study to validate these factors with a
wider audience. A sample of our study is depicted in Figure 14
in Appendix A. In our study, we ﬁrst show each participant,
randomly selected image samples depicting a factor of cyber-
bullying, and ask the participant to input the factors, due to
which the image samples have been reported as cyberbullying,
in a free text box. By providing a free text box, we ensure that
participants are not biased in any way by the factors compiled
by us. Furthermore, we also provide participants the option
to choose the images as non-cyberbullying thereby further
reducing any bias effects. We collect the free text responses for
several cyberbullying images depicting different attributes of
the cyberbullying factors. Asking participants to enter factors
on their own allows the participants to think of factors by
themselves without any bias and also allows us to validate our
factors from a larger audience.
Our study was approved by our institution’s IRB. We
recruited 104 participants from Amazon MTurk for this study.
Each task took about 10 minutes on average, and we paid a
reward of $2 for task completion. Three participants failed our
attention check questions and two participants had entered the
exact same text for all the images, and failed the attention
check questions. After ﬁltering out these ﬁve participants, we
were left with 99 total participants in our study.
13
Next, we have to determine the factors from the free text
entries that were entered by our participants. We identiﬁed
the cyberbullying factors from participants’ entries by mining
them for text keywords and phrases pertaining to individual
factors. For example, we used the words/phrases such as
“pointed”, “directed at me” and “aimed at me” to interpret that
a participant is indicating that the body-pose of the person in
the image is the cause of cyberbullying, and keywords like
“gun”, “pistol” and “ﬁrearm” to interpret that a participant
is indicating that a threatening object, such as a gun, in the
image is the cause of cyberbullying. We provide a full list of
these words and phrases in Table XI of Appendix A. In the
following, we discuss our ﬁndings from this study.
From the results of our study, the overall χ2 [61] shows sig-
niﬁcant variation (χ2(11) = 308.84, p < .0001) among the 12
conditions (e.g., body-pose, gun, knife, middle ﬁnger, etc.) for
the identiﬁed factors from participants’ entries, indicating that
different factors affected cyberbullying perception differently.
For the body-pose factor, we presented two samples to each
participant. The ﬁrst sample showed a person posing directly
towards the viewer with a threatening object (e.g., Figure 14
in Appendix A). The second sample showed a person posing
away from the viewer with a similar threatening object. For
the image sample with the person directly posing towards
the viewer, 84.61% of participants who found this image as
cyberbullying identiﬁed the factor to be the body-pose of the
person in the image. For the image sample with the person
posing away from the viewer, 72.41% of the participants
found it to be non-cyberbullying, and none of the participants
identiﬁed the body-pose of the person for this image sample.
We think it is possible that the few participants who chose
this image sample as cyberbullying could base their opinions
on the threatening objects in this sample, although the body-
pose of the person in the image is not correctly identiﬁed as
a factor by all the participants. From the participants’ entries,
we found that they were most concerned that the image with
the person posing towards the viewer is directly threatening the
viewer by this pose, from responses such as “Someone holding
a gun and pointing it at the camera could be a direct threat to
you” and “She is aiming a gun and when I look at the image it
seems to be pointed directly at me”. Thus, the participants have
identiﬁed body-pose as a factor in the cyberbullying image.
Next, we discuss the results about the facial emotion factor
in our study. In our study, each participant was shown an
image sample based on facial emotions of joy, sorrow, anger
and surprise. Overall only 9.43% of participants mentioned the
facial emotion as a factor of cyberbullying, which is consistent
with our ﬁnding in Section VI-B that the facial emotion does
not have a signiﬁcant effect over cyberbullying in images.
Thus, we believe that the facial emotion by itself is not a
strong factor of cyberbullying images.
We then discuss the results about the hand gesture factor
in our study. We showed each participant an image sample of
a person showing the middle-ﬁnger, loser sign, and thumbs
down hand gesture, all belonging to the hand gesture factor
category. Overall 80.4% of participants discussed these hand
gestures as factors of cyberbullying, with 97% of participants
speciﬁcally mentioning the loser hand sign and 82.7% of the
participants speciﬁcally mentioning the middle-ﬁnger sign as
factors of cyberbullying in images. Thus, the participants have
captured the hand gesture as an effective cyberbullying factor
in images.
For the threatening object factor, we showed each partic-
ipant image samples depicting gun, knife, and noose, which
belong to the threatening object factor category. 88.29% par-
ticipants discussed these threatening objects as the factor of
cyberbullying. We conclude that the participants have rightly
identiﬁed threatening objects as a strong factor of cyberbully-
ing in images.
Lastly, we discuss the results of the social factor of cyber-
bullying in images. In this factor category, we showed an image
sample of an anti-LGBT symbol. 89% of the participants iden-
tiﬁed this social factor for causing cyberbullying in images.
We could observe that most participants consider this factor
as a strong factor of cyberbullying in images. From this user
experiment, we observed when the participants were provided
free text boxes so that they can enter the cyberbullying factors
by themselves, these factors identiﬁed by the participants were
in agreement with the factors that we chose in our analysis.
G. Representativeness of Cyberbullying Images Dataset.
Cyberbullying in images is a complex phenomenon, and
currently there are limited datasets available to study such a
problem. Our cyberbullying images dataset takes a step closer
towards understanding this phenomenon. In order to make our
dataset representative of real-world cyberbullying in images,
we have asked participants to label cyberbullying images based
on a very general guideline (Section III-C1, cyberbullying is
“an act of online aggression carried out through images”). We
carried out another study to compare the representativeness of
the cyberbullying images in our dataset with another set of
cyberbullying images [85]. The authors of [85] have shared
their dataset of cyberbullying images with us. This dataset
is composed of Instagram posts consisting of images and
the associated comments, and the posts (i.e., the images and
the associated comments together) are labeled by participants
as cyberbullying or non-cyberbullying. We ﬁrst ﬁltered those
cyberbullying posts, which were labeled as cyberbullying
due to the content of images, so that we could ﬁlter out
those posts that are only cyberbullying due to the associated
comments. This left us with 316 images. Next, we used the
same guidelines as used by us to label the images of the
posts as cyberbullying. We recruited participants with the same
criteria as in our annotations task from Amazon MTurk for this
task, and used the same criterion for determining an image
as cyberbullying. Overall, 31 images from their dataset were
labeled as cyberbullying on their own. We conclude that their
dataset predominantly needs the associated comments along