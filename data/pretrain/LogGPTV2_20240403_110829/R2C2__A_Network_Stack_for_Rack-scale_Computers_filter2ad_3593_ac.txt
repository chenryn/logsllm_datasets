number of ﬂows and L is the number of links.
Beneﬁts. This strawman design has a few advantages. First,
it avoids the need to probe the network and induce conges-
tion signals like packets drops and queuing to infer a ﬂow’s
sending rate. High path diversity in rack-scale computers
makes the design of an efﬁcient congestion probing mecha-
nism particularly challenging. Second, it obviates the need
for any specialized queuing at the intermediate rack nodes.
Drawbacks. The computational tractability of the algorithm
comes at the expense of possible network under-utilization.
Consider the example scenario in Figure 4(a) (from [40])
with links of capacity one and two ﬂows: ﬂow f 1 from node
1 to 4, and f 2 from node 2 to 4. We assume the routes for
f 1 uses two paths (1 → 4 and 1 →
these ﬂows are given;
3 → 4) with equal probability while f 2 uses one path (2 →
3 → 4). The set of feasible rates for these ﬂows are shown
in Figure 4(b), and the ideal max-min fair allocation is {1,
1}, i.e., each ﬂow sends at a unit rate. This means ﬂow f 1
only uses the path 1 → 4. In contrast, R2C2’s congestion
control ensures that ﬂow f 1’s rate across its two paths is
equal (as dictated by its routing protocol). The restricted
set of feasible rates is shown in Figure 4(c). Our algorithm
converges to the max-min fair rate in this set, { 2
3}.
3 , 2
In summary, respecting the relative rates dictated by the
routing protocol can result in network under-utilization.
However, as described in Section 3.4, R2C2 also adapts the
routing of all long running ﬂows to alleviate this. For our
example, ﬂow f 1’s routing would be changed so it only uses
the path 1 → 4.
3.3.2 Congestion control extensions
The simple design described above makes several simpli-
fying assumptions. Below we relax these assumptions.
New ﬂows. The strawman design assumes that nodes are
aware of the rack’s current trafﬁc matrix. While a ﬂow
startup event is broadcasted to all other nodes, the sender
starts transmitting packets immediately. Thus, there is a
small time period when not all nodes are aware of a new
ﬂow that has already started. To account for such tempo-
rary discrepancy between the actual trafﬁc matrix and what
is perceived by rack nodes, we rely on bandwidth headroom.
During rate computation, we subtract this headroom from
the capacity of network links. Typical datacenter trafﬁc pat-
terns involve many short ﬂows that only contribute a small
fraction of the bytes across the network. This indicates that
a small spare capacity should absorb such short-lived ﬂows
while our congestion control protocol ensures that medium-
to-long ﬂows do not congest the network. In our experiments
detailed in Section 5, a 5% headroom is sufﬁcient even with
bursty trafﬁc patterns.
We note that R2C2 does not need a new data-plane mech-
anism (like phantom queues at switches [3]) to achieve the
bandwidth headroom. Instead, the headroom is incorporated
into the rate computation done at the control plane.
Host-limited ﬂows. The strawman design assumes all ﬂows
are network limited. In practice, however, ﬂows can be bot-
tlenecked at their end points, e.g., at the application itself or
inside the OS. To ensure high network utilization, any band-
width unused by such host-limited ﬂows should be allocated
to ﬂows that can use it. To account for host-limited ﬂows, we
estimate ﬂow demand—the maximum rate at which it can
actually send trafﬁc—at the sender. Given a ﬂow’s demand
estimate, the rate assigned to it is the minimum between its
fair share and its demand. Whenever a ﬂow’s demand drops
below it current rate allocation, the sender broadcasts this in-
formation. This allows all rack nodes to compute allocations
in a demand-aware fashion.
Demand estimation is based on the observation that a ﬂow
sending at a rate higher than what it is currently allocated
will suffer queuing at the sender. The sending node uses this
queuing information to estimate the ﬂow’s demand. Speciﬁ-
cally, ﬂow demand is estimated periodically, and is the sum
of the rate it is currently allocated and the amount of queuing
it observes. Thus,
d[i + 1] = r[i] + q[i]/T
(1)
where d[i +1] is the estimated demand for the next period,
T is the estimation period while r[i] and q[i], respectively, are
the rate allocation and queuing observed by the ﬂow in the
current period. To smooth out any noisy observations, we
use an exponentially weighted moving average of the esti-
1432f2f121f2f11(a)(b)(c)2[1,1]**[2/3,2/3]556mated demand. This estimation applies to open-loop work-
loads. For closed-loop workloads, demand can similarly be
estimated using queuing information [6].
Beyond per-ﬂow fairness. The strawman design provides
per-ﬂow fairness. However, R2C2 can accommodate richer
allocation policies by allowing the operator to specify a
ﬂow’s weight and priority. Many recently proposed high-
level fairness policies such as deadline-based [46] or tenant-
based [37], can be mapped onto these two primitives, similar
to pFabric [4]. A ﬂow’s weight and priority are included in
the broadcast packet announcing the ﬂow start, so all nodes
can use them during the rate computation.
The rate allocation algorithm accounts for ﬂow priorities
and weights as follows. Nodes invoke the allocation algo-
rithm over multiple rounds, one for each priority level. At
each round, ﬂows belonging to the corresponding priority
level are allocated any remaining capacity in a weighted
fashion, i.e., instead of using uniform weights (as in Sec-
tion 3.3.1), each ﬂow is associated with its own weight.
Periodic rate computation. The strawman design recom-
putes ﬂow rates at every ﬂow event. Even at rack-scale, this
will impose too much computation overhead. To amortize
the recomputation cost, we opted for a batch-based design
in which rates are recomputed periodically. In Section 5, we
show that for realistic workloads a recomputation interval in
the range of 500 µs-1 ms is sufﬁcient to ensure high utiliza-
tion and low queuing while introducing a CPU overhead of
less than 8% at the 99th percentile (median value is 1.7%).
While this design was motivated by the need to reduce the
computation cost, it also naturally ﬁlters out very short-lived
ﬂows, which would be pointless to rate-limit.
3.4 Selecting routing protocols
For short ﬂows, a minimal routing protocol can improve
the ﬂow completion time. Thus, new ﬂows start with min-
imal routing. As ﬂows age, their routing can be adapted
based on the rack’s trafﬁc matrix. R2C2 periodically selects
the routing protocol for each long ﬂow to maximize a global
utility metric speciﬁed by the datacenter operator. The adap-
tion is done every few seconds or minutes. Example utility
metrics include the rack’s aggregate throughput or the tail
throughput, as measured across tenants or even across jobs
and application tasks [15, 23]. For ease of exposition, here
we focus on maximizing the aggregate throughput.
Dynamic selection of routing protocols is challenging be-
cause we want to leverage the ﬂexibility of choosing a dif-
ferent routing protocol for each ﬂow. This results in a com-
binatorial number of ﬂow and routing protocol combinations
to be evaluated, which makes exhaustive search intractable.
Further, the search landscape originating from the utility
functions that we considered typically exhibits several lo-
cal maxima. Therefore, simple greedy heuristics (e.g., hill-
climbing) are not effective and more complex global search
heuristics must be adopted.
Initially, we considered techniques such as log linear
learning [5] and simulated annealing [13]. However, we
found them very sensitive to parameter tuning and workload
characteristics. Thus, we opted for genetic algorithms [27],
a search heuristic that emulates the natural selection and evo-
lution. We found it a good ﬁt for our scenario because it has
relatively few tuning parameters and our problem can be nat-
urally encoded as bit strings, where one or more bits are used
to identify the routing protocol assigned to a given ﬂow.
The heuristic works as follows. Initially, we generate a
population of ﬂow and routing protocol combinations (geno-
types) that contains the current routing allocation and other
randomly-generated ones. For each genotype, we compute
the rack’s aggregate throughput (ﬁtness) using the rate com-
putation mechanism described in Section 3.3 and rank them
accordingly. We then generate a new generation that con-
tains the top genotypes of the current population and other
genotypes obtained by recombining (crossover) and mutat-
ing existing genotypes. The process is repeated until the time
expires or until there has been no improvement over a num-
ber of generations. Once the search is over, the server gen-
erates broadcast packets containing the new assignment for
each ﬂow. Our current implementation uses four bytes for
the ﬂow identiﬁer and one byte for the routing protocol per
each ﬂow (§4.2). This means that up to 300 {ﬂow, routing
protocol} pairs can be advertised using a single 1,500-byte
packet. Since this process only applies to long ﬂows, we
consider the overhead negligible.
For simplicity, in our prototype, a single node is respon-
sible for periodically performing the routing selection pro-
cess. In practice, we expect this operation be decentralized
with each node in turn executing it. Since nodes optimize a
global utility metric, instead of selﬁshly optimizing for local
performance, this design does not suffer from any price of
anarchy inefﬁciency [42]. The next node to run the routing
selection heuristic can be chosen randomly using a probabil-
ity distribution centered around the average adaptation pe-
riod or using a deterministic token-based scheme whereby
nodes are selected serially in a round robin fashion.
3.5 Rack data-plane
R2C2 places most data-plane functionality at the source,
resulting in simpliﬁed forwarding at
intermediate rack
nodes. This design is particularly suited to direct-connect
topologies as each packet is forwarded through many nodes.
For each packet, its sender uses the corresponding ﬂow’s
routing protocol to determine the packet’s path. The path is
then encoded into the packet’s header. Further, each ﬂow is
associated with a token bucket that rate-limits the ﬂow to its
current rate allocation. Intermediate nodes forward packets
using source routing. The packet header contains a route
index ﬁeld indicating the index of the next-hop in the packet
path. Thus, every intermediate node simply forwards the
packet to the indicated next-hop after incrementing the route
index in the packet header.
4.
IMPLEMENTATION
We have developed a ﬂexible and efﬁcient emulation plat-
form for rack-scale computers, and implemented R2C2 as a
user-space network stack atop this platform.
557Figure 5: An example of a Maze server emulating a rack
node with two incoming and two outgoing links. The emu-
lated node has two applications.
4.1 Rack emulation platform
Rack-scale computers are currently difﬁcult to acquire.
Further,
they come pre-conﬁgured with static network
topologies and routing protocols, making it difﬁcult to en-
sure that a proposed idea that works on one rack-scale com-
puter will work on another. To address these issues, we
implemented Maze [60], a cluster-based network emulation
platform focusing on rack-scale fabrics. Maze runs on a
cluster of servers connected by a high-bandwidth RDMA-
based switched network. It emulates a rack’s network fabric
as a virtual network atop the switched network. Below we
describe Maze’s key properties and summarize its operation.
Maze provides three key properties: (i) It is conﬁgurable.
It can map any virtual network topology (e.g., tree, mesh,
torus, etc.) onto the underlying cluster. It can also support
multiple routing strategies and transport protocols. (ii) It of-
fers high-performance emulation, with the ability to emulate
high capacity network links. Through micro-benchmarks,
we ﬁnd that it can provide up to 38 Gbps on a 40 Gbps link
using 8 KB packets, and a latency of 3 µs per hop using
(iii) It achieves high-ﬁdelity. Our micro-
small packets.
benchmarks show that Maze can faithfully emulate many
virtual links on the same physical network link.
Figure 5 depicts the operation of a Maze server emulating
two incoming and two outgoing links. To achieve the prop-
erties mentioned above, Maze uses three main techniques:
1) It uses RDMA to transfers packets, 2) uses zero-copy for-
warding, and 3) implements ﬂow rate control.
Packet transfer.
To provide high-performance yet con-
ﬁgurability and ease of experimentation, Maze allows new
routing and transport protocols to be implemented in user-
space. It uses RDMA writes from the senders to data ring
buffers (DR) on receivers’ memory (e.g., DR R1 in Fig-
ure 5), similar to recent RDMA-based key-value stores [24,
31]. In Maze, incoming links, as well as applications (run-
ning emulated nodes), register memory to the RDMA NIC,
which is accessed during RDMA writes. Pointer rings (PR)
reference the registered memory and they are used in the
outgoing links (e.g., PR R1) during RDMA writes.
Forwarding. An outgoing link on a Maze server constitutes
a connection to another Maze server (i.e., RDMA queue
Figure 6: The format of data and broadcast packets.
pair [61]) and a number of pointer rings. To achieve zero-
copy forwarding, for each packet, depending on its routing
information, we pass the packet pointer that references the
incoming ring buffer to the associated pointer ring on the se-
lected outgoing link (e.g., PR R2 on outgoing link 1 sends
packets of DR R2 on incoming link 2). Once the packet is
sent, we zero the memory of the forwarded packet to make
space for new packets in the respective receive ring buffer.
We send application packets in the same fashion (e.g., using
DR A1 and its respective pointer rings).
Rate control. Maze provides different pointer rings for
different ﬂows. This allows for implementation of both
back-pressure based and rate-based congestion control ap-
proaches. Transport protocols can use Maze’s rate limiters
to adjust the rate at which data pointers from a ﬂow pointer
are inserted on an application’s pointer rings belonging to
outgoing links.
4.2 R2C2 implementation
We implemented R2C2 as a user-mode network stack in
Maze. For routing, we have implemented random packet
spraying [22], destination-tag routing [20] and VLB rout-
ing [45]. For congestion control, we have implemented
our rate-based congestion control protocol. This leverages
Maze’s rate control functionality. Overall, our complete pro-
totype (including Maze) consists of 9,836 lines of C++ and
967 lines of control scripts.
Rate computation. We implemented an efﬁcient variant of
the water-ﬁlling algorithm described in Section 3.3.1. A key
challenge was how to efﬁciently compute the link weights
for each ﬂow. We solved this by pre-computing on each
node the list of link weights for each {routing protocol, des-
tination} pair. Assuming a 512-node rack, the memory foot-
print per routing protocol is less than 6 MB, i.e., 511 desti-
nations times the number of links (6·512) times 4 bytes for
the weight.
Rate limiters. As explained in Section 3.3.1, the R2C2
congestion control respects the relative rates dictated by the
routing protocol. Beside reducing the computation over-
DR R2DR R1PR R1PR R2PR A1PR A2PR R1PR R2PR A1PR A2   App 2DR A2   App 1DR A1Incoming Link 1Incoming Link 2Outgoing Link 2Outgoing Link 1558head, this also implies that only one rate limiter per ﬂow
is needed as opposed to one rate limiter per each of the paths
traversed by the ﬂow. Further, ﬂows are only rate-limited
at the source and not at the intermediate nodes. Therefore,
the number of rate limiters needed by each node is equal
to the number of ﬂows that it generates. In our prototype,
we rely on software rate limiters, which can achieve very
ﬁne-grained rate limiting [29]. Furthermore, we believe that
these requirements are well within the reach of today’s NICs,
which typically support 8-128 hardware rate limiters [39].
Packet formats. Figure 6 shows the format used for data
and broadcast packets. Data packets are variable sized, while
broadcast packets have a ﬁxed size (16 bytes). The packet