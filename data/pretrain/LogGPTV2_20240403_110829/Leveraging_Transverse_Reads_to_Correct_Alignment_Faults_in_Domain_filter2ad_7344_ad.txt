As seen in Table I, the probability of a shift fault while
shifting by d domains does not increase linearly with d. Thus,
another way to decrease the probability of faults is to limit the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:37:28 UTC from IEEE Xplore.  Restrictions apply. 
(a) Protection concept
(b) Independent column errors.
(c) Two errors, one column with single error.
(d) Three errors, one column.
Line 
Parity
1 bit
LCL
Log(LCL)+2
E
C
C
E
C
C
Log(n)
E
C
C
1
E
C
C
1
E
C
C
2
E
C
C
1
E
C
C
E
C
C
1
Figure 9: Signature validation and correction concept as a proxy to detect shift misalignments.
Always correctable
May be correctable
(distribution variant)
S2
P(m2=1,m1 ≤1)
S1
Fault
Free 
(S0)
P(1≤m1 ≤3)
1-∑P
S3
P(m3
+=1,m2
-=0)
P(m1≥4)
S4
P(m2
+≥2)+P(m2
+=1,m1
+>1)
S5
Figure 10: Probability State Model
maximal shifting distance. For instance, for a request to shift
by six domains, rather than directly shifting in a single pulse,
it is more reliable to do two shifts by three. Moreover, the
decision of the maximum shift distance with a single pulse is
impacted by its performance relative to other operations of the
device. For example, the maximum shifts with a single pulse
that can occur within the same latency of a read operation is
three [10]. Thus, if the clocking speed of the device is limited
by the read operation, shifts by more than three positions will
require multiple cycles to complete. By limiting the single
operation shift distance to three, we place a lower bound on
Better process
(less shift error)
Deeper scaling 
(more fault)
d
e
d
e
e
n
s
t
i
b
C
C
E
2000
1500
1000
500
0
512
264
128
64
32
# of protected bits
16
8
A. Reliability
Figure 11: Extra Column Bits needed for Different Sub-Division
our reliability. Consequently, we used a limited shift policy,
where for each shift, we do not decrease the performance in
terms of shift cycles while minimizing the fault probability.
For example, a shift by seven requires three cycles, however,
rather than shifting by 3-3-1, a lower probability of fault will
occur if we shift by 3-2-2.
VI. RESULTS
To evaluate the effectiveness of TECC and DECC, we con-
ducted experiments that study the reliability, power consump-
tion, and performance of our methods compared to the leading
approach of HIFI. We also conduct a comparison of the area
overhead of DECC versus HIFI. To conduct these experiments
we used the architecture of a combined last-level and L1
DWM cache as described in FusedCache [10]. FusedCache
uses DWM sub-arrays as both L1 and L2 cache. The domains
aligned with the access point are logically considered to be
L1 and all the other domain are logically L2. Essentially,
L1 misses must access L2 blocks and lead to shifting in the
DWM. Otherwise, FusedCache has a similar organization to
TapeCache [8]. Using the features from a modiﬁed version
of NVSIM [32–34]. We model DECC STT-MRAM storage
as collocated with the DWM subarray, allowing the data
and parity bits to be accessed in parallel and compared and
corrected locally to minimize the communication overheads
needed to enable fault tolerance. Only data invalidations due to
faults and corrected data are reported to the memory controller.
We used the following experimental setup. The memory and
fault model were simulated using Sniper [35]. The architecture
used was an 8-way 4MB L2 cache and a 32KB L1 cache. The
sub-arrays are either composed of 512*32=16384bits (n = 32)
or 512*16=8192bits (n = 16). In order to access these bits
the different latencies are as follows: the data read latency is
0.98ns, write latency is 0.65ns, shift latency is 0.32ns, and tag
access latency is 0.28ns [10]. The CPU has four out-of-order
cores running at a clock speed of 3 GHz. All the benchmarks
used to proﬁle the performances are workloads from SPEC-
CPU2006 [36].
Figure 12 represents the average shift distance per L2 cache
access for different racetrack lengths (n = 16 and n = 32).
383
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:37:28 UTC from IEEE Xplore.  Restrictions apply. 
t
f
i
h
s
e
g
a
r
e
v
A
e
c
n
a
t
s
i
d
13
12
11
10
9
8
7
n=16
n=32
Figure 12: Average Shift Distance in L2
to get
According to this ﬁgure,
the best performance the
system should be allowed to shift by distances greater than
eight. However, in terms of reliability this may not be possible.
Recall that, according to Table I, single pulse shift reliability
scales superlinearly compared to individual shorter shift op-
erations (see Section II-B), which is, in part, why we discuss
limiting the single pulse shift distance. Thus, we report MTTF
in Figure 13 using the limited shift concept as discussed in
Section V-D. For each DECC-N, N is the limit placed on
shift distance for a single pulse. We report the conservative
error metric of detectable, unrecoverable errors (DUE). The
horizontal line represents the minimum 10 year DUE MTTF
goal set by [37].
To meet the DUE MTTF target of 10-years requires a trade-
off between reliability and performance. Hence, the maximal
shifting distance that using column ECC of 64/72 enables in
the DECC approach, with respect to reliability, is three. Using
this conﬁguration, the DUE MTTF is 15 years. Note that if a
larger intrinsic shifts on a single pulse is used, this reliability
drops below the target. Using a shift of four drops the MTTF
to approximately two years, and the MTTF can get as low as
a couple of months for a maximum allowed shift of seven.
Recall also that an intrinsic shift distance of three has the
property of having a similar latency to the read operation,
making it well sized for a clock period in the DWM. Thus,
this shift distance of three is a good tradeoff between reliability
and performance in the system. As a result, we set the limit of
the shift distance from a single pulse in our DECC results to
three unless otherwise stated. Note, p-ECC also uses a similar
limited shift concept (p-ECC-S adaptive) [11], which we also
assume for the remainder of our comparisons.
1.00E+10
10 year
target
1.00E+08
)
s
(
F
T
T
M
E
U
D
1.00E+06
Figure 13: DUE MTTF for various single pulse shift distance limits
384
B. Dimension comparison
To understand the area tradeoffs, we ﬁrst note that having
extra padding domains is necessary to preserve data integrity.
However, additional heads allocated to these domains, as is
required by p-ECC-O, are sources of signiﬁcant area con-
sumption, reducing the density of the racetrack. Making these
domains that store auxiliary information part of the data bits
also with the additional read heads, as in p-ECC, is even less
dense as n≥ 16. In contrast, adding fewer STT-MRAM cells
(i.e., DECC) scales better than adding these extra heads, and
possibly domains, to each racetrack.
We quantify this in Figure 14 using data obtained from [27]
as a reference for the cell area and sensing circuit. In the
ﬁgure, we compare the impact on area per bit using four
different techniques: H0, DECC, p-ECC and p-ECC-O. H0
is included in this ﬁgure to demonstrate the utility of our
scheme in comparison with just storing the signature. As we
can see, our scheme decreases the area overhead by 2.6× and
3.7× for n = 16 and n = 32, respectively, versus p-ECC.
Furthermore, storing the signature directly is still more area
efﬁcient than p-ECC by 1.18× and 1.21× for n = 16 and
n = 32 racetracks, respectively. Thus, if it is necessary to
correct any misalignment error, this is still more area efﬁcient
than p-ECC.
Finally, DECC is even more area efﬁcient than the most area
efﬁcient HIFI approach, p-ECC-O, reducing area by 1.8× and
1.6× for n = 16 and n = 32, respectively. Note that p-ECC-O
has a signiﬁcant degraded performance compared to p-ECC,
which we compare to DECC in the next section.
)
b
/
2
F
(
t
i
b
r
e
p
a
e
r
a
a
r
t
x
E
12
10
8
6
4
2
0
n = 16
n = 32
Signature DECC
p-ECC p-ECC-O Signature DECC
p-ECC p-ECC-O
Figure 14: Area Overhead
C. Performance
To evaluate the impact of DECC on the system performance
we study the effect of transverse reads on access latency as
compared to HIFI. Note, that we can hide the latency of TRs
during read operations by ﬁrst accessing the data post-shift,
and then conducting the TRs. If DECC reports a fault, we
correct and re-read the data. Of course, a write must delay until
DECC is complete and subsequent shift requests may also be
delayed if the TRs from the prior shift have not concluded.
Figure 15 reports the L2 cache access latency and IPC
overhead of DECC compared to no shift fault tolerance for
a variety of SPEC benchmarks. DECC has an average latency
overhead of 3.1%. In contrast, HIFI reports an average latency
overhead for p-ECC of approximately 15%. DECC has an
extremely modest performance degradation of 1.6% due to the
3.1% latency overhead. Given a 15% latency overhead from
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:37:28 UTC from IEEE Xplore.  Restrictions apply. 
HIFI, we can expect a much more signiﬁcant performance
degradation. Thus, DECC provides a better area than HIFI’s
most area efﬁcient approach, p-ECC-O, while providing a
better performance than HIFI’s best performing approach.
d
e
z
i
l
e
c
n