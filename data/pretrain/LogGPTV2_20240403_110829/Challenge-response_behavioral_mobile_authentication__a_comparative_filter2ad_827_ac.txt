tree. Random Forest is robust against noise, efficient, can estimate
the importance of the features and have shown to give promising
results in similar tasks [24, 27].
To avoid the overfitting and improve the classification perfor-
mance and results, we performed exhaustive search to find the
subset of features that results in the best F-measure for each of the
classification tasks.
6 CLASSIFICATION RESULTS
In this section, we present the classification results of our study.
6.1 Intra-Session Analysis
As mentioned in Section 4, we collected data from 20 volunteers. In
the first day of our data collection experiment, each of the volun-
teers completed 30 challenges of each of the three studied schemes.
We divided the collected data into 60 sets based on the users’ identi-
ties (ids) and the scheme. In order to build a classifier to authenticate
a user, we defined two classes. The first class contains the features
data from a given user and a given scheme, and the other class
contains randomly selected features data from other 19 users of
the same scheme. Then, we divided the data into two sets, one for
training and the other for testing. The first 18 instances of each
user and 18 instances of the randomly selected set are used to train
the classifier, while the remaining 12 are used for testing.
The results of intra-session analysis are shown in the first part of
Table 4. Without utilizing the sensors features, we find that the three
scheme provide similar classification results. The F-measure came
out to be 0.89, 0.86 and 0.83 for S-Pattern, CR-Pattern and Gametrics,
respectively. Comparing the F-measures of the three tested schemes
using Friedman test, we did not find statistical significance (F(20, 2)
= 6.7, p = 0.13)2.
The second row of the first part in Table 4 show that including the
sensors features improves the classification accuracy (F-measure =
1 for all three schemes). We employed the Wilcoxon Signed-Ranked
Test (WSRT) with Bonferroni correction to analyze the statistical
significance of the F-measure of each of the schemes with and
without the sensors features. We found statistical significant differ-
ence for all the three schemes with and without sensors features
(p < 0.01).
The three sensors that has been used most by our feature selec-
tion algorithm are Orientation, Linear Acceleration and Rotation
Vector sensors.
6.2 Inter-Session Analysis
The purpose of the inter-session analysis is to analyze the effec-
tiveness of the studied schemes over multiple sessions/days. For
the data instances we collected in day 2 and day 3, we trained the
classifier with the data instance collected in the previous day(s) and
tested with the data of that day.
The results are shown in the second and the third parts of the
Table 4. The results came inline with the results obtained in intra-
session analysis. The three schemes have similar classification ac-
curacies. Also, the results show that utilizing the sensors features
improves the accuracy for all the three studies schemes. We find
that the performance of the classifier degrades slightly compared
to the intra-session analysis, which is as expected.
Comparing the F-measures of the three schemes without sen-
sors features using Friedman test, we did not find any statistical
significant difference. Also, we did not find statistical significant
difference between the F-measures of the three schemes when the
sensor features were included.
Furthermore, for each of the tested schemes, both in day 2 and in
day 3, we compared the F-measure of the classifier that utilize the
sensors features with its correspondent without using the sensors
features using Wilcoxon Signed-Ranked Test (WSRT) with Bon-
ferroni correction. For all of the tested pairs, we found statistical
difference (p < 0.01 for all the tested pairs).
Summary of Results The results obtained in this section show
that utilizing the sensors features improves the accuracy in iden-
tifying the users and rejecting the zero-effort attacker for all the
2All statistical results reported in this paper are at the 95% confidence level
360Table 4: Performance of the classifier for three different schemes. The first part shows the performance of the classifier in
intra-session. Part two and three show the performance for inter-session
Intra-Session
Inter-Session
Day 2
Inter-Session
and Day 3
Excluding
Sensors
Including
Sensors
Excluding
Sensors
Including
Sensors
Execluding
Sensors
Including
Sensors
S-Pattern
CR-Pattern
Gametrics
S-Pattern
CR-Pattern
Gametrics
S-Pattern
CR-Pattern
Gametrics
S-Pattern
CR-Pattern
Gametrics
S-Pattern
CR-Pattern
Gametrics
S-Pattern
CR-Pattern
Gametrics
FPR FNR Precision Recall
0.92
0.15
0.90
0.20
0.90
0.27
0.01
1.00
1.00
0.00
1.00
0.00
0.77
0.20
0.27
0.81
0.85
0.31
0.91
0.06
0.94
0.11
0.12
0.94
0.84
0.22
0.81
0.27
0.24
0.87
0.93
0.07
0.94
0.11
0.07
0.93
0.08
0.10
0.10
0.00
0.00
0.00
0.23
0.19
0.15
0.09
0.06
0.06
0.16
0.19
0.13
0.07
0.06
0.07
0.87
0.83
0.78
0.99
1.00
1.00
0.81
0.75
0.73
0.95
0.90
0.89
0.80
0.75
0.79
0.93
0.90
0.93
F-Measure
0.89
0.86
0.83
1.00
1.00
1.00
0.78
0.77
0.79
0.93
0.92
0.91
0.82
0.77
0.83
0.93
0.92
0.93
three biometric schemes. The results also show that three schemes
have similar classification accuracy. The precision and recall are
up to 1 when we include the sensor data in the analysis in the
intra-sessions study and above 0.89 for all schemes in the inter-
sessions study. Similar results have been reported for other (static)
behavioral biometric schemes in the literature [3].
7 USER EXPERIENCE ANALYSIS
In this section, we present the time taken by the participants to
solve the challenges of each of the studied tasks. Further, we analyze
the user’s experience towards each of the studied tasks using the
standard usability rating questionnaire, i.e. SUS rating.
Table 5: The average (standard deviation) time taken by
the participants to solve a challenge of each of the three
schemes.
Time
Mean (Std.)
0.99 (± 0.32)
6.53 (±2.00)
8.22 (±4.61)
S-Pattern
CR-Pattern
Gametrics
Solving Time: The time that users took to perform each of the
tasks is summarized in Table 5. The users took on average around 1
second to complete a challenge of S-Pattern. The time to solve the
task increased to 6.5 and 8.2 seconds on average for CR-Pattern and
Gametrics, respectively. Note that the time for CR-Pattern is longer
than its correspondent in S-Pattern because in CR-Pattern, we also
considered the time needed to display the random pattern to the
user along with the time taken by the user to complete the task.
Further, since the pattern in CR-Pattern is different each time, the
users took a longer time to input the pattern. Comparing the av-
erage time taken by the participants to solve the challenges using
Friedman Test, we found statistical significant difference (F(1800,
2) = 2745.79, p < 0.001). Further, comparing the solving time with
Wilcoxon Signed Ranks Test, we found statistical significant differ-
ence between all the three pairs (p < 0.001).
Table 6: Mean (standard deviation) SUS Score of the three
studied biometrics
SUS Score
Mean (Std.)
82.63 (±12.29)
S-Pattern
CR-Pattern 80.50 (±12.05)
77.88 (±12.20)
Gametrics
SUS Score: We next evaluate the data collected during the post-
study phase from the participants. The SUS scores of the three
studied schemes are summarized in Table 6. The mean SUS score
came out to be the highest for S-Pattern, and slightly lower for
CR-Pattern and Gametrics.
Although, the mean SUS scores of CR-Pattern and Gametrics are
slightly lower compared to that of S-Pattern, Friedman Test did not
find any statistical differences on the mean of SUS scores among
361these three behavioral biometrics. Given that the system with SUS
score greater than 68 is considered above the average [30], our
results from SUS show that the three schemes are equally usable.
8 SECURITY ANALYSIS
Previously, in Section 6, we demonstrated that the proposed au-
thentication schemes are robust against zero-effort attacks reflected
in the high precision. In this section, we analyze and compare the
security of the proposed schemes against active impersonation
attacks.
8.1 Smudge Attacks
The first attack that we aimed to prevent in our threat model is
the smudge attack. The studied three schemes provide the secu-
rity against such type of attacks by utilizing the features based
on motion-position and touch sensors This is because even if the
attacker is able to trace the pattern, he will not get enough infor-
mation about the behavioral gesture, specifically how to hold the
phone and swipe, while entering the pattern or solving the semantic
challenges. Further, smudge attack relies on the reconstruction of
a secret (i.e., the visual pattern in our case). Since CR-Pattern and
Gametrics do not contain any secrets, these schemes, by design, are
able to prevent the smudge attack.
8.2 Shoulder-Surfing Attacks
The second attack that we aimed to prevent in our threat model
is the shoulder-surfing (or impersonation attack). We analyze the
security of the three schemes against deliberate impersonation
attacks. During our data collection, one of the researchers played the
role of an attacker (representing a relatively well-trained attacker).
He monitored the participants while they were performing the tasks
through a video recording. For the impersonation attack analysis,
the attacker picked two of the participants at random from the
pool, and tried to mimic those chosen participants by solving the
challenges in a similar way as the participants did for each of the
scheme. The impostor made 30 attempts to impersonate each of the
chosen users. Both of the chosen participants were right handed,
and preferred to perform the tasks while sitting on a chair similar
to the attacker.
Table 7: Results of Shoulder-Surfing Attacks
Average FPR
Excluding
Sensors
0.35
0.35
0.22
Including
Sensors
0.12
0
0
S-Pattern
CR-Pattern
Gametrics
Table 7 shows the performance of impersonation attack in terms
of false positive/acceptance rate. The results show that the attack
success rate decreases significantly when including the features
from various sensors under consideration. On average, when sen-
sors features were not included, the success rate of the imperson-
ation attack was 0.35 for both S-Pattern and CR-Pattern, and 0.22
for Gametrics. When sensors features were used, the attack suc-
cess rate decreased significantly to 0.12 for S-Pattern, and 0.00 for
CR-Pattern and Gametrics. Although these results are based on the
impersonation attack against only two users, similar results will
also apply for other users. This suggests that the three scheme offer
high level of resilience against shoulder-surfing attacks, especially
when the sensors features are used.
8.3 Automated Attacks & Internal Attacks
In the rest of our analysis, as a generalization of a robot and a
malware program, we consider the most powerful attack among
them because if a scheme is secure against the most powerful attack,
it will also be secure against a relatively less powerful attack. The
most powerful attack that we consider in our study is an attack
that has the ability to record the touch events as well as other
sensors values when the user interacts with the authentication
construct. Further, we assume that the attacker has the ability to
inject the touch events as well as the motion-position sensors events
at will. Such attack has been explored and implemented in [28],
where the attack takes a form of malicious code, called SMASheD,
that is accidentally installed on the device using ADB (Android
Debugging Bridge) and is therefore granted several permissions
including reading from and writing to the sensors files.
Although SMASheD attack is extremely powerful and its threat
model is very strong, such an attack can be assumed to be a gener-
alization of other types of attacks, including a human impersonator
(a human that can be trained to identically mimic another user), or a
robot such as the one proposed in [32] with no physical constrains
that can be programmed to interact with the mobile device in any
way it likes.
Recording user interactions with the device is also not straight-
forward except for the case of the SMASheD attack. However, mul-
tiple other approaches have been explored including: recording the
user interaction with a malware that looks like a normal authentica-
tion construct (i.e., using social engineering tricks), approximately
learning the user interaction with the device by manually watching
the user [31, 36], recording the user interaction using (surveillance)
camera [4], or hacking the server database to learn the stored au-
thentication token.