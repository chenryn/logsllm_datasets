## PostgreSQL 12 on 阿里云ecs 本地ssd VS essd pl3性能 - 含fio,fsync test,pgbench test，优缺点、云盘PG内核优化建议         
### 作者                                                                              
digoal                                                                                                                       
### 日期                                                                                                                       
2019-12-28                                                                                                                   
### 标签                                                                                                                       
PostgreSQL , fio , 阿里云 , local ssd , essd , PostgreSQL , 并行 , 优缺点 , IO                
----                                                                                                                       
## 背景            
云上存储支持很丰富：本地ssd，ssd，essd， essd pl1, pl2, pl3。对于PG数据库应该选哪个？优缺点是什么？不同的场景如何选择，云盘一定会增加单次io的延迟，是否所有业务都关心io延迟？如何合理规避单次io延迟的问题？      
首先先测试性能，然后分析本地与云ssd的异同。找到数据库合理选择存储的决策点。      
测试环境如下：      
## 本地盘机器      
磁盘      
```      
1.8T \* 8      
标称iops： 每块 240000       
```      
cpu:      
```      
32核64线程，2.5GHz      
```      
mem: 512GB      
```      
# lscpu      
Architecture:          x86_64      
CPU op-mode(s):        32-bit, 64-bit      
Byte Order:            Little Endian      
CPU(s):                64      
On-line CPU(s) list:   0-63      
Thread(s) per core:    2      
Core(s) per socket:    32      
Socket(s):             1      
NUMA node(s):          1      
Vendor ID:             GenuineIntel      
CPU family:            6      
Model:                 85      
Model name:            Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz      
Stepping:              4      
CPU MHz:               2500.006      
BogoMIPS:              5000.01      
Hypervisor vendor:     KVM      
Virtualization type:   full      
L1d cache:             32K      
L1i cache:             32K      
L2 cache:              1024K      
L3 cache:              33792K      
NUMA node0 CPU(s):     0-63      
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 spec_ctrl intel_stibp      
# lsblk      
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT      
vda    253:0    0  100G  0 disk       
└─vda1 253:1    0  100G  0 part /      
vdb    253:16   0  1.8T  0 disk       
vdc    253:32   0  1.8T  0 disk       
vdd    253:48   0  1.8T  0 disk       
vde    253:64   0  1.8T  0 disk       
vdf    253:80   0  1.8T  0 disk       
vdg    253:96   0  1.8T  0 disk       
vdh    253:112  0  1.8T  0 disk       
vdi    253:128  0  1.8T  0 disk       
```      
## essd PL3云盘机器      
磁盘1      
```      
1.8T \* 8       
标称iops： 每块 91800      
```      
磁盘2      
```      
20T \* 1      
标称iops： 每块 1000000      
```      
cpu:      
```      
26核52线程，2.5GHz      
```      
mem: 192GB      
```      
# lscpu      
Architecture:          x86_64      
CPU op-mode(s):        32-bit, 64-bit      
Byte Order:            Little Endian      
CPU(s):                52      
On-line CPU(s) list:   0-51      
Thread(s) per core:    2      
Core(s) per socket:    26      
Socket(s):             1      
NUMA node(s):          1      
Vendor ID:             GenuineIntel      
CPU family:            6      
Model:                 85      
Model name:            Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz      
Stepping:              7      
CPU MHz:               2500.002      
BogoMIPS:              5000.00      
Hypervisor vendor:     KVM      
Virtualization type:   full      
L1d cache:             32K      
L1i cache:             32K      
L2 cache:              1024K      
L3 cache:              36608K      
NUMA node0 CPU(s):     0-51      
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni      
# lsblk      
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT      
vda    253:0    0  100G  0 disk       
└─vda1 253:1    0  100G  0 part /      
vdb    253:16   0  1.8T  0 disk       
vdc    253:32   0  1.8T  0 disk       
vdd    253:48   0  1.8T  0 disk       
vde    253:64   0  1.8T  0 disk       
vdf    253:80   0  1.8T  0 disk       
vdg    253:96   0  1.8T  0 disk       
vdh    253:112  0  1.8T  0 disk       
vdi    253:128  0  1.8T  0 disk       
vdj    253:144  0 19.5T  0 disk       
```      
## 部署机器      
### 内核参数      
```      
vi /etc/sysctl.conf          
# add by digoal.zhou              
fs.aio-max-nr = 1048576              
fs.file-max = 76724600              
# 可选：kernel.core_pattern = /data01/corefiles/core_%e_%u_%t_%s.%p                       
# /data01/corefiles 事先建好，权限777，如果是软链接，对应的目录修改为777              
kernel.sem = 4096 2147483647 2147483646 512000                  
# 信号量, ipcs -l 或 -u 查看，每16个进程一组，每组信号量需要17个信号量。              
kernel.shmall = 107374182                    
# 所有共享内存段相加大小限制 (建议内存的80%)，单位为页。              
kernel.shmmax = 274877906944                 
# 最大单个共享内存段大小 (建议为内存一半), >9.2的版本已大幅降低共享内存的使用，单位为字节。              
kernel.shmmni = 819200                       
# 一共能生成多少共享内存段，每个PG数据库集群至少2个共享内存段              
net.core.netdev_max_backlog = 10000              
net.core.rmem_default = 262144                     
# The default setting of the socket receive buffer in bytes.              
net.core.rmem_max = 4194304                        
# The maximum receive socket buffer size in bytes              
net.core.wmem_default = 262144                     
# The default setting (in bytes) of the socket send buffer.              
net.core.wmem_max = 4194304                        
# The maximum send socket buffer size in bytes.              
net.core.somaxconn = 4096              
net.ipv4.tcp_max_syn_backlog = 4096              
net.ipv4.tcp_keepalive_intvl = 20              
net.ipv4.tcp_keepalive_probes = 3              
net.ipv4.tcp_keepalive_time = 60              
net.ipv4.tcp_mem = 8388608 12582912 16777216              
net.ipv4.tcp_fin_timeout = 5              
net.ipv4.tcp_synack_retries = 2              
net.ipv4.tcp_syncookies = 1                  
# 开启SYN Cookies。当出现SYN等待队列溢出时，启用cookie来处理，可防范少量的SYN攻击              
net.ipv4.tcp_timestamps = 1                  
# 减少time_wait              
net.ipv4.tcp_tw_recycle = 0                  
# 如果=1则开启TCP连接中TIME-WAIT套接字的快速回收，但是NAT环境可能导致连接失败，建议服务端关闭它              
net.ipv4.tcp_tw_reuse = 1                    
# 开启重用。允许将TIME-WAIT套接字重新用于新的TCP连接              
net.ipv4.tcp_max_tw_buckets = 262144              
net.ipv4.tcp_rmem = 8192 87380 16777216              
net.ipv4.tcp_wmem = 8192 65536 16777216              
net.nf_conntrack_max = 1200000              
net.netfilter.nf_conntrack_max = 1200000              
vm.dirty_background_bytes = 409600000                     
#  系统脏页到达这个值，系统后台刷脏页调度进程 pdflush（或其他） 自动将(dirty_expire_centisecs/100）秒前的脏页刷到磁盘              
#  默认为10%，大内存机器建议调整为直接指定多少字节              
vm.dirty_expire_centisecs = 3000                           
#  比这个值老的脏页，将被刷到磁盘。3000表示30秒。              
vm.dirty_ratio = 95                                        
#  如果系统进程刷脏页太慢，使得系统脏页超过内存 95 % 时，则用户进程如果有写磁盘的操作（如fsync, fdatasync等调用），则需要主动把系统脏页刷出。              
#  有效防止用户进程刷脏页，在单机多实例，并且使用CGROUP限制单实例IOPS的情况下非常有效。                
vm.dirty_writeback_centisecs = 100                          
#  pdflush（或其他）后台刷脏页进程的唤醒间隔， 100表示1秒。              
vm.swappiness = 0              
#  不使用交换分区              
vm.mmap_min_addr = 65536              
vm.overcommit_memory = 0                   
#  在分配内存时，允许少量over malloc, 如果设置为 1, 则认为总是有足够的内存，内存较少的测试环境可以使用 1 .                
vm.overcommit_ratio = 90                   
#  当overcommit_memory = 2 时，用于参与计算允许指派的内存大小。              
vm.swappiness = 0                          
#  关闭交换分区              
vm.zone_reclaim_mode = 0                   
# 禁用 numa, 或者在vmlinux中禁止.               
net.ipv4.ip_local_port_range = 40000 65535                  
# 本地自动分配的TCP, UDP端口号范围              
fs.nr_open=20480000              
# 单个进程允许打开的文件句柄上限              
# 以下参数请注意              
vm.extra_free_kbytes = 4096000              
vm.min_free_kbytes = 6291456  # vm.min_free_kbytes 建议每32G内存分配1G vm.min_free_kbytes             
# 如果是小内存机器，以上两个值不建议设置              
# vm.nr_hugepages = 66536                  
#  建议shared buffer设置超过64GB时 使用大页，页大小 /proc/meminfo Hugepagesize              
vm.lowmem_reserve_ratio = 1 1 1              
# 对于内存大于64G时，建议设置，否则建议默认值 256 256 32          
# 生效      
# sysctl -p      
```      
### 配置限制      
```      
vi /etc/security/limits.conf          
# nofile超过1048576的话，一定要先将sysctl的fs.nr_open设置为更大的值，并生效后才能继续设置nofile.              
# 注释其他行 ， 添加如下      
* soft    nofile  1024000              
* hard    nofile  1024000              
* soft    nproc   unlimited              
* hard    nproc   unlimited              
* soft    core    unlimited              
* hard    core    unlimited              
* soft    memlock unlimited              
* hard    memlock unlimited          
```      
同时修改(若有)      
```      
/etc/security/limits.d/20-nproc.conf      
```      
### 关闭透明大页（可选）      
```      
echo never > /sys/kernel/mm/transparent_hugepage/enabled       
```      
配置永久生效      
```      
chmod +x /etc/rc.d/rc.local    
vi /etc/rc.local          
touch /var/lock/subsys/local          
if test -f /sys/kernel/mm/transparent_hugepage/enabled; then              
   echo never > /sys/kernel/mm/transparent_hugepage/enabled              
fi            
```      
### 修改时钟（可选）    
```    
vi /etc/rc.local    
touch /var/lock/subsys/local    
if test -f /sys/kernel/mm/transparent_hugepage/enabled; then    
   echo never > /sys/kernel/mm/transparent_hugepage/enabled    
fi    
echo tsc > /sys/devices/system/clocksource/clocksource0/current_clocksource       
```    
支持的时钟：      
```      
cat /sys/devices/system/clocksource/clocksource0/available_clocksource       
kvm-clock tsc acpi_pm       
```      
修改时钟：      
```      
echo tsc > /sys/devices/system/clocksource/clocksource0/current_clocksource       
```      
时钟性能：      
```      
kvm-clock:       
pg_test_timing       
Testing timing overhead for 3 seconds.      
Per loop time including overhead: 38.05 ns      
Histogram of timing durations:      
  < us   % of total      count      
     1     96.22568   75875976      
     2      3.76667    2970102      
     4      0.00055        435      
     8      0.00704       5550      
    16      0.00005         42      
    32      0.00000          1      
    64      0.00000          1      
   128      0.00000          0      
   256      0.00000          0      
   512      0.00000          0      
  1024      0.00000          1      
tsc:      
pg_test_timing       
Testing timing overhead for 3 seconds.      
Per loop time including overhead: 31.93 ns      
Histogram of timing durations:      
  < us   % of total      count      
     1     96.82838   90964258      
     2      3.16507    2973390      
     4      0.00078        731      
     8      0.00574       5394      
    16      0.00003         26      
    32      0.00000          2      