Joshua Saxe and Konstantin Berlin. 2015. Deep Neural Network Based Malware
Detection using Two Dimensional Binary Program Features. In Intl. Conference
on Malicious and Unwanted So(cid:135)ware (MALWARE). 11–20.
Joshua Saxe and Konstantin Berlin. 2017. eXpose: A character-level convolutional
neural network with embeddings for detecting malicious URLs, (cid:128)le paths and
registry keys. arXiv preprint arXiv:1702.08568 (2017).
[66] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Fre-
itas. 2016. Taking the Human Out of the Loop: A Review of Bayesian Optimiza-
tion. Proc. IEEE 104, 1 (2016), 148–175.
[67] Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Net-
works for Large-scale Image Recognition. arXiv preprint arXiv:1409.1556 (2014).
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical Bayesian
Optimization of Machine Learning Algorithms. In Advances in Neural information
Processing Systems. 2951–2959.
Jure Sokoli´c, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. 2017.
Robust large margin deep neural networks. IEEE Transactions on Signal Processing
65, 16 (2017), 4265–4280.
[70] Yi Sun, Xiaogang Wang, and Xiaoou Tang. 2013. Deep convolutional network cas-
cade for facial point detection. In Proceedings of the IEEE conference on computer
vision and pa(cid:136)ern recognition. 3476–3483.
[71] Christian Szegedy, Sergey Io(cid:130)e, Vincent Vanhoucke, and Alexander A Alemi.
2017. Inception-v4, Inception-Resnet and the Impact of Residual Connections on
Learning. In AAAI, Vol. 4. 12.
[72] Christian Szegedy, Vincent Vanhoucke, Sergey Io(cid:130)e, Jon Shlens, and Zbigniew
Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In
Conference on Computer Vision and Pa(cid:136)ern Recognition. 2818–2826.
[73] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing Properties of Neural
Networks. arXiv preprint arXiv:1312.6199 (2013).
[74] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Auto-
mated testing of deep-neural-network-driven autonomous cars. In Proceedings
of the 40th international conference on so(cid:135)ware engineering. ACM, 303–314.
[68]
[69]
[75] Florian Tramr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
and Patrick McDaniel. 2018. Ensemble Adversarial Training: A(cid:138)acks and De-
fenses. In International Conference on Learning Representations.
[76] Bichen Wu, Forrest N Iandola, Peter H Jin, and Kurt Keutzer. 2017. SqueezeDet:
Uni(cid:128)ed, Small, Low Power Fully Convolutional Neural Networks for Real-Time
Object Detection for Autonomous Driving.. In CVPR Workshops. 446–454.
[77] Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song.
2018. Generating adversarial examples with adversarial networks. arXiv preprint
arXiv:1801.02610 (2018).
Jason Yosinski, Je(cid:130) Clune, Yoshua Bengio, and Hod Lipson. 2014. How transfer-
able are features in deep neural networks?. In Advances in neural information
processing systems. 3320–3328.
[79] Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang, Xiang
Gan, and Yong Yang. 2018. Transferable Adversarial Perturbations. In Computer
Vision–ECCV 2018. Springer, 471–486.
[78]
A EXAMPLES OF PERTURBATIONS
Figs. 5 and 6 show examples of perturbations from procedural noise
functions and existing white-box UAP a(cid:138)acks. Notice the visual
similarities in structure between the noise pa(cid:138)erns in Figs. 5 and 6.
Figure 5: Procedural noise patterns with (top) Gabor noise
and (bottom) Perlin noise, both with decreasing frequency
from le(cid:133) to right.
Figure 6: UAPs generated for VGG-19 using the white-box
Singular Vector Attack [30].
B TOP 5 INPUT-SPECIFIC EVASION
In Table 5, we consider the top 5 input-speci(cid:128)c evasion rate where
the true class label is outside the top 5 class labels in the perturbed
image x +s. (cid:140)e top 5 error is o(cid:137)en used alongside the top 1 error to
measure the performance of classi(cid:128)ers on ImageNet. Top 5 evasion
is more di(cid:129)cult than top 1 as the con(cid:128)dence in the true label has to
be degraded su(cid:129)ciently enough for it to be below (cid:128)ve other class
labels. Despite that, Perlin noise is able to achieve top 5 evasion
on more than half the inputs for all tested models. We see that our
procedural noise still remains a strong a(cid:138)ack for the top 5 error
metric.
14
Table 5: Top 5 input-speci(cid:128)c evasion rate (in %) for random
and procedural noise perturbations. Original refers to the
top 5 error on the unaltered original images. (cid:135)e strongest
attack on each classi(cid:128)er is highlighted.
Classi(cid:128)er
VGG-19
ResNet-50
Inception v3
IRv2
IRv2ens
Original
Random
9.5
8.4
6.3
4.5
5.1
28.0
29.5
18.0
13.2
12.1
Gabor
90.2
82.5
66.9
56.3
44.1
Perlin
92.6
85.3
79.5
66.4
53.6
C CORRELATION MATRICES
Procedural noise perturbations had their parameters chosen uni-
formly at random and with constraint (cid:96)∞-norm ε = 16. For Sect. 4,
Figs. 7 and 8 show the correlation matrices between the proce-
dural noise parameters and the corresponding universal evasion
rates on the various ImageNet classi(cid:128)ers. (cid:140)ese correlations quan-
tify the e(cid:130)ects of each parameter and the transferability of UAPs
across models. (cid:140)e labels “V19”, “R50”, and “INv3” refer to VGG-19,
ResNet-50, and Inception v3 respectively.
D BLACK-BOX ATTACK RESULTS
For Sect. 5, full results for input-speci(cid:128)c and universal black-box
a(cid:138)acks against Inception v3 are in Tables 6 and 7. We include results
for Gabor noise (gab) and Perlin noise (per). (cid:140)e bandit a(cid:138)acks [25]
include the use of time (T) and time with data (TD) priors. Methods
are separated according to the base a(cid:138)ack that was used: Gabor
noise, Perlin noise, and bandits framework.
Table 6: Results for input-speci(cid:128)c black-box attacks on In-
ception v3 with (cid:96)∞-norm ε = 16. Gabor and Perlin noise
attacks are labeled with “gab” and “per” respectively. Best
success rate per method is highlighted.
A(cid:138)ack
BayesOptgab
BayesOptgab
L-BFGSgab
L-BFGSgab
Randomgab
BayesOptper
BayesOptper
L-BFGSper
L-BFGSper
Randomper
BanditsT
BanditsT
BanditsT
BanditsTD
BanditsTD
BanditsTD
(cid:139)ery Limit
Average (cid:139)eries
Success Rate (%)
100
1,000
100
1,000
1,000
100
1,000
100
1,000
1,000
100
1,000
10,000
100
1,000
10,000
9.8
10.9
6.0
6.0
62.2
7.0
8.4
19.8
70.1
36.3
31.3
330
1,795
29.0
224
888
83.1
83.6
44.7
44.7
86.1
91.6
92.8
71.7
86.5
91.6
14.3
47.6
88.3
36.7
73.6
96.9
Figure 7: Correlations between Gabor noise parameters
(δgab = {σ , ω, λ, ξ}) and universal evasion rates on each im-
age classi(cid:128)er.
Figure 8: Correlations between Perlin noise parameters
(δper = {λx , λy , ϕsine, ω}) and universal evasion rates on each
image classi(cid:128)er.
15
Table 7: Results for universal black-box attacks on Inception v3 with (cid:96)∞-norm ε = 16. Gabor and Perlin noise attacks are
labeled with “gab” and “per” respectively. Universal evasion rates (%) of the optimized perturbations are shown for their
respective training set and the validation set.
Train Size
50
125
250
500
BayesOptgab
Val.
Train
57.6
64.0
58.0
64.6
60.0
58.4
62.4
64.8
L-BFGSgab
Train
58.0
58.4
58.8
59.8
Val.
51.6
54.6
56.0
58.0
BayesOptper
Val.
Train
71.4
78.0
70.2
77.6
71.6
71.2
72.9
75.0
L-BFGSper
Train
74.0
76.0
71.2
73.4
Val.
69.9
71.5
69.7
70.8
E ADVERSARIAL EXAMPLES
Figs. 9 and 10 contain some samples of adversarial examples from
our procedural noise a(cid:138)acks against image classi(cid:128)cation and object
detection tasks.
Figure 9: Gabor noise adversarial examples on (top) Incep-
tion v3, (middle) VGG-19, and (bottom) ResNet-50.
Figure 10: Perlin noise adversarial examples on YOLO v3.
16