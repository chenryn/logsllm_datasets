### Tolerance Requirements and Scheduling

The tolerance requirements, denoted as \( FT(G_i) \), for individual graphs dictate the number of processors needed to ensure feasible task schedules. Typically, this number exceeds \( k_{\text{min}} \). The graphs are scheduled individually using the LIST algorithm, with those requiring tighter diagnosis latencies being given higher priority and scheduled first.

### Diagnosis Deadline Estimation

The test-scheduling method described in Section 3 assumes prior knowledge of the diagnosis latency \( t_d \) for an actuator \( A_i \). We now outline a general procedure to derive this latency from system-level safety requirements.

1. **Identification of Serious Actuator Failures**: 
   - Use methods such as Failure Mode and Effect Analysis (FMEA) [4] to identify serious actuator failures.
   
2. **Quantification of Impact**:
   - Simulate the impact of these failures on system behavior over time to provide a basis for estimating \( t_d \).

#### Example: Electric Power-Steering

An unwanted steering failure causes the vehicle to deviate from its intended path without corresponding steering input from the driver. The severity of the failure is determined by the magnitude of this deviation. The maximum allowable deviation is set according to existing transportation safety rules, considering specific driving conditions and vehicle and lane characteristics [16].

- **Simulation of Actuator Fault**:
  - Simulate the actuator fault by commanding the output to the maximum value allowed by its design.
  - The time \( t_{\text{unsafe}} \) taken for the car to exceed the allowable deviation provides the high-level timing requirement for diagnosis and forms the basis for estimating \( t_d \).

### Timeline of Events

Figure 8 illustrates the sequence of events leading to an unsafe vehicle condition. Let \( a_i(k), a_i(k+1), \ldots \) be periodic commands issued by actuator \( A_i \). Assume \( A_i \) fails at some random point during the interval \([a_i(k), a_i(k+1)]\). System processors diagnose this fault by comparing the actual and estimated responses of \( A_i \) to the command \( a_i(k+1) \). The initial diagnosis time is denoted by \( t_d \).

- **Evaluation Period**:
  - After the initial diagnosis, the suspect actuator is typically evaluated for a period \( t_{\text{eval}} \) to improve confidence in the final decision [13].
  - The pipelined nature of diagnosis ensures fault detection every \( \phi \) time units following the initial diagnosis.
  - Each processor maintains an error counter, which is incremented after each positive diagnosis. The actuator is deemed faulty once the error count reaches a threshold, used to distinguish between permanent and transient failures [23] [24].
  - The fault detection latency is bounded above by \( t_{\text{unsafe}} + t_{\text{eval}} + \frac{t_r}{\phi} \), where \( t_r \) is the recovery time, such as shutting down a faulty actuator.

### Experimental Results

We present simulation results evaluating the performance of the test-scheduling algorithm for task graphs under various control-delay and diagnosis constraints. The experiments assume a small-scale embedded system with up to four graphs, where both actuator control and subsequent diagnosis are realized in triple-modular fashion, similar to real-world applications like SBW [17].

- **Comparison with Other Algorithms**:
  - The scheduling algorithms in [11] [12] use a different task model, not considering task preemption and communication delays. Therefore, our method's performance cannot be directly compared to these algorithms.

#### Evaluation Metrics

- **Cost Overhead**:
  - Represents the additional number of processors added by the proposed algorithm over the theoretical lower bound \( k_{\text{min}} \).
  - If \( k \) is the actual number of processors required, the overhead is given by:
    \[
    \text{Overhead} = \left( \frac{k - k_{\text{min}}}{k_{\text{min}}} \right) \times 100
    \]

- **Average Processor Utilization**:
  - Since tasks are executed on \( P_i \) within a frame \( F_i \) of duration \( \phi \), the individual processor utilization is:
    \[
    U_{P_i} = \sum_{c_j \in T_j \cap F_i} \left( \frac{c_j}{\phi} \right) \times 100
    \]
  - The average utilization of processors in the system is:
    \[
    \text{Average Utilization} = \frac{\sum_{k} U_{P_k}}{k}
    \]

### Design Space Exploration

- **Execution Time of Tasks**:
  - The execution time of task \( T_i \) is a random variable uniformly distributed in the range \([c_{\text{min}}, c_{\text{max}}]\). Figure 9 shows the assumed values for various control and diagnostic tasks.
  - Communication cost \( t_c \) is 1000 µs, and the sampling delay \( t_s \) for monitoring sensors is uniformly distributed between 500 and 1000 µs.

- **Graph Period**:
  - Assume a system where every graph has a period \( \phi \). The best-case period for the graph is given by the execution time of its actuator-control portion. The maximum of the best-case periods of all graphs provides the overall period of the system.
  - The laxity of the obtained system period can be varied as:
    \[
    \phi + \left( \frac{1}{\text{slack}} \times \phi \right)
    \]
    where slack is the laxity factor.

- **Diagnosis Deadlines**:
  - The diagnosis latency for each graph varies with the actuator \( A_i \) under control. The minimum time taken to diagnose a fault after the corresponding actuation command is given by the longest path \( p_l \) through the graph from the actuation task to an exit task.
  - The diagnosis deadline of each graph is:
    \[
    p_l \times \left( 1 + \text{slack} \right)
    \]
  - Laxities are categorized as low if \( 0 \leq \text{slack} \leq 0.2 \), medium if \( 0.3 \leq \text{slack} \leq 0.6 \), and high if \( 0.7 \leq \text{slack} \leq 1.0 \).

### Performance Summary

Figure 10 summarizes the performance of the LIST algorithm over the full range of design-constraint combinations. When both the period and deadlines are tight, additional processors are needed to generate a feasible schedule, resulting in moderate utilization. A shorter system period implies tighter control-task deadlines, further compounded by diagnostic tasks with tight deadlines that must also be pipelined. The algorithm resolves the resulting contention by adding more processors.

- **Relaxed Graph Periods and Tight Diagnostic Deadlines**:
  - Although control tasks have greater scheduling flexibility, additional processors may still be needed to satisfy the deadline constraints of the pipelined diagnostic tasks.
  - Best-case scenarios occur when both periods and diagnosis deadlines are relaxed.

### Conclusions

Faulty hardware components, such as actuators, must be identified and shut down before the system becomes unsafe. This paper presents a fault diagnosis method for low-cost distributed embedded systems, with specific contributions including:

- **Augmentation of Embedded Applications**:
  - A method to augment an embedded application with corresponding diagnostic tasks.
  
- **Test Scheduling Solution**:
  - A solution to the test scheduling problem under resource and deadline constraints.

Faulty actuators are identified using a two-phase diagnosis approach involving multiple processors. The diagnostic tests are implemented in software using analytical redundancy to describe actuator behavior and share processors, executing concurrently with application tasks.

- **Performance Characterization**:
  - Simulation results characterize the performance of the scheduling algorithm under various design constraints, providing useful information about the expected cost overhead and processor utilization.

Future work will focus on solving the test-scheduling problem for multi-rate systems.

### References

[1] S. Amberkar et al., “Diagnostic Development for an Electric Power Steering System,” SAE World Congress, Detroit, Paper: 2000-01-0819, 2000.
[2] M. Barborak, M. Malek, and A. Dahbura, “The Consensus Problem in Fault-Tolerant Computing,” ACM Computing Surveys, vol. 25, no. 2, pp. 171-219, June 1993.
[3] J. J. Gertler, Fault Detection and Diagnosis in Engineering Systems, Marcel Dekker, New York, 1998.
[4] H. Kopetz, Real-Time Systems: Design Principles for Distributed Embedded Applications, Kluwer Academic Publishers, Boston, 1997.
[5] H. Kopetz and G. Gruensteidl, “TTP - A Time-Triggered Protocol for Fault-Tolerant Real-Time Systems,” Proc. IEEE Fault-Tolerant Comput. Symp., pp. 524-532, 1993.
[6] R. Isermann, R. Schwarz, and S. Stolzl, “Fault-Tolerant Drive-by-Wire Systems - Concepts and Realizations,” Proc. IFAC Symp.(SAFEPROCESS), 2000.
[7] P. R. Lorczak, A. K. Caglayan, and D. E. Eckhardt, “A Theoretical Investigation of Generalized Voters for Redundant Systems,” Proc. IEEE Fault-Tolerant Comput. Symp., pp. 444-451, 1989.
[8] A. Dahbura, K. Sabnani, and W. Henry, “Spare Capacity as a Means of Fault Detection and Diagnosis in Multiprocessor Systems,” IEEE Trans. Computers, vol. 38, no. 6, pp. 881-891, 1989.
[9] S. Tridandapani, A. K. Somani, and U. R. Sandadi, “Low Overhead Multiprocessor Allocation Strategies Exploiting System Spare Capacity for Fault Detection and Location,” IEEE Trans. Computers, vol. 44, no. 7, pp. 865-877, 1995.
[10] C. J. Walter, P. Lincoln, and N. Suri, “Formally Verified On-line Diagnosis,” IEEE Trans. Software Eng., vol. 23, no. 11, pp. 684-721, 1997.
[11] G. Goossens et al., “An Efficient Microcode Compiler for Application Specific DSP Processors,” IEEE Trans. Comput.-Aided Design, vol. 9, no. 9, pp. 925-937, 1990.
[12] S. M. Heemstra de Groot, S. H. Gerez, and O. E. Hermann, “Range-Chart-Guided Iterative Data-Flow Graph Scheduling,” IEEE Trans. Circuits & Systems, vol. 39, no. 5, pp. 351-364, 1992.
[13] E. Ding, H. Fennel, and S. X. Ding, “Model-Based Diagnosis of Sensor Faults for ESP Systems,” Proc. IFAC Symposium, 2000.
[14] V. H. Allan et al., “Software Pipelining,” ACM Computing Surveys, vol. 27, no. 3, pp. 367-432, Sep. 1995.
[15] J. Berwanger et al., “FlexRay - The Communication System for Advanced Automotive Control Systems,” Proc. SAE World Congress, Paper: 2001-01-0676, 2001.
[16] American Association of State Highway and Transportation Officials, “A Policy on Geometric Design of Highways and Streets,” 1994. (http://www.aashto.org)
[17] B. P. Douglass, “Safety-Critical Embedded Systems,” Embedded Systems Programming, pp. 76-92, Oct. 1999.
[18] J.C.Y. Yang and D. W. Clarke, “The Self-Validating Actuator,” Control Eng. Practice, vol. 7, no. 3, pp. 249-260, 1999.
[19] M. Saksena and S. Hong, “An Engineering Approach to Decomposing End-to-End Delays on a Distributed Real-Time System,” Proc. IEEE Workshop on Parallel and Distributed Real-Time Systems, pp. 244-251, Apr. 1996.
[20] B. A. Schroeder, K. Schwan, and S. Aggarwal, “Software Approach to Hazard Detection Using On-line Analysis of Safety Constraints,” Proc. IEEE Symp. Reliable Dist. Systems, pp. 80-87, Oct. 1997.
[21] F. Jahanian, R. Rajkumar, and S. Raju, “Run-Time Monitoring of Timing Constraints in Distributed Real-Time Systems,” Proc. Real-Time Systems Symp., pp. 247-273, 1994.
[22] H. El-Rewini, T. G. Lewis, and H. H. Ali, Task Scheduling in Parallel and Distributed Systems, Prentice Hall, Englewood Cliffs, NJ, 1994.
[23] J. Sosnowski, “Transient Fault Tolerance in Digital Systems,” IEEE Micro, vol. 14, no. 1, pp. 24-35, 1994.
[24] A. Bondavalli et al., “Threshold-Based Mechanisms to Discriminate Transient from Intermittent Faults,” IEEE Trans. Computers, vol. 49, no. 3, pp. 230-245, 2000.