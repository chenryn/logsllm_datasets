tolerance requirements FT(Gi) of
individual graphs. The number of processors needed to
guarantee feasible task schedules is typically greater than
kmin. The graphs are scheduled individually using LIST.
Those with tighter diagnosis latencies are given higher
priority and scheduled ﬁrst.
4 Diagnosis Deadline Estimation
The test-scheduling method described in Section 3
assumes a priori knowledge of the diagnosis latency td for
an actuator Ai. We now outline a general procedure to
obtain this latency from system-level safety requirements.
First, serious actuator failures are identified by an appro-
priate method such as failure mode and effect analysis [4].
Their impact on system behavior in the time domain is
then quantiﬁed via simulation to provide the basis for esti-
mating td. We use the specific example of electric power-
steering [1] to further illustrate the process.
An unwanted steer failure causes the car to deviate from
its intended path without the corresponding steering input
from the driver. The magnitude of this path deviation deter-
mines the failure severity at the vehicle level. The maxi-
mum allowable deviation is determined using existing
transportation safety rules under specific driving condi-
tions and vehicle and lane characteristics [16]. The actua-
tor fault is then simulated by commanding the output to the
maximum value allowed by its design. The time tunsafe
taken by the car to exceed the allowable deviation provides
the high-level timing requirement for diagnosis and forms
the basis for estimating td.
The timeline in Fig. 8 shows the sequence of events
leading to an unsafe vehicle condition. Let ai(k), ai(k +
1),.... be periodic commands issued by actuator Ai. Assume
Ai fails at some random point during the time interval
[ai(k), ai(k + 1)]. System processors diagnose this fault by
Task
type
Sensing
Actuation
Command computation
and actuator diagnosis
Voting
Buffering
cmin
(µs)
100
250
350
200
50
cmax
(µs)
200
300
700
250
75
Figure 9. Execution times of the various control
and diagnostic tasks
comparing the actual and estimated responses of Ai to the
command ai(k + 1). The time taken to perform the initial
diagnosis is denoted by td. Also, a suspect actuator is typi-
cally evaluated for some period teval after the initial diag-
nosis to improve confidence in the final decision [13]. The
pipelined nature of diagnosis guarantees fault detection
every φ time units following the initial diagnosis. Each
processor maintains an error counter which is incremented
after each positive diagnosis. The actuator is considered
faulty once the error count reaches the threshold
. Such thresholding methods are typically used
to distinguish between permanent and transient failures
[23] [24]. Therefore, the fault detection latency is bounded
above by
, where tr denotes
the recovery time such as shutting down a faulty actuator.
5 Experimental Results
tunsafe
teval
tr–
–
φ⁄
φ×
φ⁄
≤
td
teval
We present simulation results evaluating the perfor-
mance of the test-scheduling algorithm for task graphs
under various control-delay and diagnosis constraints. The
experiments assume a small-scale embedded system with
up to four graphs where both actuator control and the sub-
sequent diagnosis are realized in triple-modular fashion
fashion. The generated graphs closely resemble real-world
applications of interest such as SBW [17].
The scheduling algorithms in [11] [12] assume a differ-
ent task model from the one used in this paper. For exam-
ple, task preemption and communication delays are not
considered. Therefore, the performance of our method can-
not be directly compared to the above algorithms. We use
the following general evaluation metrics:
Cost overhead: The cost overhead represents the addi-
tional number of processors added by the proposed
algorithm over the theoretical lower bound. Therefore,
if kmin denotes the lower bound and k the actual num-
ber of processors required, the overhead is given by:
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:42 UTC from IEEE Xplore.  Restrictions apply. 
Deadline laxity
Period laxity
kmin
 Slack (0.0 - 0.2)
Slack (0.3 - 0.6)
Slack (0.7 - 1.0)
6
4
3
Slack (0.0 - 0.2)
Slack (0.3 - 0.6)
Slack (0.7 - 1.0)
Overhead
Utilization
Overhead
Utilization
Overhead
Utilization
(%)
95.1
57.5
35.1
(%)
43.2
50.6
62.6
(%)
83.3
47.5
18.9
(%)
46.7
53.9
74.0
(%)
76.5
41.7
7.0
(%)
50.1
60.2
80.3
Figure 10. Performance results showing processor overhead and utilization under various period and
deadline laxity combinations
–
kmin
k
--------------------
kmin
×
100
Average processor utilization: Since tasks are executed
on Pi within a frame Fi with duration φ, the individual
processor utilization is given by:
=
UPi
∑
c j
T j Fi
∈
φ⁄
×
100
Therefore, the average utilization of processors in the
system is:
∑
k
UPk
k⁄
it
Clearly,
is desirable to generate embedded systems
having low overhead and high processor utilization. We
report on experiments exploring the design space for such
a desirable performance.
,
[
=
ci
cmin cmax
The execution time of task Ti is a random variable,
uniformly distributed in the range
. Fig-
ure 9 shows the cmin and cmax assumed for the various con-
trol and diagnostic tasks. The communication cost tc is
assumed to be 1000 µs and the sampling delay ts for the
monitoring sensors is uniformly distributed between [500,
1000] µs. The other graph constraints are generated as fol-
lows.
]
Graph period: We assume a system where every graph
has period φ. Recall from Section 2 that the best-case
period for the graph is given by the execution time of
its actuator-control portion. The maximum of the best-
case periods of all graphs provides the overall period
of the system. The laxity of the obtained system
period can be varied as
where slack is
the laxity factor.
Diagnosis deadlines: The diagnosis latency for each
graph varies with the actuator Ai under control. The
minimum time taken to diagnose a fault after the cor-
responding actuation command is given by the longest
+(
1
slack
×
φ
)
path pl through the graph from the actuation task to an
exit task. Therefore, the diagnosis deadline of each
+(
graph is simply
, where slack varies
1
with the deadline laxity.
slack
pl
×
)
≤
0
0.7
≤
slack
≤
≤
0.6
0.3
slack
slack
≤
,
The laxities are categorized as low if
0.2
≤
medium if
.
, and high if
1
The overall design space is partitioned into sub-spaces
corresponding to different period and deadline laxities and
the algorithm performance are evaluated under each. For
each design sub-space such as (low period laxity, low
deadline laxity), (low period laxity, medium deadline
laxity),..., about 100 task graphs are created and scheduled.
The processor overhead and utilization are then averaged.
Figure 10 summarizes the performance of LIST
over the full range of design-constraint combinations. The
lower bound kmin corresponding to the desired graph
period is also shown. When both the period and deadlines
are tight (shaded cells in column 3), additional processors
are needed to generate the feasible schedule and the utili-
zation is only moderate. A shorter system period implies
tighter control-task deadlines. The lack of scheduling ﬂexi-
bility is further compounded by diagnostic tasks with tight
deadlines that must also be pipelined. The algorithm
resolves the resulting contention by adding more proces-
sors. When the graph period is fixed, the cost overhead
decreases with increasing diagnostic-deadline latency
since the diagnostic tasks can now be scheduled with
greater ﬂexibility.
Consider the scenario where the graph periods are
relaxed but the diagnostic deadlines are tight. Though con-
trol tasks have greater scheduling flexibility, additional
processors may still be needed to satisfy deadline con-
straints of the pipelined diagnostic tasks. The shaded cells
in column 5 represent best-case scenarios where both peri-
ods and diagnosis deadlines are relaxed.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:42 UTC from IEEE Xplore.  Restrictions apply. 
6 Conclusions
Faulty hardware components such as actuators must
be identified and shut down before the system becomes
unsafe. To address this problem, we have presented a fault
diagnosis method for low-cost distributed embedded sys-
tems. The speciﬁc contributions of this paper are:
• A method to augment an embedded application with
the corresponding diagnostic tasks
• A solution to the test scheduling problem under
resource and deadline constraints
Faulty actuators are identiﬁed using a two-phase diagnosis
approach involving multiple processors. The diagnostic
tests are implemented in software using analytical
redundancy to describe actuator behavior. Furthermore,
these tests share processors and execute concurrently with
application tasks.
We have also solved the test scheduling problem guar-
anteeing actuator diagnosis within designer-speciﬁed dead-
lines while meeting control performance goals. To meet
these dual objectives, application and diagnostic tasks are
appropriately pipelined via static scheduling. As a second-
ary objective, the number of required processors is mini-
mized. We have presented simulation results characterizing
the performance of the scheduling algorithm under various
design constraints. Given a task workload with certain
period and diagnosis-latency requirements, these results
provide useful information about the expected cost over-
head and processor utilization. As future work, we plan to
solve the test-scheduling problem for multi-rate systems.
7 References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
S. Amberkar et al., “Diagnostic Development for an Elec-
tric Power Steering System,” SAE World Congress,
Detroit, Paper: 2000-01-0819, 2000.
M. Barborak, M. Malek, and A. Dahbura, “The Consen-
sus Problem in Fault-Tolerant Computing,” ACM Com-
puting Surveys, vol. 25, no. 2, pp. 171-219, June 1993.
J. J. Gertler, Fault Detection and Diagnosis in Engineer-
ing Systems, Marcel Dekker, New York, 1998.
H. Kopetz, Real-Time Systems: Design Principles for Dis-
tributed Embedded Applications, Kluwer Academic Pub-
lishers, Boston, 1997.
H. Kopetz and G. Gruensteidl, “TTP - A Time-Triggered
Protocol for Fault-Tolerant Real-Time Systems,” Proc.
IEEE Fault-Tolerant Comput. Symp., pp. 524-532, 1993.
R. Isermann, R. Schwarz, and S. Stolzl, “Fault-Tolerant
Drive-by-Wire Systems - Concepts and Realizations,”
Proc. IFAC Symp.(SAFEPROCESS), 2000.
P. R. Lorczak, A. K. Caglayan, and D. E. Eckhardt, “A
Theoretical
for
Investigation of Generalized Voters
[8]
[9]
[10]
Redundant Systems,” Proc. IEEE Fault-Tolerant Comput.
Symp., pp. 444-451, 1989.
A. Dahbura, K. Sabnani, and W. Henry, “Spare Capacity
as a Means of Fault Detection and Diagnosis in Multipro-
cessor Systems,” IEEE Trans. Computers, vol. 38, no. 6,
pp. 881-891, 1989.
S. Tridandapani, A. K. Somani, and U. R. Sandadi, “Low
Overhead Multiprocessor Allocation Strategies Exploit-
ing System Spare Capacity for Fault Detection and Loca-
tion,” IEEE Trans. Computers, vol. 44, no. 7, pp. 865-877,
1995.
C. J. Walter, P. Lincoln, and N. Suri, “Formally Veriﬁed
On-line Diagnosis,” IEEE Trans. Software Eng., vol. 23,
no. 11, pp. 684-721, 1997.
[12]
[11] G. Goossens et al., “An Efﬁcient Microcode Compiler for
Application Speciﬁc DSP Processors,” IEEE Trans. Com-
put.-Aided Design, vol. 9, no. 9, pp. 925-937, 1990.
S. M. Heemstra de Groot, S. H. Gerez, and O. E. Her-
rmann, “Range-Chart-Guided Iterative Data-Flow Graph
Scheduling,” IEEE Trans. Circuits & Systems, vol. 39, no.
5, pp. 351-364, 1992.
E. Ding, H. Fennel, and S. X. Ding, “Model-Based Diag-
nosis of Sensor Faults for ESP Systems,” Proc. IFAC Sym-
posium, 2000.
[13]
[14] V. H. Allan et al., “Software Pipelining,” ACM Computing
[15]
Surveys, vol. 27, no. 3, pp. 367-432, Sep. 1995.
J. Berwanger et al., “FlexRay - The Communication Sys-
tem for Advanced Automotive Control Systems,” Proc.
SAE World Congress, Paper: 2001-01-0676, 2001.
[17]
[16] American Association of State Highway and Transporta-
tion Ofﬁcials, “A Policy on Geometric Design of High-
ways and Streets,” 1994. (http:// www.aashto.org)
B. P. Douglass, “Safety-Critical Embedded Systems,”
Embedded Systems Programming, pp. 76-92, Oct. 1999.
J.C.Y. Yang and D. W. Clarke, “The Self-Validating Actu-
ator,” Control Eng. Practice, vol. 7, no. 3, pp. 249-260,
1999.
[18]
[20]
[19] M. Saksena and S. Hong, “An Engineering Approach to
Decomposing End-to-End Delays on a Distributed Real-
Time System,” Proc. IEEE Workshop on Parallel and Dis-
tributed Real-Time Systems, pp. 244-251, Apr. 1996.
B. A. Schroeder, K. Schwan, and S. Aggarwal, “Software
Approach to Hazard Detection Using On-line Analysis of
Safety Constraints,” Proc. IEEE Symp. Reliable Dist. Sys-
tems, pp. 80-87, Oct. 1997.
F. Jahanian, R. Rajkumar, and S. Raju, “Run-Time Moni-
toring of Timing Constraints in Distributed Real-Time
Systems,” Proc. Real-Time Systems Symp., pp. 247-273,
1994.
[21]
[22] H. El-Rewini, T. G. Lewis, and H. H. Ali, Task Scheduling
in Parallel and Distributed Systems, Prentice Hall, Engle-
wood Cliffs, NJ, 1994.
J. Sosnowski, “Transient Fault Tolerance in Digital Sys-
tems,” IEEE Micro, vol. 14, no. 1, pp. 24-35, 1994.
[23]
[24] A. Bondavalli et al., “Threshold-Based Mechanisms to
Discriminate Transient from Intermittent Faults,” IEEE
Trans. Computers, vol. 49, no. 3, pp. 230-245, 2000.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:42 UTC from IEEE Xplore.  Restrictions apply.