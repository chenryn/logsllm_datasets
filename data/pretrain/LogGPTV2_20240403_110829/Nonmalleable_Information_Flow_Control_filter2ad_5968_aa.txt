title:Nonmalleable Information Flow Control
author:Ethan Cecchetti and
Andrew C. Myers and
Owen Arden
Nonmalleable Information Flow Control
Department of Computer Science
Department of Computer Science
Ethan Cecchetti
Cornell University
PI:EMAIL
Andrew C. Myers
Cornell University
PI:EMAIL
Owen Arden
Department of Computer Science
University of California, Santa Cruz∗
PI:EMAIL
ABSTRACT
Noninterference is a popular semantic security condition because it
offers strong end-to-end guarantees, it is inherently compositional,
and it can be enforced using a simple security type system. Un-
fortunately, it is too restrictive for real systems. Mechanisms for
downgrading information are needed to capture real-world security
requirements, but downgrading eliminates the strong compositional
security guarantees of noninterference.
We introduce nonmalleable information flow, a new formal se-
curity condition that generalizes noninterference to permit con-
trolled downgrading of both confidentiality and integrity. While
previous work on robust declassification prevents adversaries from
exploiting the downgrading of confidentiality, our key insight is
transparent endorsement, a mechanism for downgrading integrity
while defending against adversarial exploitation. Robust declassifi-
cation appeared to break the duality of confidentiality and integrity
by making confidentiality depend on integrity, but transparent en-
dorsement makes integrity depend on confidentiality, restoring this
duality. We show how to extend a security-typed programming
language with transparent endorsement and prove that this static
type system enforces nonmalleable information flow, a new secu-
rity property that subsumes robust declassification and transparent
endorsement. Finally, we describe an implementation of this type
system in the context of Flame, a flow-limited authorization plugin
for the Glasgow Haskell Compiler.
CCS CONCEPTS
• Security and privacy → Information flow control;
Keywords: Downgrading; Information security; Security types
1 INTRODUCTION
An ongoing foundational challenge for computer security is to dis-
cover rigorous—yet sufficiently flexible—ways to specify what it
means for a computing system to be secure. Such security condi-
tions should be extensional, meaning that they are based on the
externally observable behavior of the system rather than on unob-
servable details of its implementation. To allow security enforce-
ment mechanisms to scale to large systems, a security condition
∗Work done while author was at Harvard University.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134054
should also be compositional, so that secure subsystems remain
secure when combined into a larger system.
Noninterference, along with many variants [19, 35], has been a
popular security condition precisely because it is both extensional
and compositional. Noninterference forbids all flows of information
from “high” to “low”, or more generally, flows of information that
violate a lattice policy [14].
Unfortunately, noninterference is also known to be too restrictive
for most real systems, which need fine-grained control over when
and how information flows. Consequently, most implementations
of information flow control introduce downgrading mechanisms
to allow information to flow contrary to the lattice policy. Down-
grading confidentiality is called declassification, and downgrading
integrity—that is, treating information as more trustworthy than
information that has influenced it—is known as endorsement [47].
Once downgrading is permitted, noninterference is lost. The
natural question is whether downgrading can nevertheless be con-
strained to guarantee that systems still satisfy some meaningful, ex-
tensional, and compositional security conditions. This paper shows
how to constrain the use of both declassification and endorsement
in a way that ensures such a security condition holds.
Starting with the work of Biba [7], integrity has often been
viewed as dual to confidentiality. Over time, that simple duality
has eroded. In particular, work on robust declassification [6, 11, 27,
46, 47] has shown that in the presence of declassification, confiden-
tiality depends on integrity. It is dangerous to give the adversary
the ability to influence declassification, either by affecting the data
that is declassified or by affecting the decision to perform declas-
sification. By preventing such influence, robust declassification
stops the adversary from laundering confidential data through ex-
isting declassification operations. Operationally, languages prevent
laundering by restricting declassification to high integrity program
points. Robust declassification can be enforced using a modular
type system and is therefore compositional.
This paper introduces a new security condition, transparent en-
dorsement, which is dual to robust declassification: it controls en-
dorsement by using confidentiality to limit the possible relaxations
of integrity. Transparent endorsement prevents an agent from en-
dorsing information that the provider of the information could not
have seen. Such endorsement is dangerous because it permits the
provider to affect flows from the endorser’s own secret information
into trusted information. This restriction on endorsement enforces
an often-implicit justification for endorsing untrusted inputs in
high-integrity, confidential computation (e.g., a password checker):
low-integrity inputs chosen by an attacker should be chosen with-
out knowledge of secret information.
A similar connection between the confidentiality and integrity
of information arises in cryptographic settings. A malleable encryp-
tion scheme is one where a ciphertext encrypting one value can
be transformed into a ciphertext encrypting a related value. While
sometimes malleability is intentional (e.g., homomorphic encryp-
tion), an attacker’s ability to generate ciphertexts makes malleable
encryption insufficient to authenticate messages or validate in-
tegrity. Nonmalleable encryption schemes [15] prevent such attacks.
In this paper, we combine robust declassification and transparent
endorsement into a new security condition, nonmalleable informa-
tion flow, which prevents analogous attacks in an information flow
control setting.
The contributions of this paper are as follows:
• We give example programs showing the need for a security
condition that controls endorsement of secret information.
• We generalize robust declassification to programs including
complex data structures with heterogeneously labeled data.
• We identify transparent endorsement and nonmalleable in-
formation flow, new extensional security conditions for pro-
grams including declassification and endorsement.
• We present a core language, NMIFC, which provably en-
forces robust declassification, transparent endorsement, and
nonmalleable information flow.
• We present the first formulation of robust declassification as
a 4-safety hyperproperty, and define two new 4-safety hyper-
properties for transparent endorsement and nonmalleable
information flow, the first time information security condi-
tions have been characterized as k-safety hyperproperties
with k > 2.
• We describe our implementation of NMIFC using Flame, a
flow-limited authorization library for Haskell and adapt an
example of the Servant web application framework, accessi-
ble online at http://memo.flow.limited.
We organize the paper as follows. Section 2 provides examples of
vulnerabilities in prior work. Section 3 reviews relevant background.
Section 4 introduces our approach for controlling dangerous en-
dorsements, and Section 5 presents a syntax, semantics, and type
system for NMIFC. Section 6 formalizes our security conditions
and Section 7 restates them as hyperproperties. Section 8 discusses
our Haskell implementation, Section 9 compares our approach to
related work, and Section 10 concludes.
2 MOTIVATION
To motivate the need for an additional security condition and give
some intuition about transparent endorsement, we give three short
examples. Each example shows code that type-checks under exist-
ing information-flow type systems even though it contains insecure
information flows, which we are able to characterize in a new way.
These examples use the notation of the flow-limited authoriza-
tion model (FLAM) [4], which offers an expressive way to state both
information flow restrictions and authorization policies. However,
the problems observed in these examples are not specific to FLAM;
they arise in all previous information-flow models that support
downgrading (e.g., [8, 16, 22, 26, 33, 43, 48]). The approach in this
paper can be applied straightforwardly to the decentralized label
model (DLM) [26], and with more effort, to DIFC models that are
less similar to FLAM. While some previous models lack a notion
of integrity, from our perspective they are even worse off, because
they effectively allow unrestricted endorsement.
1 StringT password;
2
3 booleanT ← check_password(StringT → guess) {
4
5
6
7 }
Figure 1: A password checker with malleable information flow
booleanT endorsed_guess = endorse(guess, T );
booleanT result = (endorsed_guess == password);
return declassify(result, T ←);
In FLAM, principals and information flow labels occupy the same
space. Given a principal (or label) p, the notation p→ denotes the
confidentiality projection of p, whereas the notation p← denotes
its integrity projection. Intuitively, p→ represents the authority to
decide where p’s secrets may flow to, whereas p← represents the
authority to decide where information trusted by p may flow from.
Robust declassification ensures that the label p→ can be removed
via declassification only in code that is trusted by p; that is, with
integrity p←.
Information flow policies provide a means to specify security
requirements for a program, but not an enforcement mechanism.
For example, confidentiality policies might be implemented using
encryption and integrity policies using digital signatures. Alterna-
tively, hardware security mechanisms such as memory protection
might be used to prevent untrusted processes from reading confi-
dential data. The following examples illustrate issues that would
arise in many information flow control systems, regardless of the
enforcement mechanism.
2.1 Fooling a password checker
Password checkers are frequently used as an example of necessary
and justifiable downgrading. However, incorrect downgrading can
allow an attacker who does not know the password to authenticate
anyway. Suppose there are two principals, a fully trusted principal T
and an untrusted principal U . The following information flows are
then secure: U → ⊑ T→ and T← ⊑ U ←. Figure 1 shows in pseudo-
code how we might erroneously implement a password checker in
a security-typed language like Jif [25]. Because this pseudo-code
would satisfy the type system, it might appear to be secure.
The argument guess has no integrity because it is supplied by an
untrusted, possibly adversarial source. It is necessary to declassify
the result of the function (at line 6) because the result indeed leaks
a little information about the password. Robust declassification,
as enforced in Jif, demands that the untrusted guess be endorsed
before it can influence information released by declassification.
Unfortunately, the check_password policy does not prevent
faulty or malicious (but well-typed) code from supplying password
directly as the argument, thereby allowing an attacker with no
knowledge of the correct password to “authenticate.” Because guess
is labeled as secret (T→), a flow of information from password to
guess looks secure to the type system, so this severe vulnerabil-
ity could remain undetected. To fix this we would need to make
guess less secret, but no prior work has defined rules that would
require this change. The true insecurity, however, lies on line 4,
which erroneously treats sensitive information as if the attacker
had constructed it. We can prevent this insecurity by outlawing
such endorsements.
A
a_bid
A←∧(A∧B)→
(A ∧ B )
T
T
a_bid
(A ∧ B )
b_bid
b_bid
b_bid
open b_bid
(A ∧ B)← ∧ (A ∨ B)→
Bids
B
a_bid
b_bid
(A∧B)
B ← ∧ ( A ∧ B )
(A∧B)
→
open a_bid
(A ∧ B)← ∧ (A ∨ B)→
a_bid
Figure 2: Cheating in a sealed-bid auction. Without knowing Alice’s
bid, Bob can always win by setting b_bid := a_bid + 1
2.2 Cheating in a sealed-bid auction
Imagine that two principals A and B (Alice and Bob) are engaging
in a two-party sealed-bid auction administered by an auctioneer T
whom they both trust. Such an auction might be implemented using
cryptographic commitments and may even simulate T without
need of an actual third party. However, we abstractly specify the
information security requirements that such a scheme would aim
to satisfy. Consider the following sketch of an auction protocol,
illustrated in Figure 2:
(1) A sends her bid a_bid to T with label A← ∧ (A ∧ B)→. This
label means a_bid is trusted only by those who trust A and can
be viewed only if both A and B agree to release it.
(2) T accepts a_bid from A and uses his authority to endorse the
bid to label (A ∧ B)← ∧ (A ∧ B)→ (identically, A ∧ B). The
endorsement prevents any further unilateral modification to
the bid by A. T then broadcasts this endorsed a_bid to A and
B. This broadcast corresponds to an assumption that network
messages can be seen by all parties.
(3) B constructs b_bid with label B← ∧ (A∧ B)→ and sends it to T .
(4) T endorses b_bid to A ∧ B and broadcasts the result.
(5) T now uses its authority to declassify both bids and send them
to all parties. Since both bids have high integrity, this declassifi-
cation is legal according to existing typing rules introduced to
enforce (qualified) robust declassification [4, 11, 27].
Unfortunately, this protocol is subject to attacks analogous to
mauling in malleable cryptographic schemes [15]: B can always
win the auction with the minimal winning bid. In Step 3 nothing
prevents B from constructing b_bid by adding 1 to a_bid, yielding
a new bid with label B← ∧ (A ∧ B)→ (to modify the value, B must
lower the value’s integrity as A did not authorize the modification).
Again an insecurity stems from erroneously endorsing overly
secret information. In step 4, T should not endorse b_bid since it
could be based on confidential information inaccessible to B—in
particular, a_bid. The problem can be fixed by giving A’s bid the
label A→ ∧ A← (identically, just A), but existing information flow
systems impose no such requirement.
2.3 Laundering secrets
Wittbold and Johnson [44] present an interesting but insecure pro-
gram:
1 while (true) do {
2
3
4
5
6 }
x = 0 [] x = 1; // generate secret probabilistically
output x to H ;
input y from H ; // implicit endorsement
output x ⊕ (y mod 2) to L
In this code, there are two external agents, H and L. Agent H
is intended to have access to secret information, whereas L is not.
The code generates a secret by assigning to the variable x a non-
deterministic, secret value that is either 0 or 1. The choice of x is
assumed not to be affected by the adversary. Its value is used as a
one-time pad to conceal the secret low bit of variable y.
Wittbold and Johnson observe that this code permits an adver-
sary to launder one bit of another secret variable z by sending z⊕x
as the value read into y. The low bit of z is then the output to L.
Let us consider this classic example from the viewpoint of a
modern information-flow type system that enforces robust declassi-
fication. In order for this code to type-check, it must declassify the
value x⊕(y mod 2). Since the attack depends on y being affected