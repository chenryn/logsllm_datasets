0
3200
2400
1600
800
0
1600
1200
800
400
0
0
1
2
4
5
3
Time (h)
6
7
8
0
1
2
4
5
3
Time (h)
6
7
8
0
1
2
4
5
3
Time (h)
6
7
8
0
1
2
4
5
3
Time (h)
6
7
8
0
1
2
4
5
3
Time (h)
6
7
8
(a) Line coverage for diff.
(b) Line coverage for find.
(c) Line coverage for grep.
(d) Line coverage for gawk.
(e) Line coverage for patch.
1200
900
600
300
0
2400
1800
1200
600
0
520
390
260
130
0
6800
5100
3400
1700
0
0
1
2
4
3
5
Time (h)
6
7
8
0
1
2
4
3
5
Time (h)
6
7
8
0
1
2
4
3
5
Time (h)
6
7
8
0
1
2
4
3
5
Time (h)
6
7
8
0
1
2
4
3
5
Time (h)
6
7
8
(f) Line coverage for objcopy.
(k) Line coverage for sqlite.
Figure 6: Line coverage for the 10 real-world programs by running KLEE with different strategies for 8h. Mean values and
standard deviations over 20 runs are plotted.
(g) Line coverage for readelf.
(i) Line coverage for make.
(j) Line coverage for cjson.
When evaluating the coreutils test set, we set the time limit to
1h for each search strategy, following [16, 48]. For the real-world
programs, the time limit was 8h. The memory limit for all programs
was 4GB, which is higher than KLEE’s default budget (2GB) and the
limit used by prior works [15, 21]. We did not input any initial seed
test to KLEE. Whenever necessary, we repeated our experiments
for 20 times and report the mean and standard deviation.
Training and testing Learch. We trained Learch by running
Algorithm 4 on the 51 coreutils training programs. The set of initial
strategies consisted of rps, nurs:cpicnt, nurs:depth, and sgs (with
subpath lengths 1, 2, and 4) as these strategies performed the best
on the training programs. We ran Algorithm 4 for 4 iterations to
train 4 strategies (feedforward networks with 3 linear layers, 64
hidden dimension, and ReLU activations). We did not include more
trained strategies because more strategies did not significantly
increase Learch’s performance. Each iteration spent around 4h (2h
on symExec, 1h on dataFromTests, and 1h on trainStrategy). When
running Learch on the test set, we ran each strategy for a quarter
of the total time limit and combined the resulted test cases.
Versions and platform. We implemented Learch on KLEE 2.1
and LLVM 6.0. We used pytorch 1.4.0 for learning. All symbolic
execution experiments were performed on a machine with 4 Intel
Xeon E5-2690 v4 CPUs (2.60 GHz) and 512 GB RAM. Each KLEE in-
stance was restricted to running on one core. The machine learning
models were trained on a machine with RTX 2080 Ti GPUs.
6.2 Code Coverage
In this section, we present our evaluation on code coverage, i.e.,
line coverage measured with gcov [2]. We first report absolute line
coverage for all files in the package. Then, we present and discuss
the percentage of covered lines.
Line coverage for coreutils programs.
In Figure 5(a), we present
the coverage of each strategy on the 52 coreutils testing programs.
On average, Learch (green bar) covered 618 lines for all files in the
Session 10A: Crypto, Symbols and Obfuscation CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2534Table 3: Percentage of covered MainLOCs.
Table 4: Percentage of covered ELOCs.
coreutils
diff
find
grep
gawk
patch
objcopy
readelf
make
cjson
sqlite
rss
66.4
30.3
52.1
21.8
39.2
13.8
9.9
4.8
33.3
79.8
8.1
rps
nurs:cpicnt
nurs:depth
sgs
portfolio
73.1
53.7
57.7
29.7
39.2
19.1
9.5
3.9
33.3
80.2
6.3
71.6
31.1
58.3
17.1
39.2
24.4
6.0
5.2
33.0
76.5
12.8
71.4
30.6
56.0
28.1
43.0
15.1
9.3
6.5
45.2
79.6
11.4
72.0
32.3
60.2
29.8
39.2
33.5
8.3
7.7
33.3
79.7
9.2
75.4
50.5
61.2
29.7
43.0
31.9
9.8
9.1
45.2
80.3
8.7
Learch
76.9
59.1
61.0
36.5
39.2
35.8
13.3
9.0
45.2
80.3
14.2
coreutils
diff
find
grep
gawk
patch
objcopy
readelf
make
cjson
sqlite
rss
13.6
1.9
1.0
2.3
0.9
1.7
0.5
1.5
3.2
7.3
5.5
rps
nurs:cpicnt
nurs:depth
sgs
portfolio
15.2
3.3
1.1
2.5
0.9
2.4
0.4
1.2
3.2
7.4
4.2
14.7
2.0
1.2
1.8
0.9
3.1
0.3
1.6
3.2
7.0
8.8
14.9
2.0
1.1
2.9
1.0
1.9
0.4
2.0
4.4
7.3
7.8
14.9
2.0
1.2
3.1
0.9
4.2
0.4
2.5
3.2
7.3
6.3
15.7
3.1
1.2
3.1
1.0
4.0
0.5
2.9
4.4
7.4
5.9
Learch
16.1
3.6
1.2
3.9
0.9
4.5
0.6
2.9
4.4
7.4
9.7
package. The best individual heuristic was rps (purple bar), which
covered 546 lines. That is, Learch achieved at least 13% more cov-
erage than any individual heuristic. portfolio (purple bar) was the
best combined heuristic but still covered 44 lines less than Learch.
In Figure 5(b), we plot the number of programs where each strat-
egy achieved the best coverage among all the strategies. For ties,
we count one for each strategy. For 29 programs, Learch was the
best strategy, outperforming portfolio by 3 and other heuristics
by a large margin. For the other 23 programs, at least one heuris-
tics performed better than Learch, but usually only by a small
margin. Moreover, we found that Learch gave more benefits on
larger coreutils programs. For example, for the largest 5 programs,
Learch achieved ∼30% more coverage than portfolio, compared to
∼8% overall for all 52 programs. For smaller programs, the manual
heuristics already covered most parts of the programs so Learch
hardly improved upon them.
Line coverage for real-world programs.
In Figure 6, we plot
the coverage of each strategy on the 10 real-world programs. To
generate the coverage curve for combinations of strategies, we treat
independent runs of each strategy sequentially. On average, Learch
covered 2, 433 lines while the best manual heuristic, portfolio, cov-
ered 2, 023 lines. Overall, Learch outperformed all the manual
heuristics by >20%.
Judging from the mean values, Learch was the best strategy for
8 of the 10 real-world programs, except for cjson and find. For cjson,
Learch was the second best, covering 10 lines less than portfolio.
For find, Learch covered 242 and 191 less lines than portfolio and
rps, respectively. Learch’s superior performance in code coverage
was consistent among the 10 programs while all manual heuristics
were unstable. For example, portfolio did well on cjson and find
but not on objcopy and sqlite. Similarly, sgs performed well on gawk
but poorly on diff.
For sqlite, the standard deviations of all strategies were high.