# Quantifying Web-Search Privacy

**Authors:**
- Arthur Gervais†
- Reza Shokri†
- Adish Singla†
- Srdjan Capkun†
- Vincent Lenders‡

**Affiliations:**
- †ETH Zurich, Switzerland
- ‡Armasuisse, Switzerland

**Contact:**
- †[EMAIL]
- ‡[EMAIL]

## Abstract

Web search queries reveal extensive personal information to search engines and potential eavesdroppers. Obfuscating search queries by adding dummy queries is a practical, user-centric method to protect users' search intentions and interests. Despite the existence of several obfuscation methods and tools, there is no standardized quantitative methodology for evaluating web-search privacy. This paper introduces a comprehensive framework to formalize the adversary's background knowledge and attacks, the users' privacy objectives, and the algorithms to assess the effectiveness of query obfuscation mechanisms. We employ machine-learning algorithms to model the linkability between user queries, encompassing the adversary's knowledge about the obfuscation mechanism and the users' web-search behavior. Our framework quantifies privacy at both the query level (link between user’s queries) and the semantic level (user’s topics of interest). We also design a generic tool to evaluate various obfuscation mechanisms and different web search behaviors. To demonstrate our approach, we analyze and compare the privacy of users using two example obfuscation mechanisms on a set of real web-search logs.

**Categories and Subject Descriptors:**
- [Security and Privacy]: Privacy Protections; E.0 [Data]: General

**Keywords:**
- Web Search
- Privacy
- Obfuscation
- Quantification Framework
- Query Privacy
- Semantic Privacy
- Machine Learning

**Copyright Notice:**
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

**CCS'14, November 3–7, 2014, Scottsdale, Arizona, USA.**
- Copyright is held by the owner/author(s). Publication rights licensed to ACM.
- ACM 978-1-4503-2957-6/14/11 ...$15.00.
- http://dx.doi.org/10.1145/2660267.2660367

## 1. Introduction

Users frequently search the web to obtain information or find websites, leaving a trail of their interests and intents. This information can be exploited by search engines and eavesdroppers to build detailed profiles and infer sensitive personal information [35, 22].

Privacy in web search can be protected through various approaches:
- **System-Centric Solutions:** Designing search engines with private information retrieval (PIR) ensures that users can receive search results without revealing their queries or activities to the search engine [20, 24, 11]. However, this solution does not protect users when using popular search engines.
- **Network-Centric Solutions:** Users can use anonymous communication techniques to hide their identities, making their queries unlinkable [31, 14]. This method can prevent adversaries from constructing detailed user profiles, but features extracted from the user’s web browser can still be used for fingerprinting and linking queries [16].
- **User-Centric Solutions:** Users can conceal their real queries by issuing interleaved fake queries [1, 12, 21]. The challenge is to generate fake queries that cannot be easily distinguished from real ones, as simple randomly generated queries can be filtered out [9, 12, 28].

In this paper, we focus on evaluating user-centric web search query obfuscation methods. Despite the existence of several obfuscation mechanisms, there is no common methodology for measuring the privacy of users. We propose a generic framework to address this gap.

## 2. Privacy Framework

Our framework for quantifying web-search privacy consists of the following main components:
- **User’s Search Behavior**
- **Obfuscation Mechanisms**
- **Adversary’s Knowledge and Linkage Function**
- **Linkage Attack**
- **Privacy Metrics**

### 2.1 User’s Search Behavior

Users issue queries to web search engines to fulfill their information needs. Each query event, along with its contextual information, is modeled as \( e : \{u, t, q, r, c\} \), where:
- \( u \) is the user identity (e.g., username, IP address, cookie identifier).
- \( t \) is the time of the query.
- \( q \) is the query string.
- \( r \) is the search result page.
- \( c \) is the set of clicked pages.

The web-search trace of a target user \( U \) is denoted as \( SU : \{e_1, e_2, \ldots, e_n\} \).

### 2.2 Obfuscation Mechanisms

User-centric obfuscation mechanisms aim to protect privacy by interleaving fake query events with real queries. Let \( SF \) be the set of fake query events. The obfuscation mechanism may generate \( SF \) by observing the target user's behavior or independently. The interleaving of fake and real queries can be done in various ways, such as sending fake queries at regular intervals or in bursts.

We evaluate two types of obfuscation mechanisms:
- **Bag of Words:** Mechanisms like TrackMeNot (TMN) [1, 21] generate fake queries by sampling from a bag of text.
- **Real Queries:** Mechanisms that use real queries from other users to generate fake queries for the target user.

The sequence of events from the user \( U \) appears as \( SO : \{e_1, e_2, \ldots, e_m\} \), obtained by interleaving \( SU \) and \( SF \).

### 2.3 Adversary’s Knowledge

The adversary aims to separate fake queries from real ones to extract accurate personal information. We assume the adversary has the following knowledge:
- **Obfuscation Mechanism:** The adversary knows or can infer the obfuscation mechanism.
- **Log History of Users’ Search Activities:** The adversary has access to a log history \( HG \) of web search activities for a set of users, excluding the target user.
- **Log History of the Target User:** The adversary may have access to the target user’s query history \( HU \).

### 2.4 Linkage Function and Attack

The linkage attack aims to partition the set of events \( SO \) and determine which events belong to the target user. The key idea is to learn a linkage function \( L(e_i, e_j) \) that quantifies the similarity between any two events \( e_i \) and \( e_j \). This function is learned using machine learning techniques and contextual and semantic features extracted from the available logs.

Given \( SO \), the adversary computes pairwise similarities and clusters the events into \( k \) clusters, \( C(SO, L, k) = \{S_1, S_2, \ldots, S_k\} \).

### 2.5 Privacy Metric

We quantify privacy in terms of the adversary’s error in correctly constructing the user’s profile. We measure privacy at two levels:
- **Query Privacy:** The user’s objective is to hide the relation between her queries. We measure the structural distance between the clusters generated by the linkage attack and the set of the user’s real queries.
- **Semantic Privacy:** The user’s objective is to conceal her topics of interest. We measure the accuracy of the adversary in inferring these topics.

## 3. Dataset and Feature Extraction

We use the AOL dataset [27] to evaluate our framework. We extract various features from each query, including time, structure, content, and landing web pages. These features are used to train gradient tree boosting regression algorithms to learn the users’ web-search behavior.

## 4. Methodology for Quantifying Privacy

We present our methodology for quantifying privacy against linkage attacks. We use the linkage function to compute pairwise similarities and cluster the events. We then quantify privacy at both the query and semantic levels using the output of the linkage attack.

## 5. Evaluation

We evaluate the privacy of users in our dataset using two example obfuscation mechanisms. The results show that our attack can easily break the privacy of most users, especially in inferring their topics of interest.

## 6. Related Work

We survey related work and put them in perspective with our contribution. We discuss existing obfuscation mechanisms and their characteristics, and how our framework provides a more comprehensive and flexible approach to quantifying web-search privacy.

## 7. Conclusion

This paper introduces a generic quantitative framework for evaluating the privacy of web search queries. Our framework models the adversary’s knowledge and attacks, the users’ privacy objectives, and the algorithms to assess the effectiveness of obfuscation mechanisms. We demonstrate the utility of our framework using real web-search logs and provide insights into the privacy risks associated with different obfuscation methods.