title:Quantifying Web-Search Privacy
author:Arthur Gervais and
Reza Shokri and
Adish Singla and
Srdjan Capkun and
Vincent Lenders
Quantifying Web-Search Privacy
Arthur Gervais†, Reza Shokri†, Adish Singla†, Srdjan Capkun†, and Vincent Lenders‡
†ETH Zurich, Switzerland, ‡Armasuisse, Switzerland
†ﬁPI:EMAIL, ‡ﬁPI:EMAIL
ABSTRACT
Web search queries reveal extensive information about users’
personal lives to the search engines and Internet eavesdrop-
pers. Obfuscating search queries through adding dummy
queries is a practical and user-centric protection mechanism
to hide users’ search intentions and interests. Despite few
such obfuscation methods and tools, there is no generic
quantitative methodology for evaluating users’ web-search
privacy. In this paper, we provide such a methodology. We
formalize adversary’s background knowledge and attacks,
the users’ privacy objectives, and the algorithms to eval-
uate eﬀectiveness of query obfuscation mechanisms. We
build upon machine-learning algorithms to learn the link-
ability between user queries. This encompasses the adver-
sary’s knowledge about the obfuscation mechanism and the
users’ web-search behavior. Then, we quantify privacy of
users with respect to linkage attacks. Our generic attack can
run against users for which the adversary does not have any
background knowledge, as well as for the cases where some
prior queries from the target users are already observed. We
quantify privacy at the query level (the link between user’s
queries) and the semantic level (user’s topics of interest). We
design a generic tool that can be used for evaluating generic
obfuscation mechanisms, and users with diﬀerent web search
behavior. To illustrate our approach in practice, we analyze
and compare privacy of users for two example obfuscation
mechanisms on a set of real web-search logs.
Categories and Subject Descriptors
[Security and privacy]: Privacy protections; E.0 [Data]:
General
Keywords
Web Search; Privacy; Obfuscation; Quantiﬁcation Frame-
work; Query Privacy; Semantic Privacy; Machine Learning
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660367 .
1.
INTRODUCTION
Users search the web to obtain information or ﬁnd web-
sites. Through this, they leave a trail of their interests and
intents. This information can be used by search engines and
eavesdroppers to build a proﬁle of users and to infer sensitive
personal information about them [35, 22].
Privacy in web search can be protected in diﬀerent ways.
In a system-centric solution, we can design a search engine
using private information retrieval, such that users can ob-
tain the results to their searches without revealing their
queries or their search activities to the search engine [20,
24, 11]. The beneﬁt of this approach is that no informa-
tion about users’ search activities is revealed to the service
provider or any eavesdropper. This solution, however, can-
not protect privacy of users with respect to the existing pop-
ular search engines. In a network-centric solution, users can
make use of anonymous communications to hide their identi-
ties with respect to the search engines, in order to make their
queries unlinkable [31, 14]. This technique can prevent an
adversary from constructing a proﬁle for each user to some
extent. Features extracted from the user’s web browser can
be used however to ﬁngerprint the user and link her queries
[16]. In a user-centric approach, users can conceal their real
queries by issuing interleaving fake queries [1, 12, 21]. The
challenge here is to generate fake queries that cannot be dis-
tinguished from real queries, as simple randomly generated
queries can be easily ﬁltered out from the set of observed
queries from a user [9, 12, 28]. Note that there is a possi-
bility of combining these approaches for designing a hybrid
protection mechanism.
In this paper, we focus on evaluating user-centric web
search query obfuscation methods. Despite the fact that
a number of obfuscation mechanisms such as [1, 32, 26, 30,
37, 21] have been proposed so far, and there exists simple
attacks to show their limitations [9, 12, 28], there is no com-
mon methodology and generic quantitative framework for
measuring privacy of users for diﬀerent obfuscation mecha-
nisms. In this paper, we propose such a framework.
We construct a generic model for the users’ web-search
behavior. This determines the relation between queries of
each user, in general. We model this using a similarity score
function between query pairs that predicts whether any two
queries could belong to the same user or not. We assume
that adversary might have access to a dataset of real queries.
We extract a variety of features from each query about its
diﬀerent dimensions including time, structure, content, and
landing web pages. We then make use of gradient tree boost-
ing regression algorithms to learn users’ web-search behav-
User’s Search Behavior
Obfuscation Mechanism
Adversary’s Knowledge and Linkage Function
User issuing query to search engine
and interacting with results
          e: {u, t, q, r, c}
Pool of available search events  SF
(e.g. generated using NLP techniques
         or sampled from other users)
Knowledge about obfuscation 
mechanism
Stream of search events from user u
                    SU: {e1, e2  ... en}
Interleaving user events 
with fake events
SO: {e1, e2  ... em}
Logs/History of search events 
from general users (HG) or 
from target user (HU)
Learn linkage function 
                L(ei, ej)
Quantifying Privacy  
Compute privacy metrics at query or 
semantic level using output of linkage 
attack C(SO , L, k), and user events  SU 
Linkage Attack
Compute pairwise similarity 
for all events ei, ej  in SO 
using linkage function L
Cluster the m events in SO into k clusters
                 C(SO , L, k) = {S1,S2 ... Sk}
Figure 1: Overview of our framework for quantifying web-search privacy.
ior in the form of linkage functions [18, 19]. We also use
our knowledge on the obfuscation mechanism in modeling
this function. In addition to this generic model, depending
on the evaluation’s settings, we might also assume that the
adversary has access to some history of the target user’s web
search behavior. This enables the adversary to construct a
speciﬁc model for the target user.
We quantify web-search privacy of users against linkage
attacks, where the attacker’s goal is to ﬁnd the link between
queries issued by the target user and separate real queries
from the fake ones. To this end, the attacker makes use of his
user behavior and obfuscation model that are compressed in
the form of linkage functions. In our framework, we rely on
the results of the linkage attack to compute a user’s privacy
with respect to diﬀerent objectives (i.e., privacy metrics).
For example, privacy can be quantiﬁed at the query level
and at the semantic level, depending on whether the user’s
objective is to hide the (linkability) structure of her queries
or to conceal her topics of interest. The randomness of the
user’s behavior also contributes to the user’s privacy against
linkage attacks. Thus, to evaluate the privacy gain of using
an obfuscation mechanism, we subtract the eﬀect of user’s
randomness and compute the relative privacy of users.
We run our methodology on queries from the AOL dataset
[27]. We consider two representative obfuscation mecha-
nisms that either make use of bag of words or real user query
logs to generate fake queries. We then evaluate and compare
users’s privacy with respect to diﬀerent metrics. The results
show that our attack can easily break privacy of majority of
user, especially in inferring their topics of interest.
The main contributions of this paper are therefore:
• We propose a generic quantitative framework using
which we can model attacks against web query obfus-
cation mechanisms as well as privacy metrics to cap-
ture the users’ privacy objectives.
• We design the linkage functions that can model the
web-search behavior of users in addition to that of the
target user. The linkage function also captures the re-
lation between fake and real queries, hence also models
the obfuscation mechanism.
• We implement a linkage attack that splits the fake
queries from the user’s queries. By comparing the
attack’s result with the user’s real set of queries, we
quantify privacy in multiple dimensions, e.g., query
trace structure and semantics.
The rest of the paper is organized as follows.
In Sec-
tion 2, we present the overall framework that we propose for
quantifying web-search privacy. In Section 3, we present the
dataset of web search queries that we are using throughout
the paper. We also seek to understand the behavior of users
with respect to their web search, and we extract features
from their web queries that reﬂect the users’ behavior. In
Section 4, we build upon our user model and we present our
methodology for quantifying privacy of users against link-
age attacks. In Section 5, we use our quantiﬁcation method
to evaluate privacy of users (in our dataset) with respect
to diﬀerent obfuscation mechanisms. In Section 6, we sur-
vey the related work and put them in perspective with our
contribution.
2. PRIVACY FRAMEWORK
In this section, we introduce our framework for quanti-
fying user’s web-search privacy. As shown in Figure 1, the
framework is composed of the following main components:
(i) user’s search behavior, (ii) obfuscation mechanisms, (iii)
adversary knowledge, (iv) linkage attack, and (v) privacy
metrics. We now provide high level details of each of these
components and introduce the required notation.
2.1 User’s Search Behavior
Users issue queries to the web search engines to seek their
information needs. In response, the search engine retrieves
a result page consisting of a ranked list of web pages. The
user interacts with the search result page by clicking and
browsing the relevant documents, or by further reﬁning the
search query to fulﬁll the information needs. This interac-
tion of the user with the search engine leaves a trace of her
web search activity as a sequence of search query events. We
model each such query event of a user along with its con-
textual information as e : hu, t, q, r, ci, where u is the user
identity (e.g., her username, IP address, cookie identiﬁer,
or any pseudonym that links the user’s queries together but
does not necessarily reveal her true identity), t is the time at
which the query is issued, q is the user’s query string which is
composed of a sequence of terms, r is the search result page
returned by the search engine containing ranked lists of web
pages, and c is the set of pages that are clicked by the user
so as to seek the required information. The web-search trace
of the target user U , given by a series of web search query
events from the user U , is denoted as SU : {e1, e2, . . . , en}.
2.2 Obfuscation Mechanisms
User-centric obfuscation mechanisms aim to protect the
privacy of a user by interleaving a set of fake query events
with the real queries of the users. Let SF be the set of query
events associated with the fake queries that are used by the
obfuscation mechanism for protecting the privacy of user U .
An obfuscation mechanism might generate the fake trace SF
by observing the behavior of target user U , or the fake trace
can be generated independently. The fake queries could also
be generated by getting information from diﬀerent sources.
Another key parameter of the obfuscation mechanisms is
the way interleaving of fake and real queries is done. For
example, the obfuscation mechanism may send fake queries
at regular intervals or send a burst of fake queries when a
real query is issued by user.
In Section 6, we provide a
survey of the various existing obfuscation mechanisms and
their characteristics.
In particular, we evaluate the following two types of ob-
fuscation mechanisms in this paper:
• Mechanisms that generate fake queries by sampling
from a bag of text. One example of such a mecha-
nism is TrackMeNot (TMN) [1, 21] that mainly uses
some RSS feeds for generating fake queries, and reﬁnes
its queries by observing the search results of its own
issued queries in the past. We choose TMN due to its
popularity and open availability.
• Mechanisms that make use of real queries from a set of
other users to generate fake queries for the target user.
We consider a speciﬁc variant of such a mechanism that
chooses one random user and uses all her queries for
this purpose.
As a result of the obfuscation, the sequence of events com-
ing from user U appears as SO : {e1, e2, . . . , em}, obtained
by interleaving SU and SF . Hence, the search engine or any
eavesdropper observes SO from the user U , where all the
events in SO have the same identity U thus appearing as if
they are issued by the target user.
2.3 Adversary’s Knowledge
The goal of the adversary is to separate the fake queries
from real ones, in order to extract accurate personal infor-
mation about the user. We quantify the privacy of users
by evaluating the eﬀectiveness of the employed obfuscation
mechanism against such attacks from the adversary. We
assume the following about the prior knowledge of the at-
tacker:
• Obfuscation mechanism: As a privacy evaluator,
our objective is to assess the robustness of particular
obfuscation mechanisms against strong attacks. Hence,
we assume knowing the obfuscation mechanism or be-
ing able to observe its behavior prior performing the
evaluation attack. Apart from knowing the exact mech-
anism, the adversary might additionally be aware of
the parameters of how the fake queries are generated,
and how the interleaving is done. Alternatively, if the
adversary does not know such exact details about the
obfuscation mechanism, we assume that he can infer
the behavior of the mechanism by observing its output
in an oﬄine training phase.
• Log history of users’ search activities: We further
assume that the adversary has access to some log his-
tory of web search activities for a set of users (exclud-
ing target user), denoted by HG. From this dataset,
the adversary can build a generic model for the users’
web-search behavior and can learn the models needed
for linkage attacks (as discussed further below).
• Log history of the target user: The adversary
might additionally have access to some history of the
target user U ’s query events, given by HU . This can
additionally enable the adversary to build a more spe-
ciﬁc model for the target user, thus further empower-
ing him to eﬀectively predict the user’s queries that
she issues over time or the topics that she is interested
in.
2.4 Linkage Function and Attack
The objective of the linkage attack is to partition the set
of events in SO and determine which of the query events
are associated with the target user. By exploiting the ad-
versary’s knowledge about the obfuscation mechanism and
the log histories, the key idea is to learn a linkage function
L(ei, ej ) that quantiﬁes the similarity of any two events ei,
ej and uses it to determine whether they belong to the same
target user or not.1 The learning is done by extracting dif-
ferent contextual and semantic features from the available
logs of the queries (and additionally from the fake auto-
generated queries depending upon the type of obfuscation
mechanism used) [36, 34]. These features are then used to
learn (i) how query events of a generic user as well as the
target user are correlated, and (ii) what feature value ranges
represent the behavior of the target user. By using machine
learning techniques to learn the linkage function, the overall
framework easily adapts to new types of obfuscation mech-
anisms, to diﬀerent data sets as well as diﬀerent levels of
prior knowledge of the adversary. The details of this learn-
ing procedure as well as the diﬀerent features extracted are
discussed in Section 3.2 and Section 4.
1Note that in this paper we limit the study on tuples of
two events, however more complex structures (e.g. cliques
of three or more queries) might improve the attacker’s accu-
racy. Nevertheless, the quantiﬁcation methodology remains
the same.
We use the linkage function L as the basis for the linkage
attacks. Given a set of query events SO, the adversary ﬁrst
computes the pairwise similarity L(ei, ej) between any two
events ei, ej ∈ SO. These pairwise similarities then allow
the adversary to cluster or partition the events SO into a set
of k clusters. Thus, the output of the linkage attack is a set
of clusters, given by C(SO, L, k) = {S1, S2, . . . , Sk}.
In case the adversary has access to the target user’s his-
tory HU , this information could further be used to label
these clusters or learn more speciﬁc linkage functions.
In
this paper, we do not elaborate on this last step.
2.5 Privacy Metric
We quantify the privacy of users given the output of the
linkage attack under various types of privacy sensitivities
that the user might have. In general, we measure privacy in
terms of the adversary’s error in correctly constructing the
user’s proﬁle.2 This reﬂects the privacy risk of issuing web
queries using a particular obfuscation mechanism.
2.5.1 Query Privacy
Let us consider that the user’s objective is to hide the
relation between her queries, so that the adversary could
not infer the relation between her diﬀerent interests (dimen-
sions of her search proﬁle). We quantify this by measuring
the structural distance between the clusters generated by the
linkage attack and the set of the user’s real queries. Consider
running the linkage attack by setting k = 2. A perfect parti-
tion by the adversary would result in C(SO, L, 2) = {S1, S2}
where S1 = SU and S2 = SF , thus completely separating the
real queries of the target user from the fake queries intro-
duced by the obfuscation mechanism.