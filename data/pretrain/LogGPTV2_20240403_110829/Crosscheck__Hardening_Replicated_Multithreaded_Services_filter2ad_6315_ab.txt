### CROSSCHECK Approach

**Figure 1: CROSSCHECK Approach**

As depicted in Figure 1, each time we update the checksum during the execution of a request, the updated checksum \( C_i \) is added to a checksum collection (CC) along with a unique object ID, \( i \in I = \{1, \ldots, n\} \), which is consistent across all replicas for the corresponding object. The unique ID serves two purposes: 

1. **Efficient Comparison**: It allows for efficient comparison of objects across replica boundaries.
2. **Control-Flow Detection**: It enables the detection of control-flow changes, as divergent execution flows result in different sequences of checksums or divergent checksum collections.

Additionally, a reference to the state object exhibiting the checksum is stored in the CC. This facilitates the identification of corrupted state objects during recovery (see Section III-C).

### State Validation

**Figure 2: CROSSCHECK State Validation Algorithm**

After executing the request, all replicas perform a state validation as shown in Figure 2. Initially, each replica broadcasts its checksum collection (CC) and a potential client reply message (RM) with its checksum (RC) to the other replicas. The broadcast message includes state object references, but these are only valid at the origin and are excluded from the message.

Upon receiving a message, a replica calls `vrfyLocal()` to verify its checksums (all \( C_i \) in CC and RC) and the modified state object IDs by comparing them with the received data from the remote replica (Line 6). Additionally, all received messages are compared to each other using `vrfyMsgSet()` (Line 12). If an error is detected (Lines 9 and 15), a recovery request is sent via the ordering stage. This ensures that all replicas eventually enter a quiescent state by finishing all running executions.

Error detection continues until a quorum of \( f+1 \) matching checksums is gathered. At this point, the system can distinguish between corrupted and correct-working replicas, and the affected state objects are added to a list for later recovery. Since the reply messages RM are also exchanged, the replica responsible for the client connection can externalize a correct reply to the client even in the presence of a fault. The reply message may contain \( RM_u \) and \( RC_v \) from any validated replica \( u \) and \( v \), enabling client-side error detection if supported.

### Recovering from Faults

As outlined in our system model (see Section II), we consider state corruptions of one or multiple state objects in \( f \) replicas. This leaves a pool of \( f+1 \) fault-free replicas that can be used for recovery. State corruptions can be detected either ahead of state object access (see Section III-A) or as part of cross-checking the execution (see Section III-B). In the latter case, control-flow errors are also detected.

In the simplest case, recovery from state corruptions detected ahead of access can be handled by generic object protection if object state duplication has been applied. If there is no local object-state copy, the affected state object must be requested from any fault-free replica. However, since the remaining fault-free replicas might have already modified their fault-free copy of the affected state object, there is no version available for direct replacement and continuation of the request execution at the faulty replica. The same problem arises if both state corruptions and control-flow errors are detected during the cross-check phase.

To address this, we use a synchronization model where the remaining fault-free replicas finish the execution of ongoing requests to provide updates for the faulty replica. A naive implementation would simply compare all state object checksums and replace faulty ones, but this would be time-consuming. To minimize overhead, we focus on the deltas between the co-executing replicas determined by their recent execution history.

When a state corruption is detected, the following steps are taken:

1. **Quiescent State**: All replicas need to reach a quiescent state. Once a fault is detected, a recovery request is distributed via the ordering stage to all replicas, ensuring that running request executions are finished and no new executions are started. Specifically, all requests distributed via the ordering stage before the recovery request are completed by the fault-free replicas. After this point, all replicas are in a consistent state. The faulty replica finishes all already executing requests and aborts request execution if a corrupted object is detected, preventing control-flow errors and containing the state corruption.

2. **Checksum List Transmission**: The faulty replica transmits a list of all state-object checksums that have been changed during or after the detection of the state corruption. This includes requests executed concurrently with the fault-detecting or faulty request. The fault-free replicas receive this list, build their own list, and compile a state delta. The state delta consists of:
   - State objects with diverging checksums.
   - State objects locally changed due to request execution but not at the faulty replica.
   - State objects changed at the faulty replica but not locally, caused by control-flow errors.

3. **State Delta Transfer**: The state delta is transmitted to the faulty replica, which uses the first complete incoming data set to update its local state. All requests not finished before recovery are discarded by the faulty replica, as the state (delta) update already covers those requests. If the faulty replica is responsible for any client connection, it must externalize the reply to the client. The state-delta transfer includes the reply messages.

4. **Recovery Completion**: After finishing recovery, the repaired replica broadcasts a recovery completion message via the ordering stage. Once received by any replica, normal operation can be safely continued.

### Implementation

As a case study, we implemented our approach in an actively replicated key-value store based on MEMCACHED++ (Figure 3), a C++ version of memcached. In MEMCACHED++, all relevant components, including a central hash table that manages all key-value pairs, individual key-value pairs, and management classes, can be individually hardened by applying Generic Object Protection (GOP).

A replicated key-value store can be used to provide a highly available source for data exchange (e.g., configuration information) in distributed applications and can be extended to offer coordination support similar to Chubby [1] and Zookeeper [21]. MEMCACHED++ offers an object-oriented design and features the same API and threading model as the original version of memcached.

As described by the system model, we need to enforce an ordered execution of requests. We achieve this by integrating ordering support into MEMCACHED++: On receipt of a client request, a replica parses the request and broadcasts a client request message to all replicas (including itself) via the Spread Toolkit [18]. Spread provides a reliable group communication channel and brings all requests in a defined order. After receiving the message, each replica registers the new request to Storyboard [14], which creates a lock-order prediction and ensures deterministic execution. All replicas then execute the request in a multi-threaded but controlled manner.

Before externalizing the reply, we perform the cross-check by exchanging checksums between the replicas and validating their content (see Section III-B). While waiting for messages to arrive, we continue execution by processing further requests.