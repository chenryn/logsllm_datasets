(b) Coverage, Websites 
Figure 7: Coverage. Proportion of ads each algorithm found, out of all ads 
found by at least one algorithm.
s
n
o
i
t
c
i
d
e
r
p
f
o
r
e
b
m
u
N
x
=
<
e
u
l
a
v
-
p
h
t
i
w
2000 
logit
1500
set_intersection
 1000
 500
 0
1e-05 
0.0001 
0.001 
0.01 
0.1 
1 
s
n
o
i
t
c
i
d
e
r
p
f
o
r
e
b
m
u
N
x
=
<
e
u
l
a
v
-
p
h
t
i
w
logit
set_intersection
 500 
 400
 300
 200
 100
0
1e-05 
0.0001 
0.001 
0.01 
0.1 
1
(a) P-value CDF (Gmail)
(b) Same as (a) with Holm 
Figure 8:  Effect of p-value correction on the distribution.  The Set In­
tersection algorithm makes much more hypothesis, and thus has more low 
p-value hypothesis.  After Holm’s correction however, the Logit algorithm 
has more low p-value hypothesis. X is log scale. 
low p-values, before and after correction.  Set Intersection outputs 
the most low p-value hypotheses (low_pvals group),  but we saw 
in Fig. 6(a) 6(c) that these hypotheses are poor predictors of the ad 
presence, with a precision below 50%. After the strictest correction 
(low_pvals_w/Holm), when all hypotheses have similar predictive 
power,  the  Logit  Stage  1  gives  the  best  coverage,  with  93%  on 
Gmail and 94% on Website, beating Lm, the Bayesian algorithm, 
and Set Intersection.  We can make the same conclusion on Gmail 
after BY correction, but the picture is not as clear on the Websites 
dataset, where Logit has a lower coverage (about 80%) but makes 
hypotheses with a better ad prediction precision (see Fig. 6(c)). 
It is interesting to understand why Set Intersection has a much 
better coverage before correction, but loses this edge to Logit af­
ter p-value correction.  This can be explained by the fact that the 
number of hypotheses, and the proportion of high p-value hypothe­
ses  play  an  important  role  in  the  correction,  both  increasing  the 
penalty applied to each p-value. To further demonstrate this effect, 
Fig. 8 shows the CDF for the distribution of the absolute number of 
hypotheses per p-value for Logit and Set Intersection. On Fig. 8(a) 
we observe that the Set Intersection Stage 1 algorithm makes more 
low p-value hypotheses with 836 hypothesis below 5%, and only 
486 for Logit.  However we can also see that the total number of 
small
large
 1 
0.8
 0.6
0.4
 0.2
s
n
o
i
t
c
i
d
e
r
p
f
o
%
x
=
<
e
u
l
a
v
-
p
h
t
i
w
small
large
1 
 0.8
 0.6
 0.4
 0.2
s
n
o
i
t
c
i
d
e
r
p
f
o
%
x
=
<
e
u
l
a
v
-
p
h
t
i
w
small
large
1 
 0.8
 0.6
 0.4
 0.2
s
n
o
i
t
c
i
d
e
r
p
f
o
%
x
=
<
e
u
l
a
v
-
p
h
t
i
w
 0
1e-05 
0.0001 
0.001 
0.01 
0.1 
1 
 0
1e-05 
0.0001 
0.001 
0.01 
0.1 
1 
 0
1e-05 
0.0001 
0.001 
0.01 
0.1 
1 
(a) P-value distribution
(b) P-value dist. after BY correction
(c) P-value dist. after Holm correction 
Figure 9: Effect of p-value correction on the distribution, at different scales. In this graph, the scale is regarding the number of ads. For our small and large 
scale Website datasets, for the Logit Stage 1, (a) shows the p-value distribution, (b) the p-value distribution after BY correction, and (c) the p-value distribution 
after Holm correction. The higher number of hypotheses in the large experiment widens the difference in distributions after correction. 
hypothesis is much higher (3282 compared to 928), and that a lot 
of these hypotheses have a high p-value.  After Holm correction 
however,  the  Logit  algorithm  retains  80%  more  low  p-value  hy­
potheses,  as shown on Fig. 8(b).  BY correction shows the same 
trend,  although with less extreme results,  explaining why on the 
Websites dataset Set Intersection keeps a higher coverage. 
We show that making many hypotheses hurts coverage after p-
value  correction,  particularly  with  the  Holm  method.  This  calls 
for algorithms that favor small number of high quality hypotheses, 
such as Logit, over algorithms that make hypotheses even on weak 
signal, such as Set Intersection.  Making only a small number of 
low p-value hypotheses is not always possible however, especially 
when scaling experiments, which we evaluate next. 
6.5  Q4: Conﬁdence at scale 
We saw in § 6.4 that favoring fewer, higher quality hypothesis is 
a winning strategy to retain low p-value hypothesis after correction. 
Unfortunately this is not always possible when scaling the number 
of outputs, for instance when experiments have more inputs, more 
proﬁles,  or  just  when  we  collected  data  for  a  longer  period.  In 
these cases analysis may also be harder, leading to fewer good p-
value hypotheses.  Fig. 9 shows a CDF of the proportion of inputs 
below a given p-value, and the same after the two p-value correc­
tion techniques we consider.  We make three observations.  First, 
it appears that hypotheses are indeed harder to make on the large 
dataset. Fig. 9(a) shows that the proportion of low p-values is lower 
even before correction, with 75% of hypotheses with a p-value be­
low 5% for the Website-large, and 88% for Website. 
Second, Fig. 9(c) shows that the Holm correction greatly reduces 
the proportion of low p-values, with the Website experiment going 
from 88% to 61%. The effect on the many hypotheses of Website-
large  is  much  stronger  however,  with  the  low  p-values  dropping 
from 75% to 21%.  We conclude that the Holm correction is very 
hard on experiments with a lot hypotheses.  The larger the prover­
bial haystack, the harder the needles will be to ﬁnd. 
Third, the BY correction method is milder than Holm’s.  Even 
though we can see the large experiment’s distribution caving more 
than the small’s, the distinction at scale is smaller.  Website-large 
still has 46% of its p-values below 5%,  while the small one has 
74%.  Despite the weaker guarantees of the BY correction, it can 
be a useful trade-off to make when dealing with large numbers of 
outputs.  Indeed,  it is a well-accepted correction in statistics and 
machine learning. We hence include it as a default in our system. 
6.6  Anecdotal experience with the data 
We already saw that high conﬁdence hypotheses give good pre­
dictors of the proﬁles in which an ad will appear. While this ad pre­
diction precision and recall reveals that our algorithms are indeed 
detecting a correlation, we also manually looked at many hypothe­
ses  to  understand  the  strengths  and  weaknesses  of  our  methods. 
We next describe the results of this experience on large, complex 
datasets from Gmail and Websites experiments.  These services do 
not  provide  ground  truth  at  this  granularity,  so  we  manually  as­
sessed  the  hypotheses’  validity.  For  this  manual  assessment  we 
looked at low p-value hypotheses, visited the website targeted by 
the ad, and decided if it made sense for the website to target the ad. 
If we did not ﬁnd a connection for at least one email in the target­
ing combination, we declared the hypothesis wrong.  For instance 
an ad for a ski resort targeting the “Ski” email was considered right, 
but the same ad targeting “Ski” and “Cars” was considered a mis­
take.  This labelling is very error prone.  On the one hand some 
associations can be non obvious but still be a real targeting. On the 
other hand it can be easy to convince ourselves that an association 
makes sense even when there is no targeting. It is thus an anecdotal 
experience of Sunlight’s performances. 
Hypothesis Precision. We examined 100 ads with high-conﬁdence 
hypotheses (p-value < 0.05 after Holm) from the Gmail and Web­
site  experiments,  and  counted  instances  where  we  could  not  ex­
plain the input/output association with high certainty.  We found 
precisions of 95% and 96%, respectively. Hypotheses we classiﬁed 
as false positives were associations of ads and emails that we just 
could not explain from the topics,  such as the luggage company 
“www.eaglecreek.com” targeting the “Cheap drugs online order” 
email from Figure 4. In this speciﬁc case the ad appears for 3 con­
secutive days, multiple times (a total of 437 impressions in 15 to 
19 different accounts), and almost only in the context of the email, 
so there does seem to be some targeting, although we cannot se­
mantically explain it. However, in other cases we have less data to 
gain conﬁdence, and we classify them as a mistake.  This example 
highlights the difﬁculty and the risk of bias of manual labelling. 
Many errors were also combinations with one input that seemed 