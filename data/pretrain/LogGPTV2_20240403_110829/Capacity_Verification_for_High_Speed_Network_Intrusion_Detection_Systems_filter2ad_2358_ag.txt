– Controlled delay. Characters do not arrive much later in Stream 2:
ti < ui < ti + 2∆.
Thus, there is no possibility that a statistical traﬃc anomaly detector can have
cause for ﬂagging a stream 2 produced by LIS as aberrant traﬃc. Also, by
controlling the parameter ∆, we control the maximum tolerable delay.
To study the properties of multiscale detectors in the LIS setting, we use
Monte-Carlo simulation. In our simulation experiments, we used for (ti) samples
from the empirical distribution of inter-keystroke times described in [9]. We
created a stream several minutes in length and we transcoded this stream three
times at three diﬀerent delay parameters: 100 msec, 200 msec, and 300 msec.
From the streams, we created time series. We selected 256 second stretches of
each stream, divided the time axis into 1/64th second intervals, and we counted
the number of character arrivals (always 0 or 1, of course) within each interval.
This gave us equally spaced series of 16384= 215 0’s and 1’s. We then used the
freely available wavelet transform routines in WaveLab [10] to perform a wavelet
analysis using the Haar wavelets.
The table below reports correlations between wavelet coeﬃcients of the orig-
inal stream and the three transformed streams. It contains the empirical correla-
tions between the wavelet coeﬃcients at various scales, deﬁning the correlation
coeﬃcient at scale j by
(cid:1)
(cid:5)(cid:1)
(cid:1)
(cid:6)1/2
Corr(j) =
α1
j,kα2
j,k/
j,k)2 ·
(α1
(α2
j,k)2
k
k
k
where each sum is over all coeﬃcients at a given scale j.
Scale ∆ =100 ms 200 ms 300 ms
0.9599 0.9382
32 sec
64 sec
0.9654 0.9371
0.9695 0.9458
128 sec
0.9964
0.9965
0.9966
These results are typical and repeatable. Correlations, of course, cannot ex-
ceed 1.0. So these correlations, which approach 1 at suﬃciently long scales, are
rather large. Evidently, given about 1 minute’s worth of data on two jittered
streams, we can obtain a substantial signal by correlation of wavelet coeﬃcients.
Note particularly the very high correlations when jittering is less than .1 sec.
8 Detecting Evasions in the Presence of Chaﬀ
In the analysis since Section 4, we have assumed that any stream transformation
being used to disguise correlations was conservative – that is, it exactly preserves
the number and content of keystrokes, but perhaps alters their timing.
32
D. Donoho et al.
We now discuss the more general situation when this is not the case. A
reasonable way to begin modeling the general situation is to say that we have
two cumulative character counting functions N1 and N2, and that now
N2(t) = N(cid:2)
1(t) + M(t),
where N(cid:2)
1(t) is the cumulative counting function of the input character arrival
times, perhaps after a conservative stream transformation, and M is the cumu-
lative counting function of chaﬀ arrival times. In short, as we watch characters
coming in, some are from the input stream – only jittered – and others are chaﬀ.
Suppose that we once again compute the statistic Corr(j) at each scale.
What happens? The results of course change. Suppose for simplicity that the
chaﬀ arrives according to the universal keyclick interarrival process, and that
the chaﬀ arrival process is stochastically independent of the N1 process. Then
one can show that instead of
we actually have
Corr(j) → 1,
j → ∞,
Corr(j) → ρ,
j → ∞,
where 0 < ρ < 1. Here ρ may be interpreted as a ‘signal/(signal+noise)’ ratio,
meaning that the limiting value ρ can be very small if the relative amount of
chaﬀ is very large, whereas it will be close to 1 if the fraction of chaﬀ is negligible.
Nevertheless, no matter how small ρ might be, any nonzero value for ρ will
be detectable, at least for suﬃciently long-lived connections. Indeed, for large
enough n, it will be clear that the empirical ﬂuctuations in correlations due
to statistical sampling eﬀects are simply too small to cause observed values of
Corr(j) which are substantially nonzero. Given more space, we would provide
a detailed analysis of this eﬀect, showing that the mathematics predicts a sub-
stantial correlation between wavelet coeﬃcients of N1 and of N2. The analysis
is entirely parallel to the analysis given in earlier sections.
In short, although certainly the presence of chaﬀ causes a more complex
problem, the statistical tools and diagnostic approaches suggested for the no-
chaﬀ case seem to be equally applicable to the chaﬀ case.
9 Discussion
This paper has considered basic ‘proof of concept’ issues. In particular, we have
not discussed here the systems-level issues of working in a setting where there
may be many hundreds of active Telnet or SSH connections into and out of a
large site (say, a university or corporate network), and it is required to monitor
and detect stepping stones in real time. A typical issue would be to consider as
a monitoring interval a speciﬁc length of time (e.g. 4 minutes), and calculate
the proper height of the threshold for the ‘stepping stone alarm’ in order that
Multiscale Stepping-Stone Detection
33
in looking at thousands of pairs of connections which are truly uncorrelated,
we control the false alarm rate to a tolerable number of false alarms per day.
Obviously, it would be very important to study such issues carefully.
We have discussed here the fact that real interactive sessions seem to have
inter-keystroke times whose distribution is Pareto in the upper tail. In the anal-
ysis section of this paper we have considered Poisson streams, which are easy
to analyze. In the appendix, we show that the analysis can generalize to other
streams. This points out that an accurate theoretical model for inter-keystroke
timing is in order, so that we can focus attention and develop mathematical
analysis associated with that model. Such a correct model would be extremely
useful in practical terms, for example in systems-level work where it could used
for false alarm calibration purposes.
Two particular components of the inter-keystroke timing model which should
be considered more closely: (a) the correlation structure of adjacent/nearby inter-
keystroke times; and (b) the chance of seeing many characters in a very short
interval. The signiﬁcance of these can be gleaned from the discussion in the
appendix. Knowing more about either or both components would help mathe-
matical analysis and simulation accuracy.
There are also other sources of information that we haven’t discussed – the
key one being the two-way nature of interactive sessions. There is far more
information than just the keystrokes on the forward path through the stepping
stones, there are also the echoes and command output on the reverse path, and
it should be possible to use information about these to substantially improve
detection.
Acknowledgments. DLD and AGF would like to thank NSF ANI-008584
(ITR). AGF would like to thank the Statistics Department of UC Berkeley for its
hospitality. JC and SS would like to thank DARPA contract N66001-00-C-8045.
References
1. Aldous, D.L.: Probability Approximations Via the Poisson Clumping Heuristic.
Springer-Verlag, New York. January 1989
2. Lindgren, G., Leadbetter, M.R., and Rootzen, H.: Extremes and related proper-
ties of stationary sequences and processes. Springer, New York (1983). Russian
translation; Nauka: Moscow (1988).
3. Shimomura, T. and Markoﬀ, J.: Takedown. The pursuit and capture of Kevin Mit-
nick, America’s most wanted computer outlaw–by the man who did it. Hyperion.
December 1995.
4. Mallat, S.: A Wavelet Tour of Signal Processing. Academic Press. Second Edition,
2000.
5. Meyer, Y.:Wavelets: Algorithms and Applications. SIAM.May 1993
6. Stoll, C.: The Cuckoo’s Egg: Tracking a Spy through the Maze of Computer Espi-
onage. Pocket Books. October 2000
7. Staniford-Chen, S. and Heberlein, L.: Holding Intruders Accountable on the Inter-
net. Proceedings of the 1995 IEEE Symposium on Security and Privacy, Oakland,
CA (1995)
34
D. Donoho et al.
8. Zhang, Y. and Paxson, V.: Detecting stepping stones. Proceedings of the 9th
USENIX Security Symposium, Denver, Colorado, August 2000.
http://www.aciri.org/vern/papers/stepping-sec00.ps.gz
9. Paxson, V. and Floyd, S.: Wide-Area Traﬃc: The Failure of Poisson Modeling.
IEEE/ACM Transactions on Networking, Vol. 3(3),June 1995, 226–244
10. Wavelab Toolbox for Wavelet Analysis. Requires Matlab.
http://www-stat.stanford.edu/ wavelab
11. Yoda, K. and Etoh, H.: Finding a Connection Chain for Tracing Intruders, In:
Guppens, F.,Deswarte, Y., Gollamann, D. and Waidner, M. (eds): 6th European
Symposium on Research in Computer Security - ESORICS 2000 LNCS -1985,
Toulouse, France, Oct 2000
Appendix
Explanation of the Bumps/Wiggles Dichotomy
Analysis by ”multiscale bumps” provides, as explained above, a collection of
multiscale block averages. In other words, the coeﬃcients measure the rate of
typing of the given stream.
Analysis by ”multiscale wiggles” provides, as explained above, a collection of
multiscale diﬀerences of block averages. In other words, the coeﬃcients measure
the changes in the rate of typing of the given stream.
It is our opinion that measuring changes in rate, and noticing the times those
occur, provides more reliable evidence for the identity of two streams; so we
believe that analysis by multiscale wiggles (i.e. what is ordinarily called simply
wavelet analysis) will give more reliable information indicating the identity of
the two streams.
(There is one other advantage of wavelet analysis: the possibility of develop-
ing detectors for non-keystroke conserving schemes which work by the multiplex-
ing of constant-rate chaﬀ together with the original stream. Suppose that two
streams diﬀer in that stream 2 contains stream 1 along with characters from an
independent chaﬀ source of constant rate (e.g. Poisson with Rate 20 char/sec). It
can be shown, by elaborating the point of view here, that the wavelet coeﬃcients
at suﬃciently long scales will have a dependable correlation < 1, but which is
stable and nonzero, and determined by a kind of statistical signal/chaﬀ ratio.
So we might notice that two streams which should be completely uncorrelated
actually exhibit correlations which are deﬁnitely nonzero).
The diﬀerent normalization of the wavelet coeﬃcients in the two cases has
to do with the appropriate means of interpretation of each type of coeﬃcient.
Averages are directly interpretable in the units of the phenomenon being mea-
sured, no matter what the scale of the average. Diﬀerences are not so universally
interpretable; the convention p = 1/2 ensures that they are normalized accord-
ing to the square-root of interval size rather than interval size. The rationale is
that for typical point processes, the coeﬃcients at diﬀerent scales will then be
of comparable size.
Multiscale Stepping-Stone Detection
35
Generalization to Non-poisson Streams
The reader will note that only in two places in the argument of Section 6 did we
use the Poisson process assumption
The ﬁrst was
max{N1(t + ∆) − N1(t) : t, t + ∆ ∈ [a, b]} ≤ OP (log(b − a)) · E{N1(t + ∆) − N1(t)}.
This condition says that within any maximum tolerable delay interval, we are
very unlikely to see character counts dramatically greater than the average char-
acter counts. This inequality is extremely easy to satisfy and many point pro-
cesses will obey it. It is also the case that real data will obey it. We might
for example stipulate that no actual human person is ever going to exceed an
absolute maximum of K characters in ∆ no matter how long we wait. If we
do, the above inequality will automatically be true, because log(b − a) grows
unboundedly with observation period, while K is an absolute constant.
Incidentally, the Pareto nature of the upper half of the inter-keystroke timing
distribution described in [9] is entirely compatible with this inequality. Indeed,
the Pareto upper tail is responsible for occasional long dead spots in a stream,
where no characters emerge. It is the lower tail – near zero inter-keystroke spacing
– that determines whether the needed condition holds. The Poisson assumption
makes the inter-keystroke distribution have a density e−t/λ/λ which is of course
bounded near t = 0; this boundedness implies that there will not typically be
large numbers of events in a short time. It seems safe to say that this aspect of
the Poisson distribution accurately models real streams of keystrokes.
The second fact used was (in, say, the multiscale block averages case)
V ar[N1(0, T ]] (cid:11) Const · T
which says that the ﬂuctuation in the number of events per unit time within an
interval grows like the square root of the interval size. This will be true for many
stationary point processes.
Now, the Pareto nature of the upper half of the inter-keystroke timing dis-
tribution described in [9], and the possibility of a non-i.i.d. behavior of inter-
keystroke times can modify this inequality, even making the variability grow like
a power T β with β (cid:12)= 1. A more detailed analysis shows that even though the
variance scaling exponents could be diﬀerent, the fundamental behavior of the
corresponding terms in the analysis would be the same.
Since our simulations indicate that the multiscale diagnostics work very well
in the Pareto case, we omit further discussion of the mathematical details of the
extension.
Detecting Malicious Software by Monitoring
Anomalous Windows Registry Accesses
Frank Apap, Andrew Honig, Shlomo Hershkop, Eleazar Eskin, and Sal Stolfo
Department of Computer Science
Columbia University, New York NY 10027, USA
{fapap, arh, shlomo, eeskin, sal}@cs.columbia.edu
Abstract. We present a host-based intrusion detection system (IDS)
for Microsoft Windows. The core of the system is an algorithm that de-
tects attacks on a host machine by looking for anomalous accesses to the
Windows Registry. The key idea is to ﬁrst train a model of normal reg-
istry behavior on a windows host, and use this model to detect abnormal
registry accesses at run-time. The normal model is trained using clean
(attack-free) data. At run-time the model is used to check each access
to the registry in real time to determine whether or not the behavior is
abnormal and (possibly) corresponds to an attack. The system is eﬀec-
tive in detecting the actions of malicious software while maintaining a
low rate of false alarms
1 Introduction
Microsoft Windows is one of the most popular operating systems today, and also
one of the most often attacked. Malicious software running on the host is often
used to perpetrate these attacks. There are two widely deployed ﬁrst lines of
defense against malicious software, virus scanners and security patches. Virus
scanners attempt to detect malicious software on the host, and security patches
are operating systems updates to ﬁx the security holes that malicious software
exploits. Both of these methods suﬀer from the same drawback. They are e.ective
against known attacks but are unable to detect and prevent new types of attacks.
Most virus scanners are signature based meaning they use byte sequences
or embedded strings in software to identify certain programs as malicious [10,
24]. If a virus scanner’s signature database does not contain a signature for a
speciﬁc malicious program, the virus scanner can not detect or protect against
that program. In general, virus scanners require frequent updating of signature
databases, otherwise the scanners become useless [29]. Similarly, security patches
protect systems only when they have been written, distributed and applied to
host systems. Until then, systems remain vulnerable and attacks can and do
spread widely.
In many environments, frequent updates of virus signatures and security
patches are unlikely to occur on a timely basis, causing many systems to remain
vulnerable. This leads to the potential of widespread destructive attacks caused
by malicious software. Even in environments where updates are more frequent,
A. Wespi, G. Vigna, and L. Deri (Eds.): RAID 2002, LNCS 2516, pp. 36–53, 2002.
c(cid:1) Springer-Verlag Berlin Heidelberg 2002
Detecting Malicious Software
37
the systems are vulnerable between the time new malicious software is created
and the time that it takes for the software to be discovered, new signatures and
patches created by experts, and the ultimate distribution to the vulnerable sys-
tems. Since malicious software may propagate through email, often the malicious
software can reach the vulnerable systems long before the updates are available.
A second line of defense is through IDS systems. Host-based IDS systems
monitor a host system and attempt to detect an intrusion. In the ideal case, an
IDS can detect the eﬀects or behavior of malicious software rather then distinct
signatures of that software. Unfortunately, the commercial IDS systems that are
widely in use are based on signature algorithms. These algorithms match host
activity to a database of signatures which correspond to known attacks. This ap-
proach, like virus detection algorithms, require previous knowledge of an attack
and is rarely eﬀective on new attacks. Recently however, there has been growing
interest in the use of data mining techniques such as anomaly detection, in IDS
systems [23,25]. Anomaly detection algorithms build models of normal behavior
in order to detect behavior that deviates from normal behavior and which may
correspond to an attack [9,12]. The main advantage of anomaly detection is that
it can detect new attacks and can be an eﬀective defense against new malicious
software. Anomaly detection algorithms have been applied to network intrusion
detection [12,20,22] and also to the analysis of system calls for host based in-
trusion detection [13,15,17,21,28]. There are two problems to the system call