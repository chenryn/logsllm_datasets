The data clearly show that IP address and useragent are not
very powerful on their own; they begin to detect a reasonable
number of account takeovers when combined, and when the
features are weighted via logistic regression they offer good
detection capability. The data also show that adding levels to
the hierarchy gives a small but noticeable performance boost.
As for the optimal smoothing method, it is clear that setting
µhk = 1 is the poorest choice. However, when µhk = |hk| the
backoff and interpolation methods are essentially equivalent,
with interpolation showing a very small edge.
Given these results, we choose as our best model a logistic
regression model over IP and useragent features using the (cid:96) =
4 hierarchy, smoothed via interpolation with µhk = |hk|. Using
this model we evaluated performance on the botnet attack and
the compromised accounts separately. We found that at 10%
FPR we could detect 95% of the botnet attack and 77% of
the compromised accounts. Thus by relaxing our allowed FPR
to 10% we can improve the detection rate of compromised
accounts by a factor of 12, while only letting an additional
4% of botnet accounts through.
E. Experiments: Evasion Attacks
To test our model against attackers with various levels of
sophistication, we simulated attack datasets for four types of
attacks. We assume that at a minimum all of the victims’
passwords are known to the attacker; in some cases the attacker
has more information. In each case we simulated the attack by
randomly sampling members from the historical period and
assigning each attacked member an IP address and useragent
as discussed in the following.
1) Password-only attacker: This is a known-credentials
attack in which the attacker does not possess any information
beyond the target members’ credentials. The attacker writes a
simple script using an off-the-shelf package and uses a hosting
provider to launch the attack. To simulate this attack, we
2) Botnet attacker: This is also a known-credentials attack
similar to the previous scenario except that the attacker here
has more resources at his disposal. The attacker in this case
knows that he should vary IP addresses and useragents to
get around simple rate-limiting and bot detection. He thus
employs a botnet to launch the attack and randomizes his
useragent string. This would imply that both IPs and useragent
of login attempts would appear to be distributed. To simulate
this attack, we sampled IPs weighted by their risk score and
sampled useragents uniformly at random from the historical
data set. Our best model did well at detecting this attack, with
AUC 0.992 and 99% TPR at 10% FPR. As expected, using
useragent features only was useless in detecting this attack:
using useragent features alone we could only detect less than
0.2% of true positives at 10% FPR.
3) Researching attacker: Moving on to a more sophisti-
cated attack, we assume that the attacker in this case scrapes
information about the target’s country. The attacker then uses
proxies only from the target’s country to make the logins look
less suspicious. To simulate this attack, we sample members
with at least one successful login from a target country (United
States in our experiments). We then sample IPs from this
country weighted by their risk score and use one common
useragent across all simulated login attempts. Our best model
did well at detecting this attack, with AUC 0.985 and 99%
TPR at 10% FPR.
4) Phishing attacker: Most motivated of all attacker types,
we assume that this category of attacker has been successfully
able to phish its targets thereby obtaining useragent of their
browser and IP address. With this information, the attacker
uses the exact same useragent and an IP from the same country
as the victim to launch the attack. To simulate this attack
type, we sample member sessions from training set. For these
members we use the same useragent and sample an IP from
the same country, again weighted by the IP risk score. As
11
Attacker
Password-only
Botnet (simulated)
Researching
Phishing
Botnet (real)
Compromised accounts
AUC
0.999
0.992
0.985
0.924
0.969
0.934
TPR at 10% FPR
1.00
0.99
0.99
0.74
0.95
0.77
TABLE III: Performance of our best model under attack.
expected, this was the hardest attack to detect. Our best model
gave AUC 0.924 and 74% TPR at 10% FPR.
A summary of our experiment results is given in Table III,
while the corresponding ROC curves are shown in Fig 4.
We also include results on the two labeled sets of account
takeovers from the LinkedIn data, i.e., the (real) botnet attack
and the compromised accounts. Notably, the performance of
our system improves against the less knowledgeable attackers
simulated in this experiment, since the real attacks present in
the LinkedIn data are actually more sophisticated.
F. Experiment: Feature Effects
Reputation systems. To test
the effect of the reputation
systems on our model, we evaluated our best model on the
same training and validation set as above, but set the reputation
scores p(A|x) to be identical for all feature values x. We found
that our ability to detect the botnet attack actually improved,
from 95% at 10% FPR to 99%. On the other hand, our ability
to detect compromised accounts degraded, going from 77% to
60% at 10% FPR. These results demonstrate that LinkedIn’s
reputation systems do reﬂect the probability of attack to some
extent; they also show that the particular botnet incident in
question came from IP addresses and/or useragents that were
not scored as particularly abusive by LinkedIn.
As for the attack data, performance on Attacks 1 and 2
remained strong 99% and 97% TP at 10% FPR, respectively,
while performance on Attack 3 decreased to 80% and detection
of Attack 4 dropped dramatically, to 19%. These results show
that for a motivated attacker having a robust and accurate set
of reputation scores is essential to repel the attack.
Member history data. We wanted to test the hypothesis that
members with more logins in their history are better protected
by our model. To do this we split the member set into two
parts: those with 8 or more logins in their history (27% of
our data set) and those with fewer than 8 logins (73% of our
data set) and evaluated our best model on each. We found that
for members with less history, compromised accounts were
slightly less likely to be detected (75% vs. 77% at 10% FPR)
and botnet victims were slightly more likely to be detected
(97% vs. 94%). We believe that this difference is mostly due to
the fact that botnet victims were proportionally overrepresented
amongst the group with few logins.
For Attacks 1–3 the model performance was essentially the
same on both segments. For Attack 4 we found that members
with more history were better protected; this is due to the fact
that the attacker picked a session randomly from the member’s
history to emulate, so if the member has little history then the
estimates for p(x|u, L) will be higher for the emulated session
than if the member has many events in her history.
VI. CLASSIFYING FEATURES
Individual features chosen for our model have various
properties that are relevant for reinforced authentication. We
now propose a classiﬁcation along three axes that allows us to
study and compare their properties and their usefulness.
Phishability. This characteristic concerns the resistance of a
feature to phishing attacks. In a traditional phishing attack,
the attacker records the username/password pair, which allows
him access to the service A. If the service A uses a scheme
such as the one studied in this work, simply recording the
username and password will not give the attacker access to A.
However, an attacker can adapt to the new situation and, in
addition to username and password, can record all the features
that normally would be recorded by the service A, and can
utilize the gained information when accessing A.
We say a feature is phishable if observing a single instance
of an accepted login allows an attacker to fake that speciﬁc
feature with high probability. We say a feature is non-phishable
if observing a single login allows the attacker to fake that
feature with negligible probability only. Some features may
be learnable not from a single observed login but from a
few. Luring the same user to a phishing site more than once
is substantially more complicated, in particular if there are
constraints such as that the logins must be consecutive. Such
features pose much less of a problem, but may need to be
considered in some circumstances.
Cost to mimic. Even phishable features can be useful in
identifying a phishing attack, as an attacker needs to mimic
the feature when logging in, which comes at a cost.
We say a feature is cheap to mimic (resp., expensive
to mimic) if mimicking the feature incurs minimal (resp.,
high) cost to the attacker. Here “cost” can refer to different
quantities: money, time required for coding, time required for
execution, and others. Also, whether a cost is deemed “high”
or “low” is most likely application-speciﬁc and outside the
scope of this discussion.
Accuracy. Discriminating properties of the different features
used in the model vary. Features with high accuracy (resp., low
accuracy) are, if matching, a strong (resp., weak) indication
that the login attempt is legitimate. The relative accuracy of
different features can be assessed by evaluating the perfor-
mance of a number of one-feature classiﬁers in a given model
framework.
A. Reinforced Authentication Features
We now apply this classiﬁcation to some features that are
used for reinforced authentication and discuss their properties.
The source IP, as well as the directly related source ASN
and source country, are phishable features; i.e., a phishing
attack immediately gives away a valid source IP. However, they
still constitute reasonable features since they are moderately
expensive to mimic. Mimicking the exact source IP is usually
quite hard; however, having a source IP from the same ASN
12
will usually be sufﬁcient. Mimicking still requires effort, e.g.,
by having access to a botnet that performs the login attempts
from a bot in the required ASN or country. Another quality is
the high accuracy of the source IP, as established in Sect. V.
We discuss here two different variants of the useragent
feature. The useragent string, i.e., the identiﬁer transmitted
as part of the HTTP header, is easily phishable. Unlike the IP
address, the useragent string is very cheap to mimic by simply
adapting the HTTP header ﬁeld. This feature is less accurate
than IP address, as demonstrated in Sect. V. However, one
does not need to rely on the useragent string to determine the
software running on the user’s machine. Browser ﬁngerprinting
techniques can be used to gather precise information on the
software running, including the browser, browser extensions,
the OS, and if it is running inside a virtual machine. While
the resulting feature is still phishable, precisely simulating
all the gathered information can become costly, which makes
the feature moderately expensive to mimic, in particular when
targeting more than a very limited number of accounts.
Time between login events is an example of a feature that is
non-phishable, and thus provides a different quality of security.
Cost to mimic can be substantial, as the attacker has to learn
from multiple (consecutive) successful login attempts by the
victim and wait a speciﬁc time between login attempts. While
we did not incorporate this feature into our models, a rough
analysis of the LinkedIn data suggests that the feature is of low
accuracy since the distributions of the feature for legitimate
and attack events are very similar.
VII. RELATED WORK
The weakness of passwords has been understood for a
long time. Password re-use is problematic as leakage from a
single password puts multiple accounts of a single user at risk.
Studies have consistently shown that users re-use passwords,
and even re-use passwords from high-value accounts on low-
value accounts [25], [1], [17]. Weak passwords is also a widely
known problem. The strength of user-chosen passwords against
password guessing attacks has been studied since the early
times of password-based authentication [8], [56], [40] Current
techniques for password guessing are Markov models [44],
[21], [37] and probabilistic context-free grammars [55]; state-
of-the-art tools include John the Ripper [51] and HashCat [52].
Historically, the strength of passwords against guessing attacks
has been assessed by using password crackers to ﬁnd weak
passwords [42]. Recently much more precise techniques have
been developed [8], [50], [13], [18], [22].
One common alternative to password-based authentication
is using authentication tokens, usually in two-factor authen-
tication. The authentication token can either be a hardware
token, an app running on a smart-phone, or a second com-
munication channel such as a mobile phone number. Security
tokens offer a high security level, if implemented and used
correctly, and are implemented at most major websites as an
optional feature. However, adoption rates are low, as users are
often unwilling to carry around the security token, which can
be lost and stolen, and needs to be connected to the device for
each authentication request. Furthermore, managing tokens for
Internet-wide service with a diverse user-base is a challenge.
Another common alternative is biometric authentication, based
on ﬁngerprints [39], face recognition [34],
typing dynam-
ics [28], [41], [32], [19], or many other factors. Biometric
schemes are rarely implemented for large online services, as
they often require special hardware on the client side, are
difﬁcult to implement securely for remote logins, and raise
privacy concerns.
There is very little work available that considers classifying
suspicious behavior at login time. While it is obvious from
personal experience that a number of websites use some
form of reinforced authentication, very few details about their
systems are known and the effectiveness has never been
publicly discussed. In a presentation [46], a Facebook engineer
gives some idea about their system at the time, which uses a
whitelist based on browser cookies, IP ranges, and geolocation,
a blacklist based on several known attack patterns (such as
many failed login attempts from an IP), and some undisclosed
additional mechanisms. A high-level comparison of commer-
cially available tools for protecting online accounts (named
risk-based authentication) is available from Forrester [38],
but no technical details are provided. RSA provides some
information about it’s risk-based authentication solution [24],
but again the disclosed information is minimal. The lack of
public evaluation is against the standards typically applied to
cryptography and security, thus risking that weaknesses found
by adversaries may get unnoticed.
Related systems are in place monitoring transactions in
ﬁnancial networks. For credit card transactions in particular,
automatic classiﬁers are used to select possibly fraudulent
transactions for manual review. An RSA white paper gives
some insight
into RSA’s solution for credit card protec-
tion [23]. Florencio et al. [26] mention that, for ﬁnancial online
accounts, the security is indirectly given by the fraud-detection
back-end, which uses machine learning as well.
In a recent survey paper, Bonneau et al. [10] give an