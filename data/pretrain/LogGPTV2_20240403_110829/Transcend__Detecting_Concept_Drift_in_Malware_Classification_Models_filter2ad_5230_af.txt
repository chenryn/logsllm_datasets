sional data sequences using ex-changeability [8, 9]. Prior
works [6, 7] use conformal prediction to detect deviation
of the data sequence from independent and identically
distributed (iid) assumption which could be caused by
concept drift. The drift is measured by creating a martin-
gale function. If the data is not iid, then the conformal
predictor outputs an invalid result. Some p-values as-
signed to the true hypotheses about data labels are too
small (or have another deviation from uniformity), and
this leads to high values of the martingale. However,
this martingale approach does not use p-values assigned
to wrong hypotheses, which is another cause of wrong
classiﬁcation, e.g., malicious samples being classiﬁed as
benign. We consider this information to be important be-
cause in the case of malware evolution, malicious sam-
ples are often specially designed to be indistinguishable
from benign samples, therefore they tend to get high p-
values assigned to wrong hypotheses. Additionally, the
martingale approach uses true labels to study the drift of
data without making any predictions, in contrast our ap-
proach does not have access to true labels and analyses
the predictions made by a given model.
Comparison with Conformal Predictor. Although
USENIX Association
26th USENIX Security Symposium    637
conformal evaluator is built on top of conformal predic-
tor (CP), it does not share the same weaknesses as that of
other solutions based on it [6, 7]. Fern and Dietterich10
also show that CP is not suited for anomaly detection as
it outputs a set of labels and hence needs to be modiﬁed
to predict quality of predictions. We further highlight the
differences between CP and CE that makes CE better-
suited to the concept drift detection task.
Conformal Predictor [24] (CP) is a machine learning
classiﬁcation algorithm.
It relies on a non-conformity
measure (NCM) to compute p-values in a way similar to
CE. For each classiﬁcation task, CP builds on such p-
values to introduce credibility—the class, in a classiﬁca-
tion problem, with the highest p-value and conﬁdence—
deﬁned as one minus the class with the second highest
p-value (these metrics are different from CE metrics, see
§ 2.4). The CP algorithm then outputs either a single
class prediction with the identiﬁed credibility and con-
ﬁdence, or, given a ﬁxed conﬁdence level 1− ε (where
ε represents the signiﬁcance level), a prediction set that
includes classes that are above it. This set is proven to
cover the true class with probability not lower than 1−ε.
CE dissects CP metrics and to extract its p-values cal-
culation. The p-values are used together with the out-
put labels provided by the algorithm under evaluation,
to build CE metrics. CP ignores these labels as it tries
to predict them. Conversely, CE uses this information to
provide quality metrics to assess the quality of the encap-
sulated algorithm. This change is of paramount impor-
tance to derive the thresholds (computed by Transcend)
used to accept or reject a prediction.
The posterior use of the labels is a key feature that en-
ables CE to detect concept drift. On the contrary, CP is
designed as a predictive tool making only use of prior in-
formation. Since labels are important pieces of informa-
tion, CE uses them to build its metrics and assessments
(see, § 2.4 and § 3). The labels used by CE are the ones
of the training samples and not the labels of the testing
samples that are unavailable at the time of classiﬁcation.
7 Conclusions
We presented Transcend—a fully tunable tool for sta-
tistically assessing the performance of a classiﬁer and
ﬁltering out unreliable classiﬁcation decisions. At the
heart of Transcend, CE’s statistical conﬁdence provides
evidence for better understanding model generalization
and class separation; for instance, CE has been suc-
cessfully adopted to selectively invoke computationally
expensive learning-based algorithms when predictions
choose classes with low conﬁdence [4], trading off per-
formance for accuracy. Our work details the CE metrics
used in [4] and extend it to facilitate the identiﬁcation of
concept drift, thus bridging a fundamental research gap
when dealing with evolving malicious software.
We present two case studies as representative use cases
of Transcend. Our approach provides sound results for
both binary and multi-class classiﬁcation scenarios on
different datasets and algorithms using proper training,
calibration and validation, and testing datasets. The di-
versity of case studies presents compelling evidence in
favor of our framework being generalizable.
Availability
We encourage the adoption of Transcend in machine
learning-based security research and deployments; fur-
ther information is available at:
https://s2lab.isg.rhul.ac.uk/projects/ce
Acknowledgments
This research has been partially supported by the
UK EPSRC grants EP/K033344/1, EP/L022710/1 and
EP/K006266/1. We gratefully acknowledge the sup-
port of NVIDIA Corporation with the donation of the
Tesla K40 GPU used for this research. We are equally
thankful to the anonymous reviewers’ comments and
Roberto Perdisci, our shepherd, for their invaluable com-
ments and suggestions to improve the paper. Also,
we thanks Technology Integrated Health Management
(TIHM) project awarded to the School of Mathematics
and Information Security at Royal Holloway as part of
an initiative by NHS England supported by Innovate UK.
We also thank the authors of [2], for their public dataset
used in our evaluation, and Mansour Ahmadi for provid-
ing us the algorithm used in [1].
References
[1] AHMADI, M., ULYANOV, D., SEMENOV, S., TROFIMOV, M.,
AND GIACINTO, G. Novel feature extraction, selection and fu-
sion for effective malware family classiﬁcation. In Proceedings
of the Sixth ACM Conference on Data and Application Security
and Privacy (New York, NY, USA, 2016), CODASPY ’16, ACM,
pp. 183–194.
[2] ARP, D., SPREITZENBARTH, M., HUBNER, M., GASCON, H.,
AND RIECK, K. DREBIN: effective and explainable detection
of android malware in your pocket. In 21st Annual Network and
Distributed System Security Symposium, NDSS 2014, San Diego,
California, USA, February 23-26, 2014 (2014).
[3] BREIMAN, L. Random Forests. Machine Learning 45, 1 (2001),
5–32.
10A. Fern and T. Dietterich. “Toward Explainable Uncertainty”.
https://intelligence.org/files/csrbai/fern-slides-1.pdf
[4] DASH, S. K., SUAREZ-TANGIL, G., KHAN, S. J., TAM, K.,
AHMADI, M., KINDER, J., AND CAVALLARO, L. Droidscribe:
638    26th USENIX Security Symposium
USENIX Association
Classifying android malware based on runtime behavior. In 2016
IEEE Security and Privacy Workshops, SP Workshops 2016, San
Jose, CA, USA, May 22-26, 2016 (2016), pp. 252–261.
[5] DEO, A., DASH, S. K., SUAREZ-TANGIL, G., VOVK, V., AND
CAVALLARO, L. Prescience: Probabilistic guidance on the re-
training conundrum for malware detection. In Proceedings of the
2016 ACM Workshop on Artiﬁcial Intelligence and Security (New
York, NY, USA, 2016), AISec ’16, ACM, pp. 71–82.
[6] FEDOROVA, V., GAMMERMAN, A. J., NOURETDINOV, I., AND
VOVK, V. Plug-in martingales for testing exchangeability on-
line. In Proceedings of the 29th International Conference on Ma-
chine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 -
July 1, 2012 (2012).
[7] HO, S. A martingale framework for concept change detection in
time-varying data streams. In Machine Learning, Proceedings of
the Twenty-Second International Conference (ICML 2005), Bonn,
Germany, August 7-11, 2005 (2005), pp. 321–327.
[8] HO, S., AND WECHSLER, H. Query by transduction.
IEEE
Trans. Pattern Anal. Mach. Intell. 30, 9 (2008), 1557–1571.
[9] HO, S., AND WECHSLER, H. A martingale framework for de-
tecting changes in data streams by testing exchangeability. IEEE
Trans. Pattern Anal. Mach. Intell. 32, 12 (2010), 2113–2127.
[10] HUBERT, M., AND VANDERVIEREN, E. An adjusted boxplot for
skewed distributions. Computational Statistics and Data Analysis
52, 12 (2008), 5186 – 5201.
[11] KAGGLE INC.
(BIG 2015).
lenge
malware-classification, 2015.
Microsoft Malware Classiﬁcation Chal-
https://www.kaggle.com/c/
[12] KANTCHELIAN, A., AFROZ, S., HUANG, L., ISLAM, A. C.,
MILLER, B., TSCHANTZ, M. C., GREENSTADT, R., JOSEPH,
A. D., AND TYGAR, J. D. Approaches to adversarial drift.
In AISec’13, Proceedings of the 2013 ACM Workshop on Artiﬁ-
cial Intelligence and Security, Co-located with CCS 2013, Berlin,
Germany, November 4, 2013 (2013), pp. 99–110.
[13] LI, P., LIU, L., GAO, D., AND REITER, M. K. On challenges in
evaluating malware clustering. In Recent Advances in Intrusion
Detection, 13th International Symposium, RAID 2010, Ottawa,
Ontario, Canada, September 15-17, 2010. Proceedings (2010),
pp. 238–255.
[14] LINDORFER, M., NEUGSCHWANDTNER, M., AND PLATZER,
C. MARVIN: efﬁcient and comprehensive mobile app classiﬁ-
In 39th IEEE An-
cation through static and dynamic analysis.
nual Computer Software and Applications Conference, COMP-
SAC 2015, Taichung, Taiwan, July 1-5, 2015. Volume 2 (2015),
pp. 422–433.
[15] MAGGI, F., ROBERTSON, W. K., KR ¨UGEL, C., AND VIGNA,
G. Protecting a moving target: Addressing web application con-
cept drift. In Recent Advances in Intrusion Detection, 12th Inter-
national Symposium, RAID 2009, Saint-Malo, France, September
23-25, 2009. Proceedings (2009), pp. 21–40.
[16] MARICONTI, E., ONWUZURIKE, L., ANDRIOTIS,
P.,
DE CRISTOFARO, E., ROSS, G., AND STRINGHINI, G.
Mamadroid: Detecting android malware by building markov
chains of behavioral models. arXiv preprint arXiv:1612.04433
(2016).
[17] PLATT, J., ET AL. Probabilistic outputs for support vector ma-
chines and comparisons to regularized likelihood methods. Ad-
vances in large margin classiﬁers 10, 3 (1999), 61–74.
[18] RIDGEWAY, G. The state of boosting. Computing Science and
Statistics (1999), 172–181.
[19] RIECK, K., HOLZ, T., WILLEMS, C., D ¨USSEL, P., AND
LASKOV, P. Learning and classiﬁcation of malware behavior. In
Detection of Intrusions and Malware, and Vulnerability Assess-
ment, 5th International Conference, DIMVA 2008, Paris, France,
July 10-11, 2008. Proceedings (2008), pp. 108–125.
[20] SHAFER, G., AND VOVK, V. A tutorial on conformal prediction.
The Journal of Machine Learning Research 9 (2008), 371–421.
[21] TANG, Y. extreme gradient boosting. https://github.com/
dmlc/xgboost.
[22] TEGELER, F., FU, X., VIGNA, G., AND KRUEGEL, C.
Botﬁnder: Finding bots in network trafﬁc without deep packet
inspection. In In Proc. Co-NEXT 12 (2012), pp. 349–360.
[23] THOMAS, K., GRIER, C., MA, J., PAXSON, V., AND SONG, D.
Design and evaluation of a real-time URL spam ﬁltering service.
In 32nd IEEE Symposium on Security and Privacy, S&P 2011,
22-25 May 2011, Berkeley, California, USA (2011), pp. 447–462.
[24] V. VOVK, A. G., AND SHAFER, G. Algorithmic learning in a
random world. Springer-Verlag New York, Inc., 2005.
[25] WECHSLER, H. Cyberspace security using adversarial learning
and conformal prediction. Intelligent Information Management
7, 04 (2015), 195.
Appendix A.
Figure 5: Multiclass Classiﬁcation Case Study: Element
kept during the test of the new class. The test elements
belong to a new class so every samples kept will be miss-
classiﬁed. The net separation between good and bed per-
formance comes from the perfect classiﬁcation of train-
ing samples used to derived the thresholds.
USENIX Association
26th USENIX Security Symposium    639
0.00.20.40.60.81.0ElementKeptthreshold0.00.20.40.60.81.0PerformancethresholdP-value:ElementkeptofUnknownClass0.00.10.20.30.40.50.60.70.80.91.0(a) Performance of p-value driven threshold
for element above the threshold.
(b) Performance of probability driven
threshold for element above the threshold.
(c) Performance difference between p-
value and probability for element above the
threshold.
(d) Performance of p-value driven thresh-
old for element below the threshold.
(e) Performance of probability driven
threshold for element below the threshold.
(f) Performance difference between p-value
and probability for element below the
threshold.
(g) Number of element above the p-value
threshold.
(h) Number of element above the probabil-
ity threshold.
(i) Difference between the number of ele-
ment above the threshold between p-value
and probability.
Figure 6: Binary Classiﬁcation Case Study [2]: complete comparison between p-value and probability metrics. Across
all the threshold range we can see that the p-value based thresholding is providing better performance than the proba-
bility one, discarding the samples that would have been incorrectly classiﬁed if kept.
640    26th USENIX Security Symposium
USENIX Association
1st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignPerformance above thresholdby P-value0.8000.8250.8500.8750.9000.9250.9500.9751st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignPerformance above thresholdby Probability0.720.750.780.810.840.870.900.930.961st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignPerformance difference0.020.040.060.080.100.120.140.161st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignPerformance below thresholdby P-value0.280.320.360.400.440.480.520.560.601st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignPerformance below thresholdby Probability0.5100.5250.5400.5550.5700.5850.6000.6151st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignPerformance difference below threshold0.240.210.180.150.120.090.060.030.001st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignElement above thresholdby p-value0.100.150.200.250.300.350.400.450.501st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignElement above thresholdby probability0.120.160.200.240.280.320.361st QT2nd QT3rd QTThreshold malicious2nd QT3rd QTThreshold benignElement above threshold difference0.000.020.040.060.080.100.12Figure 7: Multiclass Classiﬁcation Case Study [1]: P-value distribution for samples of Tracur family omitted from the
training dataset; as expected, the values are all close to zero.
Figure 8: Multiclass Classiﬁcation Case Study [1]: probability distribution for samples of Tracur family omitted from
the training dataset. Probabilities are higher then zero and not equally distributed across all the families, making the
classiﬁcation difﬁcult. It is worth noting some probabilities are skewed towards large values (i.e., greater than 0.5)
further hindering a correct classiﬁcation result.
USENIX Association
26th USENIX Security Symposium    641
Prediction:RamnitPrediction:LollipopPrediction:Kelihos_ver3Prediction:VundoPrediction:Kelihos_ver1Prediction:Obfuscator.ACYPrediction:Gatak0.00.20.40.60.81.0P-valuesP-values: RamnitP-values: LollipopP-values: Kelihos_ver3P-values: VundoP-values: Kelihos_ver1P-values: Obfuscator.ACYP-values: GatakPrediction:RamnitPrediction:LollipopPrediction:Kelihos_ver3Prediction:VundoPrediction:Kelihos_ver1Prediction:Obfuscator.ACYPrediction:Gatak0.00.20.40.60.81.0ProbabilitiesProbabilities: RamnitProbabilities: LollipopProbabilities: Kelihos_ver3Probabilities: VundoProbabilities: Kelihos_ver1Probabilities: Obfuscator.ACYProbabilities: GatakFigure 9: Multiclass Classiﬁcation Case Study [1]: a new family is discovered by relying on the p-value distribution
for known malware families. The ﬁgure shows the amount of conformity each sample has with its own family; for
each sample, there is only one family with high p-value.
Figure 10: Multiclass Classiﬁcation Case Study [1]: probability distribution for samples of families included in the
training dataset. High probabilities support the algorithm classiﬁcation choice.
642    26th USENIX Security Symposium
USENIX Association
Ramnit's samplesLollipop's samplesKelihos_ver3's samplesVundo's samplesKelihos_ver1's samplesObfuscator.ACY's samplesGatak's samples0.00.20.40.60.81.0P-valuesP-values: RamnitP-values: LollipopP-values: Kelihos_ver3P-values: VundoP-values: Kelihos_ver1P-values: Obfuscator.ACYP-values: GatakRamnit's samplesLollipop's samplesKelihos_ver3's samplesVundo's samplesKelihos_ver1's samplesObfuscator.ACY's samplesGatak's samples0.00.20.40.60.81.0P-valuesP-values: RamnitP-values: LollipopP-values: Kelihos_ver3P-values: VundoP-values: Kelihos_ver1P-values: Obfuscator.ACYP-values: Gatak