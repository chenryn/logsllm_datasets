+
×
×
++++
+
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
0.000
0.117
0.234
0.351
0.468
0.585
0.702
0.819
0.936
1.053
1.170
1.287
Figure 4: Timing of network packets observed by a McTiny client.
consuming 46 028 bytes including per-packet overhead) than
CUBIC did, and also that BBR sent more data between time
0.702 and time 0.819 than CUBIC did, saving time overall.
The bottom line is that, because of congestion control, TCP
takes about 9.1 round-trip times to send 1MB using CUBIC,
or 8.5 round-trip times to send 1MB using BBR. Smaller
congestion-control details also affect the latency: e.g., raising
the initial packet limit from 10 to 32 would have saved more
than 1 round-trip time.
A.3 Building McTiny Congestion Control
We decided to integrate TCP-like congestion control into our
McTiny software. TCP itself is incompatible with the concept
of a tiny network server, but, as mentioned earlier, congestion
control can be managed entirely by the client.
There is a software-engineering problem here. Congestion-
control software is typically developed as part of a monolithic
TCP network stack, and interacts with the rest of the network
stack through a thick interface, so reusing the software out-
side the TCP context is difﬁcult. There have been efforts to
build reliable network protocols on top of UDP, and some of
these protocols—e.g., Google’s QUIC [30]—imitate TCP’s
congestion-control mechanisms, but again we did not ﬁnd
something easy to reuse.
We thus wrote yet another implementation of congestion
control. We put some effort into designing a simple inter-
face for future reusability, taking some API ideas from [24]
but building a userspace library rather than a tool designed
to integrate with the OS kernel. We ﬁrst implemented CU-
BIC but found that the bursts of trafﬁc in CUBIC frequently
overload UDP buffers (which are typically conﬁgured by the
OS with less space than TCP buffers), creating packet losses
and often considerable slowdowns. We considered variants of
CUBIC with pacing but in the end threw CUBIC away and
implemented BBR. As explained in [10], BBR handles packet
loss much better than CUBIC, and tends to avoid overloading
buffers in the ﬁrst place.
A.4 Measuring McTiny Congestion Control
Figure 4 shows an example of the timing of all of the net-
work packets in one McTiny run between the computer in
the United States and the computer in Europe. The CPUs on
these computers were, respectively, a quad-core 3.1GHz Intel
Xeon E3-1220 v3 (Haswell) and a quad-core 3.5GHz Intel
Xeon E3-1275 v3 (Haswell). The elapsed client time mea-
sured by time was 1.664 seconds, including 0.423 seconds
of “user” CPU time (on a single core; this is about 6% of the
time available on a quad-core CPU in 1.664 seconds of real
time) and 0.009 seconds of “sys” CPU time. Most of the CPU
time is for generating an ephemeral McEliece key, which the
client could have done any time in advance.
The total vertical spacing in the ﬁgure covers 1.268 seconds,
about 10.9 round-trip times. Each packet is shown at the time
it is sent or received by the client. For comparison, Figures 2
and 3 show times on the server, but in those cases the 1MB
of data was being sent by the server whereas in Figure 4 the
1MB of data is being sent by the client.
As the ﬁgure shows, our BBR implementation paces pack-
ets somewhat more smoothly than the Linux TCP BBR imple-
mentation, but overall we increase rate along essentially the
same curve as in Figure 3. The last few round trips in McTiny
transmit much less data; the red, blue, and green curves are
close to vertical at this point. There is more data sent and re-
ceived in Figure 4 than in Figure 3—there is more overhead in
each packet for cryptographic protection, data is sent in some-
what smaller packets, and each packet is acknowledged—but
this makes relatively little difference in latency.
To summarize, our McTiny software is using the network
in this example with similar efﬁciency to TCP, plus two round-
trip times for ﬁnal cleanup in the McTiny protocol. For our
software, as for TCP, the ﬁrst megabyte of data sent through
this network is limited primarily by congestion control.
1748    29th USENIX Security Symposium
USENIX Association