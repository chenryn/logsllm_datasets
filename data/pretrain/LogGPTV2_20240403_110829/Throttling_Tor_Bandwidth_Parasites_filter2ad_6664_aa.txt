title:Throttling Tor Bandwidth Parasites
author:Rob Jansen and
Paul F. Syverson and
Nicholas Hopper
Throttling Tor Bandwidth Parasites
Rob Jansen
Paul Syverson
U.S. Naval Research Laboratory
{rob.g.jansen, paul.syverson}@nrl.navy.mil
Nicholas Hopper
University of Minnesota
PI:EMAIL
Abstract
Tor is vulnerable to network congestion and performance
problems due to bulk data transfers. A large fraction of
the available network capacity is consumed by a small
percentage of Tor users, resulting in severe service degra-
dation for the majority. Bulk users continuously drain
relays of excess bandwidth, creating new network bot-
tlenecks and exacerbating the effects of existing ones.
While this problem may currently be attributed to ratio-
nal users utilizing the network, it may also be exploited
by a relatively low-resource adversary using similar tech-
niques to contribute to a network denial of service (DoS)
attack. Degraded service discourages the use of Tor, af-
fecting both Tor’s client diversity and anonymity.
Equipped with mechanisms from communication net-
works, we design and implement three Tor-speciﬁc al-
gorithms that throttle bulk transfers to reduce network
congestion and increase network responsiveness. Unlike
existing techniques, our algorithms adapt to network dy-
namics using only information local to a relay. We exper-
iment with full-network deployments of our algorithms
under a range of light to heavy network loads. We ﬁnd
that throttling results in signiﬁcant improvements to web
client performance while mitigating the negative effects
of bulk transfers. We also analyze how throttling affects
anonymity and compare the security of our algorithms
under adversarial attack. We ﬁnd that throttling reduces
information leakage compared to unthrottled Tor while
improving anonymity against realistic adversaries.
1
Introduction
The Tor [19] anonymity network was developed in an
attempt to improve anonymity on the Internet. Onion
Routing [23,48] serves as the cornerstone for Tor’s over-
lay network design. Tor clients encrypt messages in sev-
eral “layers” while packaging them into 512-byte packets
called cells, and send them through a collection of relays
called a circuit. Each relay decrypts its layer and for-
wards the message to the next relay in the circuit. The
last relay forwards the message to the user-speciﬁed des-
tination. Each relay can determine only its predecessor
and successor in the path from source to destination, pre-
venting any single relay from linking the sender and re-
ceiver. Clients choose their ﬁrst relay from a small set
of entry guards [44, 59] in order to help defend against
passive logging attacks [58]. Trafﬁc analysis is still pos-
sible [8,22,28,30,39,42,46,49], but slightly complicated
by the fact that each relay simultaneously services mul-
tiple circuits.
Tor relays are run by volunteers located throughout
the world and service hundreds of thousands of Tor
clients [37] with high bandwidth demands. A relay’s
utility to Tor is dependent on both the bandwidth ca-
pacity of its host network and the bandwidth restrictions
imposed by its operator. Although bandwidth donations
vary widely, the majority of relays offer less than 100
KiB/s and may become bottlenecks when chosen for a
circuit. Bandwidth bottlenecks lead to network conges-
tion and impair client performance.
Bottlenecks are further aggravated by bulk users,
which make up roughly ﬁve percent of connections and
forty percent of the bytes transferred through the net-
work [38]. Bulk trafﬁc increases network-wide conges-
tion and punishes interactive users as they attempt to
browse the web and run SSH sessions. Bulk trafﬁc also
constitutes a simple denial of service (DoS) attack on
the network as a whole: with nothing but a moderate
number of bulk clients, an adversary can intentionally
signiﬁcantly degrade the performance of the entire Tor
network for most users. This is a malicious attack as op-
posed to an opportunistic use of resources without regard
for the impact on legitimate users, and could be used by
censors [16] to discourage use of Tor. Bulk trafﬁc ef-
fectively averts potential users from Tor, decreasing both
Tor’s client diversity and anonymity [10, 18].
There are three general approaches to alleviate Tor’s
performance problems: increase network capacity; opti-
mize resource utilization; and reduce network load.
Increasing Capacity. One approach to reducing bottle-
necks and improving performance is to add additional
bandwidth to the network from new relays. Previous
work has explored recruiting new relays by offering per-
formance incentives to those who contribute [32, 41, 43].
While these approaches show potential, they have not
been deployed due to a lack of understanding of the
anonymity and economic implications they would im-
pose on Tor and its users. It is unclear how an incentive
1
scheme will affect users’ anonymity and motivation to
contribute: Acquisti et al. [6] discuss how differentiating
users by performance may reduce anonymity while com-
petition may reduce the sense of community and con-
vince users that contributions are no longer warranted.
New high-bandwidth relays may also be added by the
Tor Project [4] or other organizations. While effective
at improving network capacity, this approach is a short-
term solution that does not scale. As Tor increases speed
and bandwidth, it will likely attract more users. More
signiﬁcantly, it will attract more high-bandwidth and Bit-
Torrent users, resulting in a Tragedy of the Commons [26]
scenario: the bulk users attracted to the faster network
will continue to leech the additional bandwidth.
Optimizing Resource Utilization. Another approach to
improving performance is to better utilize the available
network resources. Tor’s path selection algorithm ig-
nores the slowest small fraction of relays while selecting
from the remaining relays in proportion to their available
bandwidth. The path selection algorithm also ignores cir-
cuits with long build times [12], removing the worst of
bottlenecks and improving usability. Congestion-aware
path selection [57] is another approach that aims to bal-
ance load by using opportunistic and active client mea-
surements while building paths. However, low band-
width relays must still be chosen for circuits to mitigate
anonymity problems, meaning there are still a large num-
ber of circuits with tight bandwidth bottlenecks.
Tang and Goldberg previously explored modiﬁcations
to the Tor circuit scheduler in order to prioritize bursty
(i.e. web) trafﬁc over bulk trafﬁc using an exponentially-
weighted moving average (EWMA) of relayed cells [52].
Early experiments show small improvements at a sin-
gle relay, but full-network experiments indicate that
the new scheduler has an insigniﬁcant effect on perfor-
mance [31]. It is unclear how performance is affected
when deployed to the live Tor network. This schedul-
ing approach attempts to shift network load to better uti-
lize the available bandwidth, but does not reduce bottle-
necks introduced by the massive amount of bulk trafﬁc
currently plaguing Tor.
Reducing Load. All of the previously discussed ap-
proaches attempt
to increase performance, but none
of them directly address or provide adequate defense
against performance degradation problems created by
bulk trafﬁc clients.
In this paper, we address these by
adaptively throttling bulk data transfers at the client’s en-
try into the Tor network.
We emphasize that throttling is fundamentally differ-
ent than scheduling, and the distinction is important in
the context of the Tor network. Schedulers optimize the
utilization of available bandwidth by following policies
set by the network engineer, allowing the enforcement
of fairness among ﬂows (e.g. max-min fairness [24, 34]
or proportional fairness [35]). However, throttling may
under-utilize local bandwidth resources by intentionally
imposing restrictions on clients’ throughput to reduce ag-
gregate network load.
By reducing bulk client throughput in Tor, we effec-
tively reduce the bulk data transfer rate through the net-
work, resulting in fewer bottlenecks and a less congested,
more responsive Tor network that can better handle the
burstiness of web trafﬁc. Tor has recently implemented
token buckets, a classic trafﬁc shaping mechanism [55],
to statically (non-adaptively) throttle client-to-guard con-
nections at a given rate [17], but currently deployed con-
ﬁgurations of Tor do not enable throttling by default. Un-
fortunately, the throttling algorithm implemented in Tor
requires static conﬁguration of throttling parameters: the
Tor network must determine network-wide settings that
work well and update them as the network changes. Fur-
ther, it is not possible to automatically tune each relay’s
throttling conﬁguration with the current algorithm.
Contributions. To the best of our knowledge, we are
the ﬁrst to explore throttling algorithms that adaptively
adjust to the ﬂuctuations and dynamics of Tor and each
relay independently without the need to adjust parame-
ters as the network changes. We also perform the ﬁrst
detailed investigation of the performance and anonymity
implications of throttling Tor client connections.
In Section 3, we introduce and test three algorithms
that dynamically and adaptively throttle Tor clients us-
ing a basic token bucket rate-limiter as the underlying
throttling mechanism. Our new adaptive algorithms use
local relay information to dynamically select which con-
nections get throttled and to adjust the rate at which
those connections are throttled. Adaptively tuned throt-
tling mechanisms are paramount to our algorithm de-
signs in order to avoid the need to re-evaluate parame-
ter choices as network capacity or relay load changes.
Our bit-splitting algorithm throttles each connection at
an adaptively adjusted, but reserved and equal portion
of a guard node’s bandwidth, our ﬂagging algorithm ag-
gressively throttles connections that have historically ex-
ceeded the statistically fair throughput, and our thresh-
old algorithm throttles connections above a throughput
quantile at a rate represented by that quantile.
We implement our algorithms in Tor1 and test their
effectiveness at improving performance in large scale,
full-network deployments. Section 4 compares our algo-
rithms to static (non-adaptive) throttling under a varied
range of network loads. We ﬁnd that the effectiveness
of static throttling is highly dependent on network load
and conﬁguration whereas our adaptive algorithms work
well under various loads with no conﬁguration changes
or parameter maintenance: web client performance was
1Software patches for our algorithms have been made publicly
available to the community [5].
2
Figure 1: A Tor relay’s internal architecture.
improved for every parameter setting we tested. We con-
clude that throttling is an effective approach to achieve a
more responsive network.
Having shown that our adaptive throttling algorithms
provide signiﬁcant performance beneﬁts for web clients
and have a profound impact on network responsiveness,
Section 5 analyzes the security of our algorithms under
adversarial attack. We discuss several realistic attacks on
anonymity and compare the information leaked by each
algorithm relative to unthrottled Tor. Against intuition,
we ﬁnd that throttling clients reduces information leak-
age and improves network anonymity while minimizing
the false positive impact on honest users.
2 Background
This section discusses Tor’s internal architecture, shown
in Figure 1, to facilitate an understanding of how internal
processes affect client trafﬁc ﬂowing through a Tor relay.
Multiplexed Connections. All relays in Tor commu-
nicate using pairwise TCP connections, i.e. each relay
forms a single TCP connection to each other relay with
which it communicates. Since a pair of relays may be
communicating data for several circuits at once, all cir-
cuits between the pair are multiplexed over their single
TCP connection. Each circuit may carry trafﬁc for mul-
tiple services or streams that a user may be accessing.
TCP offers reliability, in-order delivery of packets be-
tween relays, and potentially unfair kernel-level conges-
tion control when multiplexing connections [47]. The
distinction between and interaction of connections, cir-
cuits, and streams is important for understanding Tor.
Connection Input. Tor uses libevent [1] to handle input
and output to and from kernel TCP buffers. Tor regis-
ters sockets that it wants to read with libevent and con-
ﬁgures a notiﬁcation callback function. When data ar-
rives at the kernel TCP input buffer (Figure 1a), libevent
learns about the active socket through its polling in-
terface and asynchronously executes the corresponding
callback (Figure 1b). Upon execution, the read callback
determines read eligibility using token buckets.
Token buckets are used to rate-limit connections. Tor
ﬁlls the buckets as deﬁned by conﬁgured bandwidth lim-
its in one-second intervals while tokens are removed
from the buckets as data is read, although changing that
interval to improve performance is currently being ex-
plored [53]. There is a global read bucket that limits
bandwidth for reading from all connections as well as
a separate bucket for throttling on a per-connection ba-
sis (Figure 1c). A connection may ignore a read event
if either the global bucket or its connection bucket is
empty.
In practice, the per-connection token buckets
are only utilized for edge (non-relay) connections. Per-
connection throttling reduces network congestion by pe-
nalizing noisy connections, such as bulk transfers, and
generally leads to better performance [17].
When a TCP input buffer is eligible for reading, a
round-robin (RR) scheduling mechanism is used to read
the smaller of 16 KiB and 1
8 of the global token bucket
size per connection (Figure 1d). This limit is imposed in
an attempt at fairness so that a single connection can not
consume all the global tokens on a single read. However,
recent research shows that input/output scheduling leads
to unfair resource allocations [54]. The data read from
the TCP buffer is placed in a per-connection application
input buffer for processing (Figure 1e).
Flow Control. Tor uses an end-to-end ﬂow control algo-
rithm to assist in keeping a steady ﬂow of cells through
a circuit. Clients and exit relays constitute the edges of
a circuit: each are both an ingress and egress point for
data traversing the Tor network. Edges track data ﬂow
for both circuits and streams using cell counters called
windows. An ingress edge decrements the correspond-
ing stream and circuit windows when sending cells, stops
reading from a stream when its stream window reaches
zero, and stops reading from all streams multiplexed over
a circuit when the circuit window reaches zero. Win-
dows are incremented and reading resumes upon receipt
of SENDME acknowledgment cells from egress edges.
3
By default, circuit windows are initialized to 1000
cells (500 KiB) and stream windows to 500 cells (250
KiB). Circuit SENDMEs are sent to the ingress edge af-
ter the egress edge receives 100 cells (50 KiB), allowing
the ingress edge to read, package, and forward 100 ad-
ditional cells. Stream SENDMEs are sent after receiving
50 cells (25 KiB) and allow an additional 50 cells. Win-
dow sizes can have a signiﬁcant effect on performance
and recent work suggests an algorithm for dynamically
computing them [7].
Cell Processing and Queuing. Data is immediately
processed as it arrives in connection input buffers (Fig-
ure 1f) and each cell is either encrypted or decrypted de-
pending on its direction through the circuit. The cell is
then switched onto the circuit corresponding to the next
hop and placed into the circuit’s ﬁrst-in-ﬁrst-out (FIFO)
queue (Figure 1g). Cells wait in circuit queues until the
circuit scheduler selects them for writing.
Scheduling. When there is space available in a con-
nection’s output buffer, a relay decides which of sev-
eral multiplexed circuits to choose for writing. Al-
though historically this was done using round-robin, a
new exponentially-weighted moving average (EWMA)
scheduler was recently introduced into Tor [52] and is
currently used by default (Figure 1h). EWMA records
the number of packets it schedules for each circuit, expo-
nentially decaying packet counts over time. The sched-
uler writes one cell at a time chosen from the circuit with
the lowest packet count and then updates the count. The