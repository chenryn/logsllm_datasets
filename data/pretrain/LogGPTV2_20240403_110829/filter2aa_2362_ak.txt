employ Bindiff is to look at one version of a Microsoft application, then examine a
new version that has just been patched for security reasons. The difference between
the two should yield the original bug. Such a technique might identify bugs much
quicker than an entire review of a large code base.
Also, more than a few professional hackers have expressed that sometimes the
spotting of implementation flaws in assembly can actually be easier than in source
code. This is because complex lines of C or other high-level languages can be con-
voluted and hard to read. This could also be due to numerous macros, missing code
that is not linked in or expanded until compile time. Having said all that, there cer-
tainly are advantages to having source code. Complex structure can quickly be
understood, comments are a huge advantage, and the auditor can simple grep
(search) for arbitrary combinations of code that could be problematic. 
2.5.2
Source Code Auditing
Source code auditing typically involves using automated tools, plus manual verifi-
cation, to search source code for bugs. The source could be any type (a library,
2.5
Bug Hunting Techniques
57
headers, main program code) and in any language. The process will vary from lan-
guage to language.
Again, to augment performing source code audits by hand, a variety of open
source and commercial tools exist to help highlight suspect code. The commercial
tools from companies like Coverity and Fortify tend to be very sophisticated and
capable of finding many different classes of vulnerabilities. The biggest drawback,
with regard to these static analysis tools, is the presence of false positives. While these
tools take care to minimize them, it is impossible to completely eliminate false pos-
itives, and some code that is not problematic will be identified as a vulnerability.
As an open source example, Figure 2.8 illustrates the usage of the Rough Audit-
ing Tool for Security (RATS)20 to analyze the following program:
#include
void copy(char * ptr)
{
char buf[100];
strcpy(buf, ptr);
printf("You entered: %s. Horray!\r\n");
}
int main(int argc, char * argv[])
{
if( argc != 2)
{
printf("bad args\r\n");
exit(-1);
}
58
Software Vulnerability Analysis
Figure 2.8
Running RATS against a trivial C source code file.
20www.fortifysoftware.com/security-resources/rats.jsp
copy(argv[1]);
}
In this simple case, the RATS tool effectively highlights the buffer overflow that
is present in the code. Note that it doesn’t actually prove the existence of a vulner-
ability, rather via the usage of heuristics states that one might exist because strcpy
was used. Therefore, even code using strcpy completely safely would be identified
(wrongly) as being potentially vulnerable. However, more sophisticated tools such
as those by Fortify have a more sophisticated understanding of the code and so can
do a better job at identifying which strcpy’s are actually problematic, among other
things.
2.6
Fuzzing
The remaining method of finding bugs is the topic of this book. One of the main
strengths of fuzzing is that if an input crashes an application, a problem definitely
exists in the application (no false positives). It should be noted that both source
code audits and reverse engineering are (traditionally) a purely static method for
understanding the operation (and misoperation) of a given application. However,
actually executing the target for a few minutes can often yield more understanding
than hours of reverse engineering, at least from a high level.21 What if no under-
standing of an application was available and all we could do was supply input?
What if, for whatever reason, when we supply a malformed input, the target
crashes? This is the essence and origin of fuzzing. One of the first people to employ
fuzzing was Professor Barton Miller. He found that if random inputs were given to
core Unix command line utilities (like ls, grep, ps, passwd, etc.) many of them
would crash. This lack of robustness surprised him, and he went on to write one of
the first automated tools designed specifically to crash programs. His fuzzing tool
was dumb. However, in this context, the word dumb does not mean stupid. It
means that his fuzzing tool had no knowledge of what inputs these programs might
be expecting. That is, he merely sent random data as arguments to the functions.
Conversely, if his tool had been intelligent, it would have known that command a
always expects arguments b, in the forms c, d, or e. In later sections we’ll explain
when, where, and how non-intelligent/intelligence should be applied and balanced.
We’ll look at a number of topics, including how to build a fuzzer, how to reach the
lowest level of a protocol or application, types of fuzzers, where and when fuzzers
are most effective, what metrics to consider when fuzzing, and finally current and
future trends and research.
2.6.1
Basic Terms
Coverage is an important term that is used in testing, and the same applies for
fuzzing. From a vulnerability analysis perspective, coverage typically refers to simple
2.6
Fuzzing
59
21Research on “high-level” reverse engineering is just beginning to go mainstream: www.net-security
.org/article.php?id=1082
code coverage—that is, how many lines of the existing source code or compiled
code has been tested or executed during the test. Coverage could also measure path,
branch permutations, or a variety of other code coverage metrics.
A related term to coverage is attack surface: the amount of code actually
exposed to an attacker. Some code is internal and cannot be influenced by external
data. Examples of this include when a network server parses a configuration file or
initially binds to a socket. This code should be tested, but cannot be externally
fuzzed. Since it cannot be influenced by external data, it is of little interest when
finding vulnerabilities. Thus, our interests lie in coverage of the attack surface. This
is especially true for security researchers. Quality assurance professionals may be
tasked to test all of the code.
A trust boundary is any place that data or execution goes from one trust level
to another, where a trust level is a set of permissions to resources. For example,
data sent across a network to an application that parses that data is an example of
crossing a trust boundary. If the root user of a Unix system is the only one able to
start a given application (via command line arguments), priority would probably go
to fuzzing the network interface (assuming all or most untrusted users can access
the interface) instead of the command line arguments. This is true for two reasons:
A remotely exploitable bug is more interesting to attackers (since it can be done
remotely), but in terms of trust boundaries, an elevation of privilege (from none to
whatever the process runs as) can occur in the remote situation. Conversely, if the
user must already be root to gain root privileges (unless a tricky method is devised
to run the binary without root privileges), nothing has been gained, plus the attack
would only be local. The reading of the full-disclosure mailing list will often reveal
“vulnerabilities” in software if the application runs in an elevated privilege level. In
reality, many programs do not run at an elevated privilege level (think ls, rm, cat),
so a bug in these programs may not have security implications.22 Priority is impor-
tant to software companies and attackers alike because the problem of finding bugs
is difficult and time consuming. Neither is willing to waste much time (money) for
purely academic reasons; fuzzing is known for its ability to produce results.
Input source and input space are similar terms that refer to how data will be
generated (and ultimately delivered) to the application to be fuzzed (the target). The
input space is the entire set of all possible permutations that could be sent to the tar-
get. This space is infinite, and that is why heuristics are used to limit this space.
Attack heuristics are known techniques for finding bugs in applications, normally
based on the types of bugs discovered in the past.
2.6.2
Hostile Data23
To find a vulnerability, you need to know what types of inputs will trigger the
flaws. And when you know why these inputs will cause an exception or a crash, you
will be able to optimize the tests that you need to do. The examples below illustrate
60
Software Vulnerability Analysis
22Unless those commands are executed by scripts running in higher privileges!
23Each bug type (buffer overflow, format string, etc.) may be further described in section 2.7.
a few simple heuristics against a typical (imaginary) simple string-based client-
server protocol.24
1. Buffer overflows are tested with long strings. For example:
[Client]-> “user jared\r\n”
“user Ok. Provide pass.\r\n”  “pass \r\n”
2. Integer overflows are tested with unexpected numerical values such as:
zero, small, large, negative: wrapping at numerical boundaries – 2^4, 2^8,
2^16, 2^24: wrong number system—floats vs. integers. For example:
[Client]-> “user jared\r\n”
“user Ok. Provide pass.\r\n”  “pass jared\r\n”
“pass Ok. Logged in. Proceed with next command.\r\n”  “get [-100000.98] files: *\r\n”
Format string vulnerabilities are tested with strings such as:
[Client]-> “user ”
‘%n’s are useful because of the way the printf family of functions were
designed. A percent sign followed by a letter is referred to as a format
string.25 The ‘n’ is the only switch that triggers a write and is therefore use-
ful for triggering a crash while fuzzing. ‘x’ or ‘s’ may actually be a better
choice in some cases, as the ‘n’ usage may be disabled.26
3. Parse error: NULL after string instead of \r\n. Bad string parsing code
might be expecting a linefeed (\r or 0x0d) or newline (\n or 0x0a) in a given
packet and may incorrectly parse data if nothing or a NULL exists in its
place. The NULL (0x00) is special because string functions will terminate
on it, when perhaps the parsing code wouldn’t expect it to since no new-
line is present.
[Client]-> “user jared0x00”
4. Parse error: Incorrect order and combined commands in one packet.
Often, network daemons expect each command to arrive in a separate
packet. But what if they don’t? And what if they’re out of order, and all
strung together with linefeeds in one packet? Bad things could happen to
the parser.
[Client]-> “pass jared\r\nuser jared\r\n”
2.6
Fuzzing
61
24For more example inputs from web fuzzing, see www.owasp.org/index.php/OWASP_Testing_
Guide_Appendix_C:_Fuzz_Vectors
25See section 2.7.1.1.
26See http://blogs.msdn.com/michael_howard/archive/2006/09/28/775780.aspx
5. Parse error: Totally random binary data. If there is a particular character(s)
that the parser is looking for but might not handle well in an unexpected
scenario, this might uncover such an issue.
[Client]-> “\xff\xfe\x00\x01\x42\xb5...”
6. Parse error: Sending commands that don’t make sense—multiple login.
Design or logic flaws can also sometimes be uncovered via fuzzing.
[Client]-> “user jared\r\n”
“user Ok. Provide pass.\r\n”  “pass jared\r\n”
“pass Ok. Logged in. Proceed with next command.\r\n”  “user jared\r\n”
7. Parse error: Wrong number of statement helpers such as ‘../’, ‘{’, ‘(’, ‘[’, etc.
Many network protocols such as HTTP have multiple special chapters
such as ‘:’, “\\”, etc. Unexpected behavior or memory corruption issues can
creep in if parsers are not written very carefully.
[Client]-> “user jared\r\n”
“user Ok. Provide pass.\r\n”  “pass jared\r\n”
“pass Ok. Logged in. Proceed with next command.\r\n”  “get [1] files: {{../../../../etc/password\r\n”
8. Parse error: Timing issue with incomplete command termination. Suppose
we want to DoS the server. Clients can often overwhelm servers with a
command that is known to cause processing to waiting on the server end.
Or perhaps this uses up all the validly allow connections (like a SYN
flood27) in a given window of time.
[Client]-> “user jared\r” (@ 10000 pkts/second with no read for server
response)
2.6.3
Number of Tests
Again, it is obvious that the input space is infinite. This is why heuristics are used.
For example, if a buffer overflow occurs if a particular string is larger than 1,024
bytes, this can be found by sending a string of 1 byte, then 2, then 3, etc. Or it can
be found by sending a string of size 1, 2, 4, 8, etc. It is unlikely that an overflow will
exist that will not be found using this method, and yet it can greatly reduce the
number of test cases. Likewise, it would technically be possible to send totally ran-
dom data and get the same effect as using heuristics, but instead of the fuzzer run-
time being finite and reasonable ( centuries). Furthermore, with an increased number of
tests comes an increased load of logging.
62
Software Vulnerability Analysis
27See section 2.7.5.
The goal is to cover every unique test case, input space (without too much dupli-
cation or unneeded sessions), and to log the ones that succeed in causing the target
to fail in some way. It is still an open question as to how many test cases are
“enough,” but using a metric based approach and code coverage results, it may be
possible to shed light on this difficult decision.
2.7
Defenses
This section focuses on what can be done to mitigate the risks of implementation
errors. There are many coding techniques, hardware/software protections, and
further system designs that can be put in place to minimize the risk of software fail-
ure or malicious compromise.
To this end, Microsoft’s Vista operating system has made significant strides
toward becoming a more secure operating and development platform. Section 2.7.5
will introduce some of these protections. Other operating systems have other pro-
tections, but not all can be discussed in the space allotted.
2.7.1
Why Fuzzing Works
Fuzzing has been found effective because manually conceiving and creating every
possible permutation of test data to make good test cases is difficult if not impossi-
ble. Testers try their best, but fuzzing has a way of slamming around to find inter-
esting corner cases. Of course, intelligent fuzzing is required to advance into
multi-leg, or more complex, protocols. This will be discussed later in this book.
Fuzzing works against any application that accepts input, no matter what pro-
gramming language is used: Java, C++, C, C#, PHP, Perl, or others. However, appli-
cations written in C and C++ are particularly susceptible to fuzzing. Compiled C
code is probably the fastest high-level language. For example, a network server that
needs to be able to run at very high speeds would not be written in python or ruby,
because it would be too slow. C would be the best choice for speed. This is because
C provides the programmer the ability to manage low-level operations, such as
memory management (malloc(), free(), etc.).
C and C++ are a hacker’s favorite target languages. This is because C code tra-
ditionally handles its own memory; from static buffer declarations that lead to stack
overflows to heap allocations that can easily go wrong. With the ability to optimize
memory for speed comes the ability to shoot oneself in the foot. General applica-
tions should never be managing their own memory these days. Computers are fast,
and programmers make too many mistakes. It only makes sense to code in C and
manage memory when an application’s speed is more important than an applica-
tion’s security, or you have to integrate with legacy code. In these (and really all)
applications, defensive coding should be the norm. (Kernels are also written in
C/C++ out of necessity.)
2.7.2
Defensive Coding
Defensive coding may also be known as defensive or secure programming. The gen-
eral goal is to reduce the number of bugs in software, make the source code more
2.7
Defenses
63
readable, and keep the software from executing in unpredictable ways. The following
is a short list of some of the guidelines defensive programmers should keep in mind:28
1. Reduce code complexity. Never make code more complex that it needs to
be; complexity equals bugs.
2. Source code reviews. All code should be reviewed using automatic source
code auditing tools. Many software development organizations have
source code scanning tools embedded in the build process, and they auto-
matically look for certain patterns and potentially dangerous functions.
For example, in C, strcpy() should never be used.
3. Quality control. All code should be thoroughly tested. Fuzz testing is a
must for applications with potentially vulnerable attack surfaces. This
should be part of a full security audit (design review, code review, fuzz test-
ing, and so on) Software testing is discussed more in Chapter 3.
4. Code reuse. If there are snippets that have been well tested, reuse is better
than a rewrite when applicable. This saves time (money) and is more
secure. Look out for legacy problems or buggy libraries, however.
5. Secure input/output handling. Nothing should be assumed about exter-
nally supplied data. All user input should be rigorously verified before
being used by the application.
6. Canonicalization. Remember that on Unix-based operating systems /etc/
passwd is the same as /etc/.///passwd. Input string auditing may require the
use of canonicalization APIs to defend against such tricks.
7. Principle of least privilege. Avoid running software in privileged modes if
possible. Do not grant more privileges to the application than are needed.
8. Assume the worst. If similar applications have had bugs in a particular rou-
tine, assume your code does as well. This follows the Same Bug Different
Application (SBDA) theory, which holds true surprisingly often. A touch of
paranoia is good. All code is insecure even after testing. Defense in depth
is good.
9. Encrypt/Authenticate. Encrypt everything transmitted over networks
(when possible). Local encryption may be employed as well. Use encryp-
tion libraries. Mistakes are often made in home-grown encryption. Roll-
ing custom cryptography is often a bad idea. Use public libraries when
possible.
10. Stay up to date. Exceptions can be better than return codes because they
help enforce intended API contracts, where lazy programmers may or may
not look at return codes. However, recently exception handlers are being
considered bad, because they are often used incorrectly.29
2.7.3
Input Verification
Input verification, or input handling, is how an application verifies the correctness
of data provided to it via an external source. Improper verification (sanitization)
64
Software Vulnerability Analysis
28http://en.wikipedia.org/wiki/Defensive_programming accessed on 12/10/07
29http://blogs.msdn.com/david_leblanc/archive/2007/04/03/exception-handlers-are-baaad.aspx
has led to such bugs as directory traversals, code injections, buffer overflows, and
more. Some basic filter techniques are
• Whitelist. A list of known good inputs. This is a list that essentially says “a,
b, and c are ok; all else is to be denied.” Such a listing is best but is not always
possible.
• Blacklist. A list of known bad inputs. This list says, “all are ok, but deny x
and y.” This is not as effective as whitelisting because it relies on the pro-
grammer’s thinking of every possible troublesome input.
• Terminate on input problem. This approach terminates as soon as any prob-
lem is found with the provided input data and logs the problem.
• Filter input. Takes input, even bad input, and attempts to filter. For example,
if the ‘&’ is a disallowed character, “&jared” would be interpreted as
“jared.” This is not as secure as “Terminate on Input problem,” but often
required.
• Formal grammar. Input data can also be verified via a formal grammar such
as XML. In this case, just make sure to use well-tested, public verification
software.
Generally, the most secure way to filter input is to terminate on malformed
input by using whitelists.