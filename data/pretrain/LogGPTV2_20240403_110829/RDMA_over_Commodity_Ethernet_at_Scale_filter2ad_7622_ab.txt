For data centers at our scale, OS provisioning has to
be done automatically. We use PXE (Preboot eXecu-
tion Environment) boot to install OS from the network.
When a server goes through PXE boot, its NIC does
not have VLAN conﬁguration and as a result cannot
send or receive packets with VLAN tags. But since
the server facing switch ports are conﬁgured with trunk
mode, these ports can only send packets with VLAN
tag. Hence the PXE boot communication between the
204
server and the OS provisioning service is broken. We
tried several “hacks” to ﬁx this problem, including let-
ting the switches change the switch port conﬁguration
based on the guessed state of the servers, and letting the
NICs accept all the packets with or without VLAN tag.
However, all these proved to be complex and unreliable,
and needless to say, non-standard.
Second, we have moved away from a layer-2 VLAN,
and all our switches including the ToR switches are
running layer-3 IP forwarding instead of MAC-based
layer-2 bridging. A layer-3 network has the beneﬁts of
scalability, better management and monitoring, better
safety, all public and standard instead of proprietary
protocols. However, in a layer-3 network, there is no
standard way to preserve the VLAN PCP value when
crossing subnet boundaries.
In both problems, the fundamental issue is that VLAN-
based PFC unnecessarily couples packet priority and
the VLAN ID. We broke this coupling by introducing
DSCP-based PFC. Our key observation is that the PFC
pause frames do not have a VLAN tag at all. The VLAN
tag in data packets is used only for carrying the data
packet priority. In the IP world, there is a standard and
better way to carry packet priority information, which
is the DSCP ﬁeld in the IP header.
The solution, as shown in Figure 3(b), is to move the
packet priority from the VLAN tag into DSCP. As we
can see, the change is small and only touches the data
packet format. The PFC pause frame format stays the
same. With DSCP-based PFC, data packets no longer
need to carry the VLAN tag, which solves both of the
problems mentioned earlier. The server facing ports no
longer need to be in trunk mode, which means that
PXE boot works without any issues. Also, the packet
priority information, in form of DSCP value, is correctly
propagated by IP routing across subnets.
Of course, DSCP-based PFC does not work for the
designs that need to stay in layer-2, e.g., Fibre Channel
over Ethernet (FCoE). This is not a problem for us since
we do not have any layer-2 networks in our data centers.
DSCP-based PFC requires both NICs and switches
to classify and queue packets based on the DSCP value
instead of the VLAN tag. In addition, the NIC needs to
send out data packets with the right DSCP value. For-
tunately, the switch and NIC ASICs are ﬂexible enough
to implement this. Internally, at each port, the switch
or NIC maintains eight Priority Groups (PGs), with
each PG can be conﬁgured as lossless or lossy. If a PG
i (i ∈ [0, 7]) is conﬁgured as lossless, once its ingress
buﬀer occupation exceeds the XOFF threshold, pause
frame with priority i will be generated. The mapping
between DSCP values and PFC priorities can be ﬂexible
and can even be many-to-one. In our implementation,
we simply map DSCP value i to PFC priority i.
Our DSCP-based PFC speciﬁcation is publicly avail-
able, and is supported by all major vendors (Arista Net-
works, Broadcom, Cisco, Dell, Intel, Juniper, Mellanox,
etc.). We believe DSCP-based PFC provides a simpler
and more scalable solution than the original VLAN-
based design for IP networks.
4. THE SAFETY CHALLENGES
Use of PFC and RDMA transport lead to several
safety challenges. We now describe these challenges and
the solutions we devised to address them.
4.1 RDMA transport livelock
RDMA transport protocol is designed around the as-
sumption that packets are not dropped due to network
congestion. This is achieved by PFC in RoCEv2. How-
ever, packet losses can still happen for various other rea-
sons, including FCS errors, and bugs in switch hardware
and software [21]. Ideally, we want RDMA performance
to degrade gracefully in presence of such errors.
Unfortunately, we found that the performance of RDMA
degraded drastically even with a very low packet loss
rate. We illustrate this with the following simple ex-
periment. We connected two servers A and B, via a
single switch (W), and carried out three experiments
for RDMA SEND, WRITE, and READ. In the ﬁrst ex-
periment, A used RDMA SENDs to send messages of
size 4MB each to B as fast as possible. The second ex-
periment was similar, except A used RDMA WRITE.
In the third experiment B used RDMA READ to read
4MB chunks from A as fast as possible. The switch was
conﬁgured to drop any packet with the least signiﬁcant
byte of IP ID equals to 0xﬀ. Since our NIC hardware
generates IP IDs sequentially, the packet drop rate was
1/256 (0.4%).
We found that even with this low packet drop rate,
the application level goodput was zero. In other words,
the system was in a state of livelock – the link was
fully utilized with line rate, yet the application was not
making any progress.
The root cause of this problem was the go-back-0
algorithm used for loss recovery by the RDMA trans-
port. Suppose A is sending a message to B. The mes-
sage is segmented into packets 0, 1,··· , i,··· , m. Sup-
pose packet i is dropped. B then sends an N AK(i)
to A. After A receives the NAK, it will restart sending
the message from packet 0. This go-back-0 approach
caused live-lock. A 4MB message is segmented into
4000 packets. Since the packet drop rate is a deter-
ministic 1/256, one packet of the ﬁrst 256 packets will
be dropped. Then the sender will restart from the ﬁrst
packet, again and again, without making any progress.
Note that TCP and RDMA transport make diﬀerent
assumptions on the network. TCP assumes a best-eﬀort
network, in which packets can be dropped. Thus, TCP
stacks incorporate sophisticated retransmission schemes
such as SACK [24] to deal with packet drops. On the
other hand, RDMA assumes a lossless network, hence
our vendor chose to implement a simple go-back-0 ap-
proach. In go-back-0, the sender does not need to keep
any state for retransmission.
205
Figure 4: An example to show that the interaction between Ethernet packet ﬂooding and PFC pause frame propa-
gation can cause deadlock.
This experiment clearly shows that for large network
like ours, where packet losses can still occur despite en-
abling PFC, a more sophisticated retransmission scheme
is needed. Recall however, that the RDMA transport is
implemented in the NIC. The resource limitation of the
NIC we use meant that we could not implement a com-
plex retransmission scheme like SACK. SACK would
also be overkill, as packet drops due to network conges-
tion have been eliminated by PFC.
Our solution is to replace the go-back-0 with a go-
back-N scheme.
In go-back-N, retransmission starts
from the ﬁrst dropped packet and the previous received
packets are not retransmitted. Go-back-N is not ideal
as up to RT T × C bytes , where C is the link capacity,
can be wasted for a single packet drop. But go-back-N
is almost as simple as go-back-0, and it avoids livelock.
We worked with our NIC provider to implement the
go-back-N scheme, and since doing that, we have not
observed livelock in our network. We recommend that
the RDMA transport should implement go-back-N and
should not implement go-back-0.
4.2 PFC Deadlock
We once believed that our network is deadlock-free
because of its Clos network topology and up-down rout-
ing [1, 3, 19]. In such a topology, when a source server
sends a packet to a destination server, the packets ﬁrst
climb up to one of the common ancestors of the source
and the destination, then go down the path to the desti-
nation. Hence there is no cyclic buﬀer dependency. But
to our surprise, we did run into PFC deadlock when we
ran a stress test in one of our test clusters.
As we will see later, this occurred because the unex-
pected interaction between PFC and Ethernet packet
ﬂooding broke the up-down routing.
Before diving into the details of this example, let’s
brieﬂy review how a ToR switch forwards an IP packet
to a server. Typically servers connected to the same
ToR are in the same IP subnet. This subnet is then
advertised to the rest of the network, so the rest of the
network can forward packets to the ToR switch. Once
the ToR receives an IP packet which belongs to one
of its servers, it needs to query two tables. One is the
ARP table from which the ToR switch can ﬁgure out the
MAC address of the destination server. The second is
the MAC address table from which the ToR switch can
ﬁgure out with which physical port the MAC address
is associated. The ARP table is for layer-3 IP whereas
the MAC address table is for layer-2. The ARP table is
maintained by the ARP protocol. The switch watches
which packet comes from which port to establish the
MAC address table.
Both tables use timeout to retire outdated entries.
The typical timeout values for the ARP and MAC tables
are very diﬀerent: 4 hours and 5 minutes, respectively.
The reason for using such disparate timeout values is
that the overhead of refreshing the entries in the two
tables is very diﬀerent. The MAC table is automati-
206
cally refreshed by hardware as new packets are received,
while the ARP table is updated by ARP packets, which
are handled by the switch CPU. Hence the ARP table
maintenance is more costly and thus the ARP table has
a much longer timeout value. Such disparate timeout
values can lead to an “incomplete” ARP entry – i.e. a
MAC address is present in the ARP table, but there is
no entry in the MAC address table for that MAC ad-
dress. When a packet destined to such a MAC address
arrives, the switch cannot ﬁgure out to which port to
forward the packet. The standard behavior in this case
is for the switch to ﬂood the packet to all its ports.
Below let’s use a simpliﬁed example as shown in Fig-
ure 4 to illustrate how the deadlock happens. We as-
sume all the packets in the example are lossless packets
protected by PFC.
1. Server S1 is sending packets to S3 and S5 via path
{T0, La, T1}. The purple packets are to S3 and
the black packets to S5. S3 is dead, so the purple
packets received at port T1.p3 are ﬂooded to the
rest ports of T1 including p4. The egress queue
of T1.p4 will drop the purple packets once they
are at the head of the queue since the destina-
tion MAC does not match. But before that, these
purple packets are queued there. Also T1.p2 is
congested due to incast traﬃc from S1 and other
sources. Hence the black packets are queued in
T1. As a result, the ingress port of T1.p3 begins
to pause the egress port of La.p1.
2. Consequently, as the black and purple packets build
up queues in La, the ingress port of La.p0 begins
to pause the egress port of T0.p2. For the same
reason, T0.p0’s ingress port begins to pause S1.
3. Server S4 begins to send blue packets to S2 via
path {T1, Lb, T0}. S2, unfortunately, is also dead.
Port T0.p3 then ﬂoods the blue packets to the rest
ports including T0.p2. Since all packets, including
the blue packets, at the egress port of T0.p2 cannot
be drained, the ingress port of T0.p3 begins to
pause Lb.p0.
4. As a result, the ingress port of Lb.p1 begins to
pause T1.p4, and T1.p1 begins to pause S4.
Note that T1.p3 will continue to pause La.p1 even if
the black packets leave T1 to S5 after the congestion at
T1.p2 is gone. This is because the purple packets cannot
be drained as T1.p4 is paused by Lb. A PFC pause
frame loop among the four switches is then formed. A
deadlock therefore occurs. Once the deadlock occurs, it
does not go away even if we restart all the servers.
This deadlock is a concrete example of the well-known
cyclic buﬀer dependency (see [12, 18, 22, 36] and ref-
erences therein). The cause of the cyclic dependency,
however, is ‘new’. It is caused by the ﬂooding behavior
of the switch. In an Ethernet switch, once the destina-
tion MAC address of a packet is unknown, the packet is
Figure 5: The PFC pause frame storm caused by the
malfunctioning NIC of one single server (server 0).
ﬂooded to all the ports except the receiving port. This
‘legitimate’ behavior causes the dependency circle to be
formed as we have shown in the above example.
We need to stop ﬂooding for lossless packets to pre-
vent deadlock from happening. There are several op-
tions for us to choose when an ARP entry becomes in-
complete (i.e., the IP address to MAC address mapping
is there, but the MAC address to port number mapping
is not). (1) We forward the packets to the switch CPU
and let the switch CPU ﬁgure out what to do. (2) We
set up the timeout value of the MAC table to be longer
than that of the ARP table, so that an ARP entry can-
not be incomplete. (3) We drop the lossless packets if
their corresponding ARP entry is incomplete.
We have chosen option (3). Option (1) may increase
the switch CPU overhead. Option (2) needs to either re-
duce the ARP table timeout value or increase the MAC
address table timeout value. If we reduce the ARP table
timeout value, we increase the switch CPU overhead for
ARP handling. If we increase the MAC address table
timeout value, we need longer time to tell when a server
becomes disconnected from the switch. Option (3) is a
better way to prevent deadlock as it directly prevents
the cyclic buﬀer dependency.
The lesson we have learned from the PFC deadlock is
that broadcast and multicast are dangerous for a lossless
network. To prevent deadlock from happening, we rec-
ommend that broadcast and multicast packets should
not be put into lossless classes.
4.3 NIC PFC pause frame storm
PFC pause frames prevent packets from been dropped
by pausing the upstream devices. But PFC can cause
collateral damage to innocent ﬂows due to the head-of-
line blocking. We illustrate the worst-case scenario in