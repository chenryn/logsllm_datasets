title:Impact of packet sampling on anomaly detection metrics
author:Daniela Brauckhoff and
Bernhard Tellenbach and
Arno Wagner and
Martin May and
Anukool Lakhina
Impact of Packet Sampling on Anomaly Detection Metrics
Daniela  Brauckhoff,  Bernhard Tellenbach, 
Arno  Wagner,  Martin  May
Department of Information Technology and
Electrical Engineering
Swiss Federal Institute of Technology (ETH)
Zurich, Switzerland
{brauckhoff, tellenbach, wagner,
may}@tik.ee.ethz.ch
ABSTRACT
Packet sampling methods such as Cisco’s NetFlow are widely em-
ployed by large networks to reduce the amount of trafﬁc data mea-
sured. A key problem with packet sampling is that it is inherently
a lossy process, discarding (potentially useful) information. In this
paper, we empirically evaluate the impact of sampling on anomaly
detection metrics. Starting with unsampled ﬂow records collected
during the Blaster worm outbreak, we reconstruct the underlying
packet trace and simulate packet sampling at increasing rates. We
then use our knowledge of the Blaster anomaly to build a baseline
of normal trafﬁc (without Blaster), against which we can measure
the anomaly size at various sampling rates. This approach allows
us to evaluate the impact of packet sampling on anomaly detection
without being restricted to (or biased by) a particular anomaly de-
tection method.
We ﬁnd that packet sampling does not disturb the anomaly size
when measured in volume metrics such as the number of bytes and
number of packets, but grossly biases the number of ﬂows. How-
ever, we ﬁnd that recently proposed entropy-based summarizations
of packet and ﬂow counts are affected less by sampling, and ex-
pose the Blaster worm outbreak even at higher sampling rates. Our
ﬁndings suggest that entropy summarizations are more resilient to
sampling than volume metrics. Thus, while not perfect, sampling
still preserves sufﬁcient distributional structure, which when har-
nessed by tools like entropy, can expose hard-to-detect scanning
anomalies.
Categories and Subject Descriptors: C.2.6 [Computer Com-
munication Networks]: Internetworking-Measurement
General Terms: Measurement, Security
Keywords: Anomaly Detection, Network Trafﬁc Analysis, Sam-
pling
1.
INTRODUCTION
Trafﬁc sampling has emerged as the dominant means to summa-
rize the vast amount of trafﬁc data continuously collected for net-
work monitoring. The most prevalent and widely-deployed method
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’06, October 25–27, 2006, Rio de Janeiro, Brazil.
Copyright 2006 ACM 1-59593-561-4/06/0010 ...$5.00.
Anukool Lakhina
Department of Computer Science
Boston University
Boston, MA
PI:EMAIL
of sampling trafﬁc is packet sampling, where a router inspects ev-
ery n-th packet (uniformly at random), and records its features (ad-
dresses, ports, protocol, and ﬂags). Packet sampling is attractive
because it is computationally efﬁcient, requiring minimal state and
counters, and is implemented in high-end routers today (e.g.,with
NetFlow [4]). As such, many large networks (ISPs and enterprizes)
are now using packet sampling to obtain rich views of trafﬁc di-
rectly from routers.
But, while being attractive because of efﬁciency and availability,
sampling is inherently a lossy process, where many packets are dis-
carded without inspection. Thus sampled trafﬁc is an incomplete
and more importantly, a biased approximation of the underlying
trafﬁc trace, as small ﬂows are likely to be missed entirely. Previ-
ous work has largely focused on analyzing this bias, devising better
sampling strategies [3], and recovering statistics (moments and dis-
tribution) of the underlying trafﬁc trace using inference [5, 6, 8].
Sampled trafﬁc views have recently been used for anomaly de-
tection with considerable success [11, 13]. But, little is known
about the ﬁdelity of the sampled stream for these applications, and
basic questions remain unanswered; for example: how complete
are the detections revealed by these methods on sampled trafﬁc?
and: what kind of anomalies are discarded by packet sampling?
There is little previous work on how sampling impacts network
monitoring applications, in particular, anomaly detection. Two no-
table studies are [15] and [14]. In [15], Mai et al analyzed how
packet sampling impacts three speciﬁc portscan detection methods,
TRWSYN [10], TAPS [17] and entropy-based proﬁling method
of [13, 21]. Recently, this work was extended to analyze the im-
pact of other sampling schemes in [14]. Both studies conclude that
packet sampling is inadequate to detect anomalies using these de-
tection methods.
Instead of focusing on the the performance of
speciﬁc anomaly detection methods to sampling as these studies
have, our study seeks to answer a more basic question: how does
packet sampling impact detection metrics? We study this problem
by examining the impact of packet sampling on anomaly size as
viewed through various detection metrics.
In this paper, we rely on a unique week-long dataset of unsam-
pled ﬂow records with the Blaster worm anomaly, collected from
backbone routers of a national ISP. We then simulate packet sam-
pling to construct sampled views of the same trafﬁc trace and ask
how the sampled views differ from the original trace with respect
to different anomaly detection metrics. Because we know the ex-
act characteristics of the anomaly in our trace, we can build the
ideal normal baseline that all anomaly detection methods would
strive to build. We then study the size of the worm anomaly, which
is measured as the distance from our ideal baseline, at increasing
sampling rates and for different anomaly detection metrics. The
size of an anomaly determines how effective any detection method
will be at exposing the anomaly. This approach allows us to gain
general insight into the impact of sampling on the anomaly size for
different metrics, without restricting (or biasing) us to a speciﬁc
detection scheme.
As a starting point, we investigate how packet sampling impacts
the three principal volume metrics (number of bytes, packets and
ﬂows), which have been used widely by many detection meth-
ods [1,2,12]. We ﬁnd that packet sampling impacts byte counts and
packet counts little, but impacts ﬂow counts heavily. This ﬁnding
suggests that anomalies that impact packet and byte volume only
will stand out even in sampled trafﬁc, but anomalies that impact
ﬂow counts alone (such as the Blaster worm in our data) are likely
to be discarded by packet sampling. Therefore detection schemes
based on ﬂow volume alone are likely to be inadequate for sampled
trafﬁc.
In addition to volume metrics, we also study the impact of packet
sampling on feature entropy metrics [13, 19]. The authors of [13]
showed that changes in distributions of trafﬁc features (ports and
addresses), when summarized by entropy, reveal a broad spectrum
of anomalies. We evaluated how effective entropy is at exposing
Blaster-type anomalies at increasing sampling rates. Our results
here are surprising: we ﬁnd that while ﬂow volume is grossly im-
pacted by packet sampling, ﬂow entropy is disturbed little. In par-
ticular, the Blaster worm in our data when measured in ﬂow counts
is dwarfed signiﬁcantly and is virtually undetectable at higher sam-
pling rates, but the worm remains largely unaffected by sampling
when measured from a baseline entropy. Thus, the structure of the
Blaster worm, as captured by entropy, is preserved even at high
sampling rates of 1 out of 1000. Our ﬁndings provide hope that
even though packet sampling produces imperfect trafﬁc views for
anomaly detection, there are metrics (such as entropy) that allow us
to harness useful information in sampled traces.
The rest of this paper is organized as follows. We next provide
an overview of our methodology. In Section 3, we introduce our
anomaly detection model and study the impact of packet sampling
on detecting ﬂow-based anomalies. In Section 4, we conclude and
outline directions for future work.
2. METHODOLOGY
In order to systematically evaluate the impact of packet sampling
on anomaly detection, one requires packet-level traces (at various
sampling rates) that ideally meet two criteria: (1) the traces contain
anomalies that are well understood, and (2) the traces span a long
duration (days to week). Known anomalies allow for evaluations
where the baseline is established using this knowledge. And, longer
traces are needed to compare the normal trafﬁc behavior with its di-
urnal and weekly pattern to the anomalous behavior. Unfortunately,
legal requirements (data protection) and technical limitations (stor-
age space), make it difﬁcult to collect such detailed packet-level
data. To circumvent the lack of suitable long-term packet traces,
we decided to work with unsampled ﬂow records and developed a
method to reconstruct packet-level traces from these ﬂow traces.
In this section, we introduce our unique dataset, which meets
the two criteria outlined above. We then describe our methodol-
ogy for reconstructing the underlying packet-level trace from the
ﬂow traces. Once we have the reconstructed packet trace, we can
apply packet sampling. We describe the sampling procedure, and
present ﬁrst results on the effect of sampling on volume and fea-
ture entropy metrics. Our results here underline the need to study
ﬂow-based anomalies in order to effectively evaluate the impact of
packet sampling; the last subsection provides and example of such
an anomaly (the Blaster worm) in our data.
2.1 Dataset
We are collecting data from the Swiss Academic and Research
Network (SWITCH) [18] since 2003. SWITCH is a medium-sized
Swiss backbone operator, connecting all Swiss universities and var-
ious research labs (e.g.,CERN, IBM) to the Internet. The SWITCH
IP address range contains about 2.2 million IP addresses. In 2003
SWITCH carried around 5% of all Swiss Internet trafﬁc [16]. In
2004, we captured on average 60 million NetFlow records per hour,
which is the full, unsampled number of ﬂows seen by the SWITCH
border routers. Basically, the border routers terminate and export
a ﬂow if one of the following conditions is met: (1) no packet has
been received for 30 seconds, (2) a ﬂow has been active for 15 min-
utes, implying that longer ﬂows are split and recorded in multiple
records, (3) a FIN or RST packet is received, and ﬁnally, (4) the
router runs out of memory.
For this paper, we collected unsampled data from the week of
August 8 to August 15, 2003. This dataset is unique because it
contains the Blaster worm outbreak described later in this section.
However, this dataset is inadequate, because it does not contain
the packet-level information needed to construct sampled views for
subsequent analysis. Therefore, we must ﬁrst reconstruct the un-
derlying packet-traces from ﬂow records.
2.2 Reconstructing Packet Traces
Our method to reconstruct the packet traces takes (unsampled)
NetFlow records from the SWITCH network as input and gener-
ates the corresponding packet traces. The output format of the
packet traces is again ﬂow records with ”ﬂows” that contain only
one packet. In contrast to real NetFlow records, the packet traces
contain ”ﬂows” that are sorted according to their start time.
The packet-trace reconstruction algorithm processes the ﬂows in
the order as they are stored in the ﬂow traces. For each of these
ﬂows it does the following: First, the size of the packet is calcu-
lated by dividing the total number of bytes B by the number of
packets N in the corresponding ﬂow. Since the number of bytes in
a packet is an integer but B/N does not have to be, some correc-
tions are required. To preserve the total number of bytes in a ﬂow,
we create N packets of size (cid:2)B/N(cid:3) and add to B mod N of them
another byte. Afterwards, the time stamp of the packet is randomly
selected within ﬂow bounds and with a resolution of one millisec-
ond. With this, the expected size of a packet in the ﬂow is equal
to B/N and the expected number of transferred bytes per millisec-
ond is N/M. We choose this very simple approach to reconstruct
the packet traces because the aggregation interval length is equal to
the maximal ﬂow length. Deviations from measurements with real
packet traces occur only if a ﬂow crosses the border of an aggrega-
tion interval (which occurs rarely).
Furthermore, by choosing the same packet size for all packets,
we preserve (on average) the often assumed (e.g., [7], [9]) constant
throughput property of ﬂows even if they are split over two inter-
vals. Recently, the authors of [20] presented empirical evidence
that the constant throughput property is a good approximation of
the behavior of large ﬂows (heavy hitter, elephant ﬂows) while still
being a reasonable approximation for small ones (mice ﬂows).
2.3 Effects of Sampling on Byte, Packet, and
Flow Metrics
Having reconstructed the packet traces from our NetFlow data,
we can now look at how timeseries of volume and feature entropy
metrics are impacted by packet sampling. Therefore, we sampled
our one-week data set at four different sampling rates of 1 out of
10, 1 out of 100, 1 out of 250, and 1 out of 1000. The sampling
method we applied is random probabilistic packet sampling. Thus,
1.0e+10
1.0e+09
1.0e+08
1.0e+07
1.0e+06
1.0e+05
1.0e+04
no sampling, 15min bins, tcp in
sampling 1/10, 15min bins, tcp in