pendency between two objects. For example, if an HTML object
includes two embedded objects in it, the referer of those two em-
bedded objects would be the HTML object. Also, if a user clicks a
link to move to a new Web page, the ﬁrst request of the new Web
page would have the referer ﬁeld of an object in the previous Web
page.
At a high level, each stream is a transitive closure of the referer
relation on the set of requests. Whenever the referer of a request is
empty, the request becomes the start (or root) of a new stream. If the
299l
l
a
c
e
R
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Time
Type
Time+Type
StreamStructure
 0.2
 0.4
 0.6
 0.8
 1
Precision
Figure 7: Streams, Web pages, initial pages, client-side interac-
tions, and main/embedded objects.
referer of the subsequent request matches with any of the requests
in the streams, we associate the request with the matched stream.
In case there is more than one matched stream, we choose the latest
one. If not found, we also create a new stream with the request –
this happens because its referer request could be a browser cache-
hit thus not present in the log.
Grouping requests with the referer relation allows isolating logs
from multiple browser instances or tabs, since they belong to dif-
ferent streams. It also helps identifying frames and client-side in-
teractions, since all the frames from the same Web page and all the
client-side interactions to the same Web page belong to the same
stream.
Even though the referer ﬁeld is optional, we can safely rely on
this information because most current browsers (Firefox and MSIE)
enable it by default. In fact, Firefox and MSIE together account
for more than 86% of our client population as discussed in Sec-
tion 2. When present, we use the referer ﬁeld to group requests into
streams.
Step 2. Detecting Main Objects Once we ﬁnish grouping streams,
we detect a main object for each stream. We ﬁrst generate main ob-
ject candidates by applying the type-based approach. This would
ﬁnd HTML frame objects as main object candidates, but non-HTML
interactions would be ignored. Among those main object candi-
dates, we discard those with no embedded object. This is based
on the observation that current Web pages are typically complex,
consisting of many embedded objects. We detect this by looking at
the referer of the next request. If it is not the preceding main object
candidate, we remove the preceding object from consideration.
Next, we apply the time-based approach to ﬁnalize the main ob-
ject selection. If the idle time is less than a given threshold, it is
likely that they belong to the same Web page – overlapping HTML
frame objects with a short idle time would be eliminated from the
selection.
It is noteworthy that we consider the idle time only
among the main object (HTML) candidates. This is because the
interactions in a Web page happen at an arbitrary point, and it bi-
ases the idle time calculation if included. Now all the remaining
objects between two main objects become the embedded objects
of its preceding main object. This way, we could include all the
interactions in a Web page as its embedded objects.
Step 3. Identifying Initial Pages The ﬁnal task of our algorithm is
to identify the initial pages, as the previous grouping still includes
client-side interactions in the Web pages. The basic idea is to apply
the time-based approach. However, simply checking the idle time
is inaccurate because the DNS lookup [2] or browser processing
time can vary signiﬁcantly, especially while processing the main
object before the page is fully loaded.
Figure 8: Precision and recall: StreamStructure outperforms
previous page detection algorithms, simultaneously achieving
high precision and recall. It is also robust to the idle time pa-
rameter selection.
To this end, we exploit the popular use of Google Analytics in
the pages.
It is a piece of JavaScript code that collects various
client-side information and reports to the analytics server when the
DOMContentLoaded event ﬁres. Thus, once we see this bea-
con in the access logs, we can safely assume that the Web page is
successfully loaded at that point, and start applying the time-based
approach to identify the initial page. Note that our algorithm can
also use other ways than the Google Analytics beacon to detect the
page loading event. For example, one could utilize beacons from
other analytics services, or even instrument Web pages with custom
JavaScript at the proxy.
Validation We validate the accuracy of StreamStructure and the
existing approaches on the manually collected data set by visiting
(via CoDeeN) the top 100 sites of Alexa’s list [5] with MSIE. We
not only visit the top-level pages, but also follow approximately ten
links from those top-level pages for each site, resulting 1,197 Web
pages in total. 3 We also record the URLs of those visited Web
pages (or main objects), and compare them with the URLs of the
Web pages found by each approach. Note that this data collection
is different from actual browsing patterns, but we believe that it is
sufﬁcient to capture the structure of representative Web pages and
thus useful for validation.
Figure 8 shows the precision and recall of various approaches.
Precision is deﬁned as the number of correct Web pages found di-
vided by the total number of Web pages found, and recall is de-
ﬁned as the number of correct Web pages found divided by the
total number of correct Web pages. For comparison, we also try
a simple combination of time-based and type-based approaches
(Time+Type) that does not exploit the stream and structure infor-
mation. Multiple data points represent the results of various idle
time (0.1, 0.2, 0.5, and 1–5 seconds) parameters.
The time-based approach performs in general very poorly, and
the best result achieves only a precision of 0.45 and a recall of
0.55. Also, the performance is very sensitive to the idle time selec-
tion. The type-based approach shows the highest recall above 0.88,
which implies that the main objects of most Web pages are HTML.
However, the precision is very low, only about 0.27. StreamStruc-
ture outperforms all of the previous approaches, achieving high
precision and recall above 0.8 simultaneously. Furthermore, it is
quite robust to the idle time parameter selection. The time+type
approach is less accurate, proving the importance of exploiting the
stream and structure information.
Finally, we investigate the sensitivity of the idle time parameter
3Some sites have many Web pages while others do not.
300 
n
o
c
a
e
B
e
g
o
o
G
h
l
t
i
w
s
e
g
a
P
%
US
CN
FR
BR
 40
 30
 20
 10
 0
 2006
 2007
 2009
 2010
 2008
Year
(a) % pages with Google Analytics bea-
con
)
c
e
s
(
y
c
n
e
t
a
L
e
g
a
P
n
a
d
e
M
i
 14
 13
 12
 11
 10
 9
 8
 7
 6
 5
 2006
 2007
 2008
Year
 2009
 2010
e
g
a
P
r
e
p
j
s
t
c
e
b
O
#
i
n
a
d
e
M
)
c
e
s
(
y
c
n
e
t
a
L
j
t
c
e
b
O
n
a
d
e
M
i
 18
 16
 14
 12
 10
 8
 6
 4
 2006
 2007
 2008
Year
 2009
 2010
)
B
K
(
i
e
z
S
e
g
a
P
n
a
d
e
M
i
 180
 160
 140
 120
 100
 80
 60
 40
 2006
 2007
 2008
Year
 2009
 2010
(b) Median # of objects per page
(c) Median page size
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 2006
 2007
 2009
 2010
 2008
Year
)
c
e
s
(
e
m
T
i
l
a
v
i
r
r
a
-
r
e
t
n
I
i
n
a
d
e
M
 0.35
 0.30
 0.25
 0.20
 0.15
 0.10
 0.05
 2006
 2007
 2009
 2010
 2008
Year
(d) Median page loading latency
(e) Median object latency
(f) Median object inter-arrival time
Figure 9: Initial page characteristics: The Google Analytics beacon gets increasingly popular, and pages get bigger both in terms of
the size and number of objects. On the other hand, the page loading latency has dropped in 2009 and 2010 because of the increased
number of concurrent connections and reduced object latency.
for identifying initial pages by comparing CDFs of the page loading
time, number of objects, and size of the initial pages with different
idle time thresholds. Overall, 23.9% of pages have Google Analyt-
ics beacons in our manually collected Alexa data set. We observe
that an idle time of 0.1 seconds is too short and 5 seconds is too
long, distorting the distribution signiﬁcantly. On the other hand,
an idle time between 0.5 and 2 seconds generates quite stable and
similar results.
4.3 Analysis Results
We apply the StreamStructure algorithm to our CoDeeN access
log data set, and analyze the derived Web pages in various aspects.
We choose the idle time of one second both for identifying Web
pages out of streams, and for identifying initial pages out of Web
pages. Among all the users, we ignore those who are active for less
than 30 minutes to reduce potential bias. We ﬁrst examine the char-
acteristics of initial pages, and analyze the page loading latency in
detail via simulations. Finally, we provide a simple characteriza-
tion of modern Web pages including client-side interactions.
Initial Page Characteristics We ﬁrst show the fraction of Web
pages that have a Google Analytics beacon in our data set in Fig-
ure 9 (a). It is less than 5% in 2006, but it has become increasingly
popular and accounts for about 40% in 2010. While there is a lit-
tle variation over time, the volume of the initial page trafﬁc roughly
accounts for about 40-60% of the entire Web trafﬁc in terms of both
requests and bytes. The rest of the trafﬁc is client-side interactions,
which is quite a signiﬁcant amount.
In Figure 9 (b)-(f), we examine the changes in the number of
objects, size, and latency of the initial pages, and the latency and
inter-arrival time of each individual object in the initial pages. We
compare the median values rather than the mean, since our page
detection algorithm is not perfect and the mean is more susceptible
to outliers/errors.
First of all, the pages have become increasingly complex where
we observe a consistent increase of the number of objects and the
total size in Figure 9 (b) and (c). For example, the median number
of objects per page in the United States sees an increase from 6
objects in 2006 to 12 objects in 2010, and the median page size
gets bigger from 69 KB in 2006 to 133 KB in 2010. This increase
is likely related to the popular use of advertisement/analytics, and
the object size increase of JavaScript and CSS in Figure 5 (a) and
(b).
While the page loading latency also sees a general increase in
Figure 9 (d) until 2008, instead we see it decrease in 2009 and
2010. For example in the United States, the median latency in-
creases from 5.13 seconds in 2006 to 8.45 seconds in 2008, but it
decreases to 5.98 seconds in 2010. 4 This decrease likely stems
from the increased number of concurrent connections in Figure 3.
Another decreasing factor is the reduced latency of fetching an ob-
ject in Figure 9 (e), and it also makes the object inter-arrival rate
burstier in Figure 9 (f). As the object size does not get smaller
over time, the decrease of the object latency is likely related to the
improved client bandwidth in Figure 1, as well as the improved
caching behavior of many Web sites.
Page Loading Latency Simulation As the page loading latency
is determined by many factors including the number of concurrent
connections per server, object latency, and dependency among ob-
jects, we examine the impact of these factors via simulations. Each
object is fetched from a central FIFO queue, and we use the mea-
sured object latency in the access logs for the simulated object la-
tency. The object dependency is extracted from the referer rela-
tions. Since the purpose of this simulation is to assess which fac-
tors affect the page loading latency rather than to predict the actual
latency, we simplify the simulation as follows. First, we ignore net-
work latency and browser parsing/processing time – there is no net-
4The actual user-perceived latency is smaller than our measured
latency because users recognize pages to be loaded before all of
the embedded objects are completely downloaded.
301%
 100
 80
 60
 40
 20
 0
% req % bytes
others
octet
audio
video
image
javascript
xml
css
html
s
h
m
e
lo
n
diu
g
o
rt
s
h
m
e
lo
n
diu
g
o
rt
m
m
(a) Page content type composition
)