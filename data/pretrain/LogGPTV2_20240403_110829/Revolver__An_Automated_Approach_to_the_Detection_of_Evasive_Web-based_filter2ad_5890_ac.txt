have been proposed. In particular, we solve it by using the
FLANN library [24].
In the remaining step, we compare sequence summaries
only with their malicious neighbors, thus dramatically re-
ducing the number of comparison to be performed.
Normalized node sequence similarity. Finally, we can
compute the similarity between two normalized node se-
quences. More precisely, Revolver compares the normalized
node sequence corresponding to a sequence summary s
with each normalized node sequence that corresponds to a
sequence summary of the malicious neighbors of s.
The similarity measurement is based on the pattern match-
ing algorithm by Ratcliff et al. [33]. More precisely, given
two node sequences, a and b, we ﬁrst ﬁnd their longest con-
tiguous common subsequence (LCCS). Then, we recursively
search for LCCS between the pieces of a and b to the left
and right of the matching subsequence. The similarity of a
and b is then returned as the number of nodes in common
divided by the total number of nodes in the malicious node
sequence. Therefore, identical ASTs will have similarity 1,
and the similarity values decrease toward zero as two ASTs
contain higher amounts of different nodes. This technique is
robust against injections, where one benign script includes a
malicious one, since all malicious nodes will be matched.
In addition to a numeric similarity score, the algorithm
also provides a list of insertions for the two node sequences,
that is, a list of AST nodes that would need to be added to one
sequence to transform it into the other one. This information
is very useful for our analysis, since it identiﬁes the actual
code that was added to an original malicious code.
After the similarity score is computed, we discard any
pairs that have a similarity below a predetermined thresh-
old τs.
Expansion. Once pairs of ASTs with high similarity have
been identiﬁed, we need to determine the Oracle’s classiﬁca-
tion of the scripts they originate from. We, therefore, expand
out any pairs that we deduplicated in the initial Deduplica-
tion step so that we associate the AST similarities to the
scripts that they correspond to.
3.4 Optimizations
There are several techniques that we utilize to improve
the results produced by the similarity detection steps. In
particular, our objective is to restrict the pairs identiﬁed
642  22nd USENIX Security Symposium 
USENIX Association
as similar to “interesting” ones, i.e., those that are more
likely to correspond to evasion attempts or signiﬁcant new
functionality. The techniques introduced here build upon
tree-based code similarity approaches and are speciﬁc to
malicious JavaScript.
Size matters. We observed that JavaScript code contains
a lot of very small scripts. In the extreme case, it includes
scripts comprising a single statement. We determined that
the majority of such scripts are generated dynamically
through calls to eval(), which, for example, dynamically
invoke a second function. Such tiny scripts are problematic
for our analysis: they have not enough functionality to per-
form malicious actions and they end up matching other short
scripts, but their similarity is not particularly relevant. As
a consequence, we combine ASTs that contain less than a
set number of nodes (τz). We do this by taking into account
how a script was generated: if another script generated code
under our threshold, we inline the generated script back to
its parent. If the script was not dynamically generated, then
we treat it as if one script contained all static code under our
threshold. This way the attacker cannot split the malicious
code into multiple parts under our threshold in order to evade
Revolver.
Repeated pattern detection. We also observed that, in
certain cases, an AST may contain a set of nodes repeated
a large number of times. This commonly occurs when the
script uses some JavaScript data structure that yields many
repeated AST nodes. For example, malicious scripts that un-
pack or deobfuscate their exploit payload frequently utilize
a JavaScript Array of elements to store the payload. Their
ASTs contain a node for every single element in the Array,
which, in many cases, may have thousands of instances. An
unwanted consequence, then, is that any script with a large
Array will be considered similar to the malicious script (due
to the high similarity of the array nodes), regardless of the
presence of a decoding/unpacking routine (which, instead,
is critical to determine the similarity of the scripts from a
functional point of view). These obfuscation artifacts affect
tree-based similarity algorithms, which will result in the
detection of similar code pairs where the common parts are
of no interest in the context of malicious JavaScript. To
avoid this problem, we identify sequences of nodes that are
repeated in a script more than a threshold (τp) and truncate
them.
Similarity fragmentation. Although we have identiﬁed
blocks of code that are shared across two scripts, it can be
the case that these blocks are not continuous. One script
can be broken down into small fragments that are matched
to the other script in different positions. This is why we
take into account the fragmentation of the matching blocks.
To prune these cases, we recognize a similarity only if the
fragmentation of the similarities is below a set threshold τ f .
AST
Executed nodes Classiﬁcation
=
∗
B ⊆ M
M ⊆ B
(cid:29)=
∗
=
(cid:29)=
(cid:29)=
(cid:29)=
Data-dependency
Data-dependency
JavaScript injection
Evasion
General evolution
Table 1: Candidate pairs classiﬁcation (B is a benign se-
quence, M is a malicious sequence, ∗ indicates a wildcard
value).
3.5 Classiﬁcation
The outcome of the previous similarity detection step is
a list of pairs of scripts that are similar. As we show in
§5.1 we can have hundreds of thousands of similar pairs.
Therefore, Revolver performs a classiﬁcation step of similar
pairs. That is, Revolver interprets the changes that were
made between two scripts and classiﬁes them. There are two
cases, depending on the Oracle’s classiﬁcation of the scripts
in a pair. If the pair consists solely of malicious scripts, then
we classify the similarity as a malicious evolution. The other
case is a pair in which one script is malicious and one script
is benign. We call such pairs candidate pairs (they need to
be further tested before we can classify their differences).
While the similarity detection has operated on a syntactic
level (essentially, by comparing AST nodes), Revolver now
attempts to determine the semantics of the differences.
In practice, Revolver classiﬁes the scripts and their similar-
ities into one of several categories, corresponding to different
cases where an Oracle may ﬂag differently scripts that are
similar. Table 1 summarizes the classiﬁcation algorithm
used by Revolver.
Data-dependency category. Revolver checks if a pair of
scripts belongs to the data-dependency category. A typical
example of scripts that fall into this category is packers.
Packers are tools that allow JavaScript authors to deliver
their code in a packed format, signiﬁcantly reducing the
size of the code. In packed scripts, the original code of
the script is stored as a compacted string or array, and its
clear-text version is retrieved at run-time by executing an
unpacking routine. Packers have legitimate uses (mostly,
size compression): in fact, several open-source popular
packers exist [9], and they are frequently used to distribute
packed version of legitimate JavaScript libraries, such as
jQuery. However, malware authors also rely on these very
same packers to obfuscate their code and make it harder to
be ﬁngerprinted.
Notice that the ASTs of packed scripts (generated by
the same packer) are identical, independently of their (un-
packed) payload: in fact, they consist of the nodes of the
unpacking routine (which is ﬁxed) and of the nodes holding
the packed data (typically, the assignment of a string literal
USENIX Association  
22nd USENIX Security Symposium  643
to a variable). However, the actual packed contents, which
eventually determine whether the script is malicious or be-
nign, are not retained at the AST level of the packer, but the
packed content will eventually determine the nature of the
overall script as benign or malicious.
Revolver categorizes as data-dependent pairs of scripts
that are identical and have different detection classiﬁcation.
As a slight variation to this scenario, Revolver also classi-
ﬁes as data-dependent pairs of scripts for which the ASTs are
not identical, but the set of nodes that were actually executed
are indeed the same. For example, this corresponds to cases
where a function is added to the packer but is never actually
executed during the unpacking.
Control-ﬂow differences. The remaining categories are
based on the analysis of AST nodes that are different in the
two scripts, and, speciﬁcally, of nodes representing control-
ﬂow statement. We focus on such nodes because they give
an attacker a natural way to implement a check designed
to evade detection. In fact, such checks generally test a
condition and modify the control ﬂow depending on the
result of the test.
More precisely, we consider the following control-ﬂow
related nodes: TRY, CATCH, CALL, WHILE, FOR, IF, ELSE,
HOOK, BREAK, THROW, SWITCH, CASE, CONTINUE,
RETURN, LT (), GE (>=), EQ (==)
, NE (! =), SHEQ (===), SNE (! ==), AND, and OR.
Depending on where these control-ﬂow nodes were added,
whether in the benign or in the malicious script, a candidate
pair can be classiﬁed as a JavaScript injection or an evasion.
Notice that we leverage here the execution bits to detect
control ﬂow changes that were actually executed and affected
the execution of code that was found as malicious before.
JavaScript injection category. In some cases, malware
authors insert malicious JavaScript code into existing benign
scripts on a compromised host. This is done because, when
investigating a compromise, webmasters may neglect to
inspect ﬁles that are familiar to them, and thus such injections
can go undetected. In particular, it is common for malware
authors to add their malicious scripts to the code of popular
JavaScript libraries hosted on a compromised site, such as
jQuery and SWFObject.
In these cases, Revolver identiﬁes similarities between
a benign script (the original, legitimate jQuery code) and a
malicious script (the library with the added malicious code).
In addition, Revolver detects that the difference between
the two scripts is due to the presence of control-ﬂow nodes
in the malicious script (the additional code added to the
library), which are missing in the benign script. Revolver
classiﬁes such similarities as JavaScript injections, since the
classiﬁcation of the analyzed script changes from benign to
malicious due to the introduction of additional code in the
malicious version of the script.
Evasions category. Pairs of scripts that only differ because
of the presence of additional control-ﬂow nodes in the benign
script are categorized as evasions. In fact, these correspond
to cases where a script, which was originally ﬂagged as ma-
licious by an Oracle, is modiﬁed to include some additional
functionality that modiﬁes its control ﬂow (i.e., an evasive
check) and, as a consequence, appears to be benign to the
Oracle.
General evolution cases. Finally, if none of the previous
categories applies to the current pair of scripts, it means
that their differences are caused by the insertion of control-
ﬂow nodes in both the benign and malicious scripts. Unlike
similarities in the evasion category, these similarities may
signify generic evolution between the two scripts. Revolver
ﬂags these cases for manual review, at a lower priority than
evasive cases.
4
Implementation
In this section, we discuss speciﬁc implementation choices
for our approach.
We used the Wepawet honeyclient [6] as the Oracle of
Revolver. In particular, the input to Revolver was the web
pages processed by the Wepawet tool at real-time together
with their detection classiﬁcation. We used Revolver to
extract ASTs from the pages analyzed by Wepawet, and to
perform the similarity processing described in the previous
sections.
As our processing infrastructure, we used a cluster of four
worker machines to process submissions in parallel with the
Oracle. Notice that all the steps in Revolver’s processing can
be easily parallelized. In terms of performance, we managed
to process up to 591,543 scripts on a single day, which was
the maximum number of scripts that we got on a single day
from the Oracle during our experiments.
We will now discuss the parameters that can be tuned in
our algorithms (discussed in §3), explaining the concrete
values we have chosen for our experiments.
Minimum tree size (τz). We chose 25 nodes as the min-
imum size of the AST trees that we will process before
combining them to their parent. Smaller ASTs can result
from calls to eval with tiny arguments, and from calls to
short event handlers, such as onLoad and onMouseOver. We
expect that such small ASTs correspond to short scripts that
do not implement any interesting functionality alone, but
complement the functionality of their parent script.
Minimum pattern size (τp). Another threshold that we
set is the minimum pattern size. Any node sequence that is
repeated more than this threshold is truncated to the threshold
value. The primary application of pattern detection is to
handle similar packers that decode payloads of different size.
We chose 16 for this value, as current packers either work on
relatively long arrays (longer than 16, and thus detected) or
on single strings (one node, and thus irrelevant to this issue).
This amount also excludes the possibility of compressing
interesting code sequences, since we rarely see such long
644  22nd USENIX Security Symposium 
USENIX Association
USENIX Association  
22nd USENIX Security Symposium  645
Figure5:Numberofdetectedsimilaritiesasafunctionofthedistancethreshold.Figure6:Theresultingamountofsimilaritiesfordifferentsimilaritythresholds.patternsoutsideofpackedpayloads.Reducingthisvaluewouldhavetheeffectofmakingthetreesimilarityalgorithmmuchmorelax.Nearestneighborthreshold(τn).Inthenearestneighborscomputation,wediscardnodesequencesthatarefartherthanagivendistancedfromthenodesequencecurrentlybeinginspected.Weempiricallydeterminedavalueforthisparameter,byevaluatingvariousvaluesfordandinspectingtheresultingsimilarities.FromFigure5,itisapparentthattheamountofsimilaritiesthataredetectedlevelsofffairlysharplypastd=1,000.Wedeterminedthat10,000isasafethresholdthatincludesamajorityoftreeswhileallowingthesimilaritycalculationtobecomputationallyfeasible.Normalizednodesequencesimilaritythreshold(τs).Carehastobetakenwhenchoosingthethresholdusedtoidentifysimilarnormalizednodesequences.Intuitively,ifthisvalueistoolow,weriskintroducingsigniﬁcantnoiseintoouranalysis,whichwillmakeRevolverconsiderassimilarscriptsthatinrealityarenotrelatedtoeachother.Onthecontrary,ifthevalueistoohigh,itwilldiscardinterestingsimilarities.Experimentally(seeFigure6),wedeterminedthatthisoccursforsimilarityvaluesinthe70%–80%inter-val.Therefore,wechose75%asoursimilaritythreshold(inotherwords,onlynodesequencesthatare75%ormoresimilararefurtherconsideredbyRevolver).CategorySimilarScripts#GroupsbymaliciousASTJavaScriptInjections6,996701Data-dependencies101,039475Evasions4,147155Generalevolutions2,490273Total114,6721,604Table2:BenignscriptsfromWepawetthathavesimilaritieswithmaliciousscriptsandtheirclassiﬁcationfromRevolver.5EvaluationWeevaluatedtheabilityofRevolvertoaidindetectingevasivechangestomaliciousscriptsinreal-worldscenarios.WhileRevolvercanbeleveragedtosolveotherproblems,wefeelthatautomaticallyidentifyingevasionsisthemostimportantcontributiontoimprovingthedetectionofweb-basedmalware.5.1EvasionsinthewildRevolveridentiﬁespossibleevasionattemptsbyidentifyingsimilaritiesbetweenmaliciousandbenigncode.Therefore,Revolver’sinputistheoutputofanyOraclethatclassiﬁesJavaScriptcodeaseithermaliciousorbenign.ToevaluateRevolver,wecontinuouslysubmittedtoRevolverallwebpagesthatWepawetexamined.SinceSeptember2012,wecollected6,468,623webpagesoutofwhich265,692weremalicious.Weanalyzed20,732,766totalbenignscriptsand186,032totalmaliciousscripts.Outofthesescripts,weobtained705,472uniquebenignASTsand5,701uniquemaliciousASTs.RevolverappliedtheASTsimilarityanalysisdescribedinSection3,andextractedthepairsofsimilarASTs.Table2summarizestheresultsofclassifyingthesesimilaritiesinthecategoriesconsideredbyRevolver.Inparticular,Revolveridentiﬁed6,996scriptswheremaliciousJavaScriptwasinjected,101,039scriptswithdata-dependencies,4,147evasivescripts,and2,490scriptsasgeneralevolutions.Weobservethatmanyofthesescriptscanbeeasilygroupedbytheirsimilaritieswiththesamemaliciousscript.Therefore,foreaseofanalysis,wegroupthepairsbytheirmaliciousASTcomponent,andidentify701JavaScriptinjections,475data-dependencies,155evasions,and273generalevolu-tions.Ourresultsindicateahighnumberofmaliciousscriptsthatsharesimilaritieswithbenignones.Thisisduetothefactthatinjectionsanddata-dependentmaliciousscriptsnaturallysharesimilaritieswithbenignscriptsandweareobservingmanyoftheseattacksinthewild.ToverifytheresultsproducedbyRevolver,wemanuallyanalyzedallgroupscategorizedas“evasions”.Fortherest of the categories we grouped the malicious ASTs into
families based on their similarities with each other and
examined a few similar pairs from each family. We found
the results for the JavaScript injection and data-dependencies
categories to be correct. The reason why Revolver classiﬁed
a large number of scripts as data-dependencies is due to the
extensive use of a few popular packers, such as the Edwards’
packer [9]. For example, the jQuery library was previously
ofﬁcially distributed in a packed state to reduce its size.
Of the 155 evasions groups, we found that only ﬁve were
not intended evasion attempts. We cannot describe all eva-
sions in detail here, but we provide a brief summary for the
most interesting ones in the next section.
The pairs in the “general evolutions” category consisted of
cases where Revolver identiﬁed control ﬂow changes in both
the benign and malicious scripts. We manually looked into
them and did not ﬁnd any behavior that could be classiﬁed
as evasive.
5.2 Evasions case studies
The evasions presented here exploit differences in the im-
plementation of Wepawet’s JavaScript interpreter and the
one used by regular browsers. Notice that these evasions
can affect Oracles other than Wepawet; in particular, low-
interaction honeyclients, such as the popular jsunpack [13]
and PhoneyC [25].
We describe in more detail a subset of the evasions that
we found from our experiment on real-world data. In the 22
evasion groups described here, we identiﬁed seven distinct
evasion techniques, and one programming mistake in a
malicious PDF.
We found three cases which leveraged subtle details in
the handling of regular expressions and Unicode to cause
a failure in a deobfuscation routine when executing in the