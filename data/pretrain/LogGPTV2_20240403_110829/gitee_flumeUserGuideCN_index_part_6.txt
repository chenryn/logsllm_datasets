  属性                         默认值   解释
  ---------------------------- -------- ----------------------------------------------------------------------------------------------------------------
  deserializer.maxLineLength   2048     每个Event数据所包含的最大字符数，如果一行文本字符数超过这个配置就会被截断，剩下的字符会出现再后面的Event数据里
  deserializer.outputCharset   UTF-8    解析Event所使用的编码
::: hint
::: title
Hint
:::
*deserializer.maxLineLength*
的默认值是2048，这个数值对于日志行来说有点小，如果实际使用中日志每行字符数可能超过2048，超出的部分会被截断，千万记得根据自己的日志长度调大这个值。
:::
###### AVRO
这个反序列化器能够读取avro容器文件，并在文件中为每个Avro记录生成一个Event。每个Event都会在header中记录它的模式。Event的body是二进制的avro记录内容，不包括模式和容器文件元素的其余部分。
注意如果Spooling Directory
Source发生了重新把一个Event放入channel的情况（比如，通道已满导致重试），则它将重置并从最新的Avro容器文件同步点重试。
为了减少此类情况下的潜在Event重复，请在Avro输入文件中更频繁地写入同步标记。
  属性名                    默认值   解释
  ------------------------- -------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  deserializer.schemaType   HASH     如何表示模式。 默认或者指定为 `HASH` 时，会对Avro模式进行哈希处理，并将哈希值存储在Event header中以"flume.avro.schema.hash"这个key。 如果指定为 `LITERAL` ，则会以JSON格式的模式存储在Event header中以"flume.avro.schema.literal"这个key。 与HASH模式相比，使用LITERAL模式效率相对较低。
###### BlobDeserializer
这个反序列化器可以反序列化一些大的二进制文件，一个文件解析成一个Event，例如pdf或者jpg文件等。\**注意这个解析器不太适合解析太大的文件，因为被反序列化的操作是在内存里面进行的*\*。
  属性                         默认值      解释
  ---------------------------- ----------- --------------------------------------------------------------------------------------------------------------
  **deserializer**             \--         这个解析器没有别名缩写，需要填类的全限定名： `org.apache.flume.sink.solr.morphline.BlobDeserializer$Builder`
  deserializer.maxBlobLength   100000000   每次请求的最大读取和缓冲的字节数，默认这个值大概是95.36MB
#### Taildir Source
::: note
::: title
Note
:::
**Taildir Source目前只是个预览版本，还不能运行在windows系统上。**
:::
Taildir
Source监控指定的一些文件，并在检测到新的一行数据产生的时候几乎实时地读取它们，如果新的一行数据还没写完，Taildir
Source会等到这行写完后再读取。
Taildir
Source是可靠的，即使发生文件轮换（译者注1）也不会丢失数据。它会定期地以JSON格式在一个专门用于定位的文件上记录每个文件的最后读取位置。如果Flume由于某种原因停止或挂掉，它可以从文件的标记位置重新开始读取。
Taildir
Source还可以从任意指定的位置开始读取文件。默认情况下，它将从每个文件的第一行开始读取。
文件按照修改时间的顺序来读取。修改时间最早的文件将最先被读取（简单记成：先来先走）。
Taildir
Source不重命名、删除或修改它监控的文件。当前不支持读取二进制文件。只能逐行读取文本文件。
::: hint
::: title
Hint
:::
译者注1：文件轮换（file
rotate）就是我们常见的log4j等日志框架或者系统会自动丢弃日志文件中时间久远的日志，一般按照日志文件大小或时间来自动分割或丢弃的机制。
:::
  属性名                                    默认值                            解释
  ----------------------------------------- --------------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **channels**                              \--                               与Source绑定的channel，多个用空格分开
  **type**                                  \--                               组件类型，这个是： `TAILDIR`.
  **filegroups**                            \--                               被监控的文件夹目录集合，这些文件夹下的文件都会被监控，多个用空格分隔
  **filegroups.\**          \--                               被监控文件夹的绝对路径。正则表达式（注意不会匹配文件系统的目录）只是用来匹配文件名
  positionFile                              \~/.flume/taildir_position.json   用来设定一个记录每个文件的绝对路径和最近一次读取位置inode的文件，这个文件是JSON格式。
  headers.\.\   \--                               给某个文件组下的Event添加一个固定的键值对到header中，值就是value。一个文件组可以配置多个键值对。
  byteOffsetHeader                          false                             是否把读取数据行的字节偏移量记录到Event的header里面，这个header的key是byteoffset
  skipToEnd                                 false                             如果在 *positionFile* 里面没有记录某个文件的读取位置，是否直接跳到文件末尾开始读取
  idleTimeout                               120000                            关闭非活动文件的超时时间（毫秒）。如果被关闭的文件重新写入了新的数据行，会被重新打开
  writePosInterval                          3000                              向 *positionFile* 记录文件的读取位置的间隔时间（毫秒）
  batchSize                                 100                               一次读取数据行和写入channel的最大数量，通常使用默认值就很好
  maxBatchCount                             Long.MAX_VALUE                    控制从同一文件连续读取的行数。 如果数据来源是通过tail多个文件的方式，并且其中一个文件的写入速度很快，则它可能会阻止其他文件被处理，因为这个繁忙文件将被无休止地读取。 在这种情况下，可以调低此参数来避免被一直读取一个文件
  backoffSleepIncrement                     1000                              在最后一次尝试未发现任何新数据时，重新尝试轮询新数据之前的时间延迟增量（毫秒）
  maxBackoffSleep                           5000                              每次重新尝试轮询新数据时的最大时间延迟（毫秒）
  cachePatternMatching                      true                              对于包含数千个文件的目录，列出目录并应用文件名正则表达式模式可能非常耗时。 缓存匹配文件列表可以提高性能。 消耗文件的顺序也将被缓存。 要求文件系统支持以至少秒级跟踪修改时间。
  fileHeader                                false                             是否在header里面存储文件的绝对路径
  fileHeaderKey                             file                              文件的绝对路径存储到header里面使用的key
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = TAILDIR
a1.sources.r1.channels = c1
a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /var/log/test1/example.log
a1.sources.r1.headers.f1.headerKey1 = value1
a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
a1.sources.r1.headers.f2.headerKey1 = value2
a1.sources.r1.headers.f2.headerKey2 = value2-2
a1.sources.r1.fileHeader = true
a1.sources.ri.maxBatchCount = 1000
```
#### Twitter 1% firehose Source (实验性的)
::: warning
::: title
Warning
:::
这个source
纯粹是实验性的，之后的版本可能会有改动，使用中任何风险请自行承担。
:::
::: hint
::: title
Hint
:::
从Google上搜了一下twitter firehose到底是什么东西，找到了这个 [What is
Twitter firehose and who can use
it?](https://www.quora.com/What-is-Twitter-firehose-and-who-can-use-it)，
类似于Twitter提供的实时的消息流服务的API，只有少数的一些合作商公司才能使用，对于我们普通的使用者来说没有任何意义。本节可以跳过不用看了。
:::
这个Source通过流API连接到1%的样本twitter信息流并下载这些tweet，将它们转换为Avro格式，并将Avro
Event发送到下游Flume。使用者需要有Twitter开发者账号、访问令牌和秘钥。
必需的参数已用 **粗体** 标明。
  属性                     默认值   解释
  ------------------------ -------- --------------------------------------------------------------------
  **channels**             \--      与Source绑定的channel，多个用空格分开
  **type**                 \--      组件类型，这个是： `org.apache.flume.source.twitter.TwitterSource`
  **consumerKey**          \--      OAuth consumer key
  **consumerSecret**       \--      OAuth consumer secret
  **accessToken**          \--      OAuth access token
  **accessTokenSecret**    \--      OAuth token secret
  maxBatchSize             1000     每次获取twitter数据的数据集大小，简单说就是一次取多少
  maxBatchDurationMillis   1000     每次批量获取数据的最大等待时间（毫秒）
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.twitter.TwitterSource
a1.sources.r1.channels = c1
a1.sources.r1.consumerKey = YOUR_TWITTER_CONSUMER_KEY
a1.sources.r1.consumerSecret = YOUR_TWITTER_CONSUMER_SECRET
a1.sources.r1.accessToken = YOUR_TWITTER_ACCESS_TOKEN
a1.sources.r1.accessTokenSecret = YOUR_TWITTER_ACCESS_TOKEN_SECRET
a1.sources.r1.maxBatchSize = 10
a1.sources.r1.maxBatchDurationMillis = 200
```
#### Kafka Source
Kafka Source就是一个Apache Kafka消费者，它从Kafka的topic中读取消息。
如果运行了多个Kafka
Source，则可以把它们配置到同一个消费者组，以便每个source都读取一组唯一的topic分区。
目前支持Kafka 0.10.1.0以上版本，最高已经在Kafka
2.0.1版本上完成了测试，这已经是Flume 1.9发行时候的最高的Kafka版本了。
  属性名                                                            默认值      解释
  ----------------------------------------------------------------- ----------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **channels**                                                      \--         与Source绑定的channel，多个用空格分开
  **type**                                                          \--         组件类型，这个是： `org.apache.flume.source.kafka.KafkaSource`
  **kafka.bootstrap.servers**                                       \--         Source使用的Kafka集群实例列表
  kafka.consumer.group.id                                           flume       消费组的唯一标识符。如果有多个source或者Agent设定了相同的ID，表示它们是同一个消费者组
  **kafka.topics**                                                  \--         将要读取消息的目标 Kafka topic 列表，多个用逗号分隔
  **kafka.topics.regex**                                            \--         会被Kafka Source订阅的 topic 集合的正则表达式。这个参数比 kafka.topics 拥有更高的优先级，如果这两个参数同时存在，则会覆盖kafka.topics的配置。
  batchSize                                                         1000        一批写入 channel 的最大消息数
  batchDurationMillis                                               1000        一个批次写入 channel 之前的最大等待时间（毫秒）。达到等待时间或者数量达到 batchSize 都会触发写操作。
  backoffSleepIncrement                                             1000        当Kafka topic 显示为空时触发的初始和增量等待时间（毫秒）。等待时间可以避免对Kafka topic的频繁ping操作。默认的1秒钟对于获取数据比较合适， 但是对于使用拦截器时想达到更低的延迟可能就需要配置更低一些。
  maxBackoffSleep                                                   5000        Kafka topic 显示为空时触发的最长等待时间（毫秒）。默认的5秒钟对于获取数据比较合适，但是对于使用拦截器时想达到更低的延迟可能就需要配置更低一些。
  useFlumeEventFormat                                               false       默认情况下，从 Kafka topic 里面读取到的内容直接以字节数组的形式赋值给Event。如果设置为true，会以Flume Avro二进制格式进行读取。与Kafka Sink上的同名参数或者 Kafka channel 的parseAsFlumeEvent参数相关联，这样以对象的形式处理能使生成端发送过来的Event header信息得以保留。
  setTopicHeader                                                    true        当设置为 `true` 时，会把存储Event的topic名字存储到header中，使用的key就是下面的 *topicHeader* 的值。
  topicHeader                                                       topic       如果 *setTopicHeader* 设置为 `true` ，则定义用于存储接收消息的 topic 使用header key。注意如果与 [Kafka Sink](#kafka-sink) 的 topicHeader 参数一起使用的时候要小心，避免又循环将消息又发送回 topic。
  kafka.consumer.security.protocol *more consumer security props*   PLAINTEXT   设置使用哪种安全协议写入Kafka。可选值：`SASL_PLAINTEXT` 、 `SASL_SSL` 和 `SSL` ,有关安全设置的其他信息，请参见下文。 如果使用了SASL_PLAINTEXT、SASL_SSL 或 SSL 等安全协议，参考 [Kafka security](http://kafka.apache.org/documentation.html#security) 来为消费者增加安全相关的参数配置
  Other Kafka Consumer Properties                                   \--         其他一些 Kafka 消费者配置参数。任何 Kafka 支持的消费者参数都可以使用。唯一的要求是使用"kafka.consumer."这个前缀来配置参数，比如： `kafka.consumer.auto.offset.reset`
::: note
::: title
Note
:::
Kafka Source 覆盖了两个Kafka 消费者的参数：auto.commit.enable
这个参数被设置成了false，Kafka Source 会提交每一个批处理。Kafka Source
保证至少一次消息恢复策略。 Source 启动时可以存在重复项。Kafka Source
还提供了key.deserializer（org.apache.kafka.common.serialization.StringSerializer）
和
value.deserializer（org.apache.kafka.common.serialization.ByteArraySerializer）的默认值，不建议修改这些参数。
:::
已经弃用的一些属性：
  属性名                    默认值   解释
  ------------------------- -------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  topic                     \--      改用 kafka.topics
  groupId                   flume    改用 kafka.consumer.group.id
  zookeeperConnect          \--      自0.9.x起不再受kafka消费者客户端的支持。以后使用kafka.bootstrap.servers与kafka集群建立连接
  migrateZookeeperOffsets   true     如果找不到Kafka存储的偏移量，去Zookeeper中查找偏移量并将它们提交给 Kafka 。 它应该设置为true以支持从旧版本的FlumeKafka客户端无缝迁移。 迁移后，可以将其设置为false，但通常不需要这样做。 如果在Zookeeper未找到偏移量，则可通过kafka.consumer.auto.offset.reset配置如何处理偏移量。可以从 [Kafka documentation](http://kafka.apache.org/documentation.html#newconsumerconfigs) 查看更多详细信息。
通过逗号分隔的 topic 列表进行 topic 订阅的示例：
``` properties
tier1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource
tier1.sources.source1.channels = channel1
tier1.sources.source1.batchSize = 5000
tier1.sources.source1.batchDurationMillis = 2000
tier1.sources.source1.kafka.bootstrap.servers = localhost:9092
tier1.sources.source1.kafka.topics = test1, test2
tier1.sources.source1.kafka.consumer.group.id = custom.g.id
```
正则表达式 topic 订阅的示例：
``` properties
tier1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource
tier1.sources.source1.channels = channel1
tier1.sources.source1.kafka.bootstrap.servers = localhost:9092
tier1.sources.source1.kafka.topics.regex = ^topic[0-9]$
# the default kafka.consumer.group.id=flume is used
```
**安全与加密：** Flume 和 Kafka
之间通信渠道是支持安全认证和数据加密的。对于身份安全验证，可以使用 Kafka
0.9.0版本中的 SASL、GSSAPI （Kerberos V5） 或 SSL
（虽然名字是SSL，实际是TLS实现）。
截至目前，数据加密仅由SSL / TLS提供。
当你把 *kafka.consumer.security.protocol*
设置下面任何一个值的时候意味着：
-   **SASL_PLAINTEXT** - 无数据加密的 Kerberos 或明文认证
-   **SASL_SSL** - 有数据加密的 Kerberos 或明文认证
-   **SSL** - 基于TLS的加密，可选的身份验证。
::: warning
::: title
Warning
:::
启用SSL时性能会下降，影响大小取决于 CPU 和 JVM 实现。参考 [Kafka
security
overview](http://kafka.apache.org/documentation#security_overview) 和
[KAFKA-2561](https://issues.apache.org/jira/browse/KAFKA-2561) 。
:::
**使用TLS：**