# Adapting Security Warnings to Counter Online Disinformation

**Authors:**
- Ben Kaiser, Princeton University
- Jerry Wei, Princeton University
- Eli Lucherini, Princeton University
- Kevin Lee, Princeton University
- J. Nathan Matias, Cornell University
- Jonathan R. Mayer, Princeton University

## Abstract
Disinformation is proliferating on the internet, and platforms are responding by attaching warnings to content. However, there is limited evidence that these warnings help users identify or avoid disinformation. In this work, we adapt methods and results from the information security warning literature to design and evaluate effective disinformation warnings.

In an initial laboratory study, we used a simulated search task to examine contextual and interstitial disinformation warning designs. We found that users often ignore contextual warnings but notice interstitial warnings, leading them to seek information from alternative sources. We then conducted a follow-on crowdworker study with eight interstitial warning designs. We confirmed a significant impact on user information-seeking behavior and found that the design of a warning can effectively inform users or convey a risk of harm. However, neither user comprehension nor fear of harm moderated behavioral effects.

Our work provides evidence that well-designed disinformation warnings can help users identify and avoid disinformation. We offer a path forward for designing effective warnings and contribute repeatable methods for evaluating their behavioral effects. We also highlight a potential dilemma: disinformation warnings might guide behavior through user experience friction rather than informed decision-making.

## 1. Introduction
Disinformation is spreading widely on the internet, often driven by political motives [1, 2]. Platforms are responding by attaching warnings to disinformation content to inform users and guide their actions. Facebook implemented disinformation warnings as early as December 2016 [3], and Google [4], Bing [5], and Twitter [6] have adopted similar content notices. There has been substantial public debate about the propriety of disinformation warnings, especially after Twitter began labeling tweets by U.S. President Donald Trump in May 2020 [7]. Recent studies provide limited evidence that these warnings meaningfully influence user beliefs or behaviors, raising questions about their effectiveness in combating disinformation.

Security researchers faced a similar challenge over a decade ago when studies showed that warnings for malware, phishing, and other online threats broadly failed to protect users [8, 9]. After a series of iterative, multi-method studies [10–21], security warnings now reliably inform user security decisions and help users avoid harmful and inauthentic content [10, 17].

In this work, we adapt methods and results from the information security warning literature to design and evaluate effective disinformation warnings. A key finding from security research that we adapt to disinformation is that contextual warnings, which do not interrupt the user or compel action, are far less effective at changing behavior than interstitial warnings, which interrupt the user and require interaction [8, 9, 17]. Our work, to our knowledge, is the first to evaluate interstitial disinformation warnings.

Another relevant contribution from the security literature is a set of rigorous qualitative and quantitative methods for evaluating warnings, including structured models, realistic guided tasks, user interviews, and field studies (e.g., [11, 13, 15–18]). Our work adapts these methods to empirically examine contextual and interstitial disinformation warnings.

Across two studies, we use qualitative approaches (think-aloud exercises, interviews, and inductive coding) to understand user perceptions of disinformation warnings, as well as quantitative measures of the warnings' effects on user behavior. We consider the following research questions:
1. How often do users change their behavior by opting for alternative sources of information after encountering contextual and interstitial disinformation warnings?
2. Why do some users choose not to change their behaviors after encountering contextual and interstitial disinformation warnings?
3. Can highly informative interstitial warnings effectively change user behavior?
4. Can highly threatening interstitial warnings effectively change user behavior?

We first conducted a laboratory experiment (n = 40) where participants searched for specific facts on Google and encountered interstitial or contextual disinformation warnings for certain search results (Section 3). The interstitial warning was substantially more effective at changing user behavior than the contextual warning, primarily because users did not notice or comprehend the more subtle contextual warning. In post-task interviews, participants described two reasons for the interstitial warning's strong behavioral effect: the informativeness of the warning's messaging and the risk of harm conveyed by the warning's threatening design.

We then conducted a follow-on crowdworker study (n = 238), examining eight interstitial warning designs (Section 4). We confirmed the strong behavioral effects of interstitial warnings. However, we found that neither user comprehension nor perceived risk of harm appeared to moderate those effects.

Our results provide evidence that interstitial disinformation warnings can both inform users and guide user behavior. We demonstrate scalable and repeatable methods for measuring warning effectiveness and testing theories of effect. We also surface a possible dilemma: the behavioral effects of disinformation warnings may be attributable to user experience friction rather than informed decision-making. Our work highlights a path forward for designing effective warnings, and we encourage iterative research and improvement for disinformation warnings—similar to the successful approach taken by the information security community for security warnings.

## 2. Background and Related Work
Disinformation research spans multiple academic disciplines. Recent work has focused on measurement (e.g., of content, campaigns, or user interactions) [22–40] or on developing automated detection methods [41–55].

### 2.1 Disinformation Websites
Disinformation campaigns often exploit multiple social and media channels simultaneously [56]. These campaigns use websites to host content for distribution across platforms and generate ad revenue [25, 28, 57–60]. Disinformation websites are designed to deceive users into believing they are legitimate news outlets. Our work examines whether warnings can counter this deception and help users contextualize or avoid disinformation websites.

### 2.2 Effects of Disinformation
Disinformation campaigns hijack the heuristics that users rely on to judge the truthfulness of information [63]. For example, disinformation campaigns often mimic credibility indicators from real news sources [64] or use social media bots to create the appearance of support [29]. Misperceptions that individuals hold after consuming disinformation are difficult to dispel [63]. Collectively, a misinformed populace may make social and political decisions that are not in society's best interests [65], such as failing to mitigate climate change [66]. Influencing policy by shaping public perception and creating division is a goal of many campaigns, especially by state-level actors [67].

Presenting a warning before exposure to disinformation can prevent harmful effects in several ways. Warnings can induce skepticism, making users less likely to take disinformation at face value [68]. Warnings can also make users more susceptible to corrections [63, 69]. Finally, warnings may cause users to avoid reading the disinformation altogether.

### 2.3 Responses to Disinformation
There are three main types of responses to disinformation that platforms and researchers have considered [70]:
1. **Deranking disinformation** by changing recommendation algorithms [71]. Academics have studied this approach in simulated models of social networks [72–77], though not in realistic settings or with real users.
2. **Removing disinformation content** and banning accounts that promote disinformation [78–80]. Neither platforms nor researchers have established evidence on the effects of these takedowns.
3. **Adding warnings and context** to posts [4–6, 81, 82].

These responses have important speech distinctions. Removing content unilaterally makes speech less accessible to users. Deranking leaves the content available but curtails its distribution and discoverability. The potential promise of disinformation warnings is that they respond to problematic speech with counterspeech: platforms inform and protect users without making unilateral decisions about content availability, distribution, or discoverability. As discussed in Section 4.8, our work poses a possible dilemma for disinformation warnings as speech regulation: warnings can inform users and guide behavior, but the behavioral effects may not be attributable to informed decision-making.

### 2.4 Fact Check Warnings
The most well-studied disinformation warnings are contextual labels indicating a story has been "disputed" or "rated false" by fact checkers [83–88]. These labels constituted Facebook’s first major effort to counter disinformation [81], and Google [4], Bing [5], and Twitter [6] have taken similar approaches. Facebook eventually discontinued "disputed" warnings after internal studies found them to be of limited utility [81]. More recently, Facebook, Instagram, and Twitter deployed new warning formats, including interstitials [89, 90].

Some studies of fact check warnings reported no significant effects on participant perceptions of disinformation [84, 85], while others found moderate effects under certain conditions [83, 87, 88, 91, 92]. Pennycook et al. found that fact check warnings caused participants to rate disinformation as less accurate after repeated warning exposure, but not with a single exposure [83]. Another study by Pennycook et al. identified a counterproductive implied truth effect: attaching a fact check warning to some headlines caused participants to perceive other headlines as more accurate [87]. Seo et al. found that fact check warnings caused participants to perceive stories as less accurate, but the effect did not persist when participants encountered the same stories later [88]. Mena found that fact check warnings had small negative effects on perceived credibility of news content on social media and self-reported likelihood of sharing [91]. Moravec et al. examined how fact check warnings can induce instinctual cognitive responses in users and cause them to thoughtfully incorporate new information into their decision-making; a warning design that combined both mechanisms showed a moderate effect on social media post believability [92].

### 2.5 Related Links
Bode and Vraga examined the effects of providing related links to alternative, credible sources of information alongside misinformation [93]. The study found that when related links debunked misinformation, participants who initially believed the disinformation showed a limited tendency toward corrected beliefs. Facebook, Google Search, and Bing all currently use related links warning designs.

### 2.6 Highlighting Falsehoods
Garrett et al. tested a two-part warning, where participants were first informed that a fact-checker had identified factual errors in a story, then those errors were highlighted in the body of the story [94]. Among users already predisposed to reject the misinformation, this treatment significantly increased the accuracy of beliefs, but it had no effect among users inclined to believe the misinformation.

### 2.7 Methods of Prior Work
In these studies, participants were presented with screenshots of simulated social media posts and posed survey questions about the truthfulness of the posts and their likelihood of sharing them on social media. These methods can inform theories about how users will respond in real-world settings, but generalizations are tenuous because the methods involve highly artificial tasks and self-reported predictions about behavior. Security research has found that to measure realistic responses to warnings, it is important to design experimental tasks that involve realistic systems, realistic risks, and actual participant behavior [9, 11, 95, 96].

### 2.8 Security Warnings
Effective warnings are essential for security, as there are certain security decisions that systems cannot consistently make on behalf of users. Adversaries deliberately exploit judgment errors associated with these human-in-the-loop security decisions [97]. Early studies of security warnings found that the warning formats in use generally failed to protect users from online risks [8, 95]. Modern warnings, by contrast, are extremely effective: a recent study of over 25 million browser warning impressions showed that the warnings protected users from malware and phishing websites around 75-90% of the time [10]. The immense progress in security warning effectiveness is due to numerous, rigorous studies that have tested varied warning designs using diverse experimental methods and analytic lenses.

The primary methods of early security warning studies were laboratory experiments involving supervised tasks, user interviews, and surveys [8, 9, 11, 18, 20, 95, 96]. These studies examined users' beliefs and decision-making processes, using structured models from warning science literature to identify reasons that warnings failed to change user behaviors. Security researchers typically used the Communication–Human Information Processing (C-HIP) model, which describes five information processing stages that users undergo when receiving a warning: noticing, comprehending, believing, being motivated, and behaving as intended [98]. By determining the stage at which information processing was failing, researchers learned how to modify warning designs to increase the desired effect [9, 11].

### 2.9 Limitations
It can be challenging to cause users to perceive realistic risk in a laboratory setting, requiring the use of deception and thoughtful experimental design [95, 96]. Laboratory studies have limitations, but they provide valuable insights into user behavior and the effectiveness of warnings.