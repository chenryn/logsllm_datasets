title:Adapting Security Warnings to Counter Online Disinformation
author:Ben Kaiser and
Jerry Wei and
Eli Lucherini and
Kevin Lee and
J. Nathan Matias and
Jonathan R. Mayer
Adapting Security Warnings to Counter Online Disinformation
Ben Kaiser
Princeton University
Jerry Wei
Princeton University
J. Nathan Matias
Cornell University
Abstract
Disinformation is proliferating on the internet, and platforms
are responding by attaching warnings to content. There is little
evidence, however, that these warnings help users identify or
avoid disinformation. In this work, we adapt methods and re-
sults from the information security warning literature in order
to design and evaluate effective disinformation warnings.
In an initial laboratory study, we used a simulated search
task to examine contextual and interstitial disinformation
warning designs. We found that users routinely ignore con-
textual warnings, but users notice interstitial warnings—and
respond by seeking information from alternative sources.
We then conducted a follow-on crowdworker study with
eight interstitial warning designs. We conﬁrmed a signiﬁcant
impact on user information-seeking behavior, and we found
that a warning’s design could effectively inform users or con-
vey a risk of harm. We also found, however, that neither user
comprehension nor fear of harm moderated behavioral effects.
Our work provides evidence that disinformation warnings
can—when designed well—help users identify and avoid dis-
information. We show a path forward for designing effective
warnings, and we contribute repeatable methods for evaluat-
ing behavioral effects. We also surface a possible dilemma:
disinformation warnings might be able to inform users and
guide behavior, but the behavioral effects might result from
user experience friction, not informed decision making.
1
Introduction
Disinformation is spreading widely on the internet, often
propelled by political motives [1, 2]. Platforms are responding
by attaching warnings to disinformation content, in order to
inform users and guide their actions. Facebook implemented
disinformation warnings as early as December 2016 [3], and
Google [4], Bing [5], and Twitter [6] have adopted similar
content notices. There has been substantial public debate
about the propriety of disinformation warnings, especially
after Twitter began labeling tweets by U.S. President Donald
Trump in May 2020 [7]. But recent studies provide scant ev-
idence that these warnings can meaningfully inﬂuence user
beliefs or behaviors, and it is an open question whether warn-
ings are promising or futile for combating disinformation.
Security researchers faced a similar challenge over a decade
ago, when studies showed that warnings for malware, phish-
ing, and other online threats broadly failed to protect users [8,
Eli Lucherini 
Princeton University
Jonathan Mayer
Princeton University
Kevin Lee
Princeton University
9]. After a series of iterative, multi-method studies [10–21],
security warnings now reliably inform user security decisions
and help users avoid harmful and inauthentic content [10, 17].
In this work, we adapt methods and results from the informa-
tion security warning literature in order to design and evaluate
effective disinformation warnings.
A key ﬁnding from security research that we adapt to disin-
formation is that contextual warnings, which do not interrupt
the user or compel action, are far less effective at changing be-
havior than interstitial warnings, which interrupt the user and
require interaction [8, 9, 17]. Our work is, to our knowledge,
the ﬁrst to evaluate interstitial disinformation warnings.
Another relevant contribution from the security literature
is a set of rigorous qualitative and quantitative methods for
evaluating warnings, including structured models, realistic
guided tasks, user interviews, and ﬁeld studies (e.g., [11, 13,
15–18]). Our work adapts these methods to empirically exam-
ine contextual and interstitial disinformation warnings.
Across two studies, we use qualitative approaches (think-
aloud exercises, interviews, and inductive coding) to under-
stand user perceptions of disinformation warnings, as well as
quantitative measures of the warnings’ effects on user behav-
ior. We consider the following research questions:
1. After encountering contextual and interstitial disinforma-
tion warnings, how often do users change their behavior
by opting for alternative sources of information?
2. Why do some users choose not to change their behaviors
after encountering contextual and interstitial disinforma-
tion warnings?
3. Can interstitial warnings that are highly informative ef-
fectively change user behavior?
4. Can interstitial warnings that are highly threatening ef-
fectively change user behavior?
We ﬁrst conducted a laboratory experiment (n = 40) in
which participants searched for speciﬁc facts on Google and
encountered an interstitial or contextual disinformation warn-
ing for certain search results (Section 3). The interstitial warn-
ing was substantially more effective at changing user behavior
than the contextual warning, in large part because users did
not notice or comprehend the more subtle contextual warning.
In post-task interviews, participants described two reasons for
the interstitial warning’s strong behavioral effect: the infor-
mativeness of the warning’s messaging and the risk of harm
conveyed by the warning’s threatening design.
We then conducted a follow-on crowdworker study (n =
238), examining eight interstitial warning designs (Section 4).
We conﬁrmed the strong behavioral effects of interstitial warn-
ings. We also found, however, that neither user comprehension
nor perceived risk of harm appeared to moderate those effects.
Our results provide evidence that interstitial disinformation
warnings can both inform users and guide user behavior. We
demonstrate scalable and repeatable methods for measuring
warning effectiveness and testing theories of effect. We also
surface a possible dilemma: the behavioral effects of disin-
formation warnings may be attributable to user experience
friction, rather than informed decision making. Our work high-
lights a path forward for designing effective warnings, and we
close by encouraging iterative research and improvement for
disinformation warnings—just like the information security
community has successfully done for security warnings.
2 Background and Related Work
Disinformation research is dispersed across academic disci-
plines.1 Recent work has predominantly focused on measure-
ment (e.g., of content, campaigns, or user interactions) [22–
40], or on developing automated detection methods [41–55].
In this section, we begin with background on disinforma-
tion websites, which are the targets for the warnings in our
studies. We then discuss related work on the effects of and
responses to disinformation. Finally, we describe the security
warnings literature, which is the inspiration for this work.
2.1 Disinformation Websites
Disinformation campaigns are often multimodal, exploit-
ing many different social and media channels at once [56].
These campaigns use websites as an important tool to host
content for distribution across platforms and generate ad rev-
enue [25, 28, 57–60]. Disinformation websites are designed
to intentionally deceive users into believing that they are le-
gitimate news outlets.2 Our work examines whether warnings
can counter this deception and help users contextualize or
avoid disinformation websites.
2.2 Effects of Disinformation
Disinformation campaigns hijack the heuristics that users
rely on to make accurate judgments about the truthfulness of
information [63]. For example, disinformation campaigns of-
ten mimic credibility indicators from real news sources [64] or
use social media bots to create the appearance of support [29].
Misperceptions that individuals hold after consuming dis-
information are difﬁcult to dispel [63]. Collectively, a misin-
1We use the term “disinformation” here and throughout this work, because
our studies focus on warning users about intentionally misleading websites.
We believe that our results generalize to misleading content regardless of
intent (i.e., “misinformation”), because laboratory participants did not identify
a website’s motive as a salient concern and we ﬁnd that interstitial warning
behavioral effects are not signiﬁcantly related to warning content.
2These websites are sometimes termed “fake news” or “junk news” in
related work (e.g., [57, 61, 62]).
formed populace may make social and political decisions that
are not in the society’s best interests [65] (e.g., failing to miti-
gate climate change [66]). Inﬂuencing policy in this way—by
shaping public perception and creating division—is a goal of
many campaigns, especially by state-level actors [67].
Presenting a warning before exposure to disinformation
can prevent harmful effects in several ways. Warnings can
induce skepticism, so that users are less likely to take disin-
formation at face value [68]. Warnings can also make users
more susceptible to corrections [63, 69]. Finally, warnings
may cause users to not read the disinformation at all.
2.3 Responses to Disinformation
There are three main types of responses to disinforma-
tion that platforms and researchers have considered [70]. The
ﬁrst is deranking disinformation by changing recommenda-
tion algorithms [71]. Academics have studied this approach
in simulated models of social networks [72–77], although
not in realistic settings or with real users. Second, platforms
have repeatedly removed disinformation content and banned
accounts that promote disinformation [78–80]. Neither plat-
forms nor researchers have established evidence on the effects
of these takedowns. Finally, platforms have added warnings
and similar forms of context to posts [4–6, 81, 82].
We note that there are important speech distinctions be-
tween these responses. When a platform removes content, it
unilaterally makes speech less accessible to users. When a
platform deranks content, it leaves the content available, but
it unilaterally curtails speech distribution and discoverability.
The potential promise of disinformation warnings is that they
respond to problematic speech with counterspeech: platforms
inform and protect users, without making unilateral decisions
about content availability, distribution, or discoverability. As
we discuss in Section 4.8, our work poses a possible dilemma
for disinformation warnings as speech regulation: warnings
can inform users and guide user behavior, but the behavioral
effects may not be attributable to informed decision making.
Fact Check Warnings The most well-studied disinforma-
tion warnings are contextual labels indicating a story has been
“disputed” or “rated false” by fact checkers [83–88]. These
labels constituted Facebook’s ﬁrst major effort to counter dis-
information [81], and Google [4], Bing [5], and Twitter [6]
have taken similar approaches. Facebook eventually discon-
tinued use of “disputed” warnings after determining based on
internal studies that the warnings were of limited utility [81].
More recently Facebook, Instagram, and Twitter all deployed
new warning formats, including interstitials [89, 90].
Some studies of fact check warnings reported no signiﬁ-
cant effects on participant perceptions of disinformation [84,
85], while others found moderate effects under certain con-
ditions [83, 87, 88, 91, 92]. Pennycook et al. found that fact
check warnings caused participants to rate disinformation as
less accurate after repeated warning exposure, but not with
a single exposure [83]. Another study by Pennycook et al.
identiﬁed a counterproductive implied truth effect: attaching
a fact check warning to some headlines caused participants
to perceive other headlines as more accurate [87]. Seo et al.
found that fact check warnings caused participants to perceive
stories as less accurate, but the effect did not persist when
participants encountered the same stories later [88]. Mena
found that fact check warnings had small negative effects on
perceived credibility of news content on social media and
self-reported likelihood of sharing [91]. Finally, Moravec et
al. examined how fact check warnings can induce instinctual
cognitive responses in users and cause users to thoughtfully
incorporate new information into their decision making; a
warning design that combined both mechanisms showed a
moderate effect on social media post believability [92].
Related Links Bode and Vraga examined the effects of
providing related links to alternative, credible sources of in-
formation alongside misinformation [93]. The study found
that when related links debunked misinformation, participants
who initially believed the disinformation showed a limited
tendency toward corrected beliefs. Facebook, Google Search,
and Bing all currently use related links warning designs.
Highlighting Falsehoods Garrett et al. tested a two-part
warning, where participants were ﬁrst informed that a fact-
checker had identiﬁed factual errors in a story, then those
errors were highlighted in the body of the story [94]. Among
users already predisposed to reject the misinformation, this
treatment signiﬁcantly increased accuracy of beliefs, but it had
no effect among users inclined to believe the misinformation.
Methods of Prior Work In all of these studies, participants
were presented with screenshots of simulated social media
posts, then were posed survey questions such as how truthful
they thought the posts were and whether they would con-
sider sharing the posts on social media. These methods can
inform theories about how users will respond in real-world
settings, but generalizations are tenuous because the methods
involve highly artiﬁcial tasks and self-reported predictions
about behavior. As we discuss below, security research has
found that in order to measure realistic responses to warn-
ings, it is important to design experimental tasks that involve
realistic systems, realistic risks, and measurement of actual
participant behavior [9, 11, 95, 96].
2.4 Security Warnings
Effective warnings are essential for security, because there
are certain security decisions that systems cannot consistently
make on behalf of users. Adversaries deliberately exploit
judgment errors associated with these human-in-the-loop se-
curity decisions [97]. Early studies of security warnings found
that the warning formats that were currently in use generally
failed to protect users from online risks [8, 95]. Modern warn-
ings, by contrast, are extremely effective: a recent study of
over 25 million browser warning impressions showed that the
warnings protected users from malware and phishing websites
around 75-90% of the time [10]. The immense progress in
security warning effectiveness is due to numerous, rigorous
studies that for over a decade have tested varied warning de-
signs using diverse experimental methods and analytic lenses.
The primary methods of early security warning studies
were laboratory experiments involving supervised tasks, user
interviews, and surveys [8, 9, 11, 18, 20, 95, 96]. These studies
examined users’ beliefs and decision-making processes, in
part by using structured models from warning science liter-
ature to identify reasons that warnings failed to change user
behaviors. Security researchers typically used the Communi-
cation–Human Information Processing (C-HIP) model, which
describes ﬁve information processing stages that users un-
dergo when receiving a warning. Users must notice the warn-
ing, comprehend the warning’s meaning, believe the warning,
be motivated to heed the warning, and ﬁnally, behave in the
manner intended by the warning issuer [98]. By determining
the stage of the C-HIP model at which information process-
ing was failing, researchers learned how to modify warning
designs to increase the strength of the desired effect [9, 11].
Limitations
It can be difﬁcult to cause users to perceive
realistic risk in a laboratory, requiring the use of deception and
thoughtful experimental design [95, 96]. Laboratory studies