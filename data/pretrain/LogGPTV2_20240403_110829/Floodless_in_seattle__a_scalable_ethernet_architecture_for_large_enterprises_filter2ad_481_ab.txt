into several appropriately-sized VLANs, administrators can reduce
the broadcast overhead imposed on hosts in each VLAN, and also
ensure isolation among different host groups. Compared with IP,
VLANs simplify mobility, as hosts may retain their IP addresses
while moving between bridges in the same VLAN. This also re-
duces policy reconﬁguration overhead. Unfortunately, VLANs in-
troduces several problems:
Trunk conﬁguration overhead: Extending a VLAN across multi-
ple bridges requires the VLAN to be trunked (provisioned) at each
of the bridges participating in the VLAN. Deciding which bridges
should be in a given VLAN must consider trafﬁc and mobility pat-
terns to ensure efﬁciency, and hence is often done manually.
Limited control-plane scalability: Although VLANs reduce the
broadcast overhead imposed on a particular end host, bridges pro-
visioned with multiple VLANs must maintain forwarding-table
entries and process broadcast trafﬁc for every active host in ev-
ery VLAN visible to themselves. Unfortunately, to enhance re-
source utilization and host mobility, and to reduce trunk conﬁg-
uration overhead, VLANs are often provisioned larger than neces-
sary, worsening this problem. A large forwarding table complicates
bridge design, since forwarding tables in Ethernet bridges are typi-
cally implemented using Content-Addressable Memory (CAM), an
expensive and power-intensive technology.
Insufﬁcient data-plane efﬁciency: Larger enterprises and data cen-
ters often have richer topologies, for greater reliability and perfor-
mance. Unfortunately, a single spanning tree is used in each VLAN
to forward packets, which prevents certain links from being used.
Although conﬁguring a disjoint spanning tree for each VLAN [9,
21] may improve load balance and increase aggregate throughput,
effective use of per-VLAN trees requires periodically moving the
roots and rebalancing the trees, which must be manually updated
as trafﬁc shifts. Moreover, inter-VLAN trafﬁc must be routed via
IP gateways, rather than shortest physical paths.
3. NETWORK-LAYER ONE-HOP DHT
The goal of a conventional Ethernet is to route packets to a des-
tination speciﬁed by a MAC address. To do this, Ethernet bridges
collectively provide end hosts with a service that maps MAC ad-
dresses to physical locations. Each bridge implements this service
by maintaining next-hop pointers associated with MAC addresses
in its forwarding table, and relies on domain-wide ﬂooding to keep
these pointers up to date. Additionally, Ethernet also allows hosts
to look up the MAC address associated with a given IP address by
broadcasting Address Resolution Protocol (ARP) messages.
In order to provide the same interfaces to end hosts as conven-
tional Ethernet, SEATTLE also needs a mechanism that maintains
mappings between MAC/IP addresses and locations. To scale to
large networks, SEATTLE operates a distributed directory service
built using a one-hop, network-level DHT. We use a one-hop DHT
to reduce lookup complexity and simplify certain aspects of net-
work administration such as trafﬁc engineering and troubleshoot-
ing. We use a network-level approach that stores mappings at
switches, so as to ensure fast and efﬁcient reaction to network fail-
ures and recoveries, and avoid the control overhead of a separate
directory infrastructure. Moreover, our network-level approach al-
lows storage capability to increase naturally with network size, and
exploits caching to forward data packets directly to the destination
without needing to traverse any intermediate DHT hops [22, 23].
3.1 Scalable key-value management with
a one-hop DHT
Our distributed directory has two main parts. First, running
a link-state protocol ensures each switch can observe all other
switches in the network, and allows any switch to route any other
switch along shortest paths. Second, SEATTLE uses a hash func-
tion to map host information to a switch. This host information is
maintained in the form of (key, value). Examples of these key-value
pairs are (MAC address, location), and (IP address, MAC address).
3.1.1 Link-state protocol maintaining switch topology
SEATTLE enables shortest-path forwarding by running a link-
state protocol. However, distributing end-host information in link-
state advertisements, as advocated in previous proposals [8, 6, 10,
7], would lead to serious scaling problems in the large networks we
consider. Instead, SEATTLE’s link-state protocol maintains only
the switch-level topology, which is much more compact and sta-
ble. SEATTLE switches use the link-state information to compute
shortest paths for unicasting, and multicast trees for broadcasting.
To automate conﬁguration of the link-state protocol, SEATTLE
switches run a discovery protocol to determine which of their links
are attached to hosts, and which are attached to other switches.
Distinguishing between these different kinds of links is done by
sending control messages that Ethernet hosts do not respond to.
This process is similar to how Ethernet distinguishes switches from
hosts when building its spanning tree. To identify themselves in the
link-state protocol, SEATTLE switches determine their own unique
switch IDs without administrator involvement. For example, each
switch does this by choosing the MAC address of one of its inter-
faces as its switch ID.
3.1.2 Hashing key-value pairs onto switches
Instead of disseminating per-host information in link-state ad-
vertisements, SEATTLE switches learn this information in an on-
demand fashion, via a simple hashing mechanism. This informa-
tion is stored in the form of (key= k,value= v) pairs. A publisher
switch sa wishing to publish a (k, v) pair via the directory service
uses a hash function F to map k to a switch identiﬁer F(k) = rk,
and instructs switch rk to store the mapping (k, v). We refer to rk
as the resolver for k. A different switch sb may then look up the
value associated with k by using the same hash function to iden-
tify which switch is k’s resolver. This works because each switch
Figure 1: Keys are consistently hashed onto resolver switches (si).
knows all the other switches’ identiﬁers via link-state advertise-
ments from the routing protocol, and hence F works identically
across all switches. Switch sb may then forward a lookup request
to rk to retrieve the value v. Switch sb may optionally cache the
result of its lookup, to reduce redundant resolutions. All control
messages, including lookup and publish messages, are unicast with
reliable delivery.
Reducing control overhead with consistent hashing: When the
set of switches changes due to a network failure or recovery, some
keys have to be re-hashed to different resolver switches. To mini-
mize this re-hashing overhead, SEATTLE utilizes Consistent Hash-
ing [24] for F. This mechanism is illustrated in Figure 1. A con-
sistent hashing function maps keys to bins such that the change of
the bin set causes minimal churn in the mapping of keys to bins. In
SEATTLE, each switch corresponds a bin, and a host’s information
corresponds to a key. Formally, given a set S = {s1, s2, ..., sn} of
switch identiﬁers, and a key k,
F(k) = argmin∀si∈S{D(H(k), H(si))}
where H is a regular hash function, and D(x, y) is a simple met-
ric function computing the counter-clockwise distance from x to y
on the circular hash-space of H. This means F maps a key to the
switch with the closest identiﬁer not exceeding that of the key on
the hash space of H. As an optimization, a key may be addition-
ally mapped to the next m closest switches along the hash ring, to
improve resilience to multiple failures. However, in our evaluation,
we will assume this optimization is disabled by default.
Balancing load with virtual switches: The scheme described so
far assumes that all switches are equally powerful, and hence low-
end switches will need to service the same load as more powerful
switches. To deal with this, we propose a new scheme based on
running multiple virtual switches on each physical switch. A single
switch locally creates one or more virtual switches. The switch may
then increase or decrease its load by spawning/destroying these
virtual switches. Unlike techniques used in traditional DHTs for
load balancing [23], it is not necessary for our virtual switches to
be advertised to other physical switches. To reduce size of link-
state advertisements, instead of advertising every virtual switch in
the link-state protocol, switches only advertise the number of vir-
tual switches they are currently running. Each switch then locally
computes virtual switch IDs using the following technique. All
switches use the same function R(s, i) that takes as input a switch
identiﬁer s and a number i, and outputs a new identiﬁer unique to
the inputs. A physical switch w only advertises in link-state adver-
tisements its own physical switch identiﬁer sw and the number L
of virtual switches it is currently running. Every switch can then
determine the virtual identiﬁers of w by computing R(sw, i) for
1 ≤ i ≤ L. Note that it is possible to automate determining a
desirable number of virtual switches per physical switch [25].
Enabling ﬂexible service discovery: This design also enables more
ﬂexible service discovery mechanisms without the need to perform
network-wide broadcasts. This is done by utilizing the hash func-
tion F to map a string deﬁning the service to a switch. For example,
a printer may hash the string “PRINTER” to a switch, at which it
Figure 2: Hierarchical SEATTLE hashes keys onto regions.
may store its location or address information. Other switches can
then reach the printer using the hash of the string. Services may
also encode additional attributes, such as load or network location,
as simple extensions. Multiple servers can redundantly register
themselves with a common string to implement anycasting. Ser-
vices can be named using techniques shown in previous work [26].
3.2 Responding to topology changes
The switch-level topology may change if a new switch/link is
added to the network, an existing switch/link fails, or a previously
failed switch/link recovers. These failures may or may not partition
the network into multiple disconnected components. Link failures
are typically more common than switch failures, and partitions are
very rare if the network has sufﬁcient redundancy.
In the case of a link failure/recovery that does not partition a
network, the set of switches appearing in the link-state map does
not change. Since the hash function F is deﬁned with the set of
switches in the network, the resolver a particular key maps to will
not change. Hence all that needs to be done is to update the link-
state map to ensure packets continue to traverse new shortest paths.
In SEATTLE, this is simply handled by the link-state protocol.
k
to rnew
k
k
differs from a new resolver rnew
However, if a switch fails or recovers, the set of switches in the
link-state map changes. Hence there may be some keys k whose
old resolver rold
. To deal with
k
this, the value (k, v) must be moved from rold
. This is
handled by having the switch sk that originally published k mon-
itor the liveness of k’s resolver through link-state advertisements.
When sk detects that rnew
k , it republishes (k, v)
to rnew
after a
timeout. Additionally, when a value v denotes a location, such as a
switch id s, and s goes down, each switch scans the list of locally-
stored (k, v) pairs, and remove all entries whose value v equals s.
Note this procedure correctly handles network partitions because
the link-state protocol ensures that each switch will be able to see
only switches present in its partition.
3.3 Supporting hierarchy with a multi-level,
. The value (k, v) is eventually removed from rold
differs from rold
k
k
k
one-hop DHT
The SEATTLE design presented so far scales to large, dynamic
networks [27]. However, since this design runs a single, network-
wide link-state routing protocol, it may be inappropriate for net-
works with highly dynamic infrastructure, such as networks in de-
veloping regions [3]. A single network-wide protocol may also be
inappropriate if network operators wish to provide stronger fault
isolation across geographic regions, or to divide up administrative
control across smaller routing domains. Moreover, when a SEAT-
TLE network is deployed over a wide area, the resolver could lie
far both from the source and destination. Forwarding lookups over
long distances increases latency and makes the lookup more prone
to failure. To deal with this, SEATTLE may be conﬁgured hier-
archically, by leveraging a multi-level, one-hop DHT. This mecha-
nism is illustrated in Figure 2.
A hierarchical network is divided into several regions, and a
backbone providing connectivity across regions. Each region is
connected to the backbone via its own border switch, and the back-
packet to ra. Since ra may be several hops away, sb encapsulates
the packet with an outer header with ra’s address as the destination.
Switch ra then looks up a’s location sa, and forwards the packet
on towards sa. In order to limit the number of data packets travers-
ing the resolver, ra also notiﬁes sb that a’s current location is sa.
Switch sb then caches this information. While forwarding the ﬁrst
few packets of a ﬂow via a resolver switch increases path lengths,
in the next section we describe an optimization that allows data
packets to traverse only shortest paths, by piggy-backing location
information on ARP replies.
Note SEATTLE manages per-host information via reactive reso-
lution, as opposed to the proactive dissemination scheme used in
previous approaches [8, 6, 10]. The scaling beneﬁts of this re-
active resolution increase in enterprise/data-center/access provider
networks because most hosts communicate with a small number
of popular hosts, such as mail/ﬁle/Web servers, printers, VoIP gate-
ways, and Internet gateways [5]. To prevent forwarding tables from
growing unnecessarily large, the access switches can apply various
cache-management policies. For correctness, however, the cache-
management scheme must not evict the host information of the
hosts that are directly connected to the switch or are registered with
the switch for resolution. Unlike Ethernet bridging, cache misses
in SEATTLE do not lead to ﬂooding, making the network resistant
to cache poisoning attacks (e.g., forwarding table overﬂow attack)
or a sudden shift in trafﬁc. Moreover, those switches that are not
directly connected to end hosts (i.e., aggregation or core switches)
do not need to maintain any cached entries.
4.2 Host address resolution
In conventional Ethernet, a host with an IP packet ﬁrst broadcasts
an ARP request to look up the MAC address of the host owning
the destination IP address contained in the request. To enhance
scalability, SEATTLE avoids broadcast-based ARP operations. In
addition, we extend ARP to return both the location and the MAC
address of the end host to the requesting switch. This allows data
packets following an ARP query to directly traverse shortest paths.
SEATTLE replaces the traditional broadcast-based ARP with
In particu-
an extension to the one-hop DHT directory service.
lar, switches use F with an IP address as the key. Speciﬁcally,
when host a arrives at access switch sa, the switch learns a’s IP ad-
dress ipa (using techniques described in Section 5.1), and computes
F(ipa) = va. The result of this computation is the identiﬁer of an-
other switch va. Finally, sa informs va of (ipa, maca). Switch va,
the address resolver for host a, then uses the tuple to handle future
ARP requests for ipa redirected by other remote switches. Note
that host a’s location resolver (i.e., F(maca)) may differ from a’s
address resolver (i.e., F(ipa)).
Optimizing forwarding paths via ARP: For hosts that issue an ARP
request, SEATTLE eliminates the need to perform forwarding via
the location resolver as mentioned in Section 4.1. This is done by
having the address resolver switch va also maintain the location of
a (i.e., sa) in addition to maca. Upon receiving an ARP request
from some host b, the address resolver va returns both maca and
sa back to b’s access switch sb. Switch sb then caches sa for future
packet delivery, and returns maca to host b. Any packets sent by b
to a are then sent directly along the shortest path to a.
It is, however, possible that host b already has maca in its ARP
cache and immediately sends data frames destined to maca with-
out issuing an ARP request in advance. Even in such a case, as long
as the sb also maintains a’s location associated with maca, sb can
forward those frames correctly. To ensure access switches cache
the same entries as hosts, the timeout value that an access switch
applies to the cached location information should be larger than the
Figure 3: Packet forwarding and lookup in SEATTLE.
bone is composed of the border switches of all regions. Information
about regions is summarized and propagated in a manner similar to
areas in OSPF. In particular, each switch in a region knows the
identiﬁer of the region’s border switch, because the border switch
advertises its role through the link-state protocol. In such an en-
vironment, SEATTLE ensures that only inter-region lookups are
forwarded via the backbone while all regional lookups are handled
within their own regions, and link-state advertisements are only
propagated locally within regions. SEATTLE ensures this by deﬁn-
ing a separate regional and backbone hash ring. When a (k, v) is
inserted into a region P and is published to a regional resolver rP
k
(i.e., a resolver for k in region P ), rP
k additionally forwards (k, v)
to one of the region P ’s border switches bP . Then bP hashes k
again onto the backbone ring and publishes (k, v) to another back-
Q
bone switch b
k , which is a backbone resolver for k and a border
Q
k stores k’s informa-
switch of region Q at the same time. Switch b
tion. If a switch in region R wishes to lookup (k, v), it forwards
the lookup ﬁrst to its local resolver rR
k , which in turn forwards it to
Q
bR, and bR forwards it to b
k . As an optimization to reduce load on
Q
k may hash k and store (k, v) at a switch within
border switches, b
its own region Q, rather than storing (k, v) locally. Since switch
failures are not propagated across regions, each publisher switch
periodically sends probes to backbone resolvers that lie outside of
its region. To improve availability, (k, v) may be stored at multi-
ple backbone resolvers (as described in Section 3.1.2), and multiple
simultaneous lookups may be sent in parallel.
4. SCALING ETHERNET WITH
A ONE-HOP DHT
The previous section described the design of a distributed
network-level directory service based on a one-hop DHT. In this
section, we describe how the directory service is used to provide
efﬁcient packet delivery and scalable address resolution. We ﬁrst
brieﬂy describe how to forward data packets to MAC addresses in
Section 4.1. We then describe our remaining contributions: an opti-
mization that eliminate the need to look up host location in the DHT
by piggy-backing that information on ARP requests in Section 4.2,
and a scalable dynamic cache-update protocol in Section 4.3.
4.1 Host location resolution
Hosts use the directory service described in Section 3 to pub-
lish and maintain mappings between their MAC addresses and their
current locations. These mappings are used to forward data pack-
ets, using the procedure shown in Figure 3. When a host a with
MAC address maca ﬁrst arrives at its access switch sa, the switch
must publish a’s MAC-to-location mapping in the directory ser-
vice. Switch sa does this by computing F(maca) = ra, and in-
structing ra to store (maca, sa). We refer to ra as the location re-
solver for a. Then, if some host b connected to switch sb wants to
send a data packet to maca, b forwards the data packet to sb, which
in turn computes F(maca) = ra. Switch sb then and forwards the
ARP cache timeout used by end hosts 2. Note that, even if the cache
and the host become out of sync (due to switch reboot, etc.), SEAT-
TLE continues to operate correctly because switches can resolve
a host’s location by hashing the host’s MAC address to the host’s
location resolver.
4.3 Handling host dynamics
h
h
. In this case, snew
Hosts can undergo three different kinds of changes in a SEAT-
TLE network. First, a host may change location, for example if it
has physically moved to a new location (e.g., wireless handoff), if
its link has been plugged into a different access switch, or if it is a
virtual machine and has migrated to a new hosting system that al-
lows the VM to retain its MAC address. Second, a host may change
its MAC address, for example if its NIC card is replaced, if it is a
VM and has migrated to a new hosting system that requires the VM
to use the host’s MAC address, or if multiple physical machines
collectively acting as a single server or router (to ensure high avail-
ability) experience a fail-over event [28]. Third, a host may change
its IP address, for example if a DHCP lease expires, or if the host is
manually reconﬁgured. In practice, multiple of these changes may
occur simultaneously. When these changes occur, we need to keep
the directory service up-to-date, to ensure correct packet delivery.
SEATTLE handles these changes by modifying the contents of
the directory service via insert, delete, and update operations. An
insert operation adds a new (k, v) pair to the DHT, a delete opera-
tion removes a (k, v) pair from the DHT, and the update operation
updates the value v associated with a given key k. First, in the case
of a location change, the host h moves from one access switch sold
h
to another snew
inserts a new MAC-to-location
entry. Since h’s MAC address already exists in the DHT, this action
will update h’s old location with its new location. Second, in the
case of a MAC address change, h’s access switch sh inserts an IP-
to-MAC entry containing h’s new MAC address, causing h’s old
IP-to-MAC mapping to be updated. Since a MAC address is also
used as a key of a MAC-to-location mapping, sh deletes h’s old
MAC-to-location mapping and inserts a new mapping, respectively