6
1.3 多层感知器
在实践中，全连接层也可以被矩阵乘法实现：
z =Wx+b (1.5)
式中，W ∈Rm×n是用来表示权重的矩阵，z ∈Rm,x∈Rn,b∈Rm分别用来表示输出、输
入和偏差的向量。在公式(1.5)里的例子中，m=2，n=3，即W ∈R2×3。
2 3
2 3 2 3 2 3
6x 7
6 17
6z 17 6w 11 w 12 w 1376 7 6b 17
4 5=4 56 6x 27 7+4 5 (1.6)
z w w w 4 5 b
2 21 22 23 2
x
3
1.3 多层感知器
多层感知器（Multi-LayerPerceptron，MLP）(Rosenblatt,1958;Rucketal.,1990)最初指至少有
两个全连接层的网络。图1.5展现了一个有四个全连接层的多层感知器。那些在输入层和输出层
中间的网络层被隐藏（Hidden）了，因为一般来说从网络外面是没有办法直接接触它们的，所以
被统称为隐藏层（HiddenLayers）。相比只有一个全连接层的网络，MLP可以从更复杂的数据中
学习。从另外一个角度来看，MLP的学习能力是大于单一层网络的学习能力的。但是拥有更多的
隐藏层并不意味着一个网络会有更强的学习能力。通用近似定理说的是：一个有一层隐藏层的神
经网络（类似于有一层隐藏层的MLP）和任何可挤压的激活函数（见后文的sigmoid和tanh）在
这一层网络有足够多神经元的情况下，可以估算出任何博莱尔可测函数 (Goodfellowet al., 2016;
Horniketal.,1989;Samuel,1959)。但是实际上，这样的网络可能会非常难以训练或者容易过拟合
（Overfit）（见后文）。因为隐藏层非常大，所以一般的深度神经网络都会有几层隐藏层来降低训
练难度。
为什么需要多层网络？为了回答这个问题，我们首先通过逻辑运算的几个例子来展示一个网
络是怎么估算一个方程的。我们会考虑的逻辑运算有：与（AND）、或（OR）、同或（XNOR）、
异或（XOR）、或非（NOR）、与非（NAND）。这些运算输入都是两个二进制数字，然后输出为1
或者0。如与（AND），只有两个输入同时为1，AND才会输出1。这些简单的逻辑计算可以很容
易就被感知器学习，就像公式(1.7)里展现的那样。
8
>0
f(x)= , z =w x +w x +b (1.7)
>: 1 1 2 2
0 其他情况
图1.6展示了被感知器定义的决策边界可以很轻松地把AND、OR、NOR和NAND运算的0
7
第1章 深度学习入门
和1分开出来，但是，XOR或XNOR的决策边界是不可能被找到的。
输入层 隐藏层1 隐藏层2 隐藏层3 输出层
图1.5 一个具有三个隐藏层和一个输出层的多层感知器。图中使用al 表示神经元，其中l代表
i
层的索引，i代表输出的索引
11 22
O O
O O O O
图1.6 左上：有两个输入和一个输出的感知器。剩下的是：不同的用来把0（×）和1（•）分开
的决策边界。在这个单层感知器中，能找到AND、OR、NOR和NAND的决策边界，但找
不到可以实现XOR和XNOR的决策边界
因为我们不能用一个线性模型像单个感知器那样直接估算XOR，所以必须要转化输入。图1.7
展现了一个用有一层隐藏层的MLP去估算XOR，这个MLP首先将通过估计OR和NAND运算
把x ,x 转换到了一个新的空间，然后在这个转换过的空间里，这些点就可以被一条估算AND
1 2
的平面分开了。这个被转换过后的空间也被称为特征空间。这个例子说明了怎么通过特征的学习
来改善一个模型的学习能力。
8
1.4 激活函数
O O
图1.7 左：一个可以估算XOR的MLP。中和右：把原始数据点转化到特征空间，从而使得这些
数据点变得线性可分离
1.4 激活函数
矩阵的加减和乘除运算都是线性运算符，但是一个线性模型的学习能力还是相对有限的。举
例来说，线性模型不能轻易地估算一个余弦函数。因为大多数深度神经网络解决的真实问题都不
可能被简单地映射到一个线性转换，所以非线性在深度神经网络里至关重要。
实际上，深度学习网络的非线性是通过激活函数来介入的。这些激活函数都是针对每一个元
素（Element-Wise）运算的。我们需要这些激活函数来帮助模型获得有任意数值的概率向量。激
活函数的选择要根据具体的运用场景来考虑。虽然有一些激活函数在大多数的情况下效果都是不
错的，但是在具体的实际运用中，可能还有更好的选择。所以激活函数的设计至今都还是一个活
跃的研究方向。本节主要介绍四种非常常见的激活函数：sigmoid、tanh、ReLU和softmax。
逻辑函数sigmoid在作为激活函数时，将输入控制在了0和1之间，如公式(1.8)所示。sigmoid
方程可以在网络的最后一层，使用来做一些分类的任务，以代表0%～100%的概率。比如说，一
个二维的分类器可以把sigmoid方程放在最后一层，来把其数值局限在0和1之中，然后我们可
以用一个简单的临界值决定最终输出的标签是什么（0或1）。
1
f(z)= (1.8)
1+e−z
与sigmoid函数类似的是，hyperbolictangent（tanh）把输出值控制到了−1和1之间，就
如公式(1.9)所定义那样。tanh函数可以在隐藏层中使用来提高非线性(Glorotetal.,2011)。它也
可以在输出层中使用，比如网络可以输出像素数值在−1和1的图像。
ez−e−z
f(z)= (1.9)
ez+e−z
9
第1章 深度学习入门
图1.8 展现三个元素单位运用的方程：sigmoid、tanh和ReLU。sigmoid把数值限制在了0和1之
间，而tanh则把数值限制在了−1和1之间。当输入是负数时，ReLU则输出0，但当输入
是正数时，其输出等于输入
在公式 (1.10) 中，我们定义了整流线性单元（Rectified Linear Unit，ReLU）函数，也叫作
rectifier。ReLU被广泛地使用于不同的研究当中(Caoetal.,2017;Heetal.,2016;Nohetal.,2015)，
在很多层的网络中ReLU通常会比sigmoid和tanh性能更好(Glorotetal.,2011)。
8
>:
z 当z >0
在实际运用中，ReLU有以下优势。
• 更易实现和计算：在实现ReLU的过程中，首先我们只需要把其数值和0做对比，然后根
据结果来设定输出是0还是z。而我们在实现sigmoid和tanh的过程当中，指数函数在大型
网络中会更难以计算。
• 网络更好优化：ReLU接近于线性，因为它是由两个线性函数组成的。这种性质就使得它更
容易被优化，我们在本章后面讲解优化细节时再讨论。
然而ReLU把负数变成0，可能会导致输出中信息的丧失。这可能是因为一个不合适的学习
速率或者负的偏差而导致的。带泄漏的（Leaky）ReLU则解决了这个问题(Xuetal.,2015)。我们
在公式(1.11)中对它进行了定义。标量α是一个较小的正数来控制斜率，使得来自负区间的信息
也可以被保留下来。
8
>:
z 当z >0
10
1.5 损失函数
有参数的ReLU（PReLU）(Heetal.,2015)和LeakyReLU很近似，它把α看作一个可以训练
的参数。目前我们还没有具体的证据表明ReLU、LeakyReLU或PReLU哪个是最好的，它们在
不同应用中往往有不同的效果。
不像上述的其他激活函数，在公式(1.12)中定义的softmax函数会根据前一层网络的输出提
供归一化。softmax函数首先计算指数函数ez，然后每一项都除以这个值进行归一。
ezi
f(z) = P (1.12)
i K ezk
k=1
在实际运用当中，softmax 函数只在最后的输出层用来归一输出向量 z，使其变成一个概率
向量。这个概率向量的每一个值都为非负数，然后它们的和最终会为1。所以，softmax函数在多
分类任务中被广泛使用，用以输出不同类别的概率。
1.5 损失函数
到目前为止，我们了解了神经网络结构的基础知识，那么网络的参数是怎么自动学习出来的
呢？这需要损失函数（LossFunction）来引导。具体来说，损失函数通常被定义为一种计算误差
的量化方式，也就是计算网络输出的预测值和目标值之间的损失值或者代价大小。损失值被用来
作为优化神经网络参数的目标，我们优化的参数包括权重和偏差等。在本节里，我们会介绍一些
基本的损失函数，1.6节会介绍如何使用损失函数优化网络参数。
交叉熵损失
在介绍交叉熵损失之前，首先来看一个类似的概念：Kullback-Leibler(KL)散度，其作用是衡
量两个分布P(x)和Q(x)的相似度：
P(x)
D KL(P∥Q)=E x∼P log =E x∼P[logP(x)−logQ(x)] (1.13)
Q(x)
KL散度是一个非负的指标，并且只有在P 和Q两个分布一样时才取值为0。因为KL散度
的第一个项和Q没有关系，我们引入交叉熵的概念并把公式的第一项移除。
H(P,Q)=−E x∼P logQ(x) (1.14)
因此，通过Q来最小化交叉熵就等同于最小化KL散度。在多类别分类任务中，深度神经网
络通过softmax函数输出的是不同类别概率的分布，而不是直接输出一个样本属于的类别。所以，
我们可以用交叉熵来测量预测分布有多好，从而训练网络。
以一个二分类任务为例。在二分类中，每一个数据样本x 都有一个对应的标签y （0或1）。
i i
11
第1章 深度学习入门
一个模型需要预测样本是0或者1的概率，用yˆ ，yˆ 来表示。因为yˆ +yˆ =1，可以把它
i,1 i,2 i,1 i,2
们改写为yˆ 和1−yˆ。前者可以代表一个类别的概率，后者可以代表另外一个类别的概率。因此，
i i
一个二分类的神经网络可以只有一个输出，且最后一层使用sigmoid。根据交叉熵的定义，我们
有：
XN
1
L=− y logyˆ +(1−y )log(1−yˆ) (1.15)
N i i i i
i=1
式中，N 代表了总数据样本的大小。因为 y 是一个 1 或者 0 的值，因此在 y logyˆ 和
i i i
(1−y )log(1−yˆ)中，对于每一个新样本，两个表达式的值只有一个不为零。若 ∀i,y =yˆ，则
i i i i
交叉熵就为0。
在多类别分类任务中，每一个样本x 都会被分到3个或者更多的类别中的一个。这时，一
i P
个模型需预测每一个类别的概率{yˆ ,yˆ ,··· ,yˆ }，且符合条件M ⩾ 3和 M yˆ = 1。在
i,1 i,2 i,M j=1 i,j
这里，每一个样本的目标写作c ，它的值域为[1,M]。同时，它也可以被转换成为一个独热编码
i
y =[y ,y ,··· ,y ]，其中只有y =1，其他的都是0。我们现在就可以把多类别分类的交
i i,1 i,2 i,M i,ci
叉熵写成以下形式：
XN XM XN
1 1
L=− y logyˆ =− (0+···+y logyˆ +···+0)