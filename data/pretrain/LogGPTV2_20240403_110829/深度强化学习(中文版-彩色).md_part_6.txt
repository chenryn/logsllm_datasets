### 1.3 多层感知器

在实践中，全连接层可以通过矩阵乘法实现：
\[ z = Wx + b \tag{1.5} \]
其中，\( W \in \mathbb{R}^{m \times n} \) 表示权重矩阵，\( z \in \mathbb{R}^m \)、\( x \in \mathbb{R}^n \) 和 \( b \in \mathbb{R}^m \) 分别表示输出向量、输入向量和偏差向量。在公式 (1.5) 的例子中，\( m=2 \)，\( n=3 \)，即 \( W \in \mathbb{R}^{2 \times 3} \)。

多层感知器（Multi-Layer Perceptron, MLP）最初指至少有两个全连接层的网络（Rosenblatt, 1958; Ruck et al., 1990）。图 1.5 展示了一个具有四个全连接层的多层感知器。位于输入层和输出层之间的网络层被称为隐藏层（Hidden Layers），因为从外部无法直接访问它们。相比只有一个全连接层的网络，MLP 可以从更复杂的数据中学习，因此其学习能力更强。然而，更多的隐藏层并不一定意味着更强的学习能力。通用近似定理指出：一个具有一层隐藏层的神经网络（类似于具有一个隐藏层的 MLP）和任何可挤压的激活函数（如后文中的 sigmoid 和 tanh），在该层有足够多神经元的情况下，可以估算出任何博莱尔可测函数 (Goodfellow et al., 2016; Hornik et al., 1989; Samuel, 1959)。但是实际上，这样的网络可能会非常难以训练或容易过拟合（Overfit）。因此，一般的深度神经网络通常会有几层隐藏层来降低训练难度。

**为什么需要多层网络？** 为了回答这个问题，我们通过几个逻辑运算的例子来展示一个网络如何估算方程。考虑以下逻辑运算：与（AND）、或（OR）、同或（XNOR）、异或（XOR）、或非（NOR）、与非（NAND）。这些运算的输入是两个二进制数字，输出为 1 或 0。例如，AND 运算只有在两个输入同时为 1 时才输出 1。这些简单的逻辑计算可以很容易被单个感知器学习，如公式 (1.7) 所示：
\[ f(x) = \begin{cases} 
1 & \text{if } z = w_1 x_1 + w_2 x_2 + b > 0 \\
0 & \text{otherwise}
\end{cases} \tag{1.7} \]

图 1.6 展示了单个感知器定义的决策边界可以轻松地将 AND、OR、NOR 和 NAND 运算的 0 和 1 区分开来，但对于 XOR 或 XNOR，这种决策边界是不可能找到的。

图 1.5: 一个具有三个隐藏层和一个输出层的多层感知器。图中使用 \( a_l^i \) 表示神经元，其中 \( l \) 代表层的索引，\( i \) 代表输出的索引。

图 1.6: 左上：有两个输入和一个输出的感知器。其余部分展示了不同的决策边界，用于将 0（×）和 1（•）分开。在这个单层感知器中，可以找到 AND、OR、NOR 和 NAND 的决策边界，但找不到实现 XOR 和 XNOR 的决策边界。

由于单个感知器不能直接估算 XOR，我们需要转换输入。图 1.7 展示了一个具有一层隐藏层的 MLP 如何估算 XOR。这个 MLP 首先通过估计 OR 和 NAND 运算将 \( x_1, x_2 \) 转换到一个新的特征空间，在这个新的空间中，这些点可以通过一个估算 AND 的平面分开。这个被转换后的空间也被称为特征空间。这个例子说明了通过特征学习可以提高模型的学习能力。

### 1.4 激活函数

矩阵的加减和乘除运算是线性运算符，但线性模型的学习能力有限。例如，线性模型不能轻易地估算一个余弦函数。大多数深度神经网络解决的实际问题都不可能简单地映射到一个线性转换，因此非线性在深度神经网络中至关重要。

实际上，深度学习网络的非线性是通过激活函数引入的。这些激活函数是对每个元素进行操作的。激活函数的选择取决于具体的使用场景。虽然有些激活函数在大多数情况下效果不错，但在具体应用中，可能还有更好的选择。因此，激活函数的设计仍然是一个活跃的研究方向。本节主要介绍四种常见的激活函数：sigmoid、tanh、ReLU 和 softmax。

- **Sigmoid 函数**：将输入值控制在 0 和 1 之间，如公式 (1.8) 所示。Sigmoid 函数常用于网络的最后一层，执行分类任务，表示 0% 到 100% 的概率。
  \[ f(z) = \frac{1}{1 + e^{-z}} \tag{1.8} \]

- **Tanh 函数**：将输出值控制在 -1 和 1 之间，如公式 (1.9) 所示。Tanh 函数可以在隐藏层中使用，提高非线性 (Glorot et al., 2011)。它也可以在输出层中使用，例如输出像素值在 -1 和 1 之间的图像。
  \[ f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \tag{1.9} \]

- **ReLU 函数**：在公式 (1.10) 中定义，称为整流线性单元（Rectified Linear Unit, ReLU）。ReLU 在许多研究中广泛应用 (Cao et al., 2017; He et al., 2016; Noh et al., 2015)，在多层网络中通常比 sigmoid 和 tanh 性能更好 (Glorot et al., 2011)。
  \[ f(z) = \begin{cases} 
  z & \text{if } z > 0 \\
  0 & \text{otherwise}
  \end{cases} \tag{1.10} \]

ReLU 的实际优势包括：
- 更易实现和计算：只需比较数值与 0，并根据结果设定输出。
- 网络更好优化：ReLU 接近线性，由两个线性函数组成，使其更容易优化。

然而，ReLU 将负数变为 0，可能导致信息丢失。带泄漏的 ReLU (Leaky ReLU) 解决了这个问题 (Xu et al., 2015)，如公式 (1.11) 所示。标量 \( \alpha \) 是一个小的正数，控制斜率，保留来自负区间的信息。
  \[ f(z) = \begin{cases} 
  z & \text{if } z > 0 \\
  \alpha z & \text{otherwise}
  \end{cases} \tag{1.11} \]

参数化的 ReLU (PReLU) (He et al., 2015) 类似于 Leaky ReLU，将 \( \alpha \) 视为可训练的参数。目前尚无明确证据表明 ReLU、Leaky ReLU 或 PReLU 哪个最好，它们在不同应用中有不同的效果。

- **Softmax 函数**：与其他激活函数不同，softmax 函数根据前一层的输出进行归一化，如公式 (1.12) 所示。Softmax 函数首先计算指数函数 \( e^z \)，然后每一项都除以总和进行归一。
  \[ f(z_i) = \frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}} \tag{1.12} \]

在实际应用中，softmax 函数仅用于最后一层，将输出向量归一化为概率向量。这个概率向量的每个值都是非负数，且总和为 1。因此，softmax 函数广泛应用于多分类任务，输出不同类别的概率。

### 1.5 损失函数

了解了神经网络结构的基础知识后，网络的参数是如何自动学习出来的呢？这需要损失函数（Loss Function）来引导。损失函数是一种量化误差的方法，计算网络输出的预测值和目标值之间的差异。损失值用于优化神经网络的参数，包括权重和偏差等。本节将介绍一些基本的损失函数，第 1.6 节将介绍如何使用损失函数优化网络参数。

#### 交叉熵损失

在介绍交叉熵损失之前，先来看一个类似的概念：Kullback-Leibler (KL) 散度，用于衡量两个分布 \( P(x) \) 和 \( Q(x) \) 的相似度：
\[ D_{KL}(P \| Q) = \mathbb{E}_{x \sim P} \left[ \log \frac{P(x)}{Q(x)} \right] = \mathbb{E}_{x \sim P} \left[ \log P(x) - \log Q(x) \right] \tag{1.13} \]

KL 散度是一个非负指标，仅在 \( P \) 和 \( Q \) 相同时取值为 0。因为 KL 散度的第一项与 \( Q \) 无关，我们引入交叉熵的概念并移除第一项：
\[ H(P, Q) = -\mathbb{E}_{x \sim P} \left[ \log Q(x) \right] \tag{1.14} \]

因此，通过最小化交叉熵就等同于最小化 KL 散度。在多类别分类任务中，深度神经网络通过 softmax 函数输出不同类别的概率分布，而不是直接输出样本所属的类别。因此，可以使用交叉熵来测量预测分布的质量，从而训练网络。

以二分类任务为例，每个数据样本 \( x_i \) 都有一个对应的标签 \( y_i \)（0 或 1）。模型需要预测样本属于 0 或 1 的概率，用 \( \hat{y}_{i,1} \) 和 \( \hat{y}_{i,2} \) 表示。因为 \( \hat{y}_{i,1} + \hat{y}_{i,2} = 1 \)，可以将其改写为 \( \hat{y}_i \) 和 \( 1 - \hat{y}_i \)。前者表示一个类别的概率，后者表示另一个类别的概率。因此，二分类的神经网络可以只有一个输出，且最后一层使用 sigmoid 函数。根据交叉熵的定义，我们有：
\[ L = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right] \tag{1.15} \]

其中，\( N \) 表示总数据样本数。因为 \( y_i \) 是 0 或 1 的值，所以在 \( y_i \log \hat{y}_i \) 和 \( (1 - y_i) \log (1 - \hat{y}_i) \) 中，对于每一个新样本，两个表达式的值只有一个不为零。若 \( \forall i, y_i = \hat{y}_i \)，则交叉熵为 0。

在多类别分类任务中，每个样本 \( x_i \) 被分为 3 个或更多类别中的一个。此时，模型需预测每个类别的概率 \( \{ \hat{y}_{i,1}, \hat{y}_{i,2}, \ldots, \hat{y}_{i,M} \} \)，且满足 \( M \geq 3 \) 和 \( \sum_{j=1}^M \hat{y}_{i,j} = 1 \)。每个样本的目标写作 \( c_i \)，其值域为 [1, M]。同时，它可以被转换为独热编码 \( y_i = [y_{i,1}, y_{i,2}, \ldots, y_{i,M}] \)，其中只有 \( y_{i,c_i} = 1 \)，其他均为 0。现在可以将多类别分类的交叉熵写成以下形式：
\[ L = -\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^M y_{i,j} \log \hat{y}_{i,j} \]